% We start this section remembering that if $\{X_n\}_{n \ge 0}$ is a $p_n$-\Name{} in direction $\ell$ we can write it as in~\eqref{xn-incremnto1}, hence we have
% $$
% X_n = \sum_{i=1}^n \big(1_{\{E_{i-1}\}} \xi_i + 1_{\{E_{i-1}^c\}} 1_{\{U_i > p_i \}} \xi_i + 1_{\{E_{i-1}^c\}}1_{\{ U_i \leq p_i\}} \gamma_i \big) \,,
% $$
% where $\{U_i\}_{i \geq 1}$ is a sequence  of i.i.d. random variables with uniform distribution in [0, 1], $E_i = \{ \exists\;  k < i \; \text{ such that }\;  X_k = X_i \}$ for all $i \ge 1$, $\{\xi_i, \FF_i\}_{i \geq 1}$ is an increment of a $d$-martingale with zero mean and $\{\gamma_i, \FF_i\}_{i\geq 1}$ is random vector such that $\EE[\gamma_i \cdot \ell |\FF_{i-1}] \ge \lambda$ .

Let $\{X_n\}_{n \ge 0}$ be a $p_n$-\Name{}. We can rewrite~\eqref{xn-incremnto1} as 
\begin{align}\label{xn-incremento2}
\begin{split}
X_n  & = \sum_{i=1}^n \big( \um_{ E_{i-1} \cup \{ U_i > p_i\}} \xi_i + \um_{E_{i-1}^c \cap \{ U_i \leq p_i\}} \gamma_i \big)
\\
& = \sum_{i=1}^n \big( \xi_i + \um_{E_{i-1}^c \cap \{ U_i \leq p_i\}} (\gamma_i - \xi_i) \big) \,.
\end{split}
\end{align}
%Before we provide the proofs of the main results for the $p_n$-\Name{}, let us point out that, 
Also for the sake of simplicity, we will henceforth work with the \[B_{\cdot}^n := \frac{X_{\lfloor n \cdot \rfloor}}{n^{1/2}}\;.\] instead of its interpolated version $\Hat{B}_{t}^n$ in \eqref{eq:B} which is continuous in $[0, \infty)$.  
More generally, 
in order to simplify writing and notation, if we have a sequence of c\`adl\`ag processes with values in $\mathbb{R}^m$, for $m \ge 2$, of the form $\Sigma^{n}_t = \Sigma_{\lfloor n t \rfloor}$, we denote by $\Hat{\Sigma}^{n}_t$ its linearly interpolated version, i.e., 
\begin{equation*}
\Hat{\Sigma}^{n}_t := \Sigma_{\lfloor n t \rfloor} + (nt - \lfloor nt \rfloor)(\Sigma_{\lfloor nt \rfloor + 1} - \Sigma_{\lfloor nt \rfloor}) \,, \ t\ge 0 \,,
\end{equation*}
which is a random element of $C_{\Rs^m}[0, \infty)$. Moreover by ~\cite[Proposition 10.4, Chapter 3]{ethier2009markov} if we have convergence in distribution in the Skorohod space of $\{\Sigma_{\cdot}^n\}_{n\ge 1}$ to a continuous process, then we also have convergence in distribution in $C_{\Rs^m}[0, \infty)$ of $\{\Hat{\Sigma}_{\cdot}^n\}_{n\geq 1}$ to the same limit. %Still about notation, we can also have $\Sigma^{n}_t = \Sigma_{\lfloor n t \rfloor}$ as a set value function and coherently we set
% \begin{equation*}
% \Hat{\Sigma}^{n}_t := |\Sigma_{\lfloor n t \rfloor}| + (nt - \lfloor nt \rfloor)(|\Sigma_{\lfloor nt \rfloor + 1}| - |\Sigma_{\lfloor nt \rfloor}|) \,, \ t\ge 0 \, .
% \end{equation*}


\begin{remark}\label{rem:conver}
 In the proofs, we will often make use of the following facts:
 \begin{itemize}
     \item[a)] If a sequence of processes converges in probability with respect to the uniform norm in $C_{\Rs^m}[0, T]$ for all $T >0$, then it converges in probability in $C_{\Rs^m}[0, \infty)$ under the metric $\rho$ defined in \eqref{def:rho} (this is a well-known result which can be easily proved).
     \item[b)] If a sequence of processes is tight in $C_{\Rs^m}[0, T]$ for all $T>0$ with the topology of uniform convergence in the compacts, then the sequence is tight in $C_{\Rs^m}[0, \infty)$ (see, e.g., \cite[Theorem 4.10,  Chapter 2]{karatzas2012brownian}). 
     \item[c)] \cite[Theorem 7.3]{billingsley1999probability}: A sequence of processes $\{\Hat{\Sigma}_{\cdot}^n\}_{n\geq 1}$ is tight in $C_{\Rs^m}[0, T]$  if and only if the following two conditions are satisfied:
     \begin{enumerate}[1.]
         \item For each positive $\eta$ there exist  $a$ and $n_0$ such that 
         \[ \PP\big[|\Hat{\Sigma}_{0}^n|\geq a\big]\leq \eta\,, \quad \text{ for all $n\geq n_0$}\,.
         \]
         \item For each positive $\varepsilon$ and $\eta$ there exist $\delta\in (0,1)$ and $n_0$ such that
         \[
         \PP\Big[\sup_{|s-t|\leq\delta}|\Hat{\Sigma}_{s}^n-\Hat{\Sigma}_{t}^n|\geq \varepsilon\Big]\leq \eta\,,  \quad \text{ for all $n\geq n_0$}\,.
         \]
     \end{enumerate}
 \end{itemize}
\end{remark}

%However for the computations we will do in the next sections, its clear that by Markov's inequality the second sum portion of $B_{t}^n$ converges to zero in probability. Hence, for our purposes, we only need to analyze the asymptotic behave of the first sum portion of $B_{t}^n$. 

\subsection{Case $\beta>1/2$ (easy one); proof of Proposition~\ref{pn-WGERW-Gauss}}\label{prova-pn-WGERW}

\hfill \\
%\subsection{Proof of the convergence in distribution of the $p_n$-\Name*  with $\beta>1/2$}\label{prova-pn-WGERW}


%The $p_n$-\Name* can be written as in~\eqref{xn-incremnto1}. 
Recall that for the $p_n$-\Name*, the variables $\{U_i\}_{i \geq 1}$ are independent of  both sequences $\{\gamma_i\}_{ i\geq 1}$ and $\{\xi_i\}_{i \geq 1}$. 

%\cm{In this section we will consider a weaker condition. The sequence $\{U_i\}_{i \geq 1}$ will be uncorrelated, rather than independent, with both sequences $\{\gamma_i\}_{ i\geq 1}$ and $\{\xi_i\}_{i \geq 1}$.}  %\cm{Moreover we remember that the $p_n$-W\Name{} will be the process which satisfies the Conditions~\ref{condiçao I*},~\ref{condição2} and~\ref{condição3}.}

%Besides that we will also impose in this section a weaker condition that will replace Condition~\ref{condição1} and define the $p_n$-W\Name. Before, let us
%denote  $C=((c_{i,j}))$ as a continuous, $d \times d$ matrix-valued function, defined in $[0, \infty)$, satisfying $C(0) = 0$ and 
%\begin{equation*}
%\sum_{i,j = 1}^d (c_{i,j}(t) - c_{i,j}(s))\alpha_i \alpha_j \geq 0 \quad \text{for any } \alpha \in \mathbb{R}^d, \quad t > s \geq 0\;.  
%\end{equation*}
%By Theorem 7.1.1 from~\cite{ethier2009markov} there exists an unique process $Z$, in distribution, with sample paths in $C_{\mathbb{R}^d}[0, \infty)$ such that $Z_i$ and $Z_i Z_j - c_{i,j}$, for $i, j \in \{1,2, \dots, d\}$, are local martingales with respect to $\sigma$-algebra generated by historical of the process $Z$. Furthermore the process $Z$ has independent Gaussian increments.

%Now we can enunciate the new condition

%%%%%%%%%%%%
%\setcounter{condition}{0}
%\renewcommand{\thecondition}{\Roman{condition}*}


%%%%%%%%%%%%%%

%\begin{condition}\label{condiçao I*}
%\begin{itemize}
 %   \item [i)] For all $k \geq 1$ and $\theta < \beta - 1/2$, we have
  %  \[\sup_{k \geq 1} \frac{\EE[\| \gamma_k \|]}{k^{\theta}} < \infty \;. \]
   % \item [ii)] When the process behaves like a $d$-martingale with zero mean, it holds the following,
  %  \begin{equation}\label{condGaussiano}
   % \frac{1}{n}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i \xi_i^T \to C(t) \quad \text{as } n \to \infty \;,
  %  \end{equation}
  %  in probability and
  %  \begin{equation*}
   % \lim_{k \to \infty} k^{-1/2} \EE\left[ \sup_{1 \leq i \leq k} \| \xi_i \| \right]  = 0\;. 
  %  \end{equation*}
    
    %\item [ii)] For a large $n$ we have,
    %\[ \sum_{k = 1}^{\infty} \PP[\| X_k - X_{k-1} \| > k^{\frac{\theta}{2}}] < \infty \;, \]
    %where $\theta$ is a positive constant such that $\theta/2 < \beta - 1/2$.
%\end{itemize}
%\end{condition}

%Let $\{X_n\}_{n \ge 0}$ be written as~\eqref{xn-incremnto1} where the sequence $\{U_i\}_{i \geq 1}$ is uncorrelated with both sequences $\{\gamma_i\}_{ i\geq 1}$ and $\{\xi_i\}_{i \geq 1}$. Additionally, if it satisfies Condition~\ref{condiçao I*}, \ref{condição2} and~\ref{condição3} \marginpar{\tiny\cm{acho que nem precisamos de III}} the process $X$ will be called a $p_n$-W\Name{} in direction $\ell$. 

% We rewrite a version of the Theorem 7.1.4 from~\cite{ethier2009markov} where the authors state a convergence in distribution of a $d$-martingale to a process with independents Gaussian increments. We will denote $\xrightarrow{\mathcal{D}}$ as convergence in distribution.

% \begin{theorem}[see~\cite{ethier2009markov}, Theorem 7.1.4]\label{kurtz}
% Let $(\phi_k, k \geq 1)$ be a sequence of $\mathbb{R}^d$-valued random vectors such that $\EE[\phi_k|\FF_{k-1}]=0$ where $\FF_k = \sigma(\phi_l, l \leq k)$. Define,
% \begin{align*}
% M_{\lfloor nt \rfloor} = \sum_{i=1}^{\lfloor nt \rfloor} \phi_i \quad \text{ and} \quad
% A_{\lfloor nt \rfloor} = \frac{1}{n} \sum_{i=1}^{\lfloor nt \rfloor} \phi_i \phi_i^T \;.
% \end{align*}
% Assume that the following conditions hold:
% \begin{itemize}
%     \item [i)]\begin{equation*}
%     \lim_{n \to \infty} n^{-1/2} \EE\left[ \sup_{1 \leq k \leq n} |M_{k-1} - M_k| \right]  = 0\;. \end{equation*}
%     \item[ii)] For each $t \geq 0$,
%     \begin{equation*}
%         A_{\lfloor nt \rfloor} \to C(t) \quad \text{as } n \to \infty\;,
%     \end{equation*}
%     in probability.
% \end{itemize}
% Then $M_{\lfloor n \cdot \rfloor}/n^{1/2} \xrightarrow{\mathcal{D}} Z_{\cdot}$ , where $Z$ is a process with independent Gaussian increments.
% \end{theorem}

We start stating a result that will be used in the proof of Proposition~\ref{pn-WGERW-Gauss}. %\cm{Due to this following Lemma it will be possible to see that the drift push through time will not make a difference for the process.}

\begin{lemma}\label{lem: tgD}
Let $X$ be a $p_n$-\Name* 
in direction $\ell\in \mathbb{S}^{d-1}$, on $\ZZ^d$ with $d\geq 2$, $p_n= \mathcal{C}n^{-\beta} \wedge 1$ with $\beta > 1/2$. Define  
% Considering the representation~\eqref{xn-incremento2} set 
\begin{equation*}
D_{\lfloor nt \rfloor} := \frac{1}{n^{1/2}} \sum_{i=1}^{\lfloor nt \rfloor} \um_{E_{i-1}^c \cap \{ U_i \leq  \mathcal{C} i^{-\beta}\}} (\gamma_i - \xi_i), \ t\ge 0 \,.
\end{equation*}
Then $\{\Hat{D}^n_\cdot\}_{n\ge 1}$ as a sequence of random elements of $C_{\Rs^d}[0, \infty)$,  converges in probability to the zero function.
\end{lemma}

The proof of Lemma~\ref{lem: tgD} will be postponed at the end of this section.
% \comu{Acho que este parágrafo pode ser removido. A prova é curta e bem claro o que será feito} The main idea behind the proof of Theorem~\ref{pn-WGERW-Gauss} is to use the decomposition~\eqref{xn-incremento2} and to analyze separately the sum portions suitably rescaled. We will see that the sum portion corresponding to the part of  $d$-martingale will converge in distribution and the other will converge to zero in probability. Thus, using Slutsky's Theorem we obtain the desired result. 

\begin{proof}[Proof of Proposition~\ref{pn-WGERW-Gauss}]
Using~\eqref{xn-incremento2} we write 
\begin{equation}\label{p_n-WGERW_incrementos}
    B_t^n = \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{1}{n^{1/2}} \sum_{i=1}^{\lfloor nt \rfloor} \um_{E_{i-1}^c \cap \{ U_i \leq {\mathcal{C}} i^{-\beta}\}} (\gamma_i - \xi_i) \,.
\end{equation}
%Now we will analyze separately the two portion sums of~\eqref{p_n-WGERW_incrementos}.
By Lemma~\ref{lem: tgD} the linear interpolation of the second term in~\eqref{p_n-WGERW_incrementos}  
%\begin{equation}\label{gamma_i-xi_i_wgerw}\frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} 1_{\{E_{i-1}^c\}}1_{\{ U_i \leq i^{-\beta}\}} \gamma_i - 1_{\{E_{i-1}^c\}}1_{\{ U_i \leq i^{-\beta}\}} \xi_i  \to  0 \quad \text{as } n \to \infty\,, %\PP-\text{probability}\;.\end{equation}
converges in probability to the zero function (as a sequence of random elements of $C_{\Rs^d}[0, \infty)$). For the first term in~\eqref{p_n-WGERW_incrementos} we  use~\cite[Theorem 7.1.4, 7.1.1]{ethier2009markov} to obtain that
\begin{equation}\label{xi_i->Z}
    \Big\{ \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i \Big\}_{t\ge 0} \xrightarrow[n \to \infty]{\mathcal{D}} \{ Z_{t} \}_{t\ge 0} \,,
\end{equation}
where $Z_{\cdot}$ denotes a Gaussian process with independent increments. Using Slutsky's Theorem (see~\cite[Theorem 11.4]{gut2005probability}) we finish the proof. 
\end{proof}

% \cm{Acho queue este remark poderia ficar depois da prova do lema.}
% \begin{remark}\label{rem_cond_frac}
% As we already point out in Remark~\ref{rem_cond_antes}  the Condition~\ref{condiçao I*} and the sequence $\{p_n\}_{n \ge 1}$ can be more general. Specifically, if we had $\sum_{i=1}^{\lfloor nt \rfloor} p_i \EE[||\gamma_i||] = o(\sqrt{n})$, then it will be possible to prove that the second sum portion in~\eqref{p_n-WGERW_incrementos} goes to zero in probability. Hence we finish the proof with Slutsky's Theorem (Theorem 11.4 from~\cite{gut2005probability}).
% \end{remark}

The proof of Corollary~\ref{pnESRW->BM}  (stating that the rescaled $p_n$-\Nametwo{} converges in distribution to a standard Brownian Motion) follows the proof of Proposition~\ref{pn-WGERW-Gauss} line by line.  The main difference is  in~\eqref{xi_i->Z} where instead of using ~\cite[Theorem 7.1.4]{ethier2009markov}, we can apply Donsker's Theorem, since the $\{\xi_i\}_{i\geq 1}$ corresponding to  $p_n$-\Nametwo{} are i.i.d. with zero-mean vector and finite covariance matrix (see, e.g., \cite[Theorem 8.2]{billingsley1999probability} or~\cite[Theorem 5.1.2]{ethier2009markov}).


\medskip
\begin{proof}[Proof of Lemma~\ref{lem: tgD}.] Without loss of generality, we shall assume $\CC = 1$. In light of Remark~\ref{rem:conver}-$a)$ it suffices to show convergence in $C_{\Rs^d}[0, T]$ for all $T>0$.   Let us then define 
\begin{equation*}
D_{\lfloor n t \rfloor}^{\gamma}:=\frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor n t \rfloor} \um_{E_{i-1}^c}\um_{\{ U_i \leq i^{-\beta}\}} \gamma_i \,, \ t \ge 0,
%\ \text{and} \
% D_{\lfloor n\cdot \rfloor}^{\xi} := \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} 1_{\{E_{i-1}^c\}}1_{\{ U_i \leq i^{-\beta}\}} \xi_i \,. 
\end{equation*}
and analogously $D_{\lfloor n\cdot \rfloor}^{\xi}$ replacing $\gamma_i$ by $\xi_i$ in each term of the sum.
We begin showing  that $\{\Hat{D}_\cdot ^{n,\gamma}\}_{n\ge 1}$ converges in probability to the  zero function in $C_{\Rs^d}[0, T]$ for all $T > 0$ (recall that $\Hat{D}_t ^{n,\gamma}$ is the linearly interpolated version of   $D_{\lfloor nt \rfloor}^{\gamma}$). Note that
\begin{eqnarray}\label{eq: Dgamma}
& & \PP \Big( \sup_{0 \le t \le T} \big\| \Hat{D}^{n,\gamma}_{t} \big\| > \varepsilon  \Big)  \le \PP \Big( \sup_{0 \le t \le T} \sum_{i=1}^{\lfloor nt \rfloor+1}  \big\| \um_{E_{i-1}^c \cap \{ U_i \leq i^{-\beta}\}} \gamma_i \big\| > \varepsilon n^{\frac{1}{2}} \Big) \nonumber
\\
& & \le \PP \Big( \sum_{i=1}^{\lfloor nT \rfloor +1} \big\| \um_{E_{i-1}^c \cap \{ U_i \leq i^{-\beta}\}} \gamma_i \big\| > \varepsilon n^{\frac{1}{2}} \Big) \le  \PP \Big( \sum_{i=1}^{\lfloor nT \rfloor +1} \big\| \um_{ \{ U_i \leq i^{-\beta}\}} \gamma_i \big\| > \varepsilon n^{\frac{1}{2}} \Big) \nonumber
\\
& & \le \frac{1}{n^{1/2}\varepsilon} \sum_{i=1}^{\lfloor nT \rfloor +1} \frac{1}{i^{\beta}} \EE\left[\left\| \gamma_i \right\| \right]  \leq \frac{1}{n^{1/2}\varepsilon}  \sum_{i=1}^{\lfloor nT \rfloor +1} \frac{\EE\left[\left\| \gamma_i \right\| \right]}{i^{\theta}} \times   \frac{1}{i^{\beta-\theta}}  \,.    
\end{eqnarray}  
By Condition~\ref{condiçaoI*}, we know that  for all $i \geq 1$ and all $\theta < \beta - 1/2$ there exists a positive constant $L$ such that 
$\EE\left[\left\| \gamma_i \right\| \right] \le i^{\theta}   L$. 
Going back to~\eqref{eq: Dgamma}, we get
\begin{align*}%\label{eq: Dgamma3}
\begin{split}
\PP & \Big( \sup_{0 \le t \le T} \big\| \Hat{D}^{n,\gamma}_{t} \big\| > \varepsilon  \Big) %\le \frac{1}{n^{1/2}\varepsilon}  \sum_{i=1}^{\lfloor nT \rfloor} \frac{\EE\left[\left\| \gamma_i \right\| \right]}{i^{\theta}} \times   \frac{1}{i^{\beta-\theta}}  \\
\leq \frac{L}{n^{1/2}\varepsilon} \sum_{i=1}^{\lfloor nT \rfloor + 1} \frac{1}{i^{\beta-\theta}} 
\\ 
& \leq \frac{L}{n^{1/2}\varepsilon} \times c' (\lfloor nT \rfloor +1)^{1-\beta+\theta}
 \leq \frac{Lc'}{\varepsilon} \times \frac{\lfloor nT \rfloor + 1}{n} \times \frac{n^{1/2}}{(\lfloor nT \rfloor+1)^{\beta-\theta}}
\end{split}
\end{align*}
for some $c' > 0$, since $\sum_{i=1}^{k} \frac{1}{i^{\beta}} = O(k^{1-\beta})$. Given that $\beta >1/2+\theta$, we obtain that $\{\Hat{D}^{n,\gamma}_{\cdot}\}_{n\ge 1}$ converges in probability (with respect to the uniform metric) to the  zero function for all $T >0$.
%\begin{equation*}
%\frac{\sum_{i=1}^{\lfloor nt \rfloor} 1_{\{E_{i-1}^c\}}1_{\{ U_i \leq i^{-\beta}\}} \gamma_i}{n^{1/2}} \to  0 \quad \text{as } n \to \infty\,, %\PP-\text{probability}\;.    
%\end{equation*}

Using again Condition~\ref{condiçaoI*} and applying the same 
computations as above, we show that $\{\Hat{D}^{n,\xi}_{\cdot}\}_{n\ge 1}$ also converges  in probability (with respect to the uniform metric) to the  zero function for all $T >0$.
%\begin{equation*}\frac{\sum_{i=1}^{\lfloor nt \rfloor} 1_{\{E_{i-1}^c\}}1_{\{ U_i \leq i^{-\beta}\}} \xi_i}{n^{1/2}}  \to  0 \quad \text{as } n \to \infty\,, %\PP-\text{probability}\;.    \end{equation*}
%in probability in the space $C_{\Rs^d}[0, T]$ for all $T >0$.
Therefore the same convergence also holds for $\{\Hat{D}^{n}_{\cdot} = \Hat{D}^{n,\gamma}_{\cdot} - \Hat{D}^{n,\xi}_{\cdot}\}_{n\ge 1}$. 
%
% Since convergence in $C_{\Rs^d}[0, T]$ for all $T >0$ implies convergence in $C_{\Rs^d}[0, \infty)$ under the metric $\rho$, then $D_{\lfloor n\cdot \rfloor}$ converges in probability to zero as a sequence of random elements of $C_{\Rs^d}[0, \infty)$ 
% \com{...HERE!!}
% .
\end{proof}

%\subsection{Proof of the convergence in distribution of the $p_n$-\Nametwo{} with $\beta = 1/2$ and $d=2$.}\label{prova-pn-ERW_d=2}

\subsection{Case $\beta = 1/2$; proof of Theorem~\ref{thm:main-conv}}\label{sec:main-theorem}

\hfill \\

\begin{comment}
{\color{red} \texttt{NOT SURE WE NEED.....}
We begin introducing some auxiliary results.
% required to analyze the asymptotic behavior of the $p_n$-\Nametwo{} on $\ZZ^d$, with $\beta = 1/2$ and $d\geq 2$.
%
Let $\{\xi_i\}_{i \ge 1}$ be i.i.d. $\ZZ^d$-valued random variables with zero-mean vector and finite covariance matrix. Let $\{Y_n\}_{n \ge 0}$ be the random walk on $\ZZ^d$ with increments $\{\xi_i\}_{i \ge 1}$ starting at $Y_0 = 0$, thus $Y_n = \sum_{i=1}^{n} \xi_i$, $n\ge 1$.  For $m \leq n$ define 
\[ \Rr_{[m,n]} ^Y := \{Y_m, Y_{m+1}, \ldots, Y_n\}\;,\]
and denote by $\Rr_{n}^{Y} = \Rr^{Y}_{[0,n]}$,  the range of the random walk $\{Y_n\}_{n \geq 0}$.

We now state two known results about the range of a random walk on $\ZZ^d$ with i.i.d. increments which are instrumental in our proofs. Henceforth, we denote by $\pi_d$ the probability that $\{Y_n\}_{n \geq 0}$ never returns to the origin (as in the statement of Proposition~\ref{prop:RangeERW} and Proposition~\ref{prop:RangeERW_lower}).
\begin{theorem}[{\cite[pages 38-40]{spitzer2001principles}}\&{\cite[Theorem 1]{hamana2001large}}]\label{teo: RnZ>}
Let $\{Y_n\}_{n \geq 0}$ be an aperiodic random walk with i.i.d increments on $\ZZ^d$ for $d\geq 2$. It holds that 
\begin{itemize}
    \item  {\rm Law of Large Numbers:}
\begin{align*}
\tag{LLN}\frac{|\Rr_n^Y|}{n} \xrightarrow[n \to \infty]{} \pi_d \ \text{ a.s.}\,.
\end{align*}
\item {\rm Large Deviation:}  for every  $\theta' > \pi_d$ and $n$ sufficiently large
\begin{align*}
\tag{LD}\PP[|\Rr_{n}^Y| \geq \theta' n] \leq e^{-c_{\theta'}n}\,,
\end{align*}
where $c_{\theta'}$ is a positive constant that depends on $\theta'$.
\end{itemize}
 Note that for $d=2$, we have that $\pi_d=0$, whereas for $d\geq 3 $, $\pi_d\in (0,1]$. 
\end{theorem}
%
\begin{remark}
The meaning of aperiodicity used above is the one given in \cite[Chapter II, section 7]{spitzer2001principles} and it is different from the usual one (see, e.g., \cite[page 7]{levin2017markov}).  
Aperiodicity in Theorem~\ref{teo: RnZ>} refers to the following property: 
a  process on $\ZZ^d$ is aperiodic if the group generated by the support of the process is all of $\ZZ^d$.  However, as explained in~\cite[page 188-beginning of Section 2]{hamana2001large}, this condition does not entail a loss of generality. %However, as explained in~\cite[page 188-beginning of Section 2]{hamana2001large}, this condition can be relaxed to \com{include??} periodic random walks. \comu{incluir comentário sobre definição de aperiódico}
\end{remark}
}

%If we further define  the following random variable
%\begin{equation}\label{eq: def Jn}
 %   |J_n| := \sum_{i=1}^{|\Rr_n^X|} 1_{\{U_i \leq i^{-1/2} \}}\,,
%\end{equation}
%then we obtain
%\begin{equation}\label{Kn<Jn}
%|K_n| =  \sum_{j=1}^{|\Rr_n^X|} 1_{\{U_{\tau_j} \leq \tau_j^{-1/2} \}} \preceq \sum_{i=1}^{|\Rr_n^X|} 1_{\{U_i \leq i^{-1/2} \}} = |J_n| \,,   
%\end{equation}
%by the fact that the realizations of the sequence $\{U_i\}_{i \ge 1}$ is i.i.d. and the $\tau_j$'s are stopping times.
\end{comment}

In the following, we present an important auxiliary result that will be useful in the proof of Theorem~\ref{thm:main-conv}. We still state and proof the result for any $\beta \le 1/2$ and we are only going to fix $\beta = 1/2$ in the proof of Theorem~\ref{thm:main-conv}. 

%
\begin{lemma}\label{Kn-convergence}
Take $d\geq 2$ and $p_n=n^{-\beta}$, $n\ge 1$, for $\beta \le 1/2$. Let $|K_n|$  as in~\eqref{eq: def Kn} and consider the corresponding sequence of continuous processes $\{\Hat{K}^n_{\cdot}/n^{1-\beta}\}_{n\geq 1}$. Assume that Hypothesis~\ref{hyp} holds. Then,  $\{\Hat{K}^n_{\cdot}/n^{1-\beta}\}_{n\geq 1}$ converges in probability as random elements of $C_{\Rs}[0, \infty)$ to the deterministic function $$t \mapsto \frac{\pi_d \, t^{1-\beta}}{1-\beta}, \ \, t\ge 0\,.$$ 
\end{lemma}


% {\color{red} \begin{lemma}\label{Jntight}
% Let $|K_n|$ be defined as in~\eqref{eq: def Kn} and consider the corresponding sequence of continuous  processes $\{\Hat{K}^n_{\cdot}/n^{1/2}\}_{n\geq 1}$. It holds that
% \begin{enumerate}[i)]
%     \item For $d\geq 2$,  $\{\Hat{K}^n_{\cdot }/n^{1/2}\}_{n\ge 1}$ is tight  in  $C_{\Rs}[0, \infty)$;
%     \item For $d= 2$, $\{\Hat{K}^n_{\cdot}/n^{1/2}\}_{n\geq 1}$ converges in probability as random elements of $C_{\Rs}[0, \infty)$ to the zero function.
% \end{enumerate}
% \end{lemma}
% %
% }

The proof of Lemma~\ref{Kn-convergence} will be postponed to the end of this section.

\begin{proof}[Proof of Theorem~\ref{thm:main-conv}]
% The idea of the proof of Theorem~\ref{pn-ERW-d=2} is similar to the one used in Theorem~\ref{pn-WGERW-Gauss}, however to obtain a version of Lemma~\ref{lem: tgD} in the case $d=2$ and $\beta=1/2$  we will use Proposition~\ref{prop:RangeERW} (see, proof of Lemma~\ref{Jntight}). 

By~\eqref{xn-incremento2} and the definition of $|K_n|$, we can rewrite  the process $B_t^n$ as
\begin{align}\label{p_n-ERW_incrementos_d}
\begin{split}
B_t^n & 
%= \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{1}{n^{1/2}} \sum_{i=1}^{\lfloor nt \rfloor} \um_{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}} (\gamma_i - \xi_i)\\ & 
= \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{1}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}}  (\gamma_i - \xi_i) \,.
\end{split}
\end{align}
For the first term in \eqref{p_n-ERW_incrementos_d} we apply  \cite[Theorem 7.1.4]{ethier2009markov}
% Donsker's Theorem (see, e.g., \cite[Theorem 5.1.2 $(c)$]{ethier2009markov}) 
% (\textcolor{brown}{este teorema assume que as coornedadas das $\xi_i$ s\~ao n\~ao correlacionadas. No Stroock tambem pede que a matrix de covariancia seja a identiade)} 
and  obtain that %the first sum portion of~\eqref{p_n-ERW_incrementos_d=2} converges in distribution in $C_{\Rs^2}[0, \infty)$ to a Brownian Motion, i.e.,  
\begin{equation}\label{xi_i->W}
    \Big\{ \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i \Big\}_{t\ge 0} \xrightarrow[n \to \infty]{\mathcal{D}} \{ W_{t} \}_{t\ge 0} \,,
\end{equation}
where $W_{\cdot}$ is a Brownian Motion  with zero-mean vector and covariance matrix $\EE[\xi_1 \xi_1^T]$. 
%
Now let us prove that 
\begin{equation}\label{eq:prob}
     \frac{1}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}}  (\gamma_i - \xi_i)  \xrightarrow[n \to \infty]{} 2\pi_d \sqrt{t}\EE[\gamma_1] \, \text{ in probability}.
\end{equation}
First, observe that, for $s<t$
\begin{align*}
    \Big|\Big|\frac{1}{n^{1/2}} \sum_{i \in K_{\lfloor ns \rfloor}}  (\gamma_i - \xi_i) - \frac{1}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}}  (\gamma_i - \xi_i) \Big|\Big| &= \frac{1}{n^{1/2}}\Big|\Big|\sum_{i = |K_{\lfloor ns\rfloor}|+ 1}^{|K_{\lfloor nt\rfloor}|} (\gamma_{\varphi_i} - \xi_{\varphi_i})\Big|\Big|\\
    &\le \frac{2K}{n^{1/2}} ||K_{\lfloor nt\rfloor} - K_{\lfloor ns\rfloor}||\,.
\end{align*}
Then by Lemma \ref{Kn-convergence} and Remark~\ref{rem:conver} (item $c$), we have that $\big\{\frac{1}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}}  (\gamma_i - \xi_i): t\ge 0 \big\}$ is tight. Note that
\begin{equation*}
    \frac{1}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}}  (\gamma_i - \xi_i) = 
\frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i)}{|K_{\lfloor nt \rfloor}|}\, .
\end{equation*}
Under the assumption that Hypothesis~\ref{hyp} holds, Lemma~\ref{Kn-convergence} implies that  $|K_{\lfloor nt \rfloor}| \to \infty$ as $n \to \infty$ almost surely. Moreover, the sequence of random vectors $\{\gamma_{\varphi_i} -\xi_{\varphi_i}\}_{i \geq 1}$ is i.i.d. and has the same distribution of $\{\gamma_{i} -\xi_{i}\}_{i \geq 1}$, which is i.i.d. too (see Lemma~\ref{lem: iid}). Therefore,
\begin{equation}\label{eq:prob0}
   \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i)}{|K_{\lfloor nt \rfloor}|} = \sum_{i=1}^{|K_{\lfloor nt \rfloor}|} \frac{ (\gamma_{\varphi_i} - \xi_{\varphi_i})}{|K_{\lfloor nt \rfloor}|} \xrightarrow[n \to \infty]{} \EE[\gamma_1] \quad \text{ a.s.}\,.
\end{equation}
Then by Lemma~\ref{Kn-convergence} and Slutsky's Theorem (see, e.g., \cite[Theorem 11.4]{gut2005probability}) we obtain that
\begin{align*}
    &\Big(\frac{1}{n^{1/2}} \sum_{i \in K_{\lfloor nt_1 \rfloor}}  (\gamma_i - \xi_i),\dots,\frac{1}{n^{1/2}} \sum_{i \in K_{\lfloor nt_m \rfloor}}  (\gamma_i - \xi_i)\Big)
    \\
    & \qquad \qquad \qquad \qquad \xrightarrow[n \to \infty]{} \big(2\pi_d\sqrt{t_1}\EE[\gamma_1],\dots,2\pi_d\sqrt{t_1}\EE[\gamma_1]\big)\,,
\end{align*}
in probability, for any $0\le t_1,\le \dots\le t_m $. Hence we have that the process
$\big\{\frac{1}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}}  (\gamma_i - \xi_i): t\ge 0 \big\}$
converges in distribution to the constant process $\{2\pi_d\sqrt{t}\mathbb{E}[\gamma_1]: t\ge 0\}$, which is equivalent to the convergence in probability. Then by \eqref{xi_i->W} and \eqref{eq:prob} we obtain Theorem~\ref{thm:main-conv}.
\end{proof}



\begin{remark}\label{main-ub1}
From \eqref{p_n-ERW_incrementos_d} and \eqref{eq:prob0} and observing that $\EE[\gamma_1\cdot \ell] \ge \lambda > 0$, Proposition \ref{prop:d2-23} follows if, instead of the complete convergence result of Lemma~\ref{Kn-convergence}, we have that $\{\Hat{K}^n_{\cdot}/n^{1/2}\}_{n\geq 1}$ is tight and for any limit point
$\mathcal{H}_{\cdot}$
\begin{align}\label{main-ubeq1}
& 
\PP \left[\forall t \in [0,\infty):  \mathcal{H}_t \le 2\sqrt{t} \pi_d
\right] = 1\,.
\end{align}
This can be checked directly from the proof of Lemma~\ref{Kn-convergence} using only the upper bound in Proposition \ref{prop:RangeERW}, see Remark \ref{main-ub2}.
\end{remark}


%\com{I think the proof below may be stated and adjusted  to hold for $\beta \in (0,1)$, i.e., $\frac{|K_{\lfloor n t\rfloor}|}{n^{1-\beta}} \xrightarrow[n \to \infty]{} \frac{\pi_d}{1-\beta}  t^{1-\beta}$ and adjust the proof of tightness for the process $\left\{\Hat{K}^n_{\cdot}/n^{1-\beta}\right\}_{n\geq 1}$. This could give us that for $\beta <1/2$, under Hypothesis~\ref{hyp}, we have that $B_t^n$ converges in distribution to the function $t\mapsto \frac{\pi_d}{1-\beta}  t^{1-\beta}$ ...} 

\begin{proof}[Proof of Lemma~\ref{Kn-convergence}] 
By \eqref{eq:convK_n} in Proposition~\ref{conv-Kbeta}, for all $t> 0$ we have that 
\begin{align*}
\frac{|K_{\lfloor n t\rfloor}|}{n^{1-\beta}} & \xrightarrow[n \to \infty]{} \frac{\pi_d \, t^{1-\beta}}{1-\beta} \,, 
\end{align*}
in probability, i.e., the one-dimensional distributions  converge in probability to a constant. Therefore the joint distributions corresponding to times $t_1, \ldots, t_m$ also converge in probability and we obtain the convergence in the sense of the finite-dimensional distributions for both processes.
% By Corollary 3.1 we already have the convergence of the
% finite-dimensional distributions. 
% we have the convergence of the one-dimensional distributions in probability to a constant. Since each one-dimensional random variable converges in probability to a constant, the joint convergence in probability follows naturally, which in turn implies convergence in distribution.

By~\cite[Theorem 7.1]{billingsley1999probability},  to prove the claim it only remains to prove that the sequence of processes $\{\Hat{K}^n_{\cdot}/n^{1-\beta}\}_{n\geq 1}$ is tight in $C_{\Rs^d}[0, \infty)$. 
By  Remark~\ref{rem:conver}-$b)$  it suffices to show tightness in $C_{\Rs^d}[0, T]$ for all $T > 0$ and this is equivalent to show that the sequence of processes $\left\{\Hat{K}^n_{\cdot}/n^{1-\beta}\right\}_{n\geq 1}$  satisfies  the two conditions in  Remark~\ref{rem:conver}-$c)$.
%
Since $\Hat{K}^n_{0}/n^{1-\beta}\equiv 0$ for all $n \ge 1$ and therefore the first condition in Remark~\ref{rem:conver}-$c)$ is satisfied.
%
To prove that $\{\Hat{K}^n_\cdot /n^{1-\beta}\}_{n\ge 1}$ satisfies the second condition in Remark~\ref{rem:conver}-$c)$  we  use~\cite[Corollary on page 83]{billingsley1999probability} which states that the second condition of ~\cite[Theorem 7.3]{billingsley1999probability} holds if, for every positive $\varepsilon$ and $\eta$, there exists a $\phi \in (0,1)$, and an integer $n_0$ such that
\begin{equation}\label{eq:tight}
\frac{1}{\phi} \, \PP \Big[ \sup_{t \le s \le t + \phi} \big\|\Hat{K}^n_{s} - \Hat{K}^n_{t} \big\|  \ge \varepsilon n^{1-\beta} \Big]  \le \eta \quad \forall n \ge n_0\,.
\end{equation}
Note that 
\begin{align*}
\PP &\Big[ \sup_{t \le s \le t + \phi} \big\|\Hat{K}^n_{s} - \Hat{K}^n_{t} \big\|  \ge \varepsilon n^{1-\beta} \Big]  \le \PP \Big[ \sum_{i = \lfloor nt \rfloor }^{\lfloor n(t + \phi) \rfloor +1} \um_{\big\{U_i \le i^{-\beta} \big\}} \ge \varepsilon n^{1-\beta} \Big]
\\
&
\le \exp\Big(-\varepsilon n^{1-\beta}\Big)\EE\Big[\exp\Big( \sum_{i = \lfloor nt \rfloor}^{\lfloor n(t + \phi)\rfloor+1} \um_{ \{U_i \le i^{-\beta}\}} \Big) \Big]\,, 
%& & \le \exp\Big(\frac{-\varepsilon n^{\frac{1}{2}}}{2K}\Big) \prod_{i = \lfloor nt \rfloor + 1}^{\lfloor n(t + \phi)\rfloor} \EE\left[\exp\left(  \um_{ \{U_i \le i^{-\frac{1}{2}}\}} \right) \right] \,,    
\end{align*}
where in the last inequality we have used exponential Markov's inequality.
%
Continuing the computation  we obtain that
\begin{eqnarray}\label{eq: PnA}
\lefteqn{\frac{1}{\phi} \PP \Big[ \sup_{t \le s \le t + \phi} \big\|\Hat{K}^n_{s} - \Hat{K}^n_{t} \big\|  \ge \varepsilon \Big] \le \frac{1}{\phi} e^{-\varepsilon n^{1-\beta}} \prod_{i = \lfloor nt \rfloor}^{\lfloor n(t + \phi)\rfloor + 1} \EE\left[\exp\left(  \um_{ \{U_i \le i^{-\beta} \}} \right) \right]} \nonumber \\
& & = \frac{1}{\phi} e^{-\varepsilon n^{1-\beta}} \prod_{i = \lfloor nt \rfloor}^{\lfloor n(t + \phi)\rfloor + 1} \left(1+ \frac{e-1}{i^{\beta}} \right) \le \frac{1}{\phi} e^{-\varepsilon n^{1-\beta}} \prod_{i = \lfloor nt \rfloor}^{\lfloor n(t + \phi)\rfloor + 1} \exp\left(\frac{e-1}{i^{\beta}} \right) 
\nonumber \\
& & \le \frac{1}{\phi} \exp(-\varepsilon n^{1-\beta})\exp\left(\frac{(e-1) \big((n(t + \phi))^{(1-\beta)} - (nt)^{1-\beta} + 2\big)}{1-\beta} \right) \,,  \nonumber
\end{eqnarray}
where the last inequality above follows from noticing that 
\begin{align*}
\sum_{i = \lfloor nt \rfloor}^{\lfloor n(t + \phi)\rfloor + 1}\frac{1}{i^{\beta}} & \le \int_{nt - 1}^{n(t+\phi) + 1} x^{-\beta} dx \le \frac{(n(t + \phi))^{(1-\beta)} - (nt)^{1-\beta} + 2}{1-\beta} \,.
\end{align*}
Therefore, in order to show that \eqref{eq:tight}  holds, it remains to show that for every positive $\varepsilon$ and $\eta$,  there exists a $\phi \in (0,1)$, and an integer $n_0$ such that
\begin{equation}\label{exp<et}
\frac{1}{\phi} \exp(-\varepsilon n^{1-\beta})\exp\left(\frac{(e-1) \big(n^{1-\beta} ((t + \phi)^{(1-\beta)} - t^{1-\beta}) + 2\big)}{1-\beta} \right) \le \eta
\end{equation}
for all $n \ge n_0$.
We accomplish this by choosing $\phi \in (0,1)$ sufficiently small such that $((t + \phi)^{(1-\beta)} - t^{1-\beta}) < \epsilon(1-\beta)/2(e-1)$, for all $t \in [0,T]$. Then, for every $\eta>0$, choosing $n$ sufficiently large, we obtain that~\eqref{exp<et} is satisfied. 
% One can see that, since we have $\phi \in (0,1)$,  for all $\hat{\varepsilon} > 0$, there exists a $\phi' > \phi$ such that $|\sqrt{t+\phi'} - \sqrt{t}|< \hat{\varepsilon}$. Now we can choose $\hat{\varepsilon} = c/4(e-1)$ and we obtain, for a large enough $n$, that~\eqref{exp<eta} is fulfilled for all $\eta$.
Consequently, we have that~\eqref{eq:tight} is satisfied and thus $\{\Hat{K}^n_{\cdot}\}_{n \ge 1}$ is tight in $C_{\Rs^d}[0,T]$. 
\end{proof}

\begin{remark}\label{main-ub2}
Recall Remark \ref{main-ub1}. Using only the upper bound in Proposition \ref{prop:RangeERW}, we have that $\{\Hat{K}^n_{\cdot}/n^{1/2}\}_{n\geq 1}$ is tight exactly as in the proof of Lemma~\ref{Kn-convergence} above. To check that for every limit point $\mathcal{H}_{\cdot}$ of $\{\Hat{K}^n_{\cdot}/n^{1/2}\}_{n\geq 1}$
\eqref{main-ubeq1} holds, it is enough to use \eqref{eq:UppK_n} instead of \eqref{eq:convK_n} in Proposition~\ref{conv-Kbeta}. 
\end{remark}

\begin{remark}
Consider $p_n=n^{-\beta}$, $n\ge 1$, for $\beta <1/2$ and suppose that Hypothesis~\ref{hyp} holds. Using Lemma~\ref{Kn-convergence} and proceeding as in the proof of Theorem~\ref{thm:main-conv}, we can show that $\{B_t^n\}_{t\ge 0}$ converges in distribution to the deterministic function $t\mapsto \frac{\pi_d}{1-\beta}  t^{1-\beta}$. This reinforces the result of Theorem~\ref{thm:conv-Xbeta}. We leave the details to the reader.
\end{remark}

%%%%%%%%% OLD STUFF %%%%%%%%
\begin{comment}
\subsubsection{Case $\beta = 1/2$ and $d=2$; proof of Theorem~\ref{pn-ERW-d=2}} \label{prova-pn-ERW_d=2}

\begin{proof}[Proof of Theorem~\ref{pn-ERW-d=2}] 
% The idea of the proof of Theorem~\ref{pn-ERW-d=2} is similar to the one used in Theorem~\ref{pn-WGERW-Gauss}, however to obtain a version of Lemma~\ref{lem: tgD} in the case $d=2$ and $\beta=1/2$  we will use Proposition~\ref{prop:RangeERW} (see, proof of Lemma~\ref{Jntight}). 

By~\eqref{xn-incremento2} and the definition of $K_n$, we can rewrite the the process $B_t^n$ as
\begin{align}\label{p_n-ERW_incrementos_d=2}
\begin{split}
B_t^n & 
%= \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{1}{n^{1/2}} \sum_{i=1}^{\lfloor nt \rfloor} \um_{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}} (\gamma_i - \xi_i)\\ & 
= \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{1}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}}  (\gamma_i - \xi_i) 
\\
& = 
\frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i)}{|K_{\lfloor nt \rfloor}|}\, , \  t\ge 0\,.
\end{split}
\end{align}

For the first term in \eqref{p_n-ERW_incrementos_d=2} apply Donsker's Theorem (see, e.g., \cite[Theorem 5.1.2 $(c)$]{ethier2009markov}) we obtain that %the first sum portion of~\eqref{p_n-ERW_incrementos_d=2} converges in distribution in $C_{\Rs^2}[0, \infty)$ to a Brownian Motion, i.e.,  
\begin{equation}\label{xi_i->W2}
    \Big\{ \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i \Big\}_{t\ge 0} \xrightarrow[n \to \infty]{\mathcal{D}} \{ W_{t} \}_{t\ge 0} \,,
\end{equation}
where $W_{\cdot}$ is a Brownian Motion in dimension 2 with zero-mean vector and covariance matrix $\EE[\xi_1 \xi_1^T]$. 
%
%Let us remember that $|K_{\lfloor n \cdot \rfloor}|/n^{1/2} \preceq |J_{\lfloor n \cdot \rfloor}|/n^{1/2}$ (see~\eqref{Kn<Jn}). 
%By Lemma~\ref{Jntight} $\{\Hat{K}^n_{\cdot}/n^{1/2}\}_{n\geq 1}$ converges in probability as random elements of $C_{\Rs}[0, \infty)$ to the identically zero function.
%\begin{equation}\label{eq: Kntprob}
%\left\{\frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}}\right\}_{t\geq 0} \xrightarrow[n \to \infty]{} 0 \,,    
%\end{equation}
%in probability \com{we could use the notation of convergence in probability} as random elements of 
%
%\cm{Before we analyze the second sum portion in~\eqref{p_n-ERW_incrementos_d=2} let us give a little script how the proof will continuing. First we will show that the random variable $|J_n|/\sqrt{n}$ converges in probability to 0. Then with this computation it will be possible to see that the finite dimensional of the process $|J_{\lfloor nt \rfloor}|/\sqrt{n}$ converges in probability to 0 for any $t \ge 0$.}
%\cm{Finally since the process $|J_{\lfloor nt \rfloor}|/\sqrt{n}$ is tight in the space $C[0, \infty)$ by Lemma~\ref{Jntight}, we will see that converges in distribution to zero. Hence with the stochastic domination in~\eqref{Kn<Jn} and~\eqref{xi_i->W2} we will be able to obtain the desired result.}
%Let us denote the following event, for any $\varepsilon > 0$, set $G = \{|J_{n}| > \varepsilon \sqrt{n}\}$.
%Now, from Markov's inequality, for a $\delta>0$, one can see that 
%\begin{align}\label{eq:Bn}
%\end{align}
%From~\eqref{eq:Bn} we obtain 
%\begin{align}\label{eq:Bn2}
%\end{align}
%The last equality in~\eqref{eq:Bn2}, we obtain from Proposition~\ref{Rn<dn} and the fact that $\sum_{i=1}^{\lceil \delta n\rceil} \frac{1}{i^{1/2}} = \Theta(\lceil \delta n\rceil^{1/2})$. Since we can choose $\delta$ as small as we want. One can conclude 
%\begin{equation}\label{eq:Bn3}
 %   \lim_{n \to \infty} \PP[|J_n| > \varepsilon \sqrt{n}] = 0 \;.
%\end{equation}
%\cm{Now if we set $\{|J_{\lfloor nt \rfloor}| > \varepsilon \sqrt{n}\}$ for any $t \ge 0$, we have, by same computations we did above to obtain~\eqref{eq:Bn3}}
%\cm{
%\begin{equation}\label{eq:Bnt3}
%    \lim_{n \to \infty} \PP[|J_{\lfloor nt \rfloor}| > \varepsilon \sqrt{n}] = 0 \;.
%\end{equation}}
%\cm{Then by~\eqref{eq:Bnt3} we can conclude that the the finite dimensional of the process $|J_{\lfloor nt \rfloor}|/\sqrt{n}$ converges in probability to zero.} %and the process $|J_{\lfloor n \cdot \rfloor}|/\sqrt{n}$ is tight for $t=0$ in $C[0, T]$ for all $T > 0$.}
%\cm{Now, following our little script, we will show that the process $|J_{\lfloor nt \rfloor}|/\sqrt{n}$ converges in probability to 0 in the space $C[0, T]$. Notes that for any $\delta'>0$ and $t >0$, we have
%\begin{equation*}
%\PP \left[\sup_{|s-t| \le \phi} \left| \frac{|J_{\lfloor ns \rfloor}| - |J_{\lfloor nt \rfloor}|}{n^{1/2}} \right| \ge \varepsilon  \right] = \PP \left[ ||J_{\lfloor n(t+ \phi) \rfloor}| - |J_{\lfloor nt \rfloor}|| \ge \varepsilon n^{1/2}  \right]\,,    
%\end{equation*}
%by the definition of $|J_{\lfloor nt \rfloor}|$.}
%\cm{Then by the same techniques to obtain~\eqref{eq:Bn3} we have
%\begin{equation}\label{eq: Bntight}
%\lim_{\delta' \to 0} \limsup_{n \to \infty} \frac{1}{\delta'} \PP \left[\sup_{t \le s \le t + \delta'} \left| \frac{|J_{\lfloor ns \rfloor}| - |J_{\lfloor nt \rfloor}|}{n^{1/2}} \right| \ge \varepsilon  \right] = 0 \,.    
%\end{equation}}
%\cm{Since we have that the the finite dimensional of the process $|J_{\lfloor n\cdot \rfloor}|/\sqrt{n}$ converges in probability for 0 and by Lemma~\ref{Jntight} this process is tight in $C[0, \infty)$, we obtain by Theorem 2.4.15 in~\cite{karatzas2012brownian} the following
%\begin{equation}\label{eq: Bntprob}
%\frac{|J_{\lfloor nt \rfloor}|}{n^{1/2}} \to 0 \quad \text{as } n \to \infty \,,    
%\end{equation}
%in distribution in $C[0, \infty)$ and consequently in probability.}
%Hence from the stochastic domination  and~\eqref{eq: Bntprob} we obtain 
%\begin{equation}\label{eq:prob01}
%    \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \to 0 \quad \text{as } n \to \infty \,,
%\end{equation}
%in probability in $C[0, \infty)$. 
%

Now we consider the second term in \eqref{p_n-ERW_incrementos_d=2}. Clearly with probability one either $\lim_{n \to \infty} |K_{\lfloor nt \rfloor}|< \infty$ or $\lim_{n \to \infty} |K_{\lfloor nt \rfloor}|=+ \infty$.
%
% We shall now consider two cases: $\lim_{n \to \infty} |K_{\lfloor nt \rfloor}|< \infty$ almost surely and $\lim_{n \to \infty} |K_{\lfloor nt \rfloor}|=+ \infty$ almost surely. \comu{São só esses os casos possíveis? Acho que temos que fixar a realização.}
%
If $\lim_{n \to \infty} |K_{\lfloor nt \rfloor}|< \infty$,  then 
% there exists a positive constant $L$ \com{this $L$ should be 1?} such that 
\begin{equation}\label{eq:prob02finito}
\lim_{n \to \infty} \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ \Vert \gamma_i - \xi_i \Vert}{|K_{\lfloor nt \rfloor}|} \leq 2K\,,
\end{equation}
where $K$ is from Condition~\ref{condition_A}.
%
%Hence from~\eqref{eq: Kntprob} and~\eqref{eq:prob02finito} we have \begin{equation}\label{eq:prob03} \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i)}{|K_{\lfloor nt \rfloor}|} \to 0 \quad \text{as } n \to \infty \,,\end{equation}in the space $C_{\Rs^2}[0, \infty)$ in probability. Then from~\eqref{xi_i->W2},~\eqref{eq:prob03} and Slutsky's Theorem (see~\cite[Theorem 11.4]{gut2005probability}) we have our result.
%
%\cm{Now we will show that $|K_{\lfloor nt \rfloor}| \to \infty$, almost surely as $n \to \infty$. Let us set a $\delta' \in (0,1)$. Hence we have}
%\cm{\begin{align*}
%|K_n| & = |K_n|1_{\{|\Rr_n^X| \le \delta' n \}} + |K_n|1_{\{|\Rr_n^X| > \delta' n \}}
%\\
%& \ge |K_n|1_{\{|\Rr_n^X| > \delta' n \}} \ge \sum_{i= \delta' n +1}^n 1_{\{U_{j_i} \le i^{-1/2} \}} \,.
%\end{align*}}
%
%\cm{We set the random variable $Y_i = 1_{\{U_i \le (\delta'n +i)^{-1/2} \}}$, thus we have $\sum_{i=1}^{n} Y_i = \sum_{i= \delta' n +1}^n 1_{\{U_i \ge i^{-1/2} \}}$. Let $\varepsilon' > 0$ and we obtain
%\begin{align*}
%\begin{split}
%\sum_{i=1}^n \PP[|Y_i| > \varepsilon'] & = \sum_{i=1}^n \PP[ Y_i \neq 0] =  \sum_{i=1}^n \frac{1}{(\delta' n +i)^{\frac{1}{2}}}
%\\
%& = \sum_{i= \delta' n +1}^n \frac{1}{i^{\frac{1}{2}}} \to \infty \quad \text{as } n \to \infty \,.
%\end{split}
%\end{align*}}
%\cm{Hence by the Second Lemma of Borel-Cantelli we have that $Y_i > \varepsilon'$ happens infinitely often and we can conclude $\sum_{i=1}^{n} Y_i \to \infty$ almost surely as $n \to \infty$.} 

If $\lim_{n \to \infty}|K_{\lfloor nt \rfloor}| = + \infty$, since  the sequence of random vectors $\{\gamma_{\varphi_i} -\xi_{\varphi_i}\}_{i \geq 1}$ is i.i.d. having  the same distribution as $\{\gamma_{i} -\xi_{i}\}_{i \geq 1}$, which is also i.i.d. (see Lemma~\ref{lem: iid}), we can use~\cite[Theorem 8.2 item (iii)]{gut2005probability} and obtain 
\begin{equation}\label{eq:prob02}
   \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i)}{|K_{\lfloor nt \rfloor}|} = \sum_{i=1}^{|K_{\lfloor nt \rfloor}|} \frac{ (\gamma_{\varphi_i} - \xi_{\varphi_i})}{|K_{\lfloor nt \rfloor}|} \xrightarrow[n \to \infty]{} \EE[\gamma_1 -\xi_1] \, \text{ a.s..}
\end{equation}

Thus, from Lemma~\ref{Jntight} (item $ii)$), \eqref{eq:prob02finito} and~\eqref{eq:prob02} we have
\begin{equation}\label{eq:prob03}
    \sup_{0\le t \le T} \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i)}{|K_{\lfloor nt \rfloor}|} \xrightarrow[n \to \infty]{} 0 \,,
\end{equation}
in probability.
%
Hence, from~\eqref{xi_i->W2},~\eqref{eq:prob03} and Slutsky's Theorem (see, e.g., \cite[Theorem 11.4]{gut2005probability}) we obtain our result. 
\end{proof}


% \subsubsection{Case $\beta = 1/2$ and $d=2$; proof of Theorem~\ref{pn-ERW-d=2}} \label{prova-pn-ERW_d=2}

% Let us denote by $K_n$ the set of times until time $n$ in which $X$ visits a site for the first time and becomes excited. Henceforth, without loss of generality, we  assume $\CC = 1$, i.e., $p_n=n^{-1/2}$. We can write $K_n$ as
% \begin{equation*}
% K_n = \{ i \in \{1, 2, \dots, n\} : \um_{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}} =1 \}\,.  
% \end{equation*}
% Now we set the sequence of $\FF$-stopping times $\{\varphi_i\}_{i \ge 1}$ corresponding to the times the $p_n$-\Nametwo{} visits a new site. One can easily check that
% \begin{equation}\label{eq: def Kn}
% |K_n| = \sum_{i=1}^{n} \um_{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}} = \sum_{j=1}^{|\Rr_n^X|} \um_{\{U_{\varphi_j} \leq \varphi_j^{-1/2} \}}\,. 
% \end{equation}
% %If we further define  the following random variable
% %\begin{equation}\label{eq: def Jn}
%  %   |J_n| := \sum_{i=1}^{|\Rr_n^X|} 1_{\{U_i \leq i^{-1/2} \}}\,,
% %\end{equation}
% %then we obtain
% %\begin{equation}\label{Kn<Jn}
% %|K_n| =  \sum_{j=1}^{|\Rr_n^X|} 1_{\{U_{\tau_j} \leq \tau_j^{-1/2} \}} \preceq \sum_{i=1}^{|\Rr_n^X|} 1_{\{U_i \leq i^{-1/2} \}} = |J_n| \,,   
% %\end{equation}
% %by the fact that the realizations of the sequence $\{U_i\}_{i \ge 1}$ is i.i.d. and the $\tau_j$'s are stopping times.

% Below we present an important auxiliary result that will be useful in the proof of Theorem~\ref{pn-ERW-d=2}.

% \begin{lemma}\label{Jntight}
% Let $|K_n|$ be defined as in~\eqref{eq: def Kn}.  We have that the sequence of processes $\{\Hat{K}^n_{\cdot}/n^{1/2}\}_{n\geq 1}$ converges in probability as random elements of $C_{\Rs}[0, \infty)$ to the identically zero function.
% \end{lemma}
% %
% The proof of Lemma~\ref{Jntight} will be postponed at the end of this section.







%Thereunto we will see that this process fulfills the condition on Theorem 7.3 in~\cite{billingsley1999probability}.  

%One can notice that the process $J_{\lfloor n \cdot \rfloor}/n^{1/2}$ is tight for $t=0$ for all $n \ge 1$. Thus the first condition on Theorem 7.3 in~\cite{billingsley1999probability} is satisfied.

%Then let $P_n$ be a probability measure on $C[0, T]$ and the distribution of $J_{\lfloor n\cdot \rfloor}/n^{1/2}$. We denote the set $A_t(\varepsilon, \phi):= \{f \in C[0,T] : \sup_{t \le s \le t+\phi} |f(s) - f(t)| \ge \varepsilon \}$.
%To prove that the process satisfies the second condition on Theorem 7.3 in~\cite{billingsley1999probability} we will use the Corollary on page 83 in~\cite{billingsley1999probability}.

%This Corollary states that Condition $(ii)$ of Theorem 7.3 in~\cite{billingsley1999probability} holds if, for each positive $\varepsilon$ and $\eta$, there exists a $\phi \in (0,1)$, and an integer $n_0$ such that
%\begin{equation}\label{eq: pnaJn}
%\frac{1}{\phi}  P_n[A_t(\varepsilon, \phi)] \le \eta \quad \forall n \ge n_0\,.    
%\end{equation}

%Hence we have
%\begin{equation*}
%\begin{split}
%\frac{1}{\phi}  P_n[A_t(\varepsilon, \phi)] & = \frac{1}{\phi} \PP\left[\sup_{t \le s \le t+\phi } | |J_{\lfloor ns \rfloor}| - |J_{\lfloor n t \rfloor}|| \ge \varepsilon n^{\frac{1}{2}}  \right]
%\\
%& \le \frac{1}{\phi} \PP\left[|J_{\lfloor n(t+\phi) \rfloor}|  \ge \varepsilon n^{\frac{1}{2}}\right] \,.
%\end{split}    
%\end{equation*}
%Now with the same techniques we use to achieve~\eqref{eq:Bn3} we obtain


%\subsection{Proof of the convergence in distribution of the $p_n$-\Nametwo{} with $\beta = 1/2$ and $d \ge 4$.}\label{sec: d>4}

\subsubsection{Case $\beta = 1/2$ and $d \ge 3$; proof of Theorem~\ref{pn-ERW-d=>4}}\label{sec: d>4}

\hfill \\

\com{All these results about $J_n$, $V_n$ and the corresponding continuous processes could be removed....}

Recall that $\pi_d$ denotes the probability that the random walk with i.i.d. (with zero mean and finite variance) increments $\{\xi_i\}_{i\geq 0}$ on $\ZZ^d$ never returns to the origin. 
%
Given $\delta \in (0, 1)$, let us define the following random variables:
\begin{align}
\label{Bn'}
J_n(\delta) &:= \sum_{i=1}^{\delta n} \um_{\{U_i \leq i^{-1/2} \}}\,,
\\
\label{Fn'}
V_n(\delta')&:=\sum_{i=n-\delta' n+1}^{n} \um_{\{U_i \leq i^{-1/2} \}}\,.
\end{align}

The random variables $J_n$ and $V_n$ will be important to compute the constants $c_1$ and $c_2$ in the statement of Theorem~\ref{pn-ERW-d=>4}. Below we state a few  simple results about them; we defer the proofs to the end of this section. 


\begin{lemma}\label{B'_n} Fix $d\geq 3$ and
let $\{J_n(\delta)\}_{n \geq 1}$ be defined as in~\eqref{Bn'} with $\delta \in (\pi_d, 1)$ and  $\{V_n(\delta')\}_{n \geq 1}$ be defined as in~\eqref{Fn'} with $\delta' \in (0, \pi_{d})$. Then, it holds that
%
\begin{align*}
i) &\quad \lim_{n \to \infty} \frac{\EE[J_n(\delta)]}{n^{1/2}} = 2\delta^{1/2} \quad \text{ and }\quad  \lim_{n \to \infty} \frac{\EE[V_n(\delta')]}{n^{1/2}} = 2-  2(1 -\delta')^{1/2}\,,
\\
   ii) &\quad \text{For any $\varepsilon > 0$,} 
   \\
   &\qquad \lim_{n \to \infty} \PP[|J_n(\delta) - \EE[J_n(\delta)]| > \varepsilon n^{1/2}] = 0\,, 
   % \text{ and } \lim_{n \to \infty} \PP[|V_n(\delta') - \EE[V_n(\delta')]| > \varepsilon n^{1/2}] = 0 \,.  
   \\
   &\quad \text{  and the same holds for $V_n(\delta')$. }
    \end{align*}
   
    %  \item [(iii)]
    % \begin{equation*}
    % \lim_{n \to \infty} \frac{\EE[V_n]}{n^{1/2}} = 2-  2(1 -\delta')^{1/2} ; 
    % \end{equation*}
    
    % \item[(iv)] For any $\varepsilon > 0$, 
    % \begin{equation*}
    % \lim_{n \to \infty} \PP[|V_n - \EE[V_n]| > \varepsilon n^{1/2}] = 0 ; 
    % \end{equation*}
\end{lemma}
\medskip 
%
From Lemma~\ref{B'_n} we obtain the following corollary. 

% \begin{corollary}\label{B'n->p}
% Let $\{J_n\}_{n \geq 1}$ be defined in~\eqref{Bn'} with $\delta \in (\pi_d, 1)$ and  $\{F_n\}_{n \geq 1}$ be defined in~\eqref{Fn'} with $\delta' \in (0, \pi_{d-k})$. Then, it holds that: 
% \begin{itemize}
%     \item [i)] 
%     \begin{equation*}
%         \frac{J_n}{n^{1/2}} \xrightarrow[n \to \infty]{} 2\delta^{1/2} \ \text{ in probability;}
%     \end{equation*}
    
%     \item [ii)]
%     \begin{equation*}
%         \frac{F_n}{n^{1/2}} \xrightarrow[n \to \infty]{} 2(1- \delta')^{1/2} \  \text{ in probability;}
%     \end{equation*}
    
%     \item [iii)]
%     \begin{equation*}
%          \frac{\sum_{i=1}^{n} \um_{\{U_i \le i^{1/2}\}}}{n^{1/2}}  \xrightarrow[n \to \infty]{} 2 \ \text{ in probability.}
%     \end{equation*}
% \end{itemize}
% \end{corollary}

 
\begin{corollary}\label{B'n->p}
Fix $d\geq 3$ and let $\{J_n(\delta)\}_{n \geq 1}$ be defined in~\eqref{Bn'} with $\delta \in (\pi_d, 1)$ and  $\{V_n(\delta')\}_{n \geq 1}$ be defined in~\eqref{Fn'} with $\delta' \in (0, \pi_{d})$. Then, it holds that
    \begin{equation*}
        \frac{J_n(\delta)}{n^{1/2}} \xrightarrow[n \to \infty]{} 2\delta^{1/2} \quad \text{ and } \quad  \frac{V_n(\delta')}{n^{1/2}} \xrightarrow[n \to \infty]{} 2- 2(1- \delta')^{1/2} \quad   \text{ in probability}\,.
    \end{equation*}
    % \begin{equation*}
    %  \tag{ii}   \frac{V_n}{n^{1/2}} \xrightarrow[n \to \infty]{} 2- 2(1- \delta')^{1/2} \  \text{ in probability}\,.
    % \end{equation*}
\end{corollary}

\medskip 
%
Relying on  Corollary~\ref{B'n->p}, we are able to prove the following result: 

\begin{lemma}\label{lem: tightaux} Fix $d\geq 3$ and
let $\{\Hat{J}^{n}_{\cdot}(\delta)\}_{n\geq 1}$ and $\{\Hat{V}^n_{\cdot}(\delta')\}_{n\geq 1}$ be respectively the sequences of processes in $C_{\Rs}[0, \infty)$ corresponding to  
$J_n(\delta)$ with $\delta \in (\pi_d, 1)$ and  $V_n(\delta')$ with $\delta' \in (0, \pi_{d})$  defined in~\eqref{Bn'} and \eqref{Fn'}, respectively.   Then, it holds that:
%
\begin{itemize}
    \item [i)]
    $\{\Hat{J}^n_{\cdot}(\delta)/n^{1/2}\}_{n\geq 1}$ converges in distribution as random elements of $C_{\Rs}[0, \infty)$ to the deterministic function $t \mapsto 2(\delta t)^{1/2}$, $t\ge 0$.  
    \item[ii)] $\{\Hat{V}^n_{\cdot}(\delta')/n^{1/2}\}_{n\geq 1}$
%$$
%\left\{\frac{\sum_{i=1}^{\lfloor n \cdot \rfloor}  \um_{\{U_i \leq i^{-1/2} \}} - F_{\lfloor n \cdot \rfloor}}{n^{1/2}}\right\}_{t\geq 0} 
%$$
converges in distribution as random elements of $C_{\Rs}[0, \infty)$ to the deterministic function $t \mapsto 2t^{1/2}(1\!-\!(1\!-\!\delta')^{1/2})$, $t\ge 0$.  
\end{itemize}
\end{lemma}

% The proof of Lemma~\ref{lem: tightaux} will be postponed to the end of this section.

% The next result states that $(|K_{\lfloor n\cdot \rfloor}|/n^{1/2})_{n\ge 1}$ is tight in $C_{\Rs}[0, \infty)$. %In the proof of the Theorem~\ref{pn-ERW-d=>4}, it will be more clear the importance of this result.


% \begin{lemma}\label{lem: Knt/n1/2tig}
% The sequence of processes $\{\Hat{K}^n_{\cdot }/n^{1/2}\}_{n\ge 1}$ is tight  in  $C_{\Rs}[0, \infty)$.
% \end{lemma}

By Lemma~\ref{Jntight} (item $i)$), for $d\geq 3$, every subsequence of $\{\Hat{K}^n_{\cdot }/n^{1/2}\}_{n\ge 1}$ has a limit point. The next result states that those limits points are concentrated on paths confined between the curves  $t \mapsto 2 (1-\sqrt{1 -  \pi_{d}}) \sqrt{t}$ and $t \mapsto 2  \sqrt{ \pi_d \, t}$.
% The proof of Lemma~\ref{lem: Knt/n1/2tig} will be postponed at the end of this section.

% \begin{proposition}\label{prop: bound_Kn}
% %Let $|K_{\lfloor n \cdot \rfloor}|/n^{1/2}$ be a sequence of processes in $C_{\Rs}[0, \infty)$. 
% If $H_{\cdot}$ is a limit point of $\{\Hat{K}^n_{\cdot }/n^{1/2}\}_{n\ge 1}$, then
% \begin{equation*}
% \PP \left[\forall t \in [0,\infty): 2t^{1/2}(1-(1 -  \delta')^{1/2}) \le H_t \le 2(t \delta)^{1/2} \right] = 1 \, ,  
% \end{equation*}
% where $\delta'$ and $\delta$ are positive constants such that $\delta' \in (0, \pi_{d-k})$ and $\delta \in (\pi_d, 1)$. 
% \end{proposition}

\begin{proposition}\label{prop: bound_Kn}
%Let $|K_{\lfloor n \cdot \rfloor}|/n^{1/2}$ be a sequence of processes in $C_{\Rs}[0, \infty)$. 
If $\mathcal{H}_{\cdot}$ is a limit point of $\{\Hat{K}^n_{\cdot }/n^{1/2}\}_{n\ge 1}$, then
\begin{align}
\tag{a}&\text{ For $d\geq 3$, }\, \quad 
\PP \left[\forall t \in [0,\infty):  \mathcal{H}_t \le 2(t \pi_d)^{1/2} \right] = 1 \, ; 
\\
\tag{b}&\text{ For $d\geq 22$,} \quad 
\PP \left[\forall t \in [0,\infty): \mathcal{H}_t \ge 2t^{1/2}(1-(1 -  \pi_{d})^{1/2}) \right] = 1 \, . 
\end{align}
\end{proposition} 

\begin{remark}\label{rem:conjecture}
If Conjecture~\ref{conj_range} were true, we would be able to strengthen the claim of Proposition~\ref{prop: bound_Kn} and obtain that for any $d\geq 3$ it holds that
\begin{equation*}
\PP \left[\forall t \in [0,\infty): 2t^{1/2}(1-(1 -  \pi_{d})^{1/2}) \le \mathcal{H}_t \le 2(t \pi_d)^{1/2} \right] = 1 \, . 
\end{equation*}
\end{remark}


% The proof of Proposition~\ref{prop: bound_Kn} will be postponed at the end of this section.


We now have all the auxiliaries results to prove Theorem~\ref{pn-ERW-d=>4}. The main idea is to use~\eqref{xn-incremento2} and then analyze separately the rescaled sum terms. We show  that the sequences corresponding to both terms are tight in $C_{\Rs^d}[0, \infty)$, consequently we obtain that $\{\Hat{B}_{\cdot}^n\}_{n\geq 1}$ is also tight. Finally, we  describe the processes which stochastically dominate the limit points of $\{\Hat{B}_{\cdot}^n\}_{n\geq 1}$. The strategy is similar to that used in the proof of Theorem~\ref{pn-ERW-d=2}, but here the term representing the drift direction does not go to zero. The random variables $J_n$ and $V_n$ play an important role in controlling this non-vanishing term.  
%
\begin{proof}[Proof of Theorem~\ref{pn-ERW-d=>4}]
We begin showing that 
$\{{\Hat{B}}_{\cdot}^n\}_{n\ge 1}$ is tight in $C_{\Rs^d}[0, \infty)$ for any $d\geq 3$. By Remark~\ref{rem:conver}-$b)$ it suffices  to show that $\{{\Hat{B}}_{\cdot}^n\}_{n\ge 1}$ is tight in $C_{\Rs^d}[0, T)$, for all $T>0$.

%
Using ~\eqref{xn-incremento2} we can rewrite the process $B_t^n$ as
\begin{align}\label{p_n-ERW_incrementos_d=>4}
\begin{split} 
& \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{1}{n^{1/2}} \sum_{i=1}^{\lfloor nt \rfloor} \um_{\{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}\}} (\gamma_i - \xi_i) \, , \ \forall \, t \ge 0\,.
% \\
% & = \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{1}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}}  (\gamma_i - \xi_i) 
% \\
% & 
% = \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i)}{|K_{\lfloor nt \rfloor}|}\,.
\end{split}
\end{align}
By Donsker's Theorem the first term of~\eqref{p_n-ERW_incrementos_d=>4} converges in distribution as random elements of $C_{\Rs^d}[0, \infty)$ to a Brownian Motion, i.e., 
\begin{equation}\label{xi_i->W2_d=>4}
 \Big\{ \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i \Big\}_{t\ge 0} \xrightarrow[n \to \infty]{\mathcal{D}} \{ W_{t} \}_{t\ge 0} \,,
\end{equation}
where $W_{\cdot}$ is a Brownian Motion in dimension $d$ with zero-mean vector and covariance matrix $\EE[\xi_1 \xi_1^T]$.
Then,  to show that $\{{\Hat{B}}_{\cdot}^n\}_{n\ge 1}$ is tight in $C_{\Rs^d}[0, T]$ for all $T>0$, it is enough to prove that the second term in~\eqref{p_n-ERW_incrementos_d=>4} is tight in $C_{\Rs^d}[0, T]$. 
Indeed, $\{\Hat{B}_{\cdot}^n\}_{n\geq 1}$ 
 would be the sum of two tight sequences of processes, thus also tight. %(see Lemma~\ref{lem: tight}) \cm{colocar uma referencia ou deixar?}.
%
% Then by~\cite[Theorem 4.10 in Chapter 2]{karatzas2012brownian}, since $B_{\cdot}^n$ is  tight  in $C_{\Rs^d}[0, T]$ for all $T>0$ with the topology of uniform convergence in the compacts, $B_{\cdot}^n$ is  tight  in $C_{\Rs^d}[0, \infty)$.

In order to show that the second term in~\eqref{p_n-ERW_incrementos_d=>4} is tight in $C_{\Rs^d}[0, T]$ for all $T>0$,  we use Remark~\ref{rem:conver}-$c)$.
% \cite[Theorem 7.3]{billingsley1999probability} which provides two sufficient conditions for tightness.
Recall the definition of $D_{\lfloor n t \rfloor}$ from the statement of Lemma \ref{lem: tgD}, remembering that here we have set $\mathcal{C} = 1$ and that we are under distinct hypotheses from those of Section \ref{prova-pn-WGERW}. We will show that $\{\Hat{D}^n_\cdot\}_{n\ge 1}$ is tight.
%$$
%D_{\lfloor n t \rfloor}:= \frac{1}{n^{1/2}} \sum_{i=1}^{\lfloor nt \rfloor} \um_{\{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}\}} (\gamma_i - \xi_i) \,.$$
The first condition in Remark~\ref{rem:conver}-$c)$ is satisfied, since  $\Hat{D}^n_0 \equiv 0$, for all $n\geq 1$. To prove that $\{\Hat{D}^n_\cdot\}_{n\ge 1}$ satisfies the second condition in Remark~\ref{rem:conver}-$c)$  we  use~\cite[Corollary on page 83]{billingsley1999probability} which states that the second condition of ~\cite[Theorem 7.3]{billingsley1999probability} holds if, for every positive $\varepsilon$ and $\eta$, there exists a $\phi \in (0,1)$, and an integer $n_0$ such that
\begin{equation}\label{eq: PnA}
\frac{1}{\phi} \, \PP \Big[ \sup_{t \le s \le t + \phi} \big\|\Hat{D}^n_{s} - \Hat{D}^n_{t} \big\|  \ge \varepsilon \Big]  \le \eta \quad \forall n \ge n_0\,.
\end{equation}
%\begin{equation}\label{eq: PnA}
%\frac{1}{\phi} P_n \Big[f \in C_{\Rs^d}[0,T] : \sup_{t \le s \le t+\phi} |f(s) - f(t)| \ge \varepsilon \Big] \le \eta \quad \forall n \ge n_0\,,
%\end{equation}
%where the probability measure $P_n$ on $C_{\Rs^d}[0,T]$  is the distribution of $D_{\lfloor n \cdot \rfloor}$. 
%In order to show that \eqref{eq: PnA} actually holds, note  that, if we define 
%\begin{equation}\label{eq:Atphi}
%A_t(\varepsilon, \phi):= \Big\{f \in C_{\Rs}[0,T] : \sup_{t \le s \le t+\phi} |f(s) - f(t)| \ge \varepsilon \Big\}\,,
%\end{equation} 
%the left-hand side of \eqref{eq: PnA} reduces to
% In order to show that actually holds,  let us define 
% $A_t(\varepsilon, \phi):= \{f \in C_{\Rs^d}[0,T] : \sup_{t \le s \le t+\phi} |f(s) - f(t)| \ge \varepsilon \}$. 
% Hence we obtain the following 
Note that the probability in \eqref{eq: PnA} is bounded from above by
\begin{equation}
\label{eq: PnPP}
%\PP\left[ D_{\lfloor n \cdot \rfloor} \in A_t(\varepsilon, \phi) \right] = 
%\PP \Big[ \sup_{t \le s \le t + \phi} \big\|\Hat{D}^n_{s} - \Hat{D}^n_{t} \big\|  \ge \varepsilon \Big] 
%& & = \PP \left[ \sup_{t \le s \le t+ \phi} \left\|\frac{\sum_{i=\lfloor nt \rfloor + 1}^{\lfloor ns \rfloor} 1_{\{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}\}} (\gamma_i - \xi_i)}{n^{\frac{1}{2}}}  \right\| \ge \varepsilon \right] 
\PP\Big[ \sup_{t \le s \le t+ \phi} \Big\| \sum_{i=\lfloor nt \rfloor}^{\lfloor ns \rfloor + 1} \um_{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}} (\gamma_i - \xi_i)  \Big\| \ge \varepsilon n^{\frac{1}{2}} \Big] \, ,
%\\
%& = \PP \left[ \sum_{i = \lfloor nt \rfloor + 1}^{\lfloor n(t + \phi) \rfloor} 1_{\{E_{i-1}^c \cap \{U_i \le i^{-\frac{1}{2}}\}\}} \ge \varepsilon n^{\frac{1}{2}} \right] \le \PP \left[ \sum_{i = \lfloor nt \rfloor + 1}^{\lfloor n(t + \phi) \rfloor} 1_{ \{U_i \le i^{-\frac{1}{2}}\}} \ge \varepsilon n^{\frac{1}{2}} \right]   
\end{equation}
%
% Now we only analyze the process inside the probability measure in~\eqref{eq: PnPP}. We will find an upper bound for this process for all the trajectory. 
for all $s \in [t, t+ \phi]$ and 
\begin{eqnarray}
\label{eq: trajetoria}
\Big\| \sum_{i=\lfloor nt \rfloor}^{\lfloor ns \rfloor + 1} \um_{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}} (\gamma_i - \xi_i)  \Big\| &\le & \sum_{i=\lfloor nt \rfloor}^{\lfloor ns \rfloor + 1} \big\|  \um_{ \{ U_i \leq i^{-1/2}\}} (\gamma_i - \xi_i)  \big\| \nonumber
 \\
% \le  \sum_{i=\lfloor nt \rfloor + 1}^{\lfloor ns \rfloor} \big\|  \um_{ \{ U_i \leq i^{-1/2}\}} (\gamma_i - \xi_i)  \big\| 
& \le & \sum_{i=\lfloor nt \rfloor}^{\lfloor ns \rfloor + 1}  \um_{ \{ U_i \leq i^{-1/2}\}} 2K  \,, 
\end{eqnarray}
where the second inequality follows from  triangle inequality and the last from Condition~\ref{condition_A}. 
%
Then from~\eqref{eq: PnPP} and ~\eqref{eq: trajetoria} we obtain that
\begin{eqnarray}\label{eq: PnA<1}
\lefteqn{\!\!\!\!\!\!\!\! \PP \Big[ \sup_{t \le s \le t + \phi} \big\|\Hat{D}^n_{s} - \Hat{D}^n_{t} \big\|  \ge \varepsilon \Big] \le 
\PP \Big[ \sum_{i = \lfloor nt \rfloor}^{\lfloor n(t + \phi) \rfloor +1} \um_{ \{U_i \le i^{-\frac{1}{2}}\}} 2K \ge \varepsilon n^{\frac{1}{2}} \Big] } \nonumber
\\
& & \le \exp\Big(\frac{-\varepsilon n^{\frac{1}{2}}}{2K}\Big)\EE\Big[\exp\Big( \sum_{i = \lfloor nt \rfloor}^{\lfloor n(t + \phi)\rfloor+1} \um_{ \{U_i \le i^{-\frac{1}{2}}\}} \Big) \Big] 
%& & \le \exp\Big(\frac{-\varepsilon n^{\frac{1}{2}}}{2K}\Big) \prod_{i = \lfloor nt \rfloor + 1}^{\lfloor n(t + \phi)\rfloor} \EE\left[\exp\left(  \um_{ \{U_i \le i^{-\frac{1}{2}}\}} \right) \right] \,,
\end{eqnarray}
where in the last inequality we have used exponential Markov's inequality.
%
Setting  $c = \varepsilon/(2K)$ an  continuing the computation in~\eqref{eq: PnA<1} we obtain that
\begin{eqnarray}\label{eq: PnA<}
\lefteqn{\frac{1}{\phi} \PP \Big[ \sup_{t \le s \le t + \phi} \big\|\Hat{D}^n_{s} - \Hat{D}^n_{t} \big\|  \ge \varepsilon \Big] \le \frac{1}{\phi} e^{-c n^{\frac{1}{2}}} \prod_{i = \lfloor nt \rfloor}^{\lfloor n(t + \phi)\rfloor + 1} \EE\left[\exp\left(  \um_{ \{U_i \le i^{-\frac{1}{2}}\}} \right) \right]} \nonumber \\
& & = \frac{1}{\phi} e^{-c n^{\frac{1}{2}}} \prod_{i = \lfloor nt \rfloor}^{\lfloor n(t + \phi)\rfloor + 1} \left(1+ \frac{e-1}{i^{\frac{1}{2}}} \right) \le \frac{1}{\phi} e^{-c n^{\frac{1}{2}}} \prod_{i = \lfloor nt \rfloor}^{\lfloor n(t + \phi)\rfloor + 1} \exp\left(\frac{e-1}{i^{\frac{1}{2}}} \right)  
% \\
% & \le \frac{1}{\phi} e^{-c n^{\frac{1}{2}}}  \exp\left(\sum_{i = \lfloor nt \rfloor + 1}^{\lfloor n(t + \phi)\rfloor}\frac{e-1}{i^{\frac{1}{2}}} \right) 
\nonumber \\
& & \le \frac{1}{\phi} \exp(-c n^{\frac{1}{2}})\exp\left(2(e-1)(\sqrt{n(t + \phi)} - \sqrt{nt} + 2) \right) \,,  
\end{eqnarray}
where %the second inequality follows by the moment generating function of a Bernoulli  and the third by the fact that $1+x<e^x$ for all $x$.  
the last inequality above follows from noticing that 
\begin{align*}
\sum_{i = \lfloor nt \rfloor}^{\lfloor n(t + \phi)\rfloor + 1}\frac{1}{i^{\frac{1}{2}}} & \le \int_{nt - 1}^{n(t+\phi) + 1} x^{-1/2}dx \le 2\left(\sqrt{n(t + \phi)} - \sqrt{nt} + 2\right) \,.
\end{align*}
Therefore, in order to show that \eqref{eq: PnA}  holds, it remains to show that for every positive $\varepsilon$ (recall that $c=\varepsilon/(2K)$) and $\eta$,  there exists a $\phi \in (0,1)$, and an integer $n_0$ such that
\begin{equation}\label{exp<eta}
\frac{1}{\phi} \exp(-c n^{\frac{1}{2}})\exp\bigg(2(e-1)\left(\sqrt{n}\left(\sqrt{t + \phi} - \sqrt{t} \right) + 2 \right) \bigg) \le \eta \quad \forall n \ge n_0 \,.
\end{equation}
We accomplish this choosing $\phi \in (0,1)$ sufficiently small such that $\sqrt{t+\phi} - \sqrt{t}< c/4(e-1)$. Then, for every $\eta>0$, choosing $n$ sufficiently large, we obtain that~\eqref{exp<eta} is satisfied. 
% One can see that, since we have $\phi \in (0,1)$,  for all $\hat{\varepsilon} > 0$, there exists a $\phi' > \phi$ such that $|\sqrt{t+\phi'} - \sqrt{t}|< \hat{\varepsilon}$. Now we can choose $\hat{\varepsilon} = c/4(e-1)$ and we obtain, for a large enough $n$, that~\eqref{exp<eta} is fulfilled for all $\eta$.
Consequently, we have that~\eqref{eq: PnA} is satisfied and thus $\{\Hat{D}^n_{\cdot}\}_{n \ge 1}$ is tight in $C_{\Rs^d}[0,T]$. 
 
% Since the process $B_{\cdot}^n$  is the sum of two tight processes in $C_{\Rs^d}[0, T]$,  we obtain that $B_{\cdot}^n$ is a tight process in $C_{\Rs^d}[0, T]$ for all $T>0$ as a simple exercise. %(see Lemma~\ref{lem: tight}) \cm{colocar uma referencia ou deixar?}.

% Now by~\cite[Theorem 4.10 in Chapter 2]{karatzas2012brownian} one can see that since $B_{\cdot}^n$ is  tight  in $C_{\Rs^d}[0, T]$ for all $T>0$ with the topology of uniform convergence in the compacts then  $B_{\cdot}^n$ is  tight  in $C_{\Rs^d}[0, \infty)$.

We now prove the second part of the theorem, namely the stochastic domination in $a)$ and $b)$. Let us begin rewriting $B_t^n$ again in the slightly different form
\begin{equation}
 \label{p_n-ERW_incrementos_d=>4-bis}
% & = \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{1}{n^{1/2}} \sum_{i=1}^{\lfloor nt \rfloor} \um_{\{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}\}} (\gamma_i - \xi_i) 
% % \\
% % & = \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{1}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}}  (\gamma_i - \xi_i) 
% \\
% & 
\frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i)}{|K_{\lfloor nt \rfloor}|}\,.
\end{equation}
As already mentioned the first term converges to a Brownian Motion (see \eqref{xi_i->W2_d=>4}).  We now analyze the second term in~\eqref{p_n-ERW_incrementos_d=>4-bis}. 
%
For $d\geq 22$, by Proposition~\ref{prop: bound_Kn} part $(b)$,  we have  that $|K_{\lfloor nt \rfloor}| \to \infty$ as $n \to \infty$ almost surely. Moreover, the sequence of random vectors $\{\gamma_{\varphi_i} -\xi_{\varphi_i}\}_{i \geq 1}$ is i.i.d. and has the same distribution of $\{\gamma_{i} -\xi_{i}\}_{i \geq 1}$, which is i.i.d. too (see Lemma~\ref{lem: iid}). Thus, we can use~\cite[Theorem 8.2 item (iii)]{gut2005probability} to conclude that 
\begin{equation}\label{eq:prob02_d=>4}
   \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i)}{|K_{\lfloor nt \rfloor}|} \xrightarrow[n \to \infty]{} \EE[\gamma_1 -\xi_1]  = \EE[\gamma_1] \, \text{ a.s.}\,.
\end{equation}
Recall that $\{\gamma_n\}_{n \ge 1}$ is an i.i.d. sequence of random vectors and  $\lambda  \le \EE[\gamma_i \cdot \ell] \le K$ for all $i \ge 1$, we set $\mu_{\gamma} := \EE[\gamma_i \cdot \ell]$ for all $i \ge 1$. By Lemma~\ref{Jntight} and and~\eqref{eq:prob02_d=>4} we obtain that the linearly interpolated version of the sequence 
\begin{equation}\label{eq:seq}
\left\{ \frac{|K_{\lfloor n \cdot \rfloor}|}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i) \cdot \ell}{|K_{\lfloor n \cdot \rfloor}|} \right\}_{n\ge 1}\,, 
\end{equation}
is tight in $C_{\Rs}[0, \infty)$ and, from   Proposition~\ref{prop: bound_Kn} part $(b)$, for $d\geq 22$  any of its limit points $\mathcal{H}_t$ satisfies
\begin{equation}\label{eq:c1}
\PP\left[\forall t \in [0,\infty): 
2\mu_{\gamma} t^{\frac{1}{2}}(1 - (1 - \pi_{d})^{\frac{1}{2}})
\le \mathcal{H}_t \right] = 1\,.
\end{equation}
For $d\geq 3$, with probability one either $\lim_{n \to \infty} |K_{\lfloor nt \rfloor}|< \infty$ or $\lim_{n \to \infty} |K_{\lfloor nt \rfloor}|=+ \infty$.
%
On the event $\lim_{n \to \infty} |K_{\lfloor nt \rfloor}|< \infty$,  we have that 
$\sum_{i \in K_{\lfloor nt \rfloor}} \frac{ \Vert \gamma_i - \xi_i \Vert}{|K_{\lfloor nt \rfloor}|} \leq 2K$ for all $n$ ($K$ is from Condition~\ref{condition_A}) and $\lim_{n \to \infty}\frac{|K_{\lfloor n t \rfloor}|}{n^{1/2}}=0$. Thus, on this event, the linearly interpolated version of the sequence in \eqref{eq:seq} converges to the zero function. On the event $\lim_{n \to \infty} |K_{\lfloor nt \rfloor}|=+ \infty$ \eqref{eq:prob02_d=>4} holds and, similarly to the case $d\geq 22$, by Lemma~\ref{Jntight} together with  Proposition~\ref{prop: bound_Kn} part $(a)$, any limit point $\mathcal{H}_t$ of the linearly interpolated version of the sequence in \eqref{eq:seq} satisfies $\mathcal{H}_t \le 2\mu_{\gamma}(t \pi_d)^{1/2}$. Since $2\mu_{\gamma}(t \pi_d)^{1/2}\geq 0$,  overall we then obtain that 
\begin{equation}\label{eq:c2}
\PP\left[\forall t \in [0,\infty): 
\mathcal{H}_t \le 2\mu_{\gamma}(t \pi_d)^{1/2} \right] = 1\,.
\end{equation}

% \begin{align}\label{sup_
% inf_Kn}
% \PP \left[\forall t \in [0,\infty): 2t^{1/2}(1-(1 -  \pi_{d-k})^{1/2}) \le H_t \le 2(t \pi_d)^{1/2}  \right]  = 1 \,,
% \end{align}
% where $\{H_t\}_{t\ge 0}$ is a limit point of a subsequence of $\{ \Hat{K}^n_{\cdot}/n^{1/2}\}_{n\geq 1}$.
%\com{Above $\delta --> \delta' $ and  $\hat\delta --> \dekta $ right?}
%$\delta''$, $\Hat{\delta}$, $\delta'$ and $\delta$ are positive constants such that $\delta'' \in (0, \delta')$, $\Hat{\delta} \in (\delta, 1]$, $\delta \in (\pi_d, 1]$ and $\delta' \in (0, \pi_{d-k})$.
% \begin{equation}\label{Fn<Kn<Bn}
% \frac{1}{n^{1/2}} \sum_{i=1}^{\lfloor nt \rfloor}  1_{\{U_i \leq i^{-1/2} \}} - \frac{|F_{\lfloor nt \rfloor}|}{n^{1/2}} \preceq \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \preceq \frac{|J_{\lfloor nt \rfloor}|}{n^{1/2}}\,.
% \end{equation}
% Thus, by~\eqref{Bn<=Bn'} and~\eqref{Fn<Kn<Bn}, one can see that
% \begin{align}\label{eq:Kn<B'n}
% \begin{split}
% & \PP \left[ \forall t \in [0,\infty) : \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \le \frac{|J'_{\lfloor nt \rfloor}|}{n^{1/2}} \right] \ge \PP\left[ \forall t \in [0,\infty]: \frac{|J_{\lfloor nt \rfloor}|}{n^{1/2}} \le \frac{|J'_{\lfloor nt \rfloor}|}{n^{1/2}} \right] 
% \\
% & \to 1 \quad \text{as } n \to \infty\,.
% \end{split}
% \end{align}
% Now we will obtain, in the same sense of~\eqref{eq:Kn<B'n}, a lower bound. Hence by~\eqref{Fn<=Fn'} and~\eqref{Fn<Kn<Bn} we have
% \begin{equation}\label{eq: Kn>s-F'n}
% \begin{split}
% & \PP \left[\forall t \in [0,\infty) : \frac{\sum_{i=1}^{\lfloor nt \rfloor}  1_{\{U_i \leq i^{-1/2} \}}}{n^{1/2}} - \frac{|F'_{\lfloor nt \rfloor}|}{n^{1/2}} \le \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}}  \right] 
% \\
% & \ge \PP\left[\forall t \in [0,\infty): \frac{\sum_{i=1}^{\lfloor nt \rfloor}  1_{\{U_i \leq i^{-1/2} \}} - |F'_{\lfloor nt \rfloor}|}{n^{1/2}} \le \frac{\sum_{i=1}^{\lfloor nt \rfloor}  1_{\{U_i \leq i^{-1/2} \}} - |F_{\lfloor nt \rfloor}|}{n^{1/2}} \right] 
% \\
% & \to 1 \quad \text{as } n \to \infty \,.
% \end{split}
% \end{equation}
% Hence by Lemma~\ref{lem: tightaux}, Corollary~\ref{B'n->p}, ~\eqref{eq:Kn<B'n} and~\eqref{eq: Kn>s-F'n} we obtain that for all $t_0>0$
% \begin{align}\label{sup_
% inf_Kn}
% \PP \left[\forall t \in [t_0,\infty): 2t^{1/2}(1-(1 -  \delta')^{1/2}) \le \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \le 2(t \delta)^{1/2} \right] \to 1 \,,
% \end{align}
% as $n$ goes to infinity where $\delta$ and $\delta'$ are positive constants such that $\delta \in (\pi_d, 1]$ and $\delta' \in (0, \pi_{d-k})$.
%
Since $\{\Hat{B}_{\cdot}^n\}_{n\geq 1}$ is  tight in $C_{\Rs^d}[0, \infty)$, thus relatively compact by Prohorov's Theorem (see, e.g., ~\cite[Theorem 5.1]{billingsley1999probability}),  every subsequence has a limit point.
%
By~\eqref{xi_i->W2_d=>4}, \eqref{eq:c2} and \eqref{eq:c1},  for any of those limit points $\{\mathcal{Y}_{t}\}_{t\ge 0}$ we have that for all $t \in [0, \infty)$
\begin{align*}
 \{\mathcal{Y}_t \cdot \ell\}_{t\ge 0} &\preceq \{W_t \cdot \ell + 2 c_2 \sqrt{t} \}_{t\ge 0} \,, && \text{ for $d\geq 3$}\,,\\
\{W_t \cdot \ell + 2 c_1 \sqrt{t}\}_{t\ge 0} &\preceq \{\mathcal{Y}_t \cdot \ell\}_{t\ge 0}\,, && \text{ for $d\geq 22$}\,,  
\end{align*}
where $c_1 = (1 - \sqrt{1 - \pi_{d}})\mu_{\gamma}$ and $c_2 = \sqrt{\pi_d} \, \mu_{\gamma}$ (and $0 < c_1 \le c_2$).  
\end{proof}



\begin{proof}[Proof of Lemma~\ref{Jntight}] 
Item $ii)$: 
 By   Remark~\ref{rem:conver}-$a)$,  it is enough to prove that the sequence of processes $\{\Hat{K}^n_{\cdot}/n^{1/2}\}_{n\geq 1}$ converges in probability as random elements of $C_{\Rs}[0, T)$ to the  zero function, for all $T > 0$. For the latter it suffices to prove  
 convergence in probability of $\sup_{0 \le t \le T} |K_{\lfloor nt \rfloor}|/n^{1/2}$ to zero for all $T > 0$.
% , since convergence in $C_{\Rs^d}[0, T]$ for all $T >0$ implies convergence in $C_{\Rs^d}[0, \infty)$ under the metric $\rho$ 
% \com{...HERE!!}
% . 
To this aim, let us define  
$$G_n:=\Big\{\sup_{0 \le t \le T} |K_{\lfloor nt \rfloor}| > \varepsilon \sqrt{n}\Big\} = \left\{ |K_{\lfloor nT \rfloor}| > \varepsilon \sqrt{n} \right\}\, ,
$$
For every $\varepsilon > 0$ and $\delta>0$, %consider the following event $G = \{|K_{\lfloor nT \rfloor}| > \varepsilon \sqrt{n}\}$. 
by Markov's inequality, we have that 
\begin{align*}\label{eq: Jnprob}
\begin{split}
& \PP[ G_n ] = \PP\big[
G_n \cap \{|\Rr_{\lfloor nT \rfloor} ^X| > \delta \lfloor nT \rfloor\}\big] +  \PP\big[ G_n \cap \{|\Rr_{\lfloor nT \rfloor}^X| \leq \delta \lfloor nT \rfloor\}\big]
\\
& \leq \PP\big[|\Rr_{\lfloor nT \rfloor} ^X| > \delta \lfloor nT \rfloor\big] + \PP\Big[ \sum_{i=1}^{\lceil \delta n T\rceil} \um_{\{U_i \leq i^{-1/2} \}} > \varepsilon\sqrt{n} \Big] 
\\
& \leq \PP\big[|\Rr_{\lfloor nT \rfloor} ^X| > \delta \lfloor nT \rfloor\big] + \frac{1}{\varepsilon \sqrt{n}} \sum_{i=1}^{\lceil \delta nT\rceil} \frac{1}{i^{1/2}}\,. \end{split}
\end{align*}
Note that we have used $\Rr_{\lfloor nT \rfloor}^X$ instead of $\Rr_{\lfloor nT \rfloor -1}^X$, the reader can check that indeed this does not change the inequality and simplifies notation. This will happen in other inequalities in this section.
Using  Proposition~\ref{prop:RangeERW}, we have that, for all sufficiently large $n$,  $
    \PP[ |\Rr_n ^X| \leq \delta n ] = 1$, for every  $\delta > \pi_d$. Since for $d=2$, $\pi_d =0$,  we obtain that   $\lim_{n \to \infty}\PP\big[|\Rr_{\lfloor nT \rfloor} ^X| > \delta \lfloor nT \rfloor\big]=0$, for every $\delta>0$. Moreover, noticing that   $\sum_{i=1}^{\lceil \delta n\rceil} \frac{1}{i^{1/2}} = \Theta(\lceil \delta n\rceil^{1/2})$, we conclude that
\begin{equation}\label{eq: Jnprob2}
\begin{split}
& \limsup_{n \to \infty}  \PP[ G_n ]  
%\leq \limsup_{n \to \infty} \Big( \PP\big[|\Rr_{\lfloor nT \rfloor}^X| > \delta \lfloor nT \rfloor\big] + \frac{1}{\varepsilon \sqrt{n}} \sum_{i=1}^{\lceil \delta nT \rceil} \frac{1}{i^{1/2}} \Big) \\ & \leq \limsup_{n \to \infty} \PP\big[|\Rr_{\lfloor nT \rfloor}^X| > \delta \lfloor nT \rfloor\big] + \limsup_{n \to \infty} \Big( \frac{1}{\varepsilon \sqrt{n}} \sum_{i=1}^{\lceil \delta nT \rceil} \frac{1}{i^{1/2}} \Big) 
\leq \frac{c' (\delta T)^{1/2}}{\varepsilon}\,.
\end{split}
\end{equation}
Since $\delta>0$ is arbitrary,  $\limsup_{n \to \infty} \PP[ G_n ] = 0$ for every $\varepsilon$ fixed. Therefore, for all $T > 0$ the sequence of processes $\left\{\Hat{K}^n_{\cdot}/n^{1/2}\right\}_{n\geq 1}$ converges in probability, as random elements of $C_{\Rs}[0,T]$, to the  zero function.  %(see Lemma~\ref{lem: convprob}) \cm{pensar em uma referencia ou não ha necessidade?}. 

\medskip 
Item $i)$: 
% The proof follows the very same lines of that of Theorem~~\ref{pn-ERW-d=>4}, and we omit it. 
%
By  Remark~\ref{rem:conver}-$b)$  it suffices to show tightness in $C_{\Rs}[0, T]$ for all $T > 0$ and this is equivalent to show the sequence of processes $\left\{\Hat{K}^n_{\cdot}/n^{1/2}\right\}_{n\geq 1}$  satisfies  the two conditions in  Remark~\ref{rem:conver}-$c)$. 
Note that $\Hat{K}^n_{0}/n^{1/2}\equiv 0$ for all $n \ge 1$ and therefore the first condition in Remark~\ref{rem:conver}-$c)$ is satisfied.
%
To prove that $\{\Hat{K}^n_\cdot /n^{1/2}\}_{n\ge 1}$ satisfies the second condition in Remark~\ref{rem:conver}-$c)$  we  use~\cite[Corollary on page 83]{billingsley1999probability} which states that the second condition of ~\cite[Theorem 7.3]{billingsley1999probability} holds if, for every positive $\varepsilon$ and $\eta$, there exists a $\phi \in (0,1)$, and an integer $n_0$ such that
\begin{equation*}
\frac{1}{\phi} \, \PP \Big[ \sup_{t \le s \le t + \phi} \big\|\Hat{K}^n_{s} - \Hat{K}^n_{t} \big\|  \ge \varepsilon n^{1/2} \Big]  \le \eta \quad \forall n \ge n_0\,.
\end{equation*}
From this point on, using that 
\[
\PP \Big[ \sup_{t \le s \le t + \phi} \big\|\Hat{K}^n_{s} - \Hat{K}^n_{t} \big\|  \ge \varepsilon n^{1/2} \Big]  \le \PP \Big[ \sum_{i = \lfloor nt \rfloor }^{\lfloor n(t + \phi) \rfloor +1} \um_{\{U_i \le i^{-\frac{1}{2}}\}} \ge \varepsilon n^{\frac{1}{2}} \Big]\,,
\]
the computations are very similar to those used in the proof of Theorem~\ref{pn-ERW-d=>4}, and we omit it. 
\end{proof}





\begin{proof}[Proof of Lemma~\ref{B'_n}.] To avoid clutter in the notation, we write $J_n$ and $V_n$, thus omitting the dependence on $\delta$ and $\delta'$.  
As far as $J_n$ is concerned, we have 
\begin{equation*}
\frac{\EE[J_n]}{n^{1/2}} = \frac{1}{n^{1/2}}\EE\Big[\sum_{i=1}^{\delta n} \um_{\{U_i \leq i^{-1/2} \}}\Big] = \frac{1}{n^{1/2}}\sum_{i=1}^{\delta n} i^{-1/2} \xrightarrow[n \to \infty]{} 2\delta^{1/2}  \,. 
\end{equation*}
Also,  using Chebyshev's inequality and the independence of the random variables $\{U_i\}_{i \geq 1}$ we have that 
\begin{align*}
\PP \big[ |J_n - \EE[J_n]| > \varepsilon n^{1/2} \big] &\leq \frac{1}{\varepsilon^2 n} \text{Var}\Big[ \sum_{i=1}^{\delta n} \um_{\{U_i \leq i^{-1/2} \}} \Big]
\\
&=\frac{1}{\varepsilon^2 n} \sum_{i=1}^{\delta n} \frac{1}{i^{1/2}}\left(1-\frac{1}{i^{1/2}}\right) \xrightarrow[n \to \infty]{} 0  \,. 
\end{align*}

The proofs for $V_n$ are similar once we write
\[
V_n = \underbrace{\sum_{i=1}^n  \um_{\{U_i \leq i^{-1/2} \}}}_{=:I_n} - \underbrace{\sum_{i=1}^{n -\delta' n} \um_{\{U_i \leq i^{-1/2} \}}}_{=:F_n'}\,,
\]
and observe that  
\begin{align*}
&\frac{1}{n^{1/2}}\EE[I_n] = \frac{1}{n^{1/2}}\sum_{i=1}^{n} i^{-1/2} \xrightarrow[n \to \infty]{} 2\,,
\\
&\frac{1}{n^{1/2}}\EE[F'_n] = \frac{1}{n^{1/2}}\sum_{i=1}^{n - \delta' n} i^{-1/2} \xrightarrow[n \to \infty]{} 2(1-\delta')^{1/2} \,, 
\end{align*}
and that, by Chebyshev's inequality and the independence of the random variables $\{U_i\}_{i \geq 1}$, it holds that 
\begin{align*}
\PP & [|I_n - \EE[I_n]| > \varepsilon n^{1/2}] 
 \leq \frac{1}{\varepsilon^2 n} \text{Var}\Big[ \sum_{i=1}^{n} \um_{\{U_i \leq i^{-1/2} \}} \Big] 
% = 
% \frac{1}{\varepsilon^2 n} \sum_{i=1}^{n} i^{-1/2}(1-i^{-1/2})
\xrightarrow[n \to \infty]{} 0  
\,,
\\
\PP &\big[ |F'_n - \EE[F'_n]| > \varepsilon n^{1/2} \big] \leq \frac{1}{\varepsilon^2 n} \text{Var}\Big[ \sum_{i=1}^{n -\delta' n} \um_{\{U_i \leq i^{-1/2} \}} \Big]
% \\
% &
% =  \frac{1}{\varepsilon^2 n} \sum_{i=1}^{n - \delta' n} i^{-1/2}(1-i^{-1/2}) 
\xrightarrow[n \to \infty]{} 0 
\,.
\end{align*}
% The proof of point $v)$ is also straightforward: 
% \begin{equation*}
% \frac{\EE[\sum_{i=1}^n  \um_{\{U_i \leq i^{-1/2} \}}]}{n^{1/2}} = \frac{1}{n^{1/2}}\sum_{i=1}^{n} i^{-1/2} \xrightarrow[n \to \infty]{} 2 
% \,.  
% \end{equation*}
% For the proof of point $vi)$ we use Chebyshev's inequality and the independence of the random variables $\{U_i\}_{i \geq 1}$ and we obtain
% \begin{align*}
% % \label{eq: P}
% % \begin{split}
% \PP & \Big[\Big|\sum_{i=1}^n  \um_{\{U_i \leq i^{-1/2} \}} - \EE\Big[\sum_{i=1}^n  \um_{\{U_i \leq i^{-1/2} \}}\Big]\Big| > \varepsilon n^{1/2}\Big] 
%  \leq \frac{1}{\varepsilon^2 n} \text{Var}\Big[ \sum_{i=1}^{n} \um_{\{U_i \leq i^{-1/2} \}} \Big] 
%  \\
% &= \frac{1}{\varepsilon^2 n} \sum_{i=1}^{n} i^{-1/2}(1-i^{-1/2}) \to 0  \quad \text{as } n \to \infty 
% \,.  
% \end{align*}
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem: tightaux}.] To avoid clutter in the notation we omit the dependence on $\delta$ and $\delta'$. By Corollary~\ref{B'n->p}, for all $t> 0$ we have that 
\begin{align*}
\frac{J_{\lfloor n t\rfloor}(\delta)}{\sqrt{n}} & \xrightarrow[n \to \infty]{} 2\delta^{1/2} \sqrt{t}\,, && \forall \delta \in (\pi_d,1)\,;
\\
\frac{V_{\lfloor n t\rfloor}(\delta')}{\sqrt{n}} &\xrightarrow[n \to \infty]{} \left(2 - 2(1-\delta')^{1/2}\right)\sqrt{t}\,, && \forall  \delta' \in (0,\pi_d)\,,  
\end{align*}
in probability, i.e., the one-dimensional distributions  converge in probability to a constant. Therefore the joint distributions corresponding to times $t_1, \ldots, t_m$ also converge in probability and we obtain the convergence in the sense of the finite-dimensional distributions for both processes.
% By Corollary 3.1 we already have the convergence of the
% finite-dimensional distributions. 
% we have the convergence of the one-dimensional distributions in probability to a constant. Since each one-dimensional random variable converges in probability to a constant, the joint convergence in probability follows naturally, which in turn implies convergence in distribution.

By~\cite[Theorem 7.1]{billingsley1999probability},  to prove points $i)$ and $ii)$ it only remains to prove that both sequences of processes are tight in $C_{\Rs}[0, \infty)$. By Remark~\ref{rem:conver}-$b)$ we only need to prove tightness in $C_{\Rs}[0, T]$ for all $T>0$. The proof strategy is analogous to the one used in the proof of Theorem~\ref{pn-ERW-d=>4} which relies on Remark~\ref{rem:conver}-$c)$.  %
% when we show the second sum portion in~\eqref{p_n-ERW_incrementos_d=>4} is tight in $C_{\Rs}[0, T]$, for all $T>0$.

Item $i)$:  $\{\Hat{J}^n_{\cdot }/n^{1/2}\}_{n\geq 1}$ satisfies the first condition in Remark~\ref{rem:conver}-$c)$, since $\Hat{J}^n_0 = 0$ for all $n\geq 1$.  To prove the second condition in Remark~\ref{rem:conver}-$c)$, as in \eqref{eq: PnA} we need to show that for every positive $\varepsilon$ and $\eta$, there exists a $\phi \in (0,1)$, and an integer $n_0$ such that
\begin{equation}\label{eq: PnA2}
\frac{1}{\phi} \, \PP \Big[ \sup_{t \le s \le t + \phi} \big\|\Hat{J}^n_{s} - \Hat{J}^n_{t} \big\| \ge \varepsilon \Big]  \le \eta \quad \forall n \ge n_0\,.
\end{equation}
Following the same steps as in the proof of Theorem~\ref{pn-ERW-d=>4}, we have that
\begin{equation*}
\begin{split}
&\frac{1}{\phi} \PP \Big[ \sup_{t \le s \le t + \phi} \big\|\Hat{J}^n_{s} - \Hat{J}^n_{t} \big\| \ge \varepsilon \Big] \le 
 \frac{1}{\phi} \PP \Big[ \sum_{i = \delta\lfloor nt \rfloor}^{\delta\lfloor n(t + \phi) \rfloor +1} \um_{ \{U_i \le i^{-\frac{1}{2}}\}} \ge \varepsilon n^{\frac{1}{2}} \Big] 
% \\
 %& \leq  \frac{1}{\phi} e^{-\varepsilon n^{\frac{1}{2}}}\EE\Big[\exp\Big( \sum_{i = \delta\lfloor nt \rfloor + 1}^{\delta\lfloor n(t + \phi)\rfloor} \um_{ \{U_i \le i^{-\frac{1}{2}}\}} \Big) \Big] 
% \\
% & = \frac{1}{\phi} e^{-\varepsilon n^{\frac{1}{2}}} \prod_{i = \delta\lfloor nt \rfloor + 1}^{\delta\lfloor n(t + \phi)\rfloor} \left(1+ \frac{e-1}{i^{\frac{1}{2}}} \right) \le \frac{1}{\phi} e^{-\varepsilon n^{\frac{1}{2}}} \prod_{i = \delta\lfloor nt \rfloor + 1}^{\delta\lfloor n(t + \phi)\rfloor} \exp\left(\frac{e-1}{i^{\frac{1}{2}}} \right)  
% \\
% & \le \frac{1}{\phi} e^{-c n^{\frac{1}{2}}}  \exp\left(\sum_{i = \lfloor nt \rfloor + 1}^{\lfloor n(t + \phi)\rfloor}\frac{e-1}{i^{\frac{1}{2}}} \right) 
\\
& \le \frac{1}{\phi} \exp(-\varepsilon n^{\frac{1}{2}})\exp\left(2(e-1)(\sqrt{\delta n(t + \phi)} - \sqrt{\delta nt} + 2) \right) \,.
\end{split}    
\end{equation*}
and we obtain \eqref{eq: PnA2} choosing $\phi \in (0,1)$ sufficiently small such that $\sqrt{t+\phi} - \sqrt{t}< \varepsilon/4\sqrt{\delta}(e-1)$ for all $t \in [0, T]$. 
% \begin{equation*}
% \frac{1}{\phi} \exp(-\varepsilon n^{\frac{1}{2}})\exp\bigg(2(e-1)\sqrt{n\delta}\left(\sqrt{t + \phi} - \sqrt{t}\right) \bigg) \le \eta \quad \forall n \ge n_0 \,.
% \end{equation*}
% The latter can be easily verified as it was done in the proof of Theorem~\ref{pn-ERW-d=>4}. 

% % From now on the proof follows exactly  as in Theorem~\ref{pn-ERW-d=>4}, when we prove the second sum portion in~\eqref{p_n-ERW_incrementos_d=>4} fulfills the second condition of~\cite[Theorem 7.3] {billingsley1999probability}. \comu{precisa ser mais específico aqui} Then we have that for each positive $\varepsilon$ and $\eta$, there exists a $\phi \in (0,1)$, and an integer $n_0$ such that
% % \begin{equation*}
% % \frac{1}{\phi} \PP [|J'_{\lfloor n \cdot \rfloor}|/n^{1/2} \in A_t(\varepsilon, \phi)] \le \eta \quad \forall n \ge n_0\,.
% % \end{equation*}
% % Ergo by~\cite[Theorem 7.3]{billingsley1999probability} we obtain that the sequence $|J_{\lfloor n \cdot \rfloor}|/n^{1/2}$ is a tight in $C_{\Rs}[0, T]$ for all $T > 0$ with the topology of uniform convergence in compacts and moreover by~\cite[Theorem 2.4.10]{karatzas2012brownian} \com{here we cite a different result than the one cited in the remark!} is a tight sequence of processes in $C_{\Rs}[0, \infty)$. 
The proof of item $ii)$ is similar with the only difference that we analyze separately $\sum_{i=1}^{\lfloor n \cdot \rfloor} \um_{ \{U_i \le i^{-\frac{1}{2}}\}}/n^{1/2}$ and    $\sum_{i=1}^{\lfloor (n -\delta' n) \cdot \rfloor} \um_{\{U_i \leq i^{-1/2} \}}/n^{1/2}$.
%
Using the very same computation as in  item $i)$, we conclude that the linearly interpolated version of 
$$
    \Big\{\sum_{i=1}^{\lfloor n \cdot \rfloor} \um_{ \{U_i \le i^{-\frac{1}{2}}\}}/n^{1/2}\Big\}_{n\geq 1}\,  \text{ and } \Big\{ \sum_{i=1}^{\lfloor (n -\delta' n) \cdot \rfloor} \um_{\{U_i \leq i^{-1/2} \}}/n^{1/2}\Big\}_{n\geq 1}\,,
$$
% $\{\Hat{F}^n_{\cdot}/n^{1/2}\}_{n\geq 1}$ 
are tight sequences in $C_{\Rs}[0, T]$ for all $T>0$. Thus, the same holds for their difference.
%
\end{proof}


% \begin{proof}[Proof of Lemma~\ref{lem: Knt/n1/2tig}.] The proof follows the very same lines of that of Theorem~~\ref{pn-ERW-d=>4}, and we omit it. 
% {\color{red} 
% By  Remark~\ref{rem:conver}-$b)$  it suffices to show tightness in $C_{\Rs}[0, T]$ for all $T > 0$ and this is equivalent to show the the sequence of processes $\{|K_{\lfloor n \cdot \rfloor}|/n^{1/2}\}_{n\geq 1}$  satisfies  the two conditions in  Remark~\ref{rem:conver}-$c)$. 
% Note that $|K_{\lfloor n \cdot 0 \rfloor}|/n^{1/2}\equiv 0$ for all $n \ge 1$ and therefore the first condition in Remark~\ref{rem:conver}-$c)$ is satisfied.
% %
% To prove the second condition in Remark~\ref{rem:conver}-$c)$, 
% set 
% $
% A_t(\varepsilon, \phi):= \{f \in C_{\Rs}[0,T] : \sup_{t \le s \le t+\phi} |f(s) - f(t)| \ge \varepsilon \}$. 
% Then, following the same steps as in the proof of Theorem~\ref{pn-ERW-d=>4}, we have that  
% \begin{equation*}
% \begin{split}
% &\frac{1}{\phi} \PP [ |K_{\lfloor n \cdot \rfloor}|/n^{1/2} \in A_t(\varepsilon, \phi)]  = \frac{1}{\phi} \PP\Big[\sup_{t \le s \le t+\phi } | |K_{\lfloor ns \rfloor}| - |K_{\lfloor n t \rfloor}|| \ge \varepsilon n^{\frac{1}{2}} \Big]
% \\
% & = \frac{1}{\phi} \PP \Big[ \sum_{i = \lfloor nt \rfloor + 1}^{\lfloor n(t + \phi) \rfloor} \um_{E_i^c \cap \{U_i \le i^{-\frac{1}{2}}\}} \ge \varepsilon n^{\frac{1}{2}} \Big]
%  \le \frac{1}{\phi} \PP \Big[ \sum_{i = \lfloor nt \rfloor + 1}^{\lfloor n(t + \phi) \rfloor} \um_{\{U_i \le i^{-\frac{1}{2}}\}} \ge \varepsilon n^{\frac{1}{2}} \Big]\,.
% \end{split}    
% \end{equation*}
% From this point on, the computations are exactly the same used in the proof of Theorem~\ref{pn-ERW-d=>4}, and we omit it.

% when we show the second sum portion in~\eqref{p_n-ERW_incrementos_d=>4} fulfills the second condition in~\cite[Theorem 7.3]{billingsley1999probability}.
% Then we have that for each positive $\varepsilon$ and $\eta$, there exists a $\phi \in (0,1)$, and an integer $n_0$ such that
% \begin{equation*}
% \frac{1}{\phi} P_n \Big[f \in C_{\Rs}[0,T] : \sup_{t \le s \le t+\phi} |f(s) - f(t)| \ge \varepsilon \Big] \le \eta \, , \quad \forall n \ge n_0\,.
% \end{equation*}
% Ergo by~\cite[Theorem 7.3]{billingsley1999probability} we obtain that $|K_{\lfloor n \cdot \rfloor}|/n^{1/2}$ is tight in $C_{\Rs}[0, T]$ for all $T > 0$. Thus by~\cite[Theorem 2.4.10]{karatzas2012brownian} it is also a tight sequence of processes in $(C_{\Rs}[0, \infty),\rho)$
% } 
% \end{proof}

\begin{proof}[Proof of Proposition~\ref{prop: bound_Kn}.] 
We first prove $(a)$. 
Note that the statement follows if we show that
\begin{equation}\label{oldstatement}
\PP \left[\forall t \in [0,\infty): \mathcal{H}_t \le 2(t \delta)^{1/2} \right] = 1 \, ,  
\end{equation}
for every $\delta \in (\pi_d, 1)$. Indeed taking a sequence $\delta_n \downarrow \pi_d$ as $n\to \infty$, we have that 
$$
\Big\{ \forall t \in [0,\infty): \mathcal{H}_t \le 2(t \pi_d)^{1/2} \Big\} = \bigcap_{n\ge 1} \Big\{ \forall t \in [0,\infty):  \mathcal{H}_t \le 2(t \delta_n)^{1/2} \Big\}\, .
$$

Let us begin with some instrumental fact: recall (from page \pageref{eq: def Kn}) that $\varphi_i := \psi_i + 1$, where $\{\psi_i\}_{i \ge 1}$ denotes  the sequence of $\FF$-stopping times  corresponding to the  times the $p_n$-\Nametwo{} visits a new site, and let us define: 
\begin{align*}
J'_n(\delta) := 
\sum_{i=1}^{\delta n} \um_{\{U_{\varphi_i} \le \varphi_i^{-1/2} \}}\,,  
\end{align*} 
with $\delta \in (\pi_d, 1)$. 
%
Using Lemma \ref{lem: iid} and since $\{U_i\}_{i \ge 1}$ is i.i.d. we have that 
\begin{equation*}\label{eq:dominance}
J'_n(\delta)  \preceq J_n(\delta)\,, 
\end{equation*}
for all $\delta \in (\pi_d, 1)$ where $J_n(\delta)$ is defined  in~\eqref{Bn'}.

Consider the event 
\begin{equation*}
A_n^{{\delta}, c, M}:= \Big\{\forall t \in [c, M]: \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \le 2({\delta} t)^{1/2}  \Big\} \,,   
\end{equation*}
where $c$, $M$ and ${\delta}$ are positive constants such that $M > c$ and ${\delta} \in (\pi_d, 1)$, and recall that $|K_{\lfloor nt \rfloor}|  = \sum_{j=1}^{|\Rr_{\lfloor nt \rfloor -1}^X|} \um_{\{U_{\varphi_j} \leq \varphi_j^{-1/2} \}}$ (see,  \eqref{eq: def Kn}. For every $\Hat{\delta} \in (\pi_d,\delta)$ we have that
\begin{align}\label{eq: A_t^cM<}
\begin{split}
& \PP[A_n^{{\delta}, c,M}]  \ge \PP[A_n^{\delta, c,M} \cap \{ \forall t \in [c,M]: |\Rr_{\lfloor nt \rfloor}^X| \le \Hat \delta \lfloor nt \rfloor\}]
\\
& \ge \PP\Big[ \Big\{\forall t \in [c,M]: \frac{J'_{\lfloor nt \rfloor}(\delta)}{n^{1/2}} \le 2(\delta t)^{1/2} \Big\} \cap \big\{ \forall t \in [c,M]: |\Rr_{\lfloor nt \rfloor}^X| \le \Hat \delta \lfloor nt \rfloor \big\}  \Big] \, .
\end{split}    
\end{align}
Considering the second event on the right-hand side of~\eqref{eq: A_t^cM<}, by Proposition~\ref{prop:RangeERW}, for every $\Hat{\delta} \in (\pi_d, \delta)$ there exists an integer random variable $N_{\Hat{\delta}}$ such that $\PP[|\Rr_m^X| \le \Hat{\delta} m, \, \forall m \ge N_{\Hat{\delta}}] = 1$, hence 
$$\lim_{n \to \infty} \PP[\forall t \in [c,M]: |\Rr_{\lfloor nt \rfloor}^X| \le \Hat \delta \lfloor nt \rfloor] \ge \lim_{n \to \infty} \PP[N_{\Hat{\delta}} \le cn]=1.
$$
Now concerning the first event on the right-hand side of~\eqref{eq: A_t^cM<}, since $J'_n(\delta)  \preceq J_n(\delta)$, we obtain that 
\begin{equation*} 
\PP\Big[ \forall t \in [c,M]: \frac{J'_{\lfloor nt \rfloor}(\delta)}{n^{1/2}} \le 2(\delta t)^{1/2} \Big] \ge
\PP\Big[ \forall t \in [c,M]: \frac{J_{\lfloor nt \rfloor}(\delta)}{n^{1/2}} \le 2(\delta t)^{1/2} \Big] \, ,
\end{equation*}
%
where the right-hand side converges to one by Lemma~\ref{lem: tightaux} part $i)$ (convergence in distribution to a deterministic function implies convergence in probability, see \cite[page 27]{billingsley1999probability}).
%it holds that \com{notation of convergence of processes}
%\begin{equation}\label{eq:Jninprob}
%\frac{J_{\lfloor n \cdot \rfloor}}{n^{1/2}} \to 2(\delta \cdot)^{1/2}  \text{ as } n \to \infty  \,, 
%\end{equation}
%in probability, since it converges in distribution to a deterministic function in $C_{\mathbb{R}}[0,\infty)$ (see \cite[page 27]{billingsley1999probability}).
%
Hence on the right-hand side of~\eqref{eq: A_t^cM<} we have an intersection of two events whose probability converges to 1 as $n$ goes to infinity. Thus, for every $M > c >0$ and $\delta \in (\pi_d, 1]$ 
\begin{equation*}
\lim_{n\to \infty}  \PP[A_n^{\delta, c,M}]  = 1\,.
%\PP \Big[ \forall t \in [c, M]: \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \le 2(\Hat{\delta} t)^{1/2} \Big] \to 1 \text{ as } n \to \infty \,,   
\end{equation*}
%
%Since  $\hat{\delta} > \delta$ is arbitrary \com{I ?????}, we  obtain that 
%\begin{equation*}
%\PP \Big[ \forall t \in [c, M]: \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \le 2(\delta t)^{1/2}  \Big] \to 1 \text{ as } n \to \infty\,,   
%\end{equation*}
%
Now suppose that we have monotone decreasing and increasing  sequences $\{c_j\}_{j \ge 1}$ and $\{M_j\}_{j \ge 1}$ respectively, such that  $c_j \to 0$ and $M_j \to \infty$ as $j$ goes to infinity. Let $\{
\mathcal{H}_t\}_{t\ge 0}$ be a limit point in distribution of a subsequence of $\{\Hat{K}^n_{\cdot}/n^{1/2}\}_{n\ge 1}$, which is  tight  by  
Lemma~\ref{Jntight} (item $i)$), and define
\begin{equation*}
A^{\delta}:= \left\{\forall t \in [0, \infty): \mathcal{H}_t \le 2(\delta t)^{1/2}  \right\} \,,   
\end{equation*}
and 
$$
A^{\delta, c_i, M_i} = \left\{\forall t \in [c_i, M_i]: \mathcal{H}_t \le 2(\delta t)^{1/2}  \right\}\,,
$$
and $A^{\delta} = \cap_{i=1}^{\infty} A^{ \delta ,c_i, M_i}$ (since $H_0=0$).
%
Then,  by Portmanteau Theorem we have that for every $i \geq 1$ %\com{on the rhs we should specify the dependence on $\delta$ and add that for all $\delta \in (\pi_d,1]$...}
\begin{equation*}
\PP[A^{\delta ,c_i, M_i}] \ge \limsup_{n \to \infty} \PP[A_n^{\delta, c_i, M_i}] =1 \,.    
\end{equation*}
%
Hence, for all $i\geq 1$ we obtain that $\PP[A^{\delta, c_i, M_i}] = 1$ which, in turn, implies that 
\begin{equation}\label{eq:At_1}
\PP[A^{\delta}] = \PP\Big[ \bigcap_{j=1}^{\infty} A^{\delta, c_j, M_j}\Big] = 1 \,. 
\end{equation}
%
We now prove $(b)$. Analogously to the beginning of the proof of $(a)$, it is enough to prove that 
\begin{equation}\label{oldstatement}
\PP \left[\forall t \in [0,\infty): \mathcal{H}_t \ge  2t^{1/2}(1-(1 -  \delta')^{1/2})  \right] = 1 \, ,  
\end{equation}
for every  $\delta' \in (0, \pi_{d})$.
Indeed, if $\delta'_n \uparrow \pi_{d}$  as $n\to \infty$, we have that 
$$
\Big\{ \forall t \in [0,\infty): \mathcal{H}_t \ge 2t^{1/2}(1-(1 -  \pi_{d})^{1/2})  \Big\}\,,
$$
is equal to
$$
\bigcap_{n\ge 1} \Big\{ \forall t \in [0,\infty): \mathcal{H}_t \ge 2t^{1/2}(1-(1 - \delta'_n)^{1/2})  \Big\}\, .
$$
 Moreover, defining
\begin{align*}
V'_n(\delta') :=\sum_{i = 1}^{\delta' n} \um_{\{U_{\varphi_i} \leq (\varphi_i \wedge (n-i) )^{-1/2} \}}\,, %\label{eq: F'n_domsto}
\end{align*}
and using again Lemma \ref{lem: iid} and the i.i.d. property of $\{U_i\}_{i \ge 1}$, we have that 
\begin{equation*}
V'_n(\delta')  \succeq  V_n(\delta')\,, 
\end{equation*}for all $\delta' \in (0, \pi_{d})$
where $V_n(\delta')$ is defined in \eqref{Fn'}. 


We also define the following events 
\begin{equation*}
\begin{split}
& B_n^{\delta',c, M}:= \Big\{\forall t \in [c, M]: \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \ge 2t^{1/2}(1-(1-\delta')^{1/2}) \Big\} \,,
\\
& H_n^{\delta',c,M}:= \Big\{\forall t \in [c, M]: \frac{V'_{\lfloor nt \rfloor}(\delta')}{n^{1/2}} \ge 2t^{\frac{1}{2}}(1-(1-\delta')^{\frac{1}{2}})  \Big\}\,,
% \\
% & R_{\lfloor nt \rfloor}:= \big\{ \forall t \in [c,M]: |\Rr_{\lfloor nt \rfloor}^X| \ge \delta'' \lfloor nt \rfloor \big\} \,,
\end{split}
\end{equation*}
where $c$, $M$ and  $\delta'$  are positive constants such that $M > c$ and $\delta'\in (0,\pi_{d})$. 
%
Given $\delta'' \in (\delta', \pi_{d})$,  we have that
\begin{equation}\label{eq: B_t^cM>}
\begin{split}
\PP[B_n^{\delta',c,M}]  & \ge \PP\big[B_n^{\delta',c,M} \cap \big\{ \forall t \in [c,M]: |\Rr_{\lfloor nt \rfloor}^X| \ge \delta'' \lfloor nt \rfloor \big\}\big] 
\\
&\ge \PP\big[ H_n^{\delta',c,M} \cap \big\{ \forall t \in [c,M]: |\Rr_{\lfloor nt \rfloor}^X| \ge \delta'' \lfloor nt \rfloor \big\} \big]\,.
\end{split}
\end{equation}
%
Since $V'_n(\delta')  \succeq V_n(\delta')$, it holds that 
$$
\PP\big[ H_n^{\delta',c,M} \big]
\ge \PP \Big[ \forall t \in [c, M]: \frac{V_{\lfloor nt \rfloor}(\delta')}{n^{1/2}} \ge 2t^{1/2}(1-(1-\delta')^{1/2}) \Big]\, .
$$
%and by the coupling with the lazy random walk 
%$$
%\PP\big[ \forall t \in [c,M]: |\Rr_{\lfloor nt \rfloor}^X| \ge \delta'' \lfloor nt \rfloor  \big]
% \ge \PP \big[\forall 
% t \in [c,M]:|\Rr_{\lfloor nt \rfloor}^Y| \ge \delta'' \lfloor nt \rfloor\big] \,.    
%$$ 
Now by Lemma~\ref{lem: tightaux} part $ii)$  (convergence in distribution to a deterministic function implies convergence in probability, see \cite[page 27]{billingsley1999probability}) 
we obtain that $\lim_{n \to \infty}\PP\big[ H_n^{\delta',c,M} \big]=1$, for every $\delta' \in (0,\pi_{d})$. Moreover, since $d\geq 22$, we have that $\pi_{d}>0$ and by Proposition~\ref{prop:RangeERW_lower} there exists an integer random variable $N_{\delta''}$ such that $\PP [|\Rr_{m}^X| \ge \delta''m ,\ \forall m \ge N_{\delta''} ] = 1$, thus $\lim_{n\to \infty} \PP \big[\forall 
 t \in [c,M]:|\Rr_{\lfloor nt \rfloor}^X| \ge \delta'' \lfloor nt \rfloor\big] \ge \lim_{n\to \infty} \PP [N_{\delta''} \le cn ] = 1$, for every $\delta'' \in (0,\pi_{d})$.
%we obtain that
%\begin{equation}\label{eq:Fninprob}
%\frac{\sum_{j=1}^{\lfloor n\cdot \rfloor} \um_{\{ U_j \le j^{-1/2} \}} - F_{\lfloor n\cdot \rfloor}}{n^{\frac{1}{2}}} \to 2(\cdot)^{1/2}(1-(1-\delta')^{1/2}) \text{ as } n \to \infty  \,, 
%\end{equation}
%in probability \com{notation of convergence of processes!}, since it converges in distribution to a  continuous function in $t$ (see \cite[page 27]{billingsley1999probability}).
%
Since in~\eqref{eq: B_t^cM>} we have an intersection of two events whose probability converges to 1 as $n$ goes to infinity, we may conclude that for every $M > c >0$ 
\begin{equation*}
\PP \Big[ \forall t \in [c, M]: \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \ge 2t^{1/2}(1-(1-\delta')^{1/2}) \Big] \xrightarrow[n \to \infty]{} 1 \,.
\end{equation*} 
%
Finally, we finish the proof by the same argument used to obtain~\eqref{eq:At_1}.
\end{proof}
\end{comment}