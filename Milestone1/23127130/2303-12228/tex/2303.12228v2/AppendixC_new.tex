\section{}\label{sec:appendixC}


% \begin{lemma}\label{lem: tight}
% Suppose that the sequences $X_{\cdot}^n$ and $Y_{\cdot}^n$ are tight processes in $C[0, T]$ for a $T > 0$. Then we have that the sequence $ (X_{\cdot}^n + Y_{\cdot}^n)$ is a tight process in $C[0, T]$.
% \end{lemma}

% \begin{proof}
% Let us denote the probability measure $P_n$ on $C[0, T]$ as the distribution of $ (X_{\cdot}^n + Y_{\cdot}^n)$. First we will prove that for each positive $\eta$, there exist an $a$ and an $n_0$ such that
% \begin{equation}\label{eq: ap0}
% P_n[f \in C[0, T]: |f(0)| \ge a] \le \eta \quad \text{for all } n \ge n_0 \,. 
% \end{equation}
% We denote the set $G_0 (a):=\{f \in C[0, T]: |f(0)| \ge a\}$. 

% Since $X_{\cdot}^n$ and $Y_{\cdot}^n$ are tight processes in $C[0, T]$, by Theorem 7.3 in~\cite{billingsley1999probability} there exist $a_x$, $a_y$, $n_0^x$ and $n_0^y$ such that
% \begin{equation}\label{eq: ap1}
% \begin{split}
% & \PP[|X_0^n| \ge a_x] \le \frac{\eta}{2} \quad \text{for all } n \ge n_0^x \quad \text{and}
% \\
% & \PP[|Y_0^n| \ge a_y] \le \frac{\eta}{2} \quad \text{for all } n \ge n_0^y \,.
% \end{split}    
% \end{equation}

% Now let us choose $a \ge a_x + a_y$. By triangle inequality and union bound  we have
% \begin{equation}\label{eq: ap2}
% \begin{split}
% P_n & [G_0(a)]  = \PP[|X_0^n + Y_0^n| \ge a] \le \PP[|X_0^n| + |Y_0^n| \ge a] 
% \\
% & \le \PP[\{|X_0^n| \ge a_x\} \cup \{|Y_0^n| \ge a_y\}] \le \PP[|X_0^n| \ge a_x] + \PP[|Y_0^n| \ge a_y] \,.
% \end{split}    
% \end{equation}

% Thus for a $n_0 = \max \{n_0^x, n_0^y\}$, by~\eqref{eq: ap1}, ~\eqref{eq: ap2}, we obtain~\eqref{eq: ap0}.

% We set $\omega_f (\delta)$ as the \textit{modulus of continuity} of an arbitrary function $f(\cdot)$ on $[0, T]$. Now we shall prove that for each positive $\varepsilon$ and $\eta$, there exist a $\delta \in (0, 1)$ and an $m_0$ such that
% \begin{equation}\label{eq: ap31}
% P_n[f \in C[0,T]: \omega_f (\delta) \ge \varepsilon] \le \eta \quad \text{for all } n \ge m_0 \,.    
% \end{equation}
% We denote the set $H_t(\varepsilon, \delta):= \{f \in C[0,T]: \omega_f (\delta) \ge \varepsilon \}$.


% Since $X_{\cdot}^n$ and $Y_{\cdot}^n$ are tight processes in $C[0, T]$, by Theorem 7.3 in~\cite{billingsley1999probability} there exist $\delta_x \in (0,1)$, $\delta_y \in (0,1)$, $m_0^x$ and $m_0^y$ such that
% \begin{equation}\label{eq: ap3}
% \begin{split}
% & \PP\left[\sup_{|s-t| \ge \delta_x } |X_s^n - X_t^n| \ge \frac{\varepsilon}{2} \right] \le \frac{\eta}{2} \quad \text{for all } n \ge m_0^x \quad \text{and}
% \\
% & \PP\left[\sup_{|s-t| \ge \delta_y } |Y_s^n - Y_t^n| \ge \frac{\varepsilon}{2}\right] \le \frac{\eta}{2} \quad \text{for all } n \ge m_0^y \,.
% \end{split}    
% \end{equation}

% Then we obtain by triangle inequality and union bound the following
% \begin{equation}\label{eq: ap4}
% \begin{split}
% P_n[H_t(\varepsilon,\delta)] & = \PP\left[\sup_{|s-t| \ge \delta} |X_s^n + Y_s^n - X_t^n -Y_t^n| \ge \varepsilon \right]
% \\
% & \le \PP\left[ \sup_{|s-t|\le \delta}|X_s^n - X_t^n| + \sup_{|s-t|\le \delta}|Y_s^n - Y_t^n| \ge \varepsilon \right]
% \\
% & \le \PP\left[ \left\{ \sup_{|s-t|\le \delta}|X_s^n - X_t^n| \le \frac{\varepsilon}{2} \right\} \bigcup  \left\{\sup_{|s-t|\le \delta}|Y_s^n - Y_t^n| \le \frac{\varepsilon}{2} \right\} \right]
% \\
% & \le \PP\left[ \sup_{|s-t|\le \delta}|X_s^n - X_t^n| \le \frac{\varepsilon}{2} \right] + \PP\left[\sup_{|s-t|\le \delta}|Y_s^n - Y_t^n| \le \frac{\varepsilon}{2} \right] \,.
% \end{split}    
% \end{equation}


% We choose now a $\delta= \min\{\delta_x, \delta_y\}$ and $m_0 = \max\{m_0^x , m_0^y \}$, thus by~\eqref{eq: ap3} and~\eqref{eq: ap4} we obtain~\eqref{eq: ap31}. Hence by Theorem 7.3 in~\cite{billingsley1999probability} we finish the proof.


 
% %Let us denote $A_X$, $A_Y$ and $A_{X+Y}$ as sets in $C[0,T]$ which contains the processes $X_{\cdot}^n$, $Y_{\cdot}^n$ and $(X_{\cdot}^n + Y_{\cdot}^n)$ respectively for all $n \ge 0$. 

% %Since the processes $X_{\cdot}^n$ and $Y_{\cdot}^n$ are tight by Prohorov's Theorem (see Theorem 5.1 in~\cite{billingsley1999probability}) the sets $A_X$ and $A_Y$ are relatively compacts in $C[0, T]$.

% %If we prove the set $A_{X+Y}$ is relatively compact in $C[0, T]$ by Prohorov's Theorem (see Theorem 5.2 in~\cite{billingsley1999probability}) we have the desired result. For that we will use the Arzelà-Ascoli Theorem (see Theorem 7.2 in~\cite{billingsley1999probability}).

% %By triangle inequality we have
% %\begin{equation}\label{eq: A_x+y}
% %\sup_{f \in A_{X+Y}} |f(0)| \le \sup_{f \in A_{X}} |f(0)| + \sup_{f \in A_{Y}} |f(0)| < \infty \,.  
% %\end{equation}
% %Since $A_X$ and $A_Y$ are relative compact in $C[0, T]$ we have the last inequality in~\eqref{eq: A_x+y}.

% %Let us denote $\omega_f (\delta)$ as the \textit{modulus of continuity} of an arbitrary function $f(\cdot)$ on $[0, T]$. Now one can see that for all $n \ge 0$
% %\begin{equation}\label{eq: modcon}
% %\begin{split}
% %\omega_{X_{\cdot}^n + Y_{\cdot}^n}(\delta) & = \sup_{|s-t|\le \delta} |X_s^n + Y_s^n - X_t^n -Y_t^n| 
% %\\
% %& = \sup_{|s-t|\le \delta} |(X_s^n - X_t^n) + (Y_s^n -Y_t^n)|
% %\\
% %& \le \sup_{|s-t|\le \delta}(|X_s^n - X_t^n|) + \sup_{|s-t|\le \delta}(|Y_s^n - Y_t^n|)
% %\\
% %& \le \omega_{X_{\cdot}^n}(\delta) + \omega_{ Y_{\cdot}^n}(\delta) \,.
% %\end{split}    
% %\end{equation}
% %The second inequality in~\eqref{eq: modcon} we apply triangle inequality.

% %By~\eqref{eq: modcon} and the fact that $A_X$ and $A_Y$ are relatively compacts in $C[0, T]$ we obtain
% %\begin{equation}\label{eq: A_x+y2}
% %\lim_{\delta \to 0} \sup_{f \in A_{X+Y}} \omega_f (\delta) \le \lim_{\delta \to 0} \left(\sup_{f \in A_X}\omega_f (\delta) + \sup_{f \in A_Y}\omega_f (\delta)\right) = 0 \,.     
% %\end{equation}

% %Since we have~\eqref{eq: A_x+y} and~\eqref{eq: A_x+y2} by the Arzelà-Ascoli Theorem (see Theorem 7.2 in~\cite{billingsley1999probability}), $A_{X+Y}$ is relatively compact in $C[0,T]$ and we finish the proof.
% \end{proof}

% \begin{lemma}\label{lem: convprob}
% Suppose that we have a process $Y_{\lfloor n \cdot \rfloor}$ that converges in probability to zero in the space $C_{\Rs^d}[0, T]$ with the uniform metric for all $T >0$. Then $Y_{\lfloor n \cdot \rfloor}$ converges in probability to zero in the space $C_{\Rs^d}[0, \infty)$ equipped with the following metric
% \begin{equation*}
%  \rho(f, g) := \sum_{k=1}^{\infty}\frac{1}{2^k} \sup_{0 \le t \le k}(||f(t) - g(t)|| \wedge 1) \,.   
% \end{equation*}
% \end{lemma}

% \begin{proof}
% Since $Y_{\lfloor n \cdot \rfloor}$ converges in probability to zero in $C_{\Rs^d}[0, T]$, we have
% \begin{equation}\label{eq: convprob}
%     \PP\left[ \sup_{0 \le t \le T}|| Y_{\lfloor nt \rfloor}|| > \delta \right] \to 0 \quad \text{as } n \to \infty \,,
% \end{equation}
% for any $\delta > 0$. 

% Now let $\varepsilon > 0$ and we choose a positive integer $N$ such that $\sum_{l \ge N} 2^{-l} < \varepsilon/2$. Thus we have the following
% \begin{equation}\label{eq: convprob2}
% \begin{split}
% & \PP\left[ \sum_{k=1}^{\infty} 2^{-k} \sup_{0 \le t \le k}(||Y_{\lfloor nt \rfloor}|| \wedge 1) > \varepsilon \right] =  
% \\
% & = \PP\left[ \sum_{k=1}^{N} 2^{-k} \sup_{0 \le t \le k}(||Y_{\lfloor nt \rfloor}|| \wedge 1) + \sum_{k=N+1}^{\infty} 2^{-k} \sup_{0 \le t \le k}(||Y_{\lfloor nt \rfloor}|| \wedge 1) > \varepsilon \right]
% \\
% & \le \PP\left[ \sum_{k=1}^{N} \sup_{0 \le t \le k}||Y_{\lfloor nt \rfloor}|| + \sum_{k=N+1}^{\infty} 2^{-k}  > \varepsilon \right] \,.
% \end{split} 
% \end{equation}

% Since we choose $N$ large enough by~\eqref{eq: convprob2} we obtain that
% \begin{equation}\label{eq: convprob3}
% \begin{split}
% & \PP\left[ \sum_{k=1}^{\infty}  \sup_{0 \le t \le k}(||Y_{\lfloor nt \rfloor}|| \wedge 1) > \varepsilon \right] 
% \\
% & \le  \PP\left[ \left\{\sum_{k=1}^{N}  \sup_{0 \le t \le k}||Y_{\lfloor nt \rfloor}|| > \frac{\varepsilon}{2} \right\} \bigcup \left\{ \sum_{k=N+1}^{\infty} 2^{-k}  > \frac{\varepsilon}{2} \right\}\right] 
% \\
% & \le \PP\left[ \sum_{k=1}^{N}  \sup_{0 \le t \le k}||Y_{\lfloor nt \rfloor}|| > \frac{\varepsilon}{2} \right] + \PP\left[ \sum_{k=N+1}^{\infty} 2^{-k}  > \frac{\varepsilon}{2} \right] 
% \\
% & \le \PP\left[ \bigcup_{k=1}^{N}  \sup_{0 \le t \le k}||Y_{\lfloor nt \rfloor}|| > \frac{\varepsilon}{2N} \right] \le  \sum_{k=1}^{N} \PP\left[ \sup_{0 \le t \le k}||Y_{\lfloor nt \rfloor}|| > \frac{\varepsilon}{2N} \right] \,.
% \end{split}    
% \end{equation}
% We have the third and last inequalities in~\eqref{eq: convprob3} by union bound. 

% Now one can see that all sum portions in the last inequality in~\eqref{eq: convprob3} go to zero as $n$ tends to infinity by~\eqref{eq: convprob}, ergo we have
% \begin{equation*}
% \PP\left[ \sum_{k=1}^{\infty} 2^{-k} \sup_{0 \le t \le k}(||Y_{\lfloor nt \rfloor}|| \wedge 1) > \varepsilon \right] \to 0 \quad \text{as } n \to \infty \,,    
% \end{equation*}
% for any $\varepsilon > 0$. Hence we obtain the desired result.
% \end{proof}

\begin{lemma}\label{lem: iid}
Let $\{\phi_n\}_{n \ge 1}$ be a sequence of i.i.d. random vectors on $\ZZ^d$, with $d \ge 2$ and $\{\kappa_n\}_{n \ge 1}$ an increasing sequence of $\FF_n$-predictable times defined on 
a probability space $(\Omega, \FF, \PP)$, where $\{\FF_n\}_{n\ge 1}$ is the filtration $\FF_0 = \{\emptyset, \Omega\}$, $\FF_n=\sigma(\phi_1, \dots, \phi_n)$, $n\ge 1$ and $\kappa_n < \infty$ for all $n$. Then we have that $\{\phi_{\kappa_n}\}_{n \ge 1}$ is i.i.d. and moreover $\phi_{\kappa_n}$ has the same distribution of $\phi_1$ for every $n \ge 1$. 
\end{lemma}
\begin{proof}
We begin showing that for any $n \ge 1$, $\phi_{\kappa_n}$ has the same distribution of $\phi_1$.
%
Let $A$ be a subset of   $\ZZ^d$ and fix a $j \ge 1$. Then we have that
\begin{equation}\label{eq: iid1}
\begin{split}
& \PP[\phi_{\kappa_j} \in A | \FF_{\kappa_{j}-1}] = \sum_{n=1}^{\infty} \PP[\{ \phi_{\kappa_j} \in A \} \cap \{\kappa_j = n\}| \FF_{\kappa_{j}-1}]
\\
& = \sum_{n=1}^{\infty} \PP[\phi_{\kappa_j} \in A |\{\kappa_j = n\}, \FF_{\kappa_{j}-1}] \PP[\kappa_j = n| \FF_{\kappa_{j}-1}] \\
& = \sum_{n=1}^{\infty} \PP[\phi_n \in A ]\PP[\kappa_j = n| \FF_{\kappa_{j}-1}] = \PP[\phi_1 \in A ] \underbrace{\sum_{n=1}^{\infty} \PP[\kappa_j = n| \FF_{\kappa_{j}-1}]}_{=1}\,.
\end{split}    
\end{equation}
The third equality in~\eqref{eq: iid1} follows from  the fact that $\{\kappa_n\}_{n \ge 1}$ is an increasing sequence of $\FF_n$-predictable times, $\{\phi_n\}_{n \ge 1}$ is i.i.d. and so $\phi_n$ is independent of $\FF_{n-1}$ for all $n \ge 1$.
%
It remains to prove independence. We will only prove pairwise independence, but it is straightforward to generalize the proof by induction and we leave the details to the reader.  For $B$ and $D$ subsets of $\ZZ^d$ and $j, i \in \mathbb{N}$ such that $j > i$ we have that 
\begin{equation}
\label{eq: iid2}
\begin{split}
\PP[&\{\phi_{\kappa_i} \in B\} \cap \{\phi_{\kappa_j} \in D\}] = 
\\ 
&= \sum_{n=1}^{\infty} \sum_{m > n} \PP[\{\{\phi_{\kappa_i} \in B\} \cap \{\kappa_i = n\}\} \cap \{\{\phi_{\kappa_j} \in D\} \cap \{\kappa_j = m\}\}] 
\\
&= \sum_{n=1}^{\infty} \sum_{m > n} \PP[\phi_{\kappa_j} \in D | \kappa_i = n, \kappa_j = m, \phi_{n} \in B]\PP[\kappa_i = n, \kappa_j = m, \phi_{\kappa_i} \in B] 
\\
&= \sum_{n=1}^{\infty} \sum_{m > n} \PP[\phi_m \in D]\PP[\kappa_i = n, \kappa_j = m, \phi_{\kappa_i} \in B]\,.
\end{split}
\end{equation}
The last equality in~\eqref{eq: iid2} follows from the fact that the event $\{\kappa_i = n, \kappa_j = m, \phi_{n} \in B\}$, for $m>n$, is $\FF_{m-1}$ measurable, since $\{\kappa_n\}_{n \ge 1}$ is an increasing sequence of $\FF_n$-predictable times.

Since the sequence $\{ \phi_n\}_{n \ge 1}$ is i.i.d., we can continue the computation in~\eqref{eq: iid2} and obtain that
\begin{equation*}
\begin{split}
\PP[\{\phi_{\kappa_i} \in B\} \cap \{\phi_{\kappa_j} \in D\}] & = \PP[\phi_1 \in D] \sum_{n=1}^{\infty} \sum_{m \ge n} \PP[\kappa_j = m,\kappa_i = n, \phi_{\kappa_i} \in B]
\\
& = \PP[\phi_1 \in D] \sum_{n=1}^{\infty}  \PP[\kappa_i = n, \phi_{\kappa_i} \in B] 
\\
& = \PP[\phi_1 \in D] \sum_{n=1}^{\infty} \PP[\phi_{n} \in B]\PP[\kappa_i = n]
\\
& = \PP[\phi_1 \in D] \PP[\phi_1 \in B] = \PP[\phi_{\kappa_i} \in D]\PP[\phi_{\kappa_j} \in B]\,,
\end{split}    
\end{equation*}
concluding the proof.
\end{proof}




Now we present the proof of Lemma~\ref{lemma_1}.
\begin{proof}[Proof of Lemma~\ref{lemma_1}] 
For simplicity, we denote $b_k = \lfloor k^{\delta}\rfloor$. From the definition of the set $A_{k, \delta, \beta}$ in \eqref{eq:set_A} we have that  there exists a positive constant $c_1$ such that
\begin{equation}\label{eq:n1}
\mathbb{P} (A_{k, \delta, \beta}^c) \leq 1 - \Big(1 - \frac{1}{\sqrt{k}}\Big)^{b_k} \le c_1 \frac{b_k}{\sqrt{k}} \,.
\end{equation}
Hence,  $\lim_{n\to\infty}\frac{1}{n}\sum_{k = 1}^{n}\mathbb{P}(A_{k,\delta, \beta}^c) = 0$ for all $\delta \in (0,1/2)$ and all $\beta \geq 1/2$.
%
Then, to prove the result it is enough to show that 
\begin{equation*}%\label{eq_1}
\lim_{n\to\infty} \frac{1}{n} \sum_{k = 1}^n \Big(\um_{A_{k, \delta, \beta}^c} - \mathbb{P}(A_{k, \delta, \beta}^c)\Big) = 0, \, \text{ a.s..}
\end{equation*}
By Borel-Cantelli's Lemma the latter holds if can show that 
\begin{equation}\label{eq_2}
\sum_{n\ge 1}\mathbb{E}\left[\left(\frac{1}{n}\sum_{k = 1}^n\Big(\um_{A_{k, \delta,\beta}^c} - \mathbb{P}\left(A_{k, \delta, \beta}^c\right)\Big)\right)^2\right] < \infty \,,
\end{equation}
holds for any $\delta \in (0,1/2)$ and $\beta \geq 1/2$. 
%
To avoid clutter, henceforth we will denote $A_k := A_{k, \delta, \beta}$.  Note that
\begin{equation}\label{eq:divide}
\begin{split}
\mathbb{E}\Big[\Big(\frac{1}{n}\sum_{k = 1}^{n}[\um_{A_k^c} - \mathbb{P}(A_k^c)]\Big)^2\Big] &=
 \frac{1}{n^2}\sum_{k=1}^{n}\mathbb{E}\left[(\um_{A_k^c} - \mathbb{P}(A_k^c))^2\right]
 \\
+& \frac{2}{n^2}\sum_{k = 1}^{n - 1}\sum_{m = k + 1}^{n}\mathbb{E}\left[(\um_{A_k^c} - \mathbb{P}(A_k^c))(\um_{A_m^c} - \mathbb{P}(A_m^c))\right] \,.
\end{split}
\end{equation}
For the first on the RHS of \eqref{eq:divide}, it holds that 
\begin{equation}\label{eq:n2}
\begin{split}
& \frac{1}{n^2}\sum_{k=1}^{n}\mathbb{E}\left[\left(\um_{A_k^c} - \mathbb{P}(A_k^c)\right)^2\right]  \le \frac{1}{n^2}\sum_{k = 1}^{n}\mathbb{P}(A_k^c) 
% =\frac{1}{n^2}\sum_{k = 1}^n\left[1 - \left(1 - \frac{1}{\sqrt{k}}\right)^{b_k}\right] 
% \\
% & 
\le \frac{c_1}{n^2}\sum_{k = 1}^n\frac{k^{\delta}}{\sqrt{k}} \le \frac{c_2}{n^{1 + 1/2 - \delta}}\,,
\end{split}
\end{equation}
where,  $c_2$ is a positive constant and in the second inequality we used~\eqref{eq:n1}. 
%
Since $\delta \in (0, 1/2)$, by~\eqref{eq:n2} we obtain
\begin{equation*}
\frac{1}{n^2}\sum_{n = 1}^{\infty} \sum_{k=1}^{n}\mathbb{E}\left[\left(\um_{A_k^c} - \mathbb{P}(A_k^c)\right)^2\right] < \infty\,.
\end{equation*}

For the second sum on the RHS of \eqref{eq:divide}, since the $\{U_i\}_{i \ge 1}$ are independent,   we have that  $\mathbb{E}[(\um_{A_k^c} - \mathbb{P}(A_k^c))(\um_{A_m^c} - \mathbb{P}(A_m^c))] = 0$ for $m > k + b_k$. Therefore,  we obtain that 
\begin{align*}
& \frac{1}{n^2}\sum_{k = 1}^{n - 1}\sum_{m = k + 1}^{n}\mathbb{E}\left[\left(\um_{A_k^c} - \mathbb{P}(A_k^c)\right)\left(\um_{A_m^c} - \mathbb{P}(A_m^c)\right)\right] = 
\\
& \frac{1}{n^2}\sum_{k = 1}^{n - 1}\;\;\sum_{m = k + 1}^{n\wedge(k + b_k)}\Big(\mathbb{P}\left(A_k^c\cap A_m^c\right) - \mathbb{P}(A_k^c)\mathbb{P}(A_m^c)\Big) \,.
\end{align*}

Recalling the definition of $A_k$ in \eqref{eq:set_A}, for $m\leq k +b_k$ we have that 
% \begin{align*}
% A_k\cap A_m^c & = \left(\bigcap_{n = 1}^{b_k}\left\{U_{n + k} > \frac{1}{\sqrt{k}}\right\}\right)\bigcap\left(\bigcup_{l = 1}^{b_m}\left\{U_{l + m} \le \frac{1}{\sqrt{m}}\right\}\right)
% \\
% & = \left(\bigcap_{n = 1}^{b_k}\left\{U_{n + k} > \frac{1}{\sqrt{k}}\right\}\right)\bigcap\left(\bigcup_{l =  b_k + k - m + 1}^{b_m}\left\{U_{l + m} \le \frac{1}{\sqrt{m}}\right\}\right)\,.
% \end{align*}
%
\begin{align*}
A_k\cap A_m^c & = \bigcap_{n = 0}^{b_k}\left\{U_{n + k} > \frac{1}{(k+n)^{\beta/2}}\right\}\cap \;\bigcup_{l = 0}^{b_m}\left\{U_{l + m} \le \frac{1}{(m+l)^{\beta/2}}\right\}
\\
& = \bigcap_{n = 0}^{b_k}\left\{U_{n + k} > \frac{1}{(k+n)^{\beta/2}}\right\}\cap \;\bigcup_{l = b_k + k - m + 1}^{b_m}\left\{U_{l + m} \le \frac{1}{(m+l)^{\beta/2}}\right\}\,.
\end{align*}
%
Setting  $E_{k,m, \beta} := \displaystyle\bigcup_{l =  b_k + k - m + 1}^{b_m}\left\{U_{l + m} \le \frac{1}{(m+l)^{\beta/2}}\right\}$, and noticing that $$E_{k,m, \beta} \subseteq A_{m}^c \, ,$$ together with the fact that 
\begin{equation*}
\mathbb{P}\left(A_k^c\cap A_m^c\right) = \mathbb{P}\left(A_m^c\right) - \mathbb{P}\left(A_k\cap A_m^c\right) = \mathbb{P}\left(A_m^c\right) - \mathbb{P}\left(A_k\right)\mathbb{P}\left(E_{k,m, \beta}\right)\,,
\end{equation*}
we obtain that 
\begin{align*}
& \mathbb{P}\left(A_k^c\cap A_m^c\right) - \mathbb{P}\left(A_k^c\right)\mathbb{P}\left(A_m^c\right)\\
&= \mathbb{P}\left(A_m^c\right) - \mathbb{P}\left(A_k\right)\mathbb{P}\left(E_{k,m,\beta}\right) - \mathbb{P}\left(A_k^c\right)\mathbb{P}\left(A_m^c\right) 
\\
& 
% = \mathbb{P}(A_m^c)\mathbb{P}\left(A_k\right) - \mathbb{P}\left(A_k\right)\mathbb{P}\left(E_{k,m}\right)
= \mathbb{P}\left(A_k\right)\left[\mathbb{P}\left(A_m^c\right) - \mathbb{P}\left(E_{k,m,\beta}\right)\right] 
= \mathbb{P}\left(A_k\right)\mathbb{P}\left(A_m^c\setminus E_{k,m,\beta}\right)
\\
& = \mathbb{P}\left(A_k\right) \mathbb{P}\Big(\bigcup_{l = 0}^{b_k + k - m}\Big\{U_{l + m} \le \frac{1}{(m+l)^{\beta/2}}\Big\}\Big)
\\
& \leq \mathbb{P}\left(A_k\right)\Big[ 1 - \left(1 - \frac{1}{\sqrt{m}}\right)^{b_k + k - m} \Big] \le 1 - \left(1 - \frac{1}{\sqrt{m}}\right)^{b_k + k - m} \,.
\end{align*}

Since $x \ge 1 - e^{-x}$ for all $x \ge 0$, the RHS above can be bounded  by \begin{align*}
 1 - \left(1 - \frac{1}{\sqrt{m}}\right)^{b_k + k - m} &= 1 - e^{(b_m + k - m)\log(1 - \frac{1}{\sqrt{m}})}
\\
& \le - (b_m + k - m)\log(1 - \frac{1}{\sqrt{m}}) \\
&=-\frac{b_m + k - m}{\sqrt{m}}\log(1 - \frac{1}{\sqrt{m}})^{\sqrt{m}}
\\
& \le c_3 \frac{b_m + k - m}{\sqrt{m}}\,, 
\end{align*}
for some constant $c_3 > 0$. Then
\begin{align*}
& \sum_{m = k + 1}^{n\wedge(k + b_k)}\Big(\mathbb{P}\left(A_k^c\cap A_m^c\right) - \mathbb{P}(A_k^c)\mathbb{P}(A_m^c)\Big) \le c_3 \sum_{m = k + 1}^{k + b_k}\frac{b_k + k - m}{\sqrt{m}} 
\\
& =  c_3 \sum_{m =1}^{b_k}\frac{b_k - m}{\sqrt{m + k}} 
=  \frac{c_3}{\sqrt{b_k}}\sum_{m =1}^{b_k}\frac{1 - \frac{m}{b_k}}{\sqrt{\frac{m}{b_k} + \frac{k}{b_k}}}\frac{1}{b_k} \le  \frac{c_3}{\sqrt{b_k}}\sum_{m =1}^{b_k}\frac{1 - \frac{m}{b_k}}{\sqrt{\frac{m}{b_k} + 1}}\frac{1}{b_k}\,.
\end{align*}
Using that 
\begin{equation*}
\lim_{k\to\infty}\sum_{m =1}^{b_k}\frac{1 - \frac{m}{b_k}}{\sqrt{\frac{m}{b_k} + 1}}\frac{1}{b_k} = \int_{0}^{1}\frac{1 - x}{\sqrt{1 + x}}dx\,, 
\end{equation*}
we conclude that  there exists some positive constant $c_4$ such that 
\begin{equation*}
\sum_{m = k + 1}^{n\wedge(k + b_k)}\Big(\mathbb{P}\left(A_k^c\cap A_m^c\right) - \mathbb{P}(A_k^c)\mathbb{P}(A_m^c)\Big) \le c_4\frac{1}{\sqrt{b_k}}\,,
\end{equation*}
which, in turn,  implies that 
\begin{equation*}
\begin{split}
& \sum_{n = 1}^{\infty}\frac{1}{n^2}\sum_{k = 1}^{n - 1}\sum_{m = k + 1}^{n}\mathbb{E}\left[\left(\um_{A_k^c} - \mathbb{P}(A_k^c)\right)\left(\um_{A_m^c} - \mathbb{P}(A_m^c)\right)\right]
\\
&  \le c_4 \sum_{n = 1}^{\infty} \frac{1}{n^2}\sum_{k = 1}^{n}\frac{1}{\sqrt{b_k}}  \le c_5 \sum_{n = 1}^{\infty} \frac{1}{n^{1 + \delta/2}} < \infty \,.
\end{split}
\end{equation*}
\end{proof}

The next result is a general result for a sum of geometric random variables which will be important in the proof of Proposition~\ref{prop:RangeERW_lower}.

\begin{lemma}\label{lem:geo_sum_bound}
Let $\{H_j\}_{j \ge 1}$ be a sequence of independent random variables such that for each $ j \ge 1$, $H_j\sim {\rm Geo} (1/\sqrt{k+j})$. Then, for any integer $\theta\geq 1$ there exists a constant $\tilde{C}_\theta$ depending on $\theta$ only such that for any $m\geq 1$ it holds that
\begin{equation*}
  P\left(\sum_{j=1}^m H_j\leq \frac{m^{3/2}}{6} \right)\leq \frac{\Tilde{C}_\theta}{m^{\theta}}\,.  
\end{equation*}
\end{lemma}


\begin{proof}
Let us being noticing that for any $m,k\geq 1$ it holds that 
\[
\mathbb{E}\Big[\sum_{j=1}^m H_j\Big]\geq \int_{0}^m (k+x)^{1/2}dx\geq \frac{2}{3}m(k+m)^{1/2}\,,
\]
which ensures that $\mathbb{E}\Big[\sum_{j=1}^m H_j\Big] - \frac{m(m+k)^{1/2}}{2}\geq \frac{m(m+k)^{1/2}}{6} $. 
Therefore, we have that 
% $ Y_j - \mathbb{E}[\sum_{j = 1}^m Y_j]$ and $X_{j} = Y_j - \mathbb{E}[Y_j]$. 
% Then we have that the sequence $(X_{j})_{j \ge 1}$ is independent and $\mathbb{E}[X_{j}] = 0$. 
%
\begin{align*}
\mathbb{P}\left( \sum_{j=1}^m H_j < \frac{m^{3/2}}{6} \right) &\le \mathbb{P}\left( \sum_{j=1}^m H_j < \frac{m(m+k)^{1/2}}{6} \right)
\\
&\leq \mathbb{P}\left( \sum_{j=1}^m H_j < \mathbb{E}\Big[\sum_{j=1}^m H_j\Big] -\frac{m(m+k)^{1/2}}{2} \right)\\
&\leq 
\mathbb{P}\left( \Big| \sum_{j=1}^m X_j \Big| > \frac{ m \sqrt{k+m} }{2} \right) \,,   
\end{align*}
where $X_{j} = H_j - \mathbb{E}[H_j]$. Then, for  $\theta\geq 1$  a positive integer we have that 
\begin{equation}\label{eq:P(calG)<}
\begin{split}   
% & \mathbb{P}\left( \sum_{j=1}^m Y_j < \frac{m^{3/2}}{6} \right) \le
\mathbb{P}\left( \Big| \sum_{j=1}^m X_j \Big| > \frac{ m \sqrt{k+m} }{2} \right) 
 \leq 
 % \frac{C}{m^{2\theta} (k+m)^{\theta}} \mathbb{E}[H_{m,k}^{2 \theta}] 
 % = 
 \frac{4^\theta}{m^{2 \theta} (k+m)^{\theta}} \mathbb{E}\Big[ \Big( \sum_{j = 1}^m X_{j} \Big)^{2 \theta} \Big] %= \frac{C}{i^6 (k+i)^3} \mathbb{E} \Big[ \sum_{j,h,l,s,n,m = 1}^i X_{j,k} X_{h,l} X_{l,k} X_{s,k} X_{n,k} X_{m,k} \Big]
 \,. 
\end{split}
\end{equation}
Since $\{X_j\}_{j\geq 1}$ is a sequence of independent random variables of mean value zero, when expanding $\mathbb{E}\Big[ \Big( \sum_{j = 1}^m X_{j} \Big)^{2 \theta} \Big]$ the only terms that do not vanish are those of the form $\mathbb{E}[X_{j_1}^{\alpha_1} ... X_{j_r}^{\alpha_r}]$ where $1\leq r\leq m$, $j_1 \neq j_2 \neq \dots \neq j_r$, $\alpha_1 + \alpha_2 + \dots +\alpha_r = 2 \theta$ and $\alpha_1, \dots, \alpha_r$  are integers greater or equal than 2. Note that  we should have $r \le \theta$, since $r > \theta$ implies that $\alpha_i = 1$ for some $i$. 
%%%%
% {\color{cyan} From here we do not try to obtain a sharp upper bound on the number of terms, so we fix $r=\theta$ and we allow $\alpha_1, \dots, \alpha_\theta \ge 0$. Thus we have at most $m (m-1) ... (m-\theta +1) \le m^\theta$ choices for the indexes $j_1,...,j_\theta$ and ${{3 \theta - 1}\choose{\theta -1}}$ ways to choose the integers $\alpha_1, \dots, \alpha_\theta \ge 0$ such that 
% $\alpha_1 + \alpha2 + \dots +\alpha_r = 2 \theta$. Therefore
% $$
% \mathbb{E}\Big[ \Big( \sum_{j = 1}^m X_{j} \Big)^{2 \theta} \Big] \le {{3 \theta - 1}\choose{\theta -1}} m^\theta \max_{j_1\neq ... \neq j_\theta \atop \alpha_1 + ... +\alpha_r = 2\theta} \prod_{i=1}^{\theta} \mathbb{E}[X_{j_i}^{\alpha_i}]
% $$
% Let us recall that for $W$  a random variable with Geometric distribution with parameter $p$ it holds that $\mathbb{E}[W^n] \le n!/p^n$ for all $n \ge 1$ 
% (see, e.g., Example $8f$ in Chapter 4 in [ref Ross] \texttt{check the constant!!!!...}). Using the latter, we obtain that
% \begin{align*}
% \mathbb{E}[|W - \mathbb{E}[W]|^n] &\le \mathbb{E}[(W + \mathbb{E}[W])^n]= \sum_{\ell = 0}^n \binom{n}{\ell}\mathbb{E}[W^\ell]\mathbb{E}[W]^{n-\ell}
% \\
% &\leq  \frac{1}{p^n}\sum_{\ell = 0}^n \binom{n}{\ell}\ell! \leq \frac{n!}{p^n}(1+ e)\,.
% \end{align*}
% Thus 
% $$
% \max_{j_1\neq ... \neq j_\theta \atop \alpha_1 + ... +\alpha_r = 2\theta} \prod_{i=1}^{\theta} \mathbb{E}[X_{j_i}^{\alpha_i}] \le (2\theta)! (1+e)^\theta (k+m)^\theta
% $$
% which implies that
% $$
% \mathbb{E}\Big[ \Big( \sum_{j = 1}^m X_{j} \Big)^{2 \theta} \Big] \le {{3 \theta - 1}\choose{\theta -1}} (2\theta)! (1+e)^\theta m^\theta (k+m)^\theta.
% $$
% Going back to \eqref{eq:P(calG)<}, we obtain that
% $$
% \mathbb{P}\left( \sum_{j=1}^m Y_j < \frac{m^{3/2}}{6} \right) \le \frac{C_\theta}{m^\theta}
% $$
% where $C_\theta$ is a constant depending only on $\theta$. 
% }
%%%%%%%%%%%%%%%%%%%%%%%
Therefore, 
\begin{align}\label{eq:sum_sum}
\begin{split}
&\mathbb{P}\left( \sum_{j=1}^m H_j < \frac{m^{3/2}}{6} \right)\\ & \le  \frac{4^\theta}{m^{2 \theta} (k+m)^{\theta}}
\sum_{r = 1}^{\theta \wedge m}
\sum_{\substack{I\subseteq \{1, \ldots, m\}:\\ |I|=r}}\!\!\sum_{\substack{(n_i)_{i \in I}:\\n_i\geq 2, \forall i \\
\sum_{i \in I}n_i=2\theta}}\!\!\!\frac{(2\theta)! \mathbb{E}\big[\prod_{i \in I}X_i^{n_i}\big]}{\prod_{i \in I}n_i!} \,.
\end{split}
\end{align}


Let us recall that for $W$  a random variable with Geometric distribution with parameter $p$ it holds that $\mathbb{E}[W^n] \le n!/p^n$ for all $n \ge 1$ 
% where $C(n)$ is a positive constant that depends on $n$ only 
(see, e.g., \cite[Example $8f$-Chapter 4]{ross2010}).  Using the latter, we obtain that
\begin{align*}
\mathbb{E}[|W - \mathbb{E}[W]|^n] &\le \mathbb{E}[(W + \mathbb{E}[W])^n]= \sum_{\ell = 0}^n \binom{n}{\ell}\mathbb{E}[W^\ell]\mathbb{E}[W]^{n-\ell}
\\
&\leq  \frac{1}{p^n}\sum_{\ell = 0}^n \binom{n}{\ell}\ell! \leq \frac{n!}{p^n}(1+ e)\,.
\end{align*}
 Using the above bound in ~\eqref{eq:sum_sum} and using the independence of $\{X_{j}\}_{j \ge 1}$ we obtain that 
\begin{equation*}
\begin{split}
\mathbb{P}&\left( \sum_{j=1}^m H_j < \frac{m^{3/2}}{6} \right)\\
&\le  \frac{4^\theta}{m^{2 \theta} (k+m)^{\theta}}
\sum_{r = 1}^{\theta\wedge m} 
\sum_{\substack{I\subseteq \{1, \ldots, m\}:\\ |I|=r}}\sum_{\substack{(n_i)_{i \in I}:\\n_i\geq 2, \forall i \\
\sum_{i \in I}n_i=2\theta}}\!\!\!\frac{(2\theta)!}{\prod_{i \in I}n_i!} \prod_{i \in I}\mathbb{E}\big[X_i^{n_i}\big]
\\
&
\le  \frac{4^\theta}{m^{2 \theta} (k+m)^{\theta}}
\sum_{r = 1}^{\theta\wedge m} 
\sum_{\substack{I\subseteq \{1, \ldots, m\}:\\ |I|=r}}\sum_{\substack{(n_i)_{i \in I}:\\n_i\geq 2, \forall i \\
\sum_{i \in I}n_i=2\theta}}\frac{(2\theta)!}{\prod_{i \in I}n_i!} (k+m)^\theta (1+e)^r \prod_{i \in I}n_i!  
\\
&
\le  \frac{4^\theta}{m^{2 \theta} (k+m)^{\theta}}
\sum_{r = 1}^{\theta\wedge m} 
\sum_{\substack{I\subseteq \{1, \ldots, m\}:\\ |I|=r}}\binom{2\theta - \theta -1}{\theta-1} (2\theta)!  (k+m)^\theta (1+e)^r 
\\
&
\le  \frac{4^\theta}{m^{2 \theta} (k+m)^{\theta}}
\binom{3\theta -1}{\theta-1} (2\theta)!  (k+m)^\theta \sum_{r = 1}^{\theta\wedge m} 
\binom{m}{r}(1+e)^r 
\\
&
\le  \frac{4^\theta}{m^{2 \theta} (k+m)^{\theta}}
\theta m^\theta
\binom{3\theta -1}{\theta-1} (2\theta)!  (k+m)^\theta (1+e)^\theta = \frac{C_\theta}{m^\theta}\,,  
\end{split}
\end{equation*}
where $C_\theta$ is a constant depending on $\theta$ only. 
\end{proof}

