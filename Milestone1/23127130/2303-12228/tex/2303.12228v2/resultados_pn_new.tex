% We start this section remembering that if $\{X_n\}_{n \ge 0}$ is a $p_n$-\Name{} in direction $\ell$ we can write it as in~\eqref{xn-incremnto1}, hence we have
% $$
% X_n = \sum_{i=1}^n \big(1_{\{E_{i-1}\}} \xi_i + 1_{\{E_{i-1}^c\}} 1_{\{U_i > p_i \}} \xi_i + 1_{\{E_{i-1}^c\}}1_{\{ U_i \leq p_i\}} \gamma_i \big) \,,
% $$
% where $\{U_i\}_{i \geq 1}$ is a sequence  of i.i.d. random variables with uniform distribution in [0, 1], $E_i = \{ \exists\;  k < i \; \text{ such that }\;  X_k = X_i \}$ for all $i \ge 1$, $\{\xi_i, \FF_i\}_{i \geq 1}$ is an increment of a $d$-martingale with zero mean and $\{\gamma_i, \FF_i\}_{i\geq 1}$ is random vector such that $\EE[\gamma_i \cdot \ell |\FF_{i-1}] \ge \lambda$ .

Let $\{X_n\}_{n \ge 0}$ be a $p_n$-\Name{} then we can rewrite~\eqref{xn-incremnto1} as 
\begin{align}\label{xn-incremento2}
\begin{split}
X_n  & = \sum_{i=1}^n \big( \um_{ E_{i-1} \cup \{ U_i > p_i\}} \xi_i + \um_{E_{i-1}^c \cap \{ U_i \leq p_i\}} \gamma_i \big)
\\
& = \sum_{i=1}^n \big( \xi_i + \um_{E_{i-1}^c \cap \{ U_i \leq p_i\}} (\gamma_i - \xi_i) \big) \,.
\end{split}
\end{align}
%Before we provide the proofs of the main results for the $p_n$-\Name{}, let us point out that, 
Also for the sake of simplicity, we will henceforth work with the \[B_{\cdot}^n := \frac{X_{\lfloor n \cdot \rfloor}}{n^{1/2}}\;.\] instead of its interpolated version $\Hat{B}_{t}^n$ in \eqref{eq:B} which is continuous in $[0, \infty)$.  
More generally, 
in order to simplify writing and notation, if we have a sequence of c\`adl\`ag processes with values in $\mathbb{R}^m$, for $m \ge 2$, of the form $\Sigma^{n}_t = \Sigma_{\lfloor n t \rfloor}$, we denote by $\Hat{\Sigma}^{n}_t$ its linearly interpolated version, i.e., 
\begin{equation*}
\Hat{\Sigma}^{n}_t := \Sigma_{\lfloor n t \rfloor} + (nt - \lfloor nt \rfloor)(\Sigma_{\lfloor nt \rfloor + 1} - \Sigma_{\lfloor nt \rfloor}) \,, \ t\ge 0 \,,
\end{equation*}
which is a random element of $C_{\Rs^m}[0, \infty)$. Moreover by ~\cite[Proposition 10.4, Chapter 3]{ethier2009markov} if we have convergence in distribution in the Skorohod space of $\{\Sigma_{\cdot}^n\}_{n\ge 1}$ to a continuous process, then we also have convergence in distribution in $C_{\Rs^m}[0, \infty)$ of $\{\Hat{\Sigma}_{\cdot}^n\}_{n\geq 1}$ to the same limit. %Still about notation, we can also have $\Sigma^{n}_t = \Sigma_{\lfloor n t \rfloor}$ as a set value function and coherently we set
% \begin{equation*}
% \Hat{\Sigma}^{n}_t := |\Sigma_{\lfloor n t \rfloor}| + (nt - \lfloor nt \rfloor)(|\Sigma_{\lfloor nt \rfloor + 1}| - |\Sigma_{\lfloor nt \rfloor}|) \,, \ t\ge 0 \, .
% \end{equation*}




\begin{remark}\label{rem:conver}
 In the proofs, we will often make use of the following facts:
 \begin{itemize}
     \item[a)] If a sequence of processes converges in probability with respect to the uniform norm in $C_{\Rs^m}[0, T]$ for all $T >0$, then it converges in probability in $C_{\Rs^m}[0, \infty)$ under the metric $\rho$ defined in \eqref{def:rho} (this is a well-known result which can be easily proved).
     \item[b)] If a sequence of processes is tight in $C_{\Rs^m}[0, T]$ for all $T>0$ with the topology of uniform convergence in the compacts, then the sequence is tight in $C_{\Rs^m}[0, \infty)$ (see, e.g., \cite[Theorem 4.10,  Chapter 2]{karatzas2012brownian}). 
     \item[c)] \cite[Theorem 7.3]{billingsley1999probability}: A sequence of processes $\{\Hat{\Sigma}_{\cdot}^n\}_{n\geq 1}$ is tight in $C_{\Rs^m}[0, T]$  if and only if the following two conditions are satisfied:
     \begin{enumerate}[1.]
         \item For each positive $\eta$ there exist  $a$ and $n_0$ such that 
         \[ \PP\big[|\Hat{\Sigma}_{0}^n|\geq a\big]\leq \eta\,, \quad \text{ for all $n\geq n_0$}\,.
         \]
         \item For each positive $\varepsilon$ and $\eta$ there exist $\delta\in (0,1)$ and $n_0$ such that
         \[
         \PP\Big[\sup_{|s-t|\leq\delta}|\Hat{\Sigma}_{s}^n-\Hat{\Sigma}_{t}^n|\geq \varepsilon\Big]\leq \eta\,,  \quad \text{ for all $n\geq n_0$}\,.
         \]
     \end{enumerate}
 \end{itemize}
\end{remark}

%However for the computations we will do in the next sections, its clear that by Markov's inequality the second sum portion of $B_{t}^n$ converges to zero in probability. Hence, for our purposes, we only need to analyze the asymptotic behave of the first sum portion of $B_{t}^n$. 

\subsection{Case $\beta>1/2$; proof of Proposition~\ref{pn-WGERW-Gauss}}\label{prova-pn-WGERW}

\hfill \\
%\subsection{Proof of the convergence in distribution of the $p_n$-\Name*  with $\beta>1/2$}\label{prova-pn-WGERW}


%The $p_n$-\Name* can be written as in~\eqref{xn-incremnto1}. 
Recall that for the $p_n$-\Name*, the variables $\{U_i\}_{i \geq 1}$ are independent of  both sequences $\{\gamma_i\}_{ i\geq 1}$ and $\{\xi_i\}_{i \geq 1}$. 

%\cm{In this section we will consider a weaker condition. The sequence $\{U_i\}_{i \geq 1}$ will be uncorrelated, rather than independent, with both sequences $\{\gamma_i\}_{ i\geq 1}$ and $\{\xi_i\}_{i \geq 1}$.}  %\cm{Moreover we remember that the $p_n$-W\Name{} will be the process which satisfies the Conditions~\ref{condiçao I*},~\ref{condição2} and~\ref{condição3}.}

%Besides that we will also impose in this section a weaker condition that will replace Condition~\ref{condição1} and define the $p_n$-W\Name. Before, let us
%denote  $C=((c_{i,j}))$ as a continuous, $d \times d$ matrix-valued function, defined in $[0, \infty)$, satisfying $C(0) = 0$ and 
%\begin{equation*}
%\sum_{i,j = 1}^d (c_{i,j}(t) - c_{i,j}(s))\alpha_i \alpha_j \geq 0 \quad \text{for any } \alpha \in \mathbb{R}^d, \quad t > s \geq 0\;.  
%\end{equation*}
%By Theorem 7.1.1 from~\cite{ethier2009markov} there exists an unique process $Z$, in distribution, with sample paths in $C_{\mathbb{R}^d}[0, \infty)$ such that $Z_i$ and $Z_i Z_j - c_{i,j}$, for $i, j \in \{1,2, \dots, d\}$, are local martingales with respect to $\sigma$-algebra generated by historical of the process $Z$. Furthermore the process $Z$ has independent Gaussian increments.

%Now we can enunciate the new condition

%%%%%%%%%%%%
%\setcounter{condition}{0}
%\renewcommand{\thecondition}{\Roman{condition}*}


%%%%%%%%%%%%%%

%\begin{condition}\label{condiçao I*}
%\begin{itemize}
 %   \item [i)] For all $k \geq 1$ and $\theta < \beta - 1/2$, we have
  %  \[\sup_{k \geq 1} \frac{\EE[\| \gamma_k \|]}{k^{\theta}} < \infty \;. \]
   % \item [ii)] When the process behaves like a $d$-martingale with zero mean, it holds the following,
  %  \begin{equation}\label{condGaussiano}
   % \frac{1}{n}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i \xi_i^T \to C(t) \quad \text{as } n \to \infty \;,
  %  \end{equation}
  %  in probability and
  %  \begin{equation*}
   % \lim_{k \to \infty} k^{-1/2} \EE\left[ \sup_{1 \leq i \leq k} \| \xi_i \| \right]  = 0\;. 
  %  \end{equation*}
    
    %\item [ii)] For a large $n$ we have,
    %\[ \sum_{k = 1}^{\infty} \PP[\| X_k - X_{k-1} \| > k^{\frac{\theta}{2}}] < \infty \;, \]
    %where $\theta$ is a positive constant such that $\theta/2 < \beta - 1/2$.
%\end{itemize}
%\end{condition}

%Let $\{X_n\}_{n \ge 0}$ be written as~\eqref{xn-incremnto1} where the sequence $\{U_i\}_{i \geq 1}$ is uncorrelated with both sequences $\{\gamma_i\}_{ i\geq 1}$ and $\{\xi_i\}_{i \geq 1}$. Additionally, if it satisfies Condition~\ref{condiçao I*}, \ref{condição2} and~\ref{condição3} \marginpar{\tiny\cm{acho que nem precisamos de III}} the process $X$ will be called a $p_n$-W\Name{} in direction $\ell$. 

% We rewrite a version of the Theorem 7.1.4 from~\cite{ethier2009markov} where the authors state a convergence in distribution of a $d$-martingale to a process with independents Gaussian increments. We will denote $\xrightarrow{\mathcal{D}}$ as convergence in distribution.

% \begin{theorem}[see~\cite{ethier2009markov}, Theorem 7.1.4]\label{kurtz}
% Let $(\phi_k, k \geq 1)$ be a sequence of $\mathbb{R}^d$-valued random vectors such that $\EE[\phi_k|\FF_{k-1}]=0$ where $\FF_k = \sigma(\phi_l, l \leq k)$. Define,
% \begin{align*}
% M_{\lfloor nt \rfloor} = \sum_{i=1}^{\lfloor nt \rfloor} \phi_i \quad \text{ and} \quad
% A_{\lfloor nt \rfloor} = \frac{1}{n} \sum_{i=1}^{\lfloor nt \rfloor} \phi_i \phi_i^T \;.
% \end{align*}
% Assume that the following conditions hold:
% \begin{itemize}
%     \item [i)]\begin{equation*}
%     \lim_{n \to \infty} n^{-1/2} \EE\left[ \sup_{1 \leq k \leq n} |M_{k-1} - M_k| \right]  = 0\;. \end{equation*}
%     \item[ii)] For each $t \geq 0$,
%     \begin{equation*}
%         A_{\lfloor nt \rfloor} \to C(t) \quad \text{as } n \to \infty\;,
%     \end{equation*}
%     in probability.
% \end{itemize}
% Then $M_{\lfloor n \cdot \rfloor}/n^{1/2} \xrightarrow{\mathcal{D}} Z_{\cdot}$ , where $Z$ is a process with independent Gaussian increments.
% \end{theorem}

We start stating a result that will be used in the proof of Proposition~\ref{pn-WGERW-Gauss}. %\cm{Due to this following Lemma it will be possible to see that the drift push through time will not make a difference for the process.}

\begin{lemma}\label{lem: tgD}
Let $X$ be a $p_n$-\Name* 
in direction $\ell\in \mathbb{S}^{d-1}$, on $\ZZ^d$ with $d\geq 2$, $p_n= \mathcal{C}n^{-\beta} \wedge 1$ with $\beta > 1/2$. Define  
% Considering the representation~\eqref{xn-incremento2} set 
\begin{equation*}
D_{\lfloor nt \rfloor} := \frac{1}{n^{1/2}} \sum_{i=1}^{\lfloor nt \rfloor} \um_{E_{i-1}^c \cap \{ U_i \leq  \mathcal{C} i^{-\beta}\}} (\gamma_i - \xi_i), \ t\ge 0 \,.
\end{equation*}
Then $\{\Hat{D}^n_\cdot\}_{n\ge 1}$ as a sequence of random elements of $C_{\Rs^d}[0, \infty)$,  converges in probability to the zero function.
\end{lemma}

The proof of Lemma~\ref{lem: tgD} will be postponed at the end of this section.
% \comu{Acho que este parágrafo pode ser removido. A prova é curta e bem claro o que será feito} The main idea behind the proof of Theorem~\ref{pn-WGERW-Gauss} is to use the decomposition~\eqref{xn-incremento2} and to analyze separately the sum portions suitably rescaled. We will see that the sum portion corresponding to the part of  $d$-martingale will converge in distribution and the other will converge to zero in probability. Thus, using Slutsky's Theorem we obtain the desired result. 

\begin{proof}[Proof of Proposition~\ref{pn-WGERW-Gauss}]
Using~\eqref{xn-incremento2} we write 
\begin{equation}\label{p_n-WGERW_incrementos}
    B_t^n = \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{1}{n^{1/2}} \sum_{i=1}^{\lfloor nt \rfloor} \um_{E_{i-1}^c \cap \{ U_i \leq {\mathcal{C}} i^{-\beta}\}} (\gamma_i - \xi_i) \,.
\end{equation}
%Now we will analyze separately the two portion sums of~\eqref{p_n-WGERW_incrementos}.
By Lemma~\ref{lem: tgD} the linear interpolation of the second term in~\eqref{p_n-WGERW_incrementos}  
%\begin{equation}\label{gamma_i-xi_i_wgerw}\frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} 1_{\{E_{i-1}^c\}}1_{\{ U_i \leq i^{-\beta}\}} \gamma_i - 1_{\{E_{i-1}^c\}}1_{\{ U_i \leq i^{-\beta}\}} \xi_i  \to  0 \quad \text{as } n \to \infty\,, %\PP-\text{probability}\;.\end{equation}
converges in probability to the zero function (as a sequence of random elements of $C_{\Rs^d}[0, \infty)$). For the first term in~\eqref{p_n-WGERW_incrementos} we  use~\cite[Theorem 7.1.4, 7.1.1]{ethier2009markov} to obtain that
\begin{equation}\label{xi_i->Z}
    \Big\{ \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i \Big\}_{t\ge 0} \xrightarrow[n \to \infty]{\mathcal{D}} \{ Z_{t} \}_{t\ge 0} \,,
\end{equation}
where $Z_{\cdot}$ denotes a Gaussian process with independent increments. Using Slutsky's Theorem (see~\cite[Theorem 11.4]{gut2005probability}) we finish the proof. 
\end{proof}

% \cm{Acho queue este remark poderia ficar depois da prova do lema.}
% \begin{remark}\label{rem_cond_frac}
% As we already point out in Remark~\ref{rem_cond_antes}  the Condition~\ref{condiçao I*} and the sequence $\{p_n\}_{n \ge 1}$ can be more general. Specifically, if we had $\sum_{i=1}^{\lfloor nt \rfloor} p_i \EE[||\gamma_i||] = o(\sqrt{n})$, then it will be possible to prove that the second sum portion in~\eqref{p_n-WGERW_incrementos} goes to zero in probability. Hence we finish the proof with Slutsky's Theorem (Theorem 11.4 from~\cite{gut2005probability}).
% \end{remark}

The proof of Corollary~\ref{pnESRW->BM}  (stating that the rescaled $p_n$-\Nametwo{} converges in distribution to a standard Brownian Motion) follows the proof of Proposition~\ref{pn-WGERW-Gauss} line by line.  The main difference is  in~\eqref{xi_i->Z} where instead of using ~\cite[Theorem 7.1.4]{ethier2009markov}, we can apply Donsker's Theorem, since the $\{\xi_i\}_{i\geq 1}$ corresponding to  $p_n$-\Nametwo{} are i.i.d. with zero-mean vector and finite covariance matrix (see, e.g., \cite[Theorem 8.2]{billingsley1999probability} or~\cite[Theorem 5.1.2]{ethier2009markov}).


\medskip
\begin{proof}[Proof of Lemma~\ref{lem: tgD}.] Without loss of generality, we shall assume $\CC = 1$. In light of Remark~\ref{rem:conver}-$a)$ it suffices to show convergence in $C_{\Rs^d}[0, T]$ for all $T>0$.   Let us then define 
\begin{equation*}
D_{\lfloor n t \rfloor}^{\gamma}:=\frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor n t \rfloor} \um_{E_{i-1}^c}\um_{\{ U_i \leq i^{-\beta}\}} \gamma_i \,, \ t \ge 0,
%\ \text{and} \
% D_{\lfloor n\cdot \rfloor}^{\xi} := \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} 1_{\{E_{i-1}^c\}}1_{\{ U_i \leq i^{-\beta}\}} \xi_i \,. 
\end{equation*}
and analogously $D_{\lfloor n\cdot \rfloor}^{\xi}$ replacing $\gamma_i$ by $\xi_i$ in each term of the sum.
We begin showing  that $\{\Hat{D}_\cdot ^{n,\gamma}\}_{n\ge 1}$ converges in probability to the  zero function in $C_{\Rs^d}[0, T]$ for all $T > 0$ (recall that $\Hat{D}_t ^{n,\gamma}$ is the linearly interpolated version of   $D_{\lfloor nt \rfloor}^{\gamma}$). Note that
\begin{eqnarray}\label{eq: Dgamma}
& & \PP \Big( \sup_{0 \le t \le T} \big\| \Hat{D}^{n,\gamma}_{t} \big\| > \varepsilon  \Big)  \le \PP \Big( \sup_{0 \le t \le T} \sum_{i=1}^{\lfloor nt \rfloor+1}  \big\| \um_{E_{i-1}^c \cap \{ U_i \leq i^{-\beta}\}} \gamma_i \big\| > \varepsilon n^{\frac{1}{2}} \Big) \nonumber
\\
& & \le \PP \Big( \sum_{i=1}^{\lfloor nT \rfloor +1} \big\| \um_{E_{i-1}^c \cap \{ U_i \leq i^{-\beta}\}} \gamma_i \big\| > \varepsilon n^{\frac{1}{2}} \Big) \le  \PP \Big( \sum_{i=1}^{\lfloor nT \rfloor +1} \big\| \um_{ \{ U_i \leq i^{-\beta}\}} \gamma_i \big\| > \varepsilon n^{\frac{1}{2}} \Big) \nonumber
\\
& & \le \frac{1}{n^{1/2}\varepsilon} \sum_{i=1}^{\lfloor nT \rfloor +1} \frac{1}{i^{\beta}} \EE\left[\left\| \gamma_i \right\| \right]  \leq \frac{1}{n^{1/2}\varepsilon}  \sum_{i=1}^{\lfloor nT \rfloor +1} \frac{\EE\left[\left\| \gamma_i \right\| \right]}{i^{\theta}} \times   \frac{1}{i^{\beta-\theta}}  \,.    
\end{eqnarray}  
By Condition~\ref{condiçaoI*}, we know that  for all $i \geq 1$ and all $\theta < \beta - 1/2$ there exists a positive constant $L$ such that 
$\EE\left[\left\| \gamma_i \right\| \right] \le i^{\theta}   L$. 
Going back to~\eqref{eq: Dgamma}, we get
\begin{align*}%\label{eq: Dgamma3}
\begin{split}
\PP & \Big( \sup_{0 \le t \le T} \big\| \Hat{D}^{n,\gamma}_{t} \big\| > \varepsilon  \Big) %\le \frac{1}{n^{1/2}\varepsilon}  \sum_{i=1}^{\lfloor nT \rfloor} \frac{\EE\left[\left\| \gamma_i \right\| \right]}{i^{\theta}} \times   \frac{1}{i^{\beta-\theta}}  \\
\leq \frac{L}{n^{1/2}\varepsilon} \sum_{i=1}^{\lfloor nT \rfloor + 1} \frac{1}{i^{\beta-\theta}} 
\\ 
& \leq \frac{L}{n^{1/2}\varepsilon} \times c' (\lfloor nT \rfloor +1)^{1-\beta+\theta}
 \leq \frac{Lc'}{\varepsilon} \times \frac{\lfloor nT \rfloor + 1}{n} \times \frac{n^{1/2}}{(\lfloor nT \rfloor+1)^{\beta-\theta}}
\end{split}
\end{align*}
for some $c' > 0$, since $\sum_{i=1}^{k} \frac{1}{i^{\beta}} = O(k^{1-\beta})$. Given that $\beta >1/2+\theta$, we obtain that $\{\Hat{D}^{n,\gamma}_{\cdot}\}_{n\ge 1}$ converges uniformly in probability to the  zero function for all $T >0$.
%\begin{equation*}
%\frac{\sum_{i=1}^{\lfloor nt \rfloor} 1_{\{E_{i-1}^c\}}1_{\{ U_i \leq i^{-\beta}\}} \gamma_i}{n^{1/2}} \to  0 \quad \text{as } n \to \infty\,, %\PP-\text{probability}\;.    
%\end{equation*}

Using again Condition~\ref{condiçaoI*} and applying the same 
computations as above, we show that $\{\Hat{D}^{n,\xi}_{\cdot}\}_{n\ge 1}$ also converges uniformly in probability to the  zero function for all $T >0$.
%\begin{equation*}\frac{\sum_{i=1}^{\lfloor nt \rfloor} 1_{\{E_{i-1}^c\}}1_{\{ U_i \leq i^{-\beta}\}} \xi_i}{n^{1/2}}  \to  0 \quad \text{as } n \to \infty\,, %\PP-\text{probability}\;.    \end{equation*}
%in probability in the space $C_{\Rs^d}[0, T]$ for all $T >0$.
Therefore the same convergence also holds for $\{\Hat{D}^{n}_{\cdot} = \Hat{D}^{n,\gamma}_{\cdot} - \Hat{D}^{n,\xi}_{\cdot}\}_{n\ge 1}$. 
%
% Since convergence in $C_{\Rs^d}[0, T]$ for all $T >0$ implies convergence in $C_{\Rs^d}[0, \infty)$ under the metric $\rho$, then $D_{\lfloor n\cdot \rfloor}$ converges in probability to zero as a sequence of random elements of $C_{\Rs^d}[0, \infty)$ 
% \com{...HERE!!}
% .
\end{proof}

%\subsection{Proof of the convergence in distribution of the $p_n$-\Nametwo{} with $\beta = 1/2$ and $d=2$.}\label{prova-pn-ERW_d=2}

\subsection{Case $\beta = 1/2$; proof of Theorem~\ref{pn-ERW-d=2} and Theorem~\ref{pn-ERW-d=>4}}\label{sec:main-theorem}

\hfill \\

We begin introducing some auxiliary results.
% required to analyze the asymptotic behavior of the $p_n$-\Nametwo{} on $\ZZ^d$, with $\beta = 1/2$ and $d\geq 2$.
%
Let $\{\xi_i\}_{i \ge 1}$ be i.i.d. $\ZZ^d$-valued random variables with zero-mean vector and finite variance. Let $\{Y_n\}_{n \ge 0}$ be the random walk on $\ZZ^d$ with increments $\{\xi_i\}_{i \ge 1}$ starting at $Y_0 = 0$, thus $Y_n = \sum_{i=1}^{n} \xi_i$, $n\ge 1$.  For $m \leq n$ define 
\[ \Rr_{[m,n]} ^Y := \{Y_m, Y_{m+1}, \ldots, Y_n\}\;,\]
and denote by $\Rr_{n}^{Y} = \Rr^{Y}_{[0,n]}$,  the range of the random walk $\{Y_n\}_{n \geq 0}$.

We now state two known results about the range of a random walk on $\ZZ^d$ with i.i.d. increments which are instrumental in our proofs. Henceforth, we denote by $\pi_d$ the probability that $\{Y_n\}_{n \geq 0}$ never returns to the origin (as in the statement of Proposition~\ref{prop:RangeERW} and Proposition~\ref{prop:RangeERW_lower}).
\begin{theorem}[{\cite[pages 38-40]{spitzer2001principles}}\&{\cite[Theorem 1]{hamana2001large}}]\label{teo: RnZ>}
Let $\{Y_n\}_{n \geq 0}$ be an aperiodic random walk with i.i.d increments on $\ZZ^d$ for $d\geq 2$. It holds that 
\begin{itemize}
    \item  {\rm Law of Large Numbers:}
\begin{align*}
\tag{LLN}\frac{|\Rr_n^Y|}{n} \xrightarrow[n \to \infty]{} \pi_d \ \text{ a.s.}\,.
\end{align*}
\item {\rm Large Deviation:}  for every  $\theta' > \pi_d$ and $n$ sufficiently large
\begin{align*}
\tag{LD}\PP[|\Rr_{n}^Y| \geq \theta' n] \leq e^{-c_{\theta'}n}\,,
\end{align*}
where $c_{\theta'}$ is a positive constant that depends on $\theta'$.
\end{itemize}
 Note that for $d=2$, we have that $\pi_d=0$, whereas for $d\geq 3 $, $\pi_d\in (0,1]$. 
\end{theorem}

\begin{remark}
The meaning of aperiodicity used above is the one given in \cite[Chapter II, section 7]{spitzer2001principles} and it is different from the usual one (see, e.g., \cite[page 7]{levin2017markov}).  
Aperiodicity in Theorem~\ref{teo: RnZ>} refers to the following property: 
a  process on $\ZZ^d$ is aperiodic if the group generated by the support of the process is all of $\ZZ^d$.  However, as explained in~\cite[page 188-beginning of Section 2]{hamana2001large}, this condition does not entail a loss of generality. %However, as explained in~\cite[page 188-beginning of Section 2]{hamana2001large}, this condition can be relaxed to \com{include??} periodic random walks. \comu{incluir comentário sobre definição de aperiódico}
\end{remark}

Let us denote by $K_n$ the set of times from $0$ to $n-1$ in which $X$ visits a site for the first time and becomes excited. Henceforth, without loss of generality, we  assume $\CC = 1$, i.e., $p_n=n^{-1/2}$. We can write $K_n$ as
\begin{equation*}
K_n = \big\{ i \in \{1, 2, \dots, n\} : \um_{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}} =1 \big\}\,.  
\end{equation*}
If $\{\psi_i\}_{i \ge 1}$ denotes  the sequence of $\FF$-stopping times  corresponding to the  times the $p_n$-\Nametwo{} visits a new site, then setting $\varphi_i = \psi_i + 1$, we have that
\begin{equation}\label{eq: def Kn}
|K_n| = \sum_{i=1}^{n} \um_{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}} = \sum_{j=1}^{|\Rr_{n-1}^X|} \um_{\{U_{\varphi_j} \leq \varphi_j^{-1/2} \}}\,. 
\end{equation}

%If we further define  the following random variable
%\begin{equation}\label{eq: def Jn}
 %   |J_n| := \sum_{i=1}^{|\Rr_n^X|} 1_{\{U_i \leq i^{-1/2} \}}\,,
%\end{equation}
%then we obtain
%\begin{equation}\label{Kn<Jn}
%|K_n| =  \sum_{j=1}^{|\Rr_n^X|} 1_{\{U_{\tau_j} \leq \tau_j^{-1/2} \}} \preceq \sum_{i=1}^{|\Rr_n^X|} 1_{\{U_i \leq i^{-1/2} \}} = |J_n| \,,   
%\end{equation}
%by the fact that the realizations of the sequence $\{U_i\}_{i \ge 1}$ is i.i.d. and the $\tau_j$'s are stopping times.

Below we present an important auxiliary result that will be useful in the proof of Theorem~\ref{pn-ERW-d=2} and Theorem~\ref{pn-ERW-d=>4}.

\begin{lemma}\label{Jntight}
Let $|K_n|$ be defined as in~\eqref{eq: def Kn} and consider the corresponding sequence of continuous  processes $\{\Hat{K}^n_{\cdot}/n^{1/2}\}_{n\geq 1}$. It holds that
\begin{enumerate}[i)]
    \item For $d\geq 2$,  $\{\Hat{K}^n_{\cdot }/n^{1/2}\}_{n\ge 1}$ is tight  in  $C_{\Rs}[0, \infty)$;
    \item For $d= 2$, $\{\Hat{K}^n_{\cdot}/n^{1/2}\}_{n\geq 1}$ converges in probability as random elements of $C_{\Rs}[0, \infty)$ to the zero function.
\end{enumerate}

\end{lemma}
%
The proof of Lemma~\ref{Jntight} will be postponed at the end of this section.


\subsubsection{Case $\beta = 1/2$ and $d=2$; proof of Theorem~\ref{pn-ERW-d=2}} \label{prova-pn-ERW_d=2}

\begin{proof}[Proof of Theorem~\ref{pn-ERW-d=2}] 
% The idea of the proof of Theorem~\ref{pn-ERW-d=2} is similar to the one used in Theorem~\ref{pn-WGERW-Gauss}, however to obtain a version of Lemma~\ref{lem: tgD} in the case $d=2$ and $\beta=1/2$  we will use Proposition~\ref{prop:RangeERW} (see, proof of Lemma~\ref{Jntight}). 

By~\eqref{xn-incremento2} and the definition of $K_n$, we can rewrite the the process $B_t^n$ as
\begin{align}\label{p_n-ERW_incrementos_d=2}
\begin{split}
B_t^n & 
%= \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{1}{n^{1/2}} \sum_{i=1}^{\lfloor nt \rfloor} \um_{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}} (\gamma_i - \xi_i)\\ & 
= \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{1}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}}  (\gamma_i - \xi_i) 
\\
& = 
\frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i)}{|K_{\lfloor nt \rfloor}|}\, , \  t\ge 0\,.
\end{split}
\end{align}

For the first term in \eqref{p_n-ERW_incrementos_d=2} apply Donsker's Theorem (see, e.g., \cite[Theorem 5.1.2 $(c)$]{ethier2009markov}) we obtain that %the first sum portion of~\eqref{p_n-ERW_incrementos_d=2} converges in distribution in $C_{\Rs^2}[0, \infty)$ to a Brownian Motion, i.e.,  
\begin{equation}\label{xi_i->W2}
    \Big\{ \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i \Big\}_{t\ge 0} \xrightarrow[n \to \infty]{\mathcal{D}} \{ W_{t} \}_{t\ge 0} \,,
\end{equation}
where $W_{\cdot}$ is a Brownian Motion in dimension 2 with zero-mean vector and covariance matrix $\EE[\xi_1 \xi_1^T]$. 
%
%Let us remember that $|K_{\lfloor n \cdot \rfloor}|/n^{1/2} \preceq |J_{\lfloor n \cdot \rfloor}|/n^{1/2}$ (see~\eqref{Kn<Jn}). 
%By Lemma~\ref{Jntight} $\{\Hat{K}^n_{\cdot}/n^{1/2}\}_{n\geq 1}$ converges in probability as random elements of $C_{\Rs}[0, \infty)$ to the identically zero function.
%\begin{equation}\label{eq: Kntprob}
%\left\{\frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}}\right\}_{t\geq 0} \xrightarrow[n \to \infty]{} 0 \,,    
%\end{equation}
%in probability \com{we could use the notation of convergence in probability} as random elements of 
%
%\cm{Before we analyze the second sum portion in~\eqref{p_n-ERW_incrementos_d=2} let us give a little script how the proof will continuing. First we will show that the random variable $|J_n|/\sqrt{n}$ converges in probability to 0. Then with this computation it will be possible to see that the finite dimensional of the process $|J_{\lfloor nt \rfloor}|/\sqrt{n}$ converges in probability to 0 for any $t \ge 0$.}
%\cm{Finally since the process $|J_{\lfloor nt \rfloor}|/\sqrt{n}$ is tight in the space $C[0, \infty)$ by Lemma~\ref{Jntight}, we will see that converges in distribution to zero. Hence with the stochastic domination in~\eqref{Kn<Jn} and~\eqref{xi_i->W2} we will be able to obtain the desired result.}
%Let us denote the following event, for any $\varepsilon > 0$, set $G = \{|J_{n}| > \varepsilon \sqrt{n}\}$.
%Now, from Markov's inequality, for a $\delta>0$, one can see that 
%\begin{align}\label{eq:Bn}
%\end{align}
%From~\eqref{eq:Bn} we obtain 
%\begin{align}\label{eq:Bn2}
%\end{align}
%The last equality in~\eqref{eq:Bn2}, we obtain from Proposition~\ref{Rn<dn} and the fact that $\sum_{i=1}^{\lceil \delta n\rceil} \frac{1}{i^{1/2}} = \Theta(\lceil \delta n\rceil^{1/2})$. Since we can choose $\delta$ as small as we want. One can conclude 
%\begin{equation}\label{eq:Bn3}
 %   \lim_{n \to \infty} \PP[|J_n| > \varepsilon \sqrt{n}] = 0 \;.
%\end{equation}
%\cm{Now if we set $\{|J_{\lfloor nt \rfloor}| > \varepsilon \sqrt{n}\}$ for any $t \ge 0$, we have, by same computations we did above to obtain~\eqref{eq:Bn3}}
%\cm{
%\begin{equation}\label{eq:Bnt3}
%    \lim_{n \to \infty} \PP[|J_{\lfloor nt \rfloor}| > \varepsilon \sqrt{n}] = 0 \;.
%\end{equation}}
%\cm{Then by~\eqref{eq:Bnt3} we can conclude that the the finite dimensional of the process $|J_{\lfloor nt \rfloor}|/\sqrt{n}$ converges in probability to zero.} %and the process $|J_{\lfloor n \cdot \rfloor}|/\sqrt{n}$ is tight for $t=0$ in $C[0, T]$ for all $T > 0$.}
%\cm{Now, following our little script, we will show that the process $|J_{\lfloor nt \rfloor}|/\sqrt{n}$ converges in probability to 0 in the space $C[0, T]$. Notes that for any $\delta'>0$ and $t >0$, we have
%\begin{equation*}
%\PP \left[\sup_{|s-t| \le \phi} \left| \frac{|J_{\lfloor ns \rfloor}| - |J_{\lfloor nt \rfloor}|}{n^{1/2}} \right| \ge \varepsilon  \right] = \PP \left[ ||J_{\lfloor n(t+ \phi) \rfloor}| - |J_{\lfloor nt \rfloor}|| \ge \varepsilon n^{1/2}  \right]\,,    
%\end{equation*}
%by the definition of $|J_{\lfloor nt \rfloor}|$.}
%\cm{Then by the same techniques to obtain~\eqref{eq:Bn3} we have
%\begin{equation}\label{eq: Bntight}
%\lim_{\delta' \to 0} \limsup_{n \to \infty} \frac{1}{\delta'} \PP \left[\sup_{t \le s \le t + \delta'} \left| \frac{|J_{\lfloor ns \rfloor}| - |J_{\lfloor nt \rfloor}|}{n^{1/2}} \right| \ge \varepsilon  \right] = 0 \,.    
%\end{equation}}
%\cm{Since we have that the the finite dimensional of the process $|J_{\lfloor n\cdot \rfloor}|/\sqrt{n}$ converges in probability for 0 and by Lemma~\ref{Jntight} this process is tight in $C[0, \infty)$, we obtain by Theorem 2.4.15 in~\cite{karatzas2012brownian} the following
%\begin{equation}\label{eq: Bntprob}
%\frac{|J_{\lfloor nt \rfloor}|}{n^{1/2}} \to 0 \quad \text{as } n \to \infty \,,    
%\end{equation}
%in distribution in $C[0, \infty)$ and consequently in probability.}
%Hence from the stochastic domination  and~\eqref{eq: Bntprob} we obtain 
%\begin{equation}\label{eq:prob01}
%    \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \to 0 \quad \text{as } n \to \infty \,,
%\end{equation}
%in probability in $C[0, \infty)$. 
%

Now we consider the second term in \eqref{p_n-ERW_incrementos_d=2}. Clearly with probability one either $\lim_{n \to \infty} |K_{\lfloor nt \rfloor}|< \infty$ or $\lim_{n \to \infty} |K_{\lfloor nt \rfloor}|=+ \infty$.
%
% We shall now consider two cases: $\lim_{n \to \infty} |K_{\lfloor nt \rfloor}|< \infty$ almost surely and $\lim_{n \to \infty} |K_{\lfloor nt \rfloor}|=+ \infty$ almost surely. \comu{São só esses os casos possíveis? Acho que temos que fixar a realização.}
%
If $\lim_{n \to \infty} |K_{\lfloor nt \rfloor}|< \infty$,  then 
% there exists a positive constant $L$ \com{this $L$ should be 1?} such that 
\begin{equation}\label{eq:prob02finito}
\lim_{n \to \infty} \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ \Vert \gamma_i - \xi_i \Vert}{|K_{\lfloor nt \rfloor}|} \leq 2K\,,
\end{equation}
where $K$ is from Condition~\ref{condition_A}.
%
%Hence from~\eqref{eq: Kntprob} and~\eqref{eq:prob02finito} we have \begin{equation}\label{eq:prob03} \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i)}{|K_{\lfloor nt \rfloor}|} \to 0 \quad \text{as } n \to \infty \,,\end{equation}in the space $C_{\Rs^2}[0, \infty)$ in probability. Then from~\eqref{xi_i->W2},~\eqref{eq:prob03} and Slutsky's Theorem (see~\cite[Theorem 11.4]{gut2005probability}) we have our result.
%
%\cm{Now we will show that $|K_{\lfloor nt \rfloor}| \to \infty$, almost surely as $n \to \infty$. Let us set a $\delta' \in (0,1)$. Hence we have}
%\cm{\begin{align*}
%|K_n| & = |K_n|1_{\{|\Rr_n^X| \le \delta' n \}} + |K_n|1_{\{|\Rr_n^X| > \delta' n \}}
%\\
%& \ge |K_n|1_{\{|\Rr_n^X| > \delta' n \}} \ge \sum_{i= \delta' n +1}^n 1_{\{U_{j_i} \le i^{-1/2} \}} \,.
%\end{align*}}
%
%\cm{We set the random variable $Y_i = 1_{\{U_i \le (\delta'n +i)^{-1/2} \}}$, thus we have $\sum_{i=1}^{n} Y_i = \sum_{i= \delta' n +1}^n 1_{\{U_i \ge i^{-1/2} \}}$. Let $\varepsilon' > 0$ and we obtain
%\begin{align*}
%\begin{split}
%\sum_{i=1}^n \PP[|Y_i| > \varepsilon'] & = \sum_{i=1}^n \PP[ Y_i \neq 0] =  \sum_{i=1}^n \frac{1}{(\delta' n +i)^{\frac{1}{2}}}
%\\
%& = \sum_{i= \delta' n +1}^n \frac{1}{i^{\frac{1}{2}}} \to \infty \quad \text{as } n \to \infty \,.
%\end{split}
%\end{align*}}
%\cm{Hence by the Second Lemma of Borel-Cantelli we have that $Y_i > \varepsilon'$ happens infinitely often and we can conclude $\sum_{i=1}^{n} Y_i \to \infty$ almost surely as $n \to \infty$.} 

If $\lim_{n \to \infty}|K_{\lfloor nt \rfloor}| = + \infty$, since  the sequence of random vectors $\{\gamma_{\varphi_i} -\xi_{\varphi_i}\}_{i \geq 1}$ is i.i.d. having  the same distribution as $\{\gamma_{i} -\xi_{i}\}_{i \geq 1}$, which is also i.i.d. (see Lemma~\ref{lem: iid}), we can use~\cite[Theorem 8.2 item (iii)]{gut2005probability} and obtain 
\begin{equation}\label{eq:prob02}
   \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i)}{|K_{\lfloor nt \rfloor}|} = \sum_{i=1}^{|K_{\lfloor nt \rfloor}|} \frac{ (\gamma_{\varphi_i} - \xi_{\varphi_i})}{|K_{\lfloor nt \rfloor}|} \xrightarrow[n \to \infty]{} \EE[\gamma_1 -\xi_1] \, \text{ a.s..}
\end{equation}

Thus, from Lemma~\ref{Jntight} (item $ii)$), \eqref{eq:prob02finito} and~\eqref{eq:prob02} we have
\begin{equation}\label{eq:prob03}
    \sup_{0\le t \le T} \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i)}{|K_{\lfloor nt \rfloor}|} \xrightarrow[n \to \infty]{} 0 \,,
\end{equation}
in probability.
%
Hence, from~\eqref{xi_i->W2},~\eqref{eq:prob03} and Slutsky's Theorem (see, e.g., \cite[Theorem 11.4]{gut2005probability}) we obtain our result. 
\end{proof}


% \subsubsection{Case $\beta = 1/2$ and $d=2$; proof of Theorem~\ref{pn-ERW-d=2}} \label{prova-pn-ERW_d=2}

% Let us denote by $K_n$ the set of times until time $n$ in which $X$ visits a site for the first time and becomes excited. Henceforth, without loss of generality, we  assume $\CC = 1$, i.e., $p_n=n^{-1/2}$. We can write $K_n$ as
% \begin{equation*}
% K_n = \{ i \in \{1, 2, \dots, n\} : \um_{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}} =1 \}\,.  
% \end{equation*}
% Now we set the sequence of $\FF$-stopping times $\{\varphi_i\}_{i \ge 1}$ corresponding to the times the $p_n$-\Nametwo{} visits a new site. One can easily check that
% \begin{equation}\label{eq: def Kn}
% |K_n| = \sum_{i=1}^{n} \um_{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}} = \sum_{j=1}^{|\Rr_n^X|} \um_{\{U_{\varphi_j} \leq \varphi_j^{-1/2} \}}\,. 
% \end{equation}
% %If we further define  the following random variable
% %\begin{equation}\label{eq: def Jn}
%  %   |J_n| := \sum_{i=1}^{|\Rr_n^X|} 1_{\{U_i \leq i^{-1/2} \}}\,,
% %\end{equation}
% %then we obtain
% %\begin{equation}\label{Kn<Jn}
% %|K_n| =  \sum_{j=1}^{|\Rr_n^X|} 1_{\{U_{\tau_j} \leq \tau_j^{-1/2} \}} \preceq \sum_{i=1}^{|\Rr_n^X|} 1_{\{U_i \leq i^{-1/2} \}} = |J_n| \,,   
% %\end{equation}
% %by the fact that the realizations of the sequence $\{U_i\}_{i \ge 1}$ is i.i.d. and the $\tau_j$'s are stopping times.

% Below we present an important auxiliary result that will be useful in the proof of Theorem~\ref{pn-ERW-d=2}.

% \begin{lemma}\label{Jntight}
% Let $|K_n|$ be defined as in~\eqref{eq: def Kn}.  We have that the sequence of processes $\{\Hat{K}^n_{\cdot}/n^{1/2}\}_{n\geq 1}$ converges in probability as random elements of $C_{\Rs}[0, \infty)$ to the identically zero function.
% \end{lemma}
% %
% The proof of Lemma~\ref{Jntight} will be postponed at the end of this section.







%Thereunto we will see that this process fulfills the condition on Theorem 7.3 in~\cite{billingsley1999probability}.  

%One can notice that the process $J_{\lfloor n \cdot \rfloor}/n^{1/2}$ is tight for $t=0$ for all $n \ge 1$. Thus the first condition on Theorem 7.3 in~\cite{billingsley1999probability} is satisfied.

%Then let $P_n$ be a probability measure on $C[0, T]$ and the distribution of $J_{\lfloor n\cdot \rfloor}/n^{1/2}$. We denote the set $A_t(\varepsilon, \phi):= \{f \in C[0,T] : \sup_{t \le s \le t+\phi} |f(s) - f(t)| \ge \varepsilon \}$.
%To prove that the process satisfies the second condition on Theorem 7.3 in~\cite{billingsley1999probability} we will use the Corollary on page 83 in~\cite{billingsley1999probability}.

%This Corollary states that Condition $(ii)$ of Theorem 7.3 in~\cite{billingsley1999probability} holds if, for each positive $\varepsilon$ and $\eta$, there exists a $\phi \in (0,1)$, and an integer $n_0$ such that
%\begin{equation}\label{eq: pnaJn}
%\frac{1}{\phi}  P_n[A_t(\varepsilon, \phi)] \le \eta \quad \forall n \ge n_0\,.    
%\end{equation}

%Hence we have
%\begin{equation*}
%\begin{split}
%\frac{1}{\phi}  P_n[A_t(\varepsilon, \phi)] & = \frac{1}{\phi} \PP\left[\sup_{t \le s \le t+\phi } | |J_{\lfloor ns \rfloor}| - |J_{\lfloor n t \rfloor}|| \ge \varepsilon n^{\frac{1}{2}}  \right]
%\\
%& \le \frac{1}{\phi} \PP\left[|J_{\lfloor n(t+\phi) \rfloor}|  \ge \varepsilon n^{\frac{1}{2}}\right] \,.
%\end{split}    
%\end{equation*}
%Now with the same techniques we use to achieve~\eqref{eq:Bn3} we obtain


%\subsection{Proof of the convergence in distribution of the $p_n$-\Nametwo{} with $\beta = 1/2$ and $d \ge 4$.}\label{sec: d>4}

\subsubsection{Case $\beta = 1/2$ and $d \ge 3$; proof of Theorem~\ref{pn-ERW-d=>4}}\label{sec: d>4}

\hfill \\


Recall that $\pi_d$ denotes the probability that the random walk with i.i.d. (with zero mean and finite variance) increments $\{\xi_i\}_{i\geq 0}$ on $\ZZ^d$ never returns to the origin. 
%
Given $\delta \in (0, 1)$, let us define the following random variables:
\begin{align}
\label{Bn'}
J_n(\delta) &:= \sum_{i=1}^{\delta n} \um_{\{U_i \leq i^{-1/2} \}}\,,
\\
\label{Fn'}
V_n(\delta')&:=\sum_{i=n-\delta' n+1}^{n} \um_{\{U_i \leq i^{-1/2} \}}\,.
\end{align}

The random variables $J_n$ and $V_n$ will be important to compute the constants $c_1$ and $c_2$ in the statement of Theorem~\ref{pn-ERW-d=>4}. Below we state a few  simple results about them; we defer the proofs to the end of this section. 


\begin{lemma}\label{B'_n} Fix $d\geq 3$ and
let $\{J_n(\delta)\}_{n \geq 1}$ be defined as in~\eqref{Bn'} with $\delta \in (\pi_d, 1)$ and  $\{V_n(\delta')\}_{n \geq 1}$ be defined as in~\eqref{Fn'} with $\delta' \in (0, \pi_{d})$. Then, it holds that
%
\begin{align*}
i) &\quad \lim_{n \to \infty} \frac{\EE[J_n(\delta)]}{n^{1/2}} = 2\delta^{1/2} \quad \text{ and }\quad  \lim_{n \to \infty} \frac{\EE[V_n(\delta')]}{n^{1/2}} = 2-  2(1 -\delta')^{1/2}\,,
\\
   ii) &\quad \text{For any $\varepsilon > 0$,} 
   \\
   &\qquad \lim_{n \to \infty} \PP[|J_n(\delta) - \EE[J_n(\delta)]| > \varepsilon n^{1/2}] = 0\,, 
   % \text{ and } \lim_{n \to \infty} \PP[|V_n(\delta') - \EE[V_n(\delta')]| > \varepsilon n^{1/2}] = 0 \,.  
   \\
   &\quad \text{  and the same holds for $V_n(\delta')$. }
    \end{align*}
   
    %  \item [(iii)]
    % \begin{equation*}
    % \lim_{n \to \infty} \frac{\EE[V_n]}{n^{1/2}} = 2-  2(1 -\delta')^{1/2} ; 
    % \end{equation*}
    
    % \item[(iv)] For any $\varepsilon > 0$, 
    % \begin{equation*}
    % \lim_{n \to \infty} \PP[|V_n - \EE[V_n]| > \varepsilon n^{1/2}] = 0 ; 
    % \end{equation*}
\end{lemma}
\medskip 
%
From Lemma~\ref{B'_n} we obtain the following corollary. 

% \begin{corollary}\label{B'n->p}
% Let $\{J_n\}_{n \geq 1}$ be defined in~\eqref{Bn'} with $\delta \in (\pi_d, 1)$ and  $\{F_n\}_{n \geq 1}$ be defined in~\eqref{Fn'} with $\delta' \in (0, \pi_{d-k})$. Then, it holds that: 
% \begin{itemize}
%     \item [i)] 
%     \begin{equation*}
%         \frac{J_n}{n^{1/2}} \xrightarrow[n \to \infty]{} 2\delta^{1/2} \ \text{ in probability;}
%     \end{equation*}
    
%     \item [ii)]
%     \begin{equation*}
%         \frac{F_n}{n^{1/2}} \xrightarrow[n \to \infty]{} 2(1- \delta')^{1/2} \  \text{ in probability;}
%     \end{equation*}
    
%     \item [iii)]
%     \begin{equation*}
%          \frac{\sum_{i=1}^{n} \um_{\{U_i \le i^{1/2}\}}}{n^{1/2}}  \xrightarrow[n \to \infty]{} 2 \ \text{ in probability.}
%     \end{equation*}
% \end{itemize}
% \end{corollary}

 
\begin{corollary}\label{B'n->p}
Fix $d\geq 3$ and let $\{J_n(\delta)\}_{n \geq 1}$ be defined in~\eqref{Bn'} with $\delta \in (\pi_d, 1)$ and  $\{V_n(\delta')\}_{n \geq 1}$ be defined in~\eqref{Fn'} with $\delta' \in (0, \pi_{d})$. Then, it holds that
    \begin{equation*}
        \frac{J_n(\delta)}{n^{1/2}} \xrightarrow[n \to \infty]{} 2\delta^{1/2} \quad \text{ and } \quad  \frac{V_n(\delta')}{n^{1/2}} \xrightarrow[n \to \infty]{} 2- 2(1- \delta')^{1/2} \quad   \text{ in probability}\,.
    \end{equation*}
    % \begin{equation*}
    %  \tag{ii}   \frac{V_n}{n^{1/2}} \xrightarrow[n \to \infty]{} 2- 2(1- \delta')^{1/2} \  \text{ in probability}\,.
    % \end{equation*}
\end{corollary}

\medskip 
%
Relying on  Corollary~\ref{B'n->p}, we are able to prove the following result: 

\begin{lemma}\label{lem: tightaux} Fix $d\geq 3$ and
let $\{\Hat{J}^{n}_{\cdot}(\delta)\}_{n\geq 1}$ and $\{\Hat{V}^n_{\cdot}(\delta')\}_{n\geq 1}$ be respectively the sequences of processes in $C_{\Rs}[0, \infty)$ corresponding to  
$J_n(\delta)$ with $\delta \in (\pi_d, 1)$ and  $V_n(\delta')$ with $\delta' \in (0, \pi_{d})$  defined in~\eqref{Bn'} and \eqref{Fn'}, respectively.   Then, it holds that:
%
\begin{itemize}
    \item [i)]
    $\{\Hat{J}^n_{\cdot}(\delta)/n^{1/2}\}_{n\geq 1}$ converges in distribution as random elements of $C_{\Rs}[0, \infty)$ to the deterministic function $t \mapsto 2(\delta t)^{1/2}$, $t\ge 0$.  
    \item[ii)] $\{\Hat{V}^n_{\cdot}(\delta')/n^{1/2}\}_{n\geq 1}$
%$$
%\left\{\frac{\sum_{i=1}^{\lfloor n \cdot \rfloor}  \um_{\{U_i \leq i^{-1/2} \}} - F_{\lfloor n \cdot \rfloor}}{n^{1/2}}\right\}_{t\geq 0} 
%$$
converges in distribution as random elements of $C_{\Rs}[0, \infty)$ to the deterministic function $t \mapsto 2t^{1/2}(1\!-\!(1\!-\!\delta')^{1/2})$, $t\ge 0$.  
\end{itemize}
\end{lemma}

% The proof of Lemma~\ref{lem: tightaux} will be postponed to the end of this section.

% The next result states that $(|K_{\lfloor n\cdot \rfloor}|/n^{1/2})_{n\ge 1}$ is tight in $C_{\Rs}[0, \infty)$. %In the proof of the Theorem~\ref{pn-ERW-d=>4}, it will be more clear the importance of this result.


% \begin{lemma}\label{lem: Knt/n1/2tig}
% The sequence of processes $\{\Hat{K}^n_{\cdot }/n^{1/2}\}_{n\ge 1}$ is tight  in  $C_{\Rs}[0, \infty)$.
% \end{lemma}

By Lemma~\ref{Jntight} (item $i)$), for $d\geq 3$, every subsequence of $\{\Hat{K}^n_{\cdot }/n^{1/2}\}_{n\ge 1}$ has a limit point. The next result states that those limits points are concentrated on paths confined between the curves  $t \mapsto 2 (1-\sqrt{1 -  \pi_{d}}) \sqrt{t}$ and $t \mapsto 2  \sqrt{ \pi_d \, t}$.
% The proof of Lemma~\ref{lem: Knt/n1/2tig} will be postponed at the end of this section.

% \begin{proposition}\label{prop: bound_Kn}
% %Let $|K_{\lfloor n \cdot \rfloor}|/n^{1/2}$ be a sequence of processes in $C_{\Rs}[0, \infty)$. 
% If $H_{\cdot}$ is a limit point of $\{\Hat{K}^n_{\cdot }/n^{1/2}\}_{n\ge 1}$, then
% \begin{equation*}
% \PP \left[\forall t \in [0,\infty): 2t^{1/2}(1-(1 -  \delta')^{1/2}) \le H_t \le 2(t \delta)^{1/2} \right] = 1 \, ,  
% \end{equation*}
% where $\delta'$ and $\delta$ are positive constants such that $\delta' \in (0, \pi_{d-k})$ and $\delta \in (\pi_d, 1)$. 
% \end{proposition}

\begin{proposition}\label{prop: bound_Kn}
%Let $|K_{\lfloor n \cdot \rfloor}|/n^{1/2}$ be a sequence of processes in $C_{\Rs}[0, \infty)$. 
If $\mathcal{H}_{\cdot}$ is a limit point of $\{\Hat{K}^n_{\cdot }/n^{1/2}\}_{n\ge 1}$, then
\begin{align}
\tag{a}&\text{ For $d\geq 3$, }\, \quad 
\PP \left[\forall t \in [0,\infty):  \mathcal{H}_t \le 2(t \pi_d)^{1/2} \right] = 1 \, ; 
\\
\tag{b}&\text{ For $d\geq 22$,} \quad 
\PP \left[\forall t \in [0,\infty): \mathcal{H}_t \ge 2t^{1/2}(1-(1 -  \pi_{d})^{1/2}) \right] = 1 \, . 
\end{align}
\end{proposition} 

\begin{remark}\label{rem:conjecture}
If Conjecture~\ref{conj_range} were true, we would be able to strengthen the claim of Proposition~\ref{prop: bound_Kn} and obtain that for any $d\geq 3$ it holds that
\begin{equation*}
\PP \left[\forall t \in [0,\infty): 2t^{1/2}(1-(1 -  \pi_{d})^{1/2}) \le \mathcal{H}_t \le 2(t \pi_d)^{1/2} \right] = 1 \, . 
\end{equation*}
\end{remark}


% The proof of Proposition~\ref{prop: bound_Kn} will be postponed at the end of this section.


We now have all the auxiliaries results to prove Theorem~\ref{pn-ERW-d=>4}. The main idea is to use~\eqref{xn-incremento2} and then analyze separately the rescaled sum terms. We show  that the sequences corresponding to both terms are tight in $C_{\Rs^d}[0, \infty)$, consequently we obtain that $\{\Hat{B}_{\cdot}^n\}_{n\geq 1}$ is also tight. Finally, we  describe the processes which stochastically dominate the limit points of $\{\Hat{B}_{\cdot}^n\}_{n\geq 1}$. The strategy is similar to that used in the proof of Theorem~\ref{pn-ERW-d=2}, but here the term representing the drift direction does not go to zero. The random variables $J_n$ and $V_n$ play an important role in controlling this non-vanishing term.  
%
\begin{proof}[Proof of Theorem~\ref{pn-ERW-d=>4}]
We begin showing that 
$\{{\Hat{B}}_{\cdot}^n\}_{n\ge 1}$ is tight in $C_{\Rs^d}[0, \infty)$ for any $d\geq 3$. By Remark~\ref{rem:conver}-$b)$ it suffices  to show that $\{{\Hat{B}}_{\cdot}^n\}_{n\ge 1}$ is tight in $C_{\Rs^d}[0, T)$, for all $T>0$.

%
Using ~\eqref{xn-incremento2} we can rewrite the process $B_t^n$ as
\begin{align}\label{p_n-ERW_incrementos_d=>4}
\begin{split} 
& \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{1}{n^{1/2}} \sum_{i=1}^{\lfloor nt \rfloor} \um_{\{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}\}} (\gamma_i - \xi_i) \, , \ \forall \, t \ge 0\,.
% \\
% & = \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{1}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}}  (\gamma_i - \xi_i) 
% \\
% & 
% = \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i)}{|K_{\lfloor nt \rfloor}|}\,.
\end{split}
\end{align}
By Donsker's Theorem the first term of~\eqref{p_n-ERW_incrementos_d=>4} converges in distribution as random elements of $C_{\Rs^d}[0, \infty)$ to a Brownian Motion, i.e., 
\begin{equation}\label{xi_i->W2_d=>4}
 \Big\{ \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i \Big\}_{t\ge 0} \xrightarrow[n \to \infty]{\mathcal{D}} \{ W_{t} \}_{t\ge 0} \,,
\end{equation}
where $W_{\cdot}$ is a Brownian Motion in dimension $d$ with zero-mean vector and covariance matrix $\EE[\xi_1 \xi_1^T]$.
Then,  to show that $\{{\Hat{B}}_{\cdot}^n\}_{n\ge 1}$ is tight in $C_{\Rs^d}[0, T]$ for all $T>0$, it is enough to prove that the second term in~\eqref{p_n-ERW_incrementos_d=>4} is tight in $C_{\Rs^d}[0, T]$. 
Indeed, $\{\Hat{B}_{\cdot}^n\}_{n\geq 1}$ 
 would be the sum of two tight sequences of processes, thus also tight. %(see Lemma~\ref{lem: tight}) \cm{colocar uma referencia ou deixar?}.
%
% Then by~\cite[Theorem 4.10 in Chapter 2]{karatzas2012brownian}, since $B_{\cdot}^n$ is  tight  in $C_{\Rs^d}[0, T]$ for all $T>0$ with the topology of uniform convergence in the compacts, $B_{\cdot}^n$ is  tight  in $C_{\Rs^d}[0, \infty)$.

In order to show that the second term in~\eqref{p_n-ERW_incrementos_d=>4} is tight in $C_{\Rs^d}[0, T]$ for all $T>0$,  we use Remark~\ref{rem:conver}-$c)$.
% \cite[Theorem 7.3]{billingsley1999probability} which provides two sufficient conditions for tightness.
Recall the definition of $D_{\lfloor n t \rfloor}$ from the statement of Lemma \ref{lem: tgD}, remembering that here we have set $\mathcal{C} = 1$ and that we are under distinct hypotheses from those of Section \ref{prova-pn-WGERW}. We will show that $\{\Hat{D}^n_\cdot\}_{n\ge 1}$ is tight.
%$$
%D_{\lfloor n t \rfloor}:= \frac{1}{n^{1/2}} \sum_{i=1}^{\lfloor nt \rfloor} \um_{\{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}\}} (\gamma_i - \xi_i) \,.$$
The first condition in Remark~\ref{rem:conver}-$c)$ is satisfied, since  $\Hat{D}^n_0 \equiv 0$, for all $n\geq 1$. To prove that $\{\Hat{D}^n_\cdot\}_{n\ge 1}$ satisfies the second condition in Remark~\ref{rem:conver}-$c)$  we  use~\cite[Corollary on page 83]{billingsley1999probability} which states that the second condition of ~\cite[Theorem 7.3]{billingsley1999probability} holds if, for every positive $\varepsilon$ and $\eta$, there exists a $\phi \in (0,1)$, and an integer $n_0$ such that
\begin{equation}\label{eq: PnA}
\frac{1}{\phi} \, \PP \Big[ \sup_{t \le s \le t + \phi} \big\|\Hat{D}^n_{s} - \Hat{D}^n_{t} \big\|  \ge \varepsilon \Big]  \le \eta \quad \forall n \ge n_0\,.
\end{equation}
%\begin{equation}\label{eq: PnA}
%\frac{1}{\phi} P_n \Big[f \in C_{\Rs^d}[0,T] : \sup_{t \le s \le t+\phi} |f(s) - f(t)| \ge \varepsilon \Big] \le \eta \quad \forall n \ge n_0\,,
%\end{equation}
%where the probability measure $P_n$ on $C_{\Rs^d}[0,T]$  is the distribution of $D_{\lfloor n \cdot \rfloor}$. 
%In order to show that \eqref{eq: PnA} actually holds, note  that, if we define 
%\begin{equation}\label{eq:Atphi}
%A_t(\varepsilon, \phi):= \Big\{f \in C_{\Rs}[0,T] : \sup_{t \le s \le t+\phi} |f(s) - f(t)| \ge \varepsilon \Big\}\,,
%\end{equation} 
%the left-hand side of \eqref{eq: PnA} reduces to
% In order to show that actually holds,  let us define 
% $A_t(\varepsilon, \phi):= \{f \in C_{\Rs^d}[0,T] : \sup_{t \le s \le t+\phi} |f(s) - f(t)| \ge \varepsilon \}$. 
% Hence we obtain the following 
Note that the probability in \eqref{eq: PnA} is bounded from above by
\begin{equation}
\label{eq: PnPP}
%\PP\left[ D_{\lfloor n \cdot \rfloor} \in A_t(\varepsilon, \phi) \right] = 
%\PP \Big[ \sup_{t \le s \le t + \phi} \big\|\Hat{D}^n_{s} - \Hat{D}^n_{t} \big\|  \ge \varepsilon \Big] 
%& & = \PP \left[ \sup_{t \le s \le t+ \phi} \left\|\frac{\sum_{i=\lfloor nt \rfloor + 1}^{\lfloor ns \rfloor} 1_{\{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}\}} (\gamma_i - \xi_i)}{n^{\frac{1}{2}}}  \right\| \ge \varepsilon \right] 
\PP\Big[ \sup_{t \le s \le t+ \phi} \Big\| \sum_{i=\lfloor nt \rfloor}^{\lfloor ns \rfloor + 1} \um_{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}} (\gamma_i - \xi_i)  \Big\| \ge \varepsilon n^{\frac{1}{2}} \Big] \, ,
%\\
%& = \PP \left[ \sum_{i = \lfloor nt \rfloor + 1}^{\lfloor n(t + \phi) \rfloor} 1_{\{E_{i-1}^c \cap \{U_i \le i^{-\frac{1}{2}}\}\}} \ge \varepsilon n^{\frac{1}{2}} \right] \le \PP \left[ \sum_{i = \lfloor nt \rfloor + 1}^{\lfloor n(t + \phi) \rfloor} 1_{ \{U_i \le i^{-\frac{1}{2}}\}} \ge \varepsilon n^{\frac{1}{2}} \right]   
\end{equation}
%
% Now we only analyze the process inside the probability measure in~\eqref{eq: PnPP}. We will find an upper bound for this process for all the trajectory. 
for all $s \in [t, t+ \phi]$ and 
\begin{eqnarray}
\label{eq: trajetoria}
\Big\| \sum_{i=\lfloor nt \rfloor}^{\lfloor ns \rfloor + 1} \um_{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}} (\gamma_i - \xi_i)  \Big\| &\le & \sum_{i=\lfloor nt \rfloor}^{\lfloor ns \rfloor + 1} \big\|  \um_{ \{ U_i \leq i^{-1/2}\}} (\gamma_i - \xi_i)  \big\| \nonumber
 \\
% \le  \sum_{i=\lfloor nt \rfloor + 1}^{\lfloor ns \rfloor} \big\|  \um_{ \{ U_i \leq i^{-1/2}\}} (\gamma_i - \xi_i)  \big\| 
& \le & \sum_{i=\lfloor nt \rfloor}^{\lfloor ns \rfloor + 1}  \um_{ \{ U_i \leq i^{-1/2}\}} 2K  \,, 
\end{eqnarray}
where the second inequality follows from  triangle inequality and the last from Condition~\ref{condition_A}. 
%
Then from~\eqref{eq: PnPP} and ~\eqref{eq: trajetoria} we obtain that
\begin{eqnarray}\label{eq: PnA<1}
\lefteqn{\!\!\!\!\!\!\!\! \PP \Big[ \sup_{t \le s \le t + \phi} \big\|\Hat{D}^n_{s} - \Hat{D}^n_{t} \big\|  \ge \varepsilon \Big] \le 
\PP \Big[ \sum_{i = \lfloor nt \rfloor}^{\lfloor n(t + \phi) \rfloor +1} \um_{ \{U_i \le i^{-\frac{1}{2}}\}} 2K \ge \varepsilon n^{\frac{1}{2}} \Big] } \nonumber
\\
& & \le \exp\Big(\frac{-\varepsilon n^{\frac{1}{2}}}{2K}\Big)\EE\Big[\exp\Big( \sum_{i = \lfloor nt \rfloor}^{\lfloor n(t + \phi)\rfloor+1} \um_{ \{U_i \le i^{-\frac{1}{2}}\}} \Big) \Big] 
%& & \le \exp\Big(\frac{-\varepsilon n^{\frac{1}{2}}}{2K}\Big) \prod_{i = \lfloor nt \rfloor + 1}^{\lfloor n(t + \phi)\rfloor} \EE\left[\exp\left(  \um_{ \{U_i \le i^{-\frac{1}{2}}\}} \right) \right] \,,
\end{eqnarray}
where in the last inequality we have used exponential Markov's inequality.
%
Setting  $c = \varepsilon/(2K)$ an  continuing the computation in~\eqref{eq: PnA<1} we obtain that
\begin{eqnarray}\label{eq: PnA<}
\lefteqn{\frac{1}{\phi} \PP \Big[ \sup_{t \le s \le t + \phi} \big\|\Hat{D}^n_{s} - \Hat{D}^n_{t} \big\|  \ge \varepsilon \Big] \le \frac{1}{\phi} e^{-c n^{\frac{1}{2}}} \prod_{i = \lfloor nt \rfloor}^{\lfloor n(t + \phi)\rfloor + 1} \EE\left[\exp\left(  \um_{ \{U_i \le i^{-\frac{1}{2}}\}} \right) \right]} \nonumber \\
& & = \frac{1}{\phi} e^{-c n^{\frac{1}{2}}} \prod_{i = \lfloor nt \rfloor}^{\lfloor n(t + \phi)\rfloor + 1} \left(1+ \frac{e-1}{i^{\frac{1}{2}}} \right) \le \frac{1}{\phi} e^{-c n^{\frac{1}{2}}} \prod_{i = \lfloor nt \rfloor}^{\lfloor n(t + \phi)\rfloor + 1} \exp\left(\frac{e-1}{i^{\frac{1}{2}}} \right)  
% \\
% & \le \frac{1}{\phi} e^{-c n^{\frac{1}{2}}}  \exp\left(\sum_{i = \lfloor nt \rfloor + 1}^{\lfloor n(t + \phi)\rfloor}\frac{e-1}{i^{\frac{1}{2}}} \right) 
\nonumber \\
& & \le \frac{1}{\phi} \exp(-c n^{\frac{1}{2}})\exp\left(2(e-1)(\sqrt{n(t + \phi)} - \sqrt{nt} + 2) \right) \,,  
\end{eqnarray}
where %the second inequality follows by the moment generating function of a Bernoulli  and the third by the fact that $1+x<e^x$ for all $x$.  
the last inequality above follows from noticing that 
\begin{align*}
\sum_{i = \lfloor nt \rfloor}^{\lfloor n(t + \phi)\rfloor + 1}\frac{1}{i^{\frac{1}{2}}} & \le \int_{nt - 1}^{n(t+\phi) + 1} x^{-1/2}dx \le 2\left(\sqrt{n(t + \phi)} - \sqrt{nt} + 2\right) \,.
\end{align*}
Therefore, in order to show that \eqref{eq: PnA}  holds, it remains to show that for every positive $\varepsilon$ (recall that $c=\varepsilon/(2K)$) and $\eta$,  there exists a $\phi \in (0,1)$, and an integer $n_0$ such that
\begin{equation}\label{exp<eta}
\frac{1}{\phi} \exp(-c n^{\frac{1}{2}})\exp\bigg(2(e-1)\left(\sqrt{n}\left(\sqrt{t + \phi} - \sqrt{t} \right) + 2 \right) \bigg) \le \eta \quad \forall n \ge n_0 \,.
\end{equation}
We accomplish this choosing $\phi \in (0,1)$ sufficiently small such that $\sqrt{t+\phi} - \sqrt{t}< c/4(e-1)$. Then, for every $\eta>0$, choosing $n$ sufficiently large, we obtain that~\eqref{exp<eta} is satisfied. 
% One can see that, since we have $\phi \in (0,1)$,  for all $\hat{\varepsilon} > 0$, there exists a $\phi' > \phi$ such that $|\sqrt{t+\phi'} - \sqrt{t}|< \hat{\varepsilon}$. Now we can choose $\hat{\varepsilon} = c/4(e-1)$ and we obtain, for a large enough $n$, that~\eqref{exp<eta} is fulfilled for all $\eta$.
Consequently, we have that~\eqref{eq: PnA} is satisfied and thus $\{\Hat{D}^n_{\cdot}\}_{n \ge 1}$ is tight in $C_{\Rs^d}[0,T]$. 
 
% Since the process $B_{\cdot}^n$  is the sum of two tight processes in $C_{\Rs^d}[0, T]$,  we obtain that $B_{\cdot}^n$ is a tight process in $C_{\Rs^d}[0, T]$ for all $T>0$ as a simple exercise. %(see Lemma~\ref{lem: tight}) \cm{colocar uma referencia ou deixar?}.

% Now by~\cite[Theorem 4.10 in Chapter 2]{karatzas2012brownian} one can see that since $B_{\cdot}^n$ is  tight  in $C_{\Rs^d}[0, T]$ for all $T>0$ with the topology of uniform convergence in the compacts then  $B_{\cdot}^n$ is  tight  in $C_{\Rs^d}[0, \infty)$.

We now prove the second part of the theorem, namely the stochastic domination in $a)$ and $b)$. Let us begin rewriting $B_t^n$ again in the slightly different form
\begin{equation}
 \label{p_n-ERW_incrementos_d=>4-bis}
% & = \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{1}{n^{1/2}} \sum_{i=1}^{\lfloor nt \rfloor} \um_{\{E_{i-1}^c \cap \{ U_i \leq i^{-1/2}\}\}} (\gamma_i - \xi_i) 
% % \\
% % & = \frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{1}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}}  (\gamma_i - \xi_i) 
% \\
% & 
\frac{1}{n^{1/2}}\sum_{i=1}^{\lfloor nt \rfloor} \xi_i + \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i)}{|K_{\lfloor nt \rfloor}|}\,.
\end{equation}
As already mentioned the first term converges to a Brownian Motion (see \eqref{xi_i->W2_d=>4}).  We now analyze the second term in~\eqref{p_n-ERW_incrementos_d=>4-bis}. 
%
For $d\geq 22$, by Proposition~\ref{prop: bound_Kn} part $(b)$,  we have  that $|K_{\lfloor nt \rfloor}| \to \infty$ as $n \to \infty$ almost surely. Moreover, the sequence of random vectors $\{\gamma_{\varphi_i} -\xi_{\varphi_i}\}_{i \geq 1}$ is i.i.d. and has the same distribution of $\{\gamma_{i} -\xi_{i}\}_{i \geq 1}$, which is i.i.d. too (see Lemma~\ref{lem: iid}). Thus, we can use~\cite[Theorem 8.2 item (iii)]{gut2005probability} to conclude that 
\begin{equation}\label{eq:prob02_d=>4}
   \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i)}{|K_{\lfloor nt \rfloor}|} \xrightarrow[n \to \infty]{} \EE[\gamma_1 -\xi_1]  = \EE[\gamma_1] \, \text{ a.s.}\,.
\end{equation}
Recall that $\{\gamma_n\}_{n \ge 1}$ is an i.i.d. sequence of random vectors and  $\lambda  \le \EE[\gamma_i \cdot \ell] \le K$ for all $i \ge 1$, we set $\mu_{\gamma} := \EE[\gamma_i \cdot \ell]$ for all $i \ge 1$. By Lemma~\ref{Jntight} and and~\eqref{eq:prob02_d=>4} we obtain that the linearly interpolated version of the sequence 
\begin{equation}\label{eq:seq}
\left\{ \frac{|K_{\lfloor n \cdot \rfloor}|}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i) \cdot \ell}{|K_{\lfloor n \cdot \rfloor}|} \right\}_{n\ge 1}\,, 
\end{equation}
is tight in $C_{\Rs}[0, \infty)$ and, from   Proposition~\ref{prop: bound_Kn} part $(b)$, for $d\geq 22$  any of its limit points $\mathcal{H}_t$ satisfies
\begin{equation}\label{eq:c1}
\PP\left[\forall t \in [0,\infty): 
2\mu_{\gamma} t^{\frac{1}{2}}(1 - (1 - \pi_{d})^{\frac{1}{2}})
\le \mathcal{H}_t \right] = 1\,.
\end{equation}
For $d\geq 3$, with probability one either $\lim_{n \to \infty} |K_{\lfloor nt \rfloor}|< \infty$ or $\lim_{n \to \infty} |K_{\lfloor nt \rfloor}|=+ \infty$.
%
On the event $\lim_{n \to \infty} |K_{\lfloor nt \rfloor}|< \infty$,  we have that 
$\sum_{i \in K_{\lfloor nt \rfloor}} \frac{ \Vert \gamma_i - \xi_i \Vert}{|K_{\lfloor nt \rfloor}|} \leq 2K$ for all $n$ ($K$ is from Condition~\ref{condition_A}) and $\lim_{n \to \infty}\frac{|K_{\lfloor n t \rfloor}|}{n^{1/2}}=0$. Thus, on this event, the linearly interpolated version of the sequence in \eqref{eq:seq} converges to the zero function. On the event $\lim_{n \to \infty} |K_{\lfloor nt \rfloor}|=+ \infty$ \eqref{eq:prob02_d=>4} holds and, similarly to the case $d\geq 22$, by Lemma~\ref{Jntight} together with  Proposition~\ref{prop: bound_Kn} part $(a)$, any limit point $\mathcal{H}_t$ of the linearly interpolated version of the sequence in \eqref{eq:seq} satisfies $\mathcal{H}_t \le 2\mu_{\gamma}(t \pi_d)^{1/2}$. Since $2\mu_{\gamma}(t \pi_d)^{1/2}\geq 0$,  overall we then obtain that 
\begin{equation}\label{eq:c2}
\PP\left[\forall t \in [0,\infty): 
\mathcal{H}_t \le 2\mu_{\gamma}(t \pi_d)^{1/2} \right] = 1\,.
\end{equation}

% \begin{align}\label{sup_
% inf_Kn}
% \PP \left[\forall t \in [0,\infty): 2t^{1/2}(1-(1 -  \pi_{d-k})^{1/2}) \le H_t \le 2(t \pi_d)^{1/2}  \right]  = 1 \,,
% \end{align}
% where $\{H_t\}_{t\ge 0}$ is a limit point of a subsequence of $\{ \Hat{K}^n_{\cdot}/n^{1/2}\}_{n\geq 1}$.
%\com{Above $\delta --> \delta' $ and  $\hat\delta --> \dekta $ right?}
%$\delta''$, $\Hat{\delta}$, $\delta'$ and $\delta$ are positive constants such that $\delta'' \in (0, \delta')$, $\Hat{\delta} \in (\delta, 1]$, $\delta \in (\pi_d, 1]$ and $\delta' \in (0, \pi_{d-k})$.
% \begin{equation}\label{Fn<Kn<Bn}
% \frac{1}{n^{1/2}} \sum_{i=1}^{\lfloor nt \rfloor}  1_{\{U_i \leq i^{-1/2} \}} - \frac{|F_{\lfloor nt \rfloor}|}{n^{1/2}} \preceq \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \preceq \frac{|J_{\lfloor nt \rfloor}|}{n^{1/2}}\,.
% \end{equation}
% Thus, by~\eqref{Bn<=Bn'} and~\eqref{Fn<Kn<Bn}, one can see that
% \begin{align}\label{eq:Kn<B'n}
% \begin{split}
% & \PP \left[ \forall t \in [0,\infty) : \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \le \frac{|J'_{\lfloor nt \rfloor}|}{n^{1/2}} \right] \ge \PP\left[ \forall t \in [0,\infty]: \frac{|J_{\lfloor nt \rfloor}|}{n^{1/2}} \le \frac{|J'_{\lfloor nt \rfloor}|}{n^{1/2}} \right] 
% \\
% & \to 1 \quad \text{as } n \to \infty\,.
% \end{split}
% \end{align}
% Now we will obtain, in the same sense of~\eqref{eq:Kn<B'n}, a lower bound. Hence by~\eqref{Fn<=Fn'} and~\eqref{Fn<Kn<Bn} we have
% \begin{equation}\label{eq: Kn>s-F'n}
% \begin{split}
% & \PP \left[\forall t \in [0,\infty) : \frac{\sum_{i=1}^{\lfloor nt \rfloor}  1_{\{U_i \leq i^{-1/2} \}}}{n^{1/2}} - \frac{|F'_{\lfloor nt \rfloor}|}{n^{1/2}} \le \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}}  \right] 
% \\
% & \ge \PP\left[\forall t \in [0,\infty): \frac{\sum_{i=1}^{\lfloor nt \rfloor}  1_{\{U_i \leq i^{-1/2} \}} - |F'_{\lfloor nt \rfloor}|}{n^{1/2}} \le \frac{\sum_{i=1}^{\lfloor nt \rfloor}  1_{\{U_i \leq i^{-1/2} \}} - |F_{\lfloor nt \rfloor}|}{n^{1/2}} \right] 
% \\
% & \to 1 \quad \text{as } n \to \infty \,.
% \end{split}
% \end{equation}
% Hence by Lemma~\ref{lem: tightaux}, Corollary~\ref{B'n->p}, ~\eqref{eq:Kn<B'n} and~\eqref{eq: Kn>s-F'n} we obtain that for all $t_0>0$
% \begin{align}\label{sup_
% inf_Kn}
% \PP \left[\forall t \in [t_0,\infty): 2t^{1/2}(1-(1 -  \delta')^{1/2}) \le \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \le 2(t \delta)^{1/2} \right] \to 1 \,,
% \end{align}
% as $n$ goes to infinity where $\delta$ and $\delta'$ are positive constants such that $\delta \in (\pi_d, 1]$ and $\delta' \in (0, \pi_{d-k})$.
%
\begin{comment}
Furthermore,  the sequence of random vectors $\{\gamma_{\varphi_i} -\xi_{\varphi_i}\}_{i \geq 1}$ is i.i.d. and has the same distribution of $\{\gamma_{i} -\xi_{i}\}_{i \geq 1}$, which is i.i.d. too (see Lemma~\ref{lem: iid}). Thus, we can use~\cite[Theorem 8.2 item (iii)]{gut2005probability} to conclude that 
\begin{equation}\label{eq:prob02_d=>4}
   \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i)}{|K_{\lfloor nt \rfloor}|} \xrightarrow[n \to \infty]{} \EE[\gamma_1 -\xi_1]  = \EE[\gamma_1] \, \text{ a.s..}
\end{equation}
Recall that $\{\gamma_n\}_{n \ge 1}$ is an i.i.d. sequence of random vectors and  $\lambda  \le \EE[\gamma_i \cdot \ell] \le K$ for all $i \ge 1$, we set $\mu_{\gamma} := \EE[\gamma_i \cdot \ell]$ for all $i \ge 1$. 
%\cm{Now we will prove that the second sum portion in~\eqref{p_n-ERW_incrementos_d=>4} is a tight process in $C[0,T]$. For that we will use again Theorem 7.3 in~\cite{billingsley1999probability}
%\begin{equation*}
%\begin{split}
%& \PP\left[ \sup_{t \le s \le t+ \phi} \left\| \sum_{i=\lfloor nt \rfloor + 1}^{\lfloor ns \rfloor} 1_{ \{ U_i \leq i^{-1/2}\}} (\gamma_i - \xi_i)  \right\| \ge \varepsilon n^{\frac{1}{2}} \right] \le
%\\
%& \PP\left[ \sup_{t \le s \le t+ \phi} \left( \sum_{i=\lfloor nt \rfloor + 1}^{\lfloor ns \rfloor} \left\|  1_{ \{ U_i \leq i^{-1/2}\}} (\gamma_i - \xi_i)  \right\| \right) \ge \varepsilon n^{\frac{1}{2}} \right] \le
%\\
%& \PP\left[ \sup_{t \le s \le t+ \phi} \left( \sum_{i=\lfloor nt \rfloor + 1}^{\lfloor ns \rfloor}   1_{ \{ U_i \leq i^{-1/2}\}} 2K \right) \ge \varepsilon n^{\frac{1}{2}} \right] \le
%\\
%& \PP\left[  \sum_{i=\lfloor nt \rfloor + 1}^{\lfloor n(t+\phi) \rfloor}   1_{ \{ U_i \leq i^{-1/2}\}} 2K  \ge \varepsilon n^{\frac{1}{2}} \right] \,.
%\end{split}    
%\end{equation*}}
%
From Proposition~\ref{prop: bound_Kn} \com{here the reference to Proposition seems off...} and~\eqref{eq:prob02_d=>4} we obtain that the linearly interpolated version of the sequence 
$$
\left\{ \frac{|K_{\lfloor n \cdot \rfloor}|}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i) \cdot \ell}{|K_{\lfloor n \cdot \rfloor}|} \right\}_{n\ge 1}\,, 
$$
is tight in $C_{\Rs}[0, \infty)$ and, from  {\color{red} Proposition~\ref{prop: bound_Kn} part $(a)$, for $d\geq 3$ } any of its limit points $\widetilde H_t$ satisfies
{\color{red} 
\begin{equation}\label{eq:c2}
\PP\left[\forall t \in [0,\infty): 
\widetilde H_t \le 2\mu_{\gamma}(t \pi_d)^{1/2} \right] = 1\,,
\end{equation}
whereas, from Proposition~\ref{prop: bound_Kn} part $(b)$, for $d\geq 22$  any of its limit points $\widetilde H_t$ satisfies 
\begin{equation}\label{eq:c1}
\PP\left[\forall t \in [0,\infty): 
2\mu_{\gamma} t^{\frac{1}{2}}(1 - (1 - \pi_{d})^{\frac{1}{2}})
\le \widetilde H_t \right] = 1\,.
\end{equation}

%\begin{equation}\label{eq: c1c2}
%\begin{split}
%& \PP\left[\forall t \in [0,\infty): \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i) \cdot \ell_{D_k}}{|K_{\lfloor nt \rfloor}|}  \le 2\mu_{\gamma}(t\delta)^{1/2} \right] \to 1 \quad \text{and}
%\\
%& \PP\left[\forall t \in [0,\infty): \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \sum_{i \in K_{\lfloor nt \rfloor}} \frac{ (\gamma_i - \xi_i) \cdot \ell_{D_k}}{|K_{\lfloor nt \rfloor}|} \ge 2t^{\frac{1}{2}}(1 - (1 - \delta')^{\frac{1}{2}})\mu_{\gamma}\right] \to 1 \,,
%\end{split}   
%\end{equation}
%as $n$ goes to infinity.
\end{comment}
Since $\{\Hat{B}_{\cdot}^n\}_{n\geq 1}$ is  tight in $C_{\Rs^d}[0, \infty)$, thus relatively compact by Prohorov's Theorem (see, e.g., ~\cite[Theorem 5.1]{billingsley1999probability}),  every subsequence has a limit point.
%
By~\eqref{xi_i->W2_d=>4}, \eqref{eq:c2} and \eqref{eq:c1},  for any of those limit points $\{\mathcal{Y}_{t}\}_{t\ge 0}$ we have that for all $t \in [0, \infty)$
\begin{align*}
 \{\mathcal{Y}_t \cdot \ell\}_{t\ge 0} &\preceq \{W_t \cdot \ell + 2 c_2 \sqrt{t} \}_{t\ge 0} \,, && \text{ for $d\geq 3$}\,,\\
\{W_t \cdot \ell + 2 c_1 \sqrt{t}\}_{t\ge 0} &\preceq \{\mathcal{Y}_t \cdot \ell\}_{t\ge 0}\,, && \text{ for $d\geq 22$}\,,  
\end{align*}
where $c_1 = (1 - \sqrt{1 - \pi_{d}})\mu_{\gamma}$ and $c_2 = \sqrt{\pi_d} \, \mu_{\gamma}$ (and $0 < c_1 \le c_2$).  
\end{proof}



\begin{proof}[Proof of Lemma~\ref{Jntight}] 
Item $ii)$: 
 By   Remark~\ref{rem:conver}-$a)$,  it is enough to prove that the sequence of processes $\{\Hat{K}^n_{\cdot}/n^{1/2}\}_{n\geq 1}$ converges in probability as random elements of $C_{\Rs}[0, T)$ to the  zero function, for all $T > 0$. For the latter it suffices to prove  
 convergence in probability of $\sup_{0 \le t \le T} |K_{\lfloor nt \rfloor}|/n^{1/2}$ to zero for all $T > 0$.
% , since convergence in $C_{\Rs^d}[0, T]$ for all $T >0$ implies convergence in $C_{\Rs^d}[0, \infty)$ under the metric $\rho$ 
% \com{...HERE!!}
% . 
To this aim, let us define  
$$G_n:=\Big\{\sup_{0 \le t \le T} |K_{\lfloor nt \rfloor}| > \varepsilon \sqrt{n}\Big\} = \left\{ |K_{\lfloor nT \rfloor}| > \varepsilon \sqrt{n} \right\}\, ,
$$
For every $\varepsilon > 0$ and $\delta>0$, %consider the following event $G = \{|K_{\lfloor nT \rfloor}| > \varepsilon \sqrt{n}\}$. 
by Markov's inequality, we have that 
\begin{align*}\label{eq: Jnprob}
\begin{split}
& \PP[ G_n ] = \PP\big[
G_n \cap \{|\Rr_{\lfloor nT \rfloor} ^X| > \delta \lfloor nT \rfloor\}\big] +  \PP\big[ G_n \cap \{|\Rr_{\lfloor nT \rfloor}^X| \leq \delta \lfloor nT \rfloor\}\big]
\\
& \leq \PP\big[|\Rr_{\lfloor nT \rfloor} ^X| > \delta \lfloor nT \rfloor\big] + \PP\Big[ \sum_{i=1}^{\lceil \delta n T\rceil} \um_{\{U_i \leq i^{-1/2} \}} > \varepsilon\sqrt{n} \Big] 
\\
& \leq \PP\big[|\Rr_{\lfloor nT \rfloor} ^X| > \delta \lfloor nT \rfloor\big] + \frac{1}{\varepsilon \sqrt{n}} \sum_{i=1}^{\lceil \delta nT\rceil} \frac{1}{i^{1/2}}\,. \end{split}
\end{align*}
Note that we have used $\Rr_{\lfloor nT \rfloor}^X$ instead of $\Rr_{\lfloor nT \rfloor -1}^X$, the reader can check that indeed this does not change the inequality and simplifies notation. This will happen in other inequalities in this section.
Using  Proposition~\ref{prop:RangeERW}, we have that, for all sufficiently large $n$,  $
    \PP[ |\Rr_n ^X| \leq \delta n ] = 1$, for every  $\delta > \pi_d$. Since for $d=2$, $\pi_d =0$,  we obtain that   $\lim_{n \to \infty}\PP\big[|\Rr_{\lfloor nT \rfloor} ^X| > \delta \lfloor nT \rfloor\big]=0$, for every $\delta>0$. Moreover, noticing that   $\sum_{i=1}^{\lceil \delta n\rceil} \frac{1}{i^{1/2}} = \Theta(\lceil \delta n\rceil^{1/2})$, we conclude that
\begin{equation}\label{eq: Jnprob2}
\begin{split}
& \limsup_{n \to \infty}  \PP[ G_n ]  
%\leq \limsup_{n \to \infty} \Big( \PP\big[|\Rr_{\lfloor nT \rfloor}^X| > \delta \lfloor nT \rfloor\big] + \frac{1}{\varepsilon \sqrt{n}} \sum_{i=1}^{\lceil \delta nT \rceil} \frac{1}{i^{1/2}} \Big) \\ & \leq \limsup_{n \to \infty} \PP\big[|\Rr_{\lfloor nT \rfloor}^X| > \delta \lfloor nT \rfloor\big] + \limsup_{n \to \infty} \Big( \frac{1}{\varepsilon \sqrt{n}} \sum_{i=1}^{\lceil \delta nT \rceil} \frac{1}{i^{1/2}} \Big) 
\leq \frac{c' (\delta T)^{1/2}}{\varepsilon}\,.
\end{split}
\end{equation}
Since $\delta>0$ is arbitrary,  $\limsup_{n \to \infty} \PP[ G_n ] = 0$ for every $\varepsilon$ fixed. Therefore, for all $T > 0$ the sequence of processes $\left\{\Hat{K}^n_{\cdot}/n^{1/2}\right\}_{n\geq 1}$ converges in probability, as random elements of $C_{\Rs}[0,T]$, to the  zero function.  %(see Lemma~\ref{lem: convprob}) \cm{pensar em uma referencia ou não ha necessidade?}. 

\medskip 
Item $i)$: 
% The proof follows the very same lines of that of Theorem~~\ref{pn-ERW-d=>4}, and we omit it. 
%
By  Remark~\ref{rem:conver}-$b)$  it suffices to show tightness in $C_{\Rs}[0, T]$ for all $T > 0$ and this is equivalent to show the sequence of processes $\left\{\Hat{K}^n_{\cdot}/n^{1/2}\right\}_{n\geq 1}$  satisfies  the two conditions in  Remark~\ref{rem:conver}-$c)$. 
Note that $\Hat{K}^n_{0}/n^{1/2}\equiv 0$ for all $n \ge 1$ and therefore the first condition in Remark~\ref{rem:conver}-$c)$ is satisfied.
%
To prove that $\{\Hat{K}^n_\cdot /n^{1/2}\}_{n\ge 1}$ satisfies the second condition in Remark~\ref{rem:conver}-$c)$  we  use~\cite[Corollary on page 83]{billingsley1999probability} which states that the second condition of ~\cite[Theorem 7.3]{billingsley1999probability} holds if, for every positive $\varepsilon$ and $\eta$, there exists a $\phi \in (0,1)$, and an integer $n_0$ such that
\begin{equation*}
\frac{1}{\phi} \, \PP \Big[ \sup_{t \le s \le t + \phi} \big\|\Hat{K}^n_{s} - \Hat{K}^n_{t} \big\|  \ge \varepsilon n^{1/2} \Big]  \le \eta \quad \forall n \ge n_0\,.
\end{equation*}
From this point on, using that 
\[
\PP \Big[ \sup_{t \le s \le t + \phi} \big\|\Hat{K}^n_{s} - \Hat{K}^n_{t} \big\|  \ge \varepsilon n^{1/2} \Big]  \le \PP \Big[ \sum_{i = \lfloor nt \rfloor }^{\lfloor n(t + \phi) \rfloor +1} \um_{\{U_i \le i^{-\frac{1}{2}}\}} \ge \varepsilon n^{\frac{1}{2}} \Big]\,,
\]
the computations are very similar to those used in the proof of Theorem~\ref{pn-ERW-d=>4}, and we omit it. 
\end{proof}





\begin{proof}[Proof of Lemma~\ref{B'_n}.] To avoid clutter in the notation, we write $J_n$ and $V_n$, thus omitting the dependence on $\delta$ and $\delta'$.  
As far as $J_n$ is concerned, we have 
\begin{equation*}
\frac{\EE[J_n]}{n^{1/2}} = \frac{1}{n^{1/2}}\EE\Big[\sum_{i=1}^{\delta n} \um_{\{U_i \leq i^{-1/2} \}}\Big] = \frac{1}{n^{1/2}}\sum_{i=1}^{\delta n} i^{-1/2} \xrightarrow[n \to \infty]{} 2\delta^{1/2}  \,. 
\end{equation*}
Also,  using Chebyshev's inequality and the independence of the random variables $\{U_i\}_{i \geq 1}$ we have that 
\begin{align*}
\PP \big[ |J_n - \EE[J_n]| > \varepsilon n^{1/2} \big] &\leq \frac{1}{\varepsilon^2 n} \text{Var}\Big[ \sum_{i=1}^{\delta n} \um_{\{U_i \leq i^{-1/2} \}} \Big]
\\
&=\frac{1}{\varepsilon^2 n} \sum_{i=1}^{\delta n} \frac{1}{i^{1/2}}\left(1-\frac{1}{i^{1/2}}\right) \xrightarrow[n \to \infty]{} 0  \,. 
\end{align*}

The proofs for $V_n$ are similar once we write
\[
V_n = \underbrace{\sum_{i=1}^n  \um_{\{U_i \leq i^{-1/2} \}}}_{=:I_n} - \underbrace{\sum_{i=1}^{n -\delta' n} \um_{\{U_i \leq i^{-1/2} \}}}_{=:F_n'}\,,
\]
and observe that  
\begin{align*}
&\frac{1}{n^{1/2}}\EE[I_n] = \frac{1}{n^{1/2}}\sum_{i=1}^{n} i^{-1/2} \xrightarrow[n \to \infty]{} 2\,,
\\
&\frac{1}{n^{1/2}}\EE[F'_n] = \frac{1}{n^{1/2}}\sum_{i=1}^{n - \delta' n} i^{-1/2} \xrightarrow[n \to \infty]{} 2(1-\delta')^{1/2} \,, 
\end{align*}
and that, by Chebyshev's inequality and the independence of the random variables $\{U_i\}_{i \geq 1}$, it holds that 
\begin{align*}
\PP & [|I_n - \EE[I_n]| > \varepsilon n^{1/2}] 
 \leq \frac{1}{\varepsilon^2 n} \text{Var}\Big[ \sum_{i=1}^{n} \um_{\{U_i \leq i^{-1/2} \}} \Big] 
% = 
% \frac{1}{\varepsilon^2 n} \sum_{i=1}^{n} i^{-1/2}(1-i^{-1/2})
\xrightarrow[n \to \infty]{} 0  
\,,
\\
\PP &\big[ |F'_n - \EE[F'_n]| > \varepsilon n^{1/2} \big] \leq \frac{1}{\varepsilon^2 n} \text{Var}\Big[ \sum_{i=1}^{n -\delta' n} \um_{\{U_i \leq i^{-1/2} \}} \Big]
% \\
% &
% =  \frac{1}{\varepsilon^2 n} \sum_{i=1}^{n - \delta' n} i^{-1/2}(1-i^{-1/2}) 
\xrightarrow[n \to \infty]{} 0 
\,.
\end{align*}
% The proof of point $v)$ is also straightforward: 
% \begin{equation*}
% \frac{\EE[\sum_{i=1}^n  \um_{\{U_i \leq i^{-1/2} \}}]}{n^{1/2}} = \frac{1}{n^{1/2}}\sum_{i=1}^{n} i^{-1/2} \xrightarrow[n \to \infty]{} 2 
% \,.  
% \end{equation*}
% For the proof of point $vi)$ we use Chebyshev's inequality and the independence of the random variables $\{U_i\}_{i \geq 1}$ and we obtain
% \begin{align*}
% % \label{eq: P}
% % \begin{split}
% \PP & \Big[\Big|\sum_{i=1}^n  \um_{\{U_i \leq i^{-1/2} \}} - \EE\Big[\sum_{i=1}^n  \um_{\{U_i \leq i^{-1/2} \}}\Big]\Big| > \varepsilon n^{1/2}\Big] 
%  \leq \frac{1}{\varepsilon^2 n} \text{Var}\Big[ \sum_{i=1}^{n} \um_{\{U_i \leq i^{-1/2} \}} \Big] 
%  \\
% &= \frac{1}{\varepsilon^2 n} \sum_{i=1}^{n} i^{-1/2}(1-i^{-1/2}) \to 0  \quad \text{as } n \to \infty 
% \,.  
% \end{align*}
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem: tightaux}.] To avoid clutter in the notation we omit the dependence on $\delta$ and $\delta'$. By Corollary~\ref{B'n->p}, for all $t> 0$ we have that 
\begin{align*}
\frac{J_{\lfloor n t\rfloor}(\delta)}{\sqrt{n}} & \xrightarrow[n \to \infty]{} 2\delta^{1/2} \sqrt{t}\,, && \forall \delta \in (\pi_d,1)\,;
\\
\frac{V_{\lfloor n t\rfloor}(\delta')}{\sqrt{n}} &\xrightarrow[n \to \infty]{} \left(2 - 2(1-\delta')^{1/2}\right)\sqrt{t}\,, && \forall  \delta' \in (0,\pi_d)\,,  
\end{align*}
in probability, i.e., the one-dimensional distributions  converge in probability to a constant. Therefore the joint distributions corresponding to times $t_1, \ldots, t_m$ also converge in probability and we obtain the convergence in the sense of the finite-dimensional distributions for both processes.
% By Corollary 3.1 we already have the convergence of the
% finite-dimensional distributions. 
% we have the convergence of the one-dimensional distributions in probability to a constant. Since each one-dimensional random variable converges in probability to a constant, the joint convergence in probability follows naturally, which in turn implies convergence in distribution.

By~\cite[Theorem 7.1]{billingsley1999probability},  to prove points $i)$ and $ii)$ it only remains to prove that both sequences of processes are tight in $C_{\Rs}[0, \infty)$. By Remark~\ref{rem:conver}-$b)$ we only need to prove tightness in $C_{\Rs}[0, T]$ for all $T>0$. The proof strategy is analogous to the one used in the proof of Theorem~\ref{pn-ERW-d=>4} which relies on Remark~\ref{rem:conver}-$c)$.  %
% when we show the second sum portion in~\eqref{p_n-ERW_incrementos_d=>4} is tight in $C_{\Rs}[0, T]$, for all $T>0$.

Item $i)$:  $\{\Hat{J}^n_{\cdot }/n^{1/2}\}_{n\geq 1}$ satisfies the first condition in Remark~\ref{rem:conver}-$c)$, since $\Hat{J}^n_0 = 0$ for all $n\geq 1$.  To prove the second condition in Remark~\ref{rem:conver}-$c)$, as in \eqref{eq: PnA} we need to show that for every positive $\varepsilon$ and $\eta$, there exists a $\phi \in (0,1)$, and an integer $n_0$ such that
\begin{equation}\label{eq: PnA2}
\frac{1}{\phi} \, \PP \Big[ \sup_{t \le s \le t + \phi} \big\|\Hat{J}^n_{s} - \Hat{J}^n_{t} \big\| \ge \varepsilon \Big]  \le \eta \quad \forall n \ge n_0\,.
\end{equation}
Following the same steps as in the proof of Theorem~\ref{pn-ERW-d=>4}, we have that
\begin{equation*}
\begin{split}
&\frac{1}{\phi} \PP \Big[ \sup_{t \le s \le t + \phi} \big\|\Hat{J}^n_{s} - \Hat{J}^n_{t} \big\| \ge \varepsilon \Big] \le 
 \frac{1}{\phi} \PP \Big[ \sum_{i = \delta\lfloor nt \rfloor}^{\delta\lfloor n(t + \phi) \rfloor +1} \um_{ \{U_i \le i^{-\frac{1}{2}}\}} \ge \varepsilon n^{\frac{1}{2}} \Big] 
% \\
 %& \leq  \frac{1}{\phi} e^{-\varepsilon n^{\frac{1}{2}}}\EE\Big[\exp\Big( \sum_{i = \delta\lfloor nt \rfloor + 1}^{\delta\lfloor n(t + \phi)\rfloor} \um_{ \{U_i \le i^{-\frac{1}{2}}\}} \Big) \Big] 
% \\
% & = \frac{1}{\phi} e^{-\varepsilon n^{\frac{1}{2}}} \prod_{i = \delta\lfloor nt \rfloor + 1}^{\delta\lfloor n(t + \phi)\rfloor} \left(1+ \frac{e-1}{i^{\frac{1}{2}}} \right) \le \frac{1}{\phi} e^{-\varepsilon n^{\frac{1}{2}}} \prod_{i = \delta\lfloor nt \rfloor + 1}^{\delta\lfloor n(t + \phi)\rfloor} \exp\left(\frac{e-1}{i^{\frac{1}{2}}} \right)  
% \\
% & \le \frac{1}{\phi} e^{-c n^{\frac{1}{2}}}  \exp\left(\sum_{i = \lfloor nt \rfloor + 1}^{\lfloor n(t + \phi)\rfloor}\frac{e-1}{i^{\frac{1}{2}}} \right) 
\\
& \le \frac{1}{\phi} \exp(-\varepsilon n^{\frac{1}{2}})\exp\left(2(e-1)(\sqrt{\delta n(t + \phi)} - \sqrt{\delta nt} + 2) \right) \,.
\end{split}    
\end{equation*}
and we obtain \eqref{eq: PnA2} choosing $\phi \in (0,1)$ sufficiently small such that $\sqrt{t+\phi} - \sqrt{t}< \varepsilon/4\sqrt{\delta}(e-1)$ for all $t \in [0, T]$. 
% \begin{equation*}
% \frac{1}{\phi} \exp(-\varepsilon n^{\frac{1}{2}})\exp\bigg(2(e-1)\sqrt{n\delta}\left(\sqrt{t + \phi} - \sqrt{t}\right) \bigg) \le \eta \quad \forall n \ge n_0 \,.
% \end{equation*}
% The latter can be easily verified as it was done in the proof of Theorem~\ref{pn-ERW-d=>4}. 

% % From now on the proof follows exactly  as in Theorem~\ref{pn-ERW-d=>4}, when we prove the second sum portion in~\eqref{p_n-ERW_incrementos_d=>4} fulfills the second condition of~\cite[Theorem 7.3] {billingsley1999probability}. \comu{precisa ser mais específico aqui} Then we have that for each positive $\varepsilon$ and $\eta$, there exists a $\phi \in (0,1)$, and an integer $n_0$ such that
% % \begin{equation*}
% % \frac{1}{\phi} \PP [|J'_{\lfloor n \cdot \rfloor}|/n^{1/2} \in A_t(\varepsilon, \phi)] \le \eta \quad \forall n \ge n_0\,.
% % \end{equation*}
% % Ergo by~\cite[Theorem 7.3]{billingsley1999probability} we obtain that the sequence $|J_{\lfloor n \cdot \rfloor}|/n^{1/2}$ is a tight in $C_{\Rs}[0, T]$ for all $T > 0$ with the topology of uniform convergence in compacts and moreover by~\cite[Theorem 2.4.10]{karatzas2012brownian} \com{here we cite a different result than the one cited in the remark!} is a tight sequence of processes in $C_{\Rs}[0, \infty)$. 
The proof of item $ii)$ is similar with the only difference that we analyze separately $\sum_{i=1}^{\lfloor n \cdot \rfloor} \um_{ \{U_i \le i^{-\frac{1}{2}}\}}/n^{1/2}$ and    $\sum_{i=1}^{\lfloor (n -\delta' n) \cdot \rfloor} \um_{\{U_i \leq i^{-1/2} \}}/n^{1/2}$.
%
Using the very same computation as in  item $i)$, we conclude that the linearly interpolated version of 
$$
    \Big\{\sum_{i=1}^{\lfloor n \cdot \rfloor} \um_{ \{U_i \le i^{-\frac{1}{2}}\}}/n^{1/2}\Big\}_{n\geq 1}\,  \text{ and } \Big\{ \sum_{i=1}^{\lfloor (n -\delta' n) \cdot \rfloor} \um_{\{U_i \leq i^{-1/2} \}}/n^{1/2}\Big\}_{n\geq 1}\,,
$$
% $\{\Hat{F}^n_{\cdot}/n^{1/2}\}_{n\geq 1}$ 
are tight sequences in $C_{\Rs}[0, T]$ for all $T>0$. Thus, the same holds for their difference.
%
\end{proof}


% \begin{proof}[Proof of Lemma~\ref{lem: Knt/n1/2tig}.] The proof follows the very same lines of that of Theorem~~\ref{pn-ERW-d=>4}, and we omit it. 
% {\color{red} 
% By  Remark~\ref{rem:conver}-$b)$  it suffices to show tightness in $C_{\Rs}[0, T]$ for all $T > 0$ and this is equivalent to show the the sequence of processes $\{|K_{\lfloor n \cdot \rfloor}|/n^{1/2}\}_{n\geq 1}$  satisfies  the two conditions in  Remark~\ref{rem:conver}-$c)$. 
% Note that $|K_{\lfloor n \cdot 0 \rfloor}|/n^{1/2}\equiv 0$ for all $n \ge 1$ and therefore the first condition in Remark~\ref{rem:conver}-$c)$ is satisfied.
% %
% To prove the second condition in Remark~\ref{rem:conver}-$c)$, 
% set 
% $
% A_t(\varepsilon, \phi):= \{f \in C_{\Rs}[0,T] : \sup_{t \le s \le t+\phi} |f(s) - f(t)| \ge \varepsilon \}$. 
% Then, following the same steps as in the proof of Theorem~\ref{pn-ERW-d=>4}, we have that  
% \begin{equation*}
% \begin{split}
% &\frac{1}{\phi} \PP [ |K_{\lfloor n \cdot \rfloor}|/n^{1/2} \in A_t(\varepsilon, \phi)]  = \frac{1}{\phi} \PP\Big[\sup_{t \le s \le t+\phi } | |K_{\lfloor ns \rfloor}| - |K_{\lfloor n t \rfloor}|| \ge \varepsilon n^{\frac{1}{2}} \Big]
% \\
% & = \frac{1}{\phi} \PP \Big[ \sum_{i = \lfloor nt \rfloor + 1}^{\lfloor n(t + \phi) \rfloor} \um_{E_i^c \cap \{U_i \le i^{-\frac{1}{2}}\}} \ge \varepsilon n^{\frac{1}{2}} \Big]
%  \le \frac{1}{\phi} \PP \Big[ \sum_{i = \lfloor nt \rfloor + 1}^{\lfloor n(t + \phi) \rfloor} \um_{\{U_i \le i^{-\frac{1}{2}}\}} \ge \varepsilon n^{\frac{1}{2}} \Big]\,.
% \end{split}    
% \end{equation*}
% From this point on, the computations are exactly the same used in the proof of Theorem~\ref{pn-ERW-d=>4}, and we omit it.

% when we show the second sum portion in~\eqref{p_n-ERW_incrementos_d=>4} fulfills the second condition in~\cite[Theorem 7.3]{billingsley1999probability}.
% Then we have that for each positive $\varepsilon$ and $\eta$, there exists a $\phi \in (0,1)$, and an integer $n_0$ such that
% \begin{equation*}
% \frac{1}{\phi} P_n \Big[f \in C_{\Rs}[0,T] : \sup_{t \le s \le t+\phi} |f(s) - f(t)| \ge \varepsilon \Big] \le \eta \, , \quad \forall n \ge n_0\,.
% \end{equation*}
% Ergo by~\cite[Theorem 7.3]{billingsley1999probability} we obtain that $|K_{\lfloor n \cdot \rfloor}|/n^{1/2}$ is tight in $C_{\Rs}[0, T]$ for all $T > 0$. Thus by~\cite[Theorem 2.4.10]{karatzas2012brownian} it is also a tight sequence of processes in $(C_{\Rs}[0, \infty),\rho)$
% } 
% \end{proof}

\begin{proof}[Proof of Proposition~\ref{prop: bound_Kn}.] 
We first prove $(a)$. 
Note that the statement follows if we show that
\begin{equation}\label{oldstatement}
\PP \left[\forall t \in [0,\infty): \mathcal{H}_t \le 2(t \delta)^{1/2} \right] = 1 \, ,  
\end{equation}
for every $\delta \in (\pi_d, 1)$. Indeed taking a sequence $\delta_n \downarrow \pi_d$ as $n\to \infty$, we have that 
$$
\Big\{ \forall t \in [0,\infty): \mathcal{H}_t \le 2(t \pi_d)^{1/2} \Big\} = \bigcap_{n\ge 1} \Big\{ \forall t \in [0,\infty):  \mathcal{H}_t \le 2(t \delta_n)^{1/2} \Big\}\, .
$$

Let us begin with some instrumental fact: recall (from page \pageref{eq: def Kn}) that $\varphi_i := \psi_i + 1$, where $\{\psi_i\}_{i \ge 1}$ denotes  the sequence of $\FF$-stopping times  corresponding to the  times the $p_n$-\Nametwo{} visits a new site, and let us define: 
\begin{align*}
J'_n(\delta) := 
\sum_{i=1}^{\delta n} \um_{\{U_{\varphi_i} \le \varphi_i^{-1/2} \}}\,,  
\end{align*} 
with $\delta \in (\pi_d, 1)$. 
%
Using Lemma \ref{lem: iid} and since $\{U_i\}_{i \ge 1}$ is i.i.d. we have that 
\begin{equation*}\label{eq:dominance}
J'_n(\delta)  \preceq J_n(\delta)\,, 
\end{equation*}
for all $\delta \in (\pi_d, 1)$ where $J_n(\delta)$ is defined  in~\eqref{Bn'}.

Consider the event 
\begin{equation*}
A_n^{{\delta}, c, M}:= \Big\{\forall t \in [c, M]: \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \le 2({\delta} t)^{1/2}  \Big\} \,,   
\end{equation*}
where $c$, $M$ and ${\delta}$ are positive constants such that $M > c$ and ${\delta} \in (\pi_d, 1)$, and recall that $|K_{\lfloor nt \rfloor}|  = \sum_{j=1}^{|\Rr_{\lfloor nt \rfloor -1}^X|} \um_{\{U_{\varphi_j} \leq \varphi_j^{-1/2} \}}$ (see,  \eqref{eq: def Kn}. For every $\Hat{\delta} \in (\pi_d,\delta)$ we have that
\begin{align}\label{eq: A_t^cM<}
\begin{split}
& \PP[A_n^{{\delta}, c,M}]  \ge \PP[A_n^{\delta, c,M} \cap \{ \forall t \in [c,M]: |\Rr_{\lfloor nt \rfloor}^X| \le \Hat \delta \lfloor nt \rfloor\}]
\\
& \ge \PP\Big[ \Big\{\forall t \in [c,M]: \frac{J'_{\lfloor nt \rfloor}(\delta)}{n^{1/2}} \le 2(\delta t)^{1/2} \Big\} \cap \big\{ \forall t \in [c,M]: |\Rr_{\lfloor nt \rfloor}^X| \le \Hat \delta \lfloor nt \rfloor \big\}  \Big] \, .
\end{split}    
\end{align}
Considering the second event on the right-hand side of~\eqref{eq: A_t^cM<}, by Proposition~\ref{prop:RangeERW}, for every $\Hat{\delta} \in (\pi_d, \delta)$ there exists an integer random variable $N_{\Hat{\delta}}$ such that $\PP[|\Rr_m^X| \le \Hat{\delta} m, \, \forall m \ge N_{\Hat{\delta}}] = 1$, hence 
$$\lim_{n \to \infty} \PP[\forall t \in [c,M]: |\Rr_{\lfloor nt \rfloor}^X| \le \Hat \delta \lfloor nt \rfloor] \ge \lim_{n \to \infty} \PP[N_{\Hat{\delta}} \le cn]=1.
$$
Now concerning the first event on the right-hand side of~\eqref{eq: A_t^cM<}, since $J'_n(\delta)  \preceq J_n(\delta)$, we obtain that 
\begin{equation*} 
\PP\Big[ \forall t \in [c,M]: \frac{J'_{\lfloor nt \rfloor}(\delta)}{n^{1/2}} \le 2(\delta t)^{1/2} \Big] \ge
\PP\Big[ \forall t \in [c,M]: \frac{J_{\lfloor nt \rfloor}(\delta)}{n^{1/2}} \le 2(\delta t)^{1/2} \Big] \, ,
\end{equation*}
%
where the right-hand side converges to one by Lemma~\ref{lem: tightaux} part $i)$ (convergence in distribution to a deterministic function implies convergence in probability, see \cite[page 27]{billingsley1999probability}).
%it holds that \com{notation of convergence of processes}
%\begin{equation}\label{eq:Jninprob}
%\frac{J_{\lfloor n \cdot \rfloor}}{n^{1/2}} \to 2(\delta \cdot)^{1/2}  \text{ as } n \to \infty  \,, 
%\end{equation}
%in probability, since it converges in distribution to a deterministic function in $C_{\mathbb{R}}[0,\infty)$ (see \cite[page 27]{billingsley1999probability}).
%
Hence on the right-hand side of~\eqref{eq: A_t^cM<} we have an intersection of two events whose probability converges to 1 as $n$ goes to infinity. Thus, for every $M > c >0$ and $\delta \in (\pi_d, 1]$ 
\begin{equation*}
\lim_{n\to \infty}  \PP[A_n^{\delta, c,M}]  = 1\,.
%\PP \Big[ \forall t \in [c, M]: \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \le 2(\Hat{\delta} t)^{1/2} \Big] \to 1 \text{ as } n \to \infty \,,   
\end{equation*}
%
%Since  $\hat{\delta} > \delta$ is arbitrary \com{I ?????}, we  obtain that 
%\begin{equation*}
%\PP \Big[ \forall t \in [c, M]: \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \le 2(\delta t)^{1/2}  \Big] \to 1 \text{ as } n \to \infty\,,   
%\end{equation*}
%
Now suppose that we have monotone decreasing and increasing  sequences $\{c_j\}_{j \ge 1}$ and $\{M_j\}_{j \ge 1}$ respectively, such that  $c_j \to 0$ and $M_j \to \infty$ as $j$ goes to infinity. Let $\{
\mathcal{H}_t\}_{t\ge 0}$ be a limit point in distribution of a subsequence of $\{\Hat{K}^n_{\cdot}/n^{1/2}\}_{n\ge 1}$, which is  tight  by  
Lemma~\ref{Jntight} (item $i)$), and define
\begin{equation*}
A^{\delta}:= \left\{\forall t \in [0, \infty): \mathcal{H}_t \le 2(\delta t)^{1/2}  \right\} \,,   
\end{equation*}
and 
$$
A^{\delta, c_i, M_i} = \left\{\forall t \in [c_i, M_i]: \mathcal{H}_t \le 2(\delta t)^{1/2}  \right\}\,,
$$
and $A^{\delta} = \cap_{i=1}^{\infty} A^{ \delta ,c_i, M_i}$ (since $H_0=0$).
%
Then,  by Portmanteau Theorem we have that for every $i \geq 1$ %\com{on the rhs we should specify the dependence on $\delta$ and add that for all $\delta \in (\pi_d,1]$...}
\begin{equation*}
\PP[A^{\delta ,c_i, M_i}] \ge \limsup_{n \to \infty} \PP[A_n^{\delta, c_i, M_i}] =1 \,.    
\end{equation*}
%
Hence, for all $i\geq 1$ we obtain that $\PP[A^{\delta, c_i, M_i}] = 1$ which, in turn, implies that 
\begin{equation}\label{eq:At_1}
\PP[A^{\delta}] = \PP\Big[ \bigcap_{j=1}^{\infty} A^{\delta, c_j, M_j}\Big] = 1 \,. 
\end{equation}
%
We now prove $(b)$. Analogously to the beginning of the proof of $(a)$, it is enough to prove that 
\begin{equation}\label{oldstatement}
\PP \left[\forall t \in [0,\infty): \mathcal{H}_t \ge  2t^{1/2}(1-(1 -  \delta')^{1/2})  \right] = 1 \, ,  
\end{equation}
for every  $\delta' \in (0, \pi_{d})$.
Indeed, if $\delta'_n \uparrow \pi_{d}$  as $n\to \infty$, we have that 
$$
\Big\{ \forall t \in [0,\infty): \mathcal{H}_t \ge 2t^{1/2}(1-(1 -  \pi_{d})^{1/2})  \Big\}\,,
$$
is equal to
$$
\bigcap_{n\ge 1} \Big\{ \forall t \in [0,\infty): \mathcal{H}_t \ge 2t^{1/2}(1-(1 - \delta'_n)^{1/2})  \Big\}\, .
$$
 Moreover, defining
\begin{align*}
V'_n(\delta') :=\sum_{i = 1}^{\delta' n} \um_{\{U_{\varphi_i} \leq (\varphi_i \wedge (n-i) )^{-1/2} \}}\,, %\label{eq: F'n_domsto}
\end{align*}
and using again Lemma \ref{lem: iid} and the i.i.d. property of $\{U_i\}_{i \ge 1}$, we have that 
\begin{equation*}
V'_n(\delta')  \succeq  V_n(\delta')\,, 
\end{equation*}for all $\delta' \in (0, \pi_{d})$
where $V_n(\delta')$ is defined in \eqref{Fn'}. 


We also define the following events 
\begin{equation*}
\begin{split}
& B_n^{\delta',c, M}:= \Big\{\forall t \in [c, M]: \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \ge 2t^{1/2}(1-(1-\delta')^{1/2}) \Big\} \,,
\\
& H_n^{\delta',c,M}:= \Big\{\forall t \in [c, M]: \frac{V'_{\lfloor nt \rfloor}(\delta')}{n^{1/2}} \ge 2t^{\frac{1}{2}}(1-(1-\delta')^{\frac{1}{2}})  \Big\}\,,
% \\
% & R_{\lfloor nt \rfloor}:= \big\{ \forall t \in [c,M]: |\Rr_{\lfloor nt \rfloor}^X| \ge \delta'' \lfloor nt \rfloor \big\} \,,
\end{split}
\end{equation*}
where $c$, $M$ and  $\delta'$  are positive constants such that $M > c$ and $\delta'\in (0,\pi_{d})$. 
%
Given $\delta'' \in (\delta', \pi_{d})$,  we have that
\begin{equation}\label{eq: B_t^cM>}
\begin{split}
\PP[B_n^{\delta',c,M}]  & \ge \PP\big[B_n^{\delta',c,M} \cap \big\{ \forall t \in [c,M]: |\Rr_{\lfloor nt \rfloor}^X| \ge \delta'' \lfloor nt \rfloor \big\}\big] 
\\
&\ge \PP\big[ H_n^{\delta',c,M} \cap \big\{ \forall t \in [c,M]: |\Rr_{\lfloor nt \rfloor}^X| \ge \delta'' \lfloor nt \rfloor \big\} \big]\,.
\end{split}
\end{equation}
%
Since $V'_n(\delta')  \succeq V_n(\delta')$, it holds that 
$$
\PP\big[ H_n^{\delta',c,M} \big]
\ge \PP \Big[ \forall t \in [c, M]: \frac{V_{\lfloor nt \rfloor}(\delta')}{n^{1/2}} \ge 2t^{1/2}(1-(1-\delta')^{1/2}) \Big]\, .
$$
%and by the coupling with the lazy random walk 
%$$
%\PP\big[ \forall t \in [c,M]: |\Rr_{\lfloor nt \rfloor}^X| \ge \delta'' \lfloor nt \rfloor  \big]
% \ge \PP \big[\forall 
% t \in [c,M]:|\Rr_{\lfloor nt \rfloor}^Y| \ge \delta'' \lfloor nt \rfloor\big] \,.    
%$$ 
Now by Lemma~\ref{lem: tightaux} part $ii)$  (convergence in distribution to a deterministic function implies convergence in probability, see \cite[page 27]{billingsley1999probability}) 
we obtain that $\lim_{n \to \infty}\PP\big[ H_n^{\delta',c,M} \big]=1$, for every $\delta' \in (0,\pi_{d})$. Moreover, since $d\geq 22$, we have that $\pi_{d}>0$ and by Proposition~\ref{prop:RangeERW_lower} there exists an integer random variable $N_{\delta''}$ such that $\PP [|\Rr_{m}^X| \ge \delta''m ,\ \forall m \ge N_{\delta''} ] = 1$, thus $\lim_{n\to \infty} \PP \big[\forall 
 t \in [c,M]:|\Rr_{\lfloor nt \rfloor}^X| \ge \delta'' \lfloor nt \rfloor\big] \ge \lim_{n\to \infty} \PP [N_{\delta''} \le cn ] = 1$, for every $\delta'' \in (0,\pi_{d})$.
%we obtain that
%\begin{equation}\label{eq:Fninprob}
%\frac{\sum_{j=1}^{\lfloor n\cdot \rfloor} \um_{\{ U_j \le j^{-1/2} \}} - F_{\lfloor n\cdot \rfloor}}{n^{\frac{1}{2}}} \to 2(\cdot)^{1/2}(1-(1-\delta')^{1/2}) \text{ as } n \to \infty  \,, 
%\end{equation}
%in probability \com{notation of convergence of processes!}, since it converges in distribution to a  continuous function in $t$ (see \cite[page 27]{billingsley1999probability}).
%
Since in~\eqref{eq: B_t^cM>} we have an intersection of two events whose probability converges to 1 as $n$ goes to infinity, we may conclude that for every $M > c >0$ 
\begin{equation*}
\PP \Big[ \forall t \in [c, M]: \frac{|K_{\lfloor nt \rfloor}|}{n^{1/2}} \ge 2t^{1/2}(1-(1-\delta')^{1/2}) \Big] \xrightarrow[n \to \infty]{} 1 \,.
\end{equation*} 
%
Finally, we finish the proof by the same argument used to obtain~\eqref{eq:At_1}.
\end{proof}

\section{On the range of $p_n$-\Nametwo{} for $\beta=1/2$ and $d\geq 2$}\label{sec:rangeERW}

\subsection{Upper bound for the range (proof of  Proposition~\ref{prop:RangeERW})}

\hfill \\

  
% Before we start, we introduce some auxiliary results required to analyze the asymptotic behavior of the $p_n$-\Nametwo{} on $\ZZ^d$, with $d\geq 2$ and $\beta = 1/2$.
%
% Let $\{\xi_i\}_{i \ge 1}$ be i.i.d. $\ZZ^d$-valued random variables with zero-mean vector and finite variance. Let $\{Y_n\}_{n \ge 0}$ be the random walk on $\ZZ^d$ with increments $\{\xi_i\}_{i \ge 1}$ starting at $Y_0 = 0$, thus $Y_n = \sum_{i=1}^{n} \xi_i$, $n\ge 1$.  For $m \leq n$ define 
% \[ \Rr_{[m,n]} ^Y := \{Y_m, Y_{m+1}, \ldots, Y_n\}\;,\]
% and denote by $\Rr_{n} = \Rr_{[0,n]}$,  the range of the random walk $\{Y_n\}_{n \geq 0}$.
%
% We now state a known result about the range of a random walk on $\ZZ^d$ with i.i.d. increments which is instrumental in our proof. Recall from the statement of Proposition~\ref{prop:RangeERW} that $\pi_d$ denotes the probability that $\{Y_n\}_{n \geq 0}$ never returns to the origin. 
%
% \begin{theorem}[{\cite[Theorem 1]{hamana2001large}}]\label{teo: RnZ>}
% Let $\{Y_n\}_{n \geq 0}$ be an aperiodic random walk on $\ZZ^d$, with $d\geq 2$. It holds that 
% \begin{align*}
% \tag{L}&\lim_{n \to \infty} \PP[|\Rr_{n}^Y| \geq \theta n]= 1\,, &&  \text{for every $\theta < \pi_d$}\,, 
% \\
% \tag{U}&\PP[|\Rr_{n}^Y| \geq \theta' n] \leq e^{-c_{\theta'}n}\,,    && \text{for every  $\theta' > \pi_d$ and $n$ sufficiently large}\,, 
% \end{align*}
% where $c_{\theta'}$ is a positive constant that depends of $\theta'$ (note that for $d=2$, we have that $\pi_d=0$, whereas for $d\geq 3 $, $\pi_d\in (0,1]$. 
% \end{theorem}
%
% \begin{remark}
% It is important to notice that~\cite[Theorem 1]{hamana2001large} is stated for aperiodic random walks. \cm{ The definition of aperiodicity applied in this Theorem is different than the usual one (see Yuval). here the definition used comes from Spitzer. There to a process be aperiodic means (...).  However, as explained in~\cite[page 188-beginning of Section 2]{hamana2001large}, this condition does not entail a loss of generality.} %However, as explained in~\cite[page 188-beginning of Section 2]{hamana2001large}, this condition can be relaxed to \com{include??} periodic random walks. \comu{incluir comentário sobre definição de aperiódico}
% \end{remark}
The heuristic of the proof of Proposition~~\ref{prop:RangeERW} is the following: 
suppose that the $p_n$-\Nametwo{} at time $j$ visits a new site and gets excited. Then, after $k$ further steps it visits another new site and gets excited again and  no excitation occurs between time $j+1$ and $j+k-1$. Thus,  we know that between time $j+2$ and $j+k$ the process evolves as a random walk with i.i.d. increments. Specifically, in each of these time windows between two consecutive excitations, we can use the range of the random walk with i.i.d. increments to  upper bound the range of the $p_n$-\Nametwo{}.

%The main idea of this proof is we know that the $p_n$-\Nametwo{} behaves like random walk biased in direction $\ell$ \comu{"drift random walk" não é um bom termo} when it eats a cookie. Then between two cookies we have that the process behaves like an i.i.d. random walk. Hence in this lengths, we think as independent i.i.d. random walks \comu{"i.i.d. random walk" não é um bom termo, talvez "random walk with i.i.d. increments. Recorrente.} and use the ranges of those process to upper bound the range of the $p_n$-\Nametwo{}. By Lemma~\ref{RnY_upperb} we can control the range of each independent random walk, thus we obtain the desired result.  \comu{Só neste parágrafos há vários problemas de inglês}

\begin{proof}[Proof of Proposition~\ref{prop:RangeERW}]
%\textcolor{red}{Let us denote $(N_i, i \geq 0)$  as the sequence of times that the process $X$ is allowed to eat a cookie, this happens if the position is being visited for the first time and the Bernoulli trial in the site goes in favor of the process with a drift

%\texttt{The above paragraph is confusing: $N_i$ only looks at the Bernoulli variables regardless if the sites visited at the corresponding time has already been visited before or not!}}. %, that is, it behaves like process with a drift to the right. 
%\cm{Let us denote $\{N_i\}_{i \geq 0}$ as the sequence of times that the Bernoulli trial goes in favor of the $p_n$-\Nametwo{} behaves like a random walk biased in direction $\ell$.}


%Let us denote $\{N_i\}_{i \geq 0}$ as the sequence of times that the Bernoulli trial in the site goes in favor of the process with a drift, independently of the process has visited the site or not \comu{É muito difícil entender esta última frase}.
We provide the proof for $\beta = 1/2$, which represents the most  challenging scenario. The proof for $\beta > 1/2$ can be derived by the same technique presented here.

Denote by $\{N_i\}_{i \geq 0}$ the sequence of stopping times 
\[ N_0 \equiv 0 \ \ \mathrm{and} \ \ N_i := \inf\{ k > N_{i-1}: Z_k = 1 \} \, , \ i\ge 1 \,, \]
where $Z_k = \um_{\{U_k \le k^{-1/2} \}}$, $k \geq 1$, are independent random variables with Bernoulli distribution of parameter respectively $k^{-1/2}$ for each $k$.
%
Set $\Delta N_i = N_i - N_{i-1} $ and define 
\[ M_n := \inf \Big\{ i \geq 1 : \sum_{j=1}^i \Delta N_j \geq n \Big\} \,. \]

Note that $|\Rr_n ^X| - 1  = \sum_{t=1}^n \um_{\{X_t \neq X_l, \forall l < t \}}$ is bounded from above by 
\begin{align*}
 &  \sum_{t=1}^{N_1} \um_{\{X_t \neq X_l, \forall l < t \}} + \sum_{t=N_1 +1}^{N_2} \um_{\{X_t \neq X_l, \forall l < t \}} + \dots + \sum_{t=N_{M_n -1} +1}^{N_{M_n}} \um_{\{X_t \neq X_l, \forall l < t \}}
 \\
 & \leq M_n + \sum_{j = 1}^{M_n} \sum_{t = N_{j-1} + 2}^{N_j} \um_{\{X_t \neq X_l, \forall l < t \}}\\
 &\leq M_n + \sum_{j = 1}^{M_n} \sum_{t = N_{j-1} + 2}^{N_j} \um_{\{X_t \neq X_l, \forall l \in [N_{j-1} + 1, t) \}} \,.
\end{align*}
In each time interval $[N_{j-1} + 2, N_j]$ the process $X$ behaves like a random walk with i.i.d. increments. In order to have some control on the length of these intervals, or equivalently on $\{\Delta N_j\}_{j \geq 1}$, we proceed as follows: Let $\varepsilon \in (0, 1)$ and note that 
\begin{equation}\label{eq: Rn<}
|\Rr_n ^X| \leq |\Rr_{[0,n^{\varepsilon}]} ^X| + |\Rr_{[n^{\varepsilon}, n]} ^X| \leq n^{\varepsilon} + |\Rr_{[n^{\varepsilon}, n]} ^X|\; , 
\end{equation}
this last step will be necessary to guarantee that after time $n^\varepsilon$ the time intervals $\Delta N_j$ are (in distribution) sufficiently large. Thus, we may just redefine $N_0 \equiv n^{\varepsilon}$ and apply the very same decomposition as before to obtain %\comu{Qual o efeito da escolha do $N_0$ do lado direito?} {\color{blue} (note que não podemos usar $\Rr^Y_{\Delta N_j}$ na soma abaixo, porque teríamos ranges sobre intervalos que não são disjuntos.)}
\begin{align} \label{pR_DNj}
|\Rr_{[n^{\varepsilon}, n]} ^X| -1 &\leq
 M_n + \sum_{j = 1}^{M_n} \sum_{t = N_{j-1} + 2}^{N_j} \um_{\{X_t \neq X_l, \forall l \in [N_{j-1}+1, t) \}} \nonumber
 \\ 
 & \leq  M_n + \sum_{j = 1}^{M_n} |\Rr_{[N_{j-1} + 2, N_j]} ^Y| \,. 
\end{align}
%where $\{Y_n\}_{n \geq 0}$ denotes a random walk whose i.i.d.~increments in $\ZZ^2$ are $\{\xi_i\}_{i \ge 1}$. 
%\cm{ Important to notice that $Y$ is independent of $N$, since the increments of $Y$ are determined by the sequence of $\{ \xi_i \}_{i \ge 1}$ which occur in the length $[N_{j-1}+ 2, N_j]$. ??} \comu{importante mencionar que Y é independente dos N's pois seus incrementos são deteminados pelos $\xi$'s.}
%
We pointed out that to deal properly with the rightmost sum in~\eqref{pR_DNj}, we need to keep in mind that $Y$ is independent of  $\{N_j\}_{j\ge 1}$.
Now, for any $k \in \{1, 2, \dots, n \}$ fixed, we define  the  random set 
\[
A_{n,k} \coloneqq \{ j \in \{1, 2, \dots, M_n \} : \Delta N_j \leq k \}\,,
\] 
and we write 
\begin{align*}
 \sum_{j = 1}^{M_n} |\Rr_{[N_{j-1} + 2, N_j]}^Y| & = \sum_{j \in A_{n,k}} |\Rr_{[N_{j-1} + 2, N_j]}^Y| + \sum_{j \in A_{n,k}^{c}} |\Rr_{[N_{j-1} + 2, N_j]}^Y| 
\\
& \leq (k-1)|A_{n,k}| + \sum_{j \in A_{n,k}^c} |\Rr_{[N_{j-1} + 2, N_j]}^Y| \;. 
\end{align*}
Using the simple inequality
$$
M_n = |A_{n,k}| + |A_{n,k}^c| \le |A_{n,k}| + \frac{M_n}{k} \le |A_{n,k}| + \frac{n}{k}\,,
$$
and \eqref{pR_DNj}, we obtain that 
\begin{equation}\label{R_DNj}
|\Rr_{[n^{\varepsilon}, n]} ^X| \le 1 +  \frac{n}{k} + k |A_{n,k}| + \sum_{j \in A_{n,k}^c} |\Rr_{[N_{j-1} + 2, N_j]}^Y|\,.
\end{equation}
To control the right-hand side of \eqref{R_DNj}, we begin with estimates on $|A_{n,k}|$. By coupling arguments, if $G$ denotes a Geometric random variable with parameter $n^{-\varepsilon/2}$, we have that  $G \preceq \Delta N_j$ for all $j \in \{1, 2, \dots, M_n \}$, where $\preceq$ means stochastic dominance. Thus, it holds that
\begin{equation}\label{eq: DNj<k}
\PP[\Delta N_j \leq k] \leq 1 - \left( 1 - \frac{1}{n^{\varepsilon/2}} \right)^k \,.    
\end{equation}

Since $|A_{n,k}| = \sum_{j=1}^{M_n} \um_{\{\Delta N_j \leq k \}}$ and $\{\Delta N_j\}_{j=1}^n$ are independet, using \eqref{eq: DNj<k},   for every $a>0$ and $n$ sufficiently large, it holds that
\begin{align*}
&\PP\Big[  \sum_{j=1}^{M_n} \um_{\{\Delta N_j \leq k \}}  > a \Big]  \leq  \PP\Big[  \sum_{j=1}^{n} \um_{\{\Delta N_j \leq k \}}  > a \Big]
%& \leq \PP\left[  \sum_{j=1}^{n} 1_{\{\Delta N_j \leq k \}}  > a \right]
%\\
 \leq \binom{n}{\lceil a \rceil} \PP[G \leq k]^{\lceil a \rceil} \,  
\\
& \leq \Big( \frac{ne}{\lceil a \rceil} \Big)^a \Big( 1 - \Big( 1 - \frac{1}{n^{\frac{\varepsilon}{2}}} \Big)^k \Big)^{\lceil a \rceil}
 \leq \Big( \frac{ne}{a} \Big)^{\lceil a \rceil} \Big( 1 - \exp{-\frac{3}{2}\frac{k} {n^{\frac{\varepsilon}{2}}}} \Big)^{\lceil a \rceil} 
 \\&\leq \Big( \frac{ne}{a} \times \frac{3k}{2n^{\frac{\varepsilon}{2}}} \Big)^{\lceil a \rceil} \,,
\end{align*}
where in the last inequalities we used that 
$\left(1-\frac{1}{x}\right)^x\geq e^{-3/2}$, $\forall x\geq 2$ (with $n$ sufficiently large) and that $1-e^{-x}\leq x$. 
Setting  $a= n^{1-\varepsilon/4}$ we obtain that 
\begin{equation}\label{eq:1_DNj<k}
\PP\Big[  \sum_{j=1}^{M_n} \um_{\{\Delta N_j \leq k \}}  > n^{1-\varepsilon/4} \Big] \leq \Big( \frac{3ek}{2n^{\frac{\varepsilon}{4}}} \Big)^{n^{1-\varepsilon/4}} \,.  
\end{equation}


We now set $k = \lceil \log^2 (n) \rceil$. With this choice, the deterministic first term in \eqref{R_DNj} divided by $n$ converges to zero.  Moreover, the sum in $n$ of the probabilities of the events $\{ |A_{n, \lceil \log^2 (n) \rceil}|  > n^{1-\varepsilon/4} \}$ is finite by \eqref{eq:1_DNj<k}. Thus, by Borel-Cantelli,  the second term in \eqref{R_DNj} divided by $n$ converges to zero almost surely.

Now we are left with the  analysis of the third term in~\eqref{R_DNj}. 
% First we will obtain an upper bound on the probability of the event that there exist at least one $j \in A_{n, \lceil \log^2 (n) \rceil}^c$, such that $|\Rr_{[N_{j-1} + 2, N_j]}^Y| > \gamma \Delta N_j$, where $\gamma \in (\pi_d, 1]$.
%\begin{align}\label{eq: R_DNj>y}
%\PP[\exists j \in A_{n, \lceil \log^2 (n) \rceil}^c : |\Rr_{[N_{j-1} + 2, N_j]}^Y| > \gamma \Delta N_j] & = \sum_{j=1}^{n} 1_{\{j \in A_{n, \lceil \log^2 (n) \rceil}^c \}} \PP[\Rr_{[N_{j-1} + 2, N_j]}^Y| > \gamma \Delta N_j]
%\\
%& \leq \sum_{j \in A_{n, \lceil \log^2 (n) \rceil}^c} \PP[\Rr_{[N_{j-1} + 2, N_j]}^Y| > \gamma (\Delta N_j-2)] \nonumber
%\\
%& \leq |A_{n, \lceil \log^2 (n) \rceil}^c| \exp \left(-c_{\gamma} \lceil \log^2 (n) -2\rceil \right) \nonumber
%\\
%& \leq \frac{n}{\lceil \log^2 (n) \rceil} \exp\left(-c_{\gamma} \lceil \log^2 (n) -2\rceil \right) \nonumber
%\\
%& \leq \exp\left( \log\left( \frac{n}{\lceil \log^2 (n) \rceil} \right) - c_{\gamma} \lceil \log^2 (n)-2 \rceil \right) \;. \nonumber
%\end{align}
%In the third inequality in~\eqref{eq: R_DNj>y} we use Lemma~\ref{RnY_upperb}. 
%
%Let us now analyze the probability term in~\eqref{eq: R_DNj>y}, remembering that $j \in A_{n, \lceil \log^2 (n) \rceil}^c$, then we have $\Delta N_j > \lceil \log^2 (n) \rceil$, for all $j$ 
%\begin{align}\label{eq: R_DNj>y2}
%\PP[\Rr_{[N_{j-1} + 2, N_j]}^Y| > \gamma \Delta N_j] & = \sum_{i=\lceil \log^2 (n) \rceil +1}^{\infty} 1_{\{\Delta N_j = i \}} \PP[\Rr_{[N_{j-1} + 2, N_j]}^Y| > \gamma(i-2)]
%\\
%& \leq \sum_{i=\lceil \log^2 (n) \rceil +1}^{\infty} 1_{\{\Delta N_j = i \}} \exp \left(-c_{\gamma} ( i -2)\right) \nonumber
%\\
%& \leq \exp \left(-c_{\gamma} \lceil \log^2 (n)\rceil -2 \right) \;. \nonumber
%\end{align}
%In the second inequality in~\eqref{eq: R_DNj>y2} we use Lemma~\ref{RnY_upperb}.
%
%Since we have
%\begin{equation}\label{eq: R_DNj>y3}
%|A_{n, \lceil \log^2 (n) \rceil}^c| \leq  \frac{n}{\lceil \log^2 (n) \rceil} \;,    
%\end{equation}
%for all $n$. We obtain in~\eqref{eq: R_DNj>y} from~\eqref{eq: R_DNj>y2} and~\eqref{eq: R_DNj>y3}
%\begin{align}\label{eq: gDNj}
%\PP[\exists j \in A_{n, \lceil \log^2 (n) \rceil}^c : |\Rr_{[N_{j-1} + 2, N_j]}^Y| > \gamma \Delta N_j] & \leq \frac{n}{\lceil \log^2 (n) \rceil} \exp\left(-c_{\gamma} \lceil \log^2 (n) -2\rceil \right)
%\\
%& \leq \exp\left( \log\left( \frac{n}{\lceil \log^2 (n) \rceil} \right) - c_{\gamma} \lceil \log^2 (n)-2 \rceil \right) \;. \nonumber
%\end{align}
%
First,  we observe that for all $n\geq 2 $
\begin{equation}\label{eq: R_DNj>y3}
|A_{n, \lceil \log^2 (n) \rceil}^c| \leq  \frac{n}{\lceil \log^2 (n) \rceil} \,.    
\end{equation}

By Theorem~\ref{teo: RnZ>} (LD),  for all $i> \lceil \log^2(n) \rceil$ (with   $n$ sufficiently large) and for all $\gamma \in (\pi_d,1]$, it holds that 
\begin{align}\label{eq: RYi}
\begin{split}
\PP[|\Rr_{i-2}^Y| > \gamma i] &\leq  \PP[|\Rr_{i-2}^Y| > \gamma (i-2)]
 \leq \exp(-c_{\gamma}(i-2)) 
\\
& \leq \exp(-c_{\gamma}(\lceil \log^2(n) \rceil-2)) \,.
\end{split}
\end{align}
Recalling that for all $j \in A_{n, \lceil \log^2 (n) \rceil}^c$ it holds that $\Delta N_j > \lceil \log^2 (n) \rceil$,   by~\eqref{eq: R_DNj>y3} and~\eqref{eq: RYi} we obtain that
\begin{align}\label{eq: gDNj}
\begin{split}
\PP & \left[\exists j \in A_{n, \lceil \log^2 (n) \rceil}^c : |\Rr_{[N_{j-1} + 2, N_j]}^Y| > \gamma \Delta N_j\right] 
 \leq \frac{n \exp\left(-c_{\gamma} \lceil \log^2 (n) -2\rceil \right)}{\lceil \log^2 (n) \rceil}  
\\
& \leq \exp\left( \log\left( \frac{n}{\lceil \log^2 (n) \rceil} \right) - c_{\gamma} \lceil \log^2 (n)-2 \rceil \right) \,.
\end{split}
\end{align}
%\textcolor{purple}{Remember that, for all $j \in A_{n, \lceil \log^2 (n) \rceil}^c$, then we have $\Delta N_j \in ( \lceil \log^2 (n) \rceil$, n]. Hence we obtain
%\begin{align*}
%\PP[\Rr_{[N_{j-1} + 2, N_j]}^Y| > \gamma \Delta N_j] & \leq \sum_{i=\lceil \log^2 (n) \rceil +1}^{n} \PP[\Rr_{i-2}^Y| > \gamma i| \Delta N_j = i]
%\\
%& \leq \sum_{i=\lceil \log^2 (n) \rceil +1}^{n} \PP[\Rr_{i-2}^Y| > \gamma (i-2)| \Delta N_j = i]
%\\
%& \leq \sum_{i=\lceil \log^2 (n) \rceil +1}^{n} \exp(-c_{\gamma}(i-2))
%\\
%& \leq (n-\lceil \log^2 (n) \rceil)\exp\left(-c_{\gamma} \lceil \log^2 (n) -2\rceil \right)\;. 
%end{align*}}
Since it holds that
% $\big\{ \exists j \in A_{n, \lceil \log^2 (n) \rceil}^c : |\Rr_{[N_{j-1} + 2, N_j]}^Y| > \gamma \Delta N_j\big\}$ contains 
\begin{align*}
&\left\{ \exists j \in A_{n, \lceil \log^2 (n) \rceil}^c : |\Rr_{[N_{j-1} + 2, N_j]}^Y| > \gamma \Delta N_j\right\} \\
&\supseteq
\Big\{\sum_{j \in A_{n,\lceil \log^2 (n) \rceil}^c} |\Rr_{[N_{j-1} + 2, N_j]}^Y| > \gamma n\Big\}\,,
\end{align*}
then this last event has probability bounded above by the rightmost term in \eqref{eq: gDNj}
%\begin{equation}\label{eq: sumRy}
%\begin{split}    
%    \PP\Big[ \sum_{j \in A_{n,k}^c} |\Rr_{[N_{j-1} + 2, N_j]}^Y| > \gamma n \Big] & \leq \exp\Big( \log\Big( \frac{n}{\lceil \log^2 (n) \rceil} \Big) - c_{\gamma} \lceil \log^2 (n)-2 \rceil \Big) \,.
    %\\
    %& \to 0 \quad \text{as } n \to \infty \,.
%\end{split}    
%\end{equation}
which is summable.
%$$\Big\{ \sum_{j \in A_{n,k}^c} |\Rr_{[N_{j-1} + 2, N_j]}^Y| > \gamma n \Big\}$$ 
Thus by Borell-Cantelli Lemma, the third term in~\eqref{R_DNj} divided by $n$ is bigger than $\gamma$ only finitely many times almost surely.
%
Hence,  letting $\gamma\downarrow \pi_d$ completes the proof. 
\end{proof}

\subsection{Lower bound for the range (proof of Proposition~\ref{prop:RangeERW_lower})}
\hfill \\

Given  $\beta\geq 1/2$ and  $\ell \geq 1$ set $Z_\ell^\beta=\um_{\{U_\ell \leq  \ell^{-\beta/2}\}}$, i.e., $Z_\ell^\beta$ is a random variable with Bernoulli distribution with parameter $\ell^{-\beta/2}$. 
Let us define the following set:
\begin{align}\label{eq:set_A}
A_{k,\delta, \beta} := \left\{\sum_{\ell = k}^{k + \lfloor k^\delta \rfloor}Z_\ell^\beta=0\right\}\,,
\end{align}
i.e.,  the event that in the first $\lfloor k^\delta \rfloor$ steps after time $k$ none of the corresponding Bernoulli are successful. In particular, the occurrence of the  event $A_{k,\delta, \beta}$ implies that the random walk does not get excited in the time window from $k$ to $k + \lfloor k^\delta \rfloor$. 

Before proving Proposition~\ref{prop:RangeERW_lower} we state a couple of auxiliary results. 
%
\begin{lemma}\label{lemma_1}  
Given $\beta\geq 1/2$ and  $\delta \in (0, 1/2)$  consider the sequence of events $\{A_{k, \delta,\beta}\}_{k\geq 1}$ defined in \eqref{eq:set_A}. Then, it holds that 
\begin{equation*}			
\lim_{n \to \infty} \frac{1}{n} \sum_{k = 1}^n \um_{A_{k,\delta, \beta}^c} = 0\,, \, \text{ a.s.}.
\end{equation*}
\end{lemma}

%%%%%% OLD LEMA %%%%%%%%%
% \begin{lemma}\label{lemma_1}  \com{maybe we should state this lemma for $\beta \geq 1/2$ to be more coherent with the rest....} 
% Let $\delta$ be a positive real number such that $\delta \in (0, 1/2)$ and we consider the event $A_{k, \delta}$ for $k \ge 1$. Then we have
% \begin{equation*}			
% \lim_{n \to \infty} \frac{1}{n} \sum_{k = 1}^n \um_{A_{k,\delta}^c} = 0\,, \, \text{ a.s.}.
% \end{equation*}
% \end{lemma}

The proof of Lemma~\ref{lemma_1} is given in  Appendix~\ref{sec:appendixC}.

\medskip

Given a $\mathbb{Z}^d$-valued process $\{S_n\}_{n\ge 1}$, we set
$$
e_k^S := \um_{\{ S_m \neq S_k \text{ for all } m > k \}} \,.
$$
Let $\mathcal{D}_n^S$ denote  the set of sites visited by the process $S$ up to time $n$  which are never revisited later. Then,  
\begin{equation*}
  |\mathcal{D}_n^S|= \sum_{k = 0}^{n} e_k^S \,.
\end{equation*}

\begin{lemma}\label{compXY}
Let  $X$ be a $p_n$-\Nametwo{} in direction $\ell$, on $\ZZ^d$ with $d\geq 22$, $p_n= \mathcal{C} n^{-\beta} \wedge 1$ and $\beta \ge 1/2$. For $k\ge 1$, let $Y^k =\{Y^k_i\}_{i\ge 0}$ denote a random walk on $\ZZ^d$ defined by $Y_0^k = X_k$ and for $n\geq 1$
\begin{equation*}
     Y_n^k = X_k + \sum_{i = 1}^{n}\xi_{k + i}\,.
\end{equation*}
Then, for any $\delta \in (0, 1/2)$ it holds that  
\begin{equation*}
\sum_{k = 1}^{\infty} \mathbb{P} \left( e_k^{X} \um_{A_{k,\delta,\beta}} < e_0^{Y^k} \um_{A_{k, \delta,\beta}} \right) < \infty \,.   
\end{equation*}
\end{lemma}

Before proving Lemma~\ref{compXY}, we show how the proof of Proposition~\ref{prop:RangeERW_lower} follows from it.
\medskip

\begin{proof}[Proof of Proposition~\ref{prop:RangeERW_lower}]
Note that $|\Rr_n^X| \ge |\mathcal{D}_n^X|$, since if $x \in \mathcal{D}_n^X$ then $x \in \Rr_n^X$. Therefore, it holds that 
\begin{align}\label{eq_4.10}
\liminf_{n \to \infty} \frac{|\Rr_n^X|}{n} &\ge \liminf_{n \to \infty} \frac{|\mathcal{D}_n^X|}{n}\nonumber \\
&= \liminf_{n \to \infty} \frac{1}{n}\sum_{k=1}^n e_k^X \um_{A_{k,\delta,\beta}} + \liminf_{n \to \infty} \frac{1}{n}\sum_{k=1}^n e_k^X \um_{A_{k,\delta,\beta}^c}\,.  
\end{align}
For $\delta \in (0, 1/2)$, Lemma~\ref{lemma_1} implies that 
\begin{equation}\label{eq_4.11}
    \liminf_{n \to \infty} \frac{1}{n}\sum_{k=1}^n e_k^X \um_{A_{k,\delta,\beta}^c}=0, \; \text{ a.s.}\,,
\end{equation}
whereas, from Lemma~\ref{compXY} together with Borel-Cantelli's Lemma, we conclude that 
\begin{equation}\label{eq_4.12}
\liminf_{n \to \infty} \frac{1}{n} \sum_{k = 1}^n e_k^X \um_{A_{k, \delta,\beta}} \ge \liminf_{n \to \infty} \frac{1}{n} \sum_{k = 1}^n e_0^{Y^k} \um_{A_{k, \delta,\beta}} \;, \text{ a.s.}\,.
\end{equation}
Note that
\begin{align*}
    e_0^{Y^k} = \um_{\{\sum_{i = k+1}^m\xi_{i} \neq 0, \, \forall \, m\ge k + 1\}} = \um_{\{\sum_{i = 1}^m\xi_{i} + Y_0\neq \sum_{i = 1}^k\xi_{i} + Y_0, \, \forall \, m\ge k + 1\}} =e^Y_k\,,
\end{align*}
where $Y=Y^0$ on the RHS above denotes a random walk with i.i.d. increments $\{\xi_i\}_{i\geq 1}$. 
Hence
\begin{equation}\label{eq_4.13}
    \liminf_{n \to \infty} \frac{1}{n} \sum_{k = 1}^n e_0^{Y^k} \um_{A_{k, \delta,\beta}} = \liminf_{n \to \infty} \frac{1}{n} \sum_{k = 1}^n e_k^{Y} \um_{A_{k, \delta,\beta}}\,. 
\end{equation}
Using again Lemma~\ref{lemma_1} and the known fact that $\lim_{n \to \infty} \frac{|\mathcal{D}_n^Y|}{n} = \pi_d$ (see e.g.~\cite{spitzer2001principles} page 39) we obtain that 
\begin{equation}\label{eq_4.14}
\lim_{n \to \infty} \frac{1}{n} \sum_{k = 1}^n e_k^Y \um_{A_{k, \delta,\beta}} = \pi_d \;, \text{ a.s.}\,.
\end{equation}
Then,  by equations \eqref{eq_4.10},\eqref{eq_4.11},\eqref{eq_4.12},\eqref{eq_4.13} and \eqref{eq_4.14}, we obtain that
$$
    \liminf_{n \to \infty} \frac{|\Rr_n^X|}{n} \ge \pi_d \; , \text{ a.s.}\,.
$$
\end{proof}

\begin{proof}[Proof of Lemma~\ref{compXY}] 
We conduct the proof for $\beta = 1/2$, which represents the most  challenging scenario. The case  $\beta > 1/2$ follows using the same computations and techniques presented here. To avoid clutter  we will denote $A_{k,\delta, 1/2}$ just by $A_{k,\delta}$.

Let $\nu_k^X$ be the first time the process $X$ returns to the site it visited at time $k$.  We also introduce the sequence $\{ \tau_i^k \}_{i \ge 0}$ where $\tau_i^k$ for $i \ge 1$ represents the $i$-th time  the  random walk gets excited  after time $k$. Specifically, 
\begin{align*}
& \nu^{X}_k:= \inf \{n \geq 1: X_{k+n}= X_{k}\}\,,
\\
& \tau^k_0\equiv 0, \text{ and }  \tau_i^k:=\inf\{n> \tau^k_{i-1}: X_{k+n} \text{ gets excited}\}, \text{ for $i\geq 1$}\,.
\end{align*}

For $k \geq 1$, let us also define a sequence of independent random variables $\{H_j^k\}_{j \ge 1}$, such that each $H_j^k$ has a geometric distribution with parameter $(k + j)^{-1/2}$ for all $j \ge 1$. Additionally, we introduce the following definitions   
\begin{align*}
& \mathcal{G}_m^k = \sum_{j = 1}^m H_j^k \,,
\\
& G^k_0\equiv 0 \text{ and }  G^k_i:= \inf\{\ell > G_{i-1}^k: Z_{\ell + k} =1\},  \text{ for $i\geq 1$}\,,
\end{align*}
where, $Z_\ell=\um_{\{U_\ell \leq  \ell^{-1/2}\}}$.
%\textcolor{red}{For each fixed $k \ge 1$ we define a coupling $\hat{\mathbb{P}}^k$ between the excited random walk $X$ and the random walk $Y$ with i.i.d. increments, defined such that $Y_0 = X_k$ and $Y_n := X_k + \sum_{i = k + 1}^{k + n} \xi_i$ for $n \ge 1$.} Suppose we have $\delta \in (0, 1/2)$. The first step in our proof is to establish the following
% \begin{equation*}
% \sum_{k = 1}^{\infty} \hat{\mathbb{P}} \left( e_k^{X} \um_{A_{k, \delta}} < e_k^{Y} \um_{A_{k, \delta}} \right) < \infty \,.   
% \end{equation*}
%
What we are after is to provide an upper bound for $\PP ( e_k^{X} \um_{A_{k, \delta}} < e_0^{Y^k} \um_{A_{k, \delta}} )$ for all $k \ge 1$. Recall that the process $X$ can be rewritten as in~\eqref{xn-incremento2}, i.e.
$$
X_0 = 0, \text{ and }\; X_n  = \sum_{i=1}^n \big( \um_{B_i^c} \xi_i + \um_{B_i} \gamma_i \big) \, , \ n\ge 1\,,
$$
where $B_i:= E_{i-1}^c \cap \{U_i \le i^{-1/2}\}$ for any $i \ge 1$. Then, we have that
\begin{equation*}
\begin{split}
& \mathbb{P}( e^{X}_k \um_{A_{k,\delta}} < e^{Y^k}_0 \um_{A_{k, \delta}})= \mathbb{P}(e^{X}_k=0, e^{Y^k}_0 =1, A_{k, \delta}) 
\\
& = \mathbb{P}(\nu^{X}_k < +\infty, e^{X}_k=0, e^{Y^k}_0 = 1, A_{k,\delta}) 
\\
& = \sum_{m=1}^{\infty} \mathbb{P}\left( \tau_{m}^k < \nu^{X}_k \leq \tau_{m+1}^k, e^{X}_k=0, e^{Y^k}_0 =1, A_{k, \delta} \right)
\\
& \le \sum_{m=1}^{\infty} \mathbb{P}\Big(\tau_{m}^k < \nu^{X}_k \leq \tau_{m+1}^k, \sum_{j = k+1}^{k+\nu^{X}_k} \left( \um_{B^{c}_j} \xi_j + \um_{B_j} \gamma_j \right) = 0, \sum_{j = k+1}^{k+\nu^{X}_k} \xi_j\neq 0, A_{k, \delta} \Big)\,.
\end{split}
\end{equation*}

It is important to notice that $\tau^k_i\geq G^k_i$ for all $i\geq 0$. Thus, we have that

% \textcolor{brown}{leonel: acho que o evento $A_{k,\delta}$ sobra nas fórmulas a seguir, uma vez que temos $G_m^k = l  + \lfloor k^{\delta}\rfloor$}\comu{concordo.}

\begin{equation*}
\begin{split}
& \mathbb{P}  ( e^{X}_k \um_{A_{k,\delta}} < e^{Y^k}_0 \um_{A_{k, \delta}})
\\
& \le \sum_{m=1}^{\infty} \sum_{\ell = m}^{\infty} \mathbb{P}\Big( G_m^k < \nu^{X}_k, \sum_{j=k+1}^{k+\nu^{X}_k} \xi_j = \sum_{j=1}^{m} \big( \xi_{\tau_j^k} - \gamma_{\tau_j^k} \big), G_m^k = \ell + \lfloor k^{\delta} \rfloor, A_{k, \delta} \Big)
\\
& \leq \sum_{m=1}^{\infty} \sum_{\ell=m}^\infty \sum_{n=\ell+\lfloor k^\delta\rfloor + 1}^{\infty} \mathbb{P}\Big( \nu^{X}_k=n, \sum_{j=k+1}^{k+n} \xi_j = \sum_{j=1}^{m} \big( \xi_{\tau_j^k} - \gamma_{\tau_j^k} \big), G_m^k = \ell + \lfloor k^\delta\rfloor \Big) 
\\
& \leq \sum_{m=1}^{\infty} \sum_{\ell=m}^\infty \sum_{n=\ell+\lfloor k^\delta\rfloor + 1}^{\infty} \mathbb{P}\Big(\sum_{j=k+1}^{k+n}\xi_j = \sum_{j=1}^{m}\big(\xi_{\tau_j^k} - \gamma_{\tau_j^k}\big), G_m^k = \ell + \lfloor k^\delta\rfloor \Big) \,.
\end{split}    
\end{equation*}

Let $\alpha \in (1/2,1)$ be a parameter to be determined later and let $\mu$ denote the mean vector of $\gamma$. For $m\geq 1$,  let us define 
\[
D_m:= \left\{x+y: x \in B(0, 2Km^\alpha) \text{ and } y \in B(m\mu, 2Km^\alpha)\right\}\,,
\]
where   $B(a, R):=\{x\in \mathbb{Z}^d: \Vert x-a\Vert^2\leq R\}$ with  $a \in \mathbb{Z}^d$ and $R \in [0,+\infty)$. 
Then we have that
\begin{equation}\label{eq:decomposition}
\begin{split}
& \mathbb{P}\Big(\sum_{j=k+1}^{k+n}\xi_j = \sum_{j=1}^{m} \big( \xi_{\tau_j^k} - \gamma_{\tau_j^k}\big), G_m^k = \ell + \lfloor k^\delta\rfloor \Big) 
=  
\\
& \sum_{z \in D_m} \mathbb{P}\Big(
%\nu^{\rm ERW}_k=n,
\sum_{j=k+1}^{k+n}\xi_j =z, \sum_{j=1}^{m} \big( \xi_{\tau_j^k} - \gamma_{\tau_j^k} \big) = z, G_m^k = \ell + \lfloor k^\delta\rfloor  \Big)
\\
&+ \sum_{z \in D^\complement_m} \mathbb{P} \Big(
%\nu^{\rm ERW}_k=n, 
\sum_{j=k+1}^{k+n}\xi_j = z, \sum_{j=1}^{m} \big( \xi_{\tau_j^k} - \gamma_{\tau_j^k} \big) = z, G_m^k = \ell + \lfloor k^\delta\rfloor \Big)\,.    
\end{split}
\end{equation}

We will analyze the two terms on the RHS of ~\eqref{eq:decomposition} separately. For the  first sum portion it holds that
\begin{equation}\label{eq_sum1<1}
\begin{split}
\sum_{z \in D_m} & \mathbb{P} \Big(
\sum_{j=k+1}^{k+n} \xi_j = z, \sum_{j=1}^{m} \big( \xi_{\tau_j^k} - \gamma_{\tau_j^k} \big) = z, G_m^k = \ell + \lfloor k^\delta\rfloor \Big)
\\
& \leq \sum_{z \in D_m} \mathbb{P}\Big(
\sum_{j=k+1}^{k+n}\xi_j = z \Big) \mathbb{P} ( G_m^k = \ell + \lfloor k^\delta\rfloor) 
% \\
% & \leq |D_m| \frac{C_d}{n^{d/2}} \hat{\mathbb{P}}(A_k, G_m^k = \ell + \lfloor k^\delta\rfloor) 
\\
& \leq (4Km^\alpha)^d \frac{C_d}{n^{d/2}} \mathbb{P}( G_m^k = \ell + \lfloor k^\delta\rfloor)\,,    
\end{split}    
\end{equation}
where, in the second inequality in~\eqref{eq_sum1<1} we use the fact that $|D_m| \le (4Km^{\alpha})^d$ and the Local Central Limit Theorem 
% (see inequality (2.4) in \cite[Theorem 2.1.1]{lawler2010random} and commentary in page 24)  
see, e.g., Inequality (2.8) in \cite[Theorem 2.1.3]{lawler2010random}  and the fact that $\Bar{p}_n(x) \le C_d n^{-d/2}$, with $C_d$ a positive constant. For the second term in~\eqref{eq:decomposition} we have
\begin{equation}\label{eq_sum<2}
\begin{split}
&\sum_{z \in D^c_m} \mathbb{P}\Big(
%\nu^{\rm ERW}_k=n, 
\sum_{j=k+1}^{k+n} \xi_j = z, \sum_{j=1}^{m} \big( \xi_{\tau_j^k} - \gamma_{\tau_j^k} \big) = z, G_m^k = \ell + \lfloor k^\delta\rfloor \Big)
\\
& \le \sum_{z \in D^c_m \cap B(0,2Km)} \mathbb{P}\Big(
%\nu^{\rm ERW}_k=n, 
\sum_{j=k+1}^{k+n} \xi_j = z,\sum_{j=1}^{m}\big( \xi_{\tau_j^k} - \gamma_{\tau_j^k} \big) = z \Big) %, A_k, G_m^k = \ell + \lfloor k^\delta\rfloor \right)
\\
&\leq \mathbb{P}\Big(
%\nu^{\rm ERW}_k=n, 
\sum_{j=k+1}^{k+n} \xi_j \in B(0,2Km) \setminus D_m, \sum_{j=1}^{m} \big( \xi_{\tau_j^k} - \gamma_{\tau_j^k} \big) \in B(0,2Km) \setminus D_m \Big) %, A_k, G^k_m = \ell + \lfloor k^\delta\rfloor \right) 
\\
&\leq \mathbb{P}\Big(
%\nu^{\rm ERW}_k=n, 
\sum_{j=k+1}^{k+n} \xi_j \in B(0,2Km) \setminus D_m \Big)^{1/2} \mathbb{P}\Big( \sum_{j=1}^{m} \big( \xi_{\tau_j^k} - \gamma_{\tau_j^k} \big) \in B(0,2Km) \setminus D_m \Big)^{1/2} \,, \end{split}    
\end{equation}
where, in the last  inequality above, we used Cauchy-Schwarz inequality. Regarding the first term in~\eqref{eq_sum<2}, by using again the Local Central Limit Theorem, it holds that 
\begin{equation}\label{eq_sum<2.1}
\begin{split}
\mathbb{P}\Big(
%\nu^{\rm ERW}_k=n, 
\sum_{j=k+1}^{k+n}\xi_j \in B(0,2Km) \setminus D_m \Big) & = \sum_{z \in D^c_m \cap B(0,2Km)} \mathbb{P}\Big(
%\nu^{\rm ERW}_k=n, 
\sum_{j=k+1}^{k+n} \xi_j = z\Big)
\\
& \leq \frac{C_d}{n^{d/2}} |B(0,2Km)|\,.
\end{split}
\end{equation}
For the second term in~\eqref{eq_sum<2}, let us define $F := B(0,2Km) \setminus D_m$, $G := B(0,2Km) \setminus B(0,2Km^{\alpha})$ and $A := B(0, 2Km) \setminus B(m\mu, 2Km^{\alpha})$. We then  obtain
\begin{equation}\label{eq_sum<2.2}
\begin{split}
& \mathbb{P}\Big(\sum_{j=1}^{m} \big(\xi_{\tau_j^k} - \gamma_{\tau_j^k} \big) \in F \Big) \leq \mathbb{P}\Big( \sum_{j=1}^{m} \xi_{\tau^k_j} \in G \Big)  + \mathbb{P}\Big(\sum_{j=1}^{m} \gamma_{\tau^k_j} \in A \Big) %\Big| A_k, G^k_m = \ell + \lfloor k^\delta\rfloor \right)
\\
& = \mathbb{P}\Big(\sum_{j=1}^{m}\xi_{j} \in G \Big) + \hat{\mathbb{P}}\Big(\sum_{j=1}^{m}\gamma_{j} \in A \Big) 
\\
& = \mathbb{P}\Big(\sum_{j=1}^{m}\xi_{j} \in G \Big) + \mathbb{P}\Big( \sum_{j=1}^{m}(\gamma_{j}-\mu) \in B(-m\mu,2Km) \setminus  B(0,2Km^\alpha)\Big) 
% \\
% & \leq 2|B(0,2Km)| \Hat{C}_{d,K} \Big[ e^{-2dK^2m^{2\alpha-1}}\Big(\frac{1}{m^{\frac{d}{2}}} + m^{\frac{7d-2}{2}} + \frac{1}{m^{\frac{d+2}{2}}} \Big)  + \frac{1}{m^{\frac{9d-1}{2}}} \Big]
% \\
% & \leq 2|B(0,2Km)| \Hat{C}_{d,K} \Big(m^{4d} e^{-2dK^2m^{2\alpha-1}} + \frac{1}{m^{\frac{9d-1}{2}}} \Big)\,. 
\\
& 
\leq 2|B(0,2Km)| \Hat{C}_{d,K} \Big[ e^{-2d\delta K^2m^{2\alpha-1}}\Big(\frac{1}{m^{\frac{d}{2}}} + \frac{(2Km)^{8d}}{m^{4d}m^{\frac{d+2}{2}}} + \frac{1}{m^{\frac{d+2}{2}}} \Big)  + \frac{1}{m^{\frac{9d-1}{2}}} \Big]
\\
& 
\leq 2|B(0,2Km)| \widetilde{C}_{d,K} \Big(m^{4d} e^{-2d \delta K^2m^{2\alpha-1}} + \frac{1}{m^{\frac{9d-1}{2}}} \Big)\,. 
\end{split}    
\end{equation}
In~\eqref{eq_sum<2.2}, we applied Lemma~\ref{lem: iid} to achieve the first equality. For the first inequality, we utilized the Local Central Limit Theorem see, e.g., Inequality (2.8) in \cite[Theorem 2.1.3]{lawler2010random} with $k=8d$
 and the fact that $\Bar{p}_n(x) \le C_d n^{-d/2}\exp(-||x||^2 \delta d/2n)$), with $\delta >0$ and the constant $C_d$ depending on the covariance matrix of $\xi$ (see, e.g., Equations~2.2 and 1.1 and Proposition~1.1.1 (c) in \cite{lawler2010random}).  
 %along with the observation that $(2Km^{\alpha})^2 > m$, which follows since, by definition, $\alpha > 1/2$. The final inequality is derived from the fact that $\alpha < 1$. 
%
Thus, using~\eqref{eq_sum<2.1} and~\eqref{eq_sum<2.2} we obtain that the rightmost term in~\eqref{eq_sum<2} is bounded above by 
\begin{equation}\label{eq:sum<2.3}
\begin{split}
%\sum_{z \in D^\complement_m} &\hat{\mathbb{P}}\left(
%\nu^{\rm ERW}_k=n, 
%\sum_{j=k+1}^{k+n}\xi_j =z, \sum_{j=1}^{m}\left(\xi_{\tau_j^k} - \gamma_{\tau_j^k}\right)=z \right) %, A_k, G_m^k = \ell + \lfloor k^\delta\rfloor  \right)
%\\
\Big[ 2 & |B(0,2Km)| \widetilde{C}_{d,K} \Big(m^{4d} e^{-2d\delta K^2m^{2\alpha-1}} + m^{-\frac{9d-1}{2}} \Big) \Big]^{1/2} \Big[ \frac{C_d}{n^{d/2}} |B(0,2Km)|\Big]^{1/2}
%\Big(4 & |B(0,2Km)|  \frac{C_d^2}{n^{d/2}}\Big)^{1/2} \Big( 2m^{3d+9} e^{-\frac{d(2Km^\alpha)^2}{2m}} + m^{-(d+8)}\Big)^{1/2} %\hat{\mathbb{P}}\left(A_k, G_m^k = \ell + \lfloor k^\delta\rfloor  \right)
\\
& \le \frac{m^d\Tilde{C}_{d,K}}{n^{d/4}} \Big(m^{4d} e^{-2d\delta K^2m^{2\alpha-1}} + m^{-\frac{9d-1}{2}} \Big)^{1/2} 
\\
& \le \frac{m^d\Tilde{C}_{d,K}}{n^{d/4}} \Big(m^{2d} e^{-d\delta K^2 m^{2\alpha-1}} + m^{-\frac{9d-1}{4}} \Big)\,.
%&\leq 2(2Km)^{d/2} \frac{C_d}{n^{d/4}} \Big( 2^{\frac{1}{2}}m^{\frac{3}{2}(d+3)} e^{-\frac{d(2Km^\alpha)^2}{4m}} + m^{-\frac{1}{2}(d+8)}\Big) \,.
% \hat{\mathbb{P}}\left(A_k, G_m^k = \ell + \lfloor k^\delta\rfloor  \right) 
% \\
% &=2^{1/2} (2Km)^d \left(\frac{C_d}{m^{d/2}}\right)^{1/2}  e^{-\frac{d(2Km^\alpha)^2}{4m}} \left(\frac{C_d}{n^{d/2}}\right)^{1/2} 
% %\hat{\mathbb{P}}\left(A_k, G_m^k = \ell + \lfloor k^\delta\rfloor  \right) 
% \,.
\end{split}
\end{equation}
Overall, by~\eqref{eq_sum1<1} and~\eqref{eq:sum<2.3} we obtain that

\begin{equation*}
\begin{split}
& \hat{\mathbb{P}}  ( e^{X}_k \um_{A_{k,\delta}} < e^{Y}_k \um_{A_{k, \delta}})  \leq\underbrace{\sum_{m=1}^{\infty} \sum_{\ell=m}^\infty  \sum_{n=\ell+ \lfloor k^\delta\rfloor + 1}^{\infty} (4Km^\alpha)^d \frac{C_d}{n^{d/2}} \mathbb{P}(G_m^k = \ell + \lfloor k^\delta\rfloor) }_{SUM_1}
\\
&+\underbrace{\sum_{m=1}^{\infty} \sum_{\ell = m}^\infty \sum_{n = \ell + \lfloor k^\delta\rfloor + 1}^{\infty} \frac{m^d\Tilde{C}_{d,K}}{n^{d/4}} \Big(m^{2d} e^{-d\delta K^2 m^{2\alpha-1}} + m^{-\frac{9d-1}{4}} \Big) }_{SUM_2}\,. %\hat{\mathbb{P}}\left(A_k, G_m^k = \ell + \lfloor k^\delta\rfloor \right)}_{SUM_2} \,. 
\end{split}
\end{equation*}

We will prove that $ \sum_{k \ge 1}\mathbb{P} ( e^{X}_k \um_{A_{k,\delta}} < e^{Y^k}_0 \um_{A_{k, \delta}}) < \infty$ by demonstrating that both $SUM_1$ and $SUM_2$ are summable over $k$. We begin with $SUM_1$  for which we obtain that 
\begin{equation*}
\begin{split}
& SUM_1 = \sum_{m=1}^{\infty} (4Km^\alpha)^d \sum_{\ell=m}^\infty \mathbb{P}(G_m^k = \ell + \lfloor k^\delta\rfloor) \sum_{n=\ell+ \lfloor k^\delta\rfloor + 1}^{\infty}  \frac{C_d}{n^{d/2}} 
\\
& \le C_d \sum_{m=1}^{\infty} (4Km^\alpha)^d \sum_{\ell = m}^\infty \mathbb{P}( G_m^k = \ell + \lfloor k^\delta\rfloor) \int_{\ell + \lfloor k^\delta\rfloor}^\infty  \frac{1}{x^{d/2}}dx
\\
& \overset{d>2}{=} \frac{C_d (4K)^d}{d/2-1}\sum_{m=1}^{\infty}    m^{\alpha d}   \sum_{\ell=m}^\infty \mathbb{P} (G_m^k = \ell + \lfloor k^\delta\rfloor) \frac{1}{(\ell + \lfloor k^\delta\rfloor)^{d/2-1}}
\\
& \leq C_{d,K} \sum_{m=1}^{\infty}    m^{\alpha d}   \sum_{\ell=m}^\infty \mathbb{P}(G_m^k = \ell) \frac{1}{(\ell + \lfloor k^\delta\rfloor)^{d/2-1}} 
\\
& \leq C_{d,K} \sum_{m=1}^{\infty}    m^{\alpha d} \,\mathbb{E}\Big[ \frac{1}{( G_m^k + \lfloor k^\delta\rfloor)^{d/2-1}} \Big] \,.
\end{split}    
\end{equation*}

We first note that $G^k_m \ge m$ and stochastically dominates $\mathcal{G}_m^k$, and recall that $ \mathcal{G}_m^k = \sum_{j=1}^m H^k_j$, where $\{H^k_j\}_{j\geq 1}$ are independent random variables with geometric distribution with parameter $(k+j)^{-1/2}$ for each $j \ge 1$. Now, we will compute an upper bound for $\mathbb{E}[(G_m^k + \lfloor k^\delta \rfloor)^{1-d/2}]$. 
\begin{equation*}
\begin{split}
&\mathbb{E}[(G_m^k + \lfloor k^\delta \rfloor)^{1-d/2}] = 
\\
& \mathbb{E}\Big[ \frac{1}{( G_m^k + \lfloor k^\delta\rfloor)^{d/2-1}} ; G_m^k < \frac{m^{\frac{3}{2}}}{6} \Big] +  \mathbb{E}\Big[ \frac{1}{( G_m^k + \lfloor k^\delta\rfloor)^{d/2-1}} ; G_m^k \ge \frac{m^{\frac{3}{2}}}{6} \Big]
\\
& \leq \frac{1}{( m + \lfloor k^\delta\rfloor)^{d/2-1}} \mathbb{P} \Big[ G_m^k < \frac{m^{\frac{3}{2}}}{6} \Big] + \frac{6^{d/2-1}}{(m^{3/2} + 6 \lfloor k^\delta\rfloor)^{d/2-1}}
\\
& \leq \frac{1}{( m + \lfloor k^\delta\rfloor)^{d/2-1}} \mathbb{P} \Big[ \mathcal{G}_m^k < \frac{m^{\frac{3}{2}}}{6} \Big] + \frac{6^{d/2-1}}{(m^{3/2} + 6 \lfloor k^\delta\rfloor)^{d/2-1} }\,.
\end{split}
\end{equation*}
%
Then, we use Lemma~\ref{lem:geo_sum_bound} for $\PP[\mathcal{G}_m^k < 6^{-1}m^{\frac{3}{2}}]$ with $\theta = 5$ and choose $\alpha=1/2 + 1/(cd)>1/2$, where $c$ is a sufficiently large positive constant, to obtain  the following 
\begin{equation}\label{eq:sum1_2}
\begin{split}
& SUM_1  \leq C_{d,K} \Big(\sum_{m=1}^{\infty}    \frac{m^{d/2 + 1/c - 5}}{(m+\lfloor k^\delta\rfloor)^{d/2-1}} + 6^{d/2-1} \sum_{m = 1}^{\infty} \frac{m^{d/2 + 1/c} }{(m^{3/2} + 6 \lfloor k^\delta\rfloor)^{d/2-1}}\Big)
\\
& \leq C_{d,K}' \Big(\sum_{m=1+\lfloor k \rfloor^\delta}^{\infty}    \frac{m^{d/2 + 1/c - 5}}{m^{d/2-1}} +  \int_0^{\infty} \frac{(u^{2/3})^{d/2 + 1/c}}{(u + 6 \lfloor k^{\delta} \rfloor)^{d/2 - 1}} \times \frac{2}{3} u^{-1/2} du \Big)
\\
& \leq C_{d,K}' \Big(\int_{\lfloor k^\delta \rfloor}^{\infty}\frac{1}{x^{4-1/c}}dx + 
\int_0^{\infty} \frac{(u + 6\lfloor k^{\delta} \rfloor)^{2/3(d/2 + 1/c) - 1/2}}{(u + 6 \lfloor k^{\delta} \rfloor)^{d/2 - 1}} du \Big)
\\
& \le C_{d,K,c} \Big( \frac{1}{(\lfloor k^\delta \rfloor)^{(3 -1/c)}} + 
\int_{6\lfloor k^{\delta} \rfloor}^{\infty} \frac{w^{2/3(d/2 + 1/c) - 1/2}}{w^{d/2 - 1}} dw \Big)
\\
& \le C_{d,K,c}'' \Big( \frac{1}{(\lfloor k^\delta \rfloor)^{(3 -1/c)}} + \frac{1}{\lfloor k^{\delta} \rfloor^{d/6 - 3/2 -2/3c}} \Big)\,,
\end{split}    
\end{equation}
where, in the second inequality of~\eqref{eq:sum1_2}, we used the change of variable  $u = m^{2/3}$ and, in the fifth inequality, we again changed variable with $w = u + 6\lfloor k^{\delta} \rfloor$. 

Now we will compute an upper bound for $SUM_2$.
\begin{equation}\label{eq:sum2_1}
\begin{split}
& SUM_2 = \Tilde{C}_{d,K} \sum_{m=1}^{\infty} m^d \Big( m^{2d} e^{-d\delta K^2 m^{2\alpha-1}} + \frac{1}{m^{\frac{9d-1}{4}}} \Big) \sum_{\ell = m}^\infty \sum_{n = \ell + \lfloor k^\delta\rfloor + 1}^{\infty} n^{-d/4}
\\
& \le \Tilde{C}_{d,K} \sum_{m=1}^{\infty} m^d \Big( m^{2d} e^{-d\delta K^2 m^{2\alpha-1}} + \frac{1}{m^{\frac{9d-1}{4}}} \Big) 
\sum_{\ell = m}^\infty \frac{1}{(\lfloor k^\delta\rfloor + \ell)^{\frac{d}{4}-1}}
\\
& \le \Tilde{C}_{d,K} \sum_{m=1}^{\infty} m^d \Big( m^{2d} e^{-d\delta K^2 m^{2\alpha-1}} + \frac{1}{m^{\frac{9d-1}{4}}} \Big) 
\sum_{\ell = \lfloor k^\delta\rfloor + m}^\infty \frac{1}{\ell^{\frac{d}{4}-1}}
\\
& \le \Tilde{C}_{d,K} \sum_{m=1}^{\infty} m^d \Big( m^{2d} e^{-d\delta K^2 m^{2\alpha-1}} + \frac{1}{m^{\frac{9d-1}{4}}} \Big)  \frac{1}{(m+ \lfloor k^\delta\rfloor -1)^{\frac{d}{4}-2}}
\\
& \le \frac{C_{d,K}'}{\lfloor k^\delta\rfloor^{d/4-2}} \sum_{m=1}^{\infty} m^d \Big( m^{2d} e^{-d\delta K^2 m^{2\alpha-1}} + \frac{1}{m^{\frac{9d-1}{4}}} \Big) \le \frac{C_{d,K}''}{\lfloor k^\delta\rfloor^{d/4-2}} \,,
\end{split}    
\end{equation}
where, in the last inequality of~\eqref{eq:sum2_1} we used that $\alpha > 1/2$. 
%
Now we use the bounds in~\eqref{eq:sum1_2} and~\eqref{eq:sum2_1} and, since $d \ge 22$, we finally obtain that
\begin{equation*}
\begin{split}
& \sum_{k=1}^\infty \mathbb{P}  ( e^{X}_k \um_{A_{k,\delta}} < e^{Y^k}_0 \um_{A_{k, \delta}}) 
\\
& \le \sum_{k = 1}^\infty \left(C_{d,K,c}'' \Big( \frac{1}{(\lfloor k^\delta \rfloor)^{(3 -1/c)}} + \frac{1}{\lfloor k^{\delta} \rfloor^{d/6 - 3/2 -2/3c}} \Big) +  \frac{C_{d,K}''}{\lfloor k^\delta\rfloor^{d/4-2}} \right)< \infty \,.
\end{split}
\end{equation*}
\end{proof}



%{\color{red} I will state the  result as a lemma.... something like
%\begin{lemma}\label{compXY}
%Let  $X$ be a $p_n$-\Nametwo{} in direction $\ell$, on $\ZZ^d$ with $d\geq 22$, $p_n= \mathcal{C} n^{-\beta} \wedge 1$ and $\beta \ge 1/2$ and let $Y$ denote a random walk on $\ZZ^d$ with i.i.d. increments given by  $\{\xi_i\}_i$. For $\delta \in (0, 1/2)$ and for every fixed $k\geq 1$ there exists a coupling  $\hat{\mathbb{P}}_k$ between $X$ and  $Y$ such that 
%\begin{equation*}
%\sum_{k = 1}^{\infty} \hat{\mathbb{P}}_k \left( e_k^{X} \um_{A_{k, \delta}} < e_k^{Y} \um_{A_{k, \delta}} \right) < \infty \,.   
%\end{equation*}
%\end{lemma}

%\begin{proof}[Proof of Proposition~\ref{prop:RangeERW_lower}]
%Note that $|\Rr_n^X| \ge |\mathcal{D}_n^X|$, since if $x \in \mathcal{D}_n^X$ then $x \in \Rr_n^X$ (and it is possible for a $x \in \Rr_n^X$ to be  revisited later on, thus implying $x \notin \mathcal{D}_n^X $). Therefore, it holds that 
%\begin{align*}
%\liminf_{n \to \infty} \frac{|\Rr_n^X|}{n} \ge \liminf_{n \to \infty} \frac{|\mathcal{D}_n^X|}{n} = \liminf_{n \to \infty} \frac{1}{n}\sum_{k=1}^n e_k^X \um_{A_{k,\delta}} + \liminf_{n \to \infty} \frac{1}{n}\sum_{k=1}^n e_k^X \um_{A_{k,\delta}^c}\,.  
%\end{align*}
%For $\delta \in (0, 1/2)$, Lemma~\ref{lemma_1} implies that $\liminf_{n \to \infty} \frac{1}{n}\sum_{k=1}^n e_k^X \um_{A_{k,\delta}^c}=0$ a.s., whereas from Lemma~\ref{compXY} together with Borel-Cantelli's Lemma we conclude that 
%\begin{equation*}
%\liminf_{n \to \infty} \frac{1}{n} \sum_{k = 1}^n e_k^X \um_{A_{k, \delta}} \ge \liminf_{n \to \infty} \frac{1}{n} \sum_{k = 1}^n e_k^Y \um_{A_{k, \delta}} \,, 
%\end{equation*}
%almost surely \texttt{with respect to which measure....}. Then using again Lemma~\ref{lemma_1} and the known fact that $\lim_{n \to \infty} \frac{|\mathcal{D}_n^Y|}{n} = \pi_d$ (see e.g.~\cite{spitzer2001principles} page 39) we obtain that 
%\[
%\liminf_{n \to \infty} \frac{1}{n} \sum_{k = 1}^n e_k^Y \um_{A_{k, \delta}} = \pi_d \,, \text{a.s..}
%\]
%\end{proof}
%}