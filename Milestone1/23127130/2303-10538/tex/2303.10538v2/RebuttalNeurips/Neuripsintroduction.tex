\section{Introduction}
Euclidean Travelling Salesman Problem (TSP) is one of the most famous and intensely studied NP-hard problems in the combinatorial optimization community. Exact methods, such as Concorde~\cite{applegate2006concorde}, use the cutting-plane method, iteratively solving linear programming relaxations of the TSP. These methods are usually implemented within a branch-and-cut framework, integrating the cutting-plane algorithm into a branch-and-bound search. While these exact methods can find solutions with guaranteed optimality for up to tens of thousands of nodes, the execution time can be exceedingly expensive. A different strategy is using heuristics such as LKH \cite{helsgaun2000effective}. These heuristics aim to find near-optimal solutions with a notable reduction in time complexity. Typically, they are manually crafted, drawing upon expert insights and domain-specific knowledge.


Recently, the success of Graph Neural Networks (GNNs) for a variety of machine learning tasks has sparked interests in building data-driven heuristics for approximating TSP solutions. For example, ~\cite{kwon2020pomo} uses a data-driven approach known as Policy Optimization with Multiple Optima (POMO), POMO relies on Reinforcement Learning (RL) and avoids the utilization of hand-crafted heuristics. \cite{qiu2022dimes} proposes a Meta-Learning framework which enhances the stability of RL training. \cite{sun2023difusco} applies Supervised Learning (SL) and adopts a graph-based diffusion framework. Additionally, the authors use a cosine inference schedule to improve the efficiency of their model. \cite{joshi2019efficient} trains their GNN model in a SL fashion, and the model outputs an edge adjacency matrix that indicates the likelihood of edges being part of the TSP tour. The edge predictions form a heat map, which is transformed into a valid tour through a beam search method. Similarly, \cite{fu2021generalize}  trains the GNN model on small sub-graphs to generate the corresponding heat maps using SL. These small heat maps are then integrated to build a large final heat map.  

Overall, these learning-based models usually build heuristics by reducing the length of TSP tours via RL or directly learning from the optimal solutions via SL. However,
since TSP is NP-hard, SL can cause expensive annotation problems due to the costly search time involved in generating optimal solutions. For RL, when dealing with big graphs, the model will run into the sparse reward problem because the reward is decided after decoding a complete solution. The sparse reward problem results in poor generalization performance and high training variance. Furthermore, both RL and SL suffer from expensive large-scale training. These models take more than one million training samples when dealing with TSP with 100 nodes, making the training process very time-consuming.




\section{Our Model}
In this work, we build a data-driven TSP heuristic in an Unsupervised Learning (UL) fashion and generate the heat map non-autoregressively. 
We construct a surrogate loss function with two parts: one encourages the GNN to find the shortest path, and the other acts as a proxy for the constraint that the path should be a Hamiltonian Cycle over all nodes. The surrogate loss enables us to update the model without decoding a complete solution. This helps alleviate the sparse reward problem encountered in RL, and thus, it avoids unstable training or slow convergence~\cite{kool2018attention}. Our UTSP method does not rely on labelled data, which helps the model avoid the expensive annotation problems encountered in SL and significantly reduces the time cost.
In fact, due to the prohibitive 
time cost of building training datasets for large instances, many SL methods are trained on relatively small instances 
only~\cite{fu2021generalize}\cite{joshi2019efficient}. Such SL models scale poorly to big instances, while with our UTSP model, we can train our model on larger instances directly.
Overall, our training does not rely on any labelled training data and converges faster compared to RL/SL methods. 


The model takes the coordinates as the input of GNN. The distance between two nodes determines the edge weight in the adjacency matrix. After training the GNN, the heat map is converted to a valid tour using local search. We evaluate the performance of %our 
UTSP %UL  method 
through comparisons on TSP cases of fixed graph sizes up to 1,000 nodes.
We note that UTSP is fundamentally different from RL, which may also be considered unsupervised.
While  RL requires a Markov Decision Process (MDP), and its reward is extracted after obtaining solutions, our method does not use a MDP and 
the loss function (reward) is determined based on a heat map. 
Overall, UTSP requires only a small amount of (unlabelled) data and compensates for it by employing an unsupervised surrogate loss function and an expressive GNN. 
The heat maps built using UTSP help reduce the search space and facilitate the local search. We further show that the expressive power of GNNs is critical for generating non-smooth heat maps.
