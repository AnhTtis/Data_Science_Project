\section{Experiments}
\subsection{Dataset}
Our dataset contains 2,000 samples for training and 1,000 samples for validation. We use the same test dataset in~\cite{fu2021generalize}. The test dataset contains $10,000$ 2D-Euclidean TSP instances for $n = 20,50,100$, and 128 instances for $n = 200,500,1,000$. We train our models on TSP instances with 20, 50, 100, 200, 500, and 1,000 vertices. We then build the corresponding heat maps based on these trained models.  
\input{Tables/NeuripsTable1}
% We use graph sampling to build the heat map for TSP 10,000 based on the the TSP 1,000 model.
% \subsection{Parameters}
% \textcolor{blue}{Yimeng: include the hyperparamemeters in search algorithm}
\subsection{Results}
\input{Tables/NeuripsTable2}
\input{Tables/NeuripsTable3}
Table~\ref{table:Exps01}, Table~\ref{table:Exps02} and Table~\ref{table:Exps03} present model's performance on TSP 20, 50, 100, 200, 500 and 1,000. The first three lines in the tables summarize the performance of two exact solvers (Concorde and Gurobi) and LKH3 heuristic~\cite{helsgaun2017extension}. The learning-based methods can be divided into RL sub-category and SL sub-category. 
Greedy decoding (G), Sampling (S), Beam Search (BS), and Monte Carlo Tree Search are the decoding schemes used in RL/SL. The 2-OPT is a greedy local search heuristic. 
%Given a neural network output, 2-OPT may generate a better solution.

We compare our model with existing solvers as well as different learning-based algorithms. The performance of our method is averaged of four runs with different random seeds. The running time for our method is divided into two parts: the inference time (building the heat map $\mathcal{H}$) and the search time (running search algorithm). 

On small instances, our results match the ground-truth solutions and generate average gaps of $\textbf{-0.00009\%}$, $\textbf{-0.002\%}$ and $\textbf{-0.00011\%}$ respectively on instances with $n = 20,50,100$, where the negative values are the results of the rounding problem. The total runtime of our method remains competitive w.r.t. all other learning baselines. On larger instances with $n$ = $200,500$ and $1,000$, we notice that traditional solvers (Concorde, Gurobi) fail to generate the optimal solutions within reasonable time when the size of problems grows. For RL/SL baselines, they  generate results far away from ideal solutions, particularly for cases with  $n=1,000$.   Our UTSP method is able to obtain $\textbf{0.0918\%}$, $\textbf{0.8394\%}$ and $\textbf{1.1770\%}$ on TSP $200,500$ and $1,000$, respectively. 
% We remark that the UTSP takes a shorter total running time  (inference + search) and  outperform the existing learning baselines on larger instances (TSP-200, 500, 1000) \footnote{On TSP-500, when we increase the time budget for Search, we achieve 0.47\% in 1.37m + 6.66m.}. 
% The gap between running time becomes more pronounced when the size increases to 1,000. 
We remark that UTSP outperforms the existing learning baselines on larger instances (TSP 200, 500, 1000) \footnote{On TSP 500, when we increase the time budget for Search, we achieve 0.42\% in 1.37m + 8.32m.}. 
More discussion between \cite{fu2021generalize} and  UTSP can be found in Appendix~\ref{sec:append_runtime}.




Our model takes less training time than RL/SL method because we require very few training instances. Taking TSP 100 as an example, RL/SL needs 1 million training instances, and the total training time can take one day using a  
NVIDIA V100 GPU, while our method only takes about 30 minutes with 2,000 training instances.  The training data size does not increase w.r.t. TSP size. Our training data consists of 2,000 instances for TSP 200, 500 and 1,000. At the same time, the UTSP model also remains very lightweight. On TSP 100, we use a 2-layer SAG with 64 hidden units and the model consists of 44,392 trainable parameters. In contrast, RL method in~\cite{kool2018attention} takes approximately 700,000 parameters and the SL method in~\cite{joshi2022learning} takes approximately 350,000 parameters.  




% With such a light structure, we can train our model directly on large graphs, whereas other baselines are trained on small graphs and then evaluated on large graphs.

% \input{Tables/Tsp10kperformance}

% Table~\ref{table:largeinstance} illustrates the performance of  on the 16 large instances with 10,000 vertices. When the graph size increases to 10,000, several baselines fail due to memory or time exception.
% Here, we compare our UL pipeline with remaining learning-based algorithms~\cite{kool2018attention}~\cite{fu2021generalize}. Our method significantly outperforms other learning-based algorithms  and  is able to produce solutions with average gap of \textbf{3.0295\%}. 
%%%%%% The running time of our algorithm also remains reasonable.
\subsection{Expressive Power of GNNs}
%Our results indicate the UTSP algorithm is able to generate better solutions within a reasonable time.
% Our UL method also requires fewer training samples. 
% Taking TSP 100 as an example, it takes one million samples to train SL/RL methods, while our model only uses 2,000 training samples. 
% This suggests that 
Our UL method generalizes well to unseen examples without requiring a large number of training samples. This is because the loss function in Equation~\ref{eq:loss} is fully differentiable w.r.t. the parameters in SAG and we are able to train the model in an end-to-end fashion. 
% \begin{wrapfigure}{r}{0.6\textwidth}
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=0.55\textwidth]{Figures/TrainingLoss.png}}
% \caption{TSP $100$ training curve using  Unsupervised Learning surrogate loss. We compare two GNN models: GCN~\cite{kipf2016semi} 
%  and SAG~\cite{min2022can}, where GCN is a low-pass model and SAG is a low-pass + band-pass model.}
% \label{fig:trainingloss}
% \end{center}
% \vskip -0.2in
% \end{wrapfigure}
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{Figures/TrainingLoss.png}}
\caption{TSP $100$ training curve using  Unsupervised Learning surrogate loss. We compare two GNN models: GCN~\cite{kipf2016semi} 
 and SAG~\cite{min2022can}, where GCN is a low-pass model and SAG is a low-pass + band-pass model.}
\label{fig:trainingloss}
\end{center}
\vskip -0.2in
\end{figure}
% This means we can update the model at every training step without explicitly finding a better solution. 
In other words, given a heat map $\mathcal{H}$, the model learns to assign large weights to more promising edges and small weights to less promising ones through backpropagation without any prior knowledge of the ground truth or any exploration step. However, when using SL, the model learns from the TSP solutions, which fails when multiple solutions exist or the solutions are not optimal~\cite{li2018combinatorial}. While for RL, the model often encounters an exploration dilemma and is not guaranteed to converge~\cite{bengio2021machine}\cite{joshi2019learning}.  Overall, UTSP requires fewer training samples and has better generalization compared to SL/RL models.









We aim to generate a non-smooth soft indicator matrix $\mathbb{T}$  and build an expressive heat map $\mathcal{H}$ to guide the search algorithm.


However, most GNNs aggregate information from adjacent nodes and these aggregation steps usually consist of local averaging operations, which can be interpreted as a low-pass filter and causes the oversmoothing problem~\cite{wenkel2022overcoming}.  
The low-pass model generates a smooth soft indicator matrix $\mathbb{T}$, which finally makes the elements in $\mathcal{H}$ become indistinguishable. So it becomes difficult to discriminate whether the edges belong to the optimal solution or not. In our model,  we assume all nodes in the graph are connected, so every node has $n-1$ connected to neighbouring nodes. This means every node receives messages from all other nodes and we have a global averaging operation over the graph, this can lead to severe oversmoothing issue.

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figures/NeuripsMergeHeatmap.png}}
\caption{Left: The heat map $\mathcal{H}$ generated using GCN on TSP 100. The diagonal elements are set to 0. $X$-axis and $y$-axis are the
city indices, right: The heat map $\mathcal{H}$ generated using SAG on TSP 100. The diagonal elements are set to 0. $X$-axis and $y$-axis are the
city indices.}
\label{fig:mergeheat}
\end{center}
\vskip -0.2in
\end{figure}

To avoid oversmoothing, one solution is to use shallow GNNs. However, this would result in narrow receptive fields and create the problem of underreaching~\cite{barcelo2020logical}. Our model uses SAG because this scattering-based method helps overcome the oversmoothing problem by combining band-pass wavelet filters with GCN-type filters~\cite{min2022can}. Figure~\ref{fig:trainingloss} illustrates the training loss on TSP $100$ and the differences between our SAG model and the graph convolutional network (GCN)~\cite{kipf2016semi}, where GCN only performs low-pass filtering on graph signals~\cite{nt2019revisiting}. 
When using GCN, the training loss decreases slowly, and the validation loss reaches a plateau after we train the model for 20 epochs. This is because the low-pass model generates a smooth $\mathbb{T}$. Such a smooth $\mathbb{T}$ results in an indistinguishable $\mathcal{H}$, detrimentally impacting the training process. Instead, we observe lower training and validation loss when using SAG; this suggests that SAG generates a more expressive representation which facilitates the training process. 


Figure~\ref{fig:mergeheat} illustrates the generated heat maps using GCN and SAG on a TSP 100 instance, we choose this instance from the validation set randomly. When using the GCN, due to the oversmoothing problem, the model generates a smooth representation and $\mathcal{H}$ becomes indistinguishable. The elements in $\mathcal{H}$ have a small variance and most of them are $\sim$ 0.01. Instead, the SAG generates a discriminative representation and the elements in the heat map have a larger variance.




Here, we train both GCN and SAG with the same loss function. So the differences illustrated in Figure~\ref{fig:mergeheat} are the direct result of overcoming the oversmoothing problem. 



\section{Search Space Reduction}
To understand what happens during our training process, we study how the prediction edge set $\Pi$ changes with training time. As mentioned, let $\Pi$ denote undirected edge set in $\mathcal{H}'$, and let $\Gamma$ denote the ground truth edge set, $
\eta = |\Gamma \cap \Pi|/|\Gamma|
$ is the extent of how good our prediction set $\Pi$ covers the solution $\Gamma$. If $\eta = 1$, then $\Gamma$ is a subset of $\Pi$, which means our prediction edge set successfully covers all ground truth edges. Similarly, $\eta = 0.95$ means we cover 95\% ground truth edges.

\begin{figure}[!htb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figures/Neuripsmergehisoverlap.png}}
\caption{Left:Average edge overlap coefficient $\eta$ w.r.t. training epochs using SAG and GCN on TSP 100 ($M=10$), right: Number of fully covered instances w.r.t. training epochs using SAG and GCN on TSP 100. The validation set consists of 1,000 samples ($M=10$).}
\label{fig:mergehisoverlap}
\end{center}
\vskip -0.2in
\end{figure}
Figure~\ref{fig:mergehisoverlap} shows how the average overlap coefficient $\eta$  changes with training epochs. We calculate the coefficient based on 1,000 validation instances in TSP 100. We notice that the coefficient quickly increases to $\sim 98\%$  after we train SAG for 10 epochs. This suggests that the surrogate loss successfully encourages the SAG to put more weights on the more promising edges. We also compare the performance with GCN. Since the loss does not decrease significantly during our training when using GCN (shown in Figure~\ref{fig:trainingloss}), it is not surprising to see the average overlap coefficient of GCN always maintains at a relatively low level. After training the model for 100 epochs, SAG model has an average coefficient of $99.756\%$ while GCN only has $33.893\%$.



We then study the number of cases where our prediction edge set $\Pi$ covers the ground truth solution. Figure~\ref{fig:mergehisoverlap} (right) illustrates how the number of fully covered instances ($\eta = 1$) changes with time. After training the model for 100 epochs, we observe 780 fully covered instances in 1,000 validation samples using SAG while 0 instances using GCN. Finally, we  calculate the average of size $|\Pi|$. Our results show that SAG has an average size of $583.134$ edges, while for GCN, the number is $738.739$.  

These results also indicate a correspondence between the loss and the quality of our prediction. 
In most SL tasks such as classification or regression tasks, a smaller validation loss usually means we achieve better performance and the minimum of the loss corresponds to the optimal solution (100\% accuracy). However, there is no theoretical guarantee that our loss in Equation~\ref{eq:loss} also measures the solution quality.
Our empirical results demonstrate that a lower surrogate loss encourages the model to assign larger weights on the promising edges and reduces the search space. This implies that we can assess the quality of the generated heat maps using our loss in Equation~\ref{eq:loss}.



Overall, the UL training reduces the search space from $4950$ edges to $583.134$  edges with over $99\%$ overlap accuracy on average. 
This helps explain why our search algorithm is able to perform well within reasonable search time. 









 



