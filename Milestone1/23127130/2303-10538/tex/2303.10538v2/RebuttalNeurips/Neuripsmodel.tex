\section{Methodologies}
In this paper, we study symmetric TSP on a 2D plane. Given $n$ cities and the  coordinates $(x_i,y_i) \in \mathbb{R}^2$ of these cities, our goal is to find the shortest possible route that visits each city exactly once and returns to the origin city, where $i\in \{1,2,3,...,n\}$ is the index of the city.
\subsection{Graph Neural Network}
Given a TSP instance, let $\mathbf{D}_{i,j}$ denote the Euclidean distance between city $i$ and city $j$. $\mathbf{D} \in \mathbb{R}^{n \times n}$ is the distance matrix. We first build adjacency matrix $\mathbf{W} \in \mathbb{R}^{n \times n}$ with $\mathbf{W}_{i,j} = e^{-\mathbf{D}_{i,j}/\tau}$ and node feature  $\mathbf{F} \in \mathbb{R}^{n \times 2}$ based on the input coordinates, where $\mathbf{F}_{i} = (x_i,y_i)$ and $\tau$ is the temperature.
The node feature matrix $\mathbf{F}$ and the weight matrix $\mathbf{W}$ are then fed into a GNN to generate a soft indicator matrix $\mathbb{T} \in \mathbb{R}^{n\times n}$. 

In our model, we use Scattering Attention GNN (SAG), SAG has both low-pass and band-pass filters and can build adaptive representations by learning node-wise weights for combining multiple different channels in the network using attention-based architecture.
 % \Yiwei{(Yiwei: Here we may want to introduce the purposes of low-pass filters and band-pass filters)}.
Recent studies show that SAG can output expressive representations for graph combinatorial problems such as maximum clique and remain lightweight~\cite{min2022can}. 

Let $\mathcal{S} \in \mathbb{R}^{n\times n}$  denote the output of SAG,  we first apply a column-wise Softmax activation to 
 the GNN's output and we can summarize
this operation in matrix notation as $\mathbb{T}_{i,j} = {e^{\mathcal{S}_{i,j}}}/{\sum_{k=1}^n e^{\mathcal{S}_{k,j}}}$. This ensures that each element in $\mathbb{T}$ is greater than zero and the summation of each column is 1. 
We then use $\mathbb{T}$ to build a heat map $\mathcal{H}$, where $\mathcal{H} \in   \mathbb{R}^{n\times n}$. 

In our model, we use $\mathcal{H}$  to estimate the probability of each edge
 belonging to the optimal solution and use
 $\mathbb{T}$ to build a surrogate loss of the Hamiltonian Cycle constraint. 
 % As illustrated in Figure~\ref{fig:Transition},  our approach aims to generate an expressive \bluetext{soft indicator matrix} $\mathbb{T}$ which assigns large weights (close to 1) on the transition elements and small weights (close to 0) on others.
% (\Yiwei{Yiwei: It would be better if we can introduce some intuition here about the \bluetext{soft indicator matrix} and transition elements.}) 
This will allow us to build a non-smooth heat map $\mathcal{H}$ and improve the performance of the local search.
\subsection{Building the  Heat Map using the soft indicator matrix}
Before building the unsupervised loss, let's recall the definition of TSP. The objective of TSP is to identify the shortest Hamiltonian Cycle of a graph. Therefore, the unsupervised surrogate loss should act as a proxy for two requirements: the Hamiltonian Cycle constraint and the shortest path constraint. However, designing a surrogate loss for the Hamiltonian Cycle constraint can be challenging, particularly when working with a heat map $\mathcal{H}$. To address this, we introduce the $\mathbb{T} \rightarrow \mathcal{H}$ transformation, which enables the model to implicitly encode the Hamiltonian Cycle constraint.
\tikzset{every picture/.style={line width=0.6pt}} %set default line width to 0.75pt        
\begin{figure}[!htb]
\vskip 0.2in
\include{Figures/tikz}
\caption{We use a SAG to generate a non-smooth soft indicator matrix $\mathbb{T}$. The SAG model is a function of  coordinates and weighted adjacency matrix. We then build the heat map $\mathcal{H}$ based on $\mathbb{T}$ using the transformation in Equation ~\ref{eq:TH}. }
\label{fig:TSP}
\end{figure}

To better understand the $\mathbb{T} \rightarrow \mathcal{H}$ transformation, we show a binary instance in Figure~\ref{fig:TSP}. Figure~\ref{fig:TSP} illustrates a soft indicator matrix $\mathbb{T}$, its heat map  $\mathcal{H}$ following the transformation $\mathbb{T} \rightarrow \mathcal{H}$, and their corresponding routes. When we directly use the soft indicator matrix $\mathbb{T}$ as the heat map. It can result in loops (parallel edges) between cities, such as  (2,3) and (4,5) in Figure~\ref{fig:TSP} (middle). After we apply the $\mathbb{T} \rightarrow \mathcal{H}$  transformation, the corresponding heat $\mathcal{H}$ is a Hamiltonian Cycle, as shown in the right part in Figure~\ref{fig:TSP}. 

\subsection{$\mathbb{T} \rightarrow \mathcal{H}$  transformation}
We build the heat map $\mathcal{H}$ based on $\mathbb{T}$. As mentioned, $\mathcal{H}_{i,j}$ is the probability for edge ($i$,$j$) to belong to the optimal TSP solution. We define $\mathcal{H}$ as:
\begin{equation}\label{eq:TH}
\mathcal{H} = \sum_{t=1}^{n-1}p_t p^T_{t+1} + p_np_1^T,
\end{equation}
where $p_t \in \mathbb{R}^{n \times 1}$ is the $t_{th}$ column of $\mathbb{T}$, $\mathbb{T} = [p_1|p_2|...|p_n]$.
% The elements in $\mathcal{H}$ can be written as
As shown in Figure~\ref{fig:TSP}, the first row in $\mathcal{H}$ is the probability distribution of directed edges start from city $1$, and since the third element is the only non-zero one in the first row, we then add directed edge $1 \rightarrow 3 $ to our TSP solution. Similarly, the first column in $\mathcal{H}$ can be regarded as the probability distribution of directed edges which end in city $1$.  Ideally, given a graph $\mathcal{G}$ with $n$ nodes, we want to build a soft indicator matrix where  each row and column are assigned with one value 1 (True) and $n-1$ values 0 (False), so that the heat map will only contain one valid solution. 
% Equation~\ref{eq:Hij} generate a Hamiltonian Cycle given $\mathbb{T}$. 
In practice, we will build a soft indicator matrix $\mathbb{T}$ whose heat map $\mathcal{H}$ assigns large probabilities to the edges in the TSP solution and small probabilities to the other edges.


Overall, the $\mathbb{T} \rightarrow \mathcal{H}$ transformation in Equation~\ref{eq:TH} enables us to build a proxy for the Hamiltonian Cycle constraint. We further prove that $\mathcal{H}$ represents one Hamiltonian Cycle when each row and column in  $\mathbb{T} $ have one value 1 (True) and $n-1$ value 0 (False). We refer to the proof in  appendix~\ref{sec:proof}. 

\subsection{Unsupervised Loss}
In order to generate such an expressive soft indicator matrix $\mathbb{T}$, we minimize the following objective function:
\begin{equation} \label{eq:loss}
\begin{aligned}
    \mathcal{L} =   
 \lambda_1 \underbrace{\sum_{i=1}^n (\sum_{j=1}^n \mathbb{T}_{i,j} - 1)^2}_{\text{Row-wise constraint}}  +  \lambda_2   \underbrace{\sum_{i}^n \mathcal{H}_{i,i}}_{\text{No self-loops}}  + \underbrace{\sum_{i=1}^n \sum_{j=1}^n \mathbf{D}_{i,j} \mathcal{H}_{i,j}}_{\text{Minimize the distance }}. 
\end{aligned}
\end{equation}

The first term in $\mathcal{L}$ encourages the summation of each row in $\mathbb{T}$ to be close to 1. As mentioned, we normalize each column of $\mathbb{T}$ using  Softmax activation. So when the first term is minimized to zero, each row and column in $\mathbb{T}$ are normalized (doubly stochastic). The second term penalizes the weight on the main diagonal of $\mathcal{H}$, this discourages self-loops in  TSP solutions. The third term can be regarded as the expected TSP length of the heat map $\mathcal{H}$, where $\mathbf{D}_{i,j}$ is the distance between city $i$ and $j$. As mentioned, since $\mathcal{H}$ corresponds to a Hamiltonian Cycle given an ideal soft indicator matrix with one value $1$ (True)
and $n-1$ value $0$ (False) in each row and column. Then the minimum value of $\sum_{i=1}^n \sum_{j=1}^n \mathbf{D}_{i,j} \mathcal{H}_{i,j}$ is the shortest Hamiltonian Cycle on the graph, which is the optimal solution of TSP. To summarize, we build a loss function which contains both the \textit{shortest} and the \textit{Hamiltonian Cycle} constraints. The \textit{shortest} constraint is realized by minimizing 
$
\sum_{i=1}^n \sum_{j=1}^n \mathbf{D}_{i,j} \mathcal{H}_{i,j}.
$
For the \textit{Hamiltonian Cycle} constraint, instead of writing it in a Lagrangian relaxation style penalty, we use a GNN which encourages a non-smooth representation, along with the doubly stochastic penalty, and the $\mathbb{T}\rightarrow \mathcal{H}$ transformation.

\subsection{Edge Elimination} Given a heat map $\mathcal{H}$, we consider  $M$ largest elements in each row (without diagonal elements) and set other $n-M$ %(\Yiwei{Can we replace K with M here for later consistency}) 
elements as 0. Let $\Tilde{H}$ denote the new heat map, we then symmetrize the new heat map by $\mathcal{H}' = \Tilde{H} + \Tilde{H}^T$. %because the reverse of a Hamiltonian Cycle is also a Hamiltonian Cycle. 
Let $\mathbf{E}_{ij} \in \{0,1\}$ denote whether an undirected edge ($i,j$)  is in our prediction or not. Without loss of generality, we can assume $0<i<j\leq n$ and  define $\mathbf{E}_{ij}$ as :
$$
\mathbf{E}_{ij} = 
    \begin{cases}
      1, & \text{if}\ \mathcal{H}'_{ij} = \mathcal{H}'_{ji} > 0 \\
      0, & \text{otherwise}
    \end{cases}.
$$
Let $\Pi$ denote the set of undirected edges $(i,j)$ with $\mathbf{E}_{ij} = 1$. Ideally, we would build a prediction edge set $\Pi$ with a small $M$ value, and $\Pi$ can cover all the ground truth edges so that we can reduce search space size from $n(n-1)/2$ to  $|\Pi|$. 
% In practice, generating small-size edge sets $\Pi$ which always cover the ground truth solutions is very difficult. 
In practice, we aim to let  $\Pi$ cover as many ground truth edges as possible and use $\mathcal{H}'$ to guide the local search process. 
