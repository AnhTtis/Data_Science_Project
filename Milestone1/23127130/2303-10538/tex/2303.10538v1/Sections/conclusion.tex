\section{Conclusion}
% SL/RL methods usually require a large amount of training data or suffer from the sparse reward problem. 
In this paper, we propose  UTSP, an unsupervised learning  method to solve the TSP. 
We build a surrogate loss that encourages the GNN to find the shortest path and satisfy the constraint that the path should be a Hamiltonian Cycle. The surrogate loss function does not rely on any labeled ground truth solution and helps alleviate sparse reward problems in RL.
UTSP uses a two-phase strategy. We first build a heat map based on the GNN's output. The heat map is then fed into a search algorithm. Compared with RL/SL, our method vastly reduces training cost and takes fewer training samples. We further show that our UL training helps reduce the search space. This helps explain why the generated heat maps can guide the search algorithm.  
On the model side, our results indicate that a low-pass GNN will produce an indistinguishable representation due to the oversmoothing issue, which results in  unfavorable heat maps and fails to reduce the search space. Instead, after incorporating band-pass operators into GNN, we can build efficient heat maps that successfully reduce search space. Our findings show that the expressive power of GNNs is critical for generating a non-smooth representation that helps find the solution. 


In conclusion, UTSP is competitive with or outperforms other learning-based TSP heuristics in terms of solution quality and running speed. In addition, UTSP  takes $\sim$ 10\% of the
number of parameters and $\sim$ 0.2\% of (unlabeled) training samples, compared with reinforcement learning or supervised learning methods. Our UTSP framework demonstrates that, providing a surrogate loss and an expressive GNN, we can learn to reduce the search space and build a heuristic by exploiting a small amount of  unlabeled data without any prior knowledge. 
These learned "algorithmic priors"  help facilitate the local search and lead to better solutions.
% Our findings demonstrate that with a small amount of data, it is possible to learn some "algorithmic priors" in an unsupervised manner.
Future direction includes designing more expressive GNNs (such as adding edge features)  and using different surrogate loss functions. We anticipate that these concepts will extend to more (graph) combinatorial problems.
