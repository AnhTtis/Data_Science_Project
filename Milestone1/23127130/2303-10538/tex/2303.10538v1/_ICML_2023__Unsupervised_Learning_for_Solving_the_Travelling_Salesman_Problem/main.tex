%%%%%%%% ICML 2022 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[nohyperref]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{xcolor}

\newcommand{\Carla}[1]{\textcolor{blue}{ #1}}

\setlength{\tabcolsep}{4pt} % setting column spaces



% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2022} with \usepackage[nohyperref]{icml2022} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2023}
% \usepackage{float}
% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2023}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
% \newcommand{\Yiwei}[1]{\textcolor{red}{ #1}}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:

%%%%add by Yimeng
\usepackage{multirow}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}


%%%%%

% \icmltitlerunning{Submission and Formatting Instructions for ICML 2023}

\begin{document}

\twocolumn[
\icmltitle{Unsupervised Learning for Solving the Travelling Salesman Problem}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2022
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yimeng Min}{equal,yyy}
\icmlauthor{Yiwei Bai}{equal,yyy}
\icmlauthor{Carla P. Gomes}{yyy}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Computer Science, Cornell University, Ithaca, NY, 14850, USA}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}
\icmlcorrespondingauthor{Yimeng Min}{min@cs.cornell.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Travelling Salesman Problem, GNN}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%
\printAffiliationsAndNotice{\icmlEqualContribution}
% otherwise use the standard text.

\begin{abstract}
\input{Sections/abstract}
% This document provides a basic paper template and submission guidelines.
% Abstracts must be a single paragraph, ideally between 4--6 sentences long.
% Gross violations will trigger corrections at the camera-ready phase.
\end{abstract}



% \twocolumn[
% \input{Tables/TSPperformance}
% ]


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\input{Sections/introduction}
\input{Sections/model_arxiv}
\input{Sections/MCTS}
\input{Sections/result_arxiv}
\input{Sections/conclusion}
% \nocite{langley00}
% \newpage
%\bibliography{reference}
\bibliographystyle{icml2023}
\begin{thebibliography}{22}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Applegate et~al.(2006)Applegate, Bixby, Chvatal, and
  Cook]{applegate2006concorde}
Applegate, D., Bixby, R., Chvatal, V., and Cook, W.
\newblock Concorde tsp solver, 2006.

\bibitem[Barcel{\'o} et~al.(2020)Barcel{\'o}, Kostylev, Monet, P{\'e}rez,
  Reutter, and Silva]{barcelo2020logical}
Barcel{\'o}, P., Kostylev, E.~V., Monet, M., P{\'e}rez, J., Reutter, J., and
  Silva, J.-P.
\newblock The logical expressiveness of graph neural networks.
\newblock In \emph{8th International Conference on Learning Representations
  (ICLR 2020)}, 2020.

\bibitem[Bengio et~al.(2021)Bengio, Lodi, and Prouvost]{bengio2021machine}
Bengio, Y., Lodi, A., and Prouvost, A.
\newblock Machine learning for combinatorial optimization: a methodological
  tour dâ€™horizon.
\newblock \emph{European Journal of Operational Research}, 290\penalty0
  (2):\penalty0 405--421, 2021.

\bibitem[Bresina(1996)]{random2}
Bresina, J.~L.
\newblock Heuristic-biased stochastic sampling.
\newblock In \emph{AAAI/IAAI, Vol. 1}, pp.\  271--278, 1996.

\bibitem[Croes(1958)]{2opt}
Croes, G.~A.
\newblock A method for solving traveling-salesman problems.
\newblock \emph{Operations research}, 6\penalty0 (6):\penalty0 791--812, 1958.

\bibitem[Deudon et~al.(2018)Deudon, Cournut, Lacoste, Adulyasak, and
  Rousseau]{deudon2018learning}
Deudon, M., Cournut, P., Lacoste, A., Adulyasak, Y., and Rousseau, L.-M.
\newblock Learning heuristics for the tsp by policy gradient.
\newblock In \emph{International Conference on the Integration of Constraint
  Programming, Artificial Intelligence, and Operations Research}, pp.\
  170--181. Springer, 2018.

\bibitem[Fu et~al.(2021)Fu, Qiu, and Zha]{fu2021generalize}
Fu, Z.-H., Qiu, K.-B., and Zha, H.
\newblock Generalize a small pre-trained model to arbitrarily large tsp
  instances.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  35\penalty0 (8):\penalty0 7474--7482, 2021.

\bibitem[Gomes et~al.(1998)Gomes, Selman, Kautz, et~al.]{random1}
Gomes, C.~P., Selman, B., Kautz, H., et~al.
\newblock Boosting combinatorial search through randomization.
\newblock \emph{AAAI/IAAI}, 98:\penalty0 431--437, 1998.

\bibitem[Gomes et~al.(2000)Gomes, Selman, Crato, and Kautz]{random3}
Gomes, C.~P., Selman, B., Crato, N., and Kautz, H.
\newblock Heavy-tailed phenomena in satisfiability and constraint satisfaction
  problems.
\newblock \emph{Journal of automated reasoning}, 24\penalty0 (1):\penalty0
  67--100, 2000.

\bibitem[Helsgaun(2000)]{helsgaun2000effective}
Helsgaun, K.
\newblock An effective implementation of the lin--kernighan traveling salesman
  heuristic.
\newblock \emph{European journal of operational research}, 126\penalty0
  (1):\penalty0 106--130, 2000.

\bibitem[Helsgaun(2017)]{helsgaun2017extension}
Helsgaun, K.
\newblock An extension of the lin-kernighan-helsgaun tsp solver for constrained
  traveling salesman and vehicle routing problems.
\newblock \emph{Roskilde: Roskilde University}, pp.\  24--50, 2017.

\bibitem[Joshi et~al.(2019{\natexlab{a}})Joshi, Laurent, and
  Bresson]{joshi2019efficient}
Joshi, C.~K., Laurent, T., and Bresson, X.
\newblock An efficient graph convolutional network technique for the travelling
  salesman problem.
\newblock \emph{arXiv preprint arXiv:1906.01227}, 2019{\natexlab{a}}.

\bibitem[Joshi et~al.(2019{\natexlab{b}})Joshi, Laurent, and
  Bresson]{joshi2019learning}
Joshi, C.~K., Laurent, T., and Bresson, X.
\newblock On learning paradigms for the travelling salesman problem.
\newblock \emph{arXiv preprint arXiv:1910.07210}, 2019{\natexlab{b}}.

\bibitem[Joshi et~al.(2022)Joshi, Cappart, Rousseau, and
  Laurent]{joshi2022learning}
Joshi, C.~K., Cappart, Q., Rousseau, L.-M., and Laurent, T.
\newblock Learning the travelling salesperson problem requires rethinking
  generalization.
\newblock \emph{Constraints}, pp.\  1--29, 2022.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kipf \& Welling(2016)Kipf and Welling]{kipf2016semi}
Kipf, T.~N. and Welling, M.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock \emph{arXiv preprint arXiv:1609.02907}, 2016.

\bibitem[Kool et~al.(2019)Kool, van Hoof, and Welling]{kool2018attention}
Kool, W., van Hoof, H., and Welling, M.
\newblock Attention, learn to solve routing problems!
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=ByxBFsRqYm}.

\bibitem[Min et~al.(2022)Min, Wenkel, Perlmutter, and Wolf]{min2022can}
Min, Y., Wenkel, F., Perlmutter, M., and Wolf, G.
\newblock Can hybrid geometric scattering networks help solve the maximum
  clique problem?
\newblock \emph{arXiv preprint arXiv:2206.01506}, 2022.

\bibitem[Nt \& Maehara(2019)Nt and Maehara]{nt2019revisiting}
Nt, H. and Maehara, T.
\newblock Revisiting graph neural networks: All we have is low-pass filters.
\newblock \emph{arXiv preprint arXiv:1905.09550}, 2019.

\bibitem[Qiu et~al.(2022)Qiu, Sun, and Yang]{qiu2022dimes}
Qiu, R., Sun, Z., and Yang, Y.
\newblock Dimes: A differentiable meta solver for combinatorial optimization
  problems.
\newblock \emph{arXiv preprint arXiv:2210.04123}, 2022.

\bibitem[Sylvester(1909)]{sylvester1909collected}
Sylvester, J.~J.
\newblock \emph{The Collected Mathematical Papers of James Joseph
  Sylvester...}, volume~3.
\newblock University Press, 1909.

\bibitem[Wenkel et~al.(2022)Wenkel, Min, Hirn, Perlmutter, and
  Wolf]{wenkel2022overcoming}
Wenkel, F., Min, Y., Hirn, M., Perlmutter, M., and Wolf, G.
\newblock Overcoming oversmoothness in graph convolutional networks via hybrid
  scattering networks.
\newblock \emph{arXiv preprint arXiv:2201.08932}, 2022.

\end{thebibliography}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Appendix}


\subsection{Training and Search Details}
We train our model using Adam~\cite{kingma2014adam}. All models are trained using Nvidia V100 GPU. 
All the search-related parameters are listed in Table \ref{tb:sp}. $M$ refers to the size of the candidate set of each city. $K$ is the maximal number of edges we can remove in one action, and for each round of local search, we randomly select one number from the listed interval. $T$ is the total number of actions we will try to expand one node. Here we keep the $\alpha = 0$ to show that the quality of our unsupervised learned heat map is high. Lower $\alpha$ means the local search algorithm focuses more on the edges with higher heat map value. Actually, in the experiments, we find the results are similar with $\alpha \leq 1$.
\begin{table}[h]
    \centering
    \begin{tabular}{lllllll}
        \toprule
        & $\alpha$ & $\beta$ & $M$ & $K$ & $T$  \\
        
        \midrule
        TSP-20 & 0 & 10 & 8 & 10 & 60  \\
        TSP-50 & 0 & 10 & 8 & [5, 15) & 150  \\
        TSP-100 & 0 & 10 & 8 & [5, 35) & 300  \\
        TSP-200 & 0 & 10 & 8 & [10, 90) & 600  \\
        TSP-500 & 0 & 50 & 5 & [30, 130) & 1000  \\
        TSP-1000 & 0 & 50 & 5 & [10, 110) & 2000  \\
        \bottomrule
    \end{tabular}
    \caption{Search parameters for all the TSP experiments.}
    \label{tb:sp}
\end{table}
%We select $M = 8,10,10$ for TSP 20, 50 and 100, $M = 20$ for TSP 200, 500 and 1,000 and $M = 100$ for TSP 10,000. All the models are trained using Nvidia V100 GPU. 

\subsection{Running Time Discussion}
\label{sec:append_runtime}
As discussed in \cite{kool2018attention}, running time is important but hard to compare since it is affected by many factors. We report the clock time for solving all the test instances  in Table \ref{table:Exps01}.
For the UTSP (our method) and the state-of-the-art learning-based method Att-GCRN \cite{fu2021generalize}, we run the search algorithm on exactly the same environment (one Intel  Xeon Gold 6326) for a fair comparison.  And for other baselines, we directly refer to the results from \cite{fu2021generalize}. So the time there are only for indicative purpose since the computing hardware is not the same.


%\appendix

%\include{Sections/appendix.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022. 
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
