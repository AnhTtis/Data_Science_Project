\section{Introduction}
Euclidean Travelling Salesman Problem (TSP) is one of the most intensely studied NP-hard graph problems in combintorial optimization community. Traditional methods, such as Concorde \cite{applegate2006concorde} and LKH \cite{helsgaun2000effective}, use linear programming and search to solve TSP.
Recently, the success of Graph Neural Networks (GNNs) for a variety of machine learning tasks  has sparked interest in building data-driven heuristics for approximating TSP solutions with lower time costs. These learning-based models usually build heuristics by reducing the length of TSP tours via reinforcement learning (RL) or directly learning from the optimal solutions via supervised learning (SL). However,
since TSP is NP-hard, SL can cause expensive annotation problems due to the costly search times involved in generating optimal solutions. While for RL, when dealing with big graphs, the model will run into the sparse reward problem because the reward is only decided after decoding a complete solution. The sparse reward problem results in poor generalization performance and high training variance. Furthermore, both RL and SLÂ suffer from costly large-scale training. These models take more than one million training samples when dealing with TSP with 100 nodes, making the training process very time-consuming~\cite{kool2018attention}\cite{joshi2022learning}\cite{qiu2022dimes}.

In this work, we build a data-driven TSP heuristic in an unsupervised learning fashion.
We construct a surrogate loss function with two parts: one encourages the GNN to find the shortest path and the other acts as a proxy for the constraint that the path should be a Hamiltonian Cycle over all nodes. The surrogate loss enables us to update the model without having to  decode a complete solution. This helps alleviate the sparse reward problem encountered in RL and thus it avoids unstable training or slow convergence~\cite{kool2018attention}. Our UTSP method does not rely on labeled data. This helps the model avoid the expensive annotation problems encountered in SL and significantly reduces the time cost.
In fact, due to the prohibitive time cost of building training datasets for large instances, many SL methods are trained on relatively small instances 
only~\cite{fu2021generalize}\cite{joshi2019efficient}. Such SL models scale poorly to the big instances, while with our UTSP pipeline, we are able to train our model on larger instances directly.
Overall, our training does not rely on any labeled training data and converges faster comparing to RL/SL methods. 
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figures/pipeline.png}}
\caption{Illustration of training pipeline. We use a GNN to learn a transition matrix $\mathbb{T}$ and we build a heat map $\mathcal{H}$ based on $\mathbb{T}$. We combine the transition matrix, heat map and the distance matrix to  build the surrogate loss.}
\label{fig:pipeline}
\end{center}
\vskip -0.2in
\end{figure}

Our method is shown in Figure~\ref{fig:pipeline}.  The model takes the coordinates as the input of GNN. The distance between two nodes determines the edge weight in the adjacency matrix. After training the GNN, the heat map is converted to a valid tour using local search. We evaluate the performance of %our 
UTSP %UL  method 
through comparisons on TSP cases of fixed graph sizes up to 1,000 nodes.
We note that UTSP
is fundamentally different from RL, which may also be considered unsupervised.
While  RL requires a Markov Decision Process (MDP) and its reward is extracted after obtaining  solutions, does not use an MDP and 
the loss function (reward) is determined based on a heat map. Furthermore, UTSP incorporates a data-driven approach,  which learns  hidden structure across instances in an unsupervised way, in contrast to  standard direct  optimization that typically considers one instance at a time, without a learning component across instances.
Overall, UTSP requires only a small amount of (unlabeled) data and compensates for it by employing an unsupervised surrogate loss function and an expressive GNN. 
UTSP outperforms other learning-based methods in terms of performance accuracy and speed. The heat maps built using UTSP help reduce the search space and generate "algorithmic priors" which facilitate the local search. We further show that the expressive power of GNNs is critical for generating non-smooth heat maps.