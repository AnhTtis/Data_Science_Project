\section{Experiments}
\subsection{Dataset}
Our dataset contains 2,000 samples for training and 1,000 samples for validation. We use the same test dataset in~\cite{fu2021generalize}. The test dataset contains $10,000$ 2D-Euclidean TSP instances for $n = 20,50,100$, 128 instances for $n = 200,500,1,000$. We train our models on TSP instances with 20, 50, 100, 200, 500, and 1,000 vertices. We then build the corresponding heat maps based on these trained models.  
\input{Tables/TSPperformance}
\subsection{Results}
\begin{figure}[!htb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figures/TrainingLoss.png}}
\caption{TSP $100$ training curve using  unsupervised learning surrogate loss. We compare two GNN models: GCN~\cite{kipf2016semi} 
 and SAG~\cite{min2022can}, where GCN is a low-pass model and SAG is a low-pass + band-pass model.}
\label{fig:trainingloss}
\end{center}
\vskip -0.2in
\end{figure}
\input{Tables/TSPperformance2}
Table~\ref{table:Exps01} and Table~\ref{table:Exps02} present model's performance on TSP 20, 50, 100, 200, 500 and 1,000. The first three lines in Table~\ref{table:Exps01} and Table~\ref{table:Exps02}  summarize the performance of two exact solvers (Concorde and Gurobi) and LKH3 heuristic~\cite{helsgaun2017extension}. The learning-based me
thods can be divided into RL sub-category and SL sub-category. 
Greedy decoding (G), Sampling (S), Beam Search (BS), and Monte Carlo Tree Search are the decoding schemes used in RL/SL. The 2-OPT is a greedy local search heuristic. 
%Given a neural network output, 2-OPT may generate a better solution.

We compare our model with existing solvers as well as different learning-based algorithms. The performance of our method is averaged of four runs with different random seeds. The running time for our method is divided into two parts: the inference time (building the heat map $\mathcal{H}$) and the search time (running search algorithm). 
\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figures/GCNHeatMap.png}}
\caption{The heat map $\mathcal{H}$ generated using GCN on TSP 100. The diagonal elements are set to 0. $X$-axis and $y$-axis are the
city indices.}
\label{fig:GGNheat}
\end{center}
\vskip -0.2in
\end{figure}
\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figures/SCTHeatMap.png}}
\caption{The heat map $\mathcal{H}$ generated using SAG on TSP 100. The diagonal elements are set to 0. $X$-axis and $y$-axis are the
city indices.}
\label{fig:SCTheat}
\end{center}
\vskip -0.2in
\end{figure}
On small instances, our results match the ground-truth solutions and generate average gaps of $\textbf{-0.00009\%}$, $\textbf{-0.002\%}$ and $\textbf{-0.00011\%}$ respectively on instances with $n = 20,50,100$, where the negative values are the results of the rounding problem. The total runtime of our method remains competitive w.r.t. all other learning baselines. On larger instances with $n$ = $200,500$ and $1,000$, we notice that traditional solvers and heuristics (Concorde,Gurobi and LKH3) fail to generate the optimal solutions within reasonable time when the size of problems grows. For RL/SL baselines, they  generate results far away from ideal solutions, particularly for cases with  $n=1,000$.   Our UTSP method is able to obtain $\textbf{0.0918\%}$, $\textbf{0.8394\%}$ and $\textbf{1.1770\%}$ on TSP $200,500$ and $1,000$, respectively. We remark that the UTSP takes a shorter total running time  (inference + search) and  outperform the existing learning baselines on these large instances. The gap between running time becomes more pronounced when the size increases to 1,000. More discussion between \cite{fu2021generalize} and  UTSP can be found in Appendix~\ref{sec:append_runtime}.





Our model also takes less training time because we require very few training instances. Taking TSP 100 as an example, RL/SL needs 1 million training instances, and the total training time can take one day using a  
NVIDIA V100 GPU, while our method only takes about 40 minutes with 2,000 training instances.  The training data size does not increase w.r.t. TSP size. Our training data consists of 2,000 instances for TSP 200, 500 and 1,000. At the same time, the UTSP model also remains very lightweight. On TSP 100, we use a 2-layer SAG with 64 hidden units and the model consists of 44,392 trainable parameters. In contrast, RL method in~\cite{kool2018attention} takes approximately 700,000 parameters and the SL method in~\cite{joshi2022learning} takes approximately 350,000 parameters.  


Our results indicate the UTSP algorithm is able to generate better solutions within a reasonable time.
Our UL pipeline also generalizes well to unseen examples without requiring a large number of training samples. This is because the loss function in Equation~\ref{eq:loss} is fully differentiable w.r.t the parameters in SAG and we are able to train the model in an end-to-end fashion. 
In other words, given a heat map $\mathcal{H}$, the model learns to assign large weights to more promising edges and small weights to less promising ones through backpropagation without any prior knowledge of the ground truth or any exploration step. However, when using SL, the model learns from the TSP solutions, which fails when multiple solutions exist or the solutions are not optimal. While for RL, the model often encounters an exploration dilemma and is not guaranteed to converge~\cite{bengio2021machine}\cite{joshi2019learning}.  Overall, UTSP requires fewer training samples and has better generalization comparing to SL/RL models. 

\subsection{Expressive Power of GNNs}
We aim to generate a non-smooth transition matrix $\mathbb{T}$  and build an expressive heat map $\mathcal{H}$ to guide the search algorithm.
However, most GNNs aggregate information  from adjacent nodes and these aggregation
steps usually consist of local averaging operations, which can be interpreted as a low-pass filter and causes the oversmoothing problem~\cite{wenkel2022overcoming}.  
The low-pass model generates a smooth transition matrix $\mathbb{T}$, which finally makes the elements $\mathcal{H}$ become indistinguishable. So it becomes difficult to discriminate whether the edges belong to the optimal solution or not.  In our model,  we assume all nodes in the graph are connected, so every node has $n-1$ neighboring nodes. This means every node receives messages from all other nodes and we have a global averaging operation over the graph, this can lead to severe oversmoothing issue.

To avoid oversmoothing, one solution is to use shallow GNNs.  However, this would result in narrow receptive fields and create the problem of underreaching~\cite{barcelo2020logical}. In our model, we use SAG because this scattering-based method helps  overcome the oversmoothing problem by combining band-pass wavelet filters with GCN-type filters~\cite{min2022can}. Figure~\ref{fig:trainingloss} illustrates the training loss on TSP $100$ and the differences between our SAG model and the graph convolutional network (GCN)~\cite{kipf2016semi}, where GCN 
  only performs low-pass filtering on graph signals~\cite{nt2019revisiting}. 
When using GCN, the training loss decreases slowly, and the validation loss reaches a plateau after we train the model for 20 epochs. This is because the low-pass model generates a smooth $\mathbb{T}$. Such a smooth $\mathbb{T}$ results in an indistinguishable $\mathcal{H}$, which harms the training process. Instead, we observe lower training and validation loss when using SAG; this suggests that SAG generates a more expressive representation which facilitates the training process. 



Figure~\ref{fig:GGNheat} and Figure ~\ref{fig:SCTheat} illustrate the generated heat maps using GCN and SAG on a TSP 100 instance, we choose this instance from the validation set randomly. When using the GCN, due to the oversmoothing problem, the model generates a smooth representation and $\mathcal{H}$ becomes indistinguishable. The elements in $\mathcal{H}$ have a small variance and most of them are $\sim$ 0.01. Instead, the SAG generates a discriminative representation and the elements in the heat map have a larger variance.




Here, we train both GCN and SAG with the same loss function. So the differences illustrated in Figure~\ref{fig:GGNheat} and~\ref{fig:SCTheat} are the direct result of overcoming the oversmoothing problem. 



\section{Search Space Reduction}
To understand what happens during our training process, we study how the prediction edge set $\Pi$ changes with training time. As mentioned, let $\Pi$ denote undirected edge set in $\mathcal{H}'$, and let $\Gamma$ denote the ground truth edge set, $
\eta = |\Gamma \cap \Pi|/|\Gamma|
$ is the extent of how good our prediction set $\Pi$ covers the solution $\Gamma$. If $\eta = 1$, then $\Gamma$ is a subset of $\Pi$, which means our prediction edge set successfully covers all ground truth edges. Similarly, $\eta = 0.95$ means we cover 95\% ground truth edges.

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figures/Edgefloat.png}}
\caption{Average edge overlap coefficient $\eta$ w.r.t. training epochs using SAG and GCN on TSP 100 ($M=10$).}
\label{fig:edgefloat}
\end{center}
\vskip -0.2in
\end{figure}
Figure~\ref{fig:edgefloat} shows how the average overlap coefficient $\eta$  changes with training epochs. We calculate the coefficient based on 1,000 validation instances in TSP 100. We notice that the coefficient quickly increases to $\sim 98\%$  after we train SAG for 10 epochs. This suggests that the surrogate loss successfully encourages the SAG to put more weights on the more promising edges. We also compare the performance with GCN. Since the loss does not decrease significantly during our training when using GCN (shown in Figure~\ref{fig:trainingloss}), it is not surprising to see the average overlap coefficient of GCN always maintains at a relatively low level. After training the model for 100 epochs, SAG model has an average coefficient of $99.756\%$ while GCN only has $33.893\%$.

Overall, the unsupervised learning  training reduces the search space from $4950$ edges to $583.134$ edges with over $99\%$ overlap accuracy. 
This helps explain why our search algorithm is able to perform well within reasonable time. 

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figures/OverlapHis.png}}
\caption{Number of fully covered instances w.r.t. training epochs using SAG and GCN on TSP 100. The validation set consists of 1,000 samples ($M=10$).}
\label{fig:overlaphis}
\end{center}
\vskip -0.2in
\end{figure}

We then study how many cases where our prediction edge set $\Pi$ covers the ground truth solution. Figure~\ref{fig:overlaphis} illustrates how the number of fully covered instances ($\eta = 1$) changes with time. After training the model for 100 epochs, we observed 780 fully covered instances in 1,000 validation samples using SAG while 0 instances using GCN. Finally, we  calculate the average of size $|\Pi|$. Our results show that SAG has an average size of $583.134$ edges, while for GCN, the number is $738.739$.  

These results also indicate that there is a correspondence between the loss and the quality of our prediction. 
In most SL tasks such as classification or regression tasks, a smaller validation loss usually means we achieve better performance and the minimum of the loss corresponds to the global optimal solution (100\% accuracy). However, it is no theoretical guarantee that our loss in Equation~\ref{eq:loss} is also a measure of the solution quality.
Our empirical results demonstrate that a lower surrogate loss encourages the model to assign larger weights on the promising edges and reduces the search space. This implies that we can assess the quality of the generated heat maps using our loss in Equation~\ref{eq:loss}.


We also compare the prediction edge sets and our results demonstrate that smooth representations fail to reduce the search space. Figure~\ref{fig:GGNheattop10} and Figure~\ref{fig:SCTheattop10} illustrate the difference of prediction edge sets between GCN and SAG. Figure~\ref{fig:GGNheattop10} and Figure~\ref{fig:SCTheattop10} are generated using the heat map in Figure~\ref{fig:GGNheat} and Figure~\ref{fig:SCTheat} with $M=10$, respectively. The light green regions correspond to the prediction edge set $\Pi$.  The $x$-axis  and $y$-axis  are the city indices, a light green box with position $(i,j)$ means edge $(i,j)$ belongs to $\Pi$. 
\begin{figure}[!htb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figures/GCNHeatMapTop10.png}}
\caption{Illustration of how the prediction edge set covers the ground truth edges using GCN. Light green: the prediction edge set $\Pi$; blue: $\Pi$ contains the ground truth edge, red: $\Pi$ misses the ground truth edge.}
\label{fig:GGNheattop10}
\end{center}
\vskip -0.2in
\end{figure}
When using GCN, as shown in Figure~\ref{fig:GGNheattop10}, we observe more continuous light green regions comparing to Figure~\ref{fig:SCTheattop10}. As mentioned before, a low-pass model will enforce similarity on neighboring nodes and lead to unfavorable representations.  The continuous regions in Figure~\ref{fig:GGNheattop10} are the direct result of oversmoothness. We observe fewer continuous light green regions when using SAG, this suggest that the model helps alleviate the oversmoothing problem and generates a more distinguishable representation.  
\begin{figure}[!htb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figures/SCTHeatMapTop10.png}}
\caption{Illustration of how the prediction edge set covers the ground truth edges using SAG. Light green: the prediction edge set $\Pi$; blue: $\Pi$ contains the ground truth edge, red: $\Pi$ misses the ground truth edge.}
\label{fig:SCTheattop10}
\end{center}
\vskip -0.2in
\end{figure}
We further study how the prediction sets cover the ground truth solution. In Figure~\ref{fig:GGNheattop10} and~\ref{fig:SCTheattop10}, blue and red boxes are the egdes in ground truth solution. A blue box with position $(i,j)$ corresponds to the condition that our prediction set $\Pi$ covers the right edge $(i,j)$,  while a red box at position $(i,j)$ means there is a ground truth edge $(i,j)$ but $\Pi$ fails to cover it. When using GCN, we observe 118 red boxes and 82 blue boxes, this means the GCN misses 59 correct edges, while SAG's prediction set successfully covers all the right edges.

Overall, the GCN's prediction set has 875 edges (1,750 light green boxes) and  SAG's prediction set $\Pi$ has 614 edges (1,228 light green boxes).  Although the low-pass model has a larger prediction set $\Pi$, it still falls short of covering the right edges. This emphasizes the importance of including band-pass filters and overcoming the oversmoothness problem.





 



