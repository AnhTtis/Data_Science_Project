
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries
@article{Burton20,
author = {Burton, Jason W. and Stein, Mari-Klara and Jensen, Tina Blegind},
title = {A systematic review of algorithm aversion in augmented decision making},
journal = {Journal of Behavioral Decision Making},
volume = {33},
number = {2},
pages = {220-239},
keywords = {algorithm aversion, augmented decision making, human-algorithm interaction, systematic review},
doi = {https://doi.org/10.1002/bdm.2155},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bdm.2155},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/bdm.2155},
abstract = {Abstract Despite abundant literature theorizing societal implications of algorithmic decision making, relatively little is known about the conditions that lead to the acceptance or rejection of algorithmically generated insights by individual users of decision aids. More specifically, recent findings of algorithm aversion—the reluctance of human forecasters to use superior but imperfect algorithms—raise questions about whether joint human-algorithm decision making is feasible in practice. In this paper, we systematically review the topic of algorithm aversion as it appears in 61 peer-reviewed articles between 1950 and 2018 and follow its conceptual trail across disciplines. We categorize and report on the proposed causes and solutions of algorithm aversion in five themes: expectations and expertise, decision autonomy, incentivization, cognitive compatibility, and divergent rationalities. Although each of the presented themes addresses distinct features of an algorithmic decision aid, human users of the decision aid, and/or the decision making environment, apparent interdependencies are highlighted. We conclude that resolving algorithm aversion requires an updated research program with an emphasis on theory integration. We provide a number of empirical questions that can be immediately carried forth by the behavioral decision making community.},
year = {2020}
}

@Article{Dietvorst15,
  author        = "Berkeley J. Dietvorst and Joseph P. Simmons and Cade Massey",
  title         = "Algorithm aversion: People erroneously avoid algorithms after seeing them err.",
  journal       = "Journal of Experimental Psychology: General",
  volume        = 144,
  issue         = 1,
  year          = 2015,
  pages         = {220-239},
  doi           = "10.1037/xge0000033",
  url           = "https://doi.org/10.1037/xge0000033",
}


@article{Dietvorst18,
author = {Dietvorst, Berkeley J. and Simmons, Joseph P. and Massey, Cade},
title = {Overcoming Algorithm Aversion: People Will Use Imperfect Algorithms If They Can (Even Slightly) Modify Them},
journal = {Management Science},
volume = {64},
number = {3},
pages = {1155-1170},
year = {2018},
doi = {10.1287/mnsc.2016.2643},

URL = { 
        https://doi.org/10.1287/mnsc.2016.2643
    
},
eprint = { 
        https://doi.org/10.1287/mnsc.2016.2643
    
}
,
    abstract = { Although evidence-based algorithms consistently outperform human forecasters, people often fail to use them after learning that they are imperfect, a phenomenon known as algorithm aversion. In this paper, we present three studies investigating how to reduce algorithm aversion. In incentivized forecasting tasks, participants chose between using their own forecasts or those of an algorithm that was built by experts. Participants were considerably more likely to choose to use an imperfect algorithm when they could modify its forecasts, and they performed better as a result. Notably, the preference for modifiable algorithms held even when participants were severely restricted in the modifications they could make (Studies 1–3). In fact, our results suggest that participants’ preference for modifiable algorithms was indicative of a desire for some control over the forecasting outcome, and not for a desire for greater control over the forecasting outcome, as participants’ preference for modifiable algorithms was relatively insensitive to the magnitude of the modifications they were able to make (Study 2). Additionally, we found that giving participants the freedom to modify an imperfect algorithm made them feel more satisfied with the forecasting process, more likely to believe that the algorithm was superior, and more likely to choose to use an algorithm to make subsequent forecasts (Study 3). This research suggests that one can reduce algorithm aversion by giving people some control—even a slight amount—over an imperfect algorithm’s forecast. Data, as supplemental material, are available at https://doi.org/10.1287/mnsc.2016.2643. This paper was accepted by Yuval Rottenstreich, judgment and decision making. }
}



@inproceedings{Vaccaro18,
title = "The illusion of control: Placebo effects of control settings",
keywords = "Control settings, Placebo effect, Sensemaking, Social media",
author = "Kristen Vaccaro and Dylan Huang and Motahhare Eslami and Christian Sandvig and Kevin Hamilton and Karrie Karahalios",
year = "2018",
month = apr,
day = "20",
doi = "10.1145/3173574.3173590",
language = "English (US)",
series = "Conference on Human Factors in Computing Systems - Proceedings",
publisher = "Association for Computing Machinery",
booktitle = "CHI 2018 - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems",
}


@article{Ananny18,
author = {Mike Ananny and Kate Crawford},
title ={Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability},
journal = {New Media \& Society},
volume = {20},
number = {3},
pages = {973-989},
year = {2018},
doi = {10.1177/1461444816676645},

URL = { 
        https://doi.org/10.1177/1461444816676645
    
},
eprint = { 
        https://doi.org/10.1177/1461444816676645
    
}
,
    abstract = { Models for understanding and holding systems accountable have long rested upon ideals and logics of transparency. Being able to see a system is sometimes equated with being able to know how it works and govern it—a pattern that recurs in recent work about transparency and computational systems. But can “black boxes’ ever be opened, and if so, would that ever be sufficient? In this article, we critically interrogate the ideal of transparency, trace some of its roots in scientific and sociotechnical epistemological cultures, and present 10 limitations to its application. We specifically focus on the inadequacy of transparency for understanding and governing algorithmic systems and sketch an alternative typology of algorithmic accountability grounded in constructive engagements with the limitations of transparency ideals. }
}


@Article{Simmons2020,
  author        = "Ric Simmons",
  title         = " Big Data and Procedural Justice: Legitimizing Algorithms in the Criminal Justice System",
  journal       = "Ohio State Journal of Criminal Law",
  volume        = 144,
  month         = "July",
  year          = 2020,
  doi           = "10.2139/ssrn.3659347",
  url           = "http://dx.doi.org/10.2139/ssrn.3659347",
}



@Article{Bies93,
  author        = "Bies, Robert J. and Martin, Christopher L. and Brockner, Joel",
  title         = "Just laid off, but still a “good citizen?” only if the process is fair",
  journal       = "Employee Responsibilities and Rights Journal",
  volume        = 6,
  issue = 3,
  month         = 9,
  year          = 1993,
  doi           = "10.1007/BF01419446",
  abstract = "Recent United States Congress legislation (the WARN Act of 1988) mandates that organizations must provide at least 60 days notice before a layoff of 50 or more employees can be instituted. As a consequence, individuals who are notified of their layoff often remain in their jobs for a significant period of time-and managers hope that these people will be good “organizational citizens” during thisremaining time. This article identifies different psychological factors that could explain why individuals would remain good citizens, even after notification of their impending termination. In a survey of 147 skilled employees who received notification of their layoffs, we found that the perceived fairness of the layoff process was the primary factor influencing their citizenship behavior. Additional analyses suggested that the perceived adequacy of the explanation of the layoffs, and whether the layoff victims were treated with respect and dignity, were the primary factors influencing the perceived fairness of the layoff process. The theoretical implications of these results are discussed.",
  url           = " https://doi.org/10.1007/BF01419446",
}

@article{Mazerolle13,
author = {Mazerolle, Lorraine and Antrobus, Emma and Bennett, Sarah and Tyler, Tom R.},
title = {Shaping Citizen Perceptions of Police Legitimacy: A Randomized Field Trial of Procedural Justice},
journal = {Criminology},
volume = {51},
number = {1},
pages = {33-63},
keywords = {legitimacy policing, procedural justice, randomized field trial, random breath tests},
doi = {https://doi.org/10.1111/j.1745-9125.2012.00289.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1745-9125.2012.00289.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1745-9125.2012.00289.x},
abstract = {Exploring the relationship between procedural justice and citizen perceptions of police is a well-trodden pathway. Studies show that when citizens perceive the police acting in a procedurally just manner—by treating people with dignity and respect, and by being fair and neutral in their actions—they view the police as legitimate and are more likely to comply with directives and cooperate with police. Our article examines both the direct and the indirect outcomes of procedural justice policing, tested under randomized field trial conditions. We assess whether police can enhance perceptions of legitimacy during a short, police-initiated and procedurally just traffic encounter and how this single encounter shapes general views of police. Our results show significant differences between the control and experimental conditions: Procedurally just traffic encounters with police (experimental condition) shape citizen views about the actual encounter directly and general orientations toward the police relative to business-as-usual traffic stops in the control group. The theorized model is supported by our research, demonstrating that the police have much to gain from acting fairly during even short encounters with citizens.},
year = {2013}
}

@article{Sunshine03,
 ISSN = {00239216, 15405893},
 URL = {http://www.jstor.org/stable/1555077},
 abstract = {This study explores two issues about police legitimacy. The first issue is the relative importance of police legitimacy in shaping public support of the police and policing activities, compared to the importance of instrumental judgments about (1) the risk that people will be caught and sanctioned for wrongdoing, (2) the performance of the police in fighting crime, and/or (3) the fairness of the distribution of police services. Three aspects of public support for the police are examined: public compliance with the law, public cooperation with the police, and public willingness to support policies that empower the police. The second issue is which judgments about police activity determine people's views about the legitimacy of the police. This study compares the influence of people's judgments about the procedural justice of the manner in which the police exercise their authority to the influence of three instrumental judgments: risk, performance, and distributive fairness. Findings of two surveys of New Yorkers show that, first, legitimacy has a strong influence on the public's reactions to the police, and second, the key antecedent of legitimacy is the fairness of the procedures used by the police. This model applies to both white and minority group residents.},
 author = {Jason Sunshine and Tom R. Tyler},
 journal = {Law \& Society Review},
 number = {3},
 pages = {513--548},
 publisher = {[Wiley, Law and Society Association]},
 title = {The Role of Procedural Justice and Legitimacy in Shaping Public Support for Policing},
 volume = {37},
 year = {2003}
}

@article{OTTING201827,
title = "The importance of procedural justice in Human–Machine Interactions: Intelligent systems as new decision agents in organizations",
journal = "Computers in Human Behavior",
volume = "89",
pages = "27 - 39",
year = "2018",
issn = "0747-5632",
doi = "https://doi.org/10.1016/j.chb.2018.07.022",
url = "http://www.sciencedirect.com/science/article/pii/S0747563218303443",
author = "Sonja K. Ötting and Günter W. Maier",
keywords = "Procedural justice, Human–machine interaction, Decision agent, Source of justice, Employee behavior and attitudes, Experimental vignette study",
abstract = "In the present study, the effects of procedural justice (fair or unfair) and the type of decision agent (human, robot, or computer) on employee behavior and attitudes (e.g., job satisfaction, organizational citizenship behavior, or counterproductive work behaviors) were examined. It was predicted that the type of decision agent (or the source of justice) would moderate the relationship between procedural justice and employee behavior and attitudes, with the relationship being strongest when the decision agent is a human team leader, medium when the decision agent is a humanoid robot, and weakest when the agent is a computer system. This research question was investigated with a between-subjects design in two experiments (N1 = 149 and N2 = 145) that displayed two different decision situations in organizations (allocation of new tasks and allocation of further vocational trainings). Results of both studies showed significant effects of procedural justice on employee behavior and attitudes, confirming the importance of procedural justice at the workplace for both human and system decision agents. Furthermore, both studies failed to verify any interaction effects of procedural justice and the decision agent. This further emphasizes the importance of procedural justice in decision situations because there is no difference in reactions to procedural justice of human or system decisions. Limitations and implications for future research and the integration of justice and human–machine interaction research are discussed."
}


@article{Herian12,
    author = {Herian, Mitchel N. and Hamm, Joseph A. and Tomkins, Alan J. and Pytlik Zillig, Lisa M.},
    title = "{Public Participation, Procedural Fairness, and Evaluations of Local Governance: The Moderating Role of Uncertainty}",
    journal = {Journal of Public Administration Research and Theory},
    volume = {22},
    number = {4},
    pages = {815-840},
    year = {2012},
    month = {01},
    abstract = "{The purpose of this article is to test whether the use of public participation by a local government increases perceptions of procedural fairness among the public and to propose an explanation for why fairness is a strong predictor of satisfaction with governmental decisions. To do this, we draw on the uncertainty management model to hypothesize that indications of procedural fairness can increase public support for government and its decisions and that fairness effects are greater for individuals who are more uncertain (less knowledgeable) about the governmental body in question. To test the hypothesis, we embedded an experiment in a survey of the public that was used by a local government to inform its budgetary decisions. The results provide support for the notion that governmental use of public input does increase perceptions of governmental fairness and that, in turn, perceptions of fairness have stronger relationships with overall governmental assessments for those who are relatively uncertain about a governmental institution.}",
    issn = {1053-1858},
    doi = {10.1093/jopart/mur064},
    url = {https://doi.org/10.1093/jopart/mur064},
    eprint = {https://academic.oup.com/jpart/article-pdf/22/4/815/2779202/mur064.pdf},
}

@article{Zafar_Gummadi_Weller_2018, title={Beyond Distributive Fairness in Algorithmic Decision Making: Feature Selection for Procedurally Fair Learning}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11296}, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Grgić-Hlača, Nina and Zafar, Muhammad Bilal and Gummadi, Krishna P. and Weller, Adrian}, year={2018}, month={Apr.} }

@article {Dawes1989,
	author = {Dawes, RM and Faust, D and Meehl, PE},
	title = {Clinical versus actuarial judgment},
	volume = {243},
	number = {4899},
	pages = {1668--1674},
	year = {1989},
	doi = {10.1126/science.2648573},
	publisher = {American Association for the Advancement of Science},
	abstract = {Professionals are frequently consulted to diagnose and predict human behavior; optimal treatment and planning often hinge on the consultant{\textquoteright}s judgmental accuracy. The consultant may rely on one of two contrasting approaches to decision-making--the clinical and actuarial methods. Research comparing these two approaches shows the actuarial method to be superior. Factors underlying the greater accuracy of actuarial methods, sources of resistance to the scientific findings, and the benefits of increased reliance on actuarial approaches are discussed.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/243/4899/1668},
	eprint = {https://science.sciencemag.org/content/243/4899/1668.full.pdf},
	journal = {Science}
}


@article{Castelo2019,
author = {Noah Castelo and Maarten W. Bos and Donald R. Lehmann},
title ={Task-Dependent Algorithm Aversion},
journal = {Journal of Marketing Research},
volume = {56},
number = {5},
pages = {809-825},
year = {2019},
doi = {10.1177/0022243719851788},

URL = { 
        https://doi.org/10.1177/0022243719851788
    
},
eprint = { 
        https://doi.org/10.1177/0022243719851788
    
}
,
    abstract = { Research suggests that consumers are averse to relying on algorithms to perform tasks that are typically done by humans, despite the fact that algorithms often perform better. The authors explore when and why this is true in a wide variety of domains. They find that algorithms are trusted and relied on less for tasks that seem subjective (vs. objective) in nature. However, they show that perceived task objectivity is malleable and that increasing a task’s perceived objectivity increases trust in and use of algorithms for that task. Consumers mistakenly believe that algorithms lack the abilities required to perform subjective tasks. Increasing algorithms’ perceived affective human-likeness is therefore effective at increasing the use of algorithms for subjective tasks. These findings are supported by the results of four online lab studies with over 1,400 participants and two online field studies with over 56,000 participants. The results provide insights into when and why consumers are likely to use algorithms and how marketers can increase their use when they outperform humans. }
}


@article{Prahl2017,
author = {Prahl, Andrew and Van Swol, Lyn},
title = {Understanding algorithm aversion: When is advice from automation discounted?},
journal = {Journal of Forecasting},
volume = {36},
number = {6},
pages = {691-702},
keywords = {advice, algorithm aversion, automation, computers, trust},
doi = {https://doi.org/10.1002/for.2464},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.2464},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/for.2464},
abstract = {Abstract Forecasting advice from human advisors is often utilized more than advice from automation. There is little understanding of why “algorithm aversion” occurs, or specific conditions that may exaggerate it. This paper first reviews literature from two fields—interpersonal advice and human–automation trust—that can inform our understanding of the underlying causes of the phenomenon. Then, an experiment is conducted to search for these underlying causes. We do not replicate the finding that human advice is generally utilized more than automated advice. However, after receiving bad advice, utilization of automated advice decreased significantly more than advice from humans. We also find that decision makers describe themselves as having much more in common with human than automated advisors despite there being no interpersonal relationship in our study. Results are discussed in relation to other findings from the forecasting and human–automation trust fields and provide a new perspective on what causes and exaggerates algorithm aversion.},
year = {2017}
}

@article{Logg2017TheoryOM,
  title={Theory of machine: When do people rely on algorithms?},
  author={Logg, Jennifer Marie},
  journal={Harvard Business School working paper series\# 17-086},
  year={2017}
}

@article{Tyler2015,
author = {Tom R. Tyler and Phillip Atiba Goff and Robert J. MacCoun},
title ={The Impact of Psychological Science on Policing in the United States: Procedural Justice, Legitimacy, and Effective Law Enforcement},
journal = {Psychological Science in the Public Interest},
volume = {16},
number = {3},
pages = {75-109},
year = {2015},
doi = {10.1177/1529100615617791},
    note ={PMID: 26635334},

URL = { 
        https://doi.org/10.1177/1529100615617791
    
},
eprint = { 
        https://doi.org/10.1177/1529100615617791
    
}
,
    abstract = { The May 2015 release of the report of the President’s Task Force on 21st Century Policing highlighted a fundamental change in the issues dominating discussions about policing in America. That change has moved discussions away from a focus on what is legal or effective in crime control and toward a concern for how the actions of the police influence public trust and confidence in the police. This shift in discourse has been motivated by two factors—first, the recognition by public officials that increases in the professionalism of the police and dramatic declines in the rate of crime have not led to increases in police legitimacy, and second, greater awareness of the limits of the dominant coercive model of policing and of the benefits of an alternative and more consensual model based on public trust and confidence in the police and legal system. Psychological research has played an important role in legitimating this change in the way policymakers think about policing by demonstrating that perceived legitimacy shapes a set of law-related behaviors as well as or better than concerns about the risk of punishment. Those behaviors include compliance with the law and cooperation with legal authorities. These findings demonstrate that legal authorities gain by a focus on legitimacy. Psychological research has further contributed by articulating and demonstrating empirical support for a central role of procedural justice in shaping legitimacy, providing legal authorities with a clear road map of strategies for creating and maintaining public trust. Given evidence of the benefits of legitimacy and a set of guidelines concerning its antecedents, policymakers have increasingly focused on the question of public trust when considering issues in policing. The acceptance of a legitimacy-based consensual model of police authority building on theories and research studies originating within psychology illustrates how psychology can contribute to the development of evidence-based policies in the field of criminal law. }
}

@article{dawes1979robust,
  title={The robust beauty of improper linear models in decision making.},
  author={Dawes, Robyn M},
  journal={American psychologist},
  volume={34},
  number={7},
  pages={571},
  year={1979},
  publisher={American Psychological Association}
}




@article{Bosk18,
author = {Emily Adlin Bosk},
title = {What counts? quantification, worker judgment, and divergence in child welfare decision making},
journal = {Human Service Organizations: Management, Leadership \& Governance},
volume = {42},
number = {2},
pages = {205-224},
year  = {2018},
publisher = {Routledge},
doi = {10.1080/23303131.2017.1422068},

URL = { 
        https://doi.org/10.1080/23303131.2017.1422068
    
},
eprint = { 
        https://doi.org/10.1080/23303131.2017.1422068
}}

@article{CHOR2013871,
title = {Patterns of out-of-home placement decision-making in child welfare},
journal = {Child Abuse \& Neglect},
volume = {37},
number = {10},
pages = {871-882},
year = {2013},
issn = {0145-2134},
doi = {https://doi.org/10.1016/j.chiabu.2013.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S0145213413001245},
author = {Ka Ho Brian Chor and Gary M. McClelland and Dana A. Weiner and Neil Jordan and John S. Lyons},
keywords = {Child welfare, Out-of-home placements, Team decision-making, Decision support algorithm, Log-linear modeling},
abstract = {Out-of-home placement decision-making in child welfare is founded on the best interest of the child in the least restrictive setting. After a child is removed from home, however, little is known about the mechanism of placement decision-making. This study aims to systematically examine the patterns of out-of-home placement decisions made in a state's child welfare system by comparing two models of placement decision-making: a multidisciplinary team decision-making model and a clinically based decision support algorithm. Based on records of 7816 placement decisions representing 6096 children over a 4-year period, hierarchical log-linear modeling characterized concordance or agreement, and discordance or disagreement when comparing the two models and accounting for age-appropriate placement options. Children aged below 16 had an overall concordance rate of 55.7, most apparent in the least restrictive (20.4) and the most restrictive placement (18.4). Older youth showed greater discordant distributions (62.9). Log-linear analysis confirmed the overall robustness of concordance (odd ratios [ORs] range: 2.9–442.0), though discordance was most evident from small deviations from the decision support algorithm, such as one-level under-placement in group home (OR=5.3) and one-level over-placement in residential treatment center (OR=4.8). Concordance should be further explored using child-level clinical and placement stability outcomes. Discordance might be explained by dynamic factors such as availability of placements, caregiver preferences, or policy changes and could be justified by positive child-level outcomes. Empirical placement decision-making is critical to a child's journey in child welfare and should be continuously improved to effect positive child welfare outcomes.}
}

@inproceedings{10.1145/3313831.3376229,
author = {Saxena, Devansh and Badillo-Urquiola, Karla and Wisniewski, Pamela J. and Guha, Shion},
title = {A Human-Centered Review of Algorithms Used within the U.S. Child Welfare System},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376229},
doi = {10.1145/3313831.3376229},
abstract = {The U.S. Child Welfare System (CWS) is charged with improving outcomes for foster youth; yet, they are overburdened and underfunded. To overcome this limitation, several states have turned towards algorithmic decision-making systems to reduce costs and determine better processes for improving CWS outcomes. Using a human-centered algorithmic design approach, we synthesize 50 peer-reviewed publications on computational systems used in CWS to assess how they were being developed, common characteristics of predictors used, as well as the target outcomes. We found that most of the literature has focused on risk assessment models but does not consider theoretical approaches (e.g., child-foster parent matching) nor the perspectives of caseworkers (e.g., case notes). Therefore, future algorithms should strive to be context-aware and theoretically robust by incorporating salient factors identified by past research. We provide the HCI community with research avenues for developing human-centered algorithms that redirect attention towards more equitable outcomes for CWS.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–15},
numpages = {15},
keywords = {human-centered algorithm design, algorithmic decision-making, child welfare system},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@article{estiri16,
author = {Estiri, Hossein and Lovins, Terri and Afzalan, Nader and Stephens, Kari},
year = {2016},
month = {07},
pages = {60-7},
title = {Applying a Participatory Design Approach to Define Objectives and Properties of a "Data Profiling" Tool for Electronic Health Data},
volume = {2016},
journal = {AMIA Joint Summits on Translational Science proceedings. AMIA Summit on Translational Science}
}

@article{Day_Humphrey_Cockcroft_2017, place={Australia}, title={How do the design features of health hackathons contribute to participatory medicine?}, volume={21}, url={https://journal.acs.org.au/index.php/ajis/article/view/1383}, DOI={10.3127/ajis.v21i0.1383}, abstractNote={The Hackathon concept is attracting interest as a vehicle for participatory development in both Health and Information systems. Publically available datasets, cloud based data storage, and increasingly sophisticated analytical methods, combined with user friendly development tools for mobile devices are inspiring innovation in the participatory medicine space. This has the potential to disrupt traditional methods and deliver solutions more rapidly, and in a form more likely to meet requirements. In health applications this involves putting the patient and their supports at the centre of design. This work contributes to solving the challenges involved in bringing a diverse cohort of designers, developers, problem owners, healthcare providers, patients, and citizens together to solve user-driven self-care problems using technology. We use a descriptive case study approach focussing on two weekend-long hackathons dubbed “Health Hackathon: Solving Self-care”. We gather thick data from multiple sources according to the process defined by Geertz (1994) first, to provide a rich picture of the role of hackathons in participatory medicine and second, to contribute evidence to the practise of running a hackathon. Some key originalities of our work include seeking more candid responses via self-serve interviews. Through this, controversially, we noted a marked emphasis on the creative process over concerns for privacy and ethics around the personal data cloud created by hackathon products. We build on existing theories of participatory medicine and emerging methodologies for conducting hackathons to provide evidence of the efficacy of the hacking approach both in terms of outcome and team dynamics. Through interviews, observation, twitter feeds and a pre-survey, we identify a number of success factors including (1) group size, (2) maturity of the idea, (3) level of involvement of a mentor, and (4) involvement of students. In addition we identify five skills identified by successful health hackathon participants; knowledge, patient focussed skills, analytical skills, software design skills and professional perspective. In common with previous studies we find that there are considerable social benefits that accrue in running a hackathon. Participants meet new people and learn first-hand of the challenges and opportunities provided by the skill sets and work environments of others. This work builds on the existing body of research concerning hackathons and in particular work in the context of participatory medicine}, journal={Australasian Journal of Information Systems}, author={Day, Karen and Humphrey, Gayl and Cockcroft, Sophie}, year={2017}, month={Mar.} }

@inproceedings{Saxena20,
author = {Saxena, Devansh and Guha, Shion},
title = {Conducting Participatory Design to Improve Algorithms in Public Services: Lessons and Challenges},
year = {2020},
isbn = {9781450380591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406865.3418331},
doi = {10.1145/3406865.3418331},
abstract = {Government agencies are increasingly looking towards algorithmic decision-making systems as a means to reduce costs and optimize processes. However, these algorithms are being constructed in an opaque and isolated manner with calls to adopt a more participatory approach such that stakeholders become co-designers in the process. We share our experiences from conducting participatory design to improve algorithms in the Child-Welfare System. We discuss a policy-mandated algorithm and an agency-level theory-driven algorithm to show how tensions arise when the values of workers are not embedded in the design of an algorithm.},
booktitle = {Conference Companion Publication of the 2020 on Computer Supported Cooperative Work and Social Computing},
pages = {383–388},
numpages = {6},
keywords = {child welfare system, participatory design, algorithmic decision-making},
location = {Virtual Event, USA},
series = {CSCW '20 Companion}
}

@misc{kastrenakes_2021, title={Twitter's Jack Dorsey wants to build an app store for social media algorithms}, url={https://www.theverge.com/2021/2/9/22275441/jack-dorsey-decentralized-app-store-algorithms}, journal={The Verge}, publisher={The Verge}, author={Kastrenakes, Jacob}, year={2021}, month={Feb}}

@article{Camerer99,
 ISSN = {08955646, 15730476},
 URL = {http://www.jstor.org/stable/41760945},
 abstract = {We review 74 experiments with no, low, or high performance-based financial incentives. The modal result is no effect on mean performance (though variance is usually reduced by higher payment). Higher incentive does improve performance often, typically judgment tasks that are responsive to better effort. Incentives also reduce "presentation" effects (e.g., generosity and risk-seeking). Incentive effects are comparable to effects of other variables, particularly "cognitive capital" and task "production" demands, and interact with those variables, so a narrow-minded focus on incentives alone is misguided. We also note that no replicated study has made rationality violations disappear purely by raising incentives.},
 author = {COLIN F. CAMERER and ROBIN M. HOGARTH},
 journal = {Journal of Risk and Uncertainty},
 number = {1/3},
 pages = {7--42},
 publisher = {Springer},
 title = {The Effects of Financial Incentives in Experiments: A Review and Capital-Labor-Production Framework},
 volume = {19},
 year = {1999}
}

@misc{american_civil_liberties_union_2016, title={Statement of Concern About Predictive Policing by ACLU and 16 Civil Rights Privacy, Racial Justice, and Technology Organizations}, url={https://www.aclu.org/other/statement-concern-about-predictive-policing-aclu-and-16-civil-rights-privacy-racial-justice}, journal={American Civil Liberties Union}, year={2016}, month={Aug}}

@inproceedings{Madisson18,
author = {Whitman, Madisson and Hsiang, Chien-yi and Roark, Kendall},
title = {Potential for Participatory Big Data Ethics and Algorithm Design: A Scoping Mapping Review},
year = {2018},
isbn = {9781450355742},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210604.3210644},
doi = {10.1145/3210604.3210644},
abstract = {Ubiquitous networked data collection and algorithm-based information systems have the potential to disparately impact lives around the planet and pose a host of emerging ethical challenges. One response has been a call for more transparency and democratic control over the design and implementation of such systems. This scoping mapping review focuses on participatory approaches to the design, governance, and future of these systems across a wide variety of contexts and domains.1},
booktitle = {Proceedings of the 15th Participatory Design Conference: Short Papers, Situated Actions, Workshops and Tutorial - Volume 2},
articleno = {5},
numpages = {6},
keywords = {speculative design, research through design, algorithm design},
location = {Hasselt and Genk, Belgium},
series = {PDC '18}
}

@inproceedings{Qian16,
author = {Yang, Qian and Zimmerman, John and Steinfeld, Aaron and Carey, Lisa and Antaki, James F.},
title = {Investigating the Heart Pump Implant Decision Process: Opportunities for Decision Support Tools to Help},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858373},
doi = {10.1145/2858036.2858373},
abstract = {Clinical decision support tools (DSTs) are computational systems that aid healthcare decision-making. While effective in labs, almost all these systems failed when they moved into clinical practice. Healthcare researchers speculated it is most likely due to a lack of user-centered HCI considerations in the design of these systems. This paper describes a field study investigating how clinicians make a heart pump implant decision with a focus on how to best integrate an intelligent DST into their work process. Our findings reveal a lack of perceived need for and trust of machine intelligence, as well as many barriers to computer use at the point of clinical decision-making. These findings suggest an alternative perspective to the traditional use models, in which clinicians engage with DSTs at the point of making a decision. We identify situations across patients' healthcare trajectories when decision supports would help, and we discuss new forms it might take in these situations.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {4477–4488},
numpages = {12},
keywords = {decision support tools, service design, clinical decision support systems, qualitative methods, field study},
location = {San Jose, California, USA},
series = {CHI '16}
}

@article{WeBuildAI,
author = {Lee, Min Kyung and Kusbit, Daniel and Kahng, Anson and Kim, Ji Tae and Yuan, Xinran and Chan, Allissa and See, Daniel and Noothigattu, Ritesh and Lee, Siheon and Psomas, Alexandros and Procaccia, Ariel D.},
title = {WeBuildAI: Participatory Framework for Algorithmic Governance},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359283},
doi = {10.1145/3359283},
abstract = {Algorithms increasingly govern sotal functions, impacting multiple stakeholders and social groups. How can we design these algorithms to balance varying interests in a moral, legitimate way? As one answer to this question, we present WeBuildAI, a collective participatory framework that enables people to build algorithmic policy for their communities. The key idea of the framework is to enable stakeholders to construct a computational model that represents their views and to have those models vote on their behalf to create algorithmic policy. As a case study, we applied this framework to a matching algorithm that operates an on-demand food donation transportation service in order to adjudicate equity and efficiency trade-offs. The service's stakeholders--donors, volunteers, recipient organizations, and nonprofit employees--used the framework to design the algorithm through a series of studies in which we researched their experiences. Our findings suggest that the framework successfully enabled participants to build models that they felt confident represented their own beliefs. Participatory algorithm design also improved both procedural fairness and the distributive outcomes of the algorithm, raised participants' algorithmic awareness, and helped identify inconsistencies in human decision-making in the governing organization. Our work demonstrates the feasibility, potential and challenges of community involvement in algorithm design.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {181},
numpages = {35},
keywords = {matching algorithm, collective participation, participatory algorithm design, human-centered ai, algorithmic fairness}
}

@article{Lee19,
author = {Lee, Min Kyung and Jain, Anuraag and Cha, Hea Jin and Ojha, Shashank and Kusbit, Daniel},
title = {Procedural Justice in Algorithmic Fairness: Leveraging Transparency and Outcome Control for Fair Algorithmic Mediation},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359284},
doi = {10.1145/3359284},
abstract = {As algorithms increasingly take managerial and governance roles, it is ever more important to build them to be perceived as fair and adopted by people. With this goal, we propose a procedural justice framework in algorithmic decision-making drawing from procedural justice theory, which lays out elements that promote a sense of fairness among users. As a case study, we built an interface that leveraged two key elements of the framework---transparency and outcome control---and evaluated it in the context of goods division. Our interface explained the algorithm's allocative fairness properties (standards clarity) and outcomes through an input-output matrix (outcome explanation), then allowed people to interactively adjust the algorithmic allocations as a group (outcome control). The findings from our within-subjects laboratory study suggest that standards clarity alone did not increase perceived fairness; outcome explanation had mixed effects, increasing or decreasing perceived fairness and reducing algorithmic accountability; and outcome control universally improved perceived fairness by allowing people to realize the inherent limitations of decisions and redistribute the goods to better fit their contexts, and by bringing human elements into final decision-making.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {182},
numpages = {26},
keywords = {algorithmic mediation, division, algorithmic decision, control, transparency}
}

@article {Obermeyer447,
	author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
	title = {Dissecting racial bias in an algorithm used to manage the health of populations},
	volume = {366},
	number = {6464},
	pages = {447--453},
	year = {2019},
	doi = {10.1126/science.aax2342},
	publisher = {American Association for the Advancement of Science},
	abstract = {The U.S. health care system uses commercial algorithms to guide health decisions. Obermeyer et al. find evidence of racial bias in one widely used algorithm, such that Black patients assigned the same level of risk by the algorithm are sicker than White patients (see the Perspective by Benjamin). The authors estimated that this racial bias reduces the number of Black patients identified for extra care by more than half. Bias occurs because the algorithm uses health costs as a proxy for health needs. Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care.Science, this issue p. 447; see also p. 421Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/366/6464/447},
	eprint = {https://science.sciencemag.org/content/366/6464/447.full.pdf},
	journal = {Science}
}

@article{casey2017intertemporal,
  title={Intertemporal differences among MTurk workers: Time-based sample variations and implications for online data collection},
  author={Casey, Logan S and Chandler, Jesse and Levine, Adam Seth and Proctor, Andrew and Strolovitch, Dara Z},
  journal={SAGE Open},
  volume={7},
  number={2},
  pages={2158244017712774},
  year={2017},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{berinsky_huber_lenz_2012, title={Evaluating Online Labor Markets for Experimental Research: Amazon.com's Mechanical Turk}, volume={20}, DOI={10.1093/pan/mpr057}, number={3}, journal={Political Analysis}, publisher={Cambridge University Press}, author={Berinsky, Adam J. and Huber, Gregory A. and Lenz, Gabriel S.}, year={2012}, pages={351–368}}

@inproceedings{Jordan06,
author = {Boyd-Graber, Jordan L. and Nikolova, Sonya S. and Moffatt, Karyn A. and Kin, Kenrick C. and Lee, Joshua Y. and Mackey, Lester W. and Tremaine, Marilyn M. and Klawe, Maria M.},
title = {Participatory Design with Proxies: Developing a Desktop-PDA System to Support People with Aphasia},
year = {2006},
isbn = {1595933727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1124772.1124797},
doi = {10.1145/1124772.1124797},
abstract = {In this paper, we describe the design and preliminary evaluation of a hybrid desktop-handheld system developed to support individuals with aphasia, a disorder which impairs the ability to speak, read, write, or understand language. The system allows its users to develop speech communication through images and sound on a desktop computer and download this speech to a mobile device that can then support communication outside the home. Using a desktop computer for input addresses some of this population's difficulties interacting with handheld devices, while the mobile device addresses stigma and portability issues. A modified participatory design approach was used in which proxies, that is, speech-language pathologists who work with aphasic individuals, assumed the role normally filled by users. This was done because of the difficulties in communicating with the target population and the high variability in aphasic disorders. In addition, the paper presents a case study of the proxy-use participatory design process that illustrates how different interview techniques resulted in different user feedback.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {151–160},
numpages = {10},
keywords = {participatory design, assistive technology, multi-modal interfaces, aphasia},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {CHI '06}
}

@inbook{Gorski20,
author = {Gorski, Peter Leo and Acar, Yasemin and Lo Iacono, Luigi and Fahl, Sascha},
title = {Listen to Developers! A Participatory Design Study on Security Warnings for Cryptographic APIs},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376142},
abstract = {The positive effect of security information communicated to developers through API warnings has been established. However, current prototypical designs are based on security warnings for end-users. To improve security feedback for developers, we conducted a participatory design study with 25 professional software developers in focus groups. We identify which security information is considered helpful in avoiding insecure cryptographic API use during development. Concerning console messages, participants suggested five core elements, namely message classification, title message, code location, link to detailed external resources, and color. Design guidelines for end-user warnings are only partially suitable in this context. Participants emphasized the importance of tailoring the detail and content of security information to the context. Console warnings call for concise communication; further information needs to be linked externally. Therefore, security feedback should transcend tools and should be adjustable by software developers across development tools, considering the work context and developer needs.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13}
}

@InProceedings{Sundblad11,
author="Sundblad, Yngve",
editor="Impagliazzo, John
and Lundin, Per
and Wangler, Benkt",
title="UTOPIA: Participatory Design from Scandinavia to the World",
booktitle="History of Nordic Computing 3",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="176--186",
abstract="Studies and design of information technology support for workplaces, especially workshop floors, office floors and hospital floors, have a strong tradition in Scandinavia, involving workplace users and their trade unions and other stakeholders. The projects emphasize the active cooperation between researchers and workers in the organizations to help improve their work situation. This tradition is analyzed in its historic perspective, starting with the roots in Norway in the early 1970s while highlighting the seminal UTOPIA project from the early 1980s. Today computer use and interaction possibilities are changing quickly with use contexts and application types radically broadening. Technology no longer consists of static tools belonging only to the workplace; it permeates work activity, homes, and everyday lives. The Scandinavian tradition of user involvement in development is facing up with the challenges of new contexts. The influence on past and current practices for international ICT system design is described and analyzed.",
isbn="978-3-642-23315-9"
}

@inproceedings{10.1145/3442188.3445938,
author = {Krafft, P. M. and Young, Meg and Katell, Michael and Lee, Jennifer E. and Narayan, Shankar and Epstein, Micah and Dailey, Dharma and Herman, Bernease and Tam, Aaron and Guetler, Vivian and Bintz, Corinne and Raz, Daniella and Jobe, Pa Ousman and Putz, Franziska and Robick, Brian and Barghouti, Bissan},
title = {An Action-Oriented AI Policy Toolkit for Technology Audits by Community Advocates and Activists},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445938},
doi = {10.1145/3442188.3445938},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {772–781},
numpages = {10},
keywords = {regulation, algorithmic equity, Participatory design, surveillance, accountability, algorithmic justice, participatory action research},
location = {Virtual Event, Canada},
series = {FAccT '21}
}


@book{pytlikzillig2018deliberative,
  title={Deliberative public engagement with science: An empirical investigation},
  author={PytlikZillig, Lisa M and Hutchens, Myiah J and Muhlberger, Peter and Gonzalez, Frank J and Tomkins, Alan J},
  year={2018},
  publisher={Springer Nature}
}

@misc{kaggle, title={House prices - advanced regression techniques}, url={https://www.kaggle.com/c/house-prices-advanced-regression-techniques}, organization={Kaggle}, author={Kaggle}, year={2022}} 

@article{kahneman1991anomalies,
  title={Anomalies: The endowment effect, loss aversion, and status quo bias},
  author={Kahneman, Daniel and Knetsch, Jack L and Thaler, Richard H},
  journal={Journal of Economic perspectives},
  volume={5},
  number={1},
  pages={193--206},
  year={1991}
}

@article{HOULDEN197813,
title = {Preference for modes of dispute resolution as a function of process and decision control},
journal = {Journal of Experimental Social Psychology},
volume = {14},
number = {1},
pages = {13-30},
year = {1978},
issn = {0022-1031},
doi = {https://doi.org/10.1016/0022-1031(78)90057-4},
url = {https://www.sciencedirect.com/science/article/pii/0022103178900574},
author = {Pauline Houlden and Stephen LaTour and Laurens Walker and John Thibaut},
abstract = {Research on procedural justice has suggested that the distribution of control among participants can be used to classify dispute-resolution procedures and may be an important determinant of preference for such procedures. This experiment demonstrates that control can be meaningfully divided into two components: control over the presentation of evidence and control over the final decision. The experiment placed subjects (law students and undergraduates) in a situation of conflict and varied two between-subjects factors: (1) Role, whether subjects expected to role-play third parties (law students) or litigants (undergraduates), and (2) Orientation, whether individuals focused on equity claims (appeals to a norm of fairness) or legal claims (appeals to a strict, legal interpretation of events). As a control, a third-party neutral-orientation condition was included. In addition, subjects were presented with four dispute-resolution procedures which varied in third-party control over the presentation of evidence (Process Control) and third-party control over the final decision (Decision Control) as within-subjects factors. Results revealed that both litigants and third parties preferred high rather than low third-party decision control. Litigants with an equity orientation preferred low third-party control over the presentation of evidence, particularly when third parties had high rather than low decision control. Third parties and litigants with a legal orientation preferred low rather than high third-party process control only when there was high third-party decision control. Litigant preferences were more affected by variation in process control than variation in decision control while third-party preferences were more affected by variation in decision control than in process control. As a check on external validity, military judges given a neutral orientation were asked to evaluate and express preferences for the four dispute-resolution procedures. Their results were not detectably different from those of the law students who role-played third parties in the main portion of the study.}
}

@inproceedings{ying2019understanding,
author = {Yin, Ming and Wortman Vaughan, Jennifer and Wallach, Hanna},
title = {Understanding the Effect of Accuracy on Trust in Machine Learning Models},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300509},
doi = {10.1145/3290605.3300509},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {machine learning, trust, human-subject experiments},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@article{10.1145/3479572,
author = {Fogliato, Riccardo and Chouldechova, Alexandra and Lipton, Zachary},
title = {The Impact of Algorithmic Risk Assessments on Human Predictions and Its Analysis via Crowdsourcing Studies},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479572},
doi = {10.1145/3479572},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {428},
numpages = {24},
keywords = {user study, algorithmic risk assessment instruments, human in-the-loop, algorithm-assisted decision-making}
}

@misc{pisa_product, title={Pisa products - Pisa}, url={https://www.oecd.org/pisa/pisaproducts/pisa2009database-downloadabledata.htm}, journal={PISA products - PISA}} 

@misc{mit_opencourseware, title={Reading Test Scores}, url={https://ocw.mit.edu/courses/15-071-the-analytics-edge-spring-2017/pages/linear-regression/assignment-2/reading-test-scores/}, author={MIT OpenCourseWare}, publisher={MIT OpenCourseWare}, year={2017}} 

@article{yechiam2019acceptable,
  title={Acceptable losses: The debatable origins of loss aversion},
  author={Yechiam, Eldad},
  journal={Psychological research},
  volume={83},
  number={7},
  pages={1327--1339},
  year={2019},
  publisher={Springer}
}

@article{sanders2021loss,
  title={Loss aversion fails to replicate in the coronavirus pandemic: Evidence from an online experiment},
  author={Sanders, Michael and Stockdale, Emma and Hume, Susannah and John, Peter},
  journal={Economics letters},
  volume={199},
  pages={109433},
  year={2021},
  publisher={Elsevier}
}

@incollection{wilson2013replichi,
  title={Replichi: the workshop},
  author={Wilson, Max LL and Resnick, Paul and Coyle, David and Chi, Ed H},
  booktitle={CHI'13 Extended Abstracts on Human Factors in Computing Systems},
  pages={3159--3162},
  year={2013}
}


@misc{9585852,
  doi = {10.48550/ARXIV.2110.15687},  
  url = {https://arxiv.org/abs/2110.15687},  
  author = {Hepperle, Daniel and Dienlin, Tobias and Wölfel, Matthias},  
  keywords = {Human-Computer Interaction (cs.HC), Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},  
  title = {Reducing the Human Factor in Virtual Reality Research to Increase Reproducibility and Replicability},  
  publisher = {arXiv},  
  year = {2021},  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}


@inproceedings{10.1145/3170427.3188395,
author = {Echtler, Florian and H\"{a}u\ss{}ler, Maximilian},
title = {Open Source, Open Science, and the Replication Crisis in HCI},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3188395},
doi = {10.1145/3170427.3188395},
abstract = {The open-source model of software development is an established and widely used method that has been making inroads into several scientific disciplines which use software, thereby also helping much-needed efforts at replication of scientific results. However, our own discipline of HCI does not seem to follow this trend so far. We analyze the entire body of papers from CHI 2016 and CHI 2017 regarding open-source releases, and compare our results with the discipline of bioinformatics. Based on our comparison, we suggest future directions for publication practices in HCI in order to improve scientific rigor and replicability.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–8},
numpages = {8},
keywords = {open source, hci, replication, open science},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@article{fowler2022frustration,
  title={Frustration and ennui among Amazon MTurk workers},
  author={Fowler, Craig and Jiao, Jian and Pitts, Margaret},
  journal={Behavior Research Methods},
  pages={1--17},
  year={2022},
  publisher={Springer}
}

@inproceedings{cheng2022child,
author = {Cheng, Hao-Fei and Stapleton, Logan and Kawakami, Anna and Sivaraman, Venkatesh and Cheng, Yanghuidi and Qing, Diana and Perer, Adam and Holstein, Kenneth and Wu, Zhiwei Steven and Zhu, Haiyi},
title = {How Child Welfare Workers Reduce Racial Disparities in Algorithmic Decisions},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501831},
doi = {10.1145/3491102.3501831},
articleno = {162},
numpages = {22},
keywords = {human-centered AI, machine learning, child welfare, algorithmic biases, algorithm-assisted decision-making},
location = {New Orleans, LA, USA},
series = {CHI '22}
}


@article{cheng2022heterogeneity,
author = {Cheng, Lingwei and Chouldechova, Alexandra},
title = {Heterogeneity in Algorithm-Assisted Decision-Making: A Case Study in Child Abuse Hotline Screening},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555101},
doi = {10.1145/3555101},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {376},
numpages = {33},
keywords = {organizational behavior, human-computer interaction, child welfare}
}

@inproceedings{de2020case,
author = {De-Arteaga, Maria and Fogliato, Riccardo and Chouldechova, Alexandra},
title = {A Case for Humans-in-the-Loop: Decisions in the Presence of Erroneous Algorithmic Scores},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376638},
doi = {10.1145/3313831.3376638},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {decision support, algorithm aversion, human-in-the-loop, child welfare, algorithm assisted decision making, automation bias},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{passi2019problem,
  title={Problem formulation and fairness},
  author={Passi, Samir and Barocas, Solon},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={39--48},
  year={2019}
}

@inproceedings{jussupow2020we,
author = {Jussupow, Ekaterina and Benbasat, Izak and Heinzl, Armin},
year = {2020},
month = {06},
title = {Why are we averse towards algorithms? A comprehensive literature review on algorithm aversion}
}

@article{10.1093/jcmc/zmac010,
    author = {Molina, Maria D and Sundar, S Shyam},
    title = "{When AI moderates online content: effects of human collaboration and interactive transparency on user trust}",
    journal = {Journal of Computer-Mediated Communication},
    volume = {27},
    number = {4},
    year = {2022},
    month = {07},
    abstract = "{Given the scale of user-generated content online, the use of artificial intelligence (AI) to flag problematic posts is inevitable, but users do not trust such automated moderation of content. We explore if (a) involving human moderators in the curation process and (b) affording “interactive transparency,” wherein users participate in curation, can promote appropriate reliance on AI. We test this through a 3 (Source: AI, Human, Both) × 3 (Transparency: No Transparency, Transparency-Only, Interactive Transparency) × 2 (Classification Decision: Flagged, Not Flagged) between-subjects online experiment (N = 676) involving classification of hate speech and suicidal ideation. We discovered that users trust AI for the moderation of content just as much as humans, but it depends on the heuristic that is triggered when they are told AI is the source of moderation. We also found that allowing users to provide feedback to the algorithm enhances trust by increasing user agency.As more users post in online forums, there has been a rise in harmful content such as hate speech and thoughts about committing suicide. Most online sites use humans to monitor such content. But, there is so much of this content each day that platforms have started using artificial intelligence (AI) to automatically flag and stop its spread. AI can be better than human moderators. It uses the same consistent criteria for classification, and it is faster. But, the problem is that people do not trust AI with such an important responsibility. One way to increase their trust is to involve humans in the moderation task. Another is to allow users to provide feedback on the classification. We conducted an experiment to test these ideas. Participants were told that the content was classified either by an AI, or by humans, or by both working together. Also, some participants were provided with a list of rules used for classification. Others were allowed to provide feedback about the rules. A third group did not receive any rules. We discovered that letting users provide feedback increased trust. Trust in AI also depends on the perceptions that users have about AI for moderating content.}",
    issn = {1083-6101},
    doi = {10.1093/jcmc/zmac010},
    url = {https://doi.org/10.1093/jcmc/zmac010},
    note = {zmac010},
    eprint = {https://academic.oup.com/jcmc/article-pdf/27/4/zmac010/45048191/zmac010.pdf},
}

@book{sundar2008main,
  title={The MAIN model: A heuristic approach to understanding technology effects on credibility},
  author={Sundar, S Shyam},
  year={2008},
  publisher={MacArthur Foundation Digital Media and Learning Initiative Cambridge, MA}
}

@inproceedings{heuer2022comparative,
author = {Heuer, Hendrik and Glassman, Elena Leah},
title = {A Comparative Evaluation of Interventions Against Misinformation: Augmenting the WHO Checklist},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517717},
doi = {10.1145/3491102.3517717},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {241},
numpages = {21},
keywords = {Disinformation, Fact-check, Propaganda, COVID-19, Social Media, Fake News, Misinformation, World Health Organization},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{herrmanny2021towards,
  title={Towards a User Integration Framework for Personal Health Decision Support and Recommender Systems},
  author={Herrmanny, Katja and Torkamaan, Helma},
  booktitle={Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization},
  pages={65--76},
  year={2021}
}

@inproceedings{loepp2015blended,
author = {Loepp, Benedikt and Herrmanny, Katja and Ziegler, J\"{u}rgen},
title = {Blended Recommending: Integrating Interactive Information Filtering and Algorithmic Recommender Techniques},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702496},
doi = {10.1145/2702123.2702496},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {975–984},
numpages = {10},
keywords = {information filtering, interactive recommending, recommender systems, user interfaces},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@misc{https://doi.org/10.48550/arxiv.2103.04044,
  doi = {10.48550/ARXIV.2103.04044},
  
  url = {https://arxiv.org/abs/2103.04044},
  
  author = {Wang, Zijie J. and Choi, Dongjin and Xu, Shenyu and Yang, Diyi},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Human-Computer Interaction (cs.HC), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Putting Humans in the Natural Language Processing Loop: A Survey},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{godbole2004discriminative,
author="Godbole, Shantanu
and Sarawagi, Sunita",
editor="Dai, Honghua
and Srikant, Ramakrishnan
and Zhang, Chengqi",
title="Discriminative Methods for Multi-labeled Classification",
booktitle="Advances in Knowledge Discovery and Data Mining",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="22--30",
isbn="978-3-540-24775-3"
}

@article{doi:10.1080/0735648X.2019.1655781,
author = {Leanna Ireland},
title = {Who errs? Algorithm aversion, the source of judicial error, and public support for self-help behaviors},
journal = {Journal of Crime and Justice},
volume = {43},
number = {2},
pages = {174-192},
year  = {2020},
publisher = {Routledge},
doi = {10.1080/0735648X.2019.1655781},
URL = {https://doi.org/10.1080/0735648X.2019.1655781},
eprint = {https://doi.org/10.1080/0735648X.2019.1655781}
}

@article{LOGG201990,
title = {Algorithm appreciation: People prefer algorithmic to human judgment},
journal = {Organizational Behavior and Human Decision Processes},
volume = {151},
pages = {90-103},
year = {2019},
issn = {0749-5978},
doi = {https://doi.org/10.1016/j.obhdp.2018.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0749597818303388},
author = {Jennifer M. Logg and Julia A. Minson and Don A. Moore},
keywords = {Algorithms, Accuracy, Advice-taking, Forecasting, Decision-making, Theory of machine},
abstract = {Even though computational algorithms often outperform human judgment, received wisdom suggests that people may be skeptical of relying on them (Dawes, 1979). Counter to this notion, results from six experiments show that lay people adhere more to advice when they think it comes from an algorithm than from a person. People showed this effect, what we call algorithm appreciation, when making numeric estimates about a visual stimulus (Experiment 1A) and forecasts about the popularity of songs and romantic attraction (Experiments 1B and 1C). Yet, researchers predicted the opposite result (Experiment 1D). Algorithm appreciation persisted when advice appeared jointly or separately (Experiment 2). However, algorithm appreciation waned when: people chose between an algorithm’s estimate and their own (versus an external advisor’s; Experiment 3) and they had expertise in forecasting (Experiment 4). Paradoxically, experienced professionals, who make forecasts on a regular basis, relied less on algorithmic advice than lay people did, which hurt their accuracy. These results shed light on the important question of when people rely on algorithmic advice over advice from people and have implications for the use of “big data” and algorithmic advice it generates.}
}

@book{dennett1987intentional,
  title={The intentional stance},
  author={Dennett, Daniel Clement},
  year={1987},
  publisher={MIT press}
}

@article{10.1145/3479864,
author = {Hou, Yoyo Tsung-Yu and Jung, Malte F.},
title = {Who is the Expert? Reconciling Algorithm Aversion and Algorithm Appreciation in AI-Supported Decision Making},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479864},
doi = {10.1145/3479864},
abstract = {The increased use of algorithms to support decision making raises questions about whether people prefer algorithmic or human input when making decisions. Two streams of research on algorithm aversion and algorithm appreciation have yielded contradicting results. Our work attempts to reconcile these contradictory findings by focusing on the framings of humans and algorithms as a mechanism. In three decision making experiments, we created an algorithm appreciation result (Experiment 1) as well as an algorithm aversion result (Experiment 2) by manipulating only the description of the human agent and the algorithmic agent, and we demonstrated how different choices of framings can lead to inconsistent outcomes in previous studies (Experiment 3). We also showed that these results were mediated by the agent's perceived competence, i.e., expert power. The results provide insights into the divergence of the algorithm aversion and algorithm appreciation literature. We hope to shift the attention from these two contradicting phenomena to how we can better design the framing of algorithms. We also call the attention of the community to the theory of power sources, as it is a systemic framework that can open up new possibilities for designing algorithmic decision support systems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {477},
numpages = {25},
keywords = {augmented decision making, decision aids, human-algorithm interaction, decision support systems, algorithm appreciation, algorithm aversion, expert power}
}

@misc{education_2020, title={GCSE and A level students to receive centre assessment grades}, url={https://www.gov.uk/government/news/gcse-and-a-level-students-to-receive-centre-assessment-grades}, publisher={GOV.UK}, year={2020}, month={Aug}, author={
Department for Education}} 

@article{10.1145/3359318,
author = {Harrington, Christina and Erete, Sheena and Piper, Anne Marie},
title = {Deconstructing Community-Based Collaborative Design: Towards More Equitable Participatory Design Engagements},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359318},
doi = {10.1145/3359318},
abstract = {Participatory Design (PD) is envisioned as an approach to democratizing innovation in the design process by shifting the power dynamics between researcher and participant. Recent scholarship in HCI and design has analyzed the ways collaborative design engagements, such as PD situated in the design workshop can amplify voices and empower underserved populations. Yet, we argue that PD as instantiated in the design workshop is very much an affluent and privileged activity that often neglects the challenges associated with envisioning equitable design solutions among underserved populations. Based on two series of community-based PD workshops with underserved populations in the U.S., we highlight key areas of tension and considerations for a more equitable PD approach: historical context of the research environment, community access, perceptions of materials and activities, and unintentional harm in collecting full accounts of personal narratives. By reflecting on these tensions as a call-to-action, we hope to deconstruct the privilege of the PD workshop within HCI and re-center the focus of design on individuals who are historically underserved.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {216},
numpages = {25},
keywords = {social action research, community-based participatory design, design workshops, design equity}
}

@article{qarout2019platform, title={Platform-Related Factors in Repeatability and Reproducibility of Crowdsourcing Tasks}, volume={7}, url={https://ojs.aaai.org/index.php/HCOMP/article/view/5264}, DOI={10.1609/hcomp.v7i1.5264}, number={1}, journal={Proceedings of the AAAI Conference on Human Computation and Crowdsourcing}, author={Qarout, Rehab and Checco, Alessandro and Demartini, Gianluca and Bontcheva, Kalina}, year={2019}, month={Oct.}, pages={135-143} }

@article{baker2015over,
  title={Over half of psychology studies fail reproducibility test},
  author={Baker, Monya},
  journal={Nature News},
  volume={27},
  year={2015}
}

@article{fanelli2018science,
  title={Is science really facing a reproducibility crisis, and do we need it to?},
  author={Fanelli, Daniele},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={11},
  pages={2628--2631},
  year={2018},
  publisher={National Acad Sciences}
}

@article{ioannidis2005most,
  title={Why most published research findings are false},
  author={Ioannidis, John PA},
  journal={PLoS medicine},
  volume={2},
  number={8},
  pages={e124},
  year={2005},
  publisher={Public Library of Science}
}


@article{moonesinghe2007most,
  title={Most published research findings are false—but a little replication goes a long way},
  author={Moonesinghe, Ramal and Khoury, Muin J and Janssens, A Cecile J W},
  journal={PLoS medicine},
  volume={4},
  number={2},
  pages={e28},
  year={2007},
  publisher={Public Library of Science San Francisco, USA}
}

@article{10.1145/3479531,
author = {Ram\'{\i}rez, Jorge and Sayin, Burcu and Baez, Marcos and Casati, Fabio and Cernuzzi, Luca and Benatallah, Boualem and Demartini, Gianluca},
title = {On the State of Reporting in Crowdsourcing Experiments and a Checklist to Aid Current Practices},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479531},
doi = {10.1145/3479531},
abstract = {Crowdsourcing is being increasingly adopted as a platform to run studies with human subjects. Running a crowdsourcing experiment involves several choices and strategies to successfully port an experimental design into an otherwise uncontrolled research environment, e.g., sampling crowd workers, mapping experimental conditions to micro-tasks, or ensure quality contributions. While several guidelines inform researchers in these choices, guidance of how and what to report from crowdsourcing experiments has been largely overlooked. If under-reported, implementation choices constitute variability sources that can affect the experiment's reproducibility and prevent a fair assessment of research outcomes. In this paper, we examine the current state of reporting of crowdsourcing experiments and offer guidance to address associated reporting issues. We start by identifying sensible implementation choices, relying on existing literature and interviews with experts, to then extensively analyze the reporting of 171 crowdsourcing experiments. Informed by this process, we propose a checklist for reporting crowdsourcing experiments.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {387},
numpages = {34},
keywords = {reproducibility, crowdsourcing, crowdsourcing experiments, reporting}
}

@inproceedings{xia2022tragedy,
  title={Tragedy of the Commons-A Critical Study of Data Quality and Validity Issues in Crowd Work-Based Research},
  author={Xia, Huichuan},
  booktitle={Proceedings of the 55th Hawaii International Conference on System Sciences},
  year={2022}
}

@inproceedings{wang2018learning,
author = {Wang, Jiaxuan and Oh, Jeeheh and Wang, Haozhu and Wiens, Jenna},
title = {Learning Credible Models},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3220070},
doi = {10.1145/3219819.3220070},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {2417–2426},
numpages = {10},
keywords = {model interpretability, regularization},
location = {London, United Kingdom},
series = {KDD '18}
}
