\documentclass[aps,pra,twocolumn,showpacs,floatfix,superscriptaddress,nofootinbib]{revtex4-1}


\usepackage{graphicx}%
\usepackage{dcolumn}%
\usepackage{bm}%
%\usepackage{physics}
\usepackage[colorlinks=true]{hyperref}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{braket}
\usepackage[normalem]{ulem}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{blkarray}
\usepackage{orcidlink}


% please change to your favorite color
%
\newcommand{\ts}[1]{{\color{cyan}{{#1}}}}
\newcommand{\mc}[1]{{\color{red}{{#1}}}}
\newcommand{\nw}[1]{{\color{red}{{#1}}}}
\newcommand{\phb}[1]{{\color{red}{{#1}}}}
\newcommand{\ml}[1]{{\color{red}{{#1}}}}
\newcommand{\jz}[1]{{\color{orange}{{#1}}}}
\newcommand{\as}[1]{{\color{red}{{#1}}}}

\newcommand{\old}[1]{\textcolor{blue}{\sout{#1}}}

\newcommand{\changes}[1]{#1}

%SF Math defs
\newcommand{\figref}[1]{\figurename~\ref{#1}}
\newcommand{\secref}[1]{Sec.~\ref{#1}}
\renewcommand{\eqref}[1]{Eq.~(\ref{#1})}
\newcommand{\Schr}{Schr\"odinger }
\newcommand{\ip}{I_\mathrm{p}}
\newcommand{\up}{U_\mathrm{p}}
\newcommand{\pb}{\mathbf{p}}
\newcommand{\xb}{\mathbf{x}}
\newcommand{\kb}{\mathbf{k}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\rb}{\mathbf{r}}
\newcommand{\rbh}{\hat{\mathbf{r}}}
\newcommand{\Ab}{\mathbf{A}}
\newcommand{\Eb}{\mathbf{E}}
\newcommand{\grad}{\mathbf{\nabla}}
\renewcommand\Re{\mathrm{Re}}
\renewcommand\Im{\mathrm{Im}}
\newcommand{\erf}{\;\mathrm{erf}}
\newcommand{\xhat}{\hat{\bf x}}
\newcommand{\yhat}{\hat{\bf y}}
\newcommand*\conj[1]{\overline{#1}}

\newcommand{\Qprop}{Q{\sc{prop}}}
%\usepackage{lineno}
%\linenumbers

\begin{document}

\title{Femtosecond pulse parameter estimation from photoelectron momenta using machine learning}

\author{Tomasz Szołdra \orcidlink{0000-0002-2897-0506}}
\affiliation{Doctoral School of Exact and Natural Sciences, Jagiellonian University, \L{}ojasiewicza 11, PL-30-348 Krak\'ow, Poland}
\affiliation{Instytut Fizyki Teoretycznej, Wydzia\l{} Fizyki, Astronomii i Informatyki Stosowanej, Uniwersytet Jagiello\'nski, \L{}ojasiewicza 11, PL-30-348 Krak\'ow, Poland}

\author{Marcelo F. Ciappina \orcidlink{0000-0002-1123-6460}}
\affiliation{Department of Physics, Guangdong Technion - Israel Institute of Technology, 241 Daxue Road, Shantou, Guangdong, China, 515063}
\affiliation{Technion - Israel Institute of Technology, Haifa, 32000, Israel}
\affiliation{Guangdong Provincial Key Laboratory of Materials and Technologies for Energy Conversion, Guangdong Technion - Israel Institute of Technology, 241 Daxue Road, Shantou, Guangdong, China, 515063}
\author{Nicholas Werby \orcidlink{0000-0002-1789-3170}}
\affiliation{Stanford PULSE Institute, SLAC National Accelerator Laboratory 2575 Sand Hill Road, Menlo Park, CA 94025, USA}
\affiliation{Department of Physics, Stanford University, Stanford, CA 94305, USA}
\author{Philip H. Bucksbaum~\orcidlink{0000-0003-1258-5571}}
\affiliation{Stanford PULSE Institute, SLAC National Accelerator Laboratory 2575 Sand Hill Road, Menlo Park, CA 94025, USA}
\affiliation{Department of Physics, Stanford University, Stanford, CA 94305, USA}
\affiliation{Department of Applied Physics, Stanford University, Stanford, CA 94305, USA}
\author{Maciej Lewenstein \orcidlink{0000-0002-0210-7800}}
\affiliation{ICFO-Institut de Ciencies Fotoniques, The Barcelona Institute of Science and Technology, Av. Carl Friedrich Gauss 3, 08860 Castelldefels (Barcelona), Spain}
\affiliation{ICREA, Pg. Lluís Companys 23, 08010 Barcelona, Spain}
\author{Jakub Zakrzewski \orcidlink{0000-0003-0998-9460}}
\affiliation{Instytut Fizyki Teoretycznej, Wydzia\l{} Fizyki, Astronomii i Informatyki Stosowanej, Uniwersytet Jagiello\'nski, \L{}ojasiewicza 11, PL-30-348 Krak\'ow, Poland}
\affiliation{Mark Kac Complex Systems Research Center, Jagiellonian University, \L{}ojasiewicza 11, PL-30-348 Krak\'ow, Poland}
\author{Andrew S. Maxwell \orcidlink{0000-0002-6503-4661}}
\email{andrew.maxwell@phys.au.dk}
\affiliation{Department of Physics and Astronomy, Aarhus University, DK-8000 Aarhus C, Denmark}
\date{\today}


\begin{abstract}
Deep learning models have provided huge interpretation power for image-like data. Specifically, convolutional neural networks (CNNs) have demonstrated incredible acuity for tasks such as feature extraction or parameter estimation. Here we test CNNs on strong-field ionization photoelectron spectra, training on theoretical data sets to `invert' experimental data. Pulse characterization is used as a `testing ground', specifically we retrieve the laser intensity, where `traditional' measurements typically lead to 20\% uncertainty.  We report on crucial data augmentation techniques required to successfully train on theoretical data and return consistent results from experiments, including accounting for detector saturation. The same procedure can be repeated to apply CNNs in a range of scenarios for strong-field ionization. Using a predictive uncertainty estimation, reliable laser intensity uncertainties of a few percent can be extracted, which are consistently lower than those given by traditional techniques. Using interpretability methods can reveal parts of the distribution that are most sensitive to laser intensity, which can be directly associated with holographic interferences. The CNNs employed provide an accurate and convenient ways to extract parameters, and represent a novel interpretational tool for strong-field ionization spectra.
\end{abstract}

\maketitle

\section{Introduction}
Machine learning (ML) has been transformative for science over the last two decades, providing a huge range of new analytical tools. This has affected nearly every avenue of research, with major use across the physical sciences, particularly in particle physics \cite{karagiorgi_machine_2022}, astrophysics \cite{vanderplas_introduction_2012}, and condensed matter physics \cite{carleo_machine_2019, dawid_modern_2022}. Convolutional neural networks (CNNs) have enabled major leaps in computer vision and language processing \cite{rawat_deep_2017, li_survey_2022}. This makes CNNs well-suited for pattern recognition and parameter estimation in scientific data.
For example, CNNs have been used for determining crystal symmetries in electron diffraction \cite{kaufmann_crystal_2020}, and estimating parameters related to gravitational lensing \cite{hezaveh_fast_2017}. However, in the field of strong field physics and attoscience the high interpretability power of CNNs has not been fully explored.

Strong-field physics and attoscience exploit recent advances for producing intense and short laser pulses to image and control matter over attosecond ($10^{-18}$s) timescales \cite{krausz_attosecond_2009, salieres_study_1999, lewenstein_principles_2008, ciappina_attosecond_2017}. These capabilities have led to a wide range of atomic and molecular imaging procedures, e.g., high-order harmonic spectroscopy \cite{itatani_tomographic_2004}, laser-induced electron diffraction \cite{zuo_laserinduced_1996}, photoelectron holography \cite{huismans_timeresolved_2011,figueirademorissonfaria_it_2020}, attosecond streaking \cite{hentschel_attosecond_2001,itatani_attosecond_2002}, and reconstruction of attosecond harmonic beating by interference of two-photon transitions \cite{paul_observation_2001,muller_reconstruction_2002}. However, despite ever more accuracy from experiment and theory, due to the nonlinear nature of the interactions, interpretation of the data is often very challenging. This provides an opportunity for ML methods to be used to extract parameters and physical trends from experimental data sets.

A growing number of studies have begun to use ML techniques for strong-field physics. For example, studies using neural networks to classify semi-classical trajectories \cite{liu_deep_2020}, deep learning to predict in spectra of high-harmonic generation \cite{lytova_deep_2022}, and optimization of ``quantum pathways'' in enhanced ionization of diatomic molecules \cite{chomet_controlling_2022}. In terms of parameter estimation, in a recent theoretical study, CNNs were used to extract internuclear distances and laser intensities, using data generated solving the time-dependent \Schr equation (TDSE) \cite{shvetsov-shilovski_deep_2022, shvetsov-shilovski_transfer_2023}. The power of CNNs to extract useful information from  experimental images has big implications for strong-field physics and attoscience.
Most studies, however, focus on a proof of principle, using only theoretical data. Notable exceptions are, Ref.~\cite{liu_machine_2021}, where CNNs were used to extract molecular structure parameters from experimental laser-induced electron diffraction images, and Ref.~\cite{brunner_deep_2022}, where deep neural networks were applied to streaking traces for parameter extraction and prediction of uncertainties.
Unfortunately, the analytical power of machine learning-assisted imaging is limited if the laser pulse parameters can not be accurately measured.

The characterization of laser pulses in strong-field and attosecond physics has posed a persistent problem. The high intensity of the laser pulse means that a direct measurement (see e.g. \cite{trebino_measuring_1997,kielpinski_benchmarking_2014}) of the intensity leads to significant uncertainties, in the range of $10$--$20\%$ for the strong-field regime \cite{pullen_measurement_2013,kielpinski_benchmarking_2014}. An alternative approach is to use the high sensitivity of the non-linear phenomena in question, to estimate the laser pulse parameters, known as an in situ measurement. Using this approach, laser intensity uncertainties as low as $1\%$ have been reached, by matching experimental results to TDSE theory, under highly controlled experimental conditions for atomic hydrogen \cite{pullen_measurement_2013,kielpinski_benchmarking_2014}. Despite this success, such low uncertainties are not common, and would be more difficult to achieve routinely in standard experimental conditions. Simply fitting photoelectron spectra is less effective and more advanced methods, using all the available information in photoelectron momentum distributions (PMDs), are called for. Recent results, using quantum metrology tools, suggest the uncertainty from in situ measurements could be significantly reduced by exploring quantum interferences present in the PMDs in strong-field ionization \cite{maxwell_quantum_2021}. There is a variety of in situ methods for determining laser parameters that are implemented by hand, whose performances vary across parameter regimes.
The most consistent and powerful method is to use the whole PMD, matching it to that obtained with accurate theoretical methods. As such, ML schemes, and in particular CNNs, are an ideal tool for in situ extraction of laser parameters from experimental data.


The task of using ML for laser pulse characterization has been addressed in the relativistic regime where, a theoretical study used CNNs to predict laser intensities by using proton dynamics \cite{bukharskii_restoration_2023}. In FELs, neural networks have been used to accurately reconstruct pulses, by training a model on a small set of fully diagnosed pulses \cite{sanchez-gonzalez_accurate_2017}. CNNs were also used to characterize the FEL pulses by training on simulated data \cite{ren_temporal_2020}.
CNNs have also been used in all optical measurement schemes, employing interferometric cross-correlation between pulses to characterize one of the pulses \cite{kolesnichenko_neuralnetworkpowered_2023}.
Recently, ML tools have been used for pulse characterization for strong-field ionization \cite{geffert_situ_2022} using purely theoretical data. Here, the autocorrelation function of the ionization yield from two identical pulse was used to extract the pulse duration, spectral width and relative CEP, but the method was insensitive to laser intensity.

In this work, we investigate the power of a CNN as an analysis tool for strong-field physics. We use laser pulse characterization in strong-field ionization as a testing ground, retrieving parameters from PMDs. We train the CNNs on a TDSE model, and test this on large experimental datasets, focusing on retrieving laser intensities, over a larger parameter regime than has previously been considered, for an argon target. The CNNs trained may be used on any experimental data within the parameter range, without special requirements, and the CNN models are available online for testing. Important modifications to theoretical training data, are presented, that ensure the CNN models are insensitive to common experimental imperfections. We also include predictive uncertainty estimation, which goes beyond previous methods to extract uncertainty in strong-field studies.
We produce so-called `explainability' figures that are able to highlight regions of the PMD that contribute the most to a laser intensity prediction, and connect these to the physical interference process that are most sensitive to changes in laser intensity. As such, CNNs represent the easiest way to extract laser parameters in strong-field ionization, making a key step towards producing a general tool for parameters estimation, while also providing more interpretational power. Atomic units are used unless otherwise stated.


\section{Datasets}
\label{sec:datasets}
    \begin{figure*}
        \centering
        \includegraphics[width=\linewidth]{data_samples3_inferno_histogram.pdf}
        \caption{Example PMDs (upper halves in the first row and lower halves in the second row) in the initially preprocessed a) \Qprop, b) SFA ($N=40$ cycles and $U_p=0.3575$) and f) experimental E (single) ($U_p=0.35$) datasets. Color scale corresponds to rescaled and shifted log-probability density and includes the leading 6 orders of magnitude of the original calculated/measured PMD. Panel c) shows the focal-averaged \Qprop\ PMD - minor features are smeared out, panel d) the focal- and CEP-averaged \Qprop\ PMD that does not show qualitative differences from c). Panel e) shows data from d) as seen during training of the model, i.e. at a random detector saturation level $SL=0.4$ (see text for details), contrast $0.8$, brightness $-0.2$. Histograms of pixel brightnesses in panel g) are calculated over a range of pulse ponderomotive potentials $0.15 < U_p < 0.5$. Clearly, direct comparison between theoretical and experimental PMDs is a hard task, as the distributions differ substantially due to experimental limitations.
        }
        \label{fig:ExampleDataSets}
    \end{figure*}
    \subsection{QProp}
        The main workhorse for generating our theoretical data set is the single-active-electron (SAE) TDSE solver \Qprop.
        The latest version of \Qprop~\cite{tulsky_qprop_2019}, implements a fast and accurate method for the calculation of photoelectron momentum distributions (PMDs). \Qprop\ is a velocity gauge 3-dimensional (3D) TDSE solver in the dipole approximation that allows studies within the SAE  approximation using model pseudopotentials\footnote{There is also an implementation of many-electron systems via the solution of the time-dependent Kohn–Sham equations.}. For our SAE model of the Ar atom, we have employed the model potential of Ref.~\cite{tong_empirical_2005}, which has the form
	
	\begin{align}
	    V(r)&=-\frac{Z+f(r)}{r}
	    \intertext{with}
	    f(r)&=a_1 e^{-a_2 r}+a_3 r e^{-a_4 r} + a_5 e^{-a_6 r},
	\end{align}
	where $Z=1$. For argon the coefficients $a_i$ are $a_1=16.039$, $a_2=2.007$, $a_3=-25.543$, $a_4=4.525$, $a_5=0.961$ and $a_6=0.443$~\cite{tong_empirical_2005}, which gives the correct ionization potential of $\ip=0.579$~a.u. In this computation, we considered angular momenta up to $l=55$. Total data set consists of $15712$ PMDs with ponderomotive potentials ranging from $\up=0.0075$ to $\up=0.95$, laser cycles number $N=2$ up to $N=61$. The number of CEP values depends on the pulse length, i.e. for under $N=13$ cycles we cover an interval of length $\pi$ with $10$ values of CEP, and for longer cycles, we have $5$ values of CEP spanning a shifted interval of the same length.
 
    \subsection{Strong Field Approximation}
     An extensive review of the SFA can be found in Ref~\cite{amini_symphony_2019}. Here, we use the transition amplitude for direct ATI from an initial bound state $|\psi_0\rangle$ to a final Volkov state with drift momentum $\pb$ given by \cite{becker_abovethreshold_2002,figueirademorissonfaria_highorder_2002,keldysh_ionization_1965,faisal_multiple_1973,reiss_effect_1980,maxwell_quantum_2021}
    \begin{equation}
    M(\pb)=-i\lim\limits_{t\to\infty}  e^{i S(\pb,t)} \int_{-\infty}^{t}dt' d(\pb,t') e^{i S(\pb,t')},
    \label{eq:SFA-transition-general}
    \end{equation}
    where $d(\pb,t')=\braket{ \pb+\Ab(t')|\rb\cdot\Eb(t')|\Psi_{0}}$ is the dipole prefactor, which in this study we will neglect as we retain only terms correct to exponential accuracy.  The action is given by
    \begin{equation}
        S(\mathbf{p},t)=\ip t +\frac{1}{2}\int^t d t' (\pb+\Ab(t'))^2.
        \label{eq:SFA-action-general}
    \end{equation}
    Here, $\ip$ is the ionization potential of our target.
    We employ the saddle point approximation, seeking the stationary action for the integration variable $t'$, $2\ip+\left(\pb+\Ab(t_s)\right)^2=0$. Now the probability distribution can be computed from \eqref{eq:SFA-transition-general} as
    \begin{align}
        M(\pb)=&
        \sum_s
        c(\pb,t_s,t)	d(\pb,t_s) e^{i S(\pb,t_s)},
        \intertext{where the prefactor $c(\pb,t_s, t)$, derived from the application of the saddle point approximation, includes the $t'$ independent phase from \eqref{eq:SFA-transition-general},  is given by}
        c(\pb,t_s,t)&=-i e^{i S(\pb,t)}\sqrt{\frac{2\pi i}{\partial^2 S(\pb,t_s)/\partial t_s^2}}.
    \end{align}
    In both \Qprop\ and SFA calculations we use the vector potential:
    \begin{equation}
        \Ab(t)=2\sqrt{\up}\sin^2\left( \frac{\omega t}{2 N}\right)\cos(\omega t + \phi),
    \end{equation}
    where $N$ is the number of laser cycles, while $\up$ is the ponderomotive energy or quiver energy of a free electron in the laser field, which is proportional to the peak laser intensity $I_0=2\up c \epsilon_0 \omega^2$, where $\epsilon_0$ and $c$ are the vacuum permittivity and the speed of light, respectively. The angular frequency is given by $\omega$ and the carrier-envelope phase (CEP) is given by $\phi$. We also perform focal (FA) and CEP averaging (CA), to account for variations of the intensity across the focal volume and CEP fluctuations between laser pulses, respectively. CEP-averaged QProp datasets contain $3071$ PMDs. Details are given in the Supplemental material.
    \subsection{Experimental methods}
    \label{sec:experimental_method}

    Our experimental data sets consist of PMDs of argon gas photoionized by intense, linearly polarized, 800~nm laser pulses generated by a 1~kHz commercial Ti:sapphire laser system. These laser pulses are focused in ultra-high vacuum and intersect a pulsed beam of argon gas delivered by an Even-Lavie valve \cite{even_even-lavie_2015}. Photoelectrons are collected in a velocity map imaging spectrometer (VMI), and impact a microchannel plate (MCP) and a fast phosphor detector system. A camera records the intensified phosphor, and on-the-fly peak finding is applied to the live camera feed to extract individual electron impacts.

    Six experimental data sets with $125$ PMDs in total are analyzed here. Four of these sets, labeled E1-E4, were collected using an experimental schema in which the ionizing pulse energy was the variable parameter. The pulse energy is controlled using a motorized rotation mount which manipulates a half-wave plate to rotate the pulse polarization. It then passes through a polarizing beamsplitter cube, which transmits only the component of the laser pulse polarized parallel to the optical table.
    Another data set, “E HS”, was collected for intensities over a larger range and include the highest values of intensity.
    The final set, labeled E (single), is a single-intensity PMD, which is included as it has the highest signal-to-noise ratio.
    Each dataset was collected on the timescale of approximately two days, and contains in total $O(10^9)$ electron counts, distributed between their intensity slices.

    The data sets presented have all been Abel inverted using the standard technique of polar onion peeling \cite{Roberts_2009, werby_dissecting_2021} to extract their cylindrical momentum cross sections. The ponderomotive potential, $U_p$, of the laser fields generating each slice of each data set is computed in a two-step process. First, it is roughly calculated by examining the direct electrons, which form a disk of radius $2U_p$. Then, that rough value is refined by comparing nodes found along the  ``spider-leg" holographic feature to those predicted by the Coulomb quantum orbit strong-field approximation, see Refs. \cite{maxwell_coulombcorrected_2017,maxwell_analytic_2018,maxwell_coulombfree_2018,figueirademorissonfaria_it_2020}, at nearby intensities. This procedure is described in more detail in Ref. \cite{werby_dissecting_2021}. This has an error of approximately $\pm 10\%$.


\section{Deep neural network approach}
	Our task is the following: given an experimental PMD $X$, find a physical parameter $y$, for example, the intensity of the laser pulse, that has been used to produce such PMD. We assume an underlying theoretical model of the strong-field process that allows us to generate PMDs expected at given physical parameters. The richness of the features present in the PMDs makes the task very challenging. Moreover, various imperfections are present in the experimental data, complicating the comparison with theory further, cf. histograms in Fig.~\ref{fig:ExampleDataSets}(g) showing a dramatic quantitative difference between the PMD values in both datasets.

    To address this demanding work, we then use a deep learning approach. By generating a dataset of PMDs labeled by many different physical parameters, we reformulate the problem as a standard supervised regression task. In Section~\ref{sec:cnn}, we describe our choice of deep neural network architecture.


    \subsection{Convolutional Neural Networks and Transfer Learning}
    \label{sec:cnn}
    CNNs \cite{lecun_backpropagation_1989} are designed to work with data incorporating spatial correlations such as pictures. The main building block of CNN is a convolution matrix with trainable parameters that slides over the input image and produces its filtered representation. The composition of many such filters gives a feature map of the image, ready to be used for further processing in the final fully connected part of the network.
    
    Although deep CNNs achieve state-of-the-art in image recognition \cite{krizhevsky_imagenet_2012, russakovsky_imagenet_2014, chen_symbolic_2023}, \changes{sometimes} this requires the use of prohibitively large training datasets and computing power. However, one can train deep models through the transfer learning paradigm \cite{tan_survey_2018, plested_deep_2022}: one takes a pre-trained deep model and fine-tunes it on a smaller dataset. The first few layers of the network extract general features such as edges, so often retraining only the last few layers of the model may be sufficient to achieve good performance on the new dataset. In this work we benchmark four pre-trained architectures called VGG16 \cite{simonyan_very_2015}, Xception \cite{chollet_xception_2017}, EfficientNetB7 \cite{tan_efficientnet_2020}, and EfficientNetV2L \cite{tan_efficientnetv2_2021} that achieved state-of-the-art accuracy in classification tasks on the Imagenet dataset \cite{russakovsky_imagenet_2014}. Models are ordered from least to most sophisticated. We do not describe the substantial innovations introduced in each of them but concentrate on the comparison of their general performance. Because the models were originally designed for classification and not regression, we remove the last classification layer, and replace it with a fully connected layer with a linear (identity) activation function, see Fig.~\ref{fig:schematic}. Models are implemented using Keras library \cite{chollet_keras_2015} and are available in our code repository \url{github.com/tszoldra/attoDNN}.
    
    \begin{figure}
             \centering
             \includegraphics[width=\linewidth]{scheme_v2.pdf}
             \caption{Schematic representation of the Deep Convolutional Neural Network regression problem. For given input $X$ network predicts the value of the parameter $\mu(X)$ and its uncertainty $\sigma(X)$. Adapted from \cite{iqbal_plotneuralnet_2018}.}
             \label{fig:schematic}
         \end{figure}

    \subsection{Predictive uncertainty estimation}
    \label{sec:Uncertainty}
    Along with the predicted label, we aim to provide an estimate of the model uncertainty for a given input \cite{gawlikowski_survey_2022, abdar_review_2021}. We slightly modify the model and the loss function \cite{nix_estimating_1994, lakshminarayanan_simple_2017}: instead of predicting a single value of the label $y_{pred}(X)$, we assume the label comes from a normal distribution $p(y_{true}|X) = \mathcal{N}(\mu(X), \sigma(X))$ and the model predicts its parameters $\mu(X), \sigma^2(X)$ for a given input $X$. $\sigma^2(X)$ is the output of an additional fully connected layer with a Softplus$(x) = \ln(\exp(x) + 1)$ activation function, see Fig.~\ref{fig:schematic}. The loss function to minimize is the negative log-likelihood,
    \begin{equation}
    	\text{NLL}\!=\! - \!\left\langle \ln p(y_{true}|X)\right\rangle\!=\!\frac{1}{2}\!\left\langle\! \ln\sigma^2(X) \!+\!\frac{(y_{true}\!-\!\mu(X))^2}{\sigma^2(X)}\!\right\rangle,
     \label{eq:NLL}
    \end{equation}
    where $\langle . \rangle$ denotes the mean over the training dataset. To further \changes{improve} the reliability of \changes{the} predictions and predictive uncertainties, we combine $M$ models trained on random subsets of the training dataset into a deep ensemble \cite{lakshminarayanan_simple_2017, ovadia_can_2019}. Further assuming that the ensemble prediction is a Gaussian, the ensemble mean and variance read
    \begin{equation}
        \mu_*(X) = \frac{1}{M} \sum_{m=1}^M \mu_{m}(X),
    \end{equation}
    \begin{equation}
        \sigma^2_{*}(X) = \frac{1}{M} \sum_{m=1}^M \left( \sigma^2_{m}(X) + \mu^2_{m}(X) \right) -  \mu^2_{*}(X),
        \label{eq:sigma}
    \end{equation}
    where $\mu_m(X),~\sigma_m(X)$ are the mean and variance predicted by the $m$-th model in the ensemble \cite{lakshminarayanan_simple_2017}. This goes beyond studies such as \cite{brunner_deep_2022}, where an ensemble of predictions is used to produce the uncertainty.


        \begin{table*}
\centering
\footnotesize
\begin{tabular}{l|rrrr|rrrr|rrrr|rrrr}
\toprule
 & \multicolumn{4}{c}{VGG16 @ SL=0.0} & \multicolumn{4}{c}{Xception @ SL=-0.5} & \multicolumn{4}{c}{EfficientNetB7 @ SL=0.0} & \multicolumn{4}{c}{EfficientNetV2L @ SL=-0.5} \\
 Dataset& NLL & RMSE & $\langle\sigma_*\rangle$ & MAPE & NLL & RMSE & $\langle\sigma_*\rangle$ & MAPE & NLL & RMSE & $\langle\sigma_*\rangle$ & MAPE & NLL & RMSE & $\langle\sigma_*\rangle$ & MAPE \\\midrule
train & -4.2 & 0.0042 & 0.018 & \textbf{0.77} & -2.0 & 0.11 & 0.12 & 14. & -4.4 & 0.0098 & 0.0097 & 1.3 & -4.1 & 0.015 & 0.012 & 1.7 \\
test & -4.1 & 0.0051 & 0.019 & \textbf{0.87} & -1.9 & 0.12 & 0.12 & 14. & -4.4 & 0.011 & 0.010 & 1.4 & -4.1 & 0.016 & 0.013 & 1.9 \\
E1 & -2.2 & 0.039 & 0.020 & 15. & -1.9 & 0.083 & 0.11 & 33. & 1.7 & 0.038 & 0.010 & \textbf{15}. & -0.99 & 0.040 & 0.016 & 17. \\
E2 & -0.93 & 0.046 & 0.020 & 15. & -2.5 & 0.0099 & 0.083 & \textbf{3.1} & 5.9 & 0.032 & 0.0077 & 12. & -0.70 & 0.034 & 0.012 & 12. \\
E3 & 0.61 & 0.12 & 0.050 & 29. & -2.5 & 0.035 & 0.077 & \textbf{7.2} & -0.89 & 0.037 & 0.021 & 8.5 & -2.3 & 0.043 & 0.029 & 9.8 \\
E4 & -3.4 & 0.021 & 0.015 & 6.2 & -2.2 & 0.061 & 0.089 & 22. & -3.4 & 0.014 & 0.0075 & 4.0 & -4.1 & 0.0091 & 0.014 & \textbf{2.9} \\
E (single) & -3.8 & 0.010 & 0.020 & 2.9 & -2.4 & 0.0072 & 0.087 & 2.0 & -3.1 & 0.015 & 0.0083 & 4.3 & -4.3 & 0.00067 & 0.014 & \textbf{0.19} \\
E HS & 0.70 & 0.11 & 0.041 & 19. & -1.9 & 0.11 & 0.11 & 15. & -2.1 & 0.035 & 0.023 & 4.5 & -3.1 & 0.025 & 0.025 & \textbf{4.5} \\\bottomrule
\end{tabular}



    \caption{Loss metrics for the training dataset QProp+FA+CA. For each CNN architecture, we show only saturation level $SL$ used in the image augmentation for which NLL on dataset E4 is the lowest. On E4, the best accuracy is obtained for EfficientNetV2L which simultaneously gives acceptable errors for other experimental data. Error metrics are similar on test/train subsets of QProp+FA+CA (first two rows) which is a signature of good model generalization. The lowest values of MAPE for each dataset are in bold.}
    \label{tab:models}
\end{table*}

    \subsection{Data preprocessing and augmentation}
    \label{sec:preprocessing}
    In this section, we describe a few technical steps that were necessary to preprocess the PMDs to form a viable image input for the CNNs.
    As a first step, we identified a common range of momenta accessible in all datasets to form a rectangle with $p_\perp \in [0.001 \mathrm{~a.u.}, 1.15 \mathrm{~a.u.}]$, $p_{||}\in [-1.15\mathrm{~a.u.}, 1.15\mathrm{~a.u.}]$, where the discretization is given by the resolution of the experimental dataset E1 $\Delta p_\perp=\Delta p_{||}=0.0049$~a.u. Then, we interpolated the theoretical data on the same 2-dimensional momentum grid.


	The natural scale for features contained in the PMD is logarithmic and the signal has to be transformed accordingly, see eg. \cite{zimmermann_deep_2019} for CNNs applied to diffraction images with similar properties.
    Thus, we take the logarithm of the PMDs, rescale and apply an offset to the pixel values so that in the end they fill the interval $[-1, 1]$. Thus, the pixel value $1$ represents the largest peak probability density in each PMD and $-1$ is the value smaller by a factor of $10^{-6}$. Each pixel is clipped according to $X_{ij} \rightarrow \max (-1, X_{ij})$. Six orders of magnitude are selected based on heuristics that will capture all features in the experimental data, at the same time not misguiding the network by showing extremely precise, low values in the theoretical data. All images are then resized to $224$ by $224$ pixels with $3$ (repeated) color channels to match the standard of the Imagenet dataset expected by the pre-trained models.

	We split the theoretical datasets, see Section \ref{sec:datasets}, into training ($80\%$), validation ($10\%$), and test ($10\%$) subsets. For each model in the ensemble, the training/validation split is different and random, while the test dataset is constructed once by a random selection from the full dataset.


    During the training phase, we perform image augmentation
	\cite{shorten_survey_2019}. Each input image is randomly reflected in the up-down and left-right axes, and its contrast and brightness are randomly set from the interval $(0.1, 1.0)$ and $(-1, 1)$, respectively, using the builtin Keras \cite{chollet_keras_2015} functionalities, see Fig.~\ref{fig:ExampleDataSets}e). The final image is clipped to a fixed range $[-1, 1]$. While the testing data does not include reflected images, during training we apply reflections to help the network learn the same  ``shapes" in four different settings with the aim of reducing overfitting. On the other hand, the testing data has inherently varying levels of contrast and background signal (``brightness") and we deliberately make the network insensitive to them.


    In our efforts to make the models useful for experimentalists, we encountered an obstacle: while the models performed well on theoretical data (see next Section \ref{sec:results}), they failed for experiments. We fixed it by adding a single extra step in the augmentation pipeline, motivated by the histogram in Fig.~\ref{fig:ExampleDataSets}g), that shows significant differences between distributions of pixel values in theory and experiment, especially for the brightest pixels. This suggests there was some uncontrolled detector saturation effect in the experiment, at a level not necessarily fixed between experiments. Thus, prior to all augmentations described earlier, we simulate a random detector saturation level. For each sample PMD, we draw a random  ``saturation" value $x$ from the interval $[SL, 1]$ and transform each pixel according to $X_{ij} \rightarrow \min(X_{i,j}, x) + 1 - x$. The lower bound on the random saturation value $SL$ is a parameter that has to be found by checking the performance of the model on part of the experimental data. By making the saturation level random, we increase the training difficulty, but at the same time make the model insensitive to detector saturation that occurs in a real experiment. \footnote{Since pixel brightness values are limited to the range $[-1,1]$, $SL=-1$ corresponds to a fully random saturation level, whereas $SL=1$ to no detector saturation effect present at all (case of a ``perfect detector").}


\subsection{Training}
	\label{sec:training}
    We train an ensemble of size $M=5$ of pretrained models VGG16, Xception, EfficientNetB7, EfficientNetV2L on four datasets: QProp, QProp+CA, QProp+FA, QProp+CA+FA, for a set of random detector saturation level lower bounds ${SL\in \lbrace -1, -0.5, 0, 0.5, 1 \rbrace}$, yielding $400$ models in total. We choose the Adam optimizer~\cite{kingma_adam_2014} and a batch size of $32$. During the first $50$ training epochs, the base model has fixed weights and only the added, two randomly initialized dense layers each with $1$-dimensional output $\mu(X)$ and $\sigma(X)$ are being updated at a learning rate of $10^{-3}$. This roughly sets up the last layer while not destroying pretrained filters. In the next $150$ iterations the model is fine-tuned: all weights are updated at a learning rate $10^{-4}$, which decreases by a factor of $0.5$ every $50$ iterations. We stop the training if the loss on the evaluation dataset does not decrease for more than $100$ epochs to save on computing time. All models can be trained in parallel.
    \footnote{The training effectiveness may be improved by a recent training scheme \cite{sluijterman_optimal_2023} not applied here: in the first few iterations one should optimize the mean, while keeping the variance fixed.}

    We were unable to train any models if they were initialized with random weights. It demonstrates the power of transfer learning from real-world images to physical experiments. The models are already capable of extracting basic shapes from images and need fine-tuning only.


\section{Results}
	\label{sec:results}
    The quality of all $400$ trained models is measured in terms of the NLL (see \eqref{eq:NLL}), root mean squared error (RMSE) and the mean absolute percentage error (MAPE) achieved on the test datasets. For theoretical data sets, the `true' intensity is known exactly, so RMSE and MAPE give the error on model prediction. For experimental data sets, the `true' intensity carries 10\% uncertainty, so the MAPE only needs to be within this bound.
    The mean predictive uncertainty $\sigma_*(X)$ is the models' prediction of the uncertainty. This can be compared with the RMSE to assess if the models  ``know when they're wrong". All these values are fully tabulated in the Supplemental material. Here we give a general overview of these results and describe a method of model post-selection that allows us to find the best model candidates for experimental data presented in Table~\ref{tab:models}.

    As a standard practice, no augmentation techniques are applied in the testing phase unless explicitly noted. While this can decrease performance of some models on theoretical testing data due to input distribution shifts, we concentrate more on the performance of experimental data which naturally includes imperfections.
    In the “perfect detector” augmentation scenario, $SL=1$, all models are trainable on all theoretical \Qprop\ datasets with a testing MAPE$~<1\%$ (except for the VGG16 model and QProp+CA dataset where MAPE$~=4.9\%$).

    Testing on experimental data, we notice that including focal averaging and CEP-averaging in the training results in a smaller error. This is supported by a visual inspection of the PMDs in Figs.~\ref{fig:ExampleDataSets}a), d), f), which unveils a greater resemblance between experimental and QProp+FA+CA rather than QProp data with more small-sized features. Thus, we restrict our further discussion to the QProp+FA+CA training dataset. Moreover, we observe that the quality of the prediction is always improved if detector saturation effects are included ($SL \leq 0.5$) than if they are not ($SL=1$).


    \begin{figure}[t]
        \centering
            \includegraphics[width=\columnwidth]{fig3_BayesianEfficientNetV2L_SL-0d5_1column_TWcm2.pdf}
            \caption{Performance of the EfficientNetV2L model for training dataset QProp+FA+CA, saturation level $SL=-0.5$, for different test datasets. Top plot shows the value of $U_p$ predicted by the model as a function of the true value. Perfect predictions would lie on the black diagonal line. Including focal- and CEP-averaging in the training dataset was necessary to achieve results in agreement with the experimental value, up to the estimated experimental uncertainty level of $10\%$, as shown in the middle plot where most experimental points lie below $10\%$ absolute error line. The bottom plot shows a measure of the model confidence, standard deviation $\sigma_*(U_p)$, as a percentage of the true $U_p$ value.}
            \label{fig:pred_vs_true}
        \end{figure}





   Aiming for the highest-quality models for experiments, we post-select the saturation level based on the best NLL on a single evaluation dataset E4.
   Obtained metrics are presented in Table~\ref{tab:models}. We find that the fittest model is the most sophisticated EfficientNetV2L trained with a saturation level $SL=-0.5$, reaching a MAPE of $2.9\%$ on E4, which is well below the experimental error. The predictive uncertainty is $\sigma_*(X)=0.014$~a.u., corresponding to $6\times 10^{12}$~W/cm$^2$ in typical intensity units. This is close to the reported RMSE, signaling a good calibration of the model confidence. Almost all errors on other datasets for this model also fall within the error bars of the experimental label. In Fig.~\ref{fig:pred_vs_true}, we plot the predicted value of ponderomotive energy $\up$, proportional to the laser intensity, as a function of the true $\up$, for experimental and theoretical (\Qprop\ and SFA) inputs.
   All predictions are presented with uncertainties computed using \eqref{eq:sigma}, given by the vertical error bars in Fig.~\ref{fig:pred_vs_true} (upper panel) and as a percentage (lower panel).


   Predictions on the test/train QProp+FA+CA dataset in Fig.~\ref{fig:pred_vs_true} lie within the uncertainty estimate around the true value up to $U_p\approx 0.5$. For larger $U_p$ the model slightly deviates, finishing with an error of around $7\%$ at $U_p=0.95$. We expect this drop in performance is associated with a limited range of momenta present in the training dataset. The $p=2\sqrt{\up}$ peak, used for labeling PMDs manually, is located at the border of accessible momenta at around $\up\sim 0.66$. Thus, some other, possibly less expressed features in the PMD have to be used by the CNN.

   Before we proceed to experimental data, we quickly cross-check the output of the model on the SFA dataset. Looking at the strong qualitative difference between sample images in Figs.~\ref{fig:ExampleDataSets} a) and b), not to mention the dramatic dissimilarity of the histograms of both datasets in Fig.~\ref{fig:ExampleDataSets}g), it is surprising that the model finds any structure at all in the SFA+FA+CA data, i.e., there is a significant positive correlation coefficient between the true value and the prediction. 
   %
   However, the uncertainty of SFA+FA+CA is strongly underestimated, particularly for the largest values. This is a warning that on the strongly out-of-distribution data\footnote{Out-of-distribution data describes data that is far from the kind of data that was used for training, where the model is likely to give unpredictable results.}, the model may fail to predict the true value and report a relatively high confidence.


   Testing the same model on experimental datasets we notice that errors generally stay equal or lower than the experimental uncertainty of $10\%$ (middle panel). Note that the model was selected based on its performance on evaluation dataset E4, yet its predictions are consistent for other datasets.
   Overall, there is good agreement with the vast majority of intensity predictions carrying an uncertainty below that of attainable through traditional methods, getting as low as 4\% in a number of cases.
   In particular, the high-statistic dataset E HS gives a very good agreement over a wide range of intensities. It was crucial to consider focal averaging here, otherwise, the “true” labels deviated from the predicted value, with an absolute percentage error up to $\sim50\%$ for large intensities.
   Notably, a large relative error is observed for a few points of the dataset E1 at low intensities. We believe this is primarily caused by a low contrast in the input image due to a lower number of electron counts in this setting, see Supplemental material for an example image, since this deviation can be manually removed by increasing the contrast of the input images.

    \section{Explanations}
    \label{sec:explainability}

    \subsection{Methods}

    The high accuracy of the models presented above makes them a readily useful tool for parameter estimation. On the other hand, due to a rather complex flow of information in image regression, the understanding of why a certain output is produced, is lacking, i.e., we deal with a  ``black box”. This hinders progress in the development of new, more accurate models, and, more importantly, does not give any insight into the underlying physical reasoning. These issues are addressed in the following section using so-called explainability techniques that quantify how certain features of the input contribute to the output, see recent review \cite{linardatos_explainable_2020}, or \cite{mohseni_multidisciplinary_2021} for a more general survey.

    The most popular explainability techniques were designed for classifiers, and special care needs to be taken when using them for regression, see \cite{letzgus_toward_2021}. Here we adopt the simple yet powerful strategy of Ref.~\cite{zhang_explainability_2020}, where explanations of deep regression models are obtained directly using methods for classification.

    Three basic approaches to explainable regression have been developed to date and a variety of algorithms can be found in each category \cite{letzgus_toward_2021}. The most straightforward are removal-based explanations \cite{covert_explaining_2021}, measuring the importance of a given subset of input features by hiding it from the model. Because there is an exponential number of subsets to check, usually these methods are limited to at most 15-20 features before the analysis of images becomes infeasible. Another set of methods are gradient-based explanations that rely on the computation of the gradient of the input in a single forward/backward propagation of the signal. They are built on the intuition that if some region of the image is important for the prediction, a small change in this region will noticeably change the output. Finally, propagation-based explanations aim to leverage the neural network structure to produce the feature attribution map. In particular, the layer-wise relevance propagation (LRP) algorithm \cite{bach_pixel-wise_2015} assigns a relevance score $R_i$ to each neuron $i$ based on the activations of the neurons in the next layer. Scores in a single layer are conserved, i.e., sum up to the final prediction. The relevance scores are calculated layer by layer in the backward pass from the output, until the input is reached.

    We apply 10 different explainability algorithms available in the iNNvestigate toolbox \cite{alber_innvestigate_2019}. Our analysis is restricted to the VGG16 model at $SL=0.0$ instead of the  ``best performing” EfficientNetV2L at $SL=-0.5$, presented in Fig.~\ref{fig:pred_vs_true}, due to the large size of the latter, making most methods intractable due to memory requirements, and a  ``swish” activation function which is not compatible with many algorithms. Out of all tested algorithms, for presentation, we post-select the four most relevant ones by scoring them following \cite{samek_evaluating_2017, zhang_explainability_2020}. Each explanation image is divided into $8$ by $8$ regions and perturbed one region at a time, from most to least important, according to the output of a given explanation algorithm. If the regions marked by the algorithm are indeed relevant for predictions, the accuracy of the model drops faster than when the perturbations are applied in random order. In our case, 9 out of 10 tested methods perform better than a random one, and the four presented in Fig.~\ref{fig:explanations} are noticeably better than the rest, see Supplemental Material for further details.


    \subsection{What the model learns.}
    \begin{figure}
        \centering
            \includegraphics[width=1.0\linewidth]{expl_QProp_FA_CA_Esingle_5.pdf}
            \caption{Explanations for the VGG16 model, obtained with 4 most informative methods (upper labels), from left (best) to right (worst). For QProp+FA+CA, true $\up=0.3535$ and predicted $\up=0.3550$, for experiment true $\up=0.351$ and predicted $\up=0.3344$. Red/white/blue colors correspond to positive/neutral/negative attribution in each explainability method.
}
            \label{fig:explanations}
    \end{figure}

    In Fig.~\ref{fig:explanations}, we used four explainability techniques: two variants of the LRP \cite{bach_pixel-wise_2015}, Guided Backpropagation \cite{springenberg_striving_2015}, and DeepTaylor \cite{montavon_explaining_2017} to highlight regions on the PMD that have the most effect on the predicted value. This can also be interpreted as highlighting the features that are the most sensitive to changes in the ponderomotive energy, and thus it is a unique way to extract physical meaning. The highlighted regions can be identified as known interference features that are used in photoelectron holography. The guided backpropagation technique picks out two regions. The first region is at the end of the `legs' of the so-called spider-like structure \cite{huismans_timeresolved_2011,hickstein_direct_2012}, see green dashed rectangles in Fig.~\ref{fig:explanations}. This is formed via the interference between pairs of electronic wavepackets that are forward scattered/deflected off by the residual ion and have a differing degree of interaction with the core.

    The explainability diagrams specifically pick out modulations along the spider legs above the direct boundary ($p=2\sqrt{\up}$). These modulations were discussed in \cite{werby_dissecting_2021}, where their sensitivity to the ponderomotive energy was already exploited for determining the laser intensity.
    Crucially, these modulations have been shown previously \cite{maxwell_coulombcorrected_2017} to be described approximately by circles with their centers determined by $\up$. Thus, explaining the sensitivity to the ponderomotive energy.

    Another region highlighted, with high perpendicular momentum, is the so-called carpet-like \cite{korneev_interference_2012, kang_holographic_2020} or spiral-like \cite{maxwell_spirallike_2020} structure, see blue dotted lines in  Fig.~\ref{fig:explanations}. Around $p_{||}=0$, the interference maxima can be described by $\ip+\up+E=2n\omega$ (with $n\in \mathbb{Z}$), which clearly encodes the ponderomotive energy. Away from $p_{||}=0$, in the DeepTaylor method, we see the strongest contribution. Here, the above equation will not hold exactly, but the fringes will be dependent on the interplay of two rescattered wavepackets, that undergo different rescattering angles, which will be heavily dependent on the laser intensity/ponderomotive energy, as it determines the tunnel exit and initial scattering velocity.

    Across many regions, and particularly in the DeepTaylor method, we can see the above-threshold ionization rings, which are ring-shaped interferences due to nearly identical wavepackets released at an integer number of laser cycles apart, see black dot-dashed circles in Fig.~\ref{fig:explanations}. The maxima may be described by a similar equation to the carpet-like structure $\ip+\up+E=n\omega$, which is clearly sensitive to the ponderomotive energy. In previous work, this has been shown to provide important sensitivity for determining laser intensity \cite{maxwell_quantum_2021}.



    \section{Conclusions}
    We have proposed and tested deep learning models, powerful enough to detect objects in real world images, as a versatile analysis tool for strong-field ionization processes, adopting deep CNNs as our main workhorse. We have found they are capable of extracting physical parameters of interest---the laser peak intensity---and connect the parameter back to a specific feature, such as the interferences observed in the PMDs.
    We have overcome key difficulties using theoretically-trained CNNs with experimental data, which paves the way for CNNs to be used in a variety of settings for strong-field ionization, particularly when characterizing the laser field or target. The CNNs have been tested via pulse characterization, in particular determining the laser field intensity, on a large experimental data set, consistently yielding lower uncertainties than are achievable in traditional methods. For the prediction of uncertainty, we have used a reliable predictive uncertainty approach, that provides additional evaluation of the experimental conditions. We have also verified that other laser field parameters such as the pulse length could easily be extracted.
    
    Deep CNNs can utilize information present in the picture to its full extent, while a human expert would typically be limited to using only a small subset of physical effects that are most sensitive to a change in the parameters. As such, we have developed a novel tool that can be applied to strong-field ionization photoelectron momentum spectra without any special requirements from the experimental data. 
    \changes{We achived this generalizability from our CNN by using state-of-the art pretrained models. Training such models from scratch, to work well with experimental data, was not possible with our data set. Instead using the concept of transfer learning, we found the pretrained networks to work exceptionally well. This approach is easily repeatable as these model are freely availble and it significantly reduces size of the required training data.}
    \changes{The idea of transfer learning may be exploited in future work in order to expand the parameter range of the model, without having to use as much training data. Here, the exceptional capability and large capacity of the CNN models that we use could help make them more generalizable.}
    
    We show-cased the “explainability” capability of CNNs, which highlighted the most relevant features in the PMDs, which could be associated directly with holographic interferences that display considerable sensitivity to changes in the ponderomotive energy. Thus, directly connecting the CNNs predictions to fundamental physical processes.
    \changes{Explainability methods takeaway some of the black box nature of CNNs and can to be used to highligh new physics, providing a powerful way to connect experiment and theory.}

    This study paves the way for further exploitation of CNNs to analyse strong-field ionization data, yielding new physical insights or confirming existing understanding. The recipe we developed, training the neural networks to be insensitive to various types of imperfections through the use of data augmentation techniques, made them ideal candidates for robust parameter extraction. We emphasize that the same procedure can be repeated and used to develop a range of analysis tools, which, for instance, could be highly useful for extracting atomic targets and/or pulse shapes, or further developing photoelectron holographic imaging, where inversion of experimental data is very difficult. Using these techniques, universal extraction of physical parameters is possible from existing and future experimental data, regardless of whether all details of the physical processes at play are fully understood.


%	\section*{Data availability statement}
%	The data that support the findings of this study are openly available at the following URL: \url{https://chaos.if.uj.edu.pl/ZOA/index.php?which=opendata&dataset=/2303.13940}.
	
\acknowledgments
We gratefully acknowledge Poland’s high-performance computing infrastructure PLGrid (HPC Centers: ACK Cyfronet AGH) for providing computer facilities and support within computational grant no. PLG/2022/015830. T.S. is supported by National Science Centre (Poland) under grant 2019/35/B/ST2/00034. This research was
also funded by National Science Centre (Poland) under the OPUS call within the WEAVE programme
2021/43/I/ST3/01142 (J.Z.) A
partial support by the Strategic Programme Excellence
Initiative at Jagiellonian University as well as that within the QuantEra II Programme that has received funding from the European Union's Horizon 2020 research and innovation programme under Grant Agreement No 101017733 DYNAMITE (M.L. and J.Z.). M.F.C. acknowledges financial support from the Guangdong Province Science and Technology Major Project (Future functional materials under extreme conditions, No. 2021B0301030005) and the Guangdong Natural Science Foundation (General Program project No. 2023A1515010871). A.S.M. acknowledges funding support from: The European Union’s Horizon 2020 research and innovation programme under the Marie Sk\l odowska-Curie grant agreement, SSFI No.\ 887153.

N.W. and P.H.B. are supported by the U.S. Department of Energy, Office of Science, Basic Energy Sciences (BES), Chemical Sciences, Geosciences, and Biosciences Division, AMOS Program.

M.L. acknowledges support from: ERC AdG NOQIA; Ministerio de Ciencia y Innovation Agencia Estatal de Investigaciones (PGC2018-097027-B-I00/10.13039/501100011033, CEX2019-000910-S/10.13039/501100011033, Plan National FIDEUA PID2019-106901GB-I00, FPI, QUANTERA MAQS PCI2019-111828-2, QUANTERA DYNAMITE PCI2022-132919, Proyectos de I+D+I “Retos Colaboración” QUSPIN RTC2019-007196-7); MICIIN with funding from European Union NextGenerationEU(PRTR-C17.I1) and by Generalitat de Catalunya; Fundació Cellex; Fundació Mir-Puig; Generalitat de Catalunya (European Social Fund FEDER and CERCA program, AGAUR Grant No. 2021 SGR 01452, QuantumCAT \ U16-011424, co-funded by ERDF Operational Program of Catalonia 2014-2020); Barcelona Supercomputing Center MareNostrum (FI-2022-1-0042); EU Horizon 2020 FET-OPEN OPTOlogic (Grant No 899794); EU Horizon Europe Program (Grant Agreement 101080086 — NeQST), National Science Centre, Poland (Symfonia Grant No. 2016/20/W/ST4/00314); ICFO Internal “QuantumGaudi” project; European Union’s Horizon 2020 research and innovation program under the Marie-Skłodowska-Curie grant agreement No 101029393 (STREDCH) and No 847648 (“La Caixa” Junior Leaders fellowships ID100010434: LCF/BQ/PI19/11690013, LCF/BQ/PI20/11760031, LCF/BQ/PR20/11770012, LCF/BQ/PR21/11840013). Views and opinions expressed in this work are, however, those of the author(s) only and do not necessarily reflect those of the European Union, European Climate, Infrastructure and Environment Executive Agency (CINEA), nor any other granting authority. Neither the European Union nor any granting authority can be held responsible for them.

% Bibliography
%\input{main_arxiv_28042023.bbl}
%\bibliography{SF-ML-arxiv}
%merlin.mbs apsrev4-1.bst 2010-07-25 4.21a (PWD, AO, DPC) hacked
%Control: key (0)
%Control: author (8) initials jnrlst
%Control: editor formatted (1) identically to author
%Control: production of article title (-1) disabled
%Control: page (0) single
%Control: year (1) truncated
%Control: production of eprint (0) enabled
\providecommand{\noopsort}[1]{} \providecommand{\noopsort}[1]{}
\begin{thebibliography}{85}%
\makeatletter
\providecommand \@ifxundefined [1]{%
 \@ifx{#1\undefined}
}%
\providecommand \@ifnum [1]{%
 \ifnum #1\expandafter \@firstoftwo
 \else \expandafter \@secondoftwo
 \fi
}%
\providecommand \@ifx [1]{%
 \ifx #1\expandafter \@firstoftwo
 \else \expandafter \@secondoftwo
 \fi
}%
\providecommand \natexlab [1]{#1}%
\providecommand \enquote  [1]{``#1''}%
\providecommand \bibnamefont  [1]{#1}%
\providecommand \bibfnamefont [1]{#1}%
\providecommand \citenamefont [1]{#1}%
\providecommand \href@noop [0]{\@secondoftwo}%
\providecommand \href [0]{\begingroup \@sanitize@url \@href}%
\providecommand \@href[1]{\@@startlink{#1}\@@href}%
\providecommand \@@href[1]{\endgroup#1\@@endlink}%
\providecommand \@sanitize@url [0]{\catcode `\\12\catcode `\$12\catcode
  `\&12\catcode `\#12\catcode `\^12\catcode `\_12\catcode `\%12\relax}%
\providecommand \@@startlink[1]{}%
\providecommand \@@endlink[0]{}%
\providecommand \url  [0]{\begingroup\@sanitize@url \@url }%
\providecommand \@url [1]{\endgroup\@href {#1}{\urlprefix }}%
\providecommand \urlprefix  [0]{URL }%
\providecommand \Eprint [0]{\href }%
\providecommand \doibase [0]{http://dx.doi.org/}%
\providecommand \selectlanguage [0]{\@gobble}%
\providecommand \bibinfo  [0]{\@secondoftwo}%
\providecommand \bibfield  [0]{\@secondoftwo}%
\providecommand \translation [1]{[#1]}%
\providecommand \BibitemOpen [0]{}%
\providecommand \bibitemStop [0]{}%
\providecommand \bibitemNoStop [0]{.\EOS\space}%
\providecommand \EOS [0]{\spacefactor3000\relax}%
\providecommand \BibitemShut  [1]{\csname bibitem#1\endcsname}%
\let\auto@bib@innerbib\@empty
%</preamble>
\bibitem [{\citenamefont {Karagiorgi}\ \emph {et~al.}(2022)\citenamefont
  {Karagiorgi}, \citenamefont {Kasieczka}, \citenamefont {Kravitz},
  \citenamefont {Nachman},\ and\ \citenamefont
  {Shih}}]{karagiorgi_machine_2022}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {G.}~\bibnamefont
  {Karagiorgi}}, \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont
  {Kasieczka}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Kravitz}},
  \bibinfo {author} {\bibfnamefont {B.}~\bibnamefont {Nachman}}, \ and\
  \bibinfo {author} {\bibfnamefont {D.}~\bibnamefont {Shih}},\ }\href {\doibase
  10.1038/s42254-022-00455-1} {\bibfield  {journal} {\bibinfo  {journal}
  {Nature Reviews Physics}\ }\textbf {\bibinfo {volume} {4}},\ \bibinfo {pages}
  {399} (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {VanderPlas}\ \emph {et~al.}(2012)\citenamefont
  {VanderPlas}, \citenamefont {Connolly}, \citenamefont {Ivezi{\'c}},\ and\
  \citenamefont {Gray}}]{vanderplas_introduction_2012}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {VanderPlas}}, \bibinfo {author} {\bibfnamefont {A.~J.}\ \bibnamefont
  {Connolly}}, \bibinfo {author} {\bibfnamefont {{\v Z}.}~\bibnamefont
  {Ivezi{\'c}}}, \ and\ \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {Gray}},\ }in\ \href {\doibase 10.1109/CIDU.2012.6382200} {\emph {\bibinfo
  {booktitle} {2012 {{Conference}} on {{Intelligent Data Understanding}}}}}\
  (\bibinfo {year} {2012})\ pp.\ \bibinfo {pages} {47--54}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Carleo}\ \emph {et~al.}(2019)\citenamefont {Carleo},
  \citenamefont {Cirac}, \citenamefont {Cranmer}, \citenamefont {Daudet},
  \citenamefont {Schuld}, \citenamefont {Tishby}, \citenamefont
  {{Vogt-Maranto}},\ and\ \citenamefont {Zdeborov{\'a}}}]{carleo_machine_2019}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {G.}~\bibnamefont
  {Carleo}}, \bibinfo {author} {\bibfnamefont {I.}~\bibnamefont {Cirac}},
  \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Cranmer}}, \bibinfo
  {author} {\bibfnamefont {L.}~\bibnamefont {Daudet}}, \bibinfo {author}
  {\bibfnamefont {M.}~\bibnamefont {Schuld}}, \bibinfo {author} {\bibfnamefont
  {N.}~\bibnamefont {Tishby}}, \bibinfo {author} {\bibfnamefont
  {L.}~\bibnamefont {{Vogt-Maranto}}}, \ and\ \bibinfo {author} {\bibfnamefont
  {L.}~\bibnamefont {Zdeborov{\'a}}},\ }\href {\doibase
  10.1103/RevModPhys.91.045002} {\bibfield  {journal} {\bibinfo  {journal}
  {Reviews of Modern Physics}\ }\textbf {\bibinfo {volume} {91}},\ \bibinfo
  {pages} {045002} (\bibinfo {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Dawid}\ \emph {et~al.}(2022)\citenamefont {Dawid},
  \citenamefont {Arnold}, \citenamefont {Requena}, \citenamefont {Gresch},
  \citenamefont {P{\l}odzie{\'n}}, \citenamefont {Donatella}, \citenamefont
  {Nicoli}, \citenamefont {Stornati}, \citenamefont {Koch}, \citenamefont
  {B{\"u}ttner}, \citenamefont {Oku{\l}a}, \citenamefont {{Mu{\~n}oz-Gil}},
  \citenamefont {{Vargas-Hern{\'a}ndez}}, \citenamefont {{Cervera-Lierta}},
  \citenamefont {Carrasquilla}, \citenamefont {Dunjko}, \citenamefont
  {Gabri{\'e}}, \citenamefont {Huembeli}, \citenamefont
  {{\noopsort{nieuwenburg}}{van Nieuwenburg}}, \citenamefont {Vicentini},
  \citenamefont {Wang}, \citenamefont {Wetzel}, \citenamefont {Carleo},
  \citenamefont {Greplov{\'a}}, \citenamefont {Krems}, \citenamefont
  {Marquardt}, \citenamefont {Tomza}, \citenamefont {Lewenstein},\ and\
  \citenamefont {Dauphin}}]{dawid_modern_2022}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {Dawid}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Arnold}},
  \bibinfo {author} {\bibfnamefont {B.}~\bibnamefont {Requena}}, \bibinfo
  {author} {\bibfnamefont {A.}~\bibnamefont {Gresch}}, \bibinfo {author}
  {\bibfnamefont {M.}~\bibnamefont {P{\l}odzie{\'n}}}, \bibinfo {author}
  {\bibfnamefont {K.}~\bibnamefont {Donatella}}, \bibinfo {author}
  {\bibfnamefont {K.~A.}\ \bibnamefont {Nicoli}}, \bibinfo {author}
  {\bibfnamefont {P.}~\bibnamefont {Stornati}}, \bibinfo {author}
  {\bibfnamefont {R.}~\bibnamefont {Koch}}, \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {B{\"u}ttner}}, \bibinfo {author} {\bibfnamefont
  {R.}~\bibnamefont {Oku{\l}a}}, \bibinfo {author} {\bibfnamefont
  {G.}~\bibnamefont {{Mu{\~n}oz-Gil}}}, \bibinfo {author} {\bibfnamefont
  {R.~A.}\ \bibnamefont {{Vargas-Hern{\'a}ndez}}}, \bibinfo {author}
  {\bibfnamefont {A.}~\bibnamefont {{Cervera-Lierta}}}, \bibinfo {author}
  {\bibfnamefont {J.}~\bibnamefont {Carrasquilla}}, \bibinfo {author}
  {\bibfnamefont {V.}~\bibnamefont {Dunjko}}, \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Gabri{\'e}}}, \bibinfo {author} {\bibfnamefont
  {P.}~\bibnamefont {Huembeli}}, \bibinfo {author} {\bibfnamefont
  {E.}~\bibnamefont {{\noopsort{nieuwenburg}}{van Nieuwenburg}}}, \bibinfo
  {author} {\bibfnamefont {F.}~\bibnamefont {Vicentini}}, \bibinfo {author}
  {\bibfnamefont {L.}~\bibnamefont {Wang}}, \bibinfo {author} {\bibfnamefont
  {S.~J.}\ \bibnamefont {Wetzel}}, \bibinfo {author} {\bibfnamefont
  {G.}~\bibnamefont {Carleo}}, \bibinfo {author} {\bibfnamefont
  {E.}~\bibnamefont {Greplov{\'a}}}, \bibinfo {author} {\bibfnamefont
  {R.}~\bibnamefont {Krems}}, \bibinfo {author} {\bibfnamefont
  {F.}~\bibnamefont {Marquardt}}, \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Tomza}}, \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Lewenstein}}, \ and\ \bibinfo {author} {\bibfnamefont
  {A.}~\bibnamefont {Dauphin}},\ }\href {\doibase 10.48550/arXiv.2204.04198}
  {\enquote {\bibinfo {title} {Modern applications of machine learning in
  quantum sciences},}\ } (\bibinfo {year} {2022}),\ \Eprint
  {http://arxiv.org/abs/2204.04198} {arxiv:2204.04198 [cond-mat,
  physics:quant-ph]} \BibitemShut {NoStop}%
\bibitem [{\citenamefont {Rawat}\ and\ \citenamefont
  {Wang}(2017)}]{rawat_deep_2017}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {W.}~\bibnamefont
  {Rawat}}\ and\ \bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont {Wang}},\
  }\href {\doibase 10.1162/neco_a_00990} {\bibfield  {journal} {\bibinfo
  {journal} {Neural Computation}\ }\textbf {\bibinfo {volume} {29}},\ \bibinfo
  {pages} {2352} (\bibinfo {year} {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Li}\ \emph {et~al.}(2022)\citenamefont {Li},
  \citenamefont {Liu}, \citenamefont {Yang}, \citenamefont {Peng},\ and\
  \citenamefont {Zhou}}]{li_survey_2022}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont
  {Li}}, \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Liu}}, \bibinfo
  {author} {\bibfnamefont {W.}~\bibnamefont {Yang}}, \bibinfo {author}
  {\bibfnamefont {S.}~\bibnamefont {Peng}}, \ and\ \bibinfo {author}
  {\bibfnamefont {J.}~\bibnamefont {Zhou}},\ }\href {\doibase
  10.1109/TNNLS.2021.3084827} {\bibfield  {journal} {\bibinfo  {journal} {IEEE
  Transactions on Neural Networks and Learning Systems}\ }\textbf {\bibinfo
  {volume} {33}},\ \bibinfo {pages} {6999} (\bibinfo {year}
  {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Kaufmann}\ \emph {et~al.}(2020)\citenamefont
  {Kaufmann}, \citenamefont {Zhu}, \citenamefont {Rosengarten}, \citenamefont
  {Maryanovsky}, \citenamefont {Harrington}, \citenamefont {Marin},\ and\
  \citenamefont {Vecchio}}]{kaufmann_crystal_2020}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {K.}~\bibnamefont
  {Kaufmann}}, \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Zhu}},
  \bibinfo {author} {\bibfnamefont {A.~S.}\ \bibnamefont {Rosengarten}},
  \bibinfo {author} {\bibfnamefont {D.}~\bibnamefont {Maryanovsky}}, \bibinfo
  {author} {\bibfnamefont {T.~J.}\ \bibnamefont {Harrington}}, \bibinfo
  {author} {\bibfnamefont {E.}~\bibnamefont {Marin}}, \ and\ \bibinfo {author}
  {\bibfnamefont {K.~S.}\ \bibnamefont {Vecchio}},\ }\href {\doibase
  10.1126/science.aay3062} {\bibfield  {journal} {\bibinfo  {journal}
  {Science}\ }\textbf {\bibinfo {volume} {367}},\ \bibinfo {pages} {564}
  (\bibinfo {year} {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Hezaveh}\ \emph {et~al.}(2017)\citenamefont
  {Hezaveh}, \citenamefont {Levasseur},\ and\ \citenamefont
  {Marshall}}]{hezaveh_fast_2017}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Y.~D.}\ \bibnamefont
  {Hezaveh}}, \bibinfo {author} {\bibfnamefont {L.~P.}\ \bibnamefont
  {Levasseur}}, \ and\ \bibinfo {author} {\bibfnamefont {P.~J.}\ \bibnamefont
  {Marshall}},\ }\href {\doibase 10.1038/nature23463} {\bibfield  {journal}
  {\bibinfo  {journal} {Nature}\ }\textbf {\bibinfo {volume} {548}},\ \bibinfo
  {pages} {555} (\bibinfo {year} {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Krausz}\ and\ \citenamefont
  {Ivanov}(2009)}]{krausz_attosecond_2009}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {F.}~\bibnamefont
  {Krausz}}\ and\ \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Ivanov}},\ }\href {\doibase 10.1103/RevModPhys.81.163} {\bibfield  {journal}
  {\bibinfo  {journal} {Reviews of Modern Physics}\ }\textbf {\bibinfo {volume}
  {81}},\ \bibinfo {pages} {163} (\bibinfo {year} {2009})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Sali{\`e}res}\ \emph {et~al.}(1999)\citenamefont
  {Sali{\`e}res}, \citenamefont {L'Huillier}, \citenamefont {Antoine},\ and\
  \citenamefont {Lewenstein}}]{salieres_study_1999}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {P.}~\bibnamefont
  {Sali{\`e}res}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {L'Huillier}}, \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Antoine}},
  \ and\ \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Lewenstein}},\
  }in\ \href {\doibase 10.1016/S1049-250X(08)60219-0} {\emph {\bibinfo
  {booktitle} {Advances {{In Atomic}}, {{Molecular}}, and {{Optical
  Physics}}}}},\ Vol.~\bibinfo {volume} {41},\ \bibinfo {editor} {edited by\
  \bibinfo {editor} {\bibfnamefont {B.}~\bibnamefont {Bederson}}\ and\ \bibinfo
  {editor} {\bibfnamefont {H.}~\bibnamefont {Walther}}}\ (\bibinfo  {publisher}
  {{Academic Press}},\ \bibinfo {year} {1999})\ pp.\ \bibinfo {pages}
  {83--142}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Lewenstein}\ and\ \citenamefont
  {L'Huillier}(2009)}]{lewenstein_principles_2008}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Lewenstein}}\ and\ \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {L'Huillier}},\ }in\ \href {\doibase 10.1007/978-0-387-34755-4_7} {\emph
  {\bibinfo {booktitle} {Strong {{Field Laser Physics}}}}},\ \bibinfo {series
  and number} {Springer {{Series}} in {{Optical Sciences}}},\ \bibinfo {editor}
  {edited by\ \bibinfo {editor} {\bibfnamefont {T.}~\bibnamefont {Brabec}}}\
  (\bibinfo  {publisher} {{Springer}},\ \bibinfo {address} {{New York, NY}},\
  \bibinfo {year} {2009})\ pp.\ \bibinfo {pages} {147--183}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Ciappina}\ \emph {et~al.}(2017)\citenamefont
  {Ciappina}, \citenamefont {Pérez-Hernández}, \citenamefont {Landsman},
  \citenamefont {Okell}, \citenamefont {Zherebtsov}, \citenamefont {Förg},
  \citenamefont {Schötz}, \citenamefont {Seiffert}, \citenamefont {Fennel},
  \citenamefont {Shaaran}, \citenamefont {Zimmermann}, \citenamefont {Chacón},
  \citenamefont {Guichard}, \citenamefont {Zaïr}, \citenamefont {Tisch},
  \citenamefont {Marangos}, \citenamefont {Witting}, \citenamefont {Braun},
  \citenamefont {Maier}, \citenamefont {Roso}, \citenamefont {Krüger},
  \citenamefont {Hommelhoff}, \citenamefont {Kling}, \citenamefont {Krausz},\
  and\ \citenamefont {Lewenstein}}]{ciappina_attosecond_2017}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.~F.}\ \bibnamefont
  {Ciappina}}, \bibinfo {author} {\bibfnamefont {J.~A.}\ \bibnamefont
  {Pérez-Hernández}}, \bibinfo {author} {\bibfnamefont {A.~S.}\ \bibnamefont
  {Landsman}}, \bibinfo {author} {\bibfnamefont {W.~A.}\ \bibnamefont {Okell}},
  \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Zherebtsov}}, \bibinfo
  {author} {\bibfnamefont {B.}~\bibnamefont {Förg}}, \bibinfo {author}
  {\bibfnamefont {J.}~\bibnamefont {Schötz}}, \bibinfo {author} {\bibfnamefont
  {L.}~\bibnamefont {Seiffert}}, \bibinfo {author} {\bibfnamefont
  {T.}~\bibnamefont {Fennel}}, \bibinfo {author} {\bibfnamefont
  {T.}~\bibnamefont {Shaaran}}, \bibinfo {author} {\bibfnamefont
  {T.}~\bibnamefont {Zimmermann}}, \bibinfo {author} {\bibfnamefont
  {A.}~\bibnamefont {Chacón}}, \bibinfo {author} {\bibfnamefont
  {R.}~\bibnamefont {Guichard}}, \bibinfo {author} {\bibfnamefont
  {A.}~\bibnamefont {Zaïr}}, \bibinfo {author} {\bibfnamefont {J.~W.~G.}\
  \bibnamefont {Tisch}}, \bibinfo {author} {\bibfnamefont {J.~P.}\ \bibnamefont
  {Marangos}}, \bibinfo {author} {\bibfnamefont {T.}~\bibnamefont {Witting}},
  \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Braun}}, \bibinfo
  {author} {\bibfnamefont {S.~A.}\ \bibnamefont {Maier}}, \bibinfo {author}
  {\bibfnamefont {L.}~\bibnamefont {Roso}}, \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Krüger}}, \bibinfo {author} {\bibfnamefont
  {P.}~\bibnamefont {Hommelhoff}}, \bibinfo {author} {\bibfnamefont {M.~F.}\
  \bibnamefont {Kling}}, \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont
  {Krausz}}, \ and\ \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Lewenstein}},\ }\href {\doibase 10.1088/1361-6633/aa574e} {\bibfield
  {journal} {\bibinfo  {journal} {Reports on Progress in Physics}\ }\textbf
  {\bibinfo {volume} {80}},\ \bibinfo {pages} {054401} (\bibinfo {year}
  {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Itatani}\ \emph {et~al.}(2004)\citenamefont
  {Itatani}, \citenamefont {Levesque}, \citenamefont {Zeidler}, \citenamefont
  {Niikura}, \citenamefont {P{\'e}pin}, \citenamefont {Kieffer}, \citenamefont
  {Corkum},\ and\ \citenamefont {Villeneuve}}]{itatani_tomographic_2004}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Itatani}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Levesque}},
  \bibinfo {author} {\bibfnamefont {D.}~\bibnamefont {Zeidler}}, \bibinfo
  {author} {\bibfnamefont {H.}~\bibnamefont {Niikura}}, \bibinfo {author}
  {\bibfnamefont {H.}~\bibnamefont {P{\'e}pin}}, \bibinfo {author}
  {\bibfnamefont {J.~C.}\ \bibnamefont {Kieffer}}, \bibinfo {author}
  {\bibfnamefont {P.~B.}\ \bibnamefont {Corkum}}, \ and\ \bibinfo {author}
  {\bibfnamefont {D.~M.}\ \bibnamefont {Villeneuve}},\ }\href {\doibase
  10.1038/nature03183} {\bibfield  {journal} {\bibinfo  {journal} {Nature}\
  }\textbf {\bibinfo {volume} {432}},\ \bibinfo {pages} {867} (\bibinfo {year}
  {2004})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Zuo}\ \emph {et~al.}(1996)\citenamefont {Zuo},
  \citenamefont {Bandrauk},\ and\ \citenamefont
  {Corkum}}]{zuo_laserinduced_1996}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {T.}~\bibnamefont
  {Zuo}}, \bibinfo {author} {\bibfnamefont {A.~D.}\ \bibnamefont {Bandrauk}}, \
  and\ \bibinfo {author} {\bibfnamefont {P.~B.}\ \bibnamefont {Corkum}},\
  }\href {\doibase 10.1016/0009-2614(96)00786-5} {\bibfield  {journal}
  {\bibinfo  {journal} {Chemical Physics Letters}\ }\textbf {\bibinfo {volume}
  {259}},\ \bibinfo {pages} {313} (\bibinfo {year} {1996})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Huismans}\ \emph {et~al.}(2011)\citenamefont
  {Huismans}, \citenamefont {Rouz{\'e}e}, \citenamefont {Gijsbertsen},
  \citenamefont {Jungmann}, \citenamefont {Smolkowska}, \citenamefont {Logman},
  \citenamefont {L{\'e}pine}, \citenamefont {Cauchy}, \citenamefont {Zamith},
  \citenamefont {Marchenko}, \citenamefont {Bakker}, \citenamefont {Berden},
  \citenamefont {Redlich}, \citenamefont {{\noopsort{meer}}{van der Meer}},
  \citenamefont {Muller}, \citenamefont {Vermin}, \citenamefont {Schafer},
  \citenamefont {Spanner}, \citenamefont {Ivanov}, \citenamefont {Smirnova},
  \citenamefont {Bauer}, \citenamefont {Popruzhenko},\ and\ \citenamefont
  {Vrakking}}]{huismans_timeresolved_2011}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont
  {Huismans}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {Rouz{\'e}e}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {Gijsbertsen}}, \bibinfo {author} {\bibfnamefont {J.~H.}\ \bibnamefont
  {Jungmann}}, \bibinfo {author} {\bibfnamefont {A.~S.}\ \bibnamefont
  {Smolkowska}}, \bibinfo {author} {\bibfnamefont {P.~S. W.~M.}\ \bibnamefont
  {Logman}}, \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {L{\'e}pine}},
  \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Cauchy}}, \bibinfo
  {author} {\bibfnamefont {S.}~\bibnamefont {Zamith}}, \bibinfo {author}
  {\bibfnamefont {T.}~\bibnamefont {Marchenko}}, \bibinfo {author}
  {\bibfnamefont {J.~M.}\ \bibnamefont {Bakker}}, \bibinfo {author}
  {\bibfnamefont {G.}~\bibnamefont {Berden}}, \bibinfo {author} {\bibfnamefont
  {B.}~\bibnamefont {Redlich}}, \bibinfo {author} {\bibfnamefont {A.~F.~G.}\
  \bibnamefont {{\noopsort{meer}}{van der Meer}}}, \bibinfo {author}
  {\bibfnamefont {H.~G.}\ \bibnamefont {Muller}}, \bibinfo {author}
  {\bibfnamefont {W.}~\bibnamefont {Vermin}}, \bibinfo {author} {\bibfnamefont
  {K.~J.}\ \bibnamefont {Schafer}}, \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Spanner}}, \bibinfo {author} {\bibfnamefont {M.~Y.}\
  \bibnamefont {Ivanov}}, \bibinfo {author} {\bibfnamefont {O.}~\bibnamefont
  {Smirnova}}, \bibinfo {author} {\bibfnamefont {D.}~\bibnamefont {Bauer}},
  \bibinfo {author} {\bibfnamefont {S.~V.}\ \bibnamefont {Popruzhenko}}, \ and\
  \bibinfo {author} {\bibfnamefont {M.~J.~J.}\ \bibnamefont {Vrakking}},\
  }\href {\doibase 10.1126/science.1198450} {\bibfield  {journal} {\bibinfo
  {journal} {Science}\ }\textbf {\bibinfo {volume} {331}},\ \bibinfo {pages}
  {61} (\bibinfo {year} {2011})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {{Figueira de Morisson Faria}}\ and\ \citenamefont
  {Maxwell}(2020)}]{figueirademorissonfaria_it_2020}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {C.}~\bibnamefont
  {{Figueira de Morisson Faria}}}\ and\ \bibinfo {author} {\bibfnamefont
  {A.~S.}\ \bibnamefont {Maxwell}},\ }\href {\doibase 10.1088/1361-6633/ab5c91}
  {\bibfield  {journal} {\bibinfo  {journal} {Reports on Progress in Physics}\
  }\textbf {\bibinfo {volume} {83}},\ \bibinfo {pages} {034401} (\bibinfo
  {year} {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Hentschel}\ \emph {et~al.}(2001)\citenamefont
  {Hentschel}, \citenamefont {Kienberger}, \citenamefont {Spielmann},
  \citenamefont {Reider}, \citenamefont {Milosevic}, \citenamefont {Brabec},
  \citenamefont {Corkum}, \citenamefont {Heinzmann}, \citenamefont {Drescher},\
  and\ \citenamefont {Krausz}}]{hentschel_attosecond_2001}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Hentschel}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont
  {Kienberger}}, \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont
  {Spielmann}}, \bibinfo {author} {\bibfnamefont {G.~A.}\ \bibnamefont
  {Reider}}, \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {Milosevic}},
  \bibinfo {author} {\bibfnamefont {T.}~\bibnamefont {Brabec}}, \bibinfo
  {author} {\bibfnamefont {P.}~\bibnamefont {Corkum}}, \bibinfo {author}
  {\bibfnamefont {U.}~\bibnamefont {Heinzmann}}, \bibinfo {author}
  {\bibfnamefont {M.}~\bibnamefont {Drescher}}, \ and\ \bibinfo {author}
  {\bibfnamefont {F.}~\bibnamefont {Krausz}},\ }\href {\doibase
  10.1038/35107000} {\bibfield  {journal} {\bibinfo  {journal} {Nature}\
  }\textbf {\bibinfo {volume} {414}},\ \bibinfo {pages} {509} (\bibinfo {year}
  {2001})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Itatani}\ \emph {et~al.}(2002)\citenamefont
  {Itatani}, \citenamefont {Qu{\'e}r{\'e}}, \citenamefont {Yudin},
  \citenamefont {Ivanov}, \citenamefont {Krausz},\ and\ \citenamefont
  {Corkum}}]{itatani_attosecond_2002}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Itatani}}, \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont
  {Qu{\'e}r{\'e}}}, \bibinfo {author} {\bibfnamefont {G.~L.}\ \bibnamefont
  {Yudin}}, \bibinfo {author} {\bibfnamefont {M.~Y.}\ \bibnamefont {Ivanov}},
  \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Krausz}}, \ and\ \bibinfo
  {author} {\bibfnamefont {P.~B.}\ \bibnamefont {Corkum}},\ }\href {\doibase
  10.1103/PhysRevLett.88.173903} {\bibfield  {journal} {\bibinfo  {journal}
  {Physical Review Letters}\ }\textbf {\bibinfo {volume} {88}},\ \bibinfo
  {pages} {173903} (\bibinfo {year} {2002})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Paul}\ \emph {et~al.}(2001)\citenamefont {Paul},
  \citenamefont {Toma}, \citenamefont {Breger}, \citenamefont {Mullot},
  \citenamefont {Aug{\'e}}, \citenamefont {Balcou}, \citenamefont {Muller},\
  and\ \citenamefont {Agostini}}]{paul_observation_2001}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {P.~M.}\ \bibnamefont
  {Paul}}, \bibinfo {author} {\bibfnamefont {E.~S.}\ \bibnamefont {Toma}},
  \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Breger}}, \bibinfo
  {author} {\bibfnamefont {G.}~\bibnamefont {Mullot}}, \bibinfo {author}
  {\bibfnamefont {F.}~\bibnamefont {Aug{\'e}}}, \bibinfo {author}
  {\bibfnamefont {P.}~\bibnamefont {Balcou}}, \bibinfo {author} {\bibfnamefont
  {H.~G.}\ \bibnamefont {Muller}}, \ and\ \bibinfo {author} {\bibfnamefont
  {P.}~\bibnamefont {Agostini}},\ }\href {\doibase 10.1126/science.1059413}
  {\bibfield  {journal} {\bibinfo  {journal} {Science}\ }\textbf {\bibinfo
  {volume} {292}},\ \bibinfo {pages} {1689} (\bibinfo {year}
  {2001})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Muller}(2002)}]{muller_reconstruction_2002}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {H.}~\bibnamefont
  {Muller}},\ }\href {\doibase 10.1007/s00340-002-0894-8} {\bibfield  {journal}
  {\bibinfo  {journal} {Applied Physics B}\ }\textbf {\bibinfo {volume} {74}},\
  \bibinfo {pages} {s17} (\bibinfo {year} {2002})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Liu}\ \emph {et~al.}(2020)\citenamefont {Liu},
  \citenamefont {Zhang}, \citenamefont {Li}, \citenamefont {Shi}, \citenamefont
  {Zhou}, \citenamefont {Huang}, \citenamefont {Tang}, \citenamefont {Song},\
  and\ \citenamefont {Yang}}]{liu_deep_2020}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {X.}~\bibnamefont
  {Liu}}, \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Zhang}}, \bibinfo
  {author} {\bibfnamefont {J.}~\bibnamefont {Li}}, \bibinfo {author}
  {\bibfnamefont {G.}~\bibnamefont {Shi}}, \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Zhou}}, \bibinfo {author} {\bibfnamefont {B.}~\bibnamefont
  {Huang}}, \bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {Tang}},
  \bibinfo {author} {\bibfnamefont {X.}~\bibnamefont {Song}}, \ and\ \bibinfo
  {author} {\bibfnamefont {W.}~\bibnamefont {Yang}},\ }\href {\doibase
  10.1103/PhysRevLett.124.113202} {\bibfield  {journal} {\bibinfo  {journal}
  {Physical Review Letters}\ }\textbf {\bibinfo {volume} {124}},\ \bibinfo
  {pages} {113202} (\bibinfo {year} {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Lytova}\ \emph {et~al.}(2022)\citenamefont {Lytova},
  \citenamefont {Spanner},\ and\ \citenamefont {Tamblyn}}]{lytova_deep_2022}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Lytova}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Spanner}}, \
  and\ \bibinfo {author} {\bibfnamefont {I.}~\bibnamefont {Tamblyn}},\ }\href
  {\doibase 10.1139/cjp-2022-0115} {\bibfield  {journal} {\bibinfo  {journal}
  {Canadian Journal of Physics}\ } (\bibinfo {year} {2022}),\
  10.1139/cjp-2022-0115},\ \bibinfo {note} {publisher: NRC Research
  Press}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Chomet}\ \emph {et~al.}(2022)\citenamefont {Chomet},
  \citenamefont {Plesnik}, \citenamefont {Nicolae}, \citenamefont {Dunham},
  \citenamefont {Gover}, \citenamefont {Weaving},\ and\ \citenamefont
  {Faria}}]{chomet_controlling_2022}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {H.}~\bibnamefont
  {Chomet}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Plesnik}},
  \bibinfo {author} {\bibfnamefont {D.~C.}\ \bibnamefont {Nicolae}}, \bibinfo
  {author} {\bibfnamefont {J.}~\bibnamefont {Dunham}}, \bibinfo {author}
  {\bibfnamefont {L.}~\bibnamefont {Gover}}, \bibinfo {author} {\bibfnamefont
  {T.}~\bibnamefont {Weaving}}, \ and\ \bibinfo {author} {\bibfnamefont {C.~F.
  d.~M.}\ \bibnamefont {Faria}},\ }\href {\doibase 10.1088/1361-6455/aca4b0}
  {\bibfield  {journal} {\bibinfo  {journal} {Journal of Physics B: Atomic,
  Molecular and Optical Physics}\ }\textbf {\bibinfo {volume} {55}},\ \bibinfo
  {pages} {245501} (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Shvetsov-Shilovski}\ and\ \citenamefont
  {Lein}(2022)}]{shvetsov-shilovski_deep_2022}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {N.~I.}\ \bibnamefont
  {Shvetsov-Shilovski}}\ and\ \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Lein}},\ }\href {\doibase 10.1103/PhysRevA.105.L021102}
  {\bibfield  {journal} {\bibinfo  {journal} {Physical Review A}\ }\textbf
  {\bibinfo {volume} {105}},\ \bibinfo {pages} {L021102} (\bibinfo {year}
  {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {{Shvetsov-Shilovski}}\ and\ \citenamefont
  {Lein}(2023)}]{shvetsov-shilovski_transfer_2023}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {N.~I.}\ \bibnamefont
  {{Shvetsov-Shilovski}}}\ and\ \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Lein}},\ }\href {\doibase 10.1103/PhysRevA.107.033106}
  {\bibfield  {journal} {\bibinfo  {journal} {Physical Review A}\ }\textbf
  {\bibinfo {volume} {107}},\ \bibinfo {pages} {033106} (\bibinfo {year}
  {2023})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Liu}\ \emph {et~al.}(2021)\citenamefont {Liu},
  \citenamefont {Amini}, \citenamefont {Sanchez}, \citenamefont {Belsa},
  \citenamefont {Steinle},\ and\ \citenamefont {Biegert}}]{liu_machine_2021}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {X.}~\bibnamefont
  {Liu}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Amini}}, \bibinfo
  {author} {\bibfnamefont {A.}~\bibnamefont {Sanchez}}, \bibinfo {author}
  {\bibfnamefont {B.}~\bibnamefont {Belsa}}, \bibinfo {author} {\bibfnamefont
  {T.}~\bibnamefont {Steinle}}, \ and\ \bibinfo {author} {\bibfnamefont
  {J.}~\bibnamefont {Biegert}},\ }\href {\doibase 10.1038/s42004-021-00594-z}
  {\bibfield  {journal} {\bibinfo  {journal} {Communications Chemistry}\
  }\textbf {\bibinfo {volume} {4}},\ \bibinfo {pages} {154} (\bibinfo {year}
  {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Brunner}\ \emph {et~al.}(2022)\citenamefont
  {Brunner}, \citenamefont {Duensing}, \citenamefont {Schröder}, \citenamefont
  {Mittermair}, \citenamefont {Golkov}, \citenamefont {Pollanka}, \citenamefont
  {Cremers},\ and\ \citenamefont {Kienberger}}]{brunner_deep_2022}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {C.}~\bibnamefont
  {Brunner}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Duensing}},
  \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Schröder}}, \bibinfo
  {author} {\bibfnamefont {M.}~\bibnamefont {Mittermair}}, \bibinfo {author}
  {\bibfnamefont {V.}~\bibnamefont {Golkov}}, \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Pollanka}}, \bibinfo {author} {\bibfnamefont
  {D.}~\bibnamefont {Cremers}}, \ and\ \bibinfo {author} {\bibfnamefont
  {R.}~\bibnamefont {Kienberger}},\ }\href {\doibase 10.1364/OE.452108}
  {\bibfield  {journal} {\bibinfo  {journal} {Optics Express}\ }\textbf
  {\bibinfo {volume} {30}},\ \bibinfo {pages} {15669} (\bibinfo {year}
  {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Trebino}\ \emph {et~al.}(1997)\citenamefont
  {Trebino}, \citenamefont {DeLong}, \citenamefont {Fittinghoff}, \citenamefont
  {Sweetser}, \citenamefont {Krumb{\"u}gel}, \citenamefont {Richman},\ and\
  \citenamefont {Kane}}]{trebino_measuring_1997}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {R.}~\bibnamefont
  {Trebino}}, \bibinfo {author} {\bibfnamefont {K.~W.}\ \bibnamefont {DeLong}},
  \bibinfo {author} {\bibfnamefont {D.~N.}\ \bibnamefont {Fittinghoff}},
  \bibinfo {author} {\bibfnamefont {J.~N.}\ \bibnamefont {Sweetser}}, \bibinfo
  {author} {\bibfnamefont {M.~A.}\ \bibnamefont {Krumb{\"u}gel}}, \bibinfo
  {author} {\bibfnamefont {B.~A.}\ \bibnamefont {Richman}}, \ and\ \bibinfo
  {author} {\bibfnamefont {D.~J.}\ \bibnamefont {Kane}},\ }\href {\doibase
  10.1063/1.1148286} {\bibfield  {journal} {\bibinfo  {journal} {Review of
  Scientific Instruments}\ }\textbf {\bibinfo {volume} {68}},\ \bibinfo {pages}
  {3277} (\bibinfo {year} {1997})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Kielpinski}\ \emph {et~al.}(2014)\citenamefont
  {Kielpinski}, \citenamefont {Sang},\ and\ \citenamefont
  {Litvinyuk}}]{kielpinski_benchmarking_2014}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.}~\bibnamefont
  {Kielpinski}}, \bibinfo {author} {\bibfnamefont {R.~T.}\ \bibnamefont
  {Sang}}, \ and\ \bibinfo {author} {\bibfnamefont {I.~V.}\ \bibnamefont
  {Litvinyuk}},\ }\href {\doibase 10.1088/0953-4075/47/20/204003} {\bibfield
  {journal} {\bibinfo  {journal} {Journal of Physics B}\ }\textbf {\bibinfo
  {volume} {47}},\ \bibinfo {pages} {204003} (\bibinfo {year}
  {2014})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Pullen}\ \emph {et~al.}(2013)\citenamefont {Pullen},
  \citenamefont {Wallace}, \citenamefont {Laban}, \citenamefont {Palmer},
  \citenamefont {Hanne}, \citenamefont {{Grum-Grzhimailo}}, \citenamefont
  {Bartschat}, \citenamefont {Ivanov}, \citenamefont {Kheifets}, \citenamefont
  {Wells}, \citenamefont {Quiney}, \citenamefont {Tong}, \citenamefont
  {Litvinyuk}, \citenamefont {Sang},\ and\ \citenamefont
  {Kielpinski}}]{pullen_measurement_2013}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.~G.}\ \bibnamefont
  {Pullen}}, \bibinfo {author} {\bibfnamefont {W.~C.}\ \bibnamefont {Wallace}},
  \bibinfo {author} {\bibfnamefont {D.~E.}\ \bibnamefont {Laban}}, \bibinfo
  {author} {\bibfnamefont {A.~J.}\ \bibnamefont {Palmer}}, \bibinfo {author}
  {\bibfnamefont {G.~F.}\ \bibnamefont {Hanne}}, \bibinfo {author}
  {\bibfnamefont {A.~N.}\ \bibnamefont {{Grum-Grzhimailo}}}, \bibinfo {author}
  {\bibfnamefont {K.}~\bibnamefont {Bartschat}}, \bibinfo {author}
  {\bibfnamefont {I.}~\bibnamefont {Ivanov}}, \bibinfo {author} {\bibfnamefont
  {A.}~\bibnamefont {Kheifets}}, \bibinfo {author} {\bibfnamefont
  {D.}~\bibnamefont {Wells}}, \bibinfo {author} {\bibfnamefont {H.~M.}\
  \bibnamefont {Quiney}}, \bibinfo {author} {\bibfnamefont {X.~M.}\
  \bibnamefont {Tong}}, \bibinfo {author} {\bibfnamefont {I.~V.}\ \bibnamefont
  {Litvinyuk}}, \bibinfo {author} {\bibfnamefont {R.~T.}\ \bibnamefont {Sang}},
  \ and\ \bibinfo {author} {\bibfnamefont {D.}~\bibnamefont {Kielpinski}},\
  }\href {\doibase 10.1103/PhysRevA.87.053411} {\bibfield  {journal} {\bibinfo
  {journal} {Physical Review A - Atomic, Molecular, and Optical Physics}\
  }\textbf {\bibinfo {volume} {87}},\ \bibinfo {pages} {053411} (\bibinfo
  {year} {2013})},\ \Eprint {http://arxiv.org/abs/cs/9605103}
  {arXiv:cs/9605103} \BibitemShut {NoStop}%
\bibitem [{\citenamefont {Maxwell}\ \emph {et~al.}(2021)\citenamefont
  {Maxwell}, \citenamefont {Serafini}, \citenamefont {Bose},\ and\
  \citenamefont {Faria}}]{maxwell_quantum_2021}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.~S.}\ \bibnamefont
  {Maxwell}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Serafini}},
  \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Bose}}, \ and\ \bibinfo
  {author} {\bibfnamefont {C.~F. d.~M.}\ \bibnamefont {Faria}},\ }\href
  {\doibase 10.1103/PhysRevA.103.043519} {\bibfield  {journal} {\bibinfo
  {journal} {Physical Review A}\ }\textbf {\bibinfo {volume} {103}},\ \bibinfo
  {pages} {043519} (\bibinfo {year} {2021})},\ \bibinfo {note} {publisher:
  American Physical Society}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Bukharskii}\ \emph {et~al.}(2023)\citenamefont
  {Bukharskii}, \citenamefont {Vais}, \citenamefont {Korneev},\ and\
  \citenamefont {Bychenkov}}]{bukharskii_restoration_2023}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {N.~D.}\ \bibnamefont
  {Bukharskii}}, \bibinfo {author} {\bibfnamefont {O.~E.}\ \bibnamefont
  {Vais}}, \bibinfo {author} {\bibfnamefont {P.~A.}\ \bibnamefont {Korneev}}, \
  and\ \bibinfo {author} {\bibfnamefont {V.~Y.}\ \bibnamefont {Bychenkov}},\
  }\href {\doibase 10.1063/5.0126571} {\bibfield  {journal} {\bibinfo
  {journal} {Matter and Radiation at Extremes}\ }\textbf {\bibinfo {volume}
  {8}},\ \bibinfo {pages} {014404} (\bibinfo {year} {2023})},\ \bibinfo {note}
  {publisher: American Institute of Physics}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {{Sanchez-Gonzalez}}\ \emph
  {et~al.}(2017)\citenamefont {{Sanchez-Gonzalez}}, \citenamefont {Micaelli},
  \citenamefont {Olivier}, \citenamefont {Barillot}, \citenamefont {Ilchen},
  \citenamefont {Lutman}, \citenamefont {Marinelli}, \citenamefont {Maxwell},
  \citenamefont {Achner}, \citenamefont {Ag{\aa}ker}, \citenamefont {Berrah},
  \citenamefont {Bostedt}, \citenamefont {Bozek}, \citenamefont {Buck},
  \citenamefont {Bucksbaum}, \citenamefont {Montero}, \citenamefont {Cooper},
  \citenamefont {Cryan}, \citenamefont {Dong}, \citenamefont {Feifel},
  \citenamefont {Frasinski}, \citenamefont {Fukuzawa}, \citenamefont {Galler},
  \citenamefont {Hartmann}, \citenamefont {Hartmann}, \citenamefont {Helml},
  \citenamefont {Johnson}, \citenamefont {Knie}, \citenamefont {Lindahl},
  \citenamefont {Liu}, \citenamefont {Motomura}, \citenamefont {Mucke},
  \citenamefont {O'Grady}, \citenamefont {Rubensson}, \citenamefont {Simpson},
  \citenamefont {Squibb}, \citenamefont {S{\aa}the}, \citenamefont {Ueda},
  \citenamefont {Vacher}, \citenamefont {Walke}, \citenamefont {Zhaunerchyk},
  \citenamefont {Coffee},\ and\ \citenamefont
  {Marangos}}]{sanchez-gonzalez_accurate_2017}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {{Sanchez-Gonzalez}}}, \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont
  {Micaelli}}, \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Olivier}},
  \bibinfo {author} {\bibfnamefont {T.~R.}\ \bibnamefont {Barillot}}, \bibinfo
  {author} {\bibfnamefont {M.}~\bibnamefont {Ilchen}}, \bibinfo {author}
  {\bibfnamefont {A.~A.}\ \bibnamefont {Lutman}}, \bibinfo {author}
  {\bibfnamefont {A.}~\bibnamefont {Marinelli}}, \bibinfo {author}
  {\bibfnamefont {T.}~\bibnamefont {Maxwell}}, \bibinfo {author} {\bibfnamefont
  {A.}~\bibnamefont {Achner}}, \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Ag{\aa}ker}}, \bibinfo {author} {\bibfnamefont
  {N.}~\bibnamefont {Berrah}}, \bibinfo {author} {\bibfnamefont
  {C.}~\bibnamefont {Bostedt}}, \bibinfo {author} {\bibfnamefont {J.~D.}\
  \bibnamefont {Bozek}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Buck}}, \bibinfo {author} {\bibfnamefont {P.~H.}\ \bibnamefont {Bucksbaum}},
  \bibinfo {author} {\bibfnamefont {S.~C.}\ \bibnamefont {Montero}}, \bibinfo
  {author} {\bibfnamefont {B.}~\bibnamefont {Cooper}}, \bibinfo {author}
  {\bibfnamefont {J.~P.}\ \bibnamefont {Cryan}}, \bibinfo {author}
  {\bibfnamefont {M.}~\bibnamefont {Dong}}, \bibinfo {author} {\bibfnamefont
  {R.}~\bibnamefont {Feifel}}, \bibinfo {author} {\bibfnamefont {L.~J.}\
  \bibnamefont {Frasinski}}, \bibinfo {author} {\bibfnamefont {H.}~\bibnamefont
  {Fukuzawa}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Galler}},
  \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Hartmann}}, \bibinfo
  {author} {\bibfnamefont {N.}~\bibnamefont {Hartmann}}, \bibinfo {author}
  {\bibfnamefont {W.}~\bibnamefont {Helml}}, \bibinfo {author} {\bibfnamefont
  {A.~S.}\ \bibnamefont {Johnson}}, \bibinfo {author} {\bibfnamefont
  {A.}~\bibnamefont {Knie}}, \bibinfo {author} {\bibfnamefont {A.~O.}\
  \bibnamefont {Lindahl}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Liu}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Motomura}},
  \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Mucke}}, \bibinfo
  {author} {\bibfnamefont {C.}~\bibnamefont {O'Grady}}, \bibinfo {author}
  {\bibfnamefont {J.-E.}\ \bibnamefont {Rubensson}}, \bibinfo {author}
  {\bibfnamefont {E.~R.}\ \bibnamefont {Simpson}}, \bibinfo {author}
  {\bibfnamefont {R.~J.}\ \bibnamefont {Squibb}}, \bibinfo {author}
  {\bibfnamefont {C.}~\bibnamefont {S{\aa}the}}, \bibinfo {author}
  {\bibfnamefont {K.}~\bibnamefont {Ueda}}, \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Vacher}}, \bibinfo {author} {\bibfnamefont {D.~J.}\
  \bibnamefont {Walke}}, \bibinfo {author} {\bibfnamefont {V.}~\bibnamefont
  {Zhaunerchyk}}, \bibinfo {author} {\bibfnamefont {R.~N.}\ \bibnamefont
  {Coffee}}, \ and\ \bibinfo {author} {\bibfnamefont {J.~P.}\ \bibnamefont
  {Marangos}},\ }\href {\doibase 10.1038/ncomms15461} {\bibfield  {journal}
  {\bibinfo  {journal} {Nature Communications}\ }\textbf {\bibinfo {volume}
  {8}},\ \bibinfo {pages} {15461} (\bibinfo {year} {2017})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Ren}\ \emph {et~al.}(2020)\citenamefont {Ren},
  \citenamefont {Edelen}, \citenamefont {Lutman}, \citenamefont {Marcus},
  \citenamefont {Maxwell},\ and\ \citenamefont {Ratner}}]{ren_temporal_2020}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {X.}~\bibnamefont
  {Ren}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Edelen}},
  \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Lutman}}, \bibinfo
  {author} {\bibfnamefont {G.}~\bibnamefont {Marcus}}, \bibinfo {author}
  {\bibfnamefont {T.}~\bibnamefont {Maxwell}}, \ and\ \bibinfo {author}
  {\bibfnamefont {D.}~\bibnamefont {Ratner}},\ }\href {\doibase
  10.1103/PhysRevAccelBeams.23.040701} {\bibfield  {journal} {\bibinfo
  {journal} {Physical Review Accelerators and Beams}\ }\textbf {\bibinfo
  {volume} {23}},\ \bibinfo {pages} {040701} (\bibinfo {year}
  {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Kolesnichenko}\ and\ \citenamefont
  {Zigmantas}(2023)}]{kolesnichenko_neuralnetworkpowered_2023}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {P.~V.}\ \bibnamefont
  {Kolesnichenko}}\ and\ \bibinfo {author} {\bibfnamefont {D.}~\bibnamefont
  {Zigmantas}},\ }\href {\doibase 10.1364/OE.479638} {\bibfield  {journal}
  {\bibinfo  {journal} {Opt. Exp.}\ }\textbf {\bibinfo {volume} {31}},\
  \bibinfo {pages} {11806} (\bibinfo {year} {2023})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Geffert}\ \emph {et~al.}(2022)\citenamefont
  {Geffert}, \citenamefont {Kolbasova}, \citenamefont {Trabattoni},
  \citenamefont {Calegari},\ and\ \citenamefont {Santra}}]{geffert_situ_2022}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {O.}~\bibnamefont
  {Geffert}}, \bibinfo {author} {\bibfnamefont {D.}~\bibnamefont {Kolbasova}},
  \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Trabattoni}}, \bibinfo
  {author} {\bibfnamefont {F.}~\bibnamefont {Calegari}}, \ and\ \bibinfo
  {author} {\bibfnamefont {R.}~\bibnamefont {Santra}},\ }\href {\doibase
  10.1364/OL.460513} {\bibfield  {journal} {\bibinfo  {journal} {Optics
  Letters}\ }\textbf {\bibinfo {volume} {47}},\ \bibinfo {pages} {3992}
  (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Tulsky}\ and\ \citenamefont
  {Bauer}(2019)}]{tulsky_qprop_2019}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {V.}~\bibnamefont
  {Tulsky}}\ and\ \bibinfo {author} {\bibfnamefont {D.}~\bibnamefont {Bauer}},\
  }\href {\doibase 10.1016/j.cpc.2019.107098} {\bibfield  {journal} {\bibinfo
  {journal} {Computer Physics Communications}\ }\textbf {\bibinfo {volume}
  {251}},\ \bibinfo {pages} {107098} (\bibinfo {year} {2019})},\ \bibinfo
  {note} {arXiv: 1907.08595}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Tong}\ and\ \citenamefont
  {Lin}(2005)}]{tong_empirical_2005}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {X.~M.}\ \bibnamefont
  {Tong}}\ and\ \bibinfo {author} {\bibfnamefont {C.~D.}\ \bibnamefont {Lin}},\
  }\href {\doibase 10.1088/0953-4075/38/15/001} {\bibfield  {journal} {\bibinfo
   {journal} {Journal of Physics B: Atomic, Molecular and Optical Physics}\
  }\textbf {\bibinfo {volume} {38}},\ \bibinfo {pages} {2593} (\bibinfo {year}
  {2005})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Amini}\ \emph {et~al.}(2019)\citenamefont {Amini},
  \citenamefont {Biegert}, \citenamefont {Calegari}, \citenamefont {Chacón},
  \citenamefont {Ciappina}, \citenamefont {Dauphin}, \citenamefont {Efimov},
  \citenamefont {Figueira~de Morisson~Faria}, \citenamefont {Giergiel},
  \citenamefont {Gniewek}, \citenamefont {Landsman}, \citenamefont {Lesiuk},
  \citenamefont {Mandrysz}, \citenamefont {Maxwell}, \citenamefont
  {Moszyński}, \citenamefont {Ortmann}, \citenamefont {Pérez-Hernández},
  \citenamefont {Picón}, \citenamefont {Pisanty}, \citenamefont
  {Prauzner-Bechcicki}, \citenamefont {Sacha}, \citenamefont {Suárez},
  \citenamefont {Zaïr}, \citenamefont {Zakrzewski},\ and\ \citenamefont
  {Lewenstein}}]{amini_symphony_2019}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {K.}~\bibnamefont
  {Amini}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Biegert}},
  \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Calegari}}, \bibinfo
  {author} {\bibfnamefont {A.}~\bibnamefont {Chacón}}, \bibinfo {author}
  {\bibfnamefont {M.~F.}\ \bibnamefont {Ciappina}}, \bibinfo {author}
  {\bibfnamefont {A.}~\bibnamefont {Dauphin}}, \bibinfo {author} {\bibfnamefont
  {D.~K.}\ \bibnamefont {Efimov}}, \bibinfo {author} {\bibfnamefont
  {C.}~\bibnamefont {Figueira~de Morisson~Faria}}, \bibinfo {author}
  {\bibfnamefont {K.}~\bibnamefont {Giergiel}}, \bibinfo {author}
  {\bibfnamefont {P.}~\bibnamefont {Gniewek}}, \bibinfo {author} {\bibfnamefont
  {A.~S.}\ \bibnamefont {Landsman}}, \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Lesiuk}}, \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Mandrysz}}, \bibinfo {author} {\bibfnamefont {A.~S.}\
  \bibnamefont {Maxwell}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont
  {Moszyński}}, \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Ortmann}},
  \bibinfo {author} {\bibfnamefont {J.~A.}\ \bibnamefont {Pérez-Hernández}},
  \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Picón}}, \bibinfo
  {author} {\bibfnamefont {E.}~\bibnamefont {Pisanty}}, \bibinfo {author}
  {\bibfnamefont {J.}~\bibnamefont {Prauzner-Bechcicki}}, \bibinfo {author}
  {\bibfnamefont {K.}~\bibnamefont {Sacha}}, \bibinfo {author} {\bibfnamefont
  {N.}~\bibnamefont {Suárez}}, \bibinfo {author} {\bibfnamefont
  {A.}~\bibnamefont {Zaïr}}, \bibinfo {author} {\bibfnamefont
  {J.}~\bibnamefont {Zakrzewski}}, \ and\ \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Lewenstein}},\ }\href {\doibase 10.1088/1361-6633/ab2bb1}
  {\bibfield  {journal} {\bibinfo  {journal} {Reports on Progress in Physics}\
  }\textbf {\bibinfo {volume} {82}},\ \bibinfo {pages} {116001} (\bibinfo
  {year} {2019})},\ \bibinfo {note} {publisher: IOP Publishing}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Becker}\ \emph {et~al.}(2002)\citenamefont {Becker},
  \citenamefont {Grasbon}, \citenamefont {Kopold}, \citenamefont {Milo{\v
  s}evi{\'c}}, \citenamefont {Paulus},\ and\ \citenamefont
  {Walther}}]{becker_abovethreshold_2002}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {W.}~\bibnamefont
  {Becker}}, \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Grasbon}},
  \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Kopold}}, \bibinfo
  {author} {\bibfnamefont {D.~B.}\ \bibnamefont {Milo{\v s}evi{\'c}}}, \bibinfo
  {author} {\bibfnamefont {G.~G.}\ \bibnamefont {Paulus}}, \ and\ \bibinfo
  {author} {\bibfnamefont {H.}~\bibnamefont {Walther}},\ }in\ \href {\doibase
  10.1016/S1049-250X(02)80006-4} {\emph {\bibinfo {booktitle} {Advances {{In
  Atomic}}, {{Molecular}}, and {{Optical Physics}}}}},\ Vol.~\bibinfo {volume}
  {48},\ \bibinfo {editor} {edited by\ \bibinfo {editor} {\bibfnamefont
  {B.}~\bibnamefont {Bederson}}\ and\ \bibinfo {editor} {\bibfnamefont
  {H.}~\bibnamefont {Walther}}}\ (\bibinfo  {publisher} {{Academic Press}},\
  \bibinfo {year} {2002})\ pp.\ \bibinfo {pages} {35--98}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Figueira~de Morisson~Faria}\ \emph
  {et~al.}(2002)\citenamefont {Figueira~de Morisson~Faria}, \citenamefont
  {Schomerus},\ and\ \citenamefont
  {Becker}}]{figueirademorissonfaria_highorder_2002}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {C.}~\bibnamefont
  {Figueira~de Morisson~Faria}}, \bibinfo {author} {\bibfnamefont
  {H.}~\bibnamefont {Schomerus}}, \ and\ \bibinfo {author} {\bibfnamefont
  {W.}~\bibnamefont {Becker}},\ }\href {\doibase 10.1103/PhysRevA.66.043413}
  {\bibfield  {journal} {\bibinfo  {journal} {Physical Review A}\ }\textbf
  {\bibinfo {volume} {66}},\ \bibinfo {pages} {043413} (\bibinfo {year}
  {2002})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Keldysh}(1965)}]{keldysh_ionization_1965}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {L.~V.}\ \bibnamefont
  {Keldysh}},\ }\href {\doibase 10.1234/12345678} {\bibfield  {journal}
  {\bibinfo  {journal} {Sov. Phys. JETP}\ }\textbf {\bibinfo {volume} {20}},\
  \bibinfo {pages} {1307} (\bibinfo {year} {1965})},\ \bibinfo {note} {iSBN:
  0038-5646}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Faisal}(1973)}]{faisal_multiple_1973}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {F.~H.}\ \bibnamefont
  {Faisal}},\ }\href {\doibase 10.1088/0022-3700/6/4/011} {\bibfield  {journal}
  {\bibinfo  {journal} {Journal of Physics B: Atomic and Molecular Physics}\
  }\textbf {\bibinfo {volume} {6}},\ \bibinfo {pages} {L89} (\bibinfo {year}
  {1973})},\ \bibinfo {note} {iSBN: 0953-4075}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Reiss}(1980)}]{reiss_effect_1980}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {H.~R.}\ \bibnamefont
  {Reiss}},\ }\href {\doibase 10.1103/PhysRevA.22.1786} {\bibfield  {journal}
  {\bibinfo  {journal} {Phys. Rev. A}\ }\textbf {\bibinfo {volume} {22}},\
  \bibinfo {pages} {1786} (\bibinfo {year} {1980})},\ \bibinfo {note} {iSBN:
  1050-2947}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Even}(2015)}]{even_even-lavie_2015}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {U.}~\bibnamefont
  {Even}},\ }\href {\doibase 10.1140/epjti/s40485-015-0027-5} {\bibfield
  {journal} {\bibinfo  {journal} {EPJ Techniques and Instrumentation}\ }\textbf
  {\bibinfo {volume} {2}},\ \bibinfo {pages} {17} (\bibinfo {year}
  {2015})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Roberts}\ \emph {et~al.}(2009)\citenamefont
  {Roberts}, \citenamefont {Nixon}, \citenamefont {Lecointre}, \citenamefont
  {Wrede},\ and\ \citenamefont {Verlet}}]{Roberts_2009}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {G.~M.}\ \bibnamefont
  {Roberts}}, \bibinfo {author} {\bibfnamefont {J.~L.}\ \bibnamefont {Nixon}},
  \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Lecointre}}, \bibinfo
  {author} {\bibfnamefont {E.}~\bibnamefont {Wrede}}, \ and\ \bibinfo {author}
  {\bibfnamefont {J.~R.~R.}\ \bibnamefont {Verlet}},\ }\href@noop {} {\bibfield
   {journal} {\bibinfo  {journal} {Rev. Sci. Instrum.}\ }\textbf {\bibinfo
  {volume} {80}},\ \bibinfo {pages} {053104} (\bibinfo {year}
  {2009})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Werby}\ \emph {et~al.}(2021)\citenamefont {Werby},
  \citenamefont {Maxwell}, \citenamefont {Forbes}, \citenamefont {Bucksbaum},\
  and\ \citenamefont {Faria}}]{werby_dissecting_2021}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {N.}~\bibnamefont
  {Werby}}, \bibinfo {author} {\bibfnamefont {A.~S.}\ \bibnamefont {Maxwell}},
  \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Forbes}}, \bibinfo
  {author} {\bibfnamefont {P.~H.}\ \bibnamefont {Bucksbaum}}, \ and\ \bibinfo
  {author} {\bibfnamefont {C.~F. d.~M.}\ \bibnamefont {Faria}},\ }\href
  {\doibase 10.1103/PhysRevA.104.013109} {\bibfield  {journal} {\bibinfo
  {journal} {Physical Review A}\ }\textbf {\bibinfo {volume} {104}},\ \bibinfo
  {pages} {013109} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Maxwell}\ \emph {et~al.}(2017)\citenamefont
  {Maxwell}, \citenamefont {{Al-Jawahiry}}, \citenamefont {Das},\ and\
  \citenamefont {{Figueira de Morisson
  Faria}}}]{maxwell_coulombcorrected_2017}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.~S.}\ \bibnamefont
  {Maxwell}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {{Al-Jawahiry}}}, \bibinfo {author} {\bibfnamefont {T.}~\bibnamefont {Das}},
  \ and\ \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {{Figueira de
  Morisson Faria}}},\ }\href {\doibase 10.1103/PhysRevA.96.023420} {\bibfield
  {journal} {\bibinfo  {journal} {Phys. Rev. A}\ }\textbf {\bibinfo {volume}
  {96}},\ \bibinfo {pages} {023420} (\bibinfo {year} {2017})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Maxwell}\ \emph {et~al.}(2018)\citenamefont
  {Maxwell}, \citenamefont {{Al-Jawahiry}}, \citenamefont {Lai},\ and\
  \citenamefont {Faria}}]{maxwell_analytic_2018}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.~S.}\ \bibnamefont
  {Maxwell}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {{Al-Jawahiry}}}, \bibinfo {author} {\bibfnamefont {X.~Y.}\ \bibnamefont
  {Lai}}, \ and\ \bibinfo {author} {\bibfnamefont {C.~F. d.~M.}\ \bibnamefont
  {Faria}},\ }\href {\doibase 10.1088/1361-6455/aa9e81} {\bibfield  {journal}
  {\bibinfo  {journal} {Journal of Physics B: Atomic, Molecular and Optical
  Physics}\ }\textbf {\bibinfo {volume} {51}},\ \bibinfo {pages} {044004}
  (\bibinfo {year} {2018})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Maxwell}\ and\ \citenamefont {Figueira~de
  MorissonFaria}(2018)}]{maxwell_coulombfree_2018}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.~S.}\ \bibnamefont
  {Maxwell}}\ and\ \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont
  {Figueira~de MorissonFaria}},\ }\href {\doibase 10.1088/1361-6455/aac164}
  {\bibfield  {journal} {\bibinfo  {journal} {Journal of Physics B: Atomic,
  Molecular and Optical Physics}\ }\textbf {\bibinfo {volume} {51}},\ \bibinfo
  {pages} {124001} (\bibinfo {year} {2018})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {LeCun}\ \emph {et~al.}(1989)\citenamefont {LeCun},
  \citenamefont {Boser}, \citenamefont {Denker}, \citenamefont {Henderson},
  \citenamefont {Howard}, \citenamefont {Hubbard},\ and\ \citenamefont
  {Jackel}}]{lecun_backpropagation_1989}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont
  {LeCun}}, \bibinfo {author} {\bibfnamefont {B.}~\bibnamefont {Boser}},
  \bibinfo {author} {\bibfnamefont {J.~S.}\ \bibnamefont {Denker}}, \bibinfo
  {author} {\bibfnamefont {D.}~\bibnamefont {Henderson}}, \bibinfo {author}
  {\bibfnamefont {R.~E.}\ \bibnamefont {Howard}}, \bibinfo {author}
  {\bibfnamefont {W.}~\bibnamefont {Hubbard}}, \ and\ \bibinfo {author}
  {\bibfnamefont {L.~D.}\ \bibnamefont {Jackel}},\ }\href {\doibase
  10.1162/neco.1989.1.4.541} {\bibfield  {journal} {\bibinfo  {journal} {Neural
  Computation}\ }\textbf {\bibinfo {volume} {1}},\ \bibinfo {pages} {541}
  (\bibinfo {year} {1989})},\ \bibinfo {note} {conference Name: Neural
  Computation}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Krizhevsky}\ \emph {et~al.}(2012)\citenamefont
  {Krizhevsky}, \citenamefont {Sutskever},\ and\ \citenamefont
  {Hinton}}]{krizhevsky_imagenet_2012}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {Krizhevsky}}, \bibinfo {author} {\bibfnamefont {I.}~\bibnamefont
  {Sutskever}}, \ and\ \bibinfo {author} {\bibfnamefont {G.~E.}\ \bibnamefont
  {Hinton}},\ }in\ \href
  {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf}
  {\emph {\bibinfo {booktitle} {Advances in Neural Information Processing
  Systems}}},\ Vol.~\bibinfo {volume} {25},\ \bibinfo {editor} {edited by\
  \bibinfo {editor} {\bibfnamefont {F.}~\bibnamefont {Pereira}}, \bibinfo
  {editor} {\bibfnamefont {C.}~\bibnamefont {Burges}}, \bibinfo {editor}
  {\bibfnamefont {L.}~\bibnamefont {Bottou}}, \ and\ \bibinfo {editor}
  {\bibfnamefont {K.}~\bibnamefont {Weinberger}}}\ (\bibinfo  {publisher}
  {Curran Associates, Inc.},\ \bibinfo {year} {2012})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Russakovsky}\ \emph {et~al.}(2014)\citenamefont
  {Russakovsky}, \citenamefont {Deng}, \citenamefont {Su}, \citenamefont
  {Krause}, \citenamefont {Satheesh}, \citenamefont {Ma}, \citenamefont
  {Huang}, \citenamefont {Karpathy}, \citenamefont {Khosla}, \citenamefont
  {Bernstein}, \citenamefont {Berg},\ and\ \citenamefont
  {Fei-Fei}}]{russakovsky_imagenet_2014}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {O.}~\bibnamefont
  {Russakovsky}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Deng}},
  \bibinfo {author} {\bibfnamefont {H.}~\bibnamefont {Su}}, \bibinfo {author}
  {\bibfnamefont {J.}~\bibnamefont {Krause}}, \bibinfo {author} {\bibfnamefont
  {S.}~\bibnamefont {Satheesh}}, \bibinfo {author} {\bibfnamefont
  {S.}~\bibnamefont {Ma}}, \bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont
  {Huang}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Karpathy}},
  \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Khosla}}, \bibinfo
  {author} {\bibfnamefont {M.}~\bibnamefont {Bernstein}}, \bibinfo {author}
  {\bibfnamefont {A.~C.}\ \bibnamefont {Berg}}, \ and\ \bibinfo {author}
  {\bibfnamefont {L.}~\bibnamefont {Fei-Fei}},\ }\href {\doibase
  10.48550/ARXIV.1409.0575} {\enquote {\bibinfo {title} {Imagenet large scale
  visual recognition challenge},}\ } (\bibinfo {year} {2014})\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Chen}\ \emph {et~al.}(2023)\citenamefont {Chen},
  \citenamefont {Liang}, \citenamefont {Huang}, \citenamefont {Real},
  \citenamefont {Wang}, \citenamefont {Liu}, \citenamefont {Pham},
  \citenamefont {Dong}, \citenamefont {Luong}, \citenamefont {Hsieh},
  \citenamefont {Lu},\ and\ \citenamefont {Le}}]{chen_symbolic_2023}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {X.}~\bibnamefont
  {Chen}}, \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Liang}},
  \bibinfo {author} {\bibfnamefont {D.}~\bibnamefont {Huang}}, \bibinfo
  {author} {\bibfnamefont {E.}~\bibnamefont {Real}}, \bibinfo {author}
  {\bibfnamefont {K.}~\bibnamefont {Wang}}, \bibinfo {author} {\bibfnamefont
  {Y.}~\bibnamefont {Liu}}, \bibinfo {author} {\bibfnamefont {H.}~\bibnamefont
  {Pham}}, \bibinfo {author} {\bibfnamefont {X.}~\bibnamefont {Dong}}, \bibinfo
  {author} {\bibfnamefont {T.}~\bibnamefont {Luong}}, \bibinfo {author}
  {\bibfnamefont {C.-J.}\ \bibnamefont {Hsieh}}, \bibinfo {author}
  {\bibfnamefont {Y.}~\bibnamefont {Lu}}, \ and\ \bibinfo {author}
  {\bibfnamefont {Q.~V.}\ \bibnamefont {Le}},\ }\href {\doibase
  10.48550/ARXIV.2302.06675} {\enquote {\bibinfo {title} {Symbolic discovery of
  optimization algorithms},}\ } (\bibinfo {year} {2023})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Tan}\ \emph {et~al.}(2018)\citenamefont {Tan},
  \citenamefont {Sun}, \citenamefont {Kong}, \citenamefont {Zhang},
  \citenamefont {Yang},\ and\ \citenamefont {Liu}}]{tan_survey_2018}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {C.}~\bibnamefont
  {Tan}}, \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Sun}}, \bibinfo
  {author} {\bibfnamefont {T.}~\bibnamefont {Kong}}, \bibinfo {author}
  {\bibfnamefont {W.}~\bibnamefont {Zhang}}, \bibinfo {author} {\bibfnamefont
  {C.}~\bibnamefont {Yang}}, \ and\ \bibinfo {author} {\bibfnamefont
  {C.}~\bibnamefont {Liu}},\ }\href {\doibase 10.48550/ARXIV.1808.01974}
  {\enquote {\bibinfo {title} {A survey on deep transfer learning},}\ }
  (\bibinfo {year} {2018})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Plested}\ and\ \citenamefont
  {Gedeon}(2022)}]{plested_deep_2022}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Plested}}\ and\ \bibinfo {author} {\bibfnamefont {T.}~\bibnamefont
  {Gedeon}},\ }\href {\doibase 10.48550/ARXIV.2205.09904} {\enquote {\bibinfo
  {title} {Deep transfer learning for image classification: a survey},}\ }
  (\bibinfo {year} {2022})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Simonyan}\ and\ \citenamefont
  {Zisserman}(2015)}]{simonyan_very_2015}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {K.}~\bibnamefont
  {Simonyan}}\ and\ \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {Zisserman}},\ }\href {http://arxiv.org/abs/1409.1556} {\enquote {\bibinfo
  {title} {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image}
  {Recognition}},}\ } (\bibinfo {year} {2015}),\ \bibinfo {note}
  {arXiv:1409.1556 [cs]}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Chollet}(2017)}]{chollet_xception_2017}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {F.}~\bibnamefont
  {Chollet}},\ }\href {http://arxiv.org/abs/1610.02357} {\enquote {\bibinfo
  {title} {Xception: {Deep} {Learning} with {Depthwise} {Separable}
  {Convolutions}},}\ } (\bibinfo {year} {2017}),\ \bibinfo {note}
  {arXiv:1610.02357 [cs]}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Tan}\ and\ \citenamefont
  {Le}(2020)}]{tan_efficientnet_2020}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Tan}}\ and\ \bibinfo {author} {\bibfnamefont {Q.~V.}\ \bibnamefont {Le}},\
  }\href {http://arxiv.org/abs/1905.11946} {\enquote {\bibinfo {title}
  {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural}
  {Networks}},}\ } (\bibinfo {year} {2020}),\ \bibinfo {note} {arXiv:1905.11946
  [cs, stat]}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Tan}\ and\ \citenamefont
  {Le}(2021)}]{tan_efficientnetv2_2021}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Tan}}\ and\ \bibinfo {author} {\bibfnamefont {Q.~V.}\ \bibnamefont {Le}},\
  }\href {http://arxiv.org/abs/2104.00298} {\enquote {\bibinfo {title}
  {{EfficientNetV2}: {Smaller} {Models} and {Faster} {Training}},}\ } (\bibinfo
  {year} {2021}),\ \bibinfo {note} {arXiv:2104.00298 [cs]}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Chollet}\ \emph {et~al.}(2015)\citenamefont {Chollet}
  \emph {et~al.}}]{chollet_keras_2015}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {F.}~\bibnamefont
  {Chollet}} \emph {et~al.},\ }\href@noop {} {\enquote {\bibinfo {title}
  {Keras},}\ }\bibinfo {howpublished} {\url{https://keras.io}} (\bibinfo {year}
  {2015})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Iqbal}(2018)}]{iqbal_plotneuralnet_2018}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {H.}~\bibnamefont
  {Iqbal}},\ }\href {\doibase 10.5281/zenodo.2526396} {\enquote {\bibinfo
  {title} {Harisiqbal88/plotneuralnet v1.0.0},}\ } (\bibinfo {year}
  {2018})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Gawlikowski}\ \emph {et~al.}(2022)\citenamefont
  {Gawlikowski}, \citenamefont {Tassi}, \citenamefont {Ali}, \citenamefont
  {Lee}, \citenamefont {Humt}, \citenamefont {Feng}, \citenamefont {Kruspe},
  \citenamefont {Triebel}, \citenamefont {Jung}, \citenamefont {Roscher},
  \citenamefont {Shahzad}, \citenamefont {Yang}, \citenamefont {Bamler},\ and\
  \citenamefont {Zhu}}]{gawlikowski_survey_2022}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Gawlikowski}}, \bibinfo {author} {\bibfnamefont {C.~R.~N.}\ \bibnamefont
  {Tassi}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Ali}}, \bibinfo
  {author} {\bibfnamefont {J.}~\bibnamefont {Lee}}, \bibinfo {author}
  {\bibfnamefont {M.}~\bibnamefont {Humt}}, \bibinfo {author} {\bibfnamefont
  {J.}~\bibnamefont {Feng}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {Kruspe}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Triebel}},
  \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Jung}}, \bibinfo {author}
  {\bibfnamefont {R.}~\bibnamefont {Roscher}}, \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Shahzad}}, \bibinfo {author} {\bibfnamefont
  {W.}~\bibnamefont {Yang}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont
  {Bamler}}, \ and\ \bibinfo {author} {\bibfnamefont {X.~X.}\ \bibnamefont
  {Zhu}},\ }\href {http://arxiv.org/abs/2107.03342} {\enquote {\bibinfo {title}
  {A {Survey} of {Uncertainty} in {Deep} {Neural} {Networks}},}\ } (\bibinfo
  {year} {2022}),\ \bibinfo {note} {arXiv:2107.03342 [cs, stat]}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Abdar}\ \emph {et~al.}(2021)\citenamefont {Abdar},
  \citenamefont {Pourpanah}, \citenamefont {Hussain}, \citenamefont
  {Rezazadegan}, \citenamefont {Liu}, \citenamefont {Ghavamzadeh},
  \citenamefont {Fieguth}, \citenamefont {Cao}, \citenamefont {Khosravi},
  \citenamefont {Acharya}, \citenamefont {Makarenkov},\ and\ \citenamefont
  {Nahavandi}}]{abdar_review_2021}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Abdar}}, \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Pourpanah}},
  \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Hussain}}, \bibinfo
  {author} {\bibfnamefont {D.}~\bibnamefont {Rezazadegan}}, \bibinfo {author}
  {\bibfnamefont {L.}~\bibnamefont {Liu}}, \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Ghavamzadeh}}, \bibinfo {author} {\bibfnamefont
  {P.}~\bibnamefont {Fieguth}}, \bibinfo {author} {\bibfnamefont
  {X.}~\bibnamefont {Cao}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {Khosravi}}, \bibinfo {author} {\bibfnamefont {U.~R.}\ \bibnamefont
  {Acharya}}, \bibinfo {author} {\bibfnamefont {V.}~\bibnamefont {Makarenkov}},
  \ and\ \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Nahavandi}},\
  }\href {\doibase 10.1016/j.inffus.2021.05.008} {\bibfield  {journal}
  {\bibinfo  {journal} {Information Fusion}\ }\textbf {\bibinfo {volume}
  {76}},\ \bibinfo {pages} {243} (\bibinfo {year} {2021})},\ \bibinfo {note}
  {arXiv:2011.06225 [cs]}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Nix}\ and\ \citenamefont
  {Weigend}(1994)}]{nix_estimating_1994}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.}~\bibnamefont
  {Nix}}\ and\ \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Weigend}},\
  }in\ \href {\doibase 10.1109/ICNN.1994.374138} {\emph {\bibinfo {booktitle}
  {Proceedings of 1994 IEEE International Conference on Neural Networks
  (ICNN'94)}}},\ Vol.~\bibinfo {volume} {1}\ (\bibinfo {year} {1994})\ pp.\
  \bibinfo {pages} {55--60 vol.1}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Lakshminarayanan}\ \emph {et~al.}(2017)\citenamefont
  {Lakshminarayanan}, \citenamefont {Pritzel},\ and\ \citenamefont
  {Blundell}}]{lakshminarayanan_simple_2017}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {B.}~\bibnamefont
  {Lakshminarayanan}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {Pritzel}}, \ and\ \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont
  {Blundell}},\ }\href {http://arxiv.org/abs/1612.01474} {\enquote {\bibinfo
  {title} {Simple and {Scalable} {Predictive} {Uncertainty} {Estimation} using
  {Deep} {Ensembles}},}\ } (\bibinfo {year} {2017}),\ \bibinfo {note}
  {arXiv:1612.01474 [cs, stat]}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Ovadia}\ \emph {et~al.}(2019)\citenamefont {Ovadia},
  \citenamefont {Fertig}, \citenamefont {Ren}, \citenamefont {Nado},
  \citenamefont {Sculley}, \citenamefont {Nowozin}, \citenamefont {Dillon},
  \citenamefont {Lakshminarayanan},\ and\ \citenamefont
  {Snoek}}]{ovadia_can_2019}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont
  {Ovadia}}, \bibinfo {author} {\bibfnamefont {E.}~\bibnamefont {Fertig}},
  \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Ren}}, \bibinfo {author}
  {\bibfnamefont {Z.}~\bibnamefont {Nado}}, \bibinfo {author} {\bibfnamefont
  {D.}~\bibnamefont {Sculley}}, \bibinfo {author} {\bibfnamefont
  {S.}~\bibnamefont {Nowozin}}, \bibinfo {author} {\bibfnamefont {J.~V.}\
  \bibnamefont {Dillon}}, \bibinfo {author} {\bibfnamefont {B.}~\bibnamefont
  {Lakshminarayanan}}, \ and\ \bibinfo {author} {\bibfnamefont
  {J.}~\bibnamefont {Snoek}},\ }\href {http://arxiv.org/abs/1906.02530}
  {\enquote {\bibinfo {title} {Can {You} {Trust} {Your} {Model}'s
  {Uncertainty}? {Evaluating} {Predictive} {Uncertainty} {Under} {Dataset}
  {Shift}},}\ } (\bibinfo {year} {2019}),\ \bibinfo {note} {arXiv:1906.02530
  [cs, stat]}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Zimmermann}\ \emph {et~al.}(2019)\citenamefont
  {Zimmermann}, \citenamefont {Langbehn}, \citenamefont {Cucini}, \citenamefont
  {Di~Fraia}, \citenamefont {Finetti}, \citenamefont {LaForge}, \citenamefont
  {Nishiyama}, \citenamefont {Ovcharenko}, \citenamefont {Piseri},
  \citenamefont {Plekan}, \citenamefont {Prince}, \citenamefont {Stienkemeier},
  \citenamefont {Ueda}, \citenamefont {Callegari}, \citenamefont {Möller},\
  and\ \citenamefont {Rupp}}]{zimmermann_deep_2019}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Zimmermann}}, \bibinfo {author} {\bibfnamefont {B.}~\bibnamefont
  {Langbehn}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Cucini}},
  \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Di~Fraia}}, \bibinfo
  {author} {\bibfnamefont {P.}~\bibnamefont {Finetti}}, \bibinfo {author}
  {\bibfnamefont {A.~C.}\ \bibnamefont {LaForge}}, \bibinfo {author}
  {\bibfnamefont {T.}~\bibnamefont {Nishiyama}}, \bibinfo {author}
  {\bibfnamefont {Y.}~\bibnamefont {Ovcharenko}}, \bibinfo {author}
  {\bibfnamefont {P.}~\bibnamefont {Piseri}}, \bibinfo {author} {\bibfnamefont
  {O.}~\bibnamefont {Plekan}}, \bibinfo {author} {\bibfnamefont {K.~C.}\
  \bibnamefont {Prince}}, \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont
  {Stienkemeier}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Ueda}},
  \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Callegari}}, \bibinfo
  {author} {\bibfnamefont {T.}~\bibnamefont {Möller}}, \ and\ \bibinfo
  {author} {\bibfnamefont {D.}~\bibnamefont {Rupp}},\ }\href {\doibase
  10.1103/PhysRevE.99.063309} {\bibfield  {journal} {\bibinfo  {journal}
  {Physical Review E}\ }\textbf {\bibinfo {volume} {99}},\ \bibinfo {pages}
  {063309} (\bibinfo {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Shorten}\ and\ \citenamefont
  {Khoshgoftaar}(2019)}]{shorten_survey_2019}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {C.}~\bibnamefont
  {Shorten}}\ and\ \bibinfo {author} {\bibfnamefont {T.~M.}\ \bibnamefont
  {Khoshgoftaar}},\ }\href {\doibase 10.1186/s40537-019-0197-0} {\bibfield
  {journal} {\bibinfo  {journal} {Journal of Big Data}\ }\textbf {\bibinfo
  {volume} {6}},\ \bibinfo {pages} {60} (\bibinfo {year} {2019})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Kingma}\ and\ \citenamefont
  {Ba}(2014)}]{kingma_adam_2014}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.~P.}\ \bibnamefont
  {Kingma}}\ and\ \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Ba}},\
  }\href {\doibase 10.48550/ARXIV.1412.6980} {\enquote {\bibinfo {title} {Adam:
  A method for stochastic optimization},}\ } (\bibinfo {year}
  {2014})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Sluijterman}\ \emph {et~al.}(2023)\citenamefont
  {Sluijterman}, \citenamefont {Cator},\ and\ \citenamefont
  {Heskes}}]{sluijterman_optimal_2023}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {L.}~\bibnamefont
  {Sluijterman}}, \bibinfo {author} {\bibfnamefont {E.}~\bibnamefont {Cator}},
  \ and\ \bibinfo {author} {\bibfnamefont {T.}~\bibnamefont {Heskes}},\ }\href
  {\doibase 10.48550/ARXIV.2302.08875} {\enquote {\bibinfo {title} {Optimal
  training of mean variance estimation neural networks},}\ } (\bibinfo {year}
  {2023})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Linardatos}\ \emph {et~al.}(2020)\citenamefont
  {Linardatos}, \citenamefont {Papastefanopoulos},\ and\ \citenamefont
  {Kotsiantis}}]{linardatos_explainable_2020}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {P.}~\bibnamefont
  {Linardatos}}, \bibinfo {author} {\bibfnamefont {V.}~\bibnamefont
  {Papastefanopoulos}}, \ and\ \bibinfo {author} {\bibfnamefont
  {S.}~\bibnamefont {Kotsiantis}},\ }\href {\doibase 10.3390/e23010018}
  {\bibfield  {journal} {\bibinfo  {journal} {Entropy}\ }\textbf {\bibinfo
  {volume} {23}},\ \bibinfo {pages} {18} (\bibinfo {year} {2020})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Mohseni}\ \emph {et~al.}(2021)\citenamefont
  {Mohseni}, \citenamefont {Zarei},\ and\ \citenamefont
  {Ragan}}]{mohseni_multidisciplinary_2021}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.}~\bibnamefont
  {Mohseni}}, \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {Zarei}}, \
  and\ \bibinfo {author} {\bibfnamefont {E.~D.}\ \bibnamefont {Ragan}},\ }\href
  {\doibase 10.1145/3387166} {\bibfield  {journal} {\bibinfo  {journal} {ACM
  Transactions on Interactive Intelligent Systems}\ }\textbf {\bibinfo {volume}
  {11}},\ \bibinfo {pages} {1} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Letzgus}\ \emph {et~al.}(2021)\citenamefont
  {Letzgus}, \citenamefont {Wagner}, \citenamefont {Lederer}, \citenamefont
  {Samek}, \citenamefont {Müller},\ and\ \citenamefont
  {Montavon}}]{letzgus_toward_2021}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.}~\bibnamefont
  {Letzgus}}, \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Wagner}},
  \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Lederer}}, \bibinfo
  {author} {\bibfnamefont {W.}~\bibnamefont {Samek}}, \bibinfo {author}
  {\bibfnamefont {K.-R.}\ \bibnamefont {Müller}}, \ and\ \bibinfo {author}
  {\bibfnamefont {G.}~\bibnamefont {Montavon}},\ }\href
  {http://arxiv.org/abs/2112.11407} {\enquote {\bibinfo {title} {Toward
  {Explainable} {AI} for {Regression} {Models}},}\ } (\bibinfo {year} {2021}),\
  \bibinfo {note} {arXiv:2112.11407 [cs, stat]}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Zhang}\ \emph {et~al.}(2020)\citenamefont {Zhang},
  \citenamefont {Petitjean}, \citenamefont {Yger},\ and\ \citenamefont
  {Ainouz}}]{zhang_explainability_2020}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Zhang}}, \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Petitjean}},
  \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Yger}}, \ and\ \bibinfo
  {author} {\bibfnamefont {S.}~\bibnamefont {Ainouz}},\ }in\ \href {\doibase
  10.1007/978-3-030-61166-8_8} {\emph {\bibinfo {booktitle} {Workshop on
  {Interpretability} of {Machine} {Intelligence} in {Medical} {Image}
  {Computing} at {MICCAI} 2020}}}\ (\bibinfo {address} {Lima, Peru},\ \bibinfo
  {year} {2020})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Covert}\ \emph {et~al.}(2021)\citenamefont {Covert},
  \citenamefont {Lundberg},\ and\ \citenamefont
  {Lee}}]{covert_explaining_2021}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {I.}~\bibnamefont
  {Covert}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Lundberg}}, \
  and\ \bibinfo {author} {\bibfnamefont {S.-I.}\ \bibnamefont {Lee}},\ }\href
  {http://jmlr.org/papers/v22/20-1316.html} {\bibfield  {journal} {\bibinfo
  {journal} {Journal of Machine Learning Research}\ }\textbf {\bibinfo {volume}
  {22}},\ \bibinfo {pages} {1} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Bach}\ \emph {et~al.}(2015)\citenamefont {Bach},
  \citenamefont {Binder}, \citenamefont {Montavon}, \citenamefont {Klauschen},
  \citenamefont {Müller},\ and\ \citenamefont {Samek}}]{bach_pixel-wise_2015}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.}~\bibnamefont
  {Bach}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Binder}},
  \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Montavon}}, \bibinfo
  {author} {\bibfnamefont {F.}~\bibnamefont {Klauschen}}, \bibinfo {author}
  {\bibfnamefont {K.-R.}\ \bibnamefont {Müller}}, \ and\ \bibinfo {author}
  {\bibfnamefont {W.}~\bibnamefont {Samek}},\ }\href {\doibase
  10.1371/journal.pone.0130140} {\bibfield  {journal} {\bibinfo  {journal}
  {PLOS ONE}\ }\textbf {\bibinfo {volume} {10}},\ \bibinfo {pages} {e0130140}
  (\bibinfo {year} {2015})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Alber}\ \emph {et~al.}(2019)\citenamefont {Alber},
  \citenamefont {Lapuschkin}, \citenamefont {Seegerer}, \citenamefont
  {H{{\"a}}gele}, \citenamefont {Sch{{\"u}}tt}, \citenamefont {Montavon},
  \citenamefont {Samek}, \citenamefont {M{{\"u}}ller}, \citenamefont
  {D{{\"a}}hne},\ and\ \citenamefont {Kindermans}}]{alber_innvestigate_2019}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Alber}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Lapuschkin}},
  \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Seegerer}}, \bibinfo
  {author} {\bibfnamefont {M.}~\bibnamefont {H{{\"a}}gele}}, \bibinfo {author}
  {\bibfnamefont {K.~T.}\ \bibnamefont {Sch{{\"u}}tt}}, \bibinfo {author}
  {\bibfnamefont {G.}~\bibnamefont {Montavon}}, \bibinfo {author}
  {\bibfnamefont {W.}~\bibnamefont {Samek}}, \bibinfo {author} {\bibfnamefont
  {K.-R.}\ \bibnamefont {M{{\"u}}ller}}, \bibinfo {author} {\bibfnamefont
  {S.}~\bibnamefont {D{{\"a}}hne}}, \ and\ \bibinfo {author} {\bibfnamefont
  {P.-J.}\ \bibnamefont {Kindermans}},\ }\href
  {http://jmlr.org/papers/v20/18-540.html} {\bibfield  {journal} {\bibinfo
  {journal} {Journal of Machine Learning Research}\ }\textbf {\bibinfo {volume}
  {20}},\ \bibinfo {pages} {1} (\bibinfo {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Samek}\ \emph {et~al.}(2017)\citenamefont {Samek},
  \citenamefont {Binder}, \citenamefont {Montavon}, \citenamefont
  {Lapuschkin},\ and\ \citenamefont {M\"uller}}]{samek_evaluating_2017}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {W.}~\bibnamefont
  {Samek}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Binder}},
  \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Montavon}}, \bibinfo
  {author} {\bibfnamefont {S.}~\bibnamefont {Lapuschkin}}, \ and\ \bibinfo
  {author} {\bibfnamefont {K.-R.}\ \bibnamefont {M\"uller}},\ }\href {\doibase
  10.1109/TNNLS.2016.2599820} {\bibfield  {journal} {\bibinfo  {journal} {IEEE
  Transactions on Neural Networks and Learning Systems}\ }\textbf {\bibinfo
  {volume} {28}},\ \bibinfo {pages} {2660} (\bibinfo {year}
  {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Springenberg}\ \emph {et~al.}(2015)\citenamefont
  {Springenberg}, \citenamefont {Dosovitskiy}, \citenamefont {Brox},\ and\
  \citenamefont {Riedmiller}}]{springenberg_striving_2015}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.~T.}\ \bibnamefont
  {Springenberg}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {Dosovitskiy}}, \bibinfo {author} {\bibfnamefont {T.}~\bibnamefont {Brox}}, \
  and\ \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Riedmiller}},\
  }\href {http://arxiv.org/abs/1412.6806} {\enquote {\bibinfo {title} {Striving
  for {Simplicity}: {The} {All} {Convolutional} {Net}},}\ } (\bibinfo {year}
  {2015}),\ \bibinfo {note} {arXiv:1412.6806 [cs]}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Montavon}\ \emph {et~al.}(2017)\citenamefont
  {Montavon}, \citenamefont {Lapuschkin}, \citenamefont {Binder}, \citenamefont
  {Samek},\ and\ \citenamefont {Müller}}]{montavon_explaining_2017}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {G.}~\bibnamefont
  {Montavon}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont
  {Lapuschkin}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Binder}},
  \bibinfo {author} {\bibfnamefont {W.}~\bibnamefont {Samek}}, \ and\ \bibinfo
  {author} {\bibfnamefont {K.-R.}\ \bibnamefont {Müller}},\ }\href {\doibase
  https://doi.org/10.1016/j.patcog.2016.11.008} {\bibfield  {journal} {\bibinfo
   {journal} {Pattern Recognition}\ }\textbf {\bibinfo {volume} {65}},\
  \bibinfo {pages} {211} (\bibinfo {year} {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Hickstein}\ \emph {et~al.}(2012)\citenamefont
  {Hickstein}, \citenamefont {Ranitovic}, \citenamefont {Witte}, \citenamefont
  {Tong}, \citenamefont {Huismans}, \citenamefont {Arpin}, \citenamefont
  {Zhou}, \citenamefont {Keister}, \citenamefont {Hogle}, \citenamefont
  {Zhang}, \citenamefont {Ding}, \citenamefont {Johnsson}, \citenamefont
  {Toshima}, \citenamefont {Vrakking}, \citenamefont {Murnane},\ and\
  \citenamefont {Kapteyn}}]{hickstein_direct_2012}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.~D.}\ \bibnamefont
  {Hickstein}}, \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont
  {Ranitovic}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Witte}},
  \bibinfo {author} {\bibfnamefont {X.~M.}\ \bibnamefont {Tong}}, \bibinfo
  {author} {\bibfnamefont {Y.}~\bibnamefont {Huismans}}, \bibinfo {author}
  {\bibfnamefont {P.}~\bibnamefont {Arpin}}, \bibinfo {author} {\bibfnamefont
  {X.}~\bibnamefont {Zhou}}, \bibinfo {author} {\bibfnamefont {K.~E.}\
  \bibnamefont {Keister}}, \bibinfo {author} {\bibfnamefont {C.~W.}\
  \bibnamefont {Hogle}}, \bibinfo {author} {\bibfnamefont {B.}~\bibnamefont
  {Zhang}}, \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Ding}},
  \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Johnsson}}, \bibinfo
  {author} {\bibfnamefont {N.}~\bibnamefont {Toshima}}, \bibinfo {author}
  {\bibfnamefont {M.~J.~J.}\ \bibnamefont {Vrakking}}, \bibinfo {author}
  {\bibfnamefont {M.~M.}\ \bibnamefont {Murnane}}, \ and\ \bibinfo {author}
  {\bibfnamefont {H.~C.}\ \bibnamefont {Kapteyn}},\ }\href {\doibase
  10.1103/PhysRevLett.109.073004} {\bibfield  {journal} {\bibinfo  {journal}
  {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {109}},\ \bibinfo {pages}
  {073004} (\bibinfo {year} {2012})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Korneev}\ \emph {et~al.}(2012)\citenamefont
  {Korneev}, \citenamefont {Popruzhenko}, \citenamefont {Goreslavski},
  \citenamefont {Yan}, \citenamefont {Bauer}, \citenamefont {Becker},
  \citenamefont {K{\"u}bel}, \citenamefont {Kling}, \citenamefont {R{\"o}del},
  \citenamefont {W{\"u}nsche},\ and\ \citenamefont
  {Paulus}}]{korneev_interference_2012}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {P.~A.}\ \bibnamefont
  {Korneev}}, \bibinfo {author} {\bibfnamefont {S.~V.}\ \bibnamefont
  {Popruzhenko}}, \bibinfo {author} {\bibfnamefont {S.~P.}\ \bibnamefont
  {Goreslavski}}, \bibinfo {author} {\bibfnamefont {T.~M.}\ \bibnamefont
  {Yan}}, \bibinfo {author} {\bibfnamefont {D.}~\bibnamefont {Bauer}}, \bibinfo
  {author} {\bibfnamefont {W.}~\bibnamefont {Becker}}, \bibinfo {author}
  {\bibfnamefont {M.}~\bibnamefont {K{\"u}bel}}, \bibinfo {author}
  {\bibfnamefont {M.~F.}\ \bibnamefont {Kling}}, \bibinfo {author}
  {\bibfnamefont {C.}~\bibnamefont {R{\"o}del}}, \bibinfo {author}
  {\bibfnamefont {M.}~\bibnamefont {W{\"u}nsche}}, \ and\ \bibinfo {author}
  {\bibfnamefont {G.~G.}\ \bibnamefont {Paulus}},\ }\href {\doibase
  10.1103/PhysRevLett.108.223601} {\bibfield  {journal} {\bibinfo  {journal}
  {Physical Review Letters}\ }\textbf {\bibinfo {volume} {108}},\ \bibinfo
  {pages} {223601} (\bibinfo {year} {2012})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Kang}\ \emph {et~al.}(2020)\citenamefont {Kang},
  \citenamefont {Maxwell}, \citenamefont {Trabert}, \citenamefont {Lai},
  \citenamefont {Eckart}, \citenamefont {Kunitski}, \citenamefont
  {Sch{\"o}ffler}, \citenamefont {Jahnke}, \citenamefont {Bian}, \citenamefont
  {D{\"o}rner},\ and\ \citenamefont {Faria}}]{kang_holographic_2020}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {H.}~\bibnamefont
  {Kang}}, \bibinfo {author} {\bibfnamefont {A.~S.}\ \bibnamefont {Maxwell}},
  \bibinfo {author} {\bibfnamefont {D.}~\bibnamefont {Trabert}}, \bibinfo
  {author} {\bibfnamefont {X.}~\bibnamefont {Lai}}, \bibinfo {author}
  {\bibfnamefont {S.}~\bibnamefont {Eckart}}, \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Kunitski}}, \bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Sch{\"o}ffler}}, \bibinfo {author} {\bibfnamefont
  {T.}~\bibnamefont {Jahnke}}, \bibinfo {author} {\bibfnamefont
  {X.}~\bibnamefont {Bian}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont
  {D{\"o}rner}}, \ and\ \bibinfo {author} {\bibfnamefont {C.~F. D.~M.}\
  \bibnamefont {Faria}},\ }\href {\doibase 10.1103/PhysRevA.102.013109}
  {\bibfield  {journal} {\bibinfo  {journal} {Physical Review A}\ }\textbf
  {\bibinfo {volume} {102}},\ \bibinfo {pages} {013109} (\bibinfo {year}
  {2020})},\ \Eprint {http://arxiv.org/abs/1908.03860} {arXiv:1908.03860}
  \BibitemShut {NoStop}%
\bibitem [{\citenamefont {Maxwell}\ \emph {et~al.}(2020)\citenamefont
  {Maxwell}, \citenamefont {Faria}, \citenamefont {Lai}, \citenamefont {Sun},\
  and\ \citenamefont {Liu}}]{maxwell_spirallike_2020}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.~S.}\ \bibnamefont
  {Maxwell}}, \bibinfo {author} {\bibfnamefont {C.~F. D.~M.}\ \bibnamefont
  {Faria}}, \bibinfo {author} {\bibfnamefont {X.~Y.}\ \bibnamefont {Lai}},
  \bibinfo {author} {\bibfnamefont {R.~P.}\ \bibnamefont {Sun}}, \ and\
  \bibinfo {author} {\bibfnamefont {X.~J.}\ \bibnamefont {Liu}},\ }\href
  {\doibase 10.1103/PhysRevA.102.033111} {\bibfield  {journal} {\bibinfo
  {journal} {Physical Review A}\ }\textbf {\bibinfo {volume} {102}},\ \bibinfo
  {pages} {033111} (\bibinfo {year} {2020})}\BibitemShut {NoStop}%
\end{thebibliography}%


\end{document}
