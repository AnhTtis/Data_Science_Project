% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}
%\documentclass{beamer}
%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the %CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
%\usepackage[<options>]{animate}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{GesGPT: Speech Gesture Synthesis With Text Parsing from GPT}

\author{Nan Gao$^{1}$ , Zeyu Zhao$^{1,2}$ , Zhi Zeng$^{3}$  , Shuwu Zhang$^{3}$, Dongdong Weng$^{4*}$\\
$^{1}$Institute of Automation, Chinese Academy of Sciences, Beijing, China\\
$^{2}$University of Chinese Academy of Sciences\\
$^{3}$Beijing University of Posts and Telecommunications\\
$^{4}$Beijing Engineering Research Center of Mixed Reality and Advanced Display, Beijing Institute of Technology\\
$^{*}$ Corresponding author\\
{\tt\small nan.gao@ia.ac.cn}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
\maketitle
%%%%%%%%% ABSTRACT

\begin{abstract}
   Gesture synthesis has gained significant attention as a critical research area, focusing on producing contextually appropriate and natural gestures corresponding to speech or textual input. Although deep learning-based approaches have achieved remarkable progress, they often overlook the rich semantic information present in the text, leading to less expressive and meaningful gestures. We propose GesGPT, a novel approach to gesture generation that leverages the semantic analysis capabilities of Large Language Models (LLMs), such as GPT. By capitalizing on the strengths of LLMs for text analysis, we design prompts to extract gesture-related information from textual input. Our method entails developing prompt principles that transform gesture generation into an intention classification problem based on GPT, and utilizing a curated gesture library and integration module to produce semantically rich co-speech gestures. Experimental results demonstrate that GesGPT effectively generates contextually appropriate and expressive gestures, offering a new perspective on semantic co-speech gesture generation.
\end{abstract}
\begin{figure*}[htbp]
\begin{center}
\includegraphics[width=1\linewidth]{fig1.pdf}
\end{center}
\caption{Learning-based methods generally rely on audio and textual input, utilizing massive data to train deep learning models for gesture generation. Our proposed GesGPT aims to leverage GPT to directly generate gestures from text, that is, by designing prompts for textual gesture parsing to create semantically meaningful presentation gestures. We refer to this as prompt-based gesture generation. This framework can also be integrated with deep learning approaches to complement each other.}
\label{fig:1}
\end{figure*}
%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

Gesture synthesis is a research endeavor aimed at creating natural and contextually appropriate gestures corresponding to given speech or textual input. The majority of existing methods employ deep learning-based approaches, primarily focusing on audio features extracted from speech signals to model and generate gestures\cite{ginosar2019learning}\cite{ahuja2020style}\cite{henter2020moglow}\cite{ghorbani2022zeroeggs}\cite{zhou2022audio}\cite{yi2022generating}. Several of these approaches treat gesture synthesis as a regression problem and utilize various architectures, such as convolutional neural networks\cite{qian2021speech}\cite{xu2022freeform}\cite{habibie2021learning}, sequence networks\cite{hasegawa2018evaluation}\cite{yunus2021sequence}, and generative models\cite{li2021audio2gestures}\cite{ao2022rhythmic}\cite{ye2022audio}, to establish relationships between speech and gestures.

However, these existing methods exhibit certain limitations.  Firstly, they often neglect the rich semantic information embedded in textual input, which could contribute to the generation of more expressive and meaningful gestures. As shown in the analysis of GENEA Challenge 2022\cite{yoon2022genea}, the generated gestures have exhibited superior human-like similarity compared to motion capture data; nevertheless, enhancing the semantic expressiveness of these gestures remains an area requiring further exploration. Secondly, the deep learning-based methods for gesture synthesis tend to produce average results, often failing to generate nuanced and intricate hand movements\cite{nyatsanga2023comprehensive}. Lastly, they may not adequately address the inherent  intention in gesture expressions, which can lead to the generation of less plausible or contextually inappropriate gestures. Given these limitations, it is crucial to explore alternative research approaches that can harness the wealth of information available in textual input and tackle the challenges of effectively modeling gesture-speech associations.

Large Language Models (LLMs) have demonstrated remarkable capabilities in semantic analysis, understanding context, and extracting meaningful information from text. These models, such as GPT-3 \cite{brown2020language} and BERT \cite{devlin2018bert}, have been applied to various natural language processing tasks with impressive results\cite{liu2023pre}. Recently, several studies have successfully applied GPT to the field of robotics \cite{huang2023grounded}\cite{vemprala2023chatgpt}, and given the potential applications of gestures in both agent and robotic domains \cite{holladay2016rogue}\cite{yoon2021sgtoolkit}, it is essential to explore the utilization of LLMs in the gesture generation field.

Leveraging the robust semantic analysis capabilities of LLMs, we introduce GesGPT, a novel approach to gesture generation centered around text parsing using GPT. As shown in \ref{fig:1}, while previous studies employing deep learning techniques have successfully generated realistic gestures from text or speech input, these gestures often lack semantic coherence. Our approach utilizes prompt engineering design principles with GPT to produce semantically coherent gestures from text input. Additionally, deep learning methods have proven effective in modeling the rhythmic attributes between speech and gestures\cite{kucherenko2021multimodal}. As a result, we explored the integration of existing deep learning frameworks with LLMs to mutually enhance gesture generation. In a "human-on-the-loop" paradigm, we generate semantic scripts through prompt engineering and combine them with gestures derived from deep learning methodologies.

In summary, we present a novel approach for gesture generation using text parsing with GPT, which we call GesGPT. Building upon gesture cognition research, we devise prompt principles that transform gesture generation into an intention classification problem based on LLMs. By employing a carefully curated gesture library and functions, our method generates semantically rich presentation gestures. Experimental results reveal that GesGPT effectively capitalizes on the strengths of LLMs in understanding the inherent meaning and context of text input, ultimately producing contextually appropriate and expressive gestures that enrich the overall communication experience.

%-------------------------------------------------------------------------
\section{Related Work}
\subsection{Semantic Gesture Synthesis}
Manually designed rules have been shown to effectively preserve semantics within limited domains \cite{cassell1994animated}\cite{bremner2009beat}. Various deep learning approaches have been combined with rule-based methods \cite{zhou2022gesturemaster}\cite{habibie2022motion}, or integrated with predefined gesture dictionaries \cite{zhang2022text2video}, to produce more natural and semantically rich gestures. Some studies have concentrated on decoupling semantic gestures from the overall gestures and specifically targeting their learning and modeling \cite{liang2022seeg}. Our approach is grounded in synthesing gestures based on their intent, which facilitates the generation of expressive semantic gestures with communicative functionality.
\subsection{GPT in Embodied Intelligence Applications}
GPT for Robotics \cite{vemprala2023chatgpt} discusses the application of the GPT model, which is based on natural language processing, in the field of robotics. By incorporating prompt engineering design principles and creating a sophisticated function library, GPT can be adapted to various robotic tasks and simulators, demonstrating its potential in the robotics domain. Although LLMs possess strong analytical capabilities for existing knowledge, they struggle to explain non-linguistic environments such as physical settings. Huang et al.'s work\cite{huang2023grounded} utilizes the semantic knowledge of language models and accounts for external environmental factors by constructing action sequences, which may be achieved based on a foundational model that integrates language models and environmental factors. These works highlight the immense potential of introducing LLMs like GPT into the field of embodied intelligence. In this paper, we investigate the exploration of gesture synthesis through text parsing using GPT.
\section{GesGPT}
\begin{figure*}[htbp]
\begin{center}
\includegraphics[width=0.8\linewidth]{fig2.pdf}
\end{center}
\caption{The pipeline of GesGPT. Firstly, the Text Parsing module is employed to generate a gesture script from the input text. Then, the Gesture Dictionary module is used to search and retrieve corresponding GPT gestures according to the script. Next, the semantically rich GPT gestures and rhythmic base gestures are fused together by the Gesture Integration module to produce the final gesture. }
\label{fig:2}
\end{figure*}
\subsection{Gesture Synthesis Problem Formulation}
Co-speech gesture synthesis focuses on generating corresponding gestures from audio or textual input. In existing research, audio features $S$ are commonly represented using mel-spectrograms, while textual features $T$ are depicted through time-aligned text\cite{yoon2020speech}. As a result, a mapping function can be established from audio $S$ or text $T$ to gestures $G$, where the generated gestures $G$ are generally characterized by landmark positions \cite{ginosar2019learning} or relative rotations \cite{yoon2020speech}. In particular, gestures are represented by multiple two- or three-dimensional landmarks, encompassing upper body skeletons and hand keypoints.

Deep learning-based methods generally perform gesture synthesis by dividing the process into multiple segments. Based on the segmentation of the original speaker's videos, we obtain $N$ video segments, each comprising $K$ frames. We extract body landmarks to form gesture segments $G_{i=1}^{N}={G_{1}, ..., G_{i}}$, where $G_{i}\in R^{K*D_{G}}$, and $D_{G}$ represents the corresponding dimension. The corresponding audio and text are also temporally segmented to form audio segments $S_{i=1}^{N}={S_{1}, ..., S_{i}}$ and text segments $T_{i=1}^{N}={T_{1}, ..., T_{i}}$, where $S_{i}\in R^{K*D_{S}}$ and $T_{i}\in R^{K*D_{T}}$. Consequently, the deep learning-based gesture generation method, denoted as $M$, can be expressed as $G_{i} = M(S_{i}, T_{i})$.

Current deep learning methods for gesture generation predominantly treat gesture synthesis as a regression problem, primarily focusing on audio input. Nevertheless, text encompasses rich semantic information. In this paper, we approach gesture generation as a text-based classification problem and explore the application of LLMs for text parsing. We generate gestures grounded on the classification outcomes derived from purposefully designed prompts.
\subsection{Our Method}
\subsubsection{Overall Framework}
As shown in \ref{fig:2} , our approach consists of two main components: text parsing using GPT and gesture synthesis based on the parsing results. Firstly, we perform text analysis by applying parsing principles derived from the gesture cognition literature and our constructed gesture dictionary. Leveraging the strengths of the LLMS model for text analysis, we can obtain GPT's parsing results based on the input text using prompt questions. The gesture types are pre-defined and will be explained later. Next, we convert the text parsing results into a JSON-formatted gesture script. This script serves as a prompt to search and match against the preprocessed gesture library, enabling us to obtain the GPT Gesture. Additionally, we can utilize existing neural network methods to generate base gestures and subsequently integrate them into the final gesture output.
\subsubsection{Text Parsing from GPT}
Speech gestures can aid speakers in effectively conveying information and emphasizing key points during presentations. Our work focuses on exploring how GPT can assist in generating gestures with better expressive functions. Firstly, we clarify the assistance required for generating gestures from a given text, which involves determining (i) when to use gestures and (ii) what types of gestures to use. Our analysis indicates that we can obtain these results through text analysis using GPT.

The timing of speech gestures varies among individuals. In this article, we assume a one-to-one correspondence between sentences and gestures, and we identify the position of the gesture through keyword extraction from the sentence. 

To generate expressive and meaningful auxiliary gestures, we classify them based on their intended meaning using McNeill's work\cite{mcneill2008gesture} as a framework. McNeill categorizes gestures into five types: emblematic, iconic, metaphoric, deictic, and beat gestures. Building on this, we further divide these gesture types into categories based on different speaking intents, including welcome, farewell, description, explanation, emphasis, and self-reference. We also observe that certain words correspond to recognizable gestures, such as the thumbs up gesture when saying "awesome". We classify such gestures as "semantic gestures", leading to the definition of seven gesture intention types. We then construct a gesture dictionary based on these defined types.
\subsubsection{Gesture Dictionary}
According to Kendon's research \cite{kendon1980gesticulation}, a gesture can be divided into five stages in the temporal dimension, namely rest position, preparation, stroke, hold, and retraction. Based on this, we define the basic unit in the gesture dictionary as a gesture with the aforementioned initial and final stages. Specifically, a gesture in the dictionary starts from a rest position, goes through a series of hand movements, and ends at the rest position, as illustrated in \ref{fig:3}.
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=1\linewidth]{fig3.pdf}
\end{center}
\caption{Illustration of gesture unit in gesture dictionary. Gestures typically go through complete stages, including rest pose, preparation, stroke, retraction, and return to rest pose. However, not all Gesture Units have all three stages of preparation, stroke, and retraction.}
\label{fig:3}
\end{figure}
To obtain professional-level gestures, we collected a substantial number of videos of speakers from the internet, mainly including speeches by professional hosts who used professional gestures to enhance expression, totaling approximately 20.4 hours. Using open-source tools, we extracted audio, text, and human body landmarks, forming the ZHUBO dataset. Next, we applied a gesture unit detection algorithm to extract gesture units from the ZHUBO dataset based on variations in the positions of upper body landmarks, resulting in a collection of professional gesture units. Finally, we used motion capture equipment to re-capture a professional gesture unit set. Each gesture was recorded in three versions lasting 3, 6, and 9 seconds, resulting in high-quality three-dimensional gesture units.

Based on our analysis of text-gesture pairs in the ZHUBO dataset, we have identified recurring patterns between gestures and their intended meanings. For example, explanatory gestures are often accompanied by upward or outward movements of the palm, while emphatic gestures are often accompanied by downward chopping movements of the hand. To enhance the functionality of our gesture dictionary, we annotate each gesture unit with its corresponding intention type, resulting in a comprehensive and semantically rich gesture library. Additionally, each gesture unit is available in three different durations, corresponding to its respective intention label.
\begin{figure*}[htbp]
\begin{center}
\includegraphics[width=0.8\linewidth]{fig4.pdf}
\end{center}
\caption{The user interacts with GPT to determine when and which types of gestures to synthesize based on the speech text. Our approach utilizes designed gesture intents and LLMs to classify sentence intents in the text, assigning a specific gesture type to each sentence (Experiment 1 results are presented). Moreover, we identify keyword positions within each sentence, aligning them with gesture emphasis points to ensure rhythmic gesture generation (Experiment 2 results are presented).}
\label{fig:4}
\end{figure*}
\subsubsection{Learning Based Model}
Kucherenko et al. \cite{kucherenko2021multimodal} have shown that deep learning methods can effectively capture the association between audio and movement rhythms. In this study, we adopt a learning-based framework to model rhythmic body sway as a base gesture. Following the work of \cite{xu2022freeform}, we employ a one-dimensional convolutional model in the form of a U-NET for base gesture modeling. The base gesture $G^{B}$ is generated under supervision by minimizing the L1 distance between the ground truth $G_{GT}^{B}$ and the predicted gesture $G_{i}^{B}$. To reduce the jitter in the learned gestures, we incorporate a loss function based on higher-order derivatives, as proposed in \cite{siyao2022bailando} .
\begin{equation}
\begin{split}
  L_{Base}=\frac{1}{N}  \sum_{i=1}^N \left| G_{GT}^{B} -G_{i}^{B}\right|+\\
  \left| G_{GT}^{{B}^{'}} -G_{i}^{{B}^{'}}\right|+
  \left| G_{GT}^{{B}^{"}} -G_{i}^{{B}^{"}}\right|
  \label{eq:1}
\end{split}
\end{equation}
\begin{figure*}[htbp]
\begin{center}
\includegraphics[width=1\linewidth]{fig5.pdf}
\end{center}
\caption{We present a comparison of the generated gestures for a 13-second audio clip from the TED dataset. The generated gestures were synthesized at a frame rate of 15 FPS. In order to showcase the variations in the gestures, we extracted images from the synthesized video every 5 frames and concatenated them together for display.}
\label{fig:5}
\end{figure*}
\section{Experiments}
\subsection{Experimental Data}
We conducted experiments in two languages, English and Chinese, using the TED dataset\cite{yoon2020speech} and our proposed ZHUBO dataset, respectively. We selected speech texts from the test sets and utilized text parsing and gesture dictionary techniques to obtain corresponding GPT gestures. Furthermore, we used the MFA tool\cite{mcauliffe2017montreal} to align the audio and text in the datasets to obtain textual timing, which facilitated the subsequent fusion of gestures with speech content.
\begin{figure*}[htbp]
\begin{center}
\includegraphics[width=1\linewidth]{fig6.pdf}
\end{center}
\caption{Illustration of gesture integration at the beginning of each sentence. We fuse the GPT gesture and base gesture at the beginning of each sentence according to the intent results obtained from text parsing (indicated in red). In order to better demonstrate the semantic richness of gestures, we extract frames at two-thirds of the sentence and concatenate them after extracting 5 frames from each sentence for display.}
\label{fig:6}
\end{figure*}
\subsubsection{Gesture Integration}
To obtain the final gesture $G$, we combine the GPT gesture $G^{C}$ and base gesture $G^{B}$, i.e., $G=G^{B}+G^{C}$. The GPT gesture is temporally aligned with the base gesture sequence through linear interpolation. Our base gesture sequence takes two forms: (i) we use a rest pose captured by motion capture devices as the base gesture, and (ii) we obtain the base gesture sequence through a learning-based method.


\begin{figure*}[htbp]
\begin{center}
\includegraphics[width=1\linewidth]{fig7.pdf}
\end{center}
\caption{Illustration of gesture fusion based on keyword alignment. We align the stroke of the GPT gesture and base gesture at the keyword positions identified by text parsing results (indicated by black markers), and extract 5 frames around each keyword position to display the resulting gesture after concatenation.}
\label{fig:7}
\end{figure*}
\begin{figure*}[htbp]
\begin{center}
\includegraphics[width=1\linewidth]{fig8.pdf}
\end{center}
\caption{Illustration of GesGPT with base gesture integration. We demonstrate the generation of gestures for a 12-second audio segment from the ZHUBO dataset. The generated gestures are synthesized at a frame rate of 25FPS. To showcase the variation in gestures, we extract and concatenate every 8 frames from the synthesized video.}
\label{fig:8}
\end{figure*}

\begin{figure*}[htbp]
\begin{center}
\includegraphics[width=1\linewidth]{fig9.pdf}
\end{center}
\caption{Illustration of gesture fusion based on keyword alignment. Similarly, we extract 5 frames around each keyword position in the sentence and concatenate them to display the resulting gesture.}
\label{fig:9}
\end{figure*}
\subsection{Gesture Synthesis with GPT Gesture}
The fundamental principle of GesGPT involves utilizing GPT to derive gesture intent classifications from the text of a speech, as demonstrated in \ref{fig:4}.
\begin{itemize}
\item[$\bullet$]Experiment 1
\end{itemize}
We assume each sentence corresponds to a single gesture. Utilizing GPT for gesture intent classification, we identify the most suitable gesture from the dictionary based on sentence length, choosing randomly from various sets. As research indicates that gestures often precede co-expressive speech \cite{nobe2000most}, we synthesize the selected gesture at the sentence's onset. We use TED videos as test data, employing work\cite{yoon2020speech} as a baseline, and input text, audio, or combined formats into the seq2seq, audio2gesture, and multimodal model, respectively, to obtain predicted gesture generation results. Lastly, we transfer the 3D gesture skeleton to a cartoon character for comparison, as shown in \ref{fig:5}.

We have adopted a different sampling method during the presentation to better display the complete semantic gestures, as shown \ref{fig:6}.



\begin{itemize}
\item[$\bullet$]Experiment 2
\end{itemize}
We further optimized the gesture occurrence location by leveraging the prompt engineering to extract keywords from each sentence. During gesture generation, we aligned the stroke position with the keywords to ensure synthesized rhythmicity. Using TED videos as test data, we obtained gesture generation results based on text-aligned timing, as illustrated in \ref{fig:7}.

GPT exhibits superior capabilities in analyzing sentence expressions and we propose utilizing its abilities to generate gestures that offer better auxiliary expressiveness and semantic richness. By creating a comprehensive and specialized gesture library with complete stages, we can ensure the quality of the generated gestures.
\subsection{Gesture Synthesis with GPT Gesture and Base Gesture}
We used ZHUBO videos as test data, obtaining GPT gestures through gesture intent text analysis, and employing the learning-based model introduced in the third section to acquire base gestures. The resulting merged gesture skeleton is displayed in \ref{fig:8}. Similarly, we have changed the sampling method during the display process to better showcase the complete semantic gestures, as shown in \ref{fig:9}.

Represented by GPT, LLMs possess robust text semantic analysis capabilities, making them suitable for generating semantically rich gestures. However, LLMs currently lack direct audio perception. We propose that utilizing deep learning-based models to learn rhythmic gestures from extensive data as base gestures, and subsequently integrating them with semantically meaningful gestures, provides a more optimal approach.
\subsection{More Visualization Results}
We demonstrate the video results on a longer text paragraph, incorporating semantic gesture categories based on intent. Similarly, we use text intent parsing and keyword localization, combined with the gesture dictionary. Please refer to our supplementary material.
%------------------------------------------------------------------------
\section{Conclusion}
In summary, we present GesGPT, a novel approach for semantic co-speech gesture generation based on large language models, specifically GPT. The method consists of two main parts: text parsing using GPT and gesture synthesis based on parsing results. By leveraging the strengths of LLMs for text analysis and designing prompts to extract gesture-related information, the generated gestures are more semantically meaningful and contextually appropriate. The gestures are based on a predefined set of gesture types and a constructed gesture dictionary, which is derived from the ZHUBO dataset of professional speaker videos. The proposed approach demonstrates the potential of GPT in the field of embodied intelligence and gesture synthesis, revealing the potential of GPT in gesture synthesis. We believe that a more effective and meaningful gesture generation can be achieved through intention analysis based on textual input.
\section{Limitations and Future Work}
Despite the promising results achieved by GesGPT, there are still some limitations and areas for improvement in our approach. In future work, we plan to address these limitations and explore new directions. (i) Gesture Timing: Our current approach assumes that one sentence corresponds to one gesture, which might not always be the case. Further research is needed to develop a more sophisticated method for determining the appropriate timing and duration of gestures in relation to speech content. (ii) Gesture Diversity: Although our predefined gesture types and constructed gesture dictionary capture a wide range of gestures, it is not exhaustive. Future work could involve expanding the gesture dictionary and exploring more diverse and nuanced gesture types to better represent the richness of human nonverbal communication. (iii) Integration with Other Modalities: Our current approach primarily relies on text input for gesture generation. Future research could investigate how to effectively integrate other modalities, such as speech prosody or facial expressions, to generate even more contextually appropriate and expressive gestures.


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{plain}
\bibliography{refes.bib} 
}

\end{document}
