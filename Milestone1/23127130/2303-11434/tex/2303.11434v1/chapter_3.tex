\chapter{Methodology}\label{chapter_3}

\section{Overview}

Studying the related works helped me to decide which approach should be taken. All the necessary information was gathered form these works that help me to formulate the design the architecture of the model and to implement it in test cases. From background study, we learned about Simplified molecular-input line-entry system (SMILES) and Protein Sequence as well. We also learned about the Neural Network model and various methods to deep learning and machine learning. These numerous methods that we went through, allowed us to shape a methodology of our own.

\Cref{methodology} shows the major milestones of this project. Throughout this chapter we are going to take a systematic and analytical look at each step of this project, discussing the principals and theories behind our applied methods. We will give an overview of our model and show which methods we used in each layer.\\

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/figure4.png}
  \caption{Methodology Development Road-map}
  \label{methodology}
\end{figure}

\section{Contribution}

In the baseline work of DeepDTA~\cite{deepdta} we saw that they used a two-stream network one for getting the drug representation and one for getting protein representation. Then finally concatenating these two representations together to the fully connected layers. But drug lengths and the sequence lengths aren’t same. Most of the cases protein sequences are much larger than the drugs SMILES length. Therefore, the model is not capable to fully comprehend the information in the forward layers after concatenation layer. Therefore, in our work we introduce another stream where we took the final representation of the drug and protein before global max-pooling and then concatenated their representation and pass them in a separate stream with more filters. Then finally concatenating the three streams representation before the fully connected layers.\\


Beside using a separate stream, we also used residual connection in each stream taking motivation from ResNet architecture~\cite{he2016deep}. It helped making the representation of each stream more robust so that it can hold the information in the fully connected layers and make compensation for the information loss that was occurring in the baseline. These application in our network helped overcoming the limitation of the baseline and improved the results on the test data.\\

\section{Dataset}

We evaluated our proposed model on KIBA dataset~\cite{KIBA}, which were previously used as benchmark datasets for binding affinity prediction evaluation.\\

The KIBA dataset, originated from an approach called KIBA, in which kinase inhibitor bioactivities from different sources such as $K_i$, $K_D$ and $IC50$ were combined. KIBA scores were constructed to optimize the consistency between $K_i$, $K_D$ and $IC50$ by utilizing the statistical information they contained. The KIBA dataset originally comprised 467 targets and 52,498 drugs. In ~\cite{simboost} the dataset was filtered it to contain only drugs and targets with at least 10 interactions yielding a total of 229 unique proteins and 2,111 unique drugs. \Cref{dataset_summary} summarizes this dataset in the forms that we used in our experiments

\begin{table}[H]
  \begin{center}
    \caption{Summary of the Dataset}
    \label{dataset_summary}

    \begin{tabular}{|c|c|c|c|}
      \hline
      \textbf{Dataset} & \textbf{Protein} & \textbf{Compound} & \textbf{Interaction} \\
      \hline
      KIBA & $229$ & $2,111$ & $1,18,254$ \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

The distribution of the KIBA scores is depicted in \Cref{kiba_score_dist} for the training and test set. In ~\cite{simboost} KIBA scores was pre-processed as follows: 

\begin{itemize}
    \item For each KIBA score, its negative was taken
    \item The minimum value among the negatives was chosen
    \item The absolute value of the minimum was added to all negative scores
\end{itemize}

Thus, constructing the final form of the KIBA scores.\\

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/figure5.png}
  \caption{Distribution of KIBA Score}
  \label{kiba_score_dist}
\end{figure}

The compound SMILES strings of the KIBA dataset were extracted from the Pubchem compound database based on their Pubchem CIDs ~\cite{bolton2008pubchem}. For, first the CHEMBL IDs were converted into Pubchem CIDs and then, the corresponding CIDs were used to extract the SMILES strings. \Cref{kiba_summary} illustrates the distribution of the lengths of the SMILES strings of the compounds in the KIBA datasets. For the compounds of the KIBA dataset, the maximum length of a SMILES is $590$, while the average length is equal to $58$.\\

\begin{figure}[!b]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/figure6.png}
  \caption{Summary of the KIBA Dataset}
  \label{kiba_summary}
\end{figure}

The protein sequences of the KIBA dataset were extracted from the UniProt protein database based on gene names/RefSeq accession numbers~\cite{apweiler2004uniprot}. \Cref{kiba_summary} shows the lengths of the sequences of the proteins in the KIBA dataset. The maximum length of a protein sequence is $4,128$ and the average length is $728$ characters. \\

We should also note that the Smith–Waterman (S–W) similarity among proteins of the KIBA dataset is at most $60\%$ for $99\%$ of the protein pairs. This statistic indicate that the dataset is non-redundant.\\

\section{Data Pre-processing}

We used integer/label encoding that uses integers for the categories to represent inputs. We scanned approximately 2M SMILES sequences that we collected from Pubchem and compiled 64 labels (unique letters). For protein sequences, we scanned 550K protein sequences from UniProt and extracted 25 categories (unique letters).\\

Here we represent each label with a corresponding integer (e.g. ‘C’: 1, ‘H’: 2, ‘N’: 3 etc.). The label encoding for the example SMILES, ‘CN=C=O’, is given below.\\

\begin{center}
\textbf{[ C\hspace{2em}N\hspace{2em}=\hspace{2em}C\hspace{2em}=\hspace{2em}O]\hspace{2em}$=$\hspace{2em}[1\hspace{2em}3\hspace{2em}63\hspace{2em}1\hspace{2em}63\hspace{2em}5]}\\
\end{center}

Protein sequences are encoded in a similar way using label encodings. Both SMILES and protein sequences have varying lengths. Hence, in order to create an effective representation form, we decided on fixed maximum lengths of 100 for SMILES and 1000 for protein sequences for the dataset. We chose these maximum lengths based on the distributions illustrated in \Cref{kiba_summary} so that the maximum lengths cover at least 80\% of the proteins and 90\% of the compounds in the dataset. The sequences that are longer than the maximum length is truncated, whereas shorter sequences are zero-padded. %All the label encoding used for SMILES string and the protein sequence can be found in the Appendix section.

\section{Proposed Architecture}

To solve the problem of determining the binding affinity value using the SMILES string and the Protein Sequence string, we used a deep neural network using three streams and convolutional neural network block along with residual skip connection. In this section we will discuss the whole proposed model architecture and their different section and the hyperparameters used for the model.\\

The proposed model architecture is a multi-stream network where first stream is used to create the representation for the drug, second streams create a combined representation and the final stream creates the representation for the protein sequence. Then these three representations are concatenated and used as an input to the fully-connected block. After the fully connected block we get the affinity value as output. Since we used residual skip connection in the CNN block therefore, we named our model as ResDTA. \Cref{model_architecture} shows the whole architecture of our proposed model.\\

\begin{figure}[!tb]
  \centering
  \includegraphics[width=1\textwidth]{figures/figure7.png}
  \caption{Proposed Model Architecture ResDTA}
  \label{model_architecture}
\end{figure}

Now, we will discuss about the various blocks used in the proposed model with all the hyperparameters used in different layers. Since the proposed model is a multi-stream network therefore, first I will discuss about the SMILES stream and the Protein Sequence stream cause they both contain the same blocks and later I will discuss about the combined stream,\\

\textbf{Label Encoding:} in label encoding the data was pre-processed. Deep neural network cannot work with string type data, so we represented the string in the way described in the data pre-processing section so that it can be used in the following layers. After the label encoding layers, the input will have a tensor of size 100 integer for SMILES stream and 1000 integer for sequence stream. So, for SMILES stream we get the following output from the label encoding,

\begin{equation}
l\left(X^{S M I L E}\right)=\left\{x \mid x \in N^{100}\right\}
\end{equation}

Where $l(.)$ represents the label encoder,  $X^SMILE$ represents the SMILES string and $N^100$ represents a tensor of integers of size one hundred. Similarly for the sequence stream, 

\begin{equation}
l\left(X^{Sequence}\right)=\left\{x \mid x \in N^{1000}\right\}
\end{equation}

\textbf{Embedding Layer:} It is a simple lookup table that stores embeddings of a fixed dictionary and size. This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings. This layer takes a tensor of integer as input and using its dictionary it converts that into one-hot encoding tensor then pass that tensor in a linear transformation layer used in a fully connected block. For SMILES stream we use a dictionary of size 64 as there are total of 64 unique character that can be used to represent SMILES and for similar reason the dictionary size was 25 for sequence stream. For linear transformation the output was a tensor for 128. The SMILES and sequence embedding representation is shown in the following two equation accordingly,\\

% \begin{equation}
% \operatorname{Em}\left(l^{n \times 100}\right)=\left\{x \mid x \in \mathbb{R}^{n \times 128 \times 100}\right\}
% \end{equation}

\begin{equation}
\operatorname{Em}\left(l^{n \times 100}\right)=\left\{x \mid x \in R^{n \times 128 \times 100}\right\}
\end{equation}

\begin{equation}
\operatorname{Em}\left(l^{n \times 1000}\right)=\left\{x \mid x \in R^{n \times 128 \times 1000}\right\}
\end{equation}

Where, $Em(.)$ represents embedding layer function and  $l^{n \times i}$ represents the output of the label encoding and n represents the batch size in each mini-batch.\\

\textbf{CNN Block:} CNN represents convolutional neural network. In this block we have used 1D convolutional over a signal composed of 128 signal planes. Each 1D convolution is followed by a ReLU activation and also a Global Maxpool. For the 1D convolution in the simplest case, the output value of the layer with input size $(N,C_{in} ,L)$ and output $(N,C_{out} ,L_{out})$ can be precisely described as:

\begin{equation}
\operatorname{out}\left(n_{i}, C_{\text {out } j}\right)=\operatorname{bias}\left(C_{\text {out } j}\right)+\sum_{k=0}^{C_{\text {in }}-1} \text { weight }\left(C_{\text {out } j}, k\right) \star \operatorname{input}\left(n_{i}, \mathrm{k}\right)
\end{equation}

where $\star$  is the valid cross-correlation operator, $n$ is a batch size, $C$ denotes a number of channels, $L$ is a length of signal sequence. In each CNN block we have used three sequential 1D convolution where we passed the Embedding output. We have used 32 filters in the first convolution then used 64 filters and 96 filters for the next two 1D convolution accordingly, as for padding we use no padding. We used a stride of 1 and kernel size of 8 for the filters in both stream\\

\begin{center}
   $Input: (n, C_{in}, L_{in}) or (C_{in}, L_{in})$\\
   $Output: (n, C_{out}, L_{out}) or (C_{out}, L_{out})$
\end{center}


\begin{equation}
   L_{out}=\lfloor\frac{L_{in }+2 \times{ padding }-{ dilation } \times({ kernel\_size }-1)-1}{ stride }+1\rfloor 
\end{equation}

Here, $C$ represents the channels of the convolution and $L$ represents the features of the channel.\\

The activation we used after convolution is ReLU. Each convolution is followed Global Maxpool which downsamples the input representation by taking the maximum value over the channel dimension. To apply this global maxpool we used skip connection or residual connection so that while having the final representation the information in the former layers is retained. At the end of the three convolution and the global maxpool all the features are concatenated together and passed through a linear transformation layer to get the final representation from the both streams of the same size.\\

% \begin{equation}
% \operatorname{C_{SMILES}}\left(E^{n \times 128 \times 100}\right)=\left\{x \mid x \in \mathbb{R}^{n \times 256}\right\}
% \end{equation}

\begin{equation}
\mathrm{C}_{\text {SMILES }}\left(E^{n \times 128 \times 100}\right)=\left\{x \mid x \in R^{n \times 256}\right\}
\end{equation}

\begin{equation}
\operatorname{C_{Sequence}}\left(E^{n \times 128 \times 1000}\right)=\left\{x \mid x \in R^{n \times 256}\right\}
\end{equation}

Here, $C(.)$ represents the CNN block and $E^{n\times i \times j}$ represents output of the embedding layer.\\

After CNN block in the two streams, we get the SMILES representation and the sequence representation. Now before going into details about the fully-connceted block let us first look at the combined stream and discuss about how do we get the combined representation. In this stream we begin with the output of the last convolution from the other two streams. Then we concatenate the features so that we get a tensor of the size such as $X \in R^{n \times 96 \times 1058}$  . Then we pass this tensor to a CNN block. This CNN has the similar component as the other streams but the only difference is the number of filters used by the three convolution blocks. It has 192, 288 and 96 filters for the three convolutional layer accordingly. The number of filters for the convolution layers were determined by looking at the results of the cross-validation of different folds. As this contains information about both the drug and also the target sequence therefore the output of this stream is twice as big as the other two stream. So, the output of this stream is such that $X \in R^{n \times 512}$.

Finally, we get the output of each stream (SMILES stream, combined stream and the sequence stream) and concatenate them before using it as an input to the fully-connected block. Therefore, we get a tensor of 1024 dimension as the input of the fully-connected block. The fully-connected block consist of five linear transformation layers with various output features. The transformation function is as follows,\\

\begin{equation}
    y = xA^T + b
\end{equation}

Here $y$ is the output feature and $x$ is the input feature, $A$ is the trainable weight matrix and $b$ is bias. For the five fully connected layer we used the out features as 2048, 2048, 1024, 512 and 1 accordingly. The number of layers and their output features were selected by observing the cross-validation results of different folds. All the layers have an activation of ReLU after the linear transformation but the last layer. Last layer outputs the binding affinity value therefore no activation is used in this layer. During training to reduce the bias we attached a Dropout with every fully-connected layer. Dropout is used during training to randomly zeroes some of the elements of the input tensor with probability $p$ using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call. Furthermore, the outputs are scaled by a factor of $\frac{1}{1-p}$ during training. This means that during evaluation the module simply computes an identity function. In our case the value $p$ was kept as $0.1$ for all the cases where Dropout was used.\\

% \begin{equation}
% \widehat{K}=F_{c}\left(C_{\text {SMILES }}\left(E^{n \times 128 \times 100}\right) \oplus C_{\text {Sequence }}\left(E^{n \times 128 \times 1000}\right) \oplus C_{\text {Combined }}\left(\left\{X \mid X \in \mathbb{R}^{n \times 96 \times 1058}\right\}\right)\right)
% \end{equation}

\begin{equation}
\hat{K}=F_c\left(C_{S M I L E S}\left(E^{n \times 128 \times 100}\right) \oplus C_{\text {Sequence }}\left(E^{n \times 128 \times 1000}\right) \oplus C_{\text {Combined }}\left(\left\{X \mid X \in R^{n \times 96 \times 1058}\right\}\right)\right)
\end{equation}

Here, $\widehat{K}$ represents the predicted binding affinity value and in our work, it is the predicted KIBA score, $F_c (.)$ represents the fully-connected block and $\oplus$ is used to represent concatenation.

\endinput

