\chapter{Result and Discussion}\label{chapter_4}

\section{Overview}

In any Machine Learning or Deep Learning application it is important to evaluate the network in order to understand how the network will work when it comes across real world data or future data. This is the main purpose for any predictive modeling work. In order to evaluate any model, we need to select the evaluation metric first. The choice of evaluation metric is always based on the task the model is doing. For instance, for a task like classification problem a common evaluation metric will be accuracy whereas for a regression task a common evaluation metric would be mean square error. In our task we used different evaluation metric to verify our model generalization and performance gain over the baseline models which will be discussed at later section of this chapter. In addition, we will also discuss about the training hyper-parameters and training setup of our experiments so that anyone can reproduce our work.\\

\section{Evaluation Metrics}

As we discussed earlier evaluation metrics are a key factor to understand if a predictive model is working correctly as the way they should and will it work similarly for future data. It is always a good practice not to rely on a single metric for evaluating a model cause no metric is completely perfect and cannot interpret the model performance perfectly. Our task is a regression task so here we cannot use accuracy as there are no class involved in this task. We chose different evaluation metrics for evaluating the performance of our model. These metrics are also well suited for our work and also have been used in the previous works. It helped us evaluate the model further with the baseline methods. The computation of these metrics is as follows.

\textbf{Concordance Index (CI)~\cite{gonen2005concordance}:} This metric is computed using the following equation,

\begin{equation}
C I=\frac{1}{Z} \sum_{\delta_{i}>\delta_{j}} h\left(b_{i}-b_{j}\right)
\end{equation}

where $b_i$ is the prediction value for the larger affinity $\delta_i$, $b_j$ is the prediction value for the smaller affinity $\delta_j$, $Z$ is a normalization constant, $h(x)$ is the step function described as the following equation,\\

\begin{equation}
h(x)=\left\{\begin{array}{cc}
1, & \text { if } x>0 \\
0.5, & \text { if } x=0 \\
0, & \text { if } x<0
\end{array}\right.
\end{equation}

The metric measures whether the predicted binding affinity values of two random drug–target pairs were predicted in the same order as their true values were. We used paired-t test for the statistical significance tests with 95\% confidence interval (the larger value of CI indicates better model performance). \\

\textbf{Mean Squared Error (MSE):} MSE is a common evaluation metric used for regression task. It represents the average of differences between predicted and actual output values. So, the smaller value of MSE a model generates the better that model is performing is assumed. MSE is calculated using the following equation\\

\begin{equation}
MSE=\frac{1}{n} \sum_{i=1}^{n}\left(P_{i}-Y_{i}\right)^{2}
\end{equation}

Here, $P$ is the prediction vector, and $Y$ corresponds to the vector of actual outputs. $n$ indicates the number of samples.\\

\textbf{R squared $r_m^2$ :} This denotes the external prediction performance of the model. Meanwhile, the model is acceptable only when $r_m^2 > 0.5$ and,\\

\begin{equation}
r_{m}^{2}=r^{2} \times\left(1-\sqrt{r^{2}-r_{o}^{2}}\right)
\end{equation}

here $r^{2}$ and  $r_{o}^{2}$ designate the squared correlation coefficient parameters for the predicted and actual values with and without intercept.\\


\section{Model Training}

Model training is a crucial step for any predictive models to learn properly. Without training properly, we cannot expect the model to give appropriate prediction in the future or unseen data. Besides, to reproduce the results of a model we need to know specifically how the models were trained before testing it. Below we will describe and discuss about the different hyper-parameter we used during our training and various training technique that we used for achieving better results from the baseline-\\

\textbf{Optimizer:} To ensure the model learns the best way possible from the data, some hyper-parameters needed to be specified correctly. In the case of the neural network, we need to consider non-convex optimizers as a neural network that can have more than one local optima. It is very hard to determine which of these optima is the global optima. So, while we were training our ResDTA model, we needed to focus on finding the global optima from the loss surface of the network. To ensure a global optimum we needed to keep in mind a few things. such as the value of learning rate, not getting stuck on local optima, the changing morphology of Neural Network loss surface. Considering these two-point we used the Adam~\cite{kingma2014adam} optimizer. Adaptive moment estimation optimizer utilizes two algorithms: root means squared prop and Gradient descent with momentum. It allows the optimizer to produce smooth gradients. It combines the parameters from the two methods with two of its hyper-parameters:  learning rate and epsilon. The starting learning rate was set to 0.0001 and the epsilon value was 1e-08.\\


\textbf{Loss function:} Loss or the cost function is the main function that determines how model weights should be updated and using the loss value optimizer updates the weights of the model using gradient descent. So, determining the appropriate loss function for the model is a very important task. A popular loss function for classification task is categorical cross-entropy. But since our work is a regression task so we used root mean square error as our loss function to train our model. Previously we discuss about the mean-square error and how that is important for the model that does regression task. But our incentive to use RMSE is we trained our model for many epochs and with higher epoch training loss tend to go very low whereas there is a lot learning has to be done. But as the loss become low the training and the weights update gets slowed as well. We were facing this issue using the MSE loss but as we used RMSE it fixed the problem as the loss does not go very low at early stage of the training and as an overall outcome, we got better weights for our model at later stage of the training also. \\

\begin{equation}
R M S E=\sqrt{\frac{1}{n} \sum_{i=1}^{n}\left(P_{i}-Y_{i}\right)^{2}}
\end{equation}

\textbf{Learning rate scheduler:} As the training progress the training has to slowed to reach to the global optimal in the loss plane. Therefore, the learning rate needs to be reduced when the loss is reaching towards the global optima. The learning rate was reduced ten-fold after every 200 epochs of training. Empirically we found that after every 100 epochs of training reinitializing the optimizer and loading the best model weights and resuming the training from there gave better results over training the model continuously for couple of epochs at a stretch.\\

\textbf{Gradient Accumulation:} Gradient descent works best if we supply all of the available training data in a single forward pass. But this can’t be done for the most part due to the computational limitation. Therefore, we usually divide our data in mini-batches and go through them one by one. The network predicts batch labels, which are used to compute the loss with respect to the actual targets. Next, we perform backward pass to compute gradients and update model weights in the direction of those gradients. Gradient accumulation modifies the last step of the training process. Instead of updating the network weights on every batch, we can save gradient values, proceed to the next batch and add up the new gradients. The weight update is then done only after several batches have been processed by the model. Thus, gradient accumulation helps to imitate a larger batch size on a less expensive machine. Empirically we saw that gradient accumulation always helps the model during moments when training seems to stuck at a certain position. It improves the model training and therefore improve in the model performance as well.\\

\textbf{Hyper-parameters and other training specification:} The remaining hyper-parameters that are used during training are the number of epochs and the mini-batch size. We trained our model for a total of 400 epochs among them 200 epochs were trained with a learning rate of 0.0001 and later 200 epochs were trained using a learning rate of 0.00001. Throughout the training the mini-batch size was always kept at 256. In order to learn a generalized model, we randomly divided our dataset into six equal parts in which one part is selected as the independent test set. The remaining parts of the dataset were used to determine the hyper-parameters via 5-fold cross validation. \Cref{train_setup} illustrates the partitioning of the dataset. The same train and test folds were used by the other baselines as well for a fair comparison.\\

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/figure8.png}
  \caption{Train-validation-test setup}
  \label{train_setup}
\end{figure}

For writing the codes we used the popular deep-learning framework Pytorch\footnote{https://pytorch.org/}  and for training we used Kaggle\footnote{https://www.kaggle.com/}  kernels with a GPU.\\


\section{Model Evaluation}

In this section first, we will be looking into the various model we tried out before we landed on ResDTA. We used the Concordance Index (CI) to evaluate between these models. Then at the later part of this section we will be looking how ResDTA performed in comparison with the baseline models using the 1D string representation of the drug and the target protein sequence. Finally, we will be looking into how ResDTA’s prediction stands against the measured or actual binding-affinity value for the dataset.\\


Since the input data was a string of sequence both for the drug and the target and traditionally recurrent neural networks (RNN) work better on sequence data. So, we begin our experiment with RNN at the very beginning. But we got a very low CI score for the model. We tried out tuning a lot of hyper-parameters but the model gave prediction very much randomly therefore we could not improve the score. Then we tried out long-short time memory (LSTM) networks~\cite{lstm}. LSTM is proven to perform the best among all the recurrent type of model in task that contains time-series data. Though our work did not contain time-series data, we tried LSTM for our work. As a result, we did not find that the model was working properly as we got very low CI score. Then we move from recurrent networks and starts using convolutional neural networks specifically we used the 1d convolutional neural network. In the computer vision task, various 2d convolutional neural network have been in use. As our data can be processed only by 1d convolution so we converted one of the popular vision models 2d convolutional model DenseNet~\cite{huang2017densely} into 1D DenseNet. 1d DenseNet did perform a lot better than the recurrent networks but it was still way behind the baseline CI score. So, next we used the baseline DeepDTA model with some extra fully-connected layers and increase the number of filters in each stream. But it did not improve the reported result. Then we used the baseline model and added combined stream to the model and finally we get a performance gain in CI score. And finally we landed on the ResDTA network by attaching residual skip connection with the combined stream into the model which gave a significant performance gain from the baseline. \Cref{model_eval} illustrates the results of the different models of our experiments\\

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/figure9.png}
  \caption{Model Evaluation}
  \label{model_eval}
\end{figure}

For demonstrating the competitiveness of our model, we conduct an end-to-end comparison with the cutting-edge approaches (either machine- or deep-learning approaches) adopted for predicting affinity scores, and we conducted the comparative experiments under the same conditions for fair comparison with our method.\\


In \Cref{model_eval_table}, we provide the average obtained Concordance Index (CI) score, Mean Square Error (MSE) score and rm2 score to each study on the datasets, respectively. It can be noted that machine-learning models such as KronRLS~\cite{kronrls} and SimBoost~\cite{simboost} show worse performance compared to other deep-learning approaches. This is owing to their dependence on similarity matrices between drugs and targets as well as hand-crafted features. On the other hand, deep-learning techniques that automatically capture feature representation show great performance improvement.\\

\begin{table}[!b]
  \begin{center}
    \caption{Model comparison with cutting edge approaches on the KIBA dataset} 
    \label{model_eval_table}

    \begin{tabular}{|c|c|c|c|}
      \hline
      \textbf{Models} & \textbf{CI(std)} & \textbf{MSE} & \textbf{$r_{m}^{2}(std)$} \\
      \hline
      \multicolumn{4}{|c|}{\textbf{Machine Learning Based Approaches}} \\
      \hline
      KronRLS~\cite{kronrls} & 0.782 (0.001) & 0.411 & 0.342 (0.001) \\
      \hline
      Simboost~\cite{simboost} & 0.836 (0.001) & 0.222 & 0.629 (0.007) \\
      \hline
      \multicolumn{4}{|c|}{\textbf{Deep Learning Based Approaches}} \\
      \hline
      DeepDTA~\cite{deepdta} & 0.863 (0.002) & 0.194 & 0.673 (0.009) \\
      \hline
      MT-DTI~\cite{mtdti} & 0.882 (0.001) & 0.220 & 0.584 (0.003) \\
      \hline
      DeepCPI~\cite{deepcpi} & 0.852 (0.002) & 0.211 & 0.657 (0.004) \\
      \hline
      WideDTA~\cite{widedta} & 0.875 (0.001) & 0.179 & 0.675 (0.005) \\
      \hline
      GANsDTA~\cite{gansdta} & 0.866 (0.001) & 0.224 & 0.775 (0.008) \\
      \hline
      Attention-DTA~\cite{attentiondta} & 0.882 (0.004) & 0.162 & 0.735 (0.003) \\
      \hline
      \hline
      ResDTA(w/o Residual Connection) & 0.877 (0.001) & 0.0002 & 0.653 (0.005) \\
      \hline
      \textbf{ResDTA} & \textbf{0.885 (0.001)} & \textbf{0.0002} & 0.671 (0.004) \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

Among the deep learning based approaches, Attentio-DTA~\cite{attentiondta} and MT-DTI~\cite{mtdti} yielded best results with CI of 0.882, MSE of 0.220 and 0.162 respectively. It can be observed that the proposed ResDTA has a robust performance on the datasets, achieving 0.885 (0.003 improvement) for CI and 0.0002 (reduced by 0.1618) for MSE and achieved 0.671 in $r_m^2$ and we discussed that a model will be accepted when that it has a score of  $r_m^2 > 0.5$ , so ResDTA will be accepted based on that statement. It means the prediction generated by ResDTA is not by fluke and it has actual statistical significance. The result indicates the superiority of our proposed approached compared to the most recent studies for predicting DTA. Accordingly, we observe that our model outperforms existing deep-learning methods on two measures, which can be explained due to several factors: 

\begin{itemize}
    \item In comparing with DeepDTA~\cite{deepdta} we show that our model is able to retain the information in the final representation of SMILES and sequence using the residual connection. 
    \item In comparing with WideDTA~\cite{widedta} we show that using combined stream gives much better performance than using additional information along with the data.
    \item In comparing with Attention-DTA~\cite{attentiondta} we show that residual connection gives much better performance than using attention.
\end{itemize}

Generally, the obtained results and comparisons demonstrate that our model achieves competitive performance outperforms against these baselines methods. Moreover, \Cref{reg_plot} present the scatter plots of the proposed model predicted affinity score against the actual measured value on the datasets. The model achieves better performance when the estimated affinity scores are close to the original scores, and hence the instances should appear close to the red line. Principally, for the datasets, the data instances are close to the red regression line which, in turn, demonstrates that the proposed architecture has a competitive prediction performance.\\

\begin{figure}[!b]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/figure10.png}
  \caption{Predictions from our model against measured (real) binding affinity values for the KIBA dataset (KIBA score).}
  \label{reg_plot}
\end{figure}


\section{Summary}

The proposed model was able to predict the drug-target binding affinity value much better than all the baseline that we have considered in this work. The overall concordance index score was 0.887 and mean square error was 0.0002 which is better than all the baseline. This performance was achieved on a previously unseen dataset. So, it is evident that the model was properly trained in predicting affinity value.


\endinput

