





\section{Ensemble LSR Algorithm}\label{sec:enslsr}

To solve the above problem, we propose to exploit an \emph{ensemble} of $m$ S-LSRs that, as a whole, allow for robustness against outliers. We refer to the ensemble model as ENS-LSR, i.e.,  $\text{ENS-LSR}=\{\text{S-LSR}_1,...,\text{S-LSR}_m\}$.  
The basic idea is that,  given a start and a goal observation, different visual action plans are returned by the S-LSRs and  a selection is made among them based on their similarity. 

Let $\mathcal{P}_i$ be the set of VAPs produced by the S-LSR~$i$ given start $O_s$ and goal $O_g$ observations, i.e.,  
$$\mathcal{P}_i = \{P_{i,1}, ...\,, P_{i,q_i}\},$$ 
where $P_{i,j}$ is the VAP $j$ generated by the S-LSR and $q_i$ is the  number of total VAPs. We define the set of all potential VAPs, from $O_s$ to $O_g$, associated with the ensemble model as the union of the sets $\mathcal{P}_i$, $\forall i\in\{1,...,m\}$, i.e., 
$$\mathcal{P}=\mathcal{P}_1\cup \mathcal{P}_2 \cup ... \cup \mathcal{P}_m.$$ 
Our objective is to select the subset of VAPs $\mathcal{P}^*\subseteq\mathcal{P}$ to provide as output by the ensemble model. 



An illustrative overview of the method is provided in Fig.~\ref{fig:ensemble_explained} where multiple S-LSRs composing the ensemble model are depicted. The colors of the observations represent their underlying states, e.g., observations with indices $1$ and $5$ represent the same state  in this setting.
The figure shows how, starting from  the same training tuples, different latent spaces are produced by the mapping modules in ENS-LSR and different LSRs are built. Therefore,  
given a start and a goal observation (bottom left), different visual action plans are returned by the S-LSRs. At this point, a selection is made by the ensemble algorithm among these plans based on their similarity. 
More specifically, in line with majority voting rule approaches in ensemble learning \cite{ruta2005classifier}, we propose to select the VAPs which are the most similar to each other, thus removing outlier ones. This implies that, as first step,   appropriate similarity measures must be defined. Then, an algorithm is designed to select the paths based on these measures.
 In the reminder of the section, we address these two steps.  
Note that a naive ensemble approach could be to simply propose all plans suggested by all models in ENS-LSR, i.e., $\mathcal{P}\equiv \mathcal{P}^*$.  However, in this way no outlier rejection is made and wrong paths can be possibly provided as output 
(for more details see the results in Sec.~\ref{sec:exp}).


\subsection{Similarity measures}\label{sec:sim-measure}
We identify two main ways to evaluate the similarity between VAPs: \emph{i)} with respect to actions in the action plans, 
and \emph{ii)} with respect to node compositions in the latent plans. 


\noindent
\textbf{Action similarity}
As recalled in Sec. \ref{sec:SLSR}, the action plans are obtained by  concatenating  the average actions contained in the edges of the respective latent plans. 
Let $P_{i,j}^u$ be the action plan of the $j$~th  VAP produced by the S-LSR$_i$. 
We evaluate the similarity between two action plans $P_{i,j}^u$ and $P_{k,l}^u$ by computing the cosine similarity  which captures if the vectors point towards the same direction. 
To compute this similarity, we preprocess the action plans as follows: first, in order to compare all the coordinates of the actions, we  collapse the actions of the plans in one dimension, then, in order to compare plans with different lengths, we pad the shortest with zeros to equal the lengths of the sequences.  For instance, in case of a 2D pick and place action, the action variable can be written as 
$u = (p, r)$, where $p = (p_x, p_y)$ and $r = (r_x, r_y)$ represent the pick and release positions, and its collapsed version is given by $\bar{u} = (p_x, p_y, r_x,r_y)$. We denote the vector obtained by preprocessing the action plan $P_{i,j}^u$ as $\bar{P}^u_{i,j}$. Therefore, the action similarity measure $s^u$ is obtained as 
\begin{equation}\label{eq:ac-measure}
    s^u( \bar P_{i,j}^u,\bar P_{k,l}^u):=\frac{1}{2}\left(1+\frac{\bar P_{i,j}^u\cdot \bar P_{k,l}^u}{\|\bar P_{i,j}^u  \|\,\|\bar P_{k,l}^u\|}\right),
\end{equation}
 with $i\neq k$, which is in the range $[0,1]$ and reaches $1$ when the plans are the same, and $0$ when they are maximally dissimilar, i.e., they point in opposite directions.  
Further baselines for the action similarity measure, such as the euclidean and the  edit distances,  have been validated in the Sec.~\ref{sec:exp}, which are outperformed by \eqref{eq:ac-measure}. 





\noindent
\textbf{Node similarity}
As recalled in Sec. \ref{sec:SLSR}, the nodes of the $i$\ts th LSR are obtained by clustering the latent states associated with the training observations. We endow each node with the set of indices of the respective training observations and refer to it as node \emph{composition}. For instance, in Figure~\ref{fig:ensemble_explained}, the node compositions of the two green nodes in the  top LSR are $\{2,3\}$ and $\{12,9\}$.   Clearly, 
if two LSRs  have similar node compositions, they will likely produce plans in agreement. 
Let $P_{i,j}^z$ be the latent plan $j$ of S-LSR$_i$ and $\bar P_{i,j}^z$ be the collection of indices associated with the latent plan, i.e., it is the union of the compositions of the nodes in the plan.  
We define the node similarity measure $s^n$ for two latent plans $P_{i,j}^z$ and ${P}^z_{k,l}$ as their Jaccard similarity, i.e., as the ratio of the cardinality of the intersection of the two sets to the cardinality of the union:  
\begin{equation}\label{eq:node-measure}
    s^n( \bar P_{i,j}^z,\bar P_{k,l}^z):= \frac{|\bar P_{i,j}^z\cap \bar P_{k,l}^z|}{|\bar P_{i,j}^z\cup \bar P_{k,l}^z|},
\end{equation}
with $i\neq k$ and $|\cdot|$ the cardinality of the set $(\cdot)$. This measure is in the range $[0,1]$ and reaches $1$ when the plans are the same 
and $0$ when they are maximally dissimilar  (no intersection among the plans).
Note that the above measure can deal with plans having different lengths by construction. 

\vspace{0.2cm}
We define the overall similarity measure $s$ between two VAPs $P_{i,j}$ and $P_{k,l}$ as the sum of the 
action and the node similarity measures, i.e., $s = s^u+ s^n$. Note that both the measures $s^u$ and $s^n$ are in the interval $[0,1]$,  making them comparable. 

\subsection{Ensemble Latent Space Roadmap Algorithm}
Algorithm \ref{alg::ensemble} shows the proposed Ensemble LSR algorithm. It takes as input the set of $m$ S-LSRs composing the ensemble ENS-LSR, as well as the start $O_s$ and goal $O_g$ observations, 
and returns the most suited visual action plan(s), i.e., $\mathcal{P}^*$, from all the possible ones, i.e., $\mathcal{P}$. The basic idea is to compute a \emph{cumulative comparison score} $c_{i,j}$ for each VAP $P_{i,j}$, based on both action and node similarity measures with respect to the other VAPs, and subsequently select the ones with highest scores. In particular, the score $c_{i,j}$ is obtained by considering, for each S-LSR$_k$ with $k\neq i$, the respective VAP with highest overall similarity measure to $P_{i,j}$ and then by summing up all these highest measures $\forall k\in\{1,...,m\}, k\neq i$. Note that, for each S-LSR$_k$, with $k\neq i$, we consider only one VAP in the computation of the score to ensure that all S-LSRs have equal relevance in that calculation, i.e., to prevent that S-LSRs with many VAPs than others have greater influence in the score than the latter. 



 At start, the set $\mathcal{P}$ composed of all VAPs, from $O_s$ to $O_g$, produced by the $m$ S-LSRs is computed (line \ref{lst:line:vaps}) and   an empty set $\mathcal{C}$ is initialized (line \ref{lst:line:ec}) to store the cumulative comparison scores. 
Then, the algorithm iterates over the VAPs of each $\text{S-LSR}_i$ (line \ref{lst:line:lsri}-\ref{lst:line:pij}). 
For each $P_{i,j}$,  the corresponding action $P_{i,j}^u$ and latent $P_{i,j}^z$ plans are preprocessed as previously described (lines \ref{lst:line:ai_p} and \ref{lst:line:ni_p}) and the cumulative comparison score $c_{i,j}$ is initialized to zero  (line \ref{lst:line:cinit}). 
 At this point, a comparison is made  with the plans produced by the other S-LSRs. 
Therefore, an iteration is made over the sets of VAPs $\mathcal{P}_k$, with $k\neq i$, obtained by the other S-LSRs and, for each $k$, an empty set $\mathcal{S}$ is initialized to store the similarity measures between $P_{i,j}$ and the VAPs in $\mathcal{P}_k$. For each VAP $P_{k,l}$  in $\mathcal{P}_k$, 
the corresponding preprocessed action $\bar P_{k,l}^u$ and latent $\bar P_{k,l}^z$ plans are obtained (lines \ref{lst:line:ak_p}-\ref{lst:line:nk_p}) and their action $s^u$ and node $s^n$ similarities with the plans  $\bar P_{i,j}^u$ and $\bar P_{i,j}^z$ are computed (lines \ref{lst:line:ac-sim}-\ref{lst:line:node-sim}) according to \eqref{eq:ac-measure} and \eqref{eq:node-measure}, respectively. The overall similarity between the VAPs $P_{i,j}$ and $P_{k,l}$ is calculated by summing the the individual similarities $s^u$ and $s^n$  and is stored in the set $\mathcal{S}$  (line \ref{lst:line:addtos}).  Once all the overall similarities with the VAPs of the S-LSR$_k$ have been computed, the maximum one is added  to the comparison score variable $c_{i,j}$ (line \ref{lst:line:cumscore}). The latter score is complete when all S-LSRs different from $i$ have been analyzed, and is then added to the set $\mathcal{C}$  (line \ref{lst:line:addtoc}).

Once  all the cumulative comparison scores have been computed, the set of indices $\mathcal{I}$ of the VAPs with highest scores is extracted (line \ref{lst:line:is}), i.e., 
$$\mathcal{I}^*  = \argmax_{i,j} \,\mathcal{C}, $$
and the corresponding VAPs $\mathcal{P}^*$ are selected as final output  (line \ref{lst:line:ps}). 



\begin{algorithm} \caption{Ensemble LSR Planning}
\small
\def\negsp{\vspace{-5pt}}
\def\negup{\vspace{-7pt}}
\def\negin{\vspace{-3pt}}
\setstretch{1.2}
\begin{algorithmic}[section]
\Require  $\text{ENS-LSR} = \{\text{S-LSR}_1, ..., \text{S-LSR}_m\}$, $O_s$, $O_g$
    \begin{algorithmic}[1]
    \State $\mathcal{P}\leftarrow $ Compute-VAPs (ENS-LSR, $O_s, O_g$) \label{lst:line:vaps}
    \State $\mathcal{C} \leftarrow \{ \}$ \label{lst:line:ec}
    \ForEach{$\mathcal{P}_i \in \mathcal{P} $} \label{lst:line:lsri}
        \ForEach{$P_{i,j} \in \mathcal{P}_i$}\label{lst:line:pij}
            \State $\bar P_{i,j}^u \leftarrow $ Preprocess-action-plan($P_{i,j}$) \label{lst:line:ai_p}
            \State $\bar P_{i,j}^z \leftarrow $ Preprocess-latent-plan($P_{i,j}$) \label{lst:line:ni_p}
            \State $c_{i,j} \leftarrow 0$ \label{lst:line:cinit}
            \ForEach{$\mathcal{P}_k \in \mathcal{P}\text{, with } k\neq i$} \label{lst:line:lsrj}
                    \State $\mathcal{S} = \{ \}$ \label{lst:line:ecp}          
                    \ForEach{$P_{k,l} \in \mathcal{P}_k$}
                        \State $\bar P_{k,l}^u \leftarrow $ Preprocess-action-plan($P_{k,l}$) \label{lst:line:ak_p}
                        \State $\bar P_{k,l}^z \leftarrow $ Preprocess-latent-plan($P_{k,l}$) \label{lst:line:nk_p}
                        \State $s^u \leftarrow $ Action-sim($\bar P_{i,j}^u, \bar P_{k,l}^u $)$\quad$ [eq. \eqref{eq:ac-measure}]\label{lst:line:ac-sim}
                        \State $s^n \leftarrow $ Node-sim($\bar P_{i,j}^z, \bar P_{k,l}^z $) $\quad\,$ [eq. \eqref{eq:node-measure}] \label{lst:line:node-sim}
                        
                        \State $\mathcal{S} \leftarrow  \mathcal{S} \cup \{s^u + s^n$\} \label{lst:line:addtos}
                    \EndFor
                    \State $c_{i,j}\leftarrow c_{i,j} + \max(\mathcal{S})$ \label{lst:line:cumscore}
            \EndFor   
            \State $\mathcal{C}\leftarrow\mathcal{C}\cup \{c_{i,j}\}$ \label{lst:line:addtoc}
        \EndFor
    \EndFor
    \State $\mathcal{I}^* \leftarrow\argmax_{i,j}(\mathcal{C})$\label{lst:line:is}
    \State $\mathcal{P}^* \leftarrow \text{Select-VAPs}(\mathcal{P},\mathcal{I}^*)$\label{lst:line:ps}
   
    \end{algorithmic}
\Return $\mathcal{P}^*$
\end{algorithmic}
\label{alg::ensemble}
\end{algorithm}


