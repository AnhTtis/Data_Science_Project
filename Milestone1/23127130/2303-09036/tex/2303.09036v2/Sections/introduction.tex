\section{Introduction}
3D-aware GANs~\cite{schwarz2020graf,gu2021stylenerf,deng2022gram,chan2021efficient} have experienced rapid development in recent years and shown great potential for large-scale realistic 3D content creation. The core of 3D-aware GANs is to incorporate 3D representation learning and differentiable rendering into image-level adversarial learning~\cite{goodfellow2014generative}. In this way, the generated 3D representations are forced to mimic real image distribution from arbitrary viewing angles, resulting in their faithful reconstruction of the underlying 3D structures of the subjects for free-view image synthesis. Among different 3D representations, neural radiance field (NeRF)~\cite{mildenhall2020nerf} has been proven to be effective in the 3D-aware GAN scenario~\cite{schwarz2020graf, chan2021pi}, which guarantees strong 3D consistency when synthesizing multiview images via volume rendering~\cite{kajiya1984ray}.

\input{Figures/fig_teaser}

\input{Figures/fig_ours}

However, NeRF's volumetric representation also brings high computation costs to GAN training. This hinders the generative models from synthesizing high-resolution images with fine details. Several attempts have been made to facilitate NeRF-based GAN training at high resolution, via sparse representations~\cite{schwarz2022voxgraf,deng2022gram,xiang2022gram,zhao2022generative} or patch-wise adversarial learning~\cite{skorokhodov2022epigraf}, yet the performance is still unsatisfactory and lags far behind state-of-the-art 
2D GANs~\cite{karras2020analyzing,karras2021alias}. 

Along another line, instead of using direct NeRF rendering, plenty of works~\cite{niemeyer2021giraffe,gu2021stylenerf,or2022stylesdf,chan2021efficient,xue2022giraffe} introduce 2D super-resolution module to deal with 3D-aware GAN training at high resolution. A typical procedure is to first render a NeRF-like feature field into low-resolution feature maps, then apply a 2D CNN to generate high-resolution images from them. The representative work among this line, namely EG3D~\cite{chan2021efficient}, utilizes tri-plane representation to effectively model the low-resolution feature field and leverages StyleGAN2-like~\cite{karras2020analyzing} super-resolution block to achieve image synthesis at high-quality. It sets a record for image quality among 3D-aware GANs and gets very close to that of state-of-the-art 2D GANs. However, a fatal drawback of this line of works is a sacrifice of strict 3D consistency, due to leveraging a black-box 2D CNN for image synthesis.

A question naturally arises —— \textit{Is there any way to combine the above two lines to achieve strict 3D consistency and high-quality image generation simultaneously?} The answer, as we will show in this paper, is arguably yes. The key intuition is to let the images synthesized by direct NeRF rendering to mimic those generated by a 2D super-resolution module, which we name \textit{3D-to-2D imitation}.

Specifically, we start from an EG3D backbone that adopts 2D super-resolution to generate high-resolution images from a low-resolution feature field. 
% Its generated images are of high quality, thanks to the powerful StyleGAN2 blocks, and overall 3D-consistent, due to the constraint of the low-resolution 3D field. 
Based on this architecture, we add another 3D super-resolution module to generate high-resolution NeRF from the low-resolution feature field and force the images rendered by the former to imitate those generated by the 2D super-resolution branch. This process can be seen as a multiview reconstruction process —— images sharing the same latent code from different views produced by the 2D branch are pseudo multiview data, and the high-resolution NeRF branch represents the 3D scene to be reconstructed. Previous methods~\cite{pan20202d,zhang2020image,poole2022dreamfusion} have shown that this procedure can obtain reasonable 3D reconstruction, even if the multiview data are not strictly 3D consistent. We believe this is partially due to the inductive bias (\eg, continuity and sparsity) of the underlying 3D representation. With the above process, the high-resolution NeRF learns to depict fine details of the 2D-branch images, thus enabling high-quality image rendering. The 3D consistency across different views can also be preserved thanks to the intrinsic property of NeRF. Note that if the rendered images try to faithfully reconstruct every detail of the 2D-branch images across different views, it is likely to obtain blurry results due to detail-level 3D inconsistency of the latter. To avoid this problem, we only let the images produced by the two branches be perceptually similar (\ie by LPIPS loss~\cite{zhang2018unreasonable}), and further enforce adversarial loss between the rendered images from the high-resolution NeRF and real images to maintain high-frequency details. In addition, we only render small image patches to conduct the imitative learning to reduce memory costs.

Apart from the above learning strategy, we introduce 3D-aware convolutions to the EG3D backbone to improve tri-plane learning, motivated by a recent 3D diffusion model~\cite{wang2022rodin}. The original EG3D generates tri-plane features to model the low-resolution feature field via a StyleGAN2-like generator. The generator is forced to learn 2D-unaligned features on the three orthogonal planes via 2D convolutions, which is inefficient. The 3D-aware convolution considers associated features in 3D space when performing 2D convolution, which improves feature communications and helps to produce more reasonable tri-planes. Nevertheless, directly applying 3D-aware convolution in all layers in the generator is unaffordable. As a result, we only apply them after the output layers at each resolution in the tri-plane generator. This helps us to further improve the image generation quality with only a minor increase in the total memory consumption.

With the above strategies, our generator is able to synthesize 3D-consistent images of virtual subjects with high image quality (Fig.~\ref{fig:visual}). It reaches FID scores~\cite{heusel2017gans} of $5.4$ and $4.3$ on FFHQ~\cite{karras2019style} and AFHQ-v2 Cats~\cite{choi2020stargan}, respectively, at $512\times512$ resolution, largely outperforming previous 3D-aware GANs with direct 3D rendering and even surpassing many leveraging 2D super-resolution (Fig.~\ref{fig:teaser}). A by-product of our method is a more powerful 2D-branch generator, which reaches an FID of $4.1$ on FFHQ, exceeding previous state-of-the-art EG3D.
Though our method presented in this paper is mostly based on EG3D backbone, its 3D-to-2D imitation strategy can be extended
to learning other 3D-aware GANs as well. We believe this would largely close the quality gap between 3D-aware GANs and traditional 2D GANs, and pave a new way for realistic 3D generation. 

% The core intuition behind is simple: As the 2D super-resolution module can generate images with correct structures and fine details, we let the direct rendering results to mimic those generated by 2D super-resolution. In this way, we can inherit the advantages of both of them, \ie, the high quality image generation and the 3D consistency multi-view rendering. Specifically, to obtain high-quality generated images for imitation, we first train an EG3D which generates coarse tri-planes for low-resolution NeRF rendering and utilizes StyleGAN2~\cite{karras2020analyzing}-like 2D super-resolution blocks for final image synthesis. 