\section{Related Works}
\paragraph{3D-aware GAN.} 3D-aware GANs~\cite{henzler2019escaping,nguyen2019hologan,schwarz2020graf,chan2021pi,niemeyer2021giraffe,gu2021stylenerf,zhang2022multi,deng2022gram,chan2021efficient,skorokhodov2022epigraf,zhao2022generative} aim to generate multiview images of an object category, given only in-the-wild 2D images as training data. The key is to represent the generated scenes via a 3D representation and leverage corresponding rendering techniques to synthesize images at different viewpoints for image-level adversarial learning~\cite{goodfellow2014generative}. Initially, explicit representations such as voxels~\cite{nguyen2019hologan,henzler2019escaping} and meshes~\cite{szabo2019unsupervised} are used to describe scenes. With the development of neural implicit fields~\cite{park2019deepsdf,mescheder2019occupancy,sitzmann2019scene,sitzmann2020implicit,mildenhall2020nerf,wang2021neus,oechsle2021unisurf}, implicit scene representations, especially NeRF~\cite{mildenhall2020nerf}, gradually overtake explicit ones in 3D-aware GANs~\cite{chan2021pi,or2022stylesdf,chan2021efficient}. Nevertheless, one great hurdle of NeRF-based GANs is the high computation cost, which restricts earlier works~\cite{schwarz2020graf,chan2021pi,devries2021unconstrained,xu2021generative,pan2021shading} from synthesizing high-quality images. Consequently, a large number of follow-up works~\cite{niemeyer2021giraffe,gu2021stylenerf,zhou2021cips,xue2022giraffe,or2022stylesdf,chan2021efficient, xu20223d} avoid rendering NeRF at high resolution by conducting 2D super-resolution from a low-resolution image or feature map rendered by NeRF-like fields. This is only a stopgap as the black-box 2D super-resolution module sacrifices the important 3D consistency brought by NeRF. To keep the strict 3D consistency, several works~\cite{deng2022gram,xiang2022gram,schwarz2022voxgraf,skorokhodov2022epigraf,zhao2022generative} turn to more sparse 3D representations such as sparse voxel~\cite{schwarz2022voxgraf}, radiance manifolds~\cite{deng2022gram}, and multi-plane images~\cite{zhao2022generative} to allow direct rendering at high resolution. Carefully designed training strategies such as two-stage training~\cite{xiang2022gram} or patch-wise optimization~\cite{skorokhodov2022epigraf} are also introduced to facilitate the learning process. However, their image generation quality still lags behind those with 2D super-resolution. Our method combines the advantages of both lines of works to achieve high-quality image generation and strict 3D consistency at once, by leveraging the proposed 3D-to-2D imitation.

\vspace{-8pt}
\paragraph{3D generation by 3D-to-2D imitation.} Recent studies~\cite{jahanian2019steerability, shen2020interpreting,harkonen2020ganspace} reveal that 2D generative models~\cite{brock2018large,karras2019style} have the ability to generate pseudo multiview images of a subject. Based on this observation, several methods~\cite{pan20202d,zhang2020image,shi2021lifting,pan2022gan2x} propose to distill the knowledge from a pre-trained 2D generative model for 3D generation by performing 3D reconstruction on the generated ``multiview" images. A standard procedure is to render the 3D representation of an object from multiple views, and compare them with the closest samples falling in the latent space of the pre-trained 2D generator for iterative optimization. The 2D generator ensures that the rendered results are photorealistic from different views, meanwhile the intrinsic property of the 3D representation guarantees reasonable 3D structure, thus leading to high-quality 3D generation. Some recent methods~\cite{poole2022dreamfusion,lin2022magic3d} also combine this idea with text-to-image diffusion models~\cite{rombach2022high,saharia2022photorealistic} to achieve text-driven 3D creation. Our method shares a similar spirit, which distills the knowledge from the generator's 2D super-resolution branch to its 3D rendering branch, thus achieving image generation with both photorealism and strict 3D consistency.