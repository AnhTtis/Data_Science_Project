\section{Introduction}
3D-aware GANs~\cite{schwarz2020graf,gu2021stylenerf,deng2022gram,chan2021efficient} have experienced rapid development in recent years. Trained with only in-the-wild 2D images, they are able to generate free-view images of virtual subjects, showing great potential for large-scale 3D content creation. The core of 3D-aware GANs is to incorporate 3D representation and differentiable renderer into their image generation process, where neural radiance field (NeRF)~\cite{mildenhall2020nerf} is the mostly adopted representation that guarantees strong 3D consistency across different viewing points.

However, incorporating NeRF into GAN training also brings high computation cost that hinders the generative models from synthesizing high-resolution images with fine details. To alleviate this problem, prior arts adopt two major strategies. The first one~\cite{niemeyer2021giraffe, gu2021stylenerf, or2022stylesdf, chan2021efficient} is to render NeRF at a lower resolution and introduces 2D super-resolution module to add high-frequency details to the rendered coarse images. 
% Rendering low-resolution NeRF largely reduces the memory cost and stablizes the training process, meanwhile the 2D super-resolution module is capable of producing fine image details at high resolution. 
A representative work among this line is EG3D~\cite{chan2021efficient}, which introduces tri-plane representation to model low-resolution NeRF and leverages StyleGAN2~\cite{karras2020analyzing}-like super-resolution module to generate high-resolution images. It sets a record for image generation quality among 3D-aware GANs and approximates that of state-of-the-art 2D GANs~\cite{karras2020analyzing, karras2021alias}. Despite the high image quality, a drawback of these line of works is a sacrifice of strict 3D consistency brought by direct 3D rendering of NeRF. The synthesized images of these methods usually suffer from geometry distortion and texture flickering issues during continuous camera change, due to the non-physical black-box synthesis process of their 2D super-resolution module.
The second group of works~\cite{deng2022gram, skorokhodov2022epigraf, schwarz2022voxgraf, zhao2022generative} thus avoid 2D super-resolution by utilizing more efficient 3D representations to allow direct rendering at a higher resolution. This brings them better 3D consistency under camera variations. Nevertheless, learning these 3D representations at high resolution is very challenging and often requires delicate training strategies~\cite{skorokhodov2022epigraf,xiang2022gram}. Some efficient representations~\cite{deng2022gram,zhao2022generative} also sacrifice the rendered quality at large viewing angles. With much effort made by the previous methods, the image generation quality among this line still falls behind those with a 2D super-resolution module, especially EG3D.

In this paper, we propose a 3D-aware GAN with direct 3D rendering that achieves state-of-the-art image generation quality. We inherit the tri-plane representation~\cite{chan2021efficient} from EG3D to efficiently model NeRF of a generated subject, and propose two key strategies to achieve high-quality image rendering without using a 2D super-resolution module, as described below:

1) \textit{3D-to-2D imitation}. Intuitively, as the images generated by EG3D are already photorealistic by leveraging a 2D super-resolution module, if we let the images produced by direct 3D rendering to mimic them, it is possible that our generator takes the advantages of both the 2D super-resolution module and the direct 3D rendering, \ie, the high image generation quality and the 3D consistency.
Based on this observation, we propose to learn a high-resolution tri-plane representation from the low-resolution one of EG3D, and let the rendered images from the high-resolution tri-planes to mimic the image content of those generated by the 2D super-resolution module. Note that the images generated by the latter sacrifice certain degrees of 3D consistency, if the rendered images try to mimic every detail of them across different views, it is likely to obtain blurry results due to detail misalignment. To avoid this problem, we only let the images produced by the two branches to be perceptually similar (\ie by LPIPS loss~\cite{zhang2018unreasonable}), and further enforce adversarial loss between the rendered images from the tri-planes and real images to maintain high-frequency details. In practice, rendering a whole image at high resolution for computing the perceptual similarity and adversarial loss can be costly. Therefore, we only render small image patches from the high-resolution tri-planes to conduct the imitative learning.

2) \textit{3D-aware tri-plane generator}. Originally, EG3D generates tri-plane features via a StyleGAN2-like generator. The generator is forced to learn 2D-unaligned features on the three orthogonal planes via 2D convolutions, which is inefficient. Inspired by a recently proposed 3D diffusion model~\cite{wang2022rodin}, we introduce 3D-aware convolution to the tri-plane generator to improve feature communications across the planes. The 3D-aware convolution considers associated features in 3D space when performing 2D convolution, which helps to produce more reasonable tri-planes. Nevertheless, directly applying 3D-aware convolution in all layers in the generator leads to unaffordable memory cost during training. As a result, we only apply them in the output layer at each resolution in the tri-plane generator. This helps us to further improve the image generation quality with only a minor increase of the total memory consumption.

With the above two strategies, our generator is able to synthesize free-view images of virtual subjects with state-of-the-art image quality. It reaches FID scores~\cite{heusel2017gans} of $3.9$ and $2.7$ on FFHQ~\cite{karras2019style} and AFHQ-v2 Cats~\cite{choi2020stargan} datasets respectively at $512\times512$ resolution, largely outperforming previous 3D-aware GANs with direct 3D rendering and even surpassing those with 2D super-resolution modules. The synthesized images are of strict 3D-consistency across multiple views, thanks to the direct volumetric rendering of the intermediate NeRF representation.

% The core intuition behind is simple: As the 2D super-resolution module can generate images with correct structures and fine details, we let the direct rendering results to mimic those generated by 2D super-resolution. In this way, we can inherit the advantages of both of them, \ie, the high quality image generation and the 3D consistency multi-view rendering. Specifically, to obtain high-quality generated images for imitation, we first train an EG3D which generates coarse tri-planes for low-resolution NeRF rendering and utilizes StyleGAN2~\cite{karras2020analyzing}-like 2D super-resolution blocks for final image synthesis. 