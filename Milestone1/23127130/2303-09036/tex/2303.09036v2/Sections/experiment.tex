\section{Experiments}


\inputtable{tab_fid}






% \paragraph{Datasets}


% We compare methods on the task of unconditional 3D-aware generation with FFHQ~\cite{karras2019style} and AFHQ-v2 Cats~\cite{choi2020stargan}, both of which are real-world datasets on human and cat faces. The data pre-processing follows that of EG3D~\cite{chan2021efficient}, and adaptive data augmentation \cite{karras2020training} is applied to AFHQ-v2~\cite{choi2020stargan} experiments.

\paragraph{Implementation details.}
We train our method on two real-world datasets: FFHQ~\cite{karras2019style} and AFHQ-v2 Cats~\cite{choi2020stargan}, which consists of 70K human face images of $1024^2$ resolution and 5.5K cat face images of $512^2$ resolution, respectively. We follow the data pre-processing of EG3D~\cite{chan2021efficient} to crop and resize the images to $256^2$ or $512^2$ resolution. Experiments are conducted on 8 NVIDIA Tesla A100 GPUs with
40GB memory, following the training configuration of EG3D. For FFHQ, the training process takes around 8 days, where learning the 2D branch takes 5 days and jointly training the whole framework takes additional 3 days. For AFHQ-v2, we finetune the 2D branch initially trained on FFHQ for 1 day, then jointly train the whole framework for extra 3 days. Adaptive data augmentation~\cite{karras2020training} is applied to AFHQ-v2 to facilitate training with limited data. 
See the \emph{suppl. material} for more details.

% \paragraph{Metrics}


\subsection{Visual Results} \label{sec:visual}
Figure~\ref{fig:visual} shows the multiview images generated by our 3D branch generator. It can produce high-quality images with fine details at a resolution of $512^2$. Moreover, the images are of strict 3D consistency across different views via directly rendering the generated high-resolution NeRF. More results are in Fig.~\ref{fig:geo}, \ref{fig:compare}, and the \emph{suppl. material}.

% Moreover, the images are of strict 3D consistency across different views via directly rendering the generated high-resolution NeRF. To better visualize the 3D consistency, we plot the spatiotemporal texture images following~\cite{xiang2022gram}. The spatiotemporal textures are obtained by stacking the pixels of a fixed line segment under continuous camera change, very similar to the Epipolar Line Images~\cite{bolles1987epipolar}, where smoothly tilted strips indicate better 3D consistency.


\subsection{Comparison with Prior Arts}
\paragraph{Baselines.} We compare our method with existing 3D-aware GANs, including methods leveraging 2D super-resolution: StyleSDF~\cite{or2022stylesdf}, VolumeGAN~\cite{xu20223d}, StyleNeRF~\cite{gu2021stylenerf}, and EG3D~\cite{chan2021efficient}; and methods with direct 3D rendering: GRAM~\cite{deng2022gram}, GRAM-HD~\cite{xiang2022gram}, GMPI~\cite{zhao2022generative}, EpiGRAF~\cite{skorokhodov2022epigraf}, and VoxGRAF~\cite{schwarz2022voxgraf}.

\paragraph{Qualitative comparison.}
Figure~\ref{fig:geo} shows the visual comparison between our method and EG3D. Our generated images via direct rendering have comparable quality with those generated by EG3D via 2D super-resolution. We further visualize the 3D geometry and the spatiotemporal texture images~\cite{xiang2022gram} of the two methods. The geometry is extracted via Marching Cubes~\cite{lorensen1987marching} on the density field at $512^3$ resolution. The spatiotemporal textures are obtained by stacking the pixels of a fixed line segment under continuous camera change, very similar to the Epipolar Line Images~\cite{bolles1987epipolar}, where smoothly tilted strips indicate better 3D consistency. As shown, our geometries contain finer details in that we directly learn the NeRF of a subject at high resolution. Our spatiotemporal textures are also more reasonable with fewer twisted patterns, thanks to the direct 3D rendering for image synthesis instead of using a black-box 2D super-resolution module.

Figure~\ref{fig:compare} compares our method with other 3D baselines on FFHQ at $512^2$ resolution. Visually inspected, our 3D branch produces images of higher fidelity compared to existing methods leveraging direct 3D rendering. More analysis and video results can be found in the \emph{suppl. material}.



% Figure~\ref{fig:plane} further compares the difference between our generated tri-planes and those of EG3D, where we visualize the L2 norm of each spatial location on the three planes. EG3D leverages 2D convolutions to obtain the three orthogonal planes. Its $xy$ planes dominate the representation and the other two planes are very similar across different instances. By contrast, by leveraging the 3D-aware convolutions, our generated planes depicting the side-views are more informative and better show the characters of different instances (\eg see the difference of the profiles on the $yz$ planes). Our frontal planes (\ie $xy$ planes) also demonstrate more clear head silhouettes compare to EG3D. 

% We illustrate the triplane representation in Fig.~\ref{fig:plane} and compare it with that of EG3D, where we demonstrate L2 norm of each spatial position. Since the FFHQ dataset contains a high proportion of front-face data, the $xy$ plane dominates the representation. EG3D uses a traditional generator designed for 2D image generation and shows light substance on $yz,zx$ planes. That is, the EG3D planes are individually generated and self-unconstrained, which is hard to represent a strict 3D structure. Our 3D-aware generator improves the triplane representation with across-plane interaction. As shown in the right of Fig.~\ref{fig:plane}, our triplane representation is more reasonable and balanced. For example, despite only a few side-face data in FFHQ, our $yz$-plane can present a side-view face representation. %To quantitatively evaluate the triplane quality, we calculate the information entropy of triplanes as shown in Table~\ref{tab:entropy}. Thanks to our 3D-aware triplane generator, the entropy is significantly improved, indicating that our representation is more elaborated for rendering. 

\vspace{-0.4cm}
\paragraph{Quantitative comparison.}
Table~\ref{tab:render} and Fig.~\ref{fig:teaser} show the quantitative results of different methods in terms of image generation quality and 3D consistency.
For image generation quality, We calculate the \'Frechet Inception Distance
(FID) \cite{heusel2017gans} between 50K generated images and all available real images in the training set. For 3D consistency, we follow GRAM-HD~\cite{xiang2022gram} to generate multiview images of 50 random subjects and train the multiview reconstruction method NeuS~\cite{wang2021neus} on each of them. We report the average PSNR and SSIM scores between our generated multiview images and the re-rendered images of NeuS (denoted as PSNR$_{mv}$ and SSIM$_{mv}$). Theoretically, better 3D consistency facilitates the 3D reconstruction process of NeuS, thus leading to higher PSNR and SSIM. 

As shown, our 2D branch generator demonstrates better results compared to EG3D in all metrics across different datasets, thanks to our 3D-aware stream in the tri-plane generator. Moreover, with the 3D-to-2D imitation strategy, our 3D branch generator largely improves the image generation quality among methods using direct 3D rendering, while maintaining competitive 3D consistency. Its image quality even surpasses most of the methods with 2D super-resolution and comes very close to that of EG3D.

% We first compare our 2D branch with prior arts, where we use the 3D-aware generator and 2D super-resolution module to generate images. As shown in Table~\ref{tab:render}, we outperform compared methods by a remarkable margin and set a new record on the FID score. 

\inputtable{tab_sr}

\inputfigure{fig_imitation}

\subsection{Ablation Study} \label{sec:ablation}
We conduct ablation studies to validate the efficacy of our proposed 3D-to-2D imitation and the 3D-aware tri-plane generator. For efficiency, all experiments are conducted on FFHQ dataset at $256^2$ resolution.

\vspace{-0.1cm}
\paragraph{3D-to-2D imitation strategy.} As shown in Tab.~\ref{tab:sr} and Fig.~\ref{fig:sr}, We start from a generator without using the 3D-to-2D imitation and the 3D super-resolution module $\mathcal{S}^{3D}$ (setting A), by directly rendering the coarse tri-planes $\mathbf{P}^c$ for image synthesis. The rendered images in this way are blurry and lack fine details, leading to a high FID score of $30.6$. Naively introducing the imitation loss (setting B) to improve the rendered images of $\mathbf{P}^c$ has minor influence, as the capacity of the coarse tri-planes are limited. Further incorporating the 3D super-resolution module (setting C) effectively releases the potential of the imitation loss and largely improves the image generation quality in terms of FID. However, the rendered images still lack rich details limited by the 3D-inconsistent 2D branch supervisions. Then, if the imitation loss is replaced with the adversarial loss (setting D), the image quality decreases significantly. This is due to that we only render small image patches to compute the corresponding losses for memory consideration. Under this circumstance, the adversarial loss is less stable compared to the imitation loss which is a perceptual-level reconstruction loss. This reveals the advantage of our imitation strategy, which could be extended to higher resolution via patch-wise optimization while maintaining a good image generation quality. Finally, leveraging all the three components (setting E) yields the best result, where the imitation loss keeps the overall structure reasonable and the adversarial loss helps with fine details learning.

% To explore the 3D-to-2D limitation strategy, we split model and loss designs in Table~\ref{tab:sr}. We first get rid of EG3D's $\mathcal S^{2D}$ and directly render a $256\times 256$ image as the baseline. We then train our model with 3D-to-2D imitation. Referring to Table~\ref{tab:sr}, $\mathcal S^{3D}$ and the generated dual triplane representation is important. Moreover, LPIPS loss produces a pleasing improvement to the $11.19$ FID score. However, because of the multi-view inconsistency of $\mathcal S^{2D}$ results, the performance of naive 3D-to-2D imitation is limited. Also, patch GAN breaks down without LPIPS loss since patch-based discrimination could harm the learned data distribution. Finally, our full pipeline achieves the best FID score.


\inputtable{tab_aware3d}

\inputfigure{fig_plane}

\vspace{-0.3cm}
\paragraph{3D-aware tri-plane generator.} Table~\ref{tab:aware3d} shows the ablation study on the 3D-aware tri-plane generator. We compare our design with two alternatives and one without 3D-aware convolutions originally adopted by EG3D. We report the parameter size of the tri-plane generators, the inference memory cost to generate the coarse tri-planes, as well as the final image generation quality in terms of FID. In the first alternative, we remove our 3D-aware stream, and leverage 3D-aware convolutions for the latent feature maps in the main stream, namely \textit{3D-aware latent}. Since the main stream feature maps have relatively larger feature channels, and the 3D-aware convolution requires to concatenate two additional tensors with the same size as the input tensor, this design increases the parameter size and memory consumption significantly, and raises the out-of-memory issue during training. In the second alternative, namely \textit{3D-aware tri-plane}, we directly apply 3D-aware convolutions in the output stream, by inserting them after the upsampling operations at each resolution, instead of using the additional 3D-aware stream. This strategy leads to an improvement of the image generation quality of the 2D branch, and largely reduces the parameter size and memory cost compare to the first design. Finally, our 3D-aware stream design further improves the image generation quality without introducing extra parameters and memory costs. Therefore, we adopt it as our final 3D-aware tri-plane generator for 3D-to-2D imitation. It effectively lowers the FID score of both the 2D and 3D branches compared to the original structure without 3D-aware convolutions, with only a minor increase of the parameter size and memory cost.

Figure~\ref{fig:plane} further shows the synthesized tri-planes, where we visualize the L2 norm of each spatial location on the three orthogonal planes. Our method leveraging the 3D-aware stream produces more informative tri-planes. The generated planes of the side-views better depict the characters of different instances (\eg, see the difference of the profiles on the $yz$ planes). Our frontal planes (\ie $xy$ planes) also demonstrate more clear head silhouettes compared to those without using the 3D-aware convolutions. 


% We report three structures for the ablation study of 3D-aware triplane generator in Table~\ref{tab:aware3d}, where Inference mem. indicates the memory occupation for rendering a $64\times 64$ image. First, following Rodin \cite{wang2022rodin}, we apply 3D-aware convolution to the latent space, called 3D-aware latent. As shown in Table~\ref{tab:aware3d}, the 3D-aware convolution induces a significant increase in memory and parameter size. As a result, the training of 3D-aware latent suffers from the out-of-memory issue. Therefore, we next apply 3D-aware convolution to the triplane space, called 3D-aware triplane, to obtain affordable computation costs. That is, 3D-aware convolution is applied as post-processing for multi-resolution triplanes, which leads to a remarkable improvement in the FID score. Finally, we design a 3D-aware branch as shown in Fig~\ref{fig:arch}(b). Compare to EG3D, the 3D-aware branch lead to a 3.91 FID score with on-par memory cost and parameter size.



%\inputtable{tab_plane}




