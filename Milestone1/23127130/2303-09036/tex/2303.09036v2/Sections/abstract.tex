\begin{abstract}
Generating images with both photorealism and multiview 3D consistency is crucial for 3D-aware GANs, yet existing methods struggle to achieve them simultaneously. Improving the photorealism via CNN-based 2D super-resolution can break the strict 3D consistency, while keeping the 3D consistency by learning high-resolution 3D representations for direct rendering often compromises image quality. In this paper, we propose a novel learning strategy, namely 3D-to-2D imitation, which enables a 3D-aware GAN to generate high-quality images while maintaining their strict 3D consistency, by letting the images synthesized by the generator's 3D rendering branch mimic those generated by its 2D super-resolution branch. We also introduce 3D-aware convolutions into the generator for better 3D representation learning, which further improves the image generation quality. With the above strategies, our method reaches FID scores of 5.4 and 4.3 on FFHQ and AFHQ-v2 Cats, respectively, at 512$\times$512 resolution, largely outperforming existing 3D-aware GANs using direct 3D rendering and coming very close to the previous state-of-the-art method that leverages 2D super-resolution. Project website: \url{https://seanchenxy.github.io/Mimic3DWeb}.

% A key challenge for generative 3D representation is to capture an elaborate and detailed 3D structure for high-resolution 3D-consistent rendering. Previous work reports triplane-based representation and shows significant efficiency in 3D-aware generation. Yet, prohibited by conventional model design and GAN training, the triplane is usually self-unconstrained in 3D space and fails with high-resolution rendering. This work aims to improve the triplane representation for 3D generative tasks. Motivated by the idea of 3D spatial alignment, we design a two-stream 3D-aware network to enhance the triplane representation with cross-plane interaction, resulting that the self-constrained 3D space can be modeled. Furthermore, we propose a dual triplane representation for high-resolution rendering, where coarse and detailed triplanes are employed to model 3D details. To optimize the dual triplane representation, we explore a path-based GAN training to achieve an affordable GPU memory budget. As a result, we can directly render high-resolution images without the aid of 2D super-resolution module, leading to high-quality 3D-consistent results under continuous pose variation.
\end{abstract}