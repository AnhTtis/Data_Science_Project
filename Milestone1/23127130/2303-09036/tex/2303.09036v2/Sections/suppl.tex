

\section{More Implementation Details}

\subsection{Network Structure}
Figure~\ref{fig:net} illustrates our network designs, including the 3D super-resolution module $\mathcal S^{3D}$ and the 3D-aware block in the tri-plane generator $\mathcal{E}$.

%For $\mathcal S^{2D}$ (Fig.~\ref{fig:net}(a)), we follow the design of EG3D~\cite{chan2021efficient}, where two synthesis blocks are used to upsample the feature from neural rendering and linear convolutions is employed to generate RGB images.

For $\mathcal S^{3D}$ (Fig.~\ref{fig:net}(a)), we use two modulated 2D convolution blocks~\cite{karras2020analyzing} to upsample the tri-planes.

For the 3D-aware block (Fig.~\ref{fig:net}(b)), we re-organize the tri-planes according to Fig. 4 in the main text, and apply modulated 2D convolutions for each of the three planes. We use different affine layers to generate style codes for the three modulated convolutions, respectively.

\subsection{Training Details}
We randomly sample latent code $z$ from the normal distribution and camera pose $\boldsymbol\theta$ from those of the training datasets to synthesize fake images, following EG3D~\cite{chan2021efficient}. For each viewing ray, we sample 96 points to calculate the volume rendering equation, including 48 points with stratified sampling and 48 points with importance sampling. The learning rates of the generator and the two discriminators are set to 0.0025 and 0.002, respectively. We train the 2D branch with 25M images in total, and then jointly train the whole framework with additional 15M images. The batch size during training is set as 32.
Other training settings are identical to those of EG3D \cite{chan2021efficient}. 
% Specifically, to render a fake image, we randomly sample latent code $z$ from the normal distribution and camera pose $\boldsymbol\theta$ from those of the training datasets. We sample 96 queries along a ray casting, half of which are from importance sampling based on coarse depth. 

\subsection{Patch Scale}
To reduce GPU memory costs and enable training at high resolution, we render $64^2$ patches for the 3D-to-2D imitation. Thus, the patch scale is $1/4$ or $1/8$ of the whole image for the $256^2$ or $512^2$ experiments, respectively. The patch center is uniformly sampled from the whole image space.

\subsection{The necessity of 2D super-resolution module}
The function of the 2D super-resolution in the 2D branch is to provide stable and high-quality guidance for the 3D branch. 
Previous studies have attempted to directly learn in 3D space without 2D super-resolution via the adversarial loss. However, due to the restriction of modern GPU memory, they either adopted more efficient 3D representations (\eg, radiance manifolds~\cite{deng2022gram} or MPI~\cite{zhao2022generative}) or used patch-wise loss (\eg, EpiGRAF \cite{skorokhodov2022epigraf}), yet these strategies often lead to worse diversity and image quality due to the instability of the GAN loss. By contrast, our imitation with the 2D branch via LPIPS loss provides stable gradients for learning the 3D representation, and thus supports patch-wise training without sacrificing the generation quality, which is the key to our superior results. Furthermore, our strategy also avoids troublesome training tricks (\eg, the annealed strategy in EpiGRAF \cite{skorokhodov2022epigraf}) thus easier to be adapted to other frameworks. 

\subsection{Training time/memory of 3D-to-2D imitation}
Our method requires 31 GB memory at $256^2$ resolution with a batch size of 32 when trained on 8 GPUs, compared to 27 GB memory without the 3D-to-2D imitation. Also, our training time is 1.5 times longer than that of EG3D.

\section{More Results and Comparisons}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{Images/car.pdf}
\caption{Comparison with EG3D on ShapeNet-Cars.}
\label{fig:car}
\end{center}
\vspace{-0.3cm}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{Images/more_geo.pdf}
\caption{Comparison of 2D and 3D branches. (Zoom in for better visualization.)}
\label{fig:moregeo}
\end{center}
% \vspace{-0.1cm}
\end{figure}

\subsection{End-to-end 3D-to-2D imitation learning}
Our initial motivation for the two-stage training is to leverage the powerful prior of an existing 2D generator (with 2D super-resolution) to guide our 3D branch. In fact, the overall framework (including both 2D and 3D branches) can be trained end-to-end from scratch. We conduct a simple experiment on FFHQ at $256^2$ with identical hyper parameters as described in the main paper and achieve an FID of $5.03$ for the 3D branch, which is comparable to the two-stage training result.

\subsection{More results on faces}
Figures~\ref{fig:mtconst} and \ref{fig:mtcomp} illustrate more visual comparisons. Compared to EG3D \cite{chan2021efficient}, we have more detailed geometry and smoothly tilted strips in spatiotemporal texture images, indicating better 3D consistency. Similar to ours, EpiGRAF and GMPI also generate high-resolution images via direct rendering. Yet, we have superior image quality as shown in Fig.~\ref{fig:mtcomp}.

Figures~\ref{fig:mtface} and \ref{fig:mtcat} show more of our results on FFHQ and AFHQ -v2 Cats datasets, respectively.

\textit{\textbf{Referring to the supplemental video for animations.}}

\subsection{Results on general objects.}
Our method can handle general objects with wider range of camera views. In Fig.~\ref{fig:car}, we compare our 3D branch with EG3D on ShapeNet-Cars ($128^2$) and achieve comparable image generation quality.

\subsection{Comparison of our 2D and 3D branches}

Our 3D branch can generate fine details comparable to the 2D branch. In Fig.~\ref{fig:moregeo} (red arrows), we show details produced by the 3D branch that are not visible in the 2D branch.

Our 3D branch clearly produces finer geometry details compared to the alternatives with 2D super-resolution (see Fig.~\ref{fig:moregeo}). As shown, the finer geometry details are not random noises but features of hair, teeth, wrinkles, etc (purple arrows). Furthermore, we can generate diverse nose shapes (yellow arrows), complex jaws with beards (green arrows), and wrinkles (blue arrows) on the geometries.

\input{Figures/fig_fail}

\section{Limitations and Future Works}

We thoroughly discuss the limitations of our method and possible future improvements. 

First, our learned 3D branch still has inferior image quality in terms of FID compared to the 2D branch. This may come from the current design of the 3D super-resolution module and the learning strategy. Specifically, our 3D super-resolution module adopts a similar structure to that of the 2D branch in order for a fair comparison, which may not be the optimal solution. More advanced structures, including leveraging 3D-aware convolutions could be further explored for better 3D super-resolution. Besides, the LPIPS loss during 3D-to-2D imitation leverages a pre-trained VGG network which is trained on images of $224^2$ resolution. It may not well capture the perceptual information of a small image patch. Leveraging more recent pre-trained models~\cite{he2020momentum,radford2021learning} or even multiple feature extractors could be a possible choice. Exploring better discriminators for the patch-level adversarial loss in the 3D branch could also benefit the training process.

Second, our method can produce incorrect geometries in certain cases. As shown in Fig.~\ref{fig:fail}, a typical failure case is geometry discontinuity, where the face region is not smoothly connected with the head region, leading to obvious artifacts at side views. These artifacts also occur in the original EG3D. We believe this problem can be alleviated by introducing more profile images for training, as currently the training data are mostly frontal images so that the planes for depicting side-view features may not be well-trained. In addition, certain generated geometry structures such as hairs and cat whiskers are stuck to the surfaces instead of correctly floating in the volumetric space, as shown in Fig.~\ref{fig:mtface} and~\ref{fig:mtcat}. We conjecture this is due to that the random sampling strategy with limited queries during volume rendering is hard to model thin structures, as also indicated by a previous method~\cite{deng2022gram}. Therefore, a more advanced 3D representation that could efficiently capture these complex structures is worthy of ongoing exploration.

Finally, our training strategy also requires training the 2D branch in advance, which increases the overall training time compared to the original EG3D. A possible way to reduce the training time is to jointly train the 2D and 3D branches from scratch. We leave it for our future work.


% 3D-to-2D imitation learning leverages a pre-trained VGG network to compute LPIPS loss. The network is pre-trained on $224^2$ images, which may not well capture the perceptual information of a small image patch with high-frequency details. To tackle this issue, using features in the discriminator for LPIPS loss could be an alternative way.

% Another limitation is that the 2D branch is the upper bound of 3D branch in terms of FID score under the setting of 3D-to-2D imitation. Therefore, we plan to explore a better training strategy, where the 3D branch can learn from 2D branch while at the same time getting rid of the restriction of 2D branch.

% As shown in Fig.~\ref{fig:fail}, a typical failure case is geometry discontinuity, which could split the front face with the head and lead to errors in multiview rendering. This issue is caused by the imbalance of data distribution. That is, there is a small amount of side-face data in FFHQ, so the tri-plane representation is dominated by the front plane while the side plane is not well trained. Also, certain geometry structures such as hairs and cat whiskers are stuck to the surfaces instead of correctly floating in the volumetric space. This is due to that the ray marching method is hard to model thin structures with limited queries. Therefore, an improved geometry representation is worthy of ongoing exploration.


% \clearpage

\input{Figures/fig_net}

\input{Figures/fig_mtconst}

\input{Figures/fig_mtcomp}

\input{Figures/fig_mtface}

\input{Figures/fig_mtcat}
