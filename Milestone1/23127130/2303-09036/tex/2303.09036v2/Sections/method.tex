\inputfigure{fig_arch}

\section{Approach}

Given a collection of 2D images, we aim to learn a 3D-aware generator $G$ for free-view image synthesis. The generator takes a random code $\bm z \in \mathbb{R}^{d_z}$ and an explicit camera pose $\bm \theta \in \mathbb{R}^{d_\theta}$ as input, and generates a 2D image $I$:
\begin{equation}
    G: (\bm z,\bm \theta)\in\mathbb{R}^{d_z}\times \mathbb{R}^{d_\theta}\rightarrow I \in \mathbb{R}^{H\times W \times 3}.
\end{equation}
To enable high-quality image synthesis, we adopt EG3D~\cite{chan2021efficient} as the backbone of the generator, which synthesizes low-resolution feature fields via the tri-plane representation~\cite{chan2021efficient}, and leverages 2D super-resolution for high-resolution image generation (Sec.~\ref{sec:eg3d}). Based on EG3D, we propose a 3D-to-2D imitation strategy to synthesize high-resolution NeRF for 3D-consistent image rendering. We leverage a 3D super-resolution branch to predict high-resolution tri-planes from the low-resolution ones, and force the rendered images from the former to mimic the images generated by the 2D super-resolution branch (Sec.~\ref{sec:hr}). In addition, we introduce 3D-aware convolution~\cite{wang2022rodin} to the generator for better tri-plane learning via cross-plane communications, which helps to further improve the image generation quality (Sec.~\ref{sec:aware3d}). The overview of our method is illustrated in Fig.~\ref{fig:arch}. We describe each part in detail below.


% Sampling a vector $\mathbf z$ from the latent space, we aim to synthesize 3D-consistent high-resolution multiview images under continuous camera poses $\boldsymbol\theta$ with volume rendering method (Sec.~\ref{sec:nr}). To this end, we first propose a 3D-to-2D imitation method to train the triplane representation that has the capability for high-resolution rendering (Sec.~\ref{sec:hr}). Then, we propose a 3D aware triplane generator to improve the triplane representation with cross-plane constrained 3D information (Sec.~\ref{sec:aware3d}). The overview of our proposed method is illustrated in Fig.~\ref{fig:arch}(a).

\subsection{Preliminaries: EG3D}
\label{sec:eg3d}

EG3D adopts a StyleGAN2-based~\cite{karras2020analyzing} generator $\mathcal E$ to efficiently synthesize the low-resolution feature field of a subject. The feature field is represented by the tri-plane representation which consists of three orthogonal 2D planes produced by reshaping the output feature map of $\mathcal E$, given the latent code $\bm z$ as input. For a point $\bm x \in \mathbb{R}^3$ in the 3D space, its corresponding feature $\mathbf f$ can be obtained by projecting itself onto the three planes $\mathbf{P}_{xy},\mathbf{P}_{yz},\mathbf{P}_{zx}$, and summing the 
retrieved features $\mathbf{f}_{xy},\mathbf{f}_{yz},\mathbf{f}_{zx}$. A small MLP $\mathcal{M}$ then maps this intermediate feature to volume density $\sigma \in \mathbb{R}$ and color feature $\bm c \in \mathbb{R}^{d_c}$ (the first three dimensions represent $RGB$ color), forming the low-resolution feature field:
\begin{equation}
\label{eq:decoder}
\begin{array}{l}
\mathcal{M}: \mathbf{f}\in\mathbb R^{d^{\mathbf f}} \rightarrow (\bm c, \sigma)\in\mathbb R^{d^c}\times\mathbb{R}.
\end{array}
\vspace{-0.1cm}
\end{equation}
To generate high-resolution images, EG3D enforces volume rendering~\cite{kajiya1984ray, mildenhall2020nerf} to render the above feature field to a low-resolution feature map $C$, where each pixel value $C(\bm r)$ corresponding to a viewing ray $\bm r$ can be obtained via
\begin{equation}
\small
\label{eq:render}
	C({\bm r})  
	=   \sum_{i=1}^{N}T_i(1-{\rm exp}(-\sigma_i\delta_i))\bm c_i, T_i={\rm exp}(-\sum_{j=1}^{i-1}\sigma_j\delta_j).
\end{equation}
Here, $i$ is the index of points along ray $\bm r$ sorted from near to far, and $\delta$ is the distance between adjacent points. Then, the rendered feature map $C$ is sent to a 2D super-resolution module $\mathcal S^{2D}$ consisting of several StyleGAN2-modulated convolutional layers to generate the final image~$I^{2D}$.

Although EG3D can generate free-view images of high quality, it cannot well maintain their 3D consistency across different views. This is inevitable due to incorporating the black-box CNN-based 2D super-resolution module, which breaks the physical rules of the volume rendering process. Despite that EG3D further proposes a dual-discrimination~\cite{chan2021efficient} strategy to force the high-resolution images to be consistent with their low-resolution counterparts, detail-level 3D inconsistency (\ie texture flickering) still cannot be eliminated. During continuous camera variation, these artifacts can be easily captured by human eyes, differing the synthesized results from a real video sequence. To maintain the 3D consistency meanwhile keep the high-quality image generation to the maximum extent, we propose a 3D-to-2D imitation strategy described below.


% We use the triplane representation $\mathbf P$ to model a volume. For a ray casting, we sample $N$ queries $\{\mathbf q_i\}_{i=1}^{N}$, whose features are embedded in a triplane $\mathbf P$, \ie, $\mathbf{f_q}=\mathbf{P[q]}\in\mathbb{R}^{32}$, where where $[\cdot]$ denotes grid sampling. We use an MLP-based decoder to obtain density $\sigma$ and color $\mathbf c$ as
% \begin{equation}
% \label{eq:decoder}
% \begin{array}{l}
% \mathcal{M}: \mathbf{f}\in\mathbb R^{32} \rightarrow (\mathbf c, \sigma)\in\mathbb R^4
% \end{array}
% \vspace{-0.1cm}
% \end{equation}
% We render neural fields with the volumetric method ~\cite{mildenhall2020nerf}:
% % \vspace{-0.1cm}
% \begin{equation}
% \label{eq:render}
% \begin{array}{l}
% \hat{\mathbf c}=\sum_{i=1}^N T_{i}(1-\exp(-\sigma_{i} \delta_{i})) \mathbf{c}_i \\
% T_i=\exp(-\sum_{j=1}^{i-1} \sigma_j \delta_j),
% \end{array}
% \vspace{-0.1cm}
% \end{equation}
% where $\delta$ is the distance between adjacent samples and $\hat{\mathbf c}$ is the RGB color of a pixel. By sampling from full view directions, we can render a fake image $\hat I$.

\subsection{3D-to-2D Imitation}
\label{sec:hr}
To keep the strict 3D consistency, a better way is to directly render the 3D representation instead of resorting to a 2D CNN for image synthesis. 
% However, even with the efficient tri-plane representation, rendering an image at a resolution beyond $256\times256$ is still unaffordable during training. 
% Some more sparse representations such as radiance manifolds~\cite{deng2022gram,deng2022learning} and multi-plane images~\cite{zhao2022generative} cannot well generalize to large viewing range and often sacrifice the overall generation quality. Other sparse representations, such as hash tables~\cite{muller2022instant} or volumetric primitives~\cite{lombardi2021mixture}, remain unclear whether can be applied to the 3D-aware GAN scenario. 
% EpiGRAF~\cite{skorokhodov2022epigraf} alleviates this problem via patch-wise adversarial learning to avoid rendering the whole image, yet this strategy also brings large quality drop compare to original EG3D. 
Noticing that the images generated by EG3D contain rich details, it is natural to use them as guidance for images synthesized by direct 3D rendering. If the directly-rendered images well mimic those fine details, their quality should get very close to that of EG3D. Meanwhile, since they are rendered from a continuous 3D representation, their 3D consistency across different views should be trivially maintained. This motivates us to design the 3D-to-2D imitation strategy, as depicted in Fig.~\ref{fig:arch}.

Specifically, we introduce a 3D super-resolution module $\mathcal S^{3D}$ to generate residual tri-planes $\mathbf{P}^r$ from the coarse tri-planes $\mathbf{P}^c$ produced by the tri-plane generator $\mathcal{E}$:
\begin{equation}
\small
    \mathcal S^{3D}: \mathbf{P}^c \in\mathbb{R}^{3\times H^c \times W^c\times d^{\mathbf{f}}}\rightarrow \mathbf{P}^r \in \mathbb{R}^{3\times H^r \times W^r\times d^{\mathbf{f}}}.
\end{equation}
The $\mathcal S^{3D}$ adopts several StyleGAN2-modulated convolutional layers conditioned on a latent code $\bm w$ mapped from the random code $\bm z$, similar to the 2D super-resolution module $\mathcal S^{2D}$ in EG3D. The difference is that $\mathcal S^{3D}$ conducts super-resolution on the triplane-based 3D representation instead of the rendered 2D feature map. In this way, we can generate a high-resolution 3D field for direct 3D rendering. Given the coarse and residual tri-planes (\ie $\mathbf{P}^c$ and $\mathbf{P}^r$), we obtain a more detailed intermediate feature $\mathbf{f} = \mathbf{f}^c + \mathbf{f}^r$ for a 3D point $\bm x$, and further obtain the high-resolution feature field by sending the intermediate feature into the MLP-based decoder $\mathcal{M}$ following Eq.~\eqref{eq:decoder}. The first three feature dimensions of the field derive the high-resolution NeRF for rendering 3D-consistent fine image $I^{3D}$ via Eq.~\eqref{eq:render}.

To ensure that $I^{3D}$ contains reasonable geometry structure with rich texture details, we let it to mimic the contents of $I^{2D}$ generated by the 2D branch $\mathcal{S}^{2D}$. For a pair of $I^{3D}$ and $I^{2D}$ synthesized with the same latent code $\bm z$ and camera pose $\bm \theta$, we enforce imitation loss between them to guarantee their perceptual similarity:
\begin{equation}
\label{eq:imitation}
% \vspace{-0.2cm}
\begin{array}{l}
\mathcal L_{imitation}=\mathrm{LPIPS}(I^{3D}, \mathrm{sg}(I^{2D})),
\end{array}
\vspace{-0.1cm}
\end{equation}
where $\mathrm{LPIPS}(\cdot,\cdot)$ is the perceptual loss defined in~\cite{zhang2018unreasonable}, and $\mathrm{sg}$ denotes stopping gradient to avoid undesired influence of $I^{3D}$ on the 2D branch. This process is very similar to a standard multiview reconstruction process. During training, $I^{2D}$ sharing the same code $\bm z$ are generated under different camera views from a statistical aspect, forming the multiview supervision. The high-resolution NeRF from the 3D branch renders $I^{3D}$ under the same camera views to compare with the multiview data for 3D reconstruction. Considering that $I^{2D}$ are nearly 3D-consistent, they should help to learn a reasonable NeRF for 3D-consistent image rendering. 

Nevertheless, since $I^{2D}$ are not strictly 3D-consistent, faithfully reconstructing their image contents leads to blurry results where the texture details across different views are averaged out. Therefore, we further introduce the non-saturating GAN loss with R1 regularization~\cite{mescheder2018training} between $I^{3D}$ and real images $\hat{I}$ to maintain the high-frequency details:
\begin{equation}
\label{eq:adv_3d}
\begin{aligned}
\mathcal{L}_{adv}^{3D}& = \mathbb{E}_{\boldsymbol{z} \sim p_z, \boldsymbol{\theta} \sim p_\theta}[f(D^{3D}(G^{3D}(\boldsymbol{z}, \boldsymbol{\theta})))] \\
& +\mathbb{E}_{\hat{I} \sim p_{real}}[f(-D^{3D}(\hat{I}))+\lambda\|\nabla D^{3D}(\hat{I}))\|^2],
\end{aligned}
\vspace{-0.1cm}
\end{equation}
where $f(u)=\log(1+\exp{(u)})$ is the Softplus function, $G^{3D}$ including $\{\mathcal{E},\mathcal{M},\mathcal{S}^{3D}\}$ is the 3D rendering branch of the generator, and $D^{3D}$ is the corresponding discriminator.



An advantage of the above imitation learning is that we can render small patches (\ie $64\times64$) to compute Eq.~\eqref{eq:imitation} and Eq.~\eqref{eq:adv_3d}, as shown in Fig.~\ref{fig:arch}, with only minor influence to the final image quality. 
This largely reduces the memory cost during training and enables learning the 3D branch at high resolution (\eg $512\times512$). By contrast, solely applying adversarial loss at patch-level often leads to large quality drops as shown in previous methods~\cite{schwarz2020graf, skorokhodov2022epigraf} and Tab.~\ref{tab:sr}.


% Prohibited by modern GPU memory, the 3D-aware GAN pipeline usually follows a small-size rendering paradigm, and a 2D super-resolution module is used to elevate image resolution. Despite high image quality, the super-resolution in image space introduces 3D inconsistency, \eg, texture flickering. 

Finally, we apply image-level adversarial loss to the 2D branch following EG3D to ensure that $I^{2D}$, as the supervision for the 3D branch, are of high quality:

\begin{equation}
\label{eq:adv_2d}
\begin{aligned}
\mathcal{L}_{adv}^{2D}& = \mathbb{E}_{\boldsymbol{z} \sim p_z, \boldsymbol{\theta} \sim p_\theta}[f(D^{2D}(G^{2D}(\boldsymbol{z}, \boldsymbol{\theta})))] \\
& +\mathbb{E}_{I \sim p_{real}}[f(-D^{2D}(\hat{I}))+\lambda\|\nabla D^{2D}(\hat{I}))\|^2],
\end{aligned}
\vspace{-0.1cm}
\end{equation}
where $G^{2D}$ is the 2D branch generator consisting of $\{\mathcal{E},\mathcal{M},\mathcal{S}^{2D}\}$, and $D^{2D}$ is the corresponding discriminator. The same dual discrimination is adopted as done in EG3D. 

Overall, the training objective is 
\begin{equation}
\label{eq:loss}
\mathcal L_{total}=\mathcal{L}_{imitation} + \mathcal{L}_{adv}^{3D}+\mathcal{L}_{adv}^{2D}.
\vspace{-0.1cm}
\end{equation}
In practice, we first learn the 2D branch via $\mathcal{L}_{adv}^{2D}$ to obtain reasonable synthesized images $I^{2D}$, then leverage $\mathcal L_{total}$ to simultaneously learn the 2D and 3D branches for high-quality and 3D-consistent image synthesis.



\subsection{3D-Aware Tri-plane Generator}
\label{sec:aware3d}
As depicted in Sec.~\ref{sec:hr}, the tri-plane generator $\mathcal{E}$ is responsible for synthesizing the coarse tri-planes $\mathbf{P}^c$ shared by both the 2D and 3D branches, which is an important component that would affect the final image generation quality. However, in EG3D, $\mathcal{E}$ takes a StyleGAN2 architecture originally designed for 2D generative tasks. As shown in Fig.~\ref{fig:araware3d}(a), the original tri-plane generator only contains the main stream and the output stream. The tri-planes are obtained from latent feature maps in the main stream via 2D convolutions (\ie $toRGB$ layers), and the latent feature maps are also produced by a serials of 2D synthesis blocks. Consequently, the latent feature maps are forced to learn 3D unaligned features of the three orthogonal planes and the latters also lack feature communications with each other. Inspired by a recent 3D diffusion model~\cite{wang2022rodin}, we introduce 3D-aware convolutions into our tri-plane generator $\mathcal{E}$ to enhance feature communications between 3D-associated positions across different planes, for better tri-plane generation. 

\inputfigure{fig_aware3d}

\inputfigure{fig_geo}

\inputfigure{fig_compare}

Specifically, as illustrated in Fig.~\ref{fig:araware3d}(a), we add an extra 3D-aware stream upon the original output stream after each $toRGB$ layer at different resolutions. At each resolution level $k$, the corresponding tri-planes $\mathbf P_k=[\mathbf P_{k,xy},\mathbf P_{k,yz},\mathbf P_{k,zx}]$, are summed with the tri-planes produced by the original output stream, and further sent into a 3D-aware block to produce tri-plane features for the next level. The 3D-aware block conducts similar operations on each of the three planes. For brevity, we omit the subscript $k$ here and take $\mathbf P_{xy}$ as an example to illustrate the operation process. As shown in Fig.~\ref{fig:araware3d}(b), to align $\mathbf P_{yz}$ and $\mathbf P_{zx}$ towards $\mathbf P_{xy}$, we first perform global pooling along $z$ axis of the former two to obtain $z$-squeezed feature vectors. These vectors are then repeated along the $z$ dimension to restore the original spatial size, denoted as $\mathbf P_{yr}$ and $\mathbf P_{rx}$. In this manner, the obtained $\mathbf P_{yr}$ and $\mathbf P_{rx}$ are aligned with $\mathbf P_{xy}$ from a 3D perspective, \ie, a 2D position $uv$ on $\mathbf P_{xy}$ is responsible for features in region $uvz, z \in [z_{min},z_{max}]$ in the 3D space, meanwhile the same $uv$ position on $\mathbf P_{yr}$ and $\mathbf P_{rx}$ also associate with the features in this 3D region. As a result, we can simply concatenate them along the channel dimension as $[\mathbf P_{xy},\mathbf P_{yr},\mathbf P_{rx}]$, and perform modulated 2D convolution~\cite{karras2020analyzing} on it. The 2D convolution aggregates the 3D-associated features to produce next-level $\mathbf P_{xy}$, leading to better feature communications across the planes. 
$\mathbf P_{yz}$ and $\mathbf P_{zx}$ can be processed similarly.

Note that in~\cite{wang2022rodin}, the 3D-aware convolution is applied in all layers in a U-Net structure. However, in our scenario, leveraging 3D-aware convolution for all layers, especially the main stream, introduces unaffordable memory cost during training, as it would produce multiple auxiliary tensors and triples the channel dimension for each processed latent feature map, as shown in Fig.~\ref{fig:araware3d}(b). Comparing to the latent feature maps in the main stream, the tri-planes after each output layer contains much fewer channels thus more memory-friendly to adopt the 3D-aware convolution. Empirically, our proposed 3D-aware stream helps to learn more reasonable tri-planes and improves the final image generation quality, with only a minor increase in the total memory consumption (see Sec.~\ref{sec:ablation}).



% The triplane representation reported by EG3D \cite{chan2021efficient} proves to be an efficient representation to model a volumetric space. However, in the 3D generative task, the generation of the triplane is less aware of 3D structure. That is, EG3D uses a StyleGAN2~\cite{karras2020analyzing} backbone to produce a feature map, which is originally designed for 2D generative tasks. Then, the feature map is divided along the channel dimension into 3 counterparts as the triplane. In this way, 
% the planes for $xy$, $yz$, and $zx$ spaces are generated with individual convolutional kernels such that they are weakly aligned in 3D spaces.

% To enhance the 3D structure embedded in the triplane, we develop a  3D aware generator as $\mathcal E$. As shown in Fig~\ref{fig:arch}(b), we leverage the $w$ space and modulated convolutions in StyleGAN2 to build the network. Specifically, a latent branch and a 3D-aware branch are designed to generate a 3D-structured triplane. The former is inherited from StyleGAN2 to produce a set of conventional convolutional features $\{\mathbf F_k\}$, while the latter is to generate 3D aware triplane representations $\{\mathbf P_k\}$, where $k=4,8,16,...,256$ denotes the resolution. Therefore, the latent branch models the abstract latent space while the 3D aware branch models triplane space. and they are fused with summation, $\ie$, $\mathbf P_k=\mathbf P_k+\text{conv}(\mathbf F_k)$.

% The core of the 3D-aware branch is 3D-aware convolution \cite{wang2022rodin} that can fuse spatial aligned features. For example, Fig.~\ref{fig:arch}(c) illustrates the convolution process for $xy$-plane. First, a triplane representation $\mathbf P$ is split into $xy$, $yz$, $zx$ spaces, \ie, $\mathbf P_{xy},\mathbf P_{yz},\mathbf P_{zx}$. To align $\mathbf P_{yz}$ and $\mathbf P_{zx}$ spaces towards $\mathbf P_{xy}$, we perform global pooling along $z$ axis to obtain $z$-squeezed feature vectors. Then, the vector is repeated along the original $z$ dimension to obtain the original spatial size, denoted as $\mathbf P_{yr},\mathbf P_{rx}$. In this manner $\mathbf P_{yr},\mathbf P_{rx}$ is 3D-spatially aligned with $\mathbf P_{xy}$, and they are concatenated to evolve $\mathbf P^{xy}$, $\mathbf P_{2k,xy}=\text{conv}([\mathbf P_{k,xy},\mathbf P_{k,yr},\mathbf P_{k,rx}])$. Similarly, $\mathbf P_{yz}$ and $\mathbf P_{zx}$ can be processed with 3D aware convolution.

% Our two-stream design has two-fold advantages over that in \cite{wang2022rodin} in terms of generative tasks. First, three planes are usually with unequal importance, \eg, $xy$ plane usually dominates the model performance in FFHQ data distribution. Therefore, instead of splitting a latent feature into three counterparts with the same channel size like \cite{wang2022rodin}, our latent branch can adaptively assign computation budgets for three inequivalent spaces. Second, as shown in Fig.~\ref{fig:arch}(c), 3D-aware convolution is memory unfriendly by introducing multiple auxiliary tensors. Our 3D-aware operation is conducted on the triplane, which is much more memory-efficient than the latent feature. 

