

\section{More Implementation Details}

\subsection{Network Structure}
Figure~\ref{fig:net} illustrates our network designs, including the 3D super-resolution module $\mathcal S^{3D}$ and the 3D-aware block in the tri-plane generator $\mathcal{E}$.

%For $\mathcal S^{2D}$ (Fig.~\ref{fig:net}(a)), we follow the design of EG3D~\cite{chan2021efficient}, where two synthesis blocks are used to upsample the feature from neural rendering and linear convolutions is employed to generate RGB images.

For $\mathcal S^{3D}$ (Fig.~\ref{fig:net}(a)), we use two modulated 2D convolution blocks~\cite{karras2020analyzing} to upsample the tri-planes.

For the 3D-aware block (Fig.~\ref{fig:net}(b)), we re-organize the tri-planes according to Fig. 4 in the main text, and apply modulated 2D convolutions for each of the three planes. We use different affine layers to generate style codes for the three modulated convolutions, respectively.

\subsection{Training Details}
We randomly sample latent code $z$ from the normal distribution and camera pose $\boldsymbol\theta$ from those of the training datasets to synthesize fake images, following EG3D~\cite{chan2021efficient}. For each viewing ray, we sample 96 points to calculate the volume rendering equation, including 48 points with stratified sampling and 48 points with importance sampling. The learning rates of the generator and the two discriminators are set to 0.0025 and 0.002, respectively. We train the 2D branch with 25M images in total, and then jointly train the whole framework with additional 15M images. The batch size during training is set as 32.
Other training settings are identical to those of EG3D \cite{chan2021efficient}. 
% Specifically, to render a fake image, we randomly sample latent code $z$ from the normal distribution and camera pose $\boldsymbol\theta$ from those of the training datasets. We sample 96 queries along a ray casting, half of which are from importance sampling based on coarse depth. 

\subsection{Patch Scale}
To reduce GPU memory costs and enable training at high resolution, we render $64^2$ patches for the 3D-to-2D imitation. Thus, the patch scale is $1/4$ or $1/8$ of the whole image for the $256^2$ or $512^2$ experiments, respectively. The patch center is uniformly sampled from the whole image space.

\section{More Results and Comparisons}

Figures~\ref{fig:mtconst} and \ref{fig:mtcomp} illustrate more visual comparisons. Compared to EG3D \cite{chan2021efficient}, we have more detailed geometry and smoothly tilted strips in spatiotemporal texture images, indicating better 3D consistency. Similar to ours, EpiGRAF and GMPI also generate high-resolution images via direct rendering. Yet, we have superior image quality as shown in Fig.~\ref{fig:mtcomp}.

Figures~\ref{fig:mtface} and \ref{fig:mtcat} show more of our results on FFHQ and AFHQ -v2 Cats datasets, respectively.

\textit{\textbf{Referring to the supplemental video for animations.}}

\input{Figures/fig_fail}

\section{Limitations and Future Works}

We thoroughly discuss the limitations of our method and possible future improvements. 

First, our learned 3D branch still has inferior image quality in terms of FID compared to the 2D branch. This may come from the current design of the 3D super-resolution module and the learning strategy. Specifically, our 3D super-resolution module adopts a similar structure to that of the 2D branch in order for a fair comparison, which may not be the optimal solution. More advanced structures, including leveraging 3D-aware convolutions could be further explored for better 3D super-resolution. Besides, the LPIPS loss during 3D-to-2D imitation leverages a pre-trained VGG network which is trained on images of $224^2$ resolution. It may not well capture the perceptual information of a small image patch. Leveraging more recent pre-trained models~\cite{he2020momentum,radford2021learning} or even multiple feature extractors could be a possible choice. Exploring better discriminators for the patch-level adversarial loss in the 3D branch could also benefit the training process.

Second, our method can produce incorrect geometries in certain cases. As shown in Fig.~\ref{fig:fail}, a typical failure case is geometry discontinuity, where the face region is not smoothly connected with the head region, leading to obvious artifacts at side views. These artifacts also occur in the original EG3D. We believe this problem can be alleviated by introducing more profile images for training, as currently the training data are mostly frontal images so that the planes for depicting side-view features may not be well-trained. In addition, certain generated geometry structures such as hairs and cat whiskers are stuck to the surfaces instead of correctly floating in the volumetric space, as shown in Fig.~\ref{fig:mtface} and~\ref{fig:mtcat}. We conjecture this is due to that the random sampling strategy with limited queries during volume rendering is hard to model thin structures, as also indicated by a previous method~\cite{deng2022gram}. Therefore, a more advanced 3D representation that could efficiently capture these complex structures is worthy of ongoing exploration.

Finally, our training strategy also requires training the 2D branch in advance, which increases the overall training time compared to the original EG3D. A possible way to reduce the training time is to jointly train the 2D and 3D branches from scratch. We leave it for our future work.


% 3D-to-2D imitation learning leverages a pre-trained VGG network to compute LPIPS loss. The network is pre-trained on $224^2$ images, which may not well capture the perceptual information of a small image patch with high-frequency details. To tackle this issue, using features in the discriminator for LPIPS loss could be an alternative way.

% Another limitation is that the 2D branch is the upper bound of 3D branch in terms of FID score under the setting of 3D-to-2D imitation. Therefore, we plan to explore a better training strategy, where the 3D branch can learn from 2D branch while at the same time getting rid of the restriction of 2D branch.

% As shown in Fig.~\ref{fig:fail}, a typical failure case is geometry discontinuity, which could split the front face with the head and lead to errors in multiview rendering. This issue is caused by the imbalance of data distribution. That is, there is a small amount of side-face data in FFHQ, so the tri-plane representation is dominated by the front plane while the side plane is not well trained. Also, certain geometry structures such as hairs and cat whiskers are stuck to the surfaces instead of correctly floating in the volumetric space. This is due to that the ray marching method is hard to model thin structures with limited queries. Therefore, an improved geometry representation is worthy of ongoing exploration.

\input{Figures/fig_net}

\clearpage

\input{Figures/fig_mtconst}

\input{Figures/fig_mtcomp}

\input{Figures/fig_mtface}

\input{Figures/fig_mtcat}
