\inputfigure{fig_arch}

\section{Approach}

Sampling a vector $\mathbf z$ from the latent space, we aim to synthesize 3D-consistent high-resolution multiview images under continuous camera poses $\boldsymbol\theta$ with volume rendering method (Sec.~\ref{sec:nr}). To this end, we first propose a 3D-to-2D imitation method to train the triplane representation that has the capability for high-resolution rendering (Sec.~\ref{sec:hr}). Then, we propose a 3D aware triplane generator to improve the triplane representation with cross-plane constrained 3D information (Sec.~\ref{sec:aware3d}). The overview of our proposed method is illustrated in Fig.~\ref{fig:arch}(a).

\subsection{Volume Rendering}
\label{sec:nr}
We use the triplane representation $\mathbf P$ to model a volume. For a ray casting, we sample $N$ queries $\{\mathbf q_i\}_{i=1}^{N}$, whose features are embedded in a triplane $\mathbf P$, \ie, $\mathbf{f_q}=\mathbf{P[q]}\in\mathbb{R}^{32}$, where where $[\cdot]$ denotes grid sampling. We use an MLP-based decoder to obtain density $\sigma$ and color $\mathbf c$ as
\begin{equation}
\label{eq:decoder}
\begin{array}{l}
\mathcal{M}: \mathbf{f}\in\mathbb R^{32} \rightarrow (\mathbf c, \sigma)\in\mathbb R^4
\end{array}
\vspace{-0.1cm}
\end{equation}
We render neural fields with the volumetric method ~\cite{mildenhall2020nerf}:
% \vspace{-0.1cm}
\begin{equation}
\label{eq:render}
\begin{array}{l}
\hat{\mathbf c}=\sum_{i=1}^N T_{i}(1-\exp(-\sigma_{i} \delta_{i})) \mathbf{c}_i \\
T_i=\exp(-\sum_{j=1}^{i-1} \sigma_j \delta_j),
\end{array}
\vspace{-0.1cm}
\end{equation}
where $\delta$ is the distance between adjacent samples and $\hat{\mathbf c}$ is the RGB color of a pixel. By sampling from full view directions, we can render a fake image $\hat I$.

\subsection{Training Strategy with 3D-to-2D Imitation}
\label{sec:hr}

Prohibited by modern GPU memory, the 3D-aware GAN pipeline usually follows a small-size rendering paradigm, and a 2D super-resolution module is used to elevate image resolution. Despite high image quality, the super-resolution in image space introduces 3D inconsistency, \eg, texture flickering. 

We propose a training strategy that can achieve a high-quality 3D representation. Following the conventional 3D GAN pipeline, the training model includes a generator $G$ and a discriminator $D$, where $G=\{\mathcal E, \mathcal M, \mathcal S^{2D}\}$ with a triplane generator $\mathcal E$, decoder $\mathcal M$, and a 2D super-resolution module $\mathcal S^{2D}$. Further, we introduce a patch discriminator $D^p$ to learn from high-resolution image patches. In this manner, we limit rendering resolution up to $P=64$ to obtain an affordable memory budget. After training, we get rid of the 2D super-resolution module $\mathcal S^{2D}$ and directly render high-resolution images in the inference. 

Specifically, the strategy is divided into two stages. First, we train $\{\mathcal E, \mathcal M, \mathcal S^{2D}, D\}$ according to EG3D \cite{chan2021efficient}, and use a standard GAN loss and R1 regularization for optimization as follows,
\begin{equation}
\label{eq:gan}
\begin{aligned}
\mathcal{L}_{global}&= \mathbb{E}_{\boldsymbol{z} \sim p_z, \boldsymbol{\theta} \sim p_\theta}[f(D(G(\boldsymbol{z}, \boldsymbol{\theta})))] \\
& +\mathbb{E}_{I \sim p_{real}}[f(-D(I))+\lambda\|\nabla D(I)\|^2]
\end{aligned}
\vspace{-0.1cm}
\end{equation}
where $f(u)=\log(1+\exp{u})$ is the Softplus function, and $I$ denotes real images.

The second training stage aims to transfer the capacity of $\mathcal S^{2D}$ to $\mathcal E$. To this end, we first add an extra synthesis block to $\mathcal E$, noted as $\mathcal E^+$. $\mathcal E^+$ produces a dual triplane representation, where the pre-trained one is called coarse triplane $\mathbf P^{c}$ and the added one is named fine triplane $\mathbf P^{f}$. For a query $\mathbf q$, we sample features from the dual triplane representation as 
\begin{equation}
\label{eq:dp}
\begin{aligned}
&\mathbf{f}_{\mathbf q}^c=\mathbf P^{c}[\mathbf q] \\
&\mathbf{f}_{\mathbf q}^f=\mathbf P^{c}[\mathbf q] + \mathbf P^{f}[\mathbf q].
\end{aligned}
\vspace{-0.1cm}
\end{equation}
Then, $\{\mathcal E^{+}, \mathcal M, \mathcal S^{2D}, D, D^p\}$ is trained in an end-to-end manner with the 3D-to-2D imitation strategy. We use $\mathbf{f}_{\mathbf q}^c$ to render a full fake image $\hat I$ and leverage $\mathcal S^{2D}$ to yield the high-quality version $\hat I^{sr}$ with $F\times F$ resolution. For high-resolution training with a small rendering size $P (P<F)$, we sample queries from a random $P$ view region and use $\mathbf{f}_{\mathbf q}^f$ to render a fake image patch $\tilde I$. We crop the corresponding patch on $\hat I^{sr}$, noted as $\hat I^{sr}_{P}$, that is aligned with $\tilde I$, and let $\tilde I$ to mimic the behavior of 2D super-resolution.  However, $\hat I^{sr}_{P}$ generated by 2D super-resolution fails to perform the multi-view consistency, while $\tilde I$ from volume rendering is under a strict 3D-continuous constraint. Pixel-wise 3D-to-2D imitation could incur a sacrifice of image quality, leading to blurry results. Therefore, we leverage LPIPS loss \cite{zhang2018unreasonable} to  facilitate the imitation at the perceptual level:
\begin{equation}
\label{eq:lpips}
\begin{array}{l}
\mathcal L_{imitation}=\mathrm{LPIPS}(\tilde I, \hat I^{sr}_{P})
\end{array}
\vspace{-0.1cm}
\end{equation}
In addition, we introduce adversarial learning on image patches to further improve image quality. That is, we also crop a random $P\times P$ patch on a random real image $I$, noted as $I_{P}$, and use GAN loss to generate photo-realistic image patch:
\begin{equation}
\label{eq:patch}
\begin{aligned}
\mathcal{L}_{Patch}& = \mathbb{E}_{\boldsymbol{z} \sim p_z, \boldsymbol{\theta} \sim p_\theta}[f(D^p(G^p(\boldsymbol{z}, \boldsymbol{\theta})))] \\
& +\mathbb{E}_{I \sim p_{real}}[f(-D^p(I_{P}))+\lambda\|\nabla D(I_{P}))\|^2]
\end{aligned}
\vspace{-0.1cm}
\end{equation}
where $G^p$ performs a patch volume rendering to produce $\tilde I$.

Overall, the training objective is 
\begin{equation}
\label{eq:loss}
\mathcal L=\mathcal{L}_{global} + \mathcal{L}_{patch}+\mathcal{L}_{imitation}.
\vspace{-0.1cm}
\end{equation}


\subsection{3D Aware Triplane Generator}
\label{sec:aware3d}
The triplane representation reported by EG3D \cite{chan2021efficient} proves to be an efficient representation to model a volumetric space. However, in the 3D generative task, the generation of the triplane is less aware of 3D structure. That is, EG3D uses a StyleGAN2~\cite{karras2020analyzing} backbone to produce a feature map, which is originally designed for 2D generative tasks. Then, the feature map is divided along the channel dimension into 3 counterparts as the triplane. In this way, 
the planes for $xy$, $yz$, and $zx$ spaces are generated with individual convolutional kernels such that they are weakly aligned in 3D spaces.

To enhance the 3D structure embedded in the triplane, we develop a  3D aware generator as $\mathcal E$. As shown in Fig~\ref{fig:arch}(b), we leverage the $w$ space and modulated convolutions in StyleGAN2 to build the network. Specifically, a latent branch and a 3D-aware branch are designed to generate a 3D-structured triplane. The former is inherited from StyleGAN2 to produce a set of conventional convolutional features $\{\mathbf F_k\}$, while the latter is to generate 3D aware triplane representations $\{\mathbf P_k\}$, where $k=4,8,16,...,256$ denotes the resolution. Therefore, the latent branch models the abstract latent space while the 3D aware branch models triplane space. and they are fused with summation, $\ie$, $\mathbf P_k=\mathbf P_k+\text{conv}(\mathbf F_k)$.

The core of the 3D-aware branch is 3D-aware convolution \cite{wang2022rodin} that can fuse spatial aligned features. For example, Fig.~\ref{fig:arch}(c) illustrates the convolution process for $xy$-plane. First, a triplane representation $\mathbf P$ is split into $xy$, $yz$, $zx$ spaces, \ie, $\mathbf P_{xy},\mathbf P_{yz},\mathbf P_{zx}$. To align $\mathbf P_{yz}$ and $\mathbf P_{zx}$ spaces towards $\mathbf P_{xy}$, we perform global pooling along $z$ axis to obtain $z$-squeezed feature vectors. Then, the vector is repeated along the original $z$ dimension to obtain the original spatial size, denoted as $\mathbf P_{yr},\mathbf P_{rx}$. In this manner $\mathbf P_{yr},\mathbf P_{rx}$ is 3D-spatially aligned with $\mathbf P_{xy}$, and they are concatenated to evolve $\mathbf P^{xy}$, $\mathbf P_{2k,xy}=\text{conv}([\mathbf P_{k,xy},\mathbf P_{k,yr},\mathbf P_{k,rx}])$. Similarly, $\mathbf P_{yz}$ and $\mathbf P_{zx}$ can be processed with 3D aware convolution.

Our two-stream design has two-fold advantages over that in \cite{wang2022rodin} in terms of generative tasks. First, three planes are usually with unequal importance, \eg, $xy$ plane usually dominates the model performance in FFHQ data distribution. Therefore, instead of splitting a latent feature into three counterparts with the same channel size like \cite{wang2022rodin}, our latent branch can adaptively assign computation budgets for three inequivalent spaces. Second, as shown in Fig.~\ref{fig:arch}(c), 3D-aware convolution is memory unfriendly by introducing multiple auxiliary tensors. Our 3D-aware operation is conducted on the triplane, which is much more memory-efficient than the latent feature. 

