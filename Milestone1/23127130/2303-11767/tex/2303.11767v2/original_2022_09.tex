\documentclass[letterpaper,11pt]{elsarticle}
%\documentclass[preprint]{elsarticle}
\usepackage{amsmath}
\usepackage{times}
\usepackage{hyperref}
%\usepackage{abstract, fancyhdr,rotating}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{minted}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{listings}
\usepackage{booktabs}

%\usepackage[backend=biber, sorting=none, natbib=true, giveninits=true, doi=false, url=false, isbn=false]{biblatex}
%\usepackage[backend=bibtex]{biblatex}
%\usepackage{biblatex}
%\addbibresource{biblio.bib}

\renewcommand\theFancyVerbLine{\footnotesize\arabic{FancyVerbLine}}


%16pt times new roman postscript font
%\DeclareFixedFont{\Tbfont}{T1}{ptm}{b}{n}{16pt}
\DeclareFixedFont{\Tifont}{T1}{ptm}{b}{it}{16pt}
\usepackage{setspace}
\setlength{\topmargin}{0cm}
\setlength{\oddsidemargin}{-10mm}
\setlength{\evensidemargin}{-10mm}
\setlength{\textheight}{22cm}
\setlength{\textwidth}{18.5cm}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

%\title{{\Tbfont Multi-dimensional Nonsense }}


% if you use PostScript figures in your article
% use the graphics package for simple commands
% \usepackage{graphics}
% or use the graphicx package for more complicated commands
\usepackage{graphicx}
% or use the epsfig package if you prefer to use the old commands
% \usepackage{epsfig}
% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%neat package to add in spiffy-looking algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

\newcommand{\expnumber}[2]{{#1}\mathrm{e}{#2}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\tabref}[1]{\tablename~\ref{#1}}
\newcommand{\figref}[1]{\figurename~\ref{#1}}

\newcommand{\mC}{{\mathbf{C}}}
\newcommand{\mP}{{\mathbf{P}}}

\newcommand{\dt}{{\Delta t}}
\newcommand{\du}{{\Delta u}}
\newcommand{\dv}{{\Delta v}}
\newcommand{\dx}{{\Delta x}}
\newcommand{\dy}{{\Delta y}}
\newcommand{\dz}{{\Delta z}}
\newcommand{\dK}{{\Delta K}}
\newcommand{\deta}{{\Delta \eta}}

\newcommand{\va}{{\mathbf{a}}}
\newcommand{\vb}{{\mathbf{b}}}
\newcommand{\vc}{{\mathbf{c}}}
\newcommand{\vf}{{\mathbf{f}}}
\newcommand{\vk}{{\mathbf{k}}}
\newcommand{\vn}{{\mathbf{n}}}
\newcommand{\vu}{{\mathbf{u}}}
\newcommand{\vv}{{\mathbf{v}}}
\newcommand{\vx}{{\mathbf{x}}}
\newcommand{\vy}{{\mathbf{y}}}
\newcommand{\vz}{{\mathbf{z}}}

\newcommand{\vF}{{\mathbf{F}}}
\newcommand{\vG}{{\mathbf{G}}}
\newcommand{\vH}{{\mathbf{H}}}
\newcommand{\vQ}{{\mathbf{Z}}}

\newcommand{\cN}{{\mathcal{N}}}

\newcommand{\pdd}[2]{{\frac{\partial {#1}}{\partial {#2}}}}
\newcommand{\pddt}{{\frac{\partial}{\partial t}}}

\newcommand{\grad}{\vec{\nabla}}

\begin{document}

\title{Domain-specific implementation of high order \\
Discontinuous Galerkin methods in spherical geometry}

\author[1]{Kalman Szenes}%
\ead{kalman.szenes@math.ethz.ch}
\author[2]{Niccol{\`o} Discacciati}
\ead{niccolo.discacciati@epfl.ch}
\author[3]{Luca Bonaventura}
\ead{luca.bonaventura@polimi.it}
\author[4]{William Sawyer\corref{cor4}}%
\ead{william.sawyer@cscs.ch}
%\cortext[cor4]{Corresponding author: william.sawyer@cscs.ch}
\cortext[cor4]{Corresponding author}

\affiliation[1]{organization={Swiss Federal Institute of Technology Zurich},
addressline={Raemistrasse 101},
postcode={8092},
city={Zurich},
country={Switzerland}}
\affiliation[2]{organization={Swiss Federal Institute of Technology Lausanne},
addressline={Route Cantonale},
postcode={1015},
city={Lausanne},
country={Switzerland}}
\affiliation[3]{organization={Politecnico di Milano},
addressline={Piazza Leonardo da Vinci 32},
postcode={20133},
city={Milan},
country={Italy}}
\affiliation[4]{organization={Swiss National Supercomputing Centre},
addressline={Via Trevano 131},
postcode={6900},
city={Lugano},
country={Switzerland}}

% \date{}

  
\begin{abstract}
We assess the performance of two domain-specific languages included in the GridTools ecosystem as tools for implementing a high-order Discontinuous Galerkin discretization of the shallow water equations. Also equations in spherical geometry are considered, thus providing a blueprint for the application of domain-specific languages to the development of global atmospheric models. The  results  demonstrate that  domain-specific languages designed for finite difference/volume methods  can be successfully extended to implement a Discontinuous Galerkin solver.

\paragraph*{Key Words}
Domain-specific languages, GPU programming, Discontinuous Galerkin methods
\end{abstract}


%%% \twocolumn[
  \maketitle
%  \vspace{-5mm}


%\vspace{5mm}
%%% ]
%\saythanks


% main text
\section{Introduction}  \label{sec:intro}

% Motivation is to make it easier to use high performance computers

It has always been challenging for numerical mathematicians to implement new algorithms in a way that can attain the best possible performance of the underlying platform.  The task has become more difficult with the evolution of new computing architectures, such as Graphics Processing Units (GPUs) or Field-programmable Gate Arrays (FPGAs).  Scientific programmers are forced to learn computing concepts well outside of their original domain.

Domain-specific languages (DSLs) \cite{fowler2010domain} represent an attempt  to separate the concerns of the domain scientist from the complexities of the underlying computer science issues.  In our case, the designer formulates her problem in terms of numerical operations, including time-stepping algorithms, linear algebra operations, or Partial Differential Equation (PDE) formulations, while a ``backend'' takes care of generating the appropriate code for the architecture.  While programming languages such as C++ or Fortran attempt to abstract away the hardware, they have not kept pace with the emerging hardware complexity and frequently require compiler directives or add-ons to properly exploit the architecture.

Extensive effort has been made in developing frameworks to ease the challenge of solving PDEs in given geometries.  On the far end of the spectrum, software frameworks like FEniCS \cite{alnaes2015fenics} and Firedrake \cite{rathgeber2016firedrake} offer a descriptive language to define the PDE and the given domain, along with initial and boundary conditions.  These frameworks then generate the code to solve the problem using finite element methods. The former gives the user little latitude to test a new numerical technique, making it more appropriate for domain scientists relying on standard, widely-supported methods.  Firedrake, on the other hand, has a different underlying implementation allowing more flexibility to employ code optimizations, various finite element mesh topologies, as well as parallelization features, such as the support of message-passing, multithreading or GPUs. Firedrake utilizes PyOp2~\cite{rathgeber2012PyOP2} -- a full-fledged DSL for the parallel executions of computational kernels on unstructured meshes or graphs -- to allow these interventions. More generally, PyOp2 can be viewed as a DSL for finite element calculations. PyOp2 was subsequently refined into the Psyclone~\cite{Psyclone} code-generation system, which was specifically designed to extend Fortran codes. The latter was then used to implement the LFRic~\cite{kavcic2020lfric} atmospheric model Psyclone is source code generator and  could conceptually address the algorithms we propose here, but we have chosen to use tools which have been developed at the Swiss National Supercomputing Centre (CSCS) as part of an evaluation.

Our interests are in the area of weather and climate simulation, where the mesh is often built by extrusion of a two-dimensional horizontal mesh covering, for example, the whole globe.  Our goal is to enable climate and numerical weather prediction (NWP) applications to leverage a variety of architectures by utilizing software backends.  STELLA~\cite{gysi2015stella} represents a first attempt at a C++-embedded DSL with which the COSMO dynamical core (solver of the non-hydrostatic equations of atmospheric motion) was implemented \cite{thaler2019porting}. This prototype was completely replaced by GridTools~\cite{afanasyev2021gridtools}, in which the COSMO and NICAM \cite{kunkel2020aimes} models have subsequently been implemented. The classic implementation of GridTools assumes a Cartesian grid, which explains the choices  described in Section \ref{sec:math}, however the newest version, released in 2023, also allows for a fully unstructured mesh.

In this paper we evaluate two DSLs included in the GridTools ecosystem, namely the C++-based Galerkin-for-Gridtools (G4GT) and the Python-based GridTools-for-Python (GT4Py)~\cite{dahm2021gt4py}, as tools for implementing a high-order Discontinuous Galerkin (DG) method for time-dependent problems, see e.g., \cite{giraldo:2020,hesthavenNodalDiscontinuousGalerkin2008}. More specifically, we consider an explicit time discretization and a modal DG spatial discretization for a system of conservation laws.  
Common, but non-trivial, benchmarks, namely linear advection and the shallow water equations, are used to validate our DSL implementations of a DG method.  Extensive literature is available on the results obtained for these benchmarks by a wide range of numerical methods, allowing us to accurately assess the numerical and performance results. One of the two implementations deals with equations in spherical geometry, showing that DSLs can be successfully applied to the development of prototype codes for atmospheric modeling.

%In Section~\ref{sec:related}, we give a brief overview of related approaches which attempt to separate the complexities of underlying hardware from the numerical mathematician.  
The structure of the paper is as follows.
The flux formulation of the shallow water equations in latitude-longitude coordinates follows in Section~\ref{sec:math}.  A preliminary implementation in an early C++-based DSL prototype called G4GT is presented in Section~\ref{sec:impl_G4GT}, while the new, Python-based implementation called GT4Py is discussed at length in Section~\ref{sec:impl_GT4Py}.  
The validation of the resulting implementations is discussed in Section~\ref{sec:validation} and benchmarks are presented in Section~\ref{sec:performance}.  In Section~\ref{sec:conclusions}, we recount our experiences using the DSLs and make suggestions on how to better support finite element codes in these frameworks.

%\section{Related work on domain-specific languages}  \label{sec:related}



% Standard formulation of DG
% block standard DG method

\section{The mathematical model and numerical  discretization approach}  \label{sec:math}

We are concerned with demonstrating the capabilities of DSLs for implementing numerical solutions of conservation laws. 
%These equations describe the evolution of conserved variable in a domain, determined by its flux through the boundary of any subdomain as well as by a possible source term. 
A system of conservation laws can be written as:
    \begin{equation} 
        \frac{\partial \textbf{u}}{\partial t} + \nabla \cdot \textbf{F}(\textbf{u}) = \textbf{S}(\textbf{u}),  \label{eq:fluxform}
    \end{equation}
\noindent
where $\textbf{u}$ is the vector of conserved variables, $\textbf{F}$ is the flux function and $\textbf{S}$ is the source term.
Equation \eqref{eq:fluxform} becomes well-posed once complemented with appropriate initial conditions and boundary conditions, see, for example, the discussion in \cite{levequeFiniteVolumeMethods2002}.
Since our goal is the application of DSL tools to models for weather and climate, we choose as main model equations the shallow water equations (SWEs) on the sphere, which are a common benchmark for numerical models in this area. Various formulations of these equations can be found in \cite{williamsonStandardTestSet1992}.
We consider the Earth's surface as  a sphere of
 radius %
 $R,$ that is parameterized   in latitude - longitude (lat-lon) coordinates as a rectangular domain such that
 the latitude $\theta \in [-\pi/2,\pi/2] $ and the longitude
 $\lambda \in [0,2\pi].$ Denote by $\Omega =7.292\times 10^{-5} \rm \ s^{-1}$   the Earth's rotation rate, by $f=2\Omega \sin \theta $ the Coriolis
 parameter and by  $g=9.81 \ \rm m \ s^{-2}$ the Earth's gravitational acceleration. 
Furthermore, let   $\hat{\boldsymbol \imath }$,
 $ \hat{\boldsymbol \jmath }$   denote the longitudinal and latitudinal  unit vectors, respectively.
  For a generic scalar function $\phi$
 and vector field $\mathbf{w}=w_1\hat{\boldsymbol \imath }+w_2\hat{\boldsymbol \jmath }, $  
   the spherical gradient and divergence are defined as:
 
 
 \begin{eqnarray}
 \label{eq:operators}
     \nabla \phi &=& \frac{\hat{\boldsymbol \imath }}{R \cos\theta} \partial_\lambda \phi + \frac{\hat{\boldsymbol \jmath }}{R} \partial_\theta \phi \nonumber\\
     \nabla \cdot \mathbf{w} &=& \cfrac{1}{R \cos\theta} \left[\partial_\lambda(w_1) + \partial_\theta(w_2 \cos\theta)\right].
\end{eqnarray}
 Denoting  then $h$ as the thickness
of a fluid over the spherical surface (assuming flat orography $h_b=0$) and $\mathbf{v}=u\hat{\boldsymbol \imath}+v\hat{\boldsymbol \jmath} $ the velocity field, which is a tangent vector field to the sphere, the shallow water equations
in flux form are written as: 

\begin{eqnarray} 
\label{eq:sph1}
      &&    \partial_t h + \nabla \cdot (h \mathbf{v}) = 0  \nonumber \\
&&          \partial_t (h \mathbf{v}) + \nabla \cdot (h \mathbf{v} \otimes \mathbf{v}) = - f \mathbf{\hat k} \times h \mathbf{v} -  \nabla \left(\frac{gh^2}2\right).
 \end{eqnarray}
It is well known that lat-lon coordinates entail a number of numerical difficulties. Indeed, for a Cartesian lat-lon  mesh such as the one depicted in Figure \ref{fig:lat-lon}, the elements become increasingly distorted as they approach the poles. Moreover, the elements precisely neighboring the poles have a singular edge in physical space (these elements reduce to spherical triangles instead of rectangles).

    \begin{figure}[htb]
        \centering
        \includegraphics[width=0.2\textwidth]{img/lat_lon.pdf}
        \caption[lat-lon]{Latitude-Longitude grid}
        \label{fig:lat-lon}
    \end{figure}
    \noindent
While several alternatives have been considered in the literature, see, e.g., the review in \cite{bonaventura:2012},
this setting is sufficient for the present purpose of validating  GT4Py, which, as discussed in Section \ref{sec:intro}, could only handle non-Cartesian meshes in the latest release. Furthermore, degree adaptivity techniques as in \cite{tumolo:2015} can help reduce the numerical problems of this extremely simple setting. The equations for the spherical components of the velocity
field can then be derived taking into account the non-inertial
nature of the rotating reference frame on the sphere, see 
again \cite{williamsonStandardTestSet1992} (Equations 6-7). We then obtain:

\begin{eqnarray} 
\label{eq:sph2}
&&      \partial_t (h u) + \nabla \cdot (hu \mathbf{v} ) +\frac{1}{R\cos\theta}\partial_{\lambda}\left(\frac{gh^2}2\right)
= \left(f+\frac uR \tan \theta\right)hv\nonumber \\
&& \partial_t (h v) + \nabla \cdot (hv \mathbf{v} ) +
\frac{1}{R}\partial_{\theta}\left(\frac{gh^2}2\right)
= -\left(f+\frac uR \tan \theta\right)hu.
 \end{eqnarray}
 Since
 \begin{eqnarray}
  \nabla \cdot (hu\mathbf{v}) &=& \cfrac{1}{R \cos\theta} \left[\partial_\lambda(hu^2) + \partial_\theta(huv \cos\theta)\right]\nonumber \\
  \nabla \cdot (hv\mathbf{v}) &=& \cfrac{1}{R \cos\theta} \left[\partial_\lambda(huv) + \partial_\theta(hv^2 \cos\theta)\right]\nonumber
\end{eqnarray}
and also
$$
\frac{\cos \theta}{R}\partial_{\theta}\left(\frac{gh^2}2\right)=\frac{1}{R}\partial_{\theta}\left(\frac{gh^2}{2}\cos \theta\right) + \frac{gh^2}{2 R}\sin\theta,
$$
the SWE can be rewritten component-wise as:
\begin{eqnarray} 
\label{eq:sph3}
             \partial_t (h \cos\theta)  &+& \cfrac{1}{R}\left[ \partial_\lambda \left(h u\right) + \partial_\theta \left(h v \cos\theta\right) \right] = 0 \nonumber \\
         \partial_t (h u \cos\theta)  &+&  \cfrac{1}{R}\left[\partial_\lambda \left(h u^2 + \cfrac{g h^2}{2}\right) + \partial_\theta \left(h u v \cos\theta\right) \right] \nonumber \\
 &=& \left(f\cos\theta+\frac uR \sin \theta  \right)hv \\
         \partial_t (h v \cos\theta) &+& \cfrac{1}{R}\left[ \partial_\lambda \left(h u v\right) + \partial_\theta \left(\left(h v^2 + \cfrac{g h^2}{2}\right) \cos\theta\right) \right] \nonumber \\
         &=&   - \cfrac{g h^2 \sin \theta}{2 R} 
         -\left(f\cos\theta+\frac uR \sin \theta\right)hu.
         \nonumber
    \end{eqnarray}

%After replacing the differential operators by their definitions, the SWE can be rewritten component-wise as:

  % \[
  %  \begin{cases}
   %      \partial_t (h \cos\theta)  + \cfrac{1}{R}\left[ \partial_\lambda \left(h u\right) + \partial_\theta \left(h v \cos\theta\right) \right] = 0 \\
    %     \partial_t (h u \cos\theta)  +  \cfrac{1}{R}\left[\partial_\lambda \left(h u^2 + \cfrac{g h^2}{2}\right) + \partial_\theta \left(h u v \cos\theta\right) \right] = f h v \cos\theta \\
     %    \partial_t (h v \cos\theta) + \cfrac{1}{R}\left[ \partial_\lambda \left(h u v\right) + \partial_\theta \left(\left(h v^2 + \cfrac{g h^2}{2}\right) \cos\theta\right) \right] = \cfrac{g h^2 \sin \theta}{2 R} - f h u \cos\theta.
    %\end{cases}
    %\]

    
Periodic boundary conditions are considered in the longitudinal direction, while in the latitudinal direction, the fluxes are set to zero. The SWE in spherical coordinates can therefore be written as the system of conservation laws:
    \begin{equation*}
     \partial_t (\mathbf{U}) + \partial_\lambda (\mathbf{F}(\mathbf{U})) + \partial_\theta (\mathbf{G}(\mathbf{U})) = \mathbf{S}(\mathbf{U}),
    \end{equation*} \label{eq:SWES}
\noindent
where we have defined the conserved quantities, fluxes and sources as:

    \begin{eqnarray}
       \mathbf{U} &=& \begin{pmatrix}
              h \cos \theta \\
              h u \cos \theta \\
              h v \cos \theta
         \end{pmatrix}, 
         \hskip 0.2cm  \mathbf{F(\mathbf{U})} = \cfrac{1}{R} \begin{pmatrix}
               h u \\
               h u^2 + \cfrac{g h^2}{2} \\
               h u v
         \end{pmatrix} \nonumber \\  
         \mathbf{G}(\mathbf{U}) &=& \cfrac{\cos \theta}{R} \begin{pmatrix}
              h v  \\
              h u v \\
              h v^2 + \cfrac{g h^2}{2}
         \end{pmatrix}, \hskip 0.2cm\mathbf{S}(\mathbf{U}) = \begin{pmatrix}
              0 \\
              \left(f\cos\theta+\frac uR \sin \theta  \right)hv \\
               - \cfrac{g h^2 \sin \theta}{2 R} 
         -\left(f\cos\theta+\frac uR \sin \theta\right)hu
          \end{pmatrix}.  
    \end{eqnarray}
        
\noindent   
We then present an overview of the classical DG method chosen for the demonstration of a DSL implementation. A complete description can be found, among many others, in \cite{giraldo:2020,hesthavenNodalDiscontinuousGalerkin2008}.
We consider for simplicity the discretization of a scalar conservation law,

 \begin{equation} \label{eq:cons_law_sc}
        \frac{\partial {u}}{\partial t} + \nabla \cdot \textbf{f}(u) ={s(u)},
    \end{equation}
defined on a two-dimensional rectangular domain. This domain is subdivided into $K$ elements, denoted by $D^k$, for any $k = 1\ldots K$. We restrict our attention to conforming structured meshes composed of rectangular elements $  D^k = [x^k_l, x^k_r] \times [y^k_b , y^k_t ]. $ 
In each element, let $V^k_h$ be the finite-dimensional space of multivariate polynomials up to a given degree $r $ in each spatial dimension:

\begin{equation*}
V^k_h =
  \left\{ v : v = \sum_{i,j=0}^r \alpha_{ij} x^i y^j,
               x \in [x^k_l,x^k_r], y \in [y^k_b , y^k_t]
  \right\}.
\end{equation*}

\noindent
Consequently, the finite-dimensional space in which we seek the solution is the space of discontinuous polynomials defined as
$
V_h = \left\{ 
         v \in L^2( \Omega ) : v|_{D^k} \in V^k_h
      \right\}.
$
The numerical solution can be viewed as the direct sum of local approximations:
\begin{equation}
u_h = \bigoplus_{k=1}^K u^k_h,  \label{eq:local_approx}
\end{equation}

\noindent
where $u^k_h \in V^k_h$.
Due to \eqref{eq:local_approx}, we can restrict our attention to a single mesh element, dropping the superscript $k$  for simplicity when necessary.
%Defining the local residual, we impose that it vanishes locally in a Galerkin sense:
%\begin{equation*}
%R^k_h = \pddt{u^k_h} + \nabla \cdot \vf (u^k_h) - s(u^k_h) \ \ \ \ \ \int_{D^k} R^k_h %\phi^k_h = 0
%\end{equation*}
%\noindent
We define the local residual as
\begin{equation*}
R^k_h = \pddt{u^k_h} + \nabla \cdot \vf (u^k_h) - s(u^k_h),
\end{equation*}
and impose that it vanishes locally in a Galerkin sense, i.e.,
\begin{equation*}
\int_{D^k} R^k_h \phi^k_h = 0
\end{equation*}
for any suitably defined test function $\phi^k_h \in V^k_h$.  After  integration by parts, the weak DG formulation is given by:

\begin{equation}
  \int_{D^k} \pddt u^k_h \phi^k_h + 
  \int_{\partial D^k} \vf^* (u_h) \cdot \vn_k \phi^k_h -
  \int_{D^k} \vf (u^k_h) \cdot \nabla \phi^k_h =
  \int_{D^k} s(u^k_h) \phi^k_h    .          \label{eq:dg_parts}
\end{equation}
In Equation \ref{eq:dg_parts}, the physical flux at the element boundary is replaced by a numerical approximation, denoted by $\vf^*$. This guarantees that the flux is single-valued at each edge, enforcing conservation across any edge. Note that all terms in Equation \ref{eq:dg_parts} are local to the $k$-th element, except the numerical flux, which depends on the neighboring elements.  The choice of $\vf^*$ plays a crucial role in the numerical solver's consistency, accuracy and stability.  A popular choice of $\vf^*$ is the Rusanov flux, defined as:
\begin{equation}
\vf^* (u_h) = \vf^* (u^k_h,u^{\bar{k}}_h) =
              \frac{\vf (u^k_h) + \vf (u^{\bar{k}}_h)}{2} -
              \frac{\alpha}{2}(u^{\bar{k}}_h - u^k_h ) \vn_k, \label{eq:rusanov}
\end{equation}
\noindent
where $\bar{k}$ is the index of the neighbor element to $k$ across a given edge, and $\alpha \ge 0$ is a large enough stabilization parameter, usually chosen to be an estimate  of the largest eigenvalue of the hyperbolic system associated to the conservation law.  Finally, $\vn_k$ is the normal unit vector, pointing outwards $D^k$.  
The local solution $u_h^k$ in element $k$ is then written as a linear combination of a polynomial basis $\phi^{(k)}_i$
of $V^k_h:$
    \begin{equation} \label{eq:basis_expansion}
  u_h^k= \sum_{j=1}^{n_{\phi}} \hat u_j^k\phi^{(k)}_j,
    \end{equation}
\noindent
where we have dropped the suffix $h$ in the notation for the polynomial basis. Furthermore, $\hat u_i^k$ denotes the polynomial expansion coefficients, and
%,
%using the notation in %\cite{tugnoli:2017},
$n_{\phi}=(p+1)^2$ represents the cardinality of the polynomial basis, where $p$ represents the maximum degree of the polynomials employed.
Multiple choices exist for the basis set used for the local polynomial spaces. In this study, following e.g., \cite{tumolo:2015}, we employ a modal DG approach, which relies on bivariate Legendre polynomials.
After inserting the basis expansion from Equation \eqref{eq:basis_expansion} in Equation \eqref{eq:dg_parts}
and using the $\phi^{(k)}_i$ as test functions,
 we obtain the following semi-discrete form:
    \begin{equation} \label{eq:ode}
        M \frac{d \mathbf{\hat u}}{dt} =
        \mathbf{h}(\mathbf{\hat u}) \Leftrightarrow \frac{d \mathbf{\hat u}}{dt} = M^{-1}\mathbf{h}(\mathbf{\hat u}).
    \end{equation}
\noindent
Here, the vector $\mathbf{\hat u}$
%= [\hat u_0,...,\hat u_{n_{\phi}}]^T$ 
collects the polynomial expansion coefficients for all elements, and the
matrix $M$ has a block diagonal structure, where the diagonal blocks are the
local mass matrices $M^{(k)}$:
    \begin{equation*} 
        M_{ij}^{(k)} = \int_{D^k} \phi^{(k)}_i \phi^{(k)}_j
    \end{equation*}
associated with each element. Notice that all the terms on the right-hand side have been grouped in the vector function $\mathbf{h}(\mathbf{\hat u}),$
thus obtaining spatial semi-discretization that can be fully discretized by the method of lines approach described below.
Thanks to the use of a DG discretization, the resulting mass matrix can be inverted locally for each element.
Furthermore,  in the case of spherical coordinates, we also simplify the definition of the conserved variables in Equations
\eqref{eq:sph3}
by including the $\cos \theta $ metric terms directly in the  local mass matrix: 
    \begin{equation*}
        M_{ij}^{(k)} = \int_{D^k} \phi_i(\lambda, \theta) \phi_j(\lambda, \theta) \cos(\theta), %d\lambda\, d\theta
    \end{equation*}
\noindent
so that the conserved variables are given by $h$, $hu$ and $hv$.
 Note that the mass matrices are all identical for a specific longitudinal value.

For the time discretization of Equation~\eqref{eq:ode}, we follow the classical method of lines approach employing Runge-Kutta (RK) methods. More precisely, we use explicit Strong Stability Preserving (SSP) of orders from 1 to 4, denoted later as RK1-RK4, see e.g., \cite{gottlieb:2011}, which can be defined by means of their Butcher tableaux listed in Table \ref{table:Butcher}.
\begin{table}[htb]
    \[
        \begin{array}{c|c}
            0 &  \\
            \hline
            & 1
    
        \end{array}
        \qquad
        \begin{array}{c|c c}
            0 & \\
            1 & 1 & \\
            \hline
            & 1/2 & 1/2
        \end{array}
        \qquad
        \begin{array}{c|ccc}
            0 &  & & \\
            1 & 1 & & \\
            1/2 & 1/4 & 1/4 & \\
            \hline
            & 1/6 & 1/6 & 2/3
        \end{array}
        \qquad
        \begin{array}{c|cccc}
            0 & & & & \\
            1/2 & 1/2 & & & \\
            1/2 & 0 & 1/2 & & \\
            1 & 0 & 0 & 1 & \\
            \hline
            & 1/6 & 1/3 & 1/3 & 1/6
            
        \end{array}
    \]
    \caption{Butcher tableaux of SSP Runge Kutta methods of order 1 to 4}
    \label{table:Butcher}
    \end{table}
These explicit time discretization methods are only conditionally stable. Their stability depends on the value of the non-dimensional parameter known as the Courant number, which is usually defined as 
$ c{\Delta t}/{\cal H}$, where $c$ denotes some estimate of the largest eigenvalue of the underlying hyperbolic system and $\cal H$ is the minimum element diameter. For DG methods and other high-order finite element techniques, however, it is customary
to redefine the Courant number by taking into account the
presence of internal degrees of freedom in each element, see e.g.,
\cite{orlando:2022, orlando:2023, tumolo:2015},
so that a more appropriate definition is in this case
$pc{\Delta t}/{\cal H}$,
where $p$ denotes the maximum element degree. Due to the 
reduction of the effective element size at the poles and the use
of high-order elements, rather small values of the time step
have to be chosen to allow for stable simulations.


%The mathematical goals of this work were (1) utilize a flux formulation in latitude/longitude coordinates and (2) to utilize rectangular modal DG elements which inherently avoid the pole singularities: these elements only utilize internal points and the edges lying on the north (south) pole have zero length, so they do not allow northern (southern) fluxes.

 
\section{Implementations in G4GT and GT4Py} \label{sec:implementation}

We have implemented DG solvers for conservation laws in separate projects with distinct DSLs for planar and spherical geometry.  The Galerkin-for-GridTools (G4GT) and GridTools-for-Python (GT4Py) are both part of the GridTools (GT) \cite{GridTools} ecosystem, which offers an efficient C++ library that is agnostic to the underlying architecture. It makes extensive use of template meta-programming and has backend optimizations for both CPU and GPU architectures. GT was initially designed to target numerical simulations of PDEs using regular grids and finite-difference schemes. Although the latest version of GT supports unstructured grids, the presented implementation relies on its original version.

We remark that G4GT was simply a proof of concept and is {\it no longer supported}. Indeed, the popularity of Python among modern-day programmers led the GT team to switch to the Python-based GT4Py layer, which is supported and in the public domain. However, the ideas illustrated in G4GT, in terms of both supported PDE models and computational performance, are educational. Thus, before discussing in detail the GT4Py implementation, which should be regarded as the main tool employed, we emphasize a number of features of the G4GT framework that are complementary to the ones of GT4Py.

\subsection{Galerkin for GridTools (G4GT) implementation}
\label{sec:impl_G4GT}

G4GT is a C++-based extension to the GridTools  library that supports finite element codes.  It relies on GT for the underlying implementation of computation kernels, but also on the Trilinos~\cite{heroux2005overview} libraries Intrepid~\cite{Intrepid} and Epetra~\cite{Epetra}, which provide the numerical support for finite element discretizations and specific linear algebra tools. The G4GT framework provides the link between these libraries, adding a higher-level, user-friendly layer to GT. Additionally, it adds support for finite element discretizations using GT-based codes, which is not present in GT.  

The key steps of the discretization are implemented with GT abstractions, such as the {\tt eval} functor, which subsequently evaluates the element-wise code over the entire domain. Among these steps, the most critical is the computation of the boundary fluxes, as it requires communication between neighboring elements. Its detailed implementation for the horizontal direction is reported below:


\begin{minted}[breaklines]{cpp}
struct Rusanov_lr{

using u=gt::accessor<0, enumtype::in, gt::extent<>, 5>;
using fun=gt::accessor<1, enumtype::in, gt::extent<>, 6>;
using alpha=gt::global_accessor<2>;
using normals=gt::accessor<3, enumtype::in, gt::extent<>, 6>;
using out=gt::accessor<4, enumtype::inout, gt::extent<>, 5>;
using arg_list=boost::mpl::vector<u, fun, alpha, normals, out>;

template <typename Evaluation>
GT_FUNCTION
static void Do(Evaluation & eval, x_interval) {

uint_t const num_cub_points=eval.template get_storage_dim<3>(u());
uint_t const beta_dim=eval.template get_storage_dim<4>(fun());

for (uint_t face_ : {1,3}) {

    short_t opposite_i = (short_t)(face_==1)?1:(face_==3)?-1:0;
    short_t face_opposite = (short_t)(face_==1)?3:1;
    float_type coeff = -eval(alpha());

    for (short_t qp=0; qp<num_cub_points; qp++) {
        float_type inner_prod1=0.;
        float_type inner_prod2=0.;
        for (uint_t dim=0; dim<beta_dim; dim++){
            inner_prod1 += eval(fun(0,0,0,qp,dim,face_) * normals(0,0,0,qp,dim,face_)); 
            inner_prod2 += eval(fun(opposite_i,0,0,qp,dim,face_opposite) * normals(0,0,0,qp,dim,face_));
        }
	eval(out(0,0,0,qp,face_))=(inner_prod1+inner_prod2)/2. - eval(coeff*(u(0,0,0,qp,face_)-u(opposite_i,0,0,qp,face_opposite))/2.);
    }
}
}

};
\end{minted}

The main elements of the class can be deduced in a straightforward way.
We simply mention that it relies extensively on the \texttt{gt::accessor} class, and that one should specify whether the accessor is an input or output argument.
Moreover, the indexing of each accessor refers to the field element relative to the one on a specific thread. Therefore, a value of zero refers to the thread-local element rather than to a neighbor. 

C++ templating allows for this expansion by making stages of the flux computation on the appropriate grid:

\begin{minted}[breaklines]{cpp}
auto coords_lr=gt::grid<axis>({1u, 1u, 1u, (uint_t)d1-2u, (uint_t)d1},{0u, 0u, 0u, (uint_t)d2-1u, (uint_t)d2});
coords_lr.value_list[0] = 0;
coords_lr.value_list[1] = d3-1;

auto fluxes_lr=gt::make_computation< BACKEND >(domain_iteration, coords_lr, gt::make_multistage ( execute<forward>(), gt::make_stage< functors::Rusanov_lr > (it::p_u_t_phi_bd(), it::p_fun_bd(),it::p_alpha(), it::p_normals(),  it::p_Rus()) ) );
\end{minted}

%On top of this, Intrepid and Epetra are employed to perform several simple transformations. The most relevant one is the mapping between modal and nodal values, whose implementation is reported below:

%\begin{minted}[breaklines]{cpp}
%template <uint_t Order>
%struct M2N{

%private:
%Epetra_SerialDenseMatrix m2n;
%Epetra_SerialDenseMatrix n2m;

%public:
%template <typename Storage1, typename Storage2> 
%void M2N_convert( Storage1 const & in, Storage2 & out, uint_t const d1, uint_t const d2, uint_t const d3); 
%template <typename Storage1, typename Storage2> 
%void N2M_convert( Storage1 const & in, Storage2 & out, uint_t const d1, uint_t const d2, uint_t const d3);

%uint_t invert() {
%    Epetra_SerialDenseSolver Solver;
%    n2m=m2n;
%    uint_t flag=Solver.SetMatrix(n2m); 
%    uint_t flag2=Solver.Invert();
%    return flag+flag2; 
%}

%//other members and methods omitted

%};
%\end{minted}

%As this class is never used in the time-integration loop, it does not have a strong impact on the performance of the method. This motivates the use of Trilinos in place of GT, trading performance in favor of ease of implementation. 

As the main subject of this work is the GT4Py implementation, we do not delve into additional technicalities of G4GT. However, a more detailed discussion can be found in \cite{discacciati2018implementation}.

\subsection{GT4Py Implementation}  \label{sec:impl_GT4Py}

%As the architectures of modern supercomputers grow in complexity, it has become increasingly difficult to develop and maintain high-performance applications which exploit well the compute resources. This has led to the rise of DSLs. They provide a simple interface for the domain scientists to develop their algorithms while the optimization of the code is handled automatically by the DSL.\href{https://github.com/GridTools/gt4py}{GT4Py} is an open-source DSL embedded in Python, targeting stencil computations for weather and climate simulations. 

The pipeline of GT4Py is illustrated in Figure \ref{fig:pipeline}. The domain scientist expresses the stencils in a user-friendly Python syntax called GTScript, and this code is then processed through a series of toolchains that applies optimizations and generates a high-performance executable targeting a specific architecture.

    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{img/gt4py_pipeline.png}
        \caption[gt4py-pipeline]{GT4Py compilation pipeline}
        \label{fig:pipeline}
    \end{figure}

%subsubsection{Installation}\footnotetext{Source: PASC22 poster by A. Afanasyev et al., \textit{GT4Py: High Performance Stencil Computations in Weather and Climate Applications using Python}}Instructions for installing GT4Py can be found in the \href{https://github.com/GridTools/gt4py}{repository} on Github. Use pip to install all the necessary dependencies from the \mintinline{sh}{requirements.txt} file. The backends for execution on GPUs and DaCe require the respective additional package dependencies \mintinline{python}{cupy-cuda110} and \mintinline{python}{dace} which can be installed through \mintinline{text}{pip}. Make sure that the version of \mintinline{python}{cupy-cudaXXX} matches the cudatoolkit version that you have (e.g. \mintinline{python}{cupy-cuda110} requires cudatoolkit version 11.0) 

\subsubsection{Backends}
GT4Py can compile using various backends; see Table \ref{table:backends} for a complete list of the supported ones.
    \begin{table}[htb]
        \centering
        \begin{tabular}{cc}\toprule
            Framework & Name \\\midrule
            \multirow{3}{*}{GridTools} & \mintinline{python}{gt:cpu_ifirst} \\
            & \mintinline{python}{gt:cpu_kfirst} \\ 
            & \mintinline{python}{gt:gpu} \\
            \midrule
            \multirow{2}{*}{DaCe} & \mintinline{python}{dace:cpu} \\
            & \mintinline{python}{dace:gpu} \\
            \midrule
            & \mintinline{python}{cuda} \\
            & \mintinline{python}{numpy} \\
            & \mintinline{python}{others under development} 
            \\\bottomrule
        \end{tabular}
        \caption{List of supported GT4Py backends}
        \label{table:backends}
    \end{table}
Three of the seven backends compatible with GT4Py rely on the GT framework to compile and optimize the stencil computations. They are all characterized with the prefix \mintinline{python}{gt:}. The \mintinline{python}{gt:cpu_ifirst} and \mintinline{python}{gt:cpu_kfirst}  both target the CPU architecture, while the \mintinline{sh}{gt:gpu} backend produces code for the GPU.
In addition, two backends utilize the Data Centric \cite{ben-nun_stateful_2019} (\href{https://github.com/spcl/dace}{DaCe}) parallel programming framework developed by the Scalable Parallel Computing Lab at Swiss Federal Institute of Technology Zurich, namely \mintinline{sh}{dace:cpu} and \mintinline{sh}{dace:gpu} targeting CPUs and GPUs, respectively. At the time of the DG-GT4Py implementation, only prototype implementations of the backends were available.

Alternatively, there is a native CUDA backend independent of any GridTools or DaCe routine. 
Finally, a NumPy backend exists, which can be used to inspect the generated code for debugging purposes.

%\paragraph{Caching}

%To speed up compilation, GT4Py automatically caches stencils inside a cache folder and does not recompile a stencil unless it has been modified. The cache folder contains the binaries as well as the generated source code for each stencil. It has the following directory structure:
%    \begin{minted}{sh}
%    .gt_cache/{python_version}/{backend_name}/__main__/{stencil_name}
%    \end{minted}
%The \mintinline{sh}{{stencil_name}} directory contains build folders that include the generated source code. Each build folder, identified using a unique hash value as a prefix, relates to a specific stencil version. Inside the build directories, there are two files of interest that can be inspected for debugging or performance analysis. The first one is the \mintinline{sh}{computations.hpp} file, which houses the C++ or CUDA implementation of the stencil computation, and the second one is the \mintinline{sh}{bindings.cpp} which contains the actual Python bindings.

%\paragraph{Backend Options}

%Several optional settings can be applied to configure the compilation process. Subsequently, we highlight two of them that are particularly note-worthy. The first one turns on the verbosity of the compiler. Since it is disabled by default, during the compilation of a stencil, no output is produced to the console. The user is hence left clueless, wondering why the execution of the Python script is stalled.

%The second one prompts GT4Py to recompile a stencil even if a cached version already exists. This is strongly recommended in case the user updates GT4Py to a newer version which may lead to earlier caches of stencils becoming incompatible. Alternatively, this can be accomplished by removing the \mintinline{sh}{.gt_cache} directory entirely, which will prompt the recompilation of all stencils.

%They are specified in a Python dictionary provided to the stencil definition. An example of it is given below:
%    \begin{minted}{python}
%        backend_opts = {
%            "verbose": True, # always
%            "rebuild": True  # only if recompilation is desired
%        }
%    \end{minted}

\subsubsection{Stencils}
Stencils are special GT4Py functions that operate on fields in a specific domain. Fields store the values of variables at each grid point of the domain.

\paragraph{Declaration}
In the following example, we compute the discretized 2-dimensional Laplacian operator:
    \begin{equation*}
        (\Delta u)_{i,j} = - 4 u_{i,j} + u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} 
    \end{equation*}
which can be written as the following stencil in GT4Py:
   \begin{minted}[breaklines]{python}
    import numpy as np
    import gt4py.gtscript as gtscript
    @gtscript.stencil(backend=backend, **backend_opts)
    def laplacian(
        field: gtscript.Field[np.float64],
        out: gtscript.Field[np.float64]
    ):
        with computation(PARALLEL), interval(...):
            out = - 4 * field + (field[-1,0,0] + field[1,0,0]  + field[0,1,0] + field[0,-1,0])
    \end{minted}
    
In the function decorator, we provide the target backend as well as potential back-end options. The function expects fields as arguments, on which the stencil computations are executed. GT4Py uses the Python-type hinting system to specify the data type of each field, which in this case is \mintinline{python}{np.float64}.

The body of the function requires two context managers which define the execution of the stencil in the vertical direction, the first being \mintinline{python}{computation} which accepts the arguments \mintinline{python}{PARALLEL}, \mintinline{python}{FORWARD} or \mintinline{python}{BACKWARD}. This defines the scheduling of the execution stencil. The keyword \mintinline{python}{PARALLEL}, which we use exclusively for our implementation, indicates that there is no dependence between subsequent vertical levels, and hence they can all be solved in parallel. The keywords \mintinline{python}{FORWARD} and \mintinline{python}{BACKWARD} define this dependence and indicate the direction in which the vertical levels must be solved. The second context manager \mintinline{python}{interval} allows the user to specify the vertical indices for which the stencil will be applied. The `\mintinline{text}{...}' is a shorthand notation to select the entire vertical domain.

Finally, we note that the stencil computation is applied for each grid point; hence, relative offsets are used as indices. Note that, if omitted, the offset is assumed to be \mintinline{text}{[0,0,0]}.

\paragraph{Invocation}

The above \mintinline{python}{laplacian} function can be called using the following command:

    \begin{minted}{python}
    nx, ny, nz = field.shape
    origins = {"field":(1,1,0),"out":(1,1,0)} # or {"_all_":(1,1,0)}
    laplacian(field, out, origin=origins, domain=(nx-2, ny-2, nz))
    \end{minted}

We provide the fields relevant to the stencil computation as arguments to the function. In addition, we add two optional keyword arguments, namely \mintinline{text}{domain}, which specifies the domain of execution of the stencil, and \mintinline{text}{origin}, which defines the origin for each field. In the case of the \mintinline{python}{laplacian} stencil, we set these to ensure that the stencil only operates on the inner part of the domain.

The \mintinline{text}{origin} argument indicates relative offset between the different fields. The keyword \mintinline{python}{_all_} can be utilized to set the same origin for all fields that have not been specified separately. Note that the keyword names inside the \mintinline{text}{origins} dictionary refer to the names of the fields in the stencil definition and not to the names of the fields in the call to the stencil. Upon invocation of a stencil, GT4Py searches for a cached version and relies on just-in-time (JIT) compilation in case none is found.

\subsubsection{Storages}

In GT4Py, fields are variables on which stencils can be applied. They store values at each grid point inside a 3-dimensional domain. The DSL provides a storage format for these fields which is a wrapper over the array types  \mintinline{python}{{numpy/cupy}.ndarrays} called \mintinline{python}{gt4py.storages}, which ensures that the memory layout of the data is compatible with the requested backend. The interface provides several methods for instantiating storages, including \mintinline{python}{empty()}, \mintinline{python}{ones()} and \mintinline{python}{zeros()}, as well as directly from an existing NumPy array using \mintinline{python}{from_array()}. All of these functions require several additional parameters: \mintinline{python}{shape} defines the size of the storage in the three dimensions, and \mintinline{python}{default_origin} specifies the default origin to be used in case none is specified during a stencil call. Finally, \mintinline{python}{dtype} not only defines the data type of the field but can also be used to assign higher-order tensors to each grid point instead of simple scalar values. These fields are subsequently referred to as higher-dimensional fields.

In the example below, each grid point stores a matrix of size 3x2:
    \begin{minted}{python}
    u = gt4py.storage.zeros(
            backend=backend, default_origin=(1,1,0),
            shape=(4, 4, 2), dtype=(np.float64, (3,2))
        ) 
    \end{minted}
\noindent
Moreover, suppose a field has identical values along one or more spatial dimensions. In that case, GT4Py provides a feature called `masking', which avoids the storage of unnecessary copies of the identical values while still giving the appearance of a full 3-dimensional field.  This can lead to a substantial reduction in memory consumption, which is crucial for large problem sizes.
The previous field can be masked in the vertical direction using:
    \begin{minted}{python}
    u = gt4py.storage.zeros(
            backend=backend, default_origin=(1,1),
            shape=(4, 4), dtype=(np.float64, (3,2)),
            mask=[True, True, False]
        ) 
    \end{minted}

Note that when using a GPU backend, the fields need to be explicitly synchronized from the device back to the host to obtain the results of a stencil computation. In addition, it is recommended to cast the \mintinline{text}{gt.storage} to a \mintinline{text}{numpy.ndarray} to ensure that the data has indeed been copied from the device. This can be accomplished with the following code snippet:
    \begin{minted}{python}
        x_gt.device_to_host()
        x_np = np.asarray(x_gt)
    \end{minted}

\subsubsection{Frontend}

In this section, we describe the structure of the GT4Py frontend and our contribution to expanding the functionality of higher-dimensional fields.

\paragraph{Abstract Syntax Tree (AST)}

The Python language uses an interpreter which converts the source code of a program into a representation called an Abstract Syntax Tree (AST) before compiling the program to bytecode which is executed by the computer. As the name suggests, the AST represents the logic of the program as a tree structure stripped of the specific syntax used in the source code. This representation provides a versatile way to inspect and modify Python applications.

In the case of GT4Py, the frontend parses the Python AST and converts it into a series of custom ASTs through the pipeline (Figure \ref{fig:pipeline}), which provide additional information necessary for the backends to produce well-optimized executables.

\paragraph{Limited support for higher-dimensional fields}

For our implementation, we represent each DG element by a grid point in GT4Py. Each grid point is thus assigned a vector that stores its polynomial expansion coefficients and hence yields a higher-dimensional field as a data structure. We refer to this additional dimension of the field as \mintinline{text}{data_dims}.

Initially, the support for these vector-valued fields in GT4Py was limited. In particular, there was no functionality for automatically vectorizing operations between fields with respect to the \mintinline{text}{data_dims} dimension. Indeed, operations needed to be unrolled explicitly for each vector component, reducing their utility to scalar fields. The following example illustrates a stencil performing an element-wise multiplication between two higher-dimensional fields:
    \begin{minted}{python}
    @gtscript.stencil(backend=backend)
    def mult(
        field1: gtscript.Field[(np.float64, (3,))],
        field2: gtscript.Field[(np.float64, (3,))],
        out: gtscript.Field[(np.float64, (3,))]

    ):
        with computation(PARALLEL), interval(...):
            out[0,0,0][0] = field1[0,0,0][0] * field2[0,0,0][0]
            out[0,0,0][1] = field1[0,0,0][1] * field2[0,0,0][1]
            out[0,0,0][2] = field1[0,0,0][2] * field2[0,0,0][2]
    \end{minted}
The first set of indices represents the relative offsets between the fields, while the second set of indices refers to the actual components of the \mintinline{text}{data_dims} dimension. Note that in this case, the relative offsets cannot be omitted and need to be specified explicitly.
    

\paragraph{Loop unroller}

We have implemented an automatic vectorization of operations to each vector component to facilitate the use of vector-valued fields. This has been accomplished by modifying an intermediate GT4Py AST called \mintinline{python}{definition_ir}. In this representation, we have access to the size of \mintinline{python}{data_dims} for each field which is essential to determine if an operation between two fields can be vectorized.
Our contribution allows rewriting the previous multiplication stencil using the following simple syntax:
    \begin{minted}{python}
    # ...
    with computation(PARALLEL), interval(...):
        out = field1 * field2
    \end{minted}

The goal is to modify the GT4Py frontend so that the code snippet above produces the same AST as the previous explicitly unrolled stencil. To implement this functionality, we created two helper classes which apply the necessary transformations to the \mintinline{python}{defintion_ir}. They can both be found in the file:
    \begin{minted}{text}
        gt4py/src/gt4py/frontend/defir_to_gtir.py
    \end{minted}
The first one, called \mintinline{python}{UnRoller}, receives as argument an AST node representing the right-hand side  of an assignment operation and returns a list of AST nodes where each element of the list pertains to a different index of the higher-dimensional field.
The second one is called \mintinline{python}{UnVectorisation}. It invokes the \mintinline{python}{UnRoller} and checks that the dimensions of the returned list match the dimensions of the field on the left-hand side of the assignment operation. If so, it creates the list of AST assignment nodes with the corresponding indices.

Our implementation supports not only chaining together multiple operations on higher-dimensional fields but also broadcasting of scalar values as well. The syntax and functionality should be intuitive for anyone familiar with the NumPy package.

\paragraph{Matrix multiplication}

An additional operation that we required for our DG scheme was a matrix-vector multiplication between higher-dimensional fields. This was incorporated into our existing framework and can be invoked using the "\mintinline{python}{@}" operator. Also, the multiplication of a vector by the transposed of a matrix can be achieved by appending the matrix with the "\mintinline{python}{T}" attribute. This leads to the following syntax:
\begin{minted}{python}
    @gtscript.stencil(backend=backend)
    def matmul(
        matrix: gtscript.Field[(np.float64, (3, 2))],
        vec: gtscript.Field[(np.float64, (3,))],
        out: gtscript.Field[(np.float64, (2,))]
    )
        with computation(PARALLEL), interval(...):
            out = matrix.T @ vec
\end{minted}


\paragraph{DG solver: precomputation}

At the start of the execution of the program, the GT4Py solver precomputes on the CPU certain variables  that remain constant during the whole simulation. This includes the computation of the inverse mass matrix, as well as the Gauss-Legendre quadrature points and weights for numerical integration. A helper class called Vander, defined in \mintinline{sh}{vander.py}, contains all the Vandermonde matrices  required to evaluate the polynomials stored as modal expansion coefficients at nodal values in the domain (see e.g., \cite{hesthavenNodalDiscontinuousGalerkin2008}). These matrices are instantiated as fields using \mintinline{python}{gt4py.storages}.

\paragraph{DG solver: stencils}

All subsequent computations are carried out using stencils in GT4Py. An example stencil is presented subsequently, related to our DG solver. Applying the theory derived in Section \ref{sec:math} for the linear, constant-coefficient advection problem

   
\begin{equation} \label{eq:lin_adv}
        \frac{\partial u}{\partial t} + \nabla \cdot (\boldsymbol{\beta} u) = 0,
    \end{equation}   
    with e.g., $\boldsymbol{\beta} = [1, 1]^T,$
we will need to evaluate an integral of the following form:
    \begin{equation*}
        \int_{D^k} \left [\beta_1 u \frac{\partial \phi}{\partial x} + \beta_2 u \frac{\partial \phi}{\partial y} \right ] dx dy.
    \end{equation*}
This integral can be computed using the stencil below:
\begin{minted}{python}
    #...
    with computation(PARALLEL), interval(...):
        u_qp = phi @ u_modal
        fx = u_qp * 1
        fy = u_qp * 1
        rhs = determ * (phi_grad_x.T @ (fx * w) / bd_det_x
                        + phi_grad_y.T @ (fy * w) / bd_det_y)
\end{minted}
In line 3, the modal expansion coefficients are mapped to nodal values at the quadrature points. In lines 4 and 5, the flux function in the x and y directions is applied. In this simple case, the flux function is the identity due to the constant velocity field $\boldsymbol{\beta} = [1,1]^T$. Finally, in lines 6 and 7, the numerical integration is performed. The scalar field \mintinline{python}{w} represents the quadrature weights while the matrix-valued field \mintinline{python}{phi_grad_x/y} contains the spatial derivatives of the basis functions. The terms \mintinline{python}{determ}, \mintinline{python}{bd_det_x/y} denote the Jacobians arising from the mapping of the element in physical space onto a reference element. Although not reported here, this description easily generalizes to the SWE case.


\section{Code validation}  \label{sec:validation}

Firstly, several unit tests have been performed to assess  the correctness of our implementations. For the GT4Py implementation, they rely on the existing testing infrastructure of GT4Py, which verifies the success of the code generation as well as the code execution on all backends when compared with a reference Numpy implementation. 

%The unit tests can be found in the following file: 
%    \begin{minted}{text}
%        gt4py/tests/test_integration/test_suites.py
%    \end{minted}
Subsequently, both the G4GT and GT4Py implementations were validated on benchmarks derived from the shallow water test suite~\cite{williamsonStandardTestSet1992}, as well as on tests on a planar geometry presented in \cite{tumolo:2013}. These include a convergence test on linear advection of a smooth profile and on a geostrophic zonal flow, the simulation of geostrophic adjustment on the plane, and that of a Rossby-Haurwitz wave in spherical geometry.
Although our final goal is the simulation of the SWE on the sphere, all the presented tests provide useful insight from a numerical point of view and contribute to a progressive increase in the complexity of the solutions.

\subsection{Linear advection convergence of a smooth initial condition}

Since both implementations are essentially solving the same problem, albeit with slightly different flux calculations and numerical implementations, we summarize the convergence results for both in this section.
Specifically, we  apply   our  DG discretization to  planar linear advection on the unit square, assuming periodic boundary conditions and a constant velocity field $\boldsymbol{\beta}, $
see Equation \eqref{eq:lin_adv}.
We consider a smooth initial condition:
      $   u_0(x, y) = \sin(2 \pi x) \sin(2 \pi y), $
       which allows us to achieve optimal convergence rates. 
The analytic solution of Equation (\ref{eq:lin_adv})  evolves without changing shape in the direction of the velocity field. Due to the periodic boundary conditions, the solution will coincide with the initial condition after one full rotation, i.e., at time $T=1.$
We use a uniform mesh with $K$ elements obtained from the tensor product of $\sqrt{K}$ elements in each of the coordinate directions.
We measure the error of the numerical approximation using the $L^2$ norm at the final simulation time, and we denote it by $\epsilon$.
The expected spatial convergence order for DG methods is given by \cite{hesthavenNodalDiscontinuousGalerkin2008}:
    \begin{equation}
    \label{eq:conv_order}
        \epsilon \sim O(h^{p+1})
    \end{equation}
\noindent
where $h$ is the characteristic mesh size and $p$ is the degree of the local polynomials.
%To validate the scheme, we consider the $L^2(\Omega)$-norm of the error vector evaluated at the final simulation time. Thus, let
%\begin{equation}
%\epsilon=%%\norm{u_h(\cdot,T)-u(\cdot,T)}_{L^2(\Omega)}
%\label{eqn:discretization_error}
%\end{equation}
%Theoretical results for hyperbolic problems \cite{hesthavenNodalDiscontinuousGalerkin2008} suggest that the following optimal estimate holds:
%\begin{equation}
%\epsilon \leq C_1(r) h^{m+1} \left( 1+C_2(r) T \right),
%\label{eqn:discr_error_estimate}
%\end{equation}
%provided that $u$ is sufficiently smooth and a Lax-Friedrichs/Rusanov flux is used. 
To estimate the convergence rate, we compute the discretization error using two different meshes with characteristic sizes $h_1, \ h_2,$
that we denote as $\epsilon_1, \epsilon_2, $ respectively.
%(or equivalently with number of elements equal to $K_1, \ K_2$). 
Then, the estimated rate, denoted by $r$, is computed as
\begin{equation}
r = \frac{\log(\epsilon_1)-\log(\epsilon_2)}{\log(h_1)-\log(h_2)}.
\end{equation}
The results obtained with the G4GT implementation are reported in Table \ref{tab:linear_advection_conv} and agree with the theoretical expectations, thus validating the implementation. Not surprisingly, the GT4Py implementation  achieves nearly identical convergence results to the G4GT version, as shown in   Table \ref{table:sine_conv}.
%The computed convergence is in line with the rate predicted in \ref{eqn:discr_error_p}.

\begin{table}[htbp]
	\centering
	%\resizebox{\textwidth}{!}{
	\begin{tabular}{ccccccc}
		\toprule	
		\multirow{2}{*}{$K$} & \multicolumn{2}{c}{$p=1$} & \multicolumn{2}{c}{$p=2$} &
		\multicolumn{2}{c}{$p=3$} \\ \cmidrule(r){2-3} \cmidrule(lr){4-5} \cmidrule(l){6-7}
		& $\epsilon$ & $r$ & $\epsilon$ & $r$ & $\epsilon$ 
  & $r$ \\ \midrule
		$10^2$ & $\expnumber{1.343}{-2}$ & - & $\expnumber{1.050}{-3}$ & - & $\expnumber{3.780}{-5}$ & - \\ 
		$20^2$ & $\expnumber{3.369}{-3}$ & \textbf{2.00} & $\expnumber{1.329}{-4}$ & \textbf{2.98} & $\expnumber{2.030}{-6}$ & \textbf{4.22} \\
		$40^2$ & $\expnumber{8.405}{-4}$ & \textbf{2.00} & $\expnumber{1.666}{-5}$ & \textbf{3.00} & $\expnumber{1.302}{-7}$ & \textbf{3.96} \\ 
		$80^2$ & $\expnumber{2.099}{-4}$ & \textbf{2.00} & $\expnumber{2.084}{-6}$ & \textbf{3.00} & $\expnumber{8.345}{-9}$ & \textbf{3.96} \\
		$160^2$ & $\expnumber{5.246}{-5}$ & \textbf{2.00} & $\expnumber{2.611}{-7}$ & \textbf{3.00} & & \\ 		
		\bottomrule
	\end{tabular}
	%}
	\caption{$L^2$ errors $\epsilon$ and estimated rate of convergence $r$ for the linear advection problem, G4GT implementation.}
	\label{tab:linear_advection_conv}
\end{table}

   \begin{table}[htb]
        \centering
        \begin{tabular}{ccccccc} 
            \toprule
		\multirow{2}{*}{$K$} & \multicolumn{2}{c}{$p=1$} & \multicolumn{2}{c}{$p=2$} &
		\multicolumn{2}{c}{$p=3$} \\ \cmidrule(r){2-3} \cmidrule(lr){4-5} \cmidrule(l){6-7}
		& $\epsilon$ & $r$ & $\epsilon$ & $r$ & $\epsilon$ 
  & $r$ \\ \midrule
            $20^2$  & 4.204e-3 & - &  1.330e-4 & - & 2.061e-6 & - \\
            $40^2$ & 9.004e-4 & \textbf{2.22} & 1.666e-5 & \textbf{2.99} & 1.288e-7 & \textbf{4.00} \\
            $80^2$ & 2.139e-4 & \textbf{2.07} & 2.084e-6 & \textbf{2.99} & 8.049e-9 & \textbf{4.00} \\
            $160^2$ & 5.212e-5 & \textbf{2.02} & 2.606e-7 & \textbf{3.00} & 5.030e-10 & \textbf{4.00} \\\bottomrule
        \end{tabular}
        \caption{$L^2$ errors $\epsilon$ and estimated rate of convergence $r$ for the linear advection problem, GT4Py implementation.}
        \label{table:sine_conv}
    \end{table}

 %  \begin{table}[htb]
  %      \centering
   %     \begin{tabular}{ccccccccccccc} \toprule
    %        \multirow{2}{*}{$K$}& \multicolumn{2}{c}{$p = 0$} & & \multicolumn{2}{c}{$p = 1$} & &\multicolumn{2}{c}{$p = 2$} & & \multicolumn{2}{c}{$p = 3$} \\
     %       \cmidrule{2-3}\cmidrule{5-6}\cmidrule{8-9}\cmidrule{11-12}
      %       & $\epsilon$ &  $r$ & & $\epsilon$ & $r$ & & $\epsilon$ & $r$ & & $\epsilon$ & $r$ \\\midrule
       %     $20^2$ & 4.389e-1 & - & & 4.204e-3 & - & & 1.330e-4 & - && 2.061e-6 & - \\
        %    $40^2$ & 3.117e-1 & \textbf{0.461} && 9.004e-4 & \textbf{2.223} && 1.666e-5 & \textbf{2.99} && 1.288e-7 & \textbf{4.00} \\
         %   $80^2$ & 1.932e-1 & \textbf{0.690} && 2.139e-4 & \textbf{2.074} && 2.084e-6 & \textbf{2.99} && 8.049e-9 & \textbf{4.00} \\
          %  $160^2$ & 1.084e-1 & \textbf{0.834} && 5.212e-5 & \textbf{2.020} && 2.606e-7 & \textbf{3.00} && 5.030e-10 & \textbf{4.00} \\\bottomrule
 %       \end{tabular}
  %      \caption{$L^2$ errors $\epsilon$ and estimated rate of convergence $r$ for the linear advection problem, G4Py implementation.}
   %     \label{table:sine_conv}
   % \end{table}

% \subsubsection{GT4Py implementation}

% Due to the different structure of the GT4Py and G4GT implementations, in the GT4Py case we first discuss some aspects of the code structure.

% \paragraph{Precomputation}

% At the start of the execution of the program, the GT4Py solver precomputes on the CPU certain variables  that remain constant during the whole simulation. This includes the computation of the inverse of the mass matrix, as well as the Gauss-Legendre quadrature points and weights for numerical integration. A helper-class called Vander, defined in \mintinline{sh}{vander.py}, contains all the Vandermonde matrices  required to evaluate the polynomials stored as modal expansion coefficients at nodal values in the domain (see e.g. \cite{hesthavenNodalDiscontinuousGalerkin2008}). These matrices are instantiated as Fields using \mintinline{python}{gt4py.storages}.

% \paragraph{Stencils}

% All subsequent computation are carried out using stencils in GT4Py. An example stencil is presented subsequently. Applying the theory derived in Section \ref{sec:math} for the linear advection problem, we will need to evaluate an integral of the following form:
%     \begin{equation*}
%         \int_{D^k} \beta_1 u \frac{\partial \phi}{\partial x} + \beta_2 u \frac{\partial \phi}{\partial x} dx dy
%     \end{equation*}
    
% This integral can be computed using the stencil below:
% \begin{minted}[xleftmargin=\parindent, linenos]{python}
%     #...
%     with computation(PARALLEL), interval(...):
%         u_qp = phi @ u_modal
%         fx = u_qp * 1
%         fy = u_qp * 1
%         rhs = determ * (phi_grad_x.T @ (fx * w) / bd_det_x
%                         + phi_grad_y.T @ (fy * w) / bd_det_y)
% \end{minted}
% In line 3, the modal expansion coefficients are mapped to nodal values at the quadrature points. In line 4 and 5, the flux function in x and y direction is applied. In this simple case, the flux function is the identity due to the constant velocity field $\boldsymbol{\beta} = [1,1]^T$. Finally, in line 6 and 7, the numerical integration is performed. The scalar Field \mintinline{python}{w} represents the quadrature weights while the matrix-valued Field \mintinline{python}{phi_grad_x/y} contains the spatial derivatives of the basis functions. The terms \mintinline{python}{determ}, \mintinline{python}{bd_det_x/y} denote the Jacobians arising from the mapping of the element in physical space onto a reference element.

 
%    \begin{figure}[htb]
%        \centering
%        \includegraphics[width=0.5\textwidth]{img/sine_conv.pdf}
%        \caption{Convergence of errors with mesh refinement for the linear advection problem.}
%        \label{fig:sine_conv}
%    \end{figure}
 
    
    %Notice that, in order to maintain the required accuracy also for the time discretization for each value of $p$ which characterizes the spatial convergence order order, the  Runge-Kutta  method of corresponding order was used for time discretization.
%    We have also experimented with
%    method of different order in space and time. 
%    Due to the predicted 
%    convergence rate \eqref{eq:conv_order},
%    it is reasonable to expect optimal results if 
%    are used for the time discretization,
%    coupled to a spatial discretization
%    that employs polynomials of order $p.$
%    In our experiments, we have indeed observed super-optimal convergence rates by using higher-order methods in time than in space.
%    Figure \ref{fig:mixed_order} illustrates the convergence of the error for the different polynomial degrees with fixed order in time.
 %   \begin{figure}[htb]
  %
  %\begin{subfigure}{.5\textwidth}
    %
    %\includegraphics[width=0.9\textwidth]{img/rk3.pdf}
     %       \caption{3rd order in time}
      %      \label{fig:mixed3}
       % \end{subfigure}%
        %\begin{subfigure}{.5\textwidth}
         %   \centering
          %  \includegraphics[width=.9\textwidth]{img/rk4.pdf}
           % \caption{4th order in time}
            %\label{fig:mixed4}
       % \end{subfigure}
        %\caption{Convergence of error for mixed order methods in space and time.
        %In each plot, a specific temporal order is combined with different spatial orders.}
        %\label{fig:mixed_order}
   % \end{figure}
    %In plot \ref{fig:mixed3}, a 3rd order Runge-Kutta method was used with all polynomial degrees.  
 %   We can observe that the scheme using 2nd order method in space ($p=1$) has an improved convergence rate of 3.
  %  Second, the 1st order method remains unchanged.
   %Finally, the 4th order scheme in space is limited by the 3rd order time discretization method and hence only exhibits a 3rd order convergence rate.
    %These results suggests that, for the DG scheme, a time discretization of one order of convergence higher than the space discretization method may be chosen while still achieving the higher order convergence rates.
    %The same trend is observed in the plot \ref{fig:mixed4} which illustrates the convergence rates for this numerical experiment using a 4th order Runge-Kutta method.

\subsection{G4GT implementation:  geostrophic adjustment for planar SWE}

For the G4GT implementation, we consider the SWE discretized on a Cartesian mesh in a planar domain. Specifically, we want to show that the scheme is able to reproduce the geostrophic adjustment process, see, for example, the discussion in
\cite{tumolo:2013}. Starting from a perturbation of the equilibrium state corresponding to a constant water height, gravitational and rotational forces interact, so that only part of the energy is transported away from the center, leading to a nontrivial stationary solution profile.
\begin{figure}[htbp]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/geostrophic_height_initial.pdf}
		\subcaption{Initial height.}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/geostrophic_height.pdf}
		\subcaption{Height.}
	\end{subfigure}
	
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/geostrophic_horizontal_v.pdf}
		\subcaption{Horizontal velocity.}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/geostrophic_vertical_v.pdf}
		\subcaption{Vertical velocity.}
	\end{subfigure}
	
	\caption{Numerical results for the geostrophic adjustment test case.}
	\label{fig:geostrophic_adj}
\end{figure}
\noindent 
Consider a square domain $\Omega=[0,L]^2$ with $L=10^7 \ \rm m$ and a final time of $T=36000 \ \rm s$. The initial velocities and momenta are set to zero, while the height $h$ is equal to
\begin{equation}
    h=h_0+h_1 \exp \left( - \frac{(x-L/2)^2+(y-L/2)^2}{2 \sigma^2}  \right),
\end{equation}
where $h_0=1000 \ \rm m$, $h_1=5 \ \rm m$ and $\sigma=L/20 \ \rm m$. Assuming an $f$-plane approximation, the Coriolis parameter $f$ is chosen to be constant and equal to $10^{-4} \ \rm s^{-1}$. The problem is completed with periodic boundary conditions. The simulation has been run using $50$x$50$ spatial elements, a polynomial degree $r=3$ and the RK4 scheme in time with step $\Delta t=100 \ \rm s$. The results are reported in \figref{fig:geostrophic_adj}.
The solution is consistent with the results reported in \cite{tumolo:2013}. 

\subsection{GT4Py implementation: geostrophic zonal flow for SWE on the sphere}
After validating the planar version of the GT4Py implementation, we consider two of the classical test cases in spherical geometry introduced in \cite{williamsonStandardTestSet1992}
for the shallow water equations.
%Note that the mass matrices are all identical for a specific longitudinal value. This allows us to mask them in the longitudinal direction to save memory. In fact, masking of fields was employed when possible. This turns out to be particularly important for the GPU-targeting backends, as the accelerator has limited memory capacity.
%In order to simplify the definition of the conserved variables, we incorporate their metric terms directly into the local mass matrix: 
  %  \begin{equation*}
  %      M_{ij}^{(K)} = \int_{D^k} \phi_i(\lambda, \theta) %\phi_j(\lambda, \theta) \cos(\theta) d\lambda d\theta.
  %  \end{equation*}
Periodic boundary conditions were applied in the longitudinal direction, while in the latitudinal direction the fluxes were set to zero. Indeed, since the edges become singular at the poles, the flux through them must be zero.

In the benchmark denoted as test case 2 in \cite{williamsonStandardTestSet1992}, a stationary
zonal flow in geostrophic equilibrium is considered. We perform a convergence test
for the spatial discretization, using for all polynomial degrees the RK4 scheme for time discretization
with  time steps chosen for each resolution so as to keep the Courant number constant and at a very small value. The test case has been run until time $T=2 $ days on meshes of increasing resolutions. The results are reported in Table \ref{tab:test2_conv} and show a 
convergence behavior entirely analogous to that of the linear advection case in planar geometry.

%Same number of elements in horizontal and vertical direction, $T=2$ days,  $\Delta t=C(p)h$ where $C(p)=200,100/2,80/3$ for $p=1,2,3$, respectively.

\begin{table}[htbp]
	\centering
	%\resizebox{\textwidth}{!}{
	\begin{tabular}{ccccccc}
		\toprule	
		\multirow{2}{*}{$K$} & \multicolumn{2}{c}{$p=1$} & \multicolumn{2}{c}{$p=2$} &
		\multicolumn{2}{c}{$p=3$} \\ \cmidrule(r){2-3} \cmidrule(lr){4-5} \cmidrule(l){6-7}
		& $\epsilon$ & $r$ & $\epsilon$ & $r$ & $\epsilon$ 
		& $r$ \\ \midrule
		$10^2$ & $\expnumber{9.366}{-2}$ & - & $\expnumber{1.020}{-3}$ & - & $\expnumber{6.864}{-5}$ & - \\ 
		$20^2$ & $\expnumber{1.984}{-3}$ & \textbf{5.56} & $\expnumber{1.085}{-4}$ & \textbf{3.23} & $\expnumber{3.951}{-6}$ & \textbf{4.08} \\
		$40^2$ & $\expnumber{4.508}{-4}$ & \textbf{2.13} & $\expnumber{1.490}{-5}$ & \textbf{2.86} & $\expnumber{2.362}{-7}$ & \textbf{4.06} \\ 
		$80^2$ & $\expnumber{1.111}{-4}$ & \textbf{2.02} & $\expnumber{1.986}{-6}$ & \textbf{2.90} & $\expnumber{1.471}{-8}$ & \textbf{4.00} \\ 		
		\bottomrule
	\end{tabular}
	%}
	\caption{$L^2$ errors $\epsilon$ and estimated rate of convergence $r$ for the Williamson test case 2, GT4Py implementation.}
	\label{tab:test2_conv}
\end{table}

\subsection{GT4Py implementation: Rossby-Haurwitz wave for SWE on the sphere}

The Rossby-Haurwitz wave (denoted as test case 6 in \cite{williamsonStandardTestSet1992}) consists of a large-scale planetary wave that mimics the high/low-pressure systems typical of mid-latitude weather patterns.
The test case considers initial data that would result in a stable solution for the barotropic vorticity equation, evolving from west to east without changing shape. It is known that this configuration is ultimately unstable --- see, for example, the discussion in \cite{thuburn2000numerical} --- but this instability only arises on a relatively long time scale. Therefore, it is customary to
assess the quality of numerical methods based on their capability to reproduce a stable eastward moving pattern for several days. In Figure \ref{fig:rossby-waves}, we see the results of an 8-day simulation of the Rossby-Haurwitz wave on a 40x20 grid using the RK4 method in time with $\Delta t=4$ s and $p=3$ in space.
\begin{figure}[htbp]
	\centering
	\begin{subfigure}[t]{0.32\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/rossby_0days_height.eps}
		\subcaption{$h$ at $t=0$ days.}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/rossby_0days_horizontal.eps}
		\subcaption{$u$ at $t=0$ days.}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/rossby_0days_vertical.eps}
		\subcaption{$v$ at $t=0$ days.}
	\end{subfigure}

	\begin{subfigure}[t]{0.32\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/rossby_4days_height.eps}
		\subcaption{$h$ at $t=4$ days.}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/rossby_4days_horizontal.eps}
		\subcaption{$u$ at $t=4$ days.}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/rossby_4days_vertical.eps}
		\subcaption{$v$ at $t=4$ days.}
	\end{subfigure}

	\begin{subfigure}[t]{0.32\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/rossby_8days_height.eps}
		\subcaption{$h$ at $t=8$ days.}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/rossby_8days_horizontal.eps}
		\subcaption{$u$ at $t=8$ days.}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/rossby_8days_vertical.eps}
		\subcaption{$v$ at $t=8$ days.}
\end{subfigure}

	\caption{8-day simulation of the Rossby-Haurwitz wave.}
	\label{fig:rossby-waves}
\end{figure}%


  %  \begin{figure}[htb]
    %    \centering
     %   \begin{subfigure}[tb]{0.65\textwidth}
     %       \centering
     %       \includegraphics[width=\linewidth]{img/rosby_init.png}
     %       \caption{Initial conditions}
     %   \end{subfigure}\\
        %
        %\begin{subfigure}[b]{0.5\textwidth}
         %   \centering
          %  \includegraphics[width=\linewidth]{img/rosby_30h.png}
           % \caption{After 30 hours}
        %\end{subfigure}
     %   \begin{subfigure}[tb]{0.65\textwidth}
     %       \centering
      %      \includegraphics[width=\linewidth]{img/rosby_3days.png}
       %     \caption{After 3 days}
       % \end{subfigure}\\%
        %\begin{subfigure}[tb]{0.65\textwidth}
        %    \centering
         %   \includegraphics[width=\linewidth]{img/rosby_8days.png}
          %  \caption{After 8 days}
       % \end{subfigure}
       % \caption{8-day simulation of the Rossby-Haurwitz wave using our 4th order DG scheme in space and time.
       % Each figure depicts the  height and latitudinal and longitudinal velocities at a given time.}
        %\label{fig:rosby-waves}
   % \end{figure}
   % We used a constant time step of $ \Delta t= 0.8 \ \rm s.$ 
    It can be observed that
    the numerical solution indeed evolves from west to east while maintaining a close resemblance with the initial shape and that the simulated pattern is in good agreement with reference solutions, see e.g., \cite{tumolo:2015}.
    %As expected, the scheme exhibits minor numerical artifacts concentrated at the poles for long simulation times.
    
\section{Performance} \label{sec:performance}

In this section, we present performance benchmarks of the G4GT and GT4Py implementations for the SWE in both planar and spherical geometry. In order to provide an adequate comparison, we break down the G4GT implementation into three computing blocks which allow us to better analyze the complexity and compare the execution time of the temporal loop for the various backends supported by GT4Py. In both cases, the time spent in the precomputation steps is neglected, as it becomes negligible for long simulation periods.

The G4GT simulations have been run on the compute nodes of {\it Piz Daint} at CSCS, using an Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered} E5-2690 v3, 12-core processor, characterized by a peak memory bandwidth of 68 GB/s.
On the other hand, the GT4Py benchmarks were performed on a different partition of {\it Piz Daint} with the CPU code executed on two 18-core Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered} E5-2695 v4 @ 2.10GHz processor (each with 77 GB/s peak memory bandwidth) and the GPU code on an NVIDIA\textsuperscript{\textregistered} Tesla\textsuperscript{\textregistered} P100 with 16GB of memory (540 GB/s peak memory bandwidth).  
%The Matlab implementation was run locally on a quad-core Intel\textsuperscript{\textregistered} Core i7 6700HQ @ 2.60GHz.

\subsection{G4GT performance evaluation} 
 

The geostrophic adjustment setup presented in the previous section can also be used to evaluate the performance of the method and G4GT in general. Unless stated otherwise, the physical and numerical parameters are therefore kept unchanged.

The performance evaluation is done using the Roofline model \cite{Williams2009}. This is based on the \textit{operational intensity}, i.e., the number of floating-point operations (flops) per byte of DRAM traffic, and the \textit{attainable Gflops per second}, i.e., the concrete performance measure. Here, the DRAM traffic takes into account the bytes that are read from/written in the main memory after the filter of the cache hierarchy. Because of hardware limits, the attainable flops per second cannot go beyond a fixed threshold, determined by the peak memory bandwidth and the peak floating point performance. Commonly, this threshold is determined by running benchmark cases, such as the (bandwidth-limited) \textit{stream} or the (computationally-limited) \textit{linpack} benchmark. However, at the time at which the simulations were performed, these were not available, and we relied on nominal values instead. Thus, for a given operational intensity, an efficient implementation in terms of performance should attain values close to the determined limit. In our analysis, we decided to ignore the cache effects. In other words, every access to a variable is considered for the computation of the required bytes. This is in contrast with the definition provided by the model, but a precise estimate of the DRAM traffic is far from an easy task. \\
The matrix-vector multiplication, which is the central operation in the DG implementation, is bandwidth-limited and achieves a performance \cite{discacciati2018implementation} somewhat below the leftmost (rising) roofline. \\
Based on the way in which the code is structured \cite{discacciati2018implementation}, we can recognize three different kernels:
%
\begin{enumerate}
	\item {\bf Common part:} The nodal values for the solution and the flux function are computed.
	\item {\bf Rusanov fluxes:} The boundary fluxes are computed, and the boundary conditions are applied. This requires communication among neighboring elements.
	\item {\bf Main computation:} The right-hand side is assembled, and the solution is updated.
\end{enumerate}
%
The results for varying polynomial degree $p$ are reported in Figure \ref{fig:SWE_CPU_RK4}, which compares the performances of the global program and the kernels separately. As a complement to Figure \ref{fig:SWE_CPU_RK4}, Table \ref{table:SWE_times_CPU} breaks down the computational times.
Looking at the overall performance, we observe that no significant variations in the operational intensities are present. This is because variations in the polynomial order lead to similar changes in the number of floating point operations and memory traffic. Despite the high number of optimizations, the attained performance values fall slightly below the theoretical limit. This discrepancy would be reduced by taking into account the cache effects and using peak values determined with benchmark tests instead of nominal values. We also observe that performances obtained with linear basis functions are slightly lower than higher-order polynomials. Optimizations might not be fully triggered using low orders, causing a loss in performance. 

Looking at the kernels independently, for high values of $p$ the third kernel has the most significant influence on the overall performance. This is expected since it includes the majority of the computations. Specifically, the assembly of the internal integral is the most intensive part, both in terms of resources and time. 
On the other hand, the second kernel always has a low computational cost. This is not surprising, as only boundary quantities are involved. Its performances in terms of floating point operations per second are consistently low and do not vary with $p$. Since it is the only phase that involves exchanges between neighboring elements, it is reasonable that cache misses or inefficient memory accesses are present. 
No particular trend is observed for the first kernel, except for $p=1$, in which this kernel has the dominant effect on global performance.
%
\begin{figure}[htbp]
	\centering
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/SWE_CPU_kernels_nozoom.pdf}
		\subcaption{Global range}
		\label{fig:SWE_CPU_nozoom}
	\end{subfigure}
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/SWE_CPU_kernels.pdf}
		\subcaption{Zoom into the interval $[0.09,0.18]$}
		\label{fig:SWE_CPU_zoom}
	\end{subfigure}
	\caption{Performance evaluation of the G4GT implementation of the SWEs in a Cartesian geometry with different spatial degrees $p$, RK4 scheme.}
	\label{fig:SWE_CPU_RK4}
\end{figure} 
%
%
\begin{table}[htbp]
	\centering
	\begin{tabular}{lcccc}\toprule
		$p$ & Global [$\rm s$] & Kernel 1 [$\rm s$] & Kernel 2 [$\rm s$] & Kernel  3 [$\rm s$]\\\midrule
		1 & 15.17 & 7.42 & 1.45 & 6.31 \\
		%\hline
		2 & 78.98 & 21.22 & 3.10 & 54.66 \\
		%\hline
		3 & 430.21 & 54.97 & 5.81 & 369.43 \\\bottomrule
	\end{tabular}
	\caption{Computational times of the G4GT implementation of the SWEs in a Cartesian geometry with different spatial degrees $p$, RK4 scheme.}
	\label{table:SWE_times_CPU}
\end{table}

\subsection{GT4Py performance evaluation}

For the GT4Py implementation, we consider both performance scalability while increasing the horizontal problem size 
as well as increasing the number vertical layers while holding the horizontal size constant.

\subsubsection{Horizontal scaling}
    In the first two experiments, we study the scaling of the execution time with increased horizontal problem size.
    Thus, for each subsequent data point, the number of grid points is doubled in both horizontal directions.
    This leads to an asymptotic scaling of the algorithm being $O(N^2)$, where $N$ is the number of grid points in each horizontal direction.

%%%    We use as a baseline comparison an implementation of the DG scheme written in Matlab.

    For this benchmark, we use a 4th-order scheme in space which corresponds to a vector of size 16 stored at each grid point (\mintinline{text}{data_dims = 16}).
    The performance of the various backends is illustrated in Figure \ref{fig:nz_scaling}.

    \begin{figure}[htb]
        \centering
        \includegraphics[width=0.5\textwidth]{img/nomatlab_nz_scaling.pdf}
        \caption{Benchmark of the execution time of the GT4Py backends  with increasing problem size.
        Each subsequent data point doubles the grid points in x and quadruples the total number of grid points.}
        \label{fig:nz_scaling}
    \end{figure}
    %The slowest implementation, by far, is the one written in Matlab. Since it was not clear how optimal this implementation is, we do not consider this code in the subsequent comparison.
    We use the code generated by the \mintinline{text}{dace:cpu} -- which is currently only a prototype -- as the baseline, in the hope that DaCe will produce more performant code with time. The first observation is that all other backends perform significantly better. In addition, its GPU equivalent (\mintinline{text}{dace:gpu}) is not listed, because it was not yet functionally complete.
%    This leads us to advise against using the DaCe backends for the time being as they do not exhibit good
%    performance and produce unreliable results.
    The two CPU backends powered by the GridTools framework have virtually identical execution times and both produce faster executables than the DaCe CPU backend.
    The two GPU backends (namely the \mintinline{text}{cuda} and \mintinline{text}{gt:gpu}), perform worse than the CPU backends for small problem sizes. This is due to the resources on the GPU not being fully saturated for small problem sizes. However, their scaling with increased problem size is more favorable and hence they both end up outperforming the CPU code for larger problems.
    Even so, the GridTools GPU backend undoubtedly produces a better implementation as it consistently outperforms the native CUDA backend.
    Note that we were limited to presenting a maximum problem size of 640x640 due to memory constraints on the GPU.

    Table \ref{table:speedup} summarizes the speedup observed versus the DaCe CPU baseline on the largest problem size.
    \begin{table}[htb]
        \centering
%        \begin{tabular}{c|cccccc}
        \begin{tabular}{cccccc}\toprule
%            & Matlab & \mintinline{text}{dace:cpu} & \mintinline{text}{gt:cpu_kfirst} & \mintinline{text}{gt:cpu_ifirst} & \mintinline{text}{cuda} & \mintinline{text}{gt:gpu} \\
             &\mintinline{text}{dace:cpu} & \mintinline{text}{gt:cpu_kfirst} & \mintinline{text}{gt:cpu_ifirst} & \mintinline{text}{cuda} & \mintinline{text}{gt:gpu} \\\midrule
%            speedup & 1.0x & 10.2x & 52.5x & 55.1x & 63.4x & 175.2x
             Speedup Factor & 1.00 & 5.14 & 5.40 & 6.22 & 17.2 \\\bottomrule
        \end{tabular}
        \caption{Speedup of GT4Py backends vs reference \texttt{dace:cpu} implementation on 640x640 grid.}
        \label{table:speedup}
    \end{table}
    %We observe considerable performance benefits by using GT4Py even when accounting for the single-threaded nature of the Matlab implementation and the difference in architectures on which they were executed.
One surprising observation from Table \ref{table:speedup} is that the fastest GPU backend only produces a $\sim 3.5$x speedup versus the fastest CPU backend.
    We hypothesize that the operations implemented in the frontend for higher-dimensional fields lead to poor memory access patterns in the generated code, which can particularly hinder performance benefits of accelerators.
    
    To verify our claim, we decrease the vector size stored at each grid point from 16 to only 1 (\mintinline{text}{data_dims = 1}).
    This essentially reduces the DG scheme to a first-order finite volume method. In Figure \ref{fig:scalar_scaling}, we compare the performance of the best CPU and GPU backends.
    \begin{figure}[htb]
        \centering
        \includegraphics[width=0.5\textwidth]{img/notitle_scalar_scaling.pdf}
        \caption{Benchmark of best-performing CPU and GPU backends for a simple finite volume scheme with \mintinline{python}{data_dims}=1.
        Final data point is unavailable for \mintinline{python}{gt:gpu} backend due to the memory limit being reached on the GPU.}
        \label{fig:scalar_scaling}
    \end{figure}
    The benchmark seems to validate our hypothesis as we observe a larger gap in performance for big problem sizes, reaching up to $10$x speedup on the GPU.
    
\subsubsection{Vertical scaling}
    In this Section we try to assess the potential performance of GT4Py on a 3-dimensional problem by considering a set of
    decoupled 2-dimensional SWE problem  copied in the vertical direction and solved in parallel.  This configuration increases the computational load and resembles to some extent those of low order finite difference/finite volume discretizations of 3-dimensional problems in atmospheric modelling. However, it is substantially different from a full 3-dimensional DG discretization, since all the local matrices that arise correspond to 2-dimensional rather than 3-dimensional elements.
    
    The resulting algorithm scales linearly with the number of vertical levels.
    Considering that all levels can be solved independently, this problem is embarrassingly parallel, and we might expect performance benefits on the GPU compared to the CPU.

    Figure \ref{fig:nx300} illustrates the execution time of a 4th-order DG scheme on a 300x300 grid with increasing vertical levels.
    \begin{figure}[htb]
        \centering
        \includegraphics[width=0.5\textwidth]{img/notitle_nx300_scaling.pdf}
        \caption{Benchmark of best-performing CPU and GPU backends in addition to the CUDA backend.
        The plot depicts execution time with respect to the number of identical vertical problems solved in parallel.
        The final data point for the CUDA backend is unavailable due to the memory limit reached on GPU.}
        \label{fig:nx300}
    \end{figure}
    Surprisingly, we do not observe any scaling benefits for this experiment on the GPU.
    Indeed, all backends exhibit linear scaling from the first data point, indicating that the hardware's resources are already fully saturated.
    This is most likely due to the 2-dimensional problem solved in each level being too large and already fully occupying the memory bandwidth of the GPU.
    We observe that the CUDA backend performs even worse than the GridTools CPU backend.
    Moreover, it suffers from poor memory management compared to the GridTools GPU implementation, as the last data point could not be gathered due to the memory capacity of the GPU being reached.
    This observation is corroborated by the estimated GPU memory usage reported in the job summaries on Piz Daint.
    Table \ref{table:memory} compares the memory usage between the two GPU backends for identical problems.
    In all cases, the CUDA implementation consumes significantly more memory than an equivalent GridTools implementation.
    \begin{table}[htb]
        \centering
        \begin{tabular}{lcccccc}\toprule
            Nx & 40 & 80 & 160 & 320 & 640 & 1280 \\
            \midrule
            \mintinline{text}{gt:gpu} [MB] & 479 & 507 & 631 & 1059 & 2635 & 8807 \\
            \mintinline{text}{cuda} [MB] & 3649 & 3677 & 3801 & 4229 & 5805 & 11977 \\\bottomrule
        \end{tabular}
        \caption{Max memory usage [MB] produces by Piz Daint job output file for various problem sizes.}
        \label{table:memory}
    \end{table}

\subsubsection{GPU profiling}
    In this section, we present the results of a brief profiling that was carried out for the GridTools and CUDA GPU backends using the NSight Systems tools from NVIDIA.

    We profile the 4th-order DG scheme for the 2-dimensional SWE on a 640x640 grid.
    Figure \ref{fig:profile_timeline} illustrates an extract of the profile timeline for the first two time steps of the simulation.
    \begin{figure}[htb]
        \centering
        \includegraphics[width=0.7\textwidth]{img/nsys.png}
        \caption{Extract of NSight Systems profile of \mintinline{python}{gt:gpu} backend.
        The profile indicates consistent executions of kernels on GPU and no communication between the host and device after the first time step.}
        \label{fig:profile_timeline}
    \end{figure}
    The green sections indicate memory transfers from the host to the device, while the blue areas highlight kernel executions.
    To achieve good performance on the GPU, communication between the device and the host needs to be reduced as much as possible.
    Considering the memory row in Figure \ref{fig:profile_timeline}, we only observe communication (more precisely host-to-device transfers) in the first time step.
    These are necessary to transfer the initial fields to the GPU.
    Subsequently, the GPU is able to execute kernels consistently without having to wait for unnecessary communication from the host.

    In Figure \ref{fig:profile_metrics}, we compare the performance metrics provided by the profiler for the GridTools and CUDA backends for the largest stencil in our scheme.
    \begin{figure}[htb]
        \begin{subfigure}{.5\textwidth}
            \centering
            \includegraphics[width=0.6\textwidth]{img/gpu_nsys.png}
            \caption{\mintinline{python}{gt:gpu} backend}
        \end{subfigure}%
        \begin{subfigure}{.5\textwidth}
            \centering
            \includegraphics[width=0.55\textwidth]{img/cuda_nsys.png}
            \caption{\mintinline{python}{cuda} backend}
        \end{subfigure}
        \caption{Snippet of NSight Systems profile metrics for largest (compute intensive) stencil of the DG scheme. There is an indication of poor memory management by the CUDA backend with the Local Memory Total being of the order of 18 Exabytes.}
        \label{fig:profile_metrics}
    \end{figure}
    We observe that the execution time for the kernel is almost 4x slower on the CUDA backend.
    Surprisingly, the CUDA backend is stated to have 100\% theoretical occupancy when compared to the 25\% of the GridTools backend.
    This is an indication that the generated code from GridTools could also further be improved.

\section{Conclusions}
\label{sec:conclusions}


We have presented two implementation examples of a high order DG method in the framework of the G4GT and GT4Py domain-specific languages, respectively.
The G4GT implementation illustrated that a DSL designed for finite difference/volume methods on rectangular grids, with some extensions, could be reused to achieve the implementation of a DG solver.
The related performance analysis illustrated clearly that the DG method is limited by the memory bandwidth, similar to the sparse matrix-vector (SpMV) stencil discussed in  \cite{Williams2009}.

Despite being only a proof of concept, the G4GT DSL has several advantages for the end user over GT, with the main one being the simplicity in its use. Most of the back-end optimizations and the definition of appropriate data structures are hidden in the underlying GT implementation. Therefore, the user can exploit the full capability of GT in a broader range of discretization schemes without delving into the details required to achieve computational efficiency, in complete agreement with the requirements of a good DSL.
The main drawback lies in the need for external libraries, such as Intrepid~\cite{Intrepid} and Epetra~\cite{Epetra}), on top of GT. Ensuring compatibility among all of them can introduce further complexities and limit the usability of the library. Several technical reasons, including the inability to construct nested functors, as well as the emergence of Python as a programming language, were responsible for the termination of the development of G4GT and the migration to GT4Py.

Despite the user-friendly nature of GT4Py and the increasing support and functionalities available, we had to expand the support for higher-dimensional fields by supplementing the frontend with an new, clean syntax. This allowed us to port an original Matlab implementation to GT4Py with relative ease. Moreover, we saw that we could automatically exploit available accelerators without having to modify the source code.
Nonetheless, due to limitations of the functionality of higher-dimensional fields, we were left with writing a lot of boilerplate code in GT4Py.
In particular, slicing is currently not supported, which, if implemented, would allow us to condense the conserved variables into a large matrix instead of a series of separate vectors. In addition, there is currently no method  for calling functions on higher-dimensional fields which would eliminate the need to copy the same functionality to different stencils.

In conclusion, the new GridTools support for higher-dimensional fields opens the door for implementing advanced numerical schemes like DG in a DSL environment.
However, our front-end changes should be complemented with corresponding back-end optimizations to achieve better performance. 

\section*{Acknowledgments}

The Swiss National Supercomputing Centre (CSCS) funded the four-month internships of the first two authors, N.D. and K.S.. We would like to thank Linus Groner, Till Ehrengruber, Mauro Bianco and Christopher Bignamini of CSCS for their gracious support during both internships. L.B. was partially supported by the ESCAPE-2 project, European Unions Horizon 2020
Research and Innovation Programme (Grant Agreement No. 800897).

\bibliographystyle{plainnat}
%\bibliographystyle{plain}
\bibliography{biblio}
%\printbibliography

\end{document}

%
%
