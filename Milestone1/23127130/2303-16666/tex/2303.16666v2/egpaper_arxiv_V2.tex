 \documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage[table]{xcolor}
%\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\definecolor{Gray}{gray}{0.9}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{booktabs, siunitx}
\usepackage{subcaption}

%\linespread{1.2} % Set linespace
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\usepackage{multicol} % <===============================================


\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{SC-VAE: Sparse Coding-based Variational Autoencoder with Learned ISTA}

\author{Pan Xiao\qquad Peijie Qiu\qquad Sungmin Ha\qquad Abdalla Bani \qquad Shuang Zhou \qquad Aristeidis Sotiras\\
Washington University School of Medicine, Washington University in St. Louis\\
%St Louis, MO, USA\\
{\tt\small \{panxiao, peijie.qiu, sungminha, a.bani, shuangzhou, aristeidis.sotiras\}@wustl.edu}}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Peijie Qiu\\
%Washington University in St. Louis\\
%First line of institution2 address\\
%{\tt\small aristeidis.sotiras@wustl.edu}
%\and
%Aristeidis Sotiras\\
%Washington University in St. Louis\\
%First line of institution2 address\\
%{\tt\small aristeidis.sotiras@wustl.edu}
%}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

\input{sec/0_abstract}
\input{sec/1_intro}
\input{sec/2_related_work}
\input{sec/3_preliminary}
\input{sec/4_approach}
\input{sec/5_experiments}
\input{sec/6_conclusion}

\newpage
% \clearpage
% \pagebreak
% \onecolumn % 


\input{sec/X_suppl}
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

% \pagebreak
% \onecolumn % <==========================================================
% %\input{Supplementary_Material.tex}

% %%%%%%%%% TITLE
% \begin{center}
% \textbf{\Large SC-VAE: Sparse Coding-based Variational Autoencoder with Learned ISTA}\\
% \textbf{\Large -}\\
% \textbf{\Large Supplemental Materials}
% \end{center}

% \maketitle
% % Remove page # from the first page of camera-ready.
% \ificcvfinal\thispagestyle{empty}\fi

% The supplementary material for our work  \textit{SC-VAE: Sparse Coding-based Variational Autoencoder} is structured as follows:
% First, Sec. 1 provides the detailed information of the encoder and decode architecture of the SC-VAE model. 
% Additional results on image patches clustering and unsupervised image segmentation on FFHQ and ImageNet datasets are then presented in Sec. 2 and Sec. 3, respectively.

% \setcounter{section}{0}
% %\renewcommand\thesubsection{\Alph{subsection}}


% \section{The Encoder and Decoder Architecture of SC-VAE} \label{section_1_s}
% \noindent\textbf{Encoder and Decoder.} 
% The architecture of the proposed encoder and decoder is identical to the VQGAN \cite{esser2021taming}, which is described in Table 3. 
% $H$, $W$ and $C$ denote the height, width
% and the number of image channels, respectively.
% $C'$ and $C''$ represent the number of channels of the feature maps that are produced as output by two different intermediate layers of the encoder and decode network.
% In our experiment, $C'$ and $C''$ were set to $128$ and $512$, respectively. $n$ denotes the number of dimensions of each discrete latent representation, which was set to $256$.
% The variable $d$ represents the number of blocks used for downsampling. Therefore, we can calculate the height ($h$) and width ($w$) of the output feature maps by dividing the height ($H$) and width ($W$) of the input images by $2$ raised to the power of $d$.
% \\
% %\noindent
% %\noindent\textbf{Learnbale ISTA.} The architecture of our Learnable ISTA network is shown in Table 2.
% %\noindent\textbf{Attention Network for $\alpha$ Estimation.}  Our neural network architecture follows the backbone of PixelCNN++ [52], which is a U-Net [48] based on a Wide ResNet [72]. We replaced weight normalization [49] with group normalization [66] to make the implementation simpler. Our 32 × 32 models use four feature map resolutions (32 × 32 to 4 × 4), and our 256 × 256 models use six. All models have two convolutional residual blocks per resolution level and self-attention blocks at the 16 × 16 resolution between the convolutional blocks [6].


% \begin{table*}[!htbp]
% \centering
% \caption{High-level architecture of the encoder and decoder of the proposed SC-VAE. Note that $h=\frac{H}{2^{d}}$, $w=\frac{W}{2^d}$.} 
% \begin{tabular}{c|c}
%   \toprule
%   Encoder &  Decoder\\
%   \midrule
%   $x\in \mathbb{R}^{H\times W\times C} $ &   $\mathbf{D} Z\in \mathbb{R}^{h\times w\times n} $  \\
%   2D Convolution $\rightarrow \mathbb{R}^{H\times W\times C'}$ &  2D Convolution $\rightarrow \mathbb{R}^{h\times w\times C''}$  \\
%   $d \times$\{Residual Block, Downsample Block\} $\rightarrow \mathbb{R}^{h\times w\times C''}$ &  Residual Block $\rightarrow \mathbb{R}^{h\times w\times C''}$\\
%   Residual Block $\rightarrow \mathbb{R}^{h\times w\times C''}$  & Non-Local Block $\rightarrow \mathbb{R}^{h\times w\times C''}$\\
%   Non-Local Block $\rightarrow \mathbb{R}^{h\times w\times C''}$ & Residual Block $\rightarrow \mathbb{R}^{h\times w\times C''}$\\
%   Residual Block $\rightarrow \mathbb{R}^{h\times w\times C''}$  & $d\times$\{Residual Block, Upsample Block\} $\rightarrow \mathbb{R}^{H\times W\times C'}$\\
%   Group Normalization \cite{wu2018group} $\rightarrow \mathbb{R}^{h\times w\times C''}$  & Group Normalization \cite{wu2018group} $\rightarrow \mathbb{R}^{H\times W\times C'}$\\
%   Swish Activation Function \cite{ramachandran2017searching} $\rightarrow \mathbb{R}^{h\times w\times C''}$  & Swish  Activation Function \cite{ramachandran2017searching}
%     $\rightarrow \mathbb{R}^{H\times W\times C'}$\\
%   2D Convolution $\rightarrow E(x) \in \mathbb{R}^{h\times w\times n}$  & 2D Convolution $\rightarrow G(\mathbf{D} Z) \in \mathbb{R}^{H\times W\times C}$\\
%   \bottomrule
% \end{tabular}
% \end{table*}



% % \begin{table*}[!htbp]
% % \centering
% % \caption{High-level architecture of the Learnable ISTA of our SC-VAE. Note that $k$ is the number of the unfolded ISTA block.} 
% % \begin{tabular}{c}
% %   \toprule
% %   Learnable ISTA \\
% %   \midrule
% %   $E(x)\in \mathbb{R}^{h\times w \times n} $ \\
% %   Filter Matrix $\rightarrow \mathbb{R}^{h\times w\times K}$ \\
% %   $k\times$\{Shrinkage Function, Mutual Inhibition Matrix, Addition Operator\} $\rightarrow \mathbb{R}^{h\times w\times K}$\\
% %   Shrinkage function$\rightarrow Z\in \mathbb{R}^{h\times w\times K}$\\
% %   \bottomrule
% % \end{tabular}
% % \end{table*}

% % \begin{table*}[!htbp]
% % \centering
% % \caption{The architecture of the attention network for $\alpha$ estimation} 
% % \begin{tabular}{c}
% %   \toprule
% %   Attention network \\
% %   \midrule
% %   $E(x) - \mathbf{D}Z\in \mathbb{R}^{h\times w \times n} $ \\
% %   Equivariant layer $1$ $\rightarrow \mathbb{R}^{h\times w\times L}$ \\
% %   Equivariant layer $2$ $\rightarrow \mathbb{R}^{h\times w\times 1}$\\
% %   \bottomrule
% % \end{tabular}
% % \end{table*}

% \section{Image Patches Clustering}  \label{section2_s}
% %Figures \ref{figure:s1} and \ref{figure:s2} contain additional iamge patches clustering results on the FFHQ and ImageNet dataset, respectively. For each dataset, the results are obtained using the respective pretrained SC-VAE model with downsampling block $d = 4$ and sparsity penalty $\lambda = 2$.
% Figures \ref{figure:s1} and \ref{figure:s2} exhibit more image patches clustering outcomes for the FFHQ and ImageNet datasets, respectively. These outcomes were obtained utilizing the pre-trained SC-VAE model specific to each dataset, with downsampling block $d=4$ and sparsity penalty $\lambda=2$.
% \begin{figure*}[h!]
% \centering
% \includegraphics[width=16cm]{./Figures/patches_cluster_ffhq_supple_50.png}
% \caption{50 randomly selected image patch clusters from the validation set of the FFHQ dataset generated by clustering the learned sparse code vectors of the pre-trained SC-VAE model
% using the K-means algorithm. Each row represents one cluster. Image patches with similar patterns were grouped together.}
% \label{figure:s1}
% \end{figure*}

% \begin{figure*}[h!]
% \centering
% \includegraphics[width=16cm]{./Figures/imagenet_cluster_patches_V3.png}
% \caption{50 randomly selected image patch clusters from the validation set of the ImageNet dataset generated by clustering the learned sparse code vectors of the pre-trained SC-VAE model
% using the K-means algorithm. Each row represents one cluster. Image patches with similar patterns were grouped together.}
% \label{figure:s2}
% \end{figure*}

% % \begin{figure*}[h!]
% % \centering
% % \includegraphics[width=16cm]{./Figures/segmentation_ffhq_supple3.png}
% % \caption{FFHQ.}
% % \label{figure:5}
% % \end{figure*}


% \section{Unsupervised Image Segmentation} \label{section3_s}
% Figures \ref{figure:s3} and \ref{figure:s4} contain additional unsupervised image segmentation results. 
% %We utilized two SCVAE models that were pre-trained on the training set of the FFHQ and ImageNet dataset, respectively. These models had a downsampling block of $d = 3$ and a sparsity penalty of $\lambda = 2$. 
% We employed two SC-VAE models that had been pre-trained on the training sets of the FFHQ and ImageNet datasets, respectively. These models had a downsampling block with a factor of $3$ and a penalty for sparsity with a value of $2$.
% \label{section3}
% \begin{figure*}[h!]
% \centering
% \includegraphics[width=16cm]{./Figures/segmen_ffhq_supple3.png}
% \caption{Unsupervised image segmentation results. Images are from the validation set of the FFHQ dataset.}
% \label{figure:s3}
% \end{figure*}

% \begin{figure*}[h!]
% \centering
% \includegraphics[width=16cm]{./Figures/segmentation_imagenet_supple.png}
% \caption{Unsupervised image segmentation results. Images are from the validation set of the ImageNet dataset.}
% \label{figure:s4}
% \end{figure*}

% \clearpage
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egpaper_arxiv_V2}
% }


\end{document}



