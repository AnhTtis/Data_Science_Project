\section{Related Work}
\label{sec:related work}
In this section, we delve into research closely related to SC-VAE. Earlier works on sparse coding-based VAEs \cite{barello2018sparse, fallah2022variational, tonolini2020variational, sadeghi2022sparsity} primarily built upon the vanilla VAE framework \cite{kingma2013auto}. These approaches achieved sparsity in latent variables by employing either specific prior distributions \cite{barello2018sparse, tonolini2020variational, sadeghi2022sparsity} or a learned threshold \cite{fallah2022variational}. For example, sparse-coding variational auto-encoder (SVAE) \cite{barello2018sparse} replaced the normal prior in the vanilla VAE with robust, sparsity-promoting priors (such as Laplace and Cauchy). However, it opted for a linear generator in the decoding process to reconstruct the input signal. This choice frequently resulted in suboptimal reconstruction outcomes due to the decoder's limited expressive capacity.
Variational sparse coding (VSC) model \cite{tonolini2020variational} focused on making the latent space interpretable by enforcing sparsity using a mixture of Spike and Slab distributions. 
%The Sparse VAE (SVAE), as introduced by Barello et al. (2018), departed from the conventional normal prior over latent variables and instead adopted heavy-tailed, sparsity-promoting priors like Laplace and Cauchy. However, a noteworthy limitation of SVAE emerged from its use of a linear generator in the decoding process to reconstruct the original input signal. This choice often resulted in suboptimal reconstruction outcomes due to the decoder's limited expressive capacity.
Variational sparse coding with learned thresholding (VSCLT) model \cite{fallah2022variational} imposed sparsity through a shifted soft-threshold function. However, the non-differentiability of this function led to numerical instability. This issue arose as the function acted as a barrier to gradient flow from the generator to the inference network when a latent feature was unused.
Sparsity-promoting dictionary model for variational autoencoders (SDM-VAE) \cite{sadeghi2022sparsity} adopted a zero-mean Gaussian prior distribution with learnable variances as a sparsity regularizer. It used a fixed dictionary with unit-norm atoms to avoid scale ambiguity.
As continuous VAEs, all these methods \cite{barello2018sparse, fallah2022variational, tonolini2020variational, sadeghi2022sparsity} faced the inherent challenge of posterior collapse. 
%To counteract this effect, a straightforward Spike variable warm-up strategy was employed in VSC \cite{tonolini2020variational}.
To counteract this effect, VSC \cite{tonolini2020variational} implemented a Spike variable warm-up strategy to prevent posterior collapse.
In contrast, the proposed SC-VAE explicitly learns sparse latent representations by combining the VAE framework with learned ISTA algorithm, avoiding the challenges of these prior works.

% Here, we review the work most related to SC-VAE.
% The previously sparse coding-based VAEs \cite{barello2018sparse, fallah2022variational, tonolini2020variational, sadeghi2022sparsity} mainly
% extended upon the vanilla VAE framework. 
% They induced sparsity by certain forms of priors  on latent variables \cite{barello2018sparse, tonolini2020variational, fallah2022variational, sadeghi2022sparsity}.
% SVAE \cite{barello2018sparse} replaced the standard normal prior over the latent variables with heavy-tailed, sparsity-promoting priors (e.g., Laplace and Cauchy). However, it used a linear generator as the decoder to reconstruct the original input signal, which often leads to suboptimal reconstruction results due to the lack of expressive ability.
% %results from replacing the generative model by a sparse coding model, where the decoder only contains a linear layer.
% VSC \cite{tonolini2020variational} induced interpretation through latent space
% sparsity with a mixture of Spike and Slab distributions as prior. As continuous VAEs, VSC suffered from the inherent problem of posterior collapse.
% VSCLT \cite{fallah2022variational} incorporates sparsity via a shifted soft-threshold function. This function is non-differentiable, which is problematic and often causes numerical instability since it serves as a block of gradient flowing from the generator to the inference network whenever a latent feature is not used.


% Even it allowed us to learn sparse distributions by thresholding samples, it did not allow gradients to pass through the network.

% VSCLT \cite{fallah2022variational} allowed us to learn sparse distributions by thresholding samples. 

% %\cite{sadeghi2022sparsity} A Sparsity-promoting Dictionary Model for Variational Autoencoders.
% %Disentangling Disentanglement in Variational Autoencoders.


% However, existing methods \cite{barello2018sparse, tonolini2020variational, fallah2022variational} either suffer from the inherent problem of posterior collapse as the continuous VAEs or did not allow the gradients to pass through the network with the use of the straight-through estimator.
% Moreover, \cite{barello2018sparse, fallah2022variational} treated the dictionary as a linear generator to reconstruct the original input signal, which often leads to suboptimal reconstruction results.


% The third class of methods \cite{barello2018sparse, fallah2022variational, tonolini2020variational} learns representations with sparse latent variables (hereinafter referred to as sparse coding VAEs). 
% These methods are based on the observation that sparse distributions, where only a small number of features are non-zero in each input data sample, are preferred for promoting statistically efficient representations, particularly when the input data exhibits a low-dimensional structure.
% They induced sparsity by certain forms of priors (mostly a spike and slab distribution or a laplacian distribution) on latent variables \cite{barello2018sparse, tonolini2020variational, fallah2022variational}.
% In the case of continuous VAEs, most of the latent factors are active at each time. In contrast, discrete VAEs require only one quantization index to be active at each time. Sparse coding VAEs adopt a middle ground, as is depicted in Figure \ref{figure:1}. However, existing methods \cite{barello2018sparse, tonolini2020variational, fallah2022variational} either suffer from the inherent problem of posterior collapse as the continuous VAEs or did not allow the gradients to pass through the network with the use of the straight-through estimator.
% Moreover, \cite{barello2018sparse, fallah2022variational} treated the dictionary as a linear generator to reconstruct the original input signal, which often leads to suboptimal reconstruction results.

% \noindent\subsection{Variational Autoencoder}
% Research on improving Variational Autoencoders (VAEs) can be categorized into several groups, which include enhancing the network's architecture \cite{sonderby2016ladder, vahdat2020nvae}, incorporating adversarial objectives \cite{huang2018introvae, makhzani2015adversarial, han2020joint, pidhorskyi2020adversarial, heljakka2020towards, dumoulin2016adversarially, larsen2016autoencoding}, introducing stronger priors  \cite{tomczak2018vae, razavi2019generating, kalatzis2020variational}, and applying regularization techniques \cite{ghosh2019variational, xu2020learning, kumar2020implicit, sinha2021consistency, li2021co, rhodes2021local, zhang2022improving}. Our work falls into the latter one.
% One relevant method in this category is JL1-VAE \cite{rhodes2021local}, which employs an L1 loss incorporated into the VAE's generative Jacobian during the training process. This L1 loss serves the purpose of promoting alignment of local latent variables with independent factors of variation in images of multiple objects or parts.
% %JL1-VAE \cite{rhodes2021local} is the most relevant one to our work, which employs an L1 loss incorporated into the VAE's generative Jacobian during the training process. This L1 loss serves the purpose of promoting alignment of local latent variables with independent factors of variation in images of multiple objects or parts.
% %RAE \cite{ghosh2019variational} substitutes noise injection with an explicit regularization scheme for the decoder, leading to an equally smooth and meaningful latent space without forcing it to conform to an arbitrarily chosen prior.
% %One method in this category is RAE \cite{ghosh2019variational}. 
% Besides, RAE \cite{ghosh2019variational} replaces noise injection in the decoder with an explicit regularization scheme. This modification results in a latent space that is both smooth and meaningful without imposing an arbitrary prior.
% %GRAE \cite{kumar2020implicit} consists of a deterministic autoencoding objective plus analytic regularizers that depend on the Hessian or Jacobian of the decoding mode.
% GRAE \cite{kumar2020implicit} involves a deterministic autoencoding objective and incorporates analytic regularizers that depend on the Hessian or Jacobian of the decoding mode.
% %Co-VAE \cite{li2021co} consists of two VAEs for generating drug SMILES strings and target sequences, respectively, and a co-regularization block for generating the binding affinities.
% Co-VAE \cite{li2021co} comprises a pair of VAEs dedicated to generating drug SMILES strings and target sequences. Additionally, it incorporates a co-regularization block designed to generate binding affinities.
% %JL1-VAE \cite{rhodes2021local} applys an L1 loss to the VAE’s generative Jacobian during training to encourage local latent variable alignment with independent factors of variation in images of multiple objects or images with multiple parts.
% %CR-VAE \cite{sinha2021consistency} minimizes the Kullback-Leibler (KL) divergence between the variational distribution when conditioning on the observation and the variational distribution when conditioning on a random semantic-preserving transformation of this observation.
% A prominent line of work in this category aims to reduce two distributions' distances through different regularization techniques. For examples,
% CR-VAE \cite{sinha2021consistency} seeks to enforce consistency in VAEs by reducing the Kullback-Leibler (KL) divergence between the variational distribution under the condition of the original observation and the variational distribution under the condition of a randomly applied semantic-preserving transformation of that observation.
% %RAE \cite{xu2020learning} minimizes the discrepancy between the model and target distributions, with a relational regularization on the learnable latent prior. This regularization penalizes the fused Gromov-Wasserstein (FGW) distance between the latent prior and its corresponding posterior, allowing one to flexibly learn a structured prior distribution associated with the generative model.
% RAE \cite{sinha2021consistency} aims to narrow the gap between the model and target distributions by incorporating a relational regularization on the learnable latent prior.
% %DGVAE \cite{zhang2022improving} introduces new training objectives to tackle the posterior collapse and hole problems through a novel regularization based on the probabilistic density gap between the aggregated posterior distribution and the prior distribution.
% DGVAE \cite{zhang2022improving} introduces an innovative regularization approach that relies on the probabilistic density gap between the aggregated posterior distribution and the prior distribution. 
% %The training objectives in DGVAE \cite{zhang2022improving} can address issues like posterior collapse and hole problems.
% %https://arxiv.org/pdf/2012.13253.pdf\\
% %https://openreview.net/pdf?id=wS0UFjsNYjn\\
% %https://openaccess.thecvf.com/content_CVPR_2020/papers/Ding_Guided_Variational_Autoencoder_for_Disentanglement_Learning_CVPR_2020_paper.pdf\\
% \noindent\subsection{Sparse Coding}
% Sparse Coding is a representation learning technique designed to discover a sparse representation of input data. This representation takes the form of a linear combination of a few unknown basis vectors \cite{lee2006efficient, bengio2013representation}.
% %Sparse coding seeks to approximately express observed signals by utilizing a weighted linear combination of a limited set of unknown basis vectors \cite{lee2006efficient, bengio2013representation}. 
% The task of identifying the most suitable basis is typically framed as an optimization problem, where the coefficients of a linear transformation are adjusted to jointly minimize both a reconstruction cost and a sparsity cost associated with the embedded representations.
% Sparse coding is grounded in the crucial realization that, despite the need for numerous variables to describe large collections of natural signals, individual instances can be effectively represented using only a small subset of these variables. 
% %This insight is substantiated by substantial empirical evidence. 
% Pioneering work by \cite{olshausen1996emergence} demonstrated that the visual cortex in mammals processes information as sparse signals, highlighting the alignment of biological learning with the principles of sparse coding-based models. 
% %For a more comprehensive overview of linear sparse coding, \cite{olshausen2004sparse} offers an in-depth review.
% In the field of computer vision, sparse coding is a widely adopted technique for efficiently reconstructing features or images \cite{bao2019convolutional, ravishankar2019image}, as well as for measuring similarity \cite{luo2019video}. Additionally, sparse coding has found successful applications in various computer vision tasks, including image classification \cite{sun2019supervised}, face recognition \cite{mokhayeri2020paired}, image denoising \cite{scetbon2021deep}, and more.
% \noindent\subsection{Algorithm Unrolling for Sparse Coding}
% % LISTA \cite{gregor2010learning};\\
% % LISTA-CP; LISTA-SS; LISTA-CPSS \cite{chen2018theoretical}\\
% % Analytic LISTA (ALISTA) \cite{liu2019alista};\\
% % HyperLISTA \cite{chen2021hyperparameter}\\
% % https://sparse-learning.github.io/pdf/WotaoYin.pdf\\
% % Ada-LISTA [Aberdam et al., 2021]\\
% %LISTA’s concept of unfolding the iterations of a classical optimization scheme into an RNN-like neural network and freeing its parameters to be learned appears in many works. These include an unsupervised, online training procedure [9], a multi-layer version [23], a gated mechanism, compensating shrinkage artifacts [21], as well as reduced-parameter schemes [20], [24]. This paradigm has been brought to various applications, including compressed sensing, super-resolution, communication and MRI reconstruction [25], [26], [27], [28], [29], [30].
% %Algorithm unrolling provides a concrete and systematic connection between iterative algorithms that are widely used in signal processing and deep neural networks.
% The concept of algorithm unrolling establishes a concrete and systematic link between iterative algorithms frequently applied in signal processing and deep neural networks.
% Learnable ISTA (LISTA) \cite{gregor2010learning} was the first unrolling method to develop fast neural network approximations for sparse coding. 
% %Inspired by LISTA \cite{gregor2010learning}, many works have been focusing on unfolding the iterations of a classical optimization scheme into an RNN-like neural network and freeing its parameters to be learned appears in many works.
% Taking inspiration from LISTA \cite{gregor2010learning}, numerous studies have concentrated on expanding the iterations of a classical  optimization method into a neural network with RNN-like characteristics.
% %These include an unsupervised, online training procedure [9], a multi-layer version [23], a gated mechanism, compensating shrinkage artifacts [21], as well as reduced-parameter schemes [20], [24]. 
% These methods encompass unsupervised online training \cite{sprechmann2015learning}, multi-layer extensions \cite{sulam2019multi}, the introduction of gated mechanisms to address shrinkage artifacts \cite{wu2019sparse}, as well as reduced-parameter schemes \cite{chen2018theoretical, liu2019alista}.
% %A prominent line of work investigates the success of learned solvers from a theoretical point of view [19], [31], [32], [33].
% %Moreover, \cite{zarka2019deep, wu2019sparse, chen2018theoretical, liu2019alista} have  shown learned solvers can achieve linear convergence, under specific conditions on sparsity and mutual coherence. Recently, Ada-LISTA generalizes these guarantees to a varying model scenario, proving the same weight matrix can serve different models while still reaching linear convergence.\\
% Additionally, it has been demonstrated in studies \cite{zarka2019deep, wu2019sparse, chen2018theoretical, liu2019alista} that learned solvers can achieve linear convergence, subject to specific conditions regarding sparsity and mutual coherence. These guarantees have been extended by Ada-LISTA \cite{aberdam2021ada} to accommodate varying model scenarios while maintaining linear convergence. Furthermore, HyperLISTA \cite{chen2021hyperparameter} achieves superlinear convergence by adding momentum to intermediate variables in the LISTA network. In our work, we used the original version of LISTA network.
% %Furthermore, recent advancements such as Ada-LISTA \cite{aberdam2021ada} have extended these guarantees to accommodate varying model scenarios, showcasing the ability of the same weight matrix to support different models while maintaining linear convergence.
% %This paradigm has been brought to various applications, including compressed sensing, super-resolution, communication and MRI reconstruction [25], [26], [27], [28], [29], [30].

% %Learning representations with continuous features have been the focus of many previous work.
% The variational inference provides a theoretically concrete and elegant formulation of VAEs.   
% The most standard VAEs~\cite{kingma2013auto,higgins2017beta,zhao2019infovae} learn a continuous latent representation by transforming a known continuous prior distribution to the real data distribution. Even though it is advantageous for the VAEs to capture modes of the data, the approximate posterior distribution is prone to be different from the real one resulting in crude representations of the seen data.
% In addition, the derived representations
%  are greatly affected by the problem of posterior collapse.
% %Many efforts have focuses on addressing this issue. 
% Razavi et al.~\cite{razavipreventing} claimed that the reason behind posterior collapse could be attributed to the inconsistency between the prior distribution and the approximated posterior distribution. To address this issue, $\delta$-VAE \cite{razavipreventing} was proposed, which combined a mean field posterior with a correlated prior.
% %the posterior collapse happened when the approximate posterior equals the prior distribution by optimizing the KL divergence, and proposed $\delta$-VAE by combining a mean field posterior with a correlated prior in time to construct sequential latent variables. 
% Other efforts \cite{tolstikhin2018wasserstein, An2020AE-OT} have focused on leveraging
% %WAE~\cite{tolstikhin2018wasserstein} and AE-OT~\cite{An2020AE-OT} leverage 
% the optimal transport theory to provide a better transformation from the prior distribution to the real data distribution. However, the fundamental problem of posterior collapse in VAEs still remains due to the challenge of modeling multi-modal distributions in real data through a preselected prior distribution.\\
% \noindent\textbf{Discrete VAEs.}
% %however we concentrate on discrete representations which are potentially a more natural fit for many of the modalities we are interested in. Language is inherently discrete, similarly speech is typically represented as a sequence of symbols. Images can often be described concisely by language. Furthermore, discrete representations are a natural fit for complex reasoning, planning and predictive learning (e.g., if it rains, I will use an umbrella). While using discrete latent variables in deep learning has proven challenging, powerful autoregressive models have been developed for modelling distributions over discrete variables.
% VQVAE~\cite{van2017neural} adopts an alternative approach to circumvent posterior collapse by learning a discrete latent representation. Instead of choosing an encoding distribution, VQVAE learns the prior distribution in the form of a codebook based on vector quantization. Since its introduction, many variants \cite{esser2021taming, yu2021vector, zheng2022movq, lee2022autoregressive} have been proposed with the goal of improving image reconstruction fidelity. For examples, VQGAN~\cite{esser2021taming} aimed to learn a perceptually rich and efficient  codebook by taking advantage of adversrarial training and perceptual loss \cite{johnson2016perceptual, larsen2016autoencoding}.
% %takes advantage of adversarial training and perceptual loss \cite{johnson2016perceptual, larsen2016autoencoding} to learn a perceptually rich and efficient codebook.
% VIT-VQGAN \cite{yu2021vector} achieved computational efficiency and good reconstructions by combining the strengths of both the Vision Transformer (ViT) \cite{dosovitskiy2020image} and VQGAN \cite{esser2021taming}.
% %replaces the CNN encoder and decoder with Vision Transformer (ViT) \cite{dosovitskiyimage} to  yield better computational efficiency on accelerators, and produce higher quality reconstructions. 
% RQVAE~\cite{lee2022autoregressive} utilized residual quantization to conserve the encoded information of an image while maintaining a relatively compact codebook. 
% Mo-VQGAN \cite{zheng2022movq} introduced a conditional normalization layer, which essentially adds spatially variant information to the discrete representation.
% %Mo-VQGAN \cite{zheng2022movq}  introduce a spatially conditional normalization to provide spatially variant information for different locations to avoid reconstructing repeated artifacts in similar nearby regions.
% Despite the fact that VQ-based VAEs better accommodate posterior collapse than continuous VAEs, they suffer from the issue of codebook collapse~\cite{dhariwal2020jukebox}. Additionally, the quantization operator does not allow the gradient to pass through the codebook, which is likely to make the optimization process more challenging and slower. \\
% %However, the VQGAN cannot precisely approximate the feature map of an image without a huge codebook. To mitigate this issue, RQVAE~\cite{lee2022autoregressive} takes advantage of residual quantization to conserve the encoded information of an image while maintaining a relatively compact codebook.
% %Though the VQ-based VAEs better accommodate posterior collapse in continuous VAEs, they suffer from the issue of codebook collapse~\cite{dhariwal2020jukebox}. Additionally, the Quantization operator does not allow the gradient to pass through the codebook which is likely to result in a suboptimal solution. 
%  \noindent\textbf{Sparse Coding VAEs.}
%  The previously sparse coding VAEs \cite{barello2018sparse, tonolini2020variational, fallah2022variational} mainly
% extended upon the vanilla VAE framework. SVAE \cite{barello2018sparse} results from replacing the generative model by a sparse coding model, where the decoder only contains a linear layer.
% VSC \cite{tonolini2020variational} induced interpretation  through latent space
% sparsity with a mixture of Spike and Slab distributions as prior.
% VSCLT \cite{fallah2022variational} incorporates sparsity via a shifted soft-threshold function, which did not allow gradients to pass through the network. In contrast, the proposed SC-VAE  explicitly learned sparse latent representations by unrolling the ISTA algorithm.\\
% \noindent\textbf{Sparse Coding and Algorithm Unrolling.}
% Sparse Coding is widely used in image restoration tasks~\cite{elad2006image,yang2010image,gu2015convolutional} to learn effective image representations. Sparse coding is typically formulated as an optimization problem that can be solved by K-SVD \cite{aharon2006k},  ISTA \cite{daubechies2004iterative}, FISTA \cite{beck2009fast}, 
%  and Coordinate Descent (CoD) \cite{wright2015coordinate}, etc. However, those iterative optimization algorithms cannot be directly adapted to the neural networks for end-to-end training. To address this issue, Gregor et al.~\cite{gregor2010learning} proposed a learnable version of ISTA (Learnable ISTA) that can be trained by back-propagation. Importantly, algorithm unrolling~\cite{monga2021algorithm,fu2019jpeg,simon2019rethinking,scetbon2021deep} provides a way to learn the iterative algorithms in an end-to-end fashion. In this work, we unrolled the ISTA algorithm to learn sparse coding for the discrete latent variables of a VAE, which will be discussed in detail in Section~\ref{sec:preliminaries}.