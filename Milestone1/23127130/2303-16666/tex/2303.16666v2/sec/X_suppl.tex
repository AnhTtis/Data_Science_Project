\clearpage
%\setcounter{page}{1}
\maketitlesupplementary
The supplementary material for our work  \textit{SC-VAE: Sparse Coding-based Variational Autoencoder with Learned ISTA} is structured as follows:
%Sec. \ref{section1_s} provides the detailed information of the encoder and decode architecture of the SC-VAE model. 
%Sec. \ref{section2_s} shows the visualization of the dictionary atoms.
%Sec. \ref{section3_s} shows the training loss on the ImageNet dataset with different number of downsampling (upsampling) blocks ($d$) in the encoder (decoder) of the SC-VAE model.
%Sec. \ref{section4_s} shows the visualization results of an unofficial implementation of VIT-VQGAN \cite{yu2021vector}. 
%Sec. \ref{section5_s} shows additional manipulation and interpolation results on FFHQ dataset. 
%Sec. \ref{section6_s} shows additional image patches clustering results on FFHQ and ImageNet datasets. 
%Sec. \ref{section7_s} shows additional unsupervised image segmentation results.
Section \ref{section1_s} details the encoder and decoder architecture of the SC-VAE model. In Section \ref{section2_s}, the dictionary atoms are visualized. In Section \ref{section3_s}, we provide the training losses on the ImageNet dataset when varying the number of downsampling (upsampling) blocks ($d$) in the encoder (decoder) of the SC-VAE model. In Section \ref{section4_s}, the visualized reconstruction results of an unofficial implementation of VIT-VQGAN \cite{yu2021vector} are provided. 
We provide  additional manipulation and interpolation results on the FFHQ dataset in Section \ref{section5_s}, while  additional clustering results of image patches on both FFHQ and ImageNet  are provided in Section \ref{section6_s}. Supplementary unsupervised image segmentation results are given in Section \ref{section7_s}.

%Additional results on image patches clustering and unsupervised image segmentation on FFHQ and ImageNet datasets are then presented in Sec. 2 and Sec. 3, respectively.
\setcounter{section}{0}

\section{The Encoder and Decoder Architecture of SC-VAE} \label{section1_s}
The SC-VAE model's encoder and decoder architecture mirrors that of VQGAN \cite{esser2021taming}. Details about the architecture are provided in Table \ref{figure:encoder_decoder}.
%The encoder and decoder architecture in the SC-VAE model are the same as the architecture used in VQGAN \cite{esser2021taming}, which is described in Table \ref{figure:encoder_decoder}. 
$H$, $W$ and $C$ denote the height, width
and the number of channels of an input image, respectively.
$C'$ and $C''$ represent the number of channels of the feature maps that are produced as outputs by the intermediate layers of the encoder and decode network.
In our experiment, $C'$ and $C''$ were set to $128$ and $512$, respectively. $n$ denotes the number of dimensions of each latent representation, which was set to $256$.
The variable $d$ represents the number of blocks used for downsampling and upsampling. Therefore, we can calculate the height ($h$) and width ($w$) of the encoder's output feature maps by dividing the height ($H$) and width ($W$) of input images by $2$ raised to the power of $d$.

\begin{table}[thbp!]
\centering
\caption{High-level architecture of the encoder and decoder of the SC-VAE model. $H$, $W$, and $C$ refer to the height, width, and the number of channels of an input image. 
$C'$ and $C''$ represent the number of channels of the feature maps from intermediate layers in the encoder and decoder networks. $n$ denotes the number of dimensions of each latent representation, while $d$ represents the number of downsampling (upsampling) blocks. Note that $h=\frac{H}{2^{d}}$, $w=\frac{W}{2^d}$.} 
\resizebox{1\linewidth}{!}{%
\begin{tabular}{c|c}
  \toprule
   &  $x\in \mathbb{R}^{H\times W\times C} $\\
   &  2D Convolution $\rightarrow \mathbb{R}^{H\times W\times C'}$\\
   &  $d \times$\{Residual Block, Downsample Block\} $\rightarrow \mathbb{R}^{h\times w\times C''}$\\
   &  Residual Block $\rightarrow \mathbb{R}^{h\times w\times C''}$\\
  Encoder &  Non-Local Block $\rightarrow \mathbb{R}^{h\times w\times C''}$\\
   &  Residual Block $\rightarrow \mathbb{R}^{h\times w\times C''}$\\
   &  Group Normalization \cite{wu2018group} $\rightarrow \mathbb{R}^{h\times w\times C''}$ \\
   &  Swish Activation Function \cite{ramachandran2017searching} $\rightarrow \mathbb{R}^{h\times w\times C''}$\\
   &  2D Convolution $\rightarrow E(x) \in \mathbb{R}^{h\times w\times n}$\\
  \midrule
   & $\tilde{E}(x)\in \mathbb{R}^{h\times w\times n} $  \\
   &  2D Convolution $\rightarrow \mathbb{R}^{h\times w\times C''}$  \\
   &  Residual Block $\rightarrow \mathbb{R}^{h\times w\times C''}$\\
    & Non-Local Block $\rightarrow \mathbb{R}^{h\times w\times C''}$\\
  Decoder & Residual Block $\rightarrow \mathbb{R}^{h\times w\times C''}$\\
    & $d\times$\{Residual Block, Upsample Block\} $\rightarrow \mathbb{R}^{H\times W\times C'}$\\
   & Group Normalization \cite{wu2018group} $\rightarrow \mathbb{R}^{H\times W\times C'}$\\
    & Swish  Activation Function \cite{ramachandran2017searching}
    $\rightarrow \mathbb{R}^{H\times W\times C'}$\\
    & 2D Convolution $\rightarrow G(\tilde{E}(x)) \in \mathbb{R}^{H\times W\times C}$\\
  \bottomrule
\end{tabular}}
\label{figure:encoder_decoder}
\end{table}

\section{Visualization of Dictionary Atoms}
\label{section2_s}
Figure \ref{figure:dictionary_visualization} demonstrates the $512$ columns (atoms) of the pre-determined Discrete Cosine Transform (DCT) dictionary. Each atom is of dimension $256$, which corresponds to the size of $16 \times 16$ images when shaped.
%We reshape all atoms into an image with a $16\times 16$ resolution.

\begin{figure}[tbp]
\centering
\includegraphics[width=8cm]{./Figures/visualization_of_dictionary.png}
\caption{$512$ atoms of the Discrete Cosine Transform (DCT) dictionary. All atoms were reshaped into a $16 \times 16$ image.}
\label{figure:dictionary_visualization}
\end{figure}

\section{Training Losses}  \label{section3_s}
%Training losses of inherent noises around the 140th epoch under different auxiliary dataset sizes (K)
Figures \ref{figure:TLImagenet32x32}, \ref{figure:TLImagenet16x16}, \ref{figure:TLImagenet4x4} and \ref{figure:TLImagenet1x1} show  the training losses over $120,000$ training steps on the
ImageNet dataset.
The number of downsampling (upsampling) blocks ($d$) in the encoder (decoder) of the SC-VAE model are $3, 4, 6$ and $8$, respectively.
%with the number of downsampling (upsampling) blocks ($d=3,4,6$ and $8$, respectively) in the encoder (decoder) of the SC-VAE model. 
%As is shown in these figures, the LISTA networks of the SC-VAE models converge to a fixed point no matter which downsampling (upsampling) block $d$ is used. However, SC-VAE suffer from image reconstruction when increasing $d$.
As depicted in these figures, the LISTA networks within the SC-VAE models consistently converge to a stable point regardless of the chosen downsampling (upsampling) block $d$. However, increasing $d$ leads to worse image reconstructions ($\mathcal{L}_{rec}$) in SC-VAE.

\begin{figure}[tbp]
\centering
\includegraphics[width=7.5cm]{./Figures/Imagenet32x32.png}
\caption{The training losses over $120,000$ training steps on the ImageNet dataset. The number of  downsampling (upsampling) blocks ($d$) in the encoder (decoder) of the SC-VAE model was set to $3$ and the height ($h$) and width ($w$) of latent representations were $32$. (a) Total loss $\mathcal{L}_{SC-VAE}$. (b) Image reconstruction loss $\mathcal{L}_{rec}$. (c)The mean of latent representations reconstruction loss $\frac{1}{hw}\mathcal{L}_{latent}$.}
\label{figure:TLImagenet32x32}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[width=7.5cm]{./Figures/Imagenet16x16.png}
\caption{The training losses over $120,000$ training steps on the ImageNet dataset. The number of  downsampling (upsampling) blocks ($d$) in the encoder (decoder) of the SC-VAE model was set to $4$ and the height ($h$) and width ($w$) of latent representations were $16$. (a) Total loss $\mathcal{L}_{SC-VAE}$. (b) Image reconstruction loss $\mathcal{L}_{rec}$. (c) The mean of latent representations reconstruction loss $\frac{1}{hw}\mathcal{L}_{latent}$.}
\label{figure:TLImagenet16x16}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[width=7.5cm]{./Figures/Imagenet4x4.png}
\caption{The training losses over $120,000$ training steps on the ImageNet dataset. The number of  downsampling (upsampling) blocks ($d$) in the encoder (decoder) of the SC-VAE model was set to $6$ and the height ($h$) and width ($w$) of latent representations were $4$. (a) Total loss $\mathcal{L}_{SC-VAE}$. (b) Image reconstruction loss $\mathcal{L}_{rec}$. (c) The mean of latent representations reconstruction loss $\frac{1}{hw}\mathcal{L}_{latent}$.}
\label{figure:TLImagenet4x4}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[width=7.5cm]{./Figures/Imagenet1x1.png}
\caption{The training losses over $120,000$ training steps on the ImageNet dataset. The number of  downsampling (upsampling) blocks ($d$) in the encoder (decoder) of the SC-VAE model was set to $8$ and the height ($h$) and width ($w$) of latent representations were $1$. (a) Total loss $\mathcal{L}_{SC-VAE}$. (b) Image reconstruction loss $\mathcal{L}_{rec}$. (c) The mean of latent representations reconstruction loss $\frac{1}{hw}\mathcal{L}_{latent}$.}
\label{figure:TLImagenet1x1}
\end{figure}

%\noindent
%\noindent\textbf{Learnbale ISTA.} The architecture of our Learnable ISTA network is shown in Table 2.
%\noindent\textbf{Attention Network for $\alpha$ Estimation.}  Our neural network architecture follows the backbone of PixelCNN++ [52], which is a U-Net [48] based on a Wide ResNet [72]. We replaced weight normalization [49] with group normalization [66] to make the implementation simpler. Our 32 × 32 models use four feature map resolutions (32 × 32 to 4 × 4), and our 256 × 256 models use six. All models have two convolutional residual blocks per resolution level and self-attention blocks at the 16 × 16 resolution between the convolutional blocks [6].






% \begin{table*}[!htbp]
% \centering
% \caption{High-level architecture of the Learnable ISTA of our SC-VAE. Note that $k$ is the number of the unfolded ISTA block.} 
% \begin{tabular}{c}
%   \toprule
%   Learnable ISTA \\
%   \midrule
%   $E(x)\in \mathbb{R}^{h\times w \times n} $ \\
%   Filter Matrix $\rightarrow \mathbb{R}^{h\times w\times K}$ \\
%   $k\times$\{Shrinkage Function, Mutual Inhibition Matrix, Addition Operator\} $\rightarrow \mathbb{R}^{h\times w\times K}$\\
%   Shrinkage function$\rightarrow Z\in \mathbb{R}^{h\times w\times K}$\\
%   \bottomrule
% \end{tabular}
% \end{table*}

\section{Image Reconstruction}  \label{section4_s}
Reconstruction results from unofficial implementation\footnote{https://github.com/thuanz123/enhancing-transformers} of VIT-VQGAN \cite{yu2021vector} are presented in Figure \ref{figure:ViT-VQGAN_Visualization}.
%Figures \ref{figure:ViT-VQGAN_Visualization} shows visualizations from unofficial implementation\footnote{https://github.com/thuanz123/enhancing-transformers} of VIT-VQGAN \cite{yu2021vector}. 
VIT-VQGAN \cite{yu2021vector} achieved visually appealing results. However, similar to VQ-GAN \cite{esser2021taming} and RQ-VAE \cite{lee2022autoregressive}, it faced challenges in accurately reconstructing intricate details and complex patterns.
%as VQ-GAN\cite{esser2021taming} and RQ-VAE\cite{lee2022autoregressive}. 
Additionally, its generalization performance was inferior to that of our model.

\begin{figure}[tbp]
\centering
\includegraphics[width=7.0cm]{./Figures/ViT-VQGAN_Visualization.png}
\caption{Image reconstructions from an unofficial implementation of VIT-VQGAN \cite{yu2021vector} and the SC-VAE models trained
on ImageNet dataset. Original images in the top two rows are
from the validation set of ImageNet dataset. Two external images are shown in the last two rows to demonstrate the generalizability of different methods. The numbers denote the shape of
latent codes and the learned codebook (dictionary) size, respectively.
SC-VAE achieved improved image reconstruction compared to VIT-VQGAN \cite{yu2021vector}. Zoom in to see the details in the red square area.}
\label{figure:ViT-VQGAN_Visualization}
\end{figure}

\section{Image Generation}  \label{section5_s}
Additional interpolation and manipulation results can be found in Figures \ref{figure:image_interpolation_supple} and \ref{figure:image_manipulation_supple}, respectively.

\begin{figure}[tbp]
\centering
\includegraphics[width=7.0cm]{./Figures/image_interpolation_supple.png}
\caption{Interpolation between the sparse code vectors of two samples from the SC-VAE$^{\dag}$ model trained on FFHQ.}
\label{figure:image_interpolation_supple}
\end{figure}

\begin{figure*}[tbp]
\centering
\includegraphics[width=14.5cm]{./Figures/image_manipulation_supple4.png}
\caption{Manipulating sparse code vectors on FFHQ. 
Each block contains five seed images used to infer the latent sparse code vector in the SC-VAE$^{\dag}$ model.
The disentangled attributes associated with the $i$-th component of a sparse code vector $z$ and a traversal range are shown on the top of each block.}
\label{figure:image_manipulation_supple}
\end{figure*}



% \begin{figure}[tbp]
% \centering
% \includegraphics[width=8cm]{./Figures/IG_Age.png}
% \caption{IG-Age.}
% \label{figure:IG_Age}
% \end{figure}

% \begin{figure}[tbp]
% \centering
% \includegraphics[width=8cm]{./Figures/IG_sunglasses.png}
% \caption{IG-sunglasses.}
% \label{figure:IG_sunglasses}
% \end{figure}

% \begin{figure}[tbp]
% \centering
% \includegraphics[width=8cm]{./Figures/IG_Azimuth.png}
% \caption{IG-azimuth.}
% \label{figure:IG_azimuth}
% \end{figure}

% \begin{figure}[tbp]
% \centering
% \includegraphics[width=8cm]{./Figures/IG_Fringe.png}
% \caption{IG-Fringe.}
% \label{figure:IG_Fringe}
% \end{figure}


% \begin{figure}[tbp]
% \centering
% \includegraphics[width=8cm]{./Figures/IG_skin color.png}
% \caption{IG-skin color.}
% \label{figure:IG_skin color}
% \end{figure}


% \begin{figure}[tbp]
% \centering
% \includegraphics[width=8cm]{./Figures/image_interpolation_supple.png}
% \caption{Interpolation in the latent space between two samples from a model trained on FFHQ.}
% \label{figure:interpolation}
% \end{figure}

\section{Image Patches Clustering}  \label{section6_s}
%Figures \ref{figure:s1} and \ref{figure:s2} exhibit more image patches clustering outcomes for the FFHQ and ImageNet datasets, respectively. 
Figures \ref{figure:s1} and \ref{figure:s2} showcase additional qualitative results of image patches clustering on FFHQ and ImageNet datasets, respectively.
These results were obtained utilizing the pre-trained SC-VAE$^\curlyvee$ model specific to each dataset with a downsampling block $d=4$.
\begin{figure*}[h!]
\centering
\includegraphics[width=16cm]{./Figures/patches_cluster_ffhq_supple_50.png}
\caption{50 randomly selected image patch clusters from the validation set of the FFHQ dataset generated by clustering the learned sparse code vectors of the pre-trained SC-VAE$^\curlyvee$ model
using the K-means algorithm. Each row represents one cluster. Image patches with similar patterns were grouped together.}
\label{figure:s1}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=16cm]{./Figures/imagenet_cluster_patches_V3.png}
\caption{50 randomly selected image patch clusters from the validation set of the ImageNet dataset generated by clustering the learned sparse code vectors of the pre-trained SC-VAE$^\curlyvee$ model
using the K-means algorithm. Each row represents one cluster. Image patches with similar patterns were grouped together.}
\label{figure:s2}
\end{figure*}

% \begin{figure*}[h!]
% \centering
% \includegraphics[width=16cm]{./Figures/segmentation_ffhq_supple3.png}
% \caption{FFHQ.}
% \label{figure:5}
% \end{figure*}

\section{Unsupervised Image Segmentation} \label{section7_s}
\subsection{Qualitative Analysis on FFHQ and ImageNet}
%Figures \ref{figure:s3} and \ref{figure:s4} contain additional qualitative unsupervised image segmentation results on FFHQ and ImageNet datasets, respectively. 
%We utilized two SCVAE models that were pre-trained on the training set of the FFHQ and ImageNet dataset, respectively. These models had a downsampling block of $d = 3$ and a sparsity penalty of $\lambda = 2$. 
%We employed two SC-VAE$^\curlywedge$ models that had been pre-trained on the training sets of the FFHQ and ImageNet datasets, respectively. These models had a downsampling block $d=3$.
Additional qualitative unsupervised image segmentation results on the FFHQ and ImageNet datasets can be found in Figures \ref{figure:s3} and \ref{figure:s4}, respectively. We utilized two SC-VAE$^\curlywedge$ models pre-trained on the training sets of FFHQ and ImageNet, each employing a downsampling block $d=3$.
\subsection{Quantitative comparisons to prior work}
%Figure \ref{figure:Flower_CUB} shows more qualitative results on  Flowers \cite{nilsback2008automated} and Caltech-UCSD Birds-200-2011 (CUB) \cite{WahCUB_200_2011}. Flowers \cite{nilsback2008automated} consists of $8,189$ images of $102$ classes of flowers, with segmentation masks obtained by an automated algorithm developed specifically for segmenting flowers in color photographs \cite{nilsback2007delving}. CUB \cite{WahCUB_200_2011} consists of $11,788$ images of $200$ classes of birds and segmentation masks. Flowers and CUB contain $1,020$ and $1,000$ test images, respectively.
%Figure \ref{figure:Flower_CUB} shows more qualitative results on  Flowers \cite{nilsback2008automated} and Caltech-UCSD Birds-200-2011 (CUB) \cite{WahCUB_200_2011} datasets.
Figure \ref{figure:Flower_CUB} displays additional qualitative results from the Flowers \cite{nilsback2008automated} and Caltech-UCSD Birds-200-2011 (CUB) \cite{WahCUB_200_2011} datasets.\\
\subsubsection{Evaluation Metrics}
\textbf{Intersection of Union (IoU).} %The IoU score measures the overlap of two regions A and B by calculating the ratio of intersection over union, according to
The IoU score quantifies the overlap between two regions. This is achieved by evaluating the ratio of their intersection to their union.
\begin{align}
    \textup{IoU}(A, B) = \frac{|A\cap B|}{|A\cup B|}. \nonumber
\end{align}
%where we use the inferred mask and ground-truth mask as $A$ and $B$ respectively for evaluation.\\
$A$ denotes the ground-truth mask, while $B$ denotes the inferred mask.\\
%as $B$ for assessment purposes.\\
\textbf{DICE score.} Similarly, the DICE score is defined as:
\begin{align}
    \textup{Dice}(A, B) = \frac{2|A\cap B|}{|A|+ |B|}.\nonumber
\end{align}
\noindent
Higher is better for both scores.\\
\subsubsection{Dataset Details}
\textbf{Flowers.} The Flowers \cite{nilsback2008automated} dataset consists of $8,189$ images across $102$ different flower classes. Additionally, it includes segmentation masks generated by an automated algorithm designed explicitly for color photograph flower segmentation \cite{nilsback2007delving}. 
%The images in this dataset have large scale, pose and light variations.\\
The dataset contains images that exhibit substantial variations in scale, pose, and lighting.
Flowers \cite{nilsback2008automated} contains $1,020$ test images.\\
\textbf{CUB.} The CUB \cite{WahCUB_200_2011} dataset contains $11,788$ images covering $200$ bird classes, along with their segmentation masks. 
%Each image is further annotated with $15$ part locations and $1$ bounding box. We use theprovided bounding box to extract a center square from the image, and scale it to $128\times 128$ pixels.
Every image comes with annotations for $15$ part locations, $312$ binary attributes, and $1$ bounding box. We utilized the given bounding box to crop a central square from the image. The CUB dataset includes $1,000$ test images.\\
\textbf{ISIC-2016.} The ISIC-2016 \cite{gutman2016skin} dataset is a public challenge dataset dedicated to Skin Lesion Analysis for Melanoma Detection. Derived from the extensive International Skin Imaging Collaboration (ISIC) archive, it represents a significant collection of meticulously curated dermoscopic images of skin lesions. Within this challenge, a subset of $900$ images is designated as training data, while $379$ images serve as testing data, aiming to provide representative samples for analysis.
%The ISIC-2016 \cite{gutman2016skin} dataset is a public challenge dataset of Skin Lesion Analysis Towards Melanoma Detection released with ISBI 2016. This dataset is based on the International Skin Imaging Collaboration (ISIC) Archive, which is the largest publicly available collection of quality controlled dermoscopic images of skin lesions. The challenge employs a subset of representative images with $900$ images as training data and $379$ images as testing data.

%For all experiments, we resized the input images into a resolution of $256\times 256$ and  generated a $32\times 32$ binary mask for each image utilizing the pre-trained SC-VAE$^\curlywedge$ on ImageNet dataset, a spectral clustering algorithm and boundary connectivity information. The inferred binary mask and ground truth mask were resized to $128\times 128$ to calculate the IoU and DICE scores.
For our experiments, we resized the input images into a resolution of $256\times 256$.
Subsequently, we generated a binary mask of size $32\times 32$ per image by employing the pre-trained SC-VAE$^\curlywedge$ on the ImageNet dataset, along with a spectral clustering algorithm and boundary connectivity information \cite{zhu2014saliency}. To compute the IoU and DICE scores, both the inferred binary mask and the ground truth mask were resized to $128\times 128$.
%\subsubsection{Baseline Methods}
\label{section3}
\begin{figure*}[h!]
\centering
\includegraphics[width=16cm]{./Figures/segmen_ffhq_supple3.png}
%\caption{Additional unsupervised image segmentation results. Images are from the validation set of the FFHQ dataset.}
\caption{Additional unsupervised image segmentation results. These results were generated by grouping sparse code vectors into $5$ categories per image, utilizing the pre-trained SC-VAE$^{\curlywedge}$ model and the K-means algorithm. Images are from the validation set of the FFHQ dataset.}
\label{figure:s3}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=16cm]{./Figures/segmentation_imagenet_supple.png}
%\caption{Additional unsupervised image segmentation results by applying K-means algorithm to cluster sparse code vectors per image into $5$ categories using the SC-VAE$^{\curlywedge}$ model. Images are from the validation set of the ImageNet dataset.}
\caption{Additional unsupervised image segmentation results. These results were generated by grouping sparse code vectors into $5$ categories per image, utilizing the pre-trained SC-VAE$^{\curlywedge}$ model and the K-means algorithm. Images are from the validation set of the ImageNet dataset.}
\label{figure:s4}
\end{figure*}

\begin{figure*}[tbp]
\centering
\includegraphics[width=18cm]{./Figures/flower_cub_isic2016_supple2.png}
\caption{Additional unsupervised image segmentation results on Flowers \cite{nilsback2008automated} (\textit{Left Panel}), CUB \cite{WahCUB_200_2011} (\textit{Middle Panel}) and ISIC-2016 \cite{gutman2016skin} (\textit{Right Panel}). (a) input image. (b) ground truth mask. (c) and (e) segmentation results by clustering sparse code vectors per image into $2$ or $3$ classes using a spectral clustering algorithm. (d) and (f) boundary connectivity information \cite{zhu2014saliency}
was used to decide the foreground and background.}
\label{figure:Flower_CUB}
\end{figure*}

\clearpage
\clearpage
{
   \small
   \bibliographystyle{ieee_fullname}
   \bibliography{egpaper_arxiv_V2}
}