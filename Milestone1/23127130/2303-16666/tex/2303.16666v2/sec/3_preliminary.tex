%\section{PRELIMINARIES}
\section{Preliminaries}
\label{sec:preliminaries}

In this section, we first briefly introduce the optimization framework of sparse coding. Then, we describe the algorithm unrolling version of ISTA (Learnable ISTA) \cite{gregor2010learning} involved in our approach.

\subsection{Sparse Coding}
Let $X \in \mathbb{R}^n$ be an input vector,  $Z\in 
 \mathbb{R}^K$ be a sparse code vector and $\mathbf{D}\in \mathbb{R}^{n\times K}$ be a codebook matrix whose columns are atoms.
 The goal is to find an optimal way to reconstruct $X$ based on a sparse linear combination of atoms in $\mathbf{D}$.
Sparse coding typically involves minimizing the following energy function:
\begin{align}
E_{\mathbf{D}}(X,Z) = \frac{1}{2}||X-\mathbf{D}Z||_2^2 + \alpha ||Z||_1.
\label{eq:1}
\end{align}
This energy function comprises two terms. The first term is a data term that penalizes differences between the input vector and its reconstruction as quantified by the $L2$ norm. The second term  is an $L1$ norm, which acts as regularization to induce sparsity on the sparse code vector $Z$.
$\alpha$ is a coefficient
controlling the sparsity penalty. 

\begin{figure}[t!]
\centering
\includegraphics[width=8.5cm]{./Figures/ista-lista-1.png}
\caption{(a) The diagram of the ISTA algorithm for
sparse coding. (b) The diagram of the Learnable ISTA, which is a time-unfolded version of the ISTA algorithm.}
\label{figure:2}
\vspace{-2pt}
\end{figure}


\begin{figure*}[htp!]
\centering
\includegraphics[width=16cm]{./Figures/scvae-arch.png}
\caption{A schematic representation of the proposed Sparse Coding-VAE with Learned ISTA (SC-VAE). SC-VAE integrates a Learnable ISTA network within VAE framework to learn sparse code vectors in the latent space for the input image. Each image can be represented as one or several sparse code vectors, depending on the number of downsampling blocks in the encoding process.}
\label{figure:3}
\vspace{-2pt}
\end{figure*}

\subsection{Learnable ISTA} \label{ISTA and LISTA}
A popular algorithm for learning sparse codes is called ISTA \cite{gregor2010learning}, which iterates the following recursive equation to convergence:
\begin{align}
Z(t+1) = h_{\theta}(W_{e}X + SZ(t))\quad Z(0) = 0.
\label{eq:2}
\end{align} 
The elements in  Eq. (\ref{eq:2}) are defined as follows:
\begin{align}
    \textup{filter matrix:} \quad &W_e = \frac{1}{L}\mathbf{D}^{\top} \nonumber\\
    \textup{mutual inhibition matrix:} \quad &S = I - \frac{1}{L}\mathbf{D}^{\top}\mathbf{D}  \nonumber\\
    \textup{shrinkage function:}\quad &[h_{\theta}(V)]_i = \textup{sign}(V_i)(|V_i|- \theta_i)_{+} \nonumber
\end{align}
Here, $L$ is a constant, which is defined as an upper bound on the largest eigenvalue of $\mathbf{D}^{\top}\mathbf{D}$.
Both the filter matrix and the mutual inhibition matrix depend on the codebook matrix $\mathbf{D}$.
The function $h_{\theta}(V)$ is a component-wise vector shrinkage function with a vector of thresholds $\theta$, where each element in $\theta$ is set to $\frac{\alpha}{L}$. 
In ISTA \cite{daubechies2004iterative}, the optimal sparse code is the fixed point of $Z(t+1) = h_{\theta}(W_{e}X + SZ(t))$. The block diagram is shown in Figure \ref{figure:2}(a).
In LISTA \cite{gregor2010learning}, $W_e$, $S$ and $\theta$ are treated as parameters of a time-unfolded recurrent neural network, where $S$ is shared over layers and the back-propagation algorithm can be performed over training samples. The number of rollout steps $s$ and the codebook matrix $\mathbf{D}$ in LISTA are predetermined. The block diagram of LISTA is shown in Figure \ref{figure:2}(b).



% \begin{algorithm}
% \caption{LISTA::fprop}\label{alg:cap}
% \begin{algorithmic}
% \Require :: fprop($X$, $Z$, $W_{e}$, $S$, $\theta$)\\
% ;; Arguments are passed by reference.\\
% ;; variables $Z(t)$, $C(t)$ and $B$ are saved for bprop.\\
% $B = W_{e}X = \frac{1}{L}\mathbf{D}^{\top}X$; \\
% $Z(0)=h_{\theta}(B)$\\
% \textbf{for} $t=1$ to $T$ \textbf{do}\\
%     \quad$C(t) = B + SZ(t-1)$\\
%     \quad$Z(t) = h_{\theta}(C(t))$\\
% \textbf{end for}\\
% $Z = Z(T)$
% \end{algorithmic}
% \end{algorithm}

%\subsection{Learnable ISTA has low rank properties} \label{LISTA}
%You need to prove that lista has low rank properties. Moreover, with the increasing of unrolling steps and parameters $\lambda$, the rank decreases and sparsity increase.

%\subsection{Why low rank properties are important}
%1. Low-rank dictionary
%learning not only enables us to provide a new data representation but also maintains feature correlation.\\
%Low-rank dictionary learning for unsupervised feature selection.\\


