\section{Experiments}
\label{sec:experiments}



% with sparsity presented
% \begin{table*}[!htbp] 
% \centering
% \caption{Quantitative reconstruction results on the validation sets of FFHQ \cite{karras2019style} ($10,000$ images) and ImageNet \cite{deng2009imagenet} ($50,000$ images). Comparison of PSNR, SSIM, LPIPS and rFIDs between ImageNet validation images and their reconstructed images according to the cookbook size
% ($K$) and the shape of latent codes ($\mathcal{C})$. $\dag$, $\ddag$, $\curlyvee$ and $\curlywedge$ are used to distinguish the same method with different $\mathcal{C}$ or $K$. The top two results across different metrics are highlighted in bold and red, respectively. } 
% \resizebox{0.75\linewidth}{!}{%
% \begin{tabular}{c|c|c|c|c|c|c|c|c}
%   \toprule
%   Model & Dataset & $\mathcal{C}$ & $K$ & Sparsity & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & rFID $\downarrow$\\
%   \midrule
%   VSC \cite{tonolini2020variational} &  & $256$ & - & - & $17.62$ & $0.4458$ & $0.6148$ & $450.98$\\
%   VSCLT \cite{fallah2022variational}  &  & $256$ & - & - & $12.97$ & $0.3034$ & $0.4426$ & $249.54$\\
%   Vanilla VAE \cite{kingma2013auto} &  & $256$ & - & - & $18.00$ & $0.4960$ & $0.6568$ & $178.17$ \\
%   $\beta$-VAE \cite{higgins2017beta} &  & $256$ & - & - & $16.63$ & $0.4763$ & $0.6878$ & $186.87$ \\
%   Info-VAE \cite{zhao2019infovae} &  & $256$ & - & - & $15.85$ & $0.4560$ & $0.6990$ & $232.37$ \\
%   VQGAN \cite{esser2021taming} & & $16 \times 16 \times 1$ & $1024$ & - & $22.24$ & $0.6641$ & $0.1175$ & $4.42$\\
%   ViT-VQGAN \cite{yu2021vector} & FFHQ & $32 \times 32 \times 1$ & $8192$ & - & - & - & - & $\textcolor{red}{3.13}$\\
%   RQ-VAE$^{\dag}$ \cite{lee2022autoregressive} & & $8 \times 8 \times 4$ & $2048$ & - & $22.99$ & $0.6700$ & $0.1302$ & $7.04$ \\
%   RQ-VAE$^{\ddag}$ \cite{lee2022autoregressive} &  & $16 \times 16 \times 4$ & $2048$  & - & $24.53$ & $0.7602$ & $0.0895$ & $3.88$ \\
%   Mo-VQGAN \cite{zheng2022movq} &  & $16 \times 16 \times 4$ & $1024$ & - & $26.72$ & $0.8212$ & $\textcolor{red}{0.0585}$ & $\textbf{2.26}$ \\
%   \rowcolor{Gray}
%   SC-VAE$^{\dagger}$ & & $1 \times 1\times 1$ & $512$ & $85.2\%$ & $17.25$ & $0.4946$ & $0.7177$ & $195.30$\\
%   \rowcolor{Gray}
%   SC-VAE$^{\ddag}$ & & $4 \times 4\times 1$ & $512$ & $96.0\%$ & $22.29$ & $0.6110$ & $0.4907$ & $115.91$\\
%   \rowcolor{Gray}
%   SC-VAE$^{\curlyvee}$ & & $16 \times 16\times 1$ & $512$ & $95.0\%$& $\textcolor{red}{29.70}$ & $\textcolor{red}{0.8347}$ & $0.1956$ & $41.56$\\
%   \rowcolor{Gray}
%   SC-VAE$^{\curlywedge}$ & & $32 \times 32 \times 1$ & $512$ & $75.8\%$ & $\textbf{34.92}$ & $\textbf{0.9497}$ & $\textbf{0.0080}$ & $4.21$\\
%   \midrule
%   VSC \cite{tonolini2020variational} &  & $256$ & - & - & $17.76$ & $0.5534$ & $0.5999$ & $454.75$\\
%   VSCLT \cite{fallah2022variational} &  & $256$ & - & - & $12.70$ & $0.3016$ & $0.7578$ & $336.72$\\
%   Vanilla VAE \cite{kingma2013auto} &  & $256$ & - & - & $17.88$ & $0.4441$ & $0.6957$ & $163.00$ \\
%   $\beta$-VAE \cite{higgins2017beta} & & $256$ & - & - & $16.10$ & $0.4165$ & $0.7142$ & $224.03$ \\
%   Info-VAE \cite{zhao2019infovae} &  & $256$ & - & - & $17.20$ & $0.4259$ & $0.7097$ & $178.94$ \\
%   VQGAN$^{\dag}$ \cite{esser2021taming} &  & $16 \times 16 \times 1$ & $1024$ & - & $19.47$ & $0.5214$ & $0.1950$ & $6.25$\\
%   VQGAN$^{\ddag}$ \cite{esser2021taming} &  & $16 \times 16\times 1$ & $16384$ & - & $19.93$ & $0.5424$ & $0.1766$ & $3.64$\\
%   %VQGAN$^{\S}$ &  & $32 \times 32 \times 1$ & $8192$ & $1.21$ & $23.55$ & $0.68$ \\
%   ViT-VQGAN \cite{yu2021vector} & ImageNet & $8 \times 8 \times 4$ & $2048$ & - & - & - & - & $1.28$\\
%   RQ-VAE \cite{lee2022autoregressive} &  & $8 \times 8 \times 4$ & $2048$& - & $20.76$ & $0.5416$ & $0.1381$ & $4.73$\\
%   Mo-VQGAN \cite{zheng2022movq} & & $16 \times 16 \times 4$ & $1024$ & - & $22.42$ & $0.6731$ & $\textcolor{red}{0.1132}$ & $\textcolor{red}{1.12}$\\
%   \rowcolor{Gray}
%   SC-VAE$^{\dag}$ & & $1 \times 1\times 1$ & $512$ &  $88.8\%$ & $16.86$ & $0.4306$ & $0.8163$ & $185.68$\\
%   \rowcolor{Gray}
%   SC-VAE$^{\ddag}$ & & $4 \times 4\times 1$ & $512$ & $93.6\%$& $23.04$ & $0.5906$ & $0.5081$ & $114.08$\\
%   \rowcolor{Gray}
%   SC-VAE$^{\curlyvee}$ & & $16 \times 16\times 1$ & $512$ & $93.7\%$ & $\textcolor{red}{30.74}$ & $\textcolor{red}{0.8594}$ & $0.1447$& $23.23$\\
%   \rowcolor{Gray}
%   SC-VAE$^{\curlywedge}$ & & $32 \times 32 \times 1$ & $512$ & $92.9\%$ & $\textbf{38.40}$ & $\textbf{0.9688}$ & $\textbf{0.0070}$& $\textbf{0.71}$\\
%   \bottomrule
% \end{tabular}}
% \label{Table_1}
% \end{table*}



%We present experimental results for our SC-VAE models on image reconstruction, image generation, patch clustering, and patch-based unsupervised image segmentation. An ablation study was also conducted to see the effect of the number of rollout steps in LISTA to our model. 
In this section, we first provide summaries of the datasets used to train the SC-VAE model
and elaborate on the implementation details. Then, we demonstrate the effectiveness of SC-VAE under different problem settings,  including 1) image reconstruction, 2) image generation through manipulating and interpolating learned sparse code vectors, 3) image patches clustering and 4) unsupervised image segmentation associated with its noise robustness analysis.
Moreover, an ablation study for the influence of the number of rollout steps (s) in LISTA is provided.

\noindent\textbf{Dataset.} For our experiments, we used the Flickr-Faces-HQ (FFHQ) \cite{karras2019style} and ImageNet \cite{deng2009imagenet} datasets. 
%The FFHQ dataset is a collection of high-quality images featuring human faces, which exhibits a significant amount of diversity in terms of the subjects' age, ethnicity, and the backgrounds depicted in the images. 
The FFHQ dataset comprises $70,000$ images with a training set of $60,000$ images and a validation set of $10,000$ images. ImageNet is a widely used benchmark dataset for visual recognition tasks. It consists of $1.2$ million training images and $50,000$ validation images, with each image associated with one of $1,000$ distinct categories. %The images in the dataset are high-resolution and have a wide variety of objects, scenes, and backgrounds, making it a challenging and diverse dataset for VAEs to model. 
The images in both datasets are high-resolution and diverse, making them challenging for VAEs to model. All images were resized to $256\times 256$ pixels.





\noindent\textbf{Implementation Details.} We adopted the encoder and decoder architecture of the VQGAN \cite{esser2021taming}.
We trained SC-VAE models on the FFHQ and ImageNet datasets with four different number of downsampling blocks.
The downsampling blocks $d$ of the encoder were set to $3$, $4$, $6$ and $8$, resulting in $32\times 32$, $16\times 16$, $4\times 4$ and $1\times 1$ sparse code vectors in the latent space for an input image with a resolution of $256\times 256\times 3$.
Moreover, the number of atoms in the dictionary, the number of rollout steps in LISTA and the dimension of latent vector representations were set to $512$, $16$ and $256$, respectively. 
%The number of atoms in the dictionary and the unfolded blocks in LISTA were set to $512$ and $5$ respectively. 
%The overall structure of the attention network $F$ was given by the following expression, in which $[a\times b ]$ symbolizes a multiplication by a matrix of that size: $E_{ij}(x)-\mathbf{D}Z_{ij}\rightarrow [256\times 64]\rightarrow \textup{Sigmoid} \rightarrow [64\times 1] \rightarrow \textup{Softmax} \rightarrow \alpha_{ij}$. The overcomplete
Discrete Cosine Transform (DCT) orthogonal matrix was used to generate the dictionary.
The sparsity coefficient $\alpha$ was initialized to $1$ and then updated during training.
To optimize our SC-VAE model, the Adam \cite{kingma2014adam} optimizer was used with a learning rate of $10^{-4}$. The training  ran for a total of $10$ epochs and the batch size for each iteration was set to $16$. We kept the models that achieved the lowest total loss for further evaluation. 
More detailed information about the architecture of SC-VAE along with a visualization of dictionary atoms can be found in the supplementary material.

\subsection{Image Reconstruction}
\noindent\textbf{Baselines and Evaluation Metrics.} Two sparse coding-based VAEs (VSC \cite{tonolini2020variational}  and VSCLT \cite{fallah2022variational} ), three continuous VAE (Vanilla VAE \cite{kingma2013auto}, $\beta$-VAE \cite{higgins2017beta}, and Info-VAE \cite{zhao2019infovae}) and four discrete VAE models (VQGAN \cite{esser2021taming}, ViT-VQGAN \cite{yu2021vector}, RQ-VAE \cite{lee2022autoregressive} and Mo-VQGAN \cite{zheng2022movq}) were selected as our baselines. 
%The model architecture used is the same as in the original paper.
We used the same model architectures as the ones described in the respective papers.
%Four most common evaluation metrics including PSNR, SSIM, LPIPS \cite{zhang2018unreasonable} and rFID \cite{heusel2017gans} were used to 
We evaluated the quality between reconstructed images and original images using four most common evaluation metrics (i.e., Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), Learned Perceptual Image Patch Similarity (LPIPS) \cite{zhang2018unreasonable}, and reconstructed Fréchet Inception Distance (rFID) \cite{heusel2017gans}). PSNR measures the amount of noise introduced by the reconstruction process. SSIM quantifies the similarity between two images by taking into account not only pixel values, but also the structural and textural information in the images. LPIPS and rFID use a pre-trained deep neural network to measure the perceptual distance and distribution distance between two images, respectively.

%Without sparsity presented
\begin{table}[!htbp]
\centering
\caption{Quantitative reconstruction results on the validation sets of FFHQ \cite{karras2019style} ($10,000$ images) and ImageNet \cite{deng2009imagenet} ($50,000$ images). 
%Comparison of PSNR, SSIM, LPIPS and rFIDs between  validation images and their reconstructed images according to the cookbook size ($K$) and the shape of latent codes ($\mathcal{C})$.
$\mathcal{C}$ and $K$ denote the shape of latent codes and the cookbook size, respectively.
$\dag$, $\ddag$, $\curlyvee$ and $\curlywedge$ are used to distinguish the same method with different $\mathcal{C}$. The top two results across different metrics are highlighted in bold and red, respectively.} 
\resizebox{1.0\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c|c}
  \toprule
  Model & Dataset & $\mathcal{C}$ & $K$  & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & rFID $\downarrow$\\
  \midrule
  VSC \cite{tonolini2020variational} &  & $256$ & -  & $17.62$ & $0.4458$ & $0.6148$ & $450.98$\\
  VSCLT \cite{fallah2022variational}  &  & $256$ & -  & $12.97$ & $0.3034$ & $0.4426$ & $249.54$\\
  Vanilla VAE \cite{kingma2013auto} &  & $256$ & -  & $18.00$ & $0.4960$ & $0.6568$ & $178.17$ \\
  $\beta$-VAE \cite{higgins2017beta} &  & $256$ & -  & $16.63$ & $0.4763$ & $0.6878$ & $186.87$ \\
  Info-VAE \cite{zhao2019infovae} &  & $256$ & -  & $15.85$ & $0.4560$ & $0.6990$ & $232.37$ \\
  VQGAN \cite{esser2021taming} & & $16 \times 16 \times 1$ & $1024$  & $22.24$ & $0.6641$ & $0.1175$ & $4.42$\\
  ViT-VQGAN \cite{yu2021vector} & FFHQ & $32 \times 32 \times 1$ & $8192$  & - & - & - & $\textcolor{red}{3.13}$\\
  RQ-VAE$^{\dag}$ \cite{lee2022autoregressive} & & $8 \times 8 \times 4$ & $2048$  & $22.99$ & $0.6700$ & $0.1302$ & $7.04$ \\
  RQ-VAE$^{\ddag}$ \cite{lee2022autoregressive} &  & $16 \times 16 \times 4$ & $2048$   & $24.53$ & $0.7602$ & $0.0895$ & $3.88$ \\
  Mo-VQGAN \cite{zheng2022movq} &  & $16 \times 16 \times 4$ & $1024$  & $26.72$ & $0.8212$ & $\textcolor{red}{0.0585}$ & $\textbf{2.26}$ \\
  \rowcolor{Gray}
  SC-VAE$^{\dagger}$ & & $1 \times 1\times 1$ & $512$  & $17.25$ & $0.4946$ & $0.7177$ & $195.30$\\
  \rowcolor{Gray}
  SC-VAE$^{\ddag}$ & & $4 \times 4\times 1$ & $512$  & $22.29$ & $0.6110$ & $0.4907$ & $115.91$\\
  \rowcolor{Gray}
  SC-VAE$^{\curlyvee}$ & & $16 \times 16\times 1$ & $512$ & $\textcolor{red}{29.70}$ & $\textcolor{red}{0.8347}$ & $0.1956$ & $41.56$\\
  \rowcolor{Gray}
  SC-VAE$^{\curlywedge}$ & & $32 \times 32 \times 1$ & $512$ &  $\textbf{34.92}$ & $\textbf{0.9497}$ & $\textbf{0.0080}$ & $4.21$\\
  \midrule
  VSC \cite{tonolini2020variational} &  & $256$ & - & $17.76$ & $0.5534$ & $0.5999$ & $454.75$\\
  VSCLT \cite{fallah2022variational} &  & $256$ & - & $12.70$ & $0.3016$ & $0.7578$ & $336.72$\\
  Vanilla VAE \cite{kingma2013auto} &  & $256$ & - & $17.88$ & $0.4441$ & $0.6957$ & $163.00$ \\
  $\beta$-VAE \cite{higgins2017beta} & & $256$ & - & $16.10$ & $0.4165$ & $0.7142$ & $224.03$ \\
  Info-VAE \cite{zhao2019infovae} &  & $256$ & - & $17.20$ & $0.4259$ & $0.7097$ & $178.94$ \\
  VQGAN$^{\dag}$ \cite{esser2021taming} &  & $16 \times 16 \times 1$ & $1024$ & $19.47$ & $0.5214$ & $0.1950$ & $6.25$\\
  VQGAN$^{\ddag}$ \cite{esser2021taming} &  & $16 \times 16\times 1$ & $16384$ & $19.93$ & $0.5424$ & $0.1766$ & $3.64$\\
  %VQGAN$^{\S}$ &  & $32 \times 32 \times 1$ & $8192$ & $1.21$ & $23.55$ & $0.68$ \\
  ViT-VQGAN \cite{yu2021vector} & ImageNet & $8 \times 8 \times 4$ & $2048$  & - & - & - & $1.28$\\
  RQ-VAE \cite{lee2022autoregressive} &  & $8 \times 8 \times 4$ & $2048$ & $20.76$ & $0.5416$ & $0.1381$ & $4.73$\\
  Mo-VQGAN \cite{zheng2022movq} & & $16 \times 16 \times 4$ & $1024$  & $22.42$ & $0.6731$ & $\textcolor{red}{0.1132}$ & $\textcolor{red}{1.12}$\\
  \rowcolor{Gray}
  SC-VAE$^{\dag}$ & & $1 \times 1\times 1$ & $512$  & $16.86$ & $0.4306$ & $0.8163$ & $185.68$\\
  \rowcolor{Gray}
  SC-VAE$^{\ddag}$ & & $4 \times 4\times 1$ & $512$ & $23.04$ & $0.5906$ & $0.5081$ & $114.08$\\
  \rowcolor{Gray}
  SC-VAE$^{\curlyvee}$ & & $16 \times 16\times 1$ & $512$  & $\textcolor{red}{30.74}$ & $\textcolor{red}{0.8594}$ & $0.1447$& $23.23$\\
  \rowcolor{Gray}
  SC-VAE$^{\curlywedge}$ & & $32 \times 32 \times 1$ & $512$  & $\textbf{38.40}$ & $\textbf{0.9688}$ & $\textbf{0.0070}$& $\textbf{0.71}$\\
  \bottomrule
\end{tabular}}

\label{Table_rec}
\end{table}
\begin{figure}[tbp]
\centering
\includegraphics[width=8cm]{./Figures/rec.png}
\caption{Image reconstructions from different models trained on ImageNet dataset. Original images in the top two rows are from the validation set of ImageNet dataset. Two external images are shown in the last two rows to demonstrate the generalizability of different methods. The numbers denote the shape of latent codes and learned codebook (dictionary) size, respectively. SC-VAE  achieved improved image reconstruction compared to the baselines. Zoom in to see the details of the red square area.}
\label{figure:rec}
\vspace{-2pt}
\end{figure}
Quantitative experimental results comparing the image reconstruction performance of SC-VAE with the baseline methods are listed in Table \ref{Table_rec}. We report the performance of our model with four different downsampling blocks ($d=3, 4, 6, 8$). The top two results across different metrics are highlighted in bold and red, respectively. As can be seen in the Table \ref{Table_rec}, SC-VAE significantly improved the image quality compared to other methods in terms of PSNR, SSIM and LPIPS scores in the FFHQ dataset and in terms of all scores in the ImageNet dataset when the shape of latent codes was set to $32\times32\times 1$.
%Our method outperformed other methods with a large margin in terms of PSNR and SSIM scores in the FFHQ dataset and in terms of PSNR, SSIM and LPIPS scores in the ImageNet dataset. 
Even after downsampling the original image to a shape of $16\times16\times1$, our approach significantly outperformed other methods in terms of PSNR and SSIM scores on both datasets. With increasing number of downsampling blocks, our model struggled with image reconstruction.

Among baseline methods, sparse coding-based VAEs (i.e. VSC \cite{tonolini2020variational} and VSCLT \cite{fallah2022variational}) and continuous VAEs (i.e. Vanilla VAE \cite{kingma2013auto}, $\beta$-VAE \cite{higgins2017beta}, and Info-VAE \cite{zhao2019infovae}) produced poor reconstructions of  the original images. This is because both FFHQ and ImageNet datasets exhibit a significant amount of diversity in terms of objects and background, which is challenging to model with a static prior distribution in the latent space. Discrete VAEs (i.e. VQGAN \cite{esser2021taming}, ViT-VQGAN \cite{yu2021vector}, RQ-VAE \cite{lee2022autoregressive} and Mo-VQGAN \cite{zheng2022movq}) demonstrated better performance compared to continuous VAEs. However, they produced lower PSNR and SSIM scores compared to our model. That might be because the adversarial training and perceptual loss of discrete VAEs force the model to generate more visually appealing and realistic outputs without paying sufficient attention to structural information. Furthermore, 
these models used a much larger codebook size than ours. 
%the codebooks of these models are considerably larger in size compared to ours.
The codebook collapse problem caused by the larger codebook might make it difficult for them to reconstruct detailed and accurate information.

%We compare MoVQ with the state-of-the-art methods for image reconstruction in Table 1. All instantiations of our model outperform the state-of-the-art methods under the same compression ratio (192x). This includes the concurrent works ViT-VQGAN [45] and RQ-VAE [24], which utilize higher resolution representation and recursively residual representation, respectively. Without bells and whistles, though we use a much smaller number of parameters (82.7M), similar to VQGAN (72.1M), MoVQ outperformers ViT-VQGAN [45], which employs a larger transformer model (599M) on higher resolution representation (32 × 32) for the first stage. The concurrent RQ-VAE work [24] also represents an image with multiple channels by recursively calculating the residual information between quantized vectors and their continuous ones, which requires much more embedding times. Furthermore, the entries in RQ-VAE are not equally important because the residual representation highlights the codevectors in the first round, leaving the other codevectors to capture the small residual information. In contrast, our codevectors share the same significance in all channels, resulting in larger representation capability. More importantly, our model dramatically improves the reconstructed images quality on all metrics, suggesting the reconstructed images are closer to the original inputs, which contributes to more downstream image interpolation tasks.
%Note that, our learned codebook contains only 1024 codevectors with 64 dimensionality, but interestingly outperforms other methods using larger codebook sizes. This suggests that a better quantizer can improve the codebook usage, and it is not necessary to greedily increase the codebook size.

Four images and paired reconstructed results from different models  were visualized in Figure \ref{figure:rec}.
%The first two rows display images that have been reconstructed by various models, as well as original images from ImageNet's validation set.
%Four reconstructed images across different models were visualized in Figure \ref{figure:4}. 
The models used to reconstruct these images are trained on the training set of ImageNet dataset. 
%The results from VIT-VQGAN and Mo-VQGAN were not given due to the unavailability of official code and pre-trained models.
The absence of official code and pre-trained models prevents us from providing results for VIT-VQGAN and Mo-VQGAN. We present visualizations from unofficial implementation of VIT-VQGAN in the supplementary materials for reference.
As for sparse coding and continuous VAEs, we refrained from presenting their visualizations due to their subpar reconstruction capabilities.
As is shown in the Figure \ref{figure:rec}, 
%the reconstructed results from Vanilla VAE, $\beta$-VAE, and Info-VAE were blurry. 
VQ-GAN, RQ-VAE and our models accomplished visually appealing outcomes. 
%Top two rows show the reconstructed images of different models and original images from the validation set of ImageNet.
The original images in the first two rows were randomly selected from the validation set of ImageNet. VQGAN tended to generate repeated artifacts on similar semantic patches, such as the leaves and noses. While RQ-VAE enhanced the visual aspect, it struggled to accurately reconstruct intricate details and complex patterns. In contrast to these models, SC-VAE generated much more authentic-looking details without suffering from any of the aforementioned shortcomings.
The original images in the last two rows are part of the external images of ImageNet dataset. They were used to test the generalizability of different models. SC-VAE was able to accurately identify and reconstruct patterns that have not previously been encountered. However, the reconstructed images from VQGAN and RQVAE did not preserve the detailed information and distorted the original images.



%The qualitative results are visualized in Figure \ref{figure:4}. The visualized results from VIT-VQGAN and Mo-VQGAN are not given due to the unavailability of official code and pre-trained models. All compared model are trained on ImageNet dataset. Top two rows shows the reconstructed images of different models and original images from the validation set of ImageNet dataset.


%The qualitative results are visualized in Figure \ref{figure:4}. MoVQ achieves impressive results under various conditions. In Fig. 3, we compare our MoVQ with the baseline model VQGAN [11] and the concurrent model RQ-VAE [24]. VQGAN holds repeated artifacts on the similar semantic patches, such as the grass and trees. RQ-VAE improves the visual appearance, but exhibits systematic artifacts with lossy information. Our MoVQ shows no such artifacts, providing much more realistic details.

%As is shown in the table 7, the proposed model obtains the best image reconstruction results when the shape of code map is $32\times 32 \times 1$ and $\beta=2$. Besides, there are several observations: 1) the reconstructed results increase with the decreasing of $\beta$. That is reasonable since the code vector $Z$ is less sparse with smaller $\beta$, leading to better reconstruction in the latent space; 2) when the shape of code map is set to $16\times 16 \times 1$, our results with $\beta = 1$ are competitive to other models even with much smaller codebook size. 3) when the shape of code map is set to $32\times 32 \times 1$ and $\beta=2$, our models perform much better than the existing models (FID: $0.84$, PSNR: $36.92$, SSIM: $0.97$).

%evaluation metrics. generalization.

%\textbf{Datasets.} To evaluate the proposed method, we instantiated MoVQ on both unconditional and class-conditional image generation tasks, with FFHQ \cite{karras2019style} and ImageNet \cite{russakovsky2015imagenet} respectively. The training and validation setting followed the default one of our baseline model VQGAN \cite{esser2021taming}. We trained all the models on $256 \times 256$ images.
%We evaluate the proposed method on FFHQ \cite{karras2019style} and ImageNet-1K \cite{deng2009imagenet} datasets. Extensive experiments show that our approach consistently achieves significant improvements on image reconstruction. 
%We didn't used Adversiral training but get good reconstruction results. Why are PSNR and SSIM much better?

% \begin{figure*}[t!]
% \centering
% \includegraphics[width=17cm]{./Figures/ffhq_cluster-1.png}
% \caption{Some image patches clusters from the validation set of FFHQ dataset generated by the learned sparse code vectors of SC-VAE model.}
% \label{figure:5}
% \end{figure*}


% \begin{figure*}[t!]
% \centering
% \includegraphics[width=17cm]{./Figures/imagenet_cluster-2.png}
% \caption{Some image patches clusters from the validation set of ImageNet dataset generated by the learned sparse code vectors of SC-VAE model.}
% \label{figure:6}
% \end{figure*}


\subsection{Image Generation}
\label{image_generation}
%In this section, we evaluate SC-VAE on image generation in terms of both manipulating and interpolating sparse code vectors.\\
%Nevertheless, we will illustrate in Section \ref{image_generation} that SC-VAE exhibits disentanglement behavior at the expense of its reconstruction capability.\\
In this section, we explore whether utilizing a pre-defined orthogonal DCT matrix in the SC-VAE model enables the generation of images by manipulating and interpolating sparse code vectors.\\
\noindent\textbf{Manipulating sparse code vectors.}
\begin{figure}[tbp]
\centering
\includegraphics[width=7.6cm]{./Figures/image_manipulation.png}
\caption{Manipulating sparse code vectors on FFHQ. Each row represents a different seed image used to infer
the latent sparse code vector in the SC-VAE$^\dag$ model.
The disentangled attributes associated with the $i$-th component of a sparse code vector $z$ and a traversal range are shown in the first column.}
\label{figure:manipulation}
\vspace{-2pt}
\end{figure}
%It is not possible to quantify feature disentanglement in natural data, as the source features are not known. However, similarly to \cite{kim2018disentangling}, we can qualitatively examine the effect of changing single latent variables on generated samples. 
Quantifying feature disentanglement in natural data is challenging since the source features are typically unknown. Nevertheless, as in the approach of \cite{kim2018disentangling}, we can assess the impact of altering individual components of a sparse code vector on the generated samples qualitatively.
%To this end, we train a VSC model with 100, 000 examples from the CelebA data set, encode examples from a test set, alter individually exploited dimensions in the latent space and finally generate samples from these altered latent vectors. 
To this end, we used the SC-VAE$^\dag$ model with a downsampling block of $8$ trained on the FFHQ dataset. 
%encode examples from a validation set, alter individually exploited dimensions in the sparse code vectors and finally generate samples from these altered latent sparse vectors. 
We selected examples from the validation set, manipulated individual dimensions within the sparse code vectors, and then produced samples based on these modified latent sparse vectors.
%We find that several of the dimensions exploited by the VSC model control interpretable aspects in the generated data, as shown in the examples of Figure \ref{figure:manipulation}.
We found that $10$ components of the dimensions exploited by the SC-VAE$^\dag$ model controlled interpretable aspects in the generated data, as is shown in Figure \ref{figure:manipulation}.
%The $1$-st and $354$-th dimensions contains two interpretable attributes, as is shown in the examples of Figure \ref{figure:manipulation}. 
%To the best of our knowledge, we are the first one to report the disentanglement behaviour of a deterministic VAE.
\\
\noindent\textbf{Interpolating sparse code vectors.}
Figure \ref{figure:interpolation} shows
smooth interpolation between the latent sparse code vectors of two images generated by SC-VAE$^\dag$.\\
Additional manipulation and interpolation results can be found in the supplementary material.

\begin{figure}[tbp]
\centering
\includegraphics[width=8.2cm]{./Figures/image_interpolation.png}
\caption{Interpolation between the sparse code vectors of two samples from the SC-VAE$^{\dag}$ model trained on FFHQ.}
\label{figure:interpolation}
\vspace{-2pt}
\end{figure}

\subsection{Image Patches Clustering}
The latent sparse code vectors learned by SC-VAE can be thought of as compressed representations of the input image. To better interpret the learned sparse code vectors, we aligned each of them with a corresponding patch of the input image. Image patch clustering was then performed based on these sparse code vectors.
We examined one pre-trained SC-VAE$^\curlyvee$ model on FFHQ dataset with a downsampling block of $d=4$. $1,000$ images were randomly selected from the validation set, resulting in $256,000$ pairs of image patches with a resolution of $16\times 16\times 3$ and sparse code vectors with a dimension of $512$. The sparse code vectors were clustered into $1,000$ groups using the K-means clustering algorithm. We randomly selected $15$ groups and $40$ patches from each group to visualize the clustering results. As can be seen in Figure \ref{figure:image_patches_clustering}, image patches with similar patterns were grouped together. More clustering results on the validation set of FFHQ and ImageNet datasets can be found in the supplementary material.
\begin{figure}[t!]
\centering
\includegraphics[width=8.2cm]{./Figures/clusters_ffhq_15.png}
\caption{$15$ randomly selected image patch clusters generated by clustering the learned sparse code vectors of SC-VAE$^\curlyvee$ using the K-means algorithm. Each row represents one cluster. Image patches with similar patterns were grouped together.}
\label{figure:image_patches_clustering}
\vspace{-2pt}
\end{figure}

% \begin{table}[htbp]
% \centering
% \caption{Unsupervised segmentation results on Flowers \cite{nilsback2008automated} and CUB \cite{WahCUB_200_2011}, measured in terms of IoU and DICE score. SC-VAE is compared with the state-of-the-art un(weakly-)supervised segmentation methods. Following the \cite{yu2021unsupervised}, $\dag$ indicates unfair baseline results obtained using extra ground-truth information. 
% $\divideontimes$ represents a GAN-based model.
% OneGAN$\diamond$ is a weakly supervised baseline, which requires clean backgrounds as additional inputs.} 
% \resizebox{0.9\linewidth}{!}{
% \begin{tabular}{c|cc|cc}
%   \toprule
%   \multicolumn{1}{c}{Dataset} &\multicolumn{2}{|c|}{Flowers} & \multicolumn{2}{|c}{CUB}   \\
%   \midrule
%   Methods &  IoU $\uparrow$ & DICE $\uparrow$  & IoU $\uparrow$ & DICE $\uparrow$  \\
%   \midrule
%   GrabCut \cite{rother2004grabcut} & $69.2$ & $79.1$ &  $36.0$ & $48.7$   \\
%   W-Net \cite{xia2017w} & $74.3$ & $83.0$ &  $24.8$ & $38.9$   \\
%   ReDO$\dag^{\divideontimes}$ \cite{chen2019unsupervised}  & $76.4$ & -  & $42.6$ & -   \\
%   IODINE$\dag$ \cite{greff2019multi}  & $32.6$ & $46.0$  & $30.9$ & $44.6$   \\
%   OneGAN$\diamond^{\divideontimes}$ \cite{benny2020onegan}  & - & -  & $55.5$ & $69.2$ \\
%   Slot-Attn.$\dag$ \cite{locatello2020object}  & $32.9$ & $45.7$  & $35.6$ & $51.5$  \\
%   IEM + SegNet \cite{savarese2021information}   & $78.9$ & $86.0$  & $55.1$ & $68.7$   \\
%   GANSeg$^\divideontimes$ \cite{he2022ganseg} &  $73.9$ & -  & $\textbf{62.9}$ & -  \\
%   DRC \cite{yu2021unsupervised} &  $46.9$ & $60.8$  & $56.4$ & $70.9$  \\
%   DS-ComGAN$^\divideontimes$ \cite{ding2022comgan} &  $76.9$ & $83.1$  & $60.7$ & $71.3$  \\
%   \midrule
%   SC-VAE (2 classes) &  $\textbf{81.2}$ & $\textbf{88.5}$   & $52.9$ & $67.4$  \\
%   SC-VAE (3 classes) & $67.3$ & $78.6$ & $57.5$ & $\textbf{71.4}$  \\
%   \bottomrule
% \end{tabular}}
% \label{tbl_un}
% \centering
% \end{table}
\begin{figure}[tbp]
\centering
\includegraphics[width=8cm]{./Figures/image_segmentation-3.png}
\caption{Unsupervised image segmentation results. \textbf{Top}: images from the validation set of FFHQ. \textbf{Bottom}: images from the validation set of ImageNet.}
\label{figure:UIS_quali}
\vspace{-2pt}
\end{figure}

\subsection{Unsupervised Image Segmentation}
This section investigates whether the sparse code vectors estimated by our models enable us to conduct image segmentation tasks without supervision.
\\
\textbf{Qualitative analysis.} We utilized two SC-VAE$^\curlywedge$ models that were pre-trained on the training set of the FFHQ and ImageNet datasets, respectively. These models had a downsampling block of $d = 3$. The images were transformed into sparse code vectors with a size of $32 \times 32 \times 1$ using the encoder and LISTA network of
the SC-VAE$^\curlywedge$. Afterwards, the K-means algorithm was applied to cluster these sparse code vectors into $5$ categories, generating a $32 \times 32$ mask with $5$ different classes for each image. The segmentation results were visualized in Figure \ref{figure:UIS_quali}. The faces and objects in the images were successfully detected and segmented by simply grouping the patch-level sparse codes using the K-means algorithm.\\
\noindent
\textbf{Quantitative comparisons to prior work.} 
We evaluated the SC-VAE$^\curlywedge$ model pre-trained on ImageNet dataset on Flowers \cite{nilsback2008automated}, Caltech-UCSD Birds-200-2011 (CUB) \cite{WahCUB_200_2011} and  International Skin Imaging Collaboration 2016 (ISIC-2016) \cite{gutman2016skin} datasets. 
%The SC-VAE model was pre-trained on ImageNet dataset with a downsampling block of $d = 3$.
%We compared its performance to prior works (see Table \ref{tbl_un}).
%We evaluated SC-VAE on Caltech-UCSD Birds-200-2011 (CUB) \cite{he2022ganseg} and Flowers \cite{he2022ganseg} datasets and compared its performance to prior works (see Table \ref{tbl_un}). 
We followed \cite{savarese2021information} to generate the test set and the ground-truth masks of Flowers \cite{nilsback2008automated} and CUB \cite{WahCUB_200_2011}. 
%The unsupervised segmentation results were obtained by applying a spectral clustering algorithm on sparse code vectors. 
%To obtain the unsupervised image segmentation results, we first used a spectral clustering algorithm to cluster the sparse code vectors into $2$ or $3$ classes per image. Then the boundary connectivity \cite{zhu2014saliency} was used to decide if each cluster belongs to the foreground or background.
To achieve unsupervised image segmentation results, we initially applied a spectral clustering algorithm to group the sparse code vectors into 2 or 3 classes per image. Subsequently, we utilized boundary connectivity information \cite{zhu2014saliency} to determine whether each class corresponds to the foreground or background.
We compared the performance of our method to prior works (see Table \ref{tbl_un}). 
Note that this is not a fair comparison.
Most of the methods we compare to, excluding GrabCut \cite{rother2004grabcut} and IEM \cite{savarese2021information}, require a training set with a distribution identical to that of the test set for model training. In contrast, our pre-trained SC-VAE$^\curlywedge$ model can segment an image without the need for fine-tuning on Flower \cite{nilsback2008automated}, CUB \cite{WahCUB_200_2011} and ISIC-2016 \cite{gutman2016skin} datasets.
%since all compared methods except GrabCut \cite{rother2004grabcut} require a training set with a distribution identical to the test set to train their models while our pre-trained SC-VAE$^\curlywedge$ model can segment an image without the need for fine-tuning on Flower \cite{nilsback2008automated} and CUB \cite{WahCUB_200_2011} datasets.
Interestingly, our model performed better than most of the baselines, which used more sophisticated approaches for segmentation, such as adversarial training \cite{benny2020onegan, chen2019unsupervised, he2022ganseg, yu2021unsupervised, ding2022comgan} and attention mechanisms \cite{locatello2020object}. 
%We believe that the reason that SC-VAE$^\curlywedge$ outperformed the baselines is that SC-VAE$^\curlywedge$ can learn meaningful and distinctive sparse representations, which allows for learning relations between patches. 
We attribute the superior performance of SC-VAE$^\curlywedge$ over the baselines to its capability to learn meaningful and distinct sparse representations. This ability enables SC-VAE$^\curlywedge$ to learn relationships between patches effectively.
Several qualitative results of SC-VAE$^\curlywedge$ on Flower \cite{nilsback2008automated}, CUB \cite{WahCUB_200_2011} and ISIC-2016 \cite{gutman2016skin} datasets are shown in Figure \ref{figure:UIS_flowers}. More qualitative results can be found in the supplementary material.
%This is in contrast to most of the baselines, which use more sophisticated approaches for segmentation, such as adversarial training \cite{ chen2019unsupervised, he2022ganseg, savarese2021information, savarese2021information, yu2021unsupervised, ding2022comgan} and self-supervised learning \cite{hung2019scops}. The reason that SC-VAE outperformed the baselines is that SC-VAE can learn meaningful and distinctive sparse representations, which allows for learning relations between patches.\\
\begin{figure}[tbp]
\centering
\includegraphics[width=7.8cm]{./Figures/quali_flower_cub_isic2016.png}
\caption{Unsupervised image segmentation results of SC-VAE$^\curlywedge$ on Flowers \cite{nilsback2008automated}, CUB \cite{WahCUB_200_2011} and ISIC-2016 \cite{gutman2016skin}. \textit{Left to right:} (a) input image. (b) ground truth mask. (c) and (e) segmentation results by clustering sparse code vectors per image into $2$ or $3$ clusters using a spectral clustering algorithm. (d) and (f) boundary connectivity information \cite{zhu2014saliency} was used to decide the foreground and the background.}
\label{figure:UIS_flowers}
\vspace{-2pt}
\end{figure}


\begin{table}[!htp]
\centering
\caption{Unsupervised segmentation results on Flowers \cite{nilsback2008automated} and CUB \cite{WahCUB_200_2011}, measured in terms of IoU and DICE scores. SC-VAE $^\curlywedge$ was compared with state-of-the-art unsupervised and weakly supervised segmentation methods.
$\divideontimes$ denotes a GAN-based model.
OneGAN$\diamond$ is a weakly supervised baseline that relies on clean backgrounds as additional input. The best results across different metrics and datasets are highlighted in bold.} 
%\sisetup{detect-all}
\NewDocumentCommand{\B}{}{\fontseries{b}\selectfont}
\resizebox{0.85\linewidth}{!}{%
\begin{tabular}{
  @{}
  l
  S[table-format=1.2]
  S[table-format=1.2]|
  S[table-format=1.2]
  S[table-format=1.2]|
  S[table-format=1.2]
  S[table-format=1.2]
  @{}
}
\toprule
& \multicolumn{2}{c}{Flowers} & \multicolumn{2}{c}{CUB} & \multicolumn{2}{c}{ISIC-2016} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
Methods & { IoU $\uparrow$ } & { DICE $\uparrow$ } & { IoU $\uparrow$ } & { DICE $\uparrow$ } & { IoU $\uparrow$ } & { DICE $\uparrow$ }\\
\midrule
GrabCut \cite{rother2004grabcut} & {$69.2$} & {$79.1$} &  {$36.0$} & {$48.7$} & {-} & {-}  \\
W-Net \cite{xia2017w} & {$74.3$} & {$83.0$} &  {$24.8$} & {$38.9$} &  {$31.8$} & {$44.4$}  \\
ReDO$^{\divideontimes}$ \cite{chen2019unsupervised}  & {$77.8$} & {$85.3$}  & {$43.5$} & {$56.8$} & {$20.1$} & {$29.1$}  \\
IODINE \cite{greff2019multi}  & {$32.6$} & {$46.0$}  & {$30.9$} & {$44.6$}  & {$31.9$} & {$42.4$} \\
PerturbGAN$^\divideontimes$ \cite{bielski2019emergence}  & {-} & {-}  & {$38.0$} & {-} & {-} & {-}\\
OneGAN$\diamond^{\divideontimes}$ \cite{benny2020onegan}  & {-} & {-}  & {$55.5$} & {$69.2$} & {-} & {-}\\
Slot-Attn \cite{locatello2020object}  & {$32.9$} & {$45.7$}  & {$35.6$} & {$51.5$} & {$22.2$} & {$32.8$} \\
IEM \cite{savarese2021information}   & {$76.8$} & {$84.6$}  & {$52.2$} & {$66.0$} & {$\textbf{65.5}$} & {$\textbf{75.1}$}  \\
DRC \cite{yu2021unsupervised} &  {$46.9$} & {$60.8$}  & {$56.4$} & {$70.9$} & {$44.1$} & {$56.4$} \\
%GANSeg$^\divideontimes$ \cite{he2022ganseg} &  {$73.9$} & {-}  & {$\textbf{61.0}$} & {$\textbf{73.2}$}  \\
DS-ComGAN$^\divideontimes$ \cite{ding2022comgan} &  {$76.9$} & {$83.1$}  & {$\textbf{60.7}$} & {$71.3$} & {$43.9$} & {$57.8$} \\
\midrule
SC-VAE$^\curlywedge$ (2 classes) &  {$\textbf{81.2}$} & {$\textbf{88.5}$}   & {$53.2$} & {$67.6$} & {$61.6$} & {$70.7$} \\
SC-VAE$^\curlywedge$ (3 classes) & {$67.3$} & {$78.6$} & {$58.2$} & {$\textbf{72.1}$} & {$60.8$} & {$71.5$}\\
\bottomrule
\end{tabular}}
\label{tbl_un}
\end{table}

% \begin{table}[!htp]
% \centering
% \caption{Unsupervised segmentation results on Flowers \cite{nilsback2008automated} and CUB \cite{WahCUB_200_2011}, measured in terms of IoU and DICE scores. SC-VAE $^\curlywedge$ was compared with state-of-the-art unsupervised and weakly supervised segmentation methods.
% $\divideontimes$ denotes a GAN-based model.
% OneGAN$\diamond$ is a weakly supervised baseline that relies on clean backgrounds as additional input. The best results across different metrics and datasets are highlighted in bold.} 
% \NewDocumentCommand{\B}{}{\fontseries{b}\selectfont}
% \resizebox{0.75\linewidth}{!}{%
% \begin{tabular}{
%   @{}
%   l
%   S[table-format=1.2]
%   S[table-format=1.2]
%   S[table-format=1.2]
%   S[table-format=1.2]
%   S[table-format=1.4]
%   S[table-format=1.4]
%   @{}
% }
% \toprule
% & \multicolumn{2}{c}{Flowers} & \multicolumn{2}{c}{CUB} & \multicolumn{2}{c}{ISIC-2016}\\
% \cmidrule(lr){2-3}  \cmidrule(lr){4-5}  \cmidrule(lr){6-7} 
% %Methods  & {FID$\downarrow$} & {IS$\uparrow$} & {FID$\downarrow$} & {IS$\uparrow$}  \\
% Methods &  {IoU $\uparrow$} & {DICE $\uparrow$}  & {IoU $\uparrow$} & {DICE $\uparrow$} & {IoU $\uparrow$} & {DICE $\uparrow$} \\
% \midrule
% GrabCut \cite{rother2004grabcut} & {$69.2$} & {$79.1$} &  {$36.0$} & {$48.7$}  & {-} & {-} \\
% W-Net \cite{xia2017w} & {$74.3$} & {$83.0$} &  {$24.8$} & {$38.9$} & {-} & {-}  \\
% ReDO$^{\divideontimes}$ \cite{chen2019unsupervised}  & {$77.8$} & {$85.3$}  & {$43.5$} & {$56.8$} & {-} & {-}  \\
% IODINE \cite{greff2019multi}  & {$32.6$} & {$46.0$}  & {$30.9$} & {$44.6$}   & {-} & {-}\\
% PerturbGAN$^\divideontimes$ \cite{bielski2019emergence}  & {-} & {-}  & {$38.0$} & {-} & {-} & {-}\\
% OneGAN$\diamond^{\divideontimes}$ \cite{benny2020onegan}  & {-} & {-}  & {$55.5$} & {$69.2$} & {-} & {-}\\
% Slot-Attn \cite{locatello2020object}  & {$32.9$} & {$45.7$}  & {$35.6$} & {$51.5$} & {-} & {-} \\
% IEM + SegNet \cite{savarese2021information}   & {$78.9$} & {$86.0$}  & {$55.1$} & {$68.7$} & {-} & {-}  \\
% DRC \cite{yu2021unsupervised} &  {$46.9$} & {$60.8$}  & {$56.4$} & {$70.9$} & {-} & {-} \\
% %GANSeg$^\divideontimes$ \cite{he2022ganseg} &  {$73.9$} & {-}  & {$\textbf{61.0}$} & {$\textbf{73.2}$}  \\
% DS-ComGAN$^\divideontimes$ \cite{ding2022comgan} &  {$76.9$} & {$83.1$}  & {$\textbf{60.7}$} & {$71.3$} & {-} & {-} \\
% \midrule
% SC-VAE$^\curlywedge$ (2 classes) &  {$\textbf{81.2}$} & {$\textbf{88.5}$}   & {$53.2$} & {$67.6$} & {-} & {-} \\
% SC-VAE$^\curlywedge$ (3 classes) & {$67.3$} & {$78.6$} & {$58.2$} & {$\textbf{72.1}$} & {-} & {-} \\
% \bottomrule
% \bottomrule
% \end{tabular}}
% \label{tbl_un}
% \vspace{-3pt}
% \end{table}

\noindent
\textbf{Noise robustness analysis.} 
%Learnable ISTA was widely used to perform image denoising tasks. We exaimne if the pretrained SC-VAE$^\curlywedge$ has noise robustness property towards unsupervised image segmentation.We add i.i.d. Gaussian noise with zero mean and a specified level of noise $\sigma$ to images and compared with the performance of pretrained models including ReDo, DRC and DS-ComGAN. As is shown in Figure. \ref{figure:NRA}, SC-VAE$^\curlywedge$ and DRC is robust to Gaussian noise.
Sparse coding has been extensively applied for image denoising tasks. We investigate whether the pretrained SC-VAE$^\curlywedge$ exhibits resilience to noise in the unsupervised image segmentation task. To evaluate this, we added independent and identically distributed (i.i.d.) Gaussian noise with zero mean and various levels of noise $\sigma$ to each image in the test set of CUB dataset and compared its performance with three  baselines  (ReDO \cite{chen2019unsupervised}, DRC \cite{yu2021unsupervised}, and DS-ComGAN \cite{ding2022comgan}), which have been pretrained on the training set of CUB dataset. The performance of other baselines was not shown due to the unavailability of pretrained models. As depicted in Figure \ref{figure:NRA}, both SC-VAE$^\curlywedge$ and DRC \cite{yu2021unsupervised} demonstrated robustness against Gaussian noise whereas the performance of ReDO \cite{chen2019unsupervised} and DS-ComGAN \cite{ding2022comgan} noticeably declined as the noise level increased.
DRC \cite{yu2021unsupervised} includes Total Variation norm \cite{rudin1992nonlinear} within its loss function, which potentially explains the robustness to noise it exhibited.
%while the performance of ReDo\cite{chen2019unsupervised} and DS-ComGAN \cite{ding2022comgan} decrease significantly with the increasing of noise level.
\begin{figure}[tbp]
\centering
\includegraphics[width=8cm]{./Figures/noise-robustness4.png}
\caption{Noise robustness analysis. We show the unsupervised segmentation results of SC-VAE$^\curlywedge$ and three baselines on CUB \cite{WahCUB_200_2011}  at different levels of noise $\sigma$.}
\label{figure:NRA}
\vspace{-2pt}
\end{figure}

% \begin{table}[!htp]
% \centering
% \caption{Unsupervised segmentation results on Flowers \cite{nilsback2008automated} and CUB \cite{WahCUB_200_2011}, measured in terms of IoU and DICE score. SC-VAE is compared with the state-of-the-art un(weakly-)supervised segmentation methods. $\star$ indicates unfair baseline results when additional ground-truth informaton is utilized. 
% $\divideontimes$ represents a GAN-based model.
% OneGAN$\diamond$ is a weakly supervised baseline, which requires clean backgrounds as additional inputs.} 
% \sisetup{detect-all}
% \NewDocumentCommand{\B}{}{\fontseries{b}\selectfont}
% \resizebox{0.9\linewidth}{!}{%
% \begin{tabular}{
%   @{}
%   l
%   S[table-format=1.2]
%   S[table-format=1.2]|
%   S[table-format=1.2]
%   S[table-format=1.2]|
%   S[table-format=1.2]
%   S[table-format=1.2]
%   @{}
% }
% \toprule
% \toprule
% &  \multicolumn{2}{c}{Flowers} & \multicolumn{2}{c}{CUB}  & \multicolumn{2}{c}{Dogs} \\
% \cmidrule(lr){2-3} \cmidrule(lr){4-5}  \cmidrule(lr){6-7}
% %Methods  & {FID$\downarrow$} & {IS$\uparrow$} & {FID$\downarrow$} & {IS$\uparrow$}  \\
% Methods &  {IoU $\uparrow$} & {DICE $\uparrow$}  & {IoU $\uparrow$} & {DICE $\uparrow$}  & {IoU $\uparrow$} & {DICE $\uparrow$}  \\
% \midrule
% GrabCut \cite{rother2004grabcut} & {$69.2$} & {$79.1$} &  {$36.0$} & {$48.7$} &  {$58.3$} & {$70.9$}   \\
% W-Net \cite{xia2017w} & {$74.3$} & {$83.0$} &  {$24.8$} & {$38.9$} &  {$47.7$} & {$62.1$}  \\
% ReDO$\star^{\divideontimes}$ \cite{chen2019unsupervised}  & {$76.4$} & {-}  & {$42.6$} & {-} & {$55.7$} & {$70.3$}  \\
% IODINE$\star$ \cite{greff2019multi}  & {$32.6$} & {$46.0$}  & {$30.9$} & {$44.6$} & {$54.4$} & {$67.0$}  \\
% OneGAN$\diamond^{\divideontimes}$ \cite{benny2020onegan}  & {-} & {-}  & {$55.5$} & {$69.2$}  & {$71.0$} & {$81.7$} \\
% Slot-Attn.$\star$ \cite{locatello2020object}  & {$32.9$} & {$45.7$}  & {$35.6$} & {$51.5$} & {$38.6$} & {$55.3$} \\
% IEM + SegNet \cite{savarese2021information}   & {$78.9$} & {$86.0$}  & {$55.1$} & {$68.7$} & {-} & {-}  \\
% GANSeg$^\divideontimes$ \cite{he2022ganseg} &  {$73.9$} & {-}  & {$\textbf{62.9}$} & {-}  & {-} & {-}  \\
% DRC \cite{yu2021unsupervised} &  {$46.9$} & {$60.8$}  & {$56.4$} & {$70.9$}  & {$71.7$} & {$83.2$} \\
% DS-ComGAN$^\divideontimes$ \cite{ding2022comgan} &  {$76.9$} & {$83.1$}  & {$60.7$} & {$71.3$} & {$74.5$} & {$84.6$}  \\
% \midrule
% SC-VAE (2 classes) &  {$\textbf{81.2}$} & {$\textbf{88.5}$}   & {$52.9$} & {$67.4$}  \\
% SC-VAE (3 classes) & {$67.3$} & {$78.6$} & {$57.5$} & {$\textbf{71.4}$}  \\
% \bottomrule
% \bottomrule
% \end{tabular}}
% \label{tbl_un}
% \end{table}
% \begin{figure}[tbp]
% \includegraphics[width=8.4cm]{./Figures/UIS_Flowers_CUB.png}
% \caption{Unsupervised image segmentation results of SC-VAE on Flowers and CUB. \textit{Left to right:} (a) input image. (b) ground truch mask. (c) and (e) segmentation results by clustering sparse code vectors per image into $2$ or $3$ clusters using a spectral clustering algorithm. (d) and (f) the boundary connectivity \cite{zhu2014saliency} was used to decide the foreground and background.}
% \label{figure:UIS_flowers}
% \end{figure}

%Fine-grained segmentation results can be obtained by designing more sophisticated segmentation methods based on our models. In addition, utilizing the learned sparse code vectors for segmentation is significantly more computationally efficient, making it suitable for ultra-high-resolution images. Additional segmentation outcomes for both datasets are available in the supplementary material.

% \begin{figure}[tbp]
% \includegraphics[width=8.4cm]{./Figures/CUB_unsueprvised_segmentation.png}
% \caption{Unsupervised image segmentation results.}
% \label{figure:UIS_flowers}
% \end{figure}

% \begin{table*}[!htbp] 
% \begin{tabular}{c|cc|cc|cc|cc|cc}
%   \toprule
%   \multicolumn{1}{c}{Dataset} &\multicolumn{2}{|c|}{Flowers} & \multicolumn{2}{|c|}{CUB} & \multicolumn{2}{|c|}{Stanford-Dogs} & \multicolumn{2}{|c|}{Stanford-Cars} &  \multicolumn{2}{c}{Red Sea Bream}  \\
%   \midrule
%   Methods &  IoU $\uparrow$ & DICE $\uparrow$  & IoU $\uparrow$ & DICE $\uparrow$  &  IoU $\uparrow$ & DICE $\uparrow$ &  IoU $\uparrow$ & DICE $\uparrow$ &  IoU $\uparrow$ & DICE $\uparrow$   \\
%   \midrule
%   GrabCut \cite{rother2004grabcut} & $69.2$ & $79.1$ &  $36.0$ & $48.7$ & $58.3$ & $70.9$ & $61.3$ & $73.1$ &  - & -  \\
%   W-Net \cite{xia2017w} & - & - &  $24.8$ & $38.9$ & $47.7$ & $62.1$ & $52.8$ & $67.7$ &  - & -  \\
%   %Scops \cite{hung2019scops} (2019) & $32.9$ & - & $54.4$ & - & - & - & - & - &  - & -  \\
%   %PerturbGAN \cite{bielski2019emergence} (Code Available) & - & - & $38.0$ & -  & - & - & - & - & - & -\\
%   ReDO \cite{chen2019unsupervised}  & $76.4$ & -  & $42.6$ & - & $55.7$ & $70.3$ & $52.5$ & $68.6$ & - & -  \\
%   IODINE \cite{greff2019multi}  & - & -  & $30.9$ & $44.6$ & $54.4$ & $67.0$ & $51.7$ & $67.3$ & - & -  \\
%   OneGAN \cite{benny2020onegan}  & - & -  & $55.5$ & $69.2$ & $71.0$ & $81.7$ & $71.2$ & $82.6$ & - & -  \\
%   Slot-Attn \cite{locatello2020object}  & - & -  & $35.6$ & $51.5$ & $38.6$ & $55.3$ & $41.3$ & $58.3$ & - & -  \\
%   %IEM \cite{savarese2021information} (Code Available)   & $76.8$ & $84.6$ & $52.2$ & $66.0$  & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\
%   IEM + SegNet \cite{savarese2021information}   & $78.9$ & $86.0$  & $55.1$ & $68.7$ & - & - & - & - & - & - \\
%   GANSeg \cite{he2022ganseg} &  $73.9$ & -  & $\textbf{62.9}$ & - & - & - & - & - & - & - \\
%   DRC \cite{yu2021unsupervised} &  $46.9$ & $60.8$  & $56.4$ & $70.9$\\
%   DS-ComGAN \cite{ding2022comgan} &  $76.9$ & $83.1$ & $60.7$ & $71.3$ & $74.5$ & $84.6$ & $76.7$ & $86.6$ & - & -\\
%   \midrule
%   %SC-VAE (K-means, 2 classes) & $74.3$ & $83.9$  & $45.5$ & $59.4$ \\
%   %SC-VAE (K-means, 3 classes) & $74.5$ & $83.9$  & $49.3$ & $63.5$ \\
%   %SC-VAE (Spectral clustering, 2 classes) & $(89.2, 89.6, 89.3, 88.8, 88.6, 89.1, 89.2, 89.2, 89.4, 88.6)$ & $\textbf{81.2}$ & $\textbf{88.5}$  & & $52.9$ & $67.4$ \\
%   %SC-VAE (Spectral clustering, 3 classes) & $(76.2, 75.7, 75.9, 76.3, 76.1, 76.3, 76.6, 76.0, 76.3, 75.7)$ & $67.3$ & $78.6$ &  & $57.5$ & $\textbf{71.4}$ \\
%   SC-VAE (SC, 2 classes) &  $\textbf{81.2}$ & $\textbf{88.5}$   & $52.9$ & $67.4$ & $60.4$ & $73.1$ & $50.0$ & $65.5$  & $79.5$ & $88.3$ \\
%   SC-VAE (SC, 3 classes) & $67.3$ & $78.6$ & $57.5$ & $\textbf{71.4}$ & $61.8$ & $74.7$ & $53.9$ & $68.4$  & $36.0$ & $52.5$ \\
%   \bottomrule
% \end{tabular}
% \label{tbl_un}
% \centering
% \caption{Unsupervised segmentation results on Flowers \cite{he2022ganseg} and CUB \cite{he2022ganseg}, measured in terms of IoU and DICE score.} 
% \end{table*}




\subsection{Ablation Study}
\noindent
We conducted an ablation study to analyze the influence of the number of rollout steps ($s$) in LISTA on the reconstruction ability of SC-VAE.  We trained the SC-VAE$^\curlyvee$ model ($d=4$) with different $s$ and reported the quantitative results in Table \ref{ablation_16x16}.
The sparsity of the learned sparse code vectors was calculated using the Hoyer metric \cite{hoyer2004non}. We reported the average sparsity and four reconstruction metrics among the validation set of FFHQ dataset. Sparsity first decreased and then increased as $s$ increased. Lower sparsity corresponds to better reconstruction outcomes.
%$s=1: ./results/models/FFHQ/l_alpha_3_FFHQ_num_epochs10_bs8_num_soft_thresh1_2023_09_19_19_25_25_res16x16$\\
%$s=5: ./results/models/FFHQ/l_alpha_3_FFHQ_num_epochs10_bs8_num_soft_thresh5_2023_09_19_19_15_40_res16x16$\\
%$s=10: ./results/models/FFHQ/l_alpha_3_FFHQ_num_epochs10_bs8_num_soft_thresh10_2023_09_19_19_08_42_res16x16$\\
%%%%%%%%%%%%%%%%%%%%%%$s=15: ./results/models/FFHQ/l_alpha_3_FFHQ_num_epochs10_bs8_num_soft_thresh15_2023_09_20_11_44_23_res16x16$\\
%$s=15: ./results/models/FFHQ/l_alpha_3_FFHQ_num_epochs10_bs8_num_soft_thresh15_2023_09_20_05_12_13_res16x16$\\
%$s=20: ./results/models/FFHQ/l_alpha_3_FFHQ_num_epochs10_bs8_num_soft_thresh20_2023_09_20_14_59_25_res16x16$\\
%$s=25: ./results/models/FFHQ/l_alpha_3_FFHQ_num_epochs10_bs8_num_soft_thresh25_2023_09_20_14_56_30_res16x16$\\
%$s=30: ./results/models/FFHQ/l_alpha_3_FFHQ_num_epochs10_bs8_num_soft_thresh30_2023_09_20_15_01_00_res16x16$\\
%./results/models/FFHQ/l_alpha_0.5_FFHQ_num_epochs10_bs8_num_soft_thresh1_2023_09_30_21_38_09_res1x1
%./results/models/FFHQ/l_alpha_0.5_FFHQ_num_epochs10_bs8_num_soft_thresh5_2023_09_30_21_35_27_res1x1
%./results/models/FFHQ/l_alpha_0.5_FFHQ_num_epochs10_bs8_num_soft_thresh10_2023_09_30_21_34_33_res1x1
%./results/models/FFHQ/l_alpha_0.5_FFHQ_num_epochs10_bs8_num_soft_thresh15_2023_09_30_21_31_55_res1x1
%./results/models/FFHQ/l_alpha_0.5_FFHQ_num_epochs10_bs8_num_soft_thresh20_2023_09_30_21_12_16_res1x1
%./results/models/FFHQ/l_alpha_0.5_FFHQ_num_epochs10_bs8_num_soft_thresh25_2023_09_30_21_17_15_res1x1
%./results/models/FFHQ/l_alpha_0.5_FFHQ_num_epochs10_bs8_num_soft_thresh30_2023_09_30_21_23_17_res1x1
%\subsection{Image Classification}
%\noindent
%Image as Set of Points\\
%https://openreview.net/pdf?id=awnvqZja69
% \begin{table}[htbp]
% \centering
% \caption{Quantitative reconstruction results of SC-VAE ($d=8$) with different number of rollout steps $s$.} 
% \resizebox{\linewidth}{15mm}{
% \begin{tabular}{c|c|c|c|c|c}
%   \toprule
%   Model  &  Sparsity & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & rFID $\downarrow$ \\
%   \midrule
%   SC-VAE ($s=1$)  &  $0\%$ & $17.38$ & $0.4966$ & $0.7126$ & $195.20$ \\
%   SC-VAE ($s=5$)  &  $0\%$ & $17.81$ & $0.5062$ & $0.6916$ & $188.01$ \\
%   SC-VAE ($s=10$) &  $84.2\%$ & $17.14$ & $0.4900$ & $0.7302$ & $203.21$ \\
%   SC-VAE ($s=15$) &  $87.6\%$ & $16.99$ & $0.4873$ & $0.7383$ & $211.73$ \\
%   SC-VAE ($s=20$) &  $85.5\%$ & $17.51$ & $0.4967$ & $0.7120$ & $190.15$ \\
%   SC-VAE ($s=25$) &  $88.9\%$ & $17.22$ & $0.4928$ & $0.7183$ & $198.49$ \\
%   SC-VAE ($s=30$) &  $88.6\%$ & $17.10$ & $0.4896$ & $0.7356$ & $207.09$ \\
%   \bottomrule
% \end{tabular}}
% \label{ablation_1x1}
% \end{table}
% \begin{table}[htbp]
% \centering
% \caption{Quantitative reconstruction results of SC-VAE ($d=4$) with different number of rollout steps $s$.} 
% \resizebox{0.8\linewidth}{!}{
% \begin{tabular}{c|c|c|c|c|c}
%   \toprule
%   Model  &  Sparsity & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & rFID $\downarrow$ \\
%   \midrule
%   SC-VAE ($s=1$)  &  $98.6\%$ & $27.30$ & $0.7603$ & $0.3039$ & $71.17$ \\
%   SC-VAE ($s=5$)  &  $86.3\%$ & $31.13$ & $0.8759$ & $0.1279$ & $30.07$ \\
%   SC-VAE ($s=10$) &  $88.0\%$ & $31.05$ & $0.8771$ & $0.1261$ & $29.50$ \\
%   SC-VAE ($s=15$) &  $87.0\%$ & $31.41$ & $0.8815$ & $0.1155$ & $28.11$ \\
%   SC-VAE ($s=20$) &  $91.9\%$ & $30.66$ & $0.8656$ & $0.1451$ & $32.87$ \\
%   SC-VAE ($s=25$) &  $91.5\%$ & $30.59$ & $0.8693$ & $0.1373$ & $30.45$ \\
%   SC-VAE ($s=30$) &  $94.5\%$ & $29.71$ & $0.8369$ & $0.1925$ & $41.89$ \\
%   \bottomrule
% \end{tabular}}
% \label{ablation_16x16}
% \end{table}
\begin{table}[htbp]
\centering
\caption{Quantitative reconstruction results of SC-VAE ($d=4$) with different number of rollout steps $s$.} 
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{c|c|c|c|c|c}
  \toprule
  Model  &  Sparsity & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & rFID $\downarrow$ \\
  \midrule
  SC-VAE ($s=1$)  &  $89.5\%$ & $27.30$ & $0.7603$ & $0.3039$ & $71.17$ \\
  SC-VAE ($s=5$)  &  $71.9\%$ & $31.13$ & $0.8759$ & $0.1279$ & $30.07$ \\
  SC-VAE ($s=10$) &  $75.1\%$ & $31.05$ & $0.8771$ & $0.1261$ & $29.50$ \\
  SC-VAE ($s=15$) &  $74.9\%$ & $31.41$ & $0.8815$ & $0.1155$ & $28.11$ \\
  SC-VAE ($s=20$) &  $80.9\%$ & $30.66$ & $0.8656$ & $0.1451$ & $32.87$ \\
  SC-VAE ($s=25$) &  $81.5\%$ & $30.59$ & $0.8693$ & $0.1373$ & $30.45$ \\
  SC-VAE ($s=30$) &  $83.2\%$ & $29.71$ & $0.8369$ & $0.1925$ & $41.89$ \\
  \bottomrule
\end{tabular}}
\label{ablation_16x16}
\vspace{-3pt}
\end{table}


