% \documentclass[10pt,twocolumn,letterpaper]{article}

% \usepackage{iccv}
% \usepackage{times}
% \usepackage{epsfig}
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}

% \usepackage{tabularx} % extra features for tabular environment
% \usepackage{booktabs}
% \usepackage{subcaption}



% % Include other packages here, before hyperref.

% % If you comment hyperref and then uncomment it, you should delete
% % egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% % run, let it finish, and you should be clear).
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% % \iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{12663} % *** Enter the ICCV Paper ID here
% \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% % Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

% \begin{document}

% %%%%%%%%% TITLE
% \title{SC-VAE: Sparse Coding-based Variational Autoencoder}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

% \maketitle
% % Remove page # from the first page of camera-ready.
% \ificcvfinal\thispagestyle{empty}\fi

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tabularx} % extra features for tabular environment
\usepackage{booktabs}
\usepackage{subcaption}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\usepackage{multicol} % <===============================================


\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{SC-VAE: Sparse Coding-based Variational Autoencoder}

\author{Pan Xiao\qquad Peijie Qiu\qquad Aristeidis Sotiras\\
Department of Radiology, Washington University in St. Louis\\
%St Louis, MO, USA\\
{\tt\small \{panxiao, peijie.qiu, aristeidis.sotiras\}@wustl.edu}}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Peijie Qiu\\
%Washington University in St. Louis\\
%First line of institution2 address\\
%{\tt\small aristeidis.sotiras@wustl.edu}
%\and
%Aristeidis Sotiras\\
%Washington University in St. Louis\\
%First line of institution2 address\\
%{\tt\small aristeidis.sotiras@wustl.edu}
%}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
Learning rich data representations from unlabeled data is a key challenge towards applying deep learning algorithms in downstream supervised tasks. Several variants of variational autoencoders have been proposed to learn compact data representaitons by encoding high-dimensional data in a lower dimensional space. Two main classes of VAEs methods may be distinguished depending on the characteristics of the meta-priors that are enforced in the representation learning step. The first class of methods derives a continuous encoding by assuming a static prior distribution in the latent space. The second class of methods learns instead a discrete latent representation using vector quantization (VQ) along with a codebook. However, both classes of methods suffer from certain challenges, which may lead to suboptimal image reconstruction results. The first class of methods suffers from posterior collapse, whereas the second class of methods suffers from codebook collapse. To address these challenges, we introduce a new VAE variant, termed SC-VAE (sparse coding-based VAE), which integrates sparse coding within variational autoencoder framework. Instead of learning a continuous or discrete latent representation, the proposed method learns a sparse data representation that consists of a linear combination of a small number of learned atoms. The sparse coding problem is solved using a learnable version of the iterative shrinkage thresholding algorithm (ISTA). Experiments on two image datasets demonstrate that our model can achieve improved image reconstruction results compared to state-of-the-art methods. Moreover, the use of learned sparse code vectors allows us to perform downstream task like coarse image segmentation through clustering image patches.

%Instead of learning a continuous or discrete latent space, the proposed sparse coding-based variational autoencoder (SC-VAE) utilized a learnable version of Iterative Shrinkage-Thresholding Algorithm (ISTA) to model the latent representations. SC-VAE can precisely approximate a feature map of an image and represent the image as sparse code vectors. Experiments on two image datasets verify that our model can achieve much better image reconstruction results compared to state-of-the-art methods. Moreover, the use of learned sparse code vectors enables effective clustering of image patches and unsupervised image segmentation.


%Recent advances in variational autoencoders (VAEs) have led to impressive progress in a variety of applications. One type of VAEs acquired a continuous encoding by assuming a static prior distribution in the latent space. Another type of VAEs learned a discrete latent representation using vector quantization (VQ) with a codebook. A key problem, however, is they either suffer from posterior or codebook collapse, leading to suboptimal reconstruction results in the image space. To address this issue, we introduce a new family of VAEs in this work, which combine the sparse coding technique with variational autoencoder framework. Instead of learning a continuous or discrete latent space, the proposed sparse coding-based variational autoencoder (SC-VAE) utilized a learnable version of Iterative Shrinkage-Thresholding Algorithm (ISTA) to model the latent representations. SC-VAE can precisely approximate a feature map of an image and represent the image as sparse code vectors. Experiments on two image datasets verify that our model can achieve much better image reconstruction results compared to state-of-the-art methods. Moreover, the use of learned sparse code vectors enables effective clustering of image patches and unsupervised image segmentation.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
%%%%%%%%% BODY TEXT
%A key challenge towards applying artificial intelligence to the vast amounts of unlabeled image data collected in the world is the ability to learn useful visual representations of data without supervision. These visual representations can facilitate efficient training of downstream tasks \cite{oquab2014learning, simonyan2014very} like image segmentation \cite{long2015fully} and patch clustering \cite{doersch2015unsupervised, chenni2019patch}. Recently, researchers have proposed many unsupervised representation learning methods based on the idea of variational autoencoders (VAEs) \cite{kingma2013auto} where the goal is to learn a mapping from high-dimensional observations to a lower dimensional representation space such that the original observations can be reconstructed (approximately) from the lower-dimensional representation. While these approaches have varying motivations and design choices, we argue that most of the methods either learned a continuous/discrete representation or a combination of them in the latent space.
A major challenge towards applying artificial intelligence to the enormous amounts of unlabeled image data gathered worldwide is the ability to learn effective visual representations of data without supervision. Various unsupervised representation learning techniques based on variational autoencoders (VAEs) \cite{kingma2013auto} have been proposed to address this challenge.
%by researchers based on the idea of variational autoencoders (VAEs) \cite{kingma2013auto}. 
The primary objective of VAEs is to learn a mapping between high-dimensional observations and a lower dimensional representation space, such that the original observations can be approximately reconstructed from the lower-dimensional representation. These lower-dimensional visual representations allow for effectively training downstream tasks, such as image classification  \cite{liu2018data} and clustering \cite{xu2021multi, graving2020vae}. Depending on the downstream task, different VAE variants have been proposed. These are distinguished by the assumptions they make about the worlds, which are encoded as meta-priors \cite{bengio2013representation}. Based on this, two main classes of methods can be identified.


%Although the motivations and design decisions behind these methods vary, we contend that the majority of them either learned a continuous or discrete representation in the latent space.

\begin{figure}[t!]
\centering
\includegraphics[width=8cm]{./Figures/vae-intro4-1.png}
\caption{An illustration of using sparse coding to model the latent repretations of VAEs. Combining the variational autoencoder framework with sparse coding can be conceptualized as representing the middle ground between continuous and discrete VAEs.}
\label{figure:1}
\end{figure}

%One line of research have been focusing on learning representations with continuous features from VAEs \cite{kingma2013auto, higgins2017beta, kim2018disentangling, chen2018isolating, zhao2019infovae, kumarvariational}. These models assume a prior of a gaussian distribution and a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Although these models show good disentanglement results. They suffered from sueveral disadvantages: 1) Fixed prior $\rightarrow$ The mismatch between aggregated posterior and prior. 2)Low reconstruction performance. 3) Poor quality of generated samples. 4) Posterior collapse. 5) Does not continue to yield good disentanglement results for very complex (realistic) datasets.
%Since the first VAE model was proposed, most studies focus on latent representations with continuous features. These models utilized a fixed prior (mostly a gaussian distribution) to regularize the latent space so that they can produce disentangled representations and diverse data can be generated by decoding points sampled from the latent space. Although demonstrating good disentanglement and generation behaviors for simple dataset, they typically suffer from several shortcomings when applied to complex datasets: 1) the fixed prior makes the optimization process troublesome in practice because real world datasets cannot be simply modeled by a single distribution. 2) If the autoregressive decoder is expressive enough to model the data density, then the model can learn to ignore the latent variables, resulting in a trivial posterior that collapses to the prior. posterior collapse 3) These latent variable models typically have simple decoders, where the mapping from the latent variable to the input space is uni-modal, for example using a conditional Gaussian decoder. This typically results in representations that are good at capturing the global structure in the input, but fail at capturing more complex local structure.
The first class of methods \cite{kingma2013auto, higgins2017beta, kim2018disentangling, chen2018isolating, zhao2019infovae, kumarvariational} learns representations with continuous latent variables (hereinafter referred to as continuous VAEs). These models utilized a static prior (mostly a gaussian distribution) to regularize the latent space so that disentangled representations and diverse new data can be generated. These approaches have demonstrated good disentanglement and generation performance for simple datasets \cite{zhao2019infovae, kim2018disentangling}. However, they typically suffer from several shortcomings when applied to complex datasets. First, the static prior makes the optimization process troublesome in practice because real world datasets cannot be simply modeled by a single distribution. Second, these methods tend to ignore the latent variables if the decoder is expressive enough to model the data density, which leads to posterior collapse \cite{razavipreventing}. Third, while the global structure of the input is well captured by latent representations, the more intricate local structure is not, which leads to bad reconstructions.

%Since the first VAEs \cite{kingma2013auto} model was proposed, most studies \cite{kingma2013auto, higgins2017beta, kim2018disentangling, chen2018isolating, zhao2019infovae, kumarvariational} focused on learning latent representations with continuous features (continuous VAEs for short). These models utilized a static prior (mostly a gaussian distribution) to regularize the latent space so that disentangled representations and diverse new data can be generated. Although demonstrating good disentanglement and generation behaviors for simple dataset, they typically suffer from several shortcomings when applied to complex datasets: 1) the static prior makes the optimization process troublesome in practice because real world datasets cannot be simply modeled by a single distribution, 2) these methods tend to ignore the latent variables if the decoder is expressive enough to model the data density, leading to posterior collapse problem \cite{razavipreventing}, 3) the global structure of the input is well captured by latent representations, but the more intricate local structure is not, causing bad reconstructions in the image space.

The second class of methods \cite{van2017neural, esser2021taming, yu2021vector, lee2022autoregressive, zheng2022movq} learns representations with discrete latent variables (hereinafter referred to as discrete VAEs).
 These methods typically utilize vector quantization (VQ) along with a codebook to learn a prior in the latent space. This approach not only circumvents issues of posterior collapse but has also been shown to achieve good image reconstruction and generation performance. Importantly, the perceptual quality of the reconstructed and generated samples  may be further improved through the use of an adversarial or perceptual loss \cite{johnson2016perceptual, larsen2016autoencoding}. However, discrete VAEs often require a large codebook to conserve the information of the encoded observations, which leads to the increase of model parameters and the codebook collapse problem \cite{lee2022autoregressive}. Another shortcoming of most methods \cite{van2017neural, esser2021taming, yu2021vector, lee2022autoregressive} is that they tend to generate repeated artifactual patterns in the reconstructed image because the VQ operator uses the same quantization index to incorporate similar image patches. Moreover, the VQ operator does not allow the gradient to pass through the codebook. This can make the optimization process more challenging and slower, as it requires the use of techniques such as the Gumbel-Softmax trick \cite{jangcategorical} or the straight-through estimator \cite{van2017neural} to approximate the gradients.


%Recent studies \cite{van2017neural, esser2021taming, yu2021vector, lee2022autoregressive, zheng2022movq} have shown that learning representations with discrete features in VAEs (discrete VAEs for short) are a more natural fit for many applications because image patches can often be treated as tokens and text consisting of words is inherently discrete. These methods, which utilized vector quantization (VQ) with a codebook to learn a prior in the latent space, were more flexible compared to continuous VAEs. They not only circumvented issues of posterior collapse but also demonstrated good image reconstruction and generation performance. Adversarial and perceptual loss \cite{johnson2016perceptual, larsen2016autoencoding} were also used to further improve the perceptual quality of reconstructed and generated samples. However, these models often require a huge codebook to conserve the information of the encoded observations, leading to the increase of model parameters and the codebook collapse problem \cite{lee2022autoregressive}. Another shortcoming of most methods \cite{van2017neural, esser2021taming, yu2021vector, lee2022autoregressive} is that they tend to generate repeated artifact patterns in the reconstructed image because the VQ operator uses the same quantization index to incorporate similar image patches. Moreover, the VQ operator does not allow the gradient to pass through the codebook, thus having no guarantee that local minima found by optimization algorithms are ``good enough''.

%To address these shortcomings of the existing work, we introduce a new family of VAEs model in this work. Instead of imposing a fixed prior or using VQ to learn continuous/discrete features, we propose to use sparse coding technique to learn sparse codes in the latent space. Sparse coding, as an alternative to VQ, has consistently yielded better results on benchmark recognition tasks \cite{yang2009linear, boureau2010learning}.
To address aforementioned shortcomings, we introduce a new VAE variant. Instead of using a fixed prior to learn continuous characteristics or utilizing VQ to learn discrete latent variables, we propose to learn sparse linear combinations of atoms using sparse coding (SC) \cite{rubinstein2010dictionaries}. In the case of continuous VAEs, the latent factors assume a prior distribution and thus most of the factors are active at each time. In contrast, discrete VAEs require only one quantization index to be active at each time. SC adopts a middle ground, as is depicted in Figure \ref{figure:1}. 
Each latent representation is reconstructed using a few active atoms out of a codebook.
SC has been shown to enjoy good reconstruction quality and flexibility \cite{wright2010sparse}. Therefore, we hypothesize that combining the VAE framework with sparse coding will lead to better reconstruction for the input.
Traditional algorithms for solving the sparse coding problem, such as Iterative
Shrinkage-Thresholding Algorithm (ISTA) \cite{daubechies2004iterative} and fast Iterative Shrinkage-Thresholding Algorithm (FISTA) \cite{beck2009fast}, can be easily integrated with VAE models. However, these algorithms do not allow to back-propagate gradients, which makes it impossible to train the model in an end-to-end manner. To mitigate this issue, an algorithm unrolling version of ISTA \cite{gregor2010learning} was used in this work.

We term the proposed model SC-VAE, which stands for sparse coding-based VAE. Compared to the existing work, SC-VAE enjoys several advantages. First, it can be trained in an end-to-end fashion and does not suffer from posterior and codebook collapse problems. Secondly, it allows us to obtain better image reconstruction.
Lastly, the latent sparse codes allow us to perform downstream task like coarse image segmentation through grouping image patches.
% To sum up, the contributions of this work are the following:
% \begin{itemize}
%   \item We introduce SC-VAE, a new VAE variant, which integrates the VAE framework with sparse coding.
%   \item The proposed SC-VAE is easy to train and does not suffer from shortcomings of existing works, such as posterior or codebook collapse.
%   \item We show that our approach outperforms previous state-of-the-art methods in image reconstruction tasks. \item The learned sparse codes from SC-VAE allow us to perform image patches clustering and unsupervised image segmentation effectively.
% \end{itemize}


%Disentangled representations are defined as ones where a change in a single unit of the representation corresponds to a change in single factor of variation of the data while being invariant to others (Bengio et al. (2013)). For example, a disentangled representation of 3D objects could contain a set of units each corresponding to a distinct generative factor such as position, color or scale. Most recent work on learning disentangled representations has focused on modeling continuous factors of variation (Higgins et al. (2016); Kim \& Mnih (2018); Chen et al. (2018)). However, a large number of datasets contain inherently discrete generative factors which can be difficult to capture with these methods. In image data for example, distinct objects or entities would most naturally be represented by discrete variables, while their position or scale might be represented by continuous variables. (from "Learning Disentangled Joint Continuous and Discrete Representations")

%In our work, we introduce a new family of generative models succesfully combining the variational autoencoder (VAE) framework with discrete latent representations through a novel parameterisation of the posterior distribution of (discrete) latents given an observation. Our model, which relies on vector quantization (VQ), is simple to train, does not suffer from large variance, and avoids the “posterior collapse” issue which has been problematic with many VAE models that have a powerful decoder, often caused by latents being ignored. Additionally, it is the first discrete latent VAE model that get similar performance as its continuous counterparts, while offering the flexibility of discrete distributions. We term our model the VQ-VAE.

%Since VQ-VAE can make effective use of the latent space, it can successfully model important features that usually span many dimensions in data space (for example objects span many pixels in images, phonemes in speech, the message in a text fragment, etc.) as opposed to focusing or spending capacity on noise and imperceptible details which are often local.

%Lastly, once a good discrete latent structure of a modality is discovered by the VQ-VAE, we train a powerful prior over these discrete random variables, yielding interesting samples and useful applications. For instance, when trained on speech we discover the latent structure of language without any supervision or prior knowledge about phonemes or words. Furthermore, we can equip our decoder with the speaker identity, which allows for speaker conversion, i.e., transferring the voice from one speaker to another without changing the contents. We also show promising results on learning long term structure of environments for RL.

\section{Related Work}
\noindent\textbf{Continuous VAEs.}
%Learning representations with continuous features have been the focus of many previous work.
The variational inference provides a theoretically concrete and elegant formulation of VAEs.   
The most standard VAEs~\cite{kingma2013auto,higgins2017beta,zhao2019infovae} learn a continuous latent representation by transforming a known continuous prior distribution to the real data distribution. Even though it is advantageous for the VAEs to capture modes of the data, the approximate posterior distribution is prone to be different from the real one resulting in crude representations of the seen data.
In addition, the derived representations
 are greatly affected by the problem of posterior collapse.
%Many efforts have focuses on addressing this issue. 
Razavi et al.~\cite{razavipreventing} claimed that the reason behind posterior collapse could be attributed to the inconsistency between the prior distribution and the approximated posterior distribution. To address this issue, $\delta$-VAE \cite{razavipreventing} was proposed, which combined a mean field posterior with a correlated prior.
%the posterior collapse happened when the approximate posterior equals the prior distribution by optimizing the KL divergence, and proposed $\delta$-VAE by combining a mean field posterior with a correlated prior in time to construct sequential latent variables. 
Other efforts \cite{tolstikhin2018wasserstein, An2020AE-OT} have focused on leveraging
%WAE~\cite{tolstikhin2018wasserstein} and AE-OT~\cite{An2020AE-OT} leverage 
the optimal transport theory to provide a better transformation from the prior distribution to the real data distribution. However, the fundamental problem of posterior collapse in VAEs still remains due to the challenge of modeling multi-modal distributions in real data through a preselected prior distribution. 

\noindent\textbf{Discrete VAEs.}
%however we concentrate on discrete representations which are potentially a more natural fit for many of the modalities we are interested in. Language is inherently discrete, similarly speech is typically represented as a sequence of symbols. Images can often be described concisely by language. Furthermore, discrete representations are a natural fit for complex reasoning, planning and predictive learning (e.g., if it rains, I will use an umbrella). While using discrete latent variables in deep learning has proven challenging, powerful autoregressive models have been developed for modelling distributions over discrete variables.
VQVAE~\cite{van2017neural} adopts an alternative approach to circumvent posterior collapse by learning a discrete latent representation. Instead of choosing an encoding distribution, VQVAE learns the prior distribution in the form of a codebook based on vector quantization. Since its introduction, many variants \cite{esser2021taming, yu2021vector, zheng2022movq, lee2022autoregressive} have been proposed with the goal of improving image reconstruction fidelity. For examples, VQGAN~\cite{esser2021taming} aimed to learn a perceptually rich and efficient  codebook by taking advantage of adversrarial training and perceptual loss \cite{johnson2016perceptual, larsen2016autoencoding}.
%takes advantage of adversarial training and perceptual loss \cite{johnson2016perceptual, larsen2016autoencoding} to learn a perceptually rich and efficient codebook.
VIT-VQGAN \cite{yu2021vector} achieved computational efficiency and good reconstructions by combining the strengths of both the Vision Transformer (ViT) \cite{dosovitskiy2020image} and VQGAN \cite{esser2021taming}.
%replaces the CNN encoder and decoder with Vision Transformer (ViT) \cite{dosovitskiyimage} to  yield better computational efficiency on accelerators, and produce higher quality reconstructions. 
RQVAE~\cite{lee2022autoregressive} utilized residual quantization to conserve the encoded information of an image while maintaining a relatively compact codebook. 
Mo-VQGAN \cite{zheng2022movq} introduced a conditional normalization layer, which essentially adds spatially variant information to the discrete representation.
%Mo-VQGAN \cite{zheng2022movq}  introduce a spatially conditional normalization to provide spatially variant information for different locations to avoid reconstructing repeated artifacts in similar nearby regions.
Despite the fact that VQ-based VAEs better accommodate posterior collapse than continuous VAEs, they suffer from the issue of codebook collapse~\cite{dhariwal2020jukebox}. Additionally, the quantization operator does not allow the gradient to pass through the codebook, which is likely to make the optimization process more challenging and slower. 
%However, the VQGAN cannot precisely approximate the feature map of an image without a huge codebook. To mitigate this issue, RQVAE~\cite{lee2022autoregressive} takes advantage of residual quantization to conserve the encoded information of an image while maintaining a relatively compact codebook.
%Though the VQ-based VAEs better accommodate posterior collapse in continuous VAEs, they suffer from the issue of codebook collapse~\cite{dhariwal2020jukebox}. Additionally, the Quantization operator does not allow the gradient to pass through the codebook which is likely to result in a suboptimal solution. 

\noindent\textbf{Sparse Coding and Algorithm Unrolling.}
Sparse Coding is widely used in image restoration tasks~\cite{elad2006image,yang2010image,gu2015convolutional} to learn effective image representations. Sparse coding is typically formulated as an optimization problem that can be solved by K-SVD \cite{aharon2006k},  ISTA \cite{daubechies2004iterative}, FISTA \cite{beck2009fast}, 
 and Coordinate Descent (CoD), etc. However, those iterative optimization algorithms cannot be directly adapted to the neural networks for end-to-end training. To address this issue, Gregor et al.~\cite{gregor2010learning} proposed a learnable version of ISTA (Learnable ISTA) that can be trained by back-propagation. Importantly, algorithm unrolling~\cite{monga2021algorithm,fu2019jpeg,simon2019rethinking,scetbon2021deep} provides a way to learn the iterative algorithms in an end-to-end fashion. In this work, we unrolled the ISTA algorithm to learn sparse coding for the discrete latent variables of a VAE, which will be discussed in detail in Section~\ref{sec:3}.


% \noindent\textbf{Algorithm unrolling.}

\section{Preliminaries}
\label{sec:3}
% \subsection{Vector Quantised-Variational AutoEncoder}
% Suppose that we have an image $x\in \mathbb{R}^{H\times W\times 3}$ , which can be represented by a spatial
% collection of codebook entries $d_\mathbf{q} \in \mathbb{R}^{h\times w\times n}$, where $n$ is the dimensionality of codes. An equivalent representation is a sequence of $h \times w$ indices which specify the respective entries in the learned codebook. 
% Vector Quantised Variational AutoEncoder (VQVAE) \cite{van2017neural} was proposed to effectively learn such a discrete spatial codebook.
% VQVAE is a convolutional model consisting of an encoder $E$ and a decoder $G$. It represents images with codes from a learned, discrete codebook $\mathbf{D} =\{d_{k} \}_{k=1}^{K}\subset \mathbb{R}^{n}$.
% More precisely, a given image $x$ is approximated by $\hat{x} = G(d_{\mathbf{q}})$. $d_{\mathbf{q}}$ can be obtained using the encoding $\hat{d} = E(x) \in \mathbb{R}^{h\times w\times n}$ and a subsequent element-wise quantization $\mathbf{q}(\cdot)$ of each spatial code $\hat{d}_{ij}\in \mathbb{R}^{n}$ onto its closest codebook entry $d_k$:
% \begin{align}
%     d_{\mathbf{q}} = \mathbf{q}(\hat{d}) := \biggl( \underset{d_k\in \mathbf{D}}{\textup{argmin}}||\hat{d}_{ij}-d_k|| \biggr ) \in \mathbb{R}^{h\times w \times n}.
% \end{align}

% The reconstruction $\hat{x}\approx x$ is then given by
% \begin{align}
%     \hat{x} = G(d_{\mathbf{q}}) = G(\mathbf{q}(E(x))).
% \end{align}

% Backpropagation through the non-differentiable quantization operation in Eq. (3) is achieved by a straight-through gradient estimator, which simply copies the gradients from the decoder to the encoder, such that the model and codebook can be trained end-to-end via the loss function:
% \begin{align}
%     \mathcal{L}_{VQ}(E,G,\mathcal{Z}) = ||x-\hat{x}||^2 + ||sg[E(x)]-d_{\mathbf{q}}||_{2}^{2}+ ||sg[d_{\mathbf{q}}]-E(x)||_{2}^{2}.
% \end{align}
% Here, $\mathcal{L}_{\textup{rec}} = ||x-\hat{x}||^2$ is a reconstruction loss, $sg[\cdot]$ denotes the stop-gradient operation, and $||sg[d_{\mathbf{q}}]-E(x)||_{2}^{2}$ is the so-called “commitment loss”.
In this section, we first briefly introduce sparse coding.
%, which is very popular for extracting features from raw data, particularly when the dictionary of basis vectors is learned from unlabeled data.
Then, we describe two optimization algorithms for the sparse coding problem, which include ISTA \cite{daubechies2004iterative} and an algorithm unrolling version of ISTA (Learnable ISTA) \cite{gregor2010learning}.

%\subsection{Sparse Dictionary Learning and LISTA}
\subsection{Sparse Coding}
%In the most popular form of sparse coding, the inference problem is, for a given input vector $X \in \mathbb{R}^n$, to find the optimal sparse code vector $Z^{\star}\in \mathbb{R}^m$ that minimizes an energy function that combines the square reconstruction error and an $L1$ sparsity penalty on the code:
Let $X \in \mathbb{R}^n$ be an input vector,  $Z\in 
 \mathbb{R}^K$ be a sparse code vector and $\mathbf{D}\in \mathbb{R}^{n\times K}$ be a codebook matrix whose columns are atoms.
 The goal is to find an optimal way to reconstruct $X$ based on a sparse linear combination of atoms in $\mathbf{D}$.
Sparse coding typically involves minimizing the following energy function:
\begin{align}
E_{\mathbf{D}}(X,Z) = \frac{1}{2}||X-\mathbf{D}Z||_2^2 + \lambda ||Z||_1.
\label{eq:1}
\end{align}
This energy function comprises two terms. The first term is a data term that penalizes differences between the input vector and its reconstruction as quantified by the $L2$ norm. The second term  is a $L1$ norm, which acts as regularization to induce sparsity on the sparse code vector $Z$.
%where $\mathbf{D}$ is an $n\times K$ codebook matrix whose columns are the basis vectors and
$\lambda$ is a coefficient
controlling the sparsity penalty. 
%The overcomplete case corresponds to $m > n$. 
%The optimal code fora given $X$ is defined as $Z^{*} = \textup{argmin}_{Z} E_{\mathbf{D}}(X, Z)$.


\subsection{ISTA and Learnable ISTA} \label{ISTA and LISTA}
%A popular algorithm for sparse code inference is the Iterative Shrinkage and Thresholding Algorithm (ISTA). Given an input vector $X$, ISTA iterates the following recursive equation to convergence:
%The Iterative Shrinkage and Thresholding Approach is a well-liked algorithm for sparse code inference (ISTA). The following recursive equation is iterated by ISTA until convergence using an input vector $X$:
A popular algorithm for learning sparse codes is called ISTA \cite{gregor2010learning}, which iterates the following recursive equation to convergence:
\begin{align}
Z(t+1) = h_{\theta}(W_{e}X + SZ(t))\quad Z(0) = 0.
\label{eq:2}
\end{align} 
The elements in  Eq.(~\ref{eq:2}) are defined as follows:
\begin{align}
    \textup{filter matrix:} \quad &W_e = \frac{1}{L}\mathbf{D}^{\top} \nonumber\\
    \textup{mutual inhibition matrix:} \quad &S = I - \frac{1}{L}\mathbf{D}^{\top}\mathbf{D}  \nonumber\\
    \textup{shrinkage function:}\quad &[h_{\theta}(V)]_i = \textup{sign}(V_i)(|V_i|- \theta_i)_{+} \nonumber
\end{align}
Here, $L$ is a constant, which is defined as an upper bound on the largest eigenvalue of $\mathbf{D}^{\top}\mathbf{D}$.
%The “backtracking” form of ISTA (not described here) automatically adjusts this constant as part of the algorithm. 
%The other elements in Equation $(5)$ are:
%The filter matrix is created by taking the transpose of the dictionary matrix $\mathbf{D}^{\top}$, divided by the value $L$.
Both the filter matrix and the mutual inhibition matrix depend on the codebook matrix $\mathbf{D}$.
The function $h_{\theta}(V)$ is a component-wise vector shrinkage function with a vector of thresholds $\theta$, where each element in $\theta$ is set to $\frac{\lambda}{L}$. 
%A block diagram of the ISTA algorithm for sparse coding is shown in Fig. \ref{figure:1}(a). The block diagram for the algorithm unrolling version of ISTA is shown in Fig. \ref{figure:1}(b).
In ISTA, the optimal sparse code is the fixed point of $Z(t+1) = h_{\theta}(W_{e}X + SZ(t))$. The block diagram is shown in Figure \ref{figure:2}(a).
%The block diagram of the ISTA and LISTA algorithms are shown in Figure \ref{figure:2}(a) and Figure \ref{figure:2}(b), respectively. 
%In ISTA, the optimal sparse code is the fixed point of $Z(t+1) = h_{\theta}(W_{e}X + SZ(t))$. 
In LISTA, $W_e$, $S$ and $\theta$ are treated as parameters of a time-unfolded recurrent neural network, where $S$ is shared over layers and the back-propagation algorithm can be performed over training samples.  The block diagram of LISTA is shown in Figure \ref{figure:2}(b).

%A block diagram of the ISTA algorithm for sparse coding is shown in Figure 3(a).
%The optimal sparse code is the fixed point of $Z(t+1) = h_{\theta}(W_{e}X + SZ(t))$.
%The diagram for Learnable ISTA is shown in Figure 3(b).
%The LISTA encoder takes the precise form of Equation $5$ with a fixed number of steps $T$. %The pseudo-code for computing a sparse code using LISTA is given in Algorithm 3, and the block diagram in Figure 1(b).
%Learning the parameters $W = (W_e, S, \theta)$ is performed by applying Equation $6$ repetitively over the training samples. %Computing the gradient $dL(W, X^p)/dW$ is achieved through the back-propagation procedure.
%One can view the architecture as a time-unfolded recurrent neural network, to which one can apply the equivalent of back-propagation through time (BPTT). More simply, it can be viewed as a feed-forward network in which $S$ is shared over layers. 
%Computing the gradients consists in starting from the output and back-propagating gradients down to the input by multiplying by the Jacobian matrices of the traversed modules, which is a simple application of chainrule: $\frac{dL}{dZ(t)}=\frac{dL}{dZ(t+1)}\frac{dZ(t+1)}{dZ(t)}$, and $\frac{dL}{dS}=\sum_{t=1}^{T} \frac{dL}{dZ(t)}\frac{dZ(t)}{dS}$, since $S$ is shared across time steps. Similar formulas can be applied to compute $\frac{dL}{d\theta}$ and $\frac{dL}{dW_e}$. 
%The complete back-propagation pseudo-code is given in Algorithm $4$. The $\delta$ prefix denotes the gradient of $L$ with respect to the variable that follows it. Variables and their associated gradients have the same dimensions. $h^{'}_{\theta(t)}$ denotes the jacobian of $h$ with respect to its input (a square binary diagonal matrix).
\begin{figure}[t!]
\centering
\includegraphics[width=8.5cm]{./Figures/ista-lista-1.png}
\caption{(a) The diagram of the ISTA algorithm for
sparse coding. (b) The diagram of the Learnable ISTA, which is a time-unfolded version of the ISTA algorithm.}
\label{figure:2}
\end{figure}


% \subsection{VQ-VAE}
% Suppose that we have an image $x\in \mathbb{R}^{H\times W\times 3}$ , which can be represented by a spatial
% collection of codebook entries $d_\mathbf{q} \in \mathbb{R}^{h\times w\times n}$, where $n$ is the dimensionality of codes. An equivalent representation is a sequence of $h \times w$ indices which specify the respective entries in the learned codebook. 
% %To effectively learn such a discrete spatial codebook, we propose to directly incorporate the inductive biases of CNNs and incorporate ideas from Vector Quantised Variational AutoEncoder (VQVAE) \cite{van2017neural}. 
% Vector Quantised Variational AutoEncoder (VQVAE) \cite{van2017neural} was proposed to effectively learn such a discrete spatial codebook.
% VQVAE is a convolutional model consisting of an encoder $E$ and a decoder $G$. It represents images with codes from a learned, discrete codebook $\mathbf{D} =\{d_{k} \}_{k=1}^{K}\subset \mathbb{R}^{n}$.
% %First, we learn a convolutional model consisting of an encoder $E$ and a decoder $G$, such that taken together, they learn to represent images with codes from a learned, discrete codebook $\mathcal{Z} =\{z_{k} \}_{k=1}^{K}\subset \mathbb{R}^{n_z}$. 
% More precisely, a given image $x$ is approximated by $\hat{x} = G(d_{\mathbf{q}})$. $d_{\mathbf{q}}$ can be obtained using the encoding $\hat{d} = E(x) \in \mathbb{R}^{h\times w\times n}$ and a subsequent element-wise quantization $\mathbf{q}(\cdot)$ of each spatial code $\hat{d}_{ij}\in \mathbb{R}^{n}$ onto its closest codebook entry $d_k$:
% \begin{align}
%     d_{\mathbf{q}} = \mathbf{q}(\hat{d}) := \biggl( \underset{d_k\in \mathbf{D}}{\textup{argmin}}||\hat{d}_{ij}-d_k|| \biggr ) \in \mathbb{R}^{h\times w \times n}.
% \end{align}


% The reconstruction $\hat{x}\approx x$ is then given by
% \begin{align}
%     \hat{x} = G(d_{\mathbf{q}}) = G(\mathbf{q}(E(x))).
% \end{align}

% Backpropagation through the non-differentiable quantization operation in Eq. (3) is achieved by a straight-through gradient estimator, which simply copies the gradients from the decoder to the encoder, such that the model and codebook can be trained end-to-end via the loss function:
% \begin{align}
%     \mathcal{L}_{VQ}(E,G,\mathcal{Z}) = ||x-\hat{x}||^2 + ||sg[E(x)]-d_{\mathbf{q}}||_{2}^{2}+ ||sg[d_{\mathbf{q}}]-E(x)||_{2}^{2}.
% \end{align}
% Here, $\mathcal{L}_{\textup{rec}} = ||x-\hat{x}||^2$ is a reconstruction loss, $sg[\cdot]$ denotes the stop-gradient operation, and $||sg[d_{\mathbf{q}}]-E(x)||_{2}^{2}$ is the so-called “commitment loss”.

\begin{figure*}[h!]
\centering
\includegraphics[width=16cm]{./Figures/SC-VAE10-1.png}
\caption{(a) A schematic representation of the proposed Sparse Coding-VAE (SC-VAE). SC-VAE integrates a Learnable ISTA network within VAE framework to learn sparce code vectors in the latent space for the input image. Each image can be represented as several sparse code vectors. (b) Visualization of the attention network for $\alpha$ estimation.}
\label{figure:3}
\end{figure*}

\section{Approach}
The proposed SC-VAE model aims to encode an image into a series of
latent vector representations and then utilizes sparse coding to generate sparse code vectors for these representations. The generated sparse code vectors can be subsequently decoded back to the original image with a learnable dictionary and a decoder network. The diagram of the proposed model is shown in Figure \ref{figure:3}. We discuss the model formulation in Section \ref{Model Formulation} and the loss functions in Section \ref{Loss Functions}.
%Traditional sparse dictionary learning algorithms like ISTA, fast iterative shrinkage-thresholding algorithm (FISTA) \cite{beck2009fast}, coordinate descent algorithm (CoD) \cite{wright2015coordinate}, K-SVD \cite{aharon2006k}, and orthogonal matching pursuit (OMP) \cite{pati1993orthogonal} can be integrated with VAE models easily. However, it will be time-consuming to optimize the whole model since these traditional algorithms require hundreds or thousands of iterations per forward pass. Moreover, these algorithms do not allow back-propagate gradients, which make it impossible to train the model in an end-to-end manner.
%To mitigate these disadvantages, we integrate the learnable ISTA network discussed in Section \ref{ISTA and LISTA} with VAE so that the whole model can be trained in an end-to-end manner. 

\subsection{Model Formulation} \label{Model Formulation}
 
Formally, the input of SC-VAE is an image $x \in \mathbb{R}^{H \times W \times C}$, where $H$, $W$ and $C$ denote the height, width and the number of image channels, respectively. The image
$x$ goes first through an encoder $E$ to obtain 
 latent discrete representations $E(x) \in \mathbb{R}^{h \times w \times n}$.
Here, the values of $h$ and $w$ depend on the number of downsampling blocks $d$ in the encoder. Accordingly, these are defined based on the equations $h=\frac{H}{2^d}$ and $w = \frac{W}{2^d}$. $n$ denotes the number of dimensions of each discrete latent representation $E_{ij}(x)$, where $i \in [1, h]$ and $j\in [1, w]$. $E_{ij}(x)$ is then given as an input to a Learnable ISTA network. The Learnable ISTA uses a learnable codebook $\mathbf{D}\in \mathbb{R}^{n \times K}$ to produce the sparce code vector $Z_{ij}\in \mathbb{R}^{K}$ for each $E_{ij}(x)$. 
Here, $K$ denotes the number of atoms in the codebook $\mathbf{D}$. The reconstructed latent representations can be calculated by the multiplication of $\mathbf{D}$ and $Z$, which is then used to reconstruct the original image by going through a decoder neural network $G$.
%is the input to a decoder neural network $G$ to reconstruct the original image. 
We denote the reconstructed image as $G(DZ)$.
The design of our convolutional encoder $E$ and decoder $G$ follows the architecture in \cite{esser2021taming}. 
%where its number depends on the number of downsampling blocks $d$ in the encoder.
%$x$ is first input 

%Note that the number of latent discrete representations $E(x)$ depends on the number of downsampling blocks $d$ in the encoder with a downsampling factor $2^d$, where $E(x) \in \mathbb{R}^{h \times w \times n}$. Here $h=\frac{H}{2^d}$, $w = \frac{W}{2^d}$, and $n$ denotes the number of dimensions of each discrete latent representation $E_{ij}(x)$ with $i \in [1, h]$ and $j\in [1, w]$. $E_{ij}(x)$ is then treated as the input to a learnable ISTA network, which uses a trainable dictionary $\mathbf{D}\in \mathbb{R}^{n \times K}$ to produce the sparce code vector $Z_{ij}\in \mathbb{R}^{K}$. 
%Here $K$ denotes the number of atoms in the dictionary.
%The reconstructed latent representations can be easily calculated by $\mathbf{D}Z$, which is the input to a decoder neural network $G$ to reconstruct the original image. 
%We denote the reconstructed image as $G(DZ)$.
%The reconstructed image can be denoted as $G(DZ)$, where $G$ is the decoder network.


%The architecture of our convolutional encoder and decoder models used in the SC-VAE experiments is described in Tab. 7. Note that we adopt the compression rate by tuning the number of downsampling steps $m$. 

\subsection{Loss Functions} \label{Loss Functions}
We need
to define loss functions at two levels: the image level and the latent representation level. The loss in the image level should encourage our model to provide a good reconstruction for the input image. The loss in the latent space should not only allow us to obtain good latent representation reconstruction but also to learn sparse codes, which can reconstruct the latent representations well.


\noindent\textbf{Image reconstruction.} The most common image reconstruction term utilized in VAE models is the $L2$ loss. The $L2$ loss is defined as
\begin{align} \label{eq:4}
    \mathcal{L}_{rec}  &= ||G(\mathbf{D}Z)-x||_2^2.
\end{align}


\noindent
\textbf{Latent representation reconstruction.} 
We aim to learn how to reconstruct each latent represenation $E_{ij}(x)$ based on a linear combination of atoms in the learnable dictionary $\mathbf{D}$.
%We aim to learn a sparse code vector for each latent representation $E_{ij}(x)$ using sparse coding. 
Accordingly, the loss function for the latent representation reconstruction is given by:
\begin{align} \label{eq:4}
    \mathcal{L}_{latent}  = \sum_{ij}||E_{ij}(x) -\mathbf{D}Z_{ij}||_2^2  + \lambda \sum_{ij}||Z_{ij}||_1.
\end{align}
%$Z_{ij}$ denotes the corresponding sparse code vector of a latent representation $E_{ij}(x)$.
Similar to Eq. (\ref{eq:1}), this loss consists of two terms. The first term is a $L2$ norm to penalize differences between the latent representations of input latent representations and their latent representation reconstructions. The second term imposed sparsity to each latent sparse code vector $Z_{ij}$. $\lambda$ controls the sparseness of the learned sparse code vectors $Z$.

%Imporantly, you need to define Z_{ij}. This is currently missing from the paper.


\noindent\textbf{Total loss.} An intuitive way to build the total loss function would be to simply add $\mathcal{L}_{rec}$ and $\mathcal{L}_{latent}$.
However, this loss function will not allow us to learn a good image reconstruction due to the summation term in $\mathcal{L}_{latent}$. 
This is because each input image corresponds to $h\times w$ latent representations. As a consequence, the model will focus on learning good sparse code vectors for these latent representations and pay less attention on  adequately optimizing $\mathcal{L}_{rec}$. 
%To put same emphasis on both image reconstructions and latent representation reconstructions, we introduce coefficient $\alpha_{ij}$ to each of the latent representation $E_{ij}(x)$ and 
To account for this factor, we introduce coefficients $\alpha_{ij}$ to each of the latent representation $E_{ij}(x)$, which allows for appropriately balancing the two terms.
Thus, the total loss for our model is the following:
%Moreover, these algorithms do not allow back-propagate gradients, which make it impossible to train the model in an end-to-end manner.
%To mitigate these disadvantages, we integrate the LISTA framework \ref{ISTA and LISTA} into VAEs so that the whole model can be trained in a end-to-end version. The loss function of the proposed framework is shown below:
\begin{align} \label{eq:6}
    \mathcal{L}_{SC}(E,G,\mathcal{Z})  &= ||G(\mathbf{D}Z)-x||_2^2 \\\nonumber &+ \sum_{ij}\alpha_{ij}||E_{ij}(x) -\mathbf{D}Z_{ij}||_2^2  + \lambda \sum_{ij}||Z_{ij}||_1.
\end{align}
%The first term and second term are the reconstruction loss in the image space and latent space, while the third term is an $L1$ sparsity penalty. 
%Coefficients $\alpha_{ij}$ is used to balance $\mathcal{L}_{rec}$ and $\mathcal{L}_{latent}$.

\vspace{0.2cm}
\noindent
\textbf{$\alpha$ Estimation.}
 The objective function in Eq. (\ref{eq:6}) should put the same emphasis on both image and latent representation reconstructions. Thus, we assume that the weights $\alpha_{ij}$ are non-negative numbers and $\sum_{ij} \alpha_{ij}=1$. This idea is similar to the concept of attention mechanism.
We estimated the weights $\alpha_{ij}$ using an attention network. The architecture of the attention network was designed based on two requirements: 1) $\alpha_{ij}$ should depend on the the residual difference between the discrete latent representation $E_{ij}(x)$ and the reconstructed latent representation $\mathbf{D}Z_{ij}$; 2) The output set $\{\alpha_{ij}\}$ should be equally permuted when the elements in the set $\{E_{ij}(x)-\mathbf{D}Z_{ij}\}$ are permuted. We build the attention network as
%The attention weights were estimated by the residual difference between the discrete latent representations $E(x)$ and the reconstructed latent representations $\mathbf{D}Z$ by an attention network as
\begin{align}
    \alpha_{ij} = F(E_{ij}(x)-\mathbf{D}Z_{ij};\theta_f),
\end{align}
where the attention network $F$ maps a set to another set and is parameterized by $\theta_f$. %Permuting the order of elements in the set $\{E_{ij}(x)-\mathbf{D}Z_{ij}\}$ should equivariantly permute the output set $\{\alpha_{ij}\}$. 
%To ensure $F(\cdot; \theta_f)$ is a permutation equivariant function, we construct it as a neural network with equivariant layer (EL)\cite{zaheer2017deep}.
We construct the network $F(\cdot; \theta_f)$ with equivariant layer (EL) \cite{zaheer2017deep} .
Assuming $\mathbf{H}_{ij} = E_{ij}(x)-\mathbf{D}Z_{ij} \in \mathbb{R}^{n}$, 
%where the latent representation in the $i$-th row and $j$-th column is $E_{ij}(x)-\mathbf{D}Z_{ij}\in \mathbb{R}^{n}$, 
one common way of modeling the EL is
\begin{align} \label{eq:8}
    \mathbf{H}_{ij} = \mathbf{W}(\mathbf{H}_{ij} - \textup{max}(\mathbf{H},2)) + \mathbf{b},
\end{align}
where $\textup{max}(\mathbf{H}, 2)$ is the max over all discrete latent vectors. $\mathbf{W} \in \mathbb{R}^{p\times n}$ and $\mathbf{b} \in \mathbb{R}^p$ are the parameters of the EL. 
Here, $p$ denotes the number of dimension of a EL's output.
The function in Eq. (\ref{eq:8}) is permutation-equivariant. %To ensure $F(\cdot; \theta_f)$ is permutation equivariant we construct it by composing two EL’s. 
We construct  $F(\cdot; \theta_f)$ by composing two ELs to ensure it is
a permutation equivariantly function. 
%Also, we assume that the weights $(\alpha_i)$ are non-negative numbers that sums to $1$. 
We pass the output of the EL to a softmax to obtain a distribution of weights over the discrete latent representations. The architecture of this network is shown in the Fig. \ref{figure:3}(b).

%\textbf{Sparse Coding}: Given a patch (held as a column vector of length ) corrupted by an additive zero-mean Gaussian noise with standard deviation  , we aim to derive its sparse code according to a known dictionary. This objective can be formulated as in Equation (1). An approximate solution to this problem can be obtained by eplacing the -norm with: For a proper choice of, the above can be reformulated as

%A popular and effective algorithm for solving the above problem is the Iterative Soft Thresholding Algorithm (ISTA), which is guaranteed to converge to the global optimum, where c is the square spectral norm of D and is the component-wise soft-thresholding operator,

%The motivation to adopt a proximal gradient descent method, as done above, is the fact that it allows an unrolling of the sparse coding stage into a meaningful and learnable scheme, just as practiced in. Indeed, replacing the 0 -norm by the 1 supports this goal as it allows to differentiate through this scheme. Moreover the iterative formula given by Eq. (9) is operated on each patch, which means that it is just like a convolution in terms of operating on the whole image. Because of these reasons, in this work we consider a learnable version of ISTA by keeping exactly the same recursion with a fixed number of iterations T , and letting c and D become the learnable parameters. Note that we also consider the 0 -based proximal gradient descent where the soft-thresholding is replaced by an hard one which leads to worst results. The smoothness given by the 1 -norm gives a more dense dictionary which deals better to remove the noise.

%\textbf{$\alpha$ evaluation}: We model this contribution by adaptively weighting the patches. The weight indicates the importance of a patch in predicting the overall disease severity of the lung. This idea is similar to the concept of attention mechanism in the Computer Vision and Natural Language Processing communities.

%\subsection{DCT initialization}
%To be filled in.

%\subsection{Semi-supervised learning}
%Our pipeline extends the typical unsupervised pre-train → supervised fine-tune workflow of the self-supervised research community [28, 39], which has recently demonstrated strong performance for low-shot supervised learning [15, 18, 27]. (cite from {Pushing the Limits of Simple Pipelines for Few-Shot Learning: External Data and Fine-Tuning Make a Difference})

%\subsection{computation efficience}

%------------------------------------------------------------------------
\begin{table*}[!htbp] 
\centering
\caption{Quantitative reconstruction results on the validation sets of FFHQ \cite{karras2019style} ($10,000$ images) and ImageNet \cite{deng2009imagenet} ($50,000$ images). Comparison of PSNR, SSIM, LPIPS and rFIDs between ImageNet validation images and their reconstructed images according to the cookbook size
($K$) and the shape of latent codes ($\mathcal{C})$. $\dag$ and $\ddag$ are used to distinguish the same method with different $\mathcal{C}$ or $K$. The top two results across different metrics are highlighted in bold and red, respectively. } 
%\resizebox{0.85\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c|c}
  \toprule
  Model & Dataset & $\mathcal{C}$ & $K$ & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & rFID $\downarrow$\\
  \midrule
  Vanilla VAE \cite{kingma2013auto} &  & $256$ & $-$ & $18.00$ & $0.4960$ & $0.6568$ & $178.17$ \\
  $\beta$-VAE \cite{higgins2017beta} &  & $256$ & $-$ & $16.63$ & $0.4763$ & $0.6878$ & $186.87$ \\
  Info-VAE \cite{zhao2019infovae} &  & $256$ & $-$ & $15.85$ & $0.4560$ & $0.6990$ & $232.37$ \\
  %DALLE$^{\star}$ & & $32 \times 32 \times 1$ & $8192$ & $50.14$ & $29.89$ & $0.91$\\
  VQGAN \cite{esser2021taming} & & $16 \times 16 \times 1$ & $1024$ & $22.24$ & $0.6641$ & $0.1175$ & $4.42$\\
  ViT-VQGAN \cite{yu2021vector} & FFHQ & $32 \times 32 \times 1$ & $8192$ & $-$ & $-$ & $-$ & $3.13$\\
  %VQGAN$^{\S}$ & & $32 \times 32 \times 1$ & $8192$ & $3.66$ & $24.66$ & $0.71$\\
  RQ-VAE$^{\dag}$ \cite{lee2022autoregressive} & & $8 \times 8 \times 4$ & $2048$ & $22.99$ & $0.6700$ & $0.1302$ & $7.04$ \\
  RQ-VAE$^{\ddag}$ \cite{lee2022autoregressive} &  & $16 \times 16 \times 4$ & $2048$ & $24.53$ & $0.7602$ & $0.0895$ & $3.88$ \\
  Mo-VQGAN \cite{zheng2022movq} &  & $16 \times 16 \times 4$ & $1024$ & $26.72$ & $0.8212$ & $\textcolor{red}{0.0585}$ & $\textcolor{red}{2.26}$ \\
  SC-VAE$^{\dag}$ ($\lambda=2$) & & $16 \times 16\times 1$ & $512$ & $\textcolor{red}{31.41}$ & $\textcolor{red}{0.8886}$ & $0.1053$ & $26.71$\\
  SC-VAE$^{\ddag}$ ($\lambda=2$) & & $32 \times 32 \times 1$ & $512$ & $\textbf{38.41}$ & $\textbf{0.9767}$ & $\textbf{0.0022}$ & $\textbf{0.58}$\\
  \midrule
  Vanilla VAE \cite{kingma2013auto} &  & $256$ & $-$ & $17.88$ & $0.4441$ & $0.6957$ & $163.00$ \\
  $\beta$-VAE \cite{higgins2017beta} & & $256$ & $-$ & $16.10$ & $0.4165$ & $0.7142$ & $224.03$ \\
  Info-VAE \cite{zhao2019infovae} &  & $256$ & $-$ & $17.20$ & $0.4259$ & $0.7097$ & $178.94$ \\
  VQGAN$^{\dag}$ \cite{esser2021taming} &  & $16 \times 16 \times 1$ & $1024$ & $19.47$ & $0.5214$ & $0.1950$ & $6.25$\\
  VQGAN$^{\ddag}$ \cite{esser2021taming} &  & $16 \times 16\times 1$ & $16384$ & $19.93$ & $0.5424$ & $0.1766$ & $3.64$\\
  %VQGAN$^{\S}$ &  & $32 \times 32 \times 1$ & $8192$ & $1.21$ & $23.55$ & $0.68$ \\
  ViT-VQGAN \cite{yu2021vector} & ImageNet & $8 \times 8 \times 4$ & $2048$ & $-$ & $-$ & $-$ & $1.28$\\
  RQ-VAE \cite{lee2022autoregressive} &  & $8 \times 8 \times 4$ & $2048$ & $20.76$ & $0.5416$ & $0.1381$ & $4.73$\\
  Mo-VQGAN \cite{zheng2022movq} & & $16 \times 16 \times 4$ & $1024$ & $22.42$ & $0.6731$ & $0.1132$ & $\textcolor{red}{1.12}$\\
  SC-VAE$^{\dag}$ ($\lambda=2$) & & $16 \times 16 \times 1$ & $512$ & $\textcolor{red}{32.88}$ & $\textcolor{red}{0.9125}$ & $\textcolor{red}{0.0698}$ & $7.53$\\
  SC-VAE$^{\ddag}$ ($\lambda=2$) & & $32 \times 32 \times 1$ & $512$ & $\textbf{45.25}$ & $\textbf{0.9938}$ & $\textbf{0.0006}$ & $\textbf{0.01}$\\
  \bottomrule
\end{tabular}%}
\label{Table_1}
\end{table*}

\begin{figure*}[t!]
\centering
\includegraphics[width=16.8cm]{./Figures/image-reconstruction5.png}
\caption{Image reconstructions from different models trained on ImageNet dataset. Original images in the top two rows are from the validation set of ImageNet dataset. Two external images are shown in the last two rows to demonstrate the generalizability of different methods. The numbers denote the shape of latent codes and learned codebook (dictionary) size, respectively. SC-VAE 
improves the reconstructed image quality compared to the baselines. Zoom in to see the details of the red square area.}
\label{figure:4}
\end{figure*}

\section{Experiments}
%We conduct six sets of experiments to evaluate our approach: 1) image reconstruction; 2) clustering ; 3) patches-based retrieval and image retrieval; 4) Image classification and noise-robust image classification;  6) transfer learning - generalization; 7) Swapping Autoencoder for Deep Image Manipulation.
%Besides, the quantitative analysis is also conducted to verify the effectiveness of each compnonent.
We present experimental results for our SC-VAE model on image reconstruction, patch clustering, and patch-based image segmentation. A sparsity parameter analysis was also conducted 
 to see the effect of sparsity penalty $\lambda$ to our model. 
 

\noindent\textbf{Dataset.} For our experiments, we used the Flickr-Faces-HQ (FFHQ) \cite{karras2019style} and ImageNet \cite{deng2009imagenet} datasets. 
%The FFHQ dataset is a collection of high-quality images featuring human faces, which exhibits a significant amount of diversity in terms of the subjects' age, ethnicity, and the backgrounds depicted in the images. 
The FFHQ dataset comprises $70,000$ images with a training set of $60,000$ images and a validation set of $10,000$ images. ImageNet is a widely used benchmark dataset for visual recognition tasks. It consists of $1.2$ million training images and $50,000$ validation images, which are each labeled with one of $1,000$ categories. %The images in the dataset are high-resolution and have a wide variety of objects, scenes, and backgrounds, making it a challenging and diverse dataset for VAEs to model. 
The images in both datasets are high-resolution and diverse, making them challenging for VAEs to model. All images were resized to $256\times 256$ pixels.


\noindent\textbf{Baselines and Evaluation Metrics.} Three continuous VAE (Vanilla VAE \cite{kingma2013auto}, $\beta$-VAE \cite{higgins2017beta}, and Info-VAE \cite{zhao2019infovae}) and four discrete VAE models (VQGAN \cite{esser2021taming}, ViT-VQGAN \cite{yu2021vector}, RQ-VAE \cite{lee2022autoregressive} and Mo-VQGAN \cite{zheng2022movq}) were selected as our baselines. 
%The model architecture used is the same as in the original paper.
We used the same model architectures as the ones described in the respective papers.
%Four most common evaluation metrics including PSNR, SSIM, LPIPS \cite{zhang2018unreasonable} and rFID \cite{heusel2017gans} were used to 
We evaluated the quality between reconstructed images and original images using the four most common evaluation metrics (i.e., Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), Learned Perceptual Image Patch Similarity (LPIPS) \cite{zhang2018unreasonable}, and reconstructed Fréchet Inception Distance (rFID) \cite{heusel2017gans}). PSNR measures the amount of noise introduced by the reconstruction process. SSIM quantifies the similarity between two images by not only taking into account pixel values, but also the structural and textural information in the images. LPIPS and rFID use a pre-trained deep neural network to measure the perceptual distance and distibution distance between two images, respectively.


\noindent\textbf{Implementation Details.} We adopted the encoder and decoder architecture of the VQGAN \cite{esser2021taming}.
The downsampling blocks $d$ of the encoder were set to $3$ or $4$, resulting in $32\times 32$ or $16\times 16$ sparse codes for an input image with a resolution of $256\times 256$.
Moreover, the number of atoms in the dictionary, the unfolded blocks in LISTA and the size of latent vector representations were set to $512$, $5$ and $256$, respectively. 
%The number of atoms in the dictionary and the unfolded blocks in LISTA were set to $512$ and $5$ respectively. 
The overall structure of the attention network $F$ was given by the following expression, in which $[a\times b ]$ symbolizes a multiplication by a matrix of that size: $E_{ij}(x)-\mathbf{D}Z_{ij}\rightarrow [256\times 64]\rightarrow \textup{Sigmoid} \rightarrow [64\times 1] \rightarrow \textup{Softmax} \rightarrow \alpha_{ij}$. The overcomplete
Discrete Cosine Transform (DCT) matrix was used to initialize the dictionary.
To optimize our SC-VAE model, the Adam \cite{kingma2014adam} optimizer was used with an initial learning rate of $10^{-4}$. The training runs for total $10$ epochs and the batch size for each iteration was set to $16$. We saved the models that achieved the lowest total loss for further evaluation. 
More detailed information about the architecture of SC-VAE can be found in the supplementary material.
\\
%GPU Time.\\
%\\
%We trained all the models on $256 \times 256$ images. For image reconstruction, \\

% \begin{figure*}[t!]
%   \centering
  
%   \begin{subfigure}{17cm}
%     \includegraphics[width=17cm]{./Figures/ffhq_cluster-1.png}
%     \label{fig:fig1}
%   \end{subfigure}
  
%   \vspace{0.01cm} % Add some vertical space between the subfigures
  
%   \begin{subfigure}{17cm}
%     \includegraphics[width=17cm]{./Figures/imagenet_cluster-2.png}
%     \label{fig:fig2}
%   \end{subfigure}
%   \caption{Some image patches clusters generated based on clustering the learned sparse code vectors of SC-VAE model. \textbf{Top}: results from the validation set of FFHQ. \textbf{Down}: results from the validation set of ImageNet.}
%   \label{figures:5}
% \end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=16.8cm]{./Figures/clusters_ffhq_15.png}
\caption{$15$ randomly selected image patch clusters generated by clustering the learned sparse code vectors of SC-VAE using the K-means algorithm. Each row represents one cluster. Image patches with similar
patterns were grouped together.}
\label{figure:5}
\end{figure*}

\subsection{Image Reconstruction}
Quantitative experimental results comparing the image reconstruction performance of SC-VAE with the baseline methods are listed in Table 1. We report the performance of our model with two different downsampling blocks ($d=3,4$) and a fixed sparsity penalty $\lambda=2$. The top two results across different metrics are highlighted in bold and red, respectively. As can be seen in the Table 1, SC-VAE significantly improved the image quality compared to other methods when the shape of latent codes was set to $32\times32\times 1$. Even when further down sampling the original image into $16\times16 \times1$, our method outperformed other methods with a large margin in terms of PSNR and SSIM scores in the FFHQ dataset and in terms of PSNR, SSIM and LPIPS scores in the ImageNet dataset. Among baseline methods, continuous VAEs (i.e. Vanilla VAE \cite{kingma2013auto}, $\beta$-VAE \cite{higgins2017beta}, and Info-VAE \cite{zhao2019infovae}) produced poor reconstructions of  the original images. This is because both FFHQ amd ImageNet dataset exhibit a significant amount of diversity in terms of objects and background, which is challenging to be modeled by a static prior distribution in the latent space. Discrete VAEs (i.e. VQGAN \cite{esser2021taming}, ViT-VQGAN \cite{yu2021vector}, RQ-VAE \cite{lee2022autoregressive} and Mo-VQGAN \cite{zheng2022movq}) demonstrated better performance compared to continuous VAEs. However, they produced lower PSNR and SSIM scores compared to our model. That might be because the adversarial training and perceptual loss force the model to generate more visually appealing and realistic outputs without paying sufficient attention to structural information. Moreover, these models use a much larger codebook size than ours. The codebook collapse problem caused by the huge codebook makes it difficult for them to generate detailed and accurate local information.

%We compare MoVQ with the state-of-the-art methods for image reconstruction in Table 1. All instantiations of our model outperform the state-of-the-art methods under the same compression ratio (192x). This includes the concurrent works ViT-VQGAN [45] and RQ-VAE [24], which utilize higher resolution representation and recursively residual representation, respectively. Without bells and whistles, though we use a much smaller number of parameters (82.7M), similar to VQGAN (72.1M), MoVQ outperformers ViT-VQGAN [45], which employs a larger transformer model (599M) on higher resolution representation (32 × 32) for the first stage. The concurrent RQ-VAE work [24] also represents an image with multiple channels by recursively calculating the residual information between quantized vectors and their continuous ones, which requires much more embedding times. Furthermore, the entries in RQ-VAE are not equally important because the residual representation highlights the codevectors in the first round, leaving the other codevectors to capture the small residual information. In contrast, our codevectors share the same significance in all channels, resulting in larger representation capability. More importantly, our model dramatically improves the reconstructed images quality on all metrics, suggesting the reconstructed images are closer to the original inputs, which contributes to more downstream image interpolation tasks.

%Note that, our learned codebook contains only 1024 codevectors with 64 dimensionality, but interestingly outperforms other methods using larger codebook sizes. This suggests that a better quantizer can improve the codebook usage, and it is not necessary to greedily increase the codebook size.
Four images and paired reconstructed results from different models  are visualized in Figure \ref{figure:4}.
%The first two rows display images that have been reconstructed by various models, as well as original images from ImageNet's validation set.
%Four reconstructed images across different models were visualized in Figure \ref{figure:4}. 
The models  that were used to reconstruct these images were trained on the training set of ImageNet dataset. The results from VIT-VQGAN and Mo-VQGAN were not given due to the unavailability of official code and pre-trained models. As is shown in the Figure \ref{figure:4}, the reconstructed results from Vanilla VAE, $\beta$-VAE, and Info-VAE were blurry. However, VQ-GAN, RQ-VAE and our models accomplished visually appealing outcomes. 
%Top two rows show the reconstructed images of different models and original images from the validation set of ImageNet.
The original images in the first two rows were randomly selected from the validation set of ImageNet. VQGAN tended to generate repeated artifacts on similar semantic patches, such as the leaves and hairs. While RQ-VAE enhanced the visual aspect, it struggled to accurately reconstruct intricate details and complex patterns. In contrast to these models, SC-VAE created much more authentic-looking details without suffering from any of the aforementioned shortocomings.
The original images in the last two rows were part of the external images of the ImageNet dataset. They were used to test the generalizability of the different models. SC-VAE was able to accurately identify and reconstruct patterns that had not previously encountered, like eyes, lips, etc. However, the reconstructed images from VQGAN and RQVAE did not preserve
 the detailed information and distorted the orginal images.

%The qualitative results are visualized in Figure \ref{figure:4}. The visualized results from VIT-VQGAN and Mo-VQGAN are not given due to the unavailability of official code and pre-trained models. All compared model are trained on ImageNet dataset. Top two rows shows the reconstructed images of different models and original images from the validation set of ImageNet dataset.


%The qualitative results are visualized in Figure \ref{figure:4}. MoVQ achieves impressive results under various conditions. In Fig. 3, we compare our MoVQ with the baseline model VQGAN [11] and the concurrent model RQ-VAE [24]. VQGAN holds repeated artifacts on the similar semantic patches, such as the grass and trees. RQ-VAE improves the visual appearance, but exhibits systematic artifacts with lossy information. Our MoVQ shows no such artifacts, providing much more realistic details.

%As is shown in the table 7, the proposed model obtains the best image reconstruction results when the shape of code map is $32\times 32 \times 1$ and $\beta=2$. Besides, there are several observations: 1) the reconstructed results increase with the decreasing of $\beta$. That is reasonable since the code vector $Z$ is less sparse with smaller $\beta$, leading to better reconstruction in the latent space; 2) when the shape of code map is set to $16\times 16 \times 1$, our results with $\beta = 1$ are competitive to other models even with much smaller codebook size. 3) when the shape of code map is set to $32\times 32 \times 1$ and $\beta=2$, our models perform much better than the existing models (FID: $0.84$, PSNR: $36.92$, SSIM: $0.97$).

%evaluation metrics. generalization.

%\textbf{Datasets.} To evaluate the proposed method, we instantiated MoVQ on both unconditional and class-conditional image generation tasks, with FFHQ \cite{karras2019style} and ImageNet \cite{russakovsky2015imagenet} respectively. The training and validation setting followed the default one of our baseline model VQGAN \cite{esser2021taming}. We trained all the models on $256 \times 256$ images.
%We evaluate the proposed method on FFHQ \cite{karras2019style} and ImageNet-1K \cite{deng2009imagenet} datasets. Extensive experiments show that our approach consistently achieves significant improvements on image reconstruction. 
%We didn't used Adversiral training but get good reconstruction results. Why are PSNR and SSIM much better?

% \begin{figure*}[t!]
% \centering
% \includegraphics[width=17cm]{./Figures/ffhq_cluster-1.png}
% \caption{Some image patches clusters from the validation set of FFHQ dataset generated by the learned sparse code vectors of SC-VAE model.}
% \label{figure:5}
% \end{figure*}


% \begin{figure*}[t!]
% \centering
% \includegraphics[width=17cm]{./Figures/imagenet_cluster-2.png}
% \caption{Some image patches clusters from the validation set of ImageNet dataset generated by the learned sparse code vectors of SC-VAE model.}
% \label{figure:6}
% \end{figure*}




\subsection{Image Patches Clustering}
%Clustering results compared with other methods and potential applications.
%Overcoming the limitations of patch-based learning to detect cancer in whole slide images.
The latent sparse code vectors learned by SC-VAE can be thought of as compressed representations of the input image. To better interpret the learned sparse code vectors, we aligned each of them with a corresponding patch of the input image. Image patch clustering was then performed based on these sparse code vectors.
We examined one pre-trained SC-VAE models with downsampling block $d=4$ and sparsity penalty $\lambda=2$, which was trained on the training set of the FFHQ dataset. $1,000$ images were randomly selected from the validation set, resulting in $256,000$ pairs of image patches with a resolution of $16\times 16\times 3$ and sparse code vectors with a dimension of $512$. The sparse code vectors were clustered into $1000$ groups using the K-means clustering algorithm. We randomly selected $15$ groups and $40$ patches from each group to visualize the clustering results. As can be seen in Figure \ref{figure:5}, image patches with similar patterns were grouped together. More clustering results and image patch clustering experiments on the validation set of ImageNet can be found in the supplementary material.

\begin{figure}[tbp]
\includegraphics[width=8.4cm]{./Figures/image_segmentation-1.png}
\caption{Unsupervised image segmentation results. \textbf{Top}: images from the validation set of FFHQ. \textbf{Down}: images from the validation set of ImageNet.}
\label{figure:6}
\end{figure}

\subsection{Unsupervised Image Segmentation}
This section investigates if the sparse code vectors acquired from our models enable us to conduct image segmentation tasks without supervision. We utilized two SC-VAE models that were pre-trained on the training set of the FFHQ and ImageNet dataset, respectively. These models had a downsampling block of $d = 3$ and a sparsity penalty of $\lambda = 2$. The images were transformed into sparse code vectors with a size of $32 \times 32 \times 512$ using the encoder of 
 the SC-VAE. Afterwards, the K-means algorithm was applied to cluster these sparse code vectors into $5$ categories, generating a $32 \times 32$ mask with $5$ different classes for each image. The segmentation results are visualized in Figure \ref{figure:6}. The faces and objects in the images were successfully detected and segmented by simply grouping the patch-level sparse codes using the K-means algorithm. 
 Fine-grained segmentation results can be obtained by designing more sophisticated segmentation methods based on our models. In addition, utilizing the learned sparse code vectors for segmentation is significantly more computationally efficient, making it suitable for ultra-high-resolution images.
 Additional segmentation outcomes for both datasets are available in the supplementary material.



\subsection{Sparsity Parameter Analysis}
The objective function of our model contains a single user-specified parameter, i.e., the sparsity parameter $\lambda$. This parameter determines the balance between the level of sparsity and the image reconstruction error. In this section, we explore how the quality of the reconstructed image is affected by varying the values of $\lambda$.
We trained the SC-VAE model on the training set of ImageNet dataset with the downsampling block
$d = 4$ and changing the sparsity penalty $\lambda$
from $1$ to $5$ with an interval of $1$. The quantitative results are reported in Table 2 and the reconstruction of two images randomly selected from validation set of ImageNet dataset are shown in Figure \ref{figure:7}. The results
show that there is a trade-off between sparsity of code vectors and the reconstruction image quality. As $\lambda$ increases, the code vectors become sparser and the reconstruction image quality decreases. 

%that increasing $\lambda$ resulted to obtain sparser code vectors in the latent space. However, it decreases the reconstructed image quality. The reconstructed images gradually lose detailed information and become blurry with larger $\lambda$ values. 
%in Figure 7. The results show that ISTA and FISTA achieve almost the same loss after around epoches. Furthermore, SLSTM consistently produces the smallest loss and outperforms other baselines in all tests.
% \begin{table*}[!htbp]
% \centering
% \caption{$h\times w = 16 \times 16$ .} 
% \begin{tabular}{c|c|c|c|c|c|c|c}
%   \toprule
%   Model & Dataset &  Sparsity & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & rFID $\downarrow$\\
%   \midrule
%   SC-VAE ($\lambda=1$) & &  $0.226$ & $-$ & $-$ & $-$ & $-$\\
%  SC-VAE ($\lambda=2$) &  &  $0.868$ & $-$ & $-$ & $-$ & $-$\\
%   SC-VAE ($\lambda=3$) & FFHQ &  $0.95$ & $-$ & $-$ & $-$ & $-$ \\
%   SC-VAE ($\lambda=4$) & &  $-$ & $0.97$ & $-$ & $-$ & $-$\\
%   SC-VAE ($\lambda=5$) & &  $-$ & $0.984$ & $-$ & $-$ & $-$\\
%   \midrule
%   SC-VAE ($\lambda=1$) &  &  $0.2500$ & $38.0855$ & $0.9702$ & $0.0058$ & $0.6697$\\
%   SC-VAE ($\lambda=2$) &  &  $0.8728$ & $32.8756$ & $0.9125$ & $0.0698$ & $7.5359$\\
%   SC-VAE ($\lambda=3$) & ImageNet &  $0.947$ & $29.9913$ & $0.8441$ & $0.1689$ & $23.3486$\\
%   SC-VAE ($\lambda=4$) & &  $0.9765$ & $28.2194$ & $0.7821$ & $0.2568$ & $47.42247$\\
%   SC-VAE ($\lambda=5$) & &  $0.9863$ & $-$ & $-$ & $-$ & $-$\\
%   \bottomrule
% \end{tabular}
% \end{table*}

\begin{table}[htbp]
\centering
\caption{Quantitative reconstruction results of SC-VAE with different values of $\lambda$.} 
\resizebox{\linewidth}{12mm}{
\begin{tabular}{c|c|c|c|c|c}
  \toprule
  Model  &  Sparsity & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & rFID $\downarrow$\\
  \midrule
  SC-VAE ($\lambda=1$)  &  $0.250$ & $38.09$ & $0.9702$ & $0.0058$ & $0.67$\\
  SC-VAE ($\lambda=2$)  &  $0.873$ & $32.88$ & $0.9125$ & $0.0698$ & $7.54$\\
  SC-VAE ($\lambda=3$)  &  $0.947$ & $29.99$ & $0.8441$ & $0.1689$ & $23.35$\\
  SC-VAE ($\lambda=4$) &  $0.977$ & $28.22$ & $0.7821$ & $0.2568$ & $47.42$\\
  SC-VAE ($\lambda=5$) &  $0.986$ & $27.09$ & $0.7373$ & $0.3152$ & $66.95$\\
  \bottomrule
\end{tabular}}
\end{table}

\begin{figure}[tbp]
\includegraphics[width=8.3cm]{./Figures/img_rec_betas-1.png}
\caption{Image reconstructions of the SC-VAE models with different $\lambda$ values. $\lambda$ ranges from $1$ to $5$ with an interval of $1$ from left to right.}
\label{figure:7}
\end{figure}

%\subsection{Dictionary initialization}
%1. DCT\\
%2. What is the performance of other dictionary initialization methods?

%\subsection{Adversarial Examples}
%PuVAE: A Variational Autoencoder to Purify Adversarial Examples

%\subsection{LISTA vs ISTA}

\section{Conclusion}
In this paper, we proposed a VAE variant, termed SC-VAE.
The proposed method leverages an unrolled sparse coding learning algorithm to obtain latent sparse representations for the input image. The SC-VAE enabled the gradient flow through the dictionary and overcame the shortcomings of existing VAE models. In the experiments, we demonstrated that SC-VAE performed favorably compared to popular baseline methods in image reconstruction on two benchmark datasets. Additionally, 
we demonstrated that similar image patches correspond to similar learned sparse codes. This allowed us to perform unsupervised image segmentation. The demonstrated advantages of SC-VAE compared to standard VAE approaches may enable better performance in downstream tasks, such as object recognition, image segmentation, and image retrieval.

%the learned sparse codes were properly clustered in the latent space based on the semantics of their corresponding image patches. There sparse codes also allows us to perform unsupervised image segmentation. The potential applications of SC-VAE in computer vision and image processing are numerous, including object recognition, image segmentation, and image retrieval.


% \subsection{Image Clustering}
% \subsubsection{Evaluation Metrics}
% Three widely-used clustering metrics including Normalized Mutual Information (NMI), Accuracy (ACC), and Adjusted Rand Index (ARI) are utilized to evaluate our method. Higher values of these metrics indicate better clustering performance.

% cifar10 $\rightarrow$ cifar 100 $\rightarrow$ STL 10 $\rightarrow$ Tiny-Imagenet

% \begin{table*}[htp]
% \centering
% \caption{Clustering on Imagenet validation ($50,000$ images)}
% %\scalebox{1.2}{
% \begin{tabular}{c|c|c|ccc}
%   \toprule
%   Method & $h\times w \times D$ & $K$ & NMI & ACC & ARI\\
%   \midrule
%   VQGAN & $16\times 16$ & $1024$ \\
%   VQGAN & $16\times 16$ & $16384$\\
%   RQVAE & $8\times 8\times 4$ & $2048$\\
%   \midrule
%   SC-VAE & $16\times 16$ & $512$ & $0.4324028$ & $0.0413$ & $0.00210708$\\ 
%   SC-VAE & $32\times 32$ & $512$ & $0.4296364353$ & $0.04092$ & $0.0019337$\\
%   \bottomrule
% \end{tabular}
% %}
% \end{table*}

% \begin{table*}[htp]
% \centering
% \caption{Clustering on cifar-10 using concanated sparse code vectors}
% %\scalebox{1.2}{
% \begin{tabular}{c|c|c|ccc}
%   \toprule
%   Method & $h\times w \times D$ & $K$ & NMI & ACC & ARI\\
%   \midrule
%   K-means & $-$ & $-$ & $0.087$ & $0.229$ & $0.049$ \\
%   Spetral Clustering & $-$ & $-$ & $0.103$ & $0.247$ & $0.085$\\
%   Agglomerative clustering & $-$ & $-$ & $0.105$ & $0.228$ & $0.065$\\
%   \midrule
%   VQGAN & $16\times 16$ & $1024$ \\
%   VQGAN & $16\times 16$ & $16384$\\
%   RQVAE & $8\times 8\times 4$ & $2048$\\
%   \midrule
%   SC-VAE (training set) & $16\times 16$ & $512$ & $0.08882$ & $0.20564$ & $0.04331$\\ 
%   SC-VAE (validation set)& $16\times 16$ & $512$ & $0.09355$ & $0.2023$ & $0.04442$\\ 
%   SC-VAE (training set)& $32\times 32$ & $512$ & $0.09307$ & $0.2126$ & $0.04580162$\\
%   SC-VAE (validation set)& $32\times 32$ & $512$ & $0.09645$ & $0.2113$ & $0.046235$\\
%   \bottomrule
% \end{tabular}
% %}
% \end{table*}

% \begin{table*}[htp]
% \centering
% \caption{Clustering on cifar-10 using mean of latent representations}
% %\scalebox{1.2}{
% \begin{tabular}{c|c|c|ccc}
%   \toprule
%   Method & $h\times w \times D$ & $K$ & NMI & ACC & ARI\\
%   \midrule
%   K-means & $-$ & $-$ & $0.087$ & $0.229$ & $0.049$ \\
%   Spetral Clustering & $-$ & $-$ & $0.103$ & $0.247$ & $0.085$\\
%   Agglomerative clustering & $-$ & $-$ & $0.105$ & $0.228$ & $0.065$\\
%   \midrule
%   VQGAN & $16\times 16$ & $1024$ \\
%   VQGAN & $16\times 16$ & $16384$\\
%   RQVAE & $8\times 8\times 4$ & $2048$\\
%   \midrule
%   SC-VAE (training set) & $16\times 16$ & $512$ & $0.0896968$ & $0.20634$ & $0.04031717$\\ 
%   SC-VAE (validation set)& $16\times 16$ & $512$ & $0.0949239$ & $0.1974$ & $0.0415698$\\ 
%   SC-VAE (training set)& $32\times 32$ & $512$ & $0.0770948$ & $0.18622$ & $0.0319886$\\
%   SC-VAE (validation set)& $32\times 32$ & $512$ & $0.07958$ & $0.1903$ & $0.032898$\\
%   \bottomrule
% \end{tabular}
% %}
% \end{table*}

% \begin{table*}[htp]
% \centering
% \caption{Clustering on Tiny-imagenet}
% %\scalebox{1.2}{
% \begin{tabular}{c|c|c|ccc}
%   \toprule
%   Method & $h\times w \times D$ & $K$ & NMI & ACC & ARI\\
%   \midrule
%   VQGAN & $16\times 16$ & $1024$ \\
%   VQGAN & $16\times 16$ & $16384$\\
%   RQVAE & $8\times 8\times 4$ & $2048$\\
%   \midrule
%   SC-VAE & $16\times 16$ & $512$ & $-$ & $-$ & $-$\\ 
%   SC-VAE & $32\times 32$ & $512$\\
%   \bottomrule
% \end{tabular}
% %}
% \end{table*}

% \subsection{Image Classification}

% \textbf{Linear evaluation on ImageNet} We first evaluate LISTA-VAE representation by training a linear classifier on top of the frozen representation, following the procedure described in [ 48, 74 , 41 , 10 , 8],
% and appendix D.1; we report top-1 and top-5 accuracies in \% on the test set in Table 1. With a
% standard ResNet-50 (×1) BYOL obtains 74.3\% top-1 accuracy (91.6\% top-5 accuracy), which is a
% 1.3\% (resp. 0.5\%) improvement over the previous self-supervised state of the art [ 12 ]. This tightens
% the gap with respect to the supervised baseline of [8], 76.5\%, but is still significantly below the
% stronger supervised baseline of [75 ], 78.9\%. With deeper and wider architectures, BYOL consistently
% outperforms the previous state of the art (Appendix D.2), and obtains a best performance of 79.6\%
% top-1 accuracy, ranking higher than previous self-supervised approaches. On a ResNet-50 (4×) BYOL
% achieves 78.6\%, similar to the 78.9\% of the best supervised baseline in [8] for the same architecture.

%We learn a feature representation on ImageNet ILSVRC \cite{russakovsky2015imagenet}, and compare our method with representative unsupervised learning methods.\\ \textbf{Comparisons.} We compare our method with a randomly initialized network (as a lower bound) and various unsupervised learning methods, including self-supervised learning [2, 47, 27, 48], adversarial learning [4], and Exemplar CNN [3]. The split-brain autoencoder [48] serves a strong baseline that represents the state of the art. The results of these methods are reported with AlexNet architecture [18] in their original papers, except for exemplar CNN [5], whose results are reported with ResNet-101 [3]. As the network architecture has a big impact on the performance, we consider a few typical architectures: AlexNet [18], VGG16 [36], ResNet-18, and ResNet-50 [10].
%We evaluate the performance with two different protocols: (1) Perform linear SVM on the intermediate features from $conv1$ to $conv5$. Note that there are also corresponding layers in VGG16 and ResNet [36, 10]. (2) Perform kNN on the output features. Table 2 shows that:

% \begin{table}[htp]
% \caption{Top 1 and Top 5 classification accuracy on Imagenet}
% \scalebox{0.9}{
% \begin{tabular}{c|c|c}
%   \toprule
%   Method & Top-1 & Top-5\\
%   \midrule
%   VQVAE2\\
%   DALLE\\
%   RQVAE\\
%   VQGAN\\
%   \midrule
%   Ours ($m=4$) \\
%   Ours ($m=8$) \\
%   Ours ($m=16$) \\
%   \bottomrule
% \end{tabular}
% }
% \end{table}

% \begin{table}[htp]
% \caption{Top 1 classification accuracy on Places, based directly on features learned on ImageNet, without any fine-tuning}
% \scalebox{0.7}{
% \begin{tabular}{c|c|c|c|c|c|c|c}
%   \toprule
%   Method & conv1 & conv2 & conv3 & conv4 & conv5 & kNN & dimension\\
%   \midrule
%   Random\\
%   Data-int\\
%   Context\\
%   Adversarial\\
%   Color\\
%   Jigsaw\\
%   Count\\
%   SplitBrain\\
%   Examplar\\
%   \midrule
%   VQVAE2\\
%   DALLE\\
%   RQVAE\\
%   VQGAN\\
%   \midrule
%   Ours ($m=4$) \\
%   Ours ($m=8$) \\
%   Ours ($m=16$) \\
%   \bottomrule
% \end{tabular}
% }
% \end{table}
\newpage
\newpage
\clearpage

% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

\pagebreak
\onecolumn % <==========================================================
%\input{Supplementary_Material.tex}

%%%%%%%%% TITLE
\begin{center}
\textbf{\Large SC-VAE: Sparse Coding-based Variational Autoencoder}\\
\textbf{\Large -}\\
\textbf{\Large Supplemental Materials}
\end{center}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

The supplementary material for our work  \textit{SC-VAE: Sparse Coding-based Variational Autoencoder} is structured as follows:
First, Sec. 1 provides the detailed information of the encoder and decode architecture of the SC-VAE model. 
Additional results on image patches clustering and unsupervised image segmentation on FFHQ and ImageNet datasets are then presented in Sec. 2 and Sec. 3, respectively.

\setcounter{section}{0}
%\renewcommand\thesubsection{\Alph{subsection}}


\section{The Encoder and Decoder Architecture of SC-VAE} \label{section_1_s}
\noindent\textbf{Encoder and Decoder.} 
The architecture of the proposed encoder and decoder is identical to the VQGAN \cite{esser2021taming}, which is described in Table 3. 
$H$, $W$ and $C$ denote the height, width
and the number of image channels, respectively.
$C'$ and $C''$ represent the number of channels of the feature maps that are produced as output by two different intermediate layers of the encoder and decode network.
In our experiment, $C'$ and $C''$ were set to $128$ and $512$, respectively. $n$ denotes the number of dimensions of each discrete latent representation, which was set to $256$.
The variable $d$ represents the number of blocks used for downsampling. Therefore, we can calculate the height ($h$) and width ($w$) of the output feature maps by dividing the height ($H$) and width ($W$) of the input images by $2$ raised to the power of $d$.
\\
%\noindent
%\noindent\textbf{Learnbale ISTA.} The architecture of our Learnable ISTA network is shown in Table 2.
%\noindent\textbf{Attention Network for $\alpha$ Estimation.}  Our neural network architecture follows the backbone of PixelCNN++ [52], which is a U-Net [48] based on a Wide ResNet [72]. We replaced weight normalization [49] with group normalization [66] to make the implementation simpler. Our 32 × 32 models use four feature map resolutions (32 × 32 to 4 × 4), and our 256 × 256 models use six. All models have two convolutional residual blocks per resolution level and self-attention blocks at the 16 × 16 resolution between the convolutional blocks [6].


\begin{table*}[!htbp]
\centering
\caption{High-level architecture of the encoder and decoder of the proposed SC-VAE. Note that $h=\frac{H}{2^{d}}$, $w=\frac{W}{2^d}$.} 
\begin{tabular}{c|c}
  \toprule
  Encoder &  Decoder\\
  \midrule
  $x\in \mathbb{R}^{H\times W\times C} $ &   $\mathbf{D} Z\in \mathbb{R}^{h\times w\times n} $  \\
  2D Convolution $\rightarrow \mathbb{R}^{H\times W\times C'}$ &  2D Convolution $\rightarrow \mathbb{R}^{h\times w\times C''}$  \\
  $d \times$\{Residual Block, Downsample Block\} $\rightarrow \mathbb{R}^{h\times w\times C''}$ &  Residual Block $\rightarrow \mathbb{R}^{h\times w\times C''}$\\
  Residual Block $\rightarrow \mathbb{R}^{h\times w\times C''}$  & Non-Local Block $\rightarrow \mathbb{R}^{h\times w\times C''}$\\
  Non-Local Block $\rightarrow \mathbb{R}^{h\times w\times C''}$ & Residual Block $\rightarrow \mathbb{R}^{h\times w\times C''}$\\
  Residual Block $\rightarrow \mathbb{R}^{h\times w\times C''}$  & $d\times$\{Residual Block, Upsample Block\} $\rightarrow \mathbb{R}^{H\times W\times C'}$\\
  Group Normalization \cite{wu2018group} $\rightarrow \mathbb{R}^{h\times w\times C''}$  & Group Normalization \cite{wu2018group} $\rightarrow \mathbb{R}^{H\times W\times C'}$\\
  Swish Activation Function \cite{ramachandran2017searching} $\rightarrow \mathbb{R}^{h\times w\times C''}$  & Swish  Activation Function \cite{ramachandran2017searching}
    $\rightarrow \mathbb{R}^{H\times W\times C'}$\\
  2D Convolution $\rightarrow E(x) \in \mathbb{R}^{h\times w\times n}$  & 2D Convolution $\rightarrow G(\mathbf{D} Z) \in \mathbb{R}^{H\times W\times C}$\\
  \bottomrule
\end{tabular}
\end{table*}



% \begin{table*}[!htbp]
% \centering
% \caption{High-level architecture of the Learnable ISTA of our SC-VAE. Note that $k$ is the number of the unfolded ISTA block.} 
% \begin{tabular}{c}
%   \toprule
%   Learnable ISTA \\
%   \midrule
%   $E(x)\in \mathbb{R}^{h\times w \times n} $ \\
%   Filter Matrix $\rightarrow \mathbb{R}^{h\times w\times K}$ \\
%   $k\times$\{Shrinkage Function, Mutual Inhibition Matrix, Addition Operator\} $\rightarrow \mathbb{R}^{h\times w\times K}$\\
%   Shrinkage function$\rightarrow Z\in \mathbb{R}^{h\times w\times K}$\\
%   \bottomrule
% \end{tabular}
% \end{table*}

% \begin{table*}[!htbp]
% \centering
% \caption{The architecture of the attention network for $\alpha$ estimation} 
% \begin{tabular}{c}
%   \toprule
%   Attention network \\
%   \midrule
%   $E(x) - \mathbf{D}Z\in \mathbb{R}^{h\times w \times n} $ \\
%   Equivariant layer $1$ $\rightarrow \mathbb{R}^{h\times w\times L}$ \\
%   Equivariant layer $2$ $\rightarrow \mathbb{R}^{h\times w\times 1}$\\
%   \bottomrule
% \end{tabular}
% \end{table*}

\section{Image Patches Clustering}  \label{section2_s}
%Figures \ref{figure:s1} and \ref{figure:s2} contain additional iamge patches clustering results on the FFHQ and ImageNet dataset, respectively. For each dataset, the results are obtained using the respective pretrained SC-VAE model with downsampling block $d = 4$ and sparsity penalty $\lambda = 2$.
Figures \ref{figure:s1} and \ref{figure:s2} exhibit more image patches clustering outcomes for the FFHQ and ImageNet datasets, respectively. These outcomes were obtained utilizing the pre-trained SC-VAE model specific to each dataset, with downsampling block $d=4$ and sparsity penalty $\lambda=2$.
\begin{figure*}[h!]
\centering
\includegraphics[width=16cm]{./Figures/patches_cluster_ffhq_supple_50.png}
\caption{50 randomly selected image patch clusters from the validation set of the FFHQ dataset generated by clustering the learned sparse code vectors of the pre-trained SC-VAE model
using the K-means algorithm. Each row represents one cluster. Image patches with similar patterns were grouped together.}
\label{figure:s1}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=16cm]{./Figures/imagenet_cluster_patches_V3.png}
\caption{50 randomly selected image patch clusters from the validation set of the ImageNet dataset generated by clustering the learned sparse code vectors of the pre-trained SC-VAE model
using the K-means algorithm. Each row represents one cluster. Image patches with similar patterns were grouped together.}
\label{figure:s2}
\end{figure*}

% \begin{figure*}[h!]
% \centering
% \includegraphics[width=16cm]{./Figures/segmentation_ffhq_supple3.png}
% \caption{FFHQ.}
% \label{figure:5}
% \end{figure*}


\section{Unsupervised Image Segmentation} \label{section3_s}
Figures \ref{figure:s3} and \ref{figure:s4} contain additional unsupervised image segmentation results. 
%We utilized two SCVAE models that were pre-trained on the training set of the FFHQ and ImageNet dataset, respectively. These models had a downsampling block of $d = 3$ and a sparsity penalty of $\lambda = 2$. 
We employed two SC-VAE models that had been pre-trained on the training sets of the FFHQ and ImageNet datasets, respectively. These models had a downsampling block with a factor of $3$ and a penalty for sparsity with a value of $2$.
\label{section3}
\begin{figure*}[h!]
\centering
\includegraphics[width=16cm]{./Figures/segmen_ffhq_supple3.png}
\caption{Unsupervised image segmentation results. Images are from the validation set of the FFHQ dataset.}
\label{figure:s3}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=16cm]{./Figures/segmentation_imagenet_supple.png}
\caption{Unsupervised image segmentation results. Images are from the validation set of the ImageNet dataset.}
\label{figure:s4}
\end{figure*}

\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egpaper_arxiv}
}



\end{document}


\pagebreak
\documentclass[10pt,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tabularx} % extra features for tabular environment
\usepackage{booktabs}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi




\begin{document}

%%%%%%%%% TITLE
\title{SC-VAE: Sparse Coding-based Variational Autoencoder\\---\\Supplementary Material}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

The supplementary material for our work  \textit{SC-VAE: Sparse Coding-based Variational Autoencoder} is structured as follows:
First, Sec. \ref{section1} provides the detailed information of the encoder and decode architecture of the SC-VAE model. 
Additional results on image patches clustering and unsupervised image segmentation on FFHQ and ImageNet datasets are then presented in Sec. \ref{section2} and Sec. \ref{section3}, respectively.
%In Sec. B, we present hyperparameters and architectures which were used to train our models. Next, extending the discussion of Sec. 4.3, Sec. C presents additional evidence for the importance of perceptually rich codebooks and its interpretation as a trade-off between reconstruction fidelity and sampling capability. Additional results on high-resolution image synthesis for a wide range of tasks are then presented in Sec. D, and Sec. E shows nearest neighbors of samples. Finally, Sec. F contains results regarding the ordering of image representations.

\section{The Encoder and Decoder Architecture of SC-VAE} \label{section1}
\noindent\textbf{Encoder and Decoder.} 
The architecture of the proposed encoder and decoder is identical to the VQGAN \cite{esser2021taming}, which is described in Table 1. 
$H$, $W$ and $C$ denote the height, width
and the number of image channels, respectively.
$C'$ and $C''$ represent the number of channels of the feature maps that are produced as output by two different intermediate layers of the encoder and decode network.
In our experiment, $C'$ and $C''$ were set to $128$ and $512$, respectively. $n$ denotes the number of dimensions of each discrete latent representation, which was set to $256$.
The variable $d$ represents the number of blocks used for downsampling. Therefore, we can calculate the height ($h$) and width ($w$) of the output feature maps by dividing the height ($H$) and width ($W$) of the input images by $2$ raised to the power of $d$.
\\
%\noindent
%\noindent\textbf{Learnbale ISTA.} The architecture of our Learnable ISTA network is shown in Table 2.
%\noindent\textbf{Attention Network for $\alpha$ Estimation.}  Our neural network architecture follows the backbone of PixelCNN++ [52], which is a U-Net [48] based on a Wide ResNet [72]. We replaced weight normalization [49] with group normalization [66] to make the implementation simpler. Our 32 × 32 models use four feature map resolutions (32 × 32 to 4 × 4), and our 256 × 256 models use six. All models have two convolutional residual blocks per resolution level and self-attention blocks at the 16 × 16 resolution between the convolutional blocks [6].


\begin{table*}[!htbp]
\centering
\caption{High-level architecture of the encoder and decoder of the proposed SC-VAE. Note that $h=\frac{H}{2^{d}}$, $w=\frac{W}{2^d}$.} 
\begin{tabular}{c|c}
  \toprule
  Encoder &  Decoder\\
  \midrule
  $x\in \mathbb{R}^{H\times W\times C} $ &   $\mathbf{D} Z\in \mathbb{R}^{h\times w\times n} $  \\
  2D Convolution $\rightarrow \mathbb{R}^{H\times W\times C'}$ &  2D Convolution $\rightarrow \mathbb{R}^{h\times w\times C''}$  \\
  $d \times$\{Residual Block, Downsample Block\} $\rightarrow \mathbb{R}^{h\times w\times C''}$ &  Residual Block $\rightarrow \mathbb{R}^{h\times w\times C''}$\\
  Residual Block $\rightarrow \mathbb{R}^{h\times w\times C''}$  & Non-Local Block $\rightarrow \mathbb{R}^{h\times w\times C''}$\\
  Non-Local Block $\rightarrow \mathbb{R}^{h\times w\times C''}$ & Residual Block $\rightarrow \mathbb{R}^{h\times w\times C''}$\\
  Residual Block $\rightarrow \mathbb{R}^{h\times w\times C''}$  & $d\times$\{Residual Block, Upsample Block\} $\rightarrow \mathbb{R}^{H\times W\times C'}$\\
  Group Normalization \cite{wu2018group} $\rightarrow \mathbb{R}^{h\times w\times C''}$  & Group Normalization \cite{wu2018group} $\rightarrow \mathbb{R}^{H\times W\times C'}$\\
  Swish Activation Function \cite{ramachandran2017searching} $\rightarrow \mathbb{R}^{h\times w\times C''}$  & Swish  Activation Function \cite{ramachandran2017searching}
    $\rightarrow \mathbb{R}^{H\times W\times C'}$\\
  2D Convolution $\rightarrow E(x) \in \mathbb{R}^{h\times w\times n}$  & 2D Convolution $\rightarrow G(\mathbf{D} Z) \in \mathbb{R}^{H\times W\times C}$\\
  \bottomrule
\end{tabular}
\end{table*}



% \begin{table*}[!htbp]
% \centering
% \caption{High-level architecture of the Learnable ISTA of our SC-VAE. Note that $k$ is the number of the unfolded ISTA block.} 
% \begin{tabular}{c}
%   \toprule
%   Learnable ISTA \\
%   \midrule
%   $E(x)\in \mathbb{R}^{h\times w \times n} $ \\
%   Filter Matrix $\rightarrow \mathbb{R}^{h\times w\times K}$ \\
%   $k\times$\{Shrinkage Function, Mutual Inhibition Matrix, Addition Operator\} $\rightarrow \mathbb{R}^{h\times w\times K}$\\
%   Shrinkage function$\rightarrow Z\in \mathbb{R}^{h\times w\times K}$\\
%   \bottomrule
% \end{tabular}
% \end{table*}

% \begin{table*}[!htbp]
% \centering
% \caption{The architecture of the attention network for $\alpha$ estimation} 
% \begin{tabular}{c}
%   \toprule
%   Attention network \\
%   \midrule
%   $E(x) - \mathbf{D}Z\in \mathbb{R}^{h\times w \times n} $ \\
%   Equivariant layer $1$ $\rightarrow \mathbb{R}^{h\times w\times L}$ \\
%   Equivariant layer $2$ $\rightarrow \mathbb{R}^{h\times w\times 1}$\\
%   \bottomrule
% \end{tabular}
% \end{table*}

\section{Image Patches Clustering}  \label{section2}
%Figures \ref{figure:s1} and \ref{figure:s2} contain additional iamge patches clustering results on the FFHQ and ImageNet dataset, respectively. For each dataset, the results are obtained using the respective pretrained SC-VAE model with downsampling block $d = 4$ and sparsity penalty $\lambda = 2$.
Figures \ref{figure:s1} and \ref{figure:s2} exhibit more image patches clustering outcomes for the FFHQ and ImageNet datasets, respectively. These outcomes were obtained utilizing the pre-trained SC-VAE model specific to each dataset, with downsampling block $d=4$ and sparsity penalty $\lambda=2$.
\begin{figure*}[h!]
\centering
\includegraphics[width=16cm]{./Figures/patches_cluster_ffhq_supple_50.png}
\caption{50 randomly selected image patch clusters from the validation set of the FFHQ dataset generated by clustering the learned sparse code vectors of the pre-trained SC-VAE model
using the K-means algorithm. Each row represents one cluster. Image patches with similar patterns were grouped together.}
\label{figure:s1}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=16cm]{./Figures/imagenet_cluster_patches_V3.png}
\caption{50 randomly selected image patch clusters from the validation set of the ImageNet dataset generated by clustering the learned sparse code vectors of the pre-trained SC-VAE model
using the K-means algorithm. Each row represents one cluster. Image patches with similar patterns were grouped together.}
\label{figure:s2}
\end{figure*}

% \begin{figure*}[h!]
% \centering
% \includegraphics[width=16cm]{./Figures/segmentation_ffhq_supple3.png}
% \caption{FFHQ.}
% \label{figure:5}
% \end{figure*}


\section{Unsupervised Image Segmentation} 
Figures \ref{figure:s3} and \ref{figure:s4} contain additional unsupervised image segmentation results. 
%We utilized two SCVAE models that were pre-trained on the training set of the FFHQ and ImageNet dataset, respectively. These models had a downsampling block of $d = 3$ and a sparsity penalty of $\lambda = 2$. 
We employed two SC-VAE models that had been pre-trained on the training sets of the FFHQ and ImageNet datasets, respectively. These models had a downsampling block with a factor of 3 and a penalty for sparsity with a value of 2.
\label{section3}
\begin{figure*}[h!]
\centering
\includegraphics[width=16cm]{./Figures/segmen_ffhq_supple3.png}
\caption{Unsupervised image segmentation results. Images are from the validation set of the FFHQ dataset.}
\label{figure:s3}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=16cm]{./Figures/segmentation_imagenet_supple.png}
\caption{Unsupervised image segmentation results. Images are from the validation set of the ImageNet dataset.}
\label{figure:s4}
\end{figure*}

\clearpage

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}

