\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multirow}
\usepackage[font=small,labelfont=bf,justification=justified]{caption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{cite}
\usepackage[export]{adjustbox}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{soul}
\usepackage[normalem]{ulem}

\newcommand\Mark[1]{\textsuperscript#1}
\newcommand*{\duy}{\textcolor{blue}}


\title{High Accurate and Explainable Multi-Pill Detection Framework with Graph Neural Network-Assisted Multimodal Data Fusion}


\author{
 Anh Duy Nguyen \\
  VinUni-Illinois  Smart Health Center, \\ VinUniversity, Vietnam \\
  \texttt{duy.na@vinuni.edu.vn} \\
  %% examples of more authors
   \And
 Huy Hieu Pham \\
 College of Engineering \& Computer Science, \\ VinUni-Illinois Smart Health Center, VinUniversity, Vietnam\\
  \texttt{hieu.ph@vinuni.edu.vn} \\
  \And
 Huynh Thanh Trung \\
  École Polytechnique Fédérale de Lausanne,  \\ Switzerland \\
  \texttt{thanh.huynh@epfl.ch} \\
  \And
Quoc Viet Hung Nguyen \\
  Griffith University, \\ Australia  \\
  \texttt{henry.nguyen@griffith.edu.au} \\
  \And
 Thao Nguyen Truong \\
 National Institute of Advanced Industrial Science \\ and Technology, Japan \\
  \texttt{nguyen.truong@aist.go.jp} \\
  \And
 Phi Le Nguyen \\
  School of Information and Communication Technology,  \\ Hanoi University of Science
and Technology, Vietnam\\
  \texttt{lenp@soict.hust.edu.vn}
}

\begin{document}
\maketitle
\begin{abstract}
Due to the significant resemblance in visual appearance, pill misuse is prevalent and has become a critical issue, responsible for one-third of all deaths worldwide. Pill identification, thus, is a crucial concern needed to be investigated thoroughly. 
Recently, several attempts have been made to exploit deep learning to tackle the pill identification problem. 
However, most published works consider only single-pill identification and fail to distinguish hard samples with identical appearances. 
Also, most existing pill image datasets only feature single pill images captured in carefully controlled environments under ideal lighting conditions and clean backgrounds.
In this work, we are the first to tackle the multi-pill detection problem in real-world settings, aiming at localizing and identifying pills captured by users in a pill intake. Moreover, we also introduce a multi-pill image dataset taken in unconstrained conditions. 
To handle hard samples, we propose a novel method for constructing heterogeneous a priori graphs incorporating three forms of inter-pill relationships, including co-occurrence likelihood, relative size, and visual semantic correlation. 
We then offer a framework for integrating a priori with pills' visual features to enhance detection accuracy. 
Our experimental results have proved the robustness, reliability, and explainability of the proposed framework. Experimentally, it outperforms all detection benchmarks in terms of all evaluation metrics. 
Specifically, our proposed framework improves COCO mAP metrics by $\mathbf{9.4\%}$ over Faster R-CNN, and $\mathbf{12.0\%}$ compared to vanilla YOLOv5. Our study opens up new opportunities for protecting patients from medication errors using an AI-based pill identification solution. \\ 
\\
\noindent\textbf{Keywords.} Pill Detection, Graph Neural Network, Explainable AI, Multi-modal Information Fusion. 
\end{abstract}


% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}


\section{Introduction}
\label{sec:introduction}
\noindent \textbf{Background.} 
{Oral pill is one of the most popular and commonly used methods in healthcare due to their efficacy and simplicity.
Pills usually exhibit various visual features in terms of shape, color, and imprinted text.
%Despite this, pill recognition is an error-prone task due to high-similarity in pill appearances (e.g., many pills have white color and round shape). 
Despite this, erroneously taking pills is exceptionally prevalent due to the significant similarity in pill appearances. According to a WHO report\cite{who}, drug misuse rather than illness is responsible for one-third of all deaths. Moreover, according to Yaniv et al.~\cite{yaniv}, around six to eight thousand people are killed annually by prescription errors. Recently, the US National Centers for Biomedical Computing (NCBCs) stated that taking this country alone, each year, $7,000$ to $9,000$ people die due to a medication error. This circumstance necessitates the invention of solutions to protect users/patients from taking incorrect pills. This need is more stringent than ever, given the aging of the global population and the rising prevalence of chronic diseases requiring continuous medication.}

\noindent \textbf{Image-based pill recognition.} 
{In the early stage, pill recognition was handled through a variety of online systems that allow users to identify pills by manually entering multiple attributes, such as shape, color, and imprinted text \cite{PillIdentifier1}. However, these methods are time-consuming and may not be reliable, as the predefined features may not encompass all real-world cases. Recently, Artificial Intelligence (AI) has made tremendous achievements and has emerged as a powerful tool in resolving various problems.
%including healthcare and medicine\cite{rong2020artificial}
%, including object detection and classification \cite{9839383}. 
%has been applied in  in recent years as a result of continuous improvements in data acquisition and the expansion of computational capabilities.
%\cite{}.
% We are experiencing the proliferation of AI in almost every aspect of our lives including healthcare and medicine\cite{rong2020artificial}. More and more, we witness the miracles that AI delivers in diagnosing and treating diseases\cite{litjens2016deep}. 
Although still in its infancy, AI has been used to recognize pills from images, helping prevent incorrect medication. An early effort to classify pills using a Deep Convolution Network (DCN) was introduced in \cite{wong2017development}. 
%In this study, the authors also demonstrated that DCN is superior to classic machine learning techniques such as k-NN in terms of recognition accuracy. 
In \cite{usuyama2020epillid}, the authors provided ePillID, a large pill image dataset comprising $13K$ images representing $8,184$ appearance classes.
Additionally, they conducted experiments to evaluate various baseline models on the proposed dataset.
Even with the best baseline, the experimental findings demonstrated that it fails to discriminate confusing classes.
The problem of few-shot pill recognition was addressed in \cite{ling2020few}. %To this end, the authors proposed a multi-stream deep learning model based on an innovative two-stage training technique.
The authors also provided a new pill image data named CURE. 
Recently, there have been a few works considering the multi-pill detection problem \cite{kwon_pill}. The authors adopted the two-step deep neural networks consisting of an object localization and a classifier. }
% Yu et al., in \cite{yu2014pill}showed that leveraging imprint information may help to boost the pill recognition accuracy. 


\begin{figure}[bt]
    \centering
    % \includegraphics[width=0.8\columnwidth]{hard_diff_size.pdf}
    \subfloat[Pills with similar shapes, colors, and different sizes]{
    \includegraphics[width=0.45\columnwidth]{hard_diff_size.pdf}
    % \includegraphics[width=0.45\columnwidth]{example-image-a}
    % % \caption{Pills examples with similar shape and colors, different in shape}
     \label{fig:hard_case_diff_size}
    }
    % \hspace{5em}
    \hfill
    \subfloat[Pills with similar shapes, sizes and colors]{
    \label{fig:hard_case_all} 
    \includegraphics[width=0.45\columnwidth]{hard_similar.pdf}
    % \includegraphics[width=0.45\columnwidth]{example-image-b}
    % % \caption{Pills examples with same size, shape and colors}
    }
    \caption{\textbf{Hard samples with high similarity in terms of shape, color and size (examples taken from our handcrafted dataset).} The existence of hard examples has rendered the pill identification problem complicated and challenging.\label{fig:hard_case} }
    \vspace{-15pt}
\end{figure}

\noindent  \textbf{Problem statement.} Despite several efforts that have been made, existing solutions for pill identification reveal the following critical shortcomings. 
\begin{itemize}
    \item Most existing works have been restricted to the classification of single-pill images. This constraint limits the solutions' application capacity, as in practice, users usually take multiple pills simultaneously, resulting in multi-pill images in most cases. %Although there are a few recent research tackling multi-pill identification challenges, they all consider only photos captured in ideal lab conditions, with each pill placed separately.
    \item Most of the current pill image datasets (e.g., ePillID, CURE) are limited to single-pill images. Moreover, all of them were collected in tightly-controlled settings under ideal illumination and background conditions, leading to the lack of diversity.
    % , i.e., each image consists of a single pill.  Moreover, to the best of our knowledge, all of these datasets were collected in tightly-controlled settings under ideal illumination and background conditions. Due to these issues, the current datasets lack of diversity and is not representative for real-world data.
    \item No prior work has studied the explainability of the model. This insufficiency diminishes the trustworthiness of the solutions, hence restricting their practical applications.
\end{itemize} 
We are, to the best of our knowledge, the first to tackle the multi-pill detection problem in real-world settings.
Specifically, we focus on a practical application that recognizes pills in patients’ pill intake pictures. Our targeted problem can be formulated as follows. 
\emph{Given an image capturing multiple pills in patients' pill intake, we aim to determine each pill's location and identity. }
In addition to developing a novel pill detection framework with high reliability and explainable capacity, we build a dataset of multi-pill images captured under unconstrained real-world conditions. 

\begin{figure}[bt]
    % \captionsetup{singlelinecheck = false, format= hang, justification=raggedright, font=footnotesize, labelsep=space}
    \centering
    \includegraphics[width=0.8\columnwidth]{overview2.eps}
    \caption{\textbf{Overview of our proposed solution.}
    We leverage external knowledge from prescriptions and training datasets to build the co-occurrence and relative size graph. The visual features of the pills are exploited to construct the visual semantic graph. Using the graph embedding module, the three graphs are transformed into the vector space, then fused with the visual features to provide enhanced feature vectors, which are then utilized to create the final results.
    \label{fig:intro_overview}}
    \vspace{-15pt}
\end{figure}

\noindent \textbf{Our motivation and key ideas.} 
{One of the most significant obstacles in the pill detection problem is the existence of numerous pills with similar shapes, colors, and sizes (Fig.~\ref{fig:hard_case}).
% a high degree of similarity in pill's appearance, i.e., there are numerous pills with similar shapes, colors and sizes (Fig.~\ref{fig:hard_case}).
% Indeed, there are just a few basic pill shapes, such as round, triangle, and oval; therefore, the resemblance in pill shape is inevitable.
% In fact, there are numerous pills with similar shapes, colors and sizes (Fig.~\ref{fig:hard_case}).
%and colors, and the only divergence lying in their sizes (Fig.\ref{fig:hard_case_diff_size}); in other cases, pills may exhibit a high degree of resemblance in all attributes of size, shape, and color (Fig.\ref{fig:hard_case_all}).
We call these hard samples whose occurrence renders the pill identification problem complicated and challenging to solve by generic object detection.
We argue that relying merely on pills' appearance is insufficient to improve pill detection accuracy, if not possible.
We discovered that besides the challenge (e.g., localizing pills in hard cases such as overlapping pills), the multi-pill detection problem, on the other hand, provides us with an opportunity to improve the pill recognition accuracy. 
Motivated by the human tendency and ability to integrate different data sources while making decisions, our proposed solution seeks to utilize external knowledge to improve detection accuracy.
Specifically, we provide a priori graphs incorporating three kinds of pill relationships: co-occurrence, relative size, and visual semantic correlation.
% incorporate two types of external knowledge: pill co-occurrence likelihood and relative pill size.
The first a priori, or co-occurrence graph, demonstrates the frequency with which medications are prescribed for the same diseases; thus, it reflects the likelihood that pills appear in the same image. 
By utilizing this knowledge, we can enhance the accuracy in dealing with hard samples by leveraging the high accuracy in detecting easy samples in the same image.
The second one, i.e., the relative size graph, gives us the relative size information of the pills, thus, improving our model's capacity to distinguish pills of identical shape and color but differing only in size.
Finally, the visual semantic graph learns pills' latent semantic link embedded in pills' visual appearance.
The overview of our proposed model is illustrated in Fig.\ref{fig:intro_overview}.
}
% Medicine are used to cure diseases and improve patients’ health. Medication mistakes, however, may have serious consequences, including diminishing the efficacy of the treatment, causing adverse effects, or even leading to death. As stated in a WHO report, drug abuse rather than sickness accounts for one-third of all deaths \cite{who}. Moreover, according to Yaniv \textit{et al.} \cite{yaniv}, medication errors claim the lives of about six to eight thousand people every year. Recently, US National Centers for Biomedical Computing (NCBCs) states that taking this country alone, each year, $7000$ to $9000$ people die due to a medication error. 
% The situation is also not positive in Vietnam: According to National Center of DI\&ADR \cite{adr}, adverse drug reactions and medication mistakes were reported in $17276$ cases - or $177$ per million persons. 
% To emphasize the significance of taking medication correctly, WHO has chosen the subject Medication Without Harm for World Patient Safety Day 2022~\cite{who}. This is due to the vast diversity of medications and the similarity of pill colors and shapes. 

% \noindent \textbf{Problem statement.} Medication misuse can be due to many reasons, one of which is taking the wrong medicine as prescribed by a physician [cite].
% As a preliminary step toward reducing medication misuse, This paper focuses on the topic of automatically recognizing and identifying pills from images. 
% Our problem, which we refer to as \emph{contextual pill identification}, is stated as follows. Given an image of pills in a single dose, contextual pill detection asks to localize the position and identify the name of the pills.
%Medication errors are caused mostly by the difficulty in manually differentiating tablets due to the vast diversity of medications and the similarity of pill colors and shapes. With this purpose, the Pill Detection problem is demonstrating its tremendous usefulness in aiding people to automatically and precisely identify their pills. 

% \noindent \textbf{Existing approaches and their limitations.}
% Previous publications have proposed solutions to the pill detection challenge. Nonetheless, despite several attempts, this task remains challenging for existing works due to the following factors.
% \begin{itemize}
%     \item All the previous studies dealing with this problem limit their frameworks in recognizing only a single pill per each image. While this approach can prevent occlusions causing by the overlaps of pills, it in turn constraints these frameworks to use only the visual appearance information for recognising the pills. 
%     \item There is no publicly available dataset of pill images that contains various pills appear together. One major reason is that in reality, not all the medications can be taken together, hence an additional source of information is needed to decide which pills should be captured together.
%     \item Just rely on visual appearances of pills, existing works can not distinguish pills that share almost identical shapes, colors. In addition, the produced result is not reliable there are great variances in environmental conditions of environment (lighting, shadow, angle of pill captures, \dots).
% \end{itemize}
{
\noindent \textbf{Our contributions.} Our main contributions are as follows.
\begin{itemize}
    \item We introduce the first real-world multi-pill image dataset consisting of $9,426$ images representing $96$ pill classes. The images were taken with ordinary smartphones in various settings.
    % backgrounds, illumination, camera's view, and zooming in/out conditions. This is the first multi-pill image dataset captured under real-world conditions, as far as we know. 
    The dataset will assist in the advancement of research in the field.
    %For the former, we build the first pill captures dataset that contain the actual medications taken from real prescriptions. With the use of actual prescriptions provided by doctors or pharmacists, we can reliably choose to capture which pills together since they actually prescribed together.
    \item We propose a novel pill detection framework named PGPNet (which stands for a Priori Graph-assisted Pill detection Network), which leverages three-fold graph-based a priori, including co-occurrence likelihood, relative pill size, and visual semantic correlation to tackle hard pill samples. 
    % external knowledge, including co-occurrence likelihood and relative pill size. 
    In addition, we provide a method for constructing these heterogeneous a priori graphs from given prescriptions and the training pill image dataset. Furthermore, we offer a multi-modal fusion method for incorporating graph-based inter-pill relational information with intra-pill visual features to enhance the detection result. 
    %This information is represented in the form of a heterogeneous relational graph. 
    % We introduce a multi-modal fusion method for incorporating graph-based inter-pill relational information with intra-pill visual features, to enhance the final outcomes.
    \item We conduct thorough experiments to evaluate the efficacy of the proposed solution and compare it to existing state-of-the-art (SOTA). The experimental findings demonstrated that our approach enhances the object detection accuracy by at least $9.4\%$ for COCO mAP metric compared to generic SOTA in object detection. 
    %We also perform ablation studies to examine effectiveness of our proposed modules. 
    %The code for reproducing our results will be publicly available upon the publication of this work.
    %\item We perform various methods to investigate the explainability of our prediction results. 
    % We are the first to examine the multi-pill detection challenge, which aims to recognize and localize pills inside a multi-pill image.
    % About the later, this is the very first work that study the problem of detecting many pills at a time - \emph{Contextual Pill Detection}. From the failure of previous works when dealing with \textbf{hard samples} - the pills that share very identical visual appearance with others, we formulate our idea of relying on contextual pills information. Specifically, we design a method to construct a heterogeneous graph representing different kinds of relationship between pills. This knowledge graph is then exploited to improve pill localization and classification accuracy. To address the problem of occlusions and overlaps among pills, we propose the use of classifier weights as the representative characteristics for pill categories - corresponding to node presentations; detail about which will be discussed in Section \ref{sec:method}.
\end{itemize}

The remainder of the paper is divided into four sections. We briefly summarize the literature on pill detection and pill image datasets in Section ~\nameref{sec:related_work}. In Section \nameref{sec:method}, we describe our methodology in detail.
Section \nameref{sec:result} evaluates the performance of our proposed PGPNet and compares it with the other methods. 
Finally, we conclude the paper in Section \nameref{sec:conclusion}.}


\section*{Related Works}
\label{sec:related_work}

\textbf{Pill Classification.}
Many studies have employed machine learning to tackle the pill recognition challenge~\cite{WONG2017130,8962044}. 
%Some standard techniques, such as convolutional neural networks (CNN) and Graph Neural Networks (GNN), are often used.
The authors in~\cite{WONG2017130} first utilized the Manifold ranking-based method to filter out the foreground mask from the input pill image and then used an AlexNet-based network for identifying the label. In~\cite{Ting17}, Ting et al. combined the Enhanced Feature Pyramid Networks and Global Convolution Networks to improve pill localization accuracy. %Besides, the authors leveraged the Xception network~\cite{chollet2017xception} to solve the pill recognition problem. 
% The authors in~\cite{8962044} proposed MedGlasses to help visually impaired chronic patients take their medications correctly.
%. To this end, they proposed a MedGlasses system combining AI and IoT. 
%In addition, several attempts have been made to increase the accuracy of pill detection by including handmade characteristics like color, shape, and imprint. 
Ling {et al.}~\cite{Ling_2020_CVPR} tackled the few-shot pill detection problem with a Multi-Stream (MS) deep learning model. 
%In addition, they offered a two-stage training technique to solve the data scarcity constraint; the first stage is to train with all samples, while the second concentrates only on the hard examples. 
In~\cite{Proma19}, the authors integrated three handcrafted features, namely shape, color, and imprinted text, to identify pills. 
%Specifically, the authors initially used statistical measurements from the pill's histogram to estimate the number of colors in the pill. Afterward, the text imprinted on the pill was retrieved using text recognition techniques. The author also used the decision tree technique to determine the pill shape. The color, shape, and imprinted text information are then used as input features to train the classification model.
% The primary limitation of the studies mentioned above is that they only investigate photos of a single pill. Moreover, most of them solely work with datasets collected in laboratories under ideal conditions. %These constraints have limited the applicability and generalizability of the proposed methods.


%\textbf{Multi-pill Detection.}
Recently, a few efforts have leveraged the two-stage object detection approach to solve the multi-pill detection challenge~\cite{Ou2020, kwon_pill}. 
% These works leveraged the two-stage object detection approach to solve the problem. 
In the first stage, object localization techniques are applied to determine the pills' bounding boxes. 
These bounding boxes are then fed into a classifier in the second stage to identify the pills.
Specifically, in \cite{Ou2020}, an enhanced feature pyramid network based on the ResNet-50 backbone has been built for pill localization. 
After that, the pill bounding boxes are fed into an Inception-ResNet v2 for classification. 
Authors in \cite{kwon_pill} exploited the Mask-RCNN framework to solve the problem.

Multi-pill detection solutions are still in their infancy.
All current works only investigate images acquired in laboratories under optimal lighting and background conditions, with each pill arranged separately.
In fact, existing techniques only use specific object localization models to crop the pills and then treat the issue as a typical single-pill classiﬁcation problem.

{\textbf{Pill Image Datasets.} One of the most widely used pill image datasets is NIH Pill Image Dataset \cite{nih_dataset} released by the U.S. National Library of
Medicine (NLM). This dataset consists of 4,000 high-quality reference pills and 133,000 pictures captured by digital cameras on mobile phones.
% two image types: Reference Photos and Consumer quality. 
% The former comprises more than 4,000 high-quality reference pill pictures for 2,000 drug classifications.
% %Each pill class has one image for the front side and one image for the back side of the pill. 
% These images were captured under the same laboratory conditions and using a high-resolution camera.
% The consumer Grade images consist of more than 133,000 pictures captured by digital cameras on mobile phones.
% For both types, the images are captured on a clean background and from a top-down view.
In \cite{ling2020few}, the authors provided the CURE pill dataset consisting of 8,973 single-pill images representing 196 classes. 
% Similar to the NIH dataset, CURE also comprises reference and customer images.
%The images a taken using three mobile phones (i.e., Samsung SM-J320FN, SM-N920S, and LG F500L) mounted on a Raspberry Pi 3 set above the pill holder. 
Although taken under various backgrounds and lighting conditions, all of these images are carefully captured from a top-down view and focus on the pills.
Authors in \cite{wong2017development} contributed a pill dataset capturing about 400 commonly used tablets and capsules.
Ten to twenty-five pictures were taken for each pill, resulting in 5,284 images. 
%The pills were placed on a reference board, and images were taken using two mobile devices at 72 pixels/inch resolutions. 

Unfortunately, all of these datasets provide only single-pill images.
Most images were captured under quite ideal conditions, e.g., pills were put on a clean background, and the images were taken from the top-down view with the camera focused on the pills.} %illustrates some samples randomly taken from aforementioned datasets.

% COMMENT OUT SINCE COPYRIGHT
% \begin{figure}
%     \centering
%     \begin{subfigure}[t]{0.33\columnwidth}
%         \centering
%         % \includegraphics[width=1\columnwidth]{nih.eps}
%         \caption{Samples of NIH dataset~\cite{nih_dataset}.
%         %All images were captured from the top-down view, on a clean background, and under ideal lighting conditions.
%     }
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.3\columnwidth}
%         % \includegraphics[width=\columnwidth]{cure.eps}
%         \caption{Samples of CURE dataset~\cite{ling2020few}.
%        % All images were captured from top-down view, on a clean background. 
%     }
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.315\columnwidth}
%         % \includegraphics[width=\columnwidth]{other_dataset.eps}
%         \caption{Samples of dataset provided by \cite{wong2017development}.
%         %All images were captured on the same background.
%         }
%     \end{subfigure}
%     \caption{\label{fig:related_dataset}\textbf{Samples of pill images datasets.} 
%     All of them provide only single-pill images, which are captured in carefully controlled environments. For example, all images in NIH and CURE datasets were captured from the top-down view, on clean backgrounds, while images in ~\cite{ling2020few} were taken on the same board.
%     }
%     \vspace{-16pt}
% \end{figure}

% Despite numerous efforts, pill detection remains problematic for current existing works. Firstly, all the previous studies dealing with this problem limit their frameworks in recognizing only a single pill per each image. This makes their frameworks not be highly applicable in most cases, in which patients have to take more than one pill at a time. In addition, to the best of my knowledge, there is no publicly available dataset of pill images that contains various pills in a single images, which can be a great hindrance for existing works. Secondly, pill misidentification often occurs with tablets that look substantially similar. Figure \ref{fig:ill_predicted} shows some of the misclassification results made by a deep learning model. It can be seen that the existing frameworks, which just rely on visual appearances of pills, can not distinguish pills that share almost identical shapes, colors. The situation is even worse when taking into account other conditions of environment such as variance of light, shadow, angle of pill captures.



\section*{Methodology}
\label{sec:method}

\begin{figure*}[bth]
    % \captionsetup{singlelinecheck = false, format= hang, justification=raggedright, font=footnotesize, labelsep=space}
    \centering
    \includegraphics[width=1\textwidth]{flow.eps}
    \caption{ \textbf{Workflow of PGPNet.}
    First, the \textbf{A Priori Graph Modeling} Module leverages given prescriptions to create a non-directed Medical Co-occurrence Graph $\mathcal{G}_c$ and then leverages $\mathcal{G}_c$ in conjunction with bounding box annotation information from the pill image dataset to construct a Relative Size Graph $\mathcal{G}_s$.
    %The Co-occurrence graph indicates the co-occurrence likelihood across pill classes, whereas the size graph models the relationship between pills in terms of their size. 
    Second, the pill images are passed through the \textbf{Visual Feature Extractor} to determine Regions of Interest and retrieve their visual representations. This information, together with the graphs serve as the input for the \textbf{Inter-Pill Relational Feature Extractor} to generate a heterogeneous relational graph expressing three types of relationships between pills, namely, co-occurrence, relative size, and visual semantic relation.
    % The outputs of these two steps, together with RoIs' visual features, serve as the input for the \textbf{Inter-Pill Relational Feature Extractor} to generate a heterogeneous relational graph expressing three types of relationships between pills, namely, co-occurrence, relative size, and visual semantic relation.
    Finally, the heterogeneous graph-based information and visual features are combined by \textbf{Multi-modal Data Fusion} module to form final enhanced vectors, which will be used to determine the final detection results.\label{fig:generalflow}.
    \vspace{-15pt}
    %relational graph is converted into vector space by a Graph transformer network to obtain context vectors of RoIs. These context vectors are then concatenated with RoIs' visual features to form final enhanced vectors, which will be used to determine the final detection results.\label{fig:generalflow}
     }
    %\vspace{-10pt}
\end{figure*}

In this section, we propose a novel pill detection framework named PGPNet (i.e., a \textbf{P}riori \textbf{G}raph-assisted \textbf{P}ill Detection \textbf{Net}work). 
% In Section \ref{sec:method_overview}, we introduce the primary components of the PGPNet. Section \ref{sec:method_graph_model} describes how to construct a priori graphs from a given set of prescriptions and pill images, and Section \ref{sec:method_visual_extractor} presents the process for extracting pills' visual features.
% Given the pill visual features and a priori graphs, we propose a mechanism to generate inter-pill relational graphs in Section \ref{sec:method_a_module}.
% The inter-pill relational information and intra-pill visual feature are then combined by the Multimodal information fusion block described in Section \ref{subsec:method_g_module}. 
% Finally, Section \ref{sec:method_loss} represents our proposed auxiliary loss which improves the prediction accuracy. 

\subsection*{PGPNet Overview}
\label{sec:method_overview}
%As described in the preceding sections, we examine the pill detection problem in this study. Specifically, 
% We focus on a practical application that recognizes pills in
% patient intake pictures.
%Our proposed model is shown in Figure \ref{fig:generalflow}. 
% As the input, our model gets an image of multiple pills and produces both the bounding box and the identity of each pill.
% Here, we incur a critical challenge which is \emph{how to distinguish pills with identical appearances (i.e., shape, color, and size)?}.
% We believe that relying solely on the visual features of pills is insufficient to address this issue.
% Moreover, we observe that employing the correlation between pills, rather than counting on each pill individually, may enhance the recognition accuracy.
% In light of this, we propose introducing two types of a priori, the first indicating the co-occurrence likelihood and the second modeling the relative size of pills.
% The a-priori is extracted from a given prescription and pill image training dataset and represented as heterogeneous graphs.
% In summary, the proposed model comprises four components: \textbf{A priori graph modeling}, \textbf{visual feature extrator}, \textbf{inter-pill relational feature extractor}, and \textbf{multi-modal data fusion}, as illustrated in Fig.\ref{fig:generalflow}.

{
We focus on a practical application that recognizes pills in
patient intake pictures. Our model receives a multiple-pill picture as input and generates both the bounding box and the identification of each pill. Here, we incur a critical challenge: \emph{how to distinguish pills with identical appearances (i.e., shape, color, and size)}. We believe that relying solely on the visual features of pills is insufficient to address this
issue. Moreover, employing the correlation between pills, rather than counting on each pill individually, may enhance recognition accuracy. In light of this, we propose introducing two types of a priori, the first indicating the co-occurrence likelihood and the second modeling the relative size of pills. The a-priori is extracted from a given
prescription and pill image training dataset and represented
as heterogeneous graphs. In summary, the proposed model
comprises four components: \textbf{A priori graph modeling}, \textbf{visual
feature extractor}, \textbf{inter-pill relational feature extractor}, and
\textbf{multi-modal data fusion}, as illustrated in Fig.\ref{fig:generalflow}. The overall
flow is as follows.
}

% To increase the identification accuracy, we utilize an external knowledge acquired from a specific set of prescriptions in addition to the relative size information extracted from annotations in the training dataset. The intuition behind our proposal is that by utilizing a large number of prescriptions, we may learn the co-occurrence likelihood of the pills, as well as their relative size information, thereby, improve pill detection accuracy.
% As illustrated in Figure~\ref{fig:generalflow}, the proposed model comprises three major components: \textbf{graph modeling}, \textbf{visual processing} and \textbf{graph processing}.
% The first component, a priori graph modeling, is responsible for constructing two generic graphs that model the occurrence correlation and relative sizes of all the pills.
% The visual feature extractor captures the appearance attributes of pills in the input image.
% In the meantime, the inter-pill relational feature extractor receives as the input the two generic graphs and visual features generated by the first two modules to generate a condensed graph highlighting the features of only those pills likely to occur in the input image.
% The multi-modal data fusion block then incorporates the visual characteristics retrieved by the second module and the graph-based relational data produced by the third module to generate the final result.
% In the following, we describe the overview of these four components. 
% graph processing module is responsible for integrating the graph-based and visual-based information and generating the final output.
% graph processing module attempts to depict the relationship between the pills and then combines the visual characteristics of the pills with their graph-based features to generate the final localization and classification decision. 
\begin{itemize}
    \item [$\bullet$] \textbf{Step 1 - A Priori Graph Modeling}.
    {We construct two generic graphs, namely \emph{Prescription-based Medical Co-occurrence Graph} (or Co-graph for
short) and \emph{Relative Size Graph} (Size-graph for short) that represent
the relationship between all the pills in terms of co-
occurrence and relative size, respectively. Concerning the
former, we leverage a given set of prescriptions from
which we can model the interaction between pills (i.e.,
which pills are likely to be used to treat the same diseases).
Based on this information, we developed the Co-graph, whose nodes represent the pill classes and whose
edge weights reflect the co-occurrence likelihood between
the two vertices. In the meantime, using the coordinates
of the bounding boxes from our training dataset for the
pill detection task, we determine the area of each box
and model the relative size ratios of all the pill classes in
the given images. This information is then aggregated to
formulate the Size-graph. Section~\nameref{sec:method_graph_model} covers the details of this algorithm.}
    % which pills 
    % Specifically, a graph from a given set of prescriptions, with nodes representing pills and edges reflecting drug linkages. We name this graph the \textit{Prescription-based Medical Co-occurence Graph}, or PMCG for short. Following, with the bounding boxes' coordinates information from our training dataset, we can calculate the area of each box, and model the relative size ratios of all the pills in the given images. This information is then aggregated to formulate \textit{Relative Size Graph} (RSG in short). The detailed formulation algorithm is presented in Section~\ref{sec:method_graph_model}.
    % The PMCG is then passed through a Graph Neural Network (GNN) to yield embedding vectors. Each embedding vector conveys information about a node and its relationship to the neighbors. The detailed algorithm is presented in Section~\ref{subsec:graph_modeling}.
    \item [$\bullet$] \textbf{Step 2 - Visual Feature Extraction}. {The original image containing multiple pills is passed through a Convolutional Network (ConvNet) for extracting visual features and a Region Proposal Network (RPN) for detecting potential Regions of Interest (RoI). The outputs of the two modules are fed into an RoI pooling layer to filter out all visual presentations of pills (i.e., RoIs). It is worth noting that the \textit{Visual Feature Extractor} described here follows the architecture of the two-step object detection architecture (e.g., Faster RCNN \cite{fasterrcnn}). However, PGPNet can also be implemented with one-step detection architecture.}
    %, and our experiment with YOLO-v5 backbone \cite{yolov5}, a representative one-step detection model, is covered in Section \ref{subsec:eval_backbones}.
    \item [$\bullet$] \textbf{Step 3 - Inter-pill Relational Feature Extraction}.
    {The two a priori graphs are aggregated with the pills' visual features to yield condensed versions of the Co-graph and Size-graph that highlight the relationship between only those pills that are likely to appear in the image. Besides, the pills' visual features are leveraged to construct a so-called \emph{Visual semantic graph} that captures the pills' relationships encapsulated under their appearances.}
    % The pills' visual features, along with two generic graphs will then be passed through the \textit{Inter-pill Relational Feature Extractor} to generate three condensed graphs that highlight relationship between only pills likely to appear in the image. 
    \item [$\bullet$] \textbf{Step 4 - Multi-modal Data Fusion}.
    % the beneficial context vectors. On the one hand, the visual features will be fed into \textit{adaptive graph module} to produce image-oriented graphs. On the other hand, these features are put in a \textit{feature encoder module}. The adaptive graph module's goal is to softly decide on each RoI's label so that two general graphs can be mapped into adaptive ones that are specifically designed for the input images. For the feature encoder, its output is then leverage to build up another visual-based graph - the third graph along with two mapped graphs. 
    {Now, the inter-pill relational and intra-pill visual features are fused to obtain enhanced feature vectors, each of which encapsulates the characteristics of a pill standalone and its relationship with other pills. These enhanced feature vectors are used to offer the final results.}
    %Specifically, the three condensed graphs obtained from the third step are transformed by a graph transformer network to generate context vectors for the RoIs. These context vectors are then concatenated with the RoIs' visual features acquired in the second step to produce the enhanced feature vectors, which are used to offer the final results.  
    % These three graphs, representing three different relations are got transformed by \textit{Graph Transformer Network} to generate best context vectors for each RoIs.
    % \item [$\bullet$] \textbf{Step 4 - Final predictions with enhanced features}. The RoI visual features acquired in Step 2 and the context embedding vector retrieved in Step 3 will be concatenated. The context vectors take the roles of being the presentations for the neighboring pills. By observing both visual features as well as the neighborhood context, the final Classifier module are then able to produce the final prediction results. 
    % The details of my losses functions are presented in Section~\ref{subsec:loss}. 
\end{itemize}
% The procedure covered in Step 1 is only applied for the first installation of our framework and can be reused afterwards. Hence, our use of external knowledge cause little overheads in model complexity as well as computational resource, while making a great enhancement in performance. 

\subsection*{A Priori Graph Modeling}
\label{sec:method_graph_model}
% To start off, I will discuss about the first step in the working flow of PGPNet, which is graph modeling procedure. 
{In this section, we describe our method to construct the two generic graphs, namely the co-occurrence Graph (i.e., Co-graph) and relative size graph (i.e., Size-graph) in Sections \nameref{subsec:method_coocurent_modeling} and \nameref{subsec:method_size_modeling}, relatively.}
% The central idea of my proposed methodology is to use external information to improve the precision of the focused task, namely, pill detection. The first realization is based on the link between medications as shown by their respective prescriptions. A prescription-based medical co-occurrence graph (PMCG) is developed for this purpose. In genuine pill captures, we suspect that all medications are given to treat or mitigate certain ailments or symptoms. Therefore, this implicit relationship can be established by examining the direct connections between medications and diseases. This information is included on prescriptions given to patients by pharmacists. Subsection \ref{subsec:method_coocurent_modeling} describes the formulation of PMCG in depth. For the second source of information, the minimal variations in medication's forms, colors, and patterns is its main motivation. Since there are numerous varieties of medicines, which outnumber their limited visual differences and qualities, every trait is vital in recognizing them. With the current existing frameworks of two-step object detector, however, all the information about size of RoIs is diminished after going through \emph{RoI Pooling} layer. By utilizing the information about bounding box annotation, we can formulate the relative size information, and re-merge it into the visual features for best classification accuracy. Detailed algorithm for formulate Relative Size Graph (RSG) is presented in Subsection \ref{subsec:method_size_modeling}.

% This section covers my detailed methodology for knowledge graph modeling and my framework for embedding this graph.
\subsubsection*{Prescription-based Co-occurrence Graph Modeling}
\label{subsec:method_coocurent_modeling}
% \begin{figure}[bth]
%     \centering
%     \includegraphics[width=0.7\textwidth]{images/graph.eps}
%     \caption{\textcolor{red}{A visualization of a Prescription-based Medical Knowledge Graph $\mathcal{G = <V, E, W>}$. The light and blue edges denote edges with small weights; while the sharp, black ones are the strong relationships.}}
%     \label{fig:homograph}
% \end{figure}
% In the literature, several works on multi-object detection have already utilized co-occurrence likelihood as a priori to enhance detection accuracy ~\cite{cooc_1, cooc_2, cooc_3}. 
% Typically, this information is gathered from the image dataset used for training the object detection task. Unlike existing approaches, 
{We propose to leverage an external source, namely prescriptions, to build the co-occurrence graph. 
The rationale behind our idea is that as most pills are intended to cure or alleviate certain diseases or symptoms, there is a significant likelihood that pills meant to treat the same diseases will appear concurrently.
Thus, the implicit relationship between the pills can be modeled by assessing the direct interaction between medications and diseases derived through prescriptions.
Our Co-graph, $\mathcal{G}_c = \left \langle V, E, W_c \right \rangle$, is a weighted graph whose vertices $V$ represent pill classes, and whose edges' weights $W_c$ reflect the co-occurrence likelihood of the pills. 
% Since we are dealing with the pill recognition problem, this graph focuses on a set of nodes $\mathcal{V}$ representing pill classes, as illustrated in Figure \ref{fig:homograph}. I have constructed a prescription dataset containing anonymous prescriptions from four major hospitals in Vietnam between 2021-2022. The pill images are manually labeled as discussed in \ref{subsec:dataset}. By leveraging this information, the desired knowledge graph can be built and effectively used with the introduced image dataset.
%With prescriptions as the initial data, two factors can be used to formulate graph edges $E_1$, which are diagnoses and medications. 
As the association between pills do not explicitly present in the prescriptions, we model this relationship utilizing the interaction between medications and diseases using the following criteria.}
%of the pill classes $C_i$ and $C_j$ based on the following criteria.
\begin{itemize}
    \item [$\bullet$] There is an edge between two pill classes $C_i$ and $C_j$ if and only if they have been prescribed for at least one shared disease.
    \item [$\bullet$] The greater the weight of an edge $E_{ij}$ connecting pill classes $C_i$ and $C_j$, the more likely that these two medications will be prescribed simultaneously.
    % can be determined by inspecting the number of co-occurrences of $A$ and $B$ in various prescriptions, details are presented below.
\end{itemize}
% Instead of directly weighting the \texttt{Pill-Pill} edges, I first produce the \texttt{Diagnose-Pill} weights, based on their explicit relationship. Ideally, the weight for edge $P-Q$ of diagnose $P$ and pill $Q$ should represent the importance of pill $Q$ in treating, curing or alleviating symptom $P$. The quantification of importance is derived from idea of factor Term Frequency (\texttt{tf}) — Inverse Dense Frequency (\texttt{idf}) in NLP tasks as follow
%Instead of directly weighting the \texttt{Pill-Pill} edges, I determine the weights via \texttt{Diagnose-Pill} relation.
%In particular, 
{We first define a so-called \texttt{Diagnose-Pill} impact factor, which reflects how important a pill is to a diagnosis.
%or, in other words, how often a pill is prescribed to cure the diagnosis. 
Inspired by the Term Frequency (\texttt{tf}) — Inverse Dense Frequency (\texttt{idf}) often used in the Natural Language Processing domain, we define the impact factor of a pill $P_j$ to a diagnosis $D_i$, denoted as $\mathcal{I}(P_j, D_i)$, as follows}
\begin{equation*}
    \small
    \mathcal{I}(P_j, D_i) = \texttt{tf}(D_j, P_i)\times \texttt{idf}(P_i) = \frac{|\mathbb{S}(D_j, P_i)|}{|\mathbb{S}(D_j)|} \times \log \frac{|\mathbb{S}|}{|\mathbb{S}(P_i)|},
\end{equation*}
where $\mathbb{S}$ represents the set of all prescriptions, 
$\mathbb{S}(D_j, P_i)$ depicts the collection of prescriptions containing both $D_j$ and $P_i$, and $\mathbb{S}(D_j)$ illustrates the set of prescriptions containing $D_j$. 
{Intuitively, $\texttt{tf}(D_j, P_i)$ measures how often pill $P_i$ is prescribed for diagnosis $D_j$, thus it reflects the significance of $P_i$ regarding treating $D_j$. However, in practice, some pills are more popular among prescriptions (e.g., Sustenance, Dorogyne, Betaserc, etc.), which may cause negative bias when applying only the $\texttt{tf}$ term. That effect can be mitigated by the term $\texttt{idf}(P_i)$.}

{Once finished formulating the impact factors of the pills and diagnoses, we transform each term $\mathcal{I}(P_j, D_i)$ into a probabilistic view by a simple normalization over all diagnoses as follows}
\begin{equation*}
    \small
    p(P_j, D_i) = \frac{\mathcal{I}(P_j, D_i)}{\sum_{D \in \mathbb{D}}\mathcal{I}(P_j, D)}, 
\end{equation*}
{where $\mathbb{D}$ denotes the set of all diagnoses. Given $p(P_j, D_i)$, we define the weight ${W_c}(P_i, P_j)$  of the edge $E_{ij}$ connecting vertices $P_i$ and $P_j$ as the probability $p(P_i, P_j)$ that $P_i$ and $P_j$ are prescribed for the same diseases. ${W_c}(P_i, P_j)$ can be formulated as follows. }
\begin{equation}
    \small
     {W_c}(P_i, P_j) := p(P_i, P_j) \approx \sum_{D \in \mathbb{D}} p(P_i, D) \times p(P_j, D).
\label{eq:weight_mcg}
\end{equation}

%where $\mathcal{W}(P_i, P_j)$ depicts the weight between pills $P_i, P_j$, and $\mathbb{D}$ denotes the set of all diagnoses. 
% It should be noted that Formula (\ref{eq:weight_mcg}) makes an assumption on the independence of two events $(P_i, D)$ and $(P_j, D)$.

{\subsubsection*{Relative Size Graph Modeling}
\label{subsec:method_size_modeling}
% In the following, we describe our method for constructing the Size-graph, which is responsible for modeling the inter-pill relation in terms of size.
The Size-graph is represented by a directed graph $\mathcal{G}_s = \left \langle V, E, W_s \right \rangle$.
% , where $V$ and $E$ denotes the vertices and edges, respectively; $W_s$ depicts the edges' weights. 
% The Size-graph shares the same vertices and edges as the Co-graph.
% , as the relative size between two pills can only be determined if they are in the same images.
% The difference between the Size-graph and the Co-graph lies in the edges' weights, i.e., $W_s$ is different from $W_c$. 
% Specifically, 
The edge weight $W_s$ is modeled so that the weight of an edge $\overrightarrow{E_{ij}}$ connecting from $P_i$ to $P_j$ is proportional to the size ratio of $P_i$ to $P_j$.
The primary source for constructing the Size-graph is the annotations of the training dataset's bounding boxes. As the camera locations for multiple pictures are different, the exact size of each bounding box cannot be utilized directly.
Therefore, we instead define a so-called \emph{size indicator}, a normalized representation of pill size, which is determined as follows.
%We denote by $s_i$ the size indicator of pill class $P_i$. 
%The size indicators are determined in the following manner.
\begin{itemize}
    \item \textbf{Step 1:} We begin with an arbitrary pill class by initializing its size indicator to $1$, while those of other pill classes are initialized to $0$.
    \item \textbf{Step 2:} From the current node $P_i$, we traverse through all its 1-hop neighbors $P_j$, and calculate $P_j$'s size indicator $s_j$ as $s_j := s_i \times \frac{|B_j|}{|B_i|}$, where $B_i, B_j$ are the two bounding boxes of $P_i$ and $P_j$ in a particular image in the training set. Step 2 is repeated until all the vertices of $\mathcal{G}_s$ are traversed.
\end{itemize}
Given the size indicators of all vertices, we now define the weight of edge $\overrightarrow{E_{ij}}$ as the ratio of $s_i$ to $s_j$.
% By setting a initial value $s_0 = 1$ for class 0, I can traverse through all 1-hop neighbors $v_i$ of Node $v_0$ in the graph $\mathcal{G}_1$, and recursively calculate all indicators $s_i$. The edge between two node $v_i$ and $v_j$ is then can be calculated by the ratio between two indicator $s_i$ and $s_j$. 
%Detailed formulation is presented by Algorithm 1 in the Supplementary.
%the bigger the difference in size between two pills.
% are the vertices are the same as PMCG graph $\mathcal{G}_1$. The reason for this similarity is owing to the fact that both graphs present the relationship between pills - the label, hence share $V$. Moreover, the relative size between two pills can only be formulated if they are in the same images, which, in turn, be in the same prescription, hence the edges set $E$ is also shared.
% $W_2$, however, is different as compared to $W_1$, since it represents the relationship about relative sizes between pills.
\subsection*{Visual Feature Extractor}
\label{sec:method_visual_extractor}
This block is responsible for localizing and extracting the features of Regions of Interest (RoIs).
For this purpose, we adopt components from Faster RCNN~\cite{fasterrcnn}, a conventional two-step object detector architecture. Nevertheless, our proposed framework is compatible with any alternative object detection architecture.
The Visual Feature Extractor consists of three components: a Convolutional Network, a Region Proposal Network, and an RoI Pooling Layer, as depicted in Fig.~\ref{fig:generalflow}. }
% Concerning the first component, i.e., Convolutional Network, various candidates such as VGG~\cite{vgg} or ResNet~\cite{resnet} can be utilized. 
% In our experiments, we choose ResNet-50 \cite{resnet} due to its excellent balance between the complexity and accuracy regarding our pill detection task.
% The remaining two modules, i.e.,  Region Proposal Network (RPN) and RoI Pooling Layer, are taken from the original framework Faster RCNN. 
{RPN is a fully convolutional network that takes the visual feature vector from the previous module and generates proposals with various scales and aspect ratios. The RoI Pooling layer works simply by splitting each region proposal into a grid of cells and then applying the max pooling operation to each cell in the grid. The combination of the grids' values forms the visual feature vectors of the RoIs.}
% The operation of the visual feature extractor can be mathematically formulated as follows. \\
% \noindent \textbf{Input:} \\
% $$
% img_i \text{ with shape } (c, w, h)
% $$
% \noindent \textbf{Visual Feature Extractor:} \\
% $$
% \begin{aligned}
%     z_i &= f_{convnet}(img_i), \\
%     o\_scores_i, coordinate_i &= f_{RPN}(z_i), \\
%     \Tilde{o\_scores}_i, \Tilde{coordinate}_i &= NMS_k(o\_scores_i, coordinate_i), \\
%     x_i &= f_{RoIPooling}(z_i, \Tilde{coordinate}_i), \\
% \end{aligned}
% $$
% \noindent in which: \\
% \begin{itemize}
%     \item $f_{convnet}$, $NMS$ represents the Convolutional Network.
%     \item $f_{RPN}$ represents the Region Proposal Network.
%     \item $NMS_k$ represents Non-Maximum Suppression post-processing technique, followed by threshold-cutting top $k$ RoIs with highest objectness scores.
%     \item $f_{RoIPooling}$ represents the RoI Pooling operation.
% \end{itemize}
%All values from all cells represent the feature vector. 
% Rather than using \emph{pyramids of images} or \emph{pyramids of filters}, RPN make use of $k$ anchor boxes. An anchor box is a reference box of a specific scale and aspect ratio. With multiple reference anchor boxes, then multiple scales and aspect ratios exist for the single region. This can be thought of as a pyramid of reference anchor boxes. Each region is then mapped to each reference anchor box, and thus detecting objects at different scales and aspect ratios. 
% In the following, the procedure of feature extraction for locations of interest is presented. It is the responsibility of the Visual Processing block. In order to accomplish this, this block must be able to distinguish what should be the major focus and extract the characteristics associated to that region.
% My Visual Processing module uses Faster RCNN, to be precise \cite{fasterrcnn}.
%From Figure \ref{fig:generalflow}, it can be seen that this blocks consists of three submodules: a Convolutional Network, a Region Proposal Network and a RoI Pooling Layer.
%as it well balances between the complexity and accuracy for this pill detection task. 
% After passing an image through this feature extractor, I receive a $4096$-dimensional feature vector. By forward propagating a typical picture through five convolutional blocks and one fully connected layer, features are generated (Figure \ref{fig:resnet50}). I encourage readers referring to \cite{resnet} for more network architecture details.
% \begin{figure*}
%     \centering
%     \includegraphics[width=0.8\textwidth]{rn50.png}
%     \caption[ResNet50 architecture]{\textbf{(Left)} ResNet50 architecture. The last Softmax is discarded in my feature extractor. \textbf{(Middle)} Convolution block which changes the dimension of the input. \textbf{(Right)} Identity block which will not change the dimension of the input. \\
%     \textit{Image source: \textit{Optimized Deep Convolutional Neural Networks for Identification of Macular Diseases from Optical Coherence Tomography Images} \cite{rn50_fig}.}}
%     \label{fig:resnet50}
% \end{figure*}
% \begin{figure*}[!h]
%     \centering
%     \includegraphics[width=0.7\textwidth]{rpn.png}
%     \caption[Region Proposal Network (RPN)]{Region Proposal Network (RPN). \\
%     \textit{Image source: \textit{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks} \cite{fasterrcnn}}}
%     \label{fig:rpn}
% \end{figure*}
%If the grid size is $2\times2$, then the feature vector length is $4$. The reason for this operation is to extract a fixed-length feature vector from each region proposal. With this process, the latter module of PGPNet can parallelize the computation and leverage the connections amongs RoIs for enhancing classification accuracy. However, as previously mentioned, this is the main reason for diminishing size information of pills. Relative Size Graph proposed in \ref{subsec:method_size_modeling} help alleviate this issue.

{\subsection*{Inter-Pill Relational Feature Extractor}
\label{sec:method_a_module}
% In Section \ref{sec:method_graph_model}, graph model, we presented two generic graphs that offer a priori information about the relationship between all the pills in terms of co-occurrence and size.
To enhance the efficacy of this a priori, we observed that rather than the whole graphs representing the interaction between all pills, we should utilize sub-graphs concentrating on the ones most likely to appear in the image.
Motivated by this observation, we employ the Inter-Pill Relational Feature Extractor, responsible for extracting condensed sub-graphs from generic Co-graph and Size-graph. 
Moreover, previous studies have pointed out that the appearances of pills convey implications about their efficacy or ingredients~\cite{medical3}. In light of this, utilizing pills' visual feature vectors, we develop a visual-based graph that models the implicit relationship between medications indicated by their visual appearance.}
% In what follows, we describe in the details our method to construct three condensed graph representing co-occurrence, relative size and visual correlation among pill classes that are most likely to appear in the input image. 

{\noindent \textbf{Condensed Co-graph and Size-graph.} 
Our main idea is to employ a so-called \emph{Pseudo Classifier}, which provides approximate classification results using solely visual features of RoIs. These temporary identification results are then utilized as a filter layer to eliminate redundant information from the original Co-graph and Size-graph, leaving only information about pill classes probable to appear in the input image.
%Effectively, the pills in my dataset may be divided into two categories: simple samples and difficult samples, illustrated in Fig.\ref{fig:pill_exam}. Pseudo Classifier can readily recognize the former since they possess distinguishable visual characteristics. However, the latter require extra information about nearby tablets to help in their recognition. Using only the visual-based Pseudo Classifier, I am able to filter out the majority of the simple ones and used them as context pills for recognizing the remaining hard ones. 
In current implementation, the pseudo classifier is straightforwardly implemented as a fully connected layer. 
%Having attained the results of this module, a composites of simple matrix multiplications can be applied to extract the sections of original knowledge graphs that need to be focused. 
Let $N$ be the number of pill classes and $M$ be the number of pill bounding boxes (i.e., RoIs) in the input image. Suppose $P={[p_{ij}]}_{M\times N}$ is the matrix whose row vectors represent the logits produced by the pseudo classifier, and $\mathcal{A}_c={[a^c_{kl}]}_{N\times N}$, $\mathcal{A}_s={[a^s_{kl}]}_{N \times N}$ denote the weighted adjacency matrices of the Co-graph $\mathcal{G}_c$ and Size-graph $\mathcal{G}_s$, respectively. 
The condensed adjacency matrices, denoted as $\Tilde{\mathcal{A}}_c$ and $\Tilde{\mathcal{A}}_s$ are matrices of size $M \times M$, each row depicts the condensed relational information of a pill, i.e., a specific RoI, with others in the input image. $\Tilde{\mathcal{A}_c}$ and $\Tilde{\mathcal{A}_s}$ are obtained by performing a composition of matrix multiplications as follows.
\begin{eqnarray}
    \small
    \label{eq:ac} \Tilde{\mathcal{A}}_c &= \sigma (P) \cdot \mathcal{A}_c \cdot \sigma (P)^T\\
    \label{eq:as} \Tilde{\mathcal{A}}_s &= \sigma (P) \cdot \mathcal{A}_s \cdot \sigma (P)^T, 
\end{eqnarray}
where $\sigma$ denotes the \texttt{Softmax} activation function. 
Intuitively, the item in the $i$-th row and $j$-th column of $\Tilde{\mathcal{A}}_c$ and $\Tilde{\mathcal{A}}_s$ highlights the relationship of the $i$- and $j$-th RoIs. }
% as the $i$-th column of $\sigma (P)^T$ represents the likelihood of the $i$-th RoI with respect to $N$ pill classes,  $\mathcal{A}_c \cdot \sigma (P)^T$ produces a matrix whose $j$-th column highlight the co-occurrence of the 
% the $i$-th row of $\Tilde{\mathcal{A}}_i$ is a weighted sum of all the $\mathcal{G}_i$'s adjacency matrix, whose weights are the classification probabilities corresponding to the $i$-th pill in the input image.
% Intuitively, the $i$-th row of $\Tilde{\mathcal{A}}_i$ is a weighted sum of all the $\mathcal{G}_i$'s adjacency matrix, whose weights are the classification probabilities corresponding to the $i$-th pill in the input image.

% there is implicit implication of relationship contained in the visual appearances
% instead demonstrate an actual correlation between their aesthetics and their efficacy or active constituents \cite{medical1}\cite{medical2}\cite{medical3}. 
% After utilizing external knowledge graphs to help in pill recognition, another realization can be thought of is to exploit the relation contained in the visual appearances themselves. Despite the fact that medications appear to be arbitrarily shaped or colored, several medical studies instead demonstrate an actual correlation between their aesthetics and their efficacy or active constituents \cite{medical1}\cite{medical2}\cite{medical3}. 
% about Co-graph and Size-graph which model the interaction interaction of  
% This section would cover my major contribution in the architecture of PGPNet - Graph Processing Module. It comprises of three sub-components that works for different purposes. The first one is Adaptive Graph Module, which is responsible for extracting adaptive graphs of RoIs from the original MCG and RSG graphs. Details about it would be discussed in \ref{subsec:method_adaptive_graph_module}. Following, a visual-based graph is generated with the aid of a simple feature encoder, presented in \ref{subsec:method_visual_graph_module}. Lastly, section \ref{subsec:method_graph_transformer_module} discusses about Graph Transformer module, which learn the best context presentations for enhancing information of each RoI.

% \subsubsection{Adaptive Graph Module}
% \label{subsec:method_adaptive_graph_module}
% After defining the generic knowledge graphs of MCG and RSG, this module is derived intuitively from a realization. Specifically, while addressing a specific picture - a query, PGPNet - a student should be able to choose which portion of the two graphs - two reference books - should be consulted. This concept is natural and closely related to the motivation of \emph{Attention mechanism} \cite{attention}, which has its origin from the field of Natural Language Processing (NLP).

% This module's primary component is a Pseudo Classifier, which provides approximate classification results using solely visual features of RoIs. These temporary identification results are then utilized as a filter layer to pull from the MCG and RSG graphs just information pertaining to the pills in the image (and omitting information from the nodes that are not associated with the pills in the picture). Effectively, the pills in my dataset may be divided into two categories: simple samples and difficult samples, illustrated in Fig.\ref{fig:pill_exam}. Pseudo Classifier can readily recognize the former since they possess distinguishable visual characteristics. However, the latter require extra information about nearby tablets to help in their recognition. Using only the visual-based Pseudo Classifier, I am able to filter out the majority of the simple ones and used them as context pills for recognizing the remaining hard ones. 

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=\columnwidth]{pill_samples.pdf}
%     \caption{Examples of easy and hard samples.}
%     \label{fig:pill_exam}
% \end{figure}

% In current implementation, pseudo classifier is straightforwardly implemented as a fully connected layer. Having attained the results of this module, a composites of simple matrix multiplications can be applied to extract the sections of original knowledge graphs that need to be focused. Let $N$ be the number of pill classes and $M$ be the number of pills in the input image. Suppose $P={[p_{ij}]}_{i=\overline{1,M}; j=\overline{1,N}}$ is the matrix whose row vectors represent the logits produced by the pseudo classifier, and $\mathcal{A}_1={[a^1_{kl}]}_{k=\overline{1,N}; l=\overline{1,H}}$, $\mathcal{A}_2={[a^2_{kl}]}_{k=\overline{1,N}; l=\overline{1,H}}$ denote the weighted adjacency matrices for MCG $\mathcal{G}_1$ and RSG $\mathcal{G}_2$, respectively. 
% The adaptive adjacency matrices, denoted as $\Tilde{\mathcal{A}}_1$ and $\Tilde{\mathcal{A}}_2$ are matrices of size $M \times M$, each row depicts the condensed relational information of a pill - a specific RoI with others in the input image. $\Tilde{\mathcal{A}_i}$ is calculated by performing a composition of matrix multiplications as follows.
% \begin{equation}
%     \Tilde{\mathcal{A}}_i = \sigma (P) \cdot \mathcal{A} \cdot \sigma (P)^T. 
% \end{equation}
% Here the symbol $\sigma$ denotes the \texttt{Softmax} activation function. Intuitively, the $i$-th row of $\Tilde{\mathcal{A}}_i$ is a weighted sum of all the $\mathcal{G}_i$'s adjacency matrix, whose weights are the classification probabilities corresponding to the $i$-th pill in the input image.

{\noindent \textbf{Visual Semantic Graph.}
% After utilizing external knowledge graphs to help in pill recognition, another realization can be thought of is to exploit the relation contained in the visual appearances themselves. Despite the fact that medications appear to be arbitrarily shaped or colored, several medical studies instead demonstrate an actual correlation between their aesthetics and their efficacy or active constituents \cite{medical1}\cite{medical2}\cite{medical3}. 
%With this motivation, I also make use of RoIs' visual features for creating the third graph which models the visually semantical relationship among pills in the input image. 
As mentioned above, the visual semantic graph $\Tilde{\mathcal{G}}_v = \left \langle \Tilde{V}, \Tilde{E}_v, \Tilde{W}_v \right \rangle$ is in charge of capturing the visually semantic correlation among pills in the input image. 
%Intuitively, the visual semantic graph, denoted as $\Tilde{\mathcal{G}}_v = \left \langle \Tilde{V}, \Tilde{E}_v, \Tilde{W}_v \right \rangle$
The detailed algorithm to construct this graph is as follows.
All visual feature vectors are first passed through a non-linear function $\mathcal{F}$: $\mathbb{R}^{h} \rightarrow \mathbb{R}^{h'}$ to transform from the original $h$-dimensional space into a $h'$-dimensional latent one, where their relationship can be best presented. The latent output vectors are then directly used for calculating the correlations between RoIs.
Let $R_i, R_j$ are two RoIs in the input image, and $z_i, z_j$ are their feature vectors created by the Visual Feature Extractor block, the weight of the edge connecting $R_i$ and $R_j$ is defined as $ \Tilde{W_v}(R_i, R_j) = z_i \cdot z_j$.}
% \begin{equation}
%     \Tilde{W_v}(R_i, R_j) = z_i \cdot z_j,
% \end{equation}
% in which $w^3_{ij}$ denotes the \emph{visual-based} weight between two RoIs, $z_i$ and $z_j$ are latent presentations of $i$-th and $j$-th RoI, respectively. 

% When performing a matrix multiplication instead of above pair-wise product, I can achieve the weighted adjacency matrix $W_3$ for the third graph $\mathcal{G}_3$, which is densely populated and its weight values indicate the relavances of RoIs visually.

% Now, we have three graphs with identical vertex set $\Tilde{V}$, representing the RoIs of the input images.
% These three graphs are combined to form a heterogeneous graph $\Tilde{\mathcal{G}}$ with one kind of nodes and three types of edges corresponding to three distinctive relationship senses.

% After this module, PGPNet have extracted three adaptive graphs' relations $\Tilde{\mathcal{A}}_i - i=\overline{1,3}$. Since all three possess the same set of nodes $\Tilde{V}$, which are the regions of interest (RoIs) of the input pictures, it can be interpreted as a single heterogeneous graph $\Tilde{\mathcal{G}}$ with one node type and three edge types corresponding to three distinct senses of relationship.




{\subsection*{Multi-modal Data Fusion}
\label{subsec:method_g_module}

\begin{figure}[!tb]
        \centering
        \includegraphics[width=0.3\columnwidth]{att.eps}
        \caption{\textbf{Node attribute modeling.} ${\omega}_i={\left [\omega_{1i}, \dots, \omega_{Hi} \right ]}^T$ is the classifier weights corresponding to the $i$-th pill class, capturing the representative features of this class. $\sum_{i=1}^{H}p_{ku} \times {\omega}_i$ is the attribute of the $i$-th RoI. \label{fig:att}}
    \end{figure}

\begin{figure}[!tb]
        \centering
        \includegraphics[width=0.75\columnwidth]{gtn.eps}
        \caption{\textbf{Graph Transformer Network (GTN) architecture~\cite{gtn}.} GTN softly selects adjacency matrices (edge types) from the set of adjacency matrices $\mathbb{A}$ of a heterogeneous graph $\Tilde{\mathcal{G}}$ and learns new meta-path graphs represented by $\Tilde{\mathbb{A}}$ via the matrix multiplication of two selected adjacency tensors $\mathbb{Q}_1$ and $\mathbb{Q}_2$. The soft adjacency matrix selection is the weighted sum of candidate adjacency matrices obtained by $C$ channels of $1 \times 1$ convolution with non-negative weights with \texttt{softmax} activation. \label{fig:gtn_architec}}
    \end{figure}

After going through the second and third blocks, we get the visual features of the RoIs and three relational graphs representing the relationships between the RoIs. 
This information is now fed into the Multi-modal Data Fusion to generate the final feature vectors, each of which encapsulates both the intra-Pill visual characteristic of an RoI and the inter-Pill interaction of that RoI with the others. 
The Multi-modal Data Fusion comprises two steps: graph embedding and data concatenation. 
The former obtains the heterogeneous relational graph $\mathcal{G}$ and transforms it into context features in the vector space, while the latter concatenates the context feature vectors with visual features to generate the final enhanced features.
We utilize the Graph Transformer Network (GTN)\cite{gtn} for the graph embedding.
The reason for choosing the GTN is due to its ability to handle heterogeneous input and adaptive graph structures.
% The weighted adjacency matrices extracted from $\mathcal{G}_1$, $\mathcal{G}_2$ or $\mathcal{G}_3$ can not be easily \emph{digested} and combined with RoIs' visual features without an explicit form of aggregation. In addition, the vectors should be well designed for each RoI, since the effects of different neighbors to a particular pill are not identical. Intuitively, there should be a module capable of accumulating these previously constructed relations together with information representative for each RoI in order to construct the context vector that best describes the surrounding environment for the pill under consideration. Graph Transformer Network (GTN) is the final module in charge of creating such vectors that are best suited for RoIs.
Before going into the detail of the GTN, it is crucial to define the node attribute of graph $\Tilde{\mathcal{G}}$.
As each node of $\Tilde{\mathcal{G}}$ represents an RoI, the node attribute should be the most representative characteristic of the ROIs. 
Using the retrieved RoI visual features to depict the relevant ROIs is the most natural solution but is not advantageous due to several factors, including the unreliability in dealing with ambiguous samples or the $intra-variance$ in visual features of one class \cite{reasoning_rcnn}. 
% However, earlier studies\cite{reasoning_rcnn} have indicated that this option is not advantageous due to several factors, including the unreliability in dealing with ambiguous samples or the $intra-variance$ in visual features of one class. 
%listed below.
% %There are several factors account for this behavior, listed below.
% \begin{itemize}
%     \item In the circumstances of heavy occlusions and ambiguities, the visual features are not reliable,
%     \item In the dataset, different images have different light conditions, camera angles, zoom levels, which, in turn make an $intra-variance$ in visual features of one class,
%     \item Two pills with identical appearances would in turn having similar visual features, hence not representative.
% \end{itemize}
To this end, classifier weights has been introduced as a simple yet effective alternative.
%because the classifier weights for each class actually include high-level semantic information representing the feature activation learned from all images.
According to ~\cite{weight_3}, the classifier weights connected to the $i$-th neuron in the last layer (which is denoted as ${\omega}_i={\left [\omega_{1i}, ..., \omega_{Hi} \right ]}^T$ in Fig.\ref{fig:att}) corresponds to the $i$-th pill class, encapsulating the representative characteristics of this class.
Let $p_k=\left [p_{k1}, ..., p_{kN}\right ] $ be the logit vector of the $k$-th RoI, where $p_{ki}$ depicts the likelihood for the $k$-th RoI to be classified into the $i$-th class, we define $\sum_{i=1}^{H}p_{ku}\times {\omega}_i$ the attribute of the $k$-th RoI.
Intuitively, this attribute can be considered as a decomposition of the RoI's characteristic in the space of the classes' features.
}
% \cite{sgrn}\cite{reasoning_rcnn}, the authors also provide a simple yet effective alternatives to the visual features that are the weights of the classifier (i.e., classifier weight, for short).
% for each category actually include high-level semantic information since they represent the feature activation learned from all pictures. 
% As the classifier weights are tuned with the aim of producing correct classification results regardless the variances of input features, they are more stable than the visual attribute. 
% Formally, let $\Omega \in \texttt{R}^{H \times N}$ denote the matrix weights representing the classifier weights of all pill classes, the node attributes of all a RoIs batch (denoted as $X_{RoI}$) can be obtained by multiplying $\Omega$ the output $P \in \texttt{R}^{M \times C}$ of the Pseudo Classifier, i.e., $X_{RoI} = P \cdot \Omega^T$.
% Note that as the classifiers are updated through each iteration during the training process, the node attribute $X_{RoI}$ becomes increasingly accurate overtime. Furthermore, this approach enables us to train the in an end-to-end fashion.

% Graph Transformer Network (GTN) is the aggregator for generating context vectors correlative to all RoIs. It should be able to learn the node embeddings of graphs with following characteristics: being heterogeneous in nature and having adaptive graph structures.The heterogeneity of the input graph coincides with that of my adaptive graph $\Tilde{\mathcal{G}}$. The latter criterion must be met since the input graphs for images are varied and the graph formed for a particular image during one iteration is not identical to the graph generated for the same image during subsequent iterations because the learning process is ongoing. For this module, I leverage the architecture proposed in \cite{gtn}. It is particularly suited for scenarios involving these aforementioned criteria.
{Figure~\ref{fig:gtn_architec} depicts the GTN's architecture, which consists of two phases. The former can be seen as a meta-path generator that fuses information from multiple input adjacency matrices to generate a composite graph structure. 
This newly generated graph serves as the second stage's input, which comprises a Graph Convolutional Network (GCN) and is responsible for producing a representation for each node.
% GTN's fundamental concept is to produce new graph structures and simultaneously learn node representations on the learnt graphs. GTNs seek alternative graph structures utilizing various candidate adjacency matrices to execute more effective graph convolutions and learn more potent node representations, as opposed to the majority of CNNs on graphs, which assume the graph is supplied. 
Specifically, the GTN consists of $l$ Graph Transformer (GT) layer; the $l$-th layer applies the $C$-channel 1D convolution operation on the input graph $\Tilde{\mathcal{G}}$ to obtain a stack of new graph structure $\mathbb{Q}_1^{(l)} \in \mathbb{R}^{M \times M \times C}$ as follows.
\begin{equation}
\small
\mathbb{Q}_1^{(l)} = F(\Tilde{\mathcal{G}}; W_\phi^{(l)}) = \phi(\Tilde{\mathcal{G}};\sigma(W_\phi^{(l)})),
\end{equation}
where $\phi$ indicates the convolution layer, $W_\phi^{(l)} \in \mathbb{R}^{C \times 1 \times 1 \times K}$ represents the parameter of $\phi$, and $K$ implies the number of relations contained in the original graph $\Tilde{\mathcal{G}}$.
The stacked graph $\mathbb{Q}_1^{(l)}$ serve as the first component in creating length $l$ meta-paths, while $\mathbb{Q}_2^{(l)}$ is taken as $\Tilde{\mathcal{G}}^{(l-1)}$, i.e., $\Tilde{\mathcal{G}}^{(l)} = \mathbb{Q}_2^{(l)} \odot \mathbb{Q}_2^{(l)}$. 
%Below is the mathematical formulation for creating the meta-paths at $l$-th GT layer.
%
% In Equation (\ref{eq:gtn}), $\odot$ indicate the Batch Matrix Multiplication operator.
To balance computational overheads and model performances, with PGPNet, we fix $l = 2$. }
% Among the generated stacked graphs $\mathbb{Q}_i$ ($i= 1, .., 3$), the GTN 
% Figure \ref{fig:gtn} describe the detail architecture of GTN. GTN softly selects two stacks of graph structures $\mathbb{Q}_1$ and 
% $\mathbb{Q}_2 \in \mathbb{R}^{M \times M \times C}$ from candidate adaptive adjacency matrices $\mathbb{A}$. For doing so, it computes the convex combinations of adjacency matrices by a $C$-channel $1 \times 1$ convolution as in Fig. \ref{fig:gtn} with the weights from \texttt{softmax} function as:
% \begin{equation}
% \mathbb{Q} = F(\mathbb{A}; W_\phi) = \phi(\mathbb{A};\sigma(W_\phi)), 
% \end{equation}
% where $\phi$ is the convolution layer and $W_\phi \in \mathbb{R}^{C \times 1 \times 1 \times K}$ is the parameter of $\phi$. Second, it learns $C$ new graph structure by the composition of two stacked relations (i.e., matrix multiplication of two adjacency tensors, $\mathbb{Q}_1 \cdot \mathbb{Q}_2$). 

{The resulting graph $\Tilde{\mathcal{G}}^{(2)}$, together with RoIs' representative features $X_{RoI}$, are then utilized as the input for the Graph Convolution Network (GCN) to generate the final node presentations. 
These vectors are directly concatenated with their corresponding RoIs' visual features before getting fed into the Bounding Box Regressor and Classifier to produce the final detection results.} %enhance the results of both tasks.

\subsection*{PGPNet's Losses}
\label{sec:method_loss}
{This section presents the details about our model's objectives and the corresponding losses to achieve those goals. }
% We start with common losses widely used in two-step detectors in Subsection \ref{subsec:method_normal_loss}, and then introduce our proposed auxiliary loss, based on co-occurrence information, in Section \ref{subsec:method_auxi_loss}.

\subsubsection*{Two-step Object Detectors' Losses}
\label{subsec:method_normal_loss}

\hfill\\
{\textbf{The Region Proposal Network's Losses.}
The loss for Region Proposal Network consists of two components: classification loss combined and bounding box regression loss.
Let $p_i, p_i^*$ be the predicted probability of an anchor $i$ being an object and the ground truth label whether anchor $i$ is the object, respectively; $t_i$ and $t_i^*$ depict the differences of four predicted coordinates, and the ground truth coordinates with the coordinates of the anchor boxes, respectively. 
The classification loss $\mathcal{L}_{cls}$ and bounding box regression loss $\mathcal{L}_{\mathrm{box}}$ are defined as follows.
% \textcolor{red}{@Duy: explain the meaning of notations $p_i, t_i, p^{*}_{i}}}.
\begin{equation}
\begin{split}
    \small
    \mathcal{L}_{RPN}\left(\left\{p_{i}\right\},\left\{t_{i}\right\}\right) &= \frac{1}{N_{\mathrm{cls}}} \sum_{i} \mathcal{L}_{\mathrm{cls}}\left(p_{i}, p_{i}^{*}\right) \\
    &+ \frac{\lambda}{N_{\mathrm{box}}} \sum_{i} p_{i}^{*} \cdot L_{1}^{\mathrm{smooth}}\left(t_{i}-t_{i}^{*}\right),
\end{split}
\end{equation}
where
\begin{equation}
\small
    L_{1}^{\text {smooth }}(x)= \begin{cases}0.5 x^{2} & \text { if }|x|<1 \\ |x|-0.5 & \text { otherwise }\end{cases}.
\end{equation}
%In this composite loss function, $p_i, p_i^*$ are the predicted probability of anchor $i$ being an object and the ground truth label whether anchor $i$ is the object respectively. 
Here the $\mathcal{L}_{cls}$ is a binary classification \emph{log loss}, $N_{cls}$ and $N_{box}$ are two normalization terms, where $N_{cls}$ is set to the mini-batch size, while $N_{box}$ is the number of anchor boxes. 
$\lambda$ is a hyper-parameter, which is responsible for balancing between $\mathcal{L}_{\mathrm{cls}}$ and $\mathcal{L}_{\mathrm{box}}$.}
% We adopt the smoothing $L1$ function for the regression loss. 
%Here $t_i$ and $t_i^*$ are the differences of four predicted coordinates and the ground truth coordinates with the coordinates of the anchor boxes, respectively. 

{\noindent \textbf{Output's Losses.}
The PGPNet's final results consist of the coordinates of the RoIs' bounding boxes and predicted labels for the RoIs. 
%modifications to the bounding boxes proposed by RPN predicted labels for RoIs and modifications to the bounding boxes proposed by RPN. 
We employ two distinct losses to accomplish this objective.
%associated with these outcomes. 
While the loss for a bounding box regressor is equal to that of the RPN network, the classification loss $\mathcal{L}_{cls}^{out}$ is instead the cross entropy loss for the multilabel classification task, which is represented as follows $\mathcal{L}_{cls}^{out} = - \sum^N_{i=0}p^*_i \log(p_i)$.}
% \begin{equation}
%     \mathcal{L}_{cls}^{out} = - \sum^N_{i=0}p^*_ilog(p_i).
% \end{equation}

{\subsubsection*{Triplet Co-occurrence Enhancement Loss.}
\label{subsec:method_auxi_loss}
% In Section \ref{sec:method_a_module}, we employ the Pseudo classifier to derive condensed relational graphs from the generic ones. 
In this section, we propose an auxiliary loss named Triplet Co-occurrence Enhancement Loss which leverages the co-occurrence graph to boost the accuracy of the Pseudo Classifier.
The idea behind the auxiliary loss is that it encourages the co-occurrence likelihood of pills that are close together on the co-occurrence graph.
To this end, we construct our auxiliary loss as a contrastive loss that maximizes the co-occurrence probability of positive pairings (i.e., pills joined by edges with the most significant weights in the co-occurrence graphs) while minimizing the co-occurrence probability of negative pairs (i.e., pills that are not connected or connected by edges with smallest weights). In action, for each training mini-batch, PGPNet would treat all the ground truth pills in given images as the set of anchors and build up their corresponding positive as well as negative sets. After that, Triplet Co-occurrence Enhancement Loss would do its job for enhancing the robustness of Pseudo Classifier.
The detail of the auxiliary loss is as follows.}

% Motivationally, the frequency with which distinct pills co-occur should influence PGPNet's behavior. Specifically, if the framework can identify the existence of pill $A$ in the image with a high degree of confidence, it should also make assumptions about the appearance of $A$'s neighbors based on the Medical Co-occurrence Graph $\mathcal{G}_1$.
% I suggest the usage of an auxiliary loss called Triplet Co-occurrence Enhancement Loss for this reason. 
% Given that $A$ is a groundtruth pill in the picture, this loss aims to adjust the output of Pseudo Classification by motivating the probability of $A$ and its neighbors produced by this layer.
{Let's denote the $i$-th Region of Interest as $R_i$ with its corresponding label of $l_i$. 
Moreover, let $N^i_{pos}$ and $N^i_{neg}$ be the positive and negative samples of $R_i$, where $N^i_{pos}$ comprises of $k+1$ nearest neighbors and $N^i_{neg}$ consists of $k+1$ furthest neighbors of $R_i$. 
We suppose that the groundtruth labels of $N^i_{pos}$ and $N^i_{neg}$ are  $L_{pos} = \{ l^0_{pos}, l^1_{pos}, \dots, l^{k}_{pos} \}$, and $L_{neg} = \{ l^0_{neg}, l^1_{neg}, \dots, l^{k}_{neg} \}$, respectively. 
The auxiliary loss concerning the $i$-th RoI is defined by 
\begin{equation} \label{eq:aux_i}
\begin{split}
\small
    \mathcal{L}_{aux}^i &= p_i(l_i)p(N_{pos}^i) - (1 -p_i(l_i))p(N_{neg}^i) \\
                        &= p_i(l_i)\sum_{j=0}^{k}[1 - \prod_{m=0}^{M}(1 - p_m(l^j_{pos}))] \\
                        &- (1 - p_i(l_i))\sum_{q=0}^{k}[1 - \prod_{n=0}^{M}(1 - p_n(l^q_{neg}))],
\end{split}
\end{equation}
and those for RoI is $\mathcal{L}_{aux} = \sum_{i=0}^M\mathcal{L}_{aux}^i$.
% \begin{equation}\label{eq:aux}
%     \mathcal{L}_{aux} = \sum_{i=0}^M\mathcal{L}_{aux}^i.
% \end{equation}
In Formulas (\ref{eq:aux_i}) $M$ is the total number of RoIs in the image, $p$ is the output after going through \texttt{softmax} activation of logits produced by Pseudo Classifier. 
%and (\ref{eq:aux}), 
The objective during the training process is to {maximize} $\mathcal{L}_{aux}$, which in turn maximizes each {positive} term $p_i(l_i)p(N_{pos}^i)$ while minimizing the {negative} opposition $(1 -p_i(l_i))p(N_{neg}^i)$.
}
% % The final loss is the sum of the object detection losses, i.e., $\mathcal{L}_RPN$ and $\mathcal{L}_{cls}$, and the triplet co-occurrence enhancement loss $\mathcal{L}_{aux}$ as follows
% \begin{equation*}
%     \mathcal{L} = 
% \end{equation*}
% \textcolor{red}{@Duy: finish this formula.}


\section*{Dataset and Experiment Settings}
\label{sec:result}

% \documentclass[main_plos.tex]{subfiles}
% \begin{document}
\begin{figure*}[!t]
\centering
\begin{minipage}{0.6\linewidth}
    %\begin{table}[!tb]
    \centering \small
    	\setlength\tabcolsep{4pt} % default value: 6pt
        \resizebox{\columnwidth}{!}{%
            \begin{tabular}{l|l|l|l}
                \toprule
                & \textbf{NIH} & \textbf{CURE}  & \textbf{VAIPE}            \\ \midrule
                Number of pill images     & 7,000    & 8,973  & 9,426             \\
                Number of pill categories & 1,000    & 196   & 96               \\
                Number of capture devices & 1    & 1   & $>$ 20               \\
                Instance per category     & 7       & 40-50 & $>$ 30 \\
                Illumination conditions   & 1       & 3     & $>$ 50 \\
                Backgrounds   & 1       & 6     & $>$ 50 \\
                Number of prescriptions   & 0       & 0     & 1,527            \\ \bottomrule
            \end{tabular}
        }
        \captionof{table}{\label{tab:dataset_meta} {An overview of existing public datasets for the task of image-based pill detection. To the best of our knowledge, the introduced VAIPE dataset is currently the largest dataset for pill identification, which was collected in real-world settings and came up with prescriptions.}}
    %\end{table}
\end{minipage}
\hspace{0.1cm}
% \begin{minipage}{0.36\linewidth}
%     %\begin{figure}[!tb]
%         \centering
%         % \includegraphics[width=0.9\columnwidth]{dataset.pdf}
%         \caption{{\sout{Representative samples were taken from different pill image datasets: NIH (first row), CURE (second row), and VAIPE dataset (third row). The VAIPE dataset provides multiple pills per image from diverse backgrounds and devices. \label{fig:dataset}}} \duy{Replace fig. dataset by something else}}
%     %\end{figure}
% \end{minipage}
% \hspace{0.1cm}
\begin{minipage}{0.25\linewidth}
    %\begin{table}[!tb]
    \centering \small
    	\setlength\tabcolsep{4pt} % default value: 6pt
        \resizebox{\columnwidth}{!}{%
            \begin{tabular}{ll}
            \toprule
            \multicolumn{2}{c}{\textbf{Training dataset}} \\
            \cmidrule(lr){1-2}
            \multicolumn{1}{c}{Prescriptions} & \multicolumn{1}{c}{Images} \\ \midrule
            1,527 (100\%)             & 7,514 (78\%)    \\ \midrule
             \\
             \\
            \multicolumn{2}{c}{\textbf{Testing dataset}}  \\ 
            \cmidrule(lr){1-2} %\cmidrule(lr){3-4}
            \multicolumn{1}{c}{Prescriptions} & \multicolumn{1}{c}{Images} \\ \midrule
            0 (0\%) & 1,912 (22\%)     \\ 
            \bottomrule
            \\
            \end{tabular}
        }
        \captionof{table}{Details of training and testing datasets. \label{tab:train_test_split}}
    %\end{table}
\end{minipage}
\vspace{-10pt}
\end{figure*}
{We conduct extensive experiments to validate the effectiveness of the proposed approach. 
In the following, we first introduce our in-house pill identification dataset, called VAIPE, which will be used to evaluate the proposed approach, and then explain our evaluation metrics and experimental settings. 
To assess the effectiveness of the proposed method, we conducted comparative assessments against a number of established models, including the detection backbones we selected, such as Faster R-CNN~\cite{fasterrcnn} and YOLOv5 \cite{yolov5}, as well as other related frameworks such as SGRN~\cite{sgrn} and the Mask RCNN-based approach described in ~\cite{kwon_pill}. We also perform ablation studies to investigate the efficiency of key components in our framework.
% investigate some key properties of the method by analyzing its different components through our insightful analytical studies.
\subsection*{Dataset and Pre-processing}
\textbf{Motivation.}  {To the best of our knowledge, previous studies on the pill identification problem~\cite{tan2021comparison},~\cite{cure1},~\cite{Ling_2020_CVPR},~\cite{cure} only focus on datasets collected in constrained environments. For instance, existing datasets such as {NIH Dataset} \cite{nih_dataset} are constructed under ideal conditions in lighting, backgrounds, and equipment or devices. The CURE dataset~\cite{Ling_2020_CVPR} provides only one pill per image. Hence, these datasets do not reflect the real-world scenarios in which patients take an arbitrary number of drugs, and their environmental conditions (e.g., backgrounds, lighting conditions, mobile devices, etc.) are greatly varied. Additionally, many pills have nearly identical visual appearances. The fact that they appear alone in the images of these datasets will inevitably confuse the detection frameworks. Consequently, none of the existing datasets can be directly applied to the real-world pill detection problem or can only be applied with low reliability. There is no publicly available dataset of these pills images in which the pills follow intakes of actual patients. This limits the development of machine learning algorithms for the detection of pills from images as well as for building real-world medicine inspection applications. To address this challenge, we build and introduce a new, large-scale open dataset of pill images, which we called VAIPE. }

\textbf{Data Descriptor.} The VAIPE is a large-scale and open pill image dataset for visual-based medicine inspection. %The dataset contains $1,527$ prescriptions obtained from anonymous patients at $4$ major institutes in Vietnam between 2018 and 2020. 
The dataset contains approximately 10,000 pill images that were manually collected in unconstrained environments. In this study, no hypotheses or new interventional procedures were generated. Also, no investigational products or clinical trials were used for patients. In addition, there were no changes in treatment plans for any patients involved. Pill images were retrospectives collected, and all identifiable information of patients was de-identified. Therefore, there was no requirement for ethics approval~\cite{bworld}. %After carefully examining the data and patients' privacy, each prescription is labeled under the support of Vietnamese optical character recognition (OCR) models, namely, VietORC - a TransformerOCR framework \cite{transocr} trained with Vietnamese handwritten characters dataset. %Afterward, the medications are purchased in accordance with the relevant prescriptions; then, images are taken following the correct tablets for each user throughout the day. Specifically, prescriptions will mostly be separated by timeslots to take the medicines (\emph{morning}, \emph{noon} \emph{afternoon} and \emph{evening}). For each slot, patients should only take a subset of pills contained in the prescriptions, corresponding to the amount assigned. The images are taken following those timeslots in prescriptions, 

Pill images are collected in many different contexts (e.g., various backgrounds, lighting conditions, in-hand or out-of-hand, etc.) using smartphones. These images are then manually labeled using the information from the relevant prescriptions. In summary, the number of pills per image is about $5 - 10$, and the total number of pill images collected was $9,426$ pill images with $96$ independent pill labels. To train the proposed deep learning system, the pill images from the VAIPE dataset are resized so that the shortest edges have a size of $800$, with a limit of $1,333$ on the longer edge. The ratios are kept the same as the original images if the max size is reached, then downscale so that the longer edge does not exceed $1,333$.

\textbf{Data Validation.} Patient privacy was controlled and protected. In particular, all images were manually reviewed to ensure that all individually identifiable health information of the patients has been removed to meet the General Data Protection Regulation (GDPR) \cite{gdpr}. Annotations of pill images were also carefully examined. Specifically, all images were manually reviewed case-by-case by a team of 20 human readers to improve the quality of the annotations.

\textbf{Comparison with Existing Datasets.} Table \ref{tab:dataset_meta} provides a summary of the aforementioned datasets (including NIH, CURE, and VAIPE) together with other ones of moderate sizes, meta-data, and other properties. Compared to the two previous datasets, the VAIPE dataset is constructed under a much more flexible procedure that reflects the characteristic real-world data distributions. Hence, the introduced dataset can serve as a reliable data source for training \emph{generic pill detectors}. 


\subsection*{Evaluation Metrics}
{We evaluate the proposed method and other related works by the COCO APs metrics~\cite{coco_ap}. This set of metrics is widely accepted and used for evaluating state-of-the-art object detectors. Mean Average Precision (mAP), as its name suggests, is the mean of Average Precision (AP) overall $C$ classes and all the targeted IoU thresholds in the threshold set $T$ calculated by $ mAP = \frac{1}{C |T|} \sum_{i}^{C} \sum_{t \in T} AP_{i,t}$, 
%over recall values from 0 to 1 
% as follows
% \begin{equation}
%     \small
%     mAP = \frac{1}{C |T|} \sum_{i}^{C} \sum_{t \in T} AP_{i,t},
% \end{equation}
where Average Precision ($AP_{i,t}$) is the area under the Precision-Recall curve, calculated for the class $i$ at a given IoU threshold $t$.} %The mAP is the averaged result of APs over $C$ classes. It is worth noting that  COCO Evaluator makes no distinction between AP and mAP and assumes the difference is clear from the context. Hence, we use the term AP in place of mAP for consistency. 
\iffalse 
To estimate the $AP_{i,t}$, we need to measure whether a predicted instance is True Positive (TP) or False Positive (FP). We use the IoU (Intersect over Union) factor~\footnote{IoU is the ratio between intersected area over the union area of the two boxes} to evaluate the fitness of predicted bounding boxes compared with the ground-truth ones. If the IoU of a predicted instance is higher than the predefined IoU threshold, the prediction is set to True Positive and vice versa. 
In this work, besides the main metric, i.e., APs metrics, we also measure AP metrics with different area scales to estimate the robustness of our proposed method.
Table \ref{tab:map_metric} summarizes the set of COCO APs evaluation metrics. 

\begin{table}[!tb]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|l|l}
\toprule
& Metrics & Description   \\ \midrule
\multirow{3}{2cm}{Average Precision (AP)} & mAP      &  \begin{tabular}[l]{@{}l@{}} Averaged AP across all $10$ IoU threshold in \\set $T$ = $\overline{0.5,0.95}$, step $0.05$. \end{tabular}  \\
& AP50    & AP at given IoU thres = 0.5  \\
& AP75    & AP given IoU thres = 0.75 (strict metric)   \\ \midrule
\multirow{3}{2cm}{AP Across Scales}       
& APs     & AP for small object : area $<$ $32^2$  \\
& APm     & AP for medium object: $32^2\le$ area $<96^2$ \\
& APl     & AP for large object: area  $> 96^2$                             \\ \bottomrule
\end{tabular}
}
\caption{COCO APs \cite{coco_ap} evaluation metrics set.}\label{tab:map_metric}
\end{table}
\fi 
\subsection*{Comparison with state-of-the-art methods}
\textbf{Comparison Benchmarks.} {To show the effectiveness of the proposed method, we conducted a comparison with the state-of-the-art object detectors, including our detection backbones: Faster R-CNN~\cite{fasterrcnn}, YOLOv5 \cite{yolov5}, and related works: SGRN~\cite{sgrn}, Mask RCNN-based approach \cite{kwon_pill}. Throughout the literature, the baseline with which PGPNet presently integrates is Faster R-CNN \cite{fasterrcnn}; hence, the original framework is utilized for our comparison. 
We adopt two different CNNs and one Transformer-based module for visual feature extractor, namely ResNet-50-C4, ResNet-50-FPN and Swin Transformer V2 - SwinV2 \cite{swinv2}(Fig.~\ref{fig:generalflow}). Specifically, for two ConvNets, we use a single feature map produced by convolution block C4 of the ResNet-50 model in ResNet-50-C4. In ResNet-50-FPN, we replace C4's feature map with multi-scale feature maps produced by Feature Pyramid Network (FPN)~\cite{fpn}. As for the Swin Transformer module, we are currently utilizing the SwinV2-T configuration \cite{swinv2} to ensure that the number of model parameters is comparable to that of ResNet-50. 
In addition, we also make adaption for PGPNet with YOLOv5 \cite{yolov5} detection backbone. Two configurations of YOLOv5s and YOLOv5n are currently adopted. 
%For the Pseudo Classifier, we use the original YOLOv5 output layer, which produces three output values corresponding to the level of objectness (objectness in short), class probabilities, and bounding box coordinates. We treat each anchor in the image grid as a Region of Interest and perform threshold-cutting to choose $N$ of them for each image based on the level of objectness produced by the Pseudo Classifier, similar to the idea of Region Proposal in Faster R-CNN. In addition, the pseudo classification score for each \emph{chosen} RoI is now the product between its objectness scalar and the corresponding class probabilities. This adaptation makes all the logic for our PGPNet remain intact while still ensuring the superior characteristic of the one-step detection framework (e.g., end-to-end training pipeline, fast training-inference time, etc.).
Also, the most relevant frameworks compared with our PGPNet are also put into comparison: a representative approach that utilizes an external knowledge graph for Object Detection task~\cite{sgrn}; a Mask RCNN-based baseline that also proposed to solve the same task of multi-pill detection \cite{kwon_pill}. \\
\noindent For a fair comparison, a fixed set of hyper-parameters is used for PGPNet throughout all experiments.}

\subsection*{Implementation Details}
{We conduct all the experiments using the Pytorch (version 1.10.1) on an Intel Xeon Silver 4210 2.20GHz system with $2$ $\times$ NVIDIA GeForce RTX 3090 GPUs.  We train and test all targeted models on the training and testing sub-datasets provided in Table~\ref{tab:train_test_split}. Specifically, we initialize all the networks with the weights achieved by pre-training them on COCO 2017 dataset~\cite{coco}. We then train the models in $20,000$ iterations with a batch size of $16$. AdamW~\cite{adamw} optimizer is used with the initial learning rate of $0.001$. We also augment the training data by using simple techniques such as random horizontal and vertical flips to prevent overfitting. For our PGPNet implementation, we set the dimensions of node embeddings at $64$. We also design the Graph Transformer Module with only one layer and $10$ channel set.}

% \subsection{Experimental Results of PGPNet}
% This section would discuss about the actual performance of PGPNet, together with some baselines and other related works.
\section*{Experimental Results}
This section reports our experimental results. We evaluate the effectiveness of PGPNet in three aspects: robustness, reliability, and explainability. The details are described below. % First, we study the robustness of the proposed network across various object detection backbones. Second, we compare the PGPNet with existing external knowledge-assisted object detection approaches. Last, we investigate the explainability ability of the network by analyzing predictions produced by PGPNet.

% + Trust AI - (real dataset?)
% + Explainable AI: which component contribute to the result? Which metrics.

% + Q1: proposed framework can increase the accuracy (compare with baseline, related work, )
% + Q2: Graph can help we explain the result
%     + correct result can explain by the graph relation...==> back track the graph and features to show it correct.
%     + Other metrics...
\subsection*{Robustness and Reliability of PGPNet}
\label{subsec:eval_backbones}
\subsubsection*{Comparison with Faster R-CNN and YOLOv5}
\begin{table}[!tb]
\centering
\small
\setlength\tabcolsep{3pt} % default value: 6pt
\resizebox{0.8\columnwidth}{!}{%
\begin{tabular}{lll|cccccc}
\toprule
\multicolumn{3}{c|}{\textbf{Method}}                                      & \multicolumn{1}{c}{\textbf{mAP}} & \multicolumn{1}{c}{\textbf{AP50}} & \multicolumn{1}{c}{\textbf{AP75}} & \multicolumn{1}{c}{\textbf{APs}} & \multicolumn{1}{c}{\textbf{APm}} & \multicolumn{1}{c}{\textbf{APl}} \\ \midrule
\parbox[t]{2mm}{\multirow{7}{*}{\rotatebox[origin=c]{90}{\textcolor{red}{\textbf{Two-step}}}}} & 
\multicolumn{1}{l|}{\multirow{2}{*}{
    \begin{tabular}[l]{@{}l@{}} Faster R-CNN \\ (ResNet-50-C4)\end{tabular}}}  
    & Vanilla & 62.6               & 87.0                 & 74.4                  & 75.0                     & 58.3                 & 62.9                 \\
\multicolumn{2}{l|}{}                               & PGPNet       & \textbf{68.3 (+9.2\%)}       & \textbf{92.5}        & \textbf{81.7}          & \textbf{80.0}             & \textbf{64.3}         & \textbf{68.7}         \\
\cmidrule(lr){2-9}
& \multicolumn{1}{l|}{\multirow{3}{*}{\begin{tabular}[l]{@{}l@{}} Faster R-CNN \\ (ResNet-50-FPN)\end{tabular}}} & Vanilla & 63.7                & 86.6                & 76.9                   & 71.2                 & 58.1                & 64.6               \\
\multicolumn{2}{l|}{}                               & PGPNet       & \textbf{69.7 (+9.4\%)}       & \textbf{94.4}         & \textbf{83.4}         & \textbf{90.0}    & \textbf{66.4}        & \textbf{70.1}   \\
\multicolumn{2}{l|}{} &   &                &                     &                  &                    &                  &                 \\ 
\cmidrule(lr){2-9}
& \multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[l]{@{}l@{}} \duy{Faster R-CNN} \\ \duy{(SwinV2-T)}\end{tabular}}} & \duy{Vanilla} & \duy{59.7}                & \duy{84.5}                & \duy{72.3}                   & \duy{66.9}                 & \duy{54.0}                & \duy{60.1}               \\
\multicolumn{2}{l|}{}                               & \duy{PGPNet}       & \duy{\textbf{62.6 (+4.8\%)}}       & \duy{\textbf{87.2}}         & \duy{\textbf{75.5}}         & \duy{\textbf{68.6}}    & \duy{\textbf{56.6}}        & \duy{\textbf{62.9}}   \\
\midrule
\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{{\textbf{One-step}}}}} & \multicolumn{1}{l|}{\multirow{2}{*}{YOLOv5n}}  & Vanilla & 37.9 & 50.8 &45.4    & 87.5    & 49.1    & 38.3  \\
\multicolumn{2}{l|}{}                              & PGPNet  & \textbf{43.0 (+12.0\%)}   & \textbf{58.4}    & \textbf{51.3}    & \textbf{82.5}    & \textbf{52.4}    & \textbf{43.7}    \\ \cmidrule(lr){2-9}
& \multicolumn{1}{l|}{\multirow{2}{*}{YOLOv5s}} & Vanilla & 57.5 & 75.8 & 68.3 & 85.0 & 58.3 & 57.0 \\
\multicolumn{2}{l|}{}                              & PGPNet  & \textbf{63.4 (+10.2\%)} & \textbf{85.9} & \textbf{76.4} & \textbf{89.9} & \textbf{58.3} & \textbf{64.1} \\
\bottomrule
\end{tabular}
}
\caption{Comparison of detection performance of PGPNET with state-of-the-art object detectors (Vanilla)  on VAIPE dataset. %The \textcolor{red}{\textbf{red group}} represents two-step detectors. The {\textbf{blue group}} denotes one-step detectors. 
Best results are highlighted in \textbf{bold} text. \label{tab:eval_backbone}}
\vspace{-15pt}
\end{table}
%We compare our approach with the state-of-the-art object detectors (Vanilla), and SGRN~\cite{sgrn} in terms of detection performance. As mentioned in Section~{\ref{sec:method}}, PGPNet uses enhanced features, which are the combination of the visual features and the proposed context features to localize and classify the pills inside an image. In this work, we perform the comparison based on both the two-step object detection architecture, i.e., Faster R-CNN, and the one-step object detection architecture such as YOLOv5~\cite{yolov5} framework.

\begin{figure*}[!t]
\begin{minipage}{0.32\linewidth}
    % \begin{figure}[!tb]
        \centering
        \includegraphics[width=\columnwidth]{pirate_class.eps}
        \caption{Comparison of the PGPNet performance with the Faster R-CNN baseline over each individual class. \label{fig:eval_backbone_class}}
    % \end{figure}
\end{minipage}
\hspace{0.1cm}
\begin{minipage}{0.66\linewidth}
    % \begin{figure}[tbh]
    \centering
    \subfloat[Faster R-CNN \label{fig:reliable_frcnn}]{%
      \includegraphics[clip,width=0.32\columnwidth]{reliability_frcnn.eps}%
    }
    \hfill
    \subfloat[YOLOv5 \label{fig:reliable_yolo}]{%
      \includegraphics[clip,width=0.32\columnwidth]{reliability_yolo.eps}%
    }
    \hfill
    \subfloat[SGRN \label{fig:reliable_sgrn}]{%
      \includegraphics[clip,width=0.32\columnwidth]{reliability_sgrn.eps}%
    }
    \caption{Reliability investigation for PGPNet and different baseline performances.\label{fig:reliable_all}}
    % \vspace{0.3cm}
    % \end{figure}
\end{minipage}
\vspace{-0.5cm}
\end{figure*}
\textbf{Detection Performance.}
{Table~\ref{tab:eval_backbone} shows the experimental results of PGPNet and the state-of-the-art object detectors framework (Vanilla), e.g., Faster R-CNN (two-step detector), and YOLOv5~\cite{yolov5} (one-step detector) 
on the VAIPE dataset. As shown, PGPNet obtained better results than Faster R-CNN by large performance gaps for all evaluation metrics. Specifically, when using the ResNet-50-C4 model as the visual feature extractor model, the average precision mAP of Faster R-CNN was $62.6$, while that of PGPNet was $68.3$. The proposed method improves the performance over the baseline Faster R-CNN by $9.2\%$. 
Under strict metrics, e.g., AP75, PGPNet also outperforms Faster R-CNN $8-9\%$.
In addition, we observed similar behavior when using the ResNet-50-FPN model. The proposed PGPNet makes an improvement of $9.4\%$ for the mAP metrics. With a Transformer-based backbone, here a Swin Transformer V2 configuration - SwinV2-T ~\cite{swinv2}, the results are slightly worse compared to those produced by ResNet-based counterparts, for both the vanilla or PGPNet alternatives. However, PGPNet still show its superior when being install with this backbone, as the empirical result for AP metrics is improved by $4.8\%$ compared to the vanilla SwinV2-T Faster R-CNN model.}

%We also report the results of PGPNet and YOLOv5 with two different versions are reported in Table~\ref{tab:eval_backbone}. 
{For YOLOv5, PGPNet outperformed Vanilla by a significant margin across all performance metrics in both YOLO instances. Specifically, the average precision AP of the vanilla model with YOLOv5n was $37.9$ while that of PGPNet was $43.0$ ($12\%$ improvement). In the case of a larger alternative, YOLOv5s, a similar conclusion can be drawn, namely that PGPNet improves overall mAP metrics by $5.9$, e.g., $10.2\%$.}


% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=1\columnwidth]{performace_backbone.eps}
%     \caption{Comparison of PGPNet performance with Faster R-CNN baseline. \textcolor{red}{May not need}\label{fig:eval_backbone}}
% \end{figure}

%\textbf{Class-specific Detection Performance.} 
{Figure \ref{fig:eval_backbone_class} visualizes the AP %at $IoU=.50:.05:.95$ 
for all classes in the dataset when using Faster R-CNN as the backbone. The first three bins denote Faster R-CNN alternatives, and the later three are the corresponding PGPNet configurations. The dots in the figure represent AP values for classes; the vertical line is the indicator for the mean value, while the rectangle bar is the $95\%$ High-Density Interval (HDI) band. Apart from the fact that the mean AP over all classes of PGPNet variances is better than those produced by Faster R-CNN, we found that PGPNet also has more reliable and stable results over all classes.  
Specifically, PGPNet helps to improve the AP of classes that Faster R-CNN frequently confuses (the points with low APs in the blue and pink beans). As a result, the  three beans of Faster R-CNN exhibit a large variance, i.e.,  the AP ranged from $0$ to around $90$. In contrast, the beans of PGPNet performance are more condensed and have shorter tail, i.e., the AP ranged from  $40$ (or $50$) to around $90$.}
%In an upcoming experiment, we will discuss a particular instance of these classes and interpret the results produced by our PGPNet to demonstrate its efficacy. 
%In contrast with the two beans of Faster R-CNN that exhibit a great variance, the beans of PGPNet performance are more condensed. %Additionally, the thin rectangles of two PGPNet groups indicate a %condensed $95\%$ HDI with limited value ranges. 

\textbf{Pill Classification Accuracy.}
{To further investigate the robustness of the proposed PGPNet, we adopt the visualization techniques presented in~\cite{reliable} to understand the prediction accuracy ( of the pill classification task) better. In this technique, all models' predictions are categorized by their confidence scores into different bins, in which the average accuracy can be calculated. By observing the confidence-accuracy correlation, we can tell whether the models are under or over-confidence with their predictions~\cite{reliable}. Figure~\ref{fig:reliable_all}\subref{fig:reliable_frcnn} visualize those reliability plots of Faster R-CNN and PGPNet. It implies that both models have a propensity toward over-confidence, as the average accuracy of each confidence band is lower than the mean confidence score of that bin. However, that tendency is greatly alleviated in the circumstance of PGPNet, which means that the bins' heights are much closer to the perfect Confidence-Accuracy balance line (the red dashed diagonal line).
Figure~\ref{fig:reliable_all}\subref{fig:reliable_yolo} compares PGPNet's confidence-accuracy correlation and that of YOLOv5. With this backbone, we observed that the proposed PGPNet can produce predictions with a high level of reliability. All the heights of bins are much closer to values suggested by the perfectly-balanced line compared to Vanilla's result.}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=\columnwidth]{reliability_frcnn.eps}
%     \caption{Reliability investigation for PGPNet and Faster R-CNN performance. \label{fig:reliable_frcnn}}
% \end{figure}

%\subsubsection{Comparison with YOLOv5} To comprehensively evaluate the effectiveness of the proposed PGPNet, we adapt our proposed method by using a one-step detection architecture for the visual feature extractor.
%In this section, we compare the detection performance of PGPNet with the baseline framework (Vanilla), e.g., YOLOv5~\cite{yolov5} framework.

%\textbf{Architecture Adaptation.} 
%\textbf{Detection Performance.} The numerical results of PGPNet and YOLOv5 with two different versions are reported in Table~\ref{tab:eval_backbone}. In both instances, PGPNet outperformed YOLOv5 by a significant margin across all performance metrics. Specifically, the average precision AP of the vanilla model with YOLOv5n was $37.9$ while that of PGPNet was $43.0$ ($12\%$ improvement). In the case of a larger alternative, YOLOv5s, a similar conclusion can be drawn, namely that PGPNet improves overall mAP metrics by $5.9$, e.g., $10.2\%$.

%\textbf{Class-specific Performance.} 
%\begin{figure}[!tb]
%    \centering
%    \includegraphics[width=\columnwidth]{pirate_class_yolo.eps}
%    \caption{Comparison of the PGPNet performance with the YOLOv5 baseline over each individual class. \label{fig:eval_backbone_class_yolo}}
%\end{figure}


%\textbf{Pill Classification Accuracy.} Figure~\ref{fig:reliable_yolo} compares PGPNet's confidence-accuracy correlation and that of YOLOv5. With this backbone, we observed that the proposed PGPNet can produce predictions with a high level of reliability. All the heights of bins are much closer to values suggested by the perfectly-balanced line compared to the vanilla model's result.
% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=\columnwidth]{reliability_yolo.eps}
%     \caption{Reliability investigation for PGPNet and YOLOv5 performance. \label{fig:reliable_yolo}}
% \end{figure}

% \begin{table*}[]
% \centering
% \small
% % \setlength\tabcolsep{8pt}
% \begin{tabular}{ll|cccccc}
% \toprule
% \multicolumn{2}{c|}{Backbone}                                & AP   & AP50 & AP75 & APs  & APm  & APl \\ \midrule
% \multicolumn{1}{l|}{\multirow{2}{*}{YOLOv5n}}  & Vanilla & 37.9 & 50.8 &45.4    & 87.5    & 49.1    & 38.3  \\
% \multicolumn{1}{l|}{}                              & PGPNet  & \textbf{43.0 (+12.0\%)}   & \textbf{58.4}    & \textbf{51.3}    & \textbf{82.5}    & \textbf{52.4}    & \textbf{43.7}    \\ \midrule
% \multicolumn{1}{l|}{\multirow{2}{*}{YOLOv5s}} & Vanilla & 57.5 & 75.8 & 68.3 & 85.0 & 58.3 & 57.0 \\
% \multicolumn{1}{l|}{}                              & PGPNet  & \textbf{63.4 (+10.2\%)} & \textbf{85.9} & \textbf{76.4} & \textbf{89.9} & \textbf{58.3} & \textbf{64.1} \\ \bottomrule
% \end{tabular}
% \caption{Comparison of PGPNet performance with different YOLOv5 baselines. \label{tab:eval_yolo}}
% \end{table*}

%\subsection{PGPNet's Reliability}
%Superiority over Existing External Knowledge-assisted Object Detection Benchmark}
\subsubsection*{Comparison with Existing Relavant Frameworks}
{Our work is the first to leverage an external graph in dealing with the Pill Detection challenge; thus, none of the preceding works are genuinely tight-correlated. Indeed, earlier researches only shared some common ground to our approach: (1) About methodology or (2) about research problem.

\noindent For the first group, there are works that utilized external information to solve the Object Detection problem. We adopt one of the most current studies with this direction - \cite{sgrn} to solve our targeted problem and serve as  \duy{a} baseline for PGPNet. 
% Briefly, aside from the majority of studies that construct graphs using external data (handcrafted linguistic knowledge, etc.) or by implicitly learning a fully-connected graph between regions of interest (RoIs), 
Spatial-aware Graph Relation Network (SGRN) \cite{sgrn} is a framework that adaptively discovers and incorporates key semantic and spatial relationships for reasoning over each RoI.

\noindent With respect to research problem, as stated earlier, while there are many works which target single-pill detection problem ~\cite{wong2017development, usuyama2020epillid, ling2020few}, only a few directly solve the task of detecting multiple pills per image \cite{Ou2020, kwon_pill}. We attempt to adopt the most recent technique proposed in \cite{kwon_pill} as another baseline to compare with PGPNet. In the original work, the authors purpose is somewhat different from us, since they attempt to develop a framework which is solely trained on single-pill images, since they argued that the multi-pill dataset would scale up exponentially if the number of pills inscrease. This argument is not held in our intuition, and we believe, in reality, since the pills taken together have to be prescribed by pharmacists. We keep the pipeline as the original work, with some adoption for working with our VAIPE dataset: (1) Change Mask R-CNN to Faster R-CNN; (2) The training single-pill dataset is cropped from our VAIPE dataset with bounding box annonations; (3) The automate data labeling process are skipped. Since the original work did not name the proposed pipeline, we called it as \emph{Kwon's Pipeline} for short.
}
%Due to this, SGRN requires no extra information and hence can be installed with minor changes compared to the original public work, and thus remains its original strength.
% \begin{table}[!tb]
% \centering
% \footnotesize 
% \setlength\tabcolsep{4pt} % default value: 6pt
% % \resizebox{\textwidth}{!}{%
% \begin{tabular}{l|cccccc}
% \toprule
% \textbf{Framework}    & \multicolumn{1}{c}{\textbf{mAP}} & \multicolumn{1}{c}{\textbf{AP50}} & \multicolumn{1}{c}{\textbf{AP75}} & \multicolumn{1}{c}{\textbf{APs}} & \multicolumn{1}{c}{\textbf{APm}} & \multicolumn{1}{c}{\textbf{APl}} \\ \midrule
% Faster R-CNN & 63.71               & 86.66                 & 76.92                   & 71.26                 & 58.10                  & 64.59                 \\
% SGRN \cite{sgrn}        & 65.88                  & 88.83                    & 79.64                    & 76.31                   & 61.58                   & 66.28                   \\
% PGPNet  (\textbf{ours})     & \textbf{69.70}       & \textbf{94.41}         & \textbf{83.38}         & \textbf{90.00}             & \textbf{66.45}        & \textbf{70.05}        \\ \bottomrule
% \end{tabular}
% % }
% \caption{Performance comparison of PGPNet with Faster R-CNN and SGRN.
% % {Could not think of a way to combine with table 4 - yet} 
% \label{tab:eval_related}}
% \end{table}

\textbf{Detection Performance.} 
\begin{table}[t]
\centering
% \footnotesize 
\setlength\tabcolsep{5pt} % default value: 6pt
\begin{tabular}{l|llllll}
\toprule
\textbf{\duy{Model}}           & \textbf{\duy{ mAP}}                                              & \textbf{\duy{ AP50}}                                             & \textbf{\duy{ AP75}}                     & \textbf{\duy{ APs}}                      & \textbf{\duy{ APm}}                                              & \textbf{\duy{ APl}}                      \\ \midrule
\duy{ Faster RCNN}     & \duy{ 63.7}                                             & \duy{ 86.6}                                             & \duy{ 76.9}                     & \duy{ 71.2}                     & \duy{ 58.1}                                             & \duy{ 64.6}                     \\
\duy{ SGRN}            & \duy{ 65.9}                                             & \duy{ 88.8}                                             & \duy{ 79.6}                     & \duy{ 76.3}                     & \duy{ 61.6}                                             & \duy{ 66.3}                     \\
\duy{ Kwon's Pipeline} & \duy{36.2} & \duy{38.5} & \duy{37.2} & \duy{30.3} & \duy{33.1} & \duy{36.0} \\
\duy{ PGPNet}          & \textbf{\duy{ 69.7}}                                             & \textbf{\duy{ 94.4}}                                             & \textbf{\duy{ 83.4}}                     & \textbf{\duy{ 90.0}}                     & \textbf{\duy{ 66.4}}                                             & \textbf{\duy{ 70.1}}                     \\ \bottomrule
\end{tabular}
\caption{Performance comparison of PGPNet with SGRN and Kwon's Pipeline.}\label{tab:eval_related}
\end{table}
Table \ref{tab:eval_related} summarizes the comparison of PGPNet, SGRN and Kwon's Pipeline when 
adopting the visual feature extractor architecture from Faster R-CNN with the Resnet-50-FPN model.
%, we add the result of Faster R-CNN as the reference.
%Through the result in Section~\ref{subsec:eval_backbones}, we observe that with the use of Resnet-50-FPN, Faster R-CNN and PGPNet can achieve a more robust result while remaining at an acceptable speed. 
%Hence, from now on, all the result with Faster R-CNN architecture are carried out with the Resnet-50-FPN.
Clearly, SGRN outperforms the baseline Faster R-CNN in terms of overall performance but could not outperforms our proposed method PGPNet. 
Specifically, the mAP metrics achieved by SGRN is $65.9$, and PGPNet achieves the better score with a gap of nearly $4$. Upon other metrics, AP50, AP75, APs, APm, and  APl, PGPNet shows its superior by enhancing the performance from $5.1\%$ (e.g., in AP75 metrics) up to $17.1\%$ (e.g., in APs metrics). This is an expected result because SGRN reveals a major weakness when applying to the challenge of Pill Detection. The spatial relationships between pills in an image are arbitrary and frequently changed. Such noisy and unreliable information leads to the performance of SGRN being unstable and sometimes produce not good enough results. In the case of Kwon's Pipeline, the situation is even worse, since it cannot even beat the vanilla one-step Faster RCNN trained with mutple-pill VAIPE training set. The result of this pipeline is $43.1\%$ and $48.2\%$ worse than vanilla Faster R-CNN and PGPNet respectively. One reason for this deficiency is owing to the quality of its training data. There are many circumstances in which overlap or occlusion occurs, which make the cropped images also contain parts of other pills.

\textbf{Pill Classification Accuracy.} 
Figure \ref{fig:reliable_all}\subref{fig:reliable_sgrn} shows the correlation between the confidence and accuracy of PGPNet in the comparison with those of SGRN. Both the frameworks are based on the Faster R-CNN backbone and achieve similar results e.g., an over-confidence trend in every bin. All the predictions with confidence scores smaller than $0.2$ are totally unreliable (with $0$ accuracy). In addition, PGPNet also shows its superior over SGRN in some bins, in which the over-confidence situation is reduced effectively. We do not plot the Confidence-Accuracy of Kwon's Pipeline owing to space constraint and the obvious performance gap compared to our PGPNet.

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=\columnwidth]{reliability_sgrn.eps}
%     \caption{Reliability investigation for PGPNet and SGRN performance. \label{fig:reliable_sgrn}}
% \end{figure}

\subsubsection*{Ability in Dealing with Hard Samples}
\begin{figure*}[!t]
\begin{minipage}{0.23\linewidth}
    % \begin{figure}[!bt]
    \centering \small
        \includegraphics[width=\columnwidth]{occlusion.pdf}
        \caption{Images with occlusion phenomena in custom occlusion dataset. The rectangles depict examples of tablets with overlapping boundary boxes.
        \label{fig:heavy_occlusion}}
    % \end{figure}
\end{minipage}
\hspace{0.1cm}
\begin{minipage}{0.3\linewidth}
    %\begin{table}[!bt]
    \centering
    \setlength\tabcolsep{3pt} % default value: 6pt
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{l|cc||cc}
        \toprule
        \multicolumn{1}{c|}{\begin{tabular}[]{@{}c@{}} Test \\dataset \end{tabular}} & 
        \multicolumn{2}{c}{\begin{tabular}[]{@{}c@{}} Custom \\ Occlusion \end{tabular}} & 
        \multicolumn{2}{c}{\begin{tabular}[]{@{}c@{}} Non-\\Occlusion \end{tabular}} \\
        \midrule
        \multicolumn{1}{c|}{Method} & 
        \begin{tabular}[]{@{}c@{}} Faster \\ R-CNN \end{tabular} &  PGPNet & 
        \begin{tabular}[]{@{}c@{}} Faster \\ R-CNN \end{tabular} & PGPNet \\
        \midrule
        mAP     & 59.2 & \textbf{67.5} &   65.6 &   \textbf{71.7} \\
        AP50    & 76.5 & \textbf{81.1} &   87.4 &   \textbf{92.9} \\
        AP75    & 68.9 & \textbf{76.4} &   80.8 &   \textbf{87.0} \\
        APs     & -    & -             &   80.0 &   \textbf{90.0} \\
        APm     & 61.6 & \textbf{68.3} &   56.5 &   \textbf{64.7} \\
        APl     & 60.7 & \textbf{70.1} &   65.1 &   \textbf{70.6} \\
        \bottomrule
        \end{tabular}
    }
    \captionof{table}{Impact of heavy occlusion images on testing performance of PGPNet and Faster R-CNN.\label{tab:occlusion}}
    %\end{table}
\end{minipage}
\hspace{0.1cm}
\begin{minipage}{0.2\linewidth}
    % \begin{figure}[!bt]
        \centering
        \includegraphics[width=\columnwidth]{pirate_occlusion.eps}
        \caption{Comparison of PGPNet performance with Faster R-CNN over each individual class in occlusion dataset. \label{fig:occlusion}}
    % \end{figure}
\end{minipage}
\hspace{0.1cm}
\begin{minipage}{0.2\linewidth}
    % \begin{figure}
    \centering
        \includegraphics[width=1\columnwidth]{misclassified.pdf}
        \caption{Some sample pills with very identical visual appearance with \emph{Hexinvon-8mg}\label{fig:misclassified}}
    % \end{figure}
\end{minipage}
\vspace{-0.2cm}
\end{figure*}
{In the following, we investigate the ability of PGPNet in dealing with the occlusion phenomenon caused by overlapping pills, which is one of the most critical issues in dealing with multi-pill detection. 
To this end, we create a so-call \emph{custom occlusion sub-dataset} of VAIPE, which contains images with heavy occlusion phenomena, i.e., having at least two RoIs with the IoU beyond 30\% (Fig.\ref{fig:heavy_occlusion}).  We also create 
a custom \emph{custom non-occlusion sub-dataset}
which contains samples that are in the same classes that appear in the \emph{custom occlusion sub-dataset} but with no occlusion. 
% we experiment on a sub-dataset of VAIPE, which only contains images with heavy occlusion phenomena. We define such image samples as samples that have at least one highly-overlapping couple of bounding boxes, i.e., two boxes that have the IoU (Intersect over Union) greater than $0.3$. Figure \ref{fig:heavy_occlusion} illustrates some images in this sub-dataset. 
The quantitative result is summarized in Table \ref{tab:occlusion}. 
The (-) mark in the table suggests the disregarded or unavailable metrics.
As the numbers suggest, even in cases where heavy occlusion occurs, PGPNet still shows its superior over Faster R-CNN. Specifically, the mAP over all classes in the \emph{custom occlusion sub-dataset} suggests a gap of $8.3\%$ between the two approaches. Interestingly, with the aid of classifier weight as the distinguishing characteristic for each class, PGPNet, even when dealing with occlusion cases still enhances the performance of $1.9\%$ compared to Faster R-CNN handling  the non-occlusion case (e,g, $67.5$ vs. $65.6$, respectively). Figure \ref{fig:occlusion} provides more information about the AP for each class in the \emph{custom occlusion sub-dataset}. PGPNet still outperforms Faster R-CNN in most cases with a large gap, and also produces a more reliable result by introducing a smaller variance over the AP metrics. }

\subsection*{PGPNet's Explainability}

\begin{figure*}[!t]
\begin{minipage}{0.32\linewidth}
% \begin{figure*}
        \centering
        \subfloat[Faster R-CNN]{ %
            \includegraphics[width=0.43\columnwidth]{frcnn_case1.png}
            % \caption{Movement of the Filter Kernel over image receptive fields.}
            \label{fig:eval_rcnn}
        }
        % \hspace{4em}
        \subfloat[PGPNet]{ %
            \includegraphics[width=0.43\columnwidth]{KGPNet_case1.png}
            \label{fig:eval_kgp}
            % \caption{\texttt{Conv} operation applied at a specific location.}
        }
        \caption[Predictions for a hard sample made by Faster R-CNN and PGPNet given the same image.]{Predictions for a hard sample made by Faster R-CNN and PGPNet given the same image.}
        %\textbf{\subref{fig:eval_rcnn}} - Faster R-CNN; 
        %\textbf{\subref{fig:eval_kgp}} - PGPNet.} 
        \label{fig:kg_robust_instance}
        \vspace{0.1cm}
        % \centering
        % \includegraphics[width=\columnwidth]{pseudo_scores.pdf}
        % \caption{Probabilistic scores produced by PGPNet's Pseudo Classifier.}
        % \label{fig:pseudo_scores}
% \end{figure*}
\end{minipage}
\hspace{0.1cm}
\begin{minipage}{0.64\linewidth}
    % \begin{figure*}
        \centering \small
        \subfloat[Input]{ %
            \includegraphics[width=0.21\columnwidth,valign=c]{original.png}
            % \caption{\textit{Input}}
            \label{fig:original}
        }
        \hspace{-0.15em}%
        % \hspace{4em}%
        \centering
        \subfloat[LIVOLIN-FORTE]{ %
            \includegraphics[width=0.21\columnwidth,valign=c]{ex_1.png}
            % \caption{\textit{LIVOLIN-FORTE}}
            \label{fig:exp_1}
        }
        \hspace{-0.15em}%
        % \hspace{4em}%
        \subfloat[Hapenxin]{ %
            \includegraphics[width=0.21\columnwidth,valign=c]{ex_2.png}
            \label{fig:exp_2}
            % \caption{\textit{Hapenxin}}
        }
        \hspace{-0.15em}%
        % \hspace{4em}%
        \subfloat[Hexinvon-8mg]{ %
            \includegraphics[width=0.21\columnwidth,valign=c]{ex_3_2.png}
            \label{fig:exp_3}
            % \caption{\textit{Hexinvon-8mg}}
        }
        % \includegraphics[width=0.8\textwidth]{conv_opt.png}
        \caption{The saliency maps for each of the groundtruth labels included in the image instance. For simple samples (LIVOLIN-FORTE and Hapenxin), the classifier focuses on the exact location of the tablets to determine their identity. In contrast, for the hard case (Hexivon-8mg), information on both Hexivon and LIVOLIN-FORTE served as evidence. \label{fig:XAI_heatmap} }
    % \end{figure*}
\end{minipage}
\vspace{-0.5cm}
\end{figure*}


%\subsubsection{Interpreting the Detection Results}
% \begin{figure}[tbh]
% \centering
% \begin{subfigure}{\textwidth}
%   \centering
%   % include first image
%   \includegraphics[width=\linewidth]{corr_frcnn.pdf}  
%   \caption{Faster R-CNN}
%   \label{fig:corr_frcnn}
% \end{subfigure}
% \begin{subfigure}{\textwidth}
%   \centering
%   % include second image
%   \includegraphics[width=\linewidth]{corr_kgp.pdf}  
%   \caption{PGPNet}
%   \label{fig:corr_kgp}
% \end{subfigure}
% \caption{Confusion Matrices of predicted labels made by two framework without and with MCG Leverage}
% \end{figure}

% \begin{figure}[tbh]
% \subfloat[Faster R-CNN]{%
%   \includegraphics[clip,width=\columnwidth]{corr_frcnn.pdf}%
%   \label{fig:corr_frcnn}
% }
% \vspace{10pt}
% \subfloat[PGPNet]{%
  % \includegraphics[clip,width=\columnwidth]{corr_kgp.pdf}%
  % \label{fig:corr_kgp}
% }
% \caption{Confusion Matrices of predicted labels made by two framework without and with MCG Leverage. \textcolor{red}{Need a bigger zoom-out} {Need to find different type of plot for better visualization. If use only main diagonal - cannot see which class is often miscassified as which class}}
% \end{figure}
This section is dedicated to analyzing the results produced by PGPNet through a specific sample. This example demonstrates that the operation of PGPNet is very congruent with our initial motivation and that our designed architecture can materialize this motivation.
%This section is dedicated for analysing the results produced by PGPNet through a specific sample. With this sample, the working of PGPNet can be shown to be in high accordance with our initial motivation, and our designed architecture can materialize that motivation.

% Figure \ref{fig:corr_frcnn} and \ref{fig:corr_kgp} respectively display the label confusion matrix produced by Faster R-CNN and PGPNet on testing dataset. The two confusion matrics are \textit{row-wisedly normalized} before being used for plotting. In addition, all the zero values are \textit{disregarded} in this visualisation, hence having white color. By observing the main diagonal of the matrix, it can be seen that there are many labels which are still misclassified (in red color) made by Faster R-CNN. The situation is much positive with PGPNet, most of the labels are correctly classified and the confusion cases are greatly reduced.

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=1\columnwidth]{misclassified.pdf}
%     \caption{Some sample pills with very identical visual appearance with \emph{Hexinvon-8mg}}
%     \label{fig:misclassified}
% \end{figure}

\subsubsection*{Experiment settings.} 
In this experiment, we choose a hard sample, namely \emph{Hexinvon-8mg}, with a relatively common appearance, for investigation. Figure \ref{fig:misclassified} visualizes \emph{Hexinvon-8mg} together with other pills in our dataset with almost identical visual appearance (round shape, white tint, etc.). 
As illustrated, these pills are readily confused with \emph{Hexinvon-8mg}.
%due to their almost identical physical characteristics (round shape, white tint, etc.).  
Indeed, Fig.~\ref{fig:kg_robust_instance} depicts an example in which  \emph{Hexinvon-8mg} is miscategorized as \emph{Alpha-Chymotrypsine} by Faster R-CNN. 
Our PGPNet can, however, successfully distinguish \emph{Hexinvon-8mg} with a high confidence score.
In the following, we applied several Explainable AI techniques to explain the results inferred by our PGPNet. The image of interest consists of three pills: LIVOLIN-FORTE,  Hapenxin, and Hexivon as shown in Fig.~\ref{fig:kg_robust_instance}. 

% For PGPNet, by observing the row of \emph{Hexinvon-8mg}, it can be seen that the problem is successfully alleviated with the aid of MCG graph. 
%, while in the case of Faster R-CNN, this pill is miscategorized as \emph{Alpha-Chymotrypsine}. 
% Though the positive result of PGPNet can straightforwardly be captured here, little can we conclude about our model's reason for producing that correct output. To this end, we applied several Explainable AI techniques on the computation pathway for this image instance.

% \begin{figure}
%     \centering
%     \subfloat[]{ %
%         \includegraphics[width=0.45\columnwidth]{frcnn_case1.png}
%         % \caption{Movement of the Filter Kernel over image receptive fields.}
%         \label{fig:eval_rcnn}
%     }
%     \hfill
%     \subfloat[]{ %
%         \includegraphics[width=0.45\columnwidth]{KGPNet_case1.png}
%         \label{fig:eval_kgp}
%         % \caption{\texttt{Conv} operation applied at a specific location.}
%     }
%     % \includegraphics[width=0.8\textwidth]{conv_opt.png}
%     \caption[Predictions for a hard sample made by Faster R-CNN and PGPNet given the same image.]{Predictions for a hard sample made by Faster R-CNN and PGPNet given the same image.
%         \textbf{\subref{fig:eval_rcnn}} - Faster R-CNN; 
%         \textbf{\subref{fig:eval_kgp}} - PGPNet.} 
%     \label{fig:kg_robust_instance}
%     \vspace{-10px}
% \end{figure}

\subsubsection*{Explanation of the Prediction Results}
We adopt the Excitation Backpropagation technique proposed by Zhang \cite{exb} to construct the saliency maps (Fig.\ref{fig:XAI_heatmap}), which indicate what the classifier has learned to produce the final results.
Firstly, for the easy samples, i.e., LIVOLIN-FORTE and Hapenxin, our model focuses precisely on those pill regions to make the prediction decision. 
In contrast, in the case of the hard sample, i.e., \emph{Hexinvon-8mg}, however, two regions are highlighted: one at the position of \emph{Hexinvon-8mg} and the other at the location of LIVOLIN-FORTE.
It indicates that the classifier solely requires information about LIVOLIN-FORTE and Hapenxin to identify these pills. Nevertheless, for \emph{Hexinvon-8mg}, the classifier must additionally incorporate information about its neighbor, i.e., LIVOLIN-FORTE.
This hypothesis is also supported by the Probabilistic score matrix shown in Fig.~\ref{fig:pseudo_scores}.
The probabilistic score matrix represents the prediction results generated by our Pseudo Classifier, which relies mainly on the pill's visual characteristics.
As demonstrated, Pseudo Classifier can accurately detect the proper labels of two simple samples, with their prediction scores approaching $1$,  and boost up their neighbors' probabilities (label ID $7$, $17$, etc.). However, with the case of \emph{Hexinvon-8mg}, the probability scores are relatively low, with all RoIs being investigated achieving scores of only about $0.3$.
% We adopt the Excitation Backpropagation technique proposed by Zhang \cite{exb} to compute the saliency maps (Fig.\ref{fig:XAI_heatmap}). 
% This figure figures out what has been learned by the classifier (i.e., the hilighted regions in the hear) to produce the final results. 
% Firstly, for the easy cases, i.e., LIVOLIN-FORTE and Hapenxin, our model focuses exactly on those pill region to make the prediction decision. 
% In the contrast, in the case of the hard sample, i.e., Hexivon, there are two concentrated regions, one is at the location of Hexivon, and the other is at the location of LIVOLIN-FORTE.
% It means that, the classifier needs only information related to LIVOLIN-FORTE and Hapenxin to identify these pills; however, for Hexivon, the classifier need to takes into account information of both Hexivon and LIVOLIN-FORTE. 
% This hypothesis is also supported by the Probabilistic score matrix shown in Fig.~\ref{fig:pseudo_scores}.
% This probabilistic score matrix depicts the prediction results made by our Pseudo Classifier which is mainly relies on the pill visual features. 
% As shown, Pseudo Classifier can effectively determine the correct labels of two easy samples, with their prediction scores approach $1$, as well as boost up their neighbors' probabilities (label ID $7$, $17$, etc.). However, with the case of \emph{Hexinvon-8mg}, the scores for this label are relatively low - about $0.3$ through all RoIs being investigated. 

Now, we utilize another explainable AI technique named GNNExplainer \cite{gnn_explainer} to investigate further the reason for identifying the hard sample, \emph{Hexinvon-8mg}. GNNExplainer is a model-agnostic architecture that can provide interpretable explanations for predictions of graph-based models. Specifically, GNNExplainer may identify a subgraph and a subset of node features that have a significant role in the prediction outcomes.
In our experiment, we treat our Graph Transformer Network as a module that produces regression output, i.e., the context vectors corresponding to all RoIs. For a more comprehensible result, we set the number of RoIs selected from the RPN module to ten, consisting of the five RoIs with the greatest \emph{objectness} scores and the other five with the lowest score. 
We utilize GNNExplainer to identify the sub-graph that contributes the most in recognizing \emph{Hexinvon-8mg}. 
The results are demonstrated in Fig.~\ref{fig:gnn_saliency}. 
In this figure, the white box depicts the RoI of \emph{Hexinvon-8mg}, the two orange boxes and blue boxes represent the RoIs of LIVOLIN-FORTE, and Hapenxin, respectively, while the five gray boxes indicate the RoIs of noise.
The black edges represent the vital connections, whose weights are proportionate to the width of the edges.
First, there are almost no edges between the nodes representing \emph{Hexinvon-8mg} and those of the noise RoIs.
It implies that the noise RoIs do not cue the prediction of \emph{Hexinvon-8mg}.
In contrast, there are bolded linkages between the RoIs of LIVOLIN-FORTE, Hapenxin, and \emph{Hexinvon-8mg}. 
These findings, along with the saliency map (Fig.\ref{fig:XAI_heatmap}), interpret that PGPNet has learned both the visual characteristic of the pill itself and the relationship between that pill and the others to make the final decision.

% have the maximum \emph{objectness scores} among those proposed by RPN after Non-maximum Suppression-based post-processing procedure, while $5$ remainings represents the RoIs with least scores. 
% As GNNExplainer suggests, the most critical nodes for generating this context presentation relate to RoIs that include genuine neighbor pills, and all unnecessary background RoIs are efficiently filtered out by GTN in the computation process. In addition, regarding the edges, while there are still some noisy edges which link the actual labels' nodes to the background ones, the majority and the most important ones are the connections that directly connect \emph{Hexinvon-8mg}'s RoI node with the true labels neighbors. 
% In addition, with this experiment, we only target finding the optimal sub-graph structure that directly determine a given output context presentation, without figure out which features affect the result most.  
% Figure \ref{fig:gnn_saliency} demonstrates the sub-graph that mostly contribute to the construction of context vector for \emph{Hexinvon-8mg}'s RoI. 
% The edges in black denote the important connections, whose weights are proportional to the edges' width. As GNNExplainer suggests, the most critical nodes for generating this context presentation relate to RoIs that include genuine neighbor pills, and all unnecessary background RoIs are efficiently filtered out by GTN in the computation process. In addition, regarding the edges, while there are still some noisy edges which link the actual labels' nodes to the background ones, the majority and the most important ones are the connections that directly connect \emph{Hexinvon-8mg}'s RoI node with the true labels neighbors. 

% \textbf{Pseudo Classifier Interpretation.} First, we visualize the pseudo probabilities produced by our Pseudo Classifier in Figure \ref{fig:pseudo_scores}. Here, \emph{LIVOLIN-FORTE} corresponds to label ID $63$, \emph{Hapenxin} with ID $57$, and $59$ is the hard label \emph{Hexinvon-8mg}. As the figure suggests, with the aid of our auxiliary loss, Pseudo Classifier can effectively determine the correct labels of two easy samples - with their prediction scores approach $1$, as well as boost up their neighbors' probabilities (label ID $7$, $17$, etc.). However, with the case of \emph{Hexinvon-8mg}, the scores for this label are relatively low - about $0.3$ through all RoIs being investigated. This result is reasonable and compatible with our motivation stated at the beginning, as the model is confused if only relying on its visual features. We further verify this claim with the visualization of Saliency maps in Figure \ref{fig:kg_saliency}. Here, we adopt the technique proposed by Zhang \cite{exb} to compute the saliency maps - Excitation Backpropagation. The hot regions in these maps indicate that for both \emph{Hexinvon-5mng} - our desired label and \emph{Loratadine-10mg} - another label with similar visual appearance, our framework up to the Pseudo Classifier can successfully determine the correct spatial regions that are important to this two labels, other than the noise or background in the image. However, the hot areas in both salience maps collide in one same white-round pill region, which indicates the model uncertainty about what is the exact label corresponding to those features. 



\begin{figure*}[!t]
\begin{minipage}{0.4\linewidth}
    % \begin{figure}
        \centering
        \includegraphics[width=0.9\columnwidth]{pseudo_scores.pdf}
        \caption{Probabilistic scores produced by PGPNet's Pseudo Classifier. \label{fig:pseudo_scores}}
    % \end{figure}
\end{minipage}
\hspace{0.1cm}
\begin{minipage}{0.55\linewidth}
% \begin{figure}
        \centering \small
        \subfloat[Bounding boxes of the RoIs.]{ %
            \includegraphics[width=0.4\columnwidth,valign=c]{rpn_10.png}
            % \caption{\textit{Input}}
            \label{fig:rpn_choose10}
        }
        \hfill%
        % \hspace{4em}%
        \centering
        \subfloat[Sub-graph identified by GNNExplainer.]{ %
            \includegraphics[width=0.5\columnwidth,valign=c]{salience_gnn.pdf}
            % \caption{\textit{LIVOLIN-FORTE}}
            \label{fig:gnn_saliency}
        }
        % \includegraphics[width=0.8\textwidth]{Figure/conv_opt.png}
         \caption{Interpretation of the prediction result for \emph{Hexinvon-8mg} using GNNExplainer.  \textbf{(\subref{fig:gnn_saliency})} indicate the RoIs in \textbf{(\subref{fig:rpn_choose10})} most influential to the prediction of \emph{Hexinvon-8mg}. \label{fig:gnn_explain}} 
    
    % \centering
    % \begin{subfigure}[t]{0.4\columnwidth}
    % \includegraphics[width=\columnwidth,valign=c]{Figure/rpn_10.png}
    %     \caption{Bounding boxes of the RoIs.}
    %     \label{fig:rpn_choose10}
    % \end{subfigure}
    % % \subfloat[]{ %
    % % }
    % \hfill%
    % \begin{subfigure}[t]{0.5\columnwidth}
    %     \includegraphics[width=\columnwidth,valign=c]{Figure/salience_gnn.pdf}
    %     \caption{Sub-graph identified by GNNExplainer} %, indicating the RoIs most influential to the prediction of \emph{Hexinvon-8mg}.
    %     \label{fig:gnn_saliency}
    % \end{subfigure}
    % \subfloat[]{ %
        
    % }
    % \includegraphics[width=0.8\textwidth]{Figure/conv_opt.png}
   
    % \label{fig:gnn_saliency}
% \end{figure}
\end{minipage}
% \hspace{0.1cm}
% \begin{minipage}{0.25\linewidth}
%     \centering
% \end{minipage}
\vspace{-0.3cm}
\end{figure*}

\begin{figure}[tb]
    \begin{subfigure}[t]{0.48\columnwidth}
            \includegraphics[width=0.9\textwidth]{ridgeline_edge_exp.eps}
            \caption{Distributions of Average Precision recorded over all classes produced by PGPNet with different MCG versions. \label{fig:edge_modify}}
    \end{subfigure}
    \hfill%
    \begin{subfigure}[t]{0.48\columnwidth}
        \includegraphics[width=0.9\columnwidth]{ridgeline_nodes_exp.eps}
        \caption{Distributions of Average Precision recorded over the classes in $N_A$ set produced by PGPNet with different MCG versions. \label{fig:node_modify}}
    \end{subfigure}
    \caption{Empirical result of node set and edge set modification.}
\end{figure}


% \begin{figure}
%     \centering
%     \subfloat[]{ %
%         \includegraphics[width=0.45\columnwidth]{sal_loratadine.png}
%         % \caption{Movement of the Filter Kernel over image receptive fields.}
%         \label{fig:salience_loratadine}
%     }
%     \hfill
%     \subfloat[]{ %
%         \includegraphics[width=0.45\columnwidth]{sal_hexinvon.png}
%         \label{fig:salience_hexivon}
%         % \caption{\texttt{Conv} operation applied at a specific location.}
%     }
%     % \includegraphics[width=0.8\textwidth]{conv_opt.png}
%     \caption[Saliency maps for the scores produced by Pseudo Classifier corresponding to two confusing labels.]{Saliency maps for the scores produced by Pseudo Classifier corresponding to two confusing labels.
%         \textbf{\subref{fig:salience_loratadine}} - Loratadine-10mg; 
%         \textbf{\subref{fig:salience_hexivon}} - Hexinvon-8mg.} 
%     \label{fig:kg_saliency}
%     \vspace{-10px}
% \end{figure}

% \textbf{Graph Transformer Network Interpretation.} Having gained valuable insights into the operation of the Pseudo Classifier, we analyze the actual operation of the Graph Transformer Network in order to evaluate its effectiveness in generating context vectors for each RoI - especially in this case, the RoI corresponding to \emph{Hexivon-8mg}. The technique proposed by Ying and partners \cite{gnn_explainer} is leveraged for this purpose. With this experiment, we treat our Graph Transformer Network as a module that produce regression output - the context vectors corresponding to all RoIs. For a more comprehensible result, the number of choosen RoIs from RPN module is set as $10$; the first $5$ has maximum \emph{objectness scores} among those proposed by RPN after Non-maximum Suppression-based post-processing procedure, while $5$ remainings represents the RoIs with least scores. In addition, with this experiment, we only target finding the optimal sub-graph structure that directly determine a given output context presentation, without figure out which features affect the result most.  
% Figure \ref{fig:gnn_saliency} demonstrates the sub-graph that mostly contribute to the construction of context vector for \emph{Hexinvon-8mg}'s RoI. 
% The edges in black denote the important connections, whose weights are proportional to the edges' width. As GNNExplainer suggests, the most critical nodes for generating this context presentation relate to RoIs that include genuine neighbor pills, and all unnecessary background RoIs are efficiently filtered out by GTN in the computation process. In addition, regarding the edges, while there are still some noisy edges which link the actual labels' nodes to the background ones, the majority and the most important ones are the connections that directly connect \emph{Hexinvon-8mg}'s RoI node with the true labels neighbors. 

% \textcolor{red}{@Duy: \\
% - With the aid of auxiliary loss, the desire behavior is achieved: filter out easy samples, selecting candidates for hard samples - DONE \\ 
% - Plot the GNN Explainer result to explain the context vector generation process: for the context vector of hexivon 8mg, the neighborhood pill that take the most important roles is the neighborhood one\\
% - Plot the salience map produced by Exciation Backpropagation - compare with that of Faster-RCNN: with case of PGPNet: not only the section of Hexivon, but also other sections of other pills also make impact \\
% }
% These interpretations of a particular sample image reveal the inner workings of our framework, which is highly congruent with our design aim and motivation. By basing on the context pills around the hard pill samples - in this case \emph{Hexinvon-8mg}, PGPNet have well differentiated and determined its true label, successfully solve the problem of its baseline Faster R-CNN.

\subsection*{Ablation Studies}
In this section, we perform extensive ablation studies to investigate the impacts of the main techniques proposed in our PGPNet and to investigate how each component in the proposed method helps to improve learning performance. Specifically, we alter the Co-occurrence Graph and observe how it affects the detection results in Section \nameref{sec:ablation_mcg}. We then assess the effects of using the relational graphs, the Graph transformer network, and the proposed auxiliary loss in Sections \nameref{sec:ablation_relational graphs}, \nameref{sec:ablation_gtn}, %\ref{sec:ablation_aux}, 
respectively.

\subsubsection*{Effect of Co-occurrence Graph's Quality}
\label{sec:ablation_mcg}
% This section is dedicated for describing the effects of MCG on the performance of Pill Detector frameworks.
In this section, we perform two experiments to observe how the performance is changed when the nodes set and edges set of MCG are modified respectively.

\textbf{Edge Set Modification.}
We first observe the behavior of our PGPNet when adding noise edges and removing actual edges. We set up four scenarios which are the combinations of removing $25\%$ and $50\%$ of the edges in the set $E_1$, and adding a number of synthesized edges corresponding to $25\%$ and $50\%$ of the cardinality of $E_1$.

Figure \ref{fig:edge_modify} illustrates the performances of PGPNet with all Medical Co-occurrence Graph variances when being put into comparison with the original one. The performance here is denoted by the general metrics AP. As indicated by AP density, PGPNet with original MCG generates a more concentrated density with a smaller variance and a higher mean than other variances. In addition, when $50\%$ of edges are eliminated, the performance is clearly inferior to when $25\%$ of edges are eliminated. The figure concludes with the intriguing observation that eliminating edges at random would result in a greater performance decrease than adding noisy edges. This is because, even with the addition of noisy edges, PGPNet could still filter out unnecessary information through the training process. When excluding edges, the situation is different because the framework cannot learn the external knowledge contained in the eliminated edges.

\begin{figure*}[!t]
\begin{minipage}{0.32\linewidth}
% \begin{figure}[tb]
    % \hfill%
    % \begin{subfigure}[t]{0.48\columnwidth}
        \includegraphics[width=0.9\columnwidth]{ridgeline_nodes_exp.eps}
        \caption{Distributions of Average Precision recorded over the classes in $N_A$ set produced by PGPNet with different MCG versions. \label{fig:node_modify}}
    % \end{subfigure}
% \end{figure}
\end{minipage}
\hspace{0.1cm}
\begin{minipage}{0.68\linewidth}
    %\begin{table*}[!h]
    \centering
    \small
    %\setlength\tabcolsep{3pt} % default value: 6pt
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{l|ccccc|l|l|l|l|l|l}
    \toprule
    \multicolumn{1}{l|}{} & \multicolumn{5}{c|}{\textbf{Component}}  & \multicolumn{6}{c}{\textbf{Performance}}
    \\
    & $\mathcal{G}_c$ & $\mathcal{G}_s$ & $\mathcal{G}_v$ & GTN & $\mathcal{L}_{aux}$ &   mAP & AP50 & AP75 & APs & APm & APl
    \\ \midrule
    Faster R-CNN & $\times$  & $\times$   & $\times$   & $\times$    & $\times$    &
      \begin{tabular}[c]{@{}r@{}}63.7 (-8.6)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}86.7 (-8.4)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}76.9 (-7.9)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}71.3 (-20.8)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}58.1 (-10.6)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}64.6 (-7.8)\end{tabular}  \\
    PGPNet-v1    & \checkmark  & $\times$   & $\times$   & $\times$    & $\times$     &
      \begin{tabular}[c]{@{}r@{}}65.9 (-5.5)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}91.9 (-2.9)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}79.6 (-4.6)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}72.5 (-19.4)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}62.3 (-4.3)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}66.0 (-5.8)\end{tabular}  \\
    PGPNet-v2    & \checkmark  & $\times$  & \checkmark  & \checkmark   & \checkmark   &
      \begin{tabular}[c]{@{}r@{}}66.9 (-3.9)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}92.1 (-2.7)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}81.1 (-2.8)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}80.0 (-11.1)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}61.3 (-5.9)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}67.6 (-3.6)\end{tabular} \\
    PGPNet-v3    & \checkmark  & \checkmark  & $\times$  & \checkmark   & \checkmark   &
      \begin{tabular}[c]{@{}r@{}}67.8 (-2.8)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}92.9 (-1.9)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}82.3 (-1.5)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}82.5 (-8.3)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}62.7 (-3.5)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}68.1 (-2.8)\end{tabular} \\
    PGPNet-v4    & \checkmark  & \checkmark  & \checkmark  & $\times$   & \checkmark  &
      \begin{tabular}[c]{@{}r@{}}68.4 (-1.9)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}92.6 (-2.2)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}81.7 (-2.1)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}80.0 (-11.1)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}64.4 (-1.0)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}68.8 (-1.8)\end{tabular}  \\
    PGPNet-v5    & \checkmark  & \checkmark & \checkmark  & \checkmark   & $\times$  &
      \begin{tabular}[c]{@{}r@{}}67.2 (-3.6)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}91.3 (-3.5)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}80.9 (-3.1)\end{tabular} &
      90.0 (+0.0) &
      \begin{tabular}[c]{@{}r@{}}62.2 (-4.3)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}67.3 (-3.9)\end{tabular}  \\
    PGPNet    & \checkmark  & \checkmark  & \checkmark  & \checkmark   & \checkmark   &
      69.7 &
      94.7 &
      83.5 &
      90.0 &
      65.0 &
      70.0 \\ \bottomrule
    \end{tabular}
    }
    \captionof{table}{Performance of PGPNet with the diferent combination of its components, i.e., when removing (marked as $\times$) / keeping (marked as \checkmark) the relational graph, GTN and auxiliary loss. Numbers inside the (.) represent the gap in percentage compared to the full version of PGPNet.\label{tab:ablation_result}}
    %\end{table*}
\end{minipage}
\vspace{-0.5cm}
\end{figure*}

\textbf{Node Set Modification.}
To observe PGPNet's performance when the Medical Co-occurrence Graph lacks information on some specific nodes - classes, we design two different scenarios. In the first one, $25\%$ nodes are removed in the original graph, this set is denoted as $N_A$. For the latter, $50\%$ of nodes are eliminated, and the corresponding set $N_B$ is ensured to be a superset of $N_A$. The performances of PGPNet in two circumstances are compared with itself when having the full MCG, considering only the classes appeared in the set $N_A$.

Figure \ref{fig:node_modify} depicts the outcome of this experiment. The AP across all $N_A$ classes is used to evaluate performance here. As indicated by the graph, node removals also result in a significant decrease in model performance. More interestingly, the more nodes being eliminated, the greater drop is captured. Specifically, the AP density in case MCG contains only $50\%$ of remaining nodes has a great variance, with the mean value only around $60\%$. 

In the following, we study the effectiveness of the relational graphs, Graph Transformer Network (GTN) block, and auxiliary loss. 
The detailed configurations are presented in Table \ref{tab:ablation_result}. The $+$ sign indicates the presence of a component in a specific version, while $-$ denotes the opposite.  
%Table \ref{tab:ablation_result} summarizes the experimental results. 

% Since my vanilla backbone model is Faster R-CNN, without any proposed module, it also listed in the table. The ablation of each module is quite straight-forward to adopt without any replacement needed.

% \begin{table}[]
% \begin{tabular}{l|rrrrrr}
% \toprule
% Models &
  % AP &
  % AP50 &
  % AP75 &
  % APs &
  % APm &
  % APl \\ \hline
% Faster RCNN &
  % \begin{tabular}[c]{@{}r@{}}63.713 \\ (-8.57)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}86.663 \\ (-8.44)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}76.925 \\ (-7.85)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}71.262 \\ (-20.82)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}58.109 \\ (-10.63)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}64.598 \\ (-7.78)\end{tabular} \\ \hline
% PGPNet-v1 &
  % \begin{tabular}[c]{@{}r@{}}65.879 \\ (-5.46)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}91.945 \\ (-2.86)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}79.618\\  (-4.63)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}72.525 \\ (-19.42)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}62.25 \\ (-4.26)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}65.995 \\ (-5.78)\end{tabular} \\ \hline
% PGPNet-v2 &
  % \begin{tabular}[c]{@{}r@{}}66.931 \\ (-3.95)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}92.136 \\ (-2.66)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}81.13 \\ (-2.82)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}80 \\ (-11.11)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}61.216 \\ (-5.85)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}67.562 \\ (-3.55)\end{tabular} \\ \hline
% PGPNet-v3 &
  % \begin{tabular}[c]{@{}r@{}}67.754 \\ (-2.77)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}92.904 \\ (-1.85)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}82.272 \\ (-1.45)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}82.5 \\ (-8.33)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}62.729 \\ (-3.52)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}68.121 \\ (-2.75)\end{tabular} \\ \hline
% PGPNet-v4 &
  % \begin{tabular}[c]{@{}r@{}}68.375 \\ (-1.88)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}92.559 \\ (-2.21)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}81.728 \\ (-2.1)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}80 \\ (-11.11)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}64.358 \\ (-1.02)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}68.748 \\ (-1.85)\end{tabular} \\ \hline
% PGPNet-v5 &
  % \begin{tabular}[c]{@{}r@{}}67.176 \\ (-3.6)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}91.34 \\ (-3.5)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}80.857 \\ (-3.14)\end{tabular} &
  % 90 &
  % \begin{tabular}[c]{@{}r@{}}62.23 \\ (-4.29)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}67.341 \\ (-3.86)\end{tabular} \\ \hline
% PGPNet&
  % 69.686 &
  % 94.651 &
  % 83.481 &
  % 90 &
  % 65.02 &
  % 70.046 \\ \bottomrule
% \end{tabular}
% \caption{Performance of PGPNet when removing relational graph, GTN and auxiliary loss. \label{tab:ablation_result}}
% \end{table}

\subsubsection*{Effects of the Relational Graphs}
\label{sec:ablation_relational graphs}
In this section, we study the effectiveness of the Size-graph and visual-based graph. 
To this end, we implement two simplified versions of PGPNet, namely PGPNet-v2 and  PGPNet-v3, in which we remove the Size-graph and visual-based graph, respectively. 
As shown in Table \ref{tab:ablation_result}, eliminating the Size-graph causes a decrease in performance from 3.9\% to 11.1\%, while omitting the visual-based graph reduces the accuracy from 2.8\% to 8.3\%.
An interesting finding is that the deterioration gap when removing the size graph is more significant than those when eliminating the visual-based graph in terms of all evaluation metrics. 
These findings imply the effectiveness of the Size-graph over the visual-based graph.
Moreover, it can be observed that mAP is the most impacted when the relational graphs are removed, followed by AP50, when comparing mAP, AP50, and AP75.
This can be explained as follows.
In AP75, we measure the precision of RoIs with the IoU beyond 75\%, which presumably has a high degree of confidence regarding the objective.
In contrast, when we reduce the IoU threshold, such as AP50 and mAP, the overlap area of the objective drops, resulting in a model with a significant degree of uncertainty.
In this case, integrating relational graphs provides additional data that reduces uncertainty, thereby boosting detection accuracy.
% Moreover, it can be observed that when removing the relational graph, the APs is the metric decreased the most (i.e., more than 11\%). 
\subsubsection*{Effects of the Multi-modal Data Fusion Block and Auxiliary Loss}
\label{sec:ablation_gtn}
To investigate the effectiveness of the GTN, we implement PGPNet-v4, omitting the GTN block and relying solely on the GCN to learn the node representation. 
Results in Table~\ref{tab:ablation_result} reveal that GTN enhances the model's accuracy from $1.0\%$ to $11.1\%$. 
Comparing mAP, AP50, and AP75, AP50, and AP75 are slightly more influenced by GTN than mAP, but the gaps are trivial.
% \subsubsection{Effects of the Proposed Auxiliary Loss}
% \label{sec:ablation_aux}
We employ PGPNet-v5, which eliminates the proposed auxiliary loss and compare its performance with the original PGPNet. 
% The experimental results are shown in Table \ref{tab:ablation_result} under the model name of PGPNet-v5.
As illustrated in Table \ref{tab:ablation_result}, adopting our auxiliary loss may result in a 3 to 4 percent performance gain for most evaluation metrics. 
In the final ablation study, we implement PGPNet-v1, which retains only the co-occurrence graph and removes all the other components.
As depicted in Table \ref{tab:ablation_result}, the detection accuracy degrades significantly, with a gap ranging from $2.9\%$ to $19.4\%$. However, even with this version, PGPNet is still superior to Faster RCNN, with a performance margin of up to $7.1\%$.

In conclusion, the PGPNet version with all components exhibits its superiority in all evaluation metrics. In addition, all versions of PGPNet are superior to the Faster R-CNN backbone, demonstrating the contribution of each component to the overall performance of PGPNet.


\begin{figure*}[!t]
\centering
\begin{minipage}{0.55\linewidth}
    %\begin{table}[!tb]
    \captionof{table}{\label{tab:dataset_meta} {An overview of existing public datasets for the task of image-based pill detection. To the best of our knowledge, the introduced VAIPE dataset is currently the largest dataset for pill identification, which was collected in real-world settings and came up with prescriptions.}}
    \centering \small
    	\setlength\tabcolsep{4pt} % default value: 6pt
        \resizebox{\columnwidth}{!}{%
            \begin{tabular}{l|l|l|l}
                \toprule
                & \textbf{NIH} & \textbf{CURE}  & \textbf{VAIPE}            \\ \midrule
                Number of pill images     & 7,000    & 8,973  & 9,426             \\
                Number of pill categories & 1,000    & 196   & 96               \\
                Number of capture devices & 1    & 1   & $>$ 20               \\
                Instance per category     & 7       & 40-50 & $>$ 30 \\
                Illumination conditions   & 1       & 3     & $>$ 50 \\
                Backgrounds   & 1       & 6     & $>$ 50 \\
                Number of prescriptions   & 0       & 0     & 1,527            \\ \bottomrule
            \end{tabular}
        }
    %\end{table}
\end{minipage}
\hspace{0.1cm}
% \hfill
\begin{minipage}{0.3\linewidth}
    %\begin{table}[!tb]
    \captionof{table}{Details of training and testing datasets. \label{tab:train_test_split}}
    \centering \small
    	\setlength\tabcolsep{4pt} % default value: 6pt
        \resizebox{\columnwidth}{!}{%
            \begin{tabular}{ll}
            \toprule
            \multicolumn{2}{c}{\textbf{Training dataset}} \\
            \cmidrule(lr){1-2}
            \multicolumn{1}{c}{Prescriptions} & \multicolumn{1}{c}{Images} \\ \midrule
            1,527 (100\%)             & 7,514 (78\%)    \\ \midrule
             \\
             \\
            \multicolumn{2}{c}{\textbf{Testing dataset}}  \\ 
            \cmidrule(lr){1-2} %\cmidrule(lr){3-4}
            \multicolumn{1}{c}{Prescriptions} & \multicolumn{1}{c}{Images} \\ \midrule
            0 (0\%) & 1,912 (22\%)     \\ 
            \bottomrule
            \\
            \end{tabular}
        }
    %\end{table}
\end{minipage}
\vspace{-10pt}
\end{figure*}
We conduct extensive experiments to validate the effectiveness of the proposed approach. 
In the following, we first introduce our in-house pill identification dataset, called VAIPE, which will be used to evaluate the proposed approach, and then explain our evaluation metrics and experimental settings. 
To assess the effectiveness of the proposed method, we conducted comparative assessments against a number of established models, including the detection backbones we selected, such as Faster R-CNN~\cite{fasterrcnn} and YOLOv5 \cite{yolov5}, as well as other related frameworks such as SGRN~\cite{sgrn} and the Mask RCNN-based approach described in ~\cite{kwon_pill}. We also perform ablation studies to investigate the efficiency of key components in our framework.
% investigate some key properties of the method by analyzing its different components through our insightful analytical studies.
\subsection*{Dataset and Pre-processing}

\textbf{Motivation.}  {To the best of our knowledge, previous studies on the pill identification problem~\cite{tan2021comparison},~\cite{cure1},~\cite{Ling_2020_CVPR},~\cite{cure} only focus on datasets collected in constrained environments. For instance, existing datasets such as {NIH Dataset} \cite{nih_dataset} are constructed under ideal conditions in lighting, backgrounds, and equipment or devices. The CURE dataset~\cite{Ling_2020_CVPR} provides only one pill per image. Hence, these datasets do not reflect the real-world scenarios in which patients take an arbitrary number of drugs, and their environmental conditions (e.g., backgrounds, lighting conditions, mobile devices, etc.) are greatly varied. Additionally, many pills have nearly identical visual appearances. The fact that they appear alone in the images of these datasets will inevitably confuse the detection frameworks. Consequently, none of the existing datasets can be directly applied to the real-world pill detection problem or can only be applied with low reliability. There is no publicly available dataset of these pills images in which the pills follow intakes of actual patients. This limits the development of machine learning algorithms for the detection of pills from images as well as for building real-world medicine inspection applications. To address this challenge, we build and introduce a new, large-scale open dataset of pill images, which we called VAIPE. }

\textbf{Data Descriptor.} The VAIPE is a large-scale and open pill image dataset for visual-based medicine inspection. %The dataset contains $1,527$ prescriptions obtained from anonymous patients at $4$ major institutes in Vietnam between 2018 and 2020. 
The dataset contains approximately 10,000 pill images that were manually collected in unconstrained environments. In this study, no hypotheses or new interventional procedures were generated. Also, no investigational products or clinical trials were used for patients. In addition, there were no changes in treatment plans for any patients involved. Pill images were retrospectives collected, and all identifiable information of patients was de-identified. Therefore, there was no requirement for ethics approval~\cite{bworld}. %After carefully examining the data and patients' privacy, each prescription is labeled under the support of Vietnamese optical character recognition (OCR) models, namely, VietORC - a TransformerOCR framework \cite{transocr} trained with Vietnamese handwritten characters dataset. %Afterward, the medications are purchased in accordance with the relevant prescriptions; then, images are taken following the correct tablets for each user throughout the day. Specifically, prescriptions will mostly be separated by timeslots to take the medicines (\emph{morning}, \emph{noon} \emph{afternoon} and \emph{evening}). For each slot, patients should only take a subset of pills contained in the prescriptions, corresponding to the amount assigned. The images are taken following those timeslots in prescriptions, 

Pill images are collected in many different contexts (e.g., various backgrounds, lighting conditions, in-hand or out-of-hand, etc.) using smartphones. These images are then manually labeled using the information from the relevant prescriptions. In summary, the number of pills per image is about $5 - 10$, and the total number of pill images collected was $9,426$ pill images with $96$ independent pill labels. To train the proposed deep learning system, the pill images from the VAIPE dataset are resized so that the shortest edges have a size of $800$, with a limit of $1,333$ on the longer edge. The ratios are kept the same as the original images if the max size is reached, then downscale so that the longer edge does not exceed $1,333$.

\textbf{Data Validation.} Patient privacy was controlled and protected. In particular, all images were manually reviewed to ensure that all individually identifiable health information of the patients has been removed to meet the General Data Protection Regulation (GDPR) \cite{gdpr}. Annotations of pill images were also carefully examined. Specifically, all images were manually reviewed case-by-case by a team of 20 human readers to improve the quality of the annotations.

\textbf{Comparison with Existing Datasets.} Table \ref{tab:dataset_meta} provides a summary of the aforementioned datasets (including NIH, CURE, and VAIPE) together with other ones of moderate sizes, meta-data, and other properties. Compared to the two previous datasets, the VAIPE dataset is constructed under a much more flexible procedure that reflects the characteristic real-world data distributions. Hence, the introduced dataset can serve as a reliable data source for training \emph{generic pill detectors}. 


\subsection*{Evaluation Metrics}
{We evaluate the proposed method and other related works by the COCO APs metrics~\cite{coco_ap}. This set of metrics is widely accepted and used for evaluating state-of-the-art object detectors. Mean Average Precision (mAP), as its name suggests, is the mean of Average Precision (AP) overall $C$ classes and all the targeted IoU thresholds in the threshold set $T$ calculated by $ mAP = \frac{1}{C |T|} \sum_{i}^{C} \sum_{t \in T} AP_{i,t}$, 
%over recall values from 0 to 1 
% as follows
% \begin{equation}
%     \small
%     mAP = \frac{1}{C |T|} \sum_{i}^{C} \sum_{t \in T} AP_{i,t},
% \end{equation}
where Average Precision ($AP_{i,t}$) is the area under the Precision-Recall curve, calculated for the class $i$ at a given IoU threshold $t$.} %The mAP is the averaged result of APs over $C$ classes. It is worth noting that  COCO Evaluator makes no distinction between AP and mAP and assumes the difference is clear from the context. Hence, we use the term AP in place of mAP for consistency. 
\iffalse 
To estimate the $AP_{i,t}$, we need to measure whether a predicted instance is True Positive (TP) or False Positive (FP). We use the IoU (Intersect over Union) factor~\footnote{IoU is the ratio between intersected area over the union area of the two boxes} to evaluate the fitness of predicted bounding boxes compared with the ground-truth ones. If the IoU of a predicted instance is higher than the predefined IoU threshold, the prediction is set to True Positive and vice versa. 
In this work, besides the main metric, i.e., APs metrics, we also measure AP metrics with different area scales to estimate the robustness of our proposed method.
Table \ref{tab:map_metric} summarizes the set of COCO APs evaluation metrics. 

\begin{table}[!tb]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|l|l}
\toprule
& Metrics & Description   \\ \midrule
\multirow{3}{2cm}{Average Precision (AP)} & mAP      &  \begin{tabular}[l]{@{}l@{}} Averaged AP across all $10$ IoU threshold in \\set $T$ = $\overline{0.5,0.95}$, step $0.05$. \end{tabular}  \\
& AP50    & AP at given IoU thres = 0.5  \\
& AP75    & AP given IoU thres = 0.75 (strict metric)   \\ \midrule
\multirow{3}{2cm}{AP Across Scales}       
& APs     & AP for small object : area $<$ $32^2$  \\
& APm     & AP for medium object: $32^2\le$ area $<96^2$ \\
& APl     & AP for large object: area  $> 96^2$                             \\ \bottomrule
\end{tabular}
}
\caption{COCO APs \cite{coco_ap} evaluation metrics set.}\label{tab:map_metric}
\end{table}
\fi 
\subsection*{Comparison with state-of-the-art methods}
\textbf{Comparison Benchmarks.} To show the effectiveness of the proposed method, we conducted a comparison with the state-of-the-art object detectors, including our detection backbones: Faster R-CNN~\cite{fasterrcnn}, YOLOv5 \cite{yolov5}, and related works: SGRN~\cite{sgrn}, Mask RCNN-based approach \cite{kwon_pill}. Throughout the literature, the baseline with which PGPNet presently integrates is Faster R-CNN \cite{fasterrcnn}; hence, the original framework is utilized for our comparison. 
We adopt two different CNNs and one Transformer-based module for visual feature extractor, namely ResNet-50-C4, ResNet-50-FPN and Swin Transformer V2 - SwinV2 \cite{swinv2} (Fig.~\ref{fig:generalflow}). Specifically, for two ConvNets, we use a single feature map produced by convolution block C4 of the ResNet-50 model in ResNet-50-C4. In ResNet-50-FPN, we replace C4's feature map with multi-scale feature maps produced by Feature Pyramid Network (FPN)~\cite{fpn}. As for the Swin Transformer module, we are currently utilizing the SwinV2-T configuration \cite{swinv2} to ensure that the number of model parameters is comparable to that of ResNet-50.
In addition, we also make adaption for PGPNet with YOLOv5 \cite{yolov5} detection backbone. Two configurations of YOLOv5s and YOLOv5n are currently adopted. 
%For the Pseudo Classifier, we use the original YOLOv5 output layer, which produces three output values corresponding to the level of objectness (objectness in short), class probabilities, and bounding box coordinates. We treat each anchor in the image grid as a Region of Interest and perform threshold-cutting to choose $N$ of them for each image based on the level of objectness produced by the Pseudo Classifier, similar to the idea of Region Proposal in Faster R-CNN. In addition, the pseudo classification score for each \emph{chosen} RoI is now the product between its objectness scalar and the corresponding class probabilities. This adaptation makes all the logic for our PGPNet remain intact while still ensuring the superior characteristic of the one-step detection framework (e.g., end-to-end training pipeline, fast training-inference time, etc.).
Also, the most relevant frameworks compared with our PGPNet are also put into comparison: a representative approach that utilizes an external knowledge graph for Object Detection task~\cite{sgrn}; a Mask RCNN-based baseline that also proposed to solve the same task of multi-pill detection \cite{kwon_pill}. \\
\noindent For a fair comparison, a fixed set of hyper-parameters is used for PGPNet throughout all experiments.}

\subsection*{Implementation Details}
{We conduct all the experiments using the Pytorch (version 1.10.1) on an Intel Xeon Silver 4210 2.20GHz system with $2$ $\times$ NVIDIA GeForce RTX 3090 GPUs.  We train and test all targeted models on the training and testing sub-datasets provided in Table~\ref{tab:train_test_split}. Specifically, we initialize all the networks with the weights achieved by pre-training them on COCO 2017 dataset~\cite{coco}. We then train the models in $20,000$ iterations with a batch size of $16$. AdamW~\cite{adamw} optimizer is used with the initial learning rate of $0.001$. We also augment the training data by using simple techniques such as random horizontal and vertical flips to prevent overfitting. For our PGPNet implementation, we set the dimensions of node embeddings at $64$. We also design the Graph Transformer Module with only one layer and $10$ channel set.}

% \subsection{Experimental Results of PGPNet}
% This section would discuss about the actual performance of PGPNet, together with some baselines and other related works.
\section*{Experimental Results}
This section reports our experimental results. We evaluate the effectiveness of PGPNet in three aspects: robustness, reliability, and explainability. The details are described below. % First, we study the robustness of the proposed network across various object detection backbones. Second, we compare the PGPNet with existing external knowledge-assisted object detection approaches. Last, we investigate the explainability ability of the network by analyzing predictions produced by PGPNet.

% + Trust AI - (real dataset?)
% + Explainable AI: which component contribute to the result? Which metrics.

% + Q1: proposed framework can increase the accuracy (compare with baseline, related work, )
% + Q2: Graph can help we explain the result
%     + correct result can explain by the graph relation...==> back track the graph and features to show it correct.
%     + Other metrics...
\subsection*{Robustness and Reliability of PGPNet}
\label{subsec:eval_backbones}
\subsubsection*{Comparison with Faster R-CNN and YOLOv5}
\begin{table}[!tb]
Comparing PGPNet's detection performance with state-of-the-art vanilla object detectors on VAIPE dataset.
Best results are highlighted in \textbf{bold} text. \label{tab:eval_backbone}
\centering
\small
\setlength\tabcolsep{3pt} % default value: 6pt
\resizebox{0.8\columnwidth}{!}{%
\begin{tabular}{lll|cccccc}
\toprule
\multicolumn{3}{c|}{\textbf{Method}}                                      & \multicolumn{1}{c}{\textbf{mAP}} & \multicolumn{1}{c}{\textbf{AP50}} & \multicolumn{1}{c}{\textbf{AP75}} & \multicolumn{1}{c}{\textbf{APs}} & \multicolumn{1}{c}{\textbf{APm}} & \multicolumn{1}{c}{\textbf{APl}} \\ \midrule
\parbox[t]{2mm}{\multirow{7}{*}{\rotatebox[origin=c]{90}{\textcolor{red}{\textbf{Two-step}}}}} & 
\multicolumn{1}{l|}{\multirow{2}{*}{
    \begin{tabular}[l]{@{}l@{}} Faster R-CNN \\ (ResNet-50-C4)\end{tabular}}}  
    & Vanilla & 62.6               & 87.0                 & 74.4                  & 75.0                     & 58.3                 & 62.9                 \\
\multicolumn{2}{l|}{}                               & PGPNet       & \textbf{68.3 (+9.2\%)}       & \textbf{92.5}        & \textbf{81.7}          & \textbf{80.0}             & \textbf{64.3}         & \textbf{68.7}         \\
\cmidrule(lr){2-9}
& \multicolumn{1}{l|}{\multirow{3}{*}{\begin{tabular}[l]{@{}l@{}} Faster R-CNN \\ (ResNet-50-FPN)\end{tabular}}} & Vanilla & 63.7                & 86.6                & 76.9                   & 71.2                 & 58.1                & 64.6               \\
\multicolumn{2}{l|}{}                               & PGPNet       & \textbf{69.7 (+9.4\%)}       & \textbf{94.4}         & \textbf{83.4}         & \textbf{90.0}    & \textbf{66.4}        & \textbf{70.1}   \\
\multicolumn{2}{l|}{} &    &                 &                    &                   &                    &                   &                   \\ 
\cmidrule(lr){2-9}
& \multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[l]{@{}l@{}} \duy{Faster R-CNN} \\ \duy{(SwinV2-T)}\end{tabular}}} & \duy{Vanilla} & \duy{59.7}                & \duy{84.5}                & \duy{72.3}                   & \duy{66.9}                 & \duy{54.0}                & \duy{60.1}               \\
\multicolumn{2}{l|}{}                               & \duy{PGPNet}       & \duy{\textbf{62.6 (+4.8\%)}}       & \duy{\textbf{87.2}}         & \duy{\textbf{75.5}}         & \duy{\textbf{68.6}}    & \duy{\textbf{56.6}}        & \duy{\textbf{62.9}}   \\
\midrule
\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{{\textbf{One-step}}}}} & \multicolumn{1}{l|}{\multirow{2}{*}{YOLOv5n}}  & Vanilla & 37.9 & 50.8 &45.4    & 87.5    & 49.1    & 38.3  \\
\multicolumn{2}{l|}{}                              & PGPNet  & \textbf{43.0 (+12.0\%)}   & \textbf{58.4}    & \textbf{51.3}    & \textbf{82.5}    & \textbf{52.4}    & \textbf{43.7}    \\ \cmidrule(lr){2-9}
& \multicolumn{1}{l|}{\multirow{2}{*}{YOLOv5s}} & Vanilla & 57.5 & 75.8 & 68.3 & 85.0 & 58.3 & 57.0 \\
\multicolumn{2}{l|}{}                              & PGPNet  & \textbf{63.4 (+10.2\%)} & \textbf{85.9} & \textbf{76.4} & \textbf{89.9} & \textbf{58.3} & \textbf{64.1} \\
\bottomrule
\end{tabular}
}
\vspace{-5pt}
\end{table}
%We compare our approach with the state-of-the-art object detectors (Vanilla), and SGRN~\cite{sgrn} in terms of detection performance. As mentioned in Section~{\ref{sec:method}}, PGPNet uses enhanced features, which are the combination of the visual features and the proposed context features to localize and classify the pills inside an image. In this work, we perform the comparison based on both the two-step object detection architecture, i.e., Faster R-CNN, and the one-step object detection architecture such as YOLOv5~\cite{yolov5} framework.

 \begin{figure}[!tb]
        \centering
        \includegraphics[width=0.85\columnwidth]{pirate_class.eps}
        \caption{Comparison of the PGPNet performance with the Faster R-CNN baseline over each individual class. \label{fig:eval_backbone_class}}
    \end{figure}

\begin{figure}[tbh]
    \captionsetup[subfigure]{justification=centering}
    \centering
    \subfloat[Faster R-CNN \label{fig:reliable_frcnn}]{%
      \includegraphics[clip,width=0.31\columnwidth]{reliability_frcnn.eps}%
    }%
    \hfill%
    \subfloat[YOLOv5 \label{fig:reliable_yolo}]{%
      \includegraphics[clip,width=0.31\columnwidth]{reliability_yolo.eps}%
    }%
    \hfill%
    \subfloat[SGRN \label{fig:reliable_sgrn}]{%
      \includegraphics[clip,width=0.31\columnwidth]{reliability_sgrn.eps}%
    }%
    \caption{Reliability investigation for PGPNet and different baseline performances.\label{fig:reliable_all}}
    % \vspace{0.3cm}
\end{figure}
    
% \begin{figure*}[!t]
% \begin{minipage}{0.32\linewidth}
    % \begin{figure}[!tb]
        % \centering
        % \includegraphics[width=\columnwidth]{pirate_class.eps}
        % \caption{Comparison of the PGPNet performance with the Faster R-CNN baseline over each individual class. \label{fig:eval_backbone_class}}
    % \end{figure}
% \end{minipage}
% \hspace{0.1cm}
% \begin{minipage}{0.66\linewidth}
    % \begin{figure}[tbh]
    % \centering
    % \subfloat[Faster R-CNN \label{fig:reliable_frcnn}]{%
    %   \includegraphics[clip,width=0.32\columnwidth]{reliability_frcnn.eps}%
    % }
    % \hfill
    % \subfloat[YOLOv5 \label{fig:reliable_yolo}]{%
    %   \includegraphics[clip,width=0.32\columnwidth]{reliability_yolo.eps}%
    % }
    % \hfill
    % \subfloat[SGRN \label{fig:reliable_sgrn}]{%
    %   \includegraphics[clip,width=0.32\columnwidth]{reliability_sgrn.eps}%
    % }
    % \caption{Reliability investigation for PGPNet and different baseline performances.\label{fig:reliable_all}}
    % % \vspace{0.3cm}
    % \end{figure}
% \end{minipage}
% \vspace{-0.5cm}
% \end{figure*}
\textbf{Detection Performance.}
Table~\ref{tab:eval_backbone} shows the experimental results of PGPNet and the state-of-the-art object detectors framework (Vanilla), e.g., Faster R-CNN (two-step detector), and YOLOv5~\cite{yolov5} (one-step detector) 
on the VAIPE dataset. As shown, PGPNet obtained better results than Faster R-CNN by large performance gaps for all evaluation metrics. Specifically, when using the ResNet-50-C4 model as the visual feature extractor model, the average precision mAP of Faster R-CNN was $62.6$, while that of PGPNet was $68.3$. The proposed method improves the performance over the baseline Faster R-CNN by $9.2\%$. 
Under strict metrics, e.g., AP75, PGPNet also outperforms Faster R-CNN $8-9\%$.
In addition, we observed similar behavior when using the ResNet-50-FPN model. The proposed PGPNet makes an improvement of $9.4\%$ for the mAP metrics. With a Transformer-based backbone, here a Swin Transformer V2 configuration - SwinV2-T ~\cite{swinv2}, the results are slightly worse compared to those produced by ResNet-based counterparts, for both the vanilla or PGPNet alternatives. However, PGPNet still show its superior when being install with this backbone, as the empirical result for AP metrics is improved by $4.8\%$ compared to the vanilla SwinV2-T Faster R-CNN model.

%We also report the results of PGPNet and YOLOv5 with two different versions are reported in Table~\ref{tab:eval_backbone}. 
{For YOLOv5, PGPNet outperformed Vanilla by a significant margin across all performance metrics in both YOLO instances. Specifically, the average precision AP of the vanilla model with YOLOv5n was $37.9$ while that of PGPNet was $43.0$ ($12\%$ improvement). In the case of a larger alternative, YOLOv5s, a similar conclusion can be drawn, namely that PGPNet improves overall mAP metrics by $5.9$, e.g., $10.2\%$.}


% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=1\columnwidth]{performace_backbone.eps}
%     \caption{Comparison of PGPNet performance with Faster R-CNN baseline. \textcolor{red}{May not need}\label{fig:eval_backbone}}
% \end{figure}

%\textbf{Class-specific Detection Performance.} 
Figure \ref{fig:eval_backbone_class} visualizes the AP %at $IoU=.50:.05:.95$ 
for all classes in the dataset when using Faster R-CNN as the backbone. The first three bins denote Faster R-CNN alternatives, and the later three are the corresponding PGPNet configurations. The dots in the figure represent AP values for classes; the vertical line is the indicator for the mean value, while the rectangle bar is the $95\%$ High-Density Interval (HDI) band. Apart from the fact that the mean AP over all classes of PGPNet variances is better than those produced by Faster R-CNN, we found that PGPNet also has more reliable and stable results over all classes.  
Specifically, PGPNet helps to improve the AP of classes that Faster R-CNN frequently confuses (the points with low APs in the blue and pink beans). As a result, the three beans of Faster R-CNN exhibit a large variance, i.e.,  the AP ranged from $0$ to around $90$. In contrast, the beans of PGPNet performance are more condensed and have shorter tail, i.e., the AP ranged from  $40$ (or $50$) to around $90$.
%In an upcoming experiment, we will discuss a particular instance of these classes and interpret the results produced by our PGPNet to demonstrate its efficacy. 
%In contrast with the two beans of Faster R-CNN that exhibit a great variance, the beans of PGPNet performance are more condensed. %Additionally, the thin rectangles of two PGPNet groups indicate a %condensed $95\%$ HDI with limited value ranges. 

\textbf{Pill Classification Accuracy.}
{To further investigate the robustness of the proposed PGPNet, we adopt the visualization techniques presented in~\cite{reliable} to understand the prediction accuracy ( of the pill classification task) better. In this technique, all models' predictions are categorized by their confidence scores into different bins, in which the average accuracy can be calculated. By observing the confidence-accuracy correlation, we can tell whether the models are under or over-confidence with their predictions~\cite{reliable}. Figure~\ref{fig:reliable_all}\subref{fig:reliable_frcnn} visualize those reliability plots of Faster R-CNN and PGPNet. It implies that both models have a propensity toward over-confidence, as the average accuracy of each confidence band is lower than the mean confidence score of that bin. However, that tendency is greatly alleviated in the circumstance of PGPNet, which means that the bins' heights are much closer to the perfect Confidence-Accuracy balance line (the red dashed diagonal line).
Figure~\ref{fig:reliable_all}\subref{fig:reliable_yolo} compares PGPNet's confidence-accuracy correlation and that of YOLOv5. With this backbone, we observed that the proposed PGPNet can produce predictions with a high level of reliability. All the heights of bins are much closer to values suggested by the perfectly-balanced line compared to Vanilla's result.}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=\columnwidth]{reliability_frcnn.eps}
%     \caption{Reliability investigation for PGPNet and Faster R-CNN performance. \label{fig:reliable_frcnn}}
% \end{figure}

%\subsubsection{Comparison with YOLOv5} To comprehensively evaluate the effectiveness of the proposed PGPNet, we adapt our proposed method by using a one-step detection architecture for the visual feature extractor.
%In this section, we compare the detection performance of PGPNet with the baseline framework (Vanilla), e.g., YOLOv5~\cite{yolov5} framework.

%\textbf{Architecture Adaptation.} 
%\textbf{Detection Performance.} The numerical results of PGPNet and YOLOv5 with two different versions are reported in Table~\ref{tab:eval_backbone}. In both instances, PGPNet outperformed YOLOv5 by a significant margin across all performance metrics. Specifically, the average precision AP of the vanilla model with YOLOv5n was $37.9$ while that of PGPNet was $43.0$ ($12\%$ improvement). In the case of a larger alternative, YOLOv5s, a similar conclusion can be drawn, namely that PGPNet improves overall mAP metrics by $5.9$, e.g., $10.2\%$.

%\textbf{Class-specific Performance.} 
%\begin{figure}[!tb]
%    \centering
%    \includegraphics[width=\columnwidth]{pirate_class_yolo.eps}
%    \caption{Comparison of the PGPNet performance with the YOLOv5 baseline over each individual class. \label{fig:eval_backbone_class_yolo}}
%\end{figure}


%\textbf{Pill Classification Accuracy.} Figure~\ref{fig:reliable_yolo} compares PGPNet's confidence-accuracy correlation and that of YOLOv5. With this backbone, we observed that the proposed PGPNet can produce predictions with a high level of reliability. All the heights of bins are much closer to values suggested by the perfectly-balanced line compared to the vanilla model's result.
% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=\columnwidth]{reliability_yolo.eps}
%     \caption{Reliability investigation for PGPNet and YOLOv5 performance. \label{fig:reliable_yolo}}
% \end{figure}

% \begin{table*}[]
% \centering
% \small
% % \setlength\tabcolsep{8pt}
% \begin{tabular}{ll|cccccc}
% \toprule
% \multicolumn{2}{c|}{Backbone}                                & AP   & AP50 & AP75 & APs  & APm  & APl \\ \midrule
% \multicolumn{1}{l|}{\multirow{2}{*}{YOLOv5n}}  & Vanilla & 37.9 & 50.8 &45.4    & 87.5    & 49.1    & 38.3  \\
% \multicolumn{1}{l|}{}                              & PGPNet  & \textbf{43.0 (+12.0\%)}   & \textbf{58.4}    & \textbf{51.3}    & \textbf{82.5}    & \textbf{52.4}    & \textbf{43.7}    \\ \midrule
% \multicolumn{1}{l|}{\multirow{2}{*}{YOLOv5s}} & Vanilla & 57.5 & 75.8 & 68.3 & 85.0 & 58.3 & 57.0 \\
% \multicolumn{1}{l|}{}                              & PGPNet  & \textbf{63.4 (+10.2\%)} & \textbf{85.9} & \textbf{76.4} & \textbf{89.9} & \textbf{58.3} & \textbf{64.1} \\ \bottomrule
% \end{tabular}
% \caption{Comparison of PGPNet performance with different YOLOv5 baselines. \label{tab:eval_yolo}}
% \end{table*}

%\subsection{PGPNet's Reliability}
%Superiority over Existing External Knowledge-assisted Object Detection Benchmark}
\subsubsection*{Comparison with Existing  Relavant Frameworks}
Our work is the first to leverage an external graph in dealing with the Pill Detection challenge; thus, none of the preceding works are genuinely tight-correlated. Indeed, earlier researches only shared some common ground to our approach: (1) About methodology or (2) about research problem.

\noindent For the first group, there are works that utilized external information to solve the Object Detection problem. We adopt one of the most current studies with this direction - \cite{sgrn} to solve our targeted problem and serve as a baseline for PGPNet. 
% Briefly, aside from the majority of studies that construct graphs using external data (handcrafted linguistic knowledge, etc.) or by implicitly learning a fully-connected graph between regions of interest (RoIs), 
Spatial-aware Graph Relation Network (SGRN) \cite{sgrn} is a framework that adaptively discovers and incorporates key semantic and spatial relationships for reasoning over each RoI.

\noindent With respect to research problem, as stated earlier, while there are many works which target single-pill detection problem ~\cite{wong2017development, usuyama2020epillid, ling2020few}, only a few directly solve the task of detecting multiple pills per image \cite{Ou2020, kwon_pill}. We attempt to adopt the most recent technique proposed in \cite{kwon_pill} as another baseline to compare with PGPNet. In the original work, the authors purpose is somewhat different from us, since they attempt to develop a framework which is solely trained on single-pill images, since they argued that the multi-pill dataset would scale up exponentially if the number of pills inscrease. This argument is not held in our intuition, and we believe, in reality, since the pills taken together have to be prescribed by pharmacists. We keep the pipeline as the original work, with some adoption for working with our VAIPE dataset: (1) Change Mask R-CNN to Faster R-CNN; (2) The training single-pill dataset is cropped from our VAIPE dataset with bounding box annonations; (3) The automate data labeling process are skipped. Since the original work did not name the proposed pipeline, we called it as \emph{Kwon's Pipeline} for short.
%Due to this, SGRN requires no extra information and hence can be installed with minor changes compared to the original public work, and thus remains its original strength.
% \begin{table}[!tb]
% \centering
% \footnotesize 
% \setlength\tabcolsep{4pt} % default value: 6pt
% % \resizebox{\textwidth}{!}{%
% \begin{tabular}{l|cccccc}
% \toprule
% \textbf{Framework}    & \multicolumn{1}{c}{\textbf{mAP}} & \multicolumn{1}{c}{\textbf{AP50}} & \multicolumn{1}{c}{\textbf{AP75}} & \multicolumn{1}{c}{\textbf{APs}} & \multicolumn{1}{c}{\textbf{APm}} & \multicolumn{1}{c}{\textbf{APl}} \\ \midrule
% Faster R-CNN & 63.71               & 86.66                 & 76.92                   & 71.26                 & 58.10                  & 64.59                 \\
% SGRN \cite{sgrn}        & 65.88                  & 88.83                    & 79.64                    & 76.31                   & 61.58                   & 66.28                   \\
% PGPNet  (\textbf{ours})     & \textbf{69.70}       & \textbf{94.41}         & \textbf{83.38}         & \textbf{90.00}             & \textbf{66.45}        & \textbf{70.05}        \\ \bottomrule
% \end{tabular}
% % }
% \caption{Performance comparison of PGPNet with Faster R-CNN and SGRN.
% % {Could not think of a way to combine with table 4 - yet} 
% \label{tab:eval_related}}
% \end{table}

\textbf{Detection Performance.} 
\begin{table}[t]
\caption{Performance comparison of PGPNet with SGRN and Kwon's Pipeline.\label{tab:eval_related}}
\centering
% \footnotesize 
\setlength\tabcolsep{5pt} % default value: 6pt
\begin{tabular}{l|llllll}
\toprule
\textbf{\duy{Model}}           & \textbf{\duy{ mAP}}                                              & \textbf{\duy{ AP50}}                                             & \textbf{\duy{ AP75}}                     & \textbf{\duy{ APs}}                      & \textbf{\duy{ APm}}                                              & \textbf{\duy{ APl}}                      \\ \midrule
\duy{ Faster RCNN}     & \duy{ 63.7}                                             & \duy{ 86.6}                                             & \duy{ 76.9}                     & \duy{ 71.2}                     & \duy{ 58.1}                                             & \duy{ 64.6}                     \\
\duy{ SGRN}            & \duy{ 65.9}                                             & \duy{ 88.8}                                             & \duy{ 79.6}                     & \duy{ 76.3}                     & \duy{ 61.6}                                             & \duy{ 66.3}                     \\
\duy{ Kwon's Pipeline} & \duy{36.2} & \duy{38.5} & \duy{37.2} & \duy{30.3} & \duy{33.1} & \duy{36.0} \\
\duy{ PGPNet}          & \textbf{\duy{ 69.7}}                                             & \textbf{\duy{ 94.4}}                                             & \textbf{\duy{ 83.4}}                     & \textbf{\duy{ 90.0}}                     & \textbf{\duy{ 66.4}}                                             & \textbf{\duy{ 70.1}}                     \\ \bottomrule
\end{tabular}
\end{table}
Table \ref{tab:eval_related} summarizes the comparison of PGPNet, SGRN and Kwon's Pipeline when 
adopting the visual feature extractor architecture from Faster R-CNN with the Resnet-50-FPN model.
%, we add the result of Faster R-CNN as the reference.
%Through the result in Section~\ref{subsec:eval_backbones}, we observe that with the use of Resnet-50-FPN, Faster R-CNN and PGPNet can achieve a more robust result while remaining at an acceptable speed. 
%Hence, from now on, all the result with Faster R-CNN architecture are carried out with the Resnet-50-FPN.
Clearly, SGRN outperforms the baseline Faster R-CNN in terms of overall performance but could not outperforms our proposed method PGPNet. 
Specifically, the mAP metrics achieved by SGRN is $65.9$, and PGPNet achieves the better score with a gap of nearly $4$. Upon other metrics, AP50, AP75, APs, APm, and  APl, PGPNet shows its superior by enhancing the performance from $5.1\%$ (e.g., in AP75 metrics) up to $17.1\%$ (e.g., in APs metrics). This is an expected result because SGRN reveals a major weakness when applying to the challenge of Pill Detection. The spatial relationships between pills in an image are arbitrary and frequently changed. Such noisy and unreliable information leads to the performance of SGRN being unstable and sometimes produce not good enough results. In the case of Kwon's Pipeline, the situation is even worse, since it cannot even beat the vanilla one-step Faster RCNN trained with mutple-pill VAIPE training set. The result of this pipeline is $43.1\%$ and $48.2\%$ worse than vanilla Faster R-CNN and PGPNet respectively. One reason for this deficiency is owing to the quality of its training data. There are many circumstances in which overlap or occlusion occurs, which make the cropped images also contain parts of other pills.

\textbf{Pill Classification Accuracy.} 
Figure \ref{fig:reliable_all}\subref{fig:reliable_sgrn} shows the correlation between the confidence and accuracy of PGPNet in the comparison with those of SGRN. Both the frameworks are based on the Faster R-CNN backbone and achieve similar results e.g., an over-confidence trend in every bin. All the predictions with confidence scores smaller than $0.2$ are totally unreliable (with $0$ accuracy). In addition, PGPNet also shows its superior over SGRN in some bins, in which the over-confidence situation is reduced effectively. We do not plot the Confidence-Accuracy of Kwon's Pipeline owing to space constraint and the obvious performance gap compared to our PGPNet.

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=\columnwidth]{reliability_sgrn.eps}
%     \caption{Reliability investigation for PGPNet and SGRN performance. \label{fig:reliable_sgrn}}
% \end{figure}

\subsubsection*{Ability in Dealing with Hard Samples}
\begin{figure*}[!t]
\begin{minipage}{0.25\linewidth}
    % \begin{figure}[!bt]
    \centering \small
        \includegraphics[width=\columnwidth]{occlusion.pdf}
        \caption{Images with occlusion phenomena in custom occlusion dataset. The rectangles depict examples of tablets with overlapping boundary boxes.
        \label{fig:heavy_occlusion}}
    % \end{figure}
\end{minipage}
\hspace{0.1cm}
\begin{minipage}{0.45\linewidth}
    %\begin{table}[!bt]
    \captionof{table}{Impact of heavy occlusion images on testing performance of PGPNet and Faster R-CNN.\label{tab:occlusion}}
    \centering
    \setlength\tabcolsep{3pt} % default value: 6pt
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{l|cc||cc}
        \toprule
        \multicolumn{1}{c|}{\begin{tabular}[]{@{}c@{}} Test \\dataset \end{tabular}} & 
        \multicolumn{2}{c}{\begin{tabular}[]{@{}c@{}} Custom \\ Occlusion \end{tabular}} & 
        \multicolumn{2}{c}{\begin{tabular}[]{@{}c@{}} Non-\\Occlusion \end{tabular}} \\
        \midrule
        \multicolumn{1}{c|}{Method} & 
        \begin{tabular}[]{@{}c@{}} Faster \\ R-CNN \end{tabular} &  PGPNet & 
        \begin{tabular}[]{@{}c@{}} Faster \\ R-CNN \end{tabular} & PGPNet \\
        \midrule
        mAP     & 59.2 & \textbf{67.5} &   65.6 &   \textbf{71.7} \\
        AP50    & 76.5 & \textbf{81.1} &   87.4 &   \textbf{92.9} \\
        AP75    & 68.9 & \textbf{76.4} &   80.8 &   \textbf{87.0} \\
        APs     & -    & -             &   80.0 &   \textbf{90.0} \\
        APm     & 61.6 & \textbf{68.3} &   56.5 &   \textbf{64.7} \\
        APl     & 60.7 & \textbf{70.1} &   65.1 &   \textbf{70.6} \\
        \bottomrule
        \end{tabular}
    }
    %\end{table}
\end{minipage}
\hspace{0.1cm}
\begin{minipage}{0.25\linewidth}
    % \begin{figure}[!bt]
        \centering
        \includegraphics[width=\columnwidth]{pirate_occlusion.eps}
        \caption{Comparison of PGPNet performance with Faster R-CNN over each individual class in occlusion dataset. \label{fig:occlusion}}
    % \end{figure}
\end{minipage}
% \hspace{0.1cm}
% \begin{minipage}{0.2\linewidth}
    % \begin{figure}
    % \centering
    %     \includegraphics[width=1\columnwidth]{misclassified.pdf}
    %     \caption{Some sample pills with very identical visual appearance with \emph{Hexinvon-8mg}\label{fig:misclassified}}
    % \end{figure}
% \end{minipage}
\vspace{-0.2cm}
\end{figure*}

\begin{figure}
    \centering
        \includegraphics[width=0.75\columnwidth]{misclassified.pdf}
        \caption{Some sample pills with very identical visual appearance with \emph{Hexinvon-8mg}\label{fig:misclassified}}
\end{figure}

{In the following, we investigate the ability of PGPNet in dealing with the occlusion phenomenon caused by overlapping pills, which is one of the most critical issues in dealing with multi-pill detection. 
To this end, we create a so-call \emph{custom occlusion sub-dataset} of VAIPE, which contains images with heavy occlusion phenomena, i.e., having at least two RoIs with the IoU beyond 30\% (Fig.\ref{fig:heavy_occlusion}).  We also create 
a custom \emph{custom non-occlusion sub-dataset}
which contains samples that are in the same classes that appear in the \emph{custom occlusion sub-dataset} but with no occlusion. 
% we experiment on a sub-dataset of VAIPE, which only contains images with heavy occlusion phenomena. We define such image samples as samples that have at least one highly-overlapping couple of bounding boxes, i.e., two boxes that have the IoU (Intersect over Union) greater than $0.3$. Figure \ref{fig:heavy_occlusion} illustrates some images in this sub-dataset. 
The quantitative result is summarized in Table \ref{tab:occlusion}. 
The (-) mark in the table suggests the disregarded or unavailable metrics.
As the numbers suggest, even in cases where heavy occlusion occurs, PGPNet still shows its superior over Faster R-CNN. Specifically, the mAP over all classes in the \emph{custom occlusion sub-dataset} suggests a gap of $8.3\%$ between the two approaches. Interestingly, with the aid of classifier weight as the distinguishing characteristic for each class, PGPNet, even when dealing with occlusion cases still enhances the performance of $1.9\%$ compared to Faster R-CNN handling  the non-occlusion case (e,g, $67.5$ vs. $65.6$, respectively). Figure \ref{fig:occlusion} provides more information about the AP for each class in the \emph{custom occlusion sub-dataset}. PGPNet still outperforms Faster R-CNN in most cases with a large gap, and also produces a more reliable result by introducing a smaller variance over the AP metrics. }

\subsection*{PGPNet's Explainability}

\begin{figure*}[!t]
\begin{minipage}{0.32\linewidth}
% \begin{figure*}
        \centering
        \subfloat[Faster R-CNN]{ %
            \includegraphics[width=0.43\columnwidth]{frcnn_case1.png}
            % \caption{Movement of the Filter Kernel over image receptive fields.}
            \label{fig:eval_rcnn}
        }
        % \hspace{4em}
        \subfloat[PGPNet]{ %
            \includegraphics[width=0.43\columnwidth]{KGPNet_case1.png}
            \label{fig:eval_kgp}
            % \caption{\texttt{Conv} operation applied at a specific location.}
        }
        \caption[Predictions for a hard sample made by Faster R-CNN and PGPNet given the same image.]{Predictions for a hard sample made by Faster R-CNN and PGPNet given the same image.}
        %\textbf{\subref{fig:eval_rcnn}} - Faster R-CNN; 
        %\textbf{\subref{fig:eval_kgp}} - PGPNet.} 
        \label{fig:kg_robust_instance}
        \vspace{0.1cm}
        % \centering
        % \includegraphics[width=\columnwidth]{pseudo_scores.pdf}
        % \caption{Probabilistic scores produced by PGPNet's Pseudo Classifier.}
        % \label{fig:pseudo_scores}
% \end{figure*}
\end{minipage}
\hspace{0.1cm}
\begin{minipage}{0.64\linewidth}
    % \begin{figure*}
        \centering \small
        \subfloat[Input]{ %
            \includegraphics[width=0.21\columnwidth,valign=c]{original.png}
            % \caption{\textit{Input}}
            \label{fig:original}
        }
        \hspace{-0.15em}%
        % \hspace{4em}%
        \centering
        \subfloat[LIVOLIN-FORTE]{ %
            \includegraphics[width=0.21\columnwidth,valign=c]{ex_1.png}
            % \caption{\textit{LIVOLIN-FORTE}}
            \label{fig:exp_1}
        }
        \hspace{-0.15em}%
        % \hspace{4em}%
        \subfloat[Hapenxin]{ %
            \includegraphics[width=0.21\columnwidth,valign=c]{ex_2.png}
            \label{fig:exp_2}
            % \caption{\textit{Hapenxin}}
        }
        \hspace{-0.15em}%
        % \hspace{4em}%
        \subfloat[Hexinvon-8mg]{ %
            \includegraphics[width=0.21\columnwidth,valign=c]{ex_3_2.png}
            \label{fig:exp_3}
            % \caption{\textit{Hexinvon-8mg}}
        }
        % \includegraphics[width=0.8\textwidth]{conv_opt.png}
        \caption{The saliency maps for each of the groundtruth labels included in the image instance. For simple samples (LIVOLIN-FORTE and Hapenxin), the classifier focuses on the exact location of the tablets to determine their identity. In contrast, for the hard case (Hexivon-8mg), information on both Hexivon and LIVOLIN-FORTE served as evidence. \label{fig:XAI_heatmap} }
    % \end{figure*}
\end{minipage}
\vspace{-0.5cm}
\end{figure*}


%\subsubsection{Interpreting the Detection Results}
% \begin{figure}[tbh]
% \centering
% \begin{subfigure}{\textwidth}
%   \centering
%   % include first image
%   \includegraphics[width=\linewidth]{corr_frcnn.pdf}  
%   \caption{Faster R-CNN}
%   \label{fig:corr_frcnn}
% \end{subfigure}
% \begin{subfigure}{\textwidth}
%   \centering
%   % include second image
%   \includegraphics[width=\linewidth]{corr_kgp.pdf}  
%   \caption{PGPNet}
%   \label{fig:corr_kgp}
% \end{subfigure}
% \caption{Confusion Matrices of predicted labels made by two framework without and with MCG Leverage}
% \end{figure}

% \begin{figure}[tbh]
% \subfloat[Faster R-CNN]{%
%   \includegraphics[clip,width=\columnwidth]{corr_frcnn.pdf}%
%   \label{fig:corr_frcnn}
% }
% \vspace{10pt}
% \subfloat[PGPNet]{%
  % \includegraphics[clip,width=\columnwidth]{corr_kgp.pdf}%
  % \label{fig:corr_kgp}
% }
% \caption{Confusion Matrices of predicted labels made by two framework without and with MCG Leverage. \textcolor{red}{Need a bigger zoom-out} {Need to find different type of plot for better visualization. If use only main diagonal - cannot see which class is often miscassified as which class}}
% \end{figure}
This section is dedicated to analyzing the results produced by PGPNet through a specific sample. This example demonstrates that the operation of PGPNet is very congruent with our initial motivation and that our designed architecture can materialize this motivation.
%This section is dedicated for analysing the results produced by PGPNet through a specific sample. With this sample, the working of PGPNet can be shown to be in high accordance with our initial motivation, and our designed architecture can materialize that motivation.

% Figure \ref{fig:corr_frcnn} and \ref{fig:corr_kgp} respectively display the label confusion matrix produced by Faster R-CNN and PGPNet on testing dataset. The two confusion matrics are \textit{row-wisedly normalized} before being used for plotting. In addition, all the zero values are \textit{disregarded} in this visualisation, hence having white color. By observing the main diagonal of the matrix, it can be seen that there are many labels which are still misclassified (in red color) made by Faster R-CNN. The situation is much positive with PGPNet, most of the labels are correctly classified and the confusion cases are greatly reduced.

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=1\columnwidth]{misclassified.pdf}
%     \caption{Some sample pills with very identical visual appearance with \emph{Hexinvon-8mg}}
%     \label{fig:misclassified}
% \end{figure}

\subsubsection*{Experiment settings.} 
In this experiment, we choose a hard sample, namely \emph{Hexinvon-8mg}, with a relatively common appearance, for investigation. Figure \ref{fig:misclassified} visualizes \emph{Hexinvon-8mg} together with other pills in our dataset with almost identical visual appearance (round shape, white tint, etc.). 
As illustrated, these pills are readily confused with \emph{Hexinvon-8mg}.
%due to their almost identical physical characteristics (round shape, white tint, etc.).  
Indeed, Fig.~\ref{fig:kg_robust_instance} depicts an example in which  \emph{Hexinvon-8mg} is miscategorized as \emph{Alpha-Chymotrypsine} by Faster R-CNN. 
Our PGPNet can, however, successfully distinguish \emph{Hexinvon-8mg} with a high confidence score.
In the following, we applied several Explainable AI techniques to explain the results inferred by our PGPNet. The image of interest consists of three pills: LIVOLIN-FORTE,  Hapenxin, and Hexivon as shown in Fig.~\ref{fig:kg_robust_instance}. 

% For PGPNet, by observing the row of \emph{Hexinvon-8mg}, it can be seen that the problem is successfully alleviated with the aid of MCG graph. 
%, while in the case of Faster R-CNN, this pill is miscategorized as \emph{Alpha-Chymotrypsine}. 
% Though the positive result of PGPNet can straightforwardly be captured here, little can we conclude about our model's reason for producing that correct output. To this end, we applied several Explainable AI techniques on the computation pathway for this image instance.

% \begin{figure}
%     \centering
%     \subfloat[]{ %
%         \includegraphics[width=0.45\columnwidth]{frcnn_case1.png}
%         % \caption{Movement of the Filter Kernel over image receptive fields.}
%         \label{fig:eval_rcnn}
%     }
%     \hfill
%     \subfloat[]{ %
%         \includegraphics[width=0.45\columnwidth]{KGPNet_case1.png}
%         \label{fig:eval_kgp}
%         % \caption{\texttt{Conv} operation applied at a specific location.}
%     }
%     % \includegraphics[width=0.8\textwidth]{conv_opt.png}
%     \caption[Predictions for a hard sample made by Faster R-CNN and PGPNet given the same image.]{Predictions for a hard sample made by Faster R-CNN and PGPNet given the same image.
%         \textbf{\subref{fig:eval_rcnn}} - Faster R-CNN; 
%         \textbf{\subref{fig:eval_kgp}} - PGPNet.} 
%     \label{fig:kg_robust_instance}
%     \vspace{-10px}
% \end{figure}

\subsubsection*{Explanation of the Prediction Results}
We adopt the Excitation Backpropagation technique proposed by Zhang \cite{exb} to construct the saliency maps (Fig.\ref{fig:XAI_heatmap}), which indicate what the classifier has learned to produce the final results.
Firstly, for the easy samples, i.e., LIVOLIN-FORTE and Hapenxin, our model focuses precisely on those pill regions to make the prediction decision. 
In contrast, in the case of the hard sample, i.e., \emph{Hexinvon-8mg}, however, two regions are highlighted: one at the position of \emph{Hexinvon-8mg} and the other at the location of LIVOLIN-FORTE.
It indicates that the classifier solely requires information about LIVOLIN-FORTE and Hapenxin to identify these pills. Nevertheless, for \emph{Hexinvon-8mg}, the classifier must additionally incorporate information about its neighbor, i.e., LIVOLIN-FORTE.
This hypothesis is also supported by the Probabilistic score matrix shown in Fig.~\ref{fig:pseudo_scores}.
The probabilistic score matrix represents the prediction results generated by our Pseudo Classifier, which relies mainly on the pill's visual characteristics.
As demonstrated, Pseudo Classifier can accurately detect the proper labels of two simple samples, with their prediction scores approaching $1$,  and boost up their neighbors' probabilities (label ID $7$, $17$, etc.). However, with the case of \emph{Hexinvon-8mg}, the probability scores are relatively low, with all RoIs being investigated achieving scores of only about $0.3$.
% We adopt the Excitation Backpropagation technique proposed by Zhang \cite{exb} to compute the saliency maps (Fig.\ref{fig:XAI_heatmap}). 
% This figure figures out what has been learned by the classifier (i.e., the hilighted regions in the hear) to produce the final results. 
% Firstly, for the easy cases, i.e., LIVOLIN-FORTE and Hapenxin, our model focuses exactly on those pill region to make the prediction decision. 
% In the contrast, in the case of the hard sample, i.e., Hexivon, there are two concentrated regions, one is at the location of Hexivon, and the other is at the location of LIVOLIN-FORTE.
% It means that, the classifier needs only information related to LIVOLIN-FORTE and Hapenxin to identify these pills; however, for Hexivon, the classifier need to takes into account information of both Hexivon and LIVOLIN-FORTE. 
% This hypothesis is also supported by the Probabilistic score matrix shown in Fig.~\ref{fig:pseudo_scores}.
% This probabilistic score matrix depicts the prediction results made by our Pseudo Classifier which is mainly relies on the pill visual features. 
% As shown, Pseudo Classifier can effectively determine the correct labels of two easy samples, with their prediction scores approach $1$, as well as boost up their neighbors' probabilities (label ID $7$, $17$, etc.). However, with the case of \emph{Hexinvon-8mg}, the scores for this label are relatively low - about $0.3$ through all RoIs being investigated. 

Now, we utilize another explainable AI technique named GNNExplainer \cite{gnn_explainer} to investigate further the reason for identifying the hard sample, \emph{Hexinvon-8mg}. GNNExplainer is a model-agnostic architecture that can provide interpretable explanations for predictions of graph-based models. Specifically, GNNExplainer may identify a subgraph and a subset of node features that have a significant role in the prediction outcomes.
In our experiment, we treat our Graph Transformer Network as a module that produces regression output, i.e., the context vectors corresponding to all RoIs. For a more comprehensible result, we set the number of RoIs selected from the RPN module to ten, consisting of the five RoIs with the greatest \emph{objectness} scores and the other five with the lowest score. 
We utilize GNNExplainer to identify the sub-graph that contributes the most in recognizing \emph{Hexinvon-8mg}. 
The results are demonstrated in Fig.~\ref{fig:gnn_explain}. 
In this figure, the white box depicts the RoI of \emph{Hexinvon-8mg}, the two orange boxes and blue boxes represent the RoIs of LIVOLIN-FORTE, and Hapenxin, respectively, while the five gray boxes indicate the RoIs of noise.
The black edges represent the vital connections, whose weights are proportionate to the width of the edges.
First, there are almost no edges between the nodes representing \emph{Hexinvon-8mg} and those of the noise RoIs.
It implies that the noise RoIs do not cue the prediction of \emph{Hexinvon-8mg}.
In contrast, there are bolded linkages between the RoIs of LIVOLIN-FORTE, Hapenxin, and \emph{Hexinvon-8mg}. 
These findings, along with the saliency map (Fig.\ref{fig:XAI_heatmap}), interpret that PGPNet has learned both the visual characteristic of the pill itself and the relationship between that pill and the others to make the final decision.

% have the maximum \emph{objectness scores} among those proposed by RPN after Non-maximum Suppression-based post-processing procedure, while $5$ remainings represents the RoIs with least scores. 
% As GNNExplainer suggests, the most critical nodes for generating this context presentation relate to RoIs that include genuine neighbor pills, and all unnecessary background RoIs are efficiently filtered out by GTN in the computation process. In addition, regarding the edges, while there are still some noisy edges which link the actual labels' nodes to the background ones, the majority and the most important ones are the connections that directly connect \emph{Hexinvon-8mg}'s RoI node with the true labels neighbors. 
% In addition, with this experiment, we only target finding the optimal sub-graph structure that directly determine a given output context presentation, without figure out which features affect the result most.  
% Figure \ref{fig:gnn_saliency} demonstrates the sub-graph that mostly contribute to the construction of context vector for \emph{Hexinvon-8mg}'s RoI. 
% The edges in black denote the important connections, whose weights are proportional to the edges' width. As GNNExplainer suggests, the most critical nodes for generating this context presentation relate to RoIs that include genuine neighbor pills, and all unnecessary background RoIs are efficiently filtered out by GTN in the computation process. In addition, regarding the edges, while there are still some noisy edges which link the actual labels' nodes to the background ones, the majority and the most important ones are the connections that directly connect \emph{Hexinvon-8mg}'s RoI node with the true labels neighbors. 

% \textbf{Pseudo Classifier Interpretation.} First, we visualize the pseudo probabilities produced by our Pseudo Classifier in Figure \ref{fig:pseudo_scores}. Here, \emph{LIVOLIN-FORTE} corresponds to label ID $63$, \emph{Hapenxin} with ID $57$, and $59$ is the hard label \emph{Hexinvon-8mg}. As the figure suggests, with the aid of our auxiliary loss, Pseudo Classifier can effectively determine the correct labels of two easy samples - with their prediction scores approach $1$, as well as boost up their neighbors' probabilities (label ID $7$, $17$, etc.). However, with the case of \emph{Hexinvon-8mg}, the scores for this label are relatively low - about $0.3$ through all RoIs being investigated. This result is reasonable and compatible with our motivation stated at the beginning, as the model is confused if only relying on its visual features. We further verify this claim with the visualization of Saliency maps in Figure \ref{fig:kg_saliency}. Here, we adopt the technique proposed by Zhang \cite{exb} to compute the saliency maps - Excitation Backpropagation. The hot regions in these maps indicate that for both \emph{Hexinvon-5mng} - our desired label and \emph{Loratadine-10mg} - another label with similar visual appearance, our framework up to the Pseudo Classifier can successfully determine the correct spatial regions that are important to this two labels, other than the noise or background in the image. However, the hot areas in both salience maps collide in one same white-round pill region, which indicates the model uncertainty about what is the exact label corresponding to those features. 


\begin{figure*}[!t]
\begin{minipage}{0.4\linewidth}
    % \begin{figure}
        \centering
        \includegraphics[width=0.9\columnwidth]{pseudo_scores.pdf}
        \caption{Probabilistic scores produced by PGPNet's Pseudo Classifier. \label{fig:pseudo_scores}}
    % \end{figure}
\end{minipage}
\hspace{0.1cm}
\begin{minipage}{0.55\linewidth}
% \begin{figure}
        \centering \small
        \subfloat[Bounding boxes of the RoIs.]{ %
            \includegraphics[width=0.4\columnwidth,valign=c]{rpn_10.png}
            % \caption{\textit{Input}}
            \label{fig:rpn_choose10}
        }
        \hfill%
        % \hspace{4em}%
        \centering
        \subfloat[Sub-graph identified by GNNExplainer.]{ %
            \includegraphics[width=0.5\columnwidth,valign=c]{salience_gnn.pdf}
            % \caption{\textit{LIVOLIN-FORTE}}
            \label{fig:gnn_saliency}
        }
        % \includegraphics[width=0.8\textwidth]{conv_opt.png}
         \caption{Interpretation of the prediction result for using GNNExplainer.} 
    
    % \centering
    % \begin{subfigure}[t]{0.4\columnwidth}
    % \includegraphics[width=\columnwidth,valign=c]{rpn_10.png}
    %     \caption{Bounding boxes of the RoIs.}
    %     \label{fig:rpn_choose10}
    % \end{subfigure}
    % % \subfloat[]{ %
    % % }
    % \hfill%
    % \begin{subfigure}[t]{0.5\columnwidth}
    %     \includegraphics[width=\columnwidth,valign=c]{salience_gnn.pdf}
    %     \caption{Sub-graph identified by GNNExplainer} %, indicating the RoIs most influential to the prediction of \emph{Hexinvon-8mg}.
    %     \label{fig:gnn_saliency}
    % \end{subfigure}
    % \subfloat[]{ %
        
    % }
    % \includegraphics[width=0.8\textwidth]{conv_opt.png}
   
    % \label{fig:gnn_saliency}
% \end{figure}
\end{minipage}
% \hspace{0.1cm}
% \begin{minipage}{0.25\linewidth}
%     \centering
% \end{minipage}
\vspace{-0.3cm}
\end{figure*}



\begin{figure}[!tb]
        \centering
        \includegraphics[width=0.5\columnwidth]{ridgeline_edge_exp.eps}
       \caption{Distributions of Average Precision recorded over all classes produced by PGPNet with different MCG versions. \label{fig:edge_modify}}
    \end{figure}


    \begin{figure}[!tb]
        \centering
        \includegraphics[width=0.5\columnwidth]{ridgeline_nodes_exp.eps}
       \caption{Distributions of Average Precision recorded over the classes in $N_A$ set produced by PGPNet with different MCG versions. \label{fig:node_modify}}
    \end{figure}
    
   
% \textbf{Graph Transformer Network Interpretation.} Having gained valuable insights into the operation of the Pseudo Classifier, we analyze the actual operation of the Graph Transformer Network in order to evaluate its effectiveness in generating context vectors for each RoI - especially in this case, the RoI corresponding to \emph{Hexivon-8mg}. The technique proposed by Ying and partners \cite{gnn_explainer} is leveraged for this purpose. With this experiment, we treat our Graph Transformer Network as a module that produce regression output - the context vectors corresponding to all RoIs. For a more comprehensible result, the number of choosen RoIs from RPN module is set as $10$; the first $5$ has maximum \emph{objectness scores} among those proposed by RPN after Non-maximum Suppression-based post-processing procedure, while $5$ remainings represents the RoIs with least scores. In addition, with this experiment, we only target finding the optimal sub-graph structure that directly determine a given output context presentation, without figure out which features affect the result most.  
% Figure \ref{fig:gnn_saliency} demonstrates the sub-graph that mostly contribute to the construction of context vector for \emph{Hexinvon-8mg}'s RoI. 
% The edges in black denote the important connections, whose weights are proportional to the edges' width. As GNNExplainer suggests, the most critical nodes for generating this context presentation relate to RoIs that include genuine neighbor pills, and all unnecessary background RoIs are efficiently filtered out by GTN in the computation process. In addition, regarding the edges, while there are still some noisy edges which link the actual labels' nodes to the background ones, the majority and the most important ones are the connections that directly connect \emph{Hexinvon-8mg}'s RoI node with the true labels neighbors. 

% \textcolor{red}{@Duy: \\
% - With the aid of auxiliary loss, the desire behavior is achieved: filter out easy samples, selecting candidates for hard samples - DONE \\ 
% - Plot the GNN Explainer result to explain the context vector generation process: for the context vector of hexivon 8mg, the neighborhood pill that take the most important roles is the neighborhood one\\
% - Plot the salience map produced by Exciation Backpropagation - compare with that of Faster-RCNN: with case of PGPNet: not only the section of Hexivon, but also other sections of other pills also make impact \\
% }
% These interpretations of a particular sample image reveal the inner workings of our framework, which is highly congruent with our design aim and motivation. By basing on the context pills around the hard pill samples - in this case \emph{Hexinvon-8mg}, PGPNet have well differentiated and determined its true label, successfully solve the problem of its baseline Faster R-CNN.

\subsection*{Ablation Studies}
In this section, we perform extensive ablation studies to investigate the impacts of the main techniques proposed in our PGPNet and to investigate how each component in the proposed method helps to improve learning performance. Specifically, we alter the Co-occurrence Graph and observe how it affects the detection results in Section \nameref{sec:ablation_mcg}. We then assess the effects of using the relational graphs, the Graph transformer network, and the proposed auxiliary loss in Sections \nameref{sec:ablation_relational graphs}, \nameref{sec:ablation_gtn}, %\ref{sec:ablation_aux}, 
respectively.

\subsubsection*{Effect of Co-occurrence Graph's Quality}
\label{sec:ablation_mcg}
% This section is dedicated for describing the effects of MCG on the performance of Pill Detector frameworks.
In this section, we perform two experiments to observe how the performance is changed when the nodes set and edges set of MCG are modified respectively.

\textbf{Edge Set Modification.}
We first observe the behavior of our PGPNet when adding noise edges and removing actual edges. We set up four scenarios which are the combinations of removing $25\%$ and $50\%$ of the edges in the set $E_1$, and adding a number of synthesized edges corresponding to $25\%$ and $50\%$ of the cardinality of $E_1$.

Figure \ref{fig:edge_modify} illustrates the performances of PGPNet with all Medical Co-occurrence Graph variances when being put into comparison with the original one. The performance here is denoted by the general metrics AP. As indicated by AP density, PGPNet with original MCG generates a more concentrated density with a smaller variance and a higher mean than other variances. In addition, when $50\%$ of edges are eliminated, the performance is clearly inferior to when $25\%$ of edges are eliminated. The figure concludes with the intriguing observation that eliminating edges at random would result in a greater performance decrease than adding noisy edges. This is because, even with the addition of noisy edges, PGPNet could still filter out unnecessary information through the training process. When excluding edges, the situation is different because the framework cannot learn the external knowledge contained in the eliminated edges.

% \begin{figure*}[!t]
% \begin{minipage}{0.32\linewidth}
% % % \begin{figure}[tb]
% %     % \hfill%
% %     % \begin{subfigure}[t]{0.48\columnwidth}
% %         \includegraphics[width=0.9\columnwidth]{ridgeline_nodes_exp.eps}
% %         \caption{Distributions of Average Precision recorded over the classes in $N_A$ set produced by PGPNet with different MCG versions. \label{fig:node_modify}}
% %     % \end{subfigure}
% % % \end{figure}
% \end{minipage}
% % \hspace{0.1cm}
% % \begin{minipage}{0.68\linewidth}
    
% % \end{minipage}
% \vspace{-0.5cm}
% \end{figure*}

\begin{table*}[!t]
    \captionof{table}{Performance of PGPNet with the diferent combination of its components, i.e., when removing (marked as $\times$) / keeping (marked as \checkmark) the relational graph, GTN and auxiliary loss. Numbers inside the (.) represent the gap in percentage compared to the full version of PGPNet.\label{tab:ablation_result}}
    \centering
    \small
    %\setlength\tabcolsep{3pt} % default value: 6pt
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{l|ccccc|l|l|l|l|l|l}
    \toprule
    \multicolumn{1}{l|}{} & \multicolumn{5}{c|}{\textbf{Component}}  & \multicolumn{6}{c}{\textbf{Performance}}
    \\
    & $\mathcal{G}_c$ & $\mathcal{G}_s$ & $\mathcal{G}_v$ & GTN & $\mathcal{L}_{aux}$ &   mAP & AP50 & AP75 & APs & APm & APl
    \\ \midrule
    Faster R-CNN & $\times$  & $\times$   & $\times$   & $\times$    & $\times$    &
      \begin{tabular}[c]{@{}r@{}}63.7 (-8.6)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}86.7 (-8.4)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}76.9 (-7.9)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}71.3 (-20.8)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}58.1 (-10.6)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}64.6 (-7.8)\end{tabular}  \\
    PGPNet-v1    & \checkmark  & $\times$   & $\times$   & $\times$    & $\times$     &
      \begin{tabular}[c]{@{}r@{}}65.9 (-5.5)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}91.9 (-2.9)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}79.6 (-4.6)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}72.5 (-19.4)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}62.3 (-4.3)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}66.0 (-5.8)\end{tabular}  \\
    PGPNet-v2    & \checkmark  & $\times$  & \checkmark  & \checkmark   & \checkmark   &
      \begin{tabular}[c]{@{}r@{}}66.9 (-3.9)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}92.1 (-2.7)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}81.1 (-2.8)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}80.0 (-11.1)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}61.3 (-5.9)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}67.6 (-3.6)\end{tabular} \\
    PGPNet-v3    & \checkmark  & \checkmark  & $\times$  & \checkmark   & \checkmark   &
      \begin{tabular}[c]{@{}r@{}}67.8 (-2.8)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}92.9 (-1.9)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}82.3 (-1.5)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}82.5 (-8.3)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}62.7 (-3.5)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}68.1 (-2.8)\end{tabular} \\
    PGPNet-v4    & \checkmark  & \checkmark  & \checkmark  & $\times$   & \checkmark  &
      \begin{tabular}[c]{@{}r@{}}68.4 (-1.9)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}92.6 (-2.2)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}81.7 (-2.1)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}80.0 (-11.1)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}64.4 (-1.0)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}68.8 (-1.8)\end{tabular}  \\
    PGPNet-v5    & \checkmark  & \checkmark & \checkmark  & \checkmark   & $\times$  &
      \begin{tabular}[c]{@{}r@{}}67.2 (-3.6)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}91.3 (-3.5)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}80.9 (-3.1)\end{tabular} &
      90.0 (+0.0) &
      \begin{tabular}[c]{@{}r@{}}62.2 (-4.3)\end{tabular} &
      \begin{tabular}[c]{@{}r@{}}67.3 (-3.9)\end{tabular}  \\
    PGPNet    & \checkmark  & \checkmark  & \checkmark  & \checkmark   & \checkmark   &
      69.7 &
      94.7 &
      83.5 &
      90.0 &
      65.0 &
      70.0 \\ \bottomrule
    \end{tabular}
    }
    \end{table*}

\textbf{Node Set Modification.}
To observe PGPNet's performance when the Medical Co-occurrence Graph lacks information on some specific nodes - classes, we design two different scenarios. In the first one, $25\%$ nodes are removed in the original graph, this set is denoted as $N_A$. For the latter, $50\%$ of nodes are eliminated, and the corresponding set $N_B$ is ensured to be a superset of $N_A$. The performances of PGPNet in two circumstances are compared with itself when having the full MCG, considering only the classes appeared in the set $N_A$.

Figure \ref{fig:node_modify} depicts the outcome of this experiment. The AP across all $N_A$ classes is used to evaluate performance here. As indicated by the graph, node removals also result in a significant decrease in model performance. More interestingly, the more nodes being eliminated, the greater drop is captured. Specifically, the AP density in case MCG contains only $50\%$ of remaining nodes has a great variance, with the mean value only around $60\%$. 

In the following, we study the effectiveness of the relational graphs, Graph Transformer Network (GTN) block, and auxiliary loss. 
The detailed configurations are presented in Table \ref{tab:ablation_result}. The $+$ sign indicates the presence of a component in a specific version, while $-$ denotes the opposite.  
%Table \ref{tab:ablation_result} summarizes the experimental results. 

% Since my vanilla backbone model is Faster R-CNN, without any proposed module, it also listed in the table. The ablation of each module is quite straight-forward to adopt without any replacement needed.

% \begin{table}[]
% \begin{tabular}{l|rrrrrr}
% \toprule
% Models &
  % AP &
  % AP50 &
  % AP75 &
  % APs &
  % APm &
  % APl \\ \hline
% Faster RCNN &
  % \begin{tabular}[c]{@{}r@{}}63.713 \\ (-8.57)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}86.663 \\ (-8.44)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}76.925 \\ (-7.85)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}71.262 \\ (-20.82)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}58.109 \\ (-10.63)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}64.598 \\ (-7.78)\end{tabular} \\ \hline
% PGPNet-v1 &
  % \begin{tabular}[c]{@{}r@{}}65.879 \\ (-5.46)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}91.945 \\ (-2.86)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}79.618\\  (-4.63)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}72.525 \\ (-19.42)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}62.25 \\ (-4.26)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}65.995 \\ (-5.78)\end{tabular} \\ \hline
% PGPNet-v2 &
  % \begin{tabular}[c]{@{}r@{}}66.931 \\ (-3.95)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}92.136 \\ (-2.66)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}81.13 \\ (-2.82)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}80 \\ (-11.11)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}61.216 \\ (-5.85)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}67.562 \\ (-3.55)\end{tabular} \\ \hline
% PGPNet-v3 &
  % \begin{tabular}[c]{@{}r@{}}67.754 \\ (-2.77)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}92.904 \\ (-1.85)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}82.272 \\ (-1.45)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}82.5 \\ (-8.33)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}62.729 \\ (-3.52)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}68.121 \\ (-2.75)\end{tabular} \\ \hline
% PGPNet-v4 &
  % \begin{tabular}[c]{@{}r@{}}68.375 \\ (-1.88)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}92.559 \\ (-2.21)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}81.728 \\ (-2.1)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}80 \\ (-11.11)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}64.358 \\ (-1.02)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}68.748 \\ (-1.85)\end{tabular} \\ \hline
% PGPNet-v5 &
  % \begin{tabular}[c]{@{}r@{}}67.176 \\ (-3.6)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}91.34 \\ (-3.5)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}80.857 \\ (-3.14)\end{tabular} &
  % 90 &
  % \begin{tabular}[c]{@{}r@{}}62.23 \\ (-4.29)\end{tabular} &
  % \begin{tabular}[c]{@{}r@{}}67.341 \\ (-3.86)\end{tabular} \\ \hline
% PGPNet&
  % 69.686 &
  % 94.651 &
  % 83.481 &
  % 90 &
  % 65.02 &
  % 70.046 \\ \bottomrule
% \end{tabular}
% \caption{Performance of PGPNet when removing relational graph, GTN and auxiliary loss. \label{tab:ablation_result}}
% \end{table}

\subsubsection*{Effects of the Relational Graphs}
\label{sec:ablation_relational graphs}
In this section, we study the effectiveness of the Size-graph and visual-based graph. 
To this end, we implement two simplified versions of PGPNet, namely PGPNet-v2 and  PGPNet-v3, in which we remove the Size-graph and visual-based graph, respectively. 
As shown in Table \ref{tab:ablation_result}, eliminating the Size-graph causes a decrease in performance from 3.9\% to 11.1\%, while omitting the visual-based graph reduces the accuracy from 2.8\% to 8.3\%.
An interesting finding is that the deterioration gap when removing the size graph is more significant than those when eliminating the visual-based graph in terms of all evaluation metrics. 
These findings imply the effectiveness of the Size-graph over the visual-based graph.
Moreover, it can be observed that mAP is the most impacted when the relational graphs are removed, followed by AP50, when comparing mAP, AP50, and AP75.
This can be explained as follows.
In AP75, we measure the precision of RoIs with the IoU beyond 75\%, which presumably has a high degree of confidence regarding the objective.
In contrast, when we reduce the IoU threshold, such as AP50 and mAP, the overlap area of the objective drops, resulting in a model with a significant degree of uncertainty.
In this case, integrating relational graphs provides additional data that reduces uncertainty, thereby boosting detection accuracy.
% Moreover, it can be observed that when removing the relational graph, the APs is the metric decreased the most (i.e., more than 11\%). 
\subsubsection*{Effects of the Multi-modal Data Fusion Block and Auxiliary Loss}
\label{sec:ablation_gtn}
To investigate the effectiveness of the GTN, we implement PGPNet-v4, omitting the GTN block and relying solely on the GCN to learn the node representation. 
Results in Table~\ref{tab:ablation_result} reveal that GTN enhances the model's accuracy from $1.0\%$ to $11.1\%$. 
Comparing mAP, AP50, and AP75, AP50, and AP75 are slightly more influenced by GTN than mAP, but the gaps are trivial.
% \subsubsection{Effects of the Proposed Auxiliary Loss}
% \label{sec:ablation_aux}
We employ PGPNet-v5, which eliminates the proposed auxiliary loss and compare its performance with the original PGPNet. 
% The experimental results are shown in Table \ref{tab:ablation_result} under the model name of PGPNet-v5.
As illustrated in Table \ref{tab:ablation_result}, adopting our auxiliary loss may result in a 3 to 4 percent performance gain for most evaluation metrics. 
In the final ablation study, we implement PGPNet-v1, which retains only the co-occurrence graph and removes all the other components.
As depicted in Table \ref{tab:ablation_result}, the detection accuracy degrades significantly, with a gap ranging from $2.9\%$ to $19.4\%$. However, even with this version, PGPNet is still superior to Faster RCNN, with a performance margin of up to $7.1\%$.

In conclusion, the PGPNet version with all components exhibits its superiority in all evaluation metrics. In addition, all versions of PGPNet are superior to the Faster R-CNN backbone, demonstrating the contribution of each component to the overall performance of PGPNet.


\section*{Conclusion}
\label{sec:conclusion}
\textbf{Contributions.} We proposed PGPNet, a reliable and explainable pill detection framework in real-world settings. To deal with hard samples, PGPNet leveraged external knowledge, including co-occurrence likelihood, relative pill size, and visual semantic correlation during the training process. We implemented PGPNet into two popular object detectors and evaluated the proposed method on a real-world multiple pill detection dataset. The experimental results demonstrated that it could improve these models by considerable margins. Moreover, our comprehensive ablation studies proved the robustness, reliability, and explainability of the proposed framework. Future work will aim to evaluate PGPNet under a federated learning setting as well as other image-text understanding datasets.

\noindent \textbf{Limitations and Future Works.} Although the effectiveness of PGPNet largely relies on external information from a graph, we recognize that this external knowledge may not always be practical or feasible in all hospitals and locations. Additionally, our co-occurrence graph building process currently relies on prescription data tied to the VAIPE dataset. However, in order for PGPNet to be suitable for practical settings, this graph needs to be expanded to include more pills and relationships, which may make it impractical to deploy the framework to actual devices and applications. To address this, we plan to collect more prescriptions and construct more general medical knowledge graphs in our future work. Additionally, we plan to optimize the computational requirements when scaling up our framework.


\section*{Acknowledgments}

This work was funded by Vingroup Joint Stock Company and supported by Vingroup Innovation Foundation (VINIF) under project code $VINIF.2021.DA00128$. We thank all our collaborators who participated in the collection and annotation of the VAIPE dataset.

\bibliographystyle{abbrv}
\bibliography{references}
\end{document}



