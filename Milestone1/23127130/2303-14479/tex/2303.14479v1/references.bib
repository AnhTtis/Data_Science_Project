@article{Veldkamp2009,
title = "Dose and perceived image quality in chest radiography",
journal = "European Journal of Radiology",
volume = "72",
number = "2",
pages = "209--217",
year = "2009",
author = "Wouter J.H. Veldkamp and Lucia J.M. Kroft and Jacob Geleijns",
}

@inproceedings{Ma2020,
  author    = {Jeffrey Ma and Ukash Nakarmi and Cedric Yue Sik Kin and Christopher Sandino and Joseph Y. Cheng and Ali B. Syed and Peter Wei and John M. Pauly and Shreyas Vasanawala
},
  title     = {Diagnostic Image Quality Assessment and Classification in Medical
               Imaging: Opportunities and Challenges},
  booktitle = {IEEE ISBI},
  pages     = {337--340},
  year      = {2020},
  timestamp = {Sun, 07 Jun 2020 18:49:04 +0200},
}

@article{Barrett2004,
author = {Barrett, Julia F. and Keat, Nicholas},
title = {Artifacts in CT: Recognition and Avoidance},
journal = {RadioGraphics},
volume = {24},
number = {6},
pages = {1679--1691},
year = {2004},
}

@inproceedings{Laue2020,
  author    = {S{\"{o}}ren Laue and
               Matthias Mitterreiter and
               Joachim Giesen},
  title     = {A Simple and Efficient Tensor Calculus},
  booktitle = {AAAI},
  pages     = {4527--4534},
  year      = {2020},
}

@INPROCEEDINGS{Rebuffi20,
  author={S. -A. {Rebuffi} and R. {Fong} and X. {Ji} and A. {Vedaldi}},
  booktitle={IEEE CVPR}, 
  title={There and Back Again: Revisiting Backpropagation Saliency Methods}, 
  year={2020},
  volume={},
  number={},
  pages={8836--8845},
}

@article{Selvaraju2020,
  author    = {Ramprasaath R. Selvaraju and Michael Cogswell and Abhishek Das and Ramakrishna Vedantam and Devi Parikh and Dhruv Batra},
  title     = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based
               Localization},
  journal   = {IJCV},
  volume    = {128},
  number    = {2},
  pages     = {336--359},
  year      = {2020},
  timestamp = {Fri, 13 Mar 2020 10:58:48 +0100},
}

@inproceedings{Springenberg2015,
  author    = {Jost Tobias Springenberg and
               Alexey Dosovitskiy and
               Thomas Brox and
               Martin A. Riedmiller},
  title     = {Striving for Simplicity: The All Convolutional Net},
  booktitle = {ICLR},
  year      = {2015},
}

@InProceedings{Shrikumar2016,
  author =   {Avanti Shrikumar and Peyton Greenside and Anshul Kundaje},
  title = 	 {Learning Important Features Through Propagating Activation Differences},
  booktitle = {ICML},
  pages = 	 {3145--3153},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/shrikumar17a/shrikumar17a.pdf},
}

@inproceedings{Zhang16,
  author    = {Jianming Zhang and
               Zhe L. Lin and
               Jonathan Brandt and
               Xiaohui Shen and
               Stan Sclaroff},
  editor    = {Bastian Leibe and
               Jiri Matas and
               Nicu Sebe and
               Max Welling},
  title     = {Top-Down Neural Attention by Excitation Backprop},
  booktitle = {ECCV Part {IV}},
  series    = {Lecture Notes in Computer Science},
  volume    = {9908},
  pages     = {543--559},
  year      = {2016},
  timestamp = {Wed, 25 Sep 2019 18:11:12 +0200},
}

@InProceedings{Wang2020,
    author = {Haofan Wang and Zifan Wang, Mengnan Du and Fan Yang and Zijian Zhang and Sirui Ding and Piotr Mardziel and Xia Hu},
    title = {Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks},
    booktitle = {IEEE CVPRW},
    year = {2020}
}

@INPROCEEDINGS{Deng2009,
  author={J. {Deng} and W. {Dong} and R. {Socher} and L. {Li} and  {Kai Li} and  {Li Fei-Fei}},
  booktitle={IEEE CVPR}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248--255},
}

@InProceedings{Joshi2020,
author="Joshi, Aniket  and Mishra, Gaurav and Sivaswamy, Jayanthi",
title="Explainable Disease Classification via Weakly-Supervised Segmentation",
booktitle="Interpretable and Annotation-Efficient Learning for Medical Image Computing",
year="2020",
pages="54--62",
}



@INPROCEEDINGS{Costa2017,
  author={Pedro Costa and Aurelio Campilho and Bryan Hooi and Asim Smailagic and Kris Kitani and Shenghua Liu and Christos Faloutsos and Adrian Galdran},
  booktitle={IEEE ICMLA}, 
  title={EyeQual: Accurate, Explainable, Retinal Image Quality Assessment}, 
  year={2017},
  volume={},
  number={},
  pages={323--330},
}

@misc{CXR20,
  title={Automatic detection of foreign objects on chest X-rays},
  author={JFHealthcare},
  year={2020},
  publisher={Github},
  journal={GitHub repository},
  howpublished={\url{https://github.com/jfhealthcare/object-CXR}},
  note = {Accessed on 11/27/2020}
}

@INPROCEEDINGS{Liu2017,
  author={C. {Liu} and Y. {Cao} and M. {Alcantara} and B. {Liu} and M. {Brunette} and J. {Peinado} and W. {Curioso}},
  booktitle={IEEE ICIP}, 
  title={TX-CNN: Detecting tuberculosis in chest X-ray images using convolutional neural network}, 
  year={2017},
  volume={},
  number={},
  pages={2314-2318},
  }

@ARTICLE{Oh2020a,
  author={Y. {Oh} and S. {Park} and J. C. {Ye}},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Deep Learning COVID-19 Features on CXR Using Limited Training Data Sets}, 
  year={2020},
  volume={39},
  number={8},
  pages={2688-2700},
  }

@misc{Oh2020b,
      title={Born Identity Network: Multi-way Counterfactual Map Generation to Explain a Classifier's Decision}, 
      author={Kwanseok Oh and Jee Seok Yoon and Heung-Il Suk},
      journal   = {CoRR},
      volume    = {abs/2011.10381},
      url       = {https://arxiv.org/abs/2011.10381},
      year={2020},
      eprint={2011.10381},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@article{Schut2020,
  title={Uncertainty-Aware Counterfactual Explanations for Medical Diagnosis},
  author={Schut, Lisa and Key, Oscar Key and McGrath, Rory and  Costabello, Luca and Sacaleanu, Bogdan and Corcoran,Medb and Gal, Yarin},
  journal={ML4H: Machine Learning for Health Workshop},
  year={2020}
}

@article{Schlemper2019,
title = "Attention gated networks: Learning to leverage salient regions in medical images",
journal = "Medical Image Analysis",
volume = "53",
pages = "197 - 207",
year = "2019",
author = "Jo Schlemper and Ozan Oktay and Michiel Schaap and Mattias Heinrich and Bernhard Kainz and Ben Glocker and Daniel Rueckert",
keywords = "Fully convolutional networks, Image classification, Localisation, Segmentation, Soft attention, Attention gates",
}

@article{Singh2020, title={Explainable Deep Learning Models in Medical Image Analysis}, volume={6}, number={6}, journal={Journal of Imaging}, publisher={MDPI AG}, author={Singh, Amitojdeep and Sengupta, Sourya and Lakshminarayanan, Vasudevan}, year={2020}, pages={52}}

@ARTICLE{Wang2020b,
  author={X. {Wang} and X. {Deng} and Q. {Fu} and Q. {Zhou} and J. {Feng} and H. {Ma} and W. {Liu} and C. {Zheng}},
  journal={IEEE Transactions on Medical Imaging}, 
  title={A Weakly-Supervised Framework for COVID-19 Classification and Lesion Localization From Chest CT}, 
  year={2020},
  volume={39},
  number={8},
  pages={2615-2625},
  }

@INPROCEEDINGS{Oquab2015,
  author={M. {Oquab} and L. {Bottou} and I. {Laptev} and J. {Sivic}},
  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Is object localization for free? - Weakly-supervised learning with convolutional neural networks}, 
  year={2015},
  volume={},
  number={},
  pages={685-694},
  }


@InProceedings{Glorot2011,
  title = 	 {Deep Sparse Rectifier Neural Networks},
  author = 	 {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {315--323},
  year = 	 {2011},
  editor = 	 {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
  volume = 	 {15},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Fort Lauderdale, FL, USA},
  month = 	 {11--13 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
  url = 	 {https://proceedings.mlr.press/v15/glorot11a.html},
  abstract = 	 {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training. [pdf]}
}

@inproceedings{He2016,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
  pages     = {770--778},
  publisher = {{IEEE} Computer Society},
  year      = {2016},
  url       = {https://doi.org/10.1109/CVPR.2016.90},
  doi       = {10.1109/CVPR.2016.90},
  timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/HeZRS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{He2015,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on
               ImageNet Classification},
  booktitle = {2015 {IEEE} International Conference on Computer Vision, {ICCV} 2015,
               Santiago, Chile, December 7-13, 2015},
  pages     = {1026--1034},
  publisher = {{IEEE} Computer Society},
  year      = {2015},
  url       = {https://doi.org/10.1109/ICCV.2015.123},
  doi       = {10.1109/ICCV.2015.123},
  timestamp = {Wed, 16 Oct 2019 14:14:51 +0200},
  biburl    = {https://dblp.org/rec/conf/iccv/HeZRS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{Zhu2017,
  author={Zhu, Lingxue and Laptev, Nikolay},
  booktitle={2017 IEEE International Conference on Data Mining Workshops (ICDMW)}, 
  title={Deep and Confident Prediction for Time Series at Uber}, 
  year={2017},
  volume={},
  number={},
  pages={103-110},
  doi={10.1109/ICDMW.2017.19}}
  
@inproceedings{Kingma15,
 author = {Kingma, Durk P and Salimans, Tim and Welling, Max},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Variational Dropout and the Local Reparameterization Trick},
 url = {https://proceedings.neurips.cc/paper/2015/file/bc7316929fe1545bf0b98d114ee3ecb8-Paper.pdf},
 volume = {28},
 year = {2015}
}


@InProceedings{Gal2016,
  title = 	 {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  author = 	 {Gal, Yarin and Ghahramani, Zoubin},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1050--1059},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/gal16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/gal16.html},
  abstract = 	 {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.}
}

@ARTICLE{Oksuz2020,
  author={Oksuz, Ilkay and Clough, James R. and Ruijsink, Bram and Anton, Esther Puyol and Bustin, Aurelien and Cruz, Gastao and Prieto, Claudia and King, Andrew P. and Schnabel, Julia A.},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Deep Learning-Based Detection and Correction of Cardiac MR Motion Artefacts During Reconstruction for High-Quality Segmentation}, 
  year={2020},
  volume={39},
  number={12},
  pages={4001-4010},
  doi={10.1109/TMI.2020.3008930}}
  
  @article{Oksuz2019,
  author    = {Ilkay {\"{O}}ks{\"{u}}z and
               Bram Ruijsink and
               Esther Puyol{-}Ant{\'{o}}n and
               James R. Clough and
               Gast{\~{a}}o Cruz and
               Aur{\'{e}}lien Bustin and
               Claudia Prieto and
               Ren{\'{e}} M. Botnar and
               Daniel Rueckert and
               Julia A. Schnabel and
               Andrew P. King},
  title     = {Automatic CNN-based detection of cardiac {MR} motion artefacts using
               k-space data augmentation and curriculum learning},
  journal   = {Medical Image Anal.},
  volume    = {55},
  pages     = {136--147},
  year      = {2019},
  url       = {https://doi.org/10.1016/j.media.2019.04.009},
  doi       = {10.1016/j.media.2019.04.009},
  timestamp = {Mon, 03 Jan 2022 22:08:13 +0100},
  biburl    = {https://dblp.org/rec/journals/mia/OksuzRPCCBPBRSK19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{Wang2003,
  author={Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  journal={IEEE Transactions on Image Processing}, 
  title={Image quality assessment: from error visibility to structural similarity}, 
  year={2004},
  volume={13},
  number={4},
  pages={600-612},
  doi={10.1109/TIP.2003.819861}}
  
@article{REISENHOFER2018,
title = {A Haar wavelet-based perceptual similarity index for image quality assessment},
journal = {Signal Processing: Image Communication},
volume = {61},
pages = {33-43},
year = {2018},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0923596517302187},
author = {Rafael Reisenhofer and Sebastian Bosse and Gitta Kutyniok and Thomas Wiegand},
keywords = {Image quality, Perceptual similarity, Haar wavelets, Human visual system},
abstract = {In most practical situations, the compression or transmission of images and videos creates distortions that will eventually be perceived by a human observer. Vice versa, image and video restoration techniques, such as inpainting or denoising, aim to enhance the quality of experience of human viewers. Correctly assessing the similarity between an image and an undistorted reference image as subjectively experienced by a human viewer can thus lead to significant improvements in any transmission, compression, or restoration system. This paper introduces the Haar wavelet-based perceptual similarity index (HaarPSI), a novel and computationally inexpensive similarity measure for full reference image quality assessment. The HaarPSI utilizes the coefficients obtained from a Haar wavelet decomposition to assess local similarities between two images, as well as the relative importance of image areas. The consistency of the HaarPSI with the human quality of experience was validated on four large benchmark databases containing thousands of differently distorted images. On these databases, the HaarPSI achieves higher correlations with human opinion scores than state-of-the-art full reference similarity measures like the structural similarity index (SSIM), the feature similarity index (FSIM), and the visual saliency-based index (VSI). Along with the simple computational structure and the short execution time, these experimental results suggest a high applicability of the HaarPSI in real world tasks.}
}

@ARTICLE{Ding2020,
  author={Ding, Keyan and Ma, Kede and Wang, Shiqi and Simoncelli, Eero P.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Image Quality Assessment: Unifying Structure and Texture Similarity}, 
  year={2022},
  volume={44},
  number={5},
  pages={2567-2581},
  doi={10.1109/TPAMI.2020.3045810}}
  
@article {Goldman2007,
	author = {Goldman, Lee W.},
	title = {Principles of CT: Radiation Dose and Image Quality},
	volume = {35},
	number = {4},
	pages = {213--225},
	year = {2007},
	doi = {10.2967/jnmt.106.037846},
	publisher = {Society of Nuclear Medicine},
	abstract = {This article discusses CT radiation dose, the measurement of CT dose, and CT image quality. The most commonly used dose descriptor is CT dose index, which represents the dose to a location (e.g., depth) in a scanned volume from a complete series of slices. A weighted average of the CT dose index measured at the center and periphery of dose phantoms provides a convenient single-number estimate of patient dose for a procedure, and this value (or a related indicator that includes the scanned length) is often displayed on the operator{\textquoteright}s console. CT image quality, as in most imaging, is described in terms of contrast, spatial resolution, image noise, and artifacts. A strength of CT is its ability to visualize structures of low contrast in a subject, a task that is limited primarily by noise and is therefore closely associated with radiation dose: The higher the dose contributing to the image, the less apparent is image noise and the easier it is to perceive low-contrast structures. Spatial resolution is ultimately limited by sampling, but both image noise and resolution are strongly affected by the reconstruction filter. As a result, diagnostically acceptable image quality at acceptable doses of radiation requires appropriately designed clinical protocols, including appropriate kilovolt peaks, amperages, slice thicknesses, and reconstruction filters.},
	issn = {0091-4916},
	URL = {https://tech.snmjournals.org/content/35/4/213},
	eprint = {https://tech.snmjournals.org/content/35/4/213.full.pdf},
	journal = {Journal of Nuclear Medicine Technology}
}


@INPROCEEDINGS{Oksuz2018,
  author={Oksuz, Ilkay and Ruijsink, Bram and Puyol-Antón, Esther and Sinclair, Matthew and Rueckert, Daniel and Schnabel, Julia A. and King, Andrew P.},
  booktitle={2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)}, 
  title={Automatic left ventricular outflow tract classification for accurate cardiac MR planning}, 
  year={2018},
  volume={},
  number={},
  pages={462-465},
  doi={10.1109/ISBI.2018.8363616}}
  
  @article{Lin2022,
title = {SSPNet: An interpretable 3D-CNN for classification of schizophrenia using phase maps of resting-state complex-valued fMRI data},
journal = {Medical Image Analysis},
volume = {79},
pages = {102430},
year = {2022},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2022.102430},
url = {https://www.sciencedirect.com/science/article/pii/S1361841522000810},
author = {Qiu-Hua Lin and Yan-Wei Niu and Jing Sui and Wen-Da Zhao and Chuanjun Zhuo and Vince D. Calhoun},
keywords = {Convolutional neural network, Complex-valued fMRI data, Spatial source phase, Interpretability, Saliency map, Grad-CAM},
abstract = {Convolutional neural networks (CNNs) have shown promising results in classifying individuals with mental disorders such as schizophrenia using resting-state fMRI data. However, complex-valued fMRI data is rarely used since additional phase data introduces high-level noise though it is potentially useful information for the context of classification. As such, we propose to use spatial source phase (SSP) maps derived from complex-valued fMRI data as the CNN input. The SSP maps are not only less noisy, but also more sensitive to spatial activation changes caused by mental disorders than magnitude maps. We build a 3D-CNN framework with two convolutional layers (named SSPNet) to fully explore the 3D structure and voxel-level relationships from the SSP maps. Two interpretability modules, consisting of saliency map generation and gradient-weighted class activation mapping (Grad-CAM), are incorporated into the well-trained SSPNet to provide additional information helpful for understanding the output. Experimental results from classifying schizophrenia patients (SZs) and healthy controls (HCs) show that the proposed SSPNet significantly improved accuracy and AUC compared to CNN using magnitude maps extracted from either magnitude-only (by 23.4 and 23.6\% for DMN) or complex-valued fMRI data (by 10.6 and 5.8\% for DMN). SSPNet captured more prominent HC-SZ differences in saliency maps, and Grad-CAM localized all contributing brain regions with opposite strengths for HCs and SZs within SSP maps. These results indicate the potential of SSPNet as a sensitive tool that may be useful for the development of brain-based biomarkers of mental disorders.}
}







@article{Yoneyama2012,
author = {Kihei Yoneyama and Andrea L. Vavere and Rodrigo Cerci and Rukhsar Ahmed and Andrew E. Arai and Hiroyuki Niinuma and Frank J. Rybicki and Carlos E. Rochitte and Melvin E. Clouse and Richard T. George and Joao A.C. Lima and Armin Arbab-Zadeh},
title ={Influence of Image Acquisition Settings on Radiation Dose and Image Quality in Coronary Angiography by 320-Detector Volume Computed Tomography: The CORE320 Pilot Experience},
journal = {Heart International},
volume = {7},
number = {2},
pages = {hi.2012.e11},
year = {2012},
doi = {10.4081/hi.2012.e11},
    note ={PMID: 23185678},

URL = { 
        https://doi.org/10.4081/hi.2012.e11
    
},
eprint = { 
        https://doi.org/10.4081/hi.2012.e11
    
}
,
    abstract = { The objective of this study was to investigate the impact of image acquisition settings and patients’ characteristics on image quality and radiation dose for coronary angiography by 320-row computed tomography (CT). CORE320 is a prospective study to investigate the diagnostic performance of 320-detector CT for detecting coronary artery disease and associated myocardial ischemia. A run-in phase in 65 subjects was conducted to test the adequacy of the computed tomography angiography (CTA) acquisition protocol. Tube current, exposure window, and number of cardiac beats per acquisition were adjusted according to subjects’ gender, heart rate, and body mass index (BMI). Main outcome measures were image quality, assessed by contrast/noise measurements and qualitatively on a 4-point scale, and radiation dose, estimated by the dose-length-product. Average heart rate at image acquisition was 55.0±7.3 bpm. Median Agatston calcium score was 27.0 (interquartile range 1-330). All scans were prospectively triggered. Single heart beat image acquisition was obtained in 61 of 65 studies (94\%). Sixty-one studies (94\%) and 437 of 455 arterial segments (96\%) were of diagnostic image quality. Estimated radiation dose was significantly greater in obese (5.3±0.4 mSv) than normal weight (4.6±0.3 mSv) or overweight (4.7±0.3 mSv) subjects (P<0.001). BMI was the strongest factor influencing image quality (odds ratio=1.457, P=0.005). The CORE320 CTA image acquisition protocol achieved a good balance between image quality and radiation dose for a 320-detector CT system. However, image quality in obese subjects was reduced compared to normal weight subjects, possibly due to tube voltage/current restrictions mandated by the study protocol. }
}



@article{VANDERVELDEN2022,
title = {Explainable artificial intelligence (XAI) in deep learning-based medical image analysis},
journal = {Medical Image Analysis},
pages = {102470},
year = {2022},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2022.102470},
url = {https://www.sciencedirect.com/science/article/pii/S1361841522001177},
author = {Bas H.M. {van der Velden} and Hugo J. Kuijf and Kenneth G.A. Gilhuijs and Max A. Viergever},
abstract = {With an increase in deep learning-based methods, the call for explainability of such methods grows, especially in high-stakes decision making areas such as medical image analysis. This survey presents an overview of eXplainable Artificial Intelligence (XAI) used in deep learning-based medical image analysis. A framework of XAI criteria is introduced to classify deep learning-based medical image analysis methods. Papers on XAI techniques in medical image analysis are then surveyed and categorized according to the framework and according to anatomical location. The paper concludes with an outlook of future opportunities for XAI in medical image analysis.}
}

@article{Park2022,
title = {Multi-task vision transformer using low-level chest X-ray feature corpus for COVID-19 diagnosis and severity quantification},
journal = {Medical Image Analysis},
volume = {75},
pages = {102299},
year = {2022},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2021.102299},
url = {https://www.sciencedirect.com/science/article/pii/S1361841521003443},
author = {Sangjoon Park and Gwanghyun Kim and Yujin Oh and Joon Beom Seo and Sang Min Lee and Jin Hwan Kim and Sungjun Moon and Jae-Kwang Lim and Jong Chul Ye},
keywords = {Coronavirus disease-19, Chest X-ray, Vision transformer, Multi-task learning},
abstract = {Developing a robust algorithm to diagnose and quantify the severity of the novel coronavirus disease 2019 (COVID-19) using Chest X-ray (CXR) requires a large number of well-curated COVID-19 datasets, which is difficult to collect under the global COVID-19 pandemic. On the other hand, CXR data with other findings are abundant. This situation is ideally suited for the Vision Transformer (ViT) architecture, where a lot of unlabeled data can be used through structural modeling by the self-attention mechanism. However, the use of existing ViT may not be optimal, as the feature embedding by direct patch flattening or ResNet backbone in the standard ViT is not intended for CXR. To address this problem, here we propose a novel Multi-task ViT that leverages low-level CXR feature corpus obtained from a backbone network that extracts common CXR findings. Specifically, the backbone network is first trained with large public datasets to detect common abnormal findings such as consolidation, opacity, edema, etc. Then, the embedded features from the backbone network are used as corpora for a versatile Transformer model for both the diagnosis and the severity quantification of COVID-19. We evaluate our model on various external test datasets from totally different institutions to evaluate the generalization capability. The experimental results confirm that our model can achieve state-of-the-art performance in both diagnosis and severity quantification tasks with outstanding generalization capability, which are sine qua non of widespread deployment.}
}







@article{Arun2021,
author = {Arun, Nishanth and Gaw, Nathan and Singh, Praveer and Chang, Ken and Aggarwal, Mehak and Chen, Bryan and Hoebel, Katharina and Gupta, Sharut and Patel, Jay and Gidwani, Mishka and Adebayo, Julius and Li, Matthew                         D. and Kalpathy-Cramer, Jayashree},
title = {Assessing the Trustworthiness of Saliency Maps for Localizing                     Abnormalities in Medical Imaging},
journal = {Radiology: Artificial Intelligence},
volume = {3},
number = {6},
pages = {e200267},
year = {2021},
doi = {10.1148/ryai.2021200267},

URL = { 
    
        https://doi.org/10.1148/ryai.2021200267
    
    

},
eprint = { 
    
        https://doi.org/10.1148/ryai.2021200267
    
    

}
,
}

@article{Chow2016,
title = {Review of medical image quality assessment},
journal = {Biomedical Signal Processing and Control},
volume = {27},
pages = {145-154},
year = {2016},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2016.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S1746809416300180},
author = {Li Sze Chow and Raveendran Paramesran},
keywords = {Image Quality Assessment (IQA), No reference-IQA (NR-IQA), Medical images, MRI, CT},
abstract = {Image Quality Assessment (IQA) plays an important role in assessing any new hardware, software, image acquisition techniques, image reconstruction or post-processing algorithms, etc. In the past decade, there have been various IQA methods designed to evaluate natural images. Some were used for the medical images but the use was limited. This paper reviews the recent advancement on IQA for medical images, mainly for Magnetic Resonance Imaging (MRI), Computed Tomography (CT), and ultrasonic imaging. Thus far, there is no gold standard of IQA for medical images due to various difficulties in designing a suitable IQA for medical images, and there are many different image characteristics and contents across various imaging modalities. No reference-IQA (NR-IQA) is recommended for assessing medical images because there is no perfect reference image in the real world medical imaging. We will discuss and comment on some useful and interesting IQA methods, and then suggest several important factors to be considered in designing a new IQA method for medical images. There is still great potential for research in this area.}
}

@inproceedings{Wang2020c,
  author    = {Shuo Wang and
               Giacomo Tarroni and
               Chen Qin and
               Yuanhan Mo and
               Chengliang Dai and
               Chen Chen and
               Ben Glocker and
               Yike Guo and
               Daniel Rueckert and
               Wenjia Bai},
  editor    = {Anne L. Martel and
               Purang Abolmaesumi and
               Danail Stoyanov and
               Diana Mateus and
               Maria A. Zuluaga and
               S. Kevin Zhou and
               Daniel Racoceanu and
               Leo Joskowicz},
  title     = {Deep Generative Model-Based Quality Control for Cardiac {MRI} Segmentation},
  booktitle = {Medical Image Computing and Computer Assisted Intervention - {MICCAI}
               2020 - 23rd International Conference, Lima, Peru, October 4-8, 2020,
               Proceedings, Part {IV}},
  series    = {Lecture Notes in Computer Science},
  volume    = {12264},
  pages     = {88--97},
  publisher = {Springer},
  year      = {2020},
  url       = {https://doi.org/10.1007/978-3-030-59719-1\_9},
  doi       = {10.1007/978-3-030-59719-1\_9},
  timestamp = {Mon, 05 Oct 2020 18:46:16 +0200},
  biburl    = {https://dblp.org/rec/conf/miccai/WangTQMDCGGRB20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
Brusini2020,
title={A deep learning-based pipeline for error detection and quality control of brain {MRI} segmentation results},
author={Irene Brusini and Daniel Ferreira Padilla and Jos{\'e} Barroso and Ingmar Skoog and {\"O}rjan Smedby and Eric Westman and Chunliang Wang},
booktitle={Medical Imaging with Deep Learning},
year={2020},
url={https://openreview.net/forum?id=sqbpwcNetg}
}

@ARTICLE{Hu2022,  author={Hu, Junhao and Zhang, Chenyang and Zhou, Kang and Gao, Shenghua},  journal={IEEE Transactions on Medical Imaging},   title={Chest X-Ray Diagnostic Quality Assessment: How Much Is Pixel-Wise Supervision Needed?},   year={2022},  volume={41},  number={7},  pages={1711-1723},  doi={10.1109/TMI.2022.3149171}}

@INPROCEEDINGS{Meding2017,  author={Meding, Kristof and Loktyushin, Alexander and Hirsch, Michael},  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={Automatic detection of motion artifacts in MR images using CNNS},   year={2017},  volume={},  number={},  pages={811-815},  doi={10.1109/ICASSP.2017.7952268}}

@inproceedings{
Ozer2021,
title={Explainable Image Quality Analysis of Chest X-Rays},
author={Caner Ozer and Ilkay Oksuz},
booktitle={Medical Imaging with Deep Learning},
year={2021},
url={https://openreview.net/forum?id=ln797A8lAb0}
}

@ARTICLE{Khanin2018,  author={Khanin, Alexander and Anton, Mathias and Reginatto, Marcel and Elster, Clemens},  journal={IEEE Transactions on Medical Imaging},   title={Assessment of CT Image Quality Using a Bayesian Framework},   year={2018},  volume={37},  number={12},  pages={2687-2694},  doi={10.1109/TMI.2018.2848104}}

@InProceedings{Gu2020,
author="Gu, Shuyang
and Bao, Jianmin
and Chen, Dong
and Wen, Fang",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="GIQA: Generated Image Quality Assessment",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="369--385",
abstract="Generative adversarial networks (GANs) achieve impressive results today, but not all generated images are perfect. A number of quantitative criteria have recently emerged for generative models, but none of them are designed for a single generated image. In this paper, we propose a new research topic, Generated Image Quality Assessment (GIQA), which quantitatively evaluates the quality of each generated image. We introduce three GIQA algorithms from two perspectives: learning-based and data-based. We evaluate a number of images generated by various recent GAN models on different datasets and demonstrate that they are consistent with human assessments. Furthermore, GIQA is available for many applications, like separately evaluating the realism and diversity of generative models, and enabling online hard negative mining (OHEM) in the training of GANs to improve the results.",
isbn="978-3-030-58621-8"
}


@article{Jin2022,
title = {Guidelines and evaluation of clinical explainable AI in medical image analysis},
journal = {Medical Image Analysis},
pages = {102684},
year = {2022},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2022.102684},
url = {https://www.sciencedirect.com/science/article/pii/S1361841522003127},
author = {Weina Jin and Xiaoxiao Li and Mostafa Fatehi and Ghassan Hamarneh},
keywords = {Interpretable machine learning, Medical image analysis, Multi-modal medical image, Explainable AI evaluation},
abstract = {Explainable artificial intelligence (XAI) is essential for enabling clinical users to get informed decision support from AI and comply with evidence-based medical practice. Applying XAI in clinical settings requires proper evaluation criteria to ensure the explanation technique is both technically sound and clinically useful, but specific support is lacking to achieve this goal. To bridge the research gap, we propose the Clinical XAI Guidelines that consist of five criteria a clinical XAI needs to be optimized for. The guidelines recommend choosing an explanation form based on Guideline 1 (G1) Understandability and G2 Clinical relevance. For the chosen explanation form, its specific XAI technique should be optimized for G3 Truthfulness, G4 Informative plausibility, and G5 Computational efficiency. Following the guidelines, we conducted a systematic evaluation on a novel problem of multi-modal medical image explanation with two clinical tasks, and proposed new evaluation metrics accordingly. Sixteen commonly-used heatmap XAI techniques were evaluated and found to be insufficient for clinical use due to their failure in G3 and G4. Our evaluation demonstrated the use of Clinical XAI Guidelines to support the design and evaluation of clinically viable XAI.}
}

@ARTICLE{Bai2021,  author={Bai, Ti and Wang, Biling and Nguyen, Dan and Wang, Bao and Dong, Bin and Cong, Wenxiang and Kalra, Mannudeep K. and Jiang, Steve},  journal={IEEE Transactions on Medical Imaging},   title={Deep Interactive Denoiser (DID) for X-Ray Computed Tomography},   year={2021},  volume={40},  number={11},  pages={2965-2975},  doi={10.1109/TMI.2021.3101241}}

@inproceedings{Sundararajan2017,
  author    = {Mukund Sundararajan and
               Ankur Taly and
               Qiqi Yan},
  editor    = {Doina Precup and
               Yee Whye Teh},
  title     = {Axiomatic Attribution for Deep Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning,
               {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  pages     = {3319--3328},
  publisher = {{PMLR}},
  year      = {2017},
  url       = {http://proceedings.mlr.press/v70/sundararajan17a.html},
  timestamp = {Wed, 29 May 2019 08:41:45 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/SundararajanTY17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Smilkov2017,
  author    = {Daniel Smilkov and
               Nikhil Thorat and
               Been Kim and
               Fernanda B. Vi{\'{e}}gas and
               Martin Wattenberg},
  title     = {SmoothGrad: removing noise by adding noise},
  journal   = {CoRR},
  volume    = {abs/1706.03825},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03825},
  eprinttype = {arXiv},
  eprint    = {1706.03825},
  timestamp = {Mon, 13 Aug 2018 16:48:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SmilkovTKVW17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Petsiuk2018,
  author    = {Vitali Petsiuk and
               Abir Das and
               Kate Saenko},
  title     = {{RISE:} Randomized Input Sampling for Explanation of Black-box Models},
  booktitle = {British Machine Vision Conference 2018, {BMVC} 2018, Newcastle, UK,
               September 3-6, 2018},
  pages     = {151},
  publisher = {{BMVA} Press},
  year      = {2018},
  url       = {http://bmvc2018.org/contents/papers/1064.pdf},
  timestamp = {Tue, 21 Apr 2020 23:17:57 +0200},
  biburl    = {https://dblp.org/rec/conf/bmvc/PetsiukDS18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Simonyan2014,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@article{Huang2022,
  author    = {Yijin Huang and
               Junyan Lyu and
               Pujin Cheng and
               Roger Tam and
               Xiaoying Tang},
  title     = {SSiT: Saliency-guided Self-supervised Image Transformer for Diabetic
               Retinopathy Grading},
  journal   = {CoRR},
  volume    = {abs/2210.10969},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2210.10969},
  doi       = {10.48550/arXiv.2210.10969},
  eprinttype = {arXiv},
  eprint    = {2210.10969},
  timestamp = {Tue, 25 Oct 2022 14:25:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2210-10969.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Zeiler2014,
author="Zeiler, Matthew D.
and Fergus, Rob",
editor="Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne",
title="Visualizing and Understanding Convolutional Networks",
booktitle="Computer Vision -- ECCV 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="818--833",
abstract="Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",
isbn="978-3-319-10590-1"
}

@article{Kim2022,
title = {Arrhythmia detection model using modified DenseNet for comprehensible Grad-CAM visualization},
journal = {Biomedical Signal Processing and Control},
volume = {73},
pages = {103408},
year = {2022},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2021.103408},
url = {https://www.sciencedirect.com/science/article/pii/S1746809421010053},
author = {Jin-Kook Kim and Sunghoon Jung and Jinwon Park and Sung Won Han},
keywords = {Arrhythmia classification, Electrocardiogram, Class activation mapping, Convolutional neural network},
abstract = {Diagnosing arrhythmia is difficult, requires significant efforts. Because arrhythmia can be associated with serious diseases, it is important to classify arrhythmia patients with high accuracy, and the basis for the classification model's judgment should be properly demonstrated. Traditional algorithm methods are less accurate, and simply using a high-accuracy image classification deep learning model yields incomprehensible results when the model is visualized with gradient-weighted class activation mapping (Grad-CAM). We want to achieve high-performance deep learning models can also comprehensible visualization. To obtain this, two hypotheses about Grad-CAM were established and the experiment was conducted. As a result, a method that could clearly visualize the response area using Grad-CAM with a higher classification performance of 0.98 accuracy is created.}
}

@article{Elfwing2018,
  author    = {Stefan Elfwing and
               Eiji Uchibe and
               Kenji Doya},
  title     = {Sigmoid-weighted linear units for neural network function approximation
               in reinforcement learning},
  journal   = {Neural Networks},
  volume    = {107},
  pages     = {3--11},
  year      = {2018},
  url       = {https://doi.org/10.1016/j.neunet.2017.12.012},
  doi       = {10.1016/j.neunet.2017.12.012},
  timestamp = {Mon, 03 Jan 2022 22:02:36 +0100},
  biburl    = {https://dblp.org/rec/journals/nn/ElfwingUD18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Tan2019,
  author    = {Mingxing Tan and
               Quoc V. Le},
  editor    = {Kamalika Chaudhuri and
               Ruslan Salakhutdinov},
  title     = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {6105--6114},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v97/tan19a.html},
  timestamp = {Tue, 11 Jun 2019 15:37:38 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/TanL19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{Samarasinghe2021,
  title    = "Deep learning for segmentation in radiation therapy planning: a
              review",
  author   = "Samarasinghe, Gihan and Jameson, Michael and Vinod, Shalini and
              Field, Matthew and Dowling, Jason and Sowmya, Arcot and Holloway,
              Lois",
  abstract = "Segmentation of organs and structures, as either targets or
              organs-at-risk, has a significant influence on the success of
              radiation therapy. Manual segmentation is a tedious and
              time-consuming task for clinicians, and inter-observer
              variability can affect the outcomes of radiation therapy. The
              recent hype over deep neural networks has added many powerful
              auto-segmentation methods as variations of convolutional neural
              networks (CNN). This paper presents a descriptive review of the
              literature on deep learning techniques for segmentation in
              radiation therapy planning. The most common CNN architecture
              across the four clinical sub sites considered was U-net, with the
              majority of deep learning segmentation articles focussed on head
              and neck normal tissue structures. The most common data sets were
              CT images from an inhouse source, along with some public data
              sets. N-fold cross-validation was commonly employed; however, not
              all work separated training, test and validation data sets. This
              area of research is expanding rapidly. To facilitate comparisons
              of proposed methods and benchmarking, consistent use of
              appropriate metrics and independent validation should be
              carefully considered.",
  journal  = "J Med Imaging Radiat Oncol",
  volume   =  65,
  number   =  5,
  pages    = "578--595",
  month    =  jul,
  year     =  2021,
  address  = "Australia",
  keywords = "contouring; deep learning; radiation therapy; segmentation",
  language = "en"
}

@inproceedings{Adebayo2018,
author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
title = {Sanity Checks for Saliency Maps},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {9525–9536},
numpages = {12},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@INPROCEEDINGS{Sujit2018,
  author={Sujit, Sheeba J. and Gabr, Refaat E. and Coronado, Ivan and Robinson, Melvin and Datta, Sushmita and Narayana, Ponnada A.},
  booktitle={2018 9th Cairo International Biomedical Engineering Conference (CIBEC)}, 
  title={Automated Image Quality Evaluation of Structural Brain Magnetic Resonance Images using Deep Convolutional Neural Networks}, 
  year={2018},
  volume={},
  number={},
  pages={33-36},
  doi={10.1109/CIBEC.2018.8641830}}

@article{Mortamet2009,
author = {Mortamet, Bénédicte and Bernstein, Matt A. and Jack Jr., Clifford R. and Gunter, Jeffrey L. and Ward, Chadwick and Britson, Paula J. and Meuli, Reto and Thiran, Jean-Philippe and Krueger, Gunnar},
title = {Automatic quality assessment in structural brain magnetic resonance imaging},
journal = {Magnetic Resonance in Medicine},
volume = {62},
number = {2},
pages = {365-372},
keywords = {magnetic resonance imaging, automatic quality assessment, image quality, artifact detection},
doi = {https://doi.org/10.1002/mrm.21992},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.21992},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/mrm.21992},
abstract = {Abstract MRI has evolved into an important diagnostic technique in medical imaging. However, reliability of the derived diagnosis can be degraded by artifacts, which challenge both radiologists and automatic computer-aided diagnosis. This work proposes a fully-automatic method for measuring image quality of three-dimensional (3D) structural MRI. Quality measures are derived by analyzing the air background of magnitude images and are capable of detecting image degradation from several sources, including bulk motion, residual magnetization from incomplete spoiling, blurring, and ghosting. The method has been validated on 749 3D T1-weighted 1.5T and 3T head scans acquired at 36 Alzheimer's Disease Neuroimaging Initiative (ADNI) study sites operating with various software and hardware combinations. Results are compared against qualitative grades assigned by the ADNI quality control center (taken as the reference standard). The derived quality indices are independent of the MRI system used and agree with the reference standard quality ratings with high sensitivity and specificity (>85\%). The proposed procedures for quality assessment could be of great value for both research and routine clinical imaging. It could greatly improve workflow through its ability to rule out the need for a repeat scan while the patient is still in the magnet bore. Magn Reson Med, 2009. © 2009 Wiley-Liss, Inc.},
year = {2009}
}

@inproceedings{Abdi2017,
author = {Abdi, Amir H. and Luong, Christina and Tsang, Teresa and Jue, John and Gin, Ken and Yeung, Darwin and Hawley, Dale and Rohling, Robert and Abolmaesumi, Purang},
title = {Quality Assessment of Echocardiographic Cine Using Recurrent Neural Networks: Feasibility on Five Standard View Planes},
year = {2017},
isbn = {978-3-319-66178-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-66179-7_35},
doi = {10.1007/978-3-319-66179-7_35},
abstract = {Echocardiography (echo) is a clinical imaging technique which is highly dependent on operator experience. We aim to reduce operator variability in data acquisition by automatically computing an echo quality score for real-time feedback. We achieve this with a deep neural network model, with convolutional layers to extract hierarchical features from the input echo cine and recurrent layers to leverage the sequential information in the echo cine loop. Using data from 509 separate patient studies, containing 2,450 echo cines across five standard echo imaging planes, we achieved a mean quality score accuracy of 85 compared to the gold-standard score assigned by experienced echosonographers. The proposed approach calculates the quality of a given 20 frame echo sequence within 10&nbsp;ms, sufficient for real-time deployment.},
booktitle = {Medical Image Computing and Computer Assisted Intervention − MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part III},
pages = {302–310},
numpages = {9},
keywords = {Recurrent Neural Network, Deep learning, Convolutional, LSTM, Quality assessment, Echocardiography, Echo cine loop},
location = {Quebec City, QC, Canada}
}

@INPROCEEDINGS{Chen2021,
  author={Chen, Qi and Min, Xiongkuo and Duan, Huiyu and Zhu, Yucheng and Zhai, Guangtao},
  booktitle={2021 IEEE International Conference on Image Processing (ICIP)}, 
  title={Muiqa: Image Quality Assessment Database And Algorithm For Medical Ultrasound Images}, 
  year={2021},
  volume={},
  number={},
  pages={2958-2962},
  doi={10.1109/ICIP42928.2021.9506431}}

@inproceedings{Hu2018,
  author    = {Jie Hu and
               Li Shen and
               Gang Sun},
  title     = {Squeeze-and-Excitation Networks},
  booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
  pages     = {7132--7141},
  publisher = {Computer Vision Foundation / {IEEE} Computer Society},
  year      = {2018},
  url       = {http://openaccess.thecvf.com/content\_cvpr\_2018/html/Hu\_Squeeze-and-Excitation\_Networks\_CVPR\_2018\_paper.html},
  doi       = {10.1109/CVPR.2018.00745},
  timestamp = {Tue, 31 Aug 2021 14:00:32 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/HuSS18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Article{Gaube2023,
author={Gaube, Susanne
and Suresh, Harini
and Raue, Martina
and Lermer, Eva
and Koch, Timo K.
and Hudecek, Matthias F. C.
and Ackery, Alun D.
and Grover, Samir C.
and Coughlin, Joseph F.
and Frey, Dieter
and Kitamura, Felipe C.
and Ghassemi, Marzyeh
and Colak, Errol},
title={Non-task expert physicians benefit from correct explainable AI advice when reviewing X-rays},
journal={Scientific Reports},
year={2023},
month={Jan},
day={25},
volume={13},
number={1},
pages={1383},
abstract={Artificial intelligence (AI)-generated clinical advice is becoming more prevalent in healthcare. However, the impact of AI-generated advice on physicians' decision-making is underexplored. In this study, physicians received X-rays with correct diagnostic advice and were asked to make a diagnosis, rate the advice's quality, and judge their own confidence. We manipulated whether the advice came with or without a visual annotation on the X-rays, and whether it was labeled as coming from an AI or a human radiologist. Overall, receiving annotated advice from an AI resulted in the highest diagnostic accuracy. Physicians rated the quality of AI advice higher than human advice. We did not find a strong effect of either manipulation on participants' confidence. The magnitude of the effects varied between task experts and non-task experts, with the latter benefiting considerably from correct explainable AI advice. These findings raise important considerations for the deployment of diagnostic advice in healthcare.},
issn={2045-2322},
doi={10.1038/s41598-023-28633-w},
url={https://doi.org/10.1038/s41598-023-28633-w}
}

