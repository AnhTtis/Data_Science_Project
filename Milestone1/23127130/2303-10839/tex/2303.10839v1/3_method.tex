\section{Method}
\label{method}

\begin{figure}
% \vspace{-0.5em}
  \centering
%   \setlength{\abovecaptionskip}{0.06cm}
  \includegraphics[width=0.95\linewidth]{architecture.pdf}
   \caption{ Overview of MXM-CLR framework. 
   }
   \vspace{-8pt}
   \label{fig:framework}
\end{figure}

The overview of MXM-CLR framework is shown in Figure \ref{fig:framework}.
For each instance in the mini-batch, the cross-modal pairs are constructed from its multifold text and image observations following the group-wise data pairing process, while the feature of each observation is extracted using the corresponding encoder.
Then, the MFH loss is calculated to model hard and soft relationships for the cross-modal observations.
In the following, we first revisit the representative XM-CLR methods such as CLIP \cite{AlecRadford2021LearningTV} and TriCoLo \cite{YueRuan2022TriCoLoTC}, as well as the InfoNCE \cite{AaronvandenOord2018RepresentationLW,YuhaoZhang2021ContrastiveLO} loss adopted by these methods.
Next, we introduce the MXM-CLR framework and present details for group-wise data pairing and the MFH loss.

\begin{figure*}
  \centering
%   \setlength{\abovecaptionskip}{0.03cm}
  \includegraphics[width=0.82\linewidth]{overview_v2.pdf}
  \vspace{-2pt}
  \caption{
  Given a mini-batch containing three instances (a), group-wise data pairing can be performed to construct positive and negative pairs for constrastive learning.
  Group-wise pairing results for different variations of MXM-CLR are shown in (b), (c), (d).
  The dashed boxes drawn around the positive groups indicate where the positive pair selection is performed. For (e), MXM-CLR degenerates to single-fold contrastive learning such as CLIP.
   }
   \vspace{-10pt}
   \label{fig:overview_loss}
\end{figure*}

\subsection{Revisiting XM-CLR}
\label{sec:xm-clr}
XM-CLR methods aim to learn a joint embedding space for cross-modal representation learning. CLIP \cite{AlecRadford2021LearningTV} and TriCoLo \cite{YueRuan2022TriCoLoTC} are two representative methods for representation learning of image-text and 3D-text, respectively.
For CLIP \cite{AlecRadford2021LearningTV}, it employs a dual-encoder architecture which consists of an image encoder (e.g., ResNet \cite{KaimingHe2015DeepRL} or ViT \cite{AlexeyDosovitskiy2020AnII}) and a text encoder (e.g., BERT \cite{JacobDevlin2018BERTPO}).
For TriCoLo \cite{YueRuan2022TriCoLoTC}, as the original version TriCoLo (I+V) learns from three modalities, i.e., text, image and voxel, we focus on its bimodal version TriCoLo (I) which also learns from text and image as ours.
Different from CLIP, TriCoLo aggregates the features from multi-view images by MVCNN \cite{HangSu2015MultiviewCN} and learns the alignments between text and the aggregated feature.
Based on the features obtained for each modality, CLIP and TriCoLo utilize InfoNCE loss \cite{AaronvandenOord2018RepresentationLW} to optimize the encoders for representation learning.

Given a mini-batch constructed from $N$ image-text pairs $\{(i^1,t^1),(i^2,t^2),\cdots,(i^N,t^N)$\}, we denote the features extracted by the encoders as $\{(\hat{i}^1,\hat{t}^1),(\hat{i}^2,\hat{t}^2),\cdots,(\hat{i}^N,\hat{t}^N)$\}.
For InfoNCE, it first defines the image-to-text loss as:
\begin{scriptsize}
\begin{equation}
\label{eq_1}
 % \setlength{\abovedisplayskip}{3pt}
 % \setlength{\belowdisplayskip}{3pt}
L_{I\rightarrow T}= \frac{-1}{N} \!\sum_{r=1}^{N}\log_{}{\frac{exp(s(\hat{i}^r,\hat{t}^r) )}{exp(s(\hat{i}^r,\hat{t}^r)) \!+\! \sum_{h{\in\! \mathcal{N}^r}}^{}\!\!exp(s(\hat{i}^r,\hat{t}^h))}},
\end{equation}
\end{scriptsize}%
where $s(\cdot,\cdot)$ is the temperature-controlled cosine similarity between features of cross-modal pairs;
$r$ and $h$ are indices for the samples;
$\mathcal{N}^r$ is the set of indices for negative samples of sample $r$.
The text-to-image loss $L_{T\rightarrow I}$ is defined symmetrically and the final InfoNCE loss is defined as:
\begin{equation}
% \setlength{\abovedisplayskip}{3pt}
% \setlength{\belowdisplayskip}{2pt}
L_{InfoNCE}=(L_{I\rightarrow T} + L_{T\rightarrow I})/2.
\end{equation}




\subsection{MXM-CLR Framework}
\label{sec:mxm}
In this section, we will introduce the architecture of MXM-CLR.
As MXM-CLR is a unified framework that can be applied to different modalities in various forms, we focus on the task of text-image representation learning while other forms of MXM-CLR such as text-shape learning can be derived similarly.

\textbf{Multifold cross-modal batch construction.}
% In this paper, we focus on the cross-modal data with multifold observations.
For an image-text dataset $\mathcal{D}$ that contains $C$ instances, i.e., $\mathcal{D} = \{inst_1,inst_2,\cdots,inst_{C}\}$, we consider each instance has $m$ observations for image and $n$ observations for text: $inst_k = \{(i_k^1,i_k^2,\cdots,i_k^{m}),(t_k^1,t_k^2,\cdots,t_k^{n})\}$; see Figure \ref{fig:overview_loss}(a) for an example.
Then, we define the mini-batch for multifold data as:
\begin{equation}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
mini\!\!-\!\!batch = \{inst_k | k\in\{1,2,\cdots,b\}\},
\end{equation}
where $b$ is the number of instances in a mini-batch.
Therefore, each mini-batch contains $b*m$ images and $b*n$ texts.

\textbf{Group-wise data pairing.}
Unlike the XM-CLR for which the positive and negative pairs can be directly obtained from the mini-batch, we need to generate positive and negative pairs based on the multifold observations for each data instance.
As different combinations of observations from each modality could form a positive pair, we employ Cartesian product to obtain all combinations of the cross-modal observations w.r.t. the same instance as positive pairs; then, the observations from different instances become negative pairs.
Figure \ref{fig:overview_loss}(b) shows an illustration of the resulting positive and negative pairs, and the positive pairs (in orange) naturally form groups in the diagonal.
Formally, the group consisting $m*n$ positive pairs of instance $k$ can be represented as:\par
\vspace{-10pt}
{\small
\begin{equation}
\setlength{\abovedisplayskip}{4pt}
\setlength{\belowdisplayskip}{4pt}
% g_k \!\!=\!\! \{\langle i_k^r,t_k^c \rangle| r\!\!\in\!\!\{1,2,\cdots,m\},c\!\!\in\!\!\{1,2,\cdots,n\}
g_k = \{ (i_k^r,t_k^c)| r \in \{1,2,\cdots,m\},c \in \{1,2,\cdots,n\}\}.
\end{equation}
}%

\textbf{MFH loss.}
The core of MXM-CLR is the multifold-aware hybrid loss MFH, which explicitly models the hard and soft relationships for data with multiple observations.
Figure \ref{fig:infonce_mask_mf} shows how the MFH loss as well as some InfoNCE-based losses are defined based on the positive and negative data pairs.
It can be seen that if there are multiple pairs corresponding to the same underlying instance (e.g., the image-text pairs that correspond to the same shape in TriCoLo), some positive pairs will appear at the non-diagonal position.
For the conventional InfoNCE loss \cite{AaronvandenOord2018RepresentationLW}, 
these pairs will be falsely treated as negative pairs since InfoNCE only selects the diagonal pairs as positive.
A simple improvement of InfoNCE for multifold data is to mask out the false negative pairs, denoted as InfoNCE+Mask.
However, InfoNCE+Mask is still single-fold-oriented and cannot simultaneously utilize information from multiple positive pairs.
Another way to improve InfoNCE is to learn soft-alignment targets similar to \cite{JunnanLi2021AlignBF,andonian2022robust} to alleviate the noisy relationships, denoted as InfoNCE+Soft.
In contrast, MFH considers both the hard and soft relationships based on the data pairs generated from the group-wise data pairing.


Specifically, the MFH loss consists of two terms: the hard relationship loss $L_{MFH}^{H}$ and the soft relationship loss $L_{MFH}^{S}$.
As shown in Figure \ref{fig:mask}, given the group-wise data pairs as input, to compute the hard relationship loss $L_{MFH}^{H}$, the first task is to decide which positive pairs should be selected for contrastive learning. 
Generally, we try to select multiple positive pairs from each group since we aim to utilize more information provided by the multifold data during one learning iteration. 
A simple selection scheme is to treat all pairs in a diagonal group as positive. 
However, such a scheme may involve some noisy (e.g., due to the mismatch between the image and text) or inconsistent (e.g., two positive pairs with similar captions but very different images) pairs, which are introduced by the Cartesian product operation.  
To alleviate the possible inconsistency of the positive pairs, we propose a \textit{masked random} pair selection scheme which randomly selects a positive pair from each row of the group matrix and masks the unselected positive pairs to avoid treating them as negative pairs.
This random selection operation can be repeated for multiple times to sample more combinations of positive pairs.
With the selected positive pairs, the hard relationship loss $L_{MFH}^{H}$ is defined as:
\begin{equation}
% \setlength{\abovedisplayskip}{3pt}
% \setlength{\belowdisplayskip}{1pt}
L_{MFH}^{H} = (L_{I\rightarrow T}^{H} + L_{T\rightarrow I}^{H}) / 2, \quad where
\end{equation}
\begin{scriptsize}
\begin{equation}
% \setlength{\abovedisplayskip}{1pt}
% \setlength{\belowdisplayskip}{1pt}
L_{I\rightarrow T}^{H} \!\!=\!\! \frac{-1}{N}\!\! \sum\limits_{x=1}\limits^{p}\! \sum\limits_{k=1}\limits^{b}\!\sum\limits_{r=1}\limits^{m}\! log \frac{exp(s(\hat{i}_{k}^{r},\hat{t}_{k}^{u}))}
{exp(s(\hat{i}_{k}^{r},\hat{t}_{k}^{u})) + \sum_{h\in \mathcal{N}_k^r}^{}\!\!exp(s(\hat{i}_{k}^{r},\hat{t}_{k}^{h}))}.
\end{equation}
\end{scriptsize}%
Here, $L_{I\rightarrow T}^{H}$ is the hard image-to-text loss defined in the form of group-wise masked InfoNCE; $p$ is the total repetition time for the masked random pair selection and $x$ is the variable to indicate the current repetition round; $N=p*b*m$ are the number of positive pairs selected; $u = random(0,n)$ is a random integer in the range $[0,n)$, representing the randomly selected column index;
$\mathcal{N}_k^r$ is the set of indices for negative samples of sample $r$ w.r.t. group $k$;
$L_{T\rightarrow I}^{H}$ is defined similarly as $L_{I\rightarrow T}^{H}$.

Although the hard relationship loss $L_{MFH}^{H}$ can guide the model to utilize more information of the multifold data, there are still some limitations:
1) all selected positive pairs are treated equally despite the similarity of some positive pairs may be low;
2) no potential positive samples can be discovered from the initial negative pairs.
To address these limitations, we incorporate soft-target learning into our MXM-CLR framework so that the soft relationships between both the positive and negative pairs can be modeled.
Specifically, we take the similar idea of ALBEF \cite{JunnanLi2021AlignBF} and use a momentum model to generate pseudo-targets for the soft relationship learning.
The momentum model is a continuously-evolving teacher that is composed of exponential-moving-average versions of the data encoders.
During training, features extracted by the base encoders of the input modalities and the momentum model are used to compute two pair-wise feature similarity matrices, while the one from the momentum model is used as the supervision to guide the training of base encoders.
Formally, the soft relationship loss $L_{MFH}^{S}$ is defined as:
\begin{equation}
% \setlength{\abovedisplayskip}{3pt}
% \setlength{\belowdisplayskip}{1pt}
L_{MFH}^{S} = (L_{I\rightarrow T}^{S} + L_{T\rightarrow I}^{S}) / 2, \quad where
\end{equation}
\begin{equation}
% \setlength{\abovedisplayskip}{1pt}
% \setlength{\belowdisplayskip}{1pt}
L_{I\rightarrow T}^{S} = CE(M_b,M_m).
\end{equation}%
Here, $M_b$ and $M_m$ are the image-text feature similarity matrix from the base model and momentum model respectively, while each element in the matrix is the temperature-controlled cosine similarity defined in Section \ref{sec:xm-clr};
% $\mathrm{F_b = s(\hat{I}_b,\hat{T}_b)}$ and $\mathrm{F_m = s(\hat{I}_m,\hat{T}_m)}$, $\hat{I}_b,\hat{T}_b$ are the feature embedding outputted from the base models while $\hat{I}_m,\hat{T}_m$ are outputted from momentum models.
$CE$ is the standard cross-entropy loss;
$L_{T\rightarrow I}^{S}$ is defined similarly as $L_{I\rightarrow T}^{S}$.


Finally, the MFH loss is defined as:
\begin{equation}
% \setlength{\abovedisplayskip}{4pt}
% \setlength{\belowdisplayskip}{4pt}
L_{MFH} = \alpha \cdot L_{MFH}^{H} + (1-\alpha) \cdot L_{MFH}^{S},
\end{equation}%
where $\alpha$ is a parameter (empirically set to 0.6) to control the weights of the hard and soft relationship losses. 



\begin{figure}
  \centering
%   \setlength{\abovecaptionskip}{0.03cm}
  \includegraphics[width=0.92\linewidth]{infonce_mask_MF.pdf}
   \caption{  
The illustration of how the positive and negative pairs are utilized to compute the data relationships in InfoNCE-based losses and the MFH loss.
   }
   \label{fig:infonce_mask_mf}
   \vspace{-8pt}
\end{figure}

\begin{figure}
  \centering
%   \setlength{\abovecaptionskip}{0.03cm}
  \includegraphics[width=0.9\linewidth]{mask.pdf}
  \vspace{-10pt}
   \caption{  
   The illustration of how MFH loss is computed.
   }
   \label{fig:mask}
   \vspace{-10pt}
\end{figure}



\textbf{Variations of MXM-CLR.}
As a unified framework, MXM-CLR can be flexibly adapted to different forms by aggregating features of each modality and the resulting model can be used to support different downstream tasks.
For example, by aggregating multifold text features to one feature as in Figure \ref{fig:overview_loss}(c), MXM-CLR can be used to learn for more comprehensive text-to-image retrieval, e.g., use multiple sentences together to retrieval a natural image.
Similarly, aggregating multifold image features as in Figure \ref{fig:overview_loss}(d) corresponds to the text-to-shape retrieval, while the shape is represented by multi-view images.
At last, when features of both modalities are aggregated (Figure \ref{fig:overview_loss}(e)), MXM-CLR degenerates to the single-fold XM-CLR.

