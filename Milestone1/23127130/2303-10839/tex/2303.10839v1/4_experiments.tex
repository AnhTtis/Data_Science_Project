\section{Experiments}
\label{sec:results}

To evaluate the performance MXM-CLR, we conduct experiments on two representative datasets which are often used for cross-modal representation learning, Text2Shape \cite{chen2018text2shape} and Flickr30K \cite{PeterYoung2014FromID}.
The modalities we are interested in this paper are text, image and 3D data (3D shapes, particularly).
Meanwhile, as 3D shapes can also be represented based on the multi-view images, we only focus on learning the text and image encoders in our experiments.
% \sout{If not specifically mentioned, in the following experiment, we use the masked random (10$n$) positive pair selection scheme for MXM-CLR.}

\subsection{Datasets and Metrics} 
\textbf{Synthetic 3D-text dataset.} Text2Shape is a pioneering 3D-text dataset which contains 3D shapes annotated with natural language descriptions.
It consists of 6,521 chairs and 8,378 tables selected from ShapeNet \cite{chang2015shapenet}.
About five freeform natural language descriptions are collected for each shape by asking Amazon Mechanical Turkers to describe the color, shape, material and physical appearance of the object based on a pre-rendered rotating animation of the given shape.
Following TriCoLo \cite{YueRuan2022TriCoLoTC}, we employ the image-based 3D shape representation which treats a 3D shape as the aggregation of multi-view images.
We render 6 images of each textured mesh in Text2Shape using the same camera setup as in TriCoLo and use 5 text captions for each shape.
% We randomly select 5 text captions for each shape in Text2Shape.
All chairs and tables data are trained together for the representation learning.

\textbf{Natural image-text dataset.} To evaluate how MXM-CLR works for natural images and their descriptions other than the synthetic images from Text2Shape, we also experiment with Flickr30K which is a widely-used image-text dataset containing 31,000 photographs of everyday activities, events and scenes collected from Flickr.
Each image is annotated with five text captions which describe the image content with different levels of specificity, e.g., from overall situation to specific actions.

\textbf{Metrics.} We adopt cross-modal retrieval as the downstream task to evaluate the performance of cross-modal representation learning.
Following prior works, the standard metrics of Recall rate (R@$k$) which considers a retrieval successful if at least one sample in the top $k$ retrievals is of the correct instance is used on Text2Shape and Flickr30K for quantitative comparisons.
In addition, we compute Normalized Discounted Cumulative Gain (NDCG) \cite{KalervoJrvelin2002CumulatedGE} which is a commonly-used information retrieval metric on Text2Shape and compare the results with related baselines.



\begin{figure*}
  \centering
  \setlength{\abovecaptionskip}{0.02cm}
  \includegraphics[width=1.0\linewidth]{retrieval.pdf}
  \caption{
  Qualitative comparison of MXM-CLR and other baselines for  $\langle$text, shape$\rangle$ retrieval tasks on Text2Shape dataset. The blue or red text are manually highlighted to indicate the parts that are matched or unmatched between the query and the retrieval result. For both tasks, MXM-CLR can obtain more fine-grained retrieval results. 
  }
  \label{fig:compare}
  \vspace{-6pt}
\end{figure*}

\subsection{Implementation Details}
\textbf{Encoders.}
We adopt the text and image encoders used in CLIP, i.e., BERT \cite{JacobDevlin2018BERTPO} for text, and ResNet101 \cite{KaimingHe2015DeepRL} or vision transformer \cite{AlexeyDosovitskiy2020AnII} for images.
For each encoder model, we use the pre-trained weights from CLIP and fine-tune the models jointly using MFH loss and other baseline losses on Text2Shape and Flickr30K. 



\textbf{Training details.}
To train MXM-CLR, the encoders of text and image are initialized with the pre-trained weights from CLIP and fine-tuned for 10 epochs for each task.
The framework is implemented using PyTorch and trained on three NVIDIA RTX3090 GPUs.
The training time is depending on the type of cross-modal task, the batch size $b$ and the repetition time $p$ for masked random pair selection when computing the hard relationship loss.
For example, it takes about 0.67 hours to train MXM-CLR for the text-shape retrieval task in Table \ref{tab_text2shape_results} when repeating the selection for $p=10N_{col}$ times.
Here, $N_{col}$ is the number of columns for a group in the group matrix, e.g., the number of observations for an instance of the text modality for the $L_{I \rightarrow T}^H$ loss.
If not specifically mentioned, we also set $p=10N_{col}$ in the following experiment.
More training details and the training time for other settings are provided in the supplementary material.

\begin{table}[!t]
\centering
\setlength{\tabcolsep}{1.5pt}
\resizebox{1\columnwidth}{!}{
\begin{tabular}{ccccccccccc}
\hline
\multirow{2}{*}{Method}  &           & \multicolumn{4}{c}{Text $\Rightarrow$ Shape (Agg-Image)}                                    &           & \multicolumn{4}{c}{Shape (Agg-Image) $\Rightarrow$ Text}                                    \\ \cline{3-6} \cline{8-11} 
                        &           & R@1            & R@5            & R@10           & NDCG         &           & R@1            & R@5            & R@10           & NDCG         \\ \cline{1-1} \cline{3-6} \cline{8-11} 
Text2Shape\cite{chen2018text2shape}               &           & 0.40           & 2.37           & ---            & 1.35           &           & 0.94           & 3.69           & ---            & 0.85           \\
Y2Seq2Seq\cite{ZhizhongHan2019Y2Seq2SeqCR}                &           & 2.93           & 9.23           & ---            & 6.05           &           & 6.77           & 19.30          & ---            & 5.30           \\
TriCoLo (I)\cite{YueRuan2022TriCoLoTC}              &           & 8.28           & 24.52          & ---            & 16.52          &           & 11.91          & 32.69          & ---            & 9.37           \\
TriCoLo (V)\cite{YueRuan2022TriCoLoTC}              &           & 8.73           & 26.10          & ---            & 17.53          &           & 13.07          & 35.62          & ---            & 10.33          \\
TriCoLo (I+V)\cite{YueRuan2022TriCoLoTC}            &           & 10.25          & 29.07          & ---            & 19.85          &           & 16.33          & 42.52          & ---            & 12.73          \\ 
Parts2Words\cite{ChuanTang2021Part2WordLJ}          &           & 12.72          & 32.98          & ---            & 23.13                 &           & 19.38          & 47.17          & ---            & 15.3        \\ 
\hline


$\mathrm{CLIP_{Info}}$\cite{AlecRadford2021LearningTV} &  & 15.24& 35.72 & 47.81 & 25.66 &  & 23.12 & 49.20 & 62.71 & 17.33 \\
$\mathrm{CLIP_{Info+Soft}}$ &  & 15.15 & 36.07 & 47.65 & 25.96 &  & 24.40 & 50.94 & 62.47 & 18.12 \\
Robust-XR$^*$\cite{andonian2022robust} &  & 15.32 & 36.59 & 48.71 & 26.12  &  & 23.39 & 51.74 & 62.87 & 18.55  \\ 
$\mathrm{CLIP_{Info+Mask}}$ &  & 15.90 & 37.53 & 49.48 & 27.05 &  & 25.00 & 52.68 & 64.48 & 18.53 \\
\cline{1-1} \cline{3-6} \cline{8-11} 
\textbf{MXM-CLR} & \textbf{} & \textbf{16.83} & \textbf{39.06} & \textbf{51.44} &\textbf{ 28.38} & \textbf{} & \textbf{27.48} & \textbf{56.03} & \textbf{68.23} & \textbf{20.25} \\ \hline
\end{tabular}}
\caption{Quantitative comparison of MXM-CLR with other methods for  $\langle$text, shape$\rangle$ retrieval on Text2Shape dataset. 
ViT-B/16 is used for image encoder of the CLIP-based models and MXM-CLR, with batch size (\#images,\#texts) as (600,100) and (420,350), respectively.
NDCG is measured with top 5 results.
$^*$Results of Robust-XR are based on our own implementation as their code is not released.
}
\label{tab_text2shape_results}
\vspace{-4pt}
\end{table}

\begin{table*}[]
\centering
\resizebox{1.8\columnwidth}{!}
{
\setlength{\tabcolsep}{3pt}
\begin{tabular}{cccccccccccllccccccccccll}
\hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Loss \\ function\end{tabular}} & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Image \\ Encoder\end{tabular}} & \multicolumn{11}{c}{Text2Shape}                                                                                                                          &  & \multicolumn{11}{c}{Flickr30K}                                                                                                                 \\ \cline{3-13} \cline{15-25} 
                                                                          &                                                                            & \multicolumn{3}{c}{Text$\Rightarrow$Image}                   &           & \multicolumn{3}{c}{Image$\Rightarrow$Text}                   &  & \multicolumn{3}{c}{Batch}           &  & \multicolumn{3}{c}{Text$\Rightarrow$Image}                   &  & \multicolumn{3}{c}{Image$\Rightarrow$Text}                  &  & \multicolumn{3}{c}{Batch}           \\ \cline{3-5} \cline{7-9} \cline{11-13} \cline{15-17} \cline{19-21} \cline{23-25} 
                                                                          &                                                                            & R@1            & R@5            & R@10           &           & R@1            & R@5            & R@10           &  & \multicolumn{3}{c}{(imgs,texts,GB)} &  & R@1            & R@5            & R@10           &  & R@1            & R@5            & R@10          &  & \multicolumn{3}{c}{(imgs,texts,GB)} \\ \cline{1-13} \cline{15-25} 
$\mathrm{CLIP_{Info+Mask}}$                                                                   & RN101                                                                      & 15.11          & 25.87          & 33.71          &           & 22.67          & 49.29          & 61.29          &  & \multicolumn{3}{c}{(420,420,49.16)} &  & \textbf{66.86}          & 88.88          & 93.44          &  & 82.60           & 95.70           & 97.60          &  & \multicolumn{3}{c}{(400,400,44.04)} \\
MXM-CLR                                                                & RN101                                                                      & \textbf{17.75} & \textbf{29.10} & \textbf{37.52} & \textbf{} & \textbf{27.27} & \textbf{54.50} & \textbf{66.91} &  & \multicolumn{3}{c}{(420,350,47.39)} &  & 66.60           & \textbf{89.56} & \textbf{94.52} &  & \textbf{83.10}  & \textbf{96.80}  & \textbf{98.60} &  & \multicolumn{3}{c}{(80,400,20.69)}  \\ \cline{1-13} \cline{15-25}
$\mathrm{CLIP_{Info+Mask}}$                                                                      & ViT-B/16                                                                   & 16.66          & 28.36          & \textbf{36.93}          &           & 23.63          & 50.13          & 62.96          &  & \multicolumn{3}{c}{(420,420,57.24)} &  & 78.92          & 95.40           & \textbf{97.88}          &  & 92.60           & 99.00          & 99.80          &  & \multicolumn{3}{c}{(400,400,54.16)} \\
MXM-CLR                                                                 & ViT-B/16                                                                   & \textbf{17.00} & \textbf{28.50} & 36.62 &           & \textbf{24.85} & \textbf{51.31} & \textbf{63.39}  &  & \multicolumn{3}{c}{(420,350,55.64)} &  & \textbf{79.20} & \textbf{95.66} & 97.86 &  & \textbf{93.80} & \textbf{99.10} & \textbf{99.90} &  & \multicolumn{3}{c}{(80,400,23.50)}  \\ \cline{1-13} \cline{15-25} 
$\mathrm{CLIP_{Info+Mask}}$                                                                      & ViT-B/32                                                                   & 15.67          & 27.04          & 35.70           &           & 22.02          & 47.22          & 60.30           &  & \multicolumn{3}{c}{(420,420,28.01)} &  & \textbf{73.68}          & 93.00          & 96.40           &  & 87.50           & \textbf{98.00}          & 99.20          &  & \multicolumn{3}{c}{(400,400,24.07)} \\
MXM-CLR                                                                 & ViT-B/32                                                                   & \textbf{17.19} & \textbf{29.44} & \textbf{37.41}  &           & \textbf{24.51} & \textbf{51.16} & \textbf{63.67}  &  & \multicolumn{3}{c}{(420,350,26.82)} &  & 73.58  & \textbf{93.26} & \textbf{96.60}  &  & \textbf{88.00}  & \textbf{98.00} & \textbf{99.30} &  & \multicolumn{3}{c}{(80,400,17.07)}  \\ \hline
\end{tabular}}
\caption{
Evaluation of MXM-CLR trained with different image encoders for $\langle$text, image$\rangle$ retrieval task.
With comparable memory cost on Text2Shape and much less memory cost on Flickr30K, MXM-CLR generally achieves superior performance than the baseline.
Note the InfoNCE based methods construct the batch with 1:1 ratio of images and texts and MXM-CLR performs the group-wise pairing which utilizes all available observations for the same instance, i.e., the image:text ratio is 6:5 for Text2Shape and 1:5 for Flickr.
}
\label{tab_MF}
\vspace{-12pt}
\end{table*}

\subsection{Comparisons and Evaluation}

\textbf{Quantitative comparisons.} 
We first conduct quantitative experiments on Text2Shape \cite{chen2018text2shape} dataset for the text-to-shape task and compare with the existing 3D-focused cross-modal methods including Text2Shape \cite{chen2018text2shape}, Y2Seq2Seq \cite{ZhizhongHan2019Y2Seq2SeqCR}, TriCoLo \cite{YueRuan2022TriCoLoTC} and Part2Words \cite{ChuanTang2021Part2WordLJ}.
In addition, we compare with CLIP (denoted as $\mathrm{CLIP_{Info}}$) and three other strong baseline models: $\mathrm{CLIP_{Info+Soft}}$ which incorporates similar momentum-based soft learning scheme as our paper to CLIP, $\mathrm{CLIP_{Info+Mask}}$ which masks out the false negative data pairs based on the instance labels and Robust-XR \cite{andonian2022robust} which is a SOTA method for learning the soft relationships.
For Robust-XR, since their code is not released, we implement their progressive self-distillation strategy and fine-tune the pre-trained CLIP encoders for generating the results.
Following TriCoLo, we utilize the mean feature of multi-view images (denoted Agg-Image) to represent a shape and conduct cross-modal retrieval between text and shape.
From the results in Table \ref{tab_text2shape_results}, MXM-CLR outperforms the existing 3D-focused cross-modal learning methods by a significant margin and also shows notable improvements comparing to the CLIP-based strong baseline models.




\textbf{Qualitative comparisons.} 
Figure \ref{fig:compare} shows qualitative comparison between MXM-CLR with TriCoLo (I), Part2Words and $\mathrm{CLIP_{Info+Mask}}$.
The visual results show MXM-CLR can return shape/text that is more similar to the query text/shape.
More qualitative comparisons are provided in the supplementary material.



\begin{figure*}[!t]
	\centering
	% \setlength{\abovecaptionskip}{0.01cm}
	\includegraphics[width=0.95\linewidth]{Batch.pdf}
 	% \vspace{-2pt}
	\caption{
	Ablation study on batch sizes of MXM-CLR for $\langle$text, image$\rangle$ retrieval task on Flickr30K.
	As a comparison, $\mathrm{CLIP_{Info+Mask}}$ results at two relatively large batch size settings are also visualized. 
	ResNet101 is used as the image encoder for this experiment.
	}
	\label{fig_batch}
	\vspace{-10pt}
\end{figure*}


\begin{table}[!t]
\centering
\resizebox{1\columnwidth}{!}{
\setlength{\tabcolsep}{1.8pt}
\begin{tabular}{cccccccccccll}
\hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Loss \\ function\end{tabular}} & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Image \\ Encoder\end{tabular}} & \multicolumn{11}{c}{Text2Shape}                                                                                                                                                                                                                                                  \\ \cline{3-13} 
                                                                          &                                                                            & \multicolumn{3}{c}{Text $\Rightarrow$ Agg-Image}                                                              &                      & \multicolumn{3}{c}{Agg-Image$\Rightarrow$ Text}                                                                        &  & \multicolumn{3}{c}{Batch}           \\ \cline{3-5} \cline{7-9} \cline{11-13} 
                                                                          &                                                                            & R@1                               & R@5                               & R@10                      &                      & R@1                                & R@5                                & R@10                               &  & \multicolumn{3}{c}{(imgs,texts,GB)} \\ \hline
$\mathrm{CLIP_{Info+Mask}}$                                                                    & RN101                                                                      & 15.43                             & 37.45                             & 50.00            &                      & 26.34                              & 55.43                              & 67.16                              &  & \multicolumn{3}{c}{(600,100,55.89)} \\

MXM-CLR                                                               & RN101                                                                      & \multicolumn{1}{l}{\textbf{15.84}} & \multicolumn{1}{l}{\textbf{38.41}} & \multicolumn{1}{l}{\textbf{50.54}} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{\textbf{26.47}} & \multicolumn{1}{l}{\textbf{56.10}} & \multicolumn{1}{l}{\textbf{67.90}} &  & \multicolumn{3}{c}{(420,350,47.39)} \\ \hline
$\mathrm{CLIP_{Info+Mask}}$                                                                    & ViT-B/16                                                                   & 15.90                              & 37.53                             & 49.48                     &                      & 25.00                              & 52.68                              & 64.48                              &  & \multicolumn{3}{c}{(600,100,66.27)} \\
MXM-CLR                                                               & ViT-B/16                                                                   & \textbf{16.83}                       & \textbf{39.06}                    & \textbf{51.44}            &                      & \textbf{27.48}                     & \textbf{56.03}                     & \textbf{68.23}                     &  & \multicolumn{3}{c}{(420,350,55.64)} \\ \hline
$\mathrm{CLIP_{Info+Mask}}$                                                                    & ViT-B/32                                                                   & 15.47                             & 36.30                              & 48.20                      &                      & 24.60                               & 53.08                              & 64.61                              &  & \multicolumn{3}{c}{(600,100,25.25)} \\
MXM-CLR                                                               & ViT-B/32                                                                   & \textbf{16.24}                    & \textbf{37.78}                    & \textbf{49.37}            &                      & \textbf{26.41}                     & \textbf{54.76 }                   & \textbf{66.02}                     &  & \multicolumn{3}{c}{(420,350,26.82)} \\ \hline

\end{tabular}
}
\caption{
Evaluation of MXM-CLR for $\langle$text, agg-image (shape)$\rangle$ retrieval task. Since six images are aggregated into one shape feature for this task, the input ratio for image-text is 6:1 for InfoNCE-based methods.
For MXM-CLR, it computes 70 aggregated shape features from the 420 input images and pairs the 70 shapes with the 350 texts via group-wise pairing.
}

\label{tab_img_agg_mf}
\vspace{-11pt}
\end{table}

\begin{table}[!t]
% \centering
\resizebox{1\columnwidth}{!}{
\setlength{\tabcolsep}{1.6pt}
\begin{tabular}{cccccccccccll}
\hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Loss \\ function\end{tabular}} & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Vision \\ Encoder\end{tabular}} & \multicolumn{11}{c}{Text2Shape}                                                                                                                            \\ \cline{3-13} 
                                                                          &                                                                            & \multicolumn{3}{c}{Agg-Text $\Rightarrow$ Image} &           & \multicolumn{3}{c}{Image  $\Rightarrow$ Agg-Text} &  & \multicolumn{3}{c}{Batch}            \\ \cline{3-5} \cline{7-9} \cline{11-13} 
                                                                          &                                                                            & R@1            & R@5            & R@10           &           & R@1             & R@5            & R@10           &  & \multicolumn{3}{c}{(imgs,texts,GB)}  \\ \hline
$\mathrm{CLIP_{Info+Mask}}$                                                                    & RN101                                                                      & 26.34          & 46.92          & 56.77          &           & 21.00           & 48.11          & 61.28          &  & \multicolumn{3}{c}{(256,1280,54.46)} \\

MXM-CLR                                                               & RN101                                                                      & \textbf{36.33} & \textbf{56.77} & \textbf{67.29} & \textbf{} & \textbf{24.69}   & \textbf{54.67} & \textbf{68.49} &  & \multicolumn{3}{c}{(240,200,27.59)}  \\ \hline
$\mathrm{CLIP_{Info+Mask}}$                                                                        & ViT-B/16                                                                   & 38.20           & 58.31          & 69.37          &           & 27.96           & 58.05          & 70.89          &  & \multicolumn{3}{c}{(256,1280,62.48)} \\
MXM-CLR                                                               & ViT-B/16                                                                   & \textbf{40.28} & \textbf{59.99} & \textbf{71.31} &           & \textbf{28.14}  & \textbf{59.37} & \textbf{72.54} &  & \multicolumn{3}{c}{(240,200,32.53)}  \\ \hline
$\mathrm{CLIP_{Info+Mask}}$                                                                        & ViT-B/32                                                                   & 35.46          & 55.50          & 64.88          &           & 25.25           & 53.49          & 67.27          &  & \multicolumn{3}{c}{(256,1280,42.02)} \\
MXM-CLR                                                               & ViT-B/32                                                                   & \textbf{39.95} & \textbf{59.05} & \textbf{68.36} &           & \textbf{27.49}  & \textbf{56.76} & \textbf{70.05} &  & \multicolumn{3}{c}{(240,200,15.85)}  \\ \hline
\end{tabular}
}
\caption{Evaluation of MXM-CLR for  $\langle$agg-text, image$\rangle$ retrieval task. Since five texts are aggregated into one comprehensive text feature for this task, the input ratio for image-text is 1:5 for InfoNCE-based methods.
For MXM-CLR, it computes 40 aggregated text features from the 200 input texts and pairs the 40 aggregated texts with the 240 images via group-wise pairing.
}
\label{text_agg_mf_loss}
\vspace{-11pt}
\end{table}


% \textbf{Comparison between CLIP (Mask-Info) and MF-InfoNCE.}
\textbf{Evaluation on adaptability and generalizability of MXM-CLR.}
As a unified framework, MXM-CLR can be adapted to different variations for specific tasks.
In Table \ref{tab_MF} (left), Table \ref{tab_img_agg_mf} and Table \ref{text_agg_mf_loss}, we show the quantitative results of three MXM-CLR variations on Text2Shape dataset.
The $\mathrm{CLIP_{Info+Mask}}$ which has the closest performance with MXM-CLR based on previous experiments (Table \ref{tab_text2shape_results}) is used as the baseline for comparison.
Different image encoders, including ResNet101 and different versions of vision transformers (ViT-B/16 and ViT-B/32), are tested to evaluate the generalizability of MXM-CLR.
Also, to further verify generalizability of MXM-CLR to different datasets, similar experiments are performed on Flickr30K dataset (Table \ref{tab_MF} right) for text-image retrieval.
As the batch size will affect the results for the contrastive learning methods, for a fair comparison on each task, we use the maximum batch size that a $\mathrm{CLIP_{Info+Mask}}$ model can achieve under our GPU memory limit as a reference and choose a smaller batch size for MXM-CLR.
Since the memory cost is mainly due to images, we either use the same or a smaller image number in a MXM-CLR batch than $\mathrm{CLIP_{Info+Mask}}$.
% Hence, the experiment setting for MXM-CLR is more challenging.


From the results, MXM-CLR can achieve the superior performance for most tasks with different image encoders, showing its adaptability and generalizability.
Moreover, while being able to learning better representations, the comparable or much smaller memory cost of MXM-CLR demonstrates the data efficiency of our method.
Note that for Flickr30K, we only want to show the generalizability of MXM-CLR and its superior performance comparing to the XM-CLR baselines, instead of aiming to beat the SOTA results based on vision-language pre-training such as \cite{wang2022image}. 



\subsection{Ablation Studies}
\textbf{Ablation study on MFH loss.}
As shown in Table \ref{tab_ablation}, we investigate the contributions of different terms of MFH loss.
All experiments are conducted for the $\langle$text, image$\rangle$ retrieval task with the same batch size of 420 images and 350 texts.
To focus the evaluation on the relationship modeling schemes, we compare with another two baselines ($\mathrm{CLIP^G_{Info}}$ and $\mathrm{CLIP^G_{Info+Mask}}$) which are the group-wise adaptation of the corresponding CLIP models.
Specifically, for the group-wise paired data matrix (e.g., Fig \ref{fig:overview_loss}(b)), $\mathrm{CLIP^G_{Info}}$ sets all pairs in diagonal line as positive and other pairs in the groups as negative, while $\mathrm{CLIP^G_{Info+Mask}}$ masks out the non-diagonal pairs in the groups.
It can be seen $\mathrm{CLIP^G_{Info}}$ and $\mathrm{CLIP^G_{Info+Mask}}$ perform poorly comparing to other schemes.
This is because they only consider a fixed pairing pattern and ignore other combinations of observations which are vital for multifold cross-modal representation learning.
On the other hand, regarding all pairs in the group as positives can improve the performance to a certain degree, but it is inferior than the schemes that only select part of the pairs.
The reason is there might be some not-so-well matched pairs created by the group-wise data pairing process, e.g., a text caption describing the front side of a chair and the image is from the side view are paired based on the Cartesian product.
Hence, always training the network with all pairs may affect the contribution of other well-matched positive pairs.


Also from Table \ref{tab_ablation}, directly utilizing the hard relationship loss with the masked random positive selection scheme can achieve superior performance than the above methods since more diversified positive pair selection is helpful for MXM-CLR. 
Moreover, for most metrics, the performance of the model is positively correlated with how many times the group-wise masked random selection is repeated in an iteration, while a large repetition number will lead to a longer training time. 
On the other hand, the performance is poor if only the soft-targets generated by the momentum model are used as the supervision for MXM-CLR. 
The reason is that soft-targets alone are not enough to guide the representation learning, especially for the data with multifold observations.
When combining the soft-target learning with the hard relationship modeling, the performance can be boosted comparing to only using the hard relationship loss.



\textbf{Ablation study on batch size.}
Batch size is a critical factor for contrastive learning. Therefore, we evaluate the performance of MXM-CLR under different batch size settings for $\langle$text, image$\rangle$ retrieval task on Flickr30K.
As shown in Figure \ref{fig_batch}, the performance of the MXM-CLR increases along with the batch sizes. Besides, we also visualize the $\mathrm{CLIP_{Info+Mask}}$ result under two relatively large batch size settings.  
It can be found MXM-CLR only needs much fewer images to achieve comparable or superior performance w.r.t. $\mathrm{CLIP_{Info+Mask}}$.
This demonstrates that MXM-CLR can efficiently learn from multifold observations and mine their comprehensive relationships via the group-wise data pairing and the hybrid loss MFH.


\begin{table}[]
% \centering
\resizebox{1\columnwidth}{!}{
\setlength{\tabcolsep}{1.8pt}
\begin{tabular}{cccccccccc}
\hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Relationship modeling \\ scheme\end{tabular}} & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Vision \\ Encoder\end{tabular}} & \multicolumn{7}{c}{Text2Shape}                                                                         & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Training \\ time (h)\end{tabular}} \\ \cline{3-9}
                                                                               &                                                                            & \multicolumn{3}{c}{Text$\Rightarrow$ Image}             &  & \multicolumn{3}{c}{Image $\Rightarrow$ Text}            &                                    \\ \cline{3-5} \cline{7-9}
                                                                               &                                                                            & R@1            & R@5            & R@10           &  & R@1            & R@5            & R@10           &                                    \\ \hline
$\mathrm{CLIP^G_{Info}}$                                                                       & RN101                                                                      & 8.22           & 23.04          & 32.98          &  & 9.86           & 28.71          & 41.49          & 0.62                               \\
$\mathrm{CLIP^G_{Info+Mask}}$                                                                      & RN101                                                                      & 9.84           & 26.40          & 36.94          &  & 14.19          & 35.63          & 47.91          & 0.70                               \\ 
All                                                                            & RN101                                                                      & 15.49          & 27.11          & 35.59          &  & 21.34          & 49.15          & 61.83          & 0.73                               \\ \hline
Soft                                                  & RN101                                                                      & 2.42 & 7.03          & 11.21          &  & 2.28 & 7.10 & 11.06& 0.67                               \\ 
Hard (1)                                                                 & RN101                                                                      & 15.46          & 27.13 & 35.86 &  & 24.12 & 50.51 & 62.61 & 0.80                               \\
Hard (1)+Soft                                                   & RN101                                                                      & 16.65 & 28.32          & 36.89         &  & 25.17         & 52.55          & 64.37          & 1.05                               \\

Hard ($N_{col}$)                                                              & RN101                                                                      & 16.69          & 28.71 & 37.51 &  & 25.21          & 52.70          & 65.37          & 1.01                               \\
Hard ($N_{col}$)+Soft                                                   & RN101                                                                      & 17.49 & 29.33         & 37.48         &  & 26.43          & 53.95          & 66.19          & 1.30                               \\
Hard (10$N_{col}$)                                                             & RN101                                                                      & 16.72 & 28.56          & 37.22          &  & 25.41 & 53.49 & 66.38 & 2.02                           \\ 

Hard (10$N_{col}$)+Soft                                                   & RN101                                                                      & 17.75 & 29.10          & 37.52          &  & 27.27         & 54.50         & 66.91          & 2.45                              \\




\hline
\end{tabular}}
\caption{ Ablation study on different relationship modeling schemes.
For the hard relationship modeling, the number in the bracket indicates the repetition time for the masked random pair selection when computing the $L_{I\rightarrow T}^{H}$ or $L_{T\rightarrow I}^{H}$ loss.
$\mathrm{CLIP^G_{Info}}$ and $\mathrm{CLIP^G_{Info+Mask}}$ are the group-wise adaptation of corresponding CLIP models.
}

\label{tab_ablation}
\vspace{-10pt}
\end{table}

