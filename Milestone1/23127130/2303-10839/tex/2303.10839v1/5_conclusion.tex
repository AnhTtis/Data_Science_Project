

\section{Conclusion}
% ours work
In summary, we propose MXM-CLR, a unified cross-modal constrative representation learning framework which is particularly suitable for data with multifold observations.
% And then, we demonstrates that the representative XM-CLR methods such as CLIP\cite{AlecRadford2021LearningTV} and TriCoLo\cite{YueRuan2022TriCoLoTC} can be considered as special cases of MXM-CLR.
To train MXM-CLR models, we introduce MFH, a novel multifold-aware hybrid loss, %and propose different positive pair selection schemes 
to simultaneously leverage multiple positive observations when computing the hard and soft relationships for the cross-modal data pairs.
Currently, we only consider the inter-modality relationships during the training. 
Inspired by works which consider relationships for both inter- and intra-modalities \cite{MohammadrezaZolfaghari2021CrossCLRCC,hong2022versatile}, incorporating the intra-modality relationships into our MXM-CLR by applying hard and soft relationship losses to data pairs constructed from the same-modal observations is a promising way to further improve the representation learning performance.
In addition, extending the MXM-CLR to other modalities such as videos (represented as multifold clips) is also an interesting future direction.
