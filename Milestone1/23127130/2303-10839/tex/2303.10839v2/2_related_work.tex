\section{Related Work}
\label{sec:related_work}

\textbf{Contrastive representation learning.}
The main idea of contrast learning is to pull data points in positive pairs together and push data in negative pairs away.
Due to its simplicity and flexibility, contrastive learning has attracted numerous interests for representation learning \cite{ZhirongWu2018UnsupervisedFL,AaronvandenOord2018RepresentationLW,YonglongTian2019ContrastiveMC,KaimingHe2022MomentumCF,XinleiChen2020ImprovedBW,TingChen2020ASF,TingChen2020BigSM,PrannayKhosla2022SupervisedCL}.
Wu et al. \cite{ZhirongWu2018UnsupervisedFL} first propose a contrastive loss (named NCE loss) for the instance discrimination task.
Based on NCE loss, CPC \cite{AaronvandenOord2018RepresentationLW} introduces InfoNCE loss and  utilizes the autoregressive model to learn data representations by predicting the future in latent space. 
Comparing to above works which mainly deal with data in a single modality, we focus on cross-modal contrastive learning and propose to learn the joint embeddings of data with multifold observations.
Our multifold-aware MFH loss can simultaneously leverage multiple positive observations for more comprehensive feature learning.

\textbf{Joint learning of cross-modal data.}
A visual concept can be described with different data modalities including vision, language and 3D etc.
A great deal of efforts have been paid to the joint learning of vision and language features \cite{AlecRadford2021LearningTV,ChaoJia2021ScalingUV,JunnanLi2021AlignBF,yao2021filip,yang2022vision,wang2022image,FartashFaghri2018VSEIV,AndreaFrome2013DeViSEAD,RyanKiros2014UnifyingVE,YuhaoZhang2021ContrastiveLO,JunkeWang2022OmniVLOneFM}.
Among these works, CLIP \cite{AlecRadford2021LearningTV} is a pioneering work that is trained using InfoNCE to learn transferable features between image and text.
In addition, joint learning of 3D-image \cite{li2015joint,uy2021joint,qi2021toward,liu2021fine,zhu2019label,fu2020hard} and 3D-text \cite{chen2018text2shape,ZhizhongHan2019Y2Seq2SeqCR,ChuanTang2021Part2WordLJ,YueRuan2022TriCoLoTC}, as well as the 3D-involved applications such as image-based 3D shape retrieval \cite{lin2021single,kuo2021patch2cad,fu2020hard}, image based 3D reconstruction \cite{peng2022tmvnet,zhang2021view}, 3D visual grounding \cite{PanosAchlioptas2020ReferIt3DNL,DaveZhenyuChen2020ScanRefer3O} and text-to-3D generation \cite{sanghi2022clip,hong2022avatarclip}, have also attracts great attention in graphics and vision community.
Comparing to above methods, we are the first to propose a unified cross-modal joint learning framework which can be easily adapted to different tasks and different data modalities.


\textbf{Relationship modeling with different hard pairings.}
To improve the performance for representation learning, several works have been proposed to model more complicated hard relationships of the data samples.
In contrast to CLIP which only considers pairwise positive or negative relationships in a one-to-one manner, CMC \cite{YonglongTian2019ContrastiveMC} treats all samples from different sensors as positive to maximize mutual information between different views (modalities) of the same scene.
SupCon \cite{PrannayKhosla2022SupervisedCL} leverages the label information and treats all samples with the same class label as positive.
Instead of modeling relationships for the positive pairs, CrossCLR \cite{MohammadrezaZolfaghari2021CrossCLRCC} identifies the highly related text-video samples and masks them from negative samples to avoid issues with false negatives for InfoNCE-based methods.
Comparing to these methods, we construct groups of positive pairs from different observations of the same instance and apply a group-wise masked random selection scheme to obtain multiple combinations of different positive pairs when performing the contrastive learning.

\textbf{Relationship modeling with soft-target learning.}
Apart from modeling the hard relationships for the data pairs, a few works \cite{li2021align,cheng2021data,andonian2022robust,zhong2022regionclip} take the idea of the knowledge distillation and employ a teacher model to generate the soft-alignment targets for learning the relationships between the paired data.
For example, ALBEF \cite{li2021align} uses a momentum model as a continuously-evolving teacher to generate the pseudo-targets as additional supervision for training.
Similarly, Robust-XR \cite{andonian2022robust} adopts progressive self-distillation to generate soft image-text alignments for robust representation learning from noisy data.
Also, HCMoCo attempts \cite{hong2022versatile} to explore the intra- and inter-sample hard and soft relationships for human-centric representation learning.
For our MXM-CLR, we aim to mine more comprehensive relationships from the multifold observations, while such setting can bring both benefits and challenges for the soft relationship learning.
Hence, we propose a multifold-aware hybrid learning strategy by combining the soft-target learning with the group-wise masked hard relationship modeling to boost the learning performance.
