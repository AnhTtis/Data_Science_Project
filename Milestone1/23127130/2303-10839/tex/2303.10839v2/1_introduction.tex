\section{Introduction}


\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{teaser.pdf}
    \caption{Illustration of MXM-CLR as a unified framework for contrastive learning of multifold cross-modal representations.
    Different XM-CLR models such as CLIP and TriCoLo can be derived from MXM-CLR by adjusting the data pairing and the relationship modeling schemes. 
    The gray dashed boxes represent training batches with multiple positive observations. 
    The solid or dash lines indicate the hard or soft relationships between the cross-modal data pairs.
    Note the dashed border of T3 in (d) shows the observation may not be sampled into the batch. 
    }
     \vspace{-10pt}
  \label{fig:teaser}
\end{figure}
\label{sec:intro}
   

Representation learning is a key task for deep learning \cite{YoshuaBengio2013RepresentationLA}.
Extensive efforts have been paid on learning robust and generalizable feature representations for data of different modalities, such as text \cite{JacobDevlin2018BERTPO,AshishVaswani2017AttentionIA}, images \cite{KaimingHe2015DeepRL,AlexeyDosovitskiy2020AnII} and point clouds \cite{Qi_2017_CVPR}. 
Recently, cross-modal representation learning that aims to learn a joint embedding space of features representing two different data modalities has attracted wide attention for its emerging applications in vision-language pretraining \cite{AlecRadford2021LearningTV,wang2022image,JunkeWang2022OmniVLOneFM,YanZeng2021MultiGrainedVL}, text-to-image generation \cite{AdityaRamesh2021ZeroShotTG,ramesh2022hierarchical,nuwa,Saharia2022PhotorealisticTD}, text-based 3D retrieval \cite{chen2018text2shape,ZhizhongHan2019Y2Seq2SeqCR,ChuanTang2021Part2WordLJ,YueRuan2022TriCoLoTC} and 3D visual grounding \cite{PanosAchlioptas2020ReferIt3DNL,DaveZhenyuChen2020ScanRefer3O,PinHaoHuang2021TextGuidedGN,JunhaRoh2021LanguageReferSM,ZhihaoYuan2021InstanceReferCH,LichenZhao20213DVGTransformerRM} etc.

Contrastive learning is a type of representation learning scheme that can effectively learn discriminative features in a supervised \cite{PrannayKhosla2022SupervisedCL} or unsupervised manner \cite{ZhirongWu2018UnsupervisedFL,TingChen2020ASF}.
The key idea is to construct two kinds of data pairs, i.e., positive/negative pairs corresponding to the same/different instances, and design losses so that the learned features for the positive pairs are pulled together, while the features for negative pairs are repelled from each other.
For example, InfoNCE \cite{AaronvandenOord2018RepresentationLW,YuhaoZhang2021ContrastiveLO} is a widely-used contrastive loss for representation learning and it is designed to maximally preserve the mutual information of the positive pairs.

Naturally, contrastive learning can be applied to learning inherent features for data coming from the same modality \cite{ZhirongWu2018UnsupervisedFL,AaronvandenOord2018RepresentationLW,YonglongTian2019ContrastiveMC,KaimingHe2022MomentumCF,XinleiChen2020ImprovedBW,TingChen2020ASF,TingChen2020BigSM,PrannayKhosla2022SupervisedCL} as well as to aligning features for cross-modal data \cite{AlecRadford2021LearningTV,ChaoJia2021ScalingUV,JunnanLi2021AlignBF,yang2022vision,YueRuan2022TriCoLoTC}.
In the pioneering work CLIP \cite{AlecRadford2021LearningTV}, cross-modal contrastive learning (XM-CLR) is performed on 400 million image-text pairs collected from the Internet to learn transferable language and image representations.
Beyond text and images, XM-CLR has also been explored on 3D data \cite{YueRuan2022TriCoLoTC,zhang2022pointclip,sanghi2022clip,hong2022avatarclip}.
For example, TriCoLo \cite{YueRuan2022TriCoLoTC} employs constrastive learning to train a joint embedding space and obtain aligned representations of 3D colored voxels, text descriptions and multi-view images for shapes in the Text2Shape dataset \cite{chen2018text2shape}.

Although existing XM-CLR methods have achieved intriguing results on learning aligned features for different data modalities, they are mainly single-fold-oriented as only one positive pair is considered when computing the contrastive loss.
On the other hand, the representation of a particular instance is often \textit{multifold}.
For example, as shown in Figure \ref{fig:teaser}(a), multiple text sentences can be used to describe a 3D table and the table can also be rendered into images at different views. 
Hence, the observations of a particular table are multifold for either the text or image modality.
Such multifold observations are also common for text descriptions of natural images, e.g., each image in Flickr30K \cite{PeterYoung2014FromID} and MS COCO \cite{lin2014microsoft} is annotated with about five captions describing the fine-grained content of the image.
With the multifold cross-modal observations, it is easy to obtain multiple positive pairs using the Cartesian product operation.
These additionally constructed positive pairs contain useful information for learning the representation of the corresponding data instance.
However, how to properly utilize these positive pairs in a contrastive learning framework has not been explored before.


In this paper, we propose \textbf{MXM}-CLR, a unified framework for \textbf{m}ultifold \textbf{c}ross-\textbf{m}odal contrastive representation learning.
Our key insight is, for cross-modal constrastive learning on data with multifold observations, multiple positive pairs could be conveniently constructed and the loss function should be designed to simultaneously leverage these positive pairs for more comprehensive feature learning.
Thus, we propose a novel multifold-aware hybrid loss, named MFH, to consider multiple positive observations when computing the hard and soft relationships for the cross-modal data pairs.
Specifically, we first construct \textit{group-wise} data pairings by applying Cartesian product to the observations of each modality.
In this sense, the MXM in the MXM-CLR acronym can also be understood as the Cartesian product between multifold observations of two modalities.
Then, the MFH models the hard relationships between multiple observations based on a subset of positive pairs obtained with a group-wise random selection scheme.
In addition, the soft relationships between the observations in both positive and negative pairs are learned based on the pseudo-targets generated by a momentum model \cite{li2021align}.
As more information from multiple observations is utilized by the MFH loss in a hybrid manner, our MXM-CLR can achieve superior performance on representation learning.



Comparing to previous single-fold-oriented methods such as CLIP, MXM-CLR is primarily suitable for data with multifold observations.
Meanwhile, as a unified framework, MXM-CLR can also be derived to different forms, while each variation corresponds to a model for a particular cross-modal task.
For example, as shown in Figure \ref{fig:teaser}(a), CLIP is a special case of MXM-CLR that only considers the hard relationships based on three given positive pairs in a one-to-one mapping manner.
Also, as shown in Figure \ref{fig:teaser}(d), TriCoLo (I), which is the bimodal version of TriCoLo \cite{YueRuan2022TriCoLoTC}, can be regarded as a degenerate case of MXM-CLR that is trained with multifold text features and the aggregated multi-view image feature.
Furthermore, MXM-CLR can be adapted to model other contrastive learning scenarios, including learning from the aggregated text feature and multifold image features (Figure \ref{fig:teaser}(e)), and from the aggregation of features for both modalities (Figure \ref{fig:teaser}(f)).
These two variations may correspond to novel tasks such as comprehensive text-to-image and text-to-shape retrieval, respectively.

To evaluate the performance of MXM-CLR, we perform extensive experiments on different cross-modal retrieval tasks on Text2Shape \cite{chen2018text2shape} and Flickr30K \cite{PeterYoung2014FromID}.
From the quantitative and qualitative results,
MXM-CLR models trained with MFH loss outperform the SOTA methods and multiple strong baselines with a significant margin.
We also perform ablation studies to validate the effect of each term of the MFH loss
and evaluate how the batch size can affect the results for MXM-CLR. 

In summary, our main contributions are as follows:

\begin{itemize}
    \item We propose MXM-CLR, the first unified framework for contrastive representation learning of cross-modal data with multifold observations, and we show existing XM-CLR methods such as CLIP \cite{AlecRadford2021LearningTV} and TriCoLo \cite{YueRuan2022TriCoLoTC} can be regarded as special cases of MXM-CLR.
    
    \item We introduce MFH, a novel multifold-aware hybrid loss, to explicitly model the hard and soft relationships for the data batch constructed with multiple positive observations.
    
    \item Extensive quantitative and qualitative comparisons with SOTA methods and strong baselines show the superiority of MXM-CLR in learning better representations for multifold data. 
    
\end{itemize}





