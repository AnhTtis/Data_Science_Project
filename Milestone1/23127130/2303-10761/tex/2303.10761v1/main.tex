\documentclass[12pt]{article}

\usepackage{style}
\usepackage{commands}
\addbibresource{papers.bib}


\begin{document}

\begin{titlepage}
    \begin{center}
    
        \bigskip
        % Moscow State University
        % Faculty of Computational Mathematics and Cybernetics
        % Department of Mathematical Methods of Forecasting

\title{
  \vspace{-1cm}
  \hrule height 4pt
  \vskip 0.5cm
  \textbf{Calibration of Neural Networks}
  \vskip 0.5cm
  \hrule height 1pt
  \vskip 0.5cm
}
        \date{}
        \author{
            \textbf{Ruslan Vasilev}\thanks{
            Work done in 2020--2021 at Lomonosov Moscow State University as Ruslan Vasilev coursework (3\textsuperscript{rd} year) supervised by Alexander D'yakonov.
            } \\
            \texttt{artnitolog@yandex.com}
            \and
            \textbf{Alexander D'yakonov}\footnotemark[1] \\
            \texttt{djakonov@mail.ru}
        }

        {\let\newpage\relax\maketitle}
        \thispagestyle{empty}

        % \vspace{-0.5cm}
        % {\normalsize Lomonosov Moscow State University}
    
        \begin{abstract}
            % - Что такое калибровка и почему проблема релевантна
            % - Что сделано в данной работе
            Neural networks solving real-world problems are often required not only to make accurate predictions but also to provide a confidence level in the forecast. The calibration of a model indicates how close the estimated confidence is to the true probability. This paper presents a survey of confidence calibration problems in the context of neural networks and provides an empirical comparison of calibration methods. We analyze problem statement, calibration definitions, and different approaches to evaluation: visualizations and scalar measures that estimate whether the model is well-calibrated. We review modern calibration techniques: based on post-processing or requiring changes in training. Empirical experiments cover various datasets and models, comparing calibration methods according to different criteria.
        \end{abstract}
        
    \end{center}

\end{titlepage}

{
    \hypersetup{linkcolor=black}
    \tableofcontents
}

\newpage

\section{Introduction}

% Количество областей, в которых используется глубокое обучение, стремительно растет. Нейронные сети активно применяются для диагностики заболеваний по медицинским изображениям \cite{medical_nn}, используются в алгоритмах управления беспилотными автомобилями \cite{self_driving}, а также для машинного перевода \cite{nmt_google}.

Deep learning is finding more and more applications in various fields. Neural networks are actively used in clinical practice \cite{medical_nn}, self-driving cars \cite{self_driving}, machine translation \cite{nmt_google}, and other diverse applications.

% В подобных задачах обычно требуется обучить модель, которая будет выдавать не только корректное предсказание, но и надежную степень уверенности в нем. Под \emph{уверенностью} понимается оценка вероятности прогноза. Например, если алгоритм для большой выборки пациентов предсказывает, что они здоровы с вероятностью 0.9, то мы ожидаем, что $90\%$ из них действительно окажутся здоровыми. Модель, выдающая достоверные вероятности, называется \emph{откалиброванной}. Наряду с интерпретацией предсказаний нейросетей, откалиброванность важна, когда вероятности используются на последующих этапах работы алгоритмов (например, в языковых моделях \cite{nn_lm}).

Oftentimes, real-world problems require models that produce not only correct prediction but also a reliable measure of confidence in it. \emph{Confidence} refers to probability estimate that the forecast is correct. For example, if an algorithm predicts a given sample of patients are healthy with confidence 0.9, we expect that $90\%$ of them are really healthy. A model with reliable confidence estimation is called \emph{calibrated}. Along with the interpretation of neural network predictions, confidence calibration is important when probability estimates are fed into subsequent algorithm steps (for example, language models decoding strategies \cite{decode_lm}).

% Современные нейронные сети нередко оказываются плохо откалиброванными \cite{on_cal}. Тем не менее смещенные оценки уверенности выдают и многие другие алгоритмы машинного обучения \cite{good_proba, emp_comparison}. Для <<классических>> моделей были предложены различные техники калибровки, некоторые из которых получили развитие в нейронных сетях.

Modern neural networks often turn out to be poorly calibrated \cite{on_cal}. However, many other machine learning algorithms also produce biased confidence estimates \cite{good_proba, emp_comparison}. Various calibration techniques have been proposed for ``classical'' machine learning, some of which have been developed in deep learning.

\section{Problem Statement}

% Пусть решается задача классификации объектов из множества $\mathcal{X}$ с классами ${\mathcal{Y}}=\{1,\dots,K\}$. Предположим, что мы обучили \emph{модель} -- алгоритм, который для каждого $x\in{\mathcal{X}}$ выдает вектор оценок -- \emph{уверенностей} (confidences) $\vec{a}(x)=(a_1(x),\dots,a_K(x))$, $\sum_{j=1}^{K}a_j(x)=1$. Далее объекту приписывается класс, соответствующий наибольшей уверенности:

Consider a classification problem for objects from set $\mathcal{X}$ with classes ${\mathcal{Y}}=\{1,\dots,K\}$. Suppose we have trained a \emph{model}~--- an algorithm which produces vector of scores \emph{(confidences)} $\vec{a}(x)=(a_1(x),\dots,a_K(x))$, $\sum_{j=1}^{K}a_j(x)=1$, for each $x\in{\mathcal{X}}$. Next, the class corresponding to the highest confidence is assigned to the object:

\begin{equation*}\hat{y}(x)\coloneqq\underset{j\in \mathcal{Y}}{\argmax{a_j}},\quad \hat{p}(x)\coloneqq a_{\hat{y}}.
\end{equation*}

% Оценку $\hat{p}$ мы бы хотели трактовать как вероятность того, что истинная метка $y$ совпадает с предсказанной $\hat{y}$. Если наша оценка достаточно точна, то модель называют \emph{откалиброванной}. Формально определение \emph{откалиброванности} (в \cite{on_cal} -- \engterm{perfect calibration}) можно записать следующим образом:

We would like to interpret estimator $\hat{p}$ as the probability that the true label coincides with the predicted one. If the estimate is accurate enough, then the model is called \emph{calibrated}. Formally, the definition of calibration (\emph{perfect calibration} in \cite{on_cal}) can be written as follows:

\begin{equation}\label{eq:perfect_cal_guo}
    \mathbb{P}\left(y=\hat{y}\mid \hat{p}=p\right)=p \quad \forall p\in\left[0, 1\right].
\end{equation}

% Существуют и более сильные определения откалиброванности модели, чем \eqref{eq:perfect_cal_guo}. Например, согласно \cite{isotonic} классификатор называется откалиброванным (в оригинале -- \engterm{well-calibrated}), если

There are stronger definitions of model calibration\footnote{It should be noted that the term \emph{calibration} also often refers to \emph{methods} which make model confidences accurate.} than \eqref{eq:perfect_cal_guo}. For example, according to \cite{isotonic}, the classifier is called calibrated (originally, \emph{well-calibrated}) if

\begin{equation}\label{eq:cw_cal}
    \mathbb{P}(y=j\mid a_j=p) = p \quad \forall j\in\mathcal{Y}, \quad \forall p\in \left[0, 1\right]
\end{equation}
% то есть мы ожидаем, что уверенности, выдаваемые для каждого класса (а не только предсказанного), являются откалиброванными.
that means the assurances given for each class (not just the predicted one) are calibrated.

% В случае реальных данных и моделей мы не можем напрямую проверить \eqref{eq:perfect_cal_guo} и \eqref{eq:cw_cal}, поэтому на помощь приходят различные показатели качества, а также визуализации, которые будут рассмотрены в \autoref{sec:estimate}.

In the case of real-world data and models, we cannot directly check \eqref{eq:perfect_cal_guo} and \eqref{eq:cw_cal}, so various calibration metrics come in handy as well as visualizations that are reviewed in \autoref{sec:estimate}.

% В \autoref{sec:methods} описываются методы, с помощью которых получаются откалиброванные модели. Во-первых, можно \emph{откалибровать} уверенности, то есть найти функцию, отображающую смещенные оценки в откалиброванные. Поиск наилучшего отображения достаточно нетривиален. Во-вторых, можно применить различные техники на этапе обучения, среди которых выделяются специальные модификации функции потерь.

\hyperref[sec:methods]{Section 4} describes calibration methods~--- techniques that make confidences more reliable. First, one can calibrate confidences afterwards, i.e. find a transformation that maps biased estimates to calibrated. There are different algorithms that find optimal transformations. Second, it is possible to apply special techniques during training, for example, loss function modifications.

% В \autoref{sec:experiments} проводится сравнение реализованных методов калибровки для современных архитектур нейронных сетей, а также показывается, как выбор функции потерь повлиять на откалиброванность.

\hyperref[sec:experiments]{Section 5} provides empirical comparison of calibration methods for modern neural network architectures and shows how the choice of loss function affects the calibration.

\section{Calibration Evaluation}\label{sec:estimate}
\subsection{Visualization}

% Покажем, как можно оценить откалиброванность модели в реальных задачах. Для начала упростим задачу до \emph{бинарной классификации}: $\mathcal{Y}=\{0,1\}$ -- пусть наша модель выдает \emph{уверенности} $\hat{p}$ в том, что объект принадлежит положительному классу (под \emph{положительным} понимается $y=1$). Бинарная классификация чаще встречается при использовании ``классическиx'' алгоритмов машинного обучения: логистическая регрессия, решающий лес, градиентный бустинг над деревьями, наивный байесовский классификатор, метод опорных векторов и другие -- проблемы их калибровки подробно рассматривались в \cite{good_proba, emp_comparison}.

Before defining calibration measures, we simplify the problem to binary classification: $\mathcal{Y}=\{0,1\}$~--- consider the model generates confidences $\hat{p}$ that the object belongs to the positive class \textit{($y=1$)}. Binary classification is common in various applications and ``classical'' machine learning models: logistic regression, support vector machines, gradient boosted trees and others~--- the problem of their calibration were studied in \cite{good_proba, emp_comparison}.

% \mpl{rel_intro}{Варианты визуализации надежности алгоритма. Для наглядности были сгенерированы синтетические данные, в качестве модели использован метод опорных векторов (расстояния до разделяющей гиперплоскости отмасштабированы на $[0, 1]$).}

\mpl{rel_intro}{Options for visualizing confidence calibration. For clarity, synthetic data was generated. Support vector machine is used as a model (the distances to the separating hyperplane are scaled into $[0, 1]$).}

% Разобъем множество значений уверенностей $[0, 1]$ на $M$ интервалов $I_m$ равной ширины:

Consider a set of objects \textit{(usually, another validation set)}, for each of which the true class is known and the confidence score is produced by the model. Divide the segment of all possible confidence values $[0, 1]$ into $M$ equal-width intervals $I_m$:

\begin{equation}\label{eq:binning}
I_1= \left[0, \frac{1}{M}\right),\
I_2= \left[\frac{1}{M},\frac{2}{M}\right),\
\dots,\
I_{M-1}= \left[\frac{M-2}{M},\frac{M-1}{M}\right),\
I_{M} = \left[\frac{M-1}{M}, 1\right].
\end{equation}

% Обозначим $B_m$ множество индексов тех объектов выборки, значение уверенности для которых лежит в пределах $I_m$. Будем взаимозаменяемо называть $B_m$ и соответствующие им интервалы $I_m$ \emph{бинами} (\engterm{bins}).

Thus, each confidence estimate falls into one of these intervals~--- let $B_m$ denote the set of indices of those objects which confidences are in $I_m$. Both $B_m$ and $I_m$ are called \emph{bins}.

% В каждом бине $B_m$ посчитаем долю объектов положительного класса $A^1_m$ (\engterm{positive frequency}) и среднюю уверенность $C^1_m$(\engterm{confidence}) в том, что объект принадлежит положительному классу:

For each bin $B_m$, we calculate positive frequency $A^1_m$ and average confidence $C^1_m$:

\begin{equation}\label{eq:bin_accconf}
    A^1_m=\frac{1}{|B_m|}\sum_{i\in B_m} \mathbb{1}(y_i=1),
    \quad
    C^1_m=\frac{1}{|B_m|}\sum_{i\in B_m} \hat{p}_i.
\end{equation}

% Далее построим график $(C^1_m, A^1_m)_{m=1}^M$, который называются \emph{графиком надежности} \cite{reldiag_idea, good_proba} (\engterm{reliability plot/diagram}) -- \autoref{fig:rel_intro} (a). Также полученную кривую иногда называют \emph{калибровочной кривой} (\engterm{calibration curve}). Хорошей откалиброванности соответствует кривая, близкая к диагональной. 

Finally, we can draw a plot $(C^1_m, A^1_m)_{m=1}^M$ which is called \emph{reliability plot} \autoref{fig:rel_intro} (a). Also, the resulting curve is sometimes called calibration curve. The model is considered well-calibrated if its calibration curve is close to the diagonal \textit{(in \autoref{subsec:calmetrics} we describe scalar metrics of such closeness)}.

% Можно изобразить полученные оценки с помощью гистограммы, которую называют \emph{диаграммой надежности}: на \autoref{fig:rel_intro} (b) красным показывается средняя уверенность, синим -- доля объектов положительного класса, попавших в бин. Если красный столбец выше синего, то алгоритм выдает недостаточно уверенные оценки (\engterm{underconfidence}), если синий выше красного -- слишком большие (\engterm{overconfidence}). Дополнительно на том же графике мы покажем зеленым \emph{вес} бина (\engterm{weight}) -- долю объектов (всех классов), попавших в бин. 

Likewise, $(C^1_m, A^1_m)_{m=1}^M$ can be depicted using a histogram, which is called \emph{reliability diagram}. In \autoref{fig:rel_intro} (b) the average confidence is shown in red, and the positive frequency is shown in blue. If the red bar is higher than the blue one, then the algorithm underestimates confidences~--- \emph{underconfidence}. The opposite case is called \emph{overconfidence}. For better interpretation, the fraction of objects (bin \emph{weight}) that fell into the bin can also be shown in the graph.

% В случае, когда классов $n>2$, диаграммы надежности строятся иначе. Наиболее популярный подход соответствует пониманию откалиброванности в смысле \eqref{eq:perfect_cal_guo}. Для каждого бина $B_m$ оценивается \emph{точность} (\emph{доля правильных ответов}, \engterm{accuracy}) $A_m$ и средняя уверенность в предсказании $C_m$:

When the number of classes $n>2$, reliability diagrams are built differently. The most popular approach corresponds to the definition \eqref{eq:perfect_cal_guo}. For each bin $B_m$, we estimate ``accuracy'' $A_m$ and average confidence $C_m$:

\begin{equation}\label{eq:accconf}
    A_m=\frac{1}{|B_m|}\sum_{i\in B_m} \mathbb{1}(y_i=\hat{y}_i),
    \quad
    C_m=\frac{1}{|B_m|}\sum_{i\in B_m} \hat{p}_i.
\end{equation}

% \eqref{eq:bin_accconf} и \eqref{eq:accconf} отличаются тем, что в многоклассовом случае $\hat{y}_i$ и $\hat{p}_i$ соответствуют предсказанному классу и уверенности, в то время как в бинарном варианте все считается для положительного класса.

The difference between \eqref{eq:bin_accconf} and \eqref{eq:accconf} is that in multiclass case $\hat{y}_i$ and $\hat{p}_i$ correspond to the estimated class and its confidence, while in binary case statistics are calculated only for a positive class.

% Заметим, что $A_m$ и $C_m$ оценивают соответственно левую и правую части \eqref{eq:perfect_cal_guo}. Их можно изобразить на диаграмме надежности. Для двух классов такой подход проиллюстрирован на \autoref{fig:rel_intro} (c) -- бины с границами $<0.5$ оказываются пустыми, поскольку в бинарной классификации алгоритм относит объект к классу, уверенность в котором $>0.5$.

Note that $A_m$ and $C_m$ estimate the left and the right of \eqref{eq:perfect_cal_guo}, respectively. They can be depicted on reliability diagram. For two classes, this approach is illustrated in \autoref{fig:rel_intro} (c). Bins to the left of 0.5 turn out to be empty because a binary classification algorithm assigns an object to a class that has confidence $>0.5$.

% В \cite{dirichlet} также рассматриваются \emph{поклассовые диаграммы надежности} (\engterm{classwise-reliability diagrams}): для этого мы каждый класс по отдельности объявляем положительным и строим $n$ диаграмм надежности для бинарного случая. И хотя поклассовый подход более полный \eqref{eq:cw_cal}, для большого числа классов (например, 1000 в датасете Imagenet \cite{imagenet}) строить так много графиков будет затруднительно. Поэтому почти всегда используются диаграммы надежности для предсказанных классов \eqref{eq:accconf}.

One can also consider classwise reliability diagrams \cite{dirichlet}. To make it, each class should be separately assigned to the positive, while all the others should be treated like the negative, so $n$ reliability diagrams for the binary case can be built. Although the classwise approach is more informative \eqref{eq:cw_cal}, when the number of classes is large (for example, 1'000 or 22'000 in ImageNet \cite{imagenet}), it is usually impractical due to the interpretation difficulty. Therefore, reliability diagrams considering only the prediction confidence \eqref{eq:accconf} are more common.

\subsection{Calibration Metrics}\label{subsec:calmetrics}

% Кроме визуализаций, оценить откалиброванность модели помогают различные \emph{метрики} (под \emph{метрикой} в данной работе понимается показатель качества). Одна из наиболее популярных --- ECE (\engterm{Expected Calibration Error} \cite{bayesian_ece}). Она приближает

In addition to visualizations, various metrics\footnote{Here, \emph{metric} refers to measure for the evaluation of algorithms} can be used to evaluate the calibration of the model. One of the most common is Expected Calibration Error (ECE) \cite{bayesian_ece}. It estimates the expectation of the absolute difference between confidence and associated accuracy:

\begin{equation}\label{eq:ece_exp}
    \mathbb{E}_{\hat{p}}\left|
\mathbb{P}\left(y=\hat{y}\mid \hat{p}\right)-\hat{p}\right|.
\end{equation}

% с помощью разделения уверенностей по бинам ($l$ -- общее число объектов):
\eqref{eq:ece_exp} can be approximated using the partition of the confidences into bins ($l$ is the total number of objects in the considered set):
\begin{align}\label{eq:ece}
\ece &=
\sum_{m=1}^{M}
\frac{|B_m|}{n}
\left| A_m - C_m \right| \\
&=
\sum_{m=1}^{M}
\frac{|B_m|}{n}\left|
\frac{1}{|B_m|}\sum_{i\in B_m} \mathbb{1}(y_i=\hat{y}_i)
-
\frac{1}{|B_m|}\sum_{i\in B_m} \hat{p}_i
\right| \nonumber\\
&=
\frac{1}{n}\,\sum_{m=1}^{M}\,
\left|\,
\sum_{i\in B_m} \mathbb{1}(y_i=\hat{y}_i)
-
\sum_{i\in B_m} \hat{p}_i
\,\right|\nonumber.
\end{align}

% Сравнивая \eqref{eq:ece} и диаграммы надежности для многоклассовой задачи, замечаем, что $\ece$ в точности равна взвешенному среднему длин отрезков между красными и синими столбцами.

Comparing \eqref{eq:ece} and reliability diagrams for a multiclass problem, we notice that ECE is exactly equal to the weighted average of the gaps between red and blue bars \autoref{fig:rel_intro}.

% Существуют и другие метрики на основе разбиения уверенностей по бинам, хоть и используются значительно реже. Например, можно посчитать длину максимального разрыва между уверенностью и точностью \cite{bayesian_ece}:
There are other metrics based on a partition of confidences into bins, although used less often. For example, one can calculate the maximum gap between confidence and accuracy \cite{bayesian_ece}:
\begin{equation}
    \mce=\max_m \left|A_m - C_m\right|,
\end{equation}
% или же учитывать уверенности не только за предсказанный класс, но и за все остальные (\engterm{classwise ECE}) \cite{dirichlet}:
or take into account the confidence not only for the predicted class but also for all the others (classwise ECE) \cite{dirichlet}:
\begin{equation}\label{eq:cwece}
    \cwece=\frac{1}{K}
\sum_{j=1}^K \sum_{m=1}^M
\frac{|B^j_m|}{n}|A^j_m - C^j_m|,
\end{equation}
% где $B^j_m, A^j_m, C^j_m$ -- соответственно $m$-й бин, точность и уверенность, если мы выделяем $j$-й класс как положительный, а все остальные собираем в отрицательный (то есть метрика соответствует поклассовым диаграммам надежности).
where $B^j_m, A^j_m, C^j_m$ are, respectively, $m$-th bin, accuracy and confidence, if we consider $j$-th class positive, and collect all the others into negative. This metric corresponds to classwise reliability diagrams.

% Вместо равноширинных бинов \eqref{eq:binning} можно использовать равномощные бины --- иногда таким образом строят диаграммы уверенности. В \cite{ace} предлагалось с помощью равномощных бинов считать описанные ранее метрики. Далее везде будет использоваться равноширинная схема. Также, кроме $l_1$-нормы (т.е. усреднения модулей), можно использовать $l_2$ (брать среднеквадратическое) \cite{verified_uncertainty}.

Instead of equal-width bins \eqref{eq:binning}, bins with an equal number of samples can be used --- sometimes confidence diagrams are built in this way. In \cite{ace}, it was proposed to use equal-frequency bins to count the metrics described above. Further, an equal-width scheme will be considered. Also, in addition to $l_1$-norm (i.e. averaging modules), we can use $l_2$ \cite{verified_uncertainty}.

% Помимо биннинговых метрик, для оценки откалиброванности модели можно использовать \emph{скоринговые функции ошибки} (\engterm{proper scoring rules}). Мы будем измерять NLL~(\engterm{Negative Log-Likelihood}):

The problem with binning metrics is the dependence on the number of bins. An alternative approach is to use proper scoring rules. We consider Negative Log-Likelihood (NLL):

\begin{equation}\label{eq:nll}
    \nll=-\frac{1}{l}\sum_{i=1}^n \log{a_{i, y_i}}
\end{equation}
% где $y_i$ -- истинная метка класса $i$-го объекта, $a_{i, y_i}$~-- уверенность алгоритма в ней, $n$~-- общее число объектов, $K$~-- число классов.
where $y_i$ is the true class label, $a_{i, y_i}$ is a confidence of the algorithm in it, $n$ is a total number of objects.

% Другая скоринговая функция ошибки, с помощью которой можно оценить откалиброванность модели~--- \engterm{Brier Score}:
Another proper scoring rule, which can be used to evaluate model calibration, is Brier Score:
\begin{equation}
    \bs = \frac{1}{n}\sum_{i=1}^n \sum_{j=1}^K
    \left(a_{ij} - \mathbb{1}(y_i = j)\right)^2,
\end{equation}
where $K$ is a number of classes.

\section{Calibration Methods}\label{sec:methods}

% Методы калибровки можно разделить на две основные группы. Во-первых, можно сделать \emph{постобработку} (\engterm{post-hoc calibration methods}) выходов модели. Для этого используется \emph{функция деформации} (\engterm{calibration map}) — отображение, заменяющее смещенные оценки вероятности на откалиброванные. Ко второй группе относят методы, применяющиеся на этапе обучения модели.

There are two main types of calibration techniques. First, model outputs can be post-processed. Special transformation, \emph{calibration map}, maps biased probability estimates to the calibrated ones. Second, calibration can be incorporated into the model training itself.

\subsection{Post-processing}
% Поиск функции деформации выполняется на \emph{отложенной выборке} $(x_i,y_i)_{i=1}^{n}$. Обычно используется тот же набор данных, на котором валидируется модель и подбираются гиперпараметры.

The transformation is usually found on a hold-out set \textit{(calibration set)} $(x_i,y_i)_{i=1}^{n}$. It can be the same dataset used for hyperparameter tuning, but not the training set because model outputs distribution on training data is not the same as on unseen data.

% \mpl{calibs_binary}{Визуализация различных функций деформации для бинарной классификации (те же данные и модель, что и на \autoref{fig:rel_intro}).}

\mpl{calibs_binary}{Different calibration maps for binary classification (the same data and model as in \autoref{fig:rel_intro}).}

\subsubsection{Histogram Binning}
% Изначально метод был предложен в \cite{hist_binning} для калибровки решающих деревьев и наивного байесовского классификатора. Рассмотрим бинарный случай: ищется кусочно-постоянная функция деформации. А именно, множество значений выходных уверенностей разбивается на бины $B_1,\dots,B_M$ (обычно равноширинные \eqref{eq:binning} или равномощные) и оценки, попавшие в $B_m$, заменяются на общую для данного бина $\theta_m$. Чтобы найти $\theta_1,\dots,\theta_M$, решается следующая задача оптимизации:

The method was originally introduced in \cite{hist_binning} to calibrate decision trees and naive Bayes classifiers. A calibration map in histogram binning is piecewise constant. Consider the binary case: the set of output confidence values is divided into bins $B_1,\dots,B_M$  \emph{(usually equal-width \eqref{eq:binning} or equal-frequency)}, and scores that fall into $B_m$ are replaced with the common $\theta_m$. To find $\theta_1,\dots,\theta_M$, the following optimization problem is solved:

\begin{equation}\label{eq:hist_binning}
\sum_{m=1}^{M}\sum_{i\in B_m}
\left(\theta_m - y_i\right)^2 \ \to \
\min_{\theta_1,\dots, \theta_M}.
\end{equation}

% В такой постановке $\theta_m$ будет равна доле объектов отложенной выборки положительного класса, попавших в бин $B_m$. Функция деформации проиллюстрирована на \autoref{fig:calibs_binary} (a). 

In such a statement the optimal $\theta_m$ equals the fraction of positive objects that fall into $B_m$. The calibration map is illustrated in \autoref{fig:calibs_binary} (a).

% Метод обобщается на многоклассовый случай с помощью стратегии \emph{один-против-всех} (\engterm{one-vs-rest}): каждый класс по отдельности объявляется положительным и строится $K$ кусочно-постоянных функций деформации. На этапе применения выходной вектор вероятностей нормализуется.

The method is generalized to the multiclass case using the strategy \emph{one-vs-rest}: each class is separately declared positive and $K$ piecewise constant functions are constructed. In the inference stage, a calibrated vector is additionally normalized.

\subsubsection{Isotonic Regression}

% Метод предложен в \cite{isotonic}. Для бинарного случая по отложенной выборке тоже ищется кусочно-постоянная функция деформации, но число интервалов $M$ и их границы оптимизируются, а на саму функцию дополнительно накладывается требование неубывания. Таким образом, решается следующая задача:

The method was proposed in \cite{isotonic}. For the binary case, the map is piecewise constant again, but both the number of the intervals $M$ and their boundaries are optimized. The constraint is that the map should be non-decreasing. Thus, the following problem is solved:

\begin{equation}\label{eq:isotonic}
\sum_{m=1}^{M}\sum_{i\in \tilde{B}_m}
\left(\theta_m - y_i\right)^2 \ \to \
\underset{\substack{
    M \\
    \theta_1 \leqslant \dots \leqslant \theta_M \\
    0=\alpha_0 \leqslant \alpha_1 \leqslant \dots \leqslant \alpha_{M-1} \leqslant \alpha_M = 1
}}{\min,}
\end{equation}
% где
where
$\tilde{B}_1 =\{i: \alpha_0 \leqslant \hat{p}_i < \alpha_1\},
\dots,
\tilde{B}_{m} =\{i: \alpha_{m-1} \leqslant \hat{p}_i \leqslant \alpha_m\}$.
% Вид функции проиллюстрирован на \autoref{fig:calibs_binary}.
The shape of the function is illustrated in \autoref{fig:calibs_binary}.

% Изотоническая регрессия обобщается на многоклассовый случай так же, как и гистограммный биннинг.

Isotonic regression is generalized to the multiclass case in the same way as histogram binning.

\subsubsection{Generalizations of Platt Calibration}

% Изначально метод предложен в \cite{platt} для калибровки метода опорных векторов. Как видно на иллюстрациях \autoref{fig:rel_intro}, \autoref{fig:calibs_binary}, если мы отшкалируем расстояния $r(x)$ от объектов до разделяющей гиперплоскости на $[0, 1]$ и возьмем их в качестве уверенностей в положительном классе, то график надежности будет иметь форму сигмоиды:

Originally, the method was proposed in \cite{platt} for calibration of the support vector machines. As can be seen in the illustrations \autoref{fig:rel_intro}, \autoref{fig:calibs_binary}, if we rescale the distances $r(x)$ from the objects to the separating hyperplane into $[0, 1]$ and treat them as confidences in positive class, then the reliability plot will have the form of a sigmoid:

\begin{equation}
    \hat{p}(x) = \frac{1}{1+e^{-(\alpha \cdot r(x) + \beta)}}.
\end{equation}

% Коэффициенты масштаба $\alpha$ и сдвига $\beta$ оптимизируются на отложенной выборке с помощью метода максимального правдоподобия. В данном методе функция деформации оказывается непрерывной и допускает различные обобщения на многоклассовую задачу.

Scale parameter $\alpha$ and location parameter $\beta$ are optimized on a calibration set using maximum likelihood estimation. In this method, the transformation is continuous and allows different generalizations to the multiclass case.

% Последний линейный слой нейронной сети для объекта $x$ выдает вектор \emph{логитов} (\engterm{logits}): $\vec{z} = (z_1,\dots,z_K)$. Чтобы оценить вероятности классов, вектор логитов пропускают через \engterm{softmax}, $\softmax(\cdot)$:

The last linear layer of a neural network for the object $x$ outputs the logit vector: $\vec{z} = (z_1,\dots,z_K)$. To estimate class probabilities, the vector is transformed with softmax $\softmax(\cdot)$:

\begin{equation*}
    \softmax\left(\vec{z}\right)=
    \frac{1}{\sum_{j=1}^{K} {\exp{\left(z_j\right)}}}
    \left(
        {\exp{\left(z_1\right)}},
        \dots,
        {\exp{\left(z_K\right)}}
    \right),
\end{equation*}
% тогда обобщить калибровку Платта можно введением параметров масштаба и сдвига для логитов:
so it is possible to generalize Platt Calibration by introducing scale and location parameters for logits:
\begin{equation}
    a(x) = \softmax(\vec{W}\cdot \vec{z} + \vec{b}).
\end{equation}

% Параметры $\vec{W}$ и $\vec{b}$ также оптимизируются с помощью метода максимального правдоподобия на отложенной выборке, что эквивалентно минимизации NLL \eqref{eq:nll}. В зависимости от размерности $\vec{W}$ и $\vec{b}$, можно получить разные обобщения:

Parameters $\vec{W}$ and $\vec{b}$ are also optimized with maximum likelihood estimation on a calibration set, which is equivalent to minimizing NLL \eqref{eq:nll}. Depending on $\vec{W}$ and $\vec{b}$ shapes, different generalization may be defined:

\begin{enumerate}
    \item Temperature scaling:
    $$\vec{W}=\frac{1}{T}\in\mathbb{R}, \ T>0, \ \vec{b}=\vec{0}.$$
    
    % Обобщение калибровки Платта с единственным скалярным параметром. Метод является одним из наиболее часто используемых. Увеличение температуры $T$ приводит к увеличению неопределенности --- росту энтропии выходного распределения. Уменьшение, напротив, увеличивает уверенность в предсказанном классе. При этом сама классификация остается неизменной.

    Generalization of Platt Calibration with a single scalar parameter. The method is one of the most frequently used. An increase in temperature $T$ leads to an increase in uncertainty --- an increase in the entropy of the output distribution. Decreasing $T$, on the contrary, increases confidence in the predicted class. At the same time, the predicted class remains unchanged.

    \item Vector scaling:
    $$\vec{W}=\operatorname{diag}(\vec{v})\in \mathbb{R}^{K\times K}\text{ --- diagonal matrix}, \vec{v}\in\mathbb{R}^K.$$

    % В данном подходе для каждого класса оптимизируется свой коэффициент масштаба (и сдвига, если $\vec{b} \neq \vec{0}$ тоже оптимизируется).

    In this approach, a different scale factor is optimized for each class (and the bias, if $\vec{b}\neq \vec{0}$ is optimized too).

    \item Matrix scaling:
    $$\vec{W}\in \mathbb{R}^{K\times K}, \vec{b}\in\mathbb{R}^K.$$

    % Матричное шкалирование является наиболее общей параметризацией в данной группе методов и эквивалентно логистической регрессии в пространстве логитов. Тем не менее при большом числе классов метод имеет слишком много параметров, что может привести к переобучению, а также проблемам со сходимостью.

    Matrix scaling is the most general parametrization in this group of methods and is equivalent to logistic regression in the logit space. However, with a large number of classes, the method has too many parameters, which can lead to overfitting, because a calibration set is usually not large.

\end{enumerate}
% Заметим, что для реализации любого из перечисленных вариантов достаточно добавить к обученной нейросети линейный слой (нужной размерности).

Note that to implement any of these methods, it is enough to add a linear layer (of the required dimension) to the frozen neural network.

\subsection{Calibration During Training}
% Качество работы нейронных сетей сильно зависит от функции потерь, на которую они настраиваются. Чаще всего используется NLL \eqref{eq:nll}. Для одного объекта $x$ она равна кросс-энтропии между истинным вектором классификации $\vec{y}$ и предсказанным распределением $\vec{a}$:
The performance of neural networks strongly depends on the loss function. Usually, NLL \eqref{eq:nll} is used. Given an object $x$, it is equal to the cross-entropy between the true classification one-hot distribution $\vec{y}$ and the predicted distribution:
\begin{equation}
    \operatorname{CE}(\vec{y}, \vec{a}) = -\sum_{j=1}^{K} y_j \log{a_j}.
\end{equation}

% Чтобы повысить откалиброванность модели, можно модифицировать саму функцию потерь.

To improve the calibration of the model, one can modify the loss function itself.

\subsubsection{Label Smoothing}

% В данном методе вырожденное распределение вектора классификации подменяется более сглаженным. Сила сглаживания регулируется с помощью параметра $\alpha \in [0, 1]$:

In this method, the degenerate distribution of the target is replaced by a smoothed one. The smoothing degree can be tuned using the parameter $\alpha\in [0, 1]$:

\begin{equation}
    \vec{y}=\left(y_1,\dots,y_K\right)
    \mapsto
    \left(
        (1-\alpha)y_1 + \frac{\alpha}{K},
        \dots,
        (1-\alpha)y_K + \frac{\alpha}{K}
    \right)=\vec{y}'.
\end{equation}

% С ростом $\alpha$ распределение $\vec{y}'$ становится более равномерным. После данного преобразования минимизируется кросс-энтропия $\operatorname{CE}(\vec{y}',\vec{a})$ между сглаженным вектором классификации и предсказанным распределением.

As parameter $\alpha$ increases, the distribution of $\vec{y}'$ becomes more uniform. After this transformation, $\operatorname{CE}(\vec{y}',\vec{a})$ the cross-entropy between the smoothed classification vector and the predicted distribution is minimized.

% Хотя использование сглаженных меток при обучении классификатора не новая идея, для калибровки такой подход был предложен в \cite{smoothing}.

Although using of smoothed labels to train a classifier is not a new idea, this approach was proposed for calibration in \cite{smoothing}.

\subsubsection{Focal Loss}

\begin{figure}[!h]
    \includegraphics[width=0.7\textwidth]{focal_loss}
    \centering
    \caption{Focal loss on a single object. $\hat{p}$ --- probability estimation for a true class}
    \label{fig:focal_loss}
\end{figure}

% Изначально фокальная ошибка была использована для устранения проблемы дисбаланса классов \cite{focal_detection}. С точки зрения калибровки уверенности идею впервые использовали в \cite{focal_calib}. Для объекта, принадлежащего $j$-му классу, фокальная ошибка имеет следующий вид:

The focal loss was originally introduced to tackle the problem of class imbalance \cite{focal_detection}. In terms of confidence calibration, the idea was first used in \cite{focal_calib}. For an object belonging to the $j$-th class, the focal loss has the following definition:
\begin{equation}
    \operatorname{FL}=-(1-a_{j})^{\gamma}\cdot \log a_j, \quad \gamma \geqslant 0.
\end{equation}
% причем функция потерь совпадает с кросс-энтропией при $\gamma=0$. С увеличением $\gamma$, как видим на \autoref{fig:focal_loss}, уменьшается штраф за потери на объектах с уже высокой уверенностью в истинном классе. В то время как кросс-энтропия является верхней оценкой дивергенции Кульбака --- Лейблера между истинным $\vec{y}$ и предсказанным $\vec{a}$ распределением, у фокальной ошибки из оценки вычитается энтропия предсказанного распределения $H(\vec{a})$ \cite{focal_calib}:

Note that the loss function becomes the cross-entropy when $\gamma=0$. Increasing parameter $\gamma$, as we see in \autoref{fig:focal_loss}, decreases the penalty for those objects with already high confidence in the true class . While the cross-entropy is the upper bound of the Kullback--Leibler divergence between the true $\vec{y}$ and the predicted $\vec{a}$ distribution, the focal error has the entropy of the predicted distribution $H(\vec{a})$\cite{focal_calib} subtracted from the estimate:

\begin{equation*}
    \operatorname{CE}(\vec{y},\vec{a})
    \geqslant
    \operatorname{KL}(\vec{y}||\vec{a}),
    \qquad
    \operatorname{FL}(\vec{y},\vec{a})
    \geqslant
    \operatorname{KL}(\vec{y}||\vec{a})-\gamma\cdot \operatorname{H}(\vec{a}).
\end{equation*}

% Получается, оптимизация фокальной ошибки дополнительно увеличивает энтропию предсказанного распределения, то есть помогает в борьбе с переуверенностью.

Thus, optimizing the focal error additionally increases the entropy of the predicted distribution and helps to tackle overconfidence.

\section{Empirical Experiments}\label{sec:experiments}

\subsection{Experimental Design}

% В экспериментах были использованы следующие наборы данных:

The following datasets are used in the experiments:

\begin{itemize}
    % \item \textbf{CIFAR-10} \cite{cifar}: Датасет содержит $60\,000$ цветных изображений $32\times 32$, каждое относится к одному из 10 классов. Разделение на \emph{обучающую} / \emph{валидационную} / \emph{тестовую} выборки: $50\,000\;/\;5\,000\;/\;5\,000$ изображений.

    \item \textbf{CIFAR-10} \cite{cifar}: The dataset consists of $60\,000$ color images $32\times 32$. \emph{Training} / \emph{validation} / \emph{test} splits are respectively $50\,000\;/\;5\,000\;/\;5\,000$.

    % \item \textbf{CIFAR-100} \cite{cifar}: $60\,000$ цветных изображений $32\times 32$, 100 классов. \emph{Обучение} / \emph{валидация} / \emph{тест}: $50\,000\;/\;5\,000\;/\;5\,000$.

    \item \textbf{CIFAR-100} \cite{cifar}: $60\,000$ color images $32\times 32$, 100 classes. \emph{Training} / \emph{validation} / \emph{test}: $50\,000\;/\;5\,000\;/\;5\,000$.

    % \item \textbf{ImageNet 2012} \cite{imagenet}: Крупный датасет с изображениями, разбитыми на 1000 классов. \emph{Обучение} / \emph{валидация} / \emph{тест}: $1.2\;\text{млн}\;/\;25\,000\;/\;25\,000$.

    \item \textbf{ImageNet 2012} \cite{imagenet}: Large dataset with color images organized into 1000 classes. \emph{Training} / \emph{validation} / \emph{test}: $1.2\;\text{m}\;/\;25\,000\;/\;25\,000$.

    % \item \textbf{Tiny ImageNet} \cite{imagenet}: $110\,000$ изображений $64\times 64$, разделенных на 200 классов. Является подмножеством предыдущего датасета. \emph{Обучение} / \emph{валидация} / \emph{тест}: $100\,000\;/\;5\,000\;/\;5\,000$.

    \item \textbf{Tiny ImageNet} \cite{imagenet}: $110\,000$ color images $64\times 64$, organized into 200 classes, subset of the previous dataset. \emph{Training} / \emph{validation} / \emph{test}: $100\,000\;/\;5\,000\;/\;5\,000$.
\end{itemize}

% Для вычислений использовались предобученные нейронные сети с различными архитектурами из открытых репозиториев. В экспериментах модели и датасеты разбиты на две основные группы:

Pre-trained neural networks with various architectures from open repositories were used for calculations. In experiments, models and datasets are divided into two main groups:

\begin{enumerate}
    % \item К первой группе отнесены нейронные сети, обученные на CIFAR-10, CIFAR-100, ImageNet. Веса для моделей были взяты соответственно из репозиториев \cite{pretrained_cifar10, pretrained_cifar100, pretrained_imagenet}. Модели данной группы используются для сравнения методов калибровки, основанных на постобработке.

    \item The first group includes neural networks trained on CIFAR-10, CIFAR-100, and ImageNet. The weights for the models are obtained respectively from the repositories \cite{pretrained_cifar10, pretrained_cifar100, pretrained_imagenet}. Models from this group are used to compare calibration methods based on post-processing.

    % \item Ко второй группе отнесены предобученные нейросети из репозитория \cite{focal_github}. Здесь использованы датасеты CIFAR-10, CIFAR-100 и Tiny ImageNet. Данные нейросети были обучены для статьи \cite{focal_calib}~--- для части из них использовалась фокальная ошибка и сглаживание меток.

    \item The second group includes pre-trained neural networks from the repository \cite{focal_github}. The datasets used here are CIFAR-10, CIFAR-100, and Tiny ImageNet. These neural networks were trained in \cite{focal_calib}~--- focal error and label smoothing were used for the considered models.
\end{enumerate}

% При обучении модели обучались на данных из \emph{обучающей} выборки (или ее части), калибровались на \emph{отложенной (валидационной)} выборке. Все построенные диаграммы надежности и метрики соответствуют \emph{тестовой} выборке.

The models were trained on the \emph{training} data \textit{(or its subset)}, calibrated on the \emph{validation} set~--- all diagrams and metrics correspond to the \emph{test} set.

% Все эксперименты, реализация методов калибровки и оценок были выполнены на языке Python. Температурное, векторное и матричное шкалирование настраивались на GPU и были реализованы с использованием библиотеки PyTorch, остальные методы и метрики реализованы с использованием библиотек SciPy и sklearn. При настройке гистограммного биннинга использовалось 20 бинов; ECE, cwECE и MCE считались с разбиением на 15 бинов.

Code used in all experiments is published in \cite{my_repo}: temperature, vector, and matrix scaling are implemented using PyTorch, and other methods and evaluation are implemented with SciPY and scikit-learn. Histogram binning use 20 bins; ECE, cwECE, and MCE are calculated on 15 bins partition; reliability diagrams use 10 bins.

\subsection{Experiment Results}

% Полные таблицы с измерениями приведены в 
% \hyperref[sec:appendix]{приложении} к работе, диаграммы надежности для всех рассмотренных моделей можно найти в репозитории \cite{my_repo}.

Complete tables with measurements are given in 
\hyperref[sec:appendix]{appendix}, additional reliability diagrams for all the models considered in this work can be found in the accompanying repository \cite{my_repo}.

% \addrel{1}{21}{}
\begin{figure}[!h]
    \includegraphics[width=\textwidth]{reldiag_1_21}
    \centering
    \caption{CIFAR-100, ShuffleNetV2\_x0\_5}
    \label{fig:reldiag_1_21}
\end{figure}

% Рассмотрим диаграммы надежности для ShuffleNetV2 (CIFAR-100, \autoref{fig:reldiag_1_21}): можно видеть ``типичное'' состояние откалиброванности нейросети --- переуверенность. Калибровка помогает исправить ситуацию: в данном случае лучше с точки зрения всех метрик лучше всего сработало температурное шкалирование. Гистограммный бинниг слишком агрессивно изменяет вероятности при большом числе классов (на отложенной выборке в бинах оказывается мало объектов).

Consider the reliability diagrams for ShuffleNetV2 (CIFAR-100, \autoref{fig:reldiag_1_21}): we see a ``typical'' state of modern neural networks calibration~--- overconfidence. Calibration methods help to correct the situation: in this case, temperature scaling works best for all the metrics. However, histogram binning changes probabilities too aggressively when the number of classes is large, as can be seen from the weights change. 

% Для малого числа классов, напротив, гистограммный биннинг работает лучше всего с точки зрения уверенности в предсказании (\autoref{tab:metrics:ECE_1}, \autoref{tab:metrics:ECE_2} --- почти для всех моделей на CIFAR-10). Но отметим, что здесь нейронные сети уже с очень высоким качеством решают задачу классификации. Почти все вероятности предсказанного класса близки к 1, как, например, на \autoref{fig:reldiag_2_1}. И с точки зрения MCE (\autoref{tab:metrics:MCE_1}, \autoref{tab:metrics:MCE_2}) --- метрики, в которой не учитываются ``веса'' бинов --- гистограммный биннинг дает низкое качество откалиброванности.

For a small number of classes, on the contrary, histogram binning works best in terms of confidence in prediction (\autoref{tab:metrics:ECE_1}, \autoref{tab:metrics:ECE_2}~--- for almost all models on CIFAR-10). Note that here neural networks already solve the classification problem with very high accuracy. Almost all probabilities of the predicted class are close to 1, as, for example, on \autoref{fig:reldiag_2_1}. In terms of MCE (\autoref{tab:metrics:MCE_1}, \autoref{tab:metrics:MCE_2})~--- metric, that doesn't consider weights of bins~--- histogram binning leads to low calibration.

% \addrel{2}{1}{}
\begin{figure}[!h]
    \includegraphics[width=\textwidth]{reldiag_2_1}
    \centering
    \caption{CIFAR-10, DenseNet121 (CE)}
    \label{fig:reldiag_2_1}
\end{figure}

% Для матричного шкалирования мы не приводим диаграммы надежности: метод слишком сильно переобучается при большом числе классов. В итоге матричное шкалирование существенно ухудшает качество классификации (\autoref{tab:metrics:ACC_1}, \autoref{tab:metrics:ACC_1}) для всех датасетов, кроме, опять же, малоклассового CIFAR-10.

For matrix scaling, we do not provide reliability diagrams: the method overfits too much when the number of classes is large. As a result, matrix scaling significantly degrades the classification quality (\autoref{tab:metrics:ACC_1}, \autoref{tab:metrics:ACC_2}) for all datasets, except, again, the low-class CIFAR-10.

% Одним из наиболее популярных методов калибровки нейросетей является температурное шкалирование. Метод действительно не оказывает влияния на классификацию, в то время как другие варианты калибровки почти всегда уменьшают точность (\autoref{tab:metrics:ACC_1}, \autoref{tab:metrics:ACC_2}).

One of the most common methods of neural network calibration is temperature scaling. The method doesn't affect classification predictions, while other calibration options almost always reduce accuracy (\autoref{tab:metrics:ACC_1}, \autoref{tab:metrics:ACC_2}).

% С точки зрения NLL ожидаемо лучшими оказались температурное и векторное шкалирование (ведь в процессе калибровки именно данная ошибки и оптимизировалась) --- \autoref{tab:metrics:NLL_1}, \autoref{tab:metrics:NLL_2}. Для Brier Score лучшим методом калибровки во многих случаях становилась изотоническая регрессия --- \autoref{tab:metrics:BS_1}, \autoref{tab:metrics:BS_2}. 

In terms of NLL, temperature and vector scaling are the best as expected, because this loss was minimized during calibration: \autoref{tab:metrics:NLL_1}, \autoref{tab:metrics:NLL_2}. As for Brier Score, the best calibration method in many cases was isotonic regression: \autoref{tab:metrics:BS_1}, \autoref{tab:metrics:BS_2}.

\begin{table}[h!]
    \begin{minipage}[h!]{0.47\textwidth}
        \input{tabs/fl_ECE.tex}
    \end{minipage}\hfill
    \begin{minipage}[h!]{0.47\textwidth}
        \input{tabs/fl_cwECE.tex}
    \end{minipage}
\end{table}

% \addrel{1}{29}{}
\begin{figure}[!h]
    \includegraphics[width=\textwidth]{reldiag_1_29}
    \centering
    \caption{ImageNet, EfficientNet\_b8}
    \label{fig:reldiag_1_29}
\end{figure}

% Фокальная ошибка и сглаживание меток дают более откалибровнные модели, чем при настройке на стандартную кросс-энтропию — как с точки уверенности в предсказанном класса (\autoref{tab:fl:ECE}), так и с точки зрения поклассовых оценок (\autoref{tab:fl:cwECE}). При этом далее модели можно калибровать с помощью постобработки. В оригинальной работе \cite{focal_calib} для калибровки моделей, обученных на фокальную ошибку, использовалось только температурное шкалирование. Хотя относительно ECE (\autoref{tab:metrics:ECE_2}) такой подход действительно показывает высокие результаты, для поклассовой cwECE (\autoref{tab:metrics:cwECE_2}) лучше работает векторное шкалирование. Для моделей первой группы (\autoref{tab:metrics:cwECE_1}) векторное шкалирование тоже в основном минимизирует cwECE. Такие результаты вполне ожидаемы, поскольку векторное шкалирование находит отдельные коэффициенты деформации для каждого класса.

Focal loss and label smoothing result in better calibrated models than the ones optimizing the cross-entropy~--- both in terms of calibration in the predicted class (\autoref{tab:fl:ECE}) and classwise estimates (\autoref{tab:fl:cwECE}). At the same time, such models can be further calibrated with post-processing. In the original paper \cite{focal_calib}, temperature scaling was used to calibrate models trained with focal loss. Although this approach does show good results with respect to ECE (\autoref{tab:metrics:ECE_2}), vector scaling works better for classwise cwECE (\autoref{tab:metrics:cwECE_2}). As for models of the first group (\autoref{tab:metrics:cwECE_1}), vector scaling also basically minimizes cwECE. These results are quite expected since vector scaling finds separate coefficients for each class.

% Рассмотрим также диаграммы калибровки для EfficientNet (\autoref{fig:reldiag_1_29}). Среди всех использованных моделей, у данной нейросети четче всего видна недоуверенность --- большая часть оценок сосредоточена не на $[0.9, 1]$, а на $[0.8, 0.9)$. Причина такого поведения может быть как раз в особенности обучения: модель обучалась со сглаживанием меток ($\alpha=0.1$) \cite{pretrained_imagenet}. Все методы калибровки привели к заметному повышению уверенности в ответах.

Consider also reliability diagrams for EfficientNet (\autoref{fig:reldiag_1_29}). Among all the models used, the underconfidence is most clearly visible in this neural network: most of the predictions fall not into $[0.9, 1]$, but into $[0.8, 0.9)$. The reason for this behavior may be training setup: the model was trained with label smoothing ($\alpha=0.1$) \cite{pretrained_imagenet}. All calibration methods resulted in a noticeable increase in the confidence of the answers.

\section{Conclusion}

% Таким образом, в данной работе мы сравнили основные методы калибровки уверенности, проведя эксперименты с различными архитектурами нейронных сетей.

In this work, the main methods of confidence calibration were compared on different neural network architectures, datasets, and critera.

% Применимость того или иного метода существенно зависит от количества данных и выбранного критерия качества. Алгоритмы, в которых строятся отдельные функции деформации для каждого класса, хорошо работают только при достаточном объеме данных в отложенной выборке (обычно это можно обеспечить, когда число классов невелико). Стратегии, в основе которых лежит линейное преобразование логитов (например, температурное шкалирование), показывают высокое качество в задачах с большим числом классов, но подвержены переобучению при чрезмерной параметризации (матричное шкалирование).

The applicability of a particular method depends significantly on the amount of data and the selected evaluation criterion. Algorithms, in which separate calibration maps are found for each class, work well only if there is a sufficient amount of data in the calibration set (this can usually be provided when the number of classes is small). Strategies based on linear transformation of logits (for example, temperature scaling) show high quality in problems with a large number of classes but are subject to overfitting when parameterization is excessive (matrix scaling).

% Калибровка уверенности до сих пор остается открытой проблемой в машинном обучении, и даже выбор корректного показателя качества откалиброванности может оказаться затруднительным.

Confidence calibration is still open to further research in machine learning: as shown in this work, not only calibration methods but also criteria choice can lead to different results.

\newpage
\printbibliography[
    heading=bibintoc,
    title={References}
]

\newpage
\begin{appendices}\label{sec:appendix}
\section{Classification Accuracy}
\input{tabs/metrics_ACC_1.tex}
\input{tabs/metrics_ACC_2.tex}
\clearpage

\section{Binning-based Metrics}
\input{tabs/metrics_ECE_1.tex}
\input{tabs/metrics_ECE_2.tex}
\input{tabs/metrics_cwECE_1.tex}
\input{tabs/metrics_cwECE_2.tex}
\input{tabs/metrics_MCE_1.tex}
\input{tabs/metrics_MCE_2.tex}
\clearpage

\section{Proper Scoring Rules}
\input{tabs/metrics_NLL_1.tex}
\input{tabs/metrics_NLL_2.tex}
\input{tabs/metrics_BS_1.tex}
\input{tabs/metrics_BS_2.tex}

\end{appendices}

\end{document}