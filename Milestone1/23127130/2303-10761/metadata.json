{
    "arxiv_id": "2303.10761",
    "paper_title": "Calibration of Neural Networks",
    "authors": [
        "Ruslan Vasilev",
        "Alexander D'yakonov"
    ],
    "submission_date": "2023-03-19",
    "revised_dates": [
        "2023-03-21"
    ],
    "latest_version": 1,
    "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG",
        "stat.ML"
    ],
    "abstract": "Neural networks solving real-world problems are often required not only to make accurate predictions but also to provide a confidence level in the forecast. The calibration of a model indicates how close the estimated confidence is to the true probability. This paper presents a survey of confidence calibration problems in the context of neural networks and provides an empirical comparison of calibration methods. We analyze problem statement, calibration definitions, and different approaches to evaluation: visualizations and scalar measures that estimate whether the model is well-calibrated. We review modern calibration techniques: based on post-processing or requiring changes in training. Empirical experiments cover various datasets and models, comparing calibration methods according to different criteria.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.10761v1"
    ],
    "publication_venue": null
}