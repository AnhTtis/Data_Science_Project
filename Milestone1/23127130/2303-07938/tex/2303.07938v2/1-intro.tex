% \paragraph{Problem Defined}A major issue in visual computing is how to automatically generate new 3D contents of the highest quality that are both editable and accessible. Although generative models have demonstrated their ability to create audio and visual content, their performance still needs improvement. The depiction of 3D shapes is one of the main issues with the current approaches. Point clouds are a common topic of generative models. Converting point clouds to other shape representations, however, is complex. Another area of research uses direct learning to create implicit representations of forms, such as the neural radiance field (NeRF). However, the implicit representation must be transformed into explicit representations, such as meshes, for physical simulation applications, which still needs to be solved.

Being a fundamental representation of 3D objects in computer graphics,
meshes are widely used in applications such as VR, AR, and game,
and a generative model for meshes is thus of great value.
By representing 3D objects with vertices, edges, and faces,
meshes lead to more efficient modeling and computation of object geometries.
However, such a specialized design also results in several intrinsic challenges for generative models.
The first one is the irregular data structure of meshes,
where the discrete vertex connections make it hard to define operations like convolution and up-sampling on meshes.
Moreover,
unlike point clouds and volumes, the topology of meshes is varying across different object instances in the same category.
Therefore template-based methods \cite{wang2018pixel2mesh, wen2019pixel2mesh++, gupta2020neural, liu2021deepmetahandles, yifan2020neural, jakab2021keypointdeformer} can only obtain meshes obeying the topology of used templates, 
causing defects like self-intersection when the deformation is significant.

Facing these challenges mentioned above,
we propose to generate meshes indirectly via an intermediate representation that is easier to model.
Inspired by recent successes of deep neural networks in modeling the distribution of point clouds \cite{yang2019pointflow,Luo_2021_CVPR,cai2020learning,zhou20213d,lyu2021conditional} and reconstructing meshes from point clouds \cite{peng2021shape,gupta2020neural},
we propose to use point clouds as an intermediate representation of meshes.
Consequently,
the generation of meshes is effectively reformulated as the generation of point clouds, followed by transforming point clouds into meshes.
Such a reformulation not only enables us to take advantage of the advances of point cloud generation methods,
but also successfully bypasses the aforementioned challenges,
as the distribution of point clouds is continuous and point clouds are unordered sets without explicit topology. 
In this paper, we adopt denoising diffusion probabilistic models (DDPMs)~\cite{sohl2015deep,ho2020denoising}, demonstrated promising results in modeling point clouds~\cite{Luo_2021_CVPR,zhou20213d,lyu2021conditional}, to learn the distribution of the point clouds.
And Shape as Points (SAP)~\cite{peng2021shape} is employed to reconstruct meshes from the generated point clouds, which is a powerful surface reconstruction technique that can extract high-quality watertight meshes from point clouds at low inference times.

With the introduction of point clouds as the intermediate representation of meshes, 
we can use DDPMs to model the distribution of meshes.
However, 
to ensure the quality of transformed meshes,
the generated point clouds need to be sufficiently dense.
This inevitably leads to two issues.
At first, the overall computational complexity is high,
since sampling thousands of points from DDPMs is quite time-consuming.
It is also difficult to explicitly control the structure of meshes via dense point clouds,
as the semantics of their points are not sufficiently compact.
We therefore further encode a point cloud to a sparse set of semantic latent points with features attached to every point,
and learn a \textbf{S}parse \textbf{L}atent po\textbf{I}nt \textbf{D}iffusion mod\textbf{E}l (\textbf{SLIDE})  for mesh generation following the framework of latent diffusion models~ \cite{vahdat2021score,rombach2022high}.
Specifically, 
we train two DDPMs to learn the distribution of this latent space. The first DDPM learns the distribution of positions of the sparse latent points, and the second one learns the  distribution of the features conditioned on the positions of the points.
By cascading these two DDPMs together,
we can perform unconditional generation of sparse latent points and their features.
% To establish the correspondence between dense point clouds and sparse latent points,
We adopt farthest point sampling (FPS) to obtain the positions of sparse latent points from a dense point cloud,
and a neural encoder is deployed to attach each latent point a semantically meaningful feature.
Accordingly, a neural decoder is used to recover a dense point cloud from the positions and features of the sparse latent points and then reconstruct the mesh.
In this way, 
we maintain the quality of generated meshes while being able to control their overall structures and local details respectively via controlling configurations and semantic features of the sparse latent points,
as shown in Figure~\ref{fig:controllable_generation}.
Moreover,
we find that sampling in this sparse latent point space is significantly faster than directly sampling dense point clouds.
%We can also use the sparse latent point to control the shape of the generated point cloud using the second DDPM as shown in Figure~\ref{fig:controllable_generation}.
%It is also possible to perform both global and local interpolations in this latent space to generate new shapes.

%We observe that the difficulties of directly modeling the distribution of meshes mainly come from the their irregular data structure. 
%Therefore, we propose to first encode meshes to an intermediate representation that is easier to model, and then learn a generative model on this intermediate representation, finally, use a decoder to transform the samples in this intermediate representation back to meshes.
%Inspired by the recent success of deep neural networks in modeling the distribution of point clouds~\cite{yang2019pointflow,Luo_2021_CVPR,cai2020learning,zhou20213d,lyu2021conditional} and surface reconstruction from point clouds~\cite{peng2021shape,gupta2020neural}. 
%We propose to use point clouds as an intermediate representation of meshes.
%We sample point clouds from the surface of meshes as an intermediate representation.
%Then we use denoising diffusion probabilistic models (DDPMs)~\cite{sohl2015deep,ho2020denoising}, which have demonstrated great success in modeling point clouds~\cite{Luo_2021_CVPR,zhou20213d,lyu2021conditional}, to learn the distribution of the point clouds.
%Finally, we use Shape as Points (SAP)~\cite{peng2021shape}, which is a powerful surface reconstruction technique that can extract high-quality watertight meshes from point clouds at low inference times, to reconstruct meshes from the generated point clouds.
%The benefit of using point clouds as an intermediate representation of meshes is that the topology of the final generated mesh is not restricted. The generation results could be highly diverse.

%As mentioned above, we can use DDPMs to effectively model the distribution of point clouds, but sampling from DDPMs are quite time-consuming.
%It is also difficult to explicitly control the shape and structure of the generated point clouds from DDPMs.
%Inspired by the success of latent diffusion models~\cite{vahdat2021score,rombach2022high}, we propose to further encode a point cloud to a compact representation with semantic meanings.
%Specifically, we design a neural network encoder that can encode a point cloud to a sparse set of latent points with a feature attached to each point, and a decoder to decode the sparse latent points with features back to the input point cloud.
%The sparse latent points are obtained by farthest point sampling from the input point cloud. They control the overall structure of the point cloud.
%The features on the sparse latent points control the local details of the shape.
%We train two DDPMs to learn the distribution of the positions of the sparse latent points, and the distribution of the features conditioned on the sparse latent points, respectively.
%By cascading the two DDPMs together, we can perform unconditional point cloud generation.
%We find that sampling in this latent space is significantly faster than directly sampling for point clouds.
%We can also use the sparse latent point to control the shape of the generated point cloud using the second DDPM as shown in Figure~\ref{fig:controllable_generation}.
%It is also possible to perform both global and local interpolations in this latent space to generate new shapes.

% \textcolor{red}{(need several sentences to describe experimental results.)}
We conduct experiments on the ShapeNet~\cite{chang2015shapenet} dataset to compare both point cloud and mesh generation performance of SLIDE with other methods. 
% Our method achieves highly competitive results, and demonstrates the ability of controllable shape generation.
SLIDE achieves superior performances in terms of both visual quality and quantitative metrics.
It also demonstrates great flexibility in controlling the overall structures and local part shapes of the generated objects without using any part-annotated 3D data.
In summary, the main contributions of our work are:
\textbf{1)} We propose to use point clouds as the intermediate representation of meshes.
By generating point clouds first and then reconstructing surface from them, we can generate meshes with diverse topology and high quality.
\textbf{2)} We design a novel point cloud autoencoder to further encode point clouds to a sparse set of latent points with features attached to them.
Sampling in this latent space is more efficient than directly sampling dense point clouds.
\textbf{3)} By decomposing the learning of the positions of the sparse latent points and features of them, we can perform both unconditional point cloud generation and controllable point cloud generation based on the positions of the sparse latent points as shown in Figure~\ref{fig:controllable_generation}. 
We can also perform both global and local interpolations in this latent space.