
We present our main experiment results in this section. 
We use ShapeNet~\cite{chang2015shapenet} to train our mesh generative model and compare it with other baselines.
We use the pre-processed ShapeNet dataset provided by~\cite{peng2021shape}.
% See more details of the dataset in Appendix B.1.
% and some interesting applications based on our \LDM method. 
The detailed setups and complete experiment results are provided in Appendix B. 

% \subsection{Dataset}
% \label{sec:dataset}
% We use ShapeNet~\cite{chang2015shapenet} to train our mesh generative model and compare with other baselines.
% We use the pre-processed ShapeNet dataset provided by~\cite{peng2021shape}.
% It contains $13$ categories of objects: airplane, bench, cabinet, car,
% chair, display, lamp, loudspeaker, rifle, sofa, table, telephone, and watercraft.
% It splits the dataset into training set and validation set.
% For each shape, it provide $10000$ points sampled from the mesh surface, and the signed distance field (SDF) of the shape discretized on a $128^3$ grid.
% Most shapes in it are normalized in a unit bounding box, namely, the center of the bounding box is placed at the origin, and the maximum length of the three sides of the bounding box is scaled to $1$ such that each shape ranges from $-0.5$ to $0.5$.
% We scale the shapes in the dataset by a factor of $2$ to make them ranges from $-1$ to $1$. 
% before training our generative model and other baselines.

% To fully validate the effectiveness of our two methods, we use the most wildly used dataset, ShapeNet, to benchmark. (To be finished): dataset splits and preprocssing. We use per shape normalization when preprocess the datasets. 

\subsection{Evaluation Metrics}
To evaluate the quality of generated meshes, we uniformly sample point clouds ($2048$ points) with normals from the generated meshes and reference meshes from the validation set. 
Then we use the commonly used point cloud evaluation metrics 1-NN~\cite{yang2019pointflow}, Minimum Matching Distance (MMD) and Coverage (COV) as our main evaluation tools.
All of the metrics require a distance metric to compute the distance between two point clouds.
We use the commonly used Chamfer distance (CD) and earth mover distance (EMD).
We also use the normal consistency loss between two point clouds with normals.
We find that it can better reflect the surface curvature differences between the two underlying meshes.
Details of the normal consistency loss are described in Appendix B.3.
% We also use the normal consistency loss between two point clouds $\mX$ and $\mY$:
% \begin{align*}
%     L_{\text{normal}} = \sum_{\vx \in \mX} [1 - |\cos(\vn_x, \vn_{y^*})|] + \sum_{\vy \in \mY} [1 - |\cos(\vn_{x^*}, \vn_y^)|],
% \end{align*}
% where $\vn_x, \vn_y$ denotes the normal of the points $\vx, \vy$, and $\vy^* = \argmin_{\vy \in \mY} ||\vx-\vy||$, $\vx^* = \argmin_{\vx \in \mX} ||\vx-\vy||$.
% $L_{\text{normal}}$ can be roughly interpreted as the CD loss between the normals of two point clouds. 
% We find it can better reflect the surface curvature differences between the two underlying meshes from which the point clouds are sampled.

% \section{Surface Reconstruction Experiments}
% As mentioned in Section~\ref{sec:pointcloud_representation}, we need to train an upsampling network to upsample the point cloud ($2048$ points) generated by the generative model, then use the Differentiable Poisson Surface Reconstruction (DPSR) method to reconstruct a mesh from the upsampled dense point cloud.
% We use the improved PointNet++ proposed in~\cite{lyu2021conditional} as our upsampling network, and train it following the same procedures in~\cite{peng2021shape} across the all of the $13$ categories in the pre-processed ShapeNet dataset~\cite{peng2021shape}.
% The ground truth SDF values discritized on the $128^3$ grid are used as supervision for the upsampling network.
% The MSE loss between the reconstructed SDF values and the ground truth SDF values discritized on the $128^3$ grid is $4.56 \times 10^{-4}$ tested on the validation set.

% \subsection{Autoencoder Exps}
% We train the autoencoders described in 

% \input{Tables/point_1nn.tex}
% \input{Tables/point_mmd.tex}
% \input{Tables/point_cov.tex}
% \input{Tables/mesh_1nn.tex}
% \input{Tables/mesh_mmd.tex}    
% \input{Tables/mesh_cov.tex}

\subsection{Point Cloud and Mesh Generation}
We train our sparse latent point diffusion model on $5$ categories of the ShapeNet dataset: Airplane, cabinet, car, chair, and lamp.
And compare with baselines TreeGan~\cite{DBLP:conf/iccv/TreeGan}, 
SPGAN~\cite{li2021sp},
ShapeGF~\cite{cai2020learning}, PVD~\cite{zhou20213d}, 
DPM~\cite{luo2021diffusion}.
All the baselines are trained by ourselves using their public codebase.
We compare both the point clouds that they generate and meshes reconstructed from the point clouds using SAP.
Meshes generated by our method and baselines are shown in Figure~\ref{fig:mesh_comparison}.
More examples and generated point clouds are shown in Appendix B.6, B.7, and B.8.
We can see that our method generates meshes of the highest visual quality, with smooth surfaces and sharp details.
Since all the meshes are reconstructed from the generated point clouds using the same method, SAP.
This means the quality of the generated point clouds greatly affects the quality of the reconstructed meshes.
We provide an example of generated point clouds in Figure~\ref{fig:car_pc}. More point cloud examples are provided in Appendix B.7. 
Indeed, we can see that point clouds generated by our method spread more uniformly on the surface of the objects, and bear less noise compared with other methods. 
We attribute this to the design of our novel point cloud autoencoder.
% Compared with PVD~\cite{zhou20213d}, which directly trains DDPMs on dense point clouds, our sparse latent point diffusion model demonstrates great advantages in visual quality.
% In addition, the average generation time for a single point cloud of PVD~\cite{zhou20213d} is 13.3s tested a NVIDIA GeForce RTX 2080Ti GPU, while our sparse latent point diffusion model only need 0.18 s to generate a point cloud tested on a NVIDIA A100 GPU. Even if we count the 
% \footnote{We have not been able to run PVD on an A100 GPU. We replace the Point-Voxel CNN in it with an improved PointNet++~\cite{lyu2021conditional} and test it on an A100 GPU. The original PVD need 13.3s to generate a point cloud tested on an RTX 2080Ti GPU.}.
Quantitatively, we compute 1-NN, MMD, and COV on both generated point clouds and reconstructed meshes. Results are shown in Appendix B.4 and B.6.
% Point clouds evaluation results are in Appendix B.5.
% We find that MMDs computed by normal consistency loss best match human evaluations based on samples shown in Figure~\ref{fig:mesh_comparison}, and is shown in Table~\ref{tbl:mesh_mmd}. 1-NN and COV metrics of generated meshes are shown in Appendix B.4.
In terms of efficiency, the average generation time for a single point cloud of our method is about 0.20s (See Appendix B.10 for more details of the generation time.) tested on a single NVIDIA A100 GPU, while the DDPM-based method that directly trains generative models on dense point clouds, PVD~\cite{zhou20213d}, need 2.93s to generate a point cloud.
% \vspace{-1.5em}
% \paragraph{Ablation Study.}
We also conduct an ablation study on the number of sparse latent points and the method to sample them. Results are shown in Appendix B.10.



\begin{figure}[t]
% \vspace{-1em}
    \centering
    \includegraphics[width=0.4\textwidth]{Figures/method_figures/shape_generation_diversity.png}
    \vspace{-1em}
    \caption{Our method is able to generate diverse meshes for the same set of sparse latent points due to the stochasticity in the feature generation process. Here are two pairs of generated lamps for the same set of latent points.}
    \label{fig:generation_diversity}
\vspace{-1.5em}
\end{figure}

\begin{figure}[t]
% \vspace{-2em}
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/method_figures/local_controllable_generation.png}
    \vspace{-1em}
    \caption{
    % Left side are FPS sampled sparse latent points (top) and human placed sparse latent points (bottom). We can 
    Use manually placed sparse latent points to control the rear legs of the generated chairs. 
    The blue points are moving and the red points are fixed.
    The top row generates new features for all sparse latent points, and the bottom row generates new features only for moved points and fixes the features of the rest points.}
    \label{fig:local_controllable_generation}
% \vspace{-1em}
\end{figure}

\begin{figure}[t]
\vspace{-1em}
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/method_figures/shape_combination.png}
    \vspace{-0.5em}
    \caption{Perform shape combination. 
    The first row are the sparse latent points of the original two lamps and the combined sparse latent points.
    % The first row are the sparse latent points (with features) of the original two lamps (left side and right side) and the two combined sparse latent points (the middle two).
    The second row are the original two lamps (two sides) and the two lamps (middle two) obtained by combining the top part and bottom part of the original lamps.}
    \label{fig:shape_combination}
\vspace{-1.5em}
\end{figure}

% \begin{figure}[t]
% \vspace{-1em}
%     \centering
%     \includegraphics[width=0.9\linewidth]{Figures/method_figures/decompose_feature_and_position.png}
%     \vspace{-0.5em}
%     \caption{Decompose positions and features of the sparse latent points during interpolation. See Appendix B.8 for the complete interpolation process bewteen the two shapes.}
%     \label{fig:decompose_feature_and_position}
% \vspace{-1.5em}
% \end{figure}


% \subsection{Controllable Generation.}
\vspace{-2em}
\paragraph{Controllable Generation.}
As mentioned in Section~\ref{sec:train_latent_ddpm}, we can use the sparse latent points to control the generated mesh.
Specifically, we can change the positions of the sparse latent points, then use the second DDPM to generate features at the latent points, and finally decode them to a point cloud and reconstruct the mesh.
Several examples are shown in Figure~\ref{fig:controllable_generation}. 
It shows that we can use the sparse latent points to control the overall scale of the generated mesh as well as change the position, scale, or shape of a part of the mesh.
It is worth noting that we achieve this without any part annotations of the dataset.
Our method is also able to generate diverse meshes even for the same set of sparse latent points due to the stochasticity in the feature generation process.
Figure~\ref{fig:generation_diversity} gives two pairs of examples.

The sparse latent points in Figure~\ref{fig:controllable_generation} are obtained by FPS. 
At inference, we can also manually place the sparse latent points at regions of interest other than FPS sampled points and control the corresponding part.
This is because we augment the FPS sampled sparse latent points with Gaussian noises during training and it makes our model robust to the positions of the sparse latent points.
Figure~\ref{fig:local_controllable_generation} gives an example where we manually select the sparse latent points and control the rear legs of a chair.
In addition, if we want to keep the rest part of a shape fixed while changing the part we want to edit, we can use the second DDPM to sample features only for moved sparse latent points and fix the features of rest points.
See Figure~\ref{fig:local_controllable_generation} for an example.
This is achieved by an algorithm similar to DDPM-based image inpainting and is described in Appendix A.5.

\begin{figure}[t]
\vspace{-2em}
    \centering
    \includegraphics[width=0.43\textwidth]{Figures/method_figures/interpolation_used.png}
    \vspace{-0.5em}
    \caption{Our method is able to perform both global and local interpolations. The first row is an example of global interpolation. The second row interpolates between the bottom of the two lamps.}
    \label{fig:interpolation}
\vspace{-1em}
\end{figure}
% \subsection{Shape Interpolation.}
\vspace{-1em}
\paragraph{Shape Interpolation.}
% It is straightforward to perform interpolation using our sparse latent point representation of 3D shapes.
To interpolate two shapes,
we can interpolate both the positions and features between the corresponding latent points of two shapes.
See Appendix B.8 for how to establish correspondence between two sets of sparse latent points of two shapes.
The top row of Figure~\ref{fig:interpolation} is an example of global interpolation.
Our method is also able to perform local interpolation.
We can interpolate only a part of the latent points, and keep the positions and features of the rest part of the latent points fixed.
The bottom row of Figure~\ref{fig:interpolation} is an example of local interpolation.
% We can also decompose positions and features of the sparse latent points during interpolation.
% Figure~\ref{fig:decompose_feature_and_position} gives an example where we only interpolate positions or features of the sparse latent points between two shapes.
% We can see that positions of the sparse latent points mainly control the overall size and structure of the shape, while features mainly control the local geometry of the shape.
\vspace{-1.8em}
\paragraph{Shape combination.}
We can also perform shape combinations using our sparse latent point-based representation of 3D shapes.
We can simply combine the sparse latent points and their features from two or more source shapes to form new shapes.
See Figure~\ref{fig:shape_combination} for an example.
% \subsection{Single-Class 3D Shape Generation}

% \subsection{Many-class Unconditional 3D Shape Generation}

% \subsection{Sampling Time}