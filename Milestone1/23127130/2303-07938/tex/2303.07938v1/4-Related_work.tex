% \input{Tables/mesh_mmd.tex}
\begin{figure*}[t]
    \vspace{-2em}
    \centering
    \includegraphics[width=1\textwidth]{Figures/method_figures/mesh_comparison.png}
    \vspace{-2em}
    \caption{Mesh generated by our methods and baselines. 
    % The meshes are reconstructed from the generated point clouds using SAP~\cite{peng2021shape}. 
    We can see that meshes generated by our method are more visually appealing. More examples of other baselines and our method are provided in Appendix B.5 and B.8.}
    \label{fig:mesh_comparison}
    \vspace{-1.5em}
\end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/method_figures/car_pc.png}
    \caption{Point clouds generated by our method and baselines. More examples are provided in Appendix B.7 and B.8.}
    \label{fig:car_pc}
    \vspace{-1em}
\end{figure}

% \vspace{-1em}
\paragraph{Mesh Generation.}
Most existing mesh generation methods rely on deforming a template mesh or another mesh~\cite{wang2018pixel2mesh, wen2019pixel2mesh++, gupta2020neural, liu2021deepmetahandles, yifan2020neural, jakab2021keypointdeformer, jiang2020shapeflow}, but meshes generated in this way are usually limited by the topology of the template or the initial mesh. And large deformations could cause defects. In contrast, our method is able to generate meshes from scratch with diverse topologies. 
Another line of works uses implicit representations of 3D shapes~\cite{DBLP:conf/cvpr/ChenZ19, park2019deepsdf, sitzmann2020implicit, genova2020local, zheng2022sdf, kleineberg2020adversarial,Chen_2021_ICCV, mittal2022autosdf}, but it usually requires dense neural network evaluations to extract meshes from the learned model.

\vspace{-1.5em}
\paragraph{Point cloud generation.}
Many learning-based methods are proposed to model the distribution of point clouds.
Some works use generative adversarial networks (GANs) to generate point clouds~\cite{DBLP:conf/iccv/TreeGan,li2021sp,achlioptas2018learning,li2018point}.
\cite{achlioptas2018learning} also trains a latent GAN in the latent space of a point cloud autoencoder, but the autoencoder they use can only encode a point cloud to a global feature.
Other works~\cite{yang2019pointflow,DBLP:conf/nips/SoftFlow,DBLP:conf/eccv/DPF-Net} use normalizing
flows to model the distribution of point clouds.
ShapeGF~\cite{DBLP:conf/eccv/CaiYAHBSH20} learns gradient fields to move randomly sampled points to the surface of the objects.
DDPMs have also been applied to point cloud generation~\cite{luo2021diffusion, zhou20213d}.
The generated point clouds of these methods can be transformed to meshes through surface reconstruction techniques~\cite{hanocka2020point2mesh,wei2021deep,chen2022neural,jiang2020shapeflow, williams2019deep, chibane2020implicit, gao2020learning, shen2021deep}.
In this work, we choose SAP~\cite{peng2021shape} for surface reconstruction for its efficiency and reconstruction quality.
\vspace{-1.5em}
\paragraph{Diffusion models.}
DDPMs are a kind of likelihood-based generative model that generate samples by gradually denoising a Gaussian noise~\cite{ho2020denoising,sohl2015deep}.
They have shown promising results for 3D point cloud generation~\cite{luo2021diffusion, zhou20213d}.
Our work is based on the recently proposed
latent diffusion models~\cite{rombach2022high, vahdat2021score}.
Latent diffusion models train diffusion models in the latent space of an autoencoder that encodes data samples to a more compact representation, and thus makes the training and sampling process of DDPMs faster.

\vspace{-1.5em}
\paragraph{Concurrent works.}
The concurrent work, LION~\cite{zeng2022lion}, also proposes to use a latent diffusion model to learn the distribution of point clouds and then use SAP~\cite{peng2021shape} to reconstruct meshes from point clouds, but the latent point cloud representation they use is a noisy point cloud with the same number of points as the original clean point cloud.
In contrast, we encode the original clean point cloud to a sparse set of latent points with features, which is a more compact representation and thus leads to faster training and sampling for DDPMs.
This representation also enables us to perform controllable generation using the sparse latent points.
Concurrently, NVMG~\cite{zheng2022neural} proposes to use voxels as the latent representations of meshes, but computational cost increases rapidly as the resolution of the 3D grid increases.
