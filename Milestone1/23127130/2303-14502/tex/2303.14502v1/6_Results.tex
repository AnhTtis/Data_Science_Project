\section{Results and Evaluations}
We detail our method's implementation and experiments conducted on a real robot. Then, we perform ablation studies and comparisons to highlight VERN's benefits.

\begin{figure*}[t]
    \centering
    \includegraphics[width=14.75cm,height=4.75cm]{Images/costmap_comparison_5.png}
    \caption{\small{Snapshots of the vegetation classification results on RGB images (top), and the corresponding cost maps $C_{VA}$ marked with critical obstacles (bottom) from trials in scenarios 2 (left), 3 (center), and 4 (right). A green/red rectangle in a quadrant $Q_{1,2,3,4}$ in the RGB image represents a PV/NPV classification respectively. The same quadrants are projected onto the cost maps ($Q^C_{1,2,3,4}$) along with their classifications for visualization. The cost for the vegetation within the green regions is reduced significantly using equation \ref{eqn:clearing}. The encircled regions in each cost map correspond to an obstacle in the RGB image. [\textbf{Left}]: White - Tree, Blue - Tall grass, [\textbf{Center}]: White - Tree, Blue - Bush, [\textbf{Right}]: White - Tree, Blue - Human. Our classifier accurately detects trees, and bushes as non-pliable/untraversable, and tall grass as traversable. In scenario 4, it accurately detects the human as untraversable due to its dissimilarity with all training classes and tall height.}}
    \label{fig:costmap_comparisons}
    \vspace{-10pt}
\end{figure*}

\subsection{Implementation and Dataset}
VERN's classifier is implemented using Tensorflow 2.0. It is trained in a workstation with an Intel Xeon 3.6 GHz processor and an Nvidia Titan GPU using real-world data collected from a manually teleoperated Spot robot. The robot is equipped with an Intel NUC 11 (a mini-PC with Intel i7 CPU and NVIDIA RTX 2060 GPU), a Velodyne VLP16 LiDAR, and an Intel RealSense L515 camera. The training images ($\sim600$ per class) were collected from various environments in the University of Maryland, College Park campus. Our model takes about 6 hours to train for 55 epochs. 

% \begin{figure}[t]
%       \centering
%       \includegraphics[width=\columnwidth,height=5.0cm]{paper-template-latex/Images/costmap_va_4.png}
%       \caption {\small{$C_{VA}$ corresponding to scenario 2 (\ref{fig:navigation_comparisons} [center]) where the robot is surrounded by dense, tall grass and trees. The regions with critical obstacles are in red, the homography corresponding to the four image quadrants (green indicates PV classification in the rectangular region, blue indicates NPV classification). The robot is marked as a yellow circle. [\textbf{Top right}]:} The view from the robot's camera, and the corresponding quadrants, and classifications. We observe that the left quadrants predominantly contain a tree, and the right quadrants contain tall grass. Our planner's candidate trajectories are shown in orange, and the optimal velocity's trajectory (in green) navigates the robot towards the pliable tall grass on the right.}
%       \label{fig:costmap}
%       \vspace{-15pt}
% \end{figure}


\begin{figure*}[t]
    \centering
    \includegraphics[width=17cm,height=4cm]{Images/sceanrios_w_trajectories_v3.png}
    \caption{\small{Spot robot navigating in: \textbf{[Left]} Scenario 1, \textbf{[Center]} Scenario 2, and \textbf{[Right]} Scenario 4. We observe that VERN navigates through pliable/traversable vegetation in the absence of free space. This leads to significantly higher success rates, low freezing rates, and trajectory lengths. Other methods either freeze or take long, meandering trajectories to the goal.}}
    \label{fig:navigation_comparisons}
    \vspace{-10pt}
\end{figure*}

% \begin{figure}[t]
%       \centering
%       \includegraphics[width=\columnwidth,height=3.5cm]{paper-template-latex/Images/classifier_results_qualitative.png}
%       \caption {\small{ Example vegetation classifier predictions for two camera images from the robot and their corresponding quadrants. Blue color indicates  non-pliable vegetation regions and green color indicates pliable vegetation. We observe that our classifier is able to identify trees, bushes, and solid unseen objects (see the partially visible black color trash can in the top right quadrant of image (b)) as non-pliable while sparse and no-grass regions are detected as pliable. }}
%       \label{fig:classifier_results}
%       % \vspace{-15pt}
% \end{figure}

\subsection{Evaluations} 
We use the following evaluation metrics to compare VERN's performance against several navigation methods: 1. Boston Dynamics' in-built autonomy on Spot, 2. DWA \cite{fox1997dwa}, 3. GA-Nav \cite{ganav}, and 4. GrASPE \cite{graspe}. Spot's in-built autonomy uses its stereo cameras in four directions around the robot, estimates obstacles, and navigates to a goal. DWA is a local planner that utilizes a 2D LiDAR scan for obstacle avoidance. GA-Nav combines semantic segmentation for terrain traversability estimation with elevation maps for outdoor navigation. It is trained on publically available image datasets (RUGD \cite{RUGD2019IROS} and RELLIS-3D \cite{jiang2020rellis3d}). GrASPE is a multi-modal fusion framework that estimates perception sensor reliability to plan paths through unstructured vegetation. We further compare VERN without height estimation and recovery behaviors. The metrics we use for evaluations are,

\begin{itemize}
    \item \textbf{Success Rate} - The number of successful goal-reaching attempts (while avoiding non-pliable vegetation and collisions) over the total number of trials.

    \item \textbf{Freezing Rate} - The number of times the robot got stuck or started oscillating for more than 5 seconds while avoiding obstacles over the total number of attempts. Lower values are better.

    \item \textbf{Normalized Trajectory Length} - The ratio between the robot's trajectory length and the straight-line distance to the goal in all the successful trajectories.

    \item \textbf{False Positive Rate (FPR)} - The ratio between the number of false positive predictions (i.e., actually untraversable/non-pliable obstacles predicted as traversable) and the total number of actual negative (untraversable) obstacles encountered during a trial. We report the average over all the trials.
\end{itemize}

\no If the sum of the success and freezing rates do not equal 100, it indicates that the robot has collided in those cases. We also compare these methods qualitatively using the trajectories pursued while navigating. For perception comparisons, we quantitatively evaluate the accuracy and F-score of MobileNetv3 \cite{mobilenetv3}, EfficientNet \cite{tan2019efficientnet}, and Vision Transformer \cite{vision_transformer} when trained and evaluated on our dataset. We also show VERN's classification results and cost map clearing in Fig. \ref{fig:costmap_comparisons}.

% \no \textbf{Success Rate} - The number of successful goal-reaching attempts (while avoiding non-pliable vegetation and collisions) over the total number of trials.

% \no \textbf{Freezing Rate} - The number of times the robot got stuck or started oscillating for more than 10 seconds, while avoiding obstacles over the total number of attempts.

% \no \textbf{Normalized Trajectory Length} - The ratio between the robot's trajectory length and the straight-line distance to the goal in both successful and unsuccessful trajectories. 

% \no \textbf{False Positive Rate (FPR)} - The ratio between the number of false positive predictions (i.e., actually untraversable trajectories predicted as traversable) and the total number of actual negative (untraversable) samples encountered during a trial. We report the average over all the trials. 

% \no \textbf{False Negative Rate  (FNR)} - The ratio between the number of false negative predictions (i.e., actually traversable trajectories as untraversable) and the total number of actual positive samples encountered during a trial. We report the average over all the trials. 

\vspace{-2pt}
\subsection{Navigation Test Scenarios}
We compare the performance of all the navigation methods in the following real-world outdoor scenarios (see Figs. \ref{fig:cover_image} and \ref{fig:navigation_comparisons}) that differ from the training environments. The scenarios are described in increasing order of difficulty. Ten trials are conducted in each scenario for each method.

\begin{itemize}
\item \textbf{Scenario 1} - Contains sparse tall grass, and trees.

\item \textbf{Scenario 2} - Contains trees and dense tall grass in close proximity. The robot must identify grass and pass through it to reach its goal successfully.

\item \textbf{Scenario 3} - Contains trees, bushes, and dense grass in close proximity. The robot must identify grass and pass through it. See Fig. \ref{fig:cover_image} 

\item \textbf{Scenario 4} - Contains dense grass, trees, and an obstacle (human) unseen during training. 
\end{itemize}

\begin{table}[t]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ |c |c |c |c |c |c |} 
\hline
\textbf{Metrics} & \textbf{Method} & \multicolumn{1}{|p{1cm}|}{\centering \textbf{Scenario} \\ \textbf{1}} & \multicolumn{1}{|p{1cm}|}{\centering \textbf{Scenario} \\ \textbf{2}} & \multicolumn{1}{|p{1cm}|}{\centering \textbf{Scenario} \\ \textbf{3}} & \multicolumn{1}{|p{1cm}|}{\centering \textbf{Scenario} \\ \textbf{4}}\\ [0.5ex] 
\hline
\multirow{6}{*}{\rotatebox[origin=c]{0}{\makecell{\textbf{Success}\\\textbf{Rate (\%)} \\ (Higher is \\ better)}}} 
& Spot's Inbuilt Planner & 50 & 0 & 0 & 0   \\
 & DWA \cite{fox1997dwa} & 80 & 0 & 0 & 0   \\
 & GA-Nav\cite{ganav}    & 60 & 20 & 30 & 30 \\
 & GrASPE\cite{graspe}   & 80 & 60 & 50 & 40 \\
 & VERN w/o height estimation & 80 & 60 & 40 & 30 \\
 & VERN w/o recovery behavior & 100 & 60 & 40 & 40 \\
 & VERN(ours) & \textbf{100} & \textbf{90} & \textbf{70} & \textbf{70} \\
\hline

\multirow{6}{*}{\rotatebox[origin=c]{0}{\makecell{\textbf{Freezing}\\\textbf{Rate (\%)} \\ (Lower is \\ better)}}} 
& Spot's Inbuilt Planner & 30 & 100 & 100 & 90   \\
 & DWA \cite{fox1997dwa} & 20 & 100 & 100 & 100   \\
 & GA-Nav\cite{ganav}    & 40 & 80 & 70 & 70 \\
 & GrASPE\cite{graspe}   & 10 & 20 & 20 & 50 \\
 & VERN w/o height estimation & 20 & 40 & 40 & 70 \\
 & VERN w/o recovery behavior & 0 & 40 & 60 & 60 \\
 & VERN(ours) & \textbf{0} & \textbf{10} & \textbf{20} & \textbf{30} \\
\hline


\multirow{6}{*}{\rotatebox[origin=c]{0}{\makecell{\textbf{Norm.}\\\textbf{Traj.}\\\textbf{Length}\\ (Closer to 1 \\ is better)}}} 
& Spot's Inbuilt Planner & 1.23 & 0.37 & 0.24 & 0.22   \\
 & DWA \cite{fox1997dwa}  & 1.52 & 0.46 & 0.34 & 0.62   \\
 & GA-Nav\cite{ganav} & 1.31 & 1.39 & 1.32 & 1.49 \\
 & GrASPE\cite{graspe} & 1.18 & 1.09 & 1.46 & 1.37 \\
 & VERN w/o height estimation & 1.39 & 1.41 & 1.35 & 1.30 \\
 & VERN w/o recovery behavior & 1.42 & 1.48 & 1.46 & 1.39 \\
 & VERN(ours) & 1.11 & 1.19 & 1.28 & 1.23 \\
\hline

\multirow{4}{*}{\rotatebox[origin=c]{0}{\makecell{\textbf{ False}\\\textbf{Positive }\\\textbf{Rate}}}} 
% & Spot's Inbuilt Planner & - & - & - & -   \\
% & DWA \cite{fox1997dwa} & - & - & - & -   \\
 & GA-Nav\cite{ganav} & 0.33 & 0.39 & 0.30 & 0.61 \\
 & GrASPE\cite{graspe} & 0.25 & 0.31 & 0.28 & 0.44 \\
  & EfficientNet\cite{tan2019efficientnet}& 0.28 & 0.23 & 0.32& 0.35 \\
  % & VERN w/o height estimation & 0.21 & 0.29 & 0.32 & 0.42 \\
 & VERN(ours) &  \textbf{0.15} &  \textbf{0.18} &  \textbf{0.12} &  \textbf{0.21} \\
\hline


\end{tabular}
}
\caption{\small{Navigation performance of our method compared to other methods on various metrics. VERN outperforms other methods consistently in terms of success rate, freezing rate, false positive rate, and normalized trajectory length in different unstructured outdoor vegetation scenarios.}
}
\label{tab:comparison_table}
\vspace{-10pt}
\end{table}

\begin{table}[t]
\centering
\resizebox{0.75\columnwidth}{!}{
\begin{tabular}{|c|c|c|} 
\hline
\textbf{Methods}\Tstrut \Tstrut & \textbf{Accuracy} & \textbf{F-score} \\ [0.5ex] 
\hline
MobileNetv3 \cite{mobilenetv3} & \textbf{0.957} & \textbf{0.833} \\
\hline
EfficientNet \cite{tan2019efficientnet} & 0.758 & 0.632 \\
\hline
Vision Transformer (ViT) \cite{vision_transformer} & 0.689 & 0.546 \\
\hline
\end{tabular}}

\caption{ \small{\label{Tab:Results2} The accuracy and F-scores (higher values are better) of three feature-extracting backbones used to train our dataset. We observe that our MobileNetv3 has the best accuracy and F-score because of its depth-wise separable convolutions. This leads to faster and better learning.}}
\vspace{-15pt}
\end{table}

\subsection{Analysis and Discussion}
\textbf{Classification Accuracy:} We observe from Table \ref{Tab:Results2} that MobileNetv3 has the best accuracy and F-score compared to other methods. EfficientNet is difficult to train and tune especially due to longer training time requirements. Similarly, ViT requires a lot more data for training and is not compatible with the siamese network framework. 

\textbf{Navigation Comparison:} The quantitative navigation results are presented in Table \ref{tab:comparison_table}. We observe that VERN's (with height estimation used in equation \ref{eqn:clearing} and recovery behaviors) success rates are significantly higher compared to the other methods. Spot's in-built planner and DWA consider all vegetation as obstacles. This leads to Spot becoming unstable or crashing when using its in-built planner and leads to excessive freezing and oscillations when using DWA. 

GA-Nav considers all vegetation (except trees) as partially traversable because it segments most flora as grass. It also cannot differentiate vegetation due to the lack of such precisely human-annotated datasets. Additionally, in cases where vegetation like tall grass partially occludes the RGB image (see scenario 2 in Fig. \ref{fig:navigation_comparisons} [center]), GA-Nav struggles to produce segmentation results leading to the robot freezing or oscillating. In regions with tall grass, GA-Nav's elevation maps also become erroneous.

GrASPE performs the best out of all the existing methods. It is capable of passing through sparse grass and avoiding untraversable obstacles. However, in highly dense grass that is in close proximity to trees, GrASPE is unable to accurately detect and react quickly to avoid collisions with trees.

Since some existing methods do not reach the goal even once, their trajectory lengths are less than 1. The values are still reported to give a measure of their progress toward the goal. Spot's in-built planner progress the least before the robot froze or crashed. In some cases, the methods take meandering trajectories (e.g. DWA) in scenario 1 before reaching the goal. Notably, VERN deviates the least from the robot's goal and that reflects in the low trajectory length. 

\textbf{FPR}: We compare VERN's vegetation classifier's false positive rate (FPR) with GA-Nav and the EfficientNet-based classifier using manual ground truth labeling of the vegetation in the trials. We observe that GA-Nav leads to a significantly high FPR in all four scenarios primarily because GA-Nav's terrain segmentation predictions are trained on the RUGD dataset. Moreover, GA-Nav's incorrect segmentation under varying lighting conditions and occlusions increases the false positive predictions. GrASPE and the EfficientNet-based classifier are trained using the same images we used to train VERN. However, we observe that their accuracy is comparatively lower than our VERN model. This is primarily due to the fine-grained feature learning capabilities of VERN's MobileNetv3-based backbone. Additionally, GrASPE's predictions lead to erroneous results when its 3D point cloud cannot identify the geometry of the vegetation such as trees and bushes under visually cluttered instances.  

\textbf{Ablation study}: We compared VERN and its variants without using height estimation, and recovery behaviors. We observe that when the height estimation is not used for cost map clearing, the robot's success rate drops. This is mainly because height helps differentiate short and tall pliable vegetation (where the robot could freeze). Additionally, estimating the critical regions based on the height helps avoid obstacles such as humans (Scenario 4) which are not part of our classifier. 

Using recovery behaviors helps reduce freezing in the presence of dense obstacles (especially in scenarios 2, 3, and 4). Additionally, moving the robot to a safe location, and marking and remembering the unsafe region allows the robot to preemptively avoid the region in subsequent trials. 


