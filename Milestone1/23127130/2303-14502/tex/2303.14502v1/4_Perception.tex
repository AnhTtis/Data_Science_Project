\section{VERN: Vegetation Classification}
Our vegetation classifier uses an RGB image ($I^{RGB}_t$) obtained from a camera on the robot at a time $t$ as input. Although a plant's structure is well preserved in an image, using the entire image for classification is infeasible because the scene in $I^{RGB}_t$ typically contains two or more types of vegetation together. Therefore, we split $I^{RGB}_t$ into four quadrants $Q_1$ (top-left), $Q_2$ (top-right), $Q_3$ (bottom-left), $Q_4$ (bottom-right) as shown in Figs. \ref{fig:network-arch}, \ref{fig:costmap_comparisons}. A quadrant predominantly contains a single type of vegetation typically (see Fig. \ref{fig:costmap_comparisons}). 

We classify vegetation in the quadrants into four classes: 1. sparse grass, 2. dense grass, 3. bush, and 4. tree. We separate sparse and dense grass due to their visual dissimilarity. 

% Mention how mobilenetv3 was made lighter. 
% Explain our network's outputs (distance used as confidence)

\subsection{Data Preparation}
To create the training dataset, images collected from different environments are split into quadrants and grouped together manually based on the vegetation type predominant in a quadrant. Next, pairs of images are created either from the same group or from different groups. The pair with similar images (same group) is automatically labeled 1, or 0 otherwise. The entire data preparation process takes about 30-45 minutes of manual effort. The obtained image pairs and the corresponding labels are passed into the classification network for training.

\subsection{Network Architecture}
% Architecture Explanation
Our vegetation classifier network consists of two identical feature extraction branches to identify the similarity between input image pairs. Our feature extraction branches are based on the MobileNetv3 \cite{mobilenetv3} backbone. We choose MobileNetv3 because it incorporates depth-wise separable convolutions (i.e., fewer parameters), leading to a comparatively lightweight and fast neural network. The outputs of the MobileNetv3 branches ($h_1$ and $h_2$) are one-dimensional latent feature vectors of the corresponding input images. The euclidean distance between the two feature vectors is calculated and passed through a $sigmoid$ activation layer to obtain the predictions.

% Network Archictecture figure
\begin{figure}[t]
      \centering
      \includegraphics[width=7.5cm,height=4.5cm]{Images/network_ref_images.png}
      \caption {\small{VERN's classification network architecture. Quadrants of the real-time camera image are paired with several reference images for each class and fed into the two identical branches of our network. Example reference images for each class are shown at the bottom.}}
      \label{fig:network-arch}
      \vspace{-10pt}
\end{figure}

We utilize the \textit{contrastive loss} function, which is capable of learning discriminative features to evaluate our model during training. Let $\hat{y}$ be the prediction output from the model. The contrastive loss function $J$ is:
\vspace{-5pt}
\begin{equation}
    J = \hat{y} \cdot d^2 + (1-\hat{y}) \cdot max(margin-d,0)^2,
\end{equation}

where $d$ is the euclidean distance between the feature vectors $h_1$ and $h_2$. $Margin$ is used to ensure that dissimilar image pairs are at least $margin$ distance apart.

% Outputs
\subsection{Network Outputs} \label{sec:net-outputs}
During run-time, the quadrants $Q_1, Q_2, Q_3,$ and $Q_4$ are each paired with several reference images of the four classes. The several reference images per class have different viewpoints and lighting conditions. They are fed into the classifier model $\mathcal{F}$ as a batch to obtain predictions as, 
% \vspace{-5pt}
\begin{equation}
    \mathcal{F}(Q_1, Q_2, Q_3, Q_4) = \Tilde{V}_{4 \times 4} |\,\, (\Tilde{V}_{ij} \in [0, 1]) \,\, i, j \in \{1, 2, 3, 4\} .
\end{equation}

% \no Here, $\Tilde{V}_{4 \times 4}$ is the output prediction matrix whose $i^{th}$ row corresponds to quadrant $Q_i$ ($i = \{1, 2, 3, 4\}$) and each column corresponds to one of the vegetation categories.
\no $\Tilde{V}_{4 \times 4}$ is the output prediction matrix whose values $\Tilde{V}_{i,j}$ correspond to the \textit{least} euclidean distance of the quadrant $Q_i$ from any of the reference images for the $j^{th}$ class. The closer $\Tilde{V}_{i,j}$ is to 0, the higher the similarity between $Q_i$ and class $j$. 

We extract two outputs from $\Tilde{V}_{4 \times 4}$. Namely, for each quadrant $Q_i$: 1. the vegetation class that is most similar to it ($\Tilde{v}_i$), and 2. the corresponding similarity score ($d_{i}$):
\vspace{-5pt}
\begin{equation}
    \Tilde{v}_i = \underset{j}{\operatorname{argmin}} \Tilde{V}_{i,j}, \,\, \text{and} \,\, d_{i} = \Tilde{V}_{i,j}. 
\end{equation}

\no For readability, hereafter we denote $\Tilde{v}_i$ as belonging to the Pliable Vegetation (PV) set if $\Tilde{v}_i = 1$ or $2$ (sparse/dense grass), and to the Non-Pliable Vegetation (NPV) set if $\Tilde{v}_i = 3$ or $4$ (bush/tree). We define the confidence of classification $\kappa_i$ as,
\vspace{-7pt}
\begin{equation}
    \kappa_i = e^{-\alpha \cdot d_i}, 
\end{equation}
% \vspace{-5pt}
\no where $\alpha$ is a tunable parameter. We observe that $d_i \to 0 \implies \kappa_i \to 1$, and vice versa.

% Hence the output prediction is a vector of length four, with elements indicating whether each quadrant of the image belongs to the grass category or not.

% \begin{figure}
%     \centering
%     \includegraphics[width=\columnwidth,height=5.3cm]{paper-template-latex/Images/vegetation_classes}
%     \caption{Vegetation categories used in our classifier network. (a) Sparse grass; (b) Dense grass; (c) Bushes; (d) Trees. We consider the sparse and dense grass regions as pliable, while bushes and trees are non-pliable.}
%     \label{fig:vegetation_classes}
%     \vspace{-5pt}
% \end{figure}