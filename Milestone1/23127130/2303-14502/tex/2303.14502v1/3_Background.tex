\section{Background}
In this section, we provide our problem formulation, briefly explain siamese networks used for vegetation classification, and the Dynamic Window Approach (DWA) \cite{fox1997dwa} used as a basis for our navigation. Our overall system architecture is shown in Fig. \ref{fig:sys-arch}.

\subsection{Setup and Conventions}
We assume a legged robot setup with holonomic dynamics equipped with a 3D lidar (that provides point clouds and 2D laser scans) and an RGB camera. Certain quantities are measured relative to a global coordinate frame (referred to as odom hereafter) with the robot's starting location as its origin. Some quantities are relative to a cost map grid moving with and centered at the robot. The quantities can be transformed between these frames using transformation matrices $T^O_C$ or $T^C_O$. The reference frame for each quantity is indicated in the superscript ($O$ or $C$) of its symbol. We denote vectors in bold, and use symbols such as i, j, row, col as indices. Apart from its pose w.r.t odom, the robot does not have access to any global information.

% All distances, angles and velocities are measured relative to a rigid coordinate frame attached to the robot. The positive X-axis of this frame points in the forward direction, the Y-axis points to the left, and the Z-axis points upwards. In some instances, global locations relative to the robot's starting location (referred to as odom frame) are measured.

\subsection{Problem Domain}
We consider navigation in environments containing three kinds of vegetation together: 1. Tall grass, 2. Bushes/shrubs, and 3. Trees. We make the following assumptions about the three types. 

\subsubsection{Properties of Vegetation}
Tall grass is pliable (therefore traversable), with height $> 0.3m$ (could be taller than the height the robot's sensors are mounted at), and could be of varying density in the environment. Bushes/shrubs (height $0.1 - 0.5m$), and trees (height $> 2m$) are non-pliable/untraversable and must be circumvented. 

% Trees are non-pliable, with height $> 2m$, and are considered \textit{hard} obstacles. The robot must avoid collisions with trees by maintaining a sufficient distance. 

\subsubsection{Adverse Phenomena} \label{sec:adverse-phen}
Navigating through such vegetation, the robot could encounter three adverse phenomena: 1. Freezing, 2. Physical entrapment in vegetation, and 3. Collisions. The conditions for these phenomena are, 
\begin{enumerate}
    \item Freezing:  $V_r = \varnothing$,
    
    \item Entrapment: $(v^*, \omega^*) \ne (0, 0)$ but $\Delta x^{O}_{rob} = \Delta y^{O}_{rob} = \Delta \theta^{O}_{rob} = 0$,

    \item Collisions: $dist(\text{robot}, \text{\{bushes/trees\}}) = 0$.
\end{enumerate}

% When these phenomena happen
% Freezing happens when the robot's planner cannot find any non-zero, collision-free velocity. The robot therefore halts or oscillates for an extended time period. 
\no Here, $V_r$ refers to the robot's feasible, collision-free velocity space. $(v^*, \omega^*)$ is the velocity commanded by the robot's planner, and $\Delta x^{O}_{rob}, \Delta y^{O}_{rob}, \Delta \theta^{O}_{rob}$ refer to the robot's change in position and yaw orientation relative to the global odom frame. 
Freezing occurs in densely vegetated environments if the planner cannot find any collision-free velocity to reach its goal. To avoid this, the robot must accurately identify pliable vegetation to navigate through. The robot could also get physically entrapped in dense vegetation if its legs are caught in grass, small branches of bushes, etc. Finally, the robot could collide if its planner misclassifies a non-pliable obstacle as pliable. 

% Problem Formulation
\no With these definitions, VERN's problem formulation can be stated as follows:
\begin{probform}
To compute a robot velocity $(v^*, \omega^*)$ such that $traj^O(v^*, \omega^*) \in Free^O \, \cup \, Grass^O$, $traj^O(v^*, \omega^*) \notin Tree^O \, \cup \, Bush^O$, assuming that $Tree^O \, \cap \, Grass^O = Tree^O \, \cap \, Bush^O = Bush^O \, \cap \, Grass^O = \varnothing$. 
\end{probform}

\no Here, $Tree^O, Bush^O, Grass^O$ represent the sets of locations containing trees, bushes, and grass, respectively that were detected by the robot w.r.t the odom frame. $Free^O$ represents free space locations. $traj^O()$ returns the forward-simulated trajectory of a $(v, \omega)$ pair relative to the odom frame. We assume that the robot can balance and walk on all the terrains we consider. We also assume that the robot never encounters a scene with no free space and only non-pliable vegetation (an unresolvable scenario). 

% We highlight the symbols and notation used in Table \ref{tab:symbol_defn}.


% \begin{table}[t]
% \centering
% \begin{tabularx}{\linewidth}{|c|L|} % Change the alignment to center
% \hline
% \textbf{Symbols} & \textbf{Definitions}\\
% \hline
% $dist()$ & Distance of robot from any entity. \\
% \hline
% $V_r$ & Set of feasible, collision-free robot velocities.\\
% \hline
% $(v_{curr}, \omega_{curr})$ & The robot's current linear and angular velocities. \\
% \hline
% $x^{odom}_{rob}, y^{odom}_{rob}, \theta^{odom}_{rob}$ & Robot's position and orientation w.r.t a global odometry frame. \\
% \hline
% $traj(v, \omega)$ & The trajectory or set of robot's future x, y positions when executing $(v, \omega)$ \\
% \hline
% \end{tabularx}
% \caption{\small{List of symbols used in VERN and their definitions.}}
% \label{tab:symbol_defn}
% \end{table}


\subsection{Siamese Networks}
Siamese networks \cite{SiameseNN} are a class of neural networks used for one-shot or few-shot learning (learning with very few samples). They use two sub-networks with identical structures, parameters, and weights, and are connected at the end using an energy function (e.g., L2 norm). They accept two distinct images as inputs for the two sub-networks and are trained to map similar images close to each other in a low-dimensional latent space. Dissimilar images have latent representations that are mapped far away from each other. 

% Siamese networks are typically used for verification, and identification tasks (e.g., facial recognition) where the number of training images is low. Moreover, since they are trained to identify the similarity between images, the training dataset does not require any human annotation.

% How Siamese networks are useful to us
% Using them allows our classifier to be trained with few hundreds of images corresponding to each class. Our classifier's network architecture using siamese networks is shown in Fig. \ref{fig:network-arch}.

% System architecture
\begin{figure}[t]
      \centering
      \includegraphics[width=\columnwidth,height=4.0cm]{Images/sys-arch-2.png}
      \caption {\small{VERN's overall system architecture for vegetation classification to estimate pliability/traversability. Our model uses classification, its confidence, and vegetation height to create a vegetation-aware cost map $C_{VA}$ used for planning. In high traversability cost regions, our planner executes cautious behaviors by limiting the robot's velocity space. If the robot freezes or gets entrapped, it executes holonomic recovery behaviors.}  }
      \label{fig:sys-arch}
      \vspace{-10pt}
\end{figure}

\subsection{Dynamic Window Approach} \label{sec:dwa}
Our planner adapts the Dynamic Window Approach (DWA)~\cite{fox1997dwa} to perform navigation. We represent the robot's actions as linear and angular velocity pairs $(v,\omega)$. Let $V_s = [[0, v_{max}], [-\omega_{max}, \omega_{max}]]$ be the space of all the possible robot velocities based on the maximum velocity limits. DWA considers two constraints to obtain dynamically feasible and collision-free velocities: (1) The dynamic window $V_d$ contains the reachable velocities during the next $\Delta t$ time interval based on acceleration constraints;  (2) The admissible velocity space $V_a$ includes the collision-free velocities. The resulting velocity space $V_r = V_s \cap V_d \cap V_a$ is utilized to calculate the optimal velocity pair $(v^*,\omega^*)$ by minimizing the objective function below,
\vspace{-4pt}
\begin{equation}
\label{eq:dwa_obj_func}
    Q(v,\omega) = \sigma\big(\gamma_1 . head(.) + \gamma_2 . obs(.) + \gamma_3 . vel(.) \big),
\vspace{-4pt}
\end{equation}
where $head(.)$, $obs(.)$, and $vel(.)$ are the cost functions~\cite{fox1997dwa} to quantify a velocity pair's (($v, \omega$) omitted on RHS for readability) heading towards the goal, distance to the closest obstacle in the trajectory, and the forward velocity of the robot, respectively. $\sigma$ is a smoothing function and $\gamma_i, (i=1,2,3)$ are adjustable weights.