\section{Related Work}
% TODO: 
%   1. Add content for some DRL works, Graspe
%   2. Organize it as two subsecs vegetation detection and navigation
%   3. Add sentences for VERN overcoming limitations (tall vegetation, pliable+non-pliable together)

In this section, we discuss previous works in outdoor navigation and perception of unstructured vegetation.

\subsection{Perception in Dense Vegetation}
Early works on vegetation detection were typically chlorophyll detectors \cite{Bradley-2004-8857,nguyen-1,double-check-passable}, or basic classification models \cite{auto-terrain-characterization-modeling}.  Nguyen et al. \cite{nguyen-1} proposed a sensor setup to detect near-IR reflectance and used it to define a novel vegetation index. \cite{double-check-passable} extended this detection by using an air compressor device to create strong winds and estimating the levels of resistance to robot motion using the movement of vegetation. However, it needed the robot to be static. Multiple sensor configurations have also been explored for obstacle detection within vegetation \cite{sensing-tech-obstacle-detect} of which thermal camera and RADAR stand out as effective modalities.

Modeling the frictional/lumped-drag characteristics of vegetation \cite{momentum-traversal-mobility-challenges,char-traversal-pliable-veg,thesis} as a measure of the resistance offered to motion, or modeling plant stems using their rotational stiffness and damping characteristics \cite{modeling-traversal-pliable-materials} has also been proposed. However, the method does not use visual feedback, requiring the robot to drive through vegetation first to gauge its pliability.

Wurm et al. \cite{veg-laser-data-structured-outdoor,wurm2009improving} presented methods to detect short grass from lidar scans based on its reflective properties on near IR light. Astolfi et al. \cite{vineyard} demonstrated accurate SLAM and navigation through sensor fusion in a vineyard. However, such methods operated in highly structured environments. 

There are several works that utilize semantic segmentation to understand terrain traversability \cite{ganav,semantic-mapping-auto-off-road-nav,terrain-semantics-multi-legged}. \cite{terrain-semantics-multi-legged} applied semantic segmentation classifier trained on RGB images to oct-tree maps obtained from RGB-D point cloud. Maturana et al. \cite{semantic-mapping-auto-off-road-nav} took a similar approach by training a segmenter and augmenting a 2.5D grid map with it to distinguish tall grass from regular obstacles. However, these existing methods are not suitable for perception in dense, unstructured vegetation.

% TODO: Add RL-based works. Mention lack of simulators for vegetation we wish to operate on
% Cite Graspe
\subsection{Navigation in Unstructured Vegetation}
% Generic outdoor nav works
Although there are many works on outdoor, off-road navigation to handle slopes \cite{terp} and different terrain types \cite{terrapn}, there have been only a few methods for detecting and navigating through vegetation. \cite{Overbye-1} proposed using a terrain gradient map along with the A* algorithm to navigate a large wheeled robot and demonstrated moving over tall grass and short bushes. \cite{Overbye-2} extended this by adding a segmentation layer to the map to detect soft obstacles. However, these methods mostly operate in structured, isolated vegetation and may not work well in unstructured scenarios.

% and used path optimization in the actuator space to obtain fast, optimized plans that obey the robot's kinematic constraints. However, these methods mostly operate in structured, isolated vegetation and may not work well in unstructured scenarios.

% Tall grass nav works
There are several works that have addressed navigating through pliable vegetation such as tall grass \cite{badgr,model-error-katyal}. Kahn et al. \cite{badgr} demonstrated a model that learns from a robot's real-world experiences such as collisions, bumpiness, and its position to navigate outdoor environments. The robot learned from RGB images and associated experiences (labels) to consider tall grass as traversable. However, it does not account for the presence of non-traversable bushes or trees alongside traversable vegetation. Polevoy et al. \cite{model-error-katyal} proposed a model that regresses the difference between the robot's dynamics model and its actual realized trajectory in unstructured vegetation. This acts as a measure of the surrounding vegetation and terrain's traversability. However, both these methods require ``negative" examples such as collisions during training, which may be impractical or dangerous to collect in the real world. 

% The lack of photo-realistic simulators has given rise to offline reinforcement learning methods \cite{offline-rl}. They have demonstrated promising behaviors in grassland-like environments by training with RGB images. However, it is typically difficult to converge to a good navigation policy with such methods.

With the advent of legged robots, several recent works have been focused on developing robust controllers for locomotion purely using proprioception \cite{quadruped-locomotion-challenging-terrain} or fusing it with exteroceptive perception such as elevation maps \cite{robust-perceptive-locomotion-quad}.  A few navigation works focus on estimating the underlying support surface that is hidden by vegetation \cite{support-surface-legged-robot} by fusing haptic feedback from the robot, depth images, and detecting the height of the vegetation. Our method is complementary to these existing works.  

     

