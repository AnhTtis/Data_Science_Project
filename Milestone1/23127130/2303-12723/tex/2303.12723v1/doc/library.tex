\section{Dynamic Pattern Library}\label{library}
% \subsection{Pattern Library With Online Update}
After filtering out simple cases, we need to apply a rigorous solver for critical patterns. For further efficiency improvement, we build a dynamic pattern library to store the pattern pairs: sliced pattern $\mathbf{P}$ and their corresponding post-OPC mask $\mathbf{M}_{\mathbf{P}}$, hence to reuse the mask of repeating patterns to avoid the redundant time consumption of iterations-from-scratch OPC process. The key idea is to identify a stored repeating pattern before OPC. An online update mechanism enables a brand new pattern and corresponding mask to be inserted into the library.

We construct the pattern library with a graph structure as inspired by \cite{malkov2018efficient}, where each node represents a pattern stored. Each edge connecting two nodes shows they are neighbors of each other with more similarity. Considering the number of patterns on a full design layout, the size of the graph is gigantic. Naive search for shortest distance requires pair-wise distance comparisons of nodes in the graph, which is impractical. We improve the matching time efficiency with:
\begin{itemize}
    \item Sparse neighborhood graph structure, where nodes distant from each other are sparsely connected.
    \item Graph is divided into hierarchical layers. Nodes have restricted degree at each layer. more edges reside at lower layers, enabling greedy search of nearest neighbor at each layer.
\end{itemize}
The visualization of the hierarchical sparse graph structure is in \Cref{fig:hnsw}. 

\subsection{Pattern Matching And Online Update.}
The task of matching pattern with the most similar geometric shape can be regarded as a nearest neighbor search problem (NNS). As inspired by \cite{malkov2018efficient}, we utilize the Hierarchical Navigate Small World (HNSW) algorithm for fast matching. As shown in \Cref{fig:hnsw}, the pattern matching follows a greedy strategy to traverse the graph from higher layers to the bottom layer. A list of nearest pattern node candidates is maintained through the top-down traversal. The list is updated when a closer pattern appears during searching that has a distance shorter than one of the candidates. Such matching strategy is based on proximity graph nearest neighbors search. The detailed pattern matching search strategy at each hierarchical layer is illustrated in \Cref{alg:matching Strategy}.
% \begin{figure}
%     % \raggedleft
%     \includegraphics[width=0.9\linewidth]{figs/hnsw} 
%     \caption{visualization of the graph-based pattern matching flow. Query design pattern $\mathbf{P}$ greedily traversing the hierarchical graph. The nearest node reached at layer 0 conrresponds to a match pattern $\mathbf{P'}$ which has most similar geometric shape with $\mathbf{P}$.}
%     \label{fig:hnsw}
% \end{figure}
\begin{algorithm}
    \small
    \caption{Graph-Based Pattern Matching Greedy Search}
    \label{alg:matching Strategy}
    \begin{algorithmic}[1]
    \State {\textbf{Input:} query pattern $\mathbf{P}$, starting nodes $q_{s}$, number of nearest neighbor to return $k$,layer number $l$, distance measurement $d(\cdot)$} 
    \State {\textbf{Output:} nearest pattern candidates $C$}
    \State {$V \leftarrow q_{s}$} // Visited nodes
    \State {$W \leftarrow q_{s}$} // Waiting list of nodes to visit
    \State {$C \leftarrow q_{s}$}
    \While {$\lvert W \rvert > 0$}
        \State {$q^{*} \leftarrow$ nearest pattern from $W$ to $\mathbf{P}$}
        \State {$q_{f} \leftarrow$ furthest pattern from $C$ to $\mathbf{P}$}
        \If {$d(\mathbf{P}, q^{*}) > d(\mathbf{P}, q_{f})$}
        \State {break}
        \EndIf
        \For {$e \in neighbor(q^{*})$ in layer $l$}
        \If {$e \not\in V$}
        \State {$V \leftarrow V \cup \{e\}$}
        \State {$q_{f} \leftarrow$ furtherest pattern from $C$ to $\mathbf{P}$}
        \If {$d(\mathbf{P}, e) < d(\mathbf{P},q_{f})$ or $\lvert C \rvert < k$}
        \State {$W \leftarrow W \cup \{e\}$}
        \State {$C \leftarrow C \cup \{e\}$}
        \If{$\lvert C \rvert > k$}
        \State {remove furthest pattern from $C$ to $\mathbf{P}$} 
        \EndIf
        \EndIf
        \EndIf 
        \EndFor
    \EndWhile
    \end{algorithmic}
\end{algorithm}

After reaching the bottom layer, pattern candidates in list $C$ with a distance smaller than a threshold $\sigma$ are regarded as matched patterns. If the smallest distance in $C$ is still larger than $\sigma$, we regard it as a new pattern. This approach remains fast and accurate even when the graph grows large with new patterns continuously being inserted in the library.

The pattern library updates in an online style. For every new pattern that comes with no matched ones, the mask is optimized with from-scratch OPC iterations. The library will insert the pattern and optimized mask as new node and update the edge hierarchy of the graph to store the pattern. \Cref{alg:library update} shows the library online update step details.
\begin{figure}
    % \raggedleft
    \includegraphics[width=0.98\linewidth]{figs/hnsw} 
    \caption{Visualization of the graph-based pattern matching flow. Query design pattern $\mathbf{P}$ greedily traversing the hierarchical graph. The nearest node reached at layer 0 corresponds to a match pattern $\mathbf{P'}$ which has the most similar geometric shape with $\mathbf{P}$.}
    \label{fig:hnsw}
\end{figure}
\begin{algorithm}
    \small
    \caption{New Pattern Insertion and Graph Update}
    \label{alg:library update}
    \begin{algorithmic}[1]
        \State {\textbf{Input:}hierarchical graph $G$, new pattern $\mathbf{P}$, total layer number $L$, $G$'s starting nodes $q_{s}$, max degree $M$}
        \State {\textbf{Output:} updated hierarchical graph $G$}
        \State {$l \leftarrow random(0,L)$ //exponentially decaying probability}
        \For {$l_{c} \leftarrow L,..l$}
        \State {$C \leftarrow search(P, q_{s}, k, l_{c})$} \Comment{\Cref{alg:matching Strategy}}
        \State {$q_{s} \leftarrow$ nearest pattern of $q$ in $C$}
        \EndFor
        \For {$l_{c} \leftarrow l,.. 0$}
        \State {insert $\mathbf{P}$ to layer $l_{c}$ of $G$ // add $\mathbf{P}$ into graph} 
        \State{C $\leftarrow search(P, q_{s}, k, l_{c})$ } \Comment{\Cref{alg:matching Strategy}}
        \State{$ neighbors(\mathbf(P)) \leftarrow$ top $M$ nearest patterns in $C$}
        \For {$e \leftarrow neighbors(\mathbf{P})$}
        \State {add edge $(P, e)$}
        \If {degree of $e > M$}
        % \State {eConn $\leftarrow$ $neighbors(e)$}
        % \State {remove all edges connecting $e$}
        \State {$neighbors(e) \leftarrow$ top k nearest patterns connecting $e$}
        \State {remove all edges connecting $e$}
        \State {create edges $e$ with each one in $neighbors(e)$}
        \EndIf
        \EndFor
        \EndFor 
    \end{algorithmic}
\end{algorithm}

As \Cref{alg:library update} indicates, the new pattern will be inserted to one of the hierarchical layers with decaying probability. Edges will be added at the same layer between this pattern and top $k$ nearest ones. As the neighbor nodes will have degree increase, an edge re-connection of these neighbors will be conducted once degree is above the upper bound $k$. Therefore, the degree of each node in the graph is restricted by $k$. Note that the number of edges results in the matching complexity. Such a sparse hierarchical graph enables a fast search when the size of graph grows up. 

Since vectors of the same pattern are closer to each other, we proposed some distance metrics to evaluate the similarity of vectors. The inner product of two vectors can be used to evaluate the direction difference of vectors:
\begin{equation}
    Inner(\mathbf{V}_{\mathbf{P}_{1}},\mathbf{V}_{\mathbf{P}_{2}}) = \mathbf{V}_{\mathbf{P}_{1}} \cdot \mathbf{V}_{\mathbf{P}_{2}} = \sum_{i = 0}^{k} V_{\mathbf{P}_{1},i} V_{\mathbf{P}_{2},i} ,
\end{equation}
where $\mathbf{V}_{\mathbf{P}_{1}}$ and $\mathbf{V}_{\mathbf{P}_{2}}$ are the embedded vectors of pattern $\mathbf{P}_{1}$ and $\mathbf{P}_{2}$. $k = 256$ is the dimension of the embedded vector. Note that the inner product violates the positivity property where an element can be closer to some other element than to itself. 

\textbf{Consine similarity} avoid the violation thus can be utilized to measure the similarity between two vectors of an inner product space:
\begin{equation}
\begin{aligned}
    d_{Cosine}(\mathbf{V}_{\mathbf{P}_{1}},\mathbf{V}_{\mathbf{P}_{2}})
    &= 1.0 - \frac{\mathbf{V}_{\mathbf{P}_{1}} \cdot \mathbf{V}_{\mathbf{P}_{2}}}{ \left\lVert\mathbf{V}_{\mathbf{P}_{1}}\right\lVert \left\lVert\mathbf{V}_{\mathbf{P}_{2}}\right\lVert}, \\[6 pt]
    &= 1.0 - \frac{\sum_{i = 0}^{k} V_{\mathbf{P}_{1},i} V_{\mathbf{P}_{2},i}}{\sqrt{\sum_{i = 0}^{k} V_{\mathbf{P}_{1},i}^2 } \sqrt{\sum_{i = 0}^{k} V_{\mathbf{P}_{2}, i}^2}},
\end{aligned}
\end{equation}

\textbf{Euclidean Distance} is another approach where the embedding metric space is regarded as a Euclidean Space. Each vector represents its position in the Cartesian coordinates. The similarity of two vectors can be evaluated directly by calculating the squared $\mathscr{l}$-2 norm of the difference of the coordinates: 
\begin{equation}
    d_{Euclid}(\mathbf{V}_{\mathbf{P}_{1}},\mathbf{V}_{\mathbf{P}_{2}}) = \left\| \mathbf{V}_{\mathbf{P}_1} - \mathbf{V}_{\mathbf{P}_2} \right\|_{2}^{2} = \sqrt{\sum_{i=0}^{k} (V_{\mathbf{P}_{1},i} - V_{\mathbf{P}_{2},i})^2 }.
\end{equation}
All metrics abide by the rules of nearest neighbor search (NNS) are feasible similarity measurement metrics, which leave possibility for exploration of various metrics according to different embedding space. 
% For our implementation, the embedding space learned with the pattern feature loss mentioned in \Cref{loss}, the optimal pattern matching accurarcy is achieved by Euclidean distance measurement.

\subsection{Embedding Space Construction}\label{embedding}
To reuse mask stored in library we need to match a pattern with same geometric shape. However it is not straighforward to compare geometric similarity of two patterns directly. We develop a embedding metric space which reflects the geometric property using a high-dimensional vector representation $\mathbf{V}_{\mathbf{P}}$. Original $\langle \mathbf{P}, \mathbf{M}_{\mathbf{P}}\rangle$ pair stored in library is replaced with $\langle \mathbf{V}_{\mathbf{P}}, \mathbf{M}_{\mathbf{P}} \rangle$ pair. In this way, a decision of whether two patterns are the same can be determination by a similarity metric of two embedded vectors.

% \subsection{Pattern embedding.}
% \label{metric}
% \enspace Inspired by deep learning approahes for face recognization task\cite{zhan2018consensus}, of which the problem requirements are commonly shared: 
% \begin{enumerate*}
%     \item both hold large dataset to query and match;
%     \item both deal with unconstrained changes such as face expression change or pattern random shift;
%     \item both have rigorous tolerance for false-positive samples.
% \end{enumerate*}
We transform the embedding space construction into a feature extraction process by using deep learning model, and the metric space is built through deep metric learning. The embedded vector is the output of an embedding neural network. Deep learning model for the embedding process is composed of two modules: 
\begin{itemize}
    \item  \textit{Encoder}, $Enc(\cdot)$. For each input pattern $\mathbf{P}$, the encoder will encode the input pattern to a feature map $\mathbf{F}_{P} \in \mathbb{R}^{h\times w\times c}$, where $h, w$ are the spatial size of the feature map $\mathbf{F}_{P}$ and $c$ is the number of channels. 
    \item \textit{Projector}, $Proj(\cdot)$, which embeds feature map $\mathbf{F}_{P}$ to a representation vector $\mathbf{V}_{\mathbf{P}} \in \mathbb{R}^{k}$. The output $Proj(\mathbf{F}_{P})$ is normalized to the unit hypersphere in $\mathbb{R}^{k}$ at training stage for loss calculation. 
\end{itemize}
Therefore, the embedding process is formulated as:
\begin{equation}
    \mathbf{V}_{\mathbf{P}} = Proj(Enc(\mathbf{P})) \in \mathbb{R}^{k} .
\end{equation}
Previous deep learning-based OPC approaches \cite{yang2019gan, ye2019lithogan, chen2021damo} chose UNet or special variant UNet++ as the backbone structure. A harsh requirement of OPC problem strongly limits the selection of network backbone structure: the output mask will necessarily remain the same resolution as the input design.  

Embedding process without such limitation leaves us the flexibility to more network structure candidates.
We deliberately choose one of the most common structures: Resnet-18 \cite{he2016deep} as encoder. Each input pattern $\mathbf{P}$ is a $2048 \times 2048$ 2-D picture. 
In order to restrict the heavy computation caused time delay, we downsample the pattern  in a greedy manner until 256*256 before sending it into the neural network, without noticeable performance degradation. 
After the original Resnet-18 structure, an extra $1\times1$ convolution layer follows to shrink the feature channel size from 512 to 256.
A linear layer is put at the end of the neural network to transform the 3-D feature into the final 1-D embedded vector $\mathbf{V}_{\mathbf{P}}$. 
The size of $\mathbf{V}_{\mathbf{P}}$ is a trade-off where larger size indicates higher matching accuracy but slower similarity computation and matching speed. 
We select 256 through experiments to guarantee good performance with neglectable matching time.

% \subsection{Deep Metric Learning.}\label{loss}
The embedding space $\mathbf{S}$ is specially designed with certain objectives:
\begin{enumerate*}
    \item patterns of the same shape share similar embedded vectors with the shortest distance.
    \item patterns of different shapes clustered sparsely in the embedding space, far from each other.
\end{enumerate*}
The training process of such embedding requires abundant data of different patterns as well as data of the same patterns. During training, the data from the same pattern is regarded as positive samples, and the embedded vectors will be pushed as close as possible with higher similarity, while the embedded vectors of different patterns are regarded as negative samples pushed as far as possible. We crop a great amount of patches of pattern from a real full-scale design for the training dataset. 

\textbf{Data Preparation.} 
The cropping process has two steps to generate positive samples and negative samples as required by the training objective. The first step is to randomize some anchor points along the design layer. In the second step, with a random shift around each anchor point, we crop a certain number of patches, which will hold the same pattern within the square patch but with different relative positions. For each batch of training data, patterns will be labeled by their anchor point and thus abides by the positive/negative samples requirement. 

\textbf{Supervised Contrastive Loss.}
As we draw the picture of training the neural network to learn how to embed patterns to representative vectors, the traditional cross-entropy loss is not sufficiently sensitive to handle inter-class distance or noise labels. Among the family of losses based on metric distance learning \cite{hadsell2006dimensionality, wu2018unsupervised, hjelm2018learning}, Contrastive loss \cite{chen2020simple} is one of the most powerful losses for learning representative embedding in the self-supervised learning domain. Inspired by \cite{khosla2020supervised}, we extend the contrastive loss to supervised contrastive loss as all the positive/negative samples are genuinely generated and labeled at data preparation stage. As mentioned, representation vector $\mathbf{z}$ are normalized from $\mathbf{V}_{\mathbf{P}}$:
\begin{equation}
    \mathbf{z} = \text{normalize}(Proj(Enc(\mathbf{P})) ) \in \mathbb{R}^{k}  ,
\end{equation}
Then the loss function is formulated as:
\begin{equation} 
    \\[2pt]
     \mathcal{L}_{supCon} = -\sum_{i \in I} \frac{1}{|J(i)|} \sum_{j \in J(i)} \log \frac{\exp (\mathbf{z}_{i} \cdot \mathbf{z}_{j}/\tau)}{ \sum_{a \in A(i)} \exp(\mathbf{z}_{i} \cdot \mathbf{z}_{a} / \tau) } ,
\end{equation}
where $i \in I$ is anchor indices of the training batch. $j \in J$ is the anchor indices of the positive samples. $A(i) = I \backslash \{i\}$ is all anchor indices except for $i$ in this batch and therefore $A(i) \backslash \{J(i)\}$ are the anchor indices of negative samples. $\tau$ is a scalar temperature parameter. Term $\exp (\mathbf{z}_{i} \cdot \mathbf{z}_{j}/\tau)$ in numerator denotes similarity of positive sample pairs $\mathbf{z}_{i}$ and $\mathbf{z}_{j}$. $\exp (\mathbf{z}_{i} \cdot \mathbf{z}_{a}/\tau)$ in denominator denotes similarity of all sample pairs including negative ones. By minimizing the loss, the training process enlarges the similarity of positive samples and reduces the similarity of negative samples.
