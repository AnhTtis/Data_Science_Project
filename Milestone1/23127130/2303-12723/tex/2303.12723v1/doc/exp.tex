\section{Experimental Results}

%\subsection{Implementation details}
Our framework is mainly developed in Python. All machine learning modules in our framework are implemented using PyTorch. Lithography and ILT modules are developed in C/C++ with CUDA toolkit. All performance and speed experiments are conducted on CentOS-7 system with Intel i7-5930K 3.50GHz CPU and Nvidia GTX Titan X GPU. We choose the public lithography engine from ICCAD 2013 CAD Contest \cite{banerjee2013iccad} with the 24 optical kernels. The photoresist intensity threshold is set at 0.055. We adopt lithography wavelength $193nm$ with defocus range of $\pm 25nm$ and dose range of $\pm 2\%$. EPE violation threshold $th_{EPE}$ is set to $15nm$. 


%\subsection{Data Preparation}
In accordance with the authentic OPC scenario, all data used in our experiments is from a real design which we extracted from a gds file generated by the open-source layout generation tool OpenROAD \cite{ajayi2019openroad}. We sliced patterns of size $2048\times 2048$ in alignment with previous work~\cite{yang2019gan,yu2021gpu,chen2021damo} from a full scale via layer with more than $1.9 \times 10^6$ vias, where each pixel represents $1nm^2$. 
For the training dataset of ML-Solver, we randomly slice 4000 patterns off the design layer with masks optimized by ILT Solver. We use the same patterns to train the pattern classifier. As for critical/non-critical labels, we directly apply lithography and label them by EPE number, which is intuitive as it reflects the mask optimization difficulty. 
For the training data of Metric Space embedding, we follow the steps in \Cref{embedding}, using 400 random anchor points and shift around each anchor point within $\pm10\%$ pattern width for 400 repeating patterns. The number of positive samples for each point should be equal to or larger than the number of anchors to guarantee a positive/negative ratio of each training batch.

\begin{figure}
    \hspace*{-0.3cm}\subfloat[]{ \label{fig:epe1} \input{figs/matching_time.tex} } 
    \hspace*{-0.4cm}\subfloat[]{ \label{fig:breakdown} \includegraphics[width=.5\linewidth]{figs/pie-chart-1.pdf} }
    \caption{(a) EPE convergence comparison (b) Runtime breakdown of AdaOPC on critical patterns}
    % \label{fig:epe_pvband}
\end{figure}

\begin{table}[tb!]
    \centering
   \renewcommand{\arraystretch}{0.9} 
    \begin{tabular}{c|ccc}
        \toprule
        Library Size & 128-D & 256-D & 512-D\\
        \midrule
        100 & 0.9ms& 1.3ms& 1.5ms\\
        500 & 3.4ms& 5.7ms& 8.8ms\\
        1000 & 9.0ms& 13.3ms& 22.2ms\\
        2000 & 20.4ms& 31.2ms& 52.2ms\\
        5000 & 59.8ms& 93.0ms& 156.6ms\\
        10000 & 130.1ms& 206.5ms& 413.6ms\\
        \bottomrule
    \end{tabular}
    \caption{Pattern matching speed Analysis on different embedding dimension.}
    \label{table:matching_time}
\end{table}

\begin{figure*}[tb!]
    \input{figs/iterations.tex} 
    \caption{Mask convergence speed comparison with/without Pattern Matching. The time of mask optimization without pattern matching is normalized to 1 to demonstrate the acceleration ratio clearly. Optimization with pattern matching takes calibrated mask as the initial state. }
    \label{exp:iterations}
\end{figure*}

\begin{table*}[tb!]
    \centering
    \caption{Comparisons of baseline approaches}
    \renewcommand{\arraystretch}{0.9}
    \resizebox{0.68\linewidth}{!}
    {
    \begin{tabular}{c|ccc|ccc|ccc}
        \toprule
         Test Case & \multicolumn{3}{c}{DAMO-DGS \cite{chen2021damo}} & \multicolumn{3}{c}{ILT-GPU \cite{gao2014mosaic}} & \multicolumn{3}{c}{AdaOPC} \\ 
         ID&\#EPE &PVB ($nm^{2}$) & RT (s)&\#EPE &PVB ($nm^{2}$) & RT (s)&\#EPE &PVB ($nm^{2}$) & RT (s)\\ 
        \midrule
         1 &22 &23323 & 5.20&23 &23329 &41.15 &22 &23232 &5.50\\
         2 &26 &26729 & 5.26&25 &26762 &48.5 &24 &26580 &5.41\\
         3 &27 &26938 & 5.22&24 &26720 &55.92 &24 &26718 &5.37\\
         4 &36 &27975 & 5.18&29 &28127 &70.57 &25 &27934 &5.40\\
         5 &35 &28805 & 5.32&30 &28925 &66.89 &30 &28927 &5.44\\
         6 &30 &26960 & 5.31&25 &26762 &55.81 &24 &26775 &5.38\\
         7 &33 &26382 & 5.23&28 &26453 &59.47 &28 &26281 &5.43\\
         8 &32 &30646 & 5.38&25 &29450 &54.88 &27 &29341 &5.42\\
         9 &25 &24054 & 5.25&24 &24053 &70.62 &23 &24022 &5.43\\
         10 &24 &21939 & 5.29&23 &21701 &37.59 &22 &21644 &5.53\\
         \midrule
         Avg. &29.0 &26375 &\textbf{5.26} &25.6 &26228 &56.14 &\textbf{24.9} &\textbf{26145} &5.43\\ 
         Ratio &1.165 &1.009 &\textbf{0.970} &1.028 &1.003 &10.340 &\textbf{1.000} &\textbf{1.000} &1.000 \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:main result}
\end{table*}

\subsection{Performance Analysis}
In the begining, we validate the effectiveness of mask reuse by observing EPE descending trend. Firstly we present a demo experiment on a pattern, recording the EPE descending trend of the mask during ILT iterations with a calibrated optimized mask as the initial state. In comparison, we also record the trend with no initial mask and start the ILT process from scratch. As \Cref{fig:epe1} shows, EPE number with an initial mask starts at nearly optimal 23, and the descending trend converged at 1st iteration. In contrast, ILT from scratch gives 37 EPEs initially and takes six iterations to reach the initial EPE number with mask reuse, and overall 12 iterations to converge to 22. 

To verify the efficiency of our framework, we experiment with the runtime analysis. Since non-critical patterns are handled by extremely fast machine learning-based methods, we mainly focus on critical patterns. \Cref{fig:breakdown} visually demonstrates the time cost proportion of each step for critical pattern OPC in AdaOPC. We can see that 91.7\% running time is spent on lithography and ILT OPC iterations while the time overhead of pattern matching and shift calibration combined is only 8.3\%, which is trivial to the whole process. This also verifies the extensibility of our framework, leaving room for new powerful and fast OPC tools or litho-model to be imported for further speed up. Moreover, we also considered the condition when the pattern library gets larger. Although we cannot generate too many "ground-truth" masks due to time and computation resource limits, we can test the pattern matching speed with a large number of synthesized pattern vectors. \Cref{table:matching_time} has shown the matching speed of different pattern library sizes and embedding vector dimension combinations, from which we can see that even with the pattern library enlarged to 10000 stored patterns with dimension 512, the query and matching can still be finished in 0.4s. The time overhead is fast enough to be ignored in the whole process. \Cref{exp:iterations} shows the time comparison for mask convergence speed with or without pattern, 10 cases were tested after 800 patterns were inserted into the pattern library. Iterations of mask updates required can be significantly reduced by 93.6\% on average. 

At last, we need to verify the overall performance of our AdaOPC framework by comparing it with two baseline methods that we used in different branches for critical/non-critical patterns. After inserting 800 patterns with optimized masks into the library, we randomly tested 10 patterns, and \Cref{tab:main result} shows that AdaOPC can achieve comparable EPE/PVBand performance with ILT-GPU approach with $10\times$ acceleration and no accuracy loss. On the other hand, when compared with DAMO-DMG with 1 round lithography verification, AdaOPC shows much better performance with comparable speed. Notice that in our case, the embedding vector dimension is 256. As shown in \Cref{table:matching_time}, even if the library gets large to 10000 patterns, the matching time is still around 0.2s, which is marginal to the complete OPC process time cost shown in \Cref{tab:main result}. To summarize, the AdaOPC framework achieves performance-speed dual optimization.