\section{AdapOPC Framework}

\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{figs/workflow.pdf}
    \caption{Overall workflow of AdaOPC. Colored blocks are functional modules. Red dashed lines represent library update flow.}
    \label{fig:workflow}
\end{figure*}

\subsection{Overview}
Our proposed workflow is visualized in \Cref{fig:workflow}.
In \Cref{solverpool}, we firstly introduce the extensible solver selection module to choose OPC solver for different patterns. 
In \Cref{library} we will demonstrate the dynamic graph-based Pattern library, one of the key components of AdapOPC framework for mask Optimization efficiency. 
An approximate nearest neighbor searching(ANNS) method will be utilized to match similar patterns. 
\Cref{embedding} described how we embed pattern into high dimensional vectors for pattern matching in the library.
\Cref{shift} will further discuss the mask reusability and problem of pattern shift, along with the solution by shift calibration.

% \subsection{Adaptive Pattern Recognizer} \label{recognizer}
% As we discussed previously, machine learning based methods face the challenge of uncertainty of performance despite its remarkable advantage of fast inference speed. 
% As far as we concerned, state-of-art Deep learning approaches already reached good performance for an overall evaluation on a test set of patterns.
% However, the problem of OPC has minimal tolerance of error caused by lithography. The process strictly requires the corrected mask to be printable.

\subsection{Extensible OPC Solver Selection} \label{solverpool}
Our framework maintains a flexible solver pool to select suitable OPC solution for different patterns based on their complexity. We divide sliced design patterns into two groups: critical and non-critical patterns. Then we use a solver selector to choose which OPC solver to use. This Solver selector can be regarded as a 2-class classifier and built with a simple deep learning classification model. We use Resnet-18 \cite{he2016deep} as backbone network with objective to minimize the cross-entropy loss $L$:
\begin{equation}
    \mathcal{L} = - \frac{1}{N} \sum_{i}^{N} y_{i}\log(p_{i}) + (1 - y_{i})\log(1 - p_{i}).
\end{equation}
where $y_{i}$ is the 1/0 label for critcal pattern of sample $i$, $p_{i}$ is the probability predicted by classifier model. The model is trained to predict $p_{i}$ as close as $p_{i}$. Without bells and whistles, such simple network + loss combination is capable of performing a fast and accurate prediction on pattern class.

\subsubsection{ML-Solver.}~\\
 \enspace \indent For non-critical patterns, we build our ML-Solver using a generative neural network model in accordance with the SOTA ML-based via layer OPC solver DAMO-DMG \cite{chen2021damo}. We also pick U-Net++ with residual blocks inserted in the bottleneck as our model structure and train the generative model with the same strategy as \cite{chen2021damo}. The only difference is the training data. We prepare our own training dataset with patterns from a real full-scale design and mask generated by a robust ILT OPC engine, of which the lithography model is an authentic one instead of a DNN simulator as in \cite{chen2021damo}. Such data preparation aligns with the real OPC scenario where the only groundtruth we have is the lithography model.

\subsubsection{ILT-Solver.}~\\
\enspace \indent For critical patterns, we use conventional ILT-based method as \cite{gao2014mosaic} with GPU acceleration by CUDA, despite that deep learning approaches already reached good performance for an overall evaluation on a test set of patterns. Although a data-driven blackbox deep learning model may learn to mimic and reverse the diffraction effect very well, DNN models might have difficulty dealing with optical interference of incident light caused by complex neighboring vias. On the other hand, ILT methods are flexible analytical solutions which iteratively optimize mask through many rounds of lithography.
\hfill\\
\\
\enspace \indent Note that the solver pool is extensible. Any powerful OPC solutions with certain strengths for certain patterns hold the possibility to be imported as a replacement or complementary candidate. If more than three solvers are in the solver pool, we can simply modify the classifier loss:
\begin{equation}
    \mathcal{L} = -\frac{1}{N}\sum_{i}^{N}\sum_{c=1}^{C} y_{ic}\log(p_{ic}).
\end{equation}
to transform into a multiclass problem where $C$ is the number of pattern classes, the same as number of corresponding OPC solvers.

\subsection{Embedding Space Construction}\label{embedding}
To reuse mask stored in library, we need to match a pattern with the same geometric shape. However it is not straightforward to compare the geometric similarity of two patterns directly. We develop an embedding metric space which reflects the geometric property using a high-dimensional vector representation $\mathbf{V}_{\mathbf{P}}$. Original $\langle \mathbf{P}, \mathbf{M}_{\mathbf{P}}\rangle$ pair stored in library is replaced with $\langle \mathbf{V}_{\mathbf{P}}, \mathbf{M}_{\mathbf{P}} \rangle$ pair. In this way, a decision of whether two patterns are the same can be determination by a similarity metric of two embedded vectors.

\subsubsection{Pattern embedding.}~\\
\label{metric}
% \enspace Inspired by deep learning approahes for face recognization task\cite{zhan2018consensus}, of which the problem requirements are commonly shared: 
% \begin{enumerate*}
%     \item both hold large dataset to query and match;
%     \item both deal with unconstrained changes such as face expression change or pattern random shift;
%     \item both have rigorous tolerance for false-positive samples.
% \end{enumerate*}
\enspace \indent We transform the embedding space construction into a feature extraction process by using deep learning model and the metric space is built through deep metric learning. The embedded vector is the output of an embedding neural network. Deep learning model for the embedding process is composed of two modules: 
\begin{itemize}
    \item  \textit{Encoder}, $Enc(\cdot)$. For each input pattern $\mathbf{P}$, the encoder will encode the input pattern to a feature map $\mathbf{F}_{P} \in \mathbb{R}^{h\times w\times c}$, where $h, w$ are the spatial size of the feature map $\mathbf{F}_{P}$ and $c$ is the number of channels. 
    \item \textit{Projector}, $Proj(\cdot)$, which embeds feature map $\mathbf{F}_{P}$ to a representation vector $\mathbf{V}_{\mathbf{P}} \in \mathbb{R}^{k}$. the output $Proj(\mathbf{F}_{P})$ is normalized to the unit hypersphere in $\mathbb{R}^{k}$ at training stage for loss calculation. 
\end{itemize}
Therefore, the embedding process is formulated as:
\begin{equation}
    \mathbf{V}_{\mathbf{P}} = Proj(Enc(\mathbf{P})) \in \mathbb{R}^{k} .
\end{equation}
Previous deep learning-based OPC approaches \cite{yang2019gan, ye2019lithogan, chen2021damo} chose UNet or special variant UNet++ as the backbone structure. A harsh requirement of OPC problem strongly limits the selection of network backbone structure: the output mask will necessarily remain the same resolution as the input design.  

Embedding process without such limitation leaves us the flexibility to more network structure candidates.
We delibrately choose one of the most common structures: Resnet-18 \cite{he2016deep} as encoder. Each input pattern $\mathbf{P}$ is a $2048 \times 2048$ 2-D picture. 
In order to restrict the heavy computation caused time delay, we downsample the pattern in a greedy manner until 256*256 before sending into neural network, without noticeable performance degradation. 
After the original resnet-18 structure, an extra $1\times1$ convolution layer follows to shrink down the feature channel size from 512 to 256.
A linear layer is put at the end of the neural network to transform the 3-D feature into the final 1-D embedded vector $\mathbf{V}_{\mathbf{P}}$. 
The size of $\mathbf{V}_{\mathbf{P}}$ is trade-off while a larger size indicates higher matching accuracy but slower similarity computation and matching speed. 
We select 256 through experiments to guarantee good performance with neglectable matching time.

\subsubsection{Deep Metric Learning.}~\\ \label{loss}
\enspace The embedding space $\mathbf{S}$ is specially designed with certain objectives:
\begin{enumerate*}
    \item patterns of same shape share similar embedded vectors with shortest distance.
    \item patterns of different shape clustered sparsely in the embedding space, far from each other.
\end{enumerate*}
The training process of such embedding requires abundant data of different patterns as well as data of the same patterns. During training, the data from the same pattern is regarded as positive samples and the embedded vectors will be pushed as close as possible with higher similarity while the embedded vectors of different patterns are regarded as negative samples pushed as far as possible. We crop a great number of patches of pattern from a real full-scale design for the training dataset. 

\textbf{Data Preparation.} 
The cropping process has two steps to generate positive samples and negative samples as required by training objective. The first step is to randomize some anchor points along with the design layer. In the second step, with a random shift around each anchor point, we crop a certain number of patches, which will hold same pattern within the square patch but with different relative positions. For each batch of training data, patterns will be labeled by their anchor point and thus abides by the positive/negative samples requirement. 

\textbf{Supervised Contrastive Loss.}
As we draw the picture of training the neural network to learn how to embed patterns to representative vectors, the traditional cross-entropy loss is not sufficiently sensitive to handle inter-class distance or noise labels. Among the family of losses based on metric distance learning \cite{hadsell2006dimensionality, wu2018unsupervised, hjelm2018learning}, Contrastive loss \cite{chen2020simple} is one of the most powerful losses for learning representative embedding in self-supervised learning domain. Inspired by \cite{khosla2020supervised}, we extend the contrastive loss to supervised contrastive loss as all the positive/negative samples are genuinely generated and labeled at data preparation stage. As mentioned in \Cref{metric}, representation vector $\mathbf{z}$ are normalized from $\mathbf{V}_{\mathbf{P}}$:
\begin{equation}
    \mathbf{z} = \text{normalize}(Proj(Enc(\mathbf{P})) ) \in \mathbb{R}^{k}  ,
\end{equation}
Then the loss function is formulated as:
\begin{equation} 
    \\[2pt]
     \mathcal{L}_{supCon} = -\sum_{i \in I} \frac{1}{|J(i)|} \sum_{j \in J(i)} \log \frac{\exp (\mathbf{z}_{i} \cdot \mathbf{z}_{j}/\tau)}{ \sum_{a \in A(i)} \exp(\mathbf{z}_{i} \cdot \mathbf{z}_{a} / \tau) }.
\end{equation}
where $i \in I$ is anchor indices of the training batch. $j \in J$ is the anchor indices of the positive samples. $A(i) = I \backslash \{i\}$ is all anchor indices except for $i$ in this batch and therefore $A(i) \backslash \{J(i)\}$ are the anchor indices of negative samples. $\tau$ is a scalar temperature parameter. Term $\exp (\mathbf{z}_{i} \cdot \mathbf{z}_{j}/\tau)$ in numerator denotes similarity of positive sample pairs $\mathbf{z}_{i}$ and $\mathbf{z}_{j}$. $\exp (\mathbf{z}_{i} \cdot \mathbf{z}_{j}/\tau)$ in denominator denotes similarity of all sample pairs including negative ones. By minimizing the loss, training process enlarges the similarity of positive samples and reduces the similarity of negative samples.

\subsection{Pattern Library With Online Update}\label{library}

We build a dynamic pattern library to store the pattern pairs: sliced pattern $\mathbf{P}$ and their corresponding post-OPC mask $\mathbf{M}_{\mathbf{P}}$, hence to reuse the mask of repeating pattern to avoid the redundant time consumption of iterations-from-scratch OPC process. The key idea is to identify a stored repeating pattern before OPC. An online update mechanism enables a brand new pattern and corresponding mask to be inserted into the library.

We construct the pattern library with a graph structure as inspired by \cite{malkov2018efficient}, where each node represents a pattern stored. Each edge connecting two nodes shows they are neighbors of each other with more similarity. Considering the number of patterns on a full design layout, the size of the graph is gigantic. Naive search for shortest distance cost pair-wise distance comparisons of nodes in the graph which is impractical. We improve the matching time efficiency with:
\begin{enumerate}
    \item Sparse neighborhood graph structure, where nodes distant from each other are sparsely connected.
    \item Graph is divided into hierarchical layers. Nodes have restricted degree at each layer. more edges reside at lower layers, enabling greedy search of the nearest neighbor at each layer.
\end{enumerate}
The visualization of the hierarchical sparse graph structure and the pattern matching search is in \Cref{fig:hnsw}. 

% \begin{figure}
%     % \raggedleft
%     \includegraphics[width=0.9\linewidth]{figs/hnsw} 
%     \caption{visualization of the graph-based pattern matching flow. Query design pattern $\mathbf{P}$ greedily traversing the hierarchical graph. The nearest node reached at layer 0 conrresponds to a match pattern $\mathbf{P'}$ which has most similar geometric shape with $\mathbf{P}$.}
%     \label{fig:hnsw}
% \end{figure}


\subsubsection{Pattern Matching.}~\\
\enspace \indent  As shown in \Cref{fig:hnsw}, the pattern matching follows a greedy strategy to traverse the graph from higher layers to the bottom layer. A list of nearest pattern node candidates is maintained through the top-down traversal. The list is updated when a closer pattern appears during searching that has distance shorter than one of the candidates. Such matching strategy is based on proximity graph nearest neighbors search. The detailed pattern matching search strategy at each hierarchical layer is illustrated in \Cref{alg:matching Strategy}.
\begin{figure}
    % \raggedleft
    \includegraphics[width=0.9\linewidth]{figs/hnsw} 
    \caption{visualization of the graph-based pattern matching flow. Query design pattern $\mathbf{P}$ greedily traversing the hierarchical graph. The nearest node reached at layer 0 conrresponds to a match pattern $\mathbf{P'}$ which has most similar geometric shape with $\mathbf{P}$.}
    \label{fig:hnsw}
\end{figure}
\begin{algorithm}
    \small
    \caption{Graph-Based Pattern Matching Greedy Search}
    \label{alg:matching Strategy}
    \begin{algorithmic}[1]
    \State {\textbf{Input:} query pattern $\mathbf{P}$, starting nodes $q_{s}$, number of nearest neighbor to return $k$,layer number $l$, distance measurement $d(\cdot)$} 
    \State {\textbf{Output:} nearest pattern candidates $C$}
    \State {$V \leftarrow q_{s}$} // Visited nodes
    \State {$W \leftarrow q_{s}$} // Waiting list of nodes to visit
    \State {$C \leftarrow q_{s}$}
    \While {$\lvert W \rvert > 0$}
        \State {$q^{*} \leftarrow$ nearest pattern from $W$ to $\mathbf{P}$}
        \State {$q_{f} \leftarrow$ furthest pattern from $C$ to $\mathbf{P}$}
        \If {$d(\mathbf{P}, q^{*}) > d(\mathbf{P}, q_{f})$}
        \State {break}
        \EndIf
        \For {$e \in neighbor(f^{*})$ in layer $l$}
        \If {$e \not\in V$}
        \State {$V \leftarrow V \cup \{e\}$}
        \State {$q_{f} \leftarrow$ furtherest pattern from $C$ to $\mathbf{P}$}
        \If {$d(\mathbf{P}, e) < d(\mathbf{P},q_{f})$ or $\lvert C \rvert < k$}
        \State {$W \leftarrow W \cup \{e\}$}
        \State {$C \leftarrow C \cup \{e\}$}
        \If{$\lvert C \rvert > k$}
        \State {remove furthest pattern from $C$ to $\mathbf{P}$} 
        \EndIf
        \EndIf
        \EndIf 
        \EndFor
    \EndWhile
    \end{algorithmic}
\end{algorithm}

After reaching the bottom layer, pattern candidates in list $C$ with distance smaller than a threshold $\sigma$ are regarded as matched patterns. If the smallest distance in $C$ is still larger than $\sigma$, we regard it as a new pattern. This approach remains fast and accurate even when the graph grows large with new patterns continuously being inserted in library.

\subsubsection{Library update.}~\\
\enspace \indent  The pattern library updates in an online style. For every new pattern that comes with no matched ones, the mask is optimized with from-scratch OPC iterations. The library will insert the pattern and optimized mask as new node and update the edge hierarchy of the graph to store the pattern. \Cref{alg:library update} shows the library online update step details.
\begin{algorithm}
    \small
    \caption{New Pattern Insertion and Graph Update}
    \label{alg:library update}
    \begin{algorithmic}[1]
        \State {\textbf{Input:}hierarchical graph $G$, new pattern $\mathbf{P}$, total layer number $L$, $G$'s starting nodes $q_{s}$, max degree $M$}
        \State {\textbf{Output:} updated hierarchical graph $G$}
        \State {$l \leftarrow random(0,L)$ //exponentially decaying probability}
        \For {$l_{c} \leftarrow L,..l$}
        \State {$C \leftarrow search(P, q_{s}, k, l_{c})$} \Comment{\Cref{alg:matching Strategy}}
        \State {$q_{s} \leftarrow$ nearest pattern of $q$ in $C$}
        \EndFor
        \For {$l_{c} \leftarrow l,.. 0$}
        \State {insert $\mathbf{P}$ to layer $l_{c}$ of $G$ // add $\mathbf{P}$ into graph} 
        \State{C $\leftarrow search(P, q_{s}, k, l_{c})$ } \Comment{\Cref{alg:matching Strategy}}
        \State{$ neighbors(\mathbf(P)) \leftarrow$ top $M$ nearest patterns in $C$}
        \For {$e \leftarrow neighbors(\mathbf{P})$}
        \State {add edge $(P, e)$}
        \If {degree of $e > M$}
        % \State {eConn $\leftarrow$ $neighbors(e)$}
        % \State {remove all edges connecting $e$}
        \State {$neighbors(e) \leftarrow$ top k nearest patterns connecting $e$}
        \State {remove all edges connecting $e$}
        \State {create edges $e$ with each one in $neighbors(e)$}
        \EndIf
        \EndFor
        \EndFor 
    \end{algorithmic}
\end{algorithm}

As \Cref{alg:library update} indicates, the new pattern will be inserted to one of the hierarchical layers with decaying probability. Edges will be added at the same layer between this pattern and top $k$ nearest ones. As the neighbor nodes will have a degree increase, an edge re-connection of these neighbors will be conducted once the degree is above the upper bound $k$. Therefore, the degree of each node in graph is restricted by $k$. Note that the number of edges results in the matching complexity. Such a sparse hierarchical graph enables a fast search when the size of graph grows up. 

\subsubsection{Similarity measurement.} ~\\
\label{distance} 
Given the conclusion from \Cref{metric} that vectors of same pattern are closer with each other, we proposed some distance metrics to evaluate the similarity of vectors. Inner product of two vectors can be used to evaluate the direction difference of vectors:
\begin{equation}
    Inner(\mathbf{V}_{\mathbf{P}_{1}},\mathbf{V}_{\mathbf{P}_{2}}) = \mathbf{V}_{\mathbf{P}_{1}} \cdot \mathbf{V}_{\mathbf{P}_{2}} = \sum_{i = 0}^{k} V_{\mathbf{P}_{1},i} V_{\mathbf{P}_{2},i},
\end{equation}
where $\mathbf{V}_{\mathbf{P}_{1}}$ and $\mathbf{V}_{\mathbf{P}_{2}}$ are the embedded vectors of pattern $\mathbf{P}_{1}$ and $\mathbf{P}_{2}$. $k = 256$ is the dimension of embedded vector. Note that inner product violates the positivity property where an element can be closer to some other element than to itself. 

\textbf{Consine similarity} avoid the violation thus can be utilized to measure the similarity between two vectors of an inner product space:
\begin{equation}
\begin{aligned}
    d_{Cosine}(\mathbf{V}_{\mathbf{P}_{1}},\mathbf{V}_{\mathbf{P}_{2}})
    &= 1.0 - \frac{\mathbf{V}_{\mathbf{P}_{1}} \cdot \mathbf{V}_{\mathbf{P}_{2}}}{ \left\lVert\mathbf{V}_{\mathbf{P}_{1}}\right\lVert \left\lVert\mathbf{V}_{\mathbf{P}_{2}}\right\lVert}, \\[6 pt]
    &= 1.0 - \frac{\sum_{i = 0}^{k} V_{\mathbf{P}_{1},i} V_{\mathbf{P}_{2},i}}{\sqrt{\sum_{i = 0}^{k} V_{\mathbf{P}_{1},i}^2 } \sqrt{\sum_{i = 0}^{k} V_{\mathbf{P}_{2}, i}^2}},
\end{aligned}
\end{equation}

\textbf{Euclidean Distance} is another approach where the embedding metric space is regarded as a Euclidean Space. Each vector represents its position in the Cartesian coordinates. The similarity of two vectors can be evaluated directly by calulating the squared $\mathscr{l}$-2 norm of the coordinates difference: 
\begin{equation}
    d_{Euclid}(\mathbf{V}_{\mathbf{P}_{1}},\mathbf{V}_{\mathbf{P}_{2}}) = \left\| \mathbf{V}_{\mathbf{P}_1} - \mathbf{V}_{\mathbf{P}_2} \right\|_{2}^{2} = \sqrt{\sum_{i=0}^{k} (V_{\mathbf{P}_{1},i} - V_{\mathbf{P}_{2},i})^2 }.
\end{equation}
All metrics abide by the rules of nearest neighbor search (NNS) are feasible similarity measurement metrics, which leave the possibility for exploration of various metrics according to different embedding spaces. For our implementation, the embedding space learned with the pattern feature loss mentioned in \Cref{loss}, the optimal pattern matching accuracy is achieved by Euclidean distance measurement.

\subsection{Mask Reuse With Shift Calibration}\label{shift}
% \begin{figure}
%     \includegraphics[width=0.9\linewidth]{figs/shift} 
%     \caption{Printed wafer image must share identical location shift to design pattern with no geometric shape distortion.}
%     \label{fig:shift}
% \end{figure}
\subsubsection{Mask Reusability.} ~\\
\enspace \indent  
% \enspace As mentioned in \Cref{intro}, repeatitive patterns can share mask for efficiency. If the query design pattern $\mathbf{P}$ has a matched design pattern $\mathbf{P'}$ stored in the library wtih same shape, We simply reuse the stored mask $\mathbf{M}_{\mathbf{P'}}$ of the matched reference pattern. An pattern location shift $(\Delta x, \Delta y)$ between $\mathbf{P}$ and $\mathbf{P'}$ is inevitable when the whole design is sliced into small patterns. The optimized masks after OPC shall hold same geometric shape but also with location shift $(\Delta x, \Delta y)$. Thus, we can simple correct the mas $\mathbf{M}_{\mathbf{P'}}$ with shift $(-\Delta x, -\Delta y)$ as the initial mask $\mathbf{M}_{\mathbf{P}}$.  


We mathematically prove the location shift remains unchanged before and after the lithography, in order to show the feasibility of the mask shift calibration approach, we denote the Hopkins diffraction model through lithography in \Cref{litho_model} as $Litho(\cdot)$ and the location shift as $\delta_{\Delta x, \Delta y}(\cdot)$, we show that:

\begin{theorem}[Shift Equivariance]
    \label{shift_theorem}
    Given pattern $\mathbf{P}$ and mask $\mathbf{M}_{\mathbf{P}}$ where
    \begin{equation}
        \mathbf{P} = Litho(\mathbf{M}_{\mathbf{P}}),
    \end{equation}
     The following statement always holds: 
    \begin{equation}
        \label{litho_equivariance}
        \delta_{\Delta x, \Delta y}(\mathbf{P}) = Litho(\delta_{\Delta x, \Delta y}(\mathbf{M}_{\mathbf{P}})).
    \end{equation}
\end{theorem}
\begin{proof}
    For any position (x,y) on pattern $\mathbf{P}$:
    \begin{equation}
        \begin{aligned}
            &\delta_{\Delta x, \Delta y}(\mathbf{P}(x,y)) = \mathbf{P}(x + \Delta x,y + \Delta y), \\
            &= \sum_{k = 1}^{N^2} w_{k} \left| h_{k}(x + \Delta x,y + \Delta y) \otimes \mathbf{M}_{\mathbf{P}}(x + \Delta x,y + \Delta y) \right|^{2}, \\
            &= \sum_{k = 1}^{N^2} w_{k} | \sum_{i=1}^{N} \sum_{j=1}^{N} h_{k}(i,j) \mathbf{M}_{\mathbf{P}}(x + \Delta x + i - \frac{N}{2}, y + \Delta y + j - \frac{N}{2})|^{2}, \\[4pt]
            &= \sum_{k = 1}^{N^2} w_{k} | \sum_{i=1}^{N} \sum_{j=1}^{N} h_{k}(i,j) \mathbf{M}_{\mathbf{P}}(x + i - \frac{N}{2}+ \Delta x, y + j - \frac{N}{2} + \Delta y)|^{2}, \\[4pt]
            & = \sum_{k = 1}^{N^2} w_{k} | \sum_{i=1}^{N} \sum_{j=1}^{N} h_{k}(i,j) \delta_{\Delta x, \Delta y}(\mathbf{M}_{\mathbf{P}}(x + i - \frac{N}{2}, y + j - \frac{N}{2}))|^{2}, \\[4pt]
            & = \sum_{k = 1}^{N^2} w_{k} \left| h_{k}(x,y) \otimes \delta_{\Delta x, \Delta y}(\mathbf{M}_{\mathbf{P}}(x,y)) \right|^{2}, \\[4 pt]
            & = Litho(\delta_{\Delta x, \Delta y}(\mathbf{M}_{\mathbf{P}}(x,y))) .
        \end{aligned}
    \end{equation}
    Then \Cref{litho_equivariance} is proved.
\end{proof}
\begin{figure}
    \includegraphics[width=0.9\linewidth]{figs/shift} 
    \caption{Printed wafer image must share identical location shift to design pattern with no geometric shape distortion.}
    \label{fig:shift}
\end{figure}

Since mask shift will only result in a printing shift after lithography, repeating patterns in design can share OPC-optimized mask with a simple shift correction. We pick matched mask $\mathbf{M}_{\mathbf{P'}}$ stored in pattern library and add correction $(-\Delta x, -\Delta y)$ to acquire the initial mask $\mathbf{M}_{\mathbf{P}}$ for $\mathbf{P}$.


\subsubsection{Pattern shift calibration.}~\\
\enspace \indent  We calculate the shift by computing the pixel-level similarity of two patterns  $\mathbf{P}$ and $\mathbf{P'}$. The pixel-wise \textbf{cross-correlation} of $\mathbf{P}$ and $\mathbf{P'}$ reflects the pixel-wise similarity where the pixel of highest response value on the correlation map is the position shift of center point $(x_{ctr}, y_{ctr})$. The cross-correlation computation of two large 2-D pattern is time-comsuming. The calculation process of cross-correlation equal to convolution of $\mathbf{P}$ and $Rotate(\mathbf{P'})$, where $Rotate(\cdot)$ denotes rotation of $180^{\circ}$:
\begin{equation}
    CrossCorr(\mathbf{P}, \mathbf{P'}) = Conv(\mathbf{P}, Rotate(\mathbf{P'})),
\end{equation}
We replace the calculation with Convolution and accelerate the computation with Fast Fourier Transform (FFT) \cite{vasilache2014fast}. The pattern shift can be calculated with:
\begin{equation}
    \begin{aligned}
        x^{*}, y^{*} &= \argmax_{x,y}\  Conv\_FFT (\mathbf{P}, Rotate(\mathbf{P'})), \\
        \Delta x &= x^{*} - x_{ctr},\  \Delta y = y^{*} - y_{ctr},
    \end{aligned}
\end{equation}
And the initial mask is corrected with:
\begin{equation}
    \mathbf{M}_{\mathbf{P}} = \delta_{-\Delta x, -\Delta y}(\mathbf{M}_{\mathbf{P'}}).
\end{equation}
In real application, we send calibrated mask into lithography model to verify the mask and into ILT solver for one or two further iterations if necessary, just in case of any noise caused by shift calibration operation.
We use the same pattern size as \cite{yang2019gan} $2048 \times 2048$. With our implementation, the shift calculation time is less than 0.25s on CPU.  