
%% bare_adv.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See: 
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the advanced use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE Computer
%% Society journal paper.
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


% IEEEtran V1.7 and later provides for these CLASSINPUT macros to allow the
% user to reprogram some IEEEtran.cls defaults if needed. These settings
% override the internal defaults of IEEEtran.cls regardless of which class
% options are used. Do not use these unless you have good reason to do so as
% they can result in nonIEEE compliant documents. User beware. ;)
%
%\newcommand{\CLASSINPUTbaselinestretch}{1.0} % baselinestretch
%\newcommand{\CLASSINPUTinnersidemargin}{1in} % inner side margin
%\newcommand{\CLASSINPUToutersidemargin}{1in} % outer side margin
%\newcommand{\CLASSINPUTtoptextmargin}{1in}   % top text margin
%\newcommand{\CLASSINPUTbottomtextmargin}{1in}% bottom text margin




%
\documentclass[10pt,journal,compsoc]{IEEEtran}
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}


% For Computer Society journals, IEEEtran defaults to the use of 
% Palatino/Palladio as is done in IEEE Computer Society journals.
% To go back to Times Roman, you can use this code:
%\renewcommand{\rmdefault}{ptm}\selectfont





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)



% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % The IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{nicefrac}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumitem}


% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%\usepackage{acronym}
% acronym.sty was written by Tobias Oetiker. This package provides tools for
% managing documents with large numbers of acronyms. (You don't *have* to
% use this package - unless you have a lot of acronyms, you may feel that
% such package management of them is bit of an overkill.)
% Do note that the acronym environment (which lists acronyms) will have a
% problem when used under IEEEtran.cls because acronym.sty relies on the
% description list environment - which IEEEtran.cls has customized for
% producing IEEE style lists. A workaround is to declared the longest
% label width via the IEEEtran.cls \IEEEiedlistdecl global control:
%
% \renewcommand{\IEEEiedlistdecl}{\IEEEsetlabelwidth{SONET}}
% \begin{acronym}
%
% \end{acronym}
% \renewcommand{\IEEEiedlistdecl}{\relax}% remember to reset \IEEEiedlistdecl
%
% instead of using the acronym environment's optional argument.
% The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/acronym


%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/pkg/mdwtools


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/pkg/eqparbox




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
% \fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:

% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix


%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.


% NOTE: PDF thumbnail features are not required in IEEE papers
%       and their use requires extra complexity and work.
%\ifCLASSINFOpdf
%  \usepackage[pdftex]{thumbpdf}
%\else
%  \usepackage[dvips]{thumbpdf}
%\fi
% thumbpdf.sty and its companion Perl utility were written by Heiko Oberdiek.
% It allows the user a way to produce PDF documents that contain fancy
% thumbnail images of each of the pages (which tools like acrobat reader can
% utilize). This is possible even when using dvi->ps->pdf workflow if the
% correct thumbpdf driver options are used. thumbpdf.sty incorporates the
% file containing the PDF thumbnail information (filename.tpm is used with
% dvips, filename.tpt is used with pdftex, where filename is the base name of
% your tex document) into the final ps or pdf output document. An external
% utility, the thumbpdf *Perl script* is needed to make these .tpm or .tpt
% thumbnail files from a .ps or .pdf version of the document (which obviously
% does not yet contain pdf thumbnails). Thus, one does a:
% 
% thumbpdf filename.pdf 
%
% to make a filename.tpt, and:
%
% thumbpdf --mode dvips filename.ps
%
% to make a filename.tpm which will then be loaded into the document by
% thumbpdf.sty the NEXT time the document is compiled (by pdflatex or
% latex->dvips->ps2pdf). Users must be careful to regenerate the .tpt and/or
% .tpm files if the main document changes and then to recompile the
% document to incorporate the revised thumbnails to ensure that thumbnails
% match the actual pages. It is easy to forget to do this!
% 
% Unix systems come with a Perl interpreter. However, MS Windows users
% will usually have to install a Perl interpreter so that the thumbpdf
% script can be run. The Ghostscript PS/PDF interpreter is also required.
% See the thumbpdf docs for details. The latest version and documentation
% can be obtained at.
% http://www.ctan.org/pkg/thumbpdf


% NOTE: PDF hyperlink and bookmark features are not required in IEEE
%       papers and their use requires extra complexity and work.
% *** IF USING HYPERREF BE SURE AND CHANGE THE EXAMPLE PDF ***
% *** TITLE/SUBJECT/AUTHOR/KEYWORDS INFO BELOW!!           ***
\newcommand{\embV}{\mathbf{V}}
\newcommand{\embU}{\mathbf{U}}
\newcommand{\embh}{\mathbf{h}}
\newcommand{\embH}{\mathbf{H}}
\newcommand{\embm}{\mathbf{m}}
\newcommand{\embg}{\mathbf{g}}
\newcommand{\embX}{\mathbf{X}}
\newcommand{\embx}{\mathbf{x}}
\newcommand{\embe}{\mathbf{e}}
\newcommand{\embv}{\mathbf{v}}

\newcommand{\hisH}{\overline{\mathbf{H}}}
\newcommand{\hish}{\overline{\mathbf{h}}}
\newcommand{\hisV}{\overline{\mathbf{V}}}
\newcommand{\hisv}{\overline{\mathbf{v}}}

\newcommand{\vecx}{\mathbf{x}}
\newcommand{\vecy}{\mathbf{y}}
\newcommand{\vecd}{\mathbf{d}}
\newcommand{\vecM}{\mathbf{M}}

\newcommand{\temH}{\widehat{\embH}}
\newcommand{\temh}{\widehat{\embh}}
\newcommand{\temV}{\widehat{\embV}}
\newcommand{\temv}{\widehat{\embv}}

\newcommand{\rmd}{\,\mathrm{d}}

\newcommand{\thetarec}{\theta^{\Diamond}}

\newcommand{\vb}{\mathcal{V}_\mathcal{B}}
\newcommand{\appH}{\widetilde{\embH}}

\newcommand{\update}{u}
\newcommand{\aggregate}{\oplus}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\inbatch}{\mathcal{V}_{\mathcal{B}}}
\newcommand{\compensation}{\mathbf{C}}
\newcommand{\jacobian}{\mathbf{J}}
\newcommand{\dropout}{\mathbf{Dropout}}
\newcommand{\neighbor}[1]{\mathcal{N}(#1)}
\newcommand{\kneighbor}[2]{\mathcal{N}^{#2}(#1)}
\newcommand{\fl}{f_{\theta^{l}}}

\newcommand{\acc}[2]{#1\textcolor{black!70!white}{\scriptsize{$\pm$#2}}}
\newcommand{\mr}[2]{\multirow{#1}{*}{#2}}
\newcommand{\mc}[3]{\multicolumn{#1}{#2}{#3}}
\newcommand{\udfsection}[1]{\noindent\textbf{#1}\, }

\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}

\newif\ifproof\prooftrue
% \newif\ifproof\prooffalse 

\newcommand{\showproof}[1]{\ifproof{#1}\else{}\fi}



% \newif\ifupdate\updatetrue
\newif\ifupdate\updatefalse 
\newcommand{\modify}[2]{\ifupdate{#1}\else{\color{red}#2}\fi}

\newif\ifupdateok\updateoktrue
% \newif\ifupdateok\updateokfalse 
\newcommand{\modifyok}[2]{\ifupdate{#1}\else{\color{black}#2}\fi}

\newif\ifswitch\switchtrue
% \newif\ifswitch\switchfalse 
\newcommand{\switch}[2]{\ifswitch{\color{red}#1}\else{\color{red}#2}\fi}

\newcommand\MYhyperrefoptions{bookmarks=true,bookmarksnumbered=true,
pdfpagemode={UseOutlines},plainpages=false,pdfpagelabels=true,
colorlinks=true,linkcolor={black},citecolor={black},urlcolor={black},
pdftitle={Bare Demo of IEEEtran.cls for Computer Society Journals},%<!CHANGE!
pdfsubject={Typesetting},%<!CHANGE!
pdfauthor={Michael D. Shell},%<!CHANGE!
pdfkeywords={Computer Society, IEEEtran, journal, LaTeX, paper,
             template}}%<^!CHANGE!
%\ifCLASSINFOpdf
%\usepackage[\MYhyperrefoptions,pdftex]{hyperref}
%\else
%\usepackage[\MYhyperrefoptions,breaklinks=true,dvips]{hyperref}
%\usepackage{breakurl}
%\fi
% One significant drawback of using hyperref under DVI output is that the
% LaTeX compiler cannot break URLs across lines or pages as can be done
% under pdfLaTeX's PDF output via the hyperref pdftex driver. This is
% probably the single most important capability distinction between the
% DVI and PDF output. Perhaps surprisingly, all the other PDF features
% (PDF bookmarks, thumbnails, etc.) can be preserved in
% .tex->.dvi->.ps->.pdf workflow if the respective packages/scripts are
% loaded/invoked with the correct driver options (dvips, etc.). 
% As most IEEE papers use URLs sparingly (mainly in the references), this
% may not be as big an issue as with other publications.
%
% That said, Vilar Camara Neto created his breakurl.sty package which
% permits hyperref to easily break URLs even in dvi mode.
% Note that breakurl, unlike most other packages, must be loaded
% AFTER hyperref. The latest version of breakurl and its documentation can
% be obtained at:
% http://www.ctan.org/pkg/breakurl
% breakurl.sty is not for use under pdflatex pdf mode.
%
% The advanced features offer by hyperref.sty are not required for IEEE
% submission, so users should weigh these features against the added
% complexity of use.
% The package options above demonstrate how to enable PDF bookmarks
% (a type of table of contents viewable in Acrobat Reader) as well as
% PDF document information (title, subject, author and keywords) that is
% viewable in Acrobat reader's Document_Properties menu. PDF document
% information is also used extensively to automate the cataloging of PDF
% documents. The above set of options ensures that hyperlinks will not be
% colored in the text and thus will not be visible in the printed page,
% but will be active on "mouse over". USING COLORS OR OTHER HIGHLIGHTING
% OF HYPERLINKS CAN RESULT IN DOCUMENT REJECTION BY THE IEEE, especially if
% these appear on the "printed" page. IF IN DOUBT, ASK THE RELEVANT
% SUBMISSION EDITOR. You may need to add the option hypertexnames=false if
% you used duplicate equation numbers, etc., but this should not be needed
% in normal IEEE work.
% The latest version of hyperref and its documentation can be obtained at:
% http://www.ctan.org/pkg/hyperref





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Provably Convergent Subgraph-wise Sampling for Fast GNN Training}


%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Jie~Wang,~\IEEEmembership{Senior Member,~IEEE,} Zhihao~Shi,~Xize~Liang,~Shuiwang~Ji,~\IEEEmembership{Fellow,~IEEE,}\\Bin~Li,~\IEEEmembership{Member,~IEEE,}~and~Feng~Wu,~\IEEEmembership{Fellow,~IEEE}% <-this % stops a space
\IEEEcompsocitemizethanks{
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
\IEEEcompsocthanksitem J. Wang, Z. Shi, X. Liang, B.~Li, and F. Wu are with: a) CAS Key Laboratory of Technology in GIPAS, University of Science and Technology of China, Hefei 230027, China; b) Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, Hefei 230091, China. E-mail: jiewangx@ustc.edu.cn, zhihaoshi@mail.ustc.edu.cn, xizeliang@mail.ustc.edu.cn, binli@ustc.edu.cn, fengwu@ustc.edu.cn.
\IEEEcompsocthanksitem S. Ji is with the Department of Computer Science and Engineering, Texas A\&M University, College Station, TX77843 USA. E-mail: sji@tamu.edu.
}% <-this % stops an unwanted space

\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Advanced Demo of IEEEtran.cls for IEEE Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society journal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
Subgraph-wise sampling---a promising class of mini-batch training techniques for graph neural networks (GNNs)---is critical for real-world applications.
During the message passing (MP) in GNNs, subgraph-wise sampling methods discard messages outside the mini-batches in backward passes to avoid the well-known \textit{neighbor explosion} problem, i.e., the exponentially increasing dependencies of nodes with the number of MP iterations.
However, discarding messages may sacrifice the gradient estimation accuracy, posing significant challenges to their convergence analysis and convergence speeds.
To address this challenge, we propose a novel subgraph-wise sampling method with a convergence guarantee, namely \textbf{L}ocal \textbf{M}essage \textbf{C}ompensation (LMC).
To the best of our knowledge, LMC is the \textit{first} subgraph-wise sampling method with provable convergence.
The key idea is to retrieve the discarded messages in backward passes based on a message passing formulation of backward passes.
By efficient and effective compensations for the discarded messages in both forward and backward passes, LMC computes accurate mini-batch gradients and thus accelerates convergence.
Moreover, LMC is applicable to various MP-based GNN architectures, including convolutional GNNs (finite message passing iterations with different layers) and recurrent GNNs (infinite message passing iterations with a shared layer).
Experiments on large-scale benchmarks demonstrate that LMC is significantly faster than state-of-the-art subgraph-wise sampling methods.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Graph Nerual Networks, Scalable Training, Provable Convergence, Local Message Compensation.
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when compsoc mode
% is not selected <OR> if conference mode is selected - because compsoc
% conference papers position the abstract like regular (non-compsoc)
% papers do!
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc under a non-conference mode.


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\ifCLASSOPTIONcompsoc
\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
\else
\section{Introduction}
\label{sec:introduction}
\fi
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.




% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.


\IEEEPARstart{G}{raph} neural networks (GNNs) are powerful frameworks that iteratively generate node representations depending on the structures of graphs and any feature information we might have \cite{grl}.
Recently, GNNs have achieved great success in many real-world applications involving graph-structured data, such as search engines \cite{gnn_recommendation}, recommendation systems \cite{gnn_social}, materials engineering \cite{gnn_material}, and molecular property prediction \cite{gnn_mol1, gnn_mol2}. 



Many GNNs use the message passing framework \cite{mpnn}. Message passing (MP) follows an iterative \textit{aggregation} and \textit{update} scheme.
At each MP iteration, GNNs aggregate messages from each node's neighborhood and then update node embeddings based on aggregation results.
Many graph neural networks for node property prediction are categorized into convolutional graph neural networks (ConvGNNs) and recurrent graph neural networks (RecGNNs) \cite{comprehensive}.
ConvGNNs \cite{gcn, gat, gin} perform $T$ MP iterations with different graph convolutional layers to learn node embeddings. % , capturing relation information up to $T$-hops away
However, despite their simplicity, the finite MP iterations cannot capture the dependency with a range longer than $T$-hops away from any given node \cite{ignn}.
Inspired by \cite{idl, deq}, many researchers have focused on RecGNNs recently, which approximate infinite MP iterations recurrently using a shared MP layer.
Many works \cite{recgnn1, contraction, fdgnn, ignn} demonstrate that RecGNNs can effectively capture long-range dependencies.



However, when the graph scale is large, the iterative MP scheme poses challenges to the training of GNNs.
To scale deep learning models (such as GNNs) to arbitrarily large-scale data with constrained GPU memory, a widely used approach is to approximate full-batch gradients with mini-batch gradients.
Nevertheless, in scenarios involving graph data, this approach may incur prohibitive computational costs for the loss across a mini-batch of nodes and the corresponding mini-batch gradients due to the notorious \textit{neighbor explosion} problem.
Specifically, the embedding of a node at the $k$-th MP iteration recursively depends on the embeddings of its neighbors at the $(k-1)$-th MP iteration.
Thus, the computational complexity increases exponentially with the number of MP iterations.




To deal with the neighbor explosion problem, various sampling 
techniques have recently emerged to reduce the number of nodes involved in message passing \cite{dlg}.
For instance, node-wise \cite{graphsage, vrgcn} and layer-wise \cite{fastgcn, ladies, adapt} sampling methods recursively sample neighbors over MP iterations to estimate node embeddings and corresponding mini-batch gradients.
Different from the recursive fashion, subgraph-wise sampling methods \cite{cluster_gcn, graphsaint, gas, shadow_gnn} adopt a cheap and simple one-shot sampling fashion, i.e., sampling the same subgraph constructed based on a mini-batch for different MP iterations.
By getting rid of messages outside the mini-batches, subgraph-wise sampling methods restrict the computation to the mini-batches, leading to the complexity growing linearly with the number of MP iterations.
Moreover, by directly running GNNs on the subgraphs constructed by mini-batches, we can apply subgraph-wise sampling methods to a wide range of GNN architectures without additional designs \cite{gas}.
Due to the above advantages, subgraph-wise sampling has been increasingly popular recently.




Despite the empirical success of subgraph-wise sampling methods, discarding messages outside the mini-batch decreases the gradient estimation accuracy, posing significant challenges to their convergence behaviors as follows. First, recent works \cite{vrgcn, mvs} demonstrate that the inaccurate mini-batch gradients seriously slow down the convergence speeds of GNNs.
Second, in Sections \ref{sec:smallbatchsize_conv} and \ref{sec:smallbatchsize_rec}, we demonstrate that under small batch sizes, which is commonly seen when the GPU memory is limited, many subgraph-wise sampling methods are hard to resemble full-batch performance. These problems seriously limit the real-world applications of GNNs.




In this paper, we propose a novel subgraph-wise sampling method with a convergence guarantee, namely \textbf{L}ocal \textbf{M}essage \textbf{C}ompensation (LMC), whose efficient and effective compensations can reduce the bias of mini-batch gradients and thus accelerates convergence. To the best of our knowledge, LMC is the {\it first} subgraph-wise sampling method with provable convergence.
Specifically, by \modifyok{developing unbiased mini-batch gradients for the one-shot sampling fashion \modify{}{and proposing the compensation mechanism}, \modify{we propose a bias-variance decomposition of the gradient computation errors, i.e., the bias from the discarded messages and the variance of the unbiased mini-batch gradients}{we derive the bias-variance decomposition of gradient computation errors, and show that the bias terms can tend to be arbitrarily small with appropriate learning rates}
(see Theorems \ref{thm:grad_error_conv} and \ref{thm:grad_error_rec}).}{formulating backward passes as message passing, we first develop unbiased mini-batch gradients for the one-shot sampling fashion (see Theorem \ref{thm:unbiased}).}
Then, during the approximation of unbiased mini-batch gradients, we retrieve the messages discarded by existing subgraph-wise sampling methods based on \modifyok{a message passing formulation of backward passes}{the aforementioned message passing formulation of backward passes}.
Finally, to avoid the exponentially growing time and memory consumption, we propose efficient and effective compensations for the discarded messages with the historical information in previous iterations.
An appealing feature of the compensation mechanism is that it can effectively reduce the biases of mini-batch gradients, leading to accurate gradient estimation and fast convergence speeds.
We further show that LMC converges to first-order stationary points of GNNs.
Notably, the convergence of LMC stems from the interactions between mini-batch nodes and their $1$-hop neighbors, without the need for recursive dependencies on neighborhoods far away from the mini-batches.



Another appealing feature is that LMC is applicable to MP-based GNN architectures, including ConvGNNs and RecGNNs \cite{comprehensive}.
The major challenges of ConvGNNs (finite message passing iterations with different layers) and RecGNNs (infinite message passing iterations with a shared layer) are layer-wise accumulated errors from the staleness of historical information and the convergence of \modifyok{infinite message passing iterations without a priori limitations on the number of MP layers}{recurrent embeddings without a priori limitations on the number of MP iterations} \cite{ignn, comprehensive, eignn}, respectively.
We implement LMC for the two aforementioned models respectively to tackle the problems. Experiments on large-scale benchmark tasks demonstrate that LMC significantly outperforms state-of-the-art subgraph-wise sampling methods in terms of efficiency. Moreover, under small batch sizes, LMC outperforms the baselines and resembles the prediction performance of full-batch methods.


An earlier version of this paper has been published at ICLR 2023 \cite{lmc}, which mainly focuses on ConvGNNs. This journal manuscript significantly extends the conference version to cover another popular GNN architectures with infinite layers, i.e., RecGNNs.
\modifyok{}{The major contribution of this extension is that we show the convergence of LMC under infinite MP iterations.}
To demonstrate that LMC significantly accelerates the training of RecGNNs, we provide detailed derivations, rigorous theoretical analysis, and extensive experiments in Sections \ref{subsubsec:lmc4rec_methodology}, \ref{subsubsec:lmc4rec_theoretical}, and \ref{sec:exp_recgnns}, respectively.


\section{Related Work}


In this section, we discuss some works related to our proposed method.



\subsection{Scalable Training for Graph Neural Networks}

In this section, we discuss the scalable training for graph neural networks including subgraph-wise and recursive (node-wise and layer-wise) sampling methods that are most related to our proposed method. Please refer to Appendix \ref{appendix:add_related} for other scalable training methods.

\udfsection{Subgraph-wise Sampling Methods.} Subgraph-wise sampling methods sample a mini-batch and then construct the same subgraph based on it for different MP layers \cite{dlg}.
For example, Cluster-GCN \cite{cluster_gcn} and GraphSAINT \cite{graphsaint} construct the subgraph induced by a sampled mini-batch. They encourage connections between the sampled nodes by graph clustering methods (e.g., METIS \cite{metis1} and Graclus \cite{graclus}), edge, node, or random-walk based samplers.
GNNAutoScale (GAS) \cite{gas} and MVS-GNN \cite{mvs} use historical embeddings to generate messages outside a sampled subgraph, maintaining the expressiveness of the original GNNs.
Subgraph-wise sampling methods are applicable to both ConvGNNs and RecGNNs by directly running them on the subgraphs constructed by the sampled mini-batches \cite{gas}.


\udfsection{Recursive Graph Sampling Methods.}
Both node-wise and layer-wise sampling methods recursively sample neighbors over MP layers and then construct different computation graphs for each MP iteration.
Node-wise sampling methods \cite{graphsage, vrgcn} aggregate messages from a small subset of sampled neighborhoods at each MP layer to decrease the bases in the exponentially increasing dependencies.
To avoid the exponentially growing computation, layer-wise sampling methods \cite{fastgcn, ladies, adapt} independently sample nodes for each MP layer and then use importance sampling to reduce variance, resulting in a constant sample size in each MP layer.
Many node-wise and layer-wise sampling methods are designed only for ConvGNNs. Because node-wise and layer-wise sampling methods may introduce random noise to the message passing iterations of RecGNNs, it is difficult to solve the fixed-point equations robustly (see Eqs. \eqref{eqn:transformation_rec} and \eqref{eqn:mpeq_grad}), posing challenges to the robust training for RecGNNs.





\subsection{Historical Values as an Affordable Approximation.} The historical values are affordable approximations of the exact values in practice. However, they suffer from frequent data transfers to/from the GPU and the staleness problem. For example, in node-wise sampling, VR-GCN \cite{vrgcn} uses historical embeddings to reduce the variance from neighbor sampling \cite{graphsage}.
GAS \cite{gas} proposes a concurrent mini-batch execution to transfer the active historical embeddings to and from the GPU, leading to comparable runtime with the standard full-batch approach.
GraphFM-IB and GraphFM-OB \cite{graphfm} apply a momentum step on historical embeddings for node-wise and subgraph-wise sampling methods with historical embeddings, respectively, to alleviate the staleness problem.
Both LMC and GraphFM-OB use the node embeddings in mini-batches to alleviate the staleness problem of the node embeddings outside mini-batches.
We discuss the main differences between LMC and GraphFM-OB in Appendix \ref{sec:diff_graphfm}.


\section{Preliminaries}
We introduce notations in Section \ref{sec:notations}. Then, we introduce convolutional graph neural networks and recurrent graph neural networks in Section \ref{sec:convgnn}.


\subsection{Notations}\label{sec:notations}
A graph {\small $\mathcal{G}=(\mathcal{V}, \mathcal{E})$} is defined by a set of nodes {\small$\mathcal{V}=\{v_1,v_2,\dots,v_n\}$} and a set of edges {\small $\mathcal{E}$} among these nodes. The set of nodes consists of labeled nodes {\small $\mathcal{V}_{L}$} and unlabeled nodes {\small $\mathcal{V}_{U}:=\mathcal{V} \setminus \mathcal{V}_{L}$}. The label of a node $v_i \in \mathcal{V}_L$ is $y_i$. Let {\small $(v_i,v_j)\in\mathcal{E}$} denote an edge going from node {\small $v_i\in\mathcal{V}$} to node {\small $v_j\in\mathcal{V}$}, {\small $\neighbor{v_i}=\{v_j\in\mathcal{V}| (v_i,v_j)\in\mathcal{E}\}$} denote the neighborhood of node {\small $v_i$}, and {\small $\overline{\mathcal{N}}(v_i)$} denote {\small$\neighbor{v_i} \cup \{v_i\}$}. We assume that {\small $\mathcal{G}$} is undirected, i.e., {\small $v_j \in \neighbor{v_i} \Leftrightarrow v_i \in \neighbor{v_j}$}. Let {\small $\neighbor{\mathcal{S}} = \{v\in\mathcal{V}| (v_i,v_j)\in\mathcal{E},v_i\in\mathcal{S}\}$} denote the neighborhoods of a set of nodes {\small $\mathcal{S}$} and {\small $\overline{\mathcal{N}}(\mathcal{S})=\neighbor{\mathcal{S}} \cup \mathcal{S}$}. For a positive integer {\small $L$}, {\small$[L]$} denotes {\small$\{1,\ldots,L\}$}. Let the boldface character {\small $\embx_{i} \in \mathbb{R}^{d_x}$} denote the feature of node {\small $v_i$} with dimension {\small $d_x$}. Let {\small $\embh_i\in\mathbb{R}^d$} be the {\small $d$}-dimensional embedding of the node {\small $v_i$}. Let {\small $\embX = (\embx_1,\embx_2,\dots,\embx_n) \in \mathbb{R}^{d_x \times n}$} and {\small $\embH  = (\embh_1,\embh_2,\dots,\embh_n) \in \mathbb{R}^{d \times n}$}. We also denote the embeddings of a set of nodes {\small $\mathcal{S}=\{v_{i_k}\}_{k=1}^{|\mathcal{S}|}$} by {\small $\embH_{\mathcal{S}}  = (\embh_{i_1}, \embh_{i_2}, \dots, \embh_{i_{|\mathcal{S}|}}) \in \mathbb{R}^{d \times |\mathcal{S}|}$}. For a {\small $p \times q$} matrix {\small $\mathbf{A}\in\mathbb{R}^{p\times q}$}, {\small $\Vec{\mathbf{A}} \in \mathbb{R}^{pq}$} denotes the vectorization of {\small $\mathbf{A}$}, i.e., {\small $\Vec{\mathbf{A}}_{i+(j-1)p} = \mathbf{A}_{ij}$}. We denote the {\small$j$}-th columns of {\small$\mathbf{A}$} by {\small $\mathbf{A}_{j}$}.

\subsection{Graph Neural Networks}\label{sec:convgnn}
For semi-supervised node-level prediction, Graph Neural Networks (GNNs) aim to learn node embeddings $\embH$ with parameters $\Theta$ by minimizing the objective function $\mathcal{L}=\frac{1}{|\mathcal{V}_L|}\sum_{i\in\mathcal{V}_L}\ell_{w}(\embh_i, y_i)$ such that
\begin{align}
\embH = \mathcal{GNN}(\embX, \mathcal{E};\Theta),
\end{align}
where $\ell_{w}$ is the composition of an output layer with parameters $w$ and a loss function.


GNNs follow the message passing framework in which vector messages are exchanged between nodes and updated using neural networks. Most GNNs are categorized into convolutional graph neural networks (ConvGNNs) and recurrent graph neural networks (RecGNNs) based on whether different message passing iterations share the same parameters \cite{comprehensive}.
Notably, using the same parameters potentially enable infinite message passing iterations \cite{ignn}.



An {\small$L$}-layer ConvGNN performs {\small$L$} message passing iterations with different parameters {\small$\Theta=(\theta^{l})_{l=1}^L$} to generate the final node embeddings {\small$\embH=\embH^{L}$} as
\begin{align}
    \embH^{l}=f_{\theta^{l}}(\embH^{l-1};\embX),\,\,l\in[L], \label{eqn:transformation_conv}
\end{align}
where {\small$\embH^{0}=\embX$} and {\small$f_{\theta^l}$} is the {\small$l$}-th message passing layer with parameters {\small$\theta^{l}$}.








Different from ConvGNNs, RecGNNs recurrently use the message passing layer \eqref{eqn:transformation_conv} with the same parameter $\theta^{l}=\thetarec, l \in \mathbb{N}^*$ until node embeddings $\embH^{l}$ converge to the fixed point
\begin{align}
    \embH^{\Diamond}=f_{\thetarec}(\embH^{\Diamond};\embX), \label{eqn:transformation_rec}
\end{align}
as the final node embeddings $\embH = \embH^{\Diamond}$.
Recent work \cite{ignn} shows that the recurrent embeddings of message passing \eqref{eqn:transformation_rec} converges if the well-posedness property holds (see Appendix \ref{sec:well-posedness}).
Compared with ConvGNNs, RecGNNs enjoy cheaper memory costs because of the shared parameters in different message passing iterations and can model dependencies between nodes that are any hops apart \cite{ignn}.
However, the training of RecGNNs is more unstable and inefficient than ConvGNNs due to the challenge in solving the fixed point equations \cite{ignn, eignn}.



The message passing layer $f_{\theta^{l}}$ follows an \textit{aggregation} and \textit{update} scheme, i.e.,
\begin{align}
    &\embh_i^l  =\update_{\theta^{l}}(\embh^{l-1}_i  , \embm^{l-1}_{\neighbor{v_i}}  ,\embx_i); \nonumber\\
    &\embm^{l-1}_{\neighbor{v_i}}  =\aggregate_{\theta^{l}}( \{g_{\theta^{l}}(\embh_j^{l-1}  ) \mid v_j\in\neighbor{v_i}\})\label{eqn:mpeq_update},
\end{align}
where $g_{\theta^{l}}$ is the function generating \textit{individual messages} for each neighborhood of $v_i$ of the $l$-th message passing iteration, $\aggregate_{\theta^{l}}$ is the aggregation function mapping from a set of messages to the final message $\embm^{l-1}_{\neighbor{v_i}}$, and $\update_{\theta^{l}}$ is the update function that combines previous node embedding $\embh_i^{l-1}$, message $\embm_{\neighbor{v_i}}^{l-1}$, and features $\embx_i$ to update node embeddings.
For the consistency of the notations of ConvGNNs and RecGNNs, we denote the parameters of RecGNNs in Eq. \eqref{eqn:mpeq_update} by $(\theta^l)_{l=1}^\infty$ with $\theta^l = \thetarec,\, l \in \mathbb{N}^{*}$.


\begin{figure}[t]
\centering % <-- added
\begin{subfigure}{0.50\textwidth}
  \includegraphics[width=260pt]{imgs/model/ConvGNN2.pdf}
  \caption{ConvGNNs}\label{subfig:ConvGNN}
\end{subfigure}\hfil
\begin{subfigure}{0.50\textwidth}
  \includegraphics[width=260pt]{imgs/model/RecGNN2.pdf}
    \caption{RecGNNs}\label{subfig:RecGNN}
\end{subfigure}\hfil % <-- added
\caption{
The architectures of ConvGNNs and RecGNNs. We denote message passing by \textit{MP}.} \label{fig:model}
\end{figure}

\section{Message Passing in Backward Passes}\label{sec:grad}

We introduce the gradients of ConvGNNs and RecGNNs in Sections \ref{sec:grad_convgnn} and \ref{sec:grad_recgnn}, respectively, and then formulate the corresponding backward passes as message passing.
Finally, in Section \ref{sec:naive_sgd}, we introduce an SGD variant---backward SGD, which provides unbiased gradient estimations based on the message passing formulation of backward passes.

\subsection{Backward Passes of ConvGNNs}\label{sec:grad_convgnn}


The gradient {\small$\nabla_{w} \mathcal{L}$} is easy to compute and we hence introduce the chain rule to compute {\small$\nabla_{\Theta} L$} in this section, where {\small$\Theta=(\theta^{l})_{l=1}^{L}$}. Let {\small$\embV^{l}\triangleq \nabla_{\embH^{l}} \mathcal{L}$} for {\small$l\in[L]$} be auxiliary variables. It is easy to compute {\small$\vec{\embV}^{L} = \nabla_{\vec{\embH}^{L}} \mathcal{L} = \nabla_{\vec{\embH}} \mathcal{L}$}. By the chain rule, we iteratively compute {\small$\embV^{l}$} based on {\small$\embV^{l+1}$} as
\begin{align}
    \vec{\embV}^{l}=\vec{\phi}_{\theta^{l+1}}(\embV^{l+1})\triangleq(\nabla_{\vec{\embH}^{l}}\vec{f}_{\theta^{l+1}}) \vec{\embV}^{l+1}, \label{eqn:auxiliary_recursion}
\end{align}
and
\begin{align}
    \embV^{l}=\phi_{\theta^{l+1}} \circ \cdots \circ
    \phi_{\theta^{L}}(\embV^{L}). \label{eqn:auxiliary_compute}
\end{align}
Then, we compute the gradient {\small$\nabla_{\theta^{l}} \mathcal{L}= (\nabla_{\theta^l} \vec{f}_{\theta^l})\vec{\embV}^{l},\,l\in[L]$} by autograd packages for vector-Jacobian product.


\udfsection{Message Passing Formulation of Backward Passes for ConvGNNs.} Combining Eqs. \eqref{eqn:auxiliary_recursion} and \eqref{eqn:mpeq_update} leads to
\begin{align}
    \embV^{l}_i=\sum_{v_j\in\neighbor{v_i}}(\nabla_{\embh^{l}_i} \update_{\theta^{l+1}}(\embh^{l}_j, \embm^{l}_{\neighbor{v_j}}  ,\embx_j))\embV^{l+1}_j,\,\, i\in[n], \label{eqn:mpeq_auxiliary}
\end{align}
where \modify{{\small$\embV_k^l$} is the {\small$k$}-th column of {\small$\embV^l$} and}{} {\small$\embm^{l}_{\neighbor{v_j}}$} is a function of {\small$\embh^{l}_i$} defined in Eq. \eqref{eqn:mpeq_update}. Eq. \eqref{eqn:mpeq_auxiliary} uses {\small$(\nabla_{\embh^{l}_i} \update_{\theta^{l+1}}(\embh^{l}_j, \embm^{l}_{\neighbor{v_j}}  ,\embx_j))\embV_j^{l+1}$}, sum aggregation, and the identity mapping as the generation function, the aggregation function, and the update function, respectively.


\subsection{Backward Passes of RecGNNs}\label{sec:grad_recgnn}

We compute the gradients of RecGNNs through implicit differentiation \cite{ignn, deq}.
Similar to ConvGNNs, the gradients {\small$\nabla_w \loss$} and {\small$\nabla_{\embH^{\Diamond} } \loss$} are easy to compute.
By the chain rule and implicit differentiation, we can compute the Jacobian of parameters {\small$\thetarec$} by {\small$\frac{\partial \loss}{\partial \thetarec} = \frac{\partial \loss}{\partial \Vec{\embH}^{\Diamond} } \frac{\partial \Vec{\embH}^{\Diamond} }{\partial \thetarec} = \frac{\partial \loss}{\partial \Vec{\embH}^{\Diamond} }(I - \jacobian_{\Vec{\embH}^{\Diamond}}\Vec{f}_{\thetarec})^{-1}\frac{\partial \Vec{f}_{\thetarec}}{\partial \thetarec}$}.
As the inverse of {\small$I - \jacobian_{\Vec{\embH}^{\Diamond}}\Vec{f}_{\thetarec}$} is intractable, we compute auxiliary variables {\small$(\Vec{\embV}^{\Diamond})^{\top} = \frac{\partial \loss}{\partial \Vec{\embH}^{\Diamond} }(I - \jacobian_{\Vec{\embH}^{\Diamond}}\Vec{f}_{\thetarec})^{-1}$} by solving the fixed-point equations
\begin{align}
    \Vec{\embV}^{\Diamond} = \Vec{\phi}_{\thetarec,w}(\embV^{\Diamond}) \,\,\triangleq\,\,  (\nabla_{\Vec{\embH}^{\Diamond}}\Vec{f}_{\thetarec})\Vec{\embV}^{\Diamond}  + \nabla_{\Vec{\embH}^{\Diamond}}\loss \label{eqn:mpeq_grad}.
\end{align}
If the well-posedness property \cite{ignn} holds (see Appendix \ref{sec:well-posedness}), we solve the fixed-point equations \eqref{eqn:mpeq_grad} by iterative solvers. We then use autograd packages to compute the vector-Jacobian product {\small$\nabla_{\thetarec} \loss = (\nabla_{\thetarec} \vec{f}_{\thetarec})\Vec{\embV}^{\Diamond}$}.


\udfsection{Message Passing Formulation of Backward Passes for RecGNNs.} Combining Eqs. \eqref{eqn:mpeq_grad} and \eqref{eqn:mpeq_update} leads to
\begin{align}
    &\embV_{i}^{\Diamond} = \sum_{v_j \in\neighbor{v_i}} (\nabla_{\embh_i^{\Diamond}}\update_{\thetarec}(\embh_j^{\Diamond} ,\embm_{\neighbor{v_j}}^{\Diamond}  ,\embx_j) ) \embV_{j}^{\Diamond}+ \nabla_{\embh_i^{\Diamond}} \loss \label{eqn:mpeq_grad_node}
\end{align}
for $i\in [n]$, where {\small $\embV_k^{\Diamond}$} is the $k$-th column of {\small $\embV^{\Diamond}$} and {\small $\embm_{\neighbor{v_j}}^{\Diamond}$} is a function of {\small$\embh_i^{\Diamond}$} defined in Eq. \eqref{eqn:mpeq_update}.
Eq. \eqref{eqn:mpeq_grad_node} uses {\small$(\nabla_{\embh_i^{\Diamond}}\update_{\thetarec}(\embh_j^{\Diamond},\embm_{\neighbor{v_j}}^{\Diamond}  ,\embx_j) ) \embV_{j}^{\Diamond}$}, sum aggregation, and {\small ${\rm update}_{i}(\cdot)=\cdot + \nabla_{\embh_i^{\Diamond}} \loss$} as the generation function, the aggregation function, and the update function, respectively.



\subsection{Backward SGD}  \label{sec:naive_sgd}




In this section, we develop an SGD variant---backward SGD, which provides unbiased gradient estimations based on the message passing formulation of backward passes.
Backward SGD helps retrieve the messages
discarded by existing subgraph-wise sampling methods (see Theorems \ref{thm:grad_error_conv} and \ref{thm:grad_error_rec}).



Given a sampled mini-batch $\mathcal{V}_{\mathcal{B}}$, suppose that we have computed exact node embeddings and auxiliary variables of nodes in $\mathcal{V}_{\mathcal{B}}$, i.e., $(\embH^{l}_{\mathcal{V}_{\mathcal{B}}}, \embV^{l}_{\mathcal{V}_{\mathcal{B}}})_{l=1}^{L}$ for ConvGNNs or $(\embH^{\Diamond}_{\mathcal{V}_{\mathcal{B}}}, \embV^{\Diamond}_{\mathcal{V}_{\mathcal{B}}})$ for RecGNNs.
To simplify the analysis, we assume that $\mathcal{V}_{\mathcal{B}}$ is uniformly sampled from $\mathcal{V}$ and the corresponding set of labeled nodes $\mathcal{V}_{L_{\mathcal{B}}} := \mathcal{V}_{\mathcal{B}} \cap \mathcal{V}_L$ is uniformly sampled from $\mathcal{V}_L$.
When the sampling is not uniform, we use the normalization technique \cite{graphsaint} to enforce the assumption (please refer to Appendix \ref{sec:normalization}).





First, backward SGD computes the mini-batch gradient $\mathbf{g}_{w}(\mathcal{V}_\mathcal{B})$ for parameters ${w}$ by the derivative of mini-batch loss $\mathcal{L}_{\mathcal{V}_\mathcal{B}}=\frac{1}{|\mathcal{V}_{L_{\mathcal{B}}}|}\sum_{v_j\in\mathcal{V}_{L_\mathcal{B}}}\ell_{w}(\embh_j,y_j)$, i.e.,
\begin{align}
    \mathbf{g}_{w}(\mathcal{V}_\mathcal{B})=\frac{1}{|\mathcal{V}_{L_{\mathcal{B}}}|}\sum_{v_j\in\mathcal{V}_{L_\mathcal{B}}} \nabla_{w} \ell_{w}(\embh_j,y_j).\label{eqn:mini-batch_grad_w}
\end{align}
Then, for ConvGNNs, backward SGD computes the mini-batch gradient $\mathbf{g}_{\theta^{l}}(\mathcal{V}_\mathcal{B})$ for parameters $\theta^{l}$, $l \in [L]$ as
\begin{align}
    &\mathbf{g}_{\theta^{l}}(\mathcal{V}_\mathcal{B})=\frac{|\mathcal{V}|}{|\mathcal{V}_\mathcal{B}|}\sum_{v_j\in\mathcal{V}_\mathcal{B}}(\nabla_{\theta^{l}}u_{\theta^l}(\embh^{l-1}_j, \embm^{l-1}_{\neighbor{v_j}}, \embx_j))\embV^{l}_j,\label{eqn:mini-batch_grad_theta}
\end{align}
For RecGNNs, backward SGD replaces $\theta^{l}$, $\embH^{l-1}$, $\embV^{l}$ with $\theta^{\Diamond}$, $\embH^{\Diamond}$, $\embV^{\Diamond}$ in Eq. \eqref{eqn:mini-batch_grad_theta} to compute $\mathbf{g}_{\theta^{\Diamond}}(\mathcal{V}_{\mathcal{B}})$.

Note that the mini-batch gradients $\mathbf{g}_{\theta^l}(\mathcal{V}_{\mathcal{B}})$ for different $l\in[L]$ are based on the same mini-batch $\mathcal{V}_{\mathcal{B}}$, which facilitates designing subgraph-wise sampling methods\modify{based on backward SGD.}{.}
Another appealing feature of backward SGD is that the mini-batch gradients $\mathbf{g}_w(\mathcal{V}_{\mathcal{B}})$, $\mathbf{g}_{\theta^{\Diamond}}(\mathcal{V}_{\mathcal{B}})$, and $\mathbf{g}_{\theta^l}(\mathcal{V}_{\mathcal{B}})$, $l\in [L]$ are unbiased, as shown in the following theorem. Please see Appendix \ref{appendix:proof_thm1} for the detailed proof.

\begin{theorem}\label{thm:unbiased}
    Suppose that a subgraph {\small $\inbatch$} is uniformly sampled from {\small $\mathcal{V}$} and the corresponding labeled nodes {\small$\mathcal{V}_{L_{\mathcal{B}}} = \inbatch \cap \mathcal{V}_{L}$} is uniformly sampled from {\small$\mathcal{V}_{L}$}. Then the mini-batch gradients {\small $\mathbf{g}_w(\mathcal{V}_{\mathcal{B}})$}, {\small $\mathbf{g}_{\theta^{\Diamond}}(\mathcal{V}_{\mathcal{B}})$}, and {\small $\mathbf{g}_{\theta^l}(\mathcal{V}_{\mathcal{B}})$}, {\small$l\in [L]$} in Eqs. \eqref{eqn:mini-batch_grad_w} and \eqref{eqn:mini-batch_grad_theta} are unbiased.
\end{theorem}


\begin{figure*}[t]
% \centering % <-- added
\begin{subfigure}{0.3\textwidth}
  \includegraphics[width=180pt]{imgs/convgnn/alg/graph.pdf}
  \caption{Original graph}\label{subfig:graph_conv}
\end{subfigure}\hfil
\begin{subfigure}{0.3\textwidth}
  \includegraphics[width=180pt]{imgs/convgnn/alg/gas_forward.pdf}
    \caption{Forward passes of GAS}\label{subfig:gas_forward}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.33\textwidth}
  \includegraphics[width=180pt]{imgs/convgnn/alg/lmc_forward.pdf}
  \caption{Forward passes of LMC}\label{subfig:lmc_forward}
\end{subfigure}\hfil
\begin{subfigure}{0.33\textwidth}
  \includegraphics[width=180pt]{imgs/convgnn/alg/legend.pdf}
\end{subfigure}\hfil
\begin{subfigure}{0.33\textwidth}
  \includegraphics[width=180pt]{imgs/convgnn/alg/gas_backward.pdf}
    \caption{Backward passes of GAS}\label{subfig:gas_backward}
\end{subfigure}\hfil
\begin{subfigure}{0.33\textwidth}
  \includegraphics[width=180pt]{imgs/convgnn/alg/lmc_backward.pdf}
    \caption{Backward passes of LMC}\label{subfig:lmc_backward}
\end{subfigure}\hfil
\caption{
Comparison of LMC4Conv with GAS \cite{gas}. (a) shows the original graph with in-batch nodes, 1-hop out-of-batch nodes, and other out-of-batch nodes in orange, blue, and grey, respectively. (b) and (d) show the computation graphs of forward passes and backward passes of GAS, respectively. (c) and (e) show the computation graphs of forward passes and backward passes of LMC4Conv, respectively.} \label{fig:computation}
\end{figure*}

\section{Local Message Compensation}
The exact mini-batch gradients {\small$\mathbf{g}_{w}(\mathcal{V}_\mathcal{B})$}, {\small$\mathbf{g}_{\theta^{\Diamond}}(\mathcal{V}_{\mathcal{B}})$}, and {\small$\mathbf{g}_{\theta^l}(\mathcal{V}_\mathcal{B}),\,l\in[L]$} computed by backward SGD depend on exact embeddings and auxiliary variables of nodes in the mini-batch {\small$\mathcal{V}_{\mathcal{B}}$} rather than the whole graph. However, backward SGD is not scalable, as the exact  {\small$(\embH^l_{\inbatch},\embV^l_{\inbatch})_{l=1}^L$} and {\small$(\embH^{\Diamond}_{\inbatch},\embV^{\Diamond}_{\inbatch})$} are expensive to compute due to the {\it neighbor explosion} problem. To deal with this problem, we develop a novel and scalable subgraph-wise sampling method for ConvGNNs and RecGNNs, namely \textbf{L}ocal \textbf{M}essage \textbf{C}ompensation (LMC). We introduce the methodologies of LMC for ConvGNNs (LMC4Conv) and LMC for RecGNNs (LMC4Rec) in Sections \ref{subsubsec:lmc4conv_methodology} and \ref{subsubsec:lmc4rec_methodology}, respectively. Further, we show that LMC converges to first-order stationary points of ConvGNNs and RecGNNs in Sections \ref{subsubsec:lmc4conv_theoretical} and \ref{subsubsec:lmc4rec_theoretical}, respectively.


\subsection{LMC for ConvGNNs}\label{subsec:lmc4conv}


In Algorithm \ref{alg:lmc4conv} and Section \ref{subsubsec:lmc4conv_theoretical}, we denote a vector or a matrix in the $l$-th layer at the $k$-th iteration by $(\cdot)^{l,k}$, but elsewhere we omit the superscript $k$ and simply denote it by $(\cdot)^{l}$.




\subsubsection{Methodology}\label{subsubsec:lmc4conv_methodology}

We store {\it historical embeddings and auxiliary variables} {\small$(\hisH^{l},\hisV^{l})_{l=1}^L$} to provide an affordable approximation.
At each training iteration, we sample a mini-batch of nodes {\small$\inbatch$} and update historical embeddings and auxiliary variables of nodes in the mini-batch, i.e., {\small$(\hisH^{l}_{\inbatch},\hisV^{l}_{\inbatch})_{l=1}^L$}.
We use updated {\small$(\hisH^{l}_{\inbatch},\hisV^{l}_{\inbatch})_{l=1}^L$} to approximate exact $(\embH^{l}_{\mathcal{V}_{\mathcal{B}}}, \embV^{l}_{\mathcal{V}_{\mathcal{B}}})_{l=1}^{L}$ and then use Eqs. \eqref{eqn:mini-batch_grad_w}, \eqref{eqn:mini-batch_grad_theta} to compute mini-batch gradients {\small$\widetilde{\mathbf{g}}_w$} and {\small$\widetilde{\mathbf{g}}_{\theta^{l}},\,l\in[L]$}.






\udfsection{Update of {\small$(\hisH^{l}_{\inbatch})_{l=1}^L$}.} In forward passes, we initialize the historical embeddings for {\small$l=0$} as {\small$\hisH^0_{\inbatch} = \embX_{\inbatch}$}.
At the {\small$l$}-th layer, we update the historical embedding of each node {\small$v_i \in \inbatch$} as
\begin{align}
    & \overline{\embh}_i^l = \update_{\theta^l}(\overline{\embh}_i^{l-1}, \overline{\embm}_{\neighbor{v_i}}^{l-1}, \embx_i);\nonumber\\
    & \overline{\embm}_{\neighbor{v_i}}^{l-1} = \aggregate_{\theta^l}( \{ g_{\theta^l}(\overline{\embh}_j^{l-1}) \mid v_j \in \neighbor{v_i} \cap \inbatch \}\nonumber\\
    &\quad\quad\quad\quad\quad\quad\quad \cup \{ g_{\theta^l}(\widehat{\embh}_j^{l-1}) \mid v_j \in \neighbor{v_i} \setminus \inbatch \} ). \label{eqn:mini_mpeq_update}
\end{align}
We call {\small$\mathbf{C}_f^{l} \triangleq \aggregate_{\theta^{l}}(\{ g_{\theta^{l}}(\widehat{\embh}_j^{l-1}) \mid v_j\in\neighbor{v_i}\setminus \inbatch \})$} the {\it local message compensation} in the {\small$l$}-th layer in forward passes, where {\small $\widehat{\embh}_j^{l-1}$} is the {\it temporary embedding} of node $v_j$ outside the mini-batch.
We denote the temporary embeddings of nodes outside the mini-batch by {\small$\widehat{\embH}^{l}_{\neighbor{\inbatch} \setminus \inbatch}$}.


\udfsection{Computation of {\small$\widehat{\embH}^{l}_{\neighbor{\inbatch} \setminus \inbatch}$}.} We initialize the temporary and historical embeddings for {\small$l=0$} as {\small$\widehat{\embH}^0_{\neighbor{\inbatch} \setminus \inbatch} = \hisH_{\neighbor{\inbatch} \setminus \inbatch}^0 = \embX_{\neighbor{\inbatch} \setminus \inbatch}$}.
We compute the temporary embedding of each neighbor {\small$v_i \in \neighbor{\inbatch} \setminus \inbatch$} as
\begin{align}
    &\widehat{\embh}_i^{l} = (1-\beta_{i}) \overline{\embh}_i^{l} + \beta_{i} \widetilde{\embh}_i^{l},\label{eqn:temp_compute}
\end{align}
where {\small$\beta_{i}\in [0,1]$} is the convex combination coefficient for node $v_i$, and
\begin{align}
    &\widetilde{\embh}_i^l = \update_{\theta^l}(\widehat{\embh}_i^{l-1}, \overline{\embm}_{\neighbor{v_i}}^{l-1}, \embx_i);\nonumber\\ 
    &\overline{\embm}_{\neighbor{v_i}}^{l-1} = \aggregate_{\theta^l} (\{ g_{\theta^l}(\overline{\embh}_j^{l-1}) \mid v_j\in\neighbor{v_i} \cap \inbatch \}\nonumber\\
    &\quad\quad\quad\quad\cup \{g_{\theta^l}(\widehat{\embh}_j^{l-1}) \mid v_j \in \neighbor{\inbatch}\cap\neighbor{v_i} \setminus \inbatch \} ).
    \label{eqn:subexact_compute}
\end{align}
Notably, the neighbors of node {\small$v_i \in \neighbor{\inbatch} \setminus \inbatch$} may contain the nodes in {\small$\kneighbor{\inbatch}{2} \setminus \neighbor{\inbatch}$}, which we prune in the computation of messages {\small $\overline{\embm}_{\neighbor{v_i}}^{l-1}$} to avoid the neighbor explosion problem.
Thus, we call $\widetilde{\embh}_i^l$ {\it incomplete up-to-date embeddings}.
The effectiveness of the convex combination is based on a observation that {\small$\neighbor{\inbatch}$} covers most the $1$-hop neighbors of most nodes {\small$v_i \in \neighbor{\inbatch} \setminus \inbatch$} if $|\inbatch|$ is large.
For these nodes, the pruning errors are very small and we can tune $\beta_i$ to encourage them to be close to the incomplete up-to-date embeddings. We provide the selection of $\beta_i$ in Appendix \ref{sec:selection_beta}.










\udfsection{Update of {\small$(\hisV^{l}_{\inbatch})_{l=1}^L$}.}
In backward passes, we initialize the {\it historical auxiliary variables} for {\small$l=L$} as {\small$\hisV^L_{\inbatch} = \nabla_{\hisH_{\inbatch}} \loss$.} We update the historical auxiliary variable of each {\small$v_i\in\inbatch$} as
\begin{align}
    \hisV^{l}_i=&\sum_{v_j\in\neighbor{v_i} \cap \inbatch}(\nabla_{\embh^{l}_i} \update_{\theta^{l+1}}(\overline{\embh}^{l}_j, \overline{\embm}^{l}_{\neighbor{v_j}}  ,\embx_j))\overline{\embV}^{l+1}_j\nonumber\\
    &+ \sum_{v_j \in \neighbor{v_i} \setminus \inbatch } (\nabla_{\embh^{l}_i} \update_{\theta^{l+1}}(\widehat{\embh}^{l}_j, \overline{\embm}^{l}_{\neighbor{v_j}}  ,\embx_j))\widehat{\embV}^{l+1}_j, \label{eqn:mini_mpeq_auxiliary}
\end{align}
where 
{\small$\overline{\embh}_j^l$}, {\small$\overline{\embm}^l_{\neighbor{v_j}}$}, and {\small$\widehat{\embh}_j^l$}
are computed as shown in Eqs. \eqref{eqn:mini_mpeq_update}--\eqref{eqn:subexact_compute}.
We call {\small$\mathbf{C}_b^{l} \triangleq \sum_{v_j \in \neighbor{v_i} \setminus \inbatch } (\nabla_{\embh^{l}_i} \update_{\theta^{l+1}}(\widehat{\embh}^{l}_j, \overline{\embm}^{l}_{\neighbor{v_j}}  ,\embx_j))\widehat{\embV}^{l+1}_j$} the {\it local message compensation} in the {\small$l$}-th layer in backward passes, where {\small $\widehat{\embV}_j^{l-1}$} is the {\it temporary auxiliary variable} of node $v_j$ outside the mini-batch.
We denote the temporary auxiliary variables of nodes outside the mini-batch by {\small$\widehat{\embV}^{l}_{\neighbor{\inbatch} \setminus \inbatch}$}.


\udfsection{Computation of {\small$\widehat{\embV}^{l}_{\neighbor{\inbatch} \setminus \inbatch}$}.}
We initialize the {\it hitorical auxiliary variables} for {\small$l=L$} as {\small$\widehat{\embV}^L_{\neighbor{\inbatch} \setminus \inbatch} = \hisV^L_{\neighbor{\inbatch} \setminus \inbatch} =\nabla_{\widehat{\embH}_{\neighbor{\inbatch} \setminus \inbatch}} \loss$}. We compute the temporary auxiliary variable of each neighbor {\small$v_i\in\neighbor{\inbatch} \setminus \inbatch$} as
\begin{align}
    \widehat{\embV}_i^l = (1-\beta_{i}) \overline{\embV}_i^l + \beta_{i} \widetilde{\embV}_i^l, \label{eqn:temp_compute_auxiliary}
\end{align}
where {\small$\beta_{i}$} is the convex combination coefficient used in Eq. \eqref{eqn:temp_compute}, and
\begin{align}
    \widetilde{\embV}^{l}_i={}&\sum_{ v_j\in\neighbor{v_i} \cap \inbatch}(\nabla_{\embh^{l}_i} \update_{\theta^{l+1}}(\overline{\embh}^{l}_j, \overline{\embm}^{l}_{\neighbor{v_j}}  ,\embx_j))\overline{\embV}^{l+1}_j\nonumber\\
    + &\sum_{v_j \in \neighbor{\inbatch}\cap\neighbor{v_i} \setminus \inbatch } (\nabla_{\embh^{l}_i} \update_{\theta^{l+1}}(\widehat{\embh}^{l}_j, \overline{\embm}^{l}_{\neighbor{v_j}}  ,\embx_j))\widehat{\embV}^{l+1}_j. \label{eqn:subexact_compute_auxiliary}
\end{align}
Similar to forward passes, we prune the nodes in {\small$\kneighbor{\inbatch}{2} \setminus \neighbor{\inbatch}$} in the computation of messages {\small $\overline{\embm}_{\neighbor{v_i}}^{l-1}$} to avoid the neighbor explosion problem.


\udfsection{Mini-batch Gradients of LMC4Conv.} 
Combining Eqs. \eqref{eqn:mini-batch_grad_w} and \eqref{eqn:mini-batch_grad_theta} with {\small$(\overline{\embh}_j^l, \overline{\embm}^l_{\neighbor{v_j}}, \overline{\embV}_j^l)_{l=0}^{L}$} leads to
\begin{align}
    &\widetilde{\mathbf{g}}_{w}(\mathcal{V}_\mathcal{B})=\frac{1}{|\mathcal{V}_{L_{\mathcal{B}}}|}\sum_{v_j\in\mathcal{V}_{L_\mathcal{B}}} \nabla_{w} \ell_{w}(\hish_j,y_j),\label{eqn:grad_conv_w}\\
    &\widetilde{\mathbf{g}}_{\theta^{l}}(\mathcal{V}_\mathcal{B})=\frac{|\mathcal{V}|}{|\mathcal{V}_\mathcal{B}|}\sum_{v_j\in\mathcal{V}_\mathcal{B}}(\nabla_{\theta^{l}}u_{\theta^l}(\hish^{l-1}_j, \overline{\embm}^{l-1}_{\neighbor{v_j}}, \embx_j))\hisV^{l}_j.\label{eqn:grad_conv_theta}
\end{align}


\udfsection{Time Complexity.} Notice that the total size of Eqs. \eqref{eqn:mini_mpeq_update}--\eqref{eqn:subexact_compute_auxiliary} is linear with {\small$|\neighbor{\inbatch}|$} rather than the size of the whole graph.
Suppose that the maximum neighborhood size is {\small$n_{\max}$} and the number of layers is {\small$L$}, then the time complexity in forward and backward passes is {\small$\mathcal{O}( L(n_{\max}|\inbatch|d+|\inbatch| d^2) )$}.




\udfsection{Space Complexity.} LMC4Conv additionally stores the historical node embeddings {\small$\overline{\embH}^l$} and auxiliary variables {\small$\overline{\embV}^l$} for {\small$l\in[L]$}. As pointed out in \cite{gas}, we can store the majority of historical values in RAM or hard drive storage rather than GPU memory. Thus, the active historical values in forward and backward passes employ {\small$\mathcal{O}(n_{\max} L|\inbatch| d)$ and {\small$
\mathcal{O}(n_{\max} L|\inbatch| d)$}} GPU memory, respectively. As the time and memory complexity are independent of the size of the whole graph, i.e., {\small$|\mathcal{V}|$}, LMC4Conv is scalable. We summarize the computational complexity in Appendix \ref{appendix:complexity}.


Fig. \ref{fig:computation} shows the message passing mechanisms of GAS \cite{gas} and LMC4Conv. Compared with GAS, LMC4Conv proposes compensation messages between in-batch nodes and their 1-hop neighbors simultaneously in forward and backward passes, which corrects the bias of mini-batch gradients and thus accelerates convergence.


\begin{algorithm}[H]
    \caption{LMC4Conv}
    \label{alg:lmc4conv}
    \begin{algorithmic}[1]
        \STATE {\bfseries Input:} 
        The learning rate {\small$\eta$} and the convex combination coefficients {\small$(\beta_i)_{i=1}^n$}.
        \STATE Partition {\small$\mathcal{V}$} into {\small$B$} parts {\small$(\mathcal{V}_{b})_{b=1}^B$} 
        \FOR{{\small$k = 1, \dots, N$}}
            \STATE Randomly sample {\small$\mathcal{V}_{b_k}$} from {\small$(\mathcal{V}_b)_{b=1}^B$}
            \STATE Initialize {\small$\hisH^{0,k}_{\neighbor{\mathcal{V}_{b_k}}} = \embX_{\neighbor{\mathcal{V}_{b_k}}}$}
            \STATE {\small Initialize $\widehat{\embH}^{0,k}_{\neighbor{\mathcal{V}_{b_k}} \setminus \mathcal{V}_{b_k}} = \embX_{\neighbor{\mathcal{V}_{b_k}} \setminus \mathcal{V}_{b_k}}$}
            \FOR{{\small$l=1,\dots,L$}}
                \STATE Update {\small$\overline{\embH}^{l,k}_{\mathcal{V}_{b_k}}$} by Eq. \eqref{eqn:mini_mpeq_update}
                \STATE Compute {\small$\widehat{\embH}^{l,k}_{\neighbor{\mathcal{V}_{b_k}} \setminus \mathcal{V}_{b_k}}$} by Eqs. \eqref{eqn:temp_compute} and \eqref{eqn:subexact_compute}
            \ENDFOR
            \STATE Initialize {\small$\hisV^L_{\mathcal{V}_{b_k}} = \nabla_{\hisH_{\mathcal{V}_{b_k}}} \loss$}
            \STATE {\small Initialize $\widehat{\embV}^L_{\neighbor{\mathcal{V}_{b_k}} \setminus \mathcal{V}_{b_k}} = \hisV^L_{\neighbor{\mathcal{V}_{b_k}} \setminus \mathcal{V}_{b_k}} =\nabla_{\widehat{\embH}_{\neighbor{\mathcal{V}_{b_k}} \setminus \mathcal{V}_{b_k}}} \loss$}
            \FOR{{\small$l=L-1,\dots,1$}}
                \STATE Update {\small$\overline{\embV}^{l,k}_{\mathcal{V}_{b_k}}$} by Eq. \eqref{eqn:mini_mpeq_auxiliary}
                \STATE Compute {\small$\widehat{\embV}^{l,k}_{\neighbor{\mathcal{V}_{b_k}} \setminus \mathcal{V}_{b_k}}$} by Eqs. \eqref{eqn:temp_compute_auxiliary} and \eqref{eqn:subexact_compute_auxiliary}
            \ENDFOR
            \STATE Compute {\small$\widetilde{\mathbf{g}}_w^k$} and {\small$\widetilde{\mathbf{g}}_{\theta^{l}}^k,\,l\in[L]$} by Eqs. \eqref{eqn:grad_conv_w} and \eqref{eqn:grad_conv_theta}
            \STATE Update parameters by\\
            \quad\quad\quad {\small$w^k = w^{k-1} - \eta \widetilde{\mathbf{g}}_w^k$}\\
            \quad\quad\quad {\small$\theta^{l,k} = \theta^{l,k-1} - \eta\widetilde{\mathbf{g}}_{\theta^{l}}^k,\,l\in[L]$}
        \ENDFOR
    \end{algorithmic}
\end{algorithm}






\par Algorithm \ref{alg:lmc4conv} summarizes LMC4Conv. We add a superscript {\small$k$} for each value to indicate that it is the value at the {\small$k$}-th iteration. At the preprocessing step, we partition {\small$\mathcal{V}$} into {\small$B$} parts {\small$(\mathcal{V}_b)_{b=1}^B$}.
At the {\small$k$}-th training step, LMC4Conv first randomly samples a subgraph constructed by {\small$\mathcal{V}_{b_k}$}. 
Then, LMC4Conv updates the stored historical node embeddings {\small$\overline{\embH}^{l,k}_{\mathcal{V}_{b_k}}$} in the order of {\small$l=1,\ldots,L$} by Eqs. \eqref{eqn:mini_mpeq_update}--\eqref{eqn:subexact_compute}, and the stored historical auxiliary variables {\small$\overline{\embV}^{l,k}_{\mathcal{V}_{b_k}}$} in the order of {\small$l=L-1,\ldots,1$} by Eqs. \eqref{eqn:mini_mpeq_auxiliary}--\eqref{eqn:subexact_compute_auxiliary}. By the random update, the historical values get close to the exact up-to-date values. Finally, for {\small$l\in[L]$} and {\small$v_j\in\mathcal{V}_{b_k}$}, by replacing {\small$\embh^{l,k}_j$}, {\small$\embm^{l,k}_{\neighbor{v_j}}$} and {\small$\embV^{l,k}_j$} in Eqs. \eqref{eqn:mini-batch_grad_w} and \eqref{eqn:mini-batch_grad_theta} with {\small$\overline{\embh}^{l,k}_j$}, {\small$\overline{\embm}^{l,k}_{\neighbor{v_j}}$}, and {\small$\overline{\embV}^{l,k}_j$}, respectively, LMC4Conv computes mini-batch gradients {\small$\widetilde{\mathbf{g}}_{w}^k,\widetilde{\mathbf{g}}_{\theta^1}^k,\ldots,\widetilde{\mathbf{g}}_{\theta^L}^k$} to update parameters {\small$w,\theta^1,\ldots,\theta^L$}.



\begin{figure*}[t]
% \centering % <-- added
\begin{subfigure}{0.33\textwidth}
  \includegraphics[width=170pt]{imgs/graph.pdf}
  \caption{Graph}\label{subfig:graph_rec}
\end{subfigure}\hfil
\begin{subfigure}{0.33\textwidth}
  \includegraphics[width=170pt]{imgs/sgd.pdf}
    \caption{Backward SGD}\label{subfig:naive_sgd}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.33\textwidth}
  \includegraphics[width=170pt]{imgs/cluster.pdf}
  \caption{Cluster-GCN}\label{subfig:cluster}
\end{subfigure}\hfil
\begin{subfigure}{0.33\textwidth}
  \includegraphics[width=170pt]{imgs/gas.pdf}
    \caption{GAS}\label{subfig:gas}
\end{subfigure}\hfil
\begin{subfigure}{0.33\textwidth}
  \includegraphics[width=170pt]{imgs/amp.pdf}
    \caption{LMC}\label{subfig:amp}
\end{subfigure}\hfil
\begin{subfigure}{0.33\textwidth}
  \includegraphics[width=200pt]{imgs/legend.pdf}
\end{subfigure}\hfil
\caption{
Message passing of backward SGD, Cluster-GCN \cite{cluster_gcn}, GAS \cite{gas}, and LMC for RecGNNs.} \label{fig:message_flow}
\vspace{-4mm}
\end{figure*}


\subsubsection{Theoretical Results}\label{subsubsec:lmc4conv_theoretical}

In this subsection, we provide the theoretical analysis of LMC4Conv. Theorem \ref{thm:grad_error_conv} shows that the biases of mini-batch gradients computed by LMC4Conv can tend to an arbitrarily small value by setting a proper learning rate and convex combination coefficients. Then, Theorem \ref{thm:convergence_conv} shows that LMC4Conv converges to first-order stationary points of ConvGNNs. We provide detailed proofs of the theorems in Appendix \ref{appendix:proof_conv}. In the theoretical analysis, we use the following assumptions.
\begin{assumption}\label{assmp:proof} 
    Assume that (1) at the {\small$k$}-th iteration, a batch of nodes {\small$\mathcal{V}_{\mathcal{B}}^k$} is uniformly sampled from {\small$\mathcal{V}$} and the corresponding labeled node set {\small$\mathcal{V}_{L_{\mathcal{B}}}^{k}=\mathcal{V}_{\mathcal{B}}^k \cap \mathcal{V}_L$} is uniformly sampled from {\small$\mathcal{V}_L$}, (2) functions {\small$f_{\theta^{l}}$}, {\small$\phi_{\theta^{l}}$}, {\small$\nabla_{w}\mathcal{L}$}, {\small$\nabla_{\theta^{l}}\mathcal{L}$}, {\small$\nabla_w\ell_{w}$}, and {\small$\nabla_{\theta^l} u_{\theta^l}$} are {\small$\gamma$}-Lipschitz with {\small$\gamma>1$}, {\small$\forall\, l\in[L]$}, (3) norms {\small$\|\embH^{l,k}\|_F$}, {\small$\|\hisH^{l,k}\|_F$}, {\small$\|\temH^{l,k}\|_F$}, {\small$\|\widetilde{\embH}^{l,k}\|_F$}, {\small$\|\embV^{l,k}\|_F$}, {\small$\|\hisV^{l,k}\|_F$}, {\small$\|\temV^{l,k}\|_F$}, {\small$\|\widetilde{\embV}^{l,k}\|_F$}, {\small$\|\nabla_w\loss\|_2$}, {\small$\|\nabla_{\theta^l}\loss\|_2$}, {\small$\|\widetilde{\mathbf{g}}_{\theta^l}\|_2$}, and {\small$\|\widetilde{\mathbf{g}}_{w}\|_2$} are bounded by {\small$G>1$}, {\small$\forall\, l\in[L],\,k\in\mathbb{N}^*$}.
\end{assumption}

\begin{theorem}\label{thm:grad_error_conv}
    For any $k\in\mathbb{N}^*$ and $l\in[L]$, the expectations of $\|\Delta_w^k\|_2^2 \triangleq \|\widetilde{\mathbf{g}}_w(w^k) - \nabla_w\loss(w^k)\|_2^2$ and $\|\Delta_{\theta^l}^k\|_2^2 \triangleq \|\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k}) - \nabla_{\theta^l}\loss(\theta^{l,k})\|_2^2$ have the bias-variance decomposition
    \begin{align*}
        &\mathbb{E}[\|\Delta^k_w\|_2^2] = ({\rm Bias}(\widetilde{\mathbf{g}}_w(w^k)))^2+ {\rm Var}(\widetilde{\mathbf{g}}_w(w^k)),\\
        &\mathbb{E}[\|\Delta^k_{\theta^l}\|_2^2] = ({\rm Bias}(\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})))^2+ {\rm Var}(\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})),
    \end{align*}
    where
    \begin{align*}
        &{\rm Bias}(\widetilde{\mathbf{g}}_w(w^k)) = \|\mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)] - \nabla_w\mathcal{L}(w^k)\|_2,\\
        &{\rm Var}(\widetilde{\mathbf{g}}_w(w^k)) = \mathbb{E}[\|\mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)] - \widetilde{\mathbf{g}}_w(w^k)\|_2^2],\\
        &{\rm Bias}(\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})) = \|\mathbb{E}[\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})] - \nabla_{\theta^l}\mathcal{L}(\theta^{l,k})\|_2,\\
        &{\rm Var}(\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})) = \mathbb{E}[\|\mathbb{E}[\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})] - \widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})\|_2^2].
    \end{align*}
    Suppose that Assumption \ref{assmp:proof} holds, then with {\small$\eta = \mathcal{O}(\varepsilon^2)$} and {\small$\beta_{i}=\mathcal{O}(\varepsilon^2)$}, {\small$i\in[n]$}, there exist {\small$C>0$} and {\small$\rho\in(0,1)$} such that for any $k\in\mathbb{N}^*$ and $l\in[L]$, the bias terms can be bounded as
    \begin{align*}
        &{\rm Bias}(\widetilde{\mathbf{g}}_w(w^k))\leq C\varepsilon + C\rho^{\frac{k-1}{2}},\\
        &{\rm Bias}(\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k}))\leq C\varepsilon + C\rho^{\frac{k-1}{2}}.
    \end{align*}
\end{theorem}


\begin{theorem}\label{thm:convergence_conv}
    Suppose that Assumption \ref{assmp:proof} holds. Besides, assume that the optimal value {\small$\loss^*=\inf_{w,\Theta}\loss(w,\Theta)$} is bounded by {\small$G$}. Then, with {\small$\eta=\mathcal{O}(\varepsilon^4)$}, {\small$\beta_{i}=\mathcal{O}(\varepsilon^4)$}, {\small$i\in[n]$}, and {\small$N=\mathcal{O}(\varepsilon^{-6})$}, LMC4Conv ensures to find an {\small$\varepsilon$}-stationary solution such that $\mathbb{E}[\|\nabla_{w,\Theta}\loss(w^R,\Theta^R)\|_2] \leq \varepsilon$ after running for $N$ iterations, where $R$ is uniformly selected from $[N]$ and $\Theta^R=(\theta^{l,R})_{l=1}^L$.
\end{theorem}





\subsection{LMC for RecGNNs}\label{subsec:lmc4rec}

In this section, we extend the idea for ConvGNNs to RecGNNs.
Similarly, LMC4Rec first efficiently estimates $\embH^{\Diamond}_{\mathcal{V}_{\mathcal{B}}}$ and $\embV^{\Diamond}_{\mathcal{V}_{\mathcal{B}}}$ based on {\it historical values} and {\it temporary values}, and then compute the mini-batch gradients as shown in Eqs. \eqref{eqn:mini-batch_grad_w} and \eqref{eqn:mini-batch_grad_theta}. We show that LMC4Rec converges to first-order stationary points of RecGNNs in Section \ref{subsubsec:lmc4rec_theoretical}. In Algorithm \ref{alg:lmc4rec} and Section \ref{subsubsec:lmc4rec_theoretical}, we denote a vector and a matrix at the $k$-th iteration by $(\cdot)^{\Diamond,k}$, but elsewhere we omit the superscipt $k$ and simply denote it by $(\cdot)^{\Diamond}$.


\subsubsection{Methodology}\label{subsubsec:lmc4rec_methodology}

\label{sec:cm_hist}



We store {\it historical embeddings and auxiliary variables} {\small$(\hisH^{\Diamond},\hisV^{\Diamond})$} to provide an affordable approximation. At each training iteration, we sample a mini-batch of nodes $\mathcal{V}_{\mathcal{B}}$, and update historical embeddings and auxiliary variables of nodes in the mini-batch, i.e., {\small$(\hisH^{\Diamond}_{\inbatch},\hisV^{\Diamond}_{\inbatch})_{l=1}^L$}.
Unlike LMC4Conv, LMC4Rec further computes the {\it temporary embeddings and auxiliary variables} $(\temH^{\Diamond}_{\mathcal{V}_{\mathcal{B}}}, \temV^{\Diamond}_{\mathcal{V}_{\mathcal{B}}})$ according to the recurrent architecture of RecGNNs and use them to approximate exact $(\embH^{\Diamond}_{\mathcal{V}_{\mathcal{B}}}, \embV^{\Diamond}_{\mathcal{V}_{\mathcal{B}}})$.


\udfsection{Update of {\small$\hisH^{\Diamond}_{\inbatch}$}.} In forward passes, we first update the historical embedding of each node $v_i\in\mathcal{V}_{\mathcal{B}}$ as
\begin{align}
    &\hish^{\Diamond}_i \leftarrow u_{\theta^{\Diamond}}(\hish^{\Diamond}_i, \overline{\embm}^{\Diamond}_{\neighbor{v_i}}, \embx_i);\nonumber\\
    & \overline{\embm}^{\Diamond}_{\neighbor{v_i}}=\aggregate_{\theta^{\Diamond}}(\{g_{\theta^{\Diamond}}(\hish_j^{\Diamond}) \mid v_j \in \neighbor{v_i}\})\label{eqn:his_update_rec},
\end{align}
We use the left arrow {\small$\leftarrow$} in Eq. \eqref{eqn:his_update_rec} to emphasize that the step is not to solve the fixed-point equation, but instead uses the historical embeddings in previous training iterations to update {\small$\hish^{\Diamond}_i$}.


\udfsection{Computation of {\small$\widehat{\embH}^{\Diamond}_{ \inbatch}$}.} Then, we compute temporary embeddings of nodes in $\mathcal{V}_{\mathcal{B}}$, i.e., $\temH^{\Diamond}_{\mathcal{V}_{\mathcal{B}}}$, by using iterative solvers to solve the local fixed-point equations
\begin{align}
    &\temh^{\Diamond}_i  =\update_{\theta^{\Diamond}}(\temh^{\Diamond}_i, \widehat{\embm}^{\Diamond}_{\neighbor{v_i}}  ,\embx_i);\nonumber\\ 
    &\widehat{\embm}_{\neighbor{v_i}}^{\Diamond} = \aggregate_{\theta^{\Diamond}}(\{g_{\theta^{\Diamond}}(\temh^{\Diamond}_j  ) \mid v_j \in \neighbor{v_i} \cap \mathcal{V}_{\mathcal{B}}\}\nonumber\\  
    &\quad\quad\quad\quad\quad\quad\quad \cup \{g_{\theta^{\Diamond}}(\hish^{\Diamond}_j  ) \mid v_j \in \neighbor{v_i} \setminus \mathcal{V}_{\mathcal{B}}\})\label{eqn:temp_rec}
\end{align}
We call {\small$\mathbf{C}_f^{\Diamond} \triangleq \aggregate_{\thetarec}(\{g_{\thetarec}(\hish_j^{\Diamond}) \} \mid v_j \in \neighbor{v_i} \setminus \mathcal{V}_{\mathcal{B}})$} the {\it local message compensation} in forward passes. The fixed point {\small$\temh_i^{\Diamond}  $} to Eq. \eqref{eqn:temp_rec} is an approximation solution of Eq. \eqref{eqn:mpeq_update}. 



\udfsection{Update of {\small$\hisV^{\Diamond}_{\inbatch}$}.} In backward passes, we first update the historical auxiliary variable of each node {\small$v_i\in\mathcal{V}_{\mathcal{B}}$} as
\begin{align}
    \hisV^{\Diamond}_i \leftarrow &\sum_{v_j \in \neighbor{v_i}} (\nabla_{\embh_i^{\Diamond}} u_{\theta^{\Diamond}}(\hish^{\Diamond}_j, \overline{\embm}^{\Diamond}_{\neighbor{v_j}}, \embx_j)) \hisV^{\Diamond}_j \nonumber\\
    &+ \nabla_{\embh^{\Diamond}_i} \loss(\hish^{\Diamond}_i), \label{eqn:his_aux_update_rec}
\end{align}
where {\small$\overline{\embm}^{\Diamond}_{\neighbor{v_j}}$} is computed as shown in Eq. \eqref{eqn:his_update_rec}.



\udfsection{Computation of {\small$\widehat{\embV}^{\Diamond}_{ \inbatch}$}.} Then, we compute temporary auxiliary variables of nodes in {\small$\mathcal{V}_{\mathcal{B}}$}, i.e., {\small$\temV^{\Diamond}_{\mathcal{V}_{\mathcal{B}}}$}, by using iterative solvers to solve the local fixed-point equations
\begin{align}
    \temV_{i}^{\Diamond}  = &\sum_{v_j \in\neighbor{v_i}\cap \inbatch} (\nabla_{\embh_i^{\Diamond}}\update_{\theta^{\Diamond}}(\temh_j^{\Diamond} ,\widehat{\embm}_{\neighbor{v_j}}^{\Diamond},\embx_j) ) \temV_{j}^{\Diamond} \nonumber\\
    &+ \sum_{v_j \in\neighbor{v_i}\backslash \inbatch} (\nabla_{\embh_i^{\Diamond}}\update_{\theta^{\Diamond}}(\hish_j^{\Diamond} ,\widehat{\embm}_{\neighbor{v_j}}^{\Diamond}  ,\embx_j) ) \hisV_{j}^{\Diamond} \nonumber\\
    &+ \nabla_{\embh_i^{\Diamond}} \loss(\temh^{\Diamond}_i), \label{eqn:temp_aux_rec} 
\end{align}
where {\small$\temh^{\Diamond}_j$} and {\small$\widehat{\embm}^{\Diamond}_{\neighbor{v_j}}$} are computed as shown in Eq. \eqref{eqn:temp_rec}. We call {\small$\mathbf{C}_b^{\Diamond}\triangleq \sum_{v_j \in\neighbor{v_i}\backslash \inbatch} (\nabla_{\embh_i^{\Diamond}}\update_{\theta^{\Diamond}}(\hish_j^{\Diamond} ,\widehat{\embm}_{\neighbor{v_j}}^{\Diamond}  ,\embx_j) ) \hisV_{j}^{\Diamond}$} the {\it local message compensation} in backward passes.


\udfsection{Mini-batch Gradients of LMC4Rec.} Combining Eqs. \eqref{eqn:mini-batch_grad_w} and \eqref{eqn:mini-batch_grad_theta} with {\small$(\widehat{\embh}_j^{\Diamond}, \widehat{\embm}^{\Diamond}_{\neighbor{v_j}}, \widehat{\embV}_j^{\Diamond})$} leads to
\begin{align}
    &\widetilde{\mathbf{g}}_{w}(\mathcal{V}_\mathcal{B})=\frac{1}{|\mathcal{V}_{L_{\mathcal{B}}}|}\sum_{v_j\in\mathcal{V}_{L_\mathcal{B}}} \nabla_{w} \ell_{w}(\widehat{\embh}_j,y_j),\label{eqn:grad_rec_w}\\
    &\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\mathcal{V}_\mathcal{B})=\frac{|\mathcal{V}|}{|\mathcal{V}_\mathcal{B}|}\sum_{v_j\in\mathcal{V}_\mathcal{B}}(\nabla_{\theta^{l}}u_{\theta^l}(\widehat{\embh}^{\Diamond}_j, \widehat{\embm}^{\Diamond}_{\neighbor{v_j}}, \embx_j))\widehat{\embV}^{\Diamond}_j.\label{eqn:grad_rec_theta} 
\end{align}




\udfsection{Time Complexity.} Notice that the size of Eq. \eqref{eqn:temp_rec} is linear with {\small$\mathcal{V}_{\mathcal{B}}$} rather than the whole graph. Suppose that the maximum neighborhood size is {\small$n_{\max}$} and the number of iterations is {\small$L$}, then the time complexity in forward passes is {\small$\mathcal{O}(L(n_{\max}|\inbatch|d+|\inbatch| d^2) )$}. In backward passes, LMC4Rec generates compensation messages from {\small$\widehat{\embm}_{\neighbor{v_j}}^{\Diamond}$}, {\small $v_j \in \neighbor{v_i}\setminus \inbatch$}, which depends on $2$-hop neighborhoods {\small$\kneighbor{\inbatch}{2}$}. To save computational costs, we compute the compensation term once in a single backward pass and reuse it in iterative solvers. Therefore, the time complexity in backward passes is {\small $\mathcal{O}(n_{\max}^2|\inbatch|d + Ln_{\max}|\inbatch|d )$}. As {\small$L$} is large in RecGNNs, the time complexity becomes {\small$\mathcal{O}( Ln_{\max}|\inbatch|d )$} by ignoring the term {\small$n_{\max}^2|\inbatch|d$}.



\udfsection{Space Complexity.} Similar to LMC4Conv, LMC4Rec additionally stores the historical node embeddings {\small$\hisH^{\Diamond} $} and auxiliary variables {\small$\hisV^{\Diamond}$}. For RecGCN, the active histories in forward and backward passes employ {\small$\mathcal{O}(n_{\max} |\inbatch| d)$} GPU memory, respectively. As the time and memory complexity are independent of the size of the whole graph, i.e., {\small$|\mathcal{V}|$}, LMC4Rec is scalable. We summarize the computational complexity in Appendix \ref{appendix:complexity}.





Fig. \ref{fig:message_flow} shows the message passing of backward SGD, Cluster-GCN \cite{cluster_gcn}, GAS \cite{gas}, and LMC4Rec. Compared with other subgraph-sampling methods (Cluster-GCN and GAS), LMC4Rec proposes compensation messages simultaneously in forward and backward passes, leading to similar convergence behaviors with few additional computational costs. Based on the modification, we show that LMC4Rec converges to stationary points of RecGNNs in Section \ref{subsubsec:lmc4rec_theoretical}.


\begin{algorithm}[H]
    \caption{LMC4Rec}
    \label{alg:lmc4rec}
    \begin{algorithmic}[1]
        \STATE {\bfseries Input:} 
        The learning rate {\small$\eta$}.
        \STATE Partition {\small$\mathcal{V}$} into {\small$B$} parts {\small$(\mathcal{V}_{b})_{b=1}^B$}
        \FOR{{$k = 1, \dots, N$}}
            \STATE Randomly sample {\small$\mathcal{V}_{b_k}$} from {\small$(\mathcal{V}_b)_{b=1}^B$}
            \STATE Update {\small $\hisH^{\Diamond}_{\mathcal{V}_{b_k}}$} by Eq. \eqref{eqn:his_update_rec}
            \STATE Compute {\small$\temH^{\Diamond}_{\mathcal{V}_{b_k}}$} by solving Eq. \eqref{eqn:temp_rec}
            \STATE Update {\small $\hisV^{\Diamond}_{\mathcal{V}_{b_k}}$} by Eq. \eqref{eqn:his_aux_update_rec}
            \STATE Compute {\small$\temV^{\Diamond}_{\mathcal{V}_{b_k}}$} by solving Eq. \eqref{eqn:temp_aux_rec}
            \STATE Compute {\small $\widetilde{\mathbf{g}}_w^k$} and {\small $\widetilde{\mathbf{g}}_{\thetarec}^k$} by Eqs. \eqref{eqn:grad_rec_w} and \eqref{eqn:grad_rec_theta}
            \STATE Update parameters by\\
            \quad\quad\quad {\small$w^k = w^{k-1} - \eta \widetilde{\mathbf{g}}_w^k$}\\
            \quad\quad\quad {\small$\theta^{\Diamond,k} = \theta^{\Diamond,k-1} - \eta\widetilde{\mathbf{g}}_{\theta^{\Diamond}}^k$}
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:lmc4rec} summarizes LMC4Rec.
We add a superscript {\small$k$} for each value to indicate that it is the value at the {\small$k$}-th iteration.
At the preprocessing step, we partition {\small$\mathcal{V}$} into {\small$B$} parts {\small$(\mathcal{V}_b)_{b=1}^B$}.
At the $k$-th training step, LMC4Rec first randomly samples a subgraph constructed by {\small$\mathcal{V}_{b_k}$}.
Notice that we sample more subgraphs to build a large graph in experiments, whose convergence analysis is consistent to that of sampling a single subgraph.
Then,  LMC4Rec updates the stored historical node embeddings $\hisH^{\Diamond,k}_{\mathcal{V}_{b_k}}$ by Eq. \eqref{eqn:his_update_rec} and auxiliary variables $\hisV^{\Diamond,k}_{\mathcal{V}_{b_k}}$ by Eq. \eqref{eqn:his_aux_update_rec}.
By the random update, the historical values get close to the exact up-to-date values.
Next, to approximate the exact node embeddings and auxiliary variables of $\mathcal{V}_{b_k}$ for computing mini-batch gradients, LMC4Rec recurrently aggregates the local messages from $\mathcal{V}_{b_k}$ and compensation messages from $\neighbor{\mathcal{V}_{b_k}} \setminus \mathcal{V}_{b_k}$ in Eqs. \eqref{eqn:temp_rec} and \eqref{eqn:temp_aux_rec}.
Finally, by replacing {\small$\embh_j^{\Diamond, k}$}, {\small$\embm^{\Diamond, k}_{\neighbor{v_j}}$}, and {\small$\embV_j^{\Diamond, k}$} in Eqs. \eqref{eqn:mini-batch_grad_w} and \eqref{eqn:mini-batch_grad_theta} with {\small$\temh_j^{\Diamond, k}$}, {\small $\widehat{\embm}_{\neighbor{v_j}}^{\Diamond, k}$}, and {\small$\temV_j^{\Diamond, k}$}, respectively, LMC4Rec computes mini-batch gradients {\small$\widetilde{\mathbf{g}}_{w}^k$} and {\small$\widetilde{\mathbf{g}}_{\thetarec}^k$} to update parameters {\small$w$} and {\small$\theta^{\Diamond}$}.



\subsubsection{Theoretical Results} \label{subsubsec:lmc4rec_theoretical}
In this section, we provide the theoretical analysis of LMC4Rec.
Theorem \ref{thm:grad_error_rec} shows that the biases of mini-batch gradients computed by LMC4Rec can tend to an arbitrarily small value by setting a proper learning rate. Then, Theorem \ref{thm:convergence_rec} shows that LMC4Rec converges to first-order stationary points of RecGNNs. We provide detailed proofs of the theorems in Appendix \ref{appendix:proof_rec}. In the theoretical analysis, we use the following assumptions.
\begin{assumption}\label{assmp:proof_rec}
    Assume that (1) at the {\small$k$}-th iteration, a batch of nodes {\small$\mathcal{V}_{\mathcal{B}}^k$} is uniformly sampled from {\small$\mathcal{V}$} and the corresponding labeled node set {\small$\mathcal{V}_{L_{\mathcal{B}}}^{k}=\mathcal{V}_{\mathcal{B}}^k \cap \mathcal{V}_L$} is uniformly sampled from {\small$\mathcal{V}_L$}, (2) functions {\small$f_{\theta^{\Diamond}}$}, {\small$\phi_{w,\theta^{\Diamond}}$}, {\small$\nabla_{w}\mathcal{L}$}, {\small$\nabla_{\theta^{\Diamond}}\mathcal{L}$}, {\small$\nabla_w\ell_{w}$}, and {\small$\nabla_{\theta^{\Diamond}} u_{\theta^{\Diamond}}$} are {\small$\gamma$}-Lipschitz with {\small$\gamma\in[0,1)$}, (3) norms {\small$\|\embH^{\Diamond,k}\|_F$}, {\small$\|\hisH^{\Diamond,k}\|_F$}, {\small$\|\temH^{\Diamond,k}\|_F$}, {\small$\|\embV^{\Diamond,k}\|_F$}, {\small$\|\hisV^{\Diamond,k}\|_F$}, {\small$\|\temV^{\Diamond,k}\|_F$}, {\small$\|\nabla_w\loss\|_2$}, {\small$\|\nabla_{\theta^{\Diamond}}\loss\|_2$}, {\small$\|\widetilde{\mathbf{g}}_{\theta^{\Diamond}}\|_2$}, and {\small$\|\widetilde{\mathbf{g}}_{w}\|_2$} are bounded by {\small$G>0$}, {\small$\forall\,k\in\mathbb{N}^*$}.
\end{assumption}

\begin{remark}
   The assumption that $f_{\theta^{\Diamond}}$ is $\gamma$-Lipschitz with $\gamma\in[0,1)$ is the standard contraction assumption \cite{contraction} for the fixed-point equation $\embH^{\Diamond} = f_{\theta^{\Diamond}}(\embH^{\Diamond};\embX)$, which enforces the existence and uniqueness of the fixed-point.
\end{remark}





\begin{theorem}\label{thm:grad_error_rec}
    For any $k\in\mathbb{N}^*$, the expectations of $\|\Delta_w^k\|_2^2 \triangleq \|\widetilde{\mathbf{g}}_{w}(w^{k}) - \nabla_w\loss(w^k)\|_2^2$ and $\|\Delta_{\theta^{\Diamond}}^k\|_2^2 \triangleq \|\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{k,\Diamond}) - \nabla_{\theta^{\Diamond}}\loss(\theta^{{\Diamond},k})\|_2^2$ have the bias-variance decomposition
    \begin{align*}
        &\mathbb{E}[\|\Delta_{w}^k\|_2^2] = ({\rm Bias}(\widetilde{\mathbf{g}}_w(w^k)))^2+{\rm Var}(\widetilde{\mathbf{g}}_w(w^k)),\\
        &\mathbb{E}[\|\Delta_{\theta^{\Diamond}}^k\|_2^2] = ({\rm Bias}(\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k})))^2+{\rm Var}(\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k})),
    \end{align*}
    where
    \begin{align*}
        &{\rm Bias}(\widetilde{\mathbf{g}}_w(w^k)) = \|\mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)] - \nabla_w\mathcal{L}_w(w^k)\|_2,\\
        &{\rm Var}(\widetilde{\mathbf{g}}_w(w^k)) = \mathbb{E}[\|\mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)] - \widetilde{\mathbf{g}}_w(w^k)\|_2^2],\\
        &{\rm Bias}(\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k}))= \|\mathbb{E}[\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k})] - \nabla_{\theta^{\Diamond}}\mathcal{L}_{\theta^{\Diamond}}(\theta^{\Diamond,k})\|_2,\\
        &{\rm Var}(\mathbf{g}_{\theta^{\Diamond}}(\theta^{\Diamond,k}))= \mathbb{E}[\|\mathbb{E}[\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k})] - \widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k})\|_2^2].
    \end{align*}
    Suppose that Assumption \ref{assmp:proof_rec} holds, then with $\eta = \mathcal{O}(\varepsilon)$, there exist $C>0$ and $\rho\in(0,1)$ such that for any $k\in\mathbb{N}^*$, the bias terms are bounded as
    \begin{align*}
        &{\rm Bias}(\widetilde{\mathbf{g}}_w(w^k)) \leq C\varepsilon + C\rho^{k-1},\\
        &{\rm Bias}(\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{{\Diamond},k})) \leq C\varepsilon + C\rho^{k-1}.
    \end{align*}
\end{theorem}


\begin{theorem}\label{thm:convergence_rec}
    Suppose that Assumption \ref{assmp:proof_rec} holds. Besides, assume that the optimal value $\mathcal{L}^*=\inf_{w,\theta^{\Diamond}} \loss(w,\theta^{\Diamond})$ is bounded by $G$. Then, with $\eta=\mathcal{O}(\varepsilon^2)$ and $N=\mathcal{O}(\varepsilon^{-4})$, LMC4Rec ensures to find an $\varepsilon$-stationary solution such that $\mathbb{E}[\|\nabla_{w,\theta^{\Diamond}}\loss(w^R,\theta^{\Diamond,R})\|_2] \leq \varepsilon$ after running for $N$ iterations, where $R$ is uniformly selected from $[N]$.
\end{theorem}


\section{Experiments} \label{sec:exp}

To demonstrate that LMC is an effective and widely-applicable algorithm, we conduct extensive experiments on large-scale benchmark tasks for both ConvGNNs (\modifyok{}{see Section \ref{sec:exp_convgnns}) and RecGNNs (see Section \ref{sec:exp_recgnns})}. The experiments are carried out on a single GeForce RTX 2080 Ti (11 GB).




\subsection{Experiments with ConvGNNs (Finite Number of MP Layers)}\label{sec:exp_convgnns}

We first introduce experimental settings in Section \ref{sec:setting_conv}. We then evaluate the convergence and the efficiency of LMC4Conv in Sections \ref{sec:scalable_conv} and \ref{sec:smallbatchsize_conv}. Finally, we conduct ablation studies about the proposed compensations in Section \ref{sec:ablation}.

\subsubsection{Experimental Settings} \label{sec:setting_conv}


\udfsection{Datasets.} Some recent works \cite{ogb} have indicated that many frequently-used graph datasets are too small compared with graphs in real-world applications.
Therefore, we evaluate LMC4Conv on four large datasets, PPI, REDDIT , FLICKR\cite{graphsage}, and Ogbn-arxiv \cite{ogb}.
These datasets contain thousands or millions of nodes/edges and have been widely used in previous works \cite{gas, graphsaint, graphsage, cluster_gcn, vrgcn, fastgcn}.
For more details, please refer to Appendix \ref{sec:dataset}.


\udfsection{Baselines and Implementation Details.} In terms of prediction performance, our baselines include node-wise sampling methods (GraphSAGE \cite{graphsage} and VR-GCN \cite{vrgcn}), layer-wise sampling methods (FastGCN \cite{fastgcn} and LADIES\cite{ladies}), subgraph-wise sampling methods (Cluster-GCN \cite{cluster_gcn}, GraphSAINT \cite{graphsaint}, FM \cite{graphfm} and GAS \cite{gas}), and a precomputing method (SIGN \cite{sign}).
By noticing that GAS achieves the state-of-the-art prediction performance (Table \ref{tab:largegraph}) among the baselines, we further compare the efficiency of LMC with GAS \cite{gas} and Cluster-GCN \cite{cluster_gcn}, another subgraph-wise sampling method using METIS partition. We implement LMC4Conv and Cluster-GCN based on the codes and toolkits of GAS \cite{gas} to ensure a fair comparison. For other implementation details, please refer to Appendix \ref{sec:implementation}.


\udfsection{Hyperparameters.} To ensure a fair comparison, we follow the data splits, training pipeline, and most hyperparameters in \cite{gas} except for the additional hyperparameters in LMC4Conv such as $\beta_i$. We use the grid search to find the best $\beta_i$ (see Appendix \ref{sec:selection_beta} for more details).


\begin{figure*}[ht]
\centering % <-- added
\begin{subfigure}{1.0\linewidth}
  \includegraphics[width=\linewidth]{imgs/convgnn/exp_sec2/trainloss_runtime.pdf}
  \caption{Training loss}\label{subfig:train_loss_runtime}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{1.0\linewidth}
  \includegraphics[width=\linewidth]{imgs/convgnn/exp_sec2/testacc_runtime.pdf}
    \caption{Testing accuracy}\label{subfig:test_acc_runtime}
\end{subfigure}\hfil
\caption{
Testing accuracy and training loss w.r.t. runtimes (s). The LMC in the figures refers to LMC4Conv.} \label{fig:runtime}
\end{figure*}


\subsubsection{LMC4Conv is Fast without Sacrificing Accuracy}\label{sec:scalable_conv}



Table \ref{tab:largegraph} reports the prediction performance of LMC4Conv and the baselines.
We report the mean and the standard deviation by running each experiment five times for GAS, FM, and LMC4Conv.
LMC4Conv, FM, and GAS all resemble full-batch performance on all datasets, while other baselines may fail, especially on the FLICKR dataset.
Moreover, LMC4Conv, FM, and GAS with deep ConvGNNs, e.g., GCNII \cite{gcnii}, outperform other baselines on all datasets.



\begin{table}[ht]
    \centering
      \caption{%
      \textbf{Prediction performance on large graph datasets.} OOM denotes the out-of-memory issue. Bold font indicates the best result and underline indicates the second best result.
        }\label{tab:largegraph}
    \setlength{\tabcolsep}{2.0mm}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{llcccc}
    \toprule
    \mc{2}{l}{\footnotesize{\textbf{\#\,nodes}}} & \footnotesize{230K} & \footnotesize{57K} & \footnotesize{89K} & \footnotesize{169K} \\[-0.1cm]
    \mc{2}{l}{\footnotesize{\textbf{\#\,edges}}} & \footnotesize{11.6M} & \footnotesize{794K} & \footnotesize{450K} & \footnotesize{1.2M} \\[-0.05cm]
    \mc{2}{l}{\mr{2}{\textbf{Method}}} & \mr{2}{\textsc{Reddit}} & \mr{2}{\textsc{PPI}} & \mr{2}{\textsc{Flickr}} & \texttt{ogbn-} \\
    & & & & & \texttt{arxiv} \\
    \midrule
    \mc{2}{l}{\textsc{GraphSAGE}}   & 95.40          & 61.20          & 50.10          & 71.49          \\
    \mc{2}{l}{\textsc{VR-GCN}}      & 94.50          & 85.60          & ---            & ---            \\
    \mc{2}{l}{\textsc{FastGCN}}     & 93.70          & ---            & 50.40          & ---            \\
    \mc{2}{l}{\textsc{LADIES}}      & 92.80          & ---            & ---            & ---            \\
    \mc{2}{l}{\textsc{Cluster-GCN}} & 96.60          & \underline{99.36}          & 48.10          & ---            \\
    \mc{2}{l}{\textsc{GraphSAINT}}  & 97.00          & \textbf{99.50}          & 51.10          & ---            \\
    \mc{2}{l}{\textsc{SIGN}}        & \underline{96.80}          & 97.00          & 51.40          & ---            \\
    \midrule
    \mr{2}{\rotatebox{90}{\footnotesize{\,GD}}}
    & ~~\textsc{GCN}   & 95.43 & 97.58 & 53.73 & 71.64 \\
    & ~~\textsc{GCNII} & OOM   & OOM   & 55.28 & \textbf{72.83} \\
    \midrule
    \mr{2}{\rotatebox{90}{\footnotesize{GAS}}}
    & ~~\textsc{GCN}                 & \acc{95.35}{0.01} & \acc{98.91}{0.03} & \acc{53.44}{0.11} & \acc{71.54}{0.19}\\
    & ~~\textsc{GCNII}               & \acc{96.73}{0.04} & \acc{\underline{99.36}}{0.02} & \textbf{\acc{55.42}{0.27}} & \acc{72.50}{0.28}\\
    \midrule
    \mr{2}{\rotatebox{90}{\footnotesize{FM}}}
    & ~~\textsc{GCN}                 & \acc{95.27}{0.03} & \acc{98.91}{0.01} & \acc{53.48}{0.17} & \acc{71.49}{0.33}\\
    & ~~\textsc{GCNII}               & \acc{96.52}{0.06} & \acc{99.34}{0.03} & \acc{54.68}{0.27} & \acc{72.54}{0.27}\\
    \midrule
    \mr{2}{\rotatebox{90}{\footnotesize{\textbf{LMC}}}}
    & ~~\textsc{GCN}                 & \acc{95.44}{0.02} & \acc{98.87}{0.04} & \acc{53.80}{0.14} & \acc{71.44}{0.23}\\
    & ~~\textsc{GCNII}               & \textbf{\acc{96.88}{0.03}} & \acc{99.32}{0.01}    & \acc{\underline{55.36}}{0.49}    & \acc{\underline{72.76}}{0.22}\\
    \bottomrule
  \end{tabular}
    }
\end{table}



\begin{table*}[ht]\centering
      \caption{%
      \textbf{ Efficiency of Cluster-GCN, GAS, and LMC4Conv.}
      }\label{tab:memory}
    \setlength{\tabcolsep}{1.9mm}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccccccccc}
    \toprule
    \mr{2}{\textbf{Dataset} \& \textbf{ConvGNN}} & \mc{3}{c}{\textbf{Epochs}} & \mc{3}{c}{\textbf{Runtime} (s)} & \mc{3}{c}{\textbf{Memory} (MB)} \\
    & {\small Cluster-GCN} & {\small GAS} & {\small LMC4Conv} &  {\small Cluster-GCN} & {\small GAS} & {\small LMC4Conv}  & {\small Cluster-GCN} & {\small GAS} & {\small LMC4Conv} \\
    \midrule
    Ogbn-arxiv \& GCN   & \acc{211.0}{75.6} & \acc{176.0}{43.4} & \textbf{\acc{124.4}{34.2}} &\acc{108}{39}& \acc{79}{20} & \textbf{\acc{55}{15}} & \textbf{424} & 452 & 557 \\
    FLICKR \& GCN       &\acc{379.2}{29.0}& \acc{389.4}{21.2} & \textbf{\acc{334.2}{18.6}} &\acc{127}{10}& \acc{117}{7} & \textbf{\acc{85}{5}}  & \textbf{310} & 375 & 376 \\
    REDDIT \& GCN       & \acc{239.0}{26.2} & \acc{372.4}{55.2} & \textbf{\acc{166.8}{20.9}} &\acc{516}{57}& \acc{790}{114} & \textbf{\acc{381}{47}} & \textbf{1193} & 1508 & 1829 \\
    PPI \& GCN          &\acc{428.0}{45.8}& \acc{293.6}{11.9} & \textbf{\acc{290.2}{28.5}} &\acc{359}{38}& \textbf{\acc{179}{9}} & \textbf{\acc{179}{18}} & \textbf{212} & 214 & 267 \\
    \midrule
    Ogbn-arxiv \& GCNII & --- & \acc{234.8}{30.2}  & \textbf{\acc{197.4}{34.7}} & --- & \acc{218}{28}  & \textbf{\acc{178}{31}} & --- & \textbf{453} & 568 \\
    FLICKR \& GCNII     & --- & \textbf{\acc{352}{54}} & \acc{356}{64} & --- & \textbf{\acc{465}{71}} & \acc{475}{85} & --- & \textbf{396} & 468 \\
    \bottomrule
  \end{tabular}
    }
\end{table*}


As LMC4Conv, FM, and GAS share a similar prediction performance, we additionally compare the convergence speed of LMC4Conv, FM, GAS, and Cluster-GCN, another subgraph-wise sampling method using METIS partition, in Fig. \ref{fig:runtime} and Table \ref{tab:memory}. We use a sliding window to smooth the convergence curve in Fig. \ref{fig:runtime} as the accuracy on test data is unstable.
The solid curves correspond to the mean, and the shaded regions correspond to values within plus or minus one standard deviation of the mean.
Table \ref{tab:memory} reports the number of epochs, the runtime to reach the full-batch accuracy in Table \ref{tab:largegraph}, and the GPU memory.
As shown in Table \ref{tab:memory} and Fig. \ref{subfig:train_loss_runtime}, LMC4Conv is significantly faster than GAS, especially with a speed-up of 2x on the REDDIT dataset.
Notably, the test accuracy of LMC4Conv is more stable than GAS, and thus the smooth test accuracy of LMC4Conv outperforms GAS in Fig. \ref{subfig:test_acc_runtime}.
Although GAS finally resembles full-batch performance in Table \ref{tab:largegraph} by selecting the best performance on the valid data,
it may fail to resemble under small batch sizes due to its unstable process (see Section \ref{sec:smallbatchsize_conv}).
Another appealing feature of LMC4Conv is that it shares comparable GPU memory costs with GAS, and thus avoiding the neighbor explosion problem.
FM is slower than other methods, as they additionally update historical embeddings in the storage for the nodes outside the mini-batches. Please see Appendix \ref{sec:exp_epochtime} for the comparison in terms of training time per epoch.









To further illustrate the convergence of LMC4Conv, we compare the errors of mini-batch gradients computed by Cluster-GCN, GAS, and LMC4Conv. At epoch training step, we record the relative errors {\small$\|\widetilde{\mathbf{g}}_{\theta^{l}} - \nabla_{\theta^{l}} \loss\|_2\,/\,\| \nabla_{\theta^{l}} \loss\|_2$}, where $\nabla_{\theta^{l}} \loss$ is the full-batch gradient for the parameters $\theta^{l}$ at the $l$-th MP layer and the $\widetilde{\mathbf{g}}_{\theta^{l}}$ is a mini-batch gradient.
To avoid the randomness of the full-batch gradient $\nabla_{\theta^{l}} \loss$, we set the dropout rate as zero.
We report average relative errors during training in Fig. \ref{fig:error_conv}.
LMC4Conv enjoys the smallest estimated errors in the experiments.


\begin{figure}[ht]
\centering % <-- added
\begin{subfigure}{0.33\linewidth}
  \includegraphics[width=\linewidth]{imgs/convgnn/exp_sec3/error_arxiv.pdf}
  \caption{Ogbn-arxiv}\label{subfig:error_arxiv_conv}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.33\linewidth}
  \includegraphics[width=\linewidth]{imgs/convgnn/exp_sec3/error_flickr.pdf}
    \caption{FLICKR}\label{subfig:error_flickr_conv}
\end{subfigure}\hfil
\begin{subfigure}{0.33\linewidth}
  \includegraphics[width=\linewidth]{imgs/convgnn/exp_sec3/error_reddit.pdf}
    \caption{REDDIT}\label{subfig:error_reddit}
\end{subfigure}\hfil
\caption{The average relative estimated errors of mini-batch gradients computed by Cluster-GCN, GAS, and LMC4Conv for GCN.} \label{fig:error_conv}
\end{figure}


\subsubsection{LMC4Conv is Robust in terms of Batch Sizes}\label{sec:smallbatchsize_conv}



An appealing feature of mini-batch training methods is to be able to avoid the out-of-memory issue by decreasing the batch size. Thus, we evaluate the prediction performance of LMC4Conv on Ogbn-arxiv datasets with different batch sizes (numbers of clusters).
We conduct experiments under different sizes of sampled clusters per mini-batch.
We run each experiment with the same epoch and search learning rates in the same set.
We report the best prediction accuracy in Table \ref{tab:batchsize_conv}.
LMC4Conv outperforms GAS under small batch sizes (batch size = $1$ or $2$) and achieve a comparable performance with GAS (batch size = $5$ or $10$).


\begin{table}[ht]
  \centering
  \caption{\textbf{Performance under different batch sizes on the Ogbn-arxiv dataset.}
  }\label{tab:batchsize_conv}
  \setlength{\tabcolsep}{8pt}
  \resizebox{1.0\linewidth}{!}{%
  \begin{tabular}{ccccc}
    \toprule
    \mr{2}{Batch size} & \mc{2}{l}{\quad\quad\,\,\, GCN} & \mc{2}{l}{\quad\quad \,\,\,GCNII} \\
     & GAS & LMC4Conv & GAS & LMC4Conv \\
    \midrule
    1 & 70.56 & \textbf{71.65} & 71.34 & \textbf{72.11} \\ 
    2 & 71.11 & \textbf{71.89} & 72.25 & \textbf{72.55} \\ 
    5 & \textbf{71.99} & 71.84 & 72.23 & \textbf{72.87}   \\
    10& 71.60 & \textbf{72.14} & \textbf{72.82} & 72.80 \\
    \bottomrule
  \end{tabular}
  }
\end{table}




\begin{figure*}[t]
\centering % <-- added
\begin{subfigure}{0.5\linewidth}
  \includegraphics[width=\linewidth]{imgs/convgnn/ablation/train_acc_epoch_ablation_small.pdf}
  \caption{Under small batch sizes}\label{subfig:ablation_small}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.5\linewidth}
  \includegraphics[width=\linewidth]{imgs/convgnn/ablation/train_acc_epoch_ablation_large.pdf}
    \caption{Under large batch sizes}\label{subfig:ablation_large}
\end{subfigure}\hfil
\caption{The improvement of the compensations on the Ogbn-arixv dataset.} \label{fig:ablation}
\end{figure*}


\begin{figure*}[b]
\centering % <-- added
\begin{subfigure}{1.0\linewidth}
  \includegraphics[width=\linewidth]{imgs/revised_figure/small/random_acc.pdf}
    \caption{Testing accuracy}\label{subfig:test_acc_random}
\end{subfigure}\hfil
\begin{subfigure}{1.0\linewidth}
  \includegraphics[width=\linewidth]{imgs/revised_figure/small/random_loss.pdf}
  \caption{Training loss}\label{subfig:train_loss_random}
\end{subfigure}\hfil % <-- added
\caption{
Testing accuracy and training loss w.r.t. the number of epochs under random partitions.} \label{fig:convergence_random}
\end{figure*}



\subsubsection{Ablation}\label{sec:ablation}
The improvement of LMC4Conv is due to two parts: the compensation in forward passes $\mathbf{C}_f^l$ and the compensation in back passes $\mathbf{C}_b^l$.
Compared with GAS, the compensation in forward passes $\mathbf{C}_f^l$ additionally combines the incomplete up-to-date messages.
Fig. \ref{fig:ablation} show the convergence curves of LMC4Conv using both $\mathbf{C}_f^l$ and $\mathbf{C}_b^l$ (denoted by $\mathbf{C}_f$\&$\mathbf{C}_b$), LMC4Conv using only $\mathbf{C}_f^l$ (denoted by $\mathbf{C}_f$), and GAS on the Ogbn-arixv dataset.
Under small batch sizes, the improvement mainly comes from $\mathbf{C}_b^l$ and the incomplete up-to-date messages in forward passes may hurt the performance. This is because the mini-batch and the union of their neighbors are hard to contain most neighbors of out-of-batch nodes when the batch size is small.
Thus, the compensation in back passes $\mathbf{C}_b^l$ is the most important component by correcting the bias of the mini-batch gradients.
Under large batch sizes, the improvement is due to $\mathbf{C}_f^l$, as the large batch sizes decrease the discarded messages and improve the accuracy of the mini-batch gradients.
Notably, $\mathbf{C}_b^l$ still slightly improves the performance.






\subsection{Experiments with RecGNNs (Infinite Number of MP Layers)} \label{sec:exp_recgnns}


We first introduce experimental settings in Section \ref{sec:setting_rec}. We then evaluate the convergence of LMC4Rec in Section \ref{sec:exp_fast}. Finally, we demonstrate that LMC4Rec can efficiently scale RecGNNs to large-scale graphs in Sections \ref{sec:scalable_rec} and \ref{sec:smallbatchsize_rec}.


\subsubsection{Experimental Settings} \label{sec:setting_rec}


We evaluate LMC4Rec for RecGCN (see Appendix \ref{sec:recgcn} for details) on three small datasets, including Cora, Citeseer, and PubMed from Planetoid \cite{planetoid}, and five large datasets, PPI, Reddit \cite{graphsage}, AMAZON \cite{amazon}, Ogbn-arxiv, and Ogbn-products \cite{ogb}. On the three small datasets, we evaluate the convergence performance and the ability to learn long-range dependencies. On the five large datasets, we evaluate the efficiency of scalable methods. We use the same data splits, training pipeline, subgraph partitions, and GNN structures for a fair comparison. We report the embedding dimensions, the learning rates, the number of partitions, and sampled clusters per mini-batch for each dataset in Table \ref{tab:hyperparameters} in Appendix \ref{sec:hyperparameters_recgcn}. We provide implementation details for important baselines such as GAS and Cluster-GCN in Appendix \ref{sec:implement_baseline}. For other implementation details, please refer to Appendix \ref{sec:recgcn}.

\begin{figure*}[t]
\centering % <-- added
\begin{subfigure}{1.0\linewidth}
  \includegraphics[width=\linewidth]{imgs/revised_figure/small/metis_acc.pdf}
    \caption{Testing accuracy}\label{subfig:test_acc_metis}
\end{subfigure}\hfil
\begin{subfigure}{1.0\linewidth}
  \includegraphics[width=\linewidth]{imgs/revised_figure/small/metis_loss.pdf}
  \caption{Training loss}\label{subfig:train_loss_metis}
\end{subfigure}\hfil % <-- added
\caption{
Testing accuracy and training loss w.r.t. the number of epochs under METIS partitions.} \label{fig:convergence_metis}
\end{figure*}


\subsubsection{LMC4Rec is Convergent}\label{sec:exp_fast}


To evaluate the convergence of LMC4Rec, we implement backward SGD introduced in Section \ref{sec:naive_sgd} as an exact baseline. We compare LMC4Rec with backward SGD, the state-of-the-art mini-batch training method for RecGNNs (SSE \cite{sse}), and subgraph-wise sampling methods (Cluster-GCN \cite{cluster_gcn} and GAS \cite{gas}). We plot the training loss and the testing accuracy on the small datasets under random and METIS partitions in Figs. \ref{fig:convergence_random} and \ref{fig:convergence_metis} respectively. We provide the runtime in Appendix \ref{sec:acc_runtime_small_rec}. We run each experiment with five different random seeds. The solid curves correspond to the mean, and the shaded regions correspond to values within plus or minus one standard deviation of the mean. Although LMC4Rec uses mini-batch gradients based on historical information to update parameters, the convergence behavior of LMC4Rec is comparable with that of backward SGD, which computes the exact mini-batch gradients. Moreover, LMC4Rec outperforms other scalable algorithms for RecGNNs in terms of  convergence speed. Although the running time of LMC4Rec each epoch is slightly more than state-of-the-art subgraph-wise sampling methods, LMC4Rec can accelerate the training of RecGCN due to the improvement of the convergence speed, as shown in Section \ref{sec:scalable_rec}.







Figs. \ref{fig:convergence_random} and \ref{fig:convergence_metis} demonstrate that LMC4Rec is robust in terms of different subgraph partitions. Random partitions randomly select a set of nodes to construct a subgraph, resulting in a set of unconnected nodes in a sampled subgraph. Thus, under random partitions, the compensation messages are critical to recovering the external information out of the subgraph for subgraph-wise sampling methods. Besides, METIS partitions help minimize the ignored inter-connectivity among subgraphs by constructing subgraphs with connected nodes, which hides the weakness of ignoring the external information. However, METIS can not remove the whole inter-connectivity among subgraphs. When the inter-connectivity among subgraphs is important, these subgraph-wise sampling methods also suffer from sub-optimal performance under METIS partitions (see Section \ref{sec:scalable_rec}). 
SSE is slower than LMC4Rec on all datasets, as the gradient computed by SSE is the first-order approximated solution to Eq. \eqref{eqn:mpeq_grad}, which is very different from the exact gradient.  By noticing that LMC4Rec also benefits from METIS partitions\footnote{METIS increases the range of learning rates for LMC4Rec and hence accelerates convergence.}, we use METIS partitions in the experiments in Sections \ref{sec:scalable_rec}.


\begin{figure}[ht]
\centering % <-- added
    \begin{subfigure}{.5\linewidth}
        \includegraphics[width=0.97\linewidth]{imgs/exp_sec1/herror_random.pdf}
        \caption{$\hisH_{\inbatch}$ \& random}\label{subfig:random_herror}
    \end{subfigure}\hfil
    \begin{subfigure}{.5\linewidth}
      \includegraphics[width=0.97\linewidth]{imgs/exp_sec1/graderror_random.pdf}
        \caption{$\overline{\embV}_{\inbatch}$ \& random}\label{subfig:random_graderror}
    \end{subfigure}\hfil % <-- added
    \begin{subfigure}{.5\linewidth}
      \includegraphics[width=0.97\linewidth]{imgs/exp_sec1/herror_metis.pdf}
      \caption{$\hisH_{\inbatch}$ \& METIS}\label{subfig:metis_herror}
    \end{subfigure}\hfil
    \begin{subfigure}{.5\linewidth}
      \includegraphics[width=0.97\linewidth]{imgs/exp_sec1/graderror_metis.pdf}
        \caption{$\overline{\embV}_{\inbatch}$ \& METIS}\label{subfig:metis_graderror}
    \end{subfigure}\hfil % <-- added
    \caption{
    The average relative estimated errors of node embeddings $\hisH_{\inbatch}$ and auxiliary variables $\overline{\embV}_{\inbatch}$ under random and METIS partitions. 
    } \label{fig:error_rec}
\end{figure}




To further illustrate the convergence of LMC4Rec, we compare estimated errors of node embeddings and auxiliary variables computed by SSE, Cluster-GCN, GAS, and LMC4Rec. For a subgraph $\inbatch$, we use the node embeddings and auxiliary variables computed by backward SGD as the exact values. We report average relative estimated errors of node embeddings {$\|\hisH_{\inbatch} - \embH_{\inbatch}\|_F\,/\,\|\embH_{\inbatch}\|_F$} and auxiliary variables {$ \|\overline{\embV}_{\inbatch} - \embV_{\inbatch} \|_F\,/\,\|\embV_{\inbatch} \|_F$} in Fig. \ref{fig:error_rec}. LMC4Rec enjoys the smallest estimated errors of gradients in the experiments.



\begin{table*}[t]
    \centering
      \caption{%
      \textbf{Efficiency of \textsc{RecGCN} with Cluster-GCN, GAS, and LMC4Rec.}
      }\label{tab:runtime}
    \setlength{\tabcolsep}{2mm}{
    \begin{tabular}{lccccccccc}
    \toprule
    \mr{2}{\textbf{Dataset}} & \mc{3}{c}{\textbf{Epochs}} & \mc{3}{c}{\textbf{Runtime} (s)} & \mc{3}{c}{\textbf{Memory} (MB)} \\
    & {\small Cluster-GCN} & {\small GAS} & {\small LMC4Rec} & {\small Cluster-GCN} & {\small GAS} & {\small LMC4Rec} & {\small Cluster-GCN} & {\small GAS} & {\small LMC4Rec} \\
    \midrule
    PPI & 1000 & 353 & \textbf{301} & 36020 & 13873 & \textbf{13245}  & \textbf{1999} & 3517 & 4103 \\
    REDDIT & 1000 & 119 & \textbf{102}  & 5487 & 1138 & \textbf{999}  & \textbf{5159} & 7347 & 8009 \\
    AMAZON & 1000 & 607 & \textbf{485} & 6662 & 5291 & \textbf{4414}  & \textbf{1925} & 1947 & 1955   \\
    Ogbn-arxiv & 1000 & 353  & \textbf{332}& 3865 & 2148  & \textbf{2035} & \textbf{1865}  & 3453 & 3453 \\
    Ogbn-products & 1000 & 319 & \textbf{270} & 30018 & 18560 & \textbf{16450} & \textbf{2345} & 8151 & 8295 \\
    \bottomrule
  \end{tabular}
    }
\end{table*}


\subsubsection{LMC4Rec is Fast}\label{sec:scalable_rec}


In Section \ref{sec:exp_fast}, we demonstrate that LMC4Rec slightly outperforms GAS \cite{gas} in terms of convergence speed under METIS partitions. However, as LMC4Rec additionally computes local message compensation for gradients $\mathbf{C}_b$, whether the improvement of convergence speed accelerates the training of RecGNNs is unclear. We thus compare the runtime of LMC4Rec, Cluster-GCN, and GAS on the five large datasets. Table \ref{tab:runtime} reports the GPU memory, the number of epochs and time to reach given 98.5\%, 96\%, 87\%, 72\%, and 77\% accuracy\footnote{Cluster-GCN does not reach the given accuracy on the datasets (see accuracy vs. runtime in Appendix \ref{sec:acc_runtime_large_rec}).} on PPI, REDDIT, AMAZON, Ogbn-arxiv, and Ogbn-products, respectively (see Appendix \ref{sec:acc_runtime_large_rec} for the convergence curve). As the additional computation of LMC4Rec is very cheap, LMC4Rec is faster than Cluster-GCN and GAS on the five large datasets due to the improvement of convergence speed.


\subsubsection{LMC4Rec is Robust in terms of Batch Sizes}\label{sec:smallbatchsize_rec}

We evaluate the prediction performance of LMC4Rec on AMAZON and Ogbn-arxiv datasets with different batch sizes (numbers of clusters). We set the number of partitions to 80 and 400 on Ogbn-arxiv and AMAZON, respectively, and conduct experiments under different sizes of sampled clusters per mini-batch. We run each experiment with the same epoch and report the best prediction accuracy in Table \ref{tab:batchsize_rec}. The performance of LMC4Rec outperforms GAS in each experiment, especially on the AMAZON dataset with small batch sizes. By noticing that AMAZON is a standard dataset to evaluate the ability to learn long-range dependencies \cite{ignn}, LMC4Rec can retrieve the inter-connectivity among sampled subgraphs discarded by subgraph partitions to help extract long-range patterns, while GAS, the state-of-the-art subgraph-wise sampling method, may fail. We include more experiments in terms of prediction performance in Appendix \ref{sec:prediction_large_rec}.



\begin{table}[ht]
  \centering
  \caption{\textbf{Performance under different batch sizes.}
  }\label{tab:batchsize_rec}
  \setlength{\tabcolsep}{8pt}
  \resizebox{1.0\linewidth}{!}{%
  \begin{tabular}{ccccc}
    \toprule
    \mr{2}{Batch size} & \mc{2}{l}{\quad\,\, AMAZON} & \mc{2}{l}{\quad\,\,Ogbn-arxiv} \\
     & GAS & LMC4Rec & GAS & LMC4Rec \\
    \midrule
    1 & 83.04 & \textbf{87.03} & 72.28 & \textbf{72.52} \\
    2 & 84.90 & \textbf{87.35} & 72.10 & \textbf{72.12} \\ 
    5 & 87.48 & \textbf{87.71} & 72.19 & \textbf{72.38}   \\ 
    10& 87.34 & \textbf{87.83} & 71.92 & \textbf{72.64} \\ 
    \bottomrule
  \end{tabular}
  }
\end{table}


\section{Conclusion}\label{sec:conclusion}

In this paper, we propose a novel and scalable subgraph-wise sampling with provable convergence for GNNs, namely local message compensation (LMC), based on the message passing formulation of backward passes. We show that LMC converges to stationary points of GNNs. To the best of our knowledge, LMC is the first subgraph-wise sampling method with convergence guarantees. Experiments demonstrate that LMC significantly outperforms state-of-the-art training methods in terms of efficiency without sacrificing accuracy on large-scale benchmark tasks.


\bibliographystyle{IEEEtran}
\bibliography{LMC@TPAMI}


% if have a single appendix:
%\appendix[Proof of the Zonklar Eqs.]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%



% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural formi
  \section*{\modifyok{}{Acknowledgments}}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi


\modifyok{}{The authors would like to thank all the anonymous reviewers for their insightful comments. This work was supported in part by National Nature Science Foundations of China grants U19B2026, U19B2044, 61836011, 62021001, and 61836006, and the Fundamental Research Funds for the Central Universities grant WK3490000004.}


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% \begin{thebibliography}{1}

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to {\LaTeX}}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{imgs/photos/jiewang.jpeg}}]{Jie Wang}
  received the B.Sc. degree in electronic information science and technology from University of Science and Technology of China, Hefei, China, in 2005, and the Ph.D. degree in computational science from the Florida \mbox{State} University, Tallahassee, FL, in 2011. He is currently a professor in the Department of Electronic Engineering and Information Science at University of Science and Technology of China, Hefei, China. His research interests include reinforcement learning, knowledge graph, large-scale optimization, deep learning, etc.  He is a senior member of IEEE.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{imgs/photos/zhihaoshi.jpeg}}]{Zhihao Shi}
  received the B.Sc. degree in Department of Electronic Engineering and Information Science from University of Science and Technology of China, Hefei, China, in 2020. a Ph.D. candidate in the Department of Electronic Engineering and Information Science at University of Science and Technology of China, Hefei, China. His research interests include graph representation learning and natural language processing.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{imgs/photos/xizeliang.jpg}}]{Xize Liang}
  received the B.Sc degree in information and computing sciences from the University of Science and Technology of China, Hefei, China, in 2022. He is currently a graduate student in the Department of Electronic Engineering and Information Science at the University of Science and Technology of China. His research interests include graph representation learning and AI for Science.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{imgs/photos/shuiwangji.jpeg}}]{Shuiwang Ji}
  received the PhD degree in computer science from Arizona State University, Tempe, Arizona, in 2010. Currently, he is an Associate Professor in the Department of Computer Science and Engineering, Texas A\&M University, College Station, Texas. His research interests include machine learning, deep learning, data mining, and computational biology. He received the National Science Foundation CAREER Award in 2014. He is currently an Associate Editor for IEEE Transactions on Pattern Analysis and Machine Intelligence, ACM Transactions on Knowledge Discovery from Data, and ACM Computing Surveys. He regularly serves as an Area Chair or equivalent roles for data mining and machine learning conferences, including AAAI, ICLR, ICML, IJCAI, KDD, and NeurIPS. He is a Fellow of IEEE.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.5in,clip,keepaspectratio]{imgs/photos/lb.pdf}}]{Bin Li}
received the B.Sc. degree in electrical engineering from Hefei University of Technology, Hefei, China, in 1992, the M.Sc. degree from the Institute of Plasma Physics, Chinese Academy of Sciences, Hefei, in 1995, and the Ph.D. degree in Electronic Science and Technology from the University of Science and Technology of China (USTC), Hefei, in 2001. He is currently a Professor at the School of Information Science and Technology, USTC. He has authored or co-authored over 60 refereed publications. His current research interests include evolutionary computation, pattern recognition, and human-computer interaction. Dr. Li is the Founding Chair of IEEE Computational Intelligence Society Hefei Chapter, a Counselor of IEEE USTC Student Branch, a Senior Member of Chinese Institute of Electronics (CIE), and a member of Technical Committee of the Electronic Circuits and Systems Section of CIE. He is a Member of IEEE.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{imgs/photos/fengwu.jpeg}}]{Feng Wu}
received the B.S. degree in electrical engineering from Xidian University in 1992, and the M.S. and Ph.D. degrees in computer science from the Harbin Institute of Technology in 1996 and 1999, respectively. He is currently a Professor with the University of Science and Technology of China, where he is also the Dean of the School of Information Science and Technology. Before that, he was a Principal Researcher and the Research Manager with Microsoft Research Asia. His research interests include image and video compression, media communication, and media analysis and synthesis. He has authored or coauthored over 200 high quality articles (including several dozens of IEEE Transaction papers) and top conference papers on MOBICOM, SIGIR, CVPR, and ACM MM. He has 77 granted U.S. patents. His 15 techniques have been adopted into international video coding standards. As a coauthor, he received the Best Paper Award at 2009 IEEE Transactions on Circuits and Systems for Video Technology, PCM 2008, and SPIE VCIP 2007. He also received the Best Associate Editor Award from IEEE Circuits and Systems Society in 2012. He also serves as the TPC Chair for MMSP 2011, VCIP 2010, and PCM 2009, and the Special Sessions Chair for ICME 2010 and ISCAS 2013. He serves as an Associate Editor for IEEE Transactions on Circuits and Systems for Video Technology, IEEE Transactions ON Multimedia, and several other international journals.
\end{IEEEbiography}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}




\clearpage
\appendices

\section{More Details about Experiments} \label{appendix:details_experiments}
In this section, we introduce more details about our experiments, including datasets, training and evaluation protocols, and implementations.

\subsection{Datasets} \label{sec:dataset}
We evaluate LMC on Cora, Citeseer, PubMed \cite{planetoid}, PPI, REDDIT, FLICKR \cite{graphsage}, AMAZON \cite{amazon}, Ogbn-arxiv \cite{ogb}, and Ogbn-products \cite{ogb}.


All of the datasets do not contain personally identifiable information or offensive content.
Table \ref{tab:datasets} shows the summary statistics of the datasets.
Details about the datasets are as follows.
\begin{itemize}[leftmargin=5mm]
    \item Cora, Citeseer, and PubMed are directed citation networks. Each node indicates a paper with the corresponding bag-of-words features and each directed edge indicates that one paper cites another one. The task is to classify academic papers into different subjects.
    \item PPI contains 24 protein-protein interaction graphs. Each graph corresponds to a human tissue. Each node indicates a protein with positional gene sets, motif gene sets and immunological signatures as node features. Edges represent interactions between proteins. The task is to classify protein functions.
    (proteins) and edges (interactions).
    \item REDDIT is a post-to-post graph constructed from REDDIT. Each node indicates a post and each edge between posts indicates that the same user comments on both. The task is to classify REDDIT posts into different communities based on (1) the GloVe CommonCrawl word vectors \cite{glove} of the post titles and comments, (2) the posts scores, and (3) the number of comments made on the posts.
    \item AMAZON is an Amazon product co-purchasing network. Each node indicates a product and each edge between two products indicates that the products are purchased together. The task is to predict product types without node features based on rare labeled nodes. We set the training set fraction be 0.06\% in experiments.
    \item Ogbn-arxiv is a directed citation network between all Computer Science (CS) arXiv papers indexed by MAG \cite{mag}. Each node is an arXiv paper and each directed edge indicates that one paper cites another one. The task is to classify unlabeled arXiv papers into different primary categories based on labeled papers and node features, which are computed by averaging word2vec \cite{word2vec} embeddings of words in papers' title and abstract.
    \item FLICKR categorizes types of images based on their descriptions and properties \cite{gas, graphsaint}.
    \item Ogbn-product is a large Amazon product co-purchasing network with rich node features. Each node indicates a product and each edge between two products indicates that the products are purchased together. The task is to predict product types based on low-dimensional bag-of-words features of product descriptions processed by Principal Component Analysis.
\end{itemize}

\begin{table}[htbp]
  \begin{center}
    \caption{Statistics of the datasets used in our experiments.
    }\label{tab:datasets}
    \vspace{5pt}
    \scalebox{0.9}{
    \begin{tabular}{ccccc}
    \toprule
    \textbf{Dataset} & \textbf{\#Graphs} & \textbf{\#Classes} &\textbf{Total \#Nodes} & \textbf{Total \#Edges}  \\
      \midrule
      \midrule
      Cora & 1 & 7  & 2,708 & 5,278\\
      Citeseer & 1 & 6  & 3,327 & 4,552 \\
      PubMed & 1 & 3  & 19,717 & 44,324 \\
      \midrule
      PPI & 24 & 121 & 56,944 & 793,632 \\
      REDDIT & 1 & 41 & 232,965 & 11,606,919   \\
      AMAZON & 1 & 58 & 334,863 & 925,872   \\
      Ogbn-arxiv & 1 & 40  & 169,343 & 1,157,799  \\
      FLICKR & 1 & 7  & 89,250 & 449,878  \\
       Ogbn-product & 1 & 47  & 2,449,029 & 61,859,076 \\
      \bottomrule
    \end{tabular}
    }
  \end{center}
\end{table} 




% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
\subsection{Training and Evaluation Protocols}\label{sec:training_evaluation}

We run all the experiments on a single GeForce RTX 2080 Ti (11 GB). All the models are implemented in Pytorch \cite{pytorch} and PyTorch Geometric \cite{pyg} based on the official implementation of \cite{gas}\footnote{\url{https://github.com/rusty1s/pyg_autoscale}. The owner does not mention the license.}. We use the data splitting strategies following previous works \cite{gas, ignn}.




\subsection{Implementation Details and Hyperparameters}\label{sec:implementation}


\subsubsection{Normalization Technique} \label{sec:normalization}

In Section \ref{sec:naive_sgd} in the main text, we assume that the subgraph $\inbatch$ is uniformly sampled from $\mathcal{V}$ and the corresponding set of labeled nodes {\small$\mathcal{V}_{L_{\mathcal{B}}} = \inbatch \cap \mathcal{V}_{L}$} is uniformly sampled from {\small$\mathcal{V}_{L}$}. To enforce the assumption, we use the normalization technique to reweight Eqs. (5) and (6) in the main text.



Suppose we partition the whole graph $\mathcal{V}$ into $b$ parts $\{\mathcal{V}_{\mathcal{B}_i}\}_{i=1}^b$ and then uniformly sample $c$ clusters without replacement to construct subgraph $\inbatch$. By the normalization technique, Eq. (5) becomes
\begin{align}
    \mathbf{g}_w(\inbatch) = \frac{b|\mathcal{V}_{L_{\mathcal{B}}}|}{c|\mathcal{V}_L|} \frac{1}{|\mathcal{V}_{L_{\mathcal{B}}}|} \sum_{v_j \in \mathcal{V}_{L_{\mathcal{B}}}}\nabla_{w} \ell_w(\embh_j ,y_j),
\end{align}
where $\frac{b|\mathcal{V}_{L_{\mathcal{B}}}|}{c|\mathcal{V}_L|}$ is the corresponding weight. Similarly, Eq. (6) becomes
\begin{align}
    \mathbf{g}_{\theta}(\inbatch) = \frac{b|\inbatch|}{c|\mathcal{V}|} \frac{|\mathcal{V}|}{|\inbatch|} \sum_{v_j \in \inbatch} \nabla_{\theta} \update(\embh_j ,\embm_{\neighbor{v_j}}  ,\embx_j) \embV_{j} \label{eqn:sgd_theta},
\end{align}
where $\frac{b|\inbatch|}{c|\mathcal{V}|}$ is the corresponding weight.

\begin{table*}[ht]
  \centering
  \caption{%
  \textbf{The hyperprameters used in the experiments for LMC4Rec.}
  }\label{tab:hyperparameters}
  \setlength{\tabcolsep}{5pt}
  \resizebox{1.0\linewidth}{!}{%
  \begin{tabular}{lcccccc}
    \toprule
    \textbf{Dataset}  & \textbf{Dimensions} & \textbf{Partitions} & \textbf{Clusters} & \textbf{LR (METIS partition)} & \textbf{LR (random partition)} \\
    \midrule
    \textsc{Cora} & 128 & 10 & 2 & 0.003 & 0.001\\
    \textsc{CiteSeer} & 128 & 10 & 2 & 0.01 & 0.001 \\
    \textsc{PubMed} & 128 & 10 & 2 & 0.003 & 0.0003 \\
    \textsc{PPI} & 1024 & 10 & 2 & 0.01 & - \\
    \textsc{REDDIT} & 256 & 200 & 100 & 0.003 & - \\
    \textsc{AMAZON} & 64 & 40 & 1 & 0.003 & - \\
    \textsc{Ogbn-arxiv} & 256 & 80 & 10 & 0.003 & - \\
    \textsc{Ogbn-products} & 256 & 1500 & 10 & 0.001 & - \\
    \bottomrule
  \end{tabular}
  }
\end{table*}

\subsubsection{Implement of Baslines}\label{sec:implement_baseline}

For a fair comparison, we implement GAS by setting the gradient compensation $\mathbf{C}_b$ to be zero. We implement Cluster-GCN by removing edges between partitioned subgraphs and then running GAS based on them.

\subsubsection{Other Implementation Details for ConvGNN}

\udfsection{Incorporating Batch Normalization for ConvGNNs.} We uniformly sample a mini-batch of nodes $\inbatch$ and generate the induced subgraph of $\neighbor{\inbatch}$.
If we directly feed the $\embH_{\neighbor{\inbatch}}^{(l)}$ to a batch normalization layer, the learned mean and standard deviation of the batch normalization layer may be biased.
Thus, LMC first feeds the embeddings of the mini-batch $\embH_{\inbatch}^{(l)}$ to a batch normalization layer and then feeds the embeddings outside the mini-batch $\embH_{\neighbor{\inbatch}\backslash\inbatch}^{(l)}$ to another batch normalization layer.

\udfsection{Selection of $\beta_{i}$.} \label{sec:selection_beta} We select $\beta_i = score(i) \alpha $ for each node $v_i$, where $\alpha \in [0,1]$ is a hyperparameter and $score$ is a function to measure the quality of the incomplete up-to-date messages.
We search $score$ in a $\{f(x)=x^2,f(x)=2x-x^2,f(x)=x,f(x)=1;x= deg_{local}(i)/deg_{global}(i)\}$, where $deg_{global}(i)$ is the degree of node $i$ in the whole graph and $deg_{local}(i)$ is the degree of node $i$ in the subgraph induced by $\neighbor{\inbatch}$.



\subsubsection{Hyperparameters for RecGNNs} \label{sec:hyperparameters_recgcn}

We report the embedding dimensions, the number of partitions, sampled clusters per mini-batch, and learning rates (LR) for each dataset in Table \ref{tab:hyperparameters}.
For the small datasets, we partition each graph into ten subgraphs. For the large datasets, we select the number of partitions and sampled clusters per mini-batch to avoid running out of memory on GPUs.
For a fair comparison, we use the same hyperparameters except for the learning rate. We search the best learning rate in $\{0.01, 0.003, 0.001\}$ for each training method on the large datasets and use the same learning rate on the small datasets.


\subsubsection{Other Implementation Details for RecGNNs} 
\label{sec:recgcn}

\udfsection{An Efficient Implement of LMC for RecGCN.} The equilibrium equations of RecGCN (see Equation \eqref{eqn:transformation_rec} in the main text) are
\begin{align*}
    \embH  = \sigma(  \mathbf{W} \embH   \hat{\mathbf{A}} + \mathbf{b}(\embX) ),
\end{align*}
where the nonlinear activation function $\sigma(\cdot)$ is the ReLU activation $\sigma(\cdot)=\max(\cdot,0)$, the function $\mathbf{b}(\embX) = \mathbf{P}\embX +\mathbf{c}$ is an affine function with parameters $\mathbf{P} \in \mathbb{R}^{d \times d_{x}},\mathbf{c} \in \mathbb{R}^{d}$
, the matrix $\mathbf{\hat{A}} = (\mathbf{D}+\mathbf{I})^{-1/2} (\mathbf{A}+\mathbf{I})(\mathbf{D}+\mathbf{I})^{-1/2}$ is the normalized adjacency matrix with self-loops, and $\mathbf{D}$ is the degree matrix. Let $\mathbf{Z} = \mathbf{W} \embH   \hat{\mathbf{A}} + \mathbf{b}(\embX)$.
The Jacobian vector-product is $\langle \vec{\embV}, \nabla_{\embH} \vec{f}_{\theta} \rangle$ \cite{ignn} is
\begin{align*}
     \mathbf{W}^{\top} (\sigma'(\mathbf{Z}) \odot \embV) \hat{\mathbf{A}}^{\top}.
\end{align*}


For two nodes $v_i,v_j$ such that $v_i \in \mathcal{N}(v_j)$, we have
\begin{align*}
    \vec{\embV}_i^{\top} \frac{\partial [f_{\theta}]_i}{\partial \embh_{j}} = \mathbf{W}^{\top} \left(\sigma({\mathbf{Z}}_i)' \odot {\embV}_i\right) \mathbf{\hat{A}}_{ij},
\end{align*}
which is only depend on the nodes $v_i,v_j$ rather than the 2-hop neighbors of $v_j$. Therefore, by additionally storing the historical auxiliary variables $\mathbf{Z}$, we implement LMC based on 1-hop neighbors, which employs $\mathcal{O}(n_{\max} |\inbatch| d)$ GPU memory in backward passes.


\udfsection{Randomly Using Dropout at Different Training Steps for RecGNNs.} We propose a trick to handle the randomness introduced by dropout \cite{dropout}. Let $\dropout(\embX) = \frac{1}{1-p} \mathbf{M} \circ \embX$ be the dropout operation, where $\mathbf{M}_{ij} \sim \text{Bern}(1-p)$ are i.i.d Bernoulli random variables, and $\circ$ is the element-wise product. As shown by \cite{vrgcn}, with dropout \cite{dropout} of input features $\mathbf{X}$,  historical embeddings and auxiliary variables become random variables, leading to inaccurate compensation messages.
Specifically, the solutions to Equations (1) and (3) in the main text are the function of the dropout features $\frac{1}{1-p} \mathbf{M}^{(k)} \circ \embX$ at the training step $k$, while the dropout features become $\frac{1}{1-p} \mathbf{M}^{(k+1)} \circ \embX$ at the training step $k+1$ (we assume that the learning rate $\eta=0$ to simplify the analysis). As the historical information under the dropout operation at the training step $k$ may be very inaccurate at the training step $k+1$, \cite{vrgcn} propose to first compute the random embeddings with dropout and the mean embeddings without dropout, and then use the mean embeddings to update historical information.
However, simultaneously computing two versions of embeddings in RecGNNs leads to double computational costs of solving Equations (1) and (3).
We thus propose to randomly use the dropout operation at different training steps and update the historical information when the dropout operation is invalid. Specifically, at each training step $k$, we either update the historical information without the dropout operation with probability $q$ or use the dropout operation without updating the historical information. We set $q=0.5$ in all experiments. Due to the trick, the historical embeddings and auxiliary variables depend on the stable input features $\embX$ rather than the random dropout features $\frac{1}{1-p} \mathbf{M} \circ \embX$. The trick can reduce overfitting and control variate for dropout efficiently.





\section{Well-posedness Conditions of RecGNNs}\label{sec:well-posedness}



In this section, we provide tractable well-posedness conditions \cite{ignn} of RecGNNs to ensure the existence and uniqueness of the solution to Eq. (1) in the main text.


The Eq. (1)  in the main text of RecGNNs with the message passing functions in GCN can be formulated as
\begin{align*}
    \embH   = f_{\theta}(\embH  ;\embX) = \sigma(  \mathbf{W} \embH   \hat{\mathbf{A}} + \mathbf{b}(\embX) ),
\end{align*}
where $\mathbf{\hat{A}}$ is the aggregation matrix,  $\sigma(\cdot)$ is the nonlinear activation function, and $\mathbf{b}(\embX) $ is an affine function to encode the input features.
The Perron-Frobenius (PF) sufficient condition for well-posedness \cite{idl} requires the activation function $\sigma$ is component-wise non-expansive and $\lambda_{pf}(|\hat{\mathbf{A}}^{\top} \otimes \mathbf{W}|) = \lambda_{pf}(\hat{\mathbf{A}})\lambda_{pf}(|\mathbf{W}|) < 1$
, where $\otimes$ is the Kronecker product.
As pointed out in \cite{ignn}, if the Perron-Frobenius (PF) sufficient condition holds, the solution $\embH  $ can be achieved by iterating Eq. (1) in the main text to convergence.



We further discuss how to enforce the non-convex constraint $\lambda_{pf}(|\mathbf{W}|) < (\lambda_{pf}(\hat{\mathbf{A}}))^{-1}$.
As $\lambda_{pf}(|\mathbf{W}|) \leq \| \mathbf{W} \|_p$ holds for induced norms $\| \cdot \|_p$, we enforce the stricter condition $\| \mathbf{W} \|_p < \lambda_{pf}(\hat{\mathbf{A}})^{-1}$. As pointed out in \cite{ignn}, if $p=1$ or $p=\infty$, we efficiently implement $\| \mathbf{W} \|_p < 1$ by projection. We follow the implementation in experiments and view $p$ as a hyperparameter.


\section{Computational Complexity} \label{appendix:complexity}


We summarize the computational complexity in Tables \ref{tab:complexity_conv} and \ref{tab:complexity_rec}, where $n_{\max}$ is the maximum of neighborhoods, $L$ is the number of message passing layers/iterations, $\mathcal{V}_{\mathcal{B}}$ is a set of nodes in a sampled mini-batch, $d$ is the embedding dimension, $\mathcal{V}$ is the set of nodes in the whole graph, and $\mathcal{E}$ is the set of edges in the whole graph.
As GD, backward SGD, Cluster-GCN, GAS, and LMC share the same memory complexity of parameters $\theta^{l}$ and $\theta^{\Diamond}$, we omit them in Tables \ref{tab:complexity_conv} and \ref{tab:complexity_rec}.

\begin{table}[htbp]
    \centering
    \caption{
    Time and memory complexity per gradient update for ConvGNNs (e.g. GCN \cite{gcn} and GCNII \cite{gcnii}).
    }
    \label{tab:complexity_conv}
    \scalebox{0.9}{
    \begin{tabular}{ccc}
    \toprule
        \textbf{Method} & \textbf{Time}  &\textbf{Memory}   \\
        \midrule
        GD and backward SGD  & $\mathcal{O}(L(|\mathcal{E}|d+|\mathcal{V}| d^2))$ & $\mathcal{O}(L|\mathcal{V}| d)$ \\
        Cluster-GCN \cite{cluster_gcn}  & $\mathcal{O}( L(n_{\max}|\inbatch|d+|\inbatch| d^2) )$ & $\mathcal{O}(L|\inbatch| d)$ \\
        GAS \cite{gas} & $\mathcal{O}( L(n_{\max}|\inbatch|d+|\inbatch| d^2) )$ & $\mathcal{O}( n_{\max} L|\inbatch| d)$ \\
        \midrule
        LMC4Conv & $\mathcal{O}( L(n_{\max}|\inbatch|d+|\inbatch| d^2) )$ & $\mathcal{O}(n_{\max} L|\inbatch| d)$ \\
    \bottomrule
    \end{tabular}
    }
\end{table}

\begin{table}[htbp]
    \centering
    \caption{
    Time and memory complexity for RecGCN.
    }
    \label{tab:complexity_rec}
    \scalebox{0.9}{
    \begin{tabular}{ccc}
    \toprule
        \textbf{Method} & \textbf{Time}  &\textbf{Memory}   \\
        \midrule
        GD and backward SGD  & $\mathcal{O}(L(|\mathcal{E}|d+|\mathcal{V}| d^2))$ & $\mathcal{O}(|\mathcal{V}| d)$ \\
        Cluster-GCN \cite{cluster_gcn}  & $\mathcal{O}( L(n_{\max}|\inbatch|d+|\inbatch| d^2) )$ & $\mathcal{O}(|\inbatch| d)$ \\
        GAS \cite{gas} & $\mathcal{O}( L(n_{\max}|\inbatch|d+|\inbatch| d^2) )$ & $\mathcal{O}( n_{\max} |\inbatch| d)$ \\
        \midrule
        LMC4Rec & $\mathcal{O}( L(n_{\max}|\inbatch|d+|\inbatch| d^2) )$ & $\mathcal{O}(n_{\max} |\inbatch| d)$ \\
    \bottomrule
    \end{tabular}
    }
\end{table}


\section{Additional Related Work}\label{appendix:add_related}

\subsection{Graph Neural Networks}\label{sec:related_gnn}

Graph neural networks (GNNs) aim to learn node embeddings by iteratively aggregating features and structure information of neighborhoods. Most graph neural networks for node property prediction on static graphs are categorized into convolutional graph neural networks (ConvGNNs) and recurrent graph neural networks (RecGNNs) \cite{comprehensive}. 

\subsubsection{Convolutional Graph Neural Networks}

\modifyok{}{ConvGNNs \cite{gcn, gat, gcnii} use $T$ different graph convolutional layers to learn node embeddings, where $T\in\mathbb{N}^*$ is a hyperparameter. 
Many ConvGNNs focus on the design of the message passing layer, i.e., aggregation and update operations.
For example, GCN \cite{gcn} proposes to aggregate the neighbor information by normalized averaging and GAT \cite{gat} introduces the attention mechanism into aggregation.
However, these models achieve the best performance with shallow architectures due to the \textit{over-smoothing} issues, i.e., the node embeddings of ConvGNNs may tend to be indistinguishable as $T$ increases.
To alleviate over-smoothing, GCNII \cite{gcnii} proposes the initial residual and identity mapping to develop deep GNNs.
}

\subsubsection{Recurrent Graph Neural Networks}


Inspired by \cite{idl, deq}, many researchers focus on RecGNNs recently, which approximate infinite MP layers using a shared MP layer until convergence.
Many works \cite{recgnn1, contraction, fdgnn, ignn} demonstrate that RecGNNs can effectively capture long-range dependencies. However, designing robust and scalable training methods for RecGNNs is challenging, limiting the real-world applications of RecGNNs. To improve the robustness, implicit graph neural networks (IGNNs) establish tractable well-posedness conditions and use projected gradient descent to guarantee well-posedness \cite{ignn}. Our proposed LMC4Rec focuses on the scalable training of RecGNNs, orthogonal to IGNNs.
In Appendix \ref{sec:sse}, we also discuss stochastic steady-state embedding (SSE), a scalable training algorithm for RecGNNs.


\subsection{Main differences between LMC and GraphFM}\label{sec:diff_graphfm}
First, LMC focuses on the convergence of subgraph-wise sampling methods, which is orthogonal to the idea of GraphFM-OB to alleviate the staleness problem of historical values. The advanced approach to alleviating the staleness problem of historical values can further improve the performance of LMC and is easy to establish provable convergence by the extension of LMC.

Second, LMC uses nodes in both mini-batches and their $1$-hop neighbors to compute incomplete up-to-date messages. In contrast, GraphFM-OB only uses nodes in the mini-batches. For the nodes whose neighbors are contained in the union of the nodes in mini-batches and their $1$-hop neighbors, the aggregation results of LMC are exact, while those of GraphFM-OB are not.

Third, by noticing that aggregation results are biased and the I/O bottleneck for the history access, LMC does not update the historical values in the storage outside the mini-batches. However, GraphFM-OB updates them based on the aggregation results.

\subsection{Stochastic steady-state embedding for RecGNNs}\label{sec:sse}
Stochastic Steady-state Embedding (SSE) \cite{sse} proposes a scalable training algorithm for RecGNNs, which uses a sampling method, namely stochastic fixed-point iteration, to reach global equilibrium points at each forward pass. The main differences between our proposed LMC and SSE are as follows. First, SSE performs message passing once in a backward pass, while LMC performs message passing many times until the iteration converges to a stable solution. Thus, the gradients computed by LMC are more accurate than SSE. Second, we show that LMC converges to first-order stationary points of RecGNNs, while SSE does not provide convergence analysis.

\section{Detailed proofs} \label{appendix:proof}

\subsection{Proof of Theorem \ref{thm:unbiased}: unbiased mini-batch gradients of backward SGD} \label{appendix:proof_thm1}

In this section, we give the proof of Theorem \ref{thm:unbiased}, which shows that the mini-batch gradients computed by backward SGD are unbiased.

\begin{proof}
    As $\mathcal{V}_{L_{\mathcal{B}}} = \inbatch \cap \mathcal{V}_{L}$ is uniformly sampled from $\mathcal{V}_{L}$, the expectation of $ \mathbf{g}_w(\inbatch)$ is
    \begin{align*}
        \mathbb{E}[\mathbf{g}_w(\inbatch)] &=\mathbb{E}[ \frac{1}{|\mathcal{V}_{L_{\mathcal{B}}}|} \sum_{v_j \in \mathcal{V}_{L_{\mathcal{B}}}}\nabla_{w} \ell_w(\embh_j ,y_j)]\\
        &= \nabla_{w} \mathbb{E}[ \ell_w(\embh_j ,y_j)]\\
        &= \nabla_{w} \loss.
     \end{align*}
     As the subgraph $\inbatch$ is uniformly sampled from $\mathcal{V}$, the expectation of $\mathbf{g}_{\theta^{l}}(\inbatch)$ is
    \begin{align*}
        \mathbb{E}[\mathbf{g}_{\theta^{l}}(\inbatch)] &= \mathbb{E}[\frac{|\mathcal{V}|}{|\mathcal{V}_\mathcal{B}|}\sum_{v_j\in\mathcal{V}_\mathcal{B}}\left(\nabla_{\theta^{l}}u_{\theta^l}(\embh^{l-1}_j, \embm^{l-1}_{\neighbor{v_j}}, \embx_j)\right) \embV_{j}^{l}] \\
        &= |\mathcal{V}| \mathbb{E}[ \nabla_{\theta^{l}}u_{\theta^l}(\embh^{l-1}_j, \embm^{l-1}_{\neighbor{v_j}}, \embx_j)  \embV_{j}^{l} ]\\
        &=|\mathcal{V}| \frac{1}{|\mathcal{V}|} \sum_{v_j \in \mathcal{V}} \nabla_{\theta^{l}}u_{\theta^l}(\embh^{l-1}_j, \embm^{l-1}_{\neighbor{v_j}}, \embx_j) \embV_{j}^{l}\\
        &= \sum_{v_j \in \mathcal{V}} \nabla_{\theta^{l}}u_{\theta^l}(\embh^{l-1}_j, \embm^{l-1}_{\neighbor{v_j}}, \embx_j) \embV_{j}^{l}\\
        &= \nabla_{\theta^{l}} \loss,\,\,\forall\,l\in[L].
    \end{align*}
    Similarly, we can show that $\mathbf{g}_{\theta^{\Diamond}}(\mathcal{V}_{\mathcal{B}})$ is unbiased.
\end{proof}

\subsection{Proofs for LMC4Conv}\label{appendix:proof_conv}

{\bf Notations.} Unless otherwise specified, $C$ and $C'$ with any superscript or subscript denotes constants. We denote the learning rate by $\eta$.

In this section, we suppose that Assumption \ref{assmp:proof} holds.





\subsubsection{Differences between exact values at adjacent iterations}

We first show that the differences between the exact values of the same layer in two adjacent iterations can be bounded by setting a proper learning rate.

\begin{lemma}\label{prop:exact_difference_h}
    Suppose that Assumption \ref{assmp:proof} holds. Given an $L$-layer ConvGNN, for any $\varepsilon>0$, by letting
    \begin{align*}
        \eta\leq\frac{\varepsilon}{(2\gamma)^L G}<\varepsilon,
    \end{align*}
    we have
    \begin{align*}
        \|\embH^{l,k+1}-\embH^{l,k}\|_F < \varepsilon,\,\,\forall\,l\in[L],\,k\in\mathbb{N}^*.
    \end{align*}
\end{lemma}
\begin{proof}
    Since $\eta\leq\frac{\varepsilon}{(2\gamma)^L G}<\frac{\varepsilon}{\gamma(2\gamma)^{L-1}G}$, we have
    \begin{align*}
        \|\embH^{1,k+1} - \embH^{1,k}\|_F ={}& \|f_{\theta^{1,k+1}}(\embX) - f_{\theta^{1,k}}(\embX)\|_F\\
        \leq{}& \gamma \|\theta^{1,k+1} - \theta^{1,k}\|\\
        \leq{}& \gamma \|\widetilde{\mathbf{g}}_{\theta^1}\|\eta\\
        <{}& \frac{\gamma G \varepsilon}{\gamma(2\gamma)^{L-1} G}\\
        ={}& \frac{\varepsilon}{(2\gamma)^{L-1}}.
    \end{align*}
    Then, because $\eta\leq\frac{\varepsilon}{(2\gamma)^L G}<\frac{\varepsilon}{(2\gamma)^{L-1}G}$, we have
    \begin{align*}
        \|\embH^{2,k+1} - \embH^{2,k}\|_F ={}& \|f_{\theta^{2,k+1}}(\embH^{1,k+1}) - f_{\theta^{2,k}}(\embH^{1,k})\|_F\\
        \leq{}& \|f_{\theta^{2,k+1}}(\embH^{1,k+1}) - f_{\theta^{2,k}}(\embH^{1,k+1})\|_F \\ 
        &+\|f_{\theta^{2,k}}(\embH^{1,k+1}) - f_{\theta^{2,k}}(\embH^{1,k})\|_F\\
        \leq{}& \gamma \|\theta^{2,k+1} - \theta^{2,k}\|\\
        &+ \gamma \|\embH^{1,k+1} - \embH^{1,k}\|_F\\
        \leq{}& \gamma G \eta + \frac{\varepsilon}{2(2\gamma)^{L-2}}\\
        <{}& \frac{\varepsilon}{2(2\gamma)^{L-2}}+\frac{\varepsilon}{2(2\gamma)^{L-2}}\\
        ={}& \frac{\varepsilon}{(2\gamma)^{L-2}}.
    \end{align*}
    And so on, we have
    \begin{align*}
        \|\embH^{l,k+1} - \embH^{l,k}\|_F < \frac{\varepsilon}{(2\gamma)^{L-l}},\,\,\forall\,l\in[L],\,k\in\mathbb{N}^*.
    \end{align*}
    Since $(2\gamma)^{L-l}>1$, we have
    \begin{align*}
        \|\embH^{l,k+1} - \embH^{l,k}\|_F < \varepsilon,\,\,\forall\,l\in[L],\,k\in\mathbb{N}^*.
    \end{align*}
\end{proof}

\begin{lemma}\label{prop:exact_difference_v}
    Suppose that Assumption \ref{assmp:proof} holds. Given an $L$-layer ConvGNN, for any $\varepsilon>0$, by letting
    \begin{align*}
        \eta\leq\frac{\varepsilon}{(2\gamma)^{L-1} G}<\varepsilon,
    \end{align*}
    we have
    \begin{align*}
        \|\embV^{l,k+1}-\embV^{l,k}\|_F < \varepsilon,\,\,\forall\,l\in[L],\,k\in\mathbb{N}^*.
    \end{align*}
\end{lemma}
\begin{proof}
    Since $\eta\leq\frac{\varepsilon}{(2\gamma)^{L-1} G}<\frac{\varepsilon}{\gamma(2\gamma)^{L-2}G}$, we have
    \begin{align*}
        &\|\embV^{L-1,k+1} - \embV^{L-1,k}\|_F\\
        ={}& \|\phi_{\theta^{L,k+1}}(\nabla_{\embH}\loss) - \phi_{\theta^{L,k}}(\nabla_{\embH}\loss)\|_F\\
        \leq{}& \gamma \|\theta^{L,k+1} - \theta^{L,k}\|\\
        \leq{}& \gamma \|\widetilde{\mathbf{g}}_{\theta^L}\|\eta\\
        <{}& \frac{\gamma G \varepsilon}{\gamma(2\gamma)^{L-2} G}\\
        ={}& \frac{\varepsilon}{(2\gamma)^{L-2}}.
    \end{align*}
    Then, because $\eta\leq\frac{\varepsilon}{(2\gamma)^{L-1} G}<\frac{\varepsilon}{(2\gamma)^{L-2}G}$, we have
    \begin{align*}
        &\|\embV^{L-2,k+1} - \embV^{L-2,k}\|_F\\
        ={}& \|\phi_{\theta^{L-1,k+1}}(\embV^{L-1,k+1}) - \phi_{\theta^{L-1,k}}(\embV^{L-1,k})\|_F\\
        \leq{}& \|\phi_{\theta^{L-1,k+1}}(\embV^{L-1,k+1}) - \phi_{\theta^{L-1,k}}(\embV^{L-1,k+1})\|_F\\
        &+ \|\phi_{\theta^{L-1,k}}(\embV^{L-1,k+1}) - \phi_{\theta^{L-1,k}}(\embV^{L-1,k})\|_F\\
        \leq{}& \gamma \|\theta^{L-1,k+1} - \theta^{L-1,k}\| + \gamma \|\embV^{L-1,k+1} - \embV^{L-1,k}\|_F\\
        \leq{}& \gamma G \eta + \frac{\varepsilon}{2(2\gamma)^{L-3}}\\
        <{}& \frac{\varepsilon}{2(2\gamma)^{L-3}}+\frac{\varepsilon}{2(2\gamma)^{L-3}}\\
        ={}& \frac{\varepsilon}{(2\gamma)^{L-3}}.
    \end{align*}
    And so on, we have
    \begin{align*}
        \|\embV^{l,k+1} - \embV^{l,k}\|_F < \frac{\varepsilon}{(2\gamma)^{l-1}},\,\,\forall\,l\in[L],\,k\in\mathbb{N}^*.
    \end{align*}
    Since $(2\gamma)^{l-1}>1$, we have
    \begin{align*}
        \|\embV^{l,k+1} - \embV^{l,k}\|_F < \varepsilon,\,\,\forall\,l\in[L],\,k\in\mathbb{N}^*.
    \end{align*}
\end{proof}

\subsubsection{Historical values and temporary values}




Suppose that we uniformly sample a mini-batch $\mathcal{V}_{\mathcal{B}}^k\subset \mathcal{V}$ at the $k$-th iteration and $|\mathcal{V}_{\mathcal{B}}^k| = S$. For the simplicity of notations, we denote the temporary node embeddings and auxiliary variables in the $l$-th layer by $\temH^{l,k}$ and $\temV^{l,k}$, respectively, where
\begin{align*}
    \temH^{l,k}_i = 
    \begin{cases}
        \temh^{l,k}_i, &v_i\in\neighbor{\mathcal{V}_{\mathcal{B}}^{k}}\setminus \mathcal{V}_{\mathcal{B}}^{k},\\
        \hish^{l,k}_i, &{\rm otherwise,} 
    \end{cases}
\end{align*}
and
\begin{align*}
    \temV^{l,k}_i = 
    \begin{cases}
        \temv^{l,k}_i, &v_i\in\neighbor{\mathcal{V}_{\mathcal{B}}^{k}}\setminus \mathcal{V}_{\mathcal{B}}^{k},\\
        \hisv^{l,k}_i, &{\rm otherwise.} 
    \end{cases}
\end{align*}
We abbreviate the process that LMC updates the node embeddings and auxiliary variables of $\mathcal{V}_{\mathcal{B}}^k$ in the $l$-th layer at the $k$-th iteration as
    \begin{align*}
        &\hisH^{l,k}_{\mathcal{V}_{\mathcal{B}}^k} = [f_{\theta^{l,k}}(\temH^{l-1,k})]_{\mathcal{V}_{\mathcal{B}}^k},\\
        &\hisV^{l,k}_{\mathcal{V}_{\mathcal{B}}^k} = [\phi_{\theta^{l+1,k}}(\temH^{l+1,k})]_{\mathcal{V}_{\mathcal{B}}^k}.
    \end{align*}
    


For each $v_i\in\mathcal{V}_{\mathcal{B}}^k$, the update process of $v_i$ in the $l$-th layer at the $k$-th iteration can be expressed by
\begin{align*}
    &\hish^{l,k}_i = f_{\theta^{l,k},i}(\temH^{l-1,k}),\\
    &\hisV^{l,k}_i = \phi_{\theta^{l+1,k},i}(\temV^{l+1,k}),
\end{align*}
where $f_{\theta^{l,k},i}$ and $\phi_{\theta^{l+1,k},i}$ are the components for node $v_i$ of $f_{\theta^{l,k}}$ and $\phi_{\theta^{l+1,k}}$, respectively.

    


We first focus on convex combination coefficients $\beta_i$, $i\in[n]$. For the simplicity of analysis, we assume $\beta_i=\beta$ for $i\in[i]$. The analysis of the case where $(\beta_i)_{i=1}^n$ are different from each other is the same.

\begin{lemma}\label{prop:tem_epsilon_h}
    Suppose that Assumption \ref{assmp:proof} holds. For any $\varepsilon>0$, by letting
    \begin{align*}
        \beta \leq \frac{\varepsilon}{2G},\,\,\forall\,l\in[L], \,i\in[n],
    \end{align*}
    we have
    \begin{align*}
        \|\temH^{l,k} - \embH^{l,k}\|_F \leq \|\hisH^{l,k} - \embH^{l,k}\|_F + \varepsilon,\,\,\forall\,l\in[L],\,k\in\mathbb{N}^*.
    \end{align*}
\end{lemma}
\begin{proof}
    Since $\temH^{l,k} = (1-\beta)\hisH^{l,k} + \beta \widetilde{\embH}^{l,k}$, we have
    \begin{align*}
        &\|\temH^{l,k} - \embH^{l,k}\|_F\\
        ={}& \|(1-\beta)\hisH^{l,k} + \beta \widetilde{\embH}^{l,k} - (1-\beta)\embH^{l,k}+\beta \embH^{l,k}\|_F\\
        \leq{}& (1-\beta) \|\hisH^{l,k} - \embH^{l,k}\|_F + \beta \|\widetilde{\embH}^{l,k}-\embH^{l,k}\|_F\\
        \leq{}& \|\hisH^{l,k} - \embH^{l,k}\|_F + 2\beta G.
    \end{align*}
    Hence letting $\beta \leq \frac{\varepsilon}{2G}$ leads to
    \begin{align*}
        \|\temH^{l,k} - \embH^{l,k}\|_F \leq \|\hisH^{l,k} - \embH^{l,k}\|_F + \varepsilon.
    \end{align*}
\end{proof}

\begin{lemma}\label{prop:tem_epsilon_v}
    Suppose that Assumption \ref{assmp:proof} holds. For any $\varepsilon>0$, by letting
    \begin{align*}
        \beta \leq \frac{\varepsilon}{2G},\,\, \forall\,l\in[L], \,i\in[n],
    \end{align*}
    we have
    \begin{align*}
        \|\temV^{l,k} - \embV^{l,k}\|_F \leq \|\hisV^{l,k} - \embV^{l,k}\|_F + \varepsilon.
    \end{align*}
\end{lemma}
\begin{proof}
    Since $\temH^{l,k} = (1-\beta)\hisH^{l,k} + \beta \widetilde{\embH}^{l,k}$, we have
    \begin{align*}
        &\|\temH^{l,k} - \embH^{l,k}\|\\
        ={}& \|(1-\beta)\hisH^{l,k} + \beta \widetilde{\embH}^{l,k} - (1-\beta)\embH^{l,k}+\beta \embH^{l,k}\|_F\\
        \leq{}& (1-\beta) \|\hisH^{l,k} - \embH^{l,k}\|_F + \beta \|\widetilde{\embH}^{l,k}-\embH^{l,k}\|_F\\
        \leq{}& \|\hisH^{l,k} - \embH^{l,k}\|_F + 2\beta G.
    \end{align*}
    Hence letting $\beta \leq \frac{\varepsilon}{2G}$ leads to
    \begin{align*}
        \|\temH^{l,k} - \embH^{l,k}\|_F \leq \|\hisH^{l,k} - \embH^{l,k}\|_F + \varepsilon.
    \end{align*}
\end{proof}
Next, we focus on the approximation errors of historical node embeddings and auxiliary variables
\begin{align*}
    &d_{h}^{l,k} := \left( \mathbb{E}[\|\hisH^{l,k} - \embH^{l,k}\|_F^2] \right)^{\frac{1}{2}},\,\,l\in[L],\\
    &d_{v}^{l,k} := \left( \mathbb{E}[\|\hisV^{l,k} - \embV^{l,k}\|_F^2] \right)^{\frac{1}{2}},\,\,l\in[L-1].
\end{align*}

\begin{lemma}\label{prop:approx_h_new}
    For an $L$-layer ConvGNN, suppose that Assumption \ref{assmp:proof} holds. Besides, we suppose that
    \begin{enumerate}
        \item $(d_h^{l,1})^2$ is bounded by $G>1$, $\forall\, l\in[L]$,

        \item there exists $N\in\mathbb{N}^*$ such that for any $k\in\mathbb{N}^*$ and $l\in[L]$ we have
        \begin{align*}
            &\|\temH^{l,k} - \embH^{l,k}\|_F \leq \|\hisH^{l,k} - \embH^{l,k}\|_F + \frac{1}{N^{\frac{2}{3}}},\\
            &\|\embH^{l,k} - \embH^{l,k-1}\|_F \leq \frac{1}{N^{\frac{2}{3}}}, 
        \end{align*}
    \end{enumerate}
    then there exist constants $C'_{*,1}$, $C'_{*,2}$, and $C'_{*,3}$ that do not depend on $k, l, N$, and $\eta$, such that 
    \begin{align*}
     (d^{l,k+1}_h)^2 \leq C'_{*,1} \eta + C'_{*,2} \rho^k +  \frac{C'_{*,3}}{N^{\frac{2}{3}}},\,\,\forall\, l\in[L],\,k\in\mathbb{N}^*,
    \end{align*}
    where $\rho = \frac{n-S}{n}<1$, $n=|\mathcal{V}|$, and $S$ is number of sampled nodes at each iteration.
\end{lemma}

\begin{proof}
    We have
\begin{align*}
    &(d^{l+1,k+1}_h)^2\\
    ={}& \mathbb{E}[\|\hisH^{l+1,k+1} - \embH^{l+1,k+1}\|_F^2] \\
    ={}&\mathbb{E}[\sum_{i=1}^n \| \hish^{l+1,k+1}_i - \embh^{l+1,k+1}_i\|_F^2]\\
    ={}&\mathbb{E}[\sum_{v_i\in\mathcal{V}_{\mathcal{B}}^k} \|f_{\theta^{l+1,k+1},i}(\temH^{l,k+1}) - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F^2\\
    &\quad\quad+ \sum_{v_i\not\in\mathcal{V}_{\mathcal{B}}^k} \|\hish^{l+1,k}_i - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F^2]\\
    ={}& \mathbb{E}[\frac{S}{n}\sum_{i=1}^n \|f_{\theta^{l+1,k+1},i}(\temH^{l,k+1}) - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F^2\\
    &\quad\quad+ \frac{n-S}{n} \sum_{i=1}^n \|\hish^{l+1,k}_i - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F^2]\\
    \leq{}& \frac{S}{n} \sum_{i=1}^n \mathbb{E}[\|f_{\theta^{l+1,k+1},i}(\temH^{l,k+1}) - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F^2]\\
    &+ \frac{n-S}{n} \sum_{i=1}^n \mathbb{E}[\|\hish^{l+1,k}_i - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F^2].\\
\end{align*}
About the first term, for $l\geq 1$, we have
\begin{align*}
    &\mathbb{E}[\|f_{\theta^{l+1,k+1},i}(\temH^{l,k+1}) - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F^2]\\
    \leq{}& \gamma^2 \mathbb{E}[\|\temH^{l,k+1} - \embH^{l,k+1}\|_F^2]\\
    \leq{}& \gamma^2 \mathbb{E}[(\|\hisH^{l,k+1} - \embH^{l,k+1}\|_F+\frac{1}{N^{\frac{2}{3}}})^2]\\
    \leq{}& 2\gamma^2 \mathbb{E}[\|\hisH^{l,k+1} - \embH^{l,k+1}\|_F^2] +  \frac{2\gamma^2}{N^{\frac{4}{3}}}\\
    ={}& 2\gamma^2 (d^{l,k+1}_h)^2+\frac{2\gamma^2}{N^{\frac{4}{3}}}.
\end{align*}


For $l=0$, we have
\begin{align*}
     &\mathbb{E}[\|f_{\theta^{l+1,k+1},i}(\temH^{l,k+1}) - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F^2]\\
     ={}& \mathbb{E}[\|f_{\theta^{1,k+1},i}(\temH^{0,k+1}) - f_{\theta^{1,k+1},i} (\embH^{0,k+1})\|_F^2]\\
     ={}& \mathbb{E}[\|f_{\theta^{1,k+1},i}(\embX) - f_{\theta^{1,k+1},i} (\embX)\|_F^2]\\
     ={}& 0.
\end{align*}
About the second term, for $l\geq 1$, we have
\begin{align*}
    &\mathbb{E}[\|\hish^{l+1,k}_i - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F^2]\\
    \leq{}& \mathbb{E}[\|\hish_i^{l+1,k} - \embh_i^{l+1,k} + \embh_i^{l+1,k} - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F^2]\\
    \leq{}& \mathbb{E}[\|\hish_i^{l+1,k} - \embh_i^{l+1,k}\|_F^2]\\
    &+ \mathbb{E}[\|\embh_i^{l+1,k} - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F^2]\\
    &+ 2\mathbb{E}[\langle  \hish_i^{l+1,k} - \embh_i^{l+1,k}, \embh_i^{l+1,k} - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\rangle]\\
    \leq{}& \mathbb{E}[\|\hish_i^{l+1,k} - \embh_i^{l+1,k}\|_F^2]\\
    &+\mathbb{E}[\|\embh^{l+1,k}_i - f_{\theta^{l+1,k+1},i} (\embH^{l,k})\\
    &\quad\quad+ f_{\theta^{l+1,k+1},i} (\embH^{l,k}) - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F^2]\\
    &+2\mathbb{E}[\langle  \hish_i^{l+1,k} - \embh_i^{l+1,k}, \embh_i^{l+1,k} - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\rangle]\\
    \leq{}& \mathbb{E}[\|\hish_i^{l+1,k} - \embh_i^{l+1,k}\|_F^2]\\
    &+ 2\mathbb{E}[\|\embh^{l+1,k}_i - f_{\theta^{l+1,k+1},i} (\embH^{l,k})\|_F^2]\\
    &+ 2\mathbb{E}[\|f_{\theta^{l+1,k+1},i} (\embH^{l,k}) - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F^2]\\
    &+ 4G\mathbb{E}[\|\embh_i^{l+1,k} - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F]\\
    \leq{}& \mathbb{E}[\|\hish_i^{l+1,k} - \embh_i^{l+1,k}\|_F^2]\\
    &+2\gamma^2\mathbb{E}[\|\theta^{l+1,k} - \theta^{l+1,k+1}\|^2]\\
    &+ 2\gamma^2\mathbb{E}[\|\embH^{l,k} - \embH^{l,k+1}\|_F^2]\\
    &+ 4G\gamma\mathbb{E}[\|\theta^{l+1,k} - \theta^{l+1,k+1}\|+\|\embH^{l,k} - \embH^{l,k+1}\|_F]\\
    \leq{}& \mathbb{E}[\|\hish_i^{l+1,k} - \embh_i^{l+1,k}\|_F^2]+ 2\gamma^2G^2\eta^2\\
    &+ 4G^2\gamma\eta + \frac{2\gamma^2}{N^{\frac{4}{3}}}+\frac{4G\gamma}{N^{\frac{2}{3}}}\\
    \leq{}& \mathbb{E}[\|\hish_i^{l+1,k} - \embh_i^{l+1,k}\|_F^2]+ 2G^2\gamma(\gamma+2)\eta\\
    &+ \frac{2\gamma(\gamma+2G)}{N^{\frac{2}{3}}}.
\end{align*}
For $l=0$, we have
\begin{align*}
    &\mathbb{E}[\|\hish^{l+1,k}_i - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F^2]\\
    \leq{}& \mathbb{E}[\|\hish_i^{l+1,k} - \embh_i^{l+1,k} + \embh_i^{l+1,k} - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F^2]\\
    \leq{}& \mathbb{E}[\|\hish_i^{l+1,k} - \embh_i^{l+1,k}\|_F^2]\\
    &+ \mathbb{E}[\|\embh_i^{l+1,k} - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F^2]\\
    &+ 2\mathbb{E}[\langle  \hish_i^{l+1,k} - \embh_i^{l+1,k}, \embh_i^{l+1,k} - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\rangle]\\
    \leq{}& \mathbb{E}[\|\hish_i^{l+1,k} - \embh_i^{l+1,k}\|_F^2]\\
    &+\mathbb{E}[\|\embh^{l+1,k}_i - f_{\theta^{l+1,k+1},i} (\embH^{l,k})\\
    &\quad\quad+ f_{\theta^{l+1,k+1},i} (\embH^{l,k}) - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F^2]\\
    &+2\mathbb{E}[\langle  \hish_i^{l+1,k} - \embh_i^{l+1,k}, \embh_i^{l+1,k} - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\rangle]\\
    \leq{}& \mathbb{E}[\|\hish_i^{l+1,k} - \embh_i^{l+1,k}\|_F^2]\\
    &+ 2\mathbb{E}[\|\embh^{l+1,k}_i - f_{\theta^{l+1,k+1},i} (\embH^{l,k})\|_F^2]\\
    &+ 2\mathbb{E}[\|f_{\theta^{l+1,k+1},i} (\embH^{l,k}) - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F^2]\\
    &+ 4G\mathbb{E}[\|\embh_i^{l+1,k} - f_{\theta^{l+1,k+1},i} (\embH^{l,k+1})\|_F]\\
    \leq{}& \mathbb{E}[\|\hish_i^{l+1,k} - \embh_i^{l+1,k}\|_F^2]+2\gamma^2\mathbb{E}[\|\theta^{l+1,k} - \theta^{l+1,k+1}\|^2]\\
    &+ 4G\gamma\mathbb{E}[\|\theta^{l+1,k} - \theta^{l+1,k+1}\|]\\
    \leq{}& \mathbb{E}[\|\hish_i^{l+1,k} - \embh_i^{l+1,k}\|_F^2]+ 2\gamma^2G^2\eta^2 + 4G^2\gamma\eta,\\
    \leq{}& \mathbb{E}[\|\hish_i^{l+1,k} - \embh_i^{l+1,k}\|_F^2]+ 2G^2\gamma(\gamma+2)\eta + 4G^2\gamma\eta.
\end{align*}
Hence we have
\begin{align*}
    &(d^{l+1,k+1}_h)^2\\
    \leq{}& \frac{(n-S)}{n} (d^{l+1,k}_h)^2+2(n-S)\gamma(\gamma+2)G^2\eta\\
    &+\begin{cases}
        0, &l=0,\\
        2\gamma^2 S (d^{l,k+1}_h)^2 + \frac{4n\gamma(\gamma+G)}{N^{\frac{2}{3}}}, &l\geq 1.
    \end{cases}
\end{align*}
Let $\rho = \frac{n-S}{n}<1$. For $l=0$, we have
\begin{align*}
    &(d^{1,k+1}_h)^2 - \frac{2(n-S)\gamma(\gamma+2)G^2\eta}{1-\rho}\\ 
    \leq{}& \rho((d^{1,k}_h)^2 - \frac{2(n-S)\gamma(\gamma+2)G^2\eta}{1-\rho})\\
    \leq{}& \rho^2 ((d^{1,k-1}_h)^2 - \frac{2(n-S)\gamma(\gamma+2)G^2\eta}{1-\rho})\\
    \leq{}&\cdots\\
    \leq{}& \rho^k ((d^{1,1}_h)^2 - \frac{2(n-S)\gamma(\gamma+2)G^2\eta}{1-\rho})\\
    \leq{}& \rho^k G,
\end{align*}
which leads to
\begin{align*}
    (d^{1,k+1}_h)^2 &\leq \frac{2(n-S)\gamma(\gamma+2)G^2}{1-\rho}\eta + \rho^k G\\
    &= C_{1,1}' \eta + \rho^k G.
\end{align*}
Then, for $l=1$ we have
\begin{align*}
    (d^{2,k+1}_h)^2 \leq \rho (d_h^{2,k})^2 + C_{2,1}\eta + C_{2,2}\rho^k + \frac{C_{2,3}}{N^{\frac{2}{3}}},
\end{align*}
where $C_{2,1}, C_{2,2}$, and $C_{2,3}$ are all constants. Hence we have
\begin{align*}
    &(d_h^{2,k+1})^2 - \frac{C_{2,1}\eta + C_{2,2}\rho^k + \frac{C_{2,3}}{N^{\frac{2}{3}}}}{1-\rho}\\
    \leq{}& \rho((d_h^{2,k})^2 - \frac{C_{2,1}\eta + C_{2,2}\rho^k + \frac{C_{2,3}}{N^{\frac{2}{3}}}}{1-\rho})\\
    \leq{}& \cdots\\
    \leq{}& \rho^k ((d_h^{2,1})^2 - \frac{C_{2,1}\eta + C_{2,2}\rho^k + \frac{C_{2,3}}{N^{\frac{2}{3}}}}{1-\rho})\\
    \leq{}& \rho^k G,
\end{align*}
which leads to
\begin{align*}
    (d_h^{2,k+1})^2 \leq C_{2,1}' \eta + C_{2,2}' \rho^k +\frac{C_{2,3}'}{N^{\frac{2}{3}}}.
\end{align*}
And so on, there exist constants $C'_{*,1}$, $C'_{*,2}$, and $C'_{*,3}$ that are independent with $\eta, k, l, N$ such that
\begin{align*}
    (d_h^{l,k+1})^2 \leq C'_{*,1} \eta + C'_{*,2} \rho^k + \frac{C'_{*,3}}{N^{\frac{2}{3}}},\,\,\forall\, l\in[L],\,k\in\mathbb{N}^*.
\end{align*}
\end{proof}

\begin{lemma}\label{prop:approx_v_new}
    For an $L$-layer ConvGNN, suppose that Assumption \ref{assmp:proof} holds. Besides, we suppose that
    \begin{enumerate}
        \item $(d_h^{l,1})^2$ is bounded by $G>1$, $\forall\, l\in[L]$,

        \item there exists $N\in\mathbb{N}^*$ such that for any $k\in\mathbb{N}^*$ and $l\in[L]$ we have
        \begin{align*}
            &\|\temV^{l,k} - \embV^{l,k}\|_F \leq \|\hisV^{l,k} - \embV^{l,k}\|_F + \frac{1}{N^{\frac{2}{3}}},\\
            &\|\embV^{l,k} - \embV^{l,k-1}\|_F \leq \frac{1}{N^{\frac{2}{3}}}, 
        \end{align*}
    \end{enumerate}
    then there exist constants $C'_{*,1}$, $C'_{*,2}$, and $C'_{*,3}$ that are independent with $k, l, \varepsilon^*$, and $\eta$, such that
    \begin{align*}
     (d^{l,k+1}_v)^2 \leq C'_{*,1} \eta + C'_{*,2} \rho^k + \frac{C'_{*,3}}{N^{\frac{2}{3}}},\,\,\forall\, l\in[L],\,k\in\mathbb{N}^*,
    \end{align*}
    where $\rho = \frac{n-S}{n}<1$, $n=|\mathcal{V}|$, and $S$ is number of sampled nodes at each iteration.
\end{lemma}

\begin{proof}
    Similar to the proof of Lemma \ref{prop:approx_h_new}.
\end{proof}



\subsubsection{Proof of Theorem \ref{thm:grad_error_conv}: approximation errors of mini-batch gradients}

In this subsection, we focus on the mini-batch gradients computed by LMC, i.e.,
\begin{align*}
    \widetilde{\embg}_w(w^k)=\frac{1}{|\mathcal{V}_{L}^k|} \sum_{v_j\in\mathcal{V}_{L}^k} \nabla_w \ell_{w^k}(\hish^{k}_j, y_j)
\end{align*}
and
\begin{align*}
    \widetilde{\embg}_{\theta^{l}}(\theta^{l,k}) = \frac{|\mathcal{V}|}{|\mathcal{V}_{\mathcal{B}}^k|}\sum_{v_j\in\mathcal{V}_{\mathcal{B}}^k} (\nabla_{\theta^{l}} u_{\theta^{l,k}}(\hish_j^{l-1,k}, \overline{\mathbf{m}}_{\neighbor{v_j}}^{l-1,k}, \embx_j))\hisV^{l,k}_j,
\end{align*}
where $\mathcal{V}_{\mathcal{B}}^k$ is the sampled mini-batch and $\mathcal{V}_{L_\mathcal{B}}^k$ is the corresponding labeled node set at the $k$-th iteration. We denote the mini-batch gradients computed by backward SGD by
\begin{align*}
    \embg_w(w^k)=\frac{1}{|\mathcal{V}_{L}^k|} \sum_{v_j\in\mathcal{V}_{L}^k} \nabla_w \ell_{w^k}(\embh^{k}_j, y_j)
\end{align*}
and
\begin{align*}
    \embg_{\theta^{l}}(\theta^{l,k}) = \frac{|\mathcal{V}|}{|\mathcal{V}_{\mathcal{B}}^k|}\sum_{v_j\in\mathcal{V}_{\mathcal{B}}^k} \left(\nabla_{\theta^{l}} u_{\theta^{l,k}}(\embh_j^{l-1,k}, \embm_{\neighbor{v_j}}^{l-1,k}, \embx_j)\right)\embV^{l,k}_j.
\end{align*}
The approximation errors of gradients are denoted by
\begin{align*}
    \Delta_{w}^{k} \triangleq \widetilde{\embg}_w(w^k) - \nabla_w \loss(w^k)
\end{align*}
and
\begin{align*}
    \Delta_{\theta^{l}}^{k} \triangleq \widetilde{\embg}_{\theta^{l}}(\theta^{l,k}) - \nabla_{\theta^{l}} \loss(\theta^{l,k}).
\end{align*}

\begin{lemma}\label{lemma:grad_error_bound_w_conv}
    Suppose that Assumption \ref{assmp:proof} holds. For any $k\in\mathbb{N}^*$, the difference between $\widetilde{\mathbf{g}}_w(w^k)$ and $\mathbf{g}_w(w^k)$ can be bounded as
    \begin{align*}
        \|\widetilde{\mathbf{g}}_w(w^k) - \mathbf{g}_w(w^k)\|_2 \leq \gamma \| \hisH^{L,k} - \embH^{L,k} \|_F.
    \end{align*}
\end{lemma}
\begin{proof}
    We have
    \begin{align*}
        \|\widetilde{\mathbf{g}}_w(w^k) - \mathbf{g}_w(w^k)\|_2 ={}& \frac{1}{|\mathcal{V}_L^k|}\|\sum_{v_j \in \mathcal{V}_{L}^k} \nabla_{w}\ell_{w^k}(\hish^{L,k}_j,y_j)\nonumber\\
        &\quad\quad\quad\quad\quad- \nabla_{w}\ell_{w^k}(\embh^{L,k}_j,y_j)\|_2\nonumber\\
        \leq{}& \frac{1}{|\mathcal{V}_L^k|}\sum_{v_j \in \mathcal{V}_{L}^k} \|\nabla_{w}\ell_{w^k}(\hish^{L,k}_j,y_j)\nonumber\\
        &\quad\quad\quad\quad\quad- \nabla_{w}\ell_{w^k}(\embh^{L,k}_j,y_j)\|_2\nonumber\\
        \leq{}& \frac{\gamma}{|\mathcal{V}_L^k|} \sum_{v_j \in \mathcal{V}_{L}^k}\|\hish^{L,k}_j-\embh^{L,k}_j\|_2\nonumber\\
        \leq{}& \frac{\gamma}{|\mathcal{V}_L^k|} \sum_{v_j \in \mathcal{V}_{L}^k}\|\hisH^{L,k}-\embH^{L,k}\|_F\nonumber\\
        ={}& \frac{\gamma}{|\mathcal{V}_L^k|} \cdot |\mathcal{V}_L^k|\cdot\|\hisH^{L,k}-\embH^{L,k}\|_F\nonumber\\
        ={}& \gamma \| \hisH^{L,k} - \embH^{L,k} \|_F\nonumber
    \end{align*}
\end{proof}

\begin{lemma}\label{lemma:grad_error_bound_theta_conv}
    Suppose that Assumption \ref{assmp:proof} holds. For any $k\in\mathbb{N}^*$ and $l\in[L]$, the difference between $\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})$ and $\mathbf{g}_{\theta^l}(\theta^{l,k})$ can be bounded as
    \begin{align*}
        &\|\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k}) - \mathbf{g}_{\theta^l}(\theta^{l,k})\|_2\\
        \leq{}& |\mathcal{V}| G \|\hisV^{l,k}-\embV^{l,k}\|_F + |\mathcal{V}|G\gamma \|\hisH^{l,k}-\embH^{l,k}\|_F.
    \end{align*}
\end{lemma}
\begin{proof}
    As $\|\mathbf{A}\mathbf{a}-\mathbf{B}\mathbf{b}\|_2\leq \|\mathbf{A}\|_F\|\mathbf{a}-\mathbf{b}\|_2 + \|\mathbf{A}-\mathbf{B}\|_F\|\mathbf{b}\|_2$, we can bound $\|\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k}) - \mathbf{g}_{\theta^l}(\theta^{l,k})\|_2$ by
    \begin{align*}
        &\|\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k}) - \mathbf{g}_{\theta^l}(\theta^{l,k})\|_2\\
        \leq{}& \frac{|\mathcal{V}|}{|\mathcal{V}_{\mathcal{B}}^k|}\sum_{v_i\in\mathcal{V}_{\mathcal{B}}^k}\|\left(\nabla_{\theta^{l}} u_{\theta^{l,k}}(\hish_j^{l-1,k}, \overline{\mathbf{m}}_{\neighbor{v_j}}^{l-1,k}, \embx_j)\right)\hisV_j^{l,k}\\
        &\quad\quad\quad\quad\quad-\left(\nabla_{\theta^{l}} u_{\theta^{l,k}}(\embh_j^{l-1,k}, \mathbf{m}_{\neighbor{v_j}}^{l-1,k}, \embx_j)\right)\embV_j^{l,k}\|_2\\
        \leq{}& |\mathcal{V}|\max_{v_i\in\mathcal{V}_{\mathcal{B}}^k} \|\left(\nabla_{\theta^{l}} u_{\theta^{l,k}}(\hish_j^{l-1,k}, \overline{\mathbf{m}}_{\neighbor{v_j}}^{l-1,k}, \embx_j)\right)\hisV_j^{l,k}\\
        &\quad\quad\quad\quad\quad-\left(\nabla_{\theta^{l}} u_{\theta^{l,k}}(\embh_j^{l-1,k}, \mathbf{m}_{\neighbor{v_j}}^{l-1,k}, \embx_j)\right)\embV_j^{l,k}\|_2\\
        \leq{}& |\mathcal{V}|\max_{v_i\in\mathcal{V}_{\mathcal{B}}^k} \|\nabla_{\theta^{l}} u_{\theta^{l,k}}(\hish_j^{l-1,k}, \overline{\mathbf{m}}_{\neighbor{v_j}}^{l-1,k}, \embx_j)\|_F\|\hisV_j^{l,k}-\embV_j^{l,k}\|_2\\
        &\quad\quad\quad\quad+ \|\nabla_{\theta^{l}} u_{\theta^{l,k}}(\hish_j^{l-1,k}, \overline{\mathbf{m}}_{\neighbor{v_j}}^{l-1,k}, \embx_j)\\
        &\quad\quad\quad\quad\quad\quad-\nabla_{\theta^{l}} u_{\theta^{l,k}}(\embh_j^{l-1,k}, \mathbf{m}_{\neighbor{v_j}}^{l-1,k}, \embx_j)\|_F\|\embV_j^{l,k}\|_2\\
        \leq{}& |\mathcal{V}| G \|\hisV^{l,k}-\embV^{l,k}\|_F + |\mathcal{V}|G\gamma \|\hisH^{l,k}-\embH^{l,k}\|_F.
    \end{align*}
\end{proof}

\begin{lemma}\label{prop:grad_error}
    For an $L$-layer ConvGNN, suppose that Assumption \ref{assmp:proof} holds. For any $N\in\mathbb{N}^*$, by letting
    \begin{align*}
        \eta \leq \frac{1}{(2\gamma)^L G}\frac{1}{N^{\frac{2}{3}}} = \mathcal{O}(\frac{1}{N^{\frac{2}{3}}})
    \end{align*}
    and
    \begin{align*}
        \beta_i \leq \frac{1}{2G}\frac{1}{N^{\frac{2}{3}}}=\mathcal{O}(\frac{1}{N^{\frac{2}{3}}}),\,\, i\in[n],
    \end{align*}
    there exists $G_{2,*}>0$ and $\rho\in(0,1)$ such that for any $k\in\mathbb{N}^*$ we have
    \begin{align*}
        &\mathbb{E}[\|\Delta^k_w\|_2^2] = ({\rm Bias}(\widetilde{\mathbf{g}}_w(w^k)))^2+ {\rm Var}(\widetilde{\mathbf{g}}_w(w^k)),\\
        &\mathbb{E}[\|\Delta^k_{\theta^l}\|_2^2] = ({\rm Bias}(\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})))^2+ {\rm Var}(\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})),
    \end{align*}
    where
    \begin{align*}
        &{\rm Var}(\widetilde{\mathbf{g}}_w(w^k)) = \mathbb{E}[\|\mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)] - \widetilde{\mathbf{g}}_w(w^k)\|_2^2],\\
        &{\rm Bias}(\widetilde{\mathbf{g}}_w(w^k)) = \|\mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)] - \nabla_w\mathcal{L}(w^k)\|_2,\\
        &{\rm Var}(\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})) = \mathbb{E}[\|\mathbb{E}[\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})] - \widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})\|_2^2],\\
        &{\rm Bias}(\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})) = \|\mathbb{E}[\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})] - \nabla_{\theta^l}\mathcal{L}(\theta^{l,k})\|_2,
    \end{align*}
    and
    \begin{align*}
        &{\rm Bias}(\widetilde{\mathbf{g}}_w(w^k))\leq G_{2,*}(\eta^{\frac{1}{2}} + \rho^{\frac{k-1}{2}}+\frac{1}{N^{\frac{1}{3}}}),\\
        &{\rm Bias}(\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k}))\leq G_{2,*}(\eta^{\frac{1}{2}} + \rho^{\frac{k-1}{2}}+\frac{1}{N^{\frac{1}{3}}}).
    \end{align*}
\end{lemma}
\begin{proof}
    By Lemmas \ref{prop:exact_difference_h} and \ref{prop:exact_difference_v} we know that
    \begin{align*}
        &\|\embH^{l,k+1}-\embH^{l,k}\|_F < \frac{1}{N^{\frac{2}{3}}},\,\,\forall\,l\in[L],\,k\in\mathbb{N}^*,\\
        &\|\embV^{l,k+1}-\embV^{l,k}\|_F < \frac{1}{N^{\frac{2}{3}}},\,\,\forall\,l\in[L],\,k\in\mathbb{N}^*.
    \end{align*}
    By Lemmas \ref{prop:tem_epsilon_h} and \ref{prop:tem_epsilon_v} we know that for any $k\in\mathbb{N}^*$ and $l\in[L]$ we have
    \begin{align*}
        &\|\temH^{l,k} - \embH^{l,k}\|_F \leq \|\hisH^{l,k} - \embH^{l,k}\|_F + \frac{1}{N^{\frac{2}{3}}}
    \end{align*}
    and
    \begin{align*}
        &\|\temV^{l,k} - \embV^{l,k}\|_F \leq \|\hisV^{l,k} - \embV^{l,k}\|_F + \frac{1}{N^{\frac{2}{3}}}.
    \end{align*}
    Thus, by Lemmas \ref{prop:approx_h_new} and \ref{prop:approx_v_new} we know that there exist $C_{*,1}'$, $C_{*,2}'$, and $C_{*,3}'$ that do not depend on $k,l,\eta,N$ such that for $\forall\,l\in[L]$ and $k\in\mathbb{N}^*$ hold
    \begin{align*}
        d_{h}^{l,k} \leq{}& \sqrt{C_{*,1}'\eta + C_{*,2}'\rho^{k-1} + \frac{C_{*,3}'}{N^{\frac{2}{3}}}}\\
        \leq{} &\sqrt{C_{*,1}'} \eta^{\frac{1}{2}} + \sqrt{C_{*,2}'} \rho^{\frac{k-1}{2}} + \sqrt{C_{*,3}'} \frac{1}{N^{\frac{1}{3}}}
    \end{align*}
    and
    \begin{align*}
        d_{v}^{l,k} \leq{} &\sqrt{C_{*,1}'\eta + C_{*,2}'\rho^{k-1} + \frac{C_{*,3}'}{N^{\frac{2}{3}}}}\\
        \leq{} &\sqrt{C_{*,1}'} \eta^{\frac{1}{2}} + \sqrt{C_{*,2}'} \rho^{\frac{k-1}{2}} + \sqrt{C_{*,3}'} \frac{1}{N^{\frac{1}{3}}}.
    \end{align*}
    We can decompose $\|\Delta^k_w\|_2^2$ as
    \begin{align*}
        &\|\Delta^k_w\|_2^2\\
        ={}& \|\widetilde{\mathbf{g}}_w(w^k) - \nabla_w \loss(w^k)\|_2^2\\
        ={}& \|\widetilde{\mathbf{g}}_w(w^k) - \mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)] + \mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)] - \nabla_w \loss(w^k)\|_2^2\\
        ={}& \|\widetilde{\mathbf{g}}_w(w^k) - \mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)] \|_2^2 + \|\mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)]  - \nabla_w\loss(w^k)\|_2^2\\
        &+ 2\langle \|\widetilde{\mathbf{g}}_w(w^k) - \mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)] , \mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)]  - \nabla_w \loss(w^k) \rangle.
    \end{align*}
    We take expectation of both sides of the above expression, leading to
    \begin{align}
        \mathbb{E}[\|\Delta^k_w\|_2^2] = ({\rm Bias}(\widetilde{\mathbf{g}}_w(w^k)))^2+ {\rm Var}(\widetilde{\mathbf{g}}_w(w^k)), \label{eqn:bias-variance_w_conv}
    \end{align}
    where
    \begin{align*}
        &{\rm Var}(\widetilde{\mathbf{g}}_w(w^k)) = \mathbb{E}[\|\mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)] - \widetilde{\mathbf{g}}_w(w^k)\|_2^2],\\
        &{\rm Bias}(\widetilde{\mathbf{g}}_w(w^k)) = \|\mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)] - \nabla_w\mathcal{L}(w^k)\|_2
    \end{align*}
    as
    \begin{align*}
        \mathbb{E}[\langle \|\widetilde{\mathbf{g}}_w(w^k) - \mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)] , \mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)]  - \nabla_w \loss(w^k) \rangle] = 0.
    \end{align*}
    By Lemma \ref{lemma:grad_error_bound_w_conv}, we can bound the bias term as
    \begin{align}
        {\rm Bias}(\widetilde{\mathbf{g}}_w(w^k)) ={}& \|\mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)] - \nabla_w\mathcal{L}(w^k)\|_2\nonumber\\
        ={}& \|\mathbb{E}[\widetilde{\mathbf{g}}_w(w^k) - \mathbf{g}_w(w^k)]\|_2\nonumber\\
        \leq{}& \mathbb{E}[\|\widetilde{\mathbf{g}}_w(w^k) - \mathbf{g}_w(w^k)\|_2]\nonumber\\
        \leq{}& \gamma \mathbb{E}[\|\hisH^{L,k}-\embH^{L,k}\|_F]\nonumber\\
        \leq{}& \gamma \left(\mathbb{E}[\|\hisH^{L,k}-\embH^{L,k}\|_F^2]\right)^{\frac{1}{2}}\nonumber\\
        ={}& \gamma \cdot d_h^{L,k}\nonumber\\
        \leq{}& \gamma(\sqrt{C_{*,1}'} \eta^{\frac{1}{2}} + \sqrt{C_{*,2}'} \rho^{\frac{k-1}{2}} + \sqrt{C_{*,3}'} \frac{1}{N^{\frac{1}{3}}})\nonumber\\
        \leq{}& G_{2,1}(\eta^{\frac{1}{2}} + \rho^{\frac{k-1}{2}}+\frac{1}{N^{\frac{1}{3}}}),\label{eqn:bias_bound_w_conv}
    \end{align}
    where $G_{2,1} = \gamma \max\{ \sqrt{C_{*,1}'},\sqrt{C_{*,2}'},\sqrt{C_{*,3}'} \}$.
    
    Similar to Eq. \eqref{eqn:bias-variance_w_conv}, we can decompose $\mathbb{E}[\|\Delta^k_{\theta^l}\|_2^2]$ as
    \begin{align*}
        \mathbb{E}[\|\Delta^k_{\theta^l}\|_2^2] = ({\rm Bias}(\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})))^2+ {\rm Var}(\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})),
    \end{align*}
    where
    \begin{align*}
        &{\rm Var}(\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})) = \mathbb{E}[\|\mathbb{E}[\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})] - \widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})\|_2^2],\\
        &{\rm Bias}(\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})) = \|\mathbb{E}[\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})] - \nabla_{\theta^l}\mathcal{L}(\theta^{l,k})\|_2.
    \end{align*}
    By Lemma \ref{lemma:grad_error_bound_theta_conv}, we can bound the bias term as
    \begin{align}
        {\rm Bias}(\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k}))={}& \|\mathbb{E}[\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})] - \nabla_{\theta^l}\mathcal{L}(\theta^{l,k})\|_2 \nonumber\\
        ={}& \|\mathbb{E}[\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k}) - \mathbf{g}_{\theta^l}(\theta^{l,k})]\|_2 \nonumber \\
        \leq{}& \mathbb{E}[\|\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k}) - \mathbf{g}_{\theta^l}(\theta^{l,k})\|_2] \nonumber \\
        \leq{}& |\mathcal{V}|G\mathbb{E}[\|\hisV^{l,k}-\embV^{l,k}\|_F]\nonumber\\ 
        &+ |\mathcal{V}|G\gamma \mathbb{E}[\|\hisH^{l,k}-\embH^{l,k}\|_F]\nonumber\\
        \leq{}& |\mathcal{V}|G\left(\mathbb{E}[\|\hisV^{l,k}-\embV^{l,k}\|^2_F]\right)^{\frac{1}{2}}\nonumber\\ 
        &+ |\mathcal{V}|G\gamma \left(\mathbb{E}[\|\hisH^{l,k}-\embH^{l,k}\|^2_F]\right)^{\frac{1}{2}}\nonumber\\
        ={}&|\mathcal{V}|G d_v^{l,k} + |\mathcal{V}|G\gamma d_h^{l,k}\nonumber\\
        \leq{}& G_{2,2}(\eta^{\frac{1}{2}} + \rho^{\frac{k-1}{2}}+\frac{1}{N^{\frac{1}{3}}}),\label{eqn:bias_bound_theta_conv}
    \end{align}
    where $G_{2,2}=|\mathcal{V}|G(1+\gamma)\max\{\sqrt{C_{*,1}'},\sqrt{C_{*,2}'},\sqrt{C_{*,3}'}\}$. 
    
    Let $G_{2,*}=\max\{G_{2,1}, G_{2,2}\}$, then we have
    \begin{align*}
        &{\rm Bias}(\widetilde{\mathbf{g}}_{w}(w^{k})) \leq G_{2,*}(\eta^{\frac{1}{2}} + \rho^{\frac{k-1}{2}}+\frac{1}{N^{\frac{1}{3}}}),\\
        &{\rm Bias}(\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k})) \leq G_{2,*}(\eta^{\frac{1}{2}} + \rho^{\frac{k-1}{2}}+\frac{1}{N^{\frac{1}{3}}}).
    \end{align*}
\end{proof}


By letting $\varepsilon = \frac{1}{N^{\frac{1}{3}}}$ and $C=G_{2,*}$, Theorem \ref{thm:grad_error_conv} follows immediately.






\subsubsection{Proof of Theorem \ref{thm:convergence_conv}: convergence guarantees}

In this subsection, we give the convergence guarantees of LMC. We first give sufficient conditions for the convergence.
\begin{lemma}\label{prop:suff}
    Suppose that function $f:\mathbb{R}^{n} \to \mathbb{R}$ is continuously differentiable. Consider an optimization algorithm with any bounded initialization $\vecx^1$ and an update rule in the form of
    \begin{align*}
        \vecx^{k+1} = \vecx^{k} - \eta \vecd(\vecx^{k}),
    \end{align*}
    where $\eta>0$ is the learning rate and $\vecd(\vecx^{k})$ is the estimated gradient that can be seen as a stochastic vector depending on $\vecx^{k}$. Let the estimation error of the gradient be $\Delta^{k} = \vecd(\vecx^{k}) - \nabla f(\vecx^{k}) $. Suppose that
    \begin{enumerate}
        \item the optimal value $f^*  = \inf_{\vecx} f(\vecx)$ is bounded; \label{con:1_conv}
        
        \item the gradient of $f$ is $\gamma$-Lipschitz, i.e., \label{con:2_conv}
        \begin{align*}
            \|\nabla f(\vecy) - \nabla f(\vecx)\|_2 \leq \gamma\|\vecy - \vecx\|_2,\,\forall\,\vecx,\vecy \in \mathbb{R}^{n};
        \end{align*}
        
        \item there exists $G_0>0$ that does not depend on $\eta$ such that \label{cond:suff_3}
        \begin{align*}
            \mathbb{E}[\|\Delta^{k}\|_2^2] \leq G_0,\,\forall\, k\in\mathbb{N}^*;
        \end{align*}

        \item there exists $N\in\mathbb{N}^*$ and $\rho\in(0,1)$ that do not depend on $\eta$ such that for any $k\in\mathbb{N}^*$ we have
        \begin{align*}
            |\mathbb{E}[\langle \nabla f(\vecx^{k}),\Delta^{k} \rangle]| \leq G_0(\eta^{\frac{1}{2}} +\rho^{\frac{k-1}{2}}+\frac{1}{N^{\frac{1}{3}}}),
        \end{align*}
        where $G_0$ is the same constant as that in Condition \ref{cond:suff_3},
    \end{enumerate}
    then by letting $\eta=\min\{\frac{1}{\gamma},\frac{1}{N^{\frac{2}{3}}}\}$, we have
    \begin{align*}
        &\mathbb{E}[ \|\nabla f(\vecx^{R})\|_2^2]\\
        \leq{}&\frac{2(f(\vecx^{1})-f^*+G_0)}{N^{\frac{1}{3}}} +\frac{\gamma G_0}{N^{\frac{2}{3}}}+ \frac{G_0}{N(1-\sqrt{\rho})}\\
        ={}& \mathcal{O}(\frac{1}{N^{\frac{1}{3}}}),
    \end{align*}
     where $R$ is chosen uniformly from $[N]$.
\end{lemma}

\begin{proof}
    As the gradient of $f$ is $\gamma$-Lipschitz, we have
    \begin{align*}
    	f(\vecy)={}&f(\vecx)+\int_{\vecx}^{\vecy}\nabla f(\mathbf{z})\rmd\mathbf{z}\\
    	={}&f(\vecx)+\int_0^1\langle\nabla f(\vecx+t(\vecy-\vecx)), \vecy-\vecx\rangle \rmd t\\
    	={}&f(\vecx)+\langle\nabla f(\vecx),\vecy-\vecx\rangle\\
    	&+\int_0^1\langle\nabla f(\vecx+t(\vecy-\vecx))-\nabla f(\vecx), \vecy-\vecx\rangle \rmd t\\
    	\leq{}&f(\vecx)+\langle\nabla f(\vecx),\vecy-\vecx\rangle\\
    	&+\int_0^1\|\nabla f(\vecx+t(\vecy-\vecx))-\nabla f(\vecx)\|_2\| \vecy-\vecx\|_2 \rmd t\\
    	\leq{}&f(\vecx)+\langle\nabla f(\vecx),\vecy-\vecx\rangle+\int_0^1\gamma t\|\vecy-\vecx\|^2_2\rmd t\\
    	\leq{}&f(\vecx)+\langle\nabla f(\vecx), \vecy-\vecx\rangle+\frac{\gamma}{2}\|\vecy-\vecx\|_2^2,
    \end{align*}
    Then, we have
    \begin{align*}
        &f(\vecx^{k+1})\\ \leq{}&f(\vecx^{k}) + \langle  \nabla f(\vecx^{k}), \vecx^{k+1} - \vecx^{k} \rangle+\frac{\gamma}{2}\|\vecx^{k+1}-\vecx^{k}\|_2^2 \\
        ={}& f(\vecx^{k}) - \eta \langle \nabla f(\vecx^{k}), \vecd(\vecx^{k}) \rangle+ \frac{\eta^2 \gamma}{2}\|\vecd(\vecx^{k})\|_2^2 \\
        ={}& f(\vecx^{k}) - \eta \langle \nabla f(\vecx^{k}), \Delta^{k} \rangle- \eta \|\nabla f(\vecx^{k})\|_2^2\\
        &+  \frac{\eta^2 \gamma}{2}(\|\Delta^{k}\|_2^2+\|\nabla f(\vecx^{k})\|_2^2 +2\langle \Delta^{k}, \nabla f(\vecx^{k}) \rangle)\\
        ={}& f(\vecx^{k})  - \eta (1-\eta \gamma) \langle \nabla f(\vecx^{k}), \Delta^{k} \rangle\\
        &- \eta (1-\frac{\eta \gamma}{2}) \|\nabla f(\vecx^{k})\|_2^2 + \frac{\eta^2 \gamma}{2}\|\Delta^{k}\|_2^2.
    \end{align*}
    By taking expectation of both sides, we have
    \begin{align*}
        &\mathbb{E}[f(\vecx^{k+1})]\\
        \leq{}&\mathbb{E}[f(\vecx^{k})] - \eta (1-\eta \gamma)  \mathbb{E}[\langle \nabla f(\vecx^{k}), \Delta^{k} \rangle]\\
        &- \eta (1-\frac{\eta \gamma}{2}) \mathbb{E}[ \|\nabla f(\vecx^{k})\|_2^2]+ \frac{\eta^2 \gamma}{2}\mathbb{E}[\|\Delta^{k}\|_2^2].
    \end{align*}
    By summing up the above inequalities for $k\in[N]$ and dividing both sides by $N \eta(1-\frac{\eta \gamma}{2})$, we have
    \begin{align*}
        &\frac{\sum_{k=1}^{N} \mathbb{E}[ \|\nabla f(\vecx^{k})\|_2^2]}{N}\\
        \leq{}& \frac{f(\vecx^{1}) - \mathbb{E}[f(\vecx^{N})]}{N \eta(1-\frac{\eta \gamma}{2}) } + \frac{\eta \gamma}{2-\eta \gamma} \frac{\sum_{k=1}^N \mathbb{E}[\|\Delta^{k}\|_2^2]}{N}\\
        &- \frac{(1-\eta \gamma)}{(1-\frac{\eta \gamma}{2})} \frac{\sum_{k=1}^{N} \mathbb{E}[\langle \nabla f(\vecx^{k}), \Delta^{k} \rangle]}{N} \\
        \leq{}& \frac{f(\vecx^{1}) - f^* }{N \eta(1-\frac{\eta \gamma}{2}) } + \frac{\eta \gamma}{2-\eta \gamma} \frac{\sum_{k=1}^N \mathbb{E}[\|\Delta^{k}\|_2^2]}{N}\\
        &+ \frac{\sum_{k=1}^{N} |\mathbb{E}[\langle \nabla f(\vecx^{k}), \Delta^{k} \rangle]|}{N},
    \end{align*}
    where the second inequality comes from $\eta \gamma>0$ and $f(\vecx^{k}) \geq f^* $. According to the above conditions, we have
    \begin{align*}
        &\frac{\sum_{k=1}^{N} \mathbb{E}[ \|\nabla f(\vecx^{k})\|_2^2]}{N}\\
        \leq{}&  \frac{f(\vecx^{1}) - f^* }{N \eta(1-\frac{\eta \gamma}{2}) } + \frac{\eta \gamma}{2 - \eta \gamma}G_0 + G_0\sum_{k=1}^N \frac{\eta^{\frac{1}{2}}  + \rho^{\frac{k-1}{2}}}{N} + \frac{G_0}{N^{\frac{1}{3}}}\\
        \leq{}& \frac{f(\vecx^{1}) - f^* }{N \eta(1-\frac{\eta \gamma}{2}) } + \frac{\eta \gamma}{2 - \eta \gamma}G_0+\eta^{\frac{1}{2}} G_0 + \frac{G_0}{N}\sum_{k=1}^\infty \rho^{\frac{k-1}{2}}  + \frac{G_0}{N^{\frac{1}{3}}}\\
        ={}& \frac{f(\vecx^{1}) - f^* }{N \eta(1-\frac{\eta \gamma}{2}) } + \frac{\eta \gamma}{2 - \eta \gamma}G_0+\eta^{\frac{1}{2}} G_0 + \frac{G_0}{N(1-\sqrt{\rho})}+ \frac{G_0}{N^{\frac{1}{3}}}.
    \end{align*}
    Notice that
    \begin{align*}
        \mathbb{E}[ \|\nabla f(\vecx^{R})\|_2^2] ={}& \mathbb{E}_R[\mathbb{E}\|[\nabla f(\vecx^{R})\|_2^2\mid R]]\\
        ={}&\frac{\sum_{k=1}^{N} \mathbb{E}[ \|\nabla f(\vecx^{k})\|_2^2]}{ N},
    \end{align*}
    where $R$ is uniformly chosen from $[N]$, hence we have
    \begin{align*}
        \mathbb{E}[ \|\nabla f(\vecx^{R})\|_2^2] \leq{}& \frac{f(\vecx^{1}) - f^* }{N \eta(1-\frac{\eta \gamma}{2}) } + \frac{\eta \gamma}{2 - \eta \gamma}G_0\\
        &+\eta^{\frac{1}{2}} G_0 + \frac{G_0}{N(1-\sqrt{\rho})}+ \frac{G_0}{N^{\frac{1}{3}}}.
    \end{align*}
    By letting $\eta=\min\{\frac{1}{\gamma}, \frac{1}{N^{\frac{2}{3}}}\}$, we have
    \begin{align*}
        &\mathbb{E}[ \|\nabla f(\vecx^{R})\|_2^2]\\
        \leq{}& \frac{2(f(\vecx^{1})-f^*)}{N^{\frac{1}{3}}}+\frac{\gamma G_0}{N^{\frac{2}{3}}} + \frac{G_0}{N^{\frac{1}{3}}}+\frac{G_0}{N(1-\sqrt{\rho})}+\frac{G_0}{N^{\frac{1}{3}}}\\
        \leq{}& \frac{2(f(\vecx^{1})-f^*+G_0)}{N^{\frac{1}{3}}} +\frac{\gamma G_0}{N^{\frac{2}{3}}}+ \frac{G_0}{N(1-\sqrt{\rho})}\\
        ={}& \mathcal{O}(\frac{1}{N^{\frac{1}{3}}}).
    \end{align*}
\end{proof}

Given an $L$-layer ConvGNN, following \cite{vrgcn}, we directly assume that:
\begin{enumerate}
    \item the optimal value
    \begin{align*}
        \mathcal{L}^*=\inf\limits_{w,\theta^{1},\ldots,\theta^{L}} \mathcal{L}
    \end{align*}
    is bounded by $G>1$;

    \item the gradients of $\mathcal{L}$ with respect to parameters $w$ and $\theta^{l}$, i.e.,
    \begin{align*}
        \nabla_{w}\mathcal{L},\,\nabla_{\theta^{l}}\mathcal{L}
    \end{align*}
    are $\gamma$-Lipschitz for $\forall l\in[L]$.
\end{enumerate}

To show the convergence of LMC by Lemma \ref{prop:suff}, it suffices to show that
\begin{enumerate}[resume]
    \item there exists $G_1>0$ that does not depend on $\eta$ such that
    \begin{align*}
        &\mathbb{E}[\|\Delta_{w}^{k}\|_2^2]\leq G_1,\,\,\forall\,k\in\mathbb{N}^*,\\
        &\mathbb{E}[\|\Delta_{\theta^{l}}^{k}\|_2^2]\leq G_1,\,\,\forall\,l\in[L],\,k\in\mathbb{N}^*;
    \end{align*}

    \item for any $N\in\mathbb{N}^*$, there exist $G_2>0$ and $\rho\in(0,1)$ such that for any $k\in\mathbb{N}^*$ and $l\in[L]$ we have
    \begin{align*}
        &|\mathbb{E}[\langle \nabla_{w}\loss, \Delta_{w}^{k} \rangle]| \leq G_2(\eta^{\frac{1}{2}} + \rho^{\frac{k-1}{2}}+\frac{1}{N^{\frac{1}{3}}}),\\
        &|\mathbb{E}[\langle \nabla_{\theta^{l}}\loss, \Delta_{\theta^{l}}^{k} \rangle]| \leq G_2(\eta^{\frac{1}{2}} + \rho^{\frac{k-1}{2}}+\frac{1}{N^{\frac{1}{3}}})
    \end{align*}
    by letting
    \begin{align*}
        \eta \leq \frac{1}{(2\gamma)^L G}\frac{1}{N^{\frac{2}{3}}} = \mathcal{O}(\frac{1}{N^{\frac{2}{3}}})
    \end{align*}
    and
    \begin{align*}
        \beta_i \leq \frac{1}{2G}\frac{1}{N^{\frac{2}{3}}}=\mathcal{O}(\frac{1}{N^{\frac{2}{3}}}),\,\,i\in[n].
    \end{align*}
\end{enumerate}

\begin{lemma}\label{prop:cond3}
    Suppose that Assumption \ref{assmp:proof} holds, then
    \begin{align*}
        &\mathbb{E}[\|\Delta_{w}^{k}\|_2^2] \leq G_1 \triangleq 4G^2,\,\,\forall\,k\in\mathbb{N}^*,\\
        &\mathbb{E}[\|\Delta_{\theta^{l}}^{k}\|_2^2] \leq G_1 \triangleq 4G^2, \,\,\forall\,l\in[L],\,k\in\mathbb{N}^*.
    \end{align*}
\end{lemma}
\begin{proof}
    We have
    \begin{align*}
        \mathbb{E}[\|\Delta_{w}^{k}\|_2^2] ={}& \mathbb{E}[\|\widetilde{\mathbf{g}}_w(w^k) - \nabla_{w}\loss(w^k)\|_2^2]\\
        \leq{}& 2(\mathbb{E}[\|\widetilde{\mathbf{g}}_w(w^k)\|_2^2] + \mathbb{E}[\|\nabla_{w}\loss(w^k)\|_2^2])\\
        \leq{}& 4G^2
    \end{align*}
    and
    \begin{align*}
        \mathbb{E}[\|\Delta_{\theta^{l}}^{k}\|_2^2]={}& \mathbb{E}[\|\widetilde{\mathbf{g}}_{\theta^{l}}(\theta^{l,k}) - \nabla_{\theta^{l}}\loss(\theta^{l,k})\|_2^2]\\
        \leq{}& 2(\mathbb{E}[\|\widetilde{\mathbf{g}}_{\theta^{l}}(\theta^{l,k})\|_2^2] + \mathbb{E}[\|\nabla_{\theta^{l}}\loss(\theta^{l,k})\|_2^2])\\
        \leq{}& 4G^2.
    \end{align*}
\end{proof}



\begin{lemma}\label{prop:cond4} \label{prop:inner_product_w}
    Suppose that Assumption \ref{assmp:proof} holds. For any $N\in\mathbb{N}^*$, there exist $G_2>0$ and $\rho\in(0,1)$ such that
    \begin{align*}
        &|\mathbb{E}[\langle \nabla_{w}\loss, \Delta_{w}^{k} \rangle]| \leq G_2(\eta^{\frac{1}{2}} + \rho^{\frac{k-1}{2}}+\frac{1}{N^{\frac{1}{3}}}),\,\,\forall\,k\in\mathbb{N}^*,\\
        &|\mathbb{E}[\langle \nabla_{\theta^{l}}\loss, \Delta_{\theta^{l}}^{k} \rangle]| \leq G_2(\eta^{\frac{1}{2}} + \rho^{\frac{k-1}{2}}+\frac{1}{N^{\frac{1}{3}}}),\,\,\forall\,l\in[L],\,k\in\mathbb{N}^*
    \end{align*}
    by letting
    \begin{align*}
        \eta \leq \frac{1}{(2\gamma)^L G}\frac{1}{N^{\frac{2}{3}}} = \mathcal{O}(\frac{1}{N^{\frac{2}{3}}})
    \end{align*}
    and
    \begin{align*}
        \beta_i \leq \frac{1}{2G}\frac{1}{N^{\frac{2}{3}}}=\mathcal{O}(\frac{1}{N^{\frac{2}{3}}}),\,\,i\in[n].
    \end{align*}
\end{lemma}
\begin{proof}
    By Eqs. \eqref{eqn:bias_bound_w_conv} and \eqref{eqn:bias_bound_theta_conv} we know that there exists $G_{2,*}$ such that for any $k\in\mathbb{N}^*$ we have
    \begin{align*}
        &\mathbb{E}[\|\widetilde{\mathbf{g}}_w(w^k) - \mathbf{g}_w(w^k)\|_2]\leq G_{2,*}(\eta^{\frac{1}{2}} + \rho^{\frac{k-1}{2}}+\frac{1}{N^{\frac{1}{3}}})
    \end{align*}
    and
    \begin{align*}
        &\mathbb{E}[\|\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k}) - \mathbf{g}_{\theta^l}(\theta^{l,k})\|_2]\leq G_{2,*}(\eta^{\frac{1}{2}} + \rho^{\frac{k-1}{2}}+\frac{1}{N^{\frac{1}{3}}}),
    \end{align*}
    where $\rho=\frac{n-S}{n}<1$ is a constant. Hence
    \begin{align*}
        |\mathbb{E}[\langle \nabla_{w}\loss, \Delta_{w}^{k} \rangle]|={}& |\mathbb{E}[\langle \nabla_{w}\loss, \widetilde{\mathbf{g}}_w(w^k) - \nabla_w\loss(w^k) \rangle]|\\
        ={}& |\mathbb{E}[\langle \nabla_{w}\loss, \widetilde{\mathbf{g}}_w(w^k) - \mathbf{g}_w(w^k) \rangle]|\\
        \leq{}& \mathbb{E}[\|\nabla_{w}\loss\|_2 \|\widetilde{\mathbf{g}}_w(w^k) - \mathbf{g}_w(w^k) \|_2]\\
        \leq{}& G\mathbb{E}[\|\widetilde{\mathbf{g}}_w(w^k) - \mathbf{g}_w(w^k)\|_2],\\
        \leq{}&G_{2}(\eta^{\frac{1}{2}} + \rho^{\frac{k-1}{2}}+\frac{1}{N^{\frac{1}{3}}})
    \end{align*}
    and
    \begin{align*}
        |\mathbb{E}[\langle \nabla_{\theta^l}\loss, \Delta_{\theta^l}^{k} \rangle]| ={}& |\mathbb{E}[\langle \nabla_{\theta^l}\loss, \widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k}) - \nabla_{\theta^l} \loss(\theta^{l,k}) \rangle]|\\
        ={}& |\mathbb{E}[\langle \nabla_{\theta^l}\loss, \widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k}) - \mathbf{g}_{\theta^l}(\theta^{l,k}) \rangle]|\\
        \leq{}& \mathbb{E}[\|\nabla_{\theta^l}\loss\|_2 \|\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k}) - \mathbf{g}_{\theta^l}(\theta^{l,k}) \|_2]\\
        \leq{}& G\mathbb{E}[\|\widetilde{\mathbf{g}}_{\theta^l}(\theta^{l,k}) - \mathbf{g}_{\theta^l}(\theta^{l,k})\|_2]\\
        \leq{}&G_{2}(\eta^{\frac{1}{2}} + \rho^{\frac{k-1}{2}}+\frac{1}{N^{\frac{1}{3}}}),
    \end{align*}
    where $G_{2}=GG_{2,*}$. 
\end{proof}


According to Lemmas \ref{prop:cond3} and \ref{prop:cond4}, the conditions in Lemma \ref{prop:suff} hold. By letting 
\begin{align*}
    \varepsilon ={}& \left(\frac{2(f(\vecx^{1})-f^*+G_0)}{N^{\frac{1}{3}}} +\frac{\gamma G_0}{N^{\frac{2}{3}}}+ \frac{G_0}{N(1-\sqrt{\rho})}\right)^{\frac{1}{2}}\\
    ={}& \mathcal{O}(\frac{1}{N^{\frac{1}{6}}}),
\end{align*}
we know that Theorem \ref{thm:convergence_rec} follows immediately.







\subsection{Proofs for LMC4Rec}\label{appendix:proof_rec}

In this section, we suppose that Assumption \ref{assmp:proof_rec} holds.

\subsubsection{Approximation errors of historical node embeddings and auxiliary variables}

In this subsection, we show that the approximation errors of historical node embeddings and auxiliary variables
\begin{align*}
    &d^{\Diamond,k}_h :=\left(\mathbb{E}[\|\hisH^{\Diamond,k} - \embH^{\Diamond, k}\|_F^2]\right)^{\frac{1}{2}},\\
    &d^{\Diamond,k}_v :=\left(\mathbb{E}[\|\hisV^{\Diamond,k} - \embV^{\Diamond, k}\|_F^2]\right)^{\frac{1}{2}}
\end{align*}
converge to $0$ during the training of LMC4Rec by the following theorems.

\begin{theorem}\label{thm:convergence_h_rec}
    Suppose that Assumption \ref{assmp:proof_rec} holds. Besides, we suppose that $d_h^{\Diamond,1}$ is bounded by $G$, then we have
    \begin{align*}
        d_h^{\Diamond, k+1} \leq \rho^kG + \frac{KG}{1-\rho} \eta,
    \end{align*}
    where {$\rho = \sqrt{(1-(1-\gamma^2)/B)}<1$}, {$K = \frac{2\gamma}{1-\gamma}$}, and $B$ is the number of partition subgraphs.
\end{theorem}

\begin{theorem}\label{thm:convergence_v_rec}
    Suppose that Assumption \ref{assmp:proof_rec} holds. Besides, we suppose that $d_v^{\Diamond,1}$ is bounded by $G$, then we have
    \begin{align*}
        d_v^{\Diamond, k+1} \leq \rho^kG + \frac{KG}{1-\rho} \eta,
    \end{align*}
    where {$\rho = \sqrt{(1-(1-\gamma^2)/B)}<1$}, {$K = \frac{2\gamma}{1-\gamma}$}, and $B$ is the number of partition subgraphs.
\end{theorem}

To show Theorems \ref{thm:convergence_h_rec} and \ref{thm:convergence_v_rec}, we introduce some useful lemmas.

\begin{lemma}\label{lemma:cfpi}
    Let the transition function $f:\mathbb{R}^d \rightarrow \mathbb{R}^d$ be $\gamma$-contraction. Consider the block-coordinate fixed-point algorithm with the partition of coordinates to $n$ blocks $\mathbf{I} = (S_1,S_2,\dots,S_n) \in \mathbb{R}^{d \times d}$ and an update rule
    \begin{align*}
        \vecx^{k+1} = (\mathbf{I}- \alpha \vecM^{k}) \vecx^{k} + \alpha \vecM^{k} f(\vecx^{k}),
    \end{align*}
    where $ \alpha \in (0,1]$ and the stochastic matrix $\vecM^{k}$ chosen independently and uniformly from $\{(0,\dots,S_j,\dots,0)|j=1,\dots n\}$ indicates the updated coordinates at the $k$-th iteration. Then, we have
    \begin{align*}
        \mathbb{E}[\| \vecx^{k+1} - \vecx   \|_2^2] \leq \left( 1 - \frac{\alpha}{n}(1-\gamma^2) \right)\mathbb{E}[\| \vecx^{k} - \vecx   \|_2^2].
    \end{align*}
    Moreover,
    \begin{align*}
        \mathbb{E}[\| \vecx^{k+1} - \vecx   \|_2^2] \leq \left( 1 - \frac{\alpha}{n}(1-\gamma^2) \right)^{k}\| \vecx^{1} - \vecx   \|_2^2.
    \end{align*}
\end{lemma}

\begin{proof}
    As $\vecM^{k}$ is chosen uniformly from the set
    \begin{align*}
        \{(0,\dots,S_i,\dots,0)|i=1,\dots n\},
    \end{align*}
    we have
    \begin{align*}
        \mathbb{E}[\vecM^{k}] = \frac{\mathbf{I}}{n}.
    \end{align*}
    We first compute the conditional expectation
    \begin{align*}
        &\mathbb{E}[ \| \vecx^{k+1} - \vecx   \|_2^2 | \vecx^{k}]\\
        ={}& \mathbb{E}[ \| \vecx^{k+1} - \vecx^{k} + \vecx^{k} - \vecx   \|_2^2 | \vecx^{k}] \\
        ={}& \mathbb{E}[ \| \vecx^{k+1} - \vecx^{k} \|_2^2 | \vecx^{k}] + \| \vecx^{k} - \vecx   \|_2^2 \\
        &+ 2 \langle \mathbb{E}[\vecx^{k+1}| \vecx^{k}]  - \vecx^{k} ,  \vecx^{k} - \vecx   \rangle \\
        ={}& \| \vecx^{k} - \vecx   \|_2^2 + \alpha^2 \mathbb{E}[ \| \vecM^{k}(\vecx^{k} - f(\vecx^{k})) \|_2^2 | \vecx^{k}] \\
        &+ 2 \frac{\alpha}{n} \langle f(\vecx^{k})  - \vecx^{k} ,  \vecx^{k} - \vecx   \rangle.
    \end{align*}
    Notice that 
    \begin{align*}
        &\mathbb{E}[ \| \vecM^{k}(\vecx^{k} - f(\vecx^{k})) \|_2^2 | \vecx^{k}]\\
        &{}= \frac{1}{n} \sum_{j=1}^n \| (0,\dots,S_i,\dots,0)( \vecx^{k} - f(\vecx^{k})) \|_2^2\\
        &{}=\frac{1}{n} \| \vecx^{k} - f(\vecx^{k})\|_2^2
    \end{align*}
    and
    \begin{align*}
        &2 \langle f(\vecx^{k})  - \vecx^{k} ,  \vecx^{k} - \vecx   \rangle\\
        ={}&\|f(\vecx^{k}) - \vecx   \|_2^2 - \|f(\vecx^{k})  - \vecx^{k}\|_2^2 - \|\vecx^{k} - \vecx  \|_2^2.
    \end{align*}
    Combining the above equalities and the contraction property of $f$, we have
    \begin{align*}
        &\mathbb{E}[ \| \vecx^{k+1} - \vecx   \|_2^2 | \vecx^{k}]\\
        ={}& (1-\frac{\alpha}{n})\| \vecx^{k} - \vecx   \|_2^2 - \alpha \frac{1-\alpha}{n} \| \vecx^{k} - f(\vecx^{k})\|_2^2\\
        &+\frac{\alpha}{n} (\|f(\vecx^{k}) - \vecx   \|_2^2) \\
        \leq{}& (1-\frac{\alpha}{n})\| \vecx^{k} - \vecx   \|_2^2 - \alpha \frac{1-\alpha}{n} \| \vecx^{k} - f(\vecx^{k})\|_2^2\\
        &+\frac{\alpha \gamma^2}{n} (\|\vecx^{k} - \vecx   \|_2^2) \\
        \leq{} &(1-\frac{\alpha}{n}(1-\gamma^2)) \| \vecx^{k} - f(\vecx^{k})\|_2^2.
    \end{align*}
    By the law of total expectation, we have
    \begin{align*}
        \mathbb{E}[ \| \vecx^{k+1} - \vecx   \|_2^2] \leq (1-\frac{\alpha}{n}(1-\gamma^2) \mathbb{E}[ \| \vecx^{k} - \vecx   \|_2^2 ].
    \end{align*}
    Finally, we recursively deduce that
    \begin{align*}
        \mathbb{E}[\| \vecx^{k+1} - \vecx   \|_2^2] \leq \left( 1 - \frac{\alpha}{n}(1-\gamma^2) \right)^{k} \| \vecx^{1} - \vecx   \|_2^2.
    \end{align*}
\end{proof}


By Lemma \ref{lemma:cfpi}, we can deduce that $d^{\Diamond,k}_h$ and $d^{\Diamond,k}_v$ decrease if the parameters $\theta^{\Diamond},w$ change slow. Moreover, we can set the learning rate $\eta$ to be a such small value that the parameters $\thetarec,w$ change slowly.


\begin{lemma}\label{lemma:amp_cfpi}
    Suppose that Assumption \ref{assmp:proof_rec} holds. If
    \begin{align*}
        \|\theta^{\Diamond,k+1}-\theta^{\Diamond,k}\|_F \leq \varepsilon
    \end{align*}
    at the $k$-th iteration, then we have
    \begin{align*}
        d^{\Diamond,k+1}_{h} \leq \rho d^{\Diamond,k}_h + K \varepsilon,
    \end{align*}
    where $\rho = \sqrt{(1-\frac{1}{B}(1-\gamma^2))}$ and $K = \frac{\gamma}{1-\gamma}$.
\end{lemma}

\begin{proof}
    According to Lemma \ref{lemma:cfpi}, we have
    \begin{align*}
        &\mathbb{E}[\|\hisH^{\Diamond, k+1} - \embH^{\Diamond,k+1}\|_F^2]\\
        \leq{}& \rho^2\mathbb{E}[\|\hisH^{\Diamond, k} - \embH^{\Diamond,k+1}\|_F^2] \\
        \leq{}& \rho^2\mathbb{E}[(\|\hisH^{\Diamond, k} - \embH^{\Diamond,k}\|_F + \|\embH^{\Diamond,k} - \embH^{\Diamond,k+1}\|_F )^2].
    \end{align*}
    As the transition function $f_{\theta}$ is $\gamma$-Lipschitz, we have
    \begin{align*}
        &\|\embH^{\Diamond,k} - \embH^{\Diamond,k+1}\|_F\\
        ={}& \|f_{\theta^{\Diamond,k}}(\embH^{\Diamond,k}) - f_{\theta^{\Diamond,k+1}}\embH^{\Diamond,k+1}\|_F \\
        \leq{}& \|f_{\theta^{\Diamond,k}}(\embH^{\Diamond,k}) - f_{\theta^{\Diamond,k}}\embH^{\Diamond,k+1}\|_F\\
        &+ \|f_{\theta^{\Diamond,k}}(\embH^{\Diamond,k+1}) - f_{\theta^{\Diamond,k+1}}\embH^{\Diamond,k+1}\|_F\\
        \leq{}& \gamma \|\embH^{\Diamond,k} - \embH^{\Diamond,k+1}\|_F + L\|\theta^{\Diamond,k+1}-\theta^{\Diamond,k}\|_F.
    \end{align*}
    By rearranging the terms, we have
    \begin{align*}
        \|\embH^{\Diamond,k} - \embH^{\Diamond,k+1}\|_F &\leq \frac{\gamma}{1-\gamma} \|\theta^{\Diamond,k+1}-\theta^{\Diamond,k}\|_F\\
        &\leq K \varepsilon.
    \end{align*}
    Combining the above inequalities, we have
    \begin{align*}
        &\mathbb{E}[\|\hisH^{\Diamond, k+1} - \embH^{\Diamond,k+1}\|_F^2]\\
        \leq{}& \rho^2 \mathbb{E}[(\|\hisH^{\Diamond, k} - \embH^{\Diamond,k}\|_F + K \varepsilon)^2]\\
        ={}& \rho^2 (\mathbb{E}[\|\hisH^{\Diamond, k} - \embH^{\Diamond,k}\|_F^2] + 2 \mathbb{E}[\|\hisH^{\Diamond, k} - \embH^{\Diamond,k}\|_F] K \varepsilon\\
        &+ (K \varepsilon)^2)\\
        \leq{} & \rho^2 (\mathbb{E}[\|\hisH^{\Diamond, k} - \embH^{\Diamond,k}\|_F^2] + 2 \sqrt{\mathbb{E}[\|\hisH^{\Diamond, k} - \embH^{\Diamond,k}\|_F^2} K \varepsilon\\
        &+ (K \varepsilon)^2)\\
        ={}& \rho^2 (\sqrt{\mathbb{E}[\|\hisH^{\Diamond, k} - \embH^{\Diamond,k}\|_F^2} + K\varepsilon)^2.
    \end{align*}
    The claims follows immediately.
\end{proof}




\begin{lemma}\label{lemma:amp_cfpi_b}
    Suppose that Assumption \ref{assmp:proof_rec} holds. If
    \begin{align*}
        &\|\theta^{\Diamond,k+1}-\theta^{\Diamond,k}\|_F \leq \varepsilon,\\
        &\|w^{k+1}-w^{k}\|_F \leq \varepsilon
    \end{align*}
    at the $k$-th iteration, then we have
    \begin{align*}
        d^{\Diamond,k+1}_{v} \leq \rho d^{\Diamond,k}_v + K \varepsilon,
    \end{align*}
    where $\rho = \sqrt{(1-\frac{1}{B}(1-\gamma^2))}$ and $K = \frac{2\gamma}{1-\gamma}$.
\end{lemma}


\begin{proof}
    According to Lemma \ref{lemma:cfpi}, we have
    \begin{align*}
        &\mathbb{E}[\|\hisV^{\Diamond, k+1} - \embV^{\Diamond, k+1}\|_F^2] \\
        \leq{}& \rho^2\mathbb{E}[\|\hisV^{\Diamond, k} - \embV^{\Diamond, k+1}\|_F^2] \\
        \leq{}& \rho^2\mathbb{E}[(\|\hisV^{\Diamond, k} - \embV ^{\Diamond, k}\|_F + \|\embV ^{\Diamond, k} - \embV^{\Diamond, k+1}\|_F )^2].
    \end{align*}
    As the transition function $f_{\theta}$ is  $L$-Lipschitz, we have
    \begin{align*}
        &\|\embV ^{\Diamond, k} - \embV^{\Diamond, k+1}\|_F \\
        ={}& \|\phi_{\theta^{\Diamond,k},w^k}(\embV ^{\Diamond, k}) - \phi_{\theta^{\Diamond,k+1},w^{k+1}}\embV^{\Diamond, k+1}\|_F \\
        \leq{}& \|\phi_{\theta^{\Diamond,k},w^k}(\embV ^{\Diamond, k}) - \phi_{\theta^{\Diamond,k},w^k}\embV^{\Diamond, k+1}\|_F \\
        &+ \|\phi_{\theta^{\Diamond,k},w^k}(\embV^{\Diamond, k+1}) - \phi_{\theta^{\Diamond,k+1},w^{k+1}}\embV^{\Diamond, k+1}\|_F\\
        \leq{}& \gamma \|\embV ^{\Diamond, k} - \embV^{\Diamond, k+1}\|_F\\ 
        &+ \gamma\sqrt{\|\theta^{\Diamond,k+1}-\theta^{\Diamond,k}\|_F^2+ \|w^{k+1}-w^k\|_F^2}\\
        \leq{}& \gamma \|\embV ^{\Diamond, k} - \embV^{\Diamond, k+1}\|_F\\
        &+ \gamma(\|\theta^{\Diamond,k+1}-\theta^{\Diamond,k}\|_F+ \|w^{k+1}-w^k\|_F).
    \end{align*}
    By rearranging the terms, we have
    \begin{align*}
        &\|\embV ^{\Diamond, k} - \embV^{\Diamond, k+1}\|_F\\
        \leq{}& \frac{\gamma}{1-\gamma} (\|\theta^{\Diamond,k+1}-\theta^{\Diamond,k}\|_F+ \|w^{k+1}-w^k\|_F)\\
        \leq{} &K \varepsilon.
    \end{align*}
    Combining the above inequalities, we have
    \begin{align*}
        &\mathbb{E}[\|\hisV^{\Diamond, k+1} - \embV^{\Diamond, k+1}\|_F^2]\\
        \leq{}& \rho^2 \mathbb{E}[(\|\hisV^{\Diamond, k} - \embV ^{\Diamond, k}\|_F + K \varepsilon)^2]\\
        ={}& \rho^2 (\mathbb{E}[\|\hisV^{\Diamond, k} - \embV ^{\Diamond, k}\|_F^2] + 2 \mathbb{E}[\|\hisV^{\Diamond, k} - \embV ^{\Diamond, k}\|_F] K \varepsilon \\ 
        &+ (K \varepsilon)^2)\\
        \leq{} & \rho^2 (\mathbb{E}[\|\hisV^{\Diamond, k} - \embV ^{\Diamond, k}\|_F^2] + 2 \sqrt{\mathbb{E}[\|\hisV^{\Diamond, k} - \embV ^{\Diamond, k}\|_F^2} K \varepsilon  \\
        &+ (K \varepsilon)^2)\\
        ={}& \rho^2 (\sqrt{\mathbb{E}[\|\hisV^{\Diamond, k} - \embV ^{\Diamond, k}\|_F^2} + K\varepsilon)^2.
    \end{align*}
    The claims follows immediately.
\end{proof}


Next, we give the proofs of Theorems \ref{thm:convergence_h_rec} and \ref{thm:convergence_v_rec}.

\begin{proof}
    The update rule in LMC4Rec implies that
    \begin{align*}
        &\|\theta^{\Diamond,k+1} - \theta^{\Diamond,k}\|_F =  \eta \|\widetilde{\mathbf{g}}_{\thetarec}(\theta^{\Diamond,k})\|_F \leq \eta G,\\
        &\|w^{k+1} - w^{k}\|_F =  \eta \|\widetilde{\mathbf{g}}_{w}(w^{k})\|_F \leq \eta G.
    \end{align*}
    According to Lemmas \ref{lemma:amp_cfpi} and \ref{lemma:amp_cfpi_b}, we have
    \begin{align*}
        d_h^{\Diamond, k+1} - \frac{KG}{1-\rho} \eta \leq{}& \rho( d_h^{\Diamond,k} - \frac{KG}{1-\rho} \eta)\\
        \leq{} &\cdots\\
        \leq{} & \rho^k (  d_h^{\Diamond,1} - \frac{KG}{1-\rho} \eta)\\
        \leq{} &\rho^k G
    \end{align*}
    and
    \begin{align*}
        d_v^{\Diamond, k+1} - \frac{KG}{1-\rho} \eta \leq{}& \rho( d_v^{\Diamond,k} - \frac{KG}{1-\rho} \eta)\\
        \leq{} &\cdots\\
        \leq{} & \rho^k (  d_v^{\Diamond,1} - \frac{KG}{1-\rho} \eta)\\
        \leq{} &\rho^k G.
    \end{align*}
    Then the claims follow immediately.
\end{proof}


\subsubsection{Proof of Theorem \ref{thm:grad_error_rec}: approximation errors of mini-batch gradients}

In this subsection, we focus on the mini-batch gradients computed by LMC4Rec, i.e.,
\begin{align*}
    \widetilde{\embg}_w(w^k;\mathcal{V}_{\mathcal{B}}^k)=\frac{1}{|\mathcal{V}_{L}^k|} \sum_{v_j\in\mathcal{V}_{L}^k} \nabla_w \ell_{w^k}(\temh^{k}_j, y_j)
\end{align*}
and
\begin{align*}
    &\widetilde{\embg}_{\theta^{\Diamond}}(\theta^{\Diamond,k};\mathcal{V}_{\mathcal{B}}^k)\\
    ={}& \frac{|\mathcal{V}|}{|\mathcal{V}_{\mathcal{B}}^k|}\sum_{v_j\in\mathcal{V}_{\mathcal{B}}^k} (\nabla_{\theta^{\Diamond}} u_{\theta^{\Diamond,k}}(\temh_j^{\Diamond,k}, \widehat{\mathbf{m}}_{\neighbor{v_j}}^{\Diamond,k}, \embx_j))\temV^{\Diamond,k}_j,
\end{align*}
where $\mathcal{V}_{\mathcal{B}}^k$ is the sampled mini-batch and $\mathcal{V}_{L_\mathcal{B}}^k$ is the corresponding labeled node set at the $k$-th iteration. We denote the mini-batch gradients computed by backward SGD by
\begin{align*}
    \embg_w(w^k;\mathcal{V}_{\mathcal{B}}^k)=\frac{1}{|\mathcal{V}_{L}^k|} \sum_{v_j\in\mathcal{V}_{L}^k} \nabla_w \ell_{w^k}(\embh^{k}_j, y_j)
\end{align*}
and
\begin{align*}
    &\embg_{\theta^{\Diamond}}(\theta^{\Diamond,k};\mathcal{V}_{\mathcal{B}}^k)\\
    ={}& \frac{|\mathcal{V}|}{|\mathcal{V}_{\mathcal{B}}^k|}\sum_{v_j\in\mathcal{V}_{\mathcal{B}}^k} (\nabla_{\theta^{\Diamond}} u_{\theta^{\Diamond,k}}(\embh_j^{\Diamond,k}, \embm_{\neighbor{v_j}}^{\Diamond,k}, \embx_j))\embV^{\Diamond,k}_j.
\end{align*}
In this subsection, we omit the sampled subgraph $\mathcal{V}_{\mathcal{B}}^k$ and simply write the mini-batch gradients as $\widetilde{\embg}_w(w^k)$, $\widetilde{\embg}_{\theta^{\Diamond}}(\theta^{\Diamond,k})$, $\embg_w(w^k)$, and $\embg_{\theta^\Diamond}(\theta^{\Diamond,k})$.
The approximation errors of gradients are denoted by
\begin{align*}
    \Delta_{w}^{k} \triangleq \widetilde{\embg}_w(w^k) - \nabla_w \loss(w^k)
\end{align*}
and
\begin{align*}
    \Delta_{\theta^{\Diamond}}^{k} \triangleq \widetilde{\embg}_{\theta^{\Diamond}}(\theta^{\Diamond,k}) - \nabla_{\theta^{\Diamond}} \loss(\theta^{\Diamond,k}).
\end{align*}

\begin{lemma}\label{lemma:grad_error_bound_w_rec}
    Suppose that Assumption \ref{assmp:proof_rec} holds. For any $k\in\mathbb{N}^*$, the difference between $\widetilde{\mathbf{g}}_w(w^k)$ and $\mathbf{g}_w(w^k)$ can be bounded as
    \begin{align*}
        \|\widetilde{\mathbf{g}}_w(w^k) - \mathbf{g}_w(w^k)\|_2 \leq \frac{\gamma^2}{1-\gamma} \|\hisH^{\Diamond,k} - \embH^{\Diamond,k}\|_F.
    \end{align*}
\end{lemma}
\begin{proof}
    We have
    \begin{align*}
        \|\widetilde{\mathbf{g}}_w(w^k) - \mathbf{g}_w(w^k)\|_2 ={}& \frac{1}{|\mathcal{V}_L^k|}\|\sum_{v_j \in \mathcal{V}_L^k} \nabla_w \ell_{w^k}(\temh_j^{\Diamond,k}, y_j)\nonumber\\
        &\quad\quad\quad\quad\quad\quad- \nabla_w \ell_{w^k}(\embh_j^{\Diamond,k}, y_j)\|_2\nonumber\\
        \leq{}& \frac{1}{|\mathcal{V}_L^k|}\sum_{v_j \in \mathcal{V}_L^k} \|\nabla_w \ell_{w^k}(\temh_j^{\Diamond,k}, y_j)\nonumber\\
        &\quad\quad\quad\quad\quad\quad- \nabla_w \ell_{w^k}(\embh_j^{\Diamond,k}, y_j)\|_2\nonumber\\
        \leq{}& \frac{\gamma}{|\mathcal{V}_L^k|} \sum_{v_j \in \mathcal{V}_L^k} \|\temh^{\Diamond,k}_j - \embh^{\Diamond, k}_j\|_2\nonumber\\
        \leq{}& \frac{\gamma}{|\mathcal{V}_L^k|} \sum_{v_j \in \mathcal{V}_L^k} \|\temH^{\Diamond,k}_{\mathcal{V}_L^k} - \embH^{\Diamond, k}_{\mathcal{V}_L^k}\|_F\nonumber\\
        \leq{}& \frac{\gamma}{|\mathcal{V}_L^k|}\cdot |\mathcal{V}_L^k|\cdot  \|\temH^{\Diamond,k}_{\mathcal{V}_L^k} - \embH^{\Diamond, k}_{\mathcal{V}_L^k}\|_F\nonumber\\
        ={}& \gamma \cdot  \|\temH^{\Diamond,k}_{\mathcal{V}_L^k} - \embH^{\Diamond, k}_{\mathcal{V}_L^k}\|_F\nonumber\\
        \leq{}&\gamma \cdot  \|\temH^{\Diamond,k}_{\mathcal{V}^k} - \embH^{\Diamond, k}_{\mathcal{V}^k}\|_F\nonumber
    \end{align*}
    Besides, we have
    \begin{align*}
        \|\temH^{\Diamond,k}_{\mathcal{V}^k} - \embH^{\Diamond, k}_{\mathcal{V}^k}\|_F ={} &  \|f_{\theta^{\Diamond,k}}(\temH^{\Diamond,k}_{\mathcal{V}^k}, \hisH^{\Diamond,k}_{\mathcal{V}\setminus\mathcal{V}^k})\\
        &\quad\quad- f_{\theta^{\Diamond,k}}(\embH^{\Diamond,k}_{\mathcal{V}^k}, \embH^{\Diamond,k}_{\mathcal{V}\setminus\mathcal{V}^k})\|_F\\
        ={}&  \|f_{\theta^{\Diamond,k}}(\temH^{\Diamond,k}_{\mathcal{V}^k}, \hisH^{\Diamond,k}_{\mathcal{V}\setminus\mathcal{V}^k})\\
        &\quad\quad- f_{\theta^{\Diamond,k}}(\embH^{\Diamond,k}_{\mathcal{V}^k}, \hisH^{\Diamond,k}_{\mathcal{V}\setminus\mathcal{V}^k})\\
        &\quad\quad+ f_{\theta^{\Diamond,k}}(\embH^{\Diamond,k}_{\mathcal{V}^k}, \hisH^{\Diamond,k}_{\mathcal{V}\setminus\mathcal{V}^k})\\
        &\quad\quad- f_{\theta^{\Diamond,k}}(\embH^{\Diamond,k}_{\mathcal{V}^k}, \embH^{\Diamond,k}_{\mathcal{V}\setminus\mathcal{V}^k})\|_F\\
        \leq{}& \gamma \cdot \|\temH^{\Diamond,k}_{\mathcal{V}^k} - \embH^{\Diamond, k}_{\mathcal{V}^k}\|_F\\
        &+ \gamma \cdot \|\hisH^{\Diamond,k}_{\mathcal{V}\setminus\mathcal{V}^k} - \embH^{\Diamond,k}_{\mathcal{V}\setminus\mathcal{V}^k}\|_F,
    \end{align*}
    which leads to
    \begin{align*}
        \|\temH^{\Diamond,k}_{\mathcal{V}^k} - \embH^{\Diamond, k}_{\mathcal{V}^k}\|_F \leq{}& \frac{\gamma}{1-\gamma}\|\hisH^{\Diamond,k}_{\mathcal{V}\setminus\mathcal{V}^k} - \embH^{\Diamond,k}_{\mathcal{V}\setminus\mathcal{V}^k}\|_F\\
        \leq{}& \frac{\gamma}{1-\gamma}\|\hisH^{\Diamond,k} - \embH^{\Diamond,k}\|_F.
    \end{align*}
    Hence, we have
    \begin{align*}
        \|\widetilde{\mathbf{g}}_w(w^k) - \mathbf{g}_w(w^k)\|_2 \leq{} \frac{\gamma^2}{1-\gamma} \|\hisH^{\Diamond,k} - \embH^{\Diamond,k}\|_F.
    \end{align*}
\end{proof}

\begin{lemma}\label{lemma:grad_error_bound_theta_rec}
    Suppose that Assumption \ref{assmp:proof_rec} holds. For any $k\in\mathbb{N}^*$, the difference between $\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{{\Diamond},k})$ and $\mathbf{g}_{\theta^{\Diamond}}(\theta^{{\Diamond},k})$ can be bounded as
    \begin{align*}
        &\|\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{{\Diamond},k}) - \mathbf{g}_{\theta^{\Diamond}}(\theta^{{\Diamond},k})\|_2\\
        \leq{}& \frac{|\mathcal{V}| G\gamma}{1-\gamma} \|\hisV^{\Diamond,k}-\embV^{\Diamond,k}\|_F + \frac{|\mathcal{V}|G\gamma^2}{1-\gamma} \|\hisH^{\Diamond,k}-\embH^{\Diamond,k}\|_F.
    \end{align*}
\end{lemma}
\begin{proof}
    Similar to the proof of Lemma \ref{lemma:grad_error_bound_w_rec}, we have
    \begin{align*}
        \|\temV^{\Diamond,k}_{\mathcal{V}^k} - \embV^{\Diamond,k}_{\mathcal{V}^k}\|_F \leq \frac{\gamma}{1-\gamma} \|\hisV^{\Diamond,k}_{\mathcal{V}^k} - \embV^{\Diamond,k}_{\mathcal{V}^k}\|_F.
    \end{align*}
    As $\|\mathbf{A}\mathbf{a}-\mathbf{B}\mathbf{b}\|_2\leq \|\mathbf{A}\|_F\|\mathbf{a}-\mathbf{b}\|_2 + \|\mathbf{A}-\mathbf{B}\|_F\|\mathbf{b}\|_2$, we can bound $\|\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k}) - \mathbf{g}_{\theta^\Diamond}(\theta^{\Diamond,k})\|_2$ by
    \begin{align*}
        &\|\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k}) - \mathbf{g}_{\theta^\Diamond}(\theta^{\Diamond,k})\|_2\nonumber\\
        \leq{}& \frac{|\mathcal{V}|}{|\mathcal{V}_{\mathcal{B}}^k|}\sum_{v_i\in\mathcal{V}_{\mathcal{B}}^k}\|\left(\nabla_{\theta^{\Diamond}} u_{\theta^{\Diamond,k}}(\temh_j^{\Diamond,k}, \widehat{\mathbf{m}}_{\neighbor{v_j}}^{\Diamond,k}, \embx_j)\right)\temV_j^{\Diamond,k}\nonumber\\
        &\quad\quad\quad\quad\quad-\left(\nabla_{\theta^{\Diamond}} u_{\theta^{\Diamond,k}}(\embh_j^{\Diamond,k}, \mathbf{m}_{\neighbor{v_j}}^{\Diamond,k}, \embx_j)\right)\embV_j^{\Diamond,k}\|_2\nonumber\\
        \leq{}& |\mathcal{V}|\max_{v_i\in\mathcal{V}_{\mathcal{B}}^k} \|\left(\nabla_{\theta^{\Diamond}} u_{\theta^{\Diamond,k}}(\temh_j^{\Diamond,k}, \widehat{\mathbf{m}}_{\neighbor{v_j}}^{\Diamond,k}, \embx_j)\right)\temV_j^{\Diamond,k}\nonumber\\
        &\quad\quad\quad\quad\quad-\left(\nabla_{\theta^{\Diamond}} u_{\theta^{\Diamond,k}}(\embh_j^{\Diamond,k}, \mathbf{m}_{\neighbor{v_j}}^{\Diamond,k}, \embx_j)\right)\embV_j^{\Diamond,k}\|_2\nonumber\\
        \leq{}& |\mathcal{V}|\max_{v_i\in\mathcal{V}_{\mathcal{B}}^k} \|\nabla_{\theta^{\Diamond}} u_{\theta^{\Diamond,k}}(\temh_j^{\Diamond,k}, \widehat{\mathbf{m}}_{\neighbor{v_j}}^{\Diamond,k}, \embx_j)\|_F\|\temV_j^{\Diamond,k}-\embV_j^{\Diamond,k}\|_2\nonumber\\
        &\quad\quad\quad\quad+ \|\nabla_{\theta^{\Diamond}} u_{\theta^{\Diamond,k}}(\temh_j^{\Diamond,k}, \widehat{\mathbf{m}}_{\neighbor{v_j}}^{\Diamond,k}, \embx_j)\nonumber\\
        &\quad\quad\quad\quad\quad\quad-\nabla_{\theta^{\Diamond}} u_{\theta^{\Diamond,k}}(\embh_j^{\Diamond,k}, \mathbf{m}_{\neighbor{v_j}}^{\Diamond,k}, \embx_j)\|_F\|\embV_j^{\Diamond,k}\|_2\nonumber\\
        \leq{}& |\mathcal{V}| G \|\temV^{\Diamond,k}-\embV^{\Diamond,k}\|_F + |\mathcal{V}|G\gamma \|\temH^{\Diamond,k}-\embH^{\Diamond,k}\|_F\nonumber\\
        \leq{}& \frac{|\mathcal{V}| G\gamma}{1-\gamma} \|\hisV^{\Diamond,k}-\embV^{\Diamond,k}\|_F + \frac{|\mathcal{V}|G\gamma^2}{1-\gamma} \|\hisH^{\Diamond,k}-\embH^{\Diamond,k}\|_F.
    \end{align*}
\end{proof}

\begin{lemma}\label{lemma:grad_error}
    Suppose that Assumption \ref{assmp:proof_rec} holds, then there exists $C>0$ and $\rho\in(0,1)$ such that for any $k\in\mathbb{N}^*$ we have
    \begin{align*}
        &\mathbb{E}[\|\Delta_{w}^k\|_2^2] = ({\rm Bias}(\widetilde{\mathbf{g}}_w(w^k)))^2+{\rm Var}(\widetilde{\mathbf{g}}_w(w^k)),\\
        &\mathbb{E}[\|\Delta_{\theta^{\Diamond}}^k\|_2^2] = ({\rm Bias}(\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k})))^2+{\rm Var}(\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k})),
    \end{align*}
    where
    \begin{align*}
        &{\rm Var}(\widetilde{\mathbf{g}}_w(w^k)) = \mathbb{E}[\|\mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)] - \widetilde{\mathbf{g}}_w(w^k)\|_2^2],\\
        &{\rm Bias}(\widetilde{\mathbf{g}}_w(w^k)) = \|\mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)] - \nabla_w\mathcal{L}_w(w^k)\|_2,\\
        &{\rm Var}(\mathbf{g}_{\theta^{\Diamond}}(\theta^{\Diamond,k}))= \mathbb{E}[\|\mathbb{E}[\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k})] - \widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k})\|_2^2],\\
        &{\rm Bias}(\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k}))= \|\mathbb{E}[\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k})] - \nabla_{\theta^{\Diamond}}\mathcal{L}_{\theta^{\Diamond}}(\theta^{\Diamond,k})\|_2
    \end{align*}
    and
    \begin{align*}
        &{\rm Bias}(\widetilde{\mathbf{g}}_w(w^k)) \leq C(\eta+\rho^{k-1}),\\
        &{\rm Bias}(\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k})) \leq C(\eta+\rho^{k-1}).
    \end{align*}
\end{lemma}
\begin{proof}
    Similar to Lemma \ref{prop:grad_error}, the bias-variance decomposition equations of $\mathbb{E}[\|\Delta_{w}^k\|_2^2]$ and $\mathbb{E}[\|\Delta_{\theta^{\Diamond}}^k\|_2^2]$ are obvious. Next we give the upper bounds of the bias terms. By Lemma \ref{lemma:grad_error_bound_w_rec}, we have
    \begin{align}
        {\rm Bias}(\widetilde{\mathbf{g}}_w(w^k)) ={}& \|\mathbb{E}[\widetilde{\mathbf{g}}_w(w^k)] - \nabla_w\mathcal{L}_w(w^k)\|_2\nonumber\\
        ={}& \|\mathbb{E}[\widetilde{\mathbf{g}}_w(w^k) - \mathbf{g}_w(w^k)]\|_2\nonumber\\
        \leq{}& \frac{\gamma^2}{1-\gamma} \mathbb{E}[\|\hisH^{\Diamond,k} - \embH^{\Diamond,k}\|_F^2]\nonumber\\
        \leq{}& \frac{\gamma^2}{1-\gamma} \left(\mathbb{E}[\|\hisH^{\Diamond,k} - \embH^{\Diamond,k}\|_F^2]\right)^{\frac{1}{2}}\nonumber\\
        ={}& \frac{\gamma^2}{1-\gamma} d_h^{\Diamond, k}.\label{eqn:bias_bound_w_rec}
    \end{align}
    By Theorem \ref{thm:convergence_h_rec} we know that
    \begin{align*}
        d_h^{\Diamond, k} \leq \rho^{k-1}G + \frac{KG}{1-\rho}\eta.
    \end{align*}
    Hence we have
    \begin{align*}
        {\rm Bias}(\widetilde{\mathbf{g}}_w(w^k)) \leq \frac{\gamma^2G}{1-\gamma}\rho^{k-1} + \frac{KG\gamma^2}{(1-\rho)(1-\gamma)}\eta.
    \end{align*}
    By Lemma \ref{lemma:grad_error_bound_theta_rec}, we can bound ${\rm Bias}(\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k}))$ as
    \begin{align}
        {\rm Bias}(\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k}))={}& \|\mathbb{E}[\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k})] - \nabla_{\theta^{\Diamond}}\mathcal{L}_{\theta^{\Diamond}}(\theta^{\Diamond,k})\|_2\nonumber\\
        ={}& \|\mathbb{E}[\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k}) - \mathbf{g}_{\theta^{\Diamond}}(\theta^{\Diamond,k})]\|_2 \nonumber \\
        \leq{} & \frac{|\mathcal{V}| G\gamma}{1-\gamma} \mathbb{E}[\|\hisV^{\Diamond,k}-\embV^{\Diamond,k}\|_F]\nonumber\\
        &+ \frac{|\mathcal{V}|G\gamma^2}{1-\gamma} \mathbb{E}[\|\hisH^{\Diamond,k}-\embH^{\Diamond,k}\|_F]\nonumber\\
        \leq{} & \frac{|\mathcal{V}| G\gamma}{1-\gamma} \left(\mathbb{E}[\|\hisV^{\Diamond,k}-\embV^{\Diamond,k}\|_F^2]\right)^{\frac{1}{2}}\nonumber\\
        &+ \frac{|\mathcal{V}|G\gamma^2}{1-\gamma} \left(\mathbb{E}[\|\hisH^{\Diamond,k}-\embH^{\Diamond,k}\|_F^2]\right)^{\frac{1}{2}}\nonumber\\
        ={}& \frac{|\mathcal{V}| G\gamma}{1-\gamma} d_v^{\Diamond,k}+ \frac{|\mathcal{V}|G\gamma^2}{1-\gamma} d_h^{\Diamond,k} \label{eqn:bias_bound_theta_rec}.
    \end{align}
    By Theorems \ref{thm:convergence_h_rec} and \ref{thm:convergence_v_rec} we know that
    \begin{align*}
        &d_h^{\Diamond, k} \leq \rho^{k-1}G + \frac{KG}{1-\rho}\eta,\\
        &d_v^{\Diamond, k} \leq \rho^{k-1}G + \frac{KG}{1-\rho}\eta.
    \end{align*}
    Hence we have
    \begin{align*}
        {\rm Bias}(\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k})) \leq{}& \frac{|\mathcal{V}|G^2\gamma(1+\gamma)}{1-\gamma} \rho^{k-1}\\
        &+ \frac{|\mathcal{V}|KG^2\gamma(1+\gamma)}{(1-\rho)(1-\gamma)} \eta.
    \end{align*}
    By letting
    \begin{align*}
        C = \max\{&\frac{\gamma^2G}{1-\gamma}, \frac{KG\gamma^2}{(1-\rho)(1-\gamma)},\\
        &\frac{|\mathcal{V}|G^2\gamma(1+\gamma)}{1-\gamma}, \frac{|\mathcal{V}|KG^2\gamma(1+\gamma)}{(1-\rho)(1-\gamma)}\}
    \end{align*}
    we have
    \begin{align*}
        &{\rm Bias}(\widetilde{\mathbf{g}}_w(w^k)) \leq C(\eta+\rho^{k-1}),\\
        &{\rm Bias}(\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k})) \leq C(\eta+\rho^{k-1}).
    \end{align*}
\end{proof}
By letting $\eta = \mathcal{O}(\varepsilon)$, Theorem \ref{thm:grad_error_rec} follows immediately.



\subsubsection{Proof of Theorem \ref{thm:convergence_rec}: convergence guarantees}

In this subsection, we give the convergence guarantees of LMC4Rec. We first give sufficient conditions for the convergence.
\begin{lemma}\label{lemma:sgd}
    Suppose that function $f:\mathbb{R}^{n} \rightarrow \mathbb{R}$ is continuously differentiable. Consider a optimization algorithm with any bounded initialization $\vecx^{1}$ and an update rule in the form of
    \begin{align*}
        \vecx^{k+1} = \vecx^{k} - \eta \vecd(\vecx^{k}),
    \end{align*}
    where $\eta>0$ is the learning rate and $\vecd(\vecx^{k})$ is the estimated gradient that can be seen as a stochastic vector depending on $\vecx^{k}$. Let the estimation error of gradients be $\Delta^{k} = \vecd(\vecx^{k}) - \nabla f(\vecx^{k}) $.
    Suppose that
    \begin{enumerate}
        \item the optimal value $f^*  = \inf_{\vecx} f(\vecx)$ is bounded; \label{con:1_rec}
        \item the gradient of $f$ is $\gamma$-Lipschitz, i.e., \label{con:2_rec}
        \begin{align*}
            \|\nabla f(\vecy) - \nabla f(\vecx)\|_2 \leq \gamma\|\vecy - \vecx\|_2,\,\forall\,\vecx,\vecy \in \mathbb{R}^{n};
        \end{align*}
        \item there exists $G_0>0$ that does not depend on $\eta$ such that \label{con:3}
        \begin{align*}
            \mathbb{E}[\|\Delta^k\|_2^2] \leq G_0,\,\,\forall\,k\in\mathbb{N}^*;
        \end{align*}

        \item there exists $\rho \in (0,1)$ that does not depend on $\eta$ such that for any $k\in\mathbb{N}^*$ we have \label{con:4}
        \begin{align*}
            |\mathbb{E}[\langle \nabla f(\vecx^{k}), \Delta^{k} \rangle]| \leq \eta G_0 + \rho^{k-1} G_0,
        \end{align*}
        where $G_0$ is the same constant as that in Condition \ref{con:3},
    \end{enumerate}
    then by letting $\eta = \min\{\frac{1}{\gamma},\frac{1}{N^{\frac{1}{2}}}\}$, we have
    \begin{align*}
        &\mathbb{E}[ \|\nabla f(\vecx^{(R)})\|_2^2]\\
        \leq{}&  \frac{2(f(\vecx^{1}) - f^*)+(\gamma+1)G_0 }{ N^{\frac{1}{2}} }+ \frac{G_0}{(1-\rho)N}\\
        ={}& \mathcal{O}(\frac{1}{N^{\frac{1}{2}}})
    \end{align*}
     where $R$ is chosen uniformly from $[N]$.
\end{lemma}
\begin{proof}
    As the gradient of $f$ is $\gamma$-Lipschitz, we have
    \begin{align*}
    	f(\vecy)={}&f(\vecx)+\int_{\vecx}^{\vecy}\nabla f(\mathbf{z})d\mathbf{z}\\
    	={}&f(\vecx)+\int_0^1\langle\nabla f(\vecx+t(\vecy-\vecx)), \vecy-\vecx\rangle \rmd t\\
    	={}&f(\vecx)+\langle\nabla f(\vecx),\vecy-\vecx\rangle\\
    	&+\int_0^1\langle\nabla f(\vecx+t(\vecy-\vecx))-\nabla f(\vecx), \vecy-\vecx\rangle \rmd t\\
    	\leq{}&f(\vecx)+\langle\nabla f(\vecx),\vecy-\vecx\rangle\\
    	&+\int_0^1\|\nabla f(\vecx+t(\vecy-\vecx))-\nabla f(\vecx)\|\| \vecy-\vecx\| \rmd t\\
    	\leq{}&f(\vecx)+\langle\nabla f(\vecx),\vecy-\vecx\rangle+\int_0^1Lt\|\vecy-\vecx\|^2\rmd t\\
    	\leq{}&f(\vecx)+\langle\nabla f(\vecx), \vecy-\vecx\rangle+\frac{\gamma}{2}\|\vecy-\vecx\|^2,
    \end{align*}
    Then, we have
    \begin{align*}
        &f(\vecx^{k+1})\\
        \leq{}& f(\vecx^{k}) + \langle  \nabla f(\vecx^{k}), \vecx^{k+1} - \vecx^{k} \rangle+ \frac{\gamma}{2}\|\vecx^{k+1}-\vecx^{k}\|_2^2 \\
        = {}& f(\vecx^{k}) - \eta \langle \nabla f(\vecx^{k}), \vecd(\vecx^{k}) \rangle + \frac{\eta^2 \gamma}{2}\|\vecd(\vecx^{k})\|_2^2 \\
        = {}& f(\vecx^{k}) - \eta \langle \nabla f(\vecx^{k}), \Delta^{k} \rangle- \eta \|\nabla f(\vecx^{k})\|_2^2\\
        &+ \frac{\eta^2 \gamma}{2}(\|\Delta^{k}\|_2^2 +\|\nabla f(\vecx^{k})\|_2^2 +2\langle \Delta^{k}, \nabla f(\vecx^{k}) \rangle)\\
        = {}& f(\vecx^{k})  - \eta (1-\eta \gamma) \langle \nabla f(\vecx^{k}), \Delta^{k} \rangle\\
        &- \eta (1-\frac{\eta \gamma}{2}) \|\nabla f(\vecx^{k})\|_2^2 + \frac{\eta^2 \gamma}{2}\|\Delta^{k}\|_2^2.
    \end{align*}
    By taking expectation of both sides, we have
    \begin{align*}
        \mathbb{E}[f(\vecx^{k+1})] \leq{}& \mathbb{E}[f(\vecx^{k})] - \eta (1-\eta \gamma)  \mathbb{E}[\langle \nabla f(\vecx^{k}), \Delta^{k} \rangle]\\
        &- \eta (1-\frac{\eta \gamma}{2}) \mathbb{E}[ \|\nabla f(\vecx^{k})\|_2^2] + \frac{\eta^2 \gamma}{2}\mathbb{E}[\|\Delta^{k}\|_2^2].
    \end{align*}
    By summing up the above inequalities for $k=1,2,\ldots,N$ and dividing both sides by $N \eta(1-\frac{\eta \gamma}{2})$, we have
    \begin{align*}
        &\frac{\sum_{k=1}^{N} \mathbb{E}[ \|\nabla f(\vecx^{k})\|_2^2]}{N}\\
        \leq{}& \frac{f(\vecx^{1}) - \mathbb{E}[f(\vecx^{k})]}{N \eta(1-\frac{\eta \gamma}{2}) } - \frac{(1-\eta \gamma)}{(1-\frac{\eta \gamma}{2})} \frac{\sum_{k=1}^{N} \mathbb{E}[\langle \nabla f(\vecx^{k}), \Delta^{k} \rangle]}{N} \\
        &+ \frac{\eta \gamma}{2-\eta \gamma } \frac{\sum_{k=1}^{N} \mathbb{E}[\|\Delta^{k}\|_2^2]}{k} \\
        \leq{}& \frac{f(\vecx^{1}) - f^* }{N \eta(1-\frac{\eta \gamma}{2}) }  + \frac{\sum_{k=1}^{N} |\mathbb{E}[\langle \nabla f(\vecx^{k}), \Delta^{k} \rangle]|}{N}\\
        &+ \frac{\eta \gamma}{2-\eta \gamma } \frac{\sum_{k=1}^{N} \mathbb{E}[\|\Delta^{k}\|_2^2]}{N},
    \end{align*}
    where the second inequality comes from $\eta \gamma>0$ and $f(\vecx^{k}) \geq f^* $. According to the above conditions, we have
    \begin{align*}
        &\frac{\sum_{k=1}^{N} \mathbb{E}[ \|\nabla f(\vecx^{k})\|_2^2]}{N}\\
        \leq{}& \frac{f(\vecx^{1}) - f^* }{N \eta(1-\frac{\eta \gamma}{2}) } + \eta G_0 + \frac{G_0\sum_{k=1}^{N} \rho^{k-1}}{N} + \frac{\eta \gamma}{2-\eta \gamma} G_0\\
        \leq{}& \frac{f(\vecx^{1}) - f^* }{N \eta(1-\frac{\eta \gamma}{2}) } + \eta G_0 + \frac{G_0\sum_{k=1}^{\infty} \rho^{k-1}}{N} + \frac{\eta \gamma}{2-\eta \gamma } G_0\\
        \leq{}& \frac{f(\vecx^{1}) - f^* }{N \eta(1-\frac{\eta \gamma}{2}) } + \eta G_0 + \frac{G_0}{(1-\rho)N} + \frac{\eta \gamma}{2-\eta \gamma } G_0
    \end{align*}
    Notice that $\mathbb{E}[ \|\nabla f(\vecx^{(R)})\|_2^2] = \frac{\sum_{k=1}^{N} \mathbb{E}[ \|\nabla f(\vecx^{k})\|_2^2]}{N}$. By taking $\eta = \min\{\frac{1}{\gamma},\frac{1}{N^{\frac{1}{2}}}\}$ and rearranging the terms, we have
    \begin{align*}
         &\mathbb{E}[ \|\nabla f(\vecx^{(R)})\|_2^2]\\
         ={}& \frac{\sum_{k=1}^{N} \mathbb{E}[ \|\nabla f(\vecx^{k})\|_2^2]}{N} \\
         \leq{}& \frac{f(\vecx^{1}) - f^* }{N \eta(1-\frac{\eta \gamma}{2}) } + \frac{\eta \gamma G_0}{(2-\eta \gamma) } + \eta G_0 + \frac{G_0}{(1-\rho)N} \\
         \leq{}&  \frac{2(f(\vecx^{1}) - f^*)+(\gamma+1)G_0 }{ N^{\frac{1}{2}} } + \frac{G_0}{(1-\rho)N}\\
         ={}& \mathcal{O}(\frac{1}{N^{\frac{1}{2}}}).
    \end{align*}
\end{proof}

Given an RecGNN, following \cite{vrgcn}, we directly assume that:
\begin{enumerate}
    \item the optimal value
    \begin{align*}
        \loss^* = \inf_{w,\theta^{\Diamond}} \loss 
    \end{align*}
    is bounded by $G>0$;
    \item the gradients of $\loss$ with respect to parameters $w$ and $\theta^{\Diamond}$, i.e.,
    \begin{align*}
        \nabla_w \loss,\, \nabla_{\theta^{\Diamond}} \loss
    \end{align*}
    are $\gamma$-Lipschitz.
\end{enumerate}
To show the convergence of LMC4Rec by Lemma \ref{lemma:sgd}, it suffices to show that
\begin{enumerate}[resume]
    \item there exists $G_1>0$ that does not depend on $\eta$ such that
    \begin{align*}
        &\mathbb{E}[\|\Delta^k_w\|_2^2] \leq G_1,\,\,\forall\,k\in\mathbb{N}^*,\\
        &\mathbb{E}[\|\Delta^k_{\theta^{\Diamond}}\|_2^2] \leq G_1,\,\,\forall\,k\in\mathbb{N}^*;
    \end{align*}
    
    \item there exist $G_2>0$ and $\rho\in(0,1)$ that do not depend on $\eta$ such that for any $k\in\mathbb{N}^*$ we have
    \begin{align*}
        &|\mathbb{E}[\langle \nabla_w\loss, \Delta^k_w \rangle]| \leq G_2(\eta + \rho^{k-1}),\\
        &|\mathbb{E}[\langle \nabla_{\theta^{\Diamond}}\loss, \Delta^k_{\theta^{\Diamond}} \rangle]| \leq G_2(\eta + \rho^{k-1}).
    \end{align*}
\end{enumerate}



\begin{lemma}\label{lemma:cond3}
    Suppose that Assumption \ref{assmp:proof_rec} holds, then
    \begin{align*}
        &\mathbb{E}[\|\Delta^k_w\|_2^2] \leq G_1 \triangleq 4G^2,\,\,\forall\,k\in\mathbb{N}^*,\\
        &\mathbb{E}[\|\Delta^k_{\theta^{\Diamond}}\|_2^2] \leq G_1 \triangleq 4G^2,\,\,\forall\,k\in\mathbb{N}^*.
    \end{align*}
\end{lemma}
\begin{proof}
    We have
    \begin{align*}
        \mathbb{E}[\|\Delta^k_w\|_2^2]={}&\mathbb{E}[\| \widetilde{\mathbf{g}}_w(w^k) - \nabla_{w} \mathcal{L}(w^k) \|_2^2]\\
        \leq{}& 2( \mathbb{E}[ \| \widetilde{\mathbf{g}}_w(w^k)\|_2^2] +  \mathbb{E}[ \|\nabla_{w} \mathcal{L}(w^k)\|_2^2])\\
        \leq{}& 4 G^2
    \end{align*}
    and
    \begin{align*}
        \mathbb{E}[\|\Delta^k_{\theta^{\Diamond}}\|_2^2]={}&\mathbb{E}[\| \widetilde{\mathbf{g}}_{\theta^{\Diamond}}({\theta^{\Diamond,k}}) - \nabla_{\theta^{\Diamond}} \mathcal{L}({\theta^{\Diamond,k}}) \|_2^2]\\
        \leq{}& 2( \mathbb{E}[ \| \widetilde{\mathbf{g}}_{\theta^{\Diamond}}({\theta^{\Diamond,k}})\|_2^2] +  \mathbb{E}[ \|\nabla_{\theta^{\Diamond}} \mathcal{L}({\theta^{\Diamond,k}})\|_2^2])\\
        \leq{}& 4 G^2
    \end{align*}
\end{proof}





\begin{lemma}\label{lemma:cond4}
    Suppose that Assumption \ref{assmp:proof_rec} holds, then there exist $G_2>0$ and $\rho\in(0,1)$ that do not depend on $\eta$ such that for any $k\in\mathbb{N}^*$ we have
    \begin{align*}
        &|\mathbb{E}[\langle \nabla_w\loss, \Delta^k_w \rangle]| \leq G_2(\eta + \rho^{k-1}),\\
        &|\mathbb{E}[\langle \nabla_{\theta^{\Diamond}}\loss, \Delta^k_{\theta^{\Diamond}} \rangle]| \leq G_2(\eta + \rho^{k-1}).
    \end{align*}
\end{lemma}
\begin{proof}
    By Eqs. \eqref{eqn:bias_bound_w_rec} and \eqref{eqn:bias_bound_theta_rec} we know that there exist $C>0$ and $\rho\in(0,1)$ such that for any $k\in\mathbb{N}^*$ we have
    \begin{align*}
        &\mathbb{E}[\| \widetilde{\mathbf{g}}_w(w^k) - \mathbf{g}_w(w^k)\|_2]\leq C(\eta+\rho^{k-1}),
    \end{align*}
    and
    \begin{align*}
        &\mathbb{E}[\|\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k}) - \mathbf{g}_{\theta^\Diamond}(\theta^{\Diamond,k})\|_2]\leq C(\eta+\rho^{k-1}),
    \end{align*}
    Hence we have
    \begin{align*}
        |\mathbb{E}[\langle \nabla_w\loss, \Delta^k_w \rangle]|={}& |\mathbb{E}[\langle \nabla_w\loss, \widetilde{\mathbf{g}}_w(w^k) - \nabla_w\loss(w^k) \rangle]|\\
        ={} &|\mathbb{E}[\langle \nabla_w\loss, \widetilde{\mathbf{g}}_w(w^k) - \mathbf{g}_w(w^k) \rangle]|\\
        \leq{}& \mathbb{E}[\|\nabla_w\loss\|_2\|\widetilde{\mathbf{g}}_w(w^k) - \mathbf{g}_w(w^k)\|_2]\\
        \leq{}& G \mathbb{E}[\| \widetilde{\mathbf{g}}_w(w^k) - \mathbf{g}_w(w^k)\|_2]\\
        \leq {}& G_2(\eta + \rho^{k-1})
    \end{align*}
    and
    \begin{align*}
        |\mathbb{E}[\langle \nabla_{\theta^{\Diamond}}\loss, \Delta^k_{\theta^{\Diamond}} \rangle]|={}& |\mathbb{E}[\langle \nabla_{\theta^{\Diamond}}\loss, \widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k}) - \nabla_{\theta^{\Diamond}}\loss(\theta^{\Diamond,k}) \rangle]|\\
        ={} &|\mathbb{E}[\langle \nabla_{\theta^{\Diamond}}\loss, \widetilde{\mathbf{g}}_{\theta^{\Diamond}}({\theta^{\Diamond,k}}) - \mathbf{g}_{\theta^{\Diamond}}({\theta^{\Diamond,k}}) \rangle]|\\
        \leq{}& \mathbb{E}[\|\nabla_{\theta^{\Diamond}}\loss\|_2\|\widetilde{\mathbf{g}}_{\theta^{\Diamond}}({\theta^{\Diamond,k}}) - \mathbf{g}_{\theta^{\Diamond}}({\theta^{\Diamond,k}})\|_2]\\
        \leq{}& G \mathbb{E}[\|\widetilde{\mathbf{g}}_{\theta^{\Diamond}}(\theta^{\Diamond,k}) - \mathbf{g}_{\theta^\Diamond}(\theta^{\Diamond,k})\|_2]\\
        \leq {}& G_2(\eta + \rho^{k-1}),
    \end{align*}
    where $G_2 = GC$.
\end{proof}

According to Lemmas \ref{lemma:cond3} and \ref{lemma:cond4}, the conditions in Lemma \ref{lemma:sgd} hold. By letting
\begin{align*}
    \varepsilon ={}& \left( \frac{2(f(\vecx^{1}) - f^*)+(\gamma+1)G_0 }{ N^{\frac{1}{2}} }+ \frac{G_0}{(1-\rho)N} \right)^{\frac{1}{2}}\\
    ={}& \mathcal{O}(\frac{1}{N^{\frac{1}{4}}}),
\end{align*}
we know that Theorem \ref{thm:convergence_rec} follows immediately.



\section{More Experiments}

\subsection{Experiments with ConvGNNs}

\subsubsection{Performance on Small Datasets}

Figure \ref{fig:runtime_small} reports the convergence curves GD, GAS, and LMC4Conv for GCN \cite{gcn} on three small datasets, i.e., Cora, Citeseer, and PubMed from Planetoid \cite{planetoid}.
LMC4Conv is faster than GAS, especially on the CiteSeer and PubMed datasets.
Notably, the key bottleneck on the small datasets is graph sampling rather than forward and backward passes. Thus, GD is faster than GAS and LMC4Conv, as it avoids graph sampling by directly using the whole graph. 

\begin{figure*}[t]
\centering % <-- added
\includegraphics[width=\linewidth]{imgs/convgnn/exp_sec2/testacc_runtime_small.pdf}
\caption{
Testing accuracy w.r.t. runtimes (s).} \label{fig:runtime_small}
\end{figure*}



\subsubsection{Comparison in terms of Training Time per Epoch}\label{sec:exp_epochtime}
We evaluate the training time per epoch of Cluster-GCN, GAS, FM, and LMC4Conv in Table \ref{tab:epochtime}. 
Compared with GAS, LMC4Conv additionally accesses historical auxiliary variables.
Inspired by GAS \cite{gas}, we use the concurrent mini-batch execution to asynchronously access historical auxiliary variables.
Moreover, from the convergence analysis of LMC4Conv, we can sample clusters to construct fixed subgraphs at preprocessing step (Line 2 in Algorithm \ref{alg:lmc4conv}) rather than sample clusters to construct various subgraphs at each training step\footnote{Cluster-GCN proposes to sample clusters to construct various subgraphs at each training step and LMC4Conv follows it. If a subgraph-wise sampling method prunes an edge at the current step, the GNN may observe the pruned edge at the next step by resampling subgraphs.
This avoids GNN overfitting the graph which drops some important edges as shown in Section 3.2 in \cite{cluster_gcn} (we also observe that GAS achieves the accuracy of 71.5\% and 71.1\% under stochastic subgraph partition and fixed subgraph partition respectively on the Ogbn-arxiv dataset).
}.
This further avoids sampling costs.
Finally, the training time per epoch of LMC4Conv is comparable with GAS.
Cluster-GCN is slower than GAS and LMC4Conv, as it prunes edges in forward passes, introducing additional normalization operation for the adjacency matrix of the sampled subgraph by $[\mathbf{A}_{\inbatch}]_{i,j}/\sqrt{deg_{\inbatch}(i)deg_{\inbatch}(j)}$, where $deg_{\inbatch}(i)$ is the degree in the sampled subgraph rather than the whole graph.
The normalized adjacency matrix is difficult to store and reuse, as the sampled subgraph may be different.
FM is slower than other methods, as they additionally update historical embeddings in the storage for the nodes outside the mini-batches.

\begin{table}[h]\centering
      \caption{%
      Training time (s) per epoch of Cluster-GCN, GAS, FM, and LMC4Conv.
      }\label{tab:epochtime}
    \setlength{\tabcolsep}{1.9mm}
    \scalebox{1}{
    \begin{tabular}{lcccc}
    \toprule
    \textbf{Dataset} \& \textbf{GNN}
    & {\small Cluster-GCN} & {\small GAS} & {\small FM} & {\small LMC4Conv} \\
    \midrule
    Ogbn-arxiv \& GCN   & 0.51 & 0.46 & 0.75 & \textbf{0.45}\\
    FLICKR \& GCN       & 0.33 & 0.29 & 0.45 & \textbf{0.26}\\
    REDDIT \& GCN       & 2.16 & \textbf{2.11} & 5.67 & 2.28\\
    PPI \& GCN          & 0.84 & \textbf{0.61} & 0.78 & 0.62\\
    \midrule
    Ogbn-arxiv \& GCNII & --- & 0.92  & 1.02 & \textbf{0.91}\\
    FLICKR \& GCNII     & --- & \textbf{1.32}  & 1.44 & 1.33\\
    REDDIT \& GCNII     & --- & \textbf{4.47}  & 7.89 & 4.71\\
    PPI \& GCNII        & --- & \textbf{4.61}  & 5.38 & 4.77\\
    \bottomrule
  \end{tabular}
    }
\end{table}


\subsubsection{Comparison in terms of Memory under different Batch Sizes}

In Table \ref{tab:memory_consumption}, we report the GPU memory consumption, and the proportion of reserved messages {\small $\sum_{k=1}^{b} \|\widetilde{\mathbf{A}}_{\mathcal{V}_{b_k}}^{alg}\|_0/\|\widetilde{\mathbf{A}}\|_0$} in forward and backward passes for GCN, where {\small $\widetilde{\mathbf{A}}$} is the adjacency matrix of full-batch GCN, {\small $\widetilde{\mathbf{A}}_{\mathcal{V}_{b_k}}^{alg}$} is the adjacency matrix used in a subgraph-wise method $alg$ (e.g., Cluster-GCN, GAS, and LMC4Conv), and {\small$\|\cdot\|_0$} denotes the {\small$\ell_0$}-norm. As shown in Table \ref{tab:memory_consumption}, LMC4Conv makes full use of all sampled nodes in both forward and backward passes, which is the same as full-batch GD. {\it Default} indicates the default batch size used in the codes and toolkits of GAS \cite{gas}.

\begin{table}[h]\centering
    \caption{GPU memory consumption (MB) and the proportion of reserved messages (\%) in forward and backward passes of GD, Cluster-GCN, GAS, and LMC4Conv for training GCN. {\it Default} indicates the default batch size used in the codes and toolkits of GAS \cite{gas}. }\label{tab:memory_consumption}
    \setlength{\tabcolsep}{1.9mm}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cccccc}
    \toprule
    \textbf{Batch size} & \textbf{Methods} & Ogbn-arxiv & FLICKR & REDDIT & PPI\\
    \midrule
    \mc{2}{c}{Full-batch GD} & 681/{\bf 100}\%/{\bf 100}\% & 411/{\bf 100}\%/{\bf 100}\% & 2067/{\bf 100}\%/{\bf 100}\% & 605/{\bf 100}\%/{\bf 100}\%\\
    \midrule
    \mr{3}{1} & Cluster-GCN & {\bf 177}/\,\,\,67\%/\,\,\,67\% & {\bf 138}/\,\,\,57\%/\,\,\,57\% & {\bf 428}/\,\,\,35\%/\,\,\,35\% & {\bf 189}/\,\,\,90\%/\,\,\,90\%\\
    & GAS & 178/{\bf 100}\%/\,\,\,67\% & 168/{\bf 100}\%/\,\,\,57\% & 482/{\bf 100}\%/\,\,\,35\% & 190/{\bf 100}\%/\,\,\,90\%\\
    & {\bf LMC4Conv} & 207/{\bf 100}\%/{\bf 100}\% & 177/{\bf 100}\%/{\bf 100}\% & 610/{\bf 100}\%/{\bf 100}\% & 197/{\bf 100}\%/{\bf 100}\%\\
    \midrule
    \mr{3}{\rm Default} & Cluster-GCN & 424/\,\,\,83\%/\,\,\,83\% & 310/\,\,\,77\%/\,\,\,77\% & 1193/\,\,\,65\%/\,\,\,65\% & 212/\,\,\,91\%/\,\,\,91\%\\
    & GAS & 452/{\bf 100}\%/\,\,\,83\% & 375/{\bf 100}\%/\,\,\,77\% & 1508/{\bf 100}\%/\,\,\,65\% & 214/{\bf 100}\%/\,\,\,91\%\\
    & {\bf LMC4Conv} & 557/{\bf 100}\%/{\bf 100}\% & 376/{\bf 100}\%/{\bf 100}\% & 1829/{\bf 100}\%/{\bf 100}\% & 267/{\bf 100}\%/{\bf 100}\%\\
    \bottomrule
  \end{tabular}
    }
\end{table}



\subsubsection{Ablation about $\beta_i$}\label{sec:ablation_beta}

As shown in Section \ref{sec:selection_beta}, $\beta_i = score(i)\alpha$ in LMC4Conv. We report the prediction performance under $\alpha \in \{0.0, 0.2, 0.4, 0.6, 0.8, 1.0\}$ and $score \in \{f(x)=x^2,f(x)=2x-x^2,f(x)=x,f(x)=1;x= deg_{local}(i)/deg_{global}(i)\}$ in Tables \ref{tab:ablation_alpha} and \ref{tab:ablation_score} respectively. When exploring the effect of a specific hyperparameter, we fix the other hyperparameters as their best values. Notably, $\alpha=0$ implies that LMC4Conv directly uses the historical values as affordable without alleviating their staleness, which is the same as that in GAS.
Under large batch sizes, LMC4Conv achieves the best performance with large $\beta_i=1$, as large batch sizes improve the quality of the incomplete up-to-date messages.
Under small batch sizes, LMC4Conv achieves the best performance with small $\beta_i=0.4 score_{2x-x^2}(i)$, as small learning rates alleviate the staleness of the historical values.



\begin{table}[h]\centering
      \caption{%
      Prediction performance under different $\alpha$ on the Ogbn-arxiv dataset.
      }\label{tab:ablation_alpha}
    \setlength{\tabcolsep}{1.1mm}
    \scalebox{1}{
    \begin{tabular}{cc|cccccc}
    \toprule
    \mr{2}{Batch Sizes} & \mr{2}{learning rates} & \mc{6}{c}{$\alpha$}  \\
    & & 0.0 & 0.2 & 0.4 & 0.6 & 0.8 & 1.0 \\
    \midrule
    1   & 1e-4 & 71.34 & 71.39 & \textbf{71.65} & 71.31 & 70.86 & 70.57\\
    40  & 1e-2 & 69.85 & 69.12 & 69.89 & 69.61 & 69.82 & \textbf{71.44}\\
    \bottomrule
  \end{tabular}
    }
\end{table}

\begin{table}[h]\centering
      \caption{%
      Prediction performance under different $score$ on the Ogbn-arxiv dataset.
      }\label{tab:ablation_score}
    \setlength{\tabcolsep}{0.5mm}
    \scalebox{0.75}{
    \begin{tabular}{cc|ccccc}
    \toprule
    \mr{2}{Batch Sizes} & \mr{2}{learning rates} & \mc{5}{c}{$score$}  \\
    & & $f(x)=2x-x^2$ & $f(x)=1$ & $f(x)=x^2$ & $f(x)=x$ & $f(x)=sin(x)$  \\
    \midrule
    1   & 1e-4 & \textbf{71.35} & 70.84 & 71.32 & 71.30 & 71.13 \\
    40  & 1e-2 & 67.59 & \textbf{71.44} & 69.91 & 70.03 & 70.32\\
    \bottomrule
  \end{tabular}
    }
\end{table}


\begin{figure*}[t]
\centering % <-- added
\begin{subfigure}{1.0\linewidth}
  \includegraphics[width=\linewidth]{imgs/revised_figure/small/random_acc_runtime.pdf}
    \caption{Random partitions}\label{subfig:test_acc_random_runtime}
\end{subfigure}\hfil
\begin{subfigure}{1.0\linewidth}
  \includegraphics[width=\linewidth]{imgs/revised_figure/small/metis_acc_runtime.pdf}
  \caption{Metis partitions}\label{subfig:test_acc_metis_runtime}
\end{subfigure}\hfil % <-- added
\caption{
Testing accuracy w.r.t. runtime under random and METIS partitions for RecGCN.} \label{fig:convergence_random_runtime}
\end{figure*}

\begin{figure*}[b]
\centering % <-- added
\begin{subfigure}{0.5\linewidth}
  \includegraphics[width=\linewidth]{imgs/revised_figure/batchsize/amazon_batchsize1.pdf}
    \caption{Batch size=1}\label{subfig:batchsize1}
\end{subfigure}\hfil
\begin{subfigure}{0.5\linewidth}
  \includegraphics[width=\linewidth]{imgs/revised_figure/batchsize/amazon_batchsize10.pdf}
  \caption{Batch size=10}\label{subfig:batchsize10}
\end{subfigure}\hfil % <-- added
\caption{
Testing accuracy w.r.t. runtime on the AMAZON dataset for RecGCN under different batch sizes (number of clusters).} \label{fig:bacthsize}
\end{figure*}

\subsection{Experiments with RecGNNs}

\subsubsection{Accuracy vs. Runtime on Small Graphs} \label{sec:acc_runtime_small_rec}

In this section, we report the accuracy vs. runtime on the small datasets in Figure \ref{fig:convergence_random_runtime}.
As SSE does not solve the fixed point equations during training, it converges faster at the start of the training. However, the final prediction performance of SSE does not achieve the state-of-the-art performance due to its inaccurate gradient approximations.
The efficiency of LMC4Rec achieves the-state-of-art performance on the small datasets under random and METIS partitions.




\subsubsection{Prediction Performance}\label{sec:prediction_large_rec}

In this section, we report more experiments on the prediction performance for more baselines including the state-of-the-art RecGNN (SSE \cite{sse} and IGNN \cite{ignn}) and our implemented RecGCNs trained by full-batch gradient descent (GD), Cluster-GCN \cite{deq_gcn}, GraphSAINT \cite{graphsaint}, and GAS.



\begin{table}[ht]
  \centering
  \caption{%
    \textbf{Performance on Large Graphs.}
    OOM denotes the out-of-memory issue.
  }\label{tab:largegraph_appen}
  \setlength{\tabcolsep}{2pt}
  \resizebox{\linewidth}{!}{%
  \begin{tabular}{llcccccc}
    \toprule
    \mc{2}{l}{\footnotesize{\textbf{\#\,nodes}}} & \footnotesize{57K}  & \footnotesize{230K}  & \footnotesize{335K} & \footnotesize{169K} & \footnotesize{2.4M} \\[-0.1cm]
    \mc{2}{l}{\footnotesize{\textbf{\#\,edges}}} & \footnotesize{794K}  &  \footnotesize{11.6M} & \footnotesize{926K} & \footnotesize{1.2M} & \footnotesize{61.9M} \\[-0.05cm]
    \mc{2}{l}{\textbf{Method}} & \textsc{PPI} & \textsc{REDDIT} & \textsc{Amazon}& \texttt{Ogbn-arxiv} & \texttt{Ogbn-products} \\
    \midrule
    &\textsc{SSE}         & 83.60          & ---              & 85.08                  & ---                  & --- \\
    &\textsc{IGNN}        & 97.56          & 94.95            & 85.31              & 70.88              & OOM \\
    \midrule
    & \textsc{GD}   & OOM   & 96.35   & 86.61 & 68.43   & OOM \\
    & \textsc{GraphSAINT}   & 61.14   & 96.60   & 43.53 & 70.93   & OOM \\
    & \textsc{Cluster-GCN}   & 97.37     & 94.49 & 85.69 & 69.23 & 76.32 \\ 
    & \textsc{GAS}   & 98.88   & 96.44 & 88.34 & 71.92   & 77.32 \\ 
    & \textsc{LMC4Rec}   & \textbf{98.99}     & \textbf{96.58} & \textbf{88.84} & \textbf{72.64}   & \textbf{77.73} \\
    \bottomrule
  \end{tabular}
 }
\end{table}



We evaluate these methods on the protein-protein interaction dataset (PPI), the Reddit dataset \cite{graphsage}, the Amazon product co-purchasing network dataset (AMAZON) \cite{amazon}, and open graph benchmarkings (Ogbn-arxiv and Ogbn-products) \cite{ogb} in Table \ref{tab:largegraph_appen}.
GraphSAINT suffers from the out-of-memory issue in the evaluation process on Ogbn-products.
LMC4Rec outperforms GD, GraphSAINT, and Cluster-GCN by a large margin and achieve the comparable prediction performance against GAS.
Moreover, Table 1 in the main text demonstrates that LMC4Rec enjoys much faster convergence than GAS, showing the effectiveness of LMC4Rec.
Therefore, LMC4Rec can accelerate the training of RecGNNs without sacrificing accuracy.

\subsubsection{Accuracy vs. Runtime on large datasets} \label{sec:acc_runtime_large_rec}


As shown in Table 2, LMC4Rec significantly improves the performance when long-range dependencies are important. Therefore, we first report accuracy vs. runtime on the AMAZON dataset to demonstrate the efficiency of LMC4Rec in Figure \ref{fig:bacthsize}. When the batch size (the number of sampled clusters) is equal to 1, the speedup of LMC4Rec is about 2x. On the other large datasets, Figure \ref{fig:large} demonstrates LMC4Rec is comparable to the state-of-the-art subgraph-wise sampling methods.




\begin{figure}[ht]
\centering % <-- added
\begin{subfigure}{0.5\linewidth}
  \includegraphics[width=\linewidth]{imgs/revised_figure/large/reddit.pdf}
  \caption{REDDIT}\label{subfig:reddit}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.5\linewidth}
  \includegraphics[width=\linewidth]{imgs/revised_figure/large/ppi.pdf}
  \caption{PPI}\label{subfig:ppi}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.5\linewidth}
  \includegraphics[width=\linewidth]{imgs/revised_figure/large/arxiv.pdf}
  \caption{Ogbn-arxiv}\label{subfig:arxiv}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.5\linewidth}
  \includegraphics[width=\linewidth]{imgs/revised_figure/large/products.pdf}
  \caption{Ogbn-products}\label{subfig:products}
\end{subfigure}\hfil % <-- added
\caption{
Testing accuracy w.r.t. runtime (s) on Ogbn-arxiv, PPI, REDDIT, and Ogbn-products. The LMC in the figures refers to LMC4Rec.} \label{fig:large}
\end{figure}




\section{Potential Societal Impacts}


In this paper, we propose a novel and efficient subgraph-wise sampling method for the training of GNNs, i.e., LMC.
This work is promising in many practical and important scenarios such as search engine, recommendation systems, biological networks, and molecular property prediction.
Nonetheless, this work may have some potential risks. For example, using this work in search engine and recommendation systems to over-mine the behavior of users may cause undesirable privacy disclosure. 


\end{document}


