%%
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%
\documentclass[3p,,preprint,12pt]{elsarticle}
\makeatletter\if@twocolumn\PassOptionsToPackage{switch}{lineno}\else\fi\makeatother

      \makeatletter
\usepackage{wrapfig}
\newcounter{aubio}
\long\def\bioItem{%
\@ifnextchar[{\@bioItem}{\@@bioItem}}

\long\def\@bioItem[#1]#2#3{
 \stepcounter{aubio}
 \expandafter\gdef\csname authorImage\theaubio\endcsname{#1}
 \expandafter\gdef\csname authorName\theaubio\endcsname{#2}
 \expandafter\gdef\csname authorDetails\theaubio\endcsname{#3}
}

\long\def\@@bioItem#1#2{
 \stepcounter{aubio}
 \expandafter\gdef\csname authorName\theaubio\endcsname{#1}
 \expandafter\gdef\csname authorDetails\theaubio\endcsname{#2}
}

\newcommand{\checkheight}[1]{%
  \par \penalty-100\begingroup%
  \setbox8=\hbox{#1}%
  \setlength{\dimen@}{\ht8}%
  \dimen@ii\pagegoal \advance\dimen@ii-\pagetotal
  \ifdim \dimen@>\dimen@ii
    \break
  \fi\endgroup}

\def\printBio{%
  \@tempcnta=0
   \loop
     \advance \@tempcnta by 1
     \def\aubioCnt{\the\@tempcnta}
     \setlength{\intextsep}{0pt}%
     \setlength{\columnsep}{10pt}%
     \newbox\boxa%
     \setbox\boxa\vbox{\csname authorDetails\aubioCnt\endcsname}
     \expandafter\ifx\csname authorImage\aubioCnt\endcsname\relax%
      \else%
       \checkheight{\includegraphics[height=1.25in,width=1in,keepaspectratio]{\csname authorImage\aubioCnt\endcsname}}
        \begin{wrapfigure}{l}{25mm}
         \includegraphics[height=1.25in,width=1in,keepaspectratio]{\csname authorImage\aubioCnt\endcsname}%height=145pt
        \end{wrapfigure}\par
      \fi
     {\parindent0pt\textbf{\csname authorName\aubioCnt\endcsname}\csname authorDetails\aubioCnt\endcsname \par\bigskip%
     \expandafter\ifx\csname authorImage\aubioCnt\endcsname\relax\else%
      \ifdim\the\ht\boxa < 90pt\vskip\dimexpr(90pt -\the\ht\boxa-1pc)\fi%
     \fi}%for adding additional vskip for avoiding image overlap.
      \ifnum\@tempcnta < \theaubio
   \repeat
   }

\makeatother

      
\usepackage[round,sort]{natbib}
%\usepackage{tabulary,xcolor}
\usepackage{tabulary}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage[T1]{fontenc}
\makeatletter
\let\save@ps@pprintTitle\ps@pprintTitle
\def\ps@pprintTitle{\save@ps@pprintTitle\gdef\@oddfoot{\footnotesize\itshape \null\hfill\today}}
\def\hlinewd#1{%
  \noalign{\ifnum0=`}\fi\hrule \@height #1%
  \futurelet\reserved@a\@xhline}
\def\tbltoprule{\hlinewd{.8pt}\\[-12pt]}
\def\tblbottomrule{\noalign{\vspace*{6pt}}\hline\noalign{\vspace*{2pt}}}
\def\tblmidrule{\noalign{\vspace*{6pt}}\hline\noalign{\vspace*{2pt}}}
%\AtBeginDocument{\NAT@numbersfalse}
\usepackage{natbib}

\newif\ifNAT@numbers \NAT@numbersfalse
%\biboptions{sort&compress}\fi}

%\makeatother

  


\usepackage{ifluatex}
\ifluatex
\usepackage{fontspec}
\defaultfontfeatures{Ligatures=TeX}
\usepackage[]{unicode-math}
\unimathsetup{math-style=TeX}
\else 
\usepackage[utf8]{inputenc}
\fi 
\ifluatex\else\usepackage{stmaryrd}\fi

  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Following additional macros are required to function some 
% functions which are not available in the class used.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{url,multirow,morefloats,floatflt,cancel,tfrupee}
\makeatletter


\AtBeginDocument{\@ifpackageloaded{textcomp}{}{\usepackage{textcomp}}}
\makeatother
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage[nointegrals]{wasysym}
\urlstyle{rm}
\makeatletter

%%%For Table column width calculation.
\def\mcWidth#1{\csname TY@F#1\endcsname+\tabcolsep}

%%Hacking center and right align for table
\def\cAlignHack{\rightskip\@flushglue\leftskip\@flushglue\parindent\z@\parfillskip\z@skip}
\def\rAlignHack{\rightskip\z@skip\leftskip\@flushglue \parindent\z@\parfillskip\z@skip}

%Etal definition in references
\@ifundefined{etal}{\def\etal{\textit{et~al}}}{}


%\if@twocolumn\usepackage{dblfloatfix}\fi
\usepackage{ifxetex}
\ifxetex\else\if@twocolumn\@ifpackageloaded{stfloats}{}{\usepackage{dblfloatfix}}\fi\fi

\AtBeginDocument{
\expandafter\ifx\csname eqalign\endcsname\relax
\def\eqalign#1{\null\vcenter{\def\\{\cr}\openup\jot\m@th
  \ialign{\strut$\displaystyle{##}$\hfil&$\displaystyle{{}##}$\hfil
      \crcr#1\crcr}}\,}
\fi
}

%For fixing hardfail when unicode letters appear inside table with endfloat
\AtBeginDocument{%
  \@ifpackageloaded{endfloat}%
   {\renewcommand\efloat@iwrite[1]{\immediate\expandafter\protected@write\csname efloat@post#1\endcsname{}}}{\newif\ifefloat@tables}%
}%

\def\BreakURLText#1{\@tfor\brk@tempa:=#1\do{\brk@tempa\hskip0pt}}
\let\lt=<
\let\gt=>
\def\processVert{\ifmmode|\else\textbar\fi}
\let\processvert\processVert

\@ifundefined{subparagraph}{
\def\subparagraph{\@startsection{paragraph}{5}{2\parindent}{0ex plus 0.1ex minus 0.1ex}%
{0ex}{\normalfont\small\itshape}}%
}{}

% These are now gobbled, so won't appear in the PDF.
\newcommand\role[1]{\unskip}
\newcommand\aucollab[1]{\unskip}
  
\@ifundefined{tsGraphicsScaleX}{\gdef\tsGraphicsScaleX{1}}{}
\@ifundefined{tsGraphicsScaleY}{\gdef\tsGraphicsScaleY{.9}}{}
% To automatically resize figures to fit inside the text area
\def\checkGraphicsWidth{\ifdim\Gin@nat@width>\linewidth
	\tsGraphicsScaleX\linewidth\else\Gin@nat@width\fi}

\def\checkGraphicsHeight{\ifdim\Gin@nat@height>.9\textheight
	\tsGraphicsScaleY\textheight\else\Gin@nat@height\fi}

\def\fixFloatSize#1{}%\@ifundefined{processdelayedfloats}{\setbox0=\hbox{\includegraphics{#1}}\ifnum\wd0<\columnwidth\relax\renewenvironment{figure*}{\begin{figure}}{\end{figure}}\fi}{}}
\let\ts@includegraphics\includegraphics

\def\inlinegraphic[#1]#2{{\edef\@tempa{#1}\edef\baseline@shift{\ifx\@tempa\@empty0\else#1\fi}\edef\tempZ{\the\numexpr(\numexpr(\baseline@shift*\f@size/100))}\protect\raisebox{\tempZ pt}{\ts@includegraphics{#2}}}}


\newif\ifmultipleabstract\multipleabstractfalse%
\newenvironment{typesetAbstractGroup}{}{}%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\emergencystretch 20pt \tolerance = 1500 \def\floatpagefraction{0.8}




    \makeatletter
\def\ead{\@ifnextchar[{\@uad}{\@ead}}
\gdef\@ead#1{\bgroup
   \def\_{\string\underscorechar\space}
   \def\{{\string\lbracechar\space}
   \def\textdagger{\string\textdagger\space}
   \def\texttildeapprox{\string\texttildeapprox\space}
   \def~{\hashchar\space}
   \def\}{\string\rbracechar\space}
   \edef\tmp{\the\@eadauthor}
   \immediate\write\@auxout{\string\emailauthor
     {#1}{\expandafter\strip@prefix\meaning\tmp}}
  \egroup
}
%\newcounter{ead}
\gdef\emailauthor#1#2{\stepcounter{ead}
      \g@addto@macro\@elseads{\raggedright
      \let\corref\@gobble
      \eadsep\texttt{#1} (#2)
      \def\eadsep{\unskip,\space}}
}

\makeatother
  
\usepackage{float}

\begin{document}


\begin{frontmatter}

    \title{
  \textbf{A Novel Patent Similarity Measurement Methodology} \textbf{: Semantic Distance and Technological Distance}    
}
    
\author[affc30f26b6787f42c8bcaa58b9c25632ec]{Yongmin Yoo}
\ead{yooyongmin91@gmail.com}
\author[aff7978416db64a4d3c91456a9ffa0aabea]{Cheonkam Jeong}
\ead{cheonkamjeong@arizona.edu}
\author[aff078a6c89fc564e9b9f8cc3bd09e89273]{Sanguk Gim}
\ead{worj2uk@gmail.com}
\author[aff6cca0bdaa3894c26b96f05653ec716fa]{Junwon Lee}
\ead{asdl0320@gmail.com}
\author[aa09066fcc7f8]{Zachary Schimke}
\ead{zackschimke@arizona.edu}
\author[abe732a782131]{Deaho Seo\corref{contrib-fdacfacabdee4c0ea555f879faeba1ec}}
\ead{seo\_daeho@naver.com}\cortext[contrib-fdacfacabdee4c0ea555f879faeba1ec]{Corresponding author.}
    
\address[affc30f26b6787f42c8bcaa58b9c25632ec]{Department of Computer Science\unskip, 
    University of Auckland\unskip, Auckland\unskip, New Zealand}
  	
\address[aff7978416db64a4d3c91456a9ffa0aabea]{Department of Linguistics\unskip, 
    University of Arizona\unskip, Arizona\unskip, Tucson\unskip, United States}
  	
\address[aff078a6c89fc564e9b9f8cc3bd09e89273]{
    SR Universe\unskip, Seoul\unskip, Republic of Korea}
  	
\address[aff6cca0bdaa3894c26b96f05653ec716fa]{
    Royal Melbourne Institute of Technology\unskip, Melbourne\unskip, Australia}
  	
\address[aa09066fcc7f8]{James E. Rogers College of Law\unskip, 
    University of Arizona\unskip, Tucson\unskip, Arizona\unskip, United States}
  	
\address[abe732a782131]{School of Information\unskip, 
    Yonsei University\unskip, Seoul\unskip, Republic of Korea}
  

\begin{abstract}
Measuring similarity between patents is an essential step to ensure novelty of innovation. However, a large number of methods of measuring the similarity between patents still rely on manual classification of patents by experts. Another body of research has proposed automated methods; nevertheless, most of it solely focuses on the semantic similarity of patents. In order to tackle these limitations, we propose a hybrid method for automatically measuring the similarity between patents, considering both semantic and technological similarities. We measure the semantic similarity based on patent texts using BERT, calculate the technological similarity with IPC codes using \textit{Jaccard }similarity, and perform hybridization by assigning weights to the two similarity methods. Our evaluation result demonstrates that the proposed method outperforms the baseline that considers the semantic similarity only.
\end{abstract}
      \begin{keyword}
    Patent Similarity\sep Semantic Distance\sep Technological Distance\sep Technological Management\sep Natural Language Processing
      \end{keyword}
    
  \end{frontmatter}
    
\section{Introduction}
Since the advent of the 4th revolution, technology has developed rapidly. Many new innovations and strategies have emerged every day. In this fast stream, a need to measure novelty of them in a quick but accurate manner is arising. Measuring similarity between patents has been used as an effective way to gauge novelty, and accordingly many methods to measure the patent similarity have been proposed, for instance \cite{arts2018text}. However, none of the previous methods fulfills both reliability and immediacy. Thus, we propose a novel method to measure the patent similarity.

A patent document consists of a title, an abstract, a description, and claims, along with administrative parts of the patent, such as the patent number, inventor(s), IPC codes, CPC codes, and filing date. As a patent document is full of technical terminologies, or not jargon-free, it is difficult for laypersons to understand it properly. Furthermore, as a patent document provides every detail of the patent, it is challenging to grasp the gist of it quickly. This may impede innovation processes in that it is costly to figure out whether what one would like to invent can be granted, especially in a rapidly changing world. In order to ease the processes, a large body of research has proposed measurements for the patent similarity.

There are several sources with which the patent similarity can be measured: Similarity of purposes of patents, that of application areas of patents, textual similarity of patents, bibliographic similarity, etc. \cite{moehrle2010measures}. In their seminal work, Arts et al. (2018) proposed a keyword-based method using texts of patents to measure the patent similarity \cite{arts2018text}. On the one hand, Rodriguez et al. (2015) devised a similarity measurement method by utilizing direct and indirect co-citation links between patents \cite{rodriguez2015new}. In a similar vein, Lai and Wu (2005) built a patent classification system based on a co-citation analysis of bibliometrics \cite{lai2005using}. Not only with patent texts and bibliography, linguistic structures have been also considered to be another source for the patent similarity measurement; for example, Wang et al. (2019) presented a similarity method based on semantic information of patent texts \cite{wang2019measuring}.

Another body of research focuses more on automating the measurement process by utilizing machine learning and/or deep learning, in particular Natural Language Processing (NLP), algorithms. Younge et al. (2016) built a single vector space model that measures the patent similarity with the full technical description in the patent document \cite{younge2016patent}. Lu et al. (2020) adopted a deep learning-based approach and demonstrated that their model outperforms previous text representation and classification models \cite{lu2020research}. Several works proposed neural network-based models, for instance Convolutional Neural Networks (CNN)-based models \cite{he2015multi,peng2020enhanced}. Still others adopted transformer-based approaches; for example, Lee and Hsiang (2019) applied Bidirectional Encoder Representations from Transformers (BERT; \cite{devlin2018bert}) to patent texts \cite{lee2019patentbert}.

It is true that all the previous models have their own benefits. However, none of them satisfies both reliability and immediacy, which are of primary importance in today's tech-savvy world. They mostly focused on semantic information of patent tests either manually or automatically. In order to tackle these issues, we propose a hybrid method to measure the patent similarity. To be specific, we consider both semantic and technological aspects of patents to enhance reliability, and build an automatic method to measure the patent similarity quickly. In order to achieve these, we measure semantic similarity between patents based on titles and abstracts, which we call \textit{Semantic Distance} (SD), and technological similarity between them based on IPC codes, which we call \textit{Technological} \textit{Distance} (TD). In building a model, we consider utilizing the state-of-the-art NLP techniques for similarity measurements \cite{gonzalez2020comparing,garg2020bae,yoo2021novel} .

The contributions of this research are in the respects of brevity, comprehensiveness, and versatility as follows.

\begin{itemize}
  \item \relax Brevity: The proposed model is simpler than the previous models in terms of model complexity, but with higher-level of performance, as we use minimal but essential information.
  \item \relax Comprehensiveness: Recently issued patents are much more complex to analyze than those in the past in that they encompass various sub-technologies. This tendency necessitates a comprehensive model to measure the patent similarity. The proposed model is more comprehensive than the previous models as it incorporates both semantic and technological similarities. 
  \item \relax Versatility: The proposed model is effective to gauge novelty, but it can be used for other purposes, such as trend analysis \cite{choi2014patent}. 
\end{itemize}
  This research is composed as follows. Section 2 provides literature review. Section 3 explicates the proposed model with the main components of it in a detailed manner. Section 4 describes the experimental results. Section 5 provides discussion, and Section 6 draws a conclusion. 
    
\section{Literature research}
The topics into which previous research delved are mainly in relation to the patent similarity methods and document classification of the patent. They are explicated in this section.

\subsection{Patent similarity} Previously, Moehlre et al. (2010) posited two levels of patent similarity types with respect to the content and form. According to them, the content-level similarity can be captured based on structural similarity of patents, such as purposes of the inventions, application areas of them, etc. while the form-level similarity based on textual similarity of patents, such as similarity between patent texts, bibliographic similarity, etc. \cite{moehrle2010measures}. Among the possible sources for patent similarity measurements, Arts et al. (2018) devised a keyword-based method using patent abstracts to measure the textual similarity of patents \cite{arts2018text}. They extracted keywords from patent abstracts, established a \textit{Jaccard}-based similarity method, and verified validation by an expert panel. A body of research proposed similarity measurement methods by utilizing bibliographic information. Lai and Wu (2005) established a patent classification system based on a co-citation analysis of bibliometrics \cite{lai2005using}. Similarly, Rodriguez et al. (2015) measured pairwise patent similarity by using both direct and indirect co-citation links between patents \cite{rodriguez2015new}. Recent studies have proposed similarity measurement methods considering linguistic nature of patent texts, specifically Subject-Action-Object structures \cite{wang2019measuring}, both semantic and syntactic structures \cite{yang2021measuring}, and various word senses \cite{ahmad2022novel}.

On the other hand, a large body of research has focused more on methodological aspects by adopting machine learning and deep learning, in particular NLP, techniques. Cascini and Zini (2008) devised a clustering algorithm that estimates the patent similarity by considering hierarchical and functional interactions between patents \cite{cascini2008measuring}. Vector space models were also applied to the patent analysis. Younge et al.(2016) developed a single vector space-based model that measures continuous similarity distance between patent pairs in an automatic manner \cite{younge2016patent}. In a similar vein, Feng (2020) devised a similarity measurement method via vector space representations of patent abstract using Document Vectors (Doc2Vec) \cite{feng2020proximity}. In addition to these machine learning-based techniques, NLP-based ones were applied to the patent analysis. Noh and Lee (2015) applied text mining to patent analysis through keyword selection and processing strategies \cite{noh2015keyword}; similarly, Joung and Kim (2017) adopted a keyword-based approach for technology planning \cite{joung2017monitoring}. Recently, BERT-based methods have been applied to the patent analysis, for example \cite{lee2019patentbert}. 



\subsection{Document similarity}Since patents are documents in natural languages, we introduce previous research on how to measure the pairwise document similarity in natural language. Kadhim (2019) explained a canonical workflow for the text classification by providing examples of supervised machine learning models, such as K-nearest neighbors \cite{kadhim2019survey}. More modern research has focused on improving the model performance. For instance, Farouk (2020) devised a comprehensive method to measure the sentence similarity that combines both structure and word-based similarities and exploits sentence semantic structure, specifically discourse representation structure (DRS) \cite{farouk2020measuring}.  Similarly, Lan (2022) proposed a hybrid method that incorporates the term similarity weighted tree data structure and the semantic similarity information of HowNet \cite{dong2003hownet} in order to improve low accuracy of the text similarity caused by TF-IDF (term frequency-inverse document frequency) and tackle the curse of dimensionality issue \cite{lan2022research}. Transformer-based models also have been applied to the text classification. Gonz\'{a}lez-Carvajal and Garrido-Merch\'{a}n (2020) demonstrated superiority of BERT to the traditional TF-IDF embedding method on text classification with machine learning-based models \cite{gonzalez2020comparing}. Furthermore, Garg and Ramakrishman (2020) devised a BERT-based method to generate adversarial examples for text classification to make text classification models more robust \cite{garg2020bae}. In addition, Viji et al. (2022) implemented a Bidirectional Long Short-Term Memory (Bi-LSTM) model with weighted word embeddings through BERT fine-tuning, and corroborated the Bi-LSTM model with BERT in NLP application tasks, such as measuring text similarity in questions or pairs of documents \cite{viji2022hybrid}. 
    
\section{Methodology}
Our method considers two aspects. First, we calculate the Semantic Distance, which compares the contents of the two patents. Second, we calculate the Technical Distance, which considers the technological aspects of the two patents. Our model properly hybridizes and weights the two aspects. 

\subsection{Semantic distance}
\bgroup
\fixFloatSize{imgs/model1.png}
\begin{figure*}[!htbp]
\centering \makeatletter\IfFileExists{imgs/model1.png}{\includegraphics{imgs/model1.png}}{}
\makeatother 
\caption{{Model architecture for the semantic distance measurement}}
\label{f-b2059ef534e3}
\end{figure*}
\egroup
We measure the semantic similarity between two patents using a BERT model, one of the top performing models in a large number of natural language processing tasks. Among multiple variants of the BERT models, Sentence-BERT \cite{reimers2019sentence} is adopted to calculate the semantic distance between them. First, we feed the whole abstracts with titles to BERT, by which a set of output vectors per patent is derived, as illustrated in Figure~\ref{f-b2059ef534e3}. Second, we calculate the mean of each set of output vectors using Equation~(\ref{dfg-822c2128e7b8}). Then, we calculate the cosine similarity of the two mean vectors (i.e., \textit{vector\ensuremath{_{A}}}, \textit{vector\ensuremath{_{B}}}) through Equation~(\ref{dfg-66ca3a117ff3})
\let\saveeqnno\theequation
\let\savefrac\frac
\def\dispfrac{\displaystyle\savefrac}
\begin{eqnarray}
\let\frac\dispfrac
\gdef\theequation{1}
\let\theHequation\theequation
\label{dfg-822c2128e7b8}
\begin{array}{@{}l}\style{font-size:12px}{\begin{array}{l}vector_A=\frac{\sum_{i=1}^{n}{\left(Patent_A\right)}_i}n\\vector_B=\frac{\sum_{i=1}^{n}{\left(Patent_B\right)}_i}n\end{array}}\end{array}
\end{eqnarray}
\global\let\theequation\saveeqnno
\addtocounter{equation}{-1}\ignorespaces 

\let\saveeqnno\theequation
\let\savefrac\frac
\def\dispfrac{\displaystyle\savefrac}
\begin{eqnarray}
\let\frac\dispfrac
\gdef\theequation{2}
\let\theHequation\theequation
\label{dfg-66ca3a117ff3}
\begin{array}{@{}l}\style{font-size:12px}{\cos_{A,B}\;=\;\frac{A\cdot B}{\left\vert A\right\vert\left\vert B\right\vert}=\frac{\sum_{i=1}^{n}A_i\times B_i}{\sqrt{\sum_{i=1}^{n}(A_i)^{2}}\times\sqrt{\sum_{i=1}^{n}(B_i)^{2}}}}\end{array}
\end{eqnarray}
\global\let\theequation\saveeqnno
\addtocounter{equation}{-1}\ignorespaces 
Finally, the semantic distance \textit{SD\ensuremath{_{A,B}}} is derived by normalization of the cosine similarity value using Equation~(\ref{disp-formula-group-f151122c7bb448869ffdebad9006bbd8})
\let\saveeqnno\theequation
\let\savefrac\frac
\def\dispfrac{\displaystyle\savefrac}
\begin{eqnarray}
\let\frac\dispfrac
\gdef\theequation{3}
\let\theHequation\theequation
\label{disp-formula-group-f151122c7bb448869ffdebad9006bbd8}
\begin{array}{@{}l}\style{font-size:14px}{SD_{A,B}=\frac{\left(cos_{A,B}+1\right)}2}\end{array}
\end{eqnarray}
\global\let\theequation\saveeqnno
\addtocounter{equation}{-1}\ignorespaces 

\subsection{Technological distance}
\bgroup
\fixFloatSize{imgs/model2.png}
\begin{figure*}[!htbp]
\centering \makeatletter\IfFileExists{imgs/model2.png}{\includegraphics{imgs/model2.png}}{}
\makeatother 
\caption{{Model architecture for the technological distance measurement}}
\label{f-608b9df0c687}
\end{figure*}
\egroup
We measure the technological similarity of patents with IPC codes in patent documents. To be specific, we measure to what extent two patents are similar in terms of technology by calculating the overlapping portion of the IPC codes of them. IPC codes encode section, class, sub-class, main group, and sub-group hierarchically. Since it is rare for IPC codes to match perfectly, we measure the technological distance \textit{TD} based on the three-depth structure, consisting of section, class, and sub-class.

First, we calculate how many techniques are overlapped between patents using Equation~(\ref{disp-formula-group-522a9f6506e540329b460611414fa795}) and obtain the total number of techniques between them using Equation~(\ref{disp-formula-group-dde4234332da418fb7c7ebd13c99f0de}). We then calculate a \textit{Jaccard} similarity with \textit{Intersection\ensuremath{_{A,B}}} and \textit{Union\ensuremath{_{A,B}}}as illustrated inEquation~(\ref{disp-formula-group-67cbdf72dbe54cd7b2a57e6485c44967}), which is defined as \textit{TD\ensuremath{_{A,B}}}. The workflow of calculating \textit{TD} is provided in Figure~\ref{f-608b9df0c687}.
\let\saveeqnno\theequation
\let\savefrac\frac
\def\dispfrac{\displaystyle\savefrac}
\begin{eqnarray}
\let\frac\dispfrac
\gdef\theequation{4}
\let\theHequation\theequation
\label{disp-formula-group-522a9f6506e540329b460611414fa795}
\begin{array}{@{}l}\style{font-size:12px}{Intersection{}_{A,B}=Patent_A\cap Patent_B}\end{array}
\end{eqnarray}
\global\let\theequation\saveeqnno
\addtocounter{equation}{-1}\ignorespaces 

\let\saveeqnno\theequation
\let\savefrac\frac
\def\dispfrac{\displaystyle\savefrac}
\begin{eqnarray}
\let\frac\dispfrac
\gdef\theequation{5}
\let\theHequation\theequation
\label{disp-formula-group-dde4234332da418fb7c7ebd13c99f0de}
\begin{array}{@{}l}\style{font-size:12px}{Union_{A,B}=Patent_A\cup Patent_B}\end{array}
\end{eqnarray}
\global\let\theequation\saveeqnno
\addtocounter{equation}{-1}\ignorespaces 

\let\saveeqnno\theequation
\let\savefrac\frac
\def\dispfrac{\displaystyle\savefrac}
\begin{eqnarray}
\let\frac\dispfrac
\gdef\theequation{6}
\let\theHequation\theequation
\label{disp-formula-group-67cbdf72dbe54cd7b2a57e6485c44967}
\begin{array}{@{}l}\style{font-size:12px}{TD_{A,B}=\frac{Intersection_{A,B}}{Union_{A,B}}}\end{array}
\end{eqnarray}
\global\let\theequation\saveeqnno
\addtocounter{equation}{-1}\ignorespaces 


\subsection{Hybrid similarity}We propose a hybrid similarity method, \textit{SDTD}, using both \textit{SD} and \textit{TD} defined in 3.2. To be specific, we give \textit{TD} weight to \textit{SD} and then multiply (\textit{TD} + 1) to \textit{SD}. Thereafter, we normalize the multiplied value by dividing it by 2, in order to prevent \textit{TD} value from being too large and \textit{SDTD} value from exceeding 1, as illustrated in Equation~(\ref{disp-formula-group-f758c954fd9f472b8f6a701734878dc6}).
\let\saveeqnno\theequation
\let\savefrac\frac
\def\dispfrac{\displaystyle\savefrac}
\begin{eqnarray}
\let\frac\dispfrac
\gdef\theequation{7}
\let\theHequation\theequation
\label{disp-formula-group-f758c954fd9f472b8f6a701734878dc6}
\begin{array}{@{}l}\style{font-size:12px}{SDTD_{A,B}=\{\left(TD+1\right)\ast SD\}/2}\end{array}
\end{eqnarray}
\global\let\theequation\saveeqnno
\addtocounter{equation}{-1}\ignorespaces 

    
\section{Experiment}
In this section, we provide a data evaluation method and experiment results. In training the models, we use Python version 3.10.9 and Pytorch version 1.13.1 from Seoul, Korea. The computer specifications used in the experiment are as follows: 4 GeForce RTX 2080Ti and 16 Intel(R) Core(TM) i7-9800X CPU @ 3.80GHz.

\subsection{Data}Google patent is a platform where patents from 2016 to 2020 granted by United States Patent and Trademark Office \cite{giczy2022identifying} are provided. Using Google patent, we randomly extract 420 pairs of patents from 2019 to 2020, with which we perform expert validation.

The expert panel consists of three scientists, each of whom has expertise in Data Analytics, Data Mining, and Artificial Intelligence, respectively. Before asking them to rate the patents, we set a guideline for rating, following advice by the author Zachery Schimke, a Juris Doctor candidate, who specializes in Patent at the University of Arizona. To be specific, we adopt a seven-scale rating scheme; it consists of the five odd numbers between 0 and 10, to which 0 and 10 are also added as exceptional cases. Then, we establish five criteria: Area, Task or Purpose, Methodology, and Application. Depending on the number of the overlapping criteria, rating scores are given, as described inTable~\ref{tw-80f871cf32bf}. 


\begin{table*}[!htbp]
\caption{{Criteria for patent rating}}
\label{tw-80f871cf32bf}
\def\arraystretch{1}
\ignorespaces 
\centering 
\begin{tabulary}{\linewidth}{p{\dimexpr.16199999999999996\linewidth-2\tabcolsep}p{\dimexpr.838\linewidth-2\tabcolsep}}
\tbltoprule \cAlignHack \textbf{Score} & \textbf{Criterion}\\
\tblmidrule 
\rAlignHack 0 &
  Totally different\\
\rAlignHack 1 &
  1 similar property\\
\rAlignHack 3 &
  2 similar properties\\
\rAlignHack 5 &
  3 similar properties\\
\rAlignHack 7 &
  3 similar properties \& 1 somewhat similar property\\
\rAlignHack 9 &
  4 similar properties, but not the same\\
\rAlignHack 10 &
  Exactly same\\
\tblbottomrule 
\end{tabulary}\par 
\end{table*}
Following the guideline, each rater assesses how semantically similar two patents are solely based on patent abstracts with titles. In order to ensure reliability, we winnow out the data based on a variance analysis and then ask the author Zachery Schimkem, a law expert, to rate them. As described inTable~\ref{tw-ea4d982cc503}, we calculate \textit{Distance} and then set \textit{Threshold} as 8. If \textit{Distance} is larger than \textit{Threshold}, we separately store the data for \textit{Law expert rating}; otherwise, we calculate the mean score.

\begin{table*}[!htbp]
\caption{{Data rating flow} }
\label{tw-ea4d982cc503}
\def\arraystretch{1}
\ignorespaces 
\centering 
\begin{tabulary}{\linewidth}{L}
\tbltoprule 
$\style{font-size:14px}{\begin{array}{l}\boldsymbol L\boldsymbol a\boldsymbol w\boldsymbol\;\boldsymbol e\boldsymbol x\boldsymbol p\boldsymbol e\boldsymbol r\boldsymbol t\boldsymbol\;\boldsymbol r\boldsymbol a\boldsymbol t\boldsymbol i\boldsymbol n\boldsymbol g\boldsymbol:\\Asking\;a\;law\;expert\;to\;rate\;patents\\if\;scores\;rated\;by\;the\;expert\;panel\\have\;huge\;discrepancies\\\\\begin{array}{l}\mu\;=\;(r_1+r_2+r_3)/3\\Distance\;=\;(\mu-r_1)^{2}+(\mu-r_2)^{2}+(\mu-r_3)^{2}\\Threshold\;=\;8\\\boldsymbol i\boldsymbol f\;Distance\;\geq\;Threshold:\\\;\;\;\;return\;Law\;expert\;rating\\\boldsymbol e\boldsymbol l\boldsymbol s\boldsymbol e\boldsymbol:\\\;\;\;\;Score\;=\;(\;r_1+r_2+r_3)/3\end{array}\end{array}} $\\
\tblbottomrule 
\end{tabulary}\par 
\end{table*}
Out of 420 pairs of the patents, 85 pairs turn out to be the cases for Law expert rating. After obtaining the 85 pairs rated by the law expert, we combine them with the remaining data, totaling 420 pairs. The mean score of the data is 1.844 with the standard deviation of 1.882.

For the analysis of the technical similarity, we utilize IPC codes in the patent documents. As described in 3.2., we adopt the three-depth structure, meaning that we use the code up to sub-class. For instance, if a patent consists of G06F40/30, G06F40/40, and G06F40/56 codes, we regard the patent as having one IPC code, rather than three. Accordingly, we preprocess the codes. The total number of patents is 840, ranging from 1 to 6. The mean number of IPC codes per patent is 1.602 with a standard deviation of 0.812, as illustrated in Figure~\ref{f-85d554bde98c}.  
\bgroup
\fixFloatSize{imgs/whole_ipc_code_frequency.png}
\begin{figure*}[!htbp]
\centering \makeatletter\IfFileExists{imgs/whole_ipc_code_frequency.png}{\includegraphics[width=.50\linewidth]{imgs/whole_ipc_code_frequency.png}}{}
\makeatother 
\caption{{Frequency of patents depending on the number of IPC codes per patent}}
\label{f-85d554bde98c}
\end{figure*}
\egroup

\subsection{Evaluation metrics}We use Pearson's correlation coefficient and Spearman's one as indicators to evaluate the experimental results. Pearson's correlation coefficient is a measure of the linear correlation between two variables. Pearson's correlation coefficient is calculated byEquation~(\ref{disp-formula-group-95d814cb65444d07bdfd419a3ab9e897}). 
\let\saveeqnno\theequation
\let\savefrac\frac
\def\dispfrac{\displaystyle\savefrac}
\begin{eqnarray}
\let\frac\dispfrac
\gdef\theequation{8}
\let\theHequation\theequation
\label{disp-formula-group-95d814cb65444d07bdfd419a3ab9e897}
\begin{array}{@{}l}\style{font-size:12px}{r=\frac{n\left(\sum_{}^{}xy\right)-\left(\sum_{}^{}x\right)\left(\sum_{}^{}y\right)}{\sqrt{\lbrack n\sum_{}^{}x^{2}-(\sum_{}^{}x)^{2}\rbrack\ast\lbrack n\sum_{}^{}y^{2}-(\sum_{}^{}y){}^{2}\rbrack\;}}}\end{array}
\end{eqnarray}
\global\let\theequation\saveeqnno
\addtocounter{equation}{-1}\ignorespaces 
On the other hand, the Spearman's correlation coefficient is a method of calculating the correlation coefficient using the rank of two values \noindent \noindent rather than the actual value to calculate the correlation coefficient. Spearman's correlation coefficient is obtained byEquation~(\ref{disp-formula-group-13a49fd0ac9f4ffabab334abc3a02dab}).
\let\saveeqnno\theequation
\let\savefrac\frac
\def\dispfrac{\displaystyle\savefrac}
\begin{eqnarray}
\let\frac\dispfrac
\gdef\theequation{9}
\let\theHequation\theequation
\label{disp-formula-group-13a49fd0ac9f4ffabab334abc3a02dab}
\begin{array}{@{}l}\style{font-size:12px}{r=1-\frac{6\sum_{}^{}D^{2}}{n(n-1)}}\end{array}
\end{eqnarray}
\global\let\theequation\saveeqnno
\addtocounter{equation}{-1}\ignorespaces 

\subsection{Result}In this section, we report the experiment results. As illustrated in Table~\ref{tw-67b22344da75}, the proposed model shows the best performance among the five models based on the Pearson's and Spearman's correlation coefficients, followed by the BERT model. This verifies that the proposed model considering both semantic and technological similarities outperforms any other models that incorporate the semantic similarity only.
\begin{table*}[!htbp]
\caption{{Model performance}}
\label{tw-67b22344da75}
\def\arraystretch{1}
\ignorespaces 
\centering 
\begin{tabulary}{\linewidth}{p{\dimexpr.4178\linewidth-2\tabcolsep}p{\dimexpr.27139999999999997\linewidth-2\tabcolsep}p{\dimexpr.3108\linewidth-2\tabcolsep}}
\tbltoprule 
\cellcolor[HTML]{E5E5E5}{\textbf{Model}} &
  \cellcolor[HTML]{E5E5E5}{\textbf{Pearson}} &
  \cellcolor[HTML]{E5E5E5}{\textbf{Spearman}}\\
CNN &
  0.074951 &
  0.035229\\
LSTM &
  0.05409 &
  0.05967\\
Bi-LSTM &
  0.14646 &
  0.185534\\
BERT &
  0.507742 &
  0.542219\\
\textbf{Proposed model} &
  0.56935 &
  0.644496\\
\tblbottomrule 
\end{tabulary}\par 
\end{table*}

    
\section{Discussion}
We propose a hybrid method to measure the patent similarity in terms of both semantic and technical aspects. The experiment result demonstrates that the proposed model demonstrates the best performance among the five models in Table~\ref{tw-67b22344da75}.  This champions the necessity of incorporating both semantic and technological similarities in measuring the patent similarity. In this section, we discuss applications of the proposed model and possible future works.

The proposed method can be an effective starting-off tools for invention, especially at the beginning stage of it, by solving a big list of similar problems. By the virtue of this property, recommendation systems for patents can be built. In addition, many applied studies can be conducted using the proposed method; for example, it can respond to patent trolls that have recently become a hot topic. Moreover, plagiarism between patents can be semantically and technically analyzed through the proposed similarity measurement. It is also possible to identify technology trends by drawing a patent road map based on similar patents. 

Although the proposed model is the best model among the five models, ways to improve the model performance merit discussion. With regard to the semantic similarity, we apply a vanilla BERT to patent abstracts with titles based on the length of patent abstracts. Still, we can improve the model performance by using variants of BERT. For instance, we can apply D2SBERT \cite{yoo2023multi} to longer documents that may contain more information. Moreover, we can use a more lightweight BERT model, like DistilBERT \cite{sanh2019distilbert}, for distribution. Regarding the technical similarity, we can improve the model performance by incorporating CPC codes, as well as IPC codes, and/or the bibliographic information of the patent. All of these are left for future work.
    
\section{Conclusion}
Measuring the patent similarity is a crucial step for innovation. In a rapidly changing world, it should be performed in an accurate, but fast manner. In order to improve reliability and immediacy, we devise a hybrid method to measure the patent similarity considering both semantic and technological similarities between patents. For this, we establish a BERT-based model that measures both semantic similarity with patent abstracts including titles and technological similarity with IPC codes in the patent documents. The experiment result verifies the effectiveness of the proposed method. The proposed model features sublimity in that it satisfies brevity, comprehensiveness, and versatility, but this can be even better by conducting the possible future works discussed.
    
\section*{Declaration of competing interest}
\noindent The authors declare no conflict of interest.
    
\section*{Funding}
\noindent This work was supported in part by the Business Agency, South Korea under Grant CY220049. 
    
\section*{Acknowledgements}
\noindent The authors acknowledge the helpful comments and suggestions by the anonymous reviewers and the guest editor.
\\
\noindent Author Contributions are as follows:
\noindent Conceptualization, Y.Y.; methodology, Y.Y; validation, S.G., C.J., Y.Y.; formal analysis, S.G., Y.Y; dataset, C.J., Z.S.; Experiment, Y.Y, S.G; writing{\textemdash}original draft preparation, Y.Y., J.L; writing{\textemdash}review and editing, C.J., Y.Y; visualization, S.G., Y.Y; supervision, Y.Y, D.S; All authors have read and agreed to the published version of the manuscript.

%elsarticle-num
\bibliographystyle{plainnat}

\bibliography{refs.bib}

\section*{Author biography}\noindent

\bioItem[imgs/1.jpg]{Yongmin Yoo}{ was born in 1991 in Busan, Republic of Korea. He received a master's degree in Industrial Engineering from Inha university, Korea. He is currently a PhD candidate in University of Auckland, New Zealand. He has experience working as a Natural Language Processing researcher at NHN, one of the biggest companies in Korea. His research interests are in Technological Management, Data Mining, especially Text Mining, Deep Learning, Machine Learning and Natural Language Processing. 

}

\smallskip\noindent 

\bioItem[imgs/2.jpg]{Cheonkam Jeong}{ was born in 1990 in Seoul, Republic of Korea. She earned both B.A. and M.A. degrees in English Linguistics from Hankuk (Korea) University of Foreign Studies in Seoul. She is currently a PhD candidate in Linguistics at the University of Arizona in the United States. She also holds a M.S. degree in Human Language Technology, which was earned en route to her PhD. She has experience working as a Natural Language Processing Engineer and Speech Specialist for several projects, one of which is sponsored by the U.S. Defense Advanced Research Projects Agency. Her research interests lie in Natural Language Processing and Automatic Speech Recognition/Synthesis.}

\smallskip\noindent 

\bioItem[imgs/3.jpg]{Sanguk Gim}{ was born in 1991 in Changwon, Republic of Korea. He received a bachelor's degree in Psychology from Kyungnam University. He worked as a Data Analyst at NHN Diquest. He currently works at SR Universe as a Deep Learning Engineer. 

 His research interests are in deep learning, machine learning and natural language processing.

}

\smallskip\noindent 

\bioItem[imgs/4.jpg]{Junwon Lee}{ was born in 1988 in Gangwon, Republic of Korea. He received a bachelor's degree in Business Administration from Kookmin University. He is currently studying for a master's degree in Data Science at Royal Melbourne Institute of Technology, Melbourne, Australia. He has experience working as a Field Incident Statistical Data Analyst at Group Renault, one of the biggest vehicle companies. His research interests include Data Mining, more specifically Text mining.}

\smallskip\noindent 

\bioItem[imgs/5.jpg]{Zachary Schimke}{ was born in California in the United States.  He earned his B.S. in Mechanical Engineering from California State University Northridge. He is currently a J.D. candidate at the University of Arizona James E. Rogers College of Law. He has been a member of the School's Mock Trial and Patent Moot court teams. He is interested in Intellectual Property as well as Patent Prosecution and Litigation.}

\smallskip\noindent 

\bioItem[imgs/6.jpg]{Deaho Seo}{ was born in 1991 in Seoul, Republic of Korea. He received a B.S degree in Information Systems and a M.S degree in Industrial Engineering from Hanyang University. He is a Ph.D candidate in School of Information, at Yonsei University. He has experience working as a Big Data rSsearcher at the Korea Advanced Institute of Science and Technology and the Korea Electronics Technology Institute. Currently, he is serving as the CEO of Elesther. His research interests include Text Mining, Vision-based Anomaly Detection, Start Factory, and E-commerce solution. He received the Best Young Entrepreneur Award from the Ministry of SMEs and Startups in Korea. He has published 8 books related to artificial intelligence and big data.
}
\printBio 

\end{document}
