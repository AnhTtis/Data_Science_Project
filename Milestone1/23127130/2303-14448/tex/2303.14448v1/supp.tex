\documentclass[12pt]{article}
\usepackage[a4paper, margin=1in]{geometry}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\usepackage{amssymb}
\usepackage{amsmath}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{float}
\usepackage{graphicx}
\usepackage{biblatex} %Imports biblatex package
\addbibresource{biblib.bib} %Import the bibliography 
\begin{document}


\title{Supplementary material to "Noisy dynamical systems evolve error correcting codes and modularity"}
\author{Trevor McCourt, Ila R. Fiete, Isaac Chuang}
\date{March 22, 2023}
\maketitle

\section{Boolean Networks}\label{sec:net_spec}

The state of an $N$-node Boolean network is given by a length $N$ binary vector $x$. The state of a particular node of a Boolean network $x_i$ is updated in time according to some discrete update rule,
\begin{equation}
    x_i[t+1] = f_i(x[t])
\end{equation}
Where $f_i$ is an arbitrary Boolean function. Traditionally, $f_i$ depends on $k \leq N$ of the $x$, such that each node in the network is influenced by exactly $k$ other nodes. Each $f_i$ can therefore be represented by a length $2^k$ truth table. Applying the update rule over and over again starting from some initial state generates a dynamic trajectory.  An example of an $N=4$ $k=2$ network is shown in Fig.~\ref{fig:net_fig}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{supp_network_fig.pdf}
    \caption{\textbf{An example Boolean network. a} A trajectory of an $N=4$ $k=2$ Boolean network. The network reaches a fixed point (steady-state) at $t=2$. \textbf{b} The truth tables associated with each of the $4$ nodes that, along with an initial state, generate the trajectory shown in (a).}
    \label{fig:net_fig}
\end{figure}

This kind of constant $k$ Boolean network can be compactly specified by two matrices. The binary-valued function matrix $F$ has dimension $2^k \times N$, and has columns that are the output row of the truth table for each node. For example, for the network shown in Fig.~\ref{fig:net_fig},
\begin{equation}
    F = \begin{pmatrix}
0 & 0 & 1 & 0\\
0 & 1 & 0 & 1\\
0 & 1 & 0 & 1\\
1 & 0 & 0 & 1\\
\end{pmatrix}
\end{equation}
The second matrix is the integer-valued connectivity matrix $C$. This matrix has dimension $k \times N$, and the columns contain the node labels that influence each node. For the Fig.~\ref{fig:net_fig} network,
\begin{equation}
    C = \begin{pmatrix}
1 & 2 & 3 & 0\\
3 & 0 & 1 & 2\\
\end{pmatrix}
\end{equation}

We work with ragged networks, which are networks with variable $k$. Each node has its own degree $k_i \leq k_{max}$. An example of a ragged modification of the Fig.~\ref{fig:net_fig} network is shown in Fig.~\ref{fig:rag_net_fig}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{ragged_network.pdf}
    \caption{\textbf{An example ragged Boolean network} }
    \label{fig:rag_net_fig}
\end{figure}

It is desirable to have a way of specifying ragged networks using rectangular data structures, such that batches of network specifications can be packed into tensors for accelerated simulation. To do this, we maintain the $F$ and $C$ matrices described above, which now are of dimension $2^{k_{max}} \times N$ and $k_{max} \times N$, respectively. We then define a binary masking matrix $U$ that indicates which connections in $C$ are active. For example, the $U$ matrix used to generate the Fig.~\ref{fig:rag_net_fig} network from the Fig.~\ref{fig:net_fig} network is,
\begin{equation}
    U = \begin{pmatrix}
1 & 0 & 1 & 1\\
1 & 1 & 0 & 1\\
\end{pmatrix}
\end{equation}
Therefore, a ragged boolean network can be specified by the triplet of matrices $(F, C, U)$.

\section{Accelerating Boolean Network Evaluation}

With the data structures described in Sec.~\ref{sec:net_spec} in hand, it is straightforward to accelerate the evaluation of large batches of networks. The updating of a network can be handled using vectorized tensor operations. We've open-sourced our python implementation of the update rule on GitHub \cite{mccourt_2023}. Networks can be evaluated either on a CPU using the NumPy library \cite{harris2020array} or on an NVIDIA GPU using CuPy, \cite{cupy_learningsys2017}. In practice, GPUs substantially accelerate the evaluation of large batches of networks, see Fig.~\ref{fig:accel_fig}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{network_acceleration.pdf}
    \caption{\textbf{Applying batches of binary functions on the CPU vs GPU} A GPU can accelerate the simulation of large batches of Boolean networks by more than 2 orders of magnitude. The GPU used here was an NVIDIA RTX 3090 and the CPU was an AMD Ryzen 9 5950x}
    \label{fig:accel_fig}
\end{figure}

\section{XOR Evolution Details}

One curious difference between AND and XOR is that it seems higher connectivity is required to fault-tolerantly perform XOR. Fig.~\ref{fig:and_vs_xor} shows the results of many (short, only 50k generations run) evolutionary experiments comparing AND and XOR for many different values of $N$ and $k_{max}$.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{and_vs_xor_char.pdf}
    \caption{\textbf{The evolutionary characteristics of AND and XOR for different values of $N$ and $k_{max}$} Comparing the fitness of many independent AND (\textbf{a}) and XOR (\textbf{b}) populations with different values of $N$ and $k_{max}$ after 50k generations of evolution. It seems that $k=4$ is required for XOR to reach low error rates (at least in the same amount of time as AND). }
    \label{fig:and_vs_xor}
\end{figure*}

This result is reasonable, as it is well known that XOR is a more complicated function to learn than AND \cite{minsky2017}. The reason for this is XOR has a more complicated truth table, with exactly half the bits being one (as opposed to only 1 bit for AND). As a result of this, we used $k_{max}=3$ for AND and $k_{max}=4$ for XOR in all of our experiments.

Fig.~\ref{fig:xor_traj} shows an example XOR network and noisy trajectory.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{xor_supp_fig.pdf}
    \caption{\textbf{An XOR network and a noisy trajectory a} A network that implements XOR. \textbf{b} A noisy trajectory of an XOR network.}
    \label{fig:xor_traj}
\end{figure*}

Note that the shown network only implements a distance 7 code, one bit is the same in each codeword. This seems to be typical of XOR networks, as shown in the main text figure 2. This is reasonable, as the evolutionary pressure to increase the code distance from 7 to 8 is low, and the optimization problems are more difficult for a $k_{max}=4$ network compared to a $k_{max}=3$ network, since the configuration space is larger and the loss landscape is more correlated \cite{kauffman1992}. 

\section{Effects of Various Crossover Operators}

Crossover operators are used in genetic algorithms to combine parent organisms to form a child organism, in the hope that the child will inherit the best features of all the parents and be of higher fitness. In a traditional genetic algorithm, crossover is applied to the fittest organisms in a population, and the children are then mutated and added back to the population. If no crossover operator is applied, we simply mutate one of the parents, and this process becomes equivalent to a pure random adaptive walk (greedily moving in random directions that improve fitness). We tried a few different crossover operators in this work and found that they were only slightly higher performing than a pure adaptive walk. This is shown in \ref{fig:crossover_compare}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{various_crossovers.pdf}
    \caption{\textbf{Evolving AND with various crossover operators} }
    \label{fig:crossover_compare}
\end{figure}

Rectangular crossover refers to splicing together the $F$, $C$, and $U$ matrices of two organisms. This is done by cutting each set of matrices vertically in the same position and re-assembling the components. This method does not respect the structure of the graphs, as the node label number is arbitrary. Graph crossover refers to cutting the graphs of two parents and wiring them back together in a way that respects the original connectivity \cite{Globus2013}.

The rectangular crossover slightly outperforms no crossover, converging to the same minimum error rate slightly before the random walk. This difference is small. Interestingly, it seems the graph crossover operation is the lowest performing, although it could not be tested to convergence as it is computationally intensive and requires transferring a large amount of data from GPU to CPU every generation. 

 \printbibliography

 
 
\end{document}