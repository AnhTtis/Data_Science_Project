\documentclass[a4paper, twoside, 12pt]{article}

% Add the code in the 'template.tex' file. Check this file to see which packages are already loaded.
\input{template}


% Preamble: load the packages that you need and define your own commands here
\newcommand{\todo}[1]{\textcolor{red}{[todo: #1]}}
\newcommand{\vic}[1]{\textcolor{magenta}{[vic: #1]}}
\newcommand{\rob}[1]{\textcolor{blue}{#1}}
\newcommand{\maika}[1]{\textcolor{green}{#1}}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%           I N F O R M A T I O N   T O   C H A N G E
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Author name displayed in the running head
\newcommand{\runningauthor}{R. Courant \textit{et al.}} 

% Title displayed in the running head
\newcommand{\runningheadtitle}{Transformers}

% Chapter number
\newcommand{\chapternumber}{6}

% E-mail address of the corresponding author
\newcommand{\emailaddress}{vicky.kalogeiton@lix.polytechnique.fr}

% Title of the chapter
\title{Transformers and visual Transformers} 

% Authors' names and affiliation numbers
\author[1,2]{Robin Courant}
\author[1]{Maika Edberg}
\author[1]{Nicolas Dufour}
\author[*,1]{Vicky Kalogeiton}  % Use the symbol '*' for the corresponding author

% Affiliations
\affil[1]{LIX, CNRS, Ecole Polytechnique, IP Paris}
\affil[2]{Univ.\ Rennes, CNRS, IRISA, INRIA}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\affil[$\dagger$]{Equal Contribution}
\affil[*]{Corresponding author: e-mail address: \href{mailto:\emailaddress}{\emailaddress}}

\maketitle

% Restore the geometry and change the page style for the other pages
\afterpage{\aftergroup\restoregeometry}
\pagestyle{otherpages}

\begin{abstract}
Transformers were initially introduced for natural language processing (NLP) tasks, but fast they were adopted by most deep learning fields, including computer vision. They measure the relationships between pairs of input tokens (words in the case of text strings, parts of images for visual Transformers), termed attention. The cost is exponential with the number of tokens. For image classification, the most common Transformer Architecture uses only the Transformer Encoder in order to transform the various input tokens. However, there are also numerous other applications in which the decoder part of the traditional Transformer Architecture is also used. Here, we first introduce the Attention mechanism (Section~\ref{sec:attention}), and then the Basic Transformer Block including the Vision Transformer (Section~\ref{sec:transformer}). Next, we discuss some improvements of visual Transformers to account for small datasets or less computation (Section~\ref{sec:extensions}). Finally, we introduce Visual Transformers applied to tasks other than image classification, such as detection, segmentation, generation and training without labels (Section~\ref{sec:tasks}) and other domains, such as  video or multimodality using text or audio data (Section~\ref{sec:domains}). 

\end{abstract}

\begin{keywords}
attention, Transformers, visual Transformers, multimodal attention
\end{keywords}

\section{Attention} 
\label{sec:attention}
\input{sections/1.Attention}

\section{Visual Transformers} 
\label{sec:transformer}
\input{sections/2.Transformers}

\section{Improvements over the Vision Transformer} 
\label{sec:extensions}
\input{sections/3.Extensions}

\section{Vision Transformers for tasks other than classification} 
\label{sec:tasks}
\input{sections/4.OtherTasks}

\section{Vision Transformers for other domains} 
\label{sec:domains}
\input{sections/5.OtherDomains}

\section{Conclusion} 
\label{sec:conclusion}
\input{sections/6.Conclusion}

% References section
\bibliographystyle{spbasicsort}
\bibliography{longstrings,main}

\end{document}