%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section, we present Transformer-based methods that improve over the original Vision Transformer (Section~\ref{sub:vit}) in two main ways. First, we introduce approaches that are trained on smaller datasets, unlike ViT~\cite{dosovitskiy2020vit} that requires pre-training on 300 million labelled images (Section~\ref{sub:data_efficiency}). 
Second, we present extensions over ViT that are more computational efficient than ViT, given that training a ViT is directly correlated to the image resolution and the number of patches (Section~\ref{sub:compute_efficiency}). 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data Efficiency}
\label{sub:data_efficiency}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As discussed in Section~\ref{sub:vit}, the Vision Transformer (ViT)~\cite{dosovitskiy2020vit} is pretrained on a massive proprietary dataset (JFT-300M) which contains 300 million labelled images. This need arises with Transformers because we remove the inductive biases from the architecture compared to convolutional based networks. Indeed, convolutions contain some translation equivariance. ViT does not benefit from this property and thus has to learn such biases, requiring more data. JFT-300M is an enormous dataset and to make ViT work in practice, better data-efficiency is needed. Indeed, collecting that amount of data is costly and can be infeasible for most tasks.


\begin{figure}[hbtp]
	\centering
		\includegraphics[width=0.8\textwidth]{figures/deit_v2.pdf}
	   \caption{\textbf{The DeiT architecture.} The architecture features an extra token, the distillation token. This token is used similarly to the class token. Figure inspired from~\cite{deit}.}
	\label{fig:deit}
\end{figure}

\paragraph{Data-efficient image Transformers (DeiT)~\cite{deit}.}
The first work to achieve an improved data efficiency is DeiT~\cite{deit} . The main idea of DeiT is to distil the inductive biases from a CNN into a Transformer (Figure~\ref{fig:deit}). DeiT adds another token that works similarly to the class token. When training, ground truth labels are used to train the network according to the class token output with a cross entropy loss. However, for the distillation network, the output labels are compared to the labels provided from a teacher network with a cross entropy loss. The final loss for a $N$-categorical classification task is defined as follows:
\begin{equation}
\begin{array}{c}
    \mathcal{L}_{global}^{hardDistill} = \frac{1}{2}(\mathcal{L}_{CE}(\Psi(\bm{Z}_{class}), \bm{y}) + \mathcal{L}_{CE}(\Psi(\bm{Z}_{distill}), \bm{y}_T) ) \quad , \\
    \mathcal{L}_{CE}(\bm{\hat{y}}, \bm{y}) = - \frac{1}{N}\sum_{i = 1}^{N} \left[ y_i \log \hat{y}_i + (1 - y_i) \log(1 - \hat{y}_i) \right] 
\end{array}
\end{equation}
with $\Psi$ the Softmax function, $\bm{Z}_{class}$ the class token output,  $\bm{Z}_{distill}$ the class token output, $\bm{y}$ the ground truth label and $\bm{y}_T$ the teacher label prediction. 

The teacher network is a Convolutional Neural Network (CNN). The main idea is that the distillation head will provide the inductive biases needed to improve the data efficiency of the architecture. By doing this, DeiT achieves remarkable performance on the ImageNet dataset, by training `only' on ImageNet-1K~\cite{deng2009imagenet}, which contains 1.3 million images.

\paragraph{Convit~\cite{convit}.}
The main disadvantage of DeiT~\cite{deit} is that is requires a pretrained CNN, which is not ideal, and it would be more convenient to not have this requirement. The CNN has a hard inductive bias constraint that can be a major limitation. Indeed, if enough data is available, learning the biases from the data can result in better representations. 

Convit~\cite{convit} overpasses this issue by including the inductive bias of CNNs into a Transformer in a soft way. Specifically, if the inductive bias is limiting the  training, the Transformer can discard it. The main idea is to include the inductive bias into the ViT initialization. Therefore, before beginning training, the ViT is equivalent to a CNN. Then, the network can progressively learn the needed biases and diverge from the CNN initialization. 

\begin{figure}[hbtp]
	\centering
		\includegraphics[width=\textwidth]{figures/cct_v2.pdf}
	   \caption{\textbf{Compact Convolutional Transformers.} This architecture  features a convolutional based patch extraction to leverage a smaller Transformer network, leading to higher data-efficiency. Figure inspired from~\cite{cct}.}
	\label{fig:cct}
\end{figure}

\paragraph{Compact Convolutional Transformer~\cite{cct}.}
DeiT~\cite{deit} and ConVit~\cite{convit} successfully achieve data efficiency at the ImageNet scale. However, ImageNet is a big dataset with 1.3 million images, whereas most datasets are significantly smaller. 

To reach higher data efficiency, the Compact Convolutional Transformer~\cite{cct} uses a CNN operation to extract the patches, and then uses these patches in a Transformer network (Figure~\ref{fig:cct}). 
The Compact Convolutional Transformer comes with some modifications that lead to  major improvements. First, by having a more complex encoding of patches, the system relies on the convolutional inductive biases at the lower scales and then uses a Transformer network to remove the locality constraint of the CNN. Second, the authors show that discarding the `class' token results in higher efficiency. Specifically, instead of the class token, the Compact Convolutional Transformer pools together all the patches token and classifies on top of this pooled token. These two modifications enable using smaller Transformers, while improving both the data efficiency and the computational efficiency. Therefore, these improvements allow the Compact Convolutional Transformer to be successfully trained on smaller datasets, such as CIFAR or MNIST.
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Computational Efficiency}
\label{sub:compute_efficiency}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The Vision Transformer architecture (Section~\ref{sub:vit}) suffers from a $\mathcal{O}(n^2)$ complexity with respect to the number of tokens. When considering small resolution images or big patch size, this is not a limitation; for instance, for an image of $224 \times 224$ resolution with $16 \times 16$ patches, this amounts to $196$ tokens. However, when needing to process larger images (for instance 3D images in medical imaging) or when considering smaller patches, using and training such models becomes \emph{prohibitive}. 
For instance, in tasks such as segmentation or image generation, it is needed to have more granular representations than $16 \times 16$ patches; hence, it is crucial to solve this issue to enable more applications of Vision Transformer.

\begin{figure}[hbtp]
	\centering
		\includegraphics[width=0.9\textwidth]{figures/swin.pdf}
	   \caption{\textbf{Shifting operation in the Swin Transformer~\cite{swin}.} Between each attention operation, the attention window is shifted so that each patch can communicate with different patches than before. This allows the network to gain more global knowledge with the network's depth. Figure inspired from~\cite{swin}.
	   }
	\label{fig:swin}
\end{figure}

\paragraph{Swin Transformer~\cite{swin}.}
One idea to make Transformers more computation efficient is the Swin Transformer~\cite{swin}. Instead of attending every patch in the image, the Swin Transformer proposes to add a locality constraint. Specifically, the patches can  only attend other patches that are limited to a vicinity window $K$. This restores the local inductive bias of CNNs. To allow communication across patches throughout the network, the Swin Transformer shifts the attention windows from one operation to another (Figure~\ref{fig:swin}). Therefore, the Swin Transformer is quadratic with regard to the size of the window $K$ but linear with respect to the number of tokens $n$ with complexity $\mathcal{O}(nK^2)$. In practice, however, $K$ is small, and this solves the quadratic complexity problem of attention.

\begin{figure}[hbtp]
	\centering
		\includegraphics[width=\textwidth]{figures/perceiver_v2.pdf}
	   \caption{\textbf{The Perceiver architecture~\cite{jaegle2021perceiver,perceiverio}.} A set of latent tokens retrieve information from the image through Cross-Attention. Self-Attention is performed between the tokens to refine the learned representation. These operations are linear with respect to the number of image tokens. Figure inspired from~\cite{jaegle2021perceiver,perceiverio}.}
	\label{fig:perceiver}
\end{figure}


\paragraph{Perceiver~\cite{jaegle2021perceiver,perceiverio}.}
Another idea for a more computation efficient visual Transformers is to make a more drastic change to the architecture. If instead of using self-attention, the model uses cross-attention, the problem of the quadratic complexity with regard to the number of tokens can be solved. Indeed, computing the cross attention between two sets of length $m$ and $n$, respectively, has complexity $\mathcal{O}(mn)$. This idea is introduced in the Perceiver~\cite{jaegle2021perceiver,perceiverio}. The key idea is to have a smaller set of latent variables that will be used as queries and that will retrieve information in the image token set (Figure~\ref{fig:perceiver}). Since this solves the quadratic complexity issue, it also removes the need of using patches; hence, in the case of Transformers, each pixel is mapped to a single token.