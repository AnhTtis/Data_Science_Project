Attention is a technique in Computer Science that imitates the way in which the brain can focus on the relevant parts of the input. In this section, we introduce attention: its history (Section~\ref{sub:history_attention}), its definition (Section~\ref{sub:definition_attention}), its types and variations  (Sections~\ref{sub:attention_types} and \ref{sub:variation_attention}) and its properties (Section~\ref{sub:property_attention}). \\

To understand what attention is and why it is so useful, consider the following film review:
\begin{center}
	\textit{While others claim the story is boring, I found it fascinating.}
\end{center}
Is this film review positive or negative?   
The first part of the sentence is unrelated to the critic's opinion, while the second part suggests a positive sentiment with the word ‘fascinating’. 
To a human, the answer is obvious; however, this type of analysis is not necessarily obvious to a computer. 

Typically, sequential data require \emph{context} to be understood. 
In natural language, a word has a meaning because of its position in the sentence, with respect to the other words: its \emph{context}. 
In our example, while “boring” alone suggests that the review is negative, its contextual relationship with the other words allows the reader to reach the appropriate conclusion. 
In computer vision, in a task like object detection, the nature of a pixel alone cannot be identified: we need to account for its neighbourhood, its \emph{context}. 
So, how can we formalize the concept of \emph{context} in sequential data?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The History of Attention}
\label{sub:history_attention}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This notion of \emph{context} is the motivation behind the introduction of the attention mechanism in 2015 \cite{Bahdanau2015NeuralMT}. 
Before this, language translation was mostly relying on encoder-decoder architectures:  recurrent neural networks (RNNs) ~\cite{cho_RNN_translation_2014} and in particular long short-term memory (LSTMs) networks were used to model the relationship among words~\cite{ Sutskever_LSTM_Translation_2014}. 
Specifically, each word of an input sentence is processed by the encoder sequentially. At each step, the past and present information are summarized and encoded into a fixed-length vector. 
In the end, the encoder has processed every word, and outputs a final fixed-length vector, which summarizes all input information. This final vector is then decoded, and finally translates the input information into the target language.

However, the main issue of such structure is that all the information is compressed into one fixed-length vector. 
Given that the sizes of sentences vary and as the sentences get longer, a fixed-length vector is a real bottleneck: it gets increasingly difficult not to lose any information in the encoding process due to the vanishing gradient problem~\cite{Bahdanau2015NeuralMT}. 

As a solution to this issue, Bahdanue et al.~\cite{Bahdanau2015NeuralMT} proposed the attention module in 2015. 
The attention module allows the model to consider the parts of the sentence that are relevant to predicting the next word. Moreover, this facilitates the understanding of relationships among words that are further apart.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Definition of Attention}
\label{sub:definition_attention}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
    \centering
    \includegraphics[scale=0.8]{figures/attention_v2.pdf}
    \caption{\textbf{Attention block.} Next to each element, we denote its dimensionality. Figure inspired from~\cite{vaswani2017attention}.  }
    \label{fig:attention}
\end{figure}

Given two lists of tokens, $\bm{X}\in\mathbb{R}^{N \times d_x}$ and $\bm{Y}\in\mathbb{R}^{N \times d_y}$, attention encodes information from $\bm{Y}$ into $\bm{X}$, where $N$ is the length of inputs $\bm{X}$ and $\bm{Y}$, and $d_x$ and $d_y$ are their respective dimensions. 
For this, we first define three linear mappings: 
query mapping $\bm{W}^Q\in\mathbb{R}^{d_x \times d_{q}}$,  key mapping $\bm{W}^K\in\mathbb{R}^{d_y \times d_{k}}$ and value mapping $\bm{W}^V\in\mathbb{R}^{d_y \times d_{v}}$, where $d_q$, $d_k$, and $d_v$ is the embedding dimension in which the query, key, and value are going to be computed, respectively. 

Then, we define the query $\bm{Q}$, key $\bm{K}$ and value $\bm{V}$~\cite{vaswani2017attention} as:
\begin{align*}
    \bm{Q} &= \bm{X} \bm{W}^Q \\
    \bm{K} &= \bm{Y} \bm{W}^K \\
    \bm{V} &= \bm{Y} \bm{W}^V 
\end{align*}

Next, the \emph{attention matrix} is defined as:
\begin{equation}
    \label{eq:attention_matrix}
    A(\bm{Q}, \bm{K}) = \text{Softmax}
    \left(
    \frac{\bm{Q}\bm{K}^\top}{\sqrt{d_k}}
    \right) \quad . 
\end{equation}

This is illustrated in the left part of Figure~\ref{fig:attention}. The nominator $\bm{Q K}^T \in \mathbb{R}^{N \times N}$ represents how each part of the input in $\bm{X}$ attends to each part of the input in $\bm{Y}$\footnote{Note that in the literature, there are two main attention functions: additive attention~\cite{Bahdanau2015NeuralMT} and dot-product attention (Equation~\ref{eq:attention_matrix}). In practice, the dot-product is more efficient since it is implemented using highly optimized matrix multiplication, compared to the feed forward network of the additive attention; hence, the dot-product is the dominant one.}. This dot-product is then put through the Softmax function to normalize its values and get positive values that add to 1. However, for large values of $d_k$, this may result in the Softmax to have incredibly small gradients, so it is scaled down by $\sqrt{d_k}$. 

The resulting $N \times N$ matrix encodes the relationship between $\bm{X}$ with respect to $\bm{Y}$: it measures how important a token in $\bm{X}$ is with respect to another one in $\bm{Y}$.

Finally, the \emph{attention output} is defined as:
\begin{equation}
    \label{eq:attention_output}
    \text{Attention}(\bm{Q},\bm{K}, \bm{V})= A(\bm{Q}, \bm{K}) \bm{V} \quad .
\end{equation}

Figure~\ref{fig:attention} displays this.  
The attention output encodes the information of each token by taking into account the contextual information. 
Therefore, through the learnable parameters -- queries, keys, and values--, the attention layers learn a token embedding that takes into account their relationship. 

\paragraph{Contextual relationships.}
How does Equation~\ref{eq:attention_output} encode contextual relationships? To answer this question, let us reconsider analysing the sentiment of film reviews. To encode contextual relationships into the word embedding, we first want a matrix representation of the relationship between all words. To do so, given a sentence of length $N$, we take each word vector and feed it to two different linear layers, calling one output `query' and the other output `key'. We pack the queries into the matrix $\bm{Q}$ and the keys into the matrix $\bm{K}$, by taking their product ($\bm{Q} \bm{K}^T$). The result is a $N \times N$ matrix that explains how important the $i$-th word (row-wise) is to understand the $j$-th word (column-wise). This matrix is then scaled and normalized by the division and Softmax. Next, we feed the word vectors into another linear layer, calling its output `value'.   
We multiply these two matrices together. The result of their product are attention vectors that encode the meaning of each word, by including their contextual meaning as well. Given that each of these queries, keys, and values are learnable parameters, as the attention layer is trained, the model learns how relationships among words are encoded in the data. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Types of Attention}
\label{sub:attention_types}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

There exist two dominant types of attention mechanisms: \emph{self-attention} and  \emph{cross-attention} \cite{vaswani2017attention}. 
In \emph{self-attention}, the queries, keys and values come from the same input, i.e., $\bm{X} = \bm{Y}$; 
in \emph{cross-attention}, the queries come from a different input than the key and value vectors, i.e., $\bm{X} \neq \bm{Y}$. 
These are described below in Sections~\ref{subsub:self_attention} and \ref{subsub:cross_attention}, respectively.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Self-Attention}
\label{subsub:self_attention}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In self-attention, the tokens of $\bm{X}$ attend to themselves ($\bm{X}=\bm{Y}$). Therefore, it is modelled as follows: 
\begin{equation}
    \text{SA}(\bm{X}) = \text{Attention}(
                           \bm{X}\bm{W}^Q , 
                           \bm{X}\bm{W}^K, 
                           \bm{X}\bm{W}^V ) \quad .
\end{equation}

Self-attention formalizes the concept of context. It learns the patterns underlying how parts of the input correspond to each other. By gathering information from the same set, given a sequence of tokens, a token can attend to its neighbouring tokens to compute its output.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Cross-Attention}
\label{subsub:cross_attention}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Most real-world data are multimodal -- for instance, videos contain frames, audios and subtitles, images come with captions, etc. Therefore, models that can deal with such types of multimodal information have become essential. 

Cross-attention is an attention mechanism designed to handle multimodal inputs.
Unlike self-attention, it extracts queries from one input source and key-value pairs from another one ($\bm{X} \neq \bm{Y}$). It answers the question: `which parts of input $\bm{X}$ and input $\bm{Y}$ correspond to each other?'
Cross-attention (CA) is defined as:
\begin{equation}
    \text{CA}(\bm{X}, \bm{Y}) = \text{Attention}(
        \bm{X}\bm{W}^Q , 
        \bm{Y}\bm{W}^K, 
        \bm{Y}\bm{W}^V) \quad .
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Variation of Attention}
\label{sub:variation_attention}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Attention is typically employed in two ways: 
(1) Multi-head Self-Attention (MSA, Section~\ref{subsub:multihead}) and (2) Masked Multi-head Attention (MMA, Section~\ref{subsub:maskedmultihead}). 

\paragraph{Attention Head.}
We call Attention Head the mechanism presented in Section~\ref{sub:definition_attention}, i.e., query-key-value projection, followed by scaled dot product attention (Equations~\ref{eq:attention_matrix} and \ref{eq:attention_output}). 



When employing an attention-based model, relying only on a single attention head can inhibit learning. 
Therefore, the Multi-head Attention block is introduced \cite{vaswani2017attention}. 

\paragraph{Multi-head Self-Attention (MSA).} 
\label{subsub:multihead}
MSA is shown in Figure~\ref{fig:multi_head_attention} and is defined as: 

\begin{equation}
\label{eq:multihead_self_attention}
\begin{array}{c}
    \text{MSA}(\bm{X}) = \text{Concat} \big( \text{head}_{1}(\bm{X}), \dots, \text{head}_{h}(\bm{X}) \big) \bm{W}^O \quad, \\
     \quad 
    \text{head}_{i}(\bm{X}) = \text{SA}(\bm{X}) \text{ , }\forall i \in \{1,h\} \quad ,
\end{array}
\end{equation}

\noindent where Concat is the concatenation of $h$ attention heads and $\bm{W}^O\in\mathbb{R}^{hd_v \times d}$ is projection matrix. 
This means that the initial embedding dimension $d_x$ is decomposed into $h \times d_v$, and the computation per head is carried out independently. The independent attention heads are usually concatenated and multiplied by a linear layer to match the desired output dimension. The output dimension is often the same as the input embedding dimension $d$. This allows an easier stacking of multiple blocks. 

\paragraph{Multi-head Cross-Attention (MCA).}
Similar to MSA, MCA is defined as:

\begin{equation}
\label{eq:multihead_cross_attention}
\begin{array}{c}
\text{MCA}(\bm{X}, \bm{Y}) = \text{Concat}(\text{head}_1(\bm{X}, \bm{Y}), \dots, \text{head}_h(\bm{X}, \bm{Y}))\bm{W}^O \quad, \\ 
     \quad 
    \text{head}_{i}(\bm{X}, \bm{Y}) = \text{CA}(\bm{X}, \bm{Y}) \text{ , }\forall i \in \{1,h\} \quad .
\end{array}
\end{equation}

\begin{figure}
    \centering
    \includegraphics[scale=0.8]{figures/msa_v2.pdf}
    \caption{\textbf{Multi-head Self-Attention block (MSA).} 
    First, the input $\bm{X}$ is projected to queries, keys and values and then passed through $h$ attention blocks. The $h$ resulting attention outputs are then concatenated together and finally projected to a $d$-dimensional output vector.
    Next to each element, we denote its dimensionality. Figure inspired from \cite{vaswani2017attention}. 
    }
    \label{fig:multi_head_attention}
\end{figure}



\paragraph{Masked Multi-head Self-Attention (MMSA).}
\label{subsub:maskedmultihead}
The MMSA layer~\cite{vaswani2017attention} is another variation of attention. It has the same structure as the Multi-head Self-Attention block (Section~\ref{subsub:multihead}), but all the later vectors in the target output are masked. When dealing with sequential data, this can help make training parallel. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Properties of Attention}
\label{sub:property_attention}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

While attention encodes contextual relationships, it is \emph{permutation equivalent}, as the mechanism does not account for the order of the input data. As shown in Equation~\ref{eq:attention_output}, the attention computations are all matrix multiplication and normalizations. Therefore, a permuted input results in a permuted output. In practice, however, this may not be an accurate representation of the information. For instance, consider the sentences `the monkey ate the banana' and `the banana ate the monkey'. They have distinct meanings because of the order of the words. 
If the order of the input is important, various mechanisms, such as the Positional Encoding, discussed in Section~\ref{subsub:positional_encoding}, are used to capture this subtlety. 