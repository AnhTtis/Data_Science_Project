In this section, we present applications of visual Transformers to other domains. 
First, we describe multimodal Transformers operating with vision and language (Section~\ref{sub:multimodal_transformers}), then we describe video-level attention and video Transformers (Sections~\ref{sub:video_attention}-\ref{sub:video_transformers}), and finally we present multimodal video Transformers operating with vision, language and audio (Section~\ref{sub:multi_modal}). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Multimodal Transformers: vision and language}
\label{sub:multimodal_transformers}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As Transformers have found tremendous success in both natural language processing and computer vision, their use in vision-language tasks is also of interest. In this section, we describe some representative multimodal methods for vision and language: ViLBERT (Section~\ref{subsub:vilbert}), DALL-E (Section~\ref{subsub:dalle}) and CLIP (Section~\ref{subsub:clip}). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{ViLBERT}
\label{subsub:vilbert}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Vision-and-language BERT (VilBERT)~\cite{lu_vilbert_2019} is an example of architecture that fuses two modalities. 
It consists of two parallel streams, each one working with one modality. The vision stream extracts bounding boxes from images via an object detection network, by encoding their position. The language stream embeds word vectors and extracts feature vectors using the basic Transformers encoder block~\cite{vaswani2017attention} (Figure~\ref{fig:transformer_arch} left). These two resulting feature vectors are then fused together by a Cross-Attention layer (Section~\ref{subsub:cross_attention}). This follows the standard architecture of the Transformers encoder block, where the keys and values of one modality are passed onto the MCA block of the other modality. The output of the Cross-Attention Layer is passed into another Transformers encoder block and these two layers are stacked multiple times. 

The language stream is initialized with BERT trained on Book Corpus~\cite{zhu_corpus_2015} and Wikipedia~\cite{wikidump}, while the visual stream is initialized with Faster R-CNN~\cite{ren_fastercnn(vilbert)_2015}. On top of the pre-training of each stream, the whole architecture is pre-trained on the Conceptual Captions dataset~\cite{sharma2018conceptual} on two pretext tasks. 

ViLBERT has been proven powerful for a variety of multimodal tasks. In the original paper, ViLBERT was fined-tuned to a variety of tasks, including visual question answering, visual common-sense reasoning, referring expressions, and caption-based image retrieval.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{CLIP}
\label{subsub:clip}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Connecting Text and Images (CLIP)~\cite{clip} is designed to address two major issues of deep learning models: costly datasets and inflexibility. While most deep learning models are trained on labelled datasets, CLIP is trained on 400 million text-image pairs that are scraped from the internet. This reduces the labour of having to manually label millions of images that are required to train powerful deep learning models. When models are trained on one specific dataset, they also tend to be difficult to extend to other applications. For instance, the accuracy of a model trained on ImageNet is generally limited to its own dataset and cannot be applied to real-world problems. To optimize training, CLIP models learn to perform a wide variety of tasks during pretraining, and this task allows for zero-shot transfer to many existing datasets. While there are still several potential improvements, this approach is competitive to supervised models that are trained on specific datasets.

\paragraph{CLIP architecture and training.}
CLIP is used to measure the similarity between the text input and the image generated from a latent vector. At the core of the approach is the idea of learning perception from supervision contained in natural language. Methods which work on natural language can learn passively from the supervision contained in the vast amount of text on the internet. 

Given a batch of $N$ (image, text) pairs, CLIP is trained to predict which of the $N \times N$ possible (image, text) pairings across a batch actually occurred. To do this, CLIP learns a multimodal embedding space by jointly training an image encoder and a text encoder to  maximize the cosine similarity of the image and text embeddings of the $N$ real pairs in the batch, while minimizing the cosine similarity of the embeddings of the $N^{2} {\--} N$ incorrect pairings. A symmetric cross entropy loss over these similarity scores is optimized.

Two different architectures were considered for the image encoder. For the first, ResNet-50~\cite{he_resnet50_2017} is used as the base architecture for the image encoder due to its widespread adoption and proven performance. Several modifications were made to the original version of ResNet.
For the second architecture, ViT is used with some minor modifications: first, adding an extra layer normalization to the combined patch and position embeddings before the Transformer, and second, using a slightly different initialization scheme.

The text encoder is a standard Transformer~\cite{vaswani2017attention} (Section~\ref{sub:basic_transformer}) with the architecture modifications described in~\cite{clip}. As a base size, CLIP uses a 63M- parameter 12- layer 512-wide model with 8 attention heads. The Transformer operates on a lower-cased byte pair encoding (BPE) representation of the text with a 49,152 vocab size~\cite{sennrich2015neural}. The max sequence length is capped at 76. The text sequence is bracketed with [SOS] and [EOS] tokens\footnote{[SOS]: start-of-sequence; [EOS]: end-of-sequence} and the activations of the highest layer of the Transformer at the [EOS] token are treated as the feature representation of the text which is layer normalized and then linearly projected into the multimodal embedding space.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{DALL-E and DALL-E 2}
\label{subsub:dalle}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

DALL-E~\cite{ramesh_dalle_2021} is another example of the application of Transformers in vision. It generates images from a natural language prompt -- some examples include `an armchair in the shape of an avocado' and `a penguin made of watermelon'. It uses a decoder-only model, which is similar to GPT-3~\cite{brown_gpt3_2020}. DALL-E uses 12 billion parameters and is pre-trained on Conceptual Captions~\cite{sharma2018conceptual} with over 3.3 million text-image pairs. 
DALL-E 2~\cite{ramesh2022dalle2} is the upgraded version of DALL-E, based on diffusion models and CLIP (Section~\ref{subsub:clip}), and allows better performances with more realistic and accurate generated images. 
In addition to producing more realistic results with a better resolution than DALL-E, DALL-E 2 is also able to edit the outputs. Indeed, with DALL-E 2, one can add or remove realistically an element in the output, and can also generate different variations of the same output.
These two models clearly demonstrate the powerful nature and scalability of Transformers that are capable of efficiently processing a web-scale amount of data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Flamingo}
\label{subsub:flamingo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Flamingo~\cite{alayrac2022flamingo} is a visual language model (VLM) tackling a wide range of multimodal tasks based on few-shot learning. This is an adaptation of large language models (LLMs) handling an extra visual modality with 80B parameters.

Flamingo consists of three main components: a vision encoder, a Perceiver resampler and a language model.
%
First, to encode images or videos, a vision convolutional encoder~\cite{brock2021nfnet} is pretrained in a contrastive way, using image and text pairs~\footnote{The text is encoded using a pretrained BERT model~\cite{devlin2018bert}.}.
%
Then, inspired by the Perceiver architecture~\cite{jaegle2021perceiver} (detailed in Section~\ref{subsub:cross_attention}), the Perceiver resampler takes a variable number of encoded visual features and outputs a fixed length latent code.
%
Finally, this visual latent code conditions the language model by querying language tokens through cross attention blocks. Those cross-attention blocks are interleaved with pretrained and frozen language model blocks.

The whole model is trained using three different kinds of datasets without annotations (text with image content from webpages~\cite{alayrac2022flamingo}, text and image pairs~\cite{alayrac2022flamingo, jia2021align} and text and video pairs~\cite{alayrac2022flamingo}).
Once the model is trained, it is fine-tuned using few-shot learning techniques to tackle specific tasks.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Video Attention} 
\label{sub:video_attention}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Video understanding is a long-standing problem, and despite incredible Computer Vision advances, obtaining the best video representation is still an active research area. Videos require employing effective spatio-temporal processing of RGB and time streams to capture long-range interactions~\cite{epstein2021learning,marin2019laeo}, while focusing on important video parts~\cite{nagrani2021attention} with minimum computational resources~\cite{ryoo2021tokenlearner}. 

Typically, video understanding benefits from 2D Computer Vision, by adapting 2D image processing methods to 3D spatio-temporal methods~\cite{hara2018cnn3d}.  
And through the Video Vision Transformer (ViViT)~\cite{arnab2021vivit}, history repeats itself. 
Indeed, with the rise of Transformers~\cite{vaswani2017attention} and the recent advances in image classification~\cite{dosovitskiy2020vit}, video Transformers appear as logical successors of CNNs. 

However, in addition to the computationally expensive video processing, Transformers  also  require a lot of computational resources. Thus, developing efficient spatio-temporal attention mechanisms is essential~\cite{arnab2021vivit,bertasius2021timesformer,jaegle2021perceiver}.

In this section, we first describe the general principle of video Transformers (Section~\ref{subsub:general_principle}), and then, we detail three different attention mechanisms used for video representation (Sections~\ref{subsub:full_space_time_attention}, \ref{subsub:divided_space_time_attention} and \ref{subsub:attention_bottleneck}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{General principle}
\label{subsub:general_principle}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Generally, inputs of video Transformers are RGB video clips $\bm{\mathsfit{X}} \in \mathbb{R}^{F\times H \times W \times 3}$, with $F$ frames of size $H \times W$.

To begin with, video Transformers split the input video clip $\bm{\mathsfit{X}}$ into $ST$ tokens $\bm{x}_i \in \mathbb{R}^K$, where $S$ and $T$ are respectively the number of tokens along the spatial and temporal dimension and $K$ is the size of a token.

To do so, the simplest method extracts non-overlapping 2D patches of size $P\times P$ from each frame~\cite{dosovitskiy2020vit}, as used in TimeSformer~\cite{bertasius2021timesformer}. This results in $S = HW/P^2$, $T=F$ and $K = P^2$.

However, there exist more elegant and efficient token extraction methods for videos. For instance, in ViViT~\cite{arnab2021vivit}, the authors propose to extract 3D volumes from videos (involving $T \neq F$) to capture spatio-temporal information within tokens. In TokenLearner~\cite{ryoo2021tokenlearner}, they propose a learnable token extractor to select the most important parts of the video.

Once raw tokens  $\bm{x}_i$ are extracted, Transformer architectures aim to map them into $d$-dimensional embedding vectors $\bm{Z} \in \mathbb{R}^{ST \times d}$ using a linear embedding $\bm{E} \in \mathbb{R}^{d \times K}$: 
\begin{equation}
    \bm{Z} = [\bm{z}_{cls}, E\bm{x}_1, E\bm{x}_2, \dots, E\bm{x}_{ST}] + \textbf{PE} \quad ,
\end{equation}
where $\bm{z}_{cls} \in \mathbb{R}^{d}$ is a classification token that encodes information from all tokens of a single sample~\cite{devlin2018bert}, and $\textbf{PE} \in \mathbb{R}^{ST \times d}$ is a positional embedding that encodes the spatio-temporal position of tokens, since the subsequent attention blocks are permutation invariant~\cite{vaswani2017attention}.

In the end, embedding vectors $\bm{Z}$ pass through a sequence of $L$ Transformer layers. A Transformer layer $\ell$, is composed of a series of Multi-head Self-Attention (MSA)~\cite{vaswani2017attention}, Layer Normalisation (LN)~\cite{ba2016layernorm} and MLP blocks:
\begin{equation}
\begin{array}{c}
\bm{Y}^\ell = \text{MSA}(\text{LN}(\bm{Z}^\ell)) + \bm{Z}^\ell \quad ,\\
\bm{Z}^{\ell + 1} = \text{MLP}(\text{LN}(\bm{Y}^\ell)) + \bm{Y}^\ell \quad .
\end{array}
\end{equation}

In this way, as shown in Figure~\ref{fig:multi_head_attention}, we denote four different components in a video Transformer layer: the Query-Key-Value (QKV) projection, the MSA block, the MSA projection and the MLP.
For a layer with $h$ heads, the complexity of each component is~\cite{vaswani2017attention}:
\begin{itemize}
    \item \textit{QKV projection}: $\mathcal{O}(h.(2STdd_k + STdd_v)$ 
    \item \textit{MSA}: $\mathcal{O}(hS^2T^2.(d_k + d_v))$ 
    \item \textit{MSA projection}: $\mathcal{O}(SThd_vd)$ 
    \item \textit{MLP}: $\mathcal{O}(STd^2)$ 
\end{itemize}

We note that the MSA complexity is the most impacting component, with a quadratic complexity with respect to the number of tokens. 
Hence, for comprehension and clarity purposes, in the rest of the Section, we consider the global complexity of a video Transformer with $L$ layers to equal to $\mathcal{O}(LS^2T^2)$. 

\begin{figure}[hbtp]
	\centering
	    \includegraphics[width=0.6\textwidth]{figures/full_spacetime_attention_v2.pdf}
	    \caption{\textbf{Full space-time attention mechanism.} 
	    Embedding tokens at layer $\ell - 1$, $\bm{Z}^{(\ell -1)}$ are all fed simultaneously through a unique spatio-temporal attention block. Finally, the spatio-temporal embedding is passed through a MLP and normalized to output embedding tokens of the next layer, $\bm{Z}^{\ell}$.
	    Figure inspired from~\cite{bertasius2021timesformer}. }
	\label{fig:full_space_time_attention}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Full space-time attention}
\label{subsub:full_space_time_attention}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As described in~\cite{bertasius2021timesformer,arnab2021vivit}, \textit{full space-time attention} mechanism is the most basic and direct spatio-temporal attention mechanism. As shown in Figure~\ref{fig:full_space_time_attention}, it consists in computing self-attention across all pairs of extracted tokens.

This method results in a heavy complexity of $\mathcal{O}(LS^2T^2)$~\cite{bertasius2021timesformer,arnab2021vivit}. This quadratic complexity can fast be memory consuming, which it is especially true when considering videos.
Therefore, using full space-time attention mechanism is  impractical~\cite{bertasius2021timesformer}.  

\begin{figure}[hbtp]
	\centering
	    \includegraphics[width=0.6\textwidth]{figures/divided_spacetime_attention_v2.pdf}
	    \caption{\textbf{Divided space-time attention mechanism.} 
	    Embedding tokens at layer $\ell - 1$, $\bm{Z}^{(\ell -1)}$ are first processed along the temporal dimension through a first MSA block, and the resulting tokens are processed along the spatial dimension. Finally, the spatio-temporal embedding is passed through a MLP and normalized to output embedding tokens of the next layer, $\bm{Z}^{\ell}$.
	    Figure inspired from~\cite{bertasius2021timesformer}.}
	\label{fig:divided_space_time_attention}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Divided space-time attention}
\label{subsub:divided_space_time_attention}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A smarter and more efficient way to compute spatio-temporal attention is the \textit{divided space-time attention} mechanism, first described in~\cite{bertasius2021timesformer}.

As shown in Figure~\ref{fig:divided_space_time_attention}, it relies on computing spatial and temporal attention separately in each Transformer layer. Indeed, we first compute the spatial attention, i.e., self-attention within each temporal index, and then, the temporal attention, i.e., self-attention across all temporal indices.

The complexity of this attention mechanism is $\mathcal{O}(LST.(S+T))$~\cite{bertasius2021timesformer}.
By separating the calculation of the self-attention over the different dimensions, one tames the quadratic complexity of the MSA module. This mechanism highly reduces the complexity of a model with respect to the full space-time complexity. Therefore, it is reasonable to use it to process videos~\cite{bertasius2021timesformer}.




\begin{figure}[hbtp]
	\centering
	    \includegraphics[width=0.6\textwidth]{figures/bottleneck_attention_v2.pdf}
	    \caption{\textbf{Attention bottleneck mechanism.} 
	    Raw input patches and embedding tokens at layer $\ell - 1$, $\bm{Z}^{(\ell -1)}$ are fed to a cross attention block (CA), then normalized and projected. Finally, the resulting embedding is passed through a transformer to output embedding tokens of the next layer, $\bm{Z}^{\ell}$.
	    Figure inspired from~\cite{jaegle2021perceiver}.}
	\label{fig:attention_bottleneck}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Cross-attention bottlenecks}
\label{subsub:attention_bottleneck}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

An even more refined way to reduce the computational cost of attention calculation, consists of using cross-attention as a bottleneck. For instance, as shown in Figure~\ref{fig:attention_bottleneck} and mentioned in Section~\ref{sub:compute_efficiency}, the Perceiver~\cite{jaegle2021perceiver} projects the extracted tokens $\bm{x}_i$ into a very low-dimensional embedding through a cross-attention block placed before the Transformer layers. 

Here, the cross attention block placed before the $L$ Transformer layers reduces the input dimension from $ST$ to $N$, where $N \ll ST$~\footnote{In practice, $N \le 512$ for a Perceiver~\cite{jaegle2021perceiver}, against $ST = 16 \times 16 \times (32 / 2) = 4,096$ for a ViViT-L~\cite{arnab2021vivit}}, thus resulting in a complexity of $\mathcal{O}(STN)$. Hence, the total complexity of this attention block is $\mathcal{O}(STN + LN^2)$.
It reduces again the complexity of a model with respect to the \textit{divided space-time attention} mechanism. We note that it enables to design deep architectures, as in the Perceiver~\cite{jaegle2021perceiver}, and then, it enables the extraction of higher level features.



\begin{figure}[hbtp]
	\centering
	    \includegraphics[width=\textwidth]{figures/factorised_encoder.pdf}
	    \caption{\textbf{Factorized encoder mechanism.} 
	    First, a spatial Transformer processes input tokens along the spatial dimension, Then, a temporal Transformer processes the resulting spatial embedding along the temporal dimension. 
	    Figure inspired from~\cite{jaegle2021perceiver}.}
	\label{fig:factorised_encoder}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Factorised encoder}
\label{subsub:factorised_encoder}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Lastly, the \textit{factorised encoder}~\cite{arnab2021vivit} architecture is the most efficient with respect to the complexity/performance trade-off.

As in \textit{divided space-time attention}, the \textit{factorised encoder} aims to compute spatial and temporal attention separately. Nevertheless, as shown in Figure~\ref{fig:factorised_encoder}, instead of mixing spatio-temporal tokens in each Transformer layer, here there exist two separate encoders. First, a representation of each temporal index is obtained thanks to a spatial encoder with $L_s$ layers, then, these tokens are passed through a temporal encoder with $L_t$ layers (i.e. $L = L_s + L_t$).

Hence, the complexity of a such architecture, has two main components: the spatial encoder complexity of $\mathcal{O}(L_sS^2)$, and the temporal encoder complexity of $\mathcal{O}(L_tT^2)$. It results in a global complexity of $\mathcal{O}(L_sS^2 + L_tT^2)$.
Thus, it leads to very lightweight models. However, as it first extracts per-frame features, and then aggregates them to a final representation, it corresponds to a late fusion mechanism, which can sometimes be a drawback as it does not mix spatial and temporal information simultaneously~\cite{nagrani2021bottlenecksmultimodalfusion}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Video Transformers} 
\label{sub:video_transformers}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we present two modern Transformer-based architectures for video classification.
We start by introducing the TimeSformer architecture in Section~\ref{subsub:timesformer}, and then the ViViT architecture in Section~\ref{subsub:vivit}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{TimeSformer}
\label{subsub:timesformer}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[hbtp]
	\centering
	    \includegraphics[width=0.4\textwidth]{figures/timesformer_v2.pdf}
	    \caption{\textbf{TimeSformer architecture.} 
	    The TimeSformer first projects input to embedding tokens, which are summed to positional embedding tokens. The resulting tokens are then passed through $L$ divided space-time attention blocks and then linearly projected to obtain output probabilities.}
	\label{fig:timesformer}
\end{figure}

TimeSformer~\cite{bertasius2021timesformer} is one of the first architectures with space-time attention that impacted the video classification field. It follows the same structure and principle described in Section~\ref{subsub:general_principle}.

First, it takes as input an RGB video clip sampled at a rate of $1/32$ and decomposed into 2D $16 \times 16$ patches. 

As shown in Figure~\ref{fig:timesformer}, the TimeSformer architecture is based on the ViT architecture (Section~\ref{sub:vit}), with $12$ $12$-headed MSA layers. However, the added value compared to the ViT is that TimeSfomer uses the \textit{divided space-time attention} mechanism (Section~\ref{subsub:divided_space_time_attention}). Such attention mechanism enables to capture high-level spatio-temporal features, while taming the complexity of the model. 
%
Moreover, the authors introduce three variants of the architecture: (i) TimeSformer, the standard version of the model, that operates on $8$ frames of $224 \times 224$; (ii) TimeSformer-L, a configuration with high spatial resolution, that operates on $16$ frames of $448 \times 448$; and (iii) TimeSformer-HR, a long temporal range setup, that operates on $96$ frames of $224 \times 224$.

Finally, the terminal classification token embedding is passed through an MLP to output a probability for all video classes. During inference, the final prediction is obtained by averaging the output probabilities from three different spatial crops of the input video clip (top-left, centre and bottom-right). 

TimeSformer achieves similar state-of-the-art performances as the 3D CNNs~\cite{feichtenhofer2019slowfast,carreira2017i3d} on various video classification datasets, such as Kinetics-400 and Kinetics-600~\cite{kay2017kinetics}. Note, the TimeSformer is much faster to train (416 training hours against 3840 hours~\cite{bertasius2021timesformer} for a SlowFast architecture~\cite{feichtenhofer2019slowfast}), and also, more efficient ($0.59$ TFLOPs against $1.97$ TFLOPs~\cite{bertasius2021timesformer} for a SlowFast architecture~\cite{feichtenhofer2019slowfast}). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{ViViT}
\label{subsub:vivit}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[hbtp]
	\centering
	    \includegraphics[width=0.4\textwidth]{figures/vivit_v2.pdf}
	    \caption{\textbf{ViViT architecture.}
	    The ViViT first projects input to embedding tokens, which are summed to positional embedding tokens. The resulting tokens are first passed through $L_s$ spatial attention blocks and then through $L_t$ temporal attention blocks. The resulting output is linearly projected to obtain output probabilities.
	    }
	\label{fig:vivit}
\end{figure}

ViViT~\cite{arnab2021vivit} is the main extension of the ViT~\cite{dosovitskiy2020vit} architecture (Section~\ref{sub:vit})  for video classification.

First, the authors use a $16 \time 16 \time 2$ tubelet embedding instead of a 2D patch embedding, as mentioned in Section~\ref{subsub:general_principle}. This alternate embedding method aims to capture the spatio-temporal information from the tokenization step, unlike standard architectures that fuse spatio-temporal information from the first attention block.

As shown in Figure~\ref{fig:vivit}, the ViViT architecture is based on \textit{factorised encoder} architecture (Section~\ref{subsub:factorised_encoder}), and consists of one spatial and one temporal encoder operating on input clips with $32$ frames of $224 \times 224$. 
The spatial encoder uses one of the three ViT variants as backbone~\footnote{ViT-B: $12$ $12$-headed MSA layers; ViT-L: $24$ $16$-headed MSA layers; and ViT-H: $32$ $16$-headed MSA layers.}. For the temporal encoder, the number of layers does not impact much the performance, so that, according to the performance/complexity trade-off, the number MSA layers is fixed at $4$. 
The authors show that such architecture reaches high performances, while reducing drastically the complexity.

Finally, as in TimeSformer (Section~\ref{subsub:timesformer}), ViViT outputs probabilities for all video classes through the last classification token embedding, and averages the obtained probabilities across three crops of each input clip (top-left, centre and bottom-right).

ViViT outperforms both 3D CNNs~\cite{feichtenhofer2019slowfast,carreira2017i3d} and TimeSformer~\cite{bertasius2021timesformer} on the Kinetics-400 and Kinetics-600 datasets~\cite{kay2017kinetics}. 
Note, the complexity of this architecture is highly reduced in comparison to other state-of-the-art models. For instance, the number of FLOPs for a ViViT-L/$16 \times 16 \times 2$ is $3.89 \times 10^{12}$, against $7.14 \times 10^{12}$ for a TimeSformer-L~\cite{bertasius2021timesformer}, and $7.14 \times 10^{12}$ for a SlowFast~\cite{feichtenhofer2019slowfast} architecture.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Multimodal video Transformers} 
\label{sub:multi_modal}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Nowadays, one of the main gaps between artificial and human intelligence is the ability for us to process multimodal signals and to enrich the analysis by mixing the different modalities. Moreover, until recently, deep learning models have been focusing mostly on very specific visual tasks, typically based on a single modality, such as image classification~\cite{deng2009imagenet,krizhevsky2009cifar,dosovitskiy2020vit,wu2021cvt,touvron2021cait}, audio classification~\cite{gemmeke2017audioset,gong2021ast,nagrani2021bottlenecksmultimodalfusion,jaegle2021perceiver} and machine translation~\cite{bojar2014wmt,devlin2018bert,liu2020transformerBT,edunov2018backtranslation,lin2020pre}. These two factors combined have pushed researchers to take up multimodal challenges.

The \textit{default} solution for multimodal tasks consists in first creating an individual model (or network) per modality, and then in fusing the resulting single-modal features together~\cite{owens2018audio,ramanishka2016multimodal}.  
Yet, this approach fails to model interactions or correlations among different modalities.
However, the recent rise of attention~\cite{vaswani2017attention,dosovitskiy2020vit,arnab2021vivit} is promising for multimodal applications, since attention performs very well at combining multiple inputs~\cite{chen2022mmvit,jaegle2021perceiver,nagrani2021bottlenecksmultimodalfusion,narasimhan2021clipit}.
\\

Here, we present two main ways of dealing with several modalities: 

\paragraph{1. Concatenating tokens from different modalities into one vector~\cite{chen2022mmvit,jaegle2021perceiver}.}
The multimodal Video Transformer (MM-ViT)~\cite{chen2022mmvit} combines raw RGB frames, motion features and audio spectrogram for video action recognition. To do so, the authors fuse tokens from all different modalities into a single input embedding and pass it through Transformer layers. 
However, a drawback of this method is that it fails to distinguish well one modality to another. 
To overcome this issue, the authors of the Perceiver~\cite{jaegle2021perceiver} propose to learn a modality embedding in addition to the positional embedding (see Sections~\ref{sub:compute_efficiency} and \ref{subsub:general_principle}). This allows associating each token with its modality. 
Nevertheless, given that (i) the complexity of a Transformer layer is quadratic with respect to the number of tokens  (Section~\ref{subsub:general_principle}), and (ii) with this method, the number of tokens is multiplied by the number of modalities: it may lead to skyrocketing computational cost~\cite{chen2022mmvit}.

\paragraph{2. Exploiting cross attention~\cite{nagrani2021bottlenecksmultimodalfusion,narasimhan2021clipit,zadeh2019factorized}.}
Several modern approaches exploit cross attention to mix multiple modalities, such as~\cite{nagrani2021bottlenecksmultimodalfusion} for audio and video, \cite{narasimhan2021clipit} for text and video, and \cite{zadeh2019factorized} for audio, text and video. 
The commonality among all these methods is that they exploit the intrinsic properties of cross attention by querying one modality with a key-value pair from the other one~\cite{nagrani2021bottlenecksmultimodalfusion,narasimhan2021clipit}. 
This idea can be easily generalized to more than two modalities by computing cross-attention across each combination of modalities~\cite{zadeh2019factorized}.

