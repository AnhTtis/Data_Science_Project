Sections~\ref{sec:attention}-\ref{sec:extensions} introduce visual Transformers for one main application: classification. 
Nevertheless, Transformers can be used for numerous other tasks than classification. 

In this section, we present some fundamental vision tasks where Transformers have had a major impact: 
object detection in images (Section~\ref{sub:detection}), image segmentation (Section~\ref{sub:segmentation}), 
training visual Transformers without labels (Section~\ref{sub:no_labels}), and image generation using Generative Adversarial Networks (GANs) (Section~\ref{sub:generative_models}). 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Object detection with Transformers}
\label{sub:detection}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[hbtp]
	\centering
		\includegraphics[width=1.0\textwidth]{figures/detr.png}
	   \caption{\textbf{The DETR architecture.} It refines a CNN visual representation to extract object localization and classes. Figure inspired from~\cite{carion2020end}.
	   }
	\label{fig:detr}
\end{figure}

Detection is one of the early tasks that have seen improvements thanks to Transformers. 
Detection is a combined recognition and localization problem; this means that a successful detection system should both recognize whether an object is present in an image and localize it spatially in the image. \cite{carion2020end} is the first approach that uses Transformers for detection. 

\paragraph{DEtection TRansformer (DETR)~\cite{carion2020end}.}
DETR first extracts visual representations with a convolutional network (Figure~\ref{fig:detr})\footnote{Note that in DETR, the Transformer is not directly used to extract the visual representation. Instead, it focuses on refining the visual representation to extract the object information.
}. 
Then, the encodings are processed by a Transformer network. Finally, the processed tokens are provided to a Transformer decoder. The decoder uses cross-attention between a set of learned tokens and the image tokens encoded by the encoder and outputs a set of tokens. Each output token is then passed through a feed-forward network that predicts if an object is present in an image or not; if the object is indeed present, the network also predicts the class and spatial location of the object, i.e., coordinates within the image. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Image segmentation with Transformers}
\label{sub:segmentation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[hbtp]
	\centering
		\includegraphics[width=0.8\textwidth]{figures/segmenter.png} 
	   \caption{\textbf{The Segmenter architecture}. It is a purely ViT based approach to perform semantic segmentation. Figure inspired from~\cite{strudel2021segmenter}.}
	\label{fig:segmenter}
\end{figure}

The goal of image segmentation is to assign to each pixel of an image the label of the object it belongs to. 
The Segmenter~\cite{strudel2021segmenter} is a purely ViT approach addressing image segmentation. The idea is to first use ViT to encode the image. Then, the Segmenter learns a token per semantic label. The encoded patch tokens and the semantic tokens are then fed to a 2nd Transformer. Finally, by computing the scalar product between the semantic tokens and the image tokens, the network assigns a label to each patch. Figure~\ref{fig:segmenter} displays this. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Training Transformers without labels}
\label{sub:no_labels}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Visual Transformers have initially been trained for classification tasks. However, this tasks requires having access to massive amounts of labelled data, which can be hard to obtain (as discussed in Section~\ref{sub:data_efficiency}). Sections~\ref{sub:data_efficiency}-\ref{sub:compute_efficiency} present ways to train ViT more efficiently. However, it would also be interesting to be able to train this type of networks with ``cheaper'' data. Therefore, the goal of this part is to introduce unsupervised learning with Transformers, i.e., training Transformers without any labels. 

\begin{figure}[hbtp]
	\centering
		\includegraphics[width=0.6\textwidth]{figures/dino_v2.pdf}
	   \caption{\textbf{The DINO training procedure.} It consists in matching the outputs between two networks ($\bm{p}_1$ and $\bm{p}_2$) having two different augmentations ($\bm{X}_1$ and $\bm{X}_2$) of the same image as input ($\bm{X}$). The parameters of the teacher model are updated with an exponential moving average (ema) of the student parameters. 
	   Figure inspired from~\cite{dino}.}
	\label{fig:dino_arch}
\end{figure}

\begin{figure}[hbtp]
	\centering
		\includegraphics[width=\textwidth]{figures/DINO_examples.pdf}
	   \caption{\textbf{DINO samples.} Visualization of the attention matrix of ViT heads trained with DINO. The ViT discovers the semantic structure of an image in an unsupervised way.}
	\label{fig:dino_sample}
\end{figure}

\paragraph{Self-DIstillation with NO labels (DINO)~\cite{dino}.}
DINO is one of the first works that trains a ViT with self-supervised learning (Figure~\ref{fig:dino_arch}). The main idea is to have two ViT models following the teacher-student paradigm: the first model is updated through gradient descent and the second is an exponential moving average of the first one. 
Then, the whole two-stream DINO network is trained using two augmentations of the same image, that are each passed to one of the two networks. The goal of the training is to match the output between the two networks, i.e., no matter the augmentation in the input data, both networks should produce the same result. 
The main finding of DINO is that the ViT is capable of learning a semantic understanding of the image, as the attention matrices display some semantic information. Figure~\ref{fig:dino_sample} visualizes the attention matrix of the various ViT heads trained with DINO. 

\begin{figure}[hbtp]
	\centering
		\includegraphics[width=\textwidth]{figures/mae.pdf}
	   \caption{\textbf{The MAE training procedure}. After masking some tokens of an image, the remaining tokens are fed to an encoder. Then a decoder tries to reconstruct the original image from this representation. 
	   Figure inspired from~\cite{he2021masked}.  }
	\label{fig:mae_arch}
\end{figure}

\paragraph{Masked Autoencoders (MAE)~\cite{he2021masked}.}
Another way to train a ViT without supervision is by using an autoencoder architecture. Masked Autoencoders (MAE)~\cite{he2021masked} perform a random masking of the input token and give the task to reconstruct the original image to a decoder. The encoder learns a representation that performs well in a given downstream task. This is illustrated in Figure~\ref{fig:mae_arch}. 
One of the key observations of the MAE work~\cite{he2021masked} is that the decoder does not need to be very good for the encoder to achieve good performance: by using only a small decoder, MAE successfully trains a ViT in an auto-encoder fashion.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Image generation with Transformers and Attention}
\label{sub:generative_models}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Attention and Vision Transformers have also helped in developing fresh ideas and creating new architectures for generative models, and in particular for Generative Adversarial Networks (GANs). 

\begin{figure}[hbtp]
	\centering
		\includegraphics[width=0.4\textwidth]{figures/gansformers.png}
	   \caption{\textbf{GANsformers architecture}. A set of latents contribute to bring information to a CNN feature map. Figure inspired from~\cite{hudson2021generative}.}
	\label{fig:gansformers}
\end{figure}

\paragraph{GANsformers~\cite{hudson2021generative}.} 
GANsformers are the most representative work of GANs with Transformers, as they are a hybrid architecture using both attention and CNNs. 
The GANsformers architecture is illustrated in Figure~\ref{fig:gansformers}. 
The model first splits the latent vector of a GAN into multiple tokens. Then, a cross attention mechanism is used to improve the generated feature maps and, at the same time, the GANsformers architecture retrieves information from the generated features map to enrich the tokens. This mechanism allows the GAN to have better and richer semantic knowledge, which is showed to be useful for generating multimodal images.

\paragraph{StyleSwin~\cite{zhang2021styleswin}.}
Another approach for generative modeling is to purely use a ViT architecture like StyleSwin~\cite{zhang2021styleswin}. StyleSwin is a GAN that leverages a similar type of attention as the Swin Transformer~\cite{swin}. This allows to generate high definition images without having to deal with the quadratic cost problem.


