The Transformer architecture was introduced in~\cite{vaswani2017attention} and is the first architecture that relies purely on attention to draw connections between the inputs and outputs. 
Since its debut, it revolutionized Deep Learning, making breakthroughs in numerous fields, including Natural Language Processing, Computer Vision, Chemistry, Biology, thus making its way to becoming the \textit{default} architecture for learning representations. 
Recently, the standard Transformer~\cite{vaswani2017attention} has been adapted for vision tasks~\cite{dosovitskiy2020vit}. And again, visual Transformer has become one of the central architectures in computer vision.

In this section, we first introduce the basic architecture of Transformers (Section~\ref{sub:basic_transformer}) and then present its advantages (Section~\ref{sub:advantage_transformers}). Finally, we describe the Vision Transformer (Section~\ref{sub:vit}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Basic Transformers}
\label{sub:basic_transformer}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[hbtp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/transformer_v2.pdf}
    \caption{\textbf{The Transformer architecture.} It consists of an encoder (left) and a decoder (right) block, each one consisting from a series of attention blocks (Multi-head and Masked Multi-head attention) and MLP layers. Next to each element, we denote its dimentionality. Figure inspired from~\cite{vaswani2017attention}. 
    }
    \label{fig:transformer_arch}
\end{figure}

As shown in Figure~\ref{fig:transformer_arch}, the Transformer architecture~\cite{vaswani2017attention} is an encoder-decoder model. First, it embeds input tokens $\bm{X} = (\bm{x_1}, \dots, \bm{x_N})$ into a latent space, resulting in latent vectors $\bm{Z} = (\bm{z_1}, \dots, \bm{z_N})$, which are fed to the decoder to output $\bm{Y} = (\bm{y_1}, \dots, \bm{y_M})$. 
The encoder is a stack of $L$ layers, with each one consisting of two sub-blocks: Multi-head Self-Attention (MSA) layers and a Multi-Layer Perceptron (MLP). 
The decoder is also a stack of $L$ layers, with each one consisting of three sub-blocks: Masked Multi-head Self-Attention (MMSA),  Multi-head Cross-Attention (MCA), and a MLP.

\paragraph{Overview.}
Below, we describe the various parts of the Transformer architecture, following Figure~\ref{fig:transformer_arch}. 
First, the input tokens are converted into the embedding tokens (Section~\ref{subsub:embedding}). 
Then, the Positional encoding adds a positional token to each embedding token to denote the order of tokens (Section~\ref{subsub:positional_encoding}). 
Then, the Transformer encoder follows (Section~\ref{subsub:transformer_encoder}). This consists of a stack of $L$ multi-head attention, nomalization and MLP layers and encodes the input to a set of semantically meaningful features. 
After, the decoder follows (Section~\ref{subsub:transformer_decoder}). This consists of a stack of $L$ masked multi-head attention, multi-head attention, and MLP layers followed by normalizations and decodes the input features with respect to the output embedding tokens. 
Finally, the output is projected to linear and Softmax layers. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Embedding}
\label{subsub:embedding}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The first step of Transformers consists in converting input tokens\footnote{Note, the initial Transformer architecture was proposed for natural language processing (NLP), and therefore the inputs were words.} into embedding tokens, i.e., vectors with meaningful features. 
To do so, following standard  practice~\cite{word_embedding_Press_2016}, each input is projected into an embedding space to obtain embedding tokens $\bm{Z^{\text{e}}}$. 
The embedding space is structured in a way that the distance between a pair of vectors is relative to the semantic similarity of their associated words. For the initial NLP case, this means that we get a vector of each word, such that the vectors that are closer together have similar meanings.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Positional Encoding}
\label{subsub:positional_encoding}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As discussed in Section~\ref{sub:property_attention}, the attention mechanism is positional agnostic, which means that it does not store the information on the position of each input. 
However, in most cases, the order of input tokens is relevant and should be taken into account, such as the order of words in a sentence matter as they may change its meaning. 
Therefore, \cite{vaswani2017attention} introduced the \emph{\textbf{P}ositional \textbf{E}ncoding} $\textbf{PE}\in\mathbb{R}^{N \times d_x}$, which adds a positional token to each embedding token $\bm{Z^{\text{e}}} \in\mathbb{R}^{N \times d_x}$. 

\paragraph{Sinusoidal Positional Encoding.}
The Sinusoidal Positional Encoding~\cite{vaswani2017attention} is the main positional encoding method, which encodes the position of each token with sinusoidal waves of multiples frequency. For an embedding token $\bm{Z^{\text{e}}} \in\mathbb{R}^{N\times d_x}$, its Positional Encoding $\textbf{PE}\in\mathbb{R}^{N\times d_x}$ is defined as: 

\begin{equation}
\label{eq:positional_encoding}
\begin{array}{l}
    \textbf{PE}(i,2j)   = \text{sin} \left(\frac{i}{10000^{2j/d}} \right) \\
    \textbf{PE}(i,2j+1) = \text{cos} \left(\frac{i}{10000^{2j/d}} \right), \forall i,j \in [|1,n|]\times[|1,d|] \quad .
\end{array}
\end{equation}

\paragraph{Learnable Positional Encoding.}
An orthogonal approach is to let the model learn the positional encoding. In this case, $\textbf{PE}\in\mathbb{R}^{N\times d_x}$ becomes a learnable parameter. This, however, increases the memory requirements, without necessarily bringing improvements over the sinusoidal encoding.

\paragraph{Positional Embedding.}
After its computation, the Positional Encoding \textbf{PE} is either added to the embedding tokens, or they are concatenated as follows: 

\begin{equation}
\label{eq:positional_embedding}
\begin{array}{l}
    \bm{Z}^{\text{pe}} = \bm{Z^{e}}+\bm{\textbf{PE}} \quad, \text{ or} \\
    \bm{Z}^{\text{pe}} = \text{Concat}(\bm{Z^{e}}, \bm{\textbf{PE}}) \quad, 
\end{array}
\end{equation}


\noindent where Concat denotes vector concatenation. 
Note that the concatenation has the advantage of not altering the information contained in $\bm{Z}^e$, since the positional information is only added to the unused dimension. Nevertheless, it augments the input dimension, leading to higher memory requirements.
Instead, the addition does preserve the same input dimension, while altering the content of the embedding tokens. When the input dimension is high, this content altering is trivial, as most of the content is preserved. 
Therefore, in practice, for high-dimension summing positional encodings is preferred, whereas for low dimensions, concatenating them prevails. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Encoder Block}
\label{subsub:transformer_encoder}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The encoder block takes as input the embedding and positional tokens and outputs features of the input, to be decoded by the decoder block. It consists of a stack of $L$ Multi-head Self-Attention (MSA) layers and a Multi-Layer Perceptron (MLP). Specifically, the embedding and positional tokens, $\bm{Z}_{x}^{\text{pe}} \in \mathbb{R}^{N \times d}$, go through a multi-head self-attention block. Then, a residual connection with layer normalization is deployed. In the Transformer, this operation is performed after each sub-layer. Next, we feed its output it to a MLP and a normalization layer. This operation is performed $L$ times and each time the output of each encoder block (of size $N \times d$) is the input of the subsequent block. The $L-$th time, the output of the normalization is the input of the cross attention block in the decoder (Section~\ref{subsub:transformer_decoder}). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Decoder Block}
\label{subsub:transformer_decoder}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The decoder has two inputs: first, an input that constitutes the queries $\bm{Q} \in \mathbb{R}^{N \times d}$ of the encoder, and, second, the output of the encoder that constitutes the key-value $\bm{K}, \bm{V} \in \mathbb{R}^{N \times d}$ pair. Similar to Sections~\ref{subsub:embedding}-\ref{subsub:positional_encoding}, the first step constitutes encoding the output token to output embedding token and output positional token. These tokens are fed into the main part of the decoder, which consists of a stack of $L$ Masked Multi-head Self-Attention (MMSA) layers, Multi-Head Cross-Attention (MCA) layers, and Multi-Layer Perceptron (MLP) followed by normalizations. Specifically, the embedding and positional tokens, $\bm{Z}_{y}^{\text{pe}} \in \mathbb{R}^{N \times d}$, go through a MMSA block. Then, a residual connection with layer normalization follow. Next, a MCA layer (followed by a normalization) maps the queries to the encoded keys-values before forwarding the output to a MLP. Finally, we project the output of the $L$ decoder blocks (of dimension $N \times d_y$) through a linear layer and get output probability through a soft-max layer.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Advantages of Transformers}
\label{sub:advantage_transformers}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Since their introduction, the Transformers have had a significant impact on deep learning approaches. 

In natural language processing (NLP), before Transformers, most  architectures used to rely on recurrent modules, such as RNNs~\cite{cho_RNN_translation_2014} and in particular LSTMs~\cite{Sutskever_LSTM_Translation_2014}. 
However, recurrent models process the input sequentially, meaning that, to compute the current state, they require the output of the previous state. This makes them tremendously inefficient, as they are impossible to parallelize.
On the contrary, in Transformers, each input is processed independent of the others, and the multi-head attention can perform multiple attention computations at once. This makes Transformers highly efficient, as they are highly parallelizable.  

This results in not only exceptional scalability, both in the complexity of the model and size of datasets, but also relatively fast training. Notably, the recent Switch Transformers~\cite{fedus_switch_2021} was pre-trained on 34 billion tokens from the C4 dataset~\cite{RAffel_C4_dataset_2020}, scaling the model to over 1 trillion parameters.

This scalability~\cite{fedus_switch_2021} is the principal reason for the power of the Transformer. While it was originally introduced for translation, it refrains from introducing many inductive biases, i.e., the set of assumptions that the user makes about the structure of the model input. In doing so, the Transformer relies on data to learn how they are structured. Compared to its counterparts with more biases, the Transformer requires much more data to produce comparable results~\cite{dosovitskiy2020vit}. However, if a sufficient amount of data is available, the lack of inductive bias becomes a strength. By learning the structure of the data from the data, the Transformer is able to learn better without human assumptions hindering~\cite{khan_vision_survey_2021}.

In most tasks involving Transformers, the model is first pre-trained on a large dataset, and then fine-tuned for the task at hand on a smaller dataset. The pre-training phase is essential for Transformers to learn the global structure of the specific input modality. For fine-tuning, typically fewer data suffice as the model is already rich. 
For instance, in natural language processing, BERT~\cite{devlin2018bert}, a state-of-the-art language model, is pre-trained on a Wikipedia-based dataset \cite{wikidump}, with over 6 million articles and Book Corpus \cite{zhu_corpus_2015} with over 10,000 books. Then, this model can be fine-tuned on much more specific tasks.
In Computer Vision, the Vision Transformer (ViT) is pre-trained on the JFT-300M dataset, containing over one billion labels for 300 million images \cite{dosovitskiy2020vit}.
Hence, with a sufficient amount of data, Transformers achieve results that were never possible before in various areas of Machine Learning.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Vision Transformer}
\label{sub:vit}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Transformers offer an alternative to CNNs that have long held a stranglehold on Computer Vision.  
Before 2020, most attempts to use Transformers for vision tasks were still highly reliant on CNNs, either by using self-attention jointly with convolutions~\cite{wang_vision_2017,carion_vision_2020} 
or by keeping the general structure of CNNs while using self-attention~\cite{Ramachandran_vision_2019,wang_vision_2020}.

The reason for this is rooted in the two main weaknesses of the Transformers. First, the complexity of the attention operation is high. As attention is a quadratic operation, the number of parameters skyrockets quickly when dealing with visual data, i.e., images --and even more so with videos--. For instance, in the case of ImageNet~\cite{deng2009imagenet}, inputting a single image with $256 \times 256 = 65,536$ pixels in an attention layer would be too heavy computationally. Second, Transformers suffer from lack of inductive biases. Since CNNs were specifically created for vision tasks, their architecture includes spatial inductive biases, like translation equivariance and locality. Therefore, the Transformers have to be pre-trained on a significantly large dataset to achieve similar performances.

\begin{figure}%[hbtp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/vit_v2.pdf}
    \caption{\textbf{The Vision Transformer architecture (ViT).} First, the input image is split into patches (bottom), which are linearly projected (embedding), and then concatenated with positional embedding tokens. The resulting tokens are fed into a Transformer, and finally the resulting classification token is passed through an MLP to compute output probabilities.
    Figure inspired from~\cite{dosovitskiy2020vit}.}
    \label{fig:vit_arch}
\end{figure}

The Vision Transformer (ViT)~\cite{dosovitskiy2020vit} is the first systematic approach that uses directly Transformers for vision tasks by addressing both aforementioned issues. It rids the concept of convolutions altogether, using purely a Transformer-based architecture. In doing so, it achieves the state of the art on image recognition on various datasets, including ImageNet~\cite{deng2009imagenet} and CIFAR-100~\cite{krizhevsky2009cifar}. 

Figure~\ref{fig:vit_arch} illustrates the ViT architecture. 
The input image is first split into $16 \times 16$ patches, flattened and mapped to the expected dimension through a learnable linear projection. Since the image size is reduced to $16 \times 16$, the complexity of the attention mechanism is no longer a bottleneck. Then, ViT encodes the positional information and attaches a learnable embedding to the front of the sequence, similarly to BERTâ€™s classification token~\cite{devlin2018bert}. The output of this token represents the entirety of the input -- it encodes the information from each part of the input. Then, this sequence is fed into an encoder block, with the same structure as in the standard Transformers~\cite{vaswani2017attention}. The output of the classification token is then fed into an MLP that outputs class probabilities.

Due to lack of inductive biases, when ViT is trained only on mid-sized datasets such as ImageNet, it scores some percentage points lower than the state of the art. Therefore, the proposed model is first pre-trained on the JFT-300M dataset~\cite{sun_jft300m_2017} and then fine-tuned on smaller datasets, thereby increasing its accuracy by $13\%$. 

For a complete overview of visual Transformers and follow-up works, we invite the readers to study~\cite{khan_vision_survey_2021,selva_video_survey_2022}. 

