%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/

%\documentclass[journal]{IEEEtran}
\documentclass[letterpaper, 10 pt, journal, twoside]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}
% \usepackage{indentfirst} 

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{color}
\usepackage{multirow}

\usepackage{soul}
\usepackage{url}
\usepackage{hyperref}

\usepackage{verbatim}
\usepackage{subfigure}
\usepackage{calc}
\usepackage{amstext}


\newcommand{\red}[1]{\textcolor{red}{#1}}


\usepackage{stackengine} 
\newcommand\oast{\stackMath\mathbin{\stackinset{c}{0ex}{c}{0ex}{\ast}{\bigcirc}}}

\newcommand\IncG[2][]{\addstackgap{%
\raisebox{-.5\height}{\includegraphics[#1]{#2}}}}
\usepackage{array}
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the grad camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}



% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommeroverfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.

% \title{Explicit Attention-Enhanced RGB-Thermal Fusion for Perception Tasks}
\title{Explicit Attention-Enhanced Fusion for RGB-Thermal Perception Tasks}

\author{Mingjian Liang$^{*}$, Junjie Hu$^{*}$, Chenyu Bao, Hua Feng, Fuqin Deng and Tin Lun Lam$^{\dagger} $% <-this % stops a space
% \thanks{*This work was not supported by any organization}% <-this % stops a space
% \thanks{This paper is partially supported by funding 2019-INT008 from the Shenzhen Institute of Artificial Intelligence and Robotics for Society.}% <-this % stops a space
\thanks{J. Hu, H. Feng, F. Deng, T.L. Lam are with the Shenzhen Institute of Artificial Intelligence and Robotics for Society.}%
\thanks{M. Liang, C. Bao and T.L. Lam are with the School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen.}%
\thanks{$*$ indicates equal contribution.}
\thanks{$^{\dagger}$Corresponding author: Tin Lun Lam
        {\tt\small tllam@cuhk.edu.cn}
        }%
}


% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}

    Recently, RGB-Thermal based perception has shown significant advances. Thermal information provides useful clues when visual cameras suffer from poor lighting conditions, such as low light and fog. However, how to effectively fuse RGB images and thermal data remains an open challenge. Previous works involve naive fusion strategies such as merging them at the input, concatenating multi-modality features inside models, or applying attention to each data modality. 
    These fusion strategies are straightforward yet insufficient. 
    In this paper, we propose a novel fusion method named Explicit Attention-Enhanced Fusion (EAEF) that fully takes advantage of each type of data. Specifically, we consider the following cases: i) both RGB data and thermal data, ii) only one of the types of data, and iii) none of them generate discriminative features. EAEF uses one branch to enhance feature extraction for i) and iii) and the other branch to remedy insufficient representations for ii).
    The outputs of two branches are fused to form complementary features. As a result, the proposed fusion method outperforms state-of-the-art by 1.6\% on semantic segmentation, 2.3\% on object detection, and 1.19\% on crowd counting. The code is available at https://github.com/FreeformRobotics/EAEFNet.

\end{abstract}

% Note that keywords are not normally used for peerreview papers.
% \begin{IEEEkeywords}
% IEEE, IEEEtran, journal, \LaTeX, paper, template.
% \end{IEEEkeywords}
\begin{IEEEkeywords}
Multi-modality data fusion, RGB-Thermal fusion, RGB-thermal perception
\end{IEEEkeywords}





% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\section{Introduction}
% {\color{blue}
Over the last decade, we have witnessed significant progress on many perception tasks. Based on data-driven learning, deep neural networks (DNNs) can learn to estimate semantic maps \cite{FCN,UNet,DeepLab,pspnet,upernet,gcnet}, object categories \cite{FasterRCNN,Yolo,cascadeRCNN,FCOS,TODO}, depth maps \cite{NYU,KITTI,Hu2019RevisitingSI,jiang2021plnet}, etc., from only RGB images. This paradigm has continuously boosted perception tasks for robots in which various models, loss functions, and learning strategies have been explored.


However, current methods highly depend on the quality of RGB images. In reality, visual cameras are particularly susceptible to noises \cite{suganuma2019attention}, poor lighting \cite{hu2021two}, weather \cite{liu2019dual}, etc. In these cases, DNNs tend to degrade their performance significantly. To handle these issues, researchers sought to employ thermal data to complement RGB images and developed different multi-modality fusion strategies for RGB-T based perception.

The core of RGB-T based methods is the fusion strategy of RGB data and thermal data. Previous methods \cite{UCNet,m3fd} directly combine them at the input. Some works \cite{MFNet,RTFNet} use two separate encoders for extracting features from RGB and thermal images, respectively. Then, these features are merged and outputted to a decoder to yield a final prediction.  
 Recently, most studies \cite{ABMDRNet,FEANet} attempted to utilize the attention mechanism for multi-modality data fusion. These approaches commonly apply channel attention to intermediate features of different data types and obtain the fused features by weighing their importance. However, these fusion strategies are implicit and insufficient. In particular, it is unclear how multi-modality data can (or cannot) complement each other.
 % {\color{red} How Add, CWF and FEAM fuse RGB and thermal data? }

\begin{figure}[t!]
    \centering
    \includegraphics [width=0.48\textwidth]{Fig/Fig 1}
    % \vspace{-3mm}
    \caption{Visualization of extracted features of RGB, thermal images, and the proposed fusion method, where (a) both RGB and thermal data, (b) only one of them, and (c) none, can yield distinct features. As seen, the proposed fusion method can boost feature extraction for all three cases.}
    
    % Illustration of modality difference reduction. ((a),(b),(c)) represent the cars and pedestrians that need to be identified in the original RGB , Thermal and Ground Truth, ((d),(g)) grad cam\cite{grad-cam} after ResNet\cite{ResNet} feature extraction, ((e),(f),(h),(i)) respectively grad cam after Add\cite{RTFNet}, FEAM\cite{FEANet}, CWF\cite{ABMDRNet}, IDAM}
    \label{fig_intro}
\end{figure}
% {\color{red}
Different from existing studies, we explicitly take multi-modality data fusion under three circumstances: i) both RGB and thermal images can extract useful features, as Fig.~\ref{fig_intro} (a), ii) only one of them can generate meaningful representations, as Fig.~\ref{fig_intro} (b), and iii) none of them provides useful features, as Fig.~\ref{fig_intro} (c). 
In this paper, we propose the Explicit Attention-Enhanced Fusion (EAEF) that performs a more effective fusion.
The key inspiration of EAEF is the case-specific design that uses one branch to stick to meaningful representations for i) and enhance feature extraction for iii), and the other branch to force CNNs to pay attention to insufficient representations for ii). One of these branches will generate useful features at least, and their combination will yield complementary features for a final prediction.


 \begin{figure*}[t!]
    \centering
    \includegraphics [width=0.96\textwidth]{Fig/Fig 2}
    % \vspace{-3mm}
    \caption{Diagram of encoder-decoder based network assembled with the proposed EAEF for dense prediction tasks. EAEF is used for fusing features from the RGB branch and the thermal branch at multi-scales. }
    \label{fig_network}
    % \vspace{-5mm}
\end{figure*}

To validate the effectiveness of this novel multi-modality fusion method, we design a novel RGB-T framework by integrating EAEF into an encoder-decoder network and evaluate it on various vision tasks with benchmark datasets, including semantic segmentation, object detection, and crowd counting. We confirm through extensive experiments that our method is more effective for RGB-T based vision perception.


In summary, our contributions are:
\begin{itemize}
    \item A novel multi-modality fusion method that effectively fuses RGB features and thermal features in an explicit manner.
    \item An effective encoder-decoder based network assembled with the proposed feature fusion strategy for dense prediction tasks.
    \item State-of-the-art performance on semantic segmentation, object detection, and crowd counting with open-source codes.
\end{itemize}
% }


The remainder of this paper is organized as follows. Section II reviews related studies. Section III presents the framework and the proposed multi-modality data fusion method. Section IV provides quantitative and qualitative experimental results on three tasks. Section V concludes our work. 


% \section{Introduction}
% \label{sec:intro}
% Over the past decade, one of the worthy topics in computer vision is multi-modality imaging, which is attracted significant attention in a wide range of applications for complex and changeable scenes. In visible light mode, they can provide limited information when the target object is located in darkness, exposure, fog, and other extreme conditions. In contrast, Thermal information can add to the work as supplementary information. With the rise of RGB-T fusion work in recent years, many solutions have emerged to better complete the semantic segmentation of RGB-T urban street scenes, which is an essential task in autonomous driving. The captured scene information discrepancy from the other physical imaging principles of RGB and Thermal. It poses a considerable challenge to our multimodal fusion of RGB-T.



% The previous methods\cite{ABMDRNet}\cite{RTFNet}\cite{MFNet}\cite{EGFNet}\cite{CMX} realize the problem of incomplete modal fusion caused by modal differences. Although the situation is recognized, the problem's core of the modal difference composition needs to be studied more. As a result, the RGB-T fusion task has entered a bottleneck period. Our study discusses two types of modal differences, complementary differences and decision difference.\red{The complementary difference is due primarily to the information asymmetry between modalities leading to the dominant modality. (the yellow frame in Figure a has no semantic information about pedestrians, and the yellow box in Figure b identifies the semantic information about pedestrians). The decision difference is mainly due to the problem of the modal domain gap caused by the inconsistency in semantic understanding of the different modalities. ((d), (g), the RGB and Thermal modalities activate at the same time in the position in the red box, but the semantic information of the activated area is inconsistent) There is an asymmetry in semantic information between the two modalities. After obtaining the feature maps of RGB and T, the standard method is the element-by-element summation or connection fusion. This idea is relatively simple. One of the problems may introduce noise information. Some practices with attention modules are used to optimize the quality of image feature information to alleviate these issues. However, they still cannot effectively address the problem of modal domain gaps. ((e) and (h) both supplement the semantic information of pedestrians for RGB modalities, but for the red box area, due to the semantic information bias brought by modal complementarity, it is not activated)}

% In order to further solve the problem of the domain gap, some works use the means of decision-making between modalities to learn, select the appropriate channel information through weighted decision-making and perform element summation. However, this method can be effectively improved. The learned weights are unstable, which may weaken the information between modalities, resulting in the loss of information (as shown in (f), the red box is effectively activated. The Partial complementary activation is weak). We propose a new multimodal feature fusion method, namely image decision-making, and complementation, which can obtain the feature maps of each layer of RGB stem and T stem, respectively, after being processed by ResNet branches. The channel information contains much semantic information. We will compress the spatial dimension of the feature map and extract the semantic information in the feature map. After obtaining the respective semantic information of RGB and T, we obtain RGB and T through the similarity of semantic information. The activation similarity of the channel information between the feature maps distinguishes the modal dominant and the modal domain gap. The designed decision module processes the modal domain gap to select the correct semantic information. Finally, it is input to the spatial module together with the complementary modal information and further fused in the spatial dimension. ((i) shows that after the IDAM structure processing, the yellow and red frames in the feature map are reasonably activated).

% In addition, the size of the targets in the scene is also different. In order to optimize this kind of problem more reasonably, the Turbo-decoder is similar to the principle that turbo uses the inertial momentum of the exhaust gas discharged from the engine to pressurize the air into the cylinder. Consistently, the Turbo Decoder uses the global information extracted in the high-dimensional feature layer to perform soft supervision on the local information in a weighted manner so that the local feature information can restore more efficiently and the utilization of information is improved. In addition, it is worth mentioning that Turbo Decoder has an excellent parameter amount, which is one-tenth of the previous FEANet Decoder parameter amount.

% The core contributions of the paper are summarized as followsï¼š

% (1) We design an end-to-end FATNet to deal simultaneously with the disparity of modality fusion and the combination of multi-scale contextual information. Comprehensive experimental results show that the model performs well on the MFNet\cite{MFNet} dataset.

% (2) We demonstrate two types of modality differences that exist in the fusion process of RGB and Thermal and propose an IDAM to solve this challenge.
%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Related Work}
\label{sec_related_work}
% \textbf{RGB-Thermal:} Multi-modal representation learning aims at comprehending, representing, and fusion cross-modal data through deep learning. The RGB-Thermal(RGB-T) is one of them. In recent studies, RGB-T has been used in downstream tasks to strengthen the accuracy of the model.

% \textbf{RGB-Thermal based perception tasks:}


Most vision tasks aim to make predictions from only RGB images. However, they suffer from low accuracy, robustness, and generability when images are captured under poor lighting conditions, noises, motion blur, etc. A promising solution is to utilize multi-modality data to complement RGB images. A representative is depth maps \cite{Fusenet,he2017std2p,seichter2021efficient,Hu2022DeepDC} that boost the new design of deep neural networks, loss functions, and learning strategies.
% Recently, thermal data has captured great attention for tasks such as object detection, crowd counting, and semantic segmentation. 

Recently, there has been a trend that introduces thermal data into perception tasks. The key point of these methods is the fusion strategy between RGB images and thermal images.
% {\color{red}
We can mainly categorize the current methods into two types according to whether attention mechanisms are leveraged in the fusion process.

Regarding non-attention methods, MFNet \cite{MFNet} is an early attempt where features from both modalities are extracted by a dual-encoder structure and fused into the decoder by symmetry shortcut blocks. RTFNet \cite{RTFNet} proposes an auxiliary branch for extracting thermal features, which are then element-wise summed into the main RGB branch at different spatial scales. Based on the RTFNet's structure, FuseSeg \cite{FuseSeg} further introduces skip-connection modules for improvement. Besides the dual-encoder structure, PST900 \cite{pst900} decomposes the prediction process into two stages. It makes a coarse prediction from one single RGB image in the first stage and leverages thermal input for refinement in the second stage. TarDAL \cite{m3fd} fuses the two modalities in an early fusion manner and achieves impressive performance.

As mentioned before, utilizing the attention mechanism for cross-modal fusion has become a new trend recently. ABMDRNet \cite{ABMDRNet} fuses the two modalities by implementing channel attention in their CWF (channel-wise fusion) module. FEANet \cite{FEANet} further adds a spatial attention operation right after channel attention for recovering some detailed structures. A similar combination of spatial and channel attention is also introduced in the Shallow Feature Fusion Modul (SFFM) from GMNet \cite{GMNet} but implemented more delicately. MFTNet \cite{MFTNet} resorts to the currently popular self-attention to improve the ability of feature fusion.

In this work, we propose an explicit attention-enhanced mechanism that analyzes the modal difference and takes full advantage of the modal fusion on multiple perception tasks.


%---------------------------second ver------------------------
%Although there might be some exceptions like PST900 \cite{pst900} that first makes a coarse prediction from a single RGB image and leverages thermal input for refinement in the second stage, the main fusion methods can be classified into two types in general: (1) complementary fusion: the type of methods still makes a prediction based on features from RGB but take thermal as an auxiliary complement. (2) equal fusion: this line of methods assumes equivalent importance between thermal and RGB to make the final prediction.

%The representative of the first fusion strategy is RTFNet \cite{RTFNet}, where an auxiliary branch for thermal is proposed, and feature maps from different spatial levels are element-wise summed into the main RGB branch. Based on the RTFNet's structure, FuseSeg \cite{FuseSeg} further introduces skip-connection modules for improvement. In order to better extract the complementary features, a feature-enhanced attention module is proposed by FEANet \cite{FEANet}.

%MFNet \cite{MFNet} is the pioneer when it comes to the second fusion strategy, where features from both modalities are equally extracted by a dual-encoder structure and fused into a decoder by the symmetry shortcut block. TarDAL \cite{m3fd} fuses RGB-T images in an early-fusion manner via the auto-encoder. However, these fusion strategies are quite naive. In order to reduce the difference between the two modalities, designing a specific module to generate an intermediate feature between RGB and thermal becomes a popular option. 
%A channel-wise fusion module is proposed in ABMDRNet\cite{ABMDRNet}. With a similar structure, GMNet \cite{GMNet} proposes the DFF module and further skip-connects different level intermediate features with the corresponding decoder. The latest MFTNet \cite{MFTNet} resorts to a vision transformer to improve the ability of feature fusion.
%The above two fusion strategies have their own pros and cons. Complementary fusion can help the network gather some specific features only appeared in single modality while equal fusion facilitates the network to extract common features but smears specific characteristics from different modality.
%------------------first ver-------------------
%MFNet \cite{MFNet} extracts RGB and Thermal features by two equal encoders consisting of the mini-inception module and fuses the extracted features by the shortcut blocks in decoding stage.
%RTFNet \cite{RTFNet} at first, the encoder adopted the method of element-wise summation that integrated features from Thermal to the RGB and then Upsample blocks in the decoder ameliorated to decoding results.
%RTFNet \cite{RTFNet}, in the first time, fuses the RGB and Thermal features by element-wise summation and further proposes Upsample blocks to improve the final result.
%Based on RTFNet, FuseSeg \cite{FuseSeg} replaces the backbone with DenseNet\cite{densenet} and introduces skip-connect module in the decoder.
%that supplies the low-level feature on upsample phase. 
%Pst900 \cite {pst900} first makes a coarse prediction from a single RGB image and leverages thermal input for refinement in the second stage.
%concatenates coarse prediction predicted by RGB stream with thermal image as a new input for inference by the dual-stream CNN.
%FEANet \cite{FEANet} introduces a feature-enhanced Attention extracted from the symmetry encoder to improve the quality of RGB and Thermal.
%ABMDRNet \cite{ABMDRNet} attempts to reduce the modal difference and fuse cross-modal features via incorporating a channel-wise module into the encoder.
%IADM \cite{iadm} designs a modality-shared branch to extract the complementary information of different modalities.
%EGFNet \cite{EGFNet} - GMNet \cite{GMNet} combines specific multilevel characteristics in decoder.
%TarDAL \cite{m3fd} fuses the RGB-T image in an early-fusion manner via the auto-encoder.
%MFTNet \cite{MFTNet} utilizes the vision-transformer framework to improve the ability of feature extraction.

%Although most networks have already been aware of the modal difference, more profound awareness is still required. Most modality fusion methods can be divided into two types: (1) Directly merge the modality information via element-wise summation or multiplication like \cite{RTFNet, FuseSeg, FEANet, GMNet, EGFNet,iadm} do. (2) Learn an intermediate feature between two modality features like \cite{ABMDRNet,MFTNet,cmx} do. The above strategies need to locate the composition of the modal difference accurately. In this work, we propose an explicit attention-enhanced mechanism that analyzes the modal difference and takes full advantage of the modal fusion on discrimination tasks.
% } 


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\textwidth]{Fig/eaef.pdf}
    \vspace{-2mm}
    \caption{Overview of the proposed Explicit Attention-Enhanced Fusion (EAEF). EAEF takes RGB features $F_{rgb}$ from the RGB encoder and thermal features $F_t$ from the thermal encoder as inputs, then applies attention interaction and attention complement with two branches. The final features are fused by merging outputs from these two branches. For simplicity, we only show one feature map extracted from the image encoder and the thermal encoder, respectively.
    % The inputs are an RGB steam feature and T steam feature.$\text { Then } \text { number }_i=\sum_{u, v} \mathbb{1}\left[\arg \max _c s_{u, v}^{(c)}=i\right]$ \cite{PKD}
    }
    \label{fig_eaef}
    % \vspace{-5mm}
\end{figure*}

%------------------------------------------------------------------------
\section{Methodology}
\label{sec:intro}


 \subsection{Framework Overview}
 We take the classical encoder-decoder network for dense prediction tasks. The framework consists of an image encoder, a thermal encoder, and a shared decoder.
 Similar to existing approaches, the RGB encoder is used to extract features from RGB images and the thermal encoder is used to extract features from thermal images. The proposed Explicit Attention-Enhanced Fusion (EAEF) is applied between the two encoders to fuse features at multi-scales. A diagram of our dense prediction network is given in Fig.~\ref{fig_network}, where we show a semantic segmentation network built on ResNet \cite{ResNet}. 
 The framework uses five convolutional blocks to extract multi-scale features; thus, we apply five EAEF modules to fuse RGB and thermal features. 
 
 Note that the framework naturally uses different backbones on different tasks.
 Therefore, the detailed implementation is task-specific. Nevertheless, all models built on our framework have the same technical components, i.e., one RGB encoder, one thermal encoder, EAEF modules, and a shared decoder.
 % In experiments, we will use this framework to evaluate the effectiveness of our method on three different vision tasks.
 
 % In this section, we first introduce the overall architecture of FATNet, which consists of two feature encoder streams and decoder steam, as Fig2 shows. Since the Encoder-Decoder structure has been proven to be an effective architecture in many tasks, our FATNet also adopts it. We propose that IDAM fuse multi-level features from different modalities to achieve superior segmentation performance in the semantic segmentation, object detection, and crow counting of urban scenes.



\subsection{Explicit Attention-Enhanced Fusion}

% EAEF takes inputs of two types of features, i.e., RGB features from an RGB encoder and thermal features from a thermal encoder, and outputs the fused features. 
% EAEF can be specifically decomposed into three steps as follows.




% \subsubsection{Channel-wise Attention Extraction}

% To solve the problem of modal differences, we propose a novel Image Decision and addition module (IDAM) to distinguish modal differences scenes into two types: decision differences and complementary differences. The IDAM performs more personalized processing on the two kinds of differences. As shown in Fig 3, IDAM consists of three main modules: Contrasting RGB and T Matrix (CRTM), Decision Module, and Spatial Module. CRTM aims to distinguish between modal dominance and domain gap scenarios, which offer a holistic calibration, enabling better multi-modal feature extraction and interaction.


%------------------------------------------------------------------------

% \subsubsection{Connect RGB and T matrix}
Suppose $F_{rgb}\in\mathbb{R}^{h \times w \times c}$ and $F_{t}\in\mathbb{R}^{h \times{w} \times{c}}$ are the features extracted from RGB encoder and thermal encoder at a certain scale, we first 
quantify whether $F_{rgb}$ or $F_{t}$ contains sufficiently discriminative features.
% extract their channel-wise attention. Following the common strategy \cite{clip},
We apply the global average pooling to $F_{rgb}$ and $F_{t}$ along the channel dimension and then apply an MLP to obtain the weights as follows:

\begin{equation}
 \begin{aligned}
& R = \biggl( f_{MLP} \Bigl( f_{GAP}(F_{rgb}) \Bigr) \biggr) \\
& T = \biggl( f_{MLP} \Bigl( f_{GAP}(F_{t}) \Bigr) \biggr)
 \end{aligned}
 \label{eq_at}
\end{equation}
where $R \in\mathbb{R}^{c}$ and $T \in\mathbb{R}^{c}$ are extracted weights for RGB features and thermal features, respectively;
$f_{GAP}$ and $f_{MLP}$ denote the global average pooling and MLP, respectively. 
For many previous works, the feature fusion is conducted by $\sigma(R) \oast F_{rgb} + \sigma(T) \oast F_{t}$, where $\sigma$ is the sigmoid activation that generates channel-wise attention, $\oast$ denotes channel-wise multiplication. However, this fusion method is effective only if either $\sigma(R)$ or $\sigma(T)$ has been activated sufficiently. 


 \begin{table}[t]
\caption{The comparisons between traditional attention and our explicit attention-enhanced fusion. As seen, for any possible values of $R$ and $T$, we can generate enhanced attention.}
\renewcommand\arraystretch{1.2}
\begin{center}
\label{datasets}
\begin{tabular}
{c|cccc}
\hline
 & $\sigma(R)$ & $\sigma(T)$ &$\sigma(c * R \otimes T)$ & 1- $\sigma(c * R \otimes T)$ \\ 
 \hline
 R$\geq$0, T$\geq$0 &[0.5, 1) &[0.5, 1) &[0.5, 1) &(0, 0.5]\\
R$\leq$0, T$\geq$0 &(0, 0.5] &[0.5, 1) &(0, 0.5] &[0.5, 1)\\
 R$\geq$0, T$\leq$0 &[0.5, 1) &(0, 0.5] &(0, 0.5] &[0.5, 1) \\
R$\leq$0, T$\leq$0 &(0, 0.5] &(0, 0.5] &[0.5, 1) &(0, 0.5]\\
% R, T & $\sigma(R)$  $\sigma(T)$ & &
\hline
\end{tabular}
\end{center}
\label{attention_cases}
\end{table}

Differing from any existing approaches, we delve into this feature fusion by explicitly considering the interaction of multi-modality features. 
We explicitly specify four cases for extracted attention as seen in Table~\ref{attention_cases} where $R$ and $T$ are extracted weights of feature maps by Eq.~\eqref{eq_at}. Noting that $R$ and $T$ are vectors, we treat them as scalars in Table~\ref{attention_cases} for simplicity. The positive values denote higher attention, i.e., $\sigma(R)\in[0.5,1)$; similarly, the negative values denote lower attention, i.e., $\sigma(R)\in(0, 0.5]$. 
For all of these cases, we apply attention enhancement to generate higher attention for both RGB and thermal features.

Specifically, we decompose the feature fusion into an Attention Interaction Branch (AIB) and an Attention Complement Branch (ACB), as shown in Fig.~\ref{fig_eaef}. The former handles cases where both RGB and thermal encoder or none of them capture discriminative features, and the latter tackles cases where only one encoder extracts useful features.


% \subsubsection{Attention Interaction Branch (AIB)}
AIB takes an element-wise multiplication between $R$ and $T$ to generate correlated attention, then applies channel-wise multiplication to RGB and thermal features. It is represented as:

\begin{equation}
 \begin{aligned}
&  F'_{rgb}  = \sigma( c * (R \otimes T)) \oast F_{rgb}   \\
& F'_{t}  = \sigma( c * (R \otimes T)) \oast F_{t}
 \end{aligned}
 \label{eq_cab}
\end{equation}
where $\otimes$ denotes element-wise multiplication.
$c$ is the number of channels that plays a role of scaling factor for attention enhancement, such that $  \sigma( c * (R \otimes T)) \geq  \sigma(R)$ and $  \sigma( c * (R \otimes T)) \geq \sigma(T)$.% \approx  \sigma(T)$ .}

% It is observed that Eq.\eqref{eq_cab} is most effective if both $R$ and $T$ are large. 
Let $F'_{rgb,t}$ be the concatenation of $F'_{rgb}$ and $F'_{t}$; then, AIB further performs multi-modality interaction between $F'_{rgb}$ and $ F'_{t}$ by a data interaction module:
\begin{equation}
\tilde{F}_{rgb,t} = F'_{rgb,t} \oast \sigma \biggl( f_{MLP} \biggl( f_{GMP} \Bigl( f_{dw}(F'_{rgb,t}) \Bigr)\biggr) \biggr) \\
 \label{eq_cab2}
\end{equation}
where $f_{MLP}$ and $\sigma$ denote MLP and sigmoid operations which are the same as Eq.\eqref{eq_at}, $f_{dw}$ is depth-wise convolution, $f_{GMP}$ is global max pooling operation. $\tilde{F}_{rgb,t}$ is the outputted features by the data interaction module, and it is further split to RGB features $\tilde{F}_{rgb}$ and thermal features $\tilde{F}_{rgb}$, respectively.


% \subsubsection{Attention Complement Branch (ACB)}
For cases where only one modality data provides sufficiently discriminative features, i.e., $R\geq0$, $T\leq0$ or $R\leq0$, $T\geq0$, $\sigma( c \oast (R \otimes T))$ tends to be small. Thus, we use the attention complement branch that applies the enhancement by:
\begin{equation}
 \begin{aligned}
&  \hat{F}_{rgb}  = (1 - \sigma( c * (R \otimes T))) \oast F_{rgb}  \\
& \hat{F}_{t}  = (1 - \sigma( c * (R \otimes T))) \oast F_{t}
 \end{aligned}
 \label{eq_eab}
\end{equation}

Then, the enhanced RGB and thermal features are obtained by aggregating outputs from AIB and ACB:
\begin{equation}
 \begin{aligned}
 & \overline{F}_{rgb}  = \tilde{F}_{rgb} + \hat{F}_{rgb} \\
  & \overline{F}_{t}  = \tilde{F}_{t} + \hat{F}_{t} 
   \end{aligned}
\end{equation}
We empirically found that the interaction module contributes less to the ACB regarding the model's performance. Therefore, we do not apply the interaction module in ACB to reduce the model complexity.
% of our method, we directly output $\overline{F}_{rgb}$ and $ \overline{F}_{t}$.

Finally, we apply a spatial attention mechanism to further merge the enhanced RGB and thermal features with a 1$\times$1 convolutional layer. Formally, the final merged features are obtained by:
\begin{equation}
  F_{final}  = \overline{F}_{rgb,t} \otimes SoftMax\Bigl( Conv_{1\times1} (\overline{F}_{rgb,t} )\Bigr)
\end{equation}

where $\overline{F}_{rgb,t}$ denotes the concatenated result of $\overline{F}_{rgb}$ and $\overline{F}_{t}$. $F_{final}$ is the fused features outputted by the EAEF module and is sent to the RGB encoder and the thermal encoder, respectively. 

%------------------------------------------------------------------------
%------------------------------------------------------------------------


\section{Experimental Results}
\label{sec_results}

%-----------------------------------------------

% \begin{figure}[t!]
%     \centering
%     \includegraphics [width=9cm]{Fig/Fig 6}
%     \vspace{-3mm}
%     \caption{From the left to right which is the PST900\cite{pspnet},MFNet\cite{MFNet},M3FD\cite{M3FD} and RGBT-CC\cite{RGBCC} datasets. The PST900 dataset and MF dataset is a segmentation tasks; the M3FD dataset is an object detection task. The RGBT-CC is a crowing count task.}
%     \label{Fig.main1}
%     \vspace{-5mm}
% \end{figure}


%--------------------------------------------------------------------------------------------------------

%--------------------------------------------------------------------------------------------------------
\begin{table*}[htbp]
  \centering
  \caption{Quantitative comparisons on the MFNet dataset. The best and the second best results are shown in bold font and the color blue, respectively.}
  \setlength{\tabcolsep}{0.3mm}
    \begin{tabular}{ccccccccccccccccccccc}
    \toprule
    \multicolumn{1}{c}{\multirow{2}{*}{Method}} & \multicolumn{1}{c}{\multirow{2}{*}{Backbone}} & \multicolumn{1}{c}{\multirow{2}{*}{Params.(M)}} & \multicolumn{2}{c}{Car} & \multicolumn{2}{c}{Person} & \multicolumn{2}{c}{Bike} & \multicolumn{2}{c}{Curve} & \multicolumn{2}{c}{Car stop} & \multicolumn{2}{c}{Guardrail} & \multicolumn{2}{c}{Color Cone} & \multicolumn{2}{c}{Bump} & \multirow{2}{*}{mAcc} &\multirow{2}{*}{mIoU} \\
\cmidrule{4-19}   &  &  & Acc   & IoU   & Acc   & IoU   & Acc   & IoU   & Acc   & IoU   & Acc   & IoU   & Acc   & IoU   & Acc   & IoU   & Acc   & IoU     \\
\cmidrule{1-21}
\multicolumn{1}{c}{MFNet\cite{MFNet}} &-   & 8.4  & 77.2  & 65.9  & 67.0  & 58.9  & 53.9  & 42.9  & 36.2  & 29.9  & 19.1  & 9.9   & 0.1   & 8.5   & 30.3  & 25.2  & 30.0  & 27.7  & 45.1  & 39.7  \\
    \midrule
    \multicolumn{1}{c}{FuseNet\cite{Fusenet}} & VGG16  &284.0  & 81.0  & 75.6  & 75.2  & 66.3  & 64.5  & 51.9  & 51.0  & 37.8  & 17.4  & 15.0  & 0.0   & 0.0   & 31.1  & 21.4  & 51.9  & 45.0  & 52.4  & 45.6  \\
    \midrule
    \multicolumn{1}{c}{RTFNet-50\cite{RTFNet}} & ResNet50 &245.7 & 91.3  & 86.3  & 78.2  & 67.8  & 71.5  & 58.2  & 59.8  & 43.7  & 32.1  & 24.3  & 13.4   & 3.6   & 40.4  & 26.0  & 73.5  & 57.2  & 62.2  & 51.7  \\
    \midrule
    \multicolumn{1}{c}{RTFNet-152\cite{RTFNet}} & ResNet152 &337.1 & 91.3  & 87.4  & 79.3  & 70.3  & 76.8  & 62.7  & 60.7  & 45.3  & 38.5  & 29.8  & 0.0   & 0.0   & 45.5  & 29.1  & \color{blue}74.7  & 55.7  & 63.1  & 53.2  \\
    \midrule
    \multicolumn{1}{c}{PSTNet\cite{pst900}} &ResNet18 & 105.8  & -     & 76.8  & -     & 52.6  & -     & 55.3  & -     & 29.6  & -     & 25.1  & -     & \textbf{15.1}  & -     & 39.4  & -     & 45.0  & -     & 48.4  \\
    \midrule
    \multicolumn{1}{c}{FuseSeg-161\cite{FuseSeg}} & DenseNet161 &-  & 93.1  & \textbf{87.9}  & 81.4  & 71.7  & 78.5  & \textbf{64.6}  & 68.4  & 44.8  & 29.1  & 22.7  & 63.7  & 6.4   & 55.8  & 46.9  & 66.4  & 49.7  & 70.6  & 54.5  \\
    \midrule
    \multicolumn{1}{c}{ABMDRNet\cite{ABMDRNet}} & ResNet50 &-  & 94.3  & 84.8  & \textbf{90.0}  & 69.6  & 75.7  & 60.3  & 64.0  & 45.1  & 44.1  & 33.1  & 31.0  & 5.1   & 61.7  & 47.4  & 66.2  & 50.0  & 69.5  & 54.8  \\
    \midrule
    \multicolumn{1}{c}{FEANet\cite{FEANet}} &ResNet152 & 337.1 & 93.3  & 87.8  & 82.7  & 71.1  & 76.7  & 61.1  & 65.5  & 46.5  & 26.6  & 22.1  & \color{blue}{70.8} & 6.6   & \textbf{66.6} & \color{blue}55.3  & \textbf{77.3} & 48.9  & 73.2  & 55.3  \\
    \midrule
    \multicolumn{1}{c}{GMNet\cite{GMNet}} &ResNet50 & 149.5  & 94.1  & 86.5  & 83.0  & \textbf{73.1} & 76.9  & 61.7  & 59.7  & 44.0  & \textbf{55.0}  & \textbf{42.3}  & \textbf{71.2}  & \color{blue}14.5   & 54.7  & 48.7  & 73.1  & 47.4  & 74.1  & 57.3  \\
    \midrule
    \multicolumn{1}{c}{EGFNet\cite{EGFNet}} &ResNet152 &201.3 & \textbf{95.8}  & 87.6  & \color{blue}89.0  & 69.8  & \color{blue}80.6  & 58.8  & \textbf{71.5}  & 42.8  & 48.7  & 33.8  & 33.6  & 7.0   & \color{blue}65.3  & 48.3  & 71.1  & 47.1  & 72.7  & 54.8  \\
    \midrule
    \multicolumn{1}{c}{MFTNet\cite{MFTNet}} &ResNet152 &360.9 & 95.1  & \textbf{87.9} & 85.2  & 66.8  & \textbf{83.9} & 64.4 & 64.3  & 47.1  & \color{blue}{50.8} & \color{blue}{36.1} & 45.9  & 8.4   & 62.8  & \textbf{55.5} & 73.8  & \textbf{62.2} & \color{blue}74.7  & \color{blue}57.3  \\
    \midrule
    \multicolumn{1}{c}{Ours} &ResNet50 &109.1 & 93.9  & 86.8  & 84.6  & 71.8  & 80.4  & 62.0  & 66.8  & \textbf{49.7} & 43.5  & 29.7  & 58.5  & 7.1   & 61.8  & 50.9  & 70.9  & 46.7  & 73.2 & 55.9 \\
    \multicolumn{1}{c}{Ours} &ResNet152 &200.4  & \color{blue}{95.4} & \color{blue}87.6  & 85.2  & \color{blue}72.6  & 79.9  & \color{blue}{63.8}  & \color{blue}70.6  & \color{blue}48.6  & 47.9  & 35.0  & 62.8  & 14.2 & 62.7  & 52.4  & 71.9  & \color{blue}58.3  & \textbf{75.1} & \textbf{58.9} \\
    \bottomrule
    \end{tabular}%
  \label{res_mfnet}%
\end{table*}%

\begin{figure*}[t!]
    \centering
    \includegraphics [width=\textwidth]{Fig/Fig 7}
    % \vspace{-3mm}
    \caption{Qualitative comparisons on the MFNet dataset. We can see that our EAEFNet can provide better results in various lighting conditions and environments. The comparison results demonstrate our superiority.}
    \label{qua_mfnet}
    % \vspace{-5mm}
\end{figure*}


\begin{table*}[t!]
  \centering
  \caption{Results on the PST900 dataset. The best results are shown in bold font.}
  \setlength{\tabcolsep}{0.75mm}
    \begin{tabular}{cccccccccccccc}
    \toprule
    \multicolumn{2}{c}{\multirow{2}{*}{Method}} & \multicolumn{2}{c}{Background} & \multicolumn{2}{c}{Hand-Drill} & \multicolumn{2}{c}{Backpack} & \multicolumn{2}{c}{Fire-Extinguisher} & \multicolumn{2}{c}{Survivor} & \multirow{2}{*}{mAcc} &  \multirow{2}{*}{mIoU} \\
\cmidrule{3-12}   \multicolumn{2}{c}{} & Acc   & IoU   & Acc   & IoU   & Acc   & IoU   & Acc   & IoU   & Acc   & IoU   &  &   \\
    \midrule
    \multicolumn{2}{c}{Efficient FCN (3C)\cite{EfficientFCN}} & 99.81  & 98.63  & 32.08  & 30.12  & 60.06  & 58.15  & 78.87  & 39.96  & 32.76  & 28.00  & 60.72  & 50.98  \\
    \multicolumn{2}{c}{Efficient FCN (4C)\cite{EfficientFCN}} & 99.80  & 98.85  & 48.75  & 38.58  & 69.96  & 67.59  & 76.45  & 46.28  & 38.86  & 35.06  & 66.75  & 57.27  \\
    \multicolumn{2}{c}{CCNet (3C)\cite{CCNet}} & 99.86  & 99.05  & 51.77  & 32.27  & 68.30  & 66.42  & 67.79  & 51.84  & 60.84  & 57.50  & 69.71  & 61.42  \\
    \multicolumn{2}{c}{CCNet (4C)\cite{CCNet}} & 99.59  & 97.74  & 54.09  & 51.01  & 75.96  & 72.95  & 88.06  & 73.80  & 49.45  & 33.52  & 73.43  & 66.00  \\
    \multicolumn{2}{c}{ACNet\cite{Acnet}} & \textbf{99.83}  & 99.25  & 53.59  & 51.46  & 85.56  & 83.19  & 84.88  & 59.95  & 69.10  & 65.19  & 78.67  & 71.81  \\
    \multicolumn{2}{c}{SA-Gate\cite{SAGate}} & 99.74  & 99.25  & 89.88  & 81.01  & 89.03  & 79.77  & 80.70  & 72.97  & 64.19  & 62.22  & 84.71  & 79.05  \\
    \multicolumn{2}{c}{RTFNet\cite{RTFNet}} & 99.78  & 99.25  & 7.79  & 7.07  & 79.96  & 74.17  & 62.39  & 51.93  & 78.51  & 70.11  & 65.69  & 60.46  \\
    \multicolumn{2}{c}{PSTNet\cite{pst900}} & /     & 98.85  & /     & 53.60  & /     & 69.20  & /     & 70.12  & /     & 50.03  & /     & 68.36  \\
    \multicolumn{2}{c}{GMNet\cite{GMNet}} & 99.81 & 99.44  & 90.29  & \textbf{85.17} & 89.01  & 83.82  & 88.28  & 73.79  & \textbf{80.65}  & \textbf{78.36}  & 89.61  & 84.12  \\
    \multicolumn{2}{c}{Ours} & \textbf{99.83} & \textbf{99.52} & \textbf{92.24} & 80.41& \textbf{91.14} & \textbf{87.69} & \textbf{93.25} & \textbf{83.96} & 80.63 & 76.22 & \textbf{91.42}  & \textbf{85.56} \\
    \bottomrule
    \end{tabular}%
  \label{res_pst}%
\end{table*}%

%-------------------------------------------------------------------------
%----------------------------------------------------------------------
% \subsection{Result and Analysis}

\subsection{Semantic Segmentation}

\subsubsection{Datasets}

MFNet dataset \cite{MFNet} is the most popular benchmark for RGB-T based semantic segmentation. It records nine semantic categories in urban street scenes, including one unlabeled background category and eight hand-labeled object categories. The dataset contains 1569 pairs of RGB and thermal images with a resolution of $640 \times 480$. 
Following RTFNet \cite{RTFNet}, we use 784 pairs of images for training, 392 pairs for validation, and the rest 420 pairs for testing.
% Our dataset partition is the same as in RTFNet\cite{RTFNet}, and the training set consists of 784 pairs of images. The validation set consists of 392 teams of imagesâ€”the other 420 teams' collection of images used for testing.

PST900 dataset \cite{pspnet} is also a popular benchmark for RGB-T based semantic segmentation. It contains five semantic categories and 894 RGB-T image pairs with a resolution of $ 720 \times 1280 $. Among them, 597 pairs are split for training, and the rest 297 pairs are used for testing. 
% There are five semantic categories, including XXXXXX.
% with annotations on 4 semantic categories from the DARPA\cite{DARPA} and other one is background.


\subsubsection{Implementation Details and Evaluation Metrics}
%------------------------------------------------------------------------
% Our experimental environment is set up on an NVIDIA RTX 3090Ti GPU, the network construction and training are performed in Pytorch, and 
We use the stochastic gradient descent (SGD)\cite{SGD} optimization solver for training. The initial learning rate is set to 0.02, Momentum and weight decay are set to 0.9 and 0.0005, respectively.  The batch size is set to 5, and we apply ExponentialLR to gradually decrease the learning rate. 
% Different from previous work, we refer to the solution of the current sample unevenness work, and adopt the method of 
% We employ a mixing loss functions
The loss function has a 
DiceLoss \cite{Dice} term and a SoftCrossEntropy \cite{soft} term, each term is weighed with a scalar of 0.5.
For MFNet dataset, we train the model with 100 epochs and use the best model on the validation set for evaluation.
For PST900 dataset, we train the model with 60 epochs. 

% \begin{equation}
% \text { DiceLoss }=1-\frac{2 \sum_i^N p_i g_i}{\sum_i^N p_i^2+\sum_i^N g_i^2}
% \end{equation}

% \begin{equation}
% \text { SoftCrossEntropyloss }=-\frac{1}{n} \sum_{i=1}^n \sum_{j=1}^c \hat{y}_{i j} \log \left(y_{i j}^d\right)
% \end{equation}
Same to previous works \cite{RTFNet}, we use two measures for quantifying results.
 The first is Accuracy (Acc) and the second one is Intersection Union (IoU). mAcc and mIoU are the averages over all categories.

% \begin{equation}
% \mathrm{mAcc}=\frac{1}{N} \sum_{i=1}^N \frac{\mathrm{TP}_i}{\mathrm{TP}_i+\mathrm{FN}_i},
% \end{equation}

% \begin{equation}
% \mathrm{mIoU}=\frac{1}{N} \sum_{i=1}^N \frac{\mathrm{TP}_i}{\mathrm{TP}_i+\mathrm{FP}_i+\mathrm{FN}_i},
% \end{equation}


\subsubsection{Results}
\paragraph{Results on MFNet}
We first conduct quantitative comparisons between the proposed method and other baseline approaches. We compare our method against existing approaches, including MFNet \cite{MFNet}, FuseNet \cite{Fusenet}, RTFNet-152 \cite{RTFNet}, FusSeg-161 \cite{FuseSeg}, FEANet \cite{FEANet}, GMNet \cite{GMNet}, MFTNet \cite{MFTNet}, PSTNet \cite{pst900}, RTFNet-50 \cite{RTFNet}, ABMDENet \cite{ABMDRNet}, EGFNet \cite{EGFNet}. Since the model complexity is different for existing methods, we implement our method on two backbones, including a larger ResNet-152 and a smaller ResNet-50, for fair comparisons. 

Table~\ref{res_mfnet} shows the quantitative results. It is clear that our method achieves the best mean accuracy.  As seen, when having a similarly smaller model complexity, our method beats PSTNet significantly. Besides,
our method built on ResNet152 achieved superior performance for most categories, e.g., the second best performance on ``Person'', ``Bike'', ``Curve'', ``'Bump' in IoU.
Most importantly,  the proposed method gained 0.4\% and 1.6\% improvements in mAcc and mIoU, respectively, against the current state-of-the-art MFTNet. 
The quantitative results verify that our method can extract better complementary cross-modality features.

% Compared with the SOTA RGB-T semantic segmentation methods, our proposed method has a better ability to recover detailed structure by exploiting information from different modalities.
%has dramatically improved the ability of extracting the information from single modality alone, specially in the Guardrail class.
% For example, compared with the current state-of-the-art MFTNet, our method gained 5.6\% and 2.6\% improvements in IoU on Guardrail and Curve, respectively.
% %we find the Guardrail class has the  +5.6\% IoU results in improvement.
% %At the same time, the Curve class has the +6.3\%mAcc and +2.6\%mIoU results in improvement.
% Moreover, our method also yields superior segmentation performance on other objects as shown in Table~\ref{res_pst}, which indicates the effectiveness of harvesting complementary cross-modal information.
%this indicates that our EAEFNet can confirm the effectiveness of harvesting complementary cross-modal information.}

% between our method and baseline approaches. Compare with other methods, including eight large parameter models ï¼ˆ\cite{MFNet}, \cite{FuseNet}, \cite{RTFNe}, FusSeg-161, FEANet, GMNet, MFTNet) and four small parameter models (\cite{pst900}, \cite{RTFNet}, \cite{ABMDRNet}, \cite{EGFNet}. We also propose two implementations based on ResNet50 and ResNet152 to compare models of different scales to ensure the fairness of the comparison. Because the results of Unlabel are very similar (96\% - 99\%) and not provide valid information, we do not show them in the table. 

% Figure \red{6} 
Figure~\ref{qua_mfnet} exhibits the quantitative results under different lighting conditions. 
In general, we find that our method has the following advantages. First, our method demonstrates better results than existing approaches for both night and daytime
conditions. It shows slightly better performance for daytime images and more superior results for nighttime images. 
Second, our method can capture the tiny objects both in RGB and thermal images more effectively, such as the pedestrian in the 3rd column and the bump on the road in the 5th column. 
These advantages validate the effectiveness of our strategy for multi-modality feature fusion.

% our method can further activate feature information that is not obvious in RGB and Thermal modalities. Second, our results can be seen from the visible images (the car part in the first column and the Curve in the eighth column) to see the missing information between the complementary modalities. Finally (the car is in the second column, the bicycle is in the fourth column, the Curve in the sixth column, and the car stop in the seventh column). Our method can better retain the activation information of both sides, making the segmentation result more complete. 

% \begin{figure}[t!]
%     \centering
%     \includegraphics [width=8cm]{Fig/Fig 9}
%     \vspace{-2mm}
%     \caption{The qualitative results on PST900 dataset.} 
%     \label{fig_pst}
%     \vspace{-1mm}
% \end{figure}


\begin{figure}[h]
\centering  
\begin{tabular}
{p{0.04\textwidth}<{\centering}p{0.115\textwidth}<{\centering}p{0.115\textwidth}<{\centering}p{0.115\textwidth}<{\centering}} 
% {p{0.04\textwidth}<{\centering}ccc}
{\footnotesize RGB}
&\IncG [width=0.125\textwidth]{Fig/pst/rgb/1.png} 
&\IncG [width=0.125\textwidth]{Fig/pst/rgb/2.png} 
&\IncG [width=0.125\textwidth]{Fig/pst/rgb/3.png} 
\\
{\footnotesize Thermal}
&\IncG [width=0.125\textwidth]{Fig/pst/t/1.png} 
&\IncG [width=0.125\textwidth]{Fig/pst/t/2.png} 
&\IncG [width=0.125\textwidth]{Fig/pst/t/3.png} 
\\
{\footnotesize Ground truth}
&\IncG [width=0.125\textwidth]{Fig/pst/gt/1.png} 
&\IncG [width=0.125\textwidth]{Fig/pst/gt/2.png} 
&\IncG [width=0.125\textwidth]{Fig/pst/gt/3.png} 
\\
{\footnotesize Ours}
&\IncG [width=0.125\textwidth]{Fig/pst/result/1.png} 
&\IncG [width=0.125\textwidth]{Fig/pst/result/2.png} 
&\IncG [width=0.125\textwidth]{Fig/pst/result/3.png} 
\\
\end{tabular}
\caption{The qualitative results on PST900 dataset.}
\label{fig_pst}
\end{figure}


% \begin{figure*}[t!]
%     \centering
%     \includegraphics [width=0.9\textwidth]{Fig/Fig 8}
%     % \vspace{-3mm}
%     \caption{The qualitative results on M3FD dataset.}
%     \label{fig_m3fd}
% \end{figure*}


\paragraph{Results on PST900}
We then conduct experiments on PST900 dataset. We compare our method with Efficient FCN \cite{EfficientFCN}, CCNet \cite{CCNet}, ACNet \cite{Acnet}, SA-Gate \cite{SAGate}, RTFNet \cite{RTFNet}, PSTNet \cite{pst900}, and GMNet \cite{GMNet}. 

The quantitative results are given in Table~\ref{res_pst}.
% We designed the implementation of EAENet on the PST900 dataset. As shown in Tabel \red{3} the performances of different models are reported.
It can be clearly seen that our method achieves the best results. It outperforms all previous methods, achieving 91.42 in mAcc and 85.56 in mIoU. Besides, it outperforms the state-of-the-art GMNet \cite{GMNet} by 1.81\% in mACC and 1.44\% in mIoU, respectively.
% clearly standing out among all the state-of-the-art approaches. 
We also visualize several predicted semantic maps in Fig.~\ref{fig_pst}.
% In Figure 8, our model performs well on the PST900 dataset.



% Table generated by Excel2LaTeX from sheet 'Sheet1'


\subsection{Object Detection}
\subsubsection{Dataset}
M$^{3}$FD dataset\cite{m3fd}contains a set of auto-driving scenarios. It has 4200 pairs of RGB-T images, including 33603 annotated labels in six classes, including ``People'', ``Car'', ``Bus'', ``Motorcycle'', ``Truck'', and ``Lamp''. Moreover, the dataset was split into ``Daytime'', ``Overcast'', ``Night'', and ``challenge'' scenarios according to the characteristics of the environments.

\subsubsection{Implementation Details and Evaluation Metric}
We build a network for object detection by integrating EAEF into YoloV5 \cite{mmyolo}.  We use the stochastic gradient descent (SGD)\cite{SGD} optimization solver for training. The initial learning rate is set to 0.01, Momentum and weight decay are set to 0.9 and 0.0005, respectively. The batch size is set to 32, and we apply ExponentialLR to gradually decrease the learning rate. The loss function has an IoULoss \cite{Iouloss} term and a CrossEntropy \cite{celoss} term. These two loss terms are weighed with a scalar of 0.3 and 0.7, respectively. For evaluation, we take the mAP@0.5 metric as TarDAL \cite{m3fd}. 

\begin{table}[!t]
  \centering
  \caption{Quantitative results on the M3FD object detection dataset. The best results are shown in bold font.}
    \begin{tabular}{cccccc}
    \toprule
    Method & Day   & Overcast & Night & Challenge & {mAP@0.5} \\
    \midrule
   Only RGB & 0.759 & 0.729 & 0.863 & 0.815 & 0.772 \\
   Only Thermal & 0.717 & 0.727 & 0.852 & \textbf{0.991} & 0.753 \\
    TarDAL\cite{m3fd} & 0.745 & 0.741 & 0.893 & 0.983 & 0.778 \\
    Ours & \textbf{0.783} & \textbf{0.786} & \textbf{0.895} & 0.979 & \textbf{0.801} \\
    \bottomrule
    \end{tabular}%
  \label{res_m3fd}%
\end{table}%

\begin{figure}[!t]
\centering  
\begin{tabular}
{p{0.04\textwidth}<{\centering}p{0.115\textwidth}<{\centering}p{0.115\textwidth}<{\centering}p{0.115\textwidth}<{\centering}} 
% {p{0.04\textwidth}<{\centering}ccc}
{\footnotesize RGB}
&\IncG [width=0.125\textwidth]{Fig/m3fd/rgb/1.png} 
&\IncG [width=0.125\textwidth]{Fig/m3fd/rgb/2.png} 
&\IncG [width=0.125\textwidth]{Fig/m3fd/rgb/3.png} 
\\
{\footnotesize Thermal}
&\IncG [width=0.125\textwidth]{Fig/m3fd/t/1.png} 
&\IncG [width=0.125\textwidth]{Fig/m3fd/t/2.png} 
&\IncG [width=0.125\textwidth]{Fig/m3fd/t/3.png} 
\\
{\footnotesize Ground truth}
&\IncG [width=0.125\textwidth]{Fig/m3fd/gt/1.png} 
&\IncG [width=0.125\textwidth]{Fig/m3fd/gt/2.png} 
&\IncG [width=0.125\textwidth]{Fig/m3fd/gt/3.png} 
\\
% {\footnotesize Only RGB}
% &\IncG [width=0.125\textwidth]{Fig/m3fd/or/1.png} 
% &\IncG [width=0.125\textwidth]{Fig/m3fd/or/2.png} 
% &\IncG [width=0.125\textwidth]{Fig/m3fd/or/3.png} 
% \\
% {\footnotesize Only thermal}
% &\IncG [width=0.125\textwidth]{Fig/m3fd/ot/1.png} 
% &\IncG [width=0.125\textwidth]{Fig/m3fd/ot/2.png} 
% &\IncG [width=0.125\textwidth]{Fig/m3fd/ot/3.png} 
% \\
{\footnotesize TarDAL}
&\IncG [width=0.125\textwidth]{Fig/m3fd/tardal/1.png} 
&\IncG [width=0.125\textwidth]{Fig/m3fd/tardal/2.png} 
&\IncG [width=0.125\textwidth]{Fig/m3fd/tardal/3.png} 
\\
{\footnotesize Ours}
&\IncG [width=0.125\textwidth]{Fig/m3fd/ours/00788.png} 
&\IncG [width=0.125\textwidth]{Fig/m3fd/ours/03729.png} 
&\IncG [width=0.125\textwidth]{Fig/m3fd/ours/00633.png} 
\\
\end{tabular}
\caption{The qualitative results on M$^3$FD dataset.}
\label{fig_m3fd}
\end{figure}


\subsubsection{Results}
To evaluate the effectiveness of the proposed method on object detection, we perform experiments on M$^3$FD object detection dataset.  We compare our method against approaches using only RGB images, only thermal images, and TarDAL \cite{m3fd}, which is the current state-of-the-art.

The experimental results are shown in Table~\ref{res_m3fd}. As seen, the method only using thermal data shows the worst performance.  Nevertheless, for ``Challenge'' scenarios,  it attains better performance than using RGB. Both TarDAL and our method obtain better accuracy compared to single modality data based methods.
It is also observed that our method outperforms the other three approaches by a good margin. 
We obtain 0.801 mAP, outperforming TarDAL by 2.3\%.  Figure~\ref{fig_m3fd} shows the qualitative results. As seen, although TarDal could also correctly identify the locations of objects, our method has higher recognition accuracy.
% The encoder using the EAENet dual-stream design and the decoder of YoloV5 are tested for the M3FD dataset, as shown in Table 2. In the experiment, we modified the Yolov5-m model provided by the \cite{mmyolo} framework and compared the results obtained with the three input types of RGB modality, Thermal modality, and TarDAL, elevating the state-of-the-art to more than 2.3\% in mAP0.5 .As shown in Fig 7 the 





%\begin{table}[htbp]
  %\centering
  %\caption{Add caption}
    %\begin{tabular}{cccccccc}
    %\toprule
    %\multicolumn{2}{c}{Methods} & \multicolumn{2}{c}{Params.(M)} & \multicolumn{2}{c}{GFLOPS} & mAcc  & mIoU \\
    %\midrule
    %\multicolumn{2}{c}{RTFNet50} & \multicolumn{2}{c}{245.7} & \multicolumn{2}{c}{185.2} & 62.2  & 51.7 \\
    %\multicolumn{2}{c}{RTFNet152} & \multicolumn{2}{c}{337.1} & \multicolumn{2}{c}{245.5} & 63.1  & 53.2 \\
    %\multicolumn{2}{c}{FEANet} & \multicolumn{2}{c}{337.1} & \multicolumn{2}{c}{255.2} & 73.2  & 55.3 \\
    %\multicolumn{2}{c}{EGFNet} & \multicolumn{2}{c}{201.3} & \multicolumn{2}{c}{\textbf{62.8}} & 72.7  & 54.8 \\
    %\multicolumn{2}{c}{MFTNet} & \multicolumn{2}{c}{360.9} & \multicolumn{2}{c}{330.6} & 74.7  & 57.3 \\
    %\multicolumn{2}{c}{FATNet} & \multicolumn{2}{c}{\textbf{201.2}} & \multicolumn{2}{c}{147.3} & \textbf{75.1} & \textbf{58.9} \\
    %\bottomrule
    %\end{tabular}%
  %\label{tab:addlabel}%
%\end{table}%


\subsection{Crowd counting}

\subsubsection{Dataset}
RGBT-CC dataset \cite{iadm} has 2,030 RGB-T pairs captured in public scenarios. The images have a resolution of $640 \times\ 480$. A total of 138,389 pedestrians are marked with point annotations, and approximately 68 people are marked per image. The training, validation, and test set have 1545, 300, and 1200 RGB-T pairs, respectively.

\subsubsection{Implementation Details and Evaluation Metric}
We adopt the same training strategy as IADM \cite{iadm}. We use the Adam optimizer and set the learning rate to 0.00001. We evaluate the model every 10 epochs out of 300 epochs. The best model on the validation set will be used for evaluation. For evaluation, we measure with the root mean square error (RMSE) and the grid average mean absolute error (GAME) \cite{GAME}.

\begin{table}[!t]
  \centering
  \caption{Results on the RGBT-CC dataset. The best results are shown in bold font.}
    \begin{tabular}
    % {ccccccc}
    {p{0.06\textwidth}<{\centering}p{0.055\textwidth}<{\centering}p{0.055\textwidth}<{\centering}p{0.055\textwidth}<{\centering}p{0.055\textwidth}<{\centering}p{0.055\textwidth}<{\centering}}
    \toprule
   Method & GAME(0)$\downarrow$ & GAME(1)$\downarrow$ & GAME(2)$\downarrow$ & GAME(3)$\downarrow$ & RMSE$\downarrow$
    \\
    \midrule
  UcNet\cite{UCNet} & 33.96 & 42.42 & 53.06 & 65.07 & 56.31 \\
    HDFNet\cite{HDFNet} & 22.36 & 27.79 & 33.68 & 42.48 & 33.93 \\
    BBSNet\cite{BBSNet} & 19.56 & 25.07 & 31.25 & 39.24 & 32.48 \\
   MVMS\cite{MVMS} & 19.97 & 25.1  & 31.02 & 38.91 & 33.97 \\
    % \midrule
    IADM\cite{iadm} & 15.61 & 19.95 & 24.69 & 32.89 & 28.18 \\
   Ours & \textbf{14.85} & \textbf{19.24} & \textbf{24.10} & \textbf{32.57} & \textbf{26.99} \\
    \bottomrule
    \end{tabular}%
  \label{res_rgbt}%
\end{table}%


\begin{figure}[!t]
\centering  
\begin{tabular}
{p{0.04\textwidth}<{\centering}p{0.115\textwidth}<{\centering}p{0.115\textwidth}<{\centering}p{0.115\textwidth}<{\centering}} 
% {p{0.04\textwidth}<{\centering}ccc}
{\footnotesize RGB}
&\IncG [width=0.125\textwidth]{Fig/RGBTCC/rgb/1198.png} 
&\IncG [width=0.125\textwidth]{Fig/RGBTCC/rgb/1202.png} 
&\IncG [width=0.125\textwidth]{Fig/RGBTCC/rgb/1206.png} 
\\
{\footnotesize Thermal}
&\IncG [width=0.125\textwidth]{Fig/RGBTCC/t/1198.png} 
&\IncG [width=0.125\textwidth]{Fig/RGBTCC/t/1202.png} 
&\IncG [width=0.125\textwidth]{Fig/RGBTCC/t/1206.png} 
\\
{\footnotesize Ground truth}
&\IncG [width=0.125\textwidth]{Fig/RGBTCC/gt/1198.png} 
&\IncG [width=0.125\textwidth]{Fig/RGBTCC/gt/1202.png} 
&\IncG [width=0.125\textwidth]{Fig/RGBTCC/gt/1206.png} 
\\
% {\footnotesize Only RGB}
% &\IncG [width=0.125\textwidth]{Fig/m3fd/or/1.png} 
% &\IncG [width=0.125\textwidth]{Fig/m3fd/or/2.png} 
% &\IncG [width=0.125\textwidth]{Fig/m3fd/or/3.png} 
% \\
% {\footnotesize Only thermal}
% &\IncG [width=0.125\textwidth]{Fig/m3fd/ot/1.png} 
% &\IncG [width=0.125\textwidth]{Fig/m3fd/ot/2.png} 
% &\IncG [width=0.125\textwidth]{Fig/m3fd/ot/3.png} 
% \\
{\footnotesize IADM}
&\IncG [width=0.125\textwidth]{Fig/RGBTCC/iadm/1198.png} 
&\IncG [width=0.125\textwidth]{Fig/RGBTCC/iadm/1202.png} 
&\IncG [width=0.125\textwidth]{Fig/RGBTCC/iadm/1206.png} 
\\
{\footnotesize Ours}
&\IncG [width=0.125\textwidth]{Fig/RGBTCC/ours/1198.png} 
&\IncG [width=0.125\textwidth]{Fig/RGBTCC/ours/1202.png} 
&\IncG [width=0.125\textwidth]{Fig/RGBTCC/ours/1206.png} 
\\
\end{tabular}
\caption{The qualitative results on RGBT-CC dataset.}
\label{fig_rgbcc}
\end{figure}


% {\color{blue}
\subsubsection{Results}
To evaluate the performance of our method on crowd counting task, we provide quantitative comparisons against previous approaches, including UcNet \cite{UCNet}, HDFNet \cite{HDFNet}, BBSNet \cite{BBSNet}, MVMS \cite{MVMS}, and the current state-of-the-art IADM \cite{iadm}. 

The results are shown in Table~\ref{res_rgbt}. As seen, our method achieves the best performance on all metrics. It outperforms UcBet, HDFNet, BBSNet, and MVMS by $33.97\%$ -- $56.31\%$ in RMSE. Moreover, our method also outperforms the state-of-the-art IADM by 1.19$\%$ in RMSE. The experimental results also verify the effectiveness of our method on the task of crowd counting.
% }
 % As shown in Table~\ref{res_rgbt}, we compared the performance of our model with the current state-of-the-art methods. Compared with 'BL+IADM', our model improves the accuracy by 1.19\% in RMSE, which verifies the excellent promotion of the IDAM module in crowd counting from the experimental results.


%-------------------------------------------------------------------------
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Results of ablation study on the MFNet dataset.}

  \renewcommand\arraystretch{1.2}
    \begin{tabular}{cccc|cc}
    \toprule
    & Baseline & AIB & ACB & mAcc & mIoU \\
    \hline
    \multicolumn{2}{c}{$\surd$} &       &       & 71.7  & 56.5 \\
  \multicolumn{2}{c}{$\surd$} & $\surd$     &       & 72.5  & 57.1 \\
   \multicolumn{2}{c}{$\surd$} &       & $\surd$     & 74.3  & 57.7 \\
   \multicolumn{2}{c}{$\surd$} & $\surd$     & $\surd$     & \textbf{75.1} & \textbf{58.9} \\
    \bottomrule
    \end{tabular}%
   \label{ablstudy}
\end{table}%


%-------------------------------------------------------------------------
\subsection{Ablation Study}

%\begin{table}[htbp]
  %\centering
  %\caption{The }
    %\begin{tabular}{cccccccc}
    %\toprule
    %\multicolumn{2}{c}{Methods} & \multicolumn{2}{c}{Params.(M)} & \multicolumn{2}{c}{GFLOPS} & mAcc  & mIoU \\
    %\midrule
    %\multicolumn{2}{c}{RTFNet50} & \multicolumn{2}{c}{245.7} & \multicolumn{2}{c}{185.2} & 62.2  & 51.7 \\
    %\multicolumn{2}{c}{RTFNet152} & \multicolumn{2}{c}{337.1} & \multicolumn{2}{c}{245.5} & 63.1  & 53.2 \\
    %\multicolumn{2}{c}{FEANet} & \multicolumn{2}{c}{337.1} & \multicolumn{2}{c}{255.2} & 73.2  & 55.3 \\
    %\multicolumn{2}{c}{EGFNet} & \multicolumn{2}{c}{201.3} & \multicolumn{2}{c}{\textbf{62.8}} & 72.7  & 54.8 \\
    %\multicolumn{2}{c}{MFTNet} & \multicolumn{2}{c}{360.9} & \multicolumn{2}{c}{330.6} & 74.7  & 57.3 \\
    %\multicolumn{2}{c}{FATNet} & \multicolumn{2}{c}{\textbf{201.2}} & \multicolumn{2}{c}{147.3} & \textbf{75.1} & %\textbf{58.9} \\
    %\bottomrule
    %\end{tabular}%
  %\label{tab:addlabel}%
%\end{table}%
%-------------------------------------------------------------------------
% \red{
We analyze the effectiveness of each component of our EAEF through additional experiments on the MFNet dataset. We establish a baseline by removing the AIB and ACB from the EAEF. The results are shown in Table~\ref{ablstudy}. We can observe that both AIB and ACB improved the performance of the baseline, and their combination, i.e., EAEF, gained the best performance. 
% Further, removing AIB causes minor degradation compared to removing ACB, which indicates that the information contained by RGB and thermal is mostly complementary.
% }
%In order to further verify the reason for the excellent performance of the network, in this section, we set up an ablation experiment to conduct experiments on each component. As shown in Table~\ref{ablstudy}
%\red{In order to further verify the reason for the excellent performance of the network, in this section, we set up an ablation experiment to conduct experiments on each component. The proposed Turbo-Decoder and the CRT module are first removed from the network as our baseline network (hereafter referred to as BS). Here, "TD" stands for Turbo-Decoder. At the same time, ID and IA represent the fusion of information using the decision module and the fusion of complementary information using the summation method. IDA represents the operation of the IDAM feature fusion in the channel. The quantitative experimental results are shown in Table 1. 'BS+Turbo' shows that combining multi-scale context information and the guidance of high-dimensional information with low-dimensional information can effectively help semantic segmentation tasks with better results. 

%Furthermore, we can also observe that ('BS+TD+ID,' 'BS+TD+IA' and 'BS+TD+IDA') compared with other fusion modules, our proposed IDA module optimizes the decision information alone At the same time, it can also improve the complementary information. After combining the decision information and complementary information, the performance of the fusion of RGB and Thermal information can be better. At the same time, in the experiment of ('BS+TD+Spatial'), we found that the fusion of information in the spatial dimension also significantly improves the modality information interaction.}

\section{CONCLUSIONS}
In this paper, we studied the better fusion strategy of RGB images and thermal data for perception tasks.
We explicitly specify cases where i) both RGB and thermal data, ii) only one type of data, and iii) none of them can provide sufficiently useful features.
% Existing methods fuse RGB and thermal data/features by simply applying element-wise addition, concatenation, or a straightforward attention mechanism,  often yielding unsatisfactory performance. Such fusion approaches are insufficient and hard to analyze and diagnose. 
% We aim to take full advantage of multi-modality data and extract more complementary features. 
We proposed the explicit attention-enhanced fusion (EAEF) that enhances feature extraction and provides compensation for insufficient representations. We evaluated our method on three different perception tasks, including semantic segmentation, object detection, and crowd counting. As a result, we achieved state-of-the-art performance on all three tasks, providing the robot community with a better fusion approach for RGB-thermal based perception tasks.

% Existing methods simply 


\bibliographystyle{IEEEtranS}
\bibliography{egbib}
% \end{thebibliography}



% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}