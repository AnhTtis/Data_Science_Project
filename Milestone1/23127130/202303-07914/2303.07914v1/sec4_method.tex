\section{Method}
To alleviate the mismatch between offline training and online inference, we propose a Future-Aware Streaming Translation (FAST) model, which adapts an offline ST model for streaming scenario with Future-Aware Inference (FAI) strategy and Future-Aware Distillation (FAD). 
An overview of our approach is shown in Figure \ref{fig:method}.
We will introduce the model architecture, FAI, and FAD as follows.

\begin{figure*}
\centering
\includegraphics[width=1.0\linewidth]{structure2.eps}
\caption{Illustration of offline ST model and proposed methods FAI  and FAD.}
\label{fig:method}
\end{figure*}

\subsection{Model Architecture}
Unlike previous works \cite{ren-etal-2020-simulspeech,ma-etal-2020-simulmt,zeng-etal-2021-realtrans} that require training multiple streaming models with different latency requirements, our goal is to train one offline model that can meet the requirements.
The architecture of our offline ST model is depicted in Figure \ref{fig:method}(a), which consists of an acoustic encoder, an acoustic boundary detector, a semantic encoder, and a translation decoder. 
\\
\textbf{Acoustic encoder}: Following Section \ref{sec:analysis}, we use pre-trained Wav2Vec2.0 model as the acoustic encoder, because it can learn a better speech representation \cite{ye21_interspeech,ye-etal-2022-cross}, which consists of a multi-layer convolutional subsampler $f_c$ and a Transformer encoder $f_e$. 
\\
\textbf{Acoustic boundary detector}: To enable the offline ST model  to perform chunk-wise streaming inference, we use a Continuous Integrate-and-Fire (CIF) module \cite{dong2022cif} as the acoustic boundary detector to dynamically locate the acoustic boundaries of speech segments following \cite{dong-etal-2022-learning,yi2021efficiently}. 
The CIF module first generates an integration weight $\alpha_t$ for each acoustic representation $a_t$ from the Wav2Vec2.0 model by a sigmoid function. 
Then, CIF accumulates $\alpha_t$ in a step-by-step way.
When the accumulated weight reaches a certain threshold (e.g. 1.0), the acoustic representations corresponding to these weights are integrated into a single hidden representation $h_j$ by weighted average. 
The shrunk representations $\mathbf{h}$ will be fed into the semantic encoder. 
To learn the correct acoustic boundaries, we use the source or target text length $J$ as the supervised signal.
%
\begin{equation}
\mathcal{L}_{\text{CIF}}=\left\|J-\sum_{t=1}^{T}\alpha_t\right\|_2
\label{eq:cif_loss}
\end{equation}
%
There are two benefits of using CIF as a boundary detector.
For offline ST model, it can address the length gap between speech and text.
It can also dynamically detect the acoustic boundaries of streaming audio to perform read/write policies for streaming inference.
\\
\textbf{Semantic encoder}: The semantic encoder is composed of $L_{e}$ Transformer \cite{vaswani2017attention} encoder layers, which aims to further encode the semantic information of speech representations. 
\\
\textbf{Translation decoder}: The translation decoder is composed of $L_{e}$ Transformer decoder layers, which generates the translations in an autoregressive way. 
The translation loss is defined as:
%
\begin{equation}
\mathcal{L}_{\text{ST}}(\mathbf{x},\mathbf{y})=-\sum_{j=1}^{J}\log p\left(y_j \mid y_{<j}, \mathbf{x}\right)
\label{eq:st_loss}
\end{equation}
%


\subsection{Future-Aware Inference}

The offline ST model is trained with the following objective function:
%
\begin{equation}
\mathcal{L}_{\text{offline}}=\mathcal{L}_{\text{ST}} + \lambda \cdot \mathcal{L}_{\text{CIF}}
\label{eq:offline_loss}
\end{equation}
%
where $\lambda$ is a hyper-parameter to balance two losses. 

Based on the analysis in Section \ref{sec:analysis}, we find that it is only necessary for the offline ST model to be aware of a short future during streaming encoding. 
Thus, we first propose a Future-Aware Inference (FAI) strategy to enhance the representations of streaming speech in Figure \ref{fig:method}(b). 


In this strategy, the streaming inference is directly performed on offline ST model without fine-tuning. 
Particularly, we use the mask tokens of Wave2Vec2.0 as the pseudo future context and append them to the speech tokens generated from the already consumed speech frames. 
Because the mask token embedding is trainable when pre-training Wave2Vec2.0, and Wav2vec2.0 applies span masks to the speech tokens and reconstructs \footnote{Strictly speaking, the task is to identify the quantized latent audio representation rather than reconstruction. } the corresponding latent features based on unmasked context, this is intuition that mask tokens can possibly encode future context. 
In addition, the pre-training results in approximately 49\% of all time steps being masked with a mean span length of 14.7 (300ms), it also guarantees that Wav2vec2.0 is able to extract better speech representations even with the presence of large amount of mask tokens.


Wav2Vec2.0 consists of a multi-layer convolutional subsampler $f_c$ and a Transformer encoder $f_e$. 
Concretely, for each audio prefix $\mathbf{\hat{x}}_t=(x_1,\ldots, x_{t})$ during online inference, the $f_c$ first outputs streaming speech tokens $\mathbf{\hat{c}}_t=(c_1,\ldots, c_{\tau})$, where $\mathbf{\hat{c}} \in \mathbb{R}^{\tau \times d}$ and $d$ is the dimension of model and $\tau$ is the sequence length after convolutional subsampling. 
Then, we concatenate the streaming speech tokens $\mathbf{\hat{c}}$ and $m$ mask token embeddings $\mathbf{e} \in \mathbb{R}^{d}$ along the time dimension, resulting in a longer sequence of speech tokens $\in \mathbb{R}^{(\tau + m) \times d}$. 
The new speech tokens are then fed into the Transformer encoder $f_e$, but only the first $\tau$ encoder outputs (i.e., speech features) will be kept for the CIF module because, as discussed in Section \ref{sec:analysis}, the last $m$ speech features are of poor quality and adversely affect translation quality. 
Then, if an acoustic boundary is detected by the CIF module, the decoder will emit new words based on \textit{wait-k} policy, otherwise, the streaming speech continues to be read.
The FAI strategy is outlined in Algorithm \ref{algo:mask}.



\subsection{Future-Aware Distillation}
Even FAI considers mask tokens as the pseudo future context, it is still preferred to leverage the future oracle speech tokens, which is unavailable during inference. 
Therefore, we take one step further by proposing a training framework -- Future-Aware Distillation (FAD). 
It aims to distill the knowledge from teachers with oracle future contexts into students with pseudo future contexts.
\\
\textbf{Teacher} We consider the offline ST model by optimizing the loss in Eq.~(\ref{eq:offline_loss}) as teacher and freeze its parameters.
\\
\textbf{Student} The student model is constructed in the same way as the teacher model and is initialised by the teacher model. 
However, we freeze the semantic encoder and translation decoder to retrain offline-trained ST performance.
\\
\textbf{Training} 
A naive solution is to use the triangle masking matrix to distill knowledge from the full speech into every possible streaming speech. 
However, since the length of speech tokens is typically very large, \emph{e.g.}, 300 on average, it will require large GPU memory. 
To this end, we propose a simple and efficient implementation via random sampling.

Given a full audio waveform $\mathbf{x}$, $f_c$ outputs the speech tokens $\mathbf{c} \in \mathbb{R}^{T \times d}$.
We randomly sample an integer $t \in [1, T]$ to construct the streaming speech token $\mathbf{c}_{\leq t}$.
Then, we define the teacher input with oracle future context as following:
%
\begin{equation}
\mathbf{\hat{c}}^\mathcal{T} = \mathbf{c}_{1:t+m} \in \mathbb{R}^{(t+m) \times d},
\end{equation}
%
where $m$ is a hyper-parameter to denote the number of future contexts\footnote{Note that in practice, we use $\min(m, T - t)$.} and $d$ is the dimension of model.
The most straightforward approach is to use the full speech as the teacher input. 
However, due to the bidirectional acoustic encoder, the streaming speech representation of the same position constantly changes when consuming new frames. 
According previous analysis in Sec \ref{sec:analysis}, the representations of the first $t$ speech tokens from Wav2Vec2.0 should have high quality if $m$ is larger than 10.  

To maintain consistency with the inference method FAI, we use the mask tokens as the pseudo future context and append them to the sampled speech tokens to construct the student input. 
%
\begin{equation}
\mathbf{\hat{c}}^\mathcal{S} = \operatorname{Concat}\{\mathbf{c}_{1:t}; m \times [\mathbf{e}]\} \in \mathbb{R}^{(t+m) \times d},
\end{equation}
%
where $\mathbf{e} \in \mathbb{R}^{d}$ is the mask embedding defined in Wav2Vec2.0.

The teacher model $f_e^\mathcal{T}$ and student model $f_e^\mathcal{S}$ then output the streaming speech representations $\mathbf{\hat{a}}^\mathcal{T}$ and $\mathbf{\hat{a}}^\mathcal{S}$, respectively.
%
\begin{equation}
    \mathbf{\hat{a}}^\mathcal{T}, \mathbf{\hat{a}}^\mathcal{S} = f_e^T(\mathbf{\hat{c}}^\mathcal{T}), f_e^S(\mathbf{\hat{c}}^\mathcal{S})
\end{equation}
%
We first build the distillation loss to reduce the speech representation gap by cosine similarity as following.
%
\begin{equation}
    \mathcal{L}_{KD}^{\text{W2V}} = 1-\operatorname{cosine}(\mathbf{\hat{a}}_{1:t}^\mathcal{S},\mathbf{\hat{a}}_{1:t}^\mathcal{T})
\end{equation}
%
Note that only the first $t$ speech representations will be calculated for above loss, because the representations of the ending positions in the teacher input are inferior and ignored.
 
Then we feed $\mathbf{\hat{a}}_{1:t}^\mathcal{T}$ and $\mathbf{\hat{a}}_{1:t}^\mathcal{S}$ into the CIF module to obtain teacher and student weight sequence as follows.
%
\begin{equation}
    \alpha^\mathcal{T}_{1:t}, \alpha^\mathcal{S}_{1:t} = \operatorname{CIF}(\mathbf{\hat{a}}^\mathcal{T}_{1:t}), \operatorname{CIF}(\mathbf{\hat{a}}^\mathcal{S}_{1:t}) 
\end{equation}
%
To learn more correct acoustic boundaries for online inference, we calculate the KL-divergence between teacher and student weights distribution:
%
\begin{equation}
    \mathcal{L}_{KD}^{\text{CIF}} = \sum_{\tau=1}^t \operatorname{KL}(\alpha_\tau^\mathcal{T} \| \alpha^\mathcal{S}_\tau)
\end{equation}
%
\textbf{Optimization} The total training objective of the FAD can be written as,
%
\begin{equation}
    \mathcal{L}=\mathcal{L}_{KD}^{\text{W2V}} + \mathcal{L}_{KD}^{\text{CIF}} .
\label{eq:total_loss}
\end{equation}
%
In other words, the parameters in the semantic encoder and translation decoder will not update. 
The overall training procedure of the proposed method is shown in Figure \ref{fig:method}(c).
