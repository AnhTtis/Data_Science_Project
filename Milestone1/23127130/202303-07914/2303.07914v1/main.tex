% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\input{math_commands.tex}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\graphicspath{{figs/}}
\usepackage{tikz}
\usepackage{subfigure} 
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{listings}
\usepackage{pgfplots}
\usetikzlibrary{spy}
\usepackage{color}
\usepackage{xcolor}
\usepackage{comment}
\pgfplotsset{compat=1.17}
\usepackage{multirow}
%\usepgfplotslibrary{external}
%\tikzexternalize[prefix=pdf/]

\definecolor{pink1}{HTML}{F0988C}
\definecolor{pink2}{HTML}{F6CAE5}
\definecolor{red1}{HTML}{f47721}
\definecolor{yellow1}{HTML}{FBCE02}
\definecolor{green1}{HTML}{83D350}
\definecolor{color1}{HTML}{d20962}
\definecolor{color2}{HTML}{f47721}
\definecolor{color3}{HTML}{efdf00}
\definecolor{color4}{HTML}{00a78e}
\definecolor{color5}{HTML}{7ac143}
\definecolor{color6}{HTML}{00bce4}
\definecolor{green2}{HTML}{A9D18E}
\definecolor{blue1}{HTML}{9DC3E6}
\definecolor{green3}{HTML}{00A472}
\definecolor{poscolor1}{HTML}{009ca6}
\definecolor{poscolor2}{HTML}{0689d8}
\definecolor{poscolor3}{HTML}{1428a0}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Adapting Offline Speech Translation Models for Streaming\\with Future-Aware Distillation and Inference}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Biao Fu$^{1}$\thanks{\,\, Equal contribution.},
Kai Fan$^{2}$\footnotemark[1] ,
Minpeng Liao$^{2}$\footnotemark[1] ,
Zhongqiang Huang$^{2}$, \\ 
\textbf{Boxing Chen}$^{2}$ ,
\textbf{Yidong Chen}$^{1}$\thanks{\,\, Corresponding author.},
\textbf{Xiaodong Shi}$^{1}$ \\
$^{1}$Department of Artificial Intelligence, School of Informatics, Xiamen University\\
$^{2}$Alibaba DAMO Academy \\
\texttt{biaofu@stu.xmu.edu.cn,\{ydchen,mandel\}@xmu.edu.cn} \\ \texttt{\{k.fan,minpeng.lmp,z.huang,boxing.cbx\}@alibaba-inc.com} \\}

\begin{document}
\maketitle
\begin{abstract}
A popular approach to streaming speech translation is to employ a single offline model with a \textit{wait-$k$} policy to support different latency requirements, which is simpler than training multiple online models with different latency constraints. However, there is a mismatch problem in using a model trained with complete utterances for streaming inference with partial input. We demonstrate that speech representations extracted at the end of a streaming input are significantly different from those extracted from a complete utterance. To address this issue, we propose a new approach called Future-Aware Streaming Translation (FAST) that adapts an offline ST model for streaming input. FAST includes a Future-Aware Inference (FAI) strategy that incorporates future context through a trainable masked embedding, and a Future-Aware Distillation (FAD) framework that transfers future context from an approximation of full speech to streaming input.
Our experiments on the MuST-C EnDe, EnEs, and EnFr benchmarks show that FAST achieves better trade-offs between translation quality and latency than strong baselines. Extensive analyses suggest that our methods effectively alleviate the aforementioned mismatch problem between offline training and online inference.


\end{abstract}
\input{sec1_intro}

\input{sec2_background}

\input{sec3_preliminary}

\input{sec4_method}

\input{sec5_experiments}

\input{sec6_conclusion}

% \section*{Acknowledgements}


% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}

\input{sec7_appendix.tex}

\end{document}
