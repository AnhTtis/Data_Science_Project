\section{Introduction}

Streaming speech translation (ST) systems consume audio frames incrementally and generate real-time translations, unlike their offline counterparts which have access to the complete utterances before starting to translate. 
Because of the streaming nature, streaming ST models commonly use uni-directional encoders \citep{ren-etal-2020-simulspeech,ma-etal-2020-simulmt,zeng-etal-2021-realtrans} and are trained with read and write policy that determines whether to wait for more speech frames or emit target tokens. 
In real-world applications, however, it is relatively expensive to train and maintain multiple models to satisfy different latency requirements \citep{zhang-feng-2021-universal}. 
Recently, some works \citep{papi-etal-2022-simultaneous,dong-etal-2022-learning} show that offline models with bidirectional encoder (\emph{e.g.}, Wav2Vec2.0 \cite{NEURIPS2020_92d1e1eb}) can be adapted to streaming scenarios with \textit{wait-$k$} policy \citep{ma-etal-2019-stacl} to meet different latency requirements and achieve comparable or better performance, partially due to the more powerful bidirectional encoders. 
However, there is an inherent mismatch in using a model trained with complete utterances on incomplete streaming speech during online inference \citep{ma-etal-2019-stacl}. 
\input{figs/motivation}

Intuitively, speech representations extracted from streaming inputs (Figure~\ref{fig:streaming}) are less informative than in the case with full speech encoding (Figure~\ref{fig:full}). 
A question is raised naturally: how many differences do the speech representations have between the two inference modes? 
We analyze the gap in speech representations, measured by cosine similarity, at different positions in the streaming input compared to using the full speech (Section \ref{sec:analysis}). 
We observe that there is a significantly greater gap for representations closer to the end of a streaming segment, with an average similarity score as low as 0.2 for the last frame, and the gap quickly narrows for frames earlier frames (Figure~\ref{fig:analysis_on_lastpos}). 
Moreover, we observe more degradation in translation quality for utterances with the greatest gap in speech representations between online and offline inference (see Appendix~\ref{apd:degree}).


Based on the above findings, we hypothesize that the lack of future contexts at the end of streaming inputs could be detrimental to streaming speech translation. 
To this end, we first propose a novel \textbf{F}uture-\textbf{A}ware \textbf{I}nference (FAI) strategy, as shown in Figure~\ref{fig:method}(b). 
In this approach, we append a few mask embeddings to the end of the current streaming speech tokens as additional input to the acoustic encoder. 
This idea is based on the masked modeling pre-training \cite{NEURIPS2020_92d1e1eb} that can implicitly estimate and construct future contexts in the corresponding hidden representations and extract more accurate representations for the frames in the streaming input. 

In addition, we propose a \textbf{F}uture-\textbf{A}ware \textbf{D}istillation (FAD) framework to further transfer the future contexts into current frames. 
Given a streaming speech token sequence, we append $m$ oracle speech tokens and $m$ mask tokens to construct two expanded streaming sequences with different future contexts, namely, the teacher and student speech inputs. 
The original offline model is considered as the teacher model, and a new acoustic encoder is initialized from the teacher and will be trained as a student module. 
To achieve the objective of distillation, we minimize several representations losses between the output of the teacher and student models. 
As the speech representations other than appending tokens of teacher are  similar to those of full speech, FAD tends to distill the knowledge of the full speech into the streaming speech. 
With FKD and FAI, streaming inputs can be aware of more informative future context during both training and inference, alleviating the aforementioned mismatch issue. 
We name our method FAST, a \textbf{F}uture-\textbf{A}ware \textbf{S}treaming \textbf{T}ranslation model.

We conduct experiments on the MuST-C EnDe, EnEs, and EnFr benchmarks. 
Experimental results show that our methods outperform several strong baselines on the trade-off between translation quality and latency.
In particular, in the lower latency range (when AL is less than 1000\emph{ms}), we achieve improvements of 12 BLEU in EnDe, 16 BLEU in EnEs, and 14 BLEU in EnFr. 
Extensive analyses demonstrate that introducing future context reduces the representation gap between the full speech encoding and the partial streaming encoding. 
