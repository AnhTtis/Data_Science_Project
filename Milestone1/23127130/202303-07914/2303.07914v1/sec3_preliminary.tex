
\section{Preliminary Analysis}
\label{sec:analysis}

\input{figs/fig_end}

In this section, we analyze the major mismatch in Transformer-based \citep{vaswani2017attention} ST architecture between offline training and online decoding. 
In full-sentence ST, the speech representation of each frame is obtained by attending to all unmasked frames by the multi-head attention in the transformer encoder layers. 
In other words, the encoder adopts a bidirectional structure. 
However, if directly applied to the streaming inference with the model trained offline, the speech representation of the current last frame will deteriorate because it can only attend to its previous frames. 
Recently, a common approach in speech translation is to stack a pre-trained Wav2Vec2.0 \citep{NEURIPS2020_92d1e1eb} as the acoustic encoder with a semantic NMT encoder-decoder, and achieves SOTA performance in the ST task \citep{han-etal-2021-learning,dong-etal-2022-learning,fang-etal-2022-stemm,ye-etal-2022-cross}, because it has been shown that a better speech representation can be learned via its pre-training.

To explore the precise effects of streaming inputs, we first train three offline ST models on the MuST-C EnDe dataset by following Chimera \citep{han-etal-2021-learning}, STEMM \citep{fang-etal-2022-stemm}, and MoSST \citep{dong-etal-2022-learning}, where the acoustic encoder Wav2Vec2.0 is trainable. 
After the offline ST training, we conduct an analysis on the MuST-C EnDe tst-COMMON set. 
We remove the outliers and the noisy data, and select audios with a duration between 2s and 10s, resulting in a total of 1829 examples. 

For an input sequence of audio frames $\mathbf{x}=(x_1,\ldots, x_{T})$, the convolutional subsampler of Wav2Vec2.0 shrinks the length of the raw audio by a factor 320 and outputs the full speech representation sequence $\mathbf{a}$. 
In other words, every 320 elements in $\mathbf{x}$ become a vector in $\mathbf{a}$. 
For readability reasons, we uniformly use the notation $T$ to denote the sequence length of $\mathbf{a}$, i.e., $\mathbf{a}=(a_1,\ldots, a_{T})$. 
This simplified notation does not undermine any of our conclusions while at the same time making the equations for readable\footnote{Because we can always define $\mathbf{x}=(x_{1:T})$ such that $x_t$ represents consecutive 320 audio frames.}. 
For streaming input $\forall t\leq T, \mathbf{\hat{x}}_t=(x_1,\ldots, x_{t})$, Wav2Vec2.0 will output the representation $\mathbf{\hat{a}}_t=(\hat{a}_{t,1},\ldots, \hat{a}_{t,t})$.

To measure the gap of the speech representations between the offline and online inputs, we calculate the cosine similarity $s_{t,t^{\prime}}$ between the speech representation at the $t^{\prime}$-th ($t^{\prime} \leq t$) position in the $t$-th streaming audio input $\mathbf{\hat{x}}_t$ and the speech representation at the same position in the full encoding. 
%
\begin{equation}
\text{For $t^\prime \leq t$}, s_{t,t^\prime} (\mathbf{x}) = \operatorname{cos}(\hat{a}_{t,t^{\prime}}, a_{t^{\prime}}), 
\label{eq:audio_cos}
\end{equation} 
%

Then, we average the cosine similarity over both testset $\mathcal{B}$ and time dimension with reversed index to obtain robust statistics.
%
\begin{equation}
\Bar{s}_{-t^{\prime}} = \frac{1}{|\mathcal{B}|}\sum\limits_{\mathbf{x} \in \mathcal{B}} \frac{1}{|\mathbf{x}| - t^\prime + 1} \sum_{t=t^{\prime}}^{|\mathbf{x}|} s_{t,t-t^{\prime}+1}(\mathbf{x})
\label{eq:audio_cos2}
\end{equation}
%
We calculate the metric $\Bar{s}_{-t^{\prime}}$ of the representations at the last 100 positions in the streaming speech and report the results in the Figure \ref{fig:analysis_on_lastpos}. 

The averaged cosine similarity $\Bar{s}_{-t^{\prime}}$ is greater than 0.8 when $t^\prime > 10$, indicating that the representations at such positions in the partial streaming input are close to those in the full speech. 
However, we observe that the averaged cosine similarity $\Bar{s}_{-t^{\prime}}$ sharply declines for the ending positions, especially for the last one ($t^\prime=1$). 
We conclude that \textbf{the representations of the ending position in the streaming speech are particularly inferior} and decide that \textbf{the low-quality representations of the last 10 positions cannot be ignored}\footnote{Extra preliminary analysis is provided in Appendix \ref{apd:additional_analysis}.}.



