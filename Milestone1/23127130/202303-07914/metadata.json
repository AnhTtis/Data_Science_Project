{
    "arxiv_id": "2303.07914",
    "paper_title": "Adapting Offline Speech Translation Models for Streaming with Future-Aware Distillation and Inference",
    "authors": [
        "Biao Fu",
        "Kai Fan",
        "Minpeng Liao",
        "Zhongqiang Huang",
        "Boxing Chen",
        "Yidong Chen",
        "Xiaodong Shi"
    ],
    "submission_date": "2023-03-14",
    "revised_dates": [],
    "latest_version": 1,
    "categories": [
        "cs.CL"
    ],
    "abstract": "A popular approach to streaming speech translation is to employ a single\noffline model with a \\textit{wait-$k$} policy to support different latency\nrequirements, which is simpler than training multiple online models with\ndifferent latency constraints. However, there is a mismatch problem in using a\nmodel trained with complete utterances for streaming inference with partial\ninput. We demonstrate that speech representations extracted at the end of a\nstreaming input are significantly different from those extracted from a\ncomplete utterance. To address this issue, we propose a new approach called\nFuture-Aware Streaming Translation (FAST) that adapts an offline ST model for\nstreaming input. FAST includes a Future-Aware Inference (FAI) strategy that\nincorporates future context through a trainable masked embedding, and a\nFuture-Aware Distillation (FAD) framework that transfers future context from an\napproximation of full speech to streaming input. Our experiments on the MuST-C\nEnDe, EnEs, and EnFr benchmarks show that FAST achieves better trade-offs\nbetween translation quality and latency than strong baselines. Extensive\nanalyses suggest that our methods effectively alleviate the aforementioned\nmismatch problem between offline training and online inference.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.07914v1"
    ],
    "publication_venue": "work in progress"
}