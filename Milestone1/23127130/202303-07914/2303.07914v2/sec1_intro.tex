\section{Introduction}

Streaming speech translation (ST) systems generate real-time translations by incrementally processing audio frames, unlike their offline counterparts that have access to complete utterances before translating. 
Typically, streaming ST models use uni-directional encoders \citep{zhang2019lattice,ren-etal-2020-simulspeech,ma-etal-2020-simulmt,zeng-etal-2021-realtrans,zhang2023simple} and are trained with a read/write policy that determines whether to wait for more speech frames or emit target tokens. 
However, it can be expensive to maintain multiple models to satisfy different latency requirements~\citep{zhang-feng-2021-universal,liu2021cross} in real-world applications. 
Recently, some works \citep{papi-etal-2022-simultaneous,dong-etal-2022-learning} have shown that a single offline model with bidirectional encoders (such as Wav2Vec2.0 \cite{NEURIPS2020_92d1e1eb}) can be adapted to streaming scenarios with a \textit{wait-$k$} policy \citep{ma-etal-2019-stacl} to meet different latency requirements and achieve comparable or better performance. 
However, there is an inherent mismatch in using a model bidirectionally trained with complete utterances on incomplete streaming speech during online inference. 

\input{figs/motivation}


Intuitively, speech representations extracted from streaming inputs (Figure~\ref{fig:streaming}) are less informative than those from full speech encoding (Figure~\ref{fig:full}) due to limited future context, especially toward the end of the streaming inputs, which can be exacerbated by the aforementioned mismatch problem. This raises a natural question: how much do the speech representations differ between the two inference modes? We analyze the gap in speech representations, measured by cosine similarity, at different positions in the streaming input compared to using the full speech (Section \ref{sec:analysis}). We observe a significantly greater gap for representations closer to the end of a streaming segment, with an average similarity score as low as 0.2 for the last frame, and the gap quickly narrows for earlier frames (Figure~\ref{fig:analysis_on_lastpos}). Additionally, we observe more degradation in translation quality for utterances with the greatest gap in speech representations between online and offline inference (see Appendix~\ref{apd:degree}).



We conjecture that the lack of future contexts at the end of streaming inputs can be detrimental to streaming speech translation when using an offline model. 
To this end, we propose a novel Future-Aware Inference (FAI) strategy. %, illustrated in Figure~\ref{fig:method}(b). 
This approach is inspired by masked language models' ability~\cite{NEURIPS2020_92d1e1eb} to construct representations for masked tokens from their context. 
Specifically, we append a few mask embeddings to the end of the streaming input and leverage the acoustic encoder (Wav2Vec2.0)'s ability to implicitly construct representations for future contexts, which can lead to more accurate representations for the other frames in the streaming input.


Furthermore, we propose a Future-Aware Distillation (FAD) framework that adapts the offline model to extract representations from streaming inputs that more closely resemble those from full speech encoding. 
We expand the original streaming input with two types of future contexts: one with $m$ oracle speech tokens for the teacher model, and another with $m$ mask tokens for the student model, which is initialized from the teacher model. 
We minimize several distillation losses between the output of the teacher and student models. 
By incorporating additional oracle future contexts, the speech representations for the frames in the original streaming input extracted by the teacher model resemble those when the full speech is available. 
FAD aims to adjust the offline model to extract similar representations for streaming input as it would for full speech. 
In combination with FAI, we improve the model's ability to extract quality representations during both training and inference, alleviating the aforementioned mismatch problem. 
We refer to our approach as FAST, which stands for Future-Aware Streaming Translation.



We conducted experiments on the MuST-C EnDe, EnEs, and EnFr benchmarks. 
The results show that our methods outperform several strong baselines in terms of the trade-off between translation quality and latency. 
Particularly, in the lower latency range (when AL is less than 1000\emph{ms}), our approach achieved BLEU improvements of 12 in EnDE, 16 in EnEs, and 14 in EnFr over baseline. 
Extensive analyses demonstrate that our future-aware approach significantly reduces the representation gap between partial streaming encoding and full speech encoding.

