\section{Experiments}

\input{figs/main_results.tex}
\subsection{Experimental Settings}

\textbf{Datasets} We evaluate our approach on MuST-C V1 English-German (EnDe), English-Spanish (EnEs) and English-French (EnFr) datasets \citep{di-gangi-etal-2019-must}, 
where limited previous works discussed the En-Fr streaming ST with BLEU-latency curve. 
All the corpora contain source audios, source transcriptions, and target translations, and the results reported are conducted on the corresponding tst-COMMON set. 
Detailed statistics of different language pairs are given in Appendix \ref{apd:data_statistics}.

For speech data, we normalize the raw audio wave to the range of $[-1,1)$. 
For text data, we keep punctuation and remove non-printing characters, and remain case-sensitive. 
For each translation direction, the unigram SentencePiece\footnote{\url{https://github.com/google/sentencepiece}} model \citep{kudo-richardson-2018-sentencepiece} is used to learn a shared vocabulary of size 10k. 
\\
\textbf{Model Configuration} 
For the acoustic encoder, we use Wav2vec2.0\footnote{\url{https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec\_small.pt}} \citep{NEURIPS2020_92d1e1eb} following the base configurations. 
We construct the acoustic boundary detector by applying the CIF \citep{yi2021efficiently} on the last dimension of speech representation. 
We use 8 and 6 layers for the semantic encoder and the translation decoder respectively, with 4 attention heads and 768 hidden units.
\\
\textbf{Training} 
The detailed training schedule of the offline ST model is given in Appendix \ref{apd:details_of_training}. 
We set the length $m$ of future context tokens to 50 for both FAD and FAI. 
All hyper-parameters are tuned on EnDe devset and applied to other language pairs. 
We train all models with 3.2 million frames per batch on 8 Nvidia Tesla V100 GPUs. 
We implement our models with Fairseq\footnote{\url{https://github.com/pytorch/fairseq}} \cite{ott-etal-2019-fairseq}.
\\
\textbf{Inference} We average the checkpoints of the best 10 epochs on development set for evaluation. 
We perform streaming-testing with the \textit{wait-$k$} policy. 
$k$ is counted by the detected acoustic units from the CIF module. 
To follow the tradition in simultaneous translation \citep{zeng-etal-2021-realtrans,dong-etal-2022-learning}, we do not rewrite the tokens that have already been generated. 
\\
\textbf{Evaluation Metrics} We use SacreBLEU\footnote{\url{https://github.com/mjpost/sacrebleu}} for the translation quality. 
The latency is evaluated with Average Latency (AL) \citep{ma-etal-2019-stacl}, Average Proportion (AP) \citep{cho2016can}, and Differentiable Average Lagging (DAL) \citep{cherry2019thinking} in the SimulEval\footnote{\url{https://github.com/facebookresearch/SimulEval}} \citep{ma-etal-2020-simuleval}. 
\\
\textbf{System Settings} 
We compare our method with several strong end-to-end streaming ST approaches. 
(\romannumeral1) \textit{SimulSpeech} \citep{ren-etal-2020-simulspeech} and \textit{RealTranS} \citep{zeng-etal-2021-realtrans} use uni-directional encoder rather than bidirectional one. 
(\romannumeral2) \textit{MoSST} \citep{dong-etal-2022-learning} applies an offline-trained model with a monotonic segmentation module for streaming testing and achieves competitive performance. 
(\romannumeral3) \textit{MMA-SLM} \cite{indurthi-etal-2022-language} enhances monotonic attention to make better read/write decisions by integrating future information from language models.
(\romannumeral4) \textit{ITST} \cite{zhang-feng-2022-information} learns an adaptive read/write policy by quantifying the transported information weight from source token to the target token.
(\romannumeral5) \textit{MU-ST} \citep{zhang-etal-2022-learning} learns an adaptive segmentation policy to detect meaningful units, which makes read/write decisions. 
(\romannumeral6) \textit{Baseline} is our offline-trained ST model (\textbf{B} for abbreviation). 
For fair comparisons, it has the same structure as MoSST. 




\subsection{Main Results}

We presents the main results in Figure \ref{fig:main_results} \footnote{The extended results for other latency metrics (AP and DAL) are described in Appendix \ref{apd:appendix_more_results}.}. 
Compared with the online models SimulSpeech, RealTranS, and ITST, our offline model (baseline) achieves higher translation quality with high latency as it encodes bidirectional context information during training, however, in the low latency region, it performs poorly due to the input mismatch between offline-training and online-decoding. 

\textbf{B + FAI} With the ability to reduce this mismatch, FAI is directly applied for our offline (baseline) model and can achieve higher BLEU in all latency regions. 
In particular, it outperforms our most compatible baseline \textbf{B} by large margins in lower latency regions (when AL is less than 1000\emph{ms}), with improvements over 6 BLEU in both EnDe and EnEs, 10 BLEU in EnFr.

\textbf{FAST} (FAD + FAI) Furthermore, our FAST achieves the best trade-off between translation quality and latency, especially at extremely low latency region (AL is about 200\emph{ms}, $k=1$), achieving the improvements of 6 BLEU in EnDe, 10 BLEU in EnEs, and 4 BLEU in EnFr compared to B + FAI.
It indicates that FAST can effectively mitigate the input mismatch between offline-training and online-decoding.
In addition, our method achieves comparable translation quality with full-speech translation at middle latency (at AL around 3000\emph{ms}), especially for EnEs.


\subsection{Ablation Study}
\label{sec:ablation}

\input{figs/ablation_study}
In this section, we study the effectiveness of our methods. 
All ablation results are obtained from the MuST-C EnDe tst-COMMON set. 
The results are shown in Figure \ref{fig:ablation}.

(1) \textit{w/o $\mathcal{L}_{KD}^{\text{W2V2}}$}: if removing the $\mathcal{L}_{KD}^{\text{W2V2}}$, the translation quality drops by 1-2 BLEU in all latency regions, including high latency region.
This demonstrates optimizing $\mathcal{L}_{KD}^{\text{W2V2}}$ can guarantee the full speech translation performance.

(2) \textit{w/o $\mathcal{L}_{KD}^{\text{CIF}}$}: If removing the $\mathcal{L}_{KD}^{\text{CIF}}$, the translation quality will be slightly degraded. 
However, we observe that the distances between two consecutive acoustic boundaries become larger. 
For example, the AL of this variant at \textit{wait-1} is greater than 750, but the AL of the other variants at \textit{wait-1} is approximately 150. 
As expected, optimizing $\mathcal{L}_{KD}^{\text{CIF}}$ can ensure the correct acoustic boundaries.

(3) \textit{w/o FAI}: In this variant, we use the student model by FAD with vanilla \textit{wait-k} policy for streaming inference (\emph{i.e.}, inference without mask tokens). 
However, FAD training considers mask tokens as student input, so this mismatch leads to significant performance degradation in low and middle latency regions.  
This indicates that our FAD and FAI should be used together to achieve better streaming performance.

(4) \textit{w/o mask embeddings}: During training and inference, our model appends $m$ mask tokens into streaming speech tokens as the pseudo future contexts. 
In this variant, we remove the mask tokens during both training and inference. 
Even though no mismatch, we still observe a significant drop in translation quality, especially for high latency. 
This result indicates that the pseudo future contexts can enhance the streaming speech representations.

\subsection{How much future context is needed?}

\input{figs/future_length.tex}

To answer this question, we explore the FAST (FAD + FAI) with different lengths of future context. 
Figure \ref{fig:length_bleu} shows the overall results. %in $\{0,10,20,30,40,50,60,70\}$
$m=0$ means the offline system without distillation. 
The offline system inherits the mismatch problem, but our method gradually improves the performance as $m$ increasing from 0 to 20. 
Since we found only the representation of last 10 positions is poor (in Section \ref{sec:analysis}), FAST obtains similar BLEU-AL curve when $m$ is significantly larger than 10, \emph{e.g.}, 20-100. 

After the FAD training, we investigate the representation of the last position (before mask tokens) by $\bar{s}_{-1}$ in Eq.~(\ref{eq:audio_cos2}) w.r.t. $m$. 
The results are shown in Figure \ref{fig:length_cos}.
We observe that 1) as $m$ increases, the streaming speech representation of the last position becomes better; 
2) the curves of the cosine similarity becomes flattened when $m > 10$ significantly. 
This is consistent with the trend in Figure \ref{fig:length_bleu}.


\subsection{Analysis on The Representation Gap}

\input{figs/why_work.tex}

Figure \ref{fig:whywork} plots the changes of average cosine similarity $\Bar{s}_{-t^{\prime}}$ in Eq.~(\ref{eq:audio_cos2}) of the last 40 positions (before mask tokens) in the streaming speech after applying the FAI or FAST (FAD + FAI). 
They achieve at least 0.6 and 0.8 cosine similarity at the last position, respectively. 
The baseline only has the $<0.6$ cosine similarity for the last 4 positions and only 0.2 for the last position. 
It indicates that the representations with FAI are closer to those of the full speech, especially at the ending positions, and FAD training can further close this gap. 


\subsection{What examples are improved?}
\label{sec:monotonic}
\input{tables/reorder_level.tex}
For tst-COMMON on MuST-C EnDe, we use awesome-align\footnote{\url{https://github.com/neulab/awesome-align}} \cite{dou-neubig-2021-word} to identify the token-level alignment between source transcription and target translation following \citet{zhang-feng-2022-reducing}. 
First, we define the source-to-target alignment position shift as $\max\{0, i - j\}$, where the $i$th source token is aligned to the $j$th target token. 
If $i - j$ is large, it means in order to translate the $j$th target token, the model may need to read more until seeing the $i$th source token. 
Then we calculate the monotonic level of each example as the averaged alignment position shift over the number of aligned tokens, \emph{i.e.}, 
\begin{equation}
    \mathbf{M} = \frac{1}{|\mathbf{A}|}\sum_{(i,j) \in \mathbf{A}} \max\{0, i - j\}.
\end{equation} 
where $\mathbf{M}$ denotes monotonic level and $\mathbf{A}$ represents aligned pairs.
We evenly divide the test set into three groups (Easy, Medium, and Hard) according to different monotonicity levels. %: easy ($=0$), medium ($<3$) and hard ($\geq 3$).  
For each group, we evaluate different inference methods and report the results in Table \ref{tab:reorder}. 
As we explained in \ref{apd:al}, it is almost impossible to guarantee the same AL for different inference methods. 
For a fair comparison, we try our best to set the AL of different methods to be approximately equal. 
We can see our inference strategies show a significant advantage on the non-monotonic examples (Medium and Hard groups). 

