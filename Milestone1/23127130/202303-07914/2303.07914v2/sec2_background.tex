\section{Background and Related Work}
 
Speech translation systems can be roughly categorized into non-streaming (offline) and streaming (online) depending on the
inference mode. 
Regardless of the inference mode, speech translation models typically employ the encoder-decoder architecture and are trained on an ST corpus
$\mathcal{D}=\{(\mathbf{x}, \mathbf{z}, \mathbf{y})\}$, where
$\mathbf{x}=(x_1,\ldots, x_{T})$ denotes an audio sequence,
$\mathbf{z}=(z_1,\ldots, z_{I})$ and $\mathbf{y}=(y_1,\ldots, y_{J})$
the corresponding source transcription and target translation
respectively.

\textbf{Non-Streaming Speech Translation} For the non-streaming ST task, the encoder maps the entire input audio $\mathbf{x}$ to the speech representations $\mathbf{h}$, and the decoder generates the $j$-th target token $y_j$ conditional on the full representations $\mathbf{h}$ and the previously generated tokens $y_{<j}$. 
The decoding process of non-streaming ST is defined as $p(\mathbf{y} \mid \mathbf{x})=\prod_{j=1}^{J} p\left(y_{j} \mid \mathbf{x}, \mathbf{y}_{<j}\right)$.


A significant amount of works have focused on non-streaming ST, including pre-training \citep{wang-etal-2020-curriculum,dong2021consecutive,tang-etal-2022-unified,ao-etal-2022-speecht5}, 
multi-task learning \citep{liu2020synchronous,indurthi2020end,indurthi2021task}, 
data augmentation \citep{pino-etal-2019-harnessing,di-gangi-etal-2019-data,mccarthy2020skinaugment}, 
knowledge distillation \citep{dong2021listen,zhao-etal-2021-mutual,du2022regularizing}, %,inaguma-etal-2021-source
and cross-modality representation learning \citep{tang-etal-2021-improving,fang-etal-2022-stemm,ye-etal-2022-cross}.

\textbf{Streaming Speech Translation} A streaming ST model generates the $j$-th target token $y_j$ based on streaming audio prefix $\mathbf{x}_{\leq g(j)}$ and the previous tokens $y_{<j}$ , where $g(j)$ is a monotonic non-decreasing function representing the ending timestamp of the audio prefix that needs to be consumed to generate the $j$-th word. 
The decoding probability is calculated as $p(\mathbf{y} \mid \mathbf{x})=\prod_{j=1}^{J} p\left(y_{j} \mid \mathbf{x}_{\leq g(j)}, \mathbf{y}_{<j}\right)$.


Thus, a streaming ST model requires a policy to determine whether to wait for more source speech or emit new target tokens. 
Recent studies \citep{ma-etal-2020-simulmt,ren-etal-2020-simulspeech,zeng-etal-2021-realtrans,dong-etal-2022-learning} make read/write decisions based on a variant of the \textit{wait-$k$} policy that was initially proposed for streaming text translation, which alternates write and read operations after reading the first $k$ source tokens. 
Because there is no explicit word boundaries in a streaming audio, several works attempt to detect word boundaries in the audio sequence by fixed length \citep{ma-etal-2020-simulmt}, Connectionist Temporal Classification \citep{ren-etal-2020-simulspeech,zeng-etal-2021-realtrans,papi-etal-2022-simultaneous}, ASR outputs \citep{chen-etal-2021-direct}, or continuous-integrate-and fire \citep{dong-etal-2022-learning, chang22f_interspeech}. 
Moreover, some studies \citep{arivazhagan-etal-2019-monotonic,Ma2020Monotonic,zhang-etal-2020-learning-adaptive,schneider-waibel-2020-towards,miao-etal-2021-generative,zhang-feng-2022-gaussian,zhang-feng-2022-modeling,zhang-etal-2022-learning,liu-etal-2021-cross,zhang-feng-2022-information,lin2023leapt,zhao2023adaptive} explore adaptive policies to dynamically decide when to read or write for streaming text and/or streaming speech translation. 
\citet{zhang-feng-2022-reducing} fill future source positions with positional encoding as future information during training for simultaneous machine translation (MT) within the prefix-to-prefix framework. 
In this paper, we focus on a matter less attended to -- how to alleviate the mismatch between offline training and online inference.

\textbf{Knowledge Distillation for Streaming Translation}
Existing studies on streaming text and/or speech translation usually introduce future information by distilling sequence-level knowledge from offline MT \cite{ren-etal-2020-simulspeech,zhang2021future,liu-etal-2021-cross,zhu-etal-2022-aisp,deng2023mono4simt,wang2023better} and online MT \cite{Zaidi2021DecisionAR}.
Moreover, \citet{ren-etal-2020-simulspeech} leverage the knowledge from the multiplication of attention weights of streaming ASR and MT models to supervise the attention of the streaming ST model.
However, our FAD aims to reduce the representation gap between full speech and streaming speech.

