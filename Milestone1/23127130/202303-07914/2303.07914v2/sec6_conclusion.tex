\section{Conclusion}
In this paper, we examine streaming speech translation from a new perspective. 
We investigate the effects of the input mismatch between offline-training and online-decoding.
We find that the representations at the ending positions in the streaming input are particularly poor, directly impacting the translation quality.
We propose FAST, which introduces future contexts to improve these representations during training and testing via FAD and FAI, respectively.
Experiments and analysis demonstrate their effectiveness in bridging the representation gap between full speech encoding and partial streaming encoding. 
Furthermore, our methods can be generally beneficial to streaming speech translation models that are based on Wav2Vec2.0. 
In the future, we will explore the relevant method independent on Wav2Vec2.0.

