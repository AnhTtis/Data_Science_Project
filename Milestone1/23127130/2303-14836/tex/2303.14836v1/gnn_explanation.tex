
\begin{figure*}[t]
	\centering
	\includegraphics[width=0.85\textwidth]{./picture/Framework.pdf}
	\caption{The workflow of {\name}. With a input graph and a pre-trained GNN, {\name} firstly learns the importance scores for edges and node attributes. Next, {\name} estimates the importance scores for nodes from the previous calculation. The important subgraph is then explained by removing the unimportant factors.}
	\label{fig:process}
\end{figure*}

\begin{table}[t]
	\caption{\revision List of notations.}
	\centering
	\tabcolsep = .5cm
    {\revision
	\begin{tabular}{c|l}
		\toprule
		Notation & Description \\ 
		\midrule
		$G$ & A graph \\
		$\mathcal{V}$ & The set of nodes in graph $G$ \\
		$\mathcal{E}$ & The set of edges in graph $G$ \\
		$\mathcal{X}$ & The sets of node attributes in graph $G$ \\
		$\Phi$ & A GNN model \\
		$P$ & Prediction probability \\
		$m$ & Explanation mask \\ 
		$\omega$ & Importance score \\
		\bottomrule
	\end{tabular}
	\label{tb:notations}
    }
\end{table}%


\section{Design Details of {\name} }
\label{sec_gnn_explanation}

% \subsection{Overview of {\name}}
% {\td I would rather have a separated subsection to just give an overview of {\name}.}
% \ans{I am not able to fill this section with multiple paragraphs. The explanation order can be written in a sentence. Further theoretical analysis of the order is explained later, which is where the overview comes from. This section can be messy and confusing if I separate it out. I wanted to bring out equations/analysis first and explain why the overview is like this. For this section, I imitate the writing style from the explainer papers I cited, especially GNNExplainer and PGExplainer, both are from NeurIPS. }

% In this section, we firstly overview {\name}. Later, we discuss the objective function as it is the key to our method. Next, we discuss the details on how to explain edges, node features, and nodes, respectively. Here, we summarize the main notations in Table~\ref{tb:notations}.

\subsection{Overview}

%\begin{figure*}[t]
%	\centering
%	\includegraphics[width=\textwidth]{../picture/Framework.pdf}
%	\caption{The framework of {\name}. First, edge masks $m^{(\mathcal{E})}$ and node feature masks $m^{(\mathcal{X})}$ are randomly initialized. {\name} applies masks to the input graph and feed it into GNN. The masks are optimized by the feedback of GNN. The importance scores for edges and node attributes ($\omega _{\mathcal{E}}$ and $\omega _{\mathcal{X}}$) are generated from corresponding masks. Next, node importance scores $\omega _{\mathcal{V}}$ are computed from the previous results. With the importance scores, the important subgraph can be extracted. {\td you can use some colors.}}
%	\label{fig:process}
%\end{figure*}

% \todo{rewrite as a workflow.}

The workflow of {\name} is shown in Figure~\ref{fig:process}. {\name} takes an attributed graph and a pre-trained GNN as input then generates a key subgraph that contributes to the prediction, with the importance scores as the importance measurement. 

First, {\name} learns the importance scores for edges and node attributes collectively from the input graph and the pre-trained GNN. The edge masks and attribute masks are initialized by {\name}. {\revision Using the same approach from GNNExplainer, {\name} applies the masks as learnable parameters to the input graph. Similar to GNN training, the masks are learned iteratively from the feedback of GNN.} The importance scores are then calculated from the learned masks.
Next, {\name} estimates the importance scores for nodes from the calculated importance scores for edges and node attributes. For each node, the importance scores from the related edges and attributes are aggregated for the estimation. 
Finally, an important subgraph is explained by removing the factors with low importance scores under certain constraints, e.g., the size of the subgraph.

% {\name} achieves a comprehensive explanation from an input graph with a pre-trained GNN efficiently, which works on different GNN-based security applications. For attribute explanation, we focus on node attribute explanation in this paper.
% We take the predicted label given by GNNs as our explanation target. {\name} initializes edge masks and node attribute masks, then applies them to the input graph. The masked graph is then fed into GNN. The masks are optimized iteratively by the feedback of GNN. With the learned masks, the importance scores for edges and node features are calculated, which is utilized for node importance score estimation. A subgraph with important node attributes is derived from the calculated importance scores.
% In the following subsections, we discuss how this framework is formed.
Next, we discuss the detailed design of {\name}. The main notations are summarized in Table~\ref{tb:notations}

% {\td explain input, how {\name} works, and the output. Using Figure 2 as an example to explain.}

\subsection{Objective Function}

% {\td Does ``objective'' refer to the objective function? Why would you discuss it in the first subsection?}
% \ans{1. Following other explainers, "objective" mainly formulates two things. One is the target --- maximizing MI (objective function), the other is parameters for optimization --- P(G) and P(X). 2. We have an objective and parameters to optimize, so we are able to study how to do it. Everything later serves objective function and how we optimize the parameters. As you may observe in this section, the previous analysis and equations are always used by later analysis and equation.}

% {\td Is the discussion on ``framework'' overview?}
% \ans{Yes. As we have objective function and theoretical analysis, we can form an overall framework of explainer. I should explain it with the figure which I will change. }

An attributed graph contains graph structure and attributes. Our target is to find a subgraph $G_{s}=(\mathcal{V}_{s}, \mathcal{E}_{s})$ and a subset of attributes $\mathcal{X}_{s}$ that contribute to the GNN prediction. In order to find the important factors, we use mutual information maximization as our objective function~\cite{GNNEx19}, which is defined in Equation~\ref{eq:mi}:
\begin{align}
\label{eq:mi}
\begin{aligned}
	& \max_{G_{s}} MI(Y, (G_{s}, \mathcal{X}_{s}))= H(Y) - \\
	& \qquad H(Y\mid G=G_{s}, \mathcal{X}=\mathcal{X}_{s})
\end{aligned}
\end{align}
where $Y$ is the predicted label for an input graph. The graph structure can be represented by an adjacency matrix $A$ or an edge list $\mathcal{E}$, and node attributes are represented by a node attribute matrix. However, a node consists of its connected edges and attributes.
%The prediction contributions from them should not be individually considered for a node, and an edge is shared by two nodes. 
It is not possible to directly quantify the importance score for a node. Thus, node explanation is considered after edge and node attribute explanation. Here, $G_{s} = (\mathcal{V}, \mathcal{E}_{s})$. 

\textbf{Estimation for edges.} The estimation for the objective function is not tractable since there are $2^{\mid \mathcal{E}\mid}$ different subgraphs for $G$, because each edge is independent. Following the existing works~\cite{GNNEx19, luo2020parameterized}, in consideration of relaxation, we adopt Bernoulli distribution $P(G_{s})=\prod_{(i,j)\in \mathcal{E}}P((i,j))$ for edge explanation, where $P((i,j))$ is the probability of the edge $(i,j)$'s existence. Therefore, our goal for edge explanation is considered as finding the correct $P(G_{s})$.

\textbf{Estimation for attributes.} For the basic GNNs, the same node attributes from different nodes share the same GNN parameters in each layer, while some newly developed GNNs extend the usage of node attributes. For example, GAT~\cite{velickovic2018graph} takes node attributes to calculate attention coefficients. Besides, the same node attributes perform differently when located in different nodes because of the nonlinear computation from GNNs. Node attributes should be explained individually for a graph. We use the same method from edge estimation for node attribute estimation. 

The mutual information quantifies the probability change of GNN prediction with the input limited to $G_{s}$ and $\mathcal{X}_{s}$. An edge $(i,j)$ is considered unimportant when removing it does not largely decrease the probability of prediction. With the pre-trained GNN $\Phi$ being fixed, we rewrite our objective function as minimizing $H(Y\mid G=G_{s}, \mathcal{X}=\mathcal{X}_{s})$, defined in Equation~\ref{eq:obj}, where $C$ is the set of prediction classes. In this way, we make sure the subgraph $G_{s}G_{s}$ and the subset of attributes $\mathcal{X}_{s}$ achieve the maximum probability of prediction.
\begin{align}
\label{eq:obj}
\begin{aligned}
	& \min_{P(G_{s}),P(\mathcal{X}_{s})}-\sum_{c=1}^{C}\mathbbm{1}[y=c]
	\log P_{\Phi}(Y=y\mid \\
	& \qquad G=G_{s}, \mathcal{X}=\mathcal{X}_{s})
\end{aligned}
\end{align}

% The framework of {\name} is shown in Figure~\ref{fig:process}. With the input of an attributed graph, a pre-trained GNN, {\name} firstly estimates the importance of edges and node attributes. Then it attains the importance of nodes from the estimations. The important subgraph and node features are generated from element removal according to the explanation. 

\subsection{Edge and Attribute Explanation}

%{\td why discuss in this order? I think the order of node, edge, and feature looks more natural.}
%\ans{It was explained in the previous subsection. With the changed figure, I will explain it again to highlight this order. The theoretical analysis is based on explanation instead of removal. Node explanation is based on edge/node feature explanation. Thus, node explanation is after them. In the later subsections, I highlighted the order of explanation. The equations for node are generated and derived from the previous subsections. }
%
%{\td why put edge and node feature reasoning together?}
%\ans{It was explained in the previous subsection. In this subsection, I treat them equally except sometimes using edge as example. For the figure, I will make them together too. I want to draw a figure of the overall framework of the explainer, like the survey paper and other explainers. }
%
%{\td `reasoning', I feel it's not accurate. Do they (e.g., GNNExplainer) also use `reasoning'?}
%\ans{Previously you changed "explanation" to "reasoning". They use learning/explanation/selection...I don't think it is formalized here.}

Our goal for edge and attribute explanation is to learn the correct $P(G_{s})$ and $P(\mathcal{X}_{s})$. We introduce edge masks $m^{(\mathcal{E})}$ and node attribute masks $m^{(\mathcal{X})}$ as our learning parameters. We take $P(G_{s})=\sigma (m^{(\mathcal{E})})$ and $P(\mathcal{X}_{s})=\sigma (m^{(\mathcal{X})})$, where $\sigma (\cdot)$ denotes \textit{sigmoid} function. Here, the objective function can be approximated as:
\begin{align}
\label{eq:obj_mask}
\begin{aligned}
	& \min_{m^{(\mathcal{E})},m^{(\mathcal{X})}}
	-\sum_{c=1}^{C}\mathbbm{1}[y=c]	\log P_{\Phi}(Y=y\mid G= \\
	& \qquad (\mathcal{V}, \mathcal{E}\odot \sigma (m^{(\mathcal{E})})), 
	\mathcal{X}=\mathcal{X}_{s}\odot \sigma (m^{(\mathcal{X})}))
\end{aligned}
\end{align}
where $\odot$ denotes element-wise multiplication. Edge masks learn how much message from source nodes should be passed to destination nodes. Node attribute masks learn how much of node attributes should be used for messages. % $P(G_{s})$ and $P(\mathcal{X}_{s})$ represent the importance scores for edges and node attributes. % The masks by \textit{sigmoid} function represent the importance scores.

For undirected graphs, the edge is bidirectional, where the information is passed back and forth. In this paper, we consider all the graphs as directed graphs to estimate the message passing precisely. An edge mask for undirected graph is computed by $\hat{m}_{(i,j)}^{(\mathcal{E})}=\hat{m}_{(j,i)}^{(\mathcal{E})}=Agg(\{ \hat{m}_{(i,j)}^{(\mathcal{E})}, \hat{m}_{(j,i)}^{(\mathcal{E})}\} )$, where $Agg$ is a user-defined aggregation function. GNNExplainer and PGExplainer treat both directions equally by taking the average of two directions. From our practical observation, the performance of the explanation can be improved by applying different aggregation functions. % using different aggregation functions can improve the explanation accuracy.

As Figure~\ref{fig:process} suggests, mask training is similar to GNN training. 
%While the pre-trained GNN stays fixed, the masks are learned through the objective function. 
First, we initialize the masks for edges and node attributes, respectively. Next, the masks are used to add weights on the edges and node attributes of the input graph as in Equation~\ref{eq:obj_mask}. Then, the weighted graph is fed into the pre-trained GNN for mask learning. With the feedback from GNN, the mask values are optimized by minimizing the objective function. The masks are learned iteratively through these steps, so the importance scores are gathered from the learned masks. 

\textbf{Reparameterization trick.}
The importance scores, as weights for mask training, are soft continuous values falling into $(0,1)$. However, an edge should either exist or not, meaning the edges should be binarily indicated. % GNNExplainer uses soft continuous masks. 
Using continuous importance scores will cause the ``introduced evidence'' problem~\cite{yuan2021explainability}. The importance scores add unexpected noise to the input, which does not reflect the real-world explanations. The binary importance scores, however, are not differentiable for researchers to estimate the importance level.
% An edge should either exist or not, while the values in $\sigma (m^{(\mathcal{E})})$ fall into $(0,1)$, which introduces extra values that influence the prediction. However, binary masks are not differentiable for researchers to estimate the level of importance. 
Our solution is to reparameterize importance scores into binary as weights on the input graph, while the differentiable importance scores are still retained for importance estimation. Here, we apply hard concrete distribution~\cite{louizos2018learning} as our reparameterization trick. We rewrite the distribution for edges as:
\begin{equation}
\label{eq:hard_mask}
\begin{aligned}
s &= \sigma ((\log u - \log (1-u) + m^{(\mathcal{E})}) / \beta )\\
\epsilon &= \min (1, \max (0, s(\zeta - \gamma )+\gamma ))
\end{aligned}
\end{equation}
where $u\sim \mathcal{U}(0,1)$ and $\beta$ is the temperature. With $\zeta < 0$ and $\gamma > 1$, we stretch the concrete distribution to $(\zeta , \gamma )$. Distribution in $(\zeta , 0]$ and $[1, \gamma )$ ultimately falls into 0 and 1. Thus, part of the distribution is squeezed into binary. Meanwhile, we take $s = \sigma (m^{(\mathcal{E})} / \beta )$ as the binary concrete distribution for edges, i.e., importance scores, then approximate the ``sub-edges'' as $\mathcal{E}_{s} \approx \mathcal{E}\odot \epsilon$ for edge mask training.


\subsection{Node Explanation}

With learned edge masks and node attribute masks, we need to quantify the importance scores for nodes. Inspired by the Bernoulli distribution for graph structure, the contribution from a node $v_{i}$ is quantified by:
\begin{equation}
\label{eq:prob_node}
	\omega _{v_{i}}=
	\prod_{(i,j)\in \mathcal{E}_{i}^{+}} P((i,j))^{1/\mid \mathcal{E}_{i}^{+}\mid }
	\prod_{t\in \boldsymbol{x}_{i}} P(t)^{1/\mid x_{i}\mid}
\end{equation}
Here, the contribution of a node $v_{i}$ is quantified from the importance scores of its outgoing edges $\mathcal{E}_{i}^{+}$ and node attributes $\boldsymbol{x}_{i}$.
The contribution from edges should be normalized because a node connects arbitrary numbers of edges. We multiple the importance scores of connected edges and extract the $\mid \mathcal{E}_{i}^{+}\mid$-th root of the multiplication.
For node $v_{i}$, we can define the importance score for outgoing edges as $\omega _{\mathcal{E}_{i}^{+}}=\prod_{(i,j)\in \mathcal{E}_{i}^{+}} P((i,j))^{1/\mid \mathcal{E}_{i}^{+}\mid }$, and the importance score for node attributes as $\omega _{\boldsymbol{x}_i}=\prod_{t\in \boldsymbol{x}_{i}} P(t)^{1/\mid x_{i}\mid}$.
% Here, we apply an average trick for the probabilities of edges since each node has various numbers of edges. Equation~\ref{eq:prob_node} quantifies how much a node $v_{i}$ contributes to the GNN prediction in regard to its out-edges $\mathcal{E}_{i}^{+}$ and node features $\boldsymbol{x}_{i}$. 
However, there are two problems with equation~\ref{eq:prob_node}. First, the normalization method may degrade the important edges. An important node can be connected by important and unimportant edges while the unimportant edges decrease the overall importance of its message passing path. Second, node interactions are not considered. Nodes interact through GNN computation, which leads to certain nodes being important to the prediction. 
% The node feature dimension $d$ can be very large. For simplicity, we approximate the overall probability of node features as $\omega _{\boldsymbol{x}_i}=\frac{1}{d}\sum_{t}^{d}P(x_{i,t})$ for node $v_{i}$.

In order to fix the first problem, we take an aggregation function, e.g., $\max$, to calculate the contribution from $\mathcal{E}_{i}^{+}$, $\omega _{\mathcal{E}_{i}^{+}}=Agg(\{ P((i,j))\mid (i,j) \in \mathcal{E}_{i}^{+}\})$. The aggregation function is changeable in order to adjust to different GNNs. But it cannot be directly applied to the incoming edges of $v_{i}$. In GNN computation, a node's representation $\boldsymbol{h}_{i}$ is aggregated from the message passing through its incoming edges $\mathcal{E}_{i}^{-}$. The message information depends on the source node and its connected edge. Thus, we quantify the message importance through edge $(i,j)$ as:
\begin{equation}
\label{eq:msg}
	\omega _{(i,j)} = P((i,j)) \omega _{\boldsymbol{x}_i}
\end{equation}
For a node's importance estimation, we consider the messages from and to the node (outgoing messages and incoming messages) separately since the contribution can vary. With the solution to the first problem, we firstly aggregate the importance scores for outgoing messages and incoming messages of node $v_{i}$ separately:
\begin{equation}
\label{eq:msg_2}
\begin{aligned}
	\omega _{v_{i}}^{(out)} &= Agg_{1}
	(\{ \omega _{(i,j)}\mid (i,j) \in \mathcal{E}_{i}^{+}\} )\\
	\omega _{v_{i}}^{(in)} &= Agg_{1}
	(\{ \omega _{(j,i)}\mid (j,i) \in \mathcal{E}_{i}^{-}\} ) 
\end{aligned}
\end{equation}
Then, we introduce the second aggregation function to compute the ultimate node importance scores from gathering the outgoing messages and incoming messages. Here, we compute the ultimate importance score for $v_{i}$ by:
\begin{equation}
\label{eq:w_node}
	\omega _{v_{i}} = Agg_{2}(\{ \omega _{v_{i}}^{(out)}, \omega _{v_{i}}^{(in)}\} )
\end{equation}

\textbf{Synchronized mask learning. }For different purposes, some graph factors can share the same masks. For example, for undirected graphs, two paths of the same edge can share the same edge mask in order to eliminate the pair difference problem. 
When node attribute explanation is not required, node attributes from the same node $\boldsymbol{x}_i$ can share the same masks. In this way, we are able to directly learn $\omega _{\boldsymbol{x}_i}$ for each node. Thus, the graph structure is explained efficiently with less storage requirement.

\begin{table}[t]
	\caption{The specifications of different dataset and the accuracy of the pre-trained models.}
	\label{tb:setup}
	\centering
	\tabcolsep = 0.18cm
	\begin{tabular}{lcccc}
		\toprule
		\textbf{Dataset} & \makecell{\textbf{Avg. \# }\\\textbf{of nodes}} & \makecell{\textbf{\# of train/}\\\textbf{validation/test}} & \textbf{Model} & \textbf{Accuracy} \\ 
		\midrule
		BBBP & 24.065 & 1,629/205/205 & GCN & 0.878\\
		Mutagenicity & 30.317 & 3,467/435/435 & GCN & 0.805\\
		BA-2motifs & 25.000 & 800/100/100 & GCN & 1.000\\
		Reentrancy & 4.968 & 1,340/$\cdot$/331 & DR-GCN & 0.926\\
		Infinite Loop & 3.686 & 1,056/$\cdot$/261 & DR-GCN & 0.632\\
		CWE-415 & 9.962 & 666/$\cdot$/334 & Devign & 0.949\\
		CWE-416 & 17.839 & 666/$\cdot$/334 & Devign & 0.934\\
		CWE-476 & 9.132 & 666/$\cdot$/334 & Devign & 0.841\\
		\bottomrule
	\end{tabular}
\end{table}%


