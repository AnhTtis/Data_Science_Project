\section{Discussion}
\label{sec_discussion}

%Without constraints of explanation result, the order of factor removal does not influence the ultimate explained subgraph. We suggest to remove nodes first and check if the subgraph would satisfy the constraint. 

Our method can be adjusted to different cybersecurity applications using GNNs since it is comprehensive and the importance scores are learned from the feedback of GNNs. The design is based on the common architecture of GNNs without requiring prior knowledge. The experiment further proves that {\name} improves the performance in both graph classification and node classification. % For graph classification models, we consider the node importance from the interaction of one node and its neighbor nodes. % While for node classification models, we only consider the contribution from nodes to the target node. As a node is not connected, it loses the interaction with other nodes. It will not be able to influence the prediction of the target node. Node explanation can also be conducted for the efficiency of graph structure extraction.

%The design of {\name} is based on generic GNN structure. Our experiment shows its faithfulness and compatibility to different GNN-based security applications. Thus, our explanation method shows great potential to be applied to a wider range of security applications. 
%With a comprehensive explanation, users are able to build trust and evaluate the model performance at the level of human understanding. Researchers are able to pinpoint the problems of datasets and models so as to avoid further serious problems and optimize the performance of models by the refinement of datasets and models.
In this paper, we mainly focus on node attributes as for attribute explanation, while it can be adjusted to different attributes. As the importance scores for edges and attributes are learned, node importance scores are able to be obtained. Several applications including code vulnerability detection construct graphs with edge attributes, but the attributes are not learned by GNNs. Edge attributes, as edge labels in many applications, can be learned and utilized by relational models. Then an edge is denoted as $(i, j, r)$, where $r$ indicates the relationship of the edge. There will be sets of edge lists categorized by the relationships. 
% The edge explanation from {\name} can be extended for different relationships of edges. GNNs using not only node attributes, such as multi-relational graph convolutional networks~\cite{vashishth2020compositionbased}, are also proposed.

The explainability of GNNs is not as well-explored as other traditional deep learning models. Besides understanding the contributive factors to the prediction, there is a significant space to fill in, e.g., global explanation and causal explanation.
It is observed from the EP of the remaining subgraphs that these subgraphs still contribute to the prediction. Different types of explanations are needed for cybersecurity applications. 
{\name} can easily be adjusted for counterfactual explanations by adopting CF-GNNExplainer~\cite{cf_gnnexplainer}. Due to the similarity between explanation and attack, there is work~\cite{xujing2021} to conduct backdoor attacks against GNNs with explanation methods. {\name} can also be utilized for attack and defense. 

%The goal of code vulnerability detection, as an example, is to detect any kinds of vulnerability in complex code. From our case study, we are able to pinpoint the existing problems. It is difficult for current models to extract the semantic information in source code. The existing datasets may not be able to cover all the situations of vulnerability and benignity. For certain datasets, the models may learn wrong features between benign code and vulnerable code, which makes the model vulnerable to attacks and hard to be applied to other real-world datasets with the performance retained. With {\name}, researchers are able to have a comprehensive understanding of the applications. The robustness and flaws from dataset to models are more transparent by the comprehensive and faithful explanation of {\name}.

% {\td Add more discussions from the security perspective.}

% {\td discuss the attributes, especially edge; potential extension -- other GNNs.}


\section{Conclusion}
\label{sec_conclusion}

In this paper, we propose {\name}, an explanation method that provides a comprehensive explanation for GNNs. By learning the importance scores for both graph structure and node attributes, {\name} is able to accurately explain the prediction contribution from nodes, edges, and attributes. 
%{\name} proves its efficiency by learning the importance scores collectively with one-time training. 
We apply {\name} to two cybersecurity applications. Our experiments show {\name} achieves high explanation fidelity. 
% 87.6\% of the subgraphs identified by {\name} are able to retain their original prediction, with an improvement of 10.3\% over the state-of-art methods. % and how an extracted graph contributes to the GNN prediction. 
We also demonstrate the practical usage of {\name} in cybersecurity applications. 
%The explanation results from {\name} suggest the robustness and weakness of a GNN model and the flaws from the datasets. As model transparency is obtained from {\name}, researchers are able to decide if a model is trustable and how to optimize the model performance. % \todo{number}
% The framework of a security application can be optimized and potential flaws can be alleviated according to explanations. Our explanation method also show strong potential in other GNN-based domains.
