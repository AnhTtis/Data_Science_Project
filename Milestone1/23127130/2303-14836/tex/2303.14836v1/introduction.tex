\section{Introduction}
\label{sec_introduction}

% Deep learning techniques have made great achievements in different cybersecurity applications. The use of graph structure and graph neural networks (GNNs) has shown its competence, specifically in source code related applications such as code vulnerability detection \cite{devign}, code clone detection \cite{fa-ast} and botnet detection \cite{zhou2020auto}. With great potential, GNN applications for cybersecurity domain are being developed.
% shown its competence, specifically in source code related applications such as code vulnerability detection \cite{devign}, code clone detection \cite{fa-ast} and botnet detection \cite{zhou2020auto}. With great potential, GNN applications for cybersecurity domain are being developed.

% \Ji{what is graph?}
% \Ji{Graph is widely used in cybercybersecurity applications, e.g., ...}
{\revision
Graph is a structured data representation with nodes and edges, where nodes denote the entities and edges denote the relationship between them.
Graph has been widely used in cybersecurity applications, such as code property graph for code vulnerability detection~\cite{yamaguchi2014modeling}, API-call graph for Android malware detection~\cite{xu2019droidevolver}, and website network for malicious website detection~\cite{li2013ieee}. 
}

Graph neural networks (GNNs) are multi-layer neural networks that can learn representative embeddings on structured graph data~\cite{gnn2019}. 
% GNNs can be used for various downstream tasks, including node classification~\cite{kipf2017semi}, link prediction~\cite{zhang2018link}, and graph classification~\cite{Errica2020A}.
Because of that, GNNs have achieved outstanding performance for various cybersecurity applications, such as malicious account detection~\cite{liu2018heterogeneous,wang2019fdgars}, fraud detection~\cite{liu2020alleviating, dou2020enhancing}, software vulnerability detection~\cite{devign, cheng2021deepwukong, cao2021bgnn4vd}, memory forensic analysis~\cite{song2018deepmem}, and binary code analysis~\cite{xu2017neural, li2019graph, ji2021buggraph}. 
{\revision
Existing works usually construct graphs from an application and train a GNN model that can learn the node or graph representation. The GNN model can be used for various downstream tasks, e.g., node classification~\cite{kipf2017semi}, link prediction~\cite{zhang2018link}, and graph classification~\cite{Errica2020A}.
Taking binary code similarity detection as an example, recent works~\cite{xu2017neural,ji2021buggraph} first transform binary code into an attributed control flow graph. With that graph, they train a GNN model that can represent each graph as an embedding. Finally, they use a similarity function, e.g., cosine similarity, to measure code similarity. 
}
% friend recommendation~\cite{graphrec_www19}, traffic prediction~\cite{GMAN-AAAI2020}, and person re-identification~\cite{shen2018person}. 

% Graph neural networks (GNNs) are multi-layer neural networks that learn representative embeddings on the structured graph data~\cite{gnn2019} for various tasks, including node classification~\cite{kipf2017semi}, link prediction~\cite{zhang2018link}, and graph classification~\cite{Errica2020A}. As a graph is a good representation of entities and their relationship in different networks, GNNs have been shown to yield large benefits in a number of cybercybersecurity  applications, such as malicious account detection~\cite{liu2018heterogeneous,wang2019fdgars}, fraud detection~\cite{liu2020alleviating, dou2020enhancing}, software vulnerability detection~\cite{devign, cheng2021deepwukong, cao2021bgnn4vd}, memory forensic analysis~\cite{song2018deepmem}, and binary code analysis~\cite{xu2017neural, li2019graph, ji2021buggraph}.



%\todo{applications with citations.}
% Besides, more and more graph generation techniques are developed in different application with the success of GNNs. 

%{\td GNN is good --> Explanation on GNN is required but not good enough --> Spe}

%As code property graph started being used to represent source code, the application of GNNs became a novel field to explore. There are a lot of models derived from GNNs for different cybersecurity applications, such as variable misuse prediction task (\textsc{VarMisuse})~\cite{allamanis2018learning}, code vulnerability detection~\cite{devign} and code clone detection~\cite{fa-ast}. 
%With great performance in cybersecurity domain, however, the outputs of such models remain challenging to interpret because of the complexity from both the neural networks and underlying graph structures. As a result, the adoption and adaptation of the GNNs rely empirically~\cite{xu2018how}. The development and usage of models can be ambiguous to researchers. The graph generation technique can be studied by model explanation. Since the graphs do not naturally exist in real world, the problems behind the graph datasets are in shadow. In order to make full use of the models, graphs with both small size and useful information are preferred. 
%Meanwhile, the GNNs are also vulnerable to attacks~\cite{gfattack_aaai20,zugner2018adversarial}. For example, for two representative GNNs such as GCN~\cite{kipf2017semi} and GAT~\cite{velickovic2018graph}, even one edge perturbation in a dataset is able to decrease the prediction accuracy significantly. The interpretation of GNNs is able to help researchers identify the attackers. 
%Clearly, there is an urgent need to make the GNNs more humanly understandable. 



%\begin{table}[t]
%	\caption{Comparison of different GNN explanation methods (\CIRCLE = true; \Circle = false; \LEFTcircle = partially true).}
%	\centering
%	\tabcolsep = .4cm
%	\begin{tabular}{lccc}
%		\toprule
%		{Method} & Node & Edge & Attribute  \\ 
%		\midrule
%		{GNNExplainer} & \Circle & \CIRCLE & \LEFTcircle \\
%		{PGExplainer} & \Circle & \CIRCLE & \Circle  \\
%		{GraphMask} & \Circle & \CIRCLE & \Circle  \\
%		{PGM-Explainer} & \CIRCLE & \Circle & \Circle  \\
%		\textbf{{\name}} & \CIRCLE & \CIRCLE & \CIRCLE  \\
%		\bottomrule
%	\end{tabular}
%	\label{tb:explainers}
%\end{table}%
%\raggedbottom

%\subsection{Motivation}

{\revision
\subsection{Motivation}
% clarifying which specific cybersecurity challenges informed the creation of the explanation approach and what makes ILLUMINATI a cybersecurity-related technique
% \re{cybersecurity challenges}
When a pre-trained GNN model is deployed in reality, it usually generates many positive alarms that need to be manually verified by the cybersecurity analysts to confirm their existence. Unfortunately, existing models usually generate too many alarms that the cybersecurity analysts are not able to verify them in a timely manner, which is known as the threat alert fatigue problem~\cite{hassan2019nodoze}. According to a recent study from FireEye, most organizations in US receive 17,000 alters per week while only 4\% of them are properly investigated~\cite{fireeye}.

% Unfortunately, these automated systems are notorious for
% generating high rates of false alarms [59], [2], [6]. According
% to a recent study conducted by FireEye, most organizations
% receive 17,000 alerts per week where more than 51% of the
% alerts are false positives and only 4% of the alerts get properly
% investigated [4]. Due to an enormous number of alerts, cyber
% analyst face “threat alert fatigue”1 problem and important alerts
% get lost in the noise of unimportant alerts, allowing attacks to
% breach the cybersecurity of the enterprise. One example of this is
% Target’s disastrous 2013 data breach [15], when 40 million
% card records were stolen. Despite numerous alerts, the staff at
% Target did not react to this threat in time because similar alerts
% were commonplace and the cybersecurity team incorrectly classified
% them as false positives. In Fig. 1, we demonstrate the growth
% of alerts generated by a commercial TDS [8] at NEC Labs
% America comprising 191 hosts.


% In other words, the cybersecurity analysts need to understand the deployed cybercybersecurity model.
% the understanding of cybersecurity models has become a mandatory requirement for cybersecurity analysts.
% However, existing GNN-based models often lack sufficient information of why they are predicted as positives. 
% critical for them to understand what has contributed to the prediction from the input. 
% In order to study and prevent the threats, the understanding of cybersecurity models has become a mandatory requirement for cybersecurity analysts. 

% It is more challenging to obtain transparency for cybersecurity models, because these models are usually complex and there are more cybersecurity requirements for explanation methods~\cite{WarneckeAWR20}.
% The development cycle in cybersecurity domain usually starts from data collection and transformation to practical usage, the steps in which may contain different pitfalls. A recent study~\cite{Arp22} identifies ten generic pitfalls related to machine learning in the cybersecurity domain. Overlooking the pitfalls may cause incorrect conclusions and potential risks. The lack of explanation prevents cybersecurity analysts from understanding the model behaviors clearly, identifying the cybersecurity pitfalls, and troubleshooting the errors effectively. This eventually leads to cybersecurity issues and barriers to practical usage.


% Given the predicted results from a pre-trained model, the cybersecurity analysts or end users usually need to manually verify the positive cases. It would be critical for them to understand what has contributed to the prediction from the input. In order to study and prevent the threats, the understanding of cybersecurity models has become a mandatory requirement for cybersecurity analysts. It is more challenging to obtain transparency for cybersecurity models, because these models are usually complex and there are more cybersecurity requirements for explanation methods~\cite{WarneckeAWR20}. The development cycle in cybersecurity domain usually starts from data collection and transformation to practical usage, the steps in which may contain different pitfalls. A recent study~\cite{Arp22} identifies ten generic pitfalls related to machine learning in the cybersecurity domain. Overlooking the pitfalls may cause incorrect conclusions and potential risks. The lack of explanation prevents cybersecurity analysts from understanding the model behaviors clearly, identifying the cybersecurity pitfalls, and troubleshooting the errors effectively. This eventually leads to cybersecurity issues and barriers to practical usage.

To investigate a generated alarm, the cybersecurity analysts usually need to manually figure out why it is predicted as a positive. If such information can be provided automatically, it would greatly help to accelerate the manual investigation process. 
Unfortunately, GNN models lack the explainability similar to traditional deep neural networks.
There have been efforts towards automatically explaining neural networks, such as convolutional neural networks~\cite{Zhang_2018_CVPR}, recurrent neural networks~\cite{bradbury2016quasi}. 
%Many explanation methods have wide applications for images and natural languages.
%, and Word2vec~\cite{mikolov2013efficient}. 
However, they cannot be directly applied because GNNs work on the graph, which is an irregular data structure. 
%The overall GNN explanation can be divided into two parts, graph structure explanation and extra attribute explanation such as node attributes. Studying the graph structure is mostly the key point for graph explanation. 
Each node in a graph can have arbitrary neighbors and the order may be arbitrary as well. 
%Graph structure contains binary information, where an edge should either exist or not. 
%However, each pixel in an image has its certain location and the image is considered as a grid structure. 
%In other words, the graph is in three-dimensional space, while the image is in two-dimensional space.
Therefore, the traditional explanation methods would fail to explain the interaction between node attributes without considering the message passing through edges.

%{\td The next works I would explain will 
%\ans{I rearranged and revised this paragraph.}
% It generates the important subgraph with node attributes. 
% Following GNNExplainer, new works such as PGExplainer~\cite{luo2020parameterized}, \textsc{GraphMask}~\cite{schlichtkrull2021interpreting} and PGM-Explainer~\cite{pgmexplainer} are developed for GNN explanation. {\td the former sentence is meaningless.}
%GNNExplainer provides edge and node attribute explanations by learning corresponding masks, which represent the importance score for the graph attributes.
%Similar works such as 
%share similar methodology with GNNExplainer.
%Specifically, it focuses on NLP applications.
%as they provide local explanations. Instead of local explanation.
%{\td what is local explanation? It has to be explained. Which method uses local explanation?}, 
% Thus, it can be used to explain GNN models. 
% \textsc{GraphMask} determines if an edge can be dropped in every GNN layer, specifically for NLP applications~\cite{schlichtkrull2021interpreting}. PGM-Explainer identifies important nodes by a probabilistic graphical model. Rather than instance explanation~\cite{pgmexplainer}, and XGNN~\cite{Yuan_2020} initiates model-level explanation domain. 
% {\td "based on" is not accurate. What are the actual relationships between these works and GNNExplainer? Consider expanding this paragraph.}
% \ans{I briefly introduced different explainers.}

%On the other hand, \td it should be why these models won't work, not about what they do\td a recent GNN explanation method, i.e., GNNExplainer~\cite{GNNEx19}, assumes that, given a graph, the GNN model makes the prediction based on one of its subgraphs. So, it tries to find out such a core subgraph by learning the masks of edges and attributes, which represent their importance scores.
%Similar to GNNExplainer, PGM-Explainer~\cite{pgmexplainer} and GraphMask~\cite{schlichtkrull2021interpreting} extract an important subgraph from an input graph with a trained GNN.
%In particular, PGM-Explainer identifies important nodes by random node attribute perturbation and a probabilistic graphical model. GraphMask learns to predict whether an edge can be dropped in each GNN layer. 
%These three methods provide local explanation as they explain for a certain input graph.be the ones sharing similar assumption or methodology with GNNExplainer. Then, I would switch to others.}
%Differently, PGExplainer~\cite{luo2020parameterized} provides an efficient global explanation method, which works on multiple input graphs. 
%XGNN~\cite{Yuan_2020} explains a pre-trained GNN model with graph generation. It obtains a graph pattern that represents how GNN makes the prediction.

%On the other hand, \td it should be why these models won't work, not about what they do\td 
On the other hand, several GNN explanation methods are proposed recently~\cite{pgmexplainer,subgraphx_icml21,luo2020parameterized,GNNEx19,schlichtkrull2021interpreting}. However, these methods mainly aim to provide an explanation of certain factors from the input graphs.
Table~\ref{tb:explainers} compares the recent works for GNN explanation.
In particular, PGM-Explainer~\cite{pgmexplainer} and SubgraphX~\cite{subgraphx_icml21} apply a \textit{node-centric} strategy to identify the important nodes as the explanation result. Such a method ignores the edges, which are critical for the cybersecurity analysts to investigate the alarm. The other three methods, i.e., GNNExplainer~\cite{GNNEx19}, PGExplainer~\cite{luo2020parameterized}, and GraphMask~\cite{schlichtkrull2021interpreting}, apply an \textit{edge-centric} strategy by identifying the important edges and regarding the constructed subgraph as the explanation result. Though the subgraph includes both important edges and nodes, the nodes identified in this way are usually not the truly important ones. Besides nodes and edges, only GNNExplainer investigates the important attributes. However, GNNExplainer identifies the important attributes globally, which is not specified for each node or edge.
% PGExplainer and GraphMask require access to GNN layers. 
%XGNN~\cite{Yuan_2020} explains a pre-trained GNN model with graph generation, but the generated graph pattern may not be able to represent the real-world subgraph.
}


\subsection{Requirement}

To accurately explain the GNN models, we believe an explanation method should satisfy the following requirements.
%We also find an explanation method requiring \textit{no prior knowledge} of the model parameters or training details is more reasonable because the vendors usually do not share such information.

\begin{table}[t]
	\caption{Comparison of different GNN explanation methods (\CIRCLE = true; \Circle = false; \LEFTcircle = incomplete).}
	\centering
	\tabcolsep = .08cm
	\begin{tabular}{lcccc}
		\toprule
		{Method} & Node & Edge & Attribute & No Prior Knowledge \\ 
		\midrule
		{GNNExplainer} & \Circle & \CIRCLE & \LEFTcircle &  \CIRCLE \\
		{PGExplainer} & \Circle & \CIRCLE & \Circle & \Circle \\
		{GraphMask} & \Circle & \CIRCLE & \Circle & \Circle \\
		{PGM-Explainer} & \CIRCLE & \Circle & \Circle & \CIRCLE \\
		SubgraphX & \CIRCLE & \Circle & \Circle & \CIRCLE \\
		\textbf{{\name}} & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE \\
		\bottomrule
	\end{tabular}
	\label{tb:explainers}
\end{table}
%\raggedbottom

\textbf{Requirement \#1: comprehensive explanation.}
% \re{Missing references}
{\revision We derive the comprehensiveness from completeness in \cite{WarneckeAWR20}.} Particularly to GNNs, it refers to all the major factors in an input graph, which includes nodes, edges, and attributes. 
The factors in a cybersecurity-based graph are specially constructed from real situations. The information contained by different factors is learned and used by GNNs. 
The distrust in GNNs exists as long as the decision-making is not clear to the cybersecurity analysts. A comprehensive explanation for all the major factors is crucial for them to fully understand the GNN behaviors. 
% However, existing GNN explanation methods are not able to provide such a \textit{comprehensive} explanation. Table~\ref{tb:explainers} compares the recent works on GNN explantion. PGM-Explainer applies a \textit{node-centric} strategy to identify the important nodes as the explanation result. Such a method ignores the edges, which are critical for cybersecurity analysts to investigate the relationships between the nodes. The other three methods, i.e., GNNExplainer, PGExplainer, and GraphMask, apply an \textit{edge-centric} strategy by identifying the important edges and regarding the subgraph constructed from them as the explanation result. Though the subgraph includes both important edges and nodes, the nodes identified in this way usually are usually far more than the truly important ones. Besides the graph structure, only GNNExplainer further investigates the important attributes. However, GNNExplainer identifies the important attributes in a global way, which is not specified for each node or edge. This would not benefit the cybersecurity applications and analysts because the globally identified important attributes might not apply to each node or edge, which we will discuss with the example in Figure~\ref{fig:comparison} in the following. 

\textbf{Requirement \#2: accurate explanation.}
%We will explain it in detail with the application of code vulnerability detection.
%Take the code vulnerability detection as an example shown in 
An explanation is accurate if it is able to identify the important factors that contribute to the prediction. For an accurately identified subgraph, we assume that the prediction probability of it should be close to or even higher than its original prediction probability. 
If a prediction error is not precisely addressed, the same error may lead to vulnerability from malicious attacks. The inaccurate explanation would not be able to help diagnose the error but enlarge the vulnerability.

% However, existing explanation methods for GNNs are not able to provide a \textit{accurate} explanation. As stated above, the recent works apply either \textit{node-centric} or \textit{edge-centric} strategies for graph structure explanation. The unexplained edges or nodes are automatically extracted if they are connected. The explained subgraph from GNNExplainer, for example, may not contain the truly important nodes. What's more, GNNExplainer assigns the same importance to the same attributes, neglecting the specification for different nodes and edges. This would leave challenges for cybersecurity analysts to pinpoint the specific important attributes.\todo{definition of faithful here. and why it's required in cybersecurity applications?}

\textbf{Requirement \#3: no need for prior knowledge of GNN models.}
%The explanation methods should be applied to a wide range of GNNs. 
% \re{Missing references}
The cybersecurity models are not easily accessible due to two major reasons. {\revision First, the cybersecurity applications require more complex neural network architectures~\cite{WarneckeAWR20}.} Not only do the models consist of different types of neural networks, but the GNNs are adapted differently from basic GNNs. {\revision Second, in real scenarios, the users often times are using pre-trained models~\cite{hu2020pretraining} especially for complex models.} The prediction accuracy itself does not alleviate the distrust of a model from the users, due to the lack of transparency. 
%Besides, there are specialties for different GNNs and not all the GNNs are fully accessible, and there can be multiple different implementations for the same model. 
Explanation methods without the need for prior knowledge are easier to access and utilize because of their flexibility. With the constraints, the explanation method with no prior knowledge requirement is preferred by cybersecurity analysts. 

%Similar observations also hold in other applications. Table~\ref{tb:explainers} summarizes the explanation objects from different explanation methods. They fail to provide a comprehensive explanation. Most of them focus on graph structure explanation and the attribute explanation is not considered in their frameworks. Nodes and edges are not specifically explained to construct the important graph structure, which can lead to unfaithful subgraph extraction. Particularly, PGExplainer and GraphMask require the access to the layers of GNNs. 
% \todo{briefly discuss 1 or 2 applications on both accuracy and comprehensive}.


%{\td not good enough. write in a more confident way. explain each requirement with example.}
%However, the cybersecurity-related applications require more information beyond what the current explanation methods can supply. 
%In particular, we observe four major requirements from cybersecurity-related applications as shown in Table~\ref{tb:explainers}. 

%existing explainers are not able to satisfy the rquirement of real applications, with the analysis showing in Table~\ref{tb:explainers}. 
% We summarize four requirements in 
%Taking a cybersecurity application as example, 
%Table~\ref{tb:explainers}.
% {\td revise as we discussed. The order should be same with the table.}
% \ans{I put comprehensive explanation on top, combine node+edge as graph structure}

%\todo{try to use a cybersecurity application as an example for each requirement, and it's better to use the cybersecurity applications we discussed in the first paragraph. Or try to use the two applications we deeply studied.}
%\ans{I changed the examples into cybersecurity applications.}

%\begin{table}[t]
%	\caption{Comparison of different GNN explanation methods (\CIRCLE = true; \Circle = false; \LEFTcircle = partially true). We compare explanation methods about what graph attributes they provide explanations for, and whether they require GNN details, e.g., GNN parameters. %{\td I don't think `Disassembly` is  the correct word., `Model details`?}
%		%{\td can you remove the box, just use mark or check?}
%		%\ans{I followed the table from LEMNA}
%		%\ans{This looks better.}
%	}
%	\centering
%	\tabcolsep = .18cm
%	\begin{tabular}{lcccc}
%		\toprule
%		{Method} & Node & Edge & Attribute & GNN detail \\ 
%		\midrule
%		{GNNExplainer} & \Circle & \CIRCLE & \LEFTcircle & \Circle \\
%		{PGExplainer} & \Circle & \CIRCLE & \Circle & \CIRCLE \\
%		{GraphMask} & \Circle & \CIRCLE & \Circle & \CIRCLE \\
%		{PGM-Explainer} & \CIRCLE & \Circle & \Circle & \Circle \\
%		\textbf{{\name}} & \CIRCLE & \CIRCLE & \CIRCLE & \Circle \\
%		\bottomrule
%	\end{tabular}
%	\label{tb:explainers}
%\end{table}
%\raggedbottom

%\textbf{Comprehensive Explanation.} Given the predicted result from a pre-trianed model, the cybersecurity analysists usually need to manually verify the positive casee. So, it would be critical for them to know what contributes to the prediction for the input. In the domain of GNN, that refers to all the major factors in an input graph, which are node, edge, and attribute. 
%% {\td we need to define what is comprehensive explanation?} 
%%It helps researchers understand the model and the input graphs better. 
%% For example, in image classification task, a node denotes an object or attribute, an edge indicates the relationship between nodes and node attributes are vector embeddings for a node representing the characteristic information. Different graph attributes carry different and valuable information for an image.
%Taking malicious account detection as an example~\cite{liu2018heterogeneous}, a node denotes an account or a device, an edge indicates the login behavior of an account on a device during a time period, and the node attributes are encoded by an encoder function from nodes, which represent the characteristic information of a node. 
%A comprehensive understanding of how attributes contribute to the prediction is necessary in order to fully understand the models.
%% As GNNs work in more and more domains, graph generation becomes part of the applications. All the attributes in a graph should be thoroughly explained and studied. 
%Existing explanation methods only target on specific attributes of a graph and it is painstaking to adapt and combine different explanation methods together. The combined explanation methods are not able to maintain the efficiency. 

%\todo{can we replace this example to a cybersecurity application?}

%\textbf{Graph Structure.} A graph structure contains nodes and edges, which is the key to graph explanation and should be specially treated for all the GNN related applications. Applications like smart contract vulnerability detection~\cite{ijcai2020-454} and code vulnerability detection~\cite{devign}  are graph classification tasks. The graphs are constructed from function code. The models determine whether the code is benign or vulnerable from graph classification. A node itself is still able to contribute to the prediction by its node attributes and the algorithm of GNNs, especially for models with graph embedding layers. To provide a detailed explanation for graph structure, both node and edge explanations are necessary, while GNNExplainer and PGM-Explainer only provide one of them, shown in Figure~\ref{fig:comparison}.
%% Node explanation for graph classification tasks. A node without edges is still able to contribute to the prediction by its node attributes and the algorithm of GNNs. Edge explanation itself can degrade the performance of graph structure explanation when nodes are removed with unimportant edges. Besides, node explanation by only attribute perturbation neglects the contribution of graph structure. 
%
%\textbf{Attribute.} Node attributes are the most widely used supplemental information in cybersecurity-based applications.
%In cybersecurity applications, generally, a node represents an object and an edge represents a relationship. Most GNNs are learned through the message passing of node attributes. Thus, node attributes are the most important supplemental information for a graph.
%In GNN-based fraud detection application~\cite{dou2020enhancing}, node labels representing benign or suspicious are treated as binary node attributes. 
%% Even for molecule graphs, node labels are treated as binary node attributes. 
%The node attributes should be explained individually for each node because their contributions vary, e.g., the contributions of the same atoms but from different parts of the molecule. Only GNNExplainer provides node attribute explanation but it is a global explanation for all the nodes in the same graph.


% A node usually contains multiple and different kinds of node attributes. The same node attribute from different nodes has different contributions, e.g., the same atoms from different parts of the molecule. Besides, GNNs have greatly evolved in recent years. Node attributes may also be used to calculate attention coefficients~\cite{velickovic2018graph} so influence the contribution of graph structure. 

%\textbf{GNN detail requirement.} Explanation methods that do not need to disassemble or require parameters from GNNs work on wider range of applications.
%%target on black-box models . These methods  
%In real scenarios, often times, the users are using pre-trained models. The model details and parameters of function identification in binary code~\cite{bao2014byteweight}, for example, are not totally accessible. \todo{For example, cybersecurity application.}
%Thus, an explanation method without further requiring GNN details is preferred. 
%%{\td Be consisitent with the terms, black-box needs to be defined if you really want to use it. I would prefer simply say training details.}
%%{\td The main point is why  \textbf{real applications} require them! Generate stuff does not help here. As we discussed, try to expalin with real applications (like a cybersecurity application), try to make examples.}

%{\td why do we need to explain GNNs?}

%{\td Have not hit the point of why traditional NN explanation methods can not be directly used. The following two sentences do not make sense.}
%\ans{I revised this part by stating the speciality of graph structure.}
%In order to design an explanation method and understand the GNN explanations, pre-knowledge about graph is required. Other than images or natural languages, the information of graphs may not be straightforward or humanly understandable.



\begin{figure*}[t]\centering
	\includegraphics[width=0.9\textwidth]{./picture/Comparison-cwe415-39-optional2.pdf}
	\caption{Explaining an example code predicted as vulnerable by a pre-trained GNN model with different explanation methods. (a) shows an example source code with ``double free'' vulnerability, (b) shows the converted \textbf{\underline{A}}ttributed control and data \textbf{\underline{F}}low \textbf{\underline{G}}raph (AFG) and a pre-trained model, and (c) shows the explanation results with the identified important factors colored. Specifically, GNNExplainer identifies important edges and treats the same attributes from different nodes identically, PGM-Explainer identifies important nodes only. 
	% {\todo revise the result of GNNExplaner}
		%		Generally, explanation methods take a graph and a GNN with the prediction as input, then extracts important attributes as output. GNNExplainer extract important edges for graph structure, connected nodes are automatically extracted without further analysis. Node attribute extraction is global for the input graph. It fails to explain node attributes for a specific node. PGM-Explainer focus on important node identification. {\name} makes a comprehensive explanation so extracts a more accurate subgraph.%{\td you can use some colors.}
		%	{\td This figure looks good for now. Consider making it even stronger with a real simplified example, e.g., using vulnerability detection.}
	}
	%	\td{This figure might not be appropriate in Introduction.}
	%	\td{Instead, we need an example figure to show the difference between ours and others (as we discussed). (a) Example code with vulnerability (the nodes contributing to the vulnerablity are highlighted), (b) Identified subgraph from GNNExplainer, (c) Identified important nodes, edges, and attributes from {\name}. }
	%		\ans{will change the figure}
	\label{fig:comparison}
\end{figure*}

\subsection{Contribution}

Motivated by these, we design a comprehensive and accurate GNN explanation method, {\name}. 
%A brief analysis of similar explanation methods is shown in Figure~\ref{fig:comparison}. 
%{\td We need to overview {\name} here. In the same time, explain how {\name} can satify the four requirments.}
%\ans{I revised this part and contribution.}
% Given a pre-trained GNN model and a graph as inputs, {\name} provides a comprehensive explanation by identifying the node, edge and attribute that are critical to the prediction.
Given a pre-trained GNN model and a graph as inputs, {\name} firstly learns the importance score of edges and node attributes collectively by using edge masks and attribute masks. {\name} then aggregates the learned masks and computes the importance score of nodes. In the end, our method identifies the important subgraph towards the GNN prediction. Attribute masks are applied locally to each attribute of each node so that we can identify the attributes that are important to different nodes. Further, {\name} does not require prior knowledge of the pre-trained model, which makes it more applicable to cybersecurity applications.

% \todo{first, general evaluation result.}
We compared the explanation performance of {\name} with prior works on public datasets and cybersecurity application datasets. We focused on two cybersecurity applications, i.e., smart contract vulnerability detection and code vulnerability detection. The evaluation is based on the prediction change between the input graph and the explained subgraph. 87.6\% {\name}-explained subgraphs retain their original prediction, with an improvement of 10.3\% over the baseline methods. %The decrease of prediction probability is alleviated from 21.6\% to 10.8\%. 
Then we provided case studies for explaining the two real-world applications and a deep analysis of the model behaviors. 
% We evaluated {\name} on two real-world cybersecurity applications, i.e., smart contract vulnerability detection and code vulnerability detection. The experiments show that {\name} provides strong and humanly understandable explanations. 
% \todo{second, need numbers, at least cover the abstract.}
We believe they can help cybersecurity analysts quickly understand and diagnose the alarms generated by applications using GNN models.
% {\name} treats GNNs as black-boxes without further information requirement. 

% \re{Remove the repetitive parts R\#3}
{\revision
In summary, we make three major contributions. 
\begin{itemize}
\item \textbf{New insight and method}. 
To the best of our knowledge, this is the first GNN explanation method to provide a comprehensive and cybersecurity-specialized explanation method for cybersecurity applications using GNN models. 
% We extract the core subgraph by combining node and edge explanation and explain node attributes individually. {\name} learns the importance scores for edges and attributes, then compute the node importance score for each node by aggregating the importance scores of related edges and node attributes. 

\item \textbf{Extensive evaluation.} % \todo{summary of the previously discussed experiment details.}
We evaluate the performance of {\name} quantitatively with two cybersecurity applications. The results show {\name} outperforms existing explanation methods in terms of not only accuracy but also cybersecurity requirements.
% We firstly verified the performance of {\name} quantitatively by different evaluation metrics. Then we deeply analyzed the usage of {\name} with specific cases from the two cybersecurity applications. All the evaluations show significant explainability improvements in cybersecurity domains.
% We firstly verified the performance of {\name} on public datasets. Then we analyzed the explanation fidelity on two cybersecurity applications by studying the important and unimportant subgraphs. Evaluation for attribute explanation was specially performed. With the ablation study, we proved the node contribution contains both edge and attribute contributions. Finally, we studied the explanation of {\name} on specific cases from the two applications and discussed how {\name} could help cybersecurity analysts.

\item \textbf{Cybersecurity case study.} 
We demonstrate the practical usage of {\name} with the case study of cybersecurity applications. We interpret the model behavior from both correct and incorrect predictions through the output of {\name}, as well as analyze how we can troubleshoot and improve the models.

\end{itemize}

% {\name} is different from prior works in several aspects. First, in contrast to the works that target on explaining a specific factor, {\name} is the first explanation method that provides a comprehensive explanation for GNNs. Though with a comprehensive explanation, {\name} works in reasonable time complexity. Second, existing works either explain edges or nodes for graph structure explanation. {\name} generates a detailed graph structure explanation from the combination of node and edge explanation. Especially, {\name} improves the explanation accuracy for graph classification tasks, for which a separate node explanation is necessary. Third, for node explanation, current works either ignore explaining nodes separately or the interaction from their connected edges. {\name} provides a novel method to estimate the importance of nodes, which are regarded as combined entities with different factors (edges and attributes).

% \re{Novelty} 
The main novelty of {\name} is to jointly consider the contributions of nodes, edges, and attributes. Also, we analyze and prove that explaining node importance is critical for graph classification tasks.
Further, we find the node attributes should be explained individually for better comprehensiveness and accuracy. 

{\name} is different from existing works in terms of providing a comprehensive and accurate explanation method specialized for real cybersecurity applications. Particularly, compared with a representative related work, i.e., GNNExplainer, it is a generic method that only explains edges and does not explain node attributes individually.
}


%\textbf{Organization.}
%The rest of this paper is organized as follows. In Section~\ref{sec_problem}, we define the research problem. In Section~\ref{sec_explain_apps}, we discuss the requirements of explaining GNN-based cybersecurity applications and why existing methods do not work. In Section~\ref{sec_gnn_explanation}, we present our explanation method {\name}. The experimental evaluation is discussed in Section~\ref{sec_experiment}. The case study of explaining applications is analyzed in Section~\ref{sec_case_study}. Further related works are provided in Section~\ref{sec_related_work}. Finally, we discuss and conclude the paper in Section~\ref{sec_discussion} and Section~\ref{sec_conclusion}.

% {\td Could you summarize and write them in bullets?}
%\begin{itemize} %[leftmargin=*]
%	\item Contribution 1 ...
%	\item Contribution 2 ...
%	\item Contribution 3 ...
%	
%	\item we provide fine-grained and comprehensive explanation with one-time training.
%	
%	\item we propose a method for node explanation 
%	
%	\item we pinpoint the problem of edge explanation for graph classification models and add node explanation; 
%	
%	\item we add single attribute explanation 
%	
%	\item explainer for cybersecurity applications
%
%\end{itemize}


%{\td how many contributions do we have? could you summarize and write them in bullets?}
%\ans{the paragraphs are imitated from other papers. I listed problems/goals first and then introduce my method. It can be changed if contribution with bullets works better. The contributions are: (1) we provide fine-grained and comprehensive explanation with one-time training; (2) we propose a method for node explanation (3) we pinpoint the problem of edge explanation for graph classification models and add node explanation; (4) we add single attribute explanation (5) explainer for cybersecurity applications}

