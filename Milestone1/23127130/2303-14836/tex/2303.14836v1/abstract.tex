\begin{abstract}

%With the great progress of Graph Neural Networks (GNNs) being developed and used in real-world applications, the explanation for GNN prediction, however, has not been well explored.
%Graph neural networks (GNNs), which are multi-layer neural networks working on the irregular graph data, have been applied in various applications.
%As it shares the same explanation limitation with traditional neural network, several recent works have tried to explain how GNNs make the prediction. 
%However, they failed to satisfy the explanation requirements of real applications in terms of providing a comprehensive explanation result including nodes, edges, features, and without knowing the details of the pre-trained model.
%%The real-world applications require more information beyond what the current explanation methods can supply. 
%In this study, we design {\name}, a comprehensive explanation method for GNNs.
%% that provides explanations for nodes, edges, and features. 
%Given a graph and a pre-trained GNN model without knowing the training details, {\name} is able to identify the important nodes, edges, and features that are contributing to the prediction result. 
%We evaluate {\name} on two cybersecurity applications, i.e., smart contract vulnerability detection and code vulnerability detection. Compared with previous works, {\name} achieves significant better explanation results. Not limited, the output of {\name} can be easily understood by the domain experts.


Graph neural networks (GNNs) have been utilized to create multi-layer graph models for a number of cybersecurity applications from fraud detection to software vulnerability analysis.
Unfortunately, like traditional neural networks, GNNs also suffer from a lack of transparency, that is, it is challenging to interpret the model predictions. Prior works focused on specific factor explanations for a GNN model. 
%Without a comprehensive explanation method working on nodes, edges and attributes, the trust and development of security applications are limited by the lack of transparency. 
In this work, we have designed and implemented {\name}, a comprehensive and accurate explanation framework for cybersecurity applications using GNN models. Given a graph and a pre-trained GNN model, {\name} is able to identify the important nodes, edges, and attributes that are contributing to the prediction while requiring no prior knowledge of GNN models. We evaluate {\name} in two cybersecurity applications, i.e., code vulnerability detection and smart contract vulnerability detection. The experiments show that {\name} achieves more accurate explanation results than state-of-the-art methods, specifically, 87.6\% of subgraphs identified by {\name} are able to retain their original prediction, an improvement of 10.3\% over others at 77.3\%. %\td 73.2\%. 
%The difference of prediction probability between the input graphs and the explained subgraphs is improved from 21.5\% to 11.2\% averagely by {\name}.
% By comparing the prediction decreases in terms of accuracy and probability caused by the explained subgraph, {\name} improves 0.129 from 0.259 for accuracy and 0.103 from 0.215 for probability over the existing methods. % \td need numbers \td. 
Furthermore, the explanation of {\name} can be easily understood by the domain experts, suggesting the significant usefulness for the development of cybersecurity applications.

%With the development of Graph Neural Networks (GNNs), the explanation of GNN prediction comes into a challenging problem. 
%The existing instance-based explanation frameworks identify graph structure by edge reasoning, which results in explanation inaccuracy for graph classification models. Feature explanation is based on the whole graph, while the feature contribution can be different from different nodes.
%To address the problems, we propose {\name}, a node-centric explainer for GNN prediction. {\name} identifies graph structure based on a novel node reasoning technique and explains node features individually. 
%Experiments show significant improvements on subgraph identification and feature extraction. The generated attributed subgraph can help users better understand GNNs and datasets.

%{\td what is the page limit? can you write here?}
%{\td Please use the latest USENIX template.}

%\ans{Page Limit: 13 pages, excluding bibliography and well-marked appendices.}

%\ans{Deadline: 9/22}

%{\td Read LEMNA again and follow their logic flows.}


\end{abstract}