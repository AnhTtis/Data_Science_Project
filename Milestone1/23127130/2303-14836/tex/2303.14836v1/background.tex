% \section{Problem Definition}

{\revision
\section{Security Cases and Threat Models}
\label{sec_problem}
% \subsection{\re{} Security Threats}

% In this paper, we consider two security threats, code vulnerability and smart contract vulnerability.

\subsection{Case \#1: Code Vulnerability}

% \todo{complete the section}
% ok
% Explain what is code vulnerability, and detection.
\textbf{Code vulnerability} is the flaw or weakness in the code that can cause risks and be exploited by the attackers to conduct unauthorized activities, e.g., stealing data~\cite{chen2018internet}. For example, the straightforward risks of buffer overflow are data loss, software crashes, and arbitrary code execution, which can be exploited by attackers. A program is classified as vulnerable if it contains a vulnerability. The tested CWE dataset has three types of vulnerability: ``double free'', ``use after free'', and ``NULL pointer dereference''. 

% Threat model.
\textbf{Threat model.}
The attackers can exploit the detected vulnerabilities to initialize malicious actions by using various attack patterns against the software or the system. The attackers can exploit the vulnerability simultaneously in different rewarding approaches, such as hacking tools and remote commands. These attacks may eventually lead to software crashes and data loss, profoundly, financial loss and privacy leakage. This impacts both users and developers.

\subsection{Case \#2: Smart Contract Vulnerability}
\textbf{Smart contract vulnerability} is a coding error that can be exploited by attackers to cause financial loss. A program with such a coding error is classified as vulnerable. Smart contract vulnerability is dangerous because most smart contracts deal with financial assets directly, and the blockchain cannot roll back changes. We study two types of vulnerabilities, i.e., reentrancy vulnerability and infinite loop vulnerability. The reentrancy vulnerability occurs when the contract transfers funds before the balance is updated. The infinite loop occurs when the loop never finishes.

\textbf{Threat model.}
The attackers can exploit the logical errors to conduct the attack by submitting a transaction to the blockchain. This can cause transaction failures or repeated transactions, which eventually lead to financial loss. For example, the malicious contract can drain funds from the reentrancy-vulnerable contract by recurrent re-entrant calls~\cite{Daniel_21}. The DAO attack is one exploitation case to such vulnerability. The attack conducts repeated withdrawals before the balance update. This attack has caused significant money stolen.
}

\section{Background}
% \todo{}

\subsection{Graph Neural Networks}

% S\todo{redefine, rewrite}

\textbf{Graph Neural Networks (GNNs)}, $\Phi$ takes an attributed graph $G=(\mathcal{V}, \mathcal{E})$ and $\mathcal{X}$ as input then generates a set of node representations $\mathcal{Z}$ through hidden layers, where $\mathcal{V}$ and $\mathcal{E}$ denote nodes and edges, and $\mathcal{X}$ denotes attributes.
% defined as graph structure $G=(\mathcal{V}, \mathcal{E})$ and attributes $\mathcal{X}$. GNNs then output the final node representation through the hidden layers of GNNs, which is used by task functions. For example, graph classification tasks transfer the node representation into graph representation, then make the prediction. 
% GNNs are developed and adapted variously for different security applications. In this paper, we focus on GNNs based on such framework so our explanation method is more applicable. 
% \todo{rewrite, explain this is the GNN we focus, not all the GNNs are like this.}
%\ans{Here is revised.}
%{\td why not put $\mathcal{X}$ as an element in $G$? I recall we have discussed this before. I still think node attribute should be a part of the graph, which is why it's called attributed graph.}
%\ans{In graph/GNN related papers, formally G only represents graph structure. For different usage of graphs, it is usually represented by G and other attributes like node attributes $\mathcal{X}$, edge attributes, node labels, etc. }
% $\mathcal{V}$ and $\mathcal{E}$ denote the node and edge set. Alternatively, the edge set can be represented as an adjacency matrix $A \in \lbrace 0,1\rbrace ^{\mid \mathcal{V}\mid \times \mid \mathcal{V}\mid }$. $X \in \mathbb{R}^{\mid \mathcal{V}\mid \times d}$ denotes the node attribute matrix with $d$-dimensional attributes for each node.

A GNN, $\Phi$ takes two major operations to compute node representations $\boldsymbol{h}$ in each layer~\cite{kipf2017semi, hamilton2017inductive, velickovic2018graph, ChenMX18}. In the $l$-th layer, GNN computes the neighbor representation $\boldsymbol{h}_{\mathcal{N}_{i}}^{(l)}=\textsc{Agg}(\{ \boldsymbol{h}_{j}^{(l-1)}\mid v_{j}\in \mathcal{N}_{i})\} )$ for node $v_{i}$ firstly, by aggregating its neighbor nodes' representations from the previous layer. Then, the new node representation is updated from the aggregated representation and its representation from the previous layer: $\boldsymbol{h}_{i}^{(l)}=\textsc{Update}(\boldsymbol{h}_{i}^{(l-1)}, \boldsymbol{h}_{\mathcal{N}_{i}}^{(l)})$. The final representation for node $v_{i}$ is $\boldsymbol{z}_{i}=\boldsymbol{h}_{i}^{(L)}$ after $L$ layers of computation. 
The final node representations are used for different tasks such as graph classification.
A generic graph classification model contains a pooling method and fully connected layers after GNN layers. The pooling method gathers node embeddings into a graph embedding and the fully connected layers compute the classification. 

In this paper, we design our explanation method based on the GNNs with such architecture, so our explanation method is more applicable. 

\subsection{GNN Explanation}

%{\td explain the problem of GNN Explaination here.}

\textbf{GNN explanation} takes an attributed graph and a pre-trained GNN model as input, then identifies the key factors that contribute to the prediction. % \todo{what as input, what did it do, and what it outputs.}
% provides humanly understandable graph patterns that indicate how GNNs make certain predictions.  % \todo{is ....}
% \ans{Here is revised. }
%In general, the factors consist of graph structure and attributes, where the graph structure contains nodes and edges. 
{\revision
Specifically, the task for the explanation methods is to identify the nodes, edges and attributes that contribute most to the prediction. }
% Explanation methods provide humanly understandable explanations for GNN prediction results by identifying important features from input graphs. 
For graph classification tasks, given an input graph $G$ with attributes $\mathcal{X}$ and a pre-trained GNN model $\Phi$, the GNN will make the prediction by computing the label $y$ with the probability $P_{\Phi}(Y=y\mid G, \mathcal{X})$. The task of explanation methods is to reason why the input graph is classified as $y$ by $\Phi$. The explanation offers a set of important factors that contribute to the prediction, for example, by retaining important edges~\cite{luo2020parameterized,schlichtkrull2021interpreting}.  % For GNN explanation, it can be important graph structures or node features. Thus, the decision of prediction can be explained by the identified important features, which can be understood by humans.

In this paper, we develop the explanation method {\name} for GNN models in cybersecurity domain. Existing works only focus on specific factors to explain. {\name} provides a comprehensive and accurate explanation for all the graph factors, which benefits the development of cybersecurity applications.
%{\name} provides a comprehensive and humanly understandable explanation for all the graph features.


\textbf{Example with code vulnerability detection.}
Figure~\ref{fig:comparison}(a) shows an example source code with a ``double free'' vulnerability, which happens when the second \texttt{free} (line 12) is called after the first \texttt{free} (line 9). Vulnerability detection methods firstly convert the source code to an attributed graph. For example, we construct the attributed graph from the source code as shown in Figure~\ref{fig:comparison}(b) by building the {\revision \textbf{\underline{A}}ttributed control and data \textbf{\underline{F}}low \textbf{\underline{G}}raph (AFG)} and encoding the syntax attributes for each node. The node denotes the statement, the edge denotes control or data flow between two statements, and the attributes include syntax features, such as which keywords are used in a statement. Using the AFGs and their corresponding labels (benign or vulnerable) as the training dataset, one can train a GNN model for vulnerability detection, e.g.,  Devign~\cite{devign}. 

% GNN explanation methods take an attributed graph and a pre-trained model as inputs. 
For the AFG generated from the example source code in Figure~\ref{fig:comparison}, nodes 9, 12 and the keyword \texttt{free} should be identified in the final explanation results. Figure~\ref{fig:comparison}(c) presents the output from two recent representative works and {\name}. 
%GNNExplainer does not directly identify the important nodes. Instead, it
GNNExplainer estimates the edge importance from the AFG by learning the soft continuous edge masks. In this example, GNNExplainer identifies $(4, 9)$ and $(5, 9)$ as important and considers this subgraph as the explanation result. This is not accurate because node 12 is missed due to none of its edges is considered important. 
%As for the attributes, GNNExplainer captures \texttt{free}
% GNNExplainer extracts the important graph structure only by edge explanation, and only correctly locates line 9 as a vulnerable factor. The extracted nodes (line 5, 9, 11) are all connected, which leads to unfaithful explanation and extra efforts are needed to determine the most connected subgraph. 
%The identified keywords are also not faithful enough because GNNExplainer assigns the same importance weights to the same keywords even they are from different lines of the source code. 
% \todo{how did PGM-Explainer identify 5, 9, 11?}
PGM-Explainer samples a local dataset by random attribute perturbation to the AFG. With the perturbed nodes and the prediction change being recorded, a probabilistic graphical model is utilized to identify the important nodes. As a result, nodes 5, 9, and 11 are identified.
%PGM-Explainer finds the important statements by a probabilistic graphical model, which is learned from its sampled datasets. The datasets are sampled by random node attribute perturbation. From the perspective of source code, whether a statement is perturbed and the prediction change is noted as one sampled data. As the dataset is generated, it is firstly shrunk by the Grow-Shrink (GS)~\cite{NIPS1999_GS} algorithm. The important statements are then identified by a Bayesian network.  
%Whether a node is perturbed and the prediction change is noted for dataset generation. Then the Grow-Shrink (GS)~\cite{NIPS1999_GS} algorithm is conducted to shrink the datasets and a Bayesian network is used to explain the GNN model. 
The explanation from PGM-Explainer misses node 12. 
% Without taking edges and attributes into account, the explanation is not comprehensive. 
% The explanation methods requiring prior knowledge, e.g., PGExplainer and GraphMask, are ruled out for this scenario.
% The node explanation from PGM-Explainer is attribute perturbation based, which neglects the contribution from the connected edges. The contribution from edges and node attributes is not provided by PGM-Explainer. 
% It is not able to differentiate the contribution from edges or identify the importance of statement keywords. % \todo{ continue} GNNExplainer -- no 12, the identified attributes are also not accurate; PGM-Explainer -- also misss 12, and the attributes are also not accurate. 
Such explanations will confuse a cybersecurity analyst or lead to a wrong conclusion.

%\section{Explaining GNN-based Security Applications}
%\label{sec_explain_apps}
%%{\td This section needs to rewrite. Deeply explaining one application is better.}
%%\ans{Here tried to follow LEMNA, but with one application.}
%
%%{\td PLEASE REVISE based on the previous comments?}
%%
%%{\td We should first explain why real applications (don't say security only for now) require the four key aspects listed in Table 1. Later, we explain with example applications (security applications get in way now). We need to discuss 1. why existing works, e.g., GNNExplainer, can not work well? 2. Why real applications require the explanation results on node, edge, and specific features?}
%%\ans{I use code vulnerability detection as example}
%
%%{\td This paragraph is redundant with the Introduction.}
%%With great performance and evolutions in real-world applications, however, the outputs of GNNs remain challenging to interpret because of the complexity from both the neural networks and underlying graph structures. As a result, the adoption and adaptation of the GNNs rely empirically~\cite{xu2018how}. The development and usage of GNNs can be ambiguous to researchers. 
%%%Meanwhile, the GNNs are also vulnerable to attacks~\cite{gfattack_aaai20,zugner2018adversarial}. For example, for two representative GNNs such as GCN~\cite{kipf2017semi} and GAT~\cite{velickovic2018graph}, even one edge perturbation in a dataset is able to decrease the prediction accuracy significantly. The interpretation of GNNs is able to help researchers identify the attackers. 
%%%Clearly, there is an urgent need to make the GNNs more humanly understandable. 
%%%{\td PLEASE REMOVE THE CONTENTS from what I commented out in the Introduction!}
%%There have been works for GNN explanation, however, the explanation for the real-world applications requires more than what the existing works can supply.
%
%%{\td Make sure there are no duplicates with the Introduction.}
%
%% Explanation methods have shown strong competence to provide humanly understandable explanations for different practical uses. However, the lack of transparency for GNN-based security applications limits the trust and development of these applications. 
%%The users are able to gain trust on GNN-based models and evaluate the performance in real world as they understand how the prediction is made. 
%% The researchers are able to make the right adjustment to the datasets and models with understandable explanations.
%GNNs have shown significant potential in security applications. The lack of transparency for GNNs limits the trust and development of such applications. It is crucial for security analysts to both gain trust and troubleshoot the flaws precisely. So the explanation for these applications requires to be \textit{comprehensive} and \textit{faithful}. 
%In the following, we focus on code vulnerability detection as an example for GNN-based security application. First, we introduce the framework of the application. Next, we analyze the explanation requirements for the applications. Then, we discuss why existing explanation methods are not able to provide \textit{comprehensive} and \textit{faithful} explanations. 
%
%\subsection{Framework of Applications}
%%\ans{How to do code vulnerability detection. }
%The framework to develop a GNN-based security application usually consists of graph structure generation, attribute encoding and GNN learning. 
%% In order to fully understand and develop an application, comprehensive and faithful explanation is necessary. For GNN-based security applications specifically, the requirements for the explanation method fall into these aspects: (1) \textit{comprehensive explanation}, (2) \textit{faithful explanation}. % , (3) \textit{model-agnostic explanation}. 
%The development of code vulnerability detection technique, e.g., Devign~\cite{devign},  firstly transfers the source code into code property graphs, and encodes node attributes for each node. The GNN-based model is then learned from a set of extracted code property graphs with labels indicating whether the source code is vulnerable. We summarize three steps of such applications. 
%(1) \textit{Graph Extraction.} The code property graph is the most widely used graph to represent source code. A node represents a program construct such as variables, statements and symbols; an edge contains the direction and relationship information for a pair of nodes such as control flow and data flow. 
%(2) \textit{Attribute Encoding.} To better represent the source code and fit the code property graphs to GNNs, node or edge attributes have to be encoded. Node attributes are the most widely used attributes in code vulnerability detection. There is a variety of techniques for node attribute encoding such as Word2vec~\cite{Word2Vec}, symbol or type extraction~\cite{vulsniper}. 
%(3) \textit{Model learning.} This application is conducted as a graph classification task. With the code property graphs and node attributes as input, labels of benignity and vulnerability as targets, the GNN-based model is learned from a set of datasets. To make a prediction, the source code is extracted and encoded into a code property graph with node attributes, which then are fed into the trained model and the prediction will be computed.
%
%%\ans{The requirements of code vulnerability detection explanation. }
%% The code vulnerability detection can be applied to the explanation requirements for GNN-based security applications. 
%\subsection{Explanation Requirements}
%
%\textbf{Comprehensive.}
%As stated above, the information contained by different objects in a code property graph is unique and valuable. The nodes in Figure~\ref{fig:comparison}(b) are derived from the statements of source code, the edges represent the relationship between two statements and the attributes are encoded from the keywords from the statements. From Figure~\ref{fig:comparison}(c), the explanation is still confusing without a comprehensive explanation and the incomplete graph structure explanation impacts the fidelity of explanation. It is required to understand which variables, symbols, statements, and how the execution flows contribute to the prediction, in order to fully understand and optimize the framework of code vulnerability detection. A comprehensive explanation provides a more transparent and humanly understandable information for both datasets and GNNs.
%
%\textbf{Faithful.}
%The explainability of GNNs is divided into graph structure explanation and attribute explanation. Graph structure explanation, as the key to GNN explanation, should consist of node explanation and edge explanation. For code vulnerability detection, as a graph classification task, a node without edges in a graph still influences the prediction. A code property graph is usually a directed sparse graph. There is the situation where the vulnerability comes from sets of nodes that are not directly connected. Besides, for connected nodes, we want to know which types and directions of edges causes the vulnerability prediction. From human understanding, the vulnerability in Figure~\ref{fig:comparison} is caused by "double free" but the two highlighted statements are not connected. Further edge explanation is needed to understand why another node is important because it connects both of the other nodes. Thus, both node and edge explanation is needed for faithful graph structure explanation. \\
%Attributes in code vulnerability detection are vectors encoded from the program construct. The attributes of ACDFG in Figure~\ref{fig:comparison}(b) are node attributes encoded from the keywords of each statement. The same node attributes from different nodes contribute differently to the prediction because the vulnerability usually comes from the interaction of statements. The same statements or keywords from different locations of source code should not be considered equally. In order to pinpoint the location and the reason for the prediction, a faithful explanation for attributes is needed.
%
%% \textbf{Model-agnostic Explanation.}
%% The explanation methods should be applied to a wide range of GNNs. In real scenarios, often times, the users are using pre-trained models. There are specialities for different GNNs and not all the GNNs are interpretable or fully accessible. In order to adjust to different models easily, the explanation methods should be able to work on various models and provide model-agnostic explanation.
%
%\subsection{Existing Explanation Methods}
%%\ans{How existing works explain and limitations.}
%Table~\ref{tb:explainers} compares the explanation objects between different explanation methods. 
%GNNExplainer~\cite{GNNEx19}, PGExplainer~\cite{luo2020parameterized} and GraphMask~\cite{schlichtkrull2021interpreting} only target on edges for graph structure explanation, they differentiates the importance for edges by learning the edge masks. They extract the important graph structure only by edge explanation, and the nodes are explained automatically by the connectivity, leading to an unfaithful graph structure explanation. Specifically, PGExplainer and Graphmask do not conduct a model-agnostic explanation, due to the requirement for the access to the hidden layers of GNNs. While PGM-Explainer~\cite{pgmexplainer} conducts node explanation only by a probabilistic graphical model with the generated dataset. Whether a node is perturbed and the prediction change is noted for dataset generation. Then the Grow-Shrink (GS)~\cite{NIPS1999_GS} algorithm is conducted to shrink the datasets and a Bayesian network is used to explain the GNN model. The node explanation from PGM-Explainer is based on attribute perturbation, which neglects the contribution from the connected edges. 
%% GNNExplainer further provides global node attribute explanation for one graph
%Attribute explanation is only provided by GNNExplainer, where the same node attributes from different nodes are explained equally. 
%Therefore, the existing works fail to provide a comprehensive explanation for GNNs. The explanations for graph structure are not faithful enough. 
%Besides, adapting and combining different explanation methods is energy consuming and inconvenient. The combined explanation methods do not guarantee the explanation efficiency.
%
%% We compare explanation methods that do not require preliminary knowledge of GNNs in Figure~\ref{fig:comparison}. The explanation is still confusing without a comprehensive explanation and the incomplete graph structure explanation impacts the explanation accuracy.
%
%% {\td The current flow is ok, just make sure there are no duplicates with introduction.}
%
%%{\td Using the security application, code vulnerability detection, as an example, deeply explain how to do code vulnerability detection, how existing works explained it, what are their limitations (the four aspects).}
%%\ans{I extended more about code vulnerability detection, and the limitations on code vulnerability detection. }
%
%% \begin{itemize}[leftmargin=*,label={}]
%% For a graph in real-world applications, graph features such as nodes, edges and node features represent different element and contain unique information for an object. 
%%To better understand the usefulness of graph features and how GNNs make a prediction, a comprehensive explanation is needed. 
%%\noindent \textbf{Comprehensive explanation. }For a code property graph, a node represents a program construct such as variables, statements and symbols; an edge contains the direction and relationship information for a pair of nodes such as control flow and data flow; node features extract the information for the program construct according to each node. The information contained by different features is unique and valuable. It is required to understand which variables, symbols and statements, how the execution flows contribute to the prediction, in order to fully understand and optimize the model. Thus, the explanation for nodes, edges and node features is necessary. A comprehensive explanation provides a more detailed and humanly understandable information about both graphs and GNNs.
%%%The model itself may not be able to provide the interpretation due to the complexity and closure, which can be determined by explanation methods. 
%%The existing works such as GNNExplainer~\cite{GNNEx19} and PGExplainer~\cite{luo2020parameterized} target on specific graph features to explain. Specifically, they can only provide explanation about how the data or execution flows, which is not enough for researchers to understand how the application works. Besides, adapting and combining different explanation methods is energy consuming and inconvenient. The combined explanation methods do not guarantee the explanation efficiency.
%%
%%\noindent \textbf{Graph structure. }Graph structure explanation, as the key to GNN explanation, should consist of node explanation and edge explanation. The task of a wide range of security applications is graph classification. For code vulnerability detection, as a graph classification task, a node without edges in a graph still influences the prediction, while node explanation only does not consider unimportant edges. A code property graph is usually a directed sparse graph. There is the situation where the vulnerability comes from sets of nodes that are not directly connected. Besides, for connected nodes, we want to know which edge type and direction causes the vulnerability prediction. Thus, both node and edge explanation is needed for explaining graph structure. PGExplainer and GraphMask~\cite{schlichtkrull2021interpreting} only determine which edges can be dropped without influencing the prediction. However, PGM-Explainer~\cite{pgmexplainer} only identifies important nodes without considering edges. These methods fail to provide enough explanation for graph structure extraction as researchers are not able to understand how the code structure contributes to the prediction.
%%
%%\noindent \textbf{Node features. }Node features are the most widely used supplementary information for a graph in security applications. Node features in code vulnerability detection are vectors extracted from the code construct according to each node. The same node features from different nodes contribute differently to the prediction because the vulnerability usually comes from the interaction of statements. The same statements or variables from different locations of source code should not be treated equally. In order to pinpoint the location and the reason for a prediction, a precise explanation for node features is needed. The node feature extraction methods are important components and vary in different code vulnerability detection. The size of a code property graph and the dimension of node feature vectors can be large. Only GNNExplainer provides global node feature explanation for one graph. A general study on node features for all the nodes in a graph does not help provide valuable information for researchers to pinpoint the reasons for vulnerability. Thus, each node feature from each node should be explained individually. 
%%%node features are explained globally for all the nodes in a graph. 
%%
%%\noindent \textbf{GNN detail requirement. }The explanation methods should be applied to a wide range of GNNs. There are specialities for different GNNs and not all the GNNs are interpretable. In order to adjust to different models easily, the explanation methods should be able to work on various models without asking for model details. 
%% \end{itemize}
%
%%\textbf{Motivating example.} The development of code vulnerability detection technique, e.g., Devign~\cite{devign}, is in need of a solid GNN explanation method. The vulnerability detector firstly transfers the source code into code property graphs, and extracts node features for each node, then learns the graph features by a GNN based model, so as to make predictions for other source code. This application applies to the explanation requirement. 
%%For a code property graph, a node represents a program construct such as variables, statements and symbols; an edge contains the direction and relationship information for a pair of nodes such as control flow and data flow; node features extract the information for each node according to the program construct the node represents. The size of a graph and the dimension of node features can be large. This application conducts graph classification tasks. Thus, the explanation is supposed to be comprehensive and precise. 
%
%%In some applications such as NLP and code vulnerability detection, GNNs are evolved and adapted with graph generation technique. The graph generation technique can be studied and optimized if the models and the use of graphs are interpreted by researchers. Since the graphs do not naturally exist in real world, the problems behind the graph datasets are in shadow. In order to make full use of the models, graph datasets with both small size and useful information are preferred. In order to understand both the models and datasets, a comprehensive explanation for graph features are needed, while current works mainly focus on a specific graph feature, e.g., edges. Besides, it is inefficient and painstaking to adapt and combine different explainers together.
%%Node explanation is needed since a node represents an entity in real life. A wide range of applications are derived from graph classification tasks. The explanation for graph structure does not rely on edges or nodes only. A node without connections still contributes to the prediction. A node contains its connected edges and node features. There is no existing work considering node explanation both from its edges and node features for graph classification tasks. 
%%To better understand graphs and GNNs, node features should be individually explained for each node. Only GNNExplainer~\cite{GNNEx19} provides node feature explanation but the explanation is for all the nodes from the same graph. The existing explainers for neural networks can only be applied to conduct node feature explanation.
%%Black-box explainers can be utilized and adjusted to wider range of applications. Different applications use different techniques. Also, not all the application models are accessible.
%
%%\ans{1. the problems of existing works are written in introduction. Similar paragraph was written in introduction but was commented. I let it out and added this paragraph into introduction. 2. The usage of a graph is not only the structure and GNNs not only learn the structure. A comprehensive explanation helps researchers or users to understand a model and input graph better. For code property graph especially, graph is generated by users. The code itself is not a graph. A comprehensive study helps users research in graph generation. For graph classification models, node+edge together can explain graph structure. Here, I added some of my answers into introduction.}
%
%% In this paper, we target on source code related security applications. As code property graph started being used to represent source code, the application of GNNs became a novel field to explore. There are a lot of models derived from GNNs for different security applications, such as variable misuse prediction task (\textsc{VarMisuse})~ \cite{allamanis2018learning}, code vulnerability detection~\cite{devign} and code clone detection~\cite{fa-ast}. Here, we focus on two classes of security applications: smart contract vulnerability detection and code vulnerability detection.
%
%
%In this paper, we evaluate the performance of explanation methods in real-world applications. Specifically, we focus on two classes of security applications: smart contract vulnerability detection and code vulnerability detection.
%%{\td we should not limit ourself to only security applicaitons.}
%
%% \textbf{Smart Contract Vulnerability Detection. }Smart contracts are programs running on the blockchain when certain conditions are satisfied, which simplifies the transaction without a middleman. Smart contracts without strong design are vulnerable to network attacks. In the work of~\cite{ijcai2020-454}, they construct a contract graph to represent a smart contract function. Then they propose a degree-free graph convolutional neural network (DR-GCN) and a novel temporal message propagation network (TMP) to detect the vulnerability from the contract graph. 
%
%% \textbf{Code Vulnerability Detection. }Code vulnerability, compiled into software, can cause damages in a wide range. The vulnerability we study is from successfully compiled code, which would run into errors under some circumstances. For example, the use of uninitialized variables. In the study of~\cite{devign}, a graph is built from a C code function with four types of subgraphs (Abstract syntax tree, control flow graph, data flow graph and natural code sequence). A gated graph recurrent network~\cite{ggnn} based model with a novel $Conv$ module --- Devign, is proposed to detect code vulnerability from the generated graph.
