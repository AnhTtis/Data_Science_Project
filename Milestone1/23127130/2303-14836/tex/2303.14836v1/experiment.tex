\section{Experiment}
\label{sec_experiment}

%\todo{(1) PGM-Explainer, (2) unify node/node rate, (3) single instance explanation, (4) analysis of edge/node feature reasoning}

% \todo{setup, colonial0}
The experiments are conducted on a server with two Intel Xeon E5-2683 v3 (2.00GHz) CPUs, each of which has 14 cores and 28 threads. The code in this work is available for reproduction\footnote{\url{https://github.com/iHeartGraph/Illuminati}}.

\subsection{Dataset and Pre-traind GNN Models}

\begin{table}[t]
\caption{EP (\%) of explained subgraphs for public datasets, where BBBP and Mutagenicity are real-world molecular datasets and BA-2motifs is a synthetic dataset.}
\label{tb:general_result}
\centering
\tabcolsep = 0.3cm
\begin{tabular}{lrrr}
\toprule
\textbf{Methods} & \textbf{BBBP} & \textbf{Mutagenicity} & \textbf{BA-2motifs}\\ 
\midrule
PGM-Explainer & 74.6 & 57.2 & 41.0 \\
GNNExplainer & 75.1 & 69.9 & 41.0 \\ 
PGExplainer & 76.2 & 68.2 & 41.0 \\
\textbf{{\name}} & \textbf{76.7} & \textbf{72.0} & 41.0 \\
\bottomrule
\end{tabular}
\end{table}

%The dataset details and model performances are provided in Table~\ref{tb:setup}.

% \todo{rewrite, summarize the dataset in one paragraph.}

We evaluate eight datasets as shown in Table~\ref{tb:setup}. 
We test the explanation methods on three public datasets used for the graph classification task, including two real-world datasets and a synthetic dataset. Two molecular datasets Mutagenicity~\cite{mutag} and BBBP~\cite{Ramsundar-et-al-2019} contain graphs with nodes representing the atoms, and edges representing the chemical bonds. BA-2motifs~\cite{luo2020parameterized} is a motif-based synthetic dataset, each graph from which contains a five-node house-like motif or a cycle motif.
For code vulnerability detection, we use a well-labeled dataset from NIST Software Assurance Reference Dataset (SARD), named Juliet~\cite{nistsard}, which not only labels the vulnerable functions but also provides the benign functions. For a clear explanation study, we require the datasets easy to understand and achieve good prediction accuracy. The CWEs we select for the experiment are 415, 416, and 476 which represent ``double free'', ``use after free'' and ``NULL pointer dereference'' respectively. The source code is represented by AFGs.
The datasets for smart contract vulnerability detection are from two platforms, Ethereum Smart Contracts (Reentrancy) and VNT chain Smart Contracts (Infinite loop). The contract graphs are constructed from the source code from the work of Zhuang \textit{et al.}~\cite{ijcai2020-454}.
The graphs for two cybersecurity applications will be illustrated in Section~\ref{sec_case_study}.
% We use real-world datasets for evaluation. For better explanation evaluation, we use a well labelled dataset from NIST Software Assurance Reference Dataset (SARD), named Juliet~\cite{nistsard}, which not only labels the vulnerable functions, but also provides the benign functions. The CWEs we select for the experiment are 415, 416, and 476 which represent \textsl{double free}, \textsl{use after free} and \textsl{NULL pointer dereference} respectively. 
% The PyTorch~\cite{pytorch} source code provides DR-GCN~\cite{ijcai2020-454} as the vulnerability detector. We use the source code as they provided. The datasets are from two platforms, Ethereum Smart Contracts (Reentrancy) and VNT chain Smart Contracts (Infinite loop).

% \todo{one or two paragraphs for the pre-trained models.}

We use three kinds of GNN models for different applications respectively. The models include two parts, i.e., GNN layers to generate node representations and functional layers to compute graph representations.
The dataset splits for model training and the testing accuracies are shown in Table~\ref{tb:setup}. The pre-trained models are used as pre-trained models for explanation evaluation. 

We train a basic 3-layer GCN~\cite{kipf2017semi} for public datasets. For a graph classification task, it is followed by a $max$ and $mean$ pooling layer and a fully connected layer. 
The model in Devign~\cite{devign} is used for code vulnerability detection, which consists of a 3-layer gated graph recurrent network~\cite{ggnn} with a \textit{Conv} module.
DR-GCN~\cite{ijcai2020-454} for smart contract vulnerability detection is derived from GCN with increased connectivity in each layer. A \textit{max} pooling layer and two fully connected layers are applied for graph representation after the 3-layer DR-GCN.


% For better evaluation analysis, we use Joern~\cite{joern} to extract control and data flow graphs from C/C++ code. We make sure each graph contains 32 nodes. We extract the keywords from code, e.g., \texttt{char}, \texttt{==} \texttt{*} as binary node attributes, meaning whether the statement contains such keywords. There are 96 node attributes in total.

%\textbf{Code vulnerability detection. }% With inspiration of VulSniper~\cite{vulsniper}, we argue that using GNNs should obtain better results since Vulsniper does not provide "message passing" between nodes. We compare Devign~\cite{devign} with VulSniper. 
%
%We summarize three steps for GNN-based code vulnerability detection.
%(1) Graph extraction.
%We use ACDFGs to represent the source code. Therefore, a node denotes a statement, an edge contains the direction and relationship information (control flow and data flow) for a pair of nodes.
%We use Joern~\cite{joern} to extract CDFGs from C/C++ code. We make sure each graph contains 32 nodes. 
%(2) Attribute encoding.
%In this application, we extract the keywords from each statement for node attributes. A node attribute indicates whether the statement has the corresponding keyword, e.g., \texttt{char}, \texttt{==} \texttt{*}, so it is encoded to be binary. There are 96 node attributes for each node.
%(3) Model learning.
%We use the GNN-based model Devign as the code vulnerability detector, which conducts a graph classification task. The labels of ACDFGs indicate whether the source code is vulnerable or benign.
%
%\textbf{Smart contract vulnerability detection.} 
%In this application, a GNN model is trained on the contract graphs that are generated from the smart contract source code. The code is labelled as benign or vulnerable. The contract graphs are attributed graphs representing smart contract source code. 
%From the work of \cite{ijcai2020-454}, the nodes in a contract graph are categorized into major nodes, secondary nodes and fallback nodes. The major nodes represent important functions, the secondary nodes represent model critical variables and the fallback nodes simulate the fallback function. The edges indicate the relationship between nodes, where the edge attributes are only used for graph construction, not for GNN training. The node attributes are derived from the types of functions, operations of variable, etc.


\subsection{Compared Works}

We compare {\name} with the following baseline GNN explanation methods, GNNExplainer~\cite{GNNEx19}, PGM-Explainer~\cite{pgmexplainer}, and PGExplainer~\cite{luo2020parameterized}. Here, GNNExplainer and PGM-Explainer do not require prior knowledge from GNNs.
GNNExplainer targets on edges for graph structure explanation. The importance of edges is differentiated by learning the edge masks. The important nodes are automatically extracted with the explained important edges. Attribute explanation is also provided by GNNExplainer. The same node attributes from different nodes are explained equally by learning the same attribute masks. 
PGM-Explainer~\cite{pgmexplainer} provides node explanation by a probabilistic graphical model with the generated dataset. Whether a node is perturbed and the prediction change is noted for dataset generation. Then the Grow-Shrink (GS)~\cite{NIPS1999_GS} algorithm is conducted to shrink the datasets and a Bayesian network is used to explain the GNN model.
PGExplainer takes the node embeddings from the last layer of GNNs as input, then learns the edge masks from a multi-layer neural network. Similar to GNNExplainer, the explanation of graph structure is only determined by explained edges.

%We build the explanation methods on the source code of different application models.
%Following the original implementations of baseline explanation methods, we implement them with PyTorch~\cite{pytorch} and Pytorch Geometric~\cite{pyg}, respectively to adjust to the implementations of the security applications. 
We used the shared source code of the two compared works and reimplement the interfaces to support the dataset and pre-trained GNN models.
%In order to ensure a fair evaluation, 
We compare different methods for graph structure explanation. 
Specifically, the subgraph is extracted only by node, and all the connected edges are retained. For GNNExplainer and PGExplainer, as we identify the top-$R$ (rate) or top-$K$ nodes, edges that are originally connected from the input graph are restored. Thus, only node removal is conducted and the number of remaining nodes is controlled to be equal for all the explanation methods. Also, we do not apply any additional constraints for the evaluation. We use $max$ pooling as $Agg^{(2)}$ for {\name}. 


% GNNExplainer~\cite{GNNEx19}, PGExplainer~\cite{luo2020parameterized} and GraphMask~\cite{schlichtkrull2021interpreting} only target on edges for graph structure explanation, they differentiate the importance for edges by learning the edge masks. They extract the important graph structure only by edge explanation, and the nodes are explained automatically by the connectivity, leading to an unfaithful graph structure explanation. % Specifically, PGExplainer and Graphmask do not conduct a model-agnostic explanation, due to the requirement for the access to the hidden layers of GNNs. While PGM-Explainer~\cite{pgmexplainer} conducts node explanation only by a probabilistic graphical model with the generated dataset. Whether a node is perturbed and the prediction change is noted for dataset generation. Then the Grow-Shrink (GS)~\cite{NIPS1999_GS} algorithm is conducted to shrink the datasets and a Bayesian network is used to explain the GNN model. The node explanation from PGM-Explainer is based on attribute perturbation, which neglects the contribution from the connected edges. 
% GNNExplainer further provides global node attribute explanation for one graph
% Attribute explanation is only provided by GNNExplainer, where the same node attributes from different nodes are explained equally. 
% Therefore, the existing works fail to provide a comprehensive explanation for GNNs. The explanations for graph structure are not faithful enough. 
% Besides, adapting and combining different explanation methods is energy consuming and inconvenient. The combined explanation methods do not guarantee the explanation efficiency.

\begin{table*}[t]
\caption{EP (\%) of explained subgraphs. $R=0.5$ for smart contract vulnerability detection; $K=6$ for code vulnerability detection.}%{\td have you tested other rates? We need a figure of rates from 0.1 to 1 with 0.1 as step.}
%{\td To me, this looks weird to embed figures in a Table. I would rather split them.}}
\label{tb:ep+}
\centering
\tabcolsep = 0.5cm
\begin{tabular}{lrrrrr}
\toprule
\textbf{Methods} & \textbf{Reentrancy} & \textbf{Infinite loop} & \textbf{CWE-415} & \textbf{CWE-416} & \textbf{CWE-476} \\ 
\midrule
PGM-Explainer &
61.3 & 58.6 & 79.6 & 74.3 & 72.2 \\
GNNExplainer & 
81.3 & 72.0 & 81.7 & 74.9 & 85.0 \\
PGExplainer &
84.9 & 73.6 & \textbf{90.1} & 77.2 & 92.2 \\
\textbf{{\name}} & 
\textbf{93.4} & \textbf{78.2} & 88.0 & \textbf{80.8} & \textbf{97.3} \\
\bottomrule
\end{tabular}
\end{table*}

\begin{figure*}[!t]
\centering
\begin{subfigure}{.5\linewidth}
\centering
\includegraphics[width=\linewidth]{./picture/legend.pdf}%
\end{subfigure}\\%
\begin{subfigure}{.2\linewidth}
\centering
\includegraphics[width=\linewidth]{./picture/ree_acc.pdf}
\end{subfigure}%
%\hspace{0.35cm}
\begin{subfigure}{.2\linewidth}
\centering
\includegraphics[width=\linewidth]{./picture/loop_acc.pdf}%
\end{subfigure}%
\begin{subfigure}{.2\linewidth}
\centering
\includegraphics[width=\linewidth]{./picture/cwe415_acc.pdf}
\end{subfigure}%
%\hspace{0.35cm}
\begin{subfigure}{.2\linewidth}
\centering
\includegraphics[width=\linewidth]{./picture/cwe416_acc.pdf}
\end{subfigure}%
%\hspace{0.35cm}
\begin{subfigure}{.2\linewidth}
\centering
\includegraphics[width=\linewidth]{./picture/cwe476_acc.pdf}
\end{subfigure}%
\caption{Explanation results for cybersecurity applications. We obtain EP of explained subgraphs by changing explained subgraph size.}
\label{fig:ep+}
\end{figure*}


\subsection{Performance Comparison}


%{\td I still don't quite understand what is qualitative and quantitative analysis. Do you mind of explaining them in one sentence (you can fill the following sentence and write a similar one in the following section.)}
%\ans{These are derived from other explainers. Quantitative analysis is to use numbers to evaluate different explainers, e.g., accuracy. Qualitative analysis is to study single instances, motifs...in this paper, it is case study. We study the explanations with human understandings. So case study is written in qualitative analysis. Similar sentence is added in the next subsection.}

In this subsection, we present the quantitative analysis of explanation methods with various evaluation metrics. 

%\textbf{Evaluation metrics.}
%Existing works~\cite{GNNEx19, luo2020parameterized} use AUC as evaluation metric, however, this applies to synthetic dataset and does not provide precise evaluation. It assumes the model achieves high prediction accuracy with capturing the right and clear ground truth. And the importance scores for the datasets should be consistent. For real-world datasets, there may not be clear ground truth and the performance of GNNs is not perfect. Each set of importance scores is calculated for each graph, so they can only be compared within each graph. 

%In this work, we assume the important subgraphs will retain the original predictions and probabilities, meaning causing the least prediction change and probability decrease from the original graph. Following the work by Yuan \textit{et al.}~\cite{yuan2021explainability}, we apply $Fidelity$ as our major evaluation metrics. We rewrite the equations as:
%
%\begin{align}
%	&\begin{aligned}
%	Fidelity^{acc}&=\frac{1}{N}\sum_{i}^{N}
%	(1-\mathbbm{1}[y_{s}^{(i)}=y^{(i)}]) \label{eq:fid_acc_}
%	\end{aligned}\\
%	&\begin{aligned}
%	&Fidelity^{prob}=\\
%	&\qquad \frac{1}{N}\sum_{i}^{N}
%	(P_{\Phi}(Y=y^{(i)}\mid G=G^{(i)}, \mathcal{X}=\mathcal{X}^{(i)})\\
%	&\qquad -
%	P_{\Phi}(Y=y^{(i)}\mid G=G_{s}^{(i)}, \mathcal{X}=\mathcal{X}_{s}^{(i)}))
%	\label{eq:fid_prob_} 
%	\end{aligned} 
%\end{align} 
%
%where $N$ is the number of graphs in a dataset. 
%Among $Fidelity$, we focus on $Fidelity-$ as the major evaluation metric, where $G_{s} = G_{\omega}$ meaning the subgraph is derived from factors with high importance scores. If an unimportant node is removed, the new prediction should not drop largely, and possibly should be higher. Based on the observation, $Fidelity-$ evaluates how accurate the extracted factors are to the prediction by directly comparing the prediction accuracy and probability. Similar metrics are widely used in different explanation methods for different deep learning models. The metrics are estimated by the negative prediction change from removing unimportant factors. Lower values for $Fidelity-$ indicate less negative change i.e. better explanation performance.
%$Fidelity+$ is also utilized to the performance of explanation methods and analyse the GNN prediction from the irrelevant factors. For $Fidelity+$, $G_{s} = G_{\omega-}$, meaning the subgraph is derived from factors without high importance scores.

\textbf{Evaluation metrics.}
In this work, we assume the important subgraphs will retain the original predictions, meaning causing the least prediction change from the original graphs. We define \textit{Essentialness Percentage (EP)} as our evaluation metric:
\begin{equation}
	\text{EP} = \frac{1}{N}\sum_{i}^{N}(\mathbbm{1}[y_{s}^{(i)}=y^{(i)}])
\end{equation}
where $\mathbbm{1}[\cdot]$ means the result being 1 if the statement in $[\cdot]$ is true, otherwise 0; $y_{s}$ denotes the prediction label of the subgraph, and $N$ is the number of graphs in the dataset. EP, as the percentage of subgraphs that retain the original predictions, evaluates how accurate the extracted factors are to the prediction. 
To validate the accuracy of the explained factors, we design two tests. Based on the objective of explanation, we firstly evaluate EP from the subgraphs formed by the important factors. We also consider the intuition reasonable that if the important factors are removed, the remaining subgraphs will not likely be able to retain the original predictions, which will cause lower EP. Thus, we divide the graphs into the explained subgraphs and the remaining subgraphs after explanation, where the explained subgraphs are constituted by important factors. 

An accurate explanation should be able to identify the most important factors, thus the explanation should be sparse. However, explanation methods provide continuous importance scores for different factors rather than solid binary scores. In order to evaluate the sparsity for different explanation methods, we define $Sparsity$ as follows:
\begin{equation}
	Sparsity = \frac{1}{N}\sum_{i}^{N} \text{min} \mid \mathcal{V}_{s}^{(i)} \mid
	\; \text{s.t.}\; y_{s}^{(i)}=y^{(i)}
\end{equation}
Sparsity represents the average minimum size of subgraphs that retain the original GNN predictions from a dataset. The smaller sparsity means the explanation method identifies more important factors and ignores irrelevant factors, thus provides more accurate explanations. 


%\begin{figure}[!t]
%\begin{subfigure}{\linewidth}
%\centering
%\includegraphics[width=.5\linewidth]{../picture/ree_acc.pdf}%
%\includegraphics[width=.5\linewidth]{../picture/ree_prob.pdf}
%\caption{$Fidelity-^{acc}$ and $Fidelity-^{prob}$ for Reentrancy dataset.}
%\end{subfigure}
%\begin{subfigure}{\linewidth}
%\centering
%\includegraphics[width=.5\linewidth]{../picture/loop_acc.pdf}%
%\includegraphics[width=.5\linewidth]{../picture/loop_prob.pdf}
%\caption{$Fidelity-^{acc}$ and $Fidelity-^{prob}$ for Infinite loop dataset.}
%\end{subfigure}
%\includegraphics[width=\linewidth]{../picture/legend.pdf}%
%\caption{The $Fidelity-$ for smart contract vulnerability detection. We change the rate of extracted nodes for subgraphs.}
%\label{fig:sc_result}
%\end{figure}
%
%\begin{figure}[!t]
%\begin{subfigure}{\linewidth}
%\centering
%\includegraphics[width=.5\linewidth]{../picture/cwe415_acc.pdf}%
%\includegraphics[width=.5\linewidth]{../picture/cwe415_prob.pdf}
%\caption{$Fidelity-^{acc}$ and $Fidelity-^{prob}$ for CWE-415 dataset.}
%\end{subfigure}
%\begin{subfigure}{\linewidth}
%\centering
%\includegraphics[width=.5\linewidth]{../picture/cwe416_acc.pdf}%
%\includegraphics[width=.5\linewidth]{../picture/cwe416_prob.pdf}
%\caption{$Fidelity-^{acc}$ and $Fidelity-^{prob}$ for CWE-416 dataset.}
%\end{subfigure}
%\begin{subfigure}{\linewidth}
%\centering
%\includegraphics[width=.5\linewidth]{../picture/cwe476_acc.pdf}%
%\includegraphics[width=.5\linewidth]{../picture/cwe476_prob.pdf}
%\caption{$Fidelity-^{acc}$ and $Fidelity-^{prob}$ for CWE-476 dataset.}
%\end{subfigure}
%\includegraphics[width=\linewidth]{../picture/legend.pdf}%
%\caption{The $Fidelity-$ for code vulnerability detection. We change the number of extracted nodes for subgraphs.}
%\label{fig:cv_result}
%\end{figure}

% \textbf{Fidelity analysis.}
\textbf{EP of explained subgraphs.}
We use the testing splits from Table~\ref{tb:general_result} for explanation method evaluation. All the explanation methods explain the graph by generating the importance scores for different factors. It is unclear if a factor should be kept. Thus, we evaluate the performance of the explanation by comparing the EP under the same graph size.
First, we test the explanation methods with public datasets and a trained basic 3-layer GCN, shown in Table~\ref{tb:general_result}. We extract the top-10 nodes for Mutagenicity and BBBP, and the top-5 for synthetic dataset BA-2motifs. The result suggests that PGExplainer, as an explanation method requiring prior knowledge, outperforms other compared methods without prior knowledge. Overall, the explanation result shows that {\name} achieves the best EP in real-world datasets and outperforms other explanation methods. 
%that do not require prior knowledge for all the datasets. % For graphs of the monotonous size, with the same node attributes, PGExplainer is able to provide a better explanation from a global view with knowing the prior knowledge.

The explanation results for two cybersecurity applications are shown in Figure~\ref{fig:ep+}. Table~\ref{tb:ep+} summarizes the result values in the middle from Figure~\ref{fig:ep+}. As for smart contract detection, we variate the rate of extracted nodes; and we change the number of extracted nodes in code vulnerability detection. If the graph size to be explained is larger than the input graph size, then this graph is not considered for evaluation.

%The goal of explanation methods is to minimize $Fidelity-^{prob}$. Low $Fidelity-^{acc}$ denotes the stability of the method. 
In general, {\name} shows the highest EP among other explanation methods in both applications, meaning it identifies the important subgraphs more accurately. For real-world datasets, PGM-Explainer does not perform as well as public datasets and synthetic datasets. The real-world datasets contain a more arbitrary and larger size of node attributes. PGExplainer outperforms other explanation methods in CWE-415, while the performance of {\name} is close to PGExplainer. 
% We permit $1.5\times$ running time for PGM-Explainer to execute in code vulnerability detection. The result will be improved with more random sampling, which will lead to more time consumption. 
To acquire better explanation accuracy, PGM-Explainer should be executed as the size of subgraphs changes; while GNNExplainer and {\name} only need to be executed once. As an explanation method that requires prior knowledge of GNNs, the performance of PGExplainer is generally better than the peer explanation methods without prior knowledge. However, without exploring nodes in depth, PGExplainer generally does not gain a higher EP than {\name}.
The result also suggests that as the size of explained subgraphs increases, the explanation is more accurate. We use real-world datasets, which ensure a node should not have an extremely high or low contribution. The predictions rely on the interactions between different nodes. 

\begin{table*}[t]
\caption{EP (\%) of remaining subgraphs for cybersecurity applications. $R=0.5$ for smart contract vulnerability detection; $K=6$ for code vulnerability detection.}
\label{tb:ep-}
\centering
\tabcolsep = 0.5cm
\begin{tabular}{lrrrrr}
\toprule
\textbf{Methods} & \textbf{Reentrancy} & \textbf{Infinite loop} & \textbf{CWE-415} & \textbf{CWE-416} & \textbf{CWE-476} \\ 
\midrule
PGM-Explainer &
76.1 & 70.1 & 86.5 & 85.6 & 85.3 \\
GNNExplainer & 
63.1 & 56.7 & 83.2 & 79.6 & 72.8 \\
PGExplainer &
72.2 & 59.8 & \textbf{72.2} & \textbf{59.9} & 59.6 \\
\textbf{{\name}} & 
\textbf{51.7} & \textbf{58.2} & \textbf{72.2} & 62.0 & \textbf{49.4} \\
\bottomrule
\end{tabular}
\end{table*}

\begin{figure*}[!t]
\centering
\begin{subfigure}{.5\linewidth}
\centering
\includegraphics[width=\linewidth]{./picture/legend.pdf}%
\end{subfigure}\\%
\begin{subfigure}{.2\linewidth}
\centering
\includegraphics[width=\linewidth]{./picture/ree_acc+.pdf}
\end{subfigure}%
%\hspace{0.35cm}
\begin{subfigure}{.2\linewidth}
\centering
\includegraphics[width=\linewidth]{./picture/loop_acc+.pdf}%
\end{subfigure}%
\begin{subfigure}{.2\linewidth}
\centering
\includegraphics[width=\linewidth]{./picture/cwe415_acc+.pdf}
\end{subfigure}%
%\hspace{0.35cm}
\begin{subfigure}{.2\linewidth}
\centering
\includegraphics[width=\linewidth]{./picture/cwe416_acc+.pdf}
\end{subfigure}%
%\hspace{0.35cm}
\begin{subfigure}{.2\linewidth}
\centering
\includegraphics[width=\linewidth]{./picture/cwe476_acc+.pdf}
\end{subfigure}%
\caption{The explanation results for cybersecurity applications. We obtain EP of the remaining subgraphs previously generated. The graph sizes here are for the explained subgraphs.}
\label{fig:ep-}
\end{figure*}

%\begin{figure}[!t]
%\begin{subfigure}{\linewidth}
%\centering
%\includegraphics[width=.5\linewidth]{../picture/ree_acc+.pdf}%
%\includegraphics[width=.5\linewidth]{../picture/ree_prob+.pdf}
%\caption{$Fidelity+^{acc}$ and $Fidelity+^{prob}$ for Reentrancy dataset.}
%\end{subfigure}
%\begin{subfigure}{\linewidth}
%\centering
%\includegraphics[width=.5\linewidth]{../picture/loop_acc+.pdf}%
%\includegraphics[width=.5\linewidth]{../picture/loop_prob+.pdf}
%\caption{$Fidelity+^{acc}$ and $Fidelity+^{prob}$ for Infinite loop dataset.}
%\end{subfigure}
%\includegraphics[width=\linewidth]{../picture/legend.pdf}%
%\caption{The $Fidelity+$ for smart contract vulnerability detection. We change the rate of extracted nodes for subgraphs.}
%\label{fig:sc_fid+}
%\end{figure}
%
%\begin{figure}[!t]
%\begin{subfigure}{\linewidth}
%\centering
%\includegraphics[width=.5\linewidth]{../picture/cwe415_acc+.pdf}%
%\includegraphics[width=.5\linewidth]{../picture/cwe415_prob+.pdf}
%\caption{$Fidelity+^{acc}$ and $Fidelity+^{prob}$ for CWE-415 dataset.}
%\end{subfigure}
%\begin{subfigure}{\linewidth}
%\centering
%\includegraphics[width=.5\linewidth]{../picture/cwe416_acc+.pdf}%
%\includegraphics[width=.5\linewidth]{../picture/cwe416_prob+.pdf}
%\caption{$Fidelity+^{acc}$ and $Fidelity+^{prob}$ for CWE-416 dataset.}
%\end{subfigure}
%\begin{subfigure}{\linewidth}
%\centering
%\includegraphics[width=.5\linewidth]{../picture/cwe476_acc+.pdf}%
%\includegraphics[width=.5\linewidth]{../picture/cwe476_prob+.pdf}
%\caption{$Fidelity+^{acc}$ and $Fidelity+^{prob}$ for CWE-476 dataset.}
%\end{subfigure}
%\includegraphics[width=\linewidth]{../picture/legend.pdf}%
%\caption{The $Fidelity+$ for code vulnerability detection. We change the number of extracted nodes for subgraphs.}
%\label{fig:cv_fid+}
%\end{figure}
%
%\begin{table*}[t]
%\caption{Explanation results of $Fidelity+$. $R=0.5$ for smart contract vulnerability detection; $K=6$ for code vulnerability detection.}
%\label{tb:result_fid+}
%\centering
%\tabcolsep = 0.3cm
%\begin{tabular}{lrrrrrrrrrr}
%\toprule
%\multirow{2}{*}{\textbf{Methods}} & \multicolumn{2}{r}{\textbf{Reentrancy}} & \multicolumn{2}{r}{\textbf{Infinite loop}} & \multicolumn{2}{r}{\textbf{CWE-415}} & \multicolumn{2}{r}{\textbf{CWE-416}} & \multicolumn{2}{r}{\textbf{CWE-476}} \\ 
%\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
%& acc & prob & acc & prob & acc & prob & acc & prob & acc & prob \\
%\midrule
%PGM-Explainer &
%0.239 & 0.269 & 0.299 & 0.127 & 0.135 & 0.172 & 0.144 & 0.097 & 0.147 & 0.227 \\
%GNNExplainer & 
%0.369 & 0.329 & \textbf{0.433} & 0.154 & 0.168 & 0.189 & 0.204 & 0.159 & 0.272 & 0.321 \\
%PGExplainer &
%0.278 & 0.328 & 0.402 & 0.157 & 0.278 & 0.331 & \textbf{0.401} & \textbf{0.374} & 0.404 & 0.415 \\
%\textbf{{\name}} & 
%\textbf{0.483} & \textbf{0.462} & 0.418 & \textbf{0.164} & \textbf{0.278} & \textbf{0.295} & 0.380 & 0.331 & \textbf{0.506} & \textbf{0.450} \\
%\bottomrule
%\end{tabular}
%\end{table*}

\begin{table*}[t]
\caption{Minimum graph size to retain the original GNN predictions ($Sparsity$).}
\label{tb:result_sparsity}
\centering
\tabcolsep = 0.5cm
\begin{tabular}{lrrrrr}
\toprule
\textbf{Methods} & \textbf{Reentrancy} & \textbf{Infinite loop} & \textbf{CWE-415} & \textbf{CWE-416} & \textbf{CWE-476} \\ 
\midrule
PGM-Explainer & 3.047 & 2.695 & 12.043 & 16.628 & 3.192 \\
GNNExplainer & 2.184 & 2.305 & 9.304 & 14.802 & 2.768 \\
PGExplainer & 2.118 & 2.290 & \textbf{5.928} & 9.407 & 2.838 \\
\textbf{{\name}} & \textbf{2.000} & \textbf{2.015} & 6.406 & \textbf{8.267} & \textbf{1.404} \\
\midrule
Average graph size & 4.939 & 3.695 & 13.029 & 20.047 & 9.838 \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[t]
\caption{Time complexity (seconds).}
\label{tb:result_time}
\centering
\tabcolsep = 0.5cm
\begin{tabular}{lrrrrr}
\toprule
\textbf{Methods} & \textbf{Reentrancy} & \textbf{Infinite loop} & \textbf{CWE-415} & \textbf{CWE-416} & \textbf{CWE-476} \\ 
\midrule
PGM-Explainer & 93.3 & 62.7 & 292.4 & 367.5 & 269.3 \\
GNNExplainer & 37.8 & 35.6 & 92.9 & 94.2 & 91.8 \\
PGExplainer(training) & 0.8(68.3) & 0.6(52.8) & 2.4(83.5) & 3.2(118.8) & 3.0(100.9) \\
\textbf{{\name}} & 52.5 & 37.6 & 99.3 & 103.4 & 98.7 \\
\bottomrule
\end{tabular}
\end{table*}

\textbf{EP of remaining subgraphs.}
Furthermore, we study the EP of remaining subgraphs, which is computed from the rest of top-$R$ or top-$K$ nodes. Therefore, the lower EP represents higher irrelevance of remaining subgraphs. EP is based on the intuition where the remaining input is irrelevant to the prediction since the important factors are identified and removed. We use the same values of top-$R$ and top-$K$ from the evaluation in Figure~\ref{fig:ep+}. 
We show the results for the two applications in Figure~\ref{fig:ep-}, with Table~\ref{tb:ep-} showing the results in the middle from Figure~\ref{fig:ep-}. The EP for {\name} and PGExplainer is overall lower than other explanation methods, meaning the remaining subgraphs are less related to the GNN predictions.
As it is observed in the pair of Figure~\ref{fig:ep+} and Figure~\ref{fig:ep-}, the increase of EP of explained subgraphs does not directly relate to the decrease of EP of remaining subgraphs. 

Our objective is to identify the important subgraphs that retain the original predictions, while the interaction of the remaining nodes can contribute to the prediction as well. GNNs are complex and non-linear models. The important subgraphs are not assembled by all the important nodes individually, but the important node interactions. The remaining subgraphs may contain positive node interactions and important nodes, which are weaker than the explained subgraphs. Thus, the objectives of obtaining the maximum EP of explained subgraphs and the minimum EP of remaining subgraphs are better considered separately, especially for complex models like GNNs.
It is proved that GNNs can be attacked easily by correctly identifying important nodes. The domains of attack and explanation share common techniques, e.g., counterfactual explanation. With the explanation method, the attack can be conducted by removing important nodes or identifying important nodes for an incorrect prediction.

\textbf{Sparsity.}
By default, GNNs are able to make a certain prediction from an empty graph. The default prediction for smart contract vulnerability detection is vulnerable, while the code vulnerability detection is benign. To better differentiate the performance for each explanation method, the $Sparsity$ is only evaluated from graphs with the opposite default predictions. 
We collect the $Sparsity$ from different explanation methods in Table~\ref{tb:result_sparsity}. Overall, {\name} achieves the smallest $Sparsity$, which is consistent with the result of EP of explained subgraphs. For graphs with bigger sizes from code vulnerability detection, it is only fewer than half of the nodes that lead to the final predictions. It indicates the vulnerability does not take a big part of the code, based on the assumption that GNNs make the prediction by correctly capturing the vulnerability factors. From CWE-476, the GNN identifies the significant difference between benign and vulnerable code since it is able to determine the vulnerability averagely within two nodes. The way GNN makes predictions for this dataset is mainly to find the benign factors rather than vulnerable factors. It also implies that the dataset may not be strong or complete enough to cover all the possible coding situations, as GNN only needs to capture the difference between the graphs with different labels.

\begin{table*}[t]
\caption{EP (\%) of explained subgraphs for attribute explanation study. We pick top-$3$ node attributes for smart contract vulnerability detection; top-$5$ for code vulnerability detection.}
\label{tb:result_feat}
\centering
\tabcolsep = 0.5cm
\begin{tabular}{lrrrrr}
\toprule
\textbf{Methods} & \textbf{Reentrancy} & \textbf{Infinite loop} & \textbf{CWE-415} & \textbf{CWE-416} & \textbf{CWE-476} \\ 
\midrule
GNNExplainer & 
74.3 & 64.0 & \textbf{94.3} & \textbf{87.4} & 88.9 \\
\textbf{{\name}} & 
\textbf{92.7} & \textbf{71.6} & \textbf{94.3} & 85.3 & \textbf{98.5} \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[t]
\caption{EP (\%) of explained subgraphs for ablation study. $R=0.5$ for smart contract vulnerability detection; $K=6$ for code vulnerability detection.}
\label{tb:ablation}
\centering
\tabcolsep = 0.5cm
\begin{tabular}{lrrrrr}
\toprule
\textbf{Methods} & \textbf{Reentrancy} & \textbf{Infinite loop} & \textbf{CWE-415} & \textbf{CWE-416} & \textbf{CWE-476} \\ 
\midrule
Edge only & 
83.4 & 72.8 & 82.9 & 74.9 & 80.2 \\
Attribute only & 
67.4 & 72.0 & 83.5 & 81.4 & 95.5 \\
\bottomrule
\end{tabular}
\end{table*}

\textbf{Time complexity.}
Table~\ref{tb:result_time} shows the execution time for every explanation method. We use the same training split from Table~\ref{tb:general_result} for training PGExplainer. 
GNNExplainer overall generates the fastest explanation since it directly and only learns edge masks from each graph (as for graph structure). The extra training cost from PGExplainer takes the majority of the time consumption, while extra mask learning is not needed for explanation. PGM-Explainer spends its running time in node attribute perturbation and calculation. The time consumption is affordable for simple datasets because the graph size is limited and PGM-Explainer provides the accurate explanation, while for complex cybersecurity datasets, more energy is needed for sampling the perturbed dataset. The time complexity of {\name} is closely higher than GNNExplainer due to more time consumption for the nodes and attributes. The time consumption from {\name} is acceptable since {\name} provides a comprehensive and accurate explanation. Large time complexity will be necessary if different explanation methods are combined for a comprehensive explanation.

\subsection{Ablation Study}

\textbf{Attribute explanation study. }
We further evaluate the node attribute explanation of {\name}, as shown in Table~\ref{tb:result_feat}. Generally, the highest EP values are obtained by {\name}. It proves that the node attributes contribute to the prediction differently, so the importance scores should be applied to them individually. 

% Low $Fidelity-^{prob}$ but relatively high $Fidelity-^{acc}$ for Infinite loop dataset indicate that the prediction probabilities for the dataset are close to the threshold 0.5 since the slight change of probability would highly influence the change of prediction labels. The distinct features from the dataset are not well captured by the model. 
The results also indicate that only a small number of node attributes are highly important to the prediction. Compared with node explanation, an individual node attribute can contribute more to the prediction than an individual node from the two applications. Intuitively, the attack on node attributes can be easily conducted. Besides, the attack is not as noticeable as the attack on nodes, especially for CWE-476 dataset.
%Specifically, in CWE-476 dataset, negative $Fidelity-^{prob}$ values suggest the contribution of the remaining node attributes is negative after removing the important node attributes. 

\textbf{Ablation study for node explanation. }
The node importance scores are gathered by the importance scores of message passing, requiring the importance scores for edges and node attributes. Here, we gather the importance scores for nodes by edge explanation only and attribute explanation only, in order to verify the node explanation requires both edge and node attribute explanation. The importance scores from edges only are gathered in the same way as above experiments without considering importance scores from node attributes. The importance scores from node attributes only are gathered from synchronized attribute mask learning. We evaluate the EP of explained subgraphs in Table~\ref{tb:ablation}.

Compared with the results in Table~\ref{tb:ep-}, generally, the node explanation by edge only or attribute only is not as accurate as when they are combined. 
Attribute-only explanation overall obtains lower EP in smart contract vulnerability detection but higher EP in code vulnerability detection. By comparing the difference, the results from Teentrancy indicates the graph structure makes the key contribution to the prediction, while those from CWE-416 and CWE-476 indicate the opposite. Node attributes can take an important role to estimate the importance of each node. For graph structure explanation, especially when it comes to unimportant node removal, it is necessary to have nodes specially explained. 

{\revision
\subsection{Evaluation on Node Classification Task}

Additionally, We study the explanation performance on node classification task. 

\textbf{Background.}
We use the basic Graph Convolutional Network (GCN)~\cite{kipf2017semi} as the node classifier. GCN is a GNN with the following propagation rule for one layer:
\begin{equation}
	H^{(l+1)} = \phi (\Tilde{D}^{-\frac{1}{2}}\Tilde{A}\Tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})
	\text{.}
\end{equation}
Here, $\phi$ is the activation function, $A$ is the adjacency matrix, $\Tilde{A} = A + I$, and $\Tilde{D}_{ii} = \sum_{j}\Tilde{A}_{ij}$. 
For node classification task, fully connected layers are adopted after GCN to compute the classification. 

\begin{table}[t]
    {\revision
	\caption{\revision The specifications of different dataset and the accuracy of the pre-trained models.}
	\label{tb:data}
	\centering
	\tabcolsep = 0.25cm
	\begin{tabular}{lccccc}
		\toprule
		\textbf{Dataset} & $\boldsymbol{\mid \mathcal{V}\mid}$ & $\boldsymbol{\mid \mathcal{E}\mid}$ & $\boldsymbol{\mid Y\mid}$ & $\boldsymbol{\mid \mathcal{X}\mid}$ & \textbf{Accuracy} \\ 
		\midrule
		Cora & 2,708 & 5,429 & 7 & 1,433 & 0.807 \\
		Citeseer & 3,327 & 4,732 & 6 & 3,703 & 0.711 \\
		\bottomrule
	\end{tabular}
	}
\end{table}%

\textbf{Evaluation.}
We use a 2-layer GCN with 64 hidden channels for each layer, and a fully connected layer after GCN for node classification. We adopt ReLU as the activation function. The training and testing split is the public fixed split from ~\cite{yang'16}. Table~\ref{tb:data} shows the information of the dataset we use. 
We use the test split for the explanation. We compare {\name} with GNNExplainer~\cite{GNNEx19}. Here, we extract the top-5 and top-10 nodes for both datasets and evaluate the performance with the metric Essentialness Percentage (EP). 
% EP is the percentage of explained subgraphs that retain the original predictions of the central nodes within a dataset. It measures the correctness of the explanation.
Since we use 2-layer GCN, the extracted nodes are within 2-hop neighbors.  

\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{./picture/node_result.pdf}
	\caption{\revision The explanation result of node classification tasks.}
	\label{fig:node_result}
\end{figure}%

As shown in Figure~\ref{fig:node_result}, {\name} obtains 7.1\% EP higher than GNNExplainer on average. From both datasets, {\name} outperforms GNNExplainer distinctly when the number of extracted nodes is small. Such a promising result also proves that it is necessary to jointly consider edges and attributes for node explanation. We believe {\name} will outperform significantly on GNN adaptations and alleviate the limitations of general explanation methods in cybersecurity applications.
}

% \section{Applications of GNN Explanation}
\section{Case Study}
\label{sec_case_study}
%\subsection{Case Study}

%{\td Provide case studies for both applications. For each application, try to add as many specific cases as possible.}
%
%{\td We need to provide as many security insights as possible. Some potential questions include}

In this section, we make two case studies of applying {\name} to real cybersecurity applications, code vulnerability and smart contract vulnerability detection.
In order to obtain straightforward results and comprehensive evaluations, we focus on code vulnerability detection.
% deeply explain two applications by {\name}.
% %provide explanation of {\name} for specific cases with human understandings. 
% We show how humanly understandable explanations help developers evaluate a model with not only model accuracies. We prove that the explanation results obtain transparency for GNNs by uncovering the model behaviors from case studies, by which developers are able to gain trust and troubleshoot the misbehaviors from the models. We measure the reduction in prediction accuracy for case studies, which is the probability decrease of the subgraph.
% In order to obtain straightforward and comprehensive results for the case study, we focus on code vulnerability detection. \todo{}

\begin{figure*}[t]
	\centering
	\includegraphics[width=.85\linewidth]{./picture/cwe415-164.pdf}
	\caption{The case study for ``double free''. The reduction in prediction accuracy is 0.005, from the original 1.000.}
	\label{fig:cwe415-164}
\end{figure*}

\begin{figure*}[t]
	\centering
	\includegraphics[width=.85\linewidth]{./picture/cwe416-69.pdf}
	\caption{The case study for ``use after free''. The reduction in prediction accuracy is 0.966, from the original 0.980.}
	\label{fig:cwe416-69}
\end{figure*}

\begin{figure*}[t]
	\centering
	\includegraphics[width=.85\linewidth]{./picture/cwe476-860.pdf}
	\caption{The case study for ``NULL pointer dereference''. The reduction in prediction accuracy is 0.003, from the original 1.000.}
	\label{fig:cwe476-860}
\end{figure*}

\subsection{Case \#1: Code Vulnerability Detection}

\begin{figure*}[t]
	\centering
	\includegraphics[width=.85\textwidth]{./picture/Comparison-cwe416-69.pdf}
	\caption{\revision The explanation result from ``use after free'' example. The accuracy reductions are 0.973, 0.980 and 0.977, respectively.}
	\label{fig:case_comparison}
\end{figure*}

% \textbf{Code vulnerability detection. }
%With inspiration of VulSniper~\cite{vulsniper}, we argue that using GNNs should obtain better results since Vulsniper does not provide "message passing" between nodes. We compare Devign~\cite{devign} with VulSniper. 

%\subsubsection{Background \& Evaluation}
%\subsubsection{Background \& Experimental Setup}
% \todo{}
\textbf{Background.}
We summarize three steps for code vulnerability detection using GNN models. 
(1) Graph extraction. Code property graphs (CPGs) are generated as the graph representation for source code. A node represents a program construct such as variables, statements, and symbols; an edge contains the direction and relationship information for a pair of nodes such as control flow and data flow. 
(2) Attribute encoding. To better represent the source code and fit the code property graphs to GNNs, node or edge attributes have to be encoded. Node attributes are the most widely used attributes in code vulnerability detection. 
(3) Model learning. This application is conducted as a graph classification task. With the code property graphs and node attributes as input, labels of benignity and vulnerability as targets, the model is learned from a set of datasets. 

In this experiment, we use AFGs as our CPGs. Therefore, a node denotes a statement, an edge contains the direction and relationship information (control flow and data flow) for a pair of nodes. We use Joern~\cite{joern} to extract CDFGs from C/C++ code. We make sure each graph contains 32 nodes. The keywords from each statement are extracted for node attribute encoding. A node attribute indicates whether the statement has the corresponding keyword, e.g., \texttt{char}, \texttt{==} \texttt{*}, so it is encoded to be binary. There are 96 node attributes for each node. We use the model, Devign, as the code vulnerability detector.
%, which conducts a graph classification task. The labels of ACDFGs indicate whether the source code is vulnerable or benign.

%\todo{
%	The framework to develop a GNN-based security application usually consists of graph structure generation, attribute encoding and GNN learning. 
%	% In order to fully understand and develop an application, comprehensive and faithful explanation is necessary. For GNN-based security applications specifically, the requirements for the explanation method fall into these aspects: (1) \textit{comprehensive explanation}, (2) \textit{faithful explanation}. % , (3) \textit{model-agnostic explanation}. 
%	The development of code vulnerability detection technique, e.g., Devign~\cite{devign},  firstly transfers the source code into code property graphs, and encodes node attributes for each node. The GNN-based model is then learned from a set of extracted code property graphs with labels indicating whether the source code is vulnerable. We summarize three steps of such applications. 
%	(1) \textit{Graph Extraction.} The code property graph is the most widely used graph to represent source code. A node represents a program construct such as variables, statements and symbols; an edge contains the direction and relationship information for a pair of nodes such as control flow and data flow. 
%	(2) \textit{Attribute Encoding.} To better represent the source code and fit the code property graphs to GNNs, node or edge attributes have to be encoded. Node attributes are the most widely used attributes in code vulnerability detection. There is a variety of techniques for node attribute encoding such as Word2vec~\cite{Word2Vec}, symbol or type extraction~\cite{vulsniper}. 
%	(3) \textit{Model learning.} This application is conducted as a graph classification task. With the code property graphs and node attributes as input, labels of benignity and vulnerability as targets, the GNN-based model is learned from a set of datasets. To make a prediction, the source code is extracted and encoded into a code property graph with node attributes, which then are fed into the trained model and the prediction will be computed.
%}

% \textbf{Case study.}
% commented because of repetition
% Figure~\ref{fig:cwe415-164},~\ref{fig:cwe416-69}, and \ref{fig:cwe476-860} are specific explanation cases from the three code vulnerability detection datasets. We show the source code of vulnerable functions, the corresponding AFGs, and the explanation results. For the explanation results, we show the importance scores for nodes, as well as edges and node attributes of important nodes. {\name} is able to pinpoint the key factors from the input graph that contribute to the prediction. % And it shows accurate explanation in security applications.

\textbf{Evaluating the output of {\name}.}
We measure the reduction in prediction accuracy for each case, which is the probability decrease of the explained subgraph.

The vulnerability in Figure~\ref{fig:cwe415-164} is caused by ``double free''. Different from Figure~\ref{fig:comparison}, the source code here calls a function. The key reasons for vulnerability are the same, while the model considers the function nodes in Figure~\ref{fig:cwe415-164} as the contribution. It is reasonable that the function is the path from line 12 to line 2. The output of {\name} suggests the model's competence and weakness. It successfully captures the vulnerability, but the performance drops down as the source code becomes complex. From our graph generation technique, the functions are not specially identified, which can be opened up and embedded into the major function.

Figure~\ref{fig:cwe416-69} shows an example from the dataset ``use after free''. The output suggests that the model's decision-making is the same as human knowledge. The importance score of the edge indicates the edge is not highly important to the prediction, which may be a potential risk. % From the result, the model is able to identify the important nodes as how humans understand the source code. 
% It also finds a risky node with high importance score. The risk is mostly caused by the equal mark, which can be optimized by adapting the dataset, e.g., putting more similar statements into benign functions or variate the position of such statement. While the reduction in prediction accuracy is significant, {\name} correctly determines the important factors and how GNNs make the prediction. As it is observed from Figure~\ref{fig:ep+} and Table~\ref{tb:result_sparsity}, the graph size plays an important role in the graph classification task from this dataset, especially for vulnerable code. From our experiment, explained subgraphs of the small size are not able to retain the prediction well, however, the explanation is still well performed.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{./picture/Codes.pdf}
	\caption{The explanation results of two pairs of mirrored source codes. }
	\label{fig:codes}
\end{figure*}

\begin{figure*}[t]
	\centering
	\includegraphics[width=.85\linewidth]{./picture/mispred.pdf}
	\caption{The explanation results of mispredictions. The gray box is the explanation for the mispredictions.}
	\label{fig:mispred}
\end{figure*}

The dereference of \texttt{NULL} pointer leads to the vulnerability in Figure~\ref{fig:cwe476-860}. The explanation results suggest the key reason for the prediction is node 4, where the pointer is assigned as \texttt{NULL}. However, it captures line 6 rather than 7, which is contradictory to human understanding. It is understandable because its mirrored version of benign code contains the symbol \texttt{!=} in \texttt{if} condition (line 6). The explanation suggests the dataset is well learned by the model but less confidence if the model is adopted to real applications.

% The explanation is understandable because the model accuracy for CWE-476 is not as high as other datasets. 
% As we refer to the dataset, we find that the mirrored benign source code contains the symbol \texttt{!=} in the \texttt{if} condition. The model learns such differences between benign and vulnerable codes. Our explanation suggests the dataset is well learned by the model but shows less confidence in the model performance for a more generic source code.

% As the node attributes are encoded by the keywords from each line, the semantic information and other features of the code are not completely identified or encoded. The model is learned from well-arranged and tuned real-world datasets. Source code with poor coding styles is left out, e.g., variables keeping assigned as \texttt{NULL}. Therefore, different coding styles and situations should be considered so the GNN models can be optimized for better performance in generic real-world datasets.
% The model itself can be vulnerable and attacked by changing the datasets without notice.

%{\td this is a "Use after free", good}
%
%{\td add at least one for each CWE}
%\ans{I added one case for each CWE and analyzed two pairs of mirrored code.}


% \re{Improvements \& scope}

% \textbf{The improvement from {\name}.}
{\revision
Figure~\ref{fig:case_comparison} shows the explanation result from other methods for the same vulnerable code shown in Figure~\ref{fig:cwe416-69}. One can observe that {\name} significantly outperforms other explanation methods by providing a comprehensive explanation for nodes, edges, and attributes. Missing one explanation factor can cause significant difficulty for analysis. The explanation accuracy is also degraded as seen from the reductions in prediction accuracy. PGExplainer, as a global explanation method, may not provide customized results for a single input graph. While the reduction in prediction accuracy is significant, {\name} achieves the lowest reduction and provides human-understandable explanation. As it is observed from Figure~\ref{fig:ep+} and Table~\ref{tb:result_sparsity}, the graph size plays an important role in the prediction. Wrong information from explanation methods may lead to more confusion and the wrong conclusion to the models. With the trust of {\name}, cybersecurity analysts can easily map the output to the source code and understand the model behavior. 

Besides, {\name} alleviates the limitations in graph-specific explanation methods: descriptive accuracy (DA), efficiency, robustness, and stability~\cite{Ganz21}. {\name} greatly improves DA and efficiency as the experiment shows. Specifically in code vulnerability detection, which lines of code contribute to the prediction is important to cybersecurity analysts. Each line is represented as a node in the AFG, which makes it vital to accurately determine the importance of nodes. {\name} accurately identifies the important lines and keywords. By gathering both edge and attribute information for node explanation, {\name} is robust against edge perturbation. Similarly, we believe stability is also preserved.
}

{\revision

\textbf{Using the output of {\name}.}
% \re{How can we use the output of Illuminati?}
% \textbf{The usage of {\name}.}
The explanation methods with high EP should be able to provide accurate information on which part of the code is considered vulnerable by the model. They can identify the vulnerable lines when the model's decision-making matches human knowledge. However, the usage of {\name} is not limited to this. First, {\name} helps cybersecurity analysts pinpoint the model's misbehavior even though the model gives the correct predictions. Second, {\name} helps analysts interpret why mispredictions are made. The developers can identify the pitfalls observed from the recent study~\cite{Arp22}, and take certain actions to troubleshoot and optimize the model based on the output of {\name}.

}

% Not only the vulnerable nodes, {\name} also finds a risky node with high importance score in Figure~\ref{fig:cwe416-69}. The risk is mostly caused by \texttt{=}.
%, which can be optimized by adapting the dataset, e.g., putting more similar statements into benign functions or variate the position of such statement. 

% To further analyze Figure~\ref{fig:cwe476-860}, we find that its mirrored version of benign source code contains the symbol \texttt{!=} in the \texttt{if} condition (line 6). The model learns such differences between benign and vulnerable codes. Our explanation suggests the dataset is well learned by the model but shows less confidence in the model performance for a more generic source code.

More results of paired code are shown in Figure~\ref{fig:codes}. {\name} detects the important vulnerable factors in Figure~\ref{fig:codes}(a) and (c), benign factors in Figure~\ref{fig:codes}(b) and (d) according to their predicted labels.
As the result shows, the code in Figure~\ref{fig:codes}(a) is vulnerable because of ``double free'', where the model captures the vulnerability and {\name} successfully identifies the vulnerable lines. The explanation for Figure~\ref{fig:codes}(b) shows benign statements from the source code. Combining Figure~\ref{fig:codes}(a) and (b), the explanation suggests that the model makes the classification by detecting vulnerable factors. 
The vulnerability in Figure~\ref{fig:codes}(c) is caused by ``NULL pointer dereference''. Comparing Figure~\ref{fig:codes}(c) and (d), the model detects the vulnerability by the value assignment to the variable, where \texttt{NULL} leads to vulnerability. The model also detects the difference from the conditions. From the dataset, vulnerable functions do not contain a lot of ``false'' conditions. The model fails to identify a key statement, i.e., line 7 in Figure~\ref{fig:codes}(c) because vulnerable and benign code both contain such statements. Therefore, the model for this dataset is vulnerable to attacks and is not trustable even it achieves high accuracy. To alleviate the issues, different conditions should be considered to fill the dataset, and more semantic information can be extracted. 

\begin{figure*}[t]
	\centering
	\includegraphics[width=.85\textwidth]{./picture/Reentrance_01.pdf}
	\caption{An example of Reentrancy. The reduction in prediction accuracy is 0.332, from the original 0.991.}
	\label{fig:Reentrance_01}
\end{figure*}

\begin{figure*}[t]
	\centering
	\includegraphics[width=.85\textwidth]{./picture/simple_dao.pdf}
	\caption{An example of Reentrancy. The reduction in prediction accuracy is 0.186, from the original 0.845.}
	\label{fig:simple_dao}
\end{figure*}

Furthermore, we evaluate cases of mispredictions in Figure~\ref{fig:mispred}, where the gray box is the explanation of GNN predictions (mispredictions). Further explanation for the correct label is also shown in the white box. The labels of the left column are benign, and those on the right are vulnerable. Here we show results from CWE-416 and CWE-476 since the mispredictions from CWE-415 mostly happen to small graphs.
As it can be observed, {\name} suggests GNNs still have captured the important lines for the correct label. The wrong prediction from the left column comes from \texttt{printLine}, which indicates the use of variables in the model's perspective. The model emphasizes the use of variables but fails to determine the variable is not \texttt{NULL}. More different situations should be added into training, e.g., situations of a variable being used by multiple times without being freed in CWE-416. 
The result shows GNNs are able to detect the vulnerability for CWE-416 at the right column. But the benign lines take the lead through the calculation of GNN, as the importance scores in the gray box do not vary largely. The \texttt{for} loop from CWE-476 exist in codes with different labels, so GNNs randomly assign importance of statements in line 3 to different labels. The vulnerability is identified but not strong enough because the use of variables is in an \texttt{if} condition (line 5). \texttt{printLine} usually indicates the use of variables, but here the argument is a string, which is correctly observed as a benign statement.

{\revision
From interpreting the output of {\name}, the pitfalls found in this application includes spurious correlation, the inappropriate performance measures and lab-only evaluation~\cite{Arp22}. Spurious correlation is caused by the artifacts of the dataset. Different coding styles, length of code and logical situations are not completely considered. This can be alleviated with lab-only evaluation by collecting datasets with different cases from the real world. Only evaluating the prediction accuracy may lead to the neglect of the dataset issues. This will give developers the wrong conclusion of the model. Inappropriate performance measures are addressed by strong explanation methods such as {\name}. The developers can interpret the explanation output for the decision-making and the potential risks of the model.
The output of {\name} suggests several internal drawbacks of the models as well, e.g., the model does not learn the semantic meaning. The models we use do not make full use of the source code information. Without enough semantic information of the statements and the type of edges, it prevents the model from making the correct prediction in Figure~\ref{fig:mispred}. The developers can build a solid strategy to improve the model with the output of {\name}.
}


\subsection{Case \#2: Smart Contract Vulnerability Detection}
%{\td we also need specific cases here.}
%\ans{I added 2.}


We consider cases from Reentrancy dataset, as the contract graphs from vulnerable source code contain enough nodes for the case study. 
The contract graph is constructed according to the work of Zhuang \textit{et al.}~\cite{ijcai2020-454}. The nodes in a contract graph are categorized into major nodes, secondary nodes, and fallback nodes. The major nodes represent important functions, the secondary nodes represent model critical variables and the fallback nodes simulate the fallback function. The edges indicate the relationship between nodes, where the edge attributes are only used for graph construction, not in DR-GCN. The node attributes are derived from the types of functions, operations of variables, etc.
Figure~\ref{fig:Reentrance_01} and \ref{fig:simple_dao} show case studies from Reentrancy dataset. The node M1 is the function that calls \texttt{withdraw} function, M2 is the built-in \texttt{call.value} function and M3 is the \texttt{withdraw} function, all of which are major nodes.

The vulnerability in Figure~\ref{fig:Reentrance_01} comes from the value being assigned (line 5) after checking if ether sending (line 2) goes through. From the explanation result, the GNN model successfully identifies the location of vulnerability.

With the same vulnerability in Figure~\ref{fig:simple_dao}, however, the GNN captures the factors leading to the right prediction rather than the vulnerable statements. {\revision From the code, the transaction (line 5) is after the \texttt{if} statement (line 2). So the model predicts the function as vulnerable. The explanation result shows the two key statements for the prediction. But they are not exactly the ground truth causing the vulnerability, so the decision-making of the model is still confusing to users.} To address the issue, we show the mirrored benign code as follows. 

\lstinputlisting[basicstyle=\small]{code.sol}%

% \begin{minted}[style=friendly,numbers=none]{solidity}
% function withdraw(uint amount) public {
%     if (credit[msg.sender] >= amount) {
%         credit[msg.sender] -= amount;
%         require(msg.sender.call.value(
%             amount)());
%     }
% } 
% \end{minted}

From its mirrored benign code, the value assignment and ether sending is under \texttt{if} condition. In the \texttt{if} condition, the value is assigned first, then the \texttt{call.value} function is called. Accordingly, the path in the corresponding contract graph would be S1 $\,\to\,$ S2 $\,\to\,$ M2. Here, S1 does not directly connect with M2, which causes different node representations from the code in Figure~\ref{fig:simple_dao} and they are learned by the GNN model. Thus, a potential problem from the dataset is identified.

{\revision
A common pitfall from the training datasets in the two applications is spurious correlation, specifically the lack of various real-world coding situations. The models may not make the correct predictions in different dataset because the output of {\name} suggests the models have learned some artifacts rather than the real difference between vulnerability and benignity. 
% It is highly possible that the model will not be able to correctly capture the major difference between benign and vulnerable code. 
The edge type is also neglected in this application. How developers utilize the output of {\name} and improve the model is similar to code vulnerability detection.
% The relational information is neglected after graph construction, which may lead to inaccurate node representation.
}

%Here, we take node features into consideration. We do not compare feature reasoning with other frameworks because they target on a general feature explanation for the whole graph. {\name} identifies important features for each node.

%The four lines of code and important features are explained well by {\name}. {\name} is able to identify it as an important node. For many explanations on graphs, especially ACDFGs, the relation between nodes should be taken into consideration. {\name} explains not only important features or nodes containing important features, but also maintains the graph structure.

%The dataset in this experiment has obvious ground truth for {\name} verification. For dataset that is not easily interpretable, {\name} is able to find the important subgraph with node features to help humans understand. Even for interpretable dataset, GNN models may find nodes or features that are not obvious to humans, which can be unveiled by {\name}. Sometimes it happens because of the lack of training dataset. For example, if we only use "double" as variable type in benign code, while only use "int" in vulnerable code. GNN models may classify the code by variable type, which is not related to the vulnerability. Humans can identify the reasons for prediction by {\name}, so they are able to pinpoint the errors or find new hidden features.

