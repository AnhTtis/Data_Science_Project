\section{Related Work}
\label{sec_related_work}


\textbf{Graph neural networks. }In recent years, there have been a great number of evolutions in GNNs. % Many of the state-of-art works are evolved from graph convolutional networks (GCN)~\cite{kipf2017semi}. 
% A GNN model generates node representations iteratively by neighbor node information aggregation. 
Scarselli \textit{et al.}~\cite{gnn} firstly introduced GNN as a neural network model, extending the traditional neural network for graph data processing. Bruna \textit{et al.}~\cite{bruna2014} extended the convolutional methods for graph structure by analyzing the constructions of deep neural networks on graphs. Defferrard \textit{et al.}~\cite{defferrard2016} proposed the extension of CNNs to graphs using Chebyshev polynomials. GCN identified that the simplifications can be used in the previous work and presented fast approximate convolutions on graphs.
Plenty of the GNN models, including GCN~\cite{kipf2017semi}, GraphSAGE\cite{hamilton2017inductive}, and GAT~\cite{velickovic2018graph}, generate node representations iteratively by aggregating and updating the attributes from the neighbor nodes. 
The node representations, then are used in different tasks like node classification~\cite{kipf2017semi,WuSGC19}, link prediction~\cite{mlink_aaai20,zhang2018link}, and graph classification~\cite{Errica2020A,magcnn_aaai20}.
% Also, GNNs have made great accomplishments in tasks like node classification~\cite{kipf2017semi,WuSGC19}, link prediction~\cite{mlink_aaai20,zhang2018link} and graph classification~\cite{Errica2020A,magcnn_aaai20}.
% There are also a large number of researches on understanding and analysis of GNNs. Work from~\cite{xu2018how} analyses their expressive power to capture various graph structures and puts forward an architecture based on the power.~\cite{knyazev2019understanding} study node attention in GNNs and factors that benefits their performance.
% With massive basic theories and analysis of GNNs, there has been studies of GNN applications in a variety of fields, for example, social networks~\cite{graphrec_www19} and traffic networks~\cite{GMAN-AAAI2020}. In program representation specifically, other than the applications we used, there has been LambdaNet~\cite{lambdanet} for code type annotation inference and Hoppity~\cite{hoppity} for bug detection.

\textbf{Deep learning explanation. }The generic purpose of an explanation method is to determine the decision-making by a complex deep learning model. The two major classes of an explanation method are black-box based~\cite{lime,lemna} and white-box based~\cite{ancona2018towards,oramas2018visual}. 
Methods with various techniques are proposed to uncover the behaviors of deep learning models. LIME~\cite{lime} and work from paper~\cite{zintgraf2017visualizing} treat the whole deep learning model as a blackbox. The model decision is explained by directly identifying the important factors from the input. Methods such as LRP~\cite{bach-plos15} and DeepLIFT~\cite{deeplift} decomposes the output backward through model layers and explain the contribution of neurons. Rather than providing a post-hoc explanation for deep learning models, CapsNet~\cite{capsnet} is built as a DNN model with the embedded design of explainability.
Some explanation methods work on specific models, e.g., CNN~\cite{Zhang_2018_CVPR} and RNNs~\cite{bradbury2016quasi}.
% The explanation methods can further be categorized by models, where CNNs and RNNs are fundamental categories of deep learning models. So there is a large number of explanation methods specifically targeting on CNNs~\cite{Zhang_2018_CVPR} and RNNs~\cite{bradbury2016quasi}. The explainability for deep learning models is also developed for different applications, e.g., security applications~\cite{lemna} and NLP~\cite{liu2019towards}.

\textbf{GNN explanation.}
%GNN explainability is not studied as deeply as other DNNs for image and text processing.
GNNExplainer~\cite{GNNEx19}, as the pioneering explanation method directly targeting on GNNs, provides edge and node attribute explanations by learning the corresponding masks, which represent the importance scores. PGExplainer~\cite{luo2020parameterized} provides an inductive edge explanation method working on a set of graphs, by learning edge masks with a multi-layer neural network. GraphMask~\cite{schlichtkrull2021interpreting}, however, learns the edge masks for each layer of GNNs and predicts whether an edge can be dropped while retaining the prediction. Differently, PGM-Explainer~\cite{pgmexplainer} identifies important nodes by random node attribute perturbation and a probabilistic graphical model. SubgraphX~\cite{subgraphx_icml21} explains graph in node-assembled subgraph level by Monte Carlo tree search with Shapley value as the scoring function. Different explanations for GNNs have recently been explored. CF-GNNexplainer~\cite{cf_gnnexplainer} targets on counterfactual explanations by learning a binary perturbation matrix that sparsifies the input adjacency matrix. 
With the evolution of GNN explanation methods, a recent survey~\cite{yuan2021explainability} categorized graph explanation methods into two major levels --- instance-level and model-level. The aforementioned methods belong to instance-level, which provide explanations for specific inputs. Model-level methods generate a typical graph pattern that explains how the prediction is made. XGNN~\cite{Yuan_2020} directly explains a GNN model by graph generation, using a reinforcement learning method. If trained by multiple graphs, PGExplainer is able to provide model-level explanation. 
% Instance-level explanations are divided into gradients/features-based~\cite{exp_gcnn}, perturbation based~\cite{GNNEx19}, surrogate~\cite{pgmexplainer} and decomposition methods~\cite{2020arXiv200603589S} according to the explanation technique. GNNExplainer~\cite{GNNEx19} is the pioneering work in this field. It extracts the important subgraph and important node features by edge and node feature explanation by learning the corresponding masks. Most explanation methods for GNNs focus on specific elements of the graph (mostly graph structure) and fail to provide a comprehensive explanation for the input graph.

%\textbf{General ML/DL explanation.}
%\ans{Added this part above.}


% comment out for space
%\textbf{Smart contract vulnerability detection. }There has been a variety of methods for smart contract vulnerability detection. Oyente~\cite{oyente} detects security bugs by symbolic execution for Etherum smart contract. Securify~\cite{securify} obtains code semantic information by symbolical analysis, then checks its compliance and violation patterns. ContractWard~\cite{Wang2020ContractWardAV} builds a model with five machine learning algorithms and two sampling algorithms which learns from extracted bigram features. 
%
%\textbf{Code vulnerability detection. }Graph structure has been widely used to represent the source code~\cite{allamanis2018learning} so the study on programs is not limited in natural language techniques. A previous method VulDeePecker~\cite{vuldeepecker} takes code as natural language and trains a BLSTM-based model. Vulsniper~\cite{vulsniper} represents code by code property graphs but extracts the code information and graph information into a three dimensional matrix. %Following the work of graph representation for source code \cite{allamanis2018learning}, 
%Hoppity~\cite{hoppity} represents the source code by modified abstract syntax tree (AST). For bug detection and fix, the GNNs provide a fixed size of program embedding, followed by a LSTM to perform a fix. 

