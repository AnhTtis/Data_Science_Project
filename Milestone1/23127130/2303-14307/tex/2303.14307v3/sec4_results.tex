
\input{tables/tab_diff_asr_lrs3}
\vspace{-1mm}
\subsection{Do better Librispeech ASR models provide better transcriptions for VSR?}
Given that several publicly-available ASR models are available, we use performance on Librispeech as a criterion for model selection. We use models that have achieved state-of-the-art performance on the test-clean set of Librispeech, i.e., Conformer-Transducer~\cite{DBLP:journals/corr/abs-1909-09577} and HuBERT~\cite{hsu2021hubert}. We also use  ASR models that are widely used in the speech community, wav2vec\,2.0~\cite{baevski2020wav2vec} and Whisper~\cite{radford2018improving}.
Performance of ASR models on Librispeech clean-test set is shown in the first column of Table~\ref{tab:different_asr_models_lrs3}. 
Results of the ASR and VSR models trained with the automatically-generated transcriptions on the LRS3 dataset are shown in the third and fourth columns, respectively, of Table~\ref{tab:different_asr_models_lrs3}.
We observe that overall the WER on Librispeech is not highly correlated with the performance of the ASR and VSR models trained with the automatically-generated transcriptions from the corresponding pre-trained ASR models. The same conclusion is also true when we measure the WER on the LRS3 test. We show that using the transcriptions from most ASR models (i.e., wav2vec\,2.0~\cite{baevski2020wav2vec}, Whisper~\cite{radford2018improving}, and Conformer-Transducer~\cite{DBLP:journals/corr/abs-1909-09577}) results in very similar WER for both audio-only and visual-only models. The only exception is the use of automatically-generated transcriptions from HuBERT~\cite{hsu2021hubert} which results in slightly worse performance despite being one of the best performing models on Librispeech. 
In this work, we rely on the automatically-generated transcriptions from the Conformer-Transducer~\cite{DBLP:journals/corr/abs-1909-09577} since on average it leads to the best performance for both ASR and VSR models.

\vspace{-1mm}
\subsection{Impact of the number of hours of unlabelled data}
\input{tables/tab_percent_lrs3}
Table~\ref{tab:percentage_experiments_on_lrs3} shows the impact of varying the numbers of hours of unlabelled data on the performance of ASR and VSR models on LRS3. An absolute improvement of~1.7\,\% in WER is observed for VSR by using only labelled data from LRS2 and LRS3~(818 hours) compared to~\cite{ma2022visual}. This gain is likely due to the increase in model capacity. When including~20\,\%~(526 hours) of AVSpeech~\cite{DBLP:journals/tog/EphratMLDWHFR18} and VoxCeleb2~\cite{DBLP:conf/interspeech/ChungNZ18}, the performance of audio- and visual-only models can be further improved to 1.3\,\% and 26.6\,\% WER, respectively. Increasing further the number of training hours leads to a further reduction of the WER especially for the VSR model. This is in line with the recent trend observed in the literature~\cite{ma2022visual}, where using larger training sets substantially improves performance. In this experiment, we also show that the WER can be improved even by adding data that have been automatically transcribed and inevitably have noisy labels. We also notice that the improvement for the ASR model is marginal when using more than 1\,578 hours of unlabelled training data, indicating that the ASR performance may have saturated.

\input{tables/tab_sota_lrs2}
\input{tables/tab_sota_lrs3}
\vspace{-1mm}
\subsection{Comparison with the state-of-the-art}
% new version
Results on LRS2 and LRS3 are presented in Tables~\ref{tab:sota lrs2} and~\ref{tab:sota lrs3}, respectively. For LRS2, it is clear that our visual-only, audio-only and audio-visual models further push the state-of-the-art performance to a WER of~14.6\,\%, 1.5\,\% and 1.5\,\% respectively. For LRS3, the best visual-only model has a WER of 19.1\,\%, which is outperformed only by~\cite{serdyuk2022transformer} (17.0\,\% WER) which uses 26$\times$ more training data.
Similarly, our audio-only model establishes a new state-of-the-art~\cite{DBLP:journals/corr/abs-2201-02184} by achieving a WER of 1.0\,\% when using 1\,921 hours of training data from LRW, LRS3 and VoxCeleb2 datasets. However, when further introducing AVSpeech for training, no further improvement is observed, suggesting that the ASR performance may have reached saturation. State-of-the-art performance is also achieved for AV-ASR with a WER of 0.9\,\%.


\vspace{-1mm}
\input{tables/tab_noise_lrs3}
\subsection{Noise experiments}
Results of ASR and AV-ASR models, when tested with different acoustic noise levels, are shown in Table.~\ref{tab: noise experiments}.
During training we use the babble noise from the NOISEX dataset~\cite{DBLP:journals/speech/VargaS93}, while the SNR level is selected from [-5\,dB, 0\,dB, 5\,dB, 10\,dB, 15\,dB, 20\,dB, $\infty$ dB] with a uniform distribution. For evaluation, we test three types of noise:  babble noise~\cite{DBLP:journals/speech/VargaS93}, pink and white noise from the Speech Commands dataset~\cite{DBLP:journals/corr/abs-1804-03209}.
We show that, overall, the results are consistent with those presented in~\cite{afouras2018deep, DBLP:journals/corr/abs-2102-06657, makino2019recurrent, DBLP:conf/interspeech/ShiHM22}, i.e. the performance of audio-only models is closer to the audio-visual counterpart in the presence of low levels of noise, whereas the performance gap becomes larger as the noise levels increase. We notice that when using babble noise for evaluation, the performance of either audio-only or audio-visual models has a WER lower than 10\,\% at -7.5\,dB. This is likely mainly a consequence of the overlapping noise type in the training and testing phases (despite mismatched levels of noise).