\begin{table}[!t]
\centering
\small
\renewcommand\arraystretch{0.9}
\begin{tabularx}{1.05\linewidth}{l y y y y}
\toprule
\textbf{Method} &\textbf{Type} &\textbf{Extra Data} &\textbf{Total Hours$^\ddagger$} & \textbf{WER} (\%) \\ \midrule\midrule
CM-seq2seq~\cite{DBLP:journals/corr/abs-2102-06657} &\multirow{13}{*}[-0.4em]{V} &\multirow{3}{*}[-0.2em]{\xmark} &\multirow{3}{*}[-0.1em]{438} &46.9 \\
CM-aux~\cite{ma2022visual} & & &  &37.9 \\
Ours & & &  &\textbf{36.3} \\
\cmidrule(lr){1-1}\cmidrule(lr){3-5}
KD\,+\,CTC~\cite{afouras2020asr} & &\multirow{10}{*}[-0.4em]{\cmark} &772 &59.8 \\
KD-seq2seq~\cite{DBLP:conf/cvpr/RenDLHH21} & & &818 &59.0 \\
TM-seq2seq~\cite{afouras2018deep} & & &1\,362 &58.9 \\
AVHuBERT~\cite{DBLP:journals/corr/abs-2201-02184} & & &1\,759 & 26.9 \\
RNN-T~\cite{makino2019recurrent} & & &31\,000 &33.6 \\
VTP~\cite{prajwal2022sub} & & &2\,676 &30.7 \\
ViT3D-CM~\cite{serdyuk2022transformer} & & &90\,000 &17.0 \\
Ours & & &818 &\textbf{33.0} \\
Ours& & &1\,902 &\textbf{23.5} \\
Ours & & &3\,448 &\textbf{19.1} \\
\midrule
CM-seq2seq~\cite{DBLP:journals/corr/abs-2102-06657} &\multirow{6}{*}[-0.4em]{A} &\multirow{1}{*}[-0.1em]{\xmark} &438 &2.3 \\
\cmidrule(lr){1-1}\cmidrule(lr){3-5}
RNN-T~\cite{makino2019recurrent} & &\multirow{5}{*}[-0.1em]{\cmark} &31\,000 &4.5 \\
AV-HuBERT~\cite{DBLP:journals/corr/abs-2201-02184}  & & &1\,759 &1.3 \\
Ours& & &818 &\textbf{1.5} \\
Ours& & &1\,902 &\textbf{1.0} \\
Ours& & &3\,448 &\textbf{1.0} \\
\midrule
CM-seq2seq~\cite{DBLP:journals/corr/abs-2102-06657} &\multirow{6}{*}[-0.4em]{A+V} &\multirow{1}{*}[-0.1em]{\xmark} &438 &2.3 \\
\cmidrule(lr){1-1}\cmidrule(lr){3-5}
RNN-T~\cite{makino2019recurrent} & &\multirow{5}{*}[-0.1em]{\cmark} &31\,000 &4.8 \\
AV-HuBERT~\cite{DBLP:journals/corr/abs-2201-02184} & & &1\,759 &1.4 \\
ViT3D-CM~\cite{serdyuk2022transformer} & & &90\,000 &1.6 \\
Ours& & &1\,902 &\textbf{1.0} \\
Ours& & &3\,448 &\textbf{0.9} \\
\bottomrule
\end{tabularx}
\caption{WER (\%) of our audio-only, visual-only and audio-visual models on the LRS3 dataset. $^\ddagger$ The total hours are counted by including the datasets used for both pre-training and training. Our model trained on 818 hours uses LRW, LRS2 and LRS3. Our model trained on 1\,902 hours uses LRW, LRS3 and VoxCeleb2. Our model trained on 3\,448 hours uses LRW, LRS2, LRS3, VoxCeleb2 and AVSpeech.}
\label{tab:sota lrs3}
\vspace{-1mm}
\end{table}