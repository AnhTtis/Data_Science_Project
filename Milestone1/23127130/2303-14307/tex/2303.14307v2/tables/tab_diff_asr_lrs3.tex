\begin{table}[!t]
\small
\centering
\renewcommand\arraystretch{0.8}
\begin{tabularx}{.99\linewidth}{v  y  y | y y}
\toprule
    \multirow{2}{*}[-0.1em]{\textbf{Method}} & \multicolumn{4}{c}{\textbf{WER [\%]}} \\
\cmidrule(lr){2-5}
 & A$^\dagger$ & A$^{\dagger\dagger}$ & V  & A \\
\midrule\midrule
CM-Transducer~\cite{DBLP:journals/corr/abs-1909-09577} & 1.62 &3.31 & 19.1 &0.99 \\
\midrule
HuBERT~\cite{hsu2021hubert} & 1.90 &6.87 & 19.8 &1.12 \\
\midrule
Wav2vec\,2.0~\cite{baevski2020wav2vec} & 3.40 &11.22 & 19.1 &1.06\\
\midrule
Whisper~\cite{radford2018improving} & 4.10 &1.81 & 19.0 &1.04 \\
\bottomrule
\end{tabularx}
\caption{
Impact of the pre-trained ASR models used to generate automatic transcriptions from unlabelled data on the performance of VSR/ASR models on the LRS3 dataset. $^\dagger$ and $^{\dagger\dagger}$ denote the word error rate (WER) reported on Librispeech test-clean set~\cite{panayotov2015librispeech} and LRS3 test set~\cite{afouras2018lrs3}, respectively. ``CM'' denotes Conformer. ``V'' and ``A'' denote the visual-only and audio-only models trained on LRW, LRS2, LRS3, VoxCeleb2 and AVSpeech (using the automatically-generated transcriptions from the corresponding pre-trained ASR model), with a total of~3\,448 hours.}
\label{tab:different_asr_models_lrs3}
\vspace{-5mm}
\end{table}