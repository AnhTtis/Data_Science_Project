In this work, we propose a simple and efficient method for scaling up audio-visual data for speech recognition. We present a detailed study on the performance of LRS3 in terms of the amount of unlabelled training data. By leveraging publicly-available ASR models to produce automatically-generated transcriptions, we train an AV-ASR system and achieve state-of-the-art performance on both publicly-available audio-visual benchmarks, LRS2 and LRS3. Furthermore, we show that our audio-visual model is more robust against different levels of noise than its audio-only counterpart.