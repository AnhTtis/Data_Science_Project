In the human perceptual system, the visual and audio streams often complement each other, yielding a unified robust response. It is also known that using visual signals along with audio signals leads to higher model robustness than using a single modality, especially in the presence of high levels of acoustic noise~\cite{afouras2018deep, petridis2018audio, makino2019recurrent, DBLP:conf/interspeech/ShiHM22}.

In this paper, we tackle Audio-Visual Automatic Speech Recognition~(AV-ASR, or~AVSR), which aims to transcribe continuous spoken sentences from both audio and visual streams. Recent audio-~(ASR), video-~(VSR) and audio-visual-based models have relied heavily on large-scale and well-labelled transcriptions to achieve convincing performance. However, accurate transcriptions require manual labelling, which is time-consuming and prohibitively expensive. In order to address this issue, several works have been proposed to build advanced ASR and VSR models by leveraging large-scale unlabelled audio-visual datasets. Popular approaches pre-train ASR and VSR models using self-supervised learning, where the goal is to learn audio and visual representations from large unlabelled datasets~\cite{baevski2020wav2vec, ma21c_interspeech, DBLP:journals/corr/abs-2201-02184, hsu2021hubert}. The pre-trained models are then fine-tuned on smaller labelled datasets. Alternatively, another line of work solves the task using knowledge distillation~\cite{afouras2020asr, DBLP:conf/cvpr/RenDLHH21}. For example, Afouras \etal~\cite{afouras2020asr} use a pre-trained ASR model~-~acting as a teacher~-~to provide an extra supervisory signal to the target VSR model, where the goal is to force the posterior distribution of the VSR network to match the teacher. Similarly, Ren \etal~\cite{DBLP:conf/cvpr/RenDLHH21} further improve the performance of visual-only models by distilling knowledge from pre-trained models with multiple modalities.

In this work, we propose a different approach to leverage large unlabelled datasets which does not require a two-step training approach used in self-supervised learning (SSL) (but it can easily be combined with any state-of-the-art SSL approach).  In particular, we take advantage of the availability of good publicly-available pre-trained ASR models~\cite{radford2018improving, baevski2020wav2vec, hsu2021hubert, DBLP:journals/corr/abs-1909-09577} to automatically annotate large-scale audio-visual datasets. This approach is broadly related to self-training~\cite{DBLP:conf/cvpr/XieLHL20, DBLP:conf/icassp/Kahn0H20, DBLP:conf/interspeech/ParkZJHCLWL20}, where a model is first trained on annotated data and then used to generate pseudo-labels for the unlabelled data. A new model is trained with all the annotated data, and this process is repeated for a few iterations. This iterative process might be necessary in other domains for which a high-quality pre-trained model does not exist, but this is not the case for ASR, where accurate pre-trained models are relatively abundant. Thus, we sidestep the need for a costly iterative procedure. Moreover, we incorporate the automatically-transcribed unlabelled data into the training set rather than using the pre-trained ASR model for distillation~\cite{afouras2020asr}. As a result, we can easily train a large-scale AV-ASR system by simplifying the implementation and reducing both the computational and memory costs of using a teacher during training. We also find similarities with~\cite{makino2019recurrent, serdyuk2022transformer}, but instead of using owner-uploaded transcriptions or a production-quality ASR system, all models and datasets we use are publicly accessible. 

Our main contributions can be summarised as follows:~1) we automatically generate transcriptions for more than~2\,000 hours of videos by utilising publicly-available ASR models. We then train ASR, VSR and AV-ASR models with these transcriptions and achieve state-of-the-art performance on the LRS2 and LRS3 datasets. Concretely, the proposed approach leads to a WER of~0.9\% for AV-ASR on the LRS3 dataset, which outperforms models trained on much larger training sets;~2) We show that the accuracy of the pre-trained ASR models used to automatically transcribe the unlabelled datasets is not highly correlated with the performance of the ASR and VSR models trained with these transcriptions; 3) We observe that an increase in the number of hours of automatically-transcribed data used in the training set results in reduced WER, especially for the VSR models. On the other hand, the performance of the ASR models seems to saturate beyond~1500 hours.
