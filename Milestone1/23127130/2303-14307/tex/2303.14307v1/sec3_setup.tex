\subsection{Datasets}
We conduct experiments on LRS2~\cite{chung2017lip} and LRS3~\cite{afouras2018lrs3}, which are the two largest publicly available datasets for audio-visual speech recognition in English. LRS2, collected from BBC programs, contains~144\,482 video clips with a total of~225 hours. Specifically, the pre-training, training, validation and test set contains~96\,318~(195 hours),~45\,839~(28 hours),~1\,082~(0.6 hours) and~1\,243~(0.5 hours) video clips, respectively. LRS3 consists of~151\,819 video clips from TED talks with a total of 439 hours. It contains~118\,516~(408 hours),~31\,982~(30 hours) and~1\,321 clips~(0.9 hours) in the pre-training, training-validation, and test set, respectively. For training, we also use the English-speaking videos from AVSpeech (1\,323 hours) and VoxCeleb2 (1\,307 hours) as the additional training data together with automatically-generated transcriptions.

\vspace{-2mm}
\subsection{Pre-processing}
For the visual stream, we follow previous work~\cite{DBLP:journals/corr/abs-2102-06657} to pre-process the datasets. We crop the mouth region of interests (ROIs) using a bounding box of 96 $\times$ 96. Each frame is normalised by subtracting the mean and dividing by the standard deviation of the training set. For audio streams, we only perform $z$-normalisation per utterance.

\subsection{Implementation details}
For our audio- and visual-only ASR models, we use a ResNet-based front-end module pre-trained on LRW~\cite{ma2020towards}, followed by a Conformer encoder with 12 layers, 768 input dimensions, 3\,072 feed-forward dimensions, and 16 attention heads. The decoder is a 6-layer Transformer with the same dimensions and number of heads as the encoder, resulting in a total of 243.1\,M and 250.4\,M parameters for the audio- and visual-only models, respectively. More specifically, the ASR front-end, VSR front-end, Conformer back-end, Transformer decoder and the projection layer of the CTC have~3.9\,M,~11.2\,M,~170.9\,M,~64.5\,M and~3.9\,M parameters, respectively. For the audio-visual models, we concatenate the audio-visual encoder outputs and feed them to a~2-layer multi-layer perceptron~(MLP) with hidden and output sizes of~8\,192 and~768, respectively. 

For data augmentation, we apply horizontal flipping, random cropping, and adaptive time masking~\cite{ma2022visual} to the visual inputs, while we only use adaptive time masking for the audio stream. For both streams, we choose a number of masks that is proportional to the utterance length and a maximum masking length of up to 0.4 seconds. For the target vocabulary, we use SentencePiece~\cite{DBLP:conf/acl/Kudo18} subword units with a vocabulary size of 5\,000. We train the model for 75 epochs with the AdamW~\cite{loshchilov2019decoupled} optimizer, a cosine learning rate scheduler, and a warm-up of 5 epochs. The peak learning rate is 1e-3. The maximal number of frames in each batch is 1\,800 frames. Following~\cite{ma2022visual}, our visual-only models are incorporated with a transformer-based language model trained on a corpus of 166 million characters~$^\dagger$\let\thefootnote\relax\footnote{$^\dagger$ The corpus consits of the training sets of LibriSpeech ($960$\,h)~\cite{panayotov2015librispeech}, pre-training and training sets of LRS2~\cite{chung2017lip} and LRS3~\cite{afouras2018lrs3}, TED-LIUM 3~\cite{DBLP:conf/specom/HernandezNGTE18}, Voxforge (English) and Common Voice (English)~\cite{DBLP:conf/lrec/ArdilaBDKMHMSTW20}} whereas language models are not included for ASR and AV-ASR models since no further improvements are observed.