\subsection{Leveraging pre-trained models to produce automatically-generated transcriptions for unlabelled audio-visual datasets}
\label{sec:leveraging}
In order to investigate the impact of the size of training data on the performance of audio-only, visual-only and audio-visual models, we scale the training sources by including publicly-available audio-visual clips into the training set.
An overview of our label generation pipeline can be found at the top of Fig.~\ref{fig:av_architecture}. To be specific, audio waveforms from the unlabelled audio-visual datasets are fed into a pre-trained ASR model to produce automatic transcriptions.
For the purpose of this study, we use two unlabelled datasets: VoxCeleb2~\cite{DBLP:conf/interspeech/ChungNZ18} and AVSpeech~\cite{DBLP:journals/tog/EphratMLDWHFR18}. In particular, AVSpeech~\cite{DBLP:journals/tog/EphratMLDWHFR18} contains 4\,700 hours of YouTube video segments in multiple languages, and VoxCeleb2~\cite{DBLP:conf/interspeech/ChungNZ18} consists of 2\,300 hours of video segments from more than 6\,000 people. However, we are interested in training models in English. Thus, we use the VoxLingua107 language classifier~\cite{valk2021slt}
to filter the AVSpeech dataset, resulting in a total of 1\,323 hours; the list of English data we use for VoxCeleb2 is obtained from~\cite{DBLP:journals/corr/abs-2201-02184}, and comprises 1\,307 data hours. Next, we leverage publicly-available ASR models to produce automatically generated transcriptions. It is worth pointing out that our work facilitates reproduction and comparison since all datasets and models used are publicly accessible.

\vspace{-2mm}
\subsection{Automatic speech recognition models}
\label{sec:asr}
We investigate the impact of the automatic transcriptions given by four different ASR models on the performance of audio-only and visual-only models, i.e. Whisper~\cite{radford2018improving}, wav2vec2.0~\cite{baevski2020wav2vec}, Hidden unit BERT (HuBERT)~\cite{hsu2021hubert} and Conformer-Transducer~\cite{DBLP:journals/corr/abs-1909-09577, gulati2020conformer}. In particular, wav2vec\,2.0~\cite{baevski2020wav2vec} and HuBERT~\cite{hsu2021hubert} are self-supervised learning methods for learning speech representations from unlabelled audio. In contrast, Conformer-Transducer~\cite{DBLP:journals/corr/abs-1909-09577} is a conformer-based model trained with recurrent neural network transducers (RNN-T) loss that uses the NeMo ASRSET dataset, consisting of 12\,000 hours of English speech. Finally, Whisper~\cite{radford2018improving} is a transformer-based model~\cite{vaswani2017attention} trained with a total of 680\,000\,hours of labelled audio. In this work, we access the ASR models from the Hugging Face community, ``{nvidia/stt\_en\_conformer\_transducer\_xlarge}'', ``{facebook/hubert-large-ls960-ft}'',
``{facebook/wav2vec2-base-960h}'', and ``{openai/whisper-medium.en}'', respectively.

\vspace{-2mm}
\subsection{Architecture}
\label{sec:architecture}
We adopt the off-the-shelf architecture presented in \cite{DBLP:journals/corr/abs-2102-06657}, which has achieved state-of-the-art performance on the LRS2 and LRS3 datasets without the use of external data. The architecture is shown at the bottom of Fig.~\ref{fig:av_architecture}. In particular, the VSR front-end is based on a modified ResNet-18~\cite{he2016deep, stafylakis2017combining}, where the first layer is a spatio-temporal convolutional layer with a kernel size of 5$\times$7$\times$7 and a stride of 1$\times$2$\times$2. The temporal back-end, which follows the front-end, is a Conformer~\cite{gulati2020conformer}. Similarly, the ASR encoder consists of a~1D ResNet-18~\cite{DBLP:conf/icassp/PetridisSMCTP18} followed by a Conformer. The ASR and VSR encoder outputs are fused via a multi-layer perceptron (MLP). The rest of the network consists of a projection layer and a Transformer decoder for joint CTC/attention training~\cite{watanabe2017hybrid}.