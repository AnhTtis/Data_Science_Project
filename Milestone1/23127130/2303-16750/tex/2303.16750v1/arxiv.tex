\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{csquotes}
\usepackage{mdframed}
\usepackage{tablefootnote}
\usepackage{authblk}

\newcommand{\ns}[1]{{\color{blue} \textbf{ns: #1}}}
\newcommand{\is}[1]{{\color{red} \textbf{is: #1}}}
\newcommand{\gn}[1]{{\color{magenta} \textbf{gn: #1}}}
\newcommand{\jw}[1]{{\color{green} \textbf{jw: #1}}}

\newcommand{\cmu}{Carnegie Mellon University}

\newcommand{\random}{\textsc{Random}}
\newcommand{\constant}{\textsc{Trivial}}
\newcommand{\tpms}{\textsc{TPMS}}
\newcommand{\elmo}{\textsc{ELMo}}
\newcommand{\specter}{\textsc{Specter}}
\newcommand{\mfr}{\textsc{Specter+MFR}}
\newcommand{\acl}{\textsc{ACL}}
\newcommand{\weblink}{\url{https://forms.gle/SP1Rh8eivGz54xR37}}


\renewcommand{\Authands}{ and } 


\title{A Gold Standard Dataset for the Reviewer Assignment Problem}
\author{Ivan Stelmakh, John Wieting, Graham Neubig, Nihar B. Shah\footnote{Corresponding author: \href{mailto:nihars@cs.cmu.edu}{nihars@cs.cmu.edu}}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
    Many peer-review venues are either using or looking to use algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the ``\emph{similarity score}''---a numerical estimate of the expertise of a reviewer in reviewing a paper---and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. 
    The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. 
    \emph{We address this challenge by collecting a novel dataset of similarity scores} that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously. 
    
    We use this data to compare several popular algorithms currently employed in computer science conferences and come up with recommendations for stakeholders. Our three main findings are: 
    \begin{itemize}[leftmargin=20pt, itemsep=0pt, topsep=1pt]
        \item All algorithms make a non-trivial amount of error. For the task of ordering two papers in terms of their relevance for a reviewer, the error rates range from 12\%-30\% in easy cases to 36\%-43\% in hard cases, thereby highlighting the vital need for more research on the similarity-computation problem.
        
        \item Most existing algorithms are designed to work with titles and abstracts of papers, and in this regime the {\sc Specter+MFR} algorithm performs best.

        \item To improve performance, it may be important to develop modern deep-learning based algorithms that can make use of the full texts of papers:  the classical TD-IDF algorithm enhanced with full texts of papers is on par with the deep-learning based {\sc Specter+MFR} that cannot make use of this information.
    \end{itemize}
We encourage researchers to use this dataset for developing and evaluating better similarity-computation algorithms. 
\end{abstract}



\section{Introduction}

Assigning papers to reviewers with appropriate expertise is the key prerequisite for high-quality reviews \citep{thurner2011peer, black1998makes, bianchi2015three}. Even a small fraction of incorrect reviews can negatively impact the quality of the published scientific standard~\citep{thurner2011peer} and hurt careers of researchers~\citep{merton1968matthew, squazzoni2012saint, thorngate2014numbers}. Quoting~\citet{triggle07future}:
\begin{quote}
    \emph{``An incompetent review may lead to the rejection of the submitted paper, or of the grant application, and the ultimate failure of the career of the author.''}
\end{quote}

Conventionally, the selection of reviewers for a submission was the task of a journal editor or a program committee of a conference. However, the rapid growth in the number of submissions to many publication venues~\citep{shah2021survey} has made the manual selection of reviewers extremely challenging. As a result, many peer-review venues in computer science as well as other fields are either using or looking to use algorithms to assist organizers in assigning submissions to reviewers~\citep{Garg2010papers, charlin13tpms, kobren19localfairness, kerzendorf2020distributed, stelmakh21pr4a, OpenReview2022Similarities}.

The key component of existing automated approaches for assigning reviewers to submissions is the ``\textit{similarity score}''. For any reviewer-submission pair, the similarity score is a number that captures the expertise of the reviewer in reviewing the submission. The assignment process involves first computing the similarity score for each submission-reviewer pair, and then using these scores to assign the reviewers to submissions~\cite[Section 3]{shah2021survey}. The similarity scores are typically computed by matching the text of the submission with the profile (e.g., past papers) of the reviewer, which is the primary focus of this paper. Additionally, these scores may also be augmented by manually selected subject areas or reviewer-provided manual preferences (``bids'').

Several algorithms for computing similarity scores have been already proposed and used in practice (we review these algorithms in Sections~\ref{section:relatedwork} and~\ref{section:expsetup}). However, there are two key challenges in designing and using such algorithms in a principled manner. First, there is no publicly available gold standard data that can be used for algorithm development. Second, despite the existence of many similarity-computation algorithms, the absence of the gold standard data prevents principled comparison of these algorithms. As a result, three flagship machine learning conferences---ICML, NeurIPS, and ACL---rely on three different similarity-computation algorithms, and the differences in performance of these algorithms are not well understood. 

In this work, \emph{we address the aforementioned challenges and collect a dataset of reviewers' expertise that can facilitate the progress in the reviewer assignment problem}. Specifically, we conduct a survey of computer science researchers whose experience level ranges from graduate students to senior professors. In the survey, we ask participants to report their expertise in reviewing computer science papers they read over the last year. 


\paragraph{Contributions} Overall, our contributions are threefold:
\begin{itemize}[leftmargin=20pt, itemsep=0pt, topsep=1pt]
    \item First, we collect and release a high-quality dataset of reviewers' expertise that can be used for training and/or evaluation of similarity-computation algorithms. The dataset can be found on the project's GitHub page:\vspace{-7pt}
    \begin{center}
\url{https://github.com/niharshah/goldstandard-reviewer-paper-match}
    \end{center}    
    
    \item Second, we use the collected dataset to compare existing similarity-computation algorithms and inform organizers in making a principled choice for their venue. Specifically, we observe that when all algorithms operate with titles and abstracts of papers, the most advanced \mfr{} algorithm performs best. However, when the much simpler  \tpms{} is additionally provided with full texts of papers, it achieves the same level of performance as \mfr{}.
    
    \item Third and finally, we conduct an exploratory analysis that highlights areas of improvement for existing algorithms and our insights can be used to develop better algorithms to improve peer review. For example, we believe that an important direction is to develop deep-learning algorithms that employ full texts of papers to improve performance.
\end{itemize}

\smallskip

\noindent Overall, we observe that all current algorithms exhibit non-trivial amounts of error. When tasked to compare a pair of papers in terms of the expertise of a given reviewer, all algorithms make 12\%-30\% mistakes even when two papers are selected to have a large difference in reviewer's expertise. On a more challenging task of comparing two high-expertise papers, the algorithms err with probability 36\%-43\%. This observation underscores the vital need to design better algorithms for matching reviewers to papers.


\medskip

Let us now make two important remarks. First, while our dataset comprises researchers and papers from computer science and closely-related fields, other communities may also use it to evaluate existing or develop new similarity-computation algorithms. To evaluate an algorithm from another domain on our data, researchers can fine-tune their algorithm on profiles of computer science scientists crawled from Semantic Scholar and then evaluate it on our dataset.

Second, in this work we release an \emph{initial} version of the dataset consisting of 477 data points contributed by 58 researchers. Thus, our dataset is not set in stone and we encourage researchers to participate in our survey and contribute their data to the dataset. By collecting more samples, we enable more fine-grained comparisons and also improve the diversity of the dataset in terms of both population of participants and subject areas of papers. The survey is available at:\vspace{-3pt}
\begin{center}
    \weblink
\end{center}
and we will be updating the released version regularly.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related literature}
\label{section:relatedwork}

In this section, we discuss relevant past studies. We begin with an overview of works that report comparisons of different similarity-computation algorithms. We then provide a brief discussion of the commonly used procedure for computing similarity scores, which are ultimately utilized to assign reviewers to submissions. Finally, we conclude with a list of works that design algorithms to automate other aspects of reviewer assignment.


\paragraph{Evaluation of similarity-computation algorithms} The OpenReview platform~\citep{OpenReview2022Similarities} uses an approach of predicting authorship as a proxy to measuring the quality of the similarity scores computed by any algorithm. Specifically, they consider papers authored by a number of researchers, remove one of these papers from the corpus, and predict expertise of each researcher in reviewing the selected paper. The performance of an algorithm then is measured as a fraction of times the author of the selected paper is predicted to be among the top reviewers for this paper. This authorship proxy, however, may not be representative of the actual task of similarity computation as algorithms that accurately predict the authorship relationship (and hence do well according to this approach) are not guaranteed to accurately estimate expertise in reviewing submissions authored by other researchers.


\citet{mimno07topicbased} collected a dataset with external expertise judgments. Specifically, they used 148 papers accepted to the NeurIPS 2006 conference and 364 reviewers from the NeurIPS 2005 conference and ask human annotators -- independent established researchers -- to evaluate expertise for a selected subset of 650 (paper, reviewer) pairs. While this approach results in a publicly available dataset, we note that external expertise judgments may also be noisy as judges may have incomplete information about the expertise of reviewers.


\citet{dumais1992automating}, \citet{rodriguez08coauthorsip} and \citet{anjum19pare} obtain more accurate expertise judgments by relying on self reports of expertise evaluations from reviewers. In more detail, \citet{dumais1992automating} and \citet{rodriguez08coauthorsip} rely on \emph{ex-ante} bids---preferences of reviewers in reviewing submissions made in advance of reviewing. In contrast, \citet{anjum19pare} rely on \emph{ex-post} evaluations of expertise made by reviewers after reviewing the submissions. These works construct datasets that can be used to evaluate algorithms: \citet{dumais1992automating} employ 117 papers and 15 reviewers, \citet{rodriguez08coauthorsip} employ 102 papers and 69 reviewers and \citet{anjum19pare} employ 20 papers and 33 reviewers. However, \citet{rodriguez08coauthorsip} and \citet{anjum19pare} use sensitive data that cannot be released without compromising the privacy of reviewers. Furthermore, \emph{ex-ante} evaluations of~\citet{dumais1992automating} and~\citet{rodriguez08coauthorsip} may not have a high accuracy as (i) bids may contaminate expertise judgments with \emph{willingness} to review submissions, and (ii) bids are based on a very brief acquaintance with papers. On the other hand, \emph{ex-post} data by~\citet{anjum19pare} is collected for papers assigned to reviewers using a specific similarity-computation algorithm. Thus, while collected evaluations have high precision, they may also have low recall if the employed similarity-computation algorithm erroneously assigned low expertise scores to some (paper, reviewer) pairs as evaluations of expertise for such papers were not observed.


In this work, we collect a novel dataset of reviewer expertise that (i) can be released publicly, and (ii) contains accurate self-evaluations of expertise that are based on a deep understanding of the paper and are not biased towards any existing similarity-computation algorithm.


\paragraph{Similarity scores in conferences} In modern conferences, similarity scores are typically computed by combining two types of input:
\begin{itemize}[leftmargin=20pt, itemsep=0pt, topsep=5pt]
    \item \emph{Initial automated estimates.} First, a similarity-computation algorithm is used to compute initial estimates. Many algorithms have been proposed for this task~\citep{mimno07topicbased, rodriguez08coauthorsip, charlin13tpms, liu14graphpropagation, tran17expertsuggestion, anjum19pare,kerzendorf2020distributed, OpenReview2022Similarities} and we provide more details on several algorithms used in flagship computer science conferences in Section~\ref{section:expsetup}.

    \item \emph{Human corrections.} Second, automated estimates are corrected by reviewers who can read abstracts of submissions and report bids---preferences in reviewing the submissions. A number of works focus on the bidding stage and (i) explore the optimal strategy to assist reviewers in navigating the pool of thousands of submissions~\citep{fiez2019super, meir2020market} or (ii) protect the system from strategic bids made by colluding reviewers willing to get assigned to each other's paper~\citep{jecmen2020manipulation, ruihan21making,boehmer2021combating,jecmen22dataset}.
\end{itemize}

Combining these two types of input in a principled manner is a non-trivial task. As a result, different conferences use different strategies~\citep{shah2017design,leyton2022matching} and there is a lack of empirical or theoretical evidence that would guide venue organizers in their decisions.

\paragraph{Automation of the assignment stage} At a high level, automated assignment stage consists of two steps: first, similarity scores are computed; second, reviewers are allocated to submissions such that some notion of assignment quality (formulated in terms of the similarity scores) is maximized. In this work, we focus on the first step of the process. However, for completeness, we now mention several works that design algorithms for the second step.

A popular notion of assignment quality is the cumulative similarity score, that is, the sum of the similarity scores across all assigned reviewers and papers. An algorithm pursuing such an objective is implemented in the widely employed TPMS assignment algorithm~\citep{charlin13tpms} and similar ideas are explored in many papers~\citep{goldsmith07aiconf, tang10constraied, Long13gooadandfair}. While the cumulative objective is a natural choice, it has been noted that it may discriminate against certain submissions by allocating all irrelevant reviewers to a subset of submissions, even when a more balanced assignment exists~\citep{Garg2010papers}. Thus, a number of works has explored the idea of assignment fairness, aiming at producing more balanced assignments~\citep{kobren19localfairness, stelmakh21pr4a}. Finally, other works explore the ideas of envy-freeness~\citep{tan21envy, payan22fair}, resistance to lone-wolf strategic behavior~\citep{xu2018strategyproof,dhull2022price}, and encouraging various types of diversity~\citep{Li15concert,leyton2022matching}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data collection pipeline}
\label{section:pipeline}

In this section, we describe the process of data collection. This work was performed under the approval of the Institutional Review Board (IRB) of Carnegie Mellon University.

\paragraph{Gold standard data} In this study, we aim to collect a dataset of self-evaluations of reviewers' expertise that satisfies two desiderata:
\begin{enumerate}[leftmargin=25pt, itemsep=0pt, topsep=1pt, label={(D\arabic*})]
    \item The dataset should comprise evaluations of expertise for papers participants read to a reasonable extent.\label{desiderata:one}
    \item  The dataset should be released publicly without disclosing any sensitive information.\label{desiderata:two} 
\end{enumerate}


\smallskip

Let us now discuss our approach to recruiting participants and obtaining accurate estimates of their expertise in reviewing papers included in the dataset. 

\paragraph{Participant recruiting} We recruited participants using a combination of several channels that are typically employed to recruit reviewers for computer science conferences:
\begin{itemize}[leftmargin=20pt, itemsep=0pt, topsep=5pt]
    \item \textit{Mailing lists.} First, we sent recruiting emails to relevant mailing lists of several universities and research departments of companies

    \item \textit{Social media.} Second, two authors of this paper posted a call for participation on their Twitter accounts
    
    \item \textit{Personal communication.} Third, we sent personal invites to researchers from our professional network
\end{itemize}

We had a screening criterion requiring that prospective participants have at least one paper published in the broad area of computer science. Overall, for the version of the dataset we release in this paper, we managed to recruit 58 participants, all of whom passed the screening.

\paragraph{Expertise evaluations} The key idea of our approach to expertise evaluation is to ask participants to \emph{evaluate their expertise in reviewing papers they read in the past}. After reading a paper, a researcher is in the best possible position to evaluate whether they have the right background---both in terms of the techniques used in the paper and in terms of the broader research area of the paper---to judge the quality of the paper. With this motivation, we asked participants to:
\begin{quote}
    \textit{Recall 5-10 papers in their broad research area that they read to a reasonable extent in the last year and tell us their expertise in reviewing these papers.}
\end{quote}
%
In more detail, the choice of papers was constrained by two minor conditions:
\begin{itemize}[leftmargin=20pt, itemsep=0pt, topsep=5pt]
    \item The papers reported by a participant should not be authored by them 
    \item The papers reported by a participant should be freely available online
\end{itemize}

In addition to these constraints, we gave several recommendations to the participants in order to make the dataset more diverse and useful for the research purposes:

\begin{itemize}[leftmargin=20pt, itemsep=0pt, topsep=5pt]
    \item First, we asked participants to choose papers that cover the whole spectrum of their expertise with some papers being well-separated (e.g., very high expertise and very low expertise) and some papers being nearly-tied (e.g., two high-expertise papers).
    
    \item Second, we recommended participants to avoid ties in their evaluations. To help participants comply with this recommendation, we implemented evaluation on a scale of 1 (``I am not qualified to review this paper'') to 5 (``I have background necessary to evaluate all the aspects of the paper'') with a 0.25 step size, enabling participants to report papers with small differences in expertise.

    \item Third, we asked participants to come up with papers that they think may be tricky for existing similarity-computation algorithms. For this, we relied on the commonsense understanding and did not instruct participants on the inner-workings of these algorithms.
\end{itemize}

Overall, the time needed for participants to contribute to the dataset was estimated to be 5--10 minutes. The full instructions of the survey are available in Appendix~\ref{appendix:survey}.



\paragraph{Data release} Following the procedure outlined above, we collected responses from 58 researchers. These responses constitute an initial version of the dataset that we release in this work. Each entry in the dataset corresponds to a participant and comprises evaluations of their expertise in reviewing papers of their choice. For each paper and each participant, we provide representations that are sufficient to start working on our dataset:

\begin{itemize}[itemsep=0pt, leftmargin=20pt, topsep=5pt]
    \item \textit{Participant.} Each participant is represented by their Semantic Scholar ID and complete bibliography crawled from Semantic Scholar on May 1, 2022.
    
    \item \textit{Paper.} Each paper, including papers from participants' bibliographies, is represented by its Semantic Scholar ID, title, abstract, list of authors, publication year, and arXiv identifier. Additionally, papers from participants' responses are supplied with links to freely available PDFs (whenever available).
\end{itemize}

\section{Data exploration}
\label{section:exploration}


In this section we explore the collected data and present various characteristics of the dataset. The subsequent sections then detail the results of using this data to benchmark various popular similarity-computation algorithms.

\subsection{Participants} 
\label{section:exploration:participants}

We begin with a discussion of the pool of the survey participants: Table~\ref{table:participants_demo} displays its key characteristics. First, we note that all participants work in the broad area of computer science and have rich publication profiles (at least two papers published, with the mean of 54 papers and the median of 20 papers). In many subareas of computer science, including machine learning and artificial intelligence, having two papers is usually sufficient to join the reviewer pool of flagship conferences. Given that approximately 85\% of participants either have PhD or are in the process of getting the degree, we conclude that most of the researchers who contributed to our dataset are eligible to review for machine learning and artificial intelligence conferences.

Second, we caveat that most of the participants are male researchers affiliated with US-based organizations, with about 40\% of all participants being affiliated with \cmu. Hence, the population of participants is not necessarily representative of the general population of the machine learning and computer science communities. We encourage researchers to be aware of this limitation when using our dataset. We note that the data collection process does not conclude with the publication of the present paper and we will be updating the dataset as new responses come.  We also encourage readers to contribute 5--10 minutes of their time to fill out the survey \weblink{} and make the dataset more representative.

\begin{table}[t]
\begin{center}
\textsc{Total number of participants: 58} \medskip \\
\begin{small}
\begin{sc}
\begin{tabular}{llr}
\toprule
 Characteristic &  Quantity & Value \\
\midrule
Gender & \% Male & 78 \\ 
\midrule
{Affiliation} & \% \cmu & 40 \\
\midrule
Country & \% USA & 74 \\
\midrule
\multirow{3}{*}{Position} & \% PhD student & 45 \\
 & \% Faculty &  28 \\
 & \% Post-PhD (non-faculty) &  12  \\
\midrule
\multirow{4}{*}{Experience} & Min \# publications & 2 \\
 & Max \# publications & 492 \\
 & Mean \# publications & 54 \\
 & Median \# publications & 20 \\

\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\caption{Demography of participants. For the first four characteristics, quantities represent percentages of the one or more most popular classes in the dataset. Note that classification is done manually based on publicly available information and may not be free of error. For the last characteristic, quantities are computed based on Semantic Scholar profiles.}
\label{table:participants_demo}
\end{table}


\subsection{Papers}
\label{section:exploration:papers}

We now describe the set of papers that constitute our dataset. Overall, participants evaluated their expertise in reviewing 463 unique papers. Out of these 463 papers, 12 papers appeared in reports of two participants, 1 paper was mentioned by three participants, and the remaining papers were mentioned by one participant each.

Table~\ref{table:papers_demo} presents several characteristics of the pool of papers in our dataset. First, we note that all but one of the papers are listed on Semantic Scholar, enabling similarity-computation algorithms developed on the dataset to query additional information about the papers from the Semantic Scholar database. For the paper that does not have a Semantic Scholar profile, we construct such a profile manually and keep this paper in the dataset.  Additionally, most of the papers (99\%) have their PDFs freely available online, thereby allowing algorithms to use full texts of papers to compute similarities.

Semantic Scholar has a built-in tool to identify broad research areas of papers~\citep{wade2022semantic}. According to this tool, 99\% of the papers included to our dataset belong to the broad area of computer science---the target area for our data-collection procedure. The remaining four papers belong to the neighboring fields of mathematics, philosophy, and the computational branch of biology. Finally, approximately 75\% of all papers in our dataset are published in or after 2020, ensuring that our dataset contains recent papers that similarity-computation algorithms encounter in practice.

\begin{table}[t]
\begin{center}
\textsc{Total number of papers: 463} \medskip \\
\begin{small}
\begin{sc}
\begin{tabular}{llr}
\toprule
 Characteristic &  Quantity & Value \\
\midrule
\multirow{3}{*}{Open Access} & \# On semantic scholar & 462 \\ 
& \# On arXiv & 411 \\ 
& \# PDF available & 457 \\ 
\midrule
Research Areas & \# Computer science & 459 \\
\midrule
\multirow{2}{*}{Publication Year} & \% Before 2020 & 25 \\
& \% 2020 or later & 75 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\caption{Characteristics of the 463 papers in the released dataset. Most of the papers are freely available online and belong to the broad area of computer science.}
\label{table:papers_demo}
\end{table}

\subsection{Evaluations of expertise}

\begin{figure*}[b]
\centering
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{Figures/marginal_expertise.pdf}
  \caption{Histogram of expertise values.}
  \label{fig:experise:marginal}
\end{subfigure}\hfill%
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{Figures/difference_expertise.pdf}
  \caption{Histogram of differences in expertise evaluations.}
  \label{fig:experise:diff}
\end{subfigure}
\caption{Distribution of expertise scores reported by participants.}
\label{fig:experise}
\end{figure*}

Finally, we proceed to the key aspect of our dataset---evaluations of expertise in reviewing the papers reported by participants. All but one participant reported expertise in reviewing at least 5 papers with the mean number of papers per participant being 8.2 and the total number of expertise evaluations being 477.

Figure~\ref{fig:experise} provides visualization of expertise evaluations made by participants. First, Figure~\ref{fig:experise:marginal} displays the histogram of expertise values. About two-thirds of the reported papers belong to the expertise area of participants (expertise score larger than 3), and about one-third evaluations are made for papers that reviewers are not competent in (expertise score 3 or lower), thereby yielding a good spread. 

Second, Figure~\ref{fig:experise:diff} shows the distributions of pairwise differences in expertise evaluations made by the same reviewer. To construct this figure, for each participant we considered all pairs of papers in their report. Next, we pooled the absolute values of the pairwise differences in expertise across participants. We then plotted the histogram of these differences in the figure. Observe that the distribution in Figure~\ref{fig:experise:diff} has a heavy tail, suggesting that our dataset is suitable for evaluating the accuracy of similarity-computation methods both at a coarse level (large differences between the values of expertise) and at a fine level (small differences between the values of expertise).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental setup}
\label{section:expsetup}

We now describe the setup of experiments on our dataset.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Metric}
\label{section:setup:metric}

\newcommand{\participant}{r}
\newcommand{\paperset}{\mathcal{P}}
\newcommand{\paper}{p}
\newcommand{\expertiseset}{\mathcal{E}}
\newcommand{\expertise}{\varepsilon}
\newcommand{\similarityset}{\mathcal{S}}
\newcommand{\similarity}{s}
\newcommand{\numpapers}{m}
\newcommand{\loss}{L}
\newcommand{\indicator}{\mathbb{I}}

We begin with defining a main metric that we use in this work to evaluate performance of the algorithms. For this, we rely on the Kendall's Tau distance that is closely related to the widely used Kendall's Tau~\citep{kendall1938new} rank correlation coefficient. We introduce the metric and the algorithms in this section, followed by the results in Section~\ref{section:results}. 
Subsequently in Section~\ref{section:section:easyhard}, we provide additional evaluations separating out hard and easy instances.

\paragraph{Intuition} Before we introduce the metric in full details, let us provide some intuition behind it. Consider a pair of papers and a participant who evaluated their expertise in reviewing these papers. We say that a similarity-computation algorithm makes an error if it fails to correctly predict the paper for which the participant reported the higher value of expertise.

Of course, some pairs of papers are harder to resolve than others (e.g., it is harder to resolve papers with expertise scores 4.0 and 4.25 than 4.0 and 1.0). To capture this intuition, whenever an algorithm makes an error, we penalize it by \emph{the absolute difference in the expertise values reported by the participant}. Overall, the loss of the algorithm is the sum of losses across all pairs of papers evaluated by participants normalized to take values between 0 and 1 (the lower the better).

\paragraph{Formal definition} The intuition behind the metric that we have just introduced should be sufficient to interpret the results of our study so the reader may skip the rest of this section and move directly to Section~\ref{section:setup:algorithms}. However, for the sake of rigor, we now introduce our metric more formally.

Consider any algorithm that produces real-valued predictions of reviewers' expertise in reviewing a given set of papers. We call these predictions the ``similarity scores'' given by the algorithm. We assume that a higher value of a similarity score means that the algorithm  predicts a better expertise. The metric we define below is agnostic to the range or exact values of predicted similarity scores; it only relies on the relative values across different papers.

Now consider any participant $\participant$ in our study. We let $\numpapers_\participant$ denote the number of papers for which participant $\participant$ reports their expertise. We let $\paperset_{\participant} = \{\paper_{\participant}^{(1)}, \paper_{\participant}^{(2)}, \ldots, \paper_{\participant}^{(\numpapers_\participant)}\}$ denote this set of $\numpapers_\participant$ papers. For every $i \in \{1,\ldots,\numpapers_\participant\}$, we let $\expertise_{\participant}^{(i)}  \in \left\{1, 1.25, 1.5, \ldots, 5\right\}$ denote the expertise self-reported by participant $\participant$ for paper $\paper_{\participant}^{(i)}$. Next, for every $i \in \{1,\ldots,\numpapers_\participant\}$, we let $\similarity_{\participant}^{(i)}$ denote the real-valued similarity score given by the algorithm to the pair (reviewer $\participant$, paper $\paper_{\participant}^{(i)}$). 

Having set up this notation, we now define the `unnormalized' loss of the algorithm with respect to participant $\participant$ as:

\begin{align*}
    \loss_{\participant} \! = \! &\sum\limits_{\substack{i, j = 1 \\ i < j}}^{\numpapers_\participant} \! \Bigg( \! \underbrace{\indicator \left\{ (\similarity_{\participant}^{(i)} \! - \similarity_{\participant}^{(j)}) \! \times \! (\expertise_{\participant}^{(i)} \! - \expertise_{\participant}^{(j)}) < 0 \right\}}_{\text{error}} \! \times \big| \expertise_{\participant}^{(i)} \! - \expertise_{\participant}^{(j)} \big| + \underbrace{\indicator \left\{ (\similarity_{\participant}^{(i)} \! - \similarity_{\participant}^{(j)}) \! \times \! (\expertise_{\participant}^{(i)} \! - \expertise_{\participant}^{(j)}) = 0 \right\}}_{\text{tie}} \! \times  \frac{1}{2} \big|  \expertise_{\participant}^{(i)} - \expertise_{\participant}^{(j)}  \big| \! \Bigg).
\end{align*}


In words, for each pair of papers $(\paper_{\participant}^{(i)}, \paper_{\participant}^{(j)})$ reported by participant $\participant$, the algorithm is not penalized when the ordering of papers induced by the similarity scores $\{\similarity_{\participant}^{(i)}, \similarity_{\participant}^{(j)}\}$ agrees with the ground truth expertise-based ordering $\{\expertise_{\participant}^{(i)}, \expertise_{\participant}^{(j)}\}$. When two orderings disagree (that is, the algorithm makes an error), the algorithm is penalized by the difference of expertise reported by the participant $( |\expertise_{\participant}^{(i)} - \expertise_{\participant}^{(j)} | )$. Finally, when scores computed by the algorithm indicate a tie while expertise scores are different, the algorithm is penalized by half the difference in expertise $(\nicefrac{1}{2} |\expertise_{\participant}^{(i)} - \expertise_{\participant}^{(j)} | )$.

Having the unnormalized loss with respect to a participant defined, we now compute the overall loss $\loss \in [0, 1]$ 
of the algorithm. For this, we take the sum of unnormalized losses across all participants and normalize this sum by the loss achieved by the adversarial algorithm that reverses the ground-truth ordering of expertise (that is, sets $\similarity = -\expertise$) and achieves the worst possible performance on the task. More formally,

\begin{align*}
    \loss = \frac{\sum\limits_{\participant} \loss_\participant}{\sum\limits_{\participant} \sum\limits_{\substack{i, j = 1 \\ i < j}}^{\numpapers_\participant} \big |\expertise_{\participant}^{(i)} - \expertise_{\participant}^{(j)} \big |}.
\end{align*}
Overall, our loss $\loss$ takes values from $0$ to $1$ with lower values indicating better performance. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Algorithms}
\label{section:setup:algorithms}

In this work, we evaluate several algorithms that we now discuss. All of these algorithms operate with (i) the list of submissions for which similarity scores need to be computed and (ii) reviewers' profiles comprising past publications of reviewers. Conceptually, all algorithms predict reviewers' expertise by evaluating textual overlap between each submission and papers in each reviewer's publication profile. Let us now provide more detail on how this idea is implemented in each of the algorithms under consideration.

\paragraph{Trivial baseline} First, we consider a trivial baseline that ignores the content of submissions and reviewers' profiles when computing the assignment scores: for each (participant, paper) pair, the \constant{} algorithm predicts the score $\similarity$ to be 1.

\paragraph{Toronto Paper Matching System (TPMS)} The \tpms{} algorithm~\citep{charlin13tpms}, which is based on TF-IDF similarities, is widely used by flagship conferences such as ICML and AAAI. While an exact implementation is not publicly available, in our experiments we use an open-source version by~\citet{xu2018strategyproof} which implements the basic TF-IDF logic of \tpms{}. We also note that this method is rediscovered independently by~\citet{kerzendorf2020distributed}. 

As a technical remark, in our implementation we use reviewers' profiles and all reported papers to compute the IDF part of the TF-IDF model. In principle, one may be able to get a better performance by using a larger set of papers from the respective field (e.g., all submissions to the last edition of the conference) to compute IDF.

\paragraph{OpenReview algorithms} OpenReview is a conference-management system used by machine learning conferences such as NeurIPS and ICLR. It offers a family of deep-learning algorithms for measuring expertise of reviewers in reviewing submissions. In this work, we evaluate the following algorithms which are used to compute affinity scores between submissions and reviewers:

\begin{itemize}
    \item \elmo{}. This algorithm relies on general-purpose \textbf{E}mbeddings from \textbf{L}anguage \textbf{Mo}dels~\citep{peters18deep} to compute textual similarities between submissions and reviewers' past papers.
    
    \item \specter{}. This algorithm employs more specialized document-level embeddings of scientific documents~\citep{cohan2020specter}. Specifically, \specter{} explores the citation graph to construct embeddings that are useful for a variety downstream tasks, including the similarity computation we focus on in this work.
    
    \item \mfr{}. Finally, \mfr{} further enhances \specter{}~\citep{chang2021cold}. Instead of constructing a single embedding of each paper, it constructs multiple embeddings that correspond to different facets of the paper. These embeddings are then used to compute the similarity scores.
\end{itemize}

We use implementations of these methods that are available on the OpenReview GitHub page\footnote{\url{https://github.com/openreview/openreview-expertise}} and execute them with default parameters.
 
\paragraph{ACL paper matching} The Association for Computational Linguistics (ACL) is a community of researchers working on computational problems involving natural language. The association runs multiple publication venues, including the flagship ACL conference, and has its own method to compute expertise between papers and reviewers.\footnote{\url{https://github.com/acl-org/reviewer-paper-matching}} 
This algorithm is trained on 45,309 abstracts of papers found in the ACL anthology (\url{https://aclanthology.org}) downloaded in November 2019. The key idea of the model training is to split each abstract into two non-contiguous parts and treat two parts of the same abstract as a positive example and highly similar parts of different abstracts as negative examples. The algorithm uses ideas from the work of~\citet{wieting-etal-2019-simple,wieting-etal-2022-paraphrastic} to learn a simple and efficient similarity function that separates positive examples from negative examples. This function is eventually used to compute similarity scores between papers and reviewers. More details on the \acl{} algorithm are provided in Appendix~\ref{section:acl}.

We note that the \acl{} algorithm is trained on the domain of computational linguistics and hence may suffer from a distribution shift when applied to the general machine learning domain. That said, in line with \elmo{}, \specter{}, and \mfr{}, we use the \acl{} algorithm without additional fine-tuning. 

The algorithm used by ACL is developed by a subset of authors of this paper (JW, GN) and we use this implementation in our experiments. Importantly, we note that JW and GN executed the algorithm without having access to the ground truth expertise, and the predicted similarities were independently evaluated by another author of this paper (IS).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

\label{section:results}

In this section, we report the results of evaluation of algorithms described in Section~\ref{section:setup:algorithms}. First, we juxtapose all algorithms on our data (Section~\ref{section:results:comparison}). Second, we use the \tpms{} algorithm to explore various aspects of the similarity-computation problem (Section~\ref{section:results:exploration}).

\subsection{Comparison of the algorithms}
\label{section:results:comparison}

Our first set of results compares the performance of the existing similarity-computation algorithms. To run these algorithms on our data, we need to make some modeling choices faced by conference organizers in practice:
\begin{itemize}[leftmargin=20pt, itemsep=0pt, topsep=5pt]
    \item \textit{Paper representation.} First, in their inner-workings, similarity-computation algorithms operate with some representation of the paper content. Possible choices of representations include: (i) title of the paper, (ii) title and abstract, and (iii) full text of the paper. We choose option (ii) as this option is often used in real conferences and is supported by all algorithms we consider in this work. Thus, to predict expertise, algorithms are provided with the title and abstract of each paper (both papers submitted to the conference and papers in reviewers' publication profiles).
    
    \item \textit{Reviewer profiles.} The second important parameter is the choice of papers to include in reviewers' profiles. In real conferences, this choice is often left to reviewers who can manually select the papers they find representative of their expertise. In our experiments, we construct reviewer profiles automatically by using the 20 most recent papers from their Semantic Scholar profiles. If a reviewer has less than 20 papers published, we include all of them in their profile. Our choice of the reviewer profile size is governed by the observation that the mean length of the reviewer profile in the NeurIPS 2022 conference is 16.5 papers. By setting the maximum number of papers to 20, we achieve the mean profile length of 14.8, thereby operating with the amount of information close to that available to algorithms in real conferences. 
\end{itemize}


\paragraph{Statistical aspects} To build reviewer profiles, we use publication years to order papers by recency, where we break ties uniformly at random. Thus, the content of reviewer profiles depends on randomness. To average this randomness out, we repeat the procedure of profile construction and similarity prediction 10 times, and report the mean loss over these iterations. That said, we note that the observed variability due to the randomness in the construction of reviewer profiles is negligible, with a standard deviation over all iterations is less than 0.005.

The pointwise performance estimates obtained by the procedure above depend on the selection of participants who contributed to our dataset. To quantify the associated level of uncertainty, we compute 95\% confidence intervals as follows. For 1,000 iterations, we create a new \emph{reviewer pool} by sampling participants with replacement and recomputing the loss of each algorithm on the bootstrapped set of reviewers. To save computation time, we do not reconstruct reviewer profiles for each of these iterations as the uncertainty associated with the construction of reviewer profiles is small. Instead, we reuse profiles constructed to obtain pointwise estimates.

Finally, we also build confidence intervals for the \emph{difference} in the performance of the algorithms. We do so in addition to the aforementioned procedure since even when the losses of the algorithms fluctuate with the choice of the bootstrapped dataset, the relative difference in performance of a pair of algorithms may be stable. Specifically, we use the procedure above to build confidence intervals for the difference in performance between the \tpms{} algorithm and each of the more advanced algorithms. \tpms{} is chosen as a baseline for this comparison due to its simplicity.

\medskip

\paragraph{Results of the comparison} Table~\ref{table:results_main} displays results of the comparison. The first pair of columns presents the loss of each algorithm on our dataset and the associated confidence intervals. The third and the forth columns investigate the relative difference in performance between the non-trivial algorithms: they display the differences in performance between \tpms{} and each of the more advanced algorithms (OpenReview algorithms and \acl{}) together with the associated confidence intervals. We make several important observations from Table~\ref{table:results_main}:

\begin{table*}[t]
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
Algorithm &  {Loss} & {95\% CI for Loss} & {$\Delta$ with \tpms{}} & { 95\% CI for $\Delta$} \\
\midrule
\constant{} & $0.50$ & --- & --- & ---\\ 
\midrule
\tpms{} & $0.28$ & $[0.23, 0.33]$ & --- & --- \\
\midrule
\elmo{} & $0.34$ & $[0.29, 0.40]$ & $+0.06$ & $[0.00, 0.13]$ \\
\specter{} & $0.27$ & $[0.21, 0.34]$ & $-0.01$ & $[-0.06, 0.04]$ \\
\mfr{} & \textbf{0.24} & $[0.18, 0.30]$ & $-0.04$ & $[-0.09, 0.01]$ \\
\midrule
\acl{} & $0.30$ & $[0.25, 0.35]$ & $+0.01$ & $[-0.03, 0.06]$\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\caption{Comparison of similarity-computation algorithms on the collected data. All algorithms operate with reviewer profiles consisting of the 20 most recent papers and use titles and abstracts of papers. Lower values of loss are better. A positive (respectively, negative) value of $\Delta$ indicates that the algorithm performs worse (respectively, better) than \tpms{}.${}^{*}$\\ \begin{footnotesize}
${}^{\ast}$In Table~\ref{table:results_explore} we evaluate \tpms{} algorithm when full texts of papers are additionally provided and observe that \tpms{} closes the gap with \mfr{} when using this information. 
\end{footnotesize}
}
\label{table:results_main}
\end{table*}

\begin{itemize}[leftmargin=20pt, itemsep=0pt, topsep=5pt]
    \item First, we note that all algorithms we consider in this work considerably outperform the \constant{} baseline, confirming that content of papers is useful in evaluating expertise. 
    
    \item Second, comparing three algorithms from the OpenReview toolkit, we note that \mfr{} and \specter{} outperform \elmo{}. The former two algorithms rely on domain-specific embeddings\footnote{\mfr{} and \specter{} are initialized with SciBERT~\cite{beltagy-etal-2019-scibert}} while \elmo{} uses general-purpose embeddings. Thus, the nature of the text similarity computation task in the academic context may be sufficiently different from that in other domains.

    \item Third, we note that under modeling choices (paper representation and length of reviewers' profiles) that mimic choices that have been made in real conferences, the {\sc Specter+MFR} algorithm performs best. 
    
    \item The fourth finding is the most surprising. Observe that \tpms{} algorithm is much simpler than other non-trivial algorithms: in contrast to \elmo{}, \specter{}, \mfr{}, and \acl{}, it does not rely on carefully learned embeddings. However, \tpms{} is competitive against complex \specter{} and \mfr{} and even outperforms \elmo{} and \acl{}. Moreover, \tpms{} is the only algorithm that can make use of the full text of the papers. In Section~\ref{section:results:exploration}, we find that when \tpms{} uses the full text of all papers (including reviewers' past papers), it closes the gap with \mfr{}.

    Note that the performance of advanced algorithms could be improved by fine-tuning in a dataset-specific manner, or by leveraging larger pre-trained models like T5~\citep{raffel2020exploring} with added fine-tuning in the scientific domain. However, our observation suggests that the off-the-shelf performance of the best algorithm (\mfr{}) is comparable to that of much simpler \tpms{} algorithm.


\end{itemize}

Finally, we note that some of the confidence intervals for the performance of the different algorithms, as well as for the relative differences, are overlapping. It is, therefore, crucial to increase the size of our dataset to enable more fine-grained comparisons between the algorithms.

\subsection{The role of modeling choices}
\label{section:results:exploration}


In the beginning of Section~\ref{section:results:comparison} we made two modeling choices pertaining to (i) representations of the papers provided to similarity-computation algorithms, and (ii) the size of reviewers' profiles used by these algorithms. In this section, we investigate these two questions in more detail.

\begin{itemize}[leftmargin=20pt, itemsep=0pt, topsep=5pt]
    \item \textit{Question 1 (paper representation).} Some similarity-computation algorithms are designed to work with titles and/or abstracts of papers (e.g., \specter) while others can also incorporate the full texts of the manuscripts (e.g., \tpms{}). Consequently, there is a potential trade-off between accuracy and computation time. Richer representations are envisaged to result in higher accuracy, but are also associated with an increased demand for computational power. As with the choice of the algorithm itself, there is no guidance on what amount of information should be supplied to the algorithm as the gains from using more information are not quantified. With this motivation, our first question is:
    \begin{quote}
        \emph{What are the benefits of providing richer representations of papers to similarity-computation algorithms?}
    \end{quote}
    
    \item \textit{Question 2 (reviewer profile).} The second important choice is the size of reviewers' profiles. On the one hand, by including only very recent papers in reviewers' profiles, conference organizers are at risk of not using enough data to obtain accurate values of expertise. On the other hand, old papers may not accurately represent the current expertise of researchers and hence may result in noise when used to compute expertise. Thus, our second question is:

    \begin{quote}
        \emph{What is the optimal number of the most recent papers to include in the profiles of reviewers?}
    \end{quote}


\end{itemize}

To investigate these questions, we choose the \tpms{} algorithm as the workhorse to perform experiments. We make this choice for two reasons. First, \tpms{} can work with all possible choices of the paper representation: title only, title and abstract, and full text of the paper. In contrast, other methods do not support using the full text of the papers. Second, \tpms{} is fast to execute, enabling us to compute expertise for hundreds of  configurations in a reasonable time.



With the algorithm chosen, we vary the number of papers in the reviewers' profiles from 1 to 20. For each value of the profile length, we consider three representations of the paper content: title, title+abstract, and full text of the paper. For each combination of parameters, we construct reviewer profiles and predict similarities using the approach introduced in Section~\ref{section:results:comparison}. The only exception is that we repeat the procedure for averaging out the randomness in the profile creation 5 times (instead of 10) to save the computation time. 


\paragraph{Papers and reviewer profiles} Before presenting the results, we note that in this section, we conduct all evaluations focusing on papers that have PDFs freely available. To this end, we remove 6 papers from the dataset as they are not freely available online (see Table~\ref{table:papers_demo}). Similarly, we limit reviewer profiles to papers whose semantic scholar profiles contain links to arXiv. One of the participants did not have any such papers, so we exclude them from the dataset.



\paragraph{Results} Figure~\ref{fig:results_explore} and Table~\ref{table:results_explore} provide answers to the questions we study in this section. Figure~\ref{fig:results_explore} shows the pointwise loss of the \tpms{} algorithm for each choice of parameters. To save computation time, we do not build confidence intervals for each combination of parameters. Instead, Table~\ref{table:results_explore} sets the number of papers in reviewers' profiles to 20 (consistent with Table~\ref{table:results_main}) and presents confidence intervals for losses incurred by the algorithm under different choices of paper representations. We now make two observations.

\begin{figure}[tb]
    \centering
    \includegraphics[width=10cm]{Figures/tmps_analysis.pdf}
    \caption{Impact of different choices of parameters on the quality of predicted similarities. Confidence intervals are not shown (see Table~\ref{table:results_explore}).}
    \label{fig:results_explore}
\end{figure}

\begin{table*}[tb]
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
 Paper Representation &  Loss & 95\% CI for Loss & $\Delta$ with Title+Abstract & 95\%
 CI for $\Delta$ \\
  \midrule
Title & $0.33$ & $[0.29, 0.38]$ & $+0.07$ & $[0.02, 0.12]$ \\
Title+Abstract & $0.27$ & $[0.22, 0.32]$ & --- & --- \\
Full Text & \textbf{0.24}  & $[0.19, 0.30]$ & $-0.03$ & $[-0.09, 0.03]$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\caption{Performance of the \tpms{} algorithm with 20 most recent papers included in reviewers' profiles and with different choices of the paper representation. Lower values of loss are better. A positive (respectively, negative) value of $\Delta$ indicates that the corresponding choice of the paper representation leads to a weaker (respectively, stronger) performance.}
\label{table:results_explore}
\end{table*}

First, paper abstracts are very useful in improving the quality of expertise prediction as compared to titles alone (the improvement of $0.07$). Including full texts of the papers in reviewers' and papers' profiles results in additional improvement ($0.03$). Overall, ignoring the minor differences in the datasets between this section and Section~\ref{section:results:comparison}, we observe that \tpms{} with titles, abstracts, and full texts of papers demonstrates the same performance as \mfr{} which cannot handle the full texts of papers. Thus, it may be of interest to develop modern deep-learning based algorithms to incorporate full texts of papers and further boost performance on the similarity-computation task.

Second, the loss curves plateau once reviewer profiles include 8 or more of their most recent papers. Additional increase of the profile length does not impact the quality of predictions. Thus, in practice, reviewers may be instructed to include 10 representative papers to their profile, which for many active researchers amounts to the number of papers published in 1-3 years.

\section{Additional evaluations}
\label{section:section:easyhard}

Recall that our loss metric (see Section~\ref{section:expsetup}) assigns different weights to errors of an algorithm: errors on pairs with similar values of reviewer's expertise are penalized less than errors on well-separated pairs. The overall performance of the algorithm is then captured in a single number (loss) which does not characterize the type of mistakes made by the algorithm.

To provide deeper characterization of the algorithm performance, we now conduct additional evaluations. In these evaluations, we focus on two important regions of the similarity-computation problem. Specifically, from all expertise evaluations made by participants, we select two groups of triples, where each triple consists of a participant $\participant$ and two papers $(\paper_1, \paper_2)$ evaluated by this participant:
\begin{itemize}
    \item \textbf{``Easy'' triples.} The first group comprises 261 triples where a participant reported high expertise (greater than or equal to four) in one paper and low expertise (less than or equal to two) in another.

    We call these triples ``easy'' as the gap in the expertise between the two papers is large. Note that in real conferences it is important to resolve the easy triples correctly to ensure that reviewers do not get assigned irrelevant papers.


    \item \textbf{``Hard'' triples.} The second group comprises 417 triples where a participant reported high expertise (greater than or equal to four) in both papers and the values of expertise are different for the two papers.

    We call these triples ``hard'' as the gap in the expertise between the papers is small. That said, we note that resolving hard triples in practice is also very important: in real conferences, we want to assign papers to the most suitable reviewers and we need to be able to distinguish two papers with high but not equal values of expertise to achieve this goal.
\end{itemize}

Note that by focusing on these two groups, we exclude regions of the problem which may be considered less important. For example, the ability of an algorithm to correctly order two low-expertise papers is less crucial as long as the algorithm can reliably distinguish these papers from the high-expertise papers. 

We now study the performance of the algorithms introduced in Section~\ref{section:setup:algorithms} on the easy and hard groups of triples. For all algorithms we use titles and abstracts of papers as the data source and include 20 papers in the reviewers' profiles. Specifically, for each triple we compare the ordering of papers predicted by an algorithm with the expertise-induced ordering. We then compute the accuracy of each algorithm as the fraction of triples correctly resolved by the algorithm (from 0 to 1, larger values are better). In addition, we also evaluate \tpms{} in the full text regime to estimate the effect of papers' texts on the accuracy of the similarity-computation algorithms.\footnote{\tpms{} in the full text regime is evaluated on a slightly different dataset as explained in Section~\ref{section:results:exploration}.}

Table~\ref{table:additional_eval} demonstrates the results of additional evaluations. First, note that all algorithms have moderate to high accuracy in resolving the easy triples. All of the algorithms except \elmo{} detect papers that reviewers have low expertise in with probability close to or above 80\%, with \mfr{} reaching nearly 90\% accuracy. However, we note that in a non-trivial amount of cases (more than 10\%), the algorithms are unable to distinguish a paper that a reviewer is well-qualified to review versus a paper the reviewer does not have necessary background to evaluate. Thus, there is a vital need to improve the similarity-computation algorithms.

Getting to the hard triples, we observe a significant drop in performance with all algorithms being a bit better than random: the best-performing algorithm (\tpms{}) reaches 62\% accuracy in the title+abstract regime and 64\% in the full text regime. Perhaps surprisingly, while \mfr{} and \specter{} outperform \tpms{} on easy triples, they are not better than \tpms{} on hard triples (we caveat though that error bars are too wide to make a decisive comparison). This observation suggests that there may be a value in additionally fine-tuning these advanced algorithms on hard triples to improve their practical performance.

Finally, we observe that on both easy and hard triples, full texts of the papers are instrumental in improving performance of the \tpms{} algorithm. This observation supports our intuition that similarity-computation algorithms do indeed benefit from employing full texts of papers. 

\medskip

\noindent With these remarks, we conclude evaluations of algorithms on the dataset we collected. The dataset and associated code are released on the project's GitHub page\footnote{\url{https://github.com/niharshah/goldstandard-reviewer-paper-match}} and we encourage readers to experiment with additional evaluations on our data.


\begin{table*}[tb]
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
& \multicolumn{2}{c}{Easy triples ($n  = 261$)} & \multicolumn{2}{c}{Hard triples ($n  = 417$)} \\
\cmidrule(r){2-3} 
\cmidrule(r){4-5}
Algorithm &  {Accuracy} & {95\% CI} & {Accuracy} & {95\% CI} \\
\midrule
\tpms{} (T+A) & $0.80$ & $[0.72, 0.87]$ & $0.62$ & $[0.54, 0.69]$ \\
\tpms{} (Full Text) & $0.84$ & $[0.76, 0.91]$ & \textbf{0.64} & $[0.56, 0.70]$ \\
\midrule
\elmo{} & $0.70$ & $[0.62, 0.78]$ & $0.57$ & $[0.51, 0.63]$ \\
\specter{} & $0.85$ & $[0.76, 0.92]$ & $0.57$ & $[0.50, 0.63]$ \\
\mfr{} & \textbf{0.88} & $[0.81, 0.94]$ & $0.60$ & $[0.53; 0.66]$ \\
\midrule
\acl{} & $0.78$ & $[0.69; 0.86]$ & $0.62$ & $[0.55; 0.68]$\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\caption{Results of additional evaluations. Higher values of accuracy are better. \mfr{} demonstrates the best performance on easy triples and \tpms{} (Full Text) has the highest accuracy on hard triples.}
\label{table:additional_eval}
\end{table*}



\section{Discussion}
\label{section:discussion}

In this work, we collect a novel dataset of reviewers' expertise in reviewing papers. In contrast to datasets collected in previous works, our dataset (i) is released publicly, and (ii) contains  evaluations of expertise made by scientists who have actually read the papers for their own research purposes. We use this dataset to compare several existing expertise-estimation algorithms and help venue organizers in choosing an algorithm in an evidence-based manner. 

Most importantly, we find that current algorithms make a large number of errors. Given the growing number of submissions in many fields of research, the need for automation in reviewer assignments is only growing. There is thus an urgent and vital need to develop significantly improved algorithms to match reviewers to papers, thereby in turn making the peer-review process considerably better.

Our dataset can be used to develop as well as evaluate new expertise-estimation algorithms. We encourage researchers from the natural language processing and other communities to use our data in order to improve peer review.

\paragraph{Limitations.} Finally, we mention several caveats that researchers should be aware of when working with our dataset and interpreting the results of our experiments. First, our dataset comprises evaluations of expertise in reviewing papers that were written some time ago. In contrast, in real conferences, many papers are recent and not available online. Thus, incoming citations to papers included in our dataset may constitute information that is not available to the algorithms in real life. While the algorithms we evaluate in this work do not rely on the citation relationship, this caveat may be important for future work.

Second, the experiments we conduct in this work rely on Semantic Scholar profiles. These profiles are constructed automatically and may not be accurate. Thus, mistakes of the algorithms we observe in this work can be partially due to the noise in the profile creation.

Finally, we reiterate that the present version of the dataset was constructed by participants who collectively are not representative of the general computer science community. For example, about 40\% of participants are affiliated with CMU. To alleviate this issue, we continue the data collection process and encourage the readers of this paper to contribute their data points to the dataset:
\begin{center}
    \weblink
\end{center}

\subsection*{Acknowledgments} 
We sincerely thank all participants of our survey for their contribution to the dataset. This work was supported by NSF CAREER 1942124 and NSF 1763734. 



\bibliography{bibtex}
\bibliographystyle{apalike}

\newpage

\noindent \textbf{{\LARGE Appendices}}

\bigskip

\appendix



\noindent We now provide additional discussion.

\newcommand{\abstr}{a}
\newcommand{\gabstr}{A}
\newcommand{\contrast}{t}
\newcommand{\gabs}{G}


\section{More details on the survey used for data collection}
\label{appendix:survey}

In this section we provide full instructions that were given to the participants of our data-collection procedure.

\medskip

  \begin{mdframed}

  \begin{center}\texttt{\Large{Dataset of Reviewing Expertise}}\end{center}

  \smallskip
  
  \noindent \texttt{The goal of this experiment is to collect a dataset to help researchers design better\\algorithms for computing similarities between papers and reviewers: these algorithms\\will help to improve matching of reviewers to papers in many conferences like NeurIPS, AAAI, ACL, etc.}

  \medskip

  \noindent \texttt{WHAT DO YOU NEED TO DO? \\ ***Recall 5-10 papers in your broad research area that you read to a reasonable extent in the last year and tell us your expertise in reviewing these papers.***\\(HINT: To quickly recall what papers you read recently, you can search for arxiv.org\\or an analogous website in your browser history.)}

  \medskip
  \noindent \texttt{WHICH PAPERS TO REPORT?}
  \begin{itemize}[leftmargin=20pt, itemsep=0pt, topsep=1pt]
      \item \texttt{Papers should not be authored by you}
      \item \texttt{Papers should be freely available online (preferably arXiv, but other open sources are also fine)}
  \end{itemize}

  \medskip

  \noindent \texttt{**Suggestions**}
  \begin{itemize}[leftmargin=20pt, itemsep=0pt, topsep=1pt]
      \item \texttt{Try to choose a set of papers such that some pairs are well-separated and some are very close in terms of your expertise}
      \item \texttt{Please try to avoid ties in the expertise ratings you provide}
      \item \texttt{Try to think of some papers that are less famous to make the dataset more diverse}
      \item \texttt{Try to provide some examples that are not obvious and may be tricky for the\\similarity-computation algorithms. For example, a naive computation of similarity may think that a paper on ``Theory of Information Dissemination in Social\\Networks'' has high similarity with an Information Theory researcher, but in\\reality, this researcher may not have expertise in reviewing this paper}
  \end{itemize}

  \medskip

  \noindent \texttt{WHAT PARTS OF DATA YOU PROVIDE WILL BE RELEASED?\\To facilitate the development of better algorithms for similarity computation (trained to perform well on your data!), we will publicly release data collected in this survey (except email addresses) in a non-anonymized form. Your email will not be released.}

  \medskip

  \noindent \texttt{WHO IS RUNNING THIS SURVEY?\\The survey is organized by Graham Neubig, Nihar Shah, Ivan Stelmakh (CMU), and John\\Wieting (CMU -> Google Research). Contact Ivan at stiv@cs.cmu.edu if you have any\\questions.}

  \medskip

  \noindent \texttt{[...]}

  \medskip

  \noindent \texttt{List up to 10 papers in your broad research area that you read to a reasonable extent\\in the last year and tell us your expertise in reviewing these papers. Please try to\\enter at least 5 papers.}

  \medskip

  \noindent \texttt{Link to Paper 1:\_\_\_\_\_\_\_\_}
  
  \noindent \texttt{Expertise in reviewing Paper 1:}
  \begin{itemize}[leftmargin=20pt, noitemsep, topsep=1pt]
      \item \texttt{1.0 (I am not qualified to review this paper)}
      \item \texttt{1.25}
      \item \texttt{1.5}
      \item \texttt{1.75}
      \item \texttt{2.0 (I can review some aspects of the paper, but can't make a reliable overall\\judgment)}
      \item \texttt{...}
      \item \texttt{3.0 (I can provide an adequate review, but a substantial part of the paper is\\outside of my expertise)}
      \item \texttt{...}
      \item \texttt{4.0 (I have background in most aspects of the paper, but some minor aspects are\\beyond my expertise)}
      \item \texttt{...}
      \item \texttt{5.0 (I have background necessary to evaluate all the aspects of the paper)}
  \end{itemize}

  \medskip

  \noindent \texttt{[...]}

\end{mdframed}

\vspace{5pt}

\section{More details on the \acl{} algorithm}
\label{section:acl}

In this section, we provide more details on the \acl{} algorithm that we evaluate in this paper. 

\paragraph{Training} The model is trained on a large corpus of 45,309 abstracts from the ACL anthology and is inspired by the work of~\citet{wieting-etal-2019-simple,wieting-etal-2022-paraphrastic}. Specifically, the model optimizes a max-margin contrastive learning objective which is defined as follows. First, we split each abstract $\abstr_i$ from the corpus into two disjoint segments of text uniformly at random. These segments are then uniformly at random allocated into two equally-sized groups: $\abstr_i^{(1)} \in \gabstr_1$ and $\abstr_i^{(2)} \in \gabstr_2$. 


\smallskip \noindent Second, we construct positive and negative examples:
\begin{itemize}[itemsep=0pt, leftmargin=20pt, topsep=5pt]
    \item \emph{Positive example:} For each abstract $\abstr_i$, pair ($\abstr_i^{(1)}$, $\abstr_i^{(2)}$) constitutes a positive example.
    
    \item \emph{Negative example:} For each passage $\abstr_{i}^{(1)} \in \gabstr_1$, a counterpart $\contrast_i \ne \abstr_{i}^{(2)}$ from $\gabstr_2$ is selected to maximize the notion of cosine similarity 
    \begin{align*}
        f_{\theta} (\abstr_{i}^{(1)}, t_i) = \cos\left(g(\abstr_{i}^{(1)}, \theta), g(t_i, \theta)\right),
    \end{align*} where $g$ is the sentence encoder with parameters $\theta$. Pair ($\abstr_{i}^{(1)}$, $\contrast_i$) constitutes a negative example.
\end{itemize}

\noindent Finally, with this procedure to build positive and negative examples, we can define the objective of the \acl{} algorithm:

\begin{align*}
\min_{\theta} \sum_i\Big[\delta - &f_{\theta}(\abstr_{i}^{(1)}, \abstr_{i}^{(2)}) + f_{\theta}(\abstr_{i}^{(1)}, \contrast_{i}))\Big]_+\text{\footnotemark}
\end{align*}\footnotetext{We use $\left[ \cdot \right]_+$ to denote function $h: h(x) = \max(x, 0)$.}

\smallskip \noindent Inner-working of the algorithm relies on \texttt{sentencepiece} embeddings\footnote{\url{https://github.com/google/sentencepiece}}~\citep{kudo-richardson-2018-sentencepiece} with dimension of 1,024 and vocabulary size of 20,000. The encoder, $g$, simply mean pools the learned \texttt{sentencepiece} embeddings, making for efficient encoding, even on CPU.\footnote{See~\cite{wieting-etal-2019-simple,wieting-etal-2022-paraphrastic} for more details on encoding speed.} In the training procedure, a batch size of 64 is used and the model is trained for 20 epochs. The margin, $\delta$, is set to 0.4.

\paragraph{Inference} At the inference stage, for a given pair of a submission and a reviewer, we define the similarity score as a combination of cosine similarities between the submission's abstract and three most-similar abstracts from the reviewer's profile. Specifically, let $\similarity_1, \similarity_2$ and $\similarity_3$ be the top-3 cosine similarities between the submission's abstract and abstracts from the reviewer's profile. The similarity score between the submission and the reviewer is then defined as follows:
\begin{align*}
    \similarity = \similarity_1 + \frac{\similarity_2}{2} + \frac{\similarity_3}{3}.
\end{align*}
\noindent If a reviewer has less than three abstracts in the profile, we set the corresponding cosine similarity scores $s_i$ to be zero.



\end{document}
