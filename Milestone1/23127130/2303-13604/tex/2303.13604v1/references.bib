@inproceedings{nie22_explor_then_commit_algor_for,
	title = {An Explore-Then-Commit Algorithm For Submodular Maximization Under Full-Bandit Feedback},
	language = {en},
	booktitle = {Proceedings of the {Thirty}-{Eighth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Nie, Guanyu and Agarwal, Mridul and Umrawal, Abhishek Kumar and Aggarwal, Vaneet and Quinn, Christopher John},
	month = aug,
	year = 2022,
	pages = {1541--1551},
}
@article{xiang2014joint,
  title={Joint latency and cost optimization for erasurecoded data center storage},
  author={Xiang, Yu and Lan, Tian and Aggarwal, Vaneet and Chen, Yih Farn R},
  journal={ACM SIGMETRICS Performance Evaluation Review},
  volume={42},
  number={2},
  pages={3--14},
  year={2014},
  publisher={ACM New York, NY, USA}
}
@article{zhang2019online,
  title={Online continuous submodular maximization: From full-information to bandit feedback},
  author={Zhang, Mingrui and Chen, Lin and Hassani, Hamed and Karbasi, Amin},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{chen2012time,
  title={Time-critical influence maximization in social networks with time-delayed diffusion process},
  author={Chen, Wei and Lu, Wei and Zhang, Ning},
  booktitle={Twenty-Sixth AAAI Conference on Artificial Intelligence},
  year={2012}
}
@article{iribarren2009impact,
  title={Impact of human activity patterns on the dynamics of information diffusion},
  author={Iribarren, Jos{\'e} Luis and Moro, Esteban},
  journal={Physical review letters},
  volume={103},
  number={3},
  pages={038702},
  year={2009},
  publisher={APS}
}
@article{lin2015stochastic,
  title={Stochastic online greedy learning with semi-bandit feedbacks},
  author={Lin, Tian and Li, Jian and Chen, Wei},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  year={2015}
}
@inproceedings{takemori2020submodular,
  title={Submodular bandit problem under multiple constraints},
  author={Takemori, Sho and Sato, Masahiro and Sonoda, Takashi and Singh, Janmajay and Ohkuma, Tomoko},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  pages={191--200},
  year={2020},
  organization={PMLR}
}
@inproceedings{chen2018projection,
  title={Projection-free online optimization with stochastic gradient: From convexity to submodularity},
  author={Chen, Lin and Harshaw, Christopher and Hassani, Hamed and Karbasi, Amin},
  booktitle={International Conference on Machine Learning},
  pages={814--823},
  year={2018},
  organization={PMLR}
}

@inproceedings{dani2008stochastic,
  title={Stochastic linear optimization under bandit feedback},
  author={Dani, Varsha and Hayes, Thomas P and Kakade, Sham M},
  booktitle={Annual Conference Computational Learning Theory},
  year={2008}
}

@inproceedings{rejwan2020top,
  title={Top-$ k $ combinatorial bandits with full-bandit feedback},
  author={Rejwan, Idan and Mansour, Yishay},
  booktitle={Algorithmic Learning Theory},
  pages={752--776},
  year={2020},
  organization={PMLR}
}
@article{sviridenko2004note,
  title={A note on maximizing a submodular set function subject to a knapsack constraint},
  author={Sviridenko, Maxim},
  journal={Operations Research Letters},
  volume={32},
  number={1},
  pages={41--43},
  year={2004},
  publisher={Elsevier}
}
@article{karsai2011small,
  title={Small but slow world: How network topology and burstiness slow down spreading},
  author={Karsai, M{\'a}rton and Kivel{\"a}, Mikko and Pan, Raj Kumar and Kaski, Kimmo and Kert{\'e}sz, J{\'a}nos and Barab{\'a}si, A-L and Saram{\"a}ki, Jari},
  journal={Physical Review E},
  volume={83},
  number={2},
  pages={025102},
  year={2011},
  publisher={APS}
}
@inproceedings{kempe2003maximizing,
  title={Maximizing the spread of influence through a social network},
  author={Kempe, David and Kleinberg, Jon and Tardos, {\'E}va},
  booktitle={Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={137--146},
  year={2003}
}
@inproceedings{chen2010scalable,
  title={Scalable influence maximization in social networks under the linear threshold model},
  author={Chen, Wei and Yuan, Yifei and Zhang, Li},
  booktitle={2010 IEEE international conference on data mining},
  pages={88--97},
  year={2010},
  organization={IEEE}
}
@inproceedings{domingos2001mining,
  title={Mining the network value of customers},
  author={Domingos, Pedro and Richardson, Matt},
  booktitle={Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={57--66},
  year={2001}
}
@inproceedings{niazadeh2021online,
  title={Online learning via offline greedy algorithms: Applications in market design and optimization},
  author={Niazadeh, Rad and Golrezaei, Negin and Wang, Joshua R and Susan, Fransisca and Badanidiyuru, Ashwinkumar},
  booktitle={Proceedings of the 22nd ACM Conference on Economics and Computation},
  pages={737--738},
  year={2021}
}
@article{zhu2021projection,
  title={Projection-free decentralized online learning for submodular maximization over time-varying networks},
  author={Zhu, Junlong and Wu, Qingtao and Zhang, Mingchuan and Zheng, Ruijuan and Li, Keqin},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={2328--2369},
  year={2021},
  publisher={JMLRORG}
}
@article{nemhauser1978analysis,
  title={An analysis of approximations for maximizing submodular set functions—I},
  author={Nemhauser, George L and Wolsey, Laurence A and Fisher, Marshall L},
  journal={Mathematical programming},
  volume={14},
  pages={265--294},
  year={1978},
  publisher={Springer}
}
@article{agarwal2022stochastic,
  title={Stochastic Top K-Subset Bandits with Linear Space and Non-Linear Feedback with Applications to Social Influence Maximization},
  author={Agarwal, Mridul and Aggarwal, Vaneet and Umrawal, Abhishek K and Quinn, Christopher J},
  journal={ACM/IMS Transactions on Data Science (TDS)},
  volume={2},
  number={4},
  pages={1--39},
  year={2022},
  publisher={ACM New York, NY}
}
@inproceedings{wong2003mpis,
  title={MPIS: Maximal-profit item selection with cross-selling considerations},
  author={Wong, Raymond Chi-Wing and Fu, Ada Wai-Chee and Wang, Ke},
  booktitle={Third IEEE International Conference on Data Mining},
  pages={371--378},
  year={2003},
  organization={IEEE}
}
@InProceedings{pike-burke18_bandit_delay_aggreg_anony_feedb,
  title = 	 {Bandits with Delayed, Aggregated Anonymous Feedback},
  author =       {Pike-Burke, Ciara and Agrawal, Shipra and Szepesvari, Csaba and Grunewalder, Steffen},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4105--4113},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/pike-burke18a/pike-burke18a.pdf},
  abstract = 	 {We study a variant of the stochastic $K$-armed bandit problem, which we call "bandits with delayed, aggregated anonymous feedback”. In this problem, when the player pulls an arm, a reward is generated, however it is not immediately observed. Instead, at the end of each round the player observes only the sum of a number of previously generated rewards which happen to arrive in the given round. The rewards are stochastically delayed and due to the aggregated nature of the observations, the information of which arm led to a particular reward is lost. The question is what is the cost of the information loss due to this delayed, aggregated anonymous feedback? Previous works have studied bandits with stochastic, non-anonymous delays and found that the regret increases only by an additive factor relating to the expected delay. In this paper, we show that this additive regret increase can be maintained in the harder delayed, aggregated anonymous feedback setting when the expected delay (or a bound on it) is known. We provide an algorithm that matches the worst case regret of the non-anonymous problem exactly when the delays are bounded, and up to logarithmic factors or an additive variance term for unbounded delays.}
}


@InProceedings{pmlr-v75-cesa-bianchi18a,
  title = 	 {Nonstochastic Bandits with Composite Anonymous Feedback},
  author =       {Cesa-Bianchi, Nicol\`o and Gentile, Claudio and Mansour, Yishay},
  booktitle = 	 {Proceedings of the 31st  Conference On Learning Theory},
  pages = 	 {750--773},
  year = 	 {2018},
  editor = 	 {Bubeck, Sébastien and Perchet, Vianney and Rigollet, Philippe},
  volume = 	 {75},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v75/cesa-bianchi18a/cesa-bianchi18a.pdf},
  abstract = 	 {We investigate a nonstochastic bandit setting in which the loss of an action is not immediately charged to the player, but rather spread over at most d consecutive steps in an adversarial way. This implies that the instantaneous loss observed by the player at the end of each round is a sum of as many as d loss components of previously played actions. Hence, unlike the standard bandit setting with delayed feedback, here the player cannot observe the individual delayed losses, but only their sum. Our main contribution is a general reduction transforming a standard bandit algorithm into one that can operate in this harder setting. We also show how the regret of the transformed algorithm can be bounded in terms of the regret of the original algorithm. Our reduction cannot be improved in general: we prove a lower bound on the regret of any bandit algorithm in this setting that matches (up to log factors) the upper bound obtained via our reduction. Finally, we show how our reduction can be extended to more complex bandit settings, such as combinatorial linear bandits and online bandit convex optimization.}
}


@misc{garg19_stoch_bandit_delay_compos_anony_feedb,
	title = {Stochastic {Bandits} with {Delayed} {Composite} {Anonymous} {Feedback}},
	doi = {10.48550/arXiv.1910.01161},
	publisher = {arXiv},
	author = {Garg, Siddhant and Akash, Aditya Kumar},
	month = oct,
	year = 2019,
	note = {arXiv:1910.01161 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{Wang_Wang_Huang_2021, 
    title={Adaptive Algorithms for Multi-armed Bandit with Composite and Anonymous Feedback}, 
    volume={35}, 
    DOI={10.1609/aaai.v35i11.17224}, 
    number={11}, 
    journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
    author={Wang, Siwei and Wang, Haoyun and Huang, Longbo}, 
    year={2021}, 
    month={May}, 
    pages={10210-10217} 
}


@InProceedings{pmlr-v28-joulani13,
  title = 	 {Online Learning under Delayed Feedback},
  author = 	 {Joulani, Pooria and Gyorgy, Andras and Szepesvari, Csaba},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1453--1461},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/joulani13.pdf},
  abstract = 	 {Online learning with delayed feedback has received increasing attention recently due to its several applications in distributed, web-based learning problems. In this paper we provide a systematic study of the topic, and analyze the effect of delay on the regret of online learning algorithms. Somewhat surprisingly, it turns out that delay increases the regret in a multiplicative way in adversarial problems, and in an additive way in stochastic problems. We give meta-algorithms that transform, in a black-box fashion, algorithms developed for the non-delayed case into ones that can handle the presence of delays in the feedback loop. Modifications of the well-known UCB algorithm are also developed for the bandit problem with delayed feedback, with the advantage over the meta-algorithms that they can be implemented with lower complexity.}
}


@article{agarwal2021dart, 
  title={DART: Adaptive Accept Reject Algorithm for Non-Linear Combinatorial Bandits},
  volume={35}, 
  number={8}, 
  journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
  author={Agarwal, Mridul and Aggarwal, Vaneet and Umrawal, Abhishek Kumar and Quinn, Chris}, 
  year={2021}, 
  month={May}, 
  pages={6557-6565} 
}



@inproceedings{streeter2010online,
 author = {Streeter, Matthew and Golovin, Daniel and Krause, Andreas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Online Learning of Assignments},
 volume = {22},
 year = {2009}
}


@book{billingsley1995probability,
  title={Probability and Measure},
  author={Billingsley, P.},
  isbn={9780471007104},
  lccn={gb95051456},
  series={Wiley Series in Probability and Statistics},
  year={1995},
  publisher={Wiley}
}


@InProceedings{pmlr-v119-gael20a,
  title = 	 {Stochastic bandits with arm-dependent delays},
  author =       {Gael, Manegueu Anne and Vernade, Claire and Carpentier, Alexandra and Valko, Michal},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {3348--3356},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/gael20a/gael20a.pdf},
  abstract = 	 {Significant work has been recently dedicated to the stochastic delayed bandits because of its relevance in applications. The applicability of existing algorithms is however restricted by the fact that strong assumptions are often made on the delay distributions, such as full observability, restrictive shape constraints, or uniformity over arms. In this work, we weaken them significantly and only assume that there is a bound on the tail of the delay. In particular, we cover the important case where the delay distributions vary across arms, and the case where the delays are heavy-tailed. Addressing these difficulties, we propose a simple but efficient UCB-based algorithm called the PatientBandits. We provide both problemsdependent and problems-independent bounds on the regret as well as performance lower bounds.}
}

@inproceedings{vernade2017stochastic,
  title={Stochastic Bandit Models for Delayed Conversions},
  author={Vernade, Claire and Capp{\'e}, Olivier and Perchet, Vianney},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  year={2017}
}


@InProceedings{pmlr-v139-lancewicki21a,
  title = 	 {Stochastic Multi-Armed Bandits with Unrestricted Delay Distributions},
  author =       {Lancewicki, Tal and Segal, Shahar and Koren, Tomer and Mansour, Yishay},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {5969--5978},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/lancewicki21a/lancewicki21a.pdf},
  abstract = 	 {We study the stochastic Multi-Armed Bandit&nbsp;(MAB) problem with random delays in the feedback received by the algorithm. We consider two settings: the {\it reward dependent} delay setting, where realized delays may depend on the stochastic rewards, and the {\it reward-independent} delay setting. Our main contribution is algorithms that achieve near-optimal regret in each of the settings, with an additional additive dependence on the quantiles of the delay distribution. Our results do not make any assumptions on the delay distributions: in particular, we do not assume they come from any parametric family of distributions and allow for unbounded support and expectation; we further allow for the case of infinite delays where the algorithm might occasionally not observe any feedback.}
}


@article{feige98,
  author = {Feige, Uriel},
  title = {A Threshold of Ln n for Approximating Set Cover},
  year = {1998},
  issue_date = {July 1998},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {45},
  number = {4},
  issn = {0004-5411},
  doi = {10.1145/285055.285059},
  abstract = {Given a collection ℱ of subsets of S = {1,…,n}, set cover is the problem of selecting as few as possible subsets from ℱ such that their union covers S,, and max k-cover is the problem of selecting k subsets from ℱ such that their union has maximum cardinality. Both these problems are NP-hard. We prove that (1 - o(1)) ln n is a threshold below which set cover cannot be approximated efficiently, unless NP has slightly superpolynomial time algorithms. This closes the gap (up to low-order terms) between the ratio of approximation achievable by the greedy alogorithm (which is (1 - o(1)) ln n), and provious results of Lund and Yanakakis, that showed hardness of approximation within a ratio of (log2 n) / 2 ≃0.72 ln n. For max k-cover, we show an approximation threshold of (1 - 1/e)(up to low-order terms), under assumption that P ≠ NP.},
  journal = {J. ACM},
  month = {jul},
  pages = {634–652},
  numpages = {19},
  keywords = {approximation ratio, set cover}
}

@misc{nie2023framework,
      title={A Framework for Adapting Offline Algorithms to Solve Combinatorial Multi-Armed Bandit Problems with Bandit Feedback}, 
      author={Guanyu Nie and Yididiya Y Nadew and Yanhui Zhu and Vaneet Aggarwal and Christopher John Quinn},
      year={2023},
      eprint={2301.13326},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Streeter2008AnOA,
  title={An Online Algorithm for Maximizing Submodular Functions},
  author={Matthew J. Streeter and Daniel Golovin},
  booktitle={NIPS},
  year={2008}
}

