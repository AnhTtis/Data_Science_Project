\documentclass[journal]{IEEEtai}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{wrapfig}
\usepackage{xcolor}

\usepackage{graphicx}


\bibliographystyle{IEEEtran}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{algorithm}
\usepackage{algorithmic}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{orcidlink}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

\newcommand{\todo}[1]{{\color{red}TODO: \color{blue}#1}\PackageWarning{todo}{TODO}}
\newcommand{\done}[1]{{\color{red}Resolved TODO: \color{blue}#1}\PackageWarning{todo}{Resolved TODO}}


\newcommand{\replace}[2]{{\color{red}#1}{\color{blue}#2}}
\newcommand{\nc}[1]{#1}


\usepackage{caption}
\usepackage{subcaption}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Assumption:}}


\renewcommand{\t}{\text}
\newcommand{\op}[1]{\operatorname{#1}}
\newcommand{\C}[1]{{\mathcal{#1}}} \newcommand{\B}[1]{{\mathbb{#1}}} \newcommand{\E}[1]{{\mathscr{#1}}} \newcommand{\F}[1]{{\mathfrak{#1}}}



\makeatletter
\def\blfootnote{\xdef\@thefnmark{}\@footnotetext}
\makeatother


\title{Stochastic Submodular Bandits with Delayed Composite Anonymous Bandit Feedback}
\author{
Mohammad Pedramfar
and 
Vaneet Aggarwal
\thanks{
M. Pedramfar is with McGill University, Montreal, QC, H3A 0E9 Canada
and Mila - Quebec Artificial Intelligence Institute, Montreal, QC, H2S 3H1 Canada.
This work was done while at Purdue University, West Lafayette IN, 47907 USA (email: mohammad.pedramfar@mila.quebec).

V. Aggarwal is with Purdue University, West Lafayette IN, 47907 USA (email: vaneet@purdue.edu).

This work was supported in part by National Science Foundation under grant CCF-2149588.

This paper is accepted to IEEE Transactions on Artificial Intelligence, December 2024.

{\textcopyright} 2025 IEEE.
Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.
}}

\begin{document}

\maketitle



\begin{abstract}
This paper investigates the problem of combinatorial multiarmed bandits with stochastic submodular (in expectation) rewards and full-bandit delayed feedback, where the delayed feedback is assumed to be composite and anonymous. 
In other words, the delayed feedback is composed of components of rewards from past actions, with unknown division among the sub-components. 
Three models of delayed feedback: bounded adversarial, stochastic independent, and stochastic conditionally independent are studied, and regret bounds are derived for each of the delay models. 
Ignoring the problem dependent parameters, we show that regret bound for all the delay models is $\tilde{O}(T^{2/3} + T^{1/3} \nu)$ for time horizon $T$, where $\nu$ is a delay parameter defined differently in the three cases, thus demonstrating an additive term in regret with delay in all the three delay models.
The considered algorithm is demonstrated to outperform other full-bandit approaches with delayed composite anonymous feedback. 
We also demonstrate the generalizability of our analysis of the delayed composite anonymous feedback in combinatorial bandits as long as there exists an algorithm for the offline problem satisfying a certain robustness condition.
\end{abstract}


\begin{IEEEImpStatement}
This research advances the field of stochastic combinatorial multi-armed bandits (CMAB) by addressing the complexities of delayed composite anonymous feedback in environments with submodular and monotone rewards. By introducing innovative analytical techniques and establishing novel regret bounds, the study enhances our understanding of decision-making processes in dynamic and uncertain contexts, such as social networks and online advertising. This work significantly broadens the applicability of CMAB models to real-world scenarios where actions have delayed feedback, thereby improving strategic decision-making and optimization in various industries.
\end{IEEEImpStatement}


{
\begin{IEEEkeywords}
Submodular maximization, Bandit feedback, Combinatorial multi-armed bandit, Delayed feedback, Composite anonymous delay
\end{IEEEkeywords}
}


\section{Introduction}


Many real world sequential decision problems can be modeled using the framework of stochastic multi-armed bandits (MAB), such as  scheduling,  assignment problems, ad-campaigns, and product recommendations. In these problems, the decision maker sequentially selects actions and receives stochastic rewards from an unknown distribution. The objective is to maximize the expected cumulative reward over a time horizon. Such problems result in a trade-off between trying actions to learn the system (\textit{exploration}) and  taking the action that is empirically the best seen so far (\textit{exploitation}). 

Combinatorial MAB (CMAB) involves the problem of  finding the best subset of
$K$ out of $N$ items to optimize a possibly nonlinear function of reward of each item. Such a problem has  applications in cloud storage \cite{xiang2014joint},  cross-selling item selection \cite{wong2003mpis}, social influence maximization \cite{agarwal2022stochastic}, etc. The key challenge in CMAB is the combinatorial $N$-choose-$K$ decision space, which can be very large. This problem can be converted to standard MAB with an exponentially large action space, although needing an exponentially large time horizon to even explore each action once. Thus, the algorithms for CMAB aim to not have this exponential complexity while still providing regret bounds. An important class of combinatorial bandits is submodular bandits; which is based on the intuition that opening additional restaurants in
a small market may result in diminishing returns due to
market saturation. A set
function $f : 2^\Omega \to \mathbb{R}$ defined on a finite ground set $\Omega$ is
said to be submodular if it satisfies the diminishing return
property: for all $A \subseteq B \subseteq \Omega$, and $x \in  \Omega \setminus B$, it holds that $f(A \cup \{x\}) - f(A) \geq f(B \cup \{x\}) - f(B)$ \cite{nemhauser1978analysis}. 
Multiple applications for  CMABs with submodular rewards have been described in detail in \cite{nie22_explor_then_commit_algor_for}, including social influence maximization, recommender systems, and crowdsourcing. 
In these setups, the function is also monotone (adding more restaurants give better returns, adding more seed users give better social influence), where for all $A \subseteq B \subseteq \Omega$, $f(A) \leq f(B)$, and thus we also assume monotononicity in the submodular functions. 



Feedback plays an important role in how challenging the CMAB problem is.  When the decision maker only observes a (numerical) reward for the action taken, that is known as bandit or full-bandit feedback. When the decision maker observes additional information, such as contributions of each base arm in the action, that is semi-bandit feedback. Semi-bandit feedback greatly facilitates learning. Furthermore, there are two common formalizations depending on the assumed nature of environments: the stochastic
setting and the adversarial setting. In the adversarial setting, the reward sequence is generated
by an unrestricted adversary, potentially based on the history of decision makerâ€™s actions. In the
stochastic environment, the reward of each arm is drawn independently from a fixed distribution. For CMAB with submodular and monotone rewards, stochastic setting is not a special case of the adversarial setting since in the adversarial setting, the
environment chooses a sequence of monotone and submodular functions $\{f_1, \cdots , f_T \}$, while the stochastic setup assumes $f_t$ to be monotone and submodular in expectation~\cite{nie22_explor_then_commit_algor_for}.
In the adversarial setting, even if we limit ourselves to MAB instead of CMAB, the effect of composite anonymous delay appears as a multiplicative factor in the literature (e.g.~\cite{pmlr-v75-cesa-bianchi18a}).
In this paper, we study the impact of full-bandit feedback in the stochastic setting for  CMAB with submodular rewards and cardinality constraints. 
In this case, the regret analysis with full-bandit feedback has been studied in the adversarial setting in~\nc{\cite{streeter2010online}} and~\cite{niazadeh2021online}, and in stochastic setting in~\cite{nie22_explor_then_commit_algor_for}. 


In the prior works on  CMAB as mentioned earlier, the feedback is available immediately after the action is taken. However, this may not always be the case. Instead of receiving the reward in a single step, it can be spread over multiple number of time steps after the action was chosen. 
Following each action choice, the player receives the cumulative rewards from all prior actions whose rewards are due at this specific step.
The difficulty of this setting is due to the fact that the agent does not know how
this aggregated reward has been constituted from the previous actions chosen. This setting is called delayed composite anonymous feedback. Such feedback arise in multiple practical setups. As an example, we consider a social influence maximization problem. Consider a case of social network where a company developed an application and wants
to market it through the network. The best way to do this
is selecting a set of highly influential users and hope they
can love the application and recommend their friends to use
it. Influence maximization is a problem of finding a small
subset (seed set) in a network that can achieve maximum influence. This subset selection problem in social networks is
commonly modeled as an offline submodular optimization
problem \cite{domingos2001mining,kempe2003maximizing,chen2010scalable}. However, when the seed set is selected, the propagation
of influence from one person to another may incur a certain
amount of time delay and is not immediate \cite{chen2012time}. The time-delay phenomena in information diffusion
has also been explored in statistical physics \cite{iribarren2009impact,karsai2011small}. The spread of influence diffusion, and that at each time we can only observe the aggregate reward limits us to know the composition of the rewards into the different actions in the past. Further, the application developer, in most cases, will only be able to see the aggregate reward leading to this being a bandit feedback. This motivates our study of stochastic  CMAB with submodular rewards and delayed composite anonymous bandit feedback.



To the best of our knowledge, this is the first work on stochastic CMAB with delayed composite anonymous  feedback. In this paper, we consider three models of delays. The first model of delay is `Unbounded Stochastic Independent Delay'. In this model, different delay distributions can be chosen at each time, and these delay distributions are independent of each other. The second model is `Unbounded Stochastic Conditionally Independent Delay'. In this model, the delay distribution does not only depend on time, but also on the set chosen. The third model is `Bounded Adversarial Delay'. In this model, the maximum delay at each time can be chosen arbitrarily as long as it is bounded. We note that in stochastic cases, the delay is not bounded, while is governed by the tight family of distributions.\footnote{See Section~\ref{sec:problem_statement} for a detailed description.} 
In the adversarial case, there is a bound on the maximum delay, and the process generating this delay does not need to satisfy any other assumptions.
Thus, the results of stochastic and adversarial setups do not follow from each other. In particular, this is the first work where the delay distribution is allowed to change over time. This gives new models for delayed composite anonymous feedback which are more general than that considered in the literature.  In each of the three models of delay, this paper derives novel regret bounds. 



We note that this work considers  the most general form of the notion of composite anonymous delayed feedback studied in the literature that we know of. The challenge in capturing this generalized notion of delay is how to handle infinite dimensional random vectors, i.e., random variables that take their values within the space of all probability measures over non-negative integers.  This is done by defining the notion of upper tail bounds, which measures the tightness of a family of distributions\footnote{See Assumption~\ref{assumption} and Lemma~\ref{L:tightness_equiv_upper_bound} for more details.} (i.e., probability distributions over non-negative integers), and use it to bound the regret. Our definition of composite anonymous delay together with the notion of upper tail bound provide an easy way to handle the delay where a single positive real number can capture the effect of delay even in the presence of unbounded delay. Then we use Bernstein inequality to control the effect of past actions on the observed reward of the current action that is being repeated. This allows us to obtain a regret upper bound in terms of the expected value of the upper tail bound. The use of upper tail bounds for studying regret in bandits with delayed feedback is novel and has not been considered in the literature earlier, to the best of our knowledge. 





Even for non-combinatorial bandits in \cite{Wang_Wang_Huang_2021}, the setting is more limited than what we consider and the regret bound is not closed form and depends on multiple aspects of impact of delay, while our work provide an easy way to handle the delay where a single positive real number can capture the effect of delay even in the presence of unbounded delay.  This notion allows us to simplify the proof for different types of delay and use a reduction method, captured in Lemmas 5, 6 and 7 to reduce the all types of delay into a simple setting which could be analyzed using Theorem 1. This simplified notion and analysis will be important for any analysis of such feedback in any bandits literature in the future.

A fundamental challenge in dealing with composite anonymous feedback is that the information received after each action is so little that every existing algorithm has been using the idea of repeating actions to obtain estimates of true rewards.  The SOTA for the non-combinatorial setting uses an adaptive approach to revisit actions and improve the estimate of the reward of each action over time.
The combinatorial nature of the problem does not allow for such an approach, since we will not be able to try every action once, let alone revisit them. As a result, we expect the optimal regret bound for this problem to result from using the same fixed time repeating approach used here with the optimal algorithm for the base setting without delay.



The main contributions of this paper can be summarized as follows





\noindent {\bf 1.} We introduce regret bounds for a stochastic CMAB problem with expected monotone and submodular rewards, a cardinality constraint, and composite anonymous feedback. Notably, this paper marks the first study of the regret bound any CMAB problem with composite delayed feedback, including CMAB with submodular rewards. The analysis approach using the notion of upper tail bounds is novel, and has not been considered in any literature with delayed feedback. This approach allows for analysis of the composite anonymous delayed feedback, which was not studied for combinatorial bandits prior to our work. 


\noindent {\bf 2.} We investigate the ETCG algorithm from~\cite{nie22_explor_then_commit_algor_for}, detailing its performance in three feedback delay models: bounded adversarial delay, stochastic independent delay, and stochastic conditional independent delay.
Specifically, this is the first study where the distribution of stochastic delay is permitted to vary over time.
This introduces novel models for stochastic delayed composite anonymous feedback, which are more general than those previously explored in existing literature.

\noindent {\bf 3.} Our analysis reveals the cumulative $(1 - 1/e)$-regret of ETCG under specific bounds for each delay model. When comparing stochastic independent and conditional independent delays, the former showcases better regret bounds. Generalizing beyond specific parameters, our findings suggest a regret bound of $\tilde{O}(T^{2/3} + T^{1/3}\nu)$ across delay models \nc{where $\nu$ is a delay parameter that bounds the average effect of the delay from above. 
The exact meanings of this parameter in different settings are defined in Section~\ref{sec:problem_statement}.}

\noindent {\bf 4.} Lastly, we showcase the adaptability of our analysis for delayed feedback in combinatorial bandits, given certain algorithmic conditions. 
Building on~\cite{nie22_explor_then_commit_algor_for}, we derive regret bounds for a meta-algorithm, highlighting its applicability to other CMAB problems such as submodular bandits with knapsack constraints (See~\cite{nie2023framework}).



On the technical side, we define new generalized notions of delay and introduce the notion of upper tail bounds, which measures the tightness of a family of distributions.
As discussed in Appendix~\ref{apx:related:delay}, algorithms designed for composite anonymous feedback, including those in our study, rely on the concept of repeating actions a sufficient number of times to minimize the impact of delay on the observed reward. 
We employ Bernstein's inequality to control the effect of previous actions on the observed reward of the current action that is being repeated.
This approach enables us to establish an upper bound on regret, expressed in terms of the expected value of the upper tail bound.

Further, we note that, as shown in \cite{tajdini24_nearl_minim_optim_submod_maxim_bandit_feedb}, under certain conditions, as long as we aim to avoid a regret term that is exponential in $k$, $O(T^{2/3})$ is optimal. Thus, the proposed analysis considers delay robustness in the algorithm that is optimal without delay (for algorithms that wish to avoid exponential in $k$ dependence and optimal metric being regret compared to a greedy algorithm). There are no other algorithms that have been shown to be better without delayed feedback for combinatorial bandits with full bandit feedback. Further, we obtain linear dependence with delay, which changes from no impact of delay for small delay ($\nu = O(T^{1/3})$) to linear regret for large delay \nc{($\nu = \Omega(T^{2/3})$)}. Thus, we conjecture that the proposed bound is optimal for any algorithm that does not have run time that is exponential in $k$.  


Through simulations  with synthetic data, we demonstrate that ETCG outperforms other full-bandit methods in the presence of delayed composite anonymous feedback.


\section{Problem Statement}\label{sec:problem_statement}
Let $T$ be the time horizon, $\Omega$ be the ground set of base arms, and $n := |\Omega|$.
Also let $\C{T}$ be a family of probability distributions on non-negative integers.
At each time-step $t \geq 1$, the agent chooses an action $S_t$ from the set 
$\C{S} = \{S | S \subseteq \Omega,\; |S| \leq k\}$,
where $k$ is the given positive integer.

The environment chooses a delay distribution $\delta_t \in \C{T}$.
The observation $x_t$ will be given by the formula
\begin{equation}
\label{eq:observation}
x_t = \sum_{i = 1}^t f_i(S_i) \delta_i(t-i),    
\end{equation}
where $f_t(S)$ is sampled from $F_t(S)$, the stochastic reward function taking its values in $[0, 1]$.
Moreover, we assume that $\B{E}[F_t(S)] = f(S)$, where $f : 2^{\Omega} \rightarrow [0, 1]$ \nc{ is } a monotone and submodular function. We will use $X_t$ to denote the random variable representing the observation at time $t$.


For $\alpha \in (0, 1]$, the $\alpha$-pseudo-regret is defined by
\[
\C{R}_\alpha :=
\sum_{t = 1}^{T} \left( \alpha f(S^*) - f(S_t) \right),
\]
where $S^* := \op{argmax}_{S \in \C{S}} f(S)$ is the optimal action.
Note that the choice of $\alpha = 1$ corresponds to the classical notion of pseudo-regret.
When there is no ambiguity, we will simply refer to $\C{R}_\alpha$ as the $\alpha$-regret or regret.
In the offline problem with deterministic $f$, finding the optimal action $S^*$ is NP-hard.
In fact, for $\alpha > 1 - 1/e$, \cite{feige98} showed that finding an action which is at least as good as $\alpha f(S^*)$ is NP-hard.
However, the standard greedy approach obtains a set which is at least as good as $(1-1/e)f(S^*)$ \cite{nemhauser1978analysis}.
Therefore, throughout this paper, we will focus on minimizing $(1-1/e)$-regret and drop the subscript when there is no ambiguity.

We consider three settings: bounded adversarial delay and unbounded stochastic independent delay, and unbounded stochastic conditionally independent delay, described next.  In the Bounded Adversarial Delay setting, the only restriction is that the delay is bounded, i.e., the reward of each action is spread over the next $d$ time-steps.
In the Unbounded Stochastic Independent Delay setting, we assume that delay distribution changes over time, but does not depend on the action of the agent.
For example, assume each time-step is a day and delay distribution depends on the day of the week (e.g., lower on weekend).
In the Unbounded Stochastic Conditionally Independent Delay, we assume that the delay distribution may change over time and also depend on the selected actions.
For example, assume that each time-step is a day and every weekend, the delay for some specific actions are reduced.


\begin{example}\label{example}
To elaborate on the nature of the delay, let us ignore the combinatorial aspect of the problem for the moment and consider the following setting.
A retailer, that sells both food and computer products, can buy an advertisement slot on an E-commerce platform, e.g., Amazon or eBay.
This is a 2-armed bandit where we assume that the retailer buys an ad slot for a product at each time-step.
We further assume that each time-step is a single day and the only information revealed to the retailer every day is the total added revenue as a result of the advertisements.

A delay distribution is a sequence of real numbers that add to one, e.g., $\delta = (0.9, 0.05, 0.05, 0, \cdots)$.
Such a delay means that $90\%$ of the reward (increase in revenue as a result of the ads) is received immediately, while $5\%$ of the reward is received in each of the next 2 time-steps.
Clearly it is not enough to consider a fixed delay distribution.
Therefore we consider a situation where $\Delta$ is a random variable where $\delta$ is a realization of $\Delta$.

It is reasonable to assume that the effect of an ad for food is more immediately seen in the revenue compared to the effect of an ad for computer products.
Therefore we may consider a setting where $\Delta_{F}$ is a random delay distribution corresponding to food and $\Delta_{C}$ correspond to computer products and $\Delta_F \neq \Delta_C$.
This corresponds to the setting considered in \cite{Wang_Wang_Huang_2021} and \cite{garg19_stoch_bandit_delay_compos_anony_feedb}.

Now assume that a sale for computer products, but not food, is going to start next week.
Modeling this scenario means that $\Delta$ should change over time, but should also depend on the action, since only one of the actions is affected by the sales.
This corresponds to \textit{Unbounded Stochastic Conditionally Independent Delay} considered in our paper.

If we instead assume that the delay changes over time, but does not depend on the arm (for example if the retailer is selling different types of computer products), then this will be \textit{Unbounded Stochastic Independent Delay}.

Finally, if delay is too complicated to be covered by previous settings, then we consider \textit{Bounded Adversarial Delay}.
For example, consider a scenario where different retailers pay the E-commerce platform for advertisement slots, but when the ad is shown depends on the buyers and the actions of other retailers, which can not be known in advance. The boundedness assumption guarantees that for each ad slot purchased, the effect on the revenue of the retailer will be limited to a fixed time, e.g. one month, from the purchase of the ad.
\end{example}


\subsection{Unbounded Stochastic Independent Delay}
In the unbounded stochastic independent delay case, we assume that there is a sequence of random delay distributions $(\Delta_t)_{t=1}^{\infty}$ that is pair-wise independent, such that
\[
X_t = \sum_{i = 1}^t F_i(S_i) \Delta_i(t-i).
\]
In other words, at each time-step $t$, the observed reward is based on all the actions that have been taken in the past and the action taken in time-step $i \leq t$ contributes to the observation proportional to the value of the delay distribution at time $i$, $\Delta_i$, evaluated at $t-i$.
We call this feedback model \textit{composite anonymous unbounded stochastic independent delay feedback}.

To define $\Delta_t$, let $(\delta_i)_{i \in \C{J}}$ be distributions chosen from $\C{T}$, where $\C{J}$ is a finite index set and each 
$\delta_i$ is represented by a vector of its probability mass function. 
Thus, $\delta_i(x) = \B{P}(\delta_i = x)$, for all $x \geq 0$.
Let $P_t$ be a random variables taking values in $\C{J}$, where $P_t(i) = \B{P}(P_t = i)$.
Further, we define $\Delta_t(x) := \sum_{i \in \C{J}} P_t(i) \delta_i(x)$, for all $x \geq 0$.
Finally, $\Delta_t$ is defined as a vector $(\Delta_t(0), \Delta_t(1), \cdots)$.
Note that $\sum_{i = 0}^\infty \Delta_t(i) = 1$.
The expectation of $\Delta_t$ over the randomness of $P_t$ is denoted by $\B{E}_{\C{T}}(\Delta_t)$ which is a distribution given $\delta_i$'s are distributions.

More generally, we may drop the assumption that $\C{J}$ is finite and define $\Delta_t$ more directly as follows.
Each $\Delta_t$ is a random variable taking values in the set $\C{T}$.
In other words, for all $x \geq 0$, the value of $\Delta_t(x) = \Delta_t(\{x\})$ is a random variable taking values in $[0, 1]$ such that $\sum_{i = 0}^\infty \Delta_t(i) = \Delta_t(\{0, 1, 2, \cdots\}) = 1$.
We define $\B{E}_{\C{T}}(\Delta_t)$ as the distribution over the set of non-negative integers for which we have
\[
\forall x \geq 0
,\quad
\B{E}_{\C{T}}(\Delta_t)(\{x\}) = \B{E}_{\C{T}}(\Delta_t(\{x\})) \in [0, 1].
\]

We will also explain these definitions by an example. 
Let $\C{T}$ be a family of distributions supported on $\{0, 1, 2\}$.
We choose $\C{J} = \{1, 2\}$, with $\delta_i$ as the uniform distribution over $\{0, 1\}$
and $\delta_2$ as the uniform distribution over $\{0, 2\}$.
Then, we have $\delta_1(0) = \delta_1(1) = 1/2$ and $\delta_2(0) = \delta_2(2) = 1/2$.
Further, let $P_1$ be a random variable such that $P_1(1) + P_1(2) = 1$.
Then, $\Delta_1(x) = \sum_{i = 1,2} P_1(i) \delta_i(x)$ gives 
$\Delta_1(0) = P_1(1)/2 + P_1(2)/2$, $\Delta_1(1) = P_1(1)/2$ and $\Delta_1(2) = P_1(2)/2$.


Note that the independence implies that $\Delta_t$ can not depend on the action $S_t$, as this action depends on the history of observations, which is not independent from $(\Delta_j)_{j = 1}^{t-1}$.

Without any restriction on the delay distributions, there may not be any reward within time $T$ and thus no structure of the rewards can be exploited.
Thus, we need to have some guarantee that the delays do not escape to infinity.
An appropriate formalization of this idea is achieved using the following tightness assumption.

\begin{assumption}\label{assumption}
The family of distributions $(\B{E}_{\C{T}}(\Delta_t))_{t = 1}^{\infty}$ is tight.
\end{assumption}


Recall that a family $(\delta_i)_{i \in I}$ is called tight if and only if for every positive real number $\epsilon$, there is an integer $j_{\epsilon}$ such that 
$\delta_i( \{ x \geq j_{\epsilon} \} ) \leq \epsilon$, for all $i \in I$. (See e.g. \cite{billingsley1995probability})


\begin{remark}
If $\C{T}$ is tight, then $(\B{E}_{\C{T}}(\Delta_t))_{t = 1}^{\infty}$ is trivially tight.
Note that if $\C{T}$ is finite, then it is tight.
Similarly, if $(\B{E}_{\C{T}}(\Delta_t))_{t = 1}^{\infty}$ is constant and therefore only takes one value, then it is tight.
As a special case, if $(\Delta_t)_{t=1}^{\infty}$ is identically distributed, then $(\B{E}_{\C{T}}(\Delta_t))_{t = 1}^{\infty}$ is constant and therefore tight.
\end{remark}

To quantify the tightness of a family of probability distribution, we define the notion of \textit{upper tail bound}.

\begin{definition}
Let $(\delta_i)_{i \in I}$ be a family of probability distributions over the set of non-negative integers.
We say $\delta$ is an \textit{upper tail bound} for this family if
\[ 
\delta_i( \{ x \geq j \} ) \leq \delta( \{ x \geq j \} ),
\]
for all $i \in I$ and $j \geq 0$.
\end{definition}

In the following result (with proof in Appendix \ref{proof_lem_tightness_equiv_upper_bound}), we show that the  tightness and the existence of upper tail bounds are equivalent.

\begin{lemma}\label{L:tightness_equiv_upper_bound}
Let $(\delta_i)_{i \in I}$ be a family of probability distributions over the set of non-negative integers. 
Then this family is tight, if and only if it has an upper tail bound.
\end{lemma}


A tail upper bound allows us to estimate and bound the effect of past actions on the current observed reward.
More precisely, given an upper tail bound $\tau$ for the family $(\B{E}_{\C{T}}(\Delta_t))_{t = 1}^{\infty}$, the effect of an action taken at time $i$ on the observer reward at $t$ is proportional to $\Delta_i(t-i)$, which can be bounded in expectation by $\tau$.
\[
  \B{E}_{\C{T}}(\Delta_i(t-i)) \leq \B{E}_{\C{T}}(\Delta_i(\{x \geq t-i\})) \leq \tau(\{x \geq t-i\}).
\]
As we will see, only the expected value of the upper tail bound appears in the regret bound.


\subsection{Unbounded Stochastic Conditionally Independent Delay}
In the unbounded stochastic conditionally independent delay case, we assume that there is a family of random delay distributions $\{\Delta_{t, S}\}_{t \geq 1, S \in \C{S}}$ such that for any $S \in \C{S}$, the sequence $(\Delta_{t, S})_{t=1}^{\infty}$ is pair-wise independent and
\[
X_t = \sum_{i = 1}^t F_i(S_i) \Delta_{i, S_i}(t-i).
\]
We call this feedback model \textit{composite anonymous unbounded stochastic conditionally independent delay feedback}.

In this case the delay $\Delta_{t} = \Delta_{t, S_t}$ can depend on the action $S_t$, but conditioned on the current action, it is independent of (some of the) other conditional delays. Similar to the stochastic independent delay setting, we assume that the sequence $\{\B{E}_{\C{T}}(\Delta_{t, S})\}_{t \geq 1, S \in \C{S}}$ is tight.

\begin{remark}\label{R:new_general_delay}
  In previously considered stochastic composite anonymous feedback models (e.g.,~\cite{Wang_Wang_Huang_2021, garg19_stoch_bandit_delay_compos_anony_feedb}),
  the delay distribution is independent of time.
  In other words, every action $S$ has a corresponding random delay distribution $\Delta_S$, and the sequence $(\Delta_{t, S})_{t = 1}^{\infty}$ is independent and identically distributed.
  Therefore, the number of distributions in the set $\{\B{E}_{\C{T}}(\Delta_{t, S})\}_{t \geq 1, S \in \C{S}}$ is less than or equal to the number of arms, which is finite.
  Hence the family $\{\B{E}_{\C{T}}(\Delta_{t, S})\}_{t \geq 1, S \in \C{S}}$ is tight.
\end{remark}


\subsection{Bounded Adversarial Delay}
\begin{algorithm}[H]
  \caption{ETCG algorithm}\label{ALG:main}
  \begin{algorithmic}[1]
    \REQUIRE{ Set of base arms $\Omega$, horizon $T$, cardinality constraint $k$\nc{, exploration parameter $m \geq 1$} }
    \ENSURE{ $n \leq T$  }
    \STATE{ $S^{(0)} \leftarrow \emptyset$, $n \leftarrow |\Omega|$ }
    \FOR{ phase $i \in \{1, 2, \cdots, k \}$ }
      \FOR{ arm $a \in \Omega \setminus S^{(i-1)}$ }
        \STATE{ Play $S^{(i-1)} \cup \{a\}$ arm $m$ times }
        \STATE{ Calculate the empirical mean $\bar{x}_{i, a}$ }
      \ENDFOR
      \STATE{ $a_i \leftarrow \op{argmax}_{a \in \Omega \setminus S^{(i-1)}} \bar{x}_{i, a}$ }
      \STATE{ $S^{(i)} \leftarrow S^{(i-1)} \cup \{a_i\}$ }
    \ENDFOR
    \FOR{ remaining time }
      \STATE{ Play action $S^{(k)}$ }
    \ENDFOR
  \end{algorithmic}
\end{algorithm}


In the bounded adversarial delay case, we assume that there is an integer $d \geq 0$ such that for all $\delta \in \C{T}$, we have $\delta(\{ x > d\}) = 0$. 
Here we have
\[
X_t = \sum_{i = \max\{1, t-d\}}^t F_i(S_i) \delta_i(t-i),
\]
where $(\delta_t)_{t = 1}^{\infty}$ is a sequence of distributions in $\C{T}$ chosen by the environment.
Here we used $\delta$ instead of $\Delta$ to emphasize the fact that these distributions are not chosen according to some random variable with desirable properties.
In fact, the environment may choose $\delta_t$ non-obliviously, that is with the full knowledge of the history up to the time-step $t$.
We call this feedback model \textit{composite anonymous bounded adversarial delay feedback}.


\section{Regret Analysis with Delayed Feedback}

For analyzing the impact of delay, we use the algorithm Explore-Then-Commit-Greedy (ETCG) algorithm, as proposed in \cite{nie22_explor_then_commit_algor_for}.
We start with $S^{(0)} = \emptyset$ in phase $i=0$.
In each phase $i \in \{1, \cdots, k\}$, we go over the list of all base arms $\Omega \setminus S^{(i-1)}$.
For each such base arm, we take the action $S^{(i-1)} \cup \{a\}$ for $m = \lceil (T/n)^{2/3} \rceil$ times and store the empirical mean in the variable $\bar{X}_{i, a}$.
Afterwards, we let $a_i$ to be the base arm which corresponded to the highest empirical mean and define $S^{(i)} := S^{(i-1)} \cup \{a_i\}$.
After the end of phase $k$, we  keep taking the action $S^{(k)}$ for the remaining time. The algorithm is summarized in Algorithm~\ref{ALG:main}. 




We now provide the main results of the paper that shows the regret bound of Algorithm \ref{ALG:main} with delayed composite feedback for different feedback models. We define two main events that control the delay and the randomness of the observation.
Let 
$I = \{ (i, a) \mid 1 \leq i \leq k, a \in \Omega \setminus S^{(i-1)} \}$, 
\nc{
and let $\bar{F}_{i, a}$ and $\bar{X}_{i, a}$ be the averages of $F_t$ and $X_t$ over the steps in the exploration phase where the action $S^{(i-1)} \cup \{a\}$ is taken,
}
and define
\[
\C{E} := \left\{
    |\bar{F}_{i, a} - f(S^{(i-1)}\cup\{a\})| \leq \op{rad}
    \mid 
    (i, a) \in I
\right\}, 
\]
and
\[ 
\C{E}'_d := \left\{
    |\bar{F}_{i, a} - \bar{X}_{i, a}| \leq \frac{2d}{m}
    \mid 
    (i, a) \in I
\right\}, \]
where $\op{rad}, d > 0$ are real numbers that will be specified later.
We may drop the subscript $d$ when it is clear from the context.
When $\C{E}$ happens, the average observed reward associated with each arm stays close \nc{to} its expectation, which is the value of the submodular function.
When $\C{E}'_d$ happens, the average observed reward for each arm remains close to the average total reward associated with playing that arm. The next result bounds the regret as:
\begin{theorem}\label{T:regret_bound_main}
For all $d > 0$\nc{, $m \geq 1$, and $\op{rad} > 0$,} we have
\begin{align*}
\B{E}(\C{R}) 
&\leq 
mnk + 2kT\op{rad} + \frac{4kTd}{m} + 2nkT\exp(-2m\op{rad}^2)\\& + T(1 - \B{P}(\C{E}'_d)).
\end{align*}
\end{theorem}
See Appendix~\ref{apx:base_lemmas} for a detailed proof. To obtain the regret bounds for different settings, we  need to find lower bounds for $\B{P}(\C{E}'_d)$ and use Theorem~\ref{T:regret_bound_main}.

\nc{
Note that if $d > m$, then we have $\frac{4kTd}{m} > T$ and the regret bound becomes $\Omega(T)$.
In particular, in the following theorems where we assume $m = \Theta(T^{2/3})$, the results are only non-trivial if $d = O(T^{2/3})$.
}

\begin{theorem}[Bounded Adversarial Delay]\label{T:regret_uniformly_bounded_delay}
If the delay is uniformly bounded by $d$ \nc{ and $m = \lceil (T/n)^{2/3} \rceil$ }, then we have
\[
\B{E}(\C{R})
= O(kn^{1/3} T^{2/3} (\log(T))^{1/2}) + O(kn^{2/3}T^{1/3} d).
\]
\end{theorem}
\begin{proof}
  The detailed proof is provided in Appendix \ref{apdx_ubd}. Here, we describe the proof outline.
  In this setting, there is an integer $d \geq 0$ such that $\delta_t(\{x > d\}) = 0$, for all $t \geq 1$.
  Therefore, for any $m$ consecutive time-steps $t_{i, a} \leq t \leq t'_{i, a}$, the effect of delay may only be observed in the first $d$ and the last $d$ time-steps.
  It follows that $
    \left|
      \sum_{t=t_{i, a}}^{t'_{i, a}} X_t - \sum_{t=t_{i, a}}^{t'_{i, a}} F_t
    \right| \leq 2d,$ 
  for all $(i, a) \in I$.
  Therefore, in this case, we have $\B{P}(\C{E}'_d) = 1$.
  Note that we are not making any assumptions about the delay distributions.
  Therefore, the delay may be chosen by an adversary with the full knowledge of the environment, the algorithm used by the agent and the history of actions and rewards.
  Plugging this in the bound provided by Theorem~\ref{T:regret_bound_main} completes the proof.
\end{proof}
We note that ${\widetilde{O}}(T^{2/3})$ is the best known bound for the problem in the absence of the delayed feedback, and the result here demonstrate an additive impact of the delay on the regret bounds. 


\begin{theorem}[Stochastic Independent Delay]\label{T:regret_stochastic_independent}
If the delay sequence is stochastic and independent and tight in expectation \nc{ and $m = \lceil (T/n)^{2/3} \rceil$ }, then we have
\[
\B{E}(\C{R})
= O(kn^{1/3}T^{2/3}\log(T))
  + O(kn^{2/3}T^{1/3}\B{E}(\tau)),
\]
where $\tau$ is an upper tail bound for $\{\B{E}_{\C{T}}(\Delta_t)\}_{t = 1}^{\infty}$.
\end{theorem}
\begin{proof}
  The detailed proof is provided in Appendix \ref{apdx_ubed}. Here, we describe the proof outline. 
  We start by defining the random variables 
$    C_{i, a} = \sum_{j = 1}^{t'_{i, a}} \Delta_j(\{x > t'_{i, a} - j\}),$ 
  for all $(i, a) \in I$.
  This random variable measure the effect of actions taken up to $t'_{n, i}$ on the observed rewards after $t'_{n, i}$.
  In fact, we will see that $m|\bar{X}_{i, a} - \bar{F}_{i, a}|$ may be bounded by the sum of two terms.
  One $C_{i, a}$ which bounds the amount of reward that ``escapes" from the time interval $[t_{i, a}, t'_{i, a}]$.
  The second one $C_{i', a'}$, where $(i', a')$ corresponds to the action taken before $S^{(i-1)}\cup\{a\}$.
  This bound corresponds to the total of reward of the past actions that is observed during $[t_{i, a}, t'_{i, a}]$.
  Therefore, in order for the event $\C{E}'_d$ to happen, it is sufficient to have $C_{i, a} \leq d$, for all $(i, a) \in I$.
  Since $C_{i, a}$ is a sum of independent random variables, we may use Bernstein's inequality to see that $\B{P}(C_{i, a} > \B{E}(C_{i, a}) + \lambda)
    \leq \exp\left(-\frac{\lambda^2}{2(\B{E}(\tau) + \lambda/3)}\right).$ It follows from the definition that $\B{E}(C_{i, a}) \leq \B{E}(\tau)$.
  Therefore, by setting $d = \B{E}(\tau) + \lambda$, and performing union bound on the complement of $\C{E}'_d$ gives $\B{P}(\C{E}'_d)\ge 1 - nk\exp\left(-\frac{\lambda^2}{2(\B{E}(\tau) + \lambda/3)}\right)$. Plugging this in Theorem~\ref{T:regret_bound_main} and choosing appropriate $\lambda$ gives us the desired result.
 \if 0 we get
  \begin{align*}
    \B{P}(C_{i, a} > d) 
    \leq \exp\left(-\frac{\lambda^2}{2(\B{E}(\tau) + \lambda/3)}\right).
  \end{align*}
   
  Therefore, we have
  \begin{align*}
    \B{P}(\C{E}'_d) 
    &\geq \B{P}\left( \bigcap_{i, a} \{ C_{i, a} \leq d \} \right) \\
    &= 1 - \B{P}\left( \bigcup_{i, a} \{ C_{i, a} > d \} \right) \\
    &\geq 1 - \sum_{i, a}\B{P}\left( \{ C_{i, a} > d \} \right) \\
    &\geq 1 - \sum_{i, a} \exp\left(-\frac{\lambda^2}{2(\B{E}(\tau) + \lambda/3)}\right) \\
    &\geq 1 - nk\exp\left(-\frac{\lambda^2}{2(\B{E}(\tau) + \lambda/3)}\right).
  \end{align*}
  Plugging this in Theorem~\ref{T:regret_bound_main} and choosing appropriate $\lambda$ gives us the desired result. \fi 
\end{proof}

\begin{theorem}[Stochastic Conditionally Independent Delay]\label{T:regret_stochastic_conditionally_independent}
If the delay sequence is stochastic, conditionally independent and tight in expectation \nc{ and $m = \lceil (T/n)^{2/3} \rceil$ }, then we have
\begin{align*}
\B{E}(\C{R})
= O(k^2n^{4/3}T^{2/3}\log(T))
  + O(k^2n^{5/3}T^{1/3}\B{E}(\tau))
\end{align*}
where $\tau$ is an upper tail bound for $(\B{E}_{\C{T}}(\Delta_{t, S}))_{t \geq 1, S \in \C{S}}$.
\end{theorem}
\begin{proof}
  The detailed proof is provided in Appendix \ref{apdx_ubed} and is similar to the proof of Theorem~\ref{T:regret_stochastic_independent}.
  The main difference is that here we define $C'_{i, a} = \sum_{j = t_{i, a}}^{t'_{i, a}} \Delta_j(\{x > t'_{i, a} - j\}),$  instead of $C_{i, a}$. 
  Note that the sum here is only over the time-steps where the action $S^{(i-1)}\cup\{a\}$ is taken. 
  Therefore $C'_{i, a}$ is the sum of $m$ independent term.
  On the other hand, when we try to bound $m|\bar{X}_{i, a} - \bar{F}_{i, a}|$, we decompose it into the amount of total reward that ``escapes" from the time interval $[t_{i, a}, t'_{i, a}]$ and the contribution of all the time intervals of the form $[t_{i', a'}, t'_{i', a'}]$ in the past.
  Since the total number of such intervals is bounded by $nk$, here we find the probability that $C'_{i, a} \leq \frac{2d}{nk}$ instead of $C_{i, a} \leq d$ as we did in the proof of Theorem~\ref{T:regret_stochastic_independent}.
  This is the source of the multiplicative factor of $nk$ which appears behind the regret bound of this setting when compared to the stochastic independent delay setting.\end{proof}


\section{Beyond Monotone Submodular Bandits}
We note that  \cite{nie2023framework} provided a generalized framework for combinatorial bandits with full bandit feedback, where under a robustness guarantee, explore-then-commit (ETC) based algorithm have been used to get provable regret guarantees. More precisely, let $\C{A}$ be an algorithm for the combinatorial optimization problem of maximizing a function $f : \C{S} \to \B{R}$ over a finite domain $\C{S} \subseteq 2^\Omega$ with the knowledge that $f$ belongs to a known class of functions $\C{F}$.
for any function $\hat{f} : \C{S} \to \B{R}$, let $\C{S}_{\C{A}, \hat{f}}$ denote the output of $\C{A}$ when it is run with $\hat{f}$ as its value oracle.
The algorithm $\C{A}$ called $(\alpha, \delta)$-robust if for any $\epsilon > 0$ and any function $\hat{f}$ such that $|f(S) - \hat{f}(S)| < \epsilon$ for all $S \in \C{S}$, we have
\[
f(S_{\C{A}, \hat{f}}) \geq \alpha f(S^*) - \delta \epsilon.
\]

It is shown in \cite{nie2023framework} that if $\C{A}$ is  $(\alpha, \delta)$-robust, then the C-ETC algorithm achieves $\alpha$-regret bound of $O(N^{1/3} \delta^{2/3} T^{2/3} (\log(T))^{1/2})$, where $N$ is an upper-bound for the number of times $\C{A}$ queries the value oracle (the detailed result and algorithm is given in Appendix \ref{apdx:general}). 
In this work, we show that the result could be extended directly with delayed composite anonymous bandit feedback. The proof requires small changes, and are detailed in Appendix \ref{apdx:general}. If $\C{A}$ is  $(\alpha, \delta)$-robust, then the results with bandit feedback are as follows.  

\begin{theorem}
  If the delay is uniformly bounded by $d$, then we have
  \[
    \B{E}(\C{R}_\alpha)
    = O(N^{1/3} \delta^{2/3} T^{2/3} (\log(T))^{1/2}) + O(N^{2/3} \delta^{1/3} T^{1/3} d).
  \]    
\end{theorem}

\begin{theorem}
  If the delay sequence is stochastic, then we have
  \[
    \B{E}(\C{R}_\alpha)
    = O(N^{1/3} \delta^{2/3} T^{2/3} \log(T))
      + O(N^{2/3} \delta^{1/3} T^{1/3} \B{E}(\tau)),
  \]
  where $\tau$ is an upper tail bound for $(\B{E}_{\C{T}}(\Delta_t))_{t = 1}^{\infty}$.
\end{theorem}

\begin{theorem}
  If the delay sequence is stochastic and conditionally independent, then we have
  \[
    \B{E}(\C{R}_\alpha)
    = O(N^{4/3} \delta^{2/3} T^{2/3} \log(T))
      + O(N^{5/3} \delta^{1/3} T^{1/3} \B{E}(\tau)),
  \]
  where $\tau$ is an upper tail bound for $(\B{E}_{\C{T}}(\Delta_t))_{t = 1}^{\infty}$.
\end{theorem}


This shows that the proposed approach in this paper that deals with feedback could be applied on wide variety of problems. 
The problems that satisfy the robustness guarantee include submodular bandits with knapsack constraints and submodular bandits with cardinality constraints (considered earlier). 


\begin{figure*}[t!]
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_regret_vs_T-LinearF-NoDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F1)-(D1)}
\end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_regret_vs_T-LinearF-StochasticDiscountedDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F1)-(D2)}
\end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_regret_vs_T-LinearF-StochasticIntervalDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F1)-(D3)}
\end{subfigure}

 \medskip
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_regret_vs_T-LinearF-AdvDiscountedDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F1)-(D4)}
\end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_regret_vs_T-LinearF-AdvDiracDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F1)-(D5)}
\end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_regret_vs_T-LinearF-TrueAdvDiracDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F1)-(D6)}
\end{subfigure}
 
 \medskip
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_regret_vs_T-WeightCoverF-NoDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F2)-(D1)}
\end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_regret_vs_T-WeightCoverF-StochasticDiscountedDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F2)-(D2)}
\end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_regret_vs_T-WeightCoverF-StochasticIntervalDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F2)-(D3)}
\end{subfigure}


 \medskip
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_regret_vs_T-WeightCoverF-AdvDiscountedDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F2)-(D4)}
\end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_regret_vs_T-WeightCoverF-AdvDiracDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F2)-(D5)}
\end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_regret_vs_T-WeightCoverF-TrueAdvDiracDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F2)-(D6)}
\end{subfigure}
 
\caption{
This plot shows the average cumulative 1-regret over horizon for each setting in the log-log scale.
The dashed lines are $y = a T^{2/3}$ for $a \in \{ 0.1, 1, 10 \}$.
Note that (F1) is a linear function and (D1) is the setting with no delay.
Moreover, (D2) corresponds to a delay setting where delay distributions are concentrated near zero and decay exponentially. 
}
\label{fig:regret}
\vspace{-.2in}
\end{figure*}
 

\section{Experiments}

In our experiments, { depicted in Figure~\ref{fig:regret}, }we consider two classes of submodular functions (Linear (F1) and Weight Cover (F2)) and and six types of delay (No Delay (D1), two setups of Stochastic Independent Delay (D2, D3), two setups of Stochastic Conditionally Independent Delay (D4, D5), and Adversarial Delay (D6)). For linear function (F1), we choose $F(S) := \frac{1}{k} \sum_{a \in S} g(a) + N^c(0, 0.1)$ where $N^c(0, 0.1)$ is the truncated normal distribution with mean $0$ and standard deviation $0.1$, truncated to the interval $[-0.1, 1.0]$, $n = 20$ and $k = 4$ and choose $g(a)$ uniformly from $[0.1, 0.9]$, for all $a \in \Omega$. For weight cover function (F2), we choose $f_t(S) := \frac{1}{k} \sum_{j \in J} w_t(j) \op{1}_{S \cap C_j \neq \emptyset}$, where $w_t(j) = U([0, j/5])$ be samples uniformly from $[0, j/5]$ for $j \in {1, 2, 3, 4}$, $n = 20$ and $k = 4$, $(C_j)_{j \in J}$ is a partition of $\Omega$ where  $\Omega$ is divided into 4 categories of sizes $6, 6, 6, 2$. Stochastic set cover may be viewed as a simple model for product recommendation, and more details on the function choices is in Appendix \ref{fun_del_detail}. For the delay types, (D2) assumes $\Delta_t(i) = (1 - X_t) X_t ^ i$, where $(X_t)_{t = 1}^{\infty}$ is an i.i.d sequence of random variables with the uniform distribution $U([0.5, 0.9])$ for all $t \geq 1$ and $i \geq 0$. (D3) assumes $\Delta_t$ is a distribution over $[10, 30]$ is sampled uniformly from the probability simplex using the flat Dirichlet distribution for all $t \geq 0$. (D4) assumes $\Delta_t(i) = (1 - Y_t) Y_t ^ i$, where $Y_t = 0.5 + f_t * 0.4 \in [0.5, 0.9]$ for all $t \geq 1$ and $i \geq 0$. (D5) assumes $\Delta_t$ as deterministic taking value at $l_t = \lfloor 20 f_t \rfloor + 10$ for all $t$. (D6) ssumes $\Delta_t$ as deterministic taking value at $l_t = \lfloor 20 x_{t-1} \rfloor + 10$ with $l_1 = 15$ for all $t > 1$. The setups are detailed in Appendix \ref{fun_del_detail}.


We note that this is the first work on stochastic CMAB with delayed composite anonymous feedback, and thus there are no available baselines for this setting.  We use three algorithms designed for CMAB with full-bandit feedback without delay and and algorithm designed for MAB with composite anonymous feedback as the baseline.
\begin{itemize}
	\item \textbf{CMAB-SM}
	\cite{agarwal2022stochastic}
	This algorithm assumes the expected reward functions are Lipschitz continuous of individual base arm rewards.
	CMAB-SM has a theoretical $1$-regret guarantee of $\tilde{O}(T^{2/3})$ with the assumption that if arm $a$ is better than arm $b$, then replacing $b$ by $a$ in any set not including $a$ will give better reward function. 
	
	\item \textbf{DART}
	\cite{agarwal2021dart}
	This algorithm assumes the expected reward functions are Lipschitz continuous of individual base arm rewards and the reward functions have an additional property related to the marginal gains of the base arms. 
	DART has a theoretical $1$-regret guarantee of $\tilde{O}(T^{1/2})$ with the assumption that if arm $a$ is better than arm $b$, then replacing $b$ by $a$ in any set not including $a$ will give better reward function. 
	
	\item \textbf{OG$^o$}
	\cite{Streeter2008AnOA}
	This algorithm is designed for oblivious adversarial setting with submodular rewards.
	Therefore the sequence of monotone and submodular functions is fixed in advance.
	It has an $(1-1/e)$-regret guarantee of $\tilde{O}(T^{2/3})$.
	
	\item \textbf{ARS-UCB}
	\cite{Wang_Wang_Huang_2021}
	This algorithm is designed for MAB with composite anonymous delayed feedback.
	The delay model is a special case of \textit{unbounded stochastic conditionally independent composite anonymous feedback delay} that we described.
	However, they assume that $\Delta_{t, S}$ does not depend on time.
	For our experiments, we use all subsets of $\Omega$ of size $k$ as the set of arms.
	ARS-UCB has a 1-regret guarantee of $\tilde{O}( \binom{n}{k}^{1/2}T^{1/2})$ plus a constant term that depends on delay and the number of its arms. Thus, this regret has an exponential dependence in $k$ which is what we avoid. 
\end{itemize}




 We use $n = 20$ base arms and cardinality constraint $k = 4$.
We run each experiment for different time horizons $T = \{10^2, 10^3, 10^4, 10^5, 10^6\}$.
For each horizon, we run the experiment 10 times.
In these experiments, ETCG outperforms all other baselines for the weighted cover function by almost an order of magnitude.
The linear submodular function satisfies the conditions under which DART and CMAB-SM were designed.
However, the weighed cover function does not satisfy such conditions and therefore more difficult for those algorithms to run.
In both cases, we see that any kind of delay worsens the performance of DART and CMAB-SM compared to ETCG.
OG$^o$ explores actions (including those with cardinality smaller then $k$) with a constant probability, which could account for its lower performance compared to ETCG, DART, and CMAB-SM.


While ARS-UCB does not perform well in these experiments, it should be noted that, given enough time, it should outperform ETCG.
Also note that ARS-UCB has a linear storage complexity with respect to its number of arms.
This translates to an $O(\binom{n}{k})$ storage complexity in the combinatorial setting.
Therefore, even for $n = 50$ and $k = 25$, it would require hundreds of terabytes of storage to run.
In these experiments, we have $n=20$ and $k=4$, so it has only $\binom{20}{4} = 4845$ arms.


While ARS-UCB is considered as baseline for evaluation and is closest setting in state of the art, it does not consider combinatorial bandits. Hence, for any $T<{n\choose k}$, they provide no guarantees with delayed feedback since they simply can not sample each arm at least once. Moreover, even for $n=50$ with a cardinality constraint of $k=25$, they would require hundreds of terabytes of storage to run.
This makes it practically impossible to run ARS-UCB, or any algorithm not designed for the combinatorial setting, on any real world example of any meaningful size.

The next point worth considering is that many existing algorithms, such as DART and CMAB-SM require extra assumptions beyond submodularity and in their setting provide better regret bounds.
These extra conditions are satisfied in the experiment settings we considered which is why they have a better performance than ETCG in the absence of delay.
In other words, our experiments favor existing algorithms that are designed for more specific tasks.
However, we  still see their performance drop below that of ETCG when delay is introduced.
In summary, DART and CMAB-SM are not resilient to delay, even is specific scenarios where they would outperform when no delay is present.

The only remaining algorithm is $OG^o$ which is designed for adversarial feedback setting with no delay which is more general than stochastic feedback with no delay.
It should also be noted $OG^o$ does not repeat actions.
As discussed in Appendix \ref{apx:related:delay}, every algorithm designed for delayed composite anonymous feedback so far uses repeated actions to reduce the noise introduced by the delay.
In fact, it is not easy to imagine how one might design an algorithm for delayed composite anonymous feedback that does not use this idea in some way.
Hence there really is no expectation of good performance for $OG^o$ when delay is introduced. Especially since, when limited to stochastic setting, even in the absence of delay, its performance lags behind ETCG as demonstrated both in our experiments and the experiments of \cite{nie22_explor_then_commit_algor_for}.


\section{Conclusion}
This paper considered the problem of combinatorial multi-armed bandits with stochastic submodular (in
expectation) rewards and delayed composite anonymous bandit feedback and provides first regret bound results for this setup. Three models of delayed feedback: bounded adversarial, stochastic independent, and stochastic conditionally independent are studied, and regret bounds are derived for each of the delay models. The regret bounds demonstrate an additive impact of delay in the regret term. Coming up with a matching lower bound for algorithms that do not have dependence that is exponential in $k$ is open. 



\bibliography{references}


\clearpage
\newpage
\onecolumn
\appendices

\section{Other Related Works}\label{rel_work}
We note that  this is the first work to derive regret bounds for CMAB with submodular and monotone rewards and delayed feedback. Thus, the most related work can be divided into the results for CMAB with submodular and monotone rewards, and that for MAB with delayed feedback, as will be described next. 

\subsection{Combinatorial Submodular Bandits}

CMABs have been widely studied due to multiple applications. While the problem of CMAB is general and there are multiple studies that do not use submodular rewards \cite{agarwal2021dart,agarwal2022stochastic,dani2008stochastic,rejwan2020top}, we consider CMAB with monotone and submodular rewards. The assumption of monotonicity and submodularity in reward functions is common in the literature \cite{streeter2010online,niazadeh2021online,nie22_explor_then_commit_algor_for}. For CMAB with monotone and submodular rewards, without any further constraints, the optimal selection will be the entire set. Thus, additional assumptions are introduced in the model, including cardinality constraint \cite{nemhauser1978analysis} and knapsack constraints \cite{sviridenko2004note}. This paper considers CMAB with submodular and monotone rewards and cardinality constraint. 

Further, we note that feedback pays an important role in CMAB decision making.  CMAB with submodular and monotone rewards and cardinality constraint has been studied with semi-bandit feedback \cite{lin2015stochastic,niazadeh2021online,zhang2019online,zhu2021projection,chen2018projection,takemori2020submodular}. 
The semi-bandit feedback setting provides more information as compared to the full-bandit setting.
The same is true for contextual bandit feedback~\cite{yue2011linear,chen2018contextual} as well.
Here we consider the full-bandit (or bandit) feedback without any additional feedback.
CMAB with submodular and monotone rewards,  cardinality constraint, and full-bandit feedback has been studied in both adversarial setting \cite{niazadeh2021online} and in stochastic setting \cite{nie22_explor_then_commit_algor_for}.
This paper studies the stochastic setting. 

It is worth noting that for submodular bandits, the stochastic reward case is not a special case of the adversarial reward case and the guarantees for the stochastic reward case are not necessarily better than the adversarial reward case.
In the adversarial setting, the environment chooses a sequence of monotone and submodular functions $\{f_1 , \cdots , f_T \}$.
This is incompatible with the stochastic reward setting since we only require the set function $f_t$ to be monotone and submodular in expectation.
Thus, the results on adversarial submodular bandits will not lead to results for the stochastic submodular setting.

These works for CMAB do not study regret bound with delayed feedback, which is the focus of this paper. 



\subsection{Bandits With Delayed Rewards}\label{apx:related:delay}

The bandit problem with (non-anonymous) delayed feedback has been studied extensively~\cite{mesterharm2005line,agarwal2011distributed,desautels2014parallelizing,dudik2011efficient,pmlr-v28-joulani13}.
In the non-anonymous setting, the reward will be delayed and at each time-step, the agent observers a set of the form $\{(t, r_t) \mid t \in I_t\}$ where $I_t$ is a set of time-steps in the past.
In the aggregated anonymous setting, first studied by \cite{pike-burke18_bandit_delay_aggreg_anony_feedb}, the reward for each arm is obtained at some point in the future, so that the agent will receive the aggregated reward for some of the past actions at each time-step.
\cite{pmlr-v75-cesa-bianchi18a} extended the reward model so that the reward of an action is not immediately observed by the agent, but rather spread over at most $d$ consecutive steps in an adversarial way.
However, they also assumed that the bandit is adversarial.
\cite{garg19_stoch_bandit_delay_compos_anony_feedb} considered the stochastic case and provided an algorithm with a sub-linear regret bound of $\tilde{O}(n^{1/2}T^{1/2}) + O(n\log(T)d)$.
In this setting, for each arm $a$, the there is a random distribution $\Delta_a$ over the set $\{0, 1, \cdots, d\}$ and at each time-step, when the agent plays $a$, the delay is sampled from $\Delta_a$.
\cite{Wang_Wang_Huang_2021} also considers unbounded delay and proves the regret bound of $\tilde{O}(n^{1/2}T^{1/2}) + O(\nu)$, where $\nu$ depends on the delay distribution and $n$ but not on $T$.
They also considered the case with adversarial but bounded delay and proved a regret bound of $\tilde{O}(n^{1/2}T^{2/3}) + O(T^{2/3} d)$.
We note in all of the works addressing composite anonymous feedback, including ours, a key idea is to repeat actions enough times so that we can extract meaningful information.
This is not always necessary in other types of delay.
In particular, if delay is not anonymous, there is no need to repeat actions since we will eventually know the reward for each action. 
Except for ETCG of~\cite{nie22_explor_then_commit_algor_for} and more generally, instances of the meta-algorithm C-ETC algorithm of~\cite{nie2023framework}, other algorithms discussed here for combinatoral bandits do not repeat actions and therefore there is little hope of them achieving desirable results in the presence of composite anonymous delay.

Note that, in the submodular setting, any algorithm that does not exploit the combinatorial structure of the arms must take at least every action once which can be suboptimal since the number of arms is at least $\binom{n}{k}$ which grows exponentially.

In this paper, we extend the delay model further by letting the random delay distribution also depend on time (See Example~\ref{example} and Remark~\ref{R:new_general_delay} for more details).


\section{Proof of Lemma~\ref{L:tightness_equiv_upper_bound}}\label{proof_lem_tightness_equiv_upper_bound}

\begin{proof}
If an upper tail bound $\delta$ exists, then we may simply define 
\[
j_{\epsilon} := \min\{j \mid \delta( \{ x \geq j \} ) \leq \epsilon \},
\]
to see that the family is tight.
Next we assume that the family is tight and prove the existence of an upper tail bound.

Let $\delta$ be the measure defined by
\[
\forall j \geq 0
,\quad
\delta(j) := \sup_{i \in I} \delta_i(\{x \geq j\}) - \sup_{i \in I} \delta_i(\{x \geq j+1\}).
\]
Clearly we have $\delta(j) \geq 0$, for all $j \geq 0$.
To show that $\delta$ is a probability distribution, we sum the terms and see that
\begin{align*}
\delta(\{t \mid a \leq t \leq b\}) 
= \sum_{t=a}^{b} \delta(t) 
&= \sum_{t=a}^{b} \left(
\sup_{i \in I} \delta_i(\{x \geq t\}) - \sup_{i \in I} \delta_i(\{x \geq t+1\})
\right) \\
&= \sup_{i \in I} \delta_i(\{x \geq a\}) - \sup_{i \in I} \delta_i(\{x \geq b+1\})
\end{align*}
According to the definition of tightness, for all $\epsilon > 0$ and $b \geq j_{\epsilon}$, we have
\[
\delta(\{t \mid a \leq t \leq b\}) 
= \sup_{i \in I} \delta_i(\{x \geq a\}) - \sup_{i \in I} \delta_i(\{x \geq b+1\})
\geq \sup_{i \in I} \delta_i(\{x \geq a\}) - \epsilon.
\]
Hence we have
\[
\delta(\{t \geq 0\}) 
= \lim_{j \to \infty} \delta(\{t \mid 0 \leq t \leq j\}) 
= 1 - \lim_{j \to \infty} \sup_{i \in I} \delta_i(\{x \geq j+1\})
= 1.
\]
Finally, to see that $\delta$ is indeed an upper tail bound, we note that
\[
\delta(\{t \mid a \leq t \leq b\}) 
= \sup_{i \in I} \delta_i(\{x \geq a\}) - \sup_{i \in I} \delta_i(\{x \geq b+1\})
\leq \sup_{i \in I} \delta_i(\{x \geq a\}).
\]
Therefore
\[
\delta(\{t \geq a\}) 
= \lim_{b \to \infty} \delta(\{t \mid a \leq t \leq b\}) 
\leq \sup_{i \in I} \delta_i(\{x \geq a\}).
\qedhere
\]
\end{proof}


\section{Lemmas used in the proofs}\label{apx:base_lemmas}

In this section, we will provide the Lemmas that will be used in the proof of the main results. Let $t_{i, a}$ denote the first time-step where the action $S^{(i-1)}\cup\{a\}$ is played in the exploration phase and let $t'_{i, a} := t_{i, a} + m - 1$ be the last such time-step.
Therefore, we have
\[
  \bar{F}_{i, a} = \frac{1}{m}\sum_{t = t_{i, a}}^{t'_{i, a}} F_t
  ,\quad
  \bar{X}_{i, a} = \frac{1}{m}\sum_{t = t_{i, a}}^{t'_{i, a}} X_t.
\]
Similarly, the realized value of these random variables are
\[
  \bar{f}_{i, a} = \frac{1}{m}\sum_{t = t_{i, a}}^{t'_{i, a}} f_t
  ,\quad
  \bar{x}_{i, a} = \frac{1}{m}\sum_{t = t_{i, a}}^{t'_{i, a}} x_t.
\]

For any phase $i$ and arm $a \in \Omega \setminus S^{(i-1)}$, define the event
\[ \C{E}_{i, a} := \left\{
    |\bar{F}_{i, a} - f(S^{(i-1)}\cup\{a\})| \leq \op{rad}
  \right\}, \]
where $\op{rad}$ is a non-negative real number to be specified later.
Using these events, we define
\[
  \C{E}_{i} := \bigcap_{a \in \Omega \setminus S^{(i-1)}} \C{E}_{i, a}
  ,\quad
  \C{E} := \bigcap_{i = 1}^k \C{E}_i.
\]


\begin{lemma}\label{L:bound_P_E}
  We have
  \[ \B{P}(\C{E}) \geq 1 - 2nk\exp(-2m\op{rad}^2). \]
\end{lemma}
\begin{proof}
  We have $F_t \in [0, 1]$.
  Therefore, using Hoeffding's inequality, we have
  \begin{align*}
    \B{P}(|\bar{F}_{i, a} - f(S^{(i-1)}\cup\{a\})| > \op{rad}) 
    &= \B{P}\left(
      \left| 
        \sum_{t_{i, a}}^{t'_{i, a}} F_{t} - mf(S^{(i-1)}\cup\{a\}) 
      \right| > m\op{rad} \right) \\
    &\leq 2\exp\left( -\frac{2 (m\op{rad})^2}{m} \right)
    = 2\exp(-2m\op{rad}^2).
  \end{align*}
  Hence
  \begin{align*}
    \B{P}(\C{E}) 
    &= \B{P}\left( \bigcap_{i, a}^k \C{E}_{i, a} \right) \\
    &= 1 - \B{P}\left( \bigcup_{i, a}^k (\C{E}_{i, a})^c \right) \\
    &\geq 1 - \sum_{i, a} \B{P}((\C{E}_{i, a})^c) \\
    &= 1 - \sum_{i, a} \B{P}(|\bar{F}_{i, a} - f(S^{(i-1)}\cup\{a\})| > \op{rad}) \\
    &\geq 1 - \sum_{i, a} 2\exp(-2m\op{rad}^2) \\
    &\geq 1 - 2nk \exp(-2m\op{rad}^2).
    \qedhere
  \end{align*}
\end{proof}



Next we define another set of events where the delay is controlled.
Let
\[ \C{E}'_{d, i, a} := \left\{
    |\bar{X}_{i, a} - \bar{F}_{i, a}| \leq \frac{2d}{m}
  \right\}, \]
for some $d > 0$ which will be specified later.
Similar to above, we use these events to define
\[
  \C{E}'_{d, i} := \bigcap_{a \in \Omega \setminus S^{(i-1)}} \C{E}_{d, i, a}
  ,\quad
  \C{E}'_d := \bigcap_{i = 1}^k \C{E}'_{d, i}.
\]
Note that $\C{E}'_d$ can happen even if the delay is not bounded.
Later in Lemmas~\ref{L:bound_P_E_prime_uniformly_bounded_delay} and~\ref{L:bound_P_E_prime_uniformly_bounded_tail}, we will find lower bounds on the probability of $\C{E}'_d$ in both the adversarial and the stochastic setting.

\begin{lemma}\label{L:f_S_i_minus_f_S_i_minus_one}
  Under the event $\C{E} \cap \C{E}'_d$, for all $1 \leq i \leq k$ and $d > 0$, we have
  \[
    f(S^{(i)}) - f(S^{(i-1)})
    \geq
    \frac{1}{k} \left[
      f(S^*) - f(S^{(i-1)})
    \right]
    - 2 \op{rad} - \frac{4d}{m}.
  \]
\end{lemma}
\begin{proof}
Recall that $a_i$ is the sole element in $S^{(i)} \setminus S^{(i-1)}$.
That is,
\[
a_i = \op{argmax}_{a \in \Omega \setminus S^{(i-1)}} \bar{x}_{i, a}.
\]
Define
\[
a_i^* := \op{argmax}_{a \in \Omega \setminus S^{(i-1)}} f(S^{(i-1)} \cup \{a\}).
\]
Then we have
\begin{align*}
  f(S^{(i)})
  &= f(S^{(i-1)} \cup \{a_i\}) \\
  &\geq \bar{f}_{i, a_i} - \op{rad} 
    &&\t{(definition of } \C{E} \t{)} \\
  &\geq \bar{x}_{i, a_i} - \frac{2d}{m} - \op{rad}
    &&\t{(definition of } \C{E}' \t{)} \\
  &\geq \bar{x}_{i, a_i^*} - \frac{2d}{m} - \op{rad}
    &&\t{(definition of } a_i^* \t{)} \\
  &\geq \bar{f}_{i, a_i^*} - \frac{4d}{m} - \op{rad}
    &&\t{(definition of } \C{E}' \t{)} \\
  &\geq f(S^{(i-1)} \cup \{a_i^*\}) - \frac{4d}{m} - 2\op{rad}.
    &&\t{(definition of } \C{E} \t{)}
\end{align*}
Hence we have
\[
f(S^{(i)}) - f(S^{(i-1)})
\geq f(S^{(i-1)} \cup \{a_i^*\}) - f(S^{(i-1)}) - \frac{4d}{m} - 2\op{rad}.
\]
Therefore
\begin{align*}
  f(S^{(i)}) - f(S^{(i-1)})
  &\geq f(S^{(i-1)} \cup \{a_i^*\}) - f(S^{(i-1)}) - \frac{4d}{m} - 2\op{rad} \\
  &= \op{max}_{a \in \Omega \setminus S^{(i-1)}} f(S^{(i-1)} \cup \{a\})  - f(S^{(i-1)}) - \frac{4d}{m} - 2\op{rad}
    &&\t{(definition of } a_i^* \t{)} \\
  &\geq \op{max}_{a \in S^* \setminus S^{(i-1)}} f(S^{(i-1)} \cup \{a\})  - f(S^{(i-1)}) - \frac{4d}{m} - 2\op{rad}
    && (S^* \subseteq \Omega) \\
  &\geq \frac{1}{|S^* \setminus S^{(i-1)}|} \sum_{a \in S^* \setminus S^{(i-1)}} f(S^{(i-1)} \cup \{a\})  - f(S^{(i-1)}) - \frac{4d}{m} - 2\op{rad}
    && \t{(maximum} \geq \t{mean)} \\
  &= \frac{1}{|S^* \setminus S^{(i-1)}|} \sum_{a \in S^* \setminus S^{(i-1)}} \left[ 
    f(S^{(i-1)} \cup \{a\})  - f(S^{(i-1)})
    \right]  - \frac{4d}{m} - 2\op{rad}  \\
  &\geq \frac{1}{k} \sum_{a \in S^* \setminus S^{(i-1)}} \left[ 
    f(S^{(i-1)} \cup \{a\})  - f(S^{(i-1)})
    \right]  - \frac{4d}{m} - 2\op{rad}
    && (|S^* \setminus S^{(i-1)}| \leq |S^*| = k) \\
  &\geq \frac{1}{k} \left[ 
    f(S^*)  - f(S^{(i-1)})
    \right]  - \frac{4d}{m} - 2\op{rad},
\end{align*}
where the last line follows from a well-known inequality for submodular functions.
\end{proof}

\begin{corollary}\label{C:explotation_regret_bound_per_timestep}
  Under the event $\C{E} \cap \C{E}'_d$, for all $d > 0$, we have
  \[
    f(S^{(k)}) \geq (1 - \frac{1}{e})f(S^*) - 2k \op{rad} - \frac{4kd}{m}.
  \]
\end{corollary}
\begin{proof}
Using Lemma~\ref{L:f_S_i_minus_f_S_i_minus_one}, we have
\[
  f(S^{(i)}) 
  \geq f(S^{(i-1)}) + \frac{1}{k}(f(S^*) - f(S^{(i-1)})) - 2\op{rad} - \frac{4d}{m}
  = \left[  \frac{1}{k} f(S^*) - 2\op{rad} - \frac{4d}{m} \right] + (1 - \frac{1}{k})f(S^{(i-1)}).
\]
Applying this inequality recursively, we get
\begin{align*}
  f(S^{(k)}) 
  &\geq \left[ \frac{1}{k} f(S^*) - 2\op{rad} - \frac{4d}{m} \right] + (1 - \frac{1}{k})f(S^{(k-1)}) \\
  &\geq \left[ \frac{1}{k} f(S^*) - 2\op{rad} - \frac{4d}{m} \right] + (1 - \frac{1}{k})
    \left(
      \left[ \frac{1}{k} f(S^*) - 2\op{rad} - \frac{4d}{m} \right] + (1 - \frac{1}{k})f(S^{(k-2)})
    \right) \\
  &= \left[ \frac{1}{k} f(S^*) - 2\op{rad} - \frac{4d}{m} \right] \sum_{l=0}^1 (1 - \frac{1}{k})^l + (1 - \frac{1}{k})^2 f(S^{(k-2)}) \\
  &\vdots \\
  &\geq \left[ \frac{1}{k} f(S^*) - 2\op{rad} - \frac{4d}{m} \right] \sum_{l=0}^{k-1} (1 - \frac{1}{k})^l + (1 - \frac{1}{k})^k f(S^{(0)}) \\
  &= \left[ \frac{1}{k} f(S^*) - 2\op{rad} - \frac{4d}{m} \right] \sum_{l=0}^{k-1} (1 - \frac{1}{k})^l
\end{align*}
Note that we have
\[
\sum_{l=0}^{k-1} (1 - \frac{1}{k})^l
= \frac{1 - (1 - \frac{1}{k})^k}{1 - (1 - \frac{1}{k})}
= k \left( 1 - (1 - \frac{1}{k})^k \right).
\]
Hence
\begin{align*}
  f(S^{(k)}) 
  &\geq \left[ \frac{1}{k} f(S^*) - 2\op{rad} - \frac{4d}{m} \right] k \left( 1 - (1 - \frac{1}{k})^k \right) \\
  &= \left( 1 - (1 - \frac{1}{k})^k \right) f(S^*) - 
    \left( 2k\op{rad} - \frac{4kd}{m} \right)\left( 1 - (1 - \frac{1}{k})^k \right) \\
  &\geq \left( 1 - (1 - \frac{1}{k})^k \right) f(S^*) - 2k\op{rad} - \frac{4kd}{m}.
\end{align*}
Using the well known inequality $(1 - \frac{1}{k})^k \leq \frac{1}{e}$, we get
\[
  f(S^{(k)}) 
  \geq \left( 1 - \frac{1}{e} \right) f(S^*) - 2k\op{rad} - \frac{4kd}{m}.
  \qedhere
\]
\end{proof}




\begin{lemma}\label{L:regret_conditioned_on_both}
  For all $d > 0$, we have
  \[
    \B{E}(\C{R} | \C{E} \cap \C{E}'_d) \leq mnk + 2kT\op{rad} + \frac{4kTd}{m}.
  \]
\end{lemma}
\begin{proof}
Let $\C{R} = \C{R}_{\t{exploration}} + \C{R}_{\t{exploitation}}$.
The exploration phase is at most $mnk$ steps, therefore we always have 
\[ \C{R}_{\t{exploration}} \leq mnk. \]
At each time-step in the exploitation phase, $(1 - \frac{1}{e})f(S^*) - f(S^{(k)})$ to is added to the expected regret.
Hence we have
\begin{align*}
  \B{E}(\C{R} | \C{E} \cap \C{E}_d') 
  &= \B{E}(\C{R}_{\t{exploration}} | \C{E} \cap \C{E}'_d) + \B{E}(\C{R}_{\t{exploitation}} | \C{E} \cap \C{E}'_d) \\
  &\leq mnk + T \left[ (1 - \frac{1}{e})f(S^*) - f(S^{(k)}) \right] \\
  &\leq mnk + 2kT\op{rad} + \frac{4kTd}{m},
\end{align*}
where we used Corollary~\ref{C:explotation_regret_bound_per_timestep} in the last inequality.
\end{proof}


\begin{theorem}[Theorem~\ref{T:regret_bound_main} in the main text]
  For all $d > 0$, we have
  \[
    \B{E}(\C{R}) 
    \leq 
    mnk + 2kT\op{rad} + \frac{4kTd}{m} + 2nkT\exp(-2m\op{rad}^2) + T(1 - \B{P}(\C{E}'_d)).
  \]
\end{theorem}

\begin{proof}
  Using Lemmas~\ref{L:regret_conditioned_on_both} and~\ref{L:bound_P_E}, we have
  \begin{align*}
    \B{E}(\C{R})
    &= \B{E}(\C{R} | \C{E} \cap \C{E}'_d) \B{P}(\C{E} \cap \C{E}'_d)
    + \B{E}(\C{R} | (\C{E} \cap \C{E}'_d)^c ) \B{P}((\C{E} \cap \C{E}'_d)^c) \\
    &\leq \B{E}(\C{R} | \C{E} \cap \C{E}'_d) + T \B{P}((\C{E} \cap \C{E}'_d)^c) \\
    &= \B{E}(\C{R} | \C{E} \cap \C{E}'_d) + T \B{P}(\C{E}^c \cup (\C{E}'_d)^c) \\
    &\leq \B{E}(\C{R} | \C{E} \cap \C{E}'_d) + T \B{P}(\C{E}^c) + T \B{P}((\C{E}'_d)^c) \\
    &\leq (mnk + 2kT\op{rad} + \frac{4kTd}{m}) + 2nkT\exp(-2m\op{rad}^2) + T(1 - \B{P}(\C{E}'_d)).  \qedhere
  \end{align*}
\end{proof}



\section{Uniformly Bounded Delay}\label{apdx_ubd}

\begin{lemma}\label{L:bound_P_E_prime_uniformly_bounded_delay}
  If delay is uniformly bounded by $d$, then $\B{P}(\C{E}'_d) = 1$.
\end{lemma}

\begin{proof}
  For all $t \leq 0$, let $F_t = 0$ and let $\delta_t$ be any distribution over non-negative integers.
  We have
  \begin{align*}
    \left|
      \sum_{t=t_{i, a}}^{t'_{i, a}} X_t - \sum_{t=t_{i, a}}^{t'_{i, a}} F_t
      \right|
    &= \left|
      \sum_{j = t_{i, a} - d}^{t'_{i, a}} F_j \delta_j({\{t_{i, a} -j \leq x \leq t'_{i, a}-j\}})
      - \sum_{t=t_{i, a}}^{t'_{i, a}} F_t
      \right| \\
    &= \left|
      \sum_{j = t_{i, a} - d}^{t_{i, a}-1} F_j \delta_j({\{t_{i, a} -j \leq x \leq t'_{i, a}-j\}})
      + \sum_{j = t_{i, a}}^{t'_{i, a}} F_j \delta_j({\{x \leq t'_{i, a}-j\}})
      - \sum_{t=t_{i, a}}^{t'_{i, a}} F_t
      \right| \\
    &= \left|
      \sum_{j = t_{i, a} - d}^{t_{i, a}-1} F_j \delta_j({\{t_{i, a} -j \leq x \leq t'_{i, a}-j\}})
      - \sum_{j = t_{i, a}}^{t'_{i, a}} F_j \delta_j({\{x > t'_{i, a}-j\}})
      \right| \\
    &\leq 
      \sum_{j = t_{i, a} - d}^{t_{i, a}-1} \delta_j({\{t_{i, a} -j \leq x \leq t'_{i, a}-j\}})
      + \sum_{j = t_{i, a}}^{t'_{i, a}} \delta_j({\{x > t'_{i, a}-j\}}) \\
    &\leq
      d + \sum_{j = t_{i, a}}^{t'_{i, a}} \delta_j({\{x > t'_{i, a}-j\}}).
  \end{align*}
  Note that for $j \leq t_{i, a} + m - d - 1$, we have
  \[
    \delta_j({\{x > t'_{i, a}-j\}})
    = \delta_j({\{x > t_{i, a}+m-1-j\}})
    \leq \delta_j({\{x > d\}})
    = 0.
  \]
  Therefore we have
  \begin{align*}
    \sum_{j = t_{i, a}}^{t'_{i, a}} \delta_j({\{x > t'_{i, a}-j\}}) 
    &= \sum_{j = \max\{t_{i, a}, t_{i, a} + m-d\}}^{t_{i, a} + m-1} \delta_j({\{x > t'_{i, a}-j\}}) \\
    &\leq (t_{i, a} + m-1) - \max\{t_{i, a}, t_{i, a} + m-d\} + 1 \\
    &= \min\{m, d\} \leq d.
  \end{align*}
  Hence 
  \[
    |\bar{X}_{i, a} - \bar{F}_{i, a}|
    =\frac{1}{m} \left|
      \sum_{t=t_{i, a}}^{t'_{i, a}} X_t - \sum_{t=t_{i, a}}^{t'_{i, a}} F_t
      \right|
    \leq \frac{2d}{m}.
    \qedhere
  \]
\end{proof}


\begin{theorem}[Theorem~\ref{T:regret_uniformly_bounded_delay} in the main text]
  If the delay is uniformly bounded by $d$, then we have
  \[
    \B{E}(\C{R})
    = O(kn^{1/3} T^{2/3} (\log(T))^{1/2}) + O(kn^{2/3}T^{1/3} d).
  \]
\end{theorem}
\begin{proof}
  Using Theorem~\ref{T:regret_bound_main} and Lemma~\ref{L:bound_P_E_prime_uniformly_bounded_delay}, we see that
  \[
    \B{E}(\C{R})
    \leq
    mnk + 2kT\op{rad} + \frac{4kTd}{m} + 2nkT\exp(-2m\op{rad}^2).
  \]
  Let \(\op{rad} := \sqrt{\frac{\log(T)}{m}}\).
  Then
  \[
    \exp(-2m \op{rad}^2) = T^{-2},
  \]
  and
  \begin{align*}
    \B{E}(\C{R})
    &\leq mnk + 2kT\op{rad} + \frac{4kTd}{m} + 2nkT \exp(-2m \op{rad}^2) \\
    &= mnk + 2kT\sqrt{\frac{\log(T)}{m}} + \frac{4kTd}{m} + 2nk/T \\
    &\leq mnk + 2kT\sqrt{\frac{\log(T)}{m}} + \frac{4kTd}{m} + 2k
  \end{align*}
  where we used \(T \geq n\) in the last inequality.
  Since $m = \lceil (T/n)^{2/3} \rceil$, we have $(T/n)^{2/3} \leq m \leq (T/n)^{2/3} + 1$.
  Therefore
  \begin{align*}
    \B{E}(\C{R})
    &\leq mnk + 2kT\sqrt{\frac{\log(T)}{m}} + \frac{4kTd}{m} + 2k \\
    &\leq ((T/n)^{2/3} + 1)nk + 2kT\sqrt{\log(T)/(T/n)^{2/3}} + \frac{4kTd}{(T/n)^{2/3}} + 2k \\
    &= kn^{1/3} T^{2/3} + nk + 2kn^{1/3} T^{2/3} (\log(T))^{1/2}) + 4 kn^{2/3}T^{1/3} d +2k \\
    &\leq 4 kn^{1/3} T^{2/3} (\log(T))^{1/2}) + 4 kn^{2/3}T^{1/3} d + 2k \\
    &= O(kn^{1/3} T^{2/3} (\log(T))^{1/2}) + O(kn^{2/3}T^{1/3} d).
    \qedhere
  \end{align*}
\end{proof}


\section{Unbounded Stochastic Independent Delay}\label{apdx_ubed}

\begin{lemma}\label{L:bound_P_E_prime_uniformly_bounded_tail}
  If $(\Delta_j)_{j = 1}^{\infty}$ is independent and $(\B{E}_{\C{T}}(\Delta_j))_{j = 1}^{\infty}$ is tight, then we have
  \[
    \B{P}(\C{E}'_d) \geq 1 - nk \exp\left(-\frac{\lambda^2}{2(\B{E}(\tau) + \lambda/3)}\right),
  \]
  where $d > 0$ is a real number, $\tau$ is a tail upper bound for the family $(\B{E}_{\C{T}}(\Delta_j))_{j = 1}^{\infty}$ 
  and $\lambda = \max\{0, d - \B{E}(\tau)\}$.
\end{lemma}

\begin{proof}
  If $\lambda = 0$, then the statement is trivially true.
  So we will assume that $\lambda > 0$ and $d = \B{E}(\tau) + \lambda$.
  Define 
  \[
    C_{i, a} = \sum_{j = 1}^{t'_{i, a}} \Delta_j(\{x > t'_{i, a} - j\}).
  \]
  Using the fact that $\tau$ is an upper tail bound for $(\B{E}_{\C{T}}(\Delta_j))_{j = 1}^{\infty}$, 
  we can see that
  \begin{align*}
    \B{E}(C_{i, a})
    &= \sum_{j = 1}^{t'_{i, a}} \B{E}(\Delta_j(\{ x > t'_{i, a} - j \}))
    \leq \sum_{j = 1}^{t'_{i, a}} \tau(\{ x > t'_{i, a} - j \}) \\
    &= \sum_{j = 1}^{t'_{i, a}} \tau(\{ x \geq j \})
    \leq \sum_{j = 0}^{\infty} \tau(\{ x \geq j \})
    = \B{E}(\tau).
  \end{align*}
  Using Bernstein's inequality, we have
  \begin{align*}
    \B{P}(C_{i, a} > d) 
    &= \B{P}(C_{i, a} > \B{E}(\tau) + \lambda) \\
    &\leq \B{P}(C_{i, a} > \B{E}(C_{i, a})+ \lambda) \\
    &\leq \exp\left(-\frac{\lambda^2}{2(\sum_{j = 1}^{t'_{i, a}}\B{E}(\Delta_j(\{ x > t'_{i, a}-j \})^2) + \lambda/3)}\right) \\
    &\leq \exp\left(-\frac{\lambda^2}{2(\sum_{j = 1}^{t'_{i, a}}\B{E}(\Delta_j(\{ x > t'_{i, a} - j \})) + \lambda/3)}\right) \\
    &= \exp\left(-\frac{\lambda^2}{2(\B{E}(C_{i, a}) + \lambda/3)}\right) \\
    &\leq \exp\left(-\frac{\lambda^2}{2(\B{E}(\tau) + \lambda/3)}\right).
  \end{align*}


  We have
  $ X_t = \sum_{j = 1}^t F_j(S_j) \Delta_j(t-j) $.
  Therefore
  \begin{align*}
    m|\bar{X}_{i, a} - \bar{F}_{i, a}|
    &=\left|
      \sum_{t=t_{i, a}}^{t'_{i, a}} X_t - \sum_{t=t_{i, a}}^{t'_{i, a}} F_t
      \right| \\
    &= \left|
      \sum_{j = 1}^{t'_{i, a}} F_j \Delta_j({\{t_{i, a} -j \leq x \leq t'_{i, a}-j\}})
      - \sum_{t=t_{i, a}}^{t'_{i, a}} F_t
      \right| \\
    &= \left|
      \sum_{j = 1}^{t_{i, a}-1} F_j \Delta_j({\{t_{i, a} -j \leq x \leq t'_{i, a}-j\}})
      + \sum_{j = t_{i, a}}^{t'_{i, a}} F_j \Delta_j({\{x \leq t'_{i, a}-j\}})
      - \sum_{t=t_{i, a}}^{t'_{i, a}} F_t
      \right| \\
    &= \left|
      \sum_{j = 1}^{t_{i, a}-1} F_j \Delta_j({\{t_{i, a} -j \leq x \leq t'_{i, a}-j\}})
      - \sum_{j = t_{i, a}}^{t'_{i, a}} F_j \Delta_j({\{x > t'_{i, a}-j\}})
      \right| \\
    &\leq 
      \sum_{j = 1}^{t_{i, a}-1} \Delta_j({\{t_{i, a} -j \leq x \leq t'_{i, a}-j\}})
      + \sum_{j = t_{i, a}}^{t'_{i, a}} \Delta_j({\{x > t'_{i, a}-j\}}) \\
    &\leq 
      \sum_{j = 1}^{t_{i, a}-1} \Delta_j({\{x \geq t_{i, a}-j\}})
      + \sum_{j = 1}^{t'_{i, a}} \Delta_j({\{x > t'_{i, a}-j\}}) \\
    &= \sum_{j = 1}^{t_{i, a}-1} \Delta_j({\{x > t_{i, a}-1-j\}}) + C_{i, a}.
  \end{align*}
  If $t_{i, a} = 1$, then the first sum will be zero and we have
  \[
    m|\bar{X}_{i, a} - \bar{F}_{i, a}| \leq C_{i, a}.
  \]
  Otherwise, there exists $(i', a')$ such that $t'_{i', a'} = t_{i, a} - 1$ and
  \[
    m|\bar{X}_{i, a} - \bar{F}_{i, a}| \leq C_{i', a'} + C_{i, a}.
  \]
  Let $\C{E}^*_{i, a}$ be the event that $C_{i, a} \leq d$ and define
  \[
  \C{E}^* := \bigcap_{i, a} \C{E}^*_{i, a}.
  \]
  Our discussion above shows that we have
  \[
    \B{P}(\C{E}'_d) \geq \B{P}(\C{E}^*).
  \]
  On the other hand, we have
  \begin{align*}
    \B{P}(\C{E}^*) 
    &= \B{P}\left( \bigcap_{i, a} \C{E}^*_{i, a} \right) \\
    &= 1 - \B{P}\left( \bigcup_{i, a} (\C{E}^*_{i, a})^c \right) \\
    &= 1 - \B{P}\left( \bigcup_{i, a} \{ C_{i, a} > d \} \right) \\
    &\geq 1 - \sum_{i, a}\B{P}\left( \{ C_{i, a} > d \} \right) \\
    &\geq 1 - \sum_{i, a} \exp\left(-\frac{\lambda^2}{2(\B{E}(\tau) + \lambda/3)}\right) \\
    &\geq 1 - nk\exp\left(-\frac{\lambda^2}{2(\B{E}(\tau) + \lambda/3)}\right),
  \end{align*}
  which completes the proof.
\end{proof}

\begin{theorem}[Theorem~\ref{T:regret_stochastic_independent} in the main text]
  If the delay sequence is stochastic, then we have
  \[
    \B{E}(\C{R})
    = O(kn^{1/3}T^{2/3}\log(T))
      + O(kn^{2/3}T^{1/3}\B{E}(\tau)),
  \]
  where $\tau$ is an upper tail bound for $(\B{E}_{\C{T}}(\Delta_t))_{t = 1}^{\infty}$.
\end{theorem}

\begin{proof}
  Let $\op{rad} := \sqrt{\frac{\log(T)}{m}}$. Then, using $T \geq n$, we have
  \[
    2nkT\exp(-2m\op{rad}^2)
    = 2nkT\exp(-2\log(T))
    = 2nkT^{-1}
    \leq 2k.
  \]
  We choose $d := \B{E}(\tau) + \max\{6\B{E}(\tau), 2\log(T)\}$
  and 
  $\lambda = d - \B{E}(\tau) = \max\{6\B{E}(\tau), 2\log(T)\}$.
  Then we have
  \begin{align*}
    \exp\left(-\frac{\lambda^2}{2(\B{E}(\tau) + \lambda/3)}\right)
    \leq \exp\left(-\frac{\lambda^2}{2(\lambda/6 + \lambda/3)}\right)
    = \exp(-\lambda)
    \leq \exp(-2\log(T))
    = T^{-2}.
  \end{align*}
  Therefore
  \[
    nkT\exp\left(-\frac{\lambda^2}{2(\B{E}(\tau) + \lambda/3)}\right)
    \leq nkT^{-1} \leq k.
  \]
  So, using Theorem~\ref{T:regret_bound_main} and Lemma~\ref{L:bound_P_E_prime_uniformly_bounded_tail}, we have
  \begin{align*}
    \B{E}(\C{R})
    & \leq 
    mnk + 2kT\op{rad} + \frac{4kTd}{m} + 2nkT\exp(-2m\op{rad}^2) + T(1 - \B{P}(\C{E}'_d)) \\
    & \leq 
    mnk + 2kT\op{rad} + \frac{4kTd}{m} + 2nkT\exp(-2m\op{rad}^2) + nkT\exp\left(-\frac{\lambda^2}{2(\B{E}(\tau) + \lambda/3)}\right) \\
    & \leq
    mnk + 2kT\op{rad} + \frac{4kTd}{m} + 3k \\
    & =
      mnk + 2kT\sqrt{\frac{\log(T)}{m}}
      + 4k\frac{T}{m} \left( \B{E}(\tau) + \max\{6\B{E}(\tau), 2\log(T)\} \right)
      + 3k \\
    & \leq
      mnk + 2kT\sqrt{\frac{\log(T)}{m}}
      + 4k\frac{T}{m} \left( 7\B{E}(\tau) +  2\log(T) \right)
      + 3k.
  \end{align*}

  Since $m = \lceil (T/n)^{2/3} \rceil$, we have $(T/n)^{2/3} \leq m \leq (T/n)^{2/3} + 1$.
  Therefore
  \begin{align*}
    \B{E}(\C{R})
    &\leq
      mnk + 2kT\sqrt{\frac{\log(T)}{m}}
      + \frac{4 k T}{m} \left( 7\B{E}(\tau) +  2\log(T) \right)
      + 3k \\
    &\leq
      ((T/n)^{2/3} + 1)nk + 2kT\sqrt{\frac{\log(T)}{(T/n)^{2/3}}}
      + \frac{4 k T}{(T/n)^{2/3}} \left( 7\B{E}(\tau) +  2\log(T) \right)
      + 3k \\
    &=
      kn^{1/3}T^{2/3} + kn + 2kn^{1/3}T^{2/3}\log(T)^{1/2}
      + 28 kn^{2/3}T^{1/3}\B{E}(\tau)
      + 8 kn^{2/3}T^{1/3}\log(T)
      + 3k \\
    &\leq
      12 kn^{1/3}T^{2/3}\log(T)
      + 28 kn^{2/3}T^{1/3}\B{E}(\tau)
      + 3k \\
    &= O(kn^{1/3}T^{2/3}\log(T))
      + O(kn^{2/3}T^{1/3}\B{E}(\tau)).  \qedhere
  \end{align*}
\end{proof}

\section{Unbounded Stochastic Conditionally Independent Delay}\label{apdx_ubed_ad}

\begin{lemma}\label{L:bound_P_E_prime_uniformly_bounded_tail_arm_dependent}
  If $(\Delta_{j, S})_{j = 1}^{\infty}$ is pairwise independent for all $S \in \C{S}$ and $\{\B{E}(\Delta_{j, S})\}_{j \geq 1, S \in \C{S}}$ is tight, then we have
  \[
    \B{P}(\C{E}'_d) \geq 1 - nk \exp\left(-\frac{\lambda^2}{2(\B{E}(\tau) + \lambda/3)}\right),
  \]
  where $d > 0$ is a real number, $\tau$ is a tail upper bound for the family $\{\B{E}_{\C{T}}(\Delta_{j, S})\}_{j \geq 1, S \in \C{S}}$ 
  and $\lambda = \max\left\{0, \frac{2d}{nk} - \B{E}(\tau)\right\}$.
\end{lemma}

\begin{proof}
  If $\lambda = 0$, then the statement is trivially true.
  So we will assume that $\lambda > 0$ and $d = \frac{nk}{2}(\B{E}(\tau) + \lambda)$.
  Define 
  \[
    C'_{i, a} = \sum_{j = t_{i, a}}^{t'_{i, a}} \Delta_j(\{x > t'_{i, a} - j\}).
  \]
  Note that the sum is only over the time-steps where the action $S^{(i-1)}\cup\{a\}$ is taken. 
  Therefore $C'_{i, a}$ is the sum of $m$ independent term.
  Using the fact that $\tau$ is an upper tail bound for $\{\B{E}_{\C{T}}(\Delta_{j, S})\}_{j \geq 1, S \in \C{S}}$, 
  we can see that
  \begin{align*}
    \B{E}(C'_{i, a})
    &= \sum_{j = t_{i, a}}^{t'_{i, a}} \B{E}(\Delta_j(\{ x > t'_{i, a} - j \}))
    \leq \sum_{j = t_{i, a}}^{t'_{i, a}} \tau(\{ x > t'_{i, a} - j \}) \\
    &\leq \sum_{j = 1}^{t'_{i, a}} \tau(\{ x > t'_{i, a} - j \})
    = \sum_{j = 1}^{t'_{i, a}} \tau(\{ x \geq j \})
    \leq \sum_{j = 0}^{\infty} \tau(\{ x \geq j \})
    = \B{E}(\tau).
  \end{align*}
  Using Bernstein's inequality, we have
  \begin{align*}
    \B{P}\left(C'_{i, a} > \frac{2d}{nk}\right) 
    &= \B{P}(C'_{i, a} > \B{E}(\tau) + \lambda) \\
    &\leq \B{P}(C'_{i, a} > \B{E}(C'_{i, a})+ \lambda) \\
    &\leq \exp\left(-\frac{\lambda^2}{2(\sum_{j = t_{i, a}}^{t'_{i, a}}\B{E}(\Delta_j(\{ x > t'_{i, a}-j \})^2) + \lambda/3)}\right) \\
    &\leq \exp\left(-\frac{\lambda^2}{2(\sum_{j = t_{i, a}}^{t'_{i, a}}\B{E}(\Delta_j(\{ x > t'_{i, a} - j \})) + \lambda/3)}\right) \\
    &= \exp\left(-\frac{\lambda^2}{2(\B{E}(C'_{i, a}) + \lambda/3)}\right) \\
    &\leq \exp\left(-\frac{\lambda^2}{2(\B{E}(\tau) + \lambda/3)}\right).
  \end{align*}


  We have
  $ X_t = \sum_{j = 1}^t F_j(S_j) \Delta_j(t-j) $.
  Therefore
  \begin{align*}
    m|\bar{X}_{i, a} - \bar{F}_{i, a}|
    &=\left|
      \sum_{t=t_{i, a}}^{t'_{i, a}} X_t - \sum_{t=t_{i, a}}^{t'_{i, a}} F_t
      \right| \\
    &= \left|
      \sum_{j = 1}^{t'_{i, a}} F_j \Delta_j({\{t_{i, a} -j \leq x \leq t'_{i, a}-j\}})
      - \sum_{t=t_{i, a}}^{t'_{i, a}} F_t
      \right| \\
    &= \left|
      \sum_{j = 1}^{t_{i, a}-1} F_j \Delta_j({\{t_{i, a} -j \leq x \leq t'_{i, a}-j\}})
      + \sum_{j = t_{i, a}}^{t'_{i, a}} F_j \Delta_j({\{x \leq t'_{i, a}-j\}})
      - \sum_{t=t_{i, a}}^{t'_{i, a}} F_t
      \right| \\
    &= \left|
      \sum_{j = 1}^{t_{i, a}-1} F_j \Delta_j({\{t_{i, a} -j \leq x \leq t'_{i, a}-j\}})
      - \sum_{j = t_{i, a}}^{t'_{i, a}} F_j \Delta_j({\{x > t'_{i, a}-j\}})
      \right| \\
    &\leq 
      \sum_{j = 1}^{t_{i, a}-1} \Delta_j({\{t_{i, a} -j \leq x \leq t'_{i, a}-j\}})
      + \sum_{j = t_{i, a}}^{t'_{i, a}} \Delta_j({\{x > t'_{i, a}-j\}}) \\
    &\leq \sum_{j = 1}^{t_{i, a}-1} \Delta_j({\{x > t_{i, a}-1-j\}}) + C'_{i, a}.
  \end{align*}
  Define
  \[
  I_{i, a} = \{(i', a') \mid t_{i', a'} < t_{i, a}\}.
  \]
  Then we have
  \begin{align*}
    \sum_{j = 1}^{t_{i, a}-1} \Delta_j({\{x > t_{i, a}-1-j\}})
    &= \sum_{(i',a') \in I_{i, a}} \sum_{j = t_{i', a'}}^{t'_{i', a'}} \Delta_j({\{x > t_{i, a}-1-j\}}) \\
    &\leq \sum_{(i',a') \in I_{i, a}} \sum_{j = t_{i', a'}}^{t'_{i', a'}} \Delta_j({\{x > t'_{i', a'}-j\}})
    = \sum_{(i',a') \in I_{i, a}} C'_{i',a'}.
  \end{align*}
  Therefore, we have
  \[
    m|\bar{X}_{i, a} - \bar{F}_{i, a}|
    \leq \sum_{i, a} C'_{i, a}
    \leq nk \max_{i, a}\{C'_{i, a}\}.
  \]
  
  Let $\C{E}^{**}_{i, a}$ be the event that $C_{i, a} \leq \frac{2d}{nk}$ and define
  \[
  \C{E}^{**} := \bigcap_{i, a} \C{E}^{**}_{i, a}.
  \]
  Our discussion above shows that we have
  \[
    \B{P}(\C{E}'_d) \geq \B{P}(\C{E}^{**}).
  \]
  On the other hand, we have
  \begin{align*}
    \B{P}(\C{E}^{**}) 
    &= \B{P}\left( \bigcap_{i, a} \C{E}^{**}_{i, a} \right) \\
    &= 1 - \B{P}\left( \bigcup_{i, a} (\C{E}^{**}_{i, a})^c \right) \\
    &= 1 - \B{P}\left( \bigcup_{i, a} \left\{ C'_{i, a} > \frac{2d}{nk} \right\} \right) \\
    &\geq 1 - \sum_{i, a}\B{P}\left( \left\{ C'_{i, a} > \frac{2d}{nk} \right\} \right) \\
    &\geq 1 - \sum_{i, a} \exp\left(-\frac{\lambda^2}{2(\B{E}(\tau) + \lambda/3)}\right) \\
    &\geq 1 - nk\exp\left(-\frac{\lambda^2}{2(\B{E}(\tau) + \lambda/3)}\right),
  \end{align*}
  which completes the proof.
\end{proof}

\begin{theorem}[Theorem~\ref{T:regret_stochastic_conditionally_independent} in the main text]
  If the delay sequence is stochastic and conditionally independent, then we have
  \begin{align*}
    \B{E}(\C{R})
    &= O(kn^{1/3}T^{2/3}(\log(T))^{1/2}
      + k^2n^{5/3}T^{1/3}\log(T))
      + O(k^2n^{5/3}T^{1/3}\B{E}(\tau)) \\
    &= O(k^2n^{4/3}T^{2/3}\log(T))
      + O(k^2n^{5/3}T^{1/3}\B{E}(\tau))
  \end{align*}
  where $\tau$ is an upper tail bound for $(\B{E}_{\C{T}}(\Delta_t))_{t = 1}^{\infty}$.
\end{theorem}

\begin{proof}
  Let $\op{rad} := \sqrt{\frac{\log(T)}{m}}$. Then, using $T \geq n$, we have
  \[
    2nkT\exp(-2m\op{rad}^2)
    = 2nkT\exp(-2\log(T))
    = 2nkT^{-1}
    \leq 2k.
  \]
  We choose $d := \frac{nk}{2}(\B{E}(\tau) + \max\{6\B{E}(\tau), 2\log(T)\})$
  and 
  $\lambda = \frac{2d}{nk} - \B{E}(\tau) = \max\{6\B{E}(\tau), 2\log(T)\}$.
  Then we have
  \begin{align*}
    \exp\left(-\frac{\lambda^2}{2(\B{E}(\tau) + \lambda/3)}\right)
    \leq \exp\left(-\frac{\lambda^2}{2(\lambda/6 + \lambda/3)}\right)
    = \exp(-\lambda)
    \leq \exp(-2\log(T))
    = T^{-2}.
  \end{align*}
  Therefore
  \[
    nkT\exp\left(-\frac{\lambda^2}{2(\B{E}(\tau) + \lambda/3)}\right)
    \leq nkT^{-1} \leq k.
  \]
  So, using Theorem~\ref{T:regret_bound_main} and Lemma~\ref{L:bound_P_E_prime_uniformly_bounded_tail_arm_dependent}, we have
  \begin{align*}
    \B{E}(\C{R})
    & \leq 
    mnk + 2kT\op{rad} + \frac{4kTd}{m} + 2nkT\exp(-2m\op{rad}^2) + T(1 - \B{P}(\C{E}'_d)) \\
    & \leq 
    mnk + 2kT\op{rad} + \frac{4kTd}{m} + 2nkT\exp(-2m\op{rad}^2) + nkT\exp\left(-\frac{\lambda^2}{2(\B{E}(\tau) + \lambda/3)}\right) \\
    & \leq
    mnk + 2kT\op{rad} + \frac{4kTd}{m} + 3k \\
    & =
      mnk + 2kT\sqrt{\frac{\log(T)}{m}}
      + \frac{2nk^2T}{m} \left( \B{E}(\tau) + \max\{6\B{E}(\tau), 2\log(T)\} \right)
      + 3k \\
    & \leq
      mnk + 2kT\sqrt{\frac{\log(T)}{m}}
      + \frac{2nk^2T}{m} \left( 7\B{E}(\tau) +  2\log(T) \right)
      + 3k.
  \end{align*}

  Since $m = \lceil (T/n)^{2/3} \rceil$, we have $(T/n)^{2/3} \leq m \leq (T/n)^{2/3} + 1$.
  Therefore
  \begin{align*}
    \B{E}(\C{R})
    &\leq
      mnk + 2kT\sqrt{\frac{\log(T)}{m}}
      + \frac{2 n k^2 T}{m} \left( 7\B{E}(\tau) +  2\log(T) \right)
      + 3k \\
    &\leq
      ((T/n)^{2/3} + 1)nk + 2kT\sqrt{\frac{\log(T)}{(T/n)^{2/3}}}
      + \frac{2 n k^2 T}{(T/n)^{2/3}} \left( 7\B{E}(\tau) +  2\log(T) \right)
      + 3k \\
    &=
      kn^{1/3}T^{2/3} + kn + 2kn^{1/3}T^{2/3}\log(T)^{1/2}
      + 14 k^2n^{5/3}T^{1/3}\B{E}(\tau)
      + 4 k^2n^{5/3}T^{1/3}\log(T)
      + 3k \\
    &= O(kn^{1/3}T^{2/3}(\log(T))^{1/2})
      + O(k^2n^{5/3}T^{1/3}\log(T))
      + O(k^2n^{5/3}T^{1/3}\B{E}(\tau)) \\
    &= O(k^2n^{4/3}T^{2/3}\log(T))
      + O(k^2n^{5/3}T^{1/3}\B{E}(\tau)).  \qedhere
  \end{align*}
\end{proof}

\section{ Extension to general combinatorial bandits }\label{apdx:general}

The results of this paper could be generalized to settings beyond monotone submodular bandits with cardinality constraint.
As we will see, instead of these assumptions, we only need a setting where we have an algorithm for the offline problem satisfying a specific notion of robustness.

As before, let $\Omega$ be the set of base arms and let $\C{S}$ be a subset of $2^\Omega$.
Let $\C{F}$ be a class of functions from $\C{S} \to [0, 1]$ where we know that $f \in \C{F}$.
We use $S^*$ to denote the optimal value of $f$.

\begin{definition}[\cite{nie2023framework}]
Let $\C{A}$ be an algorithm for the combinatorial optimization problem of maximizing a function $f : \C{S} \to \B{R}$ over a finite domain $\C{S} \subseteq 2^\Omega$ with the knowledge that $f$ belongs to a known class of functions $\C{F}$.
for any function $\hat{f} : \C{S} \to \B{R}$, let $\C{S}_{\C{A}, \hat{f}}$ denote the output of $\C{A}$ when it is run with $\hat{f}$ as its value oracle.
The algorithm $\C{A}$ called $(\alpha, \delta)$-robust if for any $\epsilon > 0$ and any function $\hat{f}$ such that $|f(S) - \hat{f}(S)| < \epsilon$ for all $S \in \C{S}$, we have
\[
f(S_{\C{A}, \hat{f}}) \geq \alpha f(S^*) - \delta \epsilon.
\]
\end{definition}

In this setting, $N$ is an upper-bound for the number of $\C{A}$'s queries to the value oracle.


In the previous sections, the set $\C{S}$ was the set of all subsets of $\Omega$ with size at most $k$ and $\C{F}$ was the set of monotone submodular functions on $\C{S}$.
Corollary~\ref{C:explotation_regret_bound_per_timestep} simply states that the greedy algorithm is $(1 - 1/e, 2k)$-robust.
If we choose $\C{A}$ to be the offline greedy algorithm, $\alpha = 1-1/e$, $\delta = 2k$ and $N = nk$, then Algorithm~\ref{ALG:CETC} will reduce to Algorithm~\ref{ALG:main}.


\begin{algorithm}[ht]
  \caption{C-ETC algorithm (\cite{nie2023framework}}\label{ALG:CETC}
  \begin{algorithmic}[1]
    \REQUIRE{ Set of base arms $\Omega$, horizon $T$, an offline  $(\alpha, \delta)$-robust algorithm $\C{A}$, and an upper-bound $N$ on the number of $\C{A}$'s queries to the value oracle}
    \ENSURE{ $N \leq T$  }
    
    \STATE{ $m \leftarrow \lceil (\delta T/N)^{2/3} \rceil$ }
    \WHILE{ $\C{A}$ queries the value of some action $S$ }
      \STATE{ Play $S$ arm $m$ times }
      \STATE{ Calculate the empirical mean $\bar{x}$ }
      \STATE{ Return $\bar{x}$ to $\C{A}$ }
    \ENDWHILE
    \FOR{ remaining time }
      \STATE{ Play action $S_{\C{A}}$ output by algorithm $\C{A}$ }
    \ENDFOR
  \end{algorithmic}
\end{algorithm}


The proof only needs minor changes to adapt for Algorithm~\ref{ALG:CETC}.
Lemma~\ref{L:bound_P_E} immediately generalizes to
\[ \B{P}(\C{E}) \geq 1 - 2N\exp(-2m\op{rad}^2), \]
where $nk$ is replaced by $N$.
Instead of Corollary~\ref{C:explotation_regret_bound_per_timestep}, we need the following statement.

\begin{corollary}\label{C:explotation_regret_bound_per_timestep_for_CETG}
  Under the event $\C{E} \cap \C{E}'_d$, for all $d > 0$, we have
  \[
    f(S_{\C{A}}) \geq \alpha f(S^*) - \delta \left( \op{rad} + \frac{2d}{m} \right).
  \]
\end{corollary}
\begin{proof}
Consider a time interval of length $m$, namely $t, t+1, \cdots, t+m-1$, where an action $S$ is repeated and the empirical mean $\bar{x}$ is observed.
We have
\begin{align*}
  m|\bar{x} - f(S)| 
  &= \left| \sum_{i = t}^{t + m-1} (x_t - f(S)) \right| 
  \leq \left| \sum_{i = t}^{t + m-1} (x_t - f_t) \right| 
    + \left| \sum_{i = t}^{t + m-1} (f_t - f(S)) \right|.
\end{align*}
Under the event $\C{E}$, we have 
\[
    \left| \sum_{i = t}^{t + m-1} (f_t - f(S)) \right| \leq m \op{rad},
\]
and under the event $\C{E}_d$, we have
\[
    \left| \sum_{i = t}^{t + m-1} (x_t - f_t) \right| \leq 2d.
\]
Therefore, we have
\[
|\bar{x} - f(S)| \leq \op{rad} + \frac{2d}{m}.
\]
Now the claim follows from the definition of $(\alpha, \delta)$-robustness of $\C{A}$.
\end{proof}

The proofs of Lemma~\ref{L:regret_conditioned_on_both} and Theorem~\ref{T:regret_bound_main} could be applied almost verbatim to give us
  \[
    \B{E}(\C{R}_\alpha) 
    \leq 
    mN + \delta T\op{rad} + \frac{2 \delta  Td}{m} + 2NT\exp(-2m\op{rad}^2) + T(1 - \B{P}(\C{E}'_d)),
  \]
for all $d > 0$.
The results below follow.
\begin{theorem}
  If the delay is uniformly bounded by $d$, then we have
  \[
    \B{E}(\C{R}_\alpha)
    = O(N^{1/3} \delta^{2/3} T^{2/3} (\log(T))^{1/2}) + O(N^{2/3} \delta^{1/3} T^{1/3} d).
  \]    
\end{theorem}

\begin{theorem}
  If the delay sequence is stochastic, then we have
  \[
    \B{E}(\C{R}_\alpha)
    = O(N^{1/3} \delta^{2/3} T^{2/3} \log(T))
      + O(N^{2/3} \delta^{1/3} T^{1/3} \B{E}(\tau)),
  \]
  where $\tau$ is an upper tail bound for $(\B{E}_{\C{T}}(\Delta_t))_{t = 1}^{\infty}$.
\end{theorem}

\begin{theorem}
  If the delay sequence is stochastic and conditionally independent, then we have
  \[
    \B{E}(\C{R}_\alpha)
    = O(N^{4/3} \delta^{2/3} T^{2/3} \log(T))
      + O(N^{5/3} \delta^{1/3} T^{1/3} \B{E}(\tau)),
  \]
  where $\tau$ is an upper tail bound for $(\B{E}_{\C{T}}(\Delta_t))_{t = 1}^{\infty}$.
\end{theorem}

\section{Details on Function, Delay Settings, and Baselines in Evaluations }\label{fun_del_detail}



\textbf{Submodular functions:}

\begin{enumerate}

\item[(F1)] Linear:

Here we assume that $f$ is a linear function of the individual arms.
In particular, for a function $g : \Omega \to [0, 1]$, we define
\[
f(S) := \frac{1}{k} \sum_{a \in S} g(a).
\]
More specifically, we let $n = 20$ and $k = 4$ and choose $g(a)$ uniformly from $[0.1, 0.9]$, for all $a \in \Omega$ and define $F(S) := \frac{1}{k} \sum_{a \in S} g(a) + N^c(0, 0.1)$ where $N^c(0, 0.1)$ is the truncated normal distribution with mean $0$ and standard deviation $0.1$, truncated to the interval $[-0.1, 1.0]$.

\item[(F2)] Weight Cover: 

Here we assume that $(C_j)_{j \in J}$ is a partition of $\Omega$ and there is a weight function $w_t : J \to [0, 1]$.
Then $f_t(S)$ is the sum of the weights of the the indexes $j$ where $C_j \cap S \neq \emptyset$, divided by $k$.
In other words, if $\op{1}$ is the indicator function, then
\[
f_t(S) := \frac{1}{k} \sum_{j \in J} w_t(j) \op{1}_{S \cap C_j \neq \emptyset}.
\]
More specifically, we let $n = 20$ and $k = 4$.
We divide $\Omega$ into 4 categories of sizes $6, 6, 6, 2$ and let $w_t(j) = U([0, j/5])$ be samples uniformly from $[0, j/5]$ for $j \in {1, 2, 3, 4}$.

Stochastic set cover may be viewed as a simple model for product recommendation.
Assume $n$ is the number of the products and each product belongs to exactly one of $c$ categories.
Then the reward will be equal to the sum of the weights of the categories that have been covered by the user divided by $k$.
\end{enumerate}


\textbf{Delay settings:}


\begin{enumerate}
\item[(D1)] No Delay

\item[(D2)] (Stochastic Independent Delay)
 For all $t \geq 1$ and $i \geq 0$, $\Delta_t(i) = (1 - Y_t) Y_t ^ i$, where $(Y_t)_{t = 1}^{\infty}$ is an i.i.d sequence of random variables with the uniform distribution $U([0.5, 0.9])$.
The reward for time-step $t$ will be distributed over $[t, \infty)$ according to $\Delta_t$.
In other words, at each time-step $t$, the agent plays the action $S_t$, then the environment samples $f_t(S_t)$ according to the distribution of $F_t(S_t)$ and samples $y_t$ according to the distribution $U([0.5, 0.9])$. 
Then we have $\delta_{t}(i) = (1 - y_t) y_t^i$ for all $i \geq 0$, which is used in Equation~\ref{eq:observation} to determine the observation.
In this example, the distribution of $\Delta_t$ does not depend on the action chosen by the agent and $(\Delta_t)_{t = 1}^{\infty}$ is i.i.d.

\item[(D3)] (Stochastic Independent Delay)
For all $t \geq 0$, $\Delta_t$ is a distribution over $[10, 30]$ is sampled uniformly from the probability simplex using the flat Dirichlet distribution.
The reward for time-step $t$ will be distributed over $[t+10, t+30]$ according to $\Delta_t$.
In other words, at each time-step $t$, the agent plays the action $S_t$, then the environment samples $f_t(S_t)$ according to the distribution of $F_t(S_t)$ and samples $(\beta_0, \beta_2, \cdots, \beta_{20})$ from the 20-dimensional probability simplex $\{ (z_0, \cdots, z_{20}) \mid z_i \geq 0, \sum z_i = 1 \}$, according to the flat Dirichlet distribution.
Then we have $\delta_{t}(i) = \beta_{i - 10}$ for all $10 \leq i \leq 30$ and $\delta_{t}(i) = 0$ otherwise, which is used in Equation~\ref{eq:observation} to determine the observation.
In this example, the distribution of $\Delta_t$ does not depend on the action chosen by the agent and $(\Delta_t)_{t = 1}^{\infty}$ is i.i.d.

\item[(D4)] (Stochastic Conditionally Independent Delay)
For all $t \geq 1$ and $i \geq 0$, we have $\Delta_t(i) = (1 - Y_t) Y_t ^ i$, where $Y_t = 0.5 + f_t * 0.4 \in [0.5, 0.9]$.
The reward for time-step $t$ will be distributed over $[t, \infty)$ according to $\Delta_t$.
In other words, at each time-step $t$, the agent plays the action $S_t$, then the environment samples $f_t(S_t)$ according to the distribution of $F_t(S_t)$ and picks $y_t = 0.5 + f_t(S_t) * 0.4$.
Then we have $\delta_{t}(i) = (1 - y_t) y_t^i$ for all $i \geq 0$, which is used in Equation~\ref{eq:observation} to determine the observation.
Note that there is no more randomness in delay after the value of $f_t(S_t)$ is samples from $F_t(S_t)$.
Also note that the value of $y_t$ depends on the action of the agent.
In this example $(\Delta_{t, S})_{t \geq 1}$ is pair-wise independent for any $S \in \C{S}$.

\item[(D5)] (Stochastic Conditionally Independent Delay)
At each time-step $t$, a number $l_t$ is chosen from $[10, 30]$ according to the following formula.
\[
l_t = \lfloor 20 f_t \rfloor + 10.
\]
The reward for time-step $t$ will be observed at $t + l_t$.
More specifically, at each time-step $t$, the agent plays the action $S_t$, then the environment samples $f_t(S_t)$ according to the distribution of $F_t(S_t)$ and picks $l_t = \lfloor 20 f_t(S_t) \rfloor + 10$.
Finally, we have $\delta_t(i) = \mathbf{1}_{i = l_t}$.
In other words, the higher the reward, the more it will be delayed.
In this example, delay depends on the action chosen by the agent and $(\Delta_{t, S})_{t \geq 1}$ is pair-wise independent for any $S \in \C{S}$.

\item[(D6)] (Adversarial Delay)
Let $l_1 = 15$ and for all $t > 1$, define $l_t$ according to the following formula.
\[
l_t = \lfloor 20 x_{t-1} \rfloor + 10.
\]
As in the delay (D5), the value of $l_t$ determines the amount of delay, i.e. $\delta_t(i) = \mathbf{1}_{i = l_t}$.
Note that $x_{t-1}$ is the value of the previous observation as described in Equation~\ref{eq:observation}.
In other words, the higher the previous observation, the more the current reward will be delayed.
\end{enumerate}



\section{Experiments with added regret}

In Figure~\ref{fig:added_regret}, we have considered the same functions and delay types as before.
After fixing the function and a delay type, we ran each experiment with and without delay 10 times and plotted the added regret when delay is present.
In these experiments, we see that the added regret for ETCG is consistently relatively low with low variance.
We note that one should be careful when interpreting these plots, since the regret bounds are simply upper bounds and therefore the values shown here are the difference of two values that are bounded from above. 
Specifically, having upper bounds $\C{R}^{\op{delay}} \leq a T^{2/3} + \nu T^{1/3}$ and $\C{R}^{\op{no-delay}} \leq a T^{2/3}$ do not imply $\C{R}^{\op{delay}} - \C{R}^{\op{no-delay}} \leq \nu T^{1/3}$.

\begin{figure}[t!]

 \centering{
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_added_regret_vs_T-LinearF-StochasticDiscountedDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F1)-(D2)}
\end{subfigure}
 \hspace{0.5in}
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_added_regret_vs_T-LinearF-StochasticIntervalDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F1)-(D3)}
\end{subfigure}
 }

 \medskip
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_added_regret_vs_T-LinearF-AdvDiscountedDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F1)-(D4)}
\end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_added_regret_vs_T-LinearF-AdvDiracDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F1)-(D5)}
\end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_added_regret_vs_T-LinearF-TrueAdvDiracDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F1)-(D6)}
\end{subfigure}
 
 \medskip
 
 \centering{
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_added_regret_vs_T-WeightCoverF-StochasticDiscountedDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F2)-(D2)}
\end{subfigure}
 \hspace{0.5in}
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_added_regret_vs_T-WeightCoverF-StochasticIntervalDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F2)-(D3)}
\end{subfigure}
 }


 \medskip
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_added_regret_vs_T-WeightCoverF-AdvDiscountedDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F2)-(D4)}
\end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_added_regret_vs_T-WeightCoverF-AdvDiracDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F2)-(D5)}
\end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.30\textwidth}
     \includegraphics[width=\textwidth]{plots/cum_added_regret_vs_T-WeightCoverF-TrueAdvDiracDelay-100_1000_10000_100000_1000000.pdf}
     \caption{(F2)-(D6)}
\end{subfigure}
 
\caption{
This plot shows the average added cumulative regret over horizon for each setting in the symlog-log scale over 10 runs.
The scale of the y-axis is linear for $|y| \leq 100$ and logarithmic for $|y| > 100$.
The gray dashed lines are $y = a T^{1/3}$ for $a \in \{ 10, 100, 300 \}$.
The cyan dashed lines are $y = \nu T^{1/3}$ where $\nu$ is the corresponding delay coefficient appearing in the regret bounds in Theorems~\ref{T:regret_uniformly_bounded_delay},~\ref{T:regret_stochastic_independent},  and~\ref{T:regret_stochastic_conditionally_independent}.
}
\label{fig:added_regret}
\vspace{0.2in}
\end{figure}

 

\end{document}
