% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{emnlp2021}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{xspace} 
\usepackage{diagbox}
\usepackage{hyperref}
\usepackage{amssymb} 
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{xspace} 
\usepackage{color} 
\usepackage{bm}
\usepackage{colortbl}
\usepackage{setspace}
\usepackage{array}
\usepackage{arydshln} 
\usepackage{makecell}
\usepackage{xcolor,colortbl}
\usepackage{tcolorbox}
\usepackage{listings}
\usepackage{pythonhighlight}
\usepackage{wrapfig,lipsum,booktabs}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\newcommand{\ours}{RepoCoder\xspace}
\newcommand{\ourbenchmark}{RepoEval\xspace}
\newcommand{\davinci}{\textsc{Code}-\textsc{Davinci}-002\xspace}
\newcommand{\textada}{\textsc{Text}-\textsc{Embedding}-\textsc{ada}-002\xspace}
\newcommand{\codegen}{\textsc{CodeGen}\xspace}
\newcommand{\codegenbig}{\textsc{CodeGen}-\textsc{Mono}-6B\xspace}
\newcommand{\codegensmall}{\textsc{CodeGen}-\textsc{Mono}-2B\xspace}
\newcommand{\codegentiny}{\textsc{CodeGen}-\textsc{Mono}-350M\xspace}
\newcommand{\grayline}{\rowcolor[gray]{.90}}
\newcommand{\improve}[1]{{\textcolor{red}{\scriptsize{$#1$}}}}
\newcommand{\colorblue}{\cellcolor[rgb]{0.757,0.867,1}}
\newcommand{\coloryellow}{\cellcolor[rgb]{1,0.925,0.792}}
\newcommand{\improvedouble}[2]{$_{\textcolor{red}{#1}}^{\textcolor[rgb]{0,0.392,0}{#2}}$}

\definecolor{lightblue}{rgb}{0, 1, 1}
\definecolor{grayblue}{rgb}{0.757,0.867,1}
\definecolor{grayyellow}{rgb}{1,0.925,0.792}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{RepoCoder: Repository-Level Code Completion\\ Through Iterative Retrieval and Generation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
\centerline{\makecell{Fengji Zhang$^1$\thanks{\ \ Work done during internship at Microsoft.}~, Bei Chen$^2$, Yue Zhang$^2$, Jin Liu$^1$, Daoguang Zan$^2$,\\ Yi Mao$^2$, Jian-Guang Lou$^2$, Weizhu Chen$^2$}}\\
\centerline{$^1$School of Computer Science, Wuhan University}\\
\centerline{$^2$Microsoft Corporation}\\
\centerline{\tt{\{zhangfengji, jinliu\}@whu.edu.cn, \{beichen, zhayue,}} \\
\centerline{\tt{ v-dazan, maoyi, jlou, wzchen\}@microsoft.com}}
}

\begin{document}
\maketitle
\begin{abstract}
The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose \ours, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model, which allows for the effective utilization of repository-level information for code completion and grants the ability to generate code at various levels of granularity. Furthermore, \ours utilizes a novel iterative retrieval-generation paradigm that bridges the gap between retrieval context and the intended completion target. We also propose a new benchmark \ourbenchmark, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. We test the performance of \ours by using various combinations of code retrievers and generators. Experimental results indicate that \ours significantly improves the zero-shot code completion baseline by over 10\% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of \ours through comprehensive analysis, providing valuable insights for future research. 
\end{abstract}

\section{Introduction}
In real-world software production, it is crucial for developers to be aware of other files within the repository during programming. This raises the challenging \textit{repository-level code completion} task for the research field that automated tools should also take into account the broader context in a repository to complete the unfinished code. The code files within a repository often have interrelated dependencies, such as shared utilities, configurations, and cross-API invocations caused by modularization~\cite{tu2014localness}. And each repository has its own unique naming conventions and coding style~\cite{zou2019does}, which helps to ensure clear readability and maintainability. However, such modularization and customization bring difficulties to automatic code completion tools in several ways. First, a group of methods relies on static code analysis and heuristic rules to locate useful information scattered in the repository~\cite{raychev2014code, svyatkovskiy2019pythia, svyatkovskiy2021fast}, which are limited in usability and the target completion scenario. Meanwhile, studies utilizing language models~\cite{hellendoorn2017deep, svyatkovskiy2020intellicode, ding2022cocomic} for code completion usually depend on labeled data to train their models, making them hard to generalize to unseen and customized repositories.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/framework.pdf}
    \caption{\ours: an iterative retrieval-generation framework for repository-level code completion.}
    \label{figure:framework}
\end{figure}

In this paper, we propose \ours --- a simple, generic, and effective framework for the repository-level code completion task. Utilizing a similarity-based retriever, \ours streamlines the code completion process by eliminating manual engineering efforts and code analysis tools. Additionally, our framework integrates the repository-level information directly into the prompt of the pre-trained language model, granting the ability to generalize to repositories across different domains without the need for model tuning. Utilizing language models also facilitates generating code of higher quality and larger granularity, such as completing lines, API invocations, and function bodies.

\ours incorporates a new approach to bridge the gap between the retrieval context and the intended completion target. As illustrated in Figure~\ref{figure:framework}, our approach involves an iterative retrieval-generation paradigm, where we first perform code retrieval only using the unfinished code and generate an intermediate completion. Next, we perform a second iteration of retrieval utilizing the intermediate code. This allows us to effectively capture the relationship between the input context and the intended completion target, leading to more accurate retrieval in the repository. Figure~\ref{figure:introduction_case} is an example illustrating the rationality of our design, where the model predicts a statement calling the \textit{COLMAP()} API. However, the API name and the parameters are incorrect. After retrieving again from the repository with the predicted statement, we can obtain the correct API signature and successfully complete the code.

% As illustrated in Figure~\ref{figure:framework}, \ours incorporates an iterative retrieval-generation paradigm that utilizes the code generated by the language model. By augmenting the language model with a retriever, it is able to utilize the repository-level context. And during retrieval, we can increase the likelihood of including key indicators in the query by utilizing the model prediction. This is made possible by the pre-trained language model's ability to infer the intent of the unfinished code based on its inherent knowledge from previous training. An example of this process is demonstrated in Figure~\ref{figure:introduction_case}, where the model predicts a statement calling the \textit{\_slave.start()} API. However, directly calling this API before starting \textit{BaseCollector} is incorrect. After retrieving again from the repository with the predicted statement, we can obtain a similar implementation and complete the current function accurately.

Furthermore, we propose the \ourbenchmark benchmark for the repository-level code completion task, which is crafted using the latest high-quality repositories from Github. \ourbenchmark mitigates the lack of established benchmarks in the repository-level scenario and for the first time covers three levels of code completion granularity: line, API invocation, and function body. We also utilize unit tests in the repository for evaluation to overcome the inaccuracy of similarity-based metrics. To rigorously validate the effectiveness of \ours, we conduct experiments using two different retrieval models as well as four pre-trained language models with varying sizes, including \davinci~\cite{chen2021evaluating} and \textsc{CodeGen}-\textsc{Mono}-6B/2B/350M~\cite{nijkamp2022codegen}. Our experimental results demonstrate that \ours consistently and significantly improves the zero-shot code completion performance by over 10\% across all settings. And our iterative retrieval-generation framework consistently improves the performance of vanilla retrieval-augmented generation  with only one retrieval-generation iteration. We also perform a comprehensive analysis of the effectiveness and limitations of \ours, providing valuable insights for future research.

The major contributions of this paper are summarized as follows:
 
\begin{itemize}
    \item We propose \ours, a simple yet effective framework for the repository-level code completion task.
    \item We propose a novel iterative retrieval-generation paradigm to bridge the gap between retrieval context and intended completion target.
    \item We create the \ourbenchmark benchmark for evaluating repository-level code completion, providing varying granularity levels and more accurate evaluation based on unit tests.
    \item We demonstrate via rigorous experimentation that \ours significantly outperforms the zero-shot code completion paradigm and improves the performance of vanilla retrieval-augmented generation.
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/introduction.pdf}
    \caption{An example of the generate-then-retrieve paradigm: the model first predicts to call the \textit{COLMAP()} API and then retrieves the grounding example for the second iteration of prediction.}
    \label{figure:introduction_case}
\end{figure}
\section{Methodology}
\label{section:methodology}
\subsection{Overall Framework}
The task of code completion using a language model $\mathcal{M}$ can be typically characterized as $\hat{Y}=\mathcal{M}(X)$, where $\hat{Y}$ represents the predicted tokens and $X$ represents the unfinished code. When implementing code completion tools for real-world software development, it is imperative to consider the repository-level context, including customized information such as API definitions and identifier names. We introduce \ours, as depicted in Figure \ref{figure:framework}, a simple and generic framework that integrates code generation and retrieval models in an iterative paradigm to tackle the repository-level code completion task.

First, a retrieval database is established by partitioning the repository code files into a set of code snippets $C_{repo}=\{c_1, c_2, \cdots\}$. Next, the first retrieval-generation iteration starts: a code retrieval model $\mathcal{R}$ is utilized to obtain the most relevant code snippets from $C_{repo}$ using the unfinished code $X$ as the query, yielding $C^1_{ret}=\mathcal{R}(C_{repo}, X)$. Subsequently, the language model $\mathcal{M}$ is employed to perform the \textit{retrieval-augmented generation}, yielding the prediction $\hat{Y^1}=\mathcal{M}(C^1_{ret}, X)$. After that, the second retrieval-generation iteration is then executed: the new query is established using $\hat{Y^1}$ for the retrieval, producing another set of relevant code snippets $C^2_{ret}=\mathcal{R}(C_{repo}, X, \hat{Y^1})$, which we refer to as the \textit{generation-augmented retrieval}. Finally, a new prompt is constructed using $C^2_{ret}$, and the final prediction is obtained as $\hat{Y}=\mathcal{M}(C^2_{ret}, X)$. Throughout the entire process, the parameters of $\mathcal{M}$ and $\mathcal{R}$ remain unchanged. And there is no requirement for static code analysis tools or heuristic rules to construct the retrieval database. In the following subsections, we introduce the detailed process of performing generation-augmented retrieval and retrieval-augmented generation.

\subsection{Generation-Augmented Retrieval}
\label{section:retrieval_augmented_generation}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/prompt_format.pdf}
    \caption{A visual example of the \ours prompt format, incorporating both the code snippets retrieved from the global repository and the unfinished code within the local target file.}
    \label{figure:illustrate_prompt}
\end{figure}
The retriever employed within the \ours framework can be any model capable of searching for relevant documents in response to a given query. The retrieval database utilized by \ours is constructed through the use of a \textit{sliding window}, which iteratively scans the files in the repository and extracts contiguous lines of code that fit in the window size, $S_{w}$. The sliding window moves a fixed amount of lines at each iteration, which is the sliding size, $S_{s}$.

In the first iteration of retrieval, the query is formulated using the last $S_{w}$ lines of the unfinished code $X$, and the most similar code snippets $C^1_{ret}=\mathcal{R}(C_{repo}, X)$ are retrieved. However, there exists a gap between the retrieval context and the intended completion target, as the retrieval is based on $X$, yet the intended goal is to continue writing $X$. One possible solution is to modify $C^1_{ret}$ by shifting each code snippet down by $S_{s}$ lines, with the exception of when the bottom of the file is reached. While this method has been reported to be effective in prior work~\cite{lu2022reacc}, indiscriminately shifting all retrieved code snippets regardless of their content is not always appropriate.
To mitigate this issue, we propose the generate-then-retrieve paradigm, which is to augment the retrieval query with the generated code $\hat{Y}^1$. Despite the lack of customized information for new repositories, pre-trained code language models have demonstrated remarkable general-domain understanding and generation abilities. The generated code $\hat{Y}^1$ can serve as valuable supplemental information for the retrieval process despite its uncertain correctness.
% This concept has been previously demonstrated useful in several retrieval benchmarks~\cite{mao2020generation, li2022generation, wang2023query2doc}. 
Therefore, for the second iteration of retrieval, the query is constructed by concatenating the last $(S_{w}-S_{s})$ lines of $X$ and the first $S_{s}$ lines of $\hat{Y}^1$. Ultimately, we can get the grounded retrieval results $C^2_{ret}=\mathcal{R}(C_{repo}, X, \hat{Y^1})$.

\subsection{Retrieval-Augmented Generation}
\label{section:generation_augmented_retrieval}
\begin{table*}[t]
    \centering
    \scalebox{0.85}{
        \begin{tabular}{llccccc}
        \toprule
        \textbf{Repository} & \multicolumn{1}{c}{\textbf{License}} & \textbf{Created} & \textbf{Stars} & \textbf{Py Files} & \textbf{Py Lines} & \textbf{Samples} \\
        \midrule
        \grayline \multicolumn{7}{c}{Line and API Invocation Completion Datasets} \\
        rl & MIT License & 2022-02-01 & $873$ & $165$ & $59,522$ & $400$ \\
        ACE & Apache V2.0 & 2022-11-23 & $299$ & $425$ & $66,497$ & $400$  \\
        vizier & Apache V2.0 & 2022-02-16 & $688$ & $188$ & $43,393$ & $400$  \\
        fortuna & Apache V2.0 & 2022-11-17 & $373$ & $168$ & $18,738$ & $400$  \\
        evaluate & Apache V2.0 & 2022-03-30 & $1,082$ & $180$ & $21,418$ & $400$ \\
        diffusers & Apache V2.0 & 2022-05-30 & $9,521$ & $305$ & $98,181$ & $400$  \\
        nerfstudio & Apache V2.0 & 2022-05-31 & $3,151$ & $157$ & $27,289$ & $400$ \\
        FedScope & Apache V2.0 & 2022-03-24 & $811$ & $443$ & $48,545$ & $400$ \\
        \midrule
        \grayline \multicolumn{7}{c}{Function Body Completion Dataset}\\
        trlx & MIT License & 2022-10-03 & $1,723$ & $55$ & $9,991$ & $46$ \\
        imagen & MIT License & 2022-05-23 & $6,160$ & $14$ & $7,324$ & $67$ \\
        tracr & Apache V2.0 & 2022-12-01 & $284$ & $56$ & $9,110$ & $146$ \\
        betty & Apache V2.0 & 2022-06-29 & $230$ & $96$ & $20,114$ & $36$ \\
        lightmmm & Apache V2.0 & 2022-02-10 & $353$ & $36$ & $9,676$ & $64$ \\
        inspection & Apache V2.0 & 2022-05-05 & $304$ & $16$ & $2,532$ & $32$ \\
        omnivore & CC BY-NC 4.0 & 2022-01-20 & $459$ & $66$ & $11,797$ & $22$ \\
        redframes & BSD-2-Clause & 2022-08-21 & $283$ & $49$ & $3,881$ & $42$ \\
        \bottomrule
        \end{tabular}
    }
    \caption{The repositories used for our \ourbenchmark benchmark, statistics from Github API as of January 2023. \textit{Py Files} and \textit{Py Lines} represent the total number of Python source files and non-empty code lines. \textit{Samples} indicate the number of extracted test samples. For the sake of space, the full repository identifiers are listed in Appendix~\ref{appendix:details_of_repositories}.}
    \label{table:repo_info}
\end{table*}

The generator utilized in the \ours framework can be any pre-trained language model capable of predicting the subsequent tokens given a specified prompt. As previously stated, it is essential to incorporate the \textit{global} context from the repository $C_{repo}$ and the \textit{local} context in the target file for code completion. Therefore, our goal is to find the optimal method of prompt construction, $g_\theta$, that maximizes the likelihood $P_{g_\theta}(Y|X, C_{repo}, \mathcal{R}, \mathcal{M})$, where $Y$ represents the ground truth completion.

Specifically, \ours retrieves the most relevant code examples, $C_{ret}$, from the repository and concatenates them to the unfinished code $X$. And to facilitate readability and comprehension, a prompt template is created that seamlessly integrates $X$ and $C_{ret}$, as demonstrated in Figure \ref{figure:illustrate_prompt}. The retrieved code snippets are presented in ascending order based on their similarity scores to the query. Each code snippet is designated with its original file path, and the maximum number of code snippets included in the prompt, $K$, is dependent upon the available prompt length. Eventually, the prompt contains both the local and global information needed for code completion.

\section{Benchmark Construction}
The task of code completion within a software repository is a prevalent scenario for software development. Therefore, we propose a novel \ourbenchmark benchmark to evaluate code completions tools in the repository-level scenario. Our benchmark is crafted using the latest high-quality repositories from Github and covers three levels of code completion granularity: line, API invocation, and function body. We also utilize unit tests in the repository to evaluate the correctness of completed functions instead of simply using similarity-based metrics. \ourbenchmark will be publicly available, and each sample in the benchmark is labeled with the corresponding source repository, file path, line numbers, and ground truth completion. Full copies of the repositories are included for unit test execution and further analysis.

To construct \ourbenchmark, we first carefully create a collection of Python repositories from Github that meet the following criteria: with open-source license; created after January 1, 2022; non-fork original repository; more than $100$ stars; greater than 80\% files written in Python; having explicit unit tests. Furthermore, we randomly select the repositories to prevent potential biases and construct three distinct datasets, including line, API invocation, and function body completion. Further information on the selected repositories can be found in Table \ref{table:repo_info}.

\paragraph{Line completion:} In adherence to the conventions of code completion benchmarks~\cite{lu2021codexglue, lu2022reacc}, we implement the line completion scenario. First, according to the above-mentioned criteria, we select $8$ repositories that vary in size and cover different domains. Then we randomly select $200$ lines to complete from each repository, ensuring the lines are non-repetitive, not code comments, and each line contains at least $5$ tokens. Eventually, a total of $1600$ test samples are generated for the line completion dataset. 

\paragraph{API Invocation Completion:} We also choose to test the API completion scenario, especially intra-repository APIs. It is a harder problem than the completion of built-in or third-party APIs due to the lack of customized training data~\cite{hellendoorn2019code}. We utilize the same group of repositories in the line dataset and parse the target repositories to locate invocations of intra-repository APIs. From these candidates, we then randomly select $200$ non-repetitive API invocations from each repository, resulting in a total of $1600$ test samples for the API invocation completion dataset.

\paragraph{Function Body Completion:} 
Besides the line and API completion evaluation, we also test the function body completion ability, which requires executing the unit tests included in the repository. While running tests can be time-consuming and computationally costly. Therefore, we randomly select another set of smaller-scale and easy-to-deploy repositories. We locate the functions that are covered by unit tests and select function bodies with $3$ to $30$ lines of code to complete. This results in a total of $455$ test samples for the function body completion dataset. 

\section{Experimental Setup}
\begin{table*}[t]
    \centering
    \begin{subtable}[h]{0.49\linewidth}
        \scalebox{0.77}{
            \begin{tabular}{lllll}
            \toprule
            \multirow{3}{*}{{\textbf{Method}}} & \multicolumn{2}{c}{{\textbf{API Dataset}}} & \multicolumn{2}{c}{{\textbf{Line Dataset}}} \\
            \cmidrule(lr){2-3}
            \cmidrule(lr){4-5}
            & \multicolumn{1}{c}{EM} & \multicolumn{1}{c}{ES} &\multicolumn{1}{c}{EM} & \multicolumn{1}{c}{ES}\\
            \midrule
            \grayline \multicolumn{5}{c}{\davinci}\\
            Oracle & \coloryellow $50.13$ & \coloryellow $76.19$ & \coloryellow $59.06$ & \coloryellow $76.48$ \\
            Zero-Shot & $32.50$ & $61.52$ & $38.69$ & $62.58$ \\
            RG-1 & $47.75$ & $73.30$ & $55.94$ & $74.34$ \\
            \ours & \colorblue $48.19$~\improve{$15.69$} & \colorblue $74.60$~\improve{$13.08$} & \colorblue $58.00$~\improve{$19.31$} & \colorblue $75.32$~\improve{$12.74$} \\
            \midrule
            \grayline \multicolumn{5}{c}{\codegenbig}\\
            Oracle & \coloryellow $41.00$ & \coloryellow $67.44$ & \coloryellow $48.44$ & \coloryellow $70.15$ \\
            Zero-Shot & $26.50$ & $56.50$ & $34.06$ & $59.81$ \\
            RG-1 & $37.38$ & $63.71$ & $45.50$ & $68.37$ \\
            \ours & \colorblue $39.25$~\improve{$12.75$} & \colorblue $65.14$~\improve{$8.64$} & \colorblue $47.00$~\improve{$12.94$} & \colorblue $69.08$~\improve{$9.27$} \\
            \midrule
            \grayline \multicolumn{5}{c}{\codegensmall}\\
            Oracle & \coloryellow $39.38$ & \coloryellow $66.41$ & \coloryellow $47.13$ & \coloryellow $68.94$ \\
            Zero-Shot & $25.75$ & $56.39$ & $33.00$ & $57.97$ \\
            RG-1 & $35.75$ & $63.23$ & $44.31$ & $66.74$ \\
            \ours & \colorblue $37.50$ ~\improve{$11.75$} & \colorblue $64.18$~\improve{$7.79$} & \colorblue $46.63$~\improve{$13.63$} & \colorblue $68.12$~\improve{$10.15$} \\
            \midrule
            \grayline \multicolumn{5}{c}{\codegentiny}\\
            Oracle & \coloryellow $35.00$ & \coloryellow $62.71$ & \coloryellow $44.63$ & \coloryellow $66.15$ \\
            Zero-Shot & $22.00$ & $51.62$ & $29.00$ & $54.33$ \\
            RG-1 & $31.94$ & $59.29$ & $41.50$ & $64.12$ \\
            \ours & \colorblue $33.88$ ~\improve{$11.88$} & \colorblue $60.89$~\improve{$9.27$} & \colorblue $42.94$~\improve{$13.94$} & \colorblue $64.93$~\improve{$10.60$} \\
            \bottomrule
            \end{tabular}
        }
        \caption{Using the sparse retriever.}
        \label{table:line_api_sparse}
    \end{subtable}
    \begin{subtable}[h]{0.49\linewidth}
    \scalebox{0.77}{
        \begin{tabular}{lllll}
            \toprule
            \multirow{3}{*}{{\textbf{Method}}} & \multicolumn{2}{c}{{\textbf{API Dataset}}} & \multicolumn{2}{c}{{\textbf{Line Dataset}}} \\
            \cmidrule(lr){2-3}
            \cmidrule(lr){4-5}
            & \multicolumn{1}{c}{EM} & \multicolumn{1}{c}{ES} &\multicolumn{1}{c}{EM} & \multicolumn{1}{c}{ES}\\
            \midrule
            \grayline \multicolumn{5}{c}{\davinci}\\
            Oracle & \coloryellow $49.81$ & \coloryellow $75.82$ & \coloryellow $58.00$ & \coloryellow $75.99$ \\
            Zero-Shot & $32.50$ & $61.52$ & $38.69$ & $62.58$ \\
            RG-1 & $46.94$ & $73.41$ & $55.00$ & $73.74$ \\ 
            \ours & \colorblue $48.63$~\improve{$16.13$} & \colorblue $74.54$~\improve{$13.02$} & \colorblue $56.94$~\improve{$18.25$} & \colorblue $74.79$~\improve{$12.21$} \\
            \midrule
            \grayline \multicolumn{5}{c}{\codegenbig}\\
            Oracle & \coloryellow $40.31$ & \coloryellow $66.51$ & \coloryellow $47.56$ & \coloryellow $69.85$ \\
            Zero-Shot & $26.50$ & $56.50$ & $34.06$ & $59.81$ \\
            RG-1 & $37.63$ & $63.81$ & $44.19$ & $67.40$ \\
            \ours & \colorblue $39.88$~\improve{$13.38$} & \colorblue $65.38$~\improve{$8.88$} & \colorblue $45.50$~\improve{$11.44$} & \colorblue $68.42$~\improve{$8.61$} \\
            \midrule
            \grayline \multicolumn{5}{c}{\codegensmall}\\
            Oracle & \coloryellow $38.19$ & \coloryellow $65.34$ & \coloryellow $46.25$ & \coloryellow $68.27$ \\
            Zero-Shot & $25.75$ & $56.39$ & $33.00$ & $57.97$ \\
            RG-1 & $35.75$ & $62.77$ & $43.44$ & $65.90$ \\
            \ours & \colorblue $37.88$ ~\improve{$12.13$} & \colorblue $64.03$~\improve{$7.64$} & \colorblue $45.69$~\improve{$12.69$} & \colorblue $67.76$~\improve{$9.79$} \\
            \midrule
            \grayline \multicolumn{5}{c}{\codegentiny}\\
            Oracle & \coloryellow $34.38$ & \coloryellow $61.66$ & \coloryellow $42.81$ & \coloryellow $65.53$ \\
            Zero-Shot & $22.00$ & $51.62$ & $29.00$ & $54.33$ \\
            RG-1 & $32.50$ & $59.72$ & $40.81$ & $64.00$ \\
            \ours & \colorblue $33.94$ ~\improve{$11.94$} & \colorblue $61.00$~\improve{$9.38$} & \colorblue $42.81$~\improve{$13.81$} & \colorblue $65.00$~\improve{$10.67$} \\
            \bottomrule
            \end{tabular}
        }
        \caption{Using the dense retriever.}
        \label{table:line_api_dense}
    \end{subtable}
    \caption{Performance comparison on the line and API completion datasets. RG-1 represents the method with only one Retrieval-Generation iteration. Results present the averaged performance of each method as evaluated by Exact Match (EM) and Edit Similarity (ES) scores. Numbers are shown in percentage(\%), and the red colored values highlight \ours's improvements over the zero-shot baseline.}
    \label{table:line_api_result}
\end{table*}

\subsection{Implementation Details}
\label{section:implementation_details}
\paragraph{Retriever:} We evaluate two distinct retrieval methods for \ours. The first method is a sparse bag-of-words model, which transforms the query and candidate code snippets into sets of tokens and calculates their similarity using the Jaccard index~\cite{jaccard1912distribution}. The second method is a dense text embedding model, which encodes the code snippets into embedding vectors and calculates their cosine similarity. For our experiments, we utilize a lightweight yet powerful embedding model, \textada, accessed through the API provided by OpenAI\footnote{\url{https://platform.openai.com/docs/guides/embeddings}}.

\paragraph{Generator:} We test four pre-trained language models with varying code generation capabilities for \ours. The first model, \davinci, is a state-of-the-art commercial code generation model with billions of trainable parameters and has been pre-trained on an extensive corpus of code. Access to \davinci is obtained through the API provided by OpenAI\footnote{\url{https://platform.openai.com/docs/models/codex}}. The second model, \codegen, is a state-of-the-art open-source code generation model that has multiple published versions with varying model sizes and training data. In our experiments, we utilize \codegenbig, \codegensmall, and \codegentiny, all of which were fine-tuned on the Python corpus, with $6$B, $2$B, and $350$M parameters, respectively.

\paragraph{Hyper-parameters:} Our experiments carefully consider various hyper-parameters to optimize the performance. The maximum number of tokens for the input prompt combined with output prediction is $4,096$ for \davinci and $2,048$ for \codegen. Furthermore, after thorough hyper-parameter tuning, the length of the retrieved code snippets is set to occupy half of the prompt length. Also, for line and API completion, the maximum number of tokens in the generated completion, $\hat{Y}$, as well as the line length of the sliding window, $S_{w}$, and the sliding size, $S_{s}$, are set to $100$, $20$, and $10$, respectively. For the function body completion, these values are set to $500$, $50$, and $10$, respectively. The maximum number of the retrieved code snippets in the prompt $K$ is $10$.

\subsection{Methods for Comparison}

\paragraph{Zero-shot Baseline:}Previous studies~\cite{chen2021evaluating, nijkamp2022codegen, chen2022codet} have demonstrated the effectiveness of utilizing large pre-trained language models to perform code generation in a zero-shot manner when conditioned upon the given context. Additionally, it has been established that the utilization of intra-file context is valuable for code completion scenarios~\cite{clement2021long}. It should be noted that while the unfinished code is included in the prompt, a portion of it may remain omitted due to the limited prompt length. In \ours, we mitigate such an issue by adding the omitted context to the retrieval database. And we build the zero-shot code completion baseline by filling the prompt with intra-file code up to the maximum length. 

\paragraph{Oracle Method:} A seminal contribution of RepoCode is the utilization of generation-augmented retrieval to bridge the gap between the retrieval and the intended completion target. To demonstrate the efficacy of this approach, we devise an oracle retrieval-augmented generation method for comparison. It performs one iteration of retrieval for relevant code snippets, $C^{gt}_{ret}$, by utilizing the last $S_{w}-S_{s}$ lines of $X$ and the first $S_{s}$ lines of the ground truth code, $Y$. Subsequently, the completion code, $\hat{Y}$, is generated through $\mathcal{M}(C^{gt}_{ret}, X)$. This methodology enables us to achieve the performance upper bound of \ours, conditioned on $\mathcal{R}$ and $\mathcal{M}$.

\subsection{Evaluation Metrics}
\paragraph{Similarity-based Evaluation:} 
Adhering to established practices in code completion research~\cite{lu2021codexglue, lu2022reacc}, our line and API completion datasets are evaluated using the Exact Match (EM) and Edit Similarity (ES) metrics. The EM score is binary and takes the value of $1$ if the predicted code exactly matches the ground truth code, otherwise, it is $0$. The ES metric is a finer-grained measure, calculated as $ES = 1 - \frac{Lev(\hat{Y}, Y)}{\max(|\hat{Y}|, |Y|)}$, where $Lev$ represents the Levenshtein distance~\cite{levenshtein1966binary}.

\paragraph{Execution-based Evaluation:}
We employ unit tests in the repository to evaluate the function body completion dataset, which is more reliable than similarity-based metrics to evaluate functional correctness. While collecting unit tests is time-consuming, and previous studies in code generation mainly focus on programming problems from competition websites~\cite{hendrycks2021measuring, li2022competition}. We focus on a more realistic scenario and utilize the unit tests in Github repositories to test the completed functions. We execute the generated code and report the Pass Rate (PR), where PR is $1$ if the code can pass all the corresponding test cases, and $0$ otherwise.

\section{Experimental Results}
\subsection{Line and API Completion Datasets}
We compare the performance of \ours to the zero-shot baseline, oracle method, as well as the vanilla retrieval-augmented approach with only one Retrieval-Generation iteration (RG-1). We test four pre-trained language models and two retrieval methods on the line and API completion datasets. The results, listed in Table \ref{table:line_api_sparse} and \ref{table:line_api_dense}, reveal that \ours consistently and significantly improves the zero-shot performance on both datasets across all model sizes and retrieval methods. The absolute improvements in the Exact Match (EM) and Edit Similarity (ES) scores surpass $10\%$ and $7\%$, respectively. In comparison to the oracle and RG-1 method, \ours also demonstrates competitive results, consistently outperforming RG-1 across all settings and getting close to the oracle performance, exhibiting exceptional reliability. Furthermore, our results reveal that the \codegen model with $350$M parameters, when integrated with \ours, even outperforms the zero-shot \davinci model. And the greater performance gain of the \davinci model through the use of \ours, in comparison to that of the \codegen models, serves as a testament that superior code generation models can benefit more from the iterative retrieval-generation process. Finally, the results of our comparisons between retrievers show that the simple sparse retriever has equivalent performance to the dense retriever, further emphasizing the robustness and effectiveness of \ours when applied across various code generators and retrievers. 

\subsection{Function Completion Dataset}
\iffalse
\begin{table}[t]
    \centering
    \scalebox{0.8}{
        \begin{tabular}{lccc}
            \toprule
            \multirow{3}{*}{\textbf{Repository}} & \multicolumn{3}{c}{\textbf{Correct Samples}} \\
            \cmidrule(lr){2-4}
            & Oracle & Zero & RepoCoder \\
            \midrule
            trlx  & \coloryellow $28$ & $18$ & \colorblue $27$~\improve{+$9$}  \\
            imagen  & \coloryellow $40$ & $25$ & \colorblue $39$~\improve{+$14$}  \\
            tracr  & \coloryellow $67$ & $51$ & \colorblue $64$~\improve{+$13$}  \\
            betty  & \coloryellow $21$ & $14$ & \colorblue $20$~\improve{+$6$}  \\
            lightmmm  & \coloryellow $22$ & $10$ & \colorblue $21$~\improve{+$11$}  \\
            inspection  & \coloryellow $13$ & $12$ & \colorblue $13$~\improve{+$1$}  \\
            omnivore  & \coloryellow $5$ & $4$ & \colorblue $5$~\improve{+$1$}  \\
            redframes  & \coloryellow $8$ & $4$ & \colorblue $6$~\improve{+$2$}  \\
            \midrule
            \multicolumn{1}{l}{\textbf{Pass Rate}} & \coloryellow $44.84$ & $30.33$ & \colorblue $42.86$~\improve{$12.53$}  \\
            \bottomrule
        \end{tabular}
    }
    \caption{Performance comparison on the function completion dataset using the sparse retriever and \davinci. Results present the correctly completed samples of each method as evaluated by using test cases, with numbers in red highlighting \ours's improvements over the zero-shot baseline.}
\end{table}
\fi

\begin{table}[t]
    \centering
    \scalebox{0.8}{
        \begin{tabular}{lcllll}
            \toprule
            \multirow{3}{*}{\small \textbf{Repository}} & \multirow{3}{*}{\small \textbf{Num.}} & \multicolumn{4}{c}{\textbf{Pass Rate}} \\
            \cmidrule(lr){3-6}
            & & \multicolumn{1}{c}{Oracle} & \multicolumn{1}{c}{Zero} & \multicolumn{1}{c}{RG-1} & \multicolumn{1}{c}{RepoCoder} \\
            \midrule
            trlx & $46$ & \coloryellow $60.87$ & $39.13$ & $60.87$ & \colorblue $58.70$~\improve{$19.57$}  \\
            imagen & $67$ & \coloryellow $59.70$ & $37.31$ & $56.72$ & \colorblue $58.21$~\improve{$20.90$}  \\
            tracr & $146$ & \coloryellow $45.89$ & $34.93$ & $41.78$ & \colorblue $43.84$~\improve{$8.91$}  \\
            betty & $36$ & \coloryellow $58.33$ & $38.89$ & $50.00$ & \colorblue $55.56$~\improve{$16.67$}  \\
            lightmmm & $64$ & \coloryellow $34.38$ & $15.63$ & $31.25$ & \colorblue $32.81$~\improve{$17.18$}  \\
            inspection & $32$ & \coloryellow $40.63$ & $37.50$ & $40.63$ & \colorblue $40.63$~\improve{$3.13$}  \\
            omnivore & $22$ & \coloryellow $22.73$ & $18.18$ & $18.18$ & \colorblue $22.73$~\improve{$4.55$}  \\
            redframes & $42$ & \coloryellow $19.05$ & $9.52$ & $16.67$ & \colorblue $14.29$~\improve{$4.77$}  \\
            \midrule
            \multicolumn{1}{c}{Overall} & $455$ & \coloryellow $44.84$ & $30.33$ & $41.54$ & \colorblue $42.86$~\improve{$12.53$}  \\
            \bottomrule
        \end{tabular}
    }
    \caption{Performance comparison on the function completion dataset using the sparse retriever and \davinci. RG-1 is with only one Retrieval-Generation iteration. Results present the correctly completed samples of each method as evaluated by using test cases. Numbers are shown in percentage(\%), and the red colored values highlight \ours's improvements over the zero-shot baseline. \textit{Num.} represents the number of samples from each repository.}
    \label{table:function_level}
\end{table}

We evaluate the performance of \ours on the function body completion dataset. To account for the increased difficulty of function body completion, we use the most powerful \davinci model for the experiment. We then choose to use the sparse retriever as it has a similar performance to the dense retriever according to results on line and API datasets. Our evaluation results on the function completion dataset are listed in Table~\ref{table:function_level}. We can observe that the performance of \ours is similar to our findings on line and API completion datasets. In most repositories, \ours demonstrates a significant improvement over the zero-shot baseline and has better performance than using only one Retrieval-Generation iteration. Moreover, the performance of \ours is close to the oracle method, again demonstrating the effectiveness of our approach.


\section{Analysis}
\subsection{Extending \ours to More Iterations}

\begin{table}[t]
    \centering
    \scalebox{0.77}{
        \begin{tabular}{ccccccc}
        \toprule
        \multirow{3}{*}{\textbf{Dataset}} & \multirow{3}{*}{{\textbf{Metric}}} & \multirow{3}{*}{\textbf{Oracle}} & \multicolumn{4}{c}{\textbf{RG Iterations}} \\
        \cmidrule(lr){4-7}
        & & & \multicolumn{1}{c}{$1$} & \multicolumn{1}{c}{$2$} & \multicolumn{1}{c}{$3$} & \multicolumn{1}{c}{$4$}\\
        \midrule
        \grayline \multicolumn{7}{c}{\davinci}\\
        \multirow{2}{*}{API} & EM & \coloryellow $50.13$ & $47.75$ & \colorblue $48.19$ &  \textbf{49.63} &  $49.38$ \\
        & ES & \coloryellow $76.19$ & $73.30$ & \colorblue $74.60$ &  \textbf{75.28} &  $75.00$ \\
        \cmidrule(lr){1-7}
        \multirow{2}{*}{Line} & EM & \coloryellow $59.06$ & $55.94$ & \colorblue $58.00$ &  $58.06$ &  \textbf{58.13} \\
        & ES & \coloryellow $76.48$ & $74.34$ & \colorblue $75.32$ &  \textbf{75.55} &  $75.45$ \\
        \midrule
        \grayline \multicolumn{7}{c}{\codegenbig}\\
        \multirow{2}{*}{API} & EM & \coloryellow $41.00$ & $37.38$ & \colorblue $39.25$ &  $39.56$ &  \textbf{39.63} \\
        & ES & \coloryellow $67.44$ & $63.71$ & \colorblue $65.14$ &  $65.32$ &  \textbf{65.53} \\
        \cmidrule(lr){1-7}
        \multirow{2}{*}{Line} & EM & \coloryellow $48.44$ & $45.50$ & \colorblue $47.00$ &  \textbf{47.50} &  $46.75$ \\
        & ES & \coloryellow $70.15$ & $68.37$ & \colorblue $69.08$ &  \textbf{69.66} &  $68.93$ \\   
        \midrule
        \grayline \multicolumn{7}{c}{\codegensmall}\\
        \multirow{2}{*}{API} & EM & \coloryellow $39.38$ & $35.75$ & \colorblue $37.50$ &  \textbf{38.31} &  $37.94$ \\
        & ES & \coloryellow $66.41$ & $63.23$ & \colorblue $64.18$ &  \textbf{64.62} &  $64.36$ \\
        \cmidrule(lr){1-7}
        \multirow{2}{*}{Line} & EM & \coloryellow $47.13$ & $44.31$ & \colorblue $46.63$ &  $46.56$ &  \textbf{46.75} \\
        & ES & \coloryellow $68.94$ & $66.74$ & \colorblue $68.12$ &  $68.01$ &  \textbf{68.16} \\
        \midrule
        \grayline \multicolumn{7}{c}{\codegentiny}\\
        \multirow{2}{*}{API} & EM & \coloryellow $35.00$ & $31.94$ & \colorblue \textbf{33.88} &  $33.56$ &  $33.25$ \\
        & ES & \coloryellow $62.71$ & $59.29$ & \colorblue \textbf{60.89} &  $60.81$ &  $60.34$ \\
        \cmidrule(lr){1-7}
        \multirow{2}{*}{Line} & EM & \coloryellow $44.63$ & $41.50$ & \colorblue $42.94$ &  \textbf{43.25} &  $43.06$ \\
        & ES & \coloryellow $66.15$ & $64.12$ & \colorblue $64.93$ &  $65.20$ &  \textbf{65.23} \\   
        \bottomrule
        \end{tabular}
    }
    \caption{Comparison of results using different Retrieval-Generation (RG) iterations in line and API evaluation with the sparse retriever. The scores are calculated as an average of all test samples, and numbers are shown in percentage(\%). The highest scores are emphasized in boldface. The columns in the yellow and blue background represent the scores achieved by the oracle method and \ours, respectively. }
    \label{table:varying_r_g_iteration}
\end{table}

\begin{figure*}[t]
    \centering
    \begin{subfigure}[h]{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/context_count_v2.pdf}
        \caption{}
        \label{fig:distribution_on_code_count}
    \end{subfigure}
    \begin{subfigure}[h]{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/peer_overlap_v2.pdf}
        \caption{}
        \label{fig:distribution_on_peer_overlap}
    \end{subfigure}
    \begin{subfigure}[h]{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/gt_overlap_v2.pdf}
        \caption{}
        \label{fig:distribution_on_gt_overlap}
    \end{subfigure}
    \caption{Distribution of the test samples according to (a) the number of code snippets included in prompts, (b) line overlap among the code snippets, (c) token overlap between the code snippets and the ground truth completion. The corresponding Exact Match scores are shown for (b) and (c).}
    \label{fig:code_snippet_statistics}
\end{figure*}

\ours employs the iterative retrieval-generation process to incorporate repository-level information during generation and to bridge the gap between retrieval context and the intended completion target.
We perform an exploratory study on whether additional retrieval-generation iterations could further improve the performance. Specifically, we recursively employ the generation-augmented retrieval and retrieval-augmented generation as introduced in Section~\ref{section:methodology}, resulting in the variants with three (RG-3) and four (RG-4) retrieval-generation iterations.
We evaluate the performance of extended RG iterations on both the line and API completion datasets using four language models and the sparse retriever. The results, which also include the oracle method, are listed in Table \ref{table:varying_r_g_iteration}. Our results indicate that \ours, with extended RG iterations, consistently outperforms the vanilla single-iteration retrieval-augmented approach across all settings. However, as the number of RG iterations increases, the performance does not continue to improve. This may attribute to the lack of newly introduced grounding information for three or more retrieval-generation iterations.


\subsection{Statistics on Retrieved Code Snippets}
As detailed in Section~\ref{section:generation_augmented_retrieval}, \ours integrates retrieved code snippets into the prompt to provide repository-level context. We have demonstrated this integration leads to improved results. To further examine the impact of the retrieved code snippets, we conduct a study of the key factors involved in the retrieval augmentation. Our results, shown in Figure~\ref{fig:code_snippet_statistics}, reveal the distributions of test samples created using \davinci and the sparse retriever on the API completion dataset in terms of various key factors and their influence on the prediction performance, as measured by the Exact Match score.

Figure~\ref{fig:distribution_on_code_count} provides a distribution of the test samples based on the number of code snippets in the prompt. The number of included code snippets is limited by the length of the prompt, with a minimum of $4$ code snippets and most test samples including $6$ to $8$ code snippets. The maximum number of code snippets is set to $10$, as determined by the hyper-parameter $K$. Figure~\ref{fig:distribution_on_peer_overlap} delivers the distribution of test samples based on the ratio of duplicated lines among the retrieved code snippets. Results indicate a positive correlation between higher line overlap and better performance of \ours, as higher line overlap is often indicative of more relevant and commonly found code snippets in the repository. Figure~\ref{fig:distribution_on_gt_overlap} shows the distribution of test samples in terms of the ratio of duplicated tokens between the retrieved code snippets and the ground truth completion. The results demonstrate a generally high overlap, with most of the retrieved code snippets having over $70\%$ token overlap against the ground truth completion. And we can find a positive correlation between \ours's performance and the ground truth overlap. These findings highlight the effectiveness of the retrieval process.

\subsection{Context Locations of Effective Retrieval}
\begin{table}[t]
    \centering
    \scalebox{0.8}{
        \begin{tabular}{lcc}
        \toprule
        \multicolumn{1}{l}{\textbf{}} & \textbf{API} & \textbf{Line} \\
        \midrule
        \grayline \multicolumn{3}{c}{Context Locations} \\
        Imported & $8.95\%$ & $4.53\%$ \\
        Current File & $6.54\%$ & $6.64\%$ \\
        Current Directory & $59.00\%$ & $47.72\%$ \\
        Similar Import & $86.50\%$ & $82.11\%$ \\
        Similar Name & $65.06\%$ & $56.94\%$  \\
        Others & $2.73\%$ & $5.80\%$ \\
        \midrule
        \grayline \multicolumn{3}{c}{Number of Samples}\\
        Test Samples & $297$ & $357$ \\
        Code Snippets & $1,866$ & $2,364$ \\
        \bottomrule
        \end{tabular}
    }
    \caption{Context locations of the retrieved code snippets in the cases where \ours outperforms zero-shot code completion using \davinci on the line and API completion datasets.}
    \label{table:root_cause}
\end{table}

The retrieved code snippets in the prompt bring relevant context from other files in the repository. To understand the impact of different context locations, we perform another study on the retrieved code snippets, as detailed in Table~\ref{table:root_cause}. Specifically, we first extract test samples that are successfully predicted by \ours but not by the zero-shot baseline, using \davinci and the sparse retriever on both line and API completion datasets. Out of $357$ and $297$ eligible test samples, we identify  $2,364$ and $1,866$ retrieved code snippets for the line and API completion datasets, respectively.
Next, we locate the original source of the code snippets. Inspired by \citet{shrivastava2022repository}, we use a classification scheme of five distinct file locations: 
1. Imported - code from a file imported by the target file;
2. Current File - code from the excluded content of the target file;
3. Current Directory - code from a file in the same directory as the target file;
4. Similar Import - code from a file sharing at least one intra-project API with the target file;
5. Similar Name - code from a file with a file name sharing at least one token with the target file (assuming snake-case style file names).

Our results show that the majority of code snippets are located within our defined categories, with only $5.8\%$ and $2.73\%$ of code snippets from the line  and API completion datasets originating from undefined locations, suggesting the rationality of our categorization scheme. Furthermore, the vast majority of code snippets originate from files with ``Similar Import'', ``Similar Name'', or ``Current Directory'' locations, highlighting the importance of the context information in code completion tasks. Additionally, we observe that code snippets retrieved for API completion are more likely to originate from the files categorized in our scheme compared to line completion, indicating a stronger need for information obtained from other files in API completion scenarios.

\subsection{Code Duplication in Repositories}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.89\linewidth]{figures/duplication_and_improvement_v2.pdf}
    \caption{Correlation between the absolute performance improvements by \ours over the zero-shot baseline and the duplication ratios of the repositories.}
    \label{figure:duplicate}
\end{figure}

\iffalse
\begin{table}[h]
    \centering
    \scalebox{0.77}{
        \begin{tabular}{lccccc}
        \toprule
        \multirow{3}{*}{\textbf{Repository}} & \multirow{3}{*}{\textbf{Duplicate}} & \multicolumn{2}{c}{\textbf{API Dataset}} & \multicolumn{2}{c}{\textbf{Line Dataset}} \\
        \cmidrule{3-4}
        \cmidrule{5-6}
        &  & EM & ES & EM & ES \\
        \midrule
        rl & $4.56\%$ & $3.50$ & $2.12$ & $12.00$ & $9.18$ \\
        ACE & $18.58\%$ & $22.50$ & $17.39$ & $28.00$ & $17.87$ \\
        vizier & $4.88\%$ & $7.00$ & $4.48$ & $6.50$ & $2.62$ \\
        fortuna & $18.80\%$ & $18.50$ & $13.44$ & $25.50$ & $17.98$ \\
        evaluate & $13.93\%$ & $5.50$ & $17.53$ & $13.50$ & $10.94$ \\
        diffusers & $53.96\%$ & $35.50$ & $22.94$ & $35.00$ & $22.57$ \\
        nerfstudio & $8.01\%$ & $12.50$ & $11.53$ & $10.00$ & $6.00$ \\
        FederatedScope & $12.79\%$ & $20.50$ & $15.22$ & $24.00$ & $14.72$ \\   
        \bottomrule
        \end{tabular}
    }
    \caption{\ours's absolute improvement values over the zero-shot baseline and the ratio of duplicated code lines in each repository.}
    \label{table:duplication}
\end{table}
\fi

Intuitively, the performance of our \ours might be positively correlated with the code duplication ratio of a repository since \ours utilizes similarity-based retrieval to find code exemplars. To assess the relationship, we calculate the code duplication ratio of the repositories by determining the ratio of duplicated code lines to the total code lines and then compare the duplication ratios to the absolute performance improvements achieved by \ours. The results, as seen in Figure~\ref{figure:duplicate}, demonstrate a correlation between \ours's performance and the code duplication ratio, as measured by the Exact Match (EM) metric on the line and API completion datasets using \davinci and the sparse retriever. In particular, the repository ``diffusers'' exhibits the highest duplication ratio and a performance improvement of over $35\%$ on both datasets. Meanwhile, the low duplication ratios of ``rl'' and ``vizier'' result in lower performance for \ours. However, it is important to note that the correlation between \ours's performance and the code duplication ratio is not absolute. For example, though ``FedScope'' and ``evaluate'' share similar duplication ratios, they have different performance gains. This could be due to our rough estimation of the duplication ratio based on counting identical lines, or the ability of language models to learn from the context and perform well even without exact similar code as examples.


\subsection{Study on Failed Cases}
\begin{table}[t]
    \centering
    \scalebox{0.75}{
        \begin{tabular}{cccccccc}
        \toprule
        \textbf{Method} & \textbf{Zero} & \textbf{} & \textbf{RG-1} & \textbf{} & \textbf{RG-2} & \textbf{} & \textbf{Oracle} \\
        \midrule \grayline \multicolumn{8}{c}{\davinci} \\
        API & +$520$ & \improvedouble{+275}{-31} & +$764$ & \improvedouble{+47}{-40} & \colorblue +$771$ & \improvedouble{+54}{-23} & \coloryellow +$802$ \\
        \cmidrule{1-8}
        Line & +$619$ & \improvedouble{+315}{-39} & +$895$ & \improvedouble{+63}{-30} & \colorblue +$928$ & \improvedouble{+44}{-27} & \coloryellow +$945$ \\
        \cmidrule{1-8}
        Func & +$138$ & \improvedouble{+66}{-15} & +$189$ & \improvedouble{+22}{-16} & \colorblue +$195$ & \improvedouble{+15}{-6} & \coloryellow +$204$ \\
        \midrule \grayline \multicolumn{8}{c}{\codegenbig} \\
        API & +$424$ & \improvedouble{+218}{-44} & +$598$ & \improvedouble{+62}{-32} & \colorblue +$628$ & \improvedouble{+56}{-28} & \coloryellow +$656$ \\
        \cmidrule{1-8}
        Line & +$545$ & \improvedouble{+247}{-64} & +$728$ & \improvedouble{+58}{-34} & \colorblue +$752$ & \improvedouble{+60}{-37} & \coloryellow +$775$ \\
        \midrule \grayline \multicolumn{8}{c}{\codegensmall} \\
        API & +$412$ & \improvedouble{+211}{-51} & +$572$ & \improvedouble{+63}{-35} & \colorblue +$600$ & \improvedouble{+62}{-32} & \coloryellow +$630$ \\
        \cmidrule{1-8}
        Line & +$528$ & \improvedouble{+237}{-56} & +$709$ & \improvedouble{+78}{-41} & \colorblue +$746$ & \improvedouble{+60}{-52} & \coloryellow +$754$ \\
        \midrule \grayline \multicolumn{8}{c}{\codegentiny} \\
        API & +$352$ & \improvedouble{+200}{-41} & +$511$ & \improvedouble{+56}{-25} & \colorblue +$542$ & \improvedouble{+47}{-29} & \coloryellow +$560$ \\
        \cmidrule{1-8}
        Line & +$464$ & \improvedouble{+250}{-50} & +$664$ & \improvedouble{+72}{-49} & \colorblue +$687$ & \improvedouble{+63}{-36} & \coloryellow +$714$ \\
        \bottomrule
        \end{tabular}
    }
    \caption{The changes in the number of correct code completions using the sparse retriever and different code generation models on the \ourbenchmark benchmark. RG-1 represents the method with only one Retrieval-Generation iteration, and RG-2 is equivalent to \ours.}
    \label{table:overlap_rg_iteration}
\end{table}

To further investigate the effectiveness and limitations of \ours, we present the changes in the number of correct code completions when using different methods in Table~\ref{table:overlap_rg_iteration}. The results, as evaluated by the Exact Match (EM) and Pass Rate (PR) metrics, suggest an improvement with the use of zero-shot, RG-1, RG-2, and oracle methods. However, there are still instances where the more capable methods fail. To gain a deeper understanding of these failures, we conduct a manual case study.
Our analysis reveals that a majority of failures are caused by misguided code retrievals, where code snippets retrieved conflict with the local context, leading to incorrect predictions. For example, the same API may use different sets of parameters across different files, where the retrieved API usage example can be misleading. Additionally, the model's predictions are not always suitable for retrieval. This is because the query is built by using a fixed length of the predicted code, which could contain incorrect code even when the first lines of the prediction are accurate. We also find that language models are highly sensitive to the given prompts and small variations in the order of code snippets can lead to unexpected predictions. Furthermore, we discover that many cases in the line and API datasets are considered correct while evaluated as incorrect by the EM score. This highlights the need for more accurate evaluation methods, such as utilizing the unit tests to consider the actual functionality of the code rather than just relying on exact matching.

\section{Related Work}
\paragraph{Global Context in Code Completion:}
The integration of global contexts in software repositories is a longstanding challenge for code completion tools. Conventional code completion techniques typically begin by analyzing code to identify candidate suggestions, then re-rank them~\cite{raychev2014code, svyatkovskiy2019pythia, svyatkovskiy2021fast}. While this approach provides efficient performance, it lacks the flexibility to generate code at arbitrary granularity. Another branch of research approaches code completion as a language modeling task, where the next tokens are generated based on the given context. Various methods have been proposed to incorporate global context into the training of language models, including n-grams~\cite{tu2014localness}, LSTMs~\cite{hellendoorn2017deep}, and Transformers~\cite{svyatkovskiy2020intellicode, liu2022learning, ding2022cocomic}. While the process of collecting labeled data and tuning models for various applications remains costly. In recent years, large pre-trained language models have gained much attention in the code completion task. A study by~\citet{shrivastava2022repository} also explored the repository-level code completion scenario. Despite its innovation, this study still relied on inflexible heuristics and classifier training for prompt construction. This underscores the ongoing challenges in utilizing pre-trained models for code completion and the need for continued research.

\paragraph{Joint Modeling Retrieval and Generation:}
Despite the remarkable capability of pre-trained language models~\cite{brown2020language, thoppilan2022lamda, chowdhery2022palm}, their offline training paradigm can result in a lack of customized and up-to-date information. Recent studies began exploring the joint modeling of retrieval and generation in knowledge-intensive tasks, such as question answering~\cite{guu2020retrieval, lewis2020retrieval, izacard2022few} and dialogue generation~\cite{zhang2022retgen}. This paradigm has also been extended to code generation by incorporating retrieved documents or code examples in the generation process~\cite{rizwan2021retrieval, zhou2022doccoder, lu2022reacc, zan2022language}. 
As language models have become increasingly sophisticated, there is a growing trend of in-context joint retrieval and generation, treating pre-trained language models as a frozen black box ~\cite{levine2022standing, ram2023context, shi2023replug}. Moreover, some studies have investigated utilizing the model's predictions as supplementary context to inform the retrieval process~\cite{mao2020generation, li2022generation, wang2023query2doc}. In this work, we demonstrate that joining code retrieval and generation in an iterative paradigm can serve as an effective method for the repository-level code completion task.

\section{Conclusion and Future Work}
In conclusion, we present \ours, a simple and effective framework for the repository-level code completion task. Utilizing a similarity-based retriever and a pre-trained language model, \ours makes the most use of repository-level information. Through iterative retrieval and generation, \ours can bridge the gap between retrieval context and the intended target, leading to better code completion performance. Our rigorous experiments on the \ourbenchmark benchmark show that \ours consistently and significantly improves the zero-shot code completion performance and outperforms the vanilla retrieval-augmented generation approach. Through comprehensive analysis, we also provide valuable insights into the effectiveness and limitations of \ours. With its simplicity, generality, and effectiveness, \ours has the potential to become an essential tool in real-world software development. In future work, we aim to tackle current limitations of \ours and continue improving its usability and robustness.

% \section*{Acknowledgements}

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Repository Details}
\label{appendix:details_of_repositories}
\begin{table}[h]
    \centering
        \scalebox{0.78}{
            \begin{tabular}{ll}
            \toprule
            \textbf{Repository} & \textbf{Github Identifier} \\
            \midrule
            \grayline \multicolumn{2}{c}{Line and API Invocation Completion Dataset} \\
            rl & \href{https://github.com/pytorch/rl}{pytorch/rl} \\
            ACE & \href{https://github.com/opendilab/ACE}{opendilab/ACE} \\
            vizier & \href{https://github.com/google/vizier}{google/vizier}  \\
            fortuna & \href{https://github.com/awslabs/fortuna}{awslabs/fortuna}  \\
            evaluate & \href{https://github.com/huggingface/evaluate}{huggingface/evaluate}  \\
            diffusers & \href{https://github.com/huggingface/diffusers}{huggingface/diffusers}  \\
            nerfstudio & \href{https://github.com/nerfstudio-project/nerfstudio}{nerfstudio-project/nerfstudio} \\
            FedScope & \href{https://github.com/alibaba/FederatedScope}{alibaba/FederatedScope} \\
            \midrule
            \grayline \multicolumn{2}{c}{Function Body Completion Dataset} \\
            trlx & \href{https://github.com/CarperAI/trlx}{CarperAI/trlx} \\
            imagen & \href{https://github.com/lucidrains/imagen-pytorch}{lucidrains/imagen-pytorch} \\
            tracr & \href{https://github.com/deepmind/tracr}{deepmind/tracr} \\
            betty & \href{https://github.com/leopard-ai/betty}{leopard-ai/betty} \\
            lightmmm & \href{https://github.com/google/lightweight_mmm}{google/lightweight\_mmm} \\
            inspection & \href{https://github.com/amazon-science/patchcore-inspection}{amazon-science/patchcore-inspection} \\
            omnivore & \href{https://github.com/facebookresearch/omnivore}{facebookresearch/omnivore} \\
            redframes & \href{https://github.com/maxhumber/redframes}{maxhumber/redframes} \\
            \bottomrule
            \end{tabular}
        }
    \caption{The Github identifiers of the repositories, with the corresponding URL links attached.}
    \label{table:repo_details}
\end{table}

% \noindent Github identifiers of the repositories used to build our \ourbenchmark benchmark are listed in Table \ref{table:repo_details}.
\end{document}
