% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\usepackage{times}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{textcomp}
\usepackage{subcaption}
\usepackage{xspace} 
\usepackage{diagbox}
\usepackage{hyperref}
\usepackage{amssymb} 
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{color} 
\usepackage{bm}
\usepackage{colortbl}
\usepackage{setspace}
\usepackage{array}
\usepackage{arydshln} 
\usepackage{makecell}
\usepackage{xcolor,colortbl}
\usepackage{tcolorbox}
\usepackage{listings}
\usepackage{pythonhighlight}
\usepackage{wrapfig,lipsum,booktabs}

\newcommand{\ours}{RepoCoder\xspace}
\newcommand{\ourbenchmark}{RepoEval\xspace}
\newcommand{\davinci}{GPT-3.5-Turbo\xspace}
\newcommand{\textada}{\textsc{Text}-\textsc{Embedding}-\textsc{ada}-002\xspace}
\newcommand{\codegen}{\textsc{CodeGen}\xspace}
\newcommand{\codegenbig}{\textsc{CodeGen}-\textsc{Mono}-6B\xspace}
\newcommand{\codegensmall}{\textsc{CodeGen}-\textsc{Mono}-2B\xspace}
\newcommand{\codegentiny}{\textsc{CodeGen}-\textsc{Mono}-350M\xspace}
\newcommand{\grayline}{\rowcolor[gray]{.90}}
\newcommand{\improve}[1]{{\textcolor{red}{\scriptsize{$#1$}}}}
\newcommand{\colorblue}{\cellcolor[rgb]{0.757,0.867,1}}
\newcommand{\coloryellow}{\cellcolor[rgb]{1,0.925,0.792}}
\newcommand{\colorgreen}{\cellcolor[rgb]{0.77, 0.89, 0.77}}
\newcommand{\improvedouble}[2]{$_{\textcolor{red}{#1}}^{\textcolor[rgb]{0,0.392,0}{#2}}$}

\definecolor{lightblue}{rgb}{0, 1, 1}
\definecolor{grayblue}{rgb}{0.757,0.867,1}
\definecolor{grayyellow}{rgb}{1,0.925,0.792}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{RepoCoder: Repository-Level Code Completion\\ Through Iterative Retrieval and Generation}

\author{
\centerline{\makecell{Fengji Zhang$^1$, Bei Chen$^2$, Yue Zhang$^2$, Jacky Keung$^1$, Jin Liu$^3$, \\ Daoguang Zan$^2$, Yi Mao$^2$, Jian-Guang Lou$^2$, Weizhu Chen$^2$}}\\
\centerline{$^1$City University of Hong Kong, $^2$Microsoft Corporation, $^3$Wuhan University}\\
% \centerline{$^2$Microsoft Corporation, $^3$Wuhan University}\\
\centerline{\tt{fengji.zhang@my.cityu.edu.hk, jacky.keung@cityu.edu.hk, jinliu@whu.edu.cn}} \\
\centerline{\tt{\{beichen, zhayue, v-dazan, maoyi, jlou, wzchen\}@microsoft.com}}
}

\begin{document}
\maketitle
\begin{abstract}
The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose \ours, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. \ours makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark \ourbenchmark, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that \ours significantly improves the In-File completion baseline by over 10\% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of \ours through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: 
\url{https://github.com/microsoft/CodeT/tree/main/RepoCoder}
% \footnote{\url{https://github.com/microsoft/CodeT/tree/main/RepoCoder}}
\end{abstract}

\section{Introduction}
In real-world software production, it is crucial for developers to be aware of other files within the repository during programming. This challenge gives rise to the task of \textit{repository-level code completion}, where automated tools are expected to utilize the broader context of a repository rather than relying solely on in-file information to complete unfinished code. Code files within a repository often exhibit interrelated dependencies, including shared utilities, configurations, and cross-API invocations resulting from modularization~\cite{tu2014localness}. Additionally, each repository typically follows customized naming conventions and coding styles~\cite{zou2019does}, which contribute to enhanced readability and maintainability. 
However, developing effective repository-level code completion tools remains an open problem. Although approaches relying on static code analysis and heuristic rules~\cite{raychev2014code, svyatkovskiy2019pythia, svyatkovskiy2021fast} can reliably parse specific repository context, they have limitations in the completion scenario, limiting capability for varying-length completions anywhere in a file. Meanwhile, studies~\cite{hellendoorn2017deep, svyatkovskiy2020intellicode, ding2022cocomic} tuning language models on labeled data excel in their respective evaluation scenarios but face challenges generalizing to unseen repositories without retraining.

% However, developing repository-level code completion tools is not an easy task. Approaches relying on static code analysis and heuristic rules~\cite{raychev2014code, svyatkovskiy2019pythia, svyatkovskiy2021fast} have limitations in terms of usability. Meanwhile, studies utilizing language models~\cite{hellendoorn2017deep, svyatkovskiy2020intellicode, ding2022cocomic} depend on labeled data for fine-tuning and exhibit limited generalization ability to unseen and customized repositories.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{emnlp2023-latex/figures/introduction-1.pdf}
    \caption{Illustration of the In-File code completion method, the repository-level Retrieval-Augmented Generation (RAG) method, and the iterative retrieval-generation \ours method.}
    \label{figure:framework}
\end{figure}

In this paper, we propose an approach to leverage off-the-shelf retrievers in order to locate valuable information within a repository and enhance the context for language models. We introduce a novel framework called \ours that aims to improve code retrieval and completion performance. As depicted in Figure~\ref{figure:framework}, we enhance the conventional In-File code completion method by incorporating the Retrieval-Augmented Generation (RAG) technique, which allows us to search for relevant code snippets from the repository to assist in generating the code completion. Additionally, we introduce \ours, which employs an iterative pipeline that utilizes the generated code completion to enhance the retrieval process, thus bridging the gap between the retrieval context and the intended completion target. Figure~\ref{figure:introduction_case} provides an example that illustrates the rationale behind our design. We demonstrate that relying solely on the unfinished code is insufficient to retrieve useful information from the repository. In the example, the model improvises a statement calling the
\texttt{COLMAP} API in the first iteration. The predicted parameters are reasonable yet incorrect. This is because the incomplete code preceding the code completion does not serve as an adequate retrieval query for the intended completion target. However, by performing a subsequent retrieval from the repository using the model's generated completion, we can successfully retrieve the target API signature and complete the code effectively.

Furthermore, we introduce the \ourbenchmark benchmark designed for evaluating the repository-level code completion task, which is constructed using the latest high-quality repositories sourced from GitHub\footnote{\url{https://github.com}}. By introducing \ourbenchmark, we address the lack of established benchmarks in the repository-level scenario. Notably, \ourbenchmark is the first benchmark that encompasses three levels of code completion granularity: line, API invocation, and function body. We also leverage unit tests present in the repository to enhance the accuracy of evaluation, which overcomes the limitations of similarity-based metrics.
To rigorously validate the effectiveness of \ours, we conduct extensive experiments using different language models of varying sizes, including \davinci\footnote{\url{https://platform.openai.com/docs/models/gpt-3-5}} and \textsc{CodeGen}~\cite{nijkamp2022codegen}. Experimental results demonstrate that \ours achieves significant improvements over In-File completion performance, surpassing the baseline by over 10\% across different experimental settings. Moreover, our iterative framework consistently enhances the performance of vanilla retrieval-augmented generation. We also provide a comprehensive analysis of the effectiveness and limitations of \ours, offering insights for future research. Our contributions can be summarized as follows:

\begin{itemize}
\item We propose \ours, a novel iterative retrieval-generation framework for the repository-level code completion task.
\item We introduce the \ourbenchmark benchmark, enabling the evaluation of repository-level code completion with varying levels of granularity and improved evaluation accuracy through the utilization of unit tests.
\item Through rigorous experimentation, we demonstrate that \ours significantly outperforms the In-File code completion paradigm and enhances the performance of vanilla retrieval-augmented generation.
\end{itemize}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{emnlp2023-latex/figures/introduction_case.pdf}
    \caption{A motivating example showcasing the utilization of model predictions to enhance the performance of code retrieval.}
    \label{figure:introduction_case}
\end{figure}
\section{Methodology}
\label{section:methodology}
\subsection{Overall Framework}
The task of code completion using a language model $\mathcal{M}$ can be generally described as $\hat{Y}=\mathcal{M}(X)$, where $\hat{Y}$ represents the predicted tokens and $X$ corresponds to the in-file unfinished code. By introducing an additional code retrieval model $\mathcal{R}$, we can transform the code completion pipeline into a Retrieval-Augmented Generation (RAG) approach.
Initially, we establish a retrieval database by partitioning the code files from the repository into a collection of code snippets $C_{repo}=\{c_1, c_2, \cdots\}$. Subsequently, we utilize the retrieval model $\mathcal{R}$ to extract the most relevant code snippets from $C_{repo}$ by employing the unfinished code $X$ as the retrieval query. This process yields a set of retrieved code snippets $C_{ret}=\mathcal{R}(C_{repo}, X)$. Following this, we leverage the language model $\mathcal{M}$ to perform code completion, resulting in the prediction $\hat{Y}=\mathcal{M}(C_{ret}, X)$. Consequently, we are able to incorporate the contextual information from the repository level during the code completion task.

However, using the unfinished code $X$ as the sole retrieval query introduces a gap between the retrieval context and the intended completion target, as exemplified in Figure~\ref{figure:introduction_case}. To address this limitation, we propose \ours, an iterative retrieval-generation pipeline designed to further enhance the performance of the vanilla RAG method. Specifically, for the $i$-th retrieval-generation ($i>1$) iteration, \ours utilizes the previous model prediction $\hat{Y}^{i-1}$ to construct a new query for the retrieval process. This leads to the generation of another set of relevant code snippets $C^i_{ret}=\mathcal{R}(C_{repo}, X, \hat{Y}^{i-1})$. Subsequently, a new prompt is constructed using $C^i_{ret}$, resulting in the generation of a new prediction $\hat{Y}^i=\mathcal{M}(C^i_{ret}, X)$. The newly generated code completion can serve as either the output of \ours\ or be utilized for the subsequent retrieval-generation iteration.

Importantly, it is worth noting that the parameters of $\mathcal{M}$ and $\mathcal{R}$ remain unchanged throughout the entire process. Moreover, there is no requirement for static code analysis tools or heuristic rules to construct the retrieval database. In the following subsections, we provide a detailed explanation of the code retrieval process (Section~\ref{section:retrieval_augmented_generation}) and the code generation process (Section~\ref{section:generation_augmented_retrieval}).

\subsection{Code Retrieval}
\label{section:retrieval_augmented_generation}

The retriever utilized within the \ours framework can be any model capable of searching for relevant documents given a specific query. To construct the retrieval database, a \textit{sliding window} approach is employed. The sliding window traverses the files in the repository and extracts contiguous lines of code that fit within the window size, denoted as $S_{w}$. The sliding window moves a fixed number of lines at each iteration, which is referred to as the sliding size, denoted as $S_{s}$.

During the initial retrieval process, when no model prediction is available, the query is formulated using the last $S_{w}$ lines of the unfinished code $X$. Consequently, the most similar code snippets are retrieved using the retrieval model, resulting in $C^1_{ret}=\mathcal{R}(C_{repo}, X)$. However, a gap exists between the retrieval context, based on $X$, and the intended completion target, which is to continue writing $X$. A possible solution is to adjust $C^1_{ret}$ by shifting each code snippet down by a few lines to include the subsequent code. Although this shifting approach has shown effectiveness in previous work~\cite{lu2022reacc}, indiscriminately shifting all retrieved code snippets without considering their content may not always be appropriate.

To address this issue, \ours augments the retrieval query during the $i$-th iteration ($i>1$) with the previously generated code $\hat{Y}^{i-1}$. Despite the lack of customized information for new repositories, pre-trained code language models have demonstrated impressive general-domain understanding and generation capabilities. The generated code $\hat{Y}^{i-1}$ can provide valuable supplementary information for the retrieval process, even though its correctness may not be guaranteed.
Therefore, for the $i$-th iteration of retrieval ($i>1$), the query is constructed by concatenating the last $(S_{w}-S_{s})$ lines of $X$ with the first $S_{s}$ lines of $\hat{Y}^{i-1}$. This approach yields the grounded retrieval results $C^i_{ret}=\mathcal{R}(C_{repo}, X, \hat{Y}^{i-1})$.

\subsection{Code Generation}
\label{section:generation_augmented_retrieval}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{emnlp2023-latex/figures/prompt_format.pdf}
    \caption{A visual example demonstrating the format of the \ours prompt, which combines the retrieved code snippets from the repository with the unfinished code present in the target file.}
    \label{figure:illustrate_prompt}
\end{figure}

The generator employed within the \ours framework can be any pre-trained language model capable of predicting subsequent tokens given a specific prompt. As mentioned earlier, it is crucial to incorporate both the context from the repository $C_{repo}$ and the context within the target file for effective code completion. This enables the model to leverage grounding information and enhances its generalization ability to unseen repositories.

In the \ours framework, we retrieve the most relevant code examples, denoted as $C_{ret}$, from the repository and concatenate them with the unfinished code $X$. To ensure readability and comprehension, we create a prompt template that seamlessly integrates $X$ and $C_{ret}$, as illustrated in Figure \ref{figure:illustrate_prompt}. The retrieved code snippets are arranged in ascending order based on their similarity scores to the query. Each code snippet is accompanied by its original file path, and the maximum number of code snippets included in the prompt, denoted as $K$, depends on the available prompt length. Ultimately, the prompt contains as much relevant information as possible to facilitate code completion.

\section{Benchmark Construction}
\label{sec:benchmark_construct}
\begin{table}[t]
    \centering
    \scalebox{0.77}{
        \begin{tabular}{cllccc}
        \toprule
        \textbf{ID} & \textbf{Name} & \multicolumn{1}{c}{\textbf{License}} & \textbf{Created} & \textbf{F.} & \textbf{N.} \\
        \midrule
        \grayline \multicolumn{6}{c}{Function Body Completion Dataset}\\
        $1.$ & imagen & MIT License & 2022-05-23 &  $14$ & $67$ \\
        $2.$ & tracr & Apache V2.0 & 2022-12-01 & $56$ & $146$ \\
        $3.$ & lightmmm & Apache V2.0 & 2022-02-10 & $36$ & $64$ \\
        $4.$ & inspection & Apache V2.0 & 2022-05-05 & $16$ & $32$ \\
        $5.$ & omnivore & CC BY-NC 4.0 & 2022-01-20 & $66$ & $22$ \\
        $6.$ & redframes & BSD-2-Clause & 2022-08-21 & $49$ & $42$ \\
        \midrule
        \grayline \multicolumn{6}{c}{Line and API Invocation Completion Datasets} \\
        $7.$ & rl & MIT License & 2022-02-01 & $165$ & $400$ \\
        $8.$ & ACE & Apache V2.0 & 2022-11-23 & $425$ & $400$  \\
        $9.$ & vizier & Apache V2.0 & 2022-02-16 & $188$ & $400$  \\
        $10.$ & fortuna & Apache V2.0 & 2022-11-17 & $168$ & $400$  \\
        $11.$ & evaluate & Apache V2.0 & 2022-03-30 & $180$ & $400$ \\
        $12.$ & diffusers & Apache V2.0 & 2022-05-30 & $305$ & $400$  \\
        $13.$ & nerfstudio & Apache V2.0 & 2022-05-31 & $157$ & $400$ \\
        $14.$ & FedScope & Apache V2.0 & 2022-03-24 & $443$ & $400$ \\
        \bottomrule
        \end{tabular}
    }
    \caption{The repositories utilized in our \ourbenchmark benchmark, presenting statistics obtained from the Github API as of January 2023. \textit{ID} denotes the repository IDs. \textit{F.} indicates the total number of Python source files. \textit{N.} represents the number of extracted test samples from each repository. For brevity, additional repository information can be found in Appendix~\ref{appendix:details_of_repositories}.}
    \label{table:repo_info}
\end{table}

To facilitate the evaluation of code completion tools in the repository-level scenario, we propose a novel \ourbenchmark benchmark. This benchmark is carefully constructed using the latest high-quality repositories sourced from GitHub and encompasses three levels of code completion granularity: line, API invocation, and function body. To assess the correctness of completed functions, we utilize unit tests present in the repository instead of relying solely on similarity-based metrics. Each sample in the \ourbenchmark benchmark is annotated with the corresponding source repository, file path, line numbers, and ground truth completion. For analysis and unit test execution, complete copies of the repositories are archived as of January 2023.

To construct \ourbenchmark, we first meticulously curate a collection of Python repositories from GitHub that satisfy the following criteria: open-source license, created after January 1, 2022\footnote{The training data of \davinci and \codegen is up to 2021. We use data from 2022 to prevent data leakage.}, non-fork original repositories, over $100$ stars, over $80\%$ of files written in Python, and explicit unit tests. Furthermore, to mitigate potential biases, we employ a random selection process for the repositories and create three distinct datasets for line completion, API invocation completion, and function body completion. Additional details regarding the selected repositories can be found in Table \ref{table:repo_info}.

\paragraph{Line completion:} In adherence to the conventions of code completion benchmarks~\cite{lu2021codexglue, lu2022reacc}, we implement the line completion scenario. First, according to the above-mentioned criteria, we select $8$ repositories that vary in size and cover different domains. Then we randomly select $200$ lines to complete from each repository, ensuring the lines are non-repetitive, not code comments, and each line contains at least $5$ tokens. Eventually, a total of $1600$ test samples are generated for the line completion dataset. 

\paragraph{API Invocation Completion:} We also choose to test the API completion scenario, especially in-repository defined APIs. It is a harder problem than the completion of built-in or third-party APIs due to the lack of customized training data~\cite{hellendoorn2019code}. We utilize the same group of repositories in the line dataset and parse the target repositories to locate invocations of in-repository APIs. From these candidates, we then randomly select $200$ non-repetitive API invocations from each repository, resulting in a total of $1600$ test samples for the API invocation completion dataset.

\paragraph{Function Body Completion:} Alongside the line and API completion evaluations, we also assess the ability to complete function bodies, which requires executing unit tests present in the repository. However, running tests can be time-consuming and computationally expensive. To address this, we randomly select a separate set of smaller-scale repositories that are easy to deploy. Within these repositories, we locate functions covered by unit tests and select function bodies containing $3$ to $30$ lines of code to complete. This yields a total of $373$ test samples for the function body completion dataset.

\section{Experimental Setup}
\subsection{Methods for Comparison}

\paragraph{In-File Completion:} Previous studies~\cite{chen2021evaluating, nijkamp2022codegen, chen2022codet} have demonstrated the effectiveness of utilizing large pre-trained language models for code generation in a zero-shot completion manner, conditioned on the provided context. Furthermore, it has been established that incorporating in-file context is beneficial for code completion scenarios~\cite{clement2021long}. Hence, as a baseline, we implement an In-File completion method by populating the prompt with the unfinished code and directly utilizing the pre-trained code generation model to predict the code completion.

\paragraph{Oracle Method:} A key contribution of RepoCode is the integration of model predictions for retrieval, bridging the gap between retrieval and the intended completion target. To showcase the effectiveness of this approach, we devise an oracle retrieval-augmented generation method for comparison purposes. This method performs a single retrieval process to obtain relevant code snippets, denoted as $C^{gt}_{ret}$, by utilizing the last $S_{w}-S_{s}$ lines of $X$ and the first $S_{s}$ lines of the ground truth code, $Y$. Subsequently, the completion code, denoted as $\hat{Y}$, is generated through $\mathcal{M}(C^{gt}_{ret}, X)$. This allows us to achieve the upper bound of performance for \ours, conditioned on the retrieval model $\mathcal{R}$ and the generation model $\mathcal{M}$.

\subsection{Implementation Details}
\label{section:implementation_details}
\begin{table*}[t]
    \centering
    \begin{subtable}[h]{0.49\linewidth}
        \scalebox{0.78}{
            \begin{tabular}{ccccccc}
            \toprule
            \multirow{3}{*}{{\textbf{Metric}}} & \multirow{3}{*}{\textbf{Oracle}} & \multirow{3}{*}{\textbf{In-File}}& \multicolumn{4}{c}{\textbf{RepoCoder Iterations}} \\
            \cmidrule(lr){4-7}
            & & & \multicolumn{1}{c}{\textbf{1}} & \multicolumn{1}{c}{\textbf{2}} & \multicolumn{1}{c}{\textbf{3}} & \multicolumn{1}{c}{\textbf{4}}\\
            \midrule
            \grayline \multicolumn{7}{c}{\davinci}\\
            EM & \coloryellow $57.75$ & $40.56$ & $55.31$ & $56.81$ &  \textbf{57.00} &  $56.63$ \\
            ES & \coloryellow $75.43$ & $65.06$ & $74.38$ & $75.11$ &  \textbf{75.30} &  $75.10$ \\
            \midrule
            \grayline \multicolumn{7}{c}{\codegenbig}\\
            EM & \coloryellow $48.81$ & $34.56$ & $45.81$ & $47.06$ &  \textbf{47.75} &  $47.44$ \\
            ES & \coloryellow $71.02$ & $60.67$ & $69.21$ & $70.10$ &  \textbf{70.73} &  $70.19$ \\   
            \midrule
            \grayline \multicolumn{7}{c}{\codegensmall}\\
            EM & \coloryellow $47.31$ & $33.63$ & $44.56$ & $46.94$ &  $46.69$ &  \textbf{47.13} \\
            ES & \coloryellow $69.80$ & $58.99$ & $67.68$ & $68.82$ &  $68.62$ &  \textbf{68.92} \\
            \midrule
            \grayline \multicolumn{7}{c}{\codegentiny}\\
            EM & \coloryellow $45.19$ & $29.56$ & $41.88$ & $43.06$ &  \textbf{43.94} &  $43.06$ \\
            ES & \coloryellow $67.20$ & $55.39$ & $65.05$ & $65.66$ &  \textbf{65.97} &  $65.62$ \\ 
            \bottomrule
            \end{tabular}
        }
        \caption{Line Completion.}
        \label{table:line_sparse}
    \end{subtable}
    \begin{subtable}[h]{0.49\linewidth}
        \scalebox{0.78}{
            \begin{tabular}{ccccccc}
            \toprule
            \multirow{3}{*}{{\textbf{Metric}}} & \multirow{3}{*}{\textbf{Oracle}} & \multirow{3}{*}{\textbf{In-File}}& \multicolumn{4}{c}{\textbf{RepoCoder Iterations}} \\
            \cmidrule(lr){4-7}
            & & & \multicolumn{1}{c}{\textbf{1}} & \multicolumn{1}{c}{\textbf{2}} & \multicolumn{1}{c}{\textbf{3}} & \multicolumn{1}{c}{\textbf{4}}\\
            \midrule
            \grayline \multicolumn{7}{c}{\davinci}\\
            EM & \coloryellow $50.13$ & $34.06$ & $47.69$ & $49.19$ &  $49.44$ &  \textbf{49.56} \\
            ES & \coloryellow $74.50$ & $63.22$ & $73.63$ & $74.43$ &  \textbf{74.59} &  $74.48$ \\
            \midrule
            \grayline \multicolumn{7}{c}{\codegenbig}\\
            EM & \coloryellow $40.25$ & $26.19$ & $36.69$ & $38.88$ &  $39.13$ &  \textbf{39.31} \\
            ES & \coloryellow $67.94$ & $56.45$ & $64.20$ & $65.52$ &  $65.53$ &  \textbf{65.90} \\   
            \midrule
            \grayline \multicolumn{7}{c}{\codegensmall}\\
            EM & \coloryellow $39.44$ & $25.44$ & $35.44$ & $37.56$ &  \textbf{38.44} &  $38.25$ \\
            ES & \coloryellow $66.78$ & $56.88$ & $63.47$ & $64.15$ &  $64.53$ &  \textbf{64.60} \\
            \midrule
            \grayline \multicolumn{7}{c}{\codegentiny}\\
            EM & \coloryellow $34.88$ & $22.19$ & $31.75$ & \textbf{33.88} &  $33.75$ &  $33.81$ \\
            ES & \coloryellow $63.06$ & $52.24$ & $59.82$ & $61.03$ &  $60.96$ &  \textbf{61.06} \\   
            \bottomrule
            \end{tabular}
        }
        \caption{API Invocation Completion.}
        \label{table:api_sparse}
    \end{subtable}
    \caption{Performance comparison on the line and API invocation completion datasets. Results present the average performance of each method evaluated using Exact Match (EM) and Edit Similarity (ES) scores. Numbers are shown in percentage (\%), with the best performance highlighted in bold.}
    \label{table:line_api_result}
\end{table*}

\paragraph{Retrieval Model:} For our main experiments, we employ a sparse bag-of-words model as the retrieval model, which has demonstrated effectiveness in retrieving similar code snippets~\cite{lu2022reacc}. This model transforms the query and candidate code snippets into sets of tokens and calculates their similarity using the Jaccard index~\cite{jaccard1912distribution}, computed as $Jaccard(S_q, S_c) = \frac{|S_q \cap S_c|}{|S_q \cup S_c|}$, where $S_q$ and $S_c$ represent the tokens of the query and candidate code snippets, respectively. We also experiment with a dense retriever based on UniXcoder~\cite{guo2022unixcoder}, detailed in Appendix~\ref{appendix:unixcoder}. 


\paragraph{Generation Model:} We evaluate \ours using four pre-trained language models with varying code generation capabilities. The first model, \davinci, is a state-of-the-art commercial code generation model with billions of trainable parameters and has been pre-trained on an extensive code corpus. Access to \davinci is obtained through the API provided by OpenAI. The second model, \codegen, is an open-source code generation model that has multiple published versions with varying model sizes and training data. In our experiments, we utilize three versions of \codegen model with $6$B, $2$B, and $350$M parameters.

\paragraph{Hyper-parameters:} We found that RepoCoder's performance was not highly sensitive to changes in hyper-parameters. Therefore, for our experiments on RepoEval, we assign hyper-parameters based on our experience. 
Specifically, the maximum number of tokens for the combined input prompt and output prediction is set to $4,096$ for \davinci and $2,048$ for \codegen. The length of retrieved code snippets is set to half the prompt length. For line and API completion, the maximum number of tokens in the generated completion ($\hat{Y}$), the line length of the sliding window ($S_{w}$), and the sliding size ($S_{s}$) are set to $100$, $20$, and $10$ respectively. For function body completion, these values are adjusted to $500$, $50$, and $10$. The maximum number of retrieved snippets ($K$) is set to $10$. The same hyper-parameters were used for the single-iteration RAG, iterative RepoCoder, and Oracle baselines, ensuring a fair comparison between methods. Notably, given that these parameters are intricately linked to the programming language and contextual scenarios, practitioners should make adjustments to ensure optimal real-world performance. 

% \paragraph{Hyper-parameters:} Our experiments take into careful consideration various hyper-parameters to optimize performance. The maximum number of tokens for the combined input prompt and output prediction is set to $4,096$ for \davinci and $2,048$ for \codegen. Additionally, after thorough hyper-parameter tuning, the length of the retrieved code snippets is set to occupy half of the prompt length. For line and API completion, the maximum number of tokens in the generated completion ($\hat{Y}$), the line length of the sliding window ($S_{w}$), and the sliding size ($S_{s}$) are set to $100$, $20$, and $10$ respectively. For function body completion, these values are adjusted to $500$, $50$, and $10$ respectively. The maximum number of retrieved code snippets in the prompt, denoted as $K$, is set to $10$.

\subsection{Evaluation Metrics}
\paragraph{Similarity-based Evaluation:} 
Following established practices in code completion research~\cite{lu2021codexglue, lu2022reacc}, we evaluate our line and API completion datasets using two metrics: Exact Match (EM) and Edit Similarity (ES). The EM score is a binary metric that takes the value of $1$ if the predicted code exactly matches the ground truth code, and $0$ otherwise. The ES metric provides a more fine-grained evaluation and is calculated as $ES = 1 - \frac{{\rm Lev}(\hat{Y}, Y)}{\max(|\hat{Y}|, |Y|)}$, where ${\rm Lev}$ represents the Levenshtein distance~\cite{levenshtein1966binary}.

\paragraph{Execution-based Evaluation:}
For the function body completion dataset, we utilize unit tests present in the repository to evaluate functional correctness. This approach is more reliable than similarity-based metrics in assessing the behavior of the completed functions. While collecting unit tests can be time-consuming, we focus on a realistic scenario and utilize the unit tests available in GitHub repositories to validate the generated code. We execute the completed code and report the Pass Rate (PR), where PR is $1$ if the code passes all the corresponding test cases, and $0$ otherwise.

\section{Experimental Results}
\label{section:main_results}
\subsection{Line and API Completion Datasets}

We compare the performance of \ours with the In-File completion method and the Oracle method on the line and API invocation completion datasets using four pre-trained language models and different retrieval-generation iterations. From the results listed in Table \ref{table:line_sparse} and \ref{table:api_sparse}, we find that \ours consistently improves the In-File completion performance on both datasets across all model sizes. The absolute improvements in the Exact Match (EM) and Edit Similarity (ES) scores exceed $10\%$ and $8\%$, respectively. \ours also shows competitive results compared to the Oracle method. With two or more iterations, \ours consistently outperforms the vanilla Retrieval-Augmented Generation (RAG) approach for all language models. Additionally, the \codegen model with $350$M parameters is comparable to the \davinci model with In-File completion when integrated with \ours. 
We also test \ours using a dense retriever powered by UniXcoder~\cite{guo2022unixcoder} (detailed in Appendix~\ref{appendix:unixcoder}) and find that the simple sparse retriever achieves equivalent performance, highlighting the robustness of \ours across different code retrieval and generation models.

\subsection{Function Completion Dataset}
\begin{table}[t]
    \centering
    \scalebox{0.75}{
        \begin{tabular}{cccccccc}
            \toprule
            \multirow{3}{*}{{\textbf{ID}}} & \multirow{3}{*}{\textbf{N.}} & \multirow{3}{*}{\textbf{Oracle}} & \multirow{3}{*}{\textbf{In-File}} & \multicolumn{4}{c}{\textbf{RepoCoder Iterations}} \\
            \cmidrule(lr){5-8}
            & & & & \multicolumn{1}{c}{\textbf{1}} & \multicolumn{1}{c}{\textbf{2}} & \multicolumn{1}{c}{\textbf{3}} & \multicolumn{1}{c}{\textbf{4}}\\
            \midrule
            $1.$ & $67$ & \coloryellow $56.72$ & $29.85$ & $53.73$ & \textbf{55.22} & $55.22$ & $55.22$  \\
            $2.$ & $146$ & \coloryellow $43.84$ & $27.40$ & $41.78$ & $43.84$ & \textbf{44.52} & $44.52$  \\
            $3.$ & $64$ & \coloryellow $32.81$ & $10.94$ & $25.00$ & \textbf{34.38} & $31.25$ & $32.81$  \\
            $4.$ & $32$ & \coloryellow $34.38$ & $28.13$ & $34.38$ & \textbf{37.50} & $34.38$ & $34.38$  \\
            $5.$ & $22$ & \coloryellow $40.91$ & $31.82$ & $31.82$ & \textbf{36.36} & $31.82$ & $36.36$  \\
            $6.$ & $42$ & \coloryellow $38.10$ & $9.52$ & $28.57$ & \textbf{38.10} & $38.10$ & $38.10$  \\
            \midrule
            \multicolumn{1}{c}{All} & $373$ & \coloryellow $42.63$ & $23.32$ & $38.34$ & \textbf{42.63} & $41.82$ & $42.36$  \\
            \bottomrule
        \end{tabular}
    }
    \caption{Performance comparison on the function body completion dataset using \davinci. Results display the Pass Rate (PR) of each method as evaluated using test cases. Numbers are presented in percentage (\%), with the best performance highlighted in bold. \textit{ID} represents the repository IDs, and \textit{N.} indicates the number of test samples in each repository.}
    \label{table:function_level}
\end{table}

We proceed to assess the performance of \ours on the function body completion dataset. To tackle the greater difficulty of function body completion, we employ the \davinci model due to its superior code understanding and generation capabilities, as well as its larger prompt length suitable for longer function code snippets. The evaluation results, presented in Table~\ref{table:function_level}, showcase similar trends to our findings on the line and API invocation completion datasets. Across most repositories, \ours exhibits significant improvement over the In-File completion method and competitive performance compared to the Oracle method. Moreover, with additional retrieval-generation iterations, \ours consistently outperforms the vanilla Retrieval-Augmented Generation (RAG) approach. These results reaffirm the effectiveness of our approach.

\section{Analysis}
% The experimental results presented in Section~\ref{section:main_results} have effectively showcased the effectiveness of \ours, with iterative \ours outperforming both the In-File completion method and the vanilla Retrieval-Augmented Generation (RAG) approach. 
In this section, we conduct further analyses on the retrieved code snippets to gain a deeper understanding of \ours and provide valuable insights for future research.

\subsection{Quality of Retrieved Code}
\begin{table}[t]
    \centering
        \scalebox{0.8}{
            \begin{tabular}{ccccc}
            \toprule
            \multirow{3}{*}{{\textbf{Metric}}} & \multirow{3}{*}{\textbf{GT-Code}} & \multirow{3}{*}{\textbf{In-File}}& \multicolumn{2}{c}{\textbf{RepoCoder Iter-}} \\
            \cmidrule(lr){4-5}
            & & & \multicolumn{1}{c}{\textbf{1}} & \multicolumn{1}{c}{\textbf{2}} \\
            \midrule
            \grayline \multicolumn{5}{c}{\davinci} \\
            EM &  $55.54$ & $34.42$ & $53.63$ & $55.07$ \\
            ES &  $77.67$ & $62.75$ & $77.68$ & $78.40$ \\
            % \cmidrule{1-5}
            Recall &  $100.0$ & - & $86.04$ & $90.34$ \\
            \midrule
            \grayline \multicolumn{5}{c}{\codegenbig} \\
            EM &  $44.78$ & $26.87$ & $41.09$ & $44.04$ \\
            ES &  $71.47$ & $56.42$ & $67.10$ & $68.55$ \\
            % \cmidrule{1-5}
            Recall &  $100.0$ & - & $76.27$ & $82.92$ \\
            \midrule
            % \grayline \multicolumn{5}{c}{\codegensmall} \\
            % EM &  $43.58$ & $25.85$ & $40.07$ & $42.57$ \\
            % ES &  $70.39$ & $56.97$ & $66.48$ & $68.07$ \\
            % % \cmidrule{1-5}
            % Recall &  $100.0$ & - & $76.27$ & $81.99$ \\
            % \midrule
            \grayline \multicolumn{5}{c}{\codegentiny} \\
            EM &  $37.86$ & $22.25$ & $35.64$ & $38.13$ \\
            ES &  $66.20$ & $52.40$ & $62.82$ & $64.26$ \\
            % \cmidrule{1-5}
            Recall &  $100.0$ & - & $76.27$ & $80.89$ \\
            \bottomrule
            \end{tabular}
        }
        \caption{Performance comparison on the test samples extracted from the API completion dataset using GT-Code, In-File, and \ours methods. Results present the averaged performance of each method as evaluated by Exact Match (EM), Edit Similarity (ES), and Recall. Numbers are shown in percentage (\%).}
        \label{table:gt_api_invocation}
    \end{table}


We observe a significant impact of the retrieved code's quality on code completion performance. And the most helpful code snippets typically contain code statements similar to the target completion or demonstrate example usages of the target API invocation. Then, to validate the correlation between retrieval quality and completion performance, we design an analysis experiment using the API invocation completion dataset. In this experiment, we leverage a static code analysis tool to locate code snippets in other files that include invocations of the ground truth API. 
% These located code snippets follow the same window size as described in Section~\ref{section:implementation_details}, with the ground truth API invocation positioned at the center of the sliding window. 
Subsequently, we rank these code snippets based on their similarity to the unfinished code and select the most similar ones to include in the completion prompt. We refer to this method as GT-Code and compare its performance against the In-File and \ours methods. 
Additionally, we show the recall performance of \ours by counting the number of retrieved code snippets containing invocation examples of the ground truth API. 

Since not every API in the test dataset has invocation examples in other files, and we also exclude the invocation examples existing in the input prompt for the model, we finally extract from the API invocation dataset $1046$ and $1083$ eligible test samples respectively for the \davinci and \codegen models to conduct the experiment. From the obtained results in Table~\ref{table:gt_api_invocation}, we observe that the GT-Code method, which utilizes ground truth API invocation examples, generally achieves the best performance among all methods. Furthermore, \ours with two iterations exhibits higher recall for ground truth API invocations compared to a single-iteration, which likely contributes to its superior code completion performance. Notably, as the language model grows more powerful, the recall value using \ours Iter-2 also increases, indicating the model predictions indeed assist the retrieval process and emphasizing the effectiveness of \ours.

\subsection{Locations of Retrieved Code}
\begin{table}[t]
    \centering
    \scalebox{0.78}{
        \begin{tabular}{lcccc}
        \toprule
        \multicolumn{1}{c}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Oracle}} & \multicolumn{2}{c}{\textbf{RepoCoder Iter-2}}  \\
        \cmidrule(lr){1-1}
        \cmidrule(lr){2-3}
        \cmidrule(lr){4-5}
        \multicolumn{1}{c}{\textbf{Dataset}} & \textbf{Line} & \textbf{API} & \textbf{Line} & \textbf{API} \\
        \midrule
        \grayline \multicolumn{5}{c}{Location Statistics} \\
        Imported & $4.22\%$ & $8.16\%$ & $3.22\%$ & $9.06\%$ \\
        Current File & $3.86\%$ & $4.05\%$ & $3.32\%$ & $4.10\%$ \\
        Current Directory & $46.41\%$ & $58.40\%$ & $45.68\%$ & $59.58\%$ \\
        Similar Import & $82.15\%$ & $86.71\%$ & $82.80\%$ & $87.70\%$ \\
        Similar Name & $52.23\%$ & $65.10\%$  & $53.40\%$ & $64.20\%$ \\
        Others & $7.27\%$ & $4.43\%$ & $7.77\%$ & $3.35\%$ \\
        \midrule
        \grayline \multicolumn{5}{c}{Eligible Samples}\\
        Test Samples & $333$ & $294$ & $312$ & $276$ \\
        Code Snippets & $2202$ & $1851$ & $2047$ & $1732$ \\
        \bottomrule
        \end{tabular}
    }
    \caption{Locations of retrieved code snippets when the Oracle/\ours method outperforms the In-File completion method using \davinci on the line and API completion datasets.}
    \label{table:retrieval_location}
\end{table}

The retrieval of code snippets provides valuable contextual information from other files to enhance the context for language models. We conduct a separate experiment to study the various locations from which effective retrieval occurs. Specifically, we select test samples  that are successfully predicted by the Oracle/\ours method but not by In-File completion using \davinci. This yields 
a number of eligible test samples and retrieved code snippets for the line and API invocation completion datasets.
To determine the original source of these code snippets, we adopt a classification scheme inspired by \citet{shrivastava2022repository}, consisting of five distinct file locations:
1. Imported: code from a file imported by the target file.
2. Current File: code from the excluded content of the target file.
3. Current Directory: code from a file in the same directory as the target file.
4. Similar Import: code from a file sharing at least one same API import with the target file.
5. Similar Name: code from a file with a file name sharing at least one token with the target file (assuming snake-case style file names).

The results are as outlined in Table~\ref{table:retrieval_location}. Our findings indicate a similar distribution of retrieved code snippets between the Oracle method and \ours. The majority of code snippets fall within our defined categories, and a significant portion of code snippets originates from files with ``Similar Import'', ``Similar Name'', or ``Current Directory'' locations, underscoring the importance of contextual information in code completion tasks.
Furthermore, we conduct an ablation study, wherein we restrict the retrieval process to only the aforementioned file locations. The results reveal a degradation in performance, highlighting the efficacy and simplicity of \ours in the retrieval process.

\section{Related Work}
\paragraph{Repository Context in Code Completion:}
Incorporating repository-level context into code completion tools has been a long-standing challenge. Traditional code completion techniques typically involve analyzing code to identify potential suggestions, followed by re-ranking them \cite{raychev2014code, svyatkovskiy2019pythia, svyatkovskiy2021fast}. While this approach offers efficient performance, it lacks the flexibility to generate code at arbitrary granularity. Another line of research treats code completion as a language modeling task, where the next tokens are generated based on the given context. Several methods have been proposed to incorporate repository context into the training of language models, including n-grams \cite{tu2014localness}, LSTMs \cite{hellendoorn2017deep}, and Transformers \cite{svyatkovskiy2020intellicode, liu2022learning, ding2022cocomic}. However, the process of collecting labeled data and fine-tuning models for different applications remains resource-intensive. In recent years, there has been significant attention on Large Language Models (LLMs) for code completion. A study by \citet{shrivastava2022repository} also explores the scenario of repository-level code completion. Despite its innovative approach, the study relies on inflexible heuristics and classifier training for prompt construction. This highlights the ongoing challenges in effectively leveraging LLMs for code completion and the need for further research.
\paragraph{Joint Modeling Retrieval and Generation:}
Despite the impressive capabilities of LLMs \cite{brown2020language, thoppilan2022lamda, chowdhery2022palm}, their offline training paradigm often limits access to customized and up-to-date information. Recent studies have started exploring the joint modeling of retrieval and generation in knowledge-intensive tasks, such as question answering \cite{guu2020retrieval, lewis2020retrieval, izacard2022few} and dialogue generation \cite{zhang2022retgen}. This approach has also been extended to code generation by incorporating retrieved documents or code examples into the generation process \cite{rizwan2021retrieval, zhou2022doccoder, lu2022reacc, zan2022language}. As language models have become increasingly sophisticated, there is a growing trend towards in-context joint retrieval and generation, treating the LLM as a fixed black box \cite{levine2022standing, ram2023context, shi2023replug}. Moreover, some studies have investigated utilizing the model's predictions as supplementary context to inform the retrieval process \cite{mao2020generation, li2022generation, wang2023query2doc, zemlyanskiy2022generate}. In this work, we demonstrate that adopting an iterative paradigm that combines code retrieval and generation can serve as an effective method for repository-level code completion.

\section{Conclusion and Future Work}
In conclusion, we introduce \ours, a straightforward and effective framework for the repository-level code completion task. By leveraging a retriever and a language model, \ours effectively utilizes repository-level information. Through an iterative process of retrieval and generation, \ours bridges the gap between retrieval context and the target code, resulting in improved code completion performance. Our extensive experiments conducted on the \ourbenchmark benchmark demonstrate that \ours consistently and significantly enhances In-File completion performance, surpassing the vanilla Retrieval-Augmented Generation (RAG) approach. Furthermore, our analysis provides valuable insights into the rationale and limitations of \ours. With its simplicity, versatility, and effectiveness, \ours has the potential to become an indispensable tool in real-world software development, and we aim to further enhance its usability and robustness.

\section*{Limitations}
\paragraph{Limited Effectiveness for Repositories with Low Code Duplication:} Despite we have demonstrated the effectiveness of \ours through extensive experiments and analysis, \ours may not bring significant performance improvements when a repository has few instances of code duplication. In such scenarios, the code retrieval process struggles to find sufficient relevant information from the repository to facilitate code completion. This issue is further highlighted in the study presented in Appendix~\ref{appendix:code_duplication}.

\paragraph{Difficulty in Identifying the Optimal Number of Iterations:} While \ours with two iterations outperforms the RAG method, determining the optimal number of iterations remains a challenge. Subsequent iterations of \ours may exhibit unstable performance compared to previous iterations. Appendix~\ref{appendix:fail_iterations} provides a demonstration of this issue. To mitigate this, we have explored different approaches to automatically terminate the iteration process when necessary. However, finding an optimal stopping criterion without significantly impacting \ours's performance remains an ongoing challenge. Further research is required to develop techniques that can identify the iteration at which \ours achieves the best performance.

\paragraph{Time Efficiency for Real-Time Deployment:} While RepoCoder demonstrates promising gains in code completion accuracy through iterative retrieval and generation, concerns may arise due to the latency of additional retrieval-generation steps. For real-time deployment scenarios with strict latency requirements, we can further improve RepoCoder through model optimizations such as quantization, distillation, and hardware acceleration to expedite inference. Techniques like caching frequent code snippets and pre-processing repositories can also boost speed. The model iterations can be dynamically adapted based on latency goals and contextual needs to balance accuracy and efficiency. Nevertheless, improving time efficiency is another important topic that is out of the scope of our current paper.

% \paragraph{Computing Budget and Iteration Termination:} \ours heavily relies on Large Language Models (LLMs) for code generation, which requires a considerable computing budget. Additionally, the computational cost increases with each iteration. To mitigate this, we have explored different approaches to automatically terminate the iteration process when necessary and save computing costs. However, finding an optimal stopping criterion that minimizes computing costs without significantly impacting \ours's performance remains an ongoing challenge.



\paragraph{Limited Exploration of Different Experimental Settings:} 
First, while we have validated the effectiveness of \ours, we have not yet explored the potential improvements that can be achieved through different prompt templates. We believe that more careful prompt engineering could enhance the performance of our approach even further. 
Second, our focus in this study has primarily been on exploring similarity-based retrieval models. The reason for this limited scope is rooted in the complexity of code retrieval, which involves numerous intricate details that are not directly relevant to the \ours framework. Considering alternative retrieval models or expanding the exploration to other code retrieval techniques could provide further insights and comparative evaluations.
Third, we have observed significant advancements in code generation models, such as GPT-4~\cite{openai2023gpt4}, StarCoder~\cite{li2023starcoder}, and WizardCoder~\cite{luo2023wizardcoder}. While our experiments demonstrate the efficacy of \ours across different language models (\davinci and \codegen), it would be valuable to investigate how our approach performs with these advanced code generation models. Incorporating them into our experimental setup would provide a broader evaluation of \ours across a wider range of language models.
Fourth, our experiments primarily use the In-File and Oracle methods as baselines. This decision stems from the fact that repository-level code completion using language models is a relatively new task, lacking well-established and reproducible baselines. To provide further insights, we include comparisons to other commercial code completion products. Nonetheless, it is impractical to systematically benchmark against complex, confidential commercial products. We instead conduct a study in Appendix \ref{appendix:commercial_compare} showcasing the repository-level completion ability of RepoCoder and another three major commercial products, where we can illustrate their qualitative differences. 
In summary, future work should aim to explore different prompt designs, consider alternative retrieval or generation models, and incorporate additional baselines. 

% Entries for the entire Anthology, followed by custom entries
\bibliography{emnlp2023}
\bibliographystyle{acl_natbib}

\appendix

\section{Repository Details}
\label{appendix:details_of_repositories}

As mentioned in Section~\ref{sec:benchmark_construct}, we meticulously selected repositories for our \ourbenchmark benchmark based on criteria such as open-source license, creation date, code quantity, and quality. Detailed information about these repositories is provided in Table~\ref{table:repo_details}.

\section{Using the Dense Retriever}
\label{appendix:unixcoder}
\begin{table}[h]
    \centering
    \begin{subtable}[h]{\linewidth}
    \scalebox{0.78}{
        \begin{tabular}{ccccccc}
        \toprule
        \multirow{3}{*}{{\textbf{Metric}}} & \multirow{3}{*}{\textbf{Oracle}} & \multirow{3}{*}{\textbf{In-File}}& \multicolumn{4}{c}{\textbf{RepoCoder Iterations}} \\
        \cmidrule(lr){4-7}
        & & & \multicolumn{1}{c}{\textbf{1}} & \multicolumn{1}{c}{\textbf{2}} & \multicolumn{1}{c}{\textbf{3}} & \multicolumn{1}{c}{\textbf{4}}\\
        \midrule
        \grayline \multicolumn{7}{c}{\davinci}\\
        EM & \coloryellow $57.25$ & $40.56$ & $54.56$ & $56.25$ &  \textbf{56.31} &  $55.31$ \\
        ES & \coloryellow $75.80$ & $65.06$ & $73.96$ & \textbf{74.70} &  $74.31$ &  $74.04$ \\
        \midrule
        \grayline \multicolumn{7}{c}{\codegenbig}\\
        EM & \coloryellow $47.25$ & $34.56$ & $45.56$ & \textbf{46.75} &  $46.56$ &  $46.63$ \\
        ES & \coloryellow $70.03$ & $60.67$ & $68.89$ & \textbf{69.51} &  $69.33$ &  $69.41$ \\   
        \midrule
        \grayline \multicolumn{7}{c}{\codegensmall}\\
        EM & \coloryellow $45.75$ & $33.63$ & $44.94$ & $45.13$ &  \textbf{45.81} &  $45.50$ \\
        ES & \coloryellow $68.35$ & $58.99$ & $67.43$ & $68.01$ &  \textbf{68.35} &  $68.20$ \\
        \midrule
        \grayline \multicolumn{7}{c}{\codegentiny}\\
        EM & \coloryellow $42.88$ & $29.56$ & $41.50$ & \textbf{42.69} &  $42.13$ &  $42.31$ \\
        ES & \coloryellow $65.97$ & $55.39$ & $64.71$ & \textbf{65.64} &  $65.13$ &  $65.53$ \\ 
        \bottomrule
        \end{tabular}
    }

        \caption{Line Completion.}
        \label{table:line_sparse_unixcoder}
    \end{subtable}
    \begin{subtable}[h]{\linewidth}
           \scalebox{0.78}{
            \begin{tabular}{ccccccc}
            \toprule
            \multirow{3}{*}{{\textbf{Metric}}} & \multirow{3}{*}{\textbf{Oracle}} & \multirow{3}{*}{\textbf{In-File}}& \multicolumn{4}{c}{\textbf{RepoCoder Iterations}} \\
            \cmidrule(lr){4-7}
            & & & \multicolumn{1}{c}{\textbf{1}} & \multicolumn{1}{c}{\textbf{2}} & \multicolumn{1}{c}{\textbf{3}} & \multicolumn{1}{c}{\textbf{4}}\\
            \midrule
            \grayline \multicolumn{7}{c}{\davinci}\\
            EM & \coloryellow $50.06$ & $34.06$ & $47.56$ & $49.13$ &  $49.19$ &  \textbf{49.63} \\
            ES & \coloryellow $74.95$ & $63.22$ & $72.66$ & $74.22$ &  $72.83$ &  \textbf{74.41} \\
            \midrule
            \grayline \multicolumn{7}{c}{\codegenbig}\\
            EM & \coloryellow $39.56$ & $26.19$ & $37.00$ & $38.44$ &  \textbf{39.13} &  $38.44$ \\
            ES & \coloryellow $66.82$ & $56.45$ & $64.21$ & $65.33$ &  $65.26$ &  \textbf{65.53}\\   
            \midrule
            \grayline \multicolumn{7}{c}{\codegensmall}\\
            EM & \coloryellow $37.06$ & $25.44$ & $35.00$ & \textbf{37.19} &  $36.13$ &  $36.63$ \\
            ES & \coloryellow $64.69$ & $56.88$ & $62.22$ & $63.39$ &  $63.17$ &  \textbf{63.65} \\
            \midrule
            \grayline \multicolumn{7}{c}{\codegentiny}\\
            EM & \coloryellow $33.13$ & $22.19$ & $31.50$ & $32.69$ &  \textbf{32.75} &  $31.94$ \\
            ES & \coloryellow $61.37$ & $52.24$ & $59.35$ & \textbf{60.69} &  $60.49$ &  $60.34$ \\ 
            \bottomrule
            \end{tabular}
        }
        \caption{API Invocation Completion.}
        \label{table:api_sparse_unixcoder}
    \end{subtable}
    \caption{Performance comparison on the line and API invocation completion datasets using the dense retriever. Results display the average performance of each method evaluated using Exact Match (EM) and Edit Similarity (ES) scores. Numbers are presented in percentage (\%), with the best performance highlighted in bold.}
    \label{table:line_api_result_unixcoder}
\end{table}

\begin{table*}[t]
    \centering
    \scalebox{0.85}{
        \begin{tabular}{clllcccc}
        \toprule
        \textbf{ID} & \textbf{Name} & \textbf{Github Link} & \multicolumn{1}{c}{\textbf{License}} & \textbf{Created} & \textbf{Stars} & \textbf{F.} & \textbf{L.}\\
        \midrule
        \grayline \multicolumn{8}{c}{Function Body Completion Dataset}\\
        $1.$ & imagen & \href{https://github.com/lucidrains/imagen-pytorch}{lucidrains/imagen-pytorch} & MIT License & 2022-05-23 & $6,160$ & $14$ & $7,324$ \\
        $2.$ & tracr & \href{https://github.com/deepmind/tracr}{deepmind/tracr} & Apache V2.0 & 2022-12-01 & $284$ & $56$ & $9,110$ \\
        $3.$ & lightmmm & \href{https://github.com/google/lightweight_mmm}{google/lightweight\_mmm} & Apache V2.0 & 2022-02-10 & $353$ & $36$ & $9,676$ \\
        $4.$ & inspection & \href{https://github.com/amazon-science/patchcore-inspection}{amazon-science/patchcore-inspection} & Apache V2.0 & 2022-05-05 & $304$ & $16$ & $2,532$ \\
        $5.$ & omnivore & \href{https://github.com/facebookresearch/omnivore}{facebookresearch/omnivore} & CC BY-NC 4.0 & 2022-01-20 & $459$ & $66$ & $11,797$ \\
        $6.$ & redframes & \href{https://github.com/maxhumber/redframes}{maxhumber/redframes} & BSD-2-Clause & 2022-08-21 & $283$ & $49$ & $3,881$ \\
        \midrule
        \grayline \multicolumn{8}{c}{Line and API Invocation Completion Datasets} \\
        $7.$ & rl & \href{https://github.com/pytorch/rl}{pytorch/rl} & MIT License & 2022-02-01 & $873$ & $165$ & $59,522$ \\
        $8.$ & ACE & \href{https://github.com/opendilab/ACE}{opendilab/ACE} & Apache V2.0 & 2022-11-23 & $299$ & $425$ & $66,497$ \\
        $9.$ & vizier & \href{https://github.com/google/vizier}{google/vizier}  & Apache V2.0 & 2022-02-16 & $688$ & $188$ & $43,393$ \\
        $10.$ & fortuna & \href{https://github.com/awslabs/fortuna}{awslabs/fortuna}  & Apache V2.0 & 2022-11-17 & $373$ & $168$ & $18,738$ \\
        $11.$ & evaluate & \href{https://github.com/huggingface/evaluate}{huggingface/evaluate}  & Apache V2.0 & 2022-03-30 & $1,082$ & $180$ & $21,418$ \\
        $12.$ & diffusers & \href{https://github.com/huggingface/diffusers}{huggingface/diffusers}  & Apache V2.0 & 2022-05-30 & $9,521$ & $305$ & $98,181$ \\
        $13.$ & nerfstudio & \href{https://github.com/nerfstudio-project/nerfstudio}{nerfstudio-project/nerfstudio} & Apache V2.0 & 2022-05-31 & $3,151$ & $157$ & $27,289$ \\
        $14.$ & FedScope & \href{https://github.com/alibaba/FederatedScope}{alibaba/FederatedScope} & Apache V2.0 & 2022-03-24 & $811$ & $443$ & $48,545$ \\
        \bottomrule
        \end{tabular}
    }
    \caption{Detailed information of the Github repositories used for \ourbenchmark. \textit{ID} represents the repository IDs. \textit{F.} denotes the total number of Python source files, while \textit{L.} indicates the total number of non-empty Python code lines. Statistics are accurate as of January 2023.}
    \label{table:repo_details}
\end{table*}

In our main experiments (as described in Section~\ref{section:implementation_details}), we utilize a sparse retrieval model for \ours due to its acceptable performance and computational efficiency. However, \ours is a versatile framework that can be applied with other code retrieval models as well. To further validate the effectiveness of \ours, we conduct additional experiments using a dense code retriever.

Specifically, we employ UniXcoder~\cite{guo2022unixcoder}, a state-of-the-art code embedding model, to transform code snippets into hidden vectors. We then calculate the similarity between code snippets using cosine similarity. The experimental results on the line and API invocation completion datasets using the dense retriever are presented in Table~\ref{table:line_sparse_unixcoder} and Table~\ref{table:api_sparse_unixcoder}. Notably, the performance of \ours using the dense retriever is comparable to that using the sparse retriever. Furthermore, the findings remain consistent across both retrievers, highlighting the robustness and generalizability of \ours.

\section{Code Duplication in Repositories}
\label{appendix:code_duplication}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{emnlp2023-latex/figures/duplication_3.pdf}
    \caption{Correlation between the absolute performance improvements achieved by \ours Iter-2 over the In-File method and the repository duplication ratios.}
    \label{figure:duplicate}
\end{figure}

We explore the relationship between the performance of \ours and the code duplication ratio of the repositories. Intuitively, since \ours utilizes similarity-based retrieval to find code exemplars, one might expect a positive correlation between its performance and the code duplication ratio. To assess this relationship, we calculate the code duplication ratio of the repositories by determining the ratio of duplicated code lines to the total code lines.

Figure~\ref{figure:duplicate} presents the results, demonstrating the correlation between \ours's performance, as measured by the Exact Match (EM) metric, and the code duplication ratio on the line and API completion datasets using \davinci. Notably, the repository ``diffusers'' exhibits the highest duplication ratio, which corresponds to a significant performance improvement for \ours on both datasets. Conversely, ``rl'' and ``vizier'' have low duplication ratios, resulting in comparatively lower performance for \ours. However, the correlation between \ours's performance and the code duplication ratio is not absolute. For example, ``FedScope'' and ``evaluate'' have similar duplication ratios but show different performance gains for \ours. 
% This discrepancy could be because our estimation of the duplication ratio is not accurate enough based on counting identical lines. Or it can be due to the ability of language models to learn from context and perform well even without exact similar code examples.

\section{Failed Cases between Iterations}
\label{appendix:fail_iterations}
\begin{table}[h]
    \centering
    \scalebox{0.78}{
        \begin{tabular}{ccccccccc}
        \toprule
        \multirow{3}{*}{\textbf{In-File}} & \multirow{3}{*}{\textbf{}} & \multicolumn{7}{c}{\textbf{RepoCoder Iterations}} \\
        \cmidrule{3-9}
        & & \multicolumn{1}{c}{\textbf{1}} & \textbf{} & \multicolumn{1}{c}{\textbf{2}} & \textbf{} & \multicolumn{1}{c}{\textbf{3}} & \textbf{} & \multicolumn{1}{c}{\textbf{4}} \\
        \midrule \grayline \multicolumn{9}{c}{\davinci} \\
        +$545$ & \improvedouble{+258}{-40} & $763$ & \improvedouble{+70}{-46} & $787$ & \improvedouble{+90}{-83} & $794$ & \improvedouble{+10}{-14} & $790$ \\
        \midrule \grayline \multicolumn{9}{c}{\codegenbig} \\
        +$424$ & \improvedouble{+220}{-57} & $587$ & \improvedouble{+70}{-35} & $622$ & \improvedouble{+16}{-12} & $626$ & \improvedouble{+10}{-7} & $629$ \\
        \midrule \grayline \multicolumn{9}{c}{\codegensmall} \\
        +$412$ & \improvedouble{+219}{-64} & $567$ & \improvedouble{+66}{-32} & $601$ & \improvedouble{+26}{-12} & $615$ & \improvedouble{+13}{-16} & $612$ \\
        \midrule \grayline \multicolumn{9}{c}{\codegentiny} \\
        +$352$ & \improvedouble{+202}{-46} & $508$ & \improvedouble{+59}{-25} & $542$ & \improvedouble{+16}{-18} & $540$ & \improvedouble{+12}{-11} & $541$ \\
        \bottomrule
        \end{tabular}
    }
    \caption{The changes in the number of correct code completions achieved using different methods on the API invocation completion dataset.}
    \label{table:overlap_rg_iteration}
\end{table}

In Section~\ref{section:main_results}, the evaluation results demonstrate that increasing the number of \ours iterations does not necessarily guarantee performance improvements. To further investigate this issue, we analyze the changes in the number of correct code completions achieved by different methods on the API invocation completion dataset. The prediction is considered correct when the EM score is $1$.
Table~\ref{table:overlap_rg_iteration} presents the results, showing the counts of correct code completions for each iteration of \ours. We observe that each iteration of \ours both passes cases that the previous iteration failed and fails cases that the previous iteration has passed.

Upon manually examining the failed cases, we have the following observations: Firstly, a majority of failures are caused by misleading retrieved code, which leads to incorrect predictions. For instance, the same API may have different sets of parameters across different files, and the retrieved API usage example can be misleading in such cases. Secondly, the model's predictions are not always suitable for retrieval. This is because the query is constructed using a fixed length of the predicted code, which may include noisy code beyond the initial lines of helpful code completion. Furthermore, our investigation reveals that many cases in the line and API datasets are actually correct despite being evaluated as incorrect by the EM score. This highlights the importance of considering the actual functionality of the code, rather than solely relying on exact matching, and suggests incorporating unit tests to assess code correctness.

\section{Case Study of Commercial Products}
\label{appendix:commercial_compare}
\begin{figure*}[t]
	\centering
	\begin{subfigure}[t]{0.71\linewidth}
		\centering
		\includegraphics[width=\linewidth]{emnlp2023-latex/figures/copilot_completion_example.pdf}
		\caption{Incorrect code completion of Github Copilot.}
	\end{subfigure}
	%\vfill
	\begin{subfigure}[t]{0.71\linewidth}
		\centering
		\includegraphics[width=\linewidth]{emnlp2023-latex/figures/tabnine_completion_example.pdf}
		\caption{Incorrect code completion of Tabnine.}
	\end{subfigure}
    \begin{subfigure}[t]{0.71\linewidth}
		\centering
		\includegraphics[width=\linewidth]{emnlp2023-latex/figures/codewhisperer_completion_example.pdf}
		\caption{Incorrect code completion of Amazon CodeWhisperer.}
	\end{subfigure}
    \begin{subfigure}[t]{0.71\linewidth}
		\centering
		\includegraphics[width=\linewidth]{emnlp2023-latex/figures/repocoder_completion_example.pdf}
		\caption{Correct code completion of RepoCoder.}
	\end{subfigure}
	\caption{Code completion examples of RepoCoder and three major commercial products.}
	\label{fig:commercial_case_study}
\end{figure*}

We conduct a study to showcase the repository-level code completion ability of RepoCoder and another three major commercial code completion tools: Github Copilot~\footnote{\url{https://github.com/features/copilot}}, Tabnine~\footnote{\url{https://www.tabnine.com}}, and Amazon CodeWhisperer~\footnote{\url{https://aws.amazon.com/codewhisperer}}. The experiment was conducted using the Visual Studio Code IDE, with each product providing completions as a plugin. These products are based on large language models pre-trained on code data and can perform line-level and block-level completion, similar to our study scenario.
We selected a simple API invocation example from the RepoEval dataset. The task was to complete the function body for initializing a \textit{StableDiffusionKDiffusionPipeline}, where the prefix in-file context provided little information. As shown in Figure~\ref{fig:commercial_case_study}, none of the commercial products generated the correct completion. The implementation details of these commercial products are confidential, it is difficult to perform systematic comparison. However, in our case, RepoCoder successfully predicted the correct completion by retrieving a relevant code snippet from the repository context. 
This demonstrates the need for state-of-the-art tools to effectively leverage repository-level context.

\end{document}
