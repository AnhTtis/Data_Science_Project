\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx,subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{Notations}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{bbm}
\usepackage{array}
\usepackage{etoc}
\usepackage[titles]{tocloft}
\usepackage[dvipsnames]{xcolor}
\usepackage{enumitem}
\usepackage{multirow}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}{Proposition}

% for making table of content look better
\renewcommand{\cftsecnumwidth}{17pt}
\renewcommand{\cftsubsecnumwidth}{24pt}
\renewcommand{\cftsubsubsecnumwidth}{25pt}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks=true,bookmarks=false,citecolor=teal,urlcolor=blue,linkcolor=red]{hyperref}

\makeatletter\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}
  {.5em \@plus.0ex \@minus.2ex}{-.5em}{\normalfont\normalsize\bfseries}}\makeatother

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
% \title{Towards Understanding the Effect of Pretraining Label Granularity on Deep Neural Network Generalization}
\title{Towards Understanding the Effect of Pretraining Label Granularity}

\author{
Guanzhe Hong\thanks{Work done as a student researcher at Google Research.}~~$^{1}$ \quad Yin Cui$^{2}$ \quad Ariel Fuxman$^{2}$ \quad Stanley H. Chan$^{1}$ \quad Enming Luo$^{2}$\\
$^1$Purdue University \quad $^2$ Google Research \\
{\tt\small \{hong288,stanchan\}@purdue.edu \quad \{yincui,afuxman,enming\}@google.com}
}

\maketitle
% Remove page # from the first page of camera-ready.
% \ificcvfinal\thispagestyle{empty}\fi


\etocdepthtag.toc{mtchapter}
\etocsettagdepth{mtchapter}{subsection}
\etocsettagdepth{mtappendix}{none}


%%%%%%%%% ABSTRACT
\begin{abstract}
In this paper, we study how pretraining label granularity affects the generalization of deep neural networks in image classification tasks. 
We focus on the ``fine-to-coarse'' transfer learning setting where the pretraining label is more fine-grained than that of the target problem. 
We experiment with this method using the label hierarchy of iNaturalist 2021, and observe a 8.76\% relative improvement of the error rate over the baseline. We find the following conditions are key for the improvement: 1) the pretraining dataset has a strong and meaningful label hierarchy, 2) its label function strongly aligns with that of the target task, and most importantly, 3) an appropriate level of pretraining label granularity is chosen. 
The importance of pretraining label granularity is further corroborated by our transfer learning experiments on ImageNet. Most notably, we show that pretraining at the leaf labels of ImageNet21k produces better transfer results on ImageNet1k than pretraining at other coarser granularity levels, which supports the common practice. 
Theoretically, through an analysis on a two-layer convolutional ReLU network, we prove that: 1) models trained on coarse-grained labels only respond strongly to the common or ``easy-to-learn'' features; 2) with the dataset satisfying the right conditions, fine-grained pretraining encourages the model to also learn rarer or ``harder-to-learn'' features well, thus improving the model's generalization.
% \footnote{We will make our code publicly available upon acceptance.}.
\end{abstract}




%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{ pix/illust_fine_vs_coarse_v2.pdf}
    \caption{Natural images can be labelled in various ways, depending on what features the label function considers discriminative, and how fine-grained they are. This work studies how pretraining label granularity influences DNN generalization in downstream classification tasks.}
    % \vspace{-2mm}
    \label{fig:motivation}
\end{figure}


Modern deep neural networks (DNNs) are incredibly good at image classification. In addition to architecture, regularization and training techniques \cite{samira2018,khan2020,lecun1998,alex2012,ilya2013,kingma2015,kaiming2016,simonyan2015,szegedy2015,xie2017,ioffe2015,srivastava2014,zhang2019}, the empirical success of DNNs trained in the supervised fashion heavily relies on the availability of large datasets of labelled natural images \cite{deng2009,alex2009,sun2017,zhou2018,horn2018}. 
Although much effort has been devoted to building datasets with more samples and higher-quality labels, there is still a lack of understanding on how a dataset's label space influences the generalization of the network. 
In particular, given the rich hierarchy of features present in natural images, such as the one illustrated in Figure \ref{fig:motivation}, there is surprisingly limited study on how label granularity of pretraining influences the quality of downstream classification generalization.

To study this problem carefully, it is natural to start with the simplest transfer learning setting: pretrain a DNN on an image classification task, then evaluate the model on the target problem using the pretrained backbone. 

We pay particular attention to the transfer direction of ``fine-to-coarse'', where the source task has more classes than the target task. In parallel to a lack of careful analysis in the literature, our practical motivation is that this transfer direction is common in the empirical transfer learning works, especially for large models: in addition to having more samples than target datasets, the pretraining dataset typically has a much greater number of classes \cite{vit2021,radford2021,steiner2022how,Zhang_2021_ICCV,ridnik2021}.

Our major findings and contributions are as follows. 
\begin{itemize}[noitemsep]
\item We perform comprehensive experiments using the label hierarchies of iNaturalist 2021, and observe that the following conditions enable the transfer to work well: 1) the pretraining dataset needs to have a strong and meaningful label \textit{hierarchy}, 2) strong \textit{alignment} between the pretraining and target label functions, and most importantly, 3) the pretraining \textit{label granularity} cannot be too large nor too small. 
We further verify the importance of proper choice of label granularity within the hierarchy of ImageNet. On ImageNet1k, we show that the method can fail when the pretraining dataset is too limited in sample size. Moreover, we show that pretraining on the leaf labels of ImageNet21k produces better transfer results on ImageNet1k than pretraining on other coarser granularity levels, which supports the common practice used in the community.
\item To understand why the ``fine-to-coarse'' transfer learning works, we mathematically prove that, under certain conditions on the data distribution and the model architecture, the neural network is forced to avoid ``simple'' solutions which do not discover rare features that are useful for generalization on samples which lack ``common'' or ``easy-to-learn'' features.
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\subsection{Experimental work}
In the literature, there are a number of empirical studies that have explored how to improve classification accuracy by manipulating the (pre-)training label space. One line of research exploits fine-grained labels to improve DNNs' semantic understanding of natural images and their robustness in downstream tasks \cite{mahajan2018, singh2022, yan2020, shnarch2022, juan2020, yang2021, chen2018, ridnik2021, son2023, ngiam2018, cui2018}. For example, \cite{mahajan2018, singh2022} use noisy hashtags from Instagram as pretraining labels, \cite{yan2020, shnarch2022} apply clustering on the data first and then treat the cluster IDs as pretraining labels, \cite{juan2020} uses the queries from image search results, \cite{yang2021} applies image transformations such as rotation to augment the label space, and \cite{chen2018,ridnik2021} include fine-grained manual hierarchies in their pretraining processes. 
Similar to this line of work, part of our experimental results also corroborate the utility of pretraining on fine-grained label space. However, our experimental focus is on the analysis of the operating regime of this transfer method, particularly how pretraining label granularity, label function alignment and training set size influence the quality of the model; in particular, we show that this transfer method only works well within a specific operating regime.

Another line of research exploits the hierarchical structures present in (human-generated) label space to improve classification accuracy at the most fine-grained level \cite{yan2015, zhu2017, goyal2020, sun2017, zelikman2022, silla2011, shkodrani2021, bilal2017, goo2016}. For example, \cite{yan2015} adapts the network architecture to learn super-classes at each hierarchical level, \cite{zhu2017} adds hierarchical losses in the hierarchical classification task, \cite{goyal2020} proposes a hierarchical curriculum loss for curriculum learning. Our work, in contrast, focuses on the influence of label granularity on the model's generalization on target tasks that have a coarser label space than the pretraining one.

\subsection{Theoretical work}
At the theory front, there is an increasingly popular line of thinking for justifying the success of DNNs: when a DNN is trained with stochastic gradient descent, it is implicitly regularized and exhibits bias towards ``simpler'' solutions \cite{lyu2021,nakkiran2019,ji2019,palma2019,huh2021}, consequently preventing the model from overfitting even when it is highly overparameterized. However, at the opposite side, ``shortcut learning'' can also arise in learning algorithms \cite{geirhos2020, shah2020, pezeshki2021}, which argues that stochastic gradient descent tends to pick overly simple DNN solutions that can achieve high training and testing accuracy in-distribution, but generalize poorly if used in a challenging downstream task. 
To our knowledge, \cite{shah2020, pezeshki2021} are the closest to our works in that they both demonstrate that DNNs tend to perform shortcut learning and respond weakly to features that have a weak ``presence'' in the training data. However, there are major differences between our work and \cite{shah2020, pezeshki2021}. 
On the \textit{practical} side, we focus on how the pretraining label space affects classification generalization, while \cite{shah2020, pezeshki2021} mainly focus on demonstrating that simplicity bias can be harmful to generalization. Even though \cite{pezeshki2021} proposed a regularization technique to mitigate it, its experiments are too small-scale. 
On the \textit{theory} side, \cite{pezeshki2021} uses the neural tangent kernel (NTK) model, which is unsuitable for analyzing our transfer-learning type problem, since the feature extractor of an NTK model barely changes after pretraining. The theoretical setting in \cite{shah2020} is more limited than ours, since they use the hinge loss while we use a more standard exponential-tailed cross-entropy loss. Moreover, our data distribution is more realistic, as it captures the hierarchy in natural images, which has direct impact on the generalization power of the pretrained model according to our results.

On a more technical level, our theoretical analysis is inspired by a recent line of work that analyzes the feature learning dynamics of neural networks, by tracking how the hidden neurons of shallow nonlinear neural networks evolve to solve dictionary-learning-like problems \cite{zhu2022_adv, zhu2020_kd, shen2022}, in particular, our data distribution assumption also adopts a ``multiview'' approach first proposed in \cite{zhu2020_kd}, while our network initialization strategy is similar to the one in \cite{zhu2022_adv}. However, since the learning problems we analyze and the results we aim to show are significantly different from the existing literature, we need to derive the stochastic gradient descent dynamics of the nonlinear neural network from scratch.




\section{Problem Formulation}
\label{sec:problem_formulation}
In this section, we describe the formulation of our problem in detail, and discuss our intuition of how the label function influences DNN learning.

\subsection{Notations}
For a DNN-based classifier, given input image $\mX$ we write its (pre-logit) output for class $c$ as
\begin{equation}
    F_{c}(\mX) = \left\langle \va_c, \vh(\mTheta; \mX)\right\rangle
\end{equation}
where $\va_c$ is the linear classifier for class $c$, $\vh(\mTheta; \cdot)$ is the nonlinear network backbone with parameter $\mTheta$. In the transfer learning setting, we denote the input and output spaces of the target problem as a tuple $(\calX^{\text{tgt}}, \calY^{\text{tgt}})$, and those of the source pretraining problem as $(\calX^{\text{src}}, \calY^{\text{src}})$. The training and testing distributions are denoted as $\calD_{\text{train}}$ and $\calD_{\text{test}}$, respectively. Finally, the granularity of a label space is denoted as $\calG(\calY) \coloneqq \left\vert \calY \right\vert$, which represents the total number of classes.

\textit{In-dataset transfer.} In this setting, $\calX^{\text{src}} = \calX^{\text{tgt}}$ and only the label spaces $\calY^{\text{src}}$ and $\calY^{\text{tgt}}$ may differ. The baseline in this setting is clear: train on $\calD^{\text{tgt}}_{\text{train}}$ and test on $\calD^{\text{tgt}}_{\text{test}}$. In contrast, after pretraining the backbone network $\vh(\mTheta; \cdot)$ on $\mathcal{Y}^{\text{src}} \neq \mathcal{Y}^{\text{tgt}}$, we finetune it on $\calD^{\text{tgt}}_{\text{train}}$ using the backbone and then test on $\calD^{\text{tgt}}_{\text{test}}$. We focus on the transfer direction of ``fine-to-coarse'', namely $\calG(\mathcal{Y}^{\text{src}}) > \calG(\mathcal{Y}^{\text{tgt}})$; on the surface, the pretraining problem can be harder than the target one since there are fewer samples per class and more classes to learn.

\textit{Cross-dataset transfer.} In this setting, both the input and output spaces in the source and target problems differ. This is a commonly used setting in the computer vision community (\eg, ImageNet21k $\to$ ImageNet1k).

\subsection{How does the pretraining label function influence feature learning?}
To make our discussion in this subsection concrete, let us consider an example: suppose we have a set of pretraining images consisting of cats and dogs $\calD^{\text{src}}_{\text{train}} = \calD^{\text{cat}} \cup \calD^{\text{dog}}$, and the target problem $(\calD^{\text{tgt}}_{\text{train}}, \calD^{\text{tgt}}_{\text{test}})$ requires the DNN to identify whether the animal in the image is a cat or a dog. There can be countless ways of labeling the pretraining images, but only certain label functions can possibly train a network backbone that generalizes well in downstream tasks.

A label function has to be \textit{meaningful}: it needs to assign labels to images based on some consistent policy. In contrast, if it assigns labels completely randomly, then it is unlikely to guide a classifier to learn any discriminative feature well in the data. The pretraining label function also needs to \textit{align} well with that of the downstream task, since if it classifies the images based on the type of grass, trees, furniture present in the images, it clearly cannot guide the network in learning useful features about cats and dogs. These two conditions are the bare minimum of what a pretraining label function should meet. The more subtle factor is granularity.

In a rough sense, the reason by which we can say a group of images belongs to a class is because they \textit{share} certain visual ``features'' that do \textit{not} appear, or appear weakly in every other class. At the coarsest cat-versus-dog hierarchy level, there are common cat features that distinguish the cat images from the dog ones; they are also the most noticeable features. On the other hand, as shown in Figure \ref{fig:motivation}, the existence of the human-defined fine-grained classes means that each subclass of cat and dog has some features that are \textit{only} dominant within it. This leads to an interesting observation: the fine-grained features could be rarer in the dataset, thus harder to notice. We illustrate this observation in Figure \ref{fig:cat vs dog illustration}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{ pix/Cat_vs_dog_illust.pdf}
    % \vspace{-4mm}
    \caption{A simplified symbolic representation of the cat versus dog problem. The common features (green disk and blue triangle) are more noticeable than the fine-grained ones in the dataset due to the number of samples they appear in. Furthermore, to (approximately) solve the ``cat or dog'' problem on this dataset, the learner just needs to learn the common features well.}
    % \vspace{-2mm}
    \label{fig:cat vs dog illustration}
\end{figure}


Now suppose the pretraining label assignment is binary ``cat versus dog''. For an intelligent learner, it can take the arduous route of learning not only the common, but also the hard-to-notice fine-grained features in the two classes.
But technically speaking, it is allowed to take ``shortcuts'' by learning only the common features in each class to reach low training and testing loss. The latter form of learning can be harmful to the network's generalization in downstream classification tasks, since the network can be misled by irrelevant patterns in the image if the common features are weak in signal strength in that image. 
To force the learner to learn the rarer features well, one strategy is to explicitly label the fine-grained classes: now within each fine-grained class, the fine-grained features become as easy to notice as the common features, and the network has to learn the fine-grained ones to solve the classification problem. We shall make this line of arguments rigorous in Section \ref{sec:theory}, and demonstrate, on a toy data distribution and model architecture, that neural networks indeed learn the common features well and tend to respond weakly to the rarer features with coarse-label training, and this problem is alleviated by fine-grained pretraining.

However, the benefits of fine-grained pretraining comes at a cost: higher granularity implies lower sample count per class, which means that this method demands a reasonably large pretraining dataset to work well. We also point out this issue in our experiments.

\section{Empirical Results}
In this section, we present our in-dataset and cross-dataset transfer learning experimental results. Due to space limitations, we only present the core results in the main text, and leave the experimental details and ablation studies to Appendix \ref{section: appendix A}.

\subsection{In-dataset transfer on iNaturalist 2021}
We perform a systematic study of the transfer method in the in-dataset transfer setting on iNaturalist 2021 \cite{inaturalist_2021}. This dataset is well suited for our analysis because of its manually defined label hierarchy, which is based on the biological traits of the creatures in the images. Additionally, the large sample size of this dataset reduces the likelihood of sample-starved pretraining on reasonably fine-grained hierarchy level.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{ pix/inat_error_on_various_hierarchies_std.pdf}
    \caption{\textbf{In-dataset transfer}. ResNet34 validation error (with standard deviation) of finetuning on 11 superclasses of iNaturalist 2021, pretrained on various label hierarchies. The manual hierarchy outperforms the baseline and every other hierarchy, and exhibits a U-shaped curve.}
    % \vspace{-2mm}
    \label{fig:resnet34 inat2021}
\end{figure}


\begin{table*}[t]
\setlength{\tabcolsep}{7pt}
\centering
\begin{tabular}{l c | c c c c c c c c c}
\toprule
Manual Hierarchy & $\calG(\mathcal{Y}^{\text{src}})$ & 11 & 13 & 51 & 273 & 1103 & 4884 & 6485\\
% \cmidrule{2-9}
& Validation error & \textbf{\textcolor{red}{5.25}} & 5.40 & 5.10 & 4.83 & \textbf{4.79 }& 4.82 & 4.84 \\
\hline
Random class ID & $\calG(\mathcal{Y}^{\text{src}})$ & 22 & 88 & 352 & 1,408 & 5,632 & 11,264 & 500,000\\
% \cmidrule{2-9}
& Validation error & 6.61 & 6.30 & 6.12 & 6.10 & 6.12 & 6.10 & 6.54 \\
\hline
CLIP+kMeans, per superclass & $\calG(\mathcal{Y}^{\text{src}})$ & 22 & 88 & 352 & 1408 & 2816 & 5632 & 22528\\
% \cmidrule{2-9}
& Validation error & 5.14 & 5.16 & 5.17 & 5.24 & 5.30 & 5.31 & 5.37 \\
\hline
CLIP+kMeans, whole dataset & $\calG(\mathcal{Y}^{\text{src}})$ & 22 & 44 & 88 & 352 & 1408 & 2816 & 5632 \\
% \cmidrule{2 - 9}
& Validation error & 5.52 & 5.42 & 5.45 & 5.46 & 5.60 & 5.50 & 5.47 \\
\bottomrule
\end{tabular}
\caption{\textbf{In-dataset transfer}. ResNet34 finetuning results on 11 superclasses in iNaturalist 2021, pretrained on various label hierarchies with different label granularity. Baseline (11-superclass) and best performance are highlighted. Note that [\textit{best improvement over baseline}]/[\textit{baseline error rate}] = \textbf{8.76}\%. We only present select hierarchies here due to space constraint.}
% \vspace{-2mm}
\label{table: resnet34, inat2021}
\end{table*}





Our experiments on this dataset demonstrate the importance of a meaningful label hierarchy, label function alignment and appropriate choice of pretraining label granularity.

\textit{Dataset hierarchy}. Since we are specifically interested in the ``fine-to-coarse'' transfer setting, the target problem is to classify the root level of the manual hierarchy which contains 11 superclasses. To generate a greater gap between the performance of different hierarchies and to shorten training time, we use the mini version of the training set in all our experiments.

\textit{Network choice and training}. We experiment with ResNet 34 and 50 on this dataset. For pretraining on $\calD_{\text{train}}^{\text{src}}$ with fine-grained labels, we adopt a standard 90-epoch large-batch-size training procedure commonly used on ImageNet \cite{kaiming2016,goyal2017}. Then we finetune the network for 90 epochs and test it on the 11-superclass $\calD^{\text{tgt}}_{\text{train}}$ and $\calD^{\text{tgt}}_{\text{test}}$ respectively using the pretrained backbone $\vh(\mTheta_{\text{src}}; \cdot)$: we found that finetuning at a lower batch size and learning rate improved training stability. To ensure a fair comparison, we trained the baseline model using exactly the same training pipeline, except that the pretraining stage uses $\calD^{\text{tgt}}_{\text{train}}$. We observed that this ``retraining'' baseline consistently outperformed the naive one-pass 90-epoch training baseline on this dataset. We leave the results of ResNet50 to the appendix due to space limitations.

\textit{Alternative hierarchies generation}. To better understand the transfer method's operating regime, we experiment with different ways of generating the fine-grained labels for pretraining: we perform kMeans clustering on the ViT-L/14-based CLIP embedding \cite{radford2021, dehghani2021scenic} of every sample in the training set and use the cluster IDs as pretraining class labels. We carry out this experiment in two ways. The green curve in Figure \ref{fig:resnet34 inat2021} comes from performing kMeans clustering on the embedding of each superclass \textit{separately}, while the purple one's cluster IDs are from performing kMeans on the \textit{whole dataset}. The former way preserves the implicit hierarchy of the superclasses in the cluster IDs: samples from superclass $k$ cannot possibly share a cluster ID with samples belonging to superclass $k' \neq k$, therefore, its label function is forced to align better with that of the 11 superclasses than the purple curve's. We also assign random class IDs to samples, with number of classes as high as 500,000 which is the total number of training samples. The resulting models' validation errors on the 11-superclass problem are shown in Figure \ref{fig:resnet34 inat2021} and Table \ref{table: resnet34, inat2021}.

\textit{Interpretation of results}. Figure \ref{fig:resnet34 inat2021} is especially illustrative. First, as expected, random class ID pretraining (orange curve) is the worst out of all the alternatives. The label function of this type does not generate a \textit{meaningful hierarchy}, since it has no consistency in terms of the features it considers discriminative as it decomposes the superclasses: this is in stark contrast to the manual hierarchies, which decompose the superclasses based on the finer biological traits of the creatures in the image. Second, even with high-quality (human) labels, \textit{pretraining label granularity should not be too large nor too small}. We observe the blue curve in Figure \ref{fig:resnet34 inat2021} that, as long as the pretraining label granularity is beyond the order of $10^2$, models trained on the manual hierarchies outperform every other alternative, but it exhibits a U shape: as label granularity becomes too large, the error starts rising. Intuitively this is reasonable. If the pretraining granularity is too close to the target one, we should not expect improvement. On the other extreme, if we assign a label to \textit{every} sample in the training data, it is highly likely that the only \textit{differences} a model can find between each class would be frivolous details of the images and not be considered discriminative by the label function of the target coarse-label problem, so the pretraining stage is almost meaningless and can be misleading; indeed, the label-per-sample error (red star in Figure \ref{fig:resnet34 inat2021}) is very high. Thirdly, the features which the pretraining label function considers discriminative needs to \textit{align} well with those valued by the label function of the 11-superclass hierarchy. To see this point, observe that for models trained on cluster IDs obtained by performing kMeans on the CLIP embedding samples in each superclass \textit{separately} (green curve in Figure \ref{fig:resnet34 inat2021}), their validation errors are much lower than those trained on cluster IDs obtained by performing kMeans on the whole dataset (purple curve in Figure \ref{fig:resnet34 inat2021}). Of course, the manually defined fine-grained label functions should align the best with that of the 11 superclasses; indeed, the results corroborate this view.




\subsection{In-dataset transfer on ImageNet21k}
This subsection serves as an additional source of empirical evidence for the success of the in-dataset transfer method when requirements on label function alignment and granularity levels are satisfied.

\textit{Hierarchy definition}. The label hierarchy in ImageNet21k comes from WordNet \cite{wordnet, deng2009}. To define fine-grained labels, we start by defining the leaf labels of the dataset to be Hierarchy level 0. For every image, we trace from the leaf label to the root relying on the WordNet hierarchy, and set the $k$-th synset (or the root synset, whichever is higher in level) as the level-$k$ label of this image. This procedure also applies to the multi-label samples. This is the way we generate the hierarchies shown in Table \ref{table: vitb16, im21k to 21k}.

\textit{Target problem definition}. Due to the lack of a predefined coarse-label problem, we manually define our target problem to be a binary one: given an image, if the synset ``Living Thing'' is present on the path tracing from the leaf label of the image to the root, assign label 1 to this image; otherwise, assign 0. This problem almost evenly splits the training and validation sets of ImageNet21k: 5,448,549:7,294,862 for training, 43,745:58,655 for validation. 

\textit{Network choice and training}. On this dataset, we use the more recent Vision Transformer ViT-B/16 \cite{vit2021}. The pretraining pipeline is almost identical to the one in \cite{vit2021}. For finetuning, we tried out several strategies and only report the best results in the main text; to ensure fairness of comparison, these strategies are also included in finding the best baseline result, by using $\calD^{\text{tgt}}_{\text{train}}$ in pretraining.

\textit{Results}. We report the validation error of the network in Table \ref{table: vitb16, im21k to 21k}. Observe that the largest improvement obtained by pretraining on manual fine-grained labels is significant --- about 1.3\% --- and the amount of improvement decays towards 0 as pretraining granularity approaches 2. We also know that the label functions of the source and target tasks are well-aligned, since they are both manually generated and originate from the well-defined WordNet hierarchy. 

%Due to the high demand on computational resources required by training ViT models on large datasets, we do not have results of the network pretrained on alternative hierarchies and overly high granularities (label-per-sample), and we only ran the experiments on one random seed.

\begin{table}[t!]
\centering
\begin{tabular}{c c c}
\toprule
Hierarchy level & $\calG(\mathcal{Y}^{\text{src}})$ & Validation error\\
\hline
Baseline & 2 & \textbf{\textcolor{red}{7.90}} \\
% \hline
0 (leaf) & 21843 & \textbf{6.56} \\
1 & 5995 & 6.76 \\
2 & 2281 & 6.70 \\
4 & 519 & 6.97 \\
6 & 160 & 7.31 \\
9 & 38 & 7.55 \\
\bottomrule
\end{tabular}
\caption{\textbf{In-dataset transfer}. ViT-B/16 validation error on the binary problem ``is this object a \textit{living thing}?'' of ImageNet21k. Pretrained on various hierarchy levels of ImageNet21k, finetuned on the binary problem. Observe that the maximal improvement appears at the leaf labels, and as $\calG(\mathcal{Y}^{\text{src}})$ approaches 2, the percentage improvement approaches 0.}
% \vspace{-2mm}
\label{table: vitb16, im21k to 21k}
\end{table}

\subsection{In-dataset transfer on ImageNet1k, a negative example}
We present a negative result of the transfer method on ImageNet1k, showing that it does not work at all times. In particular, if the target problem has \textit{too little training data per class}, regardless of how the fine-grained labels are obtained, pretraining on these classes shows no improvement and can be harmful at high granularity levels.

\textit{Network choice and training}. To demonstrate this, we experiment with the ResNet50 model. Its training pipeline is almost identical to that on iNaturalist.

\textit{Hierarchies generation}. Due to a lack of more fine-grained manual label on this dataset, we generate fine-grained labels by performing kMeans on the ViT-L/14 CLIP embedding of the dataset separately for each class; this procedure is identical to that in the iNaturalist case. We also generate random subclass IDs for each vanilla class separately. 

\textit{Results}. Looking at Figure \ref{fig:resnet50, 1k to 1k}, we see that there is virtually no difference between the baseline and the best errors obtained by the models trained on the custom hierarchies: they are almost equally bad. Noting that the sample size of each class in ImageNet1k is only around $10^3$, and the fact that ImageNet1k classification is a ``hard problem'' --- it is a problem of high sample complexity --- further decomposing the classes causes each fine-grained class to have too few samples, leading to the above negative results.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{ pix/error_imagenet1k_inTransfer.pdf}
    \caption{\textbf{In-dataset transfer}. ResNet50 validation error on ImageNet1k, pretrained on various hierarchies, finetuned on the 1000 classes. Observe that there is virtually no difference between the errors of different hierarchies and the baseline.}
    % \vspace{-2mm}
    \label{fig:resnet50, 1k to 1k}
\end{figure}





\subsection{Cross-dataset transfer}
\begin{table}[t]
\centering
\scalebox{1.0}{
\begin{tabular}{l c c c}
\toprule
Pretrained on & Hier. lv & $\calG(\mathcal{Y}^{\text{src}})$ & Valid. acc.\\
\hline
IM1k, 90 Ep.  & - & 1000 & \textcolor{red}{\textbf{73.99}} \\
\cmidrule{1-4}
IM1k, 300 Ep. \cite{vit2021}  & - & 1000 & \textcolor{red}{\textbf{77.91}} \\
\cmidrule{1-4}
IM21k & 0 (leaf) & 21843 & \textbf{82.51} \\
\cmidrule{2-4}
& 1 & 5995 & 81.28\\
\cmidrule{2-4}
& 2 & 2281 & 80.26 \\
\cmidrule{2-4}
& 4 & 519 & 77.53 \\
\cmidrule{2-4}
& 6 & 160 & 75.53 \\
\cmidrule{2-4}
& 9 & 38 & 72.75 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Cross-dataset transfer}. ViT-B/16 average \textit{finetuning} validation accuracy on ImageNet1k, pretrained on various hierarchy levels of ImageNet21k. The 90-epoch baseline by us is obtained by direct training on ImageNet1k following the pipeline in \cite{vit2021} almost exactly, but with 90 instead of 300 epochs of training time for more direct comparison. See Appendix \ref{section: appendix A} for details.}
% \vspace{-2mm}
\label{table: vitb16, im21k to 1k, finetune}
\end{table}


\begin{table}[t]
\centering
\begin{tabular}{l c c c}
\toprule
Pretrained on & Hier. lv & $\calG(\mathcal{Y}^{\text{src}})$ & Validation acc.\\
\hline
IM21k & 0 (leaf) & 21843 & \textbf{81.45} \\
\cmidrule{2-4}
& 1 & 5995 & 78.33\\
\cmidrule{2-4}
& 2 & 2281 & 75.66 \\
\cmidrule{2-4}
& 4 & 519 & 68.95 \\
\cmidrule{2-4}
& 6 & 160 & 63.65 \\
\cmidrule{2-4}
& 9 & 38 & 57.35 \\
\bottomrule
\end{tabular}
\caption{\textbf{Cross-dataset transfer}. ViT-B/16 average \textit{linear-probing} validation accuracy on ImageNet1k, pretrained on various hierarchy levels of ImageNet21k. }
\label{table: vitb16, im21k to 1k, linear probe}
\end{table}

In this subsection, we demonstrate that the common choice of leaf-label-based pretraining on ImageNet21k is indeed better than pretraining at lower granularities in the manual hierarchy. This result again enforces the importance of choosing the appropriate pretraining label granularity.

\textit{Network choice and training}. Again, we use the ViT-B/16 model for our problem. The pretraining setup here is identical to the one in the in-dataset transfer experiments. To evaluate the quality of the backbone on ImageNet1k, we adopt two standard strategies in transfer learning: finetuning and linear probing; we discuss the training details and ablations studies in Appendix \ref{section: appendix A}.

\textit{Results}. Looking at Tables \ref{table: vitb16, im21k to 1k, linear probe} and \ref{table: vitb16, im21k to 1k, finetune}, we see a clear trend: the best accuracy occurs at the leaf level, and as the pretraining label granularity on ImageNet21k decreases, the network's accuracy on ImageNet1k also decreases. Note that the label functions of the two datasets align well because their class labels all originate from WordNet and overlap significantly. Interestingly, as the pretraining granularity approaches 1,000, the finetuned and linear-probed accuracies become relatively close to the baselines: this suggests that, with a poorly chosen pretraining granularity, even if we have the label functions align well across the datasets and pretrain with much more data than the baseline, we cannot see as much improvement.




\section{Theory of Label Granularity and Solution Complexity}
\label{sec:theory}
\subsection{Learning problem}
Following from our discussion in Section \ref{sec:problem_formulation}, we base our data distribution assumptions on our intuition of common-versus-fine-grained feature hierarchy in natural images. To push the intuition to an extreme for the sake of theoretical tractability, let us say all the ``features'' (in their purest form) have zero correlation and similar signal strength, and within an input vector, the fine-grained features belonging to the subclass of that input and the common features appear as disjoint patches with similar frequencies. It follows that, on the scale of the whole dataset, the fine-grained features are \textit{necessarily rarer} than the common ones, therefore, they could be \textit{harder} to learn: if the neural network does not learn these rarer features properly, in downstream tasks, it might perform poorly in some ``hard'' test examples in which the common features' signal strength is weak.

% (Enming) To better explain, let us assume the coarse-label problem is a binary classification task where each image belongs to either the ``+1'' superclass or the ``-1'' superclass. The ``+1'' superclass contains images that share common features, while the ``-1'' superclass contains images that share common features that do not appear or weakly appear in the ``+1'' superclass. We also assume that there are fine-grained classes within each superclass, where each fine-grained class contains images that share certain fine-grained features that are not present in the other fine-grained classes. For simplicity and mathematical tractability, we push this intuition to an extreme and assume that the features (in their purest form) have zero correlations and are unit-norm, and the common and fine-grained features all appear in the images as disjoint patches with similar frequencies in an image. We hypothesize that the fine-grained features are harder to learn than the common features because the fine-grained features are only shared by a subset of the images and are much rarer than the common features. This means that the neural network does not learn the rarer features properly and in downstream tasks, when the test examples are hard with weak common features, the model might perform very poorly. 

We describe our intuition in a more precise language here. Assume the coarse-label problem is binary classification with labels $+1$ and $-1$. Assume that every input vector $\mX$ lies in some space $\mathbb{R}^{d\times P}$, and consists of \textit{disjoint high-dimensional} patches $\mX = (\vx_1, \vx_2, ..., \vx_P)$. Think of the patches as high-dimensional embedding of the image, coming from (intermediate layers of) an already trained neural network; it is a common assumption in the theory literature to study shallow convolutional networks \cite{zhu2020_kd,shen2022,cao2022benign,stefani2021}. We define features as elements of an orthonormal dictionary $\calD = \{\vv_i\}_{i=1}^d \subset\mathbb{R}^d$, and call $\vv_+$ the common feature unique to samples in class $+1$, and $\vv_{+,c}$ to be the fine-grained feature unique to samples of subclass $(+,c)$; for the sake of simplicity, we only consider two hierarchy levels, and assume that there are $k_+$ fine-grained classes.

For a $(+,c)$-subclass input $\mX$, we say it is a \textit{normal} example if it contains both $\alpha_p \vv_+ + \vzeta_p$ and $\alpha_{p'} \vv_{+,c} + \vzeta_{p'}$ as disjoint patches, where $\alpha_p, \alpha_{p'} \approx 1$, and $\vzeta_p, \vzeta_{p'} \sim \calN(\vzero, \sigma_{\zeta}^2\mI_d)$ are independent spherical Gaussian noise; we assume the number of patches where the two types of features dominate are approximately the same. The rest of the patches in $\mX$ are distracting patterns: noise. We define a \textit{hard} example $\mX_{\text{hard}}$ to be the same as a normal one, except that the common-feature patches are missing: these examples only contain the fine-grained view of the object of interest. The same definitions apply to the ``$-1$'' class.

To see the precise problem setup and parameter choices, please refer to Appendix \ref{section: theory, problem setup}.

\subsection{Learner and SGD update}
We assume that the learner is a two-layer average-pooling convolutional ReLU network:
\begin{equation}
    F_{c}(\mX) = \sum_{r=1}^m a_{c,r}\sum_{p=1}^P \sigma(\langle \vw_{c,r}, \vx_p\rangle + b_{c,r}),
\end{equation}
where $m$ is a low-degree polynomial in $d$ and denotes the width of the network; $\sigma(\cdot) = \max(0, \cdot)$ is the ReLU nonlinearity. To simplify analysis and only focus on the learning of the feature extractor, we freeze $a_{c,r} = 1$ throughout training. Additionally, the neurons have zero-mean Gaussian initialization with small variance. This toy architecture is similar to those in \cite{zhu2020_kd,shen2022,zhu2022_adv}, and serves as the first step towards understanding the \textit{feature-learning} process of nonlinear DNNs with realistic sizes.

Cross-entropy is the loss for both coarse and fine-grained training, and we adopt stochastic gradient descent to update the network: at every iteration $t$, there is a fresh set of iid samples $\left(\mX_n^{(t)}, y_n^{(t)}\right)_n$. Please refer to Appendix \ref{section: theory, problem setup} for the technical details of these assumptions.

\subsection{Network bias in polynomial time}
\begin{theorem} [Coarse-label training]
(Informal). Suppose we only train on normal examples, and assume the number of fine-grained classes $k_+ = k_- \in [\polyln(d), f(\sigma_{\zeta})]$, for some function $f$ of the noise standard deviation $\sigma_{\zeta}$. With high probability, with proper choice of step size $\eta$, there exists $T^* \in \poly(d)$ such that for any $T \in [T^*, \poly(d)]$, the training loss satisfies $\calL(F^{(T)}) \in o(1)$, and given normal sample $\mX$, $F_y^{(T)}(\mX) \in \omega(1)$ and $F_{y'}^{(T)}(\mX) \in o(1)$ for $y'\neq y$; however, given hard example $\mX_{\text{hard}}$, the network's response $F_+^{(T)}, F_-^{(T)} \in o(1)$.
\end{theorem}

To see the full version of this theorem, please see Theorem 5.1 and Corollary 5.1.1 in Appendix \ref{section: appendix, phase II coarse}; its proof spans Appendix sections \ref{section: appendix init geometry} to \ref{section: appendix, phase II coarse}. This theorem essentially says that, if we only train on the \textit{normal} examples, it is almost impossible for the network to learn the fine-grained features well within practically reachable time. It follows that the network would perform poorly on any slightly challenging downstream image: if the image has weaker signal in the common features, then the network can be easily misled by noise, spurious features, and other flaws in the image. 

Some natural questions arise: is the assumption of only training the network on normal examples realistic? Should there not be hard training examples too? Indeed, in a realistic setting, there could be a portion of hard examples present in the training dataset, in which case it might be possible for the fine-grained features to be learnt in polynomial time if training data is abundant and contains little noise. However, similar to the argument in \cite{zhu2020_kd}, we believe the hard samples should be much \textit{rarer} in the dataset than the normal examples: in general deep learning, architecture and training improvements bring about diminishing increase in test accuracy; the same phenomenon also arises in our experiments, in which fine-grained pretraining only improves a small percentage of test accuracy. This suggests that samples containing easy-to-learn features occupy the majority of the training and testing set, and pretraining at proper granularity levels only possibly improves network performance on the rare hard examples. The purpose of our theoretical result is to present the ``bias'' of a neural network in an exaggerated fashion, so it is natural to start with the case of ``no hard training examples at all''. Furthermore, with the understanding that the rarer features are learnt more slowly, in reality, training datasets can have many flaws which can cause the network to overfit severely before picking up the fine-grained features. We leave these deeper considerations as future directions of the theoretical work. 

\begin{theorem}[Fine-grained-label training]
(Informal). Again, suppose we only train on normal examples, but this time let the labels be fine-grained: train the network on a total of $k_+ + k_-$ subclasses instead of 2. With high probability, within $\poly(d)$ time, the final pretrained and finetuned network has response $F_y^{(T)}(\mX) \in \Omega(1)$ and $F_{y'}^{(T)}(\mX) \in o(1)$ on the target binary problem on any normal or hard example.
\end{theorem}

The full version of this result and its proof are presented in Appendix \ref{section: appendix, finegrained}. With fine-grained pretraining, the network now has nonvanishing response to both the common and fine-grained features: it has learnt all the features from the data distribution well, so the feature extractor would be more robust to misleading patterns in the input.

To provide some intuition on why fine-grained pretraining helps learn the fine-grained features, recall our previous observation that fine-grained features are necessarily rarer on the scale of the dataset. For instance, in the superclass ``$+1$'', for any subclass index $c\in[k_+]$, the number of examples in which the common feature $\vv_+$ appears in is approximate $k_+$ times the number of examples which $\vv_{+,c}$ appears in; this contributes greatly to why the neural network only learns the common feature well in coarse-label training. However, in fine-grained training, the labels are now specifically associated with every subclass $(+,c)$. It follows that, within the samples of each subclass, the ratio between the number of samples in which $\vv_+$ appears and those which $\vv_{+,c}$ appears in approaches 1. Consequently, the neural network is forced to have a nonvanishing response to all the features to solve the fine-grained learning problem. 

\subsection{Discussion}
We defer the proofs to the appendix. However, we believe there are two points of interest worth discussing here. 

First, the width of the network is (low-degree) polynomial and the weights are initialized with small variance, which immediately separates the network's regime of learning from that of the popular neural tangent kernel \cite{jacot2018}: if our model is indeed in the kernel regime, then pretraining should barely provide any improvement on the network, since the hidden neurons barely move during pretraining. For our pretraining method to possibly work well, the network architecture should allow \textit{feature learning} instead of (random) feature selection.

Second, in addition to making the architecture more realistic, the more immediate consequence of the presence of the ReLU nonlinearity in the architecture is that the model can generalize better than linear models on noisy problems that are not linearly separable. In particular, in our precise setup described in Appendix \ref{section: theory, problem setup}, due to the overwhelmingly high number of patches which only contains noise $\vzeta_p \sim \calN(\vzero, \sigma_{\zeta}^2 \mI_d)$, given a normal sample $\mX$ belonging to class ``$+1$'', the probability of successful classification approximately reduces to whether the following random event holds true: $\{\langle \sum_{p}  \mathbbm{1}\{\vx_p \text{ is noise}\} \vx_p, +1 \text{-class features in }\mX \rangle > \langle \sum_{p}  \mathbbm{1}\{\vx_p \text{ is noise}\} \vx_p,  -1 \text{-class features in }\mX\rangle \}$, which has an $\Omega(1)$ chance of failing. On the other hand, since the noise strength at each single patch is very weak, applying ReLU with a slightly negative bias can easily threshold all the noise out with high probability and leave the network decision to its response to the true features.

% Third, the core reason behind why the network does not learn the fine-grained features well during coarse-label training rests on two main points. The first is as discussed before: the fine-grained features only occupy around $\frac{1}{k_+}$ portion of the update on normal examples, therefore, the \textit{per-neuron} SGD update's correlation with the fine-grained features is weaker than its correlation with the common features. The second point is more subtle. Let us write $S_+^{(t)}(\vv)$ to be roughly the set of neurons that has a nonzero response to (training) patches on which the feature $\vv$ dominates. If $\left\vert S_+^{(t)}(\vv_{+,c}) \right\vert \gg \left\vert S_+^{(t)}(\vv_{+}) \right\vert$, then even if the per-neuron update on feature $\vv_{+,c}$ is small, the overall-network update on this feature might still be large. This turns out to be untrue, in fact, $\left\vert S_+^{(t)}(\vv_{+,c}) \right\vert \approx \left\vert S_+^{(t)}(\vv_{+}) \right\vert$ throughout training, which is a natural consequence of the random Gaussian initialization: the number of neurons which correlates strongly with each feature is evenly distributed since initialization, and this evenness is maintained by SGD.

%------------------------------------------------------------------------
\section{Conclusion}
In this paper, we studied the influence of the pretraining label space on DNN generalization in downstream image classification tasks. We conducted experiments on iNaturalist 2021, ImageNet1k and ImageNet21k, and observed the importance of a meaningful label hierarchy, label function alignment between the source and target tasks, pretraining dataset size, and most importantly, proper choice of pretraining label granularity. We further theoretically analyzed why fine-grained pretraining can outperform the vanilla coarse-grained training under certain assumptions on the data distribution and model architecture. We believe we have only scratched the surface of the topic of how the label space influences DNN generalization, and much remains unexplored. For example, how can we utilize multiple granularity levels during pretraining? Is there an optimal way to assign weights to each level? Is ``avoiding shortcut learning'' the only reason behind why fine-grained pretraining shows improvement? Are there inherent issues with the labels in standard benchmark datasets? The list goes on.

% In our in-dataset transfer experiments on iNaturalist 2021, we observed that the following conditions are required for the method to work well: the pretraining dataset needs to have a strong and meaningful hierarchy, there needs to be strong alignment between the pretraining and target label functions, and the pretraining label granularity cannot be too large nor too small. We further experimented within the hierarchies of ImageNet21k to confirm the importance of pretraining label granularity, and on ImageNet1k to show that the pretraining dataset's sample size cannot be too small. Additionally, we showed that pretraining on the leaf labels of ImageNet21k produces the better transfer results on ImageNet1k than on lower granularities. Theoretically, we showed the following results with a toy model architecture: networks trained on coarse-grained labels tend to only respond strongly to common or easy-to-learn features in the data, in contrast, assuming certain conditions on the dataset, fine-grained pretraining helps the network learn the rarer features of the dataset, thus improving its downstream generalization.

% This paper provided a systematic analysis of the operating regime of the simple fine-to-coarse-grained transfer learning method. Through experimental analysis, we found that this method can work well on the in-dataset transfer problems, but it is important to choose the pretraining label hierarchy and granularity carefully, particularly on iNaturalist 2021 and ImageNet21k. We also demonstrated on ImageNet1k that when the target coarse-label problem's training set is too small, this transfer method does not work. Moreover, we showed that cross-dataset transfer also works well when proper label granularity is chosen. Finally, we theoretically showed that coarse-label pretraining tends to bias the neural network toward simpler solutions that do not respond strongly to fine-grained features; this problem is alleviated by fine-grained pretraining if enough conditions on the data and network model are met.


\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


\onecolumn
\begin{center}
    \huge Appendix
\end{center}

\appendix
% \addcontentsline{toc}{section}{Appendices}
\etocdepthtag.toc{mtappendix}
\etocsettagdepth{mtchapter}{none}
\etocsettagdepth{mtappendix}{subsubsection}

% \renewcommand{\thesubsection}{\Alph{subsection}}
% \renewcommand\thesection{\Alph{section}}

\begingroup
\hypersetup{linkcolor=blue}
\tableofcontents
\endgroup


% \setcounter{section}{0}
\newpage
\section{Additional Experimental Results}
\label{section: appendix A}
In this section, we present the full details of our experiments and relevant ablation studies. All of our experiments were performed using tools in the Scenic library \cite{dehghani2021scenic}.
\subsection{In-dataset transfer results}
\subsubsection{iNaturalist 2021}
\label{appdx_sec:inat2021}
On iNaturalist 2021, we use the mini training dataset with size 500,000 instead of the full training dataset to show a greater gap between the results of different hierarchies and speed up training. We use the architectures ResNet 34 and 50 \cite{kaiming2016}.

\textit{Training details}. Our pretraining pipeline on iNaturalist is essentially the same as the standard large-batch-size ImageNet-type training for ResNets \cite{kaiming2016, goyal2017}. The following pipeline applies to model pretraining on any hierarchy.
\begin{itemize}
    \item Optimization: SGD with 0.9 momentum coefficient, 0.00005 weight decay, 4096 batch size, 90 epochs total training length. We perform 7 epochs of linear warmup in the beginning of training until the learning rate reaches $0.1\times 4096/256 = 1.6$, and then apply the cosine annealing schedule.
    \item Data augmentation: subtracting mean and dividing by standard deviation, image (original or its horizontal flip) resized such that its shorter side is $256$ pixels, then a $224 \times 224$ random crop is taken.
\end{itemize}

For finetuning, we keep everything in the pipeline the same except setting the batch size to $4096/4 = 1024$ and base learning rate $1.6/4 = 0.4$. We found that finetuning at higher batch size and learning rate resulted in training instabilities and severely affected the final finetuned model's validation accuracy, while finetuning at lower batch size and learning rate than the chosen one resulted in lower validation accuracy at the end even though their training dynamics was stabler.

For the baseline accuracy, as mentioned in the main text, to ensure fairness of comparison, in addition to only training the network on the target 11-superclass problem for 90 epochs (using the same pretraining pipeline), we also perform ``retraining'': follow the exact training process of the models trained on the various hierarchies, but use  $\calD_{\text{train}}^{\text{tgt}}$ as the training dataset in both the pretrianing and finetuning stage. We observed consistent increase in the final validation accuracy of the model, so we report this as the baseline accuracy. Without retraining (so naive one-pass 90-epoch training on 11 superclasses), the average accuracy with standard deviation is $94.13, 0.025$.

\textit{Clustering}. To obtain the cluster-ID-based labels, we perform the following procedure. 
\begin{enumerate}
    \item For every sample $\mX_n$ in the mini training dataset of iNaturalist 2021, obtain its ViT-L/14 CLIP embedding $\mE_n$.
    \item Per-superclass kMeans clustering. Let $C$ be the predefined number of clusters per class.
    \begin{enumerate}
        \item For every superclass $k$, for the set of embedding $\{(\mE_n, y_n = k)\}$ belonging to that superclass, perform kMeans clustering with cluster size set to $C$.
        \item Given a sample with superclass ID $k\in\{1, 2, ..., 11\}$ and cluster ID $c\in\{1, 2, ..., C\}$, define its fine-grained ID as $C\times k + c$. 
    \end{enumerate}
    \item Whole-dataset kMeans clustering. Let $C$ be the predefined number of clusters on the whole dataset.
    \begin{enumerate}
        \item Perform kMeans on the embedding of all the samples in the dataset, with the number of clusters set to $C$. Set the fine-grained class ID of a sample to its cluster ID.
    \end{enumerate}
\end{enumerate}
Some might have the concern that having the same number of kMeans clusters per superclass could cause certain classes to have too few samples, which could be a reason for why the cluster ID hierarchies perform worse than the manual hierarchies. Indeed, the number of samples per superclass on iNaturalist is different, so in addition to the above ``uniform-number-of-cluster-per-superclass'' hierarchy, we add an extra label hierarchy by performing the following crude procedure to balance the sample size of each cluster:
\begin{enumerate}
    \item Perform kMeans for each superclass with number of clusters set to 2, 8, 32, 64, 128, 256, 512, 1024 and save the corresponding image-ID-to-cluster-ID dictionaries (so we are basically reusing the clustering results of the CLIP+kMeans per superclass experiment)
    \item For each superclass, find the image-ID-to-cluster-ID dictionary with the highest granularity while still keeping the minimum number of samples for each cluster $>$ predefined threshold (e.g. 1000 samples per subclass)
    \item Now we have nonuniform granularity for each superclass while ensuring that the sample count per cluster is above some predefined threshold.
\end{enumerate}
This simple procedure somewhat improves the balance of sample count per cluster, for example, Figure \ref{fig:inat2021, kmean per superclass, rebalanced} shows the sample count per cluster for the cases of total number of clusters = 608 and 1984. Unfortunately, we do not observe any meaningful improvement on the model's validation accuracy trained on this more refined hierarchy.


\begin{table*}[t!]
\setlength{\tabcolsep}{5pt}
\centering
\scalebox{0.85}{
\begin{tabular}{l c | c c c c c c c c c}
\toprule
Manual Hierarchy & $\calG(\mathcal{Y}^{\text{src}})$ & 11 & 13 & 51 & 273 & 1103 & 4884 & 6485\\
% \cmidrule{2-9}
& Validation error & \textbf{\textcolor{red}{5.25$\pm$0.051}} & 5.40$\pm$0.075 & 5.10$\pm$0.038 & 4.83$\pm$0.041 & \textbf{4.79$\pm$0.045}& 4.82$\pm$0.056 & 4.84$\pm$0.033 \\
\hline
Random class ID & $\calG(\mathcal{Y}^{\text{src}})$ & 22 & 88 & 352 & 1,408 & 5,632 & 11,264 & 500,000\\
% \cmidrule{2-9}
& Validation error & 6.61$\pm$0.215 & 6.30$\pm$0.070 & 6.12$\pm$0.77 & 6.10$\pm$0.053 & 6.12$\pm$0.042 & 6.10$\pm$0.057 & 6.54$\pm$0.758 \\
\hline
CLIP+kMeans & $\calG(\mathcal{Y}^{\text{src}})$ & 22 & 88 & 352 & 1408 & 2816 & 5632 & 22528\\
% \cmidrule{2-9}
per superclass & Validation error & 5.14$\pm$0.049 & 5.16$\pm$0.033 & 5.17$\pm$0.027 & 5.24$\pm$0.029 & 5.30$\pm$0.029 & 5.31$\pm$0.077 & 5.37$\pm$0.032 \\
\hline
C+k per supclass & $\calG(\mathcal{Y}^{\text{src}})$ & 88 & 218 & 320 & 608 & 1040 & 1984\\
% \cmidrule{2-9}
Class rebalanced & Validation error & 5.18$\pm$0.054 & 5.17$\pm$0.038 & 5.23$\pm$0.052 & 5.28$\pm$0.045 & 5.26$\pm$0.035 & 5.21$\pm$0.040 &  \\
\hline
CLIP+kMeans & $\calG(\mathcal{Y}^{\text{src}})$ & 22 & 44 & 88 & 352 & 1408 & 2816 & 5632 \\
% \cmidrule{2 - 9}
whole dataset& Validation error & 5.52$\pm$0.015 & 5.42$\pm$0.047 & 5.45$\pm$0.049 & 5.46$\pm$0.019 & 5.60$\pm$0.029 & 5.50$\pm$0.029 & 5.47$\pm$0.029 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{In-dataset transfer, iNaturalist 2021}. ResNet34 average finetuning validation error and standard deviation on 11 superclasses in iNaturalist 2021, pretrained on various label hierarchies with different label granularity. Baseline (11-superclass) and best performance are highlighted.}
\label{table: resnet34, inat2021, appendix 1}
\end{table*}


\begin{table*}[t!]
\setlength{\tabcolsep}{5pt}
\centering
\scalebox{0.95}{
\begin{tabular}{c  c | c c c c c c c c}
\toprule
90-Epoch ckpt & $\calG(\mathcal{Y}^{\text{src}})$ & 13 & 51 & 273 & 1103 & 4884 & 6485\\
% \cmidrule{2-8}
& Validation error & 5.40$\pm$0.075 & 5.10$\pm$0.038 & 4.83$\pm$0.041 & 4.79$\pm$0.045 & 4.82$\pm$0.056 & 4.84$\pm$0.033 \\
\hline
70-Epoch ckpt & $\calG(\mathcal{Y}^{\text{src}})$ & 13 & 51 & 273 & 1103 & 4884 & 6485\\
% \cmidrule{2-8}
& Validation error & 5.43$\pm$0.055 & 5.08$\pm$0.029 & 4.86$\pm$0.037 & 4.82$\pm$0.034 & 4.83$\pm$0.064 & 4.85$\pm$0.018 \\
\hline
50-Epoch ckpt & $\calG(\mathcal{Y}^{\text{src}})$ & 13 & 51 & 273 & 1103 & 4884 & 6485\\
% \cmidrule{2-8}
& Validation error & 5.53$\pm$0.036 & 5.2$\pm$0.031 & 4.90$\pm$0.038 & 4.9$\pm$0.042 & 4.91$\pm$0.020 & 4.95$\pm$0.026 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{In-dataset transfer, iNaturalist 2021}. ResNet34 average finetuned validation error and standard deviation on 11 superclasses in iNaturalist 2021, pretrained on the manual hierarchies, with different backbone checkpoints.}
\label{table: resnet34 different backbone ckpts, inat2021, appendix 1}
\end{table*}

\begin{table*}[t!]
\setlength{\tabcolsep}{5pt}
\centering
\scalebox{0.85}{
\begin{tabular}{l c | c c c c c c c c}
\toprule
Manual Hierarchy & $\calG(\mathcal{Y}^{\text{src}})$ & 11 & 13 & 51 & 273 & 1103 & 4884 & 6485\\
% \cmidrule{2-9}
& Validation error & \textbf{\textcolor{red}{4.43$\pm$0.029}} & 4.44$\pm$0.063 & 4.36$\pm$0.062 & 4.22$\pm$0.021 & \textbf{4.20$\pm$0.035 }& 4.23$\pm$0.054 & 4.33$\pm$0.037 \\
\hline
Random class ID & $\calG(\mathcal{Y}^{\text{src}})$ & 22 & 88 & 352 & 1,408 & 5,632 & 11,264 & 500,000\\
% \cmidrule{2-9}
& Validation error & 5.36$\pm$0.111 & 5.31$\pm$0.079 & 5.24$\pm$0.093 & 5.38$\pm$0.052 & 5.37$\pm$0.033 & 5.40$\pm$0.033 & 5.13$\pm$0.072 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{In-dataset transfer, iNaturalist 2021}. ResNet50 finetuned average validation error and standard deviation on 11 superclasses in iNaturalist 2021, pretrained on label hierarchies with different label granularity.}
\label{table: resnet50, inat2021, appendix 1}
\end{table*}


\textit{Experimental procedures}. All the validation accuracies we report on ResNet34 are the averaged results of experiments performed on at least 6 random seeds: 2 random seeds for backbone pretraining and 3 random seeds for finetuning. We report the average accuracies with their standard deviation on various hierarchies in Table \ref{table: resnet34, inat2021, appendix 1}.

An additional experiment we performed with ResNet34 is a small grid search over what checkpoint of a pretrained backbone we should use for finetuning on the 11-superclass method; we tried the 50-, 70- and 90-epoch checkpoints of the backbone on the manual hierarchies. We report these results in Table \ref{table: resnet34 different backbone ckpts, inat2021, appendix 1}. As we can see, 90-epoch checkpoints performs almost equally well as the 70-epoch checkpoints and better than the 50-epoch ones by a nontrivial margin. With this observation, we chose to use the end-of-pretraining 90-epoch checkpoints in all our other experiments without further ablation studies on those hierarchies.

Our ResNet50 results are not as extensive as those on ResNet34. We present the average accuracies and standard deviations in Table \ref{table: resnet50, inat2021, appendix 1}.

\begin{figure}[t!]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=.45\linewidth]{ pix/iNat_rebalance_1984Classes_sampleCounterPerClass.pdf}
        \includegraphics[width=.45\linewidth]{ pix/iNat_rebalance_608Classes_sampleCounterPerClass.pdf}
    \end{subfigure}
    \caption{\textbf{In-dataset transfer, iNaturalist 2021}. Number of samples per cluster in the case of 608 and 1984 total clusters, after applying the crude sample size rebalancing procedure described in subsection \ref{appdx_sec:inat2021}. Observe that the sample sizes are reasonably balanced across almost all the subclasses.}
    \vspace{-2mm}
    \label{fig:inat2021, kmean per superclass, rebalanced}
\end{figure}






\subsubsection{ImageNet21k}
\begin{table*}[t!]
\setlength{\tabcolsep}{5pt}
\centering
\footnotesize
\begin{tabular}{l c | c c c | c c | c c}
\toprule
 & Eval strategy & \multicolumn{3}{c|}{90-epoch finetune} & \multicolumn{2}{c|}{Linear probe} & \multicolumn{2}{c}{10-epoch finetune} \\
\hline
% \cmidrule{2-9}
\multirow{2}{*}{Leaf-pretrained} & (Batch size, base lr) & (4096,1e-3) & (1024,2.5e-4) & (512,1.25e-4) & (4096, 1e-3) & (512, 1.25e-4) & (4096,1e-3) & (512,1.25e-4) \\
% \cmidrule{2-9}
& Validation error & 92.782 & 93.177 & 93.295 & 87.497 & 87.493 & 92.294 & \textbf{93.439}  \\
\hline
 \multirow{2}{*}{Baseline} & (Batch size, base lr) & (4096,1e-3) & (1024,2.5e-4) & (512,1.25e-4) & (4096, 1e-3) & (512, 1.25e-4) & (4096,1e-3) & (512,1.25e-4) \\
% \cmidrule{2-9}
& Validation error & \textcolor{red}{\textbf{92.102}} & 91.971 & 91.939 & 91.703 & 91.719 & 92.002 & 91.856  \\
\bottomrule
\end{tabular}
\caption{\textbf{In-dataset transfer, ImageNet21k}. ViT-B/16 validation accuracy on the binary problem ``Is the object a Living Thing'' on ImageNet21k. Ablation study on the exact finetuning/linear probing strategy.}
\label{table: vitb16, in-dataset, ablation}
\end{table*}



The ImageNet21k dataset we experiment on contains a total of 12,743,321 training samples and 102,400 validation samples, with 21843 leaf labels. A small portion of samples have multiple labels.

Caution: due to the high demand on computational resources of training ViT models on ImageNet21k, all of our experiments that require (pre-)training or finetuning/linear probing on this dataset were performed with one random seed.

\textit{Hierarchy generation}. To define fine-grained labels, we start by defining the leaf labels of the dataset to be Hierarchy level 0. For every image, we trace from the leaf synset to the root synset relying on the WordNet hierarchy, and set the $k$-th synset (or the root synset, whichever is higher in level) as the level-$k$ label of this image; this procedure also applies to the multi-label samples. This is the way we generate the manual hierarchies shown in the main text.

Due to the lack of a predefined coarse-label problem, we manually define our target problem to be a binary one: given an image, if the synset ``Living Thing'' is present on the path tracing from the leaf label of the image to the root, assign label 1 to this image; otherwise, assign 0. This problem almost evenly splits the training and validation sets of ImageNet21k: 5,448,549:7,294,772 for training, 43,745:58,655 for validation. 

\textit{Network choice and pretraining pipeline}. We experiment with the ViT-B/16 model \cite{vit2021}. The pretraining pipeline of this model follows the one in \cite{vit2021} exactly: we train the model for 90 epochs using the Adam optimizer, with $\beta_1 = 0.9, \beta_2=0.999$, weight decay coefficient equal to 0.03 and a batch size of 4096; we let the dropout rate be 0.1; the output dense layer's bias is initialized to $-10.0$ to prevent huge loss value coming from the off-diagonal classes near the beginning of training \cite{cui2019cvpr}; for learning rate, we perform linear warmup for 10,000 steps until the learning rate reaches $10^{-3}$, then it is linearly decayed to $10^{-5}$. The data augmentations are the common ones in ImageNet-type training \cite{vit2021,kaiming2016}: random cropping and horizontal flipping. Note that we use the sigmoid cross-entropy for training since the dataset has multi-label samples.

\textit{Evaluation on the binary problem}. After the 90-epoch pretraining on the manual hierarchies, we evaluate the model on the binary problem. Since we have already reported the best accuracies on all the hierarchy levels in the main text, we do not repeat them here. To get a sense of how the relevant hyperparameters influence final accuracy of the model, we try out the following finetuning/linear probing strategies on the backbone trained on the \textit{leaf labels} and the target \textit{binary problem} of the dataset, and report the results in Table \ref{table: vitb16, in-dataset, ablation} (similar to our experiments on iNaturalist, we include the backbone trained on the binary problem in these ablation studies to ensure that our comparisons against the baseline are fair) :
\begin{enumerate}
    \item 90-epochs finetuning in the same fashion as the pretraining stage, but with a small grid search over 
    \begin{equation*}
    \begin{aligned}
        (\text{batch size}, \text{ base learning rate}) 
        = & \{(4096, 0.001), (4096/4=1024, 0.001/4 = 0.00025) , \\
        & (4096/8=512, 0.001/8 = 0.000125)\}.
    \end{aligned}
    \end{equation*}
    \item Linear probing with 20 epochs training length, using exactly the same training pipeline as in pretraining. We ran a small grid search over $(\text{batch size}, \text{ base learning rate}) = \{(4096, 0.001), (4096/8 = 512, 0.001/8 = 0.000125)\}$.
    \item 10-epochs finetuning, no linear warmup, 3 epochs of constant learning rate in the beginning followed by 7 epochs of linear decay, with a small grid search over $(\text{batch size}, \text{ base learning rate}) = \{(4096, 0.001), (4096/8 = 512, 0.001/8 = 0.000125)\}$.
\end{enumerate}
Table \ref{table: vitb16, in-dataset, ablation} helps us decide the accuracies we report in the main text. First, as expected the linear probing results are much worse than the finetuning ones. Second, the ``retraining'' accuracy of 92.102 is the best baseline we can report (the same thing happened in the iNaturalist case) --- if we only train the model for 90 epochs (the naive one-pass training) on the binary problem, then the model's final validation accuracy is 91.746\%, which is lower than 92.102\% by a nontrivial margin. In contrast, the short 10-epoch finetuning strategy works best for the backbone trained on the leaf labels, therefore, we also use this strategy to evaluate the backbones trained on all the other manual hierarchies. A peculiar observation we made was that, finetuning the leaf-labels-pretrained backbone for extended period of time on the binary problem caused it to overfit severely: for batch size and base learning rate in the set $\{(4096, 0.001), (1024, 0.00025), (512, 0.000125)\}$, throughout the 90 epochs of finetuning, although its training loss exhibits the normal behavior of staying mostly monotonically decreasing, its validation accuracy actually reached its peak during the linear warmup period!














\subsubsection{ImageNet1k}
\begin{table*}[t!]
\setlength{\tabcolsep}{10pt}
\centering
\scalebox{1.0}{
\begin{tabular}{l c | c c c }
\toprule
ResNet50 CLIP+kMeans & $\calG(\mathcal{Y}^{\text{src}})$ & 2000 & 4000 & 8000 \\
% \cmidrule{2-5}
per-class & Validation error & 23.4$\pm$0.13 & 23.48$\pm$0.098 & 23.49$\pm$0.204 \\
\hline
ViT-L/14 CLIP+kMeans & $\calG(\mathcal{Y}^{\text{src}})$ & 2000 & 4000 & 8000 \\
% \cmidrule{2-5}
per-class & Validation error & 23.4$\pm$0.127 & 23.47$\pm$0.074 & 23.78$\pm$0.048 \\
\hline
Random ID & $\calG(\mathcal{Y}^{\text{src}})$ & 2000 & 4000 & 8000 \\
% \cmidrule{2-5}
per-class & Validation error & 23.4$\pm$0.068 & 23.4$\pm$0.070 & 23.65$\pm$0.071 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{In-dataset transfer, ImageNet1k}. ResNet50 finetuned average validation error and standard deviation on the vanilla 1000 classes, pretrained on label hierarchies with different label granularity.}
\label{table: resnet50, in-dataset, imagenet1k, appendix 1}
\end{table*}

Our ImageNet1k in-dataset transfer experiments are done in a very similar fashion to the iNaturalist ones. In particular, the pretraining and finetuning pipeline for ResNet50 is exactly the same as the one in the iNaturalist case, so we do not repeat it here. As for cluster ID generation, we perform kMeans on the CLIP embedding of the samples in the training dataset for every class separately; the exact procedure is also identical to the iNaturalist case. The CLIP backbones we use here are the ResNet50 version and the ViT-L/14 version. We report the average accuracies and their standard deviation in Table \ref{table: resnet50, in-dataset, imagenet1k, appendix 1}. All results are obtained from at least one random seed during pretraining and 3 random seeds during finetuning.

The baseline we report in the main text again comes from retraining: if we adopt the pretrain-then-finetune procedure but with $\calD_{\text{train}}^{\text{tgt}}$ (i.e. the vanilla 1000-class labels) set as the pretraining dataset, then we obtain an average validation error of 23.28\% with standard deviation of 0.103, averaged over results of 3 random seeds. In comparison, if we only perform the naive one-pass 90-epoch training, we obtain average valiation error 24.04\%, with standard deviation 0.057.






\subsection{Cross-dataset transfer results on ImageNet21k}

\begin{table}[t]
\centering
\setlength{\tabcolsep}{10pt}
\scalebox{1.0}{
\begin{tabular}{l | c  c  c  c  c}
\toprule
Pretrained on / Base lr & $3\times10^{-3}$ & $3\times10^{-2}$ & $6\times10^{-2}$ & $3\times10^{-1}$\\
\hline
ImageNet21k, Hier. lv. 0 & 80.87$\pm$0.012 & 82.48$\pm$0.005 & \textbf{82.51$\pm$0.042} & 81.40$\pm$0.041 \\
% \cmidrule{1-5}
ImageNet21k, Hier. lv. 1 & 77.38$\pm$0.037 & 81.03$\pm$0.054 & \textbf{81.28$\pm$0.045} & 80.40$\pm$0.087 \\
% \cmidrule{1-5}
ImageNet21k, Hier. lv. 2 & 74.91$\pm$0.012 & 79.76$\pm$0.021 & \textbf{80.26$\pm$0.05} & 79.7$\pm$0.019 \\
% \cmidrule{1-5}
ImageNet21k, Hier. lv. 4 & 63.65$\pm$0.052 & 76.43$\pm$0.033 & \textbf{77.32$\pm$0.088} & 77.53$\pm$0.078 \\
% \cmidrule{1-5}
ImageNet21k, Hier. lv. 6 & 62.17$\pm$0.012 & 73.65$\pm$0.033 & 73.92$\pm$0.073 & \textbf{75.53$\pm$0.024} \\
% \cmidrule{1-5}
ImageNet21k, Hier. lv. 9 & 53.68$\pm$0.034 & 69.33$\pm$0.045 & 71.08$\pm$0.068 & \textbf{72.75$\pm$0.071} \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Cross-dataset transfer}. ViT-B/16 average \textit{finetuning} validation accuracy on ImageNet1k along with standard deviation, pretrained on various hierarchy levels of ImageNet21k, and a small grid search over the base learning rate. The 90-epoch baseline by us is obtained by direct training on ImageNet1k following the pipeline in \cite{vit2021} almost exactly, but with 90 instead of 300 epochs of training time for more direct comparison.}
\label{table: vitb16, im21k to 1k, finetune, appendix}
\end{table}




\begin{table}[t!]
\centering
\begin{tabular}{l c c c}
\toprule
Pretrained on & Hier. lv & $\calG(\mathcal{Y}^{\text{src}})$ & Validation acc.\\
\hline
IM21k & 0 (leaf) & 21843 & \textbf{81.45$\pm$0.021} \\
\cmidrule{2-4}
& 1 & 5995 & 78.33$\pm$0.018 \\
\cmidrule{2-4}
& 2 & 2281 & 75.66$\pm$0.005 \\
\cmidrule{2-4}
& 4 & 519 & 68.95$\pm$0.051 \\
\cmidrule{2-4}
& 6 & 160 & 63.65$\pm$0.035 \\
\cmidrule{2-4}
& 9 & 38 & 57.35$\pm$0.016 \\
\bottomrule
\end{tabular}
\caption{\textbf{Cross-dataset transfer}. ViT-B/16 average \textit{linear-probing} validation accuracy on ImageNet1k along with standard deviation, pretrained on various hierarchy levels of ImageNet21k. }
\label{table: vitb16, im21k to 1k, linear probe, appendix}
\end{table}


Here we report the average validation accuracy and standard deviation of the cross-dataset transfer experiment from ImageNet21k to ImageNet1k.

\textit{Network choice}. We use the same architecture ViT-B/16 as the one in the in-dataset ImageNet21k transfer experiment. Furthermore, we use the same backbone pretrained weights obtained in that section, since the hierarchies of ImageNet21k on which we pretrain the ViT's backbone are exactly the same in the in-dataset and cross-dataset experiments. Please note again that we only have one pretraining random seed due to the highly expensive nature of ViT training on large datasets.

\textit{Finetuning}. For finetuning, our procedure is very similar to the one in the original ViT paper \cite{vit2021}, described in its Appendix B.1.1. We optimize the network for 8 epochs using SGD with momentum factor set to 0.9, zero weight decay, and batch size of 512. The dropout rate, unlike in pretraining, is set to 0. Gradient clipping at 1.0 is applied. Unlike \cite{vit2021}, we still finetune at the resolution of 224$\times$224. For learning rate, we apply linear warmup for 500 epochs until it reaches the base learning rate, then cosine annealing is applied. We perform a small grid search of $\text{base learning rate} = \{3\times10^{-3}, 3\times10^{-2}, 6\times10^{-2}, 3\times10^{-1}\}$. Every one of these grid search is repeated over 3 random seeds. We report the ImageNet1k validation accuracies and their standard deviations in Table \ref{table: vitb16, im21k to 1k, finetune, appendix}. In the main text, we report the best accuracy for each hierarchy level.

\textit{Linear probing}. For linear probing, we use the following procedure. We optimize the linear classifier for 40 epochs (similar to \cite{comp_rep2021}) using SGD with Nesterov momentum factor set to 0.9, a small weight decay coefficient $10^{-6}$, and batch size 512. We start with a base learning rate of 0.9, and multiply it by 0.97 per 0.5 epoch. In terms of data augmentation, we adopt the standard ones like before: horizontal flipping and random cropping of size 224$\times$224. We repeat this linear probing procedure over 3 random seeds given the pretrained backbone, and report the average validation accuracy and standard deviation in Table \ref{table: vitb16, im21k to 1k, linear probe, appendix}.

\textit{Baseline}. We report two baselines, the first one is directly taken from the ViT paper \cite{vit2021} (see Table 5 in it), which is trained for 300 epochs on ImageNet1k. We also provide a 90-epoch version using almost the same training procedure (we do not perform Polyak averaging) shown in that paper to provide a more direct comparison against our methods. The 90-epoch version's average accuracy is 73.99\%, with standard deviation 0.243 (obtained by repeating training over 3 random seeds).






\newpage
\section{Theory, Problem Setup}
\label{section: theory, problem setup}
\subsection{Data Distribution}
\begin{enumerate}
    \item Coarse classification: a binary task, $+1$ vs. $-1$.
    \item For a sample $\mX\in\mathbb{R}^{d\times P}$, $\mathbb{P}[y = +] = \mathbb{P}[y = -] = 1/2$.
    \item Assume there exists $k_+$ subclasses of the superclass ``$+$'', and $k_-$ subclasses of the superclass ``$-$''. Let $k_+ = k_-$.
    \item Assume orthonormal dictionary $\calD = \{\vv_1, ..., \vv_d\} \subset \mathbb{R}^d$. Define $\vv_+\in\calD$ to be the common feature of class ``$+$''. For each subclass $(+,c)$ (where $c\in[k_+]$), denote the subclass feature of it as $\vv_{+,c} \in \calD$. Similar for the ``$-$'' class.
    \item For a normal sample $\mX$ belonging to the $(+,c)$ class (for $c\in[k_+]$), we sample its patches as follows:
    
    \textbf{Definition}: we define the function $\calP: \mathbb{R}^{d\times P} \times \calD \to [P]$ (so $(\mX; \vv) \mapsto I \subseteq [P]$) to extract, from sample $\mX$, the indices of the patches on which the dictionary word $\vv\in\calD$ dominates.

    \begin{enumerate}
        \item (Common-feature patches) With probability $\frac{s^*}{P}$, a patch $\vx_p$ in $\mX$ is a common-feature patch, on which $\vx_{p} = \alpha_{p}\vv_{+} + \vzeta_{p}$ for some (random) $\alpha_{p} \in \left[\sqrt{1 - \iota}, \sqrt{1 + \iota}\right]$;
        \item (Subclass-feature patches) With probability $\frac{s^*}{P}$, a patch with index $p \in \left(\{1, 2, ..., P\} - \calP(\mX; \vv_{+})\right)$ is a subclass-feature patch, on which $\vx_{p} = \alpha_{p}\vv_{+,c} + \vzeta_{p}$, for random $\alpha_{p} \in \left[\sqrt{1 - \iota}, \sqrt{1 + \iota}\right]$;
        \item (Feature-noise patches) For the remaining $P-|\calP(\mX; \vv_{+})|-|\calP(\mX; \vv_{+,c})|$ patches, $\vx_p = \sum_{i=1}^{s_f} \alpha_{i} \vv_{j(i)} + \vzeta_p$, where $\alpha_i \in [0, \gamma]$ and $\vv_{j(i)}$ is sampled from features of the ``$-$'' class.
    \end{enumerate}
    
    \item A hard sample $\mX_{\text{hard}}$ is exactly the same as a normal one except its common-feature patches are missing. Note: we do not train on hard examples.
    \item A sample $\mX$ belongs to the ``$+$'' superclass if $|\calP(\mX; \vv_{+})| > 0$ or $|\calP(\mX; \vv_{+,c})|>0$ for any $c$. 
    \item Note: a training batch of samples contains exactly $N/2k_+$ samples for each $(+,c)$ and $(-,c)$ subclass.
\end{enumerate}

\subsection{Learner Assumptions}
Assume the learner is a two-layer convolutional ReLU network:
\begin{equation}
    F_{c}(\mX) = \sum_{r=1}^m a_{c,r}\sum_{p=1}^P \sigma(\langle \vw_{c,r}, \vx_p\rangle + b_{c,r})
\end{equation}
To simplify analysis and only focus on the learning of the feature extractor, we freeze $a_{c,r} = 1$ throughout training. The nonlinear activation $\sigma(\cdot)$ is ReLU. Note that the convolution kernels have dimension $d$ and stride $d$.

\begin{remark} 
The greatest difference between this architecture and a normal CNN is that we do not allow feature sharing across classes: for each class $c$, we are assigning a disjoint group of neurons $\vw_{c,r}$ to it. Separating neurons for each class is a somewhat common trick to lower the complexity of analysis in DNN theory literature, as it reduces complex coupling between neurons across classes which is not the center of focus of our study here.
\end{remark}


\subsection{Training Algorithm}
\textbf{Initialization}.

Sample $\vw_{c,r}^{(0)} \sim \calN(\vzero, \sigma_0^2 \mI_d)$, and set $b_{c,r}^{(0)} = - \sigma_0\sqrt{4 + 2c_0}\sqrt{\ln(d)}$.

\textbf{Training}.

We adopt the standard cross-entropy training:
\begin{equation}
    \calL(F) = \sum_{n=1}^N L(F; \mX_n, y_n) = -\sum_{n=1}^N \ln\left(\frac{\exp(F_{y_n}(\mX_n))}{\sum_{c=1}^C \exp(F_{c}(\mX_n))} \right)
\end{equation}
This induces the stochastic gradient descent update for each hidden neuron ($c\in[k], r\in[m]$) per minibatch of $N$ iid samples:
\begin{equation}
\begin{aligned}
    \vw_{c,r}^{(t+1)} 
    = \vw_{c,r}^{(t)} + \eta \frac{1}{NP} \sum_{n=1}^N \Bigg( 
    & \mathbbm{1}\{y_n = c\}[1-\text{logit}_c^{(t)}(\mX_n^{(t)})]\sum_{p\in[P]}\sigma'(\langle \vw_{c,r}^{(t)}, \vx_{n,p}^{(t)} \rangle +b_{c,r}^{(t)}) \vx_{n,p}^{(t)} + \\
    & \mathbbm{1}\{y_n\neq c\} [-\text{logit}_c^{(t)}(\mX_n^{(t)}) ]\sum_{p\in[P]} \sigma'(\langle \vw_{c,r}^{(t)}, \vx_{n,p}^{(t)}  \rangle + b^{(t)}_{c,r}) \vx_{n,p}^{(t)}\Bigg)
\end{aligned}
\end{equation}
where 
\begin{equation}
    \text{logit}_c^{(t)}(\mX) = \frac{\exp(F_{c}(\mX))}{\sum_{y=1}^C \exp(F_{y}(\mX))} 
\end{equation}

As for the bias,
\begin{equation}
    b_{c,r}^{(t+1)} = b_{c,r}^{(t)} - \frac{\|\vw_{c,r}^{(t+1)} - \vw_{c,r}^{(t)}\|_2}{\ln^5(d)}
\end{equation}

\begin{remark}
\begin{enumerate}
    \item The initialization strategy is similar to the one in \cite{zhu2022_adv}.
    \item Since the only difference between coarse and fine-grained training is the label space, the form of SGD update is identical. The only difference is the number of output nodes of the network: for coarse training $F_c$ has the index set $[2]$ (binary classification), while fine-grained training makes $F_c$ have index set $[k_+ + k_-]$, with $k_+ + k_- \gg 2$ of course.
    \item The bias is for thresholding out the neuron's noisy activations that grow slower than $1/\ln^5(d)$ times the activations on the main features which the neuron detects. This way, the bias does not really influence updates to the neuron's response to the core features which it activate strongly on, since $1 - \frac{1}{\ln^5(d)} \approx 1$, while it removes useless low-magnitude activations.
\end{enumerate}

\end{remark}

\subsection{Parameter Choices}
The following are fixed choices of parameters for the sake of simplicity in our proofs. 
\begin{enumerate}
    \item Always assume $d$ is sufficiently large. All of our asymptotic results are presented with respect to $d$;
    \item $\poly(d)$ denotes the asymptotic order ``polynomial in $d$'';
    \item $\polyln(d)$ aymptotic order ``polylogarithmic in $d$'';
    \item $\polyln(d) \le k_+ = k_- \le d^{0.4}$;
    \item Small positive constant $c_0$, e.g. $c_0 = 0.1$;
    \item $s^* \in \polyln(d)$ with a high degree;
    \item $s_f \in O(1)$;
    \item $0 \le \iota \le \frac{1}{\polyln(d)}$;
    \item $\sigma_{\zeta} = \frac{1}{\ln^{10}(d)\sqrt{d}}$;
    \item $\gamma \le \frac{1}{\ln^{10}(d)}$;
    \item $P\sigma_{\zeta} \in \omega(\polyln(d))$, and $P \le d^2$;
    \item $\sigma_0 = d^{-C_{\text{init}}}$ for some large positive integer $C_{\text{init}}$ such that $d^{C_{\text{init}}} > d^3\ln(d)$, and set $\eta = \Theta(\sigma_0)$ for simplicity;
    \item Batch of samples $\calB^{(t)}$ at every iteration has a deterministic size of $N \in \Theta(d^2)$.
    \item Set $C_p$ to be a large integer such that $d^{C_p} \gg mNP\eta^{-1}d^4$;
    \item Note: we sometimes abuse the notation  $x = a \pm b$ as an abbreviation for $x \in [a-b, a+b]$.
\end{enumerate}
\begin{remark}
We believe the range of parameter choice can be (asymptotically) wider than what is considered here, but for the purpose of illustrating the main messages of the paper, we do not consider a more general set of parameter choice necessary because having a wider range of it can significantly complicate and obscure the already lengthy proofs without adding to the core messages.

Additionally, the function $f(\sigma_{\zeta})$ in the main text is set to $\frac{\sigma_{\zeta}^{-1}}{\ln^{10}(d)d^{0.1}} = d^{0.4}$ in this appendix for derivation convenience.
\end{remark}

\subsection{Plan of presentation}
We shall devote the majority of our effort to proving results for the coarse-label learning dynamics, starting with appendix section \ref{section: appendix init geometry} and ending on \ref{section: appendix, phase II coarse}, and only devote section \ref{section: appendix, finegrained} to the fine-grained-label learning dynamics, since the analysis of fine-grained training overlaps significantly with the coarse-grained one.












\newpage
\section{Initialization Geometry}
\label{section: appendix init geometry}
\begin{definition}
Define the following sets of interest of the hidden neurons:
\begin{enumerate}
    \item $\calU_{+,r}^{(0)} = \{\vv \in \calD: \langle \vw_{+,r}^{(0)}, \vv\rangle \ge \sigma_0 \sqrt{4 + 2c_0}\sqrt{\ln(d) - \frac{1}{\ln^5(d)}}\}$
    \item Given $\vv \in \calD$, $S^{*(0)}_+(\vv) \subseteq + \times [m]$ satisfies:
    \begin{enumerate}
        \item $\langle \vw_{+,r}^{(0)}, \vv \rangle \ge \sigma_0 \sqrt{4 + 2c_0} \sqrt{\ln(d) + \frac{1}{\ln^5(d)}}$
        \item $\forall \vv' \in \calD \text{ s.t. } \vv' \perp \vv, \, \langle \vw_{+,r}^{(0)}, \vv' \rangle < \sigma_0 \sqrt{4 + 2c_0} \sqrt{\ln(d) - \frac{1}{\ln^5(d)}}$ 
    \end{enumerate}
    \item Given $\vv \in \calD$, $S_{+}^{(0)}(\vv) \subseteq + \times [m]$ satisfies:
    \begin{enumerate}
        \item $\langle \vw_{+,r}^{(0)}, \vv \rangle \ge \sigma_0 \sqrt{4 + 2c_0} \sqrt{\ln(d) - \frac{1}{\ln^5(d)}}$
    \end{enumerate}
    \item For any $(+,r) \in S_{+,reg}^{*(0)} \subseteq + \times [m]$:
    \begin{enumerate}
        \item $\langle \vw_{+,r}^{(0)}, \vv \rangle \le \sigma_0 \sqrt{10} \sqrt{\ln(d)} \; \forall \vv\in\calD$
        \item $\left\vert \calU_{+,r}^{(0)} \right\vert \le O(1)$
    \end{enumerate}
\end{enumerate}
\end{definition}
\begin{prop}
\label{prop: init geometry, coarse}
Assume $m = \Theta(d^{2 + 2c_0})$, i.e. the number of neurons assigned to the $+$ and $-$ class are equal and set to $\Theta(d^{2 + 2c_0})$.

At $t=0$, for all $\vv \in \calD$, the following properties are true with probability at least $1- d^{-2}$ over the randomness of the initialized kernels:
\begin{enumerate}
    \item $|S_+^{*(0)}(\vv)|, |S_+^{(0)}(\vv)| = \Theta\left(\frac{1}{\sqrt{\ln(d)}}\right) d^{c_0}$
    \item In particular, for any $\vv,\vv' \in \calD$, $\left\vert \frac{|S_+^{*(0)}(\vv)|}{|S_+^{*(0)}(\vv')|} - 1 \right\vert, \left\vert \frac{|S_+^{*(0)}(\vv)|}{|S_+^{(0)}(\vv')|} - 1 \right\vert \le O\left( \frac{1}{\ln^5(d)}\right)$
    \item $S_{+,reg}^{(0)} = [m]$
\end{enumerate}
\end{prop}
\begin{proof}
Recall the tail bound of $g \sim \calN(0, 1)$ for every $\epsilon > 0$:
\begin{equation}
\begin{aligned}
\frac{1}{2}\frac{1}{\sqrt{2\pi}} \frac{\epsilon}{\epsilon^2 + 1} e^{-\epsilon^2/2} \le \mathbb{P}\left[ g \ge \epsilon \right] \le \frac{1}{2} \frac{1}{\sqrt{2\pi}} \frac{1}{\epsilon} e^{-\epsilon^2/2}
\end{aligned}
\end{equation}
First note that for any $r\in[m]$, $\{\langle \vw_{+,r}^{(0)}, \vv \rangle\}_{\vv\in\calD}$ is a sequence of iid random variables with distribution $\calN(0, \sigma_0^2)$. 

The proof of the first point proceeds in two steps.
\begin{enumerate}
    \item The following properties hold at $t=0$:
    \begin{equation}
    \begin{aligned}
        p_1 \coloneqq & \mathbb{P}\left[ \langle \vw_{+,r}^{(0)}, \vv \rangle \ge \sigma_0 \sqrt{4 + 2c_0} \sqrt{\ln(d) + \frac{1}{\ln^5(d)}} \right] \\
        \in & \frac{1}{\sqrt{8\pi}} d^{-2-c_0} e^{(-2-c_0)/\ln^5(d)}\left[ \frac{\sqrt{(4+2c_0)\left(\ln(d) + \frac{1}{\ln^5(d)}\right)}}{(4+2c_0)\left(\ln(d) + \frac{1}{\ln^5(d)}\right) + 1}, \frac{1}{\sqrt{(4+2c_0)\left(\ln(d) + \frac{1}{\ln^5(d)}\right)}}\right] \\
        = &\Theta\left(\frac{1}{\sqrt{\ln(d)}}\right)d^{-2-c_0}
    \end{aligned}
    \end{equation}
    and 
    \begin{equation}
    \begin{aligned}
        p_2 \coloneqq & \mathbb{P}\left[ \langle \vw_{+,r}^{(0)}, \vv \rangle \ge \sigma_0 \sqrt{4 + 2c_0} \sqrt{\ln(d) - \frac{1}{\ln^5(d)}} \right] \\
        \in & \frac{1}{\sqrt{8\pi}} d^{-2-c_0} e^{-(-2-c_0)/\ln^5(d)}\left[ \frac{\sqrt{(4+2c_0)\left(\ln(d) - \frac{1}{\ln^5(d)}\right)}}{(4+2c_0)\left(\ln(d) -  \frac{1}{\ln^5(d)}\right) + 1}, \frac{1}{\sqrt{(4+2c_0)\left(\ln(d) -  \frac{1}{\ln^5(d)}\right)}}\right] \\
        = & \Theta\left(\frac{1}{\sqrt{\ln(d)}}\right)d^{-2-c_0}
    \end{aligned}
    \end{equation}
    Therefore, for any $r\in[m]$, the random event described in $S_{+}^{*(0)}$ holds with probability
    \begin{equation}
        p_1 \times (1-p_2)^{d-1} = \Theta\left(\frac{1}{\sqrt{\ln(d)}}\right)d^{-2-c_0} \times \left( 1 -  \Theta\left(\frac{1}{\sqrt{\ln(d)}}\right)d^{-2-c_0}\right)^{d-1} =  \Theta\left(\frac{1}{\sqrt{\ln(d)}}\right)d^{-2-c_0}
    \end{equation}
    The last equality holds because defining $f(d) = d^{-2-c_0}$ and $d$ being sufficiently large,
    \begin{equation}
        g(d) \coloneqq |(d-1)\ln(1-f(d))| \le (d-1) \times (f(d) + O(f(d)^2)) \le O(d^{-1})
    \end{equation}
    which means 
    \begin{equation}
        (1-f(d))^{d-1} = e^{-g(d)} \in (1 - O(d^{-1}) , 1)
    \end{equation}
    
    \item Given $\vv\in\calD$, $|S_+^{*(0)}(\vv)|$ is a binomial random variable, with each Bernoulli trial (ranging over $r\in[m]$) having success probability $p_1 (1-p_2)^{d-1}$. Therefore, $\mathbb{E}\left[ |S_+^{*(0)}(\vv)| \right] = m p_1 (1-p_2)^{d-1} = \Theta\left(\frac{1}{\sqrt{\ln(d)}}\right) d^{c_0}$. 
    
    Now recall the Chernoff bound of binomial random variables. Let $\{X_n\}_{n=1}^m$ be an iid sequence of Bernoulli random variable with success rate $p$, and $S_n = \sum_{n=1}^m X_n$. Then for any $\delta \in (0,1)$,
    \begin{equation}
    \begin{aligned}
        & \mathbb{P}[S_n \ge (1+\delta) mp] \le \exp\left(- \frac{\delta^2 mp}{3} \right) \\
        & \mathbb{P}[S_n \le (1-\delta) mp] \le \exp\left(- \frac{\delta^2 mp}{2} \right)
    \end{aligned}
    \end{equation}
    It follows that, for each $\vv\in\calD$, $|S_+^{*(0)}(\vv)| = \Theta\left(\frac{1}{\sqrt{\ln(d)}}\right) d^{c_0}$ with probability at least $1- \exp(-\Omega(\ln^{-1/2}(d)) d^{c_0})$. Taking union bound over all possible  $\vv\in\calD$, the random event still holds with probability at least $1- \exp(-\Omega(\ln^{-1/2}(d)) d^{c_0} + \calO(\ln(d))) \ge 1- \exp(-\Omega(d^{0.5 c_0})) $ (in sufficiently high dimension).
    
\end{enumerate}

The proof for $S_+^{(0)}(\vv)$ proceeds in virtually the same way, so we omit the calculations here.

To show the second point, in particular $ \left\vert \frac{|S_+^{*(0)}(\vv)|}{|S_+^{(0)}(\vv')|} - 1 \right\vert \le O\left( \frac{1}{\ln^5(d)}\right)$, we need to be a bit more careful in our bounds of the relevant sets. In particular, we need to directly use the CDF of gaussian random variables:
\begin{equation}
\begin{aligned}
& \left\vert \mathbb{P}\left[ \langle \vw_{+,r}^{(0)}, \vv \rangle \ge \sigma_0 \sqrt{4 + 2c_0} \sqrt{\ln(d) + \frac{1}{\ln^5(d)}} \right](1 \pm O(d^{-1})) - \mathbb{P}\left[ \langle \vw_{+,r}^{(0)}, \vv' \rangle \ge \sigma_0 \sqrt{4 + c_0} \sqrt{\ln(d) - \frac{1}{\ln^5(d)}} \right] \right\vert \\
\le  & \frac{1}{2\sqrt{2\pi}} \int^{\sqrt{4 + 2c_0} \sqrt{\ln(d) + \frac{1}{\ln^5(d)}}}_{\sqrt{4 + 2c_0} \sqrt{\ln(d) - \frac{1}{\ln^5(d)}}} e^{-\epsilon^2/2} d\epsilon + O\left(\frac{1}{d^{3+c_0}\sqrt{\ln(d)}}\right)\\
\le & \frac{1}{2\sqrt{2\pi}} d^{-2-c_0}e^{(2+c_0)/\ln^5(d)} \sqrt{4 + 2c_0}\left( \sqrt{\ln(d) + \frac{1}{\ln^5(d)}} - \sqrt{\ln(d) - \frac{1}{\ln^5(d)}}\right) + O\left(\frac{1}{d^{3+c_0}\sqrt{\ln(d)}}\right)\\
= & \frac{1}{2\sqrt{2\pi}} d^{-2-c_0}e^{(2+c_0)/\ln^5(d)} \sqrt{4 + 2c_0} \frac{\frac{2}{\ln^5(d)}}{\sqrt{\ln(d) + \frac{1}{\ln^5(d)}} + \sqrt{\ln(d) - \frac{1}{\ln^5(d)}}} + O\left(\frac{1}{d^{3+c_0}\sqrt{\ln(d)}}\right)
\end{aligned}
\end{equation}
The expected difference in number between the two sets is just the above expression multiplied by $m = \Theta(d^{2 + 2c_0})$, and with probability at least $1 - \exp(-\Omega(d^{-c_0/4}))$, the difference term satisfies
\begin{equation}
\begin{aligned}
    & \frac{1}{2\sqrt{2\pi}} (1\pm d^{-c_0/2}) \Theta(d^{c_0}) e^{(2+c_0)/\ln^5(d)} \sqrt{4 + 2c_0} \frac{\frac{2}{\ln^5(d)}}{\sqrt{\ln(d) + \frac{1}{\ln^5(d)}} + \sqrt{\ln(d) - \frac{1}{\ln^5(d)}}} \pm  O\left(\frac{d^{2+2c_0}}{d^{3+c_0}\sqrt{\ln(d)}}\right) \\
    \in & \Theta\left(\frac{1}{\sqrt{\ln(d)}}\right) d^{c_0} \times \frac{1}{\ln^5(d)}
\end{aligned}
\end{equation}
By further noting from before that $|S_+^{(0)}(\vv)| = \Theta\left(\frac{1}{\sqrt{\ln(d)}}\right) d^{c_0}$, $ \left\vert \frac{|S_+^{*(0)}(\vv)|}{|S_+^{(0)}(\vv')|} - 1 \right\vert \le O\left( \frac{1}{\ln^5(d)}\right)$ follows. The proof of $\left\vert \frac{|S_+^{*(0)}(\vv)|}{|S_+^{*(0)}(\vv')|} - 1 \right\vert\le O\left( \frac{1}{\ln^5(d)}\right)$ follows a very similar argument, so we omit the calculations here.

Now, as for the set $S_{reg}^{(0)}$, we know for any $r\in[m]$ and $\vv_i\in\calD$, 
\begin{equation}
    \mathbb{P}\left[ \langle \vw_{+,r}^{(0)}, \vv_i \rangle \ge \sigma_0 \sqrt{10} \sqrt{\ln(d)} \right] \le O\left(\frac{1}{\sqrt{\ln(d)}}\right)d^{-5}.
\end{equation}
Taking the union bound over $r$ and $i$ yields
\begin{equation}
    \mathbb{P}\left[ \exists r \text{ and } i \text{ s.t.} \langle \vw_{+,r}^{(0)}, \vv_i \rangle \ge \sigma_0 \sqrt{10} \sqrt{\ln(d)} \right] \le md O\left(\frac{1}{\sqrt{\ln(d)}}\right)d^{-5} < d^{-2}.
\end{equation}

Finally, to show $\left\vert \calU_{+,r}^{(0)} \right\vert \le O(1)$ holds for every $(+,r)$, we just need to note that for any arbitrary $(+,r)$ neuron, the probability of $\left\vert \calU_{+,r}^{(0)} \right\vert > 4$ is no greater than
\begin{equation}
\begin{aligned}
p_2^{4}\binom{d}{4} \le O\left(\frac{1}{\ln^{2}d}\right) d^{-8-4c_0} \times d^4 \le  O\left(\frac{1}{\ln^{2}d}\right) d^{-4-4c_0}
\end{aligned}
\end{equation}
Taking union bound over all $m \le O\left(d^{2+2c_0}\right)$ neurons yields the desired result.

\end{proof}




\newpage
\section{Phase I: (Almost) Constant Loss, Neurons Diversify}
\label{section: appendix, phase I coarse}
\begin{definition}
We define $T_0$ to be the first time which there exists some sample $n$ such that 
\begin{equation}
    F_c^{(T_0)}(\mX_n^{(T_0)}) \ge d^{-1}
\end{equation}
Without loss of generality assume $c = +$. Define phase I to be the time $t \in [0, T_0)$.
\end{definition}

\begin{definition}
We define $S^{(t)}_+(\vv)\subseteq +\times [m]$ as follows: $\exists n$ such that on any $p\in\calP(\mX_n^{(t)}; \vv)$, 
\begin{equation}
    \sigma\left(\langle \vw_{+,r}^{(t)}, \vx_{n,p}^{(t)} \rangle + b_{+,r}^{(t)}\right) > 0
\end{equation}

\end{definition}

\subsection{Main results}
\begin{theorem}[Phase 1 SGD update properties]
\label{prop: phase 1 sgd induction}


For every $t \in [0, T_0)$, the following properties hold with probability at least $ 1-O(mNPT_0d^{-C_p})$,
\begin{enumerate}
    \item For every possible $(+,r) \in S_{+}^{*(0)}(\vv_+)$, 
    \begin{equation}
    \begin{aligned}
    \vw_{+,r}^{(t)} 
    = & \vw_{+,r}^{(0)} + \eta t \Bigg(  \left(\frac{1}{2} \pm \psi_1^{(T)}\right) \sqrt{1 \pm \iota}\left(1 \pm s^{*-1/2}\right) \pm O\left(\frac{1}{\ln^{10}(d)}\right)\Bigg) \frac{s^*}{2P} \vv_{+}  + \eta t \vzeta^{(t)}_{+,r}
    \end{aligned}
    \end{equation}
    where $\vzeta^{(t)}_{+,r} \sim \calN(\vzero, \sigma_{\zeta}^{(t)2} \mI)$,  $\sigma_{\zeta}^{(t)} = \sigma_{\zeta} \left(  \left(\frac{1}{2} \pm \psi_1^{(T)}\right) \sqrt{1 \pm \iota}\left(1 \pm s^{*-1/2}\right)\right) \frac{s^*}{2P}$, and $\vert \psi_1^{(t)} \vert \le d^{-1}$.
    \item For every possible choice of $c$ and every possible $(+,r) \in S_{+}^{*(0)}(\vv_{+,c})$, 
    \begin{equation}
    \begin{aligned}
    \vw_{+,r}^{(t)} 
    = & \vw_{+,r}^{(0)} + \eta t \Bigg(  \left(\frac{1}{2} \pm \psi_1^{(T)}\right) \sqrt{1 \pm \iota}\left(1 \pm s^{*-1/2}\right) \pm O\left(\frac{1}{\ln^{10}(d)}\right)\Bigg)  \frac{s^*}{2 k_+ P} \vv_{+,c}  + \eta t\vzeta^{(t)}_{+,r}
    \end{aligned}
    \end{equation}
    where $\vzeta^{(t)}_{+,r} \sim \calN(\vzero, \sigma_{\zeta}^{(t)2} \mI)$, and $\sigma_{\zeta}^{(t)} = \sigma_{\zeta} \left(  \left(\frac{1}{2} \pm \psi_1^{(T)}\right) \sqrt{1 \pm \iota}\left(1 \pm s^{*-1/2}\right) \right) \frac{s^*}{2k_+P}$.
    \item For every $\vv = \vv_+, \vv_{+,c}$, every neuron $(+,r) \in S_{+}^{*(0)}(\vv)$ remain activated on any patch $p \in \calP(\mX_n^{(t)}; \vv)$ and no other patch throughout phase I, therefore, each one of them receive exactly the same update at every iteration $t$
    \item For any $(+,r) \notin S^{(0)}_+(\vv_+)$, $(+,r) \notin S^{(t)}_+(\vv)$ is also true. Therefore, $S^{(t)}_+(\vv_+) \subseteq S^{(0)}_+(\vv_+)$.
    \item For any $\vv = \vv_-, \vv_{-,c}$ and any $(+,r) \in S^{(0)}_+(\vv)$, the neurons only possibly receive updates in the nonpositive direction. In other words, $\langle \vw_{+,r}^{(t)}, \vv \rangle$ does not grow in magnitude. Furthermore, $S^{(t)}_+(\vv) \subseteq S^{(0)}_+(\vv)$.
    \item The above results also hold with the ``$+$'' and ``$-$'' signs flipped.
\end{enumerate}
\end{theorem}
\begin{proof}
We first consider neurons in the group $S^{*(0)}_+(\vv_{+})$, fix any $(+,r) \in S^{*(0)}_+(\vv_{+})$; proof for neurons in $S^{*(0)}_+(\vv_{+,c})$ proceed in virtually the same way except for estimate for the number of total patches which $\vv_{+,c}$ appears in. 

The SGD update rule produces the following update:
\begin{align}
     \vw_{+,r}^{(t+1)}
    = & \vw_{+,r}^{(t)} + \eta \frac{1}{NP} \times\\
    & \sum_{n=1}^N \Bigg( 
    \mathbbm{1}\{y_n = +\}[1-\text{logit}_+^{(t)}(\mX_n^{(t)})]\sum_{p\in[P]} \sigma'(\langle \vw_{+,r}^{(t)}, \vx_{n,p}^{(t)} \rangle + b_{+,r}^{(t)}) \vx_{n,p}^{(t)} \label{expression: common feat, on-diag}\\
    & + \mathbbm{1}\{y_n = -\} [-\text{logit}_+^{(t)}(\mX_n^{(t)}) ]\sum_{p\in[P]} \sigma'(\langle \vw_{+,r}^{(t)}, \vx_{n,p}^{(t)}  \rangle + b^{(t)}_{+,r})  \vx_{n,p}^{(t)} \Bigg) \label{expression: common feat, off-diag}
\end{align}
In particular,
\begin{equation}
\begin{aligned}
\eqref{expression: common feat, on-diag} 
= & \sum_{n=1}^N \mathbbm{1} \{ y_n = +\}\left(\frac{1}{2} \pm \psi^{(t)}_1\right) \times \\
& \Bigg\{\mathbbm{1}\{|\calP(\mX_n^{(t)}; \vv_{+})|>0\} \Bigg[\sum_{p\in\calP(\mX_n^{(t)}; \vv_{+})} \sigma'(\langle \vw_{+,r}^{(t)}, \alpha_{n,p}^{(t)} \vv_{+} + \vzeta^{(t)}_{n,p} \rangle + b_{+,r}^{(t)}) \left(\alpha_{n,p}^{(t)} \vv_{+} + \vzeta_{n,p}^{(t)} \right) \\
& + \sum_{p\notin\calP(\mX_n^{(t)}; \vv_{+})} \sigma'(\langle \vw_{+,r}^{(t)}, \vx_{n,p}^{(t)} \rangle + b_{+,r}^{(t)})  \vx_{n,p}^{(t)} \Bigg] \\
& + \mathbbm{1}\{|\calP(\mX_n^{(t)}; \vv_{+})|=0\} \sum_{p\in[P]} \sigma'(\langle \vw_{+,r}^{(t)}, \vx_{n,p}^{(t)} \rangle + b_{+,r}^{(t)}) \vx_{n,p}^{(t)} \Bigg\} \\
= & \sum_{n=1}^N \mathbbm{1} \{ y_n = +\}\left(\frac{1}{2} \pm \psi^{(t)}_1\right) \times \\
& \Bigg\{\mathbbm{1}\{|\calP(\mX_n^{(t)}; \vv_{+})|>0\} \Bigg[\sum_{p\in\calP(\mX_n^{(t)}; \vv_{+})} \mathbbm{1}\left\{ \langle \vw_{+,r}^{(t)}, \alpha_{n,p}^{(t)} \vv_{+} + \vzeta_{n,p}^{(t)} \rangle \ge b_{+,r}^{(t)} \right\}  \left(\alpha_{n,p}^{(t)}\vv_{+} + \vzeta_{n,p}^{(t)}\right) \\
& + \sum_{p\notin\calP(\mX_n^{(t)}; \vv_{+})} \mathbbm{1}\left\{\langle \vw_{+,r}^{(t)}, \vx_{n,p}^{(t)} \rangle \ge  b_{+,r}^{(t)}\right\} \vx_{n,p}^{(t)} \Bigg] \\
& + \mathbbm{1}\{|\calP(\mX_n^{(t)}; \vv_{+})|=0\} \sum_{p\in[P]} \mathbbm{1}\left\{\langle \vw_{+,r}^{(t)}, \vx_{n,p}^{(t)} \rangle \ge  b_{+,r}^{(t)}\right\} \vx_{n,p}^{(t)} \Bigg\}
\end{aligned}
\end{equation}

The rest of the proof proceeds by induction (in Phase 1).

First, recall that we set $b_{c,r}^{(0)} = - \sqrt{4 + 2c_0}\sqrt{\ln(d)}$, and $\Delta b_{c,r}^{(t)} = -\frac{\| \Delta \vw_{c,r}^{(t)} \|_2}{\ln^5(d)}$ for all $t$ in phase 1, and for any $+$-class sample $\mX_n$ with $p\in\calP(\mX_n^{(t)}; \vv_{+})$, $\alpha_{n,p}^{(t)} \in \sqrt{1\pm \iota}$ by our data assumption.

\textbf{Base case $t = 0$.}

With probability at least $1-O(mNPd^{-C_p})$, by Lemma \ref{lemma: independent gaussian vector inner product concentration}, we have for all possible choices of $r,n,p$:
\begin{equation}
    \left\vert \langle \vw_{+,r}^{(0)}, \vzeta_{n,p}^{(0)} \rangle \right\vert \le O(\sigma_0\sigma_{\zeta} \sqrt{d\ln(d)}) \le O\left(\frac{\sigma_0}{\ln^{9}(d)}\right)
\end{equation}
It follows that
\begin{equation}
\begin{aligned}
    \langle \vw_{+,r}^{(0)}, \alpha_{n,p}^{(0)} \vv_{+} + \vzeta_{n,p}^{(0)} \rangle 
    = & \sigma_0\left\{\sqrt{1 \pm \iota}\times \left(\sqrt{4+2 c_0} \sqrt{\ln(d) + 1/\ln^5(d)}, \sqrt{10}\sqrt{\ln(d)} \right) \pm \frac{1}{\ln^{9}(d)} \right\} \\
    = & \sigma_0\left\{\left(\sqrt{1 - \iota}\sqrt{4 + 2c_0}\sqrt{\ln(d) + 1/\ln^5(d)}, \sqrt{1 + \iota}\sqrt{10}\sqrt{\ln(d)} \right) \pm \frac{1}{\ln^{9}(d)} \right\} 
\end{aligned}
\end{equation}

Employing the basic identity $a - b = \frac{a^2 - b^2}{a + b}$, we have the lower bound
\begin{equation}
\begin{aligned}
    \sigma_0^{-1}\left(\langle \vw_{+,r}^{(0)}, \alpha_{n,p}^{(0)} \vv_{+} + \vzeta_{n,p}^{(0)} \rangle + b_{+,r}^{(0)} \right)
    & \ge \sqrt{(1 - \iota)(4 + 2c_0)(\ln(d) + 1/\ln^5(d))} -  \sqrt{(4 + 2c_0)\ln(d)} - O\left(\frac{1}{\ln^{9}(d)}\right) \\
    & = \frac{(1 - \iota)(4 + 2c_0)(\ln(d) + 1/\ln^5(d)) - (4 + 2c_0)\ln(d)}{\sqrt{(1 - \iota)(4 + 2c_0)(\ln(d) + 1/\ln^5(d))} +  \sqrt{(4 + 2c_0)\ln(d)}} - O\left(\frac{1}{\ln^{9}(d)}\right) \\
    & = \frac{(4+2c_0)(-\iota\ln(d) + (1-\iota)/\ln^5(d))}{\sqrt{(1 - \iota)(4 + 2c_0)(\ln(d) + 1/\ln^5(d))} +  \sqrt{(4 + 2c_0)\ln(d)}} - O\left(\frac{1}{\ln^{9}(d)}\right) \\
    & > 0
\end{aligned}
\end{equation}
The last inequality holds since $\iota \le \frac{1}{\polyln(d)}$ and $d$ is sufficiently large such that $\frac{1}{\ln^{9}(d)}$ does not drive the positive term down past $0$.

Therefore,
\begin{equation}
    \langle \vw_{+,r}^{(0)}, \alpha_{n,p}^{(0)} \vv_{+} + \vzeta_{n,p}^{(0)} \rangle + b_{+,r}^{(0)} \in \sigma_0\left(0, \sqrt{1+\iota}\sqrt{10}\sqrt{\ln(d)} + O\left(\frac{1}{\ln^{9}(d)}\right)\right]
\end{equation}
In particular, with probability at least $1-O(mNPd^{-C_p})$, for any $+$-class sample $\mX_n^{(0)}$, any $p \in \calP(\mX_n^{(t)}; \vv_{+})$, $r\in S_{+}^{(0)}$,
\begin{equation}
\begin{aligned}
    & \mathbbm{1}\left\{ \langle \vw_{+,r}^{(0)}, \alpha_{n,p}^{(0)} \vv_{+} + \vzeta_{n,p}^{(0)} \rangle \ge b_{+,r}^{(0)} \right\}  \left(\alpha_{n,p}^{(0)} \vv_{+} + \vzeta_{n,p}^{(0)}\right) \\
    = & 1 \times \left(\sqrt{1 \pm \iota} \vv_{+} + \vzeta_{n,p}^{(0)}\right),
\end{aligned}
\end{equation}

while on the rest of the patches of samples $\mX_n^{(0)}$ satisfying $|\calP(\mX_n^{(0)}; \vv_{-})| > 0$ and on any patch of samples on which $|\calP(\mX_n^{(0)}; \vv_{+/-})| = 0$, the patch $\vx_{n,p}^{(0)}$ is either a common/subclass-feature patch or a feature-noise patch. In the former case we have, for some $\vv \perp \vv_{+}$:
\begin{equation}
    \langle \vw_{+,r}^{(0)}, \vx_{n,p}^{(0)} \rangle = \langle \vw_{+,r}^{(0)}, \alpha_{n,p}^{(0)} \vv + \vzeta_{n,p}^{(0)} \rangle \le  \sigma_0 \sqrt{1+\iota} \sqrt{(4+2c_0)\left(\ln(d) - \frac{1}{\ln^5(d)}\right)} + O\left(\frac{\sigma_0}{\ln^{9}(d)}\right)
\end{equation}
This leads to
\begin{equation}
\begin{aligned}
    \langle \vw_{+,r}^{(0)}, \vx_{n,p}^{(0)} \rangle + b_{+,r}^{(0)} 
    \le & \sigma_0 \sqrt{1+\iota} \sqrt{(4+2c_0)(\ln(d) - 1/\ln^5(d))} + O\left(\frac{\sigma_0}{\ln^{9}(d)}\right) - \sqrt{4 + 2c_0}\sqrt{\ln(d) } \sigma_0\\
    = & \sigma_0 \left(\frac{(4+2c_0)(1 + \iota)(\ln(d) - 1/\ln^5(d)) - (4+2c_0)\ln(d)}{\sqrt{(4+2c_0)(\ln(d) - 1/\ln^5(d))} + \sqrt{4 + 2c_0}\sqrt{\ln(d) }}   + O\left(\frac{1}{\ln^{9}(d)}\right)\right) \\
    = & \sigma_0 \left(\frac{(4+2c_0)\iota\ln(d) - (1+\iota)/\ln^5(d)}{\sqrt{(4+2c_0)(\ln(d) - 1/\ln^5(d))} + \sqrt{4 + 2c_0}\sqrt{\ln(d) }}   + O\left(\frac{1}{\ln^{9}(d)}\right)\right) \\
    < & 0
\end{aligned}
\end{equation}
Since $\iota \le \frac{1}{\polyln(d)}$, as long as $d$ is sufficiently large, $\vw_{+,r}^{(0)}$ cannot activate on this patch $\vx_{n,p}^{(0)}$.

The fact that $\vw_{+,r}^{(0)}$ cannot activate on feature-noise patches is shown in virtually the same way, so we omit the computation here.

Now we may compute \eqref{expression: common feat, on-diag} at $t=0$ more explicitly. In particular, with probability at least $1-O(mNPd^{-C_p})$, for all possible choices of $(+,r) \in S_{+}^{*(0)}(\vv_+)$:
\begin{equation}
\begin{aligned}
\eqref{expression: common feat, on-diag} 
= & \sum_{n=1}^N \mathbbm{1} \{ y_n = +\}\left(\frac{1}{2} \pm \psi^{(0)}_1\right) \times \\
& \Bigg\{\mathbbm{1}\{|\calP(\mX_n^{(0)}; \vv_{+})|>0\} \Bigg[\sum_{p\in\calP(\mX_n^{(0)}; \vv_{+})}\left(\sqrt{1 \pm \iota} \vv_{+} + \vzeta_{n,p}^{(0)}\right) + \sum_{p\notin\calP(\mX_n^{(0)}; \vv_{+})} 0 \Bigg] \\
& + \mathbbm{1}\{|\calP(\mX_n^{(0)}; \vv_{+})|=0\} \sum_{p\in[P]} 0 \Bigg\} \\
= & \left(\frac{1}{2} \pm \psi^{(0)}_1\right) \sum_{n=1}^N \mathbbm{1}\{ y_n = +, |\calP(\mX_n^{(0)}; \vv_{+})| > 0\} \sum_{p\in\calP(\mX_n^{(0)}; \vv_{+})}\left(\sqrt{1 \pm \iota}\vv_{+} + \vzeta_{n,p}^{(0)}\right) \\
= & \left(\frac{1}{2} \pm \psi^{(0)}_1\right) \times \\
& \left\vert \left\{(n,p)\in[N]\times[P]: y_n = +, |\calP(\mX_n^{(0)}; \vv_{+})| > 0, p \in \calP(\mX_n^{(0)}; \vv_{+}) \right\} \right\vert \left(\sqrt{1 \pm \iota} \vv_{+} \right) \\
& +  \sum_{n=1}^N \sum_{p\in\calP(\mX_n^{(0)}; \vv_{+})}\{ y_n = +\}\left(\frac{1}{2} \pm \psi^{(0)}_1\right) \vzeta_{n,p}^{(0)}
\end{aligned}
\end{equation}

On average,
\begin{equation}
\begin{aligned}
    & \mathbb{E}\left[ \left\vert \left\{(n,p)\in[N]\times[P]: y_n = +, |\calP(\mX_n^{(0)}; \vv_{+})| > 0, p \in \calP(\mX_n^{(0)}; \vv_{+}) \right\} \right\vert \right] \\
    = & \frac{s^*}{P} \times P \times N_+ = \frac{s^* N}{2}
\end{aligned}
\end{equation}
Furthermore, with our parameter choices, and by concentration of binomial random variables, with probability at least $1 - e^{-\Omega(d^{0.5})}$,
\begin{equation}
     \left\vert \left\{(n,p)\in[N]\times[P]: y_n = +, |\calP(\mX_n^{(0)}; \vv_{+})| > 0, p \in \calP(\mX_n^{(0)}; \vv_{+}) \right\} \right\vert =  \frac{s^* N}{2} \left(1 \pm s^{*-1/2}\right)
\end{equation}
must be true. 

It follows that, with probability at least $1-O(mNPd^{-C_p})$,
\begin{equation}
\begin{aligned}
    \eqref{expression: common feat, on-diag} 
    = & \left(\frac{1}{2} \pm \psi^{(0)}_1\right) \times \frac{s^* N}{2} \left(1 \pm s^{*-1/2}\right) \times  \left(\sqrt{1 \pm \iota} \vv_{+} \right)  \\
    & + \sum_{n=1}^N \sum_{p\in\calP(\mX_n^{(0)}; \vv_{+})}\{ y_n = +\}\left(\frac{1}{2} \pm \psi^{(0)}_1\right) \vzeta_{n,p}^{(0)}
\end{aligned}
\end{equation}

The other component \eqref{expression: common feat, off-diag} of the SGD update can be computed easily using a similar argument, because with probability at least $1-O(mNPd^{-C_p})$, $\vw_{+,r}^{(0)}$ simply does not activate on any patch of the $-$-class samples.

Therefore, by the SGD update rule, with probability at least $1-O(mNPd^{-C_p})$, for every $(+,r) \in S_{+}^{*(0)}(\vv_+)$:
\begin{equation}
\begin{aligned}
\vw_{+,r}^{(1)} 
= & \vw_{+,r}^{(0)} + \eta \left(\frac{1}{2} \pm \psi^{(0)}_1\right)\frac{s^* N}{2NP}\left(1 \pm s^{*-1/2}\right) \times \sqrt{1 \pm \iota} \vv_{+} + \eta \vzeta^{(0)}_{+,r} \\
= &  \vw_{+,r}^{(0)} + \eta \left( \sqrt{1 - \iota}\left(1 - s^{*-1/2}\right) \left(\frac{1}{2} - \psi_1^{(0)}\right), \left(\frac{1}{2} + \psi_1^{(0)}\right)\sqrt{1 + \iota}\left(1 + s^{*-1/2}\right) \right) \frac{s^*}{2P} \vv_{+}  + \eta \vzeta^{(0)}_{+,r}
\end{aligned}
\end{equation}
where $\vzeta^{(0)}_{+,r} \sim \calN(\vzero, \sigma_{\zeta}^{(0)2} \mI)$, and $\sigma_{\zeta}^{(0)} = \sigma_{\zeta}\left( \sqrt{1 - \iota}\left(1 - s^{*-1/2}\right) \left(\frac{1}{2} - \psi_1^{(0)}\right), \left(\frac{1}{2} + \psi_1^{(0)}\right)\sqrt{1 + \iota}\left(1 + s^{*-1/2}\right) \right) \frac{s^*}{2P}$. This proves the base case for point 1. 

%With probability at least $1-e^{-\Omega(d)}$, 
%\begin{equation}
%\begin{aligned}
%    \left\vert \langle \vzeta^{(0)}_{+,r}, \sqrt{1+\iota}\vv_{+} \rangle \right\vert 
%    \le & \sigma_{\zeta}\left(\frac{1}{2} + \psi_1^{(0)}\right)\sqrt{1 + \iota}\left(1 + s^{*-1/2}\right) \frac{s^*}{2P}\sqrt{d}\\
%    \le & \left(\frac{1}{2} + \psi_1^{(0)}\right)\sqrt{1 + \iota}\left(1 + s^{*-1/2}\right) \frac{s^*}{2P} O\left(\frac{1}{\ln^{10}(d)}\right)
%\end{aligned}
%\end{equation}

%It follows that
%\begin{equation}
%\begin{aligned}
%\langle \Delta \vw_{+,r}^{(0)}, \vv_{+} \rangle 
%= & \eta \Bigg( \left(\frac{1}{2} - \psi_1^{(0)}\right) \sqrt{1 - \iota}\left(1 - s^{*-1/2}\right)  - %O\left(\frac{1}{\ln^{10}(d)}\right) , \\
%& \left(\frac{1}{2} + \psi_1^{(0)}\right)\sqrt{1 + \iota}\left(1 + s^{*-1/2}\right)  + O\left(\frac{1}{\ln^{10}(d)}\right) \Bigg) \frac{s^*}{2P}
%\end{aligned}
%\end{equation}

%Furthermore, with the update rule $\Delta b_{c,r}^{(0)} = -\frac{\| \Delta \vw_{c,r}^{(0)} \|_2}{\ln^5(d)}$, for any patch $\vx_{n,p}^{(1)}$ on which some $\vv \perp \vv_{+}$ dominates (so a common/subclass-feature patch), we have
%\begin{equation}
%\begin{aligned}
%    \langle \vw_{+,r}^{(1)}, \vx_{n,p}^{(1)} \rangle + b_{+,r}^{(1)} 
%    = & \left(\langle \vw_{+,r}^{(0)}, \vx_{n,p}^{(1)} \rangle + b_{+,r}^{(0)}\right) + \left(\langle \Delta\vw_{+,r}^{(0)}, \vx_{n,p}^{(1)} \rangle + \Delta b_{+,r}^{(0)}  \right) \\
%    \le & \left( \left\vert \langle \vw_{+,r}^{(0)}, \sqrt{1+\iota}\vv + \vzeta_{n,p}^{(1)} \rangle \right\vert - \sqrt{4+2c_0}\sqrt{\ln(d)}\sigma_0\right) + \left( \left\vert \langle \Delta\vw_{+,r}^{(0)}, \sqrt{1+\iota}\vv + \vzeta_{n,p}^{(1)} \rangle \right\vert  - \frac{\| \Delta \vw_{+,r}^{(0)} \|_2}{\ln^5(d)} \right) \\
%    \le & 0 +  \left(\eta  O(1) \times \frac{s^*}{2P} \langle \vv_{+}, \vzeta_{n,p}^{(1)}\rangle + \langle \eta \vzeta^{(0)}_{+,r}, O(1)\vv + \vzeta_{n,p}^{(1)} \rangle - \frac{\| \Delta \vw_{+,r}^{(0)} \|_2}{\ln^5(d)} \right) \\
%    < & \eta O\left(\frac{1}{\ln^{10}(d)}\right) \frac{s^*}{2P} + \eta O\left(\frac{1}{\ln^{20}(d)}\right) \frac{s^*}{2P}  - \eta \Omega\left(\frac{1}{\ln^5(d)}\right) \left[  \frac{s^*}{2P} -  O\left(\frac{1}{\ln^{10}(d)}\right) \frac{s^*}{2P} \right] \\
%    < & 0
%\end{aligned}
%\end{equation}
%The second last inequality holds by applying inverse triangular inequality to $\| \Delta \vw_{c,r}^{(0)} \|_2$ and invoking Lemma \ref{lemma: gaussian vector norm concentration}.

The same line of argument holds on the feature-noise patches.

Since the update expression is the same for every $(+,r) \in S_+^{*(0)}(\vv_+)$, the neurons in $S_+^{*(0)}(\vv_+)$ must receive exactly the same update at step $t=0$. By the above derivations, it is also not hard to see that $S^{(1)}(\vv_+) \subseteq S^{(0)}(\vv_+)$ due to the non-activation of neurons that correlate weakly with $\vv_+$ at initialization (inner product less than $\sigma_0 \sqrt{4+2c_0}\sqrt{\ln(d) - \frac{1}{\ln^5(d)}}$). So far, we have proven the base case of points 3. and 4.

Finally, it is also easy to show that for any $\vv = \vv_-, \vv_{-,c}$ and any $(+,r) \in S^{(0)}_+(\vv)$, the neurons only receive updates in the nonpositive direction. In other words, $\langle \vw_{+,r}^{(t)}, \vv \rangle$ does not grow in magnitude. We just need to note that any patch dominated by $\vv$ belongs to the ``$-$'' class (the only possibility of $\vv$ appearing in the patches of a $+$-class sample is when we have a feature-noise patch, in which case we have $\gamma \vv$ with $\gamma \le \frac{1}{\ln^{10}(d)}$; following an argument virtually identical to how we showed non-activation of neurons in $S^{*(0)}_+(\vv)$ on feature noise patches as before, we know every $(+,r) \in S^{(0)}_+(\vv)$ cannot activate on these feature-noise patches), therefore, only the $[-\logit^{(t)}(\mX_n^{(t)})]$ is active, therefore the update direction on that patch has to be dominated by the direction of $-\vv$. Furthermore, $S^{(t)}_+(\vv) \subseteq S^{(0)}_+(\vv)$ follows easily from this line of argument.


\textbf{Inductive step}: assume the neurons' expression holds for $t\in [0, T]$ (with $T < T_0$ of course), prove for $t = T+1$.

At time $t=T$, 
\begin{equation}
\begin{aligned}
\vw_{+,r}^{(T)} 
= & \vw_{+,r}^{(0)} + \eta T \Bigg(  \left(\frac{1}{2} - \psi_1^{(T)}\right) \sqrt{1 - \iota}\left(1 - s^{*-1/2}\right) - O\left(\frac{1}{\ln^{10}(d)}\right), \\
& \left(\frac{1}{2} + \psi_1^{(T)}\right)\sqrt{1 + \iota}\left(1 + s^{*-1/2}\right) +  O\left(\frac{1}{\ln^{10}(d)}\right) \Bigg) \frac{s^*}{2P} \vv_{+}  + \eta T \vzeta^{(T)}_{+,r}
\end{aligned}
\end{equation}
where $\sigma_{\zeta}^{(T)} = \sigma_{\zeta}\left( \sqrt{1 - \iota}\left(1 - s^{*-1/2}\right) \left(\frac{1}{2} - \psi_1^{(t)}\right), \left(\frac{1}{2} + \psi_1^{(t)}\right)\sqrt{1 + \iota}\left(1 + s^{*-1/2}\right) \right) \frac{s^*}{2P}$

\textbf{Off-Diagonal Non-Activation at $t=T$}.

For a patch $p \notin \calP(\mX_n^{(T)}; \vv_{+})$, if it is a subclass/common-feature patch with $\vv \perp \vv_{+}$ dominating it, we have
\begin{equation}
\begin{aligned}
    & \langle \vw_{+,r}^{(T)}, \vx_{n,p}^{(T)} \rangle + b_{+,r}^{(T)} \\ 
    = & \langle \vw_{+,r}^{(T)}, \sqrt{1 \pm \iota}\vv + \vzeta_{n,p}^{(T)} \rangle + b_{+,r}^{(T)} \\
    = &  \langle \vw_{+,r}^{(0)}, \sqrt{1 \pm \iota}\vv + \vzeta_{n,p}^{(T)} \rangle + b_{+,r}^{(0)} \\
    & + \Bigg\langle \eta T \Bigg(  \left(\frac{1}{2} \pm \psi_1^{(T)}\right) \sqrt{1 \pm \iota}\left(1 \pm s^{*-1/2}\right) \pm O\left(\frac{1}{\ln^{10}(d)}\right)\Bigg) \frac{s^*}{2P} \vv_{+}  + \eta T \vzeta^{(T)}_{+,r}, \sqrt{1\pm\iota}\vv + \vzeta_{n,p}^{(T)} \Bigg\rangle + \sum_{\tau=0}^T \Delta b_{+,r}^{(\tau)}
\end{aligned}
\end{equation}

Recall from before that with probability at least $1-O(mNPd^{-C_p})$, for all possible $r, n, p$,
\begin{equation}
\begin{aligned}
    \langle \vw_{+,r}^{(0)}, \sqrt{1 \pm \iota} \vv + \vzeta_{n,p}^{(T)} \rangle + b_{+,r}^{(0)}
    < 0
\end{aligned}
\end{equation}
We also know, with probability at least $1 - e^{-\Omega(d^{0.5})}$,
\begin{equation}
\begin{aligned}
    \left\vert \langle \vzeta_{n,p}^{(T)}, \vv_{+} \rangle \right\vert \le O\left(\frac{1}{\ln^{10}(d)} \right)
\end{aligned}
\end{equation}
therefore
\begin{equation}
\begin{aligned}
    & \left\vert \langle \eta T \Bigg(  \left(\frac{1}{2} \pm \psi_1^{(T)}\right) \sqrt{1 \pm \iota}\left(1 \pm s^{*-1/2}\right) \pm O\left(\frac{1}{\ln^{10}(d)}\right)\Bigg) \frac{s^*}{2P} \vv_{+} , \vzeta_{n,p}^{(T)} \rangle \right\vert \\
    \le & \eta T \frac{s^*}{2P}O\left(\frac{1}{\ln^{10}(d)}\right)
\end{aligned}
\end{equation}
Moreover, with probability at least $1-e^{-\Omega(d^{0.5})}$,
\begin{equation}
\begin{aligned}
    \left\vert \langle \vzeta_{+,r}^{(T)}, \vv \rangle \right\vert \le \frac{s^*}{2P} \times O\left(\frac{1}{\ln^{10}(d)}\right)
\end{aligned}
\end{equation}
and with probability $1 - e^{-\Omega(d)}$,
\begin{equation}
    \left\vert \langle \vzeta_{+,r}^{(T)}, \vzeta_{n,p}^{(T)} \rangle \right\vert \le O\left( \sigma_{\zeta} \sigma_{\zeta}^{(T)} d\right) \le O\left( \frac{s^*}{2P}\frac{1}{\ln^{20}(d) d} d \right) \le \frac{s^*}{2P} \frac{1}{\ln^{19}(d)}
\end{equation}
therefore
\begin{equation}
\begin{aligned}
\langle \eta T \vzeta^{(T)}_{+,r}, \sqrt{1\pm\iota}\vv + \vzeta_{n,p}^{(T)} \rangle 
\le \eta T \frac{s^*}{2P} O \left(\frac{1}{\ln^{10}(d)} \right).
\end{aligned}
\end{equation}
It follows that with probability at least $1 - O(e^{-\Omega(d^{0.5})})$,
\begin{equation}
\begin{aligned}
    & \langle \eta T \Bigg(  \left(\frac{1}{2} \pm \psi_1^{(T)}\right) \sqrt{1 \pm \iota}\left(1 \pm s^{*-1/2}\right) \pm O\left(\frac{1}{\ln^{10}(d)}\right)\Bigg) \frac{s^*}{2P} \vv_{+} , \vzeta_{n,p}^{(T)} \rangle + \langle \eta \vzeta^{(T)}_{+,r}, \sqrt{1\pm\iota}\vv + \vzeta_{n,p}^{(T)} \rangle \\
    \le & \eta T \frac{s^*}{2P} \times O\left(\frac{1}{\ln^{10}(d)}\right)
\end{aligned}
\end{equation}

On the other hand, since $\Delta b_{+,r}^{(t)} = - \frac{\|\Delta \vw_{+,r}^{(t)}\|_2}{\ln^5(d)}$ for every $t$, from previous results and Lemma \ref{lemma: gaussian vector norm concentration}, we have
\begin{equation}
\begin{aligned}
\sum_{t=0}^T \frac{\|\Delta \vw_{+,r}^{(t)}\|_2}{\ln^5(d)} 
\ge & \eta T \left(\sqrt{1-\iota}(1-s^{*-1/2})\left(\frac{1}{2} - \psi_1^{((t)} \right) - O\left(\frac{1}{\ln^{10}(d)}\right)\right) \frac{s^*}{2P} \frac{1}{\ln^5(d)} \\
& - \eta T \frac{s^*}{2P} \times O\left(\frac{1}{\ln^{15}(d)}\right) 
\end{aligned}
\end{equation}
Therefore,
\begin{equation}
\begin{aligned}
&  \langle \vw_{+,r}^{(T)}, \sqrt{1 \pm \iota}\vv + \vzeta_{n,p}^{(T)} \rangle + b_{+,r}^{(T)} \\
\le & \eta T \frac{s^*}{2P} O\left(\frac{1}{\ln^{10}(d)}\right) - \eta T \left(\sqrt{1-\iota}(1-s^{*-1/2})\left(\frac{1}{2} - \psi_1^{((t)} \right) - O\left(\frac{1}{\ln^{10}(d)}\right)\right) \frac{s^*}{2P} \frac{1}{\ln^5(d)} \\
& + \eta T \frac{s^*}{2P}  O\left(\frac{1}{\ln^{15}(d)}\right) \\
\le & \eta T \frac{s^*}{2P}\left( O\left(\frac{1}{\ln^{10}(d)}\right) - \Omega\left(\frac{1}{\ln^{5}(d)}\right) \right) \\
< & 0
\end{aligned}
\end{equation}
Therefore, with probability at least $1-O(mNPd^{-C_p})$, for all possible $r, n$, for any patch $p \notin \calP(\mX_n^{(T)}; \vv_{+})$, if it is a subclass/feature-noise patch with $\vv \perp \vv_{+}$ dominating it, neuron $\vw_{+,r}^{(T)}$ will not activate on it.

\textbf{On-Diagonal Activations at $t=T$}.
First note that the following upper bound holds
\begin{equation}
\begin{aligned}
    \sum_{t=0}^T \frac{\|\Delta \vw_{+,r}^{(t)}\|_2}{\ln^5(d)} 
\le & \eta T \left(\sqrt{1+\iota}(1+s^{*-1/2})\left(\frac{1}{2} + \psi_1^{((t)} \right) + O\left(\frac{1}{\ln^{10}(d)}\right)\right) \frac{s^*}{2P} \frac{1}{\ln^5(d)} + \eta T \frac{s^*}{2P} O\left(\frac{1}{\ln^{15}(d)}\right)
\end{aligned}
\end{equation}

Now suppose $\calP(\mX_n^{(t)}; \vv_{+}) > 0$ and let $p\in\calP(\mX_n^{(t)}; \vv_{+})$. With probability at least $1-O(mNPd^{-C_p})$,
\begin{equation}
\begin{aligned}
\langle \vw_{+,r}^{(T)}, \vx_{n,p}^{(T)} \rangle + b_{+,r}^{(T)}
= & \langle \vw_{+,r}^{(0)} + \eta T \Bigg(  \left(\frac{1}{2} \pm \psi_1^{(T)}\right) \sqrt{1 \pm \iota}\left(1 \pm s^{*-1/2}\right) \pm O\left(\frac{1}{\ln^{10}(d)}\right)\Bigg) \frac{s^*}{2P} \vv_{+}  + \eta \vzeta^{(T)}_{+,r}, \vx_{n,p}^{(T)}\rangle + b_{+,r}^{(T)}\\
= & \langle \vw_{+,r}^{(0)}, \sqrt{1\pm\iota}\vv_{+} + \vzeta_{n,p}^{(T)} \rangle + b_{+,r}^{(0)} \\
& + \langle \eta T\Bigg(  \left(\frac{1}{2} \pm \psi_1^{(T)}\right) \sqrt{1 \pm \iota}\left(1 \pm s^{*-1/2}\right) \pm O\left(\frac{1}{\ln^{10}(d)}\right)\Bigg) \frac{s^*}{2P} \vv_{+}, \sqrt{1\pm\iota}\vv_{+} + \vzeta_{n,p}^{(T)}\rangle \\
& + \eta \langle \vzeta_{+,r}^{(T)}, \sqrt{1\pm\iota} \vv_{+} +   \vzeta_{n,p}^{(T)} \rangle + \sum_{t=0}^T \Delta b_{+,r}^{(t)} \\
\ge & 0 \\
& + \eta T \Bigg( (1-\iota)(1-s^{*-1/2})\left(\frac{1}{2} - \psi_1^{(T)}\right) - O\left(\frac{1}{\ln^{10}(d)}\right) \Bigg) \frac{s^*}{2P} - \eta T \frac{s^*}{2P} O\left( \frac{1}{\ln^{10}(d)}  \right)\\
& - \eta T \left(\sqrt{1+\iota}(1+s^{*-1/2})\left(\frac{1}{2} + \psi_1^{((t)} \right) + O\left(\frac{1}{\ln^{10}(d)}\right)\right) \frac{s^*}{2P} \frac{1}{\ln^5(d)} - \eta T \frac{s^*}{2P} O\left(\frac{1}{\ln^{15}(d)}\right) \\
> & 0
\end{aligned}
\end{equation}

In summary, for any $(+,r)\in S_{+}^{*(0)}(\vv_+)$, at $t = T$, $\vw_{+,r}^{(t)}$ must activate on any patch $\vx_{n,p}^{(T)}$ which satisfies $p \in \calP(\mX_n^{(T)}; \vv_{+})$ and not activate on any other patch. Therefore, these neurons receive exactly the same update at step $t$.

\textbf{Expression of $\vw_{+,r}^{(T+1)}$}.

We may examine expression \eqref{expression: common feat, on-diag} at time $t = T$. The derivation is basically the same as the one at $t=0$:
\begin{equation}
\begin{aligned}
\eqref{expression: common feat, on-diag} 
= & \sum_{n=1}^N \mathbbm{1} \{ y_n = +\}\left(\frac{1}{2} \pm \psi^{(T)}_1\right) \times \\
& \Bigg\{\mathbbm{1}\{|\calP(\mX_n^{(t)}; \vv_{+})|>0\} \Bigg[\sum_{p\in\calP(\mX_n^{(T)}; \vv_{+})} \mathbbm{1}\left\{ \langle \vw_{+,r}^{(T)}, \alpha_{n,p}^{(T)} \vv_{+} + \vzeta_{n,p}^{(T)} \rangle \ge b_{+,r}^{(T)} \right\}  \left(\alpha_{n,p}^{(T)}\vv_{+} + \vzeta_{n,p}^{(T)}\right) \\
& + \sum_{p\notin\calP(\mX_n^{(T)}; \vv_{+})} \mathbbm{1}\left\{\langle \vw_{+,r}^{(T)}, \vx_{n,p}^{(T)} \rangle \ge  b_{+,r}^{(T)}\right\} \vx_{n,p}^{(T)} \Bigg] \\
& + \mathbbm{1}\{|\calP(\mX_n^{(T)}; \vv_{+})|=0\} \sum_{p\in[P]} \mathbbm{1}\left\{\langle \vw_{+,r}^{(T)}, \vx_{n,p}^{(T)} \rangle \ge  b_{+,r}^{(T)}\right\} \vx_{n,p}^{(T)} \Bigg\} \\
= & \left(\frac{1}{2} \pm \psi^{(T)}_1\right) \times \\
& \left\vert \left\{(n,p)\in[N]\times[P]: y_n = +, |\calP(\mX_n^{(T)}; \vv_{+})| > 0, p \in \calP(\mX_n^{(T)}; \vv_{+}) \right\} \right\vert \left(\sqrt{1 \pm \iota} \vv_{+} \right) \\
& +  \sum_{n=1}^N \sum_{p\in\calP(\mX_n^{(T)}; \vv_{+})}\{ y_n = +\}\left(\frac{1}{2} \pm \psi^{(T)}_1\right) \vzeta_{n,p}^{(T)}
\end{aligned}
\end{equation}
Following similar derivation as in the $t = 0$ step, we arrive at, with probability at least $1-O(mNPd^{-C_p})$,
\begin{equation}
    \vw_{+,r}^{(T+1)} = \vw_{+,r}^{(0)} + \eta (T+1) \Bigg(  \left(\frac{1}{2} \pm \psi_1^{(T)}\right) \sqrt{1 \pm \iota}\left(1 \pm s^{*-1/2}\right) \pm O\left(\frac{1}{\ln^{10}(d)}\right)\Bigg) \frac{s^*}{2P} \vv_{+}  + \eta (T+1)\vzeta^{(T+1)}_{+,r}
\end{equation}
where $\vzeta^{(T+1)}_{+,r} \sim \calN\left(\vzero, \sigma_{\zeta}^{(T+1)2} \mI\right)$, and $\sigma_{\zeta}^{(T+1)} = \sigma_{\zeta}\left(  \left(\frac{1}{2} \pm \psi_1^{(T)}\right) \sqrt{1 \pm \iota}\left(1 \pm s^{*-1/2}\right) \right) \frac{s^*}{2P}$.

This completes the proof of point 1. Proof of point 3 follows from the fact that every neuron in $S^{*(0)}_+(\vv_+)$ still activate on the common-feature patches and no others at the current iteration, therefore they must receive the same update.

For point 2, the proof strategy is almost identical, the only difference is that at every iteration, the expected number of patches in which subclass features appear in is $s^*N/2k_+$. Therefore, we do not repeat the derivations.

For point 4, from the previous derivations we already know that if a neuron does not activate on common-feature patches $p \in \calP(\mX_n^{(t)};\vv_+)$ at time $t=0$, it will remain unactivated on common-feature patches in phase I, since with probability $1 - O(NPT_0 d^{-C_p})$, for $(+,r) \notin S^{(0)}_+(\vv_+)$, $\langle \vw_{+,r}^{(t)}, \sqrt{1+\iota}\vv_+ + \vzeta_{n,p}^{(t)}\rangle + b_{+,r}^{(t)} < \sigma_0\sqrt{4+2c_0}\sqrt{\ln(d) - \frac{1}{\ln^5(d)}} + O\left(\frac{\sigma_0}{\ln^9(d)}\right) - \sigma_0\sqrt{4+2c_0}\sqrt{\ln(d)} < 0$ for all $t \le T_0$. This means $S^{(t)}_+(\vv_+) \subseteq S^{(0)}_+(\vv_+)$ in phase I. The same argument holds for $S^{(t)}_+(\vv_{+,c}) \subseteq S^{(0)}_+(\vv_{+,c})$.

Proof of point 5 is identical to the base case, so we do not repeat it here.
\end{proof}
\begin{remark}
Note that since the update to every neuron $(+,r)\in S_+^{*(0)}(\vv_+)$ is the same at all time $t$, at the end of phase I, for every $(+,r)\in S_+^{*(0)}(\vv_+)$,
\begin{equation}
    \|\vw_{+,r}^{(T_0)} - \vw_{+,r'}^{(T_0)}\|_2 \le O(\sigma_0\sqrt{\ln(d)})
\end{equation}
\end{remark}
\begin{corollary}
$T_0 \le O\left( \left(\eta \frac{s^*}{P} \right)^{-1}\right) \ll O\left( \eta^{-1}d^2\right) \in \poly(d)$.
\end{corollary}
\begin{proof}
Easily follows from Theorem \ref{prop: phase 1 sgd induction}.
\end{proof}





\subsection{Auxiliary lemmas}
\begin{lemma}
\label{lemma: phase 1, loss scale bound}
During the time $t \in [0, T_0)$, for any $\mX_n^{(t)}$,
\begin{equation}
    1 - \logit^{(t)}_+(\mX_n^{(t)}) = \frac{1}{2} \pm O(d^{-1})
\end{equation}
The same holds for $1 - \logit^{(t)}_-(\mX_n^{(t)})$.

Therefore, $\vert \psi_1^{(t)} \vert \le O(d^{-1})$ for $t \in [0, T_0)$.
\end{lemma}
\begin{proof}
By definition of $T_0$, for any $t \in [0, T_0)$, we have $F_c^{(t)}(\mX_n^{(t)}) < d^{-1}$ for all $n$, therefore, using Taylor approximation,
\begin{equation}
    1 - \logit^{(t)}_+(\mX_n^{(t)}) = \frac{\exp(F_{-}^{(t)}(\mX_n^{(t)}))}{\exp(F_{+}^{(t)}(\mX_n^{(t)})) + \exp(F_{-}^{(t)}(\mX_n^{(t)}))} < \frac{\exp(d^{-1})}{1 + 1} \le \frac{1}{2} + O(d^{-1})
\end{equation}
The lower bound can be proven due to convexity of the exponential:
\begin{equation}
    \frac{\exp(F_{-}^{(t)}(\mX_n^{(t)}))}{\exp(F_{+}^{(t)}(\mX_n^{(t)})) + \exp(F_{-}^{(t)}(\mX_n^{(t)}))} > \frac{1}{2}\exp(-d^{-1}) \ge \frac{1}{2} - \frac{1}{2d}
\end{equation}
\end{proof}








\newpage
\section{Phase II: Loss Convergence, Large Neuron Movement}
\label{section: appendix, phase II coarse}
Recall that the desired probability events in Phase I happens with probability at least $1 - O\left(mNPT_0 d^{-C_p}\right)$.

In phase II, common-feature neurons start gaining large movement and drive the training loss down to $o(1)$. We show that the desired probability events occur with probability at least $1-o(1)$. 

We study the case of asymptotic order $T_1 \le \poly(d)$.



\subsection{Main results}
\begin{theorem}
Denote $C = \eta \frac{s^*}{2P}s^*\left\vert S_+^{*(0)}(\vv_+) \right\vert$, and write $A(t) = s^*\left\vert S_+^{*(0)}(\vv_+) \right\vert A_{+,r^*}^{*(t)}$ (see Lemma \ref{lemma: phase II, general} for definition). Define $t_0 = \exp(A(T_{1,1}))$.

Then with probability at least $1 - o(1)$, during $t \in [T_{1,1}, T_1]$, 
\begin{equation}
    A(t) = \ln(C(t - T_{1,1}) +t_0) + E(t)
\end{equation}
where $|E(t)| \le O\left(\frac{\ln(t) - \ln(C^{-1}t_0)}{\ln^4(d)}\right)$.

The same results also hold with the class signs flipped.
\end{theorem}
\begin{proof}
\textbf{Sidenote}: To make the writing a bit cleaner, we assume in the proof below that $C^{-1}t_0$ is an integer. The general case is easy to extend to by observing that $\left\vert \frac{1}{t - T_{1,1} + \ceil{C^{-1}t_0} } - \frac{1}{t - T_{1,1} + C^{-1}t_0 } \right\vert \le \frac{1}{(t - T_{1,1} + \ceil{C^{-1}t_0})(t - T_{1,1} + C^{-1}t_0) }$, which can be absorbed into the error term at every iteration since $\frac{1}{t - T_{1,1} + \ceil{C^{-1}t_0} } \ll \frac{1}{\ln^4(d)}$ due to $C^{-1}t_0 \ge \Omega(\sigma_0^{-1}/(\polyln(d)d^{c_0})) \gg d \gg \ln^4(d)$.

Based on result from Lemmas \ref{lemma: phase II, general} and \ref{lemma: phase II, after T1,1}, as long as $A(t)\le O\left(\ln(d)\right)$, we know during time $t\in[T_{1,1}, T_1]$ the update rule for $A(t)$ is as follows:
\begin{equation}
\begin{aligned}
A(t+1) - A(t)
= & C \exp\left\{ - (1\pm s^{*-1/3})\sqrt{1\pm\iota} \left(1 \pm O\left(\frac{1}{\ln^5(d)}\right)\right) \left(1 \pm O\left(k_+^{-1}\right) \right)  A(t) \right\}\\
& \times \left(1 \pm O\left(\frac{1}{\ln^5(d)}\right)\right)(1\pm s^{*-1/3}) \left(\sqrt{1\pm\iota} \pm \frac{1}{\ln^{10}(d)}\right) \\
= & C \exp\left\{ - A(t) \right\} \exp\left\{\pm O\left(\frac{1}{\ln^4(d)}\right)\right\} \left(1 \pm O\left(\frac{1}{\ln^5(d)}\right)\right) \\
= & C \exp\left\{ - A(t) \right\} \left(1 \pm \frac{C_1}{\ln^4(d)}\right) 
\end{aligned}
\end{equation}
where we write $C_1$ in place of $O(\cdot)$ for a more concrete update expression.

Consider the base case $t = T_{1,1}$. At $t+1$,
\begin{equation}
\begin{aligned}
A(t+1) 
= & \ln(C(t - T_{1,1}) + t_0)  \\
& + C \exp\left\{ - \ln(C(t - T_{1,1}) + t_0)) \right\} \left(1 \pm \frac{C_1}{\ln^4(d)} \right) \\
= & \ln(C) + \ln(t - T_{1,1} + C^{-1} t_0) \\
& + C \frac{1}{C(t - T_{1,1}) +  t_0}\left(1 \pm \frac{C_1}{\ln^4(d)}\right) \\
= & \ln(C) + \sum_{\tau=1}^{t-T_{1,1} + C^{-1}t_0 - 1} \frac{1}{\tau} + \frac{1}{2}\frac{1}{t-T_{1,1} + C^{-1}t_0} + \left[0, \frac{1}{8}\frac{1}{(t-T_{1,1} + C^{-1}t_0)^2}\right] \\
& + \frac{1}{t - T_{1,1} + C^{-1} t_0}\pm \frac{C_1}{\ln^4(d)}\frac{1}{t - T_{1,1} + C^{-1} t_0} \\
= & \ln(C) + \sum_{\tau=1}^{t-T_{1,1} + C^{-1}t_0} \frac{1}{\tau} + \frac{1}{2}\frac{1}{t-T_{1,1} + C^{-1}t_0} + \left[0, \frac{1}{8}\frac{1}{(t-T_{1,1} + C^{-1}t_0)^2}\right] \\
& \pm \frac{C_1}{\ln^4(d)}\frac{1}{t - T_{1,1} + C^{-1} t_0} \\
\end{aligned}
\end{equation}

Invoking Lemma \ref{lemma: harmonic series} again,
\begin{equation}
\begin{aligned}
A(t+1) 
= & \ln(C) + \ln(t+1 - T_{1,1} + C^{-1}t_0) \\
& - \frac{1}{2}\frac{1}{t+1-T_{1,1} + C^{-1}t_0} +\frac{1}{2}\frac{1}{t-T_{1,1} + C^{-1}t_0} \\
& + \left[- \frac{1}{8}\frac{1}{(t+1-T_{1,1} + C^{-1}t_0)^2}, 0 \right] + \left[0, \frac{1}{8}\frac{1}{(t-T_{1,1} + C^{-1}t_0)^2}\right] \\
& \pm \frac{C_1}{\ln^4(d)}\frac{1}{t - T_{1,1} + C^{-1} t_0} \\
= & \ln(C) + \ln(t+1 - T_{1,1} + C^{-1}t_0) \\
& +\frac{1}{2} \frac{1}{(t+1-T_{1,1} + C^{-1}t_0)(t-T_{1,1} + C^{-1}t_0)} \pm O\left( \frac{1}{(t+1-T_{1,1} + C^{-1}t_0)^2}\right) \\
& \pm \frac{C_1}{\ln^4(d)}\frac{1}{t - T_{1,1} + C^{-1} t_0} \\
\end{aligned}
\end{equation}

Notice that at step $t+1$, since $\frac{1}{t+1-T_{1,1} + C^{-1}t_0} \ll \frac{1}{\ln^4(d)}$, the error term $\vert E(t + 1) \vert = \vert A(t+1) - \ln(C(t+1-T_{1,1}) + t_0) \vert \le O\left(\frac{1}{\ln^4(d)}\right) \sum_{\tau=C^{-1}t_0}^{t+1-1+C^{-1}t_0} \frac{1}{\tau}$.

We proceed with the induction step. Assume the hypothesis true for $t \in [T_{1,1}, T]$, prove for $t + 1 = T + 1$.

We have the update
\begin{equation}
\begin{aligned}
A(t+1) 
= & \ln(C(t - T_{1,1}) + t_0) + E(t)  \\
& + C \exp\left\{ - \ln(C(t - T_{1,1}) + t_0) - E(t)) \right\} \left(1 \pm \frac{C_1}{\ln^4(d)} \right) \\
= & \ln(C) + \ln(t - T_{1,1} + C^{-1} t_0) + E(t) \\
& + C \frac{1}{C(t - T_{1,1}) +  t_0}\left(1 - E(t) \pm O(E(t)^2) \right)\left(1 \pm \frac{C_1}{\ln^4(d)}\right) \\
= & \ln(C) + \sum_{\tau=1}^{t-T_{1,1} + C^{-1}t_0 - 1} \frac{1}{\tau} + \frac{1}{2}\frac{1}{t-T_{1,1} + C^{-1}t_0} + \left[0, \frac{1}{8}\frac{1}{(t-T_{1,1} + C^{-1}t_0)^2}\right] \\
& + \frac{1}{t - T_{1,1} + C^{-1} t_0}\pm \frac{C_1}{\ln^4(d)}\frac{1}{t - T_{1,1} + C^{-1} t_0} \\
& + E(t) + \frac{1}{t - T_{1,1} + C^{-1} t_0} \left(- E(t) \pm O(E(t)^2) \right)\left(1 \pm \frac{C_1}{\ln^4(d)}\right)\\
= & \ln(C) + \sum_{\tau=1}^{t-T_{1,1} + C^{-1}t_0} \frac{1}{\tau} + \frac{1}{2}\frac{1}{t-T_{1,1} + C^{-1}t_0} + \left[0, \frac{1}{8}\frac{1}{(t-T_{1,1} + C^{-1}t_0)^2}\right] \\
& \pm \frac{C_1}{\ln^4(d)}\frac{1}{t - T_{1,1} + C^{-1} t_0} \\
& + E(t) + \frac{1}{t - T_{1,1} + C^{-1} t_0} \left(- E(t) \pm O(E(t)^2) \right)\left(1 \pm \frac{C_1}{\ln^4(d)}\right)\\
\end{aligned}
\end{equation}
Following basically the same derivation as in the base step (to compute the new error at step $t+1$, which scales with $\frac{1}{\ln^4(d)}\frac{1}{t - T_{1,1} + C^{-1} t_0}$), and by noting that the error passed down from the previous step $t$ does not grow in this step (in fact it slightly decreases):
\begin{equation}
    \left\vert E(t) + \frac{1}{t - T_{1,1} + C^{-1} t_0} \left(- E(t) \pm O(E(t)^2) \right)\left(1 \pm \frac{C_1}{\ln^4(d)}\right) \right\vert < |E(t)| \le O\left(\frac{1}{\ln^4(d)}\right) \sum_{\tau=C^{-1}t_0}^{t-1+C^{-1}t_0} \frac{1}{\tau}
\end{equation}
we can finish the inductive step, which completes the proof.

\end{proof}
\begin{corollary}
By the end of training $t = T_1$, with probability at least $1-o(1)$, the network response to any ``$+$'' class sample $\mX$ satisfies 
\begin{equation}
    F_{+}^{(T_1)}(\mX_n) \in \left(1 \pm O\left(\frac{1}{\ln^5(d)}\right)\right) \ln\left(\eta \frac{s^{*2}}{2P} \left\vert S_+^{*(0)}(\vv_+) \right\vert (T_1 - T_{1,1}) + \left(1 \pm O\left(\frac{1}{\ln^5(d)}\right)\right)\ln^5(d)\right)
\end{equation}

For any normal sample $\mX$, with probability at least $1 - o(1)$,
\begin{equation}
    F_{+}^{(T_1)}(\mX_n) \in \omega(1), \;\; F_{-}^{(T_1)}(\mX_n) \in o(1)
\end{equation}

However, for any ``$+$''-class sample $\mX_{\text{hard}}$ on which the common-feature patches are removed, 
\begin{equation}
    F_{+}^{(T_1)}(\mX_n), F_{-}^{(T_1)}(\mX_n) \in o(1)
\end{equation}

In other words, the fine-grained features are not learnt properly.

The same results also hold with the class signs flipped.
\end{corollary}
\begin{proof}
Trivially follows from the above theorem, Lemma \ref{lemma: phase II, general} and the parameter assumption $k_+ \ge \polyln(d)$.
\end{proof}





\subsection{Auxiliary lemmas}
\begin{lemma}
\label{lemma: phase II, after T1,1}
Define time $T_{1,1}$ to be the first point in time which the following identity holds on some $\mX_n^{(t)}$ belonging to the ``$+$'' class:
\begin{equation}
    \frac{\exp(F_-^{(t)}(\mX_n^{(t)}))}{\exp(F_-^{(t)}(\mX_n^{(t)}) - F_+^{(t)}(\mX_n^{(t)})) + 1} = 1 - O\left( \frac{1}{\ln^5(d)}\right)
\end{equation}
Then $T_{1,1} \le \poly(d)$.

Additionally, at this point in time, the above asymptotic property also holds on any $\mX_n^{(t)}$ belonging to the ``$+$'' class.

The same results also hold with the class signs flipped.
\end{lemma}
\begin{proof}
We first note that, asymptotically speaking, the training loss $[1 - \logit_+^{(t)}(\mX_n^{(t)})]$ on samples belonging to the ``$+$'' class at any time during $t \in [T_0, T_1]$, is monotonically decreasing from $\frac{1}{2} - O(d^{-1})$. This can be easily proven by observing that $F_+^{(t)}(\mX_n^{(t)})$ monotonically increases from the proof of Lemma \ref{lemma: phase II, general}. Therefore, before $F_+^{(t)}(\mX_n^{(t)}) \ge \ln\ln^5(d)$ on some $\mX_n^{(t)}$ belonging to the ``$+$'' class, 
\begin{equation}
\begin{aligned}
    [1 - \logit_+^{(t)}(\mX_n^{(t)})] = \frac{\exp(F_-^{(t)}(\mX_n^{(t)}))}{\exp(F_-^{(t)}(\mX_n^{(t)})) + \exp(F_+^{(t)}(\mX_n^{(t)}))} \ge \frac{1 - O(\sigma_0\ln(d)s^*d^{c_0})}{1 + O(\sigma_0\ln(d)s^*d^{c_0}) + \ln^5(d)} \ge \Omega\left(\frac{1}{\ln^5(d)}\right)
\end{aligned}
\end{equation}
Therefore, by the update expressions in the proof of Lemma \ref{lemma: phase II, general}, $F_+^{(t)}(\mX_n^{(t)})$ can reach $\ln\ln^5(d)$ in $\poly(d)$ time. Therefore, at time $T_{1,1}$, 
\begin{equation}
    \left\vert \frac{\exp(F_-^{(t)}(\mX_n^{(t)}))}{\exp(F_-^{(t)}(\mX_n^{(t)}) - F_+^{(t)}(\mX_n^{(t)})) + 1} - 1 \right\vert = \left\vert \frac{1 \pm O(\sigma_0\ln(d)s^*d^{c_0})}{[1 \pm O(\sigma_0\ln(d)s^*d^{c_0})]\frac{1}{\ln^5(d)} + 1 } - 1\right\vert \le O\left( \frac{1}{\ln^5(d)}\right)
\end{equation}

To see why the above asymptotic estimate also holds on any $\mX_n^{(t)}$ belonging to the ``$+$'' class, we just need to note from Lemma \ref{lemma: phase II, general} and the fact that $\left\vert \sqrt{1+\iota}/\sqrt{1-\iota} - 1\right\vert\le O\left(\frac{1}{\ln^5(d)}\right)$ that
\begin{equation}
    \left\vert \frac{\min_n F_+^{(t)}(\mX_n^{(t)})}{\max_n F_+^{(t)}(\mX_n^{(t)})} - 1 \right\vert
    \le O\left( \frac{1}{\ln^5(d)}\right),
\end{equation}
setting $\max_n F_+^{(t)}(\mX_n^{(t)}) = \ln\ln^5(d)$, we see $\min_n F_+^{(t)}(\mX_n^{(t)}) \ge 1 - O\left(\frac{1}{\ln^5(d)}\right)$. The rest of the argument follows easily like before.
\end{proof}

\begin{lemma}
\label{lemma: phase II, general}
For any $T_1 \in \poly(d)$, with probability at least $1 - o(1)$, during $t \in (T_0, T_1]$, for any $(+,r) \in S_+^{*(0)}(\vv_+)$,
\begin{equation}
\begin{aligned}
\Delta \vw_{+,r}^{(t)} 
= & \eta\exp\left\{ - (1\pm s^{*-1/3})\sqrt{1\pm\iota}  \left(1 \pm O\left(\frac{1}{\ln^5(d)}\right)\right) \left(1 \pm O\left(k_+^{-1}\right) \right) s^* A_{+,r^*}^{*(t)}\left\vert S^{*(0)}_+(\vv_+) \right\vert \right\} \\
& \times  \sum_{n=1}^N \mathbbm{1} \{ y_n = +\} \frac{\exp(F_-^{(t)}(\mX_n^{(t)}))}{\exp\left(F_-^{(t)}(\mX_n^{(t)})- \exp(F_+^{(t)}(\mX_n^{(t)}))\right) + 1 } (1\pm s^{*-1/3})\frac{s^*}{NP} \left(\sqrt{1\pm\iota}\vv_{+} + \vzeta_{n,p}^{(t)}\right)
\end{aligned}
\end{equation}

In other words, every neuron in $S_+^{*(0)}(\vv_+)$ remain singly activated and receive exactly the same updates at every iteration shown above, so their differences remain at most $O(\sigma_0 \ln(d))$ in $\ell_2$ norm. 

For simpler exposition, we pick an arbitrary $(+,r^*) \in S_+^{*(0)}(\vv_+)$ and write $A_{+,r^*}^{*(t)} \coloneqq \langle \vw_{+,r^*}^{(t)}, \vv_+ \rangle$, knowing that $\left\vert \sum_{(+,r^*) \in S_+^{*(0)}(\vv_+)} \langle \vw_{+,r}, \vv_+ \rangle - A_{+,r^*}^{*(t)} \left\vert S_+^{*(0)}(\vv_+) \right\vert \right\vert \le O(\sigma_0 \ln^{1/2}(d) d^{c_0}) \ll d^{-2}$

On ``$+$''-class samples, the neural network response satisfies the estimate
\begin{equation}
\begin{aligned}
    F_+^{(t)}(\mX_n^{(t)}) 
    = & (1\pm s^{*-1/3})\sqrt{1\pm\iota}  \left(1 \pm O\left(\frac{1}{\ln^5(d)}\right)\right)  \left(1 \pm O\left(k_+^{-1}\right)\right) s^* A_{+,r^*}^{*(t)}\left\vert S^{*(0)}_+(\vv_+) \right\vert
\end{aligned}
\end{equation}

Also, we have the neuron group invariance for $\vv = \vv_{+}, \vv_{+,c}$
\begin{equation}
    S_+^{(t)}(\vv) \subseteq S_+^{(0)}(\vv)
\end{equation}

For any $\vv = \vv_-, \vv_{-,c}$ and any $(+,r) \in S^{(0)}_+(\vv)$, the neurons only receive updates in the nonpositive direction. In other words, $\langle \vw_{+,r}^{(t)}, \vv \rangle \le O(\sigma_0 \ln(d))$. Furthermore, $S^{(t)}_+(\vv) \subseteq S^{(0)}_+(\vv)$.

Finally,
\begin{equation}
\frac{ A_{+,c,r^*}^{*(t)}}{ A_{+,r^*}^{*(t)}} = \Theta\left(k_+^{-1}\right)
\end{equation}

The same claims hold for the ``$-$'' class neurons (with the class signs flipped).
\end{lemma}


\begin{proof}

\textbf{Base case}, $t = T_0$, prove for $t + 1$.

First define $A_{+,r^*}^{*(t)} \coloneqq \langle \vw_{+,r^*}, \vv_+ \rangle$, $(+,r^*) \in S_+^{*(0)}(\vv_+)$; similarly for $A_{+,c,r^*}^{*(t)} \coloneqq \langle \vw_{+,r^*}, \vv_{+,c} \rangle$. Note that the choice of $r^*$ does not really matter, since we know from phase I that every neuron in $S_+^{*(0)}(\vv_+)$ evolve at exactly the same rate, so by the end of phase I, $\|\vw_{+,r}^{(T_0)} - \vw_{+,r'}^{(T_0)}\|_2 \le O(\sigma_0\ln(d)) \ll \|\vw_{+,r}^{(T_0)}\|_2$ for any $r, r' \in S_+^{*(0)}(\vv_+)$.

Let $(+,r) \in S_+^{*(0)}(\vv_+)$.
Similar to phase I, consider the update equation
\begin{align}
    \vw_{+,r}^{(t+1)}
    = & \vw_{+,r}^{(t)} + \eta \frac{1}{NP} \times\\
    & \sum_{n=1}^N \Bigg( 
    \mathbbm{1}\{y_n = +\}[1-\text{logit}_+^{(t)}(\mX_n^{(t)})]\sum_{p\in[P]} \sigma'(\langle \vw_{+,r}^{(t)}, \vx_{n,p}^{(t)} \rangle + b_{+,r}^{(t)}) \vx_{n,p}^{(t)}\\
    & + \mathbbm{1}\{y_n = -\} [-\text{logit}_+^{(t)}(\mX_n^{(t)}) ]\sum_{p\in[P]} \sigma'(\langle \vw_{+,r}^{(t)}, \vx_{n,p}^{(t)}  \rangle + b^{(t)}_{+,r})  \vx_{n,p}^{(t)} \Bigg)
\end{align}

For the on-diagonal update expression, we have
\begin{equation}
\begin{aligned}
& \sum_{n=1}^N 
\mathbbm{1}\{y_n = +\}[1-\text{logit}_+^{(t)}(\mX_n^{(t)})]\sum_{p\in[P]} \sigma'(\langle \vw_{+,r}^{(t)}, \vx_{n,p}^{(t)} \rangle + b_{+,r}^{(t)}) \vx_{n,p}^{(t)} \\
= & \sum_{n=1}^N \mathbbm{1} \{ y_n = +\} [1-\text{logit}_+^{(t)}(\mX_n^{(t)})]\\
& \Bigg\{\mathbbm{1}\{|\calP(\mX_n^{(t)}; \vv_{+})|>0\} \Bigg[\sum_{p\in\calP(\mX_n^{(t)}; \vv_{+})} \mathbbm{1}\left\{ \langle \vw_{+,r}^{(t)}, \alpha_{n,p}^{(t)} \vv_{+} + \vzeta_{n,p}^{(t)} \rangle \ge b_{+,r}^{(t)} \right\}  \left(\alpha_{n,p}^{(t)}\vv_{+} + \vzeta_{n,p}^{(t)}\right) \\
& + \sum_{p\notin\calP(\mX_n^{(t)}; \vv_{+})} \mathbbm{1}\left\{\langle \vw_{+,r}^{(t)}, \vx_{n,p}^{(t)} \rangle \ge  b_{+,r}^{(t)}\right\} \vx_{n,p}^{(t)} \Bigg] \\
& + \mathbbm{1}\{|\calP(\mX_n^{(t)}; \vv_{+})|=0\} \sum_{p\in[P]} \mathbbm{1}\left\{\langle \vw_{+,r}^{(t)}, \vx_{n,p}^{(t)} \rangle \ge  b_{+,r}^{(t)}\right\} \vx_{n,p}^{(t)} \Bigg\}
\end{aligned}
\end{equation}
Very similar to the argument in the proof of Theorem \ref{prop: phase 1 sgd induction} (we omit the calculations here for space), the non-activation on the patches that do not contain $\vv_{+}$ can be shown to hold with probability at least $1 - O(\exp(-\Omega(d)))$ at time $t$ (note that the probability is no longer $1 - O(mNPd^{-C_p})$ now since the inner product of the initialized kernel with noise no longer pose nontrivial influence on the overall activation value now), therefore, the above expression reduces to
\begin{equation}
\begin{aligned}
& \sum_{n=1}^N \mathbbm{1} \{ y_n = +, |\calP(\mX_n^{(t)}; \vv_{+})|>0\} [1-\text{logit}_+^{(t)}(\mX_n^{(t)})] \sum_{p\in\calP(\mX_n^{(t)}; \vv_{+})} \left(\alpha_{n,p}^{(t)}\vv_{+} + \vzeta_{n,p}^{(t)}\right)
\end{aligned}
\end{equation}
Note that for samples $\mX_n^{(t)}$ with $y_n = +$,

\begin{equation}
\begin{aligned}
    [1-\text{logit}_+^{(t)}(\mX_n^{(t)})] 
    = & \frac{\exp(F_-^{(t)}(\mX_n^{(t)}))}{\exp(F_-^{(t)}(\mX_n^{(t)})) + \exp(F_+^{(t)}(\mX_n^{(t)}))} \\
\end{aligned}
\end{equation}


Now we need to estimate the network response $F_+^{(t)}(\mX_n^{(t)})$. First note that, at time $T_0$, in addition to the common-feature neurons activating on $\mX_n^{(t)}$, the subclass feature neurons belonging to subclass $(+,c(n))$ also activate according to Proposition \ref{prop: phase 1 sgd induction}. The rest of the neurons, i.e. $(+,r)\in [m] - S^{(0)}_+(\vv_+) - S^{(0)}_+(\vv_{+,c(n)})$ do not activate. Therefore, with probability at least $1 - \exp(-\Omega(s^{*1/3}))$, we have the upper bound
\begin{equation}
\begin{aligned}
    F_+^{(t)}(\mX_n^{(t)}) 
    \le & \sum_{p\in\calP(\mX^{(t)}; \vv_+)} \sum_{(+,r)\in S^{(0)}_+(\vv_+)} \langle \vw_{+,r}^{(t)}, \vv_+ + \vzeta_{n,p}^{(t)} \rangle + b_{+,r}^{(t)} \\
    & +  \sum_{p\in\calP(\mX^{(t)}; \vv_{+,c(n)})} \sum_{(+,r)\in S^{(0)}_+(\vv_{+,c(n)})} \langle \vw_{+,r}^{(t)}, \vv_{+,c(n)} + \vzeta_{n,p}^{(t)} \rangle + b_{+,r}^{(t)} \\
    \le & (1+s^{*-1/3})\sqrt{1+\iota} s^* \left(1 + O\left(\frac{1}{\ln^{9}(d)}\right)\right) \left(A_{+,r^*}^{*(t)}\left\vert S^{(0)}_+(\vv_+) \right\vert + A_{+,c(n),r^*}^{*(t)}\left\vert S^{(0)}_+(\vv_{+,c(n)}) \right\vert \right)
\end{aligned}
\end{equation}

The second inequality is true since $\max_r \langle \vw_{+,r}^{(t)}, \vv_+ \rangle \le A_{+,r^*}^{*(t)} + O(\sigma_0\ln(d))$, and for any $(+,r)\in S^{(0)}_+(\vv_+)$, $\vert\langle \vw_{+,r}^{(t)}, \vzeta_{n,p}^{(t)}\rangle\vert \le O(1/\ln^{9}(d))A_{+,r^*}^{*(t)}$. The bias value is negative (and so less than $0$). 

To further refine the bound, we recall $\left\vert S^{*(0)}_+(\vv) \right\vert / \left\vert S^{*(0)}_+(\vv') \right\vert, \left\vert S^{*(0)}_+(\vv) \right\vert / \left\vert S^{(0)}_+(\vv') \right\vert = 1 \pm O(1/\ln^5(d))$. Moreover, note that from phase I, for any $c$,
\begin{equation}
    \frac{\Delta A_{+,c,r^*}^{*(t)}}{\Delta A_{+,r^*}^{*(t)}} = \Theta\left(k_+^{-1}\right)
\end{equation}
Therefore, we obtain the bound
\begin{equation}
\begin{aligned}
    F_+^{(t)}(\mX_n^{(t)}) 
    \le & (1+s^{*-1/3})\sqrt{1+\iota} s^* \left(1 + O\left(\frac{1}{\ln^5(d)}\right)\right) \left(1 + O\left(k_+^{-1}\right) \right) \left( 1 + O\left( \frac{1}{\ln^5(d)} \right) \right) A_{+,r^*}^{*(t)}\left\vert S^{*(0)}_+(\vv_+) \right\vert
\end{aligned}
\end{equation}

Following a similar argument, we also have the lower bound
\begin{equation}
\begin{aligned}
    F_+^{(t)}(\mX_n^{(t)}) 
    \ge & \sum_{p\in\calP(\mX^{(t)}; \vv_+)} \sum_{(+,r)\in S^{*(0)}_+(\vv_+)} \sigma\left(\langle \vw_{+,r}^{(t)}, \vv_+ + \vzeta_{n,p}^{(t)} \rangle + b_{+,r}^{(t)}\right) \\
    & +  \sum_{p\in\calP(\mX^{(t)}; \vv_{+,c(n)})} \sum_{(+,r)\in S^{*(0)}_+(\vv_{+,c(n)})} \sigma\left(\langle \vw_{+,r}^{(t)}, \vv_{+,c(n)} + \vzeta_{n,p}^{(t)} \rangle + b_{+,r}^{(t)} \right)\\
    \ge & (1-s^{*-1/3})\sqrt{1-\iota} s^* \left(1 - O\left(\frac{1}{\ln^5(d)}\right)\right) \left( 1 - O\left( \frac{1}{\ln^5(d)} \right) \right)  \left(1 - O\left(k_+^{-1}\right)\right) A_{+,r^*}^{*(t)}\left\vert S^{*(0)}_+(\vv_+) \right\vert
\end{aligned}
\end{equation}

The neurons in $ S^{*(0)}_+(\vv_+)$ have to activate, therefore they serve a key role in the lower bound, the bias bound for them is simply $-A_{+,r^*}^{*(t)}\Theta(1/\ln^5(d))$; the neurons in $S^{(0)}_+(\vv_{+,c})$ contribute at least $0$ due to the ReLU activation; the rest of the neurons do not activate. The same reasoning holds for the $S^{*(0)}_+(\vv_{+,c})$.

Knowing that neurons in $S^{*(0)}_+(\vv_+)$ cannot activate on the patches in samples belonging to the ``$-$'' class, now we may write the update expression for every $(+,r)\in S^{*(t)}_+(\vv_+)$ as (their updates are identical, same as in phase I):
\begin{equation}
\begin{aligned}
\Delta \vw_{+,r}^{(t)} 
= & \frac{\eta}{NP}\sum_{n=1}^N 
\mathbbm{1}\{y_n = +\}[1-\text{logit}_+^{(t)}(\mX_n^{(t)})]\sum_{p\in[P]} \sigma'(\langle \vw_{+,r}^{(t)}, \vx_{n,p}^{(t)} \rangle + b_{+,r}^{(t)}) \vx_{n,p}^{(t)} \\
= &\frac{\eta}{NP} \sum_{n=1}^N \mathbbm{1} \{ y_n = +, |\calP(\mX_n^{(t)}; \vv_{+})|>0\} \exp(-F_+^{(t)}(\mX_n^{(t)})) \frac{\exp(F_-^{(t)}(\mX_n^{(t)}))}{\exp\left(F_-^{(t)}(\mX_n^{(t)})- \exp(F_+^{(t)}(\mX_n^{(t)}))\right) + 1 } \\
& \times \sum_{p\in\calP(\mX_n^{(t)}; \vv_{+})}  \left(\alpha_{n,p}^{(t)}\vv_{+} + \vzeta_{n,p}^{(t)}\right) \\
= & \eta \Bigg[\exp\left\{ - (1+s^{*-1/3})\sqrt{1+\iota} s^* \left(1 + O\left(\frac{1}{\ln^5(d)}\right)\right) \left(1 + O\left(k_+^{-1}\right) \right)  A_{+,r^*}^{*(t)}\left\vert S^{*(0)}_+(\vv_+) \right\vert \right\}, \\
& \exp\left\{- (1-s^{*-1/3})\sqrt{1-\iota} s^* \left( 1 - O\left( \frac{1}{\ln^5(d)} \right) \right)  \left(1 - O\left(k_+^{-1}\right)\right) A_{+,r^*}^{*(t)}\left\vert S^{*(0)}_+(\vv_+) \right\vert \right\}\Bigg] \\
& \times  \sum_{n=1}^N \mathbbm{1} \{ y_n = +\} \frac{\exp(F_-^{(t)}(\mX_n^{(t)}))}{\exp\left(F_-^{(t)}(\mX_n^{(t)})- \exp(F_+^{(t)}(\mX_n^{(t)}))\right) + 1 } \\
& \times (1\pm s^{*-1/3})\frac{s^*}{NP} \left(\sqrt{1\pm\iota}\vv_{+} + \vzeta_{n,p}^{(t)}\right)
\label{expression: phase II, common neuron update estimate}
\end{aligned}
\end{equation}

It follows that
\begin{equation}
\begin{aligned}
\Delta A_{+,r^*}^{*(t)} 
= & \eta \Bigg[\exp\left\{ - (1+s^{*-1/3})\sqrt{1+\iota} s^* \left(1 + O\left(\frac{1}{\ln^5(d)}\right)\right) \left(1 + O\left(k_+^{-1}\right) \right)  A_{+,r^*}^{*(t)}\left\vert S^{*(0)}_+(\vv_+) \right\vert \right\}, \\
& \exp\left\{- (1-s^{*-1/3})\sqrt{1-\iota} s^* \left( 1 - O\left( \frac{1}{\ln^5(d)} \right) \right)  \left(1 - O\left(k_+^{-1}\right)\right) A_{+,r^*}^{*(t)}\left\vert S^{*(0)}_+(\vv_+) \right\vert \right\}\Bigg] \\
& \times (1\pm s^{*-1/3}) \frac{s^*}{NP} \left(\sqrt{1\pm\iota} \pm \frac{1}{\ln^{10}(d)}\right) \sum_{n=1}^N \mathbbm{1} \{ y_n = +\} \frac{\exp(F_-^{(t)}(\mX_n^{(t)}))}{\exp\left(F_-^{(t)}(\mX_n^{(t)})- \exp(F_+^{(t)}(\mX_n^{(t)}))\right) + 1 } 
\end{aligned}
\end{equation}

To show that the neurons from $S_+^{*(0)}(\vv_+)$ stay activated on the ``+''-class common feature patches at step $t+1$, we simply need to note that the bias grows in norm sufficiently slowly:
\begin{equation}
\begin{aligned}
\Delta b_{+,r}^{(t)} 
= & - \frac{\|\vw_{+,r}^{(t)}\|_2}{\ln^5(d)} \\
\le & - \eta \frac{1}{\ln^5(d)}\exp\left\{ - (1+s^{*-1/3})\sqrt{1+\iota} s^* \left(1 + O\left(\frac{1}{\ln^5(d)}\right)\right) \left(1 + O\left(k_+^{-1}\right) \right)  A_{+,r^*}^{*(t)}\left\vert S^{*(0)}_+(\vv_+) \right\vert \right\}  \\
& \times  (1 - s^{*-1/3}) \frac{s^*}{NP} \left(\sqrt{1-\iota} - \frac{1}{\ln^{10}(d)}\right) \sum_{n=1}^N \mathbbm{1} \{ y_n = +\} \frac{\exp(F_-^{(t)}(\mX_n^{(t)}))}{\exp\left(F_-^{(t)}(\mX_n^{(t)})- \exp(F_+^{(t)}(\mX_n^{(t)}))\right) + 1 }
\end{aligned}
\end{equation}
which is clearly lower in magnitude than $\Delta A_{+,r^*}^{*(t)}$. Therefore, neurons in $S_+^{*(0)}(\vv_+)$ remain singly activated on the common $\vv_+$ patches at time $t + 1$ and receive exactly the same update at time $t$ (so they still differ by at most $O(\sigma_0\ln(d))$). Moreover, since neurons outside the set $S_+^{(0)}(\vv_+)$ stay unactivated on $\vv_+$ within $\poly(d)$ time (simple to show, same idea as in proof of Proposition \ref{prop: phase 1 sgd induction}), $S_+^{(t+1)}(\vv_+) \subseteq S_+^{(0)}(\vv_+)$ remains true. Similar argument holds for $S_+^{(t+1)}(\vv_{+,c}) \subseteq S_+^{(0)}(\vv_{+,c})$.

Furthermore, note that for any subclass-feature neurons $(+,r) \in S_{+}^{(t)}(\vv_{+,c})$, if they receive a nonzero update at this iteration, they can only be updated on the $(+,c)$ samples, and on these samples, the network loss is exactly the same as the one appearing above for the common-feature neurons. In other words, the same situation in phase I happens for the neurons at time step $t+1$:
\begin{equation}
    \frac{\Delta A_{+,c,r^*}^{*(t)}}{\Delta A_{+,r^*}^{*(t)}} = \Theta\left(k_+^{-1}\right) \implies \frac{ A_{+,c,r^*}^{*(t+1)}}{ A_{+,r^*}^{*(t+1)}} = \Theta\left(k_+^{-1}\right)
\end{equation}

Additionally, the way to show that the neurons in $(+,r) \in S^{(0)}_+(\vv_-) \cup S^{(0)}_+(\vv_{-,c})$ experience zero or nonpositive growth is identical to the one in the proof of Theorem \ref{prop: phase 1 sgd induction}, so we omit the proof here.

\textbf{Induction step}. Assume for $t \in [T_0, T]$, prove for $t+1 = T+1$.

At step $t$, the proof proceeds in almost exactly the same way as in the base case, since the expressions derived in that step is generic for time $t$ beyond $T_0$. We simply need to note that the $S_+^{*(0)}(\vv_+)$ stay activated on the common $\vv_+$ patches at step $t$ due to the induction hypothesis, therefore we may use the exact same estimate on the probability events (depending on the randomness of the samples at iteration $t$) to obtain the estimate shown in expression \eqref{expression: phase II, common neuron update estimate}. The rest of the argument is identical, so we do not repeat them here. This finishes the proof.


\end{proof}














\begin{lemma} [\cite{boas1971}]
\label{lemma: harmonic series}
The partial sum of harmonic series satisfies the following identity:
\begin{equation}
    \sum_{k=1}^{n-1}\frac{1}{k} = \ln(n) + \calE - \frac{1}{2n} - \epsilon_n
\end{equation}
where $\calE$ is the EulerMascheroni constant (approximately 0.58), and $\epsilon_n \in [0, 1/8n^2]$.
\end{lemma}









% ---------------------------------------------------------------------------------------------
\newpage

\section{Fine-grained Learning Dynamics}
\label{section: appendix, finegrained}
This section treats the learning dynamics of using fine-grained labels to train the NN; the analysis will be much simpler since the technical analysis overlaps significantly with that in the previous sections.

The learner is
\begin{equation}
    F_{+,c}(\mX) = \sum_{r=1}^{m_{+,c}} a_{+,c,r}\sum_{p=1}^P \sigma(\langle \vw_{+,c,r}, \vx_p\rangle + b_{+,c,r}), \;\; c\in[k_+]
\end{equation}
with frozen linear classifier weights $ a_{+,c,r} = 1$.

The SGD dynamics induced by the training loss is now
\begin{equation}
\begin{aligned}
    \vw_{+, c,r}^{(t+1)} 
    = \vw_{+, c,r}^{(t)} + \eta \frac{1}{NP} \sum_{n=1}^N \Bigg( 
    & \mathbbm{1}\{y_n = (+,c)\}[1-\text{logit}_{+,c}^{(t)}(\mX_n^{(t)})]\sum_{p\in[P]}\sigma'(\langle \vw_{+,c,r}^{(t)}, \vx_{n,p}^{(t)} \rangle +b_{c,r}^{(t)}) \vx_{n,p}^{(t)} + \\
    & \mathbbm{1}\{y_n\neq (+, c)\} [-\text{logit}_{+,c}^{(t)}(\mX_n^{(t)}) ]\sum_{p\in[P]} \sigma'(\langle \vw_{+,c,r}^{(t)}, \vx_{n,p}^{(t)}  \rangle + b^{(t)}_{c,r}) \vx_{n,p}^{(t)}\Bigg)
\end{aligned}
\end{equation}
The manual bias tuning is slightly more complicated here, as we clip off any possible abrupt changes caused by the bias, this way we can prevent activating neurons from being killed off too much (we look ahead one step in the updates to determine the actual bias value at that step, which can then be used to compute the neuron's post-nonlinearity activation value):
\begin{equation}
    b_{+, c,r}^{(t+1)} = b_{+, c,r}^{(t)} - \min \left(\frac{\|\Delta \vw_{+,c,r}^{(t)}\|_2}{\ln^5(d)}, 0.1 \max_{n,p} \langle \Delta \vw_{+, c,r}^{(t)}, \vx_{n,p}^{(t+1)} \rangle\right)
\end{equation}

We assign $m_{+,c} = \Theta(d^{1+2c_0})$ neurons to each subclass $(+,c)$.

The initialization scheme is identical to the coarse-training case, except we choose a slightly less negative $b_{c,r}^{(0)} = - \sigma_0\sqrt{2 + 2c_0}\sqrt{\ln(d)}$.

The parameter choices remain the same as before.

\subsection{Initialization geometry}
\begin{definition}
Define the following sets of interest of the hidden neurons:
\begin{enumerate}
    \item $\calU_{+,c,r}^{(0)} = \{\vv \in \calD: \langle \vw_{+,c,r}^{(0)}, \vv\rangle \ge \sigma_0 \sqrt{2 + 2c_0}\sqrt{\ln(d) - \frac{1}{\ln^5(d)}}\}$
    \item Given $\vv \in \calD$, $S^{*(0)}_{+,c}(\vv) \subseteq + \times [m]$ satisfies:
    \begin{enumerate}
        \item $\langle \vw_{+, c, r}^{(0)}, \vv \rangle \ge \sigma_0 \sqrt{2 + 2c_0} \sqrt{\ln(d) + \frac{1}{\ln^5(d)}}$
        \item $\forall \vv' \in \calD \text{ s.t. } \vv' \perp \vv, \, \langle \vw_{+,c,r}^{(0)}, \vv' \rangle < \sigma_0 \sqrt{2 + 2c_0} \sqrt{\ln(d) - \frac{1}{\ln^5(d)}}$ 
    \end{enumerate}
    \item Given $\vv \in \calD$, $S_{+,c}^{(0)}(\vv) \subseteq + \times [m]$ satisfies:
    \begin{enumerate}
        \item $\langle \vw_{+,c,r}^{(0)}, \vv \rangle \ge \sigma_0 \sqrt{2 + 2c_0} \sqrt{\ln(d) - \frac{1}{\ln^5(d)}}$
    \end{enumerate}
    \item For any $(+,r) \in S_{+, c, reg}^{*(0)} \subseteq + \times [m_{+,c}]$:
    \begin{enumerate}
        \item $\langle \vw_{+,c,r}^{(0)}, \vv \rangle \le \sigma_0 \sqrt{10} \sqrt{\ln(d)} \; \forall \vv\in\calD$
        \item $\left\vert \calU_{+,r}^{(0)} \right\vert \le O(1)$
    \end{enumerate}
\end{enumerate}
\end{definition}

\begin{prop}
At $t=0$, for all $\vv \in \calD$, the following properties are true with probability at least $1- d^{-2}$ over the randomness of the initialized kernels:
\begin{enumerate}
    \item $|S_{+,c}^{*(0)}(\vv)|, |S_{+,c}^{(0)}(\vv)| = \Theta\left(\frac{1}{\sqrt{\ln(d)}}\right) d^{c_0}$
    \item In particular, $\left\vert \frac{|S_{+,c}^{*(0)}(\vv)|}{|S_{+,c}^{(0)}(\vv)|} - 1 \right\vert= O\left( \frac{1}{\ln^5(d)}\right)$
    \item $S_{+,c,reg}^{(0)} = [m_{+,c}]$
\end{enumerate}
\end{prop}
\begin{proof}
This proof proceeds in virtually the same way as in the proof of Proposition \ref{prop: init geometry, coarse}, so we omit it here.
\end{proof}

\subsection{Pretraining}
\begin{definition}
Let $T_0 > 0$ be the first time that there exists some $\mX_n^{(t)}$ and $c$ such that $\logit_{+,c}^{(t)}(\mX_n^{(t)}) \ge \frac{1}{1.5 k_+}$.
\end{definition}

\begin{prop} 
Within time $T_0^*\le T_0 \in \poly(d)$, with probability at least $1 - o(1)$, given any hard example $\mX_{\text{hard}}$ belonging to subclass $(+,c)$ on which the common-feature patches are missing, we have the subclass network response $F_{+,c}^{(t)}(\mX_{\text{hard}}) \ge \Omega(1)$ and $F_{+,c'}^{(t)}(\mX_{\text{hard}}), F_{-,c''}^{(t)}(\mX_{\text{hard}}) < o(1)$ for all $c' \neq c$. In other words, network response to fine-grained features are sufficiently good. The same behavior can be observed on the normal examples.
\end{prop}
\begin{remark}
The network has already learnt the features properly by the end of pretraining. If we are doing in-dataset transfer to the binary target problem, we can simply set $F_{+}(\cdot) = \sum_{c\in[k_+]}F_{+,c}(\cdot)$ and we would have the desired properties, i.e. with probability at least $1-o(1)$, for any ``$+$''-class normal or hard sample $\mX$, $F_+(\mX) \in \Omega(1)$ and $F_-(\mX) \in o(1)$. 

Of course, we can further finetune the network on the target problem (in both the in- and cross-dataset transfer cases) for better robustness, by noting from the proof below that the common-feature neurons remain activated throughout pretraining, and by noting that assuming sufficiently high overlap between the features which the pretraining and target label function consider discriminative (so sufficiently high label function alignment), \textit{training on the target problem does not decrease network response to the fine-grained features while boosting its response to the more abundant common features}. For the artificial architecture that we have here (recall that for each subclass we have a separate set of neurons, while hidden neurons are shared across classes in practical situations), it could be a good idea to prune the network first before doing finetuning, to remove the duplicate $S_{+,c}^{*(0)}(\vv_+)$ neurons (for instance, we know the neurons in $S_{+,c}^{*(0)}(\vv_+)$ have vanishingly small difference in their activation value on normal examples, so we can just remove these highly similar neurons, similar to activation-value-based pruning strategies in practice), this can help lessening the noise accumulated in the common-feature neurons while preserving strong response to the true features. There are a variety of options of finetuning this network, and we will not delve deeply into it to avoid making this appendix even longer and more complicated.
\end{remark}
\begin{proof}
The neuron growth equation for $(+,c,r) \in S_{+}^{*(t)}(\vv_{+,c})$ satisfies
\begin{equation}
\begin{aligned}
\vw_{+,c,r}^{(t)} 
= & \vw_{+,c,r}^{(0)} \\
& + \eta t \Bigg(  \left(1 - \frac{1}{1.5k_+}, \left( 1 - \frac{1}{2k_+} \right) \left(1 + O(d^{-1}) \right) \right) \times \sqrt{1 \pm \iota}\left(1 \pm s^{*-1/2}\right) \pm O\left(\frac{1}{\ln^{10}(d)}\right)\Bigg)  \frac{s^*}{2 k_+ P} \vv_{+,c}  + \eta t\vzeta^{(t)}_{+,c,r}
\end{aligned}
\end{equation}
where $\vzeta^{(t)}_{+,c,r} \sim \calN(\vzero, \sigma_{\zeta}^{(t)2} \mI)$, and $\sigma_{\zeta}^{(t)} \le \sigma_{\zeta} \left(  \left(\frac{1}{2} \pm \psi_1^{(T)}\right) \sqrt{1 \pm \iota}\left(1 \pm s^{*-1/2}\right) \right) \frac{s^*}{2k_+P}$.

The derivation is virtually the same as the one in the proof of Theorem \ref{prop: phase 1 sgd induction}, so we skip it here to avoid repetitive calculations. The only difference is that the loss term $[1 - \logit_+^{(t)}](\mX_n^{(t)}) \in \frac{1}{2} \pm \psi_1^{(t)}$ in that proof is replaced by $[1 - \logit_{+,c}^{(t)}](\mX_n^{(t)}) \in \left(1 - \frac{1}{1.5k_+}, \left( 1 - \frac{1}{2k_+} \right) \left(1 + O(d^{-1}) \right) \right)$ based on the definition of $T_0$ and Lemma \ref{lemma: finegrained init loss}. 

Moreover, just like in Theorem \ref{prop: phase 1 sgd induction}, for every $ \vv_{+,c}$, every neuron $(+,c,r) \in S_{+,c}^{*(0)}(\vv_{+,c})$ remain activated on any patch $p \in \calP(\mX_n^{(t)}; \vv_{+,c})$ and no other patch throughout phase I, therefore, each one of them receive exactly the same update at every iteration $t$. Showing the same ``singleton-activation'' properties for neurons in $S_{+,c}^{*(0)}(\vv_{+})$ requires somewhat different calculations, since $\vv_+$-dominated patches can show up in the samples of any $(+,c)$ subclass. We sketch the inductive argument here. The base case is almost identical to the one in Theorem \ref{prop: phase 1 sgd induction}. As for the inductive step, assuming the neurons in $ S_{+,c}^{*(0)}(\vv_{+})$ all activate on the $\vv_+$-dominated patches at step $t$, we note that for any $(+,c,r) \in S_{+,c}^{*(0)}(\vv_{+})$, its update estimate is as follows:
\begin{equation}
\begin{aligned}
    \vw_{+, c,r}^{(t+1)} 
    = & \vw_{+, c,r}^{(t)} + \eta \frac{1}{NP} \sum_{n=1}^N \Bigg( 
    \mathbbm{1}\{y_n = (+,c)\}[1-\text{logit}_{+,c}^{(t)}(\mX_n^{(t)})]\sum_{p\in[P]}\sigma'(\langle \vw_{c,r}^{(t)}, \vx_{n,p}^{(t)} \rangle +b_{c,r}^{(t)}) \vx_{n,p}^{(t)} \\
    & + \mathbbm{1}\{y_n\neq (+, c)\} [-\text{logit}_{+,c}^{(t)}(\mX_n^{(t)}) ]\sum_{p\in[P]} \sigma'(\langle \vw_{c,r}^{(t)}, \vx_{n,p}^{(t)}  \rangle + b^{(t)}_{c,r}) \vx_{n,p}^{(t)}\Bigg) \\
    = & \vw_{+, c,r}^{(t)} + \eta \frac{1}{NP} \Bigg\{ \left(1 - \frac{1}{1.5k_+}, \left( 1 - \frac{1}{2k_+} \right) \left(1 \pm O(d^{-1}) \right) \right) \left[\sqrt{1\pm\iota}(1\pm s^{*-1/2}) \pm O\left(\frac{1}{\ln^{10}(d)}\right)\right] \frac{s^* N}{2k_+}\vv_{+} \\
    & - \left(0, \frac{k_+ -1 }{1.5k_+}\left(1 \pm O(d^{-1}) \right) \right)\left[\sqrt{1\pm\iota}(1\pm s^{*-1/2}) \pm O\left(\frac{1}{\ln^{10}(d)}\right)\right] \frac{s^* N}{2k_+}\vv_{+} \Bigg\} + \eta \vzeta^{(t)}_{+,c,r}
\end{aligned}
\end{equation}
where $\vzeta^{(t)}_{+,c,r} \sim \calN(\vzero, \sigma_{\zeta}^{(t)2} \mI)$,  $\sigma_{\zeta}^{(t)} \le \sigma_{\zeta} \left(  \left(1 - \frac{1}{2k_+}\right) \sqrt{1 \pm \iota}\left(1 \pm s^{*-1/2}\right)\right) \frac{s^*}{2P}$. Within polynomial time, the following estimate holds true with probability at least $1 - O\left(\frac{mdNPT_0}{\poly(d)}\right)$:
\begin{equation}
\begin{aligned}
    \langle \Delta \vw_{+, c,r}^{(t)}, \vv_+ \rangle
    \ge &  \eta \frac{s^*}{2k_+P} \times\frac{1}{3} \left(1 - O(d^{-1}) \right) \left[\sqrt{1-\iota}(1- s^{*-1/2}) - O\left(\frac{1}{\ln^{10}(d)}\right)\right] - \eta O\left(\frac{s^*}{k_+ P \ln^{9}(d)}\right)
\end{aligned}
\end{equation}
and for any $\vzeta_{n,p}^{(t+1)}$,
\begin{equation}
\begin{aligned}
    \left\vert \langle \Delta \vw_{+, c,r}^{(t)}, \vzeta_{n,p}^{(t+1)} \rangle \right\vert
    \le &  \eta \frac{s^*}{2k_+P} \times\left(1 + O(d^{-1}) \right) \left[\sqrt{1+\iota}(1 + s^{*-1/2}) + O\left(\frac{1}{\ln^{10}(d)}\right)\right]\times O\left( \frac{1}{\ln^{10}(d)}\right) \\
    & + \eta O\left(\frac{s^*}{k_+ P \ln^{9}(d)}\right)
\end{aligned}
\end{equation}
Therefore at time $t+1$,
\begin{equation}
\begin{aligned}
    & \langle \vw_{+, c,r}^{(t+1)}, \sqrt{1\pm\iota}\vv_+ + \vzeta_{n,p}^{(t+1)} \rangle + b_{+,r}^{(t+1)} \\
    \ge & 0 + \eta \frac{s^*}{2k_+P} \times\frac{1}{3} \left(1 - O(d^{-1}) \right) \left[(1-\iota)(1- s^{*-1/2}) - O\left(\frac{1}{\ln^{10}(d)}\right)\right] - O\left(\frac{s^*}{k_+ P \ln^{9}(d)}\right) \\
    & - \eta \frac{s^*}{2k_+P} \times\frac{1}{2} \left(1 + O(d^{-1}) \right) \left[\sqrt{1+\iota}(1 + s^{*-1/2}) + O\left(\frac{1}{\ln^{10}(d)}\right)\right]\times O\left( \frac{1}{\ln^{10}(d)}\right) - O\left(\frac{s^*}{k_+ P \ln^{9}(d)}\right) \\
    & - 0.1\eta \times \Bigg\{ \eta \frac{s^*}{2k_+P} \times \left(1 + O(d^{-1}) \right) \left[(1+\iota)(1 + s^{*-1/2}) + O\left(\frac{1}{\ln^{10}(d)}\right)\right] + O\left(\frac{s^*}{k_+ P \ln^{9}(d)}\right) \\
    & + \eta \frac{s^*}{2k_+P} \times \left(1 + O(d^{-1}) \right) \left[\sqrt{1+\iota}(1 + s^{*-1/2}) + O\left(\frac{1}{\ln^{10}(d)}\right)\right]\times O\left( \frac{1}{\ln^{10}(d)}\right) + O\left(\frac{s^*}{k_+ P \ln^{9}(d)}\right) \Bigg\} \\
    \ge & 0.1 \eta \frac{s^*}{2k_+P} \times \left(1 - O(d^{-1}) \right) \left[(1-\iota)(1- s^{*-1/2}) - O\left(\frac{1}{\ln^{10}(d)}\right)\right] \\
    > & 0
\end{aligned}
\end{equation}
which means all the neurons in $S_{+,c}^{*(0)}(\vv_+)$ remain activated at step $t+1$. Off-diagonal nonactivation can be shown similarly: for any $\vv$-dominated patch (or feature noise patch) with $\vv\perp\vv_+$,

\begin{equation}
\begin{aligned}
    & \langle \vw_{+, c,r}^{(t+1)}, \sqrt{1\pm\iota}\vv + \vzeta_{n,p}^{(t+1)} \rangle + b_{+,r}^{(t+1)} \\
    < & 0 + O\left(\frac{s^*}{k_+ P \ln^{9}(d)}\right) \\
    & + \eta \frac{s^*}{2k_+P} \times\frac{1}{2} \left(1 + O(d^{-1}) \right) \left[\sqrt{1+\iota}(1 + s^{*-1/2}) + O\left(\frac{1}{\ln^{10}(d)}\right)\right]\times O\left( \frac{1}{\ln^{10}(d)}\right) + O\left(\frac{s^*}{2k_+ P \ln^{9}(d)}\right) \\
    & - \Omega\left(\frac{s^*}{k_+P \ln^5(d)}\right)\\ 
    < & 0
\end{aligned}
\end{equation}



Finally, to show $\Omega(1)$ network response on hard examples, we just need to note that achieving loss $1 - \logit_{+,c}^{(t)}(\mX_n^{(t)}) \le 1-  \frac{1}{1.5k_+}$ during training requires the on-diagonal activation $F_{+,c}^{(t)}(\mX_n^{(t)}) \ge  \ln(\Omega( k_+ )) \gg \Omega(1)$, and since the neurons in $S_{+,c}^{*(0)}(\vv_{+,c})$ contribute at least $\Omega(1)$ portion of the overall network response, the result follows.

Furthermore, by the above results we know that by the end of training, with probability $1-o(1)$, even if the example $\mX$ only contains common-feature patches, the network response still satisfies
\begin{equation}
\begin{aligned}
    F_{+,c}^{(T_0^*)}(\mX) 
    \ge &  0.1 \eta T_0^* \frac{s^*}{2k_+P} \times\frac{1}{2} \left(1 - O(d^{-1}) \right) \left[(1-\iota)(1- s^{*-1/2}) - O\left(\frac{1}{\ln^{10}(d)}\right)\right] - O(\eta s^*/k_+P) \\
    \ge & \Omega(1)
\end{aligned}
\end{equation}

\end{proof}

\begin{lemma}
\label{lemma: finegrained init loss}
At initialization, $1 - \logit_{+,c}^{(0)}(\mX_n^{(0)}) \in \left( 1 - \frac{1}{2k_+} \right) \left(1 \pm O(d^{-1}) \right) $ on any $(+,c)$-subclass sample.
\end{lemma}
\begin{proof}
The following upper bounds hold:
\begin{equation}
\begin{aligned}
    1 - \logit^{(t)}_{+,c}(\mX_n^{(t)}) 
    = & \frac{\sum_{c'\neq c}\exp(F_{+,c'}^{(t)}(\mX_n^{(t)})) + \sum_{c''\in[k_-]}\exp(F_{-,c''}^{(t)}(\mX_n^{(t)})) }{\sum_{c'\in[k_+]}\exp(F_{+,c'}^{(t)}(\mX_n^{(t)})) + \sum_{c''\in[k_-]}\exp(F_{-,c''}^{(t)}(\mX_n^{(t)}))} \\ 
    < & \frac{(2k_+ - 1) \exp(O(d^{-1}))}{2k_+} \\
    \le & \left( 1 - \frac{1}{2k_+} \right) \left(1 + O(d^{-1}) \right)
\end{aligned}
\end{equation}
The lower bounds are computed similarly:
\begin{equation}
\begin{aligned}
    1 - \logit^{(t)}_{+,c}(\mX_n^{(t)}) 
    = & \frac{\sum_{c'\neq c}\exp(F_{+,c'}^{(t)}(\mX_n^{(t)})) + \sum_{c''\in[k_-]}\exp(F_{-,c''}^{(t)}(\mX_n^{(t)})) }{\sum_{c'\in[k_+]}\exp(F_{+,c'}^{(t)}(\mX_n^{(t)})) + \sum_{c''\in[k_-]}\exp(F_{-,c''}^{(t)}(\mX_n^{(t)}))} \\ 
    > & \frac{(2k_+ - 1)}{2k_+ \exp(O(d^{-1}))} \\
    \ge & \left( 1 - \frac{1}{2k_+} \right) \left(1 - O(d^{-1}) \right)
\end{aligned}
\end{equation}

\end{proof}




% ----------------------------------------------------------------------------------
\newpage
\section{Probability Lemmas}
\label{section: appendix, prob lemmas}
\begin{lemma}[Laurent-Massart $\chi^2$ Concentration]
\label{lemma: laurent-massart}
Let $\vg \sim \calN(\vzero, \mI_d)$. For any vector $\va \in \mathbb{R}^{d}_{\ge 0}$, any $t>0$, the following concentration inequality holds:
\begin{align}
    \mathbb{P}\left[ \sum_{i=1}^d a_i g_i^2 \ge \|\va\|_1 + 2\|\va\|_2 \sqrt{t} + 2 \|\va\|_{\infty} t \right] \le e^{-t} 
\end{align}
\end{lemma}

\begin{lemma}
\label{lemma: gaussian vector norm concentration}
Let $\vg \sim \calN(\vzero, \sigma^2 \mI_d)$. Then,
\begin{equation}
    \mathbb{P}\left[ \|\vg\|_2^2 \ge 5 \sigma^2 d \right] \le e^{-d}
\end{equation}
\end{lemma}
\begin{proof}
By Lemma \ref{lemma: laurent-massart}, setting $a_i = 1$ for all $i$ and $t = d$ yields
\begin{equation}
    \mathbb{P}\left[ \|\vg\|_2^2 \ge \sigma^2 d + 2 \sigma^2 d + 2 \sigma^2 d \right] \le e^{-d}
\end{equation}
\end{proof}

\begin{lemma} [\cite{pmlr-v162-shen22a}]
\label{lemma: independent gaussian vector inner product concentration}
Let $\vg_1 \sim \calN(\vzero, \sigma_1^2 \mI_d)$ and  $\vg_2 \sim \calN(\vzero, \sigma_2^2 \mI_d)$ be independent. Then, for any $\delta \in (0,1)$ and sufficiently large $d$,
\begin{align}
    \mathbb{P}\left[ \left\vert \langle \vg_1, \vg_2 \rangle \right\vert \le O\left(\sigma_1\sigma_2 \sqrt{d\ln(1/\delta)} \right) \right] \ge 1 - \delta
\end{align}
\end{lemma}


\end{document}