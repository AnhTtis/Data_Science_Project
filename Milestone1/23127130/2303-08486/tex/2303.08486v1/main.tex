\documentclass[%
  reprint,
  %superscriptaddress,
  %twocolumn,
  onecolumn,
  amsmath,
  amssymb,
  10pt,
  aps,
  prx,
  citeautoscript,
  notitlepage,
  longbibliography
]{revtex4-2}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{xcolor}
\definecolor{blue}{RGB}{50,50,220}
\usepackage[%
  colorlinks=true,
  allcolors=blue
]{hyperref}
\usepackage{url}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{lmodern}
\usepackage{parskip}
\usepackage[left=3cm,right=3cm]{geometry}

\renewcommand{\baselinestretch}{1.2}

\DeclareMathOperator*{\argmin}{arg\,min}

\DeclarePairedDelimiter\ppar{(}{)}              % ( )
\DeclarePairedDelimiter\pang{\langle}{\rangle}  % < >
\DeclarePairedDelimiter\pabs{\lvert}{\rvert}    % | |
\DeclarePairedDelimiter\pnrm{\lVert}{\rVert}    % || ||
\DeclarePairedDelimiter\pbkt{[}{]}              % [ ]
\DeclarePairedDelimiter\pset{\{}{\}}            % { }

\newcommand{\rtab}[1]{Tab.~(\ref{#1})}
\newcommand{\rfig}[1]{Fig.~(\ref{#1})}
\newcommand{\rsct}[1]{Sec.~(\ref{#1})}
\newcommand{\rref}[1]{Ref.~(\citenum{#1})}
\newcommand{\req}[1]{Eq.~(\ref{#1})}

\newcommand{\dd}[1]{\operatorname{d#1}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\dR}{\dd{\mathbf{R}}}
\newcommand{\dz}{\dd{\mathbf{z}}}
\newcommand{\dx}{\dd{\mathbf{x}}}
\newcommand{\dy}{\dd{\mathbf{y}}}
\newcommand{\btheta}{\pmb{\theta}}
\newcommand{\e}{\operatorname{e}}
\newcommand{\kT}{k_{\mathrm{B}}T}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\kb}{k_{\mathrm{B}}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\rhoeq}{\rho_{\mathrm{eq}}}

\newcommand{\note}[1]{{\color{red}{(#1)}}}
\newcommand{\ov}[1]{{\color{orange}{[OV: #1]}}}
\newcommand{\jr}[1]{{\color{magenta}{[jr: #1]}}}
\newcommand{\chg}[1]{{\color{red}{#1}}}
\usepackage[normalem]{ulem}

\begin{document}
\title{%
  Manifold Learning in Atomistic Simulations:\\
  A Conceptual Review
}

% Authors, affiliations, addresses.
\author{Jakub Rydzewski}
\email[]{jr@fizyka.umk.pl}
\affiliation{%
  Institute of Physics,
  Faculty of Physics, Astronomy and Informatics,
  Nicolaus Copernicus University,
  Grudziadzka 5, 87-100 Toru\'n, Poland
}

\author{Ming Chen}
%\email{chen4116@purdue.edu}
\affiliation{%
  Department of Chemistry,
  Purdue University,
  West Lafayette, Indiana 47907-2048, USA
}

\author{Omar Valsson}
%\email{omar.valsson@unt.edu}
\affiliation{%
  Department of Chemistry,
  University of North Texas,
  Denton, Texas 76201, USA
}

%\date{\today}

\begin{abstract}
%
Analyzing large volumes of high-dimensional data requires dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. Such practice is needed in atomistic simulations of dynamical systems where even thousands of degrees of freedom are sampled. An abundance of such data makes it strenuous to gain insight into a specific physical problem. Our primary aim in this review is to focus on unsupervised machine learning methods that can be used on simulation data to find a low-dimensional manifold providing a collective and informative characterization of the studied process. Such manifolds can be used for sampling long-timescale processes and free-energy estimation. We describe methods that can work on data sets from standard and enhanced sampling atomistic simulations. Compared to recent reviews on manifold learning for atomistic simulations, we consider only methods that construct low-dimensional manifolds based on Markov transition probabilities between high-dimensional samples. We discuss these techniques from a conceptual point of view, including their underlying theoretical frameworks and possible limitations.
%
\newpage
\tableofcontents
%
\end{abstract}

\maketitle

\newpage

\section{Introduction}
\label{sec:introduction}
%
Atomistic simulations are extensively used to investigate complex dynamical systems, such as proteins and other biomolecules, in chemistry and biology~\cite{10.1146/annurev-biophys-042910-155245,10.1126/science.aaz3041}, where they can be employed to gain knowledge about physical and chemical processes at the atomistic level of detail with spatiotemporal resolution inaccessible to experiments. Such biomolecular systems can easily involve ten to a hundred thousand atoms or even more; thus, the configuration space corresponding to the degrees of freedom of the system is high-dimensional. The analysis of high-dimensional configuration space is non-trivial. It requires techniques for averaging over noisy variables that correspond to fast degrees of freedom while obtaining a low-dimensional description that retains the essential characteristics of the associated physical processes. A low-dimensional representation should be physically explainable and interpretable. Paraphrasing R. Coifman:~\cite{coiman2018icm}
%
\begin{quote}
  There is innate truth in the low-dimensional manifold of the data, and we would like to have a characterization of some latent variable that intrinsically describes the changes of states,
\end{quote}
%
we can intuitively understand the motivation to develop methods for finding low-dimensional representations in atomistic simulations.

To this end, complex dynamical systems are studied in various contexts using mathematical frameworks that can alleviate the apparent problem of high dimensionality and provide a reduced representation~\cite{chandler1987introduction,coifman2005geometric,mezic2005spectral,valsson2016enhancing,wu2017variational,klus2018data,glielmo2021unsupervised,lin2021data,morishita2021time}. Classical examples are the Ginzburg--Landau theory of phase transitions~\cite{hohenberg2015introduction}, the Mori--Zwanzig formalism for transport and collective motion~\cite{zwanzig1961memory,luttinger1964theory,mori1965transport}, and Koopman's theory~\cite{wu2020variational,brunton2021modern}. To more recently developed approaches that can be used to reduce the dimensionality of dynamical systems, we can include manifold learning~\cite{borg2005modern,lee2007nonlinear,van2009dimensionality,abdi2010principal,ma2012manifold,izenman2012introduction}, a class of unsupervised machine learning methods trained directly on collected data, whose development was instigated by the innovative works of Tenenbaum et al.~\cite{tenenbaum2000global} and Roweis and Saul~\cite{roweis2000nonlinear}; both published in the same issue of Science [\textbf{290} (2000)].

Complex dynamical systems require a strict approach that ensures their informative physical characteristics are encoded in the corresponding low-dimensional manifolds. In the context of atomistic simulations, encoding essential characteristics of the physical process while averaging over remaining degrees of freedom should be performed according to several requirements~\cite{valsson2016enhancing,noe2017collective,pietrucci_strategies_2017,bussi2007accurate}:
%
\begin{enumerate}[leftmargin=0.5cm]
  \item Distinguishing between relevant states of the system.
  \item Including slowly varying degrees of freedom that correspond to the behavior of the system occuring on longer timescales.
\end{enumerate}
%

Another difficulty in finding low-dimensional manifolds from atomistic simulations results from the sampling problem. Many complex systems are characterized by metastable states separated by energy barriers much higher than thermal energy $\kT$. This metastability leads to the kinetic entrapment in a single state that makes transitions between metastable states infrequent (i.e., rare). As a result, metastable systems are sampled only in a fraction of their configuration space due to the low probability of jumping across energy barriers, and the data corresponding to this simulation cannot represent the whole configurational behavior of the system. A possible solution to improve the sampling is to employ enhanced sampling methods that bias the equilibrium probability to improve the sampling of the configuration space~\cite{valsson2016enhancing,bussi2020using,henin2022enhanced}. Unfortunately, the problem is often omitted in the context of manifold learning. Most such methods remain unable to learn from data generated by biased trajectories.

A primary goal of this review is to provide a theoretical framework for manifold learning methods appropriate for studying dynamical systems via atomistic simulations, including enhanced sampling simulations. However, we deviate from a commonly taken approach to reviewing unsupervised learning methods for finding low-dimensional representations of complex systems. We do not discuss standard methods, e.g., principal component analysis, multidimensional scaling, and their variants. Instead, we focus entirely on a class of nonlinear techniques that work by constructing Markov transition probabilities between high-dimensional samples, mainly emphasizing their use for dynamical systems simulated by unbiased or biased sampling methods. We review manifold learning methods under a single unifying framework, incorporating the latest developments to allow for easy comparison. Generally, each algorithm we review here involves the following steps:
%
\begin{enumerate}[leftmargin=0.5cm]
  \item Generation of high-dimensional samples from unbiased or biased atomistic simulations.
  \item Construction of a Markov chain on the data with pairwise transition probabilities between samples.
  \item Parametrization of a manifold using a target mapping that embeds high-dimensional samples to a reduced space through eigendecomposition (i.e., spectral embedding) or divergence optimization.
\end{enumerate}
%
For an introduction to the methods omitted here, we refer interested readers to many reviews focusing on learning from machine learning data sets~\cite{borg2005modern,lee2007nonlinear,van2009dimensionality,ma2012manifold,xie2020representation} or directly from simulation data~\cite{noe2017collective,sittel2018perspective,wang2020machine,noe2020machine,gkeka2020machine,glielmo2021unsupervised,chen2021collective}.

This review begins with relatively standard material about atomistic simulations and a general introduction to enhanced sampling techniques [\rsct{sec:atomistic-simulations}]. We cover only the concepts required to understand manifold learning in the context of standard atomistic and enhanced sampling simulations. This part, by no means exhaustive, can be supplemented by several comprehensive reviews on enhanced sampling~\cite{valsson2016enhancing,pietrucci_strategies_2017,yang2019enhanced,bussi2020using,kamenik2021enhanced,henin2022enhanced}. Next, a general problem of finding low-dimensional manifolds for the description of dynamical systems is introduced [\rsct{sec:manifold-learning}]. Subsequently, we move to the central part of this review and focus on several manifold learning techniques that can be used for learning from atomistic simulations. Each of these frameworks is introduced from the conceptual perspective, followed by applications and software implementations [\rsct{sec:tm-eigendecomposition}--\rsct{sec:tm-divergence-optimization}]. Finally, we summarize ongoing issues and provide our perspective on manifold learning in standard atomistic and enhanced sampling simulations [\rsct{sec:conclusions}].

\section{Atomistic Simulations}
\label{sec:atomistic-simulations}
%
Atomistic simulation methods such as molecular dynamics or Monte Carlo have emerged as general methods at the intersection of theoretical and computational physics~\cite{battimelli2020computer}.  In this review, we consider atomistic simulation approaches as \emph{samplers} that yield high-dimensional data sets from some underlying probability distributions.

\subsection{Statistical Physics Representation}
\label{sec:high-dimensional-space}
%
A statistical physics representation for complex many-body systems allows us to obtain macroscopic properties by considering an ensemble of microscopic configurations (i.e., states) and averaging over their characteristics~\cite{chandler1987introduction}. Such a representation generaly entails a high number of degrees of freedom $n$, for example, $n=3N$ for an $N$-atom system. Let us suppose that an $n$-dimensional vector fully characterizes a microscopic state of such a system:
%
\begin{equation}
  \label{eq:configuration-variable}
  \bx \equiv \pset{x_k}_{k=1}^n = \ppar*{x_1, x_2, \dots, x_n}^\top,
\end{equation}
%
where $\bx$ denotes any \emph{configuration variables} of the system, such as the \emph{microscopic coordinates} or functions of the microscopic coordinates, under the assumption that the space spanned by these variables is high-dimensional. In machine learning, such configurational variables are called \emph{features} or \emph{descriptors}~\cite{nuske2014variational,rydzewski2021multiscale,bonati2021deep}.

\req{eq:configuration-variable} denotes a microscopic state of the system. Consequently, a \emph{trajectory} of the system contains many such microscopic states and is given by:
%
\begin{equation}
  \label{eq:trajectory}
  X \equiv \pset{\bx_k}_{k=1}^K =
  \ppar*{\bx_1, \bx_2, \dots, \bx_K}^\top
\end{equation}
%
of $K$ high-dimensional samples recorded at consecutive times during the system dynamics. This trajectory is a matrix of size $n \times K$, where each sample $\bx_k$ is given by \req{eq:configuration-variable}, where we add an additional index to indicate the number of a sample in the trajectory.

Depending on the external conditions of the system, its dynamics can be described in various statistical ensembles. Without loss of generality, we limit our discussion to the canonical ensemble ($NVT$), in which the configuration variables evolve according to some underlying high-dimensional potential energy function $U(\bx)$ at the given temperature $T$.

If the microscopic coordinates characterize the state of the system, the equilibrium density of the system is given by the stationary Boltzmann distribution~\cite{chandler1987introduction}:
%
\begin{equation}
  \label{eq:boltzmann-density}
  \rho(\bx) = \frac{1}{\mathcal{Z}} \e^{-\beta U(\bx)},
\end{equation}
%
where $\beta=(\kT)^{-1}$ is the inverse of the thermal energy $\kT$ corresponding to the temperature $T$ with the Boltzmann constant denoted by $k_\mathrm{B}$, and $\mathcal{Z}=\int\dx\e^{-\beta U(\bx)}$ is the canonical partition function. 

When the system is defined by configuration variables other than the microscopic coordinates, the set of samples $X$ [\req{eq:trajectory}] is generally sampled according to some unknown high-dimensional equilibrium density, in contrast to \req{eq:boltzmann-density}. In this review, we explicitly mention which configuration variables we consider.

The configuration variables of the system can be sampled using atomistic simulations, e.g., molecular dynamics or Monte Carlo. Given the high-dimensional representation of the system, many variables are not informative for the studied physical process~\cite{hashemian2013modeling} as they are fast degrees of freedom slaved to some slow variables. This is especially pronounced in dynamical systems having multiple timescales. In this case, we can average over these fast degrees of freedom without losing dynamical information about the slow physical process. Furthermore, some of the configuration variables may be simply spurious or degenerate.

%
\begin{table*}
  \caption{Representations of a dynamical system considered in this review. Note that for simplicity the same symbol $\bx$ is used for both the microscopic coordinates and configuration variables (features). See \rsct{sec:high-dimensional-space} for an explanation.}
  \begin{tabular}{p{3.8cm}p{0.6cm}p{4cm}p{2.2cm}p{2.5cm}c}
    \hline
    Variables &  & Probability distribution & Type & Dimensionality & \\
    \hline\hline
    Microscopic coordinates & $\bx$ & $\rho(\bx) \propto \e^{-\beta U(\bx)}$ & Equilibrium & High & $n$ \\
    Configuration variables & $\bx$ & $\rho(\bx)$ unknown & Equilibrium & High & $n$ \\
    Collective variables & $\bz$ & $\rho(\bz) \propto \e^{-\beta F(\bz)}$ & Equilibrium & Low & $d$ \\
    Collective variables & $\bz$ & $\rho_V(\bz) \propto \e^{-\beta (F(\bz)+V(\bz))}$ & Biased & Low & $d$ \\
    \hline
  \end{tabular}
\end{table*}
%

\subsection{Collective Variables and Target Mapping}
\label{sec:collective-variables}
%
To circumvent the problem of high-dimensional representations, we often seek a reduced description of the system with fewer degrees of freedom. Such a low-dimensional space must retain the most \emph{informative} characteristics of the underlying process. In atomistic simulations, such ``optimal'' variables comprising the reduced space are often called \emph{collective variables} (CVs), order parameters, or reaction coordinates.

What variables can be considered informative? Although they may differ depending on the application, there are several requirements such variables should fulfill~\cite{abrams2014enhanced,valsson2016enhancing,pietrucci_strategies_2017,noe2017collective,sittel2018perspective,bussi2020using,neha2022collective}:
%
\begin{enumerate}[leftmargin=0.5cm]
  \item Encode essential characteristics of the physical process while minimizing the number of variables required to capture the process.
  \item Distinguish between relevant states (i.e., modes) of the system.
  \item Include slowly varying degrees of freedom corresponding to long timescale processes.
\end{enumerate}
%
We can see that the CVs and low-dimensional manifolds have equivalent requirements [\rsct{sec:introduction}]. Under this view, we can consider CVs as a low-dimensional set of the configuration variables evolving on a manifold with special characteristics.

Finding appropriate CVs is non-trivial, mainly because the selection depends on the studied process. Typically, CVs are selected based on physical or chemical intuition and trial-and-error; see \rfig{fig:metastable-potential} for an illustration. This problem motivated many theoretical advances to find interpretable and low-dimensional representations of dynamical systems automatically. For instance, recent approaches include using neural networks to construct or find CVs~\cite{zhang2018unfolding,wehmeyer2018time,bonati2020data,rydzewski2021multiscale,bonati2021deep,belkacemi2021chasing,rydzewski2022reweighted}.

To introduce the theory of CVs, let us assume now that they are correctly constructed using some procedure. (Later, we will show how CVs can be estimated using manifold learning.) CVs are usually given as functions of the configuration variables [\req{eq:configuration-variable}]. Therefore, the problem of finding CVs is equivalent to obtaining a set of functions of the configuration variables, which we call the \emph{target mapping} that embeds the high-dimensional samples into the low-dimensional CV space~\cite{rydzewski2022reweighted}:
%
\begin{equation}
  \label{eq:collective-variables}
  \boxed{%
    \bx \mapsto \xi(\bx) \equiv \pset[\big]{ \xi_k(\bx) }_{k=1}^d}
\end{equation}
%
for $d \ll n$. The target mapping $\xi(\bx)$ can be linear, nonlinear, or even an identity function (i.e., this reduces the problem to the selection). \req{eq:collective-variables} is central to our review: each manifold learning method provides a unique functional form of the target mapping used to reduce the dimensionality of the system representation.

The CV space defined by the target mapping given in \req{eq:collective-variables} is a fragment of the full configuration space. Thus we need to define a probability density for the CVs. The equilibrium distribution of CVs is given by a marginalized density obtained by integrating (i.e., averaging) over variables corresponding to fast degrees of freedom:
%
\begin{align}
  \label{eq:collective-variables-density}
  \rho(\bz) = \int\dx \delta\ppar*{\bz - \xi(\bx)} \rho(\bx)
         = \pang[\Big]{\delta\ppar*{\bz - \xi(\bx)}},
\end{align}
%
where the multidimensional Dirac delta function is:
%
\begin{equation}
  \delta\ppar{\bz-\xi(\bx)}=\prod_{k=1}^d \delta\ppar{z_k-\xi_k(\bx)},
\end{equation}
%
and $\pang*{\cdot}$ denotes an unbiased ensemble average.

The equilibrium distribution of CVs [\req{eq:collective-variables-density}] typically contains many disconnected regions of high probability (minima of energy) separated by regions of low probability, which correspond to energy barriers higher than the thermal energy $\kT$. As such, transitions between these regions are rare.

\subsection{Free-Energy Landscape and Metastable States}
\label{sec:free-energy}
%
Instead of the potential energy function $U(\bx)$ characteristic for a high-dimensional representation [\req{eq:boltzmann-density}], the reduced dynamics of the system in the CV space proceeds according to the underlying \emph{free-energy landscape} that is the negative logarithm of the marginal distribution of CVs multiplied by the thermal energy:
%
\begin{equation}
  \label{eq:free-energy}
  \boxed{%
    F(\bz) = -\frac{1}{\beta}\log \rho(\bz)}
\end{equation}
%
which is defined up to an immaterial constant. As such, the equilibrium density of CVs can be equivalently written as $\rho(\bz) = \e^{-\beta F(\bz)} / \mathcal{Z}$, where the partition function in the CV space is given as $\mathcal{Z}=\int\dz\e^{-\beta F(\bz)}$.

The free-energy landscape determines effective energy landscape for the CVs consisting of many metastable states and free-energy barriers between the metastable states. The free-energy difference (i.e., the relative population) $\Delta F_{AB}$ between states $A$ and $B$ can be calculated as~\cite{henin2022enhanced}:
%
\begin{equation}
  \label{eq:free-energy-difference}
  \Delta F_{AB} = -\frac{1}{\beta} \log{ \frac{\mathcal{Z}_{A}}{\mathcal{Z}_{B}}} = -\frac{1}{\beta} \log{\frac{\int_A\dz\, \e^{-\beta F(\bz)}}{\int_B\dz\, \e^{-\beta F(\bz)}}},
\end{equation}
%
which is the ratio between the partition functions corresponding to the states $A$ and $B$, $\mathcal{Z}_A$ and $\mathcal{Z}_B$, respectively. The free-energy difference $\Delta F_{AB}$ is sometimes referred to as the Helmholtz reaction free energy~\cite{dietschreit2022obtain}. Note that \req{eq:free-energy-difference} is valid if (and only if) the CV set properly seperates the two states $A$ and $B$. Furthermore, the free-energy difference should not be obtained by taking the difference of the values of $F(\bz)$ at the minima corresponding to the two states~\cite{dietschreit2022obtain}. 

When comparing free-energy landscapes resulting from different CVs, one should remember that the free-energy surfaces are not invariant with respect to CVs~\cite{bal2020free,dietschreit2022obtain,dietschreit2022free}. Therefore, one needs to be careful when relating free-energy barriers to the kinetics of crossing betwween states~\cite{bal2020free}, or trying to extract activation free energies from free-energy landscapes~\cite{dietschreit2022free}.

%
\begin{figure}
  \includegraphics{fig/fig-potential-metastable}
  \caption{{\bf Metastability}.
  (a) Model double-well free-energy landscape $F(z_k,z_l)$ with a free-energy barrier higher than the thermal energy $\kT$, illustrating the concept of metastability. Gray lines show cross-sections through the double-well potential.
  (b) Free energy along the $z_l$ variable indicates only one minimum, which means that $z_l$ is not an optimal CV.
  (c) Free energy along $z_k$ depicts two minima with a correct free-energy barrier, slightely higher than the thermal energy (see colorbar).
  (d) Free energy along $z_k-z_l$ shows two energy minima, but the free-energy barrier is lower than the correct value of the free-energy barrier.}
  \label{fig:metastable-potential}
\end{figure}
%

The equilibrium density of CVs $\rho(\bz)$ is frequently challenging to sample exhaustively, even for simple systems. We call it the \emph{sampling problem}, in which most dynamics is spent on sampling only fast equilibration within metastable regions, with transitions to other local free-energy minima being rare. This sampling problem is related to the fact that a typical energy landscape is characterized by many metastable states separated by barriers much higher than the thermal energy $\kT$. Such systems are called {\it metastable}; see \rfig{fig:metastable-potential} for a simplified illustration.

On the timescales accessible for standard atomistic simulations (around the microsecond to millisecond timescale for classical force fields), crossings over high free-energy barriers are {\it rare events} (or infrequent), and the system remains kinetically trapped in a single metastable state. In other words, for metastable states, there is a large timescale gap between slow and much faster degrees of freedom, typically corresponding to equilibration with a metastable state.

Many physical processes exhibit the sampling and metastability problems; examples include catalysis~\cite{piccini2022ab}, ligand interactions with proteins~\cite{baron2013molecular,rydzewski2017ligand,bruce2018new,bernetti2019kinetics} and DNA~\cite{o2021enhanced}, glass transitions in amorphous materials~\cite{van2021towards}, crystallization~\cite{neha2022collective}, and graphite etching~\cite{aussems2017atomistic}. 
%\ov{perhaps cite reviews of applications here also. OV to add}

\subsection{Enhanced Sampling and Biasing Probability Distributions}
\label{sec:enhanced-sampling}
%
Enhanced sampling methods can alleviate the sampling problem. In this review, we consider enhanced sampling methods that bias the equilibrium probability of CVs [\req{eq:collective-variables-density}] to improve the sampling along the reduced CV space. Over recent years many enhanced sampling algorithms have been proposed, including tempering~\cite{parallel_tempering,earl2005parallel,chen2012heating}, variational~\cite{valsson2014variational,reinhardt2020determining}, biasing~\cite{torrie1977nonphysical,mezei1987adaptive,laio2002escaping,barducci2008well,Maragakis-JPCB-2009,morishita2012free} approaches, or combinations thereof~\cite{invernizzi2020unified}. For a comprehensive review of such methods with classification, we refer to an article by Henin et al.~\cite{henin2022enhanced} for interested readers.

As a representative example of enhanced sampling techniques, we consider methods that enhance CV fluctuations with a non-physical bias potential. This idea can be traced back to a seminal work published in 1977 by Torrie and Valleau~\cite{torrie1977nonphysical}, which introduced umbrella sampling as a framework for biasing selected degrees of freedom.

CV-based enhanced sampling methods overcome high energy barriers much faster than unbiased simulations. However, the resulting simulation is typically biased. Concretely, the introduction of the bias potential $V(\bz)$ results in a deviation from the equilibrium distribution of CVs [\req{eq:collective-variables-density}]. This way, we obtain sampling according to a target biased distribution:
%
\begin{align}
  \label{eq:collective-variables-density-biased}
  \rho_V(\bz) = \frac{1}{\mathcal{Z}_V} \e^{-\beta \ppar*{F(\bz) + V(\bz)}}
           = \pang[\Big]{\delta\ppar*{\bz - \xi(\bx)}}_V,
\end{align}
%
where $\mathcal{Z}_V = \int\dz\e^{-\beta \ppar*{F(\bz) + V(\bz)}}$ is the biased partition function and $\pang*{\cdot}_V$ denotes an ensemble average calculated under the biasing potential $V(\bz)$. By design, the distribution in \req{eq:collective-variables-density} is easier to sample.

CV-based enhanced sampling methods differ in how free-energy barriers are flattened or reduced and the bias potential is constructed~\cite{torrie1977nonphysical,laio2002escaping,barducci2008well,Invernizzi2020opus,valsson2014variational}. In the aforementioned umbrella sampling, it was postulated that the biased probability distribution of CVs should be ``as wide and uniform as possible''~\cite{torrie1977nonphysical}, and henceforth sometimes called flat-histogram. Recent development in enhanced sampling shows that such a target distribution is not necessarily optimal for obtaining fast convergence~\cite{dayal2004performance,trebst2004optimizing,barducci2008well,valsson2015well,invernizzi2020unified}. Thus, many CV-based enhanced sampling methods deviate from this path.

%
\begin{figure*}
  \includegraphics{fig/fig-ala-traj}
  \caption{{\bf Enhanced Sampling and Statistical Weights}. (a) Alanine dipeptide in vacuum and its dihedral angles $\Phi$ and $\Psi$. (b) Sampling of the $\Phi$ dihedral angle of alanine dipeptide performed using a parallel tempering simulation. The timeseries shows the replica at 300 K. All the samples are of equal importance as they are sampled from the equilibrium distribution. (c) Enhanced sampling of $\Phi$ performed using well-tempered metadynamics at 300 K with a bias factor $\gamma$ of 5. Color corresponds to stastistical weights of the samples $w(\bz)$ which vary considerably with the most important samples belong to the metastable states.}
  \label{fig:md}
\end{figure*}
%

As an example of a non-uniform target distribution, let us consider the well-tempered distribution used in metadynamics~\cite{barducci2008well}. Well-tempered metadynamics uses a history-dependent bias potential updated iteratively by depositing Gaussians centered at the current location in the CV space:
%
\begin{equation}
  \label{eq:bias-potential}
  V(\bz) = \sum_k G_{\sigma}(\bz,\bz_k) \exp\ppar*{-\frac{1}{\gamma-1}\beta V(\bz_k)},
\end{equation}
%
where $G_{\sigma}(\bz,\bz_k)$ is a Gaussian kernel with a bandwidth set $\sigma$, $\bz_k$ is the center of $k$-th added Gaussian, and $\gamma$ is a bias factor that determines how much we enhance CV fluctuations. Well-tempered metadynamics convergences to the well-tempered distribution:
%
\begin{equation}
  \label{eq:wt-distribution}
  \rho_V(\bz) \propto \pbkt*{ \rho(\bz) }^{1/\gamma},
\end{equation}
%
in which we sample an effective free-energy landscape $F_{\gamma}(\bz)=F(\bz)/\gamma$ with barriers reduced by a factor of $\gamma$~\cite{barducci2008well,henin2022enhanced}.

\subsection{Reweighting Biased Probability Distributions}
\label{sec:reweighting}
%
Biased simulations diverge from the equilibrium CV distribution to a smoother biased CV distribution that is easier to sample. Consequently, the importance of each sample is given by a statistical weight needed to account for the effect of the bias potential when obtaining equilibrium properties such as free-energy landscapes, which can be with respect to the biased CVs or also any other set of CVs. This behavior contrasts with unbiased simulations where samples are equally important as they are sampled according to the equilibrium distribution.

A functional form of the weights depends on a particular method. Generally, in methods employing a quasi-stationary bias potential $V(\bz)$, the weight associated with a CV sample $\bz$ is given as:
%
\begin{equation}
  \label{eq:weight}
  \boxed{%
    w(\bz) = \e^{\beta V(\bz)}}.
\end{equation}
%
In contrast, well-tempered metadynamics uses an adaptive bias potential [\req{eq:bias-potential}], and the weights are modified by adding a time-dependent constant to the bias potential~\cite{tiwary_rewt,valsson2016enhancing}. However, regardless of a CV-based enhanced sampling, the statistical weights have an exponential form.

The standard reweighting works by employing the weights to obtain the stationary equilibrium distribution from the biased CV distribution, i.e., $\rho(\bz) \propto w(\bz) \rho_V(\bz)$. The unbiased probability distribution $\rho(\bz)$ can be computed by histogramming or kernel density estimation, where each sample $\bz$ is weighted by \req{eq:weight}. This is done routinely in advanced simulation codes, e.g., PLUMED~\cite{plumed,plumed-nest}.

\section{Dynamical Data}
\label{sec:dynamical-data}
%
Here, we describe how to prepare the dynamical data from standard atomistic and enhanced sampling simulations for a manifold learning method. Such data have different characteristics in comparison to ordinary data sets. Below, we show how to collect such dynamical data and how it is possible to reduce the learning set while preserving the data density.

\subsection{Learning Data Set}
%
A sufficiently sampled data set consisting of the high-dimensional samples of the configuration variables are required to learn a low-dimensional manifold of CVs. Obviously, if the learning data set does not capture the rare transitions between metastable states, the learned low-dimensional manifold will neither. Employing insufficiently sampled simulations can lead to non-optimal CVs that do not capture the slow degrees of freedom corresponding to the rare processes.

Such a data set can be represented by a trajectory of the dynamical system as given in \req{eq:trajectory}. Also, several such trajectories can be combined in a data set if they are sampled from the same probability distribution. When the sampling is done from an equilibrium density, no additional information is needed to parametrize a manifold.

However, suppose the sampling is performed from a biased probability distribution, such as in CV-based enhanced sampling methods. In that case, the data collected from atomistic simulations have additional statistical weights [\req{eq:weight}]. Therefore, the data set of $K$ high-dimensional samples:
%
\begin{align}
  \label{eq:biased-data}
  \boxed{%
    X = \pset[\big]{\ppar*{\bx_k \in \mathbb{R}^n, w(\bx_k)}}_{k=1}^K}
\end{align}
%
is augmented by the corresponding statistical weights. This representation can also be used for data sets generated using unbiased sampling. In this case, all the weights are equal, so we can ignore them weights and \req{eq:biased-data} reduces to a set consisting only of the samples [\req{eq:trajectory}].

However, the learning data set from enhanced sampling is biased and does not correspond to the physical system. Using these biased simulation data directly in manifold learning algorithms renders low-dimensional manifolds that are also biased (i.e., their geometry, density, and importance) and, thus, CVs that do not correspond to the underlying process. Therefore, we need to correctly consider that we use biased simulation data when learning CVs from enhanced sampling simulations.

The samples in the data set $X$ should ideally not be correlated. However, simulations usually render correlated samples on short timescales. To avoid this problem, samples from simulations are usually collected with a large enough time stride such that they are not correlated. For a detailed discussion about correlation in atomistic simulations, see~\rref{grossfield2018best}.

\subsection{Landmark Sampling}
\label{sec:landmark-sampling}
%
Apart from dimensionality reduction, an abundance of data from atomistic simulations often necessitates reducing the number of samples in data sets. This reduction proceeds by selecting a subset of the samples that retains the information about the geometry and density of the sampled configuration space present in the complete data set. This approach is called {\it landmark sampling} and has been applied in both machine learning~\cite{de2003global,de2004sparse,silva2005selecting} and atomistic simulations~\cite{ceriotti2013demonstrating,zhang2018unfolding,rydzewski2021multiscale}. This subsampling constructs a smaller data set of landmarks $\pset{\bx_k}_{k=1}^L \in X$, where the number of landmarks $L$ is preferably much lower than the number of samples $K$ in the data set [\req{eq:biased-data}] (here the indices are rearranged, i.e., $k$ is used to index landmarks, not samples from the data set).

For the data set with samples distributed according to the equilibrium density (i.e., unbiased data), an intuitive approach to sample landmarks is to select them randomly, as samples are of equal importance [\req{eq:biased-data}]. Then, the landmarks are approximately distributed according to the Boltzmann equilibrium distribution. 

Another procedure that can be used to sample landmarks from unbiased simulations is farthest point sampling (FPS)~\cite{hochbaum1985best}. FPS is based only on geometric criteria and results in a uniform selection of landmarks regardless of the underlying probability distribution. Specifically, in FPS, we obtain a new landmark by selecting a sample from the initial data set that is farthest from all the previously selected landmarks. FPS may be computationally expensive for large data sets.

For biased trajectories, samples need to be selected also considering their weights $w(\bx)$ [\req{eq:biased-data}]. In the simplest case, landmarks can be obtained by \emph{weighted random sampling}, where each sample is selected with a probability proportional to its weight~\cite{bortz1975new}. This method may cause, however, overpopulation of samples in deep free-energy minima while leaving other minima with a scarce subset of samples~\cite{ceriotti2013demonstrating,tribello2019using_a,tribello2019using_b,rydzewski2021multiscale}. This overpopulation is because samples in the metastable states lying lowest in free energy have much larger weights than those in the higher-lying metastable states, as the statistical weights depend exponentially on the bias potential.

To exploit the information about the underlying density and geometry of the configuration space, we can employ a well-tempered variant of FPS~\cite{ceriotti2013demonstrating} This procedure is a two-stage selection. First, FPS is applied to select $\sqrt{KL}$ samples, and the space of the selected samples is partitioned into Voronoi polyhedra $\pset{v_k}$. For each Voronoi polyhedra, a tempered weighted is calculated: 
%
\begin{equation}
  \label{eq:acc-weights}
  \omega_k(\bx) = \ppar*{\sum_l w(\bx_l)}^{1/\alpha},
\end{equation}
%
where $l \in v_k$ and $\alpha>1$ is a tempering parameter. In the second step, weighted random sampling is used to select a polyhedra according to the tempered weight in \req{eq:acc-weights}, and a landmark is sampled from the selected Voronoi polyhedra with unmodified weights (without tempering). This second step is repeated until the desired number of landmarks $L$ is obtained. In the limit of $\alpha\rightarrow\infty$, the polyhedra are selected uniformly and landmarks are uniformly distributed. For $\alpha\rightarrow 1$, the landmark selection should approximately be the equilibrium distribution, which is the Boltzmann distribution if the high-dimensional description is given by the microscopic coordinates. The procedure for well-tempered FPS is summarized in Algorithm~\ref{alg:wt-fps}.
%
\begin{algorithm}
  \label{alg:wt-fps}
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
  \Input{Biased data batch $X=\pset[\big]{(\bx_k, w(\bx_k))}_{k=1}^K$; tempering parameter $\alpha \in [0,1]$; number of landmarks $L$.}
  \Output{Training set of landmarks $\pset{\bx_k}_{k=1}^L \in X$.}
  \begin{enumerate}[leftmargin=0cm]
    \item Select $\sqrt{KL}$ landmarks using FPS.
    \item Perform the Voronoi tesselation $\pset{v_k}$ based on landmarks selected using FPS.
    \item Calculate the accumulated weights for each Voronoi polyhedra $\omega_k(\bx)$ for $\alpha$ [\req{eq:acc-weights}].
    \item Until the number of landmarks is $L$:
    \begin{enumerate}[leftmargin=0.6cm]
      \item Select a Voronoi polyhedra according to the accumulated weights $\omega_k$.
      \item Include a landmark sampled based on weighted random sampling into the training set.
    \end{enumerate}
  \end{enumerate}
  \caption{Well-tempered FPS.}
\end{algorithm}
%

Recently, a method called weight-tempered random sampling (WTRS) has been proposed~\cite{rydzewski2021multiscale}. WTRS is based on scaling the weights of samples only, without using FPS. In this landmark-sampling method, a high-dimensional sample is selected with a probability $\propto w^{1/\alpha}$, where, as in \req{eq:acc-weights}, $\alpha$ is a tempering parameter. Assuming landmarks are sampled from the well-tempered distribution [\req{eq:wt-distribution}], it can be shown that the limit $\alpha \rightarrow \infty$ corresponds to sampling landmarks according to the biased distribution. The limit $\alpha \rightarrow 1$ corresponds to sampling landmarks from the equilibrium distribution~\cite{rydzewski2021multiscale}.
%
\begin{figure}
  \includegraphics{fig/fig-landmark-sampling}
  \caption{{\bf Landmark Sampling with Weight-Tempered Random Sampling}. Sampling landmarks according to probabilities $\propto w^{1/\alpha}$ for different values of the tempering parameter $\alpha$ compared to the free-energy surface (FES) of alanine dipeptide in vacuum. Biased simulation data is generated from a well-tempered metadynamics simulation of alanine dipeptide in vacuum using a bias factor of $\gamma=5$ and the $\Phi$ and $\Psi$ dihedral angles as biased variables. By increasing $\alpha$, landmarks gradually diverge from the unbiased distribution. Data taken from~\rref{rydzewski2021multiscale}.}
\end{figure}
%

\section{Manifold Learning}
\label{sec:manifold-learning}
%
This section covers basic concepts behind manifold learning methods. We aim to describe manifold learning techniques from the perspective of atomistic simulations. A non-exhaustive list of manifold learning methods that can be used for dynamical systems includes locally linear embedding~\cite{roweis2000nonlinear}, Laplacian eigenmap~\cite{belkin2001laplacian,belkin2003laplacian,bengio2004learning}, diffusion map~\cite{coifman2005geometric,coifman2006diffusion,nadler2006diffusion,coifman2008diffusion}, spectral gap optimization of order parameters~\cite{tiwary2016spectral}, sketch-map~\cite{ceriotti2011simplifying,ceriotti2013demonstrating}, and stochastic neighbor embedding and its variants~\cite{hinton2002stochastic,maaten2008visualizing,maaten2009learning}.

\subsection{Markov Transition Matrix}
\label{sec:markov-matrix}
%
A primary assumption in manifold learning for dynamical systems is that the dynamics in the high-dimensional space populates a low-dimensional and smooth subspace called a manifold. This assumption is the \emph{manifold hypothesis} which states that the high-dimensional data lie on the low-dimensional manifold embedded in the high-dimensional space~\cite{ferguson2011integrating,ferguson2011nonlinear}. Under this view, the fast degrees of freedom are adiabatically slaved to the dynamics of the slow CVs due to fast equilibration within the metastable states, which leads to an adiabatic timescale separation and corresponds to a negligible error in the characterization of the system dynamics.

The core of most manifold learning methods is having a notion of similarity between the high-dimensional samples, usually through a distance metric~\cite{roweis2000nonlinear,tenenbaum2000global,belkin2001laplacian,hinton2002stochastic,belkin2003laplacian,hashemian2013modeling,coifman2005geometric,maaten2008visualizing}. The distances are then integrated into a global parameterization of the data through the construction of a discrete Markov chain (e.g., based on a kernel function; see \rref{ham2004kernel}), where the transition probabilities $p_{kl}$ depend on distances between the samples.

For example, a common starting point is the construction of the Markov chain based on a Gaussian kernel:
%
\begin{equation}
  \label{eq:gaussian-kernel}
  G_\varepsilon(\bx_k,\bx_l) = \exp\ppar*{-\frac{1}{\varepsilon} \| \bx_k - \bx_l\|^2},
\end{equation}
%
where $\|\cdot\|$ denotes Euclidean distance. Other distance metrics can also be used; however, Euclidean distance is a frequent choice. Additionally, such a Markov chain can be constructed using a kernel different than Gaussian~\cite{ham2004kernel}. However, most manifold learning methods discussed in this review use a normalized Gaussian kernel at some stage of the construction of the Markov chain.

Any kernel $G(\bx_k,\bx_l)$ used in the construction of the Markov chain must satisfy the following properties:
%
\begin{enumerate}[leftmargin=0.5cm]
  \item $G$ is symmetric: $G(\bx_k,\bx_l)=G(\bx_l,\bx_k)$.
  \item $G$ is positivity preserving: for all samples $G(\bx_k,\bx_l) \ge 0$.
  \item $G$ is positive semi-definite.
\end{enumerate}
%

The Gaussian kernel in \req{eq:gaussian-kernel} is characterized by the parameter $\varepsilon>0$ chosen depending on the data set as it induces a length scale ${\sim}\sqrt{\varepsilon}$ usually selected so that it matches the distance between neighboring samples. The parameter $\varepsilon$ is related to the standard deviation of the Gaussian distribution $\sigma=\sqrt{\varepsilon/2}$. A Markov transition $M(\bx_k,\bx_l)$ matrix of size $K\times K$ with probabilities $p_{kl}$ as entries is given by:
%
\begin{equation}
\label{eq:gaussian-markov-matrix}
  p_{kl} \sim M(\bx_k,\bx_l) = \frac{G_\varepsilon(\bx_k,\bx_l)}{\sum_n G_\varepsilon(\bx_k,\bx_n)},
\end{equation}
%
which describes the probability of transition from sample $\bx_k$ to sample $\bx_l$ in one step $t$:
%
\begin{align}
\label{eq:markov-chain}
   M(\bx_k, \bx_l) = \mathrm{Pr}\,\pset[\big]{\bx(t+1)=\bx_l\,|\,\bx(t)=\bx_k},
\end{align}
%
where, through the construction in \req{eq:gaussian-markov-matrix}, the probability of transition depends only on the current sample, i.e., it is called the \emph{Markovian} assumption. Note that although time step $t$ occurs in \req{eq:markov-chain}, time is stripped in such Markov chains if the kernel used in the construction is time-independent.

The Markov transition matrix can be used to propagate the corresponding Markov chain. For a time-homogeneous Markov chain, the $k$-step transition probability can be obtained as the $k$-th power of the transition matrix $M^k$. If the Markov chain is irreducible and aperiodic, then a unique stationary distribution $\pi(\bx)$ exists. In this case, $M^k$ converges to a rank-one matrix in which each row is the stationary distribution  $\lim_{k\rightarrow\infty} M^k = \pi(\bx)$ also defined $M\pi(\bx)=\pi(\bx)$ as the stationary distribution is unchanged by the Markov transition matrix. Consequently, the highest eigenvalue corresponding to the stationary distribution equals one.

Depending on the field from which a method originates, different nomenclature is used to describe $M(\bx_k,\bx_l)$. In unsupervised learning, many methods use the name \emph{affinity}, \emph{similarity}, or \emph{proximity} matrix~\cite{hinton2002stochastic,ham2004kernel,maaten2008visualizing,mcinnes2018umap}. Typically, these methods employ an additional step when building a Markov chain, e.g., the diagonal entries of $M$ are equal to zeros. Such a Markov chain is called \emph{non-lazy}. In contrast, methods devised for dynamical systems do not use this assumption as the diagonal entries may contain important information about the system~\cite{nadler2006diffusion}. In such Markov chains, there is a probability that the transition does not occur.

%
\begin{figure}
  \includegraphics{fig/fig-target-mapping}
  \caption{{\bf Learning Manifolds and Target Mapping}.
  Schematic representation of embedding high-dimensional samples $\pset{\bx}$ into a low-dimensional manifold. The target mapping $\xi(\bx): \mathbb{R}^n \mapsto \mathbb{R}^d$ is defined such that relations $p_{kl}$ between high-dimensional samples $\bx_k$ and $\bx_l$ is preserving relations $q_{kl}$ between low-dimensional samples $\bz_k$ and $\bz_l$ in the manifold. Learning a manifold is equivalent to learning a mapping which can be constructed using eigendecomposition [\rsct{sec:tm-eigendecomposition}] or divergence optimization [\rsct{sec:tm-divergence-optimization}].}
  \label{fig:manifold}
\end{figure}
%

\subsection{Target Mapping}
%
Here, we focus on a generalization of the target mapping from the high-dimensional configuration variable space [\req{eq:configuration-variable}] to a low-dimensional manifold (or the CV space) [\req{eq:collective-variables}]. As explained in \rsct{sec:collective-variables}, the problem of finding CVs is equivalent to finding a parametrization of the target mapping. The target mapping performs dimensionality reduction such that the dimensionality of the manifold is much lower than that of the high-dimensional space, i.e., $d \ll n$; see \rfig{fig:manifold}.

Manifold learning methods can be split into two categories depending on how the low-dimensional manifold is constructed by the target mapping:
%
\begin{enumerate}[leftmargin=0.5cm]
  \item[I.] \emph{Eigendecomposition} of the Markov transition matrix:
  %
  \begin{equation}
    \label{eq:target-mapping-eigen}
    M \psi_k = \lambda_k \psi_k,
  \end{equation}
  %
  where $\pset{ \psi_k }$ and $\pset{ \lambda_k }$ are the corresponding eigenfunctions and eigenvalues, respectively. The solution of \req{eq:target-mapping-eigen} determines the low-dimensional manifold~\cite{coifman2005geometric}. For instance, the target mapping can be parametrized as follows:
  %
  \begin{equation}
    \xi(\bx) = \pset[\big]{ \lambda_k \psi_k(\bx) }_{k=1}^d,
  \end{equation}
  %
  where the $k$-th manifold coordinate is $\lambda_k \psi_k(\bx)$. The eigenvalues are sorted by non-increasing value and includes only $d$ dominant eigenvalues as each eigenvalue corresponds to the importance of respective coordinates spanned by eigenfunctions.

  The eigenvalues decrease exponentially and can be related to the effective timescales of the studied physical process, as multiple timescales frequently characterize dynamical systems. As such, the dominant eigenvalues also correspond to the slowest processes. We analyze this in detail in \rsct{sec:diffusion-map}. Manifold-learning methods exploiting the eigendecomposition to find a low-dimensional manifold of CVs are described in \rsct{sec:tm-eigendecomposition}.

  \item[II.] \emph{Divergence optimization} where a divergence (i.e., a statistical difference between a pair of probability distributions) between the Markov transition matrix $M$ built from high-dimensional samples and a Markov transition matrix $Q(\bz_k,\bz_l)$, constructed from a low-dimensional samples, is minimized [\rfig{fig:manifold}]. The target mapping expressed as a parametrizable embedding is:
  %
  \begin{equation}
    \xi_{\btheta}(\bx) = \pset[\big]{ \xi_k(\bx;\btheta) }_{k=1}^d,
  \end{equation}
  %
  where $\btheta=\pset{\theta_k}$ are parameters that are varied such that the divergence between $M$ and $Q$ is minimized. In such methods, $M$ is fixed while $Q$ is estimated by the parametrized target mapping. Depending on the manifold learning method used, the minimization can be performed differently, i.e., gradient descent~\cite{maaten2008visualizing}, or stochastic gradient descent if the target mapping is represented by a neural network~\cite{maaten2009learning,zhang2018unfolding,rydzewski2021multiscale}. We introduce such manifold learning methods in \rsct{sec:tm-divergence-optimization}.
\end{enumerate}
%

\section{Target Mapping (I): Eigendecomposition}
\label{sec:tm-eigendecomposition}
%
Here, we cover manifold learning methods that employ the eigendecomposition of the Markov transition matrix to find the target mapping $\xi(\bx)$. The following manifold learning methods we discuss are diffusion map (DMAP) [\rsct{sec:diffusion-map}] and time-lagged independent component analysis (TICA) [\rsct{sec:tica}].%, and spectral gap optimization (SGOOP) [\rsct{sec:spectral-gap-optimization}].

\subsection{Diffusion Map (DMAP)}
\label{sec:diffusion-map}
%
The concept of DMAP was inspired mainly by Laplacian eigenmap. Laplacian eigenmap~\cite{belkin2001laplacian,belkin2003laplacian} originates from spectral graph theory~\cite{chung1997spectral}, which is a mathematical field studying properties of the Laplacian matrix or adjacency matrix associated with a graph. As states of the system can be represented as a graph, Laplacian eigenmap is commonly used in manifold learning to reduce data dimensionality. Laplacian eigenmap has theoretical convergence guarantees as the discrete operator approaches the Laplacian on the underlying manifold assuming the data are \textit{uniformly} sampled.

DMAP proposed by Coifman et al.~\cite{coifman2005geometric} expands the concept of Laplacian eigenmap. This algorithm yields a family of embeddings and provides theoretical understanding for the resulting embedding even when the data are \textit{nonuniformly} sampled from the manifold. DMAP can construct an informative low-dimensional embedding of a complex dynamical system. Of manifold learning methods, DMAP has a substantial theoretical background~\cite{coifman2005geometric,coifman2006diffusion,coifman2008diffusion}. Namely, the CVs spanning the low-dimensional manifold constructed by DMAP correspond to the slowest dynamical processes of the relaxation of a probability distribution evolving under a random walk over the data~\cite{nadler2006diffusion}.

\subsubsection{Diffusion Kernel}
\label{sec:diffusion-kernel}
%
We consider the anisotropic DMAP~\cite{nadler2006diffusion}. DMAP first employs a Gaussian kernel function to estimate the similarity between high-dimensional feature samples $\bx_k$ and $\bx_l$, $G_\varepsilon(\bx_k,\bx_l)$ [\req{eq:gaussian-kernel}], where $\varepsilon$ is a scale parameter. The scale parameter $\varepsilon$ can be chosen using several heuristics~\cite{coifman2008graph,rohrdanz2011determination,rohrdanz2013discovering,zheng2013rapid,kim2015systematic}. For instance, so that $\varepsilon$ is the median of the pairwise distances between samples.

Next, the Gaussian kernel is used to define a density-preserving kernel in a high-dimensional space---the anisotropic diffusion kernel:
%
\begin{equation}
  \label{eq:diffusion-map-kernel}
  L(\bx_k,\bx_l) = \frac{G_\varepsilon(\bx_k,\bx_l)}{[\varrho(\bx_k)]^\alpha [\varrho(\bx_l)]^\alpha},
\end{equation}
%
where $\varrho(\bx_k) = \sum_n G_\varepsilon(\bx_k,\bx_n)$ is up to a normalization constant a pointwise kernel density estimate at $\bx_k$ and $\alpha \in [0,1]$ is a normalization parameter, the anisotropic diffusion constant, based on which a family of different manifold parametrizations can be considered. Note that other formulations of \req{eq:diffusion-map-kernel} are also possible~\cite{coifman2005geometric}.

Then, \req{eq:diffusion-map-kernel} is row-normalized to represent Markov transition probabilities:
%
\begin{equation}
  \label{eq:diffusion-map-markov}
  p_{kl} \sim M(\bx_k,\bx_l) = \frac{L(\bx_k,\bx_l)}{\sum_n L(\bx_k,\bx_n)},
\end{equation}
%
so that $\sum_l M(\bx_k,\bx_l) = 1$ for $k$-th row. As such, \req{eq:diffusion-map-kernel} is a Markov transition matrix that contains information about the probability of the transition from $\bx_k$ to $\bx_l$. Under this view, \req{eq:diffusion-map-markov} denotes a Markov chain with the transition probability from $\bx_k$ to $\bx_l$ in one time step [\req{eq:markov-chain}]. It can be shown that there is a direct link between Laplacian eigenmap and DMAP~\cite{nadler2006diffusion}.

The anisotropic diffusion constant [\req{eq:diffusion-map-kernel}] relates to the importance of the data density of samples~\cite{coifman2006diffusion}. Specifically, when the microscopic coordinates are sampled according to the equilibrium density, in the limits $\varepsilon\rightarrow 0$ and $K\rightarrow\infty$, DMAP asymptotically converges to the stationary Boltzmann distribution of the modeled Markov chain. Under these assumptions, depending on the normalization using the anisotropic diffusion constant, we can consider the following interesting constructions of the stationary density:
%
\begin{enumerate}[leftmargin=0.5cm]
  \item $\alpha=0$: we recover the backward Fokker--Planck operator with the potential $2 U(\bx)$ and the density $\propto \pbkt{\rho(\bx)}^2$ for the classical normalized graph Laplacian~\cite{belkin2001laplacian,belkin2003laplacian,jones2008manifold}.
  \item $\alpha=1$: we get the graph Laplacian with the Laplace--Beltrami operator with data uniformly distributed on a manifold. This normalization accounts only for the data geometry while the density plays no role.
  \item $\alpha=\frac{1}{2}$: we obtain the backward Fokker--Planck operator with the underlying potential $U(\bx)$ and the density $\propto \rho(\bx)$ whose eigenfunctions capture the long-time asymptotics of data (i.e., correspond to slow variables).
\end{enumerate}
%

For the anisotropic diffusion constant $\alpha=\frac{1}{2}$, we asymptotically recover the long-time dynamics of the system whose microscopic coordinates are sampled from the Boltzmann distribution [\req{eq:boltzmann-density}]. The related backward Fokker--Planck differential equation is given by:
%
\begin{equation}
  \label{eq:sde}
  \mu \dx = -\nabla U(\bx)\mathrm{d}t + \sqrt{2\mu{\beta}^{-1}}\dd{\mathbf{w}},
\end{equation}
%
where $\mu$ is the friction coefficient, $-\nabla U(\bx)$ denotes the force acting on atoms, and $\mathbf{w}$ is an $n$-dimensional Brownian motion. The infinitesimal generator of this diffusion process is:
%
\begin{equation}
  \label{eq:infgen}
  \mathcal{L}=-\mu^{-1} \nabla U \cdot \nabla + (\mu\beta)^{-1} \nabla^2.
\end{equation}
%
The eigenvalues and eigenvectors of $\mathcal{L}$ determine the kinetic information of the diffusion process and can be used to parametrize a low-dimensional manifold.

As we are interested in finding a low-dimensional representation of a dynamical system, i.e., estimating CVs, the case for $\alpha=\frac{1}{2}$ is crucial to model the slowest degrees of freedom, accounting for both the underlying geometry and density of the manifold.

Note that, when considering configuration variables other than the microscopic coordinates, the underlying equilibrium density is not given by the Boltzmann distribution [\rsct{sec:atomistic-simulations}]. However, it can be assumed that there is a separation of timescales in the case of variables other than the microscopic coordinates. In other words, the dynamics of these variables is assumed to be slower compared to other variables.

\subsubsection{Diffusion Coordinates}
%
The idea of using eigenfunctions of the Gaussian kernel as coordinates for Riemannian manifolds originates with \rref{berard1994embedding} and in the context of data analysis with \rref{coifman2006diffusion}. The transition probability matrix $M$ can be used to solve the eigenvalue problem:
%
\begin{equation}
  M\psi_k = \lambda_k \psi_k
\end{equation}
%
for $k=1, \dots, K$. The spectrum then is synonymous with the eigenvalues $\{ \lambda_l \}$. The corresponding right eigenvectors $\{ \psi_l \}$ can be used to embed the system in a low-dimensional representation (or CVs).

Based on this eigendecomposition, the target mapping $\xi(\bx)$ [\req{eq:collective-variables}] can be defined as {\it diffusion coordinates}~\cite{coifman2005geometric,coifman2006diffusion}:
%
\begin{equation}
  \label{eq:diffusion-coordinates}
  \bx \mapsto \xi(\bx) = \ppar[\big]{\lambda_1 \psi_1(\bx), \dots, \lambda_d \psi_d(\bx)},
\end{equation}
%
where the eigenvalues and eigenvectors are given by $\pset{\lambda_l}$ and $\pset{\psi_l}$, respectively, and define reduced coordinates. In \req{eq:diffusion-coordinates}, each diffusion coordinate is defined as $z_k = \lambda_k \psi_k$, where the spectrum is sorted by non-increasing value:
%
\begin{equation}
  \label{eq:eigenvalues}
  \lambda_0=1 > \lambda_1 \ge \dots \ge \lambda_d \ge \dots \ge \lambda_K,
\end{equation}
%
where $d$ is the index at which we truncate the diffusion coordinates in \req{eq:diffusion-coordinates} and the dimensionality of the reduced representation. Thus, it is expected that the dominant timescales found in the dynamics of the high-dimensional system can be described only by several eigenvectors corresponding to the largest eigenvalues. As the dynamics in DMAP is represented by the transition probability matrix [\req{eq:diffusion-map-markov}], the eigenvalue $\lambda_0=1$ and the first diffusion coordinate $\lambda_0\psi_0$ corresponds to the Boltzmann equilibrium distribution. Therefore, we exclude it from the target mapping [\req{eq:diffusion-coordinates}].

The truncating in \req{eq:eigenvalues} can be justified as follows. As dynamical systems are usually metastable [\rsct{sec:enhanced-sampling}], it is sufficient to approximate the diffusion coordinates by a set of dominant eigenvalues [e.g., up to $d$ in \req{eq:diffusion-coordinates} and \req{eq:eigenvalues}]. This corresponds to a negligible error on the order of $O(\lambda_{d}/\lambda_{d-1})$. In other words, a sufficient condition for this is $\lambda_{d-1} \gg \lambda_{d}$ as it relates to a large {\it spectral gap}. As such, the spectral gap separates slow degrees of freedom (for $\ge \lambda_d$) and fast degrees of freedom (for $<\lambda_{d}$).

\subsubsection{Diffusion and Commute Distances}
%
In the reduced space of the diffusion coordinates, we can define the diffusion distance, a measure of proximity for the samples lying on a low-dimensional manifold. The diffusion distance is equivalent to Euclidean distance on the manifold~\cite{coifman2005geometric}:
%
\begin{align}
  \label{eq:diffusion-distance}
  D^2(\bx_k,\bx_l) =
    \sum_{n=1}^d \lambda_n^2 \ppar[\big]{\psi_n(\bx_k) - \psi_n(\bx_l)}^2 =
    \|\xi(\bx_k) - \xi(\bx_l)\|^2,
\end{align}
%
where the definition of $\xi(\bx)$ is given by \req{eq:diffusion-coordinates}. Alternatively, we can write that the diffusion distance between high-dimensional samples $\bx_k$ and $\bx_l$ is equivalent to Euclidean distance between CV samples $\bz_k = \xi(\bx_k)$ and $\bz_l = \xi(\bx_l)$.

A notion of relaxation time scales [\req{eq:effective-timescale}] is possible only when a time-lagged construction of pairwise transition probabilities is possible. For instance, this can be done using Mahalanobis distance as a distance metric which employs estimating a time covariance matrix~\cite{dsilva2015data}, the Taken theorem (a delay embedding theorem)~\cite{packard1980geometry}, or the von Neumann entropy~\cite{moon2019visualizing}.

The diffusion coordinates [\req{eq:diffusion-coordinates}] can also be defined using the relation of the effective timescales and the eigenvalues~\cite{nadler2006diffusion}. Employing the fact that the eigenvalues decay exponentially:
%
\begin{equation}
  \label{eq:effective-timescale}
  \lambda_n(\tau)=\e^{-\tau \kappa_n}=\e^{-\tau/ t_n},
\end{equation}
%
where $\tau$ is a lag time, $\kappa_n$ denotes relaxation rates and $\kappa_n^{-1}=t_n$ are effective timescales, we can rewrite \req{eq:diffusion-coordinates} to obtain kinetic map:
%
\begin{equation}
  \label{eq:lagged-target-mapping}
  \xi(\bx) = \ppar[\big]{\e^{-\tau/t_1} \psi_1(\bx), \dots, \e^{-\tau/t_d} \psi_d(\bx)},
\end{equation}
%
where $\tau$ should be selected so the samples are uncorrelated. Selecting the lag time is a known issue in modeling metastable dynamics. Namely, the target mapping in \req{eq:lagged-target-mapping} strongly depends on the lag time $\tau$. The lag time should be selected to separate fast and slow processes if there is an evident timescale separation, i.e., the lag time value is selected between fast and slow timescales. However, these timescales are often unknown before running simulations.

A particularly interesting method that does not require selecting the lag time relies on integrating \req{eq:diffusion-distance} over the lag time $\tau$. Then, we use the relation between the eigenvalues and the effective timescales [\req{eq:effective-timescale}] to arrive at commute distances~\cite{noe2016commute}:
%
\begin{align}
  \label{eq:commute-distances}
  D_c^2(\bx_k,\bx_l)
    = \sum_{n=1} \pbkt*{\ppar[\big]{\psi_n(\bx_k) - \psi_n(\bx_l)}^2 \int_0^{\infty}\dd{\tau}\e^{-\tau/t_n}}
    = \frac{1}{2}\sum_{n=1}t_n\ppar[\big]{\psi_n(\bx_k) - \psi_n(\bx_l)}^2.
\end{align}
%
\req{eq:commute-distances} is approximately the average time the systems spends to commute between $\bx_k$ and $\bx_l$, and the associated commute map is given by:
%
\begin{equation}
  \xi(\bx) = \ppar*{ \sqrt{\frac{t_1}{2}} \psi_0(\bx), \dots, \sqrt{\frac{t_d}{2}}\psi_d(\bx)},
\end{equation}
%
where we can see the difference between the diffusion and commute distances are in the coefficients $\lambda_n$ and $t_n/2$, respectively. Various methods exploit the relation of the effective timescale with eigenvalues~\cite{noe2015kinetic,boninsegna2015investigating,noe2016commute,klus2018data,banisch2020diffusion,tsai2021sgoop,evans2022computing,evans2023computing}.

\subsubsection{Reweighting Markov Transitions}
\label{sec:diffusion-reweighting}
%

%
\begin{figure}
  \includegraphics{fig/fig-dm-equilibrium}
  \caption{{\bf Diffusion Reweighting}. The equilibrium diffusion coordinate $\lambda_0\psi_0$ calculated for an alanine dipeptide data set at 300 K in vacuum generated using well-tempered metadynamics using a bias factor of 5. The data set consists of 5000 samples, with 45 features being all pairwise heavy-atom distances in the system. The standard DMAP captures a low-dimensional manifold that is biased and does not correspond to the equilibrium distribution, as seen by the lowered free-energy barriers and consequently boosted transitions between the metastable states. The reweighted DMAP (using diffusion reweighting) correctly reverts the effect of sampling from a biased probability distribution of the data set.}
  \label{fig:diffusion-reweighting}
\end{figure}
%

DMAP can learn low-dimensional CVs from unbiased atomistic simulations [\rsct{sec:manifold-learning}]. A slightly different approach is required to enable learning from enhanced sampling simulations, where sampling is performed according to some biased probability distribution. Here, we focus on a recently introduced variant of DMAP, referred to as reweighted DMAP~\cite{rydzewski2022reweighted,rydzewski2023selecting}, based on anisotropic DMAP [\rsct{sec:diffusion-map}].

As explained in \rsct{sec:manifold-learning}, a customary initial point of many manifold learning methods is the construction of the Markov transition matrix based on the Gaussian kernel [\req{eq:gaussian-kernel}]. This construction includes information about the manifold geometry represented by the pairwise distances. To provide a framework capable of reweighting the Markov transition matrix obtained from enhanced sampling simulations, we need a kernel that includes information about the manifold density and importance.

To account for the manifold density, we employ a density-preserving kernel. Let us remind the pairwise transition probabilities based on the anisotropic diffusion kernel [\req{eq:diffusion-map-kernel}]:
%
\begin{equation}
  \label{eq:anisotropic-diffusion-kernel}
  L(\bx_k, \bx_l) = \frac{G_\varepsilon(\bx_k, \bx_l)}{[\varrho(\bx_k)]^\alpha [\varrho(\bx_l)]^\alpha},
\end{equation}
%
where $\varrho(\bx)$ is a kernel density estimator and $\alpha \in [0,1]$ is the anisotropic diffusion constant; see \rsct{sec:diffusion-kernel} for a discussion. In \req{eq:anisotropic-diffusion-kernel}, the density estimator $\varrho(\bx_k)$ at a sample $\bx_k$ must be reweighted to account on the data importance:
%
\begin{equation}
  \label{eq:unbiased-density-estimate}
  \varrho(\bx_k)=\sum_l w(\bx_l) G_\varepsilon(\bx_k,\bx_l),
\end{equation}
%
which is a weighted kernel density estimate up to an unimportant multiplicative constant and corresponds to the unbiased density.

Based on \req{eq:anisotropic-diffusion-kernel}, the Markov transition matrix can be estimated by weighting each Gaussian term and normalizing it so that it is row-stochastic:
%
\begin{equation}
  \label{eq:markov}
  M(\bx_k, \bx_l) = \frac{w(\bx_l) L(\bx_k,\bx_l)}{\sum_m w(\bx_m) L(\bx_k,\bx_m)}.
\end{equation}
%
Next, by inserting \req{eq:anisotropic-diffusion-kernel} to \req{eq:markov}, we can see that the Markov transition matrix $M$ can be rewritten using the Gaussian kernels:
%
\begin{equation}
  \label{eq:markov-generator}
    M(\bx_k, \bx_l) \propto \frac{w(\bx_k)}{{[\varrho(\bx_k)]^\alpha}}
    \frac{w(\bx_l)}{{[\varrho(\bx_l)]^\alpha}}
    G_\varepsilon(\bx_k, \bx_l),
\end{equation}
%
where we skip the normalization factor for brevity. The scaling term of the Gaussian:
%
\begin{align}
  \label{eq:reweighting-factor}
  r(\bx_k,\bx_l) = \frac{w(\bx_k)}{{[\varrho(\bx_k)]^\alpha}} \frac{w(\bx_l)}{{[\varrho(\bx_l)]^\alpha}},
\end{align}
%
defines a pairwise reweighting factor required to unbiased the transition probabilities estimated from biased data~\cite{rydzewski2022reweighted,rydzewski2023selecting}. Other reweighting methods for DMAP are also available~\cite{banisch2020diffusion,trstanova2020local,evans2023computing}.

The reweighted Markov transition matrix [\req{eq:markov-generator}] encodes information about:
%
\begin{enumerate}[leftmargin=0.5cm]
  \item Geometry $G_\varepsilon(\bx_k,\bx_l)$: The probability of transitions between samples lying far from each other is low and high for those in close proximity.
  \item Density $[\varrho(\bx_l)]^\alpha$: The anisotropic diffusion constant $\alpha \in [0,1]$ is used as a density-scaling term as in DMAP. See \req{eq:anisotropic-diffusion-kernel} and the corresponding description.
  \item Importance $w(\bx_l)$: The weights from enhanced sampling indicates if a sample is important, i.e., metastable states where the weights are higher are more important then high free-energy regions.
\end{enumerate}
%

A procedure for constructing the reweighted anisotropic variant of DMAP is given in Algorithm~\ref{alg:diffusion-map}.
%
\begin{algorithm}
  \label{alg:diffusion-map}
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
  \Input{Data set $\pset*{\bx_k}_{k=1}^K$, anisotropic diffusion constant $\alpha$.}
  \Output{Eigenvalues $\pset{\lambda_k}$ and eigenvectors $\pset{\psi_k}$ of the transition matrix $M$.}
  \begin{enumerate}[leftmargin=0cm]
    \item Calculate the squared pairwise distances $\| \bx_l - \bx_l \|^2$.
    \item Estimate the Markov transition matrix $M(\bx_k,\bx_l)$:
    \begin{enumerate}[leftmargin=0.6cm]
      \item Calculate the Gaussian kernel $G_\varepsilon(\bx_k,\bx_l)$ [\req{eq:gaussian-kernel}].
      \item Construct the anisotropic diffusion kernel including the anisotropic diffusion constant $\alpha$ [\req{eq:diffusion-map-kernel}] and diffusion reweighting if the data set is sampled from a biased probability distribution [\req{eq:reweighting-factor}].
      \item Normalize to obtain the row-stochastic Markov transition matrix $M(\bx_k,\bx_l)$ [\req{eq:diffusion-map-markov}].
    \end{enumerate}
    \item Perform eigendecomposition $M\psi_k=\lambda_k\psi_k$ and based on the spectral gap and dominant eigenvalues estimate the diffusion coordinates $\xi(\bx)$ [\req{eq:diffusion-coordinates}].
  \end{enumerate}
  \caption{Reweighted Diffusion Map}
\end{algorithm}
%

\subsubsection{Applications}
%
DMAP and its variants have been used extensively to study the low-dimensional properties of dynamical systems. Some noteworthy applications include constructing CVs and low-dimensional representations, molecular kinetics, and enhanced sampling~\cite{nadler2006diffusion,coifman2008diffusion,singer2009detecting,ferguson2011nonlinear,ferguson2011integrating,rohrdanz2011determination,zheng2013rapid,zheng2013molecular,rohrdanz2013discovering,kim2015systematic,noe2015kinetic,boninsegna2015investigating,noe2016commute,chiavazzo2017intrinsic,de2017diffusion,zhang2018unfolding,long2019landmark,tan2019approximating,banisch2020diffusion,trstanova2020local}. DMAP is implemented in the \texttt{pydiffmap} package~\cite{pydiffmap}.

\subsection{Time-Lagged Independent Component Analysis (TICA)}
\label{sec:tica}
%
Molgedey and Schuster introduced TICA in 1994 to determine the mixing coefficients of linearly superimposed uncorrelated signals~\cite{molgedey1994separation}. As simulations can also be viewed as propagating ``signals,'' this inspired novel methodology development in analyzing long simulation trajectories~\cite{alakent2004application,hernandez2013identification,schwantes2013improvements,endo2018multi,tsai2020learning,wu2020variational}. In 2013, two research groups independently applied TICA in Markov state modeling to understand the kinetics of conformational changes~\cite{hernandez2013identification,schwantes2013improvements}. TICA is a commonly used method to project high-dimensional simulation data to a low-dimensional manifold which can significantly improve uncovering slow dynamical processes~\cite{wu2020variational}.

\subsubsection{Markov Propagator of Stochastic Processes}
%
As in the case of DMAP, we start from the notion that dynamics of the microscopic coordinates of the system can typically be described by the stochastic differential equation given by \req{eq:sde}. The corresponding transition probability $p_\tau(\bx,\by)\dx=\operatorname{Prob}\pset[\big]{\bx_{t+\tau} = \by\,|\,\bx_t=\bx}$ describes how probable is the transition from $\bx$ to $\by$ in a time step $\tau$. With the transition probability, we can describe the time-evolution of a time-dependent probability distribution:
%
\begin{equation}
  \rho_{t+\tau}(\by) = \int\dx p_\tau(\bx,\by) \rho_t(\bx),
\end{equation}
%
which alternatively can be described by the Markov time-propagator $\cM_{\tau}$:
%
\begin{equation}
  \rho_{t+\tau}(\bx) = \cM_{\tau} \rho_t(\bx),
\end{equation}
%
that maps a probability distribution $\rho_t(\bx)$ to $\rho_{t+\tau}(\bx)$. We want to emphasize that the time-propagator $\cM_{\tau}$ is closely related to the semigroup $\cK_\tau$ and infinitesimal generator $\cL$~\cite{wu2020variational}. The semigroup is given as:
%
\begin{equation}
  {\cK_{\tau} g}(\bx) = \mathbb{E}\pbkt[\big]{g(\bx_{t+\tau})\,|\,\bx_t=\bx},
\end{equation}
%
where $g$ is an auxilary function. Then, the generator $\cL$ is defined as $\cL g=\lim_{t\rightarrow 0^+}(\cK_\tau g-g)/t$. The adjoint operator of $\cL$, denoted as $\cL^\dagger$, determines the time-evolution of $\rho$:
%
\begin{equation}
  \frac{\partial \rho_t(\bx)}{\partial t}=\cL^\dagger \rho_t(\bx),
  \label{eq:fpe}
\end{equation}
%
which is the famous Fokker-Planck equation. From \req{eq:fpe}, we can see that the time-propagator has the form: 
%
\begin{equation}
  \cM_{\tau}=\e^{\tau\cL^\dagger}.
  \label{eq:expu}  
\end{equation}
%

To understand kinetic information of the system, it is necessary to analyze eigenvalues and eigenfunctions of the generators $\cL$ and  $\cL^\dagger$, and the propagator $\cM_{\tau}$. The eigenvalues and eigenfunctions of $\cL$ satisfy the equation:
%
\begin{equation}
    \cL \psi_k = -\lambda_k \psi_k,
    \label{eq:l-eigen}
\end{equation}
%
where the eigenvalues are sorted by increasing values $0=\lambda_0<\lambda_1 \leq \cdots$. The eigenvalues of $\cL^\dagger$ are the same as $\cL$, while its eigenfunctions are given by $\phi_k = \pi \psi_k$, where $\pi$ is the equilibrium Boltzmann distribution. Clearly, $\pi$ is also an eigenfunction of $\cL^\dagger$ with $\lambda_0=0$ so that $\psi_0=1$, as $\lambda_0=0$ is non-degenerate and corresponds to a unique equilibrium distribution. With $\lambda_k$ and $\phi_k$, we can formally write the time-dependent probability distribution:
%
\begin{equation}
    \rho_t(\bx) = \pi + \sum_{k=1} A_k \e^{-\lambda_k t} \phi_k(\bx),
    \label{eq:tdprob}
\end{equation}
%
where coefficients are determined by the initial conditions $A_k = \int\dx (\bx) \psi_k(\bx)\pi$. \req{eq:tdprob} means that $\rho_t(\bx)$ converges to $\pi$ when $t\rightarrow\infty$. The eigenfunctions $\phi_k$ can be viewed as different ``modes'' with an equilibration rate determined by $\lambda_k$, where small $\lambda_k$ suggests a slow equilibration along $\phi_k$. As $\cM_{\tau}$ shares the eigenfunctions with $\cL^\dagger$, while the eigenvalues of $\cM$ are $\lambda^\cM_k = \e^{\tau\lambda_k}$, the equilibration timescale is $t_k=-\tau/\log\lambda_k^\cM$.

\subsubsection{Variational Optimization of Dominant Eigenvectors}
%
As discussed in the previous section, the eigenfunctions of $\cM_{\tau}$ with small $\lambda^\cM_k$ can be used to define the target mapping as they correspond to slow equilibration rates. The first non-trivial eigenfunction $\psi_1$ is the most interesting mode as it corresponds to the smallest non-zero eigenvalue. It can be estimated variationally by maximizing:
%
\begin{align}
    \pang[\big]{g\,|\,\cM_{\tau}\,|\,g}_{\pi}
    = \int\dx\dy g(\bx) p_\tau(\bx,\by) g(\by) \pi,
    \label{eq:tica-loss}
\end{align}
%
with a constraint given as $\int\dx g(\bx) \pi=0$, where $g$ is a function to be optimized. \req{eq:tica-loss} can be generalized to obtain the eigenvalues and eigenfunctions of $\cM_{\tau}$~\cite{hernandez2013identification,nuske2014variational}. As $\psi_0=1$, $g(\bx)$ can be expressed as a linear combination:
%
\begin{equation}
    g(\bx) = \sum_k c_k \eta_k(\bx),
    \label{eq:linear-g}
\end{equation}
%
where $c_k$ are cofficients and $\eta_k(\bx)$ are basis functions. It can be shown that the maximization of \req{eq:tica-loss} is equivalent to solving a generalized eigenproblem:
%
\begin{equation}
    M\chi=\lambda^M S\chi,
    \label{eq:geigen}
\end{equation}
%
where $M_{kl}=\pang[\big]{\eta_i\,|\,\cM_{\tau}\,|\,\eta_j}$, the overlap matrix $S$ is defined as $S_{kl}=\pang[\big]{\eta_k\,|\,\eta_l}$, and $\chi_0$ corresponds to the equilibrium distribution. Features with zero-mean are ususally taken as
basis functions~\cite{molgedey1994separation,naritomi2011slow,schwantes2013improvements,hernandez2013identification}. However, other basis functions can also be employed. For instance, indicator functions of a partition of the configuration space~\cite{hernandez2013identification,nuske2014variational} or Gaussian functions can be used, which requires fewer macrostates and shorter $\tau$ to converge~\cite{nuske2014variational}. Neural networks can also be used~\cite{bonati2021deep}.

In practice, $M_{kl}$ depends on the lag time $\tau$ which is often taken to be on a nanosecond timescale~\cite{mcgibbon2015variational,sultan2017tica}. For unbiased simulation data, we can estimate $M_{kl}(\tau)$ as:
%
\begin{equation}
    M_{kl}(\tau) = \frac{1}{K-K_\tau}\sum_{n=1}^{K-K_\tau} x_k(n) x_l(n+K_\tau),
    \label{eq:c-tau}
\end{equation}
%
where $x_k$ and $x_l$ are $k-$th and $l$-th features, respectively, and the time lag is given as $\tau=K_\tau\Delta t$, where $\Delta t$ is a time step. As $x_k$ and $x_l$ are usually not orthonormal, the overlap matrix can be estimated as:
%
\begin{equation}
  S_{kl} = M_{kl}(\tau=0) = \frac{1}{K}\sum_{n=1}^{K}x_k(n) x_l(n),
  \label{eq:c0}
\end{equation}
%
which reduces the generalized eigenproblem [\req{eq:geigen}] to $M(\tau)\chi=\lambda^M M(0)\chi$. We can see that the matrix $M$ has a different interpretation comparing to other methods reviewed here, e.g., rather than describing the probabilities between samples, it is given by autocorrelation functions.

A procedure for performing TICA is given in Algorithm~\ref{alg:tica}.
%
\begin{algorithm}
  \label{alg:tica}
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
  \Input{Unbiased data set $\{\bx_k\}_{k=1}^K$, high-dimensional configuration variables (features) $\bx=\pset{x_k}_{k=1}^n$, preselected lag-time $\tau$.}
  \Output{Eigenfunctions $\pset{\psi_k}_{k=1}^d$.}
  \begin{enumerate}[leftmargin=0cm]
    \item Calculate ensemble average $\pang{\bx}$.
    \item Calculate shifted features $\tilde{\bx}=\bx-\pang{\bx}$.
    \item Calculate $M(\tau)$ with \req{eq:c-tau} and $M(0)$ with \req{eq:c0}.
    \item Solve the generalized eigenproblem with the AMUSE algorithm~\cite{tong1990amuse,hernandez2013identification}. This is mainly because $M(0)$ is often ill-conditioned thus $M(0)^{-1}$ is numerically unstable.
    \begin{enumerate}[leftmargin=0.6cm]
      \item Perform principle component analysis to transform $\tilde{\bx}$ to principle
      components $\mathbf{y}$.
      \item Calculate normalized $\tilde{\mathbf{y}}=\Sigma^{-1}\mathbf{y}$, where $\Sigma$ is a diagonal matrix whose $k-$th diagonal element is the standard deviation of $y_k$.
      \item Calculate $\tilde{M}(\tau)$ with respect to $\tilde{\mathbf{y}}$.
      \item Diaganoalze $\tilde{M}(\tau)$. Use the $k$-th eigenvector as coefficients to construct a linear combination of $\tilde{\mathbf{y}}$ to calculate $\psi_k$.
    \end{enumerate}
  \end{enumerate}
  \caption{Time-Independent Component Analysis}
\end{algorithm}
%

\subsubsection{TICA and Enhanced Sampling Simulations}
%
Generating TICA directly from enhanced sampling simulation is another approach worth considering. However, this remains highly challenging as the kinetics of many enhanced sampling methods do not preserve unbiased kinetics. CVs generated using TICA with a Markov state model can be used with metadynamics, which has significantly improved the sampling efficiency of complex systems~\cite{sultan2017tica}. In general, however, generating CVs using TICA requires extensive sampling, and thus it is questionable whether such CVs should be improved iteratively during enhanced sampling. The problem can be treated to some extent using transferable CVs estimated by TICA~\cite{sultan2018transferable}.

A successful example is constructing CVs with TICA and well-tempered metadynamics~\cite{jay2017variational,bonati2021deep}. It has been shown that not only can configurations of the system be unbiased~\cite{bonomi2009reconstructing,tiwary2015time,giberti2020iterative} but also an effective timescale can be obtained by a rescaling simulation time~\cite{tiwary2015time,jay2017variational}:
%
\begin{equation}
    \dd{t} \mapsto \e^{\beta\pbkt*{V(\bz,t)-c(t)}} \dd{t},
    \label{eq:change-time}
\end{equation}
%
where $V(\bz)$ is a time-dependent bias potential acting in the CV space [\req{eq:bias-potential}] and $c(t)$ is a time-dependent constant defined as:
%
\begin{equation}
    c(t)=-\frac{1}{\beta} \frac{\int\dz\, \e^{-\beta\pbkt*{F(\bz)+V(\bz,t)}}}
    {\int\dz\, \e^{-\beta F(\bz)}}.
\end{equation}
%
\req{eq:change-time} is asymptotically correct at the long-time limit~\cite{tiwary2015time}, and generally, the rescaling does not correspond to the actual unbiased time.

\subsubsection{Applications}
%
TICA has been mainly used for constructing Markov state models, kinetics and free-energy estimation of slow processes, finding CVs, and enhanced sampling~\cite{hernandez2013identification,schwantes2013improvements,hernandez2013identification,paul2017protein,wu2017variational,sultan2017tica,jay2017variational,mardt2018vampnets,sultan2018towards,ferruz2018dopamine,ahalawat2018mapping,sultan2018transferable,pantsar2018assessment,mondal2018atomic,sidky2019high,lemke2019encodermap,zhang2019improving,sengupta2019automated,brotzakis2019accelerating,tran2019dissociation,wu2020variational,abella2020markov,pantsar2020kras,bonati2021deep,barros2021markov,song2021folding,wang2021effect,jones2021determining,spiriti2022simulation,lohr2022small}. For more applications, we refer to reviews~\cite{chodera2014markov,shukla2015markov,husic2018markov}. TICA is implemented in MSMBuilder~\cite{beauchamp2011msmbuilder2} and PyEMMA~\cite{scherer2015pyemma}.

%\subsection{Spectral Gap Optimization of Order Parameters (SGOOP)}
%\label{sec:spectral-gap-optimization}
%
%\note{for Omar}
%
%As shown in the case of DMAP, a spectral eigendecomposition can be used to separate timescales between fast and slow processes. However, sometimes it is sufficient to know the difference between the slow and fast eigenvalues rather than the exact eigenvalues and the related eigenvectors based on which a low-dimensional manifold can be constructed. The spectral gap can be used to estimate the degree of the separation between the fast and slow processes. Consequently, the maximal obtainable spectrum gap corresponds to the most optimal construction of the slow variables. An approach to maximizing the spectral gap between these processes is based on the maximum entropy framework and referred to as spectral gap optimization of order parameters (SGOOP)~\cite{tiwary2016spectral} (order parameters refer to CVs).
%
%\subsubsection{Transition Probabilities from Entropy Maximization}
%
%In SGOOP, we assume as before that the dynamics of our configuration variables $\pset{\bx_k}_{k=1}^K$ is Markovian. These configurational variables can be selected as any linear or nonlinear functions of the microscopic coordinates and the space spanned by them is high-dimensional. Then, the Markov transition matrix in the high-dimensional space can be defined as:
%
%\begin{equation}
%  \label{eq:markov-entropy}
%  p_{kl} \sim M(\bx_k,\bx_l) = \e^{\tau K(\bx_k,\bx_l)},
%\end{equation}
%
%where $K$ is the rate matrix. \req{eq:markov-entropy} should not depend on the lag time $\tau$ when it is sufficiently small and the transition matrix is Markovian. In the maximum caliber approach (a generalization of maximum entropy to dynamics), one can define a path entropy dependent on the probability of micropaths for a Markovian process discrete in time and space~\cite{dixit2015inferring,ghosh2020maximum}:
%
%\begin{equation}
%  \label{eq:sgoop-entropy}
%  S = -\sum_{kl} \varrho(\bx_k) p_{kl} \log p_{kl},
%\end{equation}
%
%where we use a kernel density estimate $\varrho$ because in the case of using the configuration variables other than the microscopic coordinates their underlying equilibrium probability is unknown; see \rsct{sec:high-dimensional-space} for a detailed discussion.
%
%As has been shown~\cite{tiwary2016spectral}, maximizing the caliber is equivalent to being the least committal about missing information. Then, the Markov transition matrix is defined as:
%
%\begin{equation}
%  \label{eq:entropy-markov}
%  M(\bx_k,\bx_l) = \sqrt{\frac{\varrho(\bx_l)}{\varrho(\bx_k)}} \exp\ppar*{-\sum_n l_n c_n(\bx_k,\bx_l)},
%\end{equation}
%
%where the sum is over the restraints $c_n(\bx_k,\bx_l)$ for the samples $\bx_k$ and $\bx_l$, weighted by Lagrange multipliers $l_n$. If we take Euclidean distance $\|\bx_k-\bx_l\|^2$ as the only constraint, the corresponding $M$ takes a simple form:
%
%\begin{equation}
%  M(\bx_k,\bx_l) = \sqrt{\frac{\varrho(\bx_l)}{\varrho(\bx_k)}} \exp\ppar*{-\frac{1}{\varepsilon} \pnrm{\bx_k - \bx_l}^2},
%\end{equation}
%
%where we can see that $M$ is given by a weighted Gaussian kernel [\req{eq:gaussian-kernel}]. Additionally, it has been shown that using maximum caliber, we can obtain the diffusion kernel~\cite{dixit2019introducing}.
%
%The approach taken by SGOOP is very general. The transition probability matrix $M$ [\req{eq:entropy-markov}] can be constructed using a different approach, for instance, DMAP [\rsct{sec:diffusion-map}] or combined with the commute distance [\req{eq:commute-distances}], as is done in \rref{sgoop-d}.
%
%\subsubsection{Applications}
%
%SGOOP, despite being a fairly recent method, has been applied to many dynamical processes:
%
%\begin{enumerate}
%  \item Statistical models~\cite{tiwary2016spectral}.
%  \item Protein conformational changes~\cite{}.
%  \item Molecular kinetics and thermodynamics~\cite{tsai2021sgoop}.
%  \item Ligand unbinding~\cite{}.
%\end{enumerate}
%
%\cite{tsai2019reaction,pramanik2019can,tiwary2017molecular,tiwary2017predicting,tiwary2016wet,zou2021toward,smith2018multi}
%
%A code for SGOOP implemented in \texttt{Python} is available at \url{?}.

\section{Target Mapping (II): Divergence Optimization}
\label{sec:tm-divergence-optimization}
%
This section discusses manifold learning methods that use divergence optimization to find the parametric target mapping. In the following, we describe parametric variants of well-known methods as stochastic neighbor embedding (SNE) [\rsct{sec:sne}] and uniform manifold approximation and projection (UMAP) [\rsct{sec:umap}], which can be used to learn CVs from unbiased atomistic simulations, and multiscale reweighted stochastic embedding (MRSE) [\rsct{sec:mrse}] and stochastic kinetic embedding (StKE) [\rsct{sec:stke}] which additionally can be used to learn manifolds from enhanced sampling simulations.

\subsection{Stochastic Neighbor Embedding (SNE)}
\label{sec:sne}
%
SNE and its variants have been used in almost every field where dimensionality reduction can be employed. However, despite its widespread popularity, there have been surprisingly few investigations into the theoretical background of SNE compared to, for instance, DMAP or Laplacian eigenmap. However, a non-parametric SNE has been recently investigated from a theoretical point of view~\cite{shaham2017stochastic,arora2018analysis,linderman2019clustering,yang2021t}. Specifically, it has been shown that SNE methods can separate clusters of data in a low-dimensional embedding if well-separated clusters in a high-dimensional space exist~\cite{shaham2017stochastic,linderman2019clustering}. This result, however, has been suggested to be insufficient to establish that SNE succeeds in finding a low-dimensional representation~\cite{arora2018analysis,yang2021t}. Interestingly, a connection between SNE and spectral embedding has recently been studied~\cite{carreira2010elastic,linderman2019clustering}.

\subsubsection{Parametric Target Mapping}
%
We consider a more general approach referred to as the parametric SNE. Such variants use a parametric target mapping that can be used to express the mapping from the configuration variable space to the low-dimensional space~\cite{hinton2006reducing,maaten2009learning}. In such a setting, the target mapping is given by:
%
\begin{equation}
  \label{eq:parametric-target-mapping}
  \bx \mapsto \xi_{\boldsymbol{\theta}}(\bx) = \pset[\big]{ \xi_k(\bx;\btheta) }_{k=1}^d,
\end{equation}
%
where $\btheta = \{ \theta_k \}$ are parameters of the target mapping. \req{eq:parametric-target-mapping} can be represented by, e.g., a linear combination or a neural network. Compared to the standard SNE, the parametric target mapping has more flexibility in constructing the approximation of the low-dimensional space. The parametric target mapping is usually optimized by adjusting the parameters $\btheta$ to correspond to the minimum statistical divergence.

\subsubsection{Markov Transitions in High-Dimensional Space}
\label{sec:sne-markov}
%
Similarly to DMAP, the transition matrix $M$ in SNE is also constructed from the Gaussian kernel $G_\varepsilon$ [\req{eq:gaussian-kernel}]. The transition probabilities $p_{kl}$ are defined by the following Markov transition matrix:
%
\begin{equation}
  \label{eq:sne-affinity}
  p_{kl} \sim M(\bx_k,\bx_l) = \frac{G_{\varepsilon_k}(\bx_k,\bx_l)}{\sum_n G_{\varepsilon_k}(\bx_k,\bx_n)},
\end{equation}
%
where the diagonal elements are arbitrarily zeroed so that \req{eq:sne-affinity} defines a non-lazy Markov chain. The construction of \req{eq:sne-affinity} requires selecting the scale parameters $\{ \varepsilon_k \}_{k=0}^K$, where each row of $M$ is characterized by a specific $\varepsilon_k>0$. The scale parameters can be written as $\varepsilon_k \propto \sigma^2_k$, where $\sigma_k$ is the standard deviation of the Gaussian kernel. Then, the transition probabilities [\req{eq:sne-affinity}] are symmetrized as $(p_{kl}+p_{lk})/2$.

Selecting $\{ \varepsilon_k \}$ is the main difference compared to constructing the Markov transition matrix from the single-scale Gaussian kernel [\req{eq:gaussian-kernel}]. To find $\{ \varepsilon_k \}$, the Shannon entropy $H(\bx_k)$ of each row of $M$ must approximately hold the following condition:
%
\begin{equation}
  \label{eq:sne-entropy}
  H(\bx_k) = \log_2 P,
\end{equation}
%
where $P$ is a measure of neighborhood at $\bx_k$ refered to as perplexity; see \rref{wattenberg2016use} for additional information about selecting perplexity and its effect on low-dimensional representations. Therefore, the optimization task is to adjust $\{ \varepsilon_k \}$ so that $H(\bx_k)$ is roughly $\log_2 P$. After the fitting, the Markov transition matrix $M$ encodes information about relations between samples in the high-dimensional space.

%
\begin{figure}
  \includegraphics{fig/fig-crowding-problem}
  \caption{{\bf Crowding Problem.} The approach taken by $t$-distributed SNE to construct the low-dimensional transition probabilities from low-dimensional samples. The transition matrix with probabilities $p_{kl}$ between high-dimensional samples is modeled using a Gaussian kernel (blue), while its equivalent for low-dimensional samples $Q$ is given by a one-dimensional $t$-distribution (red). The difference between $M$ and $Q$ as (purple) measured by each term $d_{kl}=p_{kl}\log\ppar*{p_{kl}/q_{kl}}$ that summed give the Kullback--Leibler divergence $D_{\mathrm{KL}}=\sum_{kl}d_{kl}$. In relation to dynamical systems, positive values of $d_{kl}$ increase the propensity to group samples within metastable states. Negative values improve the separation between metastable states.}
  \label{fig:crowding}
\end{figure}
%

\subsubsection{Low-Dimensional Representation}
%
Depending on the variant of SNE, the low-dimensional Markov transition matrix can be represented by different normalized kernels $Q(\bz_k,\bz_l)$. In SNE, the transition probabilities $q_{kl}$ between the low-dimensional samples are given by the Gaussian kernel:
%
\begin{equation}
  \label{eq:q-gaussin-kernel}
  q_{kl} \sim Q(\bz_k,\bz_l) = \frac{\exp\ppar*{ -\frac{1}{\varepsilon}\| \bz_k - \bz_l \|^2 }}{\sum_n \exp\ppar*{ -\frac{1}{\varepsilon}\| \bz_k - \bz_n \|^2 }},
\end{equation}
%
where the low-dimensional samples are given by the parametric target mapping such that $\bz=\xi_{\btheta}(\bx)$. A constant value of $\varepsilon$ for every row of $Q$ is chosen.

In $t$-distributed SNE~\cite{maaten2008visualizing,maaten2009learning} (abbreviated as $t$-SNE), which is perhaps the most commonly used variant of SNE, $Q(\bz_k,\bz_l)$ is instead represented by a non-parametric $t$-distribution kernel with one degree of freedom (i.e., the Lorentz function) [\rfig{fig:crowding}]:
%
\begin{equation}
  \label{eq:q-t-kernel}
  q_{kl} \sim Q(\bz_k,\bz_l) = \frac{\ppar*{ 1 + \| \bz_k - \bz_l \|^2 }^{-1}}{\sum_n \ppar*{ 1 + \| \bz_k - \bz_n \|^2 }^{-1}},
\end{equation}
%
Therefore, in contrast to spectral embedding methods, SNE does not employ eigendecomposition but builds the transition matrix in the low-dimensional space~\cite{hinton2002stochastic,maaten2008visualizing,maaten2009learning}.

Using the $t$-distribution kernel instead of the Gaussian kernel is motivated by the {\it crowding problem}~\cite{maaten2008visualizing,maaten2009learning} due to which clusters of samples (or metastable states) are not correctly separated as the area of a low-dimensional space that is available to accommodate moderately distant samples is not nearly large enough compared to the area available to accommodate nearby samples. It is partly caused by the dimensionality curse~\cite{marimont1979nearest,assent2012clustering}. As the $t$-distribution kernel is heavy-tailed (as it is an infinite mixture of Gaussians with different variances), the separation of the clusters is better [\rfig{fig:crowding}]. The choice of a one-dimensional $t$-distribution is because $(1+\pnrm{\bz_k,\bz_l}^2)^{-1}$ approaches an inverse square law for large pairwise distances in the low-dimensional representation~\cite{maaten2008visualizing}.

%
\begin{figure*}
  \includegraphics[width=0.75\textwidth]{fig/fig-neural-network}
  \caption{{\bf Parametric Target Mapping}. Schematic depiction of learning parameters $\mathbf{\theta}$ for a parametric target mapping represented by a neural network. The backpropagation procedure estimates errors of the parametric target mapping $\nabla_{\mathbf{\theta}}\xi_{\mathbf{\theta}}(\bx)$ which are used to correct the parameters so that the Kullback--Leibler divergence (or any other divergence) calculated from the Markov matrix $M(\bx_k,\bx_l)$ denoting transition probabilities between high-dimensional samples and the low-dimensional transition matrix $Q(\bz_k,\bz_l)$ decreases to zero. A minimum value of the Kullback--Leibler divergence indicates that the relations between samples in the configuration space and the CV space are preserved.}
  \label{fig:parametric-target-mapping}
\end{figure*}
%

\subsubsection{Learning Algorithm}
%
In constructing the target mappings into the low-dimensional manifold, we want a negligible difference between the high-dimensional and low-dimensional Markov transition matrices, $M$ and $Q$. Many statistical divergences allow computing differences between these pairwise probability distributions. SNE uses the Kullback--Leibler divergence~\cite{kullback1951information} reformulated to compare Markov transition matrices row by row~\cite{rached2004kullback}:
%
\begin{align}
  \label{eq:kl-divergence}
  D_{\mathrm{KL}} = \sum_{kl} p_{kl} \log\ppar*{\frac{p_{kl}}{q_{kl}}} =
    \sum_{kl} p_{kl} \log p_{kl} -
    \sum_{kl} p_{kl} \log q_{kl},
\end{align}
%
where the summation goes over every pair of $k$ and $l$. As the first term in \req{eq:kl-divergence} is constant, it can be excluded from the minimization. The Kullback--Leibler divergence has larger than 0 and equal to 0 only for $p_{kl} = q_{kl}$ every pair $(k,l)$. Furthermore, it is not symmetric. Many other divergences can be used to compare $M$ and $Q$, e.g., the symmetrized Kullback--Leibler or the Jensen--Shannon divergences.

%
\begin{algorithm}
  \label{alg:parametric-sne}
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
  \Input{Data set $\{\bx_k\}_{k=1}^K$, perplexity $P$.}
  \Output{Target mapping $\xi_{\btheta}(\bx)=\pset{\xi_k(\bx; \btheta)}_{k=1}^d$.}
  \begin{enumerate}[leftmargin=0cm]
    \item Sample landmarks if necessary.
    \item Initiate parameters $\btheta$ for the target mapping $\xi_{\btheta}$.
    \item Iterate over training epoches:
      \begin{enumerate}[leftmargin=0.6cm]
        \item Map the samples to their reduced representation: $\xi_{\btheta}: \bx_l \mapsto \bz_l$.
        \item Iterate over data batches:
      \begin{enumerate}[leftmargin=0.6cm]
        \item Calculate the Markov transition matrices $M$ (dependent on perplexity $P$) and $Q$ (dependend on $\btheta$).
        \item Update optimization step with the loss given by the divergence for the updated parameters $\btheta$.
      \end{enumerate}
    \end{enumerate}
  \end{enumerate}
  \caption{Parametric Stochastic Neighbor Embedding}
\end{algorithm}
%

Training a neural network representing the parametric target mapping is done iteratively and can be performed using many stochastic gradient descent algorithms, e.g., Adam~\cite{kingma2014adam}. Backpropagation estimates errors of the parametric target mapping $\nabla_{\mathbf{\theta}}\xi_{\mathbf{\theta}}(\bx)$ which are used to correct the parameters so that the Kullback--Leibler divergence (or any other divergence) calculated from the Markov matrix $M(\bx_k,\bx_l)$ transition probabilities between high-dimensional samples and the low-dimensional transition matrix $Q(\bz_k,\bz_l)$ decreases to zero. A minimum value of the Kullback--Leibler divergence indicates that the relations between samples in the configuration space and the CV space are preserved [\rfig{fig:parametric-target-mapping}].

\subsubsection{Applications}
%
For atomistic simulations, SNE and its extensions have been applied to learning CVs, clustering metastable states, and ligand-protein interactions~\cite{rydzewski2016machine,rydzewski2017ligand,zhou2018t,romero2019mechanism,spiwok2020time,krishnamoorthy2020reactive,shires2021visualizing,rydzewski2021multiscale,decherchi2021molecular,nicoli2022classification}. $t$-SNE is implemented in the \texttt{sklearn} library~\cite{sklearn}.

\subsection{Uniform Manifold Approximation and Projection (UMAP)}
\label{sec:umap}
%
SNE inspired many manifold learning methods~\cite{yang2009heavy,venna2010information,narayan2015alpha,de2018perplexity,belkina2019automated,linderman2019fast,kobak2019art}. This variety mainly stems from the fact that SNE comprises several steps which can be solved differently. Here, however, we discuss only methods used to analyze data from atomistic simulations. An example of such a dimensionality reduction approach is UMAP introduced by McInnes~\cite{mcinnes2018umap,sainburg2021parametric}. From the conceptual point of view, UMAP and SNE are very similar; however, the main advantage of UMAP over SNE is the reduction of computational cost and better preservation of global data structure~\footnote{A.~Coenen, A.~Pearce, Google PAIR \url{https://pair-code.github.io/understanding-umap/}}.

\subsubsection{Unnormalized High-Dimensional Transition Matrix}
%
In contrast to SNE, UMAP uses an \emph{unnormalized} transition matrix to represent high-dimensional space. The lack of normalization of the transition probability matrix in UMAP is motivated mainly by reducing the computational cost related to the normalization. The Markov transition probability matrix is:
%
\begin{equation}
  \label{eq:umap-markov}
  p_{kl} \sim M(\bx_k,\bx_l) = \exp\ppar*{-\frac{1}{\varepsilon_k} \ppar{\| \bx_k - \bx_l \|^2 - d_k}},
\end{equation}
%
where the distance from each $k$-th sample to its first neighbor is denoted as $d_k$ which is used to ensure that every probability $p_{kl} \le 1$. Similarly to SNE, the diagonal elements of $M$ are zeroed. As we can see, the Markov transition matrix [\req{eq:umap-markov}] used by UMAP is very similar to that used by SNE [\req{eq:gaussian-markov-matrix}]. However, instead of symmetrizing the Markov transition matrix as in SNE, UMAP uses a different symmetrization scheme~\cite{mcinnes2018umap}.

Instead of perplexity, UMAP fits $\pset{\varepsilon_k}$ parameters by optimizing the number of nearest neighbors $m$:
%
\begin{equation}
  \label{eq:umap-neighbors}
  \log_2 m = \sum_l M(\bx_k,\bx_l),
\end{equation}
%
where calculating the entropy of $k$-th row of the Markov transition matrix is not required. [Note that due to the absence of normalization in UMAP, $\log_2 m$ is not unity, which enables using \req{eq:umap-neighbors}.]

\subsubsection{Low-Dimensional Transition Probabilities}
%
UMAP must construct the probability transition matrix from low-dimensional samples, as in every manifold learning method based on divergence optimization. The probability transition matrix is the following:
%
\begin{equation}
  \label{eq:umap-q}
  q_{kl} \sim Q(\bz_k,\bz_l) \propto \ppar*{1+a\|\bz_k-\bz_l\|^{2b}}^{-1},
\end{equation}
%
where we omit the normalization factor for brevity, and $a$ and $b$ are parameters that can be found by optimization. If we take both parameters equal to unity, \req{eq:umap-q} reduces to the kernel for the low-dimensional probabilities used in SNE [\req{eq:q-t-kernel}]. In practise, UMAP finds $a$ and $b$ from nonlinear least-square optimization to the following piecewise approximation to \req{eq:umap-q}, i.e., $Q(\bz_k,\bz_l) \approx \exp\ppar*{-\|\bz_k-\bz_l\| - \delta_m}$ for $\|\bz_k-\bz_l\|>\delta_m$ and 1 otherwise.
%
\begin{figure}
  \includegraphics{fig/fig-umap-q-dist}
  \caption{{\bf Low-Dimensional Transition Probabilities $q_{kl}$.} The approach using a parametrizable $t$-distribution taken by UMAP. The distribution with $(a,b)=(1,1)$ corresponds to the $t$-distribution used in $t$-SNE. In contrast, UMAP finds $a$ and $b$ using optimization so that they match the distribution of pairwise distrances calculated from the low-dimensional samples.}
\end{figure}
%

\subsubsection{Divergence Minimization}
%
To embed samples into a low-dimensional manifold, UMAP minimizes a loss function a modification of the Kullback--Leibler divergence, $D_{\mathrm{KL2}} = D_{\mathrm{KL}}(M, Q) + D_{\mathrm{KL}}(1-M, 1-Q)$, where $M$ and $Q$ are the high-dimensional and low-dimensional transition matrices, respectively. Therefore, the loss function is given as:
%
\begin{align}
  \label{eq:umap-loss}
  D_{\mathrm{KL2}}
    = \sum_{kl} p_{kl} \log\ppar*{\frac{p_{kl}}{q_{kl}}}
    + \sum_{kl} (1-p_{kl}) \log\ppar*{\frac{1-p_{kl}}{1-q_{kl}}},
\end{align}
%
where we can see that the first term is the Kullback--Leibler divergence $D_{\mathrm{KL}}$ used in SNE [\req{eq:kl-divergence}], which provides an attractive force between samples. The second term, in contrast, provides a repulsive force between samples.

\subsubsection{Applications}
%
UMAP has been used to analyze simulation data~\cite{shires2021visualizing,trozzi2021umap,oide2022protein,le2022behavior}. Its implementation is available at \url{https://github.com/lmcinnes/umap}.

\subsection{Reweighted Stochastic Embedding}
\label{sec:rse}
%
Reweighted stochastic embedding is a recently developed framework designed to construct low-dimensional manifolds of CVs from standard and enhanced sampling atomistic simulations~\cite{rydzewski2022reweighted}. This framework expands on ideas from DMAP and parametric versions of SNE, with an additional reweighting procedure to account for constructing the Markov transition matrix based on samples from biased distributions. To such methods, we can include stochastic kinetic embedding (StKE)~\cite{zhang2018unfolding,chen2021collective} and multiscale reweighted stochastic embedding (MRSE)~\cite{rydzewski2021multiscale}.

%
\begin{algorithm}
  \label{alg:stke}
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
  \Input{Biased data set $\{\bx_k\}_{k=1}^K$, preselected $d_0$, scale parameter of the Gaussian kernel $\varepsilon$.}
  \Output{Target mapping $\xi_{\btheta}(\bx)=\pset{\xi_k(\bx; \btheta)}_{k=1}^d$.}
  \begin{enumerate}[leftmargin=0cm]
    \item Reduce the size of the learning set by sampling landmarks:
    \begin{enumerate}[leftmargin=0.6cm]
      \item Randomly select a sample $\bx_k$ and add it to the landmark set.
      \item Iterate over all samples. If a sample $\bx_l$ satisfies $\|\bx_k-\bx_l\|\geq d_0$ for any $\bx_k$ in the landmark set, add $\bx_l$ to the landmark set.
    \end{enumerate}
    \item Calculate $\varrho(\bx_l)$ with kernel density estimation for $\bx_l$ in the landmark set.
    \item Iterate over training epoches:
      \begin{enumerate}[leftmargin=0.6cm]
        \item Map the samples in the landmark set to their reduced representation: $\xi_{\btheta}: \bx_l \mapsto \bz_l$.
        \item Iterate over batches of landmarks from the landmark set:
      \begin{enumerate}[leftmargin=0.6cm]
        \item Calculate the Markov transition matrices $M$ [\req{eq:transition-stke}] and $Q$ [\req{eq:q-gaussin-kernel}].
        \item Update optimization step with the loss function given by the Kullback--Leibler divergence to get the parameters $\btheta$.
      \end{enumerate}
    \end{enumerate}
  \end{enumerate}
  \caption{Stochastic Kinetic Embedding}
\end{algorithm}
%

\subsubsection{Stochastic Kinetic Embedding (StKE)}
\label{sec:stke}
%
Stochastic kinetic embedding (StKE) aims to learn the target mapping by matching kinetics encoded in the high-dimensional and low-dimensional spaces~\cite{zhang2018unfolding}. Similarly to DMAP, the kinetics of the configuration variables is approximated by the eigenvalues and eigenvectors of the infinitesimal generator of the diffusion process [\req{eq:infgen}].

The primary assumption in constructing the unbiased Markov transition matrix from high-dimensional samples is that these samples are uniformly distributed in the configuration space. To achieve this, a sparse set of high-dimensional samples is selected using landmark sampling [\rsct{sec:landmark-sampling}] by requiring that the distance between landmarks is larger than a predefined threshold value, $\pnrm*{\bx_k - \bx_l} > d_0$.

With such a selection of landmarks, the landmark distribution is almost uniform. Then, the unbiased Markov transition matrix can be written as:
%
\begin{equation}
    M(\bx_k,\bx_l) = \frac{w(\bx_l) L(\bx_k,\bx_l)}{\sum_n w(\bx_n) L(\bx_k,\bx_n)},
    \label{eq:transition-stke}
\end{equation}
%
where the statistical weights can be approximated by the unbiased kernel density estimates $w(\bx_k) \approx \varrho(\bx_k)$~\cite{zhang2018unfolding,rydzewski2022reweighted}. An interesting property that establishes a link between StKE and maximum entropy methods is that \req{eq:transition-stke} can also be written as~\cite{rydzewski2022reweighted}:
%
\begin{equation}
  \label{eq:sqrt}
  M(\bx_k,\bx_l) = \sqrt{\frac{\varrho(\bx_l)}{\varrho(\bx_k)}} G_\varepsilon(\bx_k,\bx_l),
\end{equation}
%
where $G_\varepsilon$ is the Gaussian kernel and we take the following approximation $\varrho(\bx_k) \approx \sum_n \varrho(\bx_n) L(\bx_k,\bx_n)$. For a detailed discussion on this approximation, see \rref{rydzewski2022reweighted}.

Apart from constructing the unbiased Markov transition matrix, the remaining elements in StKE are characteristic of manifold learning methods using the parametric target mapping. Namely, the low-dimensional transition matrix $Q(\bz_k,\bz_l)$ is constructed using a Gaussian kernel such as in the standard version of SNE [\req{eq:q-gaussin-kernel}]. The parametric target mapping $\xi_{\btheta}(\bx)$ is represented as a neural network, and the algorithm is learning by the minimization of the Kullback--Leibler divergence [\req{eq:kl-divergence}].

StKE can be implemented according to Algorithm~\ref{alg:stke}, where we additionally provide an implementation that divides the learning set into data batches, as is often done for training neural networks.

\subsubsection{Multiscale Reweighted Stochastic Embedding (MRSE)}
\label{sec:mrse}
%
As Hinton and Roweis note in their work introducing SNE~\cite{hinton2002stochastic}, the Gaussian representation of the Markov transition matrix used in SNE [\rsct{sec:sne-markov}] can be extended to kernel mixtures, where instead of using a Gaussian kernel with perplexity for determining the scale parameters for each row of the Markov transition matrix, a set of perplexities can be used.

MRSE employs this approach to construct a multiscale Markov transition matrix. As a consequence, in contrast to SNE, we have a range of perplexities $\pset{P_i}$ and, therefore, a matrix of scale parameters $\pset{\boldsymbol{\varepsilon}_k} = \pset{\varepsilon_{ki}}$, where $k$ numbers the row of the Markov transition matrix and $i$ denotes perplexity index. First, we build a Gaussian \emph{mixture} as a sum over Gaussians with different values of the scale parameters:
%
\begin{equation}
  \label{eq:gaussian-mixture-kernel}
  G_{\boldsymbol{\varepsilon}_{k}}(\bx_k,\bx_l)=\sum_{i} G_{\varepsilon_{ki}}(\bx_k, \bx_l),
\end{equation}
%
where each $\varepsilon_{ki}$ associated with $\bx_k$ is estimated by fitting to the data, so the Shannon entropy of \req{eq:gaussian-kernel} is approximately $\log_2 P_i$, where $P_i$ is a perplexity value from an automatically selected range. Then, the Gaussian mixture is constructed so that each perplexity corresponds to a different spatial scale, allowing metastable states with different geometry to be characterized. In the next stage, we express the Markov transition matrix by normalizing the Gaussian mixture [\req{eq:gaussian-mixture-kernel}] over many perplexities:
%
\begin{equation}
  \label{eq:gaussian-mixture-markov}
  p_{kl} \sim M(\bx_k,\bx_l) = \frac{G_{\boldsymbol{\varepsilon}_k}(\bx_k,\bx_l)}{\sum_n G_{\boldsymbol{\varepsilon}_k}(\bx_k,\bx_n)},
\end{equation}
%
where each entry in the Markov transition matrix is the transition probability from $\bx_k$ to $\bx_l$.

%
\begin{figure}
  \includegraphics{fig/fig-ala3}
  \caption{{\bf Free-Energy Landscape of Alanine Tetrapeptide}. Free energy calculated as a function of a low-dimensional manifold constructed using MRSE. The sines and cosines of the $\Psi$ and $\Phi$ dihedral angles of alanine tetrapeptide are taken as high-dimensional configuration variables. Data taken from~\rref{rydzewski2021multiscale}.}
\end{figure}
%

In MRSE, each Gaussian term for perplexity represents a specific spatial scale of the data. Thus, MRSE can learn a multiscale representation of metastable states of different sizes. Additionally, as perplexities are selected automatically based on the size of the data set, the user does not need to specify it.

As in StKE, which makes these methods a suitable choice for atomistic simulations, is their capability to learn low-dimensional manifolds of CVs from biased data from enhanced sampling simulations. To include the information about the importance of high-dimensional samples, \req{eq:gaussian-mixture-kernel} is redefined as:
%
\begin{equation}
  \label{eq:reweighting-kernel-mrse}
  G_{\boldsymbol{\varepsilon}_k}(\bx_k,\bx_l) = r(\bx_k,\bx_l) \sum_{i} G_{{\varepsilon}_{ki}}(\bx_k, \bx_l),
\end{equation}
%
where the pairwise reweighting factor $r(\bx_k,\bx_l)$ needed to unbias the transition probabilities is defined as:
%
\begin{equation}
  \label{eq:reweighting-factor-mrse}
  r(\bx_k,\bx_l)=\sqrt{w(\bx_k) w(\bx_l)},
\end{equation}
%
where $w(\mathbf{x})$ denotes the statistical weights from a CV-based enhanced sampling method, e.g., as in \req{eq:weight}. The reweighting factor written as a geometric mean between two statistical weights can be justified because the bias potential is additive. A geometric mean is appropriate to preserve this relation. Similar reweighting procedures have also been used~\cite{zheng2013molecular,banisch2020diffusion,trstanova2020local}.

The remaining ingredients of MRSE are similar to StKE. The low-dimensional transition matrix is represented by a non-parametric $t$-distribution as in $t$-SNE. The target mapping is expressed as a neural network that adjusts the parameters of the target mapping so that the Kullback--Leibler divergence is minimal.

\subsubsection{Biasing Manifolds}
%
A valuable advantage of the parametric target mapping is that the high-dimensional samples outside the training set can be mapped to the CV samples. Adding to this, the fact that the derivative of the target mapping with respect to the microscopic coordinates $\bx$ can be estimated through backpropagation when using a neural network enables such target mapping in enhanced sampling simulations for biasing. The additional biasing force acting on the microscopic coordinates at time $t$ is:
%
\begin{equation}
  \label{eq:biasing-force}
  F(t) = - {\frac{\partial\xi_{\btheta}(\bx)}{\partial\bx}\frac{\partial V(\bz, t)}{\partial\bz}},
\end{equation}
%
where the second partial derivative is calculated from the bias potential $V$ acting in the CV space defined by the target mapping. Additionally, the target mapping can be recomputed during the simulation, which enables iterative improvement of the estimated CVs. This is particularly helpful in insufficient sampling conditions.

\subsubsection{Applications}
%
MRSE and StKE are recent methods and have been used so far to construct CVs~\cite{zhang2018unfolding,rydzewski2021multiscale,rydzewski2022reweighted}. MRSE is implemented in an additional module called \texttt{lowlearner}~\cite{rydzewski2021multiscale} in a development version of the open-source PLUMED library~\cite{plumed,plumed-nest} (DOI: \url{https://doi.org/10.5281/zenodo.4756093}).

\section{Conclusions}
\label{sec:conclusions}
%
This review mainly focuses on manifold learning for dynamical systems based on constructing Markov transition matrices from high-dimensional samples. Our review is by no means exhaustive; for readers more interested in concepts behind a broader class of dimensionality reduction methods, we can refer to many introductions to the subject, either from a machine learning perspective~\cite{borg2005modern,lee2007nonlinear,van2009dimensionality,abdi2010principal,ma2012manifold,izenman2012introduction} or directly related to simulations~\cite{noe2017collective,sittel2018perspective,wang2020machine,noe2020machine,glielmo2021unsupervised}. An essential ongoing methodological development in manifold learning is constructing reduced representations directly from data sampled from biased probability distributions, as in enhanced sampling simulations. This topic is critical if extensive sampling is required to reach experimental timescales where many rare processes occur, such as conformational changes or glassy transitions. Recent advances in manifold learning and enhanced sampling methods may push the field of atomistic simulations in a new direction.

\section*{Acknowledgements}
%
J.R. acknowledges funding from the Polish Science Foundation (START), the National Science Center in Poland (Sonata 2021/43/D/ST4/00920, ``Statistical Learning of Slow Collective Variables from Atomistic Simulations''), and the Ministry of Science and Higher Education in Poland. O.V. acknowledges the support of University of North Texas Startup Funding. M.C. acknowledges the Purdue startup funding.

\bibliography{main.bib}

\end{document}
