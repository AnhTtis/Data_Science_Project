% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{graphicx}
\graphicspath{{Figure/}}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{subfigure}
\usepackage[noend]{algpseudocode}
\newcommand{\myfont}{\fontsize{6.7pt}{\baselineskip}\selectfont}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Deep RL with Hierarchical Action Exploration for Dialogue Generation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Itsugun Cho$^1$ \quad Ryota Takahashi$^1$ \quad Yusaku Yanase$^1$ \quad Hiroaki Saito$^1$ \\[3pt]
Keio University, Japan$^1$ \\
{\tt \{choitsugun, ryota.0226.tokky, y.y32851107\}@keio.jp}\\ \tt hxs@ics.keio.ac.jp\\}

\begin{document}
\maketitle
\begin{abstract}
Conventionally, since the natural language action space is astronomical, approximate dynamic programming applied to dialogue generation involves policy improvement with action sampling. However, such a practice is inefficient for reinforcement learning (RL) because the eligible (high action value) responses are very sparse, and the greedy policy sustained by the random sampling is flabby. This paper shows that the performance of dialogue policy positively correlated with sampling size by theoretical and experimental. We introduce a novel dual-granularity Q-function to alleviate this limitation by exploring the most promising response category to intervene in the sampling. It extracts the actions following the grained hierarchy, which can achieve the optimum with fewer policy iterations. Our approach learns in the way of offline RL from multiple reward functions designed to recognize human emotional details. Empirical studies demonstrate that our algorithm outperforms the baseline methods. Further verification presents that ours can generate responses with higher expected rewards and controllability.
\end{abstract}

\section{Introduction}
An intelligent dialogue agent is required to respond fluently and naturally while being endowed with “forward-looking” to provide a satisfactory user experience. A predominant approach to training agents is to optimize the maximum likelihood estimation (MLE) objective for the probability distribution of responses. However, this supervised technique is insufficient to learn a long-term behavior since the corpus often contains suboptimal dialogues, and MLE cannot model the future direction of the conversation. If we instead view the open-domain dialogue as a control problem, frameworks such as reinforcement learning (RL) could allow agents automatically adjust policy concerning the pre-defined appraisal functions via a trial-and-error process.

Recent work in RL for dialogue generation is well summarized by \citet{lone2022self}. Most prior works are built on the actor-critic framework to train a dialogue agent, which optimal the policy within the support of generated $N$ possible actions from the agent to maximize the action-value function (i.e., Q-function). Although this self-behavior cloning does not rely on the policy gradients, thus avoiding the issue of diverging from human language, it suffers from slow improvement and often falls into a trivial local optimum. We argue that the actions which can make the Q-function produce a higher value (i.e., Q-value) in a given state will be similar at an elevated abstraction rank. As in a conversation about favorite food, responses about steak are more likely to get higher cumulative expected rewards than those about business. If we apprehend which abstract category of actions can obtain a higher Q-value, then generating responses of that category for the greedy policy will make training more efficient. To this end, we propose a dual-granularity Q-function to evaluate the utility carried by an action under different levels. Specifically, it contains a coarse-grained Q-function by category-represented responses aiming to lock the optimal category and a fine-grained Q-function by token-represented responses striving to extract the optimal action. In this way, the infinite action space is divided into several blocks at the high-level abstraction and thus can ergodic entire action space to adapt policy on the fly. Since RL requires many costly interactions with the environment (i.e., real users), we applied offline RL to our algorithm, which leverages the previously-collected dataset DailyDialog \cite{li2017dailydialog} for policy training. Moreover, inspired by the psychology of human conversation, four reward functions are devised to improve the agent’s ability to engage in natural dialogue. Experimental results demonstrate that four state-of-the-art dialogue models achieve a significant performance improvement by training in our algorithm. The controllability and effectiveness of our approach are clarified in the discussion. Our main contributions in this work are two-fold:\\
(1) To the best of our knowledge, this is the first attempt to carry out offline RL by different-grained representations of natural language, which provides a unified algorithmic template for large action space tasks, not limited to dialogue generation.\\
(2) The quantitative and qualitative empirical verifications established the dialogue agent trained by our approach has a high level of real-world generalization and convincingness.

\section{Methodology}
\subsection{Preliminaries}
Given a Markov decision process represented by a tuple $M=(S,A,T,R,\gamma)$, where $S$ is the state space, $A$ is the action space, $T$ is the state transition function, $R$ is the reward function, and $\gamma\in(0,1)$ is a discount factor. In the dialogue setting, the agent observes a context $s$, executes its policy $\pi$ by generating a response $a$ according to $\pi(a|s)$, transitions to a new context $s^{\prime}$, and receives a reward $r=R(s,a)$. The goal is to learn separate $\pi$ to maximize cumulative reward from a pre-collected dataset $\mathcal{D}$, which consists of multiple $(s,a,r,s^{\prime})$ pairs produced under a potential behavior policy $\pi_\beta$. Hence prior works typically rely on the actor-critic style that alternates between fitting Q-function by the policy evaluation based on approximate dynamic programming (i.e., iterating the Bellman operator via minimizing the temporal difference error) and improving $\pi$ by updating it towards responses that maximize the expected Q-value.
\begin{equation}
\begin{split}
&{\rm Evaluation}:Q\leftarrow\mathop{\rm{arg\,min}}\limits_{Q}\mathbb{E}_{(s,a,r,s^{\prime})\backsim \mathcal{D}}\\
&[(r+\gamma\mathbb{E}_{a^{\prime}\backsim\pi(a^{\prime}|s^{\prime})}[Q(s^{\prime},a^{\prime})]-Q(s,a))^2]
\end{split}
\end{equation}
\begin{equation}
\begin{split}
&{\rm Improvement}:\pi\leftarrow\mathop{\rm{arg\,max}}\limits_{\pi}\mathbb{E}_{s\backsim\mathcal{D},a\backsim\pi(a|s)}\\
&\qquad\qquad\qquad\qquad[Q(s,a)] 
\end{split}
\label{2}
\end{equation}

A challenge acting offline RL is static $\mathcal{D}$ has limited coverage of $S$ and $A$, whereby $\pi$ may be biased towards out-of-distribution (OOD) actions for $\pi_\beta$ with erroneously high Q-value \cite{fujimoto2019off,kumar2020conservative,kostrikovoffline}. Therefore, we follow \citet{jang2022gpt} to employ the one-step algorithm \cite{brandfonbrener2021offline} for on-policy evaluation as shown follows, which can iterate very stably since actions are always in $\mathcal{D}$ to avoid the OOD due to distribution deviates between $\pi$ and $\pi_\beta$.
\begin{equation}
\begin{split}
&{\rm{Evaluation}}:Q\leftarrow\mathop{\rm{arg\,min}}\limits_{Q}\mathbb{E}_{(s,a,r,s^{\prime},a^{\prime})\backsim \mathcal{D}}\\
&\qquad\quad[(r+\gamma Q(s^{\prime},a^{\prime})-Q(s,a))^2]
\end{split}
\label{3}
\end{equation}
\subsection{Dual-granularity Q-function}
Traditionally, to implement the $arg\,max$ operator in Eq.(\ref{2}), a set of responses is sampled by $\pi(a|s)$, and picks the best one using the Q-function to update $\pi$. We can show that the renewed policy by more responses has a higher state value (i.e., a better performance). When the sampling size is large enough to cover the entire action space, $\pi$ can theoretically iterate to the optimum. We also show that the renewed policy by responses with a higher Q-value has a higher state value. We formalize the results in Theorem 1 and Theorem 2, respectively.\\
\textbf{Theorem 1}. Given a policy $\pi$ and the number of sampled actions $L$, if we update the new policy by:
\begin{equation}
\forall s, \,\pi^{\prime}_L=\mathop{\rm{arg\,max}}\limits_{a\in \{a_i\}^L_{i=1}\backsim\pi(a|s)}Q^{\pi}(s,a)
\nonumber
\end{equation}
then for any $N$, $M$ such that $N\ge M\ge 1$, $\forall s$, $V^{\pi^{\prime}_N}(s)\ge V^{\pi^{\prime}_M}(s)$ always holds.\\
\textbf{Theorem 2}. Given the policy $\pi_\alpha$, $\pi_\beta$, and $\pi$, s.t. $\mathbb{E}_{a\backsim\pi_\alpha(a|s)}[Q^{\pi}(s,a)]\ge\mathbb{E}_{a\backsim\pi_\beta(a|s)}[Q^{\pi}(s,a)]$, if the number of sampled actions is $L$, and we update the new policy by:
\begin{equation}
\begin{split}
&\forall s, \,\pi^{\prime}_1=\mathop{\rm{arg\,max}}\limits_{a\in \{a_i\}^L_{i=1}\backsim\pi_\alpha(a|s)}Q^{\pi}(s,a)\\
&\forall s, \,\pi^{\prime}_2=\mathop{\rm{arg\,max}}\limits_{a\in \{a_i\}^L_{i=1}\backsim\pi_\beta(a|s)}Q^{\pi}(s,a)
\end{split}
\nonumber
\end{equation}
then $\forall s$, $V^{\pi^{\prime}_1}(s)\ge V^{\pi^{\prime}_2}(s)$ always holds.

The details about the proofs are in Appendix A. Since it is impractical to exhaust all possible responses, we focus on constructing the sampling process more organized instead of randomly to make the responses with higher Q-value and learn an agent with better performance under the same sample size. We call response $a$ the fine-grained action and its category representation $\bar{a}$ the coarse-grained action, which $\bar{a}$ belongs to a finite set of categories $\{\bar{a}_i\}^N_{i=1}$ and can obtain by $F (\bar{a}|a)$, where $F$ is a classifier. The coarse-grained Q-function searches the category $\bar{a}^\ast$ with the highest Q-value from $\{\bar{a}_i\}^N_{i=1}$, which policy evaluation is given by:
\begin{equation}
\begin{split}
&\phi\leftarrow\mathop{\rm{arg\,min}}\limits_{\phi}\mathbb{E}_{(s,\bar{a},r,s^{\prime},\bar{a}^{\prime})\backsim\mathcal{D}^c}\\
&[(r+\gamma Q_{\hat{\phi}}(s^{\prime},\bar{a}^{\prime})-Q_\phi(s,\bar{a}))^2]
\end{split}
\end{equation}
where $\phi$ and $\hat{\phi}$ are the parameters of the critic and target networks \cite{mnih2015human}, and the same is true for $\theta$ and $\hat{\theta}$ in Eq.(\ref{5}). $\mathcal{D}^c$ is a new dataset built on $\mathcal{D}$ that replaced fine-grained actions with coarse-grained actions. Then, a fine-tuned control generator with policy $\pi_\psi$ generates a set of responses $\{a_i\}^N_{i=1}$ under the specified category according to $\pi_\psi(a|s,\bar{a}^\ast)$. The fine-grained Q-function selects the response $a^\ast$ with the highest Q-value from $\{a_i\}^N_{i=1}$, which policy evaluation is given by:
\begin{equation}
\begin{split}
&\theta\leftarrow\mathop{\rm{arg\,min}}\limits_{\theta}\mathbb{E}_{(s,a,r,s^{\prime},a^{\prime})\backsim\mathcal{D}}\\
&[(r+\gamma Q_{\hat{\theta}}(s^{\prime},a^{\prime})-Q_\theta(s,a))^2]
\end{split}
\label{5}
\end{equation}
Finally, the agent with policy $\pi_\mu$ is optimized by the following. The pseudocode of our algorithm is shown in Appendix B.
\begin{equation}
\mu\leftarrow\mathop{\rm{arg\,min}}\limits_{\mu}\mathbb{E}_{s\backsim\mathcal{D}}[-{\rm log}\,\pi_\mu(a^\ast|s)]
\end{equation}

\subsection{Rewards}
We desire to learn an agent intrinsically motivated to enrich the interactive content by capturing emotional cues in human reactions. Hence, we conceived four reward functions to assess how empathy the generated response is to the conversation. (1) The cosine similarity between the agent’s response and dull responses. An expression that lacks emotional engagement may limit the development of dialogue. (2) The outpouring of surprise emotion. It benefits to build trust and hold the partner’s attention throughout the conversation \cite{shum2018eliza}. (3) The length of response. It is a critical signal of engagement in conversation \cite{zhou2020design}. (4) The asking questions. It is an active listening skill that links to conversation management and responsiveness \cite{bodie2012listening}. The totaled reward was delivered to the Q-functions for the policy evaluation, with details in Appendix C.

\section{Experiments}
\begin{table*}
\centering
\begin{tabular}{cccccccccc}
\toprule
\multirow{2}[2]{*}{\textbf {Agent}}&
\multirow{2}[2]{*}{\textbf {Training Method}}&
\multicolumn{4}{c}{\textbf {Dataset-based}}&
\multicolumn{4}{c}{\textbf {Simulator-based}}\cr
\cmidrule(lr){3-6}\cmidrule(lr){7-10}
&&CS $\downarrow$&SE&RL&AQ&CS $\downarrow$&SE&RL&AQ\cr
\toprule
\multirow{3}[2]{*}{\textbf {GPT-2}}
&MLE      &0.712 &0.082 &10.396 &0.308 &0.685 &0.146 &11.276 &0.390\cr
&Standard &0.645 &0.126 &13.020 &0.550 &0.644 &0.206 &13.778 &0.526\cr
&Ours     &\textbf{0.596} &\textbf{0.191} &\textbf{14.463} &\textbf{0.555} &\textbf{0.597} &\textbf{0.238} &\textbf{15.636} &\textbf{0.566}\cr
\midrule
\multirow{3}[2]{*}{\textbf {DialoGPT}}
&MLE      &0.714 &0.069 &9.761  &0.345 &0.687 &0.142 &10.838 &0.492\cr
&Standard &0.645 &0.142 &12.182 &0.579 &0.654 &0.206 &13.772 &0.538\cr
&Ours     &\textbf{0.598} &\textbf{0.171} &\textbf{13.055} &\textbf{0.586} &\textbf{0.588} &\textbf{0.240} &\textbf{14.466} &\textbf{0.604}\cr
\midrule
\multirow{3}[2]{*}{\textbf {T5}}
&MLE      &0.720 &0.063  &9.704 &0.316 &0.651 &0.088 &10.242 &0.396\cr
&Standard &0.621 &0.147 &13.291 &0.532 &0.605 &0.224 &13.676 &0.510\cr
&Ours     &\textbf{0.567} &\textbf{0.202} &\textbf{14.834} &\textbf{0.565} &\textbf{0.553} &\textbf{0.268} &\textbf{15.134} &\textbf{0.552}\cr
\midrule
\multirow{3}[2]{*}{\textbf {GODEL}}
&MLE      &0.718 &0.064  &9.507 &0.318 &0.689 &0.112 &10.132 &0.414\cr
&Standard &0.625 &0.165 &13.553 &0.529 &0.615 &0.235 &13.108 &0.614\cr
&Ours     &\textbf{0.571} &\textbf{0.232} &\textbf{15.272} &\textbf{0.557} &\textbf{0.571} &\textbf{0.258} &\textbf{14.608} &\textbf{0.628}\cr
\bottomrule
\end{tabular}
\caption{\label{t1}For Standard and Ours, we use $N=5$ for the number of candidate responses $\{a_i\}^N_{i=1}$. We ran 1000 dialogues of 5-turn between the simulator and each method for simulator-based evaluation. The statistical test showed the differences are significant with a p-value < 0.05. The best score in each metric is in bold.}
\end{table*}
\subsection{Corpus}
We evaluated our approach on the DailyDialog dataset, which was crawled from websites that serve English dialogue in daily life. This dataset is human-rewritten and manually labeled with communication intention and emotion. We referred to its labels of act and emotion for assigning the reward (2) and (4) designed in Section 2.3 to each response. This dataset contains 11,118 / 1,000 / 1,000 multi-turn dialogues for train / test / dev, and we used the set of train and dev for Q-function training and agents fine-tuning, the set of test for evaluation and discussion.

\subsection{Agents}
The following four state-of-the-art generative methods were considered to act as agents in our experiments. \textbf{GPT-2} is an unsupervised autoregressive language model for the textual generation proposed by \citet{radford2019language}. \textbf{DialoGPT} is a pre-trained dialogue model proposed by \citet{zhang2020dialogpt}. This model is based on GPT-2, using the Reddit comments dataset. \textbf{T5} is a unified framework proposed by \citet{raffel2020exploring} that converts all text-based language tasks into a text-to-text format via the transfer learning technique. \textbf{GODEL} is a pre-trained dialogue model proposed by \citet{peng2022godel}. This model is based on T5, using the Reddit discussion dataset. All the agents used the base version of the corresponding pre-trained model.

\subsection{Implementation}
The critic and target networks are the BERT models \cite{kenton2019bert} with a fully connected head on top. The classifier\footnote{It is available in the official repository of Cardiff NLP\,:\\\url{https://huggingface.co/cardiffnlp}\\
Note that many other ways for the category decision are also feasible, such as clustering, LDA, etc. Since TweetTopic and DailyDialog were both crawled from social networking, we consider this classifier is more appropriate for our test.} is a RoBERTa model fine-tuned on the TweetTopic dataset \cite{dimosthenis-etal-2022-twitter}, which divides the responses into 19 pre-defined topics as its action category. The control generator is initialized by the corresponding agent. To drive the control generator to respond for the specified category, we append the category information at the beginning of the input for GPT-2 and DialoGPT during the learning and inference, which the injection scheme followed \citet{cho-etal-2022-personalized}. And for T5 and GODEL, we add the category information into the task prefix of the T5 framework during the learning and inference. The task prefix was set as: “Instruction: given a dialog context, you need to response related to <category information>.” Our implementation was based on PyTorch \cite{paszke2019pytorch} and HuggingFace libraries \cite{wolf2019huggingface}. All agents and the control generator were fine-tuned before executing RL. Thus early stopping was used to determine the number of training iterations of the best checkpoint, and the patience times were set as 5. The batch size was fixed at 32. The Adam algorithm \cite{kingma2015adam} was utilized for optimization with a learning rate of 2.6e-5, and a warmup step of 6000. The control generator constructs the actions by multinomial sampling with a temperature setting of 1.5 to collect diverse responses. The target networks update rate set as 2.4e-5. The synchronized interval for the target networks was 30 steps. The discount factor was set as 0.5. We consider the network to have converged and terminated iterating when the change in the loss for ten consecutive epochs of the target network is less than 0.01.

\subsection{Evaluation}
\begin{figure*}[t]
\centering
\subfigure[CS]{
\includegraphics[width=7.5cm]{./CS.png}
}
\subfigure[SE]{
\includegraphics[width=7.5cm]{./SE.png}
}
\subfigure[RL]{
\includegraphics[width=7.5cm]{./RL.png}
}
\subfigure[AQ]{
\includegraphics[width=7.5cm]{./AQ.png}
}
\caption{\label{g1}The evolution of the agent's performance for each metric with the sampling size. The scale for the X axis is a multiple of 4, and 0 is the MLE without RL. Bands show half a standard deviation.}
\end{figure*}
\subsubsection{Automatic Metrics}
We take the perspective of rewards designed in Section 2.3 to automatic evaluation. In particular, for the view of reward (2), we count the number of generated responses that contain a word that expresses surprise (i.e., Aha, Oh, Wow, Whoa, Gee, Really?, Amazing) by conservative string-matching heuristics. For the view of reward (4), we count the number of generated responses that contain a question word or a question mark. We denote CS, SE, RL, and AQ as Cosine Similarity, Surprise Emotion, Response Length, and Asking Questions, respectively. In our evaluation and discussion, each metric is in units of one response, and the scores are acquired by taking the average over all samples.

\subsubsection{Results}
We evaluated the performances of our approach on the testing set as dataset-based evaluation, compared with baseline methods that include standard offline RL algorithm (i.e., Eq.(\ref{3}) and Eq.(\ref{2}), where Eq.(\ref{3}) is equivalent to our fine-grained Q-function) and MLE without RL. We also conducted a simulator-based evaluation by interacting with the user simulator Blenderbot \cite{roller2021recipes} to assess the performances of the agents in a long-term dialogue. Table \ref{t1} reports the automatic evaluation results. We can see that, compared to MLE, the standard method achieves better performance due to the policy improvement brought by RL. Our approach was superior over the two baselines in all metrics, which demonstrates the effectiveness of our proposed dual-granularity Q-function. The qualitative analysis with generated dialogue examples can be found in Appendix D.

\section{Discussion}
Firstly, we confirmed the validity of Theorem 1 in the experiment to further exhibit that sampling size is a constraint for policy performance. Figure \ref{g1} shows the performance of the GPT-2 agents of the standard method and ours under the simulator-based evaluation obtained by setting different numbers of response candidates for learning. We discovered that the performance tends to enhance with the boost of sampling size, and this result is in line with our theoretical derivation. Ours outperforms the standard method under the same number of actions for improvement, which indicates our algorithm can iterate policy more efficiently.

Then, we further verified whether our approach satisfies the hypothesis $\mathbb{E}_{a\backsim\pi_\alpha(a|s)}[Q^{\pi}(s,a)]\ge\mathbb{E}_{a\backsim\pi_\beta(a|s)}[Q^{\pi}(s,a)]$ in Theorem 2. Our control generator that relies on the coarse-grained Q-function to provide the optimal category can be seen as $\pi_\alpha$, the agent learned by the standard method can be considered as $\pi_\beta$, and $Q^{\pi}$ is the fine-grained Q-function. The expected value was approximated by taking the average for the Q-value of each sample. We found that $\mathbb{E}_{a\backsim\pi_\alpha(a|s)}[Q^{\pi}(s,a)]$ increases by 8.76\%, 8.71\%, 10.14\%, and 9.61\% compared to $\mathbb{E}_{a\backsim\pi_\beta(a|s)}[Q^{\pi}(s,a)]$ under the agents played by GPT-2, DialoGPT, T5, and GODEL, respectively. That indicates the categories selected by the coarse-grained Q-function can produce the responses with a higher Q-value.

Finally, we wanted to check if our control generator can generate responses for the specified category. For each $\bar{a}\in\{\bar{a}_i\}^{N=19}_{i=1}$, we used $\pi_\psi(a|s,\bar{a})$ to obtain a response and operated $F (\bar{a}|a)$ to determine if this response belongs to $\bar{a}$. The results of the empirical study reveal that the ratio of correctly responding to a given category for the control generators performed by GPT-2, DialoGPT, T5, and GODEL are 28.16\%, 29.89\%, 35.42\%, and 36.37\%, respectively. That is significantly different from the correct ratio corresponding to randomly generated over 19 categories (i,e., 1/19 $\approx$ 5.26\%).

\section{Related Work}
Since the RL for dialogue requires multiple steps of pricey human interaction, several prior works have updated the agent's policy by the self-play method or the interaction with simulators \cite{li2016deep,shah2018bootstrapping,peng2018deep,liu2020you}. However, these online RL methods suffer from the issue of diverging from human language \cite{lewis2017deal,zhao2019rethinking,jang2020bayes}. On the other hand, Offline RL \cite{fujimoto2019off,kumar2020conservative,brandfonbrener2021offline,kostrikovoffline} removes all need for environment interaction or user simulators, instead operating purely on static datasets of prior human interaction. There are many closely related works \cite{jaques2019way,jaques2020human,snell2022offline,cohen2022dynamic,verma2022chai,jang2022gpt} based on offline RL that policy improvement via behavior cloning of self-generated utterances, which inherits the ability of pre-trained language models to generate human-like responses. In RL parlance, such methods could be considered policy extraction with approximate dynamic programming. Nevertheless, unlike RL tasks where the actions are finite such as Atari games \cite{mnih2015human}, that hard to ergodic all probability space in the dialogue setting, thus aforementioned are suboptimal compared to full Q-learning. Our dual-granularity Q-function focuses on the more structured action choices to carry out policy improvement effectively.

\section{Conclusion and Future Work}
This paper presented a dual-granularity Q-function for mitigating the suboptimal policy improvement due to the hard-to-traverse action space in RL, and we applied our method to the dialogue generation task. The theoretical and experimental results demonstrate that our algorithm is dependable and achieved a significant performance boost for the dialogue agent. In the future, we plan to design more types of abstract categories (e.g., based on sentence embedding) for actions, which endeavor to not only let the coarse-grained Q-function takes care of content level but also consider utterance structure and the way of expression. We would further explore the correlation between the number of action categories and policy improvement. Eventually, we plan to extend our algorithm to other NLP tasks in online RL to confirm our universality.

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix
\section{Appendix}
\subsection*{Proofs of Theorem}
See Preliminary Derivation, Proof of Theorem 1, and Proof of Theorem 2 on the next page.
\begin{figure*}
\begin{equation}
\begin{split}
&{\rm \textbf{Preliminary Derivation}}:\\
&\mathbb{E}_{\pi^{\prime}}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s,a)]\\
&=\mathbb{E}_{\pi^{\prime}}[R(s_t,\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s_t,a))+\gamma R(s_{t+1},a_{t+1})+\gamma^2 R(s_{t+2},a_{t+2})+...|s_t=s,\{a_n\backsim\pi(a|s_n)\}_{n=t+1}^T]\\
&=\mathbb{E}_{\pi^{\prime}}[R(s_t,\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s_t,a))+\gamma V^{\pi}(s_{t+1})|s_t=s,a\backsim\pi^{\prime}(a|s_t)]\\\\
&\textbf{Proof\;of\;Theorem 1.}\\
&{\rm Premise}:\quad\forall s, \,\pi^{\prime}_L(\cdot|s)=\mathop{\rm{arg\,max}}\limits_{a\in \{a_i\}^L_{i=1}\backsim\pi(a|s)}Q^{\pi}(s,a)\\
&{\rm Lemma}\,:\quad{\rm if}\;N\ge M\ge 1,\;{\rm then}\;\mathbb{E}_{\pi}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^N_{i=1}}Q^{\pi}(s,a)]\ge\mathbb{E}_{\pi}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^M_{i=1}}Q^{\pi}(s,a)]\\
&V^{\pi^{\prime}_N}(s)\\
&=\mathbb{E}_{\pi^{\prime}_N}[R(s_t,a_{t}\backsim\pi^{\prime}_N(a|s_t))+\gamma R(s_{t+1},a_{t+1}\backsim\pi^{\prime}_N(a|s_{t+1}))+...|s_t=s]\\
&=\mathbb{E}_{\pi}[R(s_t,\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^N_{i=1}}Q^{\pi}(s_t,a))+\gamma R(s_{t+1},\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^N_{i=1}}Q^{\pi}(s_{t+1},a))+...|s_t=s]\\
&=\mathbb{E}_{\pi}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^N_{i=1}}Q^{\pi}(s,a)-\gamma V^{\pi}(s_{t+1})|s_t=s]+\gamma\mathbb{E}_{\pi}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^N_{i=1}}Q^{\pi}(s_{t+1},a)-\gamma V^{\pi}(s_{t+2})|s_t=s]+...\\
&\ge\mathbb{E}_{\pi}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^M_{i=1}}Q^{\pi}(s,a)-\gamma V^{\pi}(s_{t+1})|s_t=s]+\gamma\mathbb{E}_{\pi}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^M_{i=1}}Q^{\pi}(s_{t+1},a)-\gamma V^{\pi}(s_{t+2})|s_t=s]+...\\
&=\mathbb{E}_{\pi}[R(s_t,\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^M_{i=1}}Q^{\pi}(s_t,a))+\gamma R(s_{t+1},\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^M_{i=1}}Q^{\pi}(s_{t+1},a))+...|s_t=s]\\
&=\mathbb{E}_{\pi^{\prime}_M}[R(s_t,a_{t}\backsim\pi^{\prime}_M(a|s_t))+\gamma R(s_{t+1},a_{t+1}\backsim\pi^{\prime}_M(a|s_{t+1}))+...|s_t=s]=V^{\pi^{\prime}_M}(s)\\\\
&\textbf{Proof\;of\;Theorem 2.}\\
&{\rm Premise\,1}:\quad\forall s, \,\pi^{\prime}_1(\cdot|s)=\mathop{\rm{arg\,max}}\limits_{a\in \{a_i\}^L_{i=1}\backsim\pi_\alpha(a|s)}Q^{\pi}(s,a),\;\;\pi^{\prime}_2(\cdot|s)=\mathop{\rm{arg\,max}}\limits_{a\in \{a_i\}^L_{i=1}\backsim\pi_\beta(a|s)}Q^{\pi}(s,a)\\
&{\rm Premise\,2}:\quad\mathbb{E}_{a\backsim\pi_\alpha(a|s)}[Q^{\pi}(s,a)]\ge\mathbb{E}_{a\backsim\pi_\beta(a|s)}[Q^{\pi}(s,a)],\;\;\sigma^2_{a\backsim\pi_\alpha(a|s)}[Q^{\pi}(s,a)]\approx\sigma^2_{a\backsim\pi_\beta(a|s)}[Q^{\pi}(s,a)]\\
&{\rm Lemma}\;\;\;:\;\quad\because\;{\rm Premise\,2}\;\;\therefore\;\mathbb{E}_{\pi_\alpha}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s,a)]\ge\mathbb{E}_{\pi_\beta}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s,a)]\\
&V^{\pi^{\prime}_1}(s)\\
&=\mathbb{E}_{\pi^{\prime}_1}[R(s_t,a_{t}\backsim\pi^{\prime}_1(a|s_t))+\gamma R(s_{t+1},a_{t+1}\backsim\pi^{\prime}_1(a|s_{t+1}))+...|s_t=s]\\
&=\mathbb{E}_{\pi_\alpha}[R(s_t,\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s_t,a))+\gamma R(s_{t+1},\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s_{t+1},a))+...|s_t=s]\\
&=\mathbb{E}_{\pi_\alpha}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s,a)-\gamma V^{\pi}(s_{t+1})|s_t=s]+\gamma\mathbb{E}_{\pi_\alpha}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s_{t+1},a)-\gamma V^{\pi}(s_{t+2})|s_t=s]+...\\
&\ge\mathbb{E}_{\pi_\beta}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s,a)-\gamma V^{\pi}(s_{t+1})|s_t=s]+\gamma\mathbb{E}_{\pi_\beta}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s_{t+1},a)-\gamma V^{\pi}(s_{t+2})|s_t=s]+...\\
&=\mathbb{E}_{\pi_\beta}[R(s_t,\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s_t,a))+\gamma R(s_{t+1},\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s_{t+1},a))+...|s_t=s]\\
&=\mathbb{E}_{\pi^{\prime}_2}[R(s_t,a_{t}\backsim\pi^{\prime}_2(a|s_t))+\gamma R(s_{t+1},a_{t+1}\backsim\pi^{\prime}_2(a|s_{t+1}))+...|s_t=s]=V^{\pi^{\prime}_2}(s)
\end{split}
\nonumber
\end{equation}
\end{figure*}

\section{Appendix}
\subsection*{Pseudocode of Algorithm}
We summarize our method in Algorithm \ref{a1}.
\begin{algorithm*} 	
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\caption{Dual-granularity Q-function}\label{algorithm}
\begin{algorithmic}[t]
\Require
\\
The dataset $\mathcal{D}=\{\mathcal{D}_i=(s,a,r,s^{\prime},a^{\prime})\}^N_{i=1}$, classifier $F$ with action category set $\{\bar{a}_i\}^N_{i=1}$.
\Ensure
\\
The agent with policy $\pi_\mu$.
\end{algorithmic} 
\begin{algorithmic}[1]
\State {\textbf{Initialization:}}
\State{\indent Build the dataset $\mathcal{D}^c=\{\mathcal{D}_i^c=(s,\bar{a},r,s^{\prime},\bar{a}^{\prime})\}^N_{i=1}$ base on the dataset $\mathcal{D}$ using the classifier $F$.
\State\indent Initialize the critic and target network parameters $\phi$, $\hat{\phi}$, $\theta$, $\hat{\theta}$, control generator $\pi_\psi$, and agent $\pi_\mu$.
\State\indent Fine-tuning the control generator $\pi_\psi$ by the dataset $\mathcal{D}$ and the action category set $\{\bar{a}_i\}^N_{i=1}$.}
\State{\textbf{for} $i=1$ \textbf{to} until $Q_\phi$ and $Q_\theta$ converge \textbf{do}}
\State{\indent $\#$ In our implementation, the iteration stops for the Q-function that converged first,
\State\indent $\#$ and the other continues to iterate until convergence.
\\
\State\indent $\#$ Policy Evaluation on the dual-granularity Q-function.}
\State{\indent $\phi\leftarrow\mathop{\rm{arg\,min}}\limits_{\phi}(r+\gamma Q_{\hat{\phi}}(s^{\prime},\bar{a}^{\prime})-Q_\phi(s,\bar{a}))^2\quad (s,\bar{a},r,s^{\prime},\bar{a}^{\prime})=\mathcal{D}^c_i$
\State\indent $\theta\leftarrow\mathop{\rm{arg\,min}}\limits_{\theta}(r+\gamma Q_{\hat{\theta}}(s^{\prime},a^{\prime})-Q_\theta(s,a))^2\quad (s,a,r,s^{\prime},a^{\prime})=\mathcal{D}_i$
\State\indent Every $n$ step $\hat{\phi}\leftarrow \phi$, $\hat{\theta}\leftarrow \theta$.}
\State {\textbf{end for}}
\\
\State{\textbf{for} $i=1,s\in\mathcal{D}_i$ \textbf{to} until $\pi_\mu$ converge \textbf{do}}
\State{\indent $\#$ Policy Improvement for the agent.
\State\indent $\bar{a}^\ast=\mathop{\rm{arg\,max}}\limits_{\bar{a}}Q_{\phi}(s,\bar{a})\quad\bar{a}\in\{\bar{a}_i\}^N_{i=1}\quad$
\State\indent $\{a_i\}^N_{i=1}=\pi_\psi(s,\bar{a}^\ast)\qquad\#$ Note that the $N$ in rows 17 and 18 are different.
\State\indent $a^\ast=\mathop{\rm{arg\,max}}\limits_{a}Q_{\theta}(s,a)\quad a\in\{a_i\}^N_{i=1}$
\State\indent $\mu\leftarrow\mathop{\rm{arg\,min}}\limits_{\mu}-{\rm log}\,\pi_\mu(a^\ast|s)$}
\State {\textbf{end for}}
\end{algorithmic}
\label{a1}
\end{algorithm*}

\section{Appendix}
\subsection*{Details about Rewards}
(1) Cosine Similarity: We manually made a list of dull responses consisting of utterances such as “I don’t know,” etc., which are short and occur in generation models frequently. We penalize the cosine similarity between the agent’s response and the dull responses, to avoid the agent generating the dull responses. This score is computed by leveraging a state-of-the-art sentence embedding model \cite{conneau2017supervised}, and the range is 0 to 1. Although there are more ways to generate a dull response, similar expressions are likely to fall into an adjacent vector space. The user “keeps away” from the utterances in the list is thus also away from other similar dull responses.\\
(2) Surprise Emotion: Since each utterance in the DailyDialog dataset was annotated by one of six universal emotions in human beings, we directly used the emotional label of “Surprise” to allocate a reward to each sentence, which the scale is [0, 1].\\
(3)	Response Length: From the perspective of empathy, we prefer the agents to generate longer responses. The reward is defined as follows.\par\quad
\textbf{if} \quad the number of generated tokens < 5:\par\qquad\quad
reward = -0.5\par\quad
\textbf{elif} \;the number of generated tokens < 10:\par\qquad\quad
  reward = 0\par\quad
\textbf{elif} \;the number of generated tokens < 15:\par\qquad\quad
reward = 0.5\par\quad
\textbf{else}:\par\qquad\quad
reward = 1\\
(4)	Asking Questions: Each utterance in the DailyDialog dataset was also labeled as one of four dialogue act classes. We used the act label of “Questions” to allocate a reward to each sentence. The scale of this reward is [0, 1].

\section{Appendix}
\subsection*{Case Study}
Figure \ref{g2} presents the diverse generated responses by different methods over four agents. Each dialogue consists of four consecutive turns, and the user’s utterances are extracted from the testing set. It can be observed that the response generated by our algorithm and the standard method is longer than the response generated by MLE. Both the standard and our algorithm tend to ask a question, whereas ours generates more tone of voice expressing the emotion of surprise. The responses generated by our approach are more coherent than the standard method, which may be the coarse-grained Q-function tends to determine the category of actions related to the context.
\begin{figure*}[t]
\centering
\subfigure[GPT-2]{
\includegraphics[width=15.5cm]{./GPT-2.png}
}
\subfigure[DialoGPT]{
\includegraphics[width=15.5cm]{./DialGPT.png}
}
\end{figure*}
\addtocounter{figure}{-1}
\begin{figure*} 
\addtocounter{figure}{1}
\centering
\subfigure[T5]{
\includegraphics[width=15.5cm]{./T5.png}
}
\subfigure[GODEL]{
\includegraphics[width=15.5cm]{./GODEL.png}
}
\caption{\label{g2}Case Study.}
\end{figure*}

\end{document}