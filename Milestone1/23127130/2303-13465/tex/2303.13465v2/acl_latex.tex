% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{graphicx}
\graphicspath{{Figure/}}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{subfigure}
\usepackage[noend]{algpseudocode}
\newcommand{\myfont}{\fontsize{6.7pt}{\baselineskip}\selectfont}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Deep RL with Hierarchical Action Exploration for Dialogue Generation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Itsugun Cho$^1$ \quad Ryota Takahashi$^1$ \quad Yusaku Yanase$^1$ \quad Hiroaki Saito$^1$ \\[3pt]
Keio University, Japan$^1$ \\
{\tt \{choitsugun, ryota.0226.tokky, y.y32851107\}@keio.jp}\\ \tt hxs@ics.keio.ac.jp\\}

\begin{document}
\maketitle
\begin{abstract}
Traditionally, approximate dynamic programming is employed in dialogue generation with greedy policy improvement through action sampling, as the natural language action space is vast. However, this practice is inefficient for reinforcement learning (RL) due to the sparsity of eligible responses with high action values, which leads to weak improvement sustained by random sampling. This paper presents theoretical analysis and experiments showing that the dialogue policy's performance is positively correlated with the sampling size. To alleviate this limitation, we introduce a novel dual-granularity Q-function that explores the most promising response category to intervene in the sampling process. Our approach extracts actions based on a grained hierarchy, achieving the optimum with fewer policy iterations. Additionally, we use offline RL and learn from multiple reward functions designed to capture emotional nuances in human interactions. Empirical studies demonstrate that our algorithm outperforms baselines across automatic metrics and human evaluations. Further testing reveals that ours generates responses with higher expected rewards and controllability.\let\thefootnote\relax\footnotetext{Submission history: [v1] Wed, 22 Mar 2023.}
\end{abstract}

\section{Introduction}
An intelligent dialogue agent is required to respond fluently and naturally while being endowed with “forward-looking” to provide a satisfactory user experience. A predominant approach to training agents is to optimize the maximum likelihood estimation (MLE) objective for the probability distribution of responses. However, this supervised technique is insufficient to learn a long-term behavior since the corpus often contains suboptimal dialogues, and MLE cannot model the future direction of the conversation. If we instead view the open-domain dialogue as a control problem, frameworks such as RL could allow agents automatically adjust policy concerning the pre-defined appraisal functions via a trial-and-error process.

Recent work in RL for dialogue generation is well summarized by \citet{lone2022self}. Most prior works are built on the actor-critic framework to train a dialogue agent, which optimal the policy within the support of generated $N$ possible actions from the agent to maximize the action-value function (i.e., Q-function). Although this self-behavior cloning does not rely on the policy gradients, thus avoiding the issue of diverging from human language, it suffers from slow improvement and often falls into a trivial local optimum. We argue that the actions which can make the Q-function produce a higher value (i.e., Q-value) in a given state will be similar at an elevated abstraction rank. As in a conversation about favorite food, responses about steak are more likely to get higher cumulative expected rewards than those about business. If we apprehend which abstract category of actions can obtain a higher Q-value, then generating responses of that category for the greedy policy will make training more efficient. To this end, we propose a dual-granularity Q-function to evaluate the utility carried by an action under different levels. Specifically, it contains a coarse-grained Q-function by category-represented responses aiming to lock the optimal category and a fine-grained Q-function by token-represented responses striving to extract the optimal action. In this way, the infinite action space is divided into several blocks at the high-level abstraction and thus can ergodic entire action space to adapt policy on the fly. Since RL requires many costly interactions with the environment (i.e., real users), we applied offline RL to our algorithm, which leverages the previously-collected dataset DailyDialog \cite{li2017dailydialog} for policy training. Moreover, inspired by the psychology of human conversation, four reward functions are devised to improve the agent’s ability to engage in natural dialogue. Experimental results demonstrate that four state-of-the-art dialogue models achieve a significant performance improvement by training in our algorithm. The controllability and effectiveness of our approach are clarified in the discussion. Our main contributions in this work are two-fold:\\
(1) To the best of our knowledge, this is the first attempt to carry out offline RL by different-grained representations of natural language, which provides a unified algorithmic template for large action space tasks, not limited to dialogue generation.\\
(2) The quantitative and qualitative empirical verifications have established that our approach exhibits a high level of trustworthiness.

\section{Methodology}
\subsection{Preliminaries}
Given a Markov decision process represented by a tuple $M=(S,A,T,R,\gamma)$, where $S$ is the state space, $A$ is the action space, $T$ is the state transition function, $R$ is the reward function, and $\gamma\in(0,1)$ is a discount factor. In the dialogue setting, the agent observes a context $s$, executes its policy $\pi$ by generating a response $a$ according to $\pi(a|s)$, transitions to a new context $s^{\prime}$, and receives a reward $r=R(s,a)$. The goal is to learn separate $\pi$ to maximize cumulative reward from a pre-collected dataset $\mathcal{D}$, which consists of multiple $(s,a,r,s^{\prime})$ pairs produced under a potential behavior policy $\pi_\beta$. Hence prior works typically rely on the actor-critic style that alternates between fitting Q-function by the policy evaluation based on approximate dynamic programming (i.e., iterating the Bellman operator via minimizing the temporal difference error) and improving $\pi$ by updating it towards responses that maximize the expected Q-value.
\begin{equation}
\begin{split}
&{\rm Evaluation}:Q\leftarrow\mathop{\rm{arg\,min}}\limits_{Q}\mathbb{E}_{(s,a,r,s^{\prime})\backsim \mathcal{D}}\\
&[(r+\gamma\mathbb{E}_{a^{\prime}\backsim\pi(a^{\prime}|s^{\prime})}[Q(s^{\prime},a^{\prime})]-Q(s,a))^2]
\end{split}
\end{equation}
\begin{equation}
\begin{split}
&{\rm Improvement}:\pi\leftarrow\mathop{\rm{arg\,max}}\limits_{\pi}\mathbb{E}_{s\backsim\mathcal{D},a\backsim\pi(a|s)}\\
&\qquad\qquad\qquad\qquad[Q(s,a)] 
\end{split}
\label{2}
\end{equation}

A challenge acting offline RL is static $\mathcal{D}$ has limited coverage of $S$ and $A$, whereby $\pi$ may be biased towards out-of-distribution (OOD) actions for $\pi_\beta$ with erroneously high Q-value \cite{fujimoto2019off,kumar2020conservative,kostrikovoffline}. Therefore, we follow \citet{jang2022gpt} to employ the one-step algorithm \cite{brandfonbrener2021offline} for on-policy evaluation as shown follows, which can iterate very stably since actions are always in $\mathcal{D}$ to avoid the OOD due to distribution deviates between $\pi$ and $\pi_\beta$.
\begin{equation}
\begin{split}
&{\rm{Evaluation}}:Q\leftarrow\mathop{\rm{arg\,min}}\limits_{Q}\mathbb{E}_{(s,a,r,s^{\prime},a^{\prime})\backsim \mathcal{D}}\\
&\qquad\quad[(r+\gamma Q(s^{\prime},a^{\prime})-Q(s,a))^2]
\end{split}
\label{3}
\end{equation}

\subsection{Dual-granularity Q-function}
Traditionally, to implement the $arg\,max$ operator in Eq.(\ref{2}), a set of responses is sampled by $\pi(a|s)$, and picks the best one using the Q-function to update $\pi$. We can show that the renewed policy by more responses has a higher state value (i.e., a better performance). When the sampling size is large enough to cover the entire action space, $\pi$ can theoretically iterate to the optimum. We also show that the renewed policy by responses with a higher Q-value has a higher state value. We formalize the results in Theorem 1 and Theorem 2, respectively. The detailed proofs can be found in Appendix A.\\
\textbf{Theorem 1}. Given a policy $\pi$ and the number of sampled actions $L$, if we update the new policy by:
\begin{equation}
\forall s, \,\pi^{\prime}_L=\mathop{\rm{arg\,max}}\limits_{a\in \{a_i\}^L_{i=1}\backsim\pi(a|s)}Q^{\pi}(s,a)
\nonumber
\end{equation}
then for any $N$, $M$ such that $N\ge M\ge 1$, $\forall s$, $V^{\pi^{\prime}_N}(s)\ge V^{\pi^{\prime}_M}(s)$ always holds.\\
\textbf{Theorem 2}. Given the policy $\pi_\alpha$, $\pi_\beta$, and $\pi$, s.t. $\mathbb{E}_{a\backsim\pi_\alpha(a|s)}[Q^{\pi}(s,a)]\ge\mathbb{E}_{a\backsim\pi_\beta(a|s)}[Q^{\pi}(s,a)]$, if the number of sampled actions is $L$, and we update the new policy by:
\begin{equation}
\begin{split}
&\forall s, \,\pi^{\prime}_1=\mathop{\rm{arg\,max}}\limits_{a\in \{a_i\}^L_{i=1}\backsim\pi_\alpha(a|s)}Q^{\pi}(s,a)\\
&\forall s, \,\pi^{\prime}_2=\mathop{\rm{arg\,max}}\limits_{a\in \{a_i\}^L_{i=1}\backsim\pi_\beta(a|s)}Q^{\pi}(s,a)
\end{split}
\nonumber
\end{equation}
then $\forall s$, $V^{\pi^{\prime}_1}(s)\ge V^{\pi^{\prime}_2}(s)$ always holds.
\begin{algorithm*}[t]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\caption{Dual-granularity Q-function}\label{algorithm}
\begin{algorithmic}[t]
\Require
\\
The dataset $\mathcal{D}=\{\mathcal{D}_i=(s,a,r,s^{\prime},a^{\prime})\}^M_{i=1}$, the classifier with action category set $\{\bar{a}_i\}^N_{i=1}$.
\Ensure
\\
The agent with policy $\pi_\mu$.
\end{algorithmic} 
\begin{algorithmic}[1]
\State {\textbf{Initialization:}}
\State{\indent Build the dataset $\mathcal{D}^c=\{\mathcal{D}_i^c=(s,\bar{a},r,s^{\prime},\bar{a}^{\prime})\}^M_{i=1}$ base on the dataset $\mathcal{D}$ using the classifier.
\State\indent Initialize the critic and target network parameters $\phi$, $\hat{\phi}$, $\theta$, $\hat{\theta}$, control generator $\pi_\psi$, and agent $\pi_\mu$.
\State\indent Fine-tuning the control generator $\pi_\psi$ using $(s,a,\bar{a})$ triples, where $s$ and $a$ from the dataset $\mathcal{D}$.}
\State{\textbf{for} $i=1$ \textbf{to} until $Q_\phi$ and $Q_\theta$ converge \textbf{do}}
\State{\indent $\#$ In our implementation, the iteration stops for the Q-function that converged first,
\State\indent $\#$ and the other continues to iterate until convergence.
\State\indent $\#$ Policy Evaluation on the dual-granularity Q-function.}
\State{\indent $\phi\leftarrow\mathop{\rm{arg\,min}}\limits_{\phi}(r+\gamma Q_{\hat{\phi}}(s^{\prime},\bar{a}^{\prime})-Q_\phi(s,\bar{a}))^2\quad (s,\bar{a},r,s^{\prime},\bar{a}^{\prime})=\mathcal{D}^c_i$
\State\indent $\theta\leftarrow\mathop{\rm{arg\,min}}\limits_{\theta}(r+\gamma Q_{\hat{\theta}}(s^{\prime},a^{\prime})-Q_\theta(s,a))^2\quad (s,a,r,s^{\prime},a^{\prime})=\mathcal{D}_i$
\State\indent Every $n$ step $\hat{\phi}\leftarrow \phi$, $\hat{\theta}\leftarrow \theta$.}
\State {\textbf{end for}}
\State{\textbf{for} $i=1,s\in\mathcal{D}_i$ \textbf{to} until $\pi_\mu$ converge \textbf{do}}
\State{\indent $\#$ Policy Improvement for the agent.
\State\indent $\bar{a}^\ast=\mathop{\rm{arg\,max}}\limits_{\bar{a}}Q_{\phi}(s,\bar{a})\quad\bar{a}\in\{\bar{a}_i\}^{N}_{i=1}\quad$
\State\indent $\{a_i\}^L_{i=1}=\pi_\psi(a|s,\bar{a}^\ast)$
\State\indent $a^\ast=\mathop{\rm{arg\,max}}\limits_{a}Q_{\theta}(s,a)\quad a\in\{a_i\}^L_{i=1}$
\State\indent $\mu\leftarrow\mathop{\rm{arg\,min}}\limits_{\mu}-{\rm log}\,\pi_\mu(a^\ast|s)$}
\State {\textbf{end for}}
\end{algorithmic}
\label{a1}
\end{algorithm*}

Since it is impractical to exhaust all possible responses, we focus on constructing the sampling process more organized instead of randomly to make the responses with higher Q-value and learn an agent with better performance under the same sample size. We call response $a$ the fine-grained action and its category representation $\bar{a}$ the coarse-grained action, which $\bar{a}$ belongs to a finite set of categories $\{\bar{a}_i\}^N_{i=1}$ and can obtain by $\bar{a}={\rm arg\,max}_{\bar{a}_i}F(\bar{a}_i|a)$, where the function involving $F$ is a classifier. The coarse-grained Q-function searches the category $\bar{a}^\ast$ with the highest Q-value from $\{\bar{a}_i\}^N_{i=1}$, which policy evaluation is given by:
\begin{equation}
\begin{split}
&\phi\leftarrow\mathop{\rm{arg\,min}}\limits_{\phi}\mathbb{E}_{(s,\bar{a},r,s^{\prime},\bar{a}^{\prime})\backsim\mathcal{D}^c}\\
&[(r+\gamma Q_{\hat{\phi}}(s^{\prime},\bar{a}^{\prime})-Q_\phi(s,\bar{a}))^2]
\end{split}
\end{equation}
where $\phi$ and $\hat{\phi}$ are the parameters of the critic and target networks \cite{mnih2015human}, and the same is true for $\theta$ and $\hat{\theta}$ in Eq.(\ref{5}). $\mathcal{D}^c$ is a new dataset built on $\mathcal{D}$ that replaced fine-grained actions with coarse-grained actions. Then, a fine-tuned control generator with policy $\pi_\psi$ generates a set of responses $\{a_i\}^L_{i=1}$ under the specified category according to $\pi_\psi(a|s,\bar{a}^\ast)$. The fine-grained Q-function selects the response $a^\ast$ with the highest Q-value from $\{a_i\}^L_{i=1}$, which policy evaluation is given by:
\begin{equation}
\begin{split}
&\theta\leftarrow\mathop{\rm{arg\,min}}\limits_{\theta}\mathbb{E}_{(s,a,r,s^{\prime},a^{\prime})\backsim\mathcal{D}}\\
&[(r+\gamma Q_{\hat{\theta}}(s^{\prime},a^{\prime})-Q_\theta(s,a))^2]
\end{split}
\label{5}
\end{equation}
Finally, the agent with policy $\pi_\mu$ is optimized by:
\begin{equation}
\mu\leftarrow\mathop{\rm{arg\,min}}\limits_{\mu}\mathbb{E}_{s\backsim\mathcal{D}}[-{\rm log}\,\pi_\mu(a^\ast|s)]
\end{equation}
And our pseudocode is shown in Algorithm \ref{a1}.

\subsection{Rewards}
Our goal is to develop an agent intrinsically motivated to enrich the interactive content by capturing affective cues in human reactions. To achieve this, we have devised four reward functions that assess how empathetic the response is to the conversation. (1) The average cosine similarity between the agent’s response and dull responses. An expression that lacks emotional engagement may limit the development of dialogue. (2) The outpouring of surprise emotion. It benefits to build trust and hold the partner’s attention throughout the conversation \cite{shum2018eliza}. (3) The length of response. It is a critical signal of engagement in conversation \cite{zhou2020design}. (4) The asking questions. It is an active listening skill that links to conversation management and responsiveness \cite{bodie2012listening}. The total reward was used to conduct the policy evaluation for Q-functions, and more details on the scoring design can be found in Appendix B.

\section{Experiments}
\subsection{Corpus}
We evaluated our approach on the DailyDialog dataset, which was crawled from websites that serve English dialogue in daily life. This dataset is human-rewritten and manually labeled with communication intention and emotion. We referred to its labels of act and emotion for assigning the reward (2) and (4) designed in Section 2.3 to each response. This dataset contains 11,118 / 1,000 / 1,000 multi-turn dialogues for train / test / dev, and we used the set of train and dev for Q-function training and agents fine-tuning, the set of test for evaluation and discussion.

\subsection{Agents}
The following four state-of-the-art generative methods were considered to act as agents in our experiments. \textbf{GPT-2} is an unsupervised autoregressive language model for the textual generation proposed by \citet{radford2019language}. \textbf{DialoGPT} is a pre-trained dialogue model proposed by \citet{zhang2020dialogpt}. This model is based on GPT-2, using the Reddit comments dataset. \textbf{T5} is a unified framework proposed by \citet{raffel2020exploring} that converts all text-based language tasks into a text-to-text format via the transfer learning technique. \textbf{GODEL} is a pre-trained dialogue model proposed by \citet{peng2022godel}. This model is based on T5, using the Reddit discussion dataset. All the agents used the base version of the corresponding pre-trained model.

\subsection{Implementation}
The critic and target networks are the BERT models \cite{kenton2019bert} with a fully connected head on top. The classifier\footnote{It is available in the official repository of Cardiff NLP\,:\\\url{https://huggingface.co/cardiffnlp}\\
Note that many other ways for the category decision are also feasible, such as clustering, LDA, etc. Since TweetTopic and DailyDialog were both crawled from social networking, we consider this classifier is more appropriate for our test.} is a RoBERTa model fine-tuned on the TweetTopic dataset \cite{dimosthenis-etal-2022-twitter}, which divides the responses into 19 pre-defined topics as its action category. The control generator is initialized by the corresponding agent model. To drive the control generator to respond for the specified category, we append the category representation at the beginning of the input for GPT-2 and DialoGPT during the learning and inference, which the injection scheme followed \citet{cho-etal-2022-personalized}. And for T5 and GODEL, we add the category representation into the task prefix of the T5 framework during the learning and inference. The task prefix was set as: “Instruction: given a dialog context, you need to response related to <category>.” Our implementation was based on PyTorch \cite{paszke2019pytorch} and HuggingFace libraries \cite{wolf2019huggingface}. 

All agents and the control generator were fine-tuned before executing RL. Thus early stopping was used to determine the number of training iterations of the best checkpoint, and the patience time was set as 5. The batch size was fixed at 32. The Adam algorithm \cite{kingma2015adam} was utilized for optimization with a learning rate of 2.6e-5, and a warmup step of 6000. The control generator constructs the actions by multinomial sampling with a temperature setting of 1.5 to collect diverse responses. The target networks update rate set as 2.4e-5. The synchronized interval for the target networks was 30 steps. The discount factor was set as 0.5. We consider the network to have converged and terminated iterating when the change in the loss for ten consecutive epochs of the target network is less than 0.01.

\subsection{Evaluation}
\begin{table*}[t]
\centering
\begin{tabular}{cccccccccc}
\toprule
\multirow{2}[2]{*}{\textbf {Agent}}&
\multirow{2}[2]{*}{\textbf {Training Method}}&
\multicolumn{4}{c}{\textbf {Dataset-based}}&
\multicolumn{4}{c}{\textbf {Simulator-based}}\cr
\cmidrule(lr){3-6}\cmidrule(lr){7-10}
&&CS $\downarrow$&SE&LR&AQ&CS $\downarrow$&SE&LR&AQ\cr
\toprule
\multirow{3}[2]{*}{\textbf {GPT-2}}
&MLE      &0.712 &0.082 &10.396 &0.308 &0.685 &0.146 &11.276 &0.390\cr
&Standard &0.645 &0.126 &13.020 &0.550 &0.644 &0.206 &13.778 &0.526\cr
&Ours     &\textbf{0.596} &\textbf{0.191} &\textbf{14.463} &\textbf{0.555} &\textbf{0.597} &\textbf{0.238} &\textbf{15.636} &\textbf{0.566}\cr
\midrule
\multirow{3}[2]{*}{\textbf {DialoGPT}}
&MLE      &0.714 &0.069 &9.761  &0.345 &0.687 &0.142 &10.838 &0.492\cr
&Standard &0.645 &0.142 &12.182 &0.579 &0.654 &0.206 &13.772 &0.538\cr
&Ours     &\textbf{0.598} &\textbf{0.171} &\textbf{13.055} &\textbf{0.586} &\textbf{0.588} &\textbf{0.240} &\textbf{14.466} &\textbf{0.604}\cr
\midrule
\multirow{3}[2]{*}{\textbf {T5}}
&MLE      &0.720 &0.063  &9.704 &0.316 &0.651 &0.088 &10.242 &0.396\cr
&Standard &0.621 &0.147 &13.291 &0.532 &0.605 &0.224 &13.676 &0.510\cr
&Ours     &\textbf{0.567} &\textbf{0.202} &\textbf{14.834} &\textbf{0.565} &\textbf{0.553} &\textbf{0.268} &\textbf{15.134} &\textbf{0.552}\cr
\midrule
\multirow{3}[2]{*}{\textbf {GODEL}}
&MLE      &0.718 &0.064  &9.507 &0.318 &0.689 &0.112 &10.132 &0.414\cr
&Standard &0.625 &0.165 &13.553 &0.529 &0.615 &0.235 &13.108 &0.614\cr
&Ours     &\textbf{0.571} &\textbf{0.232} &\textbf{15.272} &\textbf{0.557} &\textbf{0.571} &\textbf{0.258} &\textbf{14.608} &\textbf{0.628}\cr
\bottomrule
\end{tabular}
\caption{\label{t1}For the standard offline RL algorithm and our approach, we use $L=5$ for the number of candidate responses $\{a_i\}^L_{i=1}$. For the simulator-based evaluation, we conducted 1000 dialogues of five consecutive turns between the simulator and each method. Each metric is measured per response, and the best score in each metric is in bold. The statistical test showed the differences are significant with a p-value < 0.05.}
\end{table*}
\begin{table*}[t]
\centering
\begin{tabular}{cccccc}
\toprule
\textbf{Agent}& \textbf{Training Method}& \textbf{Quality}& \textbf{Informativeness}& \textbf{Empathy}& \textbf{Engagingness}\\
\toprule
\multirow{3}[2]{*}{\textbf {GPT-2}}
&MLE      &1.4 &1.3 &1.2 &1.1\cr
&Standard &\textbf{1.7} &1.3 &1.4 &1.4\cr
&Ours     &1.5 &\textbf{1.5} &\textbf{1.5} &\textbf{1.6}\cr
\midrule
\multirow{3}[2]{*}{\textbf {DialoGPT}}
&MLE      &1.3 &1.1 &0.7 &0.7\cr
&Standard &\textbf{1.5} &1.4 &1.2 &1.2\cr
&Ours     &1.4 &\textbf{1.5} &\textbf{1.6} &\textbf{1.6}\cr
\midrule
\multirow{3}[2]{*}{\textbf {T5}}
&MLE      &1.2 &0.9 &0.5 &0.6\cr
&Standard &1.1 &0.8 &0.6 &0.7\cr
&Ours     &\textbf{1.4} &\textbf{1.4} &\textbf{1.4} &\textbf{1.3}\cr
\midrule
\multirow{3}[2]{*}{\textbf {GODEL}}
&MLE      &1.5 &1.3 &0.8 &1.0\cr
&Standard &1.6 &1.2 &1.1 &1.1\cr
&Ours     &\textbf{1.7} &\textbf{1.6} &\textbf{1.7} &\textbf{1.6}\cr
\bottomrule
\end{tabular}
\caption{\label{t2}The final scores for each metric were calculated by taking the average of the annotator ratings. Each metric is measured per dialogue, and the best score in each metric is in bold. The Fleiss’ kappa \cite{fleiss1971measuring} score with human judges was around 0.29, which can be regarded as “fair agreement.”}
\end{table*}
\begin{figure*}[t]
\centering
\subfigure[CS]{
\includegraphics[width=7.25cm]{./CS.png}
}
\subfigure[SE]{
\includegraphics[width=7.25cm]{./SE.png}
}
\subfigure[LR]{
\includegraphics[width=7.25cm]{./LR.png}
}
\subfigure[AQ]{
\includegraphics[width=7.25cm]{./AQ.png}
}
\caption{\label{g1}The evolution of the agent's performance for each metric with the increased sampling size. The scale for the X-axis is a multiple of 4, and 0 presents the MLE without RL. Bands show half a standard deviation.}
\end{figure*}
\subsubsection{Automatic Metrics}
We apply the reward perspective designed in Section 2.3 to automatic evaluation. In particular, for the view of reward (2), we count the number of generated responses that contain a word that expresses surprise (i.e., Aha, Oh, Wow, Whoa, Gee, Really?, Amazing) by conservative string-matching heuristics. For the view of reward (4), we count the number of generated responses that contain a question word or a question mark. We denote CS, SE, LR, and AQ as Cosine Similarity, Surprise Emotion, Response Length, and Asking Questions, respectively. In our automatic evaluation and discussion, each metric is measured per response, and its score is obtained by taking the average across all samples.

\subsubsection{Human Metrics}
Ten native speakers were recruited to evaluate all agents trained using different methods. We asked the annotators to engage in a conversation with all agents about daily life topics for at least five consecutive turns and rate their overall experience based on the following criteria. The scale of these metrics is [0, 1, 2].\\
\textbf{Quality} measures the coherence and grammatical correctness of the agents’ responses. \emph{Score 0}: Most responses are incoherent or contain grammatical errors, preventing the dialogue from proceeding. \emph{Score 1}: Although some responses are incoherent or contain grammatical errors, the dialogue can continue. \emph{Score 2}: Only a few (or no) incoherent or grammatical errors in the responses, and the overall dialogue flows fluently.\\
\textbf{Informativeness} measures the diversity and hallucination of the agents’ responses. \emph{Score 0}: Most responses simply repeat information from the context or are generic. \emph{Score 1}: The information conflicts with common sense or contradicts the previous statement. \emph{Score 2}: Most responses have the appropriate information.\\
\textbf{Empathy} measures the degree to which agents respond with concern or affectivity. \emph{Score 0}: Most responses were short or showed little concern for the users in the dialogue. \emph{Score 1}: Although not very coherent, some responses convey an emotional tone or ask a question. \emph{Score 2}: Some responses are both coherent and show care for or emotional attachment to the user.\\
\textbf{Engagingness} measures the desire to talk with the agents for a long conversation. \emph{Score 0}: The responses are lackluster, making it hard to sustain the dialogue. \emph{Score 1}: The responses are not particularly engaging, but they are adequate for continuing the dialogue. \emph{Score 2}: The responses are engaging and have the potential to develop the dialogue.

\subsubsection{Results}
We assessed the performance of our approach using dataset-based evaluation and compared it with baseline methods, which include a standard offline RL algorithm (i.e., Eq.(\ref{3}) and Eq.(\ref{2}), where Eq.(\ref{3}) is equivalent to our fine-grained Q-function. It is referred to as the standard method in the following.) and MLE without RL. We also conducted a simulator-based evaluation by interacting with the user simulator Blenderbot \cite{roller2021recipes} to assess the performance of different methods in a long-term dialogue. Table \ref{t1} reports the automatic evaluation results. The standard method shows better performance than MLE, which can be credited to the policy improvement introduced by RL. In contrast, our approach achieved substantial gains in all metrics, demonstrating the effectiveness of the dual-granularity Q-function. 

Table \ref{t2} summarizes the results of human evaluation. Despite the varied strengths and weaknesses of each agent according to individual human ratings, our approach exhibited markedly better results compared to baseline methods. Furthermore, RL-based agents displayed better proficiency than MLE-based agents in empathy and engagingness by utilizing knowledge of the rewards outlined in Section 2.3. In terms of quality, most agents scored higher than other metrics because of the capacity of large-scale models to generate responses that are similar to human language and the stable one-step policy improvement of offline RL, which prevents the divergence of responses from human language. With regard to informativeness, upon analyzing instances of failure, we identified several agents that provided unrealistic information. Nevertheless, our approach generated more diverse responses, resulting in a more favorable outcome compared to other methods. The interface and platform details for human evaluation are presented in Appendix C.

\section{Discussion}
\subsection{Case Study}
To conduct a comprehensive qualitative comparison between our approach and the baseline methods, we randomly selected four dialogues with varying topics from the testing set and shortened each dialogue into four consecutive utterances, resulting in four contexts per dialogue. We then instructed all agents to generate a response for each context based on their respective training methods. The resulting cases are provided in Appendix D.

We found that our algorithm and the standard method generated longer responses compared to MLE, indicating that the RL-trained agents have better conversational engagement. Both the standard method and ours tend to ask questions, but our algorithm produces a more expressive tone of voice that conveys surprise, such as "Oh," "Wow," "Really?" and so on. Although our approach and the standard method scored similarly in the quality evaluation, upon examining these generated instances, it becomes evident that the responses generated by ours are slightly more coherent than those of the standard method. We consider that this may be attributed to the coarse-grained Q-function, which tends to determine the category of context-related actions, as we will explain later.

\subsection{Further Verification}
First, to validate Theorem 1 and illustrate the impact of sampling size on policy performance, we conducted a further experiment. Figure \ref{g1} presents a comparison of the performance between the GPT-2 agents trained using the standard method and our approach, evaluated under the simulator-based setting with varying numbers of response candidates. As suggested by our theoretical derivation, increasing the sampling size generally led to better performance. Our algorithm outperformed the standard method even when using the same number of actions for policy improvement, indicating its efficiency in iterating policy. This result emphasizes the significance of sampling size as a constraint on policy performance and highlights the effectiveness of our approach in addressing this issue.

Next, we sought to verify whether our approach satisfies the hypothesis $\mathbb{E}_{a\backsim\pi_\alpha(a|s)}[Q^{\pi}(s,a)]\ge\mathbb{E}_{a\backsim\pi_\beta(a|s)}[Q^{\pi}(s,a)]$ in Theorem 2. To do so, our control generator that relies on the coarse-grained Q-function to provide the optimal category can be seen as $\pi_\alpha$, the agent learned by the standard method can be considered as $\pi_\beta$, and $Q^{\pi}$ is the fine-grained Q-function. The expected value was approximated by averaging the Q-value of each sample. The results show that for the GPT-2, DialoGPT, T5, and GODEL agents, $\mathbb{E}_{a\backsim\pi_\alpha(a|s)}[Q^{\pi}(s,a)]$ increases by 8.76\%, 8.71\%, 10.14\%, and 9.61\%, respectively, compared to $\mathbb{E}_{a\backsim\pi_\beta(a|s)}[Q^{\pi}(s,a)]$. This indicates that the categories selected by the coarse-grained Q-function can produce responses with a higher Q-value, thereby supporting the hypothesis in Theorem 2. Overall, these findings stress the potential of employing a coarse-to-fine-grained approach to narrow the scope of action for policy improvement to enhance dialogue agent performance.

Then, we examined the behavior of the coarse-grained Q-function in selecting action categories. We extracted 3000 contexts from the testing set and obtained the corresponding optimal action category by $\bar{a}^\ast=\mathop{\rm{arg\,max}}_{\bar{a}}Q_{\phi}(s,\bar{a})\quad\bar{a}\in\{\bar{a}_i\}^{N=19}_{i=1}$. The classifier was used to assign topics to these contexts. We then tallied the number of action categories selected by the coarse-grained Q-function for each context under each topic and visualized in Figure \ref{g2}. Our analysis revealed that the selections made by the coarse-grained Q-function largely align with human intuition. For instance, when the context pertains to the “film tv and video topic”, the coarse-grained Q-function often chooses categories related to “fashion and style” and “celebrity and pop culture.” Similarly, when the context relates to the “other hobbies” topic, it tends to select categories related to “travel and adventure,” “gaming,” “music,” and “sports.” We also observed a significant proportion of choices concentrated along the matrix's main diagonal, indicating its propensity to select action categories similar to the context topic. It is worth noting that "travel and adventure" also constitute a considerable part of the selected categories. After closely analyzing the dialogues in the corpus, we observed that discussions about travel are typically lengthier and require higher participation from both parties. This may have led the coarse-grained function to learn the extensive relevance of this category in dialogues.
\begin{figure*}[t]
\centering
    \includegraphics[width=14.5cm]{./DQN.png}
    \caption{\label{g2} The label on the Y-axis represents the topic of each context, whereas the label on the X-axis represents the selected action category.}
\end{figure*}

Finally, we wanted to check if our control generator can generate responses for the specified category. For each $\bar{a}\in\{\bar{a}_i\}^{N=19}_{i=1}$, we used $\pi_\psi(a|s,\bar{a})$ to obtain a response and then used the classifier to determine if this response belongs to $\bar{a}$. The empirical results reveal that the ratio of correctly responding to a given category for the control generator performed by GPT-2, DialoGPT, T5, and GODEL are 28.16\%, 29.89\%, 35.42\%, and 36.37\%, respectively. This ratio is significantly higher than the correct ratio obtained by randomly generating responses over 19 categories (i,e., 1/19 $\approx$ 5.26\%).

\section{Related Work}
Since the RL for dialogue requires multiple steps of pricey human interaction, several prior works have updated the agent's policy by the self-play method or the interaction with simulators \cite{li2016deep,shah2018bootstrapping,peng2018deep,liu2020you}. However, these online RL methods suffer from the issue of diverging from human language \cite{lewis2017deal,zhao2019rethinking,jang2020bayes}. On the other hand, Offline RL \cite{fujimoto2019off,kumar2020conservative,brandfonbrener2021offline,kostrikovoffline} removes all need for environment interaction or user simulators, instead operating purely on static datasets of prior human interaction. There are many closely related works \cite{jaques2019way,jaques2020human,snell2022offline,cohen2022dynamic,verma2022chai,jang2022gpt} based on offline RL that policy improvement via behavior cloning of self-generated utterances, which inherits the ability of pre-trained language models to generate human-like responses. In RL parlance, such methods could be considered policy extraction with approximate dynamic programming. Nevertheless, unlike RL tasks where the actions are finite such as Atari games \cite{mnih2015human}, that hard to ergodic all probability space in the dialogue setting, thus aforementioned are suboptimal compared to full Q-learning. Our dual-granularity Q-function focuses on the more structured action choices to carry out policy improvement effectively.

\section{Conclusion and Future Work}
This paper presented a dual-granularity Q-function for mitigating the suboptimal policy improvement due to the hard-to-traverse action space in RL, and we applied our method to the dialogue generation task. Theoretical and experimental results demonstrate the reliability of our algorithm, which significantly enhances the performance of the dialogue agent. Moving forward, we intend to design additional abstract categories for actions, such as those based on sentence embedding, to allow the coarse-grained Q-function to account for not only content but also utterance structure and expression. We will also investigate the relationship between the number of action categories and policy improvement. Ultimately, we plan to apply our algorithm to other online RL tasks in NLP to verify its universality.

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix
\section{Appendix}
\subsection*{Proofs of Theorem}
See Preliminary Derivation, Proof of Theorem 1, and Proof of Theorem 2 on the next page.
\begin{figure*}
\begin{equation}
\begin{split}
&{\rm \textbf{Preliminary Derivation}}:\\
&\mathbb{E}_{\pi^{\prime}}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s,a)]\\
&=\mathbb{E}_{\pi^{\prime}}[R(s_t,\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s_t,a))+\gamma R(s_{t+1},a_{t+1})+\gamma^2 R(s_{t+2},a_{t+2})+...|s_t=s,\{a_n\backsim\pi(a|s_n)\}_{n=t+1}^T]\\
&=\mathbb{E}_{\pi^{\prime}}[R(s_t,\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s_t,a))+\gamma V^{\pi}(s_{t+1})|s_t=s,a\backsim\pi^{\prime}(a|s_t)]\\\\
&\textbf{Proof\;of\;Theorem 1.}\\
&{\rm Premise}:\quad\forall s, \,\pi^{\prime}_L(\cdot|s)=\mathop{\rm{arg\,max}}\limits_{a\in \{a_i\}^L_{i=1}\backsim\pi(a|s)}Q^{\pi}(s,a)\\
&{\rm Lemma}\,:\quad{\rm if}\;N\ge M\ge 1,\;{\rm then}\;\mathbb{E}_{\pi}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^N_{i=1}}Q^{\pi}(s,a)]\ge\mathbb{E}_{\pi}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^M_{i=1}}Q^{\pi}(s,a)]\\
&V^{\pi^{\prime}_N}(s)\\
&=\mathbb{E}_{\pi^{\prime}_N}[R(s_t,a_{t}\backsim\pi^{\prime}_N(a|s_t))+\gamma R(s_{t+1},a_{t+1}\backsim\pi^{\prime}_N(a|s_{t+1}))+...|s_t=s]\\
&=\mathbb{E}_{\pi}[R(s_t,\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^N_{i=1}}Q^{\pi}(s_t,a))+\gamma R(s_{t+1},\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^N_{i=1}}Q^{\pi}(s_{t+1},a))+...|s_t=s]\\
&=\mathbb{E}_{\pi}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^N_{i=1}}Q^{\pi}(s,a)-\gamma V^{\pi}(s_{t+1})|s_t=s]+\gamma\mathbb{E}_{\pi}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^N_{i=1}}Q^{\pi}(s_{t+1},a)-\gamma V^{\pi}(s_{t+2})|s_t=s]+...\\
&\ge\mathbb{E}_{\pi}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^M_{i=1}}Q^{\pi}(s,a)-\gamma V^{\pi}(s_{t+1})|s_t=s]+\gamma\mathbb{E}_{\pi}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^M_{i=1}}Q^{\pi}(s_{t+1},a)-\gamma V^{\pi}(s_{t+2})|s_t=s]+...\\
&=\mathbb{E}_{\pi}[R(s_t,\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^M_{i=1}}Q^{\pi}(s_t,a))+\gamma R(s_{t+1},\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^M_{i=1}}Q^{\pi}(s_{t+1},a))+...|s_t=s]\\
&=\mathbb{E}_{\pi^{\prime}_M}[R(s_t,a_{t}\backsim\pi^{\prime}_M(a|s_t))+\gamma R(s_{t+1},a_{t+1}\backsim\pi^{\prime}_M(a|s_{t+1}))+...|s_t=s]=V^{\pi^{\prime}_M}(s)\\\\
&\textbf{Proof\;of\;Theorem 2.}\\
&{\rm Premise\,1}:\quad\forall s, \,\pi^{\prime}_1(\cdot|s)=\mathop{\rm{arg\,max}}\limits_{a\in \{a_i\}^L_{i=1}\backsim\pi_\alpha(a|s)}Q^{\pi}(s,a),\;\;\pi^{\prime}_2(\cdot|s)=\mathop{\rm{arg\,max}}\limits_{a\in \{a_i\}^L_{i=1}\backsim\pi_\beta(a|s)}Q^{\pi}(s,a)\\
&{\rm Premise\,2}:\quad\mathbb{E}_{a\backsim\pi_\alpha(a|s)}[Q^{\pi}(s,a)]\ge\mathbb{E}_{a\backsim\pi_\beta(a|s)}[Q^{\pi}(s,a)],\;\;\sigma^2_{a\backsim\pi_\alpha(a|s)}[Q^{\pi}(s,a)]\approx\sigma^2_{a\backsim\pi_\beta(a|s)}[Q^{\pi}(s,a)]\\
&{\rm Lemma}\;\;\;:\;\quad\because\;{\rm Premise\,2}\;\;\therefore\;\mathbb{E}_{\pi_\alpha}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s,a)]\ge\mathbb{E}_{\pi_\beta}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s,a)]\\
&V^{\pi^{\prime}_1}(s)\\
&=\mathbb{E}_{\pi^{\prime}_1}[R(s_t,a_{t}\backsim\pi^{\prime}_1(a|s_t))+\gamma R(s_{t+1},a_{t+1}\backsim\pi^{\prime}_1(a|s_{t+1}))+...|s_t=s]\\
&=\mathbb{E}_{\pi_\alpha}[R(s_t,\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s_t,a))+\gamma R(s_{t+1},\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s_{t+1},a))+...|s_t=s]\\
&=\mathbb{E}_{\pi_\alpha}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s,a)-\gamma V^{\pi}(s_{t+1})|s_t=s]+\gamma\mathbb{E}_{\pi_\alpha}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s_{t+1},a)-\gamma V^{\pi}(s_{t+2})|s_t=s]+...\\
&\ge\mathbb{E}_{\pi_\beta}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s,a)-\gamma V^{\pi}(s_{t+1})|s_t=s]+\gamma\mathbb{E}_{\pi_\beta}[\mathop{{\rm max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s_{t+1},a)-\gamma V^{\pi}(s_{t+2})|s_t=s]+...\\
&=\mathbb{E}_{\pi_\beta}[R(s_t,\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s_t,a))+\gamma R(s_{t+1},\mathop{\rm{arg\,max}}\limits_{a\in\{a_i\}^L_{i=1}}Q^{\pi}(s_{t+1},a))+...|s_t=s]\\
&=\mathbb{E}_{\pi^{\prime}_2}[R(s_t,a_{t}\backsim\pi^{\prime}_2(a|s_t))+\gamma R(s_{t+1},a_{t+1}\backsim\pi^{\prime}_2(a|s_{t+1}))+...|s_t=s]=V^{\pi^{\prime}_2}(s)
\end{split}
\nonumber
\end{equation}
\end{figure*}

\section{Appendix}
\subsection*{Details about Rewards}
(1) Cosine Similarity: We manually made a list of dull responses consisting of utterances such as “I don’t know,” etc., which are short and occur in generation models frequently. We penalize the cosine similarity between the agent’s response and the dull responses, to avoid the agent generating the dull responses. This score is computed by leveraging a state-of-the-art sentence embedding model \cite{conneau2017supervised}, and the range is 0 to 1. Although there are more ways to generate a dull response, similar expressions are likely to fall into an adjacent vector space. The user “keeps away” from the utterances in the list is thus also away from other similar dull responses.\\
(2) Surprise Emotion: Since each utterance in the DailyDialog dataset was annotated by one of six universal emotions in human beings, we directly used the emotional label of “Surprise” to allocate a reward to each sentence, which the scale is [0, 1].\\
(3)	Response Length: From the perspective of empathy, we prefer the agents to generate longer responses. The reward is defined as follows.\par\quad
\textbf{if} \quad the number of generated tokens < 5:\par\qquad\quad
reward = -0.2\par\quad
\textbf{elif} \;the number of generated tokens < 10:\par\qquad\quad
  reward = 0\par\quad
\textbf{elif} \;the number of generated tokens < 15:\par\qquad\quad
reward = 0.2\par\quad
\textbf{else}:\par\qquad\quad
reward = 0.5\\
(4)	Asking Questions: Each utterance in the DailyDialog dataset was also labeled as one of four dialogue act classes. We used the act label of “Questions” to allocate a reward to each sentence. The scale of this reward is [0, 1].

\section{Appendix}
\subsection*{Details about Interactive}
We utilized LINE as our platform and set up a specialized account for human evaluation, which evaluators accessed through their smart device to interact with each agent and provide ratings. The server was equipped with an NVIDIA A6000 (48G) graphics card, and the program was developed using the LINE Messaging API SDK and ran continuously in the background, ready to receive requests at any time. Each agent extended an abstract class that defined key methods for conversation generation and was registered to a dictionary via a decorator. To ensure a randomized order of appearance of agents for annotators during the evaluation process, we implemented a randomized selection of dictionary indices. Furthermore, due to the substantial startup times of the agents, all agents were kept in memory at all times in the background process. The current configuration was able to support hundreds of simultaneous users and host more than 20 agents concurrently. In Figure \ref{g3} (a) and (b), we can observe the conversation interface utilized by annotators for interacting with the agents during the human evaluation process. This interface enabled participants to engage in a dialogue of at least five turns before initiating the rating phase by entering the word "end." Conversely, Figure \ref{g3} (c), (d), (e), and (f) exhibit the interface used for rating the agents after having a conversation of at least five turns with them as part of the human evaluation.
\begin{figure*}[t]
\centering
\subfigure[Chat Interface 1]{
\includegraphics[width=5cm]{./s1.png}
}
\subfigure[Chat Interface 2]{
\includegraphics[width=5cm]{./s2.png}
}
\subfigure[Ratings Interface 1]{
\includegraphics[width=5cm]{./s3.png}
}
\subfigure[Ratings Interface 2]{
\includegraphics[width=5cm]{./s4.png}
}
\subfigure[Ratings Interface 3]{
\includegraphics[width=5cm]{./s5.png}
}
\subfigure[Ratings Interface 4]{
\includegraphics[width=5cm]{./s6.png}
}
\caption{\label{g3}The chat interface and rating interface were used for human evaluation.}
\end{figure*}

\section{Appendix}
\subsection*{Case Study}
Figure \ref{g4} presents the varied responses generated by different methods across four agents. The ground truth for responding to each user utterance is the next user utterance in the dialogue.
\begin{figure*}[t]
\centering
\subfigure[GPT-2]{
\includegraphics[width=15cm]{./GPT-2.png}
}
\subfigure[DialoGPT]{
\includegraphics[width=15cm]{./DialGPT.png}
}
\end{figure*}
\addtocounter{figure}{-1}
\begin{figure*} 
\addtocounter{figure}{1}
\centering
\subfigure[T5]{
\includegraphics[width=15cm]{./T5.png}
}
\subfigure[GODEL]{
\includegraphics[width=15cm]{./GODEL.png}
}
\caption{\label{g4}Case Study.}
\end{figure*}

\end{document}