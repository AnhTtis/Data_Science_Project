{
    "arxiv_id": "2303.13465",
    "paper_title": "Deep RL with Hierarchical Action Exploration for Dialogue Generation",
    "authors": [
        "Itsugun Cho",
        "Ryota Takahashi",
        "Yusaku Yanase",
        "Hiroaki Saito"
    ],
    "submission_date": "2023-03-22",
    "revised_dates": [
        "2023-05-09"
    ],
    "latest_version": 2,
    "categories": [
        "cs.CL",
        "cs.AI"
    ],
    "abstract": "Traditionally, approximate dynamic programming is employed in dialogue generation with greedy policy improvement through action sampling, as the natural language action space is vast. However, this practice is inefficient for reinforcement learning (RL) due to the sparsity of eligible responses with high action values, which leads to weak improvement sustained by random sampling. This paper presents theoretical analysis and experiments showing that the dialogue policy's performance is positively correlated with the sampling size. To alleviate this limitation, we introduce a novel dual-granularity Q-function that explores the most promising response category to intervene in the sampling process. Our approach extracts actions based on a grained hierarchy, achieving the optimum with fewer policy iterations. Additionally, we use offline RL and learn from multiple reward functions designed to capture emotional nuances in human interactions. Empirical studies demonstrate that our algorithm outperforms baselines across automatic metrics and human evaluations. Further testing reveals that ours generates responses with higher expected rewards and controllability.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.13465v1",
        "http://arxiv.org/pdf/2303.13465v2"
    ],
    "publication_venue": null
}