\documentclass[acmlarge]{acmart}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\setcopyright{acmcopyright}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{XXXXXXX.XXXXXXX}


\let\oldFootnote\footnote
\newcommand\nextToken\relax
\renewcommand\footnote[1]{%
    \oldFootnote{#1}\futurelet\nextToken\isFootnote}
\newcommand\isFootnote{%
    \ifx\footnote\nextToken\textsuperscript{,}\fi}

\usepackage{graphicx}
\usepackage{multirow}

\usepackage{subcaption}

\definecolor{mypink}{HTML}{e72a8a}
\renewcommand\fbox{\fcolorbox{mypink}{mypink}}
\usepackage{tcolorbox}
\newtcolorbox{mybox}{colback=mypink,colframe=mypink}

\usepackage{mdframed}
\newmdenv[
  topline=false,
  bottomline=false,
  skipabove=\topsep,
  skipbelow=\topsep,
  linecolor=mypink,
]{siderules}






\usepackage{enumitem}

\usepackage{xcolor}
\definecolor{outcolor}{rgb}{0.8,0.8,0.8}

\newcommand{\pavlos}[1]{\textcolor{blue}{{#1}}} %
\newcommand{\out}[1]{\textcolor{outcolor}{{#1}}}

\newcommand{\myitem}[1]{\vspace{0.25\baselineskip}\noindent\textbf{#1}}%

\definecolor{myboxcolor}{rgb}{0.9,0.9,0.9}
\newtcolorbox{mybox1}{colback=myboxcolor,colframe=myboxcolor}


\begin{document}
\title{Uncovering Bias in Personal Informatics}

\author{Sofia Yfantidou}
\email{syfantid@csd.auth.gr}
\orcid{0000-0002-5629-3493}
\author{Pavlos Sermpezis}
\email{sermpezis@csd.auth.gr}
\author{Athena Vakali}
\email{avakali@csd.auth.gr}
\affiliation{%
  \institution{Aristotle University of Thessaloniki}
  \city{Thessaloniki}
  \country{Greece}
  \postcode{54124}
  }
\author{Ricardo Baeza-Yates}
\affiliation{%
  \institution{Institute for Experiential AI, Northeastern University}
  \city{San Jose}
  \country{United States}
  \postcode{CA 95113}
  }








\renewcommand{\shortauthors}{Yfantidou, et al}

\begin{abstract}
Personal informatics (PI) systems, powered by smartphones and wearables, enable people to lead healthier lifestyles by providing meaningful and actionable insights that break down barriers between users and their health information. Today, such systems are used by billions of users for monitoring not only physical activity and sleep but also vital signs and women's and heart health, among others. %
Despite their widespread usage, the processing of sensitive PI data may suffer from biases, which may entail practical and ethical implications. In this work, we present the first comprehensive empirical and analytical study of bias in PI systems, including biases in raw data and in the entire machine learning life cycle. We use the most detailed framework to date for exploring the different sources of bias and find that biases exist both in the data generation and the model learning and implementation streams. According to our results, the most affected minority groups are users with health issues, such as diabetes, joint issues, and hypertension, and female users, whose data biases are propagated or even amplified by learning models, while intersectional biases can also be observed.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003138</concept_id>
       <concept_desc>Human-centered computing~Ubiquitous and mobile computing</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
          <concept>
       <concept_id>10010405.10010444.10010446</concept_id>
       <concept_desc>Applied computing~Consumer health</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178</concept_id>
       <concept_desc>Computing methodologies~Artificial intelligence</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10003456.10003457.10003580.10003543</concept_id>
       <concept_desc>Social and professional topics~Codes of ethics</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~Ubiquitous and mobile computing}
\ccsdesc[300]{Applied computing~Consumer health}
\ccsdesc[300]{Computing methodologies~Artificial intelligence}
\ccsdesc[300]{Social and professional topics~Codes of ethics}

\keywords{machine learning, bias, fairness, personal informatics, ubiquitous computing, sensing data, digital biomarkers}


\maketitle

\section{Introduction}\label{introduction}
Ubiquitous technologies, such as smartphones or wearables, are an integral part of our lives today, with the current number of smartphone users worldwide in 2022 rising to 6.6 billion, or 83.4\% of the global population, from 3.6 billion (49.4\%) in 2016 \cite{mobilestatisticsreport}. More impressively, the number of connected wearable devices worldwide has more than tripled in the past six years, rising to 1.1 billion in 2022 from 325 million in 2016 \cite{statistaGlobalConnected}. The proliferation of ubiquitous technologies has given rise to Personal Informatics (PI), namely a class of systems that ``help people collect personally relevant information for the purpose of self-reflection and gaining self-knowledge'' \cite{li2010stage}. Such systems enable people to keep track of their productivity \cite{kim2016timeaware}, finances \cite{kaye2014money}, and learning \cite{garbett2018thinkactive}. Yet, tracking various aspects of physical and mental health is particularly prevalent \cite{epstein2020mapping}. %

Ubiquitous technologies for PI can continuously and unobtrusively measure and collect physiological and behavioral data, namely, ``digital biomarkers'', from users through integrated sensors. Digital biomarkers contain an uncanny amount of personal information. Even the coarser behavioral biomarkers acquired from consumer wearable devices (such as steps, burned calories, and covered distance), strongly correlate to a person's gender, height, and weight \cite{thomas2022fitbit}, while signals of finer granularity (such as accelerometer and heart rate measurements), can predict variables associated with an individual's physical health, fitness, and demographics \cite{10.1145/3450439.3451863}. Similarly, physiological signals accompanied by PI usage logs have been used to predict mood, stress and overall mental health \cite{taylor2017personalized}. At the same time, the advanced features for health tracking that are continuously integrated into consumer smartphones and wearables~\cite{apple2022health,lubitz2022detection,appleAppleEmpowering,ouraringOuraBlood} now enable advanced analytics, such as atrial fibrillation identification, fertility prediction, fall, and crash detection, sleep apnea warnings, and are paving the future of mHealth.


\myitem{Bias in PI.} However, the prevalent PI adoption embeds important challenges due to the questionable transparency and unexplored biases in the systems' algorithms. Bias in machine learning is a source of unfairness that can lead to harmful consequences, such as discrimination \cite{mehrabi2021survey}. The Fairness, Accountability, and Transparency in Machine Learning (FAT/ML) community defines fairness as a principle that ``ensures that algorithmic decisions do not create discriminatory or unjust impacts when comparing across different demographics (e.g., race, sex, etc.)'' \cite{awwad2020exploring}. Fairness is an inexorably subjective and context-dependent notion and incorporates different metrics for different definitions, some of which are even mutually incompatible \cite{friedler2021possibility}.  Contrary to the common belief that algorithmic decisions are objective and unbiased by definition, a machine learning model may actually be inherently unfair by learning, preserving, or even amplifying historical biases existent in the data \cite{pessach2022review}. 
Real-world cases of unfair machine learning models are, unfortunately, abundant. %
Examples can be drawn from criminal justice \cite{angwin2016machine}, hiring practices \cite{dastin2018amazon}, ad targeting \cite{simonite2015probing}, facial recognition \cite{raji2020saving}, healthcare \cite{wiens2020diagnosing} and language models \cite{bolukbasi2016man}. %

Despite this growing interest in machine learning biases overall, a focused emphasis on the requirements of unbiased PI systems in mHealth settings is lacking \cite{ahmad2020fairness}. %
PI systems are deployed in high-stakes health-related applications, while their input data modality makes them susceptible to propagating or even amplifying bias. Beyond algorithm performance, the existence of bias is a challenging problem in delivering equitable care. Thus, it is critical to explore biases within these systems to raise awareness regarding mitigating and regulatory actions required to avert potential negative consequences. 

\myitem{PI Idiosyncracies.} Moreover, this need for exploring bias is further highlighted by the fact that the PI domain has significant differences -in terms of bias- from previously well-studied domains, such as facial or speech recognition:
\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{\textit{The digital divide as a barrier of entry:}} To contribute data to an image or voice dataset, users do not need any prerequisite knowledge or niche device. However, to contribute to a PI dataset, users face significant ``entry barriers'' in terms of digital capacity or device ownership, creating new-found \textit{representation biases} in the domain's datasets, as verified by our analysis in Sections \ref{historicalBias} and \ref{representationBias}.
    
    \item \textbf{\textit{Emerging technologies accuracy:}} Facial or speech recognition measurement devices, e.g., camera or voice recorder, are based on mature technologies. As a result, their accuracy remains relatively unchangeable across different devices%
    . On the contrary, emerging PI devices' accuracy significantly varies across manufacturers and even across models, creating unexplored \textit{measurement biases} and discrepancies between user segments (cf. Section~\ref{measurementBias}).
    
    \item \textbf{\textit{Complex nature of data:}} It may be easy to identify biases in terms of skin color and gender (facial recognition) or accent and gender (speech recognition). Yet, identifying biases in digital biomarkers (e.g., step or sleep data) may not be straightforward. Biases in PI data can remain hidden and be further propagated or even amplified in machine learning models (cf. Sections \ref{aggregation} and \ref{learningBias}).
\end{itemize}


\myitem{Summary of contributions.} Motivated by these idiosyncrasies and the gap in the literature, in this paper, we present the first comprehensive study on bias in PI: We adopt the most complete framework to date for understanding sources of harm in the machine learning life cycle~\citet{suresh2021framework}, explore biases in the data generation and model and implementation streams, and validate them in a real-world, large-scale PI dataset. During this process, we examine the suitability of different fairness metrics for digital biomarkers, initiating a conversation within the community on how to approach biases within ubiquitous mHealth. %

Specifically, our research questions (RQs) and the respective contributions of our study are as follows:
\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{\textit{What does bias mean for PI?}} To quantify bias in relation to the PI domain, we explore diverse fairness definitions and metrics and identify differences from other domains. We delineate each metric's strengths and shortcomings and select the most appropriate metrics for the domain (Section~\ref{configuration}).
    
    \item \textbf{\textit{Are PI data susceptible to biases?}} We examine the largest real-world PI dataset to date to assess whether ubiquitous digital biomarkers incorporate biases. Specifically, we perform the first detailed study on bias in the MyHeart Counts cardiovascular health study dataset \cite{hershman2019physical}, containing physical activity, fitness, sleep, and cardiovascular health data for 50K participants across the United States. Our results identify biases across all dimensions of the data generation stream, namely \textit{historical}, \textit{representation}, and \textit{measurement} biases; these findings highlight that users should be cautious when using PI datasets, in general, and the MyHeart Counts data, in particular (Section~\ref{dataBias}).
     
    \item \textbf{\textit{Do machine learning models inherit PI data biases?}} We investigate whether biases present in PI data are propagated when applying machine learning models to these data. %
    Specifically, we evaluate long short-term memory (LSTM) sequence models as a baseline and personalized models for \textit{aggregation}, \textit{learning}, and \textit{deployment} biases. In line with prior work~\cite{paviglianiti2020vital}, our findings indicate that data biases are propagated to deep learning models, especially for intersectional user groups. Surprisingly, they are significantly amplified in their personalized counterparts, raising questions regarding the shortcomings of personalization (Section~\ref{modelBiases}).     

    \item \textbf{\textit{Can synthetic benchmarks hide the imperfect nature of PI?}} We explore whether ``perfect'' synthetic benchmark datasets can hide PI data and model ``imperfections'' and biases during evaluation. Specifically, we compare a random benchmark, representative of our data, with one designed to achieve demographic parity for evaluation biases. Our findings highlight the importance of the establishment of PI benchmarks that are representative of the intended target populations to avoid the deployment of models with unidentified biases (Section~\ref{sec:evalBias}).
\end{enumerate}

Finally, we partially apply our analysis on two different PI datasets to showcase the generalizability of our findings and share our code publicly \cite{yfantidou2023sourcecode} to encourage applicability to other datasets as well (Section~\ref{generalizability}). We then discuss limitations and recommendations for mitigating the effect of bias in PI and conclude our work (Section~\ref{discussion}).


\section{Personal Informatics Biases: Setting \& Configuration}\label{configuration}
In this section, we discuss the framework upon which we base our study of bias in PI (Section~\ref{framework}) and describe our use case configuration, which acts as a starting point for our investigation (Section~\ref{useCase}).

\begin{figure}[htb!]
\centering
\begin{subfigure}[b]{0.9\textwidth}
   \includegraphics[width=.9\linewidth]{img/dataBias.pdf}
   \caption{Sources of harm in the data generation stream.\label{fig:biasData}}
\end{subfigure}
\begin{subfigure}[b]{0.9\textwidth}
   \includegraphics[width=.9\linewidth]{img/modelBias.pdf}
   \caption{Sources of harm in the model building and implementation stream.\label{fig:biasModel}}
\end{subfigure}
\caption{Sources of harm in the data (top) and model building and implementation (bottom) streams \cite{suresh2021framework}. The training, test, and benchmark sets are common across figures.}
\end{figure}

\subsection{Sources of Bias in the Machine Learning Life Cycle Framework\label{framework}}
We base our study of bias in PI for mHealth on \citet{suresh2021framework} framework for understanding sources of harm through the machine learning life cycle, the most comprehensive framework to date for capturing biases in any machine learning system. According to Suresh and Guttang, the machine learning life cycle consists of two major streams containing seven sources of bias-related harms, the \textit{data generation} stream and the \textit{model building and implementation} stream. The data generation stream can contain historical, representational, and measurement bias, while the model building and implementation stream can contain aggregation, learning, evaluation, and deployment bias. An overview of the sources of bias-related harm in the data generation and model building and implementation streams is shown in Figures \ref{fig:biasData} and \ref{fig:biasModel}, respectively, while definitions are provided below \cite{suresh2021framework}:
\begin{itemize}[leftmargin=*,nosep]
    \item \textit{Historical biases} can occur even if the data are flawlessly measured and sampled by reflecting real-world, past, and present biases against one or more groups of people. For example, gender gaps in certain professions lead to natural language models associating gendered occupation words, such as nurse or programmer, with words representing women or men, respectively \cite{bolukbasi2016man}.
    \item \textit{Representation biases} can occur when sampling methods lead to underrepresenting general population segments. For example, in popular image datasets, the majority of the images originate from the United States or Europe, leading to performance degradation when classifying images coming from an underrepresented region \cite{denton2020bringing}.
    \item \textit{Measurement biases} can occur when choosing, collecting, and calculating features and labels for the prediction problem. For example, in medical applications, oftentimes, diagnosis  is used as a proxy for having a health condition; yet, certain gender and racial groups suffer higher rates of misdiagnosis, or underdiagnosis \cite{hoffman2016racial}.
    \item \textit{Aggregation biases} can occur when an ``one-size-fits-all'' treatment, e.g., model, is used for data in which underlying user groups should be treated separately. For example, in natural language processing, training models in generic data will fail to capture the different meanings, and off-line context behind street slang \cite{frey2020artificial}.
    \item \textit{Learning biases} can occur when modeling choices amplify performance disparities across different user segments in the data. For example, optimizing a model for privacy can reduce the influence of data originating from underrepresented groups \cite{bagdasaryan2019differential}.
    \item  \textit{Evaluation biases} can occur when the benchmark population is not representative of the real user population. For example, dark-skinned women comprise only a small percentage of popular facial images benchmark, leading to worse performance of commercial facial analysis tools on intersectional accuracy \cite{buolamwini2018gender}.
    \item \textit{Deployment biases} can occur when there exists a mismatch between the problem a model is designed to solve and how it is actually utilized. For example, risk assessment tools in criminal justice are not used in isolation but can be used in ``off-label'' ways, such as determining the length of a sentence \cite{collins2018punishing}.
\end{itemize} 
  

In the following section, we introduce the use case through which we explore bias in PI for mHealth. We then show empirically and analytically how Suresh and Guttag's seven sources of bias translate in the PI domain.


\subsection{Exploring Bias through the Largest Digital Biomarkers mHealth Dataset\label{useCase}}
To examine the existence of the seven sources of bias in the PI machine learning life cycle and provide clear answers to our research questions (RQs), introduced in Section~\ref{introduction}, we need to define an indicative -but by no means restrictive- use case to enable our analysis. For this purpose, we utilize the MyHeart Counts dataset \cite{hershman2019physical}, the largest collection of digital biomarkers in the mHealth domain to date, enabling us to perform the most comprehensive analysis of bias across diverse user demographics, including gender, ethnicity, age, BMI, and health conditions. Nevertheless, our methodology and outputs can be generalized to any PI dataset other than the prominent use case of MyHeart Counts (see Section~\ref{generalizability}). 

\subsubsection*{\textbf{Data Description}} Up till recently, general-purpose, population-scale PI datasets were unavailable, partly due to the high cost of data collection, as well as privacy concerns and data protection regulations. The most popular open datasets consisted of small to medium samples \cite{vaizman2018extrasensory,wang2014studentlife} or were domain-constrained to Human-Activity Recognition (HAR) \cite{anguita2013public} and Sleep Classification (SC) \cite{malafeev2018automatic}. However, this changed with the publication of data from the MyHeart Counts Cardiovascular Health Study, a collection of real-world physical activity, fitness, sleep, and cardiovascular health data from 50K participants in the United States. Participants completed various surveys and a 6-minute walk test and contributed PI data via an iPhone application built using Apple’s ResearchKit framework \cite{researchkitResearchKit}. They provided informed consent to make this data freely available for future research. 

\begin{table}[htb!]
\caption{The available protected attributes in the MyHeart Counts study data. For the purpose of the bias analysis, we convert the non-binary attributes to binary to ensure a sufficient sample size per group and compatibility with popular bias metrics.}
\label{tab:protectedAttributes}
\resizebox{\textwidth}{!}{%
\begin{tabular}{llll}
 & \multicolumn{1}{c}{\textit{Original Protected Attribute Values}} & \multicolumn{2}{c}{\textit{Binarized Protected Attribute Values}} \\ \hline
\multicolumn{1}{l}{\textbf{Attribute}} & \textbf{Original Groups} & \multicolumn{1}{l}{\textbf{Majority Group}} & \textbf{Minority Group} \\ \hline
\multicolumn{1}{l}{Gender} & Male, Female, N/A & \multicolumn{1}{l}{Male} & Female \\ \hline
\multicolumn{1}{l}{Ethnicity} & White, Asian, Black, Hispanic, American Indian, Pacific Islander, Other, N/A & \multicolumn{1}{l}{White} & Non-white \\ \hline
\multicolumn{1}{l}{Age} & Integer Number, N/A & \multicolumn{1}{l}{<65}  (lower risk of complications) & >=65 (higher risk of complications) \\ \hline
\multicolumn{1}{l}{BMI} & Real Number (height and weight), N/A & \multicolumn{1}{l}{<18.5 or =>25 (non-healthy)} & =>18.5 and <25 (healthy) \\ \hline
\multicolumn{1}{l}{Heart Condition} & Yes, No, N/A & \multicolumn{1}{l}{No} & Yes \\ \hline
\multicolumn{1}{l}{Hypertension} & Yes, No, N/A & \multicolumn{1}{l}{No} & Yes \\ \hline
\multicolumn{1}{l}{Joint Problem} & Yes, No, N/A & \multicolumn{1}{l}{No} & Yes \\ \hline
\multicolumn{1}{l}{Diabetes} & Yes, No, N/A & \multicolumn{1}{l}{No} & Yes \\ \hline
\end{tabular}%
}
\end{table}
Approximately 1 out of 10 participants ($N=4920$) shared their basic HealthKit data (step count, distance covered, burned calories, and flights climbed), while fewer users shared their sleep ($N=626$) and workout ($N=881$) data. We perform our analysis on the basic HealthKit data, which contains the most common data types among scientific datasets so that our findings are generalizable and our methodology is reproducible. %
Additionally, we combine these data with survey responses to attain the following user attributes: gender, ethnicity, age, BMI, and health conditions, such as heart condition, hypertension, joint problem, and diabetes. 

\subsubsection*{\textbf{Data Preprocessing}} To ensure a sufficient sample size per user group and compatibility with popular bias metrics, we convert non-binary user attributes, such as ethnicity, age, and BMI, to binary, as seen in Table~\ref{tab:protectedAttributes}. This grouping creates two user groups per protected attribute, namely a majority group (also called ``privileged'' for the purpose of this analysis) and a minority group (also called ``unprivileged'' for the purpose of this analysis). Note that the usage of the term ``privilege'' in this work does not necessarily coincide with real-world ``privilege''. For example, users with non-healthy BMI are the majority user segment in our dataset, and hence, they are referred to as the ``privileged'' user group, whereas one could argue that the opposite applies in reality.

\begin{table}[htb!]
\caption{An example of input data for the physical activity prediction use case. The step counts per hour for the past 48 hours are the features, and the total number of the next day's steps is the label. The user ID and timestamps are not used in the learning.}
\label{tab:inputData}
\resizebox{.5\textwidth}{!}{%
\begin{tabular}{llllll}
\hline
\multicolumn{5}{c}{\textbf{Features}} & \multicolumn{1}{c}{\textbf{Label}} \\ \hline
\multicolumn{1}{l}{user\_{id}} & \multicolumn{1}{l}{timestamp} & \multicolumn{1}{l}{steps at t-48h} & \multicolumn{1}{l}{...} & steps at t-1h & next day's steps \\ \hline
\multicolumn{1}{l}{1} & \multicolumn{1}{l}{23-11-2022} & \multicolumn{1}{l}{1040} & \multicolumn{1}{l}{...} & 300 & 8500 \\ \hline
\end{tabular}%
}
\end{table}

\subsubsection*{\textbf{Data Labeling}} As mentioned previously, the MyHeart Counts dataset is general-purpose, meaning that it does not introduce any new learning tasks or certain prediction labels. To this end, we select the \textit{next-day physical activity prediction from historical data} use case \cite{bampakis2022ubiwear,vasdekis2022wemod} for model training. In other words, based on the user's past activity, we try to predict how many steps they will perform the next day (see Table~\ref{tab:inputData}). Such a task may enable, for instance, the provision of personalized step goals by PI systems, which have proven to be more effective in inciting positive health behavior change compared to static, fixed goals \cite{adaptive_personalized_advisor}. The reasons behind this choice lie not only in the benefits of physical activity for physical and mental health \cite{world2019global} but also in the availability of basic digital behavioral biomarkers, such as steps. Contrary to raw sensor data, which are harder to collect at scale through consumer PI systems, basic digital behavioral biomarkers, are easy to collect and commonplace in the literature, enabling the reproducibility of our findings. At the same time, steps are the largest available sensed modality in the My Heart Counts dataset, allowing us to take advantage of a larger portion of the data for the purpose of this analysis.
Finally, it is important to note that our findings can be generalized to other PI tasks, e.g., mood and stress prediction \cite{taylor2017personalized}, or health monitoring \cite{peterson2017personalized}. %













\subsection{Quantifying Bias through Fairness Metrics}\label{fairnessMetrics}
In this section, we discuss fairness metrics that quantify bias in machine learning from the perspective of PI, providing an answer to RQ1, namely: \textit{What does bias mean for PI?} 
 







As mentioned earlier on, fairness is a social construct that defies simple definition \cite{narayanan21fairness}. Quantitative fields view fairness as a mathematical problem of ``equal or equitable allocation, representation, or error rates, for a particular task or problem'' \cite{narayanan21fairness}. 
There is a variety of fairness definitions and metrics (see Appendix~\ref{ap:definitions}). However, not all of them are relevant to the PI domain. Contrary to popular bias quantification tasks, such as recidivism prediction or loan repayment prediction, in our use case -and related PI tasks- there is no clear positive outcome for the user. In other words, in recidivism prediction, being marked as low-risk for committing a new crime is indisputably positive for the individual. 
On the contrary, a high activity goal -even though recommended- might not be realistic and thus advantageous for all individuals. Specifically, according to the Goal-Setting Theory by \citet{latham1991self}, if an individual does not believe they can achieve their goal, they are unlikely to do so. Thus, users' goals should be close to their current abilities to hold sufficient motivational power. 

To this end, we initially look into definitions based on predicted and actual outcomes, namely False Omission Rate (FOR), False Negative Rate (FNR), False Positive Rate (FPR) ratios, and Error Rate Ratio (ERR), that focus on erroneous predictions rather than solely positive outcomes (for definitions, formulas and interpretation, see Appendix~\ref{ap:A}). However, we quickly notice that such metrics are prone to data biases and imbalances, as shown in the example below. Assume you have an imbalanced dataset of 3000 women -2000 low active and 1000 highly active- and 8000 men -3500 low active and 4500 highly active-. Imagine a model that misclassifies 100\% of highly active women as low active, i.e., $FN=1000$. Then, $ER_{women}=\frac{FP+FN}{P+N}=\frac{1000}{3000}=33\%$. For men to have the same error rate, a model needs to misclassify 2640 highly active men ($FN=2640$) as $ER_{men}=\frac{2640}{8000}=33\%$. So, even though we have misclassified 100\% of the highly active part of the minority group, by misclassifying only 59\% ($\frac{2640}{8000}$) of the majority group, we can achieve in paper demographic parity with an ERR of 1.0 (optimal value). As discussed in Section~\ref{dataBias}, our data suffer from various biases and imbalances, and hence error-centric metrics would not offer an objective comparison.
Hence, for the purpose of this work, we utilize the widely used DIR, which is the ratio of base or selection rates between unprivileged and privileged groups, assuming equal ability across demographics:
\begin{equation*}
\text{Disparate Impact Ratio}=\frac{\Pr(y^+\mid G0)}{\Pr(y^+\mid G1)}
\end{equation*}
where $y^+$ is the actual or predicted positive outcome label (base or selection rate, respectively), $G0$ is the minority (protected) group, and $G1$ is the majority group. Values less than 1 indicate that the majority group has a higher proportion of predicted positive outcomes than the minority group. A value of 1 indicates demographic parity. Values greater than 1 indicate that the minority group has a higher proportion of predicted positive outcomes than the majority one. For example, a value of 0.8 for a dataset with men/women as the majority/minority groups means that for every man receiving a high activity goal, only 0.8 women do so. According to the AIF360 toolkit (\url{https://aif360.mybluemix.net/}), accepted values are within [0.8,1.25], but such ranges are not universally accepted and might be adjusted on a task-by-task basis \cite{equal1990uniform}. 

Having established our metric of choice, we move forward to our analysis of bias in the data generation (Section~\ref{dataBias}) and the model building and implementation (Section~\ref{modelBiases}) streams.


\section{Exploring Bias in Personal Informatics Data Generation}\label{dataBias}

Bias in the data generation stream can take the form of historical, representation, and measurement biases, as seen in Figure~\ref{fig:biasData}. In this section, we explore all three sources, providing an answer to RQ2: \textit{Are PI data susceptible to biases?}
\subsection{Historical Bias\label{historicalBias}}
While historical biases cannot be measured directly in the specific dataset, there is evidence that PI is susceptible to several pre-existing or present biases. For completeness, we state the main findings of related literature below.

\subsubsection*{\textbf{Physical Activity Inequalities}} In PI, physical activity data, such as step counts, are among the most common digital behavioral biomarkers. Similarly, in the MyHeart Counts dataset, they constitute the majority of the extracted HealthKit data. Specifically, the dataset includes 4920 users of step tracking compared to 626 users of sleep tracking, in line with previous research supporting that many users report not wearing their watch while sleeping \cite{jeong2017smartwatch}. However, inequalities in physical activity are well-reported \cite{guthold2018worldwide,althoff2017large,world2019global}. \citet{althoff2017large} use smartphone mobility data from over 68 million activity days by more than 700K individuals across 111 countries to quantify activity inequality. Their findings reveal variability in physical activity worldwide (measured in average step counts), where reduced activity in females explains a large portion of the observed activity inequality. Similarly, \citet{guthold2018worldwide} report that physical inactivity is twice as prevalent in high-income countries compared to low-income countries and they confirm lower activity levels in women than in men. Overall, the World Health Organization reports that ``girls, women, older adults, people of low socioeconomic position, people with disabilities and chronic diseases, marginalized populations, indigenous people and the inhabitants of rural communities often have less access to safe, accessible, affordable and appropriate spaces and places in which to be physically active'' \cite{world2019global}. Such inequalities, present in the real world, can undoubtedly creep into the behavioral data we build our models on. 

\subsubsection*{\textbf{The Digital Divide}} Similarly, as the world rapidly digitalizes, it threatens to exclude those that remain offline. Almost half the world’s population, the majority of them women or citizens of developing countries, are still disconnected \cite{mohammed2021almost}. Even in the connected world, male internet users outnumber their female counterparts across regions. This ``digital divide'' encompasses even more discrepancies, such as the digital infrastructure quality and connectivity speed in rural or remote areas and the required skills to navigate technology \cite{chetty2018bridging}. Thus, it is evident that data collected from any technological system, including PI, do not capture the entirety of the world population due to pre-existing inequalities in digital access and literacy. 

\subsubsection*{\textbf{BYOD Study Design Biases}} On top of that, PI technologies are attracting attention as novel tools for data collection in clinical research, resulting in newfound demographic imbalances. Studies adopting a bring-your-own-device (BYOD) design, such as MyHeart Counts, are gaining traction because they are more user-friendly (participants use technologies they are already familiar with), achieve better participant compliance, potentially reduce the bias of introducing new technologies, and accelerate data collection from larger cohorts \cite{demanuele2022considerations,cho2022demographic}. However, the BYOD design may not support unbiased data collection from the target population where such technologies are intended to be deployed. In their work, \citet{cho2022demographic} identify significant demographic disparities regarding race (50-85\% white cohorts) in BYOD studies. Their findings align with the reported demographic divide existent in the composition of wearable users. Even though the gap is narrowing, a report by \citet{ericsson2016} documents that the majority of existing users of wearables are fit adults between 25–34 and that whilst females are more likely to own activity trackers, 63\% of smartwatch owners are male. 
Hence, the technology used and the available participant cohorts in PI, especially for studies with BYOD design, such as the one under inspection, subject datasets to the same bias that has been exposed in the activity inequality and the digital divide literature. 


\subsection{Representation Bias\label{representationBias}}

We discuss representation biases across three dimensions: misrepresented, underrepresented, and unevenly sampled populations. 
\subsubsection*{\textbf{Misrepresented Populations}} Representation bias can emerge when the sample population does not reflect the general population (bias in rows). To evaluate for such biases in the MyHeart Counts dataset, we compare the ratios of majority and minority user segments as defined in Table~\ref{tab:protectedAttributes} with the real-world ratios extracted from United States population censuses as the MyHeart Counts recruitment was spread across the country. Specifically, we utilize the United States Census Bureau (gender, race, and age \cite{bureauGender} distributions), the Centers for Disease Control and Prevention (BMI \cite{fryar2020prevalenceBMI,fryar2020prevalenceUnderweightBMI}, joint issues \cite{theis2021prevalenceJOINT}, hypertension \cite{centers2019hypertension}, and diabetes \cite{us2020nationaldiabetes} distributions), and the American Heart Association (heart condition \cite{tsao2022heart} distribution) data to extract the real distributions. Figure~\ref{fig:representation1} showcases the results of this comparison in a radar plot. For example, while in the general United States population, we have approximately 1 female per 1 male (ratio of 1.0 in pink), in the MyHeart Counts HealthKit data, we have 0.2 females per 1 male, highlighting the strong underrepresentation of women in the dataset. The same applies to race, age, and hypertension segments, where the minority classes in the dataset (non-white users, users less than 45, and users with hypertension, respectively) do not reflect real-world ratios. An interesting finding is that, while in the United States, there exist approximately 0.3 underweight, overweight, or obese people for every person with normal weight, in the dataset, this ratio is doubled, in line with the research on BYOD design biases discussed above. Hence, potentially due to historical biases and study design choices, our analysis of the MyHeart Counts data (Figure~\ref{fig:representation1}) provides evidence that PI datasets might not represent the real target population.

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=.95\textwidth]{img/representationRadar.pdf} %
        \caption{Real (pink) versus dataset (green) ratios for different population segments in the MyHeart Counts dataset. The ratio is calculated as the number of the minority class divided by the number of the majority class instances. Larger distances between the two lines indicate larger deviations from the real ratios. We notice that attributes such as gender, age, race, and, to a smaller extent, hypertension and BMI suffer from representation bias.\label{fig:representation1}}
    \end{minipage}\hfill
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=.95\textwidth]{img/representationDIR.pdf} %
        \caption{A bar plot showing the DIR (ratio of base rates) per protected attribute. Values below 0.8 indicate bias against the minority segment, while values above 1.25 indicate bias against the majority segment. We notice that there exist biases in columns for diabetes patients, users with joint issues, and non-white minorities. While the data is borderline biased against women and people with non-healthy weight (underweight, overweight, or obese).\label{fig:representation3}}
    \end{minipage}
\end{figure}

\subsubsection*{\textbf{Underrepresented Populations}} PI datasets can still include underrepresented groups (bias in rows) even if sampled perfectly. Figure~\ref{fig:representation2} shows significant imbalances, measured in the number of samples in the dataset, between minority and majority user segments across almost all protected attributes. We notice that even for representative sampling, e.g., users with joint or heart problems, the minority group is still significantly underrepresented in the data. Thus the model will likely be less robust for those few users with these conditions because it has fewer data to learn from. Overall, we see that the MyHeart Counts HealthKit data are skewed towards \textit{white, fit males}, which needs to be considered in the preprocessing and model-building phases for a fairer machine learning life cycle. Note here that we cannot achieve realistic and equal representation unless the population is equally distributed. Ideally, a PI dataset should be representative of the target population but also large enough to consist of sufficient minority samples. In practice, this is challenging to achieve due to the effort and cost required to build large-scale PI datasets.

\begin{figure}[htb!]
  \centering
  \includegraphics[width=.9\linewidth]{img/representationBar.pdf}
  \caption{A bar plot showcasing the number of samples per user segment split based on various protected attributes. We see significant underrepresentation of minority user segments across almost all attributes.\label{fig:representation2}}
\end{figure}

\subsubsection*{\textbf{Unevenly Sampled Populations}} Even if sampling is representative and equal (e.g., 50\% male and 50\% female users), the dataset can still suffer from representation bias if the sampling method is limited or uneven, e.g., all the males in the sample are highly active, but all females happen to be low active (bias in columns). This is also the case in the MyHeart Counts HealthKit data, as seen in Figure~\ref{fig:representation3}. The figure shows the DIR value (ratio of base rates, i.e., ratio of recorded high number of steps for unprivileged versus privileged groups) per protected attribute. A low value ($DIR<0.8$) indicates a bias against the minority group, and a high value ($DIR>1.25$) indicates a bias against the majority group. For our use case, a value of $DIR<0.8$ means that the sample of the minority group is significantly less active than the sample of the majority group. For example, in the MyHeart Counts data, diabetes patients, users with joint issues, racial minorities, and to a smaller extent, women, racial minorities, and overweight and obese users systematically perform lower step counts in the dataset compared to their majority segment counterparts. On the contrary, users of different age groups with or without hypertension or heart issues do not differ significantly in terms of step counts in the data.

\subsection{Measurement Bias\label{measurementBias}}

In terms of measurement bias, we focus on the input modalities and their accuracy and discrepancies during data collection, i.e., we discuss how, in the MyHeart Counts data, the measurement method and accuracy vary across groups. 

\subsubsection*{\textbf{Device Differences}} In the MyHeart Counts HealthKit dataset, data originate from different sources. Specifically, 33\% comes from an iPhone, 11\% comes from an Apple Watch, and 56\% comes from multiple third parties. iPhones detect and calculate step counts through integrated sensors, such as an accelerometer, gyroscope, GPS, and in some models, a magnetometer. These sensor data are then analyzed by the motion coprocessor unit, namely a low-power unit that reads the sensors' output and makes the data available to applications via Apple's CoreMotion programming interface \cite{appleAppleDeveloper}. Specifically, it communicates with the CMMotionActivityManager \cite{appleAppleDeveloperManager}, which is responsible for classifying whether the user is walking, running, in a vehicle, or stationary for periods of time. However, this process cannot be fully replicated in Apple watches due to inherent differences in placement (pocket versus wrist, fit, and usage habits. For instance, phones are known to underestimate user step counts due to non-carrying time in free-living conditions \cite{amagasa2019well,duncan2018walk}. On the contrary, Apple watches have been tested to be more accurate for measuring daily step counts for healthy adults \cite{veerabhadrappa2018tracking}. 

Moreover, in the MyHeart Counts HealthKit data, there is also a statistically significant difference ($p<0.05$) across segments: in Apple Watch ownership based on gender (46\% of male participants have at least one watch entry compared to 28\% non-males), heart condition (38\% of participants with heart condition compared to 26\% without), and ethnicity (41\% of non-white participants compared to 36\% white).

\subsubsection*{\textbf{Model Differences}} To make things worse, accuracy differences have been reported across consecutive generations of iPhone devices \cite{duncan2018walk}. Incremental hardware changes may increase the quantity, modality, and quality of data available for the device to calculate the CMMotionActivityManager variables, which may improve the accuracy of activity recognition. For instance, iPhone 5S has introduced the M7 coprocessor; iPhones 6 and 6 Plus contain an M8 coprocessor; and 6S, 6S Plus, and SE have an M9 coprocessor, while prior models do not incorporate such a unit. The M8 has introduced the ability to differentiate between different activities \cite{youtubeAppleSeptember}, and the M9 has introduced ``always-on'' capabilities \cite{youtubeAppleSeptember15}. Additionally, newer versions of iOS may provide revised algorithms that improve recognition accuracy. In the MyHeartCounts HealthKit data, we encounter various iPhone models, starting from iPhone 4S (no coprocessor) and reaching iPhone 6S Plus (M9 coprocessor). We analyze whether differences in demographics correlate with differences in phone ownership, and we identify statistically significant differences ($p<0.05$) based on gender and BMI. Specifically, females and people with normal BMI tend to own older and cheaper phones with fewer capabilities (see Figure~\ref{fig:boxplotGenderBMI}).
\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=.7\linewidth]{img/boxplot-gender-phone_price.pdf}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=.7\linewidth]{img/boxplot-bmi_category-phone_price.pdf}
    \end{subfigure}
    \caption{Differences in the price of participants' phones as of September 2016 based on gender (left) and BMI (right). Females and people with BMI within the normal range tend to own older and cheaper phones with fewer capabilities.\label{fig:boxplotGenderBMI}}
\end{figure*}

\subsubsection*{\textbf{General Input Modality Differences}} Finally, most of the MyHeart Counts data comes from third parties, such as alternative wearables that communicate with the Apple Health app or fitness and well-being apps downloaded from the App Store. This is not uncommon in the PI domain, given the abundance and heterogeneity of available data sources. In our use case, we see, beyond the Apple Watch, Garmin, Polar, and Basis Peak wearable products, as well as various apps. %
With regards to third parties usage across segments, we identify statistically significant differences based on gender (91\% of male participants have at least one third-party entry compared to 85\% of non-male ones) and diabetes condition (97\% of participants with diabetes have at least one third-party entry compared to 90\% without). However, different input devices or apps are proven to have different accuracies, likely to create measurement accuracy discrepancies between different users \cite{el2015currently}. 

\begin{mybox1}
\begin{small}
\textbf{Summary of biases in data generation:}
\begin{itemize}[leftmargin=*,nosep]
    \item Pre-existing \textit{historical biases} are also present in digital biomarkers extracted from PI systems, due to well-documented phenomena, such as the global inequality in physical activity and the digital divide, leading to data generation that is not representative of the general population, which is also the case in our MyHeart Counts use case, where female, non-white, underweight, overweight or obese, young, and hypertensive users, are undersampled in the data.
    \item Even within well-sampled user groups, data imbalances, either in terms of user attributes or measured behaviors, are still prevalent due to realistic differences across user segments. %
    Specifically, in our PI use case, we see significant underrepresentation of minority groups across all protected attributes and measured behavioral differences -not necessarily realistic- for users with diabetes, joint issues, non-healthy BMI, non-white users, and females.
    \item PI is susceptible to \textit{measurement biases}, due to the heterogeneity in input modalities (smartphone versus smartwatch), performance and hardware differences across generations of devices, and usage of third-multiple party apps of unknown accuracy. Females are especially affected by such biases in our dataset, as they tend to own older devices with fewer capabilities and use to a greater extend multiple fitness-related third-party apps.
\end{itemize}
Given the awareness of certain historical, representation, and measurement biases in the data, practitioners can make informed decisions concerning appropriate preprocessing actions to alleviate potential negative effects. Such actions may include oversampling minority or undersampling majority user segments for misrepresented or underrepresented populations, choosing the appropriate sampling strategy to balance for unevenly sampled populations, or accounting for measurement differences across different devices or models.
\end{small}
\end{mybox1}

\section{Exploring Bias in Personal Informatics Model Building and Implementation\label{modelBiases}}
Bias in the model building and implementation stream can take the form of aggregation, learning, evaluation, and deployment biases, as seen in Figure~\ref{fig:biasModel}. In this section, we discuss all four sources, providing an answer to RQ3, namely: \textit{Do machine learning models inherit PI data biases? Do they mitigate, propagate, or maybe even amplify them?}
\subsection{Aggregation Bias}\label{aggregation}

We evaluate aggregation bias by plotting the DIR (selection rate, i.e., rate of high activity goals predictions) for different user segments' predictions based on heart conditions, hypertension, joint issues, diabetes, race, BMI, gender, and age. Figure~\ref{fig:DIRbaseline} shows the DIR scores for the segments, comparing data and baseline deep learning models. Specifically, we utilize two baseline models to capture the notions of ``fairness through awareness'' \cite{dwork2012fairness} and ``fairness through unawareness'' \cite{kusner2017counterfactual}. In fairness through awareness, fairness is captured by the principle that similar individuals should have similar classification outcomes. In our use case, the similarity is defined based on user demographics in the absence of other features. In practical terms, the aware model is trained on a feature set that includes protected attributes per user. On the other hand, fairness through unawareness is satisfied if no sensitive attributes are explicitly used in the learning process \cite{verma2018fairness}, namely, the unaware model is trained with features excluding protected attributes.

\subsubsection*{\textbf{Models' Description}} Our baseline models are sourced from prior work in the field of intelligent physical activity prediction, where \citet{bampakis2022ubiwear}, utilizing the MyHeart Counts dataset, benchmarked and evaluated six distinct learning paradigms from traditional machine learning models to advanced deep learning architectures. Their best model, a Long Short-Term Memory (LSTM) recurrent neural network, achieved a Mean Absolute Error (MAE) of 1087 steps, beating previous state-of-the-art approaches by 67\% on the task of physical activity prediction. 

We consider the following setting: we are given a time-series dataset $S=\{S^{G,0}, S^{G,1}\}$ of users segmented into two groups, $G0$ and $G1$, based on protected attribute $G$ (e.g., gender, age, etc.). The user data within each group are denoted as $S^{G,g}=\{s_1^{G,g},\ldots,s_K^{G,g}\}$, where $g=\{0,1\}$ and K is the number of users per group, conditioned on protected attribute $G$. Furthermore, the data of each user are stored as $s_i=\{X_i,y_i\}$, where input time series (step count values) of users $i=1,\ldots,K$, are stored in $X_i\in \mathbb{R}^{D_x x 1}$, where $D_x=48$ (unaware model) is the length (in time steps) of a sample daily activity in the data, or $D_x=56$ (aware model) is the length of a sample daily activity in the data plus the protected attribute features.
Formally, our deep neural network architecture receives as input the users' daily activity samples ($X$) and passes them through LSTM layers with parameters $\theta_l=\{W_l,b_l\}$, weight matrix, and bias, respectively, for each layer $l$, to produce the output $\hat{y}$.


The optimization of the network parameters for LSTM layers is obtained by minimizing the binary cross entropy loss $\alpha_{c}$ defined as:
\begin{gather*} 
\Omega^{*}=\underset{\Omega=\left\{\theta_{1}, \ldots, \theta_{3}\right\}}{\arg \min } \alpha_{c}(\hat{y}, y)=\underset{\Omega=\left\{\theta_{1}, \ldots, \theta_{3}\right\}}{\arg \min } -\frac{1}{N} \sum_{i=1}^{N}\Bigl( y_i\log(\hat{y}_i)+(1-y_i)\log(1-\hat{y}_i) \Bigl)
\end{gather*} 
where $N$ represents the number of training samples \textit{from both datasets $\{S^{G,0}, S^{G,1}\}$.}


\begin{figure}
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/baseline_models_dir.pdf} %
  \caption{A comparison of DIR between data, baseline model with protected attributes in the feature set (aware), and baseline model without protected attributes in the feature set (unaware). We see that the ``one-size-fits-all'' models propagate or, in some cases, amplify existing representation biases.\label{fig:DIRbaseline}}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{img/baseline_models_dir_intersectional.pdf} %
  \caption{A comparison of DIR given the unaware baseline model between user groups defined by a single protected attribute, e.g., gender, versus intersectional user groups defined by two attributes, e.g., gender and diabetes. Intersectional groups are either drawn from the minority or the majority classes for each attribute. The ``one-size-fits-all'' models' amplified biases are even more prevalent in intersectional cases.\label{fig:DIRbaselineIntersectional}}
    \end{minipage}
\end{figure}

We implement the proposed architectures in PyTorch Lightning \cite{falcon2019pytorch}. The hyperparameter tuning is performed using the standard back-propagation algorithm and Adam optimizer with the default parameters \cite{kingma2014adam}. To avoid overfitting in the deep models, we applied dropout with a varying portion of dropping nodes. 

\subsubsection*{\textbf{Single Attribute Biases}} Our findings concerning machine learning model biases measured via DIR, as shown in Figure~\ref{fig:DIRbaseline}, highlight the following:
\begin{enumerate}[leftmargin=*,nosep]
    \item Aware learning models are not foolproof against data biases in most cases (joint issues, diabetes, gender), and even amplify them for certain protected attributes (hypertension). 
    \item Even excluding protected attributes from the training process of unaware models does not guarantee unbiased results in line with prior work \cite{pedreshi2008discrimination}. Specifically, fairness through unawareness is also ineffective due to the presence of proxy features, namely attributes that work like proxies for protected attributes. Through such features, bias propagates from the data to models: for example, a person's walking behavior (measured in step counts) is a good predictor of a person's gender, BMI, and age, which can thus be inferred, despite being hidden during training~\cite{thomas2022fitbit}.
    \item Overall, diabetes patients have the largest bias gap compared to their non-diabetic counterparts, partially attributed to their highly biased training data to start with. Yet, users with hypertension have the largest difference between data and model biases since models trained on seemingly unbiased introduce bias during the learning process.
\end{enumerate}


\subsubsection*{\textbf{Intersectional Biases}} We also examine intersectional biases, as shown in Figure~\ref{fig:DIRbaselineIntersectional}; namely we quantify the biases of the unaware model not only conditioned on an single protected attribute, but also on protected attribute combinations. Specifically, we consider two attributes at a time, and two different combination strategies: \textit{minority-minority vs. rest} (e.g., diabetic women) and \textit{majority-majority vs. rest} (e.g., non-diabetic men). Our results, which we present indicatively keeping the diabetes attribute fixed,  highlight the widening intersectional biases for people who belong to more than one minority (in pink) across almost all attributes (with an exception of BMI, where people with non-healthy BMI are the majority group, despite usually being considered unprivileged in practice). The largest gap appears in people with more than one health condition, such as diabetic heart patients, and diabetic patients aged 65+. At the same time, people who do not belong to any minority groups (in purple), benefit across all attributes.

The trends in aggregation bias indicate that PI models do not tackle diverse user segments equally well, and reflect or even amplify representation biases existing in the data, especially when it comes to intersectional biases. 

\subsection{Learning Bias\label{learningBias}}
In the PI literature, there has been a move toward personalization, straying from the ``one-size-fits-all'' mentality and its shortcomings, as discussed above. Contrary to generic models, personalized models are fine-tuned given the data of a single user or user segment. Accounting for such interindividual variability has been proven to dramatically improve prediction performance in various tasks within the PI domain, such as pain detection, engagement estimation, and stress prediction from ubiquitous devices data \cite{taylor2017personalized,shi2022toward,lopez2017multi}. Given the increasing popularity of the personalization paradigm, in this study, we investigate whether personalization as a modeling choice can amplify performance disparities across different user segments in the data, given the existence of representation bias.

\begin{figure}[htb!]
  \centering
  \includegraphics[width=.7\linewidth]{img/personalizationModel.pdf}
  \caption{Our personalized deep learning architecture inspired by CultureNet \cite{rudovic2018culturenet}. The last layer is indicatively fine-tuned based on gender for female users.\label{fig:personalization}}
\end{figure}
\subsubsection*{\textbf{Model Description}} We base our approach on the work of \citet{rudovic2018culturenet} and the CultureNet package \cite{rudovic2017measuring} for building generalized and culturalized deep models to estimate engagement levels from face images of children with Autism Spectrum Condition. Specifically, we utilize our deep LSTM model, which is trained on data from all users, we freeze the network parameters $\{\theta_1,\ldots,\theta_3\}$ tuned to both minority and majority user groups, as described in Section~\ref{aggregation}, and then fine-tune the last layer ($\theta_4$), i.e., a linear fully-connected layer, to each user group separately based on the MyHeart Counts protected attributes (health condition, hypertension, joint issues, diabetes, race, BMI, gender, age). Figure~\ref{fig:personalization} delineates the personalization process. 

Formally, the learning during the fine-tuning process is attained through the last layer in the network, one for the minority and one for the majority user group. Before further optimization, the group-specific layers are initialized as $\theta_4^{G,0}\leftarrow\theta_4$ and $\theta_4^{G,1}\leftarrow\theta_4$, and then fine-tuned using the data from $G0$ ($S^{G,0}$) and $G1$ ($S^{G,1}$), respectively, \textit{for each protected attribute} $G$ as:
\begin{gather*}
   \left(\theta_{4}^{G,c}\right)^{*}=\underset{\theta_{4}}{\arg \min } -\frac{1}{N} \sum_{i=1}^{N \in S^{G,c}}\Bigl( y_i\log(\hat{y}_i)+(1-y_i)\log(1-\hat{y}_i) \Bigl), \quad c=\{0,1\} \text{ and}\\
   G=\{\text{gender, ethnicity, age, bmi, heart condition, hypertension, joint problem, diabetes}\} 
\end{gather*}

The final network weights, $\theta_l=\{W_l,b_l\}$, are then used to perform the group-specific inference of next-day physical activity level from past behavior per protected attribute. 

\subsubsection*{\textbf{Single Attribute Biases}} While we could not identify significant performance benefits either for the privileged or the unprivileged group by utilizing personalization in our use case, we encountered significant bias shortcomings of the approach. Specifically, across all protected attributes (with a borderline exception of race), we see that personalized models are more biased compared to either aware or unaware models or both. An extreme case appears in users with diabetes, where the personalized model ``learns'' that this user segment is less active than their non-diabetic counterpart in the dataset and thus provides them only with low activity goals, regardless of individual differences in physical activity levels. The intuition behind this behavior is that a personalized model is fine-tuned to a specific user segment, e.g., users with diabetes. If this segment suffers from representation bias in the dataset, which is true in many cases in PI, then personalized models amplify this bias through the fine-tuning process, as is evident in Figure~\ref{fig:DIRbaseline}. Our findings highlight that a common modeling choice in PI, such as personalization, can negatively affect biases and asks for bias-aware personalization approaches to rip the benefits of user tailoring without leading to biased results.

\subsection{Evaluation Bias\label{evaluationBias}}\label{sec:evalBias}
\subsubsection*{\textbf{Benchmark Selection}} In machine learning, models are optimized on their training data, but their quality is often evaluated based on benchmarks, such as ImageNet \cite{deng2009imagenet} in the computer vision community and MovieLens \cite{harper2015movielens} in the recommender systems community. However, the ubiquitous computing community still suffers from a lack of benchmarks, or benchmarks that are limited to traditional tasks, such as human activity recognition \cite{anguita2013public} and sleep classification \cite{zhang2018national,chen2015racial}. To make things worse, oftentimes, benchmarks within the community are not representative of the target population. For example, within the fall detection domain, datasets usually comprise imitated falls performed by younger people while they are deployed on older people \cite{sucerquia2017sisfall}. 

\begin{figure}[htb!]
  \centering
  \includegraphics[width=.45\linewidth]{img/test_sets_dir.pdf}
  \caption{A comparison of DIR between different test sets across models. We see that ``perfect'' test sets in terms of data bias (continuous lines) tend to hide imperfections in the trained models compared to the original test sets (dashed lines).\label{fig:DIRtest}}
\end{figure}

Yet, a misrepresentative benchmark encourages the development and deployment of models that perform well only on the data subset represented by the benchmark. To illustrate our point, given the lack of established benchmarks for our use case, we devise two distinct test sets for comparison purposes: our original (random) test set, $T1$, and a sampled subset of $T1$, $T0$, with demographic parity at base rate ($\text{DIR}=1.0$). We then evaluate our models, namely the baseline aware and unaware and personalized LSTMs, on these two test sets. Figure~\ref{fig:DIRtest} presents the results of our experimentation, where it is clear that $T0$, imitating a ``perfect'', fair world, consistently shows better performance concerning DIR compared to $T1$. Better performance is defined as smaller deviations from the optimal $\text{DIR}$ value of $1.0$. Essentially, an ideal-world benchmark, such as $T0$, is ``hiding'' the imperfections of our trained model, which has been proven to propagate or even amplify biases based on $T1$.

\subsubsection*{\textbf{Evaluation Metric Selection}} On a different note, evaluation bias can also emerge from the choice of metric used to quantify the models' performance. For instance, group fairness hybrid metrics, such as error rates, are prone to imbalances, as discussed earlier, and can hide disparities in other types of bias metrics, such as WAE metrics (see Appendix~\ref{ap:A}). Similarly, aggregate measures, such as accuracy, can hide subgroup under-performance or conceal shortcomings in more important metrics for certain use cases, such as false positive or false negative rate \cite{suresh2021framework}.

\subsection{Deployment Bias\label{deploymentBias}}

\subsubsection*{\textbf{Changing Deployment Scenarios}} We see at least two sources of deployment biases in PI. The first is related to the fact that the most active research areas within PI are Human-Activity Recognition and Sleep Classification. From this lens, FPs and FNs (Type I and Type II errors, respectively) in these scenarios are not critical, and models have been developed to maximize TPs. This dominant but limited view promotes deployment bias in novel use cases with the emergence of health-related intelligence embedded into PI systems. For example, given the novel ECG sensor data and AFib detection functionality, Type II errors should be minimized to avoid loss of life.  It is thus critical to reassess the conceptualization of PI systems' evaluation practices and datasets and tailor them to their context.

\subsubsection*{\textbf{Development in Isolation}} Second, learning models for PI systems are built and evaluated as if they were fully autonomous, while in reality, they operate in a complex socio-ethical system moderated by institutions and human decision-makers, also known as the ``framing trap'' \cite{selbst2019fairness}. Users may share their mHeatlh data with physicians for interpretation and disease management. Despite good performance in isolation, they may lead to harmful consequences because of human biases, such as confirmation bias. Specifically, physicians are more likely to believe AI that supports current practices, and opinions \cite{parikh2019addressing}. At the same time, research shows that physicians’ perceptions about black male patients’ physical activity behavior were significant predictors of their recommendations for coronary artery bypass graft surgery, independent of clinical factors, appropriateness, payer, and physician characteristics \cite{van2006physicians}. Such complicated interconnections highlight how evaluating a system in isolation creates unrealistic notions of its benefits and harms.

\begin{mybox1}
\begin{small}
\textbf{Summary of biases in model building and implementation:}
\begin{itemize}[leftmargin=*,nosep]
    \item Digital biomarkers representation biases are propagated or even amplified by machine (deep) learning models, regardless of the inclusion of protected attributes in the feature set, due to the existence of proxy variables in PI data, e.g., steps, calories, that can be used by the model to infer hidden protected attributes. Such \textit{aggregation biases} are also prevalent in our use case for users with joint issues, diabetes, hypertension, and female users.
    \item Common learning choices in PI, such as personalization, can introduce \textit{learning biases}, if trained on biased data. In extreme cases, as highlighted in our use case for diabetic users, they can even introduce maximum bias, i.e., $\text{DIR}=0$, while performing worse -in terms of bias- across all attributes.
    \item Our empirical results illustrate that model performance is highly susceptible to the representativeness of the PI benchmark used and highlight how \textit{evaluation biases} can affect ubiquitous models in the evaluation phase.
    \item The application of machine learning in PI is not free of \textit{deployment biases}, which can emerge from outdated evaluation practices emerging from the PI systems' early applications or the false assumption of autonomous PI systems' existence.
\end{itemize}
Given the awareness of certain aggregation and learning biases in the data, PI practitioners can make informed decisions concerning the machine learning paradigms or models to utilize or the necessary in-processing bias mitigation steps to apply. Additionally, aware of the evaluation biases present in PI data, they might choose to experiment with more benchmark datasets, evaluate their suitability for their target population apriori and select appropriate accuracy and fairness metrics to quantify performance across different user segments. Finally, they may follow a user-in-the-loop concept during the development phase, acknowledging the dependencies between PI systems and their users.
\end{small}
\end{mybox1}

\section{Generalizability}
\label{generalizability}
This section aims to (i) demonstrate the straightforward applicability of our methodology and our open-source code \cite{yfantidou2023sourcecode} to other datasets and (ii) reveal initial insights about the generality of our findings and future steps.

While our analysis was conducted on the MyHeart Counts dataset, most of our findings can be generalized to other scenarios in PI and mHealth. To showcase this, we apply part of our experiments on two distinct datasets:
\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{LifeSnaps:} LifeSnaps is a newly-released, multi-modal, time- and space-distributed dataset containing 71M rows of anthropological data, collected unobtrusively for the total course of more than four months by 71 participants. Based on data availability, we consider three protected attributes in Lifesnaps, namely gender, age, and BMI. Also, given the lack of official benchmark tasks, namely tasks built specifically on this dataset that are selected to be representative of relevant machine learning workloads and to evaluate competing models, we consider the ``next-day physical activity prediction'' task for model training, same with the MyHeart Counts dataset.
    \item \textbf{MIMIC-III:} MIMIC-III is an established, large-scale clinical dataset consisting of information concerning more than 38K patients admitted to intensive care units (ICU) at a large tertiary care hospital. Based on data availability, in MIMIC-III, we consider six protected attributes, namely gender, ethnicity, language, insurance, religion, and age. Contrary to LifeSnaps or MyHeart Counts, there exists a public benchmark suite that includes four different clinical prediction tasks for MIMIC-III \cite{harutyunyan2019multitask}. For this analysis, we utilize the ``in-hospital mortality'' task as a binary classification equivalent to the ``next-day physical activity prediction'' task.
\end{itemize}

\begin{figure}[htb!]
\centering
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{img/lifesnaps/representationRadar.pdf}  
    \caption{Misrepresented Populations\label{l1}}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[trim={0.3cm 0 0 0},clip, width=1\linewidth]{img/lifesnaps/representationBar.pdf}  
    \caption{Underrepresented Populations\label{l2}}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{img/lifesnaps/representationDIR.pdf} 
    \caption{Unevenly Sampled Populations\label{l3}}
\end{subfigure}
\caption{LifeSnaps data representation biases}
\label{fig:lifesnaps}
\end{figure}
\begin{figure}[htb!]
\centering
\begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=\linewidth]{img/mimic/representationRadar.pdf}  
    \caption{Misrepresented Populations\label{m1}}
\end{subfigure}
\hspace{-11mm}
\begin{subfigure}{.43\textwidth}
    \centering
    \includegraphics[trim={0.3cm 0 0 0},clip, width=1\linewidth]{img/mimic/representationBar.pdf}  
    \caption{Underrepresented Populations\label{m2}}
\end{subfigure}
\begin{subfigure}{.26\textwidth}
    \centering
    \includegraphics[width=\linewidth]{img/mimic/representationDIR.pdf} 
    \caption{Unevenly Sampled Populations\label{m3}}
\end{subfigure}
\caption{MIMIC-III data representation biases}
\label{fig:mimic}
\end{figure}
In exploring biases, we identified both commonalities and differences across PI datasets. Regarding the data generation stream, \textit{representation biases seem to be the norm in PI datasets}, naturally leading to \textit{learning and aggregation biases} in the model building and implementation stream and highlighting the need for increased awareness among researchers and practitioners in the field. Having said that, the identified biases are distinct in each dataset, emerging mostly from their recruitment methodology and the availability of protected attributes. 

\subsubsection*{\textbf{Bias in Rows Commonalities}} All three datasets suffer from some type of ``bias in rows'' as seen in Figures \ref{l1} and \ref{m1}. Specifically, both LifeSnaps and MIMIC-III suffer from misrepresented populations. In LifeSnaps (Figure \ref{l1}), younger people are overrepresented due to university-based recruitment, while in MIMIC-III (Figure \ref{m1}) older people are overrepresented due to ICU-based recruitment. Additionally, while gender and ethnicity representation is improved compared to MyHeart Counts, still white males are overrepresented in all three datasets. 

MIMIC-III, similarly to MyHeartCounts, suffers from underrepresented populations, such as uninsured, non-white, non-English-speaking, or non-christian users (Figure~\ref{m2}). These biases are, in turn, propagated to the baseline learning models, in line with prior work \cite{roosli2022peeking}.
\subsubsection*{\textbf{Bias in Columns Differences}} When ``bias in columns'' is explored, contrary to the MyHeart Counts data, both datasets are evenly sampled in terms of outcome labels, namely physical activity in LifeSnaps and in-hospital mortality for MIMIC-III (Figures \ref{l3} and \ref{m3}, respectively). Hence, these findings may not directly generalize to other PI datasets but are still included in our methodology for completeness and visibility. Specifically, contrary to population demographics which can capture misrepresented and underrepresented groups, an analysis for unevenly sampled populations is not commonly performed during data exploration, whereas in certain cases, such as MyHeart Counts, it could reveal behavioral discrepancies across populations.

Overall, these findings concerning generalizability highlight the need for comprehensive data and model evaluation in PI and, by extension, mHealth. It is high time PI researchers and practitioners looked beyond performance-only metrics to human-centric metrics capturing biases and demographic parity.   


\section{Related Work}\label{related-work}
With the widespread adoption of intelligent systems and applications in our everyday lives, accounting for fairness has gained significant traction in designing and deploying systems. Specifically, fairness has been studied extensively in domains such as natural language processing \cite{bolukbasi2016man,frey2020artificial}, recommender systems \cite{leonhardt2018user,wang2022survey,wang2020faircharge}, and computer vision \cite{buolamwini2018gender,wang2020towards}. Yet, evidence for fairness in the PI setting is lacking.
Closer to PI, fairness research in the healthcare setting is still in its infancy \cite{feng2022fair}. The digitization of medical data has enabled the scientific community to collect large amounts of heterogeneous, multi-modal data and develop machine learning algorithms for a variety of medical tasks. During the process, various fairness limitations have been uncovered based on the three most prominent data types, namely medical image data, structured electronic health record (EHR) data, and textual data.

First, medical imaging has been the most widely used data source for machine learning in healthcare, and biases in them have received attention \cite{jiang2017artificial}.
For example, \citet{larrazabal2020gender} utilize two commonly used X-ray image datasets to diagnose various chest diseases under different gender imbalance conditions and showcase that the minority gender group systematically performs worse than the majority gender group. Similarly, according to \citet{adamson2018machine}, relying on machine learning for skin cancer screening may exacerbate potential racial disparities in dermatology.

On a different note, EHR systems store multi-modal, heterogeneous patient data, such as demographics, diagnoses, and clinical records, and have been used for various tasks such as medical concept extraction, mortality prediction, and disease inference. Regarding EHR data fairness, \citet{meng2021mimic} identify race-level differences in the predictions of neural network models on the MIMIC-IV dataset \cite{johnson2020mimic}, with Black and Hispanic patients being less likely to receive interventions or receiving interventions of shorter average duration. Similarly, \citet{roosli2022peeking} reveal a strong class imbalance problem and significant fairness concerns for Black and publicly insured ICU patients in the same dataset. 

Concerning textual EHR data, \citet{chen2019can} examine clinical and psychiatric notes to predict intensive care unit mortality and 30-day psychiatric readmission. Their analysis reveals differences in prediction accuracy, and biases are present in terms of gender and insurance type for mortality prediction, and insurance policy for psychiatric 30-day readmission. Within the same scope, \citet{zhang2020hurtful} train deep embedding models on medical notes from the MIMIC-III database \cite{johnson2016mimic}, and find that classifiers trained from their embeddings exhibit statistically significant differences in performance, often favoring the majority group regarding gender, language, ethnicity, and insurance status.

Yet, despite the emerging research on fairness in healthcare, its proximity to PI, and the widespread adoption of PI technologies, biases in PI have been barely explored. An initial effort of capturing biases in digital biomarkers is reported by \citet{paviglianiti2020vital}. Their Vital-ECG, a wearable smart device that collects electrocardiogram and plethysmogram signals, is embedded with machine learning algorithms to monitor arterial blood pressure and is found to underestimate the risk of disease in female patients. While this is a first step in uncovering biases in PI, it is far from a complete study of bias in PI. As highlighted by research in other domains, bias has multiple facets that may affect system fairness. To this end, our work aims to raise awareness and set up a systematic approach for a comprehensive analysis of data and machine learning model biases/fairness in PI systems.


\section{Discussion \& Conclusions}\label{discussion}
This paper presents the first-of-its-kind, in-depth study of bias in PI by analyzing the most extensive digital biomarkers data to date. We provide empirical and analytical evidence of sources of bias at every stage of the PI ML development pipeline, from data ingestion to model deployment. 
In response to \textit{RQ1}, we recognize the limitations of hybrid group fairness metrics in overcoming data imbalances and conclude that there is no optimal metric as of now capturing the idiosyncrasies of PI. Additionally, in response to \textit{RQ2} and \textit{RQ3}, we show that bias exists across all stages of the machine learning lifecycle, both in the data generation and model building and implementation streams.  Different user minorities are affected by diverse types of bias, but users with diabetes, joint issues, or hypertension and female users show higher degrees of impact adversity in our MyHeart Counts use case due to representation, aggregation, and learning biases.
Our findings echo concerns similar to those raised in the evaluation for healthcare technologies \cite{ahmad2020fairness}. While some of our findings are specific to the investigated use case, they can, for the most part, be extended to PI tasks more broadly. Below we present limitations of our work that create new opportunities for future research and provide recommendations for future work for studying and mitigating bias in PI.

\subsection{Limitations}
\subsubsection*{Alternative PI Use Cases}
Our work presents the first study of bias in PI research and development and does not study and compare bias in commercial PI systems, such as consumer smartphones and wearables, which we position as a future work direction. This is due to the prevalence of commercial black box models -which can be attributed to competitive advantage in an emerging market- and closed data because of ethical and privacy considerations. Yet, such restrictions have limited our use case, which may not seem as critical as AFib detection, for instance, but provides a strong indication of how PI data and models are susceptible to bias and is large enough to ensure the generalizability of our findings. Hence, our findings should be interpreted with these limitations in mind and not be seen as a generic evaluation of bias across all PI systems. 

\subsubsection*{In-the-wild Data Quantity versus Data Quality}
In our search for large-scale data, we had to partly sacrifice data quality (e.g., missing values, noise, duplicate measurements), as often happens with in-the-wild datasets of the scale of MyHeart Counts. Nevertheless, we engaged in thorough preprocessing methods benchmarking to ensure the best possible quality for our training data, as reported in prior work \cite{bampakis2022ubiwear}. Due to the small sample sizes for certain user groups conditioned on a protected attribute, e.g., Pacific Islanders or American Indians for the ethnicity attribute, we had to binarize all protected attributes to avoid immense imbalances between majority and minority groups. Nevertheless, we recognize that some minority groups might be treated more unfairly than others by the data and algorithms, a fact not captured in the current configuration. On the same note, gender was treated as a binary concept in the MyHeart Counts dataset, and recognizing diverse gender identities was outside of our control for the purpose of this study.

\subsection{Future Work Directions}
\subsubsection*{Inclusive Training and Evaluation Datasets for Real-life Scenarios}
Appropriate PI datasets for fueling future bias research in the domain are still lacking. Due to the sensitivity of the data at hand, many datasets are proprietary with restrictive Institutional Review Board (IRB) agreements. Out of the open PI datasets, most are small-scale \cite{yfantidou2021self} due to the high effort and equipment cost required for building larger datasets or are conducted in-the-lab failing to represent the target population. To this end, any future work publishing open, large-scale, in-the-wild PI data sourced from diverse populations in terms of geographic location, gender, age, and health conditions, is significantly contributing to the advancement of the domain. Also, given the prevalence of small-scale datasets, future work should focus on quantifying biases in small digital biomarkers data, as realistically, most institutions will never acquire big data \cite{baeza2018big}. Additionally, due to the recentness of the domain and the closed-sourced data and algorithms, there is a lack of established benchmarks, especially regarding emerging PI tasks, such as fertility prediction, AFib, or fall detection. To this end, similarly to the work of \citet{harutyunyan2019multitask}, which published benchmarks for electronic health records tasks, future work should create inclusive and representative benchmarks for tasks within the PI domain.
\subsubsection*{Fairness Metrics Capturing PI Idiosyncrasies}
Digital biomarkers are essentially sequential time-series data, inherently different from images and audio, where fairness research is most advanced. Hence, there is work to be done in quantifying bias and identifying idiosyncrasies in sequential physiological and behavioral data. For instance, many PI tasks are formulated as regression problems, but regression-specific fairness metrics are limited in the literature \cite{gursoy2022error}. Beyond that, future work should explore diverse definitions of bias and their suitability for heterogeneous PI tasks. For example, how is demographic disparity defined in AFib detection, where false negatives can be fatal, versus human-activity recognition, where false positives deteriorate the user experience? Also, which metrics are most appropriate in tasks with no clear positive outcome, such as fertility prediction, given the shortcomings of error-based metrics as discussed in Section~\ref{fairnessMetrics}? The latter research gap provides opportunities for future work in redefining error-based fairness metrics that are more robust to representation biases in the data, as is the case in digital biomarkers. 

\subsubsection*{Benchmarking Bias Mitigation Approaches in PI}
While the focus of this work is uncovering the susceptibility of digital biomarkers to data and model biases, there is plenty of work to be done in benchmarking preprocessing, in-processing, and post-processing bias mitigation approaches or developing new ones for capturing the idiosyncrasies of digital biomarkers and their respective PI tasks. PI tasks are undoubtedly different from learning to rank or classification scenarios not only because of the nature of their data but also oftentimes their problem formulation as regression. Yet, fair regression is considerably overlooked compared to fair classification \cite{agarwal2019fair}, and most fairness libraries (AIF360, FairLearn) feature only a few bias mitigation algorithms' implementations for regression tasks. Additionally, PI literature uses discrete machine learning paradigms, such as self-supervised learning, multi-task learning, and personalized learning, among others, whose consequences to algorithm bias are yet to be explored. Finally, due to privacy considerations for sensitive digital biomarkers, many times PI data are not accompanied by protected attributes for the population they describe, making it cumbersome to perform a fairness evaluation. To this end, future work should investigate the space of ``fairness in unawareness'', or, in other words, how you can quantify and mitigate biases in the absence of protected attributes.



\begin{acks}
This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 813162. The content of this paper reflects only the authors' view and the Agency and the Commission are not responsible for any use that may be made of the information it contains. Results presented in this work have been produced using the Aristotle University of Thessaloniki Compute Infrastructure and Resources. The authors would like to acknowledge the support provided by the Scientific Computing Office throughout the progress of this research work.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

\pagebreak
\appendix
\section{Fairness Taxonomy in Machine Learning\label{ap:definitions}}\hfill\\
Viewed through the lens of quantitive science, machine learning research has broadly grouped fairness into two categories: \textit{individual fairness} and \textit{group fairness}. In the broad sense, group fairness partitions the general population into groups based on sensitive (a.k.a. protected) attributes and seeks statistical equality across groups. On the other hand, individual fairness seeks for similar individuals to be treated similarly \cite{dwork2012fairness,mulligan2019thing}. 

In individual fairness, determining whether individuals are similar requires first defining what features are relevant to fairness \cite{fleisher2021s}. However, in PI, it would be incomplete to define such similarity solely based on digital behavioral biomarkers, such as steps, or heart rate, as hidden contextual information might be significantly more relevant. To this end, given access to de-identified aggregated information, we proceed with group fairness metrics and definitions from now onward. This decision translates to our use case as exploring significant differences in allocation, representation, or error rates regarding future step goals across different population segments. For example, do females get systematically lower step goals than their male counterparts?

\begin{figure}[htb!]
  \centering
  \includegraphics[width=.9\linewidth]{img/metricsOrganization.pdf}
  \caption{An organization of fairness metrics in machine learning according to different worldviews: WAE, WYSIWYG, and a hybrid between the two.\label{fig:biasMeasures}}
\end{figure}


Within group fairness, there are still two opposing viewpoints: we’re all equal (WAE) and what you see is what you get (WYSIWYG) \cite{friedler2021possibility,yeom2021avoiding}. The WAE viewpoint considers that all groups have similar abilities to perform the task, e.g., all groups of people are equally capable of walking more, while the WYSIWYG viewpoint holds that the data reflect each group's ability to perform the task, e.g., some groups of people might be less capable of walking more. Group fairness metrics lie under either viewpoint or somewhere in between, as seen in Figure~\ref{fig:biasMeasures}. Overall, every metric tries to quantify -one way or another- the difference in performance between privileged and unprivileged groups of users. We provide detailed definitions, mathematical formulas, PI-specific interpretations, and visual representations of multiple fairness metrics in Appendix~\ref{ap:A}. All metrics' definitions and formulas are taken from the AI Fairness 360 (AIF360) toolkit \cite{aif360-oct-2018}.

\section{Group Fairness Metrics \& Interpretation\label{ap:A}}
This section presents the most common fairness metrics across all viewpoints, namely WAE, WYSIWYG, and hybrid. For each metric, we provide a short definition, a mathematical formulation, and the metric's usage and bias interpretation concerning our use case. The confusion matrix (see Table~\ref{tab:cm}) is the heart of performance measurement in machine learning and is also used in the fairness metrics definitions below.  All metrics are expressed as \textit{ratios} or \textit{differences} between unprivileged (u) and privileged (p) groups. Note that $D$ is the user sample, $\hat{Y}$ is the predicted label, and $\text{pos\_label}$ is what we consider as a positive outcome scenario (e.g., high physical activity). Below, we present the metrics per viewpoint.

\begin{table}[htb!]
\caption{The confusion matrix for performance measurement of models, including standard metrics.}
\label{tab:cm}
\renewcommand{\arraystretch}{2}
\begin{tabular}{lllll}
\cline{3-4}
 &  & \multicolumn{2}{c}{\textbf{True}} &  \\ \cline{3-4}
 &  & \multicolumn{1}{l}{Positive Label} & Negative Label &  \\ \hline
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Predicted}}} & Positive Label & \multicolumn{1}{l}{TP} & FP & \multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}False Discovery Rate\\ FDR = $\frac{FP}{FP+TP}$\end{tabular}} \\ \cline{2-5} 
\multicolumn{1}{c}{} & Negative Label & \multicolumn{1}{l}{FN} & TN & \multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}False Omission Rate\\ FOR = $\frac{FN}{FN+TN}$\end{tabular}} \\ \hline
 &  & \multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}False Negative Rate\\(a.k.a Miss-rate)\\FNR = $\frac{FN}{TP+FN}$\end{tabular}} & \begin{tabular}[c]{@{}l@{}}True Negative Rate\\(a.k.a. Specificity)\\TNR=$\frac{TN}{FP+TN}$\end{tabular} &  \\ \cline{3-4}
 &  & \multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}True Positive Rate\\(a.k.a. Sensitivity)\\TPR = $\frac{TP}{TP+FN}$\end{tabular}} & \begin{tabular}[c]{@{}l@{}}False Positive Rate\\(a.k.a. Fall-out)\\FPR = $\frac{FP}{FP+TN}$\end{tabular} &  \\ \cline{3-4}
\end{tabular}
 \renewcommand{\arraystretch}{1}
\end{table}

\subsection{WAE Metrics}
The WAE viewpoint supports that the data, e.g., measured physical activity, may contain biases -this holds true for physical activity, as we will discuss in a later section-, so their distribution being different across groups should not be mistaken for a difference in ability. The two most commonly used WAE metrics are the \textit{Disparate Impact Ratio (DIR)} and the \textit{Statistical Parity Difference (SPD)} (See Table~\ref{tab:WAEMetrics}).

\begin{table}[hbt!]
\caption{WAE Metrics' definitions, formulas, and task and bias interpretations specific to our use case.}
\label{tab:WAEMetrics}
\resizebox{\columnwidth}{!}{%
\renewcommand{\arraystretch}{2.5}
\begin{tabular}{lp{0.2\textwidth}p{0.25\textwidth}p{0.20\textwidth}p{0.2\textwidth}}
\hline
\textbf{Metric} & \textbf{Definition} & \textbf{Formula} & \textbf{Task Interpretation} & \textbf{Bias Interpretation} \\ \hline
DIR & The ratio of base or selection rates between unprivileged and privileged groups. & $\displaystyle\frac{\Pr(\hat{Y}=\text{pos\_label}\mid\text{D}=\text{u})}{\Pr(\hat{Y} = \text{pos\_label}\mid\text{D}=\text{p})}$ & How many users receive high activity goals in the unprivileged group compared to the privileged group? & A low $DIR$ ($DIR<1$) indicates that the unprivileged user group systematically receives fewer high activity goals. \\ \hline
SPD & The difference in selection rates between unprivileged and privileged groups. & $\displaystyle\Pr(\hat{Y}=\text{pos\_label}\mid\text{D}=\text{u})-\Pr(\hat{Y}=\text{pos\_label}\mid\text{D}=\text{p})$ & Same as above & A low $SPD$ ($DIR<0$) indicates that the unprivileged user group systematically receives fewer high activity goals. \\ \hline
\end{tabular}%
}
\end{table}

\subsection{Hybrid Metrics}
Hybrid metrics lie in-between the two viewpoints. The most commonly used hybrid metrics, depending on the problem's context, are the \textit{False Positive Rate (FPR) Ratio}, the \textit{False Negative Rate (FNR) Ratio}, the \textit{False Omission Rate (FOR) Ratio}, and the \textit{Error Rate Ratio (ERR)} (See Table~\ref{tab:HybridMetrics}).

\begin{table}[htb!]
\caption{Hybrid Metrics' definitions, formulas, and task and bias interpretations specific to our use case.}
\label{tab:HybridMetrics}
\resizebox{\columnwidth}{!}{%
\renewcommand{\arraystretch}{2.5}
\begin{tabular}{lp{0.25\textwidth}p{0.15\textwidth}p{0.20\textwidth}p{0.25\textwidth}}
\hline
\textbf{Metric} & \textbf{Definition} & \textbf{Formula} & \textbf{Task Interpretation} & \textbf{Bias Interpretation} \\ \hline
FPR Ratio & The ratio between the number of negative outcomes wrongly categorized as positive, i.e., false positives (FP), and the total number of actual negative outcomes regardless of classification. & $\displaystyle\frac{FPR_{D = \text{u}}}{FPR_{D = \text{p}}}$ & From all the low active users, how many wrongfully received high activity goals? & A low $FPR$ Ratio ($\text{FPR Ratio}<1$) indicates that the privileged low active user group systematically receives more high activity goals compared to the unprivileged low active user group. \\ \hline
FNR Ratio & The ratio between the number of positive outcomes wrongly categorized as negative, i.e., false negatives (FN), and the total number of actual positive outcomes regardless of classification. & $\displaystyle\frac{FNR_{D = \text{u}}}{FNR_{D = \text{p}}}$ & From all the highly active users, how many wrongfully received low activity goals? & A high $FNR$ Ratio ($\text{FNR Ratio}>1$) indicates that the unprivileged highly active user group systematically receives more low activity goals compared to the privileged highly active user group. \\ \hline
FOR Ratio & The ratio between the outcomes wrongly categorized as negative, i.e., FN, and the total number of classified negative outcomes. & $\displaystyle\frac{FOR_{D = \text{u}}}{FOR_{D = \text{p}}}$ & From all the users that were given low activity goals -rightfully so or not-, how many were actually highly active? & A high $FOR$ Ratio ($\text{FOR Ratio}>1$) indicates that the unprivileged user group systematically receives more wrong low activity goals compared to the privileged user group. \\ \hline
ERR & The ratio between the erroneous outcomes, i.e., FN or FP, and the total number of outcomes. & \begin{tabular}[c]{@{}l@{}}$\displaystyle\frac{ER_{D = \text{u}}}{ER_{D = \text{p}}} \text{, where}$\\ $\displaystyle\text{ER} = \frac{FP+FN}{P+N}$\end{tabular} & How many times was the activity level prediction wrong? & A high $ERR$ ($\text{ERR}>1$) indicates that the unprivileged user group systematically receives more wrong goals -low or high- compared to the privileged user group. \\ \hline
\end{tabular}%
}
\end{table}

\subsection{WYSIWYG Metrics}
The WYSIWYG viewpoint supports that the data, e.g., measured physical activity, correlates well with future activity and that there is a way to use them correctly to compare the abilities of the users. The two most commonly used WAE metrics are the \textit{Average Odds Difference (AOD)} and the \textit{Equal Opportunity Difference (EOD)}.

\begin{table}[htb!]
\caption{WYSIWYG Metrics' definitions, formulas, and task and bias interpretations specific to our use case.}
\label{tab:WYSIWYGMetrics}
\resizebox{\columnwidth}{!}{%
\renewcommand{\arraystretch}{2.5}
\begin{tabular}{lp{0.20\textwidth}p{0.30\textwidth}p{0.20\textwidth}p{0.25\textwidth}}
\hline
\textbf{Metric} & \textbf{Definition} & \textbf{Formula} & \textbf{Task Interpretation} & \textbf{Bias Interpretation} \\ \hline
EOD & The difference of true positive rates between the unprivileged and the privileged groups. & $\displaystyle\text{TPR}_{D = \text{u}} - \text{TPR}_{D = \text{p}}$ & From all the highly active users, how many were actually given high activity goals? & A low $EOD$ ($EOD<-0.1$) indicates that the unprivileged highly active user group systematically receives fewer high activity goals compared to the privileged highly active user group. \\ \hline
AOD & The average difference between the FPR and the TPR between unprivileged and privileged groups. & $\frac{(FPR_{D = \text{u}} - FPR_{D = \text{p}})+ (TPR_{D = \text{u}} - TPR_{D = \text{p}})}{2}$ & TBD & TBD \\ \hline
\end{tabular}%
}
\end{table}

\subsection{Fairness Metrics' Value Ranges}
\begin{figure}[htb!]
  \centering
  \includegraphics[width=.9\textwidth,height=\textheight,keepaspectratio]{img/metricsOverviewV2.pdf}
  \caption{A graphical overview of the fairness metrics discussed.\label{fig:overviewMetrics}}
\end{figure}

\smallskip
Figure~\ref{fig:overviewMetrics} gives a graphical overview of all fairness metrics discussed. We notice that difference-based metrics have a different range of values compared to ratio-based metrics. Specifically, difference-based metrics are within the $[-100\%,+100\%]$ range, while ratio-based metrics are within the $[0,\infty]$ range. For both categories, a value of 1.0 is optimal, indicating demographic parity. Anything greater or less than the optimal value indicates some level of bias. According to AIF360, accepted difference-based metrics' values are within the range $[-0.1,+0,1]$, and accepted ratio-based metrics' values are within [0.8,1.25], but such ranges are not universally accepted and might be adjusted on a task-by-task basis. Table~\ref{tab:metricsValues} indicates which user group, namely privileged or unprivileged, benefits based on a group fairness metric's value.
\begin{table}[htb]%
    \caption{Value interpretation for group fairness metrics. The table shows which user group is treated unfairly -in a negative manner- in each case. UN indicates the unprivileged user group, and PR indicates the privileged user group. UN $\sim$ PR indicates a fair outcome. Notice that the same values may mean different things in the case of ratio-based metrics.}
    \begin{subtable}[t]{.45\textwidth}
        \caption{Ratio-based metrics.\label{tab:metricsValues}}
        \raggedright
         \begin{tabular}{clll}
        \hline
        \multicolumn{1}{l}{} & \multicolumn{3}{c}{\textbf{Value ($v$)}} \\ \hline
        \multicolumn{1}{l|}{\textbf{Metric}} & $v<0.8$ & $0.8<v<1.25$ & $v>1.25$ \\ \hline
        \multicolumn{1}{c|}{\textit{DIR}} & UN & UN $\sim$ PR & PR \\ \hline
        \multicolumn{1}{c|}{\textit{FOR}} & PR & UN $\sim$ PR & UN \\ \hline
        \multicolumn{1}{c|}{\textit{FNR}} & PR & UN $\sim$ PR & UN \\ \hline
        \multicolumn{1}{c|}{\textit{FPR}} & UN & UN $\sim$ PR & PR \\ \hline
        \multicolumn{1}{c|}{\textit{ERR}} & PR & UN $\sim$ PR & UN \\ \hline
        \end{tabular}
    \end{subtable}%
   \begin{subtable}[t]{.45\textwidth}
        \raggedleft
        \caption{Difference-based metrics.}
        \begin{tabular}{clll}
        \hline
        \multicolumn{1}{l}{} & \multicolumn{3}{c}{\textbf{Value ($v$)}} \\ \hline
        \multicolumn{1}{l|}{\textbf{Metric}} & $v<-0.1$ & $-0.1<v<0.1$ & $v>0.1$ \\ \hline
        \multicolumn{1}{c|}{\textit{SPD}} & UN & UN $\sim$ PR & PR \\ \hline
        \multicolumn{1}{c|}{\textit{EOD}} & UN & UN $\sim$ PR & PR \\ \hline
        \multicolumn{1}{c|}{\textit{AOD}} & UN & UN $\sim$ PR & PR \\ \hline
        \end{tabular}
    \end{subtable}
\end{table}

\end{document}
