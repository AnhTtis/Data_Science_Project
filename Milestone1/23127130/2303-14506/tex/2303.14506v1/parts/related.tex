\section{Related Works}

\subsection{Image Restoration}
\noindent\textbf{Super-Resolution.}
Image super-resolution aims to generate a high-resolution image by restoring or enhancing high-frequency details from its low-resolution measurements. Interpolation operators, including nearest, bilinear, and bicubic \cite{Keys1981CubicCI}, are highly efficient and widely used on edge devices, but they often produce blurry results because the interpolation weights are calculated without considering the local structure inside the image. Example-based methods leverage a dataset of LR-HR image patch pairs \cite{DBLP:journals/ijcv/FreemanPC00,DBLP:journals/tmm/XiongXSW13,DBLP:conf/iccv/TimofteDG13,DBLP:conf/accv/TimofteSG14}, or exploit self-similarity inside the LR image \cite{DBLP:conf/iccv/GlasnerBI09}, among which sparse coding methods learn a compact representation of the patches, showing promising results \cite{DBLP:journals/tip/YangWHM10,DBLP:conf/cas/ZeydeEP10}. Other super-resolution methods based on random forests \cite{DBLP:conf/cvpr/SchulterLB15}, gradient filed sharpening \cite{DBLP:journals/tip/SongXLXWG18}, and displacement field \cite{DBLP:journals/tip/WangWP14} are also explored. Nevertheless, these classical super-resolution methods suffer from either unsatisfying visual quality or time-consuming optimization.
With the rise of DNN methods, the community has made impressive progress in the task of super-resolution \cite{DBLP:conf/eccv/DongLHT14,DBLP:conf/cvpr/KimLL16a,DBLP:conf/eccv/DongLT16,DBLP:conf/cvpr/LimSKNL17,DBLP:conf/eccv/AhnKS18,DBLP:conf/cvpr/ZhangTKZ018,DBLP:conf/eccv/ZhangLLWZF18,DBLP:conf/eccv/WangYWGLDQL18,DBLP:conf/cvpr/ChenXTZW19,DBLP:conf/cvpr/XiaoFHCX21,DBLP:conf/cvpr/ChengXC0Z21}. 

% ,DBLP:conf/cvpr/LaiHA017 ,DBLP:conf/cvpr/HarisSU18 DBLP:conf/mm/XiaoXFLZ20, DBLP:conf/mm/XuXYZX21
% However, it comes with a substantial computational burden of numerous floating-point operations. The proposed MuLUT avoids heavy computational costs by cooperating multiple look-up tables, where floating-point computations are replaced with cheap memory access.

% But, computing the sparse representation of the input patch is time-consuming. Following works such as ANR \cite{DBLP:conf/iccv/TimofteDG13}, A+ \cite{}, and RAISR \cite{DBLP:journals/tci/RomanoIM17} are proposed to accelerate the approach. 
% DBLP:journals/cga/FreemanJP02,,DBLP:journals/jvcir/Qiu00 ,DBLP:journals/tip/XiongSW10a,,DBLP:conf/cvpr/YangLC13

% (SR-LUT) 
\noindent\textbf{Demosaicing.} 
Image demosaicing aims to produce colored observation from linear responses of light sensors inside the camera. It can be viewed as a super-resolution problem with a particular color pattern, typically the Bayer pattern. Interpolation-based methods like nearest and bilinear can also be used in image demosaicing. However, they tend to produce artifacts in the region with high-frequency signal changes. Classical methods taking advantage of self-similarity inside the image \cite{DBLP:journals/tip/BuadesCMS09,DBLP:journals/tip/DuranB14} or relying on an optimization process \cite{DBLP:journals/tip/ZhangW05,DBLP:journals/tip/Jeon013} are proposed, but they require an excessive amount of computing time. Recently, DNN methods have been introduced to take advantage of powerful representations learned from large-scale datasets \cite{DBLP:journals/tog/Durand16a,DBLP:journals/tip/KokkinosL19}. 

% and thus limit their usage on edge devices
% DBLP:conf/icip/Wang14,,DBLP:journals/spic/ChangDL15 DBLP:journals/jei/0006WB011 ,DBLP:journals/tip/HirakawaP05,DBLP:conf/iccv/TomasiM98,

\input{pics/fig1_srlut_recap.tex}

\noindent\textbf{Denoising.} 
Image denoising is a long-lived task in low-level vision. Early methods treat the denoising task as an image filtering problem, producing results by modifying transform coefficients \cite{DBLP:conf/icip/SimoncelliA96} or averaging neighboring image pixels \cite{DBLP:journals/pami/PeronaM90}. Tremendous methods are explored to model image priors, including self-similarity \cite{DBLP:journals/ijcv/BuadesCM08,DBLP:journals/pami/ChenP17}, sparse representation \cite{DBLP:conf/iccv/MairalBPSZ09}, gradient filed \cite{DBLP:conf/cvpr/WeissF07}, and Markov random field \cite{DBLP:journals/ijcv/RothB09}. Among them, methods based on self-similarity such as BM3D \cite{DBLP:journals/tip/DabovFKE07} and WNNM \cite{DBLP:conf/cvpr/GuZZF14} show promising results, but the searching process for similar patches is time-consuming. Recently, DNN methods have made significant progress in image denoising, showing the advantage of learning from data \cite{DBLP:journals/tip/ZhangZCM017,DBLP:journals/tip/ZhangZZ18,DBLP:journals/pami/ChenXTZW20,DBLP:conf/cvpr/ChenXL020,DBLP:journals/pami/ZhangTKZF21,DBLP:conf/cvpr/ZamirA0HK0021,DBLP:conf/iccvw/LiangCSZGT21}. 

% DBLP:conf/cvpr/GuoY0Z019,DBLP:conf/eccv/YueZZM20, DBLP:conf/cvpr/BuadesCM05,,DBLP:conf/cvpr/GuZZF14,,DBLP:journals/tip/DongZSL13
% DBLP:journals/mmas/OsherBGXY05, DBLP:conf/eccv/LanRHB06,DBLP:journals/tip/EladA06, DBLP:conf/nips/YueYZM019,
% Although some methods are proposed to enable fast inference \cite{DBLP:journals/tip/ZhangZZ18,DBLP:conf/iccv/GuLGT19,DBLP:conf/aaai/XuZCZWR21}, a substantial computational burden is still required for DNN methods, which limits their application on edge devices. 

\noindent\textbf{Deblocking.} 
Image deblocking, also referred to as compression artifacts reduction, aims to reduce the blocking artifacts caused by the inexact approximations in lossy compression (\emph{e.g.}, JPEG, WebP, and HEIF). Early methods based on filtering \cite{DBLP:journals/tcsv/ListJLBK03,DBLP:journals/spic/WangZL13}, frequency-domain transformation \cite{DBLP:journals/tip/FoiKE07}, and optimization \cite{DBLP:journals/tip/SunC07} are explored. Recently, DNN methods are introduced to the task of image deblocking, making great progress \cite{DBLP:conf/iccv/DongDLT15,DBLP:conf/eccv/GuoC16,DBLP:conf/eccv/Ehrlich0LS20}. 
%  But the progress comes at the cost of numerous computational operations. 

% DBLP:journals/tcsv/LiewY04,DBLP:conf/ijcnn/CavigelliHB17,DBLP:conf/eccv/JancsaryNR12 ,DBLP:journals/pami/ZhangTKZF21,DBLP:conf/cvpr/ZamirA0HK0021,DBLP:conf/iccvw/LiangCSZGT21 ,DBLP:journals/tmm/GalteriSBB19,

%  However, dedicated computing engines are required to execute numerous floating-point operations in DNNs. 

% (learning-based SR methods) 
\subsection{Efficient image restoration} 
DNN methods obtain impressive performance in image restoration tasks, but generally require dedicated computing engines (\emph{e.g.}, GPU and NPU) with high power consumption. Thus, many efforts for efficient image restoration have been conducted. Taking super-resolution as an example, researchers elaborately design lightweight networks, including ESPCN \cite{DBLP:conf/cvpr/ShiCHTABRW16}, FSRCNN \cite{DBLP:conf/eccv/DongLT16}, CARN-M \cite{DBLP:conf/eccv/AhnKS18}, and IMDN \cite{DBLP:conf/cvpr/HuiWG18}, to name a few. General network compression methods like quantization \cite{DBLP:conf/eccv/LiYLZZYJ20,DBLP:journals/tip/ZhangLWZ21}, neural architecture search \cite{DBLP:conf/eccv/LeeDAVK0L20}, network pruning \cite{DBLP:conf/eccv/LiGZGT20}, and AdderNet \cite{DBLP:conf/cvpr/ChenWXSX0X20,DBLP:conf/cvpr/Song0C0XT21} have also been explored for efficient super-resolution. Beyond super-resolution, DNN methods for efficient denoising \cite{DBLP:journals/tip/ZhangZZ18,DBLP:conf/iccv/GuLGT19} have also been reported. However, a substantial computational burden is still required for DNNs to deal with numerous floating-point operations, which limits their application on edge devices with limited hardware resources and energy supply.

Most recently, Jo \emph{et al.} propose SR-LUT \cite{DBLP:conf/cvpr/JoK21} as a new efficient solution. They train a deep super-resolution network with a restricted RF and then cache the output values of the learned super-resolution network to a LUT. At test time, they retrieve the pre-computed HR output values from the LUT for the query LR input pixels. Different from the color-to-color 3D LUTs that are widely used in the image processing pipeline \cite{DBLP:journals/tip/MukherjeeM08,DBLP:journals/tog/MantiukDK08,DBLP:journals/pami/KimLLSLB12} and the image enhancement task \cite{DBLP:journals/pami/ZengCLCZ22,DBLP:conf/iccv/Wang0PMWSY21}, SR-LUT builds spatial-wise mapping between a local patch and its high-frequency counterpart, resulting in a patch-to-patch 4D LUT. However, a single LUT yields inferior performance due to the restriction of the dimension of indexing entries, which equals to the RF of the learned super-resolution network. This is proved to be critical for super-resolution \cite{DBLP:conf/cvpr/GuD21}. Our proposed MuLUT in \cite{mulut_eccv} overcomes the intrinsic limitation of SR-LUT by enabling the cooperation of multiple LUTs. A concurrent work, SPLUT \cite{Ma2022LearningSL}, also improves SR-LUT by cascading more LUTs. However, since the indexing patterns of SPLUT are similar to SR-LUT, a large number of LUTs is required to obtain a modest RF. With novel pattern designs in complementary indexing, MuLUT is able to enlarge RF more effectively. In this paper, we further extend MuLUT to more image restoration tasks with the newly introduced channel indexing and more flexible pattern designs in complementary indexing, which serves as a universal method toward ``DNN of LUTs'' for efficient image restoration.

% , thus resulting in better performance with fewer LUTs.
% , and LatticeNet \cite{DBLP:conf/eccv/LuoXZQLF20}, DBLP:conf/icpr/Chu0MXL20, DBLP:conf/aaai/Song0JCXW20, DBLP:conf/eccv/XinWJLHG20,
% reducing the computational burden of DNN methods are also explored in image
% ,DBLP:conf/aaai/XuZCZWR21

% We extend MuLUT to the image deblocking task to provide an efficient solution to avoid deploying expensive DNNs.
% We apply MuLUT to the image denoising task to show its significant performance advantage over the single-LUT solution and efficiency advantage over DNN methods.
% DBLP:conf/cvpr/LiYLYJ019,,DBLP:conf/cvpr/MeiFZHHS20,
% We adapt MuLUT to the image demosaicing task and show its versatility.