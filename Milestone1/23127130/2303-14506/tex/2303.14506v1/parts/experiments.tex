\section{Experiments and Results}


\subsection{Implementation Details}
We train the MuLUTNet on the DIV2K dataset \cite{DBLP:conf/cvpr/AgustssonT17}, which is widely used in image restoration tasks. The DIV2K dataset contains 800 training images and 100 validation images with 2K resolution. It covers multiple scenes and encapsulates diverse patches. We train the MuLUTNet with the Adam optimizer \cite{DBLP:journals/corr/KingmaB14} in the cosine annealing schedule \cite{DBLP:conf/iclr/LoshchilovH17} at a learning rate of $1 \times 10^{-4}$. We use the mean-squared error (MSE) loss function as the optimization target. The MuLUTNet is trained for $2 \times 10^5$ iterations at a batch size of $32$ and at a patch size of $48$. The cached LUTs are uniformly sampled with interval $2^4$, \emph{i.e.}, from LUT[0-255] to LUT[0-16]. After locating coordinates, the final prediction is obtained with 4D simplex interpolation \cite{DBLP:conf/cvpr/JoK21}, a 4D equivalent of 3D tetrahedral interpolation \cite{DBLP:journals/jei/KassonNPH95}. We further finetune the cached LUTs on the same training dataset for 2,000 iterations with the proposed LUT-aware finetuning strategy. Our method is implemented in the PyTorch library \cite{DBLP:conf/nips/PaszkeGMLBCKLGA19}, and the experiments are conducted on 2 NVIDIA GTX 3090 GPUs. 

% It takes about 6 days to complete the training of the MuLUT-SDYEHO-X2-C configuration.


% Besides, we compute the theoretical energy cost following AdderSR \cite{DBLP:conf/cvpr/Song0C0XT21} to evaluate the efficiency tradeoff of our method as well as other solutions.


\subsection{Efficiency Evaluation}

We conduct efficiency evaluation for different kinds of methods in terms of theoretical energy cost, real-world running time, and storage occupation.

\noindent\textbf{Energy cost.}
Taking single-image super-resolution as an example, we estimate the theoretical energy cost of interpolation, LUT-based, and DNN methods, following the protocol in AdderSR \cite{DBLP:conf/cvpr/Song0C0XT21}. We illustrate the tradeoff between restoration performance (evaluated with PSNR) and energy cost in Fig.~\ref{fig:sr_tradeoff}. As can be seen, with similar energy cost as interpolation and SR-LUT, MuLUT obtains comparable restoration performance to lightweight DNN methods (\emph{e.g.} FSRCNN \cite{DBLP:conf/eccv/DongLT16} and CARN-M \cite{DBLP:conf/eccv/AhnKS18}), achieving a better performance and efficiency tradeoff. We further compare the energy cost with the AdderNet \cite{DBLP:conf/cvpr/Song0C0XT21} and quantized versions of VDSR \cite{DBLP:conf/cvpr/KimLL16a} and CARN \cite{DBLP:conf/eccv/AhnKS18}. We calculate the statistics of multiplications and additions in different data types required by each method and estimate their total energy cost. Our estimation is based on Table~\ref{tab:energy}, where the theoretical energy cost for each operation in different data types is reported. The detailed comparison is listed in Table~\ref{tab:comp}. As can be seen, our method shows superior performance compared with interpolation methods and SR-LUT. For example, MuLUT-SDY-X2 exceeds SR-LUT by 0.6$\sim$0.8dB, while maintaining a similar energy cost. On the other hand, MuLUT maintains a clear energy cost advantage over DNN methods, even their AdderNet and quantized versions. Compared with FSRCNN, A-VDSR-8-bit, and A-CARN-1/4, MuLUT costs about $100 \times$ less energy while achieving comparable restoration performance. 

% In summary, MuLUT achieves a better performance and efficiency tradeoff, boosting the performance of SR-LUT significantly with similar computation costs.



\input{tables/runtime_table_sr.tex}

\input{tables/runtime_table_dn.tex}

\noindent\textbf{Running time.}
Besides, following SR-LUT \cite{DBLP:conf/cvpr/JoK21}, we implement the proposed MuLUT on the widely used ANDROID platform and report the running times of interpolation, LUT-based, sparse coding, and DNN methods in Table~\ref{tab:runtime_sr}. As listed, MuLUT maintains the efficiency of SR-LUT, showing a clear advantage compared to sparse coding methods and DNN methods. Note that the CPU computing architecture is not optimized for LUT, which can be embedded into on-device memory, such as those of image processors in consumer cameras for low-latency execution \cite{DBLP:conf/dac/DengZZY19,9380930}. Moreover, MuLUT can be implemented without modern computing libraries like PyTorch, thus having better practicality on edge devices. For another task, image denoising, we also report the running times of LUT-based, classical, and DNN methods in Table~\ref{tab:runtime_dn}. As can be seen, while LUT-based methods show a clear advantage over classical and DNN methods, MuLUT outperforms SR-LUT significantly in PSNR at a linearly growing cost in running time.


\input{tables/main_table_sr.tex}

\noindent\textbf{Storage occupation.}
Finally, we analyze the extra storage required by different kinds of methods. For LUT-based and sparse coding methods, the storage size is measured by their dictionary and table size. For DNN methods, we report the number of parameters. As listed in Table~\ref{tab:runtime_sr} and Table~\ref{tab:runtime_dn}, the size of MuLUT is similar to that of SR-LUT. Thus, it also can be cached into onboard memory for highly efficient access. In addition, as mentioned above, MuLUT requires simpler dependencies compared to DNN methods, leading to fewer storage requirements for execution libraries.

In summary, MuLUT shows its advantage in efficiency and practicality for real-world deployment on edge devices compared to DNN methods.


\subsection{Performance Evaluation} 

\subsubsection{Image Super-Resolution}


\noindent\textbf{Datasets and metrics.}
For image super-resolution, we evaluate our method on five benchmark datasets: Set5, Set14, BSDS100 \cite{DBLP:conf/iccv/MartinFTM01}, Urban100 \cite{DBLP:conf/cvpr/HuangSA15}, and Manga109 \cite{DBLP:journals/mta/MatsuiIAFOYA17}. For quantitative evaluation, we report peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) \cite{DBLP:journals/tip/WangBSS04}, which are widely used for image quality assessment in terms of restoration fidelity. 



\noindent\textbf{Comparison methods.}
We compare our method with various single-image super-resolution methods, including interpolation-based methods (nearest neighbor, bilinear, and bicubic interpolation), sparse coding methods (NE + LLE \cite{DBLP:conf/cvpr/ChangYX04}, Zeyde et al. \cite{DBLP:conf/cas/ZeydeEP10}, ANR \cite{DBLP:conf/iccv/TimofteDG13}, and A+ \cite{DBLP:conf/accv/TimofteSG14}), SR-LUT \cite{DBLP:conf/cvpr/JoK21}, and DNN methods (FSRCNN \cite{DBLP:conf/eccv/DongLT16}, CARN-M \cite{DBLP:conf/eccv/AhnKS18}, RCAN \cite{DBLP:conf/eccv/ZhangLLWZF18}, and SwinIR \cite{DBLP:conf/iccvw/LiangCSZGT21}). 

\noindent\textbf{Quantitative evaluation.}
The quantitative comparisons of different methods for $\times 2$, $\times 3$, and $\times 4$ super-resolution are listed in Table~\ref{tab:main_sr}. Overall, our method obtains comparable performance with FSRCNN while boosting the performance over SR-LUT significantly. For example, with 2 cascaded stages and 3 parallel blocks, MuLUT-SDY-X2 enlarges the RF size from $3\times 3$ to $9\times 9$, yields 1.1dB PSNR gain over SR-LUT on the Manga109 dataset ($\times 4$) and even exceeds FSRCNN in terms of SSIM. With only complementary indexing, MuLUT-SDY enlarges the RF from $3 \times 3$ to $5 \times 5$, improving the PSNR value over SR-LUT by 0.72dB on the same dataset. 

% Overall, MuLUT obtains comparable or better performance compared with FSRCNN. 



\input{pics/visual_sr_main.tex}

\setcounter{figure}{11}    
\input{pics/visual_dm_main.tex}
\setcounter{figure}{10}    
\input{pics/fig_dm_tradeoff.tex}
\setcounter{figure}{12}    

\setcounter{figure}{13}    
\input{pics/visual_dn_main.tex}
\input{pics/visual_cdn_main.tex}

\noindent\textbf{Qualitative evaluation.}
We compare the visual quality of our method (MuLUT-SDY-X2) with other methods in Fig.~\ref{fig:visual_sr_main}. In the first three examples, SR-LUT produces notable artifacts, \emph{e.g.}, along the border of the wing (\textit{butterfly} form Set5). MuLUT-SDY-X2 achieves similar visual quality to A+ and FSRCNN. In the last three examples, our method is able to generate sharper edges and obtain better visual quality than A+ and FSRCNN, \emph{e.g.}, the eyebrow of the character (\textit{TetsuSan} from Manga109). To sum up, our method achieves better visual quality than SR-LUT and comparable visual quality with A+ and FSRCNN. 



\subsubsection{Image Demosaicing}

\noindent\textbf{Datasets and metrics.}
We sample pixels according to the Bayer pattern to construct synthetic data pairs for image demosaicing, where the mosaiced images are simulated by applying color masks on the original images. We evaluate our method on the widely-used Kodak \cite{Li2008ImageDA} and McMaster \cite{DBLP:journals/jei/0006WB011} datasets. We report the cPSNR metric, which is averaged across three color channels.

%  and McMaster dataset \cite{DBLP:journals/jei/0006WB011} , AHD \cite{DBLP:journals/tip/HirakawaP05}, and LSLCD \cite{DBLP:journals/tip/Jeon013}

\noindent\textbf{Comparison methods.}
Besides the two single-LUT baselines illustrated in Fig.~\ref{fig:dm}, we compare our method with bilinear interpolation, a classical method (D-LMMSE \cite{DBLP:journals/tip/ZhangW05}), and a DNN method (DemosaicNet \cite{DBLP:journals/tog/Durand16a}).

\noindent\textbf{Quantitative evaluation.}
As illustrated in Fig.~\ref{fig:dm_tradeoff}, MuLUT-SDY-X2 and MuLUT-SDY-X2-C improve the performance of single-LUT baselines by a large margin, \emph{e.g.}, over 6.0dB on the Kodak dataset, while achieving a better performance and efficiency tradeoff compared with D-LMMSE and DemosaicNet. 

\noindent\textbf{Qualitative evaluation.}
As shown in Fig.~\ref{fig:visual_dm_main}, the results of Baseline-A are blurry because of subpixel shift, and Baseline-B produces noticeable blocking artifacts due to limited RF, while MuLUT-SDY-X2-C obtains comparable visual quality with computation-heavy D-LMMSE and DemosaicNet.

\input{tables/main_table_dn.tex}

\setcounter{figure}{12}    
\input{pics/fig_cdn_tradeoff.tex}
\setcounter{figure}{15}    


% CBSD68,, and McMaster

\input{pics/visual_db_main.tex}

\input{tables/main_table_db.tex}

\subsubsection{Image Denoising}

\noindent\textbf{Datasets and metrics.}
For grayscale image denoising, we evaluate our method with two benchmark datasets: Set12 and BSD68. For color image denoising, we evaluate our method on three datasets: CBSD68, Kodak24, and McMaster. For quantitative evaluation, we report PSNR for grayscale image denoising, and we report cPSNR for color image denoising.

\noindent\textbf{Comparison methods.}
We compare our method with various single-image denoising methods, including single-LUT solution (SR-LUT \cite{DBLP:conf/cvpr/JoK21}), classical methods (BM3D \cite{DBLP:journals/tip/DabovFKE07}, WNNM \cite{DBLP:conf/cvpr/GuZZF14}, and TNRD \cite{DBLP:journals/pami/ChenP17}), and DNN methods (DnCNN \cite{DBLP:journals/tip/ZhangZCM017}, FFDNet \cite{DBLP:journals/tip/ZhangZZ18}, and SwinIR \cite{DBLP:conf/iccvw/LiangCSZGT21}). We adapt SR-LUT to denoising as the single-LUT baseline by removing its $\mathtt{pixelshuffle}$ operation and retraining it on noisy and clean data pairs.


\noindent\textbf{Quantitative evaluation.}
The quantitative comparisons with other methods for grayscale image denoising at different noise levels are listed in Table~\ref{tab:main_dn}. As can be seen, MuLUT-SDY-X2 ($9 \times 9$ RF) and MuLUT-SDYEHO-X2 ($13 \times 13$ RF) improve the restoration performance over SR-LUT ($3 \times 3$ RF) significantly, showing that the RF size has a crucial influence on the performance of denoising methods. Besides, MuLUT-SDYEHO-X2 approaches a comparable performance with BM3D, \emph{e.g.}, 28.34dB vs 28.57dB on the BSD68 dataset at a noise level of 25. For color image denoising, we illustrate the performance and running time tradeoff in Fig.~\ref{fig:cdn_tradeoff}. As can be seen, MuLUT-SDY-X2 and MuLUT-SDYEHO-X2-C outperform SR-LUT by a large margin, \emph{e.g.}, 3.0dB and 3.8dB gain over single-LUT baseline, respectively, on the Kodak24 dataset at a noise level of 50, while achieving a better performance and efficiency tradeoff compared with CBM3D, DnCNN, and SwinIR. We further investigate the effectiveness of larger RF and channel indexing in Table~\ref{tab:c_cdn_abl}.

% With channel interaction enabled by channel indexing, MuLUT-SDYEHO-X2-C further improves the denoising performance. 


% \input{tables/main_table_cdn.tex}



\noindent\textbf{Qualitative evaluation.}
For grayscale image denoising, we show qualitative comparisons in Fig.~\ref{fig:visual_dn_main}. As can be seen, the results of MuLUT-SDYEHO-X2 are cleaner than those of SR-LUT. With a limited RF, SR-LUT fails to distinguish the noise and signal, resulting in corrupted structures and noisy outputs. MuLUT-SDYEHO-X2 is able to obtain similar visual quality to BM3D and DnCNN. For color image denoising, we show qualitative comparisons in Fig.~\ref{fig:visual_cdn_main}. As can be seen, MuLUT-SDYEHO-X2-C produces cleaner edges than SR-LUT. Moreover, the textures generated by MuLUT-SDYEHO-X2-C are finer than those by CBM3D, since CBM3D tends to produce blurry results where the matching patches are hard to find.


\input{tables/abl_table_net.tex}

\subsubsection{Image Deblocking}

\noindent\textbf{Datasets and metrics.}
For image deblocking, we evaluate our method with two widely used benchmark datasets: Classic5 and LIVE1. For quantitative evaluation, besides PSNR and SSIM, we also report the PSNR-B metric, which is designed to evaluate the blocking effects in images.

\noindent\textbf{Comparison methods.}
We compare our method with various image deblocking methods, including single-LUT solution (SR-LUT \cite{DBLP:conf/cvpr/JoK21}), classical method (SA-DCT \cite{DBLP:journals/tip/FoiKE07}), and DNN methods (ARCNN \cite{DBLP:conf/iccv/DongDLT15} and SwinIR \cite{DBLP:conf/iccvw/LiangCSZGT21}). The single-LUT baseline is obtained similarly to that in denoising.


\noindent\textbf{Quantitative evaluation.}
The quantitative comparisons with other methods for image deblocking at different quality factors are listed in Table~\ref{tab:main_db}. As can be seen, MuLUT-SDY-X2 and MuLUT-SDYEHO-X2 outperform SR-LUT and SA-DCT, especially in terms of PSNR-B. For example, on the LIVE1 dataset at a quality factor of 30, MuLUT-SDY-X2 and MuLUT-SDYEHO-X2 improve SR-LUT by 0.80dB and 0.85dB PSNR-B gain, respectively. Besides, with a larger RF, MuLUT-SDYEHO-X2 outperforms MuLUT-SDY-X2, approaching comparable performance with ARCNN, \emph{e.g.}, 32.84dB vs 33.14dB in terms of PSNR-B on the LIVE1 dataset at a quality factor of 40.

\noindent\textbf{Qualitative evaluation.}
We compare the visual quality of MuLUT-SDYEHO-X2 with other methods in Fig.~\ref{fig:visual_db_main}. As can be seen, the results of SR-LUT contain noticeable blocking artifacts, SA-DCT sometimes loses details and produces over-smooth results (the figures in the third example), while the results of MuLUT-SDYEHO-X2 show similar visual quality to those of ARCNN.

\subsection{Ablation Analysis}

We conduct several ablation experiments to verify the effectiveness of the design principles of MuLUT.


\noindent\textbf{Analysis of the network capacity.} 
We conduct experiments to investigate the influence of network capacity of a MuLUT block. As listed in Table~\ref{tab:net_abl}, for both SR-LUT and MuLUT, the number of filters has a limited influence on the performance. On the other hand, with a similar number of parameters, MuLUT-SDY-X2 (nf.=64) with an RF size of $9 \times 9$ outperforms SR-LUT (nf.=256), showing the critical role of RF size. Besides, the dense connection not only helps the convergence but also boosts the performance.

\input{tables/abl_table_patterns.tex}
\input{tables/abl_table_hierarchical.tex}

\noindent\textbf{The effectiveness of complementary indexing.} 
We conduct experiments with combinations of different indexing patterns of parallel LUTs. As listed in Table~\ref{tab:p_abl}, with MuLUT-S and MuLUT-D working together, MuLUT-SD is able to cover a region of $5 \times 5$, but not all pixels are covered. Still, it significantly improves the performance of SR-LUT and outperforms MuLUT-SSS with the repeating indexing patterns. Besides, we include another ``T'' pattern, where $I_0, I_2, I_5, I_8$ are indexed. Interestingly, the performance of MuLUT-SDT is better than MULUT-SD, even with no new pixels covered, indicating that different indexing patterns convey structure clues. Further, involving the novel ``Y'' shape indexing pattern, MuLUT-SDY covers all pixels in a $5 \times 5$ region and improves the performance, showing the effectiveness of complementary indexing. Similar results can also be observed for involving ``E'', ``H'', and ``O'' patterns. 

% We keep our default configuration as MuLUT-SDY-X2 because of its easily affordable training time.


\noindent\textbf{The effectiveness of hierarchical indexing.} 
We conduct an experiment with cascading different stages of LUTs. As listed in Table~\ref{tab:h_abl}, cascading more stages enlarges the RF steadily, and the performance improves accordingly. Without LUT re-indexing, the performance drops due to the inconsistency between the super-resolution network and the cached LUT. 
Note that cascading LUTs involves \textit{sub-linear} extra computational burden and storage space, since all LUTs except the ones in the last stage cache only one value for each index entry. Furthermore, with both complementary indexing and hierarchical indexing, MuLUT-SDY-X2 achieves better restoration performance over MuLUT-SDY.

%, showing their orthogonal improvement. 


\input{tables/abl_table_hybrid_dm.tex}
\input{tables/abl_table_hybrid_cdn.tex}


\noindent\textbf{The effectiveness of channel indexing.} 
We validate the influence of channel indexing by removing the channel-wise MuLUT block in color image processing. As listed in Table~\ref{tab:c_dm_abl} and Table~\ref{tab:c_cdn_abl}, for both image demosaicing and color image denoising, channel indexing improves the performance at a minor additional energy cost, showing the importance of allowing channel interaction for color image processing.

\noindent\textbf{The effectiveness of LUT-aware finetuning.} 
We compare SR-LUT and MuLUT-SDY-X2 with or without the LUT-aware finetuning strategy. We also report the corresponding network performance. As can be seen in Table~\ref{tab:ft_abl}, there is a performance drop from network predictions to LUT results, especially for the one with larger sampling intervals (3bit LUT). The proposed LUT-aware finetuning strategy is able to fill this gap consistently for different sampling intervals. Especially, after finetuning, a 3bit LUT achieves similar performance compared with a 4bit LUT, while taking $10 \times$ less storage. Further, the proposed MuLUT-SDY-X2 also benefits from the finetuning strategy, showing its effectiveness.


\input{tables/abl_table_ft.tex}

