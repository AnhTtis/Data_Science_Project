\IEEEraisesectionheading{\section{Introduction}\label{sec:intro}}

% Single-image super-resolution (SR) aims to restore a high-resolution (HR) image with high-frequency details from its low-resolution (LR) observation. 
% DBLP:journals/tip/GilboaSZ02,DBLP:journals/tip/GunturkAM04, DBLP:conf/icip/LiJF08 ,DBLP:conf/miccai/ShiCLZBBMDOR13
% DBLP:conf/cvpr/ZhangTKZ018
Image restoration aims to generate high-quality (HQ) visual data with high-frequency details from low-quality (LQ) observations (\emph{e.g.}, downscaled, noisy, and compressed images). Image restoration algorithms enjoy wide applications, ranging from visual quality enhancement \cite{DBLP:conf/cvpr/IraniP92,DBLP:conf/icmcs/XiongSW08}, digital holography \cite{DBLP:journals/ejasp/Zhang06}, satellite imaging \cite{DBLP:journals/tgrs/TatemLAN01}, medical imaging \cite{DBLP:journals/cj/Greenspan09}, and gaming \cite{amdsr,nvidiasr}. Moreover, besides improving image quality, image restoration helps in many other computer vision tasks, \emph{e.g.}, human face recognition \cite{DBLP:conf/cvpr/WangLZS21}, scene understanding \cite{DBLP:conf/fgr/ZhangLX18}, and autonomous driving \cite{DBLP:journals/ijcv/XueCWWF19}.

Recent methods based on deep neural networks (DNNs) \cite{DBLP:conf/eccv/DongLHT14,DBLP:conf/cvpr/KimLL16a,DBLP:conf/eccv/DongLT16,DBLP:conf/cvpr/LimSKNL17,DBLP:journals/tip/ZhangZCM017,DBLP:journals/tip/ZhangZZ18,DBLP:conf/eccv/WangYWGLDQL18,DBLP:journals/pami/ChenXTZW20} have made impressive progress in restoration performance, thanks to their scalability and flexibility from constructing elementary building blocks like convolutional layers. However, superior performance is usually obtained at a cost of heavy computational burden. Although this can be alleviated by elaborate network structures or dedicated computing engines (\emph{e.g.}, GPU and NPU), the hardware cost and power consumption still limit the deployment of existing deep restoration networks. Specifically, the growing number of high-definition screens on edge devices (\emph{e.g.}, smartphones and televisions) calls for a practical restoration solution.

On the other hand, in the image processing pipeline \cite{DBLP:journals/tip/MukherjeeM08,DBLP:journals/tog/MantiukDK08,DBLP:journals/pami/KimLLSLB12}, look-up table (LUT) is a widely-used mapping operator, especially for color manipulation. For sRGB color-wise mapping, the source colors and the corresponding target colors are stored in index-value pairs in a LUT. This way, each pixel can be directly mapped to the target color with highly efficient memory access. An emerging research, SR-LUT \cite{DBLP:conf/cvpr/JoK21}, adopts LUT to image super-resolution by building spatial-wise mapping between low-resolution (LR) patches and high-resolution (HR) patches. Specifically, SR-LUT utilizes a single LUT to cache the exhaustive HR patch values for later retrieval, which are computed in advance by a learned super-resolution network. At inference time, the LR patches sampled from neighboring pixels are compared with indexes in the LUT, and the cached HR patch values are retrieved. This contributes significantly to the power efficiency and inference speed, making SR-LUT a distinct solution other than existing lightweight super-resolution networks \cite{DBLP:conf/eccv/DongLT16,DBLP:conf/eccv/AhnKS18,DBLP:conf/eccv/LiYLZZYJ20}. However, in practice, the size of LUT is limited by the on-device memory. For a single LUT, the size grows \textbf{exponentially} as the dimension of indexing entries (\emph{i.e.}, indexing capacity) increases. This imposes a restriction on the indexing capacity as well as the corresponding receptive field (RF) of the super-resolution network to be cached, which is the main obstacle to performance improvement.

%  on edge devices. Therefore  and capture sensors
% Correspondingly, we propose a generalized neural network block that can be flexibly constructed and easily converted to a LUT.
% devise a cascaded framework to
%  we present a universal method for low-level vision tasks by constructing multiple LUTs like a neural network and obtaining their values through learning from data. Our method 

In this paper, we embrace the merits of LUT and propose a universal method to overcome its intrinsic limitation, by enabling the cooperation of \textbf{Mu}ltiple \textbf{LUT}s, termed MuLUT. Inspired by the construction of a common DNN, we propose three fundamental ways to construct LUTs in the spatial, depth, and channel dimensions. 1) In the spatial dimension, we devise novel \textit{complementary indexing} patterns as well as a general implementation for realizing arbitrary patterns, to construct LUTs in a parallel manner. 2) In the depth dimension, we enable \textit{hierarchical indexing} between cascaded LUTs, by proposing a re-indexing mechanism to link between LUTs from different hierarchies. 3) In the channel dimension, we introduce \textit{channel indexing}, where channel-wise LUTs are inserted between spatial-wise LUTs, to allow cross-channel interaction in processing color images. Besides, we propose a LUT-aware finetuning strategy to mitigate the performance drop after converting the learned neural network to LUTs. In these principled ways, MuLUT takes a single LUT as a network layer and works as a network of LUTs. Thus, the RF of MuLUT can be enlarged on demand. Meanwhile, the total size of MuLUT is \textbf{linear} to its indexing capacity, yielding a practical solution to obtain superior performance at high efficiency.

Extensive experiments demonstrate a clear advantage of our proposed MuLUT compared with the single-LUT solution. On five super-resolution benchmarks, MuLUT achieves up to 1.1dB PSNR improvement over SR-LUT, approaching the performance of the lightweight FSRCNN model \cite{DBLP:conf/eccv/DongLT16}. At the same time, MuLUT preserves the efficiency of the single-LUT solution. For example, the energy cost is about 100 times less than that of FSRCNN. To demonstrate the versatility of MuLUT, we also extend MuLUT to more image restoration tasks, including demosaicing, denoising, and deblocking. Although a single LUT can be built for these tasks similar to SR-LUT, it yields inferior performance due to the restricted RF. Instead, MuLUT enlarges the RF on demand with flexible architecture choices. As a result, compared with the single-LUT solution, MuLUT achieves over 6.0dB cPSNR gain for demosaicing, up to 2.8dB PSNR gain for denoising, and up to 0.85dB PSNR-B gain for deblocking.

A preliminary version of MuLUT appears in \cite{mulut_eccv}, where it is dedicated to efficient super-resolution and also applied to demosaicing. In this paper, we extend MuLUT substantially towards a universal method for efficient image restoration in the following aspects. 1) In the spatial dimension, we devise more indexing patterns to further integrate neighboring information and enlarge the RF. Moreover, we formulate a general implementation to support arbitrary indexing patterns, which increases the flexibility of MuLUT. 2) In the newly introduced channel dimension, we propose to insert channel-wise LUTs between spatial-wise LUTs to allow cross-channel interaction. In this way, MuLUT can cache more complicated neural networks for color image processing. 3) We extend MuLUT to more image restoration tasks, including denoising and deblocking, showing significant advantages over the single-LUT solution and the versatility of our method. 4) We provide a more thorough literature review, a clearer motivation of MuLUT, more comprehensive experimental settings and results, as well as in-depth discussions on open questions.

With the above extensions, the contributions of this work can be summarized as follows:

1) We devise a universal method, named MuLUT, for efficient image restoration by enabling the cooperation of multiple LUTs. Our method treats a single LUT as a network layer and constructs multiple LUTs like a common DNN.

2) We overcome the intrinsic limitation of the single-LUT solution by introducing complementary indexing, hierarchical indexing, and channel indexing. Our method enlarges the receptive field at a linearly growing cost, instead of exponentially. 

3) We propose a series of techniques, \emph{i.e.}, the general implementation for arbitrary patterns, the re-indexing mechanism, and the LUT-aware finetuning strategy, to enable multiple LUTs to be effectively learned from data.

4) Extensive experiments on four representative image restoration tasks demonstrate that MuLUT achieves a significant improvement in performance over the single-LUT solution while preserving a clear advantage in efficiency over DNNs, showing its versatility for wide applications on edge devices.

% 4) We adapt MuLUT to more low-level vision tasks, including image denoising, image deblocking, and image demosaicing, and demonstrate its superiority over SR-LUT, showing the versatility of the proposed method.

%  allows look-up tables to be constructed like a neural network and learned from data.
% improving the performance for color image restoration. In this way, we cooperate with multiple LUTs in the channel dimension, allowing MuLUT to cache more complicated neural networks.
% With channel indexing, we further improve the performance for color image denoising and image demosaicing.
% or the subpixel shift between Bayer-patterned and HR images.
% we cooperate multiple LUTs with complementary indexing and hierarchical indexing, addressing the above issues of the single LUT solution
% versatility for multiple tasks and practicality 