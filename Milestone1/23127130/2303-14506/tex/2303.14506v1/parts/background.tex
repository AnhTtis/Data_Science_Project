

\section{Motivation}
\subsection{Preliminary}

\setcounter{figure}{2}    
\input{pics/fig3_overview.tex}
\setcounter{figure}{1}    
\input{pics/fig2_motivation.tex}
\setcounter{figure}{3}    

LUT is a widely-used mapping operator, especially for color manipulation modules in the image processing pipeline \cite{DBLP:journals/tip/MukherjeeM08,DBLP:journals/tog/MantiukDK08,DBLP:journals/pami/KimLLSLB12}. A LUT is composed of records of indexes and values, which play as lookup indexing entries and interpolation candidates at the inference time, respectively. These paired indexes and values can be stored in the on-device memory, resulting in high execution efficiency. 

Recently, Jo \textit{et al.} proposed SR-LUT, adopting LUT to the task of single-image super-resolution \cite{DBLP:conf/cvpr/JoK21}. As illustrated in Fig.~\ref{fig:srlut_recap}, there are three steps to obtain a LUT for SR. Firstly, A deep super-resolution network with a restricted RF is trained. Then, the output values of the trained super-resolution network are cached into a LUT by traversing all possible inputs and pre-computing all corresponding outputs. The LUT is uniformly sampled to reduce size. Finally, the HR predictions are obtained by locating the minimum grid of the LR input pixels in the sampled LUT and interpolating cached grid values. This way, the exhaustive results of a deep super-resolution network with a $2 \times 2$ RF size can be cached and retrieved by a 4D LUT. A rotation ensemble strategy is also used to further enlarge the RF. This process can be formulated as
\begin{equation}
    \hat{HR}=\frac{1}{4} \sum_{j=0}^{3} R_{j}^{-1}\left(f\left(R_{j}\left(LR\right)\right)\right),
 \end{equation}
\noindent where $f$ denotes the forward function, $R_j$ and $R_j^{-1}$ denote the $j$ times of $\mathtt{rot90}()$ and its inverse operation, respectively. With the rotation ensemble strategy, the RF size of a 4D LUT can be enlarged from $2 \times 2$ pixels to $3 \times 3$. By caching a trained super-resolution network to a LUT, SR-LUT achieves comparable efficiency with bicubic interpolation. 


\input{pics/fig4_pattern.tex}


\subsection{Problem and Our Solution}


% The size of RF plays a critical role across high-level and low-level computer vision tasks, ranging from image classification \cite{DBLP:conf/nips/LuoLUZ16}, object detection \cite{DBLP:conf/cvpr/LinDGHHB17,DBLP:conf/cvpr/SinghD18}, and image super-resolution \cite{DBLP:conf/cvpr/GuD21}. 

Due to a relatively small RF, the performance of SR-LUT is still much inferior to lightweight DNNs like FSRCNN \cite{DBLP:conf/eccv/DongLT16}. Typically, better performance can be expected by enlarging its RF \cite{DBLP:conf/cvpr/GuD21}. On the other hand, in SR-LUT, LUT is adopted to avoid the online computation of a complex function, by caching the pre-computed results. To this end, one needs to traverse \textit{all possible combinations} of input values for the offline pre-computation. Due to the exhaustive combination, the size of cached results in a single LUT grows exponentially with respect to the increasing input dimension. For an 8bit LUT, whose indexes and values are 8bit integers, its size $S$ can be calculated as
\begin{equation} \label{eq:lut_size}
   S = (2^{8-q}+1)^n \times m \mathtt{B},
\end{equation}
\noindent where $q$ is the uniform sampling interval, $n$ the index dimension of the LUT, and $m$ the number of values for each record, \emph{e.g.}, $m=4\times4=16$ for $4 \times$ super-resolution. Thus, the exponential growth of the LUT size makes it unacceptable to obtain better performance by increasing the dimension of a single LUT.

In this work, we address this exponential disaster by enabling the cooperation of multiple LUTs, instead of staying with a single LUT. As shown in Fig.~\ref{fig:motivation}(a), our method reduces the exponential growth of the LUT size to a linear growth as the number of covered pixels increases. Consequently, as illustrated in Fig.~\ref{fig:motivation}(b), at a comparable energy cost, MuLUT outperforms SR-LUT by a large margin, approaching similar restoration performance compared with FSRCNN \cite{DBLP:conf/eccv/DongLT16} but at two orders of magnitude lower energy cost.

