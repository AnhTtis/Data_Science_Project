% \section{Discussion}

% \noindent\textbf{The appropriate receptive field size for real-world usage.}
% In this work, we expand the receptive field of LUT-based methods at a linearly growing cost. The RF can be further enlarged or even global by introducing non-local matching like BM3D \cite{DBLP:journals/tip/DabovFKE07} or attention mechanisms like RCAN \cite{DBLP:conf/eccv/ZhangLLWZF18} and SwinIR \cite{DBLP:conf/iccvw/LiangCSZGT21}. However, these operations require substantially more computational resources than local solutions, as shown in Table~\ref{tab:runtime_sr} and Table~\ref{tab:runtime_dn}. The RF of MuLUT, although limited, is already larger than that of Bicubic interpolation, \emph{i.e.}, $9 \times 9$ vs. $4 \times 4$. Since Bicubic interpolation is verified effective in many application scenarios, the proposed MuLUT, with a larger RF, should perform better. We examine this on 5 benchmark datasets for $4 \times$ SR, where MuLUT \textbf{consistently} outperforms Bicubic on each of 328 test images (+6.72dB PSNR at most and +0.14dB at least). In comparison, FSRCNN with $17 \times 17$ RF outperforms Bicubic by +5.98dB at most and +0.14dB at least. Thus, the limited RF does not weaken the stability of MuLUT. We believe MuLUT is competent for the restoration of diverse images in practical scenarios.

% \noindent\textbf{The performance and efficiency tradeoff of image restoration algorithms.} % locality vs. Global relationship
% There are two ways to achieve a better performance-efficiency trade-off, \emph{i.e.}, reducing the computation cost of performance-oriented methods, \emph{e.g.}, network quantization \cite{DBLP:conf/eccv/LiYLZZYJ20,DBLP:conf/eccv/XinWJLHG20} and network pruning \cite{DBLP:conf/eccv/LiGZGT20}, and improving the performance of highly efficient methods, \emph{e.g.}, bicubic interpolation and SR-LUT. Our work falls into the second group. Our work enables SR-LUT to obtain significantly better performance while preserving its efficiency, and this trade-off point can never be achieved by the original SR-LUT due to the exponential growth.


\section{Conclusion and Future Work}

In this work, we propose a universal method to learn multiple look-up tables from data for image restoration tasks. Our method overcomes the limitation of the receptive field of a single LUT, empowering LUTs to be constructed like a neural network. Extensive experiments on image super-resolution, denoising, deblocking, and demosaicing demonstrate that MuLUT achieves significant improvement in restoration performance over the single-LUT solution while preserving its efficiency. Overall, MuLUT takes a step toward DNN of LUTs, showing its versatility in representative image restoration tasks and practicality for deployment on edge devices.

Nevertheless, compared with modern DNNs, MuLUT is relatively shallow and simple. As shown in our experiments, keeping enlarging RF contributes to better performance. Exploring more elaborated designs of constructing LUTs to obtain larger RF is worth trying. Besides, combining MuLUT with non-local operations like attention mechanisms is promising, too. Another limitation of MuLUT lies in the lack of ability for temporal modelling. Although with the channel indexing and pyramid structure, pixels along the temporal dimension can be involved, it requires a lot of LUTs to obtain a large temporal window. Our future work would include extending MuLUT to video restoration tasks like video super-resolution and denoising.

% The limitation of MuLUT lies in extending the indexing capacity in the local region, lacking the ability to replicate the behavior of more complicated neural networks with global operations like pooling and attention. Our future work would include adapting MuLUT into these challenging operations. 

% Nevertheless, the proposed two ways to generalize SR-LUT are fundamental and have the potential to be applied in other low-level vision tasks, such as image denoising and video super-resolution.

