The widespread usage of high-definition screens on edge devices stimulates a strong demand for efficient image restoration algorithms.
The way of caching deep learning models in a look-up table (LUT) is recently introduced to respond to this demand.
However, the size of a single LUT grows exponentially with the increase of its indexing capacity, which restricts its receptive field and thus the performance.
To overcome this intrinsic limitation of the single-LUT solution, we propose a universal method to construct multiple LUTs like a neural network, termed MuLUT.
Firstly, we devise novel complementary indexing patterns, as well as a general implementation for arbitrary patterns, to construct multiple LUTs in parallel.
Secondly, we propose a re-indexing mechanism to enable hierarchical indexing between cascaded LUTs.
Finally, we introduce channel indexing to allow cross-channel interaction, enabling LUTs to process color channels jointly.
In these principled ways, the total size of MuLUT is linear to its indexing capacity, yielding a practical solution to obtain superior performance with the enlarged receptive field.
We examine the advantage of MuLUT on various image restoration tasks, including super-resolution, demosaicing, denoising, and deblocking. 
MuLUT achieves a significant improvement over the single-LUT solution, \emph{e.g.}, up to 1.1dB PSNR for super-resolution and up to 2.8dB PSNR for grayscale denoising, while preserving its efficiency, which is 100$\times$ less in energy cost compared with lightweight deep neural networks. 
Our code and trained models are publicly available at \url{https://github.com/ddlee-cn/MuLUT}.

% An emerging research, SR-LUT, responds to this demand by marrying the look-up table (LUT) with recent learning-based methods.
% %and obtain their values through learning from data.
% Existing efficient methods like interpolation suffer from unsatisfying results, while recent learning-based methods improve performance at a price of heavy computational burden.
% Moreover, we extend MuLUT to address demosaicing of Bayer-patterned images, surpassing SR-LUT on two benchmarks by a large margin.
% which is widely applied as an efficient operator for color mapping
% build the spatial-wise mapping between image patches.
% \emph{i.e.}, size of the input image patch, of a single LUT is restricted, resulting in inferior 
% In these principled ways, we construct multiple LUTs like a neural network, enlarging the receptive field on demand.  



\begin{comment}
The high-resolution screen of edge devices stimulates a strong demand for efficient image super-resolution (SR).
An emerging research, SR-LUT, responds to this demand by marrying the look-up table (LUT) with learning-based SR methods.
However, the size of a \textit{single} LUT grows \underline{exponentially} with the increase of its indexing capacity. 
Consequently, the receptive field of a single LUT is restricted, resulting in inferior performance.
To address this issue, we extend SR-LUT by enabling the cooperation of \textit{\textbf{Mu}ltiple} LUTs, termed MuLUT.
Firstly, we devise two novel complementary indexing patterns and construct multiple LUTs in parallel.
Secondly, we propose a re-indexing mechanism to enable the hierarchical indexing between multiple LUTs.
In these two ways, the total size of MuLUT is \underline{linear} to its indexing capacity, yielding a practical method to obtain superior performance.
We examine the advantage of MuLUT on five SR benchmarks. MuLUT achieves a significant improvement over SR-LUT, up to 1.1dB PSNR, while preserving its efficiency. 
Moreover, we extend MuLUT to address demosaicing of Bayer-patterned images, surpassing SR-LUT on two benchmarks by a large margin.
\end{comment}
