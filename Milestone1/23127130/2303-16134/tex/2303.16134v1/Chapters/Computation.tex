% !TEX TS-program = pdflatex
% !TEX root = ../ArsClassica.tex

%************************************************
\chapter{Principles of Bayesian Computation}
\label{chp:computation}
%************************************************

\begin{flushright}
\itshape
Anyone who considers arithmetical methods of producing\\
random digits is, of course, in a state of sin. \\
\medskip
--- John von Neumann
\end{flushright}

This chapter introduces the various methods that are used in practice in order to tackle the computational challenges of Bayesian analyses. We begin this journey by discussing some fundamental ideas about the geometry of high--dimensional probability distributions, while gradually introducing the concepts and algorithms that constitute the modern mathematical machinery of Bayesian computation.

\section{Expectation values}

Probability theory teaches us the only well defined way to extract information from probability distributions is through expectation values. By this term, we mean high--dimensional integrals of the form
\begin{equation}
    \label{eq:expectation_value_integral_formula}
    \mathbb{E}_{p}[f] = \int f(\theta) p(\theta) d\theta\,,
\end{equation}
where $p(\theta)$ is the probability density function that often corresponds to the posterior density for problems of scientific inference, $\theta$ signifies the parameters of the distribution, and $f(\theta)$ is the function that we aim to integrate. In this sense, an expectation value of a function $f(\theta)$ over a probability distribution $p(\theta$) is technically a functional of the product of the function and the probability density.

To see why expectation values hold such a central role in scientific parameter inference, let us discuss a few characteristic and common examples that a scientist often has to compute.

\begin{itemize}
    \item \textbf{Mean value} -- Perhaps the most commonly computed expectation value is the mean value. This can be calculated by choosing the function to be $f(\theta)=\theta$, the expectation value then reduces to
    \begin{equation}
        \label{eq:expectation_value_mean}
        \mu \equiv \mathbb{E}_{p}[\theta] = \int \theta\, p(\theta) d\theta\,.
    \end{equation}
    
    \item \textbf{Variance} -- One might also want to compute higher moments of the probability distribution. The first moment is the variance that corresponds to the following expectation value
    \begin{equation}
        \label{eq:expectation_value_variance}
        \sigma^{2} \equiv \mathbb{E}_{p}\left[(\theta - \mu)^{2}\right] = \int (\theta - \mu)^{2}\, p(\theta) d\theta\,.
    \end{equation}
    
    \item \textbf{Marginal distributions} -- Even marginal distribution can be thought of as expectation values. In this case, the function $f$ corresponds to a conditional distribution, for instance
    \begin{equation}
        \label{eq:expectation_value_marginal}
        p(\phi) \equiv \mathbb{E}_{p}\left[p(\phi |\theta)\right] = \int p(\phi |\theta) p(\theta) d\theta \,.
    \end{equation}
\end{itemize}

\section{Quadrature and uniform grids}

\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{grid_1d.pdf}}
    \caption{Uniform grid approximation of 1--dimensional probability distribution.}
    \label{fig:grid_1d}
\end{figure}
Having discussed a number of examples of commonly used expectation values, we can now turn our attention to the methods that are used for their computation. As we mentioned before, these expectation values are defined as high--dimensional integrals. As those integrals are not generally tractable analytically, one might attempt to approximate their value by means of a Riemann sum over a discreet grid of $n$ points:
\begin{equation}
    \label{eq:riemann_sum}
    \mathbb{E}_{p}[f] = \int f(\theta) p(\theta) d\theta \approx \sum_{i=1}^{n}f(\theta_{i})p(\theta_{i})\Delta\theta_{i}\,,
\end{equation}
where
\begin{equation}
    \label{eq:riemann_interval}
    \Delta \theta_{i} = \theta_{j+1} - \theta_{j}\,,
\end{equation}
is simply the interval between two subsequent points, $\theta_{j}$ and $\theta_{j+1}$ on the underlying grid, and
\begin{equation}
    \label{eq:riemann_midpoint}
    \theta_{i} = \frac{\theta_{j+1} + \theta_{j}}{2}\,,
\end{equation}
is just the mid--point between $\theta_{j}$ and $\theta_{j+1}$.

In principle, this idea can be extended to high dimensions by replacing the 1--dimensional intervals $\Delta \theta_{i}$ with D--dimensional hypercubes. Figure \ref{fig:grid_2d} shows one such example for a 2--dimensional probability distribution. However, as the number of dimensions increases, one immediately has to face a significant difficulty, the \emph{curse of dimensionality}~\parencite{bellman1966dynamic}. Already in 2 dimensions we require $n^{2}$ grid points to approximate the distribution. As it turns out, the number of grid points required for the evaluation of the Riemann sum increases exponentially with the number of dimensions, rendering this method of computing expectation values unusable for $D>3$. Overcoming the difficulties imposed by the \emph{curse of dimensionality} is one of the key goals of \textit{probabilistic computing}.

\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{grid_2d.pdf}}
    \caption{Uniform grid approximation of 2--dimensional probability distribution.}
    \label{fig:grid_2d}
\end{figure}

In order to reduce the computational cost of estimating expectation values in high dimensions, we need to find a way to focus our effort and computation only on those regions of parameter space that are relevant for the integral that we aim to evaluate. One simple idea would be to remove any points of the grid that the integrand $f(\theta)p(\theta)$ is very close to zero. Applying this technique would certainly reduce the total computational cost since only a few grid--cells have a non--negligible value of $f(\theta)p(\theta)$ as shown in Figure \ref{fig:grid_2d_highlighted}. The problem that we face however is that by focusing our attention on $f(\theta)p(\theta)$ we ignore a key factor in the estimation of any expectation value, the \textit{volume}.

\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{grid_2d_highlighted.pdf}}
    \caption{Uniform grid approximation of 2--dimensional probability distribution with highlighted the grid--cells that actually contribute to the calculation of an expectation value.}
    \label{fig:grid_2d_highlighted}
\end{figure}

\section{Geometry of high--dimensional spaces}

The concepts of volume and distance in high--dimensional spaces defy our everyday intuition in ways that matter for the computation of expectation values. To understand this, we will go through an example that illustrates these peculiar effects.

Let us assume that we inscribe a circle of radius $R$ inside a square of side $2 R$. We are interested in computing the area of the circle as a fraction of that of the square. We can get to the result easily using basic geometry, in particular, the ratio of the two areas is
\begin{equation}
    \label{eq:area_ratio_sphere_2d}
    \frac{A_{\textrm{circle}}}{A_{\textrm{square}}} = \frac{\pi R^{2}}{(2 R)^{2}} = \frac{\pi}{4}\,.
\end{equation}

We can now extend the same problem into three dimensions, in which we have a sphere of radius $R$ inscribed in a cube of side $2 R$. The ratio of the volume of the sphere to the volume of the cube is simply
\begin{equation}
    \label{eq:volume_ratio_sphere_3d}
    \frac{V_{\textrm{sphere}}}{V_{\textrm{cube}}} = \frac{\frac{4}{3}\pi R^{3}}{(2 R)^{3}} = \frac{\pi}{6}\,.
\end{equation}

By comparing equations \ref{eq:area_ratio_sphere_2d} and \ref{eq:volume_ratio_sphere_3d} one realises that the volume ratio has decreased going from $2$ dimensions to $3$. We will now show that this result in fact holds for any number of dimensions $D$. In $D$ dimensions, the volume of a hyper--sphere is given by
\begin{equation}
    \label{eq:volume_sphere_nd}
    V_{\textrm{sphere}} = \frac{\pi^{D/2}}{\Gamma \left( \frac{D}{2} + 1 \right)} R^{D}\,,
\end{equation}
where $\Gamma$ is Euler's gamma function which extends the factorial operation to non--integer arguments and satisfies
\begin{equation}
    \label{eq:gamma_function_integer}
    \Gamma (D) = (D-1)!\,,
\end{equation}
for positive integer $D$, and
\begin{equation}
    \label{eq:gamma_function_halfinteger}
    \Gamma \left(D+\frac{1}{2}\right) = \left(D-\frac{1}{2}\right)\times\left(D-\frac{3}{2}\right)\times\dots\times\frac{1}{2}\times\pi^{1/2}\,,
\end{equation}
for non--negative integer $D$. 

The volume of a hyper--cube in $D$ dimensions is simply
\begin{equation}
    \label{eq:volume_cube_nd}
    V_{\textrm{cube}} =(2 R)^{D}\,.
\end{equation}
Taking the ratio of the terms of equations \ref{eq:volume_sphere_nd} and \ref{eq:volume_cube_nd} yields
\begin{equation}
    \label{eq:volume_ratio_sphere_nd}
    \frac{V_{\textrm{sphere}}}{V_{\textrm{cube}}} = \frac{\pi^{D/2}}{2^{D}\Gamma \left( \frac{D}{2} + 1 \right)}\,.
\end{equation}

Figure \ref{fig:volume_ratio_sphere_nd} shows the ratio of the volume of a hypersphere to a hypercube as a function of the number of dimensions $D$. As the number of dimensions $D$ increases, the volume ratio of equation \ref{eq:volume_ratio_sphere_nd} asymptotically approaches $0$. This means that in high dimensions, almost all of the volume of a hypercube is concentrated in the corners.

\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{volume_ratio_sphere_nd.pdf}}
    \caption{The ratio of the volume of a hyper--sphere of radius $R$ to the volume of a hyper--cube of edge size $2 R$ as function of the number of dimensions $D$.}
    \label{fig:volume_ratio_sphere_nd}
\end{figure}

\section{Concentration of measure}

As we will see shortly, the strange behaviour of volume is of paramount importance in the calculation of expectation values over probability distributions. To understand this one need to think not about the probability density but instead about the probability mass. When evaluating an expectation value, not all regions of parameter space are contributing equally to the value of the integral. In fact, the contribution from some regions of parameter space dominates the calculation. We only need to take a look into the form of the expectation value integral to notice that is essentially the product of three terms that contributes. These terms are the function $f(\theta )$, the probability density function $p(\theta )$, as well as the differential volume element $d\theta$. In other words, it is the product of these three terms and their dependence on $\theta$ that determines the value of the integral. Assuming that the function $f(\theta )$ is well behaved, we can ignore its presence for a while.

For the sake of simplicity let us assume that the probability distribution is characterised by an D--dimensional Gaussian probability density function
\begin{equation}
    \label{eq:gaussian_pdf}
    p(\theta ) = \det(2\pi\Sigma)^{-\frac{1}{2}} \exp{\left[-\frac{1}{2}(\theta-\mu)^{T}\Sigma^{-1}(\theta-\mu)\right]}\,,
\end{equation}
where $\mu$ is the mean and $\Sigma$ is the covariance matrix of the distribution. Without loss of generality let us also assume that the density is centred at zero (i.e. $\mu = 0$) and the covariance matrix diagonal with the elements of its diagonal equal to $\sigma^{2}$, meaning that equation \ref{eq:gaussian_pdf} simplifies into
\begin{equation}
    \label{eq:gaussian_pdf_unit_variance}
    p(\theta ) = \frac{1}{\sqrt{(2\pi)^{D}}\sigma^{D}}\exp{\left(-\frac{|\theta|^{2}}{2\sigma^{2}}\right)}\,.
\end{equation}
Assuming further spherical coordinates, the density only depends on the magnitude $r$ of the $\theta$ parameter vector
\begin{equation}
    \label{eq:gaussian_pdf_unit_variance_spherical}
    p(r) = \frac{1}{(2\pi)^{\frac{D}{2}}\sigma^{D}}\exp{\left(-\frac{r^{2}}{2\sigma^{2}}\right)}\,.
\end{equation}
Keep in mind that $p(r)$ is not a probability density function of the magnitude $r=\vert\theta \vert$, but a D--dimensional density of $\theta$.

Let us now turn our attention to the differential volume element $dV$. Differentiating equation \ref{eq:volume_sphere_nd} that provides the volume of the hyper--sphere we get
\begin{equation}
    \label{eq:differential_volume_sphere_nd}
    dV = \frac{D \pi^{D/2}}{\Gamma \left( \frac{D}{2} + 1 \right)} r^{D-1} dr\,.
\end{equation}
\begin{figure}[H]
    \centering
	\centerline{\includegraphics[scale=0.65]{differential_volume.pdf}}
    \caption{Scaling of differential volume with the number of dimensions as a function of distance.}
    \label{fig:differential_volume}
\end{figure}

The differential probability mass $dm(r)$ is then just the product of $p(r)$ and $dV$ given by equations \ref{eq:gaussian_pdf_unit_variance_spherical} and \ref{eq:differential_volume_sphere_nd} respectively
\begin{equation}
    \label{eq:differential_mass_sphere_nd}
    dm(r) = \frac{D r^{D-1}}{\Gamma \left( \frac{D}{2} + 1 \right)2^{\frac{D}{2}}\sigma^{D}} \exp{\left(-\frac{r^{2}}{2\sigma^{2}}\right)} dr\,.
\end{equation}
The differential mass $dm(r)$ has a clear physical meaning, that of the probability mass enclosed in a hyper--spherical shell of radius $r$ and width $dr$. The probability mass differential $dm(r)$ peaks (i.e. is maximised) at the typical radius
\begin{equation}
    \label{eq:typical_radius}
    r_{\textrm{peak}} = \sqrt{D-1}\sigma\,.
\end{equation}
Equation \ref{eq:typical_radius} indicates that while in 1--D the probability mass peaks at $r_{\textrm{peak}}=0$, in higher dimensions this is not the case. As the number of dimensions increases the radius in which the probability mass peaks moves to greater distances. Table \ref{tab:typical_radius} shows the typical radius of the probability mass for a different number of dimensions for our problem. This is a direct consequence of the rapid increase of the differential volume for large $r$ values.
\begin{figure}[H]
    \centering
	\centerline{\includegraphics[scale=0.65]{typical_set_unscaled.pdf}}
    \caption{Scaling of differential probability mass with the number of dimensions as a function of distance.}
    \label{fig:typical_set_unscaled}
\end{figure}

\begin{table}[h!]
    \centering
    \caption{The typical radius $r_{\textrm{peak}}$ as function of the number of dimensions $D$.}
    \def\arraystretch{1.1}
    \begin{tabular}{cc}
        \toprule[0.75pt]
        Number of dimensions $D$ & Typical radius $r_{\textrm{peak}}$ \\
        \midrule[0.5pt]
        $1$ & $0$  \\
        $2$ & $1\sigma$  \\
        $5$ & $2\sigma$  \\
        $10$ & $3\sigma$  \\
        $17$ & $4\sigma$  \\
        $26$ & $5\sigma$  \\
        \bottomrule[0.75pt]
    \end{tabular}
    \label{tab:typical_radius}
\end{table}

In general, we expect the probability mass to form a hyper--shell of mean radius 
\begin{equation}
    \label{eq:shell_mean_radius}
    r_{\textrm{mean}} \equiv \mathbb{E}_{p}[r] = \int_{0}^{+\infty}r dm(r)
\end{equation}
and width (i.e. standard deviation) 
\begin{equation}
    \label{eq:shell_width}
    \Delta r \equiv  \sqrt{\mathbb{E}_{p}[(r-r_{\textrm{mean}})^{2}]} = \sqrt{\int_{0}^{+\infty}(r-r_{\textrm{mean}})^{2} dm(r)}
\end{equation}

\begin{figure}[H]
    \centering
	\centerline{\includegraphics[scale=0.65]{typical_set_scaled.pdf}}
    \caption{Scaling of differential probability mass with the number of dimensions as a function of distance normalised by the square root of the number of dimensions.}
    \label{fig:typical_set_scaled}
\end{figure}

\section{Typical set}

\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{typical_set.pdf}}
    \caption{Illustration of the typical set as the region in parameter space that the product of probability density and differential volume is non--negligible.}
    \label{fig:typical_set}
\end{figure}
The qualitative conclusions of the previous section are general and hold for any continuous probability distribution. The probability mass does not concentrate close to the mode where the probability density is high as there is not sufficient volume there. On the other hand, it does not concentrate on large distances because the density vanishes. Instead, it compromises on some region of intermediate distance surrounding the mode, as shown in Figure \ref{fig:typical_set}. This region is called the \textit{typical set}, and has the form of a high--dimensional thin hyper--shell surrounding the mode as shown in Figure \ref{fig:shell}. In high dimensions, the typical set exhibits the effect of \textit{concentration of measure}~\parencite{ledoux2001concentration} illustrated in Figure \ref{fig:typical_set_scaled}. 
\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{shell.pdf}}
    \caption{Illustration of the typical set as a thin hyper--shell surrounding the mode of the probability distribution.}
    \label{fig:shell}
\end{figure}

The concept of the typical set is not only important for properly understanding probability distributions, but also for developing new computational methods. The notion of the typical set is originally borrowed from the field of information theory, in which one of the main tasks is to compress and encode a \textit{message} with as few \textit{words} as possible. In probability theory, the typical set defines the most efficient way to compress a probability distribution by focusing on a limited region of parameter space. As we will see in the next sections, the task of developing powerful and effective computational methods comes down to how efficiently we can locate and approximate the typical set of a probability distribution.


\section{Laplace approximation}
\label{sec:laplace_approximation}

\looseness=-1 Before we move on to stochastic estimators of expectation values let us first discuss another simple deterministic method, called \textit{Laplace approximation}, that, unlike \textit{quadrature in a uniform grid}, can extend to higher dimensions~\parencite{tierney1986accurate}. The \textit{Laplace approximation} makes a very strong assumption about the target probability distribution. In particular, it assumes that it can be sufficiently described by a Gaussian probability density, similar to equation \ref{eq:gaussian_pdf}. The mean of the Gaussian density is determined at the point of the mode of the target density
\begin{equation}
    \label{eq:mode_of_target}
    \mu = \underset{\theta}{\mathrm{arg\,max}} ~p(\theta)\,,
\end{equation}
and the precision matrix $\Sigma^{-1}$ (i.e. inverse of the covariance matrix $\Sigma$) is given by the second-order derivatives of the negative logarithm of the target probability density function evaluated at the mode,
\begin{equation}
    \label{eq:precision_of_target}
    \left( \Sigma^{-1}\right)_{ij} = -\frac{\partial^{2}}{\partial\theta_{i}\partial\theta_{j}} \log p(\theta)\bigg\vert_{\theta=\mu}\,.
\end{equation}

\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{laplace.pdf}}
    \caption{Illustration of the Laplace approximation to a skewed probability density.}
    \label{fig:laplace}
\end{figure}
The reasoning behind this approach is quite simple, one effectively performs a Taylor expansion of the logarithm of the density, up to second order, around the \textit{maximum a posteriori} point $\mu$,
\begin{equation}
    \log p_{L}(\theta) = \log p(\theta=\mu) -\frac{1}{2}(\theta-\mu)^{T}\Sigma^{-1}(\theta-\mu)+\dots\,,
\end{equation}
where the first order term simply vanishes because we evaluate the expansion around the maximum. For this reason, this very common method is often called the saddle--point approximation. Expectation values can then be determined using the Gaussian density $p_{L}(\theta)=\mathcal{N}(\theta\vert\mu,\Sigma)$ in place of the target density $p(\theta)$ in the formula for the expectation value \ref{eq:expectation_value_integral_formula},
\begin{equation}
    \label{eq:expectation_value_laplace}
    \mathbb{E}_{p_{L}}[f]=\int f(\theta) \mathcal{N}(\theta\vert\mu,\Sigma) d\theta \,.
\end{equation}

The quality of the \textit{Laplace approximation} is determined by the overlap of the typical set of the target distribution with that of the Gaussian approximation. The greater the overlap, the more accurate the approximation will be.

\section{Monte Carlo estimators}

Another type of estimators is \textit{stochastic} estimators, and in particular \textit{Monte Carlo} estimators~\parencite{brooks2011handbook} that rely on a collection of independent points or samples,
\begin{equation}
    \label{eq:samples}
    \lbrace \theta_{1},\dots, \theta_{n} \rbrace \in \Theta\,,
\end{equation}
from the distribution $p(\theta)$, such that the ensemble average of a function $f(\theta)$,
\begin{equation}
    \label{eq:ensemble_average}
    \hat{f}_{n}^{MC} = \frac{1}{n}\sum_{i=1}^{n}f(\theta_{n})\,,
\end{equation}
\textit{asymptotically} converges to the corresponding expectation value
\begin{equation}
    \label{eq:ensemble_average_expectation}
    \lim_{n\to\infty}\hat{f}_{n}^{MC} = \mathbb{E}_{p}[f(\theta)]\,.
\end{equation}

The asymptotic result of equation \ref{eq:ensemble_average_expectation} is not particularly useful as a computational algorithm will never be able to produce infinite samples. Fortunately, the behaviour of Monte Carlo estimators can be quantified even for finite samples.

For any square--integrable (i.e. both $\mathbb{E}_{p}[f]$ and $\mathbb{E}_{p}[f^{2}]$ exist and are finite) real--valued function $f(\theta)$, the Monte Carlo estimator satisfies the \textit{central limit theorem},
\begin{equation}
    \label{eq:monte_carlo_clt}
    \frac{\hat{f}_{n}^{MC} - \mathbb{E}_{p}[f(\theta)]}{\textrm{MC--SE}_{n}[f]}\sim \mathcal{N}(0,1)\,,
\end{equation}
where $\textrm{MC--SE}_{n}[f]$ is the \textit{Monte Carlo Standard Error} defined as,
\begin{equation}
    \label{eq:monte_carlo_standard_error}
    \textrm{MC--SE}_{n}[f] = \sqrt{\frac{\textrm{Var}_{p}[f]}{n}}\,.
\end{equation}
This means that we can estimate the expected number of samples that is required to reach a certain level of precision for our estimates.

\begin{figure}[H]
    \centering
	\centerline{\includegraphics[scale=0.65]{shell_samples.pdf}}
    \caption{Illustration of the typical set including samples generated using exact Monte Carlo sampling.}
    \label{fig:shell_samples}
\end{figure}
Another interesting property of Monte Carlo estimators is that their precision, as quantified by the Monte Carlo Standard Error of equation \ref{eq:monte_carlo_standard_error}, does not depend on the dimensionality of the problem but relies only on the number $n$ of samples instead. This means that Monte Carlo estimators can be applied even in high--dimensional problems. This insensitivity to the \textit{curse of dimensionality} is directly related to the fact that the Monte Carlo samples are already distributed in the typical set as shown in Figure \ref{fig:shell_samples}. As we will discover shortly, once we discuss more advanced methods, the difficult part is to get the samples to the typical set in the first place.

We can also think of the Monte Carlo samples as a stochastic grid where the computation is mostly focused in the regions of parameter space that contribute to the computation of the expectation value. Starting from the Monte Carlo estimator,
\begin{equation}
    \label{eq:ensemble_average2}
    \hat{f}_{n}^{MC} = \frac{1}{n}\sum_{i=1}^{n}f(\theta_{n})\,,
\end{equation}
and manipulate it into the quadrature form
\begin{equation}
    \label{eq:quadrature2}
    \hat{f}_{n}^{MC} = \sum_{i=1}^{n}f(\theta_{i})p(\theta_{i})\frac{1}{n p(\theta_{i})}\,,
\end{equation}
where $\Delta\theta_{i}=1/n p(\theta_{i})$ is the effective volume of each sample.

Monte Carlo estimators are very powerful methods assuming that one can generate independent samples from the target distribution. However, in most interesting and realistic cases, this is not feasible. In that case, one has to rely to alternative methods.

\section{Importance sampling}

One alternative method to exact Monte Carlo sampling, that does not rely on exact samples from the target distribution but instead requires an \textit{auxiliary distribution} is \textit{importance sampling}. Importance sampling estimators use samples from the auxiliary distribution and correct for any deviation from the typical set of the target distribution using \textit{weighting factors}. Although~\textcite{kloek1978bayesian} is typically credited with introducing importance sampling to statistics, there are references to it in statistical physics as early as 1949~\parencite{goertzel1949quota, kahn1951estimation}.

In order to derive the \textit{importance weights} necessary for the computation of the expectation values we start with the definition of the expectation value and do some re--arrangements,
\begin{equation}
    \label{eq:expectation_value_auxiliary_rearrangement}
    \begin{split}
        \mathbb{E}_{p}[f(\theta)] &= \int_{\Theta} f(\theta) p(\theta) d\theta \\
        &= \int_{\Theta} f(\theta) \frac{p(\theta)}{q(\theta)} q(\theta) d\theta \\
        &= \mathbb{E}_{q}\left[f(\theta) \times \frac{p(\theta)}{q(\theta)}\right] \,.
    \end{split}
\end{equation}

We can now estimate the expectation value,
\begin{equation}
    \label{eq:expectation_importance_sampling}
    \Hat{f}_{n}^{IS} = \frac{1}{n} \sum_{i=1}^{n} w(\Tilde{\theta}_{i}) f(\Tilde{\theta}_{i})\,,
\end{equation}
using samples from the \textit{auxiliary} distribution,
\begin{equation}
    \label{eq:samples_from_auxiliary}
    \lbrace \Tilde{\theta}_{1}, \dots,\Tilde{\theta}_{n} \rbrace  \sim q (\Tilde{\theta})\,,
\end{equation}
and importance weights given by,
\begin{equation}
    \label{eq:importance_weights}
    w(\Tilde{\theta}) = \frac{p(\Tilde{\theta})}{q(\Tilde{\theta})}\,.
\end{equation}

For any square--integrable real--valued function $f(\theta)$ , the importance sampling estimator satisfies the \textit{central limit theorem},
\begin{equation}
    \label{eq:importance_sampling_clt}
    \frac{\hat{f}_{n}^{IS} - \mathbb{E}_{p}[f(\theta)]}{\textrm{IS--SE}_{n}[f]}\sim \mathcal{N}(0,1)\,,
\end{equation}
where $\textrm{MC--SE}_{n}[f]$ is the \textit{Importance Sampling Standard Error} defined as,
\begin{equation}
    \label{eq:importance_sampling_standard_error}
    \textrm{IS--SE}_{n}[f] = \sqrt{\frac{\textrm{Var}_{q}[w f]}{n}}\,.
\end{equation}
By comparing the expression \ref{eq:importance_sampling_standard_error} for $\textrm{IS--SE}$ to the respective expression \ref{eq:monte_carlo_standard_error} for the \textit{Monte Carlo Standard Error} we can define the \textit{Effective Sample Size (ESS)},
\begin{equation}
    \label{eq:effective_sample_size}
    \textrm{ESS}_{n}[f] = \frac{\textrm{Var}_{q}[w f]}{\textrm{Var}_{p}[f]} n \,,
\end{equation}
as the effective number of exact samples that contain the same amount of information as the $n$ samples and their importance weights.

It is important to mention here that if the target or auxiliary distribution is known only up to a normalisation factor, for instance if the computation of the normalisation constant is very costly, then the importance weights have to be normalised such that,
\begin{equation}
    \label{eq:importance_weight_normalisation}
    \sum_{i=1}^{n} w(\Tilde{\theta}_{i}) = 1\,,
\end{equation}
for the aforementioned estimators to be valid.

\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{shell_importance_sampling.pdf}}
    \caption{Illustration of the typical set including samples generated using an unsuitable importance density. In high dimensions the typical set corresponds to a very thin shell and it is difficult to achieve sufficient overlap between the typical set of the auxiliary and target distribution. Here, this is depicted by samples that do not reside in the typical set of the target and will thus have low importance weights.}
    \label{fig:shell_importance_sampling}
\end{figure}

The quality of the importance sampling estimator is determined by the amount of overlap between the auxiliary and target distribution. Samples from the auxiliary distribution residing in regions of high overlap will receive large importance weights and those residing in regions of little or no overlap will receive small importance weights.

As we showed in previous sections, in low dimensions the typical set is broad so we should expect that the construction of importance sampling estimators for low dimensional cases to be a feasible procedure. In higher dimensions however, the typical shell is very thin thus complicating the choice of auxiliary distributions with significant overlap with the target distribution.

It is common in practice to assume that an auxiliary distribution with broader density tails than the target distribution would be sufficient to construct an importance sampling estimator. Although the reasoning of this idea is appealing, it can however be misleading as it does not extend to higher dimensions in which the typical set becomes the central object of interest and not the probability density.

It is useful to define a measure of the quality of an importance sampling estimator. A straightforward choice would be to define the \textit{importance sampling effective sample size},
\begin{equation}
    \label{eq:importance_sampling_effective_sample_size}
    N_{\textrm{eff}} = \frac{\left( \sum_{i=1}^{n}w(\Tilde{\theta}_{i})\right)^{2}}{\sum_{i=1}^{n}w(\Tilde{\theta}_{i})^{2}}\,.
\end{equation}
Equation \ref{eq:importance_sampling_effective_sample_size} is just a heuristic diagnostic and it should not be confused with equation \ref{eq:effective_sample_size}.

\section{Markov chain Monte Carlo}

Importance sampling estimators trade the ability to produce exact samples from the target distribution with weighted samples from an auxiliary distribution. On the other hand, \textit{Markov chain Monte Carlo (MCMC)} estimators replace the exact samples with \textit{correlated} samples generated by a \textit{Markov chain}~\parencite{gilks1995markov, brooks2011handbook}.

Therefore, the key idea in MCMC is to explore the typical set using a sequence of local steps. Starting a point $\theta_{1}$ in parameter space $\Theta$, the next point $\theta_{2}$ is chosen stochastically in the neighbourhood of $\theta_{1}$. Then the process is repeated for the next point $\theta_{3}$ in the neighbourhood of $\theta_{2}$ and so on. At the end, we have generated a chain of $n$ samples that is \textit{Markov}, meaning that each sample conditionally depends only on the previous one,
\begin{equation}
    \label{eq:markov_property}
    P(\theta_{n}\vert \theta_{1},\dots,\theta_{n-1}) = P(\theta_{n}\vert \theta_{n-1})\,.
\end{equation}

More formally, the Markov chain can be generated by repeatedly sampling from a conditional probability distribution on the product space $\Theta \times \Theta$, known as \textit{Markov transition probability} $T(\theta'\vert \theta)$. Given an initial point $\theta_{1}$, sampling from the \textit{Markov transition probability} $T(\theta'\vert \theta_{1})$ returns sample $\theta_{1}$. We can thus construct a sequence of transitions,
\begin{equation}
    \label{eq:sequence_markov_transitions}
    \begin{split}
        \theta_{2} &\sim T(\theta_{2}\vert \theta_{1}) \\
        \theta_{3} &\sim T(\theta_{3}\vert \theta_{2}) \\
        &\dots \\
        \theta_{n} &\sim T(\theta_{n}\vert \theta_{n-1})\,,
    \end{split}
\end{equation}
that constitute the Markov chain $\lbrace \theta_{1}, \theta_{2}, \dots, \theta_{n} \rbrace$. The samples of the Markov chain are not independent, but they are \textit{correlated}. The reason for this is their sequential origin i.e. $\theta_{n}$ depends on $\theta_{n-1}$ which depends on $\theta_{n-2}$ so even samples that are not right next to each other in the Markov chain can be correlated.
\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{trace.pdf}}
    \caption{Example of trace plot of a Markov chain for parameter $\theta$. The chain reaches the stationary state after about $50$ iterations.}
    \label{fig:trace_plot}
\end{figure}

\subsection{Requirements of MCMC}

In general, the possible values $\theta$ of the Markov chain are called the \textit{states} of the Markov chain~\parencite{tierney1994markov, mackay2003information}. For a Markov chain Monte Carlo estimator to generate samples from the target distribution, the Markov chain must satisfy a couple of conditions:
\begin{enumerate}
    \item First of all, the Markov chain has to leave the target distribution $p$ \textit{invariant} or \textit{stationary}, 
    \begin{equation}
        \label{eq:invariance_condition}
        p(\theta ') = \int T(\theta'\vert\theta)p(\theta)d\theta\,.
    \end{equation}
    This means that if we start from a state $\theta$ of $p$, the next state $\theta'$ is also a state of $p$. In practice, a sufficient but not necessary condition is \textit{detailed balance}, which requires that each transition $\theta \rightarrow \theta'$ is reversible. More formally, for any pair of states $\theta$ and $\theta'$ the following relation must hold,
    \begin{equation}
        \label{eq:detailed_balance}
        T(\theta'\vert\theta)p(\theta) = T(\theta\vert\theta')p(\theta')\,,
    \end{equation}
    meaning that the probability of being at state $\theta$ and transitioning to state $\theta'$ is equal to the probability of being at state $\theta'$ and transitioning to state $\theta$.
    
    \item Furthermore, we need to make sure that the \textit{stationary} distribution is unique and that the distribution of states is able to converge to it regardless the starting point $\theta_{1}$. In other words, we need to make sure that the \textit{stationary} distribution is also the \textit{limiting} distribution. This requires two properties, \textit{irreducibility}, that is the ability to visit any state $\theta$ for which $p(\theta)>0$ in a finite number of steps, and \textit{aperiodicity}, meaning that no states are only accessible at certain regularly spaced times. These two properties combined, when met, render the Markov chain \textit{ergodic}.
\end{enumerate}

\subsection{Expected behaviour}

When all of the aforementioned conditions are obeyed, the Markov chain samples from the target distribution. The behaviour of the Markov chain, in terms of the computed expectation values, passes through four stages that characterise its normal behaviour~\parencite{betancourt2017conceptual}.
\begin{figure}[!htb]
    \centering
	\centerline{\includegraphics[scale=0.65]{expectation_initial_combined.pdf}}
    \caption{Initial stage of exploration -- no exploration has taken place.}
    \label{fig:expectation_initial_combined}
\end{figure}
\begin{figure}[!htb]
    \centering
	\centerline{\includegraphics[scale=0.65]{expectation_burnin_combined.pdf}}
    \caption{Burn--in stage of exploration --  the absolute difference between the estimate of $f$ and its expectation value slowly decreases as the chain approaches the typical set.}
    \label{fig:expectation_burnin_combined}
\end{figure}
The first stage, shown in Figure \ref{fig:expectation_initial_combined} consists of the initialisation of the Markov chain. Often we do not know where the typical set resides and thus we set the first state of the Markov chain to some arbitrary point in parameter space. During the second stage, the Markov chain moves towards the typical set as shown in Figure \ref{fig:expectation_burnin_combined}. At the same time the absolute difference of the estimated value $\Hat{f}$ from the expectation value $\mathbb{E}[f]$ slowly decreases. In the third state shown in Figure \ref{fig:expectation_initial_convergence_combined}, the Markov chain starts to explore the typical set. The absolute difference between the estimate of $f$ and its expectation value decreases very rapidly. Finally, in the fourth stage shown in Figure \ref{fig:expectation_convergence_combined} the Markov chain wanders inside the typical set and the standard error of the estimate asymptotically decreases as prescribed by the central limit theorem.
\begin{figure}[!htb]
    \centering
	\centerline{\includegraphics[scale=0.65]{expectation_initial_convergence_combined.pdf}}
    \caption{Initial convergence phase -- the absolute difference between the estimate of $f$ and the expectation value decreases rapidly as the chain approaches explores the typical set for the first time.}
    \label{fig:expectation_initial_convergence_combined}
\end{figure}
\begin{figure}[!htb]
    \centering
	\centerline{\includegraphics[scale=0.65]{expectation_convergence_combined.pdf}}
    \caption{Stationary or equilibrium phase -- the absolute difference between the estimate of $f$ and the expectation value has reached its minimum value as the chain samples fully populate the typical set.}
    \label{fig:expectation_convergence_combined}
\end{figure}



\subsection{Central limit theorem of MCMC}

Markov chain Monte Carlo estimators are particularly useful for many analyses since they obey a \textit{central limit theorem} that allows us to quantify their precision~\parencite{kipnis1986central, geyer1992practical, tierney1994markov}. In particular, given a square--integrable real--valued function $f(\theta)$ and a long enough Markov chain, the following is true,
\begin{equation}
    \label{eq:mcmc_clt}
    \frac{\Hat{f}_{n}^{\textrm{MCMC}}-\mathbb{E}_{p}[f(\theta)]}{\textrm{MCMC--SE}_{n}[f]}\sim \mathcal{N}(0,1)\,,
\end{equation}
where $\textrm{MCMC--SE}_{n}[f]$ is the \textit{Markov chain Monte Carlo Standard Error} given by,
\begin{equation}
    \label{eq:mcmc_se}
    \textrm{MCMC--SE}_{n}[f] = \sqrt{\frac{\textrm{Var}_{p}[f]}{\textrm{ESS}_{n}[f]}}\,.
\end{equation}
Comparing $\textrm{MCMC--SE}_{n}[f]$ with the standard error $\textrm{MC--SE}_{n}[f]$ of the exact Monte Carlo estimator given by equation \ref{eq:monte_carlo_standard_error}, one immediately notices that the number of samples $n$ has been replaced by the term $\textrm{ESS}_{n}[f]$. This term, called the \textit{Effective Sample Size} accounts for the loss of information due to the correlation between samples due to the Markov property of the chain. The effective sample size is given by,
\begin{equation}
    \label{eq:mcmc_ess}
    \textrm{ESS}_{n}[f] = \frac{n}{\tau[f]}\,,
\end{equation}
where $\tau[f]$ is the \textit{relaxation} or \textit{autocorrelation time} of the Markov chain. Less formally, $\tau[f]$ describes the number of steps required for the Markov chain to ``forget'' where it started, meaning that only one out of $\tau[f]$ is actually independent. A method for estimating the autocorrelation time of a Markov chain will be discussed in the next subsection.

\subsection{Autocorrelation}

The autocorrelation of the Markov chains is a necessary evil of MCMC methods and must be properly understood before making any inference~\parencite{geyer1992practical}. Figure \ref{fig:correlated_chains} shows two Markov chains with different levels of autocorrelation. In the weakly correlated chain, large jumps take place from one iteration to the next. On the other hand, the strongly correlated chain is characterised by very short jumps and more rigid trajectories.

\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{correlated_chains.pdf}}
    \caption{Markov chains with different degrees of autocorrelation.}
    \label{fig:correlated_chains}
\end{figure}

In order to quantify and measure the degree of autocorrelation of a Markov chain we need to compare the states of the chain after fixed number of iterations called \textit{lags}. Given an arbitrary function $f(\theta)$ of the states,
\begin{equation}
    \label{eq:autocorrelation_mean}
    \mu_{f} = \mathbb{E}_{p}[f]\,,
\end{equation}
is the mean value of the function expressed as the expectation value over the stationary distribution $p(\theta)$. The value $f(\theta_{i}) - \mu_{f}$ then quantifies the deviation of the $i$--th state of the chain from the mean value $\mu_{f}$. The expectation value of the product of two such deviations defines the \textit{autocovariance} of the chain,
\begin{equation}
    \label{eq:autocovariance}
    c_{ij} = \mathbb{E}_{p}\left[(f_{i} - \mu_{f})(f_{j} - \mu_{f}) \right]\,,
\end{equation}
where $f_{i}=f(\theta_{i})$ and $f_{j}=f(\theta_{j})$. Once the Markov chain has reached the stationary phase, the autocovariance will no longer depend on the particular states, $\theta_{i}$ and $\theta_{j}$, that we are comparing but on the number of iterations, called lag $\ell=j - i$, between them,
\begin{equation}
    \label{eq:autocovariance_invariance}
    c_{ij} = c_{i,i+\ell}=c_{\ell}\,.
\end{equation}
Finally, if we normalise the autocovariance by the variance,
\begin{equation}
    \label{eq:autocorrelation_variance}
    \mathrm{Var}_{p}[f] = \mathbb{E}_{p}\left[ (f-\mu_{f})^{2}\right]\,,
\end{equation}
we get the lag--$\ell$ \textit{autocorrelation function},
\begin{equation}
    \label{eq:autocorrelation_function}
    \rho_{\ell}[f] = \frac{\mathbb{E}_{p}\left[(f_{i+\ell} - \mu_{f})(f_{i} - \mu_{f}) \right]}{\mathrm{Var}_{p}[f]}\,.
\end{equation}
The normalisation ensures that the maximum possible value of $\rho_{\ell}[f]$ is $+1$ for fully correlated states and $-1$ for the completely anti--correlated states. The value of $0$ corresponds to uncorrelated samples. The lag--$\ell$ autocorrelation is always unity as any state is perfectly correlated with itself. Furthermore, the autocorrelation function depends only on the absolute lag and is invariant under changes of the sign, that is, $\rho_{\ell}[f]=\rho_{-\ell}[f]$. For this reason, only the non--negative part of the autocorrelation function is often plotted. 

\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{autocorrelation.pdf}}
    \caption{Autocorrelation as a function of lag $\ell$ for a weakly and a strongly correlated chain.}
    \label{fig:autocorrelation}
\end{figure}

Figure \ref{fig:autocorrelation} shows the autocorrelation function for the weakly and strongly correlated chains of Figure \ref{fig:correlated_chains}. We notice that although both functions begin at the value of $1$ for lag $\ell=0$, they approach the value of $0$ at different rates. In particular, the autocorrelation function of the weakly correlated chain goes to $0$ after only a few lags, whereas the one corresponding to the strongly correlated chain takes much longer. 

The asymptotic variance of an infinitely long chain is defined as,
\begin{equation}
    \label{eq:asymptotic_variance}
    \lim_{n\to\infty} n\times\mathrm{Var}_{p}\left[ \Hat{f}_{n}^{\mathrm{MCMC}}\right] = \mathrm{Var}_{p}[f]\times\tau_{I}[f]\,,
\end{equation}
where,
\begin{equation}
    \label{eq:integrated_autocorrelation_time}
    \tau_{I}[f] = \sum_{\ell=-\infty}^{\ell=+\infty}\rho_{\ell}[f] = 1 + 2\times\sum_{\ell=1}^{\ell=+\infty}\rho_{\ell}[f]\,,
\end{equation}
is the \textit{integrated autocorrelation time} and the last equality hold due to the lag--sign invariance of the autocorrelation function. Equation \ref{eq:asymptotic_variance} implies that the standard error of MCMC is,
\begin{equation}
    \label{eq:standard_error_mcmc}
    \textrm{MCMC--SE}_{n}[f] =\sqrt{ \frac{\mathrm{Var}_{p}[f]}{\mathrm{ESS}_{n}[f]} }\,,
\end{equation}
where we have defined the \textit{effective sample size} as,
\begin{equation}
    \label{eq:effective_sample_size_2}
    \mathrm{ESS}_{n}[f] = \frac{n}{1+2\times\sum_{\ell=1}^{\ell=+\infty}\rho_{\ell}[f]}\,.
\end{equation}

Estimating the integrated autocorrelation time, and thus the effective sample size, in not trivial in practice. The autocorrelation function can be very noisy in large lags, as shown in Figure \ref{fig:autocorrelation}. This means that the sum in equation \ref{eq:integrated_autocorrelation_time} needs to be truncated in practice in order avoid adding noise.
