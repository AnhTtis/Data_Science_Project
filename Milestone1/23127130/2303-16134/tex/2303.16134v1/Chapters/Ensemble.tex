% !TEX TS-program = pdflatex
% !TEX root = ../ArsClassica.tex

%************************************************
\chapter{Ensemble MCMC methods}
\label{chp:ensemble}
%************************************************

\begin{flushright}
\itshape
As for me, I am tormented with an everlasting itch for things remote.\\ I love to sail forbidden seas, and land on barbarous coasts. \\
\medskip
--- Herman Melville, Moby--Dick or, the Whale
\end{flushright}

In order to avoid issues caused by multimodality or the need for tuning the proposal distribution, ensemble MCMC methods rely on an ensemble of parallel samplers, often called \textit{walkers}, that sample from an extended probability distribution. A common way to construct such an extended probability distribution is using the product density,
\begin{equation}
    \label{eq:extended_density}
    \pi(\lbrace\theta_{k}\rbrace_{k=1}^{K}) = \prod_{k=1}^{K} p_{k}(\theta_{k})\,,
\end{equation}
where $p_{k}(\theta_{k})$ are the individual densities, one of which can correspond to the target distribution of interest (e.g. the posterior), and $K$ is the number of walkers. It is important to note here that $\theta_{k}$ is not the $k$--th component of a vector, but a $D$--dimensional vector itself.

The simplest product density that we can construct based on equation \ref{eq:extended_density} is to assume that $p_{k}(\theta_{k})=p(\theta_{k})$ for all $k$, meaning that the product density is just the product of $K$ identical copies of the target distribution $p(\theta_{k})$. A natural question to ask is then why would anyone want to do this? Why sample $K$ copies of the same distribution instead of just one? The answer is that the walkers sampling each copy do not have to be independent of each other and instead are allowed to exchange information about their current state. For instance, the proposal distribution for a single walker can depend on the current positions of the rest of the walkers in the ensemble. This allows for effective proposals that take into account the relative length--scales and positions of the modes of the target distribution.

Of course, other product densities, that do not rely on the simplifying assumption that $p_{k}(\theta_{k})=p(\theta_{k})$ for all $k$, can also be defined as we will see in the case of the \textit{parallel tempering} algorithm in Section \ref{sec:parallel_tempering}. In those cases, the goal is not usually to construct effective proposal distribution but rather to deal with the challenge of multimodality.

\section{Gaussian ensemble}

Perhaps the simplest way to construct an ensemble MCMC method that limits the requirement for tuning, to some extent, its proposal distribution is the \textit{Gaussian ensemble (GE)} algorithm. GE uses an ensemble of $K$ walkers that target a product density of the form of equation \ref{eq:extended_density}, where all copies $p_{k}(\theta_{k})$ are identical and correspond to the target distribution of interest (e.g. posterior), and the proposal distribution of each walker is simply a normal distribution informed by the positions of the rest of the walkers in the ensemble~\parencite{speagle2019conceptual}.

\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{gaussian_ensemble.pdf}}
    \caption{Illustration of the Gaussian ensemble MCMC method. A new state $\theta_{k}'$ is proposed in the vicinity of the position $\theta_{k}$ of the walker that is updated using an rescaled version of the sample covariance matrix of the rest of the walkers (i.e. excluding $\theta_{k}$) for the normal proposal distribution.}
    \label{fig:gaussian_ensemble}
\end{figure}
In particular, in a given iteration $t$ of the method, the algorithm performs a loop over the $K$ walkers updating each walker in turn. A new position $\Tilde{\theta}_{k}$ is proposed from a normal distribution,
\begin{equation}
    \label{eq:ensemble_gaussian_proposal}
    \theta_{k}' \sim \mathcal{N}\left(\theta\vert \theta_{k}^{(t-1)}, \gamma\Sigma_{-k}\right)\,,
\end{equation}
centred on the current state $\theta_{k}^{(t-1)}$ of the $k$--th walker and $\gamma$ is a multiplying factor used to scale the covariance matrix in order to achieve the optimal acceptance rate (e.g. $\gamma = 2.38^{2}/D$). The covariance matrix $\Sigma_{-k}$ of the proposal distribution is simply the sample covariance estimated using the positions of the ensemble $\lbrace\theta_{1}^{(t)},\allowbreak \dots,\allowbreak \theta_{k-1}^{(t)},\allowbreak \theta_{k+1}^{(t-1)},\allowbreak\dots ,\allowbreak \theta_{K}^{(t-1)}\rbrace$ which excludes the $k$--th walker. It is important to notice also that all the walkers up to and excluding the $k$--th have already been updated and it is their updated positions that are used to compute the proposal covariance. This is essentially a \textit{Metropolis--within--Gibbs} scheme in disguise. The new point $\Tilde{\theta}_{k}$ is then accepted or rejected based on the usual Metropolis criterion and the process continuous with the next walker until all of them have been updated.

\begin{algorithm}[ht!]
\caption{Gaussian ensemble} \algolabel{ge}
\begin{algorithmic}[1]
\REQUIRE{initial state for the ensemble $\theta^{(1)}=(\theta_{1}^{(1)},\dots,\theta_{K}^{(1)})$, (unnormalised) target density $f(\theta)\propto p(\theta)$, covariance scaling factor (e.g. $\gamma = 2.38^{2}/D$), and number of iterations $N$}
\ENSURE{Markov chain $\theta_{1}, \dots, \theta_{N}$ that has $p(\theta)$ as its equilibrium distribution}
\FOR{$t=1$ \TO $N$}
    \FOR{$k=1$ \TO $K$}
        \STATE{Compute ensemble mean $\mu_{-k}^{(t)}=\frac{1}{K-1}\sum_{i\neq k}\theta_{i}^{(t)}$ excluding the $k$th state}
        \STATE{Compute ensemble covariance matrix $\Sigma_{-k}^{(t)}=\frac{1}{K-1}\sum_{i\neq k}(\theta_{i}^{(t)}-\mu_{-k}^{(t)})(\theta_{i}^{(t)}-\mu_{-k}^{(t)})^{T}$ excluding the $k$th state}
        \STATE{Draw proposal $\theta_{k}'\sim \mathcal{N}(\theta_{k}^{(t)}, \gamma\Sigma_{-k}^{(t)})$}
        \STATE{Compute acceptance probability $\alpha_{k} = \min\left(1, f(\theta_{k}')/f(\theta_{k}^{(t)})\right)$}
        \STATE{Draw uniform number $u\sim\mathcal{U}(0,1)$}
        \IF{$u < \alpha_{k}$}
            \STATE{Accept proposed state and set $\theta_{k}^{(t+1)}\leftarrow \theta_{k}'$}
        \ELSE
            \STATE{Reject proposed state and set $\theta_{k}^{(t+1)}\leftarrow \theta_{k}^{(t)}$}
        \ENDIF
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}
\looseness=-1 GE solves the problem of tuning the proposal, up to the scaling factor $\gamma$ of the covariance matrix, but still assumes a Gaussian proposal. This means that we do not expect that GE will perform better than $K$ parallel well--tuned \textit{Random--walk Metropolis} samplers. As we will discuss in the next couple of sections, there are ways to relax this limitation and allow for more flexible proposals. Last but not least, the estimation of the proposal covariance matrix requires that the absolute minimum size of the ensemble to be $D+1$ for the covariance to be non--singular.

\section{Affine--invariant stretch move}

The \textit{affine--invariant ensemble sampler} and in particular the \textit{stretch move} introduced by \textcite{goodman2010ensemble} is perhaps the most popular ensemble MCMC method in the astronomical literature, made available in the \textit{Python} implementation \textit{emcee} \parencite{foreman2013emcee}. The \textit{stretch move} algorithm relaxes the limitation of the Gaussian proposal and instead updates each walker in turn along the direction of a different uniformly selected walker sampled from the rest of the ensemble. As we will discuss this change introduces both benefits and challenges.

\begin{figure}[!ht]
    \centering
	\centerline{\includegraphics[scale=0.65]{stretch.pdf}}
    \caption{Illustration of the affine--invariant stretch move. The selected walker $\theta_{k}$ is moved to its new position $\theta_{k}'$ along the line defined by $\theta_{j}$ and $\theta_{k}$. $\theta_{j}$ is a walker that is uniformly selected from the rest of the ensemble (i.e. excluding $\theta_{k}$).}
    \label{fig:stretch}
\end{figure}
In particular, in a given iteration $t$ of the method, the algorithm performs a loop over the $K$ walkers updating each one in turn. In the so--called \textit{stretch move}, we move a walker $\theta_{k}$ using a uniformly selected walker $\theta_{j}$ from the complementary ensemble $S_{-k} = \lbrace\theta_{1}^{(t)},\allowbreak \dots,\allowbreak \theta_{k-1}^{(t)},\allowbreak \theta_{k+1}^{(t-1)},\allowbreak \dots, \allowbreak \theta_{K}^{(t-1)}\rbrace$ that excludes $\theta_{k}$. The $\theta_{j}$ walker acts as an \textit{anchor point} for the move that consists of a proposal of the form
\begin{equation}
    \label{eq:stretch_proposal}
    \theta_{k}' = \zeta \theta_{k} + (1-\zeta) \theta_{j}\,,
\end{equation}
where $\zeta$ is a scaling variable with a probability density $g$ that satisfies the symmetry condition,
\begin{equation}
    \label{eq:symmetry_condition_zeta}
    g\left( \frac{1}{\zeta}\right) = \zeta g(\zeta)\,,
\end{equation}
such that the move expressed by equation \ref{eq:stretch_proposal} is symmetric in the \textit{Metropolis} sense. A particular density that obeys this condition is
\begin{equation}
    \label{eq:zeta_density}
    g(\zeta) \propto 
    \begin{cases}
    \frac{1}{\sqrt{\zeta}} & \mathrm{if}\;\zeta\in\left[\frac{1}{\alpha}, \alpha \right]\,, \\
    0 & \mathrm{otherwise}\,,
    \end{cases}
\end{equation}
where $\alpha > 1$ is a parameter that can be tuned to enhance the performance. The default value is usually set to $\alpha=2$. The new state $\theta_{k}'$ is then accepted with \textit{Metropolis} probability,
\begin{equation}
    \label{eq:stretch_metropolis_probability}
    \alpha(\theta_{k}',\theta_{k}) = \min\left( 1, \zeta^{D-1}\frac{p(\theta_{k}')}{p(\theta_{k})}\right)\,,
\end{equation}
where the $\zeta^{D-1}$ comes from the fact that the update takes place along a straight line. The process is then repeated for the next walker, until all the walkers are updated for the current iteration $t$ before the algorithm moves to its next iteration.

\begin{algorithm}[ht!]
\caption{Affine--invariant stretch move} \algolabel{aism}
\begin{algorithmic}[1]
\REQUIRE{initial state for the ensemble $\theta^{(1)}=(\theta_{1}^{(1)},\dots,\theta_{K}^{(1)})$, (unnormalised) target density $f(\theta)\propto p(\theta)$, and number of iterations $N$}
\ENSURE{Markov chain $\theta_{1}, \dots, \theta_{N}$ that has $p(\theta)$ as its equilibrium distribution}
\FOR{$t=1$ \TO $N$}
    \FOR{$k=1$ \TO $K$}
        \STATE{Draw a walker $\theta_{j}$ from the complementary ensemble $S_{-k} = \left\lbrace \theta_{1}^{(t+1)}, \dots, \theta_{k-1}^{(t+1)}, \theta_{k+1}^{(t)}, \dots, \theta_{K}^{(t)} \right\rbrace$}
        \STATE{Draw random number $\zeta\sim g(\zeta)$}
        \STATE{Compute proposed state $\theta_{k}' \leftarrow \zeta \theta_{k} + (1-\zeta)\theta_{j}$}
        \STATE{Compute acceptance probability $\alpha_{k} = \min\left(1, \zeta^{D-1}f(\theta_{k}')/f(\theta_{k}^{(t)})\right)$}
        \STATE{Draw uniform number $u\sim\mathcal{U}(0,1)$}
        \IF{$u < \alpha_{k}$}
            \STATE{Accept proposed state and set $\theta_{k}^{(t+1)}\leftarrow \theta_{k}'$}
        \ELSE
            \STATE{Reject proposed state and set $\theta_{k}^{(t+1)}\leftarrow \theta_{k}^{(t)}$}
        \ENDIF
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}
One of the strict requirements of this method is the minimum number of walkers to be $D+1$ for it to be \textit{ergodic} and avoid the risk of walkers getting \textit{trapped} in some hyper--plane of lower than $D$ dimensions. Practically, the actual number of walkers required is much larger as it determines the plethora of possible new directions along which updates take place in each iteration. In this sense, the initial positions of the walkers and the number of them are the only free hyperparameters of this method. A great benefit of this method is that it is \textit{affine--invariant}, that is, its performance is insensitive to any linear correlations between the parameters of the target distribution. As the astronomical community has witnessed during the past few years, this offers a great advantage over other methods.

\section{Differential evolution}

Another ensemble method in the spirit of the \textit{stretch move} is the \textit{differential evolution} MCMC~\parencite{ter2006markov, ter2008differential}. Unlike the \textit{stretch move} that requires another single walker to act as an \textit{anchor point} for a proposal, \textit{differential evolution} involves two. We will discuss shortly how this difference can affect the performance and alter the characteristics of the method.

\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{differential_evolution.pdf}}
    \caption{Illustration of the differential evolution Monte Carlo. The selected walker $\theta_{k}$ is moved to its new position $\theta_{k}'$ parallel to the line defined by $\theta_{i}$ and $\theta_{j}$. The latter are two walkers that are uniformly selected from the rest of the ensemble (i.e. excluding $\theta_{k}$).}
    \label{fig:differential_evolution}
\end{figure}
An update of the ensemble works as follows: the algorithm performs a loop over the $K$ walkers updating each one in turn. Assuming that the current walker to be updated is $\theta_{k}$, the algorithm uniformly selects two walkers (without replacement), $\theta_{i}$ and $\theta_{j}$,  from the complementary ensemble $S_{-k} = \lbrace\theta_{1}^{(t)},\allowbreak \dots,\allowbreak \theta_{k-1}^{(t)},\allowbreak \theta_{k+1}^{(t-1)},\allowbreak \dots, \allowbreak \theta_{K}^{(t-1)}\rbrace$ that excludes $\theta_{k}$. The vector $\theta_{i}-\theta_{j}$ connecting the two auxiliary walkers defines the direction along which a move is proposed. The move consists of a proposal of the form
\begin{equation}
    \label{eq:differential_evolution_proposal}
    \theta_{k}' = \theta_{k} + \gamma \times \left( \theta_{i}-\theta_{j}\right) + \epsilon\,,
\end{equation}
where $\gamma$ is a non--zero scaling factor and $\epsilon\sim\mathcal{N}(0, \sigma^{2})$ is some optional Gaussian noise. The value of $\gamma$ determines the scale of the proposal. Its default value is often set to $\gamma = 2.38 / \sqrt{2D}$ which results in the optimal acceptance rate (i.e. $23.4\%$) for normal target distributions. In practice, one can adapt $\gamma$ using some diminishing adaptation scheme during the run. The proposed update of equation \ref{eq:differential_evolution_proposal} is then accepted with Metropolis acceptance probability
\begin{equation}
    \label{eq:differential_evolution_acceptance}
    \alpha(\theta',\theta) = \min\left(1, \frac{p(\theta')}{p(\theta)}\right)\,.
\end{equation}

\begin{algorithm}[ht!]
\caption{Differential evolution} \algolabel{de}
\begin{algorithmic}[1]
\REQUIRE{initial state for the ensemble $\theta^{(1)}=(\theta_{1}^{(1)},\dots,\theta_{K}^{(1)})$, (unnormalised) target density $f(\theta)\propto p(\theta)$, proposal scale parameter (e.g. $\gamma = 2.38/\sqrt{2D}$), optional Gaussian noise standard deviation (e.g. $\sigma = 10^{-3})$, and number of iterations $N$}
\ENSURE{Markov chain $\theta_{1}, \dots, \theta_{N}$ that has $p(\theta)$ as its equilibrium distribution}
\FOR{$t=1$ \TO $N$}
    \FOR{$k=1$ \TO $K$}
        \STATE{Draw walkers $\theta_{i}$ and $\theta_{j}$ without replacement from the complementary ensemble $S_{-k} = \left\lbrace \theta_{1}^{(t+1)}, \dots, \theta_{k-1}^{(t+1)}, \theta_{k+1}^{(t)}, \dots, \theta_{K}^{(t)} \right\rbrace$}
        \STATE{Draw random noise $\epsilon\sim\mathcal{N}(0,\sigma^{2})$}
        \STATE{Compute proposed state $\theta_{k}' \leftarrow \theta_{k} + \gamma(\theta_{i}-\theta_{j}) + \epsilon$}
        \STATE{Compute acceptance probability $\alpha_{k} = \min\left(1, f(\theta_{k}')/f(\theta_{k}^{(t)})\right)$}
        \STATE{Draw uniform number $u\sim\mathcal{U}(0,1)$}
        \IF{$u < \alpha_{k}$}
            \STATE{Accept proposed state and set $\theta_{k}^{(t+1)}\leftarrow \theta_{k}'$}
        \ELSE
            \STATE{Reject proposed state and set $\theta_{k}^{(t+1)}\leftarrow \theta_{k}^{(t)}$}
        \ENDIF
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}
The advantage of \textit{differential evolution} over the \textit{stretch moves} comes down to the flexibility of their proposals. The direction along which a walker moves in the context of the \textit{stretch moves} is determined by a single walker. This means that at any given iteration, the number of equally possible directions is $K-1$. On the other hand, \textit{differential evolution} moves each walker along a direction defined by two walkers. This implies that the total number of possible directions is given by the binomial combination $\binom{K-1}{2}$. The latter increases much faster with the number size of the ensemble $K$ than the former, offering a larger variety of possible trajectories for the walkers. In other words, \textit{differential evolution} is expected to perform better even with a lower number of walkers.

\section{Parallel tempering}
\label{sec:parallel_tempering}

So far we have only discussed ensemble methods that target a trivial product density given by the product of $K$ copies of the target distribution as shown in equation \ref{eq:extended_density}. The main rationale for attempting to do this was to reduce the tuning requirements of MCMC. If we focus on addressing the challenge of multimodality, that is, the existence of multiple peaks in the target distribution, then we have to introduce a different product density as the extended target distribution.

\looseness=-1 One such choice is,
\begin{equation}
    \label{eq:product_density_pt}
    p^{*}\left(\lbrace\theta_{k}\rbrace_{k=1}^{K}\right) = \prod_{k=1}^{K} p_{k}(\theta_{k})\,,
\end{equation}
where
\begin{equation}
    \label{eq:tempered_posterior}
    p_{k}(\theta_{k}) \propto p^{\beta_{k}}(d\vert\theta,\mathcal{M})p(\theta\vert\mathcal{M})\,,
\end{equation}
is the \textit{annealed or tempered posterior} that offers a simple interpolation between the prior $p(\theta\vert \mathcal{M})$ and the unnormalised posterior density $p(d\vert\theta,\mathcal{M})\allowbreak p(\theta\vert \mathcal{M})$ for different monotonically--increasing values of $\beta_{k}\in[0,1]$. In the limit that $\beta_{k}=1$ for all values of $k$, equation \ref{eq:product_density_pt} reduces to the usual product density of equation \ref{eq:extended_density}.

\begin{figure}[!htb]
    \centering
	\centerline{\includegraphics[scale=0.65]{tempered_posterior.pdf}}
    \caption{Illustration of the gradual tempering performed in the posterior distribution. The prior distribution corresponds to $\beta\rightarrow 0$ and the posterior is recovered as $\beta\rightarrow 1$.}
    \label{fig:tempering}
\end{figure}

\looseness=-1 The method of \textit{parallel tempering (PT)}~\parencite{earl2005parallel}, also known as \textit{replica exchange Monte Carlo (REMC)}~\parencite{swendsen1986replica, hukushima1996exchange} or \textit{Metropolis--coupled Markov chain Monte Carlo (MC$^{3}$)}~\parencite{geyer1991markov}, relies on $K$ parallel Markov chains, each one targeting a different tempered density. The $\beta_{k}$ values are usually chosen \textit{a priori} using a heuristic rule (e.g. $\beta_{k}=(k-1)^{3}/(K-1)^3$), or are set adaptively during the run using some diminishing adaptation scheme. The choice of MCMC method used for each different $\beta_{k}$ is completely arbitrary and it can be anything from simple \textit{Random--walk Metropolis} to \textit{Hamiltonian Monte Carlo} or even an ensemble MCMC method.

So far, PT might look very similar to a parallel version of the sequential \textit{simulated annealing} method in which $\beta_{k}=1/T_{k}$ has the role of the inverse temperature. The crucial difference that makes PT so powerful is the fact that one can also perform \textit{between--chain exchange moves}. Either periodically (e.g. once every $10$ steps) or randomly (e.g. with probability $10\%$) a swap can take place between two states $\theta_{i}$ and $\theta_{j}$ that belong to different tempered posteriors (i.e. $\beta_{i}\neq \beta_{j}$). The reason that exchange/swap moves are desirable is that they enable the transfer of information from states of low $\beta$ to those of higher $\beta$. 

\begin{figure}[!htb]
    \centering
	\centerline{\includegraphics[scale=0.65]{beta_swaps.pdf}}
    \caption{Illustration of the parallel tempering swaps performed between adjacent temperature levels.}
    \label{fig:beta_swaps}
\end{figure}
To understand how to perform a swap in practice let us consider the extended state,
\begin{equation}
    \label{eq:extended_state_before_swap}
    \lbrace \theta_{k} \rbrace_{k=1}^{K} = \lbrace\theta_{1}, \dots, \theta_{i}, \dots, \theta_{j}, \dots, \theta_{K}\rbrace\,,
\end{equation}
prior to the swap, where $\theta_{i}$ and $\theta_{j}$ are the two states that we want to exchange. This means that the proposed new state will be,
\begin{equation}
    \label{eq:extended_state_after_swap}
    \lbrace \Tilde{\theta}_{k} \rbrace_{k=1}^{K} = \lbrace\theta_{1}, \dots, \theta_{j}, \dots, \theta_{i}, \dots, \theta_{K}\rbrace\,.
\end{equation}
Notice that the rest of the states, with the exception of $\theta_{i}$ and $\theta_{j}$, are left unaffected by this exchange proposal. The \textit{Metropolis acceptance probability} for this proposal is,
\begin{equation}
    \label{eq:exchange_metropolis_acceptance}
    \begin{split}
        \alpha_{ij} &= \min\left( 1, \frac{p^{*}\left( \lbrace \Tilde{\theta}_{k} \rbrace_{k=1}^{K}\right)}{p^{*}\left( \lbrace \theta_{k} \rbrace_{k=1}^{K}\right)} \right) \\
        &= \min\left( 1, \frac{p_{i}(\theta_{j})p_{j}(\theta_{i})}{p_{i}(\theta_{i})p_{j}(\theta_{j})}\right) \\
        &= \min\left( 1, \frac{p^{\beta_{i}}(d\vert\theta_{j},\mathcal{M})p(\theta_{j}\vert\mathcal{M})p^{\beta_{j}}(d\vert\theta_{i},\mathcal{M})p(\theta_{i}\vert\mathcal{M})}{p^{\beta_{i}}(d\vert\theta_{i},\mathcal{M})p(\theta_{i}\vert\mathcal{M})p^{\beta_{j}}(d\vert\theta_{j},\mathcal{M})p(\theta_{j}\vert\mathcal{M})} \right) \\
        &= \min\left( 1, \frac{p^{\beta_{i}}(d\vert\theta_{j},\mathcal{M})p^{\beta_{j}}(d\vert\theta_{i},\mathcal{M})}{p^{\beta_{i}}(d\vert\theta_{i},\mathcal{M})p^{\beta_{j}}(d\vert\theta_{j},\mathcal{M})} \right) \\
        &= \min\left[ 1, \left( \frac{p(d\vert\theta_{i},\mathcal{M})}{p(d\vert\theta_{j},\mathcal{M})}\right)^{(\beta_{j}-\beta_{i})} \right]\,,
    \end{split}
\end{equation}

The chains are usually chosen to be in adjacent $\beta_{k}$ levels (i.e. $i=j- 1$) such that the overlap between the typical sets of $p_{i}(\theta_{i})$ and $p_{j}(\theta_{j})$ is large, leading to high acceptance probabilities. \textcite{atchade2011towards} estimated that the optimal acceptance rate is $23.4\%$. The exchange updates are typically performed after the local MCMC updates are completed in all $\beta$ levels for a given iteration. Furthermore, there are different strategies for proposing swaps between adjacent temperature levels~\parencite{lingenheil2009efficiency}. One option is to randomly select a pair of adjacent temperature levels per iteration. Another strategy involves proposing to swap all adjacent pairs starting from the lowest or highest $\beta$ level and progressively moving towards the other end of the ladder. Finally, strategies that involve two steps, for instance, proposing to swap all even pairs in one iteration and all odd pairs in the next, have also been suggested in the literature~\parencite{lingenheil2009efficiency}.

\begin{algorithm}[ht!]
\caption{Parallel tempering} \algolabel{pt}
\begin{algorithmic}[1]
\REQUIRE{initial state for the ensemble $\theta^{(1)}=(\theta_{1}^{(1)},\dots,\theta_{K}^{(1)})$, prior probability density $\pi(\theta) \equiv p(\theta\vert\mathcal{M})$, likelihood function $\mathcal{L}(\theta)\equiv p(d\vert\theta,\mathcal{M})$, temperature ladder (e.g. $\beta_{k} = (k-1)^{3}/(K-1)^{3}$), local MCMC kernel $\theta'\leftarrow\mathcal{K}(\theta; f(\theta))$ (e.g. a single random--walk Metropolis update), and number of iterations $N$}
\ENSURE{$K$ Markov chains that each has $p_{t}(\theta)\propto \pi(\theta)\mathcal{L}(\theta)^{\beta_{k}}$ as its equilibrium distribution}
\FOR{$t=1$ \TO $N$}
    \FOR{$k=1$ \TO $K$}
        \STATE{Update state using local MCMC update $\theta_{k}'\leftarrow \mathcal{K} (\theta_{k}^{(t)}; \pi(\theta)\mathcal{L}(\theta)^{\beta_{k}})$}
    \ENDFOR
    \STATE{Draw random value of $k$ uniformly $k\sim\mathcal{U}(1, K-1)$}
    \STATE{Compute acceptance probability $\alpha_{k} = \min\left(1, \left[\frac{\mathcal{L}\left(\theta_{k}^{(t+1)}\right)}{\mathcal{L}\left(\theta_{k+1}^{(t+1)}\right)}\right]^{\beta_{k+1}-\beta_{k}}\right)$}
    \STATE{Draw uniform number $u\sim\mathcal{U}(0,1)$}
    \IF{$u < \alpha_{k}$}
        \STATE{Accept proposed swap and set $\theta_{k}^{(t+1)}\leftarrow \theta_{k+1}'$ and $\theta_{k+1}^{(t+1)}\leftarrow \theta_{k}'$}
    \ELSE
        \STATE{Reject proposed swap and set $\theta_{k}^{(t+1)}\leftarrow \theta_{k}'$ and $\theta_{k+1}^{(t+1)}\leftarrow \theta_{k+1}'$}
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}