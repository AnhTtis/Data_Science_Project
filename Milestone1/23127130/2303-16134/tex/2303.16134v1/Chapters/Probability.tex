% !TEX TS-program = pdflatex
% !TEX root = ../ArsClassica.tex

%************************************************
\chapter{Probability Theory}
\label{chp:probability}
%************************************************

\begin{flushright}
\itshape
Science is more than a body of knowledge; it is a way of thinking. \\The method of science, as stodgy and grumpy as it may seem,\\ is far more important than the findings of science.\\
\medskip
--- Carl Sagan
\end{flushright}

This chapter introduces the basic principles of Bayesian inference and presents its fundamental ideas and distinctive features.

%************************************************

\section{The goal of science}

The key goal of science is to distil the patterns of nature into mathematical language and call them physical laws. To this end, science relies on a formal way of thinking and interrogating nature, asking the right questions, interpreting observations, and updating its beliefs and hypotheses in the light of new evidence. This way of thinking, inherent in all scientific pursuits seems to be deeply connected to the mathematical notion of probability.

Science proceeds towards this elusive target with careful steps following the scientific method. The latter is often illustrated as a loop. Hypotheses are proposed and models quantifying certain aspects of those hypotheses are developed. The hypotheses give rise to predictions, in a process called \textit{deductive inference}, to be tested against experimental data. Unfortunately, as the information that we extract from nature in the form of data is always incomplete and uncertain, testing our hypotheses by comparing our model predictions to the experimental data requires us to reason in the presence of uncertainty. We thus rely on \textit{plausible inference}, that is, the process of inferring the truth of our theories about the cosmos on the basis of incomplete and uncertain information. 

Scientific statements about the physical world are uncertain by necessity. No amount of new information will ever be enough to validate or disprove a hypothesis. Furthermore, our models, despite our best intentions, are often simpler than the natural processes which they attempt to capture. Our best hope is thus to accept the existence of this inherent and unavoidable uncertainty and instead try to quantify the plausibility of our statements about the cosmos. Assessing the plausibility of scientific theories is the subject of probability theory.

%************************************************

\section{The notion of probability}

There are few concepts in science and mathematics as controversial, with their meaning so contested during the centuries, as the notion of probability. Three centuries ago people started seriously thinking about how to best make decisions and reason in the face of uncertainty. Perhaps, the first to formally articulate this problem was \textit{Jacob Bernoulli} in his seminal work \textit{Ars Conjectandi} published in $1713$.

The answer to Bernoulli's question was provided by \textit{Reverend Thomas Bayes}, in an essay named \textit{An Essay towards solving a Problem in the Doctrine of Chances}, published posthumously by his friend \textit{Richard Price} in $1763$. The paper included theorems on \textit{conditional probability} which formed the basis of what we now call \textit{Bayes' theorem}. The discovery of the latter is actually due to \textit{Laplace}, who not only developed, extended and clarified probability theory, but also applied it successfully to a plethora of problems in astronomy, medicine, and economics. 
\begin{figure}[H]
    \centering
	\centerline{\includegraphics[scale=0.65]{probability_wlabel.pdf}}
    \caption{In the Bayesian interpretation of probability, the degree of belief is distributed and the variable has a specific (unknown) fixed value.}
    \label{fig:probability_as_belief}
\end{figure}

Despite Laplace's indisputable empirical success, his theory was rejected by scholars soon after his death. Their problem with Laplace's probability theory was one of interpretation. For pioneers such as Bernoulli, Bayes, and Laplace, probability represented a degree--of--belief or plausibility of various hypotheses or statements based on the available evidence and prior knowledge. To $19$th century scholars though, this definition, or interpretation of probability, seemed too subjective and vague. For this reason, they redefined probability to mean the \textit{long--run relative frequency} with which an event occurs, given infinite trials. Since frequency can be measured experimentally, probability was then seen as an objective measure for dealing with \textit{randomness} and chance.
\begin{figure}[H]
    \centering
	\centerline{\includegraphics[scale=0.65]{frequency_wlabel.pdf}}
    \caption{In the frequentist interpretation of probability, the value of the variable itself is distributed along different experiments.}
    \label{fig:probability_as_frequency}
\end{figure}

Although the \textit{frequentist} interpretation of probability seems more objective, its range of applicability and validity is substantially more limited. For example, Laplace used \textit{Bayes' theorem} and his probability theory to estimate the mass of Saturn. He computed the \textit{posterior probability density function (pdf)} $p(M\vert d)$, that is, the probability that Saturn has a mass $M$ given the available data $d$ and model assumptions (e.g. validity of celestial mechanics). An illustration of this posterior pdf is shown in Figure \ref{fig:saturns_mass}, in which the value $M$ as the peak of the density corresponds to the most probable value for the mass of Saturn which also coincides with the mean value. $M_{\min}$ and $M_{\max}$ denote the values of the mass that deviate by $1\%$ from the mean value $M$, and the shaded area between them is the probability that the mass of Saturn is between the values of $M_{\min}$ and $M_{\max}$. Apart from the most probable value $M$, Laplace estimated that the probability (given by the area) that the real mass of Saturn is between these limits is $11327/11328=0.9999117$. In particular, he wrote ``applying to them my formulae of probability I find that it is a bet of 11,000 against one that the error of this result is not 1/100 of its value''. Today, almost two centuries after this statement was made, Laplace would have won this bet as the current best estimate of Saturn's mass differs only by $0.5\%$ from his.
\begin{figure}[H]
    \centering
	\centerline{\includegraphics[scale=0.65]{saturns_mass.pdf}}
    \caption{Illustration of posterior probability density function of Saturn's mass. $M$ corresponds to the most probable value for the mass at the peak of the density. $M_{\min}$ and $M_{\max}$ denote the values of the mass that deviate by $1\%$ by the mean value $M$. The shaded area signifies the probability that the mass of Saturn is between the values of $M_{\min}$ and $M_{\max}$.}
    \label{fig:saturns_mass}
\end{figure}

However, according to the frequentist interpretation of probability one is not allowed to use probability theory to tackle this problem, as the mass of Saturn is a fixed constant and not a \textit{random variable} that follows a frequency distribution. If we were to interpret Laplace's results from a frequentist perspective we would have to imagine an infinitely large \textit{ensemble} of universes in which everything remains the same but the mass of a single planet. Although one has the liberty to make any kind of syllogisms in order to find a solution, having to seek a frequency interpretation for every problem can be cumbersome and at risk of detaching any notion of intuition from the physical problem.

Faced with the realisation that the frequentist interpretation of probability does not allow one to tackle most scientific questions, the new subject of \textit{statistics} was invented. For instance, in the problem of estimation of Saturn's mass since the mass is not a random variable, one has to create a function, called a statistic, that relates the data to the mass. Since the data are subject to random noise, so does the statistic. One is then free to apply the standard techniques to the statistic. However, the choice and construction of the statistic are often neither clear nor principled. There is no unifying principle relating the various techniques and practices used in order to choose which statistic is more appropriate for a given task. Historically, this lack of a common framework resulted in the creation of a large number of alternative schools of thought of frequentist statistics. Most notably, great statisticians such as \textit{Neyman}, \textit{Pearson} and \textit{Fisher} were responsible for promoting different approaches.

Early in the $20$th century something changed though, Sir \textit{Harold Jeffreys} rediscovered Laplace's Bayesian probability theory, and in $1930s$ he explained and presented it in greater detail and more clearly than Laplace ever did~\parencite{jeffreys1998theory}. Although apparently not enough to convince the most militant proponents of orthodox frequentist schools of the merits of probability theory, Jeffreys' work was the triggering event that acted as a catalyst for a change that lasted until the end of the $20$th and beginning of the $21$th century.

% Cox
In $1946$, \textit{Richard Cox} attempted to end the debate by approaching the problem of \textit{plausible inference} from a different perspective, that of its logical consistency~\parencite{cox1946probability}. Starting by the assumption that we can order different statements based on their plausibility, by assigning a real number to each statement representing how plausible it is, proved that for a calculus of plausible inference to be consistent (i.e. in the sense that if two different methods are permitted they should give the same results), it has to obey the rules of probability theory as defined by Laplace and Jeffreys. The work of Cox is of paramount importance as he effectively showed that any system of plausible inference that is logically consistent has to reduce to Bayesian inference.

% Jaynes

% 90s computer revolution and epilogue
By the last decade of the $20$th century, progress in computer technology and algorithms for probabilistic computation reached and surpassed the level of maturity required for the widespread application of Bayesian inference in most fields of physical science. Therefore, it is no surprise that the principles and methods of Bayesian probability theory have now become an integral and indispensable part of modern science. In the end, Laplace was right: ``It is remarkable that a science which began with the consideration of games of chance should have become the most important object of human knowledge''.

%************************************************

\section{Bayes' theorem}

\subsection{Rules of probability}

Any statement in probability theory can be derived by starting from the Laplace--Jeffreys sum and product rule given below. From these two formulas, expressions such as the ``or'' rule, the marginalisation rule and Bayes' theorem follow easily.

\subsubsection{The ``sum'' rule}
The ``sum'' rule expresses the relation between the probabilities of two mutually exclusive statements $A$ and $\Bar{A}$,
\begin{equation}
    \label{eq:sum_rule}
    p(A\vert B) + p(\Bar{A}\vert B) = 1\,,
\end{equation}
where $p(A\vert B)$ represents the plausibility (probability) of $A$ being true given that $B$ is true and $\Bar{A}$ simply means the opposite of $A$ or that $A$ is false.

\subsubsection{The ``product'' rule}
The ``product'' rule provides a way to compute the joint probability
\begin{equation}
    \label{eq:product_rule}
    p(A,B\vert C) = p(A\vert B,C)p(B\vert C) = p(B\vert A,C)p(A\vert C)\,.
\end{equation}
of both $A$ and $B$ being true given that $C$ is true.

\subsubsection{The ``or'' rule}
For instance, the ``or'' rule that expresses the probability that either $A$ or $B$ is true, given that $C$ is true, can be written as
\begin{equation}
    \label{eq:or_rule}
    p(A\cup B\vert C) = p(A\vert C) + p(B\vert C) - p(A\cap B\vert C)\,,
\end{equation}
follows easily, where $p(A\cap B\vert C)$ is simply another notation for the joint probability $p(A,B\vert C)$ for both events $A$ and $B$ being true given that $C$ is also.

\subsubsection{The marginalisation rule}

Another useful probability rule is the \textit{marginalisation rule} for discrete probability distributions,
\begin{equation}
    \label{eq:marginalisation_discrete}
    p(A\vert C) = \sum_{i} p(A,B_{i}\vert C) = \sum_{i} p(A\vert B_{i},C)p(B_{i}\vert C)\,,
\end{equation}
and for continuous probability distributions,
\begin{equation}
    \label{eq:marginalisation_continuous}
    p(A\vert C) = \int p(A,B\vert C)dB = \int p(A\vert B,C)p(B\vert C)dB\,.
\end{equation}
Equation \ref{eq:marginalisation_discrete} is straightforward to prove starting from the sum rule of equation \ref{eq:sum_rule}, extended to multiple mutually--exclusive events
\begin{equation}
    \label{eq:sum_rule_extended}
    \sum_{i} p(A_{i}\vert B) = 1\,.
\end{equation}
Therefore, starting from equation \ref{eq:sum_rule_extended} we have
\begin{equation}
    \label{eq:marginalisation_proof_1}
    \begin{split}
        \sum_{i} p(A,B_{i}\vert C) &= \sum_{i} p(B_{i}\vert A,C)p(A\vert C) \\
        &= p(A\vert C) \sum_{i} p(B_{i}\vert A,C) = p(A\vert C)\,.
    \end{split}
\end{equation}

\subsubsection{Bayes' theorem}
Arguably the most useful equation that can be derived is the so--called \textit{Bayes' theorem}~\parencite{bayes}
\begin{equation}
    \label{eq:bayes_theorem}
    p(A \vert B, C) = \frac{p(B\vert A, C)p(A\vert C)}{p(B\vert C)}\,,
\end{equation}
that follows directly from equation \ref{eq:product_rule}.

\subsection{Updating degrees of belief}

Although Bayes' theorem is a simple identity that holds for any statements $A$, $B$, and $C$, it also has a special role in the context of plausible inference. In particular, if we set $A\leftarrow \theta$ the parameters of a physical model, $B\leftarrow d$ the experimental data, and $C\leftarrow \mathcal{M}$ the physical model that also includes all assumptions made in an analysis, we get
\begin{equation}
    \label{eq:bayes_rule}
    p(\theta \vert d, \mathcal{M}) = \frac{p(d|\theta, \mathcal{M})p(\theta\vert \mathcal{M})}{p(d\vert \mathcal{M})}\,.
\end{equation}

The importance of equation \ref{eq:bayes_rule} for scientific inference is apparent if we examine each one of the constituent components individually.

\subsubsection{Posterior probability distribution -- $p(\theta\vert d, \mathcal{M})$}

This is the probability distribution of the parameters $\theta$, given the data $d$ and the modelling assumptions $\mathcal{M}$. The posterior is often what we are aspiring to approximate in a \textit{parameter estimation} analysis.

\subsubsection{Prior probability distribution -- $p(\theta\vert \mathcal{M})$}

This probability distribution quantifies any knowledge about the possible values of the parameters $\theta$ prior to seeing the data $d$. We have a whole chapter dedicated to the choice of the prior distribution.

\subsubsection{Likelihood function and sampling distribution -- $p(d\vert \theta, \mathcal{M})$}

The likelihood function is a key component of Bayes' theorem that plays a very important role, that of being the conduit that explains how the transition from prior to posterior takes place. Before we understand the role and properties of the likelihood function we first need to look into the so--called \textit{sampling distribution}.

The \textit{sampling distribution} $p(d\vert \theta )$ expresses the probability distribution of the data $d$ given the values of the model parameters $\theta$. In this picture, the parameters $\theta$ are known and fixed and $p(d\vert \theta )$ is a distribution over the data. If instead, we know the data $d$ and fix them to a specific value of set of values, and we let $\theta$ vary as a free parameter of a set of free parameters, then $p(d\vert \theta )$ is called the \textit{likelihood function}.

The likelihood function is often symbolised as $\mathcal{L}(\theta) = p(d\vert \theta)$ to denote that it is a function of parameters $\theta$ and not a probability distribution over the data $d$. This is very important as statements such as ``the likelihood of the data'' are meaningless and completely miss the point of the likelihood. The likelihood function shows how well the different sampling distributions $p(d\vert \theta)$, parameterised by $\theta$, predict the observed data.

\begin{figure}[H]
    \centering
	\centerline{\includegraphics[scale=0.45]{likelihood_vs_sampling.pdf}}
    \caption{Left: Likelihood function $\mathcal{L}(\mu)$ for the family of sampling distributions as shown on the right. The marked letters indicate the points in which the observed data intersect the sampling distributions. Right: The different groups of contours illustrate the sampling distribution $p(d_{1},d_{2}\vert\mu)$ for different values of $\mu$, ranging from $-2$ to $2$. The vertical line corresponds to the observed data $(d_{1},d_{2})=(0,0)$ and the marked letters indicate the points in which the observed data intersect the sampling distributions.}
    \label{fig:likelihood_vs_sampling}
\end{figure}

To make this more apparent, let us consider a simple example. Let us assume that we have a family of sampling distributions $p(d_{1},d_{2}\vert \mu)$ for the two dimensional data $d=(d_{1},d_{2})$, parameterised by a single parameter $\mu$. An example of such a family of sampling distributions for $\mu\in[-2,2]$ is shown in Figure \ref{fig:likelihood_vs_sampling} on the right. One can see that different values of $\mu$ correspond to different sampling distributions. In order to get a likelihood function $\mathcal{L}(\mu)$ from this family of sampling distributions, we need to specify some observed data for each member of the family. Without loss of generality, we choose the data to be simply $(d_{1}, d_{2}) = (0, 0)$. The data are indicated by a vertical line in the plot that intersects all members of the family of sampling distributions. The points of intersection, marked with letters \textit{a} to \textit{e} in the same plot, can either be in low or high probability regions of the respective sampling distributions. If we now monitor the value of the probability at the intersection points and plot this as a function of the parameter $\mu$ we get the likelihood function shown in the same figure on the left. 

In other words, although related, the notion of likelihood is really different from that of probability in the sense that it expresses the relative capacity of different sampling distributions, belonging however to the same family, to predict and explain the observed data. \textit{Sir Ronald Aylmer Fisher} wrote in 1922 about the difference between probability and likelihood, albeit in the frequentist tradition,
\begin{quote}
    \textit{If we need a word to characterise this relative property of different values of p, I suggest that we may speak without confusion of the likelihood of one value of p being thrice the likelihood of another, bearing always in mind that likelihood is not here used loosely as a synonym of probability, but simply to express the relative frequencies with which such values of the hypothetical quantity p would in fact yield the observed sample. [...] Likelihood also differs from probability in that it is a differential element, and is incapable of being integrated: it is assigned to a particular point of the range of variation, not to a particular element.}
\end{quote}


\subsubsection{Model evidence -- $p(d\vert\mathcal{M})$}

Finally, the \textit{model evidence} is a single real number that expresses the probability of observing the data $d$ given the model $\mathcal{M}$ and acts as a normalisation constant for the posterior
\begin{equation}
    \label{eq:model_evidence_integral}
    p(d\vert \mathcal{M}) = \int p(d\vert\theta,\mathcal{M})p(\theta\vert\mathcal{M})d\theta\,,
\end{equation}
such that $\int p(\theta\vert d, \mathcal{M})d\theta = 1$. The model evidence is often referred to as the \textit{marginal likelihood} due to the way it is represented as an integral. Its role in the task of \textit{model comparison} is great and it will be discussed in great length in the following chapters.

\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{bayes_theorem.pdf}}
    \caption{Illustration of Bayes' theorem. The posterior probability is proportional to the product of the likelihood function and prior probability.}
    \label{fig:bayes_theorem}
\end{figure}

Schematically, we can summarise the above description of Bayes' theorem as
\begin{equation}
    \label{eq:bayes_rule_schematically}
    \text{posterior} = \frac{\text{likelihood}\times\text{prior}}{\text{evidence}}\,.
\end{equation}
In essence, Bayes' theorem in the form of equations \ref{eq:bayes_rule} and \ref{eq:bayes_rule_schematically} is a recipe for updating our degree of belief when new information, in the form of data, becomes available. The factor that upweights or downweights the prior $p(\theta\vert\mathcal{M})$ is the likelihood--to--evidence ratio $p(d\vert\theta,\mathcal{M})/p(d\vert\mathcal{M})$, also known as the \textit{predictive updating factor}. Keynes called this ratio the \textit{coefficient of influence} as it is this that determines how the prior is transformed into the posterior~\parencite{keynes1921treatise}. To better understand this, remember that the model evidence in the denominator is simply the expectation value of the likelihood over the prior probability distribution, so intuitively it expresses some sort of mean value of the likelihood. From this perspective, the predictive updating factor is simply the ratio of the likelihood to its mean value. This means that the prior will be upweighted for those values of $\theta$ that the likelihood is greater than its mean value and downweighted otherwise.


%************************************************


\section{Representing probability distributions}

Before we move on to any examples, it is important to explain how we represent probability distributions in practice. In general, there are two ways that we can do this, each one with different advantages and disadvantages.

\subsection{Function representation}

\subsubsection{Probability mass function}

When the parameter space $\Theta$ is discrete, then a probability distribution can be represented as a \textit{probability mass function (pmf)} $p(\theta)$ that assigns a probability value to each element of space, $p :\Theta\rightarrow [0,1]$. Any pmf has to obey the rule of total probability, that is
\begin{equation}
    \label{eq:total_probability_discrete}
    \sum_{\theta\in\Theta}p(\theta) = 1\,.
\end{equation}
The probability of any composite event $\Theta'\subset \Theta$ can be computed as
\begin{equation}
    \label{eq:composite_event_probability}
    p(\Theta') = \sum_{\theta\in\Theta'}p(\theta)\,.
\end{equation}
Finally, we can compute any expectation value
\begin{equation}
    \label{eq:expectation_value_discrete}
    \mathbb{E}_{p}[f] = \sum_{\theta\subset\Theta} f(\theta)p(\theta)
\end{equation}
for any function $f(\theta)$. Common examples of expectation values include the mean $\mu=\mathbb{E}_{p}[\theta]$ and the variance $\sigma^{2}=\mathbb{E}_{p}[(\theta-\mu)^{2}]$.

\subsubsection{Probability density function}

When the parameter space $\Theta\subseteq \mathbb{R}^{D}$ is continuous, then a probability distribution can be represented as a \textit{probability density function (pdf)} $p(\theta)$ that assigns a probability density value to each element of space, $p :\Theta\rightarrow \mathbb{R}$. Any pdf has to obey the rule of total probability, that is
\begin{equation}
    \label{eq:total_probability_continuous}
    \int_{\theta\in\Theta}p(\theta)d\theta = 1\,.
\end{equation}
Unlike a pmf, a pdf expresses probability density and thus it has to be integrated first to give probabilities. The probability of $\Theta'\subseteq\Theta$ is then
\begin{equation}
    \label{eq:probability_continuous}
    p(\Theta') = \int_{\Theta'\subseteq\Theta} p(\theta)d\theta \,.
\end{equation}
For instance, in 1--D we can compute the probability that $A\leq \theta \leq B$ as
\begin{equation}
    \label{eq:probability_of_region}
    p(A\leq \theta \leq B) = \int_{A}^{B}p(\theta)d\theta\,,
\end{equation}
as the area below the graph of $p(\theta)$ and between $A$ and $B$. Similarly, an expectation value can be computed as
\begin{equation}
    \label{eq:expectation_value_continuous}
    \mathbb{E}_{p}[f] = \int_{\Theta} f(\theta)p(\theta)d\theta\,.
\end{equation}

A crucial difference between probability mass functions and probability densities is that the latter do not transform quite as trivially under parameter transformations $g : \Theta \rightarrow \Phi$. The origin of this complication is that the differential volume $d\theta$ over which we integrate will generally change under such a transformation, and density functions have to change in the opposite way to compensate and ensure that probability is conserved. This change in volume is quantified by the absolute value of the determinant of the Jacobian matrix
\begin{equation}
    \label{eq:jacobian_matrix}
    J_{ij} = \frac{\partial g_{i}}{\partial \theta_{j}}\,,
\end{equation}
where $\phi = g(\theta)$ is the parameter transformation. Thus, the probability density $p(\theta)$ generally transforms as
\begin{equation}
    \label{eq:probability_transformation}
    p(\theta) = p(\phi) \vert \det J \vert\,.
\end{equation}

\subsection{Sample representation}

One of the inherent difficulties in the density function representation of probability distributions is that the computation of expectation values is often intractable as no closed--form solution exists for most applications. An alternative way of representing probability distributions is using a collection of points $S=\lbrace\theta_{1},\theta_{1},\dots,\theta_{n}\rbrace$ in the parameter space $\Theta$, called samples. The generation of samples for a given probability distribution will be the subject of discussion for most of this thesis. For now, it suffices to say that any probability distribution admits a sample representation
\begin{equation}
    \label{eq:sample_representation}
    \theta_{i} \sim p(\theta)\,,
\end{equation}
such that the empirical estimate
\begin{equation}
    \label{eq:empirical_expectation}
    \Hat{f}_{p} = \frac{1}{n}\sum_{i=1}^{n}f(\theta_{i})\,,
\end{equation}
asymptotically approaches the expectation value $\mathbb{E}_{p}[f]$ as $n\rightarrow \infty$.


\section{Asymptotic behaviour}

Let us now turn our attention to the question of the form of the posterior distribution in the limit of infinite data. Understanding the asymptotic behaviour of the posterior when the sample size is large is important for a number of reasons. First, there is practical utility as asymptotic results are often good first--order approximations. Second, as we will discuss in the following chapter, the asymptotic form of the posterior distribution can be utilised to automate the construction of prior distributions. Finally, the \textit{Bernstein--von Mises} theorem, which describes the asymptotic behaviour of the posterior in many cases, allows us to link Bayesian inference to frequentist results.

\subsection{Bernstein--von Mises theorem}

When the number $n$ of observations tends to infinity, the posterior distribution of a smooth finite--dimensional model approaches a normal distribution. In particular, if we denote $d^{(n)}=\lbrace d_{1}, \dots, d_{n}\rbrace$ the set of $n$ observations or data, then the posterior $p(\theta\vert d^{(n)})$ concentrates around the \textit{maximum likelihood estimate (MLE)}:
\begin{equation}
    \label{eq:maximum_likelihood_estimate}
    \Hat{\theta}_{n} = \underset{\theta}{\mathrm{arg\,max}} ~p(d^{(n)}\vert\theta)\,.
\end{equation}
Moreover, MLE is a consistent estimator which means that in the limit of infinite sample size (i.e. $n\to\infty$), $\Hat{\theta}_{n}$ converges to $\theta_{t}$, that is, the true value of the parameter vector. In other words, the asymptotic posterior is centred on the  true parameter value $\theta_{t}$. The precision matrix (i.e. inverse covariance matrix) is equal to $n\mathcal{I}(\Hat{\theta}_{n})$, where the \textit{Fisher information matrix} is defined as 
\begin{equation}
    \label{eq:fisher_information_matrix_definition}
    \begin{split}
        \mathcal{I}(\theta) &= \mathbb{E}_{d\sim p(d\vert \theta)}\left[- \frac{\partial^{2}\log p(d\vert \theta)}{\partial \theta_{i}\partial\theta_{j}}\right] \\
        &= -\int p(d\vert \theta)  \frac{\partial^{2}\log p(d\vert \theta)}{\partial \theta_{i}\partial\theta_{j}} d d \,.
    \end{split}
\end{equation}
In more mathematical terms, we can write down that
\begin{equation}
    \label{eq:posterior_converges_to_normal}
    p(\theta\vert d^{(n)}) \rightarrow \mathcal{N}(\theta\vert \Hat{\theta}_{n}, n^{-1}\mathcal{I}^{-1}(\Hat{\theta}_{n}))\,,
\end{equation}
as $n\to \infty$, where we use the notation $\mathcal{N}(\theta\vert \mu, \Sigma)$ to denote the Gaussian probability density function
\begin{equation}
    \label{eq:gaussian_probability_density_function}
    \mathcal{N}(\theta\vert \mu, \Sigma) = (2\pi )^{-D/2} \vert \Sigma \vert ^{-1/2} \exp \left\lbrace -\frac{1}{2}(\theta - \mu)^{T}\Sigma^{-1}(\theta - \mu)\right\rbrace \,,
\end{equation}
with mean $\mu$ and covariance matrix $\Sigma$, where $D$ is the number of components in the $\theta$ vector (i.e. dimensionality of parameter space). Although this result dates back to \textcite{laplace1810memoire}, today it is known as the \textit{Bernstein-von Mises} theorem~\parencite{van2000asymptotic}. 

One consequence of the above theorem, combined with the fact that the MLE asymptotically follows a normal distribution, allows us to interpret Bayesian \textit{credible intervals} as frequentist \textit{confidence intervals} in the limit of infinite data. 

\subsection{Heuristic argument}

We will now offer an intuitive heuristic argument, rather than a rigorous proof, of the \textit{Bernstein-von Mises} theorem. Let us begin by rewriting Bayes' theorem as
\begin{equation}
    \label{eq:rewrite_bayes_theorem}
    p(\theta\vert d^{(n)}) = \frac{\exp\left\lbrace \log p(\theta) + \log p(d^{(n)}\vert\theta)\right\rbrace}{P(d^{(n)})}\,,
\end{equation}
where $\log p(\theta)$ is the log--prior and
\begin{equation}
    \label{eq:log_likelihood_iid}
    \log p(d^{(n)}\vert\theta) = \sum_{i=1}^{n}\log p(d_{i}\vert \theta)\,,
\end{equation}
is the log--likelihood function of \textit{identically independently distributed (iid)} data, which readily follows from the fact that their sampling distributions are conditionally independent, meaning that
\begin{equation}
    \label{eq:likelihood_iid}
    p(d^{(n)}\vert\theta) = \prod_{i=1}^{n} p(d_{i}\vert \theta)\,.
\end{equation}

The next step is to Taylor--expand both the log--prior and the log--likelihood around their respective maxima. Starting with the log--prior, we can write
\begin{equation}
    \label{eq:log_prior_expansion}
    \log p(\theta) = \log p(\Hat{\theta}_{0}) - \frac{1}{2}(\theta - \Hat{\theta}_{0})^{T}\Lambda_{0}(\Hat{\theta}_{0})(\theta - \Hat{\theta}_{0}) + R_{0}\,,
\end{equation}
where
\begin{equation}
    \label{eq:prior_precision_matrix}
    \Lambda_{0}(\Hat{\theta}_{0}) = \left( - \frac{\partial^{2}\log p(\theta)}{\partial \theta_{i}\partial \theta_{j}}\right) \bigg\vert_{\theta = \Hat{\theta}_{0}}
\end{equation}
and $R_{0}$ denotes any higher--order terms. Notice that since the expansion takes place around the prior maximum $\Hat{\theta}_{0}$, there is no first--order term (i.e. the first derivative is equal to zero). Similarly, we can expand the log--likelihood around the MLE as follows:
\begin{equation}
    \label{eq:log_likelihood_expansion}
    \log p(d^{(n)}\vert\theta) = \log p(d^{(n)}\vert\Hat{\theta}_{n}) - \frac{1}{2}(\theta - \Hat{\theta}_{n})^{T}\Lambda_{n}(\Hat{\theta}_{n})(\theta - \Hat{\theta}_{n}) + R_{n}\,,
\end{equation}
where
\begin{equation}
    \label{eq:likelihood_precision_matrix}
    \Lambda_{n}(\Hat{\theta}_{n}) = \left( - \frac{\partial^{2}\log p(d^{(n)}\vert\theta)}{\partial \theta_{i}\partial \theta_{j}}\right) \bigg\vert_{\theta = \Hat{\theta}_{n}} = \left( - \sum_{\ell=1}^{n}\frac{\partial^{2}\log p(d_{\ell}\vert\theta)}{\partial \theta_{i}\partial \theta_{j}}\right) \bigg\vert_{\theta = \Hat{\theta}_{n}}
\end{equation}
and $R_n$ denotes any terms beyond the second order.

Assuming that the prior and likelihood are sufficiently smooth such that $R_{0}$ and $R_{n}$ can be safely ignored we can write equation \ref{eq:rewrite_bayes_theorem} as
\begin{equation}
    \label{eq:rewrite_bayes_theorem2}
    \begin{split}
        p(\theta\vert d^{(n)}) 
        &\propto \exp \bigg\lbrace  - \frac{1}{2}(\theta - \Hat{\theta}_{0})^{T}\Lambda_{0}(\Hat{\theta}_{0})(\theta - \Hat{\theta}_{0}) \\ 
        &\quad\quad\quad\;\; - \frac{1}{2}(\theta - \Hat{\theta}_{n})^{T}\Lambda_{n}(\Hat{\theta}_{n})(\theta - \Hat{\theta}_{n})\bigg\rbrace \\
        &\propto \exp \left\lbrace  - \frac{1}{2}(\theta - \Tilde{\theta}_{n})^{T}\Tilde{\Lambda}_{n}(\Tilde{\theta}_{n})(\theta - \Tilde{\theta}_{n}) \right \rbrace\,,
    \end{split}
\end{equation}
where $\Tilde{\Lambda}_{n} = \Lambda_{n} + \Lambda_{0}$ and $\Tilde{\theta}_{n}= \Tilde{\Lambda}_{n}^{-1}(\Lambda_{n}\Hat{\theta}_{n}+\Lambda_{0}\Hat{\theta}_{0})$. Comparing the above expression to equation \ref{eq:gaussian_probability_density_function}, we find that the posterior has a Gaussian probability density function
\begin{equation}
    \label{eq:assymptotic_posterior_is_normal}
    p(\theta\vert d^{(n)}) = \mathcal{N}(\theta\vert \Tilde{\theta}_{n}, \Tilde{\Lambda}_{n}^{-1})\,. 
\end{equation}

In the limit that $n\to\infty$, the sum in equation \ref{eq:likelihood_precision_matrix} completely dominates the calculation leading to $\Tilde{\Lambda}_{n} \to \Lambda_{n}$ and $\Tilde{\theta}_{n}\to \Hat{\theta}_{n}$. This means that asymptotically
\begin{equation}
    \label{eq:assymptotic_posterior_is_normal2}
    p(\theta\vert d^{(n)}) \to \mathcal{N}(\theta\vert \Hat{\theta}_{n}, \Lambda_{n}^{-1})\,. 
\end{equation}
Furthermore, according to the \textit{law of large numbers}, which states that ``the average of a large number of trials approaches the expectation value``,  $\Lambda_{n}$ as given by the sum in equation \ref{eq:likelihood_precision_matrix} is asymptotically equal to $n \mathcal{I}(\Hat{\theta}_{n})$. Therefore, we can write down that
\begin{equation}
    \label{eq:assymptotic_posterior_is_normal3}
    p(\theta\vert d^{(n)}) \to \mathcal{N}(\theta\vert \Hat{\theta}_{n}, n^{-1} \mathcal{I}^{-1}(\Hat{\theta}_{n}))\,,
\end{equation}
which concludes our heuristic derivation.

\section{Estimating parameters}

\subsection{Coin--tossing experiment}

Let us now consider a simple example of Bayesian parameter estimation. Suppose that we have a coin and we want to determine whether the coin is fair or not. A simple way to quantify the fairness of a coin is to introduce a \textit{bias} parameter $F$ such that $F=1/2$ means that the coin is fair, whereas any other value in the range $0\leq F \leq 1$ denotes that the coin is biased. $F=0$ corresponds to a coin which always lands on \textit{tails} and $F=1$ to a one that always lands on \textit{heads}. We can then divide the continuous range of $F$ into a discrete number of propositions (e.g. $ 0\leq F \leq 0.01$, $0.01\leq F \leq 0.02$, etc.). Our state of knowledge about the fairness of the coin is summarised by our degree of belief, quantified as a probability, of each one of those intervals (e.g. $p(0\leq F \leq 0.01)$, $p(0.01\leq F \leq 0.02)$, etc.).

In order to collect some data we just have to toss the coin a few times and monitor the number of times $H$ the coin lands on heads as well as the total number of trials $N$. The number of times that the coin lands on tails is simply $N-H$. Furthermore, to better understand the iterative nature of Bayes' theorem for updating our degree of belief, we will keep not only the final outcome of the experiment (i.e. the total number $H$ that the coin landed on heads in $N$ trials) but also all the intermediate values.

Since our aim is to estimate the posterior distribution $p(F\vert H,N)$, that is, the probability distribution of $F$ given the observed data in terms of the number of heads $H$ and the number of trials $N$, we need to define all the components that enter Bayes's theorem. Starting with the prior probability distribution $p(F)$ we will use two choices in order to demonstrate their effect on the posterior. The first choice of prior is to be agnostic, before seeing the data, about the fairness of the coin and thus assume that intervals of the same size in the range $0\leq F \leq 1$ are equally probable. This is quantified by the uniform probability density function
\begin{equation}
    \label{eq:uniform_prior}
    p(F) = 
    \begin{cases}
    1, & \text{if } 0 \leq F \leq 1 \\
    0, & \text{otherwise}\,.
    \end{cases}
\end{equation}
The other prior that we will test is more informative than the first and assumes that it is more probable that the coin is fair, or at least close to it. To this end, we will use a normal prior with a Gaussian probability density
\begin{equation}
    \label{eq:gaussian_prior}
    p(F\vert \mu, \sigma) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(F-\mu)^{2}}{2\sigma^{2}}\right)\,,
\end{equation}
centred around the mean value $\mu=1/2$ with standard deviation $\sigma=0.1$. This kind of prior assigns most prior probability to values of $F$ close to that of $1/2$ that correspond to a fair coin. Both priors can be seen in the top--left panel of Figure \ref{fig:coins} where the uniform prior corresponds to the continuous line and the normal prior to the dashed line.

To get the \textit{likelihood function}, we start by choosing the sampling distribution $p(H, N\vert F)$, that is, the probability distribution of the data $H$ and $N$ given the value of $F$. For this task, we choose the \textit{binomial} probability distribution with probability density given by
\begin{equation}
    \label{eq:binomial_likelihood_coins}
    p(H, N \vert F) = \binom{N}{H} F^{H} (1-F)^{N-H}\,.
\end{equation}
The above formula can be understood as follows: $R$ heads occur with probability $F^{H}$ and $N-H$ tails with probability $(1-F)^{N-H}$. The combinatorial factor that $H$ heads can occur anywhere among the $N$ trials, and there are $\binom{F}{H}$ of distributing $H$ heads between $N$ trials. If we fix $H$ and $N$ to their observed values then $\mathcal{L}(F)=p(H, N \vert F)$ is simply the likelihood function $F$.
\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{coins.pdf}}
    \caption{The evolution of the posterior probability distribution of a coin--tossing experiment for increasing number of trials. $F$ is the bias parameter that we want to estimate. $H$ is the number of times the coin landed on \textit{heads} and $N$ is the total number of trials. The continuous line corresponds to the case of using a uniform prior whereas the dashed line to a normal prior. The dotted line shows the true (unknown) value of $F$.}
    \label{fig:coins}
\end{figure}

According to Bayes' theorem then, the posterior distribution can be written as
\begin{equation}
    \label{eq:posterior_coins}
    p(F\vert H, N) = \frac{p(H, N\vert F)p(F)}{p(H,N)}\,,
\end{equation}
where the model evidence is simply the normalisation factor
\begin{equation}
    \label{eq:model_evidence_coins}
    p(H,N) = \int_{0}^{1}p(H, N\vert F)p(F)d F\,.
\end{equation}
In the case of the uniform prior of equation \ref{eq:uniform_prior} the above integral can be computed analytically. This is not however true for the case of the normal prior of equation \ref{eq:gaussian_prior}, for which numerical integration is necessary.

Figure \ref{fig:coins} shows the evolution of the posterior distribution of equation \ref{eq:posterior_coins}, starting from the prior distribution in the top--left panel, as we gradually increase the number of data points that are included in the analysis. The posteriors with both prior choices are illustrated, also highlighting the effect of the prior choice on the posterior. As we can see from the same figure, while the number of trials $N$ remains small (e.g. $N\leq 5$), the posterior corresponding to the informative normal prior remains unaffected. On the other hand, the posterior corresponding to the more agnostic uniform prior responds rapidly to the new data and concentrates close to the lower half of the $F$ range. The reason for this difference is the fact that the few initial data points do not carry sufficient information compared to the normal prior, but they do so compared to the less informative uniform prior. For a higher number of trials $N$ the behaviour
is changing though. Both posteriors rapidly concentrate around the same value of $F$. This indicates that the prior, while important in the low--data regime, does not affect the posterior when the amount of data is substantial. This behaviour is a direct consequence of the \textit{Bernstein--von Mises theorem}~\parencite{van2000asymptotic} which, under quite general conditions, states that ``for sufficiently nice prior probabilities, in the limit of infinite data the posterior converges to a Gaussian distribution independently of the initial prior''. This also explains the symmetric form of the posterior in Figure \ref{fig:coins} when the number of trials is large, as well as its reduced width.


\subsection{Fitting a model to data}

A general problem that scientists are often called to solve is that of fitting a mathematical model $m(t\vert\theta)$ to the data $d$, where the pairs $(t,d)$ constitute the measured data points. A simple example of a model is the straight line $m(t\vert \alpha, \beta) = \alpha + \beta t$. $t=\lbrace t_{i}\rbrace$ could be a sequence of time instances, positions or any other physical quantity in which the measurements $d=\lbrace d_{i} \rbrace$ are collected. The task of \textit{model fitting} lies within the context of Bayesian parameter estimation as the main goal is to approximate the posterior probability distribution $p(\theta\vert d,\mathcal{M})$, that is, the probability distribution of the parameters $\theta$, given the data $d$ and the model $\mathcal{M}$. The latter consists of the actual mathematical model $m(\theta)$ plus all the assumptions made during the analysis.
\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{fitting_data.pdf}}
    \caption{Example of data $d$ and the straight line model $m(t\vert,\alpha, \beta)=\alpha + \beta t$ that was used to generate them assuming true values $\theta^{*}=(\alpha^{*},\beta^{*})=(1,1)$ and $\epsilon=0.1$.}
    \label{fig:fitting_data}
\end{figure}
Usually, the data $d$ are assumed to be a noise--corrupted realisation of the model, meaning
\begin{equation}
    \label{eq:forward_model}
    d = m(t\vert\theta^{*}) + \epsilon\,,
\end{equation}
where $\theta^{*}$ are the true values of the parameters that we, as scientists, are aspiring to approximate, and $\epsilon$ is the noise or uncertainty added to the model realisation $m(t\vert\theta^{*})$ in order to generate the data $d$. In the absence of any noise (i.e. $\epsilon = 0$), the data are no longer corrupted and the value of $\theta^{*}$ can be estimated with certainty. As we have discussed already, this is an idealised scenario and in real life, our incomplete knowledge about the physical mechanism which produced the data introduces a non--zero noise contribution $\epsilon$.

As the assumed physical model $m(\theta)$ is often \textit{deterministic}, it follows that the sampling probability of the noise $\epsilon$ is identical to that of the data $d$, or in other words that
\begin{equation}
    \label{eq:sampling_distribution_data_noise}
    p(d\vert \theta, \mathcal{M}) = p(\epsilon\vert \theta, \mathcal{M})\,.
\end{equation}
Furthermore, as the underlying physical mechanisms that give rise to the noise, often consist of a plethora of contributing factors one usually employs the \textit{central limit theorem (CLT)} in order to justify the use of a zero--mean normal sampling distribution
\begin{equation}
    \label{eq:sampling_distribution_noise}
    \epsilon \sim \mathcal{N}(0, \Sigma)\,,
\end{equation}
where $\Sigma$ is the $D\times D$ positive--definite symmetric covariance matrix of the noise. The likelihood function is thus assumed to be Gaussian
\begin{equation}
    \label{eq:gaussian_likelihood}
    p(d\vert \theta, \mathcal{M}) = \det(2\pi\Sigma)^{-\frac{1}{2}}\exp\left\lbrace-\frac{1}{2}\left[ d - m(t\vert\theta)\right]^{T}\Sigma^{-1}\left[ d - m(t\vert\theta)\right]\right\rbrace\,.
\end{equation}
Contrary to popular opinion, and as we will discover in the next chapter where the principle of maximum entropy is discussed, a Gaussian function, or equivalently a normal sampling distribution, is quite often a very good choice. There are of course applications in which other sampling distributions will be more appropriate (e.g. \textit{Poisson} for number counts). However, when only the (co--)variance of the noise is known, the normal distribution is the most conservative choice one can make~\parencite{gregory2005bayesian,  hogg2010data,sivia2006data}. Of course, the accurate estimation of the covariance is on its own a difficult problem. Furthermore, if the covariance matrix $\Sigma$ is estimated using simulated data $d_{i}\sim p(d)$, for instance
\begin{equation}
    \label{eq:covariance_estimate}
    \Hat{\Sigma} = \frac{1}{n-1}\sum_{i=1}^{n}(d_{i}-\Bar{d})(d_{i}-\Bar{d})^{T}\,,
\end{equation}
where $\Bar{d}=n^{-1}\sum_{i=1}^{n}d_{i}$, the Gaussian likelihood function must to be modified to account for the uncertainty of the covariance estimate~\parencite{sellentin2015parameter}.

Given the model, the data, and the likelihood, the final requirement in order to conduct Bayesian inference is the prior distribution $p(\theta\vert\mathcal{M})$. This will of course depend on the specific application and we will discuss the choice of prior in more detail in the next chapter. The task of approximating the posterior $p(\theta\vert d, \mathcal{M})$, that we have discussed so far, is in general analytically intractable for all but the simplest models and prior choices. In the rest of this thesis, we will present various methods and computational tools that will allow us to tackle problems such as this one. As an illustration, we offer Figure \ref{fig:fitting_posterior} which shows the 1--D and 2--D marginal posteriors of fitting the straight line model $m(t\vert,\alpha, \beta)=\alpha + \beta t$ to the data of Figure \ref{fig:fitting_data} assuming flat/uniform priors $\alpha, \beta \sim \mathcal{U}(-5,5)$. Although this is a relatively simple model, the same principles and techniques that were used to estimate its posterior also extend to more complicated applications.

\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.85]{Graphics/fitting_posterior.pdf}}
    \caption{1--D and 2--D marginal posterior contours of fitting the straight line model $m(t\vert,\alpha, \beta)=\alpha + \beta t$ to the data of Figure \ref{fig:fitting_data} assuming flat/uniform priors $\alpha, \beta \sim \mathcal{U}(-5,5)$. The black lines show the true values of the parameters $\theta^{*}=(\alpha^{*},\beta^{*})=(1,1)$ which were used to generate the data.}
    \label{fig:fitting_posterior}
\end{figure}

%************************************************