% !TEX TS-program = pdflatex
% !TEX root = ../ArsClassica.tex

%************************************************
\chapter{Making predictions and evaluating models}
\label{chp:predictions}
%************************************************

\begin{flushright}
\itshape
Tomorrow belongs to those who can hear it coming. \\
\medskip
--- David Bowie
\end{flushright}


\section{Making predictions}

Making predictions is a paramount task for most scientific analyses. Often the parameters of a model are not observable quantities and we have to rely on simulated data to assess the validity of our models. In this section, we will discuss how different kinds of predictive checks can help us avoid various common pitfalls in Bayesian analyses.

\subsection{Prior predictive checks}

A very useful practice, that is always recommended, is to check the predictions of the prior distribution under the specified model~\parencite{gelman2013bayesian}. Prior predictive checks constitute an elegant way of finding out what kind of data are compatible (i.e. can be described or explained) by our choice of prior and model. The main benefits of this approach are two. First of all, this can help diagnose priors that are either too restrictive or too wide. Furthermore, assuming that the choice of prior distribution is justified, prior predictive checks can help shield against severe cases of \textit{model misspecification} in which no specific set of parameters corresponds to a model that describes the observed data sufficiently well.

In order to assess whether a particular choice of prior distribution is appropriate we need a way to produce simulated data that are consistent with the prior. The Bayesian way of doing this is by sampling the simulated data
\begin{equation}
    \label{eq:samples_from_prior_predictive}
    d_{sim} \sim p(d)\,,
\end{equation}
from the \textit{prior predictive distribution} 
\begin{equation}
    \label{eq:prior_predictive_distribution}
    p(d)=\int p(d\vert\theta)p(\theta)d\theta\,.
\end{equation}
Generating simulated data using the prior predictive distribution in practice can be done easily by first simulating parameters from the prior distribution
\begin{equation}
    \label{eq:sample_from_prior}
    \theta_{sim} \sim p(\theta)\,,
\end{equation}
and then simulating the data according to the sampling distribution
\begin{equation}
    \label{eq:sample_from_sampling_distribution}
    d_{sim} \sim p(d\vert\theta_{sim})\,,
\end{equation}
given the simulated parameters. The simulated pairs $(d_{sim},\theta_{sim})$ constitute samples from the joint distribution
\begin{equation}
    \label{eq:sample_from_joint_distribution}
    (d_{sim},\theta_{sim}) \sim p(d,\theta)\,,
\end{equation}
and thus
\begin{equation}
    \label{eq:samples_from_prior_predictive2}
    d_{sim} \sim p(d)\,,
\end{equation}
are simulated from the prior predictive distribution.

\subsection{Posterior predictive checks}

Similarly to the prior predictive checks, but this time conditioned on the observed data $d_{obs}$, one can perform \textit{posterior predictive checks} \parencite{gelman2013bayesian}. The latter offer a way of measuring whether a model is able to capture aspects of the data sufficiently well. Just like prior predictive checks that simulate data consistent with the prior, posterior predictive checks on the other hand simulate data that are consistent with the posterior distribution. 

In practice, the process of generating simulated data
\begin{equation}
    \label{eq:sample_from_posterior_predictive}
    d_{sim}\sim p(d\vert d_{obs})\,,
\end{equation}
from the posterior predictive distribution
\begin{equation}
    \label{eq:posterior_preditive_distribution}
    p(d \vert d_{obs}) = \int p(d\vert\theta)p(\theta \vert d_{obs})d\theta\,,
\end{equation}
starts by simulating parameters from the posterior distribution
\begin{equation}
    \label{eq:sample_from_posterior}
    \theta_{sim} \sim p(\theta\vert d_{obs})\,.
\end{equation}
It is important to remind the reader that this last step, unless conjugate priors are used, is highly non--trivial and often requires advanced computational algorithms that are the subject of the next chapter. For now, it suffices to understand that the simulation of parameters as described by the relation \ref{eq:sample_from_posterior} is possible although generally difficult, requiring careful steps. The last step is to generate the simulated data according to sampling distribution
\begin{equation}
    \label{eq:sample_from_sampling_distribution2}
    d_{sim} \sim p(d\vert\theta_{sim})\,,
\end{equation}
given the simulated parameters. $d_{sim}$ then constitute samples from the posterior predictive distribution.

%************************************************

\section{Evaluating and comparing models}

A key goal of science is to determine which model under consideration better accounts for the observed data. As we will discover shortly, this is generally done by assessing the predictive power of different models. From a Bayesian perspective, there are two ways one can approach this subject. The first uses \textit{Bayes factors} and compares models based on their \textit{prior predictive performance}, meaning their capacity to explain the observed data using only the information encoded in the prior distribution. On the other hand, the second approach uses the notion of \textit{cross--validation} in order to compare models based on their \textit{posterior predictive performance}, meaning their ability to make predictions for out--of--sample data, meaning, future or unseen data, using what we learned from the observed data.

In the \textit{prior predictive} approach, the main quantity that goes into the calculation of the \textit{Bayes factor} is the \textit{prior predictive probability} $p(d\vert \mathcal{M}_{i})$ of the observed data $d$ given a model $\mathcal{M}_{i}$, also known as the \textit{marginal likelihood} or the model \textit{evidence} \parencite{jaynes2003probability, gregory2005bayesian}. Naturally, the \textit{prior predictive} approach is sensitive to the choice of priors. On the other hand, in the \textit{posterior predictive} approach, we compute the \textit{posterior predictive probability} of some subset of the observed data given the rest of the data. Typically, \textit{cross--validation} means that this process is repeated several times, trying to predict different subsets of data, until the entire data set is assessed as held--out data.

\subsection{Bayes factors}

The probability of a model $\mathcal{M}_{i}$ given the data $d$ can be computed using Bayes' theorem
\begin{equation}
    \label{eq:probability_of_model_given_data}
    p(\mathcal{M}_{i}\vert d) = \frac{p(d\vert\mathcal{M}_{i})p(\mathcal{M}_{i}) }{p(d)}\,,
\end{equation}
where $p(d\vert\mathcal{M}_{i})$ is the probability of the data given the model, $p(\mathcal{M}_{i})$ is the prior probability of the model, and $p(d)$ is the prior predictive probability of the data. We can compare two models, $\mathcal{M}_{i}$ and $\mathcal{M}_{j}$, by computing their \textit{odds ratio}
\begin{equation}
    \label{eq:model_odds_ratio}
    \frac{p(\mathcal{M}_{i}\vert d)}{p(\mathcal{M}_{j}\vert d)} = \frac{p(d\vert\mathcal{M}_{i})p(\mathcal{M}_{i})}{p(d\vert\mathcal{M}_{j})p(\mathcal{M}_{j})}
\end{equation}
The ratio $BF_{ij}=p(d\vert\mathcal{M}_{i})/p(d\vert\mathcal{M}_{j})$ in the above expression is called the \textit{Bayes factor}. Once the model priors $p(\mathcal{M}_{i})$ and $p(\mathcal{M}_{j})$ are specified, model comparison using Bayes factors amounts to the calculation of the model evidences $p(d\vert\mathcal{M}_{i})$ and $p(d\vert\mathcal{M}_{j})$.

\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{Graphics/bayes_factor.pdf}}
    \caption{\textit{Prior predictive distributions} $p(d\vert\mathcal{M}_{i})$ and $p(d\vert\mathcal{M}_{j})$ for models ${M}_{i}$ and ${M}_{j}$ respectively. The dashed line, that intersects both distributions, corresponds to the actual observed data. The Bayes factor is simply the ration between the values at the two points of intersection, in this case favouring ${M}_{j}$ over ${M}_{i}$. It is clear that for other realisations of the actual observed data (e.g. on the right part of the data vector) the other model would be favoured.}
    \label{fig:bayes_factor}
\end{figure}

Despite the apparent simplicity of model comparison using Bayes factors, caution must be exercised when applying the method in real analyses. There are three main reasons for this warning, all of which are sometimes overlooked in practice leading to catastrophic results.

The first reason has to do with the computational difficulty of estimating the model evidence $p(d\vert \mathcal{M})$, particularly in problems with many parameters. In fact, as we will see in detail in the next part of this thesis, a large collection of methods have been designed with the sole purpose of estimating the model evidence. Therefore, the practitioner has to be familiar with the range of applicability of each method as well as their intrinsic limitations when deciding which technique to use.

The second reason, equally important with the first, is the sensitivity of the model evidence to the choice of prior distribution. This sensitivity is apparent if we just notice that the model evidence is simply the \textit{prior predictive distribution},
\begin{equation}
    \label{eq:prior_predictive_distribution_2}
    p(d\vert\mathcal{M}) = \int p(d\vert\theta,\mathcal{M})p(\theta\vert \mathcal{M})d\theta\,,
\end{equation}
evaluated at the observed data $d$. However, we argue that this sensitivity is not a weakness of the method as it is often portrayed, but a strength that needs to be properly understood.

\begin{figure}[H]
    \centering
	\centerline{\includegraphics[scale=0.65]{Graphics/model_evidence.pdf}}
    \caption{The characteristic width $\delta \theta$ of the likelihood function $p(d\vert\theta,\mathcal{M}_{1})$ and $\Delta \theta$ of the prior distribution.}
    \label{fig:model_evidence}
\end{figure}

In order to understand the effects of the choice of priors on the Bayes factor, let us consider a simple example. Imagine that we have to compare two models, $\mathcal{M}_{1}$ with a single scalar parameter $\theta$ and $\mathcal{M}_{0}$ with no free parameters. Furthermore, let us assume that $\mathcal{M}_{0}$ is nested in $\mathcal{M}_{1}$, meaning that the more complex model, $\mathcal{M}_{1}$, reduces to the simpler one, $\mathcal{M}_{0}$, for a specific parameter value, $\theta=\theta_{0}$.

Let us now assume that the prior on parameter $\theta$ is flat or uniform, such that,
\begin{equation}
    \label{eq:flat_prior_theta}
    p(\theta\vert \mathcal{M}_{1}) = \frac{1}{\Delta \theta}\,,
\end{equation}
and that the likelihood function is sharply peaked around a value $\theta_{1}$ such that,
\begin{equation}
    \label{eq:sharply_peaked_likelihood}
    \int p(d\vert \theta, \mathcal{M}_{1}) d\theta = p(d\vert \theta_{1}, \mathcal{M}_{1})\times \delta \theta\,,
\end{equation}
where $\delta \theta$ is its characteristic width. It is easy to show that for the case of Gaussian likelihood, centred around $\theta_{1}$, the characteristic width is simply $\delta\theta = \sqrt{2\pi}\sigma$, where $\sigma$ is the standard deviation.

The model evidence of $\mathcal{M}_{1}$ is then simply,
\begin{equation}
    \label{eq:model_evidence_m1}
    \begin{split}
        p(d\vert \mathcal{M}_{1}) &= \int p(d\vert \theta, \mathcal{M}_{1}) p(\theta\vert \mathcal{M}_{1}) d\theta \\
        &= p(d\vert \theta_{1}, \mathcal{M}_{1})\times \frac{\delta\theta}{\Delta \theta}\,.
    \end{split}
\end{equation}
Since the model $\mathcal{M}_{0}$ has no free parameters, no integration is required and its model evidence $p(d\vert\mathcal{M}_{0})$ is simply the likelihood function of $\mathcal{M}_{1}$ evaluated at $\theta=\theta_{0}$, or,
\begin{equation}
    \label{eq:model_evidence_m0}
    p(d\vert \mathcal{M}_{0}) = p(d\vert\theta_{0},\mathcal{M}_{1})\,.
\end{equation}
Therefore, the Bayes factor is,
\begin{equation}
    \label{eq:bayes_factor_approximate}
    B_{10} = \frac{p(d\vert \mathcal{M}_{1})}{p(d\vert \mathcal{M}_{0})} = \frac{p(d\vert \theta_{1}, \mathcal{M}_{1})}{p(d\vert\theta_{0},\mathcal{M}_{1})} \times \frac{\delta\theta}{\Delta \theta}\,.
\end{equation}
The first term in equation \ref{eq:bayes_factor_approximate} is the likelihood ratio that always favours the most complex model $\mathcal{M}_{1}$ since it contains $\mathcal{M}_{0}$ as a special case. In other words, the first term is always greater than one as the most complex model can always fit the data better than the simpler one.

On the other hand, the second term in equation \ref{eq:bayes_factor_approximate} that consists of the ratio of the likelihood width $\delta\theta$ to the prior width $\Delta\theta$ penalises the most complex model $\mathcal{M}_{1}$, since $\delta\theta < \Delta \theta$, for any ``wasted'' regions of parameter space that are ruled out by the data. This term quantifies the so--called \textit{principle of parsimony} or \textit{Occam's razor} as it most commonly known. The principle, often attributed to \textit{William of Ockham}, states that ``entities should not be multiplied beyond necessity'', meaning that between competing models or hypotheses the simplest one is often preferred. Therefore, the Bayes factor will only favour the most complex model (i.e. $\mathcal{M}_{1}$) only if the likelihood ratio is large enough to overcome the penalty introduced by \textit{Occam's razor}. This intrinsic property of Bayes factors to prefer simpler models, that originates directly from the reliance to the prior distributions, is what makes them so useful in practice.

Now that we understand how sensitive the Bayes factor is to the choice of priors we can discuss some ways that we can shield our analyses against potential problems. First and foremost, Bayesian model comparison can be performed only when \textit{proper} priors are used. By that we mean that \textit{improper} priors such as uniform/flat priors ranging from $-\infty$ to $+\infty$ are not acceptable. Only prior distributions that can be integrated and normalised to unity are \textit{proper} in this sense. However, the use of \textit{proper} priors is not enough, the choice of priors needs to be well--justified too. Priors that are not defined using a principled process (e.g. MaxEnt, Jeffreys, etc.), and sometimes even those that do, can lead to significant deviations in the value of a Bayes factor. For this reason, we urge caution not to over--emphasise the significance of, and instead mostly neglect Bayes factors of $\mathcal{O}(1)$. 

The third, and final in our list of reasons, has to do with the open--ended nature of the task model comparison. In particular, model comparison often takes place in the context of a finite set of possible models under investigation with no guarantee whatsoever that one of those models captures perfectly, or even sufficiently, the true data generating process. In that sense, in almost all cases, inference takes place under conditions of \textit{model misspecification}. This brings to mind the saying by \textit{Box}, that ``all models are wrong, but some are useful''. The fact that the value of a Bayes factor might seem to favour one model over another does not mean that the first model is ``correct'', only that it is better than the second. Both models might be far from the true data generating process and the Bayes factor will offer generally no indication of that.

\subsection{Cross--validation}

For a model to be useful in practice it must be able to make accurate predictions regarding unseen data. The generalisation uncertainty of a model is often quantified using some measure of the out--of--sample predictive accuracy. A commonly used scoring rule for the out--of--sample predictive accuracy for $n$ data points is the \textit{expected log--pointwise predictive density},
\begin{equation}
    \label{eq:elpd}
    \mathrm{ELPD} = \sum_{i=1}^{n}\int p_{t}(d_{i})\log p(d_{i}\vert d_{obs}) d d_{i}\,,
\end{equation}
where $p_{t}(d_{i})$ is the probability density of the true data generative process which is in general unknown and $p(d_{i}\vert d_{obs})$ is the posterior predictive density.

Another useful quantity is the \textit{log--pointwise predictive density},
\begin{equation}
    \label{eq:lpd}
    \mathrm{LPD} = \sum_{i=1}^{n}\log p(d_{i}\vert d_{obs}) = \sum_{i=1}^{n}\log \int p(d_{i}\vert \theta) p(\theta\vert d_{obs})d\theta\,.
\end{equation}
LPD of the observed data $d_{obs}$ is an overestimate of ELPD. We can compute LPD in practice as,
\begin{equation}
    \label{eq:lpd_estimate}
    \Hat{\mathrm{LPD}} = \sum_{i=1}^{n}\log \left( \frac{1}{J}\sum_{j=1}^{J}p(d_{i}\vert\theta_{j})\right)\,,
\end{equation}
where $\theta_{j}\sim p(\theta\vert d_{obs})$ are samples from the posterior distribution.

\subsubsection{Leave--one--out cross--validation}

The term cross--validation refers to the practice of estimating the out--of--sample predictive accuracy of a model. In general, the method requires running the analysis multiple times, each time excluding a different portion of the data. The excluded part of the data is then used in order to assess the predictive accuracy of the model. Once the whole dataset is covered, the total accuracy is computed as the average accuracy over all runs,
\begin{equation}
    \label{eq:elpd_loo}
    \mathrm{ELPD}_{\mathrm{LOO}} = \sum_{i=1}^{n}\log p(d_{i}\vert d_{-i})\,,
\end{equation}
where,
\begin{equation}
    \label{eq:loo_predictive_density}
    p(d_{i}\vert d_{-i}) = \int p(d_{i}\vert \theta) p(\theta\vert d_{-i})d\theta \,,
\end{equation}
is the leave--one--out predictive density given the data without the $i$--th datapoint~\parencite{geisser1979predictive, bernardo2009bayesian, gneiting2007strictly}.

Assuming that the $n$ datapoints are conditionally independent in the data generative model, then we can approximate equation \ref{eq:loo_predictive_density} using draws from the posterior $\theta_{j}\sim p(\theta\vert d_{obs})$ and importance weights~\parencite{gelfand1992model},
\begin{equation}
    \label{eq:loo_importance_weights}
    w_{ij} = \frac{1}{p(d_{i}\vert\theta_{j})} \propto \frac{p(\theta_{j}\vert d_{-i})}{p(\theta_{j}\vert d_{obs})}\,,
\end{equation}
leading to the importance sampling leave--one--out predictive density,
\begin{equation}
    \label{eq:isloo_predictive_density}
    p(d_{i}\vert d_{-i}) = \frac{\sum_{j=1}^{J}w_{ij}p(d_{i}\vert\theta_{j})}{\sum_{j=1}^{J}w_{ij}} = \frac{1}{\frac{1}{J}\sum_{j=1}^{J}[p(d_{i}\vert\theta_{j})]^{-1}}\,.
\end{equation}
However the posterior $p(\theta\vert d_{obs})$ is likely to have a smaller variance than then $p(\theta\vert d_{-i})$ distributions leading to insufficient overlap between their typical sets and high--variance importance weights. \textcite{ionides2008truncated} showed that truncating the importance weights,
\begin{equation}
    \label{eq:truncated_importance_weights}
    \Tilde{w}_{ij} = \min\left( w_{ij}, \sqrt{J} \,\Bar{w}_{i} \right)\,,
\end{equation}
where
\begin{equation}
    \label{eq:mean_importance_weight}
    \Bar{w}_{i} = \frac{1}{J} \sum_{j=1}^{J}w_{ij}\,,
\end{equation}
leads to provable finite--variance weights at the cost of introducing bias. \textcite{vehtari2017practical} proposed instead to fit a generalised Pareto distribution to the upper tail of the importance weights, in order to smooth the weights, leading to improved estimates.

\subsubsection{WAIC}

The \textit{Watanabe--Akaike} or \textit{widely applicable information criterion (WAIC)}~\parencite{watanabe2010asymptotic} offers a different way to approximate ELPD and is defined as,
\begin{equation}
    \label{eq:waic}
    \Hat{\mathrm{ELPD}}_{\mathrm{WAIC}} = \Hat{\mathrm{LPD}} - \Hat{p}_{\mathrm{WAIC}}\,,
\end{equation}
where,
\begin{equation}
    \label{eq:p_waic}
    \Hat{p}_{\mathrm{WAIC}} = \sum_{i=1}^{n} \mathrm{Var}_{\theta\sim p(\theta\vert d_{obs})}[\log p(d_{i}\vert \theta)]\,,
\end{equation}
is the estimated effective number of parameters expressed as the posterior variance of the log predictive density of each datapoint. Equation \ref{eq:p_waic} can be computed using posterior samples.

\subsection{Model averaging}

Standard practice ignores model uncertainty and instead focuses on the most probable models as deduced by their Bayes factors. This approach leads to over--confident estimates and ignores the fact that often more than one model can describe the data sufficiently. There is, however, a different approach that we can follow in order to deal with the model uncertainty and properly account for the plethora of plausible models, called \textit{Bayesian model averaging} \parencite{madigan1996bayesian}.

Let $\mathcal{M}_{1}, \mathcal{M}_{2}, \dots, \mathcal{M}_{M}$ be a set of $M$ models with posterior model probabilities $p(\mathcal{M}_{1}\vert d),\, p(\mathcal{M}_{2}\vert d),\, \dots,\, p(\mathcal{M}_{M}\vert d)$ and posterior distributions $p(\theta\vert d, \mathcal{M}_{1}),\,\allowbreak p(\theta\vert d, \mathcal{M}_{2}),\,\allowbreak \dots,\,\allowbreak p(\theta\vert d, \mathcal{M}_{M})$ respectively. Then \textit{Bayesian model averaging} relies on the \textit{marginal} posterior density,
\begin{equation}
    \label{eq:marginal_posterior_density}
    p(\theta\vert d) = \sum_{i=1}^{M} p(\theta\vert d, \mathcal{M}_{i}) p(\mathcal{M}_{i}\vert d)\,,
\end{equation}
which is no longer conditioned on a model.

Moreover, predictions can be made by averaging over all models, weighted proportional to their posterior model probabilities, thereby incorporating model uncertainty using the marginal posterior predictive density,
\begin{equation}
    \label{eq:marginal_posterior_predictive_density}
    p(d\vert d_{\mathrm{obs}} ) = \sum_{i=1}^{M} p(d\vert d_{\mathrm{obs}}, \mathcal{M}_{i}) p(\mathcal{M}_{i}\vert d_{\mathrm{obs}})\,,
\end{equation}
where $d_{\mathrm{obs}}$ are the available observed data and $d$ are the new predicted data. \textcite{madigan1994model} note that averaging over all models in this fashion leads to higher predictive accuracy than using any single model individually.