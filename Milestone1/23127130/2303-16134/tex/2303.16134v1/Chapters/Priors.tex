% !TEX TS-program = pdflatex
% !TEX root = ../ArsClassica.tex

%************************************************
\chapter{Quantifying prior knowledge}
\label{chp:priors}
%************************************************

\begin{flushright}
\itshape
Only entropy comes easy. \\
\medskip
--- Anton Chekhov
\end{flushright}

The discussion about Bayes' theorem so far explains how one can update one's prior knowledge in the light of new data. The question that naturally arises is how does one quantify their prior knowledge in the form of a probability distribution in the first place? In this chapter, we will attempt to provide a series of methods and practices that aim to do exactly that.

Ever since its initial development, many have criticised Bayesian inference for its dependence on prior knowledge~\parencite{efron1986isn, gelman2008objections}. Arguments against it mostly focus on the alleged subjectivity of its derived results. We maintain however that those claims are unfounded as all statistical analyses, Bayesian or not, employ prior information in some form or another. The difference with Bayesian inference is that this is explicitly done and taken into account. Indeed, anytime one has to perform a statistical analysis they have to assume a specific model (or a collection of them), often a specific set of parameters, a procedure of collecting data and a set of assumptions about the process that generated the data. In terms of the objectivity of its results, Bayesian inference is objective in the sense that any researcher possessing the same model assumptions, data, and prior knowledge will naturally reach exactly the same conclusions. Finally, the use of prior information can be understood as a great strength of Bayesian inference as it allows for the numerous scientific analyses which employ posterior distributions from old experiments as the priors for new ones, thus updating our knowledge of the world in a sequential and accumulative manner without discarding previous results. In this chapter, we will present both methods which employ this philosophy and those which attempt to provide a systematic procedure for generating prior distributions.

\section{Conjugate priors}

A prior distribution is said to be conjugate to the likelihood function if it belongs to the same family of distributions as the posterior~\parencite{gelman2013bayesian}. For instance, if the prior is a Gamma distribution and the likelihood is described by a Poisson probability mass function then the posterior is also Gamma.

From a mathematical point of view, conjugate priors are the most convenient choice as they allow us to compute the posterior analytically without the requirement of any computational method. From a scientific point of view however, conjugate priors are not well justified as they exist solely for the merit of algebraic convenience and they are not designed in order to encode the actual prior information. They are however a useful pedagogical and illustrative example of a method for choosing prior distributions.

\subsection{Binomial likelihood function with Beta prior}

Let us now consider the case of a binomial distribution
\begin{equation}
    \label{eq:binomial_distribution}
    p(s,n\vert \theta ) = \binom{n}{s}\theta^{s}(1-\theta)^{n-s}\,,
\end{equation}
which is the sampling distribution for the number of successes $s$ in $n$ \textit{Bernoulli trials} with probability of success equal to $\theta$. Fixing the number of successes $s$ and trials $n$ and letting $\theta$ vary as a free parameter, the above probability mass function will be the likelihood function for this example. It is also more convenient to express it in terms of the number of failures $f=n-s$ instead of the number of trials $n$ as
\begin{equation}
    \label{eq:binomial_distribution_in_terms_of_sf}
    p(s,f\vert \theta ) = \binom{s+f}{s}\theta^{s}(1-\theta)^{f}\,.
\end{equation}
The prior distribution that is conjugate to this likelihood function turns out to be the \textit{Beta} distribution
\begin{equation}
    \label{eq:beta_distribution}
    p(\theta ) = \frac{\theta^{\alpha-1}(1-\theta)^{\beta - 1}}{B(\alpha, \beta)}\,
\end{equation}
where $B(\alpha, \beta)$ is the \textit{Beta} function
\begin{equation}
    \label{eq:beta_function}
    B(\alpha, \beta) = \int_{0}^{1}\theta^{\alpha-1}(1-\theta)^{\beta - 1}d\theta \,,
\end{equation}
that acts as a normalisation factor for the distribution and $\alpha$ and $\beta$ are hyperparameters of the distribution. In the Bayesian context, a hyperparameter is a parameter of a prior distribution; the term is used to distinguish them from parameters of the model. For $\alpha=1$ and $\beta=1$ the \textit{Beta} distribution reduces to the uniform distribution. We can now apply \textit{Bayes' theorem} to produce the posterior distribution
\begin{equation}
    \label{eq:posterior_beta_sf}
    p(\theta\vert s, f) = \frac{p(s,f\vert\theta)p(\theta)}{p(s,f)}\,,
\end{equation}
where
\begin{equation}
    \label{eq:evidence_beta_sf}
    p(s, f) = \int_{0}^{1}p(s,f\vert\theta)p(\theta)d\theta\,,
\end{equation}
is the \textit{evidence}. Substituting equations \ref{eq:binomial_distribution_in_terms_of_sf} and \ref{eq:beta_distribution} into \ref{eq:posterior_beta_sf} and \ref{eq:evidence_beta_sf} we have
\begin{equation}
    \label{eq:posterior_beta_calculation}
    \begin{split}
        p(\theta\vert s, f) &= \frac{\binom{s+f}{s}\theta^{s}(1-\theta)^{f}\theta^{\alpha-1}(1-\theta)^{\beta - 1}/B(\alpha, \beta)}{\int_{0}^{1} \binom{s+f}{s}\theta^{s}(1-\theta)^{f}\theta^{\alpha-1}(1-\theta)^{\beta - 1}/B(\alpha, \beta) d\theta} \\
        &= \frac{\theta^{s+\alpha-1}(1-\theta)^{f+\beta -1}}{B(s+\alpha,f+\beta)}\,,
    \end{split}
\end{equation}
which is another \textit{Beta} distribution with hyperparameters $\alpha' = s+\alpha$ and $\beta ' = f+\beta$.

\section{Jeffreys priors}

There is often the need for priors that are invariant under reparameterisation, meaning that two different parameterisations $\theta$ and $\phi$ of the same model $\mathcal{M}$ yield consistent results. This type of prior was named after Sir Harold Jeffreys and it has the key feature that it is invariant under reparameterisations \parencite{robert2007bayesian}. One natural consequence of this approach is that a Jeffreys prior is fully determined by the choice of parameters, model and likelihood function. In that sense, it is often categorised as an objective prior as the preferences of the researcher affect it only indirectly through the choice of model and likelihood function. Although it is often characterised as an uninformative prior, this is actually far from true as all priors encode prior information. Perhaps a more appropriate name would be the \textit{reparametersation invariant prior}.

\subsection{One--dimensional case}

Let us assume that $\theta$ and $\phi$ are two possible parameterisations of the same model $\mathcal{M}$, and $\theta$ is a continuously differentiable function of $\phi$, then we say that the prior density $p_{\theta}(\theta)$ is invariant under the reparameterisation $\theta = \theta(\phi)$ if it is related to the prior density $p_{\phi}(\phi)$ by the usual change--of--variables theorem
\begin{equation}
    \label{eq:change_of_variables_1d}
    p_{\phi}(\phi) = p_{\theta}(\theta) \left| \frac{d\theta }{d\phi} \right|\,.
\end{equation}
Furthermore, the expected Fisher information is defined as
\begin{equation}
    \label{eq:fisher_information_theta}
    \mathcal{I}_{\theta}(\theta) = - \mathbb{E}_{d} \left[ \frac{d^{2}}{d\theta^{2}}\log p(d|\theta)\right]\,,
\end{equation}
and similarly for the $\phi$ parameterisation, where $\log p(d|\theta)$ is the logarithm of likelihood function, is transformed as
\begin{equation}
    \label{eq:fisher_information_transform}
    \mathcal{I}_{\phi}(\phi) = \mathcal{I}_{\theta}(\theta) \left( \frac{d\theta}{d\phi}\right)^{2}\,,
\end{equation}
under the reparametrisation $\theta = \theta(\phi)$.

Comparing the equations \ref{eq:change_of_variables_1d} and \ref{eq:fisher_information_transform} one can see that defining the priors as
\begin{equation}
    \label{eq:jeffreys_prior_theta_1d}
    p_{\theta}(\theta ) \propto \sqrt{\mathcal{I}_{\theta}(\theta)} \,,
\end{equation}
and
\begin{equation}
    \label{eq:jeffreys_prior_phi_1d}
    p_{\phi}(\phi ) \propto \sqrt{\mathcal{I}_{\phi}(\phi)} \,,
\end{equation}
yields the desired invariance under reparameterisation.

\subsection{Multi--dimensional case}

The generalisation to multiple dimensions is straightforward. The change--of--variables formula has the general form
\begin{equation}
    \label{eq:change_of_variables_nd}
    p_{\phi}(\phi) = p_{\theta}(\theta) \vert \det J\vert \,,
\end{equation}
where $\theta$ and $\phi$ are now sets of parameters (i.e. vectors), and $J$ is the Jacobian matrix of the transformation with components given by
\begin{equation}
    \label{eq:jacobian_matrix_components}
    J_{ij} = \frac{\partial \theta_{i}}{\partial \phi_{i}}\,,
\end{equation}
where the indices $i$ and $j$ point to the $i$--th and $j$--th component of the parameter vectors $\theta$ and $\phi$ respectively.
Similarly, the expected Fisher information matrix, defined as
\begin{equation}
    \label{eq:fisher_information_matrix_theta}
    \mathcal{I}_{\theta}(\theta)_{ij} = -\mathbb{E}_{d}\left[ \frac{\partial^{2}}{\partial \theta_{i} \partial \theta_{j}}\log p(d|\theta )\right]\,,
\end{equation}
is transformed as
\begin{equation}
    \label{eq:fisher_information_matrix_transform}
    \mathcal{I}_{\phi}(\phi) = J^{T} \mathcal{I}_{\theta}(\theta) J\,.
\end{equation}
Computing the determinant of both parts of equation \ref{eq:fisher_information_matrix_transform} leads to
\begin{equation}
    \label{eq:fisher_information_matrix_transform_determinant}
    \det \mathcal{I}_{\phi}(\phi) = \det \mathcal{I}_{\theta}(\theta) (\det J )^{2}\,.
\end{equation}
Comparing equations \ref{eq:change_of_variables_nd} and \ref{eq:fisher_information_matrix_transform_determinant} one can see that defining the priors as
\begin{equation}
    \label{eq:jeffreys_prior_theta_nd}
    p_{\theta}(\theta ) \propto \sqrt{\det \mathcal{I}_{\theta}(\theta)} \,,
\end{equation}
and
\begin{equation}
    \label{eq:jeffreys_prior_phi_nd}
    p_{\phi}(\phi ) \propto \sqrt{\det \mathcal{I}_{\phi}(\phi)} \,,
\end{equation}
once again yields the desired invariance under reparameterisation.

\subsection{Gaussian distribution with mean parameter}

Assuming that the data $d$ are Gaussian--distributed with unknown mean $\mu$ and known standard deviation $\sigma$, the probability density function of $d$ given $\mu$ can be written as
\begin{equation}
    \label{eq:gaussian_distribution_unknown_mean}
    p(d|\mu)=\frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(d-\mu)^2}{2\sigma^{2}}}\,,
\end{equation}
where $\sigma$ is fixed. Applying equation \ref{eq:jeffreys_prior_theta_1d} using equation \ref{eq:fisher_information_theta} in this case, the Jeffreys prior for parameter $\theta \equiv \mu$ is simply
\begin{equation}
    \label{eq:jeffreys_prior_gaussian_mean}
    \begin{split}
        p(\mu) &\propto \sqrt{I(\mu)} = \sqrt{- \mathbb{E}_{d} \left[ \frac{d^{2}}{d\mu^{2}}\log p(d|\mu)\right]} = \sqrt{\mathbb{E}_{d}\left[ \left( \frac{d-\mu}{\sigma} \right)^{2}\right]} \\
        &= \sqrt{\int_{-\infty}^{\infty} p(d|\mu) \left( \frac{d-\mu}{\sigma} \right)^{2} dd} = \frac{1}{\sigma} \propto 1\,.
    \end{split}
\end{equation}
The prior of $\mu$ in this case is independent of $\mu$ which means that is an improper (i.e. unnormalised) uniform prior.

\subsection{Gaussian distribution with scale parameter}

Assuming now that we know the mean parameter $\mu$ (i.e. $\mu$ is fixed) and the standard deviation $\sigma$ is unknown, the Gaussian probability density function of $d$ given $\sigma$ is simply
\begin{equation}
    \label{eq:gaussian_distribution_unknown_scale}
    p(d|\sigma)=\frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(d-\mu)^2}{2\sigma^{2}}}\,.
\end{equation}
Applying equation \ref{eq:jeffreys_prior_theta_1d} using equation \ref{eq:fisher_information_theta} in this case, the Jeffreys prior for parameter $\theta \equiv \sigma$ is
\begin{equation}
    \label{eq:jeffreys_prior_gaussian_scale}
    \begin{split}
        p(\sigma) &\propto \sqrt{I(\sigma)} = \sqrt{- \mathbb{E}_{d} \left[ \frac{d^{2}}{d\sigma^{2}}\log p(d|\sigma )\right]} = \sqrt{\mathbb{E}_{d}\left[ \left( \frac{(d-\mu)^{2}-\sigma^{2}}{\sigma^{3}} \right)^{2}\right]} \\
        &= \sqrt{\int_{-\infty}^{\infty} p(d|\sigma) \left( \frac{(d-\mu)^{2}-\sigma^{2}}{\sigma^{3}} \right)^{2} dd} = \frac{\sqrt{2}}{\sigma} \propto \frac{1}{\sigma}\,.
    \end{split}
\end{equation}

\section{Maximum entropy priors}

% Principle of indifference

In the absence of any information, one should distribute their degree of belief equally between all possible outcomes. This simple rule for assigning probabilities to discrete outcomes was considered so apparent to the fathers of probability theory, \textit{Jacob Bernoulli} and \textit{Pierre Simon Laplace}, that they did not even bother to give it a name. However, its importance in the context of probability theory was clear to both of them. In particular, Laplace wrote:
\begin{quote}
    \textit{The theory of chance consists in reducing all the events of the same kind to a certain number of cases equally possible, that is to say, to such as we may be equally undecided about in regard to their existence, and in determining the number of cases favourable to the event whose probability is sought. The ratio of this number to that of all the cases possible is the measure of this probability, which is thus simply a fraction whose numerator is the number of favourable cases and whose denominator is the number of all the cases possible.}
\end{quote}
This rule was later named the \textit{principle of insufficient reason}, possibly as a play on Leibniz's \textit{principle of sufficient reason} \parencite{brading2003symmetries}. Finally, it was renamed to the \textit{principle of indifference} by economist \textit{John Maynard Keynes} that noted that it can only be applied when one has no additional information~\parencite{keynes1921treatise}.

\looseness=-1 But what if we have some additional information, perhaps in the form of expectation values? Can we somehow incorporate that information and minimally extend the \textit{principle of indifference}? The answer to this question was provided by Jaynes in the form of the \textit{principle of maximum entropy (MaxEnt)}~\parencite{jaynes1982rationale}.

Using the notion of Shannon's ``entropy'' that quantifies the uncertainty of a probability distribution, MaxEnt is a mathematical procedure for the derivation of the maximally agnostic (i.e. least informative) probability distribution subject to a collection of known constraints. The MaxEnt principle can be applied in the assignment of prior probabilities in cases where we know some constraints about the parameters \textit{a priori} in the form of expectation values (e.g. mean, variance, lower or upper bounds, etc.) and we seek to find the least informative distribution that respects those constraints and still complies as much as possible to the \textit{principle of indifference}.

\looseness=-1 The MaxEnt principle turns the problem of defining a prior distribution into a task of optimisation. In particular, one seeks the probability distribution with the maximum entropy, that is, the least informative, subject to a collection of algebraic constraints in the form of expectation values. Before we move on to discuss some explicit examples that demonstrate the application of the MaxEnt principle, let us first present a couple of definitions for the ``entropy''.

\subsection{Shannon's entropy}

In 1948, \textit{Claude Shannon}'s pioneering work on \textit{information theory}~\parencite{shannon1948mathematical} introduced a measure of the uncertainty of a discrete probability distribution which he termed ``entropy'' and defined as
\begin{equation}
    \label{eq:shannon_entropy}
    S(p) = - \sum_{i=1}^{n} p_{i} \log p_{i}\,.
\end{equation}
The entropy of a probability distribution quantifies the amount of missing information, uncertainty or ``surprisal'' inherent in the distribution~\parencite{mackay2003information}. In other words, entropy is the expectation value $\mathbb{E}_{p}[I]$ of the information
\begin{equation}
    \label{eq:self_information}
    I_{i} = -\log p_{i}\,,
\end{equation}
which quantifies the information content of an event $i$ with probability $p_{i}$. For example, a fair coin landing ``heads--tails--heads`` with probability $1/2^{3}$ provides information of $-\log(1/2^{3})=3\log 2$ or $3\,\mathrm{bits}$. Information is measured in ``bits'' if the logarithm has base $2$ or in ``nats'' if it has base $e$. Information has a series of desired properties, namely
\begin{enumerate}
    \item An event $i$ with probability $p_{i}=1$ is certain and offers no information (i.e. $I_{i}=0$),
    \item The lower the probability of an event, the more surprising it is and thus the higher its information contribution,
    \item Information is additive, meaning that the total amount of information is the sum of the information of the individual events.
\end{enumerate}
It turns out the form of equation \ref{eq:self_information} for information is the only option if we want less probable events to have more information, and information to add for independent events. 

Let us consider a simple example in order to make the notions of information and entropy better understood. Suppose that according to the weather forecast there is a $p=1/2$ chance that it rains $1\,cm$, $p=1/4$ chance that it rains $2\,cm$ and $p=1/4$ chance that it does not rain at all. The expected amount of rain is simply $1/2\times 1\,cm+1/4\times2\,cm+1/4\times 0 = 1\,cm$. The expected amount of information that you gain when you find out how much it rains is $-1/2\log(1/2)-1/4\log(1/4)-1/4\log(1/4)=3/2 \log 2$ or $3/2\,\mathrm{bits}$, this is the Shannon entropy of the weather report.

Shannon's entropy naturally assumes that the uniform distribution $p_{i}=1/n$ where $n$ is the number of discrete events holds a very special role. In the absence of any other constraints, assigning equal probability to all outcomes (i.e. $p_{i}$ is constant) corresponds to the state of complete ignorance. In other words, the distribution that maximises Shannon's entropy is the uniform distribution in accordance with the \textit{principle of indifference}.

\subsection{Relative entropy}

Another useful, entropy--like quantity is the following
\begin{equation}
    \label{eq:kl_divergence_discrete}
    D_{KL}(p|q) = \sum_{i=1}^{n} p_{i} \log \left( \frac{p_{i}}{q_{i}}\right)\,,
\end{equation}
that has been given many names, including \textit{relative entropy}, \textit{cross--entropy}, and \textit{Kullback--Leibler (KL) divergence} as \parencite{kullback1951information} were the first ones to demonstrate its potential for statistical applications.

The latter is a measure of information gained when one updates their beliefs, initially quantified by a distribution $q$ to an updated distribution $p$. In that sense, relative entropy is a measure of statistical distance between the two distributions and it is defined as
\begin{equation}
    \label{eq:relative_entropy_discrete}
    D_{KL}(p|q) = \sum_{i=1}^{n} p_{i} \log \left( \frac{p_{i}}{q_{i}}\right)\,,
\end{equation}
for the discrete case, and
\begin{equation}
    \label{eq:relative_entropy_continuous}
    D_{KL}(p|q) = \int p(\theta) \log \left( \frac{p(\theta)}{q(\theta)}\right) d\theta\,,
\end{equation}
for the continuous case.

Relative entropy has a collection of desired properties too, including
\begin{enumerate}
    \item It is always non--negative,
        \begin{equation}
            \label{eq:Gibbs_inequality}
            D_{KL}(p|q) \ge 0\,,
        \end{equation}
        a result commonly known as \textit{Gibbs' inequality}. Relative entropy is zero if and only if $p=q$ almost everywhere.
        
    \item Relative entropy, unlike Shannon's entropy, is well defined for continuous distributions.
    
    \item Given a transformation $\theta = \theta(\phi)$ such that $p(\theta ) d\theta = p(\phi ) d\phi$ and $q(\theta ) d\theta = q(\phi ) d\phi$ the relative entropy is parameterisation invariant, meaning
        \begin{equation}
            \label{eq:relative_entropy_transform}
            \begin{split}
                D_{KL}(p|q) &= \int p(\theta) \log \left( \frac{p(\theta)}{q(\theta)}\right) d\theta \\
                &= \int p(\phi) \log \left( \frac{p(\phi)\frac{d\phi}{d\theta}}{q(\phi)\frac{d\phi}{d\theta}}\right) d\phi \\
                &= \int p(\phi) \log \left( \frac{p(\phi)}{q(\phi)}\right) d\phi\,.
            \end{split}
        \end{equation}
        
    \item Relative entropy reduces to the well known Shannon's entropy, up to a sign, for the case of a uniform distribution $q$ in the discrete case.
    
\end{enumerate}

As we mentioned in the previous sub--section, using \textit{Shannon}'s definition of entropy places the uniform distribution into a very special place, that of the maximum entropy distribution in the absence of any other constraints that provide additional information. Although this is in accordance with the \textit{principle of indifference}, there are cases in practice in which one requires a different prior distribution $q$, before taking into account any constraints. For instance, one may seek to find a distribution that minimally deviates from a Jeffreys prior subject to some constraints. In those cases, instead of maximising \textit{Shannon}'s entropy, one can minimise the relative entropy. For simplicity, we will refer to the principle of minimum relative entropy as MaxEnt too \textcite{shore1980axiomatic} proved that minimising the relative entropy is the uniquely correct way of updating probability distributions in the face of new information in the form of expectation values for both discrete and continuous cases. Furthermore, the \textit{relative entropy}, unlike Shannon's entropy, is easily generalisable to the continuous case too. 

\subsection{Lagrange multipliers}

The method of Lagrange multipliers~\parencite{riley1999mathematical} offers a powerful way of finding the local extrema (i.e. maxima and minima) of a function $f(p )$ subject to a constraint $g(p )=0$. If no constraint is available then the extrema of $f$ can be found by solving
\begin{equation}
    \label{eq:df_equals_zero}
    df = \frac{\partial f}{\partial p_{1}}dp_{1} + \dots + \frac{\partial f}{\partial p_{n}}dp_{n} = 0\,.
\end{equation}
Here the $dp_{i}$ are independent so one concludes that the extrema are simply given by $\partial f/ \partial p_{i} =0$. However, the existence of the constraint means that $dp_{i}$ are not actually independent since
\begin{equation}
    \label{eq:dg_equals_zero}
    dg = \frac{\partial g}{\partial p_{1}}dp_{1} + \dots + \frac{\partial g}{\partial p_{n}}dp_{n} = 0\,,
\end{equation}
because $g(p )$ is constant. We can combine equations \ref{eq:df_equals_zero} and \ref{eq:dg_equals_zero} by first multiplying the second by an unknown factor $\lambda$ called \textit{Lagrange multiplier}, thus yielding
\begin{equation}
    \label{eq:lagrage_multiplier}
    d(f-\lambda g) = \left(\frac{\partial f}{\partial p_{1}}-\lambda \frac{\partial g}{\partial p_{1}}\right)dp_{1} + \dots + \left(\frac{\partial f}{\partial p_{n}}-\lambda \frac{\partial g}{\partial p_{n}}\right)dp_{n} = 0\,.
\end{equation}
We can now choose $\lambda$ such that
\begin{equation}
    \label{eq:lagrange_factors}
    \frac{\partial f}{\partial p_{i}}-\lambda \frac{\partial g}{\partial p_{i}} = 0\,,
\end{equation}
for all $i\in\lbrace1,\dots,n\rbrace$. Equations \ref{eq:lagrange_factors} along with the constraint equation $g(p)$ are sufficient to determine the value of $\lambda$ and coordinates $p_{i}$ of the stationary point.

\subsection{Uniform distribution}

Assuming that the only constraint or form of information is that the sum of all probabilities is equal to one, meaning
\begin{equation}
    \label{eq:probabilities_sum_to_one}
    \sum_{i=1}^{n}p_{i} = 1\,,
\end{equation}
then in order to find the maximum entropy distribution, we have to solve
\begin{equation}
    \label{eq:lagrange_uniform}
    d\left[ -\sum_{i=1}^{n}p_{i}\log\frac{p_{i}}{q_{i}} - \lambda \left( \sum_{i=1}^{n}p_{i} - 1\right) \right] = 0\,,
\end{equation}
where the first term is the relative entropy and the second term is the constraint multiplied with the unknown Lagrange multiplier $\lambda$. Doing some simple calculus on the expression of \ref{eq:lagrange_uniform} we get
\begin{equation}
    \label{eq:lagrange_uniform_2}
    \begin{split}
        d\left[ -\sum_{i=1}^{n}p_{i}\log p_{i}+\sum_{i=1}^{n}p_{i}\log q_{i} - \lambda \left( \sum_{i=1}^{n}p_{i} - 1\right) \right] = 0 \Rightarrow \\
        -\sum_{i=1}^n dp_i\,\log p_i - \sum_{i=1}^n p_i\,d(\log p_i) \\
        + \sum_{i=1}^n dp_i\,\log q_i - \lambda\sum_{i=1}^n dp_i = 0 \Rightarrow \\
        -\sum_{i=1}^n dp_i\,\log p_i - \sum_{i=1}^n p_i\,\left( \sum_{j=1}^n \frac{\partial \log p_i}{\partial p_j}\,dp_j \right) \\
        + \sum_{i=1}^n dp_i\,\log q_i - \lambda\sum_{i=1}^n dp_i = 0 \Rightarrow \\
        -\sum_{i=1}^n dp_i\,\log p_i - \sum_{i=1}^n p_i\,\left( \sum_{j=1}^n \delta_{ij} \frac{1}{p_j}\,dp_j \right) \\
        + \sum_{i=1}^n dp_i\,\log q_i - \lambda\sum_{i=1}^n dp_i = 0 \Rightarrow \\
        -\sum_{i=1}^n dp_i\,\log p_i - \sum_{i=1}^n p_i\, \frac{1}{p_i}\,dp_i + \sum_{i=1}^n dp_i\,\log q_i - \lambda\sum_{i=1}^n dp_i = 0
        \Rightarrow \\
        \sum_{i=1}^{n}\left( -\log \frac{p_{i}}{q_{i}} - 1 - \lambda  \right)d p_{i} = 0
    \end{split}
\end{equation}
According to our previous discussion on Lagrange multipliers, for equation \ref{eq:lagrange_uniform_2} to hold, the terms in the parentheses need to vanish for every $i=1,\dots,n$, therefore we get
\begin{equation}
    \label{eq:lagrange_uniform_with_lambda}
    p_{i} = q_{i} e^{-(1+\lambda )}\,.
\end{equation}
We can determine the value of $\lambda$ using the constraint equation \ref{eq:probabilities_sum_to_one}
\begin{equation}
    \label{eq:lagrange_uniform_get_lambda}
    \sum_{i=1}^{n}q_{i} e^{-(1+\lambda )} = 1\,,
\end{equation}
$\sum_{i=1}^{n}q_{i}=1$ so $\lambda = -1$. Therefore, the distribution in the discrete case is
\begin{equation}
    \label{eq:lagrange_uniform_discrete}
    p_{i} = q_{i}\,,
\end{equation}
and in the continuous case is
\begin{equation}
    \label{eq:lagrange_uniform_continuous}
    p(\theta) = q(\theta)\,,
\end{equation}
Furthermore, assuming that $q$ is a uniform (i.e. $q_{i}=1/n$) in accordance with the \textit{principle of indifference}, then $p$ is also uniform. This means that the distribution of maximum entropy under the minimal constraint that the total probability needs to sum up to one is the uniform distribution. Let us now move on to a few more intriguing examples.

\subsection{Exponential distribution}

Suppose now that we have an additional constraint apart from equation \ref{eq:probabilities_sum_to_one} for the sum of probabilities,
\begin{equation}
    \label{eq:constraint_mean}
    \sum_{i=1}^{n}p_{i}\theta_{i} = \mu\,,
\end{equation}
indicating that the mean value of $\theta$ is known and equal to $\mu$. Having two constraints requires us to introduce two Lagrange multipliers, $\lambda$ and $\Tilde{\lambda}$, and solve
\begin{equation}
    \label{eq:lagrange_exponential}
    d\left[ -\sum_{i=1}^{n}p_{i}\log\frac{p_{i}}{q_{i}} - \lambda \left( \sum_{i=1}^{n}p_{i} - 1\right) - \Tilde{\lambda} \left( \sum_{i=1}^{n}p_{i}\theta_{i} - \mu\right) \right] = 0\,,
\end{equation}
in order to find the appropriate maximum entropy distribution. Similarly to before, after some calculus, we get
\begin{equation}
    \label{eq:lagrange_exponential_2}
    \sum_{i=1}^{n} \left( -\log\frac{p_{i}}{q_{i}} - 1 - \lambda - \theta_{i}\Tilde{\lambda} \right) d p_{i} = 0\,.
\end{equation}
Again, the term in the parentheses needs to vanish for any value of $i$, thus
\begin{equation}
    \label{eq:lagrange_exponential_with_lambda}
    p_{i} = q_{i} e^{-(1+\lambda )}e^{-\Tilde{\lambda}\theta_{i}}\,.
\end{equation}
We can now apply the two constraints to determine the values of the Lagrange multipliers. From equation \ref{eq:probabilities_sum_to_one} for the first constraint, we have
\begin{equation}
    \label{eq:lagrange_exponential_get_lambda}
    e^{-(1+\lambda )} = \frac{1}{\sum_{i=1}^{n} q_{i} e^{-\Tilde{\lambda}\theta_{i}}}\,.
\end{equation}
Similarly, from equation \ref{eq:constraint_mean} for the second constraint, we have
\begin{equation}
    \label{eq:lagrange_exponential_get_lambda_tilde}
    \sum_{i=1}^{n} q_{i} \theta_{i} e^{-\Tilde{\lambda}\theta_{i}} - \mu \sum_{i=1}^{n} q_{i} e^{-\Tilde{\lambda}\theta_{i}} = 0\,,
\end{equation}
which can only be solved numerically. Equation \ref{eq:lagrange_exponential_with_lambda} can also be written for the continuous case as
\begin{equation}
    \label{eq:lagrange_exponential_with_lambda_continuous}
    p(\theta) = q(\theta) e^{-(1+\lambda )}e^{-\Tilde{\lambda}\theta}\,.
\end{equation}

Equation \ref{eq:lagrange_exponential_with_lambda_continuous} is the general MaxEnt prior for an arbitrary pseudo--prior $q(\theta)$. However, the expression can be simplified more if we assume that our state of knowledge about $\theta$ prior to the information provided by the constraints \ref{eq:probabilities_sum_to_one} and \ref{eq:constraint_mean}, is that it is positive (i.e. $\theta > 0$). This means, that $q(\theta)$ is a uniform distribution, in agreement with the principle of indifference. Furthermore, to avoid issues with ``infinities'' and render $q$ a proper pseudo--prior, we can set an upper limit $L$ on the possible values of $\theta$, therefore
\begin{equation}
    \label{eq:bounded_uniform}
    q(\theta) = \mathcal{U}(\theta\vert 0, L) = 
    \begin{cases}
      1/L & \text{if $0<\theta\leq L$}\\
      0 & \text{otherwise}
    \end{cases} \,.
\end{equation}
Once we have derived the form of the MaxEnt prior $p(\theta)$ we can then take the limit of $L\to\+\infty$ to allow $\theta$ to be any positive real number. Including this particular choice of $q(\theta)$, equation \ref{eq:lagrange_exponential_with_lambda_continuous} takes the form
\begin{equation}
    \label{eq:exponential_useless_form}
    p(\theta) = \frac{1}{L} e^{-(1+\lambda )}e^{-\Tilde{\lambda}\theta}\,,\quad \theta > 0\,.
\end{equation}

Using the pseudo-prior of equation \ref{eq:bounded_uniform}, the first constraint, for the total probability, given by equation \ref{eq:probabilities_sum_to_one}, can be expressed as
\begin{equation}
    \label{eq:first_constraint_expression}
    \begin{split}
        \int_{-\infty}^{\infty}p(\theta)d\theta = 1 \Rightarrow \\
        \frac{1}{L}e^{-(1+\lambda)}\int_{0}^{L}e^{-\Tilde{\lambda}\theta}d\theta = 1 \Rightarrow \\
        \frac{1}{L}e^{-(1+\lambda)}\left[ -\frac{e^{-\Tilde{\lambda}\theta}}{\Tilde{\lambda}} \right]_{\theta=0}^{\theta=L} = 1 \Rightarrow \\
        \frac{1}{L}e^{-(1+\lambda)} = \frac{\Tilde{\lambda}}{1 - e^{-\Tilde{\lambda}L} }\,.
    \end{split}
\end{equation}

Similarly, the second constraint, for the expected or mean value of $\theta$, given by equation \ref{eq:constraint_mean}, can be expressed as
\begin{equation}
    \label{eq:second_constraint_expression}
    \begin{split}
        \int_{-\infty}^{\infty}\theta p(\theta)d\theta = \mu \Rightarrow \\
        \frac{1}{L}e^{-(1+\lambda)}\int_{0}^{L}\theta e^{-\Tilde{\lambda}\theta}d\theta = \mu \Rightarrow \\
        \frac{1}{L}e^{-(1+\lambda)} \left[ - \frac{e^{-\Tilde{\lambda}\theta}(\Tilde{\lambda}\theta + 1)}{\Tilde{\lambda}^{2}}\right]_{\theta=0}^{\theta=L} = \mu \Rightarrow \\
        \frac{1}{L}e^{-(1+\lambda)} \frac{1-e^{-\Tilde{\lambda}L}(\Tilde{\lambda}L + 1)}{\Tilde{\lambda}^{2}} = \mu \Rightarrow \\
        \frac{\Tilde{\lambda}}{1 - e^{-\Tilde{\lambda}L} } \times \frac{1-e^{-\Tilde{\lambda}L}(\Tilde{\lambda}L + 1)}{\Tilde{\lambda}^{2}} = \mu \Rightarrow \\
        \mu = \frac{1}{\Tilde{\lambda}} - \frac{L  e^{-\Tilde{\lambda}L}}{1-e^{-\Tilde{\lambda}L}}\,,
    \end{split}
\end{equation}
where we used equation \ref{eq:first_constraint_expression} to simplify the result.

Taking the limit $L\to +\infty$ and using equation \ref{eq:second_constraint_expression}, we find that
\begin{equation}
    \label{eq:second_constraint_limit}
    \mu \to \frac{1}{\Tilde{\lambda}}\,,
\end{equation}
as the second term vanishes. Similarly, using equation \ref{eq:first_constraint_expression}, we find that
\begin{equation}
    \label{eq:second_constraint_limit}
    \frac{1}{L}e^{-(1+\lambda)} \to \Tilde{\lambda}\,.
\end{equation}
Substituting these results into equation \ref{eq:exponential_useless_form}, we are lead to
\begin{equation}
    \label{eq:lagrange_exponential_continuous}
    p(\theta|\mu) = \frac{1}{\mu}e^{-\frac{\theta}{\mu}}\,,
\end{equation}
the well known exponential distribution. What this paragraph taught us is crucial, if we only know the mean of a non--negative parameter and nothing else, then the exponential distribution is the one that best represents the current state of knowledge, by making the fewest assumptions.

\subsection{Normal distribution}

Suppose that we also know the the standard deviation $\sigma$ given by
\begin{equation}
    \label{eq:standard_deviation_discrete}
    \sum_{i=0}^{n} p_{i} (\theta_{i} - \mu )^{2} = \sigma^{2}\,,
\end{equation}
as an additional constraint. We now have to solve
\begin{equation}
    \label{eq:lagrange_gaussian}
    d\left[ -\sum_{i=1}^{n}p_{i}\log\frac{p_{i}}{q_{i}} - \lambda \left( \sum_{i=1}^{n}p_{i} - 1\right) - \Tilde{\lambda} \left( \sum_{i=1}^{n}p_{i}(\theta_{i} - \mu)^{2}-\sigma^{2}\right) \right] = 0\,,
\end{equation}
where the first term corresponds to the entropy, the second to the constraint that the sum of all probabilities needs to add up to one, and the last term to the standard deviation constraint that also includes that about the mean $\mu$. Thus we have two \textit{Lagrange} multipliers and we follow the same procedure as before, solving equation \ref{eq:lagrange_gaussian} we have
\begin{equation}
    \label{eq:lagrange_gaussian_2}
    \sum_{i=1}^{n} \left( -\log\frac{p_{i}}{q_{i}} - 1 - \lambda - \Tilde{\lambda} (\theta_{i}-\mu)^{2} \right) d p_{i} = 0\,.
\end{equation}
The terms in the parentheses need to vanish for all values of $i$, thus
\begin{equation}
    \label{eq:lagrange_gaussian_with_lambda}
    p_{i} = q_{i} e^{-(1+\lambda )}e^{-\Tilde{\lambda}(\theta_{i}-\mu)^{2}}\,.
\end{equation}
The corresponding continuous probability density function is simply
\begin{equation}
    \label{eq:lagrange_gaussian_with_lambda_continuous}
    p(\theta) = q(\theta) e^{-(1+\lambda )}e^{-\Tilde{\lambda}(\theta-\mu)^{2}}\,.
\end{equation}
Furthermore, assuming a uniform prior $q(\theta)\propto 1$ in accordance with the \textit{principle of indifference}, equation \ref{eq:lagrange_exponential_with_lambda_continuous} reduces to
\begin{equation}
    \label{eq:lagrange_gaussian_with_lambda_continuous_2}
    p(\theta) = e^{-(1+\lambda )}e^{-\Tilde{\lambda}(\theta-\mu)^{2}}\,.
\end{equation}
We can now apply the constraint equations \ref{eq:probabilities_sum_to_one} and \ref{eq:standard_deviation_discrete} in order to uniquely determine the values of the two \textit{Lagrange} multipliers. In the continuous limit, the first constraint given by equation \ref{eq:probabilities_sum_to_one}, is written as
\begin{equation}
    \label{eq:probabilities_integrate_to_one}
    \int_{-\infty}^{\infty}p(\theta)d\theta = 1\,.
\end{equation}
Substituting equation \ref{eq:lagrange_gaussian_with_lambda_continuous_2} into equation \ref{eq:probabilities_integrate_to_one} yields
\begin{equation}
    \label{eq:lagrange_gaussian_get_lambda}
    e^{-(1+\lambda)}\int_{-\infty}^{\infty}e^{-\Tilde{\lambda}(\theta-\mu)^{2}}d\theta = 1\,.
\end{equation}
Doing the change of variables $z=\sqrt{\Tilde{\lambda}}(\theta - \mu)$ brings equation \ref{eq:lagrange_gaussian_get_lambda} into the simpler form
\begin{equation}
    \label{eq:lagrange_gaussian_get_lambda_2}
    \frac{e^{-(1+\lambda)}}{\sqrt{\Tilde{\lambda}}}\int_{-\infty}^{\infty}e^{-z^{2}}dz = 1\,,
\end{equation}
where the integral is the so called \textit{Gaussian integral} with value equal to $\sqrt{\pi}$. Therefore, 
\begin{equation}
    \label{eq:lagrange_gaussian_lambda}
    e^{-(1+\lambda)} = \sqrt{\frac{\Tilde{\lambda}}{\pi}}\,,
\end{equation}
and equation \ref{eq:lagrange_gaussian_with_lambda_continuous_2} reduces to
\begin{equation}
    \label{eq:lagrange_gaussian_with_lambda_continuous_3}
    p(\theta ) = \sqrt{\frac{\Tilde{\lambda}}{\pi}} e^{-\Tilde{\lambda}(\theta-\mu)^{2}} \,.
\end{equation}
We can now move on to determine the second \textit{Lagrange} multiplier $\Tilde{\lambda}$ by substituting equation \ref{eq:lagrange_gaussian_with_lambda_continuous_3} into \ref{eq:standard_deviation_discrete}, thus
\begin{equation}
    \label{eq:lagrange_gaussian_get_lambda_3}
    \sqrt{\frac{\Tilde{\lambda}}{\pi}} \int_{-\infty}^{\infty}e^{-\Tilde{\lambda}(\theta-\mu)^{2}}(\theta-\mu)^{2}d\theta = \sigma^{2}\,.
\end{equation}
Applying the same change of variables as before, $z=\sqrt{\Tilde{\lambda}}(\theta - \mu)$, we have
\begin{equation}
    \label{eq:lagrange_gaussian_get_lambda_4}
    \frac{1}{\Tilde{\lambda}\sqrt{\pi}} \int_{-\infty}^{\infty}e^{-z^{2}}z^{2}dz = \sigma^{2}\,.
\end{equation}
The integral can be computed using integration by parts
\begin{equation}
    \label{eq:lagrange_gaussian_get_lambda_5}
    \frac{1}{\Tilde{\lambda}\sqrt{\pi}} \left\lbrace\left[z\left(-\frac{1}{2}e^{-z^{2}}\right) \right]_{-\infty}^{\infty}-\int_{-\infty}^{\infty}-\frac{1}{2}e^{-z^{2}}dz\right\rbrace = \sigma^{2}\,,
\end{equation}
in which the first term in the braces vanishes and the second is equal to $\sqrt{\pi}/2$, thus
\begin{equation}
    \label{eq:lagrange_gaussian_lambda_2}
    \Tilde{\lambda} = \frac{1}{2\sigma^{2}}\,.
\end{equation}
Finally, substituting equation \ref{eq:lagrange_gaussian_lambda_2} into \ref{eq:lagrange_gaussian_with_lambda_continuous_3} leads to the usual Gaussian function
\begin{equation}
    \label{eq:gaussian_pdf_1d}
    p(\theta) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(\theta-\mu)^{2}}{2\sigma^{2}}}\,,
\end{equation}
as the maximum entropy probability density function. In other words, the maximum entropy probability distribution subject to the constraints of known mean and standard deviation is the normal distribution.

\section{Reference priors}

The method of \textit{reference priors}, originally proposed by~\textcite{bernardo1979reference} and later expanded by others~\parencite{berger2009formal, bernardo2005reference, kass1996selection, bernardo2009bayesian}, is another approach that utilises information--theoretic ideas. The main idea behind reference priors is to choose the  prior $p(\theta)$ to maximise some notion of discrepancy between the prior $p(\theta)$ and the posterior $p(\theta\vert d)$. One reason to do this is that such a prior would allow the data $d$ to be maximally informative and have the greatest effect on the posterior distribution. In one--dimensional cases it turns out that reference priors and Jeffreys priors are equivalent. In higher dimensional cases, however, they are generally different. The research field of reference priors has expanded substantially during the past decades. For this reason, we will cover the fundamentals in this section and direct the reader to the aforementioned references for more information.

As we discussed in the previous section regarding maximum entropy priors, a common measure of the discrepancy between two distributions is the relative entropy or KL divergence, given by equation \ref{eq:relative_entropy_continuous}. In the case of the prior and posterior distribution, this can be written as
\begin{equation}
    \label{eq:relative_entropy_prior_posterior}
    D_{KL}\left[p(\theta\vert d)\vert p(\theta)\right] = \int p(\theta\vert d)\log\frac{p(\theta\vert d)}{p(\theta)}d\theta\,.
\end{equation}
One might then wonder how can we maximise the above discrepancy measure, in order to find the prior $p(\theta)$, without knowing the posterior distribution $p(\theta\vert d)$. Reference priors address this point by maximising the \textit{expectation value} of the relative entropy of equation \ref{eq:relative_entropy_prior_posterior} over the distribution of the data $p(d^{(n)})$, where $d^{(n)}=\lbrace d_{1},\dots, d_{n}\rbrace$ are $n$ conditionally independent instances of the data. At first, this appears to be a frequentist procedure as one will base the choice of the prior on unseen fictional data, such as infinite repetitions of the same experiment (e.g. in the limit that $n\rightarrow \infty$). However, unlike frequentist approaches, once the prior is determined, the analysis proceeds in the usual Bayesian manner. Furthermore, \textit{Bernardo} argued that taking the limit of $n$ to infinity does not just lead to a convenient mathematical procedure but it is also, philosophically, the right thing to do. His argument is that when choosing a prior we should consider many future experiments than just a single one. In this sense, the reference prior procedure aims to maximise the \textit{missing information} about the parameters $\theta$ which can be obtained by repeated experiments.

\subsection{Mutual information}

The expected relative entropy between the prior and posterior, also known as the \textit{mutual information}, quantifies the missing information and can be derived as follows:
\begin{equation}
    \label{eq:mutual_information}
    \begin{split}
        I(\theta,d^{(n)})&=\mathbb{E}_{d^{(n)}}\left[ D_{KL}\left[p(\theta\vert d^{(n)})\vert p(\theta)\right]\right] = \int p(d^{(n)}) D_{KL}\left[p(\theta\vert d^{(n)})\vert p(\theta)\right] d d^{(n)} \\
        &= \int p(d^{(n)}) \int p(\theta\vert d^{(n)})\log\frac{p(\theta\vert d^{(n)})}{p(\theta)}d\theta dd^{(n)} \\
        &= \int \int p(\theta\vert d^{(n)}) p(d^{(n)})\log\frac{p(\theta\vert d^{(n)})}{p(\theta)}d\theta d  d^{(n)} \\
        &= \int \int p(\theta, d^{(n)}) \log\frac{p(\theta, d^{(n)})}{p(\theta) p(d^{(n)})}d\theta dd^{(n)} \,,
    \end{split}
\end{equation}
where we used Bayes' theorem to introduce the joint probability $p(\theta, d^{(n)})$. In order to understand the meaning and significance of the above expression, let us consider the simple case in which the parameters $\theta$ and the data $d^{(n)}$ are independent. In this case the joint probability of the two is separable, or $p(\theta, d^{(n)})=p(\theta) p(d^{(n)})$, and the mutual information $I(\theta,d^{(n)})$ is zero. In other words, the data $d^{(n)}$ have no effect on the parameters $\theta$. However, those two quantities are not generally independent, and the mutual information quantifies the influence or effect of the data $d^{(n)}$ on the parameters $\theta$. Finally, defining the reference priors in terms of the mutual information, has the advantage of sharing its reparameterisation invariance.

\subsection{Maximising the mutual information}

The reference prior $p^{*}(\theta)$ is simply the prior which maximises the mutual information in the limit that $n\rightarrow \infty$, or:
\begin{equation}
    \label{eq:maxmise_mutual_information}
    p^{*}(\theta) = \lim_{n\rightarrow \infty}p_{n}^{*}(\theta)\,, \text{ where }\,\,p_{n}^{*}(\theta)=\underset{p(\theta)}{\mathrm{arg\,max}}~I(\theta, d^{(n)})\,.
\end{equation}
The mutual information of equation \ref{eq:mutual_information} can be written as:
\begin{equation}
    \label{eq:mutual_information_short}
    I(\theta,d^{(n)})=\int p(\theta) \log \frac{f(\theta)}{p(\theta)}d\theta\,,
\end{equation}
where we have introduced the function
\begin{equation}
    \label{eq:reference_f}
    f_{n}(\theta)=\exp\left\lbrace \int p(d^{(n)}\vert \theta)\log p(d^{(n)}\vert \theta) dd^{(n)}\right\rbrace\,,
\end{equation}
where the product sampling distribution is defined as
\begin{equation}
    \label{eq:product_sampling_distribution}
    p(d^{(n)}\vert \theta) = \prod_{i=1}^{n}p(d_{i}\vert \theta)\,.
\end{equation} 
Finding the prior distribution $p_{n}(\theta)$, which maximises the mutual information $I(\theta,d^{(n)})$ subject to the constraint $\int p(\theta)d\theta = 1$, is essentially a problem that can be solved via the methods of calculus of variations. It may be simpler to derive the result by working in the discrete case. This means that we have to solve
\begin{equation}
    \label{eq:lagrange_mutual_info}
    d\left[ \sum_{i}p_{i}\log \left( \frac{f_{i}}{p_{i}}\right)+\lambda \left( \sum_{i}p_{i}-1\right)\right] = 0
\end{equation}
where $\lambda$ is a Lagrange multiplier, in order to find the prior $p_{i}$. The derivation is as follows:
\begin{equation}
    \label{eq:lagrange_mutual_info2}
    \begin{split}
       \sum_{i}dp_{i}\log \left( \frac{f_{i}}{p_{i}}\right)+\sum_{i}p_{i}\left[ \sum_{j}\frac{\partial \log (f_{i}/p_{i})}{\partial p_{j}}dp_{j}\right] +\lambda \sum_{i}dp_{i} &= 0 \Rightarrow \\
       \sum_{i}dp_{i}\log \left( \frac{f_{i}}{p_{i}}\right)+\sum_{i}p_{i}\left[ \sum_{j}\delta_{ij} \left( -\frac{1}{p_{j}}\right)dp_{j}\right] +\lambda \sum_{i}dp_{i} &= 0 \Rightarrow \\
       \sum_{i}\left[ \log \left( \frac{f_{i}}{p_{i}}\right) - 1 +\lambda\right] dp_{i} &= 0
    \end{split}
\end{equation}
For the above equation to be true, all terms in the sum must be zero, in other words we get that $p_{i}=f_{i}\exp(\lambda - 1)$ or simply $p_{i}\propto f_{i}$. Rewriting this in the continuous case, in the limit that $n\rightarrow \infty$ we have:
\begin{equation}
    \label{eq:reference_prior_expression}
    p(\theta) = \lim_{n\rightarrow \infty} \frac{f_{n}(\theta)}{f_{n}(\theta_{0})}\,,
\end{equation}
where $\theta_{0}$ is an internal point in parameter space and $f_{n}(\theta)$ is given by equation \ref{eq:reference_f}. Alternatively, $f_{n}(\theta)$ can be defined as
\begin{equation}
    \label{eq:reference_fh}
    f_{n}(\theta)=\exp\left\lbrace \int p(d^{(n)}\vert \theta)\log\left[ \frac{p(d^{(n)}\vert \theta) h(\theta)}{\int p(d^{(n)}\vert \theta) h(\theta) d\theta}  \right] dd^{(n)}\right\rbrace\,,
\end{equation}
where we have included an arbitrary pseudo--prior $h(\theta)$. Carefully selecting the functional form of $h(\theta)$ (e.g. conjugate prior) can significantly simplify the calculations.

Intuitively, equations \ref{eq:reference_prior_expression} and \ref{eq:reference_fh} state that the reference prior $p(\theta)$ depends only on the asymptotic behaviour of the posterior, and schematically can be written in the form
\begin{equation}
    \label{eq:reference_prior_schematic}
    \begin{split}
        p(\theta) &\propto \exp\left\lbrace \int p(d^{(n)}\vert \theta)\log p^{*}(\theta\vert d^{(n)}) dd^{(n)}\right\rbrace \\
        &\propto \exp\left\lbrace\mathbb{E}_{p(d^{(n)}\vert \theta)}\left[ \log p^{*}(\theta\vert d^{(n)})\right]\right\rbrace \,,
    \end{split}
\end{equation}
where $p^{*}(\theta\vert d^{(n)})$ is the asymptotic form of the posterior.

\subsection{Asymptotic solution}

Finding the reference prior is now reduced to computing $f_{n}(\theta)$ using equation \ref{eq:reference_f} or \ref{eq:reference_fh}. However, this can be quite challenging in practice. The problem can be simplified by using the \textit{Bernstein--von Mises} theorem, which, as we discussed in Chapter \ref{chp:probability}, states that under certain conditions, as the sample size approaches infinity (i.e. $n\rightarrow \infty$), the posterior distribution converges to a normal distribution centred on the \textit{maximum likelihood estimate (MLE)} $\theta_{n}$ with variance equal to $n^{-1}I^{-1}(\theta_{n})$, where $I(\theta)$ is the \textit{Fisher information} given by
\begin{equation}
    \label{eq:fisher_information}
    \mathcal{I}(\theta) = - \mathbb{E}_{p(d\vert \theta)}\left[ \frac{\partial^{2}\log p(d\vert \theta)}{\partial \theta^{2}} \right]\,.
\end{equation}

We can use the fact that MLE is a \textit{consistent} and \textit{asymptotically} sufficient estimator, meaning that
\begin{equation}
    \label{eq:mle_consistent}
    \lim_{n\to\infty}\Hat{\theta}_{n} = \theta \,,
\end{equation}
and
\begin{equation}
    \label{eq:mle_sufficient}
    \lim_{n\to\infty} \int p(d^{(n)}\vert\theta) \log \frac{p^{*}(\theta\vert d^{(n)})}{p^{*}(\theta\vert \Hat{\theta}_{n})} d d^{(n)} = 0 \,,
\end{equation}
respectively, in order to simplify the form of the reference prior. Starting with equation \ref{eq:reference_fh}, we can write
\begin{equation}
    \label{eq:reference_mle}
    \begin{split}
        f_{n}^{*}(\theta) &= \exp \left\lbrace \int p(d^{(n)}\vert\theta)\log p^{*}(\theta\vert d^{(n)})d d^{(n)}\right\rbrace \\
        &= \exp \left\lbrace \int p(d^{(n)}\vert\theta)\log p^{*}(\theta\vert \Hat{\theta}_{n})d d^{(n)}\right\rbrace \\
        &= \exp \left\lbrace \int p(\Hat{\theta}_{n}\vert\theta)\log p^{*}(\theta\vert \Hat{\theta}_{n}) d \Hat{\theta}_{n}\right\rbrace \\
        &= \exp \left\lbrace \log p^{*}(\theta\vert \Hat{\theta}_{n})\Big\vert_{\Hat{\theta}_{n}=\theta} \right\rbrace \\
        &= p^{*}(\theta\vert \Hat{\theta}_{n})\Big\vert_{\Hat{\theta}_{n}=\theta}\,.
    \end{split}
\end{equation}
Therefore, the asymptotically normal form of the posterior with mean $\Hat{\theta}_{n}$ and variance $n^{-1}\mathcal{I}^{-1}(\theta_{n})$ can be written as
\begin{equation}
    \label{eq:asymptotic_posterior}
    p_{n}^{*}(\theta\vert \Hat{\theta}_{n}) = (2\pi)^{-1/2}n^{1/2}I^{1/2}(\theta_{n}) \exp \left[ -\frac{1}{2}n \mathcal{I}(\theta_{n})(\theta - \theta_{n})^{2}\right]\bigg\vert_{\Hat{\theta}_{n}=\theta}
\end{equation}
Substituting this into equation \ref{eq:reference_mle} we find that
\begin{equation}
    \label{eq:fn_normal}
    f_{n}^{*}(\theta) = (2\pi)^{-1/2}n^{1/2}\mathcal{I}^{1/2}(\theta)\,,
\end{equation}
and using equation \ref{eq:reference_prior_expression} we get
\begin{equation}
    \label{eq:reference_is_jeffreys}
    p(\theta) \propto \mathcal{I}^{1/2}(\theta)\,.
\end{equation}
This means that the reference prior, in asymptotically normal models described by one parameter, is equivalent to the Jeffreys prior. As we will discuss shortly, this is not the case for models with many parameters where the two approaches generally produce different results.

\subsection{Numerical solution}

In many cases, equation \ref{eq:reference_fh} cannot be computed analytically and a numerical solution is required to derive the reference prior. This approach can be applied to one--parameter models and results in a numerical representation of the reference prior in the form of pairs $\lbrace \theta , p(\theta) \rbrace$ of values which can be interpolated and used to define the prior's pdf. The numerical procedure, described below, generally requires that it is computationally possible to simulate data from the sampling distribution (i.e. $d\sim p(d\vert \theta)$) in order to approximate the outer integral of equation \ref{eq:reference_fh}, and use numerical integration (e.g. quadrature) in order to compute the inner integral in the normalisation of the asymptotic posterior.

\begin{algorithm}[ht!]
\caption{Numerical reference prior} \algolabel{reference}
\begin{algorithmic}[1]
\REQUIRE{Values of $\theta_{t}\in\lbrace \theta_{1}, \dots, \theta_{T}\rbrace$ for which to compute the reference prior, a moderate value of $n$ to simulate the asymptotic posterior, number of samples $m$, an arbitrary pseudo--prior (e.g. $h(\theta)=1$)}
\ENSURE{Pairs $\lbrace \theta_{t}, p(\theta_{t})\rbrace $}
\FOR{$t=1$ \TO $T$}
    \FOR{$j=1$ \TO $m$}
        \STATE{Simulate a data set $\lbrace d_{1j}\,\dots, d_{nj}\rbrace \sim p(d\vert \theta_{t})$,}
        \STATE{Compute the integral $c_{j}=\int \prod_{i=1}^{n}p(d_{ij}\vert \theta )h(\theta)d\theta$ numerically, where the integration takes place in the prior domain of $\theta$,}
        \STATE{Evaluate $r_{j}=\log\left[c_{j}^{-1}\prod_{i=1}^{n}p(d_{ij}\vert \theta_{t})h(\theta_{t}) \right]$,}
    \ENDFOR
    \STATE{Compute and store $p(\theta_{t}) = m^{-1}\sum_{j=1}^{m}r_{j}(\theta_{t})$.}
\ENDFOR

\end{algorithmic}
\end{algorithm}


\subsection{Many parameters}

So far, we have only discussed cases where the model has a single parameter $\theta$, in which case the reference prior is identical to the Jeffreys prior under the assumption of asymptotic normality. However, the reference prior procedure can be extended to models with more than one parameter where it generally differs from the Jeffreys prior. 

In the multivariate case, the reference prior can be decomposed as
\begin{equation}
    \label{eq:decomposed_reference_prior}
    p(\theta_{1}, \dots, \theta_{D}) = p(\theta_{D}\vert \theta_{1}, \dots , \theta_{D-1})p(\theta_{D-1}\vert \theta_{1}, \dots , \theta_{D-2})\dots p(\theta_{2}\vert \theta_{1}) p(\theta_{1})\,,
\end{equation}
where $D$ is the number of dimensions and we assumed that the parameters are $\lbrace \theta_{1}, \dots, \theta_{D}\rbrace$, in decreasing degree of ``importance'' or ``relevance''. The specific ordering of the parameters in terms of their importance matters as different arrangements can result in different reference priors. Given the aforementioned parameter arrangement, the reference prior procedure works by sequentially deriving the aforementioned conditional priors in reverse order, starting with $p(\theta_{D}\vert \theta_{1}, \dots , \theta_{D-1})$ and ending with $p(\theta_{1})$. Intuitively, this means that we are seeking the reference prior that maximises the missing information about $\theta_{1}$, then $\theta_{2}\vert\theta_{1}$, then $\theta_{3}\vert \theta_{1},\theta_{2}$, and so on.

In practice, we first fix all parameters but $\theta_{D}$ and we estimate $p(\theta_{D}\vert \theta_{1}, \dots , \theta_{D-1})$ by treating the problem as one--dimensional. Assuming that the prior is proper, then $\theta_{D}$ can be marginalised, and the sampling distribution becomes
\begin{equation}
    \label{eq:marginalised_sampling_distribution}
    p(d^{(n)}\vert \theta_{1}, \dots, \theta_{D-1}) = \int p(d^{(n)}\vert \theta_{1}, \dots, \theta_{D}) p(\theta_{D}\vert \theta_{1}, \dots , \theta_{D-1}) d\theta \,,
\end{equation}
The process is then repeated for the next conditional prior $p(\theta_{D-1}\vert \theta_{1}, \dots , \theta_{D-2})$ using $p(d^{(n)}\vert \theta_{1}, \dots, \theta_{D-1})$ as the sampling distribution. After $D$ iterations of the above procedure, all conditional priors are known and the reference prior can be computed as their product according to equation \ref{eq:decomposed_reference_prior}. Although it is possible to use numerical methods in more than one dimension, it often simpler to derive results by employing the asymptotic normality of the posterior distribution when this assumption holds.

\subsubsection{Multivariate reference prior under asymptotic normality}

To derive reference priors, using the asymptotic normality of the posterior distribution, we first need to understand its conditional structure. In particular, we want to know how we can express the variance and precision of each conditional posterior distribution in terms of the components of the covariance and precision matrices of the unconditional posterior distribution. 

Let us assume that the asymptotic posterior distribution can be described as a normal distribution with covariance matrix $\Sigma$ or precision matrix $P=\Sigma^{-1}$. When the conditions of the Bernstein--von Mises theorem are met, the precision matrix can be written as $P=n \mathcal{I}(\Hat{\theta}_{n})$, where $\mathcal{I}$ is the Fisher information matrix, $n$ is the sample size, and $\Hat{\theta}_{n}$ is the MLE. Following the usual conventions, we can identify the elements of those matrices using two indices, that is, $\Sigma_{ij}$ is the element in the intersection of the $i$--th row and $j$--th column.

One way to decompose the asymptotic posterior into its conditionals is
\begin{equation}
    \label{eq:asymptotic_posterior_conditionals}
    \begin{split}
        p^{*}(\theta_{1},\dots,\theta_{D}\vert d^{(n)}) = &p^{*}(\theta_{D}\vert \theta_{1},\dots,\theta_{D-1}, d^{(n)})\\ 
        &\times p^{*}(\theta_{D-1}\vert \theta_{1},\dots,\theta_{D-2}, d^{(n)}) \\
        &\dots p^{*}(\theta_{2}\vert \theta_{1}, d^{(n)}) p^{*}(\theta_{1}\vert d^{(n)})
    \end{split}
\end{equation}

The steps that we need to follow to compute the precision of a conditional $p^{*}(\theta_{j}\vert \theta_{1}, \dots,\theta_{j-1}, d^{(n)})$ are the following:
\begin{enumerate}
    \item Construct the matrix $\Sigma_{j}$ from the upper $j\times j$ sub--matrix of $\Sigma$,
    \item Compute the inverse matrix $P_{j}=\Sigma_{j}^{-1}$,
    \item Drop the rows and columns that correspond to the conditional parameters $\theta_{1},\dots, \theta_{j-1}$. For 1--D conditionals of the form $p^{*}(\theta_{j}\vert \theta_{1}, \dots,\theta_{j-1}, d^{(n)})$, this leaves only the lower right element of $P_{j}$ that we denote as $P_{j*}$ and is equal to the precision of the conditional posterior.
\end{enumerate}

Using the above formula, the reference prior which corresponds to the ordered parameterisation $\lbrace \theta_{1}\,, \dots\,, \theta_{D}$, in terms of importance or relevance, is
\begin{equation}
    \label{eq:ordered_reference_prior}
    p(\theta_{1},\dots,\theta_{D})=p(\theta_{D}\vert \theta_{1},\dots,\theta_{D-1})\dots p(\theta_{2}\vert\theta_{1})p(\theta_{1})\,,
\end{equation}
where
\begin{equation}
    \label{eq:last_parameter_prior}
    p(\theta_{D}\vert \theta_{1},\dots,\theta_{D-1}) \propto P_{D*}^{1/2}(\theta)\,,
\end{equation}
following equation \ref{eq:reference_is_jeffreys}, and for $j=1,\dots,D-1$
\begin{equation}
    \label{eq:all_other_parameters_prior}
    \begin{split}
        p(\theta_{j}\vert \theta_{1},\dots,\theta_{j-1}) \propto \exp\bigg\lbrace &\int \prod_{\ell=j+1}^{D}p(\theta_{\ell}\vert \theta_{1},\dots,\theta_{\ell -1}) \\
        &\times\log P_{j*}^{1/2}(\theta) d\theta_{j+1}\dots d\theta_{D}\bigg\rbrace\,,
    \end{split}
\end{equation}
where we used equations \ref{eq:reference_prior_schematic} and \ref{eq:marginalised_sampling_distribution} to derive the above expression. 

In the special case that the functions $P_{j*}^{1/2}(\theta)$ factorise in the form
\begin{equation}
    \label{eq:p_factorise}
    P_{j*}^{1/2}(\theta) \propto f_{j}(\theta_{j})g_{j}(\theta_{1},\dots,\theta_{j-1},\theta_{j+1},\dots\theta_{D})\,,
\end{equation}
the reference prior is simply
\begin{equation}
    \label{eq:reference_prior_factorise}
    p(\theta_{1},\dots,\theta_{D}) = \prod_{j=1}^{D}f_{j}(\theta_{j})\,.
\end{equation}

\subsubsection{2--D example}

In this example, the joint posterior distribution $p(\theta_{1}, \theta_{2}\vert d^{(n)})$ is asymptotically normal with precision matrix $P=n\mathcal{I}(\theta_{n})$ and covariance matrix $\Sigma = P^{-1}$. Without loss of generality, we can order the parameters in increasing importance or relevance as $\lbrace \theta_{1}, \theta_{2} \rbrace$ and seek to find the reference prior $p(\theta_{1}, \theta_{2})=p(\theta_{2}\vert\theta_{1})p(\theta_{1})$. According to equation \ref{eq:last_parameter_prior}, the conditional prior $p(\theta_{2}\vert\theta_{1})$ is given by
\begin{equation}
    \label{eq:conditional_prior_2d}
    p(\theta_{2}\vert\theta_{1}) \propto P_{2*}^{1/2}(\theta_{1},\theta_{2}) \propto \mathcal{I}_{22}^{1/2}(\theta_{1}, \theta_{2})\,.
\end{equation}
The marginal prior $p(\theta_{1})$ can be derived using equation \ref{eq:all_other_parameters_prior}, and it is given by
\begin{equation}
    \label{eq:marginal_prior_2d}
    p(\theta_{1}) \propto \exp \left\lbrace \int p(\theta_{2}\vert\theta_{1})\log P_{1*}^{1/2}(\theta_{1},\theta_{2})d\theta_{2}\right\rbrace\,,
\end{equation}
where $P_{1*}^{1/2}(\theta_{1},\theta_{2}) = P_{11}-P_{12}P_{22}^{-1}P_{21} \propto \mathcal{I}_{11}-\mathcal{I}_{12}\mathcal{I}_{22}^{-1}\mathcal{I}_{21}$.

So far we have not specified any particular model for this example. In other words, the aforementioned equations hold for any 2--D likelihood function $p(d^{(n)}\vert \theta_{1}, \theta_{2})$. To make the example more specific, we choose the sampling distribution to be normal with the likelihood function parameterised by the mean $\theta_{1}=\mu$ and standard deviation $\theta_{2}=\sigma$, that is,
\begin{equation}
    \label{eq:likelihood_function_gaussian_2d}
    p(d\vert\mu,\sigma) = \mathcal{N}(d\vert\mu,\sigma)\,.
\end{equation}
Substituting the above equation into the definition of the Fisher information matrix given by
\begin{equation}
    \label{eq:fisher_matrix_once_more}
    \mathcal{I}_{ij}(\mu, \theta) = -\int p(d\vert\mu,\sigma) \frac{\partial^{2}\log p(d\vert\mu,\sigma)}{\partial \theta_{i} \partial \theta_{j}} dd\,,
\end{equation}
leads to
\begin{equation}
    \label{eq:fisher_matrix_mu_sigma}
    \mathcal{I}_{ij}(\mu, \theta) = 
    \begin{pmatrix}
        \sigma^{-2} &  0 \\
        0 & 2\sigma^{-2}
    \end{pmatrix}\,.
\end{equation}
It follows directly that the terms $P_{j*}^{1/2}$ are given by
\begin{equation}
    \label{eq:p_j_terms}
    P_{1*}^{1/2}(\mu,\theta) = \sigma^{-1}\,,\quad P_{2*}^{1/2}(\mu,\theta) = \sqrt{2}\sigma^{-1}\,.
\end{equation}
We notice that the above terms factorise into the form of equation \ref{eq:p_factorise}, thus the reference prior is simply
\begin{equation}
    \label{eq:rererence_prior_2d}
    p(\mu,\sigma) = p(\sigma\vert \mu) p(\mu) \propto \sigma^{-1} \times 1 \propto \sigma^{-1}\,.
\end{equation}

It is worth noting that, the alternative ordering of the parameters (i.e. $\theta_{1}=\sigma$ and $\theta_{2}=\mu$), which prioritises $\sigma$ over $\mu$, results in the same reference prior in this example. Furthermore, in this case, the bivariate reference prior $p_{R}(\mu,\sigma)=\sigma^{-1}$ differs markedly from the corresponding Jeffreys prior $p_{J}(\mu,\sigma)=\sigma^{-2}$. Indeed, even Jeffreys himself criticised his multivariate method, which is known to lead to marginalisation paradoxes~\parencite{dawid1973marginalization}.

\section{Weakly informative and regularisation priors}

All options that were discussed so far consist of automated methods of generating prior distributions. There is however another class of priors that is distinctly different in purpose than the ones presented above. Those are the weakly informative priors. 

In most analyses, we have some limited prior information about the range and possible values that a parameter can take based on domain expertise and the model assumptions. For instance, when constraining the mass of an elementary particle we know that it must be smaller than the mass of macroscopic objects and at the same time it has to be greater than or equal to zero. This sort of weakly informative knowledge, although not as well quantified as that in the case of \textit{Jeffreys} and \textit{maximum entropy} priors, can still be included in a Bayesian analysis with the hope of guiding the computation by providing regularisation without significantly affecting the outcome. Weakly informative and regularisation priors are used very often in practice, mostly in cases where the data are very informative and the posterior concentrates to a distribution approaching a multivariate normal in accordance with the \textit{Bernstein--von Mises} theorem~\parencite{van2000asymptotic}.

\section{Informative priors}

Finally, the last class of priors are the \textit{informative priors} the purpose of which is, unlike \textit{Jeffreys} and \textit{maximum entropy} priors which attempt to minimise the amount of prior information, to include and take into account useful information for an analysis. They are often highly concentrated in parameter space and might have been the outcome (i.e. in the form of a posterior distribution) of a previous experiment or analysis of older data. Their aim is clearly to inform the analysis and often no attempt is made to restrict the amount of information provided.


%************************************************