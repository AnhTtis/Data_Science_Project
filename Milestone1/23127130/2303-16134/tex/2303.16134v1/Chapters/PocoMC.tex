% !TEX TS-program = pdflatex
% !TEX root = ../ArsClassica.tex

%************************************************
\chapter{pocoMC}
\label{chp:pocomc}
%************************************************
 
\lstset{numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt
}    

This chapter presents \textit{pocoMC} which is the main contribution introduced in the paper titled \textit{pocoMC: A Python package for accelerated Bayesian inference in astronomy and cosmology} that was submitted for publication in the \textit{Journal of Open Source Software} in July 2022~\parencite{karamanis2022pocomc}. The content of the chapter is almost identical to that included in the aforementioned publication with the exception of minor text and figure formatting differences.

\begin{center}
\rule{0.5\textwidth}{.4pt}
\end{center}
\vspace{8pt}

\section{Summary}

\texttt{pocoMC} is a Python package for accelerated Bayesian inference in astronomy and cosmology. The code is designed to sample efficiently from posterior distributions with non--trivial geometry, including strong multimodality and non--linearity. To this end, \texttt{pocoMC} relies on the Preconditioned Monte Carlo algorithm which utilises a Normalising Flow in order to decorrelate the parameters of the posterior. It facilitates both tasks of parameter estimation and model comparison, focusing especially on computationally expensive applications. It allows fitting arbitrary models defined as a log--likelihood function and a log--prior probability density function in Python. Compared to popular alternatives (e.g. nested sampling) \texttt{pocoMC} can speed up the sampling procedure by orders of magnitude, cutting down the computational cost substantially. Finally, parallelisation to computing clusters manifests linear scaling.

\section{Statement of need}

Over the past few decades the volume of astronomical and cosmological data has increased substantially. At the same time, theoretical and phenomenological models in these fields have grown even more complex. As a response to that, a number of methods aiming at efficient Bayesian computation have been developed with the sole task of comparing those models to the available data \parencite{trotta2017bayesian, sharma2017markov}. 
In the Bayesian context, scientific inference proceeds though the use of Bayes' theorem:
\begin{equation}\label{eq_pocomc:bayes}
\mathcal{P}(\theta) = \frac{\mathcal{L}(\theta)\pi(\theta)}{\mathcal{Z}}
\end{equation}
where the posterior $\mathcal{P}(\theta)\equiv p(\theta\vert d,\mathcal{M})$ is the probability of the parameters $\theta$ given the data $d$ and the model $\mathcal{M}$. The other components of this equation are: the likelihood function $\mathcal{L}(\theta)\equiv p(d\vert \theta,\mathcal{M})$, the prior $\pi(\theta) \equiv p(\theta\vert \mathcal{M})$, and the model evidence $\mathcal{Z}=p(d\vert \mathcal{M})$. The prior and the likelihood are usually provided as input in this equation and one seeks to estimate the posterior and the evidence. Knowledge of the posterior, in the form of samples, is paramount for the task of parameter estimation whereas the ratio of model evidences yields the Bayes factor which is the cornerstone of Bayesian model comparison.

Markov chain Monte Carlo (MCMC) has been established as the standard tool for Bayesian computation in astronomy and cosmology, either as a standalone algorithm or as part of another method (e.g. nested sampling \parencite{skilling2006nested}). However, as MCMC relies on the local exploration of the posterior, the presence of non-linear correlation between parameters and multimodality can at best hinder its performance and at worst violate its theoretical guarantees of convergence (i.e. ergodicity). Usually those challenges are partially addressed by reparameterising the model using a common change--of--variables parameter transformation. However, guessing the right kind of reparameterisation a priori is not trivial as it often requires a deep knowledge of the physical model and its symmetries. These problems are usually complicated further by the substantial computational cost of evaluating astronomical and cosmological models. \texttt{pocoMC} is designed to tackle exactly these kinds of difficulties by automatically reparameterising the model such that the parameters of the model are approximately uncorrelated and standard techniques can be applied. As a result, \texttt{pocoMC} produces both samples from the posterior distribution and an unbiased estimate of the model evidence thus facilitating both scientific tasks with excellent efficiency and robustness. Compared to popular alternatives such as nested sampling, \texttt{pocoMC} can reduce the computational cost, and thus, the total run time of the analysis by orders of magnitude, in both artificial and realistic applications \parencite{karamanis2022pmc}. Finally, the code is well-tested and is currently used for research work in the field of gravitational wave parameter estimation \parencite{vretinaris2022postmerger}.

\begin{figure}[H]
    \centering
	\centerline{\includegraphics[scale=0.175]{PocoMC/logo.png}}
    \caption{Logo of \texttt{pocoMC}.}
    \label{fig_pocomc::logo}
\end{figure}

\section{Method}

\texttt{pocoMC} implements the Preconditioned Monte Carlo (PMC) algorithm. PMC combines the popular Sequential Monte Carlo (SMC) \parencite{del2006sequential} method with a Normalising Flow (NF) \parencite{papamakarios2021normalizing}. The latter works as a preconditioner for the target distribution of the former. As SMC evolves a population of particles, starting from the prior distribution and gradually approaching the posterior distribution, the NF transforms the parameters of the target distribution such that any correlation between parameters or presence of multimodality is removed. The effect of this bijective transformation is the substantial rise in the sampling efficiency of the algorithm as the particles are allowed to sample freely from the target without being hindered by its locally--curved geometry. The method is explained in detail in the accompanying publication \parencite{karamanis2022pmc} and we provide only a short summary here.

\subsection{Sequential Monte Carlo}

The basic idea of basic SMC is to sample from the posterior distribution $\mathcal{P}(\theta)$ by first defining a path of intermediate distributions starting from the prior $\pi(\theta)$. In the
case of \texttt{pocoMC} the path has the form:
\begin{equation}\label{eq_pocomc:path}
p_{t}(\theta) = \pi(\theta)^{1-\beta_{t}} \mathcal{P}(\theta)^{\beta_{t}}
\end{equation}
where $0=\beta_{1}<\beta_{2}<\dots<\beta_{T}=1$. Starting from the prior, each distribution with density $p_{t}(\theta)$ is sampled in turn using a collection of particles propagated by a number of MCMC steps. Prior to MCMC sampling, the particles are re-weighted using importance sampling and then re-sampled to account for the transition from $p_{t}(\theta)$ to $p_{t+1}(\theta)$. \texttt{pocoMC} utilises the importance weights of this step to define an estimator for the effective sample size (ESS) of the population of particles. Maintaining a fixed value of ESS during the run allows \texttt{pocoMC} to adaptively specify the $\beta_{t}$ schedule.

\subsection{Preconditioned Monte Carlo}

In vanilla SMC, standard MCMC methods (e.g. Metropolis-Hastings) are used to update the positions of the particles during each iteration. This however can become highly inefficient if the distribution $p_{t}(\theta)$ is characterised by a non--trivial geometry. \texttt{pocoMC}, which is based on PMC, utilises a NF to learn an invertible transformation that simplifies the geometry of the distribution by mapping $p_{t}(\theta)$ into a zero-mean unit-variance normal distribution. Sampling then proceeds in the latent space in which correlations are substantially reduced. The positions of the particles are transformed back to the original parameter space at the end of each iteration. This way, PMC and \texttt{pocoMC} are able to sample from very challenging posteriors very efficiently using simple Metropolis-Hastings updates in the preconditioned/uncorrelated latent space.

\section{Features}

\begin{itemize}
    \item User--friendly black-box API (only the log-likelihood, log-prior and some prior samples required from the user)
    \item Default configuration sufficient for most applications (no tuning is required but is possible for experienced users)
    \item Posterior corner, trace, and run plotting tools
    \item Support for both MAF and RealNVP normalising flows with added regularisation \parencite{papamakarios2017masked, dinh2016density}
    \item Straightforward parallelisation using MPI or multiprocessing
    \item Continuous integration, unit tests and wide range of examples available
    \item Extensive documentation available  online \url{http://pocomc.readthedocs.io}
\end{itemize}
