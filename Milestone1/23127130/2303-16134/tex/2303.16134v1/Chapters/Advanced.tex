% !TEX TS-program = pdflatex
% !TEX root = ../ArsClassica.tex

%************************************************
\chapter{Advanced methods}
\label{chp:advanced}
%************************************************

\begin{flushright}
\itshape
Look on my works, ye Mighty, and despair! \\
\medskip
--- Percy Shelley, Ozymandias
\end{flushright}

This chapter introduces two advanced Monte Carlo methods, \textit{Sequential Monte Carlo} and \textit{Nested sampling}, which combine different previously introduced methods, such as MCMC and importance sampling, in order to provide samples from posterior distributions and estimate the model evidence. What distinguishes those two methods from all the previous ones introduced in this thesis, is their level of complexity and their reliance on multiple individual algorithms as their constituent parts.

\section{Sequential Monte Carlo}

\textit{Sequential Monte Carlo (SMC)} is, from a physics point of view, conceptually related to the notion of \textit{thermodynamic reversibility}. For a physical process starting from a state $A$ and ending in a state $B$, to be thermodynamically reversible, the transition has to be slow enough such that each intermediate state of the system is approximately in equilibrium.

\subsection{Background}

The basic idea of SMC is to slowly guide a population of $n$ particles $\lbrace\theta_{i}^{(t)} \rbrace_{i=1}^{n}$, drawn from a known probability distribution $\rho(\theta)$, through a series of intermediate distributions which create a path from $\rho(\theta)$ to the target distribution of interest $p(\theta)$~\parencite{liu1998sequential}. In the context of SMC, the rate of this transition is governed by the number of intermediate distributions bridging $\rho(\theta)$ to $p(\theta)$. Just like in \textit{annealed importance sampling (AIS)}, SMC relies on a number of MCMC steps performed in each intermediate step by every particle. This aims to equilibrate the particles by letting them reach the equilibrium distribution of each step. Furthermore, when transitioning from an intermediate distribution to the next, the particle distribution is adjusted using importance sampling. This guarantees that the particle distribution at any stage is the correct equilibrium distribution. 

The main difference between SMC and AIS is the use of resampling in the case of SMC. During the run, the particle distribution might experience \textit{weight degeneracy}, that is, only a few of the particles have non--negligible importance weights with the rest of them being vanishingly small. This high weight--variance can substantially affect any expectation values. In order to address this issue, SMC performs regular resampling steps, in which the particle distribution is resampled according to their weights, and the importance weights are re-initialised to be equal.

SMC methods are particularly suited for challenging target distributions which exhibit multiple modes. Furthermore, modifications of the main algorithm that we will present here can also be used for tasks of \textit{online learning} in which the data arrive sequentially. These algorithms are most often called by the name of \textit{particle filters}~\parencite{naesseth2019elements}.

\subsection{Bridging the prior and the posterior}

A common way to construct such a sequence of intermediate distributions that bridge a known density $\rho(\theta )$ to the target density $p(\theta )$ is to interpolate between the two densities
\begin{equation}
    \label{eq:geometric_interpolation}
    p_{t}(\theta) \propto \rho^{1-\beta_{t}}(\theta) p^{\beta_{t}}(\theta)\,,\quad t=1,\dots,m\,,
\end{equation}
where $\beta_{t}$ is a temperature annealing ladder, such that
\begin{equation}
    \label{eq:temperature_ladder}
    0 = \beta_{1} < \beta_{2} < \dots < \beta_{m} = 1\,.
\end{equation}
In the Bayesian context, a natural choice is to set the prior as the auxiliary density $\rho(\theta) = p(\theta\vert\mathcal{M})$ and the posterior as the target density $p(\theta) = p(\theta\vert d,\mathcal{M})$. Equation \ref{eq:geometric_interpolation} then reduces to the usual annealed or tempered interpolation
\begin{equation}
    \label{eq:annealed_path}
    p_{t}(\theta) \propto p^{\beta_{t}}(d\vert\theta\mathcal{M}) p(\theta\vert\mathcal{M})\,.
\end{equation}
Although we will focus on this case, the algorithm is valid for any pair of distributions as long as the support of the auxiliary density encompasses that of the target.

\subsection{Correction -- Selection -- Mutation}

\begin{figure}[!ht]
    \centering
	\centerline{\includegraphics[scale=1.14]{Graphics/smc.pdf}}
    \caption{Illustration of the \textit{Sequential Monte Carlo} algorithm with its three fundamental steps. During the correction step the particles are reweighted to represent the next probability distribution. Selection removes the particles with the smaller important weights and multiplies those with larger weights. Finally, mutation diversifies the particles by moving them.}
    \label{fig:smc}
\end{figure}
Given the initial positions of the particles $\lbrace\theta_{i}^{(1)} \rbrace_{i=1}^{n}$ drawn from the prior distribution, as well as the initial weights $\lbrace W_{i}^{(1)}\rbrace _{i=1}^{n}=1/n$, SMC proceeds by the sequential application of the following three steps, selection, mutation, and correction, until the posterior density is reached. The procedure that takes place in a single iteration $t$ is illustrated in Figure \ref{fig:smc} and involves the steps:
\begin{enumerate}
    \item \textbf{Correction / reweighting} -- During this stage, the weights of the particles are updated according to
    \begin{equation}
        \label{eq:update_particle_weights}
        w_{i}^{(t)}=  W_{i}^{(t-1)}\times \frac{p_{t}(\theta_{i}^{(t-1)})}{p_{t-1}(\theta_{i}^{(t-1)})} = W_{i}^{(t-1)}\times \left[p(d\vert\theta_{i}^{(t-1)},\mathcal{M})\right]^{\beta_{t}-\beta_{t-1}}\,,
    \end{equation}
    where with $w_{i}$ we denote the unnormalised weights and with $W_{i}=w_{i}/\sum_{i=1}^{n}w_{i}$ the normalised ones.
    
    The reweighing step accounts and corrects for any deviations of the particle distribution from the typical set of the target $p_{t}$. The ratio of the normalisation constants is estimated as
    \begin{equation}
        \label{eq:ratio_of_norm_constants}
        \frac{\mathcal{Z}_{t}}{\mathcal{Z}_{t-1}} = \sum_{i=1}^{n}w_{i}^{(t)}\,.
    \end{equation}
    Assuming the density for $t=1$ corresponds to the prior for which $\mathcal{Z}_{1}=1$, equation \ref{eq:ratio_of_norm_constants} will eventually lead to the estimation of the model evidence $\mathcal{Z}_{m} = p(d\vert\mathcal{M})$. 
    
    \item \textbf{Selection / resampling} -- The particle positions $\lbrace\theta_{i}^{(t-1)} \rbrace_{i=1}^{n}$ are resampled according to their weights $\lbrace W_{i}^{(t-1)} \rbrace_{i=1}^{n}$. The weights are then set again to be equal, $W_{i}^{(t-1)}=1/n$. Their new, resampled, positions are denoted as  $\lbrace\Tilde{\theta}_{i}^{(t-1)} \rbrace_{i=1}^{n}$. Particles with small weight values are removed and those with large importance weights are multiplied.
    
    Resampling can be done using simple \textit{multinomial resampling}, in which we draw $n$ new particles, with replacement, with probabilities given by their weights, or using more advanced schemes characterised by lower variance~\parencite{li2015resampling}. This process can be performed in each iteration, or only when some criterion is triggered (e.g. when the effective sample size of the weights drops below a threshold). 
    
    Finally, caution must be taken when applying resampling too frequently. This could lead to the phenomenon of \textit{weight impoverishment} in which there is no diversity between the particle positions. Fortunately, weight impoverishment is also reduced by the next step.
    
    \item \textbf{Mutation / propagation} -- Finally, the population of particles $\lbrace\Tilde{\theta}_{i}^{(t-1)} \rbrace_{i=1}^{n}$ is updated and the particles move to their new positions $\lbrace\theta_{i}^{(t)} \rbrace_{i=1}^{n}$ by performing a number of MCMC steps targeting the density $p_{t}(\theta)$. 
    
    The purpose of this step is to diversify the particles and allow their distribution to approach the stationary distribution. An advantage of SMC is that the particle distribution from the previous iteration can be used to construct efficient proposal distributions for MCMC for the current density. Furthermore, as $n$ particles are updated at once, this step can be done in parallel. Any MCMC method can be used in this step and there is no requirement for the final/new positions to be uncorrelated from the initial ones, although in practice this helps reduce the variance of the estimates.
    
    A common approach is to use the particle covariance $\Sigma^{(t-1)}$ to construct a normal proposal distribution $q(\theta'\vert\theta)=\mathcal{N}(\theta'\vert\theta,\Sigma^{(t-1)})$.
\end{enumerate}

Once all three steps are completed, the value of $\beta$ is updated and the process is repeated again until $\beta$ reaches the value of one. The names of those three steps are inspired by natural selection and evolutionary programming. The reason is the apparent analogy with genetic algorithms~\parencite{koza1994genetic}. More specifically, reweighting, resampling and propagation have the roles of correction, selection and mutation in genetic algorithms, in which the particle positions are the \textit{genes} and the importance weights play the role of the so--called \textit{fitness}. A critical difference with most genetic algorithms is the fact that SMC solves a sampling task, not an optimisation one, and thus the solution is represented by the distribution of the particles and not by any particle individually.

\begin{algorithm}[ht!]
\caption{Sequential Monte Carlo} \algolabel{smc}
\begin{algorithmic}[1]
\REQUIRE{initial state for the ensemble $\theta^{(1)}=(\theta_{1}^{(1)},\dots,\theta_{K}^{(1)})$, prior probability density $\pi(\theta) \equiv p(\theta\vert\mathcal{M})$, likelihood function $\mathcal{L}(\theta)\equiv p(d\vert\theta,\mathcal{M})$, and a local MCMC kernel $\theta'\leftarrow\mathcal{K}(\theta; f(\theta))$ (e.g. $N$ steps of random--walk Metropolis update)}
\ENSURE{Posterior samples and estimate of the model evidence $\mathcal{Z}$}
\STATE{Initialise temperature parameter $\beta_{1} = 1$}
\STATE{Initialise estimate of evidence $\mathcal{Z} = 1$}
\FOR{$i = 1$ \TO $n$}
    \STATE{Draw particle positions from the prior $\theta_{i}^{(1)}\sim \pi(\theta)$}
    \STATE{Initialise particle weights $W_{i}^{(1)} = 1 / n$}
\ENDFOR
\WHILE{$\beta_{t}\neq 1$}
    \STATE{Update iteration index $t \leftarrow t + 1$}
    \STATE{Set temperature $\beta_{t}$ solving $\left( \sum_{i=1}^{n}w_{i}^{(t)}(\beta_{t})\right)^{2}/\sum_{i=1}^{n}\left(w_{i}^{(t)}(\beta_{t})\right)^{2} = \alpha \times n$ where the importance weights are computed as $w_{i}^{(t)}\leftarrow W_{i}^{(t-1)}\mathcal{L}(\theta_{i}^{(t-1)})^{\beta_{t}-\beta_{t-1}}$}
    \STATE{Update evidence estimate $\mathcal{Z}\leftarrow \mathcal{Z}\times n^{-1}\sum_{i=1}^{n}w_{i}^{(t)}$}
    \STATE{$\left\lbrace \Tilde{\theta}_{i}^{(t-1)}\right\rbrace_{i=1}^{n}\leftarrow$ resample $\left\lbrace \theta_{i}^{(t-1)}\right\rbrace_{i=1}^{n}$ according to $\left\lbrace w_{i}^{(t)}\right\rbrace_{i=1}^{n}$}
    \FOR{$i=1$ \TO $n$}
        \STATE{Reset weights $W_{i}^{(t)}\leftarrow 1/n$}
    \ENDFOR
    \STATE{Update particles using MCMC 
    \[
    \left\lbrace \theta_{i}^{(t)}\right\rbrace_{i=1}^{n}\leftarrow\mathcal{K}\left(\left\lbrace \Tilde{\theta}_{i}^{(t-1)}\right\rbrace_{i=1}^{n}; \pi(\theta)\mathcal{L}(\theta)^{\beta_{t}}\right)
    \]
    }
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\subsection{Effective sample size}

A common measure of the quality of the importance weights of the particles, at any iteration of the SMC run, is the \textit{effective sample size (ESS)}
\begin{equation}
    \label{eq:effective_sample_size_smc}
    \text{ESS}_{t} = \frac{\mathbb{E}_{p_{t}}\left[w^{(t)}\right]^{2}}{\mathbb{E}_{p_{t}}\left[\left(w^{(t)}\right)^{2}\right]}\,.
\end{equation}
which can be estimated as:
\begin{equation}
    \label{eq:effective_sample_size_smc_estimator}
    \Hat{\text{ESS}}_{t} = \frac{\left( \sum_{i=1}^{n}w_{i}^{(t)}\right)^{2}}{\sum_{i=1}^{n}(w_{i}^{(t)})^{2}} = \frac{1}{\sum_{i=1}^{n}(W_{i}^{(t)})^{2}}\,.
\end{equation}

\subsection{Setting the temperature ladder}

A $\beta$ ladder can be specified \textit{a priori} or determined adaptively during the run~\parencite{gelman1998simulating}. In the first case, the resampling step is usually triggered whenever the ESS drops below a prespecified threshold value (e.g. $50\% - 95\%$). In the latter case, the next value of $\beta$ is chosen adaptively such that the ESS has an approximately constant fraction $\alpha$ (e.g. $50\% - 95\%$) of the number of particles $n$ throughout the duration of the SMC run. Numerically, this can be done by solving
\begin{equation}
    \label{eq:bisection_method}
    \frac{\left[ \sum_{i=1}^{n}w_{i}^{(t)}(\beta_{t})\right]^{2}}{ \sum_{i=1}^{n}\left[w_{i}^{(t)}(\beta_{t})\right]^{2}}=\alpha \times n\,,
\end{equation}
for the next $\beta_{t}$ such that $\beta_{t-1}<\beta_{t}\leq 1$ using, for instance, the \emph{bisection method}~\parencite{burden2015numerical}.

\section{Nested sampling}

\textit{Nested sampling (NS)}, originally developed by \textcite{skilling2004nested, skilling2006nested}, is a method for estimating the model evidence $\mathcal{Z} = p(d\vert\mathcal{M})$. The basic idea is to approximate the evidence by integrating the prior in \textit{nested} shells of constant likelihood. Despite its original purpose to estimate the model evidence, NS can also provide weighted samples from the posterior distribution as an optional byproduct. Therefore, the method is suitable for both tasks of parameter estimation and model comparison~\parencite{ashton2022nested}. Over the years, many variants of NS have emerged, with each one aiming to improve a different aspect of the original version~\parencite{brewer2011diffusive, feroz2013importance, higson2019dynamic}.

\subsection{Multi--dimensional integration}

\begin{figure}[H]
    \centering
	\centerline{\includegraphics[scale=0.53]{Graphics/nested_sampling_integral_joint12.pdf}}
    \caption{Illustration comparing two ways which one can use to approximate the model evidence integral. The left panel shows the direct multi--dimensional integration over the parameters. The right panel shows the one--dimensional integration over the prior volume $X$ enclosed in the iso--likelihood contours.}
    \label{fig:nested_sampling_integral}
\end{figure}
NS attempts to compute the evidence integral,
\begin{equation}
    \label{eq:evidence_integral}
    \mathcal{Z} = \int \mathcal{L}(\theta) \pi(\theta) d\theta \,,
\end{equation}
where $\mathcal{L}(\theta) = p(d\vert\theta,\mathcal{M})$ is the likelihood function and $\pi(\theta)=p(\theta\vert \mathcal{M})$ is the prior, by transforming it into a one--dimensional integral over the prior volume
\begin{equation}
    \label{eq:prior_volume}
    X(\lambda) = \int _{\mathcal{L}(\theta)>\lambda}\pi(\theta)d\theta,
\end{equation}
enclosed in the iso--likelihood contour defined by $\mathcal{L}(\theta) = \lambda$. Equation \ref{eq:evidence_integral} can then be written as
\begin{equation}
    \label{eq:evidence_integral_transformed}
    \mathcal{Z} = \int _{0}^{+\infty}X(\lambda)d\lambda = \int_{0}^{1} \mathcal{L}(X)dX\,,
\end{equation}
assuming that $\mathcal{L}(X(\lambda)) = \lambda$ exists. Figure \ref{fig:nested_sampling_integral} illustrates the equivalency between the integrals of equations \ref{eq:evidence_integral} and \ref{eq:evidence_integral_transformed}. Unlike equation \ref{eq:evidence_integral}, the above integral is now 1--dimensional and can be approximated using standard numerical integration techniques (e.g. quadrature),
\begin{equation}
    \label{eq:evidence_quadrature}
    \mathcal{Z} = \sum_{i=1}^{m}\mathcal{L}_{i}w_{i}\,,
\end{equation}
where,
\begin{equation}
    \label{eq:ns_weights}
    w_{i} = \frac{X_{i-1}-X_{i+1}}{2}\,.
\end{equation}
This of course assumes that we are able to evaluate the iso--likelihood contours $\mathcal{L}_{i}=\mathcal{L}(X_{i})$ associated with an ordered collection of samples with prior volume $1 > X_{1} > X_{2} > \dots > X_{m} > 0$. This is illustrated in Figure \ref{fig:nested_sampling_samples} for a collection of $8$ samples that are uniformly distributed in the prior volume.
\begin{figure}[H]
    \centering
	\centerline{\includegraphics[scale=0.53]{Graphics/nested_sampling_samples_joint12.pdf}}
    \caption{Illustration of $8$ samples drawn uniformly from the prior with their respective iso--likelihood contours (\textit{left}), along with their corresponding contributions to the evidence integral (\textit{right}).}
    \label{fig:nested_sampling_samples}
\end{figure}

Using the simpler weights $w_{i}=X_{i}-X_{i+1}$ in equation \ref{eq:evidence_quadrature} a lower bound on the evidence can be estimated as
\begin{equation}
    \label{eq:evidence_integral_lower_bound}
    \mathcal{Z} \geq \sum_{i=1}^{m}\mathcal{L}_{i} \left(X_{i}-X_{i+1}\right)\,.
\end{equation}
Similarly, an upper bound also exists, using $w_{i}=X_{i-1}-X_{i}$, which can be written as
\begin{equation}
    \label{eq:evidence_integral_upper_bound}
    \mathcal{Z} \leq \sum_{i=1}^{m}\mathcal{L}_{i}\, \left(X_{i-1}-X_{i}\right)+X_{m}\mathcal{L}_{\mathrm{max}}\,,
\end{equation}
where $\mathcal{L}_{\mathrm{max}}$ is the maximum likelihood value to be found as $X\to 0$.

Soon after its original inception, it was realised that a NS run can also be used for the task of parameter estimation without any additional computation. In particular, the collected samples combined with their normalised weights
\begin{equation}
    \label{eq:posterior_weights}
    p_{i} = \frac{\mathcal{L}_{i}w_{i}}{\mathcal{Z}}\,,
\end{equation}
correspond to weighted samples from the posterior distribution and thus can be used to compute expectation values
\begin{equation}
    \label{eq:ns_expectation_values}
    \mathbb{E}_{p}\left[ f(\theta)\right] = \sum_{i}^{n}p_{i}f(\theta_{i})\,.
\end{equation}

\subsection{Sampling procedure}

\begin{figure}[H]
    \centering
	\centerline{\includegraphics[scale=0.53]{Graphics/nested_sampling_step.pdf}}
    \caption{Illustration of the nested sampling procedure. Given some uniformly distributed points from the prior, we identify and remove the worst point, that is, the point with the minimum likelihood value. $\mathcal{L}_{min}$, and replace it a new point sampled from the prior subject to the likelihood constrain $\mathcal{L} > \mathcal{L}_{min}$. Finally, the volume is contracted to account for the removal of the worst point.}
    \label{fig:nested_sampling_step}
\end{figure}
The NS algorithm begins by drawing a collection of points $\lbrace \theta_{i}\rbrace_{i=1}^{n}$ uniformly from the prior, often called \textit{live points}. We can associate each live point $\theta_{i}$ with a prior volume $X$--value, namely the volume that would be enclosed by the iso--likelihood contour $\mathcal{L}_{i}=\mathcal{L}(\theta_{i})$. On average, we expect roughly half of the live points to fall inside the iso--likelihood contour corresponding to half prior volume $X=1/2$, one quarter to $X=1/4$, one eighth to $X=1/8$ and so on. In other words, since the live points are uniformly distributed under the prior, the corresponding $X$--values are uniformly distributed between $0$ and $1$. This is illustrated in Figure \ref{fig:nested_sampling_samples} and the top panel of Figure \ref{fig:nested_sampling_step}.

What we described so far is only the first step of the algorithm, and one still needs a way to propagate the live points into regions of smaller prior volume (i.e. lower $X$) in order to probe iso--likelihood contours corresponding to higher likelihood values. NS achieves this by first identifying the live point with the lowest likelihood value $\mathcal{L}^{*}=\mathcal{L}_{1}$, corresponding to volume $X_{1}$ and removing it. The remaining live points are now distributed over a compressed volume $X_{1}$. On average, the volume compression factor is 
\begin{equation}
    \label{eq:average_volume_compression}
    t = e^{-1/n}\,,
\end{equation}
such that the compressed volume is $X_{1}=t\times X_{0}$, where $X_{0}=1$ is the initial total volume. Finally, we sample a new live point to replace the one that we removed. The new point is sampled uniformly from the prior subject to the constrain $\mathcal{L} > \mathcal{L}^{*}$, that is, from the likelihood--constrained prior
\begin{equation}
    \label{eq:likelihood_constrained_prior}
    \pi^{*}(\theta) \propto
    \begin{cases}
    \pi(\theta) & \mathrm{if }\mathcal{L}(\theta) > \mathcal{L}^{*}\,, \\
    0 & \text{otherwise}\,.
    \end{cases}
\end{equation}
This whole process, that is shown in Figure \ref{fig:nested_sampling_step}, is repeated multiple times until a criterion for termination is met. In each iteration, the volume shrinks on average by the compression factor of equation \ref{eq:average_volume_compression}.

\subsection{Termination criterion}

During an NS run, the remaining prior volume $X$ asymptotically approaches $0$. The fact that we can only perform a finite number of steps means that it is unavoidable to introduce a truncation error into the evidence estimate of equation \ref{eq:evidence_integral_transformed}. A common way of determining when to stop is to approximately estimate the amount of remaining evidence and terminate the run when this can be considered negligible for the purpose of the analysis. 

Perhaps the simplest way to roughly estimate the remaining evidence is by utilising the upper bound of equation \ref{eq:evidence_integral_upper_bound}. In this case, the remaining evidence is approximated as $\Delta\mathcal{Z}=\mathcal{L}_{\mathrm{max}}X_{m}$, where $\mathcal{L}_{\mathrm{max}}$ is the maximum likelihood value, estimated from the current population of live points, and $X_{m}$ is simply the estimate of the remaining prior volume. An alternative would be to use the mean likelihood of the live points and get $\Delta\mathcal{Z}=\Bar{\mathcal{L}}X_{m}$. The run then terminates when $\Delta \mathcal{Z}/\mathcal{Z}$ drops below a prespecified threshold.

Of course, neither of these approaches guarantees that the run will terminate early and that beyond lies a ``spike'' of huge likelihood. Upon deciding to stop, however, the current estimate of the model evidence is approximately corrected by either adding $\Delta\mathcal{Z}$ or removing the live points one--by--one in accordance with the NS procedure and adding their respective evidence contributions $\mathcal{L}_{i}X_{i}$, but without replacing them with new ones.

\subsection{Uncertainty}

So far we have assumed that the compression factor is given by equation \ref{eq:average_volume_compression}, however, this is simply the mean compression factor associated with the removal of the outermost or lowest--likelihood live point. In truth, the prior volume bounded by the iso--likelihood contour of that point can be slightly different from what the mean compression factor predicts. The compression in volume $t$ associated with the removal of the outermost of $n$ live points follows a $\mathrm{Beta}(n,1)$ probability distribution with density,
\begin{equation}
    \label{eq:beta_n1}
    p(t) = n t^{n-1}\,,
\end{equation}
where the first $n$ factor comes from the fact that any live point could be the outermost, and the second factor from the fact that the remaining $n-1$ live points lie uniformly distributed above the outermost at $t$.

The compression factors can therefore be sampled from the probability distribution of equation \ref{eq:beta_n1} instead of just assuming their expected value of equation \ref{eq:average_volume_compression}. Furthermore, we can use equation \ref{eq:beta_n1} to compute the expectation values
\begin{equation}
    \label{eq:logt_expectations}
    \mathbb{E}[\log t] = -\frac{1}{n}\,,\quad\,\mathrm{Var}[\log t] = \frac{1}{n^{2}}\,.
\end{equation}
Since the individual $\log t$ are independent, we expect that after $i$ steps, the prior volume to have shrunk to
\begin{equation}
    \label{eq:contracted_prior_volume}
    \log X_{i} = -\frac{i\pm\sqrt{i}}{n}\,.
\end{equation}
What the above expression means is that there is uncertainty in the estimates of the compression factor which enter into the prior volume estimates too. In other words, there is uncertainty in the number of steps $i$ required for the prior volume to shrink to a certain value $X_{i}$.

The uncertainty originating from the noisy estimates of the compression factor $t$ also propagates into the estimate of the model evidence. To quantify this we need to consider the information gained when transitioning from the prior $\pi(\theta)$ to the posterior $p(\theta)$, given by the \textit{Kullback--Leibler (KL) divergence},
\begin{equation}
    \label{eq:prior_to_posterior_information_gain}
    H = \int p(\theta)\log\left( \frac{p(\theta)}{\pi(\theta)}\right)d\theta\,.
\end{equation}
We can rewrite the above equation in terms of the prior volume $X$, as
\begin{equation}
    \label{eq:prior_to_posterior_information_gain_transformed}
    \begin{split}
        H &= \int p(X)\log p(X) dX \\
         &= -\int p(X)\log X dX + \int p(\log X) \log p(\log X) d \log X\,,
    \end{split}
\end{equation}
where $p(X)=\mathcal{L}(X)/\mathcal{Z}$ is the volume posterior density. Ignoring the second term on the right hand size, which is subdominant, we thus get that the KL divergence provides a measure of the compression we require to reach the bulk of the posterior mass,
\begin{equation}
    \label{eq:posterior_bulk_compression}
    H = - \log X\,.
\end{equation}
Comparing equation \ref{eq:contracted_prior_volume} and \ref{eq:posterior_bulk_compression} we roughly expect $n H \pm \sqrt{n H}$ steps to reach the bulk of the posterior mass. Equivalently, the uncertainty introduced into the estimate of the model evidence is
\begin{equation}
    \label{eq:evidence_uncertainty}
    \Delta \log\mathcal{Z} = \sqrt{\frac{H}{n}}\,.
\end{equation}
Of course, the above expression does not include any sources of numerical error such as truncation error.

\subsection{Likelihood--constrained prior sampling}

The efficient application of the NS algorithm requires sampling from the prior distribution $\pi(\theta)$ subject to the likelihood constrain $\mathcal{L} > \mathcal{L}^{*}$. Unfortunately, drawing points from the prior until the likelihood criterion is met is not feasible in practice, as the volume contained in the constrained prior shrinks exponentially with each iteration. For this reason, two different approaches, \textit{region} and \textit{step} samplers are often employed in order to produce samples from the likelihood--constrained prior.

For the sake of simplicity, both samplers usually operate in the \textit{latent} parameter space that the prior is uniform over the unit hypercube. In this case, the practitioner specifies their prior preference, not by providing a (log--) probability density function, but by defining the \textit{inverse--cumulative density function} $\Phi^{-1}$ that transforms points $\phi$ in the hypercube to points $\theta$ in the original parameter space. For instance, let us assume that we require a normal prior on a parameter $\theta\sim\mathcal{N}(\mu,\sigma^{2})$. We can transform a unit hypercube parameter $\phi\sim\mathcal{U}(0,1)$, using the standard normal distribution's \textit{inverse--cumulative density function} $\Phi^{-1}$, such that,
\begin{equation}
    \label{eq:unit_to_theta}
    \theta = \mu + \Phi^{-1}(\phi) \sigma\,.
\end{equation}

\subsubsection{Region samplers}

The basic idea behind \textit{region} samplers is to construct a hypersurface that bounds a given iso--likelihood contour. In practice, this is done using simple geometric shapes (e.g. spheres, ellipses, etc.). The hypersurface must encompass the current distribution of live points and at least contain the currently estimated volume. One can then sample uniformly from within the volume enclosed by the hypersurface until the likelihood--constraint is satisfied. In order to reduce the risk of missing parts of the currently estimated volume, the bounding region is usually expanded by a prespecified factor or using cross--validation of the live points.

Most region samplers attempt to construct such a bounding region by wrapping the current generation of live points with one or multiple ellipsoids. Using multiple ellipsoids offers some flexibility in the case of multimodal posterior distributions. The most popular such sampler is the \textit{MultiNest} algorithm that determines the shape and location of the ellipsoids based on the mean and covariance of the live points, by first estimating the number of distinct modes, and thus required ellipsoids, using a clustering algorithm~\parencite{feroz2009multinest}.

Region samplers have to face serious challenges when the complexity of the posterior or the dimensionality of the parameter space increases. In the first case, the ability to accurately bound the current volume depends on the number of live points, with a higher number often resulting in better bounding regions. The second limitation arises from the curse of dimensionality. As the number of dimensions increases, most of the volume of the bounding shape concentrates near its edges, and given that the bounding region is often chosen to be significantly broader than the encompassing likelihood--constrained volume to guarantee that no parts are encroached, the total number of samples until one is found to lie within the iso--likelihood contour increases exponentially. As a consequence, region samplers are more efficient and appropriate for low--dimensional problems, that is, $D\lesssim 10 - 20$. 

\subsubsection{Step samplers}

On the other hand, \textit{step} samplers do not rely on a bounding region and thus bypass some of the pathologies of \textit{region} samplers. Instead, they evolve a randomly chosen live point through a sequence of local steps to an approximately independent position. This is usually achieved using some MCMC method targeting the likelihood--constrained prior of equation \ref{eq:likelihood_constrained_prior} as the target distribution. The advantage of using MCMC methods in the context of NS is that, in each iteration, one can use the distribution of the live points to construct effective proposal distributions for the MCMC sampler.

Although \textit{step} samplers enjoy a better scaling with the number of dimensions than region samplers, there are still challenges in their use. First of all, determining the minimum number of steps to perform for the new point to be independent of its starting position (i.e. the randomly chosen live point) is not trivial. Although small correlations can be effectively ignored, larger violations can have catastrophic results and lead to substantial bias in the final estimates of NS. Furthermore, the step sampler must be tuned appropriately to achieve good sampling performance. Adaptation during a given iteration has to be diminishing in order to avoid spurious effects and biases.

Although any MCMC method  (e.g. random walk Metropolis, slice sampling, etc.) can in principle be used as a step sampler, there are also methods that are naturally suited and have been developed for use in the context of sampling from the likelihood--constrained prior. One such example is \textit{Galilean Monte Carlo (GMC)}~\parencite{feroz2013exploring, skilling2012bayesian, skilling2019galilean} that samples by moving consistently along a direction until a proposed point is rejected, by being outside the iso--likelihood contour. In this case, the sampler reflects off the current iso--likelihood boundary.

\subsection{Parallelisation}

Parallelising NS is not as straightforward as with other Monte Carlo algorithms (e.g. Sequential Monte Carlo), as the method relies on updating a single (worst) point at a time. In general, we would like to generate as many candidate points per step as the number of available CPUs (i.e. $n_{\mathrm{CPU}}$) and evaluate their likelihoods in parallel. In this case, there are three strategies that one can follow:
\begin{enumerate}
    \item Replace a single live point and discard as many as $n_{\mathrm{CPU}}-1$ acceptable live points. This scheme is quite wasteful, particularly in cases in which it is likely that more than one candidate point satisfies the likelihood constraint.
    \item Replace the worst (i.e. lowest likelihood) $n_{\mathrm{CPU}}$ live points in a single step. This results in linear speed--up with respect to the number of CPUs but increases the variance of the evidence estimate by a factor of $\sqrt{n_{\mathrm{CPU}}}$.
    \item Replace a single live point and consider the other $n_{\mathrm{CPU}}-1$ candidates for subsequent iterations. This results in a speed--up of $n\log(1+n_{\mathrm{CPU}}/n)$ which is approximately linear for $n_{\mathrm{CPU}}<<n$. The ``diminishing returns'' represented by the logarithmic factor in this expression originate from the fact that the likelihood threshold increases as the run progresses, and thus the points might not be valid for a subsequent iteration. This strategy is the most widely employed in practice.
\end{enumerate}

Finally, it is important to note that apart from parallelising a single NS run, it is also possible to combine different, possibly parallel, independent NS runs into a joint one, thus achieving linear scaling. In order to combine two or more runs together, we collect the points from all runs as live points and begin by removing the worst point, which with no loss of generality we assume that it belongs to run A. Then, as a replacement that satisfies the likelihood constraint, we simply take the replacement that was originally used in run A. We then proceed with the next worst point until all points are accounted for.

\begin{algorithm}[ht!]
\caption{Nested sampling} \algolabel{ns}
\begin{algorithmic}[1]
\REQUIRE{termination criterion (e.g. $\Delta \log \mathcal{Z} \leq \epsilon)$, number of live points $n$, an estimate of the compression factor e.g. $t=\exp(-1/n)$, prior distribution $\pi(\theta)$, likelihood function $p_{i}=\mathcal{L}(\theta)$)}
\ENSURE{Estimate of model evidence $\mathcal{Z}$, posterior samples $\Tilde{\theta}_{i}$ with weights $\mathcal{L}_{i}w_{i}/\mathcal{Z}$}
\STATE{Initialise volume $X = 1$ and evidence $\mathcal{Z}=0$}
\STATE{Draw $n$ live points from the prior $\theta_{1}, \theta_{2}, \dots, \theta_{n}\sim\pi(\theta)$}
\REPEAT
\STATE{Find the minimum likelihood value of the live points $\mathcal{L}^{*}\leftarrow \min\left( \mathcal{L}(\theta_{1}), \dots, \mathcal{L}(\theta_{n}) \right)$}
\STATE{Replace live point $\theta^{*}$ corresponding to $\mathcal{L}^{*}$ with a new point from the prior satisfying $\mathcal{L}>\mathcal{L}^{*}$}
\STATE{Set $w^{*}\leftarrow \Delta X$ where $\Delta X = (1-t)X$}
\STATE{Update estimate of the evidence $\mathcal{Z}\leftarrow \mathcal{Z} + \mathcal{L}^{*}w^{*}$}
\STATE{Store values of $w_{i}\leftarrow w^{*}$, $\mathcal{L}_{i}\leftarrow \mathcal{L}^{*}$,  and $\Tilde{\theta}_{i}\leftarrow \theta^{*}$}
\STATE{Contract volume $X \leftarrow t X$}
\UNTIL{termination criteria satisfied}
\STATE{Update evidence $\mathcal{Z}\leftarrow \mathcal{Z}+\frac{1}{n}\sum_{j=1}^{n}\mathcal{L}(\theta_{j})\,X$}
\end{algorithmic}
\end{algorithm}