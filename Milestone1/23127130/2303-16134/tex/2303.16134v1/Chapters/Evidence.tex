% !TEX TS-program = pdflatex
% !TEX root = ../ArsClassica.tex

%************************************************
\chapter{Evidence and Bayes factor computation}
\label{chp:evidence}
%************************************************

\begin{flushright}
\itshape
There is nothing more deceptive than an obvious fact. \\
\medskip
--- Arthur Conan Doyle
\end{flushright}

\section{Naive Monte Carlo estimator}

The simplest estimator for the evidence we can construct is just the expectation value of the likelihood function with respect to the prior distribution~\parencite{hammersley1964percolation, raftery1991three}. The, so--called, \textit{Naive Monte Carlo (NMC)} estimator can be computed as the sum
\begin{equation}
    \label{eq:naive_monte_carlo}
    \Hat{p}_{\mathrm{NMC}}(d\vert\mathcal{M})=\frac{1}{n}\sum_{i=1}^{n}p(d\vert \theta_{i},\mathcal{M})\,,\quad \mathrm{with}\quad \theta_{i}\sim p(\theta\vert \mathcal{M})\,.
\end{equation}
Although simple and unbiased, this approach can become extremely inefficient and result in a high variance in higher dimensions as the probability mass concentrates in the typical set that occupies a negligible fraction of the prior volume~\parencite{newton1994approximate}. For this reason, this technique is only recommended for low--dimensional problems (i.e. $D\leq 3$).

\section{Importance sampling estimator}

A more general strategy for the unbiased estimation of the evidence is importance sampling using samples from an auxiliary distribution $q(\theta)$. A simple estimator can then be constructed as,
\begin{equation}
    \label{eq:importance_sampling_evidence}
    \Hat{p}_{\mathrm{IS}}(d\vert\mathcal{M})=\frac{1}{n}\sum_{i=1}^{n}\frac{p(\theta_{i}\vert d,\mathcal{M})}{q(\theta_{i})}\,,\quad \mathrm{with}\quad \theta_{i}\sim q(\theta)\,.
\end{equation}
These estimators share the same difficulty as most methods based upon importance sampling, that is, a large overlap between the typical set of the proposal and posterior distribution must be achieved for the method to be effective. Constructing effective proposal distributions becomes increasingly unmanageable as the number of dimensions increases and thus the application of this method on its own is limited to low dimensions. Finally, the importance sampling estimator reduces to the NMC one when the proposal distribution is chosen to be the prior.

\section{Harmonic mean estimator}

The \textit{harmonic mean (HM)} estimator is another variation of the importance sampling estimator in which the posterior is used as the proposal and the prior as the target distribution~\parencite{newton1994approximate}. This suggests the following estimator,
\begin{equation}
    \label{eq:harmonic_mean}
    \Hat{p}_{\mathrm{HM}}(d\vert\mathcal{M}) = \frac{1}{\frac{1}{n}\sum_{i=1}^{n}\frac{1}{p(d\vert\theta_{i}\mathcal{M})}}\,,\quad \mathrm{with}\quad \theta_{i}\sim p(\theta\vert d, \mathcal{M})\,.
\end{equation}
The possible occurrence of samples with small likelihood value renders the variance of this estimator infinite~\parencite{neal2008harmonic}. This pathology can be addressed by using a mixture $q(\theta)=\delta p(\theta\vert\mathcal{M})+(1-\delta)p(\theta\vert d, \mathcal{M})$  between the prior and the posterior as the proposal distribution, where $\delta$ is very small (e.g. $\delta=0.05$). The resulting method is then called the \textit{stabilised harmonic mean (SHM)} estimator~\parencite{newton1994approximate}.

\section{Laplace estimator}

As discussed in detail in Section \ref{sec:laplace_approximation}, for a sufficiently Gaussian target distribution $p(\theta)$ we can use the \textit{Laplace approximation}, that is, a second order expansion around the mode, to estimate expectation values~\parencite{tierney1986accurate}. Assuming that the target distribution is the unnormalised posterior $p(d\vert\theta \mathcal{M})p(\theta\vert \mathcal{M})$, the Gaussian approximation's mean is given by,
\begin{equation}
    \label{eq:laplace_mean_posterior}
    \mu = \underset{\theta}{\mathrm{arg\,max}} ~\left[p(d\vert\theta, \mathcal{M})p(\theta\vert \mathcal{M})\right]\,,
\end{equation}
following equation \ref{eq:mode_of_target}, and the inverse covariance is given by,
\begin{equation}
    \label{eq:laplace_covariance_posterior}
    \left( \Sigma^{-1}\right)_{ij} = - \frac{\partial^{2}}{\partial\theta_{i}\partial\theta_{j}} \left[\log p(d\vert\theta, \mathcal{M})+\log p(\theta\vert \mathcal{M}) \right]\,,
\end{equation}
following equation \ref{eq:precision_of_target}. Then, the model evidence is approximated by the normalising constant of the Gaussian, or in other words,
\begin{equation}
    \label{eq:laplace_evidence}
    \begin{split}
        \Hat{p}_{L}(d\vert \mathcal{M}) &= \int e^{-\frac{1}{2}(\theta-\mu)^T\Sigma^{-1}(\theta-\mu)}d\theta \\
        &= (2\pi)^{D/2}\det (\Sigma)^{1/2} p(d\vert\theta=\mu,\mathcal{M}) p(\theta=\mu\vert\mathcal{M})\,.
    \end{split}
\end{equation}
As with any method, this result is only as good as the assumptions entering its calculation. The closer the posterior resembles a normal distribution, the better the outcome of the \textit{Laplace} estimator will be.

\section{Bridge sampling}
Originally, \textcite{meng1996simulating} introduced \textit{bridge sampling (BS)} as a way to directly estimate the \textit{Bayes factor} of two models, $\mathcal{M}_{1}$ and $\mathcal{M}_{2}$. However, in this section we present a version of BS that targets the model evidence of a single model $\mathcal{M}$. BS follows from the basic identity,
\begin{equation}
    \label{eq:bridge_sampling_identity}
    1 = \frac{\int p(d\vert\theta,\mathcal{M})p(\theta\vert\mathcal{M})\alpha(\theta)q(\theta)}{\int p(d\vert\theta,\mathcal{M})p(\theta\vert\mathcal{M})\alpha(\theta)q(\theta)}\,,
\end{equation}
where $q(\theta)$ is the \textit{proposal distribution} and $\alpha(\theta)$ is the so--called \textit{bridge function} the \textit{support} of which encompasses that of both the target posterior and of the proposal distribution.

Multiplying both sides of equation \ref{eq:bridge_sampling_identity} with the model evidence $p(d\vert \mathcal{M})$ results in
\begin{equation}
    \label{eq:bridge_sampling_proof}
    \begin{split}
        p(d\vert \mathcal{M}) &= \frac{\int p(d\vert\theta,\mathcal{M})p(\theta\vert\mathcal{M})\alpha(\theta)q(\theta)}{\int \frac{p(d\vert\theta,\mathcal{M})p(\theta\vert\mathcal{M})}{p(d\vert \mathcal{M})}\alpha(\theta)q(\theta)} \\
        &= \frac{\int p(d\vert\theta,\mathcal{M})p(\theta\vert\mathcal{M})\alpha(\theta)q(\theta)}{\int p(\theta\vert d, \mathcal{M})\alpha(\theta)q(\theta)} \,,
    \end{split}
\end{equation}
which can be written as,
\begin{equation}
    \label{eq:bridge_sampling_expectations}
    p(d\vert \mathcal{M}) = \frac{\mathbb{E}_{q(\theta)}\left[p(d\vert\theta,\mathcal{M})p(\theta\vert\mathcal{M})\alpha(\theta) \right]}{\mathbb{E}_{p(\theta\vert d, \mathcal{M})}\left[ \alpha(\theta)q(\theta) \right]} \,,
\end{equation}
in terms of expectation values. The model evidence can then be approximated as,
\begin{equation}
    \label{eq:bridge_sampling_estimator}
    \Hat{p}(d\vert \mathcal{M}) = \frac{\frac{1}{n_{2}}\sum_{i=1}^{n_{2}}p(d\vert\Tilde{\theta}_{i},\mathcal{M})p(\Tilde{\theta}_{i}\vert\mathcal{M})\alpha(\Tilde{\theta}_{i})}{\frac{1}{n_{1}}\sum_{j=1}^{n_{1}}\alpha(\theta_{j}^{*})q(\theta_{j}^{*})}\,,
\end{equation}
where $\Tilde{\theta}_{i}$ are samples from the proposal distribution,
\begin{equation}
    \label{eq:bridge_sampling_proposal_samples}
    \Tilde{\theta}_{i} \sim q(\theta)\,,
\end{equation}
and $\theta_{j}^{*}$ are samples from the posterior distribution,
\begin{equation}
    \label{eq:bridge_sampling_posterior_samples}
    \theta_{j}^{*} \sim p(\theta\vert d, \mathcal{M})\,.
\end{equation}

It is clear from the above discussion that BS relies on samples from both the proposal distribution $q(\theta)$, which plays the role of an \textit{importance density}, and the posterior distribution $p(\theta\vert d, \mathcal{M})$. Often, the proposal distribution is some distribution that is easy to sample from and its \textit{typical set} has a large overlap with the one of the posterior distribution. A common proposal used in practice is a normal distribution with its first two moments matching those of the posterior distribution.

Although highly arbitrary, the choice of the \textit{bridge function} $\alpha (\theta)$ can have a significant impact on the precision of the method for a given proposal distribution. For instance, setting $\alpha(\theta)=[q(\theta)]^{-1}$ the BS estimator reduces to the naive Monte Carlo estimator, whereas setting $\alpha(\theta)=[p(d\vert \theta,\mathcal{M})p(\theta\vert \mathcal{M})q(\theta)]^{-1}$ leads to the \textit{harmonic mean estimator}. \textcite{meng1996simulating} showed that the \textit{optimal bridge function}, that is, the one that minimises the mean--square--error, is,
\begin{equation}
    \label{eq:optimal_bridge_function}
    \alpha(\theta) = \frac{C}{s_{1}p(d\vert \theta, \mathcal{M})p(\theta\vert \mathcal{M}) + s_{2}p(d\vert \mathcal{M})q(\theta)}\,,
\end{equation}
where $s_{1}=n_{1}/(n_{1}+n_{2})$ and $s_{2}=n_{2}/(n_{1}+n_{2})$ and $C$ is a constant that cancels out and its value does not affect the outcome in any way. The \textit{bridge function} of equation \ref{eq:optimal_bridge_function} depends on the model evidence $p(d\vert\mathcal{M})$, the same quantity that we are trying to approximate. We can resolve this issue by employing an iterative scheme,
\begin{equation}
    \label{eq:bridge_sampling_update}
    \Hat{p}_{\mathrm{BS}}^{(t+1)}(d\vert \mathcal{M}) = \frac{\frac{1}{n_{2}}\sum_{i=1}^{n_{2}}\frac{p(d\vert\Tilde{\theta}_{i},\mathcal{M})p(\Tilde{\theta}_{i}\vert\mathcal{M})}{s_{1}p(d\vert\Tilde{\theta}_{i},\mathcal{M})p(\Tilde{\theta}_{i}\vert\mathcal{M}) + s_{2}\Hat{p}_{\mathrm{BS}}^{(t)}(d\vert \mathcal{M})q(\Tilde{\theta}_{i})}}{\frac{1}{n_{1}}\sum_{j=1}^{n_{1}}\frac{q(\theta_{j}^{*})}{s_{1}p(d\vert \theta_{j}^{*}, \mathcal{M})p(\theta_{j}^{*}\vert \mathcal{M}) + s_{2}\Hat{p}_{\mathrm{BS}}^{(t)}(d\vert \mathcal{M})q(\theta_{j}^{*})}}\,,
\end{equation}
starting from some initial guess of the value of the model evidence $\Hat{p}_{\mathrm{BS}}^{(0)}(d\vert \mathcal{M})$ and keep updating it until the estimate has converged for some arbitrary tolerance level. Rearranging the terms on the right hand side, the aforementioned estimator can be written in the simpler form
\begin{equation}
    \label{eq:bridge_sampling_update_simple}
    \Hat{p}_{\mathrm{BS}}^{(t+1)}(d\vert \mathcal{M}) = \frac{\frac{1}{n_{2}}\sum_{i=1}^{n_{2}}\frac{\ell_{2,i}}{s_{1}\ell_{2,i}+s_{2}\Hat{p}_{\mathrm{BS}}^{(t)}(d\vert \mathcal{M})}}{\frac{1}{n_{1}}\sum_{j=1}^{n_{1}}\frac{1}{s_{1}\ell_{1,j}+s_{2}\Hat{p}_{\mathrm{BS}}^{(t)}(d\vert \mathcal{M})}}\,,
\end{equation}
where we have defined
\begin{equation}
    \label{eq:bridge_sampling_l1}
    \ell_{1,j} = \frac{p(d\vert \theta_{j}^{*}, \mathcal{M})p(\theta_{j}^{*}\vert \mathcal{M})}{q(\theta_{j}^{*})}\,,
\end{equation}
and
\begin{equation}
    \label{eq:bridge_sampling_l2}
    \ell_{2,i} = \frac{p(d\vert\Tilde{\theta}_{i},\mathcal{M})p(\Tilde{\theta}_{i}\vert\mathcal{M})}{q(\Tilde{\theta}_{i})}\,.
\end{equation}
Furthermore, the numerical stability of equation \ref{eq:bridge_sampling_update_simple} can be improved and overflow issues avoided if we define
\begin{equation}
    \label{eq:bridge_sampling_r}
    \Hat{p}_{\mathrm{BS}}^{(t)}(d\vert \mathcal{M}) = \Hat{r}^{(t)}\exp(\ell^{*})\,,
\end{equation}
and use the iterative formula
\begin{equation}
    \label{eq:bridge_sampling_update_stable}
    \Hat{r}^{(t+1)} = \frac{\frac{1}{n_{2}}\sum_{i=1}^{n_{2}}\frac{\exp[\log(\ell_{2,i})-\ell^{*}]}{s_{1}\exp[\log(\ell_{2,i})-\ell^{*}]+s_{2}\Hat{r}^{(t)}}}{\frac{1}{n_{1}}\sum_{j=1}^{n_{1}}\frac{1}{s_{1}\exp[\log(\ell_{1,j})-\ell^{*}]+s_{2}\Hat{r}^{(t)}}},
\end{equation}
where $\ell^{*}$ is a constant that we can choose in order to make the sums numerically tractable, for instance $\ell^{*} = \mathrm{median}[\log(\ell_{1,j})]$.

Compared to other methods such as importance sampling or the harmonic mean estimator, BS estimates are more robust in cases in which the overlap between the \textit{typical sets} of the proposal and posterior distribution is far from perfect.

\section{Thermodynamic integration}

A large body of work in \textit{statistical physics} is concerned with methods for the estimation of normalising constants and \textit{partition functions} in particular. The method of \textit{thermodynamic integration (TI)} was developed for exactly this purpose~\parencite{gelman1998simulating}. \textcite{friel2008marginal} studied the particular case in which the normalising constant that is estimated using TI is the model evidence. To this end, they introduced the notion of the \textit{power posterior},
\begin{equation}
    \label{eq:power_posterior}
    p(\theta\vert d,\beta, \mathcal{M})\propto p^{\beta}(d\vert\theta,\mathcal{M})p(\theta\vert \mathcal{M})\,,
\end{equation}
in which $\beta$ is an auxiliary variable in the interval $[0,1]$. By construction, the normalising constant of the \textit{power posterior} is simply,
\begin{equation}
    \label{eq:power_posterior_normalisation}
    p(d\vert \beta, \mathcal{M}) = \int p^{\beta}(d\vert\theta,\mathcal{M})p(\theta\vert \mathcal{M}) d\theta \,,
\end{equation}
where $p(d\vert \beta=1,\mathcal{M})$ is the model evidence and $p(d\vert \beta=0,\mathcal{M})$ is the integral over the prior which is simply equal to $1$. Furthermore, the logarithm of the model evidence is,
\begin{equation}
    \label{eq:thermodynamic_integration_logz}
    \begin{split}
        \log p(d\vert \mathcal{M}) &= \log\left[ \frac{p(d\vert \beta=1,\mathcal{M})}{p(d\vert \beta=0,\mathcal{M})}\right] \\
        &= \int_{0}^{1}\mathbb{E}_{p(\theta\vert d, \beta, \mathcal{M})}[\log p(d\vert\theta,\beta,\mathcal{M})]d\beta \,,
    \end{split}
\end{equation}
that is, the integral over $\beta$ of the expectation value of the likelihood with respect to the posterior for each value of $\beta$. To prove the above identity we first need to notice that,
\begin{equation}
    \label{eq:thermodynamic_integration_proof}
    \begin{split}
        \frac{d\log p(d\vert\beta,\mathcal{M})}{d\beta} &= \frac{1}{p(d\vert\beta,\mathcal{M})} \times\frac{d p(d\vert\beta,\mathcal{M})}{d\beta} \\
        &= \frac{1}{p(d\vert\beta,\mathcal{M})} \int \frac{d}{d\beta} p^{\beta}(d\vert\theta,\mathcal{M})p(\theta\vert \mathcal{M}) d\theta \\
        &= \int \log p(d\vert\theta,\mathcal{M}) \frac{p^{\beta}(d\vert\theta,\mathcal{M})p(\theta\vert \mathcal{M})}{p(d\vert\beta,\mathcal{M})}d\theta \\
        &= \int \log p(d\vert\theta,\mathcal{M}) p(\theta\vert d,\beta,\mathcal{M})d\theta \\
        &= \mathbb{E}_{p(\theta\vert d,\beta,\mathcal{M})}\left[ \log p(d\vert\theta,\mathcal{M})\right]\,,
    \end{split}
\end{equation}
Integrating both sides with respect to $\beta$ leads to equation \ref{eq:thermodynamic_integration_logz} and completes the proof.

Using equation \ref{eq:thermodynamic_integration_logz} to estimate the model evidence often requires the discretisation of the integral. A sequence $0=\beta_{1}<\beta_{2}<\dots<\beta_{m}=1$ must be chosen \textit{a priori} or based on \textit{diminishing adaptation} scheme. The model evidence can then be approximated using the \textit{trapezoidal rule},
\begin{equation}
    \label{eq:thermodynamic_integration_trapezoidal}
    \Hat{p}_{\mathrm{TI}}(d\vert \mathcal{M}) = \sum_{i=1}^{m}(\beta_{i+1}-\beta_{i})\frac{E_{i}+E_{i+1}}{2}\,,
\end{equation}
where,
\begin{equation}
    \label{eq:thermodynamic_integration_average_likelihood}
    E_{i}=\mathbb{E}_{p(\theta\vert d,\beta_{i},\mathcal{M})}\left[ \log p(d\vert\theta,\mathcal{M})\right]\,,
\end{equation}
is the expected likelihood at $\beta_{i}$.

There are two sources of error in the above approximation. The first one is the Monte Carlo error that originates from the estimation of equation \ref{eq:thermodynamic_integration_average_likelihood} using a finite number of samples. The second type has to do with the choice of the discretisation of $\beta$. \textcite{calderhead2009estimating} showed that the discretisation error depends on the \textit{Kullback--Leibler (KL) divergence} between subsequent densities $p(\theta\vert d, \beta_{i},\mathcal{M})$ and $p(\theta\vert d, \beta_{i+1},\mathcal{M})$. This means that the optimal discretisation sequence of $\beta$ values is the one minimising the KL   divergence between subsequent power posteriors. Of course, knowing the optimal scheme is \textit{a priori} hardly ever possible and thus we must rely on \textit{ad hoc} choices (e.g. $\beta_{i}=(i-1)^{3}/(m-1)^{3}$) or \textit{diminishing adaptation} strategies.

Thermodynamic integration can be combined with many different MCMC methods in order to estimate the model evidence. Perhaps the simplest one is to run $m$ independent chains, either in parallel or serially, and then estimate the evidence using equation \ref{eq:thermodynamic_integration_trapezoidal} where the expected likelihood of each discrete $\beta$ value is computed with equation \ref{eq:thermodynamic_integration_average_likelihood} for each chain. Furthermore, the chains do not even have to be independent for this method to work. Lastly, a parallel tempering approach can be followed as it is often done in practice.

\section{Annealed importance sampling}

\textit{Annealed importance sampling (AIS)} is another method that relies on a sequence of annealed or tempered distributions in order to construct an importance sampling estimator for the model evidence~\parencite{neal2001annealed}, similarly to \textit{simulated annealing} and \textit{parallel tempering}.

The basic idea is to use MCMC transitions in order to push a collection of $n$ particles through a series of $m$ \textit{intermediate distributions}
\begin{equation}
    \label{eq:annealed_posterior}
    p(\theta\vert d,\beta, \mathcal{M})\propto p^{\beta}(d\vert\theta,\mathcal{M})p(\theta\vert \mathcal{M})\,,
\end{equation}
where $0=\beta_{1}<\beta_{2}<\dots<\beta_{m}=1$, connecting the prior for $\beta_{1}=0$ to the posterior for $\beta_{m}=1$. The particles are initialised by drawing samples from the prior
\begin{equation}
    \label{eq:sample_prior}
    \theta_{i}^{(1)}\sim p(\theta\vert\mathcal{M})\,,
\end{equation}
and assigned (unnormalised) importance weights
\begin{equation}
    \label{eq:initial_importance_weights}
    w_{i}^{(1)} = 1\,,
\end{equation}
for $i\in\lbrace 1, \dots, n\rbrace$ the particle index.

A number of $N$ MCMC steps is then performed for each particle before the value of $\beta$ is updated to the next value in the predefined sequence. The number $N$ of MCMC steps is chosen such that the Markov chains defined by the particle trajectories have enough time to reach the stationary distribution.
The critical difference between AIS and simulated annealing is that the associated importance weights $w_{i}$ are updated during the run every time we move from one intermediate distribution to the next,
\begin{equation}
    \label{eq:importance_weight_update}
    w_{i}^{(t+1)} = w_{i}^{(t)} \times \frac{p(\theta_{i}\vert d, \beta_{t+1}, \mathcal{M})}{p(\theta_{i}\vert d, \beta_{t}, \mathcal{M})} = w_{i}^{(t)} \times \frac{p^{\beta_{t+1}}(d\vert\theta_{i},\mathcal{M})}{p^{\beta_{t}}(d\vert\theta_{i},\mathcal{M})}\,.
\end{equation}
In practice, the logarithm of the weights is used in order to avoid numerical issues. Once the final distribution (i.e. the posterior) is reached and the particle weights are updated accordingly, the model evidence can be estimated as
\begin{equation}
    \label{eq:ais_estimator}
    \Hat{p}_{\mathrm{AIS}}(d\vert\mathcal{M}) = \frac{1}{n} \sum_{i=1}^{n}w_{i}^{(m)}\,.
\end{equation}
Furthermore, the samples $\theta_{i}^{(m)}$ combined with their respective weights $w_{i}^{(m)}$ can be used to compute arbitrary expectation values
\begin{equation}
    \label{eq:ais_expectation_values}
    \mathbb{E}_{p(\theta\vert d,\mathcal{M})}\left[f(\theta)\right] = \frac{\sum_{i=1}^{n}w_{i}^{(m)}f\left(\theta_{i}^{(m)}\right)}{\sum_{i=1}^{n}w_{i}^{(m)}}\,.
\end{equation}

Assuming that the annealing process is slow enough (i.e. large number $m$ of $\beta$ levels and number $N$ of MCMC steps) and a large enough collection of particles is used, then AIS yields unbiased estimates of the model evidence and weighted posterior samples, even in high dimensions. In the limit that the number of MCMC steps $N$ goes to zero, the AIS estimator reduces to the usual importance sampling estimator.

\section{Savage--Dickey density ratio}

Suppose now that we have two models or hypotheses and their respective parameters, $\mathcal{M}_{0}:\theta$ and $\mathcal{M}_{1}:\theta,\phi$, such that $\mathcal{M}_{0}$ is nested inside $\mathcal{M}_{1}$. This means that the more complex model, $\mathcal{M}_{1}$, is reduced to the simpler one, $\mathcal{M}_{0}$, for some specific choice of one or more of its parameters, $\phi=\phi_{0}$. This specific parameter choice is often called a \textit{point--null hypothesis} as it is associated with zero probability mass in the context of the $\mathcal{M}_{1}$ model.

Evaluating the plausibility of this hypothesis can be done by computing the \textit{Bayes factor} between the two models. The \textit{Savage--Dickey density ratio (SDDR)} is a method that aims to do exactly this that was introduced by \textcite{dickey1970weighted}, \textcite{dickey1971weighted}, \textcite{gunel1974bayes}, and \textcite{dickey1976approximate} who in turn attributed the origin of the method to \textit{Leonard Jimmie Savage}.

Although the SDDR can be only applied to nested models, it has the advantage that it is simple to compute, given some posterior samples, without making any assumptions about the Gaussianity of the posterior distribution. In particular, the \textit{Bayes factor} of $\mathcal{M}_{0}$ over $\mathcal{M}_{1}$ is simply,
\begin{equation}
    \label{eq:savage_dickey_bayes_factor}
    \mathrm{BF}_{01} = \frac{p(d\vert\mathcal{M}_{0})}{p(d\vert\mathcal{M}_{1})} = \frac{p(\phi=\phi_{0}\vert d,\mathcal{M}_{1})}{p(\phi=\phi_{0}\vert\mathcal{M}_{1})}\,,
\end{equation}
where the numerator of the right--hand--side ratio is just the \textit{marginal} posterior of $\phi$ for $\mathcal{M}_{1}$ evaluated at $\phi=\phi_{0}$, and the denominator is the prior of $\phi$ for $\mathcal{M}_{1}$ evaluated at $\phi=\phi_{0}$. In other words, the \textit{Bayes factor} is simply the marginal posterior to prior ratio for $\mathcal{M}_{1}$ evaluated at $\phi=\phi_{0}$. This means that only the parameters $\phi$ determine the value of the \textit{Bayes factor}, and the nuisance parameters $\theta$, that are common among the two models, are irrelevant. A schematic representation of SDDR is depicted in Figure \ref{fig:savage_dickey}.
\begin{figure}[H]
    \centering
	\centerline{\includegraphics[scale=0.65]{Graphics/savage_dickey.pdf}}
    \caption{The \textit{Savage--Dickey density ratio} expresses the Bayes factor $\mathrm{BF}_{01}$ as the ratio of marginal posterior to the prior density at the point $\phi_{0}$ in which model $M_{1}$ reduces to $M_{0}$.}
    \label{fig:savage_dickey}
\end{figure}

The validity of this method relies on two conditions. First, that the likelihood function of $\mathcal{M}_{1}$ has to reduce to that of $\mathcal{M}_{0}$
\begin{equation}
    \label{eq:savage_dickey_likelihood_condition}
    p(d\vert \theta, \phi=\phi_{0}, \mathcal{M}_{1}) = p(d\vert \theta, \mathcal{M}_{0})\,,
\end{equation}
and the same must be true for the prior
\begin{equation}
    \label{eq:savage_dickey_prior_condition}
    p(\theta\vert \phi=\phi_{0},\mathcal{M}_{1}) = p(\theta\vert \mathcal{M}_{0})\,.
\end{equation}
The condition of equation \ref{eq:savage_dickey_prior_condition} is satisfied by separable priors,
\begin{equation}
    \label{eq:savage_dickey_separable_priors}
    p(\theta, \phi\vert \mathcal{M}_{1}) = p(\theta\vert \mathcal{M}_{1}) p(\phi\vert \mathcal{M}_{1})\,.
\end{equation}
The proof of equation \ref{eq:savage_dickey_bayes_factor} is straightforward, starting with,
\begin{equation}
    \label{eq:evidence_0}
    \begin{split}
        p(d\vert \mathcal{M}_{0}) &= \int p(d\vert \theta, \mathcal{M}_{0}) p(\theta \vert \mathcal{M}_{0}) d\theta \\
        &= \int p(d\vert \theta, \phi=\phi_{0}, \mathcal{M}_{1}) p(\theta \vert\phi=\phi_{0} \mathcal{M}_{1}) d\theta \\
        &= p(d\vert \phi=\phi_{0} \mathcal{M}_{1})\,,
    \end{split}
\end{equation}
in which we used the fact that $\mathcal{M}_{0}$ is nested in $\mathcal{M}_{1}$ for $\phi=\phi_{0}$. The next step is simply to employ Bayes' theorem
\begin{equation}
    \label{eq:bayes_theorem_savage_dickey}
    p(\phi=\phi_{0}\vert d,\mathcal{M}_{1}) = \frac{p(d\vert \phi=\phi_{0} \mathcal{M}_{1})p(\phi=\phi_{0}\vert \mathcal{M}_{1})}{p(d\vert \mathcal{M}_{1})}\,,
\end{equation}
and solve for the ratio of model evidences by first substituting equation \ref{eq:evidence_0} into it to compete the proof.

Practical use of equation \ref{eq:savage_dickey_bayes_factor} requires the evaluation the marginal posterior of $\mathcal{M}_{1}$ at $\phi=\phi_{0}$. As the closed--form expression for the marginal posterior is rarely available, one can use samples from posterior (e.g. generated using MCMC) to create a density histogram for $\phi$. Even better, \textit{Kernel Density Estimation (KDE)}~\parencite{silverman2018density} can be used to approximate the marginal posterior from samples as,
\begin{equation}
    \label{eq:savage_dickey_kde}
    \Hat{p}_{\mathrm{KDE}}(\phi\vert d, \mathcal{M}_{1}) = \frac{1}{n} \sum_{i=1}^{n}K_{h}\left(\phi-\phi_{i}\right)\,,
\end{equation}
where $K_{h}$ is the \textit{kernel} and $h$ is the \textit{bandwidth}, a parameter that controls the smoothing. The \textit{kernel} is generally a non--negative function, and most commonly it is chosen to be a simple Gaussian,
\begin{equation}
    \label{eq:gaussian_kernel}
    K_{h}(\phi)=\frac{1}{(2\pi)^{D/2}h^{D}} e^{-\frac{\phi^{2}}{2 h^{2}}}\,,
\end{equation}
where $D$ is the dimensionality of $\phi$ (i.e. the number of elements of the $\phi$ vector). Finally, the value of $h$ can either be determined on the basis of trial--and--error, or heuristics such as,
\begin{equation}
    \label{eq:silverman_kde}
    h = 1.06 \Hat{\sigma} n^{-1/5}\,,
\end{equation}
for the $1$--D case where $\Hat{\sigma}$ is the standard deviation of the samples~\parencite{silverman2018density}.