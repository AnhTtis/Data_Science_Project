% !TEX TS-program = pdflatex
% !TEX root = ../ArsClassica.tex

%************************************************
\chapter{Auxiliary variable MCMC methods}
\label{chp:auxiliary}
%************************************************

\begin{flushright}
\itshape
Natura non facit saltus. \\
\medskip
--- Gottfried Leibniz
\end{flushright}

Auxiliary variable MCMC methods rely on the introduction of one or more additional variables in order to make sampling from the target distribution more efficient.

\section{Simulated annealing}

In metallurgy, \textit{annealing} refers to the thermal process used to harden steel. Initially, the metal is heated to a high temperature and then it is cooled down slowly enough for the atoms to \textit{self--arrange} in an ordered pattern that corresponds to the minimum energy~\parencite{cahn1996physical}. The slow rate of cooling ensures that the energy of the system will reach its global minimum instead of getting trapped in local minima.

Realising that the \textit{Metropolis--Hastings} method can be used to simulate the process of gradually cooling a solid towards a low--temperature equilibrium state, \textcite{kirkpatrick1983optimization} suggested that we should construct a sequence of \textit{Boltzmann} distributions,
\begin{equation}
    \label{eq:boltzmann_distribution}
    p_{i}(\theta) \propto e^{-\frac{H(\theta)}{T_{i}}}\,,
\end{equation}
for a series of temperatures $T_{1}>T_{2}>\dots>T_{m}$ and simulate from each one in succession by performing a number of MCMC steps in each temperature before moving on to the next. The result is a non--homogeneous Markov chain, that is, a Markov chain with time--varying target density. Assuming that $T_{1}$ is high enough and $T_{m}\approx 0$ we can find the global minimum of the energy $E(\theta)$ by simulating the cooling process of a solid.

\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{annealed_posterior.pdf}}
    \caption{Illustration of the gradual annealing performed in the posterior distribution. The prior distribution corresponds to $1/T\rightarrow 0$ and the posterior is recovered as $1/T\rightarrow 1$.}
    \label{fig:annealing}
\end{figure}

\textit{Simulated annealing} can be used for sampling too, not just for optimisation. By stopping the cooling process earlier at $T_{m}=1$ we can sample from any target distribution $p(\theta)$, not just \textit{Boltzmann} distributions, simply setting $H(\theta)=-\log p(\theta)$.  Furthermore, for applications in which the target distribution is the posterior distribution we can construct the following sequence of densities,
\begin{equation}
    \label{eq:boltzmann_distribution_bayesian}
    p_{i}(\theta) \propto p(\theta) e^{\frac{\log p(d\vert\theta)}{T_{i}}}\,,
\end{equation}
where $T_{1}>T_{2}>\dots>T_{m} = 1$. In this case, for $T_{1} >> 1$ we effectively sample from the prior distribution,
\begin{equation}
    \label{eq:boltzmann_prior}
    p_{i}(\theta) \propto p(\theta)\,,
\end{equation}
whereas in the limit that $T_{m} = 1$ we acquire samples from the posterior,
\begin{equation}
    \label{eq:boltzmann_prior_2}
    p_{i}(\theta) \propto p(\theta)p(d\vert\theta)\,.
\end{equation}
The number of MCMC steps to perform in each temperature, before moving on to the next one, is arbitrary and different mixing criteria can be utilised (e.g. Gelman--Rubin, autocorrelation thresholds, etc.). The benefit of using simulated annealing for sampling is that by simulating multiple Markov chains, possibly in parallel, through this sequence of densities, the risk of the chains getting trapped in isolated modes of the posterior distribution is minimised. This means that this approach can be used when the probability distribution is strongly multimodal. Furthermore, if the number of temperature levels is large enough and the spacing between them small enough, then the Markov chain is approximately always in equilibrium, meaning that no, or minor, burn--in is required to be discarded.

\begin{algorithm}[ht!]
\caption{Simulated annealing} \algolabel{sa}
\begin{algorithmic}[1]
\REQUIRE{initial state $\theta_{1}^{(1)}$, temperature schedule $T_{1}>T_{2}>\dots>T_{m}=1$, prior density $\pi(\theta)\equiv p(\theta\vert\mathcal{M})$, likelihood function $\mathcal{L}(\theta)\equiv p(d\vert\theta,\mathcal{M})$, and number of MCMC iterations $N$ per temperature}
\ENSURE{Multiple Markov chains $\theta^{(1)}, \theta^{(2)}, \dots, \theta^{(m)}$ with the last one having $p(\theta)$ as its equilibrium distribution}
\FOR{$i=1$ \TO $m$}
    \STATE{Set annealed density $p_{i}(\theta) \propto \pi(\theta)\mathcal{L}(\theta)^{1/T_{i}}$}
    \STATE{Generate Markov chain $\theta_{1}^{(i)},\dots,\theta_{N}^{(i)}$ targeting $p_{i}(\theta)$ (e.g. using Metropolis--Hastings)}
    \STATE{Set last state as the first state for the next annealed density $\theta_{1}^{(i+1)}\leftarrow \theta_{N}^{(i)}$}
\ENDFOR
\end{algorithmic}
\end{algorithm}

%************************************************

\section{Slice sampling}

Slice sampling is another MCMC method that relies on an auxiliary variable in order to make sampling easier~\parencite{besag1993spatial, neal1997markov, neal2003slice}. The method is based on the realisation that sampling from the target distribution with density $p(\theta)$ is equivalent to uniform sampling from the area or volume below the curve or surface of $f(\theta)\propto p(\theta)$. This is equivalent to the introduction of an auxiliary variable $\phi$, called \textit{height}, such that the joint distribution $p(\theta, \phi)$ is uniform over the region,
\begin{equation}
    \label{eq:uniform_region}
    U = \left\lbrace (\theta, \phi)\,:\,0 < \phi < f(\theta) \right\rbrace\,.
\end{equation}
In other words, the joint distribution can be written as,
\begin{equation}
    \label{eq:joint_distribution}
    p(\theta,\phi) = 
    \begin{cases}
    1/\mathcal{Z} & \text{if } 0 < \phi < f(\theta)\,, \\
    0 & \text{otherwise}\,,
    \end{cases}
\end{equation}
where
\begin{equation}
    \label{eq:normalisation_constant}
    \mathcal{Z} = \int f(\theta) d\theta\,.
\end{equation}
To sample from the target distribution $p(\theta)$ we first sample uniformly from $p(\theta, \phi)$ and then marginalise over $\phi$ by dropping the $\phi$--value of each sample and keeping the $\theta$--value. The proof that this results in the marginal density for $\theta$ is straightforward,
\begin{equation}
    \label{eq:marginal_density}
    p(\theta) = \int p(\theta, \phi) d\phi = \int_{0}^{f(\theta)} \frac{1}{\mathcal{Z}} d\phi = \frac{f(\theta)}{\mathcal{Z}}\,.
\end{equation}

Generating independent samples from the uniform joint density $p(\theta, \phi)$ is rarely possible in practice. Instead, one might prefer to construct a Markov chain that leaves the distribution $p(\theta, \phi)$ invariant. One such option is to use \textit{Gibbs sampling}, that is, to sample alternately from the conditional distribution $p(\phi\vert\theta)$, which is uniform over the interval $(0, f(\theta))$, and then from the conditional distribution $p(\theta\vert\phi)$, which is uniform over the region,
\begin{equation}
    \label{eq:slice_definition}
    S = \left\lbrace \theta\,:\,\phi < f(\theta)\right\rbrace\,,
\end{equation}
called the \textit{slice}. Applying this procedure repeatedly will produce a Markov chain that has the joint distribution $p(\theta, \phi)$ as its stationary distribution.

Sampling uniformly from the aforementioned slice is not trivial either. However, the fact that the conditional density $p(\theta\vert\phi)$ is uniform allows us to construct procedures to sample from it which would otherwise would not have worked. \textcite{neal2003slice} proposed the following sequence of steps for univariate probability distributions,
\begin{enumerate}
    \item Uniformly sample a real value $\phi$ in the interval $(0, f(\theta_{0}))$, therefore defining the horizontal slice $S = \left\lbrace \theta\,:\,\phi < f(\theta_{0})\right\rbrace$ that always includes $\theta_{0}$,
    \item Find an interval $I=(L,R)$ around $\theta_{0}$ along the slice that contains all, or much of, the slice,
    \item Sample a new value $\theta_{1}$ from the part of the slice within the interval, that is, from $I\cap S$.
\end{enumerate}
It is important to mention that as we often work with $g(\theta) = \log f(\theta)$, to avoid numerical issues, one can use the variable $\psi = \log(\phi) = g(\theta_{0}) - e$, where $e$ is exponentially distributed with mean one, to define the slice as $S = \lbrace \theta\,:\,\psi<g(\theta_{0}) \rbrace$.

\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{slice_sampling.pdf}}
    \caption{Illustration of the stepping--out and shrinking procedures used in slice sampling. Given an initial state $\theta$ in the Markov chain, an auxiliary variable $\phi$ is sampled corresponding to the height thus defining the extended state $(\theta, \phi)$ shown here as a \textit{blue} point. An interval of a certain width is placed uniformly around the current point $(\theta, \phi)$ and expanded in steps of size equal to the initial width until both of its ends, $L$ and $R$, are outside the graph. A new state, shown in \textit{red}, is then proposed uniformly along the interval $(L,R)$. Since the proposed state lies above the graph of $f(\theta)$ (i.e. not in the slice shown as a continuous line) it is rejected. A new state, shown in \textit{green}, is the proposed uniformly between the rejected state and $R$. Since the proposed state is below the graph, and thus in the slice, it is accepted and added to the Markov chain. The whole process is then repeated.}
    \label{fig:slice_sampling}
\end{figure}

The first step in the above procedure is trivial, yet steps two and three require more serious consideration. \textcite{neal2003slice} suggested to use the so--called \textit{stepping--out} and \textit{shrinking} procedures for those steps respectively. \textit{Stepping--out} works by uniformly positioning an interval of width $w$ around $\theta_{0}$ such that it includes $\theta_{0}$, and then expanding the interval in steps of size $w$ until both its ends $(L, R)$ are outside the slice $S$. This effectively constructs the interval $I=(L, R)$. It is worth noting that the algorithm is valid even if only a pre--specified number of expansions take place and the interval ends up not covering the entirety of the slice $S$. The \textit{shrinking} procedure that follows functions by uniformly sampling points in the interval $I=(L, R)$ until one of them lies in the slice $S$. Every time a point is rejected, being outside of the slice, the interval $I$ shrinks such that the rejected point now defines one of its two boundaries, determined by whether the rejected point lies left or right of $\theta_{0}$.

The fact that the three--step procedure presented so far describes a slice sampling update from a univariate probability distribution $p(\theta)$ does not prohibit its use in multivariate cases. In particular, there are many ways the aforementioned recipe can be generalised and used in target distribution with more than one parameter. Perhaps the simplest one is to apply this univariate scheme along each coordinate axis in turn, updating one parameter at a time. This corresponds to a \textit{Metropolis--within--Gibbs} scheme. Another option is to apply 1--D updates in random directions. This is more general than the previous one, and there is freedom to choose the distribution of the random directions. The directions can be drawn from a multivariate zero--mean normal distribution with unit--diagonal covariance matrix or a more appropriate non--diagonal covariance matrix that encodes some of the correlations of the parameters of the target distribution. Such a covariance can be configured \textit{a priori}, estimated during a short preliminary run from samples from the target, or adaptively tuned using an appropriate algorithm for \textit{diminishing adaptation}.

One of the great benefits of slice sampling is the fact that it has a single hyper--parameter, the initial width $w$ of the interval $I$. Furthermore, the value of $w$ is adapted continuously by the \textit{stepping--out} and \textit{shrinking} procedures. This sort of \textit{local adaptation} is absent from many MCMC that assume a global proposal scale. Another characteristic of slice sampling is the lack of rejected samples in the Markov chain. Unlike methods that include a Metropolis acceptance criterion, slice sampling always moves to a new state in every iteration.

\begin{algorithm}[ht!]
\caption{Slice sampling} \algolabel{ss}
\begin{algorithmic}[1]
\REQUIRE{initial state $\theta_{1}$, (unnormalised) target density $f(\theta)\propto p(\theta)$, number of maximum expansions $m$, and number of iterations $N$}
\ENSURE{Markov chain $\theta_{1}, \dots, \theta_{N}$ that has $p(\theta)$ as its equilibrium distribution}
\STATE{Draw ``height'' auxiliary variable $\phi \sim \mathcal{U}(0,f(\theta_{1}))$}
\FOR{$t=1$ \TO $N$}
    \STATE{Draw left bound of the $I$ interval $L\sim\mathcal{U}(\theta_{t}-w,\theta_{t})$}
    \STATE{Set right bound $R\leftarrow L + w$}
    \STATE{Draw uniform variable $u\sim\mathcal{U}(0,1)$}
    \STATE{Set maximum number of interval expansions to the left $J\leftarrow \text{Floor}(m\times u)$}
    \STATE{Set maximum number of interval expansions to the right $K\leftarrow (m-1)-J$}
    \WHILE{$J>0$ \AND $\phi<f(L)$}
        \STATE{Expand left boundary $L\leftarrow L - w$ in steps of $w$}
        \STATE{Reduce count $J \leftarrow J - 1$ by one}
    \ENDWHILE
    \WHILE{$K>0$ \AND $\phi<f(R)$}
        \STATE{Expand right boundary $L\leftarrow R + w$ in steps of $w$}
        \STATE{Reduce count $K \leftarrow K - 1$ by one}
    \ENDWHILE
    \REPEAT
        \STATE{Draw state within interval $\theta'\sim \mathcal{U}(L,R)$}
        \IF{$\theta'<\theta_{t}$}
            \STATE{Contract interval $L\leftarrow \theta'$}
        \ELSE
            \STATE{Contract interval $R\leftarrow \theta'$}
        \ENDIF
    \UNTIL{$\phi < f(\theta')$}
    \STATE{Accept new state $\theta_{t+1}\leftarrow \theta'$}
\ENDFOR
\end{algorithmic}
\end{algorithm}

%************************************************

\section{Hamiltonian Monte Carlo}

\textit{Hamiltonian Monte Carlo (HMC)} introduces a \textit{momentum} auxiliary variable and uses the gradient of the target probability density to efficiently explore the typical set. HMC turns the problem of sampling from the target distribution into the approximate simulation of Hamiltonian dynamics with a subsequent \textit{Metropolis} correction step~\parencite{neal2011mcmc}. In the statistical physics literature HMC was suggested as a method of efficiently simulating states from a physical system~\parencite{duane1987hybrid}, which was then employed to statistical inference problems~\parencite{neal1992bayesian, neal1993probabilistic, neal1996, liu2001monte}.

% https://mc-stan.org/docs/2_29/reference-manual/hamiltonian-monte-carlo.html

\begin{figure}[ht!]
    \centering
	\centerline{\includegraphics[scale=0.65]{hamiltonian.pdf}}
    \caption{Illustration of Hamiltonian trajectories in parameter space. The \textit{black} points correspond to the accepted states.}
    \label{fig:hamiltonian}
\end{figure}

\subsection{Auxiliary momentum variable}
HMC introduces an auxiliary variable $\rho$ and samples from the joint probability density,
\begin{equation}
    \label{eq:hmc_joint_distribution}
    p(\rho, \theta) = p(\rho\vert\theta)p(\theta)\,.
\end{equation}
In most applications of HMC, the momentum variable $\rho$ chosen to be Gaussian--distributed,
\begin{equation}
    \label{eq:hmc_momentum}
    \rho \sim \mathcal{N}(0,M)\,,
\end{equation}
and its probability density function to be independent of the state variable $\theta$ (i.e. $p(\rho\vert\theta)=p(\theta)$). $M$ is the symmetric, \textit{positive definite} \textit{mass matrix} that has the role of the \textit{Euclidean metric}, that is to define the relative \textit{length scales} between parameters. In practice, $M$ can be chosen to be $\Sigma^{-1}$, meaning the inverse of the sample covariance matrix that characterises the target distribution assuming that it is known or easy to estimate.

\subsection{The Hamiltonian}
HMC treats sampling from the joint distribution $p(\rho,\theta)$ as a problem of solving the \textit{Hamiltonian dynamics} given the \textit{Hamiltonian},
\begin{equation}
    \label{eq:hamiltonian}
    \begin{split}
        \mathcal{H}(\rho, \theta) &= - \log p(\rho, \theta) \\
         &= -\log p(\rho\vert\theta) - \log p(\theta) \\
         &= T(\rho\vert\theta) + V(\theta) \,,
    \end{split}
\end{equation}
where
\begin{equation}
    \label{eq:kinetic_energy}
    T(\rho\vert\theta) = -\log p(\rho\vert\theta)\,,
\end{equation}
is the \textit{kinetic energy}, and,
\begin{equation}
    \label{eq:potential_energy}
    V(\theta) = - \log p(\theta)\,,
\end{equation}
is the \textit{potential energy}.

\subsection{Hamilton's equations}
The dynamics of a system (i.e. its evolution in time) that is characterised by the \textit{Hamiltonian} of  equation \ref{eq:hamiltonian} are given by solving \textit{Hamilton's equations},
\begin{equation}
    \label{eq:hamiltons_equations}
    \begin{split}
        \frac{d\theta}{dt} &= \frac{\partial\mathcal{H}}{\partial\rho} = \frac{\partial T}{\partial\rho}\,,\\
        \frac{d\rho}{dt} &= -\frac{\partial\mathcal{H}}{\partial\theta} = -\frac{\partial T}{\partial\theta}-\frac{\partial V}{\partial\theta}\,,
    \end{split}
\end{equation}
or, in the case that the momentum variable $\rho$ is independent of the state variable $\theta$, that is $p(\rho\vert\theta)=p(\rho)$,
\begin{equation}
    \label{eq:hamiltons_equations_2}
    \begin{split}
        \frac{d\theta}{dt} &= \frac{\partial\mathcal{H}}{\partial\rho} = \frac{\partial T}{\partial\rho}\,,\\
        \frac{d\rho}{dt} &= -\frac{\partial\mathcal{H}}{\partial\theta} = -\frac{\partial V}{\partial\theta}\,.
    \end{split}
\end{equation}
Therefore, given an initial state $(\rho, \theta)$, the system's evolution in time is completely determined by equations \ref{eq:hamiltons_equations_2}.

\subsection{Leapfrog integration}
Solving \textit{Hamilton's equations} analytically is only feasible for very simple systems that correspond to simple target probability distributions. In practice, however, we aim to solve equations \ref{eq:hamiltons_equations_2} for systems of arbitrary complexity. To this end, we turn to numerical methods for integrating this system of differential equations.

The most commonly used numerical method is the \textit{leapfrog integration algorithm}~\parencite{leimkuhler2004simulating} that begins by sampling a value for the momentum variable $\rho$ according to equation \ref{eq:hmc_momentum} and then proceeds by applying $L$ times the following steps,
\begin{equation}
    \label{eq:leapfrog_integration}
    \begin{split}
        \rho &\leftarrow \rho - \frac{\epsilon}{2} \frac{\partial V}{\partial\theta}\,, \\
        \theta &\leftarrow \theta + \epsilon M^{-1} \rho\,, \\
        \rho &\leftarrow \rho - \frac{\epsilon}{2} \frac{\partial V}{\partial\theta}\,,
    \end{split}
\end{equation}
where $\epsilon$ is the \textit{integration step size} that determines the smallest time interval. The length of the trajectory will then be $\epsilon L$ and the new state of the system is denoted as $(\rho',\theta')$. The numerical error introduced into the calculation by the \textit{leapfrog} algorithm is of the order of $\epsilon^{3}$ per step and $\epsilon^{2}$ globally~\parencite{leimkuhler2004simulating}.

\subsection{Metropolis acceptance criterion}
If the \textit{leapfrog algorithm} were perfect and did not introduce any numerical error, we would not have to do anything more than re--sample the \textit{momentum} variable every $L$ integration steps. However, the \textit{leapfrog integrator} is far from this which means that we need to account for the numerical error that it introduces before it accumulates. To this end, we only accept and add the new state $(\rho',\theta')$ into the Markov chain with probability
\begin{equation}
    \label{eq:hmc_acceptance}
    \alpha(\theta',\theta) = \min\left(1, \frac{p(\rho',\theta')}{p(\rho,\theta)} \right)\,,
\end{equation}
and reject it otherwise by adding $(\rho,\theta)$ into the chain. Equation \ref{eq:hmc_acceptance} is simply the \textit{Metropolis acceptance probability} for HMC. Therefore, we see that HMC is essentially a case of \textit{Metropolis--Hastings} with symmetric proposal distribution in the augmented state space of $(\rho,\theta)$.

\begin{algorithm}[ht!]
\caption{Hamiltonian Monte Carlo} \algolabel{hmc}
\begin{algorithmic}[1]
\REQUIRE{initial state $\theta_{1}$, potential energy $V(\theta)=-\log p(\theta)$ up to an additive constant, kinetic energy definition $T(\rho)=\rho^{T}M^{-1}\rho / 2$, number of leapfrog steps $L$, integration step size $\epsilon$, and number of iterations $N$}
\ENSURE{Markov chain $\theta_{1}, \dots, \theta_{N}$ that has $p(\theta)$ as its equilibrium distribution}
\STATE{Draw momentum variable $\rho_{t}\sim \mathcal{N}(0,M)$}
\STATE{Set proposed state $\theta'\leftarrow \theta_{t}$}
\STATE{Set proposed momentum $\rho'\leftarrow \rho$}
\FOR{$=1$ \TO $L$}
    \STATE{Update momentum $\rho' \leftarrow \rho' - \frac{\epsilon}{2}\frac{\partial V}{\partial\theta}$}
    \STATE{Update position $\theta'\leftarrow \theta' + \epsilon M^{-1} \rho'$}
    \STATE{Update momentum $\rho' \leftarrow \rho' - \frac{\epsilon}{2}\frac{\partial V}{\partial\theta}$}
\ENDFOR
\STATE{Reverse momentum $\rho' \leftarrow -\rho'$}
\STATE{Compute acceptance probability 
\[
\alpha = \min\left( 1, \exp\left[ V(\theta) - V(\theta') + T(\theta) - T(\theta') \right]\right)
\]}
\STATE{Draw uniform number $u\sim\mathcal{U}(0, 1)$}
\IF{$u < \alpha$}
    \STATE{Accept proposed state and set $\theta_{t+1}\leftarrow\theta'$}
\ELSE
    \STATE{Reject proposed state and set $\theta_{t+1}\leftarrow\theta$}
\ENDIF
\end{algorithmic}
\end{algorithm}

\subsection{Performance and tuning}

The sampling performance of HMC is very sensitive to its tuning~\parencite{neal2011mcmc, hoffman2014no} and many efforts have been made to develop heuristics and automated tuning procedures for the two hyperparameters, $\epsilon$ and $L$, that the method relies upon. The step size $\epsilon$ can be adaptively tuned by trying to match the observed acceptance rate to the theoretically optimal value of $0.65$. Tuning the number of steps $L$ is more cumbersome in practice. In principle, $L$ can be tuned by minimising the autocorrelation time of the Markov chain. In practice this requires running multiple preliminary runs with different values of $L$ in order to determine the most efficient one. For this reason, other approaches, such as \textit{Empirical HMC}~\parencite{wu2018faster} and the \textit{No U-Turn Sampler (NUTS)}~\parencite{hoffman2014no}, have been proposed that automate the use of HMC for many applications.

%************************************************
