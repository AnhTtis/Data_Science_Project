% !TEX TS-program = pdflatex
% !TEX root = ../ArsClassica.tex

%************************************************
\chapter{Simple MCMC methods}
\label{chp:mcmc}
%************************************************

\begin{flushright}
\itshape
Not all those who wander are lost. \\
\medskip
--- J.R.R. Tolkien
\end{flushright}

During the first half of the twentieth century, research efforts were focused on the task of understanding the equilibrium behaviour of thermodynamic systems. Furthermore, it was well understood that this behaviour was described by specific probability distributions (e.g. \textit{canonical} distribution for a system in constant temperature). The physicists of that time showed great interest in methods that produced \textit{exact samples} from such probability distributions. Enrico Fermi, for instance, would exploit such methods to make amazingly--quick predictions of experimental outcomes as early as 1930s~\parencite{metropolis1987beginning}. During the next two decades, \textit{Stan Ulam} and \textit{John von Neumann} developed various such algorithms which, collectively, were anointed with the name ``Monte Carlo'' after the infamous casino.

After the war, \textit{Nicholas Metropolis} lead the group in \textit{Los Alamos} in applying Monte Carlo methods to increasingly complex thermodynamic systems. As \textit{exact sampling} was possible only for a limited number of distributions, Metropolis, along with \textit{Arianna Rosenbluth, Marshall Rosenbluth, Edward Teller}, and \textit{Augusta Teller} introduced the so--called \textit{Metropolis} algorithm that produced correlated samples from a wider range of probability distributions~\parencite{metropolis1953equation}. Arianna implemented the algorithm on the \textit{MANIAC} computer~\parencite{gubernatis2005marshall} and thus she is considered the first person in history to implement a MCMC method.

After decades of empirical success in physics and chemistry, the statistician Hastings~\parencite{hastings1970} generalised the method by realising that by introducing a small modification he could allow for any \textit{proposal} distribution, not just a limited family of symmetric ones. The method is known today as \textit{Metropolis--Hastings}. Despite \textit{Hasting}'s seminal contribution, it was not until 1984 that \textcite{geman1984stochastic} introduced the \textit{Gibbs sampler} for the task of image reconstruction, for the broader statistical community to realise the potential of MCMC methods for parameter inference~\parencite{gelfand1990sampling}.

\section{Metropolis--Hastings}

The key idea of the \textit{Metropolis--Hastings} algorithm is to separate the \textit{Markov transition probability} into two steps, a \textit{proposal} and an \textit{acceptance} step. During the \textit{proposal} step, a new state $\theta'$ is generated conditional on the current state $\theta$,
\begin{equation}
    \label{eq:proposal_distribution}
    \theta' \sim q(\theta'\vert\theta)\,,
\end{equation}
by sampling from a proposal distribution $q(\theta'\vert\theta)$. The aim of this step is to produce a new state that is likely, but not necessary, to reside in the typical set of the target distribution. The form of the conditional proposal distribution can be chosen based on the particular target distribution. As we will see shortly, many of the developments in the field of MCMC focus explicitly on the choice of the proposal distribution.

Once the new state $\theta '$ is generated, its validity (i.e. whether or not it belongs to the typical set) is assessed in the \textit{acceptance} step. In particular, the new state $\theta'$ is accepted with probability,
\begin{equation}
    \label{eq:acceptance_criterion}
    \alpha(\theta',\theta) = \min\left(1, \frac{p(\theta')q(\theta\vert\theta')}{p(\theta)q(\theta'\vert\theta)} \right)\,.
\end{equation}
Equation \ref{eq:acceptance_criterion} is often called the \textit{Metropolis acceptance probability}. If accepted, the new state $\theta'$ is added to the Markov chain and the process is repeated with that as the current state (i.e. $\theta \leftarrow \theta')$. On the other hand, if the state $\theta'$ is rejected, the current state $\theta$ is added (i.e. repeated) on the chain. This acceptance/rejection procedure based on equation \ref{eq:acceptance_criterion} is often referred to as the \textit{Metropolis acceptance criterion}.

It is important to mention here that the \textit{Metropolis acceptance criterion} can be evaluated even if we are only able to compute $p(\theta)$ up to a normalisation constant, as any such factor would cancel out in the ratio $p(\theta')/p(\theta)$ that appears in equation \ref{eq:acceptance_criterion}. This is a very important feature of the algorithm and one of the reasons for its widespread success. In practice, it is very difficult to know the exact value for the \textit{model evidence} $\mathcal{Z}=p(d)$ that acts as the normalisation factor for the posterior distribution $p(\theta\vert d)$ that might be the target distribution.

It is straightforward to show that the \textit{Metropolis--Hastings} algorithm leaves the target distribution $p(\theta)$ \textit{stationary} by first proving that it preserves \textit{detailed balance}. The \textit{Markov transition probability} is simply,
\begin{equation}
    \label{eq:markov_transition_probability_mh}
    T(\theta'\vert\theta) = q(\theta'\vert\theta) \alpha(\theta',\theta)\,,
\end{equation}
that is, the probability of proposing the new state $\theta'$ given the old state, times the probability of accepting it. Therefore, the \textit{Markov transition probability} for the \textit{Metropolis--Hastings} algorithm preserves detailed balance,
\begin{equation}
    \label{eq:mh_preserves_detailed_balance}
    \begin{split}
        T(\theta'\vert\theta)p(\theta) &= q(\theta'\vert\theta) \alpha(\theta',\theta) p(\theta) \\
        &= q(\theta'\vert\theta) \min\left(1, \frac{p(\theta')q(\theta\vert\theta')}{p(\theta)q(\theta'\vert\theta)} \right) p(\theta) \\
        &= \min\left( p(\theta)q(\theta'\vert\theta),  p(\theta')q(\theta\vert\theta')\right) \\
        &= q(\theta\vert\theta') \alpha(\theta,\theta') p(\theta') \\
        &= T(\theta\vert\theta')p(\theta')\,,
    \end{split}
\end{equation}

\begin{algorithm}[ht!]
\caption{Metropolis--Hastings} \algolabel{mh}
\begin{algorithmic}[1]
\REQUIRE{initial state $\theta_{1}$, (unnormalised) target density $f(\theta)\propto p(\theta)$, proposal density $q(\theta'\vert\theta)$, and number of iterations $N$}
\ENSURE{Markov chain $\theta_{1}, \theta_{2}, \dots, \theta_{N}$ that has $p(\theta)$ as its equilibrium distribution}
\FOR{$t=1$ \TO $N$}
\STATE{Draw new state from proposal distribution $\theta'\sim q(\theta'\vert\theta_{t})$}
\STATE{Compute acceptance probability $\alpha = \min\left(1, \frac{f(\theta')q(\theta_{t}\vert \theta')}{f(\theta_{t})q(\theta'\vert \theta_{t})} \right)$}
\STATE{Draw uniform random number $u\sim \mathcal{U}(0,1)$}
\IF{$u < \alpha$}
    \STATE{Accept proposed state and set $\theta_{t+1}\leftarrow \theta'$}
\ELSE
    \STATE{Reject proposed state and set current state as the next $\theta_{t+1}\leftarrow \theta_{t}$}
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Random--walk Metropolis}

A very common, and simplifying in practice, choice for the proposal distribution is the conditional normal distribution $q(\theta'\vert\theta)=\mathcal{N}(\theta'\vert\theta,\Sigma)$ centred around the current state $\theta$ with covariance matrix $\Sigma$~\parencite{metropolis1953equation, tierney1994markov}. The probability density has the usual Gaussian functional form,
\begin{equation}
    \label{eq:gaussian_proposal}
    q(\theta'\vert\theta) = \det(2\pi\Sigma)^{-\frac{1}{2}} \exp{\left[-\frac{1}{2}(\theta'-\theta)^{T}\Sigma^{-1}(\theta'-\theta)\right]}\,.
\end{equation}
The symmetry of this proposal distribution,
\begin{equation}
    \label{eq:gaussian_proposal_symmetry}
    q(\theta'\vert\theta) = q(\theta\vert\theta')\,,
\end{equation}
means that the \textit{Metropolis acceptance probability} of equation \ref{eq:acceptance_criterion} is simplified and the $q$ terms drop out,
\begin{equation}
    \label{eq:acceptance_criterion_symmetric}
    \alpha(\theta',\theta) = \min\left(1, \frac{p(\theta')}{p(\theta)} \right)\,.
\end{equation}

\subsection{Independence Metropolis}

Another simple choice of proposal distribution is to make it independent of the current state $\theta$. For instance, one can choose a normal distribution $q(\theta)=\mathcal{N}(\theta\vert\mu,\Sigma)$ with mean $\mu$ and covariance matrix $\Sigma$, both of which must be known \textit{a priori} and can not depend on the current state. In this case, the \textit{Metropolis acceptance probability} reduces to,
\begin{equation}
    \label{eq:acceptance_criterion_independence}
    \alpha(\theta',\theta) = \min\left(1, \frac{p(\theta')q(\theta)}{p(\theta)q(\theta')} \right)\,.
\end{equation}
One of the benefits of \textit{Independence Metropolis}~\parencite{hastings1970, tierney1994markov}, as this approach is called, is that any states produced are independent samples from the target distribution $p(\theta)$. However, it suffers from similar problems to \textit{Importance sampling}. Instead of vanishingly small importance weights, in \textit{Independence Metropolis} we might experience vanishingly small \textit{acceptance probability} when the overlap of the typical set of the proposal distribution $q$ with the target $p$ is small. For this reason, the use of \textit{Independence Metropolis} is wise only when we have good reasons to believe that the proposal distribution is sufficiently close to the target distribution or the dimensionality is low.

\subsection{Metropolis--adjusted Langevin algorithm}

As we have seen, the normal proposal distribution of \textit{Random--walk Metropolis} can utilise only \textit{global} information about the target distribution (i.e. the covariance matrix) in order to achieve efficient sampling. Although sufficient in low to moderate dimensional problems, this strategy can become inefficient as the number of parameters of the target distribution increases. In practice, \textit{Random--walk Metropolis} proposes new states indiscriminately along directions of great covariance without taking into account the local structure of the typical set. This results in either low acceptance probabilities or small proposed steps being accepted as the typical set become thinner in high dimensions.

One way to circumvent this effect and achieve better sampling performance is to capitalise on the knowledge of the gradient of the target distribution in order to ``bias'' the proposed states towards directions that are more likely to lead to higher acceptance probability. \textit{Metropolis--adjusted Langevin algorithm (MALA)}~\parencite{roberts2002langevin} achieves this by using a conditional normal distribution,
\begin{equation}
    \label{eq:mala_proposal}
    q(\theta'\vert\theta) = \mathcal{N}\left(\theta+\tau\Sigma \nabla \log p(\theta), 2\tau\Sigma \right)\,,
\end{equation}
where its mean $\theta+\tau\Sigma \nabla \log p(\theta)$ is shifted from the current state $\theta$ along the direction of the gradient of the logarithm of the target distribution $\nabla \log p(\theta)$. If known, $\Sigma$ can be an approximate covariance matrix that characterises the target distribution, otherwise, a unit--diagonal matrix can be used. $\tau$ is the step size of the method and determines the amount of shift of the proposal distribution. In the limit that $\tau\rightarrow 0$, MALA reduces to RWM. The step size $\tau$ can be modified in order to achieve the theoretically optimal acceptance probability of $0.574$. Despite the fact that the aforementioned acceptance rate has only been proven to be optimal for certain types of target distributions~\parencite{roberts1998optimal}, we expect that values in the range between $0.4$ and $0.8$ would result in a high performance for most applications.

In terms of the typical set, we can think of the gradient of the log probability as a guide that allows for better--informed proposals that are more likely to belong to the typical shell. 

\subsection{Adaptive Metropolis}

Hyperparameters of the proposal distribution, such as the covariance matrix $\Sigma$ of RWM or the step size $\tau$ of MALA, do not have to be chosen \textit{a priori} or based on preliminary MCMC runs but they can instead be adaptively tuned during the run. \textcite{haario2001adaptive} presented a prototype adaptive version of RWM in which the proposal distribution is continuously adapted during the run using all of the collected samples in order to estimate its covariance matrix. The estimation of the covariance matrix is efficient as only incremental updates are required using simple recursive formulas.

In order to achieve this kind of proposal adaptation in practice we need to abandon the Markov property of the chain. In general, this is not a problem as there is nothing special about the Markov property apart from its simplicity. However, continuous tuning of the proposal distribution during the run requires the adaptation to be \textit{diminishing}, with very specific characteristics, in order to preserve the \textit{ergodicity} of the method~\parencite{brooks2011handbook}.

One of the most commonly used algorithms for \textit{diminishing adaptation} is the \textit{stochastic approximation} algorithm of \textcite{robbins1951stochastic}. Suppose that we have a function $f(\lambda_{i})=f_{i}$ , which encodes some aspect of the behaviour of the $i$--th state of chain (e.g. the acceptance probability) as a function of some tunable property $\lambda$ (e.g. the proposal scale), that has expectation value,
\begin{equation}
    \label{eq:robbins_monro_expectation}
    \mathbb{E}[f(\lambda)] = \frac{1}{n}\sum_{i=1}^{n}f(\lambda_{i})\,.
\end{equation}
The solution to the equation $\mathbb{E}[f(\lambda)] = f^{*}$ can be found iteratively, using the recursive formula,
\begin{equation}
    \label{eq:robbins_monro_recursion}
    \lambda_{i+1} = \lambda_{i} - \gamma_{i} \left(f_{i} - f^{*} \right)\,,
\end{equation}
assuming that $f$ is a non--decreasing function of $\lambda$ that is uniformly bounded \parencite{andrieu2008tutorial}. The parameter $\gamma_{i}$ determines the \textit{learning rate} or the rate of convergence of the approximation and has to obey two conditions,
\begin{equation}
    \label{eq:robbins_monro_conditions}
    \sum_{i=1}^{n}\gamma_{i}=\infty\,,\quad \sum_{i=1}^{n}\gamma_{i}^{2}<\infty \,.
\end{equation}
The former condition ensures that any point $\theta$ can eventually be reached, and the latter condition ensures that the fluctuations introduced by new iterations is contained and does not prevent convergence. A commonly used schedule for the learning rate that satisfies the above conditions has the form $\gamma_{i}=i^{-\kappa}$ for $\kappa\in(0.5,1]$.

\begin{algorithm}[ht!]
\caption{Adaptive Metropolis} \algolabel{am}
\begin{algorithmic}[1]
\REQUIRE{initial state $\theta_{1}$, (unnormalised) target density $f(\theta)\propto p(\theta)$, the target acceptance rate $\alpha^{*}$, learning rate schedule (e.g. $g_{t}=1/t$), and number of iterations $N$}
\ENSURE{Markov chain $\theta_{1}, \theta_{2}, \dots, \theta_{N}$ that has $p(\theta)$ as its equilibrium distribution}
\STATE{Initialise $\mu_{1}=0$, $\Sigma_{1} = 1$, $\log\lambda_{1}=0$}
\FOR{$t=1$ \TO $N$}
\STATE{Draw new state from proposal distribution $\theta'\sim \mathcal{N}(\theta'\vert\theta_{t},\lambda_{t}\Sigma_{t})$}
\STATE{Compute acceptance probability $\alpha_{t} = \min\left(1, f(\theta')/f(\theta_{t}) \right)$}
\STATE{Draw uniform random number $u\sim \mathcal{U}(0,1)$}
\IF{$u < \alpha_{t}$}
    \STATE{Accept proposed state and set $\theta_{t+1}\leftarrow \theta'$}
\ELSE
    \STATE{Reject proposed state and set current state as the next $\theta_{t+1}\leftarrow \theta_{t}$}
\ENDIF
\STATE{Update mean estimate $\mu_{t+1}\leftarrow\mu_{t}-\gamma_{t} (\mu_{t} - \theta_{t+1})$}
\STATE{Update covariance estimate 
\[
\Sigma_{t+1}\leftarrow \Sigma_{t} - \gamma_{t}\left[ \Sigma_{t} - (\mu_{t} - \theta_{t+1})(\mu_{t} - \theta_{t+1})^{T}\right]
\]}
\STATE{Update proposal scale estimate $\log\lambda_{t+1}\leftarrow\log\lambda_{t}-\gamma_{t} (\alpha_{t} - \alpha^{*})$}
\ENDFOR
\end{algorithmic}
\end{algorithm}
Let us now go through an example of developing an \textit{adaptive} version of the commonly used RWM, in which we tune the covariance matrix $\Sigma$ of the Gaussian proposal distribution, $\theta'\sim\mathcal{N}(\theta'\vert\theta,\lambda\Sigma)$, using the following \textit{diminishing adaptation} scheme, 
\begin{equation}
    \label{eq:covariance_adaptation}
    \begin{split}
        \mu_{i+1} &= \mu_{i} - \gamma_{i}\left( \mu_{i} - \theta_{i+1}\right)\,, \\
        \Sigma_{i+1} &= \Sigma_{i}- \gamma_{i}\left[ \Sigma_{i} - (\mu_{i} - \theta_{i+1})(\mu_{i} - \theta_{i+1})^{T}\right]\,,
    \end{split}
\end{equation}
where the $\mu$ is the mean value used for the estimation of the covariance $\Sigma$, and $\gamma_{i}=1/i$ is the \textit{learning rate}. At the same time we can also tune the \textit{magnitude} of the proposal scale, $\lambda_{i}$, by attempting to match the acceptance probability $\alpha$ to the theoretically optimal value of $\alpha^{*}=0.234$,
\begin{equation}
    \label{eq:proposal_scale_adaptation}
    \log\lambda_{i+1} = \log\lambda_{i} - \gamma_{i}\left( \alpha_{i} - \alpha^{*}\right)\,.
\end{equation}
Understanding equation \ref{eq:proposal_scale_adaptation} is straightforward, if the observed acceptance rate is greater than the target (i.e. $\alpha_{i} < \alpha^{*}$) then the logarithm of the magnitude of the proposal scale $\log\lambda$ is reduced and \textit{vice versa}. The \textit{Adaptive Metropolis} method presented in this paragraph constitutes a generalisation of the method presented by \textcite{haario2001adaptive}. With the inclusion of the adaptation of the proposal scale using equation \ref{eq:proposal_scale_adaptation} the algorithm resembles that of \textcite{andrieu2008tutorial}.

\section{Gibbs sampling}

Another very popular Markov chain Monte Carlo method, to which we partly owe the widespread use of Bayesian inference today, is \textit{Gibbs sampling}. Initially known as the \textit{heat bath} algorithm in the \textit{statistical physics} literature, the \textit{Gibbs sampler} enjoyed great success in the \textit{statistical community} following the seminal paper by \textcite{geman1984stochastic} that demonstrated its benefits for analysing \textit{Gibbs} distributions on lattices in the context of image processing.

\subsection{Gibbs sampler}

Gibbs sampler attempts to overcome the \textit{curse of dimensionality} using \textit{conditioning}~\parencite{casella1992explaining}. In particular, assuming that exact sampling from the conditional distributions of the target distribution $p(\theta)$ is possible, we can generate samples from the target distribution by sequentially sampling from its full set of conditionals instead. Given an initial state $\theta=(\theta_{1}, \dots, \theta_{D})$, the next state in the Markov chain can be generated as,
\begin{equation}
    \label{eq:gibbs_sampler}
    \begin{split}
        \theta_{1}' &\sim p(\theta_{1}\vert\theta_{2},\dots,\theta_{D}) \\
         & \vdots \\
        \theta_{k}' &\sim p(\theta_{k}'\vert\theta_{1},\dots,\theta_{k-1}',\theta_{k+1},\dots\theta_{D}) \\
         & \vdots \\
        \theta_{D}' &\sim p(\theta_{D}'\vert\theta_{1},\dots,\dots\theta_{D-1}')\,.
    \end{split}
\end{equation}
The current state $\theta$ is then replaced by the new state $\theta'=(\theta_{1}', \dots, \theta_{D}')$ and the process is repeated until enough states are collected in the Markov chain. The order of the state updates of equation \ref{eq:gibbs_sampler} can be either fixed (with a possible reversal after every iteration), as shown above, or randomised  to ensure detailed balance.

\begin{algorithm}[ht!]
\caption{Gibbs sampler} \algolabel{gibbs}
\begin{algorithmic}[1]
\REQUIRE{initial state $\theta^{(1)}=(\theta_{1}^{(0)}, \theta_{2}^{(0)}, \dots, \theta_{D}^{(0)})$, all the conditional distributions $p(\theta_{k}\vert \theta_{1},\dots,\theta_{k-1},\theta_{k+1},\dots,\theta_{D})$ of target $p(\theta)$, and number of iterations $N$}
\ENSURE{Markov chain $\theta^{(1)}, \theta^{(2)}, \dots, \theta^{(N)}$ that has $p(\theta)$ as its equilibrium distribution}
\FOR{$t=1$ \TO $N$}
    \FOR{$k=1$ \TO $D$}
        \STATE{Draw from conditional $\theta_{k}'\sim p(\theta_{k}\vert \theta_{1}',\dots,\theta_{k-1}',\theta_{k+1}^{(t)},\dots,\theta_{D}^{(t)})$}
    \ENDFOR
    \STATE{Set $\theta^{(t+1)}\leftarrow \theta' = (\theta_{1}',\dots,\theta_{D}')$}
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Metropolis--within--Gibbs sampler}

The \textit{Gibbs sampler} relies on our ability to produce samples from each one of the conditional distributions. This however is not always feasible as some of the components of the full conditional set might not admit an exact sampling solution. Instead of abandoning \textit{Gibbs sampler} altogether, \textcite{muller1991generic,muller1992alternatives} suggested the use of a compromise between the \textit{Gibbs sampler} and the \textit{Metropolis--Hastings} algorithm.

The key idea behind the \textit{Metropolis--within--Gibbs} sampler is to use \textit{Gibbs sampling} for as many of the conditional distributions as possible in order to produce exact samples, and rely on correlated samples generated using \textit{Metropolis--Hastings} for any conditional distributions that \textit{exact sampling} is not possible.

Suppose that we have a \textit{partial} state $(\theta_{1}',\dots,\theta_{k-1}',\theta_{k},\dots,\theta_{D}$) and we have difficulty generating exact samples from the conditional distribution $\theta_{k}'\sim p(\theta_{k}\vert\theta_{1}',\dots,\theta_{k-1}',\theta_{k+1},\dots,\theta_{D})$. We can then treat $p(\theta_{k}\vert\theta_{1}',\dots,\theta_{k-1}',\theta_{k+1},\dots,\theta_{D})$ as the target distribution for a \textit{Metropolis--Hastings} estimator as follows, in order to proceed with the computation,
\begin{enumerate}
    \item First, we have to propose a new sample $\theta_{k}^{*}\sim q(\theta_{k}\vert\theta_{1}',\dots,\theta_{k-1}',\theta_{k+1},\dots,\theta_{D})$ from an arbitrary proposal distribution,
    \item Then, compute the \textit{Metropolis acceptance probability} 
    \begin{equation}
        \label{eq:acceptance_probability_metropolis_within_gibbs}
        \begin{split}
            \alpha(\theta_{k}^{*},\theta_{k}) = \min\bigg(1, &\frac{p(\theta_{k}^{*}\vert\theta_{1}',\dots,\theta_{k-1}',\theta_{k+1},\dots,\theta_{D}) }{p(\theta_{k}\vert\theta_{1}',\dots,\theta_{k-1}',\theta_{k+1},\dots,\theta_{D}) } \\
            \times &\frac{q(\theta_{k}\vert\theta_{1}',\dots,\theta_{k-1}',\theta_{k+1},\dots,\theta_{D})}{q(\theta_{k}^{*}\vert\theta_{1}',\dots,\theta_{k-1}',\theta_{k+1},\dots,\theta_{D})}\bigg)\,,
        \end{split}
    \end{equation}
    \item Finally, accept the new state $\theta_{k}'\leftarrow\theta_{k}^{*}$ with probability $\alpha(\theta_{k}^{*},\theta_{k})$, otherwise reject and keep the previous state $\theta_{k}'\leftarrow\theta_{k}$\,.
\end{enumerate}
Using the algorithm presented above we can replace \textit{exact sampling} from conditional distributions where it is not feasible with \textit{Metropolis--Hastings} estimates.

\begin{algorithm}[ht!]
\caption{Metropolis--within--Gibbs sampler} \algolabel{metropolis_within_gibbs}
\begin{algorithmic}[1]
\REQUIRE{initial state $\theta^{(1)}=(\theta_{1}^{(0)}, \theta_{2}^{(0)}, \dots, \theta_{D}^{(0)})$, proposal distributions $q(\theta_{k}\vert \theta_{1},\dots,\theta_{k-1},\theta_{k+1},\dots,\theta_{D})$, (unnormalised) target distribution $f(\theta)\propto p(\theta)$, the conditional distributions $p(\theta_{\ell}\vert \theta_{1},\dots,\theta_{\ell-1},\theta_{\ell+1},\dots,\theta_{D})$ of target $p(\theta)$ where $\ell \in L$ and $L$ the set of indices for which exact sampling of the conditional is possible, and number of iterations $N$}
\ENSURE{Markov chain $\theta^{(1)}, \theta^{(2)}, \dots, \theta^{(N)}$ that has $p(\theta)$ as its equilibrium distribution}
\FOR{$t=1$ \TO $N$}
    \FOR{$k=1$ \TO $D$}
        \IF{$k\in L$}
            \STATE{Draw from conditional $\theta_{k}'\sim p(\theta_{k}\vert \theta_{1}',\dots,\theta_{k-1}',\theta_{k+1}^{(t)},\dots,\theta_{D}^{(t)})$}
        \ELSE
            \STATE{Draw from proposal $\theta_{k}^{*}\sim q(\theta_{k}\vert     \theta_{1}',\dots,\theta_{k-1}',\theta_{k+1}^{(t)},\dots,\theta_{D}^{(t)})$}
            \STATE{Compute acceptance probability
            \begin{equation*}
                \begin{split}
                    \alpha = \min\bigg( 1, &\frac{p(\theta_{k}^{*}\vert \theta_{1}',\dots,\theta_{k-1}',\theta_{k+1}^{(t)},\dots,\theta_{D}^{(t)})}{p(\theta_{k}\vert \theta_{1}',\dots,\theta_{k-1}',\theta_{k+1}^{(t)},\dots,\theta_{D}^{(t)})} \\
                    \times &\frac{q(\theta_{k}\vert \theta_{1}',\dots,\theta_{k-1}',\theta_{k+1}^{(t)},\dots,\theta_{D}^{(t)})}{q(\theta_{k}^{*}\vert \theta_{1}',\dots,\theta_{k-1}',\theta_{k+1}^{(t)},\dots,\theta_{D}^{(t)})} \bigg)
                \end{split}
            \end{equation*}
            }
            \STATE{Draw uniform number $u\sim\mathcal{U}(0,1)$}
            \IF{$u < \alpha$}
                \STATE{accept new partial state and set $\theta_{k}'\leftarrow \theta_{k}^{*}$}
            \ELSE
                \STATE{reject new partial state and set     $\theta_{k}'\leftarrow \theta_{k}^{(t)}$}
            \ENDIF
        \ENDIF
    \ENDFOR
    \STATE{Set $\theta^{(t+1)}\leftarrow \theta' = (\theta_{1}',\dots,\theta_{D}')$}
\ENDFOR
\end{algorithmic}
\end{algorithm}