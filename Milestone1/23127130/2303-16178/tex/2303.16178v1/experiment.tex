\section{Experimental Design}
In this section, we discuss the design of our experiment.
This includes the research objectives of this paper, the datasets we use to perform the experiments, the models we use to train the data, the metrics we use to evaluate model performance, and any threats to validity we might have.

\vspace{-0.2cm}
\subsection{Research Questions}
\label{sec:rqs}
Our research objective is to evaluate the extent to which label smoothing improves source code summarization models, and the configurations that maximize this performance improvement. 
To this end, we ask the following research questions:
\begin{itemize}
	\item[\textbf{RQ1}] What is the effect of label smoothing on the overall performance of recently-published source code summarization baselines in terms of BLEU, METEOR and USE+c scores?
	\item[\textbf{RQ2}] What is the best label smoothing configuration that maximizes model performance and how is this configuration affected by the output vocabulary size?
	\item[\textbf{RQ3}] What is the effect of label smoothing on the diversity of target vocabulary and how does this affect  performance?
\end{itemize}

The rationale behind RQ1 is that label smoothing is not a commonly used regularization technique in automatic source code summarization, despite evidence of success in other NLG applications.
Many models have been proposed for source code summarization, but to the best of our knowledge, none use label smoothing as a regularizer.
While there is evidence that label smoothing will improve the performance of these models, there has been no extensive research to demonstrate this.
Our goal with this RQ is to quantify how including label smoothing to train these models can change their performance.
We use three different metrics to verify our findings: BLEU, METEOR and USE+c.
These metrics are further discussed in Section~\ref{sec:metrics}

The rationale behind RQ2 is that our application of label smoothing has underlying hyperparameters that require tuning.
As noted earlier, $\epsilon\in (0,1]$.
But the research literature is unclear about what the optimal values are for these hyperparameters, especially in the problem of code summarization.
Additionally, we also do not know how the size of the output vocabulary ($N_t$) affects model performance with this configuration.
The larger the vocabulary size, the smaller the smoothed probability per output token ($\frac{\epsilon}{N_t}$).
Our goal with this RQ is to identify a suitable value of $\epsilon$. % that increases BLEU, METEOR and USE+c scores the most.
Furthermore, we aim to understand the relationship of this probability $\epsilon$ with the size of output vocabulary, $N_t$.

The rationale behind RQ3 is that there are competing intuitions as to how label smoothing affects model performance.
As noted in Section~\ref{sec:backgroundls}, label smoothing artificially introduces uncertainty in the one-hot output vector.
On one hand, the introduction of uniform uncertainty across the output vector could exacerbate the problem of label noise~\cite{xie2016disturblabel}.
However, this uncertainty makes the model less confident about its predictions and allows it to consider rare words, thus reducing the problem of class imbalance in source code summarization~\cite{haque2021action}.
On the other hand, preventing overconfidence could mitigate label noise by improving model generalization, thus focusing on more commonly occurring words~\cite{lukasik2020does}.
Our goal with this RQ is to study which of these competing intuitions is borne out in practice for the problem of source code summarization.

\subsection{Datasets}
We use two datasets in this paper: one is Java and the other is C/C++.
The Java dataset, first published by LeClair~\emph{et al.}, introduce some of the best practices for developing a dataset for source code summarization that are now a standard among the research community~\cite{leclair2019recommendations}.
It consists of 2.1m methods from more than 28k projects.
The training, validation and test set are split by projects to prevent data from train set to leak into the test set by virtue of being in the same project.
This dataset has since been used in many peer-reviewed publications~\cite{haque2020improved, leclair2020improved, stapleton2020human, bansal2021project, xie2021exploiting} and new additions has since been made to it, including context tokenization.
We use a filtered version of this dataset, with 1.9m functions, published by Bansal~\emph{et al.} that remove code clones in accordance with recommendations by Allamanis~\emph{et al.}~\cite{allamanis2019adverse}.

The C/C++ dataset was first published by Haque~\emph{et al.}~\cite{haque2021action} following an extraction model proposed by Eberhart~\emph{et al.}~\cite{eberhart2019automatically} to adhere to the idiosyncrasies of C/C++, while maintaining the same strict standards proposed by LeClair~\emph{et al.}~\cite{leclair2019recommendations}.
It consists of 1.1m methods from more than 33k projects.
% We perform similar filtration on this dataset to remove code clones~\cite{allamanis2019adverse, bansal2021project}.

Additionally, we extract individual statements on the top 10\% largest methods from the Java dataset (Java-q90) and the top-25\% largest methods from the C/C++ dataset (C/C++-q75).
While this filtration reduces the size of the dataset, it eliminates simple getters/setters and other small functions whose comments are easy to predict.
The remaining functions have more statements, which is more challenging and representative of real world use-case scenario.
We chose top 10\% for Java and 25\% for C/C++ to keep the average size and number of subroutines similar for both datasets.
%We call these the Java-q90 dataset and C/C++-q75.

For RQ3, we use the method outlined by Haque~\emph{et al.} to convert both the Java-q90 and C/C++-q75 dataset to action word prediction dataset~\cite{haque2021action}.
We extract action words from comments and stem them to reduce vocabulary size (e.g. to ensure that `delete', `deletes', `deleted', `deleting' are classified as the same action word).

\vspace{-0.2cm}
\subsection{Baselines}
\label{sec:baselines}
%To demonstrate the effect of label smoothing, we explore a wide range of architectures in this paper.
We use six baselines in this paper.
We chose each baseline because it represents a family of similar approaches or is a well-cited approach used as a baseline in many papers.
%The baselines use a range of inputs.
%Some use source code only, other use the AST and others use a combination.
%They also vary in inputs, spanning from code-only to code+AST to code+AST+context.
%Due to space limitations, we only discuss them briefly here.
%Detailed implementation of each model can be found in our online appendix (Section~\ref{sec:appendix}).

\textbf{attendgru} This baseline is a simple unidirectional RNN-based attentional neural encoder-decoder architecture.
It takes only source code tokens as encoder input and English comment as decoder input.
It was first introduced by Iyer~\emph{et al.}~\cite{iyer2016summarizing} as an off-the-shelf NMT/NLG approach to generate source code summaries.
For our implementation, we use GRUs instead of LSTMs because they are much faster while providing comparable performance~\cite{leclair2019neural}.

\textbf{transformer} This baseline is another simple encoder-decoder architecture, but it replaces the recurrent layers with stacked muti-head attention layers~\cite{vaswani2017attention}.
As mentioned in Section~\ref{sec:backgroundnmt}, transformers introduce a position embedding layer that captures the sequential order of tokens which allows the multi-head attention layer to process the entire sequence at the same time.
On the encoder side, the multi-head attention layer computes dot-product based self-attention on the source code tokens. %before passing this to the feed-forward layer.
On the decoder side, there are two multi-head attention layers: a masked multi-head attention layer that computes self-attention on the comment tokens followed by a regular multi-head attention layer that computes attention between the encoder and the masked attention layer.
%The output of this layer is then passed through a feed forward layer to generate output.
%Ahmad~\emph{et al.} shows that transformer based models perform well on source code summarization task~\cite{ahmad2020transformer}.

\textbf{ast-attendgru} This baseline is an enhancement over the attendgru model by including AST information on the encoder side along with source code tokens.
This idea was first proposed by Hu~\emph{et al}~\cite{hu2018deep} who designed a Structure-Based Traversal (SBT) algorithm to flatten the AST and include the source code and AST input together in the encoder.
LeClair~\emph{et al.} improved this model by incorporating this flat AST on a separate recurrent layer and concatenating this encoder output with the original encoder output with just the code tokens~\cite{leclair2019neural}.
For our evaluation, we use the implementation by LeClair~\emph{et al.} because it's more recent and performs better.

\textbf{code2seq} This baseline, proposed by Alon~\emph{et al.}~\cite{alon2019code2seq} is similar to ast-attendgru as it also takes both source code tokens and AST as input.
However, instead of flattening the AST, they encode pairwise paths between nodes in the AST.
Then, they randomly select a subset of these paths as training input.
Randomly selecting paths prevents over-fitting while keeping the model size reasonable.
To reduce architectural variations and manage resource constraints, we set the number of paths explored to 100.

\textbf{codegnngru} This baseline provides another different technique for representing AST along the encoder.
Proposed by LeClair~\emph{et al.}, it uses Convolutional Graph Neural Networks (ConvGNN) to process the AST input~\cite{leclair2020improved}.
They pass the AST nodes through an embedding layer.
The output of this layer along with the AST edge data is then input to the ConvGNN.
%They pass the AST nodes and AST edge data to the ConvGNN.
For each node, ConvGNN adds the neighboring node to it's current node during a hop.
The model shows best performance for 2 hops; each AST node adds the neighboring node twice, thus propagating information between nodes that are separated by 2 hops.
The output of the ConvGNN layer is then passed to a recurrent layer before the result is concatenated with the output of the encoder that processes the source code tokens. 

\textbf{ast-attendgru-fc} This baseline improves upon the ast-attendgru model by including file context information on the encoder side along with source code tokens and AST.
Haque~\emph{et al.} applied the concept of using contextual information from other functions in the same file to a few different baselines~\cite{haque2020improved}.
They introduced a new encoder to process other functions in the file.
All encoder outputs are combined before passing it to the decoder for prediction.
While their results showed improvements in all baselines using file context, ast-attendgru-fc was the highest performing model.


\begin{figure*}[!b]
	%\vspace{0.5cm}
	\minipage{0.32\textwidth}
	\vspace{-0.6cm}
	\includegraphics[width=\linewidth, height=6.5cm]{figures/rq1_bar.png}
	%\caption{A really Awesome Image}\label{fig:rq1q90}
	\endminipage\hfill
	%\vspace{0.5cm}
	\minipage{0.32\textwidth}
	\vspace{-0.4cm}
	\includegraphics[width=\linewidth, height=6.5cm]{figures/rq1_bar2.png}
	%\caption{A really Awesome Image}\label{fig:rq1q75}
	\endminipage\hfill
	%\vspace{0.5cm}
	\minipage{0.32\textwidth}
	\vspace{-0.4cm}
	\includegraphics[width=\linewidth, height=6.5cm]{figures/rq1_bar3.png}
	\endminipage
	\captionsetup{justification=centering}
	\vspace{-0.2cm}
	\caption{Comparison of baselines with and without label smoothing ($\epsilon$=0.10). Blue indicates baseline aggregate METEOR score. Orange indicates increase in METEOR score for identical model with label smoothing added. Note y-axis starts at 27 METEOR for the Java-q90 dataset (left), 15 for the C/C++-q75 dataset (center) and 33 for the full Java dataset figure (right).}\label{fig:rq1}
	%\vspace{-0.1cm}
\end{figure*}

\vspace{-0.2cm}
\subsection{Metrics}
\label{sec:metrics}
We evaluate these baselines using three different evaluation metrics: BLEU, METEOR and USE+c.
Additionally, for RQ3, we use precision, recall and F1-score to evaluate action word prediction~\cite{powers2011evaluation}.
BLEU and METEOR are n-gram matching metrics while USE+c is a semantic similarity metric.
We use BLEU because it is the most popular evaluation metric for source code summarization.
BLEU is a precision based measure of N-gram overlap~\cite{papineni2002bleu}.
It was first introduced by Papineni~\emph{et al.} in 2002 for automatic evaluation of machine translation tasks.
It compares n-grams in the predicted and target summaries.
Typical implementations of BLEU scores set the range of n from 1 to 4.
An average BLEU score is then computed by combining these individual n-gram scores using predetermined weights.
Most code summarization models use a uniform weight distribution of 0.25 for each n, so we adhere to the same weights.

%We include BLEU score to allow a commensurable comparison between our work and related work.
Recent studies~\cite{stapleton2020human} have shown that BLEU does not correlate well with human judgement of source code comments. 
Roy~\emph{et al.}~\cite{roy2021reassessing} and Haque~\emph{et al.}~\cite{haque2022semantic} have proposed METEOR and USE+c as alternatives that better correlate with human evaluation.
METEOR~\cite{banerjee2005meteor} was introduced in 2005 to address the concerns of using BLEU~\cite{papineni2002bleu} or ROUGE~\cite{lin2004rouge}.
It combines n-gram precision and n-gram recall by taking their harmonic mean to compute a measure of similarity.
Like BLEU, METEOR tries to find exact n-gram matches.
If however, an exact match is not to be found, it performs a second pass to match word stems and finally a third pass, to match word synonyms.% using the wordnet corpora.
METEOR does not have a long history of being used in source code summarization.
However, it was recently shown to better reflect human quality assessment of generated summaries than BLEU or ROUGE, so we report this score.

USE+c~\cite{haque2022semantic} is a new evaluation metric proposed for source code summarization.
It differs from BLEU and METEOR because it does not focus on n-gram matching.
Instead, it uses the pre-trained Universal Sentence Encoder (USE)~\cite{cer2018universal} to compute a vector representation of both target and predicted sentence.
Note that USE trains stacked transformer encoders to compute context aware representation of words in a sentence before turning them into fixed length embeddings.
USE+c then computes the cosine similarity between these fixed length vectors.
A shortcoming of USE+c is that it does not include a brevity penalty.
Unlike BLEU or METEOR, it is also difficult to explain.
However, a recent survey by Haque~\emph{et al.}~\cite{haque2022semantic} showed that USE+c is better correlated with human ratings in terms of similarity, accuracy and completeness than most n-gram based metrics (including BLEU and METEOR).
Therefore, we report USE+c as an additional metric.

%Additionally, for RQ3, we use precision, recall and F1-score to evaluate action word prediction~\cite{powers2011evaluation}.
%This is because action word prediction is a multi-class classification problem, while BLEU, METEOR and USE+c are more suited for full sentence prediction.
%We also use confusion matrices to demonstrate model performance across the most commonly occurring words.

\vspace{-0.1cm}
\subsection{Threats to Validity}

Like any study, our experiment carries threats to validity.
One key threat affecting this paper is the datasets we use.
We aim to mitigate the first threat by using two peer-reviewed datasets with millions of examples in two different and widely-used programming languages.
While we perform some additional filtration of the datasets, we still train our models on a couple hundred thousand functions.
%These datasets have been vetted by the research community.
However, it is still possible the results may not be representative to all datasets.
In this regard, we advise caution on generalizing our findings outside of Java and C/C++ datasets. 

Another threat to validity is the plethora of implementation decisions we had to make for our baselines.
Each baseline discussed in section~\ref{sec:baselines} have different hyperparameters. 
These hyperparameters may change how label smoothing affect model performance.
Each model is also different in terms of parameters tuned during training (model size).
Due to hardware limitations, we do not compute the effect of label smoothing on each model using different model-intrinsic hyperparameters. Instead,
we mitigate this threat by taking the best hyperparameters listed for each baseline.
However, different hyperparameters could alter our conclusions.

%One final threat to validity is that the automatic metrics used to evaluate model performance may not be representative of human judgement.
%We mitigate this threat by using 3 different metrics: METEOR and USE+c was recently shown to better correlate with human ratings while BLEU is reported to allow direct comparison with previous work that do not report these ``newer'' metrics.
