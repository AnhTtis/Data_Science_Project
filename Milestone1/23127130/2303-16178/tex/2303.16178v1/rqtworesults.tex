\begin{table}[!t]
	\small
	\caption{\small{Scores of attendgru for different $\epsilon$. Output vocabulary size for the top table is 10k and for the bottom table is 44k.}}
	\vspace{-0.1cm}
	\begin{tabular}{p{0.6cm}|p{0.8cm}p{0.8cm}p{0.8cm}|p{0.5cm}p{0.5cm}}
		& \multicolumn{3}{c|}{\emph{metric scores}} & \multicolumn{2}{c}{\emph{t-stat, p-value}} \\
		$\epsilon$ 	& \multicolumn{1}{c}{\small{METEOR}} 		& \multicolumn{1}{c}{\small{USE}} 	& \multicolumn{1}{c|}{\small{BLEU}}
		& \multicolumn{1}{c}{\small{METEOR}}  & \multicolumn{1}{c}{\small{USE}} \\  \cline{1-6}
		0       & \multicolumn{1}{c}{32.82} & \multicolumn{1}{c}{50.21} & \multicolumn{1}{c|}{18.87} 
		& \multicolumn{1}{c}{-} 	& \multicolumn{1}{c}{-} \\
		%& \multicolumn{1}{c}{\emph{t-stat, p-value}} 	& \multicolumn{1}{c}{\emph{t-stat, p-value}} \\
		0.001       & \multicolumn{1}{c}{32.79} & \multicolumn{1}{c}{50.22} & \multicolumn{1}{c|}{18.85} 			
		& \multicolumn{1}{r}{-0.26,\textcolor{white}{$<$}0.80} 	& \multicolumn{1}{r}{\textcolor{white}{$<$}0.13,\textcolor{white}{$<$}0.90}  \\
		0.003       & \multicolumn{1}{c}{33.01} & \multicolumn{1}{c}{50.87} & \multicolumn{1}{c|}{18.82} 			
		& \multicolumn{1}{r}{1.34,\textcolor{white}{$<$}0.18} 	& \multicolumn{1}{r}{4.60,$<$0.01} \\
		0.007       & \multicolumn{1}{c}{32.99} & \multicolumn{1}{c}{50.36} & \multicolumn{1}{c|}{18.99} 			
		& \multicolumn{1}{r}{1.32,\textcolor{white}{$<$}0.19} 	& \multicolumn{1}{r}{1.13,\textcolor{white}{$<$}0.26} \\
		0.02       	& \multicolumn{1}{c}{32.97} & \multicolumn{1}{c}{50.36} & \multicolumn{1}{c|}{19.01} 			
		& \multicolumn{1}{r}{1.07,\textcolor{white}{$<$}0.28} 	& \multicolumn{1}{r}{0.99,\textcolor{white}{$<$}0.32} \\
		0.05	    & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{51.00} & \multicolumn{1}{c|}{19.30} 			
		& \multicolumn{1}{r}{3.18,$<$0.01} 	& \multicolumn{1}{r}{5.19,$<$0.01} \\
		0.10       	& \multicolumn{1}{c}{33.39} & \multicolumn{1}{c}{51.41} & \multicolumn{1}{c|}{19.45} 			
		& \multicolumn{1}{r}{3.76,$<$0.01} 	& \multicolumn{1}{r}{7.84,$<$0.01} \\
		0.25       	& \multicolumn{1}{c}{33.29} & \multicolumn{1}{c}{51.05} & \multicolumn{1}{c|}{19.32} 			
		& \multicolumn{1}{r}{2.93,$<$0.01} 	& \multicolumn{1}{r}{5.12,$<$0.01} \\
		0.40        & \multicolumn{1}{c}{33.34} & \multicolumn{1}{c}{50.98} & \multicolumn{1}{c|}{19.32} 			
		& \multicolumn{1}{r}{3.25,$<$0.01} 	& \multicolumn{1}{r}{4.63,$<$0.01}
	\end{tabular}
	\vspace{0.2cm}
	
	\begin{tabular}{p{0.6cm}|p{1.0cm}p{1.0cm}p{1.0cm}|p{1.5cm}p{1.5cm}}
		& \multicolumn{3}{c|}{\emph{metric scores}} & \multicolumn{2}{c}{\emph{t-stat, p-value}} \\
		$\epsilon$ 	& \multicolumn{1}{c}{\small{METEOR}} 		& \multicolumn{1}{c}{\small{USE}} 	& \multicolumn{1}{c|}{\small{BLEU}}
		& \multicolumn{1}{c}{\small{METEOR}}  & \multicolumn{1}{c}{\small{USE}} \\  \cline{1-6}
		0       & \multicolumn{1}{c}{32.94} 	& \multicolumn{1}{c}{50.30} 		& \multicolumn{1}{c|}{18.94} 			
		& \multicolumn{1}{c}{-} 	& \multicolumn{1}{c}{-} \\
		0.001       & \multicolumn{1}{c}{32.94} 	& \multicolumn{1}{c}{50.23} 		& \multicolumn{1}{c|}{18.89} 			
		& \multicolumn{1}{r}{-0.07,\textcolor{white}{$<$}0.94} 	& \multicolumn{1}{r}{-0.79,\textcolor{white}{$<$}0.43} \\
		0.003       & \multicolumn{1}{c}{32.79} 	& \multicolumn{1}{c}{50.10} 		& \multicolumn{1}{c|}{18.95} 			
		& \multicolumn{1}{r}{-1.24,\textcolor{white}{$<$}0.21} 	& \multicolumn{1}{r}{-1.66,\textcolor{white}{$<$}0.10} \\
		0.007       & \multicolumn{1}{c}{32.71} 	& \multicolumn{1}{c}{50.11} 		& \multicolumn{1}{c|}{18.86} 			
		& \multicolumn{1}{r}{-1.70,\textcolor{white}{$<$}0.09} 	& \multicolumn{1}{r}{-1.42,\textcolor{white}{$<$}0.16} \\
		0.02       	& \multicolumn{1}{c}{32.85} 	& \multicolumn{1}{c}{50.60} 		& \multicolumn{1}{c|}{18.90} 			
		& \multicolumn{1}{r}{-0.63,\textcolor{white}{$<$}0.53} 	& \multicolumn{1}{r}{2.06,\textcolor{white}{$<$}0.04} \\
		0.05	    & \multicolumn{1}{c}{32.95} 	& \multicolumn{1}{c}{50.86} 		& \multicolumn{1}{c|}{19.02} 			
		& \multicolumn{1}{r}{0.07,\textcolor{white}{$<$}0.95} 	& \multicolumn{1}{r}{3.66,$<$0.01} \\
		0.10       	& \multicolumn{1}{c}{33.20} 	& \multicolumn{1}{c}{50.97} 		& \multicolumn{1}{c|}{19.05} 			
		& \multicolumn{1}{r}{1.83,\textcolor{white}{$<$}0.07} 	& \multicolumn{1}{r}{4.37,$<$0.01} \\
		0.25       	& \multicolumn{1}{c}{33.18} 	& \multicolumn{1}{c}{50.88} 		& \multicolumn{1}{c|}{19.05} 			
		& \multicolumn{1}{r}{1.45,\textcolor{white}{$<$}0.15} 	& \multicolumn{1}{r}{3.55,$<$0.01} \\
		0.40       & \multicolumn{1}{c}{33.16} 	& \multicolumn{1}{c}{50.89} 		& \multicolumn{1}{c|}{19.22} 			
		& \multicolumn{1}{r}{1.32,\textcolor{white}{$<$}0.19} 	& \multicolumn{1}{r}{3.64,$<$0.01}
	\end{tabular}
	\vspace{-0.2cm}
	\label{tab:rq2attendgru}
\end{table}

\section{RQ2 - Hyperparameter Tuning}

\subsubsection*{Methodology}
\label{sec:rq2_method}

To answer RQ2, we choose four out of the six models used in RQ1 to study in greater detail: \texttt{attendgru}, \texttt{transformer}, \texttt{codegnngru} and \texttt{ast-attendgru-fc}.
We use these models because they each represent a different family of source code summarizaton models: \texttt{attendgru} from simple seq2seq family, \texttt{transformer} from self-attention architecture family, \texttt{codegnngru} from code + AST represented as GNN in a seq2seq architecture family and \texttt{ast-attendgru-fc} from code + flat AST + contextual information in a seq2seq model family.
For each model we vary the value of $\epsilon$; we start with 0.001 and increase the value by a factor of $e^n$ for n=1 to 6.
Along the way, we also include 0.25 (~0.001$\times e^{5.5}$) for a more granular look as $\epsilon$ increases exponentially for large values of n.
We run each model for these 8 different configurations on the Java-q90 dataset.
We only choose one dataset for RQ2 because we want to eliminate any experimental variables that may be introduced by the dataset, but also due to resource constraints.
We choose the Java-q90 dataset because it is part of a peer-vetted and widely used source code summarization dataset.
To understand how output vocabulary size affects model performance with label smoothing, we then change the size of output vocabulary more than four-fold (from 10k to 44k) and run each model again.
We train each model configuration for eight epochs and choose the model with highest validation accuracy.
Similar to RQ1, we evaluate the performance of these models using automated evaluation techniques discussed in Section~\ref{sec:metrics}.
Additionally we perform a paired t-test between each configuration with label smoothing and the corresponding prediction without label smoothing to identify the statistical significance for METEOR and USE. % as recommended by related work.
Notice again that we do not perform a t-test on BLEU score because BLEU is a corpus level metric.

\begin{table}[!t]
	\small
	\setlength\tabcolsep{5pt}
	\caption{\small{Scores of transformer for different $\epsilon$. Output vocabulary size for the top table is 10k and for the bottom table is 44k.}}
	\vspace{-0.1cm}
	\begin{tabular}{p{0.6cm}|p{0.6cm}p{0.6cm}p{0.6cm}|p{1.5cm}p{1.5cm}}
		& \multicolumn{3}{c|}{\emph{metric scores}} & \multicolumn{2}{c}{\emph{t-stat, p-value}} \\
		$\epsilon$ 	& \multicolumn{1}{c}{\small{METEOR}} 		& \multicolumn{1}{c}{\small{USE}} 	& \multicolumn{1}{c|}{\small{BLEU}}
		& \multicolumn{1}{c}{\small{METEOR}}  & \multicolumn{1}{c}{\small{USE}} \\  \cline{1-6}
		0       & \multicolumn{1}{c}{33.64} & \multicolumn{1}{c}{51.81} & \multicolumn{1}{c|}{18.99} 			
		& \multicolumn{1}{c}{-} 	& \multicolumn{1}{c}{-} \\
		0.001       & \multicolumn{1}{c}{33.53} & \multicolumn{1}{c}{51.46} & \multicolumn{1}{c|}{19.18} 			
		& \multicolumn{1}{r}{-0.83, \textcolor{white}{$<$}0.41} 	& \multicolumn{1}{r}{-2.71, $<$0.01} \\
		0.003       & \multicolumn{1}{c}{33.27} & \multicolumn{1}{c}{51.36} & \multicolumn{1}{c|}{19.06} 			
		& \multicolumn{1}{r}{-2.53, \textcolor{white}{$<$}0.01} 	& \multicolumn{1}{r}{-3.18, $<$0.01} \\
		0.007       & \multicolumn{1}{c}{33.77} & \multicolumn{1}{c}{51.49} & \multicolumn{1}{c|}{19.13} 			
		& \multicolumn{1}{r}{0.81, \textcolor{white}{$<$}0.42} 	& \multicolumn{1}{r}{-2.05, \textcolor{white}{$<$}0.04} \\
		0.02       	& \multicolumn{1}{c}{33.84} & \multicolumn{1}{c}{52.57} & \multicolumn{1}{c|}{19.08} 			
		& \multicolumn{1}{r}{1.22, \textcolor{white}{$<$}0.22} 	& \multicolumn{1}{r}{4.76, $<$0.01} \\
		0.05	    & \multicolumn{1}{c}{33.65} & \multicolumn{1}{c}{51.64} & \multicolumn{1}{c|}{19.11} 			
		& \multicolumn{1}{r}{0.07, \textcolor{white}{$<$}0.95} 	& \multicolumn{1}{r}{-1.14, \textcolor{white}{$<$}0.26} \\
		0.10       	& \multicolumn{1}{c}{33.75} & \multicolumn{1}{c}{52.25} & \multicolumn{1}{c|}{19.11} 			
		& \multicolumn{1}{r}{0.69, \textcolor{white}{$<$}0.49} 	& \multicolumn{1}{r}{2.72, $<$0.01} \\
		0.25       	& \multicolumn{1}{c}{33.72} & \multicolumn{1}{c}{52.27} & \multicolumn{1}{c|}{18.97} 			
		& \multicolumn{1}{r}{0.50, \textcolor{white}{$<$}0.62} 	& \multicolumn{1}{r}{2.96, $<$0.01} \\
		0.40        & \multicolumn{1}{c}{33.84} & \multicolumn{1}{c}{52.10} & \multicolumn{1}{c|}{19.19} 			
		& \multicolumn{1}{r}{1.21, \textcolor{white}{$<$}0.23} 	& \multicolumn{1}{r}{1.73, \textcolor{white}{$<$}0.08}
	\end{tabular}
	\vspace{0.2cm}
	
	\begin{tabular}{p{0.6cm}|p{0.6cm}p{0.6cm}p{0.6cm}|p{1.5cm}p{1.5cm}}
		& \multicolumn{3}{c|}{\emph{metric scores}} & \multicolumn{2}{c}{\emph{t-stat, p-value}} \\
		$\epsilon$ 	& \multicolumn{1}{c}{\small{METEOR}} 		& \multicolumn{1}{c}{\small{USE}} 	& \multicolumn{1}{c|}{\small{BLEU}}
		& \multicolumn{1}{c}{\small{METEOR}}  & \multicolumn{1}{c}{\small{USE}} \\  \cline{1-6}
		0       & \multicolumn{1}{c}{33.28} 	& \multicolumn{1}{c}{51.55} 		& \multicolumn{1}{c|}{18.73} 			
		& \multicolumn{1}{c}{-} 	& \multicolumn{1}{c}{-} \\
		0.001       & \multicolumn{1}{c}{33.38} 	& \multicolumn{1}{c}{50.95} 		& \multicolumn{1}{c|}{18.85} 			
		& \multicolumn{1}{r}{0.62, \textcolor{white}{$<$}0.54} 	& \multicolumn{1}{r}{-3.67, $<$0.01} \\
		0.003       & \multicolumn{1}{c}{33.09} 	& \multicolumn{1}{c}{51.23} 		& \multicolumn{1}{c|}{18.77} 			
		& \multicolumn{1}{r}{-1.21, \textcolor{white}{$<$}0.23} 	& \multicolumn{1}{r}{-2.11, \textcolor{white}{$<$}0.04} \\
		0.007       & \multicolumn{1}{c}{33.30} 	& \multicolumn{1}{c}{51.55} 		& \multicolumn{1}{c|}{18.75} 			
		& \multicolumn{1}{r}{-0.83, \textcolor{white}{$<$}0.41} 	& \multicolumn{1}{r}{0.01, \textcolor{white}{$<$}0.99} \\
		0.02       	& \multicolumn{1}{c}{33.30} 	& \multicolumn{1}{c}{51.82} 		& \multicolumn{1}{c|}{18.85} 			
		& \multicolumn{1}{r}{0.13, \textcolor{white}{$<$}0.90} 	& \multicolumn{1}{r}{1.71, \textcolor{white}{$<$}0.09} \\
		0.05	    & \multicolumn{1}{c}{33.32} 	& \multicolumn{1}{c}{51.74} 		& \multicolumn{1}{c|}{18.77} 			
		& \multicolumn{1}{r}{0.27, \textcolor{white}{$<$}0.79} 	& \multicolumn{1}{r}{1.21, \textcolor{white}{$<$}0.22} \\
		0.10       	& \multicolumn{1}{c}{33.77} 	& \multicolumn{1}{c}{52.17} 		& \multicolumn{1}{c|}{18.94} 			
		& \multicolumn{1}{r}{3.04, $<$0.01} 	& \multicolumn{1}{r}{3.89, $<$0.01} \\
		0.25       	& \multicolumn{1}{c}{33.84} 	& \multicolumn{1}{c}{52.51} 		& \multicolumn{1}{c|}{19.22} 			
		& \multicolumn{1}{r}{3.40, $<$0.01} 	& \multicolumn{1}{r}{5.95, $<$0.01} \\
		0.40       & \multicolumn{1}{c}{34.23} 	& \multicolumn{1}{c}{52.27} 		& \multicolumn{1}{c|}{19.56} 			
		& \multicolumn{1}{r}{5.57, $<$0.01} 	& \multicolumn{1}{r}{4.33, $<$0.01}
	\end{tabular}
	\vspace{-0.4cm}
	\label{tab:rq2transformer}
\end{table}


\subsubsection*{Key Findings}

Our key finding in answering RQ2 is that higher values of $\epsilon$ generally lead to higher model performance, although this increase in performance is less pronounced for $\epsilon>$ 0.1.
Furthermore, this trend is unaffected as we increase the vocabulary size from 10k to 44k.
Therefore, the value of the smoothed probability per token ($\frac{\epsilon}{N_t-1}$) does not affect model performance.
While one must decide the best hyperparameters for oneself based on prevailing experimental conditions, our recommendation is to set $\epsilon$=0.1 for initial evaluation.
%Additional evaluation of these models for higher values of $\epsilon$ might then be worthwhile to squeeze out the highest metric score.
Caution is advised for high values of $\epsilon$ as it forces models to predict more common words and eliminate rare/unique words.
We discuss this issue in greater detail in RQ3.

Table~\ref{tab:rq2attendgru} shows the effect of increasing the label smoothing factor, $\epsilon$ for attendgru model.
For this architecture, we notice an increase in model performance on all metrics as we increase the value of $\epsilon$ from 0.001 to 0.1.
METEOR, USE and BLEU scores all achieve the highest score for this configuration of $\epsilon$=0.1.
%While performance dips slightly as we further increase $\epsilon$ to 0.25 and 0.4, it is still significantly higher than the model instance with no label smoothing ($\epsilon$=0).
As we increase the output vocabulary size from 10k to 44k, we notice that both METEOR and USE scores decrease slightly for small values of $\epsilon$ but then they increase, reaching highest METEOR and USE score for $\epsilon$=0.1.

\begin{table}[!t]
	\small
	\setlength\tabcolsep{5pt}
	\caption{\small{Scores of codegnngru for different $\epsilon$. Output vocabulary size for the top table is 10k and for the bottom table is 44k.}}
	\vspace{-0.1cm}
	\begin{tabular}{p{0.6cm}|p{1.0cm}p{1.0cm}p{1.0cm}|p{1.5cm}p{1.5cm}}
		& \multicolumn{3}{c|}{\emph{metric scores}} & \multicolumn{2}{c}{\emph{t-stat, p-value}} \\
		$\epsilon$ 	& \multicolumn{1}{c}{\small{METEOR}} 		& \multicolumn{1}{c}{\small{USE}} 	& \multicolumn{1}{c|}{\small{BLEU}}
		& \multicolumn{1}{c}{\small{METEOR}}  & \multicolumn{1}{c}{\small{USE}} \\  \cline{1-6}
		0	        & \multicolumn{1}{c}{32.30} & \multicolumn{1}{c}{49.37} & \multicolumn{1}{c|}{18.00} 			
		& \multicolumn{1}{c}{-} 	& \multicolumn{1}{c}{-} \\
		0.001       & \multicolumn{1}{c}{32.53} & \multicolumn{1}{c}{49.94} & \multicolumn{1}{c|}{18.22} 			
		& \multicolumn{1}{r}{1.55, \textcolor{white}{$<$}0.12} 	& \multicolumn{1}{r}{3.64, $<$0.01} \\
		0.003       & \multicolumn{1}{c}{32.46} & \multicolumn{1}{c}{49.76} & \multicolumn{1}{c|}{18.24} 			
		& \multicolumn{1}{r}{0.98, \textcolor{white}{$<$}0.33} 	& \multicolumn{1}{r}{2.30, \textcolor{white}{$<$}0.02} \\
		0.007       & \multicolumn{1}{c}{32.18} & \multicolumn{1}{c}{49.69} & \multicolumn{1}{c|}{17.96} 			
		& \multicolumn{1}{r}{-0.79, \textcolor{white}{$<$}0.43} 	& \multicolumn{1}{r}{2.06, \textcolor{white}{$<$}0.04} \\
		0.02       	& \multicolumn{1}{c}{32.53} & \multicolumn{1}{c}{49.88} & \multicolumn{1}{c|}{18.42} 			
		& \multicolumn{1}{r}{1.40, \textcolor{white}{$<$}0.16} 	& \multicolumn{1}{r}{3.02, $<$0.01} \\
		0.05	    & \multicolumn{1}{c}{32.65} & \multicolumn{1}{c}{49.92} & \multicolumn{1}{c|}{18.59} 			
		& \multicolumn{1}{r}{2.16, \textcolor{white}{$<$}0.03} 	& \multicolumn{1}{r}{3.22, $<$0.01} \\
		0.10       	& \multicolumn{1}{c}{33.17} & \multicolumn{1}{c}{50.49} & \multicolumn{1}{c|}{18.76} 			
		& \multicolumn{1}{r}{5.46, $<$0.01} 	& \multicolumn{1}{r}{6.82, $<$0.01} \\
		0.25       	& \multicolumn{1}{c}{33.36} & \multicolumn{1}{c}{51.08} & \multicolumn{1}{c|}{18.97} 			
		& \multicolumn{1}{r}{6.65, \textcolor{white}{$<$}0.01} 	& \multicolumn{1}{r}{10.19, $<$0.01} \\
		0.40        & \multicolumn{1}{c}{33.37} & \multicolumn{1}{c}{51.35} & \multicolumn{1}{c|}{19.00} 			
		& \multicolumn{1}{r}{6.51, $<$0.01} 	& \multicolumn{1}{r}{11.79, $<$0.01}
	\end{tabular}
	\vspace{0.2cm}
	
	\begin{tabular}{p{0.6cm}|p{0.6cm}p{0.6cm}p{0.6cm}|p{1.5cm}p{1.5cm}}
		& \multicolumn{3}{c|}{\emph{metric scores}} & \multicolumn{2}{c}{\emph{t-stat, p-value}} \\
		$\epsilon$ 	& \multicolumn{1}{c}{\small{METEOR}} 		& \multicolumn{1}{c}{\small{USE}} 	& \multicolumn{1}{c|}{\small{BLEU}}
		& \multicolumn{1}{c}{\small{METEOR}}  & \multicolumn{1}{c}{\small{USE}} \\  \cline{1-6}
		0	        & \multicolumn{1}{c}{32.26} 	& \multicolumn{1}{c}{49.33} 		& \multicolumn{1}{c|}{18.20} 			
		& \multicolumn{1}{c}{-} 	& \multicolumn{1}{c}{-} \\
		0.001       & \multicolumn{1}{c}{32.56} 	& \multicolumn{1}{c}{49.85} 		& \multicolumn{1}{c|}{18.22} 			
		& \multicolumn{1}{r}{1.87, \textcolor{white}{$<$}0.06} 	& \multicolumn{1}{r}{3.31, $<$0.01} \\
		0.003       & \multicolumn{1}{c}{32.25} 	& \multicolumn{1}{c}{49.62} 		& \multicolumn{1}{c|}{18.19} 			
		& \multicolumn{1}{r}{-0.07, \textcolor{white}{$<$}0.94} 	& \multicolumn{1}{r}{1.77, \textcolor{white}{$<$}0.08} \\
		0.007       & \multicolumn{1}{c}{32.75} 	& \multicolumn{1}{c}{49.68} 		& \multicolumn{1}{c|}{18.57} 			
		& \multicolumn{1}{r}{3.02, $<$0.01} 	& \multicolumn{1}{r}{2.10, \textcolor{white}{$<$}0.04} \\
		0.02       	& \multicolumn{1}{c}{32.36} 	& \multicolumn{1}{c}{49.76} 		& \multicolumn{1}{c|}{18.34} 			
		& \multicolumn{1}{r}{0.62, \textcolor{white}{$<$}0.53} 	& \multicolumn{1}{r}{2.68, $<$0.01} \\
		0.05	    & \multicolumn{1}{c}{32.86} 	& \multicolumn{1}{c}{50.31} 		& \multicolumn{1}{c|}{18.53} 			
		& \multicolumn{1}{r}{3.65, $<$0.01} 	& \multicolumn{1}{r}{5.98, $<$0.01} \\
		0.10       	& \multicolumn{1}{c}{32.71} 	& \multicolumn{1}{c}{49.97} 		& \multicolumn{1}{c|}{18.48} 			
		& \multicolumn{1}{r}{2.64, $<$0.01} 	& \multicolumn{1}{r}{3.77, $<$0.01} \\
		0.25       	& \multicolumn{1}{c}{32.68} 	& \multicolumn{1}{c}{49.92} 		& \multicolumn{1}{c|}{18.53} 			
		& \multicolumn{1}{r}{2.51, \textcolor{white}{$<$}0.01} 	& \multicolumn{1}{r}{3.53, $<$0.01} \\
		0.40       & \multicolumn{1}{c}{32.88} 	& \multicolumn{1}{c}{50.50} 		& \multicolumn{1}{c|}{18.72} 			
		& \multicolumn{1}{r}{3.72, $<$0.01} 	& \multicolumn{1}{r}{7.11, $<$0.01}
	\end{tabular}
	\vspace{-0.4cm}
	\label{tab:rq2codegnngru}
\end{table}

Table~\ref{tab:rq2transformer} shows the effect of increasing $\epsilon$ for transformers.
We initially see a decrease in metric scores for small values of $\epsilon$.
As we increase $\epsilon$, the metric scores start to increase.
Interestingly, for the 44k output vocabulary, we see a substantially significant increase in metric scores for all values of $\epsilon >$ 0.05.
We attribute this to the fact that higher output vocabulary size increases the amount of rare words in the prediction. 
Label smoothing helps models generalize by focusing on more commonly occuring words.
%This finding is further validated in RQ3.

Table~\ref{tab:rq2codegnngru} shows the effect of increasing $\epsilon$ for codegnngru.
For the 10k output vocabulary set, we notice a pattern similar to attendgru and transformers.
%The metrics scores change slightly for small values of $\epsilon$.
While USE shows a statistically significant improvement for all cases of $\epsilon$, METEOR only starts demonstrating a statistically significant improvement for all values of $\epsilon >$ 0.05.
For the 44k output vocabulary set, we see an improvement across the board, that is explained by the model eliminating noise, introduced by the large output vocabulary, set using label smoothing.

Table~\ref{tab:rq2astattendgrufc} shows the effect of increasing $\epsilon$ for astattendgru-fc.
For the 10k output vocabulary set, we see a statistically significant decrease in the model performance for $\epsilon$=0.001.
For $\epsilon$=0.003, we see a slight increase in metric scores, although it is not statistically significant.
For all but one other configuration, we see a statistically significant increase in metric scores for both METEOR and USE.
We see a similar pattern for the 44k output vocabulary set as the metric scores change insignificantly for $\epsilon$=0.001 and $\epsilon$=0.003 but then achieve high improvement for all other configurations.
Note that we do not recommend any particular model for the problem of source code summarization.
Rather we demonstrate how all existing models benefit in performance from adding label smoothing.

\begin{table}[!t]
	\small
	\caption{\small{Scores of astattendgru-fc for different $\epsilon$. Output vocabulary size for the top table is 10k and for the bottom is 44k.}}
	\vspace{-.1cm}
	\setlength\tabcolsep{5pt}
	\begin{tabular}{p{0.6cm}|p{0.6cm}p{0.6cm}p{0.6cm}|p{1cm}p{1cm}}
		& \multicolumn{3}{c|}{\emph{metric scores}} & \multicolumn{2}{c}{\emph{t-stat, p-value}} \\
		$\epsilon$ 	& \multicolumn{1}{c}{\small{METEOR}} 		& \multicolumn{1}{c}{\small{USE}} 	& \multicolumn{1}{c|}{\small{BLEU}}
		& \multicolumn{1}{c}{\small{METEOR}}  & \multicolumn{1}{c}{\small{USE}} \\  \cline{1-6}
		0	        & \multicolumn{1}{c}{33.09} & \multicolumn{1}{c}{50.05} & \multicolumn{1}{c|}{18.76}
		& \multicolumn{1}{c}{-} 	& \multicolumn{1}{c}{-} \\
		0.001       & \multicolumn{1}{c}{32.74} & \multicolumn{1}{c}{49.53} & \multicolumn{1}{c|}{18.66} 			
		& \multicolumn{1}{r}{-2.26, \textcolor{white}{$<$}0.02} 	& \multicolumn{1}{r}{-3.24, $<$0.01} \\
		0.003       & \multicolumn{1}{c}{33.32} & \multicolumn{1}{c}{50.23} & \multicolumn{1}{c|}{19.07} 			
		& \multicolumn{1}{r}{1.40, \textcolor{white}{$<$}0.16} 	& \multicolumn{1}{r}{1.04, \textcolor{white}{$<$}0.30} \\
		0.007       & \multicolumn{1}{c}{33.62} & \multicolumn{1}{c}{50.60} & \multicolumn{1}{c|}{19.08} 			
		& \multicolumn{1}{r}{3.26, $<$0.01} 	& \multicolumn{1}{r}{3.25, $<$0.01} \\
		0.02       	& \multicolumn{1}{c}{33.49} & \multicolumn{1}{c}{50.16} & \multicolumn{1}{c|}{19.18} 			
		& \multicolumn{1}{r}{2.40, \textcolor{white}{$<$}0.02} 	& \multicolumn{1}{r}{0.64, \textcolor{white}{$<$}0.52} \\
		0.05	    & \multicolumn{1}{c}{33.50} & \multicolumn{1}{c}{50.81} & \multicolumn{1}{c|}{19.41} 			
		& \multicolumn{1}{r}{2.47, \textcolor{white}{$<$}0.01} 	& \multicolumn{1}{r}{4.56, $<$0.01} \\
		0.10       	& \multicolumn{1}{c}{33.66} & \multicolumn{1}{c}{50.91} & \multicolumn{1}{c|}{19.38} 			
		& \multicolumn{1}{r}{3.47, $<$0.01} 	& \multicolumn{1}{r}{5.10, $<$0.01} \\
		0.25       	& \multicolumn{1}{c}{33.80} & \multicolumn{1}{c}{50.88} & \multicolumn{1}{c|}{19.54} 			
		& \multicolumn{1}{r}{4.21, $<$0.01} 	& \multicolumn{1}{r}{4.76, $<$0.01} \\
		0.40        & \multicolumn{1}{c}{33.98} & \multicolumn{1}{c}{51.22} & \multicolumn{1}{c|}{19.58} 			
		& \multicolumn{1}{r}{5.48, $<$0.01} 	& \multicolumn{1}{r}{7.06, $<$0.01}
	\end{tabular}
	\vspace{0.2cm}
	
	\begin{tabular}{p{0.6cm}|p{0.6cm}p{0.6cm}p{0.6cm}|p{1cm}p{1cm}}
		& \multicolumn{3}{c|}{\emph{metric scores}} & \multicolumn{2}{c}{\emph{t-stat, p-value}} \\
		$\epsilon$ 	& \multicolumn{1}{c}{\small{METEOR}} 		& \multicolumn{1}{c}{\small{USE}} 	& \multicolumn{1}{c|}{\small{BLEU}}
		& \multicolumn{1}{c}{\small{METEOR}}  & \multicolumn{1}{c}{\small{USE}} \\  \cline{1-6}
		0       & \multicolumn{1}{c}{32.69} 	& \multicolumn{1}{c}{49.87} 		& \multicolumn{1}{c|}{18.65} 
		& \multicolumn{1}{c}{-} 	& \multicolumn{1}{c}{-} \\
		0.001       & \multicolumn{1}{c}{33.03} 	& \multicolumn{1}{c}{49.54} 		& \multicolumn{1}{c|}{18.96} 			
		& \multicolumn{1}{r}{2.06, \textcolor{white}{$<$}0.04} 	& \multicolumn{1}{r}{-1.97, \textcolor{white}{$<$}0.05} \\
		0.003       & \multicolumn{1}{c}{32.89} 	& \multicolumn{1}{c}{50.19} 		& \multicolumn{1}{c|}{18.85} 			
		& \multicolumn{1}{r}{1.24, \textcolor{white}{$<$}0.21} 	& \multicolumn{1}{r}{1.93, \textcolor{white}{$<$}0.05} \\
		0.007       & \multicolumn{1}{c}{33.36} 	& \multicolumn{1}{c}{50.57} 		& \multicolumn{1}{c|}{19.07} 			
		& \multicolumn{1}{r}{4.13, $<$0.01} 	& \multicolumn{1}{r}{4.31, $<$0.01} \\
		0.02       	& \multicolumn{1}{c}{33.69} 	& \multicolumn{1}{c}{50.73} 		& \multicolumn{1}{c|}{19.24} 			
		& \multicolumn{1}{r}{6.15, $<$0.01} 	& \multicolumn{1}{r}{5.23, $<$0.01} \\
		0.05	    & \multicolumn{1}{c}{33.58} 	& \multicolumn{1}{c}{50.92} 		& \multicolumn{1}{c|}{19.39} 			
		& \multicolumn{1}{r}{5.42, $<$0.01} 	& \multicolumn{1}{r}{6.37, $<$0.01} \\
		0.10       	& \multicolumn{1}{c}{33.75} 	& \multicolumn{1}{c}{51.37} 		& \multicolumn{1}{c|}{19.35} 			
		& \multicolumn{1}{r}{6.34, $<$0.01} 	& \multicolumn{1}{r}{8.77, $<$0.01} \\
		0.25       	& \multicolumn{1}{c}{34.27} 	& \multicolumn{1}{c}{51.95} 		& \multicolumn{1}{c|}{19.75} 			
		& \multicolumn{1}{r}{9.76, $<$0.01} 	& \multicolumn{1}{r}{12.77, $<$0.01} \\
		0.40       & \multicolumn{1}{c}{33.84} 	& \multicolumn{1}{c}{51.21} 		& \multicolumn{1}{c|}{19.46} 			
		& \multicolumn{1}{r}{7.21, $<$0.01} 	& \multicolumn{1}{r}{8.18, $<$0.01}
	\end{tabular}
\vspace{-0.4cm}
\label{tab:rq2astattendgrufc}
\end{table}