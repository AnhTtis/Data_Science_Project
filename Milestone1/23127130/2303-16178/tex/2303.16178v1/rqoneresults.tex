\section{RQ1 - Overall Performance}

This section discusses the methodology for answering RQ1 as well as our key findings.  In general, we find that label smoothing leads to improvements to several baselines.

\input rqonetable

\subsubsection*{Methodology}

To answer RQ1, we follow the established methodology that have become standard in source code summarization and NMT tasks in NLP~\cite{leclair2019neural, vaswani2017attention}.
We use two standard datasets in two common programming languages.
We perform further filtration on the dataset by taking the top 10\% largest methods from the Java dataset (Java-q90) and the top-25\% largest methods from the C/C++ dataset (C/C++-q75).
We do this to find functions that have a large number of statements, which is more representative of real world use-case scenario.
We also train the models on the full 1.9m Java dataset to evaluate model performance on a full dataset to make commensurable comparison with related work.
For all three dataset instances, we train each baseline twice: once with and again without label smoothing.
We keep the value of $\epsilon$ constant (0.1) for all models.
For each architecture, we train both instances (with and without label smoothing) for 10 epochs and choose the model with the highest validation accuracy score for our comparison (standard practice in related work).
We then evaluate the performance of these models using automated evaluation techniques discussed in Section~\ref{sec:metrics}.
Roy~\emph{et al.} at FSE'21 recommended to perform a paired t-test between baseline model predictions and new model predictions for different sentence-level metrics.
Therefore, we perform a paired t-test to identify if the performance difference between two comparing model predictions is statistically significant.
Note that we do not perform a t-test on BLEU score because BLEU is a corpus level metric.
%The test is paired because the prediction files are related and one-sided because we are only interested if the performance of a model improves with label smoothing.

\subsubsection*{Key Findings}

Our key finding in answering RQ1 is that adding label smoothing as a regularizer improves model performance in most cases.
All models show performance improvement in terms of USE for Java-q90, C/C++-q75 and full Java dataset.
All models using the Java-q90 and C/C++-q75 datasets and most models using the full Java dataset also report performance improvement in term of METEOR and BLEU.
Furthermore, most of the performance improvements are statistically significant (we choose $\alpha=0.05$).
This overall trend of performance improvement is depicted in Figure~\ref{fig:rq1}.
The dark blue bar shows the METEOR scores for models without label smoothing.
The orange bar on top shows the increase in the METEOR score after we add label smoothing.
As the figures show, there is an increase in METEOR score in almost all cases.

Table~\ref{tab:rq1q90} shows the effect of label smoothing on the overall performance of each baseline for the Java-q90 dataset.
We find that all models report higher evaluation metric score with the addition of label smoothing. 
However, Code2seq improves the most with 4.8\% improvement on METEOR, 5.4\% improvement on USE and 8.3\% improvement on BLEU.
Codegnngru also shows significant improvement with 2.7\% improvement on METEOR, 2.3\% improvement on USE and 4.2\% improvement on BLEU.
Attendgru and ast-attendgru-fc also show similar improvement with 1.7\% improvement for both on METEOR, 2.4\% and 1.7\% improvement on USE and 3.1\% and 3.3\% improvement on BLEU respectively.
Each of these models vary with respect to encoder input.
Each model has different inputs as well as different data structure for common inputs.
The higher scores are encouraging as we observe statistically significant performance improvement across different model architectures.
Transformers, however, show the least overall improvement in performance with an increment of less than 1\% for all metrics, although the 0.85\% increase in USE score is statistically significant.
One possible explanation for this result is that Transformers have built-in dropout layers after each multi-head attention layer.  These dropout layers likely already improve regularization.

Table~\ref{tab:rq1q75} shows the effect of label smoothing on the overall performance of each baseline for the C/C++-q75 dataset.
For this dataset, we notice a large improvement in performance for ast-attendgru, code2seq, codegnngru and ast-attendgru-fc baselines.
These performance increase range from 7.2\%-8.5\% for METEOR, 5.6\%-10.3\% for USE and 8.3\%-10.2\% for BLEU.
It is interesting to note that attendgru does not show a large percentage increase in performance ($<$2\% across all metrics) for this dataset, compared to the aforementioned models.
Furthermore, similar to the Java-q90 dataset, transformers also show little improvement (about 1-2\% across all metrics).
We expect this result for transformers due to the built-in dropout regularizer in the transformer architecture.
However, the performance increase for both attendgru and transformers is still statistically significant with p-values $<$0.01 for both METEOR and USE.

Table~\ref{tab:rq1full} shows the effect of label smoothing on the overall performance of each baseline for the full Java dataset of 1.9m methods.
Again we see a trend of performance improvement in most cases.
However, while the increase in metric score is significant in all but two configurations, we do notice that the percentage increment is not as high as for the Java-q90 and C/C++-q75 dataset ($<$2\% across all metrics for all models).
We attribute this to the fact that higher training examples act as a regularizer in and of itself.
For attendgru, we notice a statistically significant decrease in METEOR score but a statistically significant increase in USE score.
We also notice a small decrease in BLEU score.
Since all three metrics do not point in one way or another, we do not draw any conclusion for this setup.
For codegnngru, we notice a 0.11\% decrease in BLEU score, a 0.22\% increase in METEOR score which is not statistically significant but a 0.47\% increase in USE score which is statistically significant.
Since the two metric scores that show performance improvement are also correlated best with human evaluation, we conclude that label smoothing positively affects codegnngru.
Once again, code2seq shows the highest performance increase for the Java full dataset.
One likely explanation for why code2seq achieves the highest performance increase is that it is the largest model among the baselines.  However, its parameters do require more time to train than others.
Like for other models, label smoothing appears to help code2seq generalize.% because of this.

%\newpage