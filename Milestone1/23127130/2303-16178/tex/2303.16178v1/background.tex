\section{Background \& Related Work}

In this section, we discuss the key background ideas and supporting work related to this paper, namely source code summarization, neural encoder-decoder architecture, and label smoothing.

\vspace{-0.1cm}
\subsection{Source Code Summarization}
\label{sec:backgroundscs}
Early work in source code summarization included heuristic-based approaches~\cite{sridhara2011automatically, mcburney2014automatic}.
These models relied on techniques of Information Retrieval (IR) to extract salient words from source code~\cite{de2012using, rodeghero2014improving}.
These words were then put into manually-defined templates to produce meaningful sentences~\cite{moreno2014automatic, zhang2016towards}.

\input backgroundTable

The landscape changed with the advent of deep learning.
The explosion of data-driven models in the mid-2010's and their state-of-the-art performance in various natural language processing (NLP) tasks inspired researchers to use them to automatically generate source code summaries.
Iyer~\emph{et al.}~\cite{iyer2016summarizing} was one of the earliest to use such models for code summarization.
Since then, the field has embraced this sequence-to-sequence (seq2seq) architecture as the standard architecture to generate comments.
Figure~\ref{tab:screlated} lists some of the most prominent papers that use some modified version of an attentional neural encoder-decoder model to generate code summaries.
This list is not exhaustive, but only includes peer-reviewed papers where a new idea was first introduced.

The table shows that the first generation of these data-driven approaches (marked only in column $N$) only looked at the source code tokens to generate comments.
Later, Hu~\emph{et al.}~\cite{hu2018deep} and LeClair~\emph{et al.}~\cite{leclair2019neural} noted the importance of including structural information about the source code by using the Abstract Syntax Tree (AST).
They both flattened the AST into sequential tokens, but incorporated these tokens in different ways in their model.
Other research papers soon explored various ways of capturing these structural information from the AST~\cite{allamanis2018learning, alon2019code2seq, leclair2020improved, fernandes2018structured}.
At the same time, a parallel research track delved into incorporating contextual information.
This context encompasses API calls to learn the mapping between API sequences and natural language description~\cite{hu2018summarizing} as well as other functions in the file to provide supporting information for the code~\cite{haque2020improved}.
Bansal~\emph{et al.} further expanded the latter idea by including project context information~\cite{bansal2021project}.
Recently, some research has been dedicated into using transformer models~\cite{vaswani2017attention} for this task.
Ahmad~\emph{et al.}~\cite{ahmad2020transformer} used pairwise relationship between tokens to capture their mutual interaction beyond positional encoding while Gong~\emph{et al.}~\cite{gong2022source} introduces another layer in the encoder side of the transformer for the AST.

This paper aims to improve the performance of these data-driven models by exploring how label smoothing can enhance their performance.
We show how the performance of different established baselines improves using label smoothing.
Note that we do not seek to compete with any one code summarization approach -- our aim is to benefit all approaches.

\vspace{-0.1cm}
\subsection{Neural Encoder-Decoder Architecture}
\label{sec:backgroundnmt}
The neural encoder-decoder architecture is the backbone of almost all current code summarization approaches~\cite{haque2021action}.
These models are borrowed from the field of Neural Machine Translation (NMT).
Essentially, these models have 2 parallel components: the encoder and the decoder.
The encoder takes words (tokens) from the input language (e.g. English for NMT/Java for source code summarization) and represents them into fixed length vectors.
The decoder takes this vector representation and translates them to the target language (e.g. Spanish for NMT/English for source code summarization).

The standard setup for these encoder-decoder models use recurrent layers on both sides~\cite{sutskever2011generating}.
% Cite original LSTM paper here
These recurrent layers are typically a sequence of LSTM~\cite{hochreiter1997lstm} or GRU~\cite{cho2014learning} cells.
Information propagates through these layers, one token per cell.
Each cell in the encoder layer not only receives the corresponding token, but also the vector representation of all the tokens that came before.
The first cell in decoder layer receives the final vector representation of the entire input sequence~\cite{sutskever2014sequence}.
Consecutive cells in the decoder layer receive a vector that encapsulates not only the entire input sequence but also the output so far.

While this architecture has existed for some time, Bahdanau~\emph{et al.}~\cite{bahdanau2014neural} enhanced its performance by introducing attention in 2014. 
The intuition behind attention is that some tokens in the input sequence are more important than others when trying to generate specific predictions.
The goal of attention is to map the relative importance of each input token to every output prediction.
It does this by computing the similarity between the input sequence and the output sequence so far to identify the important features to predict the next word.
The two most common attention functions are additive and multiplicative (dot-product based).
In all the models evaluated in this paper, we use multiplicative attention.

The dot-product based attention inspired a new generation of neural encoder-decoder architecture called Transformers, that eschews the sequential nature of these models.
First introduced by Vaswani~\emph{et al.}~\cite{vaswani2017attention}, these models replace the RNN cells with a self-attention layer (discussed in greater detail in section~\ref{sec:baselines}) followed by a fully connected feed-forward layer.
Without the recurrent layer to propagate sequential information, transformers introduce a positional encoding before the encoder and decoder respectively to capture the order of the sequence.
This allows for parallel processing of tokens, making them faster.
These models have also been shown to better capture long-range relationships~\cite{vaswani2017attention}.

%\vspace{-0.1cm}
\subsection{Label Smoothing}
\label{sec:backgroundls}
Label smoothing is a regularization technique used in neural networks to improve generalization.
It was first introduced by Szegedy~\emph{et al.}~\cite{szegedy2015rethinking} for image classification, although it has since been shown to improve performance for many other deep learning tasks, including NMT~\cite{muller2019does}.
Essentially, label smoothing involves introducing some uncertainty in the training data to prevent over-fitting~\cite{lienen2021label, lukasik2020does, zhang2021delving}.

Most Natural Language Generation (NLG) models use categorical cross-entropy loss to calculate the error between target tokens and predicted tokens.
Minimizing this loss function means reducing the error between the target and predicted tokens, thus improving model performance.
The target distribution ($t(k)$) for a neural network is a Dirac delta function: 
\[t(k) = \delta_{k, y}\] 
where $k$ is the predicted output and $y$ is the target output and $t(k)$ is 1 when $k=y$ and 0 for all other $k$.
In practice, we use a one-hot vector to represent the Dirac delta function.
The size of the vector is the number of tokens in the output vocabulary ($N_t$), with all elements set to 0 except for the index of the target token, which is set to 1.
We can therefore read it as a probability distribution that is 1 for the correct target word and 0 for all other words in the vocabulary.
This is undesirable because it makes the model overconfident about certain predictions.

With label smoothing, we encourage the model to be ``less confident.''
We achieve this, in essence, by taking some probability, $\epsilon$, off the top of the target token, $y$ and uniformly distributing over the rest of the vocabulary.
This probability, $\epsilon$ where $\epsilon\in(0,1]$, is the key parameter that determines the extent to which label smoothing will affect model performance.
We represent this new distribution as:
\[t(k) = (1-\epsilon)\delta_{k, y}+(1-\delta_{k, y})\frac{\epsilon}{N_t-1}\]
where $t(k)$ is now $(1-\epsilon)$ for $k=y$ and $\frac{\epsilon}{N_t-1}$ for $k\neq y$.

The current research frontier in source code summarization lies in enhancing the neural networks to generate better comments.
This paper lies at the heart of this frontier by not only quantifying the regularization effect of these models with label smoothing but also identifying the best configuration to maximize model performance.