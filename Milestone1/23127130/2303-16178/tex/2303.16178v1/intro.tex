\section{Introduction}

The backbone of much software documentation is the ``source code summary''~\cite{haiduc2010use, sridhara2010towards, kramer1999api}.  
A summary is a short description in natural language that provides high level information about the purpose and behavior of the low level details implemented in source code.  
The idea is that a programmer can understand the functionality of a section of code by reading the corresponding summary without reading the code itself~\cite{forward2002relevance, xia2017measuring}.  
The expense of writing these summaries by hand has long made automated code summarization a ``holy grail''~\cite{leclair2019neural} of software engineering research~\cite{zhao2020survey}.  
The dream is that programmers could read code documentation even if other programmers did not write any.

The state-of-the-art in code summarization research depends on the neural encoder-decoder architecture~\cite{leclair2019neural, zugner2021languageagnostic, hu2018deep}.  
Basically the idea is that an encoder forms a representation of source code in a vector space, while the decoder forms a representation of the summary in a different vector space. 
With sufficient training data (usually millions of samples~\cite{allamanis2018survey, leclair2019recommendations}), another part of the model learns to connect features in one space to the other and can be used to predict output summaries for arbitrary input source code.  
Neural designs based on the encoder-decoder model have almost completely supplanted earlier template- and heuristic-based approaches~\cite{zhao2020survey}.

These neural models generate summaries one word at a time (usually using the teacher forcing procedure~\cite{toomarian1992learning} in a seq2seq-like design~\cite{sutskever2014sequence, zhao2020survey}).  In a nutshell, the model is shown the source code and a start of sequence token for the desired output summary.  The model then predicts the first word of the output summary.  Then the model is shown the source code and the start of sequence token plus the first predicted word, then predicts the second word.  This process continues until the model predicts an end of sequence token.  The point is that the model is tasked with predicting a single word several times -- not the whole output summary at once.  Each individual word prediction is a weakpoint: if the model makes an error, the subsequent predictions are also likely to be wrong because they depend on the previous one~\cite{haque2021action}.  If the model is ``overconfident'' in predicting some words, it may develop a tendency to miss rarer words and produce repetitive, dull outputs~\cite{jiang2018sequence}.

A solution proffered in several areas of natural language generation is \emph{label smoothing}~\cite{muller2019does, lukasik2020does, yuan2020revisiting}.  Label smoothing seeks to make the model ``less confident'' in each prediction by altering the target output.  The neural model's output is a predicted probability distribution over the entire vocabulary of words.  If the vocabulary has 10,000 words, then the output is technically a 10,000-length vector, in which the position of the correct word will hopefully have the highest value.  During training, the target vector would have a 1 in the position of the correct word, and a 0 in the other 9,999 positions.  What label smoothing does is reduce the value in the correct position slightly, say to 0.95, then spread the remainder over the rest of the distribution, so all other positions would be e.g., (0.05/9999).  While the mechanism by which label smoothing works is not fully understood~\cite{lukasik2020does}, different empirical studies have repeatedly shown it to be effective~\cite{muller2019does}.

In this paper, we present an empirical study on label smoothing for neural source code summarization.  Label smoothing requires considerable tuning to achieve optimal results in different domains.  While it is reasonable to hypothesize that label smoothing would improve neural code summarization given the similarities between neural code summarization and other natural language generation technologies, the hypothesis has not been tested, and effective parameters and procedures have not been established.  We conduct an experiment of label smoothing for several baselines from the source code summarization literature, to serve guide for future researchers.

%The training procedure behind these neural models requires an optimization function.  An optimization/loss function in a neural model is a function whose input is the model's prediction and the reference answer, and output is a score of how accurate the model's prediction is.  By far the most common in code summarization is cross entropy, which is basically just a distance measure of the predicted probability distribution to the reference distribution.  The distance acts like a penalty for incorrect predictions, and is then used to propagate error to the neural network and update network weights.  Loss functions tend to be a weak point in neural models because the model becomes highly optimized to these functions, so will reflect any problems with the loss function. % In general, efforts to improve optimization functions focus on increasing or decreasing penalties based on various factors that are believed to influence the quality of the results e.g.~\cite{wan2018improving}.

%An obvious shortcoming to current approaches is that they insist on a perfect match between the predicted and reference probability.  If a reference word is ``deletes'' and the model predicts ``removes'', the model is penalized to the same degree as if it had predicted the word ``adds.''  This problem is well-known in several corners of NLP research; Wieting~\emph{et al.}~\cite{wieting-etal-2019-beyond} describe it as the model not receiving ``partial credit'' for partially correct predictions.  Current neural code summarization approaches tend to focus on improved model designs and input data, while depending on the same perfect match-based optimization procedure.

%In this paper, we present...

%In our evaluation, we demonstrate...

