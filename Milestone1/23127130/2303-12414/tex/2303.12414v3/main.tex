\pdfoutput=1
 % \documentclass[12pt, journal,onecolumn,draftclsnofoot]{IEEEtran}
\documentclass[12pt, draftclsnofoot, onecolumn]{IEEEtran}
% \documentclass[journal]{IEEEtran}
\IEEEoverridecommandlockouts
% \usepackage[left=0.67in,right=0.67in,top=0.7in,bottom=1.1in]{geometry}
\usepackage{amsmath,graphicx,epstopdf,amssymb,amsthm}% ,epstopdf,spconf,amssymb,epsfig,fullpage}%, ,graphicx,psfrag,url,amsmath,amsthm,comment} 
%\usepackage{acronym} 
\usepackage{cite} 
\usepackage{hyperref}
% \usepackage{ulem,color}   
\usepackage{epstopdf} 
\usepackage{makecell}
\usepackage{amsmath,bm}    
\usepackage{verbatim} 
\usepackage{array}
%\usepackage{unicode-math} 
%\usepackage{graphicx}  
%\usepackage[]{graphics} 
\newcommand{\ie}{\textit{i.e. }}
\newcommand{\eg}{\textit{e.g. }}
%\def\yesnumber{\global\@myeqnswtrue}
%\newcommand{\noteout}[1]{\textcolor{red}{\sout{}}}       
%\def\noteout    
\usepackage[utf8]{inputenc}     
\usepackage[english]{babel}   
%\usepackage{slashbox}  
\usepackage{caption}    
\usepackage{enumitem} 
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\DeclareCaptionLabelFormat{lc}{\MakeLowercase{#1}~#2}
\captionsetup{labelfont=sc,labelformat=lc}
\usepackage[dvipsnames]{xcolor}
 
\newenvironment{skproof}{\noindent\textit{Sketch of  Proof:}}{\hfill$\blacksquare$}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma} 
\newtheorem{fact}{Fact}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{claim}{Claim} 
\newtheorem{assumption}{Assumption}
\newtheorem{condition}{Condition}
\newtheorem{observation}{Observation}
\newcommand{\note}[1]{\textcolor{blue}{#1}}
\newcommand{\rem}[1]{{\color{blue} {\bf [REMOVE: #1]}}}
\allowdisplaybreaks
\usepackage[protrusion=true,expansion=true]{microtype}
\pdfoutput=1
\renewcommand{\figurename}{Fig.}
\usepackage[font=small]{caption}
\newcommand{\ali}[1]{{\color{magenta} {{\bf}#1}}}
\newcommand{\chris}[1]{{\color{ForestGreen} {{\bf [Chris: #1]}}}}
\newcommand{\shams}[1]{{\color{cyan} {{\bf [S: #1]}}}}
\newcommand{\new}[1]{{\color{blue}#1}}
\newcommand{\mathcolorbox}[2]{\colorbox{#1}{$\displaystyle #2$}}
\usepackage{float}
\usepackage{mathtools}
\newcommand{\frank}[1]{{\color{blue} {\bf FL: #1}}}
\newcommand{\addFL}[1]{\textcolor{blue}{#1}}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand{\cgb}[1]{{\color{blue} {\bf CGB: #1}}}


\newcommand{\nm}[1]{{\color{red} {\bf [NM: #1]}}}
\newcommand{\add}[1]{\textcolor{red}{#1}}
\newcommand{\sst}[1]{\st{#1}}
\usepackage{cancel}
\newcommand\mst[2][red]{\setbox0=\hbox{$#2$}\rlap{\raisebox{.45\ht0}{\textcolor{#1}{\rule{\wd0}{2pt}}}}#2}   
\usepackage{soul,xcolor}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
% \newtheorem{definition}{Definition}
% \newtheorem{lemma}{Lemma}
% \newtheorem{theorem}{Theorem}
% \newtheorem{proposition}{Proposition}
% \newtheorem{corollary}{Corollary}
% \newtheorem{assumption}{Assumption}
% \newtheorem{condition}{Condition}
% \usepackage{bbm, dsfont}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\setulcolor{red}
\setul{red}{2pt}
\setstcolor{red}
%\newcommand\semiSmall{\fontsize{23.8}{20.38}\selectfont}
%\title{D2D-assisted Federated Learning: Hybrid Distributed Machine Learning in Two Timescales}
\title{Delay-Aware Hierarchical Federated Learning}
% : Intelligent Model Aggregation in Large-scale Wireless Edge Networks}

% Distributed model training under cooperative D2D communications.. 


% A consensus-driven distributed model training platform via cooperative device-to-device communications



% Some keywords: Device-to-device, peer-to-peer, Non-i.i.d data (to emphasize our new definition...), resource constrained, local descent method (we have multiple local descents),
% D2D, P2P, cluster-based, local cooperation.., locally cooperative devices ....



%  D2D-assisted hybrid federated learning with Aperiodic Consensus

\author{Frank Po-Chen Lin,~\IEEEmembership{Student Member,~IEEE},  Seyyedali~Hosseinalipour,~\IEEEmembership{Member,~IEEE}, Nicol\`o Michelusi, \IEEEmembership{Senior~Member,~IEEE}, and Christopher G. Brinton,~\IEEEmembership{Senior~Member,~IEEE}
\thanks{F. Lin and C. Brinton are with the School of Electrical and Computer Engineering, Purdue University, IN, USA. e-mail: \{lin1183,cgb\}@purdue.edu. Brinton and Lin acknowledge support from ONR grants N000142212305 and N000142112472.}
\thanks{S. Hosseinalipour is with the Department of Electrical Engineering, University at Buffalo, NY, USA. e-mail: alipour@buffalo.edu.}
\thanks{N. Michelusi is with the School of Electrical, Computer and Energy Engineering, Arizona State University, AZ, USA. e-mail: nicolo.michelusi@asu.edu. Part of his research has been funded by NSF under grant CNS-2129015.}
\thanks{A condensed version of this paper was presented at IEEE Globecom 2020~\cite{frank2020delay}.}}
\maketitle

\begin{abstract}
Federated learning has gained popularity as a means of training models distributed across the
wireless edge.
The paper introduces delay-aware hierarchical federated learning ({\tt DFL}) to improve the efficiency of distributed machine learning (ML) model training by accounting for communication delays between edge and cloud. Different from traditional federated learning, {\tt DFL} leverages multiple stochastic gradient descent iterations on device datasets within each global aggregation period and intermittently aggregates model parameters through edge servers in local subnetworks. During global synchronization, the cloud server consolidates local models with the outdated global model using a local-global combiner, thus preserving crucial elements of both, enhancing learning efficiency under the presence of delay. A set of conditions is obtained to achieve the sub-linear convergence rate of $\mathcal O(1/k)$. Based on these findings, an adaptive control algorithm is developed for {\tt DFL}, implementing policies to mitigate energy consumption and communication latency while aiming for a sublinear convergence rate. Numerical evaluations show {\tt DFL}'s superior performance in terms of faster global model convergence, reduced resource consumption, and robustness against communication delays compared to existing FL algorithms. In summary, this proposed method offers improved efficiency and results when dealing with both convex and non-convex loss functions.
%Federated learning relies on device-server communications to train a machine learning (ML) model, neglecting device-to-device (D2D) communications promoted in wireless networks. In this paper, we propose a novel ML model training architecture called \textit{two timescale hybrid federated learning} ({\tt TT-HF}), where we consider a cooperative cluster-based model training methodology and introduce a hybrid learning paradigm, where cooperative D2D communications are utilized in conjunction with device-server communications. In {\tt TT-HF}, during  each  global  aggregation interval, devices perform multiple local stochastic gradient descent (SGD) iterations and aperiodically synchronize their model parameters through local consensus via D2D  communications. This enables model training in two timescales capturing local SGD iterations and D2D communication rounds.
%We introduce a general definition on the gradient diversity, and investigate the convergence of {\tt TT-HF} that leads us to new convergence bounds for distributed ML. We exploit our theoretical findings to develop an online algorithm that actively tunes the step size, the D2D communications rounds, and the interval of global aggregations.
%Through extensive simulations, we demonstrate that the proposed framework is robust against extreme heterogeneity of the users' datasets and can achieve a higher accuracy as compared to the current art methods while imposing a smaller network cost.    
\end{abstract} 

\begin{IEEEkeywords}
\noindent Federated learning, edge intelligence, network optimization, convergence analysis, hierarchical architecture.
\end{IEEEkeywords}

\input{intro.tex}
\input{system.tex}
\input{convergence.tex}
\input{control.tex}
\input{experiments.tex}


 



\section{Conclusion and Future Work}
\noindent In this work, we proposed {\tt DFL}, which is a novel methodology that aims to improve the efficiency of distributed machine learning model training by mitigating the round-trip communication delay between the edge and the cloud. {\tt DFL} quantifies the effects of delay and modifies the FL algorithm by introducing a linear local-global model combiner used in the local model synchronization steps.
% , optimizing model performance under a hierarchical model training architecture subject to communication delay. 
We investigated the convergence behavior of {\tt DFL} under a generalized data heterogeneity metric and obtained a set of conditions to achieve sub-linear convergence. Based on these characteristics, we developed an adaptive control algorithm that adjusts the learning rate, local aggregation rounds, combiner weight, and global synchronization periods. Our numerical evaluation showed that {\tt DFL} leads to a faster global model convergence, lower resource consumption, and a higher robustness against communication delay compared to existing FL algorithms.
% Future work includes the implementation of {\tt DFL} in a real-world system and testing its performance in a real-world setting. Moreover, 
Future research directions include improving the robustness of {\tt DFL} against different types of network impairments, such as jitter and packet loss, and investigating its performance under flexible device participation.

\iffalse
\noindent We proposed {\tt TT-HF}, a methodology which improves the efficiency of federated learning in D2D-enabled wireless networks by augmenting global aggregations with cooperative consensus formation among device clusters. We conducted a formal convergence analysis of {\tt TT-HF}, resulting in a bound which quantifies the impact of gradient diversity, consensus error, and global aggregation periods on the convergence behavior. Using this bound, we characterized a set of conditions under which {\tt TT-HF} is guaranteed to converge sublinearly with rate of $\mathcal{O}(1/t)$. Based on these conditions, we developed an adaptive control algorithm that actively tunes the device learning rate, cluster consensus rounds, and global aggregation periods throughout the training process. Our experimental results demonstrated the robustness of {\tt TT-HF} against data heterogeneity among edge devices, and its improvement in trained model accuracy, training time, and/or network resource utilization in different scenarios compared to the current art.

There are several avenues for future work. To further enhance the flexibility of {\tt TT-HF}, one may consider (i) heterogeneity in computation capabilities across edge devices, (ii) different communication delays from the clusters to the server, and (iii) wireless interference caused by D2D communications.
\fi
% \pagebreak 
\bibliographystyle{IEEEtran}
\bibliography{ref}

\pagebreak
\clearpage
\setcounter{page}{1}
\setcounter{figure}{0}
\setcounter{table}{0}
\begingroup
% \let\clearpage\relax 
% \onecolumn %%% For
\onecolumn
% \large
% \onehalfspacing
% \nm{please use reference to the manuscript using ref, pageref and eqref. }
% \nm{use pageref instead! For instance, Theorem \ref{thm:subLin} on page \pageref{thm:subLin}. Let latex automatically do that.. otherwise, if you change something in the manuscript you will have to redo all the labeling in the response..}

% \nm{why is there such weird spacing below?  Also, can you increase linespacing? I had to set lare fontsize, it was too small..is there some weird configuration?}
% \nm{once the main text in thepaper has been finalized, you will need to copy/paste the updated quotes in the responses}
\iffalse
\section*{Responses to the Editor and the Reviewers}
\noindent July 10, 2023 \\

\noindent Author Response Letter \\

\noindent Paper Number: TCCN-TPS-23-0102 \\

% \noindent Old Title: ``Two Timescale Hybrid Federated Learning with Cooperative D2D Local Model Aggregations'' \\

% \noindent New Title: ``Semi-Decentralized Federated Learning with Cooperative D2D Local Model Aggregations'' \\

\noindent Authors: Frank Po-Chen Lin, Seyyedali~Hosseinalipour, Nicol\`o Michelusi, and Christopher G. Brinton \\

\noindent Dear TCCN Guest Editors:
\\

We would like to thank you and the anonymous reviewers for the time and efforts spent on reviewing our manuscript. We have taken all of the reviewers' comments into careful consideration and revised the manuscript accordingly. In this response letter, we provide a point-by-point response to the specific comments, and pointers to the locations in the manuscript where they have been addressed. We hope that the changes are satisfactory. We will be happy to address further comments if there is any.

To facilitate the second round of revisions, we highlighted the major changes using {\color{blue}blue font} in the revised manuscript. Although the manuscript has been thoroughly revised for clarity, consistence of notation, etc., minor typographical changes and rewording have not been highlighted.\\

In the revised manuscript, we have also slightly updated the author order to reflect contributions after the revision. Please kindly approve this change. \\


% Also, we have slightly changed the title of the paper from ``Two Timescale Hybrid Federated Learning with Cooperative D2D Local Model Aggregations" in the previous submission to ``Semi-Decentralized Federated Learning with Cooperative D2D Local Model Aggregations" to give a better insight about the learning architecture proposed in the paper. Please kindly approve this change.
% \\

\noindent Best regards,\\
F. Po-Chen Lin, Seyyedali~Hosseinalipour, Nicol\`o Michelusi, and Christopher G. Brinton
 
%  \nm{please dont use pagebreak! use newpage instead! Thjis is what was messing up with the spacing!}
\newpage


\noindent \textbf{Editor's Comment:}
\textit{Reviewers recommended either a major revision or resubmit. There are some major comments to be addressed: discuss more on the advantage of DFL than FL, improve paper organization, and the setting of non-iid.
}\\

\noindent Dear Editor,\\

\noindent We would like to thank you and the reviewers for the valuable comments and suggestions on how to further improve our manuscript. We have taken all of these comments into careful consideration and revised the manuscript accordingly. In this response letter, we provide a point-by-point response to the specific comments, and pointers to locations in the revised manuscript where they have been addressed (which is highlighted for ease of check). Regarding your specific comments above, the following changes have been made:
\begin{enumerate}
    \item \textbf{Highlighting the advantage of DFL over FL:} Detailed analysis explaining how the DFL algorithm achieves the outlined advantages is provided in sections Sec.~\ref{ssec:conv-eval}, Sec.~\ref{subsubsec:combiner}, and Sec.~\ref{ssec:ctrl_DFL} of the paper.
     
    % \item \textbf{Additional experiment illustrating the effectiveness of the local-global combiner in DFL approach over the standard FL technique:} We have integrated a comparative study between DFL with fixed parameters and the conventional Hierarchical FL in Sec.~\ref{subsubsec:combiner} (Page \pageref{subsubsec:combiner}). This comparison accentuates the advantages of implementing our local-global combiner during global synchronization, thereby showcasing its enhanced effectiveness over the traditional global aggregation/synchronization technique utilized in other studies\cite{Feng2022HFL,Lim2021HFL,Xu2022HFL,Luo2020HFEL,wang2021HFL,Mhaisen2022HFL}. 

    \item \textbf{Additional experiment to highlight the advantages of adaptive parameter adjustment over time in DFL:} We have introduced a comparison between the DFL algorithm with fixed control parameters (Algorithm 1) and the DFL control algorithm (Algorithm 2) to underscore the benefits of DFL's adaptive parameter adjustment over time. (See Sec.~\ref{ssec:ctrl_DFL} on Page~\pageref{ssec:ctrl_DFL}).
    
    \item \textbf{Additional experiment to demonstrate the benefit of DFL control algorithm with new basline (hierarchical FL with parameter control):} We included a comparison between the DFL control algorithm and Hierarchical FL with adaptive parameter control in Sec.~\ref{ssec:ctrl_DFL} (Page~\pageref{ssec:ctrl_DFL}). This highlights the advantages of incorporating the local-global combiner during global synchronization under adaptive parameter tuning, ensuring a fair and comprehensive evaluation.
    
    \item \textbf{Revision of the paper's structural layout:} we have created an independent section dedicated to related work (Sec.\ref{sec:RW}) along with refining Sec.\ref{sec:intro} and Sec.~\ref{sec:tthf}. This reorganization serves to make the content more accessible and logical for the reader.

    \item \textbf{Addition of tables for acronyms, and notations to improve comprehension:} Table~\ref{table:res} and Table~\ref{table:notation} serve as a ready reference, providing explicit definitions and offering a more straightforward way to understand the parameters and variables utilized throughout this paper.

    \item \textbf{Elucidation of the experimental setup concerning data distribution across edge devices:} We modified the text to clarify that the experiments are conducted under non-i.i.d. setting, outlined in Sec.~\ref{ssec:setup} on page~\pageref{ssec:setup}. We distributed the data across devices such that each device holds data points from only three out of ten possible labels, thereby simulating a degree of statistical data heterogeneity among devices, which is a common method in the literature~\cite{lin2021timescale,wang2021device,hosseinalipour2020multi}.
\end{enumerate}

% \begin{enumerate}
%     \item \textbf{Including additional symbols and notations in the table:} Table 1 of the paper has been revised and multiple notations are added.  Please refer to our response to \textbf{R1.1} for more details.
    
%     \item \textbf{Justifying the independence of the random variable $I^{i}_j$ and the UAV trajectory:} The relevant part in Section II.A has been revised  to better clarify the concept and the assumption. Please refer to our response to \textbf{R1.2} for more details.
    
%     \item \textbf{Analysis on the expected trajectory length of the UAV:} The expected trajectory length of the UAV is now specified in Lemma 2 in Appendix B. Please refer to our response to \textbf{R1.3} for more details.
    
%     \item \textbf{Characteristics of the UAV surveillance:}  A discussion is added to the first paragraph of the introduction to clarify the UAV-assisted surveillance characteristics. Please refer to our response to \textbf{R2.1} for more details.
    
    
    
%     \item \textbf{Air-to-ground communication channels:}  This work is mainly concerned with the design of a novel stochastic UAV-assisted surveillance system. The system model proposed is by itself general and can accommodate the physical layer aspects. Please refer to our response to \textbf{R2.3} for more explanations.
    
%     \item \textbf{Complexity and tradeoff analysis:} The complexity and tradeoff analysis for the proposed centralized and decentralized algorithms is given  in Section IV.D of the revised manuscript. Please refer to our response to \textbf{R2.4} for more explanations.
% \end{enumerate} 

\noindent We look forward to hearing from you again soon. \\

\noindent Best Regards, \\

\noindent Frank Po-Chen Lin, Seyyedali~Hosseinalipour, Nicol\`o Michelusi, and Christopher G. Brinton



\newpage



\section*{Response to Reviewer 1}

\noindent\textbf{Reviewer's General Note:} \textit{In this paper, Authors introduces delay-aware federated learning to improve the efficiency of distributed machine learning model training. The reviewer thinks the writing logic and standards need to be improved so that readers can read more easily.}
\\

% \textit{
% By modeling the systems with Markov chains, the desired UAV paths are derived by solving optimization problems. Centralized and distributed system configurations have also been investigated in the paper.}

% \textit{
% I found the paper interesting and it is well presented. I have the following comments:}
% \\

\noindent \textbf{To Reviewer 1:} We would like to thank you for taking the time to review our paper and provide us with valuable feedback. In the following, you will find our responses to each of your comments and pointers to locations in the revised manuscript where they have been addressed.
\vspace{4mm}
\hrule
\vspace{4mm}

\noindent\textbf{R1.1:} \textit{In fig.1, authors describes a two-layer hierarchical federated learning, but the system model in Sec.I.A is represented by top, middle, and bottom hierarchy. In addition, I think the authors mislabeled the icons of edge servers and edge devices.}
\\

% the original author called the edge-device as two layers, we use the most common way 
\noindent In Fig.~\ref{fig2_1}, our intention was to illustrate the relationship between the main server and the edge server, conceptualizing them as a two-layer hierarchy. We understand that this might have caused some confusion.
To address your concerns and improve the clarity of our work, we have updated the system model description in Sec.~\ref{subsec:syst1} to represent a three-layer hierarchy. This revised hierarchy now includes the top (main server), middle (edge server), and bottom (edge devices) layers, aligning with the overall structure depicted in Fig.~\ref{fig2_1}.

\begin{figure*}[b]
  \centering
%   \vspace{-0.4in}
    \includegraphics[width=0.96\textwidth]{image/fedDel_struct.png}
     \caption{
     The {\color{blue}three-layer} hierarchical federated learning network architecture consists of edge devices forming local subnet topologies based on their proximity to the local aggregator.}
     \label{fig2_1}
     \vspace{-0.7em}
\end{figure*}

Furthermore, we appreciate you highlighting the mislabeling of the icons for the edge servers and edge devices. This error has been rectified in the updated version of our paper.

We hope these amendments provide a clearer depiction of our system model and address your concerns effectively.\\
%We break down our response into two parts as follows:

% This is now clarified in the main text in Sec.~\ref{subsec:syst2} on page~\pageref{Assump:SmoothStrong}, which we quote below for the ease of review:


% \nm{if these areferences are already in the manuscript, just refer to that!}
% \nm{please move the list of references at the end of Reviewer 1 response.}
% {\footnotesize
% \noindent [R1]  S. Wang, T. Tuor, T. Salonidis, K. Leung, C. Makaya, T. He and K. Chan ``Adaptive federated learning in resource constrained edge computing systems," IEEE J. Sel. Areas Commun., vol. 37, no. 6, pp. 1205-1221, 2019.
% \\

% \noindent [R2] M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor and S. Cui, ``A Joint Learning and Communications Framework for Federated Learning Over Wireless Networks," IEEE Trans. Wireless Commun., vol. 20, no. 1, pp. 269-283, Jan. 2021.
% \\

% \noindent [R3] N. H. Tran, W. Bao, A. Zomaya, M. N. H. Nguyen and C. S. Hong, ``Federated Learning over Wireless Networks: Optimization Model Design and Analysis," in Proc. IEEE Conf. Comput. Commun. (INFOCOM), 2019, pp. 1387-1395.
% \\

% \noindent [R4] H. H. Yang, Z. Liu, T. Q. S. Quek and H. V. Poor, ``Scheduling Policies for Federated Learning in Wireless Networks," IEEE Trans. Commun., vol. 68, no. 1, pp. 317-333, 2020.
% \\

% \noindent [R5] C. T. Dinh et al., ``Federated Learning Over Wireless Networks: Convergence Analysis and Resource Allocation," in IEEE/ACM Trans. Networking, vol. 29, no. 1, pp. 398-409, 2021.
% \\

% \noindent [R6] Z. Yang, M. Chen, W. Saad, C. S. Hong and M. Shikh-Bahaei, ``Energy Efficient Federated Learning Over Wireless Communication Networks," IEEE Trans. Wireless Commun., vol. 20, no. 3, pp. 1935-1949, 2021.
% \\

% \noindent [R7] M. Chen, H. V. Poor, W. Saad and S. Cui, ``Convergence Time Optimization for Federated Learning Over Wireless Networks," IEEE Trans. Wireless Commun., vol. 20, no. 4, pp. 2457-2471, 2021.
% \\

% \noindent [R8] M. M. Amiri, D. Gündüz, S. R. Kulkarni and H. V. Poor, ``Convergence of Update Aware Device Scheduling for Federated Learning at the Wireless Edge," IEEE Trans. Wireless Commun., vol. 20, no. 6, pp. 3643-3658, 2021.
% \\

% \noindent [R9] L. Liu, J. Zhang, S. Song, and K. B. Letaief, “Client-edge-cloud hierarchical federated learning,” in Proc. IEEE Int. Conf. Commun. (ICC), 2020, pp. 1–6.
% \\

% \noindent [R10] F. P.-C. Lin, S. Hosseinalipour, S. S. Azam, C. G. Brinton, and N. Michelusi, ``\nm{update title? We will have to udpate arxiv anyway}Two timescale hybrid federated learning with cooperative D2D local model aggregations,”arXiv preprint arXiv:2103.10481, 2021.
% }
% \\

\noindent\textbf{R1.2:}  \textit{The authors should improve the writing logic of this paper, and maybe add a table that show notation and definition.}
\\ 

% separated related work into an independent section
\noindent In response to your comments, we have undertaken a thorough revision of the paper. We have revised the paper's structure to enhance its coherence and clarity. Specifically, we have created an independent section dedicated to related work (Sec.~\ref{sec:RW}) and refined Sec.~\ref{sec:intro} and Sec.~\ref{sec:tthf}. This reorganization serves to make the content more accessible and logical for the reader. 
Moreover, recognizing the need for clarity of terms used, we have incorporated a table into Sec.~\ref{sec:tthf} that outlines the notations and their corresponding definitions.\\

The updated portion of Sec.~\ref{subsec:contrib} is quoted below for a simplified review process:\\

\textit{``{\color{blue}In this paper, we are motivated to address the above three challenges. In particular, we consider model training under a
% \nm{are you defining a new metric, or extending an existing one? Based on your Def 1 and Def 2, I think you are extending it} 
metric of data heterogeneity extended from the current art, which can capture extreme cases of data diversity across the devices. Also, we explicitly account for the delay in model aggregation and introduce a linear local-global model combining scheme. This scheme retains essential elements of both the outdated global model and the current local model, thereby improving the overall learning efficiency. Our methodology can augment existing studies by incorporating our strategy with those already established. Finally, we consider model training over a realistic hierarchical network edge architecture.}"}\\

Additionally, we also made revisions to the Outline and Summary of Contributions in Sec.~\ref{subsec:contrib}. For ease of review, we quote the revised version below:\\ 
 
{\textit{\color{blue} ``We propose \textit{delay-aware federated learning} ({\tt DFL}), a novel methodology for improving distributed ML model training efficiency by accounting for the round-trip delay between edge and cloud. 
% (Sec. \ref{sec:tthf}). 
{\tt DFL} accounts for the effects of delays by introducing a local-global model combiner scheme during global synchronization, which conserve vital aspects of both the stale global model and the current local model, thereby enhancing overall learning efficiency."}}\\

{\textit{\color{blue} ``Leveraging the convergence characteristics, we introduce an adaptive control algorithm for {\tt DFL}, which targets a joint optimization of communication energy, latency, and ML performance while preserving a sub-linear convergence rate. This involves solving a non-convex integer programming problem, adapting (i) the global aggregation interval with the cloud, (ii) the local aggregation interval with the edge servers, (iii) the learning rate, and (iv) the combiner weight over time."}}\\

We have included a footnote to elucidate the procedure of local aggregation within the system overview and rationale, as presented in Sec.~\ref{subsec:syst3} located on page~\pageref{subsec:syst3}. The changes are as follows:\\

\textit{``{\color{blue} Note that in Fig.~\ref{fig:twoTimeScale}, the parameters $m_1$ and $m_2$ may not be equal. In fact, {\tt DFL} allows for aperiodic local aggregations during each local model training interval, the frequency of which will be later tuned in our control algorithms to mitigate network resource consumption.}"}\\

We have revised the content in Sec.~\ref{sec:tthf} for greater clarity. The modified content can be found in Sec.~\ref{subsec:proc} (page~\pageref{subsec:proc}):\\

\textit{``{\color{blue} At each instance of global aggregation, the models of the devices are collected (i.e., they engage in uplink transmissions) and arrive at the server with a certain delay. The server will then aggregate the received models into a global model and broadcast it (i.e., engage in downlink transmission) to the devices, which involves another delay. In parallel, devices proceed with local updates before the global model arrives. Upon reception of the global model, devices synchronize their local models in a manner accounting for both uplink and downlink delays. The relationships between the timescales are  depicted in Fig.~\ref{fig:twoTimeScale}.}"}\\

\textit{``{\color{blue}The length of the $k$th local model training interval is denoted by $\tau_k = |\mathcal{T}_k|$. We have $t_k = \sum_{\ell=0}^{k-1}\tau_\ell$, $\forall k$, where $t_0 = 0$ and $t_K=T$.} This allows for varying the length of local model training intervals across global synchronizations (i.e., $\tau_{k} \neq \tau_{k'}$ for $k \neq k'$). During the $k$th local model training interval, we define $\mathcal{T}^\mathsf{L}_{k,c} \subset \mathcal{T}_k$ as the set of time instances when the edge server pulls the models of devices in subnet $\mathcal{S}_c$ (i.e., devices engage in uplink transmissions) and performs local aggregations of their models. Local aggregation synchronizes the local models within the same subnet by computing their weighted average. We next formalize the above procedure."}\\

We have additionally undertaken revisions to the discussion of the \textbf{tentative local model} within the DFL formalization, which can be found in Sec.~\ref{subsec:form2} on page~\pageref{subsec:form2}. The alterations are as follows:\\

\textit{``\textbf{Tentative Local Model}: Using the gradient estimate $\widehat{\mathbf g}_{i}^{(t-1)}$, each device computes its \textit{tentative local model} $ {\widetilde{\mathbf{w}}}_i^{(t)}$ as
\vspace{-3.5mm}
\begin{align} \label{8}
    {\widetilde{\mathbf{w}}}_i^{(t)} = 
           \mathbf w_i^{(t-1)}-\eta_{k} \widehat{\mathbf g}_{i}^{(t-1)},~t\in\mathcal T_k, 
\end{align}
{\color{blue} where $\eta_{k} > 0$ denotes the step size. We denote this tentative because it is an intermediate calculation prior to any potential aggregation. Specifically, 
based on ${\widetilde{\mathbf{w}}}_i^{(t)}$, the \textit{updated local model} $\mathbf{w}_i^{(t)}$ is computed either through setting it to ${\widetilde{\mathbf{w}}}_i^{(t)}$ or through a local aggregation, discussed next.}"}\\

We have further updated the section detailing the \textbf{Communication Delay and Global Model Aggregations} in the DFL formalization (including footnote), located in Sec.\ref{subsec:form2}. The revised content is as follows:\\

\textit{``{\color{blue} In conducting global aggregations, to account for the round-trip delay, devices send their local models to the edge servers $\Delta_k$ time instances prior to the completion of each local model training interval $\mathcal{T}_k$, i.e., at $t=t_{k+1}-\Delta_k \in \mathcal{T}_k$. Concurrently, these devices carry out an additional $\Delta_k$ local updates using SGD/local aggregation before they receive the updated global model. We assume that $\Delta_k$ can be reasonably estimated, e.g., from round-trip delays observed in recent time periods}\footnote{\color{blue}\textit{``Our methodology is not tied to any particular procedure for estimating $\Delta_k$. In practice, an approach based on recent time periods is motivated by Transport Control Protocol (TCP) analysis, where round trip times have been noted to hold relatively stable over time ranges up to tens of seconds \cite{hao2002RTT}. For our experimental settings in Sec.~\ref{sec:experiments}, we find that our methodology can tolerate delay estimation errors of $20$ ms with only a $1\%$ drop in the resulting trained model accuracy. Using a metric of delay variations being within $0.5$-$1$ ms for a round-trip delay of $50$ ms~\cite{James2007TV}, this estimation approach could handle networks with round-trip times on the order of seconds, which would be rather large, while only experiencing marginal degredations."}}"}.\\ 

%We break down our response into the following parts:
The table is now added in the main text in Sec.~\ref{subsec:syst2} on page~\pageref{Assump:SmoothStrong}, which we refer below in Table~\ref{table:acr_res_1} and~\ref{table:notation_1} for the ease of review:\\

\begin{table}[h]
  \centering
  \caption{List of acronyms}
  \label{table:acr_res_1}
  \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Acronym} & \textbf{Definition} & \textbf{Acronym} & \textbf{Definition} \\
    \hline
    ML & Machine Learning & FL & Federated Learning \\
    \hline
    DFL & Delay-Aware Federated Learning & SGD & Stochastic Gradient Descent \\
    \hline
    SVM & Support Vector Machine & CNN & Convolutional Neural Network \\
    \hline
  \end{tabular}
\end{table}

\begin{table}[h]
  \centering
  \caption{List of key notations}
  \label{table:notation_1}
  \begin{tabular}{|c|m{6.3cm}|c|m{6.3cm}|}
    \hline
    \textbf{Notation} & \textbf{Description} & \textbf{Notation} & \textbf{Description} \\
    \hline
    $\mathcal N$ & Set of all edge servers & $N$ & Number of all edge servers \\
    \hline
    $\mathcal I$ & Set of all edge devices & I & Number of all edge devices \\
    \hline
    $\mathcal S_c$ & Set of edge devices in subnet $c$ & $s_c$ & Number of edge devices in subnet $c$ \\
    \hline
    $n_c$ & Index of local model aggregator (edge server) & $\ell(\cdot)$ & Loss function associated with each datapoint \\
    \hline
    $\mathcal D_i$ & Dataset of edge devices $i$ & $D_i$ & Number of data points in edge device $i$ \\
    \hline
    $F_i(\cdot)$ & Local loss function for device $i$ & $\bar{F}_c(\cdot)$ & Subnet loss function for subnet $c$ \\
    \hline
    $F(\cdot)$ & Global loss function & $\mathbf w^*$ & Optimal global model \\
    \hline
    $\delta, \zeta$ & Inter-gradient diversity parameters across subnets & $\delta_c,\zeta_c$ & Intra-gradient diversity parameters in subnet $c$ \\
    \hline
    $t$ & Time index of local model training iteration & $k$ & Index of local model training interval\\
    \hline
    $\mathcal T_k$ & $k$-th local model training interval & $\tau_k$ & Length of the $k$-th local model training interval \\
    \hline
    $\mathcal T_{k,c}^\mathsf{L}$ & Set of local aggregation instances in the $k$-th local model training interval & $\widehat{\mathbf g}_{i}^{(t)}$ & Local stochastic gradient at device $i$ and time $t$ \\
    \hline
    $\mathbf w_i^{(t)}$ & Local model at device $i$ and time $t$ & $\widetilde{\mathbf w}_i^{(t)}$ & Tentative Local model at device $i$ and time $t$ \\
    \hline
    $\bar{\mathbf{w}}_{c}^{(t)}$ & Instantaneous aggregated local model across devices $i\in\mathcal S_c$ & $\bar{\mathbf w}^{(t)}$ & Global model at time $t$ \\
    \hline
    $\Theta_c^{(t)}$ & Indicator of local aggregation for subnet $c$ & $\alpha_k$ & Combiner weight for the linear local-global combiner \\
    \hline
    $\Delta^\mathsf{U}_k$ & Upstream delay between edge device and main server & $\Delta^\mathsf{D}_k$ & Downstream delay between edge device and main server \\
    \hline
  \end{tabular}
\end{table}

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ``\textit{\textit{Channel model:}} We assume that the D2D communications are conducted using orthogonal frequency division techniques, e.g., OFDMA, to reduce the interference across the devices. We consider the instantaneous channel capacity for transmitting data from node $i$ to $i'$, both belonging to the same cluster $c$ following this formula:
% \begin{equation}\label{eq:shannon}
%     C_{i,i'}^{(t)}=W\log_2 \left(1+ \frac{p_i^{(t)} \vert {h}^{(t)}_{i,i'}\vert^2}{\sigma^2} \right),
% \end{equation}
% where $\sigma^2=N_0 W$ is the noise power, with $N_0=-173$ dBm/Hz denoting the white noise power spectral density. 
% We consider the bandwidth of $W=1$ MHz and transmit power of $p^{(t)}_i=24$ dBm, $\forall i,t$. We incorporate the effect of both large-scale and small scaling fading in $h^{(t)}_{i,i'}$, given by \cite{tse2005fundamentals,channelJun2021}:
% \begin{equation}
%      h_{i,i'}^{(t)}= \sqrt{\beta_{i,i'}^{(t)}} u_{i,i'}^{(t)},
% \end{equation}
% where $\beta_{i,i'}^{(t)}$ is the large-scale fading coefficient and $ u_{i,i'}^{(t)} \sim \mathcal{CN}(0,1)$ captures Rayleigh fading. We assume channel reciprocity, i.e., $h^{(t)}_{i,i'}=h^{(t)}_{i',i}$, for simplicity. We use the standard definition of $\beta_{i,i'}^{(t)}$ \cite{tse2005fundamentals,channelJun2021}:
% \begin{equation}
%     \beta_{i,i'}^{(t)} = \beta_0 - 10\alpha\log_{10}(d^{(t)}_{i,i'}/d_0).
% \end{equation}
% where $\beta_0=-30$ dB denotes the large-scale fading coefficient at the referenced distance $d_0=1$ m, $\alpha$ is the path loss exponent chosen as $3.75$ suitable for urban areas, and $d^{(t)}_{i,i'}$ denotes the instantaneous Euclidean distance between the respective nodes.
% % We assume that the nodes are placed in an urban area with path-loss exponent 3.75. We consider the noise spectral density of $N_0=-173$ dBm/Hz and further incorporate fading coefficient as Rayleigh fading channel, where the fading coefficient varies according to the distance between two devices.
% We assume that the devices in each cluster are placed uniformly at random in a $50m\times50m$ square field. 
% \\  

% \textit{D2D network configuration:} 
% We incorporate wireless channel model explained above into our scenario to define the set of D2D neighbors and configure the cluster topologies.
% % We consider two cases: (i) unknown Channel State Information (CSI) to the devices and (ii) known CSI at the devices. For both cases, 
% We assume that the nodes moves slowly so that their locations remain static during each global aggregation period, although it may change between consecutive global aggregation.  
% We build the cluster topology based on channel reliability across the nodes quantified via the outage probability. In particular, considering~(\ref{eq:shannon}), the probability of outage upon transmitting with data rate of $R^{(t)}_{i,i'}$ between two nodes $i,i'$  is given by
% \begin{equation}\label{eq:out}
%     \textrm{p}^{\mathsf{out},(t)}_{i,i'}=1-\exp\left(\frac{-(2^{R_{i,i'}^{(t)}}-1)}{\mathrm{SNR}^{(t)}_{i,i'}}\right),
% \end{equation}
%  where $\mathrm{SNR}^{(t)}_{i,i'}=\frac{p_i^{(t)}\vert h_{i,i'}^{(t)}\vert^2}{\sigma^2}$.
% To construct the graph topology of each cluster $c$, we consider an edge between two nodes $i$ and $i'$ if and only if the respective outage probability satisfies $\textrm{p}^{\mathsf{out},(t)}_{i,i'} \leq 5\%$ given a defined common data rate $R^{(t)}_{i,i'}=R^{(t)}_c$, where $R^{(t)}_c$ is obtained as follows.
% The configuration of $R_c^{(t)}$ in the two scenarios (i.e., unknown and known CSI) are described as follows: 


% \begin{enumerate}[leftmargin=5mm]
%      \item
     
    %  Determining the common data rate under unknown CSI at the devices: In this scenario, 
     
    %  Given the location of the nodes, considering $\textrm{p}^{\mathsf{out},(t)}_{i,i'}\leq 5\%$ for all the links gives us the maximum achievable data rate  between the nodes, i.e., $R^{(t)}_{i,i'}$ satisfying~(\ref{eq:out}), $\forall i,i'$. We then find the numerical value of $R^{(t)}_c$, $\forall c$, to have average degree of two inside the clusters. 
    %  After obtaining $R^{(t)}_c$, during ML model training, if the instantaneous channel capacity between two nodes $i$ and $i'$ (under realization of fast fading) is higher than $R^{(t)}_c$, (we find that in our setting $R_c^{(t)}=14Mbps$)
     
    %  \nm{one thing I quite dont understand.. why are you optimizing the rate with a constraint on the degree? What if this rate is too low to support the assumption that quantization error should be negligible? Or are you quantizing the signal in your simulatoin with the given rate?  (if you do quantize the signa, then it is ok...)
    %  To me, the folloiwng makes more sense: 1) you first pick the rate, that should be large enough so that it is ok to consider quantization to be negligible 2) you build your network based on the outage probability.}
    %  {\color{red} we choose $R_c^{(t)}=14Mbps$, if the instantaneous data rate is not greater than that, then outage ???...} 
    %  we consider a link among them where each node {transmits data with common data rate $R_c^{(t)}$;} 
    %  Idea of the story:
    %  1. we fix and design the rate, we create the topology based this and we get the average degree of 2.
    %  2. We want a certain connectivity. we can design the rate to has certain desired connectivity (i.e. if we want fully connected toplogy, this may make rate too small)
    %  3. The reason we design the rate is because we want to ensure that the quatization error is egligible. 
     
     
    %  We numerically set the common rate to $R_c^{(t)}=14Mbps$, this value is obtained since it is large enough to neglect the effect of quantization error in digital communication of the signals
     
     
    %  We find out that with this common rate, we get an average degree around $2$ in each cluster.
     
     
     

    %  \nm{what effect does the rate have on the signals? Are you quantizing them based on the given rate? IF NOT, then this statement is inaccurate..}
    %  {\color{red} delay... } we make the rate large enough to make we focus on fadig ant outage
     
    %  otherwise, we consider that the respective nodes do not communicate, i.e., outage occurs and the packet is lost. Thus, although the location of the nodes is assumed to be fixed during each global aggregation period,
    %  the cluster topology, i.e., the edge configuration among the nodes, changes
    %  with respect to every local SGD iterations in a model training interval due to the variation of the fast fading coefficient.
     
    %  {\color{red} Consensus and global knowledge:}
     
    %  Given a communication graph we choose $d_c=1/8$ to form the consensus iteration at each node $i$ as $\textbf{z}_{i}^{(t'+1)} = \textbf{z}_{i}^{(t')}+{\color{blue}d_{{c}}}\sum_{j\in \mathcal{N}_i} (\textbf{z}_{j}^{(t')}-\textbf{z}_{i}^{(t')})$  (refer to the discussion provided above Assumption~\eqref{assump:cons}). Note that given $d_c$, each node can conduct D2D communications and local averaging without any global coordination.  
    % %  \hl{the consensus matrix of the cluster, changes}
    % %  \nm{uhm.. doesnt this require global knowledge? How do you ensure that itis a stochastic matrix? You need to be more precise!}
    % "

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
     
     
%      \nm{The rest below should be removed (I have reorganized it above)}
%      \\

% \hl{We consider digital transmission using state-of-the-art techniques in encoding/decoding,}
% \nm{are you using SoA codes in your simulation? I don't think so! This statement is not accurate since it does not accurately reflect YOUR paper! You need to be more precise..}
% e.g., LDPC codes, that achieve the bit error rate (BER) is reasonable small and negligible [R3]. So, the devices' transmitted model parameters  can be recovered at the server almost perfectly unless outage occurs, which can be avoided using precoding [R1]. Also, in the prior work [R4] from one of the authors, we have demonstrated that upon having digital quantization, the average consensus methods can be adapted to have perfect consensus across the nodes. We have further clarified this in Sec. II, which we quote the added text below for the ease of review:
% Also, consider digital communication with D2D communication using quantized signals, we can also apply [R3] to obtain the required rounds of distributed consensus for {\tt TT-HF} to converge with the desired convergence rate. Note that we are able to distributedly estimate the spectral radius in the presence of additive channel noise by applying the result in [R4].
% \\

% This is now clarified in Sec. II (Page 6) of the paper, which we quote below for the ease of review:
% \\

% ``It is worth mentioning that our theoretical results obtained in Proposition 1, Theorem 1 and Theorem 2 are general results, which hold for a wide range of algorithms that augment the global aggregations with (imperfect) local model aggregations in federated learning. In particular, upon using any D2D protocol for conducting local model aggregations, as long as the model parameter at each node $i$ can be written as (13) for a general $\mathbf{e}_i$, our analysis readily holds. Note that we consider digital transmission (in both D2D and uplink/downlink communications) where using state-of-the-art techniques in encoding/decoding, e.g., low density parity check (LDPC) codes, the bit error rate (BER) is reasonably small and negligible [48], and also the effect of quantized model transmissions can be readily incorporated using the techniques in~[49]. So, the devices' transmitted model parameters  can be recovered at the server almost perfectly unless outage occurs, which can be avoided using precoding [50] or taken into account to obtain proper rounds of D2D communications via the method of~[51]." 
% % {\color{blue} Some citation regarding consensus under noisy channels will be added here}


% \nm{tjhis is not quite clear, you need to go straight to the point... sahy that you assume that there is an encoding/decoding procedure, so that the suignals are not sent directly (i.e. uncoded) through the wireless channel.  And there is an error correction code that takes care of correcting errors. In our analysis, we make the idealized assumption that there are not packet losses but we extended the simualtion setup to cinlude thepossiblity that packets are not receiver.}
% \textit{(1) Generality of the the conducted analysis upon using different wireless communication models:}
% \nm{saying that your analysis is "general" is a risky statements. We DO NOT consider packet errors, fading etc. So our analysis is NOT general!}
% We would like to mention that the theoretical results obtained in the paper (Proposition 1, Theorem~1 and Theorem 2) are general results, which hold for a wide range of algorithms that augment the global aggregations with (imperfect) local model aggregations in federated learning. This is due to the fact that the only requirement to conduct this analysis is representing the final local model at the nodes after each local aggregation as a noisy version of perfect aggregated models as in (13)\nm{13 what?}. It can be seen that even considering wireless communication effect this equation holds, e.g., see Eq. (13) in [R1] demonstrating that using a precoding, the final local aggregated model still follows (13) considering fading channels. Once the local model at the nodes can be modeled in the form of (13), we can apply [R2] to obtain the required rounds of distributed consensus on imperfect channels with link failures and channel noise.



% \nm{same please move to end of R1 response letter and specify where it is cited in manuscript}
% % \noindent [R1] T. Sery, N. Shlezinger, K. Cohen and Y. C. Eldar, ``Over-the-Air Federated Learning from Heterogeneous Data," IEEE Trans. Signal Process., 2021.
% % \\

% % \noindent [R2] S. Kar and J. M. F. Moura, ``Distributed Consensus Algorithms in Sensor Networks With Imperfect Communication: Link Failures and Channel Noise," IEEE Trans Signal Process., vol. 57, no. 1, pp. 355-369, 2009.
% % \\

% % \noindent [R3] R. Gallager, ``Low-density parity-check codes," IRE Trans Inf. Theory, vol. 8, no. 1, pp. 21-28, 1962.
% % \\

% % \noindent [R4] C. -S. Lee, N. Michelusi and G. Scutari, ``Finite Rate Distributed Weight-Balancing and Average Consensus Over Digraphs," IEEE Trans Autom. Control.
% % \\

% % \noindent [5] G. Muniraju, C. Tepedelenlioglu and A. Spanias, ``Distributed Spectral Radius Estimation in Wireless Sensor Networks," Asilomar Conf. Signals, Syst., and Computers, 2019, pp. 1506-1510.
% \textit{(2) Clarifying the effect of wireless communications in adaptive algorithm:} In addition to the clarifications made above, we now conduct our simulations under more precise considerations of wireless communications effect. In particular, we now consider realistic channel models deployed in D2D communications and incorporate wireless fading and white noise into our scenario. We have added a discussion in Section V (Page 13-14) to discuss these considerations, which we quote below for the ease of review:
 
% \setcounter{equation}{58}
% ``
% \textit{\textit{Channel model:}} We assume that the D2D communications are conducted using orthogonal frequency division techniques, e.g., OFDMA, to reduce the interference across the devices. We consider the instantaneous channel capacity for transmitting data from node $i$ to $i'$, both belonging to the same cluster $c$ following this formula:
% \begin{equation}\label{eq:shannon}
%     C_{i,i'}^{(t)}=W\log_2 \left(1+ \frac{p_i^{(t)} \vert {h}^{(t)}_{i,i'}\vert^2}{\sigma^2} \right),
% \end{equation}
% where $\sigma^2=N_0 W$ is the noise power, with $N_0=-173$ dBm/Hz denoting the white noise power spectral density. 
% We consider the bandwidth of $W=1$ MHz and transmit power of $p^{(t)}_i=24$ dBm, $\forall i,t$. We incorporate the effect of both large-scale and small scaling fading in $h^{(t)}_{i,i'}$, given by [54],[55]:
% \begin{equation}
%      h_{i,i'}^{(t)}= \sqrt{\beta_{i,i'}^{(t)}} u_{i,i'}^{(t)},
% \end{equation}
% where $\beta_{i,i'}^{(t)}$ is the large-scale fading coefficient and $ u_{i,i'}^{(t)} \sim \mathcal{CN}(0,1)$ captures Rayleigh fading. We assume channel reciprocity, i.e., $h^{(t)}_{i,i'}=h^{(t)}_{i',i}$, for simplicity. We use the standard definition of $\beta_{i,i'}^{(t)}$ [54],[55]:
% \begin{equation}
%     \beta_{i,i'}^{(t)} = \beta_0 - 10\alpha\log_{10}(d^{(t)}_{i,i'}/d_0).
% \end{equation}
% where $\beta_0=-30$ dB denotes the large-scale fading coefficient at the referenced distance $d_0=1$ m, $\alpha$ is the path loss exponent chosen as $3.75$ suitable for urban areas, and $d^{(t)}_{i,i'}$ denotes the instantaneous Euclidean distance between the respective nodes.
% % We assume that the nodes are placed in an urban area with path-loss exponent 3.75. We consider the noise spectral density of $N_0=-173$ dBm/Hz and further incorporate fading coefficient as Rayleigh fading channel, where the fading coefficient varies according to the distance between two devices.
% We assume that the devices in each cluster are placed uniformly at random in a $50m\times50m$ square field. 
% \\

% \textit{D2D network configuration:} 
% We incorporate wireless channel model explained above into our scenario to define the set of D2D neighbors and configure the cluster topologies.
% % We consider two cases: (i) unknown Channel State Information (CSI) to the devices and (ii) known CSI at the devices. For both cases, 
% We assume that the nodes moves slowly so that their locations remain static during each global aggregation period, although it may change between consecutive global aggregation.  
% We build the cluster topology based on channel reliability across the nodes quantified via the outage probability. In particular, considering~(\ref{eq:shannon}), the probability of outage upon transmitting with data rate of $R^{(t)}_{i,i'}$ between two nodes $i,i'$  is given by
% \begin{equation}\label{eq:out}
%     \textrm{p}^{\mathsf{out},(t)}_{i,i'}=1-\exp\left(\frac{-(2^{R_{i,i'}^{(t)}}-1)}{\mathrm{SNR}^{(t)}_{i,i'}}\right),
% \end{equation}
%  where $\mathrm{SNR}^{(t)}_{i,i'}=\frac{p_i^{(t)}\vert h_{i,i'}^{(t)}\vert^2}{\sigma^2}$.
% To construct the graph topology of each cluster $c$, we consider an edge between two nodes $i$ and $i'$ if and only if the respective outage probability satisfies $\textrm{p}^{\mathsf{out},(t)}_{i,i'} \leq 5\%$ given a defined common data rate $R^{(t)}_{i,i'}=R^{(t)}_c$, where $R^{(t)}_c$ is obtained as follows.
% % The configuration of $R_c^{(t)}$ in the two scenarios (i.e., unknown and known CSI) are described as follows: 
% \\

% % \begin{enumerate}[leftmargin=5mm]
% %      \item
     
%     %  Determining the common data rate under unknown CSI at the devices: In this scenario, 
     
%      Given the location of the nodes, considering $\textrm{p}^{\mathsf{out},(t)}_{i,i'}\leq 5\%$ for all the links gives us the maximum achievable data rate  between the nodes, i.e., $R^{(t)}_{i,i'}$ satisfying~(\ref{eq:out}), $\forall i,i'$. We then find the numerical value of $R^{(t)}_c$, $\forall c$, to have average degree of two inside the clusters. 
%      After obtaining $R^{(t)}_c$, during ML model training, if the instantaneous channel capacity between two nodes $i$ and $i'$ (under realization of fast fading) is higher than $R^{(t)}_c$, we consider a link among them where each node transmits data with common data rate $R_c^{(t)}$; otherwise, we consider that the respective nodes do not communicate.  Thus, although the location of the nodes is assumed to be fixed during each global aggregation period,
%      the cluster topology, i.e., the consensus matrix of the cluster, changes with respect to every local SGD iterations in a model training interval due to the variation of the fast fading coefficient."
%      \\
     
    % For each local model training interval, the determination of the cluster topology consists of two phases, i.e., (a) pre-training and (b) training. In (a), we determine a common data rate threshold $R_c$ and in (b) we construct the cluster topology by applying $R_c$. The procedure of the two phases are described as follows:
    % \begin{enumerate}[(a)]
        % \item During the pre-training phase, we first require the outage probability $P_{out}$ of a Rayleigh fading channel to be less than $5\%$, i.e.,  $P_{out} \leq 5\%$. Then based on this requirement, we obtain a local data rate threshold $R_{c,thres}^{(i,j)}$ between each set of two nodes $\{i,j\},~\forall i,j$, at which the requirement on $P_{out}$ is satisfied. Note that $R_{c,thres}^{(i,j)}$ varies across every two nodes due to the difference in geographical proximity, resulting in different path loss. 
    %     \noindent To guarantee that all transmission in the cluster satisfies the the outage probability requirement when transmitting at common rate $R_c$. If $R_{c,thres}^{(i,j)}\geq R_c$ (which means that the requirement for the outage probability, i.e., $P_{out}\leq 5\%$, is satisfied when data is transmitted at rate $R_c$ between $i$ and $j$) we establish an edge between $i$ and $j$. Otherwise, the edge between the two nodes does not exist. The common threshold $R_c$ is chosen from $R_{i,j}^{\mathsf{th},(t)},~\forall i,j$ to make the average degree of the cluster be around $2$. Note that the topology established here is a hypothetical structure used to determine $R_c$. The actual configuration of the cluster topology depends on the actual realization of channels in each SGD iteration in the training phase.
    %     \item In the training phase, we compute the real-time channel capacity $C_{(i,j)}^{(t)}=W\log_2(1+\frac{p}{N_0W}\vert h_{ij}^{(t)}\vert^2)$ based on the realization of the channel condition $h_{ij}^{(t)}$ in every local SGD iteration $t$. For every iteration, if the real-time channel capacity on an edge between some node $i$ and $j$ is larger than $R_c$, i.e., $C_{(i,j)}^{(t)}\geq R_c$, then an edge between $i$ and $j$ is established. Otherwise, the edge between the two nodes does not exist. The transmission data rate for all node is set to $R_c$.
    % % \end{enumerate}
    % \item Determining the common data rate under known CSI at the devices: In this scenario, we exploit precoding~\cite{precoding2021} to fix the cluster topology during each local model training interval. Using an adequate precoder~\cite{precoding2021} and a target common rate $R_c^{(t)}$, using~(\ref{eq:shannon}) if each device $i$ transmits with instantaneous power $N_0 W(2^{R_c^{(t)}/W}-1)/\Vert h^{(t)}_{i,i'}\Vert ^2$, it can transmit with the common data rate. Thus, considering a topology (a set of neighbors for each node), each node $i$ should transmit with power $p_i^{(t)}= \displaystyle \max_{i'\in \mathcal N_i^{(t)}}\{N_0 W(2^{R_c^{(t)}/W}-1)/\Vert h^{(t)}_{i,i'}\Vert ^2\}$, where $N_i^{(t)}$ denotes the set of neighbors of node $i$. The common data rate $R_c^{(t)}$ is obtained the same as for the known CSI (i.e., using the outage probability of $5\%$ to have average degree of two inside the cluster). We consider that two nodes $i,i'$ are neighbors if $R_{i,i'}^{(t)}$ obtained for having $\textrm{p}^{\mathsf{out},(t)}_{i,i'}\leq 5\%$ from~(\ref{eq:out}) satisfies $R_{i,i'}^{(t)}\geq R_c^{(t)}$.
    % \end{enumerate}
    % \noindent [R5] J. Kim, S. Hosseinalipour, T. Kim and D. J. Love and Christopher G. Brinton, ``Multi-IRS-assisted Multi-Cell Uplink MIMO Communications under Imperfect CSI: A Deep Reinforcement Learning Approach,”arXiv preprint arXiv:2011.01141, 2021.
    % \\
    
    % \noindent [R6] T. Sery, N. Shlezinger, K. Cohen and Y. C. Eldar, ``Over-the-Air Federated Learning from Heterogeneous Data," IEEE Trans. Signal Process., 2021å
    % for having an average degree of two inside the clusters, based on which the topology is determined. 
    % AVG C_{i,j}\geq th ......
    % the determination of the cluster topology consists of two phases, i.e., (a) pre-training and (b) training. In (a), we determine a common data rate threshold $R_c$ and the cluster topology for the training phase and in (b) we determine the transmission power at each device using the knowledge of CSI. The procedure of the two phases are described as follows: 
    
    % \begin{enumerate} [(a)]
    %     \item During the pre-training phase, we first require the outage probability $P_{out}$ of a Rayleigh fading channel to be less than $5\%$, i.e.,  $P_{out} \leq 5\%$. Then based on this requirement, we obtain a local data rate threshold $R_{c,thres}^{(i,j)}$ between each set of two nodes $\{i,j\},~\forall i,j$, at which the requirement on $P_{out}$ is satisfied. Note that $R_{c,thres}^{(i,j)}$ varies across every two nodes due to the difference in geographical proximity, resulting in different path loss. 
    %     \\
    %     \noindent To guarantee that all transmission in the cluster satisfies the the outage probability requirement when transmitting at common rate $R_c$. If $R_{c,thres}^{(i,j)}\geq R_c$ (which means that the requirement for the outage probability, i.e., $P_{out}\leq 5\%$, is satisfied when data is transmitted at rate $R_c$ between $i$ and $j$) we establish an edge between $i$ and $j$. Otherwise, the edge between the two nodes does not exist. The common threshold $R_c$ is chosen from $R_{c,thres}^{(i,j)},~\forall i,j$ to make the average degree of the cluster be around $2$.
        % \item In the training phase, based on the real-time CSI, we compute the required transmission power $p_i$ for each device $i$ to guarantees successful delivery of the data (at rate $R_c$) at its neighbor devices $\forall j\in \mathcal N_i$, i.e., $C_{(i,j)}^{(t)}=W\log_2(1+\frac{p}{N_0W}\vert h_{ij}^{(t)}\vert^2)\geq R_c,~\forall j\in \mathcal N_i$. In order to be sufficient for all neighbors $j\in \mathcal N_i$ to transmit at rate $R_c$, we choose $p_i=\max_{j\in \mathcal N_i}\{N_0 W(2^{R_c/w}-1)/|hij|^2\}$.
    % \end{enumerate}å


\noindent\textbf{R1.3:}  \textit{The authors should explain why the $\Delta_k$ can be accurately estimated, I think from round-trip delays observed in recent time periods is not accurate because of dynamics of communication conditions.}\\

\noindent 
% You correctly highlight the potential inaccuracies of estimating $\Delta_k$ from round-trip delays observed in recent time periods due to the dynamics of communication conditions. potential experiments

In our approach with DFL control, we acknowledge that accurately determining real-time round-trip delay during the training process may not always be achievable. Therefore, we resort to using the most recently observed round-trip delay from the previous local model training interval as an approximation for the next. This method aims to reduce the overhead associated with real-time delay estimation.

It's worth noting that several federated learning studies, including, \cite{lin2021timescale, wang2019adaptive,wang2021HFL,Lim2021HFL}, rely on the most recent observed metrics such as energy and delay from the preceding aggregation to estimate the upcoming local model training interval.

In this work, we adopt the assumption that device-to-edge links are significantly shorter than edge-to-cloud links, thereby suggesting that the round-trip delay is predominantly dictated by propagation delay. These are typically over wired connections, frequently employing Transport control protocol (TCP). Prior research corroborates this view, indicating that the round-trip time distributions for TCP connections remain relatively stable within a {\color{blue}timescale of tens of seconds} \cite{hao2002RTT}.

Let's consider an extreme scenario where the local model training interval involves 50 (SGD) updates and there is a substantial round-trip delay of 40 (SGD) between the edge and the cloud. Supposing local model update processing rates are $R(\xi_i^{(t)})=200$ updates/s (the processing rate based on experimental results obtained using the F-MNIST dataset~[R1]), the round-trip delay for the succeeding local model training interval will be estimated 
% 20 (SGD) (0.1 s) prior to 
and being utilized for the next interval lasting 50 (SGD) (0.25 s). Given these conditions, we expect the variation in round-trip time to remain relatively stable within the 0.25 s duration compared to tens of seconds.

In addition, to elucidate the robustness of our approach to inaccurate delay estimations, we conducted a supplementary experiment, examining the impact of estimation errors on the test accuracy. This experiment involved intentionally misestimating the delay in every training round. The findings, as shown in the subsequent table, reveal that when the estimation error of the delay falls within $20$ (ms), the test accuracy degradation remains within $1\%$ for both CNN and SVM models. 

% When the estimation error exceeds $6$ (SGDs), the test accuracy starts to show approximately a $2\%$ variation for both models. However, given that the delay is re-estimated at each global aggregation stage, and assuming a network where traffic doesn't see drastic fluctuations, the approach of approximating the device-to-main server communication delay observed in the most recent local model training interval for the current training interval should provide a reasonably accurate approximation for our design.

\begin{table}[h]
  \centering
  \caption{Comparative test accuracy of DFL in relation to delay estimation error, considering round-trip delay $\Delta=10$ SGD or $50$ ms.}
  % loss of acuracy 
  \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Estimation error (\#SGD)} & \textbf{ML model} & \textbf{Degradation in test accuracy} \\
    \hline
    $\pm 2$ ($10$ ms) & SVM & $0.21\%$  \\
    \hline
    $\pm 2$ ($10$ ms) & CNN & $0.26\%$  \\
    \hline
    $\pm 4$ ($20$ ms) & SVM & $0.76\%$  \\
    \hline
    $\pm 4$ ($20$ ms) & CNN & $0.97\%$  \\
    \hline
    $\pm 6$ ($30$ ms) & SVM & $1.6\%$ \\
    \hline
    $\pm 6$ ($30$ ms) & CNN & $2.1\%$ \\
    \hline
  \end{tabular}
\end{table}
Typically, the delay variation ranges from $1\%$ to $2\%$ of the round-trip time~\cite{James2007TV}. This corresponds to a delay of $0.5$-$1$ (ms) with a round-trip delay of $50$ (ms). Therefore, these observations validate our strategy for estimating the round-trip delay.\\
% citations: why estimating the last one is the right approach 1. witihn a certain time scale , it's reasonable (conjestion control) 2. error within some range, accuracy is robust.





% We appreciate your feedback and believe these clarifications better contextualize our approach. 
This is now clarified in the main text in Sec.~\ref{subsec:form2} on page~\pageref{15}, which we quote below (including footnote) for the ease of review:\\

\textit{{\color{blue}``We assume that $\Delta_k$ can be reasonably estimated, e.g., from round-trip delays observed in recent time periods"\footnote{\textit{\color{blue}``Our methodology is not tied to any particular procedure for estimating $\Delta_k$. In practice, an approach based on recent time periods is motivated by Transport Control Protocol (TCP) analysis, where round trip times have been noted to hold relatively stable over time ranges up to tens of seconds \cite{hao2002RTT}. For our experimental settings in Sec.~\ref{sec:experiments}, we find that our methodology can tolerate delay estimation errors of $20$ ms with only a $1\%$ drop in the resulting trained model accuracy. Using a metric of delay variations being within $0.5$-$1$ ms for a round-trip delay of $50$ ms~\cite{James2007TV}, this estimation approach could handle networks with round-trip times on the order of seconds, which would be rather large, while only experiencing marginal degredations".}}.}}\\

[R1] D. J. Beutel, T. Topal, A. Mathur, X. Qiu, T. Parcollet, and N. D. Lane, “Flower: A friendly federated learning research framework,” arXiv:2007.14390, 2020.\\
 
\noindent\textbf{R1.4:}  \textit{Authors should explain why ignore the related parameters about local train on edge devices, such as local train latency, energy consumption, and computation capacity of edge devices.}\\

% time resolution, accounting for the local computation consumption 
% we focusing on convergence analysis and communication aspects, references also consider merely communication aspects rather than computation
\noindent Given that the design of DFL inherently factors in the presence of delay, our primary objective with DFL control parameters is to concentrate on communication aspects, particularly mitigating the communication cost associated with model training. The problem complexity embodies a non-convex integer programming problem with objectives encapsulating local communication delay between edge servers and their corresponding devices, which emerges from transmission delay during local aggregations, as well as global communication delay ensuing from global aggregation and synchronization.


However, we acknowledge the significance of parameters related to edge device's local training. The updated version of our performance comparison now incorporates considerations for both communication and local computation consumption, which includes the latency and energy expenditure of local training at edge devices. Nonetheless, the control algorithm we have implemented focuses primarily on reducing communication costs. These modifications were made in line with the configurations outlined in Sec.~\ref{ssec:setup} on page~\pageref{ssec:setup}. The revised experimental results are illustrated in the following figure.  
\setcounter{figure}{6}
\begin{figure}[h]
\includegraphics[width=1.\columnwidth]{control_alg/resource3.png}
\centering
\caption{Compared to the baselines, {\tt DFL} with adaptive parameter control (Algorithm~\ref{GT}) achieves significantly better results in terms of total energy and delay metrics for both CNN and SVM, as shown in (a) and (b) when reaching 80\% testing accuracy.}
\label{fig:res_r}
\vspace{-6mm}
\end{figure}

Our optimization framework offers flexibility, with the capability to be easily extended to consider local computation consumption on edge devices. This could involve introducing them into the objective function or as constraints. Such adjustments would account for local computation latency, computation energy consumption, and computation capacity, further refining our system's efficiency and performance.

The primary objective of {\tt DFL} is now clarified in the main text in Sec.~\ref{subsec:contrib} on page~\pageref{subsec:contrib}, which we quote below for the ease of review:\\

% change in the main text
\textit{{\color{blue} ``Leveraging the convergence characteristics, we introduce an adaptive control algorithm for {\tt DFL}, which targets a joint optimization of communication energy, latency, and ML performance while preserving a sub-linear convergence rate. This involves solving a non-convex integer programming problem, adapting (i) the global aggregation interval with the cloud, (ii) the local aggregation interval with the edge servers, (iii) the learning rate, and (iv) the combiner weight over time."}}\\

The modifications of the experimental setup, which encompasses local training latency and energy consumption of edge devices, are now clearly outlined in the main text, specifically in Sec.~\ref{ssec:setup} on page~\pageref{ssec:setup}. For easy review, we quote the revised version below:\\

\textit{``{\color{blue}\textbf{Communication parameters} For device-to-edge wireless communications, we assume a transmission power of $p_i=0.25$ W for device $i$, $W=1$ MHz bandwidth, and $N_0=-173$ dBm/Hz white noise spectral density. Fading and pathloss are modeled based on~\cite{tse2005fundamentals}, using $h^{(t)}_{i}= \sqrt{\beta_{i}^{(t)}} u_{i}^{(t)}$, with $\beta_{i}^{(t)} = \beta_0 - 10\widehat{\alpha}\log_{10}(d^{(t)}_{i}/d_0)$ as the large-scale pathloss coefficient and $u_{i}^{(t)} \sim \mathcal{CN}(0,1)$ for Rayleigh fading. The parameters here are chosen as $\beta_0=-30$ dB, $d_0=1$ m, $\widehat{\alpha}=3.75$, and device-to-edge server distance $d_i^{(t)}$ determined by their proximity. Channel reciprocity is assumed for simplicity. For edge-to-cloud wired communication, we employ a transmission power of $\bar{p}_{n_c}=6.3$ W, $\bar{R}_{n_c}^{(t)}=100$ Mbps data rate, and a $50$ ms propagation delay."}}\\

\textit{``{\color{blue}\textbf{Computation parameters} Each device's local computation time for performing $\tau_k$ mini-batch SGD iterations on the ML model at time $t$ can be modeled in general $T^{(t)}_{i,Comp}=\min_{i\in\mathcal I}\tau_ka_i\vert\xi_i^{(t)}\vert/f_i^{(t)}$. For our simulations, we set every device's CPU frequency to $f_i^{(t)}=15.36$ MHz, mini-batch size to $\vert\xi_i^{(t)}\vert=128$ data points, and number of CPU cycles needed to process one datapoint to $a_i=600$. The computation energy consumption of each device is modeled as $E^{(t)}_{i,Comp}=\tau_kZ_ia_iB_i^{(t)}(f_i^{(t)})^2$, where the effective chipset capacitance~\cite{Tran2019cap} is $Z_i = 2\times10^{-22}$ for all devices. With these parameters, the resultant local model update processing rates are $R(\xi_i^{(t)})=200$ updates/s for all devices. In Sec.~\ref{ssec:ctrl_DFL}, the round-trip delay $\Delta_k$ (measured in SGD iterations) will vary according to the particular realizations generated in each round $k$.}"}\\

\noindent\textbf{R1.5:}  \textit{In Sec. V, the authors should further explain why the proposed method can improve the performance.
} \\
% treating alpha a degree of freedom, compensate for the delay, how alpha actually work

A primary contribution of our method lies in its unique incorporation of a local-global combiner during the global synchronization phase in a hierarchical federated learning framework, introducing a degree of freedom to compensate for the existing delay between the device and the cloud server. This combiner weight amalgamates the outdated global model and the up-to-date local models at the time of global synchronization.

The combiner weight, $\alpha$, enables the model to place greater emphasis on the global model during the local-global model combination process. This emphasis is crucial to mitigate the bias that can occur with local models under data heterogeneity. In such instances, the local model might tend to deviate from the global optimum, particularly when the local dataset is not representative of the entire dataset distributed across the network. This aligns with the intuition presented in Theorem 1.

As the round-trip delay increases, our DFL approach tends to place more emphasis on the local model at the moment of local model synchronization with the global model. This is because the global model becomes more outdated as the delay grows. This nuanced approach contrasts with standard federated learning designs, which always replace all local models with the global model, irrespective of these considerations.

Furthermore, our method strategically structures hierarchical federated learning with aperiodic local aggregations and global aggregation times. This design gives the DFL control algorithm the flexibility to adaptively determine when to perform these operations, which in turn, allows the framework to further reduce resource consumption.

We hope this explanation helps clarify how the unique aspects of our proposed method can lead to improved performance. This is now clarified in the main text in Sec. V-B (now Sec.~\ref{ssec:conv-eval}), which we quote below for ease of review:\\

\textit{``{\color{blue}This highlights the benefits of a hierarchical model training structure within an edge-to-cloud network, where frequent local aggregations prevent non-i.i.d. data-driven deviation of local models from the optimum within each subnet.}"}\\

This has now been detailed in the main text, specifically in Sec.~\ref{subsubsec:combiner}. For convenience, we've quoted the revised portion below:\\

\textit{``{\color{blue}The performance obtained by {\tt DFL} originates from the introduction of the linear local-global combiner during global synchronization. This approach enables the synchronization process to simultaneously consider the outdated yet more generalized global model and the up-to-date yet potentially overfitted local model. Particularly in circumstances where delays are substantial, the system will benefit from preserving a portion of the local model instead of fully synchronizing it with an outdated global model. This approach ensures that the most timely insights derived from the local models are maintained.}"}\\

We have now explained this more clearly in the main text under Sec.~\ref{ssec:ctrl_DFL}. For your convenience, we've included the relevant quotation below:\\

\textit{``{\color{blue}These results demonstrate the improvements in resource-efficiency provided by {\tt DFL} with adaptive parameter control, attributed to its parameter tuning approach that concurrently considers the tradeoff between the optimality gap, as derived in Theorem~\ref{thm:subLin_m}, the communication delay, and the energy consumption. The improvement of $20-25\%$ over baseline (iv) in both metrics in particular highlights the benefit provided by our local-global model combiner strategy.}"}
% This dynamic adjustment of learning parameters according to the problem's real-time characteristics makes the {\tt DFL} control algorithm a robust and adaptable tool for managing complexities and ensuring optimal performance in distributed federated learning tasks.}





% \nm{why are you referering to R1.2? There are no details on the tx parameters there... please refer directlyto the manuscript!}
% \\
% \newpage
% \noindent [R1] T. Sery, N. Shlezinger, K. Cohen and Y. C. Eldar, ``Over-the-Air Federated Learning from Heterogeneous Data," IEEE Trans. Signal Process., 2021.
% \\

% \noindent [R2] S. Kar and J. M. F. Moura, ``Distributed Consensus Algorithms in Sensor Networks With Imperfect Communication: Link Failures and Channel Noise," IEEE Trans Signal Process., vol. 57, no. 1, pp. 355-369, 2009.
% \\

% \noindent [R1] T. Richardson and S. Kudekar, ``Design of Low-Density Parity Check Codes for 5G New Radio," IEEE Commun. Mag., vol. 56, no. 3, pp. 28-34, 2018.
% \\

% \noindent [R4] C. -S. Lee, N. Michelusi and G. Scutari, ``Finite Rate Distributed Weight-Balancing and Average Consensus Over Digraphs," IEEE Trans Autom. Control.
% \\


\newpage

\section*{Response to Reviewer 2}

\noindent\textbf{Reviewer's General Note:} \textit{The paper introduces delay-aware federated learning (DFL) to improve the efficiency of distributed machine learning (ML) model training by mitigating the round-trip communication delay between edge and cloud under a hierarchical model training architecture. Additionally, the authors investigate the convergence behavior of DFL and present an adaptive control algorithm for DFL to reduce energy consumption and edge-to-cloud communication latency while ensuring optimal ML model performance.}
\\ 
{\noindent\textbf{Positives:} 
\textit{1. The authors provide theoretical analysis on convergence guarantee. 2. The authors provide a control algorithm for tuning the controllable parameters in DFL.}}

\noindent \textbf{To Reviewer 2:} We appreciate you for reviewing our manuscript and providing us with thoughtful comments. In the following, please find point-by-point responses to your comments, as well as pointers to where they have been addressed in the revised manuscript.
\vspace{4mm}
\hrule
\vspace{4mm}

\noindent\textbf{R2.1:} \textit{Reducing communication overhead is a very common research point in federated learning. After reading the introduction, it is difficult to find that DFL is superior to other methods.}\\

% our approach can complementary to these scheme, not replacing, after reducing the delay, the propagation delay is still there.
% although we focus, no work targeting the delay, the major point is nor reduction but to account for the effect of round-trip delay, to futher bring out this points, we added XXX in sec. X. to further clarified contribution, related work and contribution

% related work blue text from the following content

% we are not trying to reducing the delay per round, but reducing the delay due to less rounds of aggregations but maximize the performance in the presence of delay [Intro]
\noindent Certainly, numerous studies aim to reduce communication overhead in federated learning, employing techniques such as model compression, edge association, asynchronous communication, and resource allocation, to name a few. These strategies often balance between model performance and communication overhead~\cite{Lim2022EI,Zhao2020HFL,Feng2022HFL,Lim2021HFL,Xu2022HFL,Luo2020HFEL,wang2021HFL,Mhaisen2022HFL}. Despite their varied approaches, they all employ the standard global aggregation or synchronization process found in FedAvg.

% add the senten
By contrast, our DFL methodology strives to not just reduce delay but also take it into account effectively. The aim is not to cut down on the communication resources needed at each aggregation to reduce delay. Instead, we focus on decreasing the total communication delay by enhancing the model's convergence rate. This approach ensures fewer rounds of model updates to achieve the desired model performance in the presence of considerable round-trip delay. Our method can be used to enhance existing works by integrating our scheme with those already established. We do this by proposing a unique approach to global synchronization (see eq.~\eqref{eq:aggr_alpha}), an aspect that is not typically emphasized or addressed in other studies. Our approach bolsters the efficiency of our federated learning model even in the presence of delays. By introducing a local-global combiner ($\alpha$), we skillfully conserve vital aspects of both the stale global model—attributed to substantial round-trip network delays between the device and cloud server—and the current local model. This technique ensures the inclusion of crucial information from both local and global models, thereby enhancing overall learning efficiency. This is a different approach as compared to the other state-of-art approach which tends to neglect the freshness of the local models of the devices and the outdatedness (i.e., staleness) of the global model in the process of global synchronization.
% hence imposes a trade-off between the model performance and communication overhead.

In summary, our approach makes better use of the global synchronization process in the presence of delay. It preserves the strengths of both global and local models in network environments with substantial round-trip delays between devices and the cloud. Unlike the conventional global synchronization process that unilaterally replaces all local models with the global one, our approach is complementary, enhancing existing works by implementing our scheme alongside them.\\

% sperated the description for each para
This is now clarified in the abstract, which we quote below for ease of review:\\

\textit{``Federated learning has gained popularity as a means of training models distributed across the
wireless edge.
{\color{blue}The paper introduces delay-aware hierarchical federated learning ({\tt DFL}) to improve the efficiency of distributed machine learning (ML) model training by accounting for communication delays between edge and cloud}. Different from traditional federated learning, {\tt DFL} leverages multiple stochastic gradient descent iterations on device datasets within each global aggregation period and intermittently aggregates model parameters through edge servers in local subnetworks. {\color{blue}During global synchronization, the cloud server consolidates local models with the outdated global model using a local-global combiner, thus preserving crucial elements of both, enhancing learning efficiency under the presence of delay.} A set of conditions is obtained to achieve the sub-linear convergence rate of $\mathcal O(1/k)$. Based on these findings, an adaptive control algorithm is developed for {\tt DFL}, implementing policies to mitigate energy consumption and communication latency while aiming for a sublinear convergence rate. Numerical evaluations show {\tt DFL}'s superior performance in terms of faster global model convergence, reduced resource consumption, and robustness against communication delays compared to existing FL algorithms. In summary, this proposed method offers improved efficiency and results when dealing with both convex and non-convex loss functions.
Federated learning has gained popularity as a means of training models distributed across the wireless edge."}\\

Furthermore, we have made changes to the content of Sec.~\ref{subsec:contrib}. The updated portion is quoted below for a simplified review process:\\

\textit{``{\color{blue}In this paper, we are motivated to address the above three challenges. In particular, we consider model training under a
% \nm{are you defining a new metric, or extending an existing one? Based on your Def 1 and Def 2, I think you are extending it} 
metric of data heterogeneity extended from the current art, which can capture extreme cases of data diversity across the devices. Also, we explicitly account for the delay in model aggregation and introduce a linear local-global model combining scheme. This scheme retains essential elements of both the outdated global model and the current local model, thereby improving the overall learning efficiency. Our methodology can augment existing studies by incorporating our strategy with those already established. Finally, we consider model training over a realistic hierarchical network edge architecture.}"}\\

Additionally, we also made revisions to the \textbf{Outline and Summary of Contributions} in Sec.~\ref{subsec:contrib}. For ease of review, we quote the revised version below:\\ 
 
{\textit{\color{blue} ``We propose \textit{delay-aware federated learning} ({\tt DFL}), a novel methodology for improving distributed ML model training efficiency by accounting for the round-trip delay between edge and cloud. 
% (Sec. \ref{sec:tthf}). 
{\tt DFL} accounts for the effects of delays by introducing a local-global model combiner scheme during global synchronization, which conserve vital aspects of both the stale global model and the current local model, thereby enhancing overall learning efficiency."}}\\

% \noindent This is now clarified in Sec. II (Page \pageref{rem:2}). We quote the added sentence below for the ease of review:\\

% ``{\color{blue} Note that the graph $G_c$ may
%     % \sst{
%     %  parameters $\mathcal S_c$, $s_c$, $\mathcal N_i$, and $G_c(\mathcal S_c,\mathcal E_c)$ for each cluster $c$ can in general} 
%      change {over time $t$}. In this paper, we only require that
%      {the set of devices in each cluster} remain fixed during each global aggregation period $k$. We drop the dependency on $t$ for simplicity of presentation, although
%     %  \nm{I dont think though is formal. used although instead (I found a few instances)}
%      the analysis implicitly accommodates it. We similarly do so in notations for node and cluster weights $\rho_{i,c}$, $\varrho_c$ introduced in Sec.~\ref{subsec:syst2} and consensus parameters $v_{i,j}$, $\mathbf V_c$, $\lambda_c$ in Sec.~\ref{subsec:syst3}. 
%     Assuming a fixed vertex set during each global aggregation period is a practical assumption, especially when the devices move slowly and do not leave the cluster during each local training interval. 
%     {Moreover, although in the analysis we assume that transmissions are outage- and error-free, in Sec.~\ref{sec:experiments} we will perform a numerical evaluation to evaluate the impact of fast fading and limited channel state information (CSI), resulting in outages and time-varying link configurations.}
%     }"
% \\

% We assume that the clusters are formed based on the ability of devices to conduct low-energy D2D communications, e.g., geographic proximity. Thus, one cluster may be a fleet of drones while another is a collection of local IoT sensors. In general, we do not place any restrictions on the composition of devices within a cluster, as long as they possess a common D2D protocol. The set of its D2D neighbors is determined based on the transmit power of the nodes, the channel conditions between them, and their physical distances. Due to the mobility of the devices, the topology of each cluster (i.e., the number of nodes and their positions inside the cluster) can change over time. For the simplicity, we will assume this evolution is slow compared to the time in between two global aggregations. 

% We have further conducted our numerical experiments considering a scenario that the edge set of each cluster, and subsequently consensus matrix among the nodes, changes between each local iteration index $t$ (please refer to Sec. V, page~\pageref{sec:experiments}). 
% \\

% \textit{(2) Impact of fast changing environment on our method:} \hl{Upon having a fast changing environment, in which due to high mobility of network nodes the cluster graph is not fixed even during one global aggregation round, our analysis still holds.}
% \nm{no please you cannot make this claim! You need to stick to our assumptions, .e. we assume that signals are received with NO ERROR. See my comment to Rev1 response. If you truly think that the model can incorporate wireless comm impariments such as error and fading, than these should have been incorporated in the model and in the analysis. Since we did not do this, let's not claim what we didnt do. Just talk about the analysis, i.e. "althoughn our analysis makes certain abstractions, we did some analysis that incorpate effeect of fading blahblahblah"}
% In particular, all the main results obtained in Proposition 1, Theorem 1, and Theorem 2 are obtained in a general setting under the relationship in (13), which implies that after performing local model aggregations, the local model at the node is a noisy version of the perfect local model. This assumption is very general and not dependent on the cluster topology. It can be imagined that upon having time varying topology, the error inside the expression (13) would be affected by a time varying graph. This only impacts the rounds of D2D communications in (50), where a time varying spectral radius $\lambda_c^{(t)}$ instead of $\lambda_c$ (and possibly time varying number of nodes inside the cluster $s_c^{(t)}$ instead of fixed $s_c$) should be used, which does not impact our methodology. 
% {\color{blue} Distributed estimation of spectral radius under noisy channel estimation will be added here}

\noindent\textbf{R2.2:}   \textit{Some symbols are very confusing (such as some time symbols in the paper). It is recommended to use some common symbols in previous FL papers.}
\\

Given the intricate nature of the system architecture and algorithm operation explored in this paper, the complexity of our notations is somewhat inevitable. Our study encompasses several factors such as aperiodic local and global aggregations, different time-scales for each local model training iteration and global aggregation, the introduction of a linear global combiner, and the definition of intermediate models to fortify and validate our convergence analysis. These diverse elements inherently contribute to the intricate nature of the notations. However, to aid in comprehension and clarity of our process, we've endeavored to simplify and explain these notations and terminologies wherever possible. For example, the network architecture can readily be extended to time-varying topologies, e.g., mobile devices switching subnets. We omit this for notational simplicity. In addition, to simplify the presentation of the convergence analysis, we assume $\alpha_k=\alpha$ (i.e., the weighing coefficient in \eqref{eq:EFGS}) $\tau_k=\tau$ (i.e., the length of local model training interval) and $\Delta_k=\Delta$ (i.e., the delay), are constants throughout the training, for all $k$.\\
% we omit subscript for time index k ..., find other place to omit subscript

To this end, we have further added two comprehensive tables (Table~\ref{table:res} and Table~\ref{table:notation}) outlining the acronyms and symbols as shown below (added to page~\pageref{table:notation} in the main text). These tables serve as a ready reference, providing explicit definitions and offering a more straightforward way to understand the parameters and variables utilized throughout this paper. We hope the addition of these tables greatly assist in understanding the context and mechanisms discussed in our work. 
% Our intent is to ensure that the complexity of the notations does not hinder the reader's comprehension and ability to follow our analysis and findings.

\setcounter{table}{0} 
\begin{table}[h]
  \centering
  \caption{List of acronyms}
  \label{table:acr_res2}
  \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Acronym} & \textbf{Definition} & \textbf{Acronym} & \textbf{Definition} \\
    \hline
    ML & Machine Learning & FL & Federated Learning \\
    \hline
    DFL & Delay-Aware Federated Learning & SGD & Stochastic Gradient Descent \\
    \hline
    SVM & Support Vector Machine & CNN & Convolutional Neural Network \\
    \hline
  \end{tabular}
\end{table}

\begin{table}[h]
  \centering
  \caption{List of key notations}
  \label{table:notation2}
  \begin{tabular}{|c|m{6.3cm}|c|m{6.3cm}|}
    \hline
    \textbf{Notation} & \textbf{Description} & \textbf{Notation} & \textbf{Description} \\
    \hline
    $\mathcal N$ & Set of all edge servers & $N$ & Number of all edge servers \\
    \hline
    $\mathcal I$ & Set of all edge devices & I & Number of all edge devices \\
    \hline
    $\mathcal S_c$ & Set of edge devices in subnet $c$ & $s_c$ & Number of edge devices in subnet $c$ \\
    \hline
    $n_c$ & Index of local model aggregator (edge server) & $\ell(\cdot)$ & Loss function associated with each datapoint \\
    \hline
    $\mathcal D_i$ & Dataset of edge devices $i$ & $D_i$ & Number of data points in edge device $i$ \\
    \hline
    $F_i(\cdot)$ & Local loss function for device $i$ & $\bar{F}_c(\cdot)$ & Subnet loss function for subnet $c$ \\
    \hline
    $F(\cdot)$ & Global loss function & $\mathbf w^*$ & Optimal global model \\
    \hline
    $\delta, \zeta$ & Inter-gradient diversity parameters across subnets & $\delta_c,\zeta_c$ & Intra-gradient diversity parameters in subnet $c$ \\
    \hline
    $t$ & Time index of local model training iteration & $k$ & Index of local model training interval\\
    \hline
    $\mathcal T_k$ & $k$-th local model training interval & $\tau_k$ & Length of the $k$-th local model training interval \\
    \hline
    $\mathcal T_{k,c}^\mathsf{L}$ & Set of local aggregation instances in the $k$-th local model training interval & $\widehat{\mathbf g}_{i}^{(t)}$ & Local stochastic gradient at device $i$ and time $t$ \\
    \hline
    $\mathbf w_i^{(t)}$ & Local model at device $i$ and time $t$ & $\widetilde{\mathbf w}_i^{(t)}$ & Tentative Local model at device $i$ and time $t$ \\
    \hline
    $\bar{\mathbf{w}}_{c}^{(t)}$ & Instantaneous aggregated local model across devices $i\in\mathcal S_c$ & $\bar{\mathbf w}^{(t)}$ & Global model at time $t$ \\
    \hline
    $\Theta_c^{(t)}$ & Indicator of local aggregation for subnet $c$ & $\alpha_k$ & Combiner weight for the linear local-global combiner \\
    \hline
    $\Delta^\mathsf{U}_k$ & Upstream delay between edge device and main server & $\Delta^\mathsf{D}_k$ & Downstream delay between edge device and main server \\
    \hline
  \end{tabular}
\end{table}

The revised content is now clarified in the main text in Sec.~\ref{subsec:syst2} (Page~\pageref{subsec:syst2}), which we quote below for ease of review:\\

% however, in general they may vary across k
\textit{``{\color{blue}This model is capable of being adapted to time-varying topologies, for instance,  mobile devices switching subnet. However, we omit this for notational simplicity.}"}\\

This point has now been emphasized in the main text under Sec.~\ref{sec:convAnalysis} (Page~\pageref{sec:convAnalysis}). For convenience, we have quoted the revised section below:\\

\textit{``{\color{blue}To simplify the presentation of the convergence analysis, we assume that $\alpha_k=\alpha$ (the weighting given in \eqref{eq:EFGS}), $\tau_k=\tau$ (the duration of the local model training interval), and $\Delta_k=\Delta$ (the delay) remain constant throughout the training process, for all $k$.}"}\\


% We believe this additional reference tool will greatly assist in understanding the context and mechanisms discussed in our work. Our intent is to ensure that the complexity of the notations does not hinder the reader's comprehension and ability to follow our analysis and findings.\\

%  To further demonstrate this point, we have performed a numerical evaluation, depicted below, based on our derived expressions above for linear regression problem.
% We observe that our proposed bound is tighter than the conventional one, which is instead quite pessimistic.  Moreover, as we increase $\Vert\mathbf w-\mathbf w^*\Vert_2$, we observe that $\Vert\nabla F_i(\mathbf w)-\nabla F(\mathbf w)\Vert_2$ increases linearly, which is consistent with our defininition of gradient diversity.



% \nm{remove the rest}
% \\

% to demonstrate that our 
% proposed definition results in a smaller parameters\nm{I dont care about smaller parameters, I just care about tighter BOUND} used in the upper bounds.\sst{ In particular, $\delta_{new}$ is smaller as compared to the parameter $\delta_{classic}$ used in the classic definition, which makes our bound more reliable. This is achieved via adding the term $\zeta$ in our bound.}


% Adding the term $\zeta$ brings a coupling between the diversity of gradient and the distance to the global optima, which is natural as we show in (8)\nm{8 what? "Eql (8) in the revised manuscript" Be precise otherwise it can be confusing}. Our modified gradient diversity only requires $\Vert\nabla\hat F_c(\mathbf w^*)\Vert \leq \delta$, which is a bounded local gradient at the optimal global model. This is a more practical assumption in literature requiring $\Vert\nabla\hat F_c(\mathbf w)-\nabla F(\mathbf w)\Vert \leq \delta$ that must hold $\forall\mathbf w$, which is not true even for a simple quadratic function.\footnote{Consider the case for the simplest problem of linear regression. In this case
% $$ 
% \nabla F_i(\mathbf w)=-\mathbf X_i(\mathbf y_i-\mathbf X_i^T\mathbf w)
% $$
% $$
% \nabla F(\mathbf w)=-\sum_{j}\rho_j\mathbf X_j(\mathbf y_j-\mathbf X_j^T\mathbf w)
% $$
% and
% $$
% \nabla F_i(\mathbf w)-\nabla F(\mathbf w)
% =
% \mathbf X_i\mathbf X_i^T
% [\sum_{j}\rho_j\mathbf X_j\mathbf X_j^T]^{-1}\sum_{j}\rho_j\mathbf X_j\mathbf y_j
% -\mathbf X_i\mathbf y_i
% +\left[\mathbf X_i\mathbf X_i^T
% -\sum_{j}\rho_j\mathbf X_j\mathbf X_j^T\right](\mathbf w-\mathbf w^*)
% $$
% where
% $$
% \mathbf w^*
% =[\sum_{j}\rho_j\mathbf X_j\mathbf X_j^T]^{-1}\sum_{j}\rho_j\mathbf X_j\mathbf y_j
% $$
% is the optimal point.
% It can be easily seen that 
% $\Vert\nabla F_i(\mathbf w)-\nabla F(\mathbf w)\Vert\to\infty$
% when $\Vert\mathbf w-\mathbf w^*\Vert\to\infty$ (i.e., large gap between the model and the optimal model), which is expected to occur specially at the beginning of model training. This makes the classic definition $\Vert\nabla F_i(\mathbf w)-\nabla F(\mathbf w)\Vert\leq\delta$ becomes impractical since the bound built on top of it would suffer from very large values of $\delta$.
% However, in our definition we bound
% $$\Vert\nabla F_i(\mathbf w)-\nabla F(\mathbf w)\Vert\to\infty\leq\delta+\zeta\Vert\mathbf w-\mathbf w^*\Vert$$
% where the parameters
% $$
% \delta=\Vert\mathbf X_i\mathbf X_i^T
% [\sum_{j}\rho_j\mathbf X_j\mathbf X_j^T]^{-1}\sum_{j}\rho_j\mathbf X_j\mathbf y_j
% -\mathbf X_i\mathbf y_i\Vert
% $$$$
% \zeta=\Vert\mathbf X_i\mathbf X_i^T
% -\sum_{j}\rho_j\mathbf X_j\mathbf X_j^T\Vert
% $$ are much more tractable/stable.}  Nevertheless, adding the term $\zeta$ leads to unique characteristics of the bounds, obtaining of which requires a new set of bounding techniques such as mathematics of dynamic systems, exploiting Riemann theorem to bound summations with integrals, and a series of algebraic manipulations, which are among the first to appear in federated learning literature. 
 
  
%  \nm{doesnt seem that important...}
% Finally, the two terms $\delta$ and $\zeta$ repeatedly appear in the final results, e.g., in the main result of Proposition 1 in (21) and in the main result of Theorem 2 in (37) encapsulated in $\nu$ (note that the term $\omega$ appearing in the bounds is defined as $\omega=\frac{\zeta}{2\beta}$). 

% This assumption is imposed since in federated learning, the local dataset of the devices are assumed to be non identical and independently distributed (non-i.i.d), and the convergence performance is always dependent on the level of heterogeneity across the devices local datasets. The gradient diversity defined in the paper, characterized by two parameters $\delta$ and $\zeta$, is a more generalized as compared to the conventional definition of gradient diversity (with $\zeta = 0$) defined in the highly cited paper [R1], which is used in a lot of contemporary works % {\color{red} please add some citations here!}.  
% [R2]-[R6]. 
 


% In particular, $\delta_{new}$ is smaller as compared to the parameter $\delta_{classic}$ used in the classic definition, which makes our bound more reliable. This is achieved via adding the term $\zeta$ in our bound.
% \begin{figure}[H]
% % \vspace{-0.15in}
% \centering
% \includegraphics[width=0.6\textwidth]{GDn100.png}
% \hspace{0.3in}
% \caption{
% \nm{please start the x axis from 0! The figure does not help support your point as it is..
% }
% \nm{I had suggested the following: instead of mnist, generate w randomly around w* as a Gaussian. by changin the variance of the gaussian you can generate differnt points in the x axis.}
% Comparison of gradient diversity upper bound between the classic, i.e., $\Vert\nabla\hat F_c(\mathbf w)-\nabla F(\mathbf w)\Vert \leq \delta_{classic}$, and proposed definition, i.e., $\Vert\nabla\hat F_c(\mathbf w)-\nabla F(\mathbf w)\Vert \leq \delta_{new}+ \zeta \Vert\mathbf w-\mathbf w^*\Vert$. The bounds are obtained for a linear regression problem with the expressions derived in \textbf{R2.2}. The solid blue points are actual realizations of the diversity obtained usin theg MNIST dataset. Our proposed definition results in a tighter upper bound compared to the classic definition.}
% % \nm{the proposed bound is not as tight as it could be... can you make it look tighter? Also, please remove delta and zeta curves from the figure, they are quite confusing..
% % Also, can you scale to larger values of |w-w*|? To do so, take w*. Take w as Gaussian with mean w* and variance sigma, where you vary sigma to different values. I would like to see much larger values. Should be very quick to do.
% % \hl{There exist $20$ solid blue points that are obtained using the first $20$ iterations of the model training using the local and global gradient values.}
% % \nm{no need to do this to prove your point... just do what I suggest, it is faster and you generate a lot more points}$\mathbf w^*$ is the global parameter at iteration $25$.

% In particular, \hl{$\delta_{new}$ is smaller as compared to the parameter $\delta_{classic}$ used in the classic definition}}
% \nm{I dont think this is the main point... you just want to show that the proposed bound is below the conventional one..} which makes our bound \hl{more reliable}\nm{what do you mean by more realiable?}. This is achieved via adding the term $\zeta$ in our bound.}
% \label{fig:overview}
% \end{figure}

% {\color{red} TO DO!}
% observed in the convergence analysis of Proposition 1 and Theorem 2, this makes the theoretical analysis completely different from state-of-art federated learning literature in deriving the convergence bound. The intuition behind the definition is that when the updated model $\mathbf w$ is far away from the global optimum $\mathbf w^*$, the parameters $\delta$ and $\zeta$ remains small and this makes the analysis more tractable. This is especially true in the initial iterations of the model training, where $\Vert\mathbf w-\mathbf w^*\Vert$ can be arbitrary large. However, we observe that if we directly apply the conventional definition of gradient diversity (solely using $\delta$ to capture quantity of gradient diversity) can be troublesome since in that case $\delta$ can become prohibitively large.
% In R1, they assume that zeta=0, some paper also.. reference,
% (i) does not apply to quadratic 
% (ii) if zeta=0, delta is very large
% adding that the constraint is only required at w* for nabla F_c, add strongly convex does not staisy the convention definition
% adding numerical simulation: 
% \noindent [R1] S. Wang, T. Tuor, T. Salonidis, K. Leung, C. Makaya, T. He and K. Chan ``Adaptive federated learning in resource constrained edge computing systems," IEEE J. Sel. Areas Commun., vol. 37, no. 6, pp. 1205-1221, 2019.
% \\


\noindent\textbf{R2.3:}  \textit{The overall structure of the paper is rather chaotic, it is recommended to reorganize the structure. Related work can be considered as an independent chapter, and the introduction can only introduce work that is more related to this paper. The second chapter introduces that the workflow of DFL is too complicated, and some symbol definitions of time are easily confusing.}
\\

In response to your feedback, we have undertaken a reorganization of our manuscript. The section detailing the related work has now been restructured to stand independently. It provides an overview of the existing research relevant to our study, placing our work in a broad context and aiding in a clearer understanding of the novelty and significance of our contribution. The updated arrangement can be viewed on page~\pageref{sec:RW} of our revised manuscript. In addition (see our response to R2.2), we have also added a table as to clarify the definition of symbols.\\

We have included a footnote to elucidate the procedure of local aggregation within the system \textbf{overview and rationale} of chapter two, now labeled as Sec.~\ref{subsec:syst3} located on page~\pageref{subsec:syst3}. The changes are as follows:\\

\textit{``{\color{blue} Note that in fig.~\ref{fig:twoTimeScale}, the parameters $m_1$ and $m_2$ may not be equal. In fact, {\tt DFL} allows for aperiodic local aggregations during each local model training interval, the frequency of which will be later tuned in our control algorithms to mitigate network resource consumption.}"}\\

We have also revised the content in Sec.~\ref{subsec:proc} (page~\pageref{subsec:proc}) for greater clarity:\\

\textit{``{\color{blue} At each instance of global aggregation, the models of the devices are collected (i.e., they engage in uplink transmissions) and arrive at the server with a certain delay. The server will then aggregate the received models into a global model and broadcast it (i.e., engage in downlink transmission) to the devices, which involves another delay. In parallel, devices proceed with local updates before the global model arrives. Upon reception of the global model, devices synchronize their local models in a manner accounting for both uplink and downlink delays. The relationships between the timescales are  depicted in Fig.~\ref{fig:twoTimeScale}.}"}\\

\textit{``{\color{blue}The length of the $k$th local model training interval is denoted by $\tau_k = |\mathcal{T}_k|$. We have $t_k = \sum_{\ell=0}^{k-1}\tau_\ell$, $\forall k$, where $t_0 = 0$ and $t_K=T$.} This allows for varying the length of local model training intervals across global synchronizations (i.e., $\tau_{k} \neq \tau_{k'}$ for $k \neq k'$). During the $k$th local model training interval, we define $\mathcal{T}^\mathsf{L}_{k,c} \subset \mathcal{T}_k$ as the set of time instances when the edge server pulls the models of devices in subnet $\mathcal{S}_c$ (i.e., devices engage in uplink transmissions) and performs local aggregations of their models. Local aggregation synchronizes the local models within the same subnet by computing their weighted average. We next formalize the above procedure."}\\

We have additionally undertaken revisions to the discussion of the \textbf{tentative local model} within the DFL formalization, which can be found in Sec.~\ref{subsec:form2} on page~\pageref{subsec:form2}. The alterations are as follows:\\

\textit{``\textbf{Tentative Local Model}: Using the gradient estimate $\widehat{\mathbf g}_{i}^{(t-1)}$, each device computes its \textit{tentative local model} $ {\widetilde{\mathbf{w}}}_i^{(t)}$ as
\vspace{-3.5mm}
\begin{align} \label{8}
    {\widetilde{\mathbf{w}}}_i^{(t)} = 
           \mathbf w_i^{(t-1)}-\eta_{k} \widehat{\mathbf g}_{i}^{(t-1)},~t\in\mathcal T_k, 
\end{align}
{\color{blue} where $\eta_{k} > 0$ denotes the step size. We denote this tentative because it is an intermediate calculation prior to any potential aggregation. Specifically, 
based on ${\widetilde{\mathbf{w}}}_i^{(t)}$, the \textit{updated local model} $\mathbf{w}_i^{(t)}$ is computed either through setting it to ${\widetilde{\mathbf{w}}}_i^{(t)}$ or through a local aggregation, discussed next.}"}\\

We have further updated the section detailing the \textbf{Communication Delay and Global Model Aggregations} in the DFL formalization (including footnote), located in Sec.~\ref{subsec:form2}. The revised content is as follows:\\

\textit{``{\color{blue} In conducting global aggregations, to account for the round-trip delay, devices send their local models to the edge servers $\Delta_k$ time instances prior to the completion of each local model training interval $\mathcal{T}_k$, i.e., at $t=t_{k+1}-\Delta_k \in \mathcal{T}_k$. Concurrently, these devices carry out an additional $\Delta_k$ local updates using SGD/local aggregation before they receive the updated global model. We assume that $\Delta_k$ can be reasonably estimated, e.g., from round-trip delays observed in recent time periods}\footnote{\color{blue}\textit{``Our methodology is not tied to any particular procedure for estimating $\Delta_k$. In practice, an approach based on recent time periods is motivated by Transport Control Protocol (TCP) analysis, where round trip times have been noted to hold relatively stable over time ranges up to tens of seconds \cite{hao2002RTT}. For our experimental settings in Sec.~\ref{sec:experiments}, we find that our methodology can tolerate delay estimation errors of $20$ ms with only a $1\%$ drop in the resulting trained model accuracy. Using a metric of delay variations being within $0.5$-$1$ ms for a round-trip delay of $50$ ms~\cite{James2007TV}, this estimation approach could handle networks with round-trip times on the order of seconds, which would be rather large, while only experiencing marginal degredations."}}"}.\\


%%%% add quote to the change in section III. %%%%
% we simpified the notations..
%  just make it a independent section

\noindent\textbf{R2.4:}  \textit{The comparison between DFL and FedAvg is not fair enough since FedAvg is a very standard algorithm and its goal is not to reduce resource consumption. DFL should be compared with other hierarchical FL methods.}
\\
%  we had added a table
% reference the baselines we are using are used in all these hierarchical paper. to see if we can add an experiment. Since this is an extension, we can show some relevant boost
% alpha = 0 (it reduces global aggregation to fedavg), it's fair to compare with the same optimization framework. the delay has a certain impact on the performance, people does not consider this issue

% the main focus is in accouting for delay

% \noindent 
% We appreciate your feedback on the comparison between our method (DFL) and the standard Federated Averaging (FedAvg). Our intention in making this comparison was to highlight the potential advantages of employing a local-global combiner in a hierarchical federated learning context. It's worth noting that many existing studies on hierarchical federated learning employ FedAvg as their global synchronization method, as referenced in our work~\cite{Feng2022HFL,Lim2021HFL,Xu2022HFL,Luo2020HFEL,wang2021HFL,Mhaisen2022HFL}.

% Our evaluation in Sec.~\ref{ssec:control-eval} sought to underscore the effectiveness of DFL's optimization framework, particularly its adaptive parameter control. This mechanism adjusts learning parameters to strike a balance between performance and efficiency, taking into account both communication resources and energy consumption.
% We've also carried out supplementary experiments to assess the optimization efficiency of the control algorithm. Specifically, we've drawn comparisons between DFL with fixed parameters (Algorithm~\ref{DFL}) and DFL with adaptive parameter control (Algorithm~\ref{GT}), as detailed in Section~\ref{ssec:ctrl_DFL}. The outcomes demonstrated that Algorithm~\ref{GT} necessitated $28.5\%$ and $29.5\%$ less energy for the CNN and SVM models respectively, compared to Algorithm~\ref{DFL}. Moreover, Algorithm~\ref{GT} consumed $28.3\%$ and $30.3\%$ less energy for the CNN and SVM models respectively, when compared to Algorithm~\ref{DFL}. These results further highlight the efficacy of our approach in the control algorithm design.

\noindent We acknowledge your concerns about the fairness of the comparison between our DFL method and the standard FedAvg algorithm. In response, 
\iffalse
we have performed additional experiments to compare our DFL (with a predetermined control parameter of $\alpha=0.5$) with the conventional hierarchical FedAvg. The conditions for both DFL and hierarchical FedAvg were identical, including a local model training interval of $\tau=20$, local aggregations performed after each edge device carried out $5$ local model updates, and a round-trip delay of $\Delta=10$. The sole difference resided in the global synchronization process.

According to our results, DFL showcased significant improvements in energy efficiency and latency reduction compared to hierarchical FL. More specifically, DFL reduced energy consumption by $11.7\%$ and $12.4\%$ for CNN and SVM models, respectively. Also, as shown in Fig.~\ref{fig:res_r}, DFL decreased the delay by $11.3\%$ and $12.5\%$ compared to hierarchical FedAvg for the CNN and SVM models, respectively.
\fi
% \begin{figure}[h]
% \includegraphics[width=0.8\columnwidth]{control_alg/resource_set.png}
% \centering
% \caption{Compared to the hierarchical {\tt FedAvg}, {\tt DFL} with $\alpha=0.5$ (Algorithm~\ref{GT}) achieves significantly better results in terms of total energy and delay metrics for both CNN and SVM, as shown in (a) and (b) upon reaching 80\% testing accuracy when delay $\Delta=10$.}
% \label{fig:res_set2}
% \vspace{-5mm}
% \end{figure} 

we conducted a comparison between the DFL control algorithm and Hierarchical FL with adaptive parameter control. The hierarchical FedAvg with adaptive parameter control was constructed by setting $\alpha=0$ in the {\tt DFL} control algorithm, thereby reducing the DFL's global aggregation to the standard method commonly employed in current hierarchical federated learning studies~\cite{Feng2022HFL,Lim2021HFL,Xu2022HFL,Luo2020HFEL,wang2021HFL,Mhaisen2022HFL}. The only distinguishing feature between the two algorithms was the global synchronization process.

Our results indicated that the DFL control algorithm significantly decreased energy consumption and latency compared to hierarchical FL with adaptive parameter control. Specifically, the DFL control algorithm reduced energy consumption by $19.9\%$ and $21.1\%$ for CNN and SVM models, respectively. Also, as depicted in Fig.~\ref{fig:res_r}, the DFL control algorithm required $25.8\%$ and $24.5\%$ less delay than the hierarchical FL with adaptive parameter control for CNN and SVM models, respectively.

We believe these additional experimental results offer a fairer comparison and demonstrate the efficiency of DFL in comparison to hierarchical federated learning methods. 
% not meant to reduce delay and energy, but using this, is a way to represent the benefit for accouting for delay
\noindent This is now highlighted in Sec.~\ref{ssec:ctrl_DFL} (Page~\pageref{ssec:ctrl_DFL}). We quote the added sentence below for ease of review:\\
\setcounter{figure}{6}
\begin{figure}[h]
\includegraphics[width=1.\columnwidth]{control_alg/resource3.png}
\centering
\caption{Compared to the baselines, {\tt DFL} with adaptive parameter control (Algorithm~\ref{GT}) achieves significantly better results in terms of total energy and delay metrics for both CNN and SVM, as shown in (a) and (b) when reaching 80\% testing accuracy.}
\label{fig:res_r}
\vspace{-6mm}
\end{figure}

% {\color{blue}To further underscore the advantage of {\tt DFL}'s rapid convergence rate under the existence of delay, we evaluated its resource consumption performance—(a) communication energy consumption and (b) communication delay—against hierarchical FL when achieving an 80\% testing accuracy under the existence of non-negligible delay (i.e., $\Delta=10$). The results depicted in Fig.~\ref{fig:res} reveal that {\tt DFL} necessitates $11.7\%$ and $12.4\%$ lower energy consumption compared to hierarchical FL for CNN and SVM models respectively. Similarly, Fig.~\ref{fig:res} illustrates that {\tt DFL} calls for a $11.3\%$ and $12.5\%$ reduction in delay relative to hierarchical FL for CNN and SVM models correspondingly.}
% {\color{blue}The commendable performance of {\tt DFL} originates from the introduction of the linear local-global combiner during global synchronization. This approach enables the synchronization process to simultaneously consider the outdated global model and the up-to-date local model. Particularly in circumstances where delays are substantial, the system gives prominence to the local model instead of just synchronizing it with an outdated global model. This approach ensures that the valuable insights derived from the local models are maintained.}\\

\textit{``{\color{blue}The results presented in Fig.~\ref{fig:res} compare the performance of {\tt DFL} with adaptive parameter control (Algorithm~\ref{GT}) with four baseline approaches: (i) FL with full device participation and $\tau=1$; (ii) hierarchical FL with $\alpha=0$, $\tau=20$, where local aggregations are performed after every $5$ local model updates; (iii) {\tt DFL} with fixed parameters (Algorithm~\ref{DFL}) with $\alpha=0.5$, $\tau=20$, where local aggregations are performed after every $5$ local model updates; and (iv) {\tt HFL} with parameter control, constructed by setting $\alpha=0$ in the DFL control algorithm, thereby reducing the
DFL’s global aggregation to the standard global aggregation method in~\cite{Feng2022HFL,Lim2021HFL,Xu2022HFL,Luo2020HFEL,wang2021HFL,Mhaisen2022HFL}. Two metrics are used to compare the performance: (M$_1$) total energy consumption and (M$_2$) total delay, each measured upon reaching $80\%$ testing accuracy. For (M$_1$), the results indicated by the blue bars of Fig.~\ref{fig:res}(a) \& (b) show that {\tt DFL} with adaptive parameter control significantly outperforms baseline approaches. Specifically, {\tt DFL} requires $91.8\%$ and $90.5\%$ less energy than baseline (i), $71.4\%$ and $70.4\%$ less energy than baseline (ii), $67.5\%$ and $66.1\%$ less energy than baseline (iii), and $19.9\%$ and $21.1\%$ less energy than baseline (iv) for CNN and SVM models, respectively. Similarly, for (M$_2$), {\tt DFL} requires substantially less communication delay than both baseline approaches as shown in the red bars of Fig.~\ref{fig:res}(a) \& (b). Specifically, {\tt DFL} requires $94.2\%$ and $95.1\%$ less delay than baseline (i), $44.1\%$ and $43.2\%$ less delay than baseline (ii), $36.6\%$ and $35.1\%$ less delay than baseline (iii), and $25.8\%$ and $24.5\%$ less delay than baseline (iv) for CNN and SVM models, respectively. These results demonstrate the improvements in resource-efficiency provided by {\tt DFL} with adaptive parameter control, attributed to its parameter tuning approach that concurrently considers the tradeoff between the optimality gap, as derived in Theorem~\ref{thm:subLin_m}, the communication delay, and the energy consumption. The improvement of $20-25\%$ over baseline (iv) in both metrics in particular highlights the benefit provided by our local-global model combiner strategy.}"}\\

% {\color{blue}To highlight the efficiency of the {\tt DFL} approach more convincingly, we've drawn a comparative analysis between the performance of {\tt DFL} employing a local-global combiner and the hierarchical {\tt FedAvg} model as outlined in~\cite{liu2020client,Feng2022HFL,Lim2021HFL,Xu2022HFL}, with both methods leveraging local aggregations.}\\ 

\noindent\textbf{R2.5:}  \textit{The paper mentions the problem of non-iid, but the paper did not do experiments under non-iid settings.}
\\

%  we have re emphasize the setting
\noindent Sorry for the confusion, we would like to mention that we have indeed conducted experiments under non-IID settings, though it may not have been explicitly stated.

In our experimental setup, outlined in Sec.~\ref{ssec:setup} on page~\pageref{ssec:setup}, we distributed the data across devices such that each device holds data points from only three out of ten possible labels, thereby simulating a degree of statistical data heterogeneity among devices, which is a common method in the literature~\cite{lin2021timescale,wang2021device,hosseinalipour2020multi}.

Furthermore, in Figure 6, we have indicated how the choice of the combiner weight parameter $\alpha$ varies with the number of labels present in the data distributed across device, thus representing different degrees of non-IID data.

% The methodology for representing non-IID data across devices is also validated in the following research papers:

% To address your comment and enhance clarity, we have undertaken a revision and re-emphasize the pertinent descriptions within Section V.

This is now clarified in Sec.~\ref{ssec:setup} (Page \pageref{ssec:setup}). We quote the added sentence below for the ease of review:\\

\textit{``{\color{blue}We distribute the dataset across devices in a non-i.i.d. manner such that each device has data points exclusively from $s$ of the $10$ total labels. This results in an inherent data skewness across devices which is commonly employed to simulate non-i.i.d. settings in federated learning~\cite{lin2021timescale,wang2021device,hosseinalipour2020multi}.  By default, we set $s = 3$.}"}

% \nm{use pageref instead! For instance, Theorem \ref{thm:subLin} on page \pageref{thm:subLin}} containing more details is quoted below:

% \nm{this is too long, no need to have it here, just refer to the section in the paper.}
% % XXXXXX . Once we demonstrate~\eqref{eq:innerIter1} holds at $t+1$, we simultaneously prove that~\eqref{eq:globalIter1} holds for $t=t_k$ and completes the proof."

% \setcounter{lemma}{0}
% \setcounter{proposition}{0}
% \setcounter{theorem}{0}
% \setcounter{definition}{0}
% \setcounter{assumption}{0}


% \setcounter{equation}{37}
% \begin{skproof}
% ``
% The complete proof is provided in Appendix~D of our technical report~[42]. Here, we summarize the key steps.

% The proof is carried out by induction. We aim to prove  
% \begin{align} \label{eq:thm2_result-1-T}
%     &\mathbb E\left[F(\hat{\mathbf w}^{(t)})-F(\mathbf w^*)\right]  
%     \leq \frac{\nu}{t+\alpha},\ \forall t,
% \end{align}
% holds, considering the effects of all global aggregations and the local model training interval between consecutive global aggregations. To do so, we need to do two inductions: (i) outer induction: induction across all the global aggregation indices, i.e., $k$, to demonstrate that~\eqref{eq:thm2_result-1-T} holds across all global aggregations, and (ii) inner induction: induction across the local model training interval $t\in\mathcal T_k,~\forall k$. We start with the outer induction and consider $t_0$ as the basis of induction. We see that the condition in~\eqref{eq:thm2_result-1-T} trivially holds when $t = t_0 = 0$, since $
% \nu\geq\alpha[F(\hat{\mathbf w}^{(0)})-F(\mathbf w^*)]$ by its definition. We then focus on the the outer induction hypothesis, and presume that
% \begin{align} \label{eq:globalIter1}
%     \mathbb E\left[F(\hat{\mathbf w}^{(t_{k-1})})-F(\mathbf w^*)\right]  
%         \leq \frac{\nu}{t_{k-1}+\alpha}
% \end{align}
% holds for $t=t_{k-1}$ for some $k\geq 1$. Finally, to complete the proof for the outer induction, we aim to prove that the induction holds for $t=t_{k}$, i.e., $\mathbb E\left[F(\hat{\mathbf w}^{(t_{k})})-F(\mathbf w^*)\right]  \leq \frac{\nu}{t_{k}+\alpha}$. Note that since proving the general relationship
% \begin{align} \label{eq:innerIter1}
%         &\mathbb E\left[F(\hat{\mathbf w}^{(t)})-F(\mathbf w^*)\right]  
%         \leq \frac{\nu}{t+\alpha},\ \forall t\in \mathcal{T}_k,
% \end{align}
% directly implies that~\eqref{eq:globalIter1} holds for $t=t_{k}$, we focus on proving~\eqref{eq:innerIter1}, for which the proof can is carried out via the inner induction over $t\in \mathcal T_k$. To complete the proof using the inner induction, consider $t = t_{k-1}$ as the basis of induction. We note that the condition in~\eqref{eq:thm2_result-1-T} holds as a result of the induction hypothesis from the outer induction. Now, for the induction hypothesis in the inner induction, we suppose
% \begin{align} \label{eq:final}
%         &\mathbb E\left[F(\hat{\mathbf w}^{(t)})-F(\mathbf w^*)\right]  
%         \leq \frac{\nu}{t+\alpha},
% \end{align}
% holds for $t\in \{t_{k-1},\dots,t_k-1\}$, and aim to demonstrate that it holds at $t+1$ as follows.
    
%     From the result of Theorem~1,
%     % \begin{align} \label{eq:thm1_temp}
%     %     &\mathbb E\left[F(\hat{\mathbf w}^{(t+1)})-F(\mathbf w^*)\right]
%     %     \leq
%     %     (1-\tilde{\mu}\tilde{\eta}_{t})\frac{\nu}{t+\alpha}
%     %     +\frac{\tilde{\eta}_{t}\beta}{2}A^{(t)}
%     %     +\frac{1}{2}[\tilde{\eta}_{t}^3\tilde{\phi}^2+\tilde{\eta}_{t}^2\tilde{\sigma}^2+\tilde{\eta}_{t+1}^2\tilde{\phi}^2].
%     % \end{align} 
% using the induction hypothesis, the bound on $A^{(t)}$, $\epsilon^{(t)}=\eta_t\phi$, and the facts that $\eta_{t+1}\leq \eta_t$, $\eta_t\leq \eta_0\leq\frac{\mu}{\beta^2}\leq 1/\beta$ and $\epsilon^{(0)}=\eta_0\phi\leq\phi/\beta$, we get
% \begin{equation} \label{eq:thm1_SigT}
%     % \hspace{-4mm}\resizebox{.94\linewidth}{!}{$
%     \begin{aligned}
%         &\mathbb E\left[F(\hat{\mathbf w}^{(t+1)})-F(\mathbf w^*)\right]
%         \leq
%         \left(1-\mu\eta_{t}\right)\frac{\nu}{t+\alpha} + \frac{\eta_t^2\beta}{2}\left(\sigma^2+2\phi^2\right)
%          \\&
%         +\frac{8\eta_t\omega^2\beta^2}{\mu}\underbrace{(\Sigma_{+,t})^2}_{(a)}\frac{\nu}{t_{k-1}+\alpha}
%         % \nonumber \\&
%         \;\; +\frac{25}{2}\eta_t\underbrace{(\Sigma_{+,t})^2}_{(b)}
%         \left(\sigma^2+\phi^2+\delta^2\right)  ,
%     \end{aligned}
%     % $} \hspace{-3mm}
% \end{equation}
% \vspace{-0.05in}

% \noindent where $\Sigma_{+,t}$ is given in Proposition~1.
% To get a tight upper bound for~\eqref{eq:thm1_SigT}, we bound the two instances of $(\Sigma_{+,t})^2$ appearing in $(a)$ and $(b)$ differently. For $(a)$, we first use the fact that $
%     \lambda_+ =1-\frac{\mu}{4\beta}+\sqrt{(1+\frac{\mu}{4\beta})^2+2\omega}\in[2,1+\sqrt{3}]$,
% which implies
% % \begin{align}
% %     \Sigma_{+,t}
% %   =&\sum_{\ell=t_{k-1}}^{t-1}
% %   \left[\prod_{j=t_{k-1}}^{\ell-1}(1+\tilde{\eta}_j\lambda_{+})\right]
% %   \eta_\ell
% %   \left[\prod_{j=\ell+1}^{t-1}(1+\tilde{\eta}_j)\right]
% %   \nonumber \\&
% %   \leq
% %   \left[\prod_{j=t_{k-1}}^{t-1}(1+\tilde{\eta}_j\lambda_{+})\right]
% % \sum_{\ell=t_{k-1}}^{t-1}\frac{\tilde{\eta}_\ell}{1+\tilde{\eta}_\ell\lambda_{+}}.
% % \end{align}
% % Also, with the choice of step size $\tilde{\eta}_\ell=\frac{\tilde{\gamma}}{\ell+\alpha}$, we get
% \begin{equation} \label{eq:Sigma_1T}
%     % \hspace{-6.5mm}\resizebox{.92\linewidth}{!}{$
%         \Sigma_{+,t}
%           \leq
%           \gamma\beta\underbrace{\Bigg(\prod_{j=t_{k-1}}^{t-1}\Big(1+\frac{\gamma\beta\lambda_+}{j+\alpha}\Big)\Bigg)}_{(i)}
%           \underbrace{\sum_{\ell=t_{k-1}}^{t-1}\frac{1}{\ell+\alpha+\gamma\beta\lambda_+}}_{(ii)}.
%         %   $} \hspace{-7mm}
% \end{equation}
% To bound $(ii)$, since $\frac{1}{\ell+\alpha+\gamma\beta\lambda_+}$ is decreasing in $\ell$, we have
% \begin{equation}
% % \resizebox{.8\linewidth}{!}{$
% \begin{aligned}
%     &\sum_{\ell=t_{k-1}}^{t-1}\frac{1}{\ell+\alpha+\gamma\beta\lambda_+}
%     \leq
%     \int_{t_{k-1}-1}^{t-1}\frac{1}{\ell+\alpha+\gamma\beta\lambda_+}\mathrm d\ell
%     \\&
%     \qquad = \ln\left(1+\frac{t-t_{k-1}}{t_{k-1}-1+\alpha+\gamma\beta\lambda_+}\right).
% \end{aligned}
% % $}
% \end{equation}
% \vspace{-0.05in}

% \noindent To bound $(i)$, we first rewrite it as $
%         \prod_{j=t_{k-1}}^{t-1}\big(1+\frac{\gamma\beta\lambda_+}{j+\alpha}\big)
%         =
%         e^{\sum_{j=t_{k-1}}^{t-1}\ln\big(1+\frac{\gamma\beta\lambda_+}{j+\alpha}\big)}
% $, and use the fact that $\ln(1+\frac{\gamma\beta\lambda_+}{j+\alpha})$ is a decreasing function with respect to $j$, and that $\alpha >1$, to get
% {\small\begin{equation} \label{eq:ln(i)}
% % \hspace{-3mm}\resizebox{.96\linewidth}{!}{$
%     \begin{aligned}
%         &\sum\limits_{j=t_{k-1}}^{t-1}\ln(1+\frac{\gamma\beta\lambda_+}{j+\alpha})
%         \leq
%         \int_{t_{k-1}-1}^{t-1}\ln(1+\frac{\gamma\beta\lambda_+}{j+\alpha})\mathrm dj
%         \nonumber \\&
%         \leq
%         \gamma\beta\lambda_+\int_{t_{k-1}-1}^{t-1}\frac{1}{j+\alpha}\mathrm dj
%         =
%         \gamma\beta \lambda_+\ln\left(1+\frac{t-t_{k-1}}{t_{k-1}-1+\alpha}\right),
%     \end{aligned}
%     % $}\hspace{-3mm}
% \end{equation}}

% \noindent which yields $
%         \prod_{j=t_{k-1}}^{t-1}\left(1+\frac{\gamma\beta\lambda_+}{j+\alpha}\right)
%         \leq
%         \left(1+\frac{t-t_{k-1}}{t_{k-1}-1+\alpha}\right)^{\gamma\beta \lambda_+}
% $.
    
    
%     Using the results obtained for bounding $(i)$ and $(ii)$ back in~\eqref{eq:Sigma_1T},
%     % we get:
%     % \begin{align} \label{eq:Sigma1_bound}
%     %     \Sigma_{+,t}
%     %     \leq
%     %     \tilde{\gamma}\ln\left(1+\frac{t-t_{k-1}}{t_{k-1}-1+\alpha+\tilde{\gamma}\lambda_+}\right)
%     %     \left(1+\frac{t-t_{k-1}}{t_{k-1}-1+\alpha}\right)^{\tilde{\gamma} \lambda_+}.
%     % \end{align}
%     and using the fact that $\ln(1+x) \leq 2\sqrt{x}$ for $x\geq 0$,
%     and performing some algebraic manipulations, we get
%         \begin{equation} \label{eq:Sigma_1stP}
%         % \hspace{-3mm} \resizebox{.99\linewidth}{!}{$ 
%         ({\Sigma}_{+,t})^2
%         \leq
%         4(\tau-1)\left(1+\frac{\tau}{\alpha-1}\right)^{2}
%         \left(1+\frac{\tau-1}{\alpha-1}\right)^{6\gamma\beta}
%         \eta_t^2\beta^2[t_{k-1}+\alpha].
%         % $}\hspace{-2mm}
%     \end{equation}
    
% %     \begin{align} \label{eq:Sig}
% %         &\Sigma_{+,t}
% %         \leq
% %         2\tilde{\gamma}\sqrt{\frac{t-t_{k-1}}{t_{k-1}-1+\alpha+\tilde{\gamma}\lambda_+}}
% %         \left(1+\frac{t-t_{k-1}}{t_{k-1}+\alpha-1}\right)^{\tilde{\gamma}\lambda_+}
% %         \nonumber \\&
% %         \leq
% %         2\tilde{\gamma}\sqrt{\frac{t-t_{k-1}}{t_{k-1}+\alpha+1}}
% %         \left(1+\frac{t-t_{k-1}}{t_{k-1}+\alpha-1}\right)^{3\tilde{\gamma}},
% %     \end{align}
% %     where in the last inequality we used
% %      $2\leq\lambda_+ < 3$ and $\tilde{\gamma}\geq \frac{\tilde{\mu}}{\beta}\tilde{\gamma} >1$.
% % Taking the square from the both hand sides of~\eqref{eq:Sig} followed by multiplying the both hand sides with $\frac{[t+\alpha]^2}{\tilde{\mu}\tilde{\gamma}[t_{k-1}+\alpha]}$ gives us:
%     % \begin{align} \label{eq:sig_sqaured}
%     %     &\hspace{-3mm}[{\Sigma}_{+,t}]^2\frac{[t+\alpha]^2}{\tilde{\mu}\tilde{\gamma}[t_{k-1}+\alpha]}
%     %     \leq
%     %     \frac{4\tilde{\gamma}}{\tilde{\mu}}\frac{[t-t_{k-1}][t+\alpha]^2}{[t_{k-1}+\alpha+1][t_{k-1}+\alpha]}
%     %     \left(1+\frac{t-t_{k-1}}{t_{k-1}+\alpha-1}\right)^{6\tilde{\gamma}}
%     %     \nonumber \\&\hspace{-3mm}
%     %     \leq
%     %     \frac{4\tilde{\gamma}}{\tilde{\mu}}(\tau-1)\left(1+\frac{1}{\tau+t_{k-1}+\alpha-2}\right)^2
%     %     \frac{[t_{k-1}+\alpha-1]^2}{[t_{k-1}+\alpha+1][t_{k-1}+\alpha]}
%     %     \left(1+\frac{\tau-1}{t_{k-1}+\alpha-1}\right)^{6\tilde{\gamma}+2}\hspace{-6mm}. \hspace{-3mm}
%     % \end{align}
%     % since $t\leq t_{k-1}+\tau_k-1\leq t_{k-1}+\tau-1$. 
    
%     % To bound~\eqref{eq:sig_sqaured}, we use the facts that
%     % \begin{align}
%     %     1+\frac{1}{\tau+t_{k-1}+\alpha-2}
%     %     \leq
%     %     1+\frac{1}{\tau+\alpha-2},\ 1+\frac{\tau-1}{t_{k-1}+\alpha-1}\leq 1+\frac{\tau-1}{\alpha-1},
%     % \end{align}
%     % and
%     % \begin{align}
%     %     \frac{[t_{k-1}+\alpha-1]^2}{[t_{k-1}+\alpha+1][t_{k-1}+\alpha]}\leq 1,
%     % \end{align}
%     % which yield
%     % \begin{align}
%     %     [{\Sigma}_{+,t}]^2\frac{[t+\alpha]^2}{\tilde{\mu}\tilde{\gamma}[t_{k-1}+\alpha]}
%     %     \leq
%     %     \frac{4\tilde{\gamma}}{\tilde{\mu}}(\tau-1)\left(1+\frac{\tau}{\alpha-1}\right)^{2}
%     %     \left(1+\frac{\tau-1}{\alpha-1}\right)^{6\tilde{\gamma}}.
%     % \end{align}
%     % Consequently, we have
    
    
    

    
% On the other hand, we bound $(b)$ in~\eqref{eq:thm1_SigT} as follows:
% \begin{equation*}
% % \hspace{-0.1mm}\resizebox{.98\linewidth}{!}{$
% \begin{aligned}
%     (t+\alpha) (\Sigma_{+,t})^2
%     &\leq
%     4\gamma^2\beta^2\frac{(t-t_{k-1})(t+\alpha)}{t_{k-1}+\alpha+1}
%     \left(1+\frac{t-t_{k-1}}{t_{k-1}+\alpha-1}\right)^{6\gamma\beta}
%     \\
%     &\leq
%     4\gamma^2\beta^2(\tau-1)\left(1+\frac{\tau-2}{\alpha+1}\right)
%     \left(1+\frac{\tau-1}{\alpha-1}\right)^{6\gamma\beta},
% \end{aligned}
% % $}
% \end{equation*}
% which implies
% \begin{equation} \label{eq:Sigma_2ndP}
% % \hspace{-5mm}\resizebox{.92\linewidth}{!}{$
%     (\Sigma_{+,t})^2
%     \leq
%     4\gamma\beta(\tau-1)\left(1+\frac{\tau-2}{\alpha+1}\right)
%     \left(1+\frac{\tau-1}{\alpha-1}\right)^{6\gamma\beta}\eta_t\beta.
%     % $}\hspace{-3mm}
% \end{equation}
% Substituting~\eqref{eq:Sigma_1stP} and~\eqref{eq:Sigma_2ndP} into~\eqref{eq:thm1_SigT}, we get
% \begin{equation} \label{eq:induce_formT}
% \begin{aligned}
%     & \mathbb E[F(\hat{\mathbf w}^{(t+1)})-F(\mathbf w^*)]
%     % \nonumber \\&
%     \leq \\& \qquad
%     \left(1-\mu\eta_t+Z_1\omega^2\eta_t^2\beta^2 \right)\frac{\nu}{t+\alpha}
%     +\eta_t^2\beta^2 Z_2,
% \end{aligned}
%  \hspace{-1.3mm}
% \end{equation}
% where $Z_1$ and $Z_2$ are given in the statement of the theorem.

% To complete the induction, we need to show that the right hand side of~\eqref{eq:induce_formT} is less than or equal to $\frac{\nu}{t+1+\alpha}$. This condition 
% % \begin{align}\label{eq:lastBeforeInd}
% %     \mathbb E[F(\hat{\mathbf w}^{(t+1)})-F(\mathbf w^*)]
% %     \leq
% %     \left(1-\tilde{\mu}\tilde{\eta}_t+Z_1\omega^2\tilde{\eta}_t^2 \right)\frac{\nu}{t+\alpha}
% %     +\tilde{\eta}_t^2Z_2
% %     \leq
% %     \frac{\nu}{t+1+\alpha},
% % \end{align}
%  can be represented equivalently as the following inequality:
% % We transform the condition in~\eqref{eq:lastBeforeInd} through the set of following algebraic steps to an inequality condition on a convex function:
% % \begin{align}\label{eq:longtrasform}
% %     &
% %      \left(-\frac{\tilde{\mu}}{\tilde{\eta}_t^2}+\frac{Z_1\omega^2}{\tilde{\eta}_t} \right)\frac{\nu}{t+\alpha}
% %     +\frac{Z_2}{\tilde{\eta_t}}
% %     +\frac{\nu}{t+\alpha}\frac{1}{\tilde{\eta_t}^3}
% %     \leq\frac{\nu}{t+1+\alpha}\frac{1}{\tilde{\eta_t}^3}
% %     \nonumber \\&
% %     \Rightarrow
% %      \left(-\frac{\tilde{\mu}}{\tilde{\eta}_t}+Z_1\omega^2 \right)\frac{\nu}{t+\alpha}\frac{1}{\tilde{\eta}_t}
% %     +\frac{Z_2}{\tilde{\eta_t}}
% %     +\left(\frac{\nu}{t+\alpha}-\frac{\nu}{t+1+\alpha}\right)\frac{1}{\tilde{\eta_t}^3}
% %     \leq0
% %     \nonumber \\&
% %     \Rightarrow
% %      \left(-\frac{\tilde{\mu}}{\tilde{\eta}_t}+Z_1\omega^2 \right)\frac{\nu}{\tilde{\gamma}}
% %     +\frac{Z_2}{\tilde{\eta_t}}
% %     +\left(\frac{\nu}{t+\alpha}-\frac{\nu}{t+1+\alpha}\right)\frac{(t+\alpha)^3}{\tilde{\gamma}^3}
% %     \leq0
% %     \nonumber \\&
% %     \Rightarrow
% %      \left(-\frac{\tilde{\mu}}{\tilde{\eta}_t}+Z_1\omega^2 \right)\frac{\nu}{\tilde{\gamma}}
% %     +\frac{Z_2}{\tilde{\eta_t}}
% %     +\frac{\nu}{(t+\alpha)(t+\alpha+1)}\frac{(t+\alpha)^3}{\tilde{\gamma}^3}
% %     \leq0
% %     \nonumber \\&
% %     \Rightarrow
% %      \left(-\frac{\tilde{\mu}}{\tilde{\eta}_t}+Z_1\omega^2 \right)\frac{\nu}{\tilde{\gamma}}
% %     +\frac{Z_2}{\tilde{\eta_t}}
% %     +\frac{\nu}{t+\alpha+1}\frac{(t+\alpha)^2}{\tilde{\gamma}^3}
% %     \leq0
% %     \nonumber \\&
% %     \Rightarrow
% %       \tilde{\gamma}^2\left(-\frac{\tilde{\mu}}{\tilde{\eta}_t}+Z_1\omega^2 \right)\nu
% %     +\frac{Z_2}{\tilde{\eta_t}} \tilde{\gamma}^3
% %     +\frac{(t+\alpha)^2}{t+\alpha+1}\nu
% %     \leq0
% %     \nonumber \\&
% %     \Rightarrow
% %       \tilde{\gamma}^2\left(-\frac{\tilde{\mu}}{\tilde{\eta}_t}+Z_1\omega^2 \right)\nu
% %     +\frac{Z_2}{\tilde{\eta_t}} \tilde{\gamma}^3
% %     +\left(\frac{(t+\alpha+1)(t+\alpha-1)}{t+\alpha+1}\nu+\frac{\nu}{t+\alpha+1}\right)
% %     \leq 0,
% % \end{align}
% % where the last condition in~\eqref{eq:longtrasform} can be written as:
% % \begin{align}\label{eq:finTransform}
% %     \tilde{\gamma}^2\left(-\frac{\tilde{\mu}}{\tilde{\eta}_t}+Z_1\omega^2\right)\nu
% %     +\frac{Z_2}{\tilde{\eta}_t}\tilde{\gamma}^3
% %     +\nu[t+\alpha-1]
% %     +\frac{\nu}{t+1+\alpha}
% %     \leq 0.
% % \end{align}
%  \begin{align}\label{eq:fin2P}
%     &\gamma^2\beta^2\left(-\frac{\mu}{\gamma\beta^2}(t+\alpha)+Z_1\omega^2 \right)\nu
%     +Z_2 \gamma^2\beta^2(t+\alpha)
%     \nonumber \\&
%     \qquad +\nu(t+\alpha-1)
%     +\frac{\nu}{t+1+\alpha}
%     \leq 0.
%  \end{align}
% \eqref{eq:fin2P} needs to be satisfied $\forall t\geq 0$. Since the expression on the left hand side is convex in $t$,
%  it is sufficient to satisfy this condition for $t\to\infty $ and $t=0$. Obtaining these limits gives us the following set of conditions: $\mu\gamma-1>0$, $\omega<\frac{1}{\gamma\beta}\sqrt{\alpha\frac{\mu\gamma-1+\frac{1}{1+\alpha}}{Z_1}}$ and $\nu\geq Z_2\max\{\frac{\beta^2\gamma^2}{\mu\gamma-1},\frac{\alpha}{Z_1\left(\omega_{\max}^2-\omega^2\right)}\}$, which completes the induction and thus the proof."
% \end{skproof}
% \\

% \noindent [R1] F. P.-C. Lin, S. Hosseinalipour, S. S. Azam, C. G. Brinton, and N. Michelusi, ``Two timescale hybrid federated learning with cooperative D2D local model aggregations,”arXiv preprint arXiv:2103.10481, 2021.
% \\

% In the appendix of the full version of the paper:
% ``{\color{red} TO DO!}"
% \\

% \noindent\textbf{R2.6:}  \textit{In Section V, the authors ignored to analyze the reason for the simulation results.}
% \\

% \noindent Thanks for this comment. We now added more explanations and justification to Section V (Page 16). This includes the intuition on why TT-HF outperforms the state of the art methods in convergence rate, and why it leads to network resource savings. We quote the added/revised text below for the ease of review:

% {\color{blue}
% ``Data heterogeneity in local dataset across local devices can result in considerable performance degradation  of federated learning algorithms. In this case, longer local update periods will result in models that are significantly
% biased towards local datasets and degrade the convergence speed of the global model and the resulting model accuracy. By blending federated aggregations with cooperative D2D consensus procedure among local device clusters in {\tt TT-HF}, we effectively decrease the bias of the local models to the local datasets and speed up the convergence at a lower cost (i.e., utilizing low power D2D communications to reduce the frequency of performing global aggregation via uplink transmissions). Due to the low network cost in performing D2D transmission, {\tt TT-HF} provides a practical solution for federated learning to achieve faster convergence or to prolong the local model training interval, leading to delay and energy consumption savings."}
% \\

% -- \textbf{Negative comments:} 
% \begin{enumerate}
%     \item \emph{The derivation of theorems is difficult to follow such as Theorem 2.}
%     \begin{itemize}
%         \item We believe this comment has been addressed in our response to your comment \textbf{R2.5}.
%     \end{itemize}
%     \item \emph{The influence of device clustering is ignored in the convergence analysis.}
%     \begin{itemize}
%         \item  We believe this comment has been addressed in our response to your comment \textbf{R2.4}.
%         \\
%     \end{itemize}
% \end{enumerate}


% \newpage
% \noindent[R1] S. S. Azam, S. Hosseinalipour, Q. Qiu and C. Brinton
% ``Recycling Model Updates in Federated Learning: Are Gradient Subspaces Low-Rank?," in Proc. Conf. Learn. Repres. (ICLR), 2022.
% \\


% % \noindent [R3] L. Liu, J. Zhang, S. Song, and K. B. Letaief, “Client-edge-cloud hierarchical federated learning,” in Proc. IEEE Int. Conf. Commun. (ICC), 2020, pp. 1–6.
% % \\

% \noindent [R2] K. Wei et al.,“Federated Learning With Differential Privacy: Algorithms and Performance Analysis," IEEE Trans. Inf. Forensics Security, vol. 15, pp. 3454-3469, 2020.
% \\

% \noindent [R3] J. Wang, A. K. Sahu, Z. Yang, G. Joshi and S. Kar, "MATCHA: Speeding Up Decentralized SGD via Matching Decomposition Sampling," Indian Control Conference (ICC), 2019, pp. 299-300.
% \\

% \noindent [R4] F. P.-C. Lin, C. G. Brinton, and N. Michelusi, “Federated  learningwith communication delay in edge networks,” in Proc. IEEE Int. Glob.Commun. Conf. (GLOBECOM), 2020, pp. 1–6.
% \\



% \nm{what about responding to the negatives? These are also comments from the Rveiwer that need to be addressed:
% 1. The derivation of theorems is difficult to follow such as Theorem 2.
% 2. The influence of device clustering is ignored in the convergence analysis.
% At least, you need to provide a concise response and mention in which response it has been addressed.
% For instance, you could simply say: "we believe this comment has been addressed in our response to your comment R2.X."
% }


\newpage

\section*{Response to Reviewer 3}

\noindent\textbf{Reviewer's General Note:} \textit{In this paper, the authors propose a novel delay-aware federated learning (DFL) technique to enhance the efficiency of hierarchical federated learning by addressing communication delays between edge and cloud. DFL's main learning processes involve local multi-round training of edge devices, local aggregation of edge servers, and global aggregation of cloud servers. Given the delay between edge and cloud, edge devices continue to keep training and models updated, then global model updates are obtained from a linear local-global model combiner. Based on the convergence behavior of DFL, an adaptive control algorithm is designed that implements policies to mitigate energy consumption and edge-to-cloud communication latency while achieving a sub-linear convergence rate. Sufficient numerical experiments on real datasets demonstrate that DFL offers superior performance in terms of faster global model convergence, reduced resource consumption, and robustness against communication delays when compared to existing federated learning algorithms. The technical derivations are solid, and the reviewer is positive towards this paper, nevertheless, several comments need to be addressed.}
\\
% \noindent\textbf{Positives:} \textit{The paper is well written and organised. Interesting results have been obtained in this research, also, the paper has strong readability and solid analysis of the proposed method.}


\noindent \textbf{To Reviewer 3:} We would like to thank you for reviewing our manuscript and providing us with thoughtful comments. We also appreciate the positive feedback. In the following, please find point-by-point responses to your comments, as well as pointers to where they have been addressed in the revised manuscript.
\vspace{4mm}
\hrule
\vspace{4mm}

\noindent\textbf{R3.1:} \textit{In the subsection of DFL Model Training Procedure, under the given time index t, whether the time of each local aggregation is the same, it corresponds to $m_1$ and $m_2$ in Figure 2. If so, we recommend changing $m_2$ to $2 m_1$. Are the intervals equal when the time duration is partitioned into multiple local model training intervals?}
\\

%  in that figure, it is not necceary equal

\noindent
We would like to clarify that our framework is designed to enable aperiodic local aggregations within each local model training interval. This means that the intervals for each local aggregation, corresponding to $m_1$ and $m_2$ in Figure 2, do not necessarily need to be equal.

In fact, the flexibility of performing local aggregation at non-uniform intervals is a key feature of our DFL control algorithm. This enables DFL to adaptively tune when these aggregations should be performed, which further assists in minimizing resource consumption.

% We appreciate your suggestion for changing $m_2$ to $2 m_1$. However, in our design, we do not necessarily constrain $m_2$ to be twice $m_1$, but instead, we aim for the flexibility to adapt the local aggregation time intervals according to the network conditions.

% To improve clarity on this point, we have revised the descriptions in Section II to further emphasize the design of aperiodic local and global aggregations.
%We break down our response to the following three points:

\noindent This is now clarified by adding a footnote in Sec.~\ref{sec:tthf} (Page \pageref{fig:twoTimeScale}). We quote the added sentence below for the ease of review:\\

\textit{``{\color{blue} Note that in fig.~\ref{fig:twoTimeScale}, the parameters $m_1$ and $m_2$ may not be equal. In fact, {\tt DFL} allows for aperiodic local aggregations during each local model training interval, the frequency of which will be later tuned in our control algorithms to minimize network resource consumption.}"}\\



% \nm{this point is out of scope! IT does not respond the reviewer comment!}
% \noindent \textit{(2) Popularity of the assumption and equivalency between ``convexity" and ``strong convexity" in ML loss function:} It is worth mentioning that \textit{convexity} or \textit{strong convexity} of the loss function are  common assumptions in the federated learning literature in the area of wireless communication and networking (See [R1]-[R9] below which are among the popular articles in this area
% % {\color{red} please revise these references and pick from the highly cited papers like "Federated Learning over Wireless Networks: Optimization Model Design and Analysis" should be there ...}
% ).  Note that in practice, when training a machine learning model, the loss function is implemented with L2 regularization to prevent model over-fitting. Thus, having either convex or strongly convex assumption on the loss function does not differ since a convex function implemented with L2 regularization will become strongly convex. 

% \nm{again, if the reference is in the main manuscript, just refer to that using cite.. otherwise, add the list of missing references at the end of Rev3 section.}
% \noindent [R1] S. Wang, T. Tuor, T. Salonidis, K. Leung, C. Makaya, T. He and K. Chan ``Adaptive federated learning in resource constrained edge computing systems," IEEE J. Sel. Areas Commun., vol. 37, no. 6, pp. 1205-1221, 2019.
% \\

% \noindent [R2] M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor and S. Cui, ``A Joint Learning and Communications Framework for Federated Learning Over Wireless Networks," IEEE Trans. Wireless Commun., vol. 20, no. 1, pp. 269-283, Jan. 2021.
% \\

% \noindent [R3] N. H. Tran, W. Bao, A. Zomaya, M. N. H. Nguyen and C. S. Hong, ``Federated Learning over Wireless Networks: Optimization Model Design and Analysis," in Proc. IEEE Conf. Comput. Commun. (INFOCOM), 2019, pp. 1387-1395.
% \\

% \noindent [R4] H. H. Yang, Z. Liu, T. Q. S. Quek and H. V. Poor, ``Scheduling Policies for Federated Learning in Wireless Networks," IEEE Trans. Commun., vol. 68, no. 1, pp. 317-333, 2020.
% \\

% \noindent [R5] C. T. Dinh et al., ``Federated Learning Over Wireless Networks: Convergence Analysis and Resource Allocation," in IEEE/ACM Trans. Networking, vol. 29, no. 1, pp. 398-409, 2021.
% \\

% \noindent [R6] Z. Yang, M. Chen, W. Saad, C. S. Hong and M. Shikh-Bahaei, ``Energy Efficient Federated Learning Over Wireless Communication Networks," IEEE Trans. Wireless Commun., vol. 20, no. 3, pp. 1935-1949, 2021.
% \\

% \noindent [R7] M. Chen, H. V. Poor, W. Saad and S. Cui, ``Convergence Time Optimization for Federated Learning Over Wireless Networks," IEEE Trans. Wireless Commun., vol. 20, no. 4, pp. 2457-2471, 2021.
% \\

% \noindent [R8] M. M. Amiri, D. Gündüz, S. R. Kulkarni and H. V. Poor, ``Convergence of Update Aware Device Scheduling for Federated Learning at the Wireless Edge," IEEE Trans. Wireless Commun., vol. 20, no. 6, pp. 3643-3658, 2021.
% \\

% \noindent [R9] L. Liu, J. Zhang, S. Song, and K. B. Letaief, “Client-edge-cloud hierarchical federated learning,” in Proc. IEEE Int. Conf. Commun. (ICC), 2020, pp. 1–6.
% \\

% \noindent [R10] F. P.-C. Lin, S. Hosseinalipour, S. S. Azam, C. G. Brinton, and N. Michelusi, ``Two timescale hybrid federated learning with cooperative D2D local model aggregations,”arXiv preprint arXiv:2103.10481, 2021.
% \\

\noindent\textbf{R3.2:}   \textit{Algorithms 1 and 2 correspond to DFL with set control parameters and DFL with adaptive control parameters, respectively. Both algorithms outperform hierarchical FedAvg in terms of performance. However, the advantages of DFL with adaptive control parameters compared to DFL with a set control parameter could be highlighted. Adaptive control parameter algorithms typically result in better performance (accuracy, energy or delay) because they can adjust to changes in the system over time. If both DFL schemes have the same performance, then adaptive DFL is meaningless.}
\\

% choose some random alpha to compare with adaptive
\noindent We appreciate your point on highlighting the benefits of our DFL with adaptive control parameters over DFL with set control parameters.
In our approach, DFL with set control parameters (Algorithm 1) primarily focuses on enhancing the model performance without explicitly mitigating resource consumption. In contrast, DFL with adaptive parameters control (Algorithm 2) is designed to address both model performance and resource consumption simultaneously. The adaptive control approach enables the algorithm to dynamically adjust the value of $\alpha$ and length of local model training interval $\tau_k$ to enhance performance metrics such as accuracy, energy, and delay.

In response to your suggestion for clearer distinction, we have incorporated an additional experiment in our study. This new test contrasts Algorithm 1 and Algorithm 2. The comparison underlines the resource consumption improvements achievable when employing DFL with adaptive control. In the configuration of Algorithm 1, we have set $\alpha=0.5$, a local model training interval of $\tau=20$, local aggregations performed after every $5$ local model updates by each edge device, and a round-trip delay of $\Delta=10$. In contrast, Algorithm 2 dynamically adjusts these parameters as needed.

According to our results, Algorithm 2 showcased significant improvements in energy efficiency and latency reduction compared to Algorithm 1. More specifically, Algorithm 2 reduced energy consumption by $67.5\%$ and $66.1\%$ for CNN and SVM models, respectively. Also, as shown in Fig.~\ref{fig:res_r}, Algorithm 2 decreased the delay by $36.6\%$ and $35.1\%$ compared to Algorithm 1 for the CNN and SVM models, respectively.

\noindent This is now highlighted in Sec.~\ref{ssec:ctrl_DFL} (Page~\pageref{ssec:ctrl_DFL}). We quote the added sentence below for ease of review:\\

\setcounter{figure}{6}
\begin{figure}[h]
\includegraphics[width=1.\columnwidth]{control_alg/resource3.png}
\centering
\caption{Compared to the baselines, {\tt DFL} with adaptive parameter control (Algorithm~\ref{GT}) achieves significantly better results in terms of total energy and delay metrics for both CNN and SVM, as shown in (a) and (b) when reaching 80\% testing accuracy.}
\label{fig:res_r}
\vspace{-6mm}
\end{figure}

\textit{``{\color{blue}The results presented in Fig.~\ref{fig:res} compare the performance of {\tt DFL} with adaptive parameter control (Algorithm~\ref{GT}) with four baseline approaches: (i) FL with full device participation and $\tau=1$; (ii) hierarchical FL with $\alpha=0$, $\tau=20$, where local aggregations are performed after every $5$ local model updates; (iii) {\tt DFL} with fixed parameters (Algorithm~\ref{DFL}) with $\alpha=0.5$, $\tau=20$, where local aggregations are performed after every $5$ local model updates; and (iv) {\tt HFL} with parameter control, constructed by setting $\alpha=0$ in the DFL control algorithm, thereby reducing the
DFL’s global aggregation to the standard global aggregation method in~\cite{Feng2022HFL,Lim2021HFL,Xu2022HFL,Luo2020HFEL,wang2021HFL,Mhaisen2022HFL}. Two metrics are used to compare the performance: (M$_1$) total energy consumption and (M$_2$) total delay, each measured upon reaching $80\%$ testing accuracy. For (M$_1$), the results indicated by the blue bars of Fig.~\ref{fig:res}(a) \& (b) show that {\tt DFL} with adaptive parameter control significantly outperforms baseline approaches. Specifically, {\tt DFL} requires $91.8\%$ and $90.5\%$ less energy than baseline (i), $71.4\%$ and $70.4\%$ less energy than baseline (ii), $67.5\%$ and $66.1\%$ less energy than baseline (iii), and $19.9\%$ and $21.1\%$ less energy than baseline (iv) for CNN and SVM models, respectively. Similarly, for (M$_2$), {\tt DFL} requires substantially less communication delay than both baseline approaches as shown in the red bars of Fig.~\ref{fig:res}(a) \& (b). Specifically, {\tt DFL} requires $94.2\%$ and $95.1\%$ less delay than baseline (i), $44.1\%$ and $43.2\%$ less delay than baseline (ii), $36.6\%$ and $35.1\%$ less delay than baseline (iii), and $25.8\%$ and $24.5\%$ less delay than baseline (iv) for CNN and SVM models, respectively. These results demonstrate the improvements in resource-efficiency provided by {\tt DFL} with adaptive parameter control, attributed to its parameter tuning approach that concurrently considers the tradeoff between the optimality gap, as derived in Theorem~\ref{thm:subLin_m}, the communication delay, and the energy consumption. The improvement of $20-25\%$ over baseline (iv) in both metrics in particular highlights the benefit provided by our local-global model combiner strategy.}"}\\


\noindent\textbf{R3.3:}   \textit{There might be an error in the legend of Figure 4 (b). Specifically, the legend currently reads "$\Delta=0$, $\alpha=0.5$ (DFL)," but we suggest that it should be updated to read "$\Delta=0$, $\alpha=0$ (DFL)." This would reflect the fact that, if the delay is $0$, the edge devices do not train and global model updates do not need to be combined linearly.}
% the framework allows DFL to continue perform local update during the delay period, DFL when alpha 0 is a specical case of DFL with resemble standard fedavg, when delay=0, , we show when delay 0 and alpha 0, as delay increases the sel of alpha becomes important, signifies the importance of alpha
\\

\noindent We apologize for the confusion. In the DFL context, $\alpha=0$ simulates the conventional hierarchical FedAvg. Our findings illustrate that the most optimal approach for DFL, when no delay exists ($\Delta=0$), is to establish $\alpha=0$. This validates that the conventional global aggregation procedure is indeed the most effective in the absence of delay. As the delay increases, the assignment for $\alpha$ becomes essential and is shaped by numerous variables.

To stress the importance of precise tuning of the combiner weight ($\alpha$), we executed a simulation under conditions of $\Delta=0$ and $\alpha=0.5$. As expected, this underperformed compared to the $\Delta=0, \alpha=0$ setup, which is analogous to the standard hierarchical FedAvg.

Moreover, in a scenario with zero delay, edge devices would maintain local updates until they transmit their local models to the central server for global aggregation/synchronization. Given the instant global aggregation/synchronization in this case, edge devices would not perform additional local updates after transmitting their local models to the main server.\\

This is clarified in the main text in Sec.~\ref{subsubsec:combiner} on page~\pageref{fig:mnist_poc_2_all} which we quote below for the ease of review:\\
% , Sec.~\ref{subsec:proc} on page~\pageref{subsec:proc}, and Sec.~\ref{subsec:form2} on page~\pageref{eq:the}  

\textit{``{\color{blue}In fact, in {\tt DFL}, $\alpha=0$ simulates the conventional hierarchical {\tt FedAvg}: our results indicate that, when no delay exists ($\Delta=0$), the best approach in {\tt DFL} is to emulate hierarchical {\tt FedAvg}. This validates that the conventional global aggregation procedure is indeed the most effective in the absence of delay. As the delay increases, the choice of $\alpha$ becomes essential and is shaped by several system variables.}"}\\

% {\color{blue} At each instance of global aggregation, the models of the devices are collected (i.e., they engage in uplink transmissions) and arrive at the server with a certain delay. The server will then aggregate the received models into a global model and broadcast it (i.e., engage in downlink transmission) to the devices, which involves another delay. In parallel, devices proceed with local SGD/aggregation updates before the global model arrives. Upon reception of the global model, devices synchronize their local models in a unique manner accounting for both uplink and downlink delays, as depicted in Fig.~\ref{fig:twoTimeScale}.}\\


% {\color{blue} In conducting global aggregations, to compensate for the round-trip delay, the devices will transmit their local models to the edge servers $\Delta_k$ time instances before the end of each local model training interval $\mathcal{T}_k$, i.e., at $t=t_{k+1}-\Delta_k \in \mathcal{T}_k$. In the meantime, the devices continue to perform $\Delta_k$ more local updates via SGD/local aggregation before receiving the global model.}\\

% , we quote the text in the appendix below for the ease of review:


% \nm{please remove quotes from tech resport! Just provide the discussion and figures as a response to the reviewr}{\color{blue}``
% \subsection{Extension to Other Federated Learning Methods}
% Although we develop our algorithm based on federated learning with vanilla SGD local optimizer, our method can benefit other counterparts in literature. In particular, we perform some numerical experiments on FedProx~\cite{sahu2018convergence} to demonstrate the superiority of our semi-decentralized architecture. The performance improvement is due to the fact that, intuitively, conducting D2D communications via the method proposed by us reduces the local bias of the nodes' models to their local datasets. This benefits the convergence of federated learning methods via counteracting the effect of data heterogeneity across the nodes. The simulation results are provided in Fig.~\ref{fig:prox1_app} and~\ref{fig:prox2_app}}."

% \setcounter{figure}{0}
% \begin{figure}[H]
% % \includegraphics[width=0.5\textwidth]{mnist_125/FedProx.png}
% \centering
% \caption{Performance comparison between {\tt TT-HF} and FedProx~\cite{sahu2018convergence} when varying the number of D2D consensus rounds ($\Gamma$). Under the same period of local model training ($\tau$), increasing $\Gamma$ results in a considerable improvement in the model accuracy/loss over time as compared to the baseline when data is non-i.i.d. (MNIST, Neural Network)}
% \label{fig:prox1}
% \vspace{-5mm}
% \end{figure}

% \begin{figure}[H]
% % \includegraphics[width=0.5\textwidth]{mnist_125/FedProx2.png}
% \centering
% \caption{Performance comparison between {\tt TT-HF} and FedProx~\cite{sahu2018convergence} when varying the local model training interval ($\tau$) and the number of D2D consensus rounds ($\Gamma$). With a larger $\tau$, {\tt TT-HF} can still outperform the baseline method if $\Gamma$ is increased, i.e., local D2D communications can be used to offset the frequency of global aggregations. (MNIST, Neural Network)}
% \label{fig:prox2}
% \end{figure}


% \noindent [R1] S. Wang, T. Tuor, T. Salonidis, K. Leung, C. Makaya, T. He and K. Chan ``Adaptive federated learning in resource constrained edge computing systems," IEEE J. Sel. Areas Commun., vol. 37, no. 6, pp. 1205-1221, 2019.

% \noindent [R1] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-Efficient Learning of Deep Networks from Decentralized Data,” in Proc. Int. Conf. Artificial Intell. Stat. (AISTATS), 2017.
% \\

% \noindent [R2] K. Yang, T. Jiang, Y. Shi and Z. Ding, ``Federated Learning via Over-the-Air Computation," IEEE Trans. Wireless Commun., vol. 19, no. 3, pp. 2022-2035, 2020.
% \\
 
% \noindent [R3] F. Sattler, S. Wiedemann, K. -R. Müller and W. Samek, ``Robust and Communication-Efficient Federated Learning From Non-i.i.d. Data," IEEE Trans. Neural Netw. Learn. Syst., vol. 31, no. 9, pp. 3400-3413, 2020.
% \\


\noindent\textbf{R3.4:}   \textit{The authors demonstrate that DFL provides better resource efficiency compared to other federated learning algorithms, particularly in terms of energy and delay. However, to make these results more reasonable and understandable, it would be beneficial for the authors to provide detailed statements or explanations to analyze why the DFL algorithm can achieve these advantages.}  
\\

\noindent A significant aspect of our methodology is the novel integration of a local-global combiner during the global synchronization stage within a hierarchical federated learning framework. This approach introduces a new variable to account for the existing delay between the device and the cloud server. The combiner weight effectively blends the outdated global model with the most recent local models at the point of global synchronization.

The combiner weight, represented as $\alpha$, empowers the model to lean more heavily on the global model during the local-global model fusion process. Such emphasis is essential to counteract the potential bias that may emerge in local models due to data heterogeneity. In these situations, the local model might deviate from the global optimum, especially when the local dataset does not accurately reflect the overall dataset distributed throughout the network. This concept aligns with the rationale outlined in Theorem~\ref{thm:subLin_m}.

As the round-trip delay expands, our DFL approach tends to prioritize the local model at the time of synchronizing the local model with the global model. This is because as the delay escalates, the global model becomes increasingly outdated. This intricate method diverges from conventional federated learning designs, which persistently substitute all local models with the global model, disregarding any delay considerations.

Additionally, our method strategically organizes hierarchical federated learning with aperiodic local aggregations and global aggregation periods. This design affords the DFL control algorithm the adaptability to determine the optimal times to execute these operations, thereby enabling the framework to further decrease resource consumption.

We hope that this elaboration serves to illuminate how the distinctive facets of our proposed method can contribute to enhanced performance. This is now elaborated upon in the main text in Sec.~\ref{ssec:conv-eval}, which we have quoted below for your convenience:\\

\textit{``{\color{blue}This highlights the benefits of a hierarchical model training structure within an edge-to-cloud network, where frequent local aggregations prevent non-i.i.d. data-driven deviation of local models from the optimum within each subnet.}"}\\

This is now elaborated upon in the main text in Sec.~\ref{subsubsec:combiner}, which we have quoted below for your convenience:\\

\textit{``{\color{blue}The performance obtained by {\tt DFL} originates from the introduction of the linear local-global combiner during global synchronization. This approach enables the synchronization process to simultaneously consider the outdated yet more generalized global model and the up-to-date yet potentially overfitted local model. Particularly in circumstances where delays are substantial, the system will benefit from preserving a portion of the local model instead of fully synchronizing it with an outdated global model. This approach ensures that the most timely insights derived from the local models are maintained.}"}\\

This point is now detailed further in the main body of the text within Sec.~\ref{ssec:ctrl_DFL}. The revised portion has been quoted below for easy reference:\\

\textit{``{\color{blue}These results demonstrate the improvements in resource-efficiency provided by {\tt DFL} with adaptive parameter control, attributed to its parameter tuning approach that concurrently considers the tradeoff between the optimality gap, as derived in Theorem~\ref{thm:subLin_m}, the communication delay, and the energy consumption. The improvement of $20-25\%$ over baseline (iv) in both metrics in particular highlights the benefit provided by our local-global model combiner strategy.}"}
\fi
% -- \textbf{Main negatives:}
% \begin{enumerate}
%     \item \emph{Main negatives I found are about the insufficiency of comparative studies and unsatisfactory performance in the classification. Please see more detailed comments above.}
%     \begin{itemize}
%         \item We believe this comment has been addressed in our response to your comments \textbf{R3.3} and \textbf{R3.4}.
%         \\
%     \end{itemize}
% \end{enumerate}


% \nm{what about the comment "Main negatives I found are about the insufficiency of comparative studies and unsatisfactory performance in the classification. Please see more detailed comments above."? You need to address this as well. As I said before, smt like "We believe we have addressed this comment in our response to R3.X" is eough}
% \nm{where do you refer R1? I only see R2-R5...}

% \newpage
% \noindent [R1] He, Kaiming, et al. ``Delving deep into rectifiers: Surpassing human-level performance on imagenet classification," Proceedings of the IEEE international conference on computer vision. 2015.
% \\

% \noindent [R2] Y. Chen, X. Sun and Y. Jin, ``Communication-Efficient Federated Deep Learning With Layerwise Asynchronous Model Update and Temporally Weighted Aggregation," IEEE Trans. Neural Netw. Learn. Syst., vol. 31, no. 10, pp. 4229-4238, Oct. 2020.
% \\

% \noindent [R3] D. Ye, R. Yu, M. Pan and Z. Han, ``Federated Learning in Vehicular Edge Computing: A Selective Model Aggregation Approach," IEEE Access, vol. 8, pp. 23920-23935, 2020.
% \\

% \noindent [R4] W. Liu, L. Chen, Y. Chen and W. Zhang, ``Accelerating Federated Learning via Momentum Gradient Descent," IEEE Trans. Parallel Distrib. Syst., vol. 31, no. 8, pp. 1754-1766, 1 Aug. 2020.
% \\

% \noindent [R5] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar and V. Smith ``Federated Optimization in Heterogeneous Networks,”arXiv preprint arXiv:1812.06127, 2020.
% \\

\iffalse{
\newpage

\section*{Response to Reviewer 4}

\noindent\textbf{Reviewer's General Note:} \textit{This paper proposed a two-timescale hybrid federated learning framework with cooperative local D2D model aggregation. The convergence results are established, followed by the network resource optimization to improve the learning performance. The proposed models, theoretical results, and experiments are well defined with solid results. The main concern is the novelty compared with the authors' previous work (“Multi-Stage Hybrid Federated Learning over Large-Scale D2D-Enabled Fog Networks” https://arxiv.org/abs/2007.09511).}
\\

\noindent \textbf{To Reviewer 4:} We appreciate your time and effort spent on reviewing our paper. We also thank you for the positive feedback. In the following, please find point-by-point responses to your comments and also your main concern mentioned above, as well as pointers to where they have been addressed in the revised manuscript.
\vspace{4mm}
\hrule
\vspace{4mm}

\noindent\textbf{R4.1:} \textit{The paper has 15 pages with double columns. If this exceed the maximum pages based on the policy in JSAC?}
\\

\noindent We thank you for pointing out this possible issue. Recently, JSAC has changed its submission style from one column to double column with no initial page limit. We have also verified with one of the guest editors about this policy modification and verified that the length of the paper after applying the revisions is authorized. 
\\

\noindent\textbf{R4.2:}   \textit{Please compare the results and technical tools/results in the paper from the same authors’ group (“Multi-Stage Hybrid Federated Learning over Large-Scale D2D-Enabled Fog Networks” https://arxiv.org/abs/2007.09511), as they look quite similar.}
\\

\noindent 
As mentioned by the reviewer, the paper “Multi-Stage Hybrid Federated Learning over Large-Scale D2D-Enabled Fog Networks” is also written by us. That work also
 studies cluster-based consensus procedure between global aggregations. However,  it is different from this manuscript in the following aspects:\\
 
 \textit{(1) System Model and Local Iterations:} In this manuscript, we consider that in between each global aggregations, the devices carry out multiple rounds of local SGD iterations. However, in our prior work, only one round of gradient descent is assumed. This completely changes the convergence analysis and the bounds as we further describe in the following aspects. 
%  \nm{are there any other system model differences? Such as the fact that only one node is sampled, gradient diversity, SGD, etc.. }
 
 \textit{(2) Aperiodic Local Consensus and global aggregations:} In this manuscript, the devices engage in aperiodic local consensus triggered when the local models deviation exceeds a threshold. Furthermore, the length of the global aggregation intervals are adaptively tuned over time. However, in our prior work, the devices conduct consensus after each gradient descent iterations, and global aggregations are always conducted after conducting consensus.
 
 
 \textit{(3) Convergence Analysis:} In addition to a more complicated system model in this manuscript, explained above, we conduct the analysis under a new definition of gradient diversity (7), which does not exist in our prior work. In particular, having this definition in conjunction with conducting multiple local SGD iterations, makes our convergence analysis \underline{completely different} from our prior work. Furthermore, the convergence analysis that we propose in this paper uses a new set of bounding techniques such as mathematics of dynamic systems, exploiting Riemann theorem to bound summations with integrals, and a series of algebraic manipulations, which are among the first to appear in federated learning literature. 
%  (i) devices may conduct multiple (stochastic) gradient iterations between global aggregations, (ii) the global aggregations are aperiodic, and (iii) consensus procedure among the devices may occur aperiodically during each global aggregation. (iv) we introduced a new definition of gradient diversity to capture the data diversity across clusters. The gradient diversity defined in the paper, characterized by $\delta$ and $\zeta$, is more generalized as compared to the conventional definition of gradient diversity (with $\zeta = 0$) defined in a highly cited paper [R1]. The appearance of $\delta$ and $\zeta$ can be observed in the convergence analysis of Proposition 1 and Theorem 2, this makes the theoretical analysis completely different from state-of-art federated learning literature in deriving the convergence bound. All Theorems except Lemma 1 in completed different and novel.
% \\
% in contrast, -(i),...
% \noindent [R1] S. Wang, T. Tuor, T. Salonidis, K. Leung, C. Makaya, T. He and K. Chan ``Adaptive federated learning in resource constrained edge computing systems," IEEE J. Sel. Areas Commun., vol. 37, no. 6, pp. 1205-1221, 2019.
% \\
\\

These differences are clarified in Sec.~\ref{sec:related} on page~\pageref{sec:related}  of the revised manuscript, which we cite below for  ease of review:

{\color{blue}``Different from~\cite{hosseinalipour2020multi}, we consider the case where (i) devices may conduct multiple (stochastic) gradient iterations between global aggregations, (ii) the global aggregations are aperiodic, and (iii) consensus procedure among the devices may occur aperiodically during each global aggregation. We further introduce a new metric of gradient diversity that extends the previous existing definition in literature. Doing so leads to a more complex system model, which we analyze to provide improvements to resource efficiency and model convergence. Consequently, the techniques used in the convergence analysis and the bound obtained differ significantly from~\cite{hosseinalipour2020multi}."}
\\

\noindent\textbf{R4.3:}   \textit{The D2D communication is in the wireless scenario, the set of D2D neighbors needs to be well defined. If there is any threshold for the wireless channel links to define the partial connectivity? If the set of neighbors is not well defined, the proof will be questionable.}
\\

% \nm{"If there is any threshold for the wireless channel links to define the partial connectivity?" Please add the table showing the degree of conectivity and spectral radious as you did for another reviewer, plus accompanying discussion!}

Thanks for this comment. We have now conducted the simulations under realistic wireless channel settings and further added details in the simulation section to clarify how the set of D2D neighbors are defined. The connectivity among the nodes is determined by wireless channel link characteristics (fast fading and path loss). Also, the rate threshold used to determine the connectivity is obtained via the outage probability. 
In the following, we quote the added materials to Section V (Page 13) for ease of review:

{\color{blue}
\setcounter{equation}{35}
``\textit{\textit{Channel model:}} We assume that the D2D communications are conducted using orthogonal frequency division techniques, e.g., OFDMA, to reduce the interference across the devices. We consider the instantaneous channel capacity for transmitting data from node $i$ to $i'$, both belonging to the same cluster $c$ following this formula:
\begin{equation}\label{eq:shannon}
    C_{i,i'}^{(t)}=W\log_2 \left(1+ \frac{p_i^{(t)} \vert {h}^{(t)}_{i,i'}\vert^2}{\sigma^2} \right),
\end{equation}
where $\sigma^2=N_0 W$ is the noise power, with $N_0=-173$ dBm/Hz denoting the white noise power spectral density;
$W=1$ MHz is the bandwidth; $p^{(t)}_i=24$ dBm, $\forall i,t$ is the transmit power; ${h}^{(t)}_{i,i'}$ is the channel coefficient. We incorporate the effect of both large-scale and small scaling fading in $h^{(t)}_{i,i'}$, given by \cite{tse2005fundamentals,channelJun2021}:
\begin{equation}
     h_{i,i'}^{(t)}= \sqrt{\beta_{i,i'}^{(t)}} u_{i,i'}^{(t)},
\end{equation}
where $\beta_{i,i'}^{(t)}$ is the large-scale pathloss coefficient and $ u_{i,i'}^{(t)} \sim \mathcal{CN}(0,1)$ captures Rayleigh fading, varying i.i.d. over time. We assume channel reciprocity, i.e., $h^{(t)}_{i,i'}=h^{(t)}_{i',i}$, for simplicity. We model $\beta_{i,i'}^{(t)}$ as \cite{tse2005fundamentals,channelJun2021}
\begin{equation}
    \beta_{i,i'}^{(t)} = \beta_0 - 10\alpha\log_{10}(d^{(t)}_{i,i'}/d_0).
\end{equation}
where $\beta_0=-30$ dB denotes the large-scale pathloss coefficient at a reference distance of $d_0=1$ m, $\alpha$ is the path loss exponent chosen as $3.75$ suitable for urban areas, and $d^{(t)}_{i,i'}$ denotes the instantaneous Euclidean distance between the respective nodes.
% We assume that the nodes are placed in an urban area with path-loss exponent 3.75. We consider the noise spectral density of $N_0=-173$ dBm/Hz and further incorporate fading coefficient as Rayleigh fading channel, where the fading coefficient varies according to the distance between two devices.
% \nm{moved:}%We assume that the devices in each cluster are placed uniformly at random in a $50m\times50m$ square field. 
\\

\textit{D2D network configuration:} 
We incorporate the wireless channel model explained above into our scenario to define the set of D2D neighbors and configure the cluster topologies.
% We consider two cases: (i) unknown Channel State Information (CSI) to the devices and (ii) known CSI at the devices. For both cases, 
We assume that the nodes moves slowly so that their locations remain static during each global aggregation period, although it may change between consecutive global aggregations.  
We build the cluster topology based on channel reliability across the nodes quantified via the outage probability. Specifically, considering~(\ref{eq:shannon}), the probability of outage upon transmitting with data rate of $R^{(t)}_{i,i'}$ between two nodes $i,i'$  is given by

\begin{equation}\label{eq:out}
    \textrm{p}^{\mathsf{out},(t)}_{i,i'}=1-\exp\Big(\frac{-(2^{R_{i,i'}^{(t)}}-1)}{\mathrm{SNR}^{(t)}_{i,i'}}\Big),
\end{equation}
 where $\mathrm{SNR}^{(t)}_{i,i'}=\frac{p_i^{(t)}\vert h_{i,i'}^{(t)}\vert^2}{\sigma^2}$.
To construct the graph topology of each cluster $c$, we create an edge between two nodes $i$ and $i'$ if and only if their respective outage probability satisfies $\textrm{p}^{\mathsf{out},(t)}_{i,i'} \leq 5\%$ given a defined common data rate $R^{(t)}_{i,i'}=R^{(t)}_c$, chosen as $R_c^{(t)}=14$ Mbps. This value is used since it is large enough to neglect the effect of quantization error in digital communication of the signals, and at the same time results in connected graphs inside the clusters (numerically, we found an average degree of 2 nodes in each cluster).  %In particular, we find out that with this common rate, an average degree around $2$ in each cluster is achieved.
{After creating the topology based on the large-scale pathloss and outage probability requirements, we model outages during the consensus phase as follows:} if the {instantaneous channel capacity} (given by~\eqref{eq:shannon}, which captures the effect of fast fading) on an edge drops below $R_c^{(t)}$, outage occurs, so that the packet is lost and the model update is not received at the respective receiver. Therefore, although nodes are assumed to be static during each global aggregation period,
     the {instantaneous} cluster topology, i.e., the communication configuration among the nodes, changes
     with respect to every local SGD iteration in a model training interval due to outages.
% The configuration of $R_c^{(t)}$ in the two scenarios (i.e., unknown and known CSI) are described as follows: 
\\

% \begin{enumerate}[leftmargin=5mm]
%      \item
     
    %  Determining the common data rate under unknown CSI at the devices: In this scenario, 
     
    %  Given the location of the nodes, considering $\textrm{p}^{\mathsf{out},(t)}_{i,i'}\leq 5\%$ for all the links gives us the maximum achievable data rate  between the nodes, i.e., $R^{(t)}_{i,i'}$ satisfying~(\ref{eq:out}), $\forall i,i'$. We then find the numerical value of $R^{(t)}_c$, $\forall c$, to have average degree of two inside the clusters. 
    %  After obtaining $R^{(t)}_c$, during ML model training, if the instantaneous channel capacity between two nodes $i$ and $i'$ (under realization of fast fading) is higher than $R^{(t)}_c$, (we find that in our setting $R_c^{(t)}=14Mbps$)
     
    %  \nm{one thing I quite dont understand.. why are you optimizing the rate with a constraint on the degree? What if this rate is too low to support the assumption that quantization error should be negligible? Or are you quantizing the signal in your simulatoin with the given rate?  (if you do quantize the signa, then it is ok...)
    %  To me, the folloiwng makes more sense: 1) you first pick the rate, that should be large enough so that it is ok to consider quantization to be negligible 2) you build your network based on the outage probability.}
    %  {\color{red} we choose $R_c^{(t)}=14Mbps$, if the instantaneous data rate is not greater than that, then outage ???...} 
    %  we consider a link among them where each node {transmits data with common data rate $R_c^{(t)}$;} 
    %  Idea of the story:
    %  1. we fix and design the rate, we create the topology based this and we get the average degree of 2.
    %  2. We want a certain connectivity. we can design the rate to has certain desired connectivity (i.e. if we want fully connected toplogy, this may make rate too small)
    %  3. The reason we design the rate is because we want to ensure that the quatization error is egligible. 
     
     

     
     
     

    %  \nm{what effect does the rate have on the signals? Are you quantizing them based on the given rate? IF NOT, then this statement is inaccurate..}
    %  {\color{red} delay... } we make the rate large enough to make we focus on fadig ant outage
     
    %  otherwise, we consider that the respective nodes do not communicate, i.e., outage occurs and the packet is lost. 
     
     Given a communication graph we choose $d_c=1/8$ to form the consensus iteration at each node $i$ as $\textbf{z}_{i}^{(t'+1)} = \textbf{z}_{i}^{(t')}+{\color{blue}d_{{c}}}\sum_{j\in \mathcal{N}_i} (\textbf{z}_{j}^{(t')}-\textbf{z}_{i}^{(t')})$  (refer to the discussion provided after Assumption~\ref{assump:cons}). Note that given $d_c$, broadcast by the server at the beginning of each global aggregation, each node can conduct D2D communications and local averaging without any global coordination.  
    "\\} 

It is worth mentioning that the connectivity among the nodes in turn dictates the consensus error $\Vert \mathbf e_i^{(t)} \Vert \leq \left(\lambda_{{c}}\right)^{\Gamma^{(t)}_{{c}}} \sqrt{s_c }{\Upsilon^{(t)}_{{c}}},~ \forall i\in \mathcal{S}_c$, which is given in Lemma~\ref{lemma:cons} ($s_c$ denotes the number of nodes in cluster $c$, and $\Upsilon_c^{(t)}$ denotes the divergence of parameters inside the cluster), the smaller the spectral radius or the larger the number of consensus rounds, the smaller the consensus error, which is a desirable property in our algorithm.  Typically, $\lambda_c$ decreases for more tightly connected graphs, and increases with  the number of nodes \cite{xiao2004fast}. More concretely, we have performed a numerical evaluation of the impact of connectivity on the spectral radius, whose results are provided in the following table:

% \nm{how are you generating the graph here? And what do youmean by "average" degrfee? Are you using the wireless-based model? (you should, just to be more concrete) IF so, you need to clarify (just mention that we use a wireless comm model whose details are provided in xx). Also, why is the spc. rad. =1 for avg degree 2?}

\begin{table}[h]
    \centering
    \begin{tabular}{ |c||c|c|c|c|c| } 
        \hline
        Outage probability & 5\% & 10\% & 15\% & 20\% & 25\% \\
        \hline
        Average degree & 3.8 & 5.8 & 6.8 & 8 & 10 \\
        \hline
        Spectral radius $\lambda_c$ & 0.99 & 0.85 & 0.78 & 0.61 & 0.45 \\ 
        \hline
        \end{tabular}
    \caption{Average degree of a $12$-node graph and its corresponding value of $\lambda_c$. Each graph is a realization under the wireless model setup {described in Sec.~\ref{sec:experiments}} with different values of the outage probability requirement. The table demonstrates the impact of connectivity on the spectral radius $\lambda_c$. Smaller value of $\lambda_c$ corresponds to a tightly connected cluster whereas larger value of $\lambda_c$ refers to a loosely connected cluster.}
    \label{tab:my_label}
\end{table}

This table is obtained using the wireless communication model described in Sec.~\ref{channelModel} of the revised manuscript, where we change the outage probability threshold to get graphs with different average node degree. Based on each graph, we build the consensus matrix as $\mathbf V=\mathbf I-d\mathbf L$, where we use $d=1/D_{max}-.01$, where $D_{max}$ is the maximum degree of the nodes in the graph~\cite{xiao2004fast}. Note that $\mathbf I$ denotes the identity matrix and $\mathbf L$ denotes the Laplacian matrix of the adjacency matrix of the graph.

As can be seen from the table, as we increase the average degree for each graph, the spectral radius decreases.
\\
 

\noindent\textbf{R4.4:}   \textit{In (13), the wireless fading channels need to be incorporated. The wireless D2D communication may incur outage or failure for data transmission. Please clarify the communication protocols in the D2D communication system.}
\\

\noindent We are now explicitly considering the effect of fading channels and outage in our simulations. Please refer to our response to your above comment (\textbf{R4.3}) for the detailed explanation. Our new results presented in Sec.~\ref{sec:experiments} of the revised manuscript demonstrate that our method is robust against packet losses induced by fading.
\\

% \nm{Can you concisely comment on the results? I.e explain that our results demonstrate that the algorithm is robust against packet losses induced by fading,etc.}

% This paper focuses on tackling the abstract-level of wireless communication in the MAC layer aspect. So fundamentally, different effects of wireless communication in the physical layer is embedded in the cluster topology and what that would come down to is that the consensus model would have some error added to it during each round of local aggregation. [R2] has demonstrated that consensus averaging update on graphs under general noisy channels using a particular class of distributed consensus algorithms (I'm still checking if the consensus matrix L used in [R2] satisfies the three condition we have: L satisfies (i) $\left(\mathbf{V}_{{c}}\right)_{m,n}=0~~\textrm{if}~~ \left({m},{n}\right) \notin \mathcal{E}_{{c}}$, i.e., nodes only receive from their neighbors; (ii) $\mathbf{V}_{{c}}\textbf{1} = \textbf{1}$, i.e., row stochasticity) converge almost surely to exact consensus for finite variance noise.
% \\ 
% reliable connection between server and device. The station can sample a node with good channel, which is better than the D2D channel. So the noise here can be more negligible. std 

% \noindent [R2] R. Rajagopal and M. J. Wainwright, "Network-Based Consensus Averaging With General Noisy Channels," IEEE Trans. Signal Process., vol. 59, no. 1, pp. 373-385, 2011.

\noindent\textbf{R4.5:}   \textit{In (18), the global model aggregation does not have any noise, please explain it. Again, what’s kind of communication protocols adopted, adaptive rate or fixed data rate with outage?}
\\

Regarding the noise added to the signals transmitted in D2D or uplink to the main server,
% \nm{please revise according to my edits to Reviewer 1. I.e, explain that signal is quantized, encoded, transmtited, and then decoded. But in the analysis we make abstractions where we assume that no errors are introruced in this process.}
%%%%%%%%%%
{we would like to remark that we do not consider analog transmission of signals in this work. Instead,
the signals exchanged by the nodes during the D2D consensus period are quantized, encoded, transmitted over the wireless channel, and then decoded at the receiver. The same holds for the signal transmission between the devices and the main server, based on which the global aggregation is performed. Although in our analysis we abstract this encoding/decoding process and assume that the signals are received with no errors at the receiver (as commonly assumed in the consensus literature, see for instance \cite{xiao2004fast}), in the revised manuscript we have included an evaluation on the effect of packet losses resulting from fading channels, which shows that our proposed algorithm performs well even under these impairments. Specifically:}
%We break down our response into the following parts:


\textit{(1) Quantization/encoding/decoding:}
{As mentioned, we assume that the signals are quantized and encoded/decoded for digital transmission. In practice, this may be achieved using state-of-the-art coding techniques (such as LDPC codes), that achieve small bit error rates~\cite{8316763}, along with
precoding techniques to mitigate the effect of signal outage due to fading~\cite{precoding2021}.
}

{In regard to quantization of the signals during consensus, we would like to remark that prior work~\cite{consensus2009quantize} from one of the authors demonstrated that exact consensus may be achieved in a mesh network even under quantization of the consensus signals. For analytical tractability, our analysis assumes that the signals received during the consensus and model aggregation processes are free from errors due to quantization and/or signal outage.}
This is now clarified in Sec. II (Page \pageref{rem:3}) of the paper, which we quote below for  ease of review:
\\
% no please don;t say this. we CANNOT make this claim in our analysis..
% , but saying that the model is general opens the door to more questions from the reviewer
% \nm{talk about extension (i.e., how to incorporate quantization)}

% ``{\color{blue} It is worth mentioning that upon using any D2D protocol for conducting local model aggregations, as long as the model parameter at each node $i$ can be written as (13) for a general $\mathbf{e}_i$, our analysis and the subsequent results (Proposition 1, Theorem 1 and Theorem 2) readily hold.  Note that we consider digital transmission (in both D2D and uplink/downlink communications) where using state-of-the-art techniques in encoding/decoding, e.g., low density parity check (LDPC) codes, the bit error rate (BER) is reasonably small and negligible~\cite{8316763}, and also the effect of quantized model transmissions can be readily incorporated using the techniques in~\cite{consensus2009quantize}. So, the devices' transmitted model parameters  can be recovered at the server almost perfectly unless outage occurs, which can be avoided using precoding~\cite{precoding2021} or taken into account to obtain proper rounds of D2D communications via the method of~\cite{consensus2009wireless}"} 
% % {\color{blue} Some citation regarding consensus under noisy channels will be added here}
{\color{blue}
 ``Note that we consider digital transmission (in both D2D and uplink/downlink communications) where using state-of-the-art techniques in encoding/decoding, e.g., low density parity check (LDPC) codes, the bit error rate (BER) is reasonably small and negligible~\cite{8316763}. Moreover, the effect of quantized model transmissions can be readily incorporated using techniques developed in~\cite{consensus2009quantize},
and precoding techniques may be used to mitigate the effect of signal outage due to fading~\cite{precoding2021}.
 Therefore, in this analysis, we assume that the model parameters transmitted by the devices to their neighbors (during consensus) and then to the server (during global aggregation) are received with no errors at the respective receivers. The impact of outages due to fast fading and lack of CSI will be studied numerically in Sec. \ref{sec:experiments}."
}   
\\




%%%%%%%%%%%%%%%%%%%

% \noindent We would like to mention that we do not consider analog transmission of signals in this work. In particular, we consider digital transmission where using state-of-the-art techniques in encoding/decoding, e.g., LDPC codes, the bit error rate (BER) is reasonable small and negligible [R4]. So, the devices' transmitted model parameters  can be recovered at the server almost perfectly unless outage occurs, which can be avoided using precoding [R1]. We have further clarified this in Sec. II (page 6), which we quote the added text below for the ease of review:






% ``It is worth mentioning that the theoretical results obtained in Proposition 1, Theorem 1 and Theorem 2 are general results, which hold for a wide range of algorithms that augment the global aggregations with (imperfect) local model aggregations in federated learning. In particular, upon using any D2D protocol for conducting local model aggregations, as long as the model parameter at each node $i$ can be written as (13) for a general $\mathbf{e}_i$, our analysis readily holds. Note that we consider digital transmission (in both D2D and uplink/downlink communications) where using state-of-the-art techniques in encoding/decoding, e.g., low density parity check (LDPC) codes, the bit error rate (BER) is reasonably small and negligible [48], and also the effect of quantized model transmissions can be readily incorporated using the techniques in~[49]. So, the devices' transmitted model parameters  can be recovered at the server almost perfectly unless outage occurs, which can be avoided using precoding [50] or taken into account to obtain proper rounds of D2D communications via the method of~[51]."

% \noindent [R1] T. Sery, N. Shlezinger, K. Cohen and Y. C. Eldar, ``Over-the-Air Federated Learning from Heterogeneous Data," IEEE Trans. Signal Process., 2021.
% \\

% \noindent [R2] S. Kar and J. M. F. Moura, ``Distributed Consensus Algorithms in Sensor Networks With Imperfect Communication: Link Failures and Channel Noise," IEEE Trans Signal Process., vol. 57, no. 1, pp. 355-369, 2009.
% \\

% \noindent [R3] C. -S. Lee, N. Michelusi and G. Scutari, ``Finite Rate Distributed Weight-Balancing and Average Consensus Over Digraphs," IEEE Trans Autom. Control.
% \\

% \noindent [R4] T. Richardson and S. Kudekar, ``Design of Low-Density Parity Check Codes for 5G New Radio," IEEE Commun. Mag., vol. 56, no. 3, pp. 28-34, 2018.
% \\

% This paper focuses on tackling the abstract-level of wireless communication in the MAC layer aspect, which is also a topic of interest in this JSAC special issue, which is similar to [R3]. 
% The main contribution of this research is to consider improving distributed model training efficiency by blending federated aggregations with cooperative consensus procedure among local device clusters at a shorter timescale and developed an adaptive control algorithm for {\tt TT-HF} that tunes the global aggregation intervals, the rounds of D2D performed by each cluster, and the learning rate over time to minimize energy consumption and added delay while maximizing trained model accuracy. Further details on how to incorporate the physical layer effect during global model aggregation remains open and will be stated as a future research interest in the conclusion. 
% That this the future work, noisy, fading, scheduling sampling. 

% \noindent [R3] S. Wang, T. Tuor, T. Salonidis, K. Leung, C. Makaya, T. He and K. Chan ``Adaptive federated learning in resource constrained edge computing systems," IEEE J. Sel. Areas Commun., vol. 37, no. 6, pp. 1205-1221, 2019.

\noindent\textbf{R4.6:}   \textit{In definition 3, why the consensus error can be bounded absolutely in the wireless fading and noise scenarios?}
\\
% \nm{the answer is not appropriate...you need to say explicity that, for analytical tractability, we assume that the signals are received with no errors due to quantization or fading. However, we included new simulation resutls to evaluate the effect of fgading and packet losses.}

\noindent We would like to remark that, in our rigorous mathematical analysis, for tractability we make the assumptions that the signals transmitted by the nodes during the consensus phase are received with no errors at the corresponding receiver (please see our response to \textbf{R4.5}). Therefore, wireless fading and noise are not accounted for in our convergence analysis.
Under this setting, as shown in Eq.~\eqref{eq:cons} in the revised manuscript, the consensus error decays
as $\Vert \mathbf e_i^{(t)} \Vert \leq \left(\lambda_{{c}}\right)^{\Gamma^{(t)}_{{c}}} \sqrt{s_c}{\Upsilon^{(t)}_{{c}}},~ \forall i\in \mathcal{S}_c$, where $\Gamma_c^{(t)}$ is the number of consensus steps and $\lambda_c$ is the spectral radius.
However, our new simulations consider the effect of packet losses due to fading and outage, which demonstrate that our method is robust against packet losses induced by fading (please refer to \textbf{R4.3}, where we explain the setting under which the simulations are performed).
\\
% Our new results presented in Sec.~\ref{sec:experiments} of the revised manuscript demonstrate that our method is robust against packet losses induced by fading.

% ... \nm{at this point talk about simulation! Use a similar structure as used for another reviewr with a similar qusetion}

% \noindent Similar to our above response, we consider digital transmission where using state-of-the-art techniques in encoding/decoding, e.g., LDPC codes, the bit error rate (BER) is reasonable small and negligible. So, the bits transferred across the devices can be recovered almost perfectly. However, fading and noise (incorporated in outage probability) are used to determine the D2D topology (see our response to \textbf{R4.3}).
% \\

% $\Gamma_c^{(t)}=\max\Big\{\log\Big(\frac{\eta_t\phi}{s_c\Upsilon_c^{(t)}M}\Big)/\log\Big(\lambda_c\Big),0\Big\},\forall c$, which is obtained
% using Lemma 1 (in the expression, $s_c$ denotes the number of  nodes in cluster $c$, $\Upsilon_c^{(t)}$ is the divergence of the parameters in cluster $c$, $M$ is the dimension of the model, and $\lambda_c$ is the spectral radius of matrix $\mathbf{V}_{{c}}-\frac{\textbf{1} \textbf{1}^\top}{s_c}$, where $\mathbf V_c$ is the consensus matrix of the respective cluster

\noindent\textbf{R4.7:}   \textit{Provide the computational complexity for solving problem (50).} 
\\

\noindent Solving our optimization problem involves two steps: (1) linear regression of the constants used in (53), i.e., $A_c^{(k)},B_c^{(k)},a_c^{(k)},b_c^{(k)}$ using the history of observations, and (ii) line search over the feasible integer values for $\tau_k$. The complexity of part (i) is  $O(\tau_{k-1})$, since the dimension of each observant, i.e., $\Upsilon_c^{(t)}$, is one and the observations are obtained via looking back into the previous global aggregation interval.
Also, the complexity of (ii) is $O(\tau_{\mathsf{max}})$ since it is just an exhaustive search. We have now added the complexity of the solver into the Section IV. (Page~\pageref{parg:in-cluster}) after our optimization problem, which we quote below for  ease of review:

{\color{blue}
``Solving our optimization problem involves two steps: (i) linear regression of the constants used in (53), i.e., $A_c^{(k)},B_c^{(k)},a_c^{(k)},b_c^{(k)}$ using the history of observations, and (ii) line search over the feasible integer values for $\tau_k$. The complexity of part (i) is $\mathcal O(\tau_{k-1})$, since the dimension of each observant, i.e., $\Upsilon_c^{(t)}$, is one and the observations are obtained via looking back into the previous global aggregation interval.   Also, the complexity of (ii) is $\mathcal O(\tau_{\max})$ since it is just an exhaustive search 
{over} the range of $\tau\leq\tau_{\max}$, where $\tau_{\max}$ is the maximum tolerable interval that satisfies the feasibility conditions in Sec.~\ref{subsec:learniParam}."
}
\\

\noindent\textbf{R4.8:}   \textit{How to determine the global model aggregation frequency?}
\\

% \noindent Triggering the global aggregation rounds, i.e., tuning the local model training interval $\tau_k$, $\forall k$, is conducted in our adaptive control algorithm according to the joint impact of three metrics: energy consumption, training delay imposed by consensus, and trained model performance formulated in the optimization problem $(\bm{\mathcal{P}})$ in Sec. IV-B. It can be noted that in a scenario when D2D communications require significant energy consumption and delay, $(\bm{\mathcal{P}})$ will force faster global aggregations to guarantee a certain convergence guarantee. However, if the D2D communications are significantly low power and delay, $(\bm{\mathcal{P}})$ prolongs the global aggregations rounds and exploits D2D communication-based aggregations to keep the local models synchronized.
% \textit{(2) Triggering global aggregations:}  
\noindent Triggering the global aggregation rounds, i.e., tuning the local model training interval $\tau_k$, $\forall k$, is conducted in our adaptive control algorithm according to the joint impact of three metrics: energy consumption, training delay imposed by consensus, and trained model performance formulated in the optimization problem $(\bm{\mathcal{P}})$ in Sec. IV-B. It can be noted that in a scenario when D2D communications require significant energy consumption and delay, $(\bm{\mathcal{P}})$ will force faster global aggregations to guarantee a certain convergence guarantee. However, if the D2D communications are significantly low power and delay, $(\bm{\mathcal{P}})$ prolongs the global aggregations rounds and exploits D2D communication-based aggregations to keep the local models synchronized.

This is now clarified in Sec.~\ref{subsec:learnduration}, page~\pageref{subsec:learnduration}, which we quote below for the ease of review:

``{\color{blue}  We thus propose tuning the $\tau_k$ and $\Gamma^{(t)}_c$ parameters according to the joint impact of three metrics: energy consumption, training delay imposed by consensus, and trained model performance. To capture this {trade-off}, we formulate an optimization problem $(\bm{\mathcal{P}})$ solved by the main server at the beginning of each global aggregation period $\mathcal{T}_k$,}"
}\fi

% \nm{did you add any clarifications/rewording in the manuscript to help the reviewer understand this? (you should..)}
% \noindent\textbf{Global aggregation:} Using Theorem 2, given a desired convergence bound, i.e., $$\mathbb E\left[(F(\hat{\mathbf w}^{(t)})-F(\mathbf w^*))\right]\leq\frac{\nu}{t+\alpha}$$ with fixed $\nu$, local model training interval in between two consecutive global aggregation $\tau_k$ is required to be less than a threshold $\tau$. Taking this fact into account, the tuning of local model training interval in between two consecutive global aggregation $\tau_k$ in the adaptive control algorithm is also determined according to the joint impact of three metrics: energy consumption, training delay imposed by consensus, and trained model performance formulated in the optimization problem $(\bm{\mathcal{P}})$.
% \\
\endgroup

% \input{appendices.tex}
% \input{appendices2.tex}


\end{document}

