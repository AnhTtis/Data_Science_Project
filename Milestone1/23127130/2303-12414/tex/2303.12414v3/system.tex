\section{System Model and Machine Learning Methodology}
\label{sec:tthf}
\noindent In this section, we first describe our hierarchical edge network system model (Sec.~\ref{subsec:syst1}) and formalize the ML task of our interest (Sec.~\ref{subsec:syst2}). Then, we develop our delay-aware federated learning procedure, {\tt DFL} (Sec.~\ref{subsec:syst3}).
% \addFL{For the following descriptions, we omit the superscript $(k)$ without causing confusion, but keep in mind that XXX may vary with $k$.}

\subsection{Edge Network System Model}
\label{subsec:syst1}
We consider ML model learning over the hierarchical network architecture depicted in Fig.~\ref{fig2}. The network consists of three layers of nodes. At the top of hierarchy, 
a main server (e.g., a cloud server) is in charge of global model aggregations. In the middle of the hierarchy, there are $N$ edge servers (e.g., coexisting with cellular base stations), collected via the set $\mathcal{N}=\{n_1,\cdots,n_N\}$, which act as local model aggregators. Finally, at the bottom of the hierarchy, there are $I$ edge devices represented via the set $\mathcal{I}=\{1,\cdots,I\}$ in charge of local model training. 
% , $N$ edge servers (e.g., at a base station) and $I$ edge devices indexed by the set $\mathcal{I}=\{1,\cdots,I\}$. 
% \nm{$\mathcal{I}$ appears to be unused..}
The edge devices are organized into subnets (subnetworks), with a one-to-one mapping between the subnets and the edge servers, providing them uplink/downlink connectivity. In particular, 
 the devices
are partitioned into $N$ sets $\{\mathcal S_c\}_{c=1}^{N}$, where set $\mathcal{S}_c$ is associated with edge server $n_c \in \mathcal{N}$ for uplink/downlink model transmissions, with
$\mathcal{S}_c \cap \mathcal{S}_{c'} =\emptyset~\forall c\neq c'$ and $\cup_{c=1}^{N} \mathcal{S}_{c}=\mathcal{I}$. 
Each subnet $\mathcal{S}_c$ consists of $s_c = |\mathcal{S}_c|$ edge devices, where $\sum_{c=1}^{N}s_c=I$.\footnote{This model is capable of being adapted to time-varying topologies, for instance,  mobile devices switching subnet. However, we omit this for notational simplicity.}
% \nm{nnotational simplicity?}}


\begin{table}[h]
  \centering
  \caption{List of acronyms}
  \label{table:res}
  \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Acronym} & \textbf{Definition} & \textbf{Acronym} & \textbf{Definition} \\
    \hline
    ML & Machine Learning & FL & Federated Learning \\
    \hline
    DFL & Delay-Aware Federated Learning & SGD & Stochastic Gradient Descent \\
    \hline
    SVM & Support Vector Machine & CNN & Convolutional Neural Network \\
    \hline
  \end{tabular}
\end{table}

\begin{table}[h]
  \centering
  \caption{List of key notations}
  \label{table:notation}
  \begin{tabular}{|c|m{6.3cm}|c|m{6.3cm}|}
    \hline
    \textbf{Notation} & \textbf{Description} & \textbf{Notation} & \textbf{Description} \\
    \hline
    $\mathcal N$ & Set of all edge servers & $N$ & Number of all edge servers \\
    \hline
    $\mathcal I$ & Set of all edge devices & I & Number of all edge devices \\
    \hline
    $\mathcal S_c$ & Set of edge devices in subnet $c$ & $s_c$ & Number of edge devices in subnet $c$ \\
    \hline
    $n_c$ & Index of local model aggregator (edge server) & $\ell(\cdot)$ & Loss function associated with each datapoint \\
    \hline
    $\mathcal D_i$ & Dataset of edge devices $i$ & $D_i$ & Number of data points in edge device $i$ \\
    \hline
    $F_i(\cdot)$ & Local loss function for device $i$ & $\bar{F}_c(\cdot)$ & Subnet loss function for subnet $c$ \\
    \hline
    $F(\cdot)$ & Global loss function & $\mathbf w^*$ & Optimal global model \\
    \hline
    $\delta, \zeta$ & Inter-gradient diversity parameters across subnets & $\delta_c,\zeta_c$ & Intra-gradient diversity parameters in subnet $c$ \\
    \hline
    $t$ & Time index of local model training iteration & $k$ & Index of local model training interval\\
    \hline
    $\mathcal T_k$ & $k$-th local model training interval & $\tau_k$ & Length of the $k$-th local model training interval \\
    \hline
    $\mathcal T_{k,c}^\mathsf{L}$ & Set of local aggregation instances in the $k$-th local model training interval & $\widehat{\mathbf g}_{i}^{(t)}$ & Local stochastic gradient at device $i$ and time $t$ \\
    \hline
    $\mathbf w_i^{(t)}$ & Local model at device $i$ and time $t$ & $\widetilde{\mathbf w}_i^{(t)}$ & Tentative Local model at device $i$ and time $t$ \\
    \hline
    $\bar{\mathbf{w}}_{c}^{(t)}$ & Instantaneous aggregated local model across devices $i\in\mathcal S_c$ & $\bar{\mathbf w}^{(t)}$ & Global model at time $t$ \\
    \hline
    $\Theta_c^{(t)}$ & Indicator of local aggregation for subnet $c$ & $\alpha_k$ & Combiner weight for the linear local-global combiner \\
    \hline
    $\Delta^\mathsf{U}_k$ & Upstream delay between edge device and main server & $\Delta^\mathsf{D}_k$ & Downstream delay between edge device and main server \\
    \hline
  \end{tabular}
\end{table}



% Each cluster is associated with a edge server $n_c\in\mathcal{N}$, where $N=\{n_1,\cdots,n_N\}$, allowing the devices $i \in \mathcal{S}_c$ to perform local aggregation among the devices $j \in \mathcal{S}_c, \forall j\neq i$ in the same cluster $\mathcal{S}_c$.
% \nm{what do you mean? Each device only communicates directly with its servber..}
%For convenience, $\mathcal{S}_c$ will be used to refer to both the cluster itself and to the set of nodes inside it. 

% We assume that the clusters are formed based on the ability of devices to conduct low-energy D2D communications, e.g., geographic proximity. Thus, one cluster may be a fleet of drones while another is a collection of local IoT sensors. In general, we do not place any restrictions on the composition of devices within a cluster, as long as they possess a common D2D protocol~\cite{hosseinalipour2020federated} and communicate with a common server.

% For edge device $i\in \mathcal S_c$, we let $\mathcal{N}_i\subseteq S_c$ denote the set of its D2D neighbors, determined based on the transmit power of the nodes, the channel conditions between them, and their physical distances. We assume that D2D communications are bidirectional, i.e., $i \in \mathcal{N}_{i'}$ if and only if ${i'}\in{\mathcal N}_i$, $\forall i,i'\in\mathcal{S}_c$. Based on this, we associate a network graph $G_c(\mathcal{S}_c, \mathcal E_c)$ to each cluster $\mathcal{S}_c$, where $\mathcal{S}_c$ and $\mathcal E_c$ are the sets of nodes and edges: $(i,{i'})\in \mathcal E_c$ if and only if $i,i'\in\mathcal{S}_c$ and $i \in \mathcal {N}_{i'}$.






% 0 .... 1

% $i \in \mathcal{S}_c$ will participate in local aggregation with devices $j \in \mathcal{S}_c, \forall j\neq i$ in the same cluster $\mathcal{S}_c$. 


% Due to the mobility of the devices, the topology of each cluster (i.e., the number of nodes and their positions inside the cluster) can change over time, though we will assume this evolution is slow compared to the time in between two global aggregations.
% \nm{need to be precise on what you mean by time varying: when are clusters allowed to change?} }


%  As described later, our analysis can easily incorporate the case where D2D communications are not performed in some clusters. 
%  For tractability, it is assumed that the graph structure remains fixed during the interval of one global aggregation, while it can change from one global aggregation to another.
% We consider a dynamic network setting, where the set of neighbors of the nodes can evolve over time, e.g., due to device movements. However, for tractable analysis the set of nodes inside the cluster is assumed to be fixed. 

% \begin{remark}
%     Note that the parameters $\mathcal S_c$, $s_c$ for each cluster $c$ can change depending on the global aggregation index $k$. We drop the dependency on $k$ for simplicity of presentation, Although the analysis implicitly accommodates it. We will similarly do so in notations for node and cluster weights $\rho_{i,c}$, $\varrho_c$ introduced in Sec.~\ref{subsec:syst2}. %All of our proposed analysis are general and can readily accommodate to this case and capture the dependency on the global aggregation index.
% \end{remark}

% The model learning topology in this paper (Fig.~\ref{fig2}) is a distinguishing feature compared to the conventional federated learning star topology (Fig.~\ref{fig:simpleFL}). Most existing literature is based on Fig.~\ref{fig:simpleFL}, e.g.,~\cite{8737464,chen2019joint,yang2019energy,yang2020federated,smith2017federated,corinzia2019variational,9006060,jiang2019improving}, where devices only communicate with the edge server.

% \nm{need to be careful with "time-varying" graphs, this can rise several questions to the reviewer, and may add several challenges to the analysis. I would focus on the case where the network graph is fixed, or at least the graph does not change between two global aggregations}

% consists of a single edge server and multiple edge clusters indexed by $c = 1,2,...,N$. Each cluster $\mathcal G(\mathcal{S}_c^{(k)}, \mathcal E_c)$ contain multiple edge devices $s_c^{(k)} = 1,2,...,s_c^{(k)}$ and a set of edges $\mathcal E_c$, where $\{i,j\}\in \mathcal E_c$. The set of neighbors of device $i$ in cluster $c$ is denoted as $\mathcal N_i^{(c)}=\{j|\{i,j\}\in\mathcal E_c\}$. The edge devices collect data and perform local updates to optimize a loss function $F_i(\cdot)$ corresponding to a machine learning task (described next). To enhance the overall model learning speed, device-to-device (D2D) communications between the edge nodes are enabled to allow locally trained parameters $\mathbf w_i$ to be transferred to neighboring devices such that the devices inside each cluster are capable of computing the aggregation of their locally-trained parameters in a distributed manner, through message passing and consensus formation.  The edge server (the cloud) plays the role of an aggregator, collecting one of the locally trained parameters $\mathbf w$ from each cluster to perform a global update. Local updates are taken to be gradient descent steps on the local loss functions $F_i(\mathbf w)$ after consensus, while global updates refers to aggregation followed by synchronization. Aggregation denotes the computation of a global model obtained using the weighted average of local models, while synchronization represents the update of local models at the edge after aggregation \cite{Tu}.

\begin{figure*}[t]
  \centering
%   \vspace{-0.4in}
    \includegraphics[width=0.96\textwidth]{image/fedDel_struct.png}
     \caption{
    %  \nm{figure is too dense. Maybe show only 2 clusters since it captures the essence of the model? You can write in the caption "example with two clusters"}
     The {\color{blue}three-layer} hierarchical federated learning network architecture consists of edge devices forming local subnet topologies based on their proximity to the local aggregator.}
     % Local aggregations among these subnets occur between global aggregations by the server.}
    %  \nm{simplify to only two clusters..}
     \label{fig2}
     \vspace{-0.7em}
\end{figure*}
 
\subsection{Machine Learning Model} \label{subsec:syst2}
% \subsubsection{Training dataset}
We assume that each edge device $i\in\mathcal{I}$ owns a dataset $\mathcal{D}_i$ with $D_i=|\mathcal{D}_i|$ data points. In general, the $\mathcal{D}_i$'s are non-i.i.d. (non-independent and identically distributed), i.e., there exists statistical dataset diversity across the devices. Each data point $(\mathbf x,y)\in\mathcal D_i$, $\forall i$, consists of an $m$-dimensional feature vector $\mathbf x\in\mathbb R^m$ and a label $y\in \mathbb{R}$.
% \subsubsection{Loss functions and ML objective}
We let $\ell(\mathbf x,y;\mathbf w)$ denote the \textit{loss} associated with 
data point $(\mathbf x,y)$ under \textit{ML model parameter vector} $\mathbf w \in \mathbb{R}^M$, where $M$ denotes the dimension of the model. The loss quantifies the precision of the ML model with respect to the underlying ML task; for example, in linear regression, $\ell(\mathbf x,y;\mathbf w) = \frac{1}{2}(y-\mathbf w^\top\mathbf x)^2$. The \textit{local loss function} at device $i$ is defined as
% \nm{the way you use bar is a bit inconsistent. Sometime you use it for "average" but you use it also to denote the loss on a single datapoint. Can you use something else other than $\bar f$?}
\begin{align}\label{eq:1}
    F_i(\mathbf w)=\frac{1}{D_i}\sum\limits_{(\mathbf x,y)\in\mathcal D_i}
    \ell(\mathbf x,y;\mathbf w).
\end{align}
% \chris{Should point out here that we make no assumptions on $\mathcal{D}_i$ except that they are a subset of the global dataset.} 

We subsequently define the \textit{subnet loss function} for each $\mathcal{S}_c$ as the average loss of devices in the subnet, i.e.,
\begin{align}\label{eq:c}
    \bar F_c(\mathbf w)=\sum\limits_{i\in\mathcal{S}_c} \rho_{i,c} F_i(\mathbf w),
\end{align}
% \nm{there is a problem here.. since clusters are time varying, $\bar F_c(\mathbf w)$ is TV as well! Is our analysis able to handle this scenario?}
where $\rho_{i,c} = D_i/\sum_{j\in\mathcal S_c}D_j$ is the weight associated with edge device $i\in \mathcal{S}_c$ relative to its respective subnet. 
% \nm{how is this computed? Di over sum Di in cluster c?}
% \chris{This definition of $\rho$ is weighting each device the same. Can we generalize to the case where we weight by number of datapoints? It's more realistic.} 
% \chris{I agree with Nicolo, per my previous comment. If we want to motivate with heterogeneity it would be better to handle the case where the number of datapoints are not the same at each device. I suspect that our analysis holds even for a general $\rho_{i,c}^{(k)}$.}
The \textit{global loss function} is defined as the average loss across the subnets
\begin{align} \label{eq:2}
    F(\mathbf w)=\sum\limits_{c=1}^{N} \varrho_c \bar F_c(\mathbf w),
\end{align}
where $\varrho_c = \sum_{i\in\mathcal S_c}D_i/\sum_{j\in\mathcal I}D_j$ is the weight associated with subnet $\mathcal{S}_c$ relative to the network.
% \nm{how is this computed? Di over sum Di in cluster c?}

% The weight of each edge node $i \in\mathcal{S}_c$ relative to the network can thus be obtained as $\rho_i=\varrho_c\cdot\rho_{i,c}= D_i /\sum_{j\in\mathcal I}D_j$,
% \nm{whre do you get this from? Why not Di over sum Di?}
% meaning each node has an equal weight in $F$.
% \chris{Following on the previous comment, I think we can write $\rho_{i,c}^{(k)} = D_i (\sum_{j \in \mathcal{S}_c^{(k)}} D_j)^{-1}$ and $\varrho_c^{(k)} = (\sum_{j \in \mathcal{S}_c^{(k)}} D_j) (\sum_{i \in \mathcal{I}} D_i)^{-1}$.}
% \nm{why is $\rho_i$ not a function of k?}
The goal of ML model training is to find the optimal global model parameter vector $\mathbf w^* \in \mathbb{R}^M$: % \nm{you have already defined this as $m$ before!}
\begin{align}
    \mathbf w^* = \mathop{\argmin_{\mathbf w \in \mathbb{R}^M} }F(\mathbf w).
\end{align}
% \nm{you also need to discuss the existence of $w^*$: its existence is implied by strong convexity of F.}

% \begin{remark}
% An alternative way of defining~\eqref{eq:2} is as an average performance over the datapoints, i.e., $F(\mathbf{w})=\sum_{i=1}^{I} \frac{D_i F_i(\mathbf w)}{\sum_j D_j}$~\cite{tu2020network,wang2019adaptive}. Both approaches can be justified: our formulation promotes equal performance across the devices, at the expense of giving devices with lower numbers of datapoints the same priority in the global model. Our analysis can readily be extended to this other formulation too, in which case the distributed consensus algorithms introduced in Sec.~\ref{subsec:syst3} would take a weighted form instead.
% \end{remark}

%In this work, for simplicity of analysis we focus on minimizing the risk/loss over each device, where the devices are weighted equally in the global loss function~\eqref{eq:2}. One alternative way is to define the global loss as an average performance over the data points, i.e., $F(\mathbf{ w})=\sum_{i=1}^{I} \frac{D_i F_i(\mathbf w)}{D}$, where $D$ denotes the total number of data points across the devices,
%in which case instead of distributed consensus introduced later, the weighted distributed consensus algorithm~\cite{hernandes2018improved} should be used. In real scenarios, either of these cases can be used. Our formulation of global loss function achieves a global model that works well for all the devices and avoids having devices with very low performance. This comes with the drawback of giving the devices with very low number of data points the same priority as those with very large number of data points. Also, the aforementioned alternative way of formulation of loss function would bias the global model to the data distribution at the devices with higher number of data points, in which case the devices with lower number of data points may suffer from poor performance.}

In the following, we introduce some  assumptions regarding the above-defined loss functions that are commonly employed in distributed ML literature~\cite{haddadpour2019convergence,wang2019adaptive,chen2019joint,friedlander2012hybrid}. These assumptions further imply the existence and uniqueness of $\mathbf w^*$. 
\begin{assumption}[Loss Functions Characteristics]\label{Assump:SmoothStrong}
\label{beta}
Henceforth, the following assumptions are made:
\begin{itemize}[leftmargin=3mm]
\item  \textbf{Strong convexity}:
% \nm{define $\mu\beta$ as the strong convexity param so that $\mu\in[0,1]$..}
 The global loss function $F$ is $\mu$-strongly convex, i.e.,
 %\footnote{In practical settings, convex ML loss functions, e.g., squared SVM and linear regression, are implemented along with a regularization term to improve the ML model convergence and avoid overfitting, which makes them strongly convex.}
\begin{equation} \label{eq:11_mu} 
    % \hspace{-5.5mm}\resizebox{.98\linewidth}{!}{$
    F(\mathbf w_1) \geq  F(\mathbf w_2)+\nabla F(\mathbf w_2)^\top(\mathbf w_1-\mathbf w_2)
    % \nonumber \\&
    % ~~~~
    +\frac{\mu}{2}\Big\Vert\mathbf w_1-\mathbf w_2\Big\Vert^2 \hspace{-1.2mm},~\hspace{-.5mm}\forall { \mathbf w_1,\mathbf w_2} \in  \mathbb{R}^M.
    % $}\hspace{-1.mm}
\end{equation} 
    \item  \textbf{Smoothness:} Each local loss function $F_i$ is $\beta$-smooth $\forall i\in\mathcal{I}$, i.e., 
    \begin{align} \label{eq:11_beta}
\Big\Vert \nabla F_i(\mathbf w_1)-\nabla F_i(\mathbf w_2)\Big\Vert \leq & \beta\Big\Vert \mathbf w_1-\mathbf w_2 \Big\Vert, ~\forall \mathbf w_1, \mathbf w_2 \in \mathbb{R}^M,
 \end{align}
where $\beta>\mu$. These assumptions imply the $\beta$-smoothness of $\bar{F}_c$ and $F$ as well. \footnote{Throughout, $\Vert \cdot \Vert$ is  used to denote 2-norm of the vectors.}

% \nm{Define the condition number of the optimization problem as kappa = beta over mu as well. I assume you use it later.}
%  , i.e., $\Vert\nabla F(\mathbf w)\Vert^2 \leq 2\beta[F(\mathbf w)-F(\mathbf w^*)]$, $\forall \mathbf w$.\nm{why do you call this inequality "$\beta$-smoothness of the global function,"?}
\end{itemize}
% \begin{description}
% \item[$\beta$-smoothness:] 

% \nm{are you assuming that $F_i$ are convex? You need to state that..}
% \frank{I am not assuming $F_i$ to be convex}
% \end{description}
\end{assumption}
% We further define $\vartheta = \mu/\beta < 1$.\nm{no dont add this, keep notation simple..with my def above, $\mu$ is already the ratio} 
The above assumptions are leveraged in our theoretical analysis in Sec.~\ref{sec:convAnalysis}, and subsequently to develop the control algorithm in Sec.~\ref{Sec:controlAlg}. Our experiments in Sec.~\ref{sec:experiments} demonstrate the effectiveness of our methodology, even for non-convex loss functions (e.g., neural networks).

We next introduce measures of gradient diversity to quantify the statistical heterogeneity across local datasets. Different from our existing work~\cite{lin2021timescale}, we consider this both across and within subnets, which will be important to our analysis:

% \begin{assumption} \label{PL}
% $F(\cdot)$ satisfies the Polyak-≈Åojasiewicz (PL) condition with constant $\mu$.
% \begin{align} \label{eq:11}
%     \frac{1}{2}\Big\Vert\nabla F(\mathbf w)\Big\Vert^2 \geq \mu(F(\mathbf w)-F(\mathbf w^*)),\ \forall \mathbf w,
% \end{align}
% \end{assumption} 

% \begin{assumption} \label{gradVar}
%     $$\mathbb E[\Vert(\widehat{\mathbf g}_j(\mathbf w_j(t))-\nabla F_j(\mathbf w_j(t))\Vert_2^2]\leq\sigma^2$$
% \end{assumption}

\begin{definition}[Inter-Subnet Gradient Diversity]\label{gradDiv}
The inter-subnet gradient diversity across the device subnets is measured via two non-negative constants $\delta,\zeta$ that satisfy 
\begin{align} \label{eq:11}
    \left\Vert\nabla\bar F_c(\mathbf w)-\nabla F(\mathbf w)\right\Vert
    \leq \delta+ \zeta \Vert\mathbf w-\mathbf w^*\Vert,~\forall c, \mathbf w.
\end{align}
% \nm{why for all i?}
\end{definition} 
\begin{definition}[Intra-Subnet Gradient Diversity]\label{gradDiv_c}
The intra-subnet gradient diversity across the devices belonging to subnet $\mathcal{S}_c$ is measured via non-negative constants $\delta_c,\zeta_c$ that satisfy
\begin{align} \label{eq:clust_div}
    \left\Vert\nabla F_i(\mathbf w)-\nabla\bar F_c(\mathbf w)\right\Vert
    \leq \delta_c+\zeta_c\Vert\mathbf w-\mathbf w^*\Vert,~\forall i\in\mathcal{S}_c,  ~\forall c, \mathbf w.
\end{align}
% where $\bar{\delta}_c=\sum\limits_{c=1}^N\varrho_c\delta_c$ and $\bar{\zeta}_c=\sum\limits_{c=1}^N\varrho_c\zeta_c$.
% \nm{This definition should go right before/after the global grad diversity. Why here?}
\end{definition}
% \nm{before proceding, explain in a couple of lines the meaning of Def 1 and 2 and how they ralate to "statistical heterogeneiy"}
The definitions of gradient diversity are used to measure the degree of heterogeneity across the local datasets of devices in federated learning settings, which may be non-i.i.d., and can impact the convergence performance. A high value of gradient diversity implies a large dissimilarity between the local datasets and the global data distributions.
We further define ratios $\omega=\frac{\zeta}{2\beta}\leq 1$ and $\omega_c=\frac{\zeta_c}{2\beta}\leq 1$, which will be important in our analysis. The above definitions are obtained using the $\beta$-smoothness of the loss functions. Specifically, for~\eqref{eq:11},
\begin{align}\label{eq:motivNewDiv}
    &\Vert\nabla\bar F_c(\mathbf w)-\nabla F(\mathbf w)\Vert
    = \Vert\nabla\bar F_c(\mathbf w)-\nabla\bar F_c(\mathbf w^*)+\nabla\bar F_c(\mathbf w^*)+\underbrace{\nabla F(\mathbf w^*)}_{=0}-\nabla F(\mathbf w)\Vert
    \nonumber \\&
    \leq \Vert\nabla\bar F_c(\mathbf w)-\nabla\bar F_c(\mathbf w^*)\Vert+\Vert\nabla\bar F_c(\mathbf w^*)\Vert + \Vert\nabla F(\mathbf w)-\nabla F(\mathbf w^*)\Vert
    \leq \delta+2\beta\Vert\mathbf w-\mathbf w^*\Vert,
\end{align}
% \nm{where $\delta=$...}
where $\Vert\nabla\bar F_c(\mathbf w^*)\Vert\leq\delta$. The bound in~\eqref{eq:motivNewDiv} demonstrates the fact that by applying the smoothness property of the functions $\bar F_c(\cdot)$ and $F(\cdot)$ along with imposing an upper bounded $\delta$ on the subnet gradients at the optimum, the definition of gradient diversity in~\eqref{eq:clust_div} is a general expression for~\eqref{eq:motivNewDiv},
which in turn results in~\eqref{eq:11} for $\frac{\zeta}{2\beta}\leq 1$. A Similar concept holds for~\eqref{eq:clust_div}.
% \nm{your argument here is not very clear.. you need to explain better the connectiong between 7 abd 9}
Using the same steps~\eqref{eq:clust_div} can be obtained, as we discuss in~\cite{lin2021timescale}.
These gradient diversity metrics extend 
the conventional definition of gradient diversity used in literature, e.g., as in \cite{wang2019adaptive}, which is a special case of~\eqref{eq:11} and~\eqref{eq:clust_div} with $\zeta=\zeta_c=0$, $\forall c$. Given that in FL settings, the local dataset of the devices may be extremely non-i.i.d., very large values of $\delta$ and $\delta_c$ in~\eqref{eq:11} and~\eqref{eq:clust_div} are required when $\zeta=\zeta_c=0$, which will, in turn, make the convergence bounds very loose and ineffective in describing the system behavior.
% \nm{do you have numerical results toback up this sttement?}
The addition of terms with coefficients $\zeta, \zeta_c$ in~\eqref{eq:11} and~\eqref{eq:clust_div} will lead to bounded
% \nm{what is reasonable?} 
values for $\delta$ and $\delta_c$, especially in the beginning of training when $\Vert \mathbf w-\mathbf w^*\Vert$ can take an arbitrary large value. At the same time, the addition of $\zeta, \zeta_c$ in~\eqref{eq:11},~\eqref{eq:clust_div} forms a coupling between the gradient diversity and the optimality gap $\Vert \mathbf w-\mathbf w^*\Vert$. As we will see, this coupling makes our convergence analysis unique from the current art and rather non-trivial.
 
% {\color{blue}
% The definition of gradient diversity captures the data diversity across clusters.
% This assumption is imposed since, in federated learning settings, the local dataset of the devices may be non identical and independently distributed (non-i.i.d), and the convergence performance may therefore depend on the degree of heterogeneity across the devices' local datasets, which gradient diversity aims to capture.
% In the extreme case in which the gradient diversity is zero, it means that the distribution of the data across the clusters is i.i.d, where for any given model parameter, the local gradients are equal to the global gradient (i.e., the gradient computed on local datasets at each cluster is equal to the gradient computed on the global dataset); on the other hand, if the gradient diversity is large, it implies a large dissimilarity between the local datasets and the global data distributions.
% We would like to remark that a version of gradient diversity has been proposed in the literature, such as the highly cited paper~\cite{wang2019adaptive}, then used in several state-of-the-art works, such as [R1]-[R4] (references provided at the end of this response) and~\cite{liu2020client}. 
% The definition of gradient diversity used in these papers reads as
% $$\Vert\nabla\hat F_c(\mathbf w)-\nabla F(\mathbf w)\Vert \leq \delta,\ \forall\mathbf w,\text{ (state-of-the-art)}$$
% which can be seen as
%  a special case of our definition
%  $$\Vert\nabla\hat F_c(\mathbf w)-\nabla F(\mathbf w)\Vert \leq \delta+\zeta\Vert\mathbf w-\mathbf w^*\Vert_2,\ \forall\mathbf w,\text{ (our version)}$$
%  with the parameter $\zeta = 0$. Therefore, the definition used in our paper is more general, and is characterized  by two parameters $\delta$ and $\zeta$.
% To motivate our more general definition of gradient diversity, let us look at the case of a linear regression problem (we have not included this discussion in the revised manuscript, due to space constraints). For simplicity, assume that each cluster only contains one node.
%  In this case, the local objective function of each node/cluster is given by $F_i=\frac{1}{2}\Vert\mathbf y_i-\mathbf X_i^T\mathbf w\Vert^2$, so that
% %  \nm{isnt the gradient diversity defined per cluster rather than per node? So why are you looking at nabla Fi below?}
% $$ 
% \nabla F_i(\mathbf w)=-\mathbf X_i(\mathbf y_i-\mathbf X_i^T\mathbf w)
% $$
% $$
% \nabla F(\mathbf w)=-\sum_{j}\rho_j\mathbf X_j(\mathbf y_j-\mathbf X_j^T\mathbf w)
% $$
% and
% $$
% \nabla F_i(\mathbf w)-\nabla F(\mathbf w)
% =
% \mathbf X_i\mathbf X_i^T
% [\sum_{j}\rho_j\mathbf X_j\mathbf X_j^T]^{-1}\sum_{j}\rho_j\mathbf X_j\mathbf y_j
% -\mathbf X_i\mathbf y_i
% +\left[\mathbf X_i\mathbf X_i^T
% -\sum_{j}\rho_j\mathbf X_j\mathbf X_j^T\right](\mathbf w-\mathbf w^*)
% $$
% where
% $$
% \mathbf w^*
% =[\sum_{j}\rho_j\mathbf X_j\mathbf X_j^T]^{-1}\sum_{j}\rho_j\mathbf X_j\mathbf y_j
% $$
% is the optimal point.
% It can be easily seen that 
% $\Vert\nabla F_i(\mathbf w)-\nabla F(\mathbf w)\Vert\to\infty$
% when $\Vert\mathbf w-\mathbf w^*\Vert\to\infty$ (i.e., large gap between the model and the optimal model), which
% may be problematic, especially at the beginning of model training, when the initial estimate may be far off from the optimal $\mathbf w^*$. This makes the classic definition $\Vert\nabla F_i(\mathbf w)-\nabla F(\mathbf w)\Vert\leq\delta$ impractical and quite pessimistic, since the bound built on top of it would suffer from very large values of $\delta$.
% On the other hand, we can see that 
% $$
% \Vert\nabla F_i(\mathbf w)-\nabla F(\mathbf w)\Vert_2
% \leq
% \Big\Vert\mathbf X_i\mathbf X_i^T
% [\sum_{j}\rho_j\mathbf X_j\mathbf X_j^T]^{-1}\sum_{j}\rho_j\mathbf X_j\mathbf y_j-\mathbf X_i\mathbf y_i\Big\Vert_2
% $$$$
% +\Big\Vert
% \Big[\mathbf X_i\mathbf X_i^T
% -\sum_{j}\rho_j\mathbf X_j\mathbf X_j^T\Big](\mathbf w-\mathbf w^*)\Big\Vert_2
% $$
% (via triangle inequality) and furthermore
% $$
% \Big\Vert
% \Big[\mathbf X_i\mathbf X_i^T
% -\sum_{j}\rho_j\mathbf X_j\mathbf X_j^T\Big](\mathbf w-\mathbf w^*)\Big\Vert_2
% \leq
% \sigma_{\max}\Big(\mathbf X_i\mathbf X_i^T
% -\sum_{j}\rho_j\mathbf X_j\mathbf X_j^T\Big)
% \Vert\mathbf w-\mathbf w^*\Vert_2
% $$
% where $\sigma_{\max}(\mathbf A)$ is the largest singular value of matrix $\mathbf A$. Therefore, our definition of gradient diversity can be accommodated by letting
% $$
% \delta=\max_i\Big\Vert\mathbf X_i\mathbf X_i^T
% [\sum_{j}\rho_j\mathbf X_j\mathbf X_j^T]^{-1}\sum_{j}\rho_j\mathbf X_j\mathbf y_j-\mathbf X_i\mathbf y_i\Big\Vert_2
% $$
% and
% $$
% \zeta=\max_i\sigma_{\max}\Big(\mathbf X_i\mathbf X_i^T
% -\sum_{j}\rho_j\mathbf X_j\mathbf X_j^T\Big).
% $$
% To summarize, this argument demonstrates that the conventional definition of gradient diversity used in the literature \emph{is not valid}  even in linear regression problems, which our definition can instead accommodate.
% In the revised manuscript, we generalized this argument to the general class of $\beta-$smooth functions, showing that for these functions (not only linear regression) we can \emph{always} define $\delta$ and $\zeta$ that satisfy the condition~\eqref{eq:11} in the revised manuscript (please refer to~\eqref{eq:motivNewDiv} in the revised manuscript for further explanations).
% }

% However, we observe that solely using $\delta$ on the right hand side of~\eqref{eq:11} can be troublesome since in that case $\delta$ can become prohibitively large,\footnote{This is especially true in the initial iterations of the model training, where $\Vert\mathbf w-\mathbf w^*\Vert$ can be arbitrary large.} reducing the usefulness of convergence bounds derived based on $\delta$ by making them too loose\nm{the sentence is quite intricate..}. To arrive at this definition, note that we can upper bound the gradient diversity using the triangle inequality as follows:

% where in the last step above we used the smoothness condition and upper bounded the cluster gradients at the optimal model as $\Vert\nabla\bar F_c(\mathbf w^*)\Vert \leq \delta$. This is a more practical assumption than $\Vert\nabla\bar F_c(\mathbf w)-\nabla F(\mathbf w)\Vert \leq \delta$ that must hold $\forall\mathbf w$.
% \nm{please reorganize, you talk about this issue in the previous paragraph:
% 1) show (8); 2) Explain the issue with prior literatur that relies on delta alone.}
%     % From \eqref{eq:motivNewDiv}, it also follows that $\zeta\leq 2\beta$. 
% We then introduce a ratio $\omega=\frac{\zeta}{2\beta}$, where $\omega\leq 1$ according to~\eqref{eq:motivNewDiv}. Considering $\zeta$ in~\eqref{eq:11} changes the dynamics of the convergence analysis and requires new techniques to obtain the convergence bounds, which are part of our contributions in this work.
% \nm{there is no mention of this contribution in the intro.}


% \begin{assumption} \label{eDiv}
% The weighted gradient diversity across the clusters is bounded as 
% \begin{align} \label{eq:11}
%     \frac{\sum\limits_{c=1}^N\varrho_c^{(k)}\left\Big\Vert e_{i_c}^{(\Gamma_c(t+1))}(t+1)\right\Big\Vert^2}{\left\Big\Vert e_{i_c}^{(\Gamma_c(t+1))}(t+1)\right\Big\Vert^2}\leq \delta,\ \forall \mathbf w,
% \end{align}
% where $\delta\geq 0$ is the dissimilarity parameter for all devices.
% \end{assumption} 

% Note that Assumption \ref{assum1} is satisfied in many popular machine learning models such as linear regression, logistic regression, squared-SVM, etc. 

% \begin{assumption} \label{def:2}
% $\nabla F(\mathbf w)$ is bounded on a closed and convex set $\mathcal{H}$ such that
% \begin{align} \label{eq:11}
%      &\Big\Vert \nabla F(\mathbf w)\Big\Vert \leq L,
% \end{align}
% where $L>0$ is a finite scalar.
% \end{assumption} 

% \subsubsection{Centralized gradient descent}
% Loss functions are typically minimized by gradient descent (GD) iterations. In a centralized case, where the global loss $F$ can be optimized directly, this is defined as
% \begin{align} \label{eq:3}
%     \mathbf w_{GD}(t)=\mathbf w_{GD}(t-1)-\eta_k\nabla F(\mathbf w_{GD}(t-1)),\
% \end{align}
% where $\mathbf w_{GD}(0)$ is an initialization,
%  $t\geq 1$ is the iteration index, and $\eta_k>0$ is the step size. If $F$ is convex and $\eta_k \leq \frac{1}{\beta}$, then gradient descent converges to the globally optimal solution $\mathbf w^*$ with rate $\mathcal O(1/T)$, where $T$ is the number of iterations \cite{Bubeck}.

% However, centralized gradient descent cannot be directly applied to the FL framework in Fig.\ref{fig2} since no device has direct access to all the data. 
% In addition, communication to the cloud is costly in terms of network resources, so the aggregation and synchronization processes are done only periodically.
% Finally, communication delay between edge and cloud is usually non-negligible, which we address next in developing the {\tt FedDelAvg} algorithm. 

\begin{figure*}[t]
\includegraphics[width=\textwidth]{image/Time_Elapse_Del.png}
\centering
\caption{Illustration of the timescale in {\tt DFL}. The time index $t$ represents local model updates through SGD, local aggregation, as well as global aggregations and synchronizations.}
\label{fig:twoTimeScale}
\vspace{-5mm}
\end{figure*}

\subsection{{\tt DFL}: Delay-Aware Federated Learning} \label{subsec:syst3}
\subsubsection{Overview and Rationale}
% \nm{whatdo you mean by prelim? Maybe "Overview:"?}
% {\tt DFL} performs a sequence of local model training intervals in-between aperiodic global aggregations. During each interval, the devices conduct local stochastic gradient descent (SGD) iterations and aperiodically synchronize their model parameters within their clusters by performing local aggregations via the edge servers.
In the context of {\tt DFL}, the process of training the ML model comprises of a sequence of local model training intervals that occur between two successive \textit{global synchronizations}, which are subsequently followed from \textit{global aggregations}. During each interval, the edge devices carry out stochastic gradient descent (SGD) iterations based on their local data to optimize their local loss functions $F_i(\cdot),~\forall i$. Additionally, the edge server conducts aperiodic \textit{local aggregations}\footnote{Note that in Fig.~\ref{fig:twoTimeScale}, the parameters $m_1$ and $m_2$ may not be equal. In fact, {\tt DFL} allows for aperiodic local aggregations during each local model training interval, the frequency of which will be later tuned in our control algorithms to mitigate network resource consumption.} to synchronize the local model parameters with the edge devices within its corresponding subnet. Aggregation refers to computing an \textit{aggregated model} by taking the weighted average of local models from edge devices. In contrast, synchronization refers to updating local models at edge devices using the aggregated model obtained after aggregation. On one hand, local aggregations are conducted within subnets through device-to-edge communication links, resulting in small-scale communication delays because edge servers are typically physically closer to the edge devices. On the other hand, global aggregations occur across the entire network through edge-to-cloud communication links, which typically have non-negligible propagation delays in the range of hundreds of milliseconds to several seconds, depending on network bandwidth~\cite{Lin}. This delay can significantly degrade learning performance in FL because the main server is often far from the edge devices. In this paper, we neglect the delay of local aggregations within subnets and focus on the global aggregation delay, assuming device-to-edge links have a much shorter range than edge-to-cloud links. Details of the {\tt DFL} procedure and the modeling of communication delay will be described in the following.

% In general, these delay components may vary between aggregation intervals due to congestion, interference, or other factors. 
% \nm{then, shouldnt it depend on c as well? Why is it the same across clusters?Note that, typically, the device-to-edge links are wireless links over short distances (of the order of XXXX). On the other hand, the edge-to-cloud links are constistuted of ....., which are affected by much larger communication delays. For this reason, in this paper....}
% We neglect the delay of local model aggregations within subnets as our focus is on the global aggregation delay; for the systems we consider, the device-to-edge links are much shorter range than the edge-to-cloud links.

\subsubsection{{\tt DFL} Model Training Procedure}\label{subsec:proc} In the following, we provide a high-level description of our methodology, which will be later formalized in the next subsection, i.e., Sec.~\ref{subsec:form2}.
We consider a slotted-time representation of a network where each edge device conducts a \textit{local model training iteration} via SGD at each time index $t=0,1,\cdots,T$. The time duration of $0$ to $T$ is partitioned into multiple \textit{local model training intervals} indexed by $k=0,1,\cdots,K-1$, each capturing time interval $\mathcal{T}_k = \{t_{k} + 1,...,t_{k+1}\}\subset \{0,1,\cdots,T\}$. The model training procedure at the devices starts with the server broadcasting the initial global model $\bar{\mathbf w}^{(0)}$ to all the devices at $t_0=0$, proceeding through a series of global aggregations.  Local model training intervals occur in between two consecutive global model synchronizations at time $t_k$ and $t_{k+1}$, during which the local models of devices are updated via SGD. 
% At global synchronization, as depicted in Fig.~\ref{fig:twoTimeScale}, the local models are synchronized with the global aggregated model computed during the previous global aggregation step. 
At each instance of global aggregation, the models of the devices are collected (i.e., they engage in uplink transmissions) and arrive at the server with a certain delay. The server will then aggregate the received models into a global model and broadcast it (i.e., engage in downlink transmission) to the devices, which involves another delay. In parallel, devices proceed with local updates before the global model arrives. Upon reception of the global model, devices synchronize their local models in a manner accounting for both uplink and downlink delays. The relationships between the timescales are  depicted in Fig.~\ref{fig:twoTimeScale}.

% \nm{We.. please proofread the paper! Tehre should be an automatic spelling check feature. Or download to TeXShop, it has that feature under Edit --> show spelling and grammar. } thus\nm{Grammar! We thus what??} 
The length of the $k$th local model training interval is denoted by $\tau_k = |\mathcal{T}_k|$. We have $t_k = \sum_{\ell=0}^{k-1}\tau_\ell$, $\forall k$, where $t_0 = 0$ and $t_K=T$. This allows
for varying the length of local model training intervals across global synchronizations (i.e., $\tau_{k} \neq \tau_{k'}$ for $k \neq k'$).
During the $k$th local model training interval, we define $\mathcal{T}^\mathsf{L}_{k,c} \subset \mathcal{T}_k$ as the set of time instances when the edge server pulls the models of devices in subnet $\mathcal{S}_c$ (i.e., devices engage in uplink transmissions) and performs local aggregations of their models. Local aggregation synchronizes the local models within the same subnet by computing their weighted average.
We next formalize the above procedure.
% We assume a local model training iteration via SGD occurs prior to any local aggregation and global synchronization at any given time $t$. 

% Global ML model training is carried out through a sequence of global aggregation
% , indexed by $k=0,1,\cdots,K-1$, where 
% % for each $k > 0$ the local models of all the edge devices are aggregated at the main server. 
% We assume that 
% global synchronization $k$ occurs at time $t=t_k$, and. 
% Let $\mathcal{T}_k = \{t_{k} + 1,...,t_{k+1}\}$  denote the set of time indices between synchronizations $k$ and $k+1$. Letting $\tau_k = |\mathcal{T}_k|$ denote the length of the $k$th local model training interval, we have $t_k = \sum_{\ell=0}^{k-1}\tau_\ell$, $\forall k$, where $t_0 = 0$ and $t_K=T$. In general, this system model allows varying the length of local model training interval $\tau_k$ across global synchronizations (i.e., $\tau_{k} \neq \tau_{k'}$ for $k \neq k'$).
% \nm{to me this looks like all tau need to be different from each other. Better say "This model allows possibly varying aggregation periods."}


% \nm{I dont think you are referring to Fig 3}
% \nm{at this point, it is still unclear to me the difdference between local model agg vs global vs local update. You need to be more clear in explaining these high level concepts. 
% At the end of this paragraph, the reader should have a pretty good high level idea of your model, and a roadmap with pointers that allows him/her to follow the details explained next.. I dont think you are doing that}
% % \nm{no mention of delay here?? }
% % \nm{the delay is one of the key modeling aspects of the paper but shows up only very late. You need to talk about delay in the "Overview"}
% \nm{You need to motivate more clearly. The connection between shorter range and neglecting some delays is not clear:
% "Note that, typically, the device-to-edge links are wireless links over short distances (of the order of XXXX). On the other hand, the edge-to-cloud links are constistuted of ....., which are affected by much larger communication delays. For this reason, in this paper...." Also, this shold go i nthe overview section not here.} 
% \nm{somewhere (maybe not here) you need to motivate why you only consider the delay for global model agg, but not for local model agg.}


% These operations are further formalized in  Sec.~\ref{subsec:syst3}.


% There are three main practical reasons for incorporating the local consensus procedure into the learning paradigm. First, local consensus can help further suppress any bias of device models to their local datasets, which is one of the main challenges faced in federated learning in environments where data may be non-i.i.d. across the network. Second, an iteration of local consensus can be expected to incur much lower device power consumption compared with the global aggregations, which require uplink transmissions to potentially far away aggregation points (e.g., from smartphone to base station) while local aggregations are performed over short ranges~\cite{hmila2019energy,dominic2020joint}. 
% This leads to less dependency on energy-intensive global aggregations (due to the requirement of uplink transmissions) for parameter synchronization, where the global aggregations can occur less frequently. 
% \nm{I got lost with the explanations in this paragraph}

% \nm{it is essential that this is explained clearly. I think it needs to be improved. The consensus is explained too late in the paragraph. I think you should follow a more logical order: SGD --> consensus --> synchronization --> repeat}
% We consider a set of discrete time indices $\mathcal{T} = \{1, 2, ...\}$ for model learning, and  use $t \in \mathcal{T}$ to index time. Global aggregation $k$ occurs at time $t_k \in \mathcal{T}$, and $\mathcal{T}_k = \{t_{k-1} + 1,...,t_k\}$  denotes the set of time indices between aggregations $k-1$ and $k$, referred to as the $k$th \textit{local model training interval}. With $\tau_k = |\mathcal{T}_k|$ as the length of the $k$th interval, we have $t_k = \sum_{\ell=1}^{k}\tau_\ell$ $\forall k$ and $t_0 = 0$. Since global aggregations are aperiodic, in general $\tau_{k} \neq \tau_{k'}$ for $k \neq k'$.

%As in federated learning, there will be $K$ \textit{global aggregations}, which we will index as $k = 1,...,K$. 


% Local model: Model at the edge server, Global model....

% The model computed by the server at the $k$th global aggregation is denoted as 
% % The $k$th global aggregation of model parameters computed at the server will be denoted as 
% $\bar{\mathbf w}^{(t_k)} \in \mathbb{R}^M$, which will be defined in \eqref{15}.


%At each global aggregation round $k$, each edge device $i$  performs successive local SGD iterations on its local model.  

\subsubsection{Formalizing {\tt DFL}}\label{subsec:form2}
Next, we formalize the local training, local model aggregations, and global model aggregation and synchronization. Through this process, we will also introduce new notions used for orchestrating
% \nm{concertizing?? Weird, Use different word: orchestrating?? } 
the formulation of the system. 
\begin{definition}[Conditional Expectation]\label{def:cond_E}
$\mathbb{E}_t(\cdot)$ represents the conditional expectation conditioned on $\mathcal F_t$, where $\mathcal F_t$ denotes the $\sigma$-algebra generated by all random sampling up to but excluding, $t$.
% where $\bar{\delta}_c=\sum\limits_{c=1}^N\varrho_c\delta_c$ and $\bar{\zeta}_c=\sum\limits_{c=1}^N\varrho_c\zeta_c$.
% \nm{This definition should go right before/after the global grad diversity. Why here?}
\end{definition}

\textbf{Local SGD iterations}: 
% \hl{For two consecutive time instances $t-1,t\in \mathcal{T}_k$}\nm{? Why consecutive? I dont understand}, the local model at device $i$ is updated using an SGD iteration.
% % Let   $\mathbf w_i^{(t-1)} \in \mathbb{R}^M$ at time $t-1\in \mathcal{T}_k$. 
% Specifically, 
% \nm{remove previous line (confusing) jsut start here}
At $t \in \mathcal{T}_k$, device $i$ randomly samples a mini-batch $\xi_i^{(t)}$ of datapoints from its local dataset $\mathcal D_i$, and uses it to calculate the unbiased \textit{stochastic gradient estimate} using its previous local model $\mathbf w_i^{(t)}$ as
\vspace{-2mm}
\begin{align}\label{eq:SGD} 
    \widehat{\mathbf g}_{i}^{(t)}=\frac{1}{\vert\xi_i^{(t)}\vert}\sum\limits_{(\mathbf x,y)\in\xi_i^{(t)}}
    \nabla\ell(\mathbf x,y;\mathbf w_i^{(t)}).
\end{align}
% which is an unbiased estimate of the local gradient, i.e., $\mathbb E_{t}[\widehat{\mathbf g}_{i}^{(t)}]=\nabla F_i(\mathbf w_i^{(t)})$\nm{what about its variance?}. 
% \nm{I think this assumption should be moved to conv analysis sectino}

\textbf{Tentative Local Model}: Using the gradient estimate $\widehat{\mathbf g}_{i}^{(t-1)}$, each device computes its \textit{tentative local model} $ {\widetilde{\mathbf{w}}}_i^{(t)}$ as
\vspace{-3.5mm}
\begin{align} \label{8}
    {\widetilde{\mathbf{w}}}_i^{(t)} = 
           \mathbf w_i^{(t-1)}-\eta_{k} \widehat{\mathbf g}_{i}^{(t-1)},~t\in\mathcal T_k, 
\end{align}
where $\eta_{k} > 0$ denotes the step size. We denote this tentative because it is an intermediate calculation prior to any potential aggregation. Specifically, 
based on ${\widetilde{\mathbf{w}}}_i^{(t)}$, the \textit{updated local model} $\mathbf{w}_i^{(t)}$ is computed either through setting it to ${\widetilde{\mathbf{w}}}_i^{(t)}$ or through a local aggregation, discussed next.

\textbf{Updated Local Model}: 
%\nm{simplify notation: you can say that the discussion refers to a reference round k, so you drop the dependence on k but should kkep in mind that v may vary with k..}
%At each time $t \in \mathcal{T}_k$, each cluster may engage in local model aggregation if $t\in \mathcal T^\mathsf{L}_{k,c}$.
% The decision of whether to engage in local aggregation at time $t$ -- and if so, how many iterations of this process to run -- will be developed in Sec.~\ref{Sec:controlAlg} based on a performance-efficiency tradeoff optimization. 
% Let $\mathcal T_c^{k}$
% % \nm{what is ell? You were talking about k before, please be consistent} 
% denote the set of time indices device $i\in\mathcal S_c$ in cluster $c$ engage in local aggregation and $\Gamma_c$ denote the number of local aggregations performed within a local model training interval. 
At time $t$, if subnet $\mathcal{S}_c$ does not conduct a local aggregation, i.e., $t\in \mathcal T_k\setminus \mathcal T^\mathsf{L}_{k,c}$, the final updated local model is obtained based on the the conventional rule ${{\mathbf{w}}}_i^{(t)} = {\widetilde{\mathbf{w}}}_i^{(t)}$ from \eqref{8}. Otherwise, i.e., if $t\in \mathcal T^\mathsf{L}_{k,c}$, then each device $i$ in subnet $\mathcal{S}_c$ transmits its tentative updated local model $\tilde{\mathbf w}_i^{(t)}$ to edge server 
$n_c$ which computes the \textit{instantaneous aggregated local model} as follows:
\vspace{-2mm}
\begin{equation}
    \label{eq:local_agg}
    \bar{\mathbf w}_c^{(t)} = \sum_{i\in\mathcal{S}_c} \rho_{i,c} \tilde{\mathbf w}_i^{(t)}.
\end{equation}
The edge server $n_c$ then broadcasts $\bar{\mathbf w}_c^{(t)}$ across its subnet. The devices subsequently obtain their final updated models with local aggregation as
%\begin{align} \label{eq:loc_agg}
    ${{\mathbf{w}}}_i^{(t)} =\bar{\mathbf w}_c^{(t)}, i\in\mathcal{S}_c$.
%\end{align}
% where $\bar{\mathbf w}_c^{(t)} = \sum_{i\in\mathcal{S}_c} \rho_{i,c} \tilde{\mathbf w}_i^{(t)}$ is the average of the local models in the cluster. 

% In particular, letting $t'$ index the rounds, each node $i \in \mathcal{S}_c$ carries out the following for $t'=0,...,\Gamma^{(t)}_{{c}}-1$, where $\Gamma^{(t)}_{{c}}$ is the total number of rounds for cluster $c$ at time $t$:
%During each local model training interval, the devices aperiodically form consensus through engaging in cooperative D2D communications, determining the time of occurrence of which and the number of D2D communications performed are parts of design elaborated in Sec.~\ref{Sec:controlAlg}. If at time $t\in\mathcal T_k$ the devices do not form consensus, we have the conventional model update rule ${{\mathbf{w}}}_i^{(t)} = {\widetilde{\mathbf{w}}}_i^{(t)}=
%           \mathbf w_i^{(t-1)}-\eta^{(k)} \widehat{\mathbf g}_{i}^{(t-1)}$. Upon forming consensus at time $t$, the devices perform multiple \textit{rounds} of D2D,
        %   \nm{does this happen within t? Why are the consensus updates not counted at the same timescale as local SGD? How do you make sure that the clusters are synced?} 
%        each of which consists of parameter transfers between neighboring devices. In particular, each node $i\in\mathcal{S}_c$ conducts the following iteration for $t'=0,\cdots,\Gamma^{(t)}_{{c}}-1$, where $\Gamma^{(t)}_{{c}}$ denotes the rounds of D2D in the respective cluster:
%     \begin{equation}\label{eq:ConsCenter}
%       \textbf{z}_{i}^{(t'+1)}= v_{i,i} \textbf{z}_{i}^{(t')}+ \sum_{j\in \mathcal{N}_i} v_{i,j}\textbf{z}_{j}^{(t')},
%   \end{equation}

% \vspace{-0.1in}
% \noindent 
% where $\textbf{z}_{i}^{(0)}=\widetilde{\mathbf{w}}_i^{(t)}$ is the node's intermediate local model from \eqref{8}, and $v_{i,j}\geq 0$, $\forall i,j$ is the consensus weight that node $i$ applies to the vector received from $j$. At the end of this process, node $i$ takes $\mathbf w_i^{(t)} = \textbf{z}_{i}^{(\Gamma^{(t)}_{{c}})}$ as its updated local model.
%\nm{you need to make an effor to simplify notation.. these nested superscripts are really unappealing..}

% The index $t'$ corresponds to the second timescale in {\tt TT-HF}, referring to the consensus process, as opposed to the index $t$ which captures the time elapsed by the local gradient iterations. Fig.~\ref{fig:twoTimeScale} illustrates these two timescales, where at certain local iterations $t$ the consensus process $t'$ is run.

% To analyze this update process, we will find it convenient to express the consensus formation in matrix form. Let $\widetilde{\mathbf{W}}^{(t)}_{{c}} \in \mathbb{R}^{s_c \times M}$ denote the matrix of intermediate updated local models of the $s_c$ nodes in cluster $\mathcal{S}_c$ prior to consensus at time $t\in\mathcal{T}_k$, where the $i$-th row of $\widetilde{\mathbf{W}}^{(t)}_{{c}}$ corresponds to device $i$'s intermediate local model $\widetilde{{\mathbf{w}}}_i^{(t)}$. The evolution of the devices' parameters can be compressed into the following one step matrix update:
% %\nm{notation!}
%   \begin{equation}\label{eq:consensus}
%       \mathbf{W}^{(t)}_{{c}}= \left(\mathbf{V}_{{c}}\right)^{\Gamma^{(t)}_{{c}}} \widetilde{\mathbf{W}}^{(t)}_{{c}}, ~t\in\mathcal T_k,
%   \end{equation}
% where ${\mathbf{W}}^{(t)}_{{c}}$ denotes the matrix of updated device parameters after the consensus, $\Gamma^{(t)}_{{c}}$ denotes the rounds of D2D consensus in the cluster, and $\mathbf{V}_{{c}}=[v_{i,j}]_{1\leq i,j\leq s_c} \in \mathbb{R}^{s_c \times s_c}$ denotes the \textit{consensus matrix}, which we characterize further below. The $i$-th row of ${\mathbf{W}}^{(t)}_{{c}}$ corresponds to device $i$'s local update ${{\mathbf{w}}}_i^{(t)}$, which is then used in~\eqref{eq:SGD} to calculate the gradient estimate for the next local update. For the times $t \in \mathcal T_k$ where consensus is not used, we set $\Gamma^{(t)}_{{c}}=0$, implying $\mathbf{W}^{(t)}_{{c}}= \widetilde{\mathbf{W}}^{(t)}_{{c}}$ so that devices use their individual gradient updates.
%\nm{we should make a rule that we should NEVER have nested subscript/superscripts. It is really bad to see. I can bet that the reviewers will complain a lot about the notation..}

\iffalse
\textbf{Consensus characteristics}: The consensus matrix $\mathbf{V}_{{c}}$ is assumed to be fixed during each local model training interval $\mathcal{T}_k$, though it can change from interval to interval (e.g., due to device mobility). It can be constructed in several ways based on the cluster topology $G_c$. From distributed consensus literature, one common choice for~\eqref{eq:ConsCenter} is $\textbf{z}_{i}^{(t'+1)} = \textbf{z}_{i}^{(t')}+d_{{c}}\sum_{j\in \mathcal{N}_i} (\textbf{z}_{j}^{(t')}-\textbf{z}_{i}^{(t')})$, where $0 < d_{{c}} < 1 / D_{{c}}$ and $D_{{c}}$ denotes the maximum degree among the nodes in $G_{{c}}$~\cite{xiao2004fast}. This results in a consensus matrix for the cluster with $v_{i,i}=1-|\mathcal{N}_i|d_{{c}}$ and $v_{i,j}=d_{{c}}, i \neq j$. 
% Our theoretical results in Sec. \ref{sec:convAnalysis} will show how properties of $\mathbf{V}_{{c}}$, particularly the spectral radius $\rho(\cdot)$ of the matrix $\mathbf{V}_{{c}}-\frac{\textbf{1} \textbf{1}^\top}{s_c}$, affect the convergence rate of {\tt TT-HF}. 
We consider the family of consensus matrices that have the following standard properties~\cite{xiao2004fast}:

\begin{assumption}\label{assump:cons}
The consensus matrix $\mathbf{V}_{{c}}$ satisfies the following conditions: (i) $\left(\mathbf{V}_{{c}}\right)_{m,n}=0~~\textrm{if}~~ \left({m},{n}\right) \notin \mathcal{E}_{{c}}$, i.e., nodes only receive from their neighbors; (ii) $\mathbf{V}_{{c}}\textbf{1} = \textbf{1}$, i.e., row stochasticity; (iii) $\mathbf{V}_{{c}} = {\mathbf{V}_{{c}}}^\top$, i.e., symmetry; and (iv)~$\rho \big(\mathbf{V}_{{c}}-\frac{\textbf{1} \textbf{1}^\top}{s_c} \big) < 1$, i.e., the largest eigenvalue of $\mathbf{V}_{{c}}-\frac{\textbf{1} \textbf{1}^\top}{s_c}$ has magnitude $<1$. %where $\textbf{1}$ is the vector of 1s and \underline{$\rho(\mathbf{A})$ is the spectral radius of matrix $\mathbf{A}$}.
\end{assumption}
\fi

Based on the above described procedure, we can obtain the following general local model update rule at each device $i\in\mathcal S_c$:
\vspace{-2mm}
\begin{align} \label{eq:w_i-gen}
    \mathbf w_i^{(t)} = (1-\Theta_c^{(t)})\widetilde{\mathbf{w}}_i^{(t)}+\Theta_c^{(t)}\bar{\mathbf w}_c^{(t)},~\forall t \in \mathcal T_k,
\end{align}
\vspace{-0mm}
where ${\Theta_c^{(t)}}$ is the indicator of local model aggregation, defined as
\vspace{-0mm}
\begin{align} \label{eq:the}
        {\Theta_c^{(t)}} = 
        \begin{cases}
            0  ,& t\in \mathcal T_k\setminus \mathcal T^\mathsf{L}_{k,c} \\
            1 ,& t\in \mathcal T^\mathsf{L}_{k,c}.
        \end{cases}
\end{align} 
% We next introduce a definition of the divergence across intermediate updated local models, which we can use to derive an upper bound on the cluster deviation error:
% \nm{?? what do you mean}
% \nm{since you are "hiding" the consensus steps within the time index $t$, one may argue that you are not really capturing these time constraints..}


\begin{algorithm}[t]
\scriptsize 
\SetAlgoLined
\caption{Delay-aware federated learning {\tt DFL} with set control parameters.} \label{DFL}
% \KwResult{Write here the result }
\KwIn{Length of training $T$, number of global aggregations $K$, local aggregation instances $\mathcal T^\mathsf{L}_{k,c}, c=1,...,N$, length of local model training intervals $\tau_k,~\forall k$, combiner weights $\alpha_k,~\forall k$, learning rates $\eta_k,~\forall k$, minibatch sizes $\vert\xi_i^{(t)}\vert,~ i\in\mathcal{I},~\forall t$} 
\KwOut{Final global model $\bar{\mathbf w}^{(T)}$}
 // Initialization by the server \\
 Initialize $\bar{\mathbf w}^{(0)}$ and broadcast it across edge devices, resulting in $\mathbf w_i^{(0)}=\bar{\mathbf w}^{(0)}$, $i\in\mathcal I$.
%  \nm{what do you mean by "sampled devices"? Are these selected randomly? I dont think so, since the local server is typically a BS (you also mentioned earlier).}
 \\
 \For{$k=0:K-1$}{
     \For{$t=t_{k}+1:t_{k+1}$}{
        \For(// Procedure at each subnet $\mathcal{S}_c$){$c=1:N$}
        {
         % // Procedure at each cluster $\mathcal{S}_c$  \\
         Local SGD update 
        %  ~\eqref{eq:SGD} and~\eqref{8}  to obtain the tentative updated local model~$\widetilde{\mathbf{w}}_i^{(t)}$.\\
         with:
          $\widetilde{\mathbf w}_i^{(t)} = 
          \mathbf w_i^{(t-1)}-\eta_{t-1} \widehat{\mathbf g}_{i}^{(t-1)}$\; 
        \uIf{$t\in\mathcal T^\mathsf{L}_{k,c}$}{
        Local model aggregation with:
        % ~\eqref{eq:local_agg}, resulting in the updated local model $\bar{\mathbf{w}}_c^{(t)}$ for each device $i\in\mathcal{S}_c$.
        $\bar{\mathbf w}_c^{(t)} = \sum_{i\in\mathcal{S}_c} \rho_{i,c} \tilde{\mathbf w}_i^{(t)}$ and $\mathbf w_i^{(t)}=\bar{\mathbf w}_c^{(t)}$\;
        }
        \Else{
        $\mathbf{w}_i^{(t)} = \widetilde{\mathbf{w}}_i^{(t)}$.
        }
        % on $\widetilde{\mathbf w}_i^{(t)}$ as:\\
        % $\textbf{z}_{i}^{(0)}=\widetilde{\mathbf{w}}_i^{(t)}$\;
        % \For{$t'=0:\Gamma^{(t)}_{{c}}-1$}{
        %     $\textbf{z}_{i}^{(t'+1)}= v^{(k)}_{i,i} \textbf{z}_{i}^{(t')}+\hspace{-2mm} \sum_{j\in \mathcal{N}^{(k)}_i} v^{(k)}_{i,j}\textbf{z}_{j}^{(t')}$
        % }
        % $\mathbf w_i^{(t)} = \textbf{z}_{i}^{(\Gamma^{(t)}_{{C}})}$\; 
      }
      \uIf{$t=t_{k+1}-\Delta_k$}{
      % // Procedure at each cluster $\mathcal{S}_c$ \\
      \For(// Procedure at each subnet $\mathcal{S}_c$){$c=1:N$}{
      Local model aggregation: %   ~\eqref{eq:local_agg}.\\
      $\bar{\mathbf w}_c^{(t)} = \sum_{i\in\mathcal{S}_c} \rho_{i,c} \tilde{\mathbf w}_i^{(t)}$ and uplink transmission.\\
      }
          }
      \uElseIf( // Procedure at main server){$t=t_{k+1}-\Delta_k^{\mathsf{D}}$}{
      % // Procedure at main server \\
      Global model aggregation: $\bar{\mathbf w}^{(t_{k+1}-\Delta_k)}= \sum\limits_{c=1}^N \varrho_c \bar{\mathbf w}_{c}^{(t_{k+1}-\Delta_k)}$ and downlink broadcast.\\}
      \ElseIf( // Procedure at each cluster $\mathcal{S}_c$){$t=t_{k+1}$ and $t\neq T$}{
      % // Procedure at each cluster $\mathcal{S}_c$ \\
    %   Estimate $\delta$\;
     Local-Global model combiner:
     $\mathbf w_i^{(t)} = (1-\alpha_k) \bar{\mathbf{w}}^{(t-\Delta_k)}
    +\alpha_k\left((1-\Theta_c^{(t)})\widetilde{\mathbf{w}}_i^{(t)}+\Theta_c^{(t)}\sum_{j\in\mathcal{S}_c} \rho_{j,c} \widetilde{\mathbf w}_j^{(t)}\right)$.\\
    % Each device synchronize its local model to $\mathbf w_i^{(t)}=\widehat{\mathbf w}_i^{(t)}$.
    %  update rule~\eqref{eq:aggr_alpha} to synchronize its local model with the received global model.
      }
    %  \uIf{$t=T$}{
    %   // Procedure at each cluster $\mathcal{S}_c$ \\
    %  Local-Global model combiner:
    %  $\mathbf w_i^{(t)} = (1-\alpha_k) \bar{\mathbf{w}}^{(t-\Delta_k)}
    % +\alpha_k\left(\Theta_c^{(t)}\widetilde{\mathbf{w}}_i^{(t)}+(1-\Theta_c^{(t)})\sum_{j\in\mathcal{S}_c} \rho_{j,c} \widetilde{\mathbf w}_j^{(t)}\right)$ and uplink transmission.\\
    %   }
    %  \ElseIf{$t=T+\Delta_k^{\mathsf{U}}$}{
    %       // Procedure at main server \\
    %       Global model aggregation: $\bar{\mathbf w}^{(T)}=\alpha_K\bar{\mathbf{w}}^{(T-\Delta_K)}+(1-\alpha_K)\sum_{c=1}^N\varrho_c\sum_{j\in\mathcal{S}_c} \rho_{j,c} \mathbf w_j^{(T)}$ and downlink broadcast.}
 }
}
\end{algorithm}
% {\color{red} 
% \begin{definition}\label{def:clustDiv}
% The upper bound of the divergence of intermediate updated local model parameters in cluster $\mathcal{S}_c$ at time $t\in\mathcal{T}_k$, denoted by $\Upsilon^{(t)}_{{c}}$, is defined as follows: \begin{equation}\label{eq:Updef}
%   \big\Vert \widetilde{\mathbf{w}}^{(t)}_{{i}} -\widetilde{\mathbf{w}}^{(t)}_{{j}} \big\Vert \leq \Upsilon^{(t)}_{{c}} ,~ \forall {i},{j}\in\mathcal{S}_c.
% \end{equation} 
% % where $(.)_z$ denotes the $z$-th element of the indexed vector.
% \end{definition}
% }


% \begin{lemma}\label{lemma:cons}
% The cluster deviation error $\mathbf e_i^{(t)}$ is upper-bounded as:
% \begin{equation}
%   \Vert \mathbf e_i^{(t)} \Vert \leq \Theta_c^{(t)}{\Upsilon^{(t)}_{{c}}},~ \forall i\in \mathcal{S}_c.
% \end{equation}
% % where each $\lambda_{{c}}$ is a constant such that $1 > \lambda_{{c}} \geq \rho \big(\mathbf{V}_{{c}}-\frac{\textbf{1} \textbf{1}^\top}{s_c} \big)$.
% \end{lemma}

\iffalse
\begin{skproof}
  Let matrix $\overline{\mathbf{W}}^{(t)}_{{c}}$ contain the average of each model parameter $q = 1,...,M$ across the devices in cluster $c$:
  $
      \big[\overline{\mathbf{W}}^{(t)}_{{c}}\big]_q=\frac{\mathbf 1_{s_c} \mathbf 1_{s_c}^\top\left[\widetilde{\mathbf{W}}^{(t)}_{{c}}\right]_q}{s_c},~\forall q
  $, where $[\cdot]_q$ denotes the $q$-th column.
  We then define vector $\mathbf r$ as 
  \begin{align} \label{eq:r}
      \mathbf r= \big[\widetilde{\mathbf{W}}^{(t)}_{{c}}\big]_q-\big[\overline{\mathbf{W}}^{(t)}_{{c}}\big]_q,
  \end{align}
  which implies $\langle\mathbf 1_{s_c},\mathbf r\rangle=0$. 
  Using Assumption \ref{assump:cons}, we bound each element of the cluster deviation error as follows:
  \begin{align} \label{eq:consensus-bound-1}
      &\Vert(\mathbf e_{i}^{(t)})_q\Vert\leq
      (\lambda_{{c}})^{\Gamma^{(t)}_{{c}}} \Vert\mathbf r\Vert,~\forall i\in\mathcal S_c,~1\leq q\leq M.
  \end{align}
  By Definition \ref{def:clustDiv}, we have
  $
      \Vert(\mathbf r)_z\Vert
       \leq
       \frac{s_c-1}{s_c}\Upsilon^{(t)}_{{c}},~ i\in\mathcal S_c
  $, which implies
$
      \Vert\mathbf r\Vert\leq (s_c-1)\Upsilon^{(t)}_{{c}}\leq s_c\Upsilon^{(t)}_{{c}}
$.
  Performing some algebraic manipulations concludes the proof. For the complete proof refer to Appendix \ref{app:consensus-error}.
\end{skproof}
\fi

% This second case would be impractical, however, due to energy and delay considerations, which emphasizes the tradeoffs our adaptive control algorithm in Sec. \ref{Sec:controlAlg} must optimize over in tuning $\Gamma_c^{(t)}$.

\textbf{Communication Delay and Global Model Aggregations}: 
Recall that one of the contributions of this work is accounting for the impact of communication delay from edge to the cloud on model training. During global aggregation $k$, the upstream communication delay of local models from devices to the main server is denoted as $\Delta^\mathsf{U}_k$, while the downstream delay of the global model to edge devices is denoted as $\Delta^\mathsf{D}_k$. We then define the \textit{round-trip communication delay} as the time taken from when the edge devices send their local models to the edge servers (which forward them to the main server) until each device completes its local model synchronization based on the received global model, expressed as $\Delta_k=\Delta^\mathsf{U}_k+\Delta^\mathsf{D}_k$, assuming $0\leq \Delta_k \leq \tau_k-1,~\forall k$. 
% In other words, edge devices conduct $\Delta_k$ local model updates according to~\eqref{eq:w_i-gen} from when they send their local models for global aggregation to when they receive a new global model. 
% \nm{reorganize: defined DeltaU DeltaD first, and then Deltak as the sum.}
% \nm{you are not explaining the delay model well here..please restructure}

In conducting global aggregations, to account for the round-trip delay, devices send their local models to the edge servers $\Delta_k$ time instances prior to the completion of each local model training interval $\mathcal{T}_k$, i.e., at $t=t_{k+1}-\Delta_k \in \mathcal{T}_k$. Concurrently, these devices carry out an additional $\Delta_k$ local updates using SGD/local aggregation before they receive the updated global model.
% \nm{in the meantime, do they keep learning? need to specify here}
% {\color{blue}We leveraged the device-to-main server communication delay observed in the most recent local model training interval as an approximation for the current training interval, aiming to reduce the overhead that comes with estimating real-time round-trip delay.}
We assume that $\Delta_k$ can be reasonably estimated, e.g., from round-trip delays observed in recent time periods\footnote{Our methodology is not tied to any particular procedure for estimating $\Delta_k$. In practice, an approach based on recent time periods is motivated by Transport Control Protocol (TCP) analysis, where round trip times have been noted to hold relatively stable over time ranges up to tens of seconds \cite{hao2002RTT}. For our experimental settings in Sec.~\ref{sec:experiments}, we find that our methodology can tolerate delay estimation errors of $20$ ms with only a $1\%$ drop in the resulting trained model accuracy. Using a metric of delay variations being within $0.5$-$1$ ms for a round-trip delay of $50$ ms~\cite{James2007TV}, this estimation approach could handle networks with round-trip times on the order of seconds, which would be rather large, while only experiencing marginal degredations.}. The edge servers then forward the locally aggregated models $\bar{\mathbf w}_{c}^{(t)}$ to the main server with the delay of $\Delta^{\mathsf{U}}_k$. The main server builds the global model based on the \textit{stale} local models as
        %   . Formally, at time index $t = t_k$, the main server samples device $n_c^{(k)} \in \mathcal{S}_c^{(k)}$ for each cluster $c$, and updates the global model as follows:
% \begin{align} \label{15}
%     \bar{\mathbf w}^{(t)} &= 
%           \sum\limits_{c=1}^N \varrho_c \bar{\mathbf w}_{c}^{(t-\Delta^{\mathsf{U}})}, \;\; t=t_k-\Delta^{\mathsf{D}}, k=1,2,...,
% \end{align} 
\begin{align} \label{15}
    \bar{\mathbf w}^{(t)} &= 
          \sum\limits_{c=1}^N \varrho_c \bar{\mathbf w}_{c}^{(t)}, \;\; t=t_{k+1}-\Delta_k, k=0,1,...,
\end{align}
% \chris{shouldn't (15) be $\bar{\mathbf w}^{(t+\Delta_U)}$?}
where $\bar{\mathbf w}^{(t)}=\sum\limits_{c=1}^N \varrho_c \bar{\mathbf w}_{c}^{(t)}$ is the global average of local models for all $t$ and $\varrho_c$ is defined in~\eqref{eq:2}. Note that the computation of the global model $\bar{\mathbf w}^{(t_{k+1}-\Delta_k)}$ is performed by the main server at $t=t_{k+1}-\Delta^{\mathsf{D}}_k$. After the computation at the main server, the global model $\bar{\mathbf w}^{(t_{k+1}-\Delta_k)}$ is then broadcast and received at the devices with the delay of $\Delta^{\mathsf{D}}_k$. The devices then synchronize their local models at time $t_{k+1}$ via a linear local-global model combiner, as follows.


% the global model $\mathbf{w}$ will be updated based on the local model updates in each cluster at times $t\in\{t_k-\Delta,\forall k\}$.
% These local models are received at the server with delay $\Delta/2$ to compute $\bar{\mathbf w}^{(t)}$; $\bar{\mathbf w}^{(t)}$ is then sent back to the nodes with delay $\Delta/2$. so that each edge device receives it at time $t_k$. 


% Referring to Fig.~\ref{fig2}, at each instance of global aggregation $k$, each edge server $n_c$ first aggregates its associated devices' local models at time $t=t_k-\Delta$ and obtain $\bar{\mathbf w}_{c}^{(t)}$, the main server then aggregates $\bar{\mathbf w}_{c}^{(t)}$ and obtains the global model $\bar{\mathbf w}^{(t)}$ at $t+\Delta/2=t_k-\Delta/2$. Finally, the server broadcasts $\bar{\mathbf w}^{(t)}$ across the edge servers, which is received at the edge devices at time $t+\Delta=t_k$, which is used to synchronize the local models.


% Finally, $\bar{\mathbf w}^{(t)}$ is then sent back to the devices at $t=t_k$ (with delay $\Delta/2$).
% This sampling technique is introduced to reduce the uplink communication cost, by a factor of the cluster sizes, due to the fact that local models have already been (imperfectly) aggregated through consensus formation \cite{hosseinalipour2020federated}. 
% To track the global model variations, we introduce the instantaneous global model $\bar{\mathbf w}^{(t)}= 
%           \sum\limits_{c=1}^N \varrho_c \bar{\mathbf w}_{c}^{(t)}$, where $\bar{\mathbf w}^{(t)}$ is realized at the server at the instance of the global aggregations as
%         %   . Formally, at time index $t = t_k$, the main server samples device $n_c^{(k)} \in \mathcal{S}_c^{(k)}$ for each cluster $c$, and updates the global model as follows:
% \begin{align} \label{15}
%     \bar{\mathbf w}^{(t)} &= 
%           \sum\limits_{c=1}^N \varrho_c \bar{\mathbf w}_{c}^{(t)}, \;\; t=t_k-\Delta, k=1,2,...
% \end{align}


\textbf{Linear Local-Global Model Combiner (Global Synchronization)}: The conventional rule for updating the local model in FL is to synchronize based on the global model, i.e., $\mathbf w_i^{(t_{k})} = \bar{\mathbf w}^{(t_{k+1}-\Delta_k)}$. However, in our setting, the devices have conducted $\Delta_k$ more local updates before receiving the global model, which this standard synchronization would effectively neglect, resulting in synchronizing the local model with stale global model $\bar{\mathbf w}^{(t_{k+1}-\Delta_k)}$.
% \nm{and also, $\bar{\mathbf w}^{(t_{k}-\Delta_k)}$ has become stale} 
To address this, we propose a linear global-local model combiner scheme in which each edge device $i$ updates its local model based on the received global model at time $t_{k+1}$ as
%and thus simply synchronizing their local models with the global model ignores all the further local model steps they have carried out
% the global model is a \textit{stale}
% The global model is then broadcast by the main server to all of the edge devices at $t=t_k$ with communication delay of $\Delta/2$
% \nm{you mean, with delay Delta/2, which are then received at time tk? Please be more clear.}, which override their local models at time $t_k$: 
\begin{align} \label{eq:aggr_alpha}
    \hspace{-0.156in}
    \mathbf w_i^{(t_{k+1})} \hspace{-0.05in}&= \hspace{-0.02in} (1-\alpha_k) \bar{\mathbf{w}}^{(t_{k+1}-\Delta_k)}
    +\alpha_k\left((1-\Theta_c^{(t_{k+1})})\widetilde{\mathbf{w}}_i^{(t_{k+1})}+\Theta_c^{(t_{k+1})}\sum_{j\in\mathcal{S}_c} \rho_{j,c} \widetilde{\mathbf w}_j^{(t_{k+1})}\right)\hspace{-0.05in}, \hspace{-0.08in}~\forall i \in \mathcal{I},
\end{align}
where $\alpha_k \in [0,1)$ is the \textit{combiner weight} employed in update iteration $k$, with $\alpha_k=0$ corresponding to the conventional synchronization rule. Devices will then commence their local SGD iterations over $\mathcal{T}_{k+1}$ initialized based on~\eqref{eq:aggr_alpha}.
% \nm{why not just using w without hat above? No need to introduce new notation..just compare this with eq 13 done for t neq tk} 
Intuitively, $\alpha_k$ should be carefully tuned to compensate for the tradeoff between the \textit{staleness} of the global model and the potential for local model \textit{overfitting} to each device's dataset. In particular, when we have a larger delay $\Delta_k$, $\alpha_k$ 
% \nm{is expected to be smaller since...}
is expected to be larger since the global model received will be based on more outdated local models. In Sec.~\ref{Sec:controlAlg}, we will develop a control algorithm (i.e., Algorithm~\ref{GT}) to determine $\tau_k$ and $\alpha_k$ given round-trip delay $\Delta_k$ for each local model training interval.  
% \nm{mention t he control algo to tune alpha}

\iffalse
\textbf{Final Global Deployed Model:} At the end of training, the main server will construct the final trained model for the system, which should incorporate (i) the last global aggregation (from $t = t_K - \Delta_{K-1}=T-\Delta_{K-1}$) and (ii) the most recent local models (from $t = T$). Thus, it applies the local-global combiner scheme based on $\bar{\mathbf{w}}^{(T-\Delta_{K-1})}$ and devices' current local models, resulting in:
% \nm{this is confusing.. I think you are overcomplicating things, Why not just using $\bar{\mathbf{w}}^{(T-\Delta_K)}$?? Also, the second portion is going to arrive late because of delays. Delays dont seem to be properly accounted for here..}
\begin{align} \label{eq:sync_alpha}
    \bar{\mathbf w}^{(T)} &= 
    \sum_{c=1}^N\varrho_c\sum_{j\in\mathcal{S}_c} \rho_{j,c}\left[(1-\alpha_K)\bar{\mathbf{w}}^{(T-\Delta_{K-1})}
    +\alpha_K\left(\Theta_c^{(T)}\widetilde{\mathbf{w}}_i^{(T)}+(1-\Theta_c^{(T)})\sum_{j'\in\mathcal{S}_c} \rho_{j',c} \widetilde{\mathbf w}_{j'}^{(T)}\right)\right]
    \nonumber \\&
    = (1-\alpha_K)\bar{\mathbf{w}}^{(T-\Delta_{K-1})}
    +\alpha_K\sum_{c=1}^N\varrho_c\sum_{j\in\mathcal{S}_c} \rho_{j,c} \mathbf w_j^{(T)},
\end{align}
where the computation of the global deployed model $\widehat{\mathbf w}^{(T)}$ is performed by the main server at $t=T+\Delta_K^{\mathsf{U}}$.
\fi
The pseudocode for the {\tt DFL} algorithm with preset control parameters is given in Algorithm~\ref{DFL}. In Section~\ref{Sec:controlAlg}, we present the corresponding control algorithm (Algorithm~\ref{GT}) that tunes the algorithm parameters to achieve a sublinear convergence rate based on the bound derived in Sec.~\ref{sec:convAnalysis}, while mitigating network costs.
% \nm{to do what? Elaboratge a. bit more}.


% The process then repeats for $\mathcal{T}_{k+1}$.
% \nm{you need to explain a bit more about alpha here.. and compare with prior work}
% \chris{For the sake of proofs we define the global model update at any time instance as ...., although it is only realized at the instance of global aggregations at the main server.}


% Let $\mathcal{T}_{k}$ denote the time span between two consecutive global aggregations $k-1$ and $k$, which is referred to as $k$-th \textit{local model training interval}, and $\tau_{k}=|\mathcal{T}_{k}|$ denote the respective interval length, where $\mathcal{T}_k\in \{t_{k-1}+1,...,t_k\}$ with $t_k\triangleq \sum_{\ell=1}^{k}\tau_\ell$, $\forall k$. Since global aggregations are aperiodic, in general $\tau_{k}$ can be different from $\tau_{k'}$, $k\neq k'$.  The model training starts with an initial broadcasting of global model $\bar {\mathbf w}(0)$. At any time instance $t\add{\in\mathcal T_k}$, each edge device $i$ owns a local model denoted by $\mathbf w_i^{(t)}$. The global model is updated based on the devices' local models at the end of each local model training interval, where the main server samples one device from each cluster uniformly at random and requests parameter transmission from it to conduct global aggregation and obtain the global model $\bar{\mathbf w}$ as follows:
% \begin{align} \label{15}
%     \bar{\mathbf w}^{(k)} &= 
%           \sum\limits_{c=1}^N \varrho_c^{(k)} \mathbf w_{s_c}^{(t_k)}, ~~k=1,2,\cdots.
% \end{align}
% In~\eqref{15}, $\mathbf w_{s_c}$\nm{just call this $\bar{\mathbf w}_{c}^{(k)}$?}
% \nm{$s_c$ already used for smt else... notation is unnecessarily complicated} refers to the model parameter of the sampled device from cluster $c$. The global model is then broadcast by the main server,
%  using which the devices override their current local models: $\mathbf w_i^{(t_{k})}=\mathbf{\bar{w}}^{(t_{k})}$, $\forall i,k$. During the local model training interval, the devices perform successive local SGD iterations on their local loss functions, where at $t\in\mathcal T_k$, device $i$ samples a mini-batch $\xi_i^{(t)}$ of fixed size from its own local dataset $\mathcal D_i$ using \textit{simple random sampling without replacement} technique and uses it to calculate the local gradient estimate
% \begin{align}\label{eq:SGD}
%     \widehat{\mathbf g}_{i}^{(t)}=\frac{1}{\vert\xi_i^{(t)}\vert}\sum\limits_{(\mathbf x,y)\in\xi_i^{(t)}}
%     \bar{f}(\mathbf x,y;\mathbf w_i^{(t)}),
% \end{align}
% which is an unbiased estimate of the local gradient, i.e., $\mathbb E_{t}[\widehat{\mathbf g}_{i}^{(t)}]=\nabla F_i(\mathbf w_i^{(t)})$\nm{what about its variance?}. Using the gradient estimate each device then computes the \textit{intermediate updated local model} as follows:
% \begin{align} \label{8}
%     {\tilde{\mathbf{w}}}_i^{(t)} = 
%           \mathbf w_i^{(t-1)}-\eta^{(k)} \widehat{\mathbf g}_{i}^{(t-1)},~t\in\mathcal T_k,
% \end{align}
% where $\eta^{(k)}$ denotes the step size during the respective interval.

% The devices aperiodically form consensus through engaging in cooperative D2D communications during each local model training interval, determining the time of occurrence of which is a part of design elaborated in Sec.~\ref{Sec:controlAlg}. Upon forming consensus at time $t$, the devices perform multiple \textit{rounds} of D2D,\nm{does this happen within t? Why are the consensus updates not counted at the same timescale as local SGD? How do you make sure that the clusters are synced?} each of which consists of parameter transfers between neighboring devices. 
% To account for this, we express the updated local parameter of device $i\in\mathcal{S}_c$ using a general form. Letting $\widetilde{\mathbf{W}}^{(t)}_{{C}} \in \mathbb{R}^{|\mathcal{S}_c^{(k)}| \times M}$\nm{why C and not c?} denote the matrix of intermediate updated local model of the nodes in cluster $\mathcal{S}_c^{(k)}$ prior to consensus at time $t$, the evolution of the devices' parameters is given by:
%   \begin{equation}\label{eq:consensus}
%       \mathbf{W}^{(t)}_{{C}}= \left(\mathbf{V}^{(k)}_{{C}}\right)^{\Gamma^{(t)}_{{C}}} \widetilde{\mathbf{W}}^{(t)}_{{C}}, ~t\in\mathcal T_k,\nm{notation!}
%   \end{equation}
% where  ${\mathbf{W}}^{(t)}_{{C}}$ denotes the matrix of updated devices' parameters after the consensus, $\Gamma^{(t)}_{{C}}$ denotes the rounds of D2D in the respective cluster,  and $\mathbf{V}^{(k)}_{{C}}=[v^{(k)}_{i,j}]_{1\leq i,j\leq \mathcal{S}_c^{(k)}}$ denotes the consensus matrix with non-negative real elements. In particular, the $i$-th row of ${\mathbf{W}}^{(t)}_{{C}}$ correspondents to ${{\mathbf{w}}}_i^{(t)}$, which is used in~\eqref{eq:SGD} to calculate the next gradient estimate for the next local update. The consensus matrix is assumed to be fixed during the local model training period and is built based on the cluster topology $G^{(k)}_j$. It also only allows message exchange among the neighboring nodes. In particular, each node $i\in\mathcal{S}_c$ conducts the following iteration for $t'=0,\cdots,\Gamma^{(t)}_{{C}}-1$:
% \vspace{-1mm}
%     \begin{equation}\label{eq:ConsCenter}
%       \textbf{z}_{i}^{(t'+1)}= v^{(k)}_{i,i} \textbf{z}_{i}^{(t')}+\hspace{-2mm} \sum_{j\in \mathcal{N}^{(k)}_i} v^{(k)}_{i,j}\textbf{z}_{j}^{(t')},
%   \end{equation}
% where $\textbf{z}_{i}^{(0)}=\tilde{\mathbf{w}}_i^{(t)}$ is the node intermediate updated local model and $\textbf{z}_{i}^{\Gamma^{(t)}_{{C}}}=\mathbf w_i^{(t)}$ is the node updated model. Here, $t'$ corresponds to the second timescale used in the method that refers to the time spent during the consensus in addition to $t$ capturing the time elapse of the local iterations. 

% Finally, if at time $t\in\mathcal T_k$ the devices do not form consensus, we have $\Gamma^{(t)}_{{C}}=0$, implying $\mathbf{W}^{(t)}_{{C}}=  \mathbf I \times \widetilde{\mathbf{W}}^{(t)}_{{C}}$, which results in  ${{\mathbf{w}}}_i^{(t)} = 
%           \mathbf w_i^{(t-1)}-\eta^{(k)} \widehat{\mathbf g}_{i}^{(t-1)}$. A schematic of the procedure of {\tt TT-HF} is depicted in  Fig.~\ref{fig:twoTimeScale}.





% are the \textit{consensus weights} associated with node $n$  during $k$. We assume that each node has knowledge of these weights at each global iteration. For instance, one common choice for these weights~\cite{xiao2004fast} gives $\textbf{z}_{n}^{(t+1)} = \textbf{z}_{n}^{(t)}+d^{(k)}_{{C}}\sum_{m\in \mathcal{\zeta}^{(k)}(n)} (\textbf{z}_{m}^{(t)}-\textbf{z}_{n}^{(t)})$, $0 < d^{(k)}_{{C}} < 1 / D^{(k)}_{{C}}$, where $D^{(k)}_{{C}}$ is the maximum degree of the nodes in $G^{(k)}_{{C}}$.


% where $\eta^{(k)}$ denotes the step size during the respective interval and $\psi^{(k)}_{ij}\in[0,\infty)$, $\forall i,j$, denote the averaging weights. In~\eqref{8}, the updated parameter of each node $i$ is represented as an average between its own and its neighbors $\mathcal N_i^{(t)}$ parameters. If the consensus does not occur at $t$, we have $\psi^{(k)}_{ii}=1$ and $\psi^{(k)}_{ij}=0$, $i\neq j$, which implies ${\mathbf w}_i^{(t)} =  
%           \mathbf w_i^{(t-1)}-\eta^{(k)} \widehat{\mathbf g}_{i}^{(t-1)}$. However, if the consensus occurs, devices inside each cluster exchange and update their locals parameters through a sequence of D2D communications. We consider the general family of linear distributed consensus algorithms where device $\Psi^{(k)}=[\mathbf \psi_{ij}^{(k}]_{i,j\in\mathcal{S}_c^{(k)}}$, 

% During consensus, devices inside each cluster exchange and update their locals parameters. We consider the general family of linear distributed consensus algorithms where device




% Formally, during global iteration $k$, each node $n \in \mathcal{C}^{(k)}$  engages in the following rounds of iterative updates for $t=0,...,\theta^{(k)}_{{C}}-1$: 
% \vspace{-1mm}
%     \begin{equation}\label{eq:ConsCenter}
%       \textbf{z}_{n}^{(t+1)}= v^{(k)}_{n,n} \textbf{z}_{n}^{(t)}+\hspace{-2mm} \sum_{m\in \mathcal{\zeta}^{(k)}(n)} v^{(k)}_{n,m}\textbf{z}_{m}^{(t)},
%   \end{equation}
  
%   \vspace{-3mm}
%   \begin{equation}\label{eq:ConsCenter}
%       \textbf{z}_{n}^{(t+1)}= \textbf{z}_{n}^{(t)}+d^{(k)}_{\mathcal{C}}\hspace{-2mm} \sum_{m\in \mathcal{N}_{(k)}(n)} (\textbf{z}_{m}^{(t)}-\textbf{z}_{n}^{(t)}),
%   \end{equation} 
%   \noindent where $\textbf{z}_{n}^{(0)} = \mathbf{w}_{n}^{(k)}$ corresponds to node $n$'s initial parameter vector, and $\textbf{z}_{n}^{\big(\theta^{(k)}_{{C}}\big)}$ denotes the parameter after the D2D consensus process. $\mathcal{\zeta}^{(k)}(n)$ denotes the set of neighbors of node $n$ during global iteration $k$, and $v^{(k)}_{n,p}$, $p\in \{n\} \cup \mathcal{\zeta}^{(k)}(n)$
%   \nm{for convenience you can simply assume that $\mathcal{\zeta}^{(k)}(n)$ contains n as well} \ali{Yes. But writing as 7  may get a better perspective to the reader. Finally, we will work with the matrix form that encapsulates the neighbors in its elements.}
% are the \textit{consensus weights} associated with node $n$  during $k$. We assume that each node has knowledge of these weights at each global iteration. For instance, one common choice for these weights~\cite{xiao2004fast} gives $\textbf{z}_{n}^{(t+1)} = \textbf{z}_{n}^{(t)}+d^{(k)}_{{C}}\sum_{m\in \mathcal{\zeta}^{(k)}(n)} (\textbf{z}_{m}^{(t)}-\textbf{z}_{n}^{(t)})$, $0 < d^{(k)}_{{C}} < 1 / D^{(k)}_{{C}}$, where $D^{(k)}_{{C}}$ is the maximum degree of the nodes in $G^{(k)}_{{C}}$. {\tt TT-HF} thus observes the time from two different scales: (i) \textit{micro scale}, that concerns with devices' parameters evolution during the local updates; (ii) \textit{macro scale} that concerns with server' parameter evolution at the instances  of global aggregations. We let $\mathbf w_i(t)$ be the local model parameter vector of edge device $i$ at time $t$, initialized as $\mathbf w_i(0)=\mathbf w(0)$ at time $0$.






% that $[\mathbf \psi_{ij,k}^{(c)}]_{i,j\in\mathcal{S}_c^{(k)}}$ are the symmetric and doubly stochastic weights imposed by the distributed average consensus method used at the $k$th global aggregation.

% We consider a scenario in which the edge devices perform multiple rounds of local gradient descents in between a sequence of global aggregations. At each global aggregation,  the server acquires the parameters of the devices and compute the global parameter and broadcasts it among the devices. We consider a general case where \textit{aperiodic} global aggregations are conducted, i.e., the length of the time span in between two consecutive global aggregations, and thus the number of local descents, varies between different consecutive global aggregations. \chris{One of the main contributions of this paper will be studying the optimization of these periods.}
% \nm{who dictates when the aggregations are done? Is that random? Is it based on certain criteria? Please clarify} \ali{this will be investigated using the bounds. We would say to get a certain convergence rate, this many times of consensus inside a period of length ... are required tht each performs this many rounds of consensus.}\nm{ok but if you dont clearly state that now you will leave the reviewer wondering...}

% Let the $t=0,1,\cdots,T$ denote the evolution of time during the training phase. To capture the aperiodic global aggregations, we assume that the number of local updates carried out between the $k$th global aggregation, i.e., the number of local updates between global aggregation $k-1$ and $k$, is $\tau_k$, $k\in\mathbb{N}$.
%  \nm{the definition of $\tau$ is unclear and seems contradictory... here it is defined as the time between two global aggrs.;
%  later (highlighted) it is defined as the number of local gradient updates between two globals..
% why so?
%  } \ali{I think these two are equivalent. Like if one does one global aggregation every time, he would do one local + one global. If he does one global aggregation every two time instance, he would get local+local+global. So, we need to emphasize that the notion of "time" is defined based on local descents.} \chris{I think the question here is about clarifying that $\tau$ is an integer? So that ``time between'' is a count?}




% parameter sharing via D2D communication between edge devices on learning performance is incorporated into the design of the FL system. We divide the learning process into discrete time intervals $t\in\{1,2,...,T\}$, where the duration between two consecutive aggregations is denoted as $\tau_k$. We let $\mathbf w_i(t)$ be the local model parameter vector of edge device $i$ at time $t$, initialized as $\mathbf w_i(0)=\mathbf w(0)$ at time $0$ across all devices $i\in\mathcal S$.

% The \textit{hybrid} platform is achieved via performing both D2D and device-to-server communications, where in D2D mode, the devices perform periodic \textit{distributed average consensus}, which we refer to as consensus for brevity, during the time span between two global aggregations. Also, in device-to-server communications, the server \textit{aperiodically} gathers the parameters of the devices to update the global parameter which is then broadcasted among the devices. In the following, we describe the procedure the devices perform in detail.


% \begin{assumption} \label{SGD}
% We make the following assumptions on $\widehat{\mathbf g}_{i,t}$:
%     \begin{description}
%         \item[Unbiased local estimation:] $\widehat{\mathbf g}_{i,t}$ is an unbiased estimate of local gradient $\nabla F_j(\mathbf w_j(t))$ obtained from the uniformly sampled mini-batch over each device’s local data:
%          \begin{align}
%             \mathbb E_{t}[\widehat{\mathbf g}^{(t)}_{j}]=\nabla F_j(\mathbf w_j(t))
%          \end{align}
%          \item[Bounded local variance:] The variance of $\widehat{\mathbf g}_{i,t}$ is bounded by $\sigma^2$.
%         \begin{align} 
%             \mathbb E_{t}[\Vert n_i(t)\Vert_2^2]\leq 
%             \sigma^2,
%         \end{align}
%         where $n_i(t)=\widehat{\mathbf g}_{i,t}-\nabla F_i(\mathbf w_i(t))$ and the expectation is $\mathbb E_t(\cdot)$ taken with respect to the random selection of the mini batch.
%     \end{description}
% \end{assumption}

% \add{i.e., $\bar g$ is an unbiased estimate of the true local gradient $\nabla F$, with variance bounded by $\sigma^2$.}
% \nm{you need to specify that the expectation is with respect to the random selection of the mini batch}

% \subsubsection{Distributed consensus}
% \nm{I do not understand the model here... are you assuming you do several local and one consensus step? But later one you say that each consensus step includes $\gamma_k$ consensus steps...be more precise!
% Also, why are you doing several local steps followed by consensus, and not 1 local followed by consensus? Does it help in some way?}

% Consider the time span between the global aggregations $k$ and $k+1$, $k\in\{0,\dots, K-1\}$, gathered by the set $\mathcal T_k = \{k\tau_k+1,...,(k+1)\tau_k\}$.
% At each time $t \in \mathcal T_k \setminus\{k\tau_k\}$ \ali{This is not in the set? am I missing sth?}, each edge device performs $u$ steps of local gradient update along with multiple rounds of consensus update (we assume that $\tau_k=h_k\mu_k$ such that $h_k$ times of consensus update is carried out at $\mathcal H_k\in\{k\tau_k+\mu, k\tau_k+2\mu,\dots,(k+1)\tau_k\}$.
% We consider a case in which in the time span between global aggregations $k-1$ and $k$, where total of $\tau_k$ local gradient descent is performed at each device, the devices enagage in consensus periodically after every $\mu_k$ time instances. We assume that $\tau_k=\mu_k h_k$, where $h_k$ denotes the number of times the devices engage in consensus in the respective time span.

% We consider a case in which in the time span between global aggregations $k-1$ and $k$, where a total of $\tau_k$ local gradient descent is performed at each device, the devices engage in consensus periodically after every $\mu_k$ time instances. We assume that $\tau_k=\mu_k(h_k+1)$,\nm{something is unclear: one time slot includes 1 local and $\mu_k-1$ consensus updates?} \frank{It is $\mu_k-1$ local + $\Gamma_c(\mu_k)$ rounds of consensus}
%  where $h_k$ denotes the number of times the devices engage in consensus in the respective time span carried out at $\mathcal H_k = \{t_{k-1}+\mu_k, t_{k-1}+2\mu_k,\dots,t_{k-1}+h_k\mu_k\}$, where $k=1,2,\cdots,K$ and $t_0=0$. During consensus, devices inside each cluster exchange and update their locals parameters. We consider the general family of linear distributed consensus algorithms where device $i\in\mathcal{S}_c^{(k)}$ is sequentially updated via receiving the neighboring devices parameters as
% \begin{align} \label{9}
%     \mathbf w_i(t) = \sum\limits_{j\in\mathcal N_i^{(t)}} \psi_{ij,k}^{(c)}\tilde{\mathbf w}_j(t),
% \end{align}
% \nm{Here we go back to the problem I talked about with time varying graphs: if Nit is timee varying, then $\psi_{ij}^{(c)}$ needs to be time varying otherwise you cannot guaranteee the synnetric doubly stoch property that you need in the consensus}
% \nm{the use of t' and t is quite confusing, since they follow different timescales:
% larger timescale: k, faster timescale: t; fastest timescale, t'. Please define a clear and consistent notation} \chris{I agree it might be cleanest to define everything in terms of $t$, and just have different operations happening at different intervals of $t$. But the advantage of having one timescale for local updates and another for consensus rounds is that we can point out the difference in resource utilization each operation may cause.}
% for $t'=0,\cdots,\Gamma_c(t)-1$, where $\Gamma_c(t)$ denotes the \textit{rounds of consensus} performed at time\nm{so one time includes multiple consensus steps? Why is it dependent on t and c? Does it mean that some clusters are faster than other?} $t$\nm{in cluster $c$?}, $\mathbf x_i(0)=\mathbf w_i(t)$ and overwrite $\mathbf w_i(t)\leftarrow\mathbf x_i(\Gamma_c(t))$, where $s_c^{(k)}=|\mathcal{S}_c^{(k)}|$.
% \nm{does not make sense, you ahve already defined
% $\mathbf w_i(t) =  
%           \mathbf w_i(t-1)-\eta_k \widehat{\mathbf g}_{i,t-1}$ in (12), Please use different notation, i.e., w before and after consensus}
% such that $[\mathbf \psi_{ij,k}^{(c)}]_{i,j\in\mathcal{S}_c^{(k)}}$ are the symmetric and doubly stochastic weights imposed by the distributed average consensus method used at the $k$th global aggregation.
% \chris{Regarding the dependency of $\Gamma_c(t)$ on cluster $c$, I think this would be important if the clusters have different properties, e.g., different data distributions, computation capabilities, or local topologies.}



% \chris{I think the case where $\mu_k = 1$ and $\Gamma_c(\mu_k) = 1$ will be particularly interesting to consider. One gradient iteration followed by one consensus round.}

% \subsubsection{Aperiodic global aggregations} At time $t_k$, $k=1,2,\cdots,K$, the server randomly samples one local parameters $\mathbf w_{s_c}(t_k)$ from node $s_c$ in each cluster $c$ and computes the new global parameter $\bar{\mathbf w}(t_k)$ as

% Sample one node from each cluster:
% \begin{align} \label{eq15}
%     \mathbf w(t) &= 
%           \sum\limits_{c=1}^N \varrho_c^{(k)} \mathbf w_{s_c^{(k)}}(t)
%           \nonumber \\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \big(\bar{\mathbf w}_c(t)+s_c^{(k)}\eta_k e_{s_c^{(k)}}^{(\Gamma_c(t))}\big)
%           \nonumber \\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \sum\limits_{i\in\mathcal{S}_c^{(k)}} \frac{1}{s_c^{(k)}} \underbrace{\left[\mathbf w_i(t-1)-\eta_k\nabla F_i(\mathbf w_i(t-1))\right]}_{\tilde{w}_{i}(t)}
%           \nonumber \\&
%           + \eta_k\sum\limits_{c=1}^N \varrho_c^{(k)} s_c^{(k)} e_{s_c^{(k)}}^{(\Gamma_c(t))}
%           \nonumber \\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}} \sum\limits_{i\in\mathcal{S}_c^{(k)}}
%           \big(\bar{\mathbf w}_c(t-1)+s_c^{(k)}\eta_k e_{i}^{(\Gamma_c(t-1))}\big)
%           \nonumber \\&
%           -\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}} \sum\limits_{i\in\mathcal{S}_c^{(k)}} 
%           \eta_k\nabla F_i(\mathbf w_i(t-1))
%           + \eta_k\sum\limits_{c=1}^N \varrho_c^{(k)} s_c^{(k)} e_{s_c^{(k)}}^{(\Gamma_c(t))}
%           \nonumber \\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}} \sum\limits_{i\in\mathcal{S}_c^{(k)}}
%           \bar{\mathbf w}_c(t-1)
%           -\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}} \sum\limits_{i\in\mathcal{S}_c^{(k)}} 
%           \eta_k\nabla F_i(\mathbf w_i(t-1))
%           \nonumber \\&
%           + \eta_k\sum\limits_{c=1}^N \varrho_c^{(k)} s_c^{(k)} e_{s_c^{(k)}}^{(\Gamma_c(t))}
%           \nonumber \\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}} \sum\limits_{i\in\mathcal{S}_c^{(k)}}
%           \big(\mathbf w_{s_c^{(k)}}(t-1)-s_c^{(k)}\eta_k e_{s_c^{(k)}}^{(\Gamma_c(t-1))}\big)
%           \nonumber \\&
%           -\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}} 
%           \eta_k\nabla F_i(\mathbf w_i(t-1))
%           + \eta_k\sum\limits_{c=1}^N \varrho_c^{(k)} s_c^{(k)} e_{s_c^{(k)}}^{(\Gamma_c(t))},
% \end{align}

% Sample all nodes
%     \begin{align*}
%     \mathbf w(t) &= 
%           \sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}} \sum\limits_{i\in\mathcal{S}_c^{(k)}}\overline{\mathbf w_{c}}(t)
%           \nonumber\\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}} \sum\limits_{i\in\mathcal{S}_c^{(k)}}\big(\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \left[\mathbf w_j(t-1)-\eta_k\nabla F_j(\mathbf w_j(t-1))\right]\big)
%           \nonumber\\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \mathbf w_j(t-1)
%           \nonumber\\&
%           -\eta_k\sum\limits_{c=1}^N \varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}} \nabla F_i(\mathbf w_i(t-1))
%           \nonumber\\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \Big(\bar{\mathbf w}_c(t-1)+s_c^{(k)} e_{j}^{(\Gamma_c(t-1))}\Big)
%           \nonumber\\&
%           -\eta_k\sum\limits_{c=1}^N \varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}} \nabla F_i(\mathbf w_i(t-1))
%           \nonumber\\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \bar{\mathbf w}_c(t-1)
%           -\eta_k\sum\limits_{c=1}^N \varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}} \nabla F_i(\mathbf w_i(t-1))
%           \nonumber\\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \Big(\mathbf w_j(t-1)-s_c^{(k)} e_{j}^{(\Gamma_c(t-1))}\Big)
%           \nonumber\\&
%           -\eta_k\sum\limits_{c=1}^N \varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}} \nabla F_i(\mathbf w_i(t-1))
%           \nonumber\\&
%           = \mathbf w(t-1)
%           -\eta_k\sum\limits_{c=1}^N \varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}} \nabla F_i(\mathbf w_i(t-1))
%     \end{align*}

% \begin{align} \label{15}
%     \bar{\mathbf w}(t_k) &= 
%           \sum\limits_{c=1}^N \varrho_c^{(k)} \mathbf w_{s_c}(t_k)
% \end{align}
% % \nm{didnt you say that you sample a single device from each cluster?} \chris{$s_c^{(k)}$ is the node index that is being sampled?}
% % where $\mathbf w(t)$ consists of the perfect average of the cluster parameters (a) across the  network with an additional term caused by performing finite consensus rounds (b). 
% The server then broadcasts this parameter across the network and each edge device replaces its local parameter $\mathbf w_i(t)$ with $\bar{\mathbf w}(t)$ and starts a series of new local descents steps. 

% \begin{algorithm}
% \small
% \SetAlgoLined
% \caption{Two Timescale Hybrid Federated Learning ({\tt TT-HF})} \label{GT}
% % \KwResult{Write here the result }
% \KwIn{$\Gamma$,$\gamma$,$K$} 
% \KwOut{Global model $\bar{\mathbf w}(t_K)$}
%  \textbf{Initialization operated by the server:} (i) Initialize the local model as $\mathbf w_i(0)=\bar{\mathbf w}(0),\  \forall i$, (ii) Set the step size as $\eta_t=\frac{\gamma}{t+\alpha}$, where $\alpha=\max\{\beta\gamma/\kappa, \beta\gamma\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\}$\;
%  \For{$k=1:K$}{
%      \For{$t=t_{k-1}+1:t_k$}{
%       \uIf{$t=t_k$}{
%       Estimate $\bar{\beta}\leftarrow\sum_{c=1}^N \varrho_c^{(k)}\bar{\beta}_i$\;
%       Estimate $\bar{\mu}\leftarrow\sum_{c=1}^N \varrho_c^{(k)}\bar{\mu}_i$\;
%       Estimate $\delta,\zeta\leftarrow\Vert\nabla F(\bar{\mathbf w}(t_k))-\nabla F_i(\bar{\mathbf w}(t_k))\Vert$\;
%       Estimate parameter for the variance of SGD $\sigma$\;
%       Compute $\bar{\mathbf w}(t)$ with \eqref{15} at the server\;
%       Synchronize local models with the global model $\mathbf w_i(t)=\bar{\mathbf w}(t),~\forall i$ //Global Synchronization\;
%       Randomly choose some $\tau_{k+1}<\log_4\frac{\kappa}{16\omega^2}$\;
%         \uIf{$\tau_{k+1}>\log_4\left\{(\mu\gamma-1)\kappa/(16\omega^2\beta^2\gamma^2)\right\}$} 
%         {
%         $\tau_{k+1} = \tau_{k+1}/2$ // The initial choice of $\tau_{k+1}$ could not be satisfied\;
%         }
%         \Else{
%         Tune the number of D2D consensus such that \\
%         $\tau_k \leq \log_4\left\{[(\mu\gamma-1)\nu-B]/[\gamma^2\beta A/2 +16\omega^2\beta^2\gamma^2(\Gamma-\phi^2\gamma/2)/\kappa]\right\}$ is satisfied\;
%         }
%           }
%     \Else{
%       For each edge device $i\in\mathcal{S}_c^{(k)}$ in parallel, perform local update with\\
%       $\mathbf w_i(t+1) =  
%           \sum\limits_{i\in \mathcal S_c^{(k)}}a_{i,j}^{(c)}[\mathbf w_j(t)-\eta_t\nabla F_j(\mathbf w_j(t))]$, \\
%           where the weight $a_{i,j}^{(c)}$ embed several rounds of D2D consensus\;
%           \uIf{$\bar{\beta}_i<\Vert\nabla F_i(\mathbf w _i(t))-\nabla F_i(\bar{\mathbf w}(t_{k-1}))\Vert/\Vert\mathbf w _i(t)-\bar{\mathbf w}(t_{k-1})\Vert$}
%           {
%           Estimate $\bar{\beta}_i\leftarrow\Vert\nabla F_i(\mathbf w _i(t))-\nabla F_i(\bar{\mathbf w}(t_{k-1}))\Vert/\Vert\mathbf w _i(t)-\bar{\mathbf w}(t_{k-1})\Vert$\;
%           }
%           \uIf{$\bar{\mu}_i>(\nabla F_i(\mathbf w _i(t))-\nabla F_i(\bar{\mathbf w}(t_{k-1})))^\top(\mathbf w _i(t)-\bar{\mathbf w}(t_{k-1}))/\Vert\mathbf w _i(t)-\bar{\mathbf w}(t_{k-1})\Vert^2$}
%           {Estimate $\bar{\mu}_i\leftarrow(\nabla F_i(\mathbf w _i(t))-\nabla F_i(\bar{\mathbf w}(t_{k-1})))^\top(\mathbf w _i(t)-\bar{\mathbf w}(t_{k-1}))/\Vert\mathbf w _i(t)-\bar{\mathbf w}(t_{k-1})\Vert^2$\;
%           }
          
%       }
%      }
%  }
% \end{algorithm}

