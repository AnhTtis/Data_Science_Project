\section{Convergence Analysis of {\tt DFL}} \label{sec:convAnalysis}
\noindent In this section, we aim to provide a theoretical analysis of the convergence behavior of the global deployed model under {\tt DFL}. To facilitate this analysis, 
% \nm{
we adopt an approach similar to~\cite{err_free2022}. We break down the errors
between the local models and the global optimum into: 1) the error between the local models and \emph{virtual} models following noise-free dynamics; and 2) the errors between the latter and the global optimum. To this end,
% }
we define a virtual subnet \textit{noise-free variable}
% \nm{"error" is confusing here. To me, error is between model and global opt. I would use "noise" to use any othersource of error (SGD, inter-cluster diversity)} 
(i.e., it considers the full-batch gradient of $\bar{F}_c(\cdot)$, thereby neglecting the SGD noise and intra-subnet diversity) that remains constant throughout the entire training period as
\begin{align} \label{eq:errFree_c}
    \bar{\mathbf v}_c^{(t+1)}=\bar{\mathbf v}_c^{(t)}
        -\eta_{k}\nabla \bar{F}_c(\bar{\mathbf v}_c^{(t)}),~\forall t\in\mathcal{T}_k\setminus{\{t_k\}}.
\end{align}
% \nm{eq 19 must go right after 17...}
% \nm{connected to above: why not just $\bar{\mathbf{w}}^{(t-\Delta_k)}$?}
The \textit{subnet noise-free variable at global synchronization} is defined as 
\begin{align}\label{eq:EFGS}
    \bar{\mathbf v}_c^{(t_{k+1})}
    =& (1-\alpha_k)\bar{\mathbf v}^{(t_{k+1}-\Delta_k)}
    +\alpha_k\widetilde{\mathbf v}_c^{(t_{k+1})},
\end{align}
where $\widetilde{\mathbf v}_c^{(t_{k+1})}$ is the noise-free variable right before global synchronization, as opposed to $\mathbf v_c^{(t_{k+1})}$ defined immediately after global synchronization. Similarly, we define the \textit{virtual global noise-free variable} as  
\begin{equation} \label{eq:v_m}
    \bar{\mathbf v}^{(t+1)}=\sum\limits_{d=1}^N\varrho_{d}\bar{\mathbf v}_d^{(t+1)},~\forall t\in\mathcal{T}_k.
\end{equation}
% \footnote{The reason behind the using the word ``virtual" is that $\widehat{\mathbf w}^{(t)}$ is only realized at the server upon conducting a global aggregation at $t=T$ and $\bar{\mathbf w}^{(t)}$ is only realized upon conducting global aggregations at $t=t_k-\Delta_k^{\mathsf{D}},~\forall k$.} 
To simplify the presentation of the convergence analysis, we assume $\alpha_k=\alpha$ (i.e., the weighing coefficient in \eqref{eq:EFGS}) $\tau_k=\tau$ (i.e., the length of local model training interval) and $\Delta_k=\Delta$ (i.e., the delay), are constants throughout the training, for all $k$.
% \nm{wait: I think the analysis is based on the assumption that these are constants(and tau as well). So saying that you are dropping the dependence on k as a substitute notation is actually wrong..}. 
This matches the design of the control algorithm presented in Algorithm~\ref{GT} (Sec.~\ref{Sec:controlAlg}), where an optimization formulation is formulated at the beginning of each global synchronization to optimize the performance metrics of our interest for the remaining of ML model training time and determine $\tau_k$ and $\alpha_k$ for each aggregation given the edge-to-cloud communication delay $\Delta_k$. Although $\alpha$, $\tau$, and $\Delta$ are assumed to be fixed in the convergence analysis, we will later use the analysis results to obtain instantaneous $\alpha_k$ for {\tt DFL}.

% The formulation inherently assumes that the optimization variables $\tau_k$, $\alpha_k$ and $\Delta_k$ are fixed over the remaining period from $t_{k}$ to $T$. However, the proof can be readily extended to the case when $\alpha$ and $\Delta$ are changing across global aggregations with more complicated expressions.
% Note that at the beginning of each local model training interval the virtual global aggregated model is synchronized to the virtual global deployed model, i.e., $\bar{\mathbf w}^{(t)} = \widehat{\mathbf w}^{(t)}$, $\forall t=t_{k-1}$.   

% We analyze the convergence behavior of the virtual client-deployed global model across time $t$ and show that since $\bar{\mathbf w}^{(t)} = \widehat{\mathbf w}^{(t)}$ at time $t_k,~\forall k$, the global average of the local models $\bar{\mathbf w}^{(t)}$ as well as the global model built by the cloud server has the same convergence behavior.

\subsection{Intermediate Quantities and Results}
% \nm{ I dont agree in the way you intrudoce the assumptions below. A reviewer might think that you are introducing unrealistic and strong assumptions. It should be more clear that these assumptions are actually going to be proved and can be satisfied by your control algo. The way this section is structured does not make it clear enough..}
% Our main results are presented in Sec.~\ref{ssec:convAvg} and Sec.~\ref{ssec:sublinear}. Before then, in Sec.~\ref{ssec:definitions}, we introduce some additional definitions and a key proposition for the analysis.
We make the following assumptions and define three quantities used throughout the analysis.
% \nm{SGD noise assumption should go here..}
\begin{assumption}[SGD Noise Characteristics] \label{assump:SGD_noise}
    Let ${\mathbf n}_{i}^{(t)}=\widehat{\mathbf g}_{i}^{(t)}-\nabla F_i(\mathbf w_{i}^{(t)})$
    $\forall i,t$ denote the noise of the estimated gradient through the SGD process for device $i$ at time $t$. The conditional expectation based on time $t$ is $\mathbb{E}_t[{\mathbf n}_{i}^{(t)}]=\bm{0}$ with an upper bound $\sigma^2$ on the variance of the noises, such that $\exists \sigma>0: \mathbb{E}_t[\Vert{\mathbf n}_{i}^{(t)}\Vert^2]\leq \sigma^2, \forall i,t$. 
\end{assumption}
\begin{assumption}[Subnet Deviation Noise] \label{assump:sub_err}
    We assume that $\Theta_c^{(t)}$ is chosen such that the following noise within a subnet is upper bounded by $\phi^2$
    \begin{align}
        \sum\limits_{c=1}^N\varrho_c(1-\Theta_c^{(t)})(2\delta_c^2+4\omega_c^2\beta^2\Vert\bar{\mathbf v}_c^{(t)}-\mathbf w^*\Vert^2)\leq\phi^2,
    \end{align}
    where $\phi>0$ is a constant. 
\end{assumption}
We will ensure the enforcement of the above assumption through the control algorithm in Sec.~\ref{Sec:controlAlg}.
% These assumptions will be then used in our main results in Sec.~\ref{ssec:convAvg}.
% In our convergence analysis, we adopt a similar approach to~\cite{err_free2022} to break down the errors involved in model training. 
% \nm{last sentence moved earlier}
We now define the following set of error terms.
\subsubsection{Subnet Deviation Error}
% Considering the local update process in \eqref{eq:w_i-gen}, the local model of each device $i\in \mathcal{S}_c$ can be viewed as an imperfect aggregation of the local models of the devices in the same subnet as
% \begin{align} \label{eq14}
%     \mathbf w_i^{(t)} = \bar{\mathbf w}_c^{(t)} + \mathbf e_i^{(t)},
% \end{align}      
% where $\mathbf e_i^{(t)} \in \mathbb{R}^M$ denotes the \textit{device deviation error}
% caused by data heterogeneity among the devices.
We define the \textit{subnet deviation error} as follows:  
\begin{align} \label{def:eps_c}
   e_1^{(t+1)}\triangleq\mathbb E\Big[\sum\limits_{c=1}^N\varrho_c\sum_{j\in\mathcal S_c}\rho_{j,c}\Vert\mathbf w_j^{(t+1)} - \bar{\mathbf v}_c^{(t+1)}\Vert^2\Big]^{1/2}.
\end{align} 
$e_1^{(t)}$ captures the average deviation error of local models of devices within a subnet from the subnet noise-free variable.
\subsubsection{Expected Model Dispersion and Optimality Gap of the Noise-Free Variable}
We next define 
\begin{align}\label{eq:defA}
e_2^{(t)} = \sum\limits_{c=1}^N\varrho_{c}\mathbb E[\Vert\bar{\mathbf v}_c^{(t)}-\bar{\mathbf v}^{(t)}\Vert]
\end{align}
as the \textit{expected model dispersion} of the noise-free variable with $\bar{\mathbf v}_c^{(t)}$ and $\bar{\mathbf v}^{(t)}$ defined in~\eqref{eq:errFree_c} and~\eqref{eq:v_m} respectively. $e_2^{(t)}$ measures the degree to which the 
% \nm{virtual? You used the def of virtual before but you are not using it..} 
subnet noise-free variable deviates from the global noise-free variable during the local model training interval. In addition, we define
\begin{align}\label{eq:defB}
    e_3^{(t)}=\mathbb E[\Vert\bar{\mathbf v}^{(t)}-\mathbf w^*\Vert]
\end{align}
as the \textit{expected optimality gap of the Noise-Free variable}. $e_3^{(t)}$ measures the degree to which the global noise-free variable deviates from the optimum during the local model training interval.

Obtaining a general upper bound on $e_2^{(t)}$ and $e_3^{(t)}$ is non-trivial due to the coupling between the gradient diversity and the model parameters imposed by~\eqref{eq:11} and~\eqref{eq:clust_div}. For an appropriate choice of step size in~\eqref{8}, we upper bound these quantities by analyzing coupled variable systems. Upper bounds of $e_1^{(t)}$, $e_2^{(t)}$ and $e_3^{(t)}$ are illustrated in the following Lemma and Proposition. Using these bounds, we later obtain the convergence bounds of the global deployed model obtained in {\tt DFL}.
\begin{lemma}[One-step behavior of $e_1^{(t)}$, $e_2^{(t)}$ and $e_3^{(t)}$] \label{lem:An_oneSTP_m}
For $t\in\mathcal T_k$ before performing global synchronization, under Assumptions \ref{beta},~\ref{assump:SGD_noise} and~\ref{assump:sub_err}, if $\eta_{k}\leq\frac{2}{\mu+\beta},~\forall k$, using {\tt DFL} for ML model training, in $t\in\mathcal T_k$, the one-step behaviors of $(e_1^{(t+1)})^2$, $e_2^{(t+1)}$ and $e_3^{(t+1)}$ are presented as follows:
% \nm{move to very beginning of App}
\begin{align} \label{eq:e1_oneSTP}
    &(e_1^{(t+1)})^2
    \leq
    (1-\mu\eta_{k})^2(e_1^{(t)})^2
    +\eta_{k}^2(\sigma^2+\phi^2),
    \\\label{eq:e2_oneSTP}
    &e_2^{(t+1)}\leq
    (1+\eta_{k}(\beta-\mu))e_2^{(t)} 
    +2\omega\eta_{k}\beta e_3^{(t)}
    +\eta_{k}\delta,
\\ \label{eq:e3_oneSTP}
&   e_3^{(t+1)} \leq
     (1-\eta_{k}\mu)e_3^{(t)} 
    +\eta_{k}\beta e_2^{(t)}.
\end{align} 
%\nm{assumption on control algo (phi) missing. state as an Assumption in the main text and recall it here}
\end{lemma}
Lemma~\ref{lem:An_oneSTP_m} characterizes the one-step dynamics of $e_1^{(t)}$, $e_2^{(t)}$, and $e_3^{(t)}$ within a local model training interval. During local model updates, the upper bounds of $e_1^{(t)}$ and $e_3^{(t)}$ display a contraction behavior among different terms, while the upper bound of $e_2^{(t)}$ exhibits a monotonic increase. Given that the upper bounds for $e_2^{(t)}$ and $e_3^{(t)}$ are interrelated, it is essential to investigate their mutual impact and the role of the combiner weight in shaping their behavior at the global synchronization stage. This investigation will be conducted in the subsequent proposition.
% \nm{I dont think we need to talk about e=0/ It is pretty obvious that you are going to have some error due to different datasets}
% Intuitively, $\mathbf e_i^{(t)} = \mathbf 0$ in two cases: (i) all the devices inside the subnet have the same tentative updated local model (i.e., $\widetilde{\mathbf w}_i^{(t)} = \widetilde{\mathbf w}_{i'}^{(t)}$ for $i, i' \in \mathcal{S}_c$), \hl{which occurs when the datasets of the devices are completely i.i.d.}\nm{this is confusing and does not make  sense to me.. Even if the datasets are iid, the gradients are going to be different, causing widetilde w to be different..}
% \nm{the only case when widetilde are the same is when the datasets are exactly the same, that's it, it has nothing to do with whether they are iid or not..}
% (i.e., when $\delta_c=\zeta_c=0$ in~\eqref{eq:clust_div})\nm{also, not true that delta zera are zero for iid datasets! The most you can say is that, For iid datasets, they "tend" to be smaller than for non iid datasets. Remember that we are looking at ONE REALIZATION of the datasets, which are kept fixed! GIVEN the realization, delta and zeta are determined.} under performing full-batch SGD; (ii) at the instance of local model aggregations $t\in \mathcal T^\mathsf{L}_{k,c}$ (i.e., when $\Theta_c^{(t)}=0$). 


% \nm{Why so?  You need to motivate.. Also, why alpha and delta constant but not tau?
% Does the analysis break down if alpha and delta are not constnat?}
% Subsequently, $\mathbf e_i^{(t)}$ will increase upon having highly non-i.i.d. data across the devices and when the period of local model aggregation increases and $t\notin \mathcal T^\mathsf{L}_{k,c}$. 

% \nm{please introduce the assumption}


% \begin{assumption} [Bounded Subnet Deviation Error] \label{assum:clust_div}
%     For $t\in\mathcal T_k$, $\forall k$, using {\tt DFL} for ML model training, under the choice of aggregation indicator sequence $\{\Theta_c^{(t)}\}_{t\in\mathcal T_k}$ obtained based on local aggregation time instance $T^\mathsf{L}_{k,c}$ according to~\eqref{eq:w_i-gen}, $\forall c$, the subnet deviation error at each subnet $\mathcal{S}_c$ is  upper-bounded by a finite positive constant $\epsilon_c^{(t)}$ as follows:
%     % \nm{I dont undestand.. isnit the error affected by the choice of chi? Ie wheher you do local aggrr or not? So, to find a bound, you need a strategy on how to select chi, but there is no specification of that}
%     \begin{align} \label{def:eps_c}
%       \sum\limits_{i\in\mathcal S_c}\rho_{i,c}\mathbb E[\Vert \mathbf e_i^{(t)} \Vert^2] 
%       &\leq (\epsilon_c^{(t)})^2.
%     %   \prod_{m=t_{k-1}}^{t-1}\Theta_c^{(m+1)}(1+2\eta_m\beta)\sum\limits_{j\in\mathcal S_c}\rho_{j,c}\mathbb E\Vert \mathbf e_j^{(t_{k-1})} \Vert 
%     % \nonumber \\&
%     % +\sum_{\ell=t_{k-1}}^{t-1}\Theta_c^{(\ell+1)}\eta_\ell\prod_{j=\ell+1}^{t-1}\Theta_c^{(j+1)}(1+2\eta_j\beta)\left(\delta_c+2 \sigma\right),~ \forall i\in \mathcal{S}_c. 
%     \end{align}
%     % \nm{$\mathbb E_t$ insteaf of $\mathbb E_{t_{k-1}}$??}
%      We further define the subnet average deviation error bound $\epsilon^{(t)}$ as 
%      \begin{equation}\label{def:eps}
%           (\epsilon^{(t)})^2 \triangleq \sum\limits_{c=1}^N \varrho_c(\epsilon_c^{(t)})^2.
%      \end{equation}
%     %  \nm{well this is obvious.. since epsc is deterministic and finite, of couse eps is. Shouldt this just be an equality? No need to have ineq.
%     %  I would write: "We also define the network average deviation error bound as \begin{equation}\label{def:eps}
%     %     (\epsilon^{(t)})^2\triangleq  \sum\limits_{c=1}^N \varrho_c(\epsilon_c^{(t)})^2
%     %  \end{equation}"
%     %  }
% %     where \nm{REMOVE: Epsilon is a controllable parameter. The control algorithm will make sure that the error is within epsilon}
% %     \begin{align}
% %         (\epsilon_c^{(t)})^2 &= 2[\Sigma_{\{c,+\},t}]^2\bigg[2\omega_c\Vert\bar{\mathbf w}_c^{(t_{k-1})}-\mathbf w^*\Vert
% %         %  \nonumber \\&
% % +
% % \Big(\frac{4}{3}\sigma/\beta+\frac{2\omega_c+1}{2(1-\omega_c)}\delta_c/\beta+\frac{\omega_c}{1-\omega_c}\delta/\beta+\sigma/\beta\Big)\bigg]^2
% % \nonumber \\&
% % +2[\Sigma_{\{c,+\},t}]^2\sum\limits_{j\in\mathcal S_c}\rho_{j,c} \Vert\mathbf e_{j}^{(t_{k-1})}\Vert^2,
% %     \end{align}
% %         \begin{align}
% %             &\Sigma_{\{c,+\},t}=\sum\limits_{\ell=t_{k-1}}^{t-1}\left(\prod_{j=t_{k-1}}^{\ell-1}(1+\eta_j\beta\lambda_{\{c,+\}})\right)\beta\eta_\ell\left(\prod_{j=\ell+1}^{t-1}(1+\eta_j\beta)\right),
% %         \end{align}
% %     and
% %     % \nm{please dont use the symbol $\vartheta$. Replace it with $\mu/\beta$ or normalize $\mu$, so that $\mu\beta$ is the strong convexity param}
% %     \begin{equation}
% %       \lambda_{\{c,+\}} =\frac{1}{2}(3+\sqrt{8\omega_c+1}).
% %     \end{equation}
% \end{assumption}
\begin{proposition}[Upper bounds for $e_1^{(t_{k+1})}$, $e_2^{(t_{k+1})}$ and $e_3^{(t_{k+1})}$] \label{prop:An_oneSTP_t}
    Under Assumptions \ref{beta},~\ref{assump:SGD_noise} and~\ref{assump:sub_err}, if $\eta_{k}=\frac{\eta_{\mathrm{max}}}{1+\gamma k}$, where $\eta_{\mathrm{max}}<\min\left\{\frac{2}{\beta+\mu},\frac{(\tau-\Delta)\mu}{\beta^2[(1+\lambda_+)^{\tau}-1-\tau\lambda_+]}\right\}$, there exists constants $C_1$, $C_2$, $C_3$, $K_1$, $K_2$ and $\lambda_\pm$ such that $(e_1^{(t_{k+1})})^2$, $e_2^{(t_{k+1})}$ and $e_3^{(t_{k+1})}$ across global synchronizations in {\tt DFL} can be bounded as 
\begin{align} \label{eq:e1_main_m}
    (e_1^{(t_{k+1})})^2
    \leq&
    \left(1-\eta_k/\eta_{\mathrm{max}}C_1\right)(e_1^{(t_k)})^2
    % \nonumber \\&
    +\eta_{k}^2 (\tau-(1-\alpha)\Delta)(\sigma^2+\phi^2),
    % \nm{ERROR, SEE\ BELOW}
\end{align} 
\begin{align} \label{eq:e2_main_m}
    e_2^{(t_{k+1})}&\leq 
    % \nonumber \\&
    \alpha(1+\lambda_+)^{\tau}e_2^{(t_k)}
    +\eta_k\alpha 2\omega C_2e_3^{(t_k)}
    % \nonumber\\& 
    +\eta_k\alpha K_1\delta,
\end{align}
\begin{align} \label{eq:e3_sync_m}
     &  e_3^{(t_{k+1})}\leq
    (1-\eta_k\beta C_3) e_3^{(t_k)}
    +C_2 \eta_ke_2^{(t_k)}
    +\eta_k^2 K_2\delta,
\end{align}
 where the expressions of the constants are provided in Appendix~\ref{app:main_gap}.
\end{proposition}
\begin{skproof}
% \nm{I honestly don't find this sketch very helpful, since as a reviewer I will not have access to the appendix (and I may not want to download the arxiv paper and look at it). I think it is better to provide explicitly here the one step dynamics of Lemma 1 and provide their intuition, then maybe the rest is easier to follow.}
The complete proof is provided in Appendix~\ref{app:main_gap}. To prove Proposition~\ref{prop:An_oneSTP_t}, we first use Lemma~\ref{lem:An_oneSTP} to derive the one-step dynamics of $e_1^{(t+1)}$, $e_2^{(t+1)}$, and $e_3^{(t+1)}$ across the local model training period. Next, we apply the one-step dynamics from Lemma~\ref{lem:An_oneSTP} repeatedly to solve the coupled dynamics and obtain the recurrence relationship of $e_1^{(t_{k+1})}$, $e_2^{(t_{k+1})}$, and $e_3^{(t_{k+1})}$ across the global synchronization periods as follows:
\begin{align} \label{eq:e1_sync_m}
\hspace{-0.1in}
    (e_1^{t_{k+1}})^2
    \leq [(1-\alpha)(1-\mu\eta_{k})^{2(\tau-\Delta)}+\alpha(1-\mu\eta_{k})^{2\tau}](e_1^{(t_k)})^2
    % \nonumber\\&
    +[\tau-(1-\alpha)\Delta]\eta_{k}^2(\sigma^2+\phi^2),
    \hspace{-0.1in}
\end{align} 
\begin{align} \label{eq:e2_sync_m}
    e_2^{(t_{k+1})}&\leq 
    \alpha\Pi_{+,t_{k+1}}e_2^{(t_k)}
    +\alpha\frac{4\omega}{\sqrt{8\omega+1}}[\Pi_{+,t_{k+1}}-1]e_3^{(t_k)}
    % \nonumber \\&
    % \nonumber\\& 
    +\alpha\frac{\mu}{-\beta^2\lambda_+\lambda_-}[\Pi_{+,t_{k+1}}-1]\delta,
\end{align}
\begin{align} \label{eq:e3_init_m}
         &e_3^{(t_{k+1})}\leq
        \Psi_1(\eta_k) e_3^{(t_k)}
          % \nonumber \\&
        +\underbrace{2g_{3}[(1-\alpha)\Pi_{+,t_{k+1}-\Delta}+\alpha\Pi_{+,t_{k+1}}-1]}_{(a)}e_2^{(t_k)}
        \nonumber\\&
        % +[g_{5}(\Pi_{+,t}-1)+g_{6}(\Pi_{-,t}-1)]
        % [\sigma/\beta+\sum\limits_{d=1}^N\varrho_{d}\epsilon_{d}^{(0)}] \nonumber\\&
        +\underbrace{\left[(1-\alpha)[g_{5}(\Pi_{+,t_{k+1}-\Delta}-1)+g_{6}(\Pi_{-,t_{k+1}-\Delta}-1)]
        +\alpha[g_{5}(\Pi_{+,t_{k+1}}-1)+g_{6}(\Pi_{-,t_{k+1}}-1)]\right]}_{(b)}\delta/\beta
    \end{align}
with the expression of $\Psi_1(\cdot)$, $g_3$, $g_5$, $g_6$, and $\Pi_{\{+,-\},t}=[1+\eta_{k}\beta\lambda_{\{+,-\}}]^{t-t_{k}}$ provided in Lemma~\ref{lem:main} in Appendix~\ref{app:lemmas}. Utilizing the convexity of $[(1-\alpha)(1-\mu\eta_{k})^{2(\tau-\Delta)}+\alpha(1-\mu\eta_{k})^{2\tau}]$ in~\eqref{eq:e1_sync_m} and $\Pi_{+,t_{k+1}}$~\eqref{eq:e2_sync_m}, we are able to derive the bounds presented in~\eqref{eq:e1_main_m} and~\eqref{eq:e2_main_m}.
% \begin{align} \label{eq:cvx_eta_m}
% &(1-\alpha)(1-\mu\eta_{k})^{2(\tau-\Delta)}+\alpha(1-\mu\eta_{k})^{2\tau}
% \leq
% 1-\eta_k/\eta_{\mathrm{max}}C_1,
% \end{align}
% Utilizing the result in~\eqref{eq:cvx_eta_m}, we can simplify the recurrence relationship of $e_1^{(t_{k+1})}$ as shown in~\eqref{eq:e1_main_m}. Similarly, we can bound $e_2^{(t_{k+1})}$ by applying the convexity of $\Pi_{+,t_{k+1}}$
% % \nm{when I read this I have no idea what Pi is... again, remember that the manuscript you submit will NOT include the appendix. The paper needs to be self-standing} with respect to $\eta_k\beta$ which results in the bound $\Pi_{+,t_{k+1}}\leq1+ \eta_k\beta [(1+\lambda_+)^{\tau}-1]$. 
% Applying this to the recurrence relationship of $e_2^{(t_{k+1})}$ yields~\eqref{eq:e2_main_m}. 
 Finally, considering~\eqref{eq:e3_init_m},
    we bound $\Psi_1(\eta_k)$ as follows: 
    Since $\lambda_-\leq\lambda_+$, $\eta_k\leq\eta_{\mathrm{max}}$
    and $\eta_k\beta\leq1$, we apply the binomial theorem along with a set of algebraic manipulations to get
   %  \begin{align}
   %      &\frac{g_{1}\Pi_{+,t_{k}+\ell}+g_{2}\Pi_{-,t_{k}+\ell}-1}{\eta_k\beta}
   %      \leq -\ell\mu/\beta +\eta_{\mathrm{max}}\beta[(1+\lambda_+)^\ell-1-\ell\lambda_+],
   %  \end{align}
   % which implies that
    \begin{align}
        &\frac{\Psi_1(\eta_k)-1}{\eta_k\beta}
        \leq 
        -(\tau-\Delta)\mu/\beta 
        + \eta_{\mathrm{max}}\beta[(1+\lambda_+)^{\tau}-1-\tau\lambda_+]
        \triangleq -C_3,
    \end{align}
    and therefore $\Psi_1(\eta_k)\leq 1-\eta_k\beta C_3<1$. We then proceed by bounding $(a)$ in~\eqref{eq:e3_init_m}. Using the convexity of $\Pi_{+,t}-1$ with respect to both $\eta_k\beta$ and $\eta_{\mathrm{max}}\beta$ with the constraint $\eta_{\mathrm{max}}\beta\leq1$, we obtain
$
        2g_3[(1-\alpha)\Pi_{+,t_{k+1}-\Delta}+\alpha\Pi_{+,t_{k+1}}-1]
        % \nonumber \\&
        % \leq
        % \eta_{k} \frac{2\beta}{\sqrt{8\omega+1}}[(1-\alpha)(1+\lambda_+)^{\tau-\Delta}+\alpha(1+\lambda_+)^{\tau}-1]
        \leq 
        \eta_k C_2.
$
Finally, we bound $(b)$ in~\eqref{eq:e3_init_m}, using the binomial expansion and the expressions of $g_5$ and $g_6$, which yields
    $
    g_{5}(\Pi_{+,t}-1)+g_{6}(\Pi_{-,t}-1)
    % =(\eta_k\beta)^2\frac{1}{\sqrt{1+8\omega}}\sum_{\ell=0}^{t-t_k-2}\left(\begin{array}{c}t-t_k\\\ell+2\end{array}\right)(\eta_k\beta)^{\ell}[\lambda_+^{\ell+1}-\lambda_-^{\ell+1}]
    \leq \eta_k^2\beta K_2.
    $
Replacing these bounds in~\eqref{eq:e3_init_m} leads to the final result in~\eqref{eq:e3_sync_m}.
\end{skproof}

Proposition~\ref{prop:An_oneSTP_t} provides insight about the impact of subnets on the convergence of model training by bounding the subnet deviation error $(e_1^{(t_{k+1})})^2$ in~\eqref{eq:e1_main_m}. It shows that $(e_1^{(t_{k+1})})^2$ is (i) dependent on the optimality gap of the subnet error-free variable (i.e., $\mathbb E[\Vert\bar{\mathbf v}_c^{(t)}-\mathbf w^*\Vert^2],~\forall t\in\mathcal T_k$ encapsulated in $\phi$), (ii) sensitive to SGD noise and intra-subnet data diversity encapsulated in $\phi$ (the bound increases as $\sigma$, $\delta_c$ and $\omega_c$ increase), and (iii) getting larger as the period of local aggregation (i.e., $t^{\mathsf{L}}_{k}$) increases. To demonstrate the convergence of $(e_1^{(t_{k+1})})^2$, it is necessary to ensure Assumption~\ref{assump:sub_err} is satisfied by monitoring the dynamic of $\mathbb E[\Vert\bar{\mathbf v}_c^{(t)}-\mathbf w^*\Vert^2]$ and selecting appropriate local aggregation instances $\{\Theta_c^{(t)}\}_{t\in\mathcal T_k}$. Our control algorithm in Sec.~\ref{Sec:controlAlg} demonstrates how this can be achieved. Proposition~\ref{prop:An_oneSTP_t} also reveals the evolution of $e_2^{(t_k)}$ and $e_3^{(t_k)}$ across global synchronizations. The bounds~\eqref{eq:e2_main_m} and~\eqref{eq:e3_sync_m} demonstrate that $e_2^{(t_k)}$ and $e_3^{(t_k)}$ are (i) interdependent, forming a coupled relationship, and (ii) influenced by the inter-subnet data diversity (the bounds increase as $\delta$ increases for a fixed value of $\omega$). The bound in \eqref{eq:e2_main_m} also establishes a necessary condition for the combiner weight $\alpha$ to exhibit a contraction behavior during training. In particular, to achieve this behavior, $\alpha$ must be strictly less than $\frac{1}{(1+\lambda_+)^\tau}$. This condition is crucial for {\tt DFL} to achieve convergence and will be further used in Theorem~\ref{thm:subLin_m}.
% We will later use this to design the period of local aggregation in our control algorithm in Sec.~\ref{Sec:controlAlg}. 

% \addFL{We further define the subnet average deviation error bound $\epsilon^{(t)}$ as 
%      \begin{equation}\label{def:eps}
%           (\epsilon^{(t)})^2 \triangleq \sum\limits_{c=1}^N \varrho_c(\epsilon_c^{(t)})^2.
%      \end{equation}
% }
% This proposition provides the upper bound of $\sum\limits_{j\in\mathcal S_c}\rho_{j,c}\mathbb E_{t_{k-1}}[\Vert \mathbf e_j^{(t)} \Vert^2]$. The bound demonstrates that, under given condition on learning rate, i.e., $\eta_t=\frac{\gamma}{t+\Lambda}$, $(\epsilon_c^{(t)})^2 \leq \mathcal{O}(\eta_t)$ is satisfied (since it can be verified in Theorem~\ref{thm:subLin} that $(\Sigma_{\{c,+\},t})^2$ is in the order of $\eta_t$, i.e., $(\Sigma_{\{c,+\},t})^2\leq\mathcal{O}(\eta_t)$). This results from the fact that, as model training proceeds, the value of $\Vert\bar{\mathbf w}_c^{(t_{k-1})}-\mathbf w^*\Vert$ is expected to converge to a certain finite value as the global model converges. we can apply the result in Appendix~\ref{app:clust_div} (Lemma~\ref{lem:clust_div}) to control the value of $\epsilon^{(t)}$ through the number of local aggregations in each subnet $c$ perform between two consecutive global aggregations. We will use this property in the development of our control algorithm to determine the instants for performing local aggregations and the total round of local aggregations ($\Gamma_c$) required in each local model training interval in Sec.~\ref{Sec:controlAlg}.



% \nm{This is confusing: here you state a (strong) assumption on A, but you say that later in Prop 4 you are going to prove it..}
% \subsubsection{Characterizing Model Dispersion and Optimality Gap of the ML Model}
% We next characterize $A^{(t)}$ and $B^{(t)}$ appearing in~\eqref{eq:defA} and in~\eqref{eq:defB}:


% In the following of this section, we will demonstrate two main observations: (i) the effect of imperfect synchronization resulted from the local-global combiner design will be revealed in the upper bound of subnet error deviation $A^{(t)}$ (Lemma~\ref{Local_disperse})  

% considering the right-hand side of the bound in~\eqref{eq:disp_mainT}, combiner weight $\alpha$ has a mixed impact on the bound. On the one hand, the coefficient of the first term (i.e., $(a)$) gets smaller as $\alpha$ decreases. On the other hand, the coefficients of the rest of the terms (i.e., $(b)$ and $(c)$) get larger as $\alpha$ decreases. \addFL{This is due to the fact that, as $\alpha$ decreases, {\tt DFL} updates the ML model with more emphasis on the local models, reducing the effect on the global aggregated model. This is reflected on $(i)$ the decrease of the coefficients for the optimality gap of the outdated global aggregated model (i.e., $(a)$) and $(ii)$ the increase of the coefficients for $A^{(t)}$, $B^{(t)}$, $\epsilon^{(t)}$ and $\sigma$ (i.e., $(b)$ and $(c)$), which characterizes the instant behavior of the up-to-date local models. }

% and, more importantly, (iii) increase as upper bound on the initial subnet deviation error  (i.e., $\epsilon^{(0)}$) and the expected deviation of the subnet average model and the global aggregated model at the end of local model training (i.e., $A^{(t_{k-1})}$) grow large, which in turn encapsulate the impact of local aggregations $\{\Theta_c^{(t)}\}_{t\in\mathcal T_k} ~\forall k$ and the local-global combiner weight $\alpha\in(0,1]$. In practice, $\{\Theta_c^{(t)}\}_{t\in\mathcal T_k} ~\forall k$ and $\alpha\in(0,1]$ are \textit{control parameters} that need to tuned to control the value of $A^{(t)}$. 

% Nevertheless, considering the staleness of the global aggregated model in~\eqref{eq:aggr_alpha}, 
% \addFL{To further investigate the impact of local aggregations $\{\Theta_c^{(t)}\}_{t\in\mathcal T_k} ~\forall k$ and the local-global combiner weight $\alpha\in(0,1]$ on $A^{(t)}$ and $B^{(t)}$, we consider $A^{(t_{k-1})}$ as the \textit{average synchronization error} and impose the following Proposition.} 

\subsection{General Convergence Behavior of {\tt DFL}}
\label{ssec:convAvg}

% We next pursue two goals: \underline{(G-1)}\nm{what does G- stand for? Unclear} showing that the global deployed model under {\tt DFL} can attain sub-linear convergence to the optimal model, and \underline{(G-2)} demonstrating that the \addFL{Propositions} made during the convergence analysis are realistic. We achieve (G-1) via first characterizing the optimality gap of
% model training under {\tt DFL} followed by investigating the behavior of model dispersion and optimality gap of ML model. We achieve (G-2) via characterizing the synergies between delay $\Delta$ and the average subnet deviation error $\epsilon^{(t)}$ followed by average synchronization error $\Psi$.
Using the aforementioned results, 
we will next demonstrate that the global model deployed under {\tt DFL} can achieve sublinear convergence to the optimum model.

% Theorem \ref{co1} illustrates how the global parameter $\bar{\mathbf w}(t_k)$ converges to the optimal $\mathbf w^*$ with respect to each global update in {\tt DFL} and how the learning and system parameters affects this convergence. 

\begin{theorem}[Sublinear Convergence of {\tt DFL}] \label{thm:subLin_m}
 Under Assumptions \ref{beta},~\ref{assump:SGD_noise} and~\ref{assump:sub_err}, if $\eta_{k}=\frac{\eta_{\mathrm{max}}}{1+\gamma k},~\forall k$ and $\vert\mathcal T_k\vert\leq\tau,~\forall k$, there exists constants $Y_1$, $Y_2$ and $Y_3$ such that the distance between the global model and the optimal model at each global synchronization in {\tt DFL} can be bounded as 
    \begin{align}\label{eq:main_m}
        \mathbb E[\Vert\bar{\mathbf w}^{(t_k)}-\mathbf w^*\Vert^2]
        \leq \underbrace{2Y_1^2 \eta_k}_{(a)}+\underbrace{2Y_3^2\eta_k^2}_{(b)},
    \end{align}
    % \nm{??? Isnt it $\mathbb E[\Vert\bar{\mathbf w}^{(t_k)}-\mathbf w^*\Vert^2]\leq (e1+e3)^2\leq 2e1^2+2e3^2\leq 2Y_1^2 \eta_k+2Y_3^2\eta_k^2$? I think you need to keep them separate since they show a stronger result: the impact of delta decays faster with k.}
    where 
    $\eta_{\mathrm{max}}<\min\left\{\frac{2}{\beta+\mu},\frac{(\tau-\Delta)\mu}{\beta^2[(1+\lambda_+)^{\tau}-1-\tau\lambda_+]}\right\}$, $\gamma<\min\left\{1-(1-\mu\eta_{\mathrm{max}})^{2(\tau-\Delta)},C_3\eta_{\mathrm{max}}\beta\right\}$,
    \begin{align}
        \alpha<\alpha^* \triangleq \frac{1}{\frac{C_2\eta_{\max}^2}{\eta_{\max}\beta C_3-\gamma}
         2\omega C_2(1+\gamma)
        +(1+\gamma)(1+\lambda_+)^{\tau}},
    \end{align}
    with the expression of the constants provided in Appendix~\ref{app:subLin}.
    % \begin{align}\label{eq:Y1}
%         Y_1 \triangleq\sqrt{\frac{(\tau-(1-\alpha)\Delta)(\sigma^2+\phi^2)\eta_{\mathrm{max}}}{C_1-\gamma}},
%     \end{align}
%     \begin{align}\label{eq:Y2}
%     Y_2\triangleq\max\left\{
%     \frac{\eta_{\mathrm{max}}^2\alpha 2\omega C_2(1+\gamma)
%     e_3^{(0)}
%     +\alpha K_1\delta(1+\gamma)
%     }{1-\alpha(1+\gamma)(1+\lambda_+)^{\tau}},
%     \frac{\frac{ K_1\delta}{\eta_{\mathrm{max}} 2\omega C_2}
%     +\frac{K_2\delta}{\beta C_3-\gamma}
%     }{
%     \frac{[1-\alpha(1+\gamma)(1+\lambda_+)^{\tau}]}{\eta_{\mathrm{max}}\alpha 2\omega C_2(1+\gamma)}
%     -\frac{C_2}{\beta C_3-\gamma}
%     }\right\},
% \end{align}
%     % \begin{align}
%     %     Y_2 \triangleq\frac{\eta_{\mathrm{max}}K_1\nm{K1(alpha??)}Y_3(1+\gamma)+K_2\delta(1+\gamma)}{1-\alpha(1+\gamma)(1+\lambda_+)^{\tau}},
%     % \end{align}
%     \begin{align}\label{eq:Y3}
%         Y_3 \triangleq \max\left\{\eta_{\mathrm{max}}e_3^{(0)},
%         \frac{[C_2Y_2+K_2\delta]\eta_{\max}}{\eta_{\max}\beta C_3-\gamma}\right\}.
%     \end{align}
%     with $K_1=\frac{\mu}{-\beta\lambda_+\lambda_-}[(1+\lambda_+)^{\tau}-1]$, $K_2=\frac{\beta}{\sqrt{1+8\omega}}\sum_{\ell=0}^{\tau-2}\Big(\begin{array}{c}\tau\\\ell+2\end{array}\Big)[\lambda_+^{\ell+1}-\lambda_-^{\ell+1}]$, $C_1=1-((1-\alpha)(1-\mu\eta_{\mathrm{max}})^{2(\tau-\Delta)}+\alpha(1-\mu\eta_{\mathrm{max}})^{2\tau})$, $C_2=\frac{2\beta}{\sqrt{8\omega+1}}[(1+\lambda_+)^{\tau}-1]$, $C_3=(\tau-\Delta)\mu/\beta 
%         - \eta_{\mathrm{max}}\beta[(1+\lambda_+)^{\tau}-1-\tau\lambda_+]$ and $\lambda_{\pm} =\frac{1}{2}-\frac{\mu}{\beta}\pm\frac{\sqrt{8\omega+1}}{2}$.

    % \nm{you need to tell where these vars are defined.}
    % \nm{since K1 and K2 are prop to alphha redefine $K_1\gets K_1(1)$, so that $K_1(\alpha)$ becomes $K_1\alpha$. Same for K2.}
\end{theorem}

\begin{skproof}
The complete proof is provided in Appendix~\ref{app:subLin}. 
We first obtain that \\
$
    \sqrt{\mathbb E[\Vert\bar{\mathbf w}^{(t_{k})}-\mathbf w^*\Vert^2]} 
    \leq e_1^{(t_k)}+e_3^{(t_k)},
$
and thus
\begin{align}\label{eq:final_m}
    \mathbb E[\Vert\bar{\mathbf w}^{(t_k)}-\mathbf w^*\Vert^2]\leq (e_1^{(t_k)}+e_3^{(t_k)})^2\leq 2(e_1^{(t_k)})^2+2(e_3^{(t_k)})^2.
\end{align}
We will prove~\eqref{eq:main_m} by induction, showing that $e_1^{(t_k)}\leq Y_1\sqrt{\eta_k}$, $e_2^{(t_k)}\leq Y_2\eta_k$, and $e_3^{(t_k)}\leq Y_3\eta_k$. The base of induction trivially holds since $Y_1\geq0$, $Y_2\geq0$, and $Y_3\geq\eta_{\mathrm{max}}e_3^{(0)}$ at the start of training ($k=0$). For the induction step, assume that the statement holds true for some $k \in \mathbb{N}$. We then show that it also holds for $k+1$. 
To show $e_1^{(t_{k+1})}\leq\sqrt{\eta_{k+1}}Y_1$, we use~\eqref{eq:e1_main_m}  and the induction hypothesis ($e_1^{(t_{k})}\leq\sqrt{\eta_{k}}Y_1$), yielding the sufficient condition for all $k\geq 0$:
%  \begin{align} 
% &\left(1-\eta_k/\eta_{\mathrm{max}}C_1\right)\eta_{k}Y_1^2
%     % \nonumber \\&
%     +\eta_{k}^2(\tau-(1-\alpha)\Delta)(\sigma^2+\phi^2)
%     -\eta_{k+1}Y_1^2\leq 0.
% \end{align}
% Using the expression of $\eta_k=\frac{\eta_{\mathrm{max}}}{1+\gamma k}$, the above condition is equivalent to
% \begin{align}
%     &\left(1-\eta_k/\eta_{\mathrm{max}}C_1\right)\eta_{k}Y_1^2
%     % \nonumber \\&
%     +\eta_{k}^2(\tau-(1-\alpha)\Delta)(\sigma^2+\phi^2)
%     -\eta_{k+1}Y_1^2\leq 0.
% \end{align}
% The above condition is equivalent to satisfying the following condition for all $k\geq 0$:
$$
\eta_{\mathrm{max}}(\tau-(1-\alpha)\Delta)(\sigma^2+\phi^2)
\leq 
[C_1-\gamma]Y_1^2,
$$
which is verified since 
$\gamma<1-(1-\mu\eta_{\mathrm{max}})^{2(\tau-\Delta)}\leq C_1$
% }
 and $Y_1^2=\frac{(\tau-(1-\alpha)\Delta)(\sigma^2+\phi^2)\eta_{\mathrm{max}}}{C_1-\gamma}.$
Thus we can show for $e_1^{(t_k)}$ that $(e_1^{(t_k)})^2\leq\eta_{k}Y_1^2,~\forall k$.
To show $e_2^{(t_{k+1})}\leq\eta_{k+1}Y_2$, we use~\eqref{eq:e2_main_m} and the induction hypothesis ($e_2^{(t_{k})}\leq\eta_{k}Y_2$), yielding 
 \begin{align} 
&\alpha(1+\lambda_+)^{\tau}\eta_k Y_2 
    +\alpha 2\omega C_2\eta_k^2 Y_3
    % \nonumber\\& 
    +\alpha K_1\eta_k\delta
    -\eta_{k+1}Y_2\leq 0,
\end{align} 
To satisfy the above condition for all $k\geq 0$, it is enough to have
 \begin{align}\label{eq:e2_cond2_m}
    & \frac{[1-\alpha(1+\gamma)(1+\lambda_+)^{\tau}]Y_2-\alpha K_1\delta(1+\gamma)}{\eta_{\mathrm{max}}}
    -\alpha 2\omega C_2Y_3(1+\gamma)
    \geq 0.
 \end{align} Holding on proving the final results of the upper bound on $e_2^{(t_{k+1})}$, we take a look at $e_3^{(t_{k+1})}$
 to show $e_3^{(t_{k+1})}\leq\eta_{k+1}Y_3$. We use~\eqref{eq:e3_sync_m} and the induction hypothesis ($e_3^{(t_{k})}\leq\eta_{k}Y_3$), yielding 
\begin{align}
    (1-\eta_k\beta C_3)Y_3\eta_k
      % \nonumber \\&
    +[C_2Y_2+K_2\delta] \eta_k^2
    - Y_3\eta_{k+1}\leq 0.
\end{align}
The above condition is equivalent to satisfying the following condition for all $k\geq 0$:
\begin{align} \label{eq:cond_Y3_m}
    Y_3[\gamma -\eta_{\max}\beta C_3]
    +[C_2Y_2+K_2\delta]\eta_{\max}
    \leq 0.
\end{align}
To demonstrate $e_2^{(t_k)}\leq\eta_{k+1}Y_2$ and $e_3^{(t_k)}\leq\eta_{k+1}Y_3$, it is necessary that conditions $e_3^{(t_k)}\geq \eta_{\mathrm{max}}e_3^{(0)}$, \eqref{eq:e2_cond2_m}, and~\eqref{eq:cond_Y3_m} hold simultaneously, which can be achieved by noting that $\alpha<\alpha^*$ and utilizing the definition of $Y_2$ in~\eqref{eq:Y2}.
Completing the induction, we show that $e_2^{(t_k)}\leq\eta_{k+1}Y_2$ and $e_3^{(t_k)}\leq\eta_{k+1}Y_3$. Finally, substituting these results back into~\eqref{eq:final_m} completes the proof.
\end{skproof}
 
 
%  To obtain these limits, we first express~\eqref{eq:finTransform} as follows:

%  Upon $t\to\infty$ considering the dominant terms yields
%  \begin{align}\label{eq:condinf}
%      &-\tilde{\gamma}\tilde{\mu}\nu t
%     +Y_2 \tilde{\gamma}^2t
%     +\nu t
%     \leq0
%     \nonumber \\&
%     \Rightarrow
%     \left[1-\tilde{\gamma}\tilde{\mu}\right]\nu t
%     +Y_2 \tilde{\gamma}^2 t
%     \leq 0.
%  \end{align}
% To satisfy~\eqref{eq:condinf}, the necessary condition is given by:
% \begin{equation}
%     \tilde{\mu}\tilde{\gamma}-1>0,
% \end{equation}
%  \begin{align} \label{eq:nu1}
%      \nu \geq \frac{\tilde{\gamma}^2Z_2}{\tilde{\mu}\tilde{\gamma}-1}.
%  \end{align}
%  Also, upon $t\rightarrow 0$, from~\eqref{eq:fin2} we have
%  \begin{align}
%     &\left(-\tilde{\mu}\tilde{\gamma}\alpha+Y_1\omega^2\tilde{\gamma}^2 \right)\nu
%     +Y_2 \tilde{\gamma}^2\alpha
%     +\nu[\alpha-1]
%     +\frac{\nu}{1+\alpha}\leq0
%     \nonumber \\&
%     \Rightarrow
%     \nu\left(\alpha(\tilde{\mu}\tilde{\gamma}-1)+\frac{\alpha}{1+\alpha}-Y_1\omega^2\tilde{\gamma}^2\right)
%     \geq
%   \tilde{\gamma}^2 Y_2\alpha,
%  \end{align}
% which implies the following conditions
% \begin{align}
%     \omega
%     <
%     \frac{1}{\tilde{\gamma}}\sqrt{\alpha\frac{\tilde{\mu}\tilde{\gamma}-1+\frac{1}{1+\alpha}}{Y_1}},
% \end{align}
% % \nm{note that we can make this arbitrarily large by making $\alpha$ large for ANY tau.. in fact, $Y_1\to 32\gamma/\mu(\tau-1)$ for $\alpha\to\infty$}
% and
% \begin{align} \label{eq:nu2}
%     \nu\geq\frac{Y_2\alpha}{Y_1\left(\omega_{\max}^2-\omega^2\right)}.
% \end{align}
% Combining~\eqref{eq:nu1} and~\eqref{eq:nu2}, when $\omega<\omega_{\max}$
% and
% \begin{align}
%     \nu \geq Y_2\max\{\frac{\beta^2\gamma^2}{\mu\gamma-1},
%     \frac{\alpha}{Y_1\left(\omega_{\max}^2-\omega^2\right)}\},
% \end{align}
% completes the induction and thus the proof.
%   This concludes the proof. For the detailed proof, refer to Appendix~\ref{app:subLin}.  

% \begin{lemma}
%     Under assumption \ref{beta}, \ref{PL} and \ref{gradDiv}, there exist some $\eta_k^*$ such that for $\eta_k\leq\eta_k^*$, the following condition
%     \begin{align*}
%         \eta_k\leq\frac{1}{\beta+\frac{2\beta^2\delta\tau_k}{\mu(1-\mu\eta_k)^{\tau_k-1}}}
%     \end{align*}
%     holds.
% \end{lemma}

% \begin{proof}
    
% \end{proof}



% For $t\in\cup_{n=1}^k (\mathcal T_n\setminus \mathcal H_k)$, the local models perform gradient update without consensus. These local models can be deemed as models with perfect average within one subnet $c$ plus an additional consensus error with $\Gamma_c(t)=0$, which is upper bounded by $s_c^{(k)}^2\epsilon_c^2(t)\leq\sigma_c^{(t)}$, resulting in (b).

\iffalse
\begin{theorem} \label{subLin}
    Under Assumption \ref{beta} and realization of {\tt DFL}, for $\gamma>1/\mu$, $\eta_t=\frac{\gamma}{t+\alpha}$\nm{is this compatible with $\eta\leq \kappa/\beta$? (needed in Thm 1)},\nm{notation! time indices k and t are used to denote different timescales.} $\epsilon^2(t)=\eta_t\phi$\nm{what is phi?}, 
    % $\eta_0\leq \min \Big\{\kappa/\beta, 1/\left(\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\beta\right)\Big\}$ 
        \nm{$\epsilon^2(k)$ and $\eta_k$?}
    and
    $\tau_k<\log_4\frac{(\mu\gamma-1)\kappa}{16\omega^2\beta^2\gamma^2}$, where 
    $\alpha\geq\max\{\beta\gamma/\kappa, \beta\gamma\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\},
    $ we have
    \begin{align}
        \mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right] \leq \frac{\Gamma}{t+\alpha},
    \end{align}
    \nm{why do you care for this condition to hold for all t? and not just at the end of the sync times In other words, using Thm1, I just want to find conditions such that
    \begin{align}
&    \underbrace{\left((1-\mu\eta_k)^{\tau_k}+16\eta_k\omega^2\beta/\kappa\left[\frac{(1-\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\right)}_{(a)}\frac{\Gamma}{t_{k-1}+\alpha}
    \nonumber \\&
    + \frac{\eta_k\beta^2}{2}\left[\frac{(1+\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\{153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
    \nonumber \\&
    +\frac{1-(1-\mu\eta_k)^{\tau_k}}{2}\kappa[\eta_k\sigma^2+\beta\epsilon^2(k)]
    +\frac{\beta}{2}\epsilon^2(k)
    \leq\frac{\Gamma}{t_{k}+\alpha}
\end{align}
    }
    where $\Gamma\geq\max\bigg\{\alpha[F(\bar{\mathbf w}(0))-F(\mathbf w^*)],\frac{\gamma^2\beta}{2}[Q_k A+B]/[\mu\gamma-(1+16\omega^2\beta^2\gamma^2 Q_k/\kappa)]+\beta\phi\gamma/2\bigg\}$, $A=(16\omega^2/\kappa+153/16)\beta\phi+81/16\sigma^2+\delta^2$, $B=\sigma^2+\beta\phi$ and $Q_k=4^{\tau_k}$.
\end{theorem}

\nm{I would rewrite the Thm as follows:}
\add{
\begin{theorem} \label{subLin}
Define
$A=(16\omega^2/\kappa+153/16)\beta\phi+81/16\sigma^2+\delta^2$, $B=\sigma^2+\beta\phi$
and 
let $\gamma>1/\mu$,
$\alpha\geq\beta\gamma\max\{\kappa^{-1}, 1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\}
    $, $\phi>0$  and
$\Gamma\geq\max\{\alpha[F(\bar{\mathbf w}(0))-F(\mathbf w^*)],\beta\phi\gamma/2+\frac{\gamma^2\beta}{2[\mu\gamma-1]}B\}$
be design parameters.
Then,
    under Assumption \ref{beta} and any realization of {\tt DFL}, by choosing  $\eta_k=\frac{\gamma}{k+\alpha}$, $\epsilon^2(k)=\eta_k\phi$
    and
    $$\tau_k\leq
    \min\{\log_4\frac{(\mu\gamma-1)\kappa}{16\omega^2\beta^2\gamma^2},
    \log_4\left(\frac{[\Gamma-\beta\phi\gamma/2]
    [\mu\gamma-1]
    -\frac{\gamma^2\beta}{2}B}{
    \frac{\gamma^2\beta}{2} A
+[\Gamma-\beta\phi\gamma/2]16\omega^2\beta^2\gamma^2/\kappa
    }\right)\}\ \forall k
    $$
    \nm{I think the second term in the min is always stricter...}
    \nm{you further need the condition
    $
    \Gamma
    >\frac{\gamma^2\beta}{2[\mu\gamma-1]}B+\beta\phi\gamma/2
    $ (introduced earlier)
    }
    we have that
    \begin{align}
        \mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right] \leq \frac{\Gamma}{t+\alpha},
    \end{align}
    where
\end{theorem}
}

\begin{skproof}
In this proof, we first demonstrate sublinear convergence in each aggregation period with respect to $t$ by showing that $\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]\leq\frac{\bar{\nu}}{t+\alpha}$ for some $\bar{\nu}$ for $t=0,1,\dots,T$ by induction, where $\nu_0=\alpha[F(\bar{\mathbf w}(0))-F(\mathbf w^*)]$ and $\nu=\frac{\gamma^2\beta}{2}[Q_k A+B]/[\mu\gamma-(1+16\omega^2\beta^2\gamma^2 Q_k/\kappa)]$ at $t\in\mathcal T_k$ by induction:
\nm{What is the point of having Thm1 if you never use it? I think Thm1 should provide the 1 step analysis, so that you can use that to prove this Thm}

    For $m=1$,\nm{m already used} $\mathbb E\left[F(\bar{\mathbf w}(m))-F(\mathbf w^*)\right]\leq\frac{\nu_0}{m+\alpha}$ holds based on the definition. 
    Assume $\mathbb E\left[F(\bar{\mathbf w}(m))-F(\mathbf w^*)\right]\leq\frac{\nu}{m+\alpha}$ holds for $m=t$, we now show that $\mathbb E\left[F(\bar{\mathbf w}(m))-F(\mathbf w^*)\right]\leq\frac{\nu}{m+\alpha}$ holds for $m=t+1$.
    
    From \eqref{26} and \eqref{39}, we have   
    \nm{I dont understand, isnt eta constant within a synch period? See eq 12}
    \begin{align} \label{wbarM}
        &\mathbb E\left[F(\bar{\mathbf w}(t+1))-F(\mathbf w^*)\right]
        \nonumber \\&
        \leq 
        (1-\mu\eta_t)\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]
        +
        16\omega^2\beta\eta_t/\kappa[\lambda_1^{2(t-t_{k-1})}-1][F(\bar{\mathbf w}(t_{k-1}))-F(\mathbf w^*)+\frac{\beta}{2}\epsilon^2(t)]
        \nonumber \\&
        +\frac{\eta_t\beta^2}{2}
        [\lambda_1^{2(t-t_{k-1})}-1]\{153/16\epsilon^2(t)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
        +\frac{\eta_t\beta}{2}[\eta_t\sigma^2+\beta\epsilon^2(t)].
    \end{align}
    Based on the condition of $\eta_t$ and $\epsilon(t)$, \eqref{wbarM} becomes bounded by 
    \begin{align} \label{condBound}
        &\mathbb E\left[F(\bar{\mathbf w}(t+1))-F(\mathbf w^*)\right]
        % \nonumber \\&
        \leq
        \frac{t+\alpha-1}{(t+\alpha)^2}\nu-\frac{\mu\gamma_k-\big[1+16\omega^2\beta^2\gamma_k^2 Q_k/\kappa\big]}{(t+\alpha)^2}\nu
        % \nonumber \\&
        +\frac{\gamma_k^2\beta}{2(t+\alpha)^2}
        \left[Q_k A+B\right].
    \end{align} 
    Then for some $\gamma>1/\mu$,
    $
        \tau_k<\log_4\frac{(\mu\gamma-1)\kappa}{16\omega^2\beta^2\gamma^2}.
    $
    and 
    $
        \nu 
        \geq 
        \frac{\gamma^2\beta}{2}[Q_k A+B]
        /
        [\mu\gamma-(1+16\omega^2\beta^2\gamma^2 Q_k/\kappa)],
    $
    we show that \eqref{condBound} is bounded by
    \begin{align} \label{sub_t+1}
        &\mathbb E\left[F(\bar{\mathbf w}(t+1))-F(\mathbf w^*)\right]
        \leq
        \frac{\nu}{t+\alpha+1},
    \end{align}
    and thus show that $\mathbb E\left[F(\bar{\mathbf w}(m))-F(\mathbf w^*)\right]\leq\frac{\nu}{m+\alpha}$ holds for $m=t+1$.
    Thus we have 
    \begin{align} \label{wbar_pfd2}
        &\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]
        \leq
        \frac{\bar{\nu}}{t+\alpha},~\forall t=0,1,\dots,T,
    \end{align}
    Now, we want to prove $\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]\leq\frac{\Gamma}{t+\alpha}$: \\
    For $t=0$, we have $\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right] \leq \frac{\Gamma_1}{t+\alpha}$ holds for $\Gamma_1=\alpha\left[F(\bar{\mathbf w}(0))-F(\mathbf w^*)\right]$. \\
    For $t>0$, we use the connection between $\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]$ and $\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]$ to obtain:
    \begin{align}
        &\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]  
        \leq \mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]
        +\frac{\beta}{2}\epsilon^2(t)
        \nonumber \\&
        \leq \frac{\nu}{t+\alpha} +\frac{\beta\epsilon^2(t)}{2}
        \leq \frac{\nu}{t+\alpha} +\frac{\eta_t\beta\phi}{2}.
    \end{align}
    Choose $\eta_t=\frac{\gamma}{t+\alpha}$, we then have
    \begin{align}
        &\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]  
        \leq \frac{\nu}{t+\alpha} +\frac{\beta\phi\gamma}{2(t+\alpha)}=\frac{\Gamma_2}{t+\alpha},
    \end{align}
    where $\Gamma_2=\nu+\beta\phi\gamma/2$ for $t>0$. Therefore, we show that, for $\Gamma\geq\max\{\Gamma_1,\Gamma_2\}$, we have 
    \begin{align}
        &\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]  
        \leq \frac{\Gamma}{t+\alpha}.
    \end{align}
    
    To further satisfy the initial condition of for $\eta_0\leq\min\{\kappa/\beta,1/\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\beta\}$, the range of $\alpha$ is given as 
    \begin{align}
        \alpha \geq \max\{\beta\gamma/\kappa, \beta\gamma\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\}.
    \end{align}
For more detailed proof, see Appendix A.
\end{skproof}
\fi


% Inspecting the conditions in the statement of Theorem~\ref{thm:subLin_m}, the feasible value for the step size $\eta_k$ decreases ($\Lambda$ increases) as the gradient diversity (i.e., $\delta,\omega$) becomes larger, meaning that to avoid divergence of model training, smaller step-sizes should be used upon having highly non-i.i.d data across the network.
% \nm{I would avoid using these "extreme" words} 

% effect of having larger gradient diversity would be tolerated by smaller step size to achieve convergence.
Investigating the bound in~\eqref{eq:main_m} of Theorem~\ref{thm:subLin_m}, we found conditions for {\tt DFL} to achieve a convergence rate of $\mathcal{O}(1/t)$. Note that the decay rate of $(a)$ is faster than $(b)$ across global synchronizations, indicating that the impact of inter-gradient diversity (i.e., $\omega$ and $\delta$) incorporated into $Y_3$ on the bound decays faster than that of intra-gradient diversity incorporated into $Y_1$.
% This aligns with the intuition that the convergence performance of {\tt DFL} is eventually dominated by the impact of SGD noise. 
The bound also demonstrates the effect of the combiner weight $\alpha$ on convergence. In particular, when $\Delta=0$, the optimal choice of $\alpha$ is the trivial solution (i.e., $\alpha=0$ since this choice leads to minimizing $Y_3$), indicating that it is optimal for {\tt DFL} to perform the standard {\tt FedAvg} algorithm without local-global model combination. Given a fixed value of $\omega$, as the gradient diversity (i.e., $\delta$) increases, it becomes more favorable to choose a smaller value of $\alpha$ to achieve better convergence, demonstrating the importance of putting a higher importance on the global model during local-global model combination to avoid having biased local models under data heterogeneity.  The value of the bound increases with respect to the subnet deviation noise (i.e., $\phi$), implying that {\tt DFL} can achieve the same performance under less frequent global aggregations with more frequent local aggregations. This further suggests that the local model training interval can be prolonged upon performing more rounds of local aggregations in between global aggregations. Moreover, the convergence bound shows the non-triviality of selecting $\alpha$ for the best convergence behavior (as discussed in Sec.~\ref{Sec:controlAlg}).

In the next section, we will leverage these relationships in developing an adaptive control algorithm for {\tt DFL} that tunes the algorithm parameters to achieve the convergence bound in Theorem~\ref{thm:subLin} while reducing the network costs.
% the information across the edge devices encapsulated in the delayed global aggregated model becomes more important than the up-to-date local models as the local models would be diverging from the optimum as the local datasets would be biased towards their own distribution.

% In addition, the bound also shows the impact of the selection of $\tau$ on the convergence. Increasing
% $\tau$ (i.e., less frequent global aggregations) results in a rapid increase of the value of $\nu$ (i.e., a worse convergence).
% Moreover, 

% On the one hand, the bound shows the impact of the duration of the local model training intervals (i.e., $\tau$ embedded in terms $Y_1$ and $Y_3$). In particular, it can be seen that increasing the maximum local model training interval results in a sharp increase in the upper bound. On the other hand, 

% Finally, in order for {\tt DFL} to achieve sublinear convergence, Theorem~\ref{thm:subLin} demonstrates that the instances of local aggregation $\{\Theta_c^{(t)}\}_{t\in\mathcal T_k} ~\forall k$ needs to be selected properly such that $(\epsilon^{(t)})^2=\eta_t\phi$ is satisfied, where $\phi>0$ denotes the subnet deviation error coefficient. 

% This condition will be used in developing the local aggregation scheme in the control algorithm.

% We next revisit two of the key assumptions made above to obtain the results $(\epsilon^{(t)})^2=\eta_t\phi$ for a finite positive constant $\phi$ and a bounded value for $\Psi$ both of which are used in Theorem~\ref{thm:subLin}.

% \subsubsection{Validity of Assumptions}
% We first aim to show that under proper choices of the frequency of local aggregations, $(\epsilon^{(t)})^2=\eta_t\phi$ (assumed in Theorem~\ref{thm:subLin}) holds for a finite positive constant $\phi$. To this end, considering~\eqref{def:eps_c} and~\eqref{def:eps}, it is enough to show that $(\epsilon_c^{(t)})^2 \leq \mathcal{O}(\eta_t)$, which is proven below.

% proposition for subnet deviation error %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{proposition} (Upper bound for Subnet Deviation Error) \label{prop:clust_div}
%     For $t\in\mathcal T_k$, under Assumptions \ref{beta} and~\ref{assump:SGD_noise}, if $\eta_t=\frac{\gamma}{t+\Lambda}$, using {\tt DFL} for ML model training, in between any two consecutive local aggregations conducted $t^{\mathsf{L}}_{k}$ time instances apart, where $\Theta_c^{(t-t')} = 0$ and $\Theta_c^{(t-t'+t^{\mathsf{L}}_{k}-1)} = 0$, and $\Theta_c^{(t)} = 1$, $1\leq t'< t^{\mathsf{L}}_{k}-1$,
%     the subnet deviation error $\epsilon_c^{(t)}$ can be upper bounded as\footnote{This bound also captures the subnet deviation error right after global synchronization at time $t-t'=t_{k-1}$.}
%     \begin{align} \label{eq:bound_epsc_mn}
%         \sum\limits_{j\in\mathcal S_c}\rho_{j,c}\mathbb E[\Vert \mathbf e_j^{(t)} \Vert^2] \leq (\epsilon_c^{(t)})^2, 
%     \end{align}
%     where 
%     % the outer sum below should be between the two local aggregations. If the period of local aggregation increases, the sum would increase over the interval.
%         \begin{align}
%             &(\epsilon_c^{(t)})^2 \triangleq [\Sigma_{\{c,+\},t}]^2\Bigg[2\omega_c\mathbb E\Vert\bar{\mathbf w}_c^{(t-t')}-\mathbf w^*\Vert
%             \nonumber \\&
%             +
%             \left(\frac{4}{3}\sigma/\beta+\frac{2\omega_c+1}{2(1-\omega_c)}\delta_c/\beta+\frac{\omega_c}{1-\omega_c}\delta/\beta+\sigma/\beta+\mathbb E\left(\sum\limits_{j\in\mathcal S_c}\rho_{j,c} \Vert\mathbf e_{j}^{(t-t')}\Vert\right)\right)\Bigg]^2,
%         \end{align}
%         \begin{align} \label{eq:sigma_c_mn}
%             &\Sigma_{\{c,+\},t}=\sum\limits_{\ell=t-t'}^{t-1}\left(\prod_{j=t-t'}^{\ell-1}(1+\eta_j\beta\lambda_{\{c,+\}})\right)\beta\eta_\ell\left(\prod_{j=\ell+1}^{t-1}(1+\eta_j\beta)\right),
%         \end{align}
%     and
%     % \nm{please dont use the symbol $\vartheta$. Replace it with $\mu/\beta$ or normalize $\mu$, so that $\mu\beta$ is the strong convexity param}
%     \begin{equation}
%       \lambda_{\{c,+\}} =\frac{1}{2}(3+\sqrt{8\omega_c+1}).
%     \end{equation}
% \end{proposition}
 
% The proposition above provides the upper bound of $(\epsilon_c^{(t)})^2$. 
% % The bound demonstrates that, under given condition on step size, i.e., $\eta_t=\frac{\gamma}{t+\Lambda}$, $(\epsilon_c^{(t)})^2 \leq \mathcal{O}(\eta_t)$ is satisfied (since it is verified in Theorem~\ref{thm:subLin} that $[\Sigma_{\{c,+\},t}]^2$  in the right hand side of~\eqref{eq:bound_epsc_mn} is in the order of $\eta_t$, i.e., $[\Sigma_{\{c,+\},t}]^2\leq\mathcal{O}(\eta_t)$). 
% Inspecting the nested sum in~\eqref{eq:sigma_c_mn}, it can be construed that as the period of local aggregation (i.e., $t^{\mathsf{L}}_{k}$) increases, the bound in~\eqref{eq:bound_epsc_mn} increases (i.e., less frequent local aggregations result in larger subnet deviation errors). We will later use this to design the period of local aggregation in our control algorithm in Sec.~\ref{Sec:controlAlg}. 

    % the outer sum below should be between the two local aggregations. If the period of local aggregation increases, the sum would increase over the interval.


% This results from the fact that, as model training proceeds, the value of $\Vert\bar{\mathbf w}_c^{(t_{k-1})}-\mathbf w^*\Vert$ is expected to converge to a certain finite value as the global model converges. we can apply the result in Appendix~\ref{app:clust_div} (Lemma~\ref{lem:clust_div}) to control the value of $\epsilon^{(t)}$ through the number of local aggregations in each subnet $c$ perform between two consecutive global aggregations. We will use this property in the development of our control algorithm to determine the instants for performing local aggregations and the total round of local aggregations ($\Gamma_c$) required in each local model training interval in Sec.~\ref{Sec:controlAlg}.
% We make several key observations. First, the bound obtained in~\eqref{eq:thm2_result-1-T} shows how any observed gradient diversity ($\omega$) can be handled with a proper choice of step size ($\eta_t$). In particular, it shows that $\mathcal{O}(1/t)$ convergence is achieved when $\omega=\frac{\zeta}{2\beta} <\omega_{max}$, where $\omega_{max}$ is a function of $\gamma$ and $\alpha$. Given the fact that $\omega_{max}$ is increasing in $\Lambda$, this implies that larger values of $\zeta$ can be handled via larger values of $\Lambda$. This conforms to the intuition that larger gradient diversity requires a smaller step size for convergence.

% Note that the impact of imperfect synchronization can be revealed in $Y_2$. In particular, when data across different client are extremely non-i.i.d,. $\Psi$ becomes large, resulting in a degradation in ML model performance (larger value of $\nu$). 

% when the delay $\Delta$ is large The bound also shows the impact of the duration of previous local model training intervals (i.e., $\tau$ embedded in terms $Y_1$ and $Y_2$) on the coefficient of the instantaneous upper bound of convergence (i.e., $\nu$). In particular, from~\eqref{eq:thm2_result-1-T}, it can be seen that increasing the maximum local model training interval results in a sharp increase of the upper bound of convergence. A quadratic impact on the upper bound is also observed with respect to the consensus error $\epsilon^{(t)}$ (i.e., through $\phi$). Moreover, all else constant, increasing the value of $\tau$ requires a smaller value of $\phi$ for a desired value of $\nu$ in~\eqref{eq:thm2_result-1-T}. This is consistent with how {\tt DFL} is architected, since the motivation for including consensus rounds (which decrease $\epsilon^{(t)}$) is to reduce the global aggregation frequency, and thus uplink bandwidth utilization.
 


% An immediate result of Theorem~\ref{thm:subLin} is that for a given (attainable) value of $\tilde{\Gamma}\geq\gamma^2\beta^2(A+B)/\Big[\mu\gamma-[3+8(1/\vartheta+(2\omega-\vartheta/2))^2]\Big]+\vartheta\gamma\phi^2/2$, we can guarantee the sublinear convergence condition in~\eqref{eq:thm2_result-1} by choosing $\tau$ such that
%         \begin{align} \label{eq:thm2_case2-1}
%              1 \leq \tau \leq \sqrt{\Big[\mu\gamma-[3+8(1/\vartheta+(2\omega-\vartheta/2))^2]\Big]\Big[\tilde{\Gamma}-\vartheta\gamma\phi^2/2\Big]/(\gamma^2\beta^2 B)-A/B}.
%         \end{align} 
% From~\eqref{eq:thm2_case2-1}, for a given setting of $\tilde{\Gamma}$, the local model training interval periods are always bounded. Furthermore, from~\eqref{eq:thm2_case2-1}, larger gradient diversity (i.e., from $\omega$, and from $\delta$ through the $A$ term) lowers the upper range on $\tau$: as the local datasets exhibit more statistical heterogeneity, we need shorter local model training intervals with more frequent global aggregations to obtain the sublinear convergence characteristic. The same phenomena can be observed considering the impact of the consensus error (i.e., through $\phi$) and of the SGD noise (i.e., through $\sigma^2$ from the $A$ and $B$ terms) on~\eqref{eq:thm2_case2-1}: as these errors increase, we need more frequent global aggregations. In the case of $\epsilon^{(t)}$ this is consistent with how {\tt DFL} is architected, since the motivation for including consensus rounds (which decreases $\epsilon^{(t)}$) is to reduce the global aggregation frequency.



% \begin{lemma} \label{prop:opt_glob}
%     Under Assumptions \ref{beta} and~\ref{assump:SGD_noise}, if $\eta_t=\frac{\gamma}{t+\Lambda}$, $\epsilon^{(t)}$ is non-increasing with respect to $t\in \mathcal T_k$, i.e., $\epsilon^{(t+1)}/\epsilon^{(t)} \leq 1$ and $\Lambda\geq\max\{\beta\gamma[\frac{\mu}{4\beta}-1+\sqrt{(1+\frac{\mu}{4\beta})^2+2\omega}],\frac{\beta^2\gamma}{\mu}\}$, using {\tt DFL} for ML model training, the following upper bound on the expected model dispersion across the clusters holds:
%         \begin{align}
%             &B^{(t)}\triangleq\mathbb E[\Vert\bar{\mathbf w}^{(t)}-\mathbf w^*\Vert^2]\leq
%             [\xi_{+,t}]^2\Vert\bar{\mathbf w}^{(t_{k-1})}-\mathbf w^*\Vert^2
%             \nonumber \\&
%             +8[\xi_{+,t}]^2
%             \left[\sigma^2/\beta^2+\delta^2/\beta^2+(\epsilon^{(0)})^2+\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c^{(t_{k-1})}-\bar{\mathbf w}^{(t_{k-1})}\Vert^2\right].
%         \end{align}
% \end{lemma}  

% proposition on upper bounding the sync error %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\begin{proposition} [Upper Bound on the Average Synchronization Error] \label{prop:clust_glob_agg}
% Consider a fixed number of local model training interval $\tau$ between each global aggregation, 
% Given a delay $\Delta$ and combiner weight $\alpha\in(0,1]$, using {\tt DFL} for ML model training, $\Psi$ is upper bounded by a constant value as follows: {\color{blue}to be revisited with $\Delta$ in it.}
% worst case alpha
Under Assumptions \ref{beta} and~\ref{assump:SGD_noise}, presume $\eta_t=\frac{\gamma}{t+\Lambda}$, $\epsilon^{(t)}$ is non-increasing with respect to $t\in \mathcal T_k$, i.e., $\epsilon^{(t+1)}/\epsilon^{(t)} \leq 1$, and $\Lambda\geq\max\{\beta\gamma[\frac{\mu}{4\beta}-1+\sqrt{(1+\frac{\mu}{4\beta})^2+2\omega}],\frac{\beta^2\gamma}{\mu}\}$. Considering the same local model training intervals across global aggregations $\tau=\tau_k$ $\forall k$, 
the expected model dispersion across the subnets is upper bounded by a constant $\Psi$ for any given value of $\alpha\in(0,1]$ upon using {\tt DFL} for ML model training:
\begin{align}
    \mathbb E\left[\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c^{(t_{k})}-\bar{\mathbf w}^{(t_{k})}\Vert^2\right] \leq \Psi,~\forall k,
\end{align}
where   
\begin{align} 
    \Psi &= 
    \exp\bigg(\frac{(10\omega^2+66)^2 D_2^2\gamma^2}{2e\tau\Lambda}\bigg)\mathbb E\Vert\bar{\mathbf w}^{(0)}-\mathbf w^*\Vert^2 
    \nonumber \\&
    + \left(\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}+(\epsilon^{(0)})^2\right)\left(\sum\limits_{n=1}^{\bar{n}} \exp\left(\frac{(66 D_2\gamma)^2}{2e\tau(\tau n+\Lambda)}\right)
    +\frac{66 D_2\gamma}{\tau+\Lambda}\right)
\end{align}
and $\bar{n}=\lfloor\frac{66 D_2\gamma}{\tau}\rfloor$.
% \nm{what is $\bar n$?}
\end{proposition} 
% To compensate the effect of communication delay between the edge server and the cloud on model convergence, the weighted global aggregation scheme introduces an initial error due to imperfect synchronization.
Proposition~\ref{prop:clust_glob_agg} demonstrates that average synchronization error can always be upper bounded by a constant under having a finite delay $\Delta$ upon choosing $\alpha\in(0,1]$. When no global aggregation is performed (i.e., $\tau\rightarrow \infty$), the synchronization error diverges, and frequent aggregations 
(i.e., $\tau=1$) lead to a small synchronization error.
\fi



% which explains the reason why the global deployed model could not achieve sublinear convergence when performing ML model training under {\tt DFL} without sharing information across devices via global aggregation (as shown in Theorem~\ref{thm:subLin}).    
% the initial error induced in $A^{(t)}$ can be bounded by some constant $\Psi$. This implies that any given choice of synchronization weight $\alpha$ would not result in the divergence of the global model when parameters are finely tuned to satisfy the conditions provided in Theorem~\ref{thm:subLin}.
 


% {\color{blue}revised up to here on June 3rd!}

% % The design of these parameters affects how the global model $\bar{\mathbf w}(t_k)$ converges to the optimum as $k$ increases. Different design of these learning parameters would lead to different convergence result (i.e. different convergence rate).
% From the expression in \eqref{main}, we see that $\eta_k$, $\tau_k$ and $\epsilon(k)$ can be designed to change dynamically according to different characteristics of the loss function (i.e. $\mu,\beta$), conditions of data diversity (i.e. related to the values of $\delta,\zeta$) and variance of the estimated gradient used in SGD (i.e. $\sigma$). Since the communication cost between the server and local devices is expensive in terms of bandwidth and delay, so ideally, we would want the local devices to be more often trained locally, or in other words, to have a larger value of $\tau_k$ between consecutive global aggregations. However, when the data diversity $\delta,\zeta,\epsilon(k)$ among different devices are large, the increase of $\tau_k$ would result in the degradation of the model performance. Therefore, by enabling D2D communication among the local devices, we could adaptively tune the round of consensus to modify the consensus error resulted from data diversity characterized by $\epsilon(k)$ and result in better learning performance with respect to a larger value of $\tau_k$. Note that the design of $\tau_k$ also depends on $\eta_k$. From \eqref{main}, we see that a larger value of $\eta_k$ would result in a smaller value of $\tau_k$, vice versa. This also demonstrates the coupled nature of the tunable learning parameters along with all the influencing factors mentioned above. 

% Theorem \ref{subLin} provides a direction for designing the tunable learning parameters ($\eta_k,\tau_k,\epsilon(t)$) to guarantee convergence to the optimum with rate of $\mathcal{O}(1/t)$. Given a predetermined value of $\Gamma$, $\gamma$, and $\nu$, we can find the condition for $\eta_k,\tau_k$ and $\epsilon(k)$\nm{eps is decreasing over k, which means that consensus needs to be more and more accurate, i.e. more conesnsus step.. CAn you provide guarantees o nthe number of consensus steps?} such that the model $\bar{\mathbf w}(t)$ obtained from {\tt DFL} is guaranteed to converge sublinearly to the optimal point $\mathbf w^*$ with respect to $t$. 

% To achieve convergence with the rate of $\mathcal{O}(1/t)$, it can be observed from Theorem \ref{subLin} that we want to design the step size as a linearly decreasing function $\eta_t=\frac{\gamma}{t+\alpha}$ with respect to $t$, the number of local updates $\tau_k$ restricted within $\tau_k \leq \log_4\left\{[(\mu\gamma-1)\nu-B]/[\frac{\gamma^2\beta}{2}A+\frac{16\omega^2\beta^2\gamma^2\nu}{\kappa}]\right\}$ given some predetermined $\nu\geq B/(\mu\gamma-1)$ and the consensus error to be in the order of the step size, depicted as $\epsilon^2(t)=\eta_t\phi$. 

% The value for $\gamma$ resides within the range of $\frac{\kappa\mu-\kappa\sqrt{\mu^2-64\cdot 4^{\tau_k}\omega^2\beta^2 /(\kappa)}}{32\cdot 4^{\tau_k}\omega^2\beta^2 }<\gamma<\frac{\kappa\mu+\kappa\sqrt{\mu^2-64\cdot 4^{\tau_k}\omega^2\beta^2 /(\kappa)}}{32\cdot 4^{\tau_k}\omega^2\beta^2}$. Consider the special case of $\omega=0$, the range of $\gamma$ becomes $\gamma>1/\mu$ with no constraints on the value of $\tau_k$, which is equivalent to the expression in \cite{hosseinalipour2020multi,Li}. For $\omega>0$, we can see that the range of $\gamma$ depends on the value of $\tau_k$ for all $k$. Note that since we do not know the value of $\tau_k$ for all $k$ beforehand in practice, we could obtain the value of $\gamma$ by numerically analyze the feasible range of $\gamma$ with respect to various values of global aggregation period $\tau_k<\log_4\frac{\kappa}{16\omega^2}$.

% The value for the predetermined value of $\nu$ is coupled with the decision of $\tau_k$. Given some $\nu\geq B/(\mu\gamma-1)$, $\tau_k$ should reside in the range of $\tau_k \leq \log_4\left\{[(\mu\gamma-1)\nu-B]/[\frac{\gamma^2\beta}{2}A+\frac{16\omega^2\beta^2\gamma^2\nu}{\kappa}]\right\}$.
% Or equivalently, given some $\tau_k<\log_4\frac{\kappa^3}{64\omega^2}$, the choice of $\nu$ should satisfy $\nu\geq\frac{\gamma^2\beta}{2}[Q_k A+B]/[\mu\gamma-(1+16\omega^2\beta^2\gamma^2 Q_k/\kappa)]$ to guarantee sublinear convergence. We can see that the value of $\nu$ increases with $\tau_k$, $\delta$, $\omega$ and $\phi$. Since $\delta$ and $\omega$ are directly based on the characteristics of data and cannot be changed manually, this shows that, to guarantee sublinear convergence with respect to a predetermined value of $\nu$, we can tune $\tau_k$ and $\phi$ such that $\nu$ satisfies the requirement to achieve convergence. In particular, the value of $\tau_k$ increases when $\phi$ decreases and vice versa, this shows that we can allow larger value of $\tau_k$ with a smaller value of $\phi$ by performing more rounds of consensus during local updates. This result also leads to the development of the control algorithm for {\tt DFL} is section IV.  

% \nm{what is the point of this prop? IT seems disconnected from the rest}

% \begin{proposition}\label{lem2}
% Under Assumption \ref{beta} and Algorithm \ref{GT}, for $\eta_k\leq\frac{\kappa}{\beta}$ and $t \in \mathcal T_k$, we have
%     \begin{align*}
%         &\mathbb E\left[\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}}\Big\Vert\bar{\mathbf w}(t)-\mathbf w_i(t)\Big\Vert^2\right]
%         \nonumber \\& 
%         \leq 
%         [\lambda_1^{2t}-1]\{32\omega^2/\mu[F\bar{\mathbf w}(t_{k-1}))-F(\mathbf w^*)]+153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}+\epsilon^2(k),
%     \end{align*}
%     where $\lambda_1 = 1+\eta_k\beta\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]$.
% \end{proposition}
% \begin{skproof}
% Combining the following expression of $\bar{\mathbf w}_c(t)$ and $\bar{\mathbf w}(t)$
% \begin{align}
%         &\bar{\mathbf w}_c(t)=\bar{\mathbf w_c}(t-1)-\eta_k\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\widehat{\mathbf g}_{j,l}
%     \end{align}
%     and 
%     \begin{align}
%         &\bar{\mathbf w}(t)=
%         \bar{\mathbf w}(t-1)-\eta_k\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\widehat{\mathbf g}_{j,l},
%     \end{align}
%     with properties of Assumption \ref{beta}, \ref{SGD} and Definition \ref{gradDiv}, we obtain the following system dynamics 
%     \begin{align} \label{69}
%         \mathbf x(t)
%         &\leq 
%         \mathbf M x(t-1)
%         + \eta_k(\sigma+\beta\epsilon(k))\mathbf 1+\eta_k(\delta+\beta\epsilon(k)+(\sqrt{2}-1)\sigma)\mathbf e_1,
%     \end{align}
%     where $x_1(t)=\sqrt{\mathbb E[(\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert)^2]}$, $x_2(t)=\sqrt{\mathbb E[\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert^2]}$, \\$\mathbf M=\begin{bmatrix} 1-\eta_k\mu/2 & \eta_k\beta\\ \eta_k\zeta & 1+2\eta_k\beta\end{bmatrix}$ 
%     with the largest eigenvalue of $$\lambda_1 = 1+\eta_k\beta\big[1-\kappa/4+\sqrt{C}\big]=1+\eta_k\beta\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big].$$
%     By eigen-decomposition of $\mathbf M$, we obtain the following bound 
%     \begin{align} \label{errorC}
%         &\sum\limits_{c=1}^N\varrho_c^{(k)}\mathbb E[\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert^2]
%         % \nonumber \\&
%         \leq
%         [\lambda_1^{2t}-1]\{32\omega^2/\mu[F\bar{\mathbf w}(t_{k-1}))-F(\mathbf w^*)]+153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}.
%     \end{align}
%     Finally, since
%     \begin{align} 
%         \Vert\mathbf w_i(t)-\bar{\mathbf w}(t)\Vert_2^2
%         =
%         \Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert_2^2
%         +\Vert e_i^{(t)}\Vert^2
%         +2[\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)]^\tope_i^{(t)}
%     \end{align}
% such that
% \begin{align} \label{errors2}
% \mathbb E\left[\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_c^{(k)}}\sum_i\Vert\mathbf w_i(t)-\bar{\mathbf w}(t)\Vert_2^2\right]
%         =
%         \sum\limits_{c=1}^N\varrho_{c}\mathbb E\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert_2^2
%         +\mathbb E\left[\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_c^{(k)}}\sum_i\Vert{e_i^{(t)}\Vert}^2\right],
% \end{align}    
%     the result of the Proposition directly follows.
% \end{skproof}

% Proposition \ref{lem2} provides an upper bound for the \emph{Local model dispersion}, which quantifies the difference between the local model $\mathbf w_i(t)$ and the auxiliary global model $\bar{\mathbf w}(t)$. We also see that the bound for \emph{Local model dispersion} is dominated by $[\lambda_1^{2t}-1]$, where $\lambda_1$ refers to the largest eigenvalue of the coupled system dynamics with respect to the expected difference between subnet and auxiliary global model $\sqrt{\mathbb E[(\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert)^2]}$ and the expected difference between the auxiliary global and optimal model $\sqrt{\mathbb E[\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert^2]}$.  
% \