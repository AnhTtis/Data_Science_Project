\vspace{-1mm}
\section{Experimental Evaluation}
\label{sec:experiments}

% \noindent In this section, we conduct numerical experiments to verify the performance of {\tt TT-HF}. After describing the setup in Sec.~\ref{ssec:setup}, we study model performance/convergence in Sec.~\ref{ssec:conv-eval} and the impact of our adaptive control algorithm in Sec.~\ref{ssec:control-eval}. Overall, we will see that {\tt TT-HF} provides substantial improvements in training time, accuracy, and/or resource utilization compared to conventional federated learning~\cite{wang2019adaptive,Li}. 
\noindent This section presents numerical experiments to validate the performance of {\tt DFL}.
% ~\cite{NEURIPS2019_9015}. 
In Sec.~\ref{ssec:setup}, we provide the simulation setup. Then, in Sec.~\ref{ssec:conv-eval}, we study the model training performance and convergence behavior of {\tt DFL} with set control parameters (i.e., $\tau_k$, $\mathcal{T}^{\mathsf{L}}_{k,c}$, and $\alpha_k$), revealing the importance of addressing the hierarchical FL architecture and tuning the combiner weight in the presence of communication delay. In Sec.~\ref{ssec:control-eval}, we compare the convergence behavior of our {\tt DFL} control algorithm with baslines in FL~\cite{wang2019adaptive,Li}, verify our theoretical results (Sec.~\ref{ssec:convAvg}), and illustrate the improvements in resource efficiency achieved by the {\tt DFL} control algorithm. 
\vspace{-2mm}
\subsection{System Setup}
\label{ssec:setup} 
 
  
% \noindent 
% \textbf{Network configuration.}
% The data rate between each device and its associated edge server is assumed to be $R_i=35$ (Mbps),$~\forall i$. 
We consider a network of $50$ edge devices distributed across $10$ equally-sized subnets, with $5$ devices per subnet. In Sec.~\ref{ssec:control-eval}, devices within each subnet are uniformly distributed within a $30$ m $\times$ $30$ m square field, and the base station is located at the center. Device-to-edge wireless communication features a $p_i=0.25$ W transmission power, 1 MHz bandwidth, and $-173$ dBm/Hz white noise spectral density. Fading and pathloss are modeled based on~\cite{tse2005fundamentals}, using $h^{(t)}_{i}= \sqrt{\beta_{i}^{(t)}} u_{i}^{(t)}$, with $\beta_{i}^{(t)} = \beta_0 - 10\widehat{\alpha}\log_{10}(d^{(t)}_{i}/d_0)$ as the large-scale pathloss coefficient and $u_{i}^{(t)} \sim \mathcal{CN}(0,1)$ for Rayleigh fading. Parameters include $\beta_0=-30$ dB, $d_0=1$ m, $\widehat{\alpha}=3.75$, and the device-edge server distance (i.e., $d^{(t)}_{i}$). Channel reciprocity is assumed for simplicity. Edge-to-cloud wired communication has $\bar{p}_c=6.3$ W transmission power, $100$ Mbps data rate, and a $50$ ms delay that includes transmission, propagation, and processing. Local model update processing rates are set at $R(\xi_i^{(t)})=200$ updates/s \footnote{The processing rate may vary due to factors such as hardware configurations or data and batch sizes. We estimate the processing rate based on experimental results obtained using the F-MNIST dataset~\cite{beutel2020flower}.} for all $i, t$, yielding $\Delta=10$. 
% Note that the reason to neglect the transmission delay for performing global aggregations is due to the fact that, in the system model, we assume device-to-edge links have much shorter range than the edge-to-cloud links such that the propagation delay becomes a dominant portion of the communication delay. Therefore, we neglect the transmission delay in device-to-edge links for global aggregations in the objective function.\\
% \textbf{Datasets.}
We employ the Fashion-MNIST (F-MNIST) dataset~\cite{beutel2020flower} for image classification, comprising $10$ labels of fashion products. The dataset includes a total of $70$K images, with $60$K designated for training and $10$K for testing. 
% For brevity, we present the results for MNIST here, and refer the reader to Appendix~\ref{app:experiments} for FMNIST; the results are qualitatively similar.\\
% \textbf{Data distribution.}
Datasets are distributed across devices, where each device $i$ holds datapoints from only $3$ out of $s=10$ labels, simulating a certain degree of statistical data heterogeneity among devices.
% per node, where each of which corresponds to a particular level of data heterogeneity. (a) represents the case of extreme non-i.i.d, where each device possesses data labeled with only $1$ of the $10$ classes; (b) represents the case of moderate non-i.i.d, where each device samples data from $3$ classes out of $10$; (c) represents the case of i.i.d, where each device has samples uniformly sampled from the entire dataset.\\
% \textbf{ML models.} 
We evaluate {\tt DFL} on two models: support vector machine (SVM) with regularized squared hinge loss 
% $\ell(\mathbf x,y;\mathbf w)=\frac{q}{2}\Vert\mathbf w\Vert^2+\frac{1}{2}\max\{0,1-y\mathbf w^\top\mathbf x\}^2$ ($q$ is a constant), 
and a fully connected neural network (NN) with softmax and cross-entropy loss, 
% $\ell(\mathbf x,y;\mathbf w)= -\sum\limits_{j=1}^{s}\{y=j\}\log e^{\mathbf w_j^T \mathbf x}\Big(\sum\limits_{l=1}^{s}e^{\mathbf w_l^\top \mathbf x}\Big)^{-1}$, 
both having model dimension $M = 7840$. 
% The SVM uses a linear kernel and the initialization of model parameters follows a uniform distribution with mean and variance computed according to~\cite{he2015delving}. 
This allows us to assess {\tt DFL}'s performance on loss functions satisfying Assumption~\ref{Assump:SmoothStrong} (SVM) and those exhibiting non-convex properties (NN).
% \nm{what about the other parameters, such as tx power, throughput etc??}
%For brevity, we present the results for SVM here, and refer the reader to Appendix~\ref{app:experiments} in our technical report for the NN results, where we also explain the implementation of our control algorithm for non-convex loss functions.

% All of our implementations can be accessed at \url{https://github.com/shams-sam/TwoTimeScaleHybridLearning}.

% where we explain the implementaion of our control algorithm for non-convex functions and ....
% More details on the experiment setup is given in Appendix.... , where we also complement the results by investigating the performance considering other network size and datasets. 

% \begin{figure}[t]
% \includegraphics[width=1.0\columnwidth]{mnist_125/XXX}
% \centering
% \caption{Performance comparison between {\tt DFL} and baseline methods when varying the number of D2D consensus rounds ($\Gamma$). Under the same period of local model training ($\tau$), increasing $\Gamma$ results in a considerable improvement in the model accuracy/loss over time as compared to the current art~\cite{wang2019adaptive,Li} when data is non-i.i.d.}

% % , even $\Gamma=1$ increases the convergence rate (magenta line) compared to the case with no D2D consensus between global aggregations (red line marked FL). This difference is not apparent when the data is i.i.d distributed (all 10 labels per node: subplot (a) and (d)) but becomes prominent in the extreme non-i.i.d case (1 labels per node: subplots (c) and (f)).}
% % moderate non-i.i.d (3 label per node: subplots (a) and (b)), but becomes more visible in extreme non-i.i.d case (1 labels per node: subplots (c) and (d)). The difference in performance is not apparent when the data is i.i.d distributed (all 10 labels per node: subplot (e)).} 
% \label{fig:mnist_poc_1_all}
% \vspace{-5mm}
% \end{figure} 


\subsection{{\tt DFL} Model Training Performance and Convergence}
\label{ssec:conv-eval}
% One of the main premises of {\tt TT-HF} is that cooperative consensus formation within clusters during the local model training interval can (i) increase the model training accuracy and/or (ii) preserve model performance while reducing the required frequency of global aggregations, especially when statistical data heterogeneity is present across the devices. Our first set of experiments seek to validate these facts:
% In this section, we aim to validate the effectiveness of {\tt DFL} in improving FL model training efficiency under the existence of non-negligible round-trip communication delay between the edge and main server. 

We first discuss how the two major design aspects of {\tt DFL} (i.e., hierarchical FL framework (Sec.~\ref{subsubsec:hierarchical}) and local-global combiner (Sec.~\ref{subsubsec:combiner})) can affect the speed of ML model convergence and mitigate the effect of delay. We consider {\tt FedAvg}~\cite{mcmahan2017communication}, which performs global aggregations after each round of training with $\tau=1$, as our benchmark. This represents an upper bound on the learning performance as it mimics centralized model training.
\begin{figure}[t]
% \vspace{-0.2in}
\includegraphics[width=1.0\columnwidth]{fmnist_125/a.png}
\centering
\caption{Performance comparison between {\tt DFL} and standard {\tt FedAvg}~\cite{mcmahan2017communication} under various choices of delay: {\tt DFL} surpasses standard {\tt FedAvg} with accuracy gains of $2\%$ (CNN) and $4\%$ (SVM) when delay is negligible ($\Delta=0$), and further outperforms with gains of $6\%$ (CNN) and $8\%$ (SVM) when delay is non-negligible ($\Delta=10$).}
\label{fig:mnist_poc_1_all}
\vspace{-6mm}
\end{figure} 

% \addFL{
% (a) star topology with no local aggregations: fix $\Delta$ and run experiment with different $\alpha$, compare the performance of given delay with  $\alpha=0.5$ with no-delay and standard fedavg. 
% (a)-i hierarchical vs star with no delay
% (a)-ii hierarchical vs star with delay
% (b)-i DFL with delay
% (b)-ii DFL with $\alpha=0.5$ under delay and no delay}
 
\subsubsection{Model convergence of hierarchical FL} \label{subsubsec:hierarchical}
We compare the performance of {\tt DFL}  (Algorithm~\ref{DFL}) with standard {\tt FedAvg} model aggregation/synchronization~\cite{mcmahan2017communication}. In standard {\tt FedAvg}, we consider a (hypothetical) scenario that the edge devices are directly connected to the main server. Thus the comparison would reveal the impact of hierarchical structure of {\tt DFL} on model training. We consider two different scenarios: (i) when delay is negligible, and (ii) when delay is non-negligible. In scenario (i), we set the combiner weight of {\tt DFL} to $0$ (i.e., $\alpha=0$) with delay $\Delta=0$. In scenario (ii), we set the combiner weight of {\tt DFL} as $\alpha=0.5$ with delay $\Delta=10$. For both scenarios, {\tt DFL} establishes a local model training interval $\tau_k = \tau = 20$ and performs local aggregation in the subnets after each edge device does $5$ local SGD updates.

Fig.~\ref{fig:mnist_poc_1_all} validates the benefit of introducing hierarchical FL with local model aggregations by showing that {\tt DFL} outperforms the standard {\tt FedAvg} under both negligible and non-negligible delay. In scenarios where the delay is insignificant (i.e., $\Delta=0$), {\tt DFL} exhibits superior performance over {\tt FedAvg}, achieving an accuracy gain of $2\%$ for CNN and $4\%$ for SVM. This highlights the advantages of exploring a hierarchical model training architecture incorporating frequent local aggregations within an edge-to-cloud network. On the other hand, in situations where the delay is non-negligible (i.e., $\Delta=10$), {\tt DFL} outperforms {\tt FedAvg} with  accuracy gains of $6\%$ for CNN and $8\%$ for SVM, demonstrating the benefits of integrating the local-global combiner with the hierarchical model training architecture.
% in an edge-cloud network architecture, benefiting from the fact that the edge-to-cloud delay is non-negligible while the delay within the edge network can usually be negligible.

To further highlight the resilience of {\tt DFL}'s to delays, we consider the performance of standard {\tt FedAvg} with negligible delay ($\Delta=0$) as a benchmark and compare it with {\tt DFL} when delay is non-negligible ($\Delta=10$). As illustrated in Fig.~\ref{fig:mnist_poc_1_all}, under a large delay of $\Delta=10$, {\tt DFL} attains an accuracy within $1\%$ of the benchmark for CNN and maintains a performance advantage over the benchmark, achieving a $2\%$ higher accuracy for SVM after $100$ global aggregations.
\begin{figure}[t]
% \vspace{-0.2in}
\includegraphics[width=1.0\columnwidth]{fmnist_125/b2.png}
\centering
\caption{Performance comparison between {\tt DFL} and hierarchical {\tt FedAvg} for various $\alpha$ values in {\tt DFL}: With $\alpha=0.5$, {\tt DFL} significantly outperforms hierarchical {\tt FedAvg} when round-trip delay is $\Delta=10$. Hierarchical {\tt FedAvg} ({\tt DFL} with $\alpha=0$) performs better than {\tt DFL} without delay (i.e., $\Delta=0$), in line with Theorem~\ref{thm:subLin}.}
\label{fig:mnist_poc_2_all-2} 
\vspace{-6mm} 
\end{figure}
% This also demonstrates the delay-robustness of {\tt DFL}.
 
% \begin{figure}[t]
% \includegraphics[width=1.0\columnwidth]{fmnist_nn/b_performance.pdf} 
% \centering
% \caption{Performance comparison between {\tt DFL} and hierarchical {\tt FedAvg} when varying the local-global combiner weight $\alpha$ in {\tt DFL}. With $\alpha=0.5$, {\tt DFL} significantly outperforms hierarchical {\tt FedAvg} under round-trip delay $\Delta=18$. The performance of {\tt DFL} becomes the worst with $\alpha=0$, when no global aggregation is conducted.}
% \label{fig:mnist_poc_2_all}
% \vspace{-5mm}
% \end{figure}


% In Fig.~\ref{fig:mnist_poc_1_all}, we compare the performance of {\tt TT-HF} in Algorithm~\ref{TT-HF} with current federated learning algorithms that do not exploit local D2D model consensus formation. The baselines both assume full device participation (i.e., all devices upload their local model to the server at each global aggregation), and thus are 5x more uplink resource-intensive at each aggregation. One baseline conducts global aggregations after each round of training ($\tau = 1$), and the other, based on~\cite{wang2019adaptive}, has local update intervals of $20$ ($\tau = 20$). For {\tt TT-HF}, we set $\tau_k = \tau = 20$ and conduct a fixed number of D2D rounds in  clusters after every $5$ time instances, i.e., $\Gamma^{(t)}_c = \Gamma$ for different values of $\Gamma$. The $\tau = 1$ baseline is an upper bound on the achievable performance since it replicates centralized model training.

% Fig.~\ref{fig:mnist_poc_1_all} verifies that local D2D communications can significantly boost the performance of ML model training. Specifically, when the data distributions are moderate non-i.i.d ((b) and (e)) or extreme non-i.i.d. ((c) and (f)), we see that increasing $\Gamma$ improves the trained model accuracy/loss substantially from FL with $\tau = 20$. It also reveals that there is a diminishing reward of increasing $\Gamma$ as the performance of {\tt TT-HF} approaches that of FL with $\tau = 1$. Finally, we observe that the gains obtained through D2D communications are only present when the data distributions across the nodes are non-i.i.d., as compared to the i.i.d. scenario ((a) and (d)), which emphasizes the purpose of {\tt TT-HF} for handling statistical heterogeneity.

\subsubsection{Model convergence under local-global combiner} \label{subsubsec:combiner}
We compare the performance of {\tt DFL} with a local-global combiner to that of hierarchical {\tt FedAvg}~\cite{liu2020client}, where both utilize local aggregations. Hierarchical {\tt FedAvg} is basically a special case of {\tt DFL} when $\alpha=0$. {\tt DFL} is executed with a fixed combiner weight of  $\alpha=0.5$, which places equal emphasis on the stale global model and up-to-date local models. The local model training interval is set to $\tau_k = \tau = 20$ for both {\tt DFL} and hierarchical {\tt FedAvg}, with a delay of $\Delta=10$. Local aggregations are conducted after each edge device does $5$ local model updates. Furthermore, we plot the convergence behavior of {\tt DFL} when $\alpha=1$, which implies that global model is never used in local devices. 
 
Fig.~\ref{fig:mnist_poc_2_all-2} reveals the effectiveness of the local-global combiner in {\tt DFL}. In particular, it shows that {\tt DFL} outperforms vanilla hierarchical {\tt FedAvg} by utilizing the local-global combiner and achieves an accuracy gain of $5\%$ for CNN and $8\%$ for SVM when the delay is large (i.e., $\Delta=10$). Conversely, Fig.~\ref{fig:mnist_poc_2_all-2} illustrates that when there is no delay (i.e., $\Delta=0$), hierarchical {\tt FedAvg} achieves better convergence performance than {\tt DFL}, with an accuracy increase of $1\%$ for both CNN and SVM. These findings align with the result in Theorem~\ref{thm:subLin}, suggesting that $\alpha=0$ is the optimal choice of the combiner weight when $\Delta=0$. 

\begin{figure}[t]
% \vspace{-0.2in}
\includegraphics[width=1.0\columnwidth]{fmnist_125/b1_acc.png} 
\centering
\caption{Performance comparison between {\tt DFL} and hierarchical {\tt FedAvg} under various choice of $\alpha$ in {\tt DFL} under delay $\Delta=10$. With $\alpha=0.5$, {\tt DFL} significantly outperforms hierarchical {\tt FedAvg}. However, the performance of {\tt DFL} was the worst with $\alpha=0$, as no global synchronization was conducted.}
\label{fig:mnist_poc_2_all}
\vspace{-6mm}
\end{figure}

To highlight the robustness of {\tt DFL} against delays, Fig.~\ref{fig:mnist_poc_2_all-2} compares its performance with hierarchical {\tt FedAvg} with a negligible delay (i.e., $\Delta=0$) as the benchmark under a non-negligible delay (i.e., $\Delta=10$). The figure shows that even when the delay is significant, {\tt DFL} achieves an accuracy  within $1\%$ and $2\%$  of the benchmark for CNN and SVM after $100$ global aggregations, demonstrating its delay-robustness.
Lastly, Fig.~\ref{fig:mnist_poc_2_all} shows the convergence performance of {\tt DFL} with no usage of global model (i.e., $\alpha=1$), where the ML model plateaus after reaching a low accuracy. This verifies the condition in Theorem~\ref{thm:subLin}, implying that {\tt DFL} may not guarantee sublinear convergence when $\alpha=1$.



% when communication delay between edge and cloud is negligible (∆ = 0), FedAvg obtains
% the best performance in terms of convergence rate, verifying
% our conclusion in Theorem 1 that α = 1 is the optimum
% when ∆ = 0. To further demonstrate the delay robustness of
% FedDelAvg, we set the performance of FedAvg with no delay (∆ = 0) as benchmark and compare it with FedDelAvg
% under delay (∆ = 9) when α is optimized. We observe
% that, even when the delay is non-negligible, FedDelAvg
% achieves an accuracy of 80% while only requiring 10% extra
% training iterations compared with the benchmark. After 100
% aggregations, FedDelAvg achieves an accuracy within 3%
% of the benchmark, whereas FedAvg has a severely degraded
% performance, thus demonstrating the delay-robustness of the
% proposed algorithm.

% In Fig.~\ref{fig:mnist_poc_2_all}, we compare the performance of {\tt TT-HF} for increased local model training intervals $\tau$ against the baselines. Recall that longer local training periods are desirable to reduce the frequency of communication between devices and the main server. As in Fig.~\ref{fig:mnist_poc_1_all}, we also conduct consensus after every $t = 5$ time instances, and increase $\Gamma$ as $\tau$ increases.

% Fig.~\ref{fig:mnist_poc_2_all} confirms that {\tt TT-HF} can still outperform the baseline FL with $\tau = 20$ when the frequency of global aggregations is decreased: in other words, increasing $\tau$ can be counteracted with a higher degree of local consensus formation $\Gamma$. Considering the moderate non-i.i.d. plots ((b) and (e)), we also see that the jumps in global model performance, while less frequent, are substantially larger for {\tt TT-HF} than this baseline. This result demonstrates that D2D communications can reduce reliance on the main server for a more distributed model training process.

%One of the main motivations for prolonging the period of local model training interval is to remove the burden of extensive uplink communications from the devices. We thus compare the method of~\cite{wang2019adaptive,Li} when global models are performed after every 20 SGD local updates (red curve) with {\tt TT-HF} when longer period of local model training are deployed in Fig.~\ref{fig:mnist_poc_2_all}. In this simulation we assume that $\Gamma$ D2D communication rounds are performed after every $5$ local SGD updates at the devices.  The results demonstrates that {\tt TT-HF} can outperform the baseline method (red curve) even when the duration of local model training are extended. This result demonstrate that with the exploitation of D2D communications {\tt TT-HF} has less reliance on the main server can model training can be prolonged while a satisfactory performance achieved.




% \subsubsection{Convergence behavior}
% Recall that the upper bound on convergence in Theorem~\ref{co1} is dependent on the expected model dispersion $A^{(t)}$ and the consensus error $\epsilon^{(t)}$ across clusters. For the settings in Figs.~\ref{fig:mnist_poc_1_all}\&\ref{fig:mnist_poc_2_all}, increasing the local model training period $\tau$ and decreasing the consensus rounds $\Gamma$ will result in increased $A^{(t)}$ and $\epsilon^{(t)}$, respectively, for a given $t$. In Fig.~\ref{fig:poc_3_iid_1_gamma_1_lut_50}, we show that {\tt TT-HF} suffers from poor convergence behavior in the extreme non-i.i.d. case when the period of local descents $\tau$ are excessively prolonged, similar to the baseline FL when $\tau = 50$~\cite{wang2019adaptive}. This further emphasizes the importance of Algorithm~\ref{GT} tuning these parameters around Theorem~\ref{thm:subLin}'s result.

%Although prolonging the local descent intervals can be achieved via D2D communications conducted in {\tt TT-HF}, as highlighted by the result of Theorem~\ref{thm:subLin}, similar to the current art \cite{wang2019adaptive}, {\tt TT-HF} also suffers from model divergence (poor convergence behavior) when the period of local descents are excessively prolonged. 
%Fig.~\ref{fig:poc_3_iid_1_gamma_1_lut_50} demonstrates this phenomenon in which after increasing the local model training interval beyond 60, marginal performance gain are achieved at the instance of global aggregations. 
% In this section, we study the behavior of {\tt TT-HF} under different hyper-parameters. In particular, in ... we fix the rounds of D2D communications and change the period of local model training interval. This result demonstrate that after a certain threshold, the model training does not exhibit a convincing behavior, which is also highlighted in Theorem 2, via the upper bound on the value of $\tau$. Furthermore, it can be seen that increasing the rounds of D2D communications results in longer tolerable local model training intervals.


% \begin{figure}[t]
% \includegraphics[width=1.0\columnwidth]{mnist_125/poc_3_iid_1_gamma_1_lut_50.eps}
% \centering
% \caption{Performance of {\tt TT-HF} in the extreme non-i.i.d. case for the setting in Fig.~\ref{fig:mnist_poc_2_all} when $\Gamma$ is small and the local model training interval length is increased substantially. {\tt TT-HF} exhibits poor convergence behavior when $\tau$ exceeds a certain value, due to model dispersion.}
% % similar to the baseline~\cite{wang2019adaptive,Li} (red curve).} 
% \label{fig:poc_3_iid_1_gamma_1_lut_50}
% \vspace{-5mm}
% \end{figure}

% \begin{figure}[t]
% \hspace{-3mm}
% \includegraphics[width=0.99\columnwidth]{mnist_125/resource_bar_0.01_0.15.eps}
% \centering
% \caption{Comparing total (a) cost, (b) power, and (c) delay metrics from the optimization objective in $(\bm{\mathcal{P}})$ achieved by {\tt TT-HF} versus baselines upon reaching $75\%$ of peak accuracy, for different configurations of delay and energy consumption. {\tt TT-HF} obtains a significantly lower total cost in (a). (b) and (c) demonstrate the region under which {\tt TT-HF} attains energy savings and delay gains.}
% \label{fig:resource_bar_0}
% \vspace{-5mm}
% \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Adaptive Parameter Control for {\tt DFL}}
\label{ssec:control-eval}
Next, we  analyze the behavior of {\tt DFL} through parameter tuning described in Algorithm~\ref{GT}.

\subsubsection{Impact of delay on the choice of $\alpha$}
We explore the effect of various values of delay on the selection of the combiner weight $\alpha$. The delay is increased incrementally from $\Delta=5$ to $\Delta=25$ in steps of $5$, while $\tau_k=\tau=30$ is kept constant. Fig.~\ref{fig:alpha}(a) illustrates the average value of $\alpha$ generated by Algorithm~\ref{GT} across global synchronizations. It is evident from the figure that $\alpha$ increases as the delay increases for both CNN and SVM. This aligns with the intuition that as the delay increases, {\tt DFL} would place more emphasis on the local model at the instance of local model synchronization with the global model since the global model becomes more obsolete as the delay increases. 
% Therefore, the contribution of the local model becomes more significant in reducing the divergence of the global deployed model.
Furthermore, as delay reaches a threshold, the selection of $\alpha$ ceases to increase due to the constraint in $(\bm{\mathcal{P}})$, limiting it to a feasible range. Algorithm~\ref{GT}'s ability to ensure feasible $\alpha$ values is vital for the convergence behavior of {\tt DFL} described in Theorem~\ref{thm:subLin_m}. 
% Overall, these observations demonstrate that the proposed Algorithm~\ref{GT} can adaptively control the value of $\alpha$ based on the level of delay to achieve better convergence behavior.

\begin{figure}[t]
\includegraphics[width=1.0\columnwidth]{control_alg/alpha_adap.png}
\centering
\caption{Impact of delay and data diversity on $\alpha$: Decision on $\alpha$ increases with delay and decreases with data diversity, indicating that {\tt DFL} emphasizes local models during global synchronization with high delay and global models with high data diversity.}
\label{fig:alpha}
\vspace{-5mm}
\end{figure} 
\subsubsection{Impact of data diversity on the choice of $\alpha$}
We  explore the impact of levels of data diversity across edge devices on the choice of combiner weight $\alpha$ obtained under Algorithm~\ref{GT}. We increase the data diversity by reducing the number of labels in each device's local dataset. We consider $\Delta=10$ and $\tau_k=\tau=30$. Fig.~\ref{fig:alpha}(b) indicates that $\alpha$ decreases as data diversity increases for both CNN and SVM. This is because
local models diverge more from each other when data diversity is high, making the global model's contribution more critical (i.e., reduction in $\alpha$) in aligning the local models of the devices. 

% This aligns with the intuition that {\tt DFL} emphasizes the global model during global synchronization as data diversity increases. 

% This is because
% local models diverge more from the global optimum during local model updates when data diversity is high, making the global model's contribution more critical in reducing global deployed model divergence. 

 


% These findings illustrate how Algorithm~\ref{GT} can adaptively tune $\alpha$ based on data diversity levels to achieve better convergence behavior.

\subsubsection{Enhanced resource efficiency in comparison to baseline methods}
\begin{figure}[t]
\includegraphics[width=1.\columnwidth]{control_alg/resource.png}
\centering
\caption{Compared to the baselines, {\tt DFL} achieves significantly better results in terms of total energy and delay metrics for both CNN and SVM, as shown in (a) and (b) when reaching 80\% testing accuracy.}
\label{fig:res}
\vspace{-6mm}
\end{figure} 
The results presented in Fig.~\ref{fig:res} compare the performance of {\tt DFL} with two baseline approaches: (i) FL with full device participation and $\tau=1$, and (ii) hierarchical FL with $\alpha=0$, $\tau=20$, where local aggregations are performed after every $5$ local model updates. We consider $\Delta=10$ for both scenarios. Two metrics are used to compare the performance: (a) communication energy consumption and (b) communication delay upon reaching $80\%$ of testing accuracy. For (a), the results in Fig.~\ref{fig:res}(a) show that {\tt DFL} significantly outperforms both baseline approaches. Specifically, {\tt DFL} requires $93.6\%$ and $93.3\%$ less energy than baseline (i) and $81.4\%$ and $83.2\%$ less energy than baseline (ii) for CNN and SVM models, respectively. Similarly, for (b), {\tt DFL} requires substantially less communication delay than both baseline approaches as shown in  Fig.~\ref{fig:res}(b). Specifically, {\tt DFL} requires $93.7\%$ and $93.3\%$ less delay than baseline (i) and $45.6\%$ and $51.1\%$ less delay than baseline (ii) for CNN and SVM models, respectively. These results demonstrate the resource-efficiency of {\tt DFL}.

\iffalse
We turn now to evaluating the efficacy and analyzing the behavior of {\tt TT-HF} under parameter tuning from Algorithm~\ref{GT}.

%Our control algorithm migrates from static choice of $\alpha$, $\gamma$ and fixed values of rounds of consensus over the clusters $\Gamma$, consensus error coefficient $\phi$, and local model training intervals $\tau$ to a dynamic setting where these parameters are tuned over time with respect to the gradient diversity and divergence of model parameters. Our next set of experiments evaluate the resource efficiency and behavior of our control algorithm.


\subsubsection{Improved resource efficiency compared with baselines}
Fig.~\ref{fig:resource_bar_0} compares the performance of {\tt TT-HF} under our control algorithm with the two baselines: (i) FL with full device participation and $\tau = 1$ (from Sec.~\ref{ssec:conv-eval}), and (ii) FL with $\tau = 20$ but only one device sampled from each cluster for global aggregations.\footnote{The baseline of FL, $\tau = 20$ with full participation is omitted because it results in very poor costs.} The result is shown under different ratios of delays $\frac{\Delta_{\textrm{D2D}}}{\Delta_{\textrm{Glob}}}$ and different ratios of energy consumption $\frac{E_{\textrm{D2D}}}{E_{\textrm{Glob}}}$ between D2D communications and global aggregations.\footnote{These plots are generated for some typical ratios observed in literature. For example, a similar data rate in D2D and uplink transmission can be achieved via typical values of transmit powers of $10$dbm in D2D mode and $24$dbm in uplink mode~\cite{hmila2019energy,dominic2020joint}, which coincides with a ratio of ${E_{\textrm{D2D}}}/{E_{\textrm{Glob}}}=0.04$. In practice, the actual values are dependent on many environmental factors.} Three metrics are shown: (a) total cost based on the objective of $(\bm{\mathcal{P}})$, (b) total energy consumed, and (c) total delay experienced up to the point where $75\%$ of peak accuracy is reached.

Overall, in (a), we see that {\tt TT-HF} (depicted through the bars) outperforms the baselines (depicted through the horizontal lines) substantially in terms of total cost, by at least 75\% in each case. In (b), we observe that for smaller values of ${E_{\textrm{D2D}}}/{E_{\textrm{Glob}}}$, {\tt TT-HF} lowers the overall power consumption, but after the D2D energy consumption reaches a certain threshold, it does not result in energy savings anymore. The same impact can be observed regarding the delay from (c), i.e., once $\frac{\Delta_{\textrm{D2D}}}{\Delta_{\textrm{Glob}}} \approx 0.1$ there is no longer an advantage in terms of delay. Ratios of $0.1$ for either of these metrics, however, is significantly larger than what is being observed in 5G networks~\cite{hmila2019energy,dominic2020joint}, indicating that {\tt TT-HF} would be effective in practical systems.

%This figure provides a guideline to the network designers to determine the gain of conducting D2D communications. In particular, when D2Ds are low power and low delay, they can result in both significant model performance gain (Figs.~\ref{fig:mnist_poc_1_all} and~\ref{fig:mnist_poc_2_all}) and network costs savings (Fig.~\ref{fig:resource_bar_0}).

\begin{figure}[t]
\vspace{4mm}
\hspace{-2.5mm}
\includegraphics[width=1\columnwidth]{mnist_125/behaviour_of_tau.eps}
\centering
\caption{Value of the second local model training interval obtained through $(\bm{\mathcal{P}})$ for different configurations of weighing coefficients $c_1, c_2, c_3$ (default $c_1 = 10^{-3}, c_2=10^2, c_3=10^4$). Higher weight on energy and delay (larger $c_1$ and $c_2$) prolongs the local training period, while higher weight on the global model loss (larger $c_3$) decreases the length, resulting in more rapid global aggregations.
% \textbf{Effect of $c_1$, $c_2$, $c_3$ on $\tau$.} In (a), $c_2=10^2$ and $c_3=10^4$, in (b) $c_1=10^{-3}$ and $c_3=10^4$, and in (c) $c_1=10^{-3}$ and $c_3=10^2$. Increasing $c_1$ and $c_2$ promotes energy and delay efficiency, prolonging the value of $\tau$; increasing $c_3$ promotes higher performance gain obtained through rapid aggregations, decreasing the value of $\tau$.
}
\label{fig:behaviour_of_tau}
\vspace{-5mm}
\end{figure}

\subsubsection{Impact of design choices on local model training interval}
We are also interested in how the design weights $c_1, c_2, c_3$ in $(\bm{\mathcal{P}})$ affect the behavior of the control algorithm. In Fig.~\ref{fig:behaviour_of_tau}, we plot the value of $\tau_2$, i.e., the length of the second local model training interval, for different configurations of $c_1$, $c_2$ and $c_3$.\footnote{The specific ranges of values chosen gives comparable objective terms $(a)$, $(b)$, and $(c)$ in $(\bm{\mathcal{P}})$.} The maximum tolerable value of $\tau$ is assumed to be $40$. As we can see, increasing $c_1$ and $c_2$ -- which elevates the priority on minimizing energy consumption and delay, respectively -- results in a longer local model training interval, since D2D communication is more efficient. On the other hand, increasing $c_3$ -- which prioritizes the global model convergence rate -- results in a quicker global aggregation.
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsubsection{Impact of non-convex ML model}


% It can be seen that as the energy consumption of D2D communications decreases the tendency to remain in the local model training increases via tuning longer period of local model training intervals. The same impact can be seen as the delay of D2D communications start decreasing. 




% \begin{figure*}[t]
% \includegraphics[width=1.01\textwidth]{mnist_125/resource_consumption.eps}
% \centering
% \caption{\textbf{Resource consumption under different configurations}.} 
%     \label{fig:resource_consumption}
% % \vspace{-7mm}
% \end{figure*}