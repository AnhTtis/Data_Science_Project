\pdfoutput=1
 % \documentclass[12pt, journal,onecolumn,draftclsnofoot]{IEEEtran}
\documentclass[12pt, draftclsnofoot, onecolumn]{IEEEtran}
% \documentclass[journal]{IEEEtran}
\IEEEoverridecommandlockouts
% \usepackage[left=0.67in,right=0.67in,top=0.7in,bottom=1.1in]{geometry}
\usepackage{amsmath,graphicx,epstopdf,amssymb,amsthm}% ,epstopdf,spconf,amssymb,epsfig,fullpage}%, ,graphicx,psfrag,url,amsmath,amsthm,comment} 
%\usepackage{acronym} 
\usepackage{cite} 
\usepackage{hyperref}
% \usepackage{ulem,color}   
\usepackage{epstopdf} 
\usepackage{amsmath,bm}    
\usepackage{verbatim} 
%\usepackage{unicode-math} 
%\usepackage{graphicx}  
%\usepackage[]{graphics} 
\newcommand{\ie}{\textit{i.e. }}
\newcommand{\eg}{\textit{e.g. }}
%\def\yesnumber{\global\@myeqnswtrue}
%\newcommand{\noteout}[1]{\textcolor{red}{\sout{}}}       
%\def\noteout    
\usepackage[utf8]{inputenc}     
\usepackage[english]{babel}   
%\usepackage{slashbox}  
\usepackage{caption}    
\usepackage{enumitem} 
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\DeclareCaptionLabelFormat{lc}{\MakeLowercase{#1}~#2}
\captionsetup{labelfont=sc,labelformat=lc}
\usepackage[dvipsnames]{xcolor}
 
\newenvironment{skproof}{\noindent\textit{Sketch of  Proof:}}{\hfill$\blacksquare$}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma} 
\newtheorem{fact}{Fact}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{claim}{Claim} 
\newtheorem{assumption}{Assumption}
\newtheorem{condition}{Condition}
\newtheorem{observation}{Observation}
\newcommand{\note}[1]{\textcolor{blue}{#1}}
\newcommand{\rem}[1]{{\color{blue} {\bf [REMOVE: #1]}}}
\allowdisplaybreaks
\usepackage[protrusion=true,expansion=true]{microtype}
\pdfoutput=1
\renewcommand{\figurename}{Fig.}
\usepackage[font=small]{caption}
\newcommand{\ali}[1]{{\color{magenta} {{\bf}#1}}}
\newcommand{\chris}[1]{{\color{ForestGreen} {{\bf [Chris: #1]}}}}
\newcommand{\shams}[1]{{\color{cyan} {{\bf [S: #1]}}}}
\newcommand{\mathcolorbox}[2]{\colorbox{#1}{$\displaystyle #2$}}
\usepackage{float}
\usepackage{mathtools}
\newcommand{\frank}[1]{{\color{blue} {\bf FL: #1}}}
\newcommand{\addFL}[1]{\textcolor{blue}{#1}}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand{\cgb}[1]{{\color{blue} {\bf CGB: #1}}}


\newcommand{\nm}[1]{{\color{red} {\bf [NM: #1]}}}
\newcommand{\add}[1]{\textcolor{red}{#1}}
\newcommand{\sst}[1]{\st{#1}}
\usepackage{cancel}
\newcommand\mst[2][red]{\setbox0=\hbox{$#2$}\rlap{\raisebox{.45\ht0}{\textcolor{#1}{\rule{\wd0}{2pt}}}}#2}   
\usepackage{soul,xcolor}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
% \newtheorem{definition}{Definition}
% \newtheorem{lemma}{Lemma}
% \newtheorem{theorem}{Theorem}
% \newtheorem{proposition}{Proposition}
% \newtheorem{corollary}{Corollary}
% \newtheorem{assumption}{Assumption}
% \newtheorem{condition}{Condition}
% \usepackage{bbm, dsfont}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\setulcolor{red}
\setul{red}{2pt}
\setstcolor{red}
%\newcommand\semiSmall{\fontsize{23.8}{20.38}\selectfont}
%\title{D2D-assisted Federated Learning: Hybrid Distributed Machine Learning in Two Timescales}
\title{Delay-Aware Hierarchical Federated Learning}
% : Intelligent Model Aggregation in Large-scale Wireless Edge Networks}

% Distributed model training under cooperative D2D communications.. 


% A consensus-driven distributed model training platform via cooperative device-to-device communications



% Some keywords: Device-to-device, peer-to-peer, Non-i.i.d data (to emphasize our new definition...), resource constrained, local descent method (we have multiple local descents),
% D2D, P2P, cluster-based, local cooperation.., locally cooperative devices ....



%  D2D-assisted hybrid federated learning with Aperiodic Consensus

\author{Frank Po-Chen Lin,~\IEEEmembership{Student Member,~IEEE},  Seyyedali~Hosseinalipour,~\IEEEmembership{Member,~IEEE}, Christopher G. Brinton,~\IEEEmembership{Senior~Member,~IEEE}, and Nicol\`o Michelusi, \IEEEmembership{Senior~Member,~IEEE}
\thanks{F. Lin and C. Brinton are with the School of Electrical and Computer Engineering, Purdue University, IN, USA. e-mail: \{lin1183,cgb\}@purdue.edu. Brinton and Lin acknowledge support from ONR grants N000142212305 and N000142112472.}
\thanks{S. Hosseinalipour is with the Department of Electrical Engineering, University at Buffalo, NY, USA. e-mail: alipour@buffalo.edu.}
\thanks{N. Michelusi is with the School of Electrical, Computer and Energy Engineering, Arizona State University, AZ, USA. e-mail: nicolo.michelusi@asu.edu. Part of his research has been funded by NSF under grant CNS-2129015.}
\thanks{A condensed version of this paper was presented at IEEE Globecom 2020~\cite{frank2020delay}.}}
\maketitle


\begin{abstract}
Federated learning has gained popularity as a means of training models distributed across the wireless edge.
The paper introduces delay-aware federated learning ({\tt DFL}) to improve the efficiency of distributed machine learning (ML) model training by addressing communication delays between edge and cloud. {\tt DFL} employs multiple stochastic gradient descent iterations on device datasets during each global aggregation interval and intermittently aggregates model parameters through edge servers in local subnetworks. The cloud server synchronizes the local models with the global deployed model computed via a local-global combiner at global synchronization. The convergence behavior of {\tt DFL} is theoretically investigated under a generalized data heterogeneity metric. A set of conditions is obtained to achieve the sub-linear convergence rate of $\mathcal O(1/k)$. Based on these findings, an adaptive control algorithm is developed for {\tt DFL}, implementing policies to mitigate energy consumption and edge-to-cloud communication latency while aiming for a sublinear convergence rate. Numerical evaluations show {\tt DFL}'s superior performance in terms of faster global model convergence, reduced resource consumption, and robustness against communication delays compared to existing FL algorithms. In summary, this proposed method offers improved efficiency and satisfactory results when dealing with both convex and non-convex loss functions.
%Federated learning relies on device-server communications to train a machine learning (ML) model, neglecting device-to-device (D2D) communications promoted in wireless networks. In this paper, we propose a novel ML model training architecture called \textit{two timescale hybrid federated learning} ({\tt TT-HF}), where we consider a cooperative cluster-based model training methodology and introduce a hybrid learning paradigm, where cooperative D2D communications are utilized in conjunction with device-server communications. In {\tt TT-HF}, during  each  global  aggregation interval, devices perform multiple local stochastic gradient descent (SGD) iterations and aperiodically synchronize their model parameters through local consensus via D2D  communications. This enables model training in two timescales capturing local SGD iterations and D2D communication rounds.
%We introduce a general definition on the gradient diversity, and investigate the convergence of {\tt TT-HF} that leads us to new convergence bounds for distributed ML. We exploit our theoretical findings to develop an online algorithm that actively tunes the step size, the D2D communications rounds, and the interval of global aggregations.
%Through extensive simulations, we demonstrate that the proposed framework is robust against extreme heterogeneity of the users' datasets and can achieve a higher accuracy as compared to the current art methods while imposing a smaller network cost.    
\end{abstract} 

\begin{IEEEkeywords}
\noindent Federated learning, edge intelligence, network optimization, convergence analysis, hierarchical architecture.
\end{IEEEkeywords}

\input{intro.tex}
\input{system.tex}
\input{convergence.tex}
\input{control.tex}
\input{experiments.tex}


 



\section{Conclusion and Future Work}
\noindent In this work, we proposed {\tt DFL}, which is a novel methodology that aims to improve the efficiency of distributed machine learning model training by mitigating the round-trip communication delay between the edge and the cloud. {\tt DFL} quantifies the effects of delay and modifies the FL algorithm by introducing a linear local-global model combiner used in the local model synchronization steps.
% , optimizing model performance under a hierarchical model training architecture subject to communication delay. 
We investigated the convergence behavior of {\tt DFL} under a generalized data heterogeneity metric and obtained a set of conditions to achieve sub-linear convergence. Based on these characteristics, we developed an adaptive control algorithm that adjusts the learning rate, local aggregation rounds, combiner weight, and global synchronization periods. Our numerical evaluation showed that {\tt DFL} leads to a faster global model convergence, lower resource consumption, and a higher robustness against communication delay compared to existing FL algorithms.
% Future work includes the implementation of {\tt DFL} in a real-world system and testing its performance in a real-world setting. Moreover, 
Future research directions include improving the robustness of DFL against different types of network impairments, such as jitter and packet loss, and investigating its performance under flexible device participation.

\iffalse
\noindent We proposed {\tt TT-HF}, a methodology which improves the efficiency of federated learning in D2D-enabled wireless networks by augmenting global aggregations with cooperative consensus formation among device clusters. We conducted a formal convergence analysis of {\tt TT-HF}, resulting in a bound which quantifies the impact of gradient diversity, consensus error, and global aggregation periods on the convergence behavior. Using this bound, we characterized a set of conditions under which {\tt TT-HF} is guaranteed to converge sublinearly with rate of $\mathcal{O}(1/t)$. Based on these conditions, we developed an adaptive control algorithm that actively tunes the device learning rate, cluster consensus rounds, and global aggregation periods throughout the training process. Our experimental results demonstrated the robustness of {\tt TT-HF} against data heterogeneity among edge devices, and its improvement in trained model accuracy, training time, and/or network resource utilization in different scenarios compared to the current art.

There are several avenues for future work. To further enhance the flexibility of {\tt TT-HF}, one may consider (i) heterogeneity in computation capabilities across edge devices, (ii) different communication delays from the clusters to the server, and (iii) wireless interference caused by D2D communications.
\fi
% \pagebreak 
\bibliographystyle{IEEEtran}
\bibliography{ref}

\pagebreak

% \input{appendices.tex}
\input{appendices2.tex}


\end{document}

