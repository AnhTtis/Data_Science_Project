% \nm{Where is the abstract?}

% \nm{I downloaded the latex to work offline on it and add my comments, but there are several compilation errors. Please fix the errors!}

Machine Learning (ML) has become a popular tool for carrying out a variety of practical applications in   computer vision, speech recognition, natural language processing, and robotic control~\cite{wu2017squeezedet,AI_jordan,goldberg2017neural}. Conventionally, ML model training for these applications is conducted in a centralized manner, where
data from different sources is collected and processed in a single server/datacenter.
Nevertheless, in many applications, data used for model training is generated/gathered at the modern Internet-of-Things (IoT) devices located at the edge of the network (e.g., autonomous vehicles, mobile phones, and wearable devices)~\cite{CVN}, which makes centralized model training impractical. In fact, transferring the massive amount of data collected from the IoT devices to a central location imposes high latency and power/resource consumption~\cite{Chiang}, which is not desired especially in real-time applications~\cite{wu2017squeezedet,hard2018federated}. 
Also, privacy concerns related to transmitting private data across the network have progressively advocated for storing data locally and shifting the ML model training to the network edge where the data gets collected. This has led to an emerging area of distributed ML over the network edge, which exploits the distributed computing power of IoT devices to realize an intelligent edge/fog~\cite{hosseinalipour2020federated,park2019wireless}.

% This explosive growth of Internet-of-Things (IoT) devices has brought in a new era in edge computing, providing a greater distribution of computing power.


% In addition, coupled with privacy concerns over transmitting private information across the network, it has become progressively attractive to store data locally and shift model training computation to the edge. 

% Moreover, due to the excessive growth of data on each device, addressing the costs of bandwidth for data traveling long distances from edge to a remote server has become critical to enabling real-time applications~\cite{Chiang}, including object detection and next word prediction on autonomous vehicles and smartphones~\cite{wu2017squeezedet,hard2018federated}.




% This explosive growth of Internet-of-Things (IoT) devices has brought in a new era in edge computing, providing a greater distribution of computing power.

% In addition, coupled with privacy concerns over transmitting private information across the network, it has become progressively attractive to store data locally and shift model training computation to the edge. Moreover, due to the excessive growth of data on each device, addressing the costs of bandwidth for data traveling long distances from edge to a remote server has become critical to enabling real-time applications~\cite{Chiang}, including object detection and next word prediction on autonomous vehicles and smartphones~\cite{wu2017squeezedet,hard2018federated}. 



% Consequently, this trend results in a paradigm shift from the conventional ML training approach to distributed ML training. Multiple edge devices collaboratively train a shared model locally by exchanging information across the network.


Federated learning (FL)~\cite{mcmahan2017communication}, a popular distributed ML framework, trains an ML model via engaging edge devices in collaborative model training while keeping their data locally.
% as depicted in Fig.~\ref{fig:simpleFL}.
% to train an ML model collaboratively while keeping the training data locally without transmitting them to a centralized location.
It does so by executing three steps repeatedly: (i) \textit{local model training} at each edge device using its local dataset, (ii) \textit{aggregation of the local models} to a global model by a server, and (iii) \textit{synchronization of the local models} with the newly obtained global model.
% , 
% during which each edge device updates the local model with the global one. 
Upon being implemented over a real-world network edge, FL faces many challenges and design problems given the heterogeneities existing at the wireless edge \cite{lim2020federated,hosseinalipour2020federated}. (i) \textit{Extreme Data Heterogeneity:}
the local datasets of the devices may exhibit significant heterogeneity, making them non-independent and identically distributed (non-i.i.d.), causing the locally trained models in each edge device to be significantly biased towards the local dataset~\cite{hosseinalipour2020federated}. 
(ii) \textit{Delay of Model Aggregations:} Variable distances and quality of communications between the edge devices and the point of aggregation (e.g., a cloud server) may result in considerable model aggregation and synchronization delays, making naive synchronization of the received global model with the local model impractical~\cite{frank2020delay}. 
(iii) \textit{Hierarchical Architecture of Network Edge:} Large-scale edge networks do not admit the conventional FL network architecture.
% in Fig.~\ref{fig:simpleFL}. 
In particular, edge devices are often not directly connected to a cloud server, instead, they are connected to the edge servers, which facilitate the communication to the cloud server~\cite{wang2021HFL}.
% federated learning, aims to enable large-scale model training by massive devices at the edge without exposing their local datasets to a third-party device. However, performing model training via the traditional cloud-based network architecture suffers from the communication bottleneck in the core network when implemented at scale~\cite{wang2021HFL}.
In this paper, we are motivated to address the above three challenges. In particular, we consider model training under a
% \nm{are you defining a new metric, or extending an existing one? Based on your Def 1 and Def 2, I think you are extending it} 
metric of data heterogeneity extended from the current art, which can capture extreme cases of data diversity across the devices. Also, we explicitly account for the delay in model aggregation and introduce a linear local-global model combining scheme. This scheme retains essential elements of both the outdated global model and the current local model, thereby improving the overall learning efficiency. Our methodology can augment existing studies by incorporating our strategy with those already established. Finally, we consider model training over a realistic hierarchical network edge architecture.


\textbf{Summary of Contributions}\label{subsec:contrib}:\\
Our contributions in this work can be summarized as follows:
\begin{itemize}[leftmargin=5mm]
\item We propose \textit{delay-aware federated learning} ({\tt DFL}), a novel methodology for improving distributed ML model training efficiency by accounting for the round-trip delay between edge and cloud. 
% (Sec. \ref{sec:tthf}). 
{\tt DFL} accounts for the effects of delays by introducing a local-global model combiner scheme during global synchronization, which conserve vital aspects of both the stale global model and the current local model, thereby enhancing overall learning efficiency.

\item  We theoretically investigate the convergence behavior of {\tt DFL} under a generalized data heterogeneity metric. Our convergence analysis introduces new techniques, such as coupled dynamic systems, to the current art in hierarchical FL~\cite{Feng2022HFL,Lim2021HFL,Xu2022HFL,Luo2020HFEL,wang2021HFL,Mhaisen2022HFL}. We obtain a set of conditions to achieve the sub-linear convergence rate of $\mathcal{O}(1/k)$ for strongly convex and smooth loss functions while mitigating the communication delay, which resembles the convergence rate of stochastic gradient descent in centralized model training without delay.

\item Leveraging the convergence characteristics, we introduce an adaptive control algorithm for {\tt DFL}, which targets a joint optimization of communication energy, latency, and ML performance while preserving a sub-linear convergence rate. This involves solving a non-convex integer programming problem, adapting (i) the global aggregation interval with the cloud, (ii) the local aggregation interval with the edge servers, (iii) the learning rate, and (iv) the combiner weight over time.

\item Our numerical evaluations demonstrate the effectiveness of {\tt DFL} in terms of convergence speed and resource consumption under various network settings, under both convex and non-convex loss functions. We observe that {\tt DFL} achieves (i) faster convergence of the global model, (ii) reduced resource consumption, and (iii) improved robustness against communication delays compared to existing FL algorithms.
%The combination of device-to-device and device-to-server communications aims to minimize network costs while maximizing trained global model accuracy and convergence speed.

%Based on an organization of edge devices into local clusters, {\tt TT-HF} augments organizes edge devices into clusters and orchestrates the edge devices by partitioning them into multiple clusters in each of which cooperative D2D communications can be performed. Subsequently, we introduce a \textit{hybrid} learning paradigm where the model training consists of both device-server (inter-cluster) and cooperative D2D (intra-cluster) communications.  Also, we constitute a \textit{two timescale} learning paradigm that relies on successive SGD interactions complemented by aperiodic consensus formation among the devices via heterogeneous number of D2D communications  in conjunction with aperiodic global aggregations.

% We theoretically derive the convergence behavior of {\tt DFL} on the decrease \nm{?? (sentence makes no sense)} optimality gap between the global model loss and optimal loss achieved at each round in the presence of non-i.i.d data distributions across different edge devices.

% Through techniques including coupled dynamic systems with a general\nm{what do you mean by general?} definition of gradient diversity (Sec. \ref{sec:convAnalysis}), our bounds quantify \nm{THE please use articles properly} dependence of model performance on properties of the loss function, the weighting used in synchronization, and the communication delay. In doing so, we obtain a set of conditions to achieve a sublinear convergence rate of $\mathcal{O}(1/t)$ while mitigating network resource utilization.
% \nm{are you sure you are "minimizing"? Can you demonstrate that your allocation achieves MINiMum cost? IF not, you need to sue a different word. mitigating? reducing?}

%and obtain an upper bound on its conv under a new general assumption on gradient diversity that accommodates for extreme heterogeneity among the local datasets. Our convergence bound relates topology structure inside different clusters, gradient diversity, number of D2D communications, and interval of consecutive  local and global aggregations to the convergence behavior of {\tt TT-HF}. We then obtain the condition under which the {\tt TT-HF} converges with the rate of $\mathcal{O}(1/t)$. 

% (Sec. \ref{Sec:controlAlg}). 
% This control algorithm obtains the $\mathcal{O}(1/t)$ convergence rate by including our derived conditions. 
% \hl{as constraints in the optimization.}\nm{too much d etails I dont think the reader can follow}

%\item Using our convergence relationships, we develop an adaptive control algorithm for {\tt TT-HF} that obtain a set of requirements on parameters of the consensus process (D2D rounds per local update instance), local learning algorithm (gradient step size), and global coordination (interval between model aggregations) to guarantee certain convergence behaviors. We then use these conditions to develop an online control algorithm for {\tt TT-HF} that tunes these parameters during the learning period (Sec. \ref{Sec:controlAlg}).

%\item Through studying the convergence characteristic, we obtain a set of new general policies on the required number of D2D rounds at each instance of local aggregation, the choice of step-size, and the interval of local training to guarantee certain convergence behavior. We subsequently utilize our theoretical findings to develop an online control algorithm to tune these design parameters during the learning period in real-time.

% We also show the delay-robustness of {\tt DFL} relative to existing FL algorithms, i.e., that performance is much more stable for variations in delay.

% \item Our subsequent experiments on popular learning tasks (Sec. \ref{sec:experiments}) verify that {\tt DFL} outperforms federated learning substantially in terms of resource consumption and/or training time over D2D-enabled wireless devices. They also confirm that the control algorithm is able to address resource limitations and data heterogeneity across devices by adapting the local and global aggregation periods.
\end{itemize}
The structure of this paper is as follows: Section~\ref{sec:RW} delves into works related to this study, specifically addressing the challenges mentioned in Federated Learning (FL). Subsequently, Section~\ref{sec:tthf} outlines the system model and details the machine learning methodology implemented in {\tt DFL}. A theoretical analysis of {\tt DFL}'s convergence behavior is provided in Section~\ref{sec:convAnalysis}, followed by a discussion on our adaptive control algorithm designed to optimize controllable parameters within {\tt DFL} in Section~\ref{Sec:controlAlg}. Lastly, Section~\ref{sec:experiments} showcases numerical experiments conducted to assess the performance of {\tt DFL}.
For brevity, proofs are provided as sketches with full versions in the appendices.


\section{Related Work}\label{sec:RW}
We categorize the related work to this study with respect to the three aforementioned challenges of FL. For a comprehensive survey of works on FL, we refer the interested reader to~\cite{Kairouz}.

\textbf{Non-i.i.d. Data Across the Devices.}
Non-i.i.d data across the devices has been shown to significantly reduce the performance of FL due to local model bias \cite{mcmahan2017communication}. To counteract this effect, \cite{wang2019adaptive,Sun2021adaptive} tunes the frequency of global model aggregations, \cite{tu2020network,wang2021device} conduct data transfer across the devices to reduce the heterogeneity of data, and~\cite{Tran2019cap,chen2019joint,yang2020energy} conduct efficient resource allocation under non-i.i.d. data. However, most of these works rely on a simple modeling of non-i.i.d. data across the devices which cannot be generalized to real-world settings.
Recently, we introduced a new metric of data heterogeneity in \cite{lin2021timescale} that extends the current art and is able to capture extreme non-i.i.d. data across the devices. However, in \cite{lin2021timescale}, this metric is exploited in a completely different framework, where devices conduct local device-to-device (D2D) communications for model consensus, as compared to this works. In this work, we aim to conduct convergence analysis of model training under this general data heterogeneity metric, while taking into account for communication delay and hierarchical network architecture.

% Variable wireless connections in edge networks can result in unreliable communication, which operates at lower rates and thus cause communication delays that can impact distributed ML techniques. 


\textbf{Model Training under Delay Considerations.} Upstream/downstream communication between the edge and server can
% be very extremely expensive considering the 
often lead to non-negligible transmission delays in FL. 
% One major technique that has been developed to address this issue in federated learning is decreasing the frequency of local model updates in between global aggregations~\cite{wang2019adaptive,tu2020network}. The paper \cite{McMahan} proposed reducing the frequency of communication in federated learning by performing global updates after multiple iterations of local updates, i.e., decreasing the total number of rounds. In addition, the paper \cite{K2} proposed combining random sparsification with probabilistic quantization on model updates to further reduce the number of upstream and downstream communication rounds required. However, by increasing the length of the period of local model updates, the local datasets may exhibit significant heterogeneity in their statistical distributions causing the local models in each edge device to be significantly biased towards their own local datasets~\cite{hosseinalipour2020federated}. This can in turn result in significant degradation of accuracy performance of the global model along with the convergence speed~\cite{wang2019adaptive}. On the other hand, significant communication bandwidth for model/gradient exchange is required for distributed training in large-scale networks, limiting the scalability of multi-device training at the edge and requiring expensive high-bandwidth network infrastructure. Consequently, other efforts have proposed compression techniques to reduce the communication required between the edge devices and the server. In particular, some proposed compression methods, i.e., cutting the gradient
% size via gradient compression, to reduce the bandwidth required in each transmission~\cite{Lin,K3,Horvath}. Furthermore, the papers \cite{Tu} and \cite{lin2021timescale} proposed a network-aware federated learning architecture that trades off communication demand with model convergence.
% \nm{most of the works you mention above are not quite relevant to what you are doing. You should instead stress the fact that these works do not consider hte impact of delay which maydegrade the performance of these algos..}
% Different from these prior works, rather than minimizing communication costs or frequency of federated learning\nm{are you saying that your scheme does not reduce communication cost? After you have been saying in the intro that comm cost is one of the biggest hurdles of FL..
% if I am a reviewer and I read this sentence, I would be concerned to read this..
% }, we focus on the impact of delays on the performance of the trained global model, i.e., convergence rate and accuracy, and develop an adaptive algorithm to address them. \nm{why are state of art schemes not adequately addressing this issue? Your literature review does not address this question}
Delay of model training in FL has been modeled and considered in several recent works \cite{Tran2019cap,PSL2022federated,Shi2021latency,sam2022latency,Zhao2022layency,Gao2021latency}. These works model the delay with respect to the channel conditions between the devices and the server and the devices' local computation power. The delay is often aimed to be minimized in these works to have the fastest model training scheme. However, none of these works aim to \textit{mitigate} the impact of delay via intelligent synchronization of the received global model and the local model at the devices. In this work, we study this under-explored topic via proposing a linear local-global model combiner.



\textbf{Hierarchical Federated Learning.}
% The combination of AI and edge computing has given rise to edge intelligence, leveraging the capabilities of end devices to process data~\cite{Lim2022EI}. One of the promising solutions, 
% federated learning\nm{you have been talking about FL until now, you make it seem like you are talking about something new..}, aims to enable large-scale model training by massive devices at the edge without exposing their local datasets to a third-party device. However, performing model training via the traditional cloud-based network architecture suffers from the communication bottleneck in the core network when implemented at scale~\cite{wang2021HFL}. 
% As a result,  
Several studies have aimed to propose new system architectures to improve the scalability of ML model training in FL~\cite{liu2020client,Lim2022EI,wang2021HFL,lin2021timescale,Zhao2020HFL}. Specifically, hierarchical federated learning has been proposed as a realistic system model, which allows edge devices to first transmit their local model parameters to edge servers for intermediate aggregation before edge servers conduct global aggregation through a cloud server, which reduces the frequency of performing resource intensive global aggregations~\cite{Feng2022HFL,Lim2021HFL,Xu2022HFL,Luo2020HFEL,wang2021HFL,Mhaisen2022HFL}. This architecture has been investigated through the lens of resource allocation \cite{Xu2022HFL,Feng2022HFL}, and edge association~\cite{Luo2020HFEL,wang2021HFL,Lim2021HFL,Mhaisen2022HFL}. Although these works provide valuable design insights, they mostly consider a simplistic data heterogeneity metric and do not take into account for smart local synchronization of the local models with the global model. As compared to these works, we introduce a new ML convergence analysis scheme that takes into account for a generalized data heterogeneity metric under a new linear local-global model combiner scheme. Drawing from the convergence properties, we devise a control algorithm for {\tt DFL}, which incorporates strategies aimed at mitigating energy consumption and communication latency.
% \nm{I think you need to talk about your control scheme as well that aims at optimizing resource utilization..}

% Our work considers a large-scale hierarchical federated learning system, accounting for the non-negligible delays between the edge devices and the cloud, and formulate a joint problem of edge aggregation interval and global aggregation interval control to minimize the global training loss as well as resource consumption. 



% The paper \cite{Xu2022HFL} formulated a joint problem of edge aggregation interval control and resource allocation to minimize the weighted sum of training loss and training latency for a hierarchical federated learning framework. Specifically, given the resource allocation strategy, a relaxation and rounding method is proposed to optimize the edge aggregation interval. Some develop edge association schemes for hierarchical federated learning taking into account resource consumption by formulating a joint resource allocation and edge association problem to achieve global cost minimization~\cite{Luo2020HFEL,wang2021HFL,Lim2021HFL}. 

% The paper \cite{Mhaisen2022HFL} optimized the edge association problem based on statistical properties and network topology to accord edge-level data distributions across edge devices and enhance the learning performance. The paper \cite{Feng2022HFL} redesigned the access mechanism, local update rule, and model aggregation scheme for a hierarchical learning framework to account for the impact of user mobility on the learning performance. The paper \cite{Luo2020HFEL} introduced a hierarchical federated edge learning framework with partial model aggregation migrated to edge servers from the cloud. They formulated a joint computation and communication resource allocation and edge association problem via resource scheduling for edge devices to achieve global cost minimization. Other studies proposed a server-less hierarchical federated learning framework to reduce the reliance on a central controller and the risk of a single point of failure~\cite{Ng2021HFL,Lim2021HFL}. The paper \cite{Lim2021HFL} considers a two-level resource allocation and incentive mechanism design problem applying the evolutionary game theory to model the dynamics of the cluster selection process. In addition, the paper \cite{Ng2021HFL} proposed a reputation-aware scheme under a two-layer network architecture, taking into account the incentive design for edge devices marginal contributions by rewarding their marginal contribution to the cluster.
% \nm{What are the weaknesses of these works that you aim to address? You need to identify these, otherwise it is not clear how you work improves over the state of art..}


\iffalse
A multitude of works on federated learning has emerged in the past few years. Most existing literature has studied optimizations/enhancements over the classic structure depicted in Fig.~\ref{fig:simpleFL}. Popular topics of investigation have included addressing communication and computation constraints of wireless devices~\cite{8737464,chen2019joint,yang2019energy,yang2020federated}, supporting multi-task learning~\cite{smith2017federated,corinzia2019variational,9006060}, and personalized model training~\cite{jiang2019improving,fallah2020personalized}. We refer the reader to e.g.,~\cite{rahman2020survey,li2020federated} for comprehensive surveys of the federated learning literature; in this section, we will focus on those addressing resource efficiency, statistical data heterogeneity, and cooperative learning as we do in this paper.


%we refer the reader to \cite{rahman2020survey} for a comprehensive survey. Most existing literature in this area has studied optimizations of federated learning over its classic structure depicted in Fig.~\ref{fig:simpleFL}. In particular, the consideration of computation and communication constraints of devices over wireless networks is an active area of research~\cite{8737464,chen2019joint,yang2019energy,yang2020federated}.
%Also, federated learning has been studied in other contexts, such as multi-task learning and personalized model training~\cite{smith2017federated,corinzia2019variational,9006060,jiang2019improving,fallah2020personalized}, where individual models are tailored for different users. We refer the reader to~\cite{li2020federated} and references therein for other research directions and interesting problems investigated.

In terms of wireless communication efficiency, several works have investigated the impact of performing multiple rounds of local gradient updates in-between consecutive global aggregations~\cite{haddadpour2019convergence,wang2019adaptive}, including optimizing the aggregation period according to a total resource budget~\cite{wang2019adaptive}. To further reduce the demand for global aggregations,~\cite{liu2020client} proposed a hierarchical system model for federated learning where edge servers are utilized for partial global aggregations. Regarding device processing efficiency,~\cite{tu2020network} showed that data offloading in D2D-enabled wireless networks can reduce resource utilization while preserving model accuracy. Model quantization~\cite{amiri2020federated} and sparsification~\cite{sattler2019robust} techniques have also been proposed.

% Conducting multiple local descents is mostly advocated in terms of energy efficiency since it can reduce the number of global aggregations in a certain period that are more energy consuming.
%However, these approaches are prone to the existence of nodes with non-i.i.d datasets, where multiple local descent updates may lead to the model biasedness toward the local datasets, which in turn could lead to severe deterioration of the convergence speed of the model training~\cite{wang2019adaptive}.

Other works have considered improving model training in the presence of heterogeneous data among the devices. Many of these techniques have involved raw data sharing over the network, e.g.,~\cite{9149323,wang2021device,zhao2018federated}. In~\cite{9149323}, the authors propose uploading portions of the local datasets to the server, which is then used to augment global model training. The work in~\cite{zhao2018federated} can be seen as an additional step, where the server shares a portion of its aggregated data among the devices to make their local data distributions less heterogeneous.~\cite{wang2021device} also attempts to promote homogeneity among local datasets, but instead through  D2D data offloading optimization. Raw data sharing, however, may suffer from privacy concerns or bandwidth limitations.

Different from these works, we propose a methodology that addresses the communication efficiency and data heterogeneity challenges simultaneously. To do this, we introduce distributed cooperative learning among devices into the local update process -- as has been advocated recently~\cite{hosseinalipour2020federated,chen2020wireless} -- resulting in a novel system architecture with P2P-augmented learning. In this regard, the most relevant existing work is~\cite{hosseinalipour2020multi}, which also studies cluster-based consensus formation between global aggregations. Different from~\cite{hosseinalipour2020multi}, we consider the case where (i) devices may conduct multiple (stochastic) gradient iterations between global aggregations, (ii) the global aggregations are aperiodic, and (iii) consensus formation among the devices may occur aperiodically during each global aggregation. Doing so leads to a more complex system model, which we analyze to provide improvements to resource efficiency and model convergence. There is also an emerging set of works on fully decentralized (server-less) federated learning~\cite{9154332,8950073,hu2019decentralized,lalitha2019peer}: our work can be seen as an intermediate between the star topology and fully distributed approaches when a server is available.

Finally, note that there is a well-developed literature on consensus-based optimization, e.g.,~\cite{yuan2011distributed,shi2014linear}. Our work employs the distributed average consensus technique~\cite{xiao2004fast} and contributes new results on distributed ML to this literature.
\fi

% \nm{the submitted paper needs to be self-standing. Since I don't think you will include the appendices, you need to refer to the extended version on arxiv.}


% \nm{the submitted paper needs to be self-standing. Since I don't think you will include the appendices, you need to refer to the extended version on arxiv.}