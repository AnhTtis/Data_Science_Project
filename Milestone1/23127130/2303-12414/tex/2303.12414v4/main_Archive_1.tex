% \documentclass[10pt, conference, letterpaper]{IEEEtran}
\documentclass[12pt, draftcls, onecolumn]{IEEEtran}
\IEEEoverridecommandlockouts
% \usepackage[left=0.67in,right=0.67in,top=0.7in,bottom=1.1in]{geometry}
\usepackage{amsmath,graphicx,epstopdf,amssymb,amsthm}% ,epstopdf,spconf,amssymb,epsfig,fullpage}%, ,graphicx,psfrag,url,amsmath,amsthm,comment}
%\usepackage{acronym}
\usepackage{cite}
\usepackage{hyperref}
% \usepackage{ulem,color}
\usepackage{epstopdf}
\usepackage{verbatim}
%\usepackage{unicode-math}
%\usepackage{graphicx}
%\usepackage[]{graphics}
\newcommand{\ie}{\textit{i.e. }}
\newcommand{\eg}{\textit{e.g. }}
%\def\yesnumber{\global\@myeqnswtrue}
%\newcommand{\noteout}[1]{\textcolor{red}{\sout{}}}
%\def\noteout
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
%\usepackage{slashbox}
\usepackage{caption}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\DeclareCaptionLabelFormat{lc}{\MakeLowercase{#1}~#2}
\captionsetup{labelfont=sc,labelformat=lc}
\usepackage[dvipsnames]{xcolor}

\newenvironment{skproof}{\noindent\textit{Sketch of  Proof:}}{\hfill$\blacksquare}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{claim}{Claim}
\newtheorem{assumption}{Assumption}
\newtheorem{condition}{Condition}
\newtheorem{observation}{Observation}
\newcommand{\note}[1]{\textcolor{blue}{#1}}
\allowdisplaybreaks
\usepackage[protrusion=true,expansion=true]{microtype}
\renewcommand{\figurename}{Fig.}
\usepackage[font=small]{caption}
\newcommand{\ali}[1]{{\color{magenta} {{\bf Ali}: #1}}}
\newcommand{\chris}[1]{{\color{ForestGreen} {{\bf Chris}: #1}}}

\usepackage{mathtools}
\newcommand{\frank}[1]{{\color{blue} {\bf FL: #1}}}
\newcommand{\addFL}[1]{\textcolor{blue}{#1}}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\nm}[1]{{\color{red} {\bf [NM: #1]}}}
\newcommand{\add}[1]{\textcolor{red}{#1}}
\newcommand{\sst}[1]{\st{#1}}
\usepackage{cancel}
\newcommand\mst[2][red]{\setbox0=\hbox{$#2$}\rlap{\raisebox{.45\ht0}{\textcolor{#1}{\rule{\wd0}{2pt}}}}#2}   
\usepackage{soul,xcolor}
\DeclareMathOperator*{\argmin}{arg\,min}
% \newtheorem{definition}{Definition}
% \newtheorem{lemma}{Lemma}
% \newtheorem{theorem}{Theorem}
% \newtheorem{proposition}{Proposition}
% \newtheorem{corollary}{Corollary}
% \newtheorem{assumption}{Assumption}
% \newtheorem{condition}{Condition}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\setulcolor{red}
\setul{red}{2pt}
\setstcolor{red}
\newcommand\semiSmall{\fontsize{23.8}{20.38}\selectfont}
\title{\semiSmall D2D-assisted Federated Learning: Hybrid Distributed Machine Learning in Two Timescales}

%  D2D-assisted hybrid federated learning with Aperiodic Consensus

% %\author{Frank Po-Chen Lin,~\IEEEmembership{Student Member,~IEEE}, Sheikh Shams Azam, Seyyedali~Hosseinalipour,~\IEEEmembership{Member,~IEEE},   Nicolo Michelusi,\\\IEEEmembership{Senior Member,~IEEE}, and Christopher G. Brinton,~\IEEEmembership{Senior~Member,~IEEE}
%\thanks{F. Lin, S. Azam, S. Hosseinalipour, N. Michelusi, and C. Brinton are with Purdue University, IN, USA e-mail: \{lin1183,azam1,hosseina,michelus,cgb\}@purdue.edu. 
%}} 
\maketitle
\begin{abstract} 
    Federated learning is promoted as one of the main platforms of distributed machine learning (ML) over wireless edge. It assumes a two-way communication paradigm consisting of device-to-server and server-to-device to train an ML model, neglecting the third way of communications in wireless edge that is device-to-device (D2D). In this paper, we propose a new model training architecture called \textit{two timescale hybrid federated learning} ({\tt TT-HF}). 
    In {\tt TT-HF} we consider a cooperative cluster-based model training methodology and introduce a hybrid learning paradigm, where cooperative D2D communications are used in conjunction with server-device communications to train an ML model. 
    In {\tt TT-HF} we rely on successive stochastic gradient descent (SGD) iterations to conduct local model training at the edge devices  and carry out the global model training in two timescales: (i) macro-scale, where global aggregations are  carried out \textit{aperiodically} via device-server interactions, and (ii) micro-scale, where the local aggregations are carried out  \textit{aperiodically} via consensus formation using D2D communications in different device clusters. 
    We carefully investigate the convergence characteristics of {\tt TT-HF} that leads us to a new upper bound of convergence for distributed model training. We obtain the condition under which the {\tt TT-HF} converges with the rate of $\mathcal{O}(1/t)$. We exploit our theoretical findings to develop an online algorithm that
    actively tunes (i) the number of successive local SGD iterations at the devices and the interval between two consecutive local aggregations, (ii) the step-size of SGD, (iii) the number of D2D communications rounds at the instance of each local aggregation, and (iv) the interval between two consecutive global aggregations.
      Through extensive simulations, we demonstrate that the proposed framework is robust against extreme heterogeneity of the users' datasets and can achieve a higher accuracy as compared to the current art methods while imposing the same energy consumption on the devices.
    % Two of the long lasting problems faced in implementing federated learning over wireless networks are conducting model training upon having edge devices with the following characteristics: (i) heterogeneous datasets, and (ii) resource limitations in terms of energy and the shared bandwidth. In this work, we introduce the novel \textit{two-timescale hybrid Federated learning} ({\tt TT-HF}) platform, in which the edge devices perform multiple local gradient descent updates (accounting for resource limitation) followed by periodic consensus realized through device to device (D2D) communications (accounting for heterogeneous datasets). We carefully investigate the convergence characteristics of the proposed framework that leads us to a new upper bound of convergence. We further obtain the number of required consensus rounds that needs to be performed in between the global aggregations to guarantee different convergence criteria.
\end{abstract}

\section{Introduction}
\noindent Machine learning (ML) techniques have exhibited a notable success to perform a variety of complicated tasks, such as image segmentation, object detection and tracking in video, and natural language processing~\cite{ghosh2019understanding,kang2016object,goldberg2017neural}. Classic supervised ML techniques conduct model training in a central location, where computation infrastructure and the dataset required to train the ML model coexist. However, in many applications, e.g., keyboard next word prediction in cellular phones~\cite{hard2018federated} and object detection for autonomous vehicles~\cite{wu2017squeezedet}, the data required to train the ML model is generated at the edge devices, e.g., cellular phones and smart cars, using the plethora of their mounted sensing devices and interactions with the users. In these applications, transferring of the raw data to a central location may not be feasible/practical due to the following reasons: (i) the size of the dataset can become prohibitively large upon data acquisition from a large number of users, which is cumbersome to store in a central location; (ii) transferring of the raw data from the end users to a central server can be extremely time and power consuming; (iii) the end devices may not be willing to share their gathered data due to privacy concerns. 

Federated learning has emerged as an elegant distributed ML technique to addressed these challenges~\cite{mcmahan2017communication,konevcny2016federated,yang2019federated}. A schematic of conventional federated learning architecture is depicted in Fig.~\ref{fig:simpleFL}. Federated learning conducts model training via exchanging the model parameters of the devices and eliminates the need to exchange the devices' raw data. In particular, 
it distributes the model training across the edge devices where each device trains a local model often using a gradient descent-based method. The models of the devices are then aggregated by a main server, which computes the weighted average of the devices' parameters and updates the devices with the new global model, based on which the devices initiate the next rounds of local training.  Although federated learning is predicted to be implemented over large-scale wireless networks~\cite{lim2020federated}, its conventional learning architecture does not fully match with the reality of wireless edge. In particular,  (i) sequential uplink transmissions are very energy consuming for resource constrained wireless devices (e.g., energy limited sensors, cellular phones, and drones); (ii) sequential transmission of model parameters from a large number of devices requires prohibitive bandwidth and may incur low quality-of-service (QoS) for the other users, (iii) the devices may posses extremely heterogeneous datasets (i.e., non-i.i.d datasets), which further hinders the convergence performance upon carrying out arbitrary local training at the devices.


One of the main techniques to implement federated learning in such environments is reducing the number of global aggregations during the period of learning via conducting multiple local gradient descent rounds at the edge devices between two consecutive global aggregations~\cite{wang2019adaptive,tu2020network}. However, upon having extreme non-i.i.d datasets, this method does not perform well since it results in model bias toward the local datasets, which is boosted by increasing the number of local descent updates that can severely affect the performance of the model training~\cite{wang2019adaptive}.


We recently introduced fog learning (FogL) to conduct model training over large-scale fog/edge networks~\cite{hosseinalipour2020federated}, which incorporates device-to-device (D2D) communications to the learning architecture. In this work, we exploit the paradigm of FogL to propose a novel learning platform, called \textit{two timescale hybrid federated learning} ({\tt TT-HF}), where the edge devices are partitioned into multiple clusters inside each of which cooperative D2D communications are enabled. In ({\tt TT-HF}), during the local training interval, the edge devices in each cluster perform multiple rounds of local stochastic gradient descent (SGD) updates in between which they aperiodically engage in D2D communications to form consensus on their model parameters to suppress local models biasedness. After the local training interval, the main server randomly selects one device from each cluster and requests uplink transmission of model parameters which is then used to carry out the global aggregation followed by broadcasting of the new global model that initiates the next round of local updates. {\tt TT-HF} migrates from the conventional learning architecture of federated learning depicted in~\ref{fig:simpleFL} and forms a complex learning architecture that is (i) \textit{two timescale}, i.e., heterogeneous number of D2D communications rounds are carried out aperiodically at different clusters in conjunction with aperiodic global aggregations, and (ii) \textit{hybrid}, i.e., the model training consists of both device-server (inter-cluster) and cooperative D2D (intra-cluster) communications. {\tt TT-HF} aims to address the aforementioned three challenges of federated learning over wireless edge by (i) reducing the number of uplink transmissions during the model training interval via conducting low-power D2D communications during the period of local updates, (ii) reducing the bandwidth consumption via engaging only one device from each cluster, i.e., cluster-heads, in each instant of global aggregation,\footnote{Note that D2D communications can be performed in out-band mode~\cite{kar2018overview}, which does not occupy the licensed spectrum.} (iii) taking into account for the extreme heterogeneity of data at the wireless edge via imposing a new general definition on the gradient diversity. 
\begin{figure}[t]
\includegraphics[width=.65\textwidth]{SimpFL_star.pdf}
\centering
\caption{Architecture of conventional federated learning. The learning network resembles a \textit{star} topology, where the communications only occur between the devices and the server. In each training round, edge devices perform local model updates based on their datasets followed by an aggregation at the main server to compute the global model, which is broadcast to the edge devices for the next round of local updates.}
\label{fig:simpleFL}
\vspace{-7mm}
\end{figure}


\subsection{Related Work}
 
 
 Most of the existing literature on federated learning study implementation of federated learning in its classic star topology structure depicted in Fig.~\ref{fig:simpleFL}. In this body of literature, studying the federated learning in terms of computation and communication constraints over wireless networks is an active area of research~\cite{8737464,chen2019joint,yang2019energy,yang2020federated}.
Also, federated learning has been studied in other contexts, such as multi-task learning and personalized model training~\cite{smith2017federated,corinzia2019variational,9006060,jiang2019improving,fallah2020personalized}, where individual models are tailored for different users. We refer the reader to~\cite{li2020federated} and references therein for other research directions and interesting problems investigated. Federated learning under
periodic averaging scheme in which the devices perform multiple rounds of local gradient descent updates between two consecutive global aggregations are rigorously studied  in~\cite{haddadpour2019convergence,wang2019adaptive}. These works have been widely extended to other settings, such as hierarchical system models~\cite{liu2020client}, where edge servers are utilized for partial global aggregations, and network aware data offloading schemes among the devices~\cite{tu2020network}. Conducting multiple local descents is mostly advocated in terms of energy efficiency since it can reduce the number of global aggregations in a certain period that are more energy consuming. However, these approaches are prone to the existence of nodes with non-i.i.d datasets, where multiple local descent updates may lead to the model biasedness toward the local datasets, which in turn could lead to severe deterioration of the convergence speed of the model training~\cite{wang2019adaptive}. To combat non-i.i.d datasets, some techniques such as offloading a portion of the users' datasets to the server, which is kept in a central node and used in parallel to train the model~\cite{9149323} and sharing a portion of pre-existing i.i.d data among the devices~\cite{zhao2018federated} are proposed, which face privacy concerns and data acquisition challenges.

As compared to the aforementioned literature, our proposed learning network architecture, and thus the problem investigated in this paper are substantially different. The most relevant work is our prior work on FogL~\cite{hosseinalipour2020multi}, where we considered another hybrid learning paradigm in which periodic global aggregations are conducted after one epoch of local gradient descent updates at the D2D-enabled edge devices. \chris{I would not be so direct that it is our own prior work.} As compared to~\cite{hosseinalipour2020multi},  in this work, we consider carrying out multiple local SGD iterations in between aperiodic global aggregations. Also, we consider a scenario where during the period of local learning, the devices aperiodically form consensus on their learning parameters. This is further complemented by a general assumption on gradient diversity to account for extreme data heterogeneity among the devices. 
These considerations make our learning architecture and analysis completely different and more complex as compared to~\cite{hosseinalipour2020multi}. Finally, it is worth mentioning that the consensus among the devices is achieved via utilizing \textit{distributed average consensus} technique~\cite{xiao2004fast,1333204}. There is a well-developed literature on consensus-based optimization~\cite{yuan2011distributed,nedic2009distributed,shi2014linear,4739339}, from which our system model and learning architecture substantially differ. Thus, although inspired, our work is complementary to that literature.

\subsection{Summary of Contributions}
Our contributions in this work can be summarized as follows:

\begin{itemize}[leftmargin=8mm]
\item We propose a novel distributed ML framework called \textit{two timescale hybrid federated learning} ({\tt TT-HF}). {\tt TT-HF} orchestrates the edge devices by partitioning them into multiple clusters in each of which cooperative D2D communications can be performed. Subsequently, we introduce a \textit{hybrid} learning paradigm where the model training consists of both device-server (inter-cluster) and cooperative D2D (intra-cluster) communications.  Also, we constitute a \textit{two timescale} learning paradigm that relies on successive SGD interactions complemented by aperiodic consensus formation among the devices via heterogeneous number of D2D communications  in conjunction with aperiodic global aggregations. 
\item We investigate the convergence behavior of {\tt TT-HF} and obtain its upper bound of convergence under a new general assumption on gradient diversity that accommodates for extreme heterogeneity among the local datasets. Our convergence bound relates topology structure inside different clusters, gradient diversity, number of D2D communications, and interval of consecutive  local and global aggregations to the convergence behavior of {\tt TT-HF}. We then obtain the condition under which the {\tt TT-HF} converges with the rate of $\mathcal{O}(1/t)$.
\item Through studying the convergence characteristic, we obtain a set of new general policies on the required number of D2D rounds at each instance of local aggregation, the choice of step-size, and the interval of local training to guarantee certain convergence behavior. We subsequently utilize our theoretical findings to develop an online control algorithm to tune these design parameters during the learning period in real-time.
\item Through numerical simulations, we reveal the fact that {\tt TT-HF} is robust against extreme heterogeneity in users' datasets. We also reveal the fact that under limited energy constraints, {\tt TT-HF} can outperform the current art method by a significant margin.
\end{itemize}



\section{System Model}
\noindent In this section, we first describe our system model for D2D-assisted federated learning, which introduces decentralized cooperation among the devices through D2D communications. Then we explain the ML task and introduce the two timescale hybrid federated learning ({\tt TT-HF}).  

\subsection{Edge Network Model of D2D-assisted Federated Learning}
We consider a network consisting of an edge server and $I$ edge devices gathered by the set $\mathcal{I}=\{1,\cdots,I\}$\nm{$\mathcal{I}$ appears to be unused..}. We assume that the model training is carried out through a sequence of global aggregations indexed by $k=1,2,\cdots$ (further explained in Sec.~\ref{subsec:syst3}). We propose a cluster-based representation of the edge and partition the edge devices into $N$ sets $\mathcal{S}^{(k)}_1,\cdots,\mathcal{S}^{(k)}_N$, where $\mathcal{S}^{(k)}_j \cap \mathcal{S}^{(k)}_{j'} =\emptyset,~\forall j\neq j'$.\footnote{Without causing ambiguity, $\mathcal{S}^{(k)}_i$ is used to refer to the cluster and also the set of nodes inside it.}
Cluster $\mathcal{S}^{(k)}_i$ contains $s^{(k)}_i\triangleq |\mathcal{S}^{(k)}_i|$ edge devices, $\sum_{i=1}^{N}s^{(k)}_i=I$, capable of performing cooperative D2D communications (see Fig.~\ref{fig2}). We assume that due to the mobility of the devices, the size of each cluster can be time varying,\nm{need to be precise on what you mean by time varying: when are clusters allowed to change?} although the number of clusters $N$ remains fixed.
The edge devices in a cluster participate in message exchange with their neighbors via cooperative D2D communications.
For edge device $i\in \mathcal S^{(k)}_j$ at global aggregation $k$, let $\mathcal 
{N}_i^{(k)}$ denote the set of its neighbors, determined based on the transmit power of the nodes, the channel conditions between them, and their physical distances. We assume that D2D communications are bidirectional, i.e., $n_i \in \mathcal{N}^{(k)}_{i'}$ if and only if ${i'}\in{\mathcal N}^{(k)}_i$, $\forall i,i'\in\mathcal{S}^{(k)}_j, \forall j$.
 As described later, our analysis can easily incorporate the case where D2D communications are not performed in some clusters. 
%  For tractability, it is assumed that the graph structure remains fixed during the interval of one global aggregation, while it can change from one global aggregation to another.
% We consider a dynamic network setting, where the set of neighbors of the nodes can evolve over time, e.g., due to device movements. However, for tractable analysis the set of nodes inside the cluster is assumed to be fixed. 
Finally, we associate a time varying network graph to each cluster $\mathcal{S}^{(k)}_j$ denoted by $ G^{(k)}_j(\mathcal{S}^{(k)}_j, \mathcal E^{(k)}_j)$
% \nm{I do not understand this notation.. is G a function with two arguments S and E? \ali{Yes!  it can be thought of as a graph uniquely defined by a set of edges and nodes.}}
, where $\mathcal{S}^{(k)}_j$ denotes the set of nodes and $\mathcal E^{(k)}_j$ denotes the set of edges, where $(i,{i'})\in \mathcal E^{(k)}_j$ if \add{and only if} $i,i'\in\mathcal{S}^{(k)}_j$ and $i \in \mathcal {N}^{(k)}_{i'}$.  The structure described above is \add{a distinguishing feature}  compared to the conventional federated learning star topology (see Fig.~\ref{fig:simpleFL}) used in the current literature, e.g.,~\cite{8737464,chen2019joint,yang2019energy,yang2020federated,smith2017federated,corinzia2019variational,9006060,jiang2019improving}, which does not consider the topology between the edge devices.
% \nm{need to be careful with "time-varying" graphs, this can rise several questions to the reviewer, and may add several challenges to the analysis. I would focus on the case where the network graph is fixed, or at least the graph does not change between two global aggregations}

% consists of a single edge server and multiple edge clusters indexed by $c = 1,2,...,N$. Each cluster $\mathcal G(\mathcal{S}_c^{(k)}, \mathcal E_c)$ contain multiple edge devices $s_c^{(k)} = 1,2,...,s_c^{(k)}$ and a set of edges $\mathcal E_c$, where $\{i,j\}\in \mathcal E_c$. The set of neighbors of device $i$ in cluster $c$ is denoted as $\mathcal N_i^{(c)}=\{j|\{i,j\}\in\mathcal E_c\}$. The edge devices collect data and perform local updates to optimize a loss function $F_i(\cdot)$ corresponding to a machine learning task (described next). To enhance the overall model learning speed, device-to-device (D2D) communications between the edge nodes are enabled to allow locally trained parameters $\mathbf w_i$ to be transferred to neighboring devices such that the devices inside each cluster are capable of computing the aggregation of their locally-trained parameters in a distributed manner, through message passing and consensus formation.  The edge server (the cloud) plays the role of an aggregator, collecting one of the locally trained parameters $\mathbf w$ from each cluster to perform a global update. Local updates are taken to be gradient descent steps on the local loss functions $F_i(\mathbf w)$ after consensus, while global updates refers to aggregation followed by synchronization. Aggregation denotes the computation of a global model obtained using the weighted average of local models, while synchronization represents the update of local models at the edge after aggregation \cite{Tu}.

\begin{figure}[t]
  \centering
  \vspace{-2em}
    \includegraphics[width=0.65\textwidth]{image/D2D_FL.png}
     \caption{Network architecture of D2D-assisted federated learning. This architecture incorporates the direct D2D communications into the learning paradigm and migrates from the conventional star topology of federated learning to more a distributed setting with less reliance on a main server.}
     \label{fig2}
     \vspace{-0.5em}
\end{figure}

\subsection{Machine Learning Task} \label{subsec:syst2}
% \subsubsection{Training dataset}
Each edge device $i$ owns a dataset $\mathcal{D}_i$ with $D_i=|\mathcal{D}_i|$ data points. Each data point $(\mathbf x,y)\in\mathcal D_i$ consists of an $m$-dimensional feature vector $\mathbf x\in\mathbb R^m$ and a label $y\in \mathbb{R}$.
% \subsubsection{Loss functions and ML objective}
We let $\hat{f}(\mathbf x,y;\mathbf w)$ denote the \textit{loss} associated with the 
data point $(\mathbf x,y)$ based on \textit{learning model parameter vector} $\mathbf w$, e.g., in linear regression $\hat{f}(\mathbf x,y;\mathbf w)\triangleq \frac{1}{2}(y-\mathbf w^T\mathbf x)^2$. The \textit{local loss function} at device $i$ is defined as:
\begin{align}\label{eq:1}
    F_i(\mathbf w)=\frac{1}{D_i}\sum\limits_{(\mathbf x,y)\in\mathcal D_i}
    \hat{f}(\mathbf x,y;\mathbf w).
\end{align}
\chris{Should point out here that we make no assumptions on $\mathcal{D}_i$ except that they are a subset of the global dataset.} We define the \textit{cluster loss function} for cluster $\mathcal{S}_c^{(k)}$ as:
\begin{align}\label{eq:c}
    \hat F_c(\mathbf w)=\sum\limits_{i\in\mathcal{S}_c^{(k)}} \rho_{i,c}^{(k)} F_i(\mathbf w),
\end{align}
\nm{there is a problem here.. since clusters are time varying, $\hat F_c(\mathbf w)$ is TV as well! Is our analysis able to handle this scenario?}
where $\rho_{i,c}^{(k)}$ is the weight associated with edge device $i\in \mathcal{S}_c^{(k)}$ relative to its respective cluster\nm{how are these weights determined?}. For simplicity of analysis\nm{a reviewer will ask what happens if that is not the case? Does the analysis break apart? I suggest to remove $\rho_{i,c}^{(k)}$ altogether and assume $\rho_{i,c}^{(k)}= 1/s_c^{(k)}$, unless you can provide a clear explanation of what happens in the more general case..}, we assume $\rho_{i,c}^{(k)}= 1/s_c^{(k)}$, $\forall i\in \mathcal{S}_c^{(k)}$. 
% \chris{This definition of $\rho$ is weighting each device the same. Can we generalize to the case where we weight by number of datapoints? It's more realistic.} 
\chris{I agree with Nicolo, per my previous comment. If we want to motivate with heterogeneity it would be better to handle the case where the number of datapoints are not the same at each device. I suspect that our analysis holds even for a general $\rho_{i,c}^{(k)}$.}
The \textit{global loss function} is given by:
\begin{align} \label{eq:2}
    F(\mathbf w)=\sum\limits_{c=1}^{N} \varrho_c^{(k)} \hat F_c(\mathbf w).
\end{align}
where $\varrho_c^{(k)}= s_c^{(k)} \left(\sum_{i=1}^N s_i^{(k)}\right)^{-1}$ is the weight associated with cluster $\mathcal{S}_c^{(k)}$ relative to the network. \chris{Following on the previous comment, I think we can write $\rho_{i,c}^{(k)} = D_i (\sum_{j \in \mathcal{S}_c^{(k)}} D_j)^{-1}$ and $\varrho_c^{(k)} = (\sum_{j \in \mathcal{S}_c^{(k)}} D_j) (\sum_{i \in \mathcal{I}} D_i)^{-1}$.}
We also define the weight of each edge node $i \in\mathcal{S}_c^{(k)}$ relative to the network as follows:
\begin{align}
    \rho_i=\varrho_c^{(k)}\cdot\rho_{i,c}^{(k)}= 1 / \sum_{i=1}^N s_i^{(k)}.
\end{align}
\nm{why is $\rho_i$ not a function of k?}
The goal of ML model training is to find the optimal model parameters $\mathbf w^*\in \mathbb{R}^M$, where $M$ denotes the dimension of the model\nm{you have already defined this as $m$ before!}, such that
\begin{align}
    \mathbf w^* = \mathop{\argmin_{\mathbf w \in \mathbb{R}^M} }F(\mathbf w).
\end{align}
% \nm{you also need to discuss the existence of $w^*$: its existence is implied by strong convexity of F.}

In the following, we make some standard assumptions~\cite{haddadpour2019convergence,wang2019adaptive,chen2019joint,friedlander2012hybrid} on the ML loss function that also imply the existence and uniqueness of $\mathbf w^*$, and then define a new generic metric to measure the degree of heterogeneity/non-i.i.d. \add{structure of the data} across the local datasets
\begin{assumption}
\label{beta}
The following assumptions are made throughout the paper:
\begin{itemize}[leftmargin=5mm]
\item  \textbf{Strong convexity}:
 $F$ is $\mu$-strongly convex, i.e.,\footnote{Note that convex ML loss functions, e.g., squared SVM and linear regression, are implemented with a regularization term in practice to improve the convergence and avoid model overfitting, which makes them strongly convex.}
\begin{align} 
    F(\mathbf w_1) \geq  F(\mathbf w_2)&+\nabla F(\mathbf w_2)^T(\mathbf w_1-\mathbf w_2)+\frac{\mu}{2}\Big\Vert\mathbf w_1-\mathbf w_2\Big\Vert^2,~\forall { \mathbf w_1,\mathbf w_2}.\label{eq:11}
\end{align}
    \item  \textbf{Smoothness:} $F_i$ is $\beta$-smooth, $\forall i$, i.e., 
    \begin{align}
\Big\Vert \nabla F_i(\mathbf w_1)-\nabla F_i(\mathbf w_2)\Big\Vert \leq & \beta\Big\Vert \mathbf w_1-\mathbf w_2 \Big\Vert,~\forall i, \mathbf w_1, \mathbf w_2,
 \end{align}
implying $\beta$-smoothness of the global function as well.
%  , i.e., $\Vert\nabla F(\mathbf w)\Vert^2 \leq 2\beta[F(\mathbf w)-F(\mathbf w^*)]$, $\forall \mathbf w$.\nm{why do you call this inequality "$\beta$-smoothness of the global function,"?}
 
\end{itemize}
% \begin{description}
% \item[$\beta$-smoothness:] 

% \nm{are you assuming that $F_i$ are convex? You need to state that..}
% \frank{I am not assuming $F_i$ to be convex}
% \end{description}
\end{assumption}
\chris{While we leverage these assumptions in our theoretical development, our experiments in Section XX will show that our resulting algorithms still obtain substantial performance improvements in the case of non-convex loss functions, e.g., for neural networks.}

% \begin{assumption} \label{PL}
% $F(\cdot)$ satisfies the Polyak-≈Åojasiewicz (PL) condition with constant $\mu$.
% \begin{align} \label{eq:11}
%     \frac{1}{2}\Big\Vert\nabla F(\mathbf w)\Big\Vert^2 \geq \mu(F(\mathbf w)-F(\mathbf w^*)),\ \forall \mathbf w,
% \end{align}
% \end{assumption} 

% \begin{assumption} \label{gradVar}
%     $$\mathbb E[\Vert(\widehat{\mathbf g}_j(\mathbf w_j(t))-\nabla F_j(\mathbf w_j(t))\Vert_2^2]\leq\sigma^2$$
% \end{assumption}

\begin{definition}[Gradient Diversity]\label{gradDiv}
The gradient diversity across the device clusters $c$ is measured via two non-negative constants $\delta,\zeta$ that satisfy 
\begin{align} \label{eq:11}
    \Vert\nabla\hat F_c(\mathbf w)-\nabla F(\mathbf w)\Vert
    \leq \delta+ \zeta \Vert\mathbf w-\mathbf w^*\Vert,~\forall c, \mathbf w.
\end{align}
\end{definition} 
The conventional definition of gradient diversity used in literature, e.g., \cite{wang2019adaptive}, is \add{a special case of}~\eqref{eq:11} with $\zeta=0$. However, solely using $\delta$ on the right hand side of~\eqref{eq:11} can be troublesome since in that case $\delta$ can become prohibitively large,\footnote{This is specially the case in the initial iterations of the model training, where $\Vert\mathbf w-\mathbf w^*\Vert$ can be arbitrary large.} which reduces the usefulness of the derived convergence bounds based on $\delta$ by making them too loose. The cornerstone of the above definition is in the definition of gradient diversity, which can be upperbounded as follows:
\begin{align}\label{eq:motivNewDiv}
    &\Vert\nabla\hat F_c(\mathbf w)-\nabla F(\mathbf w)\Vert
    = \Vert\nabla\hat F_c(\mathbf w)-\nabla\hat F_c(\mathbf w^*)+\nabla\hat F_c(\mathbf w^*)-\underbrace{\nabla F(\mathbf w^*)}_{=0}-\nabla F(\mathbf w)\Vert
    \\ \nonumber &
    \leq \Vert\nabla\hat F_c(\mathbf w)-\nabla\hat F_c(\mathbf w^*)\Vert+\Vert\nabla\hat F_c(\mathbf w^*)\Vert + \Vert\nabla F(\mathbf w)-\nabla F(\mathbf w^*)\Vert
    \leq \delta+2\beta\Vert\mathbf w-\mathbf w^*\Vert,
\end{align}
\add{where in the last step above we used the smoothness condition and we upper bounded the cluster gradients} at the optimal model  as $\Vert\nabla\hat F_c(\mathbf w^*)\Vert \leq \delta$, which
\add{is a more practical assumption than $\Vert\nabla\hat F_c(\mathbf w)-\nabla F(\mathbf w)\Vert
    \leq \delta$, which must hold $\forall\mathbf w$. From \eqref{eq:motivNewDiv}, it also follows that $\zeta\leq 2\beta$, hence}
\sst{should trivially hold since otherwise the optimal model will not work properly.} We introduce a ratio  $\omega=\frac{\zeta}{2\beta}$, where $\omega\leq 1$ according to~\eqref{eq:motivNewDiv}. Considering $\zeta$ in~\eqref{eq:11} changes the dynamics of the convergence analysis and requires new techniques to obtain the convergence bounds, which are part of the contributions of this paper. 

% \begin{assumption} \label{eDiv}
% The weighted gradient diversity across the clusters is bounded as 
% \begin{align} \label{eq:11}
%     \frac{\sum\limits_{c=1}^N\varrho_c^{(k)}\left\Big\Vert e_{i_c}^{(\Gamma_c(t+1))}(t+1)\right\Big\Vert^2}{\left\Big\Vert e_{i_c}^{(\Gamma_c(t+1))}(t+1)\right\Big\Vert^2}\leq \delta,\ \forall \mathbf w,
% \end{align}
% where $\delta\geq 0$ is the dissimilarity parameter for all devices.
% \end{assumption} 

% Note that Assumption \ref{assum1} is satisfied in many popular machine learning models such as linear regression, logistic regression, squared-SVM, etc. 

% \begin{assumption} \label{def:2}
% $\nabla F(\mathbf w)$ is bounded on a closed and convex set $\mathcal{H}$ such that
% \begin{align} \label{eq:11}
%      &\Big\Vert \nabla F(\mathbf w)\Big\Vert \leq L,
% \end{align}
% where $L>0$ is a finite scalar.
% \end{assumption} 

% \subsubsection{Centralized gradient descent}
% Loss functions are typically minimized by gradient descent (GD) iterations. In a centralized case, where the global loss $F$ can be optimized directly, this is defined as
% \begin{align} \label{eq:3}
%     \mathbf w_{GD}(t)=\mathbf w_{GD}(t-1)-\eta_k\nabla F(\mathbf w_{GD}(t-1)),\
% \end{align}
% where $\mathbf w_{GD}(0)$ is an initialization,
%  $t\geq 1$ is the iteration index, and $\eta_k>0$ is the learning rate. If $F$ is convex and $\eta_k \leq \frac{1}{\beta}$, then gradient descent converges to the globally optimal solution $\mathbf w^*$ with rate $\mathcal O(1/T)$, where $T$ is the number of iterations \cite{Bubeck}.

% However, centralized gradient descent cannot be directly applied to the FL framework in Fig.\ref{fig2} since no device has direct access to all the data. 
% In addition, communication to the cloud is costly in terms of network resources, so the aggregation and synchronization processes are done only periodically.
% Finally, communication delay between edge and cloud is usually non-negligible, which we address next in developing the {\tt FedDelAvg} algorithm. 

\subsection{Two Timescale hybrid federated learning {\tt TT-HF}} \label{subsec:syst3}
\subsubsection{{\tt TT-HF} in a Nutshell} {\tt TT-HF} consists of model training through a sequence of aperiodic global aggregations, during which the devices perform a sequence of local SGD iterations and aperiodically synchronize their parameters by forming consensus through cooperative D2D communications. The consensus formation among the devices can occur multiple times during one interval of global aggregations. We incorporate forming consensus through D2D communications, which can be considered as \textit{imperfect local aggregations}\footnote{This imperfections is due to the practical consideration of finite rounds of D2D communications.},\nm{I dont understand this} to the learning paradigm due to the following reasons. (i) It suppresses the bias of the local models to the local datasets, which is one of the main challenges faced in federated learning in environments with non-i.i.d data across the devices. This enables the devices to remain in the local training mode for longer period of time without performing resource demanding communications to the cellular BS. (ii) It incurs much lower power consumption on the devices as compared to uplink transmission to the cellular BS since D2D communications are mostly performed in short ranges~\cite{hmila2019energy,dominic2020joint}. (iii) It is highly advocated in 5G-and-beyond wireless systems~\cite{7994918,9048621}. 
\nm{I got lost with the explanations in this paragraph}

\subsubsection{{\tt TT-HF} Procedure}
\nm{it is essential that this is explained clearly. I think it needs to be improved. The consensus is explained too late in the paragraph. I think you should follow a more logical order: SGD --> consensus --> synchronization --> repeat}
Let $\mathcal{T}_{k}$ denote the time span between two consecutive global aggregations $k-1$ and $k$, which is referred to as $k$-th \textit{local model training interval}, and $\tau_{k}=|\mathcal{T}_{k}|$ denote the respective interval length, where $\mathcal{T}_k\in \{t_{k-1}+1,...,t_k\}$ with $t_k\triangleq \sum_{\ell=1}^{k}\tau_\ell$, $\forall k$. Since global aggregations are aperiodic, in general $\tau_{k}$ can be different from $\tau_{k'}$, $k\neq k'$.  The model training starts with an initial broadcasting of global model $\hat {\mathbf w}(0)$. At any time instance $t\add{\in\mathcal T_k}$, each edge device $i$ owns a local model denoted by $\mathbf w_i^{(t)}$. The global model is updated based on the devices' local models at the end of each local model training interval, where the main server samples one device from each cluster uniformly at random and requests parameter transmission from it to conduct global aggregation and obtain the global model $\hat{\mathbf w}$ as follows:
\begin{align} \label{15}
    \hat{\mathbf w}^{(t_k)} &= 
          \sum\limits_{c=1}^N \varrho_c^{(k)} \mathbf w_{s_c}^{(t_k)}, ~~k=1,2,\cdots.
\end{align}
In~\eqref{15}, $\mathbf w_{s_c}$\nm{just call this $\hat{\mathbf w}_{c}^{(k)}$?}
\nm{$s_c$ already used for smt else... notation is unnecessarily complicated} refers to the model parameter of the sampled device from cluster $c$. The global model is then broadcast by the main server,
 using which the devices override their current local models: $\mathbf w_i^{(t_{k})}=\mathbf{\hat{w}}^{(t_{k})}$, $\forall i,k$. During the local model training interval, the devices perform successive local SGD iterations on their local loss functions, where at $t\in\mathcal T_k$, device $i$ samples a mini-batch $\xi_i^{(t)}$ of fixed size from its own local dataset $\mathcal D_i$ using \textit{simple random sampling without replacement} technique and uses it to calculate the local gradient estimate
\begin{align}\label{eq:SGD}
    \widehat{\mathbf g}_{i}^{(t)}=\frac{1}{\vert\xi_i^{(t)}\vert}\sum\limits_{(\mathbf x,y)\in\xi_i^{(t)}}
    \hat{f}(\mathbf x,y;\mathbf w_i^{(t)}),
\end{align}
which is an unbiased estimate of the local gradient, i.e., $\mathbb E_{t}[\widehat{\mathbf g}_{i}^{(t)}]=\nabla F_i(\mathbf w_i^{(t)})$\nm{what about its variance?}. Using the gradient estimate each device then computes the \textit{intermediate updated local model} as follows:
\begin{align} \label{8}
    {\tilde{\mathbf{w}}}_i^{(t)} = 
           \mathbf w_i^{(t-1)}-\eta^{(k)} \widehat{\mathbf g}_{i}^{(t-1)},~t\in\mathcal T_k,
\end{align}
where $\eta^{(k)}$ denotes the step size during the respective interval.

The devices aperiodically form consensus through engaging in cooperative D2D communications during each local model training interval, determining the time of occurrence of which is a part of design elaborated in Sec.~\ref{Sec:controlAlg}. Upon forming consensus at time $t$, the devices perform multiple \textit{rounds} of D2D,\nm{does this happen within t? Why are the consensus updates not counted at the same timescale as local SGD? How do you make sure that the clusters are synced?} each of which consists of parameter transfers between neighboring devices. 
To account for this, we express the updated local parameter of device $i\in\mathcal{S}_c$ using a general form. Letting $\widetilde{\mathbf{W}}^{(t)}_{{C}} \in \mathbb{R}^{|\mathcal{S}_c^{(k)}| \times M}$\nm{why C and not c?} denote the matrix of intermediate updated local model of the nodes in cluster $\mathcal{S}_c^{(k)}$ prior to consensus at time $t$, the evolution of the devices' parameters is given by:
  \begin{equation}\label{eq:consensus}
      \mathbf{W}^{(t)}_{{C}}= \left(\mathbf{V}^{(k)}_{{C}}\right)^{\Gamma^{(t)}_{{C}}} \widetilde{\mathbf{W}}^{(t)}_{{C}}, ~t\in\mathcal T_k,\nm{notation!}
  \end{equation}
where  ${\mathbf{W}}^{(t)}_{{C}}$ denotes the matrix of updated devices' parameters after the consensus, $\Gamma^{(t)}_{{C}}$ denotes the rounds of D2D in the respective cluster,  and $\mathbf{V}^{(k)}_{{C}}=[v^{(k)}_{i,j}]_{1\leq i,j\leq \mathcal{S}_c^{(k)}}$ denotes the consensus matrix with non-negative real elements. In particular, the $i$-th row of ${\mathbf{W}}^{(t)}_{{C}}$ correspondents to ${{\mathbf{w}}}_i^{(t)}$, which is used in~\eqref{eq:SGD} to calculate the next gradient estimate for the next local update. The consensus matrix is assumed to be fixed during the local model training period and is built based on the cluster topology $G^{(k)}_j$. It also only allows message exchange among the neighboring nodes. In particular, each node $i\in\mathcal{S}_c$ conducts the following iteration for $t'=0,\cdots,\Gamma^{(t)}_{{C}}-1$:
\vspace{-1mm}
    \begin{equation}\label{eq:ConsCenter}
       \textbf{z}_{i}^{(t'+1)}= v^{(k)}_{i,i} \textbf{z}_{i}^{(t')}+\hspace{-2mm} \sum_{j\in \mathcal{N}^{(k)}_i} v^{(k)}_{i,j}\textbf{z}_{j}^{(t')},
  \end{equation}
where $\textbf{z}_{i}^{(0)}=\tilde{\mathbf{w}}_i^{(t)}$ is the node intermediate updated local model and $\textbf{z}_{i}^{\Gamma^{(t)}_{{C}}}=\mathbf w_i^{(t)}$ is the node updated model. Here, $t'$ corresponds to the second timescale used in the method that refers to the time spent during the consensus in addition to $t$ capturing the time elapse of the local iterations. 

Finally, if at time $t\in\mathcal T_k$ the devices do not form consensus, we have $\Gamma^{(t)}_{{C}}=0$, implying $\mathbf{W}^{(t)}_{{C}}=  \mathbf I \times \widetilde{\mathbf{W}}^{(t)}_{{C}}$, which results in  ${{\mathbf{w}}}_i^{(t)} = 
           \mathbf w_i^{(t-1)}-\eta^{(k)} \widehat{\mathbf g}_{i}^{(t-1)}$. A schematic of the procedure of {\tt TT-HF} is depicted in  Fig.~\ref{fig:twoTimeScale}.

\begin{assumption}\label{assump:cons}
The consensus matrix $\mathbf{V}^{(k)}_{{C}}$ satisfies following properties~\cite{xiao2004fast}: (i) $\left(\mathbf{V}^{(k)}_{{C}}\right)_{m,n}=0~~\textrm{if}~~ \left({m},{n}\right)\notin \mathcal{E}^{(k)}_{{C}}$, (ii) $\mathbf{V}^{(k)}_{{C}}\textbf{1} = \textbf{1}$, (iii) $\mathbf{V}^{(k)}_{{C}} = {\mathbf{V}^{(k)}_{{C}}}^\top$, and (iv)~$ \rho \left(\mathbf{V}^{(k)}_{{C}}-\frac{\textbf{1} \textbf{1}^\top}{|\mathcal{S}_c^{(k)}|}\right) \leq \lambda^{(k)}_{{C}} < 1$, where $\textbf{1}$ is the vector of 1s and \underline{$\rho(\mathbf{A})$ is the spectral radius of matrix $\mathbf{A}$}.
\end{assumption}


% are the \textit{consensus weights} associated with node $n$  during $k$. We assume that each node has knowledge of these weights at each global iteration. For instance, one common choice for these weights~\cite{xiao2004fast} gives $\textbf{z}_{n}^{(t+1)} = \textbf{z}_{n}^{(t)}+d^{(k)}_{{C}}\sum_{m\in \mathcal{\zeta}^{(k)}(n)} (\textbf{z}_{m}^{(t)}-\textbf{z}_{n}^{(t)})$, $0 < d^{(k)}_{{C}} < 1 / D^{(k)}_{{C}}$, where $D^{(k)}_{{C}}$ is the maximum degree of the nodes in $G^{(k)}_{{C}}$.


% where $\eta^{(k)}$ denotes the step size during the respective interval and $\psi^{(k)}_{ij}\in[0,\infty)$, $\forall i,j$, denote the averaging weights. In~\eqref{8}, the updated parameter of each node $i$ is represented as an average between its own and its neighbors $\mathcal N_i^{(t)}$ parameters. If the consensus does not occur at $t$, we have $\psi^{(k)}_{ii}=1$ and $\psi^{(k)}_{ij}=0$, $i\neq j$, which implies ${\mathbf w}_i^{(t)} =  
%           \mathbf w_i^{(t-1)}-\eta^{(k)} \widehat{\mathbf g}_{i}^{(t-1)}$. However, if the consensus occurs, devices inside each cluster exchange and update their locals parameters through a sequence of D2D communications. We consider the general family of linear distributed consensus algorithms where device $\Psi^{(k)}=[\mathbf \psi_{ij}^{(k}]_{i,j\in\mathcal{S}_c^{(k)}}$, 

% During consensus, devices inside each cluster exchange and update their locals parameters. We consider the general family of linear distributed consensus algorithms where device




% Formally, during global iteration $k$, each node $n \in \mathcal{C}^{(k)}$  engages in the following rounds of iterative updates for $t=0,...,\theta^{(k)}_{{C}}-1$: 
% \vspace{-1mm}
%     \begin{equation}\label{eq:ConsCenter}
%       \textbf{z}_{n}^{(t+1)}= v^{(k)}_{n,n} \textbf{z}_{n}^{(t)}+\hspace{-2mm} \sum_{m\in \mathcal{\zeta}^{(k)}(n)} v^{(k)}_{n,m}\textbf{z}_{m}^{(t)},
%   \end{equation}
  
%   \vspace{-3mm}
%   \begin{equation}\label{eq:ConsCenter}
%       \textbf{z}_{n}^{(t+1)}= \textbf{z}_{n}^{(t)}+d^{(k)}_{\mathcal{C}}\hspace{-2mm} \sum_{m\in \mathcal{N}_{(k)}(n)} (\textbf{z}_{m}^{(t)}-\textbf{z}_{n}^{(t)}),
%   \end{equation} 
%   \noindent where $\textbf{z}_{n}^{(0)} = \mathbf{w}_{n}^{(k)}$ corresponds to node $n$'s initial parameter vector, and $\textbf{z}_{n}^{\big(\theta^{(k)}_{{C}}\big)}$ denotes the parameter after the D2D consensus process. $\mathcal{\zeta}^{(k)}(n)$ denotes the set of neighbors of node $n$ during global iteration $k$, and $v^{(k)}_{n,p}$, $p\in \{n\} \cup \mathcal{\zeta}^{(k)}(n)$
%   \nm{for convenience you can simply assume that $\mathcal{\zeta}^{(k)}(n)$ contains n as well} \ali{Yes. But writing as 7  may get a better perspective to the reader. Finally, we will work with the matrix form that encapsulates the neighbors in its elements.}
% are the \textit{consensus weights} associated with node $n$  during $k$. We assume that each node has knowledge of these weights at each global iteration. For instance, one common choice for these weights~\cite{xiao2004fast} gives $\textbf{z}_{n}^{(t+1)} = \textbf{z}_{n}^{(t)}+d^{(k)}_{{C}}\sum_{m\in \mathcal{\zeta}^{(k)}(n)} (\textbf{z}_{m}^{(t)}-\textbf{z}_{n}^{(t)})$, $0 < d^{(k)}_{{C}} < 1 / D^{(k)}_{{C}}$, where $D^{(k)}_{{C}}$ is the maximum degree of the nodes in $G^{(k)}_{{C}}$. {\tt TT-HF} thus observes the time from two different scales: (i) \textit{micro scale}, that concerns with devices' parameters evolution during the local updates; (ii) \textit{macro scale} that concerns with server' parameter evolution at the instances  of global aggregations. We let $\mathbf w_i(t)$ be the local model parameter vector of edge device $i$ at time $t$, initialized as $\mathbf w_i(0)=\mathbf w(0)$ at time $0$.

Subsequently, we model the local parameter at device $i\in \mathcal{S}_c^{(k)}$ by
\begin{align} \label{eq14}
    \mathbf w_i^{(t)} = \bar{\mathbf w}_c^{(t)} + e_i^{(t)},
\end{align}
where $\bar{\mathbf w}_c^{(t)}\triangleq \sum\limits_{i\in\mathcal{S}_c^{(k)}} \rho_{i,c}^{(k)} \tilde{\mathbf w}_i^{(t)}$ is the perfect average\nm{what do you mean by the average to be perfect? ITs just the average} of across all the devices' parameters in $\mathcal{S}_c^{(k)}$ and $e_i^{(t)}\in\mathbb{R}$ denotes the error caused by limited D2D rounds among the devices. In particular, $e_i^{(t)}=0$ if all the devices inside the cluster have the same model parameter or when infinite rounds of D2D are performed, i.e., $\Gamma_c(t)\rightarrow \infty$, which is impractical due to time constraints.
\nm{since you are "hiding" the consensus steps within the time index $t$, one may argue that you are not really capturing these time constraints..}

\nm{this is where you should introduce a lemma that states the properties of the consensus error. I.e. how does it decay with the number of consensus steps?}


% that $[\mathbf \psi_{ij,k}^{(c)}]_{i,j\in\mathcal{S}_c^{(k)}}$ are the symmetric and doubly stochastic weights imposed by the distributed average consensus method used at the $k$th global aggregation.

% We consider a scenario in which the edge devices perform multiple rounds of local gradient descents in between a sequence of global aggregations. At each global aggregation,  the server acquires the parameters of the devices and compute the global parameter and broadcasts it among the devices. We consider a general case where \textit{aperiodic} global aggregations are conducted, i.e., the length of the time span in between two consecutive global aggregations, and thus the number of local descents, varies between different consecutive global aggregations. \chris{One of the main contributions of this paper will be studying the optimization of these periods.}
% \nm{who dictates when the aggregations are done? Is that random? Is it based on certain criteria? Please clarify} \ali{this will be investigated using the bounds. We would say to get a certain convergence rate, this many times of consensus inside a period of length ... are required tht each performs this many rounds of consensus.}\nm{ok but if you dont clearly state that now you will leave the reviewer wondering...}

% Let the $t=0,1,\cdots,T$ denote the evolution of time during the training phase. To capture the aperiodic global aggregations, we assume that the number of local updates carried out between the $k$th global aggregation, i.e., the number of local updates between global aggregation $k-1$ and $k$, is $\tau_k$, $k\in\mathbb{N}$.
%  \nm{the definition of $\tau$ is unclear and seems contradictory... here it is defined as the time between two global aggrs.;
%  later (highlighted) it is defined as the number of local gradient updates between two globals..
% why so?
%  } \ali{I think these two are equivalent. Like if one does one global aggregation every time, he would do one local + one global. If he does one global aggregation every two time instance, he would get local+local+global. So, we need to emphasize that the notion of "time" is defined based on local descents.} \chris{I think the question here is about clarifying that $\tau$ is an integer? So that ``time between'' is a count?}




% parameter sharing via D2D communication between edge devices on learning performance is incorporated into the design of the FL system. We divide the learning process into discrete time intervals $t\in\{1,2,...,T\}$, where the duration between two consecutive aggregations is denoted as $\tau_k$. We let $\mathbf w_i(t)$ be the local model parameter vector of edge device $i$ at time $t$, initialized as $\mathbf w_i(0)=\mathbf w(0)$ at time $0$ across all devices $i\in\mathcal S$.

% The \textit{hybrid} platform is achieved via performing both D2D and device-to-server communications, where in D2D mode, the devices perform periodic \textit{distributed average consensus}, which we refer to as consensus for brevity, during the time span between two global aggregations. Also, in device-to-server communications, the server \textit{aperiodically} gathers the parameters of the devices to update the global parameter which is then broadcasted among the devices. In the following, we describe the procedure the devices perform in detail.


% \begin{assumption} \label{SGD}
% We make the following assumptions on $\widehat{\mathbf g}_{i,t}$:
%     \begin{description}
%         \item[Unbiased local estimation:] $\widehat{\mathbf g}_{i,t}$ is an unbiased estimate of local gradient $\nabla F_j(\mathbf w_j(t))$ obtained from the uniformly sampled mini-batch over each device’s local data:
%          \begin{align}
%             \mathbb E_{t}[\widehat{\mathbf g}_{j,t}]=\nabla F_j(\mathbf w_j(t))
%          \end{align}
%          \item[Bounded local variance:] The variance of $\widehat{\mathbf g}_{i,t}$ is bounded by $\sigma^2$.
%         \begin{align} 
%             \mathbb E_{t}[\Vert n_i(t)\Vert_2^2]\leq 
%             \sigma^2,
%         \end{align}
%         where $n_i(t)=\widehat{\mathbf g}_{i,t}-\nabla F_i(\mathbf w_i(t))$ and the expectation is $\mathbb E_t(\cdot)$ taken with respect to the random selection of the mini batch.
%     \end{description}
% \end{assumption}

% \add{i.e., $\hat g$ is an unbiased estimate of the true local gradient $\nabla F$, with variance bounded by $\sigma^2$.}
% \nm{you need to specify that the expectation is with respect to the random selection of the mini batch}

% \subsubsection{Distributed consensus}
% \nm{I do not understand the model here... are you assuming you do several local and one consensus step? But later one you say that each consensus step includes $\gamma_k$ consensus steps...be more precise!
% Also, why are you doing several local steps followed by consensus, and not 1 local followed by consensus? Does it help in some way?}

% Consider the time span between the global aggregations $k$ and $k+1$, $k\in\{0,\dots, K-1\}$, gathered by the set $\mathcal T_k = \{k\tau_k+1,...,(k+1)\tau_k\}$.
% At each time $t \in \mathcal T_k \setminus\{k\tau_k\}$ \ali{This is not in the set? am I missing sth?}, each edge device performs $u$ steps of local gradient update along with multiple rounds of consensus update (we assume that $\tau_k=h_k\mu_k$ such that $h_k$ times of consensus update is carried out at $\mathcal H_k\in\{k\tau_k+\mu, k\tau_k+2\mu,\dots,(k+1)\tau_k\}$.
% We consider a case in which in the time span between global aggregations $k-1$ and $k$, where total of $\tau_k$ local gradient descent is performed at each device, the devices enagage in consensus periodically after every $\mu_k$ time instances. We assume that $\tau_k=\mu_k h_k$, where $h_k$ denotes the number of times the devices engage in consensus in the respective time span.

% We consider a case in which in the time span between global aggregations $k-1$ and $k$, where a total of $\tau_k$ local gradient descent is performed at each device, the devices engage in consensus periodically after every $\mu_k$ time instances. We assume that $\tau_k=\mu_k(h_k+1)$,\nm{something is unclear: one time slot includes 1 local and $\mu_k-1$ consensus updates?} \frank{It is $\mu_k-1$ local + $\Gamma_c(\mu_k)$ rounds of consensus}
%  where $h_k$ denotes the number of times the devices engage in consensus in the respective time span carried out at $\mathcal H_k = \{t_{k-1}+\mu_k, t_{k-1}+2\mu_k,\dots,t_{k-1}+h_k\mu_k\}$, where $k=1,2,\cdots,K$ and $t_0=0$. During consensus, devices inside each cluster exchange and update their locals parameters. We consider the general family of linear distributed consensus algorithms where device $i\in\mathcal{S}_c^{(k)}$ is sequentially updated via receiving the neighboring devices parameters as
% \begin{align} \label{9}
%     \mathbf w_i(t) = \sum\limits_{j\in\mathcal N_i^{(t)}} \psi_{ij,k}^{(c)}\tilde{\mathbf w}_j(t),
% \end{align}
% \nm{Here we go back to the problem I talked about with time varying graphs: if Nit is timee varying, then $\psi_{ij}^{(c)}$ needs to be time varying otherwise you cannot guaranteee the synnetric doubly stoch property that you need in the consensus}
% \nm{the use of t' and t is quite confusing, since they follow different timescales:
% larger timescale: k, faster timescale: t; fastest timescale, t'. Please define a clear and consistent notation} \chris{I agree it might be cleanest to define everything in terms of $t$, and just have different operations happening at different intervals of $t$. But the advantage of having one timescale for local updates and another for consensus rounds is that we can point out the difference in resource utilization each operation may cause.}
% for $t'=0,\cdots,\Gamma_c(t)-1$, where $\Gamma_c(t)$ denotes the \textit{rounds of consensus} performed at time\nm{so one time includes multiple consensus steps? Why is it dependent on t and c? Does it mean that some clusters are faster than other?} $t$\nm{in cluster $c$?}, $\mathbf x_i(0)=\mathbf w_i(t)$ and overwrite $\mathbf w_i(t)\leftarrow\mathbf x_i(\Gamma_c(t))$, where $s_c^{(k)}=|\mathcal{S}_c^{(k)}|$.
% \nm{does not make sense, you ahve already defined
% $\mathbf w_i(t) =  
%           \mathbf w_i(t-1)-\eta_k \widehat{\mathbf g}_{i,t-1}$ in (12), Please use different notation, i.e., w before and after consensus}
% such that $[\mathbf \psi_{ij,k}^{(c)}]_{i,j\in\mathcal{S}_c^{(k)}}$ are the symmetric and doubly stochastic weights imposed by the distributed average consensus method used at the $k$th global aggregation.
% \chris{Regarding the dependency of $\Gamma_c(t)$ on cluster $c$, I think this would be important if the clusters have different properties, e.g., different data distributions, computation capabilities, or local topologies.}



% \chris{I think the case where $\mu_k = 1$ and $\Gamma_c(\mu_k) = 1$ will be particularly interesting to consider. One gradient iteration followed by one consensus round.}


 
 \begin{figure}[t]
\includegraphics[width=.9\textwidth]{Time_Elapse2.pdf}
\centering
\caption{Time evolution in {\tt TT-HF}. The time index $t$ captures the local descent iterations and subsequent global aggregation. In each local training interval, the nodes engage in aperiodic consensus formation through cooperative D2D communications. Time index $t'$ captures the time elapse through the D2D communications.}
\label{fig:twoTimeScale}
\vspace{-5mm}
\end{figure}
% \subsubsection{Aperiodic global aggregations} At time $t_k$, $k=1,2,\cdots,K$, the server randomly samples one local parameters $\mathbf w_{s_c}(t_k)$ from node $s_c$ in each cluster $c$ and computes the new global parameter $\hat{\mathbf w}(t_k)$ as

% Sample one node from each cluster:
% \begin{align} \label{eq15}
%     \mathbf w(t) &= 
%           \sum\limits_{c=1}^N \varrho_c^{(k)} \mathbf w_{s_c^{(k)}}(t)
%           \nonumber \\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \big(\bar{\mathbf w}_c(t)+s_c^{(k)}\eta_k e_{s_c^{(k)}}^{(\Gamma_c(t))}\big)
%           \nonumber \\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \sum\limits_{i\in\mathcal{S}_c^{(k)}} \frac{1}{s_c^{(k)}} \underbrace{\left[\mathbf w_i(t-1)-\eta_k\nabla F_i(\mathbf w_i(t-1))\right]}_{\tilde{w}_{i}(t)}
%           \nonumber \\&
%           + \eta_k\sum\limits_{c=1}^N \varrho_c^{(k)} s_c^{(k)} e_{s_c^{(k)}}^{(\Gamma_c(t))}
%           \nonumber \\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}} \sum\limits_{i\in\mathcal{S}_c^{(k)}}
%           \big(\bar{\mathbf w}_c(t-1)+s_c^{(k)}\eta_k e_{i}^{(\Gamma_c(t-1))}\big)
%           \nonumber \\&
%           -\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}} \sum\limits_{i\in\mathcal{S}_c^{(k)}} 
%           \eta_k\nabla F_i(\mathbf w_i(t-1))
%           + \eta_k\sum\limits_{c=1}^N \varrho_c^{(k)} s_c^{(k)} e_{s_c^{(k)}}^{(\Gamma_c(t))}
%           \nonumber \\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}} \sum\limits_{i\in\mathcal{S}_c^{(k)}}
%           \bar{\mathbf w}_c(t-1)
%           -\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}} \sum\limits_{i\in\mathcal{S}_c^{(k)}} 
%           \eta_k\nabla F_i(\mathbf w_i(t-1))
%           \nonumber \\&
%           + \eta_k\sum\limits_{c=1}^N \varrho_c^{(k)} s_c^{(k)} e_{s_c^{(k)}}^{(\Gamma_c(t))}
%           \nonumber \\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}} \sum\limits_{i\in\mathcal{S}_c^{(k)}}
%           \big(\mathbf w_{s_c^{(k)}}(t-1)-s_c^{(k)}\eta_k e_{s_c^{(k)}}^{(\Gamma_c(t-1))}\big)
%           \nonumber \\&
%           -\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}} 
%           \eta_k\nabla F_i(\mathbf w_i(t-1))
%           + \eta_k\sum\limits_{c=1}^N \varrho_c^{(k)} s_c^{(k)} e_{s_c^{(k)}}^{(\Gamma_c(t))},
% \end{align}

% Sample all nodes
%     \begin{align*}
%     \mathbf w(t) &= 
%           \sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}} \sum\limits_{i\in\mathcal{S}_c^{(k)}}\overline{\mathbf w_{c}}(t)
%           \nonumber\\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}} \sum\limits_{i\in\mathcal{S}_c^{(k)}}\big(\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \left[\mathbf w_j(t-1)-\eta_k\nabla F_j(\mathbf w_j(t-1))\right]\big)
%           \nonumber\\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \mathbf w_j(t-1)
%           \nonumber\\&
%           -\eta_k\sum\limits_{c=1}^N \varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}} \nabla F_i(\mathbf w_i(t-1))
%           \nonumber\\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \Big(\bar{\mathbf w}_c(t-1)+s_c^{(k)} e_{j}^{(\Gamma_c(t-1))}\Big)
%           \nonumber\\&
%           -\eta_k\sum\limits_{c=1}^N \varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}} \nabla F_i(\mathbf w_i(t-1))
%           \nonumber\\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \bar{\mathbf w}_c(t-1)
%           -\eta_k\sum\limits_{c=1}^N \varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}} \nabla F_i(\mathbf w_i(t-1))
%           \nonumber\\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \Big(\mathbf w_j(t-1)-s_c^{(k)} e_{j}^{(\Gamma_c(t-1))}\Big)
%           \nonumber\\&
%           -\eta_k\sum\limits_{c=1}^N \varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}} \nabla F_i(\mathbf w_i(t-1))
%           \nonumber\\&
%           = \mathbf w(t-1)
%           -\eta_k\sum\limits_{c=1}^N \varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}} \nabla F_i(\mathbf w_i(t-1))
%     \end{align*}

% \begin{align} \label{15}
%     \hat{\mathbf w}(t_k) &= 
%           \sum\limits_{c=1}^N \varrho_c^{(k)} \mathbf w_{s_c}(t_k)
% \end{align}
% % \nm{didnt you say that you sample a single device from each cluster?} \chris{$s_c^{(k)}$ is the node index that is being sampled?}
% % where $\mathbf w(t)$ consists of the perfect average of the cluster parameters (a) across the  network with an additional term caused by performing finite consensus rounds (b). 
% The server then broadcasts this parameter across the network and each edge device replaces its local parameter $\mathbf w_i(t)$ with $\hat{\mathbf w}(t)$ and starts a series of new local descents steps. 

% \begin{algorithm}
% \small
% \SetAlgoLined
% \caption{Two Timescale Hybrid Federated Learning ({\tt TT-HF})} \label{GT}
% % \KwResult{Write here the result }
% \KwIn{$\Gamma$,$\gamma$,$K$} 
% \KwOut{Global model $\hat{\mathbf w}(t_K)$}
%  \textbf{Initialization operated by the server:} (i) Initialize the local model as $\mathbf w_i(0)=\hat{\mathbf w}(0),\  \forall i$, (ii) Set the learning rate as $\eta_t=\frac{\gamma}{t+\alpha}$, where $\alpha=\max\{\beta\gamma/\kappa, \beta\gamma\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\}$\;
%  \For{$k=1:K$}{
%      \For{$t=t_{k-1}+1:t_k$}{
%       \uIf{$t=t_k$}{
%       Estimate $\hat{\beta}\leftarrow\sum_{c=1}^N \varrho_c^{(k)}\hat{\beta}_i$\;
%       Estimate $\hat{\mu}\leftarrow\sum_{c=1}^N \varrho_c^{(k)}\hat{\mu}_i$\;
%       Estimate $\delta,\zeta\leftarrow\Vert\nabla F(\hat{\mathbf w}(t_k))-\nabla F_i(\hat{\mathbf w}(t_k))\Vert$\;
%       Estimate parameter for the variance of SGD $\sigma$\;
%       Compute $\hat{\mathbf w}(t)$ with \eqref{15} at the server\;
%       Synchronize local models with the global model $\mathbf w_i(t)=\hat{\mathbf w}(t),~\forall i$ //Global Synchronization\;
%       Randomly choose some $\tau_{k+1}<\log_4\frac{\kappa}{16\omega^2}$\;
%         \uIf{$\tau_{k+1}>\log_4\left\{(\mu\gamma-1)\kappa/(16\omega^2\beta^2\gamma^2)\right\}$} 
%         {
%         $\tau_{k+1} = \tau_{k+1}/2$ // The initial choice of $\tau_{k+1}$ could not be satisfied\;
%         }
%         \Else{
%         Tune the number of D2D consensus such that \\
%         $\tau_k \leq \log_4\left\{[(\mu\gamma-1)\nu-B]/[\gamma^2\beta A/2 +16\omega^2\beta^2\gamma^2(\Gamma-\phi^2\gamma/2)/\kappa]\right\}$ is satisfied\;
%         }
%           }
%     \Else{
%       For each edge device $i\in\mathcal{S}_c^{(k)}$ in parallel, perform local update with\\
%       $\mathbf w_i(t+1) =  
%           \sum\limits_{i\in \mathcal S_c^{(k)}}a_{i,j}^{(c)}[\mathbf w_j(t)-\eta_t\nabla F_j(\mathbf w_j(t))]$, \\
%           where the weight $a_{i,j}^{(c)}$ embed several rounds of D2D consensus\;
%           \uIf{$\hat{\beta}_i<\Vert\nabla F_i(\mathbf w _i(t))-\nabla F_i(\hat{\mathbf w}(t_{k-1}))\Vert/\Vert\mathbf w _i(t)-\hat{\mathbf w}(t_{k-1})\Vert$}
%           {
%           Estimate $\hat{\beta}_i\leftarrow\Vert\nabla F_i(\mathbf w _i(t))-\nabla F_i(\hat{\mathbf w}(t_{k-1}))\Vert/\Vert\mathbf w _i(t)-\hat{\mathbf w}(t_{k-1})\Vert$\;
%           }
%           \uIf{$\hat{\mu}_i>(\nabla F_i(\mathbf w _i(t))-\nabla F_i(\hat{\mathbf w}(t_{k-1})))^T(\mathbf w _i(t)-\hat{\mathbf w}(t_{k-1}))/\Vert\mathbf w _i(t)-\hat{\mathbf w}(t_{k-1})\Vert^2$}
%           {Estimate $\hat{\mu}_i\leftarrow(\nabla F_i(\mathbf w _i(t))-\nabla F_i(\hat{\mathbf w}(t_{k-1})))^T(\mathbf w _i(t)-\hat{\mathbf w}(t_{k-1}))/\Vert\mathbf w _i(t)-\hat{\mathbf w}(t_{k-1})\Vert^2$\;
%           }
          
%       }
%      }
%  }
% \end{algorithm}


\section{Convergence Analysis of {\tt TT-HF}} \label{sec:convAnalysis}
\noindent  In this section, we study the convergence of
{\tt TT-HF} in terms of the optimality gap
 $F(\hat{\mathbf w}(t_K))-F(\mathbf w^*)$\nm{$\hat{\mathbf w}(t_K)-->\hat{\mathbf w}^k$?}between the
global objective function at the algorithm output $\hat{\mathbf w}(t_K)=\mathbf w(T)$\nm{?} and at the globally optimal parameter vector $\mathbf w^*$. 

\begin{definition} \label{paraDiv}
We define $\epsilon_c(k)$ as an upper bound of $\Vert e_i^{(t)}\Vert$ such that:
    \begin{align}
        \frac{1}{s_c^{(k)}}\sum\limits_{i\in \mathcal S_c^{(k)}}\Vert e_i^{(t)}\Vert^2 \leq \epsilon_c^2(k), ~\forall i\in\mathcal{S}_c^{(k)}, ~\forall t\in\mathcal{T}_k,
    \end{align}
    and we define $\epsilon(k)^2=\sum\limits_c\rho_c\epsilon_c^2(k)$.
    % \chris{Is $\epsilon(k)$ another definition? It sounds weird to say ``where'' since it does not appear in (13).}
\end{definition}
In Definition \ref{paraDiv}, we characterize the consensus error in each cluster $c$ by $\epsilon_c(k)$ and the average of consensus error across clusters in the network by $\epsilon(k)$. The value of $\epsilon_c(k)$ and $\epsilon(k)$ decreases when more rounds of consensus is performed and vice versa. In Section III and IV, we demonstrate that by tuning the number of D2D rounds and the interval between consecutive consensus formations, $\epsilon(k)$ can be enforced to be a desired value to guarantee certain convergence behavior. 
% \nm{you need to explain more what this epsilon is about.. why do you need it? How are you going to design it? The reviewer will ask about this so you need to make it very clear early on}

\begin{theorem} \label{co1}
Under Assumption \ref{beta} and after $k$ global iterations of {\tt TT-HF}, for $\eta_k\leq\kappa/\beta$ and $t \in \mathcal T_k$,
\add{we have that}\sst{an upper bound of $\mathbb E\left[F(\hat{\mathbf w}(t_k))-F(\mathbf w^*)\right]$ is given as follows:}
\begin{align} \label{main}
    &\mathbb E\left[F(\hat{\mathbf w}(t_k))-F(\mathbf w^*)\right] 
    \nonumber \\&
    \leq  
    \underbrace{\left((1-\mu\eta_k)^{\tau_k}+16\eta_k\omega^2\beta/\kappa\left[\frac{(1-\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\right)}_{(a)} [ F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)]
    \nonumber \\&
    + \frac{\eta_k\beta^2}{2}\left[\frac{(1+\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\{153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
    \nonumber \\&
    +\frac{1-(1-\mu\eta_k)^{\tau_k}}{2}\kappa[\eta_k\sigma^2+\beta\epsilon^2(k)]
    +\frac{\beta}{2}\epsilon^2(k),
\end{align}
where $\lambda_1 = 1+\eta_k\beta\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]$.
\end{theorem}
\begin{skproof}
\nm{in the sketch, make sure to define all quantities: what is $\bar{\mathbf w}$?}
To derive \eqref{main}, we first bound the sub-optimality gap of the global average using $\beta$-smoothness of the global function $F$, $\mathbb E_t[\widehat{\mathbf g}_{j,t}]=\nabla F_j(\mathbf w_j(t))$ and $\mathbb E_t[\Vert(\widehat{\mathbf g}_{j,t}-\nabla F_j(\mathbf w_j(t))\Vert_2^2]\leq \sigma^2$\nm{this is an assumption, but was never stated earlier!}:
\begin{align} \label{A_rel}
    &\mathbb E\left[F(\bar{\mathbf w}(t+1))-F(\mathbf w^*)\right]
    \leq
    (1-\mu\eta_k)\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]+\frac{\eta_k\beta^2}{2}A(t)+\frac{\eta_k^2 \beta\sigma^2}{2},
\end{align}
where we have defined the \emph{local model dispersion } 
\begin{align}
    A(t)=\mathbb E\left[\sum\limits_{c=1}^N\varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}}\Big\Vert\bar{\mathbf w}(t)-\mathbf w_j(t)\Big\Vert^2\right].
\end{align} 
Then we bound the \emph{Local model dispersion } by Proposition \ref{lem2} in \eqref{A_rel} and recursively apply the relationship to obtain the upper bound for $\mathbb E\left[F(\bar{\mathbf w}(t_k))-F(\mathbf w^*)\right]$:
\begin{align} \label{bar}
    &\mathbb E\left[F(\bar{\mathbf w}(t_k))-F(\mathbf w^*)\right] 
    \nonumber \\&
    \leq  
    \left((1-\mu\eta_k)^{\tau_k}+16\eta_k\omega^2\beta/\kappa\left[\frac{(1-\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\right)[ F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)] 
    \nonumber \\&
    + \frac{\eta_k\beta^2}{2}\left[\frac{(1+\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\{153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
    \nonumber \\&
    +\frac{1-(1-\mu\eta_k)^{\tau_k}}{2\kappa}[\eta_k\sigma^2+\beta\epsilon^2(k)].
\end{align}
To find the bound for $\mathbb E\left[F(\hat{\mathbf w}(t_k))-F(\mathbf w^*)\right]$, we use $\beta$-smoothness of the global function $F(\cdot)$ to obtain the connection between $\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]$ and $\mathbb E\left[F(\hat{\mathbf w}(t))-F(\mathbf w^*)\right]$ as:
\begin{align} \label{bar-hat}
    &\mathbb E\left[F(\hat{\mathbf w}(t))-F(\mathbf w^*)\right]  
    \leq \mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]
    +\frac{\beta}{2}\epsilon^2(k).
\end{align}
Finally, combining \eqref{bar} and \eqref{bar-hat}, the result of the Theorem then directly follows. 
For more detailed proof, see Appendix A. 
\end{skproof}
 
Theorem \ref{co1} illustrates how the global parameter $\hat{\mathbf w}(t_k)$ converges to the optimal $\mathbf w^*$ with respect to each global update in {\tt TT-HF} and how the learning and system parameters affects this convergence. From \eqref{main}, we observe that the convergence of {\tt TT-HF} depends on (i) the characteristics of the loss function (i.e. $\mu,\beta$); (ii) the learning rate that determines the step size at each iteration of moving toward a minimum of the loss function (i.e. $\eta_k$); (iii) the time span between global aggregation (i.e. $\tau_k$); (iv) the error of consensus rounds across
clusters (i.e. $\epsilon(k)$); (v) the variance of the unbiased estimated local gradient (i.e. $\sigma$); (vi) the gradient diversity across local devices (i.e. $\delta$, $\zeta$). In particular, it can be seen that introducing $\zeta$ in the more generalized definition of gradient diversity results in an extra term of $\omega$ in the damping factor (a) in \eqref{main}. The value of (a) depends quadratically on $\omega$. For the speical case when $\omega=0$, (a) reduces to form of $(1-\mu\eta_k)^{\tau_k}$, which is an often seen expression for the damping factor in other literature as \cite{haddadpour2019convergence,Li}. We also see that the upper bound for $\mathbb E\left[F(\hat{\mathbf w}(t_k))-F(\mathbf w^*)\right]$ depends on $\lambda_1$, which refers to the largest eigenvalue of the coupled system dynamics with respect to the expected difference between cluster and auxiliary global model $\sqrt{\mathbb E[(\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert)^2]}$ and the expected difference between the auxiliary global and optimal model $\sqrt{\mathbb E[\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert^2]}$. 
Note that we use the term auxiliary global model for $\bar{\mathbf w}(t)$ to differentiate it from the actual global model $\hat{\mathbf w}(t)$ estimated at the server.\nm{Move earlier!}

\subsubsection{Tunable learning parameters}
Upon the factors mentioned above, $\eta_k,\tau_k$ and $\epsilon(k)$ are the three tunable learning parameters that are directly related to the learning performance of {\tt TT-HF}. The design of these parameters affects how the global model $\hat{\mathbf w}(t_k)$ converges to the optimum as $k$ increases. Different design of these learning parameters would lead to different convergence result (i.e. different convergence rate). From the expression in \eqref{main}, we see that $\eta_k$, $\tau_k$ and $\epsilon(k)$ can be designed to change dynamically according to different characteristics of the loss function (i.e. $\mu,\beta$), conditions of data diversity (i.e. related to the values of $\delta,\zeta$) and variance of the estimated gradient used in SGD (i.e. $\sigma$). Since the communication cost between the server and local devices is expensive in terms of bandwidth and delay, so ideally, we would want the local devices to be more often trained locally, or in other words, to have a larger value of $\tau_k$ between consecutive global aggregations. However, when the data diversity $\delta,\zeta,\epsilon(k)$ among different devices are large, the increase of $\tau_k$ would result in the degradation of the model performance. Therefore, by enabling D2D communication among the local devices, we could adaptively tune the round of consensus to modify the consensus error resulted from data diversity characterized by $\epsilon(k)$ and result in better learning performance with respect to a larger value of $\tau_k$. Note that the design of $\tau_k$ also depends on $\eta_k$. From \eqref{main}, we see that a larger value of $\eta_k$ would result in a smaller value of $\tau_k$, vice versa. This also demonstrates the coupled nature of the tunable learning parameters along with all the influencing factors mentioned above. 

\iffalse
\subsubsection{Consensus and $\tau_k$}
Term (a) results from the consensus error with $\Gamma_c(t)$ rounds of consensus when consensus is performed whereas term (b) results from the error when only local gradient descent update is carried out. Since $\sigma_c^{(k)}\leq s_c^{(k)}^4\epsilon_c^2(t)$, we have (a) smaller than (b). This demonstrates two things: (i) the consensus operation in between local gradient descent updates helps decrease the total error resulted from local model training; (ii) given the same amount of local updates between global aggregations, the learning performance increases by performing fewer rounds of local gradient descent in between consensus. Thus, by incorporating the consensus scheme, the number of local updates $\tau_k$ can be increased so that the communication frequency between the server and the local devices can be effectively reduced. In other words, larger $\tau_k$ can be tolerated by increasing the number of consensus rounds within $t\in\mathcal T_k$. 

\subsubsection{$\eta_k$ and $\tau_k$}
From the condition of the learning rate, it shows that as $\tau_k$ continue to increase, stricter condition would also be imposed on the learning rate $\eta_k$ such that the learning rate becomes smaller and smaller. Note that when $\tau_k=1$, we have $\tau_k=h_k$\nm{do you mean $h_k=1$?} and $\sigma_c^{(k)}=0$ such that the condition on the learning rate is relaxed to $\eta_k\leq1/\beta$. and \eqref{globBD} reduces to the case of centralized descent, which coincides with our intuition.

\subsubsection{$\delta(\mathbf w)$ and $\eta_k$}
Data similarity across different clusters is captured by the gradient diversity $\delta(\mathbf w)$.\nm{which $\mathbf w$?} From the condition on learning rate, since the learning rate is inversely proportional to $\delta(\mathbf w)$, a smaller gradient diversity helps achieve a faster convergence rate. When all the data among the clusters are similar to each other, $\delta(\mathbf w)$ becomes small resulting in a larger learning rate that helps the model to converge faster.
In contrast, when data across cluster are very dissimilar to each other that the gradients become orthogonal to each other, $\delta(\mathbf w)$ becomes very large such that the learning rate goes to zero, indicating very slow convergence.
\fi
Theorem \ref{co1} provides a high-level idea on how the learning parameters are coupled together and how they could affect the learning performance. This motivates us to further study and provide guidelines on the decision of these learning parameters $\eta_k$, $\tau_k$ and $\epsilon(k)$ with respect to the learning performance. In the following Theorem, we show that by introducing conditions on $\eta_k$, $\tau_k$ and $\epsilon(k)$ through time, {\tt TT-HF} is guaranteed to converge to the optimal point with rate $\mathcal{O}(1/t)$.


% \begin{lemma}
%     Under assumption \ref{beta}, \ref{PL} and \ref{gradDiv}, there exist some $\eta_k^*$ such that for $\eta_k\leq\eta_k^*$, the following condition
%     \begin{align*}
%         \eta_k\leq\frac{1}{\beta+\frac{2\beta^2\delta\tau_k}{\mu(1-\mu\eta_k)^{\tau_k-1}}}
%     \end{align*}
%     holds.
% \end{lemma}

% \begin{proof}
    
% \end{proof}



% For $t\in\cup_{n=1}^k (\mathcal T_n\setminus \mathcal H_k)$, the local models perform gradient update without consensus. These local models can be deemed as models with perfect average within one cluster $c$ plus an additional consensus error with $\Gamma_c(t)=0$, which is upper bounded by $s_c^{(k)}^2\epsilon_c^2(t)\leq\sigma_c^{(t)}$, resulting in (b).


\begin{theorem} \label{subLin}
    Under Assumption \ref{beta} and any realization of {\tt TT-HF}\nm{what do you mean by "any realization"? You are taking expectations below!}, for $\gamma>1/\mu$, $\eta_t=\frac{\gamma}{t+\alpha}$\nm{is this compatible with $\eta\leq \kappa/\beta$? (needed in Thm 1)},\nm{notation! time indices k and t are used to denote different timescales.} $\epsilon^2(t)=\eta_t\phi$\nm{what is phi?}, 
    % $\eta_0\leq \min \Big\{\kappa/\beta, 1/\left(\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\beta\right)\Big\}$ 
        \nm{$\epsilon^2(k)$ and $\eta_k$?}
    and
    $\tau_k<\log_4\frac{(\mu\gamma-1)\kappa}{16\omega^2\beta^2\gamma^2}$, where 
    $\alpha\geq\max\{\beta\gamma/\kappa, \beta\gamma\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\},
    $ we have
    \begin{align}
        \mathbb E\left[F(\hat{\mathbf w}(t))-F(\mathbf w^*)\right] \leq \frac{\Gamma}{t+\alpha},
    \end{align}
    \nm{why do you care for this condition to hold for all t? and not just at the end of the sync times In other words, using Thm1, I just want to find conditions such that
    \begin{align}
&    \underbrace{\left((1-\mu\eta_k)^{\tau_k}+16\eta_k\omega^2\beta/\kappa\left[\frac{(1-\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\right)}_{(a)}\frac{\Gamma}{t_{k-1}+\alpha}
    \nonumber \\&
    + \frac{\eta_k\beta^2}{2}\left[\frac{(1+\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\{153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
    \nonumber \\&
    +\frac{1-(1-\mu\eta_k)^{\tau_k}}{2}\kappa[\eta_k\sigma^2+\beta\epsilon^2(k)]
    +\frac{\beta}{2}\epsilon^2(k)
    \leq\frac{\Gamma}{t_{k}+\alpha}
\end{align}
    }
    where $\Gamma\geq\max\bigg\{\alpha[F(\hat{\mathbf w}(0))-F(\mathbf w^*)],\frac{\gamma^2\beta}{2}[Q_k A+B]/[\mu\gamma-(1+16\omega^2\beta^2\gamma^2 Q_k/\kappa)]+\beta\phi\gamma/2\bigg\}$, $A=(16\omega^2/\kappa+153/16)\beta\phi+81/16\sigma^2+\delta^2$, $B=\sigma^2+\beta\phi$ and $Q_k=4^{\tau_k}$.
\end{theorem}

\nm{I would rewrite the Thm as follows:}
\add{
\begin{theorem} \label{subLin}
Define
$A=(16\omega^2/\kappa+153/16)\beta\phi+81/16\sigma^2+\delta^2$, $B=\sigma^2+\beta\phi$
and 
let $\gamma>1/\mu$,
$\alpha\geq\beta\gamma\max\{\kappa^{-1}, 1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\}
    $, $\phi>0$  and
$\Gamma\geq\max\{\alpha[F(\hat{\mathbf w}(0))-F(\mathbf w^*)],\beta\phi\gamma/2+\frac{\gamma^2\beta}{2[\mu\gamma-1]}B\}$
be design parameters.
Then,
    under Assumption \ref{beta} and any realization of {\tt TT-HF}, by choosing  $\eta_k=\frac{\gamma}{k+\alpha}$, $\epsilon^2(k)=\eta_k\phi$
    and
    $$\tau_k\leq
    \min\{\log_4\frac{(\mu\gamma-1)\kappa}{16\omega^2\beta^2\gamma^2},
    \log_4\left(\frac{[\Gamma-\beta\phi\gamma/2]
    [\mu\gamma-1]
    -\frac{\gamma^2\beta}{2}B}{
    \frac{\gamma^2\beta}{2} A
+[\Gamma-\beta\phi\gamma/2]16\omega^2\beta^2\gamma^2/\kappa
    }\right)\}\ \forall k
    $$
    \nm{I think the second term in the min is always stricter...}
    \nm{you further need the condition
    $
    \Gamma
    >\frac{\gamma^2\beta}{2[\mu\gamma-1]}B+\beta\phi\gamma/2
    $ (introduced earlier)
    }
    we have that
    \begin{align}
        \mathbb E\left[F(\hat{\mathbf w}(t))-F(\mathbf w^*)\right] \leq \frac{\Gamma}{t+\alpha},
    \end{align}
    where
\end{theorem}
}

\begin{skproof}
In this proof, we first demonstrate sublinear convergence in each aggregation period with respect to $t$ by showing that $\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]\leq\frac{\hat{\nu}}{t+\alpha}$ for some $\hat{\nu}$ for $t=0,1,\dots,T$ by induction, where $\nu_0=\alpha[F(\hat{\mathbf w}(0))-F(\mathbf w^*)]$ and $\nu=\frac{\gamma^2\beta}{2}[Q_k A+B]/[\mu\gamma-(1+16\omega^2\beta^2\gamma^2 Q_k/\kappa)]$ at $t\in\mathcal T_k$ by induction:
\nm{What is the point of having Thm1 if you never use it? I think Thm1 should provide the 1 step analysis, so that you can use that to prove this Thm}

    For $m=1$,\nm{m already used} $\mathbb E\left[F(\hat{\mathbf w}(m))-F(\mathbf w^*)\right]\leq\frac{\nu_0}{m+\alpha}$ holds based on the definition. 
    Assume $\mathbb E\left[F(\bar{\mathbf w}(m))-F(\mathbf w^*)\right]\leq\frac{\nu}{m+\alpha}$ holds for $m=t$, we now show that $\mathbb E\left[F(\bar{\mathbf w}(m))-F(\mathbf w^*)\right]\leq\frac{\nu}{m+\alpha}$ holds for $m=t+1$.
    
    From \eqref{26} and \eqref{39}, we have   
    \nm{I dont understand, isnt eta constant within a synch period? See eq 12}
    \begin{align} \label{wbarM}
        &\mathbb E\left[F(\bar{\mathbf w}(t+1))-F(\mathbf w^*)\right]
        \nonumber \\&
        \leq 
        (1-\mu\eta_t)\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]
        +
        16\omega^2\beta\eta_t/\kappa[\lambda_1^{2(t-t_{k-1})}-1][F(\bar{\mathbf w}(t_{k-1}))-F(\mathbf w^*)+\frac{\beta}{2}\epsilon^2(t)]
        \nonumber \\&
        +\frac{\eta_t\beta^2}{2}
        [\lambda_1^{2(t-t_{k-1})}-1]\{153/16\epsilon^2(t)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
        +\frac{\eta_t\beta}{2}[\eta_t\sigma^2+\beta\epsilon^2(t)].
    \end{align}
    Based on the condition of $\eta_t$ and $\epsilon(t)$, \eqref{wbarM} becomes bounded by 
    \begin{align} \label{condBound}
        &\mathbb E\left[F(\bar{\mathbf w}(t+1))-F(\mathbf w^*)\right]
        % \nonumber \\&
        \leq
        \frac{t+\alpha-1}{(t+\alpha)^2}\nu-\frac{\mu\gamma_k-\big[1+16\omega^2\beta^2\gamma_k^2 Q_k/\kappa\big]}{(t+\alpha)^2}\nu
        % \nonumber \\&
        +\frac{\gamma_k^2\beta}{2(t+\alpha)^2}
        \left[Q_k A+B\right].
    \end{align} 
    Then for some $\gamma>1/\mu$,
    $
        \tau_k<\log_4\frac{(\mu\gamma-1)\kappa}{16\omega^2\beta^2\gamma^2}.
    $
    and 
    $
        \nu 
        \geq 
        \frac{\gamma^2\beta}{2}[Q_k A+B]
        /
        [\mu\gamma-(1+16\omega^2\beta^2\gamma^2 Q_k/\kappa)],
    $
    we show that \eqref{condBound} is bounded by
    \begin{align} \label{sub_t+1}
        &\mathbb E\left[F(\bar{\mathbf w}(t+1))-F(\mathbf w^*)\right]
        \leq
        \frac{\nu}{t+\alpha+1},
    \end{align}
    and thus show that $\mathbb E\left[F(\bar{\mathbf w}(m))-F(\mathbf w^*)\right]\leq\frac{\nu}{m+\alpha}$ holds for $m=t+1$.
    Thus we have 
    \begin{align} \label{wbar_pfd2}
        &\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]
        \leq
        \frac{\hat{\nu}}{t+\alpha},~\forall t=0,1,\dots,T,
    \end{align}
    Now, we want to prove $\mathbb E\left[F(\hat{\mathbf w}(t))-F(\mathbf w^*)\right]\leq\frac{\Gamma}{t+\alpha}$: \\
    For $t=0$, we have $\mathbb E\left[F(\hat{\mathbf w}(t))-F(\mathbf w^*)\right] \leq \frac{\Gamma_1}{t+\alpha}$ holds for $\Gamma_1=\alpha\left[F(\hat{\mathbf w}(0))-F(\mathbf w^*)\right]$. \\
    For $t>0$, we use the connection between $\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]$ and $\mathbb E\left[F(\hat{\mathbf w}(t))-F(\mathbf w^*)\right]$ to obtain:
    \begin{align}
        &\mathbb E\left[F(\hat{\mathbf w}(t))-F(\mathbf w^*)\right]  
        \leq \mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]
        +\frac{\beta}{2}\epsilon^2(t)
        \nonumber \\&
        \leq \frac{\nu}{t+\alpha} +\frac{\beta\epsilon^2(t)}{2}
        \leq \frac{\nu}{t+\alpha} +\frac{\eta_t\beta\phi}{2}.
    \end{align}
    Choose $\eta_t=\frac{\gamma}{t+\alpha}$, we then have
    \begin{align}
        &\mathbb E\left[F(\hat{\mathbf w}(t))-F(\mathbf w^*)\right]  
        \leq \frac{\nu}{t+\alpha} +\frac{\beta\phi\gamma}{2(t+\alpha)}=\frac{\Gamma_2}{t+\alpha},
    \end{align}
    where $\Gamma_2=\nu+\beta\phi\gamma/2$ for $t>0$. Therefore, we show that, for $\Gamma\geq\max\{\Gamma_1,\Gamma_2\}$, we have 
    \begin{align}
        &\mathbb E\left[F(\hat{\mathbf w}(t))-F(\mathbf w^*)\right]  
        \leq \frac{\Gamma}{t+\alpha}.
    \end{align}
    
    To further satisfy the initial condition of for $\eta_0\leq\min\{\kappa/\beta,1/\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\beta\}$, the range of $\alpha$ is given as 
    \begin{align}
        \alpha \geq \max\{\beta\gamma/\kappa, \beta\gamma\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\}.
    \end{align}
For more detailed proof, see Appendix A.
\end{skproof}

Theorem \ref{subLin} provides a direction for designing the tunable learning parameters ($\eta_k,\tau_k,\epsilon(t)$) to guarantee convergence to the optimum with rate of $\mathcal{O}(1/t)$. Given a predetermined value of $\Gamma$, $\gamma$, and $\nu$, we can find the condition for $\eta_k,\tau_k$ and $\epsilon(k)$\nm{eps is decreasing over k, which means that consensus needs to be more and more accurate, i.e. more conesnsus step.. CAn you provide guarantees o nthe number of consensus steps?} such that the model $\hat{\mathbf w}(t)$ obtained from {\tt TT-HF} is guaranteed to converge sublinearly to the optimal point $\mathbf w^*$ with respect to $t$. 

To achieve convergence with the rate of $\mathcal{O}(1/t)$, it can be observed from Theorem \ref{subLin} that we want to design the learning rate as a linearly decreasing function $\eta_t=\frac{\gamma}{t+\alpha}$ with respect to $t$, the number of local updates $\tau_k$ restricted within $\tau_k \leq \log_4\left\{[(\mu\gamma-1)\nu-B]/[\frac{\gamma^2\beta}{2}A+\frac{16\omega^2\beta^2\gamma^2\nu}{\kappa}]\right\}$ given some predetermined $\nu\geq B/(\mu\gamma-1)$ and the consensus error to be in the order of the learning rate, depicted as $\epsilon^2(t)=\eta_t\phi$. 

The value for $\gamma$ resides within the range of $\frac{\kappa\mu-\kappa\sqrt{\mu^2-64\cdot 4^{\tau_k}\omega^2\beta^2 /(\kappa)}}{32\cdot 4^{\tau_k}\omega^2\beta^2 }<\gamma<\frac{\kappa\mu+\kappa\sqrt{\mu^2-64\cdot 4^{\tau_k}\omega^2\beta^2 /(\kappa)}}{32\cdot 4^{\tau_k}\omega^2\beta^2}$. Consider the special case of $\omega=0$, the range of $\gamma$ becomes $\gamma>1/\mu$ with no constraints on the value of $\tau_k$, which is equivalent to the expression in \cite{hosseinalipour2020multi,Li}. For $\omega>0$, we can see that the range of $\gamma$ depends on the value of $\tau_k$ for all $k$. Note that since we do not know the value of $\tau_k$ for all $k$ beforehand in practice, we could obtain the value of $\gamma$ by numerically analyze the feasible range of $\gamma$ with respect to various values of global aggregation period $\tau_k<\log_4\frac{\kappa}{16\omega^2}$.

The value for the predetermined value of $\nu$ is coupled with the decision of $\tau_k$. Given some $\nu\geq B/(\mu\gamma-1)$, $\tau_k$ should reside in the range of $\tau_k \leq \log_4\left\{[(\mu\gamma-1)\nu-B]/[\frac{\gamma^2\beta}{2}A+\frac{16\omega^2\beta^2\gamma^2\nu}{\kappa}]\right\}$.
Or equivalently, given some $\tau_k<\log_4\frac{\kappa^3}{64\omega^2}$, the choice of $\nu$ should satisfy $\nu\geq\frac{\gamma^2\beta}{2}[Q_k A+B]/[\mu\gamma-(1+16\omega^2\beta^2\gamma^2 Q_k/\kappa)]$ to guarantee sublinear convergence. We can see that the value of $\nu$ increases with $\tau_k$, $\delta$, $\omega$ and $\phi$. Since $\delta$ and $\omega$ are directly based on the characteristics of data and cannot be changed manually, this shows that, to guarantee sublinear convergence with respect to a predetermined value of $\nu$, we can tune $\tau_k$ and $\phi$ such that $\nu$ satisfies the requirement to achieve convergence. In particular, the value of $\tau_k$ increases when $\phi$ decreases and vice versa, this shows that we can allow larger value of $\tau_k$ with a smaller value of $\phi$ by performing more rounds of consensus during local updates. This result also leads to the development of the control algorithm for {\tt TT-HF} is section IV.  

\nm{what is the point of this prop? IT seems disconnected from the rest}
\begin{proposition}\label{lem2}
Under Assumption \ref{beta} and Algorithm \ref{GT}, for $\eta_k\leq\frac{\kappa}{\beta}$ and $t \in \mathcal T_k$, we have
    \begin{align*}
        &\mathbb E\left[\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}}\Big\Vert\bar{\mathbf w}(t)-\mathbf w_i(t)\Big\Vert^2\right]
        \nonumber \\& 
        \leq 
        [\lambda_1^{2t}-1]\{32\omega^2/\mu[F\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)]+153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}+\epsilon^2(k),
    \end{align*}
    where $\lambda_1 = 1+\eta_k\beta\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]$.
\end{proposition}
\begin{skproof}
Combining the following expression of $\bar{\mathbf w}_c(t)$ and $\bar{\mathbf w}(t)$
\begin{align}
        &\bar{\mathbf w}_c(t)=\bar{\mathbf w_c}(t-1)-\eta_k\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\widehat{\mathbf g}_{j,l}
    \end{align}
    and 
    \begin{align}
        &\bar{\mathbf w}(t)=
        \bar{\mathbf w}(t-1)-\eta_k\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\widehat{\mathbf g}_{j,l},
    \end{align}
    with properties of Assumption \ref{beta}, \ref{SGD} and Definition \ref{gradDiv}, we obtain the following system dynamics 
    \begin{align} \label{69}
        \mathbf x(t)
        &\leq 
        \mathbf M x(t-1)
        + \eta_k(\sigma+\beta\epsilon(k))\mathbf 1+\eta_k(\delta+\beta\epsilon(k)+(\sqrt{2}-1)\sigma)\mathbf e_1,
    \end{align}
    where $x_1(t)=\sqrt{\mathbb E[(\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert)^2]}$, $x_2(t)=\sqrt{\mathbb E[\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert^2]}$, \\$\mathbf M=\begin{bmatrix} 1-\eta_k\mu/2 & \eta_k\beta\\ \eta_k\zeta & 1+2\eta_k\beta\end{bmatrix}$ 
    with the largest eigenvalue of $$\lambda_1 = 1+\eta_k\beta\big[1-\kappa/4+\sqrt{C}\big]=1+\eta_k\beta\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big].$$
    By eigen-decomposition of $\mathbf M$, we obtain the following bound 
    \begin{align} \label{errorC}
        &\sum\limits_{c=1}^N\varrho_c^{(k)}\mathbb E[\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert^2]
        % \nonumber \\&
        \leq
        [\lambda_1^{2t}-1]\{32\omega^2/\mu[F\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)]+153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}.
    \end{align}
    Finally, since
    \begin{align} 
        \Vert\mathbf w_i(t)-\bar{\mathbf w}(t)\Vert_2^2
        =
        \Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert_2^2
        +\Vert e_i^{(t)}\Vert^2
        +2[\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)]^Te_i^{(t)}
    \end{align}
such that
\begin{align} \label{errors2}
\mathbb E\left[\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_c^{(k)}}\sum_i\Vert\mathbf w_i(t)-\bar{\mathbf w}(t)\Vert_2^2\right]
        =
        \sum\limits_{c=1}^N\varrho_{c}\mathbb E\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert_2^2
        +\mathbb E\left[\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_c^{(k)}}\sum_i\Vert{e_i^{(t)}\Vert}^2\right],
\end{align}    
    the result of the Proposition directly follows.
\end{skproof}

Proposition \ref{lem2} provides an upper bound for the \emph{Local model dispersion}, which quantifies the difference between the local model $\mathbf w_i(t)$ and the auxiliary global model $\bar{\mathbf w}(t)$. We also see that the bound for \emph{Local model dispersion} is dominated by $[\lambda_1^{2t}-1]$, where $\lambda_1$ refers to the largest eigenvalue of the coupled system dynamics with respect to the expected difference between cluster and auxiliary global model $\sqrt{\mathbb E[(\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert)^2]}$ and the expected difference between the auxiliary global and optimal model $\sqrt{\mathbb E[\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert^2]}$.  

\section{Adaptive Control Algorithm for {\tt TT-HF}}\label{Sec:controlAlg}
\noindent In Theorem \ref{subLin}, the policy to guarantee convergence to the optimum with rate of $\mathcal{O}(1/t)$. Given a predetermined value of $\Gamma$, $\gamma$, and $\nu$, we can find the condition for $\eta_k,\tau_k$ and $\epsilon(k)$ such that the model $\hat{\mathbf w}(t)$ obtained from {\tt TT-HF} is guaranteed to converge sublinearly to the optimal point $\mathbf w^*$ with respect to $t$. 
To realize the policies provided in Theorem \ref{subLin}, we design a two-phase adaptive control algorithm to tune the learning parameters in practice. It is assumed that the sever
has an estimate about the topology of each cluster, and thus the upper-bound of the
spectral radius $\lambda_c$ at each global iteration.

In this section, we describe the design of the two-phase adaptive control algorithm {\tt TT-HF} based on the results obtained from Theorem \ref{subLin} in Section III. In phase I, we design an parameter estimation scheme to estimate the value for defining the characteristics of the loss function (i.e. $\beta,\mu$), gradient diversity (i.e. $\delta,\zeta$) and upper bound on the variance of the estimated gradient used in SGD (i.e. $\sigma$). 
% the convergence parameters (i.e. $\Gamma,\gamma$) and the initial value on the length of the 1-st \emph{local model training interval} (i.e. $\tau_1$).
In phase II, we develop an adaptive control scheme for {\tt TT-HF} that adaptively tunes the learning rate, length of the local model training interval and the number of D2D rounds through time using the parameters estimated in the first phase.

% According to \eqref{cond1}, \eqref{cond2} and \eqref{cond3}, the tuning of learning parameters requires the knowledge on the upper bound of (A.) parameter divergence $\Upsilon_c(k)$ to satisfy the condition provided by Proposition \ref{genLin} and \ref{strLin}. Therefore, we first derive an adaptive estimation for the divergence of parameters in a distributed manner for every local iteration $t$. 

\begin{algorithm}
\small
\SetAlgoLined
\caption{Two Timescale Hybrid Federated Learning ({\tt TT-HF})-Phase I} \label{GT}
% \KwResult{Write here the result }
\KwIn{$K$} 
\KwOut{Global model $\hat{\mathbf w}(t_K)$}
 \textbf{Initialization operated by the server:} (i) Initialize the local model as $\mathbf w_i(0)=\hat{\mathbf w}(0),\  \forall i$, (ii) Set the learning rate as \add{$\eta_t=\frac{\gamma}{t+\alpha}$}, where $\alpha=\max\{\beta\gamma/\kappa, \beta\gamma\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\}$\;
 \For{$k=1:K$}{
     \For{$t=t_{k-1}+1:t_k$}{
      \uIf{$t=t_k$}{
      Estimate $\hat{\beta}\leftarrow\sum_{c=1}^N \varrho_c^{(k)}\hat{\beta}_i$\;
      Estimate $\hat{\mu}\leftarrow\sum_{c=1}^N \varrho_c^{(k)}\hat{\mu}_i$\;
      Estimate $\delta,\zeta\leftarrow\Vert\nabla F(\hat{\mathbf w}(t_k))-\nabla F_i(\hat{\mathbf w}(t_k))\Vert$\;
    %   Estimate parameter for the variance of SGD $\sigma$\;
      Compute $\hat{\mathbf w}(t)$ with \eqref{15} at the server\;
      Synchronize local models with the global model $\mathbf w_i(t)=\hat{\mathbf w}(t),~\forall i$ //Global Synchronization\;
        % \uIf{$\tau_{k+1}>\log_4\left\{(\mu\gamma-1)\kappa/(16\omega^2\beta^2\gamma^2)\right\}$} 
        % {
        % $\tau_{k+1} = \tau_{k+1}/2$ // The initial choice of $\tau_{k+1}$ could not be satisfied\;
        % }
        % \Else{
        % Tune the number of D2D consensus such that \\
        % $\tau_k \leq \log_4\left\{[(\mu\gamma-1)\nu-B]/[\gamma^2\beta A/2 +16\omega^2\beta^2\gamma^2(\Gamma-\phi^2\gamma/2)/\kappa]\right\}$ is satisfied\;
        % }
          }
    \Else{
      For each edge device $i$ in parallel, perform local update with\\
      $\mathbf w_i(t+1) =  
          \mathbf w_j(t)-\eta_t\nabla F_j(\mathbf w_j(t))$\;
          \uIf{$\hat{\mu}_i>(\nabla F_i(\mathbf w _i(t))-\nabla F_i(\hat{\mathbf w}(t_{k-1})))^T(\mathbf w _i(t)-\hat{\mathbf w}(t_{k-1}))/\Vert\mathbf w _i(t)-\hat{\mathbf w}(t_{k-1})\Vert^2$}
          {Estimate $\hat{\mu}_i\leftarrow(\nabla F_i(\mathbf w _i(t))-\nabla F_i(\hat{\mathbf w}(t_{k-1})))^T(\mathbf w _i(t)-\hat{\mathbf w}(t_{k-1}))/\Vert\mathbf w _i(t)-\hat{\mathbf w}(t_{k-1})\Vert^2$\;
          }
          \uIf{$\hat{\beta}_i<\Vert\nabla F_i(\mathbf w _i(t))-\nabla F_i(\hat{\mathbf w}(t_{k-1}))\Vert/\Vert\mathbf w _i(t)-\hat{\mathbf w}(t_{k-1})\Vert$}
          {
          Estimate $\hat{\beta}_i\leftarrow\Vert\nabla F_i(\mathbf w _i(t))-\nabla F_i(\hat{\mathbf w}(t_{k-1}))\Vert/\Vert\mathbf w _i(t)-\hat{\mathbf w}(t_{k-1})\Vert$\;
          }
          \uIf{$\hat{\tilde{\beta}}_i<\Vert\nabla \hat{f}(\mathbf x_{i,b};\mathbf w_i(t))-\nabla \hat{f}(\mathbf x_{i,b'};\mathbf w_i(t))\Vert/\Vert\mathbf x_{i,b}-\mathbf x_{i,b'}\Vert$}
          {
          Estimate $\hat{\tilde{\beta}}_i\leftarrow\Vert\nabla F_i(\mathbf w _i(t))-\nabla F_i(\hat{\mathbf w}(t_{k-1}))\Vert/\Vert\mathbf w _i(t)-\hat{\mathbf w}(t_{k-1})\Vert$\;
          }
      }
     }
 }
 Estimate $\hat{\sigma}_i^2 
\leftarrow
\Big(1-\frac{\vert\xi_i^{(t)}\vert}{D_i}\Big)\frac{\tilde{\beta}^2}{D_i}
\frac{\sum_{b=1}^{D_i}\sum_{b'=1}^{D_i}\Vert b-b'\Vert^2}
{\vert\xi_i^{(t)}\vert(D_i-1)}$\;
\end{algorithm}

\begin{algorithm}
\small 
\SetAlgoLined
\caption{Two Timescale Hybrid Federated Learning ({\tt TT-HF})-Phase II} \label{GT}
% \KwResult{Write here the result }
\KwIn{$\Gamma>\sigma^2/(\mu\gamma-1)$, $\gamma>1/\mu$, $\tau_1<\max\left\{\log_4\frac{(\mu\gamma-1)\kappa}{16\omega^2\beta^2\gamma^2},\log_4[(\mu\gamma-1)\kappa/(16\omega^2\beta^2\gamma^2)]\right\}$, $K$} 
\KwOut{Global model $\hat{\mathbf w}(t_K)$}
 \textbf{Initialization operated by the server:} (i) Initialize the local model as $\mathbf w_i(0)=\hat{\mathbf w}(0), \forall i$, (ii) Initialize the learning rate as $\eta_t=\frac{\gamma}{t+\alpha}$, where $\alpha=\max\{\beta\gamma/\kappa, \beta\gamma\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\}$\;
 \For{$k=1:K$}{
    \While{$\tau_k>\max\left\{\log_4\frac{(\mu\gamma-1)\kappa}{16\omega^2\beta^2\gamma^2},\log_4[(\mu\gamma-1)\kappa/(16\omega^2\beta^2\gamma^2)]\right\}$}
    {
    $\tau_{k} = \tau_k/2$\;
    }
     \For{$t=t_{k-1}+1:t_k$}{
      \uIf{$t=t_k$}{
    %   Estimate $\delta$\;
      Compute $\hat{\mathbf w}(t)$ with \eqref{15} at the server\;
      Synchronize local models with the global model $\mathbf w_i(t)=\hat{\mathbf w}(t),~\forall i$ //Global Synchronization\;
          \uIf{\max\limits_{t\in\mathcal{T}_k}\theta_c^{(t)}>\Theta}
          {
          $\tau_{k+1} = \tau_{k+1}/2$\;
          }
          \Else{$\tau_{k+1} = 2\tau_{k+1}$\;}
      }
    \Else{
        \For{$c=1:N$}
        {
        \uIf{$\tau_k > \log_4\left\{[(\mu\gamma-1)(\Gamma-\beta\phi\gamma/2)-B]/[\gamma^2\beta A/2 +16\omega^2\beta^2\gamma^2(\Gamma-\beta\phi\gamma/2)/\kappa]\right\}$ or $\Gamma< \beta\phi\gamma/2+B/(\mu\gamma-1)$}
        {Tune the number of D2D consensus such that 
        \begin{align*}
            \begin{cases}
                \tau_k \leq \log_4\left\{[(\mu\gamma-1)(\Gamma-\beta\phi\gamma/2)-B]/[\gamma^2\beta A/2 +16\omega^2\beta^2\gamma^2(\Gamma-\beta\phi\gamma/2)/\kappa]\right\}  \\
                \Gamma\geq \beta\phi\gamma/2+B/(\mu\gamma-1) 
            \end{cases}
        \end{align*}
        holds, where $A=(16\omega^2/\kappa+153/16)\beta\phi+81/16\sigma^2+\delta^2$, $B=\sigma^2+\beta\phi$ and $\theta_c^{(t)}\geq\log\Big(\eta_t\phi/(s_c\Upsilon_c^{(t)})\Big)/\log\Big(\lambda_c^{(t_{k-1})}\Big)$\;
        }
         For each edge device $i\in\mathcal{S}_c^{(k)}$ in parallel, perform local update with\\
          $ \left[\mathbf{W}^{(t)}_{{C}}\right]_i= \left[\left(\mathbf{V}^{(k)}_{{C}}\right)^{\Gamma^{(t)}_{{C}}} \widetilde{\mathbf{W}}^{(t)}_{{C}}\right]_i$,\\
          where $[\mathbf M]_i$ corresponds to the $i$-th row of matrix $\mathbf M$\;
          \add{$\tilde{\mathbf{w}}_i^{(t)} = 
           \mathbf w_i^{(t-1)}-\eta^{(k)} \widehat{\mathbf g}_{i}^{(t-1)}$\;
        followed by rounds of consensus \\
        $\mathbf{w}_i^{(t)} = ...$}
        }
      }
     }
 }
\end{algorithm}

\subsection{Estimation of Parameters (Phase I)} 
In Phase I, we perform several rounds of the standard federated averaging {\tt FedAvg} \cite{McMahan} and estimate the parameters throughout the training process. {\tt FedAvg} generally consist of three steps repeated in sequence: (i) several iterations of parallel, local model training at each device using their own local datasets, (ii) aggregation of the local models at an edge server into a single, global model, and (iii) synchronization of the local models at each device with this global model. During the training process, we continuing to estimate $\beta,\mu,\delta,\zeta$ and $\sigma$ with the local and global models. After several rounds of training, we obtain the estimated value of all the parameters, and use them for the scheme in Phase II. The estimation scheme for each of the parameters is demonstrated as follows. In the following, we use the same notations for the system model of {\tt TT-HF} as described in Section II:

\subsubsection{Estimation of $\mu$}
We estimate the value of $\mu$ by the monoticity property of strongly convex of $F_i(\cdot)$ expressed as 
$$
\exists \mu>0: \mu\leq\big(\nabla F_i(\mathbf w_1)-\nabla F_i(\mathbf w_2)\big)^T\big(\mathbf w_1-\mathbf w_2\big)/\Vert\mathbf w _1-\mathbf w_2\Vert^2,~\forall \mathbf w_1, \mathbf w_2.
$$ 
During the training phase of {\tt FedAvg}, we collect the global model at the beginning of each \emph{local model training interval} at $t_{k-1}$, and continually calculate $[\nabla F_i(\mathbf w _i(t))-\nabla F_i(\hat{\mathbf w}(t_{k-1}))]^T[\mathbf w _i(t)-\hat{\mathbf w}(t_{k-1})]/\Vert\mathbf w _i(t)-\hat{\mathbf w}(t_{k-1})\Vert^2,~t\in\mathcal{T}_k$ as the candidates for $\hat{\mu_i}$ at every edge device $i$. The smallest value of candidates estimated at every edge device $i$ throughout the entire training process will be chosen as $\hat{\mu_i}$. Then, we compute $\hat{\mu}\leftarrow\sum_{c=1}^N \varrho_c^{(k)}\hat{\mu}_i$ as the estimate for $\mu$.
\subsubsection{Estimation of $\beta$}
We estimate the value of $\beta$ by the characteristic of $\beta$-smooth for $F_i(\cdot)$ expressed in Assumption \ref{beta} as:
\begin{align}
\exists \beta>0: \Big\Vert \nabla F_i(\mathbf w_1)-\nabla F_i(\mathbf w_2)\Big\Vert \leq & \beta\Big\Vert \mathbf w_1-\mathbf w_2 \Big\Vert,~\forall i, \mathbf w_1, \mathbf w_2.
 \end{align}
 During the training phase of {\tt FedAvg}, we collect the global model at the beginning of each \emph{local model training interval} at $t_{k-1}$, and continually calculate $\Vert\nabla F_i(\mathbf w _i(t))-\nabla F_i(\hat{\mathbf w}(t_{k-1}))\Vert/\Vert\mathbf w _i(t)-\hat{\mathbf w}(t_{k-1})\Vert,~t\in\mathcal{T}_k$ as the candidates for $\hat{\mu_i}$ at every edge device $i$. The largest value of candidates estimated at every edge device $i$ throughout the entire training process will be chosen as $\hat{\beta}_i$. Then, we compute $\hat{\beta}\leftarrow\sum_{c=1}^N \varrho_c^{(k)}\hat{\beta}_i$ as the estimate for $\beta$.
\subsubsection{Estimation of $\delta,\zeta$}
To estimate the value of $\delta,\zeta$, we first upper bound the gradient diversity in Definition \ref{gradDiv} as 
\begin{align} 
    \Vert\nabla\hat F_c(\mathbf w)-\nabla F(\mathbf w)\Vert
    \leq \delta+ \zeta \Vert\mathbf w-\mathbf w^*\Vert 
    \leq \underbrace{\delta+\zeta\Vert\mathbf w^*\Vert}_{(a)} + \zeta \Vert\mathbf w\Vert,~\forall c, \mathbf w.
\end{align}

\subsubsection{Estimation of $\delta,\zeta$}
To estimate the value of $\delta,\zeta$, we first upper bound the gradient diversity in Definition \ref{gradDiv} as 
\begin{align} 
    \Vert\nabla\hat F_c(\mathbf w)-\nabla F(\mathbf w)\Vert
    \leq \delta+ \zeta \Vert\mathbf w-\mathbf w^*\Vert 
    \leq \underbrace{\delta+\zeta\Vert\mathbf w^*\Vert}_{(a)} + \zeta \Vert\mathbf w\Vert,~\forall c, \mathbf w.
\end{align}

\subsubsection{Estimation of $\tilde{\beta}$}
We estimate the value of $\tilde{\beta}$ by the following Assumption as:
\begin{assumption}[Data Variability]\label{dataDiv}
    The data variability for the loss functions $\hat{f}(\cdot)$ is $\tilde{\beta}$-smooth,$~\forall i$, i.e.,
    \begin{align}
        \exists\tilde{\beta}:\Vert\nabla \hat{f}(\mathbf x_{i,b};\mathbf w)-\nabla \hat{f}(\mathbf x_{i,b'};\mathbf w)\Vert\leq \tilde{\beta}\Vert\mathbf x_{i,b}-\mathbf x_{i,b'}\Vert,~\forall \mathbf w,\mathbf x_{i,b},\mathbf x_{i,b'}, 
    \end{align}
    where $\mathbf x_{i,b}\in\mathcal{D}_i,~\forall i,b$.
\end{assumption}
 During the training phase of {\tt FedAvg}, we collect the global model at the beginning of each \emph{local model training interval} at $t_{k-1}$, and continually calculate $\Vert\nabla \hat{f}(\mathbf x_{i,b};\mathbf w_i(t))-\nabla \hat{f}(\mathbf x_{i,b'};\mathbf w_i(t))\Vert/\Vert\mathbf x_{i,b}-\mathbf x_{i,b'}\Vert$ as the candidates for $\tilde{\beta}$ with respect to the local data set $\mathcal D_i$ at every edge device $i$. The largest value of candidates estimated at every edge device $i$ throughout the entire training process will be chosen as $\hat{\tilde{\beta}}$. Then, we compute $\hat{\tilde{\beta}}\leftarrow\sum_{c=1}^N \varrho_c^{(k)}\hat{\tilde{\beta}}$ as the estimate for $\tilde{\beta}$.

\subsubsection{Estimation of $\sigma$}
% \begin{assumption}[Data Variability]\label{dataDiv}
%     The data variability for the local functions $F_i(\cdot)$ is bounded by a constant for each device $i$
%     \begin{align}
%         \exists\vartheta_i:\Vert\nabla \hat{f}(\mathbf x_{i,b};\mathbf w)-\nabla \hat{f}(\mathbf x_{i,b'};\mathbf w)\Vert^2\leq \vartheta_i,~\forall i,\omega,b,b', 
%     \end{align}
%     where $\mathbf x_{i,b}\in\mathcal{D}_i,~\forall i,b$.
% \end{assumption}

\begin{proposition} \label{SGD_estimate}
    $\mathbb E\Vert\widehat{\mathbf g}_{i}^{(t)}-\nabla F_i(\mathbf w_i(t))\Vert^2$ can be upper bounded as
    \begin{align}
        \mathbb E_t\Vert\widehat{\mathbf g}_{i}^{(t)}-\nabla F_i(\mathbf w_i(t))\Vert^2 
        \leq 
        \Big(1-\frac{\vert\xi_i^{(t)}\vert}{D_i}\Big)\frac{\tilde{\beta}^2}{D_i}
        \frac{\sum_{b=1}^{D_i}\sum_{b'=1}^{D_i}\Vert b-b'\Vert^2}
        {\vert\xi_i^{(t)}\vert(D_i-1)},
    \end{align}
    % such that the value of $\sigma^2$ can be obtained as follows
    % \begin{align}
    %     \sigma^2 = \Big(1-\frac{\vert\xi_i^{(t)}\vert}{D_i}\Big)\frac{\vartheta_i}{\vert\xi_i^{(t)}\vert},
    % \end{align}
    % where $\tilde{\beta}$ refers to the upper bound of data diversity in Assumption \ref{dataDiv}. 
\end{proposition}
\begin{proof}
    \begin{align}
        &\mathbb E_t\Vert\widehat{\mathbf g}_{i}^{(t)}-\nabla F_i(\mathbf w_i(t))\Vert^2 
        =\mathbb E\left\Vert\frac{1}{D_i}\sum\limits_{b=1}^{D_i}\nabla\hat{f}(\mathbf x_{i,b};\mathbf w_i(t))-\frac{1}{\vert\xi_i^{(t)}\vert}\sum\limits_{b\in\xi_i^{(t)}}\hat{f}(\mathbf x_{i,b};\mathbf w_i(t))\right\Vert^2
        \nonumber \\&
        = 
        \Big(1-\frac{\vert\xi_i^{(t)}\vert}{D_i}\Big)
        \frac
        {\sum_{b=1}^{D_i}\mathbb E\left\Vert\nabla \hat{f}(\mathbf x_{i,b};\mathbf w_i(t))-\nabla F_i(\mathbf w_i(t))\right\Vert^2}
        {\vert\xi_i^{(t)}\vert(D_i-1)}  
        \nonumber \\&
        = 
        \Big(1-\frac{\vert\xi_i^{(t)}\vert}{D_i}\Big)
        \frac
        {\sum_{b=1}^{D_i}\mathbb E\left\Vert\nabla \hat{f}(\mathbf x_{i,b};\mathbf w_i(t))-\frac{1}{D_i}\sum_{b'\in D_i} \hat{f}(\mathbf x_{i,b'};\mathbf w_i(t))\right\Vert^2}
        {\vert\xi_i^{(t)}\vert(D_i-1)} 
        \nonumber \\&
        = 
        \Big(1-\frac{\vert\xi_i^{(t)}\vert}{D_i}\Big)
        \frac
        {\sum_{b=1}^{D_i}\frac{1}{D_i^2}\mathbb E\left\Vert D_i\nabla\hat{f}(\mathbf x_{i,b};\mathbf w_i(t))-\sum_{b'\in D_i} \hat{f}(\mathbf x_{i,b'};\mathbf w_i(t))\right\Vert^2}
        {\vert\xi_i^{(t)}\vert(D_i-1)} 
        \nonumber \\&
        % \overset{(a)}{\leq}
        % \Big(1-\frac{\vert\xi_i^{(t)}\vert}{D_i}\Big)
        % \frac
        % {\sum_{b=1}^{D_i}\frac{D_i-1}{D_i}\vartheta_i}
        % {\vert\xi_i^{(t)}\vert(D_i-1)}
        % \nonumber \\&
        \overset{(b)}{\leq}
        \Big(1-\frac{\vert\xi_i^{(t)}\vert}{D_i}\Big)
        \frac
        {\sum_{b=1}^{D_i}\frac{D_i-1}{D_i^2}\tilde{\beta}^2\sum_{b'=1}^{D_i}\Vert b-b'\Vert^2}
        {\vert\xi_i^{(t)}\vert(D_i-1)}
        \nonumber \\&
        \leq
        \Big(1-\frac{\vert\xi_i^{(t)}\vert}{D_i}\Big)\frac{\tilde{\beta}^2}{D_i}
        \frac
        {\sum_{b=1}^{D_i}\sum_{b'=1}^{D_i}\Vert b-b'\Vert^2}
        {\vert\xi_i^{(t)}\vert(D_i-1)}.
    \end{align}
\end{proof}

Since the variance of $\mathbb E\Vert\widehat{\mathbf g}_{i}^{(t)}-\nabla F_i(\mathbf w_i(t))\Vert^2$ is upper bounded by $\sigma^2$, the value of $\sigma$ would be dependent on the value of the estimated gradient $\widehat{\mathbf g}_{i}^{(t)}$ and the value of the true gradient $\nabla F_i(\mathbf w_i(t))$. However, we do not directly calculate the value of gradient for two reasons: first, continually estimating the value of gradient throughout the training process consume a lot of computation resources; second, it makes no sense to calculate $\nabla F_i(\mathbf w_i(t))$ but using SGD (which performs stochastic gradient descent) throughout local training. Therefore, instead of directly compute the gradient, we refer to the result in Proposition \ref{SGD_estimate} and estimate the value of $\sigma^2$ as $\hat{\sigma}=\sum_{c=1}^N \varrho_c^{(k)}\hat{\sigma}_i,$ where 
$$
\hat{\sigma}_i^2 
=
\Big(1-\frac{\vert\xi_i^{(t)}\vert}{D_i}\Big)\frac{\tilde{\beta}^2}{D_i}
\frac{\sum_{b=1}^{D_i}\sum_{b'=1}^{D_i}\Vert b-b'\Vert^2}
{\vert\xi_i^{(t)}\vert(D_i-1)}.
$$
With Proposition \ref{SGD_estimate}, instead of using the dependency of $\sigma$ on the variance of the estimated gradient $\widehat{\mathbf g}_{i}^{(t)}$ to estimate $\sigma$, we turn to estimate the value of $\sigma$ based on its dependency on the data variability. As a result, we can directly estimate the value of $\sigma$ based on the local data we have in each device, which efficiently reduces the complexity of the estimation. 

\subsection{The Adaptive Control Scheme}
By Definition \ref{paraDiv} and given the conditions in XXX, since the server needs to determine the learning parameters at the beginning of global aggregation, we are interesting in approximating the prediction of $\epsilon_c(k)$ for $t\in\mathcal T_k$ by $\epsilon_c(k)$ at the beginning of the global aggregation $k$. Therefore, $\epsilon_c(k)$ is approximated as
% \begin{align*}
%     \epsilon_c(k) &\geq \Vert\mathbf w_i(t)-\mathbf w_j(t)\Vert
%     \\&
%     \geq \vert\Vert\mathbf w_i(t)\Vert-\Vert\mathbf w_j(t)\Vert\vert, ~\forall i, j\in\mathcal{S}_c^{(k)}, ~\forall t\in\mathcal T_k
% \end{align*}

\begin{align}
    \Upsilon_c(k) &\approx \max_{i, j\in\mathcal{S}_c^{(k)}.}\{\Vert\mathbf w_i(t)\Vert-\Vert\mathbf w_j(t)\Vert\}
    \\& \label{epsProx}
    =\underbrace{\max_{i\in\mathcal{S}_c^{(k)}}\Vert\mathbf w_i(t)\Vert}_{(a)}-\underbrace{\min_{j\in\mathcal{S}_c^{(k)}}\Vert\mathbf w_j(t)\Vert}_{(b)}, ~\forall t\in\mathcal T_k,
\end{align}

To obtain (a) and (b) for each cluster, each device in the cluster computes $\Vert\mathbf w_i(t_k)\Vert$ and share it with its neighbors iteratively. During 
each iteration, each device memorizes both the (i) maximum and (ii) minimum values among the exchanged values of $\Vert\mathbf w_i(t_k)\Vert$ such that the approximation of $\epsilon_c(k)$ can be estimated.


\iffalse
\begin{corollary} \label{SPEgenLin}
    Consider the case that each device performs rounds of distributed average consensus after each local gradient update, under Assumption \ref{beta}, \ref{PL}, Definition \ref{gradDiv} and Algorithm \ref{GT}, if the number of consensus rounds at different clusters of the network satisfies 
    $$
    \Gamma_c(k)\geq
    \left[\frac{\log(\sigma_c^{(k)})-2\log(s_c^{(k)}^2\varepsilon_c(k))}{2\log(\lambda_c)}\right]^+
    $$
    such that $\sigma_c^{(k)}$ satisfies the following inequalities 
    \begin{equation} \label{cond2}
        \begin{aligned}
            \begin{cases}
                \lim_{k\rightarrow\infty}B_{k+1}/B_k\leq 1, \\
                \eta_k\leq\frac{1}{\beta+\frac{2\beta^2\delta(\mathbf w)\tau_k}{\eta_k(1-\mu\eta_k)^{\tau_k-1}}\big[1-(1-\mu\eta_k)^{\tau_k-1}\big]},
            \end{cases},
        \end{aligned}
    \end{equation}
    \nm{which $\mathbf w$?}
    where $B_k = \frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu}\beta^2\tau_k m^2 h_k\sum\limits_{c=1}^N\varrho_{c} \sigma_c^{(k)}$. 
    Then Algorithm \ref{GT} is guaranteed to converge:
    \begin{align*}
        &F(\mathbf w(t_{k}))-F(\mathbf w^*)
        \\&
        \leq\prod_{n=1}^k(1-\mu\eta_n)^{\tau_n} \Big(F(\mathbf w(0))-F(\mathbf w^*)\Big)+\mathcal O(C_k),
    \end{align*}
    where $C_k=\max\{B_k,\Big((1-\mu\eta_k)^{\tau_k}+\xi\Big)^{t_k}\}$ for any $\xi>0$.
\end{corollary}

For the case that each device performs rounds of distributed average consensus after each local gradient update, corollary \ref{SPEgenLin} 
provides a guideline for designing the number of consensus rounds $\Gamma_c(k)$ at different network clusters over each global aggregation $k$, the number of local updates in the $k$-th global aggregation $\tau_k$ and the learning rate $\eta_k$ to guarantee weak linear convergence.

The condition to guarantee convergence is more relaxed compared to the condition for the case in proposition \ref{genLin} that consensus are performed after several rounds of local gradient updates since $\sigma_c^{(k)}$ is always upper bounded by $s_c^{(k)}^2\epsilon_c^2(t)$.
It shows that by performing less number of local gradient updates between consensus, the system obtains better performance, which coincides with the result in our simulation.

\begin{corollary} \label{SPEstr}
    Consider the case that each device performs rounds of distributed average consensus after each local gradient update, under Assumption \ref{beta}, \ref{PL} and Definition \ref{gradDiv}, if the number of consensus rounds at different clusters of the network are chosen to satisfy
    $$
    \Gamma_c(k)\geq
    \left[\frac{\log(\sigma_c^{(k)})-2\log(s_c^{(k)}^2\varepsilon_c(k))}{2\log(\lambda_c)}\right]^+
    $$
    where $\sigma_c^{(k)}$, $\tau_k$ and $\eta_k$ are determined to satisfy the following inequalities   
    \begin{align} \label{cond3}
            \begin{cases} \label{35}
            h_k\sum\limits_{c=1}^N\varrho_{c} \sigma_c^{(k)}
            \leq\frac{\mu^2(\gamma_k-\lambda_k)}{2\beta^4 m^2 \tau_k\left[1-(1-\mu\eta_k)^{\tau_k}\right]}\Big\Vert\nabla F(\mathbf w(t_{k-1}))\Big\Vert^2, \\
            \eta_k\leq\frac{1}{\beta+\frac{2\beta^2\delta(\mathbf w)\tau_k}{\eta_k(1-\mu\eta_k)^{\tau_k-1}}\big[1-(1-\mu\eta_k)^{\tau_k-1}\big]},
            \end{cases}.
    \end{align}
    \nm{what is gamma and is lambdak$<$gamma?}
    Then Algorithm \ref{GT} is guaranteed to achieve linear convergence:
    \begin{align*}
        &F(\mathbf w(t_{k}))-F(\mathbf w^*)
        \leq (1-\lambda_k)\Big(F(\mathbf w(t_{k-1}))-F(\mathbf w^*)\Big)
    \end{align*}
    for $0\leq\lambda_k\leq1-(1-\mu\eta_k)^{\tau_k}$.
\end{corollary}
\nm{how do you choose tau? Why not choosing it to be constant over k?
}
Similar to corollary \ref{SPEgenLin}, consider the case that each device performs rounds of distributed average consensus after each local gradient update, corollary \ref{SPEstr} 
provides a guideline for designing the number of consensus rounds $\Gamma_c(k)$ at different network clusters over each global aggregation $k$, the number of local updates in the $k$-th global aggregation $\tau_k$ and the learning rate $\eta_k$ to guarantee strong linear convergence with a rate $\lambda$ upper bounded by $1-(1-\mu\eta_k)^{\tau_k}$. 

The condition to guarantee convergence is more relaxed compared to the condition for the case in proposition \ref{strLin} that consensus are performed after several rounds of local gradient updates\\
since $\sigma_c^{(k)}$ is always upper bounded by $s_c^{(k)}^2\epsilon_c^2(t)$\\
It shows that by performing less number of local gradient updates between consensus, the system obtains better performance, which coincides with the result in our simulation.

\begin{corollary} \label{inf}
    Consider the case that each device performs infinite rounds of distributed average consensus after each local gradient update, under Assumption \ref{beta}, \ref{PL} and Definition \ref{gradDiv}, Algorithm \ref{GT} is always guaranteed to achieve linear convergence when $\eta_k\leq\frac{1}{\beta+\frac{2\beta^2\delta(\mathbf w)\tau_k}{\eta_k(1-\mu\eta_k)^{\tau_k-1}}\big[1-(1-\mu\eta_k)^{\tau_k-1}\big]}$:
    \begin{align*}
        &F(\mathbf w(t_{k}))-F(\mathbf w^*)
        \leq (1-\mu\eta_k)^{\tau_k}\Big(F(\mathbf w(t_{k-1}))-F(\mathbf w^*)\Big).
    \end{align*}
\end{corollary} 

Corollary \ref{inf} asserts that each device performs consensus after local gradient update,
if each device performs infinite rounds of consensus within their cluster and the learning rate is properly chosen to satisfy $\eta_k\leq\frac{1}{\beta+\frac{2\beta^2\delta\tau_k}{\mu(1-\mu\eta_k)^{\tau_k-1}}}$,\nm{since both sides of the eq depend on eta, when is it satisfied? (it is for eta=0), hence there exists a  range $[0,\eta_{th}]$, where $\eta_{th}$ is a function of beta delta tau, mu, for which it is satisfied} linear convergence is always guaranteed.  

For each local model $i$, instead being updated based on its own data, it is equivalent to be updated based upon the entire dataset from the cluster it belongs to.

\section{Real-time adaptive {\tt TT-HF} algorithm} \label{sec:control}
In this section, we develop a real-time adaptive {\tt TT-HF} algorithm to tune the learning parameters on the learning rate, time span between consecutive global aggregation and the number of consensus based on the result of convergence analysis in section \ref{sec:convAnalysis}.

In Propositions \ref{genLin} and \ref{strLin}, policies to guarantee linear convergence are provided. Proposition \ref{genLin} provides condition in \eqref{cond1} and \eqref{cond2} on the learning parameters $\eta_k$, $\tau_k$ and $\Gamma_c(k)$ to guarantee a general linear convergence to the optimal whereas Proposition \ref{strLin} provides a stricter condition \eqref{cond1} and \eqref{cond3} on these learning parameters to guarantee strong linear convergence to the optimal with convergence rate $\lambda\leq1-(1-\mu\eta_k)^{\tau_k}$.

To realize the policies provided in Propositions 1 and 2, we design a real-time adaptive algorithm to tune the learning parameters in practice. It is assumed that the sever
has an estimate about the topology of each cluster, and thus the upper-bound of the
spectral radius $\lambda_c$ at each global iteration.

According to \eqref{cond1}, \eqref{cond2} and \eqref{cond3}, the tuning of learning parameters requires the knowledge on the upper bound of (A.) parameter divergence $\epsilon_c(k)$ to satisfy the condition provided by Proposition \ref{genLin} and \ref{strLin}. In addition, Proposition \ref{strLin} further requires information of the (B.) global gradient $\Vert\nabla F(\mathbf w(t_{k-1}))\Vert$. Therefore, we first derive an adaptive estimation for the divergence of parameters in a distributed manner for every local iteration $t$. Then, we focus on the approach to approximate the global gradient $\Vert\nabla F(\mathbf w(t_{k-1}))\Vert$ at each global iteration $k$. 

\subsection{Estimation of parameter divergence} 
By Definition \ref{paraDiv} and given the conditions in \eqref{cond1} and \eqref{cond2}, since the server needs to determine the learning parameters at the beginning of global aggregation, we are interesting in approximating the prediction of $\epsilon_c(k)$ for $t\in\mathcal T_k$ by $\epsilon_c(k)$ at the beginning of the global aggregation $k$. Therefore, $\epsilon_c(k)$ is approximated as
% \begin{align*}
%     \epsilon_c(k) &\geq \Vert\mathbf w_i(t)-\mathbf w_j(t)\Vert
%     \\&
%     \geq \vert\Vert\mathbf w_i(t)\Vert-\Vert\mathbf w_j(t)\Vert\vert, ~\forall i, j\in\mathcal{S}_c^{(k)}, ~\forall t\in\mathcal T_k
% \end{align*}

\begin{align}
    \epsilon_c(k) &\approx \max_{i, j\in\mathcal{S}_c^{(k)}.}\{\Vert\mathbf w_i(t_k)\Vert-\Vert\mathbf w_j(t_k)\Vert\}
    \\& \label{epsProx}
    =\underbrace{\max_{i\in\mathcal{S}_c^{(k)}}\Vert\mathbf w_i(t_k)\Vert}_{(a)}-\underbrace{\min_{j\in\mathcal{S}_c^{(k)}}\Vert\mathbf w_j(t_k)\Vert}_{(b)}, ~\forall t\in\mathcal T_k,
\end{align}

To obtain (a) and (b) for each cluster, each device in the cluster computes $\Vert\mathbf w_i(t_k)\Vert$ and share it with its neighbors iteratively. During 
each iteration, each device memorizes both the (i) maximum and (ii) minimum values among the exchanged values of $\Vert\mathbf w_i(t_k)\Vert$ such that the approximation of $\epsilon_c(k)$ can be estimated.

\subsection{Approximation of global gradient}
In this section, we propose a two-step method to approximate $\Vert\nabla F(\mathbf w(t_{k-1}))\Vert$. By \eqref{15}, we first approximate $\nabla F(\mathbf w(t_{k-2}))$ as 
$\nabla \widehat{F}(\mathbf w(t_{k-2}))=\mathbf w(t_{k-2})-\mathbf w(t_{k-1})/\eta_{k-2}$.\nm{why?}
Since $F(\cdot)$ is strongly convex, the value $\Vert\nabla F(\mathbf w(t_{k}))\Vert$ is expected to decrease over $k$. Thus, $\Vert\nabla F(\mathbf w(t_{k-1}))\Vert$ can be further approximated by $\Vert\nabla \widehat{F}(\mathbf w(t_{k-1}))\Vert=\alpha\Vert\nabla F(\mathbf w(t_{k-2}))\Vert$, where $0\leq\alpha\leq 1$. The two-step approximation can be summarized as
\begin{align} \label{gradProx}
    \Vert\nabla \widehat{F}(\mathbf w(t_{k-1}))\Vert=\alpha\big(\mathbf w(t_{k-2})-\mathbf w(t_{k-1})\big)/\eta_{k-2}.
\end{align}
\nm{norm of RHS?}
\chris{Reviewers are also going to ask how much the value of $\alpha$ affects the analysis. Is there a way to get creative about how we select $\alpha$, like from sampling?}
% bibtex
\fi

\section{Possible Optimization Formulations}
1) Optimization formulation:
In the first proposed formulation, we aim to optimize the next interval for the local training period at each global aggregation with a look-ahead method. In particular, we try to minimize the total cost consisting of communication energy and delay starting from the beginning of k-th global aggregation (i,e., $t=t_{k-1}$) all the way to the end (i.e., $t=T$) to determine the value of local training period for the current global aggregation period (i.e., $\tau_{k}$). We assume that the value of $\tau_{k}$ is also going to be used for the rest of the training period, i.e., $\tau_{k}=\tau_{k+1}=....$. The intuition behind this is that we want to find the best choice of $\tau_{k}$ that minimizes the overall cost for the remaining training process as the decision for $\tau_{k}$. 

We propose the following optimization  problem:
\begin{align}
   &\min_{\tau_k,\phi,\Gamma,\gamma,\alpha}  c_1 \times \left(\sum_{t=t_{k-1}}^{T}\theta_{c}(t)*E_{D2D} + K*E_{Glob} \right) + c_2\times \left(\sum_{t=t_{k-1}}^{T}\theta_{c}(t)*\Delta_{D2D}+K*\Delta_{Glob}\right) \label{obj}
    \\
   & \textrm{s.t.}\\
   & \theta_c^{(t)}\geq\max\Big\{\log\Big(\eta_t\phi/(s_c\Upsilon_c^{(t)})\Big)/\log\Big(\lambda_c^{(t_{k-1})}\Big),0\Big\} \\
%   &0<\tau_k<\log_4\frac{(\mu\gamma-1)\kappa}{16\omega^2\beta^2\gamma^2} \\
%   &(T+\alpha) \xi>\Gamma\geq \alpha [F(\hat{\mathbf w}(0))] \\
%   &[(\mu\gamma-1)(\Gamma-\beta\phi\gamma/2)-B]/[\frac{\gamma^2\beta}{2}A+\frac{16\omega^2\beta^2\gamma^2(\Gamma-\beta\phi\gamma/2)}{\kappa}]>4 \\
   &0<\tau_k \leq \log_4\left\{[(\mu\gamma-1)(\Gamma-\beta\phi\gamma/2)-B]/[\frac{\gamma^2\beta}{2}A+\frac{16\omega^2\beta^2\gamma^2(\Gamma-\beta\phi\gamma/2)}{\kappa}]\right\} \\
%   &T\zeta/[F(\hat{\mathbf w}(0))-\xi]>\alpha\geq\max\{\beta\gamma/\kappa, \beta\gamma\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\} \\
%   &\Upsilon_c^{(t_k)} = 0\\
%   &\Upsilon_c^{(k)} \leq d(\tau)\Upsilon_c^{(k-1)} \\
    &K=\floor{\frac{T-t_{k-1}}{\tau_k}} \\
    & \Upsilon_c^{(t_{k-1})}=0 \\
    & \Upsilon_c^{(t)} = c(t)\Upsilon_c^{(t-1)}+c'(t),~~t\in \mathcal{T}_k\\
    & \Upsilon_c^{(t_{k+1}+1)} = c''(k)\Upsilon_c^{(t_{k}+1)}, c''(k)<1\\
    &t_{k-1}\leq t\leq T
\end{align}
where $\theta_c^{(t)}$ is the rounds of consensus, $E_{D2D}$ is the energy consumption for each D2D communication round, $E_{Glob}$ is the energy consumption for device-to-server communication, $\Delta_{D2D}$ is the D2D communication delay and $\Delta_{Glob}$ is the device-to-server communication delay. $c_1$ and $c_2$ define the tradeoff between the total energy consumption and the total delay, where $c_1+c_2=1$. At the first glance, from the objective function, it may seem like that choosing a smaller value of $K$ would result in smaller total cost. However, from (53), we see that smaller $K$ would also result in larger $\tau_{k}$ that results in larger value of $\Upsilon_c^{(t)}$ (since the value of $\Upsilon_c^{(t)}$ is reset to $0$ at synchronization and starts to increase with respect to local gradient descents, so larger $\tau_{k}$ results in larger $\Upsilon_c^{(t)}$ and thus larger $\theta_c^{(t)}$). Consequently, smaller value of $K$ does not necessarily provide the best result. 


\textbf{Successive Estimation of Divergence of Parameters ($\Upsilon$)}: rate of increase inside a global aggregation, periodic behavior, rate of decrease in increase in rate between two global aggregation... 
Clearly reveal how this is a function of tau!!!!

1.3 , 5..
1.2 , 6..
1.2 * ... , 6

1.2/1.3



We can see that the above optimization problem is highly non-convex. Thus, we propose to estimate and set a few of the parameters such as $\phi,\Gamma,\alpha,\gamma$ during a first global aggregation and keep tuning the $\tau_k$ for the rest of global aggregations. Note that even fixing all the parameters except $\tau_k$ does not result in a unique $\tau_k$ since the value of $\Upsilon_c^{(t)}$ keeps changing over time and we need to keep tracking and estimating it, which changes the value of $\tau_k$ since it affects the number of D2D rounds $\theta_c^{(t)}$.

Now the logic to solve the optimization problem is as follows: First, the designer determines the value of $\xi$ and $T$ to obtain a desired accuracy of $\xi$ at iteration $T$, this can be mathematically described as:
\begin{equation}\label{42}
    \frac{\Gamma}{T+\alpha} \leq \xi,
\end{equation}
on the other hand we have:
From the range of $\Gamma$ in Theorem \ref{subLin}, the following inequality is immediate:
\begin{align} \label{46}
    \alpha F(\hat{\mathbf w}(0))\leq\Gamma<(T+\alpha) \xi,
\end{align}
where the lower bound comes from the first condition for $\Gamma$ from Theorem \ref{subLin} (i.e.,  $\Gamma\geq\alpha[F(\hat{\mathbf w}(0))-F(\mathbf w^*)]$, assuming that $F(\mathbf w^*)$ is very small and can be neglected). Note that the upper bound is obtained from \eqref{42}.
% from this, we obtain $\Gamma$ and $\alpha$. Logic: larger gamma always tend to reduce the number of consensus. 
From \eqref{46}, we then obtain a upper bound for $\alpha$ as $\alpha<\frac{T\xi}{F(\hat{\mathbf w(0)})-\xi}$. Then based the condition of learning rate described in Theorem \ref{subLin}, we obtain the lower bound of $\alpha$, implying the following range of $\alpha$:
\begin{align} \label{45}
    \frac{T\xi}{F(\hat{\mathbf w}(0))-\xi}>\alpha\geq\max\{\beta\gamma/\kappa, \beta\gamma\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\}.
\end{align}
Since the lower bound of $\alpha$ depends on $\gamma$, to determine the value for $\alpha$, we first determine the value of $\gamma$ such that the range of local training period described by
\begin{align}
    \tau<\log_4\frac{(\mu\gamma-1)\kappa}{16\omega^2\beta^2\gamma^2} 
\end{align}
resides within an desired range (e.g., to have a least 10 global aggregations: $\tau\leq T/10$, which gives us a range for $\gamma$ from which an arbitrary value is chosen). Then after deciding the value of $\gamma$
we choose the value for $\alpha$.  Note that the designer needs to choose $T$ and $\xi$ such that the range in \eqref{45} exists. Up to this point, the designer has chosen the step-size behavior uniquely identified by $\alpha,\gamma$. 
% Also, to be able to have control over the D2D rounds, we need to have the second term in the definition of $\Gamma$ dominating, i.e., otherwise the D2D rounds do not matter and do not help!
 
% Now, assume that you have a large $\alpha$ such that the first term in the definition of $\nu$ is dominating. That, means we are caring just about our initial choice of model parameters. In this case, $\alpha>\frac{\gamma^2\beta}{2}[Q_k A+B]/
%         \{[\mu\gamma-(1+16\omega^2\beta^2\gamma^2 Q_k/\kappa)]F(\hat{ \mathbf{w}}(0)\}$, where 
% We assumed that $F(w^*)$ is very small. The convergence bound in this case, will look like this:
%  \begin{align}
%         \mathbb E\left[F(\hat{\mathbf w}(t))-F(\mathbf w^*)\right] \leq \frac{\alpha F(\hat{ \mathbf{w}}(0))}{t+\alpha},
%     \end{align}
% if we can satisfy~\eqref{42} as follows:
%  \begin{align}
%       \frac{\alpha F(\hat{ \mathbf{w}}(0))+\beta\phi\gamma/2}{T+\alpha} \leq \Xi
%     \end{align}
%     that implies (possibly negative?): 
% \begin{equation}
%     \phi \leq \frac{2[(T\Xi+\alpha(\Xi-F(\hat{ \mathbf{w}}(0)))]}{\beta\gamma}.
% \end{equation}

% Now, assume that you have a upper bound on $\alpha$ such that the second term in the definition of $\nu$ is dominating. That, means we are caring just about our initial choice of model parameters. In this case, $\alpha\leq\frac{\gamma^2\beta}{2}[Q_k A+B]/
%         \{[\mu\gamma-(1+16\omega^2\beta^2\gamma^2 Q_k/\kappa)]F(\hat{ \mathbf{w}}(0)\}$, where we assumed that $F(w^*)$ is very small. The convergence bound in this case, will look like this:
%  \begin{align}
%         \mathbb E\left[F(\hat{\mathbf w}(t))-F(\mathbf w^*)\right] \leq \frac{\alpha F(\hat{ \mathbf{w}}(0))}{t+\alpha},
%     \end{align}
% if we can satisfy~\eqref{42} as follows:
%  \begin{align}
%       \frac{\Gamma}{T+\alpha}=\frac{\frac{\gamma^2\beta}{2}[Q_k A+B]/
%         [\mu\gamma-(1+16\omega^2\beta^2\gamma^2 Q_k/\kappa)]+\beta\phi\gamma/2}{T+\alpha} \leq \Xi
%     \end{align}
    
% Now we revisit~\eqref{42}, we get: 

% \begin{align}
% &\Gamma<{T+\alpha} \Xi\\
% &\Gamma<{T+\frac{\nu +\beta\phi\gamma/2}{F(\hat{ \mathbf{w}}(0))}} \Xi,
% \end{align}
Given $\alpha$, we then choose the value of $\Gamma$ from (56) as large as possible (since that would result in the fewest consensus rounds). Based on this selection of $\Gamma$, we now turn to the consensus tuning parameter and find the range of $\phi$ that satisfies the following:
\begin{align} \label{47}
    [(\mu\gamma-1)(\Gamma-\beta\phi\gamma/2)-B]/[\frac{\gamma^2\beta}{2}A+\frac{16\omega^2\beta^2\gamma^2(\Gamma-\beta\phi\gamma/2)}{\kappa}]>4,
\end{align} 
and allows the value of $\tau_k$ 
    \begin{align} \label{60}
        \tau_k \leq \log_4\left\{[(\mu\gamma-1)(\Gamma-\beta\phi\gamma/2)-B]/[\frac{\gamma^2\beta}{2}A+\frac{16\omega^2\beta^2\gamma^2(\Gamma-\beta\phi\gamma/2)}{\kappa}]\right\}
    \end{align}
to resides within a desirable range. Note that satisfying \eqref{47} and \eqref{60} is equivalent to satisfy the second condition for $\Gamma$ in Theorem \ref{subLin} $\Gamma\geq\frac{\gamma^2\beta}{2}[Q_k A+B]/[\mu\gamma-(1+16\omega^2\beta^2\gamma^2 Q_k/\kappa)]+\beta\phi\gamma/2$.

% want to find some $\tau$ and $\phi$ such that the objective in (48) can be minimized while satisfying the second term in the lower bound of $\Gamma$ as follows:
% \begin{align}
%      &\Gamma\geq \frac{\gamma^2\beta}{2}[Q_k A+B]/[\mu\gamma-(1+16\omega^2\beta^2\gamma^2 Q_k/\kappa)]+\beta\phi\gamma/2
% \end{align}
% can be satisfied.
% which forces $\phi,\tau$ to have an upper bound. Now, intuitively, larger gamma always helps with reducing the consensus rounds, so we choose to select the largest $\Gamma$. Note that $\Gamma$ is a function of $\tau$ and $\phi$. In general, larger $\phi$ makes $\Gamma$ larger and so does larger $\tau$.
% $A=(16\omega^2/\kappa+153/16)\beta\phi+81/16\sigma^2+\delta^2$, $B=\sigma^2+\beta\phi$

% \begin{align}
%     \frac{\gamma^2\beta}{2}[97/4\sigma^2+\delta^2]/
%     \{[\mu\gamma-(1+16\omega^2\beta^2\gamma^2 Q_k/\kappa)]F(\hat{ \mathbf{w}}(0)\}\geq\alpha\geq\max\{\beta\gamma/\kappa, \beta\gamma\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\}
% \end{align}
% \addFL{
% Or equivalently, for the given value of $\Gamma$, we want to find the maximum value of $\tau$ based on a given value of $\phi$.
%     $$
%     [(\mu\gamma-1)(\Gamma-\beta\phi\gamma/2)-B]/[\frac{\gamma^2\beta}{2}A+\frac{16\omega^2\beta^2\gamma^2(\Gamma-\beta\phi\gamma/2)}{\kappa}]>4,
%     $$
%     we obtain the range of $\tau$ as  
%     \begin{align}
%         \tau \leq \log_4\left\{[(\mu\gamma-1)(\Gamma-\beta\phi\gamma/2)-B]/[\frac{\gamma^2\beta}{2}A+\frac{16\omega^2\beta^2\gamma^2(\Gamma-\beta\phi\gamma/2)}{\kappa}]\right\}.
%     \end{align}
% }
Now based on the process described above (with fixed $\Gamma,\gamma,\alpha,T,\xi$ and a given range of $\phi$), we keep changing $\phi$ in the interval obtained from \eqref{47} and \eqref{60} (for example, we can use line search with a certain quantization step) and compute the optimization of \eqref{obj} with convex optimization techniques to obtain the corresponding value of $\tau_k$ for the first round of global aggregation (we relax the (53) and remove the floor operator). This will give us multiple pairs of $\tau_1$ and $\phi$. We choose the pair of $\tau_1,\phi$ that gives us the best result with respect to the objective and fix this value of $\phi$ for the rest of the training.

Note that the dynamics of $\Upsilon_c^{(t)}$ with respect to different values of $\tau$ given some fixed value of $\phi$ will be obtained in the estimation phase.

\iffalse
Formulation 2: In this formulation, the designer also wants to minimize the convergence bound at a given time instance $T$.
\begin{align}
    &\min_{\tau,\phi}  
    c_0 \xi
    +c_1 \times \left(\sum_{t=t_{k-1}}^{T}\theta_{c}(t)*E_{D2D} + K*E_{Glob} \right) 
    +c_2\times \left(\sum_{t=t_{k-1}}^{T}\theta_{c}(t)*\Delta_{D2D}+K*\Delta_{Glob}\right)
    \\
   & \textrm{s.t.}\\
   & \frac{\Gamma}{T+\alpha}\leq\xi \\
   & \theta_c^{(t)}\geq\max\Big\{\log\Big(\eta_t\phi/(s_c\Upsilon_c^{(t)})\Big)/\log\Big(\lambda_c^{(t_{k-1})}\Big),0\Big\} \\
   &0<\tau_k<\log_4\frac{(\mu\gamma-1)\kappa}{16\omega^2\beta^2\gamma^2} \\
   &(T+\alpha) \xi>\Gamma\geq \alpha [F(\hat{\mathbf w}(0))] \\
   &[(\mu\gamma-1)(\Gamma-\beta\phi\gamma/2)-B]/[\frac{\gamma^2\beta}{2}A+\frac{16\omega^2\beta^2\gamma^2(\Gamma-\beta\phi\gamma/2)}{\kappa}]>4 \\
   &\tau_k \leq \log_4\left\{[(\mu\gamma-1)(\Gamma-\beta\phi\gamma/2)-B]/[\frac{\gamma^2\beta}{2}A+\frac{16\omega^2\beta^2\gamma^2(\Gamma-\beta\phi\gamma/2)}{\kappa}]\right\} \\
   &T\zeta/[F(\hat{\mathbf w}(0))-\xi]>\alpha\geq\max\{\beta\gamma/\kappa, \beta\gamma\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\} \\
%   &\Upsilon_c^{(t_k)} = 0\\
%   & \Upsilon_c^{(t)} \leq c\Upsilon_c^{(t-1)}+c',~~t\in \mathcal{T}_k\\
%   &\Upsilon_c^{(k)} \leq d(\tau)\Upsilon_c^{(k-1)} \\
    &K=\floor{\frac{T-t_{k-1}}{\tau}}
    \\
    &t_{k-1}\leq t\leq T
\end{align}
In this formulation, the value of $\xi$ is also one of the optimization variables. Based on the solving technique discussed above, since we have different values of $\tau_k$ for different rounds of global aggregations, we need to recompute the optimization process with different values of $\xi$ and find the corresponding optimal choice of $\tau_k$ repeatedly for every aggregation rounds. However, different decisions on the value of $\xi$ in different global aggregation round correspondingly results in different range for the options of $\alpha$ and $\Gamma$ (can be observed from \eqref{45} and \eqref{46}). And since $\eta_t=\frac{\gamma}{t+\alpha}$ depends on the value of $\alpha$, the learning rate would vibrate up and down across aggregations (instead of steadily decreasing linearly). Another approach is to fix all the variables as described above for a fixed $\xi$ and keep changing the $\xi$ via a line search to obtain the best combination of $(\xi,\tau_1,\phi)$ and then we fix everything except $\tau_k$ and keep recomputing it through the algorithm. 
\fi

% for the case when $\alpha>\frac{\gamma^2\beta}{2}[Q_k A+B]/\{[\mu\gamma-(1+16\omega^2\beta^2\gamma^2 Q_k/\kappa)]F(\hat{ \mathbf{w}}(0)\}$, $\Gamma$ depends on the value of $\phi$, whereas for the case when $\alpha\leq\frac{\gamma^2\beta}{2}[Q_k A+B]/\{[\mu\gamma-(1+16\omega^2\beta^2\gamma^2 Q_k/\kappa)]F(\hat{ \mathbf{w}}(0)\}$, $\Gamma$ depends both on the value of $\phi$ and $\tau$. 

\section{Simulations}
\ali{\begin{itemize}
\item (For all of them calculate the number of consensus according to the proposition)
    \item Not converging. Adding consensus, start to convergence.
    \item With consensus, we can stay in local descent mode for a much longer period of time.
    \item Decreasing the period of local updates (+decreasing/increasing the number of consensus (fix them here)). Increasing the ..... (period of local descents can be the same for all the clusters)
    \item How many nodes? 125--> 
    \item The figures that we have (Two sided figure: comparing not doing consensus with different number of local --- the same local descent for different number of consensus....)
    \item plot that delta during the local descents
    \item plot the step size....
\end{itemize}
}

\pagebreak
\bibliographystyle{IEEEtran}
\bibliography{ref}

\pagebreak
\appendices \label{apxA}
\setcounter{lemma}{0}
\setcounter{proposition}{0}
\setcounter{theorem}{0}
\section{}

\begin{theorem} \label{co1}
Under Assumption \ref{beta} and Algorithm \ref{GT}, for $\eta_k\leq\kappa/\beta$ and $t \in \mathcal T_k$, we have:
% https://www.overleaf.com/project/5f7c9b5ce460a000011dc1e1
\begin{align*} 
    &\mathbb E\left[F(\hat{\mathbf w}(t_k))-F(\mathbf w^*)\right] 
    \nonumber \\&
    \leq  
    \left((1-\mu\eta_k)^{\tau_k}+16\eta_k\omega^2\beta/\kappa\left[\frac{(1-\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\right)[ F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)] 
    \nonumber \\&
    + \frac{\eta_k\beta^2}{2}\left[\frac{(1-\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\{153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
    \nonumber \\&
    +\frac{1-(1-\mu\eta_k)^{\tau_k}}{2}\kappa[\eta_k\sigma^2+\beta\epsilon^2(k)]
    +\frac{\beta}{2}\epsilon^2(k),
\end{align*}
where $\lambda_1 = 1+\eta_k\beta\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]$.
\end{theorem}

\begin{proof}
In this proof, we use the following definition. We define 
the cluster average as


\begin{align}
    \bar{\mathbf w}_c(t)=\frac{1}{s_c^{(k)}}
    \sum\limits_{j\in\mathcal{S}_c^{(k)}}\mathbf w_j(t)
\end{align}
and the global average as
\begin{align}
    \bar{\mathbf w}(t)&
        =\sum_c\varrho_c^{(k)}\bar{\mathbf w}_c(t)
\end{align}

Consider $t \in \mathcal T_k$, using \eqref{8}, \eqref{eq14}, \eqref{15} and the fact that $\sum\limits_{i\in\mathcal{S}_c^{(k)}} e_{i}^{(t)}=0$, %\chris{Isn't $e_i$ indexed by $t$, not $t'$? I guess this is just part of cleaning up the notation.}
the global average follows the following dynamics:
\begin{align}
    \bar{\mathbf w}(t+1)=
    \bar{\mathbf w}(t)
    -\eta_t\sum\limits_{c=1}^N \varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} 
    \nabla F_j(\mathbf w_j(t))
              -\eta_t\sum\limits_{c=1}^N \varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} 
             (\widehat{\mathbf g}_{j,t}-\nabla F_j(\mathbf w_j(t)))
\end{align}

We first bound the sub-optimality gap of the global average. Using $\beta$-smoothness of the global function $F$ and the fact that $\mathbb E_t[\widehat{\mathbf g}_{j,t}]=\nabla F_j(\mathbf w_j(t))$, we can bound it as
\begin{align}
    &\mathbb E_{t}\left[F(\bar{\mathbf w}(t+1))-F(\mathbf w^*)\right]
    \leq F(\bar{\mathbf w}(t))-F(\mathbf w^*)
    \nonumber \\&
    -\eta_t\nabla F(\bar{\mathbf w}(t))^T \sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}}\nabla F_j(\mathbf w_j(t))
    \nonumber \\&
    +\frac{\eta_t^2 \beta}{2}\Big
    \Vert\sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))
    \Big\Vert^2
    \\&
    + \frac{\eta_t^2 \beta}{2}\mathbb E_t\left[
    \Big
    \Vert\sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}}(\widehat{\mathbf g}_j(\mathbf w_j(t))-\nabla F_j(\mathbf w_j(t))
    \Big\Vert^2
    \right].
\end{align}
% \add{where we used the fact that $\mathbb E_t[\widehat{\mathbf g}_{j,t}]=\nabla F_j(\mathbf w_j(t))$}
% \nm{in the last step you used the fact that SGD is unbiased.. please explain}
Since $\mathbb E_t[\Vert(\widehat{\mathbf g}_{j,t}-\nabla F_j(\mathbf w_j(t))\Vert_2^2]\leq \sigma^2$ and gradient noise is independent across devices,
we obtain
\begin{align}
    &\mathbb E_{t}\left[F(\bar{\mathbf w}(t+1))-F(\mathbf w^*)\right]
        \leq F(\bar{\mathbf w}(t))-F(\mathbf w^*)
        \nonumber \\&
        -\eta_t\nabla F(\bar{\mathbf w}(t))^T \sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}}\nabla F_j(\mathbf w_j(t))
        \nonumber \\&
        +\frac{\eta_t^2 \beta}{2}\Big
        \Vert\sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))
        \Big\Vert^2
        +\frac{\eta_t^2 \beta\sigma^2}{2}.
\end{align}
Using Lemma \ref{lem1}, we then obtain
\begin{align}
    &\mathbb E_{t}\left[F(\bar{\mathbf w}(t+1))-F(\mathbf w^*)\right]
    \leq
        (1-\mu\eta_t)(F(\bar{\mathbf w}(t))-F(\mathbf w^*))
        \\&
       -\frac{\eta_t}{2}(1-\eta_t \beta)\Big\Vert\sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))\Big\Vert^2
       +\frac{\eta_t^2 \beta\sigma^2}{2}
    %   \\&
    %     + \eta_k^2 \beta \zeta
    %     \sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \Vert\nabla F_j(\mathbf w_j(t))\Vert^2 
        \\&
        +\frac{\eta_t\beta^2}{2}\sum\limits_{c=1}^N\varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}}\Big\Vert\bar{\mathbf w}(t)-\mathbf w_j(t)\Big\Vert^2.
\end{align}
Noting that $\eta_t\leq\frac{1}{\beta}$, and taking the unconditional expectation, we can write
\begin{align} \label{27}
    &\mathbb E\left[F(\bar{\mathbf w}(t+1))-F(\mathbf w^*)\right]
    \leq
    % \nonumber \\&
    (1-\mu\eta_t)\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]+\frac{\eta_t\beta^2}{2}A(t)+\frac{\eta_t^2 \beta\sigma^2}{2},
    %   \\&
    %     + \eta_k^2 \beta \zeta
    %     \mathbb E\left[\sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \Vert\nabla F_j(\mathbf w_j(t))\Vert^2 \right]
        % \\&
\end{align}
where we have defined the \emph{Local model dispersion } 
\begin{align}
    A(t)=\mathbb E\left[\sum\limits_{c=1}^N\varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}}\Big\Vert\bar{\mathbf w}(t)-\mathbf w_j(t)\Big\Vert^2\right],
\end{align}
showing how much the local solutions deviate from the average solution. 
% \addFL{
By Proposition \ref{lem2}, we bound the \emph{local model dispersion} in \eqref{27} and obtain
\begin{align} \label{26}
        &\mathbb E\left[F(\bar{\mathbf w}(t+1))-F(\mathbf w^*)\right]
        \leq (1-\mu\eta_t)\mathbb E\left[(F(\bar{\mathbf w}(t))-F(\mathbf w^*))\right] 
        \nonumber \\&
        +\frac{\eta_t\beta^2}{2}
        \bigg[ 
        [\lambda_1^{2t}-1]\{32\omega^2/\mu[F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)]+153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}+\epsilon^2(k)
        \bigg]
        +\frac{\eta_t^2 \beta\sigma^2}{2}
        \nonumber \\&
        \leq 
        (1-\mu\eta_t)\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]
        +
        16\omega^2\beta\eta_t/\kappa[\lambda_1^{2t}-1][F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)]
        \nonumber \\&
        +\frac{\eta_t\beta^2}{2}
        [\lambda_1^{2t}-1]\{153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
        +\frac{\eta_t\beta}{2}[\eta_t\sigma^2+\beta\epsilon^2(k)]
\end{align}

% \iffalse
By applying \eqref{26} recursively, we have
\begin{align} \label{30}
    &\mathbb E\left[F(\bar{\mathbf w}(t_k))-F(\mathbf w^*)\right] 
    \nonumber \\&
    \leq  
    \left((1-\mu\eta_k)^{\tau_k}+16\eta_k\omega^2\beta/\kappa\left[\frac{(1-\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\right)[ F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)] 
    \nonumber \\&
    + \frac{\eta_k\beta^2}{2}\left[\frac{(1-\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\{153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
    \nonumber \\&
    +\frac{1-(1-\mu\eta_k)^{\tau_k}}{2\kappa}[\eta_k\sigma^2+\beta\epsilon^2(k)].
    % \nonumber \\&
    % \leq  
    % \left((1-\mu\eta_k)^{\tau_k}+16\eta_k\omega^2\beta/\kappa\left[\frac{(1-\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\right)[ F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)] 
    % \nonumber \\&
    % + \frac{\eta_k\beta^2}{2}\left[\frac{(1+\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\{153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
    % \nonumber \\&
    % +\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\cdot\frac{\eta_k\beta}{2}[\eta_k\sigma^2+\beta\epsilon(k)].
\end{align}
% \fi
By $\beta$-smoothness of $F(\cdot)$, we have
\begin{align} \label{39}
    &\mathbb E\left[F(\hat{\mathbf w}(t))-F(\mathbf w^*)\right]  
    \leq \mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]
    \nonumber \\&
    +\mathbb E\left[\nabla F(\bar{\mathbf w}(t))^T\Big(\hat{\mathbf w}(t)-\bar{\mathbf w}(t)\Big)\right]+\frac{\beta}{2}\mathbb E\left[\Vert\hat{\mathbf w}(t)-\bar{\mathbf w}(t)\Vert^2\right]
    \nonumber \\&
    \leq \mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]+\nabla F(\bar{\mathbf w}(t))^T\sum\limits_{c=1}^N\varrho_c^{(k)} \mathbb E\left[e_{s_c^{(k)}}^{(t)}\right]
    +\frac{\beta}{2}\sum\limits_{c=1}^N\varrho_c^{(k)}\mathbb E\left[\Vert e_{s_c^{(k)}}^{(t)}\Vert^2\right]
    \nonumber \\&
    \leq \mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]
    +\frac{\beta}{2}\epsilon^2(k).
\end{align}
% \iffalse
Combine \eqref{30} and \eqref{39}, we have  
\begin{align} \label{32}
    &\mathbb E\left[F(\hat{\mathbf w}(t_k))-F(\mathbf w^*)\right] 
    \nonumber \\&
    \leq  
    \left((1-\mu\eta_k)^{\tau_k}+16\eta_k\omega^2\beta/\kappa\left[\frac{(1-\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\right)[ F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)] 
    \nonumber \\&
    + \frac{\eta_k\beta^2}{2}\left[\frac{(1-\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\{153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
    \nonumber \\&
    +\frac{1-(1-\mu\eta_k)^{\tau_k}}{2\kappa}[\eta_k\sigma^2+\beta\epsilon^2(k)]
    +\frac{\beta}{2}\epsilon^2(k).
\end{align}
The derivative of \eqref{32} with respect to $\eta_k=0$ is $$-\mu\tau_k[ F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)]\leq 0.$$
% }
% \fi
\end{proof}
% \addFL{
\begin{theorem} \label{subLin}
    Under Assumption \ref{beta} and any realization of {\tt TT-HF}, for $\gamma>1/\mu$, $\eta_t=\frac{\gamma}{t+\alpha}$, $\epsilon^2(t)=\eta_t\phi$, 
    % $\eta_0\leq \min \Big\{\kappa/\beta, 1/\left(\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\beta\right)\Big\}$ 
    and
    $\tau_k<\log_4\frac{(\mu\gamma-1)\kappa}{16\omega^2\beta^2\gamma^2}$, where 
    $\alpha\geq\max\{\beta\gamma/\kappa, \beta\gamma\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\},
    $ we have
    \begin{align}
        \mathbb E\left[F(\hat{\mathbf w}(t))-F(\mathbf w^*)\right] \leq \frac{\Gamma}{t+\alpha},
    \end{align}
    where $\Gamma\geq\max\bigg\{\alpha[F(\hat{\mathbf w}(0))-F(\mathbf w^*)],\frac{\gamma^2\beta}{2}[Q_k A+B]/[\mu\gamma-(1+16\omega^2\beta^2\gamma^2 Q_k/\kappa)]+\beta\phi\gamma/2\bigg\}$, $A=(16\omega^2/\kappa+153/16)\beta\phi+81/16\sigma^2+\delta^2$, $B=\sigma^2+\beta\phi$ and $Q_k=4^{\tau_k}$.
\end{theorem}
\begin{proof}
    In this proof, we first find the condition such that $\mathbb E\left[F(\hat{\mathbf w}(t))-F(\mathbf w^*)\right]\leq\frac{\hat{\nu}}{t+\alpha}$ for $t\in\mathcal T_k$. Then we derive the condition for $\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]\leq\frac{\hat{\nu}}{t+\alpha}$ to hold for $t=0,1,\dots,T$. Finally, we prove the result for the Theorem. \\
    To demonstrate sub-linear convergence in each aggregation period with respect to $t$, we prove that $\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]\leq\frac{\hat{\nu}}{t+\alpha}$ for some $\hat{\nu}$ at $t\in\mathcal T_k$ by induction.\\
    Let $\hat{\nu}\geq\max\{\nu_0,\nu\}$, where $\nu_0=\alpha[F(\hat{\mathbf w}(0))-F(\mathbf w^*)]$ and $\nu=\frac{\gamma^2\beta}{2}[Q_k A+B]/[\mu\gamma-(1+16\omega^2\beta^2\gamma^2 Q_k/\kappa)]$\nm{implies that $Q_k$ is constant!}.
    From \eqref{26} and \eqref{39}, we have   
    \begin{align} \label{wbar}
        &\mathbb E\left[F(\bar{\mathbf w}(t+1))-F(\mathbf w^*)\right]
        \nonumber \\&
        \leq 
        (1-\mu\eta_t)\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]
        +
        16\omega^2\beta\eta_t/\kappa[\lambda_1^{2(t-t_{k-1})}-1][F(\bar{\mathbf w}(t_{k-1}))-F(\mathbf w^*)+\frac{\beta}{2}\epsilon^2(t)]
        \nonumber \\&
        +\frac{\eta_t\beta^2}{2}
        [\lambda_1^{2(t-t_{k-1})}-1]\{153/16\epsilon^2(t)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
        +\frac{\eta_t\beta}{2}[\eta_t\sigma^2+\beta\epsilon^2(t)].
    \end{align}
    \nm{please be careful with t k indices!}
    It can observed from the definition of $\nu$ that $\mathbb E\left[F(\bar{\mathbf w}(m))-F(\mathbf w^*)\right]\leq\frac{\nu_0}{m+\alpha}$ holds for $m=0$. 
    We then assume $\mathbb E\left[F(\bar{\mathbf w}(m))-F(\mathbf w^*)\right]\leq\frac{\nu}{m+\alpha}$ holds for $m=t$, and want to show that $\mathbb E\left[F(\bar{\mathbf w}(m))-F(\mathbf w^*)\right]\leq\frac{\nu}{m+\alpha}$ holds for $m=t+1$. 
    \add{Assuming by the induction hypothesis that it holds for...}
    From \eqref{wbar}, we have
    \begin{align} 
        &\mathbb E\left[F(\bar{\mathbf w}(t+1))-F(\mathbf w^*)\right]
        \nonumber \\&
        \leq 
        (1-\mu\eta_t)\frac{\nu}{t+\alpha}
        +
        16\omega^2\beta\eta_t/\kappa[\lambda_1^{2(t-t_{k-1})}-1][\frac{\nu}{t_{k-1}+\alpha}+\frac{\beta}{2}\epsilon^2(t)]
        \nonumber \\&
        +\frac{\eta_t\beta^2}{2}
        [\lambda_1^{2(t-t_{k-1})}-1]\{153/16\epsilon^2(t)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
        +\frac{\eta_t\beta}{2}[\eta_t\sigma^2+\beta\epsilon^2(t)]
    \end{align}
    Since $\eta_t=\frac{\gamma}{t+\alpha}$, we have 
    $$
    &\eta_t\leq\eta_k\leq C_k\eta_t\Rightarrow\frac{\gamma}{t_k+\alpha}\leq C_k\frac{\gamma}{t_k+\tau_k+\alpha}\Rightarrow\frac{\tau_k}{t_k+\alpha}+1\leq C_k
    $$
    Since $\lambda_1^{2(t-t_{k-1})}-1\leq\eta_t\beta Q_k$\nm{how did you get this? looks wrong to me} \frank{we assume that $\eta_0\leq1/\big(\beta[1-\kappa/4+\sqrt{(1-\kappa/4)^2+2\omega}]\big)$ such that $\eta_t\beta[1-\kappa/4+\sqrt{(1-\kappa/4)^2+2\omega}]\leq 1$}, for some constant $Q_k=4^{\tau_k}$, we can further upper bound
    \begin{align}
        &\mathbb E\left[F(\bar{\mathbf w}(t+1))-F(\mathbf w^*)\right]
        \nonumber \\&
        \leq 
        (1-\mu\eta_t)\frac{\nu}{t+\alpha}
        +
        16\omega^2\beta^2\eta_t^2Q_k/\kappa[\frac{\nu}{t_{k-1}+\alpha}+\frac{\beta}{2}\epsilon^2(t)]
        \nonumber \\&
        +\frac{\eta_t^2\beta^3}{2}
        C_k Q_k\{153/16\epsilon^2(t)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
        +\frac{\eta_t\beta}{2}[\eta_t\sigma^2+\beta\epsilon^2(t)]
        \nonumber \\&
        \overset{(a)}{\leq}
        (1-\mu\eta_t)\frac{\nu}{t+\alpha}
        + 
        16\omega^2\beta^2\eta_t^2Q_k/\kappa[\frac{\nu}{t_{k-1}+\alpha}+\frac{\beta}{2}\eta_t\phi]
        \nonumber \\&
        +\frac{\eta_t^2\beta^3}{2}
        C_k Q_k\{153/16\eta_t\phi+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
        +\frac{\eta_t\beta}{2}[\eta_t\sigma^2+\eta_t\beta\phi]
        \nonumber \\&
        \overset{(b)}{\leq}
        (1-\mu\eta_t)\frac{\nu}{t+\alpha}
        +
        16\omega^2\beta^2\eta_t^2Q_k\frac{\nu}{\kappa(t_{k-1}+\alpha)}
        \nonumber \\&
        +\frac{\eta_t^2\beta}{2}
        \left[C_k Q_k\big((16\omega^2/\kappa+153/16)\beta\phi+81/16\sigma^2+\delta^2\big)
        +(\sigma^2+\beta\phi)\right]
        \nonumber \\&
        \overset{(c)}{\leq}
        (1-\frac{\gamma_k}{t+\alpha}\mu)\frac{\nu}{t+\alpha}
        +
        16\omega^2\beta^2C_k Q_k\frac{\gamma_k^2}{(t+\alpha)^2}\frac{\nu}{\kappa(t_{k-1}+\alpha)}
        \nonumber \\&
        +\frac{\gamma_k^2\beta}{2(t+\alpha)^2}
        \left[C_k Q_k\big((16\omega^2/\kappa+153/16)\beta\phi+81/16\sigma^2+\delta^2\big)
        +(\sigma^2+\beta\phi)\right]
        \nonumber \\&
        =
        \frac{t+\alpha-1}{(t+\alpha)^2}\nu-\frac{\mu\gamma_k-\big[1+16\omega^2\beta^2\gamma_k^2 C_k Q_k/[\kappa(t_{k-1}+\alpha)]\big]}{(t+\alpha)^2}\nu
        \nonumber \\&
        +\frac{\gamma_k^2\beta}{2(t+\alpha)^2}
        \left[C_k Q_k\big((16\omega^2/\kappa+153/16)\beta\phi+81/16\sigma^2+\delta^2\big)
        +(\sigma^2+\beta\phi)\right]
        \nonumber \\&
        \leq
        \frac{t+\alpha-1}{(t+\alpha)^2}\nu-\frac{\mu\gamma-\big[1+16\omega^2\beta^2\gamma_k^2 C_k Q_k/\kappa\big]}{(t+\alpha)^2}\nu
        +\frac{\gamma_k^2\beta}{2(t+\alpha)^2}
        \left[C_k Q_k A+B\right],
    \end{align} 
    where $A=(16\omega^2/\kappa+153/16)\beta\phi+81/16\sigma^2+\delta^2$ and $B=\sigma^2+\beta\phi$. $(a)$ holds because $\epsilon^2(t)=\eta_t\phi$, $(b)$ comes from $\eta_0\leq\kappa/\beta$ and $(c)$ comes from $\eta_t=\frac{\gamma}{t+\alpha}$.\\
    Now, we want to find the condition such that 
    \begin{align} \label{sub_t+1}
        &\mathbb E\left[F(\bar{\mathbf w}(t+1))-F(\mathbf w^*)\right]
        \leq
        \frac{t+\alpha-1}{(t+\alpha)^2}\nu
        \leq
        \frac{\nu}{t+\alpha+1}
    \end{align}
    holds. 
    Let
    \begin{align*}
         &A_k(\gamma)=1+16\omega^2\beta^2\gamma^2 Q_k/\kappa-\mu\gamma,
    \end{align*}
    then the range of $\tau$ for $A_k<0$ is, for some $\gamma>1/\mu$,
    \begin{align} \label{gammaRange}
        \tau_k<\log_4\frac{(\mu\gamma-1)\kappa}{16\omega^2\beta^2\gamma^2}.
    \end{align}
    To obtain the condition for $\nu$ such that \eqref{sub_t+1} holds, we investigate the condition for $A_k\nu+\frac{\gamma^2\beta}{2}\left[Q_k A+B\right]\leq0$ and obtain that 
    \begin{align}
        &\nu 
        \geq 
        \frac{\gamma^2\beta}{2}[Q_k A+B]
        /
        [\mu\gamma-(1+16\omega^2\beta^2\gamma^2 Q_k/\kappa)],~\forall k.
    \end{align}
    Or equivalently, for some given value of 
    $$
    [(\mu\gamma-1)\nu-B]/[\frac{\gamma^2\beta}{2}A+\frac{16\omega^2\beta^2\gamma^2\nu}{\kappa}]>4,
    $$
    we obtain the range of $\tau_k$ as  
    \begin{align}
        \tau_k \leq \log_4\left\{[(\mu\gamma-1)\nu-B]/[\frac{\gamma^2\beta}{2}A+\frac{16\omega^2\beta^2\gamma^2\nu}{\kappa}]\right\}.
    \end{align}
    To further satisfy the initial condition of for $\eta_0\leq\min\{\kappa/\beta,1/\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\beta\}$, for any $\gamma$ chosen in the range of \eqref{gammaRange}, the range of $\alpha$ is given as 
    \begin{align}
        \alpha \geq \max\{\beta\gamma/\kappa, \beta\gamma\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\}.
    \end{align}
    \iffalse
    Let 
    \begin{align*}
         &A_k=[1+16\omega^2\beta^2\gamma^2 Q_k/\kappa-\mu\gamma]\nu
            % \nonumber \\&
            +\frac{\gamma^2\beta}{2}
            \left[Q_k A+B\right]
            \nonumber \\&
            =\left[16\omega^2\beta^2Q_k\nu/\kappa+\beta(Q_k A+B)/2\right]\gamma^2-\mu\nu\gamma_k+\nu,
    \end{align*}
    then the range of $\gamma_k$ for $A_k<0$ is
    \begin{align} \label{range_gam}
        &\frac{\mu\nu-\sqrt{\mu^2\nu^2-4\left[16\omega^2\beta^2Q_k\nu/\kappa+\beta(Q_k A+B)/2\right]\nu}}{2\left[16\omega^2\beta^2Q_k\nu/\kappa+\beta(Q_k A+B)/2\right]}
        <\gamma_k, 
        \nonumber \\&
        \gamma_k
        < \frac{\mu\nu+\sqrt{\mu^2\nu^2-4\left[16\omega^2\beta^2Q_k\nu/\kappa+\beta(Q_k A+B)/2\right]\nu}}{2\left[16\omega^2\beta^2Q_k\nu/\kappa+\beta(Q_k A+B)/2\right]}.
    \end{align}
    To make $A_k <0$, we need to satisfy the following
    \begin{align} \label{condA_k}
        &\Delta=\mu^2\nu^2-4\left[16\omega^2\beta^2Q_k\nu/\kappa+\beta(Q_k A+B)/2\right]\nu
        \nonumber \\&
        =\nu\left(\left[\mu^2-64\omega^2\beta^2Q_k/\kappa\right]\nu-2\beta(Q_k A+B)\right)
        >0,
    \end{align}
    which indicates that for some 
    \begin{align} \label{cond_tau}
        \tau_k<\log_4\frac{\kappa^3}{64\omega^2}.
    \end{align}
    we need to choose some $\nu$ based on
    \begin{align} \label{cond_nu}
        \nu > 2\beta(Q_k A+B)/\left[\mu^2-64\omega^2\beta^2Q_k/\kappa\right]
    \end{align}
    to guarantee sub-linear convergence of $\bar{\mathbf w}(t)$.\\
    To further satisfy the initial condition of for $\eta_0\leq\min\{\kappa/\beta,1/\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\beta\}$, for any $\gamma_k$ chosen in the range of \eqref{range_gam}, the range of $\alpha$ is given as 
    \begin{align}
        \alpha \geq \max\{\beta\gamma_k/\kappa, \beta\gamma_k\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]\}.
    \end{align}
    Finally, combining the two cases and generalize the condition to all $k$, if 
    $$
    \log_4 \frac{\kappa}{32\omega^2\beta}\leq\tau_k<\log_4\frac{\alpha\kappa^3}{64\omega^2},~\forall k
    $$ 
    holds, then for some $\alpha>\max\{\frac{2}{\beta\kappa^2},\frac{1}{\kappa^2}\}$, if
    $$
    \max\{1/\mu,\frac{2}{\kappa\beta^2}\}\leq\gamma<\frac{\alpha\kappa}{\beta},
    $$
    and if
    \begin{align}
        \nu\geq\max\{\frac{\alpha\kappa^2\beta\left[\big((16\omega^2/\kappa+153/16)\phi^2+81/16\sigma^2+\delta^2\big)+32\omega^2\beta(\sigma^2+\phi^2)/\kappa\right]}{64\omega^2\beta^2\left[\mu-(1+\frac{1}{2}\kappa)\right]},
        \nonumber \\
        \frac{\alpha\kappa^2\beta\left[\big(\alpha\kappa^3(16\omega^2/\kappa+153/16)\phi^2/64\omega^2+81/16\sigma^2+\delta^2\big)+(\sigma^2+\phi^2)\right]}{2\left(\mu-1\right)}\},
    \end{align}
    \fi
    which then results in
    \begin{align} \label{wbar_pfd}
        &\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]
        \leq
        \frac{\nu}{t+\alpha},~\forall t=0,1,\dots,T.
    \end{align}
    
    Now, we want to prove $\mathbb E\left[F(\hat{\mathbf w}(t))-F(\mathbf w^*)\right]\leq\frac{\Gamma_1}{t+\alpha}$ based on the result in \eqref{wbar_pfd}. 
    For $t=0$, we have $\mathbb E\left[F(\hat{\mathbf w}(0))-F(\mathbf w^*)\right] \leq \frac{\Gamma_1}{\alpha}$ holds for $\Gamma_1=\nu_0$.
    
    For $t>0$, from \eqref{wbar_pfd} and \eqref{39}, we have
    \begin{align}
        &\mathbb E\left[F(\hat{\mathbf w}(t))-F(\mathbf w^*)\right]  
        \leq \mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]
        +\frac{\beta}{2}\epsilon^2(t)
        \nonumber \\&
        \leq \frac{\nu}{t+\alpha} +\frac{\beta\epsilon^2(t)}{2}
        \leq \frac{\nu}{t+\alpha} +\frac{\beta\eta_t\phi}{2}
        \leq \frac{\nu}{t+\alpha} +\frac{\eta_t\beta\phi}{2}.
    \end{align}
    Choose $\eta_t=\frac{\gamma}{t+\alpha}$, we then have
    \begin{align}
        &\mathbb E\left[F(\hat{\mathbf w}(t))-F(\mathbf w^*)\right]  
        \leq \frac{\nu}{t+\alpha} +\frac{\beta\phi\gamma}{2(t+\alpha)}=\frac{\Gamma_2}{t+\alpha},
    \end{align}
    where $\Gamma_2\geq\nu+\beta\phi\gamma/2$. Therefore, we show that, for $\Gamma\geq\max\{\Gamma_1,\Gamma_2\}$, we have 
    \begin{align}
        &\mathbb E\left[F(\hat{\mathbf w}(t))-F(\mathbf w^*)\right]  
        \leq \frac{\Gamma}{t+\alpha}.
    \end{align}
\end{proof}
%%%%%%%%%%%%%%%%%%%%%
\iffalse
\begin{theorem}
    Under Assumption \ref{beta} and Algorithm \ref{GT}, for $\eta_k\leq\kappa/\beta$, when 
    \begin{align*}
        &\frac{\eta_k\beta^2}{2}\left[\frac{(1+\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\{153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
        \nonumber \\&
        +\frac{1-(1-\mu\eta_k)^{\tau_k}}{2}\kappa[\eta_k\sigma^2+\beta\epsilon^2(k)]
        +\frac{\beta}{2}\epsilon^2(k)
        \nonumber \\&
        \leq \frac{\mu}{2\beta^2}\left(1-\vartheta-\Big((1-\mu\eta_k)^{\tau_k}+16\eta_k\omega^2\beta/\kappa\left[\frac{(1-\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\Big)\right)\Vert\nabla F(\hat{\mathbf w}(k))\Vert^2,
    \end{align*}
    where $0<\vartheta\leq 1-\Big((1-\mu\eta_k)^{\tau_k}+16\eta_k\omega^2\beta/\kappa\left[\frac{(1-\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\Big)$ holds, we have
    \begin{align*}
        &\mathbb E\left[F(\hat{\mathbf w}(t_k))-F(\mathbf w^*)\right] 
        \leq  
        \left(1-\vartheta\right)[ F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)] 
    \end{align*}
\end{theorem}
\begin{proof}
    From \eqref{32}, we have
    \begin{align}
        &\mathbb E\left[F(\hat{\mathbf w}(t_k))-F(\mathbf w^*)\right] 
        \nonumber \\&
        \leq  
        \left((1-\mu\eta_k)^{\tau_k}+16\eta_k\omega^2\beta/\kappa\left[\frac{(1-\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\right)[ F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)] 
        \nonumber \\&
        + \frac{\eta_k\beta^2}{2}\left[\frac{(1+\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\{153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
        \nonumber \\&
        +\frac{1-(1-\mu\eta_k)^{\tau_k}}{2}\kappa[\eta_k\sigma^2+\beta\epsilon^2(k)]
        +\frac{\beta}{2}\epsilon^2(k).
    \end{align}
    If the following condition holds for all $k$
    \begin{align*}
        &\frac{\eta_k\beta^2}{2}\left[\frac{(1+\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\{153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
        \nonumber \\&
        +\frac{1-(1-\mu\eta_k)^{\tau_k}}{2}\kappa[\eta_k\sigma^2+\beta\epsilon^2(k)]
        +\frac{\beta}{2}\epsilon^2(k)
        \nonumber \\&
        \leq \frac{\mu}{2\beta^2}\left(1-\vartheta-\Big((1-\mu\eta_k)^{\tau_k}+16\eta_k\omega^2\beta/\kappa\left[\frac{(1-\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\Big)\right)\Vert\nabla F(\hat{\mathbf w}(k))\Vert^2,
    \end{align*}
    where $0<\vartheta\leq 1-\Big((1-\mu\eta_k)^{\tau_k}+16\eta_k\omega^2\beta/\kappa\left[\frac{(1-\mu\eta_k)^{\tau_k}\lambda_1^{2\tau_k}-1}{(1-\mu\eta_k)\lambda_1^2-1}-\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\right]\Big)$, then the following relationship
    \begin{align}
        &\mathbb E\left[F(\hat{\mathbf w}(t_k))-F(\mathbf w^*)\right] 
        % \nonumber \\&
        \leq  
        \left(1-\vartheta\right)[ F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)] 
    \end{align}
    holds.
    % In this proof, we first find the condition such that $\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]\leq(1-\vartheta)\mathbb E\left[F(\bar{\mathbf w}(t-1))-F(\mathbf w^*)\right]$ hold for $t=1,2,\dots,T$. Finally, we prove the result for this Proposition. \\
    % From \eqref{26} and \eqref{39}, we have   
    % \begin{align} \label{wbar2}
    %     &\mathbb E\left[F(\bar{\mathbf w}(t+1))-F(\mathbf w^*)\right]
    %     \nonumber \\&
    %     \leq 
    %     (1-\mu\eta_t)\mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]
    %     +
    %     16\omega^2\beta\eta_t/\kappa[\lambda_1^{2(t-t_{k-1})}-1][F(\bar{\mathbf w}(t_{k-1}))-F(\mathbf w^*)+\frac{\beta}{2}\epsilon^2(k)]
    %     \nonumber \\&
    %     +\frac{\eta_t\beta^2}{2}
    %     [\lambda_1^{2(t-t_{k-1})}-1]\{153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
    %     +\frac{\eta_t\beta}{2}[\eta_t\sigma^2+\beta\epsilon^2(k)].
    % \end{align}
    % If for all $k$
    % \begin{align*}
    %     &16\omega^2\beta\eta_t/\kappa[\lambda_1^{2(t-t_{k-1})}-1][F(\bar{\mathbf w}(t_{k-1}))-F(\mathbf w^*)+\frac{\beta}{2}\epsilon^2(k)]
    %     \nonumber \\&
    %     +\frac{\eta_t\beta^2}{2}
    %     [\lambda_1^{2(t-t_{k-1})}-1]\{153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
    %     +\frac{\eta_t\beta}{2}[\eta_t\sigma^2+\beta\epsilon^2(k)]
    %     \nonumber \\&
    %     \leq \frac{\mu}{2\beta^2}(\mu\eta_k-\vartheta)\Vert\nabla F(\bar{\mathbf w}(t))\Vert^2,
    % \end{align*}
    % holds for all $k$,
    % then we show that, for $t=1,2,\dots,T$, we have
    % \begin{align}
    %     \mathbb E\left[F(\bar{\mathbf w}(t))-F(\mathbf w^*)\right]\leq(1-\vartheta)\mathbb E\left[F(\bar{\mathbf w}(t-1))-F(\mathbf w^*)\right].
    % \end{align}
\end{proof}
\fi
% \add{
% Or we can also upper bound \eqref{26} as 
% \begin{align} 
%         &\mathbb E\left[F(\bar{\mathbf w}(t+1))-F(\mathbf w^*)\right]
%         \leq 
%         (1-\mu\eta_k)\mathbb E\left[(F(\bar{\mathbf w}(t))-F(\mathbf w^*))\right]
%         +2\cdot16^{\tau_k}\frac{\eta_k\beta^2\omega^2}{\mu}[F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)]
%         \nonumber \\&
%         +\frac{\eta_k\beta^2}{2}\bigg[ 
%         \frac{16^{\tau_k}}{4}\phi \big(\eta_k\sigma^2+2\eta_k^2[\delta+2\sigma+2\beta\sum\limits_{c=1}^N\varrho_{c}\epsilon_c(k)]^2\big)
%         +\sum\limits_{c=1}^N\varrho_{c}\epsilon_c^2(k)\bigg]
%         +\frac{\eta_k^2 \beta\sigma^2}{2}
% \end{align}
% By applying \eqref{26} recursively, we have
% \begin{align} 
%     &\mathbb E\left[F(\bar{\mathbf w}(t_k))-F(\mathbf w^*)\right] 
%     \nonumber \\&
%     \leq  
%     \left((1-\mu\eta_k)^{\tau_k}+\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu^2}\cdot2\cdot16^{\tau_k}\beta^2\omega^2\right)[ F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)] 
%     \nonumber \\&
%     +\frac{1-(1-\mu\eta_k)^{\tau_k}}{2\mu}\beta\Big(\frac{16^{\tau_k}}{4}\phi\beta \big(\eta_k\sigma^2+2\eta_k^2[\delta+2\sigma+2\beta\sum\limits_{c=1}^N\varrho_{c}\epsilon_c(k)]^2\big)+\beta\sum\limits_{c=1}^N\varrho_{c}\epsilon_c(k)^2+\eta_k\sigma^2\Big).
% \end{align}
% }


\iffalse
\addFL{
\begin{proposition} \label{converge}
    Under Assumption \ref{beta}, Definition \ref{gradDiv} and Algorithm \ref{GT}, for $\eta_k\leq\frac{1}{\beta}$ and $t\in\mathcal T_k$, suppose the following condition is satisfied, 
    \begin{align} \label{condition}
        &\frac{\eta_k\beta^2}{2}\bigg[\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\Big[(\frac{1+\beta\eta_k}{\beta\eta_k})\sum\limits_{c=1}^N\varrho_{c}\epsilon_c(k)+\frac{\eta_k\sigma^2}{\beta}\Big]
        \nonumber \\&
        + 8\psi(\eta_k^2[\delta+2\sigma+2\beta\sum\limits_{c=1}^N\varrho_{c}\epsilon_c(k)]^2+\eta_k\sigma^2)
        \left(\frac{1}{\eta_k^2\beta^2(3+\sqrt{1+4\omega})^2}\frac{1-[\frac{(\eta_k\beta(3+\sqrt{1+4\omega})+2)^2}{4(1-\mu\eta_k)}]^{\tau_k}}{1-[\frac{(\eta_k\beta(3+\sqrt{1+4\omega})+2)^2}{4(1-\mu\eta_k)}]}\right)\bigg]
        \nonumber \\&
        \leq \frac{\mu}{2\beta^2}\left[1-\lambda_k-\left((1-\mu\eta_k)^{\tau_k}+\frac{\eta_k\psi\beta^2\omega^2}{4\mu(1+4\omega)(1-\mu\eta_k)}\frac{1-[\frac{(\eta_k\beta(3+\sqrt{1+4\omega})+2)^2}{4(1-\mu\eta_k)}]^{\tau_k}}{1-[\frac{(\eta_k\beta(3+\sqrt{1+4\omega})+2)^2}{4(1-\mu\eta_k)}]}\right)\right]\big\Vert\nabla F(\hat{\mathbf w}(t_{k-1}))\big\Vert^2
    \end{align}
    Algorithm \ref{GT} is guaranteed to converge to the optimum:
    \begin{align*}
        &\mathbb E[F(\hat{\mathbf w}(t_{k}))-F(\mathbf w^*)]
        \leq (1-\lambda_k)[F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)]
    \end{align*}
    for $0<\lambda_k\leq 1-\left((1-\mu\eta_k)^{\tau_k}+\frac{\eta_k\psi\beta^2\omega^2}{4\mu(1+4\omega)(1-\mu\eta_k)}\frac{1-[\frac{(\eta_k\beta(3+\sqrt{1+4\omega})+2)^2}{4(1-\mu\eta_k)}]^{\tau_k}}{1-[\frac{(\eta_k\beta(3+\sqrt{1+4\omega})+2)^2}{4(1-\mu\eta_k)}]}\right)$.
\end{proposition}
% \begin{align}
%     E[F^k-F^*] \leq \Pi_{k=0}^{K} (1-\lambda_k) [Ft_0 -f^*]
%     \rightarrow linear\\ convergence--> E[F^k-is F^*] \leq \theta^k [Ft_0 -f^*]\\
%     --->\theta=\max_{k} (1-\lambda_k)
% \end{align}
\begin{proof}
    By $\beta$-smoothness and $\mu$-strong convexity of $F(\cdot)$, we have
    \begin{align} \label{145}
        \big\Vert \nabla F(\hat{\mathbf w}(t_{k-1}))\big\Vert^2
        \leq\frac{2\beta^2}{\mu}[F(\hat{\mathbf w}(t_{k-1}))- F(\mathbf w^*)].
    \end{align}
    Combining \eqref{145} with \eqref{condition}, we have
    \begin{align}
         &\frac{\eta_k\beta^2}{2}\bigg[\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\Big[(\frac{1+\beta\eta_k}{\beta\eta_k})\sum\limits_{c=1}^N\varrho_{c}\epsilon_c(k)+\frac{\eta_k\sigma^2}{\beta}\Big]
        \nonumber \\&
        + 8\psi(\eta_k^2[\delta+2\sigma+2\beta\sum\limits_{c=1}^N\varrho_{c}\epsilon_c(k)]^2+\eta_k\sigma^2)
        \left(\frac{1}{\eta_k^2\beta^2(3+\sqrt{1+4\omega})^2}\frac{1-[\frac{(\eta_k\beta(3+\sqrt{1+4\omega})+2)^2}{4(1-\mu\eta_k)}]^{\tau_k}}{1-[\frac{(\eta_k\beta(3+\sqrt{1+4\omega})+2)^2}{4(1-\mu\eta_k)}]}\right)\bigg]
        \nonumber \\&
        \leq \left[1-\lambda_k-\left((1-\mu\eta_k)^{\tau_k}+\frac{\eta_k\psi\beta^2\omega^2}{4\mu(1+4\omega)(1-\mu\eta_k)}\frac{1-[\frac{(\eta_k\beta(3+\sqrt{1+4\omega})+2)^2}{4(1-\mu\eta_k)}]^{\tau_k}}{1-[\frac{(\eta_k\beta(3+\sqrt{1+4\omega})+2)^2}{4(1-\mu\eta_k)}]}\right)\right][F(\hat{\mathbf w}(t_{k-1}))- F(\mathbf w^*)],
    \end{align}
    such that the main result in Theorem \ref{co1} becomes
    \begin{align}
        &\mathbb E\left[F(\hat{\mathbf w}(t_k))-F(\mathbf w^*)\right] 
    \nonumber \\&
    \leq  
    \left((1-\mu\eta_k)^{\tau_k}+\frac{\eta_k\psi\beta^2\omega^2}{4\mu(1+4\omega)(1-\mu\eta_k)}\frac{1-[\frac{(\eta_k\beta(3+\sqrt{1+4\omega})+2)^2}{4(1-\mu\eta_k)}]^{\tau_k}}{1-[\frac{(\eta_k\beta(3+\sqrt{1+4\omega})+2)^2}{4(1-\mu\eta_k)}]}\right)[ F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)] 
    \nonumber \\&
    \left[1-\lambda_k-\left((1-\mu\eta_k)^{\tau_k}+\frac{\eta_k\psi\beta^2\omega^2}{4\mu(1+4\omega)(1-\mu\eta_k)}\frac{1-[\frac{(\eta_k\beta(3+\sqrt{1+4\omega})+2)^2}{4(1-\mu\eta_k)}]^{\tau_k}}{1-[\frac{(\eta_k\beta(3+\sqrt{1+4\omega})+2)^2}{4(1-\mu\eta_k)}]}\right)\right][F(\hat{\mathbf w}(t_{k-1}))- F(\mathbf w^*)].
    \nonumber \\&
    = (1-\lambda_k)\Big(F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)\Big).
    \end{align}
\end{proof}
}
\fi
% \subsection{Detail proof of Theorem \ref{co1}}

% % \nm{please use $\mathbf w_{c,i}(t)$ for node i in cluster c,
% % $F_{c,i}$ for local function
% % $F_c$ for cluster function
% % $F$ for global fiunction; otherwise your notation is confusing}

% \begin{proof}
% Using $\beta$-smoothness of $F(\cdot)$, we have
%     \begin{align} \label{12}
%         F(\mathbf v)-F(\mathbf m)\leq \nabla F(\mathbf m)^T(\mathbf v-\mathbf m)+\frac{\beta}{2}\Big\Vert\mathbf v-\mathbf m\Big\Vert^2.
%     \end{align}
%     Consider $t \in \mathcal T_k$, let $\mathbf v = \mathbf w(t+1)$ and $\mathbf m = \mathbf w(t)$, from \eqref{8}, \eqref{eq14} and \eqref{15} we have
%     \begin{align*}
%     \mathbf w(t) &= 
%           \sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}} \sum\limits_{i\in\mathcal{S}_c^{(k)}}\mathbf w_{i}(t)
%           \nonumber\\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}} \sum\limits_{i\in\mathcal{S}_c^{(k)}}\big(\overline{\mathbf w_{c}}(t)+s_c^{(k)} e_{i}^{(\Gamma_c(t))}\big)
%     \end{align*}
%     Since $\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}} \sum\limits_{i\in\mathcal{S}_c^{(k)}}s_c^{(k)} e_{i}^{(\Gamma_c(t))}=0$ for all $t$, $\mathbf w(t)$ can be written as
%     \nm{no need for all these steps: replace with}
%     \add{Consider $t \in \mathcal T_k$, let $\mathbf v = \mathbf w(t+1)$ and $\mathbf m = \mathbf w(t)$, from \eqref{8}, \eqref{eq14} and \eqref{15}
%     and using the fact that $\sum\limits_{i\in\mathcal{S}_c^{(k)}} e_{i}^{(\Gamma_c(t))}=0$ we have
%     \begin{align*}
%     \mathbf w(t) &= \mathbf w(t-1)
%           -\eta_k\sum\limits_{c=1}^N \varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}} \nabla F_i(\mathbf w_i(t-1))
%     \end{align*}}
%     \begin{align*}
%     \mathbf w(t) &= 
%           \sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}} \sum\limits_{i\in\mathcal{S}_c^{(k)}}\overline{\mathbf w_{c}}(t)
%           \nonumber\\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}} \sum\limits_{i\in\mathcal{S}_c^{(k)}}\big(\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \left[\mathbf w_j(t-1)-\eta_k\nabla F_j(\mathbf w_j(t-1))\right]\big)
%           \nonumber\\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \mathbf w_j(t-1)
%           \nonumber\\&
%           -\eta_k\sum\limits_{c=1}^N \varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}} \nabla F_i(\mathbf w_i(t-1))
%           \nonumber\\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \Big(\bar{\mathbf w}_c(t-1)+s_c^{(k)} e_{j}^{(\Gamma_c(t-1))}\Big)
%           \nonumber\\&
%           -\eta_k\sum\limits_{c=1}^N \varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}} \nabla F_i(\mathbf w_i(t-1))
%           \nonumber\\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \bar{\mathbf w}_c(t-1)
%           -\eta_k\sum\limits_{c=1}^N \varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}} \nabla F_i(\mathbf w_i(t-1))
%           \nonumber\\&
%           =\sum\limits_{c=1}^N \varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \Big(\mathbf w_j(t-1)-s_c^{(k)} e_{j}^{(\Gamma_c(t-1))}\Big)
%           \nonumber\\&
%           -\eta_k\sum\limits_{c=1}^N \varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}} \nabla F_i(\mathbf w_i(t-1))
%           \nonumber\\&
%           = \mathbf w(t-1)
%           -\eta_k\sum\limits_{c=1}^N \varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}} \nabla F_i(\mathbf w_i(t-1))
%     \end{align*}
%     and thus
%     \begin{align} \label{13}
%         &F(\mathbf w(t+1))-F(\mathbf w(t))
%         \\&
%         \leq -\eta_k\nabla F(\mathbf w(t))^T \sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))
%         \\& \label{60}
%         +\frac{\eta_k^2 \beta}{2}\Big\Vert\sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))\Big\Vert^2.
%     \end{align}
%     %%%%%%%%%%%%%%%%%
%     By bounding the first term of \eqref{13} with Lemma \ref{lem1} \add{and \eqref{60} with lemma \ref{lem2}}, we obtain
%     \nm{remove this one!}
%     \begin{align}
%         &F(\mathbf w(t+1))-F(\mathbf w(t))
%         \leq -\mu\eta_k(F(\mathbf w(t))-F(\mathbf w^*))
%         \\&
%         +\frac{\eta_k}{2}(\eta_k\beta-1)\Big\Vert\sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))\Big\Vert^2
%         \\&
%         +\frac{\eta_k}{2}\beta^2\sum\limits_{c=1}^N\varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}}\Big\Vert\mathbf w(t)-\mathbf w_i(t)\Big\Vert^2.
%     \end{align}
%     By bounding \eqref{60} with lemma \ref{lem2}, we obtain
%     \begin{align} \label{46}
%         &F(\mathbf w(t+1))-F(\mathbf w(t))
%         \leq -\mu\eta_k(F(\mathbf w(t))-F(\mathbf w^*))
%         \\&
%         +\frac{\eta_k}{2}(\eta_k\beta-1)\Big\Vert\sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))\Big\Vert^2
%         \\&
%         +\frac{\eta_k}{2}\beta^2\eta_k^2\tau_k\sum\limits_{l=t_{k-1}}^{t-1}\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}}
%         \\&
%         \Big\Vert\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\nabla F_j(\mathbf w_j(l)) 
%         -s_c^{(k)} e_{i}^{(l+1)}\Big\Vert^2.
%     \end{align}
%     Since $\Vert\mathbf a+\mathbf b\Vert^2\leq 2(\Vert\mathbf a\Vert^2+\Vert\mathbf b\Vert^2)$, \eqref{46} can be bounded as
%     \begin{align} \label{47}
%         &F(\mathbf w(t+1))-F(\mathbf w(t))
%         \leq -\mu\eta_k(F(\mathbf w(t))-F(\mathbf w^*))
%         \\&
%         +\frac{\eta_k}{2}(\eta_k\beta-1)\Big\Vert\sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))\Big\Vert^2
%         \\&
%         +\eta_k\beta^2\eta_k^2\tau_k\sum\limits_{l=t_{k-1}}^{t-1}\sum\limits_{c=1}^N\varrho_{c}\Big\Vert\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}}\nabla F_j(\mathbf w_j(l))\Big\Vert^2
%         \\&
%         +\eta_k\beta^2\eta_k^2\tau_k\sum\limits_{l=t_{k-1}}^{t-1}\sum\limits_{c=1}^N\varrho_{c} s_c^{(k)}\sum\limits_{i\in\mathcal{S}_c^{(k)}}\Big\Vert e_i^{(\Gamma_c(l+1))}\Big\Vert^2.
%     \end{align}
%     \eqref{47} is equivalent to 
%     \begin{align} \label{73}
%         &F(\mathbf w(t+1))-F(\mathbf w^*)
%         \leq (1-\mu\eta_k)(F(\mathbf w(t))-F(\mathbf w^*)) 
%         \nonumber \\&
%         +\frac{\eta_k}{2}(\eta_k\beta-1)\Big\Vert\sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))\Big\Vert^2
%         \\&
%         +\eta_k\beta^2\eta_k^2\tau_k\sum\limits_{l=t_{k-1}}^{t-1}\sum\limits_{c=1}^N\varrho_{c}\Big\Vert\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}}\nabla F_j(\mathbf w_j(l))\Big\Vert^2
%         \\&
%         +\eta_k\beta^2\tau_k\sum\limits_{l=t_{k-1}}^{t-1}\sum\limits_{c=1}^N\varrho_{c} s_c^{(k)}\sum\limits_{i\in\mathcal{S}_c^{(k)}}\Big\Vert E_i^{(\Gamma_c(l+1))}\Big\Vert^2.
%     \end{align}
    
%     Based on assumption \ref{gradDiv}, \eqref{73} can be bounded as
%     \begin{align} \label{74}
%         &F(\mathbf w(t+1))-F(\mathbf w^*)
%         \leq (1-\mu\eta_k)(F(\mathbf w(t))-F(\mathbf w^*)) 
%         \nonumber \\&
%         +\frac{\eta_k}{2}(\eta_k\beta-1)\Big\Vert\sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))\Big\Vert^2
%         \\&
%         +\eta_k\beta^2\eta_k^2\delta\tau_k\sum\limits_{l=t_{k-1}}^{t-1}\Big\Vert\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}}\nabla F_j(\mathbf w_j(l))\Big\Vert^2
%         \\&
%         +\eta_k\beta^2\tau_k\sum\limits_{l=t_{k-1}}^{t-1}\sum\limits_{c=1}^N\varrho_{c} s_c^{(k)}\sum\limits_{i\in\mathcal{S}_c^{(k)}}\Big\Vert E_i^{(\Gamma_c(l+1))}\Big\Vert^2.
%     \end{align}
   
%     Since $\Big\Vert G_{i}^{(\Gamma_c(t))} \Big\Vert^2 \leq \lambda_c^{2\Gamma_c(t)}s_c^{(k)}^2\epsilon_c^2(t) m^2$, \eqref{74} can bounded as
%     \begin{align} \label{96}
%         &F(\mathbf w(t+1))-F(\mathbf w^*)
%         \leq (1-\mu\eta_k)(F(\mathbf w(t))-F(\mathbf w^*)) 
%         \nonumber \\&
%         +\frac{\eta_k}{2}(\eta_k\beta-1)\Big\Vert\sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))\Big\Vert^2
%         \\&
%         +\eta_k\beta^2\eta_k^2\delta\tau_k\sum\limits_{l=t_{k-1}}^{t-1}\Big\Vert\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}}\nabla F_j(\mathbf w_j(l))\Big\Vert^2
%         \\&
%         +\eta_k\beta^2\tau_k\sum\limits_{l=t_{k-1}}^{t-1}\sum\limits_{c=1}^N\varrho_{c} \lambda_c^{2\Gamma_c(l+1)}s_c^{(k)}^4\epsilon_c^2(l+1) m^2.
%     \end{align}
%     Since consensus is performed at $t\in\mathcal H_k$, the error in \eqref{96} can be written as as the sum of two components: error with zero rounds of consensus at $t\in\mathcal T_k \setminus\mathcal H_k$ and error with $\Gamma_c(t)$ rounds of consensus at $t\in\mathcal H_k$.
%     Let $\epsilon_c(k)\leq\varepsilon_c(k)$ for all $t\in\mathcal T_k$, if the number of consensus rounds for cluster $c$ at $t\in\mathcal H_k$ satisfies
%     \begin{align*} 
%         \begin{cases}
%             \Gamma_c(k)\geq\frac{\log(\sigma_c^{(k)})-2\log(s_c^{(k)}^2\varepsilon_c(k))}{2\log(\lambda_c)} ,& \sigma_c^{(k)} \leq s_c^{(k)}^4\varepsilon_c^2(k), \forall c, k,  \\
%             \Gamma_c(k)\geq 0 ,& \text{otherwise}
%         \end{cases},
%     \end{align*}
%     \eqref{96} can be upper bounded as
%     \begin{align} \label{97}
%         &F(\mathbf w(t+1))-F(\mathbf w^*)
%         \leq (1-\mu\eta_k)(F(\mathbf w(t))-F(\mathbf w^*)) 
%         \nonumber \\&
%         +\frac{\eta_k}{2}(\eta_k\beta-1)\Big\Vert\sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))\Big\Vert^2
%         \\&
%         +\eta_k\beta^2\eta_k^2\delta\tau_k\sum\limits_{l=t_{k-1}}^{t-1}\Big\Vert\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}}\nabla F_j(\mathbf w_j(l))\Big\Vert^2
%         \\&
%         +\eta_k\beta^2\tau_k m^2 h_k\sum\limits_{c=1}^N\varrho_{c} \sigma_c^{(k)}
%         \\&
%         +\eta_k\beta^2\tau_k m^2 (\tau_k-h_k)\sum\limits_{c=1}^N\varrho_{c} s_c^{(k)}^4\varepsilon_c^2(k).
%     \end{align}
    
% Finally, by Lemma \ref{lem3}, we obtain the update relationship as 
%     \begin{align}
%         &F(\mathbf w(t_{k}))-F(\mathbf w^*) \label{140}
%         \leq 
%         (1-\mu\eta_{k})^{\tau_{k}} \Big(\mathbf w(t_{k-1})-F(\mathbf w^*)\Big)
%         \nonumber \\& 
%         +\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\eta_k\beta^2\tau_k m^2 h_k\sum\limits_{c=1}^N\varrho_{c} \sigma_c^{(k)}
%         \nonumber \\&
%         +\frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu\eta_k}\eta_k\beta^2\tau_k m^2
%         (\tau_k-h_k)\sum\limits_{c=1}^N\varrho_{c} s_c^{(k)}^4\varepsilon_c^2(k).
%     \end{align}
%     With this relationship, we can recursively obtain
%     \begin{align}
%         &F(\mathbf w(t_{k}))-F(\mathbf w^*)
%         \leq\prod_{n=1}^k(1-\mu\eta_n)^{\tau_n} \Big(F(\mathbf w(0))-F(\mathbf w^*)\Big)
%         \nonumber \\&
%         +\sum\limits_{n=0}^{k-1}\frac{1-(1-\mu\eta_n)^{\tau_n}}{\mu\eta_n}\eta_n\beta^2\tau_n m^2 h_n\sum\limits_{c=1}^N\varrho_{c} \sigma_c^{(n)}
%         \nonumber \\&
%         +\sum\limits_{n=0}^{k-1}\frac{1-(1-\mu\eta_n)^{\tau_n}}{\mu\eta_n}\eta_n\beta^2\tau_n m^2
%         (\tau_n-h_n)\sum\limits_{c=1}^N\varrho_{c} s_c^{(k)}^4\varepsilon_c^2(n).
%     \end{align}
% \end{proof}

% \subsection{Proof of Proposition \ref{genLin}}

% \begin{proposition} \label{genLin}
%     Under Assumption \ref{beta}, \ref{PL}, \ref{gradDiv} and Algorithm \ref{GT}, suppose the number of consensus rounds at different clusters of the network satisfying
%     $$
%     \Gamma_c(k)\geq
%     \left[\frac{\log(\sigma_c^{(k)})-2\log(s_c^{(k)}^2\varepsilon_c(k))}{2\log(\lambda_c)}\right]^+
%     $$
%     such that $\sigma_c^{(k)}$ satisfies the following inequalities 
%     \begin{equation} \label{cond2}
%         \begin{aligned}
%             \begin{cases}
%                 \lim_{k\rightarrow\infty}B_{k+1}/B_k\leq 1, \\
%                 \eta_k\leq\frac{1}{\beta+\frac{2\beta^2\delta(\mathbf w)\tau_k}{\eta_k(1-\mu\eta_k)^{\tau_k-1}}\big[1-(1-\mu\eta_k)^{\tau_k-1}\big]},
%             \end{cases},
%         \end{aligned}
%     \end{equation}
%     where $B_k = \frac{1-(1-\mu\eta_k)^{\tau_k}}{\mu}\beta^2\tau_k m^2 \Big(h_k\sum\limits_{c=1}^N\varrho_{c} \sigma_c^{(k)}+(\tau_k-h_k)\sum\limits_{c=1}^N\varrho_{c} s_c^{(k)}^4\varepsilon_c^2(k)\Big)$. 
%     Then Algorithm \ref{GT} is guaranteed to converge:
%     \begin{align*}
%         &F(\mathbf w(t_{k}))-F(\mathbf w^*)
%         \\&
%         \leq\prod_{n=1}^k(1-\mu\eta_n)^{\tau_n} \Big(F(\mathbf w(0))-F(\mathbf w^*)\Big)+\mathcal O(C_k),
%     \end{align*}
%     where $C_k=\max\{B_k,\Big((1-\mu\eta_k)^{\tau_k}+\xi\Big)^{t_k}\}$ for any $\xi>0$.

% \end{proposition}

% \begin{proof}
%     Let $\upsilon=(1-\mu\eta)^{\tau_k}$
%     % \begin{align}
%     %     &B_k=\frac{m^2}{\eta_k}\sum\limits_{c=1}^N \varrho_c^{(k)}\Big(\sum\limits_{t \in \mathcal H_k}(1-\mu\eta_k)^{(k+1)\tau_k-t}\sigma_c^{(t)}
%     %     \\&
%     %     +s_c^{(k)}^2\sum\limits_{t \in \mathcal T_k \setminus\mathcal H_k}(1-\mu\eta_k)^{(k+1)\tau_k-t}\epsilon_c^2(t)\Big),
%     % \end{align}
%     \eqref{31} becomes
%     \begin{align}
%         &F(\mathbf w(t_{k}))-F(\mathbf w^*)
%         \\&
%         \leq(1-\mu\eta_k)^{\tau_k} \Big(F(\mathbf w(t_{k-1}))-F(\mathbf w^*)\Big)
%         +B_k.
%     \end{align}
%     Recursively, we obtain  
%      \begin{align*}
%         &F(\mathbf w(t_{k}))-F(\mathbf w^*)
%         \\&
%         \leq (1-\mu\eta_k)^{\tau_k}\Big(F(\mathbf w(t_{k-1}))-F(\mathbf w^*)\Big)+\mathcal O(\zeta_k),
%     \end{align*}
%     where $\zeta_k=\sum_{n=0}^{k-1} \upsilon^{k-n-1}B_n$ such that $\zeta_{k+1}=B_k+\upsilon\zeta_k$.
%     Based on $\lim_{k\rightarrow\infty}B_{k+1}/B_k\leq 1$ and the definition of $C_k$ that $\lim_{k\rightarrow\infty}C_{k+1}/C_k\geq \upsilon+\pi$. Therefore, there exist some $N$ such that $C_{k+1}/C_k-\upsilon\geq\epsilon/2$ for all $k\geq N$. Since $\zeta_k$ are finite and $\pi>0$, we can always choose $\phi$ such that
%     \begin{align*}
%         \begin{cases}
%             \zeta_k \leq \phi C_k,\ \forall k\leq N,\\
%             \phi\pi/2\geq 1,
%         \end{cases}
%     \end{align*}
%     Assume this holds for some arbitrary $k\geq N$, we have
%     \begin{align}
%         &\zeta_{k+1}=B_k+\upsilon\zeta_k\leq C_k+\upsilon\pi C_k
%         \leq \pi(\phi/2)C_k+\upsilon\pi C_k
%         \\&
%         \leq\pi(C_{k+1}/C_k-\upsilon)C_k+\upsilon\pi C_k=\pi C_{k+1}.
%     \end{align}
%     Therefore, $\zeta_k=\mathcal O(C_k)$ for all $k$.
% \end{proof}

% \subsection{Proof of Proposition \ref{strLin}}

% \begin{proposition} \label{strLin}
%     Under Assumption \ref{beta}, \ref{PL} and \ref{gradDiv}, suppose the number of consensus rounds at different clusters of the network satisfies
%     $$
%     \Gamma_c(k)\geq
%     \left[\frac{\log(\sigma_c^{(k)})-2\log(s_c^{(k)}^2\varepsilon_c(k))}{2\log(\lambda_c)}\right]^+
%     $$
%     where $\sigma_c^{(k)}$, $\tau_k$ and $\eta_k$ are determined to satisfy the following inequalities   
%     \begin{align} \label{cond3}
%             \begin{cases} \label{35}
%             h_k\sum\limits_{c=1}^N\varrho_{c} \sigma_c^{(k)}+(\tau_k-h_k)\sum\limits_{c=1}^N\varrho_{c} s_c^{(k)}^4\varepsilon_c^2(k)
%             \\
%             \leq\frac{\mu^2(\gamma_k-\lambda_k)}{2\beta^4 m^2 \tau_k\left[1-(1-\mu\eta_k)^{\tau_k}\right]}\Big\Vert\nabla F(\mathbf w(t_{k-1}))\Big\Vert^2, \\
%             \eta_k\leq\frac{1}{\beta+\frac{2\beta^2\delta(\mathbf w)\tau_k}{\eta_k(1-\mu\eta_k)^{\tau_k-1}}\big[1-(1-\mu\eta_k)^{\tau_k-1}\big]},
%             \end{cases}.
%     \end{align}
%     Then Algorithm \ref{GT} is guaranteed to achieve linear convergence:
%     \begin{align*}
%         &F(\mathbf w(t_{k}))-F(\mathbf w^*)
%         \leq (1-\lambda_k)\Big(F(\mathbf w(t_{k-1}))-F(\mathbf w^*)\Big)
%     \end{align*}
%     for $0\leq\lambda_k\leq1-(1-\mu\eta_k)^{\tau_k}$.
% \end{proposition}

% \begin{proof}   
%     Let $\gamma_k=1-(1-\mu\eta_k)^{\tau_k}$, from \eqref{35}, we have
%     \begin{align} \label{36}
%         &h_k\sum\limits_{c=1}^N\varrho_{c} \sigma_c^{(k)}+(\tau_k-h_k)\sum\limits_{c=1}^N\varrho_{c} s_c^{(k)}^4\varepsilon_c^2(k)
%         \nonumber \\&
%         \leq\frac{\mu^2(\gamma_k-\lambda_k)}{2\beta^4 m^2 \tau_k\left[1-(1-\mu\eta_k)^{\tau_k}\right]}\Big\Vert\nabla F(\mathbf w(t_{k-1}))\Big\Vert^2.
%     \end{align}

%     By $\beta$-smoothness and $\mu$-strong convexity of $F(\cdot)$, we have
%     \begin{align} \label{145}
%         \Big\Vert \nabla F(\mathbf w(t_{k-1}))\Big\Vert^2
%         \leq\frac{2\beta^2}{\mu}\Big(F(\mathbf w(t_{k-1}))- F(\mathbf w^*)\Big)
%     \end{align}
%     Combining \eqref{31}, \eqref{35} and \eqref{36}, we have
%     \begin{align}
%         &F(\mathbf w(t_{k}))-F(\mathbf w^*)
%         % \leq(1-\gamma_k) \Big(F(\mathbf w(t_{k-1}))-F(\mathbf w^*)\Big)
%         % \\&
%         % + (\gamma_k-\lambda_k)\big(F(\mathbf w(t_{k-1}))- F(\mathbf w^*)\big)
%         % \\&
%         \leq (1-\lambda_k)\Big(F(\mathbf w(t_{k-1}))-F(\mathbf w^*)\Big)
%     \end{align}
% \end{proof}

\setcounter{lemma}{0}
\setcounter{proposition}{0}
\setcounter{theorem}{0}
\section{}

\begin{lemma} \label{lem1}
Under Assumption \ref{beta} and Algorithm \ref{GT}, we have
    \begin{align*}
        &-\eta_k\nabla F(\bar{\mathbf w}(t))^T \sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}}\nabla F_j(\mathbf w_j(t))
        \\&
        \leq
        -\mu\eta_k(F(\bar{\mathbf w}(t))-F(\mathbf w^*))
        \\&
        -\frac{\eta_k}{2}\Big\Vert\sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))\Big\Vert^2
        \\&
        +\frac{\eta_k\beta^2}{2}\sum\limits_{c=1}^N\varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}}\Big\Vert\bar{\mathbf w}(t)-\mathbf w_{\add{j}}(t)\Big\Vert^2.
    \end{align*}
\end{lemma}

\begin{proof}
    \begin{align} \label{14}
        &-\eta_k\nabla F(\bar{\mathbf w}(t))^T
        \sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))
        \nonumber \\&
        =\frac{\eta_k}{2}\bigg[-\Big\Vert\nabla F(\bar{\mathbf w}(t))\Big\Vert^2
        -\Big\Vert\sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))\Big\Vert^2
        \nonumber \\&
        +\Big\Vert\nabla F(\bar{\mathbf w}(t))-\sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))\Big\Vert^2\bigg]
    \end{align}
    By the convexity of $\Vert\cdot\Vert^2$, we can further bound
    \begin{align} \label{20}
        &-\eta_k\nabla F(\bar{\mathbf w}(t))^T \sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))
        \nonumber \\&
        \leq\frac{\eta_k}{2}\bigg[-\Big\Vert\nabla F(\bar{\mathbf w}(t))\Big\Vert^2
        -\Big\Vert\sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))\Big\Vert^2
        \nonumber \\&
        +\sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}}\Big\Vert\nabla F_j(\bar{\mathbf w}(t))-\nabla F_j(\mathbf w_j(t))\Big\Vert^2\bigg]
    \end{align}
    
    By $\mu$-strong convexity and $\beta$-smoothness of $F_i(\cdot)$, 
    we can bound \eqref{20} as 
    \begin{align}
        &-\eta_k\nabla F(\bar{\mathbf w}(t))^T \sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))
        \nonumber \\&
        \leq
        -\mu\eta_k(F(\bar{\mathbf w}(t))-F(\mathbf w^*))
        \nonumber \\&
        -\frac{\eta_k}{2}\Big\Vert\sum\limits_{c=1}^N\varrho_c^{(k)}\frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}} \nabla F_j(\mathbf w_j(t))\Big\Vert^2
        \nonumber \\&
        +\frac{\eta_k\beta^2}{2}\sum\limits_{c=1}^N\varrho_c^{(k)} \frac{1}{s_c^{(k)}}\sum\limits_{j\in\mathcal{S}_c^{(k)}}\Big\Vert\bar{\mathbf w}(t)-\mathbf w_j(t)\Big\Vert^2
    \end{align}
    and thus prove the Lemma.
\end{proof}

\begin{proposition}\label{lem2}
Under Assumption \ref{beta} and Algorithm \ref{GT}, for $\eta_k\leq\frac{\kappa}{\beta}$ and $t \in \mathcal T_k$, we have
    \begin{align*}
        &\mathbb E\left[\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_c^{(k)}}\sum\limits_{i\in\mathcal{S}_c^{(k)}}\Big\Vert\bar{\mathbf w}(t)-\mathbf w_i(t)\Big\Vert^2\right]
        \nonumber \\& 
        \leq 
        [\lambda_1^{2t}-1]\{32\omega^2/\mu[F\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)]+153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}+\epsilon^2(k),
    \end{align*}
    where $\lambda_1 = 1+\eta_k\beta\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]$.
\end{proposition}
\begin{proof}
% \addFL{
Since
    \begin{align}
        \Vert\mathbf w_i(t)-\bar{\mathbf w}(t)\Vert_2^2
        =
        \Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert_2^2
        +\Vert e_i^{(t)}\Vert^2
        +2[\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)]^Te_i^{(t)}
    \end{align}
and therefore it follows
\begin{align} \label{errors}
\mathbb E\left[\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_c^{(k)}}\sum_i\Vert\mathbf w_i(t)-\bar{\mathbf w}(t)\Vert_2^2\right]
        =
        \sum\limits_{c=1}^N\varrho_{c}\mathbb E\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert_2^2
        +\mathbb E\left[\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_c^{(k)}}\sum_i\Vert{e_i^{(t)}\Vert}^2\right],
\end{align}
since consensus preserves average.
For $t \in \mathcal T_k$, 
\begin{align}
        &\bar{\mathbf w}(t)-\mathbf w^* = \bar{\mathbf w}(t-1)-\mathbf w^*-\eta_k \nabla F(\bar{\mathbf w}(t-1))
        \nonumber \\&
        -\eta_k \sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}} [\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\mathbf w_j(t-1))]
        \nonumber \\&
        -\eta_k \sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}} [\nabla F_j(\mathbf w_j(t-1))-\nabla F_j(\bar{\mathbf w}_c(t-1))]
        \nonumber \\&
        -\eta_k \sum\limits_{c=1}^N\varrho_{c} [\nabla F_c(\bar{\mathbf w}_c(t-1))-\nabla F_c(\bar{\mathbf w}(t-1))].
    \end{align}
    By triangular inequality, we obtain
    \begin{align} \label{29}
        &\Vert\bar{\mathbf w}(t)-\mathbf w^*\Vert \leq \Vert\bar{\mathbf w}(t-1)-\mathbf w^*-\eta_k \nabla F(\bar{\mathbf w}(t-1))\Vert
        \nonumber \\&
        +\eta_k \sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}} \Vert\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\mathbf w_j(t-1))\Vert
        \nonumber \\&
        +\eta_k \sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}} \Vert\nabla F_j(\mathbf w_j(t-1))-\nabla F_j(\bar{\mathbf w}_c(t-1))\Vert
        \nonumber \\&
        +\eta_k \sum\limits_{c=1}^N\varrho_{c} \Vert\nabla F_c(\bar{\mathbf w}_c(t-1))-\nabla F_c(\bar{\mathbf w}(t-1))\Vert.
    \end{align}
    By $\mu$-strongly convex and $\beta$-smoothness of $F(\cdot)$, when $\eta_k\leq\frac{\kappa}{\beta}$, we have 
    \begin{align}
        &\Vert\bar{\mathbf w}(t)-\mathbf w^*-\eta_k \nabla F(\bar{\mathbf w}(t))\Vert
        \nonumber \\&
        =
        \sqrt{\Vert\bar{\mathbf w}(t)-\mathbf w^*\Vert^2+\eta_k^2\Vert\nabla F(\bar{\mathbf w}(t))\Vert^2-2\eta_k(\bar{\mathbf w}(t)-\mathbf w^*)^T\nabla F(\bar{\mathbf w}(t))}
        \nonumber \\&
        \leq 
        \sqrt{(1-2\eta_k\mu)\Vert\bar{\mathbf w}(t)-\mathbf w^*\Vert^2+\eta_k^2\Vert\nabla F(\bar{\mathbf w}(t))\Vert^2}
        \nonumber \\&
        \leq 
        \sqrt{(1-2\eta_k\mu+\eta_k^2\beta^2)}\Vert\bar{\mathbf w}(t)-\mathbf w^*\Vert
        \nonumber \\&
        \leq 
        \sqrt{1-\eta_k\mu}\Vert\bar{\mathbf w}(t)-\mathbf w^*\Vert
        \nonumber \\&
        \leq 
        (1-\frac{\eta_k\mu}{2})\Vert\bar{\mathbf w}(t)-\mathbf w^*\Vert.
    \end{align}
    Therefore, we can further bound \eqref{29} as
    \begin{align}
        &\Vert\bar{\mathbf w}(t)-\mathbf w^*\Vert \leq (1-\frac{\eta_k\mu}{2})\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert
        \nonumber \\&
        +\eta_k \sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}} \Vert\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\mathbf w_j(t-1))\Vert
        \nonumber \\&
        +\eta_k\beta \sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}} \Vert\mathbf w_j(t-1)-\bar{\mathbf w}_c(t-1)\Vert
        \nonumber \\&
        +\eta_k\beta \sum\limits_{c=1}^N\varrho_{c} \Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert.
    \end{align}
    By Definition \ref{paraDiv}, we further bound
    \begin{align} \label{57}
        &\Vert\bar{\mathbf w}(t)-\mathbf w^*\Vert \leq  (1-\frac{\eta_k\mu}{2})\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert
        \nonumber \\&
        +\eta_k \sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}} \Vert\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\mathbf w_j(t-1))\Vert
        \nonumber \\&
        +\eta_k\beta \sum\limits_{c=1}^N\varrho_{c} \Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert
        \nonumber \\&
        +\eta_k\beta \sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}} \Vert\mathbf w_j(t-1)-\bar{\mathbf w}_c(t-1)\Vert.
    \end{align}
    Since $\sqrt{\mathbb E[(\sum\limitS_i x_i)^2]}\leq \sum\limitS_i \sqrt{\mathbb E[x_i^2]}$, then we obtain 
    \begin{align} 
        &\sqrt{\mathbb E[\Vert\bar{\mathbf w}(t)-\mathbf w^*\Vert^2]} \leq  \bigg\{(1-\frac{\eta_k\mu}{2})\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert
        \nonumber \\&
        +\eta_k \sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}} \Vert\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\mathbf w_j(t-1))\Vert
        \nonumber \\&
        +\eta_k\beta \sum\limits_{c=1}^N\varrho_{c} \Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert
        \nonumber \\&
        +\eta_k\beta \sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}} \Vert\mathbf w_j(t-1)-\bar{\mathbf w}_c(t-1)\Vert\bigg\}^{1/2}
        \nonumber \\&
        \leq
        (1-\frac{\eta_k\mu}{2})\sqrt{\mathbb E[\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert^2]}
        \nonumber \\&
        +\eta_k \sqrt{\mathbb E[(\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}} \Vert\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\mathbf w_j(t-1))\Vert)^2]}
        \nonumber \\&
        +\eta_k\beta \sqrt{\mathbb E[(\sum\limits_{c=1}^N\varrho_{c} \Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert)^2]}
        \nonumber \\&
        +\eta_k\beta \sqrt{\mathbb E[(\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}} \Vert\mathbf w_j(t-1)-\bar{\mathbf w}_c(t-1)\Vert)^2]}
        \nonumber \\&
        \leq
        (1-\frac{\eta_k\mu}{2})\sqrt{\mathbb E[\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert^2]}
        \nonumber \\&
        +\eta_k\beta \sqrt{\mathbb E[(\sum\limits_{c=1}^N\varrho_{c} \Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert)^2]}
        \nonumber \\&
        +\eta_k(\beta\epsilon(k)+\sigma)
    \end{align}
    In addition, since
    \begin{align}
        &\bar{\mathbf w}_c(t)=\bar{\mathbf w_c}(t-1)-\eta_k\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\widehat{\mathbf g}_{j,l}
    \end{align}
    and 
    \begin{align}
        &\bar{\mathbf w}(t)=
        \bar{\mathbf w}(t-1)-\eta_k\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\widehat{\mathbf g}_{j,l},
    \end{align}
    we have
    \begin{align} 
        &\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)=\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)
        \nonumber \\&
        -\eta_k\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Big[\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\bar{\mathbf w}_j (t-1))\Big]
        +\eta_k\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Big[\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\bar{\mathbf w}_j(t-1))\Big]
        \nonumber \\&
        -\eta_k\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Big[\nabla F_j(\bar{\mathbf w}_j (t-1))-\nabla F_j(\bar{\mathbf w}_c (t-1))\Big]
        +\eta_k\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Big[\nabla F_j(\bar{\mathbf w}_j (t-1))-\nabla F_j(\bar{\mathbf w}_c(t-1))\Big]
        \nonumber \\&
        -\eta_k\Big[\nabla\hat F_c(\bar{\mathbf w}_c(t-1))-\nabla\hat F_c(\bar{\mathbf w}(t-1))\Big]
        +\eta_k\sum\limits_{c=1}^N\varrho_{c}\Big[\nabla\hat F_c(\bar{\mathbf w}_c(t-1))-\nabla\hat F_c(\bar{\mathbf w}(t-1))\Big]
        \nonumber \\&
        -\eta_k\Big[\nabla\hat F_c(\bar{\mathbf w}(t-1))-\nabla F(\bar{\mathbf w}(t-1))\Big].
    \end{align}   
    \addFL{
    We can rewrite it as
    \begin{align} 
        &\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)=\Big(\bar{\mathbf w}_c(t-1)-\mathbf w^*-\eta_k\nabla\hat F_c(\bar{\mathbf w}_c(t-1))\Big)
        -\Big[\bar{\mathbf w}(t-1)-\mathbf w^*-\eta_k\nabla\hat F_c(\bar{\mathbf w}(t-1))\Big]
        \nonumber \\&
        -\eta_k\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Big[\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\bar{\mathbf w}_j (t-1))\Big]
        +\eta_k\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Big[\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\bar{\mathbf w}_j(t-1))\Big]
        \nonumber \\&
        -\eta_k\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Big[\nabla F_j(\bar{\mathbf w}_j (t-1))-\nabla F_j(\bar{\mathbf w}_c (t-1))\Big]
        +\eta_k\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Big[\nabla F_j(\bar{\mathbf w}_j (t-1))-\nabla F_j(\bar{\mathbf w}_c(t-1))\Big].
    \end{align}  
    By triangular inequality, we can bound 
    \begin{align} \label{40}
        &\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert
        \leq 
        \Vert\bar{\mathbf w}_c(t-1)-\mathbf w^*-\eta_k\nabla\hat F_c(\bar{\mathbf w}_c(t-1))\Vert
        +\Vert\bar{\mathbf w}(t-1)-\mathbf w^*-\eta_k\nabla\hat F_c(\bar{\mathbf w}(t-1))\Vert
        \nonumber \\&
        +\eta_k\left\Vert\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}[\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\mathbf w_j(t-1))]
        +\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}[\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\mathbf w_j(t-1))]\right\Vert
        \nonumber \\&
        +\eta_k\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Vert\nabla F_j(\mathbf w_j(t-1))-\nabla F_j(\bar{\mathbf w}_c(t-1))\Vert
        +\eta_k\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Vert\nabla F_j(\mathbf w_j(t-1))-\nabla F_j(\bar{\mathbf w}_c(t-1))\Vert.
    \end{align}   
    By $\mu$-strongly convex and $\beta$-smoothness of $F(\cdot)$, when $\eta_k\leq\frac{\kappa}{\beta}$, we have 
    \begin{align}
        &\Vert\bar{\mathbf w}(t)-\mathbf w^*-\eta_k \nabla F(\bar{\mathbf w}(t))\Vert
        \nonumber \\&
        =
        \sqrt{\Vert\bar{\mathbf w}(t)-\mathbf w^*\Vert^2+\eta_k^2\Vert\nabla F(\bar{\mathbf w}(t))\Vert^2-2\eta_k(\bar{\mathbf w}(t)-\mathbf w^*)^T\nabla F(\bar{\mathbf w}(t))}
        \nonumber \\&
        \leq 
        \sqrt{(1-2\eta_k\mu)\Vert\bar{\mathbf w}(t)-\mathbf w^*\Vert^2+\eta_k^2\Vert\nabla F(\bar{\mathbf w}(t))\Vert^2}
        \nonumber \\&
        \leq 
        \sqrt{(1-2\eta_k\mu+\eta_k^2\beta^2)}\Vert\bar{\mathbf w}(t)-\mathbf w^*\Vert
        \nonumber \\&
        \leq 
        \sqrt{1-\eta_k\mu}\Vert\bar{\mathbf w}(t)-\mathbf w^*\Vert
        \nonumber \\&
        \leq 
        (1-\frac{\eta_k\mu}{2})\Vert\bar{\mathbf w}(t)-\mathbf w^*\Vert.
    \end{align}
    }
    By triangular inequality, we have
    \begin{align} \label{40}
        &\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert
        \leq 
        \Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert
        \nonumber \\&
        +\eta_k\left\Vert\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}[\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\mathbf w_j(t-1))]
        +\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}[\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\mathbf w_j(t-1))]\right\Vert
        \nonumber \\&
        +\eta_k\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Vert\nabla F_j(\mathbf w_j(t-1))-\nabla F_j(\bar{\mathbf w}_c(t-1))\Vert
        +\eta_k\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Vert\nabla F_j(\mathbf w_j(t-1))-\nabla F_j(\bar{\mathbf w}_c(t-1))\Vert
        \nonumber \\&
        +\eta_k\Vert\nabla\hat F_c(\bar{\mathbf w}_c(t-1))-\nabla\hat F_c(\bar{\mathbf w}(t-1))\Vert
        +\eta_k\sum\limits_{c=1}^N\varrho_{c}\Vert\nabla\hat F_c(\bar{\mathbf w}_c(t-1))-\nabla\hat F_c(\bar{\mathbf w}(t-1))\Vert
        \nonumber \\&
        +\eta_k\Vert\nabla\hat F_c(\bar{\mathbf w}(t-1))-\nabla F(\bar{\mathbf w}(t-1))\Vert.
    \end{align}   
    By $\beta$-smoothness of $F_i(\cdot)$, Definition \ref{gradDiv} and Definition \ref{paraDiv}, we further bound 
    \begin{align} \label{48}
        &\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert
        \leq (1+\eta_k\beta)\Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert
        \nonumber \\&
        +\eta_k\beta\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\left\Vert\mathbf w_j(t-1)-\bar{\mathbf w}_c(t-1)\right\Vert+\eta_k\beta\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\left\Vert\mathbf w_j(t-1)-\bar{\mathbf w}_c(t-1)\right\Vert
        \nonumber \\&
        +\eta_k\beta\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert
        % \nonumber \\&
        +\eta_k(\delta+\zeta\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert)
        \nonumber \\&
        +\eta_k\left\Vert\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}n_j(t-1)
        +\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}n_j(t-1)\right\Vert
        \nonumber \\&
        \leq 
        (1+\eta_k\beta)\Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert
        % \nonumber \\&
        +\eta_k\zeta\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert
        \nonumber \\&
        +\eta_k\beta\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert
        \nonumber \\&
        +\eta_k\left\Vert\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}n_j(t-1)
        +\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}n_j(t-1)\right\Vert
        \nonumber \\&
        +\eta_k(\delta+\beta\epsilon_c(k)+\beta\epsilon(k))
    \end{align}
    By computing $\sum\limits_c\varrho_c^{(k)}$ over \eqref{48}, we further bound
    \begin{align} \label{53}
        &\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert
        \leq (1+2\eta_k\beta)\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert
        \nonumber \\&
        +\eta_k\zeta\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert
        % \nonumber \\&
        +\eta_k\sum\limits_{c=1}^N\varrho_{c}\left\Vert\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}n_j(t-1)
        +\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}n_j(t-1)\right\Vert
        \nonumber \\&
        +\eta_k(\delta+2\beta\epsilon(k)).
    \end{align}
    Therefore, we have
    \begin{align}
        &\sqrt{\mathbb E[(\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert)^2]}
        \leq (1+2\eta_k\beta)\sqrt{\mathbb E[(\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert)^2]}
        \nonumber \\&
        +\eta_k\zeta\sqrt{\mathbb E[\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert^2]}
        % \nonumber \\&
        +\eta_k(\delta+2\sigma+\sqrt{2}\beta\epsilon(k)).
    \end{align}
    Let $x_1(t)=\sqrt{\mathbb E[(\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert)^2]}$, $x_2(t)=\sqrt{\mathbb E[\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert^2]}$, we obtain the coupled dynamics of the system as follows:
    \begin{align} \label{69}
        \mathbf x(t)
        &\leq 
        \mathbf M x(t-1)
        + \eta_k(\sigma+\beta\epsilon(k))\mathbf 1+\eta_k(\delta+\beta\epsilon(k)+(\sqrt{2}-1)\sigma)\mathbf e_1,
    \end{align}
    where $\mathbf M=\begin{bmatrix} 
    1+2\eta_k\beta & \eta_k\zeta \\
    \eta_k\beta & 1-\eta_k\mu/2 
    \end{bmatrix}$ with eigenvalues $$\lambda_1 = 1+\eta_k\beta\big[1-\kappa/4+\sqrt{C}\big]=1+\eta_k\beta\big[1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}\big]$$ and $$\lambda_2 = 1+\eta_k\beta\big[1-\kappa/4-\sqrt{C}\big]=1+\eta_k\beta\big[1-\kappa/4-\sqrt{(1+\kappa/4)^2+2\omega}\big].$$
    \nm{these eigenvalues are wrong!}
    
    \add{
    \begin{align} \label{69}
        \mathbf x(t)
        &= 
        [\mathbf I+\eta_t\beta\mathbf B]\mathbf x(t-1)
        + \eta_t(\sigma+\beta\epsilon_t)\mathbf 1+\eta_t(\delta+\beta\epsilon_t+(\sqrt{2}-1)\sigma)\mathbf e_1,
    \end{align}
    where $\mathbf B=\begin{bmatrix} 2 & 2\omega
    \\ 1 & -\kappa/2\end{bmatrix}$.
    Upper bound
    $$(\sigma+\beta\epsilon_t)\mathbf 1+(\delta+\beta\epsilon_t+(\sqrt{2}-1)\sigma)\mathbf e_1
    \leq \mathbf z/\beta
    $$
    to get
    \begin{align} \label{69}
        \mathbf x(t)
        &= 
        [\mathbf I+\eta_t\beta\mathbf B]\mathbf x(t-1)
        + \eta_t\beta\mathbf z,
    \end{align}
    $\mathbf B=\mathbf U\mathbf D\mathbf U^{-1}$ eigenv decomp of $\mathbf B$.
    Let $\mathbf f_t=\mathbf U^{-1}\mathbf x(t)+\mathbf U^{-1}\mathbf B^{-1}z$ to get
    \begin{align} \label{69}
        \mathbf f_{t}
        &= 
         [\mathbf I+\eta_t\beta\mathbf D]\mathbf f_{t-1},
    \end{align}
    or equivalently:
    \begin{align} \label{69}
        f_1(t)=[1+\eta_t\beta \lambda_1]f_1(t-1)
        \\
        f_2(t)=[1-\eta_t\beta \lambda_2]f_2(t-1)
    \end{align}
    where
    $$\lambda_1 =1-\kappa/4+\sqrt{(1+\kappa/4)^2+2\omega}>0$$ and $$\lambda_2 =-(1-\kappa/4)+\sqrt{(1+\kappa/4)^2+2\omega}>0$$
    With the choice of stepsize:
    \begin{align} \label{69}
        f_1(t)=[1+\frac{\gamma}{t+\alpha}\beta \lambda_1]f_1(t-1)
        \\
        f_2(t)=[1-\frac{\gamma}{t+\alpha}\beta \lambda_2]f_2(t-1)
    \end{align}
    For the second one we obtain for $t\geq 1$
    $$
    [t+\alpha]f_2(t)=\left[\frac{t+\alpha-\gamma\beta \lambda_2}{t+\alpha-1}\right][t+\alpha-1]f_2(t-1)
    $$$$
    \leq[t+\alpha-1]f_2(t-1)
    \max_{u\geq 1}\left[\frac{u+\alpha-\gamma\beta \lambda_2}{u+\alpha-1}\right]
    \leq
    [t+\alpha-1]f_2(t-1)
    \leq\alpha f_2(0)
    $$
    as long as
    $\gamma\beta \lambda_2\geq 1$ (conditionon gamma), where last step follows from induction. Therefore,
    $$
    f_2(t)
    \leq\frac{\alpha}{t+\alpha} f_2(0)
    $$
    The other one is more tricky:
    $$
    \ln f_1(t)=\ln [1+\frac{\gamma}{t+\alpha}\beta \lambda_1]+\ln f_1(t-1)
    =\ln f_1(0)+\sum_{u=1}^t\ln [1+\frac{\gamma}{u+\alpha}\beta \lambda_1]
    $$
    and since$\ln [1+\frac{\gamma}{u+\alpha}\beta \lambda_1]$ is a decreasing function of $u$,
    $$
    \ln f_1(t)
    \leq\ln f_1(0)+\int_0^t\ln [1+\frac{\gamma}{u+\alpha}\beta \lambda_1]\mathrm du
    $$$$
    =\ln f_1(0)+\int_0^t\ln [u+\alpha+\gamma\beta \lambda_1]\mathrm du
    -\int_0^t\ln [u+\alpha]\mathrm du
    $$$$
    =\ln f_1(0)
    +[t+\alpha]\ln\left[1+\frac{\gamma\beta \lambda_1}{t+\alpha}\right]
    +\gamma\beta \lambda_1\ln\left[1+\frac{t}{\alpha+\gamma\beta \lambda_1}\right]
    -\alpha\ln[1+\gamma\beta \lambda_1/\alpha]
    $$
    hence it follows
    $$
f_1(t)
    \leq f_1(0)
    \left[1+\frac{\gamma\beta \lambda_1}{t+\alpha}\right]^{[t+\alpha]}
    \left[1+\frac{t}{\alpha+\gamma\beta \lambda_1}\right]^{\gamma\beta \lambda_1}
    [1+\gamma\beta \lambda_1/\alpha]^{-\alpha}
    $$$$
    \leq
    f_1(0)
    e^{\gamma\beta \lambda_1}[1+\gamma\beta \lambda_1/\alpha]^{-\alpha}
    \left[1+\frac{t}{\alpha+\gamma\beta \lambda_1}\right]^{\gamma\beta \lambda_1}
    $$
    }
    \addFL{
    With the choice of stepsize:
    \begin{align} \label{69}
        f_1(t-t_{k-1})=[1+\frac{\gamma}{t+\alpha}\beta \lambda_1]f_1(t-t_{k-1}-1)
        \\
        f_2(t-t_{k-1})=[1-\frac{\gamma}{t+\alpha}\beta \lambda_2]f_2(t-t_{k-1}-1)
    \end{align}
    For the second one we obtain for $t\geq 1$
    $$
    [t+\alpha]f_2(t-t_{k-1})
    \leq\left[\frac{t+\alpha-\gamma\beta \lambda_2}{t+\alpha-1}\right][t+\alpha-1]f_2(t-t_{k-1}-1)
    $$$$
    \leq[t+\alpha-1]f_2(t-1)
    \max_{u\geq 1}\left[\frac{u+\alpha-\gamma\beta \lambda_2}{u+\alpha-1}\right]
    \leq 
    [t+\alpha-1]f_2(t-t_{k-1}-1)
    \leq\alpha f_2(t_{k-1})
    $$
    as long as
    $\gamma\beta \lambda_2\geq 1$ (conditionon gamma), where last step follows from induction. Therefore,
    $$
    f_2(t)
    \leq\frac{\alpha}{t+\alpha} f_2(t_{k-1})
    $$
    The other one is more tricky:
    $$
    \ln f_1(t-t_{k-1})=\ln [1+\frac{\gamma}{t+\alpha}\beta \lambda_1]+\ln f_1(t-t_{k-1}-1)
    =\ln f_1(t_{k-1})+\sum_{u=t_{k-1}+1}^{t-t_{k-1}}\ln [1+\frac{\gamma}{u+\alpha}\beta \lambda_1]
    $$
    and since$\ln [1+\frac{\gamma}{u+\alpha}\beta \lambda_1]$ is a decreasing function of $u$,
    $$
    \ln f_1(t-t_{k-1})
    \leq\ln f_1(t_{k-1})+\int_{t_{k-1}}^{t-t_{k-1}}\ln [1+\frac{\gamma}{u+\alpha}\beta \lambda_1]\mathrm du
    $$$$
    =\ln f_1(t_{k-1})+\int_{t_{k-1}}^{t-t_{k-1}}\ln [u+\alpha+\gamma\beta \lambda_1]\mathrm du
    -\int_{t_{k-1}}^{t-t_{k-1}}\ln [u+\alpha]\mathrm du
    $$$$
    =\ln f_1(t_{k-1})
    +[t-t_{k-1}+\alpha]\ln\left[1+\frac{\gamma\beta \lambda_1}{t-t_{k-1}+\alpha}\right]
    +\gamma\beta \lambda_1\ln\left[\frac{\alpha+\gamma\beta\lambda_1}{t_{k-1}+\alpha+\gamma\beta\lambda_1}+\frac{t-t_{k-1}}{t_{k-1}+\alpha+\gamma\beta \lambda_1}\right]
    $$
    $$
    -(t_{k-1}+\alpha)\ln[1+\gamma\beta \lambda_1/(t_{k-1}+\alpha)]
    $$$$
    \leq \ln f_1(t_{k-1})
    +[t-t_{k-1}+\alpha]\ln\left[1+\frac{\gamma\beta \lambda_1}{t-t_{k-1}+\alpha}\right]
    +\gamma\beta \lambda_1\ln\left[1+\frac{t-t_{k-1}}{\alpha+\gamma\beta \lambda_1}\right]
    -\alpha\ln[1+\gamma\beta \lambda_1/\alpha)]
    $$
    hence it follows
    $$
f_1(t)
    \leq f_1(t_{k-1})
    \left[1+\frac{\gamma\beta \lambda_1}{\tau_k+\alpha}\right]^{[\tau_k+\alpha]}
    \left[1+\frac{\tau_k}{\alpha+\gamma\beta \lambda_1}\right]^{\gamma\beta \lambda_1}
    [1+\gamma\beta \lambda_1/\alpha]^{-\alpha}
    $$$$
    \leq
    f_1(0)
    e^{\gamma\beta \lambda_1}[1+\gamma\beta \lambda_1/\alpha]^{-\alpha}
    \left[1+\frac{\tau_k}{\alpha+\gamma\beta \lambda_1}\right]^{\gamma\beta \lambda_1}
    $$
    we have 
    $$
    U=
    \begin{bmatrix}
    1+\kappa/4+\sqrt{C} & 1+\kappa/4-\sqrt{C} \\
    1 & 1
    \end{bmatrix},
    $$
    and
    $$
    U^{-1}=
    \begin{bmatrix}
    \frac{1}{2\sqrt{C}} & -\frac{1+\kappa/4}{2\sqrt{C}}+\frac{1}{2} \\
    \frac{-1}{2\sqrt{C}} & \frac{1+\kappa/4}{2\sqrt{C}}+\frac{1}{2}
    \end{bmatrix},
    $$
    where $C=(1+\kappa/4)^2+2\omega$.\\
    From $\mathbf f_t=\mathbf U^{-1}\mathbf x(t)+\mathbf U^{-1}\mathbf B^{-1}\mathbf z$, we have
    \begin{align}
        &\mathbf x(t)=\mathbf U \mathbf f_t-\mathbf B^{-1}\mathbf z
        \nonumber \\&
        \leq    
        % \begin{bmatrix}
        %     1+\kappa/4+\sqrt{C} & 1+\kappa/4-\sqrt{C} \\
        %     1 & 1 
        % \end{bmatrix}
        \mathbf U
        \begin{bmatrix}
            c_1 & 0 \\
            0 & c_2
        \end{bmatrix}
        \begin{bmatrix}
            f_1(t_{k-1}) \\
            f_2(t_{k-1})
        \end{bmatrix} 
        % \nonumber \\&
        % - \frac{1}{\kappa+2\omega}
        % \begin{bmatrix}
        %     \kappa & 2\omega \\
        %     1 & -2
        % \end{bmatrix}
        -\mathbf B^{-1}
        \mathbf z
        \nonumber \\&
        =    
        \mathbf U
        \begin{bmatrix}
            c_1 & 0 \\
            0 & c_2
        \end{bmatrix}
        \Big(\mathbf U^{-1}\mathbf x(0)+\mathbf U^{-1}\mathbf B^{-1}\mathbf z\Big)
        -\mathbf B^{-1}
        \mathbf z
        \nonumber \\&
        =    
        \begin{bmatrix}
            1+\kappa/4+\sqrt{C} & 1+\kappa/4-\sqrt{C} \\
            1 & 1 
        \end{bmatrix}
        \begin{bmatrix}
            c_1 & 0 \\
            0 & c_2
        \end{bmatrix}
        \begin{bmatrix}
            \frac{1}{2\sqrt{C}} & \frac{1}{2}-\frac{1+\kappa/4}{2\sqrt{C}} \\
            -\frac{1}{2\sqrt{C}} & \frac{1}{2}+\frac{1+\kappa/4}{2\sqrt{C}}
        \end{bmatrix}
        \Big(\mathbf x(t_{k-1})+\mathbf B^{-1}\mathbf z\Big)
        -\mathbf B^{-1}
        \mathbf z
        \nonumber \\&
        =    
        \begin{bmatrix}
            \frac{c_1(1+\kappa/4+\sqrt{C})-c_2(1+\kappa/4-\sqrt{C})}{2\sqrt{C}} & (\frac{1}{2}-\frac{1+\kappa/4}{2\sqrt{C}})(1+\kappa/4+\sqrt{C})c_1
            +(\frac{1}{2}+\frac{1+\kappa/4}{2\sqrt{C}})(1+\kappa/4-\sqrt{C})c_2  \\
            \frac{1}{2\sqrt{C}}(c_1-c_2) & (\frac{1}{2}-\frac{1+\kappa/4}{2\sqrt{C}})c_1
            +(\frac{1}{2}+\frac{1+\kappa/4}{2\sqrt{C}})c_2 
        \end{bmatrix}
        \begin{bmatrix}
            0 \\ x_2(t_{k-1})
        \end{bmatrix}
        \nonumber \\&
        +
        \begin{bmatrix}
            \frac{c_1(1+\kappa/4+\sqrt{C})-c_2(1+\kappa/4-\sqrt{C})}{2\sqrt{C}}-1 & (\frac{1}{2}-\frac{1+\kappa/4}{2\sqrt{C}})(1+\kappa/4+\sqrt{C})c_1
            +(\frac{1}{2}+\frac{1+\kappa/4}{2\sqrt{C}})(1+\kappa/4-\sqrt{C})c_2  \\
            \frac{1}{2\sqrt{C}}(c_1-c_2) & (\frac{1}{2}-\frac{1+\kappa/4}{2\sqrt{C}})c_1
            +(\frac{1}{2}+\frac{1+\kappa/4}{2\sqrt{C}})c_2 -1
        \end{bmatrix}
        \mathbf B^{-1}\mathbf z
        \nonumber \\&
        =    
        \begin{bmatrix}
            \frac{c_1(1+\kappa/4+\sqrt{C})-c_2(1+\kappa/4-\sqrt{C})}{2\sqrt{C}} & (\frac{1}{2}-\frac{1+\kappa/4}{2\sqrt{C}})(1+\kappa/4+\sqrt{C})c_1
            +(\frac{1}{2}+\frac{1+\kappa/4}{2\sqrt{C}})(1+\kappa/4-\sqrt{C})c_2  \\
            \frac{1}{2\sqrt{C}}(c_1-c_2) & (\frac{1}{2}-\frac{1+\kappa/4}{2\sqrt{C}})c_1
            +(\frac{1}{2}+\frac{1+\kappa/4}{2\sqrt{C}})c_2 
        \end{bmatrix}
        \begin{bmatrix}
            0 \\ x_2(t_{k-1})
        \end{bmatrix}
        \nonumber \\&
        +
        \begin{bmatrix}
            (\frac{1}{2}-\frac{1+\kappa/4}{2\sqrt{C}})\Big[(1+\kappa/4+\sqrt{C})c_1+\kappa c_2\Big]
            +(\frac{1}{2}+\frac{1+\kappa/4}{2\sqrt{C}})\Big[\kappa c_1+(1+\kappa/4-\sqrt{C})c_2\Big]-\kappa 
            & 
            (1-\frac{1+\kappa/4}{\sqrt{C}})\Big[\omega c_2-(1+\kappa/4+\sqrt{C})c_1\Big]
            +(1+\frac{1+\kappa/4}{\sqrt{C}})\Big[\omega c_1-(1+\kappa/4-\sqrt{C})c_2\Big]-2\omega
            \\
            (\frac{1}{2}-\frac{1-3\kappa/4}{2\sqrt{C}})c_1
            +(\frac{1}{2}+\frac{1-3\kappa/4}{2\sqrt{C}})c_2-1
            & 
            (\frac{1+\omega+\kappa/4}{\sqrt{C}}-1)c_1
            -(1+\frac{1+\omega+\kappa/4}{\sqrt{C}})c_2+2
        \end{bmatrix}
        \mathbf z
    \end{align}
    where $c_1 = e^{\gamma\beta \lambda_1}[1+\gamma\beta \lambda_1/\alpha]^{-\alpha} \left[1+\frac{\tau_k}{\alpha+\gamma\beta \lambda_1}\right]^{\gamma\beta \lambda_1}$, $c_2=\frac{\alpha}{t+\alpha}$.
    Therefore, we have 
    \begin{align}
        &x_1(t)
        \leq
        \left[(\frac{1}{2}-\frac{1+\kappa/4}{2\sqrt{C}})(1+\kappa/4+\sqrt{C})c_1
            +(\frac{1}{2}+\frac{1+\kappa/4}{2\sqrt{C}})(1+\kappa/4-\sqrt{C})c_2\right] x_2(t_{k-1})
            \nonumber \\&
            +\frac{1}{\kappa+2\omega}\left[(\frac{1}{2}-\frac{1+\kappa/4}{2\sqrt{C}})\Big[(1+\kappa/4+\sqrt{C})c_1+\kappa c_2\Big]
            +(\frac{1}{2}+\frac{1+\kappa/4}{2\sqrt{C}})\Big[\kappa c_1+(1+\kappa/4-\sqrt{C})c_2\Big]-\kappa \right]z_1
            \nonumber \\&
            +\frac{1}{\kappa+2\omega}\left[(1-\frac{1+\kappa/4}{\sqrt{C}})\Big[\omega c_2-(1+\kappa/4+\sqrt{C})c_1\Big]
            +(1+\frac{1+\kappa/4}{\sqrt{C}})\Big[\omega c_1-(1+\kappa/4-\sqrt{C})c_2\Big]-2\omega\right]z_2
    \end{align}
    \begin{align}
        \mathbf B^{-1}=
        \frac{1}{\kappa+2\omega}
        \begin{bmatrix}
            \kappa & 2\omega \\
            1 & -2
        \end{bmatrix}
    \end{align}
    To find the condition for $\tau_k$ such that $c_1$ is the order of $\eta_t$, we have 
    \begin{align}
        &\frac{e^{\gamma\beta \lambda_1}}{[1+\gamma\beta \lambda_1/\alpha]^{\alpha}} \left[1+\frac{\tau_k}{\alpha+\gamma\beta \lambda_1}\right]^{\gamma\beta \lambda_1}\leq \frac{\gamma'}{t+L}
        \nonumber \\&
        \Rightarrow
        \left[1+\frac{\tau_k}{\alpha+\gamma\beta \lambda_1}\right]^{\gamma\beta \lambda_1}\leq \frac{[1+\gamma\beta \lambda_1/\alpha]^{\alpha}}{e^{\gamma\beta \lambda_1}}\frac{\gamma'}{t+L}
        \leq\frac{\gamma'}{t+L}=\frac{\gamma'}{\tau_k+t_{k-1}+L}
    \end{align}
    }
    By induction, we obtain
    \begin{align} 
        \mathbf x(t)
        &\leq 
        \mathbf M^t \mathbf e_2x_2(0)
        + \eta_k(\sigma+\beta\epsilon(k))\sum\limits_{j=0}^{t-1}\mathbf M^j\mathbf 1+\eta_k(\delta+\beta\epsilon(k)+(\sqrt{2}-1)\sigma)\sum\limits_{j=0}^{t-1}\mathbf M^j\mathbf e_1.
    \end{align}
    Then we have 
    \begin{align} \label{x_1c}
        &\sqrt{\mathbb E[\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert^2]}
        \nonumber \\&
        \leq
        \eta_k\beta
        \begin{bmatrix}
            1 & 2\omega    
        \end{bmatrix}
        \sum\limits_{j=1}^t(1+\eta_k\beta)^{t-j}\mathbf x(j)+[(1+\eta_k\beta)^t-1][\epsilon(k)+\epsilon_c(k)+\sqrt{2}\sigma/\beta+\delta/\beta]
        \nonumber \\&
        \leq
        x_2(0)\mathbf g_t(0)\mathbf e_2+(\sigma/\beta+\epsilon(k))\mathbf g_t(1)\mathbf 1+(\delta+\beta\epsilon(k)+(\sqrt{2}-1)\sigma)\mathbf g_t(1)\mathbf e_1
        \nonumber \\&
        +[(1+\eta_k\beta)^t-1][\epsilon(k)+\epsilon_c(k)+\sqrt{2}\sigma/\beta+\delta/\beta],
    \end{align}
    where
    \begin{align}
        &\mathbf g_t(x)=\beta\eta_k\sum\limits_{j=1}^t[(1+\eta_k\beta)^{t-j}-x]
        \begin{bmatrix}
            1&2\omega
        \end{bmatrix}
        \mathbf M^j
        \nonumber \\&
        =\frac{1}{2\sqrt{C}}
        \begin{bmatrix}
            \frac{\lambda_1^t-(1+\eta_k\beta)^t}{-\kappa/4+\sqrt{C}}-x\frac{\lambda_1^t-1}{1-\kappa/4+\sqrt{C}} \\ \frac{\lambda_2^t-(1+\eta_k\beta)^t}{-\kappa/4+\sqrt{C}}-x\frac{\lambda_2^t-1}{1-\kappa/4+\sqrt{C}}
        \end{bmatrix}^T
        \begin{bmatrix}
            1+\kappa/4+2\omega+\sqrt{C} & 2\omega\frac{1+\kappa/2+2\omega}{\sqrt{C}+\kappa/4}\\
            -\omega\frac{2+4\omega+\kappa}{\sqrt{C}+1+\kappa/4} & 2\omega(\kappa/4+\sqrt{C})
        \end{bmatrix}
        \nonumber \\&
        =x
        \begin{bmatrix}
            (A_1(t)-A_0(t))+A_0(t)\frac{\kappa/2}{\kappa+2\omega} \\
            A_0(t)\frac{2\omega}{\kappa+2\omega}
        \end{bmatrix}^T
        +(1-x)A_1(t)\mathbf e_i^t
        +B(t)
        \begin{bmatrix}
            1+\kappa/4-x\frac{\kappa/2[1+\kappa/4+\omega]}{\kappa+2\omega}\\
            2(1-x)\omega-2x\omega\frac{1-\kappa/4}{\kappa+2\omega}
        \end{bmatrix}^T,
    \end{align}
    where $$A_x(t)=[\lambda_1^t+\lambda_2^t-2]/2+x[1-(1+\eta_k\beta)^t]$$ and $$B(t)=[\lambda_1^t-\lambda_2^t]/2/\sqrt{C}.$$
    Since $2\omega/\sqrt{C}$ is a concave increasing function of $\omega$, for $\mathbf g_t(0)\mathbf e_2$, we can upper bound
    \begin{align}
        \mathbf g_t(0)\mathbf e_2&=\frac{\omega}{\sqrt{C}}[\lambda_1^t-\lambda_2^t]
        % \nonumber \\&
        \leq \frac{2\omega}{\sqrt{(1+\kappa/4)^2+2\omega}}[\lambda_1^t-1]
        % \nonumber \\&
        \leq 2\omega[\lambda_1^t-1].
    \end{align}
    For $\mathbf g_t(1)\mathbf 1$, we can upper bound
    \begin{align}
        &\mathbf g_t(1)\mathbf 1=1-(1+\eta_k\beta)^t+\frac{\kappa[1+\kappa/4+\omega]+2\sqrt{C}[\kappa/2+2\omega]}{4\sqrt{C}[\kappa+2\omega]}[\lambda_1^t-1]
        \nonumber \\&
        -\frac{2\sqrt{C}[\kappa/2+2\omega]-\kappa[1+\kappa/4+\omega]}{4\sqrt{C}[\kappa+2\omega]}[1-\lambda_2^t]
        \nonumber \\&
        \overset{(a)}{\leq}
        1-(1+\eta_k\beta)^t+\left[\frac{\kappa[1+\kappa/4+\omega]}{4\sqrt{C}[\kappa+2\omega]}+\frac{[\kappa/2+2\omega]}{2[\kappa+2\omega]}\right][\lambda_1^t-1]
        \nonumber \\&
        \overset{(b)}{\leq}
        1-(1+\eta_k\beta)^t+7/8[\lambda_1^t-1],
    \end{align}
    where $(a)$ holds because $2\sqrt{C}[\kappa/2+2\omega]-\kappa[1+\kappa/4+\omega]>0$ and $(b)$ comes from the fact that $1/2\leq\left[\frac{\kappa[1+\kappa/4+\omega]}{4\sqrt{C}[\kappa+2\omega]}+\frac{[\kappa/2+2\omega]}{2[\kappa+2\omega]}\right]\leq7/8$.
    Then for $\mathbf g_t(1)\mathbf e_1$, we can upper bound
    \begin{align}
        &\mathbf g_t(1)\mathbf e_1 \leq 1-(1+\eta_k\beta)^t+\frac{\kappa[1+4/\kappa]+2\omega}{2\sqrt{C}[\kappa+2\omega]}[\lambda_1^t-1]
        \nonumber \\&
        \overset{(a)}{\leq}
        1-(1+\eta_k\beta)^t+1/2[\lambda_1^t-1],
    \end{align}
    where $(a)$ holds because $1/4\leq\frac{\kappa[1+4/\kappa]+2\omega}{2\sqrt{C}[\kappa+2\omega]}\leq1/2$.
    Therefore, we can upper bound \eqref{x_1c} as
    \begin{align}
        &\sqrt{\mathbb E[\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert^2]}
        \nonumber \\&
        \leq [\lambda_1^t-1]\{2\omega\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert+11/8\epsilon+9/8\frac{\sigma}{\beta}+1/2\frac{\delta}{\beta}\}+[(1+\eta_k\beta)^t-1][\epsilon_c(k)-\epsilon(k)]
    \end{align}
    We would then further obtain
    \begin{align}
        &\sum\limits_{c=1}^N\varrho_c^{(k)}\mathbb E[\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert^2]
        \nonumber\\&
        \leq [\lambda_1^{2t}-1]\{2\omega\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert+11/8\epsilon(k)+9/8\frac{\sigma}{\beta}+1/2\frac{\delta}{\beta}\}^2+[\lambda_1^{2t}-1]2\epsilon^2(k)
        \nonumber \\&
        \leq [\lambda_1^{2t}-1]\{16\omega^2\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert^2+153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
        \nonumber \\&
        \overset{(a)}{\leq}
        [\lambda_1^{2t}-1]\{32\omega^2/\mu[F\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)]+153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}
    \end{align}
    Follows from \eqref{errors}, we have
    \begin{align}
        &\mathbb E\left[\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_c^{(k)}}\sum_i\Vert\mathbf w_i(t)-\bar{\mathbf w}(t)\Vert_2^2\right]
        \nonumber \\&
        \leq
        [\lambda_1^{2t}-1]\{32\omega^2/\mu[F\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)]+153/16\epsilon^2(k)+81/16\frac{\sigma^2}{\beta^2}+\frac{\delta^2}{\beta^2}\}+\epsilon^2(k).
    \end{align}
    % }
    \iffalse
    To obtain the eigenvalue of $M$, we have
    \begin{align}
        &\det(M-\lambda I) = \begin{vmatrix} 1+\eta_k\beta-\lambda & \eta_k\beta \\ \eta_k\zeta & 1+2\eta_k\beta-\lambda \end{vmatrix} = 0
        \nonumber \\&
        \Rightarrow (1+\eta_k\beta-\lambda)(1+2\eta_k\beta-\lambda) - \eta_k^2\zeta\beta=0
        \nonumber \\&
        \Rightarrow \lambda^2 - (3\eta_k\beta+2)\lambda+2\eta_k^2\beta^2+3\eta_k\beta-\eta_k^2\zeta\beta+1=0
        \nonumber \\&
        \Rightarrow \lambda = \frac{(3\eta_k\beta+2) \pm \sqrt{(3\eta_k\beta+2)^2-4(2\eta_k^2\beta^2+3\eta_k\beta-\eta_k^2\zeta\beta+1)}}{2}
        \nonumber \\&
        =\frac{\eta_k(3\beta\pm\sqrt{\beta^2+4\zeta\beta})}{2}+1.
    \end{align}
    Let $\lambda_1=\frac{\eta_k\beta(3+\sqrt{1+4\omega})}{2}+1$ and $v^{(1)}=\begin{bmatrix}v_1^{(1)} & v_2^{(1)}\end{bmatrix}^T$, we have
    \begin{align}
        &\begin{bmatrix} 1+\eta_k\beta-\frac{\eta_k\beta(3+\sqrt{1+4\omega})}{2}-1 & \eta_k\beta\\ \eta_k\zeta & 1+2\eta_k\beta-\frac{\eta_k\beta(3+\sqrt{1+4\omega})}{2}-1\end{bmatrix}\begin{bmatrix} v_1^{(1)} \\ v_2^{(1)} \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
        \nonumber \\&
        \Rightarrow
        \begin{bmatrix} \eta_k[\frac{-\beta-\sqrt{\beta^2+4\zeta\beta}}{2}] & \eta_k\beta\\ \eta_k\zeta & \eta_k[\frac{\beta-\sqrt{\beta^2+4\zeta\beta}}{2}]\end{bmatrix}\begin{bmatrix} v_1^{(1)} \\ v_2^{(1)} \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
        \nonumber \\&
        \Rightarrow v_2^{(1)} = \frac{1+\sqrt{1+4\omega}}{2} v_1^{(1)}.
    \end{align}
    If we let $v_1^{(1)}=2$, we have $v_2^{(1)} = 1 + \sqrt{1+4\omega}$. Thus, an eigenvector for $\lambda_1$ is $v^{(1)}=\begin{bmatrix} 2 & 1 + \sqrt{1+4\omega} \end{bmatrix}^T$. Similarly, let $\lambda_2=\frac{\eta_k\beta(3-\sqrt{1+4\omega})}{2}+1$ and $v^{(2)}=\begin{bmatrix}v_1^{(2)} & v_2^{(2)}\end{bmatrix}^T$, we have
    \begin{align}
        &\begin{bmatrix} 1+\eta_k\beta-\frac{\eta_k\beta(3-\sqrt{1+4\omega})}{2}-1 & \eta_k\beta\\ \eta_k\zeta & 1+2\eta_k\beta-\frac{\eta_k\beta(3-\sqrt{1+4\omega})}{2}-1\end{bmatrix}\begin{bmatrix} v_1^{(2)} \\ v_2^{(2)} \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
        \nonumber \\&
        \Rightarrow
        \begin{bmatrix} \eta_k[\frac{-\beta+\sqrt{\beta^2+4\zeta\beta}}{2}] & \eta_k\beta\\ \eta_k\zeta & \eta_k[\frac{\beta+\sqrt{\beta^2+4\zeta\beta}}{2}]\end{bmatrix}\begin{bmatrix} v_1^{(2)} \\ v_2^{(2)} \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
        \nonumber \\&
        \Rightarrow v_2^{(2)} = \frac{1-\sqrt{1+4\omega}}{2} v_1^{(2)}
    \end{align}
    If we let $v_1^{(2)}=2$, we have $v_2^{(2)} = 1 - \sqrt{1+4\omega}$. Thus, an eigenvector for $\lambda_2$ is $v^{(2)}=\begin{bmatrix} 2 & 1 - \sqrt{1+4\omega} \end{bmatrix}^T$.
    Therefore, we can diagonalize $M$ as 
    \begin{align}
        &M = \begin{bmatrix} v^{(1)T} & v^{(2)T} \end{bmatrix} \begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{bmatrix} \begin{bmatrix} v^{(1)T} & v^{(2)T} \end{bmatrix}^{-1}
        \nonumber \\&
        =\begin{bmatrix} 2 & 2 \\ 1 + \sqrt{1+4\omega} & 1 - \sqrt{1+4\omega} \end{bmatrix} 
        \begin{bmatrix} 1+\frac{\eta_k\beta(3+\sqrt{1+4\omega})}{2} & 0 \\ 0 & 1 + \frac{\eta_k\beta(3-\sqrt{1+4\omega})}{2} \end{bmatrix}
        \nonumber \\&
        \frac{-1}{4\sqrt{1+4\omega}}\begin{bmatrix} 1 - \sqrt{1+4\omega} & -2 \\ -1 - \sqrt{1+4\omega} & 2 \end{bmatrix}.
    \end{align}
    The coupled dynamics of the system in $\eqref{69}$ becomes
    \begin{align}
        &x(t)=\begin{bmatrix}\Vert\bar{\mathbf w}(t)-\mathbf w^*\Vert \\\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert\end{bmatrix}
        \leq \sum\limits_{i=0}^{t-1} M^{t-i-1}\begin{bmatrix} d_1+e_1(i) \\ d_2+2e_1(i) \end{bmatrix}
        +M^t x(0)
        \nonumber \\&
        = \sum\limits_{i=0}^{t-1} \begin{bmatrix} 2 & 2 \\ 1 + \sqrt{1+4\omega} & 1 - \sqrt{1+4\omega} \end{bmatrix} 
        \begin{bmatrix} \lambda_1^{t-i-1} & 0 \\ 0 & \lambda_2^{t-i-1} \end{bmatrix}
        % \nonumber \\&
        \begin{bmatrix} \frac{-1}{4\sqrt{1+4\omega}} + \frac{1}{4} & \frac{1}{2\sqrt{1+4\omega}} \\ \frac{1}{4\sqrt{1+4\omega}} + \frac{1}{4} & \frac{-1}{2\sqrt{1+4\omega}} \end{bmatrix}
        \begin{bmatrix} d_1+e_1(i) \\ d_2+2e_1(i) \end{bmatrix}
        \nonumber \\&
        +\begin{bmatrix} 2 & 2 \\ 1 + \sqrt{1+4\omega} & 1 - \sqrt{1+4\omega} \end{bmatrix}
        \begin{bmatrix} \lambda_1^{t} & 0 \\ 0 & \lambda_2^{t} \end{bmatrix}
        % \nonumber \\&
        \begin{bmatrix} \frac{-1}{4\sqrt{1+4\omega}} + \frac{1}{4} & \frac{1}{2\sqrt{1+4\omega}} \\ \frac{1}{4\sqrt{1+4\omega}} + \frac{1}{4} & \frac{-1}{2\sqrt{1+4\omega}} \end{bmatrix} \begin{bmatrix}\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert \\ 0 \end{bmatrix}.
    \end{align}
    \addFL{
    Then we obtain the following upper bound 
    \begin{align}
        &\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert
        \nonumber \\&
        \leq
        \sum\limits_{i=0}^{t-1}\bigg(\frac{d_1+e_1(i)}{\sqrt{1+4\omega}}\left[\omega(\lambda_1^{t-i-1}-\lambda_2^{t-i-1})\right]
        % \nonumber \\&
        +\frac{d_2+2e_1(i)}{2}\left[(\lambda_1^{t-i-1}+\lambda_2^{t-i-1})+\frac{\lambda_1^{t-i-1}-\lambda_2^{t-i-1}}{\sqrt{1+4\omega}}\right]\bigg)
        \nonumber \\&
        +\frac{\omega(\lambda_1^{t}-\lambda_2^{t})}{\sqrt{1+4\omega}}\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert
        \nonumber \\&
        =
        \frac{d_1\omega}{\sqrt{1+4\omega}}\sum\limits_{i=0}^{t-1}\left[\lambda_1^{t-i-1}-\lambda_2^{t-i-1}\right]
        % \nonumber \\&
        +\frac{d_2}{2}\sum\limits_{i=0}^{t-1}\left[(\lambda_1^{t-i-1}+\lambda_2^{t-i-1})+\frac{\lambda_1^{t-i-1}-\lambda_2^{t-i-1}}{\sqrt{1+4\omega}}\right]
        \nonumber \\&
        +\frac{\omega(\lambda_1^{t}-\lambda_2^{t})}{\sqrt{1+4\omega}}\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert
        +\sum\limits_{i=0}^{t-1}\bigg(\frac{e_1(i)}{\sqrt{1+4\omega}}\left[\omega(\lambda_1^{t-i-1}-\lambda_2^{t-i-1})\right]
        \nonumber \\&
        +e_1(i)\left[(\lambda_1^{t-i-1}+\lambda_2^{t-i-1})+\frac{\lambda_1^{t-i-1}-\lambda_2^{t-i-1}}{\sqrt{1+4\omega}}\right]\bigg)
        \nonumber \\&
        =
        \frac{2d_1\omega+d_2}{2\sqrt{1+4\omega}}\sum\limits_{i=0}^{t-1}\left[\lambda_1^{t-i-1}-\lambda_2^{t-i-1}\right]
        % \nonumber \\&
        +\frac{d_2}{2}\sum\limits_{i=0}^{t-1}\left[\lambda_1^{t-i-1}+\lambda_2^{t-i-1}\right]
        +\frac{\omega(\lambda_1^{t}-\lambda_2^{t})}{\sqrt{1+4\omega}}\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert
        \nonumber \\&
        +\sum\limits_{i=0}^{t-1}\bigg(\frac{(1+\omega)e_1(i)}{\sqrt{1+4\omega}}(\lambda_1^{t-i-1}-\lambda_2^{t-i-1})
        +e_1(i)(\lambda_1^{t-i-1}+\lambda_2^{t-i-1})\bigg)
        \nonumber \\&
        =
        \sum\limits_{i=0}^{t-1}\left((\frac{d_2}{2}+\frac{2d_1\omega+d_2}{2\sqrt{1+4\omega}})\lambda_1^{t-i-1}+(\frac{d_2}{2}-\frac{2d_1\omega+d_2}{2\sqrt{1+4\omega}})\lambda_2^{t-i-1}\right)
        +\frac{\omega(\lambda_1^{t}-\lambda_2^{t})}{\sqrt{1+4\omega}}\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert
        \nonumber \\&
        +\sum\limits_{i=0}^{t-1}e_1(i)\bigg[(1+\frac{1+\omega}{\sqrt{1+4\omega}})\lambda_1^{t-i-1}
        +(1-\frac{1+\omega}{\sqrt{1+4\omega}})\lambda_2^{t-i-1})\bigg]
        \nonumber \\&
        \leq
        d_2\sum\limits_{i=0}^{t-1}\lambda_1^{t-i-1}
        +\omega(\lambda_1^{t}-1)\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert
        \nonumber \\&
        +\sum\limits_{i=0}^{t-1}e_1(i)\bigg[(1+\frac{1+\omega}{\sqrt{1+4\omega}})\lambda_1^{t-i-1}
        +(1-\frac{1+\omega}{\sqrt{1+4\omega}})\lambda_2^{t-i-1})\bigg]
    \end{align}
    Since $\eta_k\beta\leq 1$, replacing $\lambda_1=\frac{\eta_k\beta(3+\sqrt{1+4\omega})}{2}+1$, we have
    \begin{align}
        &\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert
        \leq
        \frac{d_2}{2}(4^t-1)
        +\omega(4^t-1)\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert
        \nonumber \\&
        +\sum\limits_{i=0}^{t-1}e_1(i)\bigg[(1+\frac{1+\omega}{\sqrt{1+4\omega}})\lambda_1^{t-i-1}
        +(1-\frac{1+\omega}{\sqrt{1+4\omega}})\lambda_2^{t-i-1})\bigg]
        \nonumber \\&
        \leq
        \frac{4^t}{2}d_2
        +4^t\omega\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert
        % \nonumber \\&
        +\sum\limits_{i=0}^{t-1}e_1(i)\bigg[(1+\frac{1+\omega}{\sqrt{1+4\omega}})\lambda_1^{t-i-1}
        +(1-\frac{1+\omega}{\sqrt{1+4\omega}})\lambda_2^{t-i-1})\bigg]
    \end{align}
    Then, we can further upper bound 
    \begin{align}
        &\mathbb E\left(\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert\right)^2
        % \nonumber \\&
        \leq
        \bigg[\frac{4^t}{2}d_2
        +4^t\omega\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert\bigg]^2
        \nonumber \\&
        +\mathbb E\bigg[\sum\limits_{i=0}^{t-1}e_1(i)\Big[(1+\frac{1+\omega}{\sqrt{1+4\omega}})\lambda_1^{t-i-1}
        +(1-\frac{1+\omega}{\sqrt{1+4\omega}})\lambda_2^{t-i-1})\Big]\bigg]^2
        \nonumber \\&
        =
        \bigg[\frac{4^t}{2}d_2
        +4^t\omega\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert\bigg]^2
        \nonumber \\& \label{56}
        +\sum\limits_{i=0}^{t-1}\bigg[(1+\frac{1+\omega}{\sqrt{1+4\omega}})\lambda_1^{t-i-1}
        +(1-\frac{1+\omega}{\sqrt{1+4\omega}})\lambda_2^{t-i-1}\bigg]^2\mathbb E[e_1(i)]^2
    \end{align}
    Since 
    \begin{align}
        &\mathbb E[e_1(t)]^2=\mathbb E\left[\eta_k \sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Big(\Vert\widehat{\mathbf g}_{j,t}-\nabla F_j(\mathbf w_j(t))\Vert-\mathbb E\Vert\widehat{\mathbf g}_{j,t}-\nabla F_j(\mathbf w_j(t))\Vert\Big)\right]^2
        \nonumber \\&
        \leq
        \eta_k \sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\mathbb E\Big(\Vert\widehat{\mathbf g}_{j,t}-\nabla F_j(\mathbf w_j(t))\Vert-\mathbb E\Vert\widehat{\mathbf g}_{j,t}-\nabla F_j(\mathbf w_j(t))\Vert\Big)^2
        \nonumber \\&
        =
        \eta_k \sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\mathbb E\Big(\Vert\widehat{\mathbf g}_{j,t}-\nabla F_j(\mathbf w_j(t))\Vert^2+\big(\mathbb E\Vert\widehat{\mathbf g}_{j,t}-\nabla F_j(\mathbf w_j(t))\Vert\big)^2
        \nonumber \\&
        -2\langle\Vert\widehat{\mathbf g}_{j,t}-\nabla F_j(\mathbf w_j(t))\Vert,\mathbb E\Vert\widehat{\mathbf g}_{j,t}-\nabla F_j(\mathbf w_j(t))\Vert\rangle\Big)
        \nonumber \\&
        =
        \eta_k \sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Big[\mathbb E\Vert\widehat{\mathbf g}_{j,t}-\nabla F_j(\mathbf w_j(t))\Vert^2-\big(\mathbb E\Vert\widehat{\mathbf g}_{j,t}-\nabla F_j(\mathbf w_j(t))\Vert\big)^2\Big]
        \nonumber \\&
        \leq 
        \eta_k \sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\mathbb E\Vert\widehat{\mathbf g}_{j,t}-\nabla F_j(\mathbf w_j(t))\Vert^2
        \leq
        \eta_k\sigma^2,
    \end{align}
    \eqref{56} can be bounded as 
    \begin{align}
        &\mathbb E\left(\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert\right)^2
        % \nonumber \\&
        \leq
        \bigg[\frac{4^t}{2}d_2
        +4^t\omega\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert\bigg]^2
        \nonumber \\& 
        +\eta_k\sigma^2\sum\limits_{i=0}^{t-1}\bigg[(1+\frac{1+\omega}{\sqrt{1+4\omega}})\lambda_1^{t-i-1}
        +(1-\frac{1+\omega}{\sqrt{1+4\omega}})\lambda_2^{t-i-1}\bigg]^2
        \nonumber \\& 
        \leq
        \bigg[\frac{4^t}{2}d_2
        +4^t\omega\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert\bigg]^2
        +2\eta_k\sigma^2\sum\limits_{i=0}^{t-1}\lambda_1^{2(t-i-1)}
        \nonumber \\& 
        \leq
        \bigg[\frac{4^t}{2}d_2
        +4^t\omega\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert\bigg]^2
        +\frac{1}{4}\eta_k\sigma^2(16^t-1)
        \nonumber \\& \label{58}
        \leq
        \bigg[\frac{4^t}{2}d_2
        +4^t\omega\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert\bigg]^2
        +\frac{16^t}{4}\eta_k\sigma^2.
    \end{align}
    By choosing $\phi=\max\limitS_i\frac{1}{\rho_i}$, we have 
    \begin{align} \label{59}
        &\sum\limits_{c=1}^N\varrho_{c}\mathbb E\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert^2
        \leq \phi\mathbb E\left(\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert\right)^2.
    \end{align}
    Combining \eqref{58} and \eqref{59}, we obtain
    \begin{align}
        &\sum\limits_{c=1}^N\varrho_{c}\mathbb E\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert^2
        % \nonumber \\&
        \leq 
        \phi\bigg[\frac{4^t}{2}d_2
        +4^t\omega\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert\bigg]^2
        +\frac{16^t}{4}\phi\eta_k\sigma^2
        \nonumber \\&
        \leq
        \frac{16^t}{4}\phi (\eta_k\sigma^2+2d_2^2)
        +2\cdot16^t\omega^2\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert^2
        \nonumber \\&
        \leq
        \frac{16^t}{4}\phi \big(\eta_k\sigma^2+2\eta_k^2[\delta+2\sigma+2\beta\sum\limits_{c=1}^N\varrho_{c}\epsilon_c(k)]^2\big)
        +2\cdot16^t\omega^2\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert^2.
    \end{align}
    From \eqref{errors}, we have 
    \begin{align}
        &\mathbb E\left[\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_c^{(k)}}\sum_i\Vert\mathbf w_i(t)-\bar{\mathbf w}(t)\Vert_2^2\right]
        \nonumber \\& \label{62}
        \leq
        \frac{16^t}{4}\phi \big(\eta_k\sigma^2+2\eta_k^2[\delta+2\sigma+2\beta\sum\limits_{c=1}^N\varrho_{c}\epsilon_c(k)]^2\big)
        +2\cdot16^t\omega^2\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert^2 
        +\sum\limits_{c=1}^N\varrho_{c}\epsilon_c^2(k).
    \end{align}
    By strong convexity of $F(\cdot)$, \eqref{62} can be further bounded as 
    \begin{align}
        &\mathbb E\left[\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_c^{(k)}}\sum_i\Vert\mathbf w_i(t)-\bar{\mathbf w}(t)\Vert_2^2\right]
        \nonumber \\&
        \leq
        \frac{16^t}{4}\phi \big(\eta_k\sigma^2+2\eta_k^2[\delta+2\sigma+2\beta\sum\limits_{c=1}^N\varrho_{c}\epsilon_c(k)]^2\big)
        +4\cdot16^t\frac{\omega^2}{\mu}[F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)]
        +\sum\limits_{c=1}^N\varrho_{c}\epsilon_c^2(k).
    \end{align}
    }
    \fi
    \iffalse
    \add{
    \textbf{Consider the case that we do not do consensus in every round of local update:}\\
    For $x_1(t)=\Vert\bar{\mathbf w}(t)-\mathbf w^*\Vert \leq (1+\eta_k\beta)\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert$:
    \begin{align}
        &\Vert\bar{\mathbf w}(t)-\mathbf w^*\Vert \leq (1+\eta_k\beta)\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert
        \nonumber \\&
        +\eta_k \sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}} \Vert\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\mathbf w_j(t-1))\Vert
        \nonumber \\&
        +\eta_k\beta \sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}} \Vert\mathbf w_j(t-1)-\bar{\mathbf w}_c(t-1)\Vert
        \nonumber \\&
        +\eta_k\beta \sum\limits_{c=1}^N\varrho_{c} \Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert.
    \end{align}
    For $x_2(t)=\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert$:
    \begin{align} 
        &\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert
        \leq \sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert
        \nonumber \\&
        +2\eta_k\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\left\Vert\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\mathbf w_j(t-1))\right\Vert
        \nonumber \\&
        +2\eta_k\beta\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\left\Vert\mathbf w_j(t-1)-\bar{\mathbf w}_c(t-1)\right\Vert
        \nonumber \\&
        +2\eta_k\beta\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert
        \nonumber \\&
        +\eta_k\sum\limits_{c=1}^N\varrho_{c}\Vert\nabla\hat F_c(\bar{\mathbf w}(t-1))-\nabla F(\bar{\mathbf w}(t-1))\Vert.
    \end{align} 
    For $x_3(t)=\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Vert\mathbf w_j(t)-\bar{\mathbf w}_c(t)\Vert$:
    \begin{align}
        \mathbf w_i(t) = \sum\limits_{j\in\mathcal S_{c}}m_{i,j}^{(c)}[\mathbf w_j(t-1)-\eta_k\widehat{\mathbf g}_{j,t-1}],
    \end{align}
    we have
    \begin{align} 
        &\mathbf w_i(t)-\bar{\mathbf w}_c(t)
        % \nonumber \\&
        = \sum\limits_{j\in\mathcal S_{c}}(m_{i,j}^{(c)}-\frac{1}{s_{c}})\Big(\mathbf w_j(t-1)-\bar{\mathbf w}_c(t-1)-\eta_k[\widehat{\mathbf g}_{j,t-1}-\nabla F(\bar{\mathbf w} (t-1))]\Big)
    \end{align} 
    such that
    \begin{align}
        &\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Vert\mathbf w_j(t)-\bar{\mathbf w}_c(t)\Vert
        \nonumber \\&
        \leq
        \frac{\lambda_c^{\Gamma_c^{k}}}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Big\Vert\mathbf w_j(t-1)-\bar{\mathbf w}_c(t-1)-\eta_k[\widehat{\mathbf g}_{j,t-1}-\nabla F(\bar{\mathbf w} (t-1))]\Big\Vert
        \nonumber \\&
        \leq 
        \frac{\lambda_c^{\Gamma_c^{k}}}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Big\Vert\mathbf w_j(t-1)-\bar{\mathbf w}_c(t-1)-\eta_k[\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\mathbf w_j (t-1))+\nabla F_j(\bar{\mathbf w}_j (t-1))-\nabla F_j(\mathbf w_c (t-1))
        \nonumber \\&
        +\nabla F_j(\bar{\mathbf w}_c (t-1))-\nabla F_j(\bar{\mathbf w} (t-1))+\nabla F_j(\bar{\mathbf w} (t-1))-\nabla F(\bar{\mathbf w}(t-1))]\Big\Vert
        \nonumber \\&
        \leq 
        \frac{\lambda_c^{\Gamma_c^{k}}}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Big[\Vert\mathbf w_j(t-1)-\bar{\mathbf w}_c(t-1)\Vert
        \nonumber \\&
        +\eta_k\Vert\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\mathbf w_j (t-1))\Vert +\eta_k\Vert\nabla F_j(\mathbf w_j (t-1))-\nabla F_j(\bar{\mathbf w}_c (t-1))\Vert
        \nonumber \\&
        +\eta_k\Vert\nabla F_j(\bar{\mathbf w}_c (t-1))-\nabla F_j(\bar{\mathbf w} (t-1))\Vert
        +\eta_k\Vert\nabla F_j(\bar{\mathbf w} (t-1))-\nabla F(\bar{\mathbf w} (t-1))\Vert\Big]
    \end{align}
    where $\lambda_c$ is the spectral radius and $\Gamma_c^{k}$ is the number of consensus for cluster $c$ at $t\in\mathcal T_k$. By $\beta$-smoothness of $F(\cdot)$, we further bound
    \begin{align}
        &\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Vert\mathbf w_j(t)-\bar{\mathbf w}_c(t)\Vert
        \nonumber \\&
        \leq 
        \frac{\lambda_c^{\Gamma_c^{k}}}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Big[(1+\eta_k\beta)\Vert\mathbf w_j(t-1)-\bar{\mathbf w}_c(t-1)\Vert+\eta_k\Vert\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\mathbf w_j (t-1))\Vert
        \nonumber \\&
        +\eta_k\beta\Vert\bar{\mathbf w}_c (t-1)-\bar{\mathbf w} (t-1)\Vert
        +\eta_k\Vert\nabla F_j(\bar{\mathbf w} (t-1))-\nabla F(\bar{\mathbf w} (t-1))\Vert\Big]
    \end{align}
    By Definition of $\Vert\nabla F_i(\mathbf w)-\nabla F(\mathbf w)\Vert
    \leq \delta+ \zeta \Vert\mathbf w-\mathbf w^*\Vert$, we have
    \begin{align}
        &\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Vert\mathbf w_j(t)-\bar{\mathbf w}_c(t)\Vert
        \nonumber \\&
        \leq 
        \lambda_c^{\Gamma_c^{k}} \Big[(1+\eta_k\beta)\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Vert\mathbf w_j(t-1)-\bar{\mathbf w}_c(t-1)\Vert+\eta_k\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Vert\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\mathbf w_j (t-1))\Vert
        \nonumber \\&
        +\eta_k\beta\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c (t-1)-\bar{\mathbf w} (t-1)\Vert
        +\eta_k\delta+\eta_k\zeta\Vert\bar{\mathbf w} (t-1)-\bar{\mathbf w}^*\Vert\Big]
    \end{align}
    Finally, we obtain the coupled dynamics of the system as follows:
    \begin{align} 
        x(t)=\begin{bmatrix}x_1(t)\\x_2(t)\\x_3(t)\end{bmatrix}
        \leq \begin{bmatrix} 
            1+\eta_k\beta & \eta_k\beta & \eta_k\beta\\ 
            \eta_k\zeta & 1+2\eta_k\beta & 2\eta_k\beta\\
            \lambda_c^{\Gamma_c^{k}} \eta_k\zeta &\lambda_c^{\Gamma_c^{k}}\eta_k\beta & \lambda_c^{\Gamma_c^{k}}(1+\eta_k\beta)
        \end{bmatrix} 
        \begin{bmatrix}
            x_1(t-1)\\x_2(t-1)\\x_3(t-1)
        \end{bmatrix}
        +\begin{bmatrix}
            e_1(t-1)\\e_2(t-1)\\e_3(t-1)
         \end{bmatrix}
        +\begin{bmatrix}
            d_1\\d_2\\d_3\end{bmatrix}.
         \end{align}
    }
    \fi
\end{proof}

\addFL{
\begin{proposition}\label{lem3}
Under Assumption \ref{beta} and Algorithm \ref{GT}, for $\eta_k=\max_t\{\eta_t\}\leq\frac{\kappa}{\beta}$ and $t \in \mathcal T_k$, we have
    \begin{align*}
        &\mathbb E\left[\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_c^{(k)}}\sum_i\Vert\mathbf w_i(t)-\bar{\mathbf w}(t)\Vert_2^2\right]
        \nonumber \\&
        \leq
        [\lambda_1^{2t}-1]\{\frac{32}{\mu}\omega^2 [F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)]+16\epsilon^2(k)+18\sigma^2/\beta^2+16\delta^2/\beta^2\}+\epsilon^2(k).
    \end{align*}
\end{proposition} 
\begin{proof}
    From 
    \begin{align}
        &\bar{\mathbf w}(t)-\mathbf w^* = \bar{\mathbf w}(t-1)-\mathbf w^*-\eta_t \nabla F(\bar{\mathbf w}(t-1))
        \nonumber \\&
        -\eta_t \sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}} [\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\mathbf w_j(t-1))]
        \nonumber \\&
        -\eta_t \sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}} [\nabla F_j(\mathbf w_j(t-1))-\nabla F_j(\bar{\mathbf w}_c(t-1))]
        \nonumber \\&
        -\eta_t \sum\limits_{c=1}^N\varrho_{c} [\nabla F_c(\bar{\mathbf w}_c(t-1))-\nabla F_c(\bar{\mathbf w}(t-1))],
    \end{align}
    we have
    \begin{align} 
        &\sqrt{\mathbb E[\Vert\bar{\mathbf w}(t)-\mathbf w^*\Vert^2]} 
        % \nonumber \\&
        \leq
        (1+\eta_t\beta)\sqrt{\mathbb E[\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert^2]}
        \nonumber \\&
        +\eta_t\beta \sqrt{\mathbb E[(\sum\limits_{c=1}^N\varrho_{c} \Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert)^2]}
        % \nonumber \\&
        +\eta_t(\beta\epsilon(k)+\sigma),
    \end{align}
    and from 
    \begin{align} 
        &\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)=\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)
        \nonumber \\&
        -\eta_t\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Big[\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\bar{\mathbf w}_j (t-1))\Big]
        +\eta_t\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Big[\widehat{\mathbf g}_{j,t-1}-\nabla F_j(\bar{\mathbf w}_j(t-1))\Big]
        \nonumber \\&
        -\eta_t\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Big[\nabla F_j(\bar{\mathbf w}_j (t-1))-\nabla F_j(\bar{\mathbf w}_c (t-1))\Big]
        +\eta_t\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_{c}}\sum\limits_{j\in\mathcal S_{c}}\Big[\nabla F_j(\bar{\mathbf w}_j (t-1))-\nabla F_j(\bar{\mathbf w}_c(t-1))\Big]
        \nonumber \\&
        -\eta_t\Big[\nabla\hat F_c(\bar{\mathbf w}_c(t-1))-\nabla\hat F_c(\bar{\mathbf w}(t-1))\Big]
        +\eta_t\sum\limits_{c=1}^N\varrho_{c}\Big[\nabla\hat F_c(\bar{\mathbf w}_c(t-1))-\nabla\hat F_c(\bar{\mathbf w}(t-1))\Big]
        \nonumber \\&
        -\eta_t\Big[\nabla\hat F_c(\bar{\mathbf w}(t-1))-\nabla F(\bar{\mathbf w}(t-1))\Big],
    \end{align}   
    we have
    \begin{align}
        &\sqrt{\mathbb E[(\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert)^2]}
        \leq (1+2\eta_t\beta)\sqrt{\mathbb E[(\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert)^2]}
        \nonumber \\&
        +\eta_t\zeta\sqrt{\mathbb E[\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert^2]}
        % \nonumber \\&
        +\eta_t(\delta+2\sigma+\sqrt{2}\beta\epsilon(k)).
    \end{align}
    Let $x_1(t)=\sqrt{\mathbb E[\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert^2]}$, $x_2(t)=\sqrt{\mathbb E[(\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert)^2]}$, $d_1=\eta_k(\sigma+\beta\epsilon(k))$, $d_2=\eta_k(\delta+\sqrt{2}\sigma+2\beta\epsilon(k))$, where $\eta_k=\max_t\{\eta_t\}$ we obtain the coupled dynamics of the system as follows:
    \begin{align} \label{69}
        \mathbf x(t)=\begin{bmatrix}x_1(t)\\x_2(t)\end{bmatrix}
        &\leq 
        \begin{bmatrix} 1+\eta_k\beta & \eta_k\beta\\ \eta_k\zeta & 1+2\eta_k\beta\end{bmatrix} \begin{bmatrix}x_1(t-1)\\x_2(t-1)\end{bmatrix}
        + \begin{bmatrix}d_1\\d_2\end{bmatrix}.
    \end{align}
    Let $\mathbf M=\begin{bmatrix} 1+\eta_k\beta & \eta_k\beta\\ \eta_k\zeta & 1+2\eta_k\beta\end{bmatrix}$, we then have
    % \begin{align}
    %     \mathbf x(t) \leq
    %     \mathbf M^t\mathbf e_1 x_1(0)+\eta_k[\beta\epsilon(k)+\sigma]\sum\limits_{i=0}^{t-1} \mathbf M^{i}\mathbf 1+\eta_k[\delta+\beta\epsilon(k)+(\sqrt{2}-1)\sigma]\sum\limits_{i=0}^{t-1} \mathbf M^{i}\mathbf e_2.
    % \end{align}
    \begin{align} \label{50}
        x_1(t)
        \leq \begin{bmatrix} 1 & 0 \end{bmatrix}
        \sum\limits_{i=1}^{t} \mathbf M^{t-i}\begin{bmatrix} d_1 \\ d_2 \end{bmatrix}
        +\begin{bmatrix} 1 & 0 \end{bmatrix}\mathbf M^t \begin{bmatrix}x_1(t_{k-1}) \\ 0 \end{bmatrix}
    \end{align}
    and 
    \begin{align} \label{51}
        &x_2(t)
        \leq \begin{bmatrix} 0 & 1 \end{bmatrix}
        \sum\limits_{i=1}^{t} \mathbf M^{t-i}\begin{bmatrix} d_1 \\ d_2 \end{bmatrix}
        +\begin{bmatrix} 0 & 1 \end{bmatrix}\mathbf M^t \begin{bmatrix}x_1(t_{k-1}) \\ 0 \end{bmatrix}.
    \end{align}
    % \addFL{
    By eigen-decomposition, we have
    \begin{align} 
        &\mathbf M^t = \begin{bmatrix} v_1^{(1)} & v_1^{(2)} \\ v_2^{(1)} & v_2^{(2)} \end{bmatrix}
        \begin{bmatrix}
            \lambda_1^t & 0 \\
            0 & \lambda_2^t
        \end{bmatrix} 
        \begin{bmatrix} \hat{v_1}^{(1)} & \hat{v_1}^{(2)} \\ \hat{v_2}^{(1)} & \hat{v_2}^{(2)} \end{bmatrix}
        \nonumber \\&
        =
        \begin{bmatrix} 2 & 2 \\ 1 + \sqrt{1+8\omega} & 1 - \sqrt{1+8\omega} \end{bmatrix}
        \begin{bmatrix} (\frac{\eta_k\beta(3+\sqrt{1+8\omega})}{2}+1)^{t} & 0 \\ 0 & (\frac{\eta_k\beta(3-\sqrt{1+8\omega})}{2}+1)^{t} \end{bmatrix}
        % \nonumber \\&
        \begin{bmatrix} \frac{-1}{4\sqrt{1+8\omega}} + \frac{1}{4} & \frac{1}{2\sqrt{1+8\omega}} \\ \frac{1}{4\sqrt{1+8\omega}} + \frac{1}{4} & \frac{-1}{2\sqrt{1+8\omega}} \end{bmatrix}.
    \end{align}
    % \addFL{
    Substitute $\mathbf M^t$ into \eqref{50} and \eqref{51}, we have 
    \begin{align} \label{x1}
        &x_1(t)
        \leq 
        \sum\limits_{i=1}^{t}\big[(\lambda_1^{t-i} v_1^{(1)}\hat{v_1}^{(1)}+\lambda_2^{t-i} v_1^{(2)}\hat{v_2}^{(1)})d_1+(\lambda_1^{t-i} v_1^{(1)}\hat{v_1}^{(2)}+\lambda_2^{t-i} v_1^{(2)}\hat{v_2}^{(2)})d_2\big]
        \nonumber \\&
        +(\lambda_1^{t} v_1^{(1)}\hat{v_1}^{(1)}+\lambda_2^{t} v_1^{(2)}\hat{v_2}^{(1)})x_1(t_{k-1}) 
    \end{align}
    and
    \begin{align} \label{x2}
        &x_2(t)
        \leq 
        \sum\limits_{i=1}^{t} \big[(\lambda_1^{t-i} v_2^{(1)}\hat{v_1}^{(1)}+\lambda_2^{t-i} v_2^{(2)}\hat{v_2}^{(1)})d_1+(\lambda_1^{t-i} v_2^{(1)}\hat{v_1}^{(2)}+\lambda_2^{t-i} v_2^{(2)}\hat{v_2}^{(2)})d_2\big]
        \nonumber \\&
        +(\lambda_1^{t} v_2^{(1)}\hat{v_1}^{(1)}+\lambda_2^{t} v_2^{(2)}\hat{v_2}^{(1)})x_1(t_{k-1})
    \end{align}
    From \eqref{48}, we have
    \begin{align} \label{54}
        &\sqrt{\mathbb E_t[\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert^2]}
        \nonumber \\&
        \leq
        (1+\eta_k\beta)\sqrt{\mathbb E_t[\Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert^2]}
        % \nonumber \\&
        +\eta_k\zeta\sqrt{\mathbb E_t\big[\Vert\bar{\mathbf w}(t-1)-\mathbf w^*\Vert^2\big]}
        \nonumber \\&
        +\eta_k\beta\sqrt{\mathbb E_t\Big[\big(\sum\limits_{c=1}^N\varrho_{c}\Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert\big)^2\Big]}
        % \nonumber \\&
        +\eta_k(\sqrt{2}\sigma+\beta\epsilon_c(k)+\beta\epsilon(k)+\delta).
    \end{align}   
    Combining \eqref{x1}, \eqref{x2} and \eqref{54}, we then obtain
    \begin{align} \label{55}
        &\sqrt{\mathbb E_t[\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert^2]}
        \leq 
        (1+\eta_k\beta)\sqrt{\mathbb E_t[\Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert^2]}
        \nonumber \\&
        +\eta_k\zeta\bigg(\sum\limits_{i=2}^{t}\big[(\lambda_1^{t-i} v_1^{(1)}\hat{v_1}^{(1)}+\lambda_2^{t-i} v_1^{(2)}\hat{v_2}^{(1)})d_1+(\lambda_1^{t-i} v_1^{(1)}\hat{v_1}^{(2)}+\lambda_2^{t-i} v_1^{(2)}\hat{v_2}^{(2)})d_2\big]\bigg)
        \nonumber \\&
        +\eta_k\beta\bigg(\sum\limits_{i=2}^{t} \big[(\lambda_1^{t-i} v_2^{(1)}\hat{v_1}^{(1)}+\lambda_2^{t-i} v_2^{(2)}\hat{v_2}^{(1)})d_1+(\lambda_1^{t-i} v_2^{(1)}\hat{v_1}^{(2)}+\lambda_2^{t-i} v_2^{(2)}\hat{v_2}^{(2)})d_2\big]\bigg)
        \nonumber \\&
        % +\eta_k\zeta(\lambda_1^{t-1} v_1^{(1)}\hat{v_1}^{(1)}+\lambda_2^{t} v_1^{(2)}\hat{v_2}^{(1)})\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert 
        % \nonumber \\&
        % +\eta_k\beta(\lambda_1^{t-1} v_2^{(1)}\hat{v_1}^{(1)}+\lambda_2^{t-1} v_2^{(2)}\hat{v_2}^{(1)})\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert
        % \nonumber \\&
        +\eta_k\Big[\lambda_1^{t-1} \hat{v_1}^{(1)}(\beta v_2^{(1)}+\zeta v_1^{(1)})+\lambda_2^{t-1} \hat{v_2}^{(1)}(\beta v_2^{(2)}+\zeta v_1^{(2)})\Big]\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert
        \nonumber \\&
        +\eta_k(\beta\epsilon_c(k)+\beta\epsilon(k)+\sqrt{2}\sigma+\delta)
        \nonumber \\&
        =
        (1+\eta_k\beta)\sqrt{\mathbb E_t[\Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert^2]}
        \nonumber \\&
        +\eta_k\zeta 
        \sum\limits_{i=2}^{t} \Big[\lambda_1^{t-i}v_1^{(1)}(\hat{v_1}^{(1)}d_1+\hat{v_1}^{(2)}d_2)+\lambda_2^{t-i} v_1^{(2)}(\hat{v_2}^{(1)}d_1+\hat{v_2}^{(2)}d_2)\Big]
        \nonumber \\&
        +\eta_k\beta 
        \sum\limits_{i=2}^{t}\Big[\lambda_1^{t-i} v_2^{(1)}(\hat{v_1}^{(1)}d_1+\hat{v_1}^{(2)}d_2)+\lambda_2^{t-i} v_2^{(2)}(\hat{v_2}^{(1)}d_1+\hat{v_2}^{(2)}d_2)\Big]
        \nonumber \\&
        +\eta_k\Big[\lambda_1^{t-1} \hat{v_1}^{(1)}(\beta v_2^{(1)}+\zeta v_1^{(1)})+\lambda_2^{t-1} \hat{v_2}^{(1)}(\beta v_2^{(2)}+\zeta v_1^{(2)})\Big]\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert
        \nonumber \\&
        +\eta_k(\beta\epsilon_c(k)+\beta\epsilon(k)+\sqrt{2}\sigma+\delta)
        \nonumber \\&
        =
        (1+\eta_k\beta)\sqrt{\mathbb E_t[\Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert^2]}
        \nonumber \\&
        +\eta_k
        \sum\limits_{i=2}^{t} \Big[\lambda_1^{t-i}(\zeta v_1^{(1)}+\beta v_2^{(1)})(\hat{v_1}^{(1)}d_1+\hat{v_1}^{(2)}d_2)+\lambda_2^{t-i} (\zeta v_1^{(2)}+\beta v_2^{(2)})(\hat{v_2}^{(1)}d_1+\hat{v_2}^{(2)}d_2)\Big]
        \nonumber \\&
        +\eta_k\Big[\lambda_1^{t-1} \hat{v_1}^{(1)}(\beta v_2^{(1)}+\zeta v_1^{(1)})+\lambda_2^{t-1} \hat{v_2}^{(1)}(\beta v_2^{(2)}+\zeta v_1^{(2)})\Big]\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert
        \nonumber \\&
        +\eta_k(\beta\epsilon_c(k)+\beta\epsilon(k)+\sqrt{2}\sigma+\delta)
        \nonumber \\&
        =
        (1+\eta_k\beta)\sqrt{\mathbb E_t[\Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert^2]}
        \nonumber \\&
        +\eta_k
        \sum\limits_{i=2}^{t} \bigg[\lambda_1^{t-i}(\zeta v_1^{(1)}+\beta v_2^{(1)})(\hat{v_1}^{(1)}d_1+\hat{v_1}^{(2)}d_2)
        \nonumber \\&
        +\lambda_2^{t-i} \Big((\zeta v_1^{(2)}+\beta v_2^{(2)})(\hat{v_2}^{(1)}+\hat{v_2}^{(2)})d_1+(\zeta v_1^{(2)}\hat{v_2}^{(2)}+\beta v_2^{(2)}\hat{v_2}^{(2)})(d_2-d_1)\Big)\bigg]
        \nonumber \\&
        +\eta_k\Big[\lambda_1^{t-1} \hat{v_1}^{(1)}(\beta v_2^{(1)}+\zeta v_1^{(1)})+\lambda_2^{t-1} \hat{v_2}^{(1)}(\beta v_2^{(2)}+\zeta v_1^{(2)})\Big]\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert
        \nonumber \\&
        +\eta_k(\beta\epsilon_c(k)+\beta\epsilon(k)+\sqrt{2}\sigma+\delta)
    \end{align}
        Since $\lambda_1\geq\lambda_2\geq 0$, $(\zeta v_1^{(2)}+\beta v_2^{(2)})(\hat{v_2}^{(1)}+\hat{v_2}^{(2)})\geq 0$, $\zeta v_1^{(2)}\hat{v_2}^{(2)}+\beta v_2^{(2)}\hat{v_2}^{(2)}\leq 0$ and $\hat{v_2}^{(1)}(\beta v_2^{(2)}+\zeta v_1^{(2)})\geq 0$
        % $\beta v_2^{(2)}\hat{v_2}^{(2)}\geq 0$ and $\zeta v_1^{(2)}\hat{v_2}^{(2)}\leq0$, 
        we further bound \eqref{55} as
    \begin{align}
        &\sqrt{\mathbb E_t[\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert^2]}
        \leq 
        (1+\eta_k\beta)\sqrt{\mathbb E_t[\Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert^2]}
        \nonumber \\&
        +\eta_k\zeta\bigg(\sum\limits_{i=2}^{t}\lambda_1^{t-i}\big[ (v_1^{(1)}\hat{v_1}^{(1)}+v_1^{(2)}\hat{v_2}^{(1)})d_1+(v_1^{(1)}\hat{v_1}^{(2)}+ v_1^{(2)}\hat{v_2}^{(2)})d_1+\cancel{v_1^{(1)}\hat{v_1}^{(2)}(d_2-d_1)+ v_1^{(2)}\hat{v_2}^{(2)}(d_2-d_1)}\big]\bigg)
        \nonumber \\&
        +\eta_k\beta\bigg(\sum\limits_{i=2}^{t} \lambda_1^{t-i}\big[(v_2^{(1)}\hat{v_1}^{(1)}+v_2^{(2)}\hat{v_2}^{(1)})d_1+ (v_2^{(1)}\hat{v_1}^{(2)}+ v_2^{(2)}\hat{v_2}^{(2)})d_1\big]\bigg)
        \nonumber \\&
        +\eta_k\Big(\lambda_1^{t-1} [\beta (v_2^{(1)}\hat{v_1}^{(1)}+v_2^{(2)}\hat{v_2}^{(1)})+\zeta (v_1^{(1)}\hat{v_1}^{(1)}+ v_1^{(2)}\hat{v_2}^{(1)})]\Big)\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert
        \nonumber \\&
        +\eta_k(\beta\epsilon_c(k)+\beta\epsilon(k)+\sqrt{2}\sigma+\delta)
        \nonumber \\&
        = 
        (1+\eta_k\beta)\sqrt{\mathbb E_t[\Vert\bar{\mathbf w}_c(t-1)-\bar{\mathbf w}(t-1)\Vert^2]}
        \nonumber \\&
        +\eta_k d_1(\zeta+\beta)\sum\limits_{i=2}^{t} \lambda_1^{t-i}
        \nonumber \\&
        +\eta_k\zeta\lambda_1^{t-1}\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert
        \nonumber \\&
        +\eta_k(\beta\epsilon_c(k)+\beta\epsilon(k)+\sqrt{2}\sigma+\delta)
    \end{align}
    Multiplying both sides of \eqref{55} by $(1+\eta_k\beta)^{-t}$ and summing up over $t$, we obtain 
    \begin{align} 
        &\sqrt{\mathbb E_t\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert^2}
        \nonumber \\&
        \leq 
        \eta_k d_1(\zeta+\beta)
        \sum\limits_{j=1}^t(1+\eta_k\beta)^{t-j}\sum\limits_{i=2}^{j}\lambda_1^{j-i}
        \nonumber \\&
        +\eta_k\zeta\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert\sum\limits_{j=1}^t(1+\eta_k\beta)^{t-j}\lambda_1^{j-1}
        \nonumber \\&
        +\eta_k\sum\limits_{j=1}^t(1+\eta_k\beta)^{t-j}(\beta\epsilon_c(k)+\beta\epsilon(k)+\sqrt{2}\sigma+\delta)
        % \nonumber \\&
        % = 
        % \frac{\eta_k[\zeta(1-2v_1^{(1)}\hat{v_1}^{(2)})d_1+(\beta+2\zeta v_1^{(1)}\hat{v_1}^{(2)}) d_2]}{\lambda_1-1}
        % \sum\limits_{j=1}^t(1+\eta_k\beta)^{t-j}[\lambda_1^{j-1}-1]
        % \nonumber \\&
        % +\eta_k\zeta\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert\sum\limits_{j=1}^t(1+\eta_k\beta)^{t-j}\lambda_1^{j-1}
        % \nonumber \\&
        % +\eta_k\sum\limits_{j=1}^t(1+\eta_k\beta)^{t-j}(\beta\epsilon_c(k)+\beta\epsilon(k)+\sqrt{2}\sigma+\delta)
    \end{align}  
    Since 
    \begin{align}
        &\sum\limits_{j=1}^t(1+\eta_k\beta)^{t-j}\lambda_1^{j-1}
        =[(\frac{\lambda_1}{1+\eta\beta})^t-1]\cdot(1+\eta_k\beta)^{t-1}/(\frac{\lambda_1}{1+\eta\beta}-1)
        \nonumber \\&
        =\frac{\lambda_1^t-(1+\eta_k\beta)^t}{\lambda_1-(1+\eta_k\beta)}
        % \nonumber \\&
        =\frac{\lambda_1^t-(1+\eta_k\beta)^t}{\eta_k\beta[-\kappa/4+\sqrt{(1+\kappa/4)^2+\omega}]}
        \leq 
        \frac{\lambda_1^t-1}{\eta_k\beta},
    \end{align}
    we rewrite the geometric sums in closed form, we obtain
    \begin{align} \label{62}
        &\sqrt{\mathbb E_t\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert^2}
        \nonumber \\&
        \leq 
        [\lambda_1^t-1]2\omega\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert
        \nonumber \\&
        +\frac{\eta_k d_1(\zeta+\beta)}{\lambda_1-1}
        \left[\frac{\lambda_1^t-1}{\eta_k\beta}-\frac{(1+\eta_k\beta)^t-1}{\eta_k\beta}\right]
        \nonumber \\&
        +[(1+\eta_k\beta)^t-1](\epsilon_c(k)+\epsilon(k)+\sqrt{2}\sigma/\beta+\delta/\beta)
        \nonumber \\&
        \leq 
        [\lambda_1^t-1]2\omega\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert
        \nonumber \\&
        +\frac{d_1(2\omega+1)}{2\eta_k\beta}
        \left(\lambda_1^t-1-[(1+\eta_k\beta)^t-1]\right)
        \nonumber \\&
        +[(1+\eta_k\beta)^t-1](\epsilon_c(k)+\epsilon(k)+\sqrt{2}\sigma/\beta+\delta/\beta)
    \end{align}  
    Since 
    \begin{align}
        &\frac{d_1(2\omega+1)}{2\eta_k\beta}\leq \frac{3d_1}{2\eta_k\beta}\leq 2(\epsilon(k)+\sigma/\beta+\delta/\beta),
        % =
        % \omega(1-2/\sqrt{(\kappa/2+2)^2+4\omega}) d_1+ (1+2\omega/\sqrt{(\kappa/2+2)^2+4\omega})d_2
        % \nonumber \\&
        % \leq
        % d_1+\frac{2\sqrt{3}}{3}d_2
        % = \eta_k(\beta\epsilon(k)+\sigma)+\frac{2\sqrt{3}}{3}\eta_k(\delta+\sqrt{2}\sigma+2\beta\epsilon(k))
        % \nonumber \\&
        % = \frac{2\sqrt{3}}{3}\delta+\frac{2\sqrt{6}+3}{3}\sigma+(\frac{4\sqrt{3}}{3}+1)\epsilon(k)\beta 
    \end{align}
    % and
    % \begin{align}
    %     &-(\omega+v_2^{(1)}\hat{v_1}^{(1)}) d_1+ (1+\omega v_1^{(1)}\hat{v_1}^{(2)})d_2
    %     \nonumber \\&
    %     = 
    %     -\omega(1+1/\sqrt{(\kappa/2+2)^2+4\omega}) d_1- (1+\omega/\sqrt{(\kappa/2+2)^2+4\omega})d_2
    %     \nonumber \\&
    %     \leq
    %     -d_2
    %     = -\eta_k(\delta+\sqrt{2}\sigma+2\beta\epsilon(k)),
    % \end{align}
    % and 
    % \begin{align}
    %     \eta_k(\beta v_2^{(2)}\hat{v_2}^{(1)} d_1+ \zeta v_1^{(2)}\hat{v_2}^{(2)}d_2)=\beta\omega(d_1-d_2)/\sqrt{(\kappa/2+2)^2+4\omega}
    % \end{align}
    we can further bound \eqref{62} as 
    \begin{align}
        &\sqrt{\mathbb E_t\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert^2}
        \nonumber \\&
        \leq 
        [\lambda_1^t-1]\{2\omega\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert+2(\epsilon(k)+\sigma/\beta+\delta/\beta)\}
        \nonumber \\&
        +[(1+\eta_k\beta)^t-1][\epsilon_c(k)-\epsilon(k)-(2-\sqrt{2})\frac{\sigma}{\beta}-\frac{\delta}{\beta}]
        \nonumber \\&
        \leq 
        [\lambda_1^t-1]\{2\omega\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert+2(\epsilon(k)+\sigma/\beta+\delta/\beta)\}
        \nonumber \\&
        +[(1+\eta_k\beta)^t-1][\epsilon_c(k)-\epsilon(k)].
    \end{align}
    Therefore, we obtain
    \begin{align}
        &\sum\limits_{c=1}^N\rho_c\mathbb E_t\Vert\bar{\mathbf w}_c(t)-\bar{\mathbf w}(t)\Vert^2
        \nonumber \\&
        \leq
        [\lambda_1^{2t}-1]\{2\omega\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert+2(\epsilon(k)+\sigma/\beta+\delta/\beta)\}^2
        \nonumber \\&
        +[\lambda_1^{2t}-1]\sum\limits_{c=1}^N\rho_c[\epsilon_c(k)-\epsilon(k)]^2
        \nonumber \\&
        \leq
        [\lambda_1^{2t}-1]\{2\omega\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert+2(\epsilon(k)+\sigma/\beta+\delta/\beta)\}^2
        \nonumber \\&
        +[\lambda_1^{2t}-1]2\epsilon^2(k)
        \nonumber \\&
        \leq
        [\lambda_1^{2t}-1]\{16\omega^2\Vert\hat{\mathbf w}(t_{k-1})-\mathbf w^*\Vert+16\epsilon^2(k)+18\sigma^2/\beta^2+16\delta^2/\beta^2\}
        \nonumber \\&
        \leq
        [\lambda_1^{2t}-1]\{\frac{32}{\mu}\omega^2 [F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)]+16\epsilon^2(k)+18\sigma^2/\beta^2+16\delta^2/\beta^2\}
    \end{align}
    Follows from \eqref{errors}, we have
    \begin{align}
        &\mathbb E\left[\sum\limits_{c=1}^N\varrho_{c}\frac{1}{s_c^{(k)}}\sum_i\Vert\mathbf w_i(t)-\bar{\mathbf w}(t)\Vert_2^2\right]
        \nonumber \\&
        \leq
        [\lambda_1^{2t}-1]\{\frac{32}{\mu}\omega^2 [F(\hat{\mathbf w}(t_{k-1}))-F(\mathbf w^*)]+16\epsilon^2(k)+18\sigma^2/\beta^2+16\delta^2/\beta^2\}+\epsilon^2(k)
    \end{align}
\end{proof}
}
\end{document}

