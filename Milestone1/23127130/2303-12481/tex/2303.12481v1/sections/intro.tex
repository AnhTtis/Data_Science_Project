\section{Introduction}
\label{sec:intro}
Deep learning has achieved breakthrough improvement in numerous tasks and has developed as a powerful tool in various applications, including computer vision~\cite{long2015fully,simonyan2014very,redmon2016you}, speech processing~\cite{mikolov2011strategies,hinton2012deep,mei2022towards}, bioinformatics~\cite{chicco2014deep,spencer2014deep,kim2018deep}.
Despite their success, deep neural networks are known to be vulnerable to adversarial examples, carefully perturbed examples perceptually indistinguishable from original samples~\cite{szegedy2013intriguing}. This can lead to a significant disruption of the inference result of deep neural networks. It has important implications for safety and security-critical applications of machine learning models.
Various methods have been proposed to mitigate the adversarial vulnerability~\cite{lee2018simple,wong2018provable}. In known, robust classification can be reached by detecting adversarial examples or pushing data samples further from the classifier's decision boundary. A large body of work has been done on designing more robust deep classifiers~
\cite{XieWZRY18,wong2018provable,lee2018simple}. Among these methods, adversarial training~ \cite{szegedy2013intriguing, madry2017towards} has emerged as the primary approach, which augments adversarial examples to the training set to improve intrinsic network robustness.
 \begin{figure}[t]
    \begin{center}
            \includegraphics[width=0.45\textwidth]{photos/Demo/oghab.pdf}
            \end{center}
            \vspace{-2mm}
            \caption{Adversarial examples for ImageNet, as computed by SDF on a ResNet-$50$~\cite{he2016deep} architecture. The original image, which was classified as an "American eagle" is classified as an "iguana" by the perturbation vector obtained from SDF.}
            \vspace{-5mm}
            \label{fig:oghab}
\end{figure}

Our goal in this paper is to introduce a parameter-free and simple method for accurately and reliably evaluating the adversarial robustness of deep networks in a fast and geometrically-based fashion. Most of the current attack methods rely on general-purpose optimization techniques, such as Projected Gradient Descent~(PGD)~\cite{madry2017towards} and Augmented Lagrangian~\cite{rony2021augmented}, which are oblivious to the geometric properties of models. However, deep neural networks' robustness to adversarial perturbations is closely tied to their geometric landscape~\cite{dauphin2014identifying,poole2016exponential}. Given this, it would be beneficial to exploit such properties when designing and implementing adversarial attacks. This allows to create more effective and computationally efficient attacks on classifiers.
Formally, for a given classifier $\hat{k}$, we define an adversarial perturbation as the minimal perturbation $\boldsymbol{r}$ that is sufficient to
change the estimated label $\hat{k}(\x)$:
\begin{align}
    \Delta(\x;\hat{k}):=\min_{\boldsymbol{r}} \| \boldsymbol{r} \|_2\text{  s.t  } \hat{k}(\x+\boldsymbol{r}) \neq \hat{k}(\x),
    \label{global-formula}
\end{align}
Depending on the data type, $\x$ can be any data, and $\hat{k}(\x)$ is the estimated label.

DeepFool~(DF)~\cite{Moosavi-Dezfooli_2016_CVPR} was among the earliest attempts to exploit the ``excessive linearity''~\cite{goodfellow2014explaining} of deep networks to find minimum-norm adversarial perturbations.
However, more sophisticated attacks were later developed that could find smaller perturbations at the expense of significantly greater computation time. 

In this paper, we exploit the geometric characteristics of minimum-norm adversarial perturbations to design a family of fast yet simple algorithms
that become state-of-the-art in finding minimal adversarial perturbation.
Our proposed algorithm is based on the geometric principles underlying DeepFool, but incorporates novel enhancements that improve its performance significantly, while maintaining simplicity and efficiency that are only slightly inferior to those of DF. Our main contributions are summarized as follows:
\begin{itemize}
    \item We introduce a novel family of fast yet accurate algorithms to find minimal adversarial perturbations. We extensively evaluate and compare our algorithms with state-of-the-art attacks in various settings.
    
    \item Our algorithms are developed in a systematic and well-grounded manner, based on theoretical analysis.      
    
    \item We further improve the robustness of state-of-the-art image classifiers to minimum-norm adversarial attacks via adversarial training on the examples obtained by our algorithms.

   \item  We significantly improve the time efficiency of the state-of-the-art Auto-Attack (AA)~\cite{croce2020reliable} by adding our proposed method to the set of attacks in AA.
   
    
\end{itemize}

\textbf{Related works.}
It has been observed that deep neural networks are vulnerable to adversarial examples before in~\cite{szegedy2013intriguing,carlini2017towards,Moosavi-Dezfooli_2016_CVPR,goodfellow2014explaining}. The authors in~\cite{szegedy2013intriguing} studied adversarial examples by solving a penalized optimization problems.
The optimization approach used in~\cite{szegedy2013intriguing} is complex and computationally inefficient; therefore, it can not scale to large datasets.
The method proposed in~\cite{goodfellow2014explaining} simplify the optimization problem used in~\cite{szegedy2013intriguing} by optimizing it for the $\ell_{\infty}$-norm and relaxing the condition very close to image $\x$. DF was the first to attempt to find minimum-norm adversarial perturbations by utilizing an iterative approach that uses a linearization of the classifier at each iteration to estimate the minimal adversarial perturbation. Carlini and Wagner (C$\&$W)~\cite{carlini2017towards} transform the optimization problem in~\cite{szegedy2013intriguing} into an unconstrained optimization problem. C$\&$W by leveraging the first-order gradient-based optimizers to minimize a balanced loss between the norm of the perturbation and misclassification confidence. Inspired by the geometrical idea of DF, FAB~\cite{croce2020minimally} presents an approach to minimize the norm of adversarial perturbations by employing complex projections and approximations while maintaining proximity to the decision boundary. By utilizing gradients to estimate the local geometry of the boundary, this method formulates minimum-norm optimization without the need for tuning a weighting term. FAB cannot be used for large datasets. DDN~\cite{Rony_2019_CVPR} uses projections on the $\ell_{2}$-ball for a given perturbation budget $\epsilon$. FMN~\cite{pintor2021fast} extends the DDN attack to other $\ell_{p}$-norms. By formulating~(\ref{global-formula}) with Lagrange's method, ALMA~\cite{rony2021augmented} introduced a framework for finding adversarial examples for several distances.

Classifying adversarial attacks help us to understand their core idea. For this reason, we can classify adversarial attacks into two main categories. The first is white-box attacks, and the second is black-box attacks~\cite{chen2020hopskipjumpattack,rahmati2020geoda}. In white-box attacks, the attacker knows "everything": model architecture, parameters, defending mechanism, etc.
In black-box attacks, the attacker has limited knowledge about the model. It can query the model on inputs to observe outputs.
In this paper, our focus is on white-box attacks. Formally we can categorize white-box attacks into two sub-categories; the first one is bounded-norm attacks, and the second one is minimum-norm attacks. DeepFool, C$\&$W, FMN, FAB, DDN, and ALMA are minimum-norm attacks, and FGSM, PGD, and momentum extension of PGD~\cite{uesato2018adversarial} are bounded-norm attacks.

The rest of the paper is organized as follows: In Section 2, we introduce minimum-norm adversarial perturbations and characterize their geometrical aspects. Section 3 provides an efficient method for computing $\ell_{2}$ adversarial perturbations by introducing a geometrical solution. Finally, the evaluation and analysis of the computed $\ell_{2}$ perturbations are provided in Section 4.











