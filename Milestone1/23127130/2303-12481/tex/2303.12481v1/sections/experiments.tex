\section{Experimental Results}
\label{sec:experiments}
In this section, we conduct extensive experiments to demonstrate the effectiveness of our method in different setups and for several networks. We first introduce our experimental settings, including datasets, models, and attacks.
Next, we compare our method with state-of-the-art $\ell_{2}$-norm adversarial attacks in various settings, demonstrating the superiority of our simple yet fast algorithm for finding adversarial examples.
Moreover, we add SDF  to the collection of attacks used in Auto-Attack~\cite{croce2020reliable}, and call the new set of attacks Auto-Attack++. This setup meaningfully speeds up the process of finding norm-bounded adversarial perturbations. We also demonstrate that a model adversarially training using the SDF perturbations becomes more robust compared to the models\footnote{we only compare to the publicly available models.} trained using other minimum-norm attacks.

\subsection{Setup}
We test our algorithms on deep convolutional neural network architectures trained on MNIST~\cite{lecun1998mnist}, CIFAR-10, and ImageNet~\cite{deng2009imagenet} datasets.
For the evaluation, we use all the MNIST test dataset, while for CIFAR-10 and ImageNet we use $1000$ samples randomly chosen from their corresponding test datasets.
For MNIST, we use a robust model called IBP from~\cite{zhang2019towards}. For CIFAR-$10$, we use three models: an adversarially trained PreActResNet-$18$~\cite{he2016identity} from~\cite{rade2021helper}, a regularly trained Wide ResNet $28$-$10$~(WRN-$28-10$) from~\cite{zagoruyko2016wide} and LeNet~\cite{lecun1999object}. These models are obtainable via the RobustBench library~\cite{croce2020robustbench}. On ImageNet, we test the attacks on two ResNet-$50$~(RN-$50$) models: one regularly trained and one $\ell_{2}$ adversarially trained, obtainable through the robustness library~\cite{robustness}.

\begin{table}
	\centering
	\caption{The cosine of the angle between the perturbation
    vector ($\boldsymbol{r}$) and $\nabla f(\x +\boldsymbol{r})$. We performed this experiment on three models trained on CIFAR-$10$ dataset.  The evaluation is done using $1000$ random samples. 
    }

    \begin{small}
    \begin{sc}
	\begin{tabular}{lrrr}
        \toprule
		\multirow{2}{*}{Attack} & \multicolumn{3}{c}{Models}\\
        \cmidrule{2-4}
        & LeNet & ResNet-18 & WRN-28-10\\
		\midrule
		DF & $0.89$ &  $0.14$ & $0.21$\\
		SDF~(1,1) & $0.90$ & $0.63$  & $0.64$ \\
		SDF~(1,3) & $0.88$ & $0.61$  & $0.62$\\
		SDF~(3,1) & $\mathbf{0.92}$ & $0.70$ & $0.72$ \\
        SDF & $\mathbf{0.92}$ & $\mathbf{0.72}$  & $\mathbf{0.80}$\\
		\bottomrule
	\end{tabular}
\end{sc}
\end{small}
	\label{tab:last_grad}
\end{table}

\begin{table}[t!]
    \selectfont
	\centering
	\caption{Comparison of the mean and the median of $\ell_{2}$-norm of perturbations for DF and SDF family algorithms. We performed this experiment on CIFAR-$10$. We use the same model
    architectures and hyperparameters for training as in \cite{carlini2017towards,Rony_2019_CVPR}. For more details about the architecture see the appendix.}
    \begin{small}
    \begin{sc}
	\begin{tabular}{lrrr}
		\toprule
		Attack & Mean-$\ell_{2}$ & Median-$\ell_{2}$ & Grads \\
		\midrule
		DF & $0.17$ & $0.15$ & $\mathbf{14}$ \\
		SDF~(1,1) & $0.14$ & $0.13$ & $22$ \\
		SDF~(1,3) & $0.16$ & $0.14$ & $26$ \\
		SDF~(3,1) & $0.12$ & $0.11$ & $30$ \\
		SDF & $\mathbf{0.11}$ & $\mathbf{0.10}$ & $32$ \\
		\bottomrule
	\end{tabular}
\end{sc}
\end{small}
	\label{tab:CIFAR10_architecture_DF}
\end{table}
\subsection{Comparison with DeepFool} \label{sec:exp-sdf}
In this part, we compare our algorithm in terms of orthogonality and size of the $\ell_{2}$-norm perturbations with DF. Assume $\boldsymbol{r}$ is the perturbation vector obtained by an adversarial attack. First, we measure the orthogonality of perturbations by measuring the inner product between $\nabla f(\x+\boldsymbol{r})$ and $\boldsymbol{r}$.
As we explained in Section~\ref{DeepFool}, a larger inner product between $\boldsymbol{r}$ and the gradient vector at $f(\x+\boldsymbol{r})$ indicates that the perturbation vector is closer to the optimal perturbation vector $\boldsymbol{r}^{*}$. 
We compare the orthogonality of different members of the SDF family and DF for three networks trained on CIFAR-10; namely, LeNet, ResNet-$18$, and WRN-$28$-$10$.
The results are shown in Table~\ref{tab:last_grad}. We observe that DF finds perturbations orthogonal to the decision boundary for low-complexity models such as LeNet, but fails to perform effectively when evaluated against more complex ones. In contrast, attacks from the SDF family consistently found perturbations with a larger cosine of the angle for all three models.

\begin{figure}[t]
\center
\includegraphics[width=0.93\columnwidth]{photos/orthogonality_SDF_S.pdf}
\caption{\label{fig:orth-SDF}Histogram of the cosine angle distribution between the gradient in the last step of SDF and the perturbation vector obtained by SDF. This experiment has been performed on $1000$ images from the CIFAR-$10$~\cite{krizhevsky2009learning} dataset with the ResNet-$18$~\cite{he2016deep} model.} 
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.93\columnwidth]{photos/Overly_perturbed_SDF_S.pdf}
    \caption{We generated $1000$ images with one hundred $\gamma$ between zero and one, and the fooling rate of the SDF is reported. This experiment is done on the CIFAR-$10$ dataset and ResNet-$18$ model. The accuracy of this network is $94\%$.}
    \label{fig:overly_SDF}
\end{figure}
Table~\ref{tab:CIFAR10_architecture_DF} shows the comparison between the size of the perturbations.
These results show that the SDF family finds more accurate perturbations than DF. SDF~($\infty$,1), the strong member of this family, makes a significant difference with DF with little cost. 
\paragraph{\textbf{Comparison with results in Section~\ref{DeepFool}}.}
We also repeat the experiments we presented for DF in Section 2 for SDF, to show that our method indeed improves the two issues with DF that we highlighted. Namely, the alignment of the perturbation with the normal to the decision boundary and the problem of over-perturbation. In Figure~\ref{fig:orth-SDF}, we can see that unlike Figure~\ref{fig:orthogonality}, the cosine of the angle is accumulated around one, which shows that the found perturbation is more aligned with the normal vector to the decision boundary. Moreover, Figure~\ref{fig:overly_SDF} shows a sharp decline in the fooling rate (going down quickly to zero). This is consistent with our expectation for a minimal perturbation attack.
\begin{table}[t!]
    \caption{Performance for attacks on the MNIST dataset with IBP models. The numbers between parentheses indicate the number of iterations.}
    \label{tab:IBP}
    \centering
    \begin{small}
        \begin{sc}
            \begin{tabular}{lrrr}
            \toprule
                 Attack & Fooling-rate & Median-$\ell_{2}$ & Grads \\
                \midrule
                  ALMA ($1000$) & $\mathbf{100}$ & $\mathbf{1.26}$ &  $1\,000$\\
                  ALMA  ($100$) & $98.90$ & $4.96$ &  $100$\\
                  DDN ($1000$)  & $99.27$ & $1.97$ &  $1\,000$ \\
                  DDN ($100$)  & $94.34$ & $1.46$ & $100$ \\
                  FAB ($1000$) & $99.98$ & $3.34$ &  $10\,000$ \\
                  FAB  ($100$)  & $99.98$ & $5.19$ &  $1\,000$ \\
                  FMN ($1000$)  & $89.08$ & $1.34$ &  $1\,000$ \\
                  FMN  ($100$)  & $67.80$ & $2.14$ &  $100$ \\
                  C\&W  & $4.63$ & -- &  $90\,000$  \\
                  SDF & $\mathbf{100}$ & $1.37$ & $\mathbf{52}$\\
                 \bottomrule
            \end{tabular}    
        \end{sc}
    \end{small}
\end{table}


%%%%%%%% Table For CIFAR-10 %%%%%%%%%%%%%%
\begin{table}[t!]
\caption{Performance for attacks on the CIFAR-$10$ dataset with WRN-$28$-$10$. For see the results on adversarially trained network see Appendix.}
    \label{tab:CIFAR-10}
\centering
\begin{small}
\begin{sc}
    \begin{tabular}{lrrr}
    \toprule
         Attack & Fooling-rate & Median-$\ell_{2}$ & Grads \\
        \midrule
          ALMA & $100$ & $0.10$ &  $100$\\

          DDN  & $100$ & $0.13$ &  $100$ \\

          FAB  & $100$ & $0.11$ &  $100$ \\

          FMN  & $97.3$ & $0.11$ &  $100$ \\

          C\&W  & $100$ & $0.12$ &  $90\,000$  \\
          SDF & $100$ & $\mathbf{0.09}$ & $\mathbf{25}$\\
         \bottomrule
    \end{tabular}    
\end{sc}
\end{small}
\end{table}



\subsection{Comparison with Minimum-Norm Attacks}
We now compare our SDF with other minimum $\ell_{2}$-norm state-of-the-art attacks such as C\&W, FMN, DDN, ALMA, FAB. For C\&W, we use the same hyperparameters as~\cite{carlini2017towards,Rony_2019_CVPR}. We use FMN, FAB, DDN, and ALMA with budgets of $100$ and $1000$ iterations and report the best performance. In terms of implementation, we use Pytorch~\cite{paszke2019pytorch}. Foolbox~\cite{rauber2017foolbox} and Torchattcks~\cite{kim2020torchattacks} libraries are used to implement adversarial attacks.

We evaluated the robustness of the IBP model, which is adversarially trained on the MNIST dataset, against state-of-the-art attacks in Table~\ref{tab:IBP}. SDF and ALMA are the only attacks that achieve a $100\%$ percent fooling rate against the robust model, whereas C\&W is unsuccessful on most of the data samples. The fooling rates of the remaining attacks also degrade when evaluated with $100$ iterations. For instance, FMN's fooling rate decreases from $89\%$ to $67.8\%$ when the number of iterations is reduced from $1000$ to $100$. This observation shows that, unlike SDF, selecting the necessary number of iterations is critical for the success of fixed-iteration attacks. Even for ALMA which can achieve a nearly perfect misclassification rate, decreasing the number of iterations from $1000$ to $100$ causes the median norm of perturbations to increase fourfold. In contrast, SDF is able to compute adversarial perturbations using the fewest number of gradient computations while still outperforming the other algorithms, except ALMA, in terms of the perturbation norm. However, it is worth noting that ALMA requires twenty times more gradient computations compared to SDF to achieve a  marginal improvement in the perturbation norm.



Table~\ref{tab:CIFAR-10} compares SDF with state-of-the-art attacks on the CIFAR-$10$ dataset. The results show that state-of-the-art attacks have a similar norm of perturbations, but an essential point is the speed of attacks. SDF finds more accurate adversarial perturbation very quickly rather than other algorithms. 
We also evaluated all attacks on an adversarially trained model for the CIFAR-$10$ dataset. SDF achieves smaller perturbations with half the gradient calculations than other attacks. SDF finds smaller adversarial perturbations for adversarially trained networks at a significantly lower cost than other attacks, requiring only $20\%$ of FAB's cost and $50\%$ of DDN's and ALMA's.


Unlike models trained on CIFAR-$10$, where the attacks typically result in perturbations with similar norm, the differences between attacks are more nuanced for ImageNet models.
In particular, attacks such as FAB, DDN, and FMN lose their accuracy when the dataset changes. In contrast, SDF achieves smaller perturbations at a significantly lower cost than ALMA.
This shows that the geometric interpretation of optimal adversarial perturbation, rather than viewing (\ref{global-formula}) as a non-convex optimization problem, can lead to an efficient solution.
On the complexity aspect, the proposed approach is substantially faster than the other methods. In contrast, these approaches involve a costly minimization of a series of objective functions. We empirically observed that SDF converges in less than $5$ or $6$  iterations to a fooling perturbation; our observations show that SDF consistently achieves state-of-the-art minimum-norm perturbations across different datasets, models, and training strategies, while requiring the least number of gradient computations. This makes it readily suitable to be used as a baseline method to estimate the robustness of very deep neural networks on large datasets.



\begin{table}[!t]
    \caption{The performance comparison of SDF with other state-of-the-art attacks for median $\ell_{2}$ on ImageNet dataset. FR columns show the fooling rates of attacks.}
    \label{tab:ImageNet}  
    \centering    
    \begin{small}
    \begin{sc}
    \begin{tabular}{llrrr}
         \toprule
         Model
         & Attack
         & FR
         & Median-$\ell_{2}$
         & Grads \\
         \midrule
        \multirow{8}{*}{\makecell{RN-$50$}}
         & ALMA & $\mathbf{100}$ & $0.10$ & $100$\\
         & DDN & $99.9$ & $0.17$ & $1\,000$ \\
         & FAB & $99.3$ & $0.10$ & $900$ \\
         & FMN  & $99.3$ & $0.10$ & $1\,000$ \\
         & C\&W  & $\mathbf{100}$ & $0.21$ & $82\,667$  \\
         & SDF & $\mathbf{100}$ & $\mathbf{0.09}$ & $\mathbf{37}$\\
         \midrule
        \multirow{8}{*}{\makecell{RN-$50$\\(AT)}}
         & ALMA & $\mathbf{100}$ & $0.85$ & $100$\\
         & DDN  & $99.7$ & $1.10$ & $1\,000$ \\
         & FAB  & $\mathbf{100}$ & $0.81$ & $900$ \\
         & FMN  & $99.9$ & $0.82$ & $1\,000$ \\
         & C\&W & $99.9$ & $1.17$ & $52\,000$  \\
         & SDF & $\mathbf{100}$ & $\mathbf{0.80}$ & $\mathbf{49}$\\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}  
\end{table}





\subsection{SDF Adversarial Training (AT)}
In this section, we evaluate the performance of a model adversarially trained using SDF against minimum-norm attacks and AutoAttack~\cite{croce2020reliable}. 
Our experiments provide valuable insights into the effectiveness of adversarial training with SDF and sheds light on its potential applications in building more robust models.

Adversarial training has become a powerful approach to fortify deep neural networks against adversarial perturbations. 
However, some adversarial attacks, such as C$\&$W, pose a challenge due to their high computational cost and complexity, making them unsuitable for adversarial training. Therefore, an attack that is parallelizable on batch size and gradient computation is necessary for successful adversarial training. SDF possesses these crucial properties, making it a promising candidate for building more robust models.





We adversarially train a WRN-$28$-$10$ on CIFAR-$10$. Similar to the procedure followed in~\cite{Rony_2019_CVPR}, we restrict $\ell_{2}$-norms of perturbation to $2.6$ and set the maximum number of iterations for SDF to $6$. We train the model on clean examples for the first $200$ epochs, and we then fine-tune it with SDF generated adversarial examples for $60$ more epochs. Our model reaches a test accuracy of $90.8\%$ while the model by~\cite{Rony_2019_CVPR} obtains $89.0\%$.
In~\cite{Rony_2019_CVPR}, vanilla adversarial training with DDN generated adversarial examples is shown build a more robust model than a model trained with PGD~\cite{madry2017towards}. For this reason, we compare our model with~\cite{Rony_2019_CVPR}. SDF adversarially trained model does not overfit to SDF attack because, as Table~\ref{tab:AT-CIFAR10} shows, SDF obtains the smallest perturbation. It is evident that SDF adversarially trained model can significantly improve the robustness of model against minimum-norm attacks up to $30\%$.
In terms of comparison of these two adversarially trained models with Auto Attack~(AA)~\cite{croce2020reliable}, our model outperformed the~\cite{Rony_2019_CVPR} by improving about $8.4\%$ against $\ell_{\infty}$-AA, for $\varepsilon=8/255$, and $0.6\%$ against $\ell_{2}$-AA, for $\varepsilon=0.5$.

\begin{table}
	\centering
	\caption{The comparison between $\ell_2$ robustness of our adversarial trained model and~\cite{Rony_2019_CVPR} model. We perform this experiment on $1000$ samples from CIFAR-$10$ dataset. Gain is calculated as a percentage of the difference between the medians.
    The gain column shows the amount of gain that our model obtains.}

    \begin{small}
        \begin{sc}
        	\begin{tabular}{lrrrrrr}
                \toprule
                \multirow{2}{*}{Attack} & \multicolumn{2}{c}{SDF (Ours)} & \multicolumn{2}{c}{DDN} & \multirow{1}{*}{} \\
                \cmidrule{2-5}
        		& Mean & Median & Mean & Median & Gain\\
        		\midrule
        		DDN & $1.09$ & $1.02$ & $0.86$ & $0.73$ & $\uparrow$$29\%$\\
        		FAB & $1.12$ & $1.03$ & $0.92$ & $0.75$ & $\uparrow$$28\%$\\
        		FMN & $1.48$ & $1.43$ & $1.47$ & $1.43$ & $0\%$\\
        		ALMA & $1.17$ & $1.06$ & $0.84$ & $\mathbf{0.71}$ & $\uparrow$$35\%$ \\
        		SDF & $\mathbf{1.06}$ & $\mathbf{1.01}$ & $\mathbf{0.81}$ & $0.73$ & $\uparrow$$28\%$ \\
        		\bottomrule
        	\end{tabular}
        \end{sc}
    \end{small}
	\label{tab:AT-CIFAR10}
    % }
\end{table}

Furthermore, compared to a network trained on DDN samples, our adversarially trained model has a smaller input curvature (Table~\ref{tab:curvature}). This observation corroborates the idea that a more robust network will exhibit a smaller input curvature \cite{moosavi2019robustness,srinivasefficient,qin2019adversarial}.

\begin{table}

  \centering
    \caption{The average input curvature of WRN-$28$-$10$ models trained on CIFAR-$10$ dataset, according to the measures proposed in~\cite{srinivasefficient}. The second column shows the average spectral-norm of the Hessian w.r.t. input, $\|\nabla^2 f(\x)\|_2$, and the third column shows the average of the same quantity normalized by the norm of the input gradient, $ \C_f(\x)=\|\nabla^2 f(\x)\|_2/\|\nabla f(\x)\|_2$.
    }
     \label{tab:curvature}
     \begin{small}
         \begin{sc}                    
    \begin{tabular}{rcc} 
    \toprule
     Model & $\E_{\x} \| \grad^2 f(\x) \|_2$ & $\E_{\x} \C_f(\x)$\\ 
     \midrule
     % \midrule
     Standard & $600.06$ \std{$29.76$} & $73.99$ \std{$6.62$}\\
     DDN AT  & $2.86$ \std{$1.22$} & $4.32$ \std{$2.91$}\\

     SDF AT (Ours) & $\mathbf{0.73}$ \std{$0.08$} & $\mathbf{1.66}$ \std{$0.86$}\\
     \bottomrule
    \end{tabular}
\end{sc}
     \end{small}
\end{table}



\subsection{AutoAttack++}
\begin{table}
    \caption{An analysis of robust accuracy~($\%$) for various defense strategies against AA++ and AA with $\varepsilon = 0.5$. The "acc" column denotes the robust accuracies of different models. R1~\cite{rebuffi2021fixing}, R2~\cite{Proxy}, R3~\cite{gowal2020uncovering} and R4~\cite{rice2020overfitting} have been trained adversarially, and model S~\cite{zagoruyko2016wide} is a regularly trained model. All models are taken from the RobustBench library~\cite{croce2020robustbench}.}

    \begin{small}
        \begin{sc}
    % \begin{tabular}{@{}rccccc@{}}
    \begin{tabular}{rccccc}
      \toprule
      \multirow{2}{*}{\makecell[c]{Models}}
      & Clean
      & \multicolumn{2}{c}{AA}
      & \multicolumn{2}{c}{AA++}\\
      \cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-6} 
      & acc& acc & Grads & acc & Grads \\
      \midrule

      R1 & $95.7\%$ & $82.3\%$ & $1259.2$ & $\mathbf{82.1\%}$ & $\mathbf{599.5}$\\
      R2  & $90.3\%$ & $76.1\%$ & $1469.1$ & $76.1\%$ & $\mathbf{667.7}$\\
      
      R3 & $89.4\%$ & $63.4\%$ & $1240.4$ & $\mathbf{62.2\%}$ & $\mathbf{431.5}$\\
      R4 &  $88.6\%$ & $\mathbf{67.6\%}$ & $933.7$ & $68.4\%$ & $\mathbf{715.3}$ \\
      S & $94.7\%$ & $0.00\%$ & $208.6$ & $0.00$ & $\mathbf{121.1}$\\
    \bottomrule  
    \end{tabular}
    \end{sc}
    \end{small}
  \label{AA++}
\end{table}
In this part, we introduce a new variant of AutoAttack~\cite{croce2020reliable} by introducing AutoAttack++~(AA++). Auto-Attack~(AA) is a reliable and powerful ensemble attack that contains three types of white-box and a strong black-box attacks. AA evaluates the robustness of a trained model to adversarial perturbations whose $\ell_2$/$\ell_\infty$-norm is bounded by $\varepsilon$. By substituting SDF with the attacks in the AA, we significantly increase the performance of AA in terms of computational time. Since SDF is an $\ell_{2}$-norm attack, we use the $\ell_{2}$-norm version of AA as well. We restrict maximum iterations of SDF to $10$. If the norm of perturbations exceeds $\varepsilon$, we renormalize the perturbation to ensure its norm stays $\leq\varepsilon$.
In this context, we have modified the AA algorithm by replacing APGD$^\top$~\cite{croce2020reliable} with SDF due to the former's cost and computation bottleneck in the context of AA. We compare the fooling rate and computational time of AA++ and AA on the stat-of-the-art models from the RobustBench~\cite{croce2020robustbench} leaderboard. In Table~\ref{AA++}, we observe that AA++ is up to three times faster than AA. In an alternative scenario, we added the SDF to the beginning of the AA set, resulting in a version that is up to two times faster than the original AA, despite now containing five attacks~(see Appendix). This outcome highlights the efficacy of SDF in finding adversarial examples. These experiments suggest that leveraging efficient \textit{minimum-norm} and \textit{non-fixed iteration} attacks, such as SDF, can enable faster and more reliable evaluation of the robustness of deep models.


