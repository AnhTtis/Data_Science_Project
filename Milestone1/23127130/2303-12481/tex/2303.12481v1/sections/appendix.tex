\section{Proofs}
\label{DeepFool-Proof}
% \begin{proposition}
%     Let the binary classifier $f:\mathbb{R}^{d} \rightarrow \mathbb{R}$ be continuously differentiable and its gradient $\nabla f$ be $L^{'}$-Lipschitz. For a given input sample $\x_0$, suppose $B(\x_{0},\epsilon)$ is a ball centered around $\x_0$ with radius $\epsilon$, such that there exists $\x\in B(\x_{0},\epsilon)$ that $f(\x)=0$. If $\|\nabla f\|_{2}\geq \zeta$ for all $\x\in B$ and $\epsilon < \frac{\zeta^2}{{L^{'}}^{2}}$, then DF iterations converge to a point on the decision boundary.
% \end{proposition}
 
\textbf{Proof of Proposition 1.}

Since $\nabla f(\x)$ is Lipschitz-continuous, for $\x,\y \in B(\x_{0},\epsilon)$, we have:
\begin{equation}
    \label{Deepfool_Optimality_step1}
    |f(\x)-f(\y)+\nabla f(\y)^T(\x-\y)|\leq \frac{L^{'}}{2}\|\x-\y\|^2
\end{equation}
DeepFool updates the new $\x_{n}$ in each according to the following equation:
\begin{equation}
    \label{Deepfool_step}
    \x_{n} = \x_{n-1} + \frac{\nabla f(\x_{n-1})}{\|\nabla f(\x_{n-1})\|_{2}}f(\x_{n-1})
\end{equation}
Hence if we substitute $\x=\x_{n}$ and $\y=\x_{n-1}$ in \ref{Deepfool_Optimality_step1}, we get:

\begin{equation}
    \label{five}
    |f(\x_{n})|\leq \frac{L^{'}}{2} \|\x_{n}-\x_{n-1}\|^2.
\end{equation}
Now, let $s_{n}:=||\x_n - \x_{n-1}||$. Using \ref{five} and DeepFool's step, we get:
\begin{equation}
    \label{6}
    s_{n+1}=\frac{f(\x_{n})}{\|\nabla f(\x_{n})\|} \leq \frac{L^{'}}{2\zeta}\frac{f(\x_{n})^2}{\|\nabla f(\x_{n})\|^2}
\end{equation}
We also know that for $\x,\x^{*} \in B(\x_{0},\epsilon)$ and the Lipschitz property:
\begin{equation}
    |f(\x)-f(\x^{*})| \leq 2L^{'}\epsilon
\end{equation}
From property $\x^{*}$ we know that $f(\x^{*})=0$, so:
\begin{equation}
    \label{epsilon-bound}
    |f(\x)|\leq 2L^{'}\epsilon
\end{equation}

\begin{equation}
    s_{n+1}=\frac{f(\x_{n})}{||\nabla f(\x_{n})||} \leqslant s_{n} \epsilon \frac{L^{'2}}{\zeta^2}
\end{equation}
Using the assumptions of the  theorem,\hspace{1mm}We have $\frac{L^{'} \epsilon}{\zeta^2} < 1$,\hspace{1mm} and hence $s_{n}$ converges to $0$ when $n\rightarrow \infty$.
\hspace{1mm}We conclude that $\{\x_{n}\}$ is a Cauchy sequence.
Denote by $\x_{\infty}$ the limit point of $\{\x_n\}$. Using the continuity of $f$ and Eq.(\ref{five}),\hspace{1mm} we obtain 
\begin{equation}
    \lim_{n \rightarrow \infty}|f(\x_{n})|=|f(\x_{\infty})|=|f(\x^{*})|=0,
\end{equation}
It is clear that $\x^*$ is $\x$. 
which concludes the proof of the theorem.


% \begin{proposition}
% For a differentiable $f$ and a given $\boldsymbol{r}_0$, $\boldsymbol{r}_i$ in the iterations \eqref{eq:iterative_proj} either converge to a solution of \eqref{eq:proj_max} or a trivial solution (i.e., $\boldsymbol{r}_i\rightarrow 0$).
% \end{proposition}
\textbf{Proof of Proposition 2.}
\label{Proof-orthogonality}
Let us denote the acute angle between $\nabla f(\x_0+\boldsymbol{r}_i)$ and $\boldsymbol{r}_i$ by $\theta_i$ ($0\leq \theta_i\leq \pi/2$). Then from (4) we have $\vert\boldsymbol{r}_{i+1}\vert = \vert\boldsymbol{r}_i\vert \cos \theta_i$. Therefore, we get
\begin{equation}
    \vert\boldsymbol{r}_{i+1}\vert = \prod_{i=1}^i \cos \theta_i \vert\boldsymbol{r}_0 \vert.
\end{equation}
Now there are two cases, either $\theta_i\to 0$ or not. Let us first consider the case where zero is not the limit of $\theta_i$. Then there exists some $\epsilon_0>0$ such that for any integer $N$ there exists some $n>N$ for which we have $\theta_n>\epsilon_0$. Now for $\epsilon_0$, we can have a series of integers $n_i$ where for all of them we have $\theta_{n_i}> \epsilon_0$. Since we have $0\leq \vert\cos \theta \vert \leq 1$, we have the following inequality:
\begin{equation}
    0\leq \prod_{i=0}^{\infty} \vert\cos \theta_i\vert \leq  \prod_{i=0}^{\infty} \vert \cos \theta_{n_i}\vert \leq \prod_{i=0}^{\infty} \vert\cos \epsilon_0\vert
\end{equation}
The RHS of the above inequality goes to zero which proves that $\boldsymbol{r}_i\to 0$. This leaves us with the other case where $\theta_i \to 0$. This means that $\cos \theta_i \to 1$ which is the maximum of \eqref{eq:proj_max}, this completes the proof.


\section{On the benefits of line search}
\label{line-search}
As we show in Figure~\ref{fig:overly} DF typically finds an overly perturbed point. SDF's gradients depend on DF, so overly perturbing DF is problematic. Line search is a mechanism that we add to the end of our algorithms to tackle this problem. For a fair comparison between adversarial attacks, we add this algorithm to the end of other algorithms to investigate the effectiveness of line search. 
\begin{table}[h]
	\centering

	\caption{Comparison of the effectiveness of line search on the CIFAR-$10$ data for SDF and DF. We 
      use one regularly trained model S~(WRN-$28$-$10$) and three adversarially trained models~(shown with R1~\cite{Rony_2019_CVPR}, R2~\cite{augustin2020adversarial} and R3~\cite{rade2021helper}).~\checkmark and \xmark ~indicate the presence and absence of line search respectively.}
	\small

	\begin{tabular}{rcccc}
		\toprule
		
		\multirow{2}{*}{Model}&\multicolumn{2}{c}{DF}&\multicolumn{2}{c}{SDF}\\
		\cmidrule(lr){2-3} \cmidrule(lr){4-5}
		&\checkmark&\xmark&\checkmark&\xmark\\
        
        \midrule
        
            S & $0.16$ & $0.19$  & $\mathbf{0.09}$& $0.10$\\        
		R1&$0.87$&$1.02$&$\mathbf{0.73}$&$0.76$\\
		
		R2 &$1.40$&$1.73$&$\mathbf{0.91}$&$0.93$\\
		
		R3&$1.13$&$1.36$&$\mathbf{1.04}$&$1.09$\\
		\bottomrule
  
	\end{tabular}
	\label{tab:LS-1}
\end{table}

 As shown in Table~\ref{tab:LS-1}, we observe that line search can increase the performance of the DF significantly. However, this effectiveness for SDF is a little.

We now measure the effectiveness of line search for other attacks. As observed from Table~\ref{tab:LS-2}, line search effectiveness for DDN and ALMA is small.
\begin{table*}
    \centering

    \caption{Comparison of the effectiveness of line search on
        the CIFAR-10 data for other attacks. Line search effects are a little for DDN and ALMA. For FMN and FAB because they use line search at the end of their algorithms~(they remind this algorithm as a \textit{binary search} and \textit{final search}, respectively), line search does not become effective.}
    \label{tab:LS-2}

    \begin{small}
        \begin{sc}
    \begin{tabular}{@{}ccccccccc@{}}
        \toprule		
        \multirow{2}{*}{Model}&\multicolumn{2}{c}{DDN}&\multicolumn{2}{c}{ALMA}&\multicolumn{2}{c}            
             {FMN}&\multicolumn{2}{c}{FAB}\\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
        
        &\checkmark&\xmark&\checkmark&\xmark&\checkmark&\xmark&\checkmark&\xmark\\
        \midrule
        WRN-$28$-$10$& $0.12$ & $0.13$& $0.10$ & $0.10$ & $0.11$ & $0.11$ & $0.11$ & $0.11$  \\
        
    
        R1~\cite{Rony_2019_CVPR} & $0.73$ & $0.73$ & $0.71$ & $0.71$ & $1.10$ & $1.10$ & $0.75$ & $0.75$\\
        
        
        R2~\cite{augustin2020adversarial}&$0.96$&$0.97$&$0.93$&$0.94$&$0.95$&$0.95$&$1.03$&$1.03$\\
        
        
        R3~\cite{rade2021helper}&$1.04$&$1.04$&$1.06$&$1.06$&$1.08$&$1.08$&$1.07$&$1.07$\\		
    \bottomrule
    \end{tabular}        
    \end{sc}
    \end{small}
	
\end{table*}





\section{Comparison on CIFAR-$10$ with the PRN-$18$}
\label{Rade-CIFAR-10}In this section, we compare SDF with other minimum-norm attacks against an adversarially trained network~\cite{rade2021helper}. In Table~\ref{tab:CIFAR_RADE}, SDF achieves smaller perturbation compared to other attacks, whereas it costs only half as much as other attacks.

\begin{table}[h]
    \caption{ Comparison of SDF with other state-of-the-art attacks for median $\ell_{2}$ on CIFAR-$10$ dataset for adversarially trained network (PRN-$18$~\cite{rade2021helper}).}
    \label{tab:CIFAR_RADE}  

    \centering    
    \begin{small}
    \begin{sc}
    \begin{tabular}{rccc}
         \toprule         
         Attack
         & FR
         & Median-$\ell_{2}$
         & Grads \\
         \midrule
        
         ALMA & $100$ & $0.68$ & $100$\\
         DDN & $100$ & $0.73$ & $100$ \\
         FAB  & $100$ & $0.77$ & $210$ \\
         FMN  & $99.7$ & $0.81$ & $100$ \\
         SDF & $100$ & $\mathbf{0.65}$ & $\mathbf{46}$\\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}  
\end{table}

\section{Performance comparison of adversarially trained models versus Auto-Attack~(AA)}
\label{AA-AT}
Evaluating the adversarially trained models with attacks used in the training process is not a standard evaluation in the robustness literature. For this reason, we evaluate robust models with AA. We perform this experiment with two modes; first, we measure the robustness of models with $\ell_\infty$ norm, and in a second mode, we evaluate them in terms of $\ell_{2}$ norm. Tables~\ref{tab:AA-L-inf} and \ref{tab:AA-L-2} show that adversarial training with SDF samples is more robust against reliable AA than the model trained on DDN samples~\cite{Rony_2019_CVPR}.



\begin{table}[h]
	\centering
    	\caption{Robustness results of adversarially trained models on CIFAR-$10$ with $\ell_{\infty}$-AA. We perform this experiment on $1000$ samples for each $\epsilon$.}

     \begin{small}
     \begin{sc}
    	\begin{tabular}{rcccc}
            \toprule
            Model &Natural & $\varepsilon=\frac{6}{255}$ & $\frac{8}{255}$ & $\frac{10}{255}$\\

    		\midrule
    		DDN&$89.1$&$45$&$29.6$&$17.6$\\
    		
    		SDF (Ours)&$90.8$&$\mathbf{47.5}$&$\mathbf{38.1}$&$\mathbf{25.4}$\\
    		\bottomrule
    	\end{tabular}
     \end{sc}
     \end{small}


	\label{tab:AA-L-inf}
\end{table}

\begin{table}[h]
	\centering
    % \vspace{0.2mm}
    	\caption{Robustness results of adversarially trained models on CIFAR-$10$ with $\ell_{2}$-AA. We perform this experiment on $1000$ samples for each $\epsilon$.}

    \begin{small}
     \begin{sc}
    	\begin{tabular}{rccccc}
            \toprule
            Model &Natural & $\varepsilon=0.3$ & $0.4$ & $0.5$ & $0.6$\\

    		\midrule
    		DDN& $89.1$ & $78.1$  & $73$  & $67.5$  & $61.7$\\
    		
    		SDF (Ours) &$90.8$ & $\mathbf{83.1}$ & $\mathbf{79.7}$ & $\mathbf{68.1}$ & $\mathbf{63.9}$\\
    		\bottomrule
    	\end{tabular}
     \end{sc}
     \end{small}
     
        % \vspace{-2mm}
	\label{tab:AA-L-2}
\end{table}


\section{Another variants of AA++}
As we mentioned, in an alternative scenario, we added the SDF to the beginning
of the AA set, resulting in a version that is up to two times
faster than the original AA. In this scenario, we do not exchange the SDF with APGD. We add SDF to the AA configuration. So in this configuration, AA has five attacks~(SDF, APGD, APGD$^\top$, FAB, Square). By this design, we guarantee the performance of AA. An interesting phenomenon observed from these tables is that when the budget increases, the speed of the AA++ increases. We should note that we restrict the number of iterations for SDF to $10$.
\begin{figure}[h]
	\centering
	\begin{subfigure}[t]{0.49\linewidth} 
		% \centering
		\includegraphics[width=1.0\linewidth]{photos/Curves/AA/AA_1.pdf}
		\caption{R1}
		\label{fig:4_1}
	\end{subfigure} 

	\begin{subfigure}[t]{0.499\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{photos/Curves/AA/AA_Radde_1.pdf}
		\caption{S}
		\label{fig:4_2}
	\end{subfigure} 
	\vspace{-3mm}
	\caption{ In this figure, we show the time ratio of AA to AA++. For regularly 
        trained model~(WRN-$28$-$10$) and adversarially trained model~\cite{rade2021helper}~(R1). We perform this experiment on $1000$ samples from CIFAR-$10$ data.}
	\label{fig:AA-time-comparsion}
	\vspace{-5mm}
\end{figure}


\section{Why do we need stronger minimum-norm attacks?}
Bounded-norm attacks like FGSM~\cite{goodfellow2014explaining}, PGD~\cite{madry2017towards}, and momentum variants of PGD~\cite{uesato2018adversarial}, by optimizing the difference between the logits of the true class and the best non-true class, try to find an adversarial region with maximum confidence within a given, fixed perturbation size. 
Bounded-norm attacks only evaluate the robustness of deep neural networks; this means that they report a single scalar value as robust accuracy for a fixed budget.
The superiority of minimum-norm attacks is to report a distribution of perturbation norms, and they do not report a percentage of fooling rates~(robust accuracy) by a single scalar value. This critical property of minimum-norm attacks helps to accelerate to take an in-depth intuition about the geometrical behavior of deep neural networks.

We aim to address a phenomenon we observe by using the superiority of minimum-norm attacks. We observed that a minor change within the design of deep neural networks affects the performance of adversarial attacks. 
To show the superiority of minimum-norm attacks, we show how minimum-norm attacks verify these minor changes rather than bounded-norm attacks.

Modeling with max-pooling was a fundamental aspect of convolutional neural networks when they were first introduced as the best image classifiers. Some state-of-the-art classifiers such as~\cite{krizhevsky2017imagenet,simonyan2014very,he2016deep} use this layer in network configuration. We use the pooling layers to show that using the max-pooling and Lp-pooling layer in the network design leads to finding perturbation with a bigger $\ell_2$-norm.

Assume that we have a classifier ($f$). We train $f$ in two modes until the training loss converges. In the first mode, $f$ is trained in the presence of the pooling layer in its configuration, and in the second mode, $f$ does not have a pooling layer. 
When we measure the robustness of these two networks with regular budgets used in bounded-norms attacks like PGD~($\varepsilon=8/255$), we observe that the robust accuracy is equal to $0\%$. This is precisely where bounded-norm attacks such as PGD mislead robustness literature in its assumptions regarding deep neural network properties. However, a solution to solve the problem of bounded-norm attack scan be proposed: \textit{" Analyzing the quantity of changes in robust accuracy across different epsilons reveal these minor changes."} Is this case, the solution is costly.
This is precisely where the distributive view of perturbations from worst-case to best-case of minimum-norm attacks detects this minor change.



To show these changes, we trained ResNet-$18$~(RN-$18$) and Mobile-Net~\cite{howard2017mobilenets} in two settings. In the first setting, we trained them in the presence of a pooling layer until the training loss converged, and in the second setting, we trained them in the absence of a pooling layer until the training loss converged. We should note that we remove all pooling-layers in these two settings.
For a fair comparison, we train models until they achieve zero training loss using a multi-step learning rate. We use max-pooling and Lp-pooling, for $p=2$, for this minor changes.

Table~\ref{tab:max-pooling} shows that using a pooling layer in network configuration can increase robustness. DF has an entirely different behavior according to the presence or absence of the pooling layer; max-pooling affects up to $50\%$ of DF performance. This effect is up to $9\%$ for DDN and FMN. ALMA and SDF show a $4\%$ impact in their performance, which shows their consistency compared to other attacks.
\begin{table*}
	\centering
	\caption{This table shows the $\ell_{2}$-median for the minimum-norm attacks. For all 
        networks, we set learning rate = $0.01$ and weight decay = $0.01$. For training with Lp-pooling, we set $p=2$ for all settings.}

	\begin{sc}
	    \begin{small}	        	    

	\begin{tabular}{rccccccccccc}
		\toprule		
		\multirow{2}{*}{Attack}&\multicolumn{3}{c}{ResNet-$18$}&\multicolumn{3}{c}{MobileNet}&\\
		\cmidrule(lr){2-4} \cmidrule(lr){5-8}
	 	&no pooling&max-pooling&Lp-pooling&no pooling&max-pooling&Lp-pooling\\

        \midrule
		DF & $0.40$ & $0.90$  & $0.91$ & $0.51$ & $0.95$  & $0.93$ \\
		DDN & $0.16$ & $0.25$  & $0.26$ & $0.22$ & $0.27$  & $0.26$\\
		FMN & $0.18$ & $0.27$ & $0.30$ & $0.24$ & $0.30$  & $0.29$ \\
		C$\&$W & $0.18$ & $0.25$  & $0.27$ & $0.22$ & $0.26$   & $0.24$ \\
		ALMA & $0.19$ & $0.23$ &  $0.23$ & $\mathbf{0.20}$ & $0.25$  & $0.22$ \\
		SDF & $\mathbf{0.16}$ & $\mathbf{0.21}$ & $\mathbf{0.22}$ & $\mathbf{0.20}$ &   $\mathbf{0.23}$  & $\mathbf{0.21}$ \\
		\bottomrule
	\end{tabular}
	\end{small}
	\end{sc}
	\label{tab:max-pooling}
\end{table*}

As shown in Table~\ref{tab:max-pooling-AA-PGD}, we observe that models with pooling-layers have more robust accuracy when facing adversarial attacks such as AA and PGD. It should be noted that using regular epsilon for AA and PGD will not demonstrate these modifications. For this reason, we choose an epsilon for AA and PGD lower~($\varepsilon = 2/255$) than the regular format~($\varepsilon = 8/255$).

\begin{table*}
	\centering
	\caption{This table shows the robust accuracy for all networks against to the AA and 
        PGD. For training with Lp-pooling, we set $p=2$ for all settings.}

	\begin{sc}
	    \begin{small}	        	    

	\begin{tabular}{rccccccccccc}
		\toprule		
		\multirow{2}{*}{Attack}&\multicolumn{3}{c}{ResNet-$18$}&\multicolumn{3}{c}{MobileNet}&\\
		\cmidrule(lr){2-4} \cmidrule(lr){5-8}
	 	&no pooling&max-pooling&Lp-pooling&no pooling&max-pooling&Lp-pooling\\

        \midrule
            AA & $1.1\%$ & $17.2\%$ & $16.3\%$ & $8.7\%$ & $21.3\%$ & $20.2\%$\\
            PGD & $9.3\%$ & $28\%$ & $26.2\%$ & $16.8\%$ & $31.4\%$ & $28.7\%$\\
		\bottomrule

		
	\end{tabular}
	\end{small}
	\end{sc}
	\label{tab:max-pooling-AA-PGD}
\end{table*}

Table~\ref{tab:max-pooling} and~\ref{tab:max-pooling-AA-PGD} demonstrate that including pooling-layers can enhance the ability of the neural network to withstand various types of adversarial attacks. Powerful attacks such as SDF and ALMA show high consistency in these setup modifications, highlighting the need for powerful attacks.



\subsection{Analysis of max-pooling from the perspective of curvature and Hessian norms.}
\label{curvature-maxpooling}
Here, we take a step further and investigate why max-pooling impacts the robustness of models. In order to perform this analysis, we analyze gradient norms, Hessian norms, and the model's curvature. The curvature of a point is a mathematical quantity that indicates the degree of nonlinearity. A function's curvature is often expressed as the norm of the Hessian at a particular point~\cite{moosavi2019robustness}. Mainly, robust models are characterized by low gradient norms~\cite{hein2017formal}, implying smaller Hessian norms. In order to investigate robustness independent of non-linearity,~\cite{srinivasefficient} propose \textit{normalized curvature}, which normalizes the Hessian norm by its corresponding gradient norm.
They defined \textit{normalized curvature} for a neural network classifier $f$ as  $\C_f(\x) = \| \grad^2 f(\x) \|_2 / (\| \grad f(\x) \|_2 + \varepsilon)$. Where $\| \grad f(\x) \|_2$ and $\| \grad^2 f(\x) \|_2$ are the $\ell_2$-norm of the gradient and the spectral norm of the Hessian, respectively, where $ \grad f(\x) \in \bbR^d, \grad^2 f(\x) \in \bbR^{d \times d}$, and $\varepsilon > 0$ is a small constant to ensure the proper behavior of the measure. We use this metric to conduct our experiments.
\begin{table}[h]
    \caption{Model geometry of different ResNet-$18$ models. W~(with pooling) and W/O~(without pooling).}
    \label{tab:geometry-2}
    \begin{sc}
        \begin{small}            
        \begin{tabular}{rccc} 
        \toprule
         \textbf{Model} & $\E_{\x} \| \grad f(\x) \|_2$ & $\E_{\x} \| \grad^2 f(\x) \|_2$ & $\E_{\x} \C_f(\x)$\\ 
         \midrule
         w & $\mathbf{4.75}$ \std{$1.54$} & $\mathbf{120.70}$ \std{$48.74$} & $\mathbf{14.94}$ \std{$0.52$}\\
         w/o & $7.04$ \std{$2.44$} & $269.74$ \std{$10.23$} & $22.81$ \std{$2.58$}\\ \bottomrule
        \end{tabular}
    \end{small}
    \end{sc}
\end{table}
\begin{table}[h]
    \caption{Model geometry for regular and adversarially trained models.}
     \label{tab:geometry}
     \begin{small}
         \begin{sc}                    
    \begin{tabular}{rccc} 
    \toprule
     Model & $\E_{\x} \| \grad f(\x) \|_2$ & $\E_{\x} \| \grad^2 f(\x) \|_2$ & $\E_{\x} \C_f(\x)$\\ 
     \midrule
     Standard & $9.54$ \std{$1.02$} & $600.06$ \std{$29.76$} & $73.99$ \std{$6.62$}\\
     DDN AT & $0.91$ \std{$0.34$} & $2.86$ \std{$1.22$} & $4.32$ \std{$2.91$}\\

     SDF AT & $\mathbf{0.38}$ \std{$0.60$} & $\mathbf{0.73}$ \std{$0.08$} & $\mathbf{1.66}$ \std{$0.86$}\\
     \bottomrule
    \end{tabular}
\end{sc}
     \end{small}
\end{table}


\section{CNN architecture used for the CIFAR-$10$ in one scenario}
 Table~\ref{tab:cnns_attack} shows layers used in~\cite{carlini2017towards} and~\cite{Rony_2019_CVPR} for evaluation on CIFAR-$10$.
\begin{table}[!t]
    \small
    \centering
    % \begin{minipage}{0.2\textwidth}
    % \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{ll}
    \toprule
    Layer Type & CIFAR-$10$ \\ 
    \midrule
    Convolution + ReLU & $3\times 3 \times 64$\\
    Convolution + ReLU & $3\times 3 \times 64$\\
    max-pooling & $2 \times 2$\\
    Convolution + ReLU & $3\times 3 \times 128$\\
    Convolution + ReLU & $3\times 3 \times 128$\\
    max-pooling & $2 \times 2$\\
    Fully Connected + ReLU & $256$ \\
    Fully Connected + ReLU & $256$ \\
    Fully Connected + Softmax & $10$ \\
    
    \bottomrule
    \end{tabular}
    % }
    \caption{CNN architecture.}
    \label{tab:cnns_attack}
    % \end{minipage}
\end{table}
 


\section{Multi class algorithms for SDF(1,3) and SDF(1,1)}
We have introduced how to generate adversarial examples with the SDF algorithm in the main body of this paper. This section gives more details about combining SDF steps for multi-class settings. We provide
the pseudo-code of SDF(1,1) and SDF(1,3) in Algorithm~\ref{alg:SDF(1,1)} and~\ref{alg:SDF(1,3)}.

% \AlgoDontDisplayBlockMarkers
% \RestyleAlgo{ruled}
% \SetAlgoNoLine
% \LinesNumbered
\begin{algorithm*}
	\SetKwFunction{Union}{Union}\SetKwFunction{DeepFool}{\texttt{DeepFool}}\SetKwFunction{l}{\texttt{projection step}}
 	\KwIn{image $\x$, classifier $f$.}
 	\KwOut{perturbation $\boldsymbol{r}$}
  
	Initialize: $\x_{0}\leftarrow\x,\enskip i\leftarrow 0$
	
    \While{$\hat{k}(\x_{i})= \hat{k}(\x_{0})$}
    	 {  
        \smallskip
        \For{$k\neq \hat{k}(\x_0)$}     
        { 
            \smallskip
             $\w'_k\gets \nabla f_k(\x_i)-\nabla f_{\hat{k}(\x_0)}(\x_i)$ 
             
            
            \smallskip
            $f'_k\gets f_k(\x_i)-f_{\hat{k}(\x_0)}(\x_i)$
        }
        \smallskip
        $\hat{l}\gets\argmin_{k\neq{\hat{k}(\x_0)}}\frac{\left|f'_k\right|}{\|\w'_k\|_2}$
        
        \smallskip
        $\boldsymbol{\widetilde{r}}\gets \frac{\left|f'_{\hat{l}}\right|}{\|\w'_{\hat{l}}\|_2^2}\w'_{\hat{l}}$
      
      \smallskip
      $\widetilde{\x}_{i} = \x_{i} + \boldsymbol{\widetilde{r}}$

    \smallskip
    $\w_{i}\leftarrow\nabla f_{k(\widetilde{\x}_{i})}(\widetilde{\x}_{i}) - \nabla f_{k(\x_{0})}(\widetilde{\x}_{i})$
    
    \smallskip
    $\x \gets \x_0 + \frac{(\widetilde{\x}_{i}-\x_0)^\top \w_{i}}{\| \w_{i}\|^2} \w_{i}$

    		
    		\smallskip
    		$i\leftarrow i+1$
    	  }	
 	\KwRet $\boldsymbol{r}=\x_{i}-\x_{0}$
    \vspace{2mm}
\caption{SDF ($1$,$1$)}
\label{alg:SDF(1,1)}
\end{algorithm*}

% \AlgoDontDisplayBlockMarkers
% \RestyleAlgo{ruled}
% \SetAlgoNoLine
% \LinesNumbered
\begin{algorithm*}
	\SetKwFunction{Union}{Union}\SetKwFunction{DeepFool}{\texttt{DeepFool}}\SetKwFunction{l}{\texttt{projection step}}
 	\KwIn{image $\x$, classifier $f$.}
 	\KwOut{perturbation $\boldsymbol{r}$}
  
	Initialize: $\x_{0}\leftarrow\x,\enskip i\leftarrow 0$
	
    \While{$\hat{k}(\x_{i})= \hat{k}(\x_{0})$}
    	 {  
        \smallskip
        \For{$k\neq \hat{k}(\x_0)$}     
        { 
            \smallskip
             $\w'_k\gets \nabla f_k(\x_i)-\nabla f_{\hat{k}(\x_0)}(\x_i)$ 
             
            
            \smallskip
            $f'_k\gets f_k(\x_i)-f_{\hat{k}(\x_0)}(\x_i)$
        }
        \smallskip
        $\hat{l}\gets\argmin_{k\neq{\hat{k}(\x_0)}}\frac{\left|f'_k\right|}{\|\w'_k\|_2}$
        
        \smallskip
        $\boldsymbol{\widetilde{r}}\gets \frac{\left|f'_{\hat{l}}\right|}{\|\w'_{\hat{l}}\|_2^2}\w'_{\hat{l}}$
      
      \smallskip
      $\widetilde{\x}_{i} = \x_{i} + \boldsymbol{\widetilde{r}}$

    
    \smallskip
     
    \For{$3$ steps}{
      \smallskip
      $\w_{i}\leftarrow\nabla f_{k(\widetilde{\x}_{i})}(\widetilde{\x}_{i}) - \nabla f_{k(\x_{0})}(\widetilde{\x}_{i})$
      
      \smallskip
      $\x_{i} \gets \x_0 + \frac{(\widetilde{\x}_{i}-\x_0)^\top \w_{i}}{\| \w_{i}\|^2}  \w_{i}$
    
    
    }		
    		\smallskip
    		$i\leftarrow i+1$
    	  }	
 	\KwRet $\boldsymbol{r}=\x_{i}-\x_{0}$
  \vspace{2mm}
\caption{SDF ($1$,$3$)}
\label{alg:SDF(1,3)}
\end{algorithm*}


\clearpage

% \section{Examples of adversarial images generated using SDF}
\begin{figure*}
\centering
{\onecolumn

    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[width=\linewidth]{photos/ImageNet/makaroni.pdf}
    \end{subfigure}\!
    \begin{subfigure}[b]{\columnwidth}
    \includegraphics[width=\linewidth]{photos/ImageNet/tower.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[width=\linewidth]{photos/ImageNet/mouse.pdf}
    \end{subfigure}\!
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[width=\linewidth]{photos/ImageNet/shirini.pdf}
    \end{subfigure}
    \caption{Adversarial examples for ImageNet, as computed by SDF on a ResNet-$50$ architecture.~(Perturbations are magnified $\sim10\times$ for better visibility.)}
    }
    \label{fig:pert_imagenet}
\end{figure*}