\vspace{-0.08in}
\paragraph{Why does $\ell_2$ white-box adversarial robustness matter?}
\vspace{-0.11in}
The reasons for using $\ell_2$ norm perturbations are manifold.
We acknowledge that $\ell_2$ threat model may not seem particularly realistic in practical scenarios (at least for images); however, it can be perceived as a basic threat model amenable to both theoretical and empirical analyses, potentially leading insights in tackling adversarial robustness in more complex settings. The fact that, despite considerable advancements in AI/ML, we are yet to solve adversarial vulnerability, motivates part of our community to return to the basics and work towards finding fundamental solutions to this issue~\cite{Jailbreakbench-Maksym,Carlini-Prompt,Wong-Prompt}.
In particular, thanks to their intuitive geometric interpretation, $\ell_2$ perturbations provide valuable insights into the geometry of classifiers. They can serve as an effective tool in the "interpretation/explanation" toolbox to shed light on what/how these models learn.
Moreover, it has been demonstrated that~\cite{Guille_Optimism,engstrom2019adversarial}, $\ell_2$ robustness has several applications beyond security~(for more details on the necessity of robustness to $\ell_{p}$ norms, please refer to Appendix~\ref{L-P}).\looseness=-1
%A note on the spectral properties of $\ell_2$ perturbations: It is not entirely accurate to categorize $\ell_2$ perturbations as “high-frequency”. On the contrary, these perturbations tend to align more with "low-frequency" bands (see, e.g., [11]).
\vspace{-0.12in}
\section{DeepFool (DF) and Minimal Adversarial Perturbations}
\vspace{-0.10in}
\label{DeepFool}
In this section, we first discuss the geometric interpretation of the minimum-norm adversarial perturbations, i.e., solutions to the optimization problem in~\eqref{global-formula}.
We then examine DF to demonstrate why it may fail to find the optimal minimum-norm perturbation. Then in the next section, we introduce our proposed method that exploits DF to find smaller perturbations.

Let $f$ : $\mathbb{R}^{d} \rightarrow \mathbb{R}^{C}$ denote a $C$-class classifier, where $f_k$ represents the classifier's output associated to the $k$th class. Specifically, for a given datapoint $\x$ $\in$ $\mathbb{R}^{d}$, the estimated label is obtained by $\hat{k}(\x)= \text{argmax}_{k} f_{k}(\x)$, where $f_{k}(\x)$ is the $k^{\text{th}}$ component of $f(\x)$ that corresponds to the $k^{\text{th}}$ class.
Note that the classifier $f$ can be seen as a mapping that partitions the input space $\mathbb{R}^{d}$ into classification regions, each of which has a constant estimated label (i.e., $\hat{k}(.)$ is constant for each such region). The decision boundary $\mathscr{B}$ is defined as the set of points in $\mathbb{R}^d$ such that $f_i(\x)=f_j(\x)=\max_{k}f_k(\x)$ for some distinct $i$ and $j$.
Additive $\ell_2$-norm adversarial perturbations are inherently related to the geometry of the decision boundary. More formally, Let $\x \in \mathbb{R}^{d}$, and $\boldsymbol{r}^{*}(\x)$ be the minimal adversarial perturbation defined as the minimizer of~\eqref{global-formula}. Then: 
\insight{\textbf{\textit{Properties of \textbf{minimal adversarial perturbation $\rightarrow$ $\boldsymbol{r}^{*}(\x)$}}}}
{

\circled{1} It is orthogonal to the decision boundary of the classifier $\mathscr{B}$.
	
\circled{2} Its norm, i.e., $\|\boldsymbol{r}^{*}(\x)\|_2$ measures the Euclidean distance between $\x$ and $\mathscr{B}$, that is $\x+\boldsymbol{r}^{*}$ lies on $\mathscr{B}$.}
%Then  $\boldsymbol{r}^{*}(\x)$,
% 1) is orthogonal to the decision boundary of the classifier $\mathscr{B}$, and 2) its norm $\|\boldsymbol{r}^{*}(\x)\|_2$ measures the Euclidean distance between $\x$ and $\mathscr{B}$, that is $\x+\boldsymbol{r}^{*}$ lies on $\mathscr{B}$. 
 We aim to investigate whether the perturbations generated by DF satisfy the aforementioned two conditions. Let $\boldsymbol{r}_\text{DF}$ denote the perturbation found by DF for a datapoint $\x$.
We expect $\boldsymbol{x}+\boldsymbol{r}_\text{DF}$ to lie on the decision boundary. Hence, if $\boldsymbol{r}$ is the minimal perturbation, for all $0<\gamma <1$, we expect the perturbation $\gamma \boldsymbol{r}$  to remain in the same decision region as of $\x$ and thus fail to fool the model.
%on about the geometrical behavior of deep neural networks.
\begin{wrapfigure}[16]{r}{0.40\textwidth}
	\vspace{-0.30cm}
	\centering
	\includegraphics[width=0.3\textwidth]{photos/orthogonality_illustration.pdf}
	\caption{Illustration of the optimal adversarial example $\x+\boldsymbol{r}^*$ for a binary classifier $f$; the example lies on the decision boundary (set of points where $f(\x)=0$) and the perturbation vector $\boldsymbol{r}^*$ is orthogonal to this boundary.}
	\label{fig:orthogonality_illust}
\end{wrapfigure}
%\paragraph{\textbf{Geometric conditions of optimal perturbations.}} 

Fig.~\ref{fig:orthogonality_illust} illustrates the two conditions discussed in Section~\ref{DeepFool}. In the figure, \(n_1\) and \(n_2\) represent two orthogonal vectors to the decision boundary. The optimal perturbation vector \(\boldsymbol{r}^*\) aligns parallel to \(n_2\). On the other hand, a non-optimal perturbation \(\boldsymbol{r}_{\text{DF}}\) forms an angle \(\alpha\) with \(n_1\).

In Fig.~\ref{fig:DF_analysis}~(left), we consider the fooling rate of $\gamma\,\boldsymbol{r}_\text{DF}$ for $0.2<\gamma<1$. For a minimum-norm perturbation, we expect an immediate sharp decline for $\gamma$ close to one. However, in Fig.~\ref{fig:DF_analysis}~(top-left) we cannot observe such a decline (a sharp decline happens close to $\gamma=0.9$, not 1).
This is a confirmation that DF typically finds an overly perturbed point. One potential reason for this is the fact that DF stops when a misclassified point is found, and this point might be an overly perturbed one within the adversarial region, and not necessarily on the decision boundary.

%\begin{wrapfigure}[11]{r}{0.30\textwidth}
%	\vspace{-0.780cm}
%    \centering
%    \includegraphics[width=0.25\textwidth]{photos/orthogonality_illustration.pdf}
%    \caption{\orig{Illustration of the optimal adversarial example $\x+\boldsymbol{r}^*$ for a binary classifier $f$; the example lies on the decision boundary (set of points where $f(\x)=0$) and the perturbation vector $\boldsymbol{r}^*$ is orthogonal to this boundary.}}
%    \label{fig:orthogonality_illust}
%\end{wrapfigure}

Now, let us consider the other characteristic of the minimal adversarial perturbation. That is, the perturbation should be orthogonal to the decision boundary. We measure the angle between the found perturbation $\boldsymbol{r}_\text{DF}$ and the normal vector orthogonal to the decision boundary~($\nabla f(\x+\boldsymbol{r}_\text{DF})$). To do so, we first scale $\boldsymbol{r}_\text{DF}$ such that $\x+\gamma\boldsymbol{r}_\text{DF}$ lies on the decision boundary. It can be simply done via performing a line search along $\boldsymbol{r}_\text{DF}$. We then compute the cosine of the angle between $\boldsymbol{r}_\text{DF}$ and the normal to the decision boundary at $\x+\gamma\boldsymbol{r}_\text{DF}$~(this angle is denoted by $\cos(\alpha)$). 
A necessary condition for $\gamma\boldsymbol{r}_\text{DF}$ to be an optimal perturbation is that it must be parallel to the normal vector of the decision boundary.
In Fig.~\ref{fig:DF_analysis}~(right) , we show the distribution of cosine of this angle. Ideally, we wanted this distribution to be accumulated around one. However, it clearly shows that this is not the case, which is a confirmation that $\boldsymbol{r}_\text{DF}$ is not necessarily the minimal perturbation.
% \begin{figure}[t]
% % \begin{wrapfigure}[7]{r}{0.5\textwidth}
%     \centering
%     \includegraphics[width=0.3\columnwidth]{photos/Overly_perturbed_DF_S.pdf}
%     \caption{We generated $1000$ images with one hundred $\gamma$ between zero and one, and the fooling rate of the DF is reported. This experiment is done on the CIFAR-$10$ dataset and ResNet-$18$~\cite{he2016deep} model. The accuracy of this Network is $94\%$.}
%     \label{fig:overly}
% \end{figure}
% % \end{wrapfigure}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.3\columnwidth]{photos/orthogonality-2_S.pdf}
%     \caption{Histogram of the cosine angle distribution between the gradient in the last step of DF and the perturbation vector obtained by DF.
%     This experiment has been performed on $1000$ images from the CIFAR-$10$~\cite{krizhevsky2009learning} dataset with the ResNet-$18$~\cite{he2016deep} model.}
%     \label{fig:orthogonality}
% \end{figure}

% We generated $1000$ images with one hundred $\gamma$ between zero and one, and the fooling rate of the DF is reported. This experiment is done on the CIFAR-$10$ dataset and ResNet-$18$~\cite{he2016deep} model. The accuracy of this Network is $94\%$

% Histogram of the cosine angle distribution between the gradient in the last step of DF and the perturbation vector obtained by DF.
        % This experiment has been performed on $1000$ images from the CIFAR-$10$~\cite{krizhevsky2009learning} dataset with the ResNet-$18$~\cite{he2016deep} model.

        
% \begin{figure}[t]
% \centering
%     \begin{subfigure}[b]{0.30\columnwidth}
%         % \caption*{$\leq0.01\%$}
%         \includegraphics[width=\linewidth]{photos/Overly_perturbed_DF_S.pdf}
%         \vspace{-2mm}
%         \caption{Overly perturbed analysis of DF.}
%     \end{subfigure}\!
%     \begin{subfigure}[b]{0.31\columnwidth}
%         % \caption*{$(0.01\%,0.1\%)$}
%         \includegraphics[width=\linewidth]{photos/orthogonality-2_S.pdf}
%         \vspace{-2.1mm}
%         \caption{Orthogonality analysis of DF.}
%     \end{subfigure}\!
%     \caption{In \textbf{(a)} we generated 1000 images with one hundred $\gamma$ between zero and one, and the fooling rate of the DF is reported. This experiment is done on the CIFAR10 dataset and ResNet18 model.
%     In \textbf{(b)} histogram of the cosine angle distribution between the gradient in the last step of DF and the perturbation vector obtained by DF.
%         This experiment has been performed on 1000 images from the CIFAR-10 dataset with the ResNet-18 model. }
%     \label{fig:DF_analysis}
% \end{figure}

\begin{figure}[h]
\centering
\begin{tabular}{c c}
% \raisebox{-0.5\height}
%{\includegraphics[width=0.33\linewidth]{photos/Overly_perturbed_DF_S_iclr.pdf}}&
% \raisebox{-0.5\height}
%{\includegraphics[width=0.33\textwidth]{photos/orthogonality-2_S_iclr.pdf}} \\
{\includegraphics[width=0.35\textwidth]{photos/Overly_perturbed_SDF_S_iclr.pdf}} &
{\includegraphics[width=.35\linewidth]{photos/orthogonality_SDF_S_iclr.pdf}}
% \raisebox{-0.5\height}

\end{tabular}

\caption{\textbf{(\textit{Left})} we generated 1000 images with one hundred $\gamma$ between zero and one, and the fooling rate of the DeepFool and SuperDeepFool is reported. This experiment is done on the CIFAR10 dataset and ResNet18 model. \textbf{(\textit{Right})} histogram of the cosine angle between the normal to the decision boundary and the perturbation vector obtained by DeepFool and SuperDeepFool has been showed.\looseness=-1}
\label{fig:DF_analysis}
\end{figure}


