\vspace{-0.25in}
\section{Experimental Results}
\vspace{-0.10in}
\label{sec:experiments}
In this section, we conduct extensive experiments to demonstrate the effectiveness of our method in different setups and for several natural and adversarially trained networks. We first introduce our experimental settings, including datasets, models, and attacks.
Next, we compare our method with state-of-the-art $\ell_{2}$-norm adversarial attacks in various settings, demonstrating the superiority of our simple yet fast algorithm for finding accurate adversarial examples.
Moreover, we add SDF  to the collection of attacks used in AutoAttack, and call the new set of attacks AutoAttack++. This setup meaningfully speeds up the process of finding norm-bounded adversarial perturbations. We also demonstrate that a model adversarially training using the SDF perturbations becomes more robust compared to the models\footnote{We only compare to publicly available models.} trained using other minimum-norm attacks. Please refer to Appendix~\ref{Setup} for details of the experimental setup and metrics.

% \subsection{Setup}



% \begin{table}
% 	\centering
% 	\caption{The cosine of the angle between the perturbation
%     vector ($\boldsymbol{r}$) and $\nabla f(\x +\boldsymbol{r})$. We performed this experiment on three models trained on CIFAR-$10$ dataset.  The evaluation is done using $1000$ random samples. 
%     }

%     \begin{small}
%     \begin{sc}
% 	\begin{tabular}{lrrr}
%         \toprule
% 		\multirow{2}{*}{Attack} & \multicolumn{3}{c}{Models}\\
%         \cmidrule{2-4}
%         & LeNet & ResNet-18 & WRN-28-10\\
% 		\midrule
% 		DF & $0.89$ &  $0.14$ & $0.21$\\
% 		SDF~(1,1) & $0.90$ & $0.63$  & $0.64$ \\
% 		SDF~(1,3) & $0.88$ & $0.61$  & $0.62$\\
% 		SDF~(3,1) & $\mathbf{0.92}$ & $0.70$ & $0.72$ \\
%         SDF & $\mathbf{0.92}$ & $\mathbf{0.72}$  & $\mathbf{0.80}$\\
% 		\bottomrule
% 	\end{tabular}
% \end{sc}
% \end{small}
% 	\label{tab:last_grad}
% \end{table}




% \begin{table}[t!]
%     \selectfont
% 	\centering
% 	\caption{Comparison of the mean and the median of $\ell_{2}$-norm of perturbations for DF and SDF family algorithms. We performed this experiment on CIFAR-$10$. We use the same model
%     architectures and hyperparameters for training as in \cite{carlini2017towards,Rony_2019_CVPR}. For more details about the architecture see the appendix.}
%     \begin{small}
%     \begin{sc}
% 	\begin{tabular}{lrrr}
% 		\toprule
% 		Attack & Mean-$\ell_{2}$ & Median-$\ell_{2}$ & Grads \\
% 		\midrule
% 		DF & $0.17$ & $0.15$ & $\mathbf{14}$ \\
% 		SDF~(1,1) & $0.14$ & $0.13$ & $22$ \\
% 		SDF~(1,3) & $0.16$ & $0.14$ & $26$ \\
% 		SDF~(3,1) & $0.12$ & $0.11$ & $30$ \\
% 		SDF & $\mathbf{0.11}$ & $\mathbf{0.10}$ & $32$ \\
% 		\bottomrule
% 	\end{tabular}
% \end{sc}
% \end{small}
% 	\label{tab:CIFAR10_architecture_DF}
% \end{table}

%\begin{table}
%	\begin{minipage}{0.5\linewidth}
%		\centering
%		\caption{The cosine similarity between the perturbation vector($\boldsymbol{r}$) and $\nabla f(\x +\boldsymbol{r})$. We performed this experiment on three models trained on CIFAR10.}
%		% \vspace{-3mm}
%		\begin{small}
%			 \begin{sc}
%			\resizebox{0.90\linewidth}{!}{
%				\begin{tabular}{lrrr}
%					\toprule
%					\multirow{2}{*}{Attack} & \multicolumn{3}{c}{Models} \\
%					\cmidrule{2-4}
%					& LeNet & RN18 & WRN-28-10 \\
%					\midrule
%					DF & $0.89$ & $0.14$ & $0.21$ \\
%					SDF~(1,1) & $0.90$ & $0.63$ & $0.64$ \\
%					SDF~(1,3) & $0.88$ & $0.61$ & $0.62$ \\
%					SDF~(3,1) & $\mathbf{0.92}$ & $0.70$ & $0.72$ \\
%					\rowcolor{Gray} SDF~$(\infty,1)$ & $\mathbf{0.92}$ & $\mathbf{0.72}$ & $\mathbf{0.80}$ \\
%					\bottomrule
%				\end{tabular}
%			}
%			 \end{sc}
%		\end{small}
%		\label{tab:last_grad}
%	\end{minipage}
%%\hfill
%%	\begin{minipage}{0.47\linewidth}
%%		\centering
%%		\caption{Comparison of $\ell_{2}$-norm perturbations using DF and SDF algorithms on CIFAR10, employing consistent model architectures and hyperparameters as those used in~\cite{carlini2017towards,Rony_2019_CVPR} studies.}
%%		\begin{small}
%%			 \begin{sc}
%%			% \vspace{-3mm}
%%			\resizebox{0.8\linewidth}{!}{
%%				\begin{tabular}{lrr}
%%					\toprule
%%					Attack & Median-$\ell_{2}$ & Grads \\
%%					\midrule
%%					DF  & $0.15$ & $\mathbf{14}$ \\
%%					SDF~(1,1)& $0.13$ & $22$ \\
%%					SDF~(1,3)& $0.14$ & $26$ \\
%%					SDF~(3,1)& $0.11$ & $30$ \\
%%					\rowcolor{Gray}SDF$(\infty,1)$& $\mathbf{0.10}$ & $32$ \\
%%					\bottomrule
%%				\end{tabular}
%%			}
%%			 \end{sc}
%%		\end{small}
%%		\label{tab:CIFAR10_architecture_DF}
%%	\end{minipage}
%	% \caption{Two Tables Side by Side}
%	% \vspace{-0.5cm}
%\end{table}


\subsection{Comparison with DeepFool (DF)} \label{sec:exp-sdf}
% \begin{wraptable}{r}{0.5\textwidth}
%     \centering
%     \caption{The cosine of the angle between the perturbation vector ($\boldsymbol{r}$) and $\nabla f(\x +\boldsymbol{r})$. We performed this experiment on three models trained on CIFAR10 dataset.}
%     % \vspace{-3mm}
%     \begin{small}
%         % \begin{sc}
%         \resizebox{0.99\linewidth}{!}{
%             \begin{tabular}{cccc}
%                 \toprule
%                 \multirow{2}{*}{Attack} & \multicolumn{3}{c}{Models} \\
%                 \cmidrule{2-4}
%                 & LeNet & ResNet18 & WRN-28-10 \\
%                 \midrule
%                 DF & $0.89$ & $0.14$ & $0.21$ \\
%                 SDF~(1,1) & $0.90$ & $0.63$ & $0.64$ \\
%                 SDF~(1,3) & $0.88$ & $0.61$ & $0.62$ \\
%                 SDF~(3,1) & $\mathbf{0.92}$ & $0.70$ & $0.72$ \\
%                 SDF~$(\infty,1)$ & $\mathbf{0.92}$ & $\mathbf{0.72}$ & $\mathbf{0.80}$ \\
%                 \bottomrule
%             \end{tabular}
%         }
%         % \end{sc}
%     \end{small}
%     \label{tab:last_grad}
% \end{wraptable}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{wraptable}[12]{r}{0.37\textwidth}
%	\centering
%	%	\vspace{-0.05cm}
%	\vspace{-0.50cm}
%	\caption{The cosine similarity between the perturbation vector($\boldsymbol{r}$) and $\nabla f(\x +\boldsymbol{r})$. We performed this experiment on three models trained on CIFAR10.}
%	\vspace{-0.3cm}
%	%		\begin{small}
%	%			\begin{sc}
%	\resizebox{1.001\linewidth}{!}{
%		\begin{tabular}{lrrr}
%			\toprule
%			\multirow{2}{*}{Attack} & \multicolumn{3}{c}{Models} \\
%			\cmidrule{2-4}
%			& LeNet & RN18 & WRN-28-10 \\
%			\midrule
%			DF & $0.89$ & $0.14$ & $0.21$ \\
%			SDF~(1,1) & $0.90$ & $0.63$ & $0.64$ \\
%			SDF~(1,3) & $0.88$ & $0.61$ & $0.62$ \\
%			SDF~(3,1) & $\mathbf{0.92}$ & $0.70$ & $0.72$ \\
%			\rowcolor{Gray} SDF~$(\infty,1)$ & $\mathbf{0.92}$ & $\mathbf{0.72}$ & $\mathbf{0.80}$ \\
%			\bottomrule
%		\end{tabular}
%	}
%	%			\end{sc}
%	%		\end{small}
%	\label{tab:last_grad}
%\end{wraptable} 
In this part, we compare our algorithm in terms of orthogonality and size of the $\ell_{2}$-norm perturbations especially with DF. Assume $\boldsymbol{r}$ is the perturbation vector obtained by an adversarial attack. First, we measure the orthogonality of perturbations by measuring the inner product between $\nabla f(\x+\boldsymbol{r})$ and $\boldsymbol{r}$.
As we explained in Section~\ref{DeepFool}, a larger inner product between $\boldsymbol{r}$ and the gradient vector at $f(\x+\boldsymbol{r})$ indicates that the perturbation vector is closer to the optimal perturbation vector $\boldsymbol{r}^{*}$.
We compare the orthogonality of different members of the SDF family and DF.

\begin{wraptable}[11]{r}{0.45\textwidth}
%	\centering
	\vspace{-0.45cm}
%	\vspace{-0.10cm}
	\caption{The cosine similarity between the perturbation vector($\boldsymbol{r}$) and $\nabla f(\x +\boldsymbol{r})$. We performed this experiment on three models trained on CIFAR10.}
	\vspace{-0.15cm}
	%		\begin{small}
	%			\begin{sc}
	\resizebox{1.005\linewidth}{!}{
		\begin{tabular}{lrrr}
			\toprule
			\multirow{2}{*}{Attack} & \multicolumn{3}{c}{Models} \\
			\cmidrule{2-4}
			& LeNet & RN18 & WRN-28-10 \\
			\midrule
			DF & $0.89$ & $0.14$ & $0.21$ \\
			SDF~(1,1) & $0.90$ & $0.63$ & $0.64$ \\
			SDF~(1,3) & $0.88$ & $0.61$ & $0.62$ \\
			SDF~(3,1) & $\mathbf{0.92}$ & $0.70$ & $0.72$ \\
			\rowcolor{Gray} SDF~$(\infty,1)$ & $\mathbf{0.92}$ & $\mathbf{0.72}$ & $\mathbf{0.80}$ \\
			\bottomrule
		\end{tabular}
	}
	%			\end{sc}
	%		\end{small}
	\label{tab:last_grad}
\end{wraptable}

The results are shown in Table~\ref{tab:last_grad}. We observe that DF finds perturbations orthogonal to the decision boundary for low-complexity models such as LeNet, but fails to perform effectively when evaluated against more complex ones. In contrast, attacks from the SDF family consistently found perturbations with a larger cosine of the angle for all three models.

\paragraph{\textbf{Verifying optimality conditions for SDF}.}
We validate the optimality conditions of the perturbations generated by SDF using the procedure outlined in Section~\ref{DeepFool}. Comparing \Figref{fig:DF_analysis} DF and SDF, it becomes evident that our approach effectively mitigates the two issues we previously highlighted for DF. Namely, the alignment of the perturbation with the normal to the decision boundary and the problem of over-perturbation. We can see that unlike DF, the cosine of the angle for SDF is more concentrated around one, which indicates that the SDF perturbations are more aligned with the normal to the decision boundary. Moreover, Fig.~\ref{fig:DF_analysis} shows a sharper decline in the fooling rate (going down quickly to zero) when $\gamma$ decreases. This is consistent with our expectation for an accurate minimal perturbation attack.

\begin{wraptable}[14]{r}{0.40\textwidth}
	\vspace{-0.60cm}
	\centering
	\caption{We evaluate the performance of iteration-based attacks on MNIST using IBP models, noting the iteration count in parentheses. Our analysis focuses on the best-performing versions, highlighting their significant costs when encountered powerful robust models.}
	\vspace{-1.30mm}
	\label{tab:IBP}
	\resizebox{1.001\linewidth}{!}{
		\begin{tabular}{lrrr}
			\toprule
			
			Attack & FR & Median-$\ell_{2}$ & Grads \\
			\midrule
			DF & $93.4$ & $5.31$ & $43$	\\
			
			ALMA ($1000$) & $\mathbf{100}$ & $\mathbf{1.26}$ &  $1\,000$\\
			%                    ALMA  ($100$) & $98.90$ & $4.96$ &  $100$\\
			DDN ($1000$)  & $99.27$ & $1.46$ &  $1\,000$ \\
			%                    DDN ($100$)  & $94.34$ & $1.97$ & $100$ \\
			FAB ($1000$) & $99.98$ & $3.34$ &  $10\,000$ \\
			%                    FAB  ($100$)  & $99.98$ & $5.19$ &  $1\,000$ \\
			FMN ($1000$)  & $89.08$ & $1.34$ &  $1\,000$ \\
			%                    FMN  ($100$)  & $67.80$ & $2.14$ &  $100$ \\
			C\&W  & $4.63$ & \orig{--} &  $90\,000$  \\
			\rowcolor{Gray}SDF & $\mathbf{100}$ & $1.37$ & $\mathbf{52}$\\
			\bottomrule
		\end{tabular}    
	}
\end{wraptable}
\subsection{Comparison with minimum-norm attacks}
\label{Compare-Other}
We now compare SDF with SOTA minimum $\ell_{2}$-norm attacks: C\&W, FMN, DDN, ALMA, and FAB. For C\&W, we use the same hyperparameters as in~\cite{Rony_2019_CVPR}. We use FMN, FAB, DDN, and ALMA with budgets of $100$ and $1000$ iterations and report the best performance. For a fair comparison, we clip the pixel-values of SDF-generated adversarial images to $[0,1]$, consistent with the other minimum-norm attacks. We report the average number of gradient computations per sample, as these operations are computationally intensive and provide a consistent metric unaffected by hardware differences. We also provide a runtime comparison~(Appendix Table~\ref{tab:time-CIFAR10-AT}).


We evaluate the robustness of the IBP model, which is adversarially trained on the MNIST dataset, against SOTA attacks in Table~\ref{tab:IBP}.
We choose this robust model as it allows us to have a more nuanced comparison between different adversarial attacks.
SDF and ALMA are the only attacks that achieve a $100\%$ percent fooling rate against this model, whereas C\&W is unsuccessful on most of the data samples. The fooling rates of the remaining attacks also degrade when evaluated with $100$ iterations. For instance, FMN's fooling rate decreases from $89\%$ to $67.8\%$ when the number of iterations is reduced from $1000$ to $100$. This observation shows that, unlike SDF, selecting the \textbf{\textit{necessary number of iterations}} is critical for the success of \textbf{\textit{fixed-iteration}} attacks. Even for ALMA which can achieve a nearly perfect FR, decreasing the number of iterations from $1000$ to $100$ causes the median norm of perturbations to increase fourfold. In contrast, SDF is able to compute adversarial perturbations using the fewest number of gradient computations while still outperforming the other algorithms, except ALMA, in terms of the perturbation norm. However, it is worth noting that ALMA requires twenty times more gradient computations compared to SDF to achieve a  marginal improvement in the perturbation norm.

\begin{wraptable}[12]{r}{0.46\textwidth}
	%	\begin{minipage}[t]{0.40\linewidth}
	\vspace{-0.48cm}
	\centering
	\caption{Performance of attacks on the CIFAR-10 dataset with naturally trained WRN-$28$-$10$.}
	\label{tab:CIFAR-10}
	\vspace{-0.20cm}
	\resizebox{0.9\linewidth}{!}{
		\begin{tabular}{lrrr}
			\toprule
			Attacks & FR & Median-$\ell_{2}$ & Grads \\
			\midrule
			DF & $100$ & $0.26$ & $\mathbf{14}$ \\
			ALMA & $100$ & $0.10$ & $100$ \\
			DDN & $100$ & $0.13$ & $100$ \\
			FAB & $100$ & $0.11$ & $100$ \\
			FMN & $97.3$ & $0.11$ & $100$ \\
			C\&W & $100$ & $0.12$ & $90\,000$ \\
			\rowcolor{Gray}SDF & $100$ & $\mathbf{0.09}$ & $25$ \\
			\bottomrule
		\end{tabular}
	}
	%			\end{sc}
	%		\end{small}
	%	\end{minipage}
	% \vspace{-0.5cm}
\end{wraptable}

Table~\ref{tab:CIFAR-10} compares SDF with SOTA attacks on the CIFAR10 dataset. The results show that SOTA attacks have a similar norm of perturbations, but an essential point is the speed of attacks. SDF finds more accurate adversarial perturbation very quickly rather than other algorithms. 

We also evaluated all attacks on an adversarially trained model for the CIFAR10 dataset. SDF achieves smaller perturbations with half the gradient calculations than other attacks. SDF finds smaller adversarial perturbations for adversarially trained networks at a significantly lower cost than other attacks, requiring only $20\%$ of FAB's cost and $50\%$ of DDN's and ALMA's~(See Tables~\ref{tab:CIFAR_RADE},~\ref{tab:time-CIFAR10-AT} in the Appendix).

\begin{wraptable}[10]{r}{0.5\textwidth}
	\vspace{-0.5cm}
	\centering
	\caption{Performance comparison of SDF with other SOTA attacks on ImageNet dataset with natural trained RN-50 and adversarially trained RN-50.}
	\label{tab:ImageNet}
	\vspace{-0.3cm}
	
	%		\begin{small}
	%	\begin{minipage}{0.40\linewidth}
	\resizebox{1.01\linewidth}{!}{
		\begin{tabular}{lrr|p{2.1cm}|rr|r p{2.3cm}}
			\toprule
			& \multicolumn{3}{c}{RN-50} & \multicolumn{3}{c}{RN-50 (AT)} \\
			\cmidrule(lr){2-4} \cmidrule(lr){5-7}
			Attack & FR & Median-$\ell_{2}$ & Grads & FR & Median-$\ell_{2}$ & Grads \\
			\midrule
			DF &  $99.1$ & $0.31$ & $\mathbf{23}$ & $98.8$ & $1.36$ & $\mathbf{34}$ \\
			ALMA &  $\mathbf{100}$ & $0.10$ & $100$ & $\mathbf{100}$ & $0.85$ & $100$ \\
			DDN &  $99.9$ & $0.17$ & $1,000$ & $99.7$ & $1.10$ & $1,000$ \\
			FAB  & $99.3$ & $0.10$ & $900$ & $\mathbf{100}$ & $0.81$ & $900$ \\
			FMN  & $99.3$ & $0.10$ & $1,000$ & $99.9$ & $0.82$ & $1,000$ \\
			C\&W  & $\mathbf{100}$ & $0.21$ & $82,667$ & $99.9$ & $1.17$ & $52,000$ \\
			\rowcolor{Gray}SDF  & $\mathbf{100}$ & $\mathbf{0.09}$ & $37$ & $\mathbf{100}$ & $\mathbf{0.80}$ & $49$ \\
			\bottomrule
		\end{tabular}
	}
	%	\end{minipage}
	%		\end{small}
	\label{tab:results}
%	\vspace{-0.9cm}
\end{wraptable}

Table~\ref{tab:ImageNet} demonstrates the performance of SDF on a naturally and adversarially trained models on ImageNet dataset. Unlike models trained on CIFAR10, where the attacks typically result in perturbations with similar norm, the differences between attacks are more nuanced for ImageNet models.

In particular, FAB, DDN, and FMN performance degrades when the dataset changes. In contrast, SDF achieves smaller perturbations at a significantly lower cost than ALMA.
This shows that the geometric interpretation of optimal adversarial perturbation, rather than viewing (\ref{global-formula}) as a non-convex optimization problem, can lead to an efficient solution.
On the complexity aspect, the proposed approach is substantially faster than the other methods. In contrast, these approaches involve a costly minimization of a series of objective functions. We empirically observed that SDF converges in less than $5$ or $6$  iterations to a fooling perturbation; our observations show that SDF consistently achieves SOTA minimum-norm perturbations across different datasets, models, and training strategies, while requiring the least number of gradient computations.
This makes it readily suitable to be used as a baseline method to estimate the robustness of very deep neural networks on large datasets.


% \begin{wraptable}{l}{0.45\textwidth}
% % \begin{table}
%     \centering
%     \caption{Performance comparison of SDF with other state-of-the-art attacks for median $\ell_{2}$ on ImageNet dataset. FR columns show the fooling rates of attacks.}
%     \label{tab:ImageNet}
%     \centering
%     \begin{small}
%     \begin{tabular}{llrrr}
%                 \toprule
%                 Model & Attack & FR & Median-$\ell_{2}$ & Grads \\
%                          \midrule
%         \multirow{8}{*}{\makecell{RN-$50$}}
%          & DF & $99.1$ & $0.31$ & $\mathbf{23}$\\
%          & ALMA & $\mathbf{100}$ & $0.10$ & $100$\\
%          & DDN & $99.9$ & $0.17$ & $1\,000$ \\
%          & FAB & $99.3$ & $0.10$ & $900$ \\
%          & FMN  & $99.3$ & $0.10$ & $1\,000$ \\
%          & C\&W  & $\mathbf{100}$ & $0.21$ & $82\,667$  \\
%          & SDF & $\mathbf{100}$ & $\mathbf{0.09}$ & $37$\\
%                 \midrule
                
%                 \multirow{8}{*}{\makecell{RN-$50$\\(AT)}}
%                 & DF & $98.8$ & $1.36$ & $\mathbf{34}$ \\
%                 & ALMA & $\mathbf{100}$ & $0.85$ & $100$ \\
%                 & DDN & $99.7$ & $1.10$ & $1,000$ \\
%                 & FAB & $\mathbf{100}$ & $0.81$ & $900$ \\
%                 & FMN & $99.9$ & $0.82$ & $1,000$ \\
%                 & C\&W & $99.9$ & $1.17$ & $52,000$ \\
%                 & SDF & $\mathbf{100}$ & $\mathbf{0.80}$ & $49$ \\
%                 \bottomrule
%             \end{tabular}        
%     \end{small}
% % \end{table}
% \end{wraptable}
%%%%%%%%%%%% ImageNet %%%%%%%%%%%%%
% \begin{table}[!t]
%     \caption{The performance comparison of SDF with other state-of-the-art attacks for median $\ell_{2}$ on ImageNet dataset. FR columns show the fooling rates of attacks.}
%     \vspace{-3mm}
%     \label{tab:ImageNet}  
%     \centering    
%     \begin{small}
%     % \begin{sc}
%     \resizebox{0.39\linewidth}{!}{
%     \begin{tabular}{llrrr}
%          \toprule
%          Model
%          & Attack
%          & FR
%          & Median-$\ell_{2}$
%          & Grads \\
%         %  \midrule
%         % \multirow{8}{*}{\makecell{RN-$50$}}
%         %  & DF & $99.1$ & $0.31$ & $\mathbf{23}$\\
%         %  & ALMA & $\mathbf{100}$ & $0.10$ & $100$\\
%         %  & DDN & $99.9$ & $0.17$ & $1\,000$ \\
%         %  & FAB & $99.3$ & $0.10$ & $900$ \\
%         %  & FMN  & $99.3$ & $0.10$ & $1\,000$ \\
%         %  & C\&W  & $\mathbf{100}$ & $0.21$ & $82\,667$  \\
%         %  & SDF & $\mathbf{100}$ & $\mathbf{0.09}$ & $37$\\
%          \midrule
%         \multirow{8}{*}{\makecell{RN-$50$\\(AT)}}
%          & DF & $98.8$ & $1.36$ & $\mathbf{34}$\\
%          & ALMA & $\mathbf{100}$ & $0.85$ & $100$\\
%          & DDN  & $99.7$ & $1.10$ & $1\,000$ \\
%          & FAB  & $\mathbf{100}$ & $0.81$ & $900$ \\
%          & FMN  & $99.9$ & $0.82$ & $1\,000$ \\
%          & C\&W & $99.9$ & $1.17$ & $52\,000$  \\
%          & SDF & $\mathbf{100}$ & $\mathbf{0.80}$ & $49$\\
%         \bottomrule
%     \end{tabular}
%     }
%     % \end{sc}
%     \end{small}  
% \end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{SDF Adversarial Training (AT)}
\label{SDF-AT}
\begin{wraptable}[10]{r}{0.42\textwidth}
	\vspace{-0.43cm}
	\centering
	\caption{The comparison between $\ell_2$ robustness of our adversarial trained model and~\cite{Rony_2019_CVPR} model.}
	\vspace{-0.25cm}
	\label{tab:AT-CIFAR10}
%	\begin{small}
		% \begin{sc}
		\resizebox{1.001\linewidth}{!}{
			\begin{tabular}{lrrrrr}
				\toprule
				\multirow{2}{*}{Attack} & \multicolumn{2}{c}{SDF (Ours)} & \multicolumn{2}{c}{DDN} & \multirow{1}{*}{} \\
				\cmidrule(lr){2-3}\cmidrule(lr){4-5}
				& Mean & Median & Mean & Median
				%& Gain
				\\
				\midrule
				DDN & $1.09$ & $1.02$ & $0.86$ & $0.73$ %& $\uparrow$$29\%$
				\\
				FAB & $1.12$ & $1.03$ & $0.92$ & $0.75$ %& $\uparrow$$28\%$
				\\
				FMN & $1.48$ & $1.43$ & $1.47$ & $1.43$ 
				%& $0\%$
				\\
				ALMA & $1.17$ & $1.06$ & $0.84$ & $\mathbf{0.71}$ 
				%& $\uparrow$$35\%$ 
				\\
				\rowcolor{Gray}SDF & $\mathbf{1.06}$ & $\mathbf{1.01}$ & $\mathbf{0.81}$ & $0.73$ 
				%& $\uparrow$$28\%$
				\\
				\bottomrule
			\end{tabular}
		}
		% \end{sc}
%	\end{small}
\end{wraptable}
In this section, we evaluate the performance of a model adversarially trained using SDF against minimum-norm attacks and AutoAttack. 
Our experiments provide valuable insights into the effectiveness of adversarial training with SDF and sheds light on its potential applications in building more robust models.
%Adversarial training has become a powerful approach to fortify deep neural networks against adversarial perturbations. However, some adversarial attacks, such as C$\&$W, pose a challenge due to their high computational cost and complexity, making them unsuitable for adversarial training.
Adversarial training requires computationally efficient attacks, making costly options such as C$\&$W unsuitable.
Therefore, an attack that is parallelizable (both on batch size and gradient computation) is desired for successful adversarial training. SDF possesses these crucial properties, making it a promising candidate for building more robust models.

We adversarially train a WRN-$28$-$10$ on CIFAR10. Similar to the procedure followed in~\cite{Rony_2019_CVPR}, we restrict $\ell_{2}$-norms of perturbation to $2.6$ and set the maximum number of iterations for SDF to $6$. We train the model on clean examples for the first $200$ epochs, and we then fine-tune it with SDF generated adversarial examples for $60$ more epochs. Since a model trained using DDN-generated samples~\cite{Rony_2019_CVPR} has demonstrated greater robustness compared to a model trained using PGD~\cite{madry2017towards}, we compare our model with that one~(for more details about AT please refer to Appendix~\ref{AT-reg}).
%\begin{table}[t]
%	\begin{minipage}{0.403\linewidth}
%%		\centering
%		\caption{The comparison between $\ell_2$ robustness of our adversarial trained model and~\cite{Rony_2019_CVPR} model. We perform this experiment by using three different random seeds for training on CIFAR10. }
%		% \vspace{-3mm}
%		\label{tab:AT-CIFAR10}
%		\begin{small}
%			% \begin{sc}
%			\resizebox{1.1\linewidth}{!}{
%			\begin{tabular}{lrrrrr}
%				\toprule
%				\multirow{2}{*}{Attack} & \multicolumn{2}{c}{SDF (Ours)} & \multicolumn{2}{c}{DDN} & \multirow{1}{*}{} \\
%				\cmidrule(lr){2-3}\cmidrule(lr){4-5}
%				& Mean & Median & Mean & Median
%				%& Gain
%				\\
%				\midrule
%				DDN & $1.09$ & $1.02$ & $0.86$ & $0.73$ %& $\uparrow$$29\%$
%				\\
%				FAB & $1.12$ & $1.03$ & $0.92$ & $0.75$ %& $\uparrow$$28\%$
%				\\
%				FMN & $1.48$ & $1.43$ & $1.47$ & $1.43$ 
%				%& $0\%$
%				\\
%				ALMA & $1.17$ & $1.06$ & $0.84$ & $\mathbf{0.71}$ 
%				%& $\uparrow$$35\%$ 
%				\\
%				\rowcolor{Gray}SDF & $\mathbf{1.06}$ & $\mathbf{1.01}$ & $\mathbf{0.81}$ & $0.73$ 
%				%& $\uparrow$$28\%$
%				\\
%				\bottomrule
%			\end{tabular}
%		}
%			% \end{sc}
%		\end{small}
%	\end{minipage}\hfill
%	\begin{minipage}{0.41\linewidth}
%		\centering
%		\caption{Average input curvature of AT models. According to the measures proposed in~\cite{srinivasefficient}. The second column shows the average spectral-norm of the Hessian w.r.t. input, and the third column shows the average of the same quantity normalized by the norm of the input gradient.}
%		\label{tab:curvature}
%		\begin{small}
%%			 \begin{sc}
%			\resizebox{1.001\linewidth}{!}{
%			\begin{tabular}{lrr} 
%				\toprule
%				Model & $\E_{\mathbf{x}} \|\grad^2 f(\mathbf{x})\|_2$ & $\E_{\mathbf{x}} \mathcal{C}_f(\mathbf{x})$ \\ 
%				\midrule
%				Standard & $600.06$ (29.76) & $73.99$ (6.62) \\
%				DDN AT & $2.86$ (1.22) & $4.32$ (2.91) \\
%				SDF AT (Ours) & $\mathbf{0.73}$ (0.08) & $\mathbf{1.66}$ (0.86) \\
%				\bottomrule
%			\end{tabular}
%		}
%%			 \end{sc}
%		\end{small}
%	\end{minipage}
%	\vspace{-0.5cm}
%\end{table}
%In~\cite{Rony_2019_CVPR}, vanilla adversarial training with DDN generated adversarial examples is shown build a more robust model than a model trained with PGD~\cite{madry2017towards}. we compare our model with it.
%\begin{wraptable}[12]{r}{0.40\textwidth}
%	%	\begin{minipage}{0.41\linewidth}
%	\vspace{-0.43cm}
%	\centering
%	\caption{Average input curvature of AT models. According to the measures proposed in~\cite{srinivasefficient}. The second column shows the average spectral-norm of the Hessian w.r.t. input, and the third column shows the average of the same quantity normalized by the norm of the input gradient.}
%	\vspace{-0.2cm}
%	\label{tab:curvature}
%	\resizebox{0.99\linewidth}{!}{
%		\begin{tabular}{lrr} 
%			\toprule
%			Model & $\E_{\mathbf{x}} \|\grad^2 f(\mathbf{x})\|_2$ & $\E_{\mathbf{x}} \mathcal{C}_f(\mathbf{x})$ \\ 
%			\midrule
%			Standard & $600.06$ (29.76) & $73.99$ (6.62) \\
%			DDN AT & $2.86$ (1.22) & $4.32$ (2.91) \\
%			SDF AT (Ours) & $\mathbf{0.73}$ (0.08) & $\mathbf{1.66}$ (0.86) \\
%			\bottomrule
%		\end{tabular}
%	}
%\end{wraptable}
Our model reaches a test accuracy of $90.8\%$ while the model by~\cite{Rony_2019_CVPR} obtains $89.0\%$. SDF adversarially trained model does not overfit to SDF attack because, as Table~\ref{tab:AT-CIFAR10} shows, SDF obtains the smallest perturbation. It is evident that SDF adversarially trained model can significantly improve the robustness of model against minimum-norm attacks up to $30\%$.
In terms of comparison of these two adversarially trained models with AA, our model outperformed the~\cite{Rony_2019_CVPR} by improving about $8.4\%$ against $\ell_{\infty}$-AA, for $\varepsilon=8/255$, and $0.6\%$ against $\ell_{2}$-AA, for $\varepsilon=0.5$.
 
\begin{wraptable}[7]{r}{0.40\textwidth}
	%	\begin{minipage}{0.41\linewidth}
	\vspace{-0.45cm}
	%	\centering
	\caption{Average input curvature of AT models. According to the measures proposed in~\cite{srinivasefficient}.}
	\vspace{-0.2cm}
	\label{tab:curvature}
	\resizebox{0.99\linewidth}{!}{
		\begin{tabular}{lrr} 
			\toprule
			Model & $\E_{\mathbf{x}} \|\grad^2 f(\mathbf{x})\|_2$ & $\E_{\mathbf{x}} \mathcal{C}_f(\mathbf{x})$ \\ 
			\midrule
			Standard & $600.06$ (29.76) & $73.99$ (6.62) \\
			DDN AT & $2.86$ (1.22) & $4.32$ (2.91) \\
			SDF AT (Ours) & $\mathbf{0.73}$ (0.08) & $\mathbf{1.66}$ (0.86) \\
			\bottomrule
		\end{tabular}
	}
\end{wraptable}
Furthermore, compared to a network trained on DDN samples, our adversarially trained model has a smaller input curvature (Table~\ref{tab:curvature}). The second column shows the average spectral-norm of the Hessian w.r.t. input, $\|\nabla^2 f(\mathbf{x})\|_2$, and the third column shows the average of the same quantity normalized by the norm of the input gradient, $\mathcal{C}_f(\mathbf{x}) = \|\nabla^2 f(\mathbf{x})\|_2/\|\nabla f(\mathbf{x})\|_2$. The standard deviation is denoted by numbers enclosed in brackets.


This observation corroborates the idea that a more robust network will exhibit a smaller input curvature \cite{moosavi2019robustness,srinivasefficient,qin2019adversarial,CO-Guille,ELLE,GradAlign}.



\myparagraph{AutoAttack++}
%\subsection{AutoAttack++}
\label{AA++-section}
\begin{wraptable}[13]{r}{0.51\textwidth} % Right-aligned, half-page width
%\begin{table}[h]
	\centering
	\vspace{-0.45cm}
	\caption{Analysis of robust accuracy for various defense strategies against AA++ and AA with $\varepsilon = 0.5$ for six adversarially trained models on CIFAR10. All models are taken from the RobustBench library~\cite{croce2020robustbench}.}
	\vspace{-0.2cm}
	\begin{small}
		\resizebox{1.0\linewidth}{!}{
			\begin{tabular}{lrrrrr}
				\toprule
				\multirow{2}{*}{\makecell[c]{Models}}
				& 
				& \multicolumn{2}{c}{AA}
				& \multicolumn{2}{c}{AA++}\\
				\cmidrule(lr){3-4} \cmidrule(lr){5-6} 
				& Clean acc.& Robust acc. & Grads & Robust acc. & Grads \\
				\midrule
				R1~\cite{rebuffi2021fixing} & $95.7\%$ & $82.3\%$ & $1259.2$ & $\mathbf{82.1\%}$ & $\mathbf{599.5}$\\
				R2~\cite{Proxy}  & $90.3\%$ & $76.1\%$ & $1469.1$ & $76.1\%$ & $\mathbf{667.7}$\\
				R3~\cite{gowal2020uncovering} & $89.4\%$ & $63.4\%$ & $1240.4$ & $\mathbf{62.2\%}$ & $\mathbf{431.5}$\\
				R4~\cite{rice2020overfitting} &  $88.6\%$ & $\mathbf{67.6\%}$ & $933.7$ & $68.4\%$ & $\mathbf{715.3}$ \\
				R5~\cite{rice2020overfitting} &  $89.05\%$ & $66.4\%$ & $846.3$ & $\mathbf{62.5\%}$ & $\mathbf{613.7}$ \\
				R6~\cite{ding2018mma} &  $88.02\%$ & $67.6\%$ & $721.4$ & $\mathbf{63.4\%}$ & $\mathbf{511.1}$ \\
				Natural & $94.7\%$ & $0.00\%$ & $208.6$ & $0.00$ & $\mathbf{121.1}$\\
				\bottomrule  
				\end{tabular}
	}
	\end{small}
	\label{tab:AA++}
\end{wraptable}
%\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Although it is not the primary focus of this paper, in this section we notably enhance the time efficiency of the AA~\cite{croce2020reliable} by incorporating SDF method into the set of attacks in AA.

We introduce a new variant of AA by introducing AutoAttack++~(AA++). AA is a reliable and powerful ensemble attack that contains three types of white-box and a strong black-box attacks. AA evaluates the robustness of a trained model to adversarial perturbations whose $\ell_2$/$\ell_\infty$-norm is bounded by $\varepsilon$. By substituting SDF with the attacks in the AA, we significantly increase the performance of AA in terms of \textbf{\textit{computational time}}. Since SDF is an $\ell_{2}$-norm attack, we use the $\ell_{2}$-norm version of AA as well. We restrict maximum iterations of SDF to $10$. If the norm of perturbations exceeds $\varepsilon$, we renormalize the perturbation to ensure its norm stays $\leq\varepsilon$.
In this context, we have modified the AA algorithm by replacing APGD$^\top$~\cite{croce2020reliable} with SDF due to the former's cost and computation bottleneck in the context of AA~(See Appendix~\ref{APGD} for more details).
Our decision to replace APGD$^{\top}$ with SDF was primarily motivated by the former being a computational bottleneck in AA. As it is shown in Table~\ref{tab:AA++}, AA and AA++ achieve similar fooling rates, with AA++ being notably faster. We compared the sets of points that were fooled or not fooled by SDF/APGD$^{\top}$ across 1000 samples ($\varepsilon=0.5$). The results indicate that both algorithms fool approximately the same set of points, differing only in a handful of samples for this epsilon value. Therefore, the primary benefit of using SDF is the reduction in computation time.
We compare the fooling rate and computational time of AA++ and AA on the models from the RobustBench. In Table~\ref{tab:AA++}, we observe that AA++ is up to \textit{three} times faster than AA. In an alternative scenario, we added the SDF to the beginning of the AA set, resulting in a version that is up to two times faster than the original AA, despite now containing five attacks~(See Appendix~\ref{Another-AA}). This outcome highlights the efficacy of SDF in finding adversarial examples. These experiments suggest that leveraging efficient \textbf{\textit{minimum-norm}} and \textbf{\textit{non-fixed iteration}} attacks, such as SDF, can enable faster and more reliable evaluation of the robustness of deep models.



