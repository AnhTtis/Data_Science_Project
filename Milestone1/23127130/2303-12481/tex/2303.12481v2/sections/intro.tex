\vspace{-0.25in}
\section{Introduction}
\vspace{-0.11in}
% \begin{wrapfigure}{r}{0.5\textwidth}
\begin{wrapfigure}[17]{r}{0.5\textwidth}
    \vspace{-1.2cm}
    \centering
    \includegraphics[width=0.70\linewidth]{photos/illus_2_S.pdf}
    \caption{The average number of gradient computations vs the mean $\ell_2$-norm of perturbations. It shows that our novel fast and accurate method, SDF, outperforms other minimum-norm attacks. SDF finds significantly smaller perturbations compared to DF, with only a small increase in computational cost. SDF also outperforms other algorithms in optimality and speed. The numbers are taken from Table~\ref{tab:ImageNet}.}
    \label{fig:trade-off}
\end{wrapfigure}
\label{sec:intro}
Deep learning has achieved breakthrough improvement in numerous tasks and has developed as a powerful tool in various applications, including computer vision~\cite{long2015fully} and speech processing~\cite{mikolov2011strategies}.~Despite their success, deep neural networks are known to be vulnerable to adversarial examples, carefully perturbed examples perceptually indistinguishable from original samples~\cite{szegedy2013intriguing}. This can lead to a significant disruption of the inference result of deep neural networks. It has important implications for safety and security-critical applications of machine learning models.
%Various methods have been proposed to mitigate the adversarial vulnerability~\cite{lee2018simple,wong2018provable}. 

% In known, robust classification can be reached by detecting adversarial examples or pushing data samples further from the classifier's decision boundary. A large body of work has been done on designing more robust deep classifiers~
% \cite{XieWZRY18,wong2018provable,lee2018simple}.
% Among these methods, adversarial training~ \cite{szegedy2013intriguing, madry2017towards} has emerged as the primary approach, which augments adversarial examples to the training set to improve intrinsic network robustness.

%  \begin{figure}[t]
%     \begin{center}     \includegraphics[width=0.45\textwidth]{photos/Demo/oghab.pdf}
%             \end{center}
%             \vspace{-2mm}
%             \caption{Adversarial examples for ImageNet, as computed by SDF on a ResNet-$50$~\cite{he2016deep} architecture. The original image, which was classified as an "American eagle" is classified as an "iguana" by the perturbation vector obtained from SDF.}
%             \vspace{-5mm}
%             \label{fig:oghab}
% \end{figure}

Our goal in this paper is to introduce a parameter-free and simple method for accurately and reliably evaluating the adversarial robustness of deep networks in a fast and geometrically-based fashion. Most of the current attack methods rely on general-purpose optimization techniques, such as Projected Gradient Descent~(PGD)~\cite{madry2017towards} and Augmented Lagrangian~\cite{rony2021augmented}, which are oblivious to the geometric properties of models. However, deep neural networks' robustness to adversarial perturbations is closely tied to their geometric landscape~\cite{dauphin2014identifying,poole2016exponential,Guille_Optimism,kanbak2017geometric}. Given this, it would be beneficial to exploit such properties when designing and implementing adversarial attacks. This allows to create more effective and computationally efficient attacks on classifiers.
Formally, for a given classifier $\hat{k}$ and input $\x$, we define an adversarial perturbation as the minimal perturbation $\boldsymbol{r}$ that is sufficient to
change the estimated label $\hat{k}(\x)$:
\begin{align}
    \Delta(\x;\hat{k}):=\min_{\boldsymbol{r}} \| \boldsymbol{r} \|_2 \text{  s.t  } \hat{k}(\x+\boldsymbol{r}) \neq \hat{k}(\x).
    \label{global-formula}
\end{align}
%Depending on the data type, $\x$ can be any data, and $\hat{k}(\x)$ is the estimated label.
%  \begin{figure}[t]
%     \begin{center}     \includegraphics[width=0.45\textwidth]{photos/Demo/oghab.pdf}
%             \end{center}
%             \vspace{-2mm}
%             \caption{Adversarial examples for ImageNet, as computed by SDF on a ResNet-$50$~\cite{he2016deep} architecture. The original image, which was classified as an "American eagle" is classified as an "iguana" by the perturbation vector obtained from SDF.}
%             \vspace{-5mm}
%             \label{fig:oghab}
% \end{figure}
% \begin{figure}[t]
% \centering
%     \begin{subfigure}[b]{0.50\columnwidth}
%         % \caption*{$\leq0.01\%$}
%         \includegraphics[width=\linewidth]{photos/Demo/oghab.pdf}
%         \vspace{-1mm}
%         \caption{Adversarial examples for ImageNet, as computed by SDF on a ResNet-$50$~\cite{he2016deep} architecture.}
%     \end{subfigure}\!
%     \begin{subfigure}[b]{0.50\columnwidth}
%         % \caption*{$(0.01\%,0.1\%)$}
%         \includegraphics[width=\linewidth]{photos/illus_2.pdf}
%         \vspace{-7mm}
%         \caption{Norms of perturbations across the number of gradient computations.}
%     \end{subfigure}\!
%     \caption{In \textbf{(a)}, we present the adversarial instances for the ImageNet dataset, which were generated using the SDF algorithm on a ResNet-50. The initial classification of the image as a "American eagle" has been reclassified as a "iguana" based on the perturbation vector derived from the SDF.\newline
%     In \textbf{b}, the y-axis represents the number of gradient computations, while the x-axis represents the l2-norm of perturbations. The presented figure demonstrates the superior performance of our novel adversarial attack, referred to as SDF, in comparison to all previous minimum-norm attacks. Specifically, SDF exhibits a lower norm of perturbations while maintaining a somewhat comparable computational efficiency to DF. Moreover, SDF beats other algorithms significantly in terms of optimality and speed.
% }
%     \label{fig:pert_imagenet}
% \end{figure}



DeepFool~(DF)~\cite{Moosavi-Dezfooli_2016_CVPR} was among the earliest attempts to exploit the ``excessive linearity''~\cite{goodfellow2014explaining} of deep networks to find minimum-norm adversarial perturbations.
However, more sophisticated attacks were later developed that could find smaller perturbations at the expense of significantly greater computation time. 

In this paper, we exploit the geometric characteristics of minimum-norm adversarial perturbations to design a family of fast yet simple algorithms
that achieves a better trade-off between computational cost and accuracy in finding $\ell_2$ adversarial perturbations (see \Figref{fig:trade-off}).
Our proposed algorithm, guided by the characteristics of the optimal solution to \eqref{global-formula}, enhances DF to obtain smaller perturbations, while maintaining simplicity and computational efficiency that are only slightly inferior to those of DF. Our main contributions are summarized as follows:
\begin{itemize}
%    \item We introduce a novel family of fast yet accurate algorithms~(\orig{SuperDeepFool}) to find minimal adversarial perturbations. We extensively evaluate and compare our algorithms with state-of-the-art attacks~(SOTA) in various settings \orig{to show that our algorithm can find minimal accurate perturbations significantly faster than other SOTA algorithms}.
	\item We introduce a novel family of fast yet accurate algorithms to find minimal adversarial perturbations.
		We conduct a comprehensive evaluation of our algorithms against state-of-the-art (SOTA) adversarial attack methods across multiple scenarios. Our findings demonstrate that our algorithm identifies minimal yet accurate perturbations with significantly greater efficiency than competing SOTA approaches~(\ref{sec:experiments}).
    
    \item Our algorithms are developed in a systematic and well-grounded manner, based on theoretical analysis~(\ref{How-to-Improve-DF}).
    
%    \item \orig{We revisit the importance of minimal adversarial perturbations as a proxy to demystify deep neural network properties~(\ref{Stronger}, \ref{L-P}).}      
    
    \item We further improve the robustness of state-of-the-art image classifiers to minimum-norm adversarial attacks via adversarial training on the examples obtained by our algorithms~(\ref{SDF-AT}).

   \item  We significantly improve the time efficiency of the state-of-the-art Auto-Attack (AA)~\cite{croce2020reliable} by adding our proposed method to the set of attacks in AA~(\ref{AA++-section}).
   
   \item We revisit the importance of minimal adversarial perturbations as a proxy to demystify deep neural network properties~(Appendix~\ref{Stronger}, Appendix~\ref{L-P}).
   
    
\end{itemize}

\textbf{Related works.}
It has been observed that deep neural networks are vulnerable to adversarial examples~\cite{szegedy2013intriguing,Moosavi-Dezfooli_2016_CVPR,goodfellow2014explaining}. To exploit this vulnerability, a range of methods have been developed for generating adversarial perturbations for image classifiers. These attacks occur in two settings: white-box, where the attacker has complete knowledge of the model, including its architecture, parameters, defense mechanisms, etc.; and black-box, where the attacker's knowledge is limited, mostly relying on input queries to observe outputs ~\cite{chen2020hopskipjumpattack,rahmati2020geoda}. Further, adversarial attacks can be broadly categorized into two categories: bounded-norm attacks (such as FGSM~\cite{goodfellow2014explaining} and PGD~\cite{madry2017towards}) and minimum-norm attacks (such as DF and C$\&$W~\cite{carlini2017towards}) with the latter aimed at solving Eq.~(\ref{global-formula}). In this work, we specifically focus on white-box minimum $\ell_2$-norm attacks.

%FGSM, PGD, and momentum extension of PGD~\cite{uesato2018adversarial} are examples of the former, and DeepFool, C$\&$W, FMN, FAB, DDN, and ALMA are examples of the latter.
The authors in~\cite{szegedy2013intriguing} studied adversarial examples by solving a penalized optimization problem.
The optimization approach used in~\cite{szegedy2013intriguing} is complex and computationally inefficient; therefore, it cannot scale to large datasets.
The method proposed in~\cite{goodfellow2014explaining} applied a single-step of the input gradient to generate adversarial examples efficiently. DF was the first method to seek minimum-norm adversarial perturbations, employing an iterative approach. It linearizes the classifier at each step to estimate the minimal adversarial perturbations efficiently. C$\&$W attack~\cite{carlini2017towards} transform the optimization problem in~\cite{szegedy2013intriguing} into an unconstrained optimization problem. C$\&$W leverages the first-order gradient-based optimizers to minimize a balanced loss between the norm of the perturbation and misclassification confidence. Inspired by the geometric idea of DF, FAB~\cite{croce2020minimally} presents an approach to minimize the norm of adversarial perturbations by employing complex projections and approximations while maintaining proximity to the decision boundary. By utilizing gradients to estimate the local geometry of the boundary, this method formulates minimum-norm optimization without the need for tuning a weighting term.
DDN~\cite{Rony_2019_CVPR} uses projections on the $\ell_{2}$-ball for a given perturbation budget $\epsilon$. FMN~\cite{pintor2021fast} extends the DDN attack to other $\ell_{p}$-norms. By formulating~(\ref{global-formula}) with Lagrange's method, ALMA~\cite{rony2021augmented} introduced a framework for finding adversarial examples for several distances.

% The rest of the paper is organized as follows: In Section 2, we introduce minimum-norm adversarial perturbations and characterize their geometrical aspects. Section 3 provides an efficient method for computing $\ell_{2}$ adversarial perturbations by introducing a geometrical solution. Finally, the evaluation and analysis of the computed $\ell_{2}$ perturbations are provided in Section 4.











