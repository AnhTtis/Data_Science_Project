\newpage
\section{Appendix}
\subsection{Proofs}
\label{DeepFool-Proof}
% \begin{proposition}
%     Let the binary classifier $f:\mathbb{R}^{d} \rightarrow \mathbb{R}$ be continuously differentiable and its gradient $\nabla f$ be $L^{'}$-Lipschitz. For a given input sample $\x_0$, suppose $B(\x_{0},\epsilon)$ is a ball centered around $\x_0$ with radius $\epsilon$, such that there exists $\x\in B(\x_{0},\epsilon)$ that $f(\x)=0$. If $\|\nabla f\|_{2}\geq \zeta$ for all $\x\in B$ and $\epsilon < \frac{\zeta^2}{{L^{'}}^{2}}$, then DF iterations converge to a point on the decision boundary.
% \end{proposition}
 
\textbf{Proof of Proposition 1.}

Since $\nabla \mathcal{F}(\x)$ is Lipschitz-continuous, for $\x,\y \in \mathcal{B}(\x_{0},\varepsilon)$, we have:
\begin{equation}
    \label{Deepfool_Optimality_step1}
    |\mathcal{F}(\x) - \mathcal{F}(\y)+\nabla \mathcal{F}(\y)^T(\x-\y)|\leq \frac{\beta}{2}\|\x-\y\|^2
\end{equation}
DeepFool updates the new $\x_{n}$ in each according to the following equation:
\begin{equation}
    \label{Deepfool_step}
    \x_{n} = \x_{n-1} + \frac{\nabla \mathcal{F}(\x_{n-1})}{\|\nabla \mathcal{F}(\x_{n-1})\|^2_{2}}\mathcal{F}(\x_{n-1})
\end{equation}
Hence if we substitute $\x=\x_{n}$ and $\y=\x_{n-1}$ in~(\ref{Deepfool_Optimality_step1}), we get:
\begin{equation}
    \label{five}
    |\mathcal{F}(\x_{n})|\leq \frac{\beta}{2} \|\x_{n}-\x_{n-1}\|^2.
\end{equation}
Now, let $s_{n}:=||\x_n - \x_{n-1}||$. Using~(\ref{five}) and DeepFool's step, we get:
\begin{equation}
    \label{6}
    s_{n+1}=\frac{\mathcal{F}(\x_{n})}{\|\nabla \mathcal{F}(\x_{n})\|} \leq \frac{\beta}{2\zeta}\frac{\mathcal{F}(\x_{n})^2}{\|\nabla \mathcal{F}(\x_{n})\|^2}
\end{equation}
%We also know that for $\x,\x^{*} \in B(\x_{0},\epsilon)$ and the Lipschitz property:
%\begin{equation}
%    |f(\x)-f(\x^{*})| \leq 2L^{'}\epsilon
%\end{equation}
%From property $\x^{*}$ we know that $f(\x^{*})=0$, so:
%\begin{equation}
%    \label{epsilon-bound}
%    |f(\x)|\leq 2L^{'}\epsilon
%\end{equation}

\begin{equation}
    s_{n+1}=\frac{\mathcal{F}(\x_{n})}{||\nabla \mathcal{F}(\x_{n})||} \leqslant s_{n} \epsilon \dfrac{\beta^{2}}{\zeta^2}
\end{equation}
Using the assumptions of the  theorem, we have $\dfrac{\beta \varepsilon}{\zeta^2} < 1$,\hspace{1mm} and hence $s_{n}$ converges to $0$ when $n\rightarrow \infty$.
\hspace{1mm}We conclude that $\{\x_{n}\}$ is a \textit{\textbf{Cauchy sequence}}.
Denote by $\x_{\infty}$ the limit point of $\{\x_n\}$. Using the continuity of $\mathcal{F}$ and Eq.(\ref{five}),\hspace{1mm} we obtain 
\begin{equation}
    \lim_{n \rightarrow \infty}|\mathcal{F}(\x_{n})|=|\mathcal{F}(\x_{\infty})|=|\mathcal{F}(\x^{\star})|=0,
\end{equation}
%It is clear that $\x^{\star}$ is $\x$. 
Which concludes the proof of the theorem.


% \begin{proposition}
% For a differentiable $f$ and a given $\boldsymbol{r}_0$, $\boldsymbol{r}_i$ in the iterations \eqref{eq:iterative_proj} either converge to a solution of \eqref{eq:proj_max} or a trivial solution (i.e., $\boldsymbol{r}_i\rightarrow 0$).
% \end{proposition}
\textbf{Proof of Proposition 2.}
\label{Proof-orthogonality}
Let us denote the acute angle between $\nabla f(\x_0+\boldsymbol{r}_i)$ and $\boldsymbol{r}_i$ by $\theta_i$ ($0\leq \theta_i\leq \pi/2$). Then from (4) we have $\vert\boldsymbol{r}_{i+1}\vert = \vert\boldsymbol{r}_i\vert \cos \theta_i$. Therefore, we get
\begin{equation}
    \vert\boldsymbol{r}_{i+1}\vert = \prod_{i=1}^i \cos \theta_i \vert\boldsymbol{r}_0 \vert.
\end{equation}
Now there are two cases, either $\theta_i\to 0$ or not. Let us first consider the case where zero is not the limit of $\theta_i$. Then there exists some $\epsilon_0>0$ such that for any integer $N$ there exists some $n>N$ for which we have $\theta_n>\epsilon_0$. Now for $\epsilon_0$, we can have a series of integers $n_i$ where for all of them we have $\theta_{n_i}> \epsilon_0$. Since we have $0\leq \vert\cos \theta \vert \leq 1$, we have the following inequality:
\begin{equation}
    0\leq \prod_{i=0}^{\infty} \vert\cos \theta_i\vert \leq  \prod_{i=0}^{\infty} \vert \cos \theta_{n_i}\vert \leq \prod_{i=0}^{\infty} \vert\cos \epsilon_0\vert
\end{equation}
The RHS of the above inequality goes to zero which proves that $\boldsymbol{r}_i\to 0$. This leaves us with the other case where $\theta_i \to 0$. This means that $\cos \theta_i \to 1$ which is the maximum of \eqref{eq:proj_max}, this completes the proof.
%
%\begin{proposition}
%	\label{proposition_3}
%	\textit{Given a radius $\boldsymbol{r} > 0$ and $\Psi _{\boldsymbol{r}}$ is the set of all samples whose distance from the decision boundary $\mathscr{B}$ is less than $\boldsymbol{r}$. 
%		%	let $\Psi_{\boldsymbol{r}} := \{ \x \in \mathbb{R}^d : d(\x) < \boldsymbol{r} \}$ be the tubular neighborhood of $\mathscr{B}$ of radius $\boldsymbol{r}$, where
%		%\begin{equation}
%		%d(\x) := \min_{p \in \mathscr{B}} \| \x - p \|,
%		%\end{equation}
%		%i.e., $\Psi _{\boldsymbol{r}}$ is the set of all samples whose distance from the decision boundary $\mathscr{B}$ is less than $\boldsymbol{r}$.
%		For each angle $|\theta| \in \left( 0, \frac{\pi}{2} \right)$, there exists a distance $\boldsymbol{\widetilde{r}}_{(\theta)}$, such that, for all $\x \in \Psi_{\boldsymbol{\widetilde{r}}_{(\theta)}}$, the following inequality holds:
%		\begin{equation}
%			\frac{\nabla f(\x)^T \nabla f(\mathbb{P}_{\mathbf{s}}(\x))}{\|\nabla f(\x)\| \|\nabla f(\mathbb{P}_{\mathbf{s}}(\x))\|} > \cos(\theta),
%		\end{equation}
%		\textit{where $\mathbb{P}_{\mathbf{s}}(\x)$ is the unique projection of $\x$ on the $\mathscr{B}$.}}
%\end{proposition}

\textbf{Proof of proposition 3~(\cite{brau2022minimal})}
We use assumptions discussed in proposition 1~(the continuity of $\nabla f$). We derive that there exists a distance $\boldsymbol{r}$ such that $\|\nabla f(x)\| \neq 0$ in $\bar{\Psi} _{\boldsymbol{r}}$ (the smallest closed set containing $\Psi _{\boldsymbol{r}}$), and so we derive that $\frac{\nabla f}{\|\nabla f\|}$ is uniformly continuous in $\bar{\Psi} _{\boldsymbol{r}}$. Hence, for each $\varepsilon$, there exists a distance $\boldsymbol{r}_\varepsilon \leq \boldsymbol{r}$ such that, for each $\x, \y \in \bar{\Psi}_{\boldsymbol{r}}$ and $\| \x - \y\| < \boldsymbol{r}_\varepsilon$, the following inequality holds:
\begin{equation}
\left\| \frac{\nabla f(\x)}{\|\nabla f(\x)\|} - \frac{\nabla f(\y)}{\|\nabla f(\y)\|} \right\| < \varepsilon ,
\end{equation}
from triangle inequality for norms, we can derive:
%By remembering that $\|v - w\|^2 = \|v\|^2 + \|w\|^2 - 2v^T w$ for each $v, w \in \mathbb{R}^n$, we can deduce the following inequality
\begin{equation}
1 - \frac{1}{2} \varepsilon^2 < \frac{\nabla f(\x)^T \nabla f(\y)}{\|\nabla f(\x)\| \|\nabla f(\y)\|}.
\end{equation}
In conclusion, by taking $\y = \mathcal{P}_{\mathcal{S}}(\x)$ and by choosing $\varepsilon = \sqrt{2 - 2 \cos(\theta)}$, we achieve upper bound for $\cos(\theta)$ where $\boldsymbol{\widetilde{r}}_{(\theta)} = \min(\boldsymbol{r}_{\texttt{max}}, \boldsymbol{r}_\varepsilon)$. Where $\boldsymbol{r}_{\texttt{max}}$ is a maximum distance such that for
each $\x$ in the $\Psi_{\boldsymbol{r}_\texttt{max}}$
there exists a 
$\mathcal{P}_{\mathcal{S}}(\x) \in \mathscr{B}$ solves the minimum-norm optimization problem.

%\orig{\textbf{Revisiting the absence or presence of projection of $\x$~($\mathbb{P}_{\mathbf{s}}(\x)$) on decision boundary~($\mathscr{B}$) }.}


\section{Setup}
\label{Setup}
We test our algorithms on architectures trained on MNIST, CIFAR10, and ImageNet datasets.
For MNIST, we use a robust model called IBP from~\cite{zhang2019towards} and naturally trained model called SmallCNN. For CIFAR10, we use three models: an adversarially trained PreActResNet-18~\cite{he2016identity} from~\cite{rade2021helper}, a regularly trained Wide ResNet 28-10~(WRN-28-10) from~\cite{zagoruyko2016wide} and LeNet~\cite{lecun1999object}. These models are obtainable via the RobustBench library~\cite{croce2020robustbench}. On ImageNet, we test the attacks on two ResNet-50~(RN-50) models: one regularly trained and one $\ell_{2}$ adversarially trained, obtainable through the robustness library~\cite{robustness}. We additionally evaluate the robustness of Vision Transformers~(ViT-B-16~\cite{VIT}) and reevaluate the comparative analysis between ViTs and CNNs.

\section{On the benefits of line search}
\label{line-search}
As we show in Figure~\ref{fig:DF_analysis}, DF typically finds an overly perturbed point. SDF's gradients depend on DF, so overly perturbing DF is problematic. Line search is a mechanism that we add to the end of our algorithms to tackle this problem. For a fair comparison between adversarial attacks, we add this algorithm to the end of other algorithms to investigate the effectiveness of line search. 
\begin{table}
	\centering
	\vspace{-0.3cm}
	\caption{Comparison of the effectiveness of line search on the CIFAR10 data for SDF and DF. We 
      use one regularly trained model S~(WRN-$28$-$10$) and three adversarially trained models~(shown with R1~\cite{Rony_2019_CVPR}, R2~\cite{augustin2020adversarial} and R3~\cite{rade2021helper}).~\checkmark and \xmark ~indicate the presence and absence of line search respectively.}
	\small

	\begin{tabular}{rcccc}
		\toprule
		
		\multirow{2}{*}{Model}&\multicolumn{2}{c}{DF}&\multicolumn{2}{c}{SDF}\\
		\cmidrule(lr){2-3} \cmidrule(lr){4-5}
		&\checkmark&\xmark&\checkmark&\xmark\\
        
        \midrule
        
            S & $0.16$ & $0.19$  & $\mathbf{0.09}$& $0.10$\\        
		R1&$0.87$&$1.02$&$\mathbf{0.73}$&$0.76$\\
		
		R2 &$1.40$&$1.73$&$\mathbf{0.91}$&$0.93$\\
		
		R3&$1.13$&$1.36$&$\mathbf{1.04}$&$1.09$\\
		\bottomrule
  
	\end{tabular}
	\label{tab:LS-1}
\end{table}
As shown in Table~\ref{tab:LS-1}, we observe that line search can increase the performance of the DF significantly. However, this effectiveness for SDF is a little.
We now measure the effectiveness of line search for other attacks. As observed from Table~\ref{tab:LS-2}, line search effectiveness for DDN and ALMA is small.
\begin{table*}
    \centering
	\vspace{-0.1cm}
    \caption{Comparison of the effectiveness of line search on
        the CIFAR-10 data for other attacks. Line search effects are a little for DDN and ALMA. For FMN and FAB because they use line search at the end of their algorithms~(they remind this algorithm as a \textit{binary search} and \textit{final search}, respectively), line search does not become effective.}
    \label{tab:LS-2}
	\vspace{-0.1cm}
    \begin{small}
        \begin{sc}
    \begin{tabular}{@{}lrrrrrrrr@{}}
        \toprule		
        \multirow{2}{*}{Model}&\multicolumn{2}{c}{DDN}&\multicolumn{2}{c}{ALMA}&\multicolumn{2}{c}            
             {FMN}&\multicolumn{2}{c}{FAB}\\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
        
        &\checkmark&\xmark&\checkmark&\xmark&\checkmark&\xmark&\checkmark&\xmark\\
        \midrule
        WRN-$28$-$10$& $0.12$ & $0.13$& $0.10$ & $0.10$ & $0.11$ & $0.11$ & $0.11$ & $0.11$  \\
        
    
        R1~\cite{Rony_2019_CVPR} & $0.73$ & $0.73$ & $0.71$ & $0.71$ & $1.10$ & $1.10$ & $0.75$ & $0.75$\\
        
        
        R2~\cite{augustin2020adversarial}&$0.96$&$0.97$&$0.93$&$0.94$&$0.95$&$0.95$&$1.03$&$1.03$\\
        
        
        R3~\cite{rade2021helper}&$1.04$&$1.04$&$1.06$&$1.06$&$1.08$&$1.08$&$1.07$&$1.07$\\		
    \bottomrule
    \end{tabular}        
    \end{sc}
    \end{small}
	
\end{table*}





\section{Comparison on CIFAR10 with the AT PRN-$18$}
\label{Rade-CIFAR-10}In this section, we compare SDF with other minimum-norm attacks against an adversarially trained network~\cite{rade2021helper}. In Table~\ref{tab:CIFAR_RADE}, SDF achieves smaller perturbation compared to other attacks, whereas it costs only half as much as other attacks.

\begin{table}[h]
    \caption{Comparison of SDF with other state-of-the-art attacks for median $\ell_{2}$ on CIFAR-$10$ dataset for adversarially trained network (PRN-$18$~\cite{rade2021helper}).}
    \label{tab:CIFAR_RADE}  
	\vspace{+0.3cm}
    \centering    
    \begin{small}
    \begin{sc}
    \begin{tabular}{lrrr}
         \toprule         
         Attack
         & FR
         & Median-$\ell_{2}$
         & Grads \\
         \midrule
        
         ALMA & $100$ & $0.68$ & $100$\\
         DDN & $100$ & $0.73$ & $100$ \\
         FAB  & $100$ & $0.77$ & $210$ \\
         FMN  & $99.7$ & $0.81$ & $100$ \\
         \rowcolor{Gray}SDF & $100$ & $\mathbf{0.65}$ & $\mathbf{46}$\\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}  
\end{table}

\section{Performance comparison of adversarially trained models versus Auto-Attack~(AA)}
\label{AA-AT}
Evaluating the adversarially trained models with attacks used in the training process is not a standard evaluation in the robustness literature. For this reason, we evaluate robust models with AA. We perform this experiment with two modes; first, we measure the robustness of models with $\ell_\infty$ norm, and in a second mode, we evaluate them in terms of $\ell_{2}$ norm. Tables~\ref{tab:AA-L-inf} and \ref{tab:AA-L-2} show that adversarial training with SDF samples is more robust against reliable AA than the model trained on DDN samples~\cite{Rony_2019_CVPR}.



\begin{table}[h]
	\centering
    	\caption{Robustness results of adversarially trained models on CIFAR-$10$ with $\ell_{\infty}$-AA. We perform this experiment on $1000$ samples for each $\varepsilon$.}

     \begin{small}
     \begin{sc}
    	\begin{tabular}{lrrrr}
            \toprule
            Model &Natural & $\varepsilon=\frac{6}{255}$ & $\frac{8}{255}$ & $\frac{10}{255}$\\

    		\midrule
    		DDN&$89.1$&$45$&$29.6$&$17.6$\\
    		
    		SDF (Ours)&$90.8$&$\mathbf{47.5}$&$\mathbf{38.1}$&$\mathbf{25.4}$\\
    		\bottomrule
    	\end{tabular}
     \end{sc}
     \end{small}


	\label{tab:AA-L-inf}
\end{table}

\begin{table}[h]
	\centering
    % \vspace{0.2mm}
    	\caption{Robustness results of adversarially trained models on CIFAR-$10$ with $\ell_{2}$-AA. We perform this experiment on $1000$ samples for each $\varepsilon$.}

    \begin{small}
     \begin{sc}
    	\begin{tabular}{rccccc}
            \toprule
            Model &Natural & $\varepsilon=0.3$ & $0.4$ & $0.5$ & $0.6$\\

    		\midrule
    		DDN& $89.1$ & $78.1$  & $73$  & $67.5$  & $61.7$\\
    		
    		SDF (Ours) &$90.8$ & $\mathbf{83.1}$ & $\mathbf{79.7}$ & $\mathbf{68.1}$ & $\mathbf{63.9}$\\
    		\bottomrule
    	\end{tabular}
     \end{sc}
     \end{small}
     
        % \vspace{-2mm}
	\label{tab:AA-L-2}
\end{table}


\section{Another variants of AA++}
\label{Another-AA}
As we mentioned, in an alternative scenario, we added the SDF to the beginning
of the AA set, resulting in a version that is up to two times
faster than the original AA. In this scenario, we do not exchange the SDF with APGD. We add SDF to the AA configuration. So in this configuration, AA has five attacks~(SDF, APGD, APGD$^\top$, FAB, Square). By this design, we guarantee the performance of AA. An interesting phenomenon observed from these tables is that when the budget increases, the speed of the AA++ increases. We should note that we restrict the number of iterations for SDF to $10$.
\begin{figure}
	\centering
	\begin{subfigure}[t]{0.33\linewidth} 
		% \centering
		\includegraphics[width=1.0\linewidth]{photos/Curves/AA/AA_1.pdf}
		\caption{R1}
		\label{fig:4_1}
	\end{subfigure} 
	\begin{subfigure}[t]{0.333\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{photos/Curves/AA/AA_Radde_1.pdf}
		\caption{S}
		\label{fig:4_2}
	\end{subfigure} 
	\vspace{-3mm}
	\caption{ In this figure, we show the time ratio of AA to AA++. For regularly 
        trained model~(WRN-$28$-$10$) and adversarially trained model~\cite{rade2021helper}~(R1). We perform this experiment on $1000$ samples from CIFAR10 data.}
	\label{fig:AA-time-comparsion}
	\vspace{-5mm}
\end{figure}

\subsection{Why do we replace SDF with  APGD$^\top$?}
\label{APGD}
It is well established that AutoAttack (AA) is a robust method for evaluating model robustness, unaffected by gradient obfuscation~\cite{Obfuscated}. The primary limitation of AA, however, is its computational intensity. To thoroughly evaluate a model, it must be subjected to four distinct attacks sequentially. Our empirical analysis identified the APGD$^\top$ attack as the main computational bottleneck in AA. For example, when attacking a standard WRN-28-10 model trained on CIFAR-10, APGD$^\top$ requires approximately \textbf{4310} backward passes to achieve a 100$\%$ fooling rate. Similarly, for an adversarially trained WRN-28-10~\cite{carmon2022unlabeled} model on CIFAR-10, APGD$^\top$ necessitates around \textbf{5660} backward passes to attain a 100$\%$ fooling rate. To address this issue, rather than simply replacing SDF with another minimum-norm attack such as FAB in AA, we mitigate the bottleneck by employing a faster minimum-norm attack like SDF.
% \newpage

\section{Why do we need stronger minimum-norm attacks?}
\label{Stronger}
Bounded-norm attacks like FGSM~\cite{goodfellow2014explaining}, PGD~\cite{madry2017towards}, and momentum variants of PGD~\cite{uesato2018adversarial}, by optimizing the difference between the logits of the true class and the best non-true class, try to find an adversarial region with maximum confidence within a given, fixed perturbation size. 
Bounded-norm attacks only evaluate the robustness of deep neural networks; this means that they report a single scalar value as robust accuracy for a fixed budget.
The superiority of minimum-norm attacks is to report a distribution of perturbation norms, and they do not report a percentage of fooling rates~(robust accuracy) by a single scalar value. This critical property of minimum-norm attacks helps to accelerate to take an in-depth intuition about the geometrical behavior of deep neural networks.

We aim to address a phenomenon we observe by using the superiority of minimum-norm attacks. We observed that a minor change within the design of deep neural networks affects the performance of adversarial attacks. 
To show the superiority of minimum-norm attacks, we show how minimum-norm attacks verify these minor changes rather than bounded-norm attacks.

Modeling with max-pooling was a fundamental aspect of convolutional neural networks when they were first introduced as the best image classifiers. Some state-of-the-art classifiers such as~\cite{krizhevsky2017imagenet,simonyan2014very,he2016deep} use this layer in network configuration. We use the pooling layers to show that using the max-pooling and Lp-pooling layer in the network design leads to finding perturbation with a bigger $\ell_2$-norm.

Assume that we have a classifier $f$. We train $f$ in two modes until the training loss converges. In the first mode, $f$ is trained in the presence of the pooling layer in its configuration, and in the second mode, $f$ does not have a pooling layer. 
When we measure the robustness of these two networks with regular budgets used in bounded-norms attacks like PGD~($\varepsilon=8/255$), we observe that the robust accuracy is equal to $0\%$. This is precisely where bounded-norm attacks such as PGD mislead robustness literature in its assumptions regarding deep neural network properties. However, a solution to solve the problem of bounded-norm attack scan be proposed: \textit{" Analyzing the quantity of changes in robust accuracy across different epsilons reveal these minor changes."} Is this case, the solution is costly.
This is precisely where the distributive view of perturbations from worst-case to best-case of minimum-norm attacks detects this minor change.



To show these changes, we trained ResNet-$18$ and Mobile-Net~\cite{howard2017mobilenets} in two settings. In the first setting, we trained them in the presence of a pooling layer until the training loss converged, and in the second setting, we trained them in the absence of a pooling layer until the training loss converged. We should note that we remove all pooling-layers in these two settings.
For a fair comparison, we train models until they achieve zero training loss using a multi-step learning rate. We use max-pooling and Lp-pooling, for $p=2$, for this minor changes.

Table~\ref{tab:max-pooling} shows that using a pooling layer in network configuration can increase robustness. DF has an entirely different behavior according to the presence or absence of the pooling layer; max-pooling affects up to $50\%$ of DF performance. This effect is up to $9\%$ for DDN and FMN. ALMA and SDF show a $4\%$ impact in their performance, which shows their consistency compared to other attacks.
\begin{table*}
	\centering
	\caption{This table shows the $\ell_{2}$-median for the minimum-norm attacks. For all 
        networks, we set learning rate = $0.01$ and weight decay = $0.01$. For training with Lp-pooling, we set $p=2$ for all settings.}

	\begin{sc}
	    \begin{small}	        	    

	\begin{tabular}{rccccccccccc}
		\toprule		
		\multirow{2}{*}{Attack}&\multicolumn{3}{c}{RN18}&\multicolumn{3}{c}{MobileNet}&\\
		\cmidrule(lr){2-4} \cmidrule(lr){5-8}
	 	&no pool&max-pool&Lp-pool&no pool&max-pool&Lp-pool\\

        \midrule
		DF & $0.40$ & $0.90$  & $0.91$ & $0.51$ & $0.95$  & $0.93$ \\
		DDN & $0.16$ & $0.25$  & $0.26$ & $0.22$ & $0.27$  & $0.26$\\
		FMN & $0.18$ & $0.27$ & $0.30$ & $0.24$ & $0.30$  & $0.29$ \\
		C$\&$W & $0.18$ & $0.25$  & $0.27$ & $0.22$ & $0.26$   & $0.24$ \\
		ALMA & $0.19$ & $0.23$ &  $0.23$ & $\mathbf{0.20}$ & $0.25$  & $0.22$ \\
		\rowcolor{Gray}SDF & $\mathbf{0.16}$ & $\mathbf{0.21}$ & $\mathbf{0.22}$ & $\mathbf{0.20}$ &   $\mathbf{0.23}$  & $\mathbf{0.21}$ \\
		\bottomrule
	\end{tabular}
	\end{small}
	\end{sc}
	\label{tab:max-pooling}
\end{table*}

As shown in Table~\ref{tab:max-pooling-AA-PGD}, we observe that models with pooling-layers have more robust accuracy when facing adversarial attacks such as AA and PGD. It should be noted that using regular epsilon for AA and PGD will not demonstrate these modifications. For this reason, we choose an epsilon for AA and PGD lower~($\varepsilon = 2/255$) than the regular format~($\varepsilon = 8/255$).

\begin{table*}
	\centering
	\caption{This table shows the robust accuracy for all networks against to the AA and 
        PGD. For training with Lp-pooling, we set $p=2$ for all settings.}

	\begin{sc}
	    \begin{small}	        	    

	\begin{tabular}{rccccccccccc}
		\toprule		
		\multirow{2}{*}{Attack}&\multicolumn{3}{c}{RN18}&\multicolumn{3}{c}{MobileNet}&\\
		\cmidrule(lr){2-4} \cmidrule(lr){5-8}
	 	&no pool&max-pool&Lp-pool&no pool&max-pool&Lp-pool\\

        \midrule
            AA & $1.1\%$ & $17.2\%$ & $16.3\%$ & $8.7\%$ & $21.3\%$ & $20.2\%$\\
            PGD & $9.3\%$ & $28\%$ & $26.2\%$ & $16.8\%$ & $31.4\%$ & $28.7\%$\\
		\bottomrule

		
	\end{tabular}
	\end{small}
	\end{sc}
	\label{tab:max-pooling-AA-PGD}
\end{table*}

Table~\ref{tab:max-pooling} and~\ref{tab:max-pooling-AA-PGD} demonstrate that pooling-layers can affect adversarial robustness of deep networks. Powerful attacks such as SDF and ALMA show high consistency in these setups, highlighting the need for powerful attacks.



\subsection{Max-pooling's effect on the decision boundary's curvature}
\label{curvature-maxpooling}
Here, we take a step further and investigate why max-pooling impacts the robustness of models. In order to perform this analysis, we analyze gradient norms, Hessian norms, and the model's curvature. The curvature of a point is a mathematical quantity that indicates the degree of non-linearity. It has been observed that robust models are characterized by their small curvature~\cite{moosavi2019robustness}, implying smaller Hessian norms. In order to investigate robustness independent of non-linearity,~\cite{srinivasefficient} propose \textit{normalized curvature}, which normalizes the Hessian norm at a given input $\x$ by its corresponding gradient norm.
They defined \textit{normalized curvature} for a neural network classifier $f$ as  $\C_f(\x) = \| \grad^2 f(\x) \|_2 / (\| \grad f(\x) \|_2 + \varepsilon)$. Where $\| \grad f(\x) \|_2$ and $\| \grad^2 f(\x) \|_2$ are the $\ell_2$-norm of the gradient and the spectral norm of the Hessian, respectively, where $ \grad f(\x) \in \bbR^d, \grad^2 f(\x) \in \bbR^{d \times d}$, and $\varepsilon > 0$ is a small constant to ensure the proper behavior of the measure. In Table~\ref{tab:geometry-2}, we measure these quantities for two trained models, one with max-pooling and one without. It clearly shows that the model incorporating max-pooling exhibits a smaller curvature. This finding corroborates the observation that models with greater robustness tend to have a smaller curvature value.
\begin{table}[h]
    \centering
    \caption{Model geometry of different ResNet-$18$ models. W~(with pooling) and W/O~(without pooling).}
    \label{tab:geometry-2}
    \vspace{-3mm}
    \begin{sc}
        \begin{small}            
        \begin{tabular}{rccc} 
        \toprule
         Model & $\E_{\x} \| \grad f(\x) \|_2$ & $\E_{\x} \| \grad^2 f(\x) \|_2$ & $\E_{\x} \C_f(\x)$\\ 
         \midrule
         W & $\mathbf{4.75}$ \std{$1.54$} & $\mathbf{120.70}$ \std{$48.74$} & $\mathbf{14.94}$ \std{$0.52$}\\
         W/O & $7.04$ \std{$2.44$} & $269.74$ \std{$10.23$} & $22.81$ \std{$2.58$}\\ \bottomrule
        \end{tabular}
    \end{small}
    \end{sc}
\end{table}

\begin{table}[h]
	\vspace{-0.4cm}
	\centering
	\caption{Model geometry for regular and adversarially trained models.}
%	\vspace{-3mm}
	\label{geometry}
	\begin{small}
		\begin{sc}                    
			\begin{tabular}{rccc} 
				\toprule
				Model & $\E_{\x} \| \grad f(\x) \|_2$ & $\E_{\x} \| \grad^2 f(\x) \|_2$ & $\E_{\x} \C_f(\x)$\\ 
				\midrule
				Standard & $9.54$ \std{$1.02$} & $600.06$ \std{$29.76$} & $73.99$ \std{$6.62$}\\
				DDN AT & $0.91$ \std{$0.34$} & $2.86$ \std{$1.22$} & $4.32$ \std{$2.91$}\\
				
				SDF AT & $\mathbf{0.38}$ \std{$0.60$} & $\mathbf{0.73}$ \std{$0.08$} & $\mathbf{1.66}$ \std{$0.86$}\\
				\bottomrule
			\end{tabular}
		\end{sc}
	\end{small}
	
\end{table}
\section{Model geometry for AT models}
In this section we provide curvature analysis of our adversarially trained networks, SDF AT, and DDN AT model. Table~\ref{geometry} shows that our AT model decreases the curvature of network more than DDN AT model.



% \subsection{Orthogonality analysis for minimum-norm attacks}
% \ali{TO DO}
% \begin{figure}[t]
%     \centering    \includegraphics[width=0.3\columnwidth]{photos/orthogonality_CW.pdf}
%     \caption{Histogram of the cosine angle distribution between the gradient in the last step of C$\&$W and the perturbation vector obtained by C$\&$W. This experiment has been performed on 1000 images from the CIFAR10 dataset with the ResNet18 model.}
%     \label{fig:orthogonality_CW}
% \end{figure}

%\section{Orthogonality conditions for other attacks}
%
%In Fig~\ref{fig:orthogonality_CW}, we show the histogram of the cosine angle distribution between normals to the decision boundary and perturbations for C$\&$W and FMN attacks.

%\begin{figure}
%\centering
%    \begin{subfigure}[b]{0.33\columnwidth}
%        % \caption*{$\leq0.01\%$}
%        \includegraphics[width=\linewidth]{photos/orthogonality_CW.pdf}
%        \vspace*{-5mm}
%        \caption*{C\&W}
%    \end{subfigure}\!
%    \begin{subfigure}[b]{0.38\columnwidth}
%        % \caption*{$(0.01\%,0.1\%)$}
%        \includegraphics[width=\linewidth]{photos/FMN.pdf}
%        \vspace*{-5mm}
%        \caption*{FMN}
%    \end{subfigure}\!
%        \caption{Histogram of the cosine angle distribution between the gradient in the last step of C$\&$W(~\textbf{\textit{left}}), FMN(~\textbf{\textit{right}}) and the perturbation vector obtained by C$\&$W and FMN. As previously mentioned in the analysis of \textit{Linesearch}~\ref{line-search}, it is observed that a majority of adversarial attacks exhibit not-overly perturbed perturbations due to the incorporation of the \textit{Linesearch} mechanism into their algorithms. Therefore, due to this justification, gamma analysis is not an effective tool for distinguishing power and comparing algorithms. }
%    \label{fig:orthogonality_CW}
%    
%\end{figure}


\section{CNN architecture used in Table~\ref{tab:CIFAR10_architecture_DF}}
\begin{table}[h]
	%\begin{wraptable}[5]{r}{0.45\textwidth}
	\vspace{-0.5cm}
	\centering
	\label{tab:cnns_attack}
	% \begin{minipage}{0.2\textwidth}
	\resizebox{0.40\linewidth}{!}{
		\begin{tabular}{lr}
			\toprule
			Layer Type & CIFAR-$10$ \\ 
			\midrule
			Convolution + ReLU & $3\times 3 \times 64$\\
			Convolution + ReLU & $3\times 3 \times 64$\\
			max-pooling & $2 \times 2$\\
			Convolution + ReLU & $3\times 3 \times 128$\\
			Convolution + ReLU & $3\times 3 \times 128$\\
			max-pooling & $2 \times 2$\\
			Fully Connected + ReLU & $256$ \\
			Fully Connected + ReLU & $256$ \\
			Fully Connected + Softmax & $10$ \\
			
			\bottomrule
		\end{tabular}
	}
	%    \caption{CNN architecture.}
	%    \vspace{1mm}
		
	% \end{minipage}
	%\end{wraptable}
\end{table}
The architecture used to compare SDF variants and DF (Table~\ref{tab:CIFAR10_architecture_DF}) is summarized in above Table.


 

% \newpage


\newpage

\section{ViT-B-16 for CIFAR-10}
Given our available computational resources, we conduct experiments on a ViT-B-16~\cite{VIT} trained on CIFAR-10, achieving 98.55$\%$ accuracy. The results are summarized in the following table:
\begin{center}
	\begin{tabular}{c|c|c|c}
%		\hline
		Attack & FR (\%) & Median-$\ell_2$ & Grads \\
		\hline
		DF & 98.2 & 0.29 & 19 \\
		ALMA & 100 & 0.12 & 100 \\
		DDN & 100 & 0.14 & 100 \\
		FAB & 100 & 0.14 & 100 \\
		FMN & 99.1 & 0.15 & 100 \\
		C\&W & 100 & 0.15 & 91,208 \\
		\rowcolor{Gray}SDF & 100 & 0.10 & 32 \\
		\hline
	\end{tabular}
\end{center}

As seen, this transformer model does not exhibit significantly greater robustness compared to CNNs, with only a negligible difference of 0.01 compared to a WRN-28-10 trained on CIFAR-10. These results support the notion that there might not be a substantial disparity between the adversarial robustness of ViTs and CNNs. This aligns with the findings of~\cite{bai2021transformers}. They argue that earlier claims of transformers being more robust than CNNs stems from an unfair comparison and evaluation methods. We believe that thorough evaluations using minimum norm attacks could be helpful in resolving this debate.

\section{Natural~(Regular) Trained MNIST Model}
\label{MNIST-Natural}
In Table~\ref{tab:mnist} we show the results of evaluating adversarial attacks on naturally trained SmallCNN on MNIST dataset. Our algorithm demonstrates a higher rate of convergence compared to other algorithms, as the perturbations for all algorithms are generally similar.

% \begin{wraptable}{r}{0.45\textwidth}
%\begin{table}[h]
\begin{table}[h]
    \centering
    \vspace{-0.2cm}
%    \small
    \caption{We compare the performance of all algorithms on the natural SmallCNN model that was trained on the MNIST dataset.}
    \vspace{1mm}
    \label{tab:mnist}
    \begin{tabular}{c|c|c|c}
        \toprule
        Attacks & FR & Median-$\ell_2$ & Grads  \\
        \midrule
        ALMA & 100 & \textbf{1.34} & 1000   \\
        DDN & 100 & 1.36 & 1000    \\
        FAB & 100 & 1.36 & 10000   \\
        FMN & 97.10 & 1.37 & 1000    \\
        C$\&$W & 99.80 & 1.35 & 90000    \\
        \rowcolor{Gray}SDF & 100 & \textbf{1.34} & \textbf{67}   \\
        \bottomrule
        
    \end{tabular}
% \end{wraptable}
\end{table}
% \end{center}



\section{Runtime Comparison}
We report the number of gradient computations as a main proxy for computional cost comparison. In Table~\ref{tab:time-CIFAR10-AT}, we have compared  the runtime of different attacks for a fixed hardware. SDF is significantly faster.

\begin{table}[h]
        % \small
    \vspace{-0.5cm}
	\centering
	\caption{Runtime comparison for adversarial attacks on WRN-28-10 architecture trained on CIFAR10, for both naturally trained model and adversarially trained models.}
        \vspace{1mm}
	% \small
	% \renewcommand{\arraystretch}{1}
	\resizebox{0.60\linewidth}{!}{
	\begin{tabular}{lrrrr}
		\toprule
        &\multicolumn{2}{c}{Natural}&\multicolumn{2}{c}{R1~\cite{rebuffi2021fixing}}\\
        \cmidrule(lr){2-3}  \cmidrule(lr){4-5}
        
		Attacks&Time~(S)&Median-$\ell_2$&Time~(S)&Median-$\ell_2$\\
		\midrule
		ALMA&1.71&0.10&13.10&1.22\\
		DDN&1.54&0.13&12.44&1.53\\
		FAB&2.33&0.11&16.21&1.66\\
		FMN&1.42&0.11&10.25&1.83\\
            C$\&$W&734.8&0.12&5402.1&1.68\\
            \rowcolor{Gray}SDF&\textbf{0.48}&\textbf{0.09}&\textbf{2.93}&\textbf{1.19}\\		
        \bottomrule
	\end{tabular}
	}
	\label{tab:time-CIFAR10-AT}
\end{table}

% \section{\orig{Additional networks for Table~\ref{AA++}}}
% \begin{table}[h]
%     \centering
%     \caption{An analysis of robust accuracy~($\%$) for various defense strategies against AA++ and AA with $\varepsilon = 0.5$. The \orig{acc} column denotes the robust accuracies of different models. R1~\cite{rebuffi2021fixing}, R2~\cite{Proxy}, R3~\cite{gowal2020uncovering}, R4~\cite{rice2020overfitting}, R5~\cite{rice2020overfitting} and R6~\cite{ding2018mma} have been trained adversarially, and model S~\cite{zagoruyko2016wide} is a regularly trained model. All models are taken from the RobustBench library~\cite{croce2020robustbench}.}
%     \vspace{-2mm}
%     \begin{small}
%         \begin{minipage}{\linewidth}
%             \centering
%             \begin{tabular}{rlllll}
%                 \toprule
%                 \multirow{2}{*}{\makecell[c]{Models}}
%                 & Clean
%                 & \multicolumn{2}{c}{AA}
%                 & \multicolumn{2}{c}{AA++}\\
%                 \cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-6} 
%                 & acc& acc & Grads & acc & Grads \\
%                 \midrule

%                 R1 & $95.7\%$ & $82.3\%$ & $1259.2$ & $\mathbf{82.1\%}$ & $\mathbf{599.5}$\\
%                 R2  & $90.3\%$ & $76.1\%$ & $1469.1$ & $76.1\%$ & $\mathbf{667.7}$\\
%                 R3 & $89.4\%$ & $63.4\%$ & $1240.4$ & $\mathbf{62.2\%}$ & $\mathbf{431.5}$\\
%                 R4 &  $88.6\%$ & $\mathbf{67.6\%}$ & $933.7$ & $68.4\%$ & $\mathbf{715.3}$ \\
%                 R5 &  $89.05\%$ & $66.4\%$ & $846.3$ & $\mathbf{62.5\%}$ & $\mathbf{613.7}$ \\
%                 R6 &  $88.02\%$ & $67.6\%$ & $721.4$ & $\mathbf{63.4\%}$ & $\mathbf{511.1}$ \\
%                 S & $94.7\%$ & $0.00\%$ & $208.6$ & $0.00$ & $\mathbf{121.1}$\\
%                 \bottomrule  
%             \end{tabular}
%         \end{minipage}
%     \end{small}
%     \label{AA++}
% \end{table}

% \newpage
%\section{Geometric conditions of optimal perturbations}
%\orig{Figure~\ref{fig:orthogonality_illust} illustrates the two conditions discussed in Section~\ref{DeepFool}. In the figure, \(n_1\) and \(n_2\) represent two orthogonal vectors to the decision boundary. The optimal perturbation vector \(\boldsymbol{r}^*\) aligns parallel to \(n_2\). On the other hand, a non-optimal perturbation \(\boldsymbol{r}_{\text{DF}}\) forms an angle \(\alpha\) with \(n_1\).

%\begin{figure}[h]
%    \centering
%    \includegraphics[width=0.4\textwidth]{photos/orthogonality_illustration.pdf}
%    \caption{Illustration of the optimal adversarial example $\x+\boldsymbol{r}^*$ for a binary classifier $f$; the example lies on the decision boundary (set of points where $f(\x)=0$) and the perturbation vector $\boldsymbol{r}^*$ is orthogonal to this boundary.}
%    \label{fig:orthogonality_illust}
%\end{figure}
%}

\section{Query-Distortion Curves}
%\begin{figure}[20]{r}{0.40\textwidth}
\begin{figure}[h]
%	\vspace{-1.20cm}
	\centering
	\includegraphics[width=0.50\textwidth]{photos/Query_norm_WRN_Rony.pdf}
	\caption{
		As demonstrated in~\cite{pintor2021fast}, query-distortion curves are utilised as a metric for evaluating computational complexity of white-box attacks. In this particular context, the term ``query'' refers to the quantity of forward passes available to find adversarial perturbations.}
	\label{fig:Query-perturbation}
\end{figure}

%As demonstrated in~\cite{pintor2021fast}, query-distortion curves are utilised as a metric for evaluating computational complexity of white-box attacks. In this particular context, the term ``query'' refers to the quantity of forward passes available to find adversarial perturbations.

Unlike FMN and ALMA, SDF (and DF) does not allow control over the number of forward and backward computations. They typically stop once a successful adversarial example is found. Terminating the process prematurely could prevent them from finding an adversarial example. Hence, we instead opted to plot the median norm of achievable perturbations for a given maximum number of queries (Figure~\ref{fig:Query-perturbation}) Although this is not directly comparable to the query-distortion curves in~\cite{pintor2021fast}, it provides a more comprehensive view of the query distribution than the median alone.

%\begin{figure}
%    \centering
%    % \begin{subfigure}[b]{0.70\textwidth}
%        \centering
%        \includegraphics[width=.5\linewidth]{photos/Query_norm_WRN_Rony (1).pdf}
%        % \caption{DF}
%    % \end{subfigure}
%    \hspace{1cm}
%    % \begin{subfigure}[b]{0.45\textwidth}
%    %     \centering
%    %     \includegraphics[width=0.8\linewidth]{photos/Query_norm_SDF_WRN_Madry.pdf}
%    %     \caption{SDF}
%    % \end{subfigure}
%    \caption{\orig{Query-perturbation curves illustrating the relationship between the number of queries and norms of perturbations for the DF and SDF attacks. The attacks are performed on a WRN-28-10 adversarially trained on CIFAR-10 dataset.}}
%    \label{fig:Query-perturbation}
%\end{figure}

%\newpage
\section{Limitations}
\label{sec:limitations}
In this section, we discuss some limitations and potential extensions of SDF.

\paragraph{Extension to other $\ell_p$-norms and targeted attacks.} The proposed attack is primarily designed for $\ell_2$-norm adversarial perturbations. Moreover, our method, similar to DeepFool (DF), is non-targeted. Though there are potential approaches for adapting SDF to targeted and $\ell_p$ attacks, these aspects remain largely unexplored in our work.

Nevertheless, we here demonstrate how one could possibly extend SDF to other $p$-norms.  A simple way is to replace the $\ell_2$ projection (Line 5 of Algorithm~\ref{alg:SuperDeepFool-multi}) with a projection operator minimizing  $\ell_p$ norm similar to the derivations used in~\cite{Moosavi-Dezfooli_2016_CVPR}. In particular, for $p=\infty$, the following projection would replace the line 5 of Algorithm~\ref{alg:SuperDeepFool-multi}:
\begin{equation}
    \boldsymbol{x}\gets \boldsymbol{x}_0 + \frac{(\widetilde{\boldsymbol{x}}-\boldsymbol{x}_0)^\top\boldsymbol{w}}{||\boldsymbol{w}||_1} \text{sign}(\boldsymbol{w})
\end{equation}

In Table~\ref{table:linf}, we compare the performance of this modified version of SDF, named $\text{SDF}_{\ell_\infty}$ with FMN, FAB, and DF, on two pretrained networks M1~\cite{madry2017towards} and M2~\cite{Rony_2019_CVPR} on CIFAR-10 dataset. %adversarially trained networks on the CIFAR10 dataset.
Our findings indicate that $\text{SDF}_{\ell_\infty}$ also exhibits superior performance compared to other algorithms in discovering smaller  perturbations.

\begin{table}[h]
\caption{Performance of $\text{SDF}_{\ell_\infty}$ on two robust networks trained on CIFAR-10 dataset.
}
\centering
\begin{small}
	\begin{sc}
		\resizebox{0.7\linewidth}{!}{
			\begin{tabular}{lrrrrrr}
			    \toprule
			    \multirow{2}{*}{Attacks} & \multicolumn{3}{c}{M1} & \multicolumn{3}{c}{M2} \\
			    \cmidrule(lr){2-4} \cmidrule(lr){5-7}
			            & Median $\ell_\infty$ & FR & Grads & Median $\ell_\infty$ & FR & Grads  \\ 
			    \midrule
			    DF & 0.031 & 96.7 & \textbf{24} & 0.043 & 97.4 & \textbf{31} \\ 
			    FAB & 0.025 & 99.1 & 100 & 0.038 & 99.6 & 100\\ 
			    FMN & 0.024 & \textbf{100} & 100 & 0.035 & \textbf{100} & 100 \\ 
			    \rowcolor{Gray}$\text{SDF}_{\ell_\infty}$ & \textbf{0.019} & \textbf{100} & 33 & \textbf{0.027} & \textbf{100} & 46 \\ 
			    \bottomrule
			\end{tabular}
		}
	\end{sc}
\end{small}
\label{table:linf}
\end{table}

Furthermore, we can convert SDF to a targeted attack by replacing the line 3 of Algorithm~\ref{alg:SuperDeepFool-multi} with the targeted version of DeepFool, and the line 4 with the following:

\begin{equation}
    \w\leftarrow\nabla f_{t}(\widetilde{\x}) - \nabla f_{\hat{k}(\x_0)}(\widetilde{\x}),
\end{equation}
where $t$ is the target label. We followed the procedure outlined in~\cite{carlini2017towards} to measure the performance in the targeted setting. The result is summarized in Table~\ref{table:targeted}. While SDF is effective in quickly finding smaller perturbations, it does not achieve a $100\%$ fooling rate. Further analysis is required to understand the factors preventing SDF from converging in certain cases. This aspect remains an area for future work.

\begin{table}[t]
\caption{Performance of targeted SDF on a standard trained WRN-28-10 on CIFAR-10, measured using 1000 random samples. }
%\centering
    \begin{small}
         \begin{sc}
        		\resizebox{\linewidth}{!}{
	                \begin{tabular}{lrrrrrrrrrr}
	                    \toprule
	                    % \multirow{3}{*}{Models} &
	                    \multirow{3}{*}{Attacks} & \multicolumn{4}{c}{Targeted} & \multicolumn{4}{c}{Untargeted} & \multirow{1}{*}{} \\
	                    \cmidrule(lr){2-5}\cmidrule(lr){6-9}
	                    & FR & Mean $\ell_2$ & Median $\ell_2$ & Grads & FR & Mean $\ell_2$ & Median $\ell_2$ & Grads
	                    %& Gain
	                    \\
	                    \midrule
	                    DDN & $100$ & $0.24$ & $0.25$ & $100$ & $100$ & $0.13$& $0.14$ & $100$
	                    \\
	                    FMN & $96.2$ & $0.22$ & 0.24 & 100 & $97.3$ & $0.11$ & 0.13 & 100
	                    \\
	                    \rowcolor{Gray}SDF (targeted) & $98.2$ & $0.21$ & 0.22 & $25$ & 100 &  $0.10$ & 0.11 & 34
	                    \\
	                    \bottomrule
	                    
	                \end{tabular}
            	}
         \end{sc}
    \end{small}
    \label{table:targeted}
\end{table}

\paragraph{Convergence guarantees.}
\label{convergence}
A common challenge for all gradient-based optimization methods applied to non-convex problems is the lack of a guarantee in finding globally optimal perturbations for SotA neural networks. Obtaining even local guarantees is not trivial. Nevertheless, in Propositions~\ref{prop:deepfool} and \ref{prop:projection} we worked towards this goal. We have established local guarantees showing the convergence of each individual operation, namely the DeepFool step and projection step. However, further analysis is needed to establish local guarantees for the overall algorithm.

\paragraph{Adaptive attacks.} 
\label{Adaptive}
It is known that gradient-based attacks, ours included, are prone to gradient obfuscation/masking~\cite{carlini2019evaluating}. To counter this challenge, adaptation, as outlined in~\cite{Tramer}, is needed. It is also important to recognize that adapting geometric attacks such as SDF,  does not follow a one-size-fits-all approach, as opposed to loss-based ones such as PGD. While this might be perceived as a weakness, it actually underscores a broader trend in the community. The predominant focus has been on loss-based attacks. This emphasis has inadvertently led to less exploration and development in the realm of geometric attacks. 

\section{Vanila Adversarial Training}
\paragraph{Vanila Adversarial Training without Additional Regularization.}
\label{AT-reg}
Our primary objective was to evaluate which adversarial attacks technique most effectively enhances robustness among PGD~\cite{madry2017towards}, DDN~\cite{Rony_2019_CVPR}, and SDF. This focus differs from comparing various adversarial training strategies such as TRADES~\cite{TRADES}, TRADES-AWP~\cite{TRADES-AWP}, HAT~\cite{rade2021helper}, and UIAT~\cite{Enemy}. These strategies often include additional regularization techniques to enhance Madry's method using PGD adversarial examples. Therefore, our assertion is not aimed at developing a state-of-the-art robust model. Instead, we aim to demonstrate that vanilla AT, when combined with minimum-norm attacks like SDF, can potentially outperform PGD-based models. Accordingly, we selected vanilla adversarial training with SDF-generated samples for our study and compared its effectiveness against a network trained with DDN samples. While TRADES or similar AT strategies could also integrate SDF, exploring this combination will be addressed in future research endeavors.

\paragraph{Why $\ell_p$ norm is Critical?}
\label{L-P}
The existing literature has explored a variety of approaches to understanding adversarial examples. For example, training on \(\ell_p\)-norm adversarial examples has been identified as a form of spectral regularization~\cite{Spectral-Norm-Reg}, and adversarial perturbations, seen as counterfactual explanations, have been connected to saliency maps in image classifiers~\cite{Saliency-Map}. The rapid and accurate generation of these perturbations is critical for the empirical investigation of such phenomena. Moreover, minimal \(\ell_p\) adversarial perturbations are often considered "first order approximations of the decision boundary," illuminating the local geometric characteristics of models near data samples. This insight underscores the need for quick and precise methods for such explorations. Additionally, these minimal perturbations provide a data-dependent, worst-case analysis of certain test-time corruptions, facilitating worst-case evaluations not only in the input space but also in the transformation space~\cite{kanbak2017geometric}. Within the context of Large Language Models (LLMs), these perturbations could potentially act as probing tools within their embedding space to examine their geometric properties. However, it is important to note that our interest in these topics was driven more by academic curiosity than by their practical applications in this specific study.
%\newpage
\section{Multi-class algorithms for SDF~(1,3) and SDF~(1,1)}
Algorithm~(\ref{alg:SDF(1,1)},\ref{alg:SDF(1,3)}) summarizes pseudo-codes for the multi-class versions of SDF$(1,1)$ and SDF$(1,3)$.
\begin{figure}[htbp]
	\centering
	\begin{minipage}[t]{0.45\textwidth}
		\begin{algorithm}[H]
			\SetKwFunction{Union}{Union}\SetKwFunction{DeepFool}{\texttt{DeepFool}}\SetKwFunction{l}{\texttt{projection step}}
			\KwIn{image $\x$, classifier $f$.}
			\KwOut{perturbation $\boldsymbol{r}$}
			
			Initialize: $\x_{0}\leftarrow\x,\enskip i\leftarrow 0$
			
			\While{$\hat{k}(\x_{i})= \hat{k}(\x_{0})$}
			{  
				\For{$k\neq \hat{k}(\x_0)$}     
				{ 
					$\w'_k\gets \nabla f_k(\x_i)-\nabla f_{\hat{k}(\x_0)}(\x_i)$
					$f'_k\gets f_k(\x_i)-f_{\hat{k}(\x_0)}(\x_i)$
				}
				$\hat{l}\gets\argmin_{k\neq{\hat{k}(\x_0)}}\frac{\left|f'_k\right|}{\|\w'_k\|_2}$
				$\boldsymbol{\widetilde{r}}\gets \frac{\left|f'_{\hat{l}}\right|}{\|\w'_{\hat{l}}\|_2^2}\w'_{\hat{l}}$
				$\widetilde{\x}_{i} = \x_{i} + \boldsymbol{\widetilde{r}}$
				$\w_{i}\leftarrow\nabla f_{k(\widetilde{\x}_{i})}(\widetilde{\x}_{i}) - \nabla f_{k(\x_{0})}(\widetilde{\x}_{i})$
				$\x \gets \x_0 + \frac{(\widetilde{\x}_{i}-\x_0)^\top \w_{i}}{\| \w_{i}\|^2} \w_{i}$
				$i\leftarrow i+1$
			}
			\KwRet $\boldsymbol{r}=\x_{i}-\x_{0}$
			\caption{SDF ($1$,$1$)}
			\label{alg:SDF(1,1)}
		\end{algorithm}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.45\textwidth}
		\begin{algorithm}[H]
			\SetKwFunction{Union}{Union}\SetKwFunction{DeepFool}{\texttt{DeepFool}}\SetKwFunction{l}{\texttt{projection step}}
			\KwIn{image $\x$, classifier $f$.}
			\KwOut{perturbation $\boldsymbol{r}$}
			
			Initialize: $\x_{0}\leftarrow\x,\enskip i\leftarrow 0$
			
			\While{$\hat{k}(\x_{i})= \hat{k}(\x_{0})$}
			{  
				\For{$k\neq \hat{k}(\x_0)$}     
				{ 
					$\w'_k\gets \nabla f_k(\x_i)-\nabla f_{\hat{k}(\x_0)}(\x_i)$
					$f'_k\gets f_k(\x_i)-f_{\hat{k}(\x_0)}(\x_i)$
				}
				$\hat{l}\gets\argmin_{k\neq{\hat{k}(\x_0)}}\frac{\left|f'_k\right|}{\|\w'_k\|_2}$
				$\boldsymbol{\widetilde{r}}\gets \frac{\left|f'_{\hat{l}}\right|}{\|\w'_{\hat{l}}\|_2^2}\w'_{\hat{l}}$
				$\widetilde{\x}_{i} = \x_{i} + \boldsymbol{\widetilde{r}}$
				
				\For{$3$ steps}{
					$\w_{i}\leftarrow\nabla f_{k(\widetilde{\x}_{i})}(\widetilde{\x}_{i}) - \nabla f_{k(\x_{0})}(\widetilde{\x}_{i})$
					$\x_{i} \gets \x_0 + \frac{(\widetilde{\x}_{i}-\x_0)^\top \w_{i}}{\| \w_{i}\|^2}  \w_{i}$
				}
				$i\leftarrow i+1$
			}
			\KwRet $\boldsymbol{r}=\x_{i}-\x_{0}$
			\caption{SDF ($1$,$3$)}
			\label{alg:SDF(1,3)}
		\end{algorithm}
	\end{minipage}
\end{figure}

%\begin{algorithm}
%	\SetKwFunction{Union}{Union}\SetKwFunction{DeepFool}{\texttt{DeepFool}}\SetKwFunction{l}{\texttt{projection step}}
%	\KwIn{image $\x$, classifier $f$.}
%	\KwOut{perturbation $\boldsymbol{r}$}
%	
%	Initialize: $\x_{0}\leftarrow\x,\enskip i\leftarrow 0$
%	
%	\While{$\hat{k}(\x_{i})= \hat{k}(\x_{0})$}
%	{  
%		\smallskip
%		\For{$k\neq \hat{k}(\x_0)$}     
%		{ 
%			\smallskip
%			$\w'_k\gets \nabla f_k(\x_i)-\nabla f_{\hat{k}(\x_0)}(\x_i)$ 
%			
%			
%			\smallskip
%			$f'_k\gets f_k(\x_i)-f_{\hat{k}(\x_0)}(\x_i)$
%		}
%		\smallskip
%		$\hat{l}\gets\argmin_{k\neq{\hat{k}(\x_0)}}\frac{\left|f'_k\right|}{\|\w'_k\|_2}$
%		
%		\smallskip
%		$\boldsymbol{\widetilde{r}}\gets \frac{\left|f'_{\hat{l}}\right|}{\|\w'_{\hat{l}}\|_2^2}\w'_{\hat{l}}$
%		
%		\smallskip
%		$\widetilde{\x}_{i} = \x_{i} + \boldsymbol{\widetilde{r}}$
%		
%		\smallskip
%		$\w_{i}\leftarrow\nabla f_{k(\widetilde{\x}_{i})}(\widetilde{\x}_{i}) - \nabla f_{k(\x_{0})}(\widetilde{\x}_{i})$
%		
%		\smallskip
%		$\x \gets \x_0 + \frac{(\widetilde{\x}_{i}-\x_0)^\top \w_{i}}{\| \w_{i}\|^2} \w_{i}$
%		
%		
%		\smallskip
%		$i\leftarrow i+1$
%	}	
%	
%	% \ali{$\x = \text{clip}(\x,0,1)$}
%	
%	\KwRet $\boldsymbol{r}=\x_{i}-\x_{0}$
%	\vspace{2mm}
%	\caption{SDF (1,1)}
%	\label{alg:SDF(1,1)}
%\end{algorithm}
%%\vspace{-3mm}
%% \AlgoDontDisplayBlockMarkers
%% \RestyleAlgo{ruled}
%% \SetAlgoNoLine
%% \LinesNumbered
%\begin{algorithm}
%	\SetKwFunction{Union}{Union}\SetKwFunction{DeepFool}{\texttt{DeepFool}}\SetKwFunction{l}{\texttt{projection step}}
%	\KwIn{image $\x$, classifier $f$.}
%	\KwOut{perturbation $\boldsymbol{r}$}
%	
%	Initialize: $\x_{0}\leftarrow\x,\enskip i\leftarrow 0$
%	
%	\While{$\hat{k}(\x_{i})= \hat{k}(\x_{0})$}
%	{  
%		\smallskip
%		\For{$k\neq \hat{k}(\x_0)$}     
%		{ 
%			\smallskip
%			$\w'_k\gets \nabla f_k(\x_i)-\nabla f_{\hat{k}(\x_0)}(\x_i)$ 
%			
%			
%			\smallskip
%			$f'_k\gets f_k(\x_i)-f_{\hat{k}(\x_0)}(\x_i)$
%		}
%		\smallskip
%		$\hat{l}\gets\argmin_{k\neq{\hat{k}(\x_0)}}\frac{\left|f'_k\right|}{\|\w'_k\|_2}$
%		
%		\smallskip
%		$\boldsymbol{\widetilde{r}}\gets \frac{\left|f'_{\hat{l}}\right|}{\|\w'_{\hat{l}}\|_2^2}\w'_{\hat{l}}$
%		
%		\smallskip
%		$\widetilde{\x}_{i} = \x_{i} + \boldsymbol{\widetilde{r}}$
%		
%		
%		\smallskip
%		
%		\For{$3$ steps}{
%			\smallskip
%			$\w_{i}\leftarrow\nabla f_{k(\widetilde{\x}_{i})}(\widetilde{\x}_{i}) - \nabla f_{k(\x_{0})}(\widetilde{\x}_{i})$
%			
%			\smallskip
%			$\x_{i} \gets \x_0 + \frac{(\widetilde{\x}_{i}-\x_0)^\top \w_{i}}{\| \w_{i}\|^2}  \w_{i}$
%			
%			
%		}		
%		\smallskip
%		$i\leftarrow i+1$
%	}	
%	
%	% \ali{$\x = \text{clip}(\x,0,1)$}
%	
%	\KwRet $\boldsymbol{r}=\x_{i}-\x_{0}$
%	\vspace{2mm}
%	\caption{SDF (1,3)}
%	\label{alg:SDF(1,3)}
%\end{algorithm}

%\newpage


% \section{PGD and SDF comparison}

% \begin{figure}
%     \centering
%     % \begin{subfigure}[b]{0.70\textwidth}
%         \centering
%         \includegraphics[width=.5\linewidth]{photos/Curves/PGD-SDF.pdf}
%         % \caption{DF}
%     % \end{subfigure}
%     \hspace{1cm}
%     % \begin{subfigure}[b]{0.45\textwidth}
%     %     \centering
%     %     \includegraphics[width=0.8\linewidth]{photos/Query_norm_SDF_WRN_Madry.pdf}
%     %     \caption{SDF}
%     % \end{subfigure}
%     \caption{}
%     % \label{fig:Query-perturbation}
% \end{figure}


%It is observed that although they are more precise than DF. However, SDF achieves more accurate perturbations.}

% \section{\orig{Ablation study on FAB's performance in AA}
% }
% In this section we provide a new variant of AA++. We replace FAB with SDF. As we already mentioned, SDF outperforms FAB in various settings.

% \begin{table}[h]
%     \centering
%     \caption{An analysis of robust accuracy~($\%$) for various defense strategies against AA++~(replace FAB with SDF) and AA++~(replace APGD with SDF) with $\varepsilon = 0.5$. The \orig{acc} column denotes the robust accuracy of different models. R1~\cite{rebuffi2021fixing} and R2~\cite{Proxy} have been trained adversarially. All models are taken from the RobustBench library~\cite{croce2020robustbench}.}
%     \vspace{-2mm}
%     \begin{small}
%         \begin{minipage}{\linewidth}
%             \centering
%             \begin{tabular}{rlllll}
%                 \toprule
%                 \multirow{2}{*}{\makecell[c]{Models}}
%                 & Clean
%                 & \multicolumn{2}{c}{AA++ replace FAB with SDF}
%                 & \multicolumn{2}{c}{AA++ replace APGD with SDF}\\
%                 \cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-6} 
%                 & acc& acc & Grads & acc & Grads \\
%                 \midrule

%                 R1 & $95.7\%$ & $82.1\%$ & $996.4$ & $82.1\%$ & $\mathbf{599.5}$\\
%                 R2  & $90.3\%$ & $76.1\%$ & $1469.1$ & $76.1\%$ & $\mathbf{667.7}$\\
%                 \bottomrule  
%             \end{tabular}
%         \end{minipage}
%     \end{small}
%     \label{AA++}
% \end{table}




% \begin{wrapfigure}{r}{0.89\textwidth}
%     \centering
%     \includegraphics[width=0.89\linewidth]{photos/Curves/ccdf_combined_plots.pdf}
%     \caption{The presented graphic illustrates the Query-Fooling curves. The plot on the left corresponds to the model (WRN-28-10) that was trained using natural methods. The optimal plot refers to the adversarially trained model developed by~\cite{rade2021helper}.}
%     \label{fig:trade-off}
% \end{wrapfigure}

% \newpage


% \clearpage

% \section{Examples of adversarial images generated using SDF}
% \begin{figure*}
% \centering
% {\onecolumn

%     \begin{subfigure}[b]{\columnwidth}
%         \includegraphics[width=\linewidth]{photos/ImageNet/makaroni.pdf}
%     \end{subfigure}\!
%     \begin{subfigure}[b]{\columnwidth}
%     \includegraphics[width=\linewidth]{photos/ImageNet/tower.pdf}
%     \end{subfigure}
%     \begin{subfigure}[b]{\columnwidth}
%         \includegraphics[width=\linewidth]{photos/ImageNet/mouse.pdf}
%     \end{subfigure}\!
%     \begin{subfigure}[b]{\columnwidth}
%         \includegraphics[width=\linewidth]{photos/ImageNet/shirini.pdf}
%     \end{subfigure}
%     \caption{Adversarial examples for ImageNet, as computed by SDF on a ResNet-$50$ architecture.~(Perturbations are magnified $\sim10\times$ for better visibility.)}
%     }
%     \label{fig:pert_imagenet}
% \end{figure*}