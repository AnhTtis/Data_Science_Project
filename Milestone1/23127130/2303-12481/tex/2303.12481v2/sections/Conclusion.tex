\section{Conclusion and Future Works}
In this work, we have introduced a family of parameter-free, fast, and parallelizable algorithms for crafting optimal adversarial perturbations. Our proposed algorithm, SDF, \textit{\textbf{consistently}}
	finds smaller norm perturbations on various networks and
	datasets with only a small additional computation
	cost compared to DF (which is still significantly faster than
	all SOTA attacks). Furthermore, we have shown that adversarial training using the examples generated by SDF builds more robust models. While our primary focus in this work has been on minimal $\ell_2$ attacks, there exists potential for extending SDF families to other threat models, including general $\ell_{p}$-norms and targeted attacks. 
In the Appendix, we have demonstrated straightforward modifications that highlight the applicability of SDF to both targeted and $\ell_{\infty}$-norm attacks. However, a more comprehensive evaluation remains a direction for future work. Moreover, further limitations of our proposed method are elaborated upon in Appendix~\ref{sec:limitations}. In the end, by revisiting the necessity of $\ell_{p}$-norm robustness and characterizing a toy example on robustness-free phenomena, we underscore the pivotal role of minimum-norm attacks in ensuring secure AI systems.

\section{Acknowledgments}
 We want to thank Kosar Behnia and Mohammad Azizmalayeri for their helpful feedback. We are very grateful to Fabio Brau and Jérôme Rony for providing code and models and answering questions on their papers. 