\section{Documentation Framework}
\label{sec:framework}

To document the foundation model ecosystem, we introduce the \EG framework.
Informally, the framework is defined by a graph comprised of (i) \textit{assets} (\eg ChatGPT), (ii) \textit{dependencies} (\eg datasets used to build to ChatGPT, applications built upon ChatGPT), and (iii) \textit{ecosystem cards} (\eg metadata on ChatGPT). 

\subsection{Definition}
    \begin{figure*}
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/basic_primitive.png}
            \caption{Canonical}    
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{figures/adaptation_primitive.png}
            \caption{Adaptation}
        \end{subfigure}
        \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{figures/application_primitive.png}
            \caption{Application layers}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{figures/application_dependency_primitive.png}
            \caption{Application dependence}
        \end{subfigure}
\caption{\textbf{Primitive subgraphs in the ecosystem.} 
We spotlight a few 3-node subgraphs to build intuition for \EG.
(a) The standard pipeline: The Jurassic-1 dataset is used to train the Jurassic-1 FM, which is used in the AI21 Playground.
(b) A common adaptation process: BLOOM is adapted by fine-tuning on XP3 to produce the instruction-tuned BLOOMZ model.
(c) Layering of applications: The GPT-4 API powers Microsoft365 Copilot, which is integrated into Microsoft Word.
(d) Dependence on applications: While applications often are the end of pipelines (\ie sinks in the graph), applications like Google Search both depend on FMs like MUM and support FMs (\eg through knowledge retrieval) like Sparrow.
}
\label{fig:primitives}
\end{figure*}

The ecosystem graph is defined in terms of assets: each asset $a \in A$ has a \textit{type} $T(a) \in \{\text{dataset},~\text{model},~\text{application}\}$.
Examples include The Pile dataset, the Stable Diffusion model, and the Bing Search application. 
To define the graph structure, each asset $a$ has a set of dependencies $D(a) \subset A$, which are the assets required to build $a$.
For example, following the Stable Diffusion example in \refsec{ecosystem}, LAION-5B is a dependency for Stable Diffusion and Stable Diffusion is a dependency for Stable Diffusion Reimagine.
Dependencies correspond to directed edges in the ecosystem graph.
In \reffig{primitives}, we given several examples of primitive structures (\ie subgraphs) that we observe in the full ecosystem graph.

To enrich assets with contextual metadata, each asset $a$ is annotated with \textit{properties} $p(a)$ that are stored in the asset's ecosystem card.
Properties include the "organization" that created the asset, the "license" enforced to use the asset, and type-specific properties (\eg the "size" of a model).
For each property (\eg the "license"), we annotate the property as it applies to the asset (\eg the license for Stable Diffusion is the CreativeML OpenRAIL M license).\footnote{\url{https://github.com/CompVis/stable-diffusion/blob/main/LICENSE}} 

\paragraph{Caveats.}
The definition of \EG is deliberately minimalistic: our objective is to make the ecosystem simple to understand to ensure it is legible to diverse stakeholders.
Under the hood, we introduce two additional forms of complexity that we revisit in discussing our implementation of \EG. 
First, while we conceptualize the "nodes" of the ecosystem graph as individual assets, in practice they will instead correspond to sets of closely-related assets (\eg the different model sizes in the GPT-3 model family).
Second, we will annotate properties (\eg the "license") by specifying both a structured \textit{value} (\ie the type of license) and a contextual/qualitative \textit{description} (\eg the provenance for the information). 

Given our definition, we identify five challenges that arise directly from the definition:
\begin{enumerate}
    \item \textbf{Asset discovery.} How do we identify and prioritize the assets?
    \item \textbf{Asset representation.} How do we represent assets?
    \item \textbf{Dependency discovery.} How do we identify the dependencies?
    \item \textbf{Metadata representation.} How do we represent the metadata properties?
    \item \textbf{Metadata annotation.} How do we annotate the metadata for every node?
\end{enumerate}
Beyond this, we should ask who does this work, why they would do it, how the information is maintained (since the ecosystem itself is everchanging), and why it should be trusted.
To ground the \EG framework, we present our concrete implementation before returning to these conceptual challenges and how we navigated them.

\subsection{Implementation}
The ecosystem graph that we have documented thus far is available at \websiteURL.
As of \releasedate, the graph contains \numnodes nodes (\numdatasets datasets, \nummodels models, \numapplications applications) built by \numorganizations organizations that are linked together by \numedges dependencies.
% We briefly summarize all of these assets in \autoref{tab:assets}.

\paragraph{Views.}

\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{figures/music_subgraph.png}
  \caption{\textbf{Graph view} for \EG as of \releasedate (Google's music-related subgraph).
  We highlight salient foundation models (Noise2Music, AudioLM, MusicLM) as well as the shared and intricate dependencies (\eg on SoundStream and MuLan).
  We also observe that music models at present are often language-controlled (\eg introducing a dependency on LaMDA) that thereby links Google's music models with the more extensive (and more productionized) language models (\eg LaMDA, PaLM).
  More generally, beyond these language-mediated dependencies (\eg outbound dependencies of PaLM), Google's music subgraph is fully contained to the nodes depicted in this figure at present (to our knowledge).
  }
  \label{fig:graph-view}
\end{figure*}

\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{figures/latest_models.png}
  \caption{\textbf{Table view} for \EG as of \releasedate (sorted by recent models).
  Over 10 models were released in the 15 day period, with prominent recent models (\eg OpenAI's GPT-4, Anthropic's Claude) disclosing very little to the public (\eg model size, dependency structure). 
  }
  \label{fig:table-view}
\end{figure*}

To visualize the graph structure of the \EG, we provide a simple graph interface shown in \reffig{graph-view}.
Users can zoom into specific regions of the graph to better understand specific subgraphs.
Alternatively, we provide an interactive table (\reffig{table-view}) to search for assets or filter on specific metadata, which can also be exported to a CSV. 
Users can include or exclude specific fields as well as sort by column (\eg the "organization", the asset "type").
Clicking on the node's "name" in either the graph or table views will take the user to the associated ecosystem card.

\paragraph{Ecosystem cards.}

\begin{figure*}
\centering
  \includegraphics[width=0.6\textwidth]{figures/gpt3_ec.png}
  \caption{\textbf{GPT-3 Ecosystem Card.} 
  The card contains basic information (\eg that OpenAI developed GPT-3 and when they announced it to the public in 2020), information on how it was built (\eg a standardized statistics on training emissions in tons of C02 emitted and training time in petaflop/s-days), and how it can be built upon (\eg access is available through the OpenAI API and what uses are prohibited by the API usage guidelines). 
  }
  \label{fig:ecosystem-card}
\end{figure*}

Each node is associated with an ecosystem card.
To navigate between adjacent nodes in the graph, the node's dependencies (upstream) and dependents (downstream) are linked to at the top of the page. 
In \reffig{ecosystem-card}, we provide the ecosystem card for GPT-3 from \assetlongURL{GPT-3}: each property includes a help icon that clarifies what the property refers to (see \autoref{tab:all-properties}).
As we describe subsequently, the ecosystem card design aims to centralize useful information, as can be inferred by the abundance of links, rather than replicate the information.

\paragraph{Code.}
On the backend, \EG is a collection of \texttt{YAML} files that store the annotation metadata against a pre-specified schema of fields that matches \reftab{all-properties}.
All aspects of asset selection are handled by the annotator in choosing what to specify in the \texttt{YAML} file: all specified assets are rendered.
For constructing the graph, the dependencies field is used to build edges between the graph: if a dependency is specified but no ecosystem card has been annotated, a stub node is created in the graph.
Anyone can contribute (\eg adding new assets, editing existing assets) by submitting a pull request at \githubURL that will be reviewed by a verified maintainer of the \EG effort. 

\subsection{Assets and nodes}
Assets and nodes are the building blocks for \EG, framing the ecosystem.
We explore how to identify assets (\textit{asset discovery}) and group assets into nodes (\textit{asset representation}).

\paragraph{Asset discovery.}
The value proposition for \EG is to centralize information that was previously distributed: the first step is to be aware of the assets.
For datasets and models that are discussed in research papers, this is relatively straightforward: indeed, many of the foundation models we currently document are the subject of research papers.
However, even for artifacts discussed in papers, we already identify the presence of \textit{dark matter}: assets that must exist, but the public simply knows (essentially) nothing about.
The GPT-4 paper provides a striking example: no details whatsoever are disclosed on the dataset(s) underlying GPT-4 with the report reading "the competitive landscape and the safety implications of large-scale models like GPT-4, this report contains no further details about \dots dataset construction \dots" \citep{openai2023gpt4}.

As we expand our sights to applications and deployment, we encounter further obscurity.
For example, most company-internal foundation models used in products (\eg many of the models used in Google Search) are not disclosed in any regard, let alone the datasets involved in training and improving these models.
And, conversely, we often do not know all the products and applications that depend on publicly-disclosed foundation models (\eg every downstream application built on ChatGPT or Stable Diffusion). 
For this reason, as part of our asset discovery process, we make use of less conventional resources beyond standard academic research: news articles, corporate press releases, venture capital indices, and other heterogeneous sources.
In the future, we may consider automated information extraction from the Internet to keep pace with the scaling of the foundation model ecosystem, though at present the asset discovery process is fully manual and somewhat \textit{ad hoc}.

In discovering assets, we prioritize assets.
While appealing to track every asset, we found this renders the resulting ecosystem graph cumbersome to navigate and introduces untenable annotation burden. 
As a lower bound, there are 150k+ models and 20k+ datasets available on Hugging Face alone.
Consequently, we make our priorities and decisionmaking explicit through principle(s) that influence asset selection.
Namely, we include assets that 
(i) are socially salient, 
(ii) have outsized impact 
or (iii) represent a broader class (\eg ensuring we include at least one music foundation model).
These are imprecise and subjective criteria: we believe all current assets can be justified under them, but we tend to be liberal in our interpretation, preferring to over-include rather than under-include.
As the ecosystem graph expands and its value/uses become more clear, we imagine revisiting these criteria to arrive at more durable and precise principles.

\paragraph{Asset representation.}
Once we have determined the assets to be included in principle, we observe that many assets are very closely connected.
Therefore, pragmatically, we choose to group closely-related assets together into a single node in representing the ecosystem graph.
We believe this representational choice improves the legibility of the ecosystem.
Concretely, we group assets together into a single node when they are presented as belonging to the same category in the view of their developers or other entities in the ecosystem.
As examples, variants of datasets are often referred to using the same term (\eg many datasets are called "English Wikipedia") as are products (\eg the unannounced versions of Google Search). 
Further, models often belong to a shared collective even if they differ (most often in size) such as the four models introduced by \citet{brown2020gpt3} that are collectively referred to as GPT-3. 
There are trade-offs: by collapsing to one node, we potentially obscure slight differences, though if we observe a distinction is relevant, we can choose to dis-aggregate the node back into its constituent assets. 
Overall, similar to how we determine which assets to include, we construct graph nodes by prioritizing usefulness over faithfulness to the true ecosystem.

\subsection{Dependencies}
To define the graph structure, we need to identify the dependencies of each node (\ie specify $D(a)$ for every $a$).
In practice, determining the dependencies of assets introduced in research papers is fairly straightforward, though there are increasingly exceptions (\eg the aforementioned GPT-4 dataset).
More frequently, even for assets described in research, the assets they depend upon themselves may be opaque.
For example, the AI21 Jurassic-1 models are trained on a dataset that is obliquely described in a single sentence: "Our model was trained with the conventional self-supervised auto-regressive training objective on 300B tokens drawn from publicly available resources, attempting, in part, to replicate the structure of the training data as reported in Brown et al. (2020)." \citep{lieber2021jurassic}.
As a result, we have an asset called "Jurassic-1 training dataset" that the Jurassic-1 models depend upon which will be poorly documented (since we only know the dataset size and nothing else). 
But in other cases, especially those involving assets that are commercial products, the dependency structure itself is almost entirely unknown. 

Overall, we found that annotating the dependency structure is complicated in many practical settings, yet essential for structuring the ecosystem.
% Could make more explicit that we annotate the asset backwards (i.e. target annotates source) but can be understood in either direction. Of course should then also remind that the edge is not bidirectional/is directed.
For example, from the dependencies alone, we come to realize that assets built by 5+ organizations all depend on EleutherAI's The Pile (see \reffig{hubs}).
And it identifies other forms of structure: for example, many more applications today depend on OpenAI language models than Anthropic language models, even though the models benchmark similarly \citep{liang2022helm}, suggesting differential social impact.

\subsection{Ecosystem cards}
\input{tables/all_properties.table.tex}

Having defined the graph structure, we instrument each node by further documenting properties, drawing inspiration from existing documentation approaches such as data sheets \citep{gebru2018datasheets} and model cards \citep{mitchell2018modelcards}.
We iteratively developed our collection of properties based on two principles:
(i) we emphasize properties that are ecosystem-centric (\eg how nodes are influenced by their dependencies or shape their dependents)
and (ii) we offload documentation that exists elsewhere (\eg pointing to existing model cards) to avoid reinventing the wheel.
For each node, we refer to the associated metadata as the node's \textit{ecosystem card}.

In \reftab{all-properties}, we decompose the processing of filling out the ecosystem card into:
\begin{enumerate}
    \item \textit{Basic} properties of the node (\eg the developer organization).
    \item \textit{Construction} properties of the node (\eg the training emissions for models).
    \item \textit{Downstream} properties of the node (\eg the license and terms-of-service for applications).
\end{enumerate}
For each property, we annotate a \textit{value} and, potentially, a \textit{description} that justifies/explains how the value should be interpreted (\eg attributes a source to provide provenance). 

\subsubsection{Basic properties}
To understand nodes and assets even independent of the broader ecosystem, certain basic information is necessary such as the "name" of the asset(s) and the "organization" that produced the asset(s).
To document these properties proves to be fairly straightforward in practice, though there are some complexities.
Unfortunately, there may be opacity even for these basic properties.  (For example, the naming convention for OpenAI models has been historically unclear\footnote{See \url{https://platform.openai.com/docs/model-index-for-researchers}.}
Or the granularity may not be obvious in the case of organizations: as an example, we annotate Copilot as developed by GitHub even though GitHub has been acquired by Microsoft.
In the case of the "description" field, we generally quote the asset(s) developers, along with providing the "URL" that disclosed the node to the public (\eg the paper or press release). 
In addition to these properties, we specifically highlight the "created date": as \EG is maintained over time, filtering on the date can be used to automatically build timelines and understand how the ecosystem evolves.
As an example, filtering for node(s) before January 1, 2023 vs. after January 1, 2023 makes obvious both (i) the early adopters of foundation models and (ii) how publicly-announced deployments have rapidly accelerated in 2023.

\subsubsection{Construction properties}
A node's dependencies by no means fully determine its nature: there are many choices on how to use these dependencies (\eg many products can be built from the same model, different models can arise from training on the same dataset).
However, it is challenging to meaningfully summarize this: the training procedure for many prominent foundation models can amount to dozens of pages in a well-written paper \citep[see][]{chowdhery2022palm}, meaning even a summary would be very long and likely not much more useful than pointing to the paper itself. 
Having surveyed a variety of assets, we converged to 
(i) a broad umbrella category of \textit{quality control} for all assets, 
(ii) deliberate \textit{inclusion/exclusion} for datasets (\eg filtering out "toxic" content based on a word list, which may have the side-effect of removing LGBTQ+ language \citep{dodge2021c4, gururangan2022whose}), 
(iii) material \textit{training costs} for models (\eg to contextualize environmental impact \citep{lacoste2019quantifying, strubell2019energy, henderson2020towards, luccioni2023counting}) and 
(iv) \textit{adaptation} details for applications (\eg fine-tuning details and UI design). 
We found these details provide important context since dependencies visually look the same in \EG, yet these relationships are non-equal. 

\subsubsection{Downstream properties}
To construct the ecosystem graph, we specify dependencies on the target asset: given a node, we annotate its parents.\footnote{We found this natural since, in general, we may not know the dependents of a given upstream asset (\eg ChatGPT continues to accrue new dependents well after its initial release), but we can better trace the lineage when annotating the downstream asset.} 
However, some properties of an asset influence how it can be built upon, rather than how it was built.
Most notably, the \textit{access} status of an asset directly determines who can build on it, whereas the \textit{intended/prohibited uses} influence how the asset should be built upon (in addition to the \textit{license} and \textit{terms of service}).
In general, we found these properties to be straightforward to annotate, though we find discussion of intended/prohibited uses is quite uneven and in some cases no license/terms of service could be found.

Beyond how the asset is built upon, we further annotate fields that determine the asset's downstream social impact.
Most notably, this makes clear the sense in which applications ground out much of the impact: applications have \textit{end users}.
Further, to build upon the transparency of \EG, we highlight important mechanisms for accountability/recourse that we track: 
(i) can asset developers \textit{monitor} the usage of their assets downstream,
(ii) do specific \textit{failures} or harms concretely arise,
and (iii) when these issues come up, do \textit{feedback} mechanisms exist to propagate this information back upstream?
These fields signal assets that are having high impact, which could confer recognition (or even payment) to those who contributed to their widespread downstream impact (\eg valuing data creators whose data generates value downstream).
 
\subsubsection{Complementarity of construction and downstream properties}
The construction and downstream properties together enrich the underlying graph in an essential way.
When edges are interpreted in the forward direction, they indicate how assets are built; when the edges are interpreted in the backwards direction, they indicate how feedback would flow back upstream.
We stress this point as a reflection of the immaturity of the foundation model ecosystem at present by analogy to other industries.

Concretely, we juxtapose the FM ecosystem with the automobile industry as a more established industry with robust practices for its supply chain.
The National Highway Traffic Safety Administration (NHTSA; an agency under the US Department of Transportation), among other entities, is tasked with ensuring automotive safety.\footnote{See their guidelines on motor vehicle safety at \url{https://www.nhtsa.gov/sites/nhtsa.gov/files/documents/14218-mvsdefectsandrecalls_041619-v2-tag.pdf}.}
Following the forward flow of materials through the supply chain, when a batch of parts (\eg brakes) is found to be sub-standard, established protocols mandate the recall of the fleet of cars built using those parts.
Since the National Traffic and Motor Vehicle Safety Act was enacted in 1966, the NHTSA has recalled over 390 million cars due to safety defects.
Conversely, when several cars (possibly from different manufacturers) are reported to be faulty, the shared source of the defect can be traced by attributing their common sources/parts.
Critically, centralized infrastructure exists for consumers to identify how to report issues to the NHTSA (\eg the Department of Transporation's Vehicle Safety Hotline), to interpret how their report will be used, to understand the investigation process the NHTSA implements, and to understand the legal remedies and consumer protections they are afforded.
And the Federal Motor Vehicle Safety Standards sets formal and transparent standards on what constitutes the minimum performance requirements for each (safety-relevant) automotive part.

In short, the automobile industry and its practices/norms illustrate the virtues of observing the ecosystem as a whole and how both forward and backwards traversals can directly inform action.
If an upstream asset (akin to the faulty brakes) is identified to be faulty in the FM ecosystem, we are not confident that broad communication, let alone interventions like a recall, can reliably be expected to occur.
For example, if LAION-5B was shown to be data poisoned \citep{carlini2023poisoning}, how would the developers of Stable Diffusion, the subsequent application developers who built upon Stable Diffusion, and the broader end userbase be notified?
In part, this uncertainty simply is due to the absence of comparable entities to the NHTSA that are responsible for governing the ecosystem, but also due to the absence of infrastructure for information propagation.
Similarly, many assets themselves lack monitoring mechanisms (especially when assets are released openly, monitoring is currently often nonexistent), let alone publicly-disclosed means for relaying feedback and incident reports upstream.
While we expect the organizations may have partnerships (\eg when Khan Academy users surface issues, Khan Academy and OpenAI likely have significant dialogue to diagnose if these problems arise from Khan Academy's use of OpenAI's GPT-4), we specifically highlight the inadequacy for end users and consumers.
In other words, the basic consumer protections for ensuring an individual adversely affected by a product downstream is able to communicate this upstream and be taken seriously do not exist.


\subsection{Annotation practices}
To annotate each property requires dealing with many types of variation: different assets often have specific properties that are idiosyncratic and, in many instances, information is not publicly available.
As we iterated on annotation best practices, we identified two key concepts: (i) how should we interpret \textit{missing} data entries and (ii) how can we \textit{trust} the recorded information?

\paragraph{Missing data.}
We identified four forms of missing data that arise under different conditions: each form has different semantics, which we clarify by annotating the value for the associated property differently.
\begin{enumerate}
    \item \textbf{None}. A property is annotated with the value \textit{none} if an annotator looked for the information and was unable to find it. For example, for many nodes, we could not find any feedback mechanism or monitoring information. It is possible a feedback form does exist, but given we could not find it, we believe this information is effectively too obscure (\eg unreasonable to expect a consumer to find when reporting feedback).
    \item \textbf{Unknown}. A property is annotated with the value \textit{unknown} if the information must exist (and, potentially, an annotator looked for the information and was unable to find it). For example, the training hardware and training time are often not disclosed for many models, but of course the models were trained on some hardware and took some time to train.
    \item \textbf{Empty string}. A property is annotated with the empty string if the annotator chose to not annotate the property, which generally indicates a lack of time. To ensure all ecosystem cards have non-zero information, we minimally require the basic properties be annotated. 
    \item \textbf{N/A}. A property is annotated with the value \textit{N/A} if the property is not applicable to the node/asset(s). By design, the properties are chosen to be broadly applicable, so we encounter this annotation very rarely.
\end{enumerate}
When aggregated across the entire ecosystem, these conventions for missing data help to articulate pervasive opacity (\ie many \textit{unknown} values) and immaturity (\ie many \textit{none} values) in the foundation model ecosystem. 

\paragraph{Trust.}
To ensure information in \EG is legitimate and credible, we implement two mechanisms.
First, to add or modify information, all such requests must be verified by a vetted maintainer to ensure the correctness of the information as well as the consistency with prior annotations.
Consequently, especially as \EG expands to be a community-maintained artifact, this form of moderation provide some guarantees on the sustained quality of the resource.
In fact, since \EG is implemented as a GitHub repository, the full version history of commits (and any associated discussion) is maintained to ensure the provenance of information (much akin to the Wikipedia change log). 
Moving forward, more sophisticated moderation (\eg akin to Wikipedia maintenance) could be developed.
Second, to source the information, we require that information be attributable to a publicly-available source that is provided in the description accompanying the property's value.
In other words, information should have clear provenance to both ensure the annotation matches the source and that the source itself is reliable.
In the future, we imagine this constraint may prove restrictive (namely because it is inconvenient for reporting privately-disclosed information), but we imagine conventions akin to those in journalism can be adopted if necessary. 

\subsection{Maintenance and Incentives}
Everything in the ecosystem graph is subject to change. 
Since foundation models are being deployed remarkably quickly, keeping pace in documenting the ecosystem is an ongoing challenge.
For example, in the week of March 13, 2023, over a dozen products were announced that all depend on OpenAI's GPT-4.
Further, even for existing assets, their dependencies or metadata may change over time: Anthropic's Claude and Google's PaLM were initially closed access, but now are limited access with the release of their respective APIs for downstream development.

For this reason, we explore \textit{who} will maintain \EG and whether \textit{incentives misalignment} introduces challenges, given much of the value is contingent on the graph being up-to-date and correct.
Moving forward, as foundation models feature more centrally in broader social discourse and \EG sees greater adoption, maintenance could be mandated as a policy requirement to ensure sustained transparency \citep[see][]{bommasani2023transparency, madry2023supplychain}. 

\paragraph{Who maintains the ecosystem graph?}
To this point, the ecosystem graph has been built and maintained by the authors of this work.
Moving forward, while this will continue, this will be increasingly insufficient given the growing scale of the ecosystem.
Therefore, we envision two complementary strategies for expanding the group involved in maintaining \EG. 
Building on traditions of open source software development (\eg Wikipedia, Mozilla, PyTorch, Hugging Face), we actively encourage contributions.
To broaden accessibility, new assets can be submitted from a public Google form to remove the barriers of having a GitHub account and familiarity with GitHub.
Further, for this reason we use a lightweight process with explicit guidelines on how to create and edit entries in \texttt{YAML}.
Drawing upon trends in open source, we will implement processes for top contributors to be recognized for their achievement and signal-boosted in the broader community.\footnote{While we do not currently implement any extrinsic bounties, works like \citet{zhao2017devising} and \citet{chowdhury2021twitter} demonstrate their efficacy, warranting further consideration in the future.}

While open source contributions can be very powerful, permitting decentralized contribution to the shared knowledge resource, they require a culture to form that supports and sustains them.
As \EG grows and is more broadly adopted, we envision it may become a broad-use public repository that organizations themselves are incentivized to maintain.
In the future, we propose that major FM organizations each select a dedicated representative responsible for the upkeep of the organization's nodes (and, possibly, some direct neighbors).
This mechanism introduces \textit{accountability}: the veracity of an organization's nodes and dependencies is the responsibility of this maintainer.
Here, we could lean on practices of periodic public reporting (\eg quarterly financial earnings) in reminding the representative to update the graph on a specific cadence.
We imagine the specifics of this will further sharpen as \EG is more broadly adopted, and as we better understand both the rate of change in the ecosystem and the informational needs that \EG serves. 
In the future, the process of updating the ecosystem graph could be integrated into organizational-internal data entry, since much of what is tracked in \EG is likely already being tracked within organizations. 

\paragraph{The compatibility of incentives.}

Ensuring the ecosystem is transparent serves many informational needs and benefits many stakeholders.
Much akin to other shared knowledge resources (\eg Wikipedia, the US Census), downstream use cases continuously arise, further incentivizing the sustained upkeep of the resource.
In the FM ecosystem specifically, we expect asset developers will be incentivized to disclose the impact of their assets (\eg organizations often put out press releases to disclose information on the widespread use of their products).\footnote{For example, see the strategic partnership between Hugging Face and Amazon: \url{https://huggingface.co/blog/aws-partnership}.}
Or, akin to how food vendors proactively announce their food is organic or how cosmetics companies proactively indicate their cosmetics involve the humane treatment of animals, asset developers should definitely be incentivized to highlight their own responsible conduct.
While incentives may not exist for every bit of information to be made transparent, we hope \EG will encourage increased transparency by demonstrating the value of making information and knowledge public.

However, we do recognize there simultaneously exist pernicious incentives for organizations to maintain opacity in the ecosystem: most directly, transparency could infringe on corporate secrets and commercial interest.
Central to our approach in \EG is recognizing that commercial interest need not entail a blanket ban on transparency: in many cases, information can be made transparent to the public while not compromising any commercial agenda.
In other words, the information in question is common knowledge amidst an organization's competitors, and it is better for the public to have partial transparency rather than to have nothing at all.
This approaches aligns with having representatives of each organization: the process of making assets transparent can involve engaging with the organization, flexibly and iteratively identifying the boundaries of transparency in an organization-specific and asset-specific way (\eg OpenAI's desire for transparency may change from 2021 to 2022, or from CLIP to ChatGPT). 
More expansively, \EG mediates an incremental process for building norms of transparency \citep{liang2022community-norms, bommasani2023transparency} and functions as an inroad for policy intervention as specific informational needs grow more important.