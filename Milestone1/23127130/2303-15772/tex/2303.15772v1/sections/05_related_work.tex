\section{Related Work}
\label{sec:related-work}

To situate our work, we consider both our objective to make foundation models transparent and our methodology to track dependencies.

\subsection{Transparency in AI}
To make AI systems transparent, we observe three broad classes of approaches.

First, \textit{evaluation} is a widespread practice for articulating the properties and measuring the behavior of systems: in the research community, it is customary to evaluate systems against particular benchmarks to assess their performance.
Evaluations can vary in the specific type of transparency \citep[see][]{bommasani2023transparency} they provide: some evaluations quantify the accuracy of models \citep[\eg ImageNet;][]{deng2009imagenet}, others stress-test models \citep[\eg CheckList;][]{ribeiro2020beyond} or adversarially identify failures \cite[\eg red-teaming;][]{perez2022red} and still others characterize models along a range of dimensions \citep[\eg HELM;][]{liang2022helm}.
In general, while some efforts expand evaluation to datasets \citep{bommasani2020intrinsic, swayamdipta2020dataset, ethayarajh2022dataset, mitchell2022measuring} or adopt methodologies from human-computer interaction to consider human factors like user experience \citep{lee2022coauthor, lee2022interaction}, for the most part, evaluation aims to characterize a specific model in isolation.

Second, \textit{documentation} is a growing practice for specifying metadata about the broader context that situates model and system development.
Formative works like data sheets \citep{gebru2018datasheets} and model cards \citep{mitchell2018modelcards} brought this approach to the fore, complementing evaluations by articulating design decisions and developer positions involved in creating assets.
Subsequent efforts have enriched these approaches to make these documentation artifacts more useful, accessible, or otherwise aligned to specific informational needs \citep{crisan2022interactive}.\footnote{See \url{https://huggingface.co/docs/hub/model-cards}.} 
In general, documentation efforts aim to contextualize a specific asset against a broader social backdrop, often with an inclination towards how the asset came to be and with greater uptake for research-centric assets to our knowledge.

Third, \textit{analyses} and critiques have become increasingly relevant, showcasing much of the latent and oft-overlooked underpinnings of AI development and deployment.
These works often bring questions of values and power to the fore, frequently appealing to concepts or methodologies from the social sciences and disciplines beyond computer science \citep[\eg][]{dotan2020value,
ethayarajh2020utility, 
scheuerman2021politics,
raji2021ai,
koch2021reduced,
denton2021genealogy,
paullada2021data,
birhane2022values,
bommasani2022evaluation}. 
Rather than specific assets (other than for case study/illustration), analytic work centers broader themes \citep[\eg algorithmic intermediaries;][]{lazar2023algorithmiccity} or classes of technology \citep[\eg predictive optimization;][]{wang2022against}.

Our framework shares the objective of making AI (specifically foundation models) transparent. 
However, it differs along important axes from all of these established approaches, perhaps most closely resembling the documentation class (since, indeed, \EG is a documentation framework).
In contrast to current interpretations of evaluation and documentation, \EG is fundamentally about the ecosystem rather any specific asset: the value of \EG arises from tracking all assets.\footnote{We do note other works exist at ecosystem scale in other senses such AI100 \citep{stone2022artificial}, AI Index \citep{zhang2021ai}, and various data/model repositories \citep{wolf2020transformers, lhoest2021datasets}; see \url{https://modelzoo.co/}.} 
This introduces a variety of new challenges (\eg partial observability of certain information, more complicated maintenance as there is constant change across the ecosystem even if particular assets may not change for extended periods).
Further, \EG especially highlights the importance of grounding out into applications (for which a general-purpose analogue of data sheets and model cards does not exist to our knowledge) and, more generally, moving beyond research artifacts to commercial artifacts that affect broader society. 
Finally, in comparison to analytic/critical methods, \EG retains much of the concreteness of evaluation/documentation: we believe \EG provides valuable descriptive understanding that could support future normative analyses and critiques. 

Beyond these distinctions, we emphasize that our contribution extends beyond most prior works on documentation in AI.
Concretely, most prior works \citep[\eg][]{gebru2018datasheets, bender2018data, mitchell2018modelcards} propose the framework to document artifacts, perhaps with an additional proposal of who will conduct this documentation and how/why.
In contrast, we concretely execute, implementing the \EG framework in our codebase and public website. 
This mirrors works like HELM \citep{liang2022helm} where, in addition to designing an evaluation, the contributions include evaluating all language models available at present. 
The infrastructure, sustained upkeep and, ultimately, the resource itself are what provide value: ensuring transparency requires we follow through and enact the conceptual frameworks we design. 

\subsection{Dependencies}
\label{sec:related-work-dependencies}
At the technical level, \EG foreground the tracking of dependencies, whereas at the social level, \EG delineates institutional relationships.
Both of these constructs are encountered in almost every mature industry and, therefore, have been studied across a range of fields.
Concretely, almost every commercial product is the composite of some collection of materials/ingredients, meaning it has a complex supply chain.
As a result, we specifically point to related work in open-source software (which share similar implementation to \EG) and market structure (which emulate \EG in terms of organizations).

\paragraph{Open-source software.}
Much like foundation models, open-source software development is sustained by an immense network of dependencies.
Akin to our efforts to track the foundation model ecosystem, the demand to track the open-source software ecosystem is immense: the software bill of materials (SBOM) is a national-level initiative of the US's Cybersecurity and Infrastructure Security Agency to maintain an inventory of the ingredients that make up software \citep{whitehouse2021cybersecurity}.\footnote{See \url{https://www.cisa.gov/sbom}.}
These approaches have clarified how to ensure compliance from different stakeholders (\eg software vendors) and how to standardize information to support a range of use cases, providing inspiration for the abstractions we make in \EG. 
To implement this vision, a range of efforts have been put forth over the years\footnote{See \url{https://libraries.io/}, \url{http://deps.dev/}, and \url{https://ecosyste.ms/}.} with applied policy work mapping out the sociotechnical challenges for maintaining and funding these efforts \citep{ramaswami2021securing, scott2023atlanticcouncil}.
Further, they present an exemplar of policy uptake towards mandatory public reporting of these dependencies as exemplified by the proposed Securing Open Source Software Act of 2022.\footnote{\url{https://www.congress.gov/bill/117th-congress/senate-bill/4913}}
And, much akin to the \refsec{uses} we consider, these efforts already have shown how descriptive understanding of the ecosystem directly informs decision-making and characterizes the impact of assets.\footnote{See \url{https://docs.libraries.io/overview.html\#sourcerank} and \url{https://github.com/ossf/criticality\_score}.}


\paragraph{Market structure.}
In defining \EG, we made the fundamental decision to define the graph in terms of assets.
We contrast this with approaches more common in disciplines like economics and sociology, where it would be customary to instead foreground the organizations/institutions responsible for creating these assets \citep{rowlinson1997organisations}.
We believe this (fairly techno-centric) choice provides valuable leverage given the status quo: the number of assets is currently still manageable (on the order of hundreds to thousands), the assets themselves are distinctive (\eg they are not exchangeable in the way oil or steel may be in other market analyses), and specific assets markedly contextualize our understanding of organizations (\eg Stable Diffusion dramatically shapes our perception of Stability AI). 
In spite of these advantages, we point to a range of works that foreground institutions in mapping out market structure and the dynamics by which actors interact to shape the economy.
For example, given we draw upon a comparative analysis of the FM ecosystem to the automotive ecosystem, \citet{weingast1988industrial} demonstrate that institution-centrism better allow for comparisons/juxtapositions across sectors.
Alternatively, \citet{einav2010empirical} showcase how grounding to institutions facilitates various forms of measurement (\eg due to firm-level requirements on information disclosure).
Finally, many works in political and institutional sociology prime us to view institutions as the natural unit for studying power relations in modern networks and markets \cite[][\textit{inter alia}]{frickel2006new, dequech2006institutions, fleury2014sociology}.