
% \vspace{5pt}
\section{Related Work}

% Recently, there has been a lot of interest 
There is a growing interest in utilizing intermediate representations of LMs for interpretability and efficiency.
For interpretability, one seeks to understand the prediction construction process of the model \cite{tenney-etal-2019-bert, voita-etal-2019-bottom, geva-etal-2022-transformer}, or the features stored in its hidden representations \cite{adi2017finegrained, conneau-etal-2018-cram,liu-etal-2019-linguistic}. Our work is different as it converts intermediate representations into a final-layer form, which is interpretable by design.

Previous works on early exiting cut the computation 
% Our work is also related to a line of works on early exit, where computation is cut 
at a dynamically-decided earlier stage 
\cite{schwartz-etal-2020-right,xin-etal-2020-deebert,schuster2022confident,gera-etal-2023-benefits}, or a fixed network is utilized to parallelize inference \citep{leviathan2022fast, chen2023accelerating}. However, these methods propagate intermediate representations directly, which we show is substantially worse than our approach. Also, our method requires training of considerably fewer parameters than methods such as \citet{schuster-etal-2021-consistent}, which learn a different output softmax for each layer.  

Last, skipping transformer layers and analyzing the linearity properties of transformer components have been discussed in prior works \cite{Zhao2021of,mickus-etal-2022-dissect,wang-etal-2022-skipbert,lamparth2023analyzing}.
Specifically, a concurrent work by \citet{belrose2023eliciting} proposed to train affine transformations from hidden to final representations to increase model transparency. Our work is different in that we train \textit{linear} transformations \textit{across all layers}. Moreover, while \citet{belrose2023eliciting} use SGD for training while minimizing KL divergence, we use linear regression, which requires much less compute. It will be valuable to compare the accuracy of both methods.
