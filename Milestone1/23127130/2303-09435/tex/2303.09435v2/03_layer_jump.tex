\section{Linear Shortcut Across Blocks}
\label{sec:layer_jump}

To use a hidden representation from layer $\ell<L$ as a final representation, we propose to cast it using linear regression, while skipping the computation in-between these layers. More generally, this approach can be applied to cast any $\ell$-th hidden representation to any subsequent layer $\ell'>\ell$.


\subsection{Method}
\label{subsec:methodology_linear_shortcut}

Given a source layer $\ell$ and a target layer $\ell'$ such that $0 \leq \ell < \ell' \leq L$, our goal is to learn a mapping
%$A_{\ell', \ell} \in \mathbb{R}^{d_h \times d_h}$
from hidden representations at layer $\ell$ to those at layer $\ell'$. To this end, we first collect a set of corresponding hidden representation pairs $(h^\ell, h^{\ell'})$. Concretely, we run a set $\mathcal{T}$ of input sequences through the model, and for each input $s$, we extract the hidden representations $h_{i_s}^{\ell}, h_{i_s}^{\ell'}$, where $i_s$ is a random position in $s$.
Next, we learn a matrix $A_{\ell', \ell} \in \mathbb{R}^{d_h \times d_h}$ by fitting linear regression over $\mathcal{T}$, i.e., $A_{\ell', \ell}$ is a numerical minimizer for:
$$ A \mapsto \sum_{s \in \mathcal{T}} || A \cdot h_{i_s}^\ell - h_{i_s}^{\ell'} ||^2,$$ 
and define the mapping of a representation $h$ from layer $\ell$ to layer $\ell'$ as:
\begin{equation}
\label{eq:linear_jump}
    \matl{} (h) \coloneqq A_{\ell', \ell} \cdot h.
\end{equation}


\subsection{Baseline}
\label{subsec:baseline}

We evaluate 
% our method against 
the prevalent approach of ``reading'' hidden representations directly, without any transformation. 
Namely, the propagation of a hidden representation from layer $\ell$ to layer $\ell'$ is given by the identity function, dubbed \id{}:

$$ \idl{} (h) \coloneqq h.$$

% Notably, 
This baseline 
assumes that representations at different layers operate in the same linear space.

\subsection{Quality of Fit}
\label{subsec:experiments_r2}

We first evaluate our method by measuring how well the learned linear mappings approximate the representations at the target layer. To this end, we calculate the (coordinate-averaged) $r^2$-score of our mapping's outputs with respect to the representations obtained from a full inference pass, and compare to the same for the \id{} baseline.


\paragraph{Models.}

We use \gpt{} \cite{radford2019language}, a decoder-only auto-regressive LM, with $L = 48$, $d_h = 1600$, and \bert{} \cite{devlin-etal-2019-bert}, an encoder-only model trained with masked language modeling, with $L=24$, $d_h=1024$.
% \footnote{\label{footnote:hf}We use models and data from Huggingface \cite{wolf-etal-2020-transformers,lhoest-etal-2021-datasets}.}
%For masked token prediction, we use a masked LM head pre-trained for our \bert{} model.

% \footnote{Specifically, we use the Huggingface Transformers \cite{wolf-etal-2020-transformers} implementations of all these models.}

%\sy{We use \gpt{} \cite{radford2019language}, a decoder-only auto-regressive LM, coming in four scales; $\texttt{gpt2}$ ($L = 12$, $d_h = 768$), $\texttt{gpt2-medium}$ ($L = 24$, $d_h = 1024$), $\texttt{gpt2-large}$ ($L = 36$, $d_h = 1280$) and $\texttt{gpt2-xl}$ ($L = 48$, $d_h = 1600$). Also, we use \bert{} \cite{devlin-etal-2019-bert}, an encoder-only model trained with masked language modeling, coming in two scales;  \texttt{bert-base-uncased} ($L=12$, $d_h=768$) and \texttt{bert-large-uncased} ($L=24$, $d_h=1024$). For masked token prediction, we use masked LM heads pre-trained for our models. Specifically, we use the Huggingface Transformers \cite{wolf-etal-2020-transformers} implementations of all these models. The plots presented in this section are for $48$-layered \gpt{} and $24$-layered \bert{}.}

%\sy{We use \gpt{} \cite{radford2019language}, a decoder-only auto-regressive LM, in the Huggingface \cite{wolf-etal-2020-transformers} implementation\footnote{\url{https://huggingface.co/gpt2}}, coming in four scales; $\texttt{gpt2}$ ($L = 12$, $d_h = 768$), $\texttt{gpt2-medium}$ ($L = 24$, $d_h = 1024$), $\texttt{gpt2-large}$ ($L = 36$, $d_h = 1280$) and $\texttt{gpt2-xl}$ ($L = 48$, $d_h = 1600$). Also, we use \bert{} \cite{devlin-etal-2019-bert}, an encoder-only model trained with masked language modeling, in the Hugginface implementation, coming in two scales;  \texttt{bert-base-uncased}\footnote{\url{https://huggingface.co/bert-base-uncased}} ($L=12$, $d_h=768$) and \texttt{bert-large-uncased}\footnote{\url{https://huggingface.co/bert-large-uncased}} ($L=24$, $d_h=1024$). For masked token prediction, we use the \texttt{BertForMaskedLM} heads from Huggingface, pretrained for these models. The plots presented in this section are for $48$-layered \gpt{} and $24$-layered \bert{}.}

\paragraph{Data.}
We sample random sentences from Wikipedia,
% \footref{footnote:hf} 
collecting 9,000 (resp. 3,000) sentences for the training set $\mathcal{T}$ (resp. validation set $\mathcal{V}$).\footnote{We use sentences rather than full documents to simplify the analysis.}
%\sy{We use two data sources to evaluate our method. One is Wikiepdia \cite{lhoest-etal-2021-datasets}\footnote{\url{https://huggingface.co/datasets/wikipedia}}; we use \texttt{spaCy}\footnote{\url{https://spacy.io/}} to divide documents into sentences\footnote{We use sentences rather than full documents to simplify the analysis.}\footnote{We pick randomly a Wikipedia document and then pick randomly a sentence ending in a newline character in it. \sy{[maybe this footnote is not needed?]}}, collecting 9,000 (resp. 3,000) random sentences for the training set $\mathcal{T}$ (resp. validation set $\mathcal{V}$). The second is a news article sentences dataset, the 10K English 2020 news sentences corpus
% \footnote{\url{https://downloads.wortschatz-leipzig.de/corpora/eng_news_2020_10K.tar.gz}} from the Leipzig Corpora Collection \cite{goldhahn-etal-2012-building}, which we randomly divide into a training set $\mathcal{T}$ consisting of 9,000 examples and a validation set $\mathcal{V}$ consisting of 1,000 examples.
% We truncate sentences to the maximal token length allowed by the model \mg{do we ever need to truncate? a sentence has about 10 words and the max. input len is thousands} \sy{[I surely did not need to in Leipzig, but discovered (via a transformers runtime warning) that I do need to for some (probably a minority) of the Wikipedia sentences. This probably has to do with that it is not really ``sentences" necessarily, for example, I noticed that it has some listings or something like that (bulleted items)... So some minority might get very long I guess...]}.
For each example $s$, we select a random position $i_s$ and extract the hidden representations $h_{i_s}^{\ell}$ at that position from all the layers.
For \bert{}, we first replace the input token at position $i_s$ with a \mask{} token, as our motivation is interpreting predictions, which are obtained via masked tokens in \bert{} (see \S\ref{subsec:BERT}).
Thus, in this case, the hidden representations we consider
%in the case of \bert{}
are of \mask{} tokens only.
%As we observed highly similar results for the two data sources across all our experiments, throughout the paper we will mainly report results for Wikipedia (except for \S\ref{sec:robustness}, where we cross-validate).


\begin{figure}[t]
\includegraphics[scale=0.2]{figs/r2_scores_48.pdf}
% \includegraphics[width=\columnwidth]{figs/r2_scores_48.pdf}
\caption{The coordinate-averaged $r^2$-score of $\matl{}$ (left) and $\idl{}$ (right) (\gpt{}).}
\label{fig:r2_scores}
\end{figure}


\begin{figure}[t]
\setlength{\belowcaptionskip}{-10pt}
\includegraphics[scale=0.2]{figs/bertmask_r2_scores_24.pdf}
% \includegraphics[width=\columnwidth]{figs/bertmask_r2_scores_24.pdf}
\caption{The coordinate-averaged $r^2$-score of $\matl{}$ (left) and $\idl{}$ (right) (\bert{}).}
\label{fig:bertmask_r2_scores}
\end{figure}



\paragraph{Evaluation.}
For every pair of layers $\ell, \ell'$, such that $0 \leq \ell < \ell' \leq L$, we use the training set $\mathcal{T}$ to fit linear regression as described in \S\ref{subsec:methodology_linear_shortcut}, and obtain a mapping $\matl{}$. 
Next, we evaluate the quality of $\matl{}$ as well as of $\idl{}$ using the $r^2$-coefficient, uniformly averaged over all coordinates. Concretely, we compute the $r^2$-coefficient of each of the predicted representations $\matl{} (h_{i_s}^{\ell})$ and $\idl{} (h_{i_s}^{\ell})$ versus the true representations $h_{i_s}^{\ell'}$
over all $s \in \mathcal{V}$.
%as we vary $s \in \mathcal{V}$.
%for every $s \in \mathcal{V}$.



\paragraph{Results.}
Results for \gpt{} and \bert{} are presented in Figs.~\ref{fig:r2_scores} and~\ref{fig:bertmask_r2_scores}, respectively.
In both models, \mat{} consistently yields better approximations than \id{}, as it obtains higher $r^2$-scores (in blue) across the network. 
This gap between \mat{} and \id{} is especially evident in \bert{}, where \id{} completely fails to map the representations between most layers, suggesting that hidden representations are modified  substantially by every transformer block.
Overall, this highlights the shortcoming of existing practices to inspect representations in the same linear space, and the gains from using our method to approximate future layers.
% in the network.