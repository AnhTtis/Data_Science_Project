
\section{Conclusion and Future Work}

We present a simple and effective method for enhancing utilization of hidden representations in transformer-based LMs, that uses 
pre-fitted context-free and token-uniform linear mappings.
Through a series of experiments on different data sources, model architectures and scales, we show that our method consistently outperforms the prevalent practice of interpreting representations in the final-layer space of the model, yielding better approximations of succeeding representations and the predictions they induce, thus allowing a more faithful interpretation of the model's prediction-formation.
We demonstrate the practicality of our method for improving computation efficiency, saving a substantial amount of compute on top of prominent early exiting approaches. 
Also, by extending our method to sub-modules, 
% more specifically the attention sub-modules, 
we observe that replacing a part of the transformer inference by a non-contextual linear computation often results in a small deterioration of the prediction.
This opens new research directions for improving model efficiency,
% and parallelizability.
% including breaking the computation into several parallelizable tasks.
including breaking the computation into parallel tasks.