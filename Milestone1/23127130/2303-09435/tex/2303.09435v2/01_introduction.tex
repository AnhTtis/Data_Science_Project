
\section{Introduction}
\label{sec:introduction}

Transformer-based language models (LMs) process an input sequence of tokens by first representing it as a sequence of vectors and then repeatedly transforming it through a fixed number of attention and feed-forward network (FFN) layers 
\cite{NIPS2017_3f5ee243}. While each transformation creates new representations, only the final representations are used to obtain model predictions. Correspondingly, LM loss minimization directly optimizes the final representations, while hidden representations are only optimized implicitly, thus making their interpretation and usefulness more obscure.


\begin{figure}
\setlength{\belowcaptionskip}{-10pt}
\centering
\includegraphics[scale=0.5]{figs/main.pdf}
\caption{An illustration of our approach to enhance interpretability and utilization of hidden representations.  
% short-cutting transformer inference in-between certain layers 
We use linear mappings $A = A_{\ell',\ell}$ to short-cut transformer inference in-between layers ($\matl{}$), instead of the prevalent baseline of propagating the hidden representation as-is to the further layer ($\idl{}$).}
\label{fig:main}
\end{figure}

However, utilizing hidden representations is highly desirable; a successful interpretation of them can shed light on the ``decision-making process'' in the course of transformer inference \cite{tenney-etal-2019-bert, voita-etal-2019-bottom, slobodkin-etal-2021-mediators,geva-etal-2022-transformer}, and obtaining predictions from them can substantially reduce computational cost \cite{schwartz-etal-2020-right,xu2021survey}.

Previous attempts to harness hidden representations viewed the hidden representations of an input token as a sequence of approximations of its final representation \cite{Elhage, geva-etal-2022-transformer}. This view is motivated by the additive updates induced via the residual connections \cite{He} around each layer in the network. Indeed, previous works \cite{geva-etal-2021-transformer, geva-etal-2022-lm, ram2022token, alammar-2021-ecco} followed a simplifying assumption that representations \textit{at any layer} can be transformed into a distribution over the output vocabulary via projection to the output embeddings.
While this approach has proven to be surprisingly effective for interpretability \cite{geva-etal-2022-lm, dar2022analyzing} and computation efficiency \cite{schuster2022confident, xin-etal-2020-deebert, schwartz-etal-2020-right}, it oversimplifies the model's computation and assumes that all the layers in the network operate in the same space.


A natural question that arises is whether there is a more accurate way to cast hidden representations into final representation substitutes than interpreting them as they are. 
In this work, we tackle this question by learning linear transformations across layers in the network (illustrated in Fig.~\ref{fig:main}). For any two layers $\ell < \ell'$, we fit a linear regression to transform hidden representations from layer $\ell$ to layer $\ell'$. 
We show that this method, denoted as \mat{}, produces substantially more accurate approximations than the above-discussed identity mapping, dubbed \id{}, applied in previous works (\S\ref{sec:layer_jump}).
As \mat{} is a non-contextual mapping that operates on single hidden representations, 
this suggests that there is more linearity to transformer inference than could be estimated by the \id{} mapping.

Next, we test if these gains in approximating future representations also translate to better prediction estimations (\S\ref{sec:prediction}).
To this end, we
% We further test our approach in the context of language modeling (\S\ref{sec:prediction}), 
measure 
how often language modeling predictions from final representation substitutes produced by \mat{}, and by alternations between \mat{} and regular inference, agree with those of actual final representations. 
Through experiments with two data sources and various scales of \gpt{} \cite{radford2019language} and \bert{} \cite{devlin-etal-2019-bert}, we observe large accuracy gains ($15\%$-$40\%$ at most layers) in prediction estimation by \mat{} over naive projections (\id{}).
Moreover, we show that our mappings generalize well across different data distributions (\S\ref{sec:robustness}).
% Moreover, we observe that our mappings often 
% (in $>30\%$ of the cases) 
% produce correct predictions when applied to the very early layers in the network.
% Moreover, through
% evaluation on various scales
% and transfer learning experiments (\S\ref{sec:robustness}), we show the generality and utility of our approach to different model scales, architectures, and data distributions.
% \sy{Next (\S\ref{sec:robustness}) we validate the robustness of our method by varying model size and training data source.}

We leverage these findings for enhancing model efficiency and demonstrate our method's utility in the setting of early exiting -- a strategy for dynamically deciding at which layer to stop the inference pass and use that layer's representation for prediction.
% as a final layer representation substitute. 
While previous works have utilized these hidden representations intact (i.e. using \id{}), we transform them using \mat{}, showing that our method performs better than the baseline in this setting as well (\S\ref{sec:applications}), allowing for the saving of additional $7.9\%$ (resp. $5.4\%$) of the layers for \gpt{} (resp. \bert{}) when aiming at $95\%$ accuracy.

Last, we analyze how well the different sub-modules of transformer computation -- attention, FFN, and layer normalization -- can be estimated linearly (\S\ref{sec:submodules}), by applying the same methodology of linear mappings.
% to replace the attention, FFN, and layer normalization with linear mappings. 
We find that linearly approximating attention, the only sub-module that has contextual processing, results in the least reduction of precision. This hints at an interesting possibility of compute time reduction, as non-contextual inference is parallelizable.

To conclude, we propose a method for casting hidden representations across transformer layers, that is light to train, cheap to infer, and provides more accurate and robust representation approximations than the commonly-used
% hitherto implicitly accepted 
baseline of identical propagation.
Beyond interpretability, our method holds potential for enhancing efficiency.
% also has concrete applications for efficiency.
% Our method is not only appealing for model analysis, but 