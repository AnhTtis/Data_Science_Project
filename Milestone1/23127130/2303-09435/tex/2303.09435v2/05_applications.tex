\section{Implication to Early Exiting}
\label{sec:applications}

%The fact that it is often possible to approximate
The possibility of approximating
the final prediction already in the early layers has important implications for efficiency; applying our linear mapping instead of executing transformer blocks of quadratic time complexity, could save a substantial portion of the computation. In this section, we demonstrate this in the context of early exiting.

When 
% performing transformer model inference under 
using an early exit strategy \cite{schwartz-etal-2020-right, xin-etal-2020-deebert, schuster2022confident}, one aims at deciding dynamically at which layer to stop the computation and ``read'' the prediction from the hidden representation of that layer.
More precisely, under a confidence measure paradigm, one decides to stop the computation for a position $i$ at layer $\ell$ based on a confidence criterion, that is derived from casting the hidden representation $h_i^\ell$ as a final-layer representation and converting it to an output probability distribution. Specifically, following \citet{schuster2022confident}, a decision to exit is made if the difference between the highest and the second highest probabilities is bigger than $$ 0.9 \cdot \lambda + 0.1 \cdot {\rm exp} (-4 i / N),$$
where $N$ is the average length of the input until position $i_s$ for $s \in \mathcal{V}$, and $\lambda$ is a hyper-parameter.

\begin{figure}[t]
\setlength{\belowcaptionskip}{-10pt}
\centering
\includegraphics[width=\columnwidth]{figs/ee_gpt2bert.pdf}
\caption{Precision@$1$ with early exit and ``fixed exit'', applied to the $24$-layer \gpt{} for next token prediction (left) and the $24$-layer \bert{} for masked token prediction (right). Varying the confidence parameter $\lambda$, the $x$-coordinate is the average number of layers processed before an early exit decision is reached.}
\label{fig:ee_gpt2bert}
\end{figure}

\quash{
\begin{figure}[t]
\setlength{\belowcaptionskip}{-10pt}
\centering
\includegraphics[scale=0.35]{figs/ee_pre1_24.pdf}
\caption{Precision@$1$ for the various early exit methods, and previous ``fixed exit'' methods for comparison ($24$-layer \gpt{} next token prediction task). Varying the confidence parameter $\lambda$, the $x$-coordinate is the average number of layers processed before an early exit decision is reached.}
\label{fig:ee_pre1}
\end{figure}
}

\paragraph{Experiment.}
We assess the utility of our mapping $\matlL{}$ for early exit as a plug-and-play replacement for $\idlL{}$, through which intermediate representations are cast into final-layer representations.
We use \gpt{} for the next token prediction and \bert{} for masked token prediction (both with 24 layers).
We run each of the models over the validation set examples, while varying the confidence parameter $\lambda$ and using either $\idlL{}$ or $\matlL{}$ for casting intermediate representations.
Furthermore, we compare these early exit variants to the ``fixed exit'' strategy from \S\ref{sec:prediction}, where the computation is stopped after a pre-defined number of layers rather than relying on a dynamic decision.
We evaluate each variant in terms of both prediction's accuracy, using the Precision@$1$ metric (see \S\ref{sec:prediction}), and efficiency, measured as the average number of transformer layers processed during inference.


\paragraph{Results.}
%Figs.~\ref{fig:ee_pre1} and~\ref{fig:bertmask_ee_pre1}
Fig.~\ref{fig:ee_gpt2bert}
plots the average Precision@$1$ score against the average number of layers processed, for $24$-layer \gpt{} and $24$-layer \bert{}. For both models, under an early exit strategy our mapping \mat{} again provides a substantial improvement over \id{}.
For example, aiming at $95\%$ average precision, \mat{} saves $\sim3.3$ ($13.8$\%) layers in \gpt{} compared to only $\sim1.4$ ($5.9$\%) layers by \id{}, and $\sim4.8$ ($20$\%) layers in \bert{} versus $\sim3.5$ ($14.6$\%) layers by \id{}.
These results highlight the potential gains prominent early exit methods can obtain by using our method.
Notably, in both models and for each of the mapping methods, early exit obtains better results than fixed layer exit, as expected. 

\quash{
\begin{figure}[t]
\setlength{\belowcaptionskip}{-10pt}
\centering
\includegraphics[scale=0.35]{figs/bertmask_ee_pre1_24.pdf}
\caption{Precision@$1$ for the various early exit methods, and previous ``fixed exit'' methods for comparison ($24$-layer \bert{} masked token prediction task). Varying the confidence parameter $\lambda$, the $x$-coordinate is the average number of layers processed before an early exit decision is reached.}
\label{fig:bertmask_ee_pre1}
\end{figure}
}