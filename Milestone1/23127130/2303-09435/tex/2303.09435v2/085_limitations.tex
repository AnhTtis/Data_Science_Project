\section{Limitations}

First, while it is possible to define many different mappings in-between layers, for example, affine or non-linear transformations, we focus on the ``simple'' case of linear transformations.
Our choice is motivated by the wide success of the simplest mapping (i.e. the identity baseline, of inspecting hidden representations in the same linear space), while we are asking if there is more linearity in transformer inference that can be exploited for interpretability.
% the deep linear structures resulting from the residual connections in transformers, which naturally raises the question of what extent these structures are being exploited during inference.

Second, we find that there is more linear structure to parts of the transformer computation (both full layers and sub-modules) than could be explained solely by the residual connection. However, we do not elucidate a reason for that, leaving exploration of this interesting research question for future work.

Third, our experiments focus on post-hoc interpretability, that is, analyzing a trained model without changing its weights. Future work should also consider analyzing the utility of such linear mappings when those are integrated into the model training.
% and evaluate the effect of this on utilization of hidden representations in transformers.

Last, in our experiments we use only data in English. Nonetheless, given the comprehensiveness of our experiments and the fact that our method does not rely on any language-specific features, we would expect our findings to hold in other languages as well.

% Although we see in this work that there is more linear structure to transformer inference than could be explained solely by the residual connection, we do not elucidate a reason for that. Also, for simplicity, we focus on the case of linear transformations for the shortcuts. More generally, we do not try to formulate formal criteria according to which to judge, in principle, the quality of ways of short-cutting transformer inference in-between layers. In addition, our experiments cover only English data.
%Also, for simplicity, we do not study non-linear options for shortcut.