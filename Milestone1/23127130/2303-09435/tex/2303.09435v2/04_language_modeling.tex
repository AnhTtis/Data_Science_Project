\section{Linear Shortcut for Language Modeling}
\label{sec:prediction}

We saw that our method approximates future hidden representations substantially better than a naive propagation. 
In this section, we show that this improvement also translates to better predictive abilities from earlier layers. Concretely, we use our method to estimate the final prediction from intermediate representations, in the context of two fundamental LM tasks; next token prediction and masked token prediction.

\paragraph{Evaluation Metrics.}
Let $h, h' \in \mathbb{R}^{d_h}$ be a final representation and its  substitute obtained by some mapping, and denote by $\delta (h), \delta (h') \in \mathbb{R}^{d_v}$ their corresponding output probability distributions
%(obtained through projection to the output vocabulary -- see details below).
(see details below).
We measure the prediction quality of $h'$ with respect to $h$ using two metrics:
\begin{itemize}
[leftmargin=*,topsep=2pt,parsep=1pt]
    \item \textbf{Precision@$k$} ($\uparrow$ is better): This checks whether the token with the highest probability according to $\delta(h')$ appears in the top-$k$ tokens according to $\delta(h)$. Namely, we sort $\delta(h)$ and assign a score of $1$ if $\arg\max(\delta(h'))$ appears in the top-$k$ tokens by $\delta(h)$, and $0$ otherwise.
    
    \item \textbf{Surprisal} ($\downarrow$ is better): We measure the negative log likelihood according to $\delta(h)$, of the highest-probability token according to $\delta(h')$. Intuitively, low values mean that the model sees the substitute result as probable and hence not surprising.
\end{itemize}

\noindent We report the average Precision@$k$ and Surprisal over the validation set $\mathcal{V}$.



\subsection{Next Token Prediction}
\label{subsec:next_token_prediction_task}

Auto-regressive LMs output for every position a probability distribution over the vocabulary for the next token. Specifically, the output distribution for every position $i$ is given by $\delta (h_i^L)$, where
\begin{equation}\label{eq:output_distribution}
    \delta (h) = \texttt{softmax} ( E^\top \cdot h) \in \mathbb{R}^{d_v}.
\end{equation}
For some LMs, including \gpt{}, a layer normalization $\texttt{ln\_f}$ is applied to the final layer representation before this conversion (i.e., computing $\delta (\texttt{ln\_f}(h))$ rather than $\delta (h)$).

Recall that our goal is to measure how well this distribution can be estimated from intermediate representations, i.e. estimating $\delta (h_i^L)$ from
$h_i^{\ell}$
%$\delta (h_i^\ell)$
where $\ell<L$. 
Thus, we first run the validation set examples through the model, while extracting for each example $s$ and every layer the hidden representation at a random position $i_s$. Next, we apply our mappings $\matlL{}$ and $\idlL{}$ to cast the hidden representations of every layer $\ell$ to final layer substitutes (see \S\ref{sec:layer_jump}). 
Last, we convert every final-layer substitute to an output distribution (Eq.~\ref{eq:output_distribution}) and compute for each layer the average Precision@$k$ (for $k=1,5,10$) and Surprisal scores with respect to the final output distribution, over the validation set.

\begin{figure}[t]
\setlength{\belowcaptionskip}{-10pt}
\centering
%\includegraphics[scale=0.26]
\includegraphics[width=\columnwidth]{figs/presurp_48.pdf}
\caption{Precision@$k$ ($k = 1,5, 10$) and Surprisal for $\matlL{}$ and $\idlL{}$ (\gpt{} next token prediction task). 95\% confidence intervals are shown for Surprisal.}
\label{fig:presurp}
\end{figure}

\quash{
\begin{figure}[t]
\centering
\includegraphics[scale=0.4]{figs/pre_48.pdf}
\caption{Precision@$k$ ($k = 1,5, 10$) for $\matlL{}$ and $\idlL{}$ (\gpt{} next token prediction task).}
\label{fig:pre}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]{figs/surp_48.pdf}
\caption{Surprisal for $\matlL$ and the baseline $\idlL{}$ (\gpt{} next token prediction task). A 95\% confidence interval surrounds the lines.}
\label{fig:surp}
\end{figure}
}

\paragraph{Results.}
Fig.~\ref{fig:presurp} shows the average Precision@$k$ and Surprisal scores per layer in \gpt{}. Across all layers, \mat{} outperforms \id{} in terms of both scores, often by a large margin (e.g. till layer $44$ the Precision@$1$ achieved by \mat{} is bigger than that of $\id{}$ by more than $20\%$). 
This shows that linear mappings enable not just better estimation of final layer representations, but also of the predictions they induce. Moreover, the relatively high Precision@$k$ scores of \mat{} in early layers ($62\%$-$82\%$ for $k=10$, $52\%$-$74\%$ for $k=5$, and $28\%$-$45\%$ for $k=1$) suggest that early representations often accurately approximate the final prediction. Also, the substantially lower Surprisal scores of \mat{} compared to \id{} imply that our method allows for a more representative reading into the layer-wise prediction-formation of the model than allowed via direct projection to the vocabulary.

\subsection{Masked Token Prediction}
\label{subsec:BERT}

We conduct the same experiment in \S\ref{subsec:next_token_prediction_task} for masked language modeling, where the model predicts a probability distribution for a masked token in the input. 
% rather than the token that follows the input. 
Unlike next token prediction, where the output distribution is computed from representations of varying input tokens, in masked token prediction the output is always obtained from representations of the same input token (i.e. \texttt{[MASK]}).

For this experiment, we use \bert{}, on top of which we use a pretrained masked language model head $\delta$; given a token sequence $s$, a \mask{} token inside it and its final representation $h$, $\delta (h) \in \mathbb{R}^{d_v}$
 is a probability distribution over tokens giving the model's assessment
 of the likelihood of tokens to be fitting in place of the \mask{} token in $s$.

\begin{figure}[t]
\setlength{\belowcaptionskip}{-10pt}
\centering
%\includegraphics[scale=0.28]
\includegraphics[width=\columnwidth]{figs/bertmask_presurp_24.pdf}
\caption{Precision@$k$ ($k = 1,5, 10$) and Surprisal for $\matlL{}$ and $\idlL{}$ (\bert{} masked token prediction task). 95\% confidence intervals are shown for Surprisal.}
\label{fig:bertmask_presurp}
\end{figure}

\quash{
\begin{figure}[t]
\centering
\includegraphics[scale=0.4]{figs/bertmask_pre_24.pdf}
\caption{Precision@$k$ ($k = 1,5, 10$) for  $\matlL{}$ and the baseline $\idlL{}$ (\bert{} masked token prediction task).}
\label{fig:bertmask_pre}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]{figs/bertmask_surp_24.pdf}
\caption{Surprisal for $\matlL{}$ and the baseline $\idlL{}$ (\bert{} masked token prediction task). A 95\% confidence interval surrounds the lines.}
\label{fig:bertmask_surp}
\end{figure}
}

\paragraph{Results.}
Fig.~\ref{fig:bertmask_presurp} shows the average Precision@$k$ and Surprisal scores per layer in \bert{}, overall showing trends similar to those observed for next token prediction in \gpt{} (\S\ref{subsec:next_token_prediction_task}). This is despite the differences between the two tasks and the considerable architectural differences between the models.
Notably, the superiority of \mat{} over \id{} in this setting is even more prominent; 
while, in the first ten layers, \mat{}'s precision is between $8\%$-$52\%$  (Fig.~\ref{fig:bertmask_presurp}), \id{}'s precision for all values of $k$ is close to zero, again strongly indicating that our method allows for better reading into early layer hidden representations. 
More generally, \mat{} improves the Precision@$1$ of \id{} by more than $17\%$ at most layers, and unveils that a substantial amount of predictions ($>25\%$ starting from layer $3$) appear already in the very first layers.
Interestingly, the (rough) divide between the first last halves of layers for $\id{}$ in Fig.~\ref{fig:bertmask_presurp} seems to align with the two-hump shape of the blue region for $\mat{}$ in Fig.~\ref{fig:bertmask_r2_scores}.

\begin{table}
\setlength{\belowcaptionskip}{-10pt}
    \footnotesize
    \begin{tabularx}{\columnwidth}
    {p{1.3cm}lp{1.4cm}p{0.7cm}p{1.4cm}}
        $\texttt{id}_{4}$ & $\texttt{mat}_{4}$ & $\texttt{id}_{12}$ & $\texttt{mat}_{12}$ & $\texttt{id}_{24}$ \\ \midrule[1pt]
        %$\texttt{id}_{4 \rightarrow 24}$ & $\texttt{mat}_{4 \rightarrow 24}$ & $\texttt{id}_{12 \rightarrow 24}$ & $\texttt{mat}_{12 \rightarrow 24}$ & $\texttt{id}_{24 \rightarrow 24}$ \\ \midrule[1pt]
        \multicolumn{5}{l}{Input: \textit{aldridge had shoulder surgery in \mask{}.}} \\ \midrule
        fellowship & \tcbox{time} & cyclist & \tcbox{2009} & \tcbox{september} \\
        employment & \tcbox{it} & emergencies & \tcbox{2008} & \tcbox{november} \\
        agreement & her & seniors & \tcbox{2010} & \tcbox{december} \\
        \#\#ostal & them & cycling & \tcbox{2006} & \tcbox{august} \\
        \#\#com & work & \tcbox{pennsylvania} & \tcbox{2007} & \tcbox{july} \\ \midrule[1pt]
        \multicolumn{5}{p{7cm}}{Input: \textit{on your next view you will be asked to \mask{} continue reading.}} \\ \midrule
        \#\#com & be & be & be & \tcbox{please} \\
        accreditation & get & undergo & \tcbox{please} & \tcbox{simply} \\ 
        $\copyright$ & go & spartans & help & \tcbox{also} \\ 
        fellowship & \tcbox{help} & seniors & \tcbox{simply} & \tcbox{again} \\ 
        summer & have & * & say & \tcbox{immediately} \\ \bottomrule
    \end{tabularx}
    \caption{Examples of top-$5$ \bert{} masked token predictions at layers $4$, $12$ and $24$, under the mappings $\matlL{}$ (abbreviated $\mat{}_{\ell}$) and $\idlL{}$ (abbreviated $\id{}_{\ell}$). Plausible predictions (according to a human annotator) are marked in \tcbox{blue}. Note that for $\ell=L=24$, predictions of $\mat{}_{\ell}$ and $\id{}_{\ell}$ are the same.}
    \label{tab:manual}
\end{table}

\quash{
\begin{table}
\footnotesize
\centering
%\setlength\tabcolsep{0.5pt}
\begin{tabular}{ccccc} \toprule
%\begin{tabularx}{\columnwidth}{xxxxx} \toprule
$\texttt{id}_{4 \rightarrow 24}$ & $\texttt{mat}_{4 \rightarrow 24}$ & $\texttt{id}_{12 \rightarrow 24}$ & $\texttt{mat}_{12 \rightarrow 24}$ & $\texttt{id}_{24 \rightarrow 24}$ \\ \midrule[1pt]
\multicolumn{5}{|c|}{aldridge had shoulder surgery in \mask{}.} \\ \midrule
fellowship & \tcbox{time} & cyclist & \tcbox{2009} & \tcbox{september} \\
employment & \tcbox{it} & emergencies & \tcbox{2008} & \tcbox{november} \\
agreement & her & seniors & \tcbox{2010} & \tcbox{december} \\
\#\#ostal & them & cycling & \tcbox{2006} & \tcbox{august} \\
\#\#com & work & \tcbox{pennsylvania} & \tcbox{2007} & \tcbox{july} \\ \midrule[1pt]
\multicolumn{5}{|c|}{on your next view you will be asked to \mask{} continue reading.} \\ \midrule
\#\#com & be & be & be & \tcbox{please} \\
accreditation & get & undergo & \tcbox{please} & \tcbox{simply} \\ 
$	\copyright$ & go & spartans & help & \tcbox{also} \\ 
fellowship & \tcbox{help} & seniors & \tcbox{simply} & \tcbox{again} \\ 
summer & have & * & say & \tcbox{immediately} \\ \bottomrule
\end{tabular}
\caption{Two examples of top-$5$ \bert{} masked token predictions at layers $4$, $12$ and $24$, under the mappings $\matlL{}$ and $\idlL{}$. Plausible predictions (according to a human annotator) are marked in \tcbox{blue}. Note that at layer $24$ the predictions of $\matlL{}$ and $\idlL{}$ are the same (by definition).} 
\label{tab:manual}
\end{table}
}

\quash{
\begin{table*}
\footnotesize
\centering
\setlength\tabcolsep{2pt}
\setlength{\belowcaptionskip}{-8pt}
% \begin{tabular}{p{0.3\linewidth}ccccc}
\begin{tabular}{p{3cm}ccccc}
& $\texttt{id}_{4 \rightarrow 24}$ & $\texttt{mat}_{4 \rightarrow 24}$ & $\texttt{id}_{12 \rightarrow 24}$ & $\texttt{mat}_{12 \rightarrow 24}$ & $\texttt{id}_{24 \rightarrow 24}$ \\ \midrule
\multirow{5}{=}{aldridge had shoulder surgery in \mask{}.} & fellowship & \tcbox{time} & cyclist & \tcbox{2009} & \tcbox{september} \\
& employment & \tcbox{it} & emergencies & \tcbox{2008} & \tcbox{november} \\
& agreement & her & seniors & \tcbox{2010} & \tcbox{december} \\
& \#\#ostal & them & cycling & \tcbox{2006} & \tcbox{august} \\
& \#\#com & work & \tcbox{pennsylvania} & \tcbox{2007} & \tcbox{july} \\ \midrule
\multirow{5}{=}{on your next view you will be asked to \mask{} continue reading.} & \#\#com & be & be & be & \tcbox{please} \\
& accreditation & get & undergo & \tcbox{please} & \tcbox{simply} \\ 
& $	\copyright$ & go & spartans & help & \tcbox{also} \\ 
& fellowship & \tcbox{help} & seniors & \tcbox{simply} & \tcbox{again} \\ 
& summer & have & * & say & \tcbox{immediately} \\ \bottomrule
\end{tabular}
\caption{Examples of top-$5$ predictions at layers $4$, $12$ and $24$, under the mappings $\matlL{}$ and $\idlL{}$, for \bert{}. Grammatically plausible predictions (according to a human annotator) are marked in \tcbox{blue}. Note that at layer $24$ the predictions of $\matlL{}$ and $\idlL{}$ are the same (by definition).} 
\label{tab:manual}
\end{table*}
}



\paragraph{Analysis.}
We manually compare the predictions of our mapping $\matlL{}$ with $\idlL{}$, for the \bert{} model.  Concretely, we select $50$ random sentences from the Leipzig dataset (see \S\ref{subsec:robust_datasets}). Next, for each layer $\ell$, we manually analyze how many of the top-$5$ tokens according to $\matlL{}$ and $\idlL{}$ fit into context. We consider a token to fit into context if it is grammatically plausible within the sentence (see Tab.~\ref{tab:manual} for examples).
In the resulting $1,250$ instances (i.e. $50$ sentences $\times$ $25$ representations), we observe a substantially higher plausibility rate of $85.4\%$ for \mat{} compared to $52.8\%$ for \id{}. In fact, only in less than $4.3\%$ of the instances, there are more plausible tokens among the top-$5$ tokens according to \id{} than according to \mat{}, further supporting the Surprisal results above.
%In fact, only in less than $4.3\%$ of the instances there are more plausible tokens among the top-$5$ tokens according to \id{} than among the top-$5$ tokens according to \mat{}, further supporting the Surprisal results above.Å’


\subsection{Alternation Schemes}
\label{subsec:alternating}
Thus far, we considered direct mappings to the last layer. We now check if mappings between intermediate layers can 
% be leveraged to 
improve prediction estimations further. 
To this end, we obtain final-representation substitutes by alternating between transformer-inference and linear mappings.
% We also briefly experiment with final-representation substitutes gotten by alternating between transformer-inference and mapping-application; namely, 
For $\ell < \ell'$, let us abbreviate
$$ \bll{\ell}{\ell'} := \texttt{b}^{\ell'} \circ \cdots \circ \texttt{b}^{\ell + 2} \circ \texttt{b}^{\ell + 1},$$
i.e. $\bll{\ell}{\ell'}$ is the application of transformer inference from layer $\ell$ to layer $\ell'$.
For a sequence $0 = \ell_0 < \ell_1 < \ldots < \ell_n = L$, we consider either of \begin{equation}\label{eq:alt} \cdots \circ \bll{\ell_2}{\ell_3} \circ \matll{\ell_1}{\ell_2} \circ \bll{\ell_0}{\ell_1},\end{equation}\begin{equation}\label{eq:alt1} \cdots \circ \matll{\ell_2}{\ell_3} \circ \bll{\ell_1}{\ell_2} \circ \matll{\ell_0}{\ell_1}.\end{equation}
%where we have abbreviated $$ \bll{\ell}{\ell'} := \texttt{b}^{\ell'} \circ \cdots \circ \texttt{b}^{\ell + 2} \circ \texttt{b}^{\ell + 1}.$$
In other words, those are inference modes alternating between transformer inference and application of our linear mappings in a prescribed manner.
We then collect two sets of alternation schemes:
%We then employ two alternation strategies:
\begin{itemize}
[leftmargin=*,topsep=2pt,parsep=1pt]
    \item \textbf{$r^2$-informed (R2)}: We define the $r^2$-score of Eq.~\ref{eq:alt} (resp. Eq.~\ref{eq:alt1}) to be the product of the $r^2$-scores of  $\matll{\ell_i}{\ell_{i+1}}$ (computed in \S\ref{subsec:experiments_r2}), for $i = 1, 3, \ldots$ (resp. $i = 0, 2, \ldots$). For each $\ell$, we consider the scheme that employs $\ell$ transformer blocks with the maximal $r^2$-score.
    % , among the schemes employing $\ell$ transformer blocks, the one with the maximal $r^2$-score.
    
    \item \textbf{Weighted round-robin (WRR)}: For $a,b \ge 1$ such that $a+b$ divides $L$, we consider $(\ell_0, \ell_1, \ldots )$ given by $(0, a, a+b, 2a+b, 2a+2b, \ldots)$ and the two corresponding schemes Eq.~\ref{eq:alt},~\ref{eq:alt1}.
    In other words, here we alternate between performing $a$ transformer blocks and application of our linear mapping across $b$ layers, for some fixed values of $a$ and $b$.
\end{itemize}

\noindent For the experiment, we use \gpt{} with $24$ layers
% \footnote{We use this smaller model to save on compute.} 
(see \S\ref{subsec:robust_scale}) and measure Precision@$1$.


\begin{figure}[t]
\setlength{\belowcaptionskip}{-10pt}
\centering
\includegraphics[scale=0.35]{figs/alt_24.pdf}
\caption{Precision@$1$ for various alternation schemes and previous mappings for comparison ($24$-layer \gpt{} next token prediction task).}
\label{fig:alt_24}
\end{figure}


\paragraph{Results.}
Fig.~\ref{fig:alt_24} presents Precision@$1$ for various alternation schemes. We see, first of all, that some alternation schemes provide better precision than the previously considered $\matlL{}$. Second, the best-$r^2$-score tactic for choosing an alternation scheme seems to work well for the first half of layers, but under-achieves (relative to other possible alternation schemes) for the second half. It is, therefore, interesting to try to devise more clever tactics in the future; perhaps, for example, by weighting $r^2$-scores according to layer index.
