\section{Method Robustness}
\label{sec:robustness}

% We demonstrate the robustness of our method by testing it on different model sizes and on out-of-distribution samples.

% In this section, we validate our findings by replicating them across model sizes and testing on out-of-distribution sample distributions. Overall, the results prove to be robust. \sy{[this last comment (added by L. I think), seems to be out-of-style, as we don't seem to put conclusions in the introductory paragraphs of sections...]}

\subsection{Robustness Across Model Scales}
\label{subsec:robust_scale}

We repeat our experiments in \S\ref{sec:prediction}, with three additional scales of \gpt{} and one additional scale of \bert{}. Overall, the models are $\texttt{gpt2}$ ($L = 12$, $d_h = 768$), $\texttt{gpt2-medium}$ ($L = 24$, $d_h = 1024$), $\texttt{gpt2-large}$ ($L = 36$, $d_h = 1280$) and $\texttt{gpt2-xl}$ ($L = 48$, $d_h = 1600$), and \texttt{bert-base-uncased} ($L=12$, $d_h=768$) and \texttt{bert-large-uncased} ($L=24$, $d_h=1024$).

Fig.~\ref{fig:rob_presurp} (resp. Fig.~\ref{fig:rob_bert_presurp}) depicts the Precision@$1$ and Surprisal scores as functions of the relative depth of the model (i.e. $\ell / L$), for \gpt{} models (resp. \bert{} models).
%Figs.~\ref{fig:rob_pre},~\ref{fig:rob_surp} depict the Precision@$1$ and Surprisal scores as functions of the relative depth of the model (i.e. $\ell / L$).
% by letting the $x$-axis value be $\ell / L$, and in 
%Likewise, Fig.~\ref{fig:rob_bert_pre},~\ref{fig:rob_bert_surp} show the scores for \bert{} models.
% Fig.~\ref{fig:rob_bert_pre},~\ref{fig:rob_bert_surp} we do the same for the two \bert{} models.
The plots show the same trends observed in \S\ref{sec:prediction} across various model scales, with \mat{} exhibiting substantially higher predictive abilities from intermediate layers than \id{}.
% We see that the conclusions of \S\ref{sec:prediction} are left unchanged when varying model sizes.
Interestingly, there is a great overlap between \gpt{} scores of different scales, but not between the scores of \bert{} models.
%Hypothetically, the simple and ``universal'' nature of the \gpt{} objective, relative to that of \bert{}, creates such across-scale alignment. \mg{not sure that I understand this explanation}
% In addition, we observe that the \gpt{} plots across the various model sizes exhibit great overlap, which can not be said for the \bert{} plots. Hypothetically, the simple and ``universal'' nature of the \gpt{} objective, relative to that of \bert{}, creates such across-scale alignment.

%Hypothetically, this is due to the simple\lc{simple in what sense?} nature of the loss design of \gpt{}, compared to the rather complicated one of \bert{}\lc{so the losses are different, why does it tell us anything about models of different sizes?} \sy{[I agree, this is a very speculative take... We can remove it, I just would ``like'' it to be true - the gpt2 objective is much more ``clean'' and ``universal''... bert has two objectives, and both are kind of clumsy in a sense (masking is still fine maybe, but the next sentence thing is quite non-elementary). In other words, using ``information theory ideation'', to describe mathematically what gpt2 tries to do (predict a distribution over sentences) takes less bits...]}.\lc{I see, the last sentence is what got me to understand this. So if we leave the speculation in one form or another, I think we should mention the information theory ideation argument.}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figs/gpt2_presurp.pdf}
\caption{Precision@$1$ and Surprisal for $\matlL$ and $\idlL{}$, for next token prediction with \gpt{}. 95\% confidence intervals are shown for Surprisal.}
\label{fig:rob_presurp}
\end{figure}

\quash{
\begin{figure}[t]
\centering
\includegraphics[scale=0.35]{figs/gpt2_pre.pdf}
\caption{Precision@$1$ for $\matlL{}$ and $\idlL{}$ (\gpt{} next token prediction task, four model sizes).}
\label{fig:rob_pre}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]{figs/gpt2_surp.pdf}
\caption{Surprisal for $\matlL$ and $\idlL{}$ (\gpt{} next token prediction task, four model sizes). A 95\% confidence interval surrounds the lines.}
\label{fig:rob_surp}
\end{figure}
}

\begin{figure}[t]
\setlength{\belowcaptionskip}{-10pt}
\centering
\includegraphics[width=\columnwidth]{figs/bert_presurp.pdf}
\caption{Precision@$1$ and Surprisal for $\matlL$ and $\idlL{}$, for masked token prediction with \bert{}. 95\% confidence intervals are shown for Surprisal.}
\label{fig:rob_bert_presurp}
\end{figure}

\quash{
\begin{figure}[t]
\centering
\includegraphics[scale=0.35]{figs/bert_pre.pdf}
\caption{Precision@$1$ for $\matlL{}$ and $\idlL{}$ (\bert{} masked token prediction task, two model sizes).}
\label{fig:rob_bert_pre}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]{figs/bert_surp.pdf}
\caption{Surprisal for $\matlL$ and $\idlL{}$ (\bert{} masked token prediction task, two model sizes). A 95\% confidence interval surrounds the lines.}
\label{fig:rob_bert_surp}
\end{figure}
}

\subsection{Robustness Across Data Distributions}
\label{subsec:robust_datasets}

We test whether the linear mappings learned from one data distribution are useful for predictions on other data distributions.
To this end, we use a second dataset of news article sentences, the 10K English 2020 news sentences corpus
% \footnote{\url{https://downloads.wortschatz-leipzig.de/corpora/eng_news_2020_10K.tar.gz}} 
from the Leipzig Corpora Collection \cite{goldhahn-etal-2012-building}, which we randomly divide into a training set $\mathcal{T}$ consisting of 9,000 examples and a validation set $\mathcal{V}$ consisting of 1,000 examples. For our experiments, we use the $24$-layer \gpt{} and \bert{} models.
First, we replicate previous experiments on the Leipzig dataset, obtaining results that are extremely similar; for example, the average (across layers) difference between the Precision@$1$ score of Wikipedia and Leipzig is $0.3\%$ for \gpt{} and $-1.4\%$ for \bert{}.
Next, we use Leipzig (resp. Wikipedia) samples to fit linear mappings $\matlL{}$ (as described in \S\ref{sec:layer_jump}), and then evaluate these mappings in the context of next-token prediction, on samples from Wikipedia (resp. Leipzig) (as in \S\ref{sec:prediction}). When swapping the original mappings with those trained on the other dataset, we observe a decrease of $0.1\%$ (resp. increase of $1.1\%$) relative to the original Precision@$1$ scores for \bert{} and a decrease of $5.5\%$ (resp. decrease of $8\%$) relative to the original Precision@$1$ scores for \gpt{}, on average across layers.
Overall, this shows that our method generalizes well to out-of-distribution samples. Moreover, our linear mappings capture general, rather than domain specific, features of the model's inference pass.
% \sy{[is the usage of ``relative'' clear here? I mean that observation $b$ is relatively a $5\%$ decrease of observation $a$ if $(a-b)/a = 0.05$, \underline{not} if $a-b = 0.05$...]}
%Next, we use Wikipedia (resp. Leipzig) samples to fit linear mappings $\matlL{}$ \quash{for $\ell\in [0,L-1]$} (as described in \S\ref{sec:layer_jump}), and then evaluate these mappings in the context of next-token prediction, on samples from Leipzig (resp. Wikipedia) (as in \S\ref{sec:prediction}). For these experiments, we use the $24$-layer \gpt{} and \bert{} models.
%When swapping the original mappings with those trained on the other dataset, we observe only a small decrease in Precision@1 scores of $-6\%$ (resp. $-8\%$) on average across layers, \sy{[these percents are not the absolute precision@1 percents, but relative to the bigger score... Don't know if this is clear from the formulation as it is now...]} .
%Overall, this shows that our method generalizes well (albeit not perfectly) to out-of-distribution samples. Moreover, it suggests that our linear mappings capture general, rather than domain specific, features of the model's inference pass.

% We find that the relative change in Precision@1 scores, when swapping the original matrices with those trained on the other dataset, is non-positive for all $\ell$, with a mean decrease of $\sim -0.06$ ($\sim -0.08$). 

% We conduct experiments to study the out-of-distribution robustness of our method. Let us abbreviate $\omega := \textnormal{Wikipedia}$ and $\lambda := \textnormal{Leipzig}$. We use the $24$-layered \gpt{} model. For $\sharp, \flat \in \{ \omega, \lambda\}$, let us denote by $p^{\sharp}_{\flat} (\ell)$ the Precision@$1$ obtained using the mapping $\matlL{}$ when matrices are fitted using the dataset $\sharp$ and the validation set is that of dataset $\flat$. 
% We find that the relative change in accuracy when swapping matrices, $\tfrac{p^\lambda_\omega (\ell) - p^\omega_\omega (\ell)}{p^{\omega}_{\omega} (\ell)}$ (resp. $\tfrac{p^\omega_\lambda (\ell) - p^\lambda_\lambda (\ell)}{p^{\lambda}_{\lambda} (\ell)}$) is non-positive for all $\ell$, having mean $\sim -0.06$ (resp. $\sim -0.08$). Thus our method does show, albeit not perfect, out-of-distribution stability.

%We find that $p^\omega_\omega (\ell) - p^\lambda_\omega (\ell)$ (resp. $p^\lambda_\lambda (\ell) - p^\omega_\lambda (\ell)$) is non-negative for all $\ell$, achieving a maximum of $~4.5\%$ (resp. $~7.3\%$) for $\ell = 3$ (resp. $\ell = 16$) and having mean $~2.1\%$ (resp. $~3.2\%$). Thus, although not perfect, our method does show an out-of-distribution stability. \lc{I am trying to think, but somehow it doesn't convince me that it is not much. what is 3\% worse precision at 1? if it was 8\% p@1 to begin with it is a lot, right?} \sy{[maybe it is a lot, I don't know... But what do you mean that it was $8\%$ to begin with? our p@1 are kind of starting with $15\%$ and more, going quickly to over $40\%$... Or maybe we are talking about different numbers?]}\lc{I am saying that to even start convincing yourself that it is not much, you need the number you just mentioned. We need the numbers we cite here to tell us *by themselves* that the difference is small. One option would be something like: "For comparison when the difference between the two was just 2.1, the out-of-domain performed well by itself, and better than the in-domain id with XXX and YYY p@1}