\section{Linear Shortcut Across Sub-Modules}
\label{sec:submodules}

% Our experiments show that
% , despite the commonly-applied simplification by interpretability works, transformer layers do not operate in the same linear space and 
% there is a major gap in approximating future representations using an identity mapping (\S\ref{sec:layer_jump}, \S\ref{sec:prediction}).
% Here, 
In this section, we investigate whether discrepancies across layers result from specific sub-modules or are a general behaviour of all sub-modules in the network.  
This is done by extending our approach to test how well particular components in transformer blocks can be linearly approximated. 


\paragraph{Method.}

Consider \gpt{} for definiteness, then:
% we have 
$$ \texttt{b}_{\ell} = \texttt{b}_{\ell}^{\texttt{ffn}} \circ \texttt{b}_{\ell}^{\texttt{attn}}$$ 
% with
\begin{equation}\label{eq:attn} \texttt{b}^{\texttt{attn}}_{\ell} (H) = \texttt{attn}_{\ell} (\texttt{ln1}_{\ell} (H)) + H,\end{equation} 
where $\texttt{attn}_{\ell}$ is
%a multi-head self-attention
a MHSA
layer and \texttt{ln1} is a layer normalization (LN), and 
$$ \texttt{b}^{\texttt{ffn}}_{\ell} (H) = \texttt{ffn}_{\ell} (\texttt{ln2}_{\ell} (H)) + H,$$  
where $\texttt{ffn}_{\ell}$ is
%a feed-forward network
an FFN
layer and $\texttt{ln2}$ is a LN.
\quash{
Given a block $\texttt{b}_\ell$ and one of its sub-modules $\texttt{ln1}_\ell, \ \texttt{attn}_\ell, \ \texttt{ln2}_\ell$, or $\texttt{ffn}_\ell$, we fit linear regression approximating the output of the sub-module given its input and then use it in order to define mappings, as we now describe.
}
Given a block $\texttt{b}_\ell$ and one of its sub-modules $\texttt{ln1}_\ell, \ \texttt{attn}_\ell, \ \texttt{ln2}_\ell$, or $\texttt{ffn}_\ell$, we fit linear regression approximating the output of the sub-module given its input, and then use it to define mappings $\matattnl{}$, $\matlnl{}$ and $\matffl{}$.
%We provide the definition of $\matattnl{}$ below, and that of the other two in App. \ref{sec:app_submodule_skip_description}.
We provide the formal definitions of these mappings in App. \ref{sec:app_submodule_skip_description}.
\iffalse
\paragraph{$\matattnl{}$.}
%Illustrating this on $\texttt{attn}_\ell$ for definiteness,
For an input $s$, let $v^\ell_{i_s}$ be the vector at position $i_s$ in the output of $\texttt{attn}_\ell (\texttt{ln1}_\ell (H^{\ell - 1}))$. We denote by $A_\ell^{\texttt{attn}} \in \mathbb{R}^{d_h \times d_h}$ the matrix numerically minimizing 
$$ A \mapsto \sum_{s \in \mathcal{T}} || A \cdot \texttt{ln1}_\ell (h^{\ell-1}_{i_s}) - v^\ell_{i_s}||^2,$$
and define an attention sub-module replacement (Eq.~\ref{eq:attn}) by $$
\texttt{b}^{\overline{\texttt{attn}}}_\ell (h) \coloneqq A_{\ell}^{\texttt{attn}} \cdot \texttt{ln1}_\ell (h) + h. $$
We then define a mapping between two layers ${\ell \rightarrow \ell'}$ by:
$$ \matattnl{} (h) \coloneqq $$
$$ \texttt{b}^{\texttt{ffn}}_{\ell'} ( \texttt{b}^{\overline{\texttt{attn}}}_{\ell'} ( \ldots (\texttt{b}^{\texttt{ffn}}_{\ell+1} ( \texttt{b}^{\overline{\texttt{attn}}}_{\ell+1} (h)))\ldots)).$$ 
Namely, when applying each $\ell''$-th block, $\ell < \ell'' \leq \ell'$, we replace its attention sub-module $\texttt{attn}_{\ell''}$ by its linear approximation.
%In an analogous way, we consider the mappings $\matffl{}$ and $\matlnl{}$, where in the latter we perform the linear shortcut both for \texttt{ln1} and for \texttt{ln2} (see~\S\ref{sec:app_submodule_skip_description} for precise descriptions).
Importantly, unlike the original attention module, the approximation $\texttt{b}^{\overline{\texttt{attn}}}_\ell$ operates on each position independently, and therefore applying $\matattnl{}$ disables any contextualization between the layers $\ell$ and $\ell'$. Note that this is not the case for $\matffl{}$ and $\matlnl{}$, which retain the self-attention sub-modules and operate contextually.
\fi

\paragraph{Evaluation.}


We analyze the $24$-layered \gpt{}, and proceed completely analogously to \S\ref{subsec:next_token_prediction_task}, evaluating the Precision@$1$ and Surprisal metrics for the mappings $\matattnlL{}$, $\matfflL{}$ and $\matlnlL{}$.

\begin{figure}[t]
\setlength{\belowcaptionskip}{-0pt}
\centering
%\includegraphics[scale=0.2]
\includegraphics[width=\columnwidth]{figs/parts_presurp_24.pdf}
\caption{Precision@$1$ and Surprisal for the various sub-module linear mappings, and $\matlL{}$ for comparison ($24$-layer \gpt{} next token prediction task). A 95\% confidence interval surrounds the Surprisal lines.}
\label{fig:parts_presurp}
\end{figure}

\quash{
\begin{figure}[t]
\centering
\includegraphics[scale=0.4]{figs/parts_pre1_24.pdf}
\caption{Precision@$1$ for the various sub-module linear shortcut mappings, and the mapping $\matlL{}$ for comparison (\gpt{} next token prediction task).}
\label{fig:parts_pre1}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]{figs/parts_surp_24.pdf}
\caption{Surprisal for the various sub-module linear shortcut mappings, and the mapping $\matlL{}$ for comparison (\gpt{} next token prediction task). A 95\% confidence interval surrounds the lines.}
\label{fig:parts_surp}
\end{figure}
}

\paragraph{Results.}
Fig.~\ref{fig:parts_presurp} shows the average Precision@$1$ and Surprisal scores per layer.
From a certain layer (\textasciitilde$7$), all sub-module mappings achieve better results than the full-block mapping $\matlL{}$. Thus, it is not just the cumulative effect of all the sub-modules in the transformer block that is amenable to linear approximation, but also individual sub-modules can be linearly approximated. 
Furthermore, the linear approximation of attention sub-modules is less harmful than that of the FFN or LN sub-modules. 
% Hypothetically, 
A possible reason is that the linear replacement of FFN or LN ``erodes'' the self-attention computation after a few layers. 
Moreover, the good performance of $\matattnlL{}$ suggests that contextualization often exhausts itself in early layers; speculatively, it is only in more delicate cases that the self-attention of late layers adds important information. Last, remark the sharp ascent of the scores for layer normalization in layers $5$-$8$, for which we do not currently see a particular reason. To conclude, we see that the possibility of linear approximation permeates
%the various
transformer components.
