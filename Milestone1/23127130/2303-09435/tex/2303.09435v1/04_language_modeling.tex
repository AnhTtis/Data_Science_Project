\section{Linear Shortcut for Language Modeling}
\label{sec:prediction}

We saw that our method approximates future hidden representations substantially better than a naive propagation. 
In this section, we will show that this improvement also translates to better predictive abilities from earlier layers. Specifically, we will use our method to estimate how often intermediate representations encode the final prediction, in the context of two fundamental LM tasks; next token prediction and masked token prediction.

\paragraph{Evaluation Metrics.}
Let $h, h' \in \mathbb{R}^{d_h}$ be a final representation and a substitute final representation obtained by some mapping, and denote by $\delta (h), \delta (h') \in \mathbb{R}^{d_v}$ their corresponding output probability distributions (obtained through projection to the output vocabulary -- see details below). 
We measure the prediction quality of $h'$ with respect to $h$ using two metrics:
\begin{itemize}
[leftmargin=*,topsep=1pt,parsep=1pt]
    \item \textbf{Precision@$k$} ($\uparrow$ is better): This checks whether the token with the highest probability according to $\delta(h')$ appears in the top-$k$ tokens according to $\delta(h)$. Namely, we sort $\delta(h)$ and assign a score of $1$ if $\arg\max(\delta(h'))$ appears in the top-$k$ tokens by $\delta(h)$, and $0$ otherwise.
    
    \item \textbf{Surprisal} ($\downarrow$ is better): We measure the minus log-probability according to $\delta(h)$, of the highest-probability token according to $\delta(h')$. Intuitively, low values mean that the model sees the substitute result as probable and hence not surprising.
\end{itemize}

\noindent We report the average Precision@$k$ and Surprisal over the validation set $\mathcal{V}$.



\subsection{Next Token Prediction}
\label{subsec:next_token_prediction_task}

Auto-regressive LMs output for every position a probability distribution over the vocabulary for the next token. Specifically, the output distribution for every position $i$ is given by $\delta (h_i^L)$, where:
\begin{equation}\label{eq:output_distribution}
    \delta (h) = \texttt{softmax} ( E^\top \cdot h) \in \mathbb{R}^{d_v}
\end{equation}
For some LMs, including \gpt{}, a layer normalization $\texttt{ln\_f}$ is applied to the final layer representation before this conversion (i.e., computing $\delta (\texttt{ln\_f}(h))$ rather than $\delta (h)$).

Recall that our goal is to measure how well this distribution can be estimated from intermediate representations, i.e. estimating $\delta (h_i^L)$ from $\delta (h_i^\ell)$ where $\ell<L$. To this end, we first run examples from the validation set through the model, while extracting for each example $s$ the hidden representation of a random position $i_s$ at every layer. Next, we apply our mappings $\matlL{}$ and the $\idlL{}$ baseline to cast the hidden representations of every layer $\ell$ to final layer substitutes (see \S\ref{sec:layer_jump}). Last, for each layer, we convert its corresponding final-layer substitute to an output distribution (Eq.~\ref{eq:output_distribution}) and compute the average Precision@$k$ (for $k=1,5,10$) and Surprisal scores with respect to the final output distribution, over the validation set.

\paragraph{Results.}
Figs.~\ref{fig:pre} and~\ref{fig:surp} show the average Precision@$k$ and Surprisal scores per layer in $48$-layered \gpt{}, respectively (the plots for the other \gpt{} models are presented in \S\ref{sec:app_scale}). Across all layers, \mat{} outperforms \id{} in terms of both scores, often by a large margin (e.g. till layer $44$ the Precision@$1$ achieved by \mat{} is bigger than that of $\id{}$ by more than $0.2$). 
This shows that linear mappings enable not just better estimation of final layer representations, but also of the predictions they induce. Moreover, the relatively high Precision@$k$ scores of \mat{} in early layers ($0.62$-$0.82$ for $k=10$, $0.52$-$0.74$ for $k=5$, and $0.28$-$0.45$ for $k=1$) suggest that early representations already encode a good estimation of the final prediction. Also, the substantially lower Surprisal scores of \mat{} compared to \id{} imply that our method allows for a more representative reading into the layer-wise prediction-formation of the model than allowed through direct projection to the vocabulary.

\begin{figure}[t]
\centering
\includegraphics[scale=0.4]{figs/pre_48.pdf}
\caption{Precision@$k$ ($k = 1,5, 10$) of $\matlL{}$ and $\idlL{}$ for next token prediction in $48$-layered \gpt{}.}
\label{fig:pre}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]{figs/surp_48.pdf}
\caption{Surprisal for $\matlL$ and the baseline $\idlL{}$ ($48$-layered \gpt{} next token prediction task). A 95\% confidence interval surrounds the lines.}
\label{fig:surp}
\end{figure}

\subsection{Masked Token Prediction}
\label{subsec:BERT}

We now conduct the same experiment for the task of masked language modeling, where the model predicts a probability distribution of a masked token in the input rather than the token that follows the input. Unlike next token prediction, where the output distribution is computed from representations of varying input tokens, in masked token prediction the output is always obtained from representations of the same input token (i.e. \texttt{[MASK]}).

For this experiment, we use \bert{}, on top of which we use a pretrained masked language model head $\delta$; given a token sequence $s$, a \mask{} token inside it and its final representation $h$, $\delta (h) \in \mathbb{R}^{d_v}$
 is a probability distribution over tokens giving the model's assessment
 of the likelihood of tokens to be fitting in place of the \mask{} token in $s$.


\begin{figure}[t]
\centering
\includegraphics[scale=0.4]{figs/bertmask_pre_24.pdf}
\caption{Precision@$k$ ($k = 1,5, 10$) for  $\matlL{}$ and the baseline $\idlL{}$ ($24$-layered \bert{} masked token prediction task).}
\label{fig:bertmask_pre}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]{figs/bertmask_surp_24.pdf}
\caption{Surprisal for $\matlL{}$ and the baseline $\idlL{}$ ($24$-layered \bert{} masked token prediction task). A 95\% confidence interval surrounds the lines.}
\label{fig:bertmask_surp}
\end{figure}

\paragraph{Results.}
Figs.~\ref{fig:bertmask_pre} and~\ref{fig:bertmask_surp} present the average Precision@$k$ and Surprisal scores per layer in $24$-layered \bert{} (the plots for the $12$-layered \bert{} model are presented in \S\ref{sec:app_scale}), overall showing trends similar to those observed for next token prediction in \gpt{} (\S\ref{subsec:next_token_prediction_task}). This is despite the differences between the two tasks and the considerable architectural differences between \bert{} and \gpt{}.
Notably, the superiority of \mat{} over \id{} in this setting is even more prominent; 
while \mat{}'s precision is between $0.2-0.6$ in the first ten layers (Fig.~\ref{fig:bertmask_pre}), \id{}'s precision for all values of $k$ is close to zero, again strongly indicating that our method allows for better reading into early layer hidden representations. 
More generally, \mat{} improves the Precision@$1$ of \id{} by more than $17\%$ at most layers, and unveils that a substantial amount of predictions ($>25\%$ starting from layer $3$) appear already in the very first layers.
Interestingly, the (rough) divide between the first half of layers and last half of layers for $\id{}$ in Figs.~\ref{fig:bertmask_pre},~\ref{fig:bertmask_surp} seems to align with the two-hump shape of the blue region for $\mat{}$ in Fig.~\ref{fig:bertmask_r2_scores}.

\paragraph{Analysis.}
We manually compare the predictions of our mapping $\matlL{}$ with $\idlL{}$, for a $24$-layered \bert{} model.  Concretely, we select 50 random sentences from the Leipzig dataset. Next, for each layer $\ell$, we manually analyze how many of the top-$5$ tokens according to $\matlL{}$ and $\idlL{}$ fit into context. We consider a token to fit into context if it is grammatically plausible within the sentence (see Tab.~\ref{tab:manual} for concrete examples).
In the resulting $1250$ instances (i.e. $50$ sentences $\times$ $25$ representations), we observe a substantially higher plausibility rate of $85.36\%$ for \mat{} compared to $52.8\%$ for \id{}. In fact, only in less than $4.3\%$ of the instances there are more plausible tokens among the top-$5$ tokens according to \id{} than among the top-$5$ tokens according to \mat{}, further supporting the Surprisal results above.

\begin{table*}
\footnotesize
\setlength{\belowcaptionskip}{-15pt}
\begin{tabular}{p{0.3\linewidth}ccccc}
& $\texttt{id}_{4 \rightarrow 24}$ & $\texttt{mat}_{4 \rightarrow 24}$ & $\texttt{id}_{12 \rightarrow 24}$ & $\texttt{mat}_{12 \rightarrow 24}$ & $\texttt{id}_{24 \rightarrow 24}$ \\ \midrule
\multirow{5}{=}{aldridge had shoulder surgery in \mask{}.} & fellowship & \tcbox{time} & cyclist & \tcbox{2009} & \tcbox{september} \\
& employment & \tcbox{it} & emergencies & \tcbox{2008} & \tcbox{november} \\
& agreement & her & seniors & \tcbox{2010} & \tcbox{december} \\
& \#\#ostal & them & cycling & \tcbox{2006} & \tcbox{august} \\
& \#\#com & work & \tcbox{pennsylvania} & \tcbox{2007} & \tcbox{july} \\ \midrule
\multirow{5}{=}{on your next view you will be asked to \mask{} continue reading.} & \#\#com & be & be & be & \tcbox{please} \\
& accreditation & get & undergo & \tcbox{please} & \tcbox{simply} \\ 
& $	\copyright$ & go & spartans & help & \tcbox{also} \\ 
& fellowship & \tcbox{help} & seniors & \tcbox{simply} & \tcbox{again} \\ 
& summer & have & * & say & \tcbox{immediately} \\ \bottomrule
\end{tabular}
\caption{Examples of top-$5$ predictions at layers $4$, $12$ and $24$, under the mappings $\matlL{}$ and $\idlL{}$, for a $24$-layered \bert{} model. Grammatically plausible predictions (according to a human annotator) are marked in \tcbox{blue}. Note that at layer $24$ the predictions of $\matlL{}$ and $\idlL{}$ are the same (by definition).} 
\label{tab:manual}
\end{table*}
