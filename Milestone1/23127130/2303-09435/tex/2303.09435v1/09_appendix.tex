\appendix

\section{Experimental Setup Details}
\label{sec:exp_set}

Here we specify the experimental setup details.

\subsection{Models}
\label{subsec:exp_set_models}

\paragraph{\gpt{}. }
We use the four versions of the \gpt{} model\footnote{\url{https://huggingface.co/gpt2}} from Huggingface \cite{wolf-etal-2020-transformers}, having $L = 12, 24, 36, 48$ hidden layers and hidden dimensions $d_h = 768, 1024, 1280, 1600$ respectively.

\paragraph{\bert{}. }
We use the \texttt{bert-large-uncased} model\footnote{\url{https://huggingface.co/bert-large-uncased}} from Huggingface, having $L = 24$ hidden layers and hidden dimension $d_h = 1024$, and the \texttt{bert-base-uncased} model\footnote{\url{https://huggingface.co/bert-base-uncased}} from Huggingface, having $L = 12$ hidden layers and hidden dimension $d_h = 768$. We use the \texttt{BertForMaskedLM} heads from Huggingface, pretrained for these models.

\subsection{Data}
\label{subsec:exp_set_corpus}

When experimenting on the Wikipedia dataset, to generate an input sequence for our training and validation sets, we pick a random document from the Wikipedia dataset\footnote{\url{https://huggingface.co/datasets/wikipedia}} from Huggingface, use \texttt{spaCy}\footnote{\url{https://spacy.io/}} to break the document into sentences and pick a random sentence ending with a newline character among those.

When experimneting on the news article sentences dataset, we use the 10K English 2020 news sentences corpus\footnote{\url{https://downloads.wortschatz-leipzig.de/corpora/eng_news_2020_10K.tar.gz}} from the Leipzig Corpora Collection, which we randomly divide into a training set $\mathcal{T}$ consisting of 9,000 examples and a validation set $\mathcal{V}$ consisting of 1,000 examples.

In the \gpt{} (resp. \bert{}) experiment, a tokenized sentence with more than 1024 (resp. 512) tokens was truncated to have 1024 (resp. 512) tokens.

\subsection{Values of $\lambda$ Used in \S\ref{sec:applications}}
\label{subsec:exp_set_lambda}

In \S\ref{sec:applications}, to have a plot which is gradual enough, we use the following values of $\lambda$:
\begin{itemize}
    \item $\{-1.112\} \cup \{ 0.1 \cdot i \ : 0 \leq i \leq 10 \} \cup \{ 1.112 \}$.
    \item For the baseline method \id{}, to  the values above we also add $$ \{ 1 + 0.0112 \cdot i \ : \  0 < i < 10\}.$$
\end{itemize}

\section{Results on News Articles Data}
\label{sec:app_Leipzig}


Here we record the results of the main experiments when evaluated on the Leipzig Corpora news article sentences dataset, in Figs.~\ref{fig:L_r2_scores},~\ref{fig:L_bertmask_r2_scores} (quality of fit), Figs.~\ref{fig:L_pre}, \ref{fig:L_surp},~\ref{fig:L_bertmask_pre},~\ref{fig:L_bertmask_surp} (linear shortcut for language modeling), Figs.~\ref{fig:L_parts_pre1},~\ref{fig:L_parts_surp} (linear shortcut across sub-modules in blocks) and Figs.~\ref{fig:L_ee_pre1},~\ref{fig:L_bertmask_ee_pre1} (early exit).


\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figs/Leipzig/r2_scores_24.pdf}
\caption{The coordinate-averaged $r^2$-score of $\matl{}$ (left) and $\idl{}$ (right)  ($24$-layered \gpt{}) - Leipzig dataset.}
\label{fig:L_r2_scores}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figs/Leipzig/bertmask_r2_scores_24.pdf}
\caption{The coordinate-averaged $r^2$-score of $\mat{}_{\ell', \ell}$ (left) and $\idl{}$ (right) ($24$-layered \bert{} masked inputs) - Leipzig dataset.}
\label{fig:L_bertmask_r2_scores}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[scale=0.35]
{figs/Leipzig/pre_24.pdf}
\caption{Precision@$k$ ($k = 1,5, 10$) for $\matlL{}$ and the baseline $\idlL{}$ ($24$-layered \gpt{} next token prediction task) - Leipzig dataset.}
\label{fig:L_pre}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]
{figs/Leipzig/surp_24.pdf}
\caption{Surprisal for  $\matlL{}$ and the baseline $\idlL{}$ ($24$-layered \gpt{} next token prediction task) - Leipzig dataset.}
\label{fig:L_surp}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]
{figs/Leipzig/bertmask_pre_24.pdf}
\caption{Precision@$k$ for $\matlL{}$ and the baseline $\idlL{}$ ($24$-layered \bert{} masked token prediction task) - Leipzig dataset.}
\label{fig:L_bertmask_pre}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]
{figs/Leipzig/bertmask_surp_24.pdf}
\caption{Surprisal for $\matlL{}$ and the baseline $\idlL{}$ ($24$-layered \bert{} masked token prediction task) - Leipzig dataset.}
\label{fig:L_bertmask_surp}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]
{figs/Leipzig/parts_pre1_24.pdf}
\caption{Precision@$1$ for the various sub-module linear shortcut mappings, and the mapping $\matlL{}$ for comparison ($24$-layered \gpt{} next token prediction task) - Leipzig dataset.}
\label{fig:L_parts_pre1}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]
{figs/Leipzig/parts_surp_24.pdf}
\caption{Surprisal for the various sub-module linear shortcut mappings, and the mapping $\matlL{}$ for comparison ($24$-layered \gpt{} next token prediction task) - Leipzig dataset.}
\label{fig:L_parts_surp}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]
{figs/Leipzig/ee_pre1_24.pdf}
\caption{Precision@$1$ for the various early exit methods, and previous fixed exit methods for comparison ($24$-layered \gpt{} next token prediction task) - Leipzig dataset. Varying the confidence parameter $\lambda$, the $x$-coordinate is the average number of layers processed before an early exit decision is reached.}
\label{fig:L_ee_pre1}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]
{figs/Leipzig/bertmask_ee_pre1_24.pdf}
\caption{Precision@$1$ for the various early exit methods, and previous fixed exit methods for comparison ($24$-layered \bert{} masked token prediction task) - Leipzig dataset. Varying the confidence parameter $\lambda$, the $x$-coordinate is the average number of layers processed before an early exit decision is reached.}
\label{fig:L_bertmask_ee_pre1}
\end{figure}


\section{Descriptions of $\matffl{}$ and $\matlnl{}$}
\label{sec:app_submodule_skip_description}

Here we detail the definitions of the mappings $\matffl{}$ and $\matlnl{}$ utilized in \S\ref{sec:submodules}.

\paragraph{Description of $\matffl{}$.}
Let $v^\ell_{i_s}$ be the vector at position $i_s$ in the output of $\texttt{ln2}_{\ell} (\texttt{b}_\ell^{\texttt{attn}} (H^{\ell - 1}))$, for a given input $s$. We denote by $A_\ell^{\texttt{ffn}} \in \mathbb{R}^{d_h \times d_h}$ the matrix numerically minimizing 
$$ A \mapsto \sum_{s \in \mathcal{T}} || A \cdot v^{\ell}_{i_s} - \texttt{ffn}_{\ell} (v^\ell_{i_s})||^2,$$
and define a replacement of the feed-forward sub-module $\texttt{b}_{\ell}^{\texttt{ffn}}$ by $$ \texttt{b}^{\overline{\texttt{ffn}}}_\ell (H) \coloneqq A_{\ell}^{\texttt{ffn}} \cdot \texttt{ln2}_\ell (H) + H.$$
We then define a mapping between two layers ${\ell \rightarrow \ell'}$ by:
$$ \matffl{} (H) \coloneqq $$
$$ \texttt{b}^{\overline{\texttt{ffn}}}_{\ell'} ( \texttt{b}^{\texttt{attn}}_{\ell'} ( \ldots (\texttt{b}^{\overline{\texttt{ffn}}}_{\ell+1} ( \texttt{b}^{\texttt{attn}}_{\ell+1} (H))\ldots)).$$

\paragraph{Description of $\matlnl{}$.}
Let $v^\ell_{i_s}$ be the vector at position $i_s$ in the output of $\texttt{b}^{\texttt{attn}}_{\ell} (H^{\ell - 1})$, for a given input $s$. We denote by $A_\ell^{\texttt{ln1}} \in \mathbb{R}^{d_h \times d_h}$ the matrix numerically minimizing 
$$ A \mapsto \sum_{s \in \mathcal{T}} || A \cdot h^{\ell}_{i_s} - \texttt{ln1}_{\ell} (h^\ell_{i_s})||^2$$ and we denote by $A_\ell^{\texttt{ln2}} \in \mathbb{R}^{d_h \times d_h}$ the matrix numerically minimizing $$ A \mapsto \sum_{s \in \mathcal{T}} || A \cdot v^{\ell}_{i_s} - \texttt{ln2}_{\ell} (v^\ell_{i_s})||^2.$$ We define a replacement of the block $\texttt{b}^{\texttt{attn}}_{\ell}$ by \begin{equation} \texttt{b}^{\overline{\texttt{ln1}}}_\ell (H) \coloneqq \texttt{attn}_{\ell} (A_{\ell}^{\texttt{ln1}} \cdot H) + H\end{equation} and we define a replacement of the block $\texttt{b}^{\texttt{ffn}}_{\ell}$ by \begin{equation} \texttt{b}^{\overline{\texttt{ln2}}}_\ell (H) \coloneqq \texttt{ffn}_{\ell} (A_{\ell}^{\texttt{ln2}} \cdot H) + H.\end{equation}
We then define a mapping between two layers ${\ell \rightarrow \ell'}$ by:
$$ \matlnl{} (H) \coloneqq $$
$$ \texttt{b}^{\overline{\texttt{ln2}}}_{\ell'} ( \texttt{b}^{\overline{\texttt{ln1}}}_{\ell'} ( \ldots (\texttt{b}^{\overline{\texttt{ln2}}}_{\ell+1} ( \texttt{b}^{\overline{\texttt{ln1}}}_{\ell+1} (H))\ldots)).$$

\section{Results for Models of Various Sizes}
\label{sec:app_scale}

Here we record results of some of the experiments when performed with $12$, $24$ and $36$-layered versions of \gpt{} and $12$-layered version of \bert{}, on the Wikipedia dataset; Figs.~\ref{fig:r2_scores_12},~\ref{fig:r2_scores_24},~\ref{fig:r2_scores_36},~\ref{fig:bertmask_r2_scores_12} (quality of fit), Figs.~\ref{fig:pre_12},~\ref{fig:pre_24},~\ref{fig:pre_36},~\ref{fig:bertmask_pre_12} (layer linear shortcut, Precision@$k$), Figs.~\ref{fig:surp_12},~\ref{fig:surp_24},~\ref{fig:surp_36},~\ref{fig:bertmask_surp_12} (layer linear shortcut, Surprisal), 


\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figs/r2_scores_12.pdf}
\caption{The coordinate-averaged $r^2$-score of $\matl{}$ (left) and $\idl{}$ (right) ($12$-layered \gpt{}).}
\label{fig:r2_scores_12}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figs/r2_scores_24.pdf}
\caption{The coordinate-averaged $r^2$-score of $\matl{}$ (left) and $\idl{}$ (right) ($24$-layered \gpt{}).}
\label{fig:r2_scores_24}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]
{figs/r2_scores_36.pdf}
\caption{The coordinate-averaged $r^2$-score of $\matl{}$ (left) and $\idl{}$ (right) ($36$-layered \gpt{}).}
\label{fig:r2_scores_36}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]
{figs/bertmask_r2_scores_12.pdf}
\caption{The coordinate-averaged $r^2$-score of $\matl{}$ (left) and $\idl{}$ (right) ($12$-layered \bert{} masked inputs).}
\label{fig:bertmask_r2_scores_12}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[scale=0.35]
{figs/pre_12.pdf}
\caption{Precision@$k$ ($k = 1,5, 10$) for  $\matlL{}$ and the baseline $\idlL{}$ ($12$-layered \gpt{} next token prediction task).}
\label{fig:pre_12}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]
{figs/pre_24.pdf}
\caption{Precision@$k$ ($k = 1,5, 10$) for  $\matlL{}$ and the baseline $\idlL{}$ ($24$-layered \gpt{} next token prediction task).}
\label{fig:pre_24}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]
{figs/pre_36.pdf}
\caption{Precision@$k$ ($k = 1,5, 10$) for  $\matlL{}$ and the baseline $\idlL{}$ ($36$-layered \gpt{} next token prediction task).}
\label{fig:pre_36}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]
{figs/bertmask_pre_12.pdf}
\caption{Precision@$k$ ($k = 1,5, 10$) for  $\matlL{}$ and the baseline $\idlL{}$ ($12$-layered \bert{} masked token prediction task).}
\label{fig:bertmask_pre_12}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[scale=0.35]
{figs/surp_12.pdf}
\caption{Surprisal for  $\matlL{}$ and the baseline $\idlL{}$ ($12$-layered \gpt{} next token prediction task).}
\label{fig:surp_12}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]
{figs/surp_24.pdf}
\caption{Surprisal for  $\matlL{}$ and the baseline $\idlL{}$ ($24$-layered \gpt{} next token prediction task).}
\label{fig:surp_24}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]
{figs/surp_36.pdf}
\caption{Surprisal for  $\matlL{}$ and the baseline $\idlL{}$ ($36$-layered \gpt{} next token prediction task).}
\label{fig:surp_36}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]
{figs/bertmask_surp_12.pdf}
\caption{Surprisal for  $\matlL{}$ and the baseline $\idlL{}$ ($12$-layered \bert{} masked token prediction task).}
\label{fig:bertmask_surp_12}
\end{figure}


\section{More Figures}
\label{sec:app_more_figures}

Here we record Figs.~\ref{fig:parts_pre5},~\ref{fig:parts_pre10} mentioned in \S\ref{sec:submodules}.

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]
{figs/parts_pre5_24.pdf}
\caption{Precision@$5$ for the various sub-module linear shortcut methods, and the previous method \mat{} for comparison ($24$-layered \gpt{} next token prediction task) - Wikipedia dataset. For a layer $\ell$, which serves as the $x$-coordinate, the $y$-coordinate is the rate at which the top $1$ token according to a method lies in the top $5$ tokens according to the model.}
\label{fig:parts_pre5}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]
{figs/parts_pre10_24.pdf}
\caption{Precision@$10$ for the various sub-module linear shortcut methods, and the previous method \mat{} for comparison ($24$-layered \gpt{} next token prediction task) - Wikipedia dataset. For a layer $\ell$, which serves as the $x$-coordinate, the $y$-coordinate is the rate at which the top $1$ token according to a method lies in the top $10$ tokens according to the model.}
\label{fig:parts_pre10}
\end{figure}