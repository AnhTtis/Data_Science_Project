\section{Background and Notation}
\label{sec:background}

The input to a transformer-based LM \cite{NIPS2017_3f5ee243} is a sequence of tokens $t_1, ..., t_n$ from a vocabulary $\mathcal{V}$ of size $|\mathcal{V}| = d_v$. The tokens are first represented as vectors using an embedding matrix $E \in \mathbb{R}^{d_h \times d_v}$, where $d_h$ is the hidden dimension of the model, to create the initial \emph{hidden representations} $$ H^0 = ( h_1^0, \ldots, h_n^0 ) \in \mathbb{R}^{d_h \times n}.$$ 
These representations are then repeatedly transformed through $L$ transformer blocks, where each block outputs hidden representations that are the inputs to the next block:
$$ \forall \ell \in [1, L]: \;\; \texttt{b}^\ell (H^{\ell-1}) = H^{\ell} $$ 
where $$H^\ell = (h^{\ell}_1, \ldots, h^{\ell}_n) \in \mathbb{R}^{d_h \times n}.$$

The $\ell$-th transformer block is constructed as a composition of two layers: $$ \texttt{b}_\ell = \texttt{b}^{\texttt{ffn}}_\ell \circ \texttt{b}^{\texttt{attn}}_\ell,$$ where $\texttt{b}^{\texttt{attn}}_\ell$  (resp. $\texttt{b}^{\texttt{ffn}}_\ell$) is a multi-head self-attention layer (resp. FFN layer) enclosed by a residual connection, and potentially interjected with layer normalization \cite{ba2016layer}.

The \emph{final representations}, denoted as $$ H^L = ( h_{1}^{L}, \ldots, h_{n}^{L}), $$ are considered as the transformer stack's output. These representations are used to form various predictions. In this work, we investigate whether and how hidden representations from \textit{earlier layers} can be utilized for this purpose instead.