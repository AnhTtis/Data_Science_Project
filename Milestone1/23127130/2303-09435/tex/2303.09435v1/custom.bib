% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").


@inproceedings{schuster2022confident,
title={Confident Adaptive Language Modeling},
author={Tal Schuster and Adam Fisch and Jai Gupta and Mostafa Dehghani and Dara Bahri and Vinh Q. Tran and Yi Tay and Donald Metzler},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=uLYc4L3C81A}
}

@misc {HFgpt2,
	author       = { {Huggingface} },
	title        = { gpt2 },
	year         = 2023,
	url          = { https://huggingface.co/gpt2 },
	publisher    = { Hugging Face }
}

@misc {HFBERT,
	author       = { {Huggingface} },
	title        = { bert-large-uncased },
	year         = 2023,
	url          = { https://huggingface.co/bert-large-uncased },
	publisher    = { Hugging Face }
}

@misc {leipzig_corpora,
	author       = { {Leipzig Corpora Collection} },
	title        = { \url{https://corpora.uni-leipzig.de} },
	url          = { https://corpora.uni-leipzig.de }
}

@misc{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  url={ https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf },
  year={2019}
}

@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings {He,
author = {K. He and X. Zhang and S. Ren and J. Sun},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Deep Residual Learning for Image Recognition},
year = {2016},
volume = {},
issn = {1063-6919},
pages = {770-778},
doi = {10.1109/CVPR.2016.90},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2016.90},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@inproceedings{geva-etal-2022-lm,
    title = "{LM}-Debugger: An Interactive Tool for Inspection and Intervention in Transformer-Based Language Models",
    author = "Geva, Mor  and
      Caciularu, Avi  and
      Dar, Guy  and
      Roit, Paul  and
      Sadde, Shoval  and
      Shlain, Micah  and
      Tamir, Bar  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the The 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-demos.2",
    pages = "12--21"
}

@inproceedings {Elhage,
	author = { Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris },
	title        = { A Mathematical Framework for Transformer Circuits },
	year         = 2021,
	url          = { https://transformer-circuits.pub/2021/framework/index.html },
    booktitle = { Transformer Circuits Thread }
}

@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{scao2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{xu2021survey,
  title={A survey on green deep learning},
  author={Xu, Jingjing and Zhou, Wangchunshu and Fu, Zhiyi and Zhou, Hao and Li, Lei},
  journal={arXiv preprint arXiv:2111.05193},
  year={2021}
}

@article{ram2022you,
  title={What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary},
  author={Ram, Ori and Bezalel, Liat and Zicher, Adi and Belinkov, Yonatan and Berant, Jonathan and Globerson, Amir},
  journal={arXiv preprint arXiv:2212.10380},
  year={2022}
}

@article{dar2022analyzing,
  title={Analyzing transformers in embedding space},
  author={Dar, Guy and Geva, Mor and Gupta, Ankit and Berant, Jonathan},
  journal={arXiv preprint arXiv:2209.02535},
  year={2022}
}

@InProceedings{Zhao2021of,
	author=	{Sumu Zhao and Damian Pascual and Gino Brunner and Roger Wattenhofer},
	title=	{{Of Non-Linearity and Commutativity in BERT}},
	booktitle=	{{International Joint Conference on Neural Networks (IJCNN), Virtual-only}},
	month=	{July},
	year=	{2021},
}

@article{lamparth2023analyzing,
  title={Analyzing And Editing Inner Mechanisms of Backdoored Language Models},
  author={Lamparth, Max and Reuel, Anka},
  journal={arXiv preprint arXiv:2302.12461},
  year={2023}
}

@article{ba2016layer,
  author    = {Lei Jimmy Ba and
               Jamie Ryan Kiros and
               Geoffrey E. Hinton},
  title     = {Layer Normalization},
  journal   = {CoRR},
  volume    = {abs/1607.06450},
  year      = {2016},
  url       = {http://arxiv.org/abs/1607.06450},
  eprinttype = {arXiv},
  eprint    = {1607.06450}
}

@inproceedings{
adi2017finegrained,
title={Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks},
author={Yossi Adi and Einat Kermany and Yonatan Belinkov and Ofer Lavi and Yoav Goldberg},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=BJh6Ztuxl}
}

@article{leviathan2022fast,
  title={Fast Inference from Transformers via Speculative Decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  journal={arXiv preprint arXiv:2211.17192},
  year={2022}
}
@article{chen2023accelerating,
  title={Accelerating large language model decoding with speculative sampling},
  author={Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  journal={arXiv preprint arXiv:2302.01318},
  year={2023}
}