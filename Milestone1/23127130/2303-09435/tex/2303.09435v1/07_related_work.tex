
\section{Related Work}

Recently, there was a lot of interest in utilizing intermediate representations in transformer-based LMs, both for interpretability and for efficiency.

In the direction of interpretability, one seeks to understand the prediction construction process of the model \cite{tenney-etal-2019-bert, voita-etal-2019-bottom}.

More recent works use mechanistic interpretability and view the inference pass as a residual stream of information \cite{dar2022analyzing,geva-etal-2022-transformer}. Additionally, there are works on probing, attempting to understand what features are stored in the hidden representations \cite{adi2017finegrained, conneau-etal-2018-cram,liu-etal-2019-linguistic}. Our work is different in that it attempts to convert intermediate representations into a final-layer form, which is interpretable by design.

In the direction of efficiency, there is the thread of work on early exit, where computation is cut at a dynamically-decided earlier stage \cite{schwartz-etal-2020-right,xin-etal-2020-deebert,schuster2022confident}. Other works utilize a fixed early stage network to parallelize inference \citep{leviathan2022fast, chen2023accelerating}. However, intermediate representations are directly propagated in these works, which we show is substantially worse than our approach. Moreover, our method requires training considerably less parameters than methods such as \citet{schuster-etal-2021-consistent}, that learn a different output softmax for each intermediate layer.  

More broadly, skipping transformer layers and analyzing the linearity properties of transformer components have been discussed in prior works \cite{Zhao2021of,mickus-etal-2022-dissect,wang-etal-2022-skipbert,lamparth2023analyzing}.
