\section{Linear Shortcut Across Blocks}
\label{sec:layer_jump}

To use a hidden representation from layer $\ell<L$ as a final representation, we propose to cast it using linear regression, while skipping the computation in-between these layers. More generally, this approach can be applied to cast any $\ell$-th hidden representation to any succeeding layer $\ell'>\ell$.


\subsection{Method}
\label{subsec:methodology_linear_shortcut}

Given a source layer $\ell$ and a target layer $\ell'$ such that $\ell < \ell' \leq L$, our goal is to learn a mapping $A_{\ell', \ell} \in \mathbb{R}^{d_h \times d_h}$ from hidden representations at layer $\ell$ to those at layer $\ell'$. To this end, we first collect a set of corresponding hidden representation pairs $(h^\ell, h^{\ell'})$. Concretely, we run a set $\mathcal{T}$ of input sequences through the model, and for each such input $s$ we extract the hidden representations $h_{i_s}^{\ell}, h_{i_s}^{\ell'}$, where $i_s$ is a random position in $s$.

Next, we learn a matrix $A_{\ell', \ell} \in \mathbb{R}^{d_h \times d_h}$ by fitting linear regression over $\mathcal{T}$, namely, $A_{\ell', \ell}$ is a numerical minimizer for:
$$ A \mapsto \sum_{s \in \mathcal{T}} || A \cdot h_{i_s}^\ell - h_{i_s}^{\ell'} ||^2.$$ 

We define the mapping of a representation $h$ from layer $\ell$ to layer $\ell'$ as:
\begin{equation}
\label{eq:linear_jump}
    \matl{} (h) \coloneqq A_{\ell', \ell} \cdot h
\end{equation}


\subsection{Baseline}
\label{subsec:baseline}

We evaluate our method against the prevalent approach of ``reading'' hidden representations directly, without any transformation. 
Namely, the propagation of a hidden representation from layer $\ell$ to layer $\ell'$ is given by the identity function, dubbed \id{}:

$$ \idl{} (h) \coloneqq h.$$

Notably, this commonly-used baseline 
assumes that representations at different layers operate in the same linear space.

\subsection{Quality of Fit}
\label{subsec:experiments_r2}

We first evaluate our method by measuring how well the learned linear mappings approximate the representations at the target layer. To this end, we calculate the (coordinate-averaged) $r^2$-score of our mapping's outputs with respect to the ``real'' representations obtained from a full inference pass, and compare to the same for the \id{} baseline.


\paragraph{Models.} 
We use \gpt{} \cite{radford2019language}, a decoder-only auto-regressive LM, and \bert{} \cite{devlin-etal-2019-bert}, an encoder-only model trained with masked language modeling.
We conduct our evaluation over multiple scales of these models, with $12$, $24$, $36$, and $48$ layers for \gpt{}, and $12$ and $24$ layers for \bert{}.
The plots presented in this section are for $48$-layered \gpt{} and $24$-layered \bert{}; in~\S\ref{sec:app_scale} we gather the plots for the rest of the models.

\paragraph{Data.}
We use two data sources to evaluate our method -- Wikipedia and news articles in English from the Leipzig Corpora Collection \cite{goldhahn-etal-2012-building}.
From each source, we take 9,000 random sentences for training ($\mathcal{T}$).\footnote{We use sentences rather than full documents to simplify the analysis.}
As a validation set, $\mathcal{V}$, we use additional 3,000 and 1,000 instances from Wikipedia and the Leipzig Corpora, respectively.
For each example $s$, we select a random position $i_s$ and extract the hidden representations $h_{i_s}^{\ell}$ at that position from all the layers.
For \bert{}, we convert the input token at position $i_s$ to a \mask{} token, as our main motivation is to interpret predictions, which are generated through masked tokens in \bert{} (see \S\ref{subsec:BERT}).
Thus, the hidden representations we consider in the case of \bert{} are just of \mask{} tokens.

As we observed similar results for the two data sources across all our experiments, throughout the paper we will report results for Wikipedia and provide the results for the news articles in \S\ref{sec:app_Leipzig}. 
Further details on the data and models are provided in \S\ref{sec:exp_set}.


\paragraph{Evaluation.}
For every pair of layers $\ell, \ell'$, such that $0 \leq \ell < \ell' \leq L$, we use the training set $\mathcal{T}$ to fit linear regression as described in \S\ref{subsec:methodology_linear_shortcut}, and obtain a mapping $\matl{}$. 
Next, we evaluate the quality of $\matl{}$ as well as of $\idl{}$ using the $r^2$-coefficient, uniformly averaged over all coordinates. Concretely, we compute the $r^2$-coefficient of the true representations $h_{i_s}^{\ell'}$ versus each of the predicted representations $\matl{} (h_{i_s}^{\ell})$ and $\idl{} (h_{i_s}^{\ell})$ for every $s \in \mathcal{V}$.


\begin{figure}[t]
\includegraphics[width=\columnwidth]{figs/r2_scores_48.pdf}
\caption{The coordinate-averaged $r^2$-score of $\matl{}$ (left) and $\idl{}$ (right) ($48$-layered \gpt{}).}
\label{fig:r2_scores}
\end{figure}

\begin{figure}[t]
\includegraphics[width=\columnwidth]{figs/bertmask_r2_scores_24.pdf}
\caption{The coordinate-averaged $r^2$-score of $\matl{}$ (left) and $\idl{}$ (right) ($24$-layered \bert{} masked inputs).}
\label{fig:bertmask_r2_scores}
\end{figure}


\paragraph{Results.}
Results for $48$-layered \gpt{} and $24$-layered \bert{} are presented in Fig.~\ref{fig:r2_scores} and~\ref{fig:bertmask_r2_scores}, respectively.
In both models, \mat{} consistently yields better approximations than \id{}, as it obtains higher $r^2$-scores (in blue) across the network. 

This gap between \mat{} and \id{} is especially evident in \bert{}, where \id{} completely fails to map the representations between most layers, suggesting that hidden representations are modified  substantially by every transformer block.
Overall, this highlights the shortcoming of existing practices to inspect representations in the same linear space, and the gains from using our method to approximate future layers in the network.   
