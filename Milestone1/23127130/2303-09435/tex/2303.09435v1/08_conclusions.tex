
\section{Conclusion and Future Work}

We present a simple method for inspection of hidden representations in transformer models, by using pre-fitted context-free and token-uniform linear mappings. Through a series of experiments on different data sources, model architectures and scales, we show that our method consistently outperforms the prevalent practice of interpreting representations in the final-layer space of the model, yielding better approximations of succeeding representations and the predictions they induce.

We also demonstrate the practicality of our method for improving computation efficiency, saving a substantial amount of compute on top of prominent early exiting approaches. 

Last, by extending our method to sub-modules, more specifically the attention sub-modules, we observe that in some cases replacing a part of transformer inference by a non-contextual linear computation only results in a small deterioration of the prediction.
This opens new research directions for improving model efficiency, including breaking the computation into several parallelizable tasks.