
\section{Introduction}
\label{sec:introduction}

Transformer-based language models (LMs) process an input sequence of tokens by first representing it as a sequence of vectors and then repeatedly transforming it through a fixed number of attention and feed-forward network (FFN) layers 
\cite{NIPS2017_3f5ee243}. While each transformation creates new token representations, only the final representations are used to obtain model predictions. Correspondingly, LM loss minimization directly optimizes the final representations, while hidden representations are only optimized implicitly, thus making their interpretation and usefulness more obscure.

However, utilizing hidden representations is highly desirable; a successful interpretation of them can shed light on the ``decision-making process'' in the course of transformer inference \cite{tenney-etal-2019-bert, voita-etal-2019-bottom, slobodkin-etal-2021-mediators,geva-etal-2022-transformer}, and obtaining predictions from them can substantially reduce compute resources \cite{schwartz-etal-2020-right,xu2021survey}.

\begin{figure}
\centering
\includegraphics[scale=0.5]{figs/main.pdf}
\caption{An illustration of our method $\matl{}$ (in blue) of short-cutting away transformer inference in-between certain layers by applying a matrix $A = A_{\ell',\ell}$ learnt by fitting linear regression, versus the baseline method $\idl{}$ (in red) of propagating the hidden representation as-is to the further layer.}
\label{fig:main}
\end{figure}

Previous attempts to exploit hidden representations viewed the hidden representations of an input token as a sequence of approximations of its final representation \cite{Elhage, geva-etal-2022-transformer}. This view is motivated by the additive updates induced via the residual connections \cite{He} around each layer in the network. Indeed, previous works \cite{geva-etal-2021-transformer, geva-etal-2022-lm, ram2022you, alammar-2021-ecco} followed a simplifying assumption that representations \textit{at any layer} can be transformed into a distribution over the output vocabulary by the output embeddings.
While this approach has proven to be surprisingly effective for interpretability \cite{geva-etal-2022-lm, dar2022analyzing} and computation efficiency \cite{schuster2022confident, xin-etal-2020-deebert, schwartz-etal-2020-right}, it oversimplifies the model's computation and assumes that all the hidden layers in the network operate in the same space.

A natural question that arises is whether there is a more accurate way to cast hidden representations into final representation substitutes than simply interpreting them as they are. 
In this work, we tackle this question by learning linear transformations across layers in the network (illustrated in Fig.~\ref{fig:main}). Concretely, for any two layers $\ell < \ell'$, we fit linear regression to transform hidden representations from layer $\ell$ to layer $\ell'$. We show that this method, called \mat{}, produces substantially more accurate approximations than the above-discussed identity mapping, dubbed \id{}, applied in previous works (\S\ref{sec:layer_jump}).
This suggests that there is more linearity to transformer inference than could be explained solely by the residual connection structure.


We further test our approach in the context of language modeling (\S\ref{sec:prediction}), checking 
how often predictions from final representation substitutes produced by \mat{} agree with those of actual final representations. 
Experiments across two data sources and different scales of \gpt{} \cite{radford2019language} and \bert{} \cite{devlin-etal-2019-bert} show large accuracy gains ($15\%$-$40\%$ at most layers) in prediction estimation by \mat{} over naive projections (\id{}).
Moreover, we observe that our mappings often (in $>30\%$ of the cases) produce correct predictions when applied to the very early layers in the network.


We then leverage these findings for improving model efficiency and demonstrate our method's utility in the setting of early exiting, a strategy according to which one dynamically decides at which layer to stop the inference pass and use that layer's representation as a final layer representation substitute. While previous works have utilized these hidden representations intact (i.e. using \id{}), we transform them using \mat{}, showing that our method performs better than the baseline in this setting as well (\S\ref{sec:applications}), allowing for the saving of additional $7.9\%$ (resp. $5.4\%$) of the layers for \gpt{} (resp. \bert{}) when aiming at $95\%$ accuracy.

Last, we analyze how well different parts of the transformer computation can be estimated linearly (\S\ref{sec:submodules}). To this end, we apply the same methodology to replace the sub-modules of attention, FFN, and layer normalization with linear mappings. Interestingly, we find that linearly approximating attention, the only sub-module in the network that has contextual processing, results in the least reduction of precision. This hints at an interesting possibility of compute time reduction, since non-contextual inference is parallelizable.

To conclude, we propose a method for casting hidden representations across transformer layers, that is light to train, cheap to infer, and provides more accurate representation approximations  than the hitherto implicitly accepted baseline of identical propagation. The method is not only appealing for model analysis but also has concrete applications for efficiency.
