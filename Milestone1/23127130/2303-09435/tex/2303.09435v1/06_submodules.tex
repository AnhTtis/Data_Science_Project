\section{Linear Shortcut Across Sub-modules}
\label{sec:submodules}

Our experiments show that, despite the commonly-applied simplification by interpretability works, transformer layers do not operate in the same linear space and there is a major gap in approximating future representations using an identity mapping (\S\ref{sec:layer_jump}, \S\ref{sec:prediction}).
In this section, we investigate whether discrepancies across layers result from specific sub-modules or are a general behaviour of all sub-modules in the network.  

This is done by extending our approach to test how well particular components in transformer blocks can be linearly approximated. 


\paragraph{Method.}

Discussing \gpt{} for definiteness, we have 
$$ \texttt{b}_{\ell} = \texttt{b}_{\ell}^{\texttt{ffn}} \circ \texttt{b}_{\ell}^{\texttt{attn}}$$ with
\begin{equation}\label{eq:attn} \texttt{b}^{\texttt{attn}}_{\ell} (H) = \texttt{attn}_{\ell} (\texttt{ln1}_{\ell} (H)) + H,\end{equation} 

where $\texttt{attn}_{\ell}$ is a multi-head self-attention layer and \texttt{ln1} is a layer normalization, and 
$$ \texttt{b}^{\texttt{ffn}}_{\ell} (H) = \texttt{ffn}_{\ell} (\texttt{ln2}_{\ell} (H)) + H,$$  
where $\texttt{ffn}_{\ell}$ is a feed-forward network layer and $\texttt{ln2}$ is a layer normalization.

Given a block $\texttt{b}_\ell$ and one of its sub-modules $\texttt{ln1}_\ell, \ \texttt{attn}_\ell, \ \texttt{ln2}_\ell$, or $\texttt{ffn}_\ell$, we fit a linear regression approximating the output of the sub-module given its input. 
Illustrating this on $\texttt{attn}_\ell$ for definiteness, let $v^\ell_{i_s}$ be the vector at position $i_s$ in the output of $\texttt{attn}_\ell (\texttt{ln1}_\ell (H^{\ell - 1}))$, for a given input $s$. We denote by $A_\ell^{\texttt{attn}} \in \mathbb{R}^{d_h \times d_h}$ the matrix numerically minimizing 
$$ A \mapsto \sum_{s \in \mathcal{T}} || A \cdot \texttt{ln1}_\ell (h^{\ell-1}_{i_s}) - v^\ell_{i_s}||^2,$$

and define a replacement of the attention sub-module (Eq.~\ref{eq:attn}) by \begin{equation}\label{eq:attn_linear} \texttt{b}^{\overline{\texttt{attn}}}_\ell (h) \coloneqq A_{\ell}^{\texttt{attn}} \cdot \texttt{ln1}_\ell (h) + h. \end{equation}

We then define a mapping between two layers ${\ell \rightarrow \ell'}$ by:
$$ \matattnl{} (h) \coloneqq $$
$$ \texttt{b}^{\texttt{ffn}}_{\ell'} ( \texttt{b}^{\overline{\texttt{attn}}}_{\ell'} ( \ldots (\texttt{b}^{\texttt{ffn}}_{\ell+1} ( \texttt{b}^{\overline{\texttt{attn}}}_{\ell+1} (h))\ldots)).$$ In other words, when applying each $\ell''$-th block, $\ell < \ell'' \leq \ell'$, we replace its attention sub-module $\texttt{attn}_{\ell''}$ by its linear approximation.



In an analogous way, we consider the mappings $\matffl{}$ and $\matlnl{}$, where in the latter we perform the linear shortcut both for \texttt{ln1} and for \texttt{ln2} (see~\S\ref{sec:app_submodule_skip_description} for precise descriptions).
Importantly, unlike the original attention module, the approximation $\texttt{b}^{\overline{\texttt{attn}}}_\ell$ operates on each position independently, and therefore, applying the mapping $\matattnl{}$ disables any contextualization between the layers $\ell$ and $\ell'$. Note that this is not the case for $\matffl{}$ and $\matlnl{}$, which retain the self-attention sub-modules and therefore operate contextually.


\paragraph{Evaluation.}


We analyze the $24$-layered \gpt{}, and proceed completely analogously to \S\ref{subsec:next_token_prediction_task}, evaluating the Precision@$k$ and Surprisal metrics for the additional mappings $\matattnlL{}$, $\matfflL{}$ and $\matlnlL{}$.


\begin{figure}[t]
\centering
\includegraphics[scale=0.35]{figs/parts_pre1_24.pdf}
\caption{Precision@$1$ for the various sub-module linear shortcut mappings, and the mapping $\matlL{}$ for comparison (\gpt{} next token prediction task).}
\label{fig:parts_pre1}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.35]{figs/parts_surp_24.pdf}
\caption{Surprisal for the various sub-module linear shortcut mappings, and the mapping $\matlL{}$ for comparison (\gpt{} next token prediction task). A 95\% confidence interval surrounds the lines.}
\label{fig:parts_surp}
\end{figure}

\paragraph{Results.}
Figs.~\ref{fig:parts_pre1} and~\ref{fig:parts_surp} show the average Precision@$1$ and Surprisal scores per layer in the $24$-layered \gpt{}, respectively.\footnote{Results for Precision@$5$ and Precision@$10$ exhibit a similar trend and are provided in \S\ref{sec:app_more_figures}.} However, from a certain layer (\textasciitilde$7$), all sub-module mappings achieve better results than the full-block mapping $\matlL{}$. Thus, it is not just the cumulative effect of all the sub-modules in the transformer block that is amenable to linear approximation, but 
also individual sub-modules can be linearly approximated. Our experiments expose further that the linear approximation of attention sub-modules is much less harmful than that of the FFN or layer normalization sub-modules. Hypothetically, a possible reason is that the linear replacement of FFN or layer normalization ``erodes'' the self-attention computation after a few layers. Moreover, the good performance of $\matattnlL{}$ suggests that in many cases the consideration of context exhausts itself early in the procession of layers; speculatively, it is only in more delicate cases that the self-attention of late layers adds important information. Last, remark the sharp ascent of the results of the layer normalization mapping between layers $5$-$8$, for which we do not currently see a particular reason. To conclude, we learn that the possibility of linear approximation permeates the various transformer components.