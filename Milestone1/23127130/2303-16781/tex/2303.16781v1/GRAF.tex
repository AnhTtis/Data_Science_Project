\documentclass[10pt]{article}
\usepackage[top=0.5in,left=0.8in,footskip=0.75in,marginparwidth=1in]{geometry}
%\usepackage[top=0.5in,left=1.3in,footskip=0.75in,marginparwidth=2in]{geometry}
\usepackage{bbm}

% use Unicode characters - try changing the option if you run into troubles with special characters (e.g. umlauts)
\usepackage[utf8]{inputenc}
% clean citations
%% \usepackage{cite, amsmath}
\usepackage{amsmath}

% hyperref makes references clicky. use \url{www.example.com} or \href{www.example.com}{description} to add a clicky url
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% improves typesetting in LaTeX
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% text layout - change as needed
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 7.00in 
\textheight 9in

% Remove % for double line spacing
%\usepackage{setspace} 
%\doublespacing

% use adjustwidth environment to exceed text width (see examples in text)
\usepackage{changepage}

% adjust caption style
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,singlelinecheck=off]{caption}

% remove brackets from references
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% headrule, footrule and page numbers
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
\pagestyle{myheadings}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\fancyhf{}
\rfoot{\thepage/\pageref{LastPage}}
%\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
%\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}

% use \textcolor{color}{text} for colored text (e.g. highlight to-do areas)
\usepackage{color}

% define custom colors (this one is for figure captions)
\definecolor{Gray}{gray}{.25}

% this is required to include graphics
\usepackage{graphicx}

% use if you want to put caption to the side of the figure - see example in text
\usepackage{sidecap}

%\usepackage[sorting=none]{biblatex}

% use for have text wrap around figures
\usepackage{wrapfig}
\usepackage[pscoord]{eso-pic}
\usepackage[fulladjust]{marginnote}
\reversemarginpar

% zz
%\usepackage{xcolor, soul}
%\sethlcolor{yellow}
%\usepackage[draft]{todonotes}
\usepackage{float}
\usepackage{ragged2e}
\justifying
 \renewcommand{\familydefault}{\sfdefault}
%\usepackage{lineno}
%\linenumbers
\usepackage[ruled,vlined]{algorithm2e}%[linesnumbered,ruled,vlined]{algorithm2e}
%%% Coloring the comment as blue
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}              % set the Output
\newlength\mylen
\newcommand\myinput[1]{%
  \settowidth\mylen{\KwIn{}}%
  \setlength\hangindent{\mylen}%
  \hspace*{\mylen}#1\\}
  

% zz
\usepackage{color,soul}
\usepackage{booktabs, caption, multicol, multirow}
%\newcolumntype{?}{!{\vrule width 1pt}}
\usepackage{tabularx}
\newcolumntype{?}[1]{!{\vrule width #1}}
%zz

 
% document begins here
\begin{document}
\vspace*{0.35in}

% title goes here:
\begin{flushleft}
{\Large
\textbf\newline{GRAF: Graph Attention-aware Fusion Networks}
}
\newline
% authors go here:
\\
Ziynet Nesibe Kesimoglu\textsuperscript{1},
Serdar Bozdag\textsuperscript{1,2,3,*}
\\
\bigskip
\bf{1} Dept. of Computer Science and Engineering, University of North Texas, Denton, TX
\\
\bf{2} Dept. of Mathematics, University of North Texas, Denton, TX
\\
\bf{3} BioDiscovery Institute, University of North Texas, Denton, TX
\\
\bigskip
* Serdar.Bozdag@unt.edu

\end{flushleft}

\section*{Abstract}
A large number of real-world networks include multiple types of nodes and edges. Graph Neural Network (GNN) emerged as a deep learning framework to utilize node features on graph-structured data showing superior performance. However, popular GNN-based architectures operate on one homogeneous network. Enabling them to work on multiple networks brings additional challenges due to the heterogeneity of the networks and the multiplicity of the existing associations. In this study, we present a computational approach named GRAF utilizing GNN-based approaches on multiple networks with the help of attention mechanisms and network fusion. Using attention-based neighborhood aggregation, GRAF learns the importance of each neighbor per node (called \textit{node-level attention}) followed by the importance of association (called \textit{association-level attention}) in a hierarchical way. Then, GRAF processes a network fusion step weighing each edge according to learned node- and association-level attention, which results in a fused enriched network. Considering that the fused network could be a highly dense network with many weak edges depending on the given input networks, we included an edge elimination step with respect to edges' weights. Finally, GRAF utilizes Graph Convolutional Network (GCN) on the fused network and incorporates the node features on the graph-structured data for the prediction task or any other downstream analysis. Our extensive evaluations of prediction tasks from different domains showed that GRAF outperformed the state-of-the-art methods. Utilization of learned node-level and association-level attention allowed us to prioritize the edges properly. The source code for our tool is publicly available at \href{https://github.com/bozdaglab/GRAF}{https://github.com/bozdaglab/GRAF}.


\begin{multicols}{2}

\section{Introduction and Related Work}

Real-world networks mostly have multiple types of nodes and edges, called multiplex heterogeneous networks. For instance, there are multiple relations between publications in citation networks (e.g., connected by the same venues or cited by the same papers), and multiple paths to infer from movies (e.g., connected by the same actors or same directors). Graph representation learning attracted high interest, however, the heterogeneity of the graphs and the multiplicity of the existing associations bring big challenges. Considering the recent advances, it would be beneficial to utilize node features and multiple networks with learned importance for supervised and unsupervised analysis of graph-structured data. 


Various computational approaches have been developed to cluster nodes on a network integrating multiple data. Since multiple networks may contain complementary information, some studies integrated multiple networks into one network to perform clustering on \cite{wang2014similarity, ma2017integrate}. For instance, Similarity Network Fusion (SNF) \cite{wang2014similarity} builds a patient similarity network based on each data modality, fuses all networks into one consensus network by applying a nonlinear step, and performs the clustering on that consensus network. Affinity Network Fusion (ANF) \cite{ma2017integrate} builds upon SNF by simplifying the computational operations needed. Network fusion methods show good progress without using probabilistic modeling, however, these tools highly depend on building a similarity network to integrate information from multiple data modalities. In addition, these tools can not utilize node features on the network, which could be potentially informative in encoding the graph structure and features of the nodes.

Graph Neural Network (GNN) emerged as a powerful architecture allowing utilization of node features on graph-structured data \cite{gori2005new, scarselli2008graph, kipf2016semi, velivckovic2017graph}. GNN is a framework to define deep neural networks on graph-structured data generating node representations that depend on the graph structure, as well as node features. Iteratively, every node updates its current embedding by aggregating information from its local neighborhood. GNN-based approaches have been applied to different domains \cite{ramirez2020classification,zitnik2018modeling,hamilton2017inductive,mohamed2020discovering, rhee2017hybrid}. Several GNN-based architectures are developed depending on how the features are aggregated from the local structure and how the representations are combined \cite{xu2018powerful, wu2019simplifying, kipf2016semi, velivckovic2017graph}.


Graph Convolutional Network (GCN) is one of the most popular GNN architectures \cite{kipf2016semi}. GCN uses a type of aggregation where self nodes are included in the neighborhood and normalization is applied across neighbors with equal importance \cite{kipf2016semi}. There are multiple cancer types, and the cancer type prediction problem helps to assign patients to the correct cancer type. In \cite{ramirez2020classification}, authors leverage GCN on a gene coexpression and protein-protein interaction (PPI) network separately to predict the cancer type of patients. The convolution is done on the gene expression dataset only, thus, missing the complementary information of other data modalities and other networks. In \cite{rhee2017hybrid}, the authors leveraged GCN and relation network with PPI network on the classification task, while in \cite{zitnik2018modeling}, authors used a GCN-based model on drug and protein interaction network and separately deals with different edge types. 

Generalizing the self-attention mechanisms of Transformer's \cite{vaswani2017attention}, Graph Attention Networks (GAT) has been developed using attention-based neighborhood aggregation learning the importance of each neighbor \cite{velivckovic2017graph}. A follow-up study has shown that GAT computes static attention having the same ranking for attention coefficients in the same graph, and has proposed GATv2 \cite{brody2021attentive} with a simple fix by applying attention layer after the nonlinearity and weight after the concatenation. They proved that they improved the expressiveness of GAT. GNN-based architectures including GCN and GATs are highly utilized in different domains \cite{defferrard2016convolutional, ying2018graph, hamilton2017inductive}, however these models are mostly applicable to a single homogeneous network (i.e., one network with one type of node and edge), thus limiting their utilization of integrative approaches. 

To utilize more knowledge, studies improved GNN-based architectures to operate on multiple networks \cite{wang2021mogonet, kesimoglu2022supreme}. For instance, MOGONET runs three different GCN on three patient similarity networks from three different data modalities separately \cite{wang2021mogonet}. Then, it uses the label distribution from three different models and utilizes them to predict the final label of each node. In \cite{kesimoglu2022supreme}, we improved a GCN-based node classification framework operating on multiple networks, encoding multiple features on multiple networks. As compared to MOGONET, we utilized the intermediate embedding, not just the labels, and integrated them with node features, resulting in consistent and improved performance. Heterogeneous Graph Attention Network (HAN) applies GNN-based architecture on a heterogeneous network utilizing attention mechanisms \cite{wang2019heterogeneous}. A meta-path is defined as a series of relations between nodes defining a new relationship between its starting and ending node types. HAN generates meta-path-based networks from the heterogeneous network. Then it applies one transformation matrix for each node type, and learns node-level attention (for each node using its meta-path-based neighborhood) and association-level attention (for each meta-path for the prediction task). Then HAN optimizes the model via backpropagation. Even though this approach gives us a good approximation of the importance of the given associations, the hierarchical attention strategy could limit the utilization of the data.

To integrate multiple associations with node features on homogeneous or heterogeneous networks, we need to address the following requirements:

\textbf{Neighborhood Extraction.}
Based on multiple associations, we can generate a series of relations between nodes to define a neighborhood between the starting and ending nodes. For instance, the ACM database has paper-subject and paper-term associations. Connecting the papers, which are the node of interest, we might end up with two associations: paper-subject-paper and paper-term-paper. 
This type of network generated as a result of a meta-path is called a meta-path-based network and they are highly utilized on heterogeneous networks. 

\textbf{Importance of Neighbor Nodes.} With meta-path neighborhood, we connected a node with multiple neighbors since they have a predefined composed association. As the above example of the ACM database, we assumed if papers are connected with the same subject or term, then they will likely be similar according to our desired task. However, based on the defined associations, we might have many neighbors for the nodes. 
Supposedly, not all the papers connected by a subject/term will not have the same effect on each other. Considering that, each node requires weighing each meta-path neighbor and giving more importance to highly-attended neighbors. 

\textbf{Importance of Association Types.} Since we need to capture more information, we utilize multiple associations, however, the associations will not have a similar impact on the desired task. For instance, we might expect that paper-subject-paper will be more important than a paper-term-paper association. Therefore, we need to learn the importance of each association type and consider that in the following steps.

\textbf{Network Fusion.} Each node pair might have multiple associations, but with different importance for different associations. Some node pairs might have edges from insignificant associations, while some node pairs have only one edge, but from a very important association. Therefore, we cannot only assume the existence of the edges but the weight of the node pairs should be also considered. Moreover, to utilize GNN-based popular architectures, we need to have a single network. To this end, we might integrate multiple associations into a fused network properly combining both node-level and association-level importance. 

\textbf{Utilization from Node Features.} 
In addition to associations, there are features available for nodes, which could be important to utilize. With GNN-based architectures, we can aggregate and utilize node features on the graph-structured data. 
For instance, for the ACM database, our node type of interest is paper, and we might utilize word embedding techniques of natural language processing \cite{mikolov2013efficient, pennington2014glove} and we might use word embeddings as our node features.

In this study, we introduce a computational approach named GRAF as a GNN-based approach utilizing attention mechanisms and network fusion simultaneously. To this end, first, we generate meta-path-based networks if the given network is heterogeneous or kept the networks from homogeneous networks as they are. After that, we obtain node-level and association-level attention. Node-level attention helps us learn the importance of each node to its neighbors, while association-level attention gives an overall interpretation of the impact of the association type. Combining both node-level and association-level knowledge, we weigh each association and fuse the networks having rich information about the associations. On the fused network, we utilize GCN incorporating all the node features that we might have plenty of, and optimize the model with backpropagation.

The contributions of our work are summarized as follows:
\begin{itemize}
\item We developed a novel GNN-based approach called GRAF with an attention-aware network fusion strategy. Our work operates on multiple networks where the associations are properly weighted utilizing attention mechanisms. Then GRAF runs GCN on the fused network for the desired prediction or clustering task.
\item  GRAF's network fusion step enables us to utilize the GNN-based model with a single network, while we enriched the multiple associations according to their importance. Our edge weighting considers both the importance of the association and each specific neighbor separately, thus prioritizing the edges properly. 
\item We conducted extensive evaluations to measure the performance of our model. We applied GRAF to multiple problems from different domains with different types of graph structures. Our results suggest that the utilization of the attention mechanism and network fusion strategy simultaneously could have superior performance as compared to the state-of-the-art methods. 
\end{itemize}

\section{GRAF}

GRAF is a computational approach for node classification with convolutions on graph-structured data utilizing attention mechanisms and multiple networks simultaneously (Figure \ref{gfig:01}).
Briefly, the first step is data collection and meta-path-based neighborhood generation. In the second step, we obtain node-level and association-level attention. Then, GRAF fuses multiple networks into one weighted network utilizing node-level and association-level attention mechanisms. In the last step, GRAF does the prediction using GCN with the convolutions on the fused network. In the following section, we explain each step of GRAF in detail.


\begin{figure*}[htbp]
  \includegraphics[width=\linewidth]{GRAF_pipeline}
  \caption{GRAF pipeline. GRAF first generates meta-path-based neighborhood. Then, it obtains node-level and association-level attention. Utilizing those attentions, GRAF fuses multiple networks into one weighted network. GRAF excludes low weighted edges and does the prediction with the graph convolutions on the fused network.}
 \label{gfig:01}
\end{figure*}




\subsection{Neighborhood Generation}

For simplicity, we will explain the details of the algorithm on a homogeneous network with one node type. Since our tool is applicable to heterogeneous networks, we might have more than one node type. In that case, we will have multiple transformation matrices (i.e., different ${W}$'s) for each node type, keeping the rest of the algorithm the same. 

Let's assume that we have a graph $ \mathcal{G}= (\mathcal{V},\mathcal{E})$ where $\mathcal{V}$ is a set of $n$ nodes, i.e., $\mathcal{V} = \{v_1, v_2, ..., v_n\}$, and $\mathcal{E}^{\phi}$ is a set of edges between nodes. The edge $(v_i,v_j) \in \mathcal{E}^{\phi}$ when $v_i$ and $v_j$ have an association based on association $\phi$ where $\phi \in \{1,2...\Phi\}$ where $\Phi$ is the total number of associations. Graph $\mathcal{G}$ is an undirected graph, that is, $(v_i,v_j) \in \mathcal{E}^{\phi} \iff (v_j,v_i) \in \mathcal{E}^{\phi}$.
  
  
We have a feature matrix $\mathcal{X} \in \mathbbm{R}^{nxf}$ where ${x_i}$ represents the original node features of $v_i \in  \mathcal{V}$ and $f$ is the feature size. We can represent $\mathcal{X}^\tau=(x_{1}^\tau\ x_{2}^\tau  \hdots 
x_{n}^\tau)$ where $\tau$ represents transpose.


$\mathcal{N}^{\phi}_{i}$ is the set of nodes having an association with the node $v_i$ based on the association $\phi$. Thus, we represent the neighborhood as 
$\mathcal{N}^{\phi}_{i}=\left\{v_{j}:\left(v_{i}, v_{j}\right) \in \mathcal{E}^{\phi}\right\}$


\subsection{Computing Node- and Association-level Attention}
To learn the importance of each neighbor per association and the impact of each association on the given prediction task, we had end-to-end training where we learn the node-level and association-level attention based on the final task. 


\noindent\textbf{Node-level attention.} To get the importance of each neighbor, we learned the node-level attention on the given graph. Node-level attention preserves the asymmetry so that the importance of node $v_i$ to $v_j$ is not necessarily the same as the importance of node $v_j$ to $v_i$. In this section, we used the word ``directed'' to show that the value is specific to an edge in the given order. Since each neighbor might also have different importance for different associations, we learned multiple node-level attention per association.

First, we did the transformation, which allows us to project the original features of different node types into the same feature space. For simplicity, we have one node type $\Theta$ where its transformation matrix is represented with ${M_{\Theta}}$, however, for the heterogeneous networks with more than one node type, we will have different transformation matrices. The projection representation is ${h_i} = {{M_{\Theta}}}.{ x_i}$ where $x_i$ is the original feature for node $v_i$ and $h_i$ is the corresponding projected features.

To aggregate the representations of neighbors as a learned weighted average, we learned the attention of each node. To this end, we have $e$, which computes a score for each edge. Since we have multiple associations, and we will have different node-level attention for each association, we represented multiple scoring for each corresponding association. For an edge $(v_i,v_j)$ on the association $\phi$, $e_{ij}^{\phi}$ represents the importance of the features of the neighbor $v_j$ to the node $v_i$ based on the association $\phi$, and the formulation is as follows:

\begin{equation} e_{ij}^{\phi} = \text{LeakyReLU}\left(({{a}}^{\phi})^{\tau}.{[h_i||h_j]}\right) \end{equation}


\noindent where ${\bf{a}}^{\phi}$ is node-level attention vector for the association $\phi$, and $||$ denotes vector concatenation. Then we normalized the attention scores across neighbors using softmax and obtained the node-level attention $\alpha_{ij}^{\phi}$. The attention of neighbor $v_j$ to the node $v_i$ based on the association $\phi$ for the edge $(v_i,v_j)$ is denoted as:
            
\begin{equation}\alpha_{ij}^{\phi}=\text{softmax}_j(e_{ij}^{\phi}) = \dfrac{\text{exp}(e_{ij}^{\phi})}{\sum_{k\in \mathcal{N}^{\phi}_{i}}\text{exp}\left(e_{ik}^{\phi}\right)}\end{equation} 


\noindent Then we calculated new node representation $z_i^{\phi}$ for the node $v_i$ on the association $\phi$ by computing the learned weighted average of the representations of neighbors followed by a non-linear activation function (represented by $\sigma$). The association-specific node embedding $z_i^{\phi}$ is represented as follows:

\begin{equation}z_i^{\phi}=\sigma\left(\sum_{v_j \in \mathcal{N}^{\phi}_{i}}\alpha_{ij}^{\phi}.{ h_j}\right)\end{equation} 

Considering the high variance in the data, multi-head attention is highly preferred for stabilizing the training. Multi-head attention is to repeat the node embedding generation multiple times and to concatenate the embeddings. Following \cite{velivckovic2017graph}, we preferred eight attention heads, thus, we concatenated eight embeddings to generate association-specific embeddings $z_i^{\phi}$. For simplicity, we did not involve multi-head attention in our formulation.

Combining all the association-specific node embeddings, we can get an association-specific embedding matrix $\mathcal{Z}^{\phi}$ where $(\mathcal{Z}^{\phi})^\tau=\left((z_1^{\phi})^\tau (z_2^{\phi})^\tau  \hdots  (z_n^{\phi})^\tau\right)$.


\noindent\textbf{Association-level attention.} To learn the overall importance of an association type, we add association-level attention following node-level attention. After learning the importance of each association, we used the weighted sum of embeddings for the prediction task.

Following \cite{wang2019heterogeneous}, we first transformed association-specific embedding through a nonlinear transformation (denoted by ${\bf M_0}$) and computed the importance of each embedding as the similarity of transformed embedding with an association-level attention vector (introduced by ${\bf q}$). The average of all the node embeddings in the association $\phi$ was considered as the importance of that association, represented by $f^{\phi}$:
\begin{equation}
f^{\phi} = \dfrac{1}{|\mathcal{V}|}. \sum_{v_i\in \mathcal{V}} {{q}}^{\tau}.\text{tanh}({{M_0}}.{ z_i^{\phi}})
\end{equation}
These parameters are shared for all the association and association-specific node embeddings.



Then we normalized across all $f^{\phi}$'s using softmax and obtained association-level attention $\beta^{\phi}$ as:
\begin{equation}
\beta^{\phi}=softmax_\phi(f^{\phi})=\dfrac{\text{exp}(f^{\phi})}{\text{exp}\left(\sum_{i\in \Phi}f^{i}\right)}
\end{equation}
representing the importance of the association $\phi$ for the prediction task. Then we generated the final node embedding $\mathcal{Z}$ as the learned weighted average of the node representations from all associations followed by a non-linear activation function (represented by $\sigma$). Since higher $\beta^{\phi}$ means a more important association, we used $\beta^{\phi}$ as the normalized importance of each association $\phi$ as follows:
\begin{equation}\mathcal{Z}=\sigma\left(\sum_{i \in \Phi}\beta^{\phi}.\mathcal{Z}^{\phi}\right)\end{equation}

Then we used the final embedding for a specific task. For our node classification tasks, we minimized the cross-entropy over all labeled nodes between the ground truth and the prediction as our loss function. Loss could be represented as $L = -\sum_{l \in \mathcal{Y}_{L}}Y_{l} ln(C.\mathcal{Z}_{l})$ where $C$ is the classifier, $\mathcal{Y}_{L}$ is the set of nodes with labels, and $Y_{l}$ and $Z_{l}$ are the corresponding node's label and embedding, respectively.


Using backpropagation as our proposed model, we optimized the model and learned the node embeddings. In this way, we utilized node-level attention followed by association-level attention in an end-to-end hierarchical way. Unlike HAN \cite{wang2019heterogeneous} which ends up with predictions at this stage, we continue with the learned attention. Due to the variation in the association-level and node-level attention, we repeated the whole process multiple times (10 times in our experiments) and used the average attention values further. 

\subsection{Attention-aware Network Fusion}
Utilizing node-level and association-level attention, we fused the networks into one, on which we did the convolution for the prediction task while leveraging the node features. To utilize GNN-based architecture, which operates on a single network, we fused multiple networks with learned attention values. 

Since association-level attention represents the importance of association, and node-level attention shows the importance of directed edge on that association, we computed directed edge scores of the directed edge from $v_i$ to $v_j$ (that is, $score_{\left(v_{i}, v_{j}\right)}$) as the weighted sum of available associations as follows:
\begin{equation}
score_{\left(v_{i}, v_{j}\right)}=\sum_{\phi \in \{1,2...\Phi\}
}  \left(
\beta^{\phi} \alpha_{ij}^{\phi}  \text{ if }\left(v_{i}, v_{j}\right) \in {\displaystyle \mathcal{E}^{\phi} } \right)
\end{equation}


\noindent These scores will be utilized to generate a weighted network, where weights are these directed edge scores. 


Using the attention as priors, we fused multiple networks into one weighted network to use in the prediction task. Intuitively, we give higher weight to the directed edge if it comes from a relatively more important network. But at the same time, we give higher weights to the directed edge if it has higher attention on networks of similar importance. Thus, the importance of node neighbors and the corresponding associations are taken into account, and this edge scoring allowed us to prioritize all directed edges properly. 

\subsection{Edge Elimination} 
Our fusion step keeps all the edges coming from multiple networks, even though they have very weak weights. 
This might cause a highly-dense network with many irrelevant edges, depending on the quality of the input networks. Considering that we included an edge elimination step, where we eliminated some portion of the edges. 

To achieve this, we normalized edge weights obtained in the previous step. To give the probability proportionally to their edge scores, we used the normalized weight of an edge as its probability to keep that edge in the network. In this way, we will randomly eliminate edges but keep the higher-quality ones. 
Intuitively, edges that are lowly attended and/or that have less important associations will be eliminated from the fused network.
    
Having rich information from multiple associations but also eliminating the irrelevant edges, the fused network is ready to be utilized in GNN-based approaches for the prediction task.

\subsection{Prediction}

After extracting features and generating a fused network from multiple data, we generated final node embeddings, which capture the topology of the network as well as node features. For this purpose, we utilized the GCN model \cite{kipf2016semi}, which includes self-edges into convolution and normalizes the sum of aggregated features across the neighbors. Since we already included self-edges in networks from different associations, we did not add them here again.

The input for a GCN model is a feature matrix $\mathcal{X} \in \mathbbm{R}^{nxf}$ where $f$ is the feature size, and the adjacency matrix $\mathcal{A} \in \mathbbm{R}^{nxn}$ defined as:
\begin{equation}
\mathcal{A}[i,j]=\left\{\begin{array}{lr}
score_{\left(v_{i}, v_{j}\right)} & \text { if }\left(v_{i}, v_{j}\right) \in \mathcal{E}
\\
0 & \text { otherwise }
\end{array}\right.
\end{equation}

\noindent The iteration process of the model is as follows: 
\begin{equation}
\mathcal{H}^{(l+1)}=\sigma\left(\mathcal{D}^{-\frac{1}{2}} \mathcal{A} \mathcal{D}^{-\frac{1}{2}} \mathcal{H}^{(l)} \mathcal{W}^{(l)}\right)
\end{equation}

\noindent with $\mathcal{H}^{(0)} = \mathcal{X}$ where
\begin{equation}
\mathcal{D}[i,i]=\sum_{j=1}^{n} \mathcal{A}[i,j],\end{equation}

\noindent $\mathcal{H}^{(l)}$ is the activation matrix in the $l^{th}$ layer, $\mathcal{W}^{(l)}$ is the trainable weight matrix in the $l^{th}$ layer and $\sigma$ is the activation function. Feature aggregation on the local neighborhood of each node was done by multiplying $\mathcal{X}$ by the $nxn$-sized scaled adjacency matrix $\mathcal{A}^{'}$ where $\mathcal{A}^{\prime}=\mathcal{D}^{-\frac{1}{2}} \mathcal{A} \mathcal{D}^{-\frac{1}{2}}$.

\noindent Using a 2-layered GCN model, we had the forward model giving the output $\mathcal{Z}$ where
\begin{equation}\mathcal{Z}=\operatorname{softmax}\left(\mathcal{A}^{\prime} \operatorname{ReLU}\left(\mathcal{A}^{\prime} \mathcal{X} \mathcal{W}^{(1)}\right) \mathcal{W}^{(2)}\right)\end{equation}

\noindent where $\mathcal{W}^{(1)} \in \mathbbm{R}^{kxh}$, $\mathcal{W}^{(2)} \in \mathbbm{R}^{hxc}$ were the trainable weights for the first and second layers, respectively, $h$ was the hidden layer size, and $c$ was the number of classes to predict. The loss function was calculated by cross-entropy error as in the previous section. Adam optimization \cite{kingma2014adam} was used as the state-of-the-art for stochastic gradient descent algorithm, dropout was added for the first GCN layer, and early stopping was used with 30 patience and with a minimum of 200 epochs. 

The overall process is shown in Algorithm \ref{alg:graf}. 
Bias vectors before applying the non-linearity are omitted for simplicity.





\begin{algorithm*}[!ht]
 \DontPrintSemicolon
  \setlength{\lineskip}{5pt}
  \KwInput{Graph $ \mathcal{G}= (\mathcal{V},\mathcal{E})$ where $\mathcal{V}$ is a set of $n$ nodes, i.e., $\mathcal{V} = \{v_1, v_2, ..., v_n\}$, and $\mathcal{E}^{\phi}$ is a set of edges between nodes \newline
  $(v_i,v_j) \in \mathcal{E}^{\phi}$ when $v_i$ and $v_j$ have based on the association $\phi$ a
  \newline
  $(v_i,v_j) \in \mathcal{E}^{\phi} \iff (v_j,v_i) \in \mathcal{E}^{\phi}$ (undirected graph)\newline
  feature matrix $\mathcal{X} \in \mathbbm{R}^{nxf}$ 
  }
  
  
  \KwOutput{Adjacency matrix $\mathcal{A}$}
  \For{$c \in \{1,2...C\}$}{
  
    \For{$\phi \in \{1,2...\Phi\}$} 
    {
        Node type-specific transformation: ${h_i} = {{M_{\Theta}}}.{ x_i}$ \;
        \For{$v_i \in \mathcal{V}$}
        {
            \For{$v_j \in \mathcal{N}^{\phi}_{i}=\left\{v_{j}:\left(v_{i}, v_{j}\right) \in \mathcal{E}^{\phi}\right\}$}
            {
                $e_{ij}^{\phi} = $LeakyReLU$({{a}}^{T}.{[h_i||h_j]})$  \;
                Weight coefficient $\alpha_{ij}^{\phi}=$softmax$_j(e_{ij}^{\phi}) = \dfrac{\text{exp}(e_{ij}^{\phi})}{\sum_{k\in \mathcal{N}^{\phi}_{i}}\text{exp}\left(e_{ik}^{\phi}\right)}$ \;
            }
        }
        Association-specific embedding ${z_i^{\phi}}=\sigma\left(\sum_{v_j \in \mathcal{N}^{\phi}_{i}}\alpha_{ij}^{\phi}.{ h_j}\right)$ \;
        
        Association-specific combined embedding ${ \mathcal{Z}^{\phi}}$ \;
        $f^{\phi} = \dfrac{1}{|\mathcal{V}|}. \sum_{v_i\in \mathcal{V}} {{q}}^{T}.$tanh$({{M_0}}.{ z_i^{\phi}})$  \;
    }
   Association weight coefficient $\beta^{\phi}=softmax_\phi(f^{\phi})=\dfrac{\text{exp}(f^{\phi})}{\text{exp}\left(\sum_{i\in \Phi}f^{i}\right)}$ \;

    Final embedding $\mathcal{Z}=\sigma\left(\sum_{i \in \Phi}\beta^{\phi}.\mathcal{Z}^{\phi}\right)$ \;
    Cross-entropy loss $L = -\sum_{l \in \mathcal{Y}_{L}}Y_{l} ln(C.\mathcal{Z}_{l})$ \;

    Backpropagation and final parameter obtention\;

    $\beta^{\phi}_{c} = \beta^{\phi}$ \; 
    $\alpha_{ijc}^{\phi} = \alpha_{ij}^{\phi}$\;

}
$\beta^{\phi} = \frac{1}{C}\sum_{c \in \{1,2...C\}}\beta^{\phi}_{c}$ \;
$\alpha_{ij}^{\phi} = \frac{1}{C}\sum_{c \in \{1,2...C\}}\alpha_{ijc}^{\phi}$ \;
$
\mathcal{A}[i,j]=\sum_{\phi \in \{1,2...\Phi\}
}  \left(
\beta^{\phi} \alpha_{ij}^{\phi}  \text{ if }\left(v_{i}, v_{j}\right) \in {\displaystyle \mathcal{E}^{\phi} } \right)
$
\caption{Attention-aware Network Fusion}
\label{alg:graf}\end{algorithm*}



\section{Experiments}

\subsection{Datasets}
 We applied our tool to two different prediction tasks: paper area prediction task from ACM dataset (\href{http://dl.acm.org}{http://dl.acm.org}) and movie genre prediction from IMDB data (\href{https://www.imdb.com}{https://www.imdb.com}). A detailed description of each dataset is shown in Table \ref{tab:features}.



\noindent\textbf{IMDB} For the movie genre prediction task, we collected and processed IMDB data from PyTorch Geometric library \cite{fey2019fast}. There are three node types: movie (M), actor (R), and director (D) along with two associations: movie-actor and movie-director. 
We generated two associations: MRM and MDM. The movies have three genre classes: action, comedy, and drama. Original training, validation, and test splits and node features for the movies (which are the elements of a bag-of-words) are from the library's data processing.        

\noindent\textbf{ACM} For the paper-type prediction task, we collected the ACM dataset from \cite{wang2019dgl}, and used the original training, validation, and test splits from there. There are three node types: paper (P), author (A), and subject (S). There are three associations: paper-author and paper-subject. We extracted 2 meta-paths: PAP and PSP. Labels of papers correspond to three areas: database, wireless communication, and data mining). Paper features are the elements of a bag-of-words representation of keywords.





\begin{table*}[ht]
  \caption{Datasets.}
  \label{tab:features}
  \begin{tabular}{|l|r|r|r|r|r|}
    \hline
    \textbf{Dataset} & \textbf{\# Samples}& \textbf{\# Features}& \textbf{\# Classes} & \textbf{Association}*&\textbf{\# Edges} \\
    \hline
    
    \multirow{2}*{IMDB} & \multirow{2}*{4,278} & \multirow{2}*{3,066} & \multirow{2}*{3} & MRM & 85,358\\
    &  &  & & MDM & 17,446\\
    \hline
   \multirow{2}*{ACM} & \multirow{2}*{3,025} & \multirow{2}*{1,870} & \multirow{2}*{3} & PAP & 29,281\\
    &  &  & & PSP & 2,210,761\\

    \hline
    
  \end{tabular}
    \begin{flushright} \begin{adjustwidth}{0cm}{0cm}
 {
 *A: Author, D: Director, M: Movie, P: Paper, R: Actor, S: Subject.
 }
 \end{adjustwidth}\end{flushright}
\end{table*}




\subsection{Baselines}
To show the performance of GRAF, we compared it against some state-of-the-art methods and did ablation studies showing the importance of specific steps against some variants of GRAF:


\textbf{GCN} \cite{kipf2016semi}: Since GCN works on a homogeneous graph, we tested all association-based networks and reported the best performance. Since we utilized GCN in our final prediction, this is also a variant of GRAF, where we do not have any attention mechanism and network fusion.

\textbf{GAT} \cite{velivckovic2017graph}: Since GAT involves attention mechanism while aggregating on a homogeneous graph, we tested all association-based networks and reported the best performance.

\textbf{GATv2} \cite{brody2021attentive}: Since GATv2 works on a homogeneous graph, we tested all association-based networks and reported the best performance. 

\textbf{HAN} \cite{wang2019heterogeneous}: As HAN operates on multiple heterogeneous graphs utilizing attention mechanisms, we ran and reported the prediction result. HAN is also a variant of GRAF, where we do not have network fusion and edge elimination steps.

\textbf{$\text{GRAF}_{node}$}: It is a variant of GRAF, which includes the utilization of only node-level attention in edge scoring (excluding association-level attention). Therefore the score function in methodology is replaced with:
\begin{equation}    
score_{\left(v_{i}, v_{j}\right)}=\sum_{\phi \in \{1,2...\Phi\}
}  \left(\alpha_{ij}^{\phi}  \text{ if }\left(v_{i}, v_{j}\right) \in {\displaystyle \mathcal{E}^{\phi} } \right)
\end{equation}
In other words, this variant assigns the same importance to each association.

\textbf{$\text{GRAF}_{asc}$}: It is a variant of GRAF including the utilization of only association-level attention in edge scoring (thus excluding node-level attention). Therefore the score function in methodology is replaced with:
\begin{equation}    
score_{\left(v_{i}, v_{j}\right)}=
\dfrac{\sum_{\phi \in \{1,2...\Phi\}
}  \left(\beta^{\phi}  \text{ if }\left(v_{i}, v_{j}\right) \in {\displaystyle \mathcal{E}^{\phi} } \right)}{\sum_{\phi \in \{1,2...\Phi\}
}  \left(1  \text{ if }\left(v_{i}, v_{j}\right) \in {\displaystyle \mathcal{E}^{\phi} } \right)}
\end{equation}
This variant assigns the same importance to each edge in an association-specific network. Thus, the scoring is equal to the association's learned weight for edges occurring in exactly one network, while it is the average of weights of associations for recurrent edges.

\textbf{$\text{GRAF}_{att}$}:  It is a variant of GRAF, which includes both node-level and association-level attention in edge weighting, however, it does not eliminate edges (i.e., uses all the fused edges).
    

\subsection{Implementation Details}
The original ACM dataset has 600 samples for training, 300 samples for validation, and 2125 samples for testing. Similarly, the original IMDB dataset has 400/400/3478 for training/validation/test splits. In addition to these splits, we kept 20\% of data for testing, and generated 20\%, 40\%, 60\%, and 80\% of the remained data as training splits, while using the rest as validation.

We used the training and validation splits to tune the hyperparameters (i.e., hidden layer size and learning rate) of the final model, where training and validation splits were randomly selected as stratified. We repeated the whole process 10 times for each hyperparameter combination and used the hyperparameter combination giving the best median macro-weighted F1 (macro F1) score on the validation data. Using this hyperparameter combination, the final model was built and evaluated 10 times on the test data, which was never seen during training and hyperparameter tuning. The evaluation metrics (macro F1 score, weighted-averaged F1 (weighted F1) score, and accuracy) were obtained from the median of these 10 runs.


\subsection{Classification}


We evaluated tools and reported their performance including three metrics: Macro-F1 score, Weighted-F1 score, and accuracy (Table \ref{tab:grafResults}). We repeated each run 10 times and included the standard deviations with the median of repeats.



\begin{table*}[ht]
  \caption{Prediction Results.}
  \label{tab:grafResults}

  \begin{tabular}{|l|l|l?{0.6mm}l|l?{0.6mm}l|l|}
    \hline
    & \multicolumn{2} {c?{0.6mm}} {\textbf {Macro F1}} & \multicolumn{2} {c?{0.6mm}} {\textbf {Accuracy}} & \multicolumn{2} {c|} {\textbf {Weighted F1}}\\
    \hline
    \textbf{Method} & \textbf{IMDB}& \textbf{ACM}& \textbf{IMDB}& \textbf{ACM} & \textbf{IMDB}&\textbf{ACM} \\
 \hline
 GCN & 0.587±0.00 & 0.915±0.00 & 0.585±0.00 & 0.915±0.00 & 0.585±0.00 & 0.915±0.00  \\
GAT & 0.568±0.00 & 0.910±0.00  & 0.569±0.00 & 0.910±0.00 & 0.569±0.00 & 0.910±0.00 \\
GATv2 & 0.568±0.01 & 0.909±0.01& 0.568±0.01 & 0.909±0.01& 0.568±0.01 & 0.909±0.01  \\
HAN & 0.609±0.00 & 0.920±0.01  & 0.611±0.00 & 0.919±0.01 & 0.611±0.00 & 0.919±0.01  \\
\hline 
$\text{GRAF}_{asc}$  & 0.563±0.00 & 0.819±0.04 & 0.569±0.00 & 0.818±0.04 &   0.569±0.00 & 0.818±0.04\\
$\text{GRAF}_{node}$  & 0.613±0.00 & 0.905±0.02 & 0.616±0.00 & 0.904±0.02   & 0.616±0.00 & 0.904±0.02 \\
$\text{GRAF}_{att}$  & \textbf{0.621±0.00}* & 0.923±0.00  & \textbf{0.623±0.00}* & 0.922±0.00   & \textbf{0.623±0.00}* & 0.922±0.00 \\
\hline 
GRAF & \textbf{0.621±0.00}* & \textbf{0.925±0.00} & \textbf{0.623±0.00}* & \textbf{0.924±0.00} & \textbf{0.623±0.00}* & \textbf{0.924±0.00}\\

   \hline
  \end{tabular}
    \begin{flushright} \begin{adjustwidth}{0cm}{0cm}
 {
 Prediction performance for two different tasks: paper type prediction task for ACM dataset and movie genre prediction task for IMDB dataset. $\text{GRAF}_{asc}$ and $\text{GRAF}_{node}$ denote the GRAF approach with only association-level attention and only node-level attention considered in edge weighting, respectively. $\text{GRAF}_{att}$ denotes the GRAF approach without edge elimination (that is, both node-level and association-level attention are considered in edge weighting). 
 GCN, GAT, and GATv2 are evaluated for every single network per association separately, and the best performance is reported in this table. Macro F1: Macro-averaged F1 score, Weighted F1: Weighted-averaged F1 score.
 *same results for GRAF and $\text{GRAF}_{att}$.}
 \end{adjustwidth}\end{flushright}
\end{table*}

According to our results, GRAF achieves the best performance with higher performance for all metrics for both datasets. 
Overall, integrative approaches (GRAF and HAN) had consistently better performance. We had an improved performance as compared to GCN, GAT, and GATv2 showing the importance of multiple association integration. As compared to HAN, we had improved performance showing the importance of the network fusion with learned attentions. The performance becomes lower when we utilized only node-level or only association-level attention, showing that our edge scoring works to properly weigh the edges. $\text{GRAF}_{att}$ mostly had comparable performance. IMDB dataset did not eliminate edges, but ACM datasets, which has bigger networks, utilized this step to get rid of the redundant edges. 

Overall, GRAF had better performance than other state-of-the-art methods, allowing to combine node- and association-level attention with network fusion in heterogeneous graphs.


\subsection{Clustering}
In addition to the prediction task, we utilized each tool to obtain node embedding, and perform K-means clustering using these embedding. We fixed $k$ to the number of classes in each prediction task. We reported the performance of tools in Table \ref{tab:grafResults2} with two metrics: Adjusted Rand Index and Normalized Mutual Information. We repeated 10 clusterings using the corresponding node embeddings of 10 runs from the prediction task and reported the median of repeats including the standard deviation. 



\begin{table*}[ht]
  \caption{Clustering Results.}
  \label{tab:grafResults2}

  \begin{tabular}{|l|c|c|}
    \hline
    \textbf{Method} & \textbf{ARI}& \textbf{NMI}\\
 \hline
GCN & 0.119±0.01 & 0.113±0.01\\
GAT & 0.125±0.01 & 0.119±0.00\\
GATv2 & 0.124±0.01 & 0.120±0.01\\
HAN & 0.168±0.00 & 0.157±0.00\\
 \hline
$\text{GRAF}_{asc}$  & 0.125±0.00 & 0.111±0.00\\
$\text{GRAF}_{node}$  & 0.156±0.05 & 0.143±0.05\\
$\text{GRAF}_{att}$  & \textbf{0.172±0.00}* & \textbf{0.159±0.00}*\\
\hline 
GRAF & \textbf{0.172±0.00}* & \textbf{0.159±0.00}*\\
   \hline
  \end{tabular}
    \begin{flushright} \begin{adjustwidth}{0cm}{0cm}
 {
 Clustering performance metrics for movie genre prediction task from IMDB dataset. $\text{GRAF}_{asc}$ and $\text{GRAF}_{node}$ denote the GRAF approach with only association-level attention and only node-level attention considered in edge weighting, respectively. $\text{GRAF}_{att}$ denotes the GRAF approach without edge elimination (that is, both node-level and association-level attention are considered in edge weighting). 
 ARI: Adjusted Rand Index, NMI: Normalized Mutual Information. * same results for GRAF and $\text{GRAF}_{att}$.}
 \end{adjustwidth}\end{flushright}
\end{table*}

According to our results, GRAF consistenlty outperforms all other tools in terms of both evaluation metrics. 
As compared to GCN, we had improved performance in all cases, showing the importance of our attention-aware network fusion strategy. 
$\text{GRAF}_{node}$ and $\text{GRAF}_{asc}$ had lower performance, showing that including node-related and association-related attentions is important. Even though HAN had a better performance than the methods which are not integrative (i.e., GCN, GAT, and GATv2), GRAF outperformed all. This shows the importance of the inclusion of network fusion with attention mechanisms. 

Overall, our evaluations on different domains showed that GRAF increased the utilization of the attention mechanism and network fusion strategy simultaneously with superior performance over the state-of-the-art methods. This analysis suggests that we can have better node representation from GRAF having rich information of multiple associations.

\subsection{Ablation Studies}
In previous analysis, we compared against the modified versions of GRAF to show the impact of each step on performance. In addition to them, here, we analyzed the robustness of GRAF when we changed the input data splits. For this reason, we generated four different data splits. We excluded 20\% of the data as test for all setups. Then we divided the remaining data with different training and validation portions ($x$\% of the remaining data was used as training split where $x$ was 20\%, 40\%, 60\%, and 80\% separately). We repeated each evaluation 10 times and reported the median of those runs including the standard deviation. Table \ref{tab:grafResults-split} shows the performance for each different setup on IMDB dataset.


\begin{table*}[ht]
  \caption{Different Splitting Analysis}
  \label{tab:grafResults-split}
  \begin{tabular}{|l|c|c|c|c|c|}
    \hline
    \textbf{Dataset} & \textbf{IMDB}& \textbf{20\%}& \textbf{40\%}& \textbf{60\%} & \textbf{80\%} \\
 \hline
GCN & 0.587±0.00 & 0.588±0.00 & 0.639±0.00 & 0.637±0.00 & 0.654±0.00  \\
GAT & 0.568±0.04 & 0.583±0.01 & 0.635±0.01 & 0.642±0.01 & 0.654±0.01  \\
GATv2 & 0.568±0.01 & 0.588±0.00 & 0.637±0.01 & 0.644±0.01 & 0.650±0.01  \\
HAN & 0.609±0.00 & 0.640±0.06 & 0.658±0.00 & 0.654±0.01 & 0.667±0.01  \\
 \hline
$\text{GRAF}_{asc}$  & 0.563±0.00 & 0.603±0.00 & 0.617±0.00 & 0.595±0.00 & 0.633±0.01  \\
$\text{GRAF}_{node}$  & 0.613±0.00 & 0.625±0.00 & 0.645±0.00 & 0.641±0.00 & 0.653±0.00  \\
$\text{GRAF}_{att}$  & \textbf{0.621±0.00}* & 0.638±0.00 & 0.651±0.01 & 0.654±0.00 & \textbf{0.681±0.01}*  \\
\hline 
GRAF & \textbf{0.621±0.00}* & \textbf{0.644±0.00} & \textbf{0.662±0.00} & \textbf{0.663±0.01} & \textbf{0.681±0.01}* \\
   \hline
  \end{tabular}
    \begin{flushright} \begin{adjustwidth}{0cm}{0cm}
 {
Macro F1 scores for different splitting in paper type prediction task of IMDB dataset. 
 $\text{GRAF}_{asc}$ and $\text{GRAF}_{node}$ denote the GRAF approach with only association-level attention and only node-level attention considered in edge weighting, respectively. $\text{GRAF}_{att}$ denotes the GRAF approach without edge elimination (that is, both node-level and association-level attention are considered in edge weighting. x\% means that x\% of data is used as training split, while the remaining is used as validation (20\% of the data is separated for test before this splitting). GCN, GAT, and GATv2 are evaluated for every single network per association separately, and the best performance is reported in this table. * same results for GRAF and $\text{GRAF}_{att}$.}
 \end{adjustwidth}\end{flushright}
\end{table*}

According to our results, GRAF outperformed other methods consistently regardless of the training data percentage. Generally, we had a better performance from methods when we had more training.

\subsection{Interpretation of Results} \label{sec:interpretation}
GRAF allows us to interpret the results of given tasks. For IMDB data, the attention for MRM association was 0.44, while it was 0.56 for MDM. This was consistent with the selected performance of GCN, GAT and GATv2 methods. We had better performance when we used MRM association than MDM for all these methods. This was similar for ACM dataset, where we had 0.66 for attention for PAP association, while we had 0.34 for attention of PSP. All GCN, GAT, and GATv2 had higher performance for PSP association, so we reported the results from PSP. This was not surprising, since paper-subject association will be more important for the determining the paper research area prediction task, and same for the importance of director to determine the movie genre rather than actor.


\section{Conclusion}
In this study, we developed a computational approach to utilize multiple heterogeneous networks with attention mechanisms and network fusion. The proposed GRAF showed increased performance over the state-of-the-art methods and the variants of the tool itself. Our tool learns the importance of node-level and association-level relations. With network fusion, we were able to utilize a GCN model with a fused network, but at the same time, we enriched the fused network with multiple associations.

\section*{Acknowledgments}

This work was supported by the National Institute of General Medical Sciences of the National Institutes of Health under Award Number R35GM133657 to SB.
\end{multicols}


\bibliography{bib}

\bibliographystyle{unsrt}

\end{document}
