\section{Experiment Setup}
\label{sec:experiment_setup}
\subsection{Dataset}
We conducted experiments on a large-scale 3D human dance motion dataset AIST++ \cite{li2021learn}. It contains $1408$ dance sequences with a total duration of $18694$ seconds, and 10 dance genres (e.g., pop and street jazz). For motion data, we converted the SMPL representation to $219$-D motion features, including the flattened rotation matrix of $24$ body key points and the position of the root. We sampled motions at $20$ frames per second. Moreover, we adopted mirror motion schemes for data augmentation.

\subsection{Implementation Details}
The experiments were conducted on a TitanX GPU. In the training process, we set the batch size to $128$ and used the Adam optimizer \cite{kingma2014adam}. The model was trained for $100$k iterations, and the learning rate was set to $1e$-$4$ at the beginning of training, and decayed to $1e$-$5$ and $1e$-$6$ after $35$k and $60$k iterations, respectively. All Transformers have $10$ attention heads with $640$ hidden size, the dimension of hidden layers of FNN is $1920$. For $\mathcal{E}_M$ and $\mathcal{E}_X$, the number of attention layers is $3$. For $\mathcal{G}$, the number of attention layers is $12$, and a $2$-layer MLP with $1920$ hidden layer dimension is used in the conditional layer normalization. The dropout rate is set to $0.1$.

\subsection{Evaluation Metrics}
%
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/demo.pdf} 
    \caption{\textbf{Dance Examples.} The dance motion synthesized by our model on different styles of music. }
    \label{fig:demo}
\end{figure*}
%
\begin{table*}[t]
    \centering
    \caption{\textbf{Quantitative and Qualitative Results on AIST++ Test Set.} Comparison of our method to DanceRevolution \cite{huang2020dance} and FACT \cite{li2021learn}. AIST++(random) means random-paired motion-music data.}
    \label{tab:analysis_real}
    \begin{tabular}{ccccccccc} 
        \hline
         & \multicolumn{2}{c}{Quality} & \multicolumn{2}{c}{Diversity} & \multicolumn{2}{c}{Motion-Music Corr} &
         \multicolumn{2}{c}{User Study} \\
         &  $\text{FID}_{k} \downarrow$ & $\text{FID}_{g} \downarrow$ & $\text{Dist}_{k} \uparrow$ & $\text{Dist}_{g} \uparrow$ &  $\text{BeatAlign}\uparrow$ & $\text{Acc}_S\uparrow$
         & Human Score$\uparrow$ & Ours WinRate \\
        \hline
        AIST++ & $-$ & $-$ & 7.90 & 8.21 & 0.278 & 0.67 & $4.23 \pm 0.18$ & 4.3\% \\
        AIST++(random) & $-$ & $-$ & 7.90 & 8.21 & 0.194 & - & $3.42 \pm 0.46$ & 35.2\% \\
        \hline
        DanceRevolution & 41.51 & 22.21 & 4.43 & 4.16 & 0.204 & 0.31 & $2.59\pm0.17$ & 78.5\% \\
        FACT & 38.13 & 20.31 & 5.78 & 6.18 & 0.222 & 0.35 & $2.36\pm0.35$ & 81.0\% \\
        Ours & \textbf{25.10} & \textbf{15.63} & \textbf{6.81} & \textbf{7.04} & \textbf{0.223} & \textbf{0.50} & \textbf{3.03} $\pm$ \textbf{0.29} & $-$\\
        \hline
    \end{tabular}
\end{table*}

\subsubsection{Motion Quality}
Following \cite{li2021learn}, we calculated the kinetic and geometric Fr{\'e}chet Inception Distances (FIDs) between generated motion and real motion to evaluate the motion quality. The corresponding FIDs are denoted as ${\eqword{FID}_k}$ and ${\eqword{FID}_g}$, respectively. 
    
\subsubsection{Motion Diversity}
To measure the diversity of generated motion, we adopted the metric proposed by \cite{li2021learn} to calculate the average Euclidean distance of the kinetic and geometric features, denoted as $\eqword{Dist}_k$ and $\eqword{Dist}_g$, respectively. 
    
\subsubsection{Beat Alignment}
We followed \cite{li2021learn} to use beat alignment score to evaluate how the generated motion align with the music beats. It is the average distance between each kinematic beat and its nearest music beat:
\begin{equation}
    \eqword{BeatAlign}=\frac{1}{m} \sum_{i=1}^{m} \exp \left(-\frac{\min _{\forall t_{j}^{y} \in B^{y}}\left\|t_{i}^{x}-t_{j}^{y}\right\|^{2}}{2 \sigma^{2}}\right),
\end{equation}
where $\left\{t_{i}^{x}\right\}$ and $\left\{t_{j}^{y}\right\}$ are kinematic and music beats respectively. The normalization parameter $\sigma$ is set to $1$.

\subsubsection{Style Matching}
We trained a SVM classifier on AIST++ to measure whether the style of generated dance matches the input music. The classifier takes extracted kinetic and geometric features as inputs, and we calculate the classification accuracy $\eqword{Acc}_S$ as the metric.