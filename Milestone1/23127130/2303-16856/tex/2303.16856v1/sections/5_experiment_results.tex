\section{Results}
\label{sec:experiment_results}
\fig\ref{fig:demo} shows the dance synthesis results for the music pieces from the AIST++ test set. Different styles of realistic dance movements were generated by our method.

We compared our model with two strong baselines. \emph{DanceRevolution} \cite{huang2020dance} is a seq2seq-based dance generation model with a curriculum learning strategy to alleviate error accumulation during long-term motion generation. \emph{FACT} \cite{li2021learn} is a full-attention cross-modal Transformer. Both baselines are reproduced using official released code with the optimal settings.

\subsection{Quantitative Evaluation}
Table \ref{tab:analysis_real} summarizes the objective results. Despite not using paired data, our method still achieves the lowest FID valued. The diversity of our generated motion is comparable to the strong baseline methods. The style classification accuracy of our model reaches $50\%$, which significantly exceeds strong baseline methods by $15\%$.

\subsection{Long-term Dance Synthesis}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/long.pdf} 
    \caption{\textbf{Experiment of Long-term Dance Synthesis.} Kinetic FID curves of different baselines over time.}
    \label{fig:long}
\end{figure}

\begin{table*}[t]
    \centering
    \caption{\textbf{Ablation Study.} \emph{paired} and \emph{unpaired} mean training with paired and unpaired data, respectively. \emph{MT} denotes fusion using multimodal Transformer \cite{li2021learn}. \emph{CLN} denotes fusion using conditional layer normalization \cite{kexuefm-7124}. \emph{Long} denotes the long-history motion encoder.}
    \label{tab:ablation_real}
    \begin{tabular}{lcccccccc} 
        \hline
         & \multicolumn{2}{c}{Quality} & \multicolumn{2}{c}{Diversity} & \multicolumn{2}{c}{Motion-Music Corr} &
         \multicolumn{1}{c}{User Study} \\
         &  $\text{FID}_{k} \downarrow$ & $\text{FID}_{g} \downarrow$ & $\text{Dist}_{k} \uparrow$ & $\text{Dist}_{g} \uparrow$ &  $\text{BeatAlign}\uparrow$ & $\text{Acc}_S\uparrow$ & Human Score$\uparrow$ \\
        \hline
        paired only MT & 81.32 & 26.38 & 4.71 & 4.00 & 0.219 & 0.14 & $2.25 \pm 0.29$\\
        paired only CLN & 61.47 & 21.54 & 5.58 & 4.41 & 0.220 & 0.51 & $2.40 \pm 0.31$\\
        paired w/o. Long & 33.23 & 16.25 & 5.19 & 5.61 & 0.222 & 0.52 & 2.96 $\pm$ 0.33 \\
        paired & \textbf{20.01} & \textbf{13.13} & \textbf{7.12} & 6.87 & 0.223 & \textbf{0.54} & \textbf{3.06} $\pm$ \textbf{0.35} \\
        unpaired & 25.10 & 15.63 & 6.81 & \textbf{7.04} & \textbf{0.223} & 0.50 & 3.03 $\pm$ 0.29 \\
        \hline
    \end{tabular}
\end{table*}
We conducted experiments to synthesize long-term dance. Because the test music clips in AIST++ are all within $30$ seconds, we repeat each sample to obtain music clip longer than $60$ seconds, and use them as test inputs. For each long clip, we determine an anchor per $3$ seconds and take an $1$-second generated motion clip centered on it. Then we calculate the $\eqword{FID}_k$ using the motion clips of each anchor. Finally, we get the $\eqword{FID}_k$ curve to measure the long-term generation performance of different baselines. As shown in Figure \ref{fig:long}, motions synthesized by our method have the lowest $\eqword{FID}_k$ score, and $\eqword{FID}_k$ remains stable over time. The model are difficulty to predict the correct subsequent dance of seed motion, so the $\eqword{FID}_k$ in the first few seconds is relatively high. Moreover, style consistency study of long-term generation was included in the supplementary materials.

\subsection{User Study} 
We organized a user study to evaluate the performance of our system qualitatively. The study was conducted with $11$ participants and consists of win rate scale and human score. For the win rate scale, each participant was requested to compare $40$ disordered pairs of motion and choose the better one in each pair. Each pair contains two motion sequences synthesized by our model and a baseline method, respectively. The final result is the probability that our model wins. The human score is a five-point Likert scale. Each participant was asked to grade the generated motions of each model from $1$ to $5$, with $1$ being the worst, and $5$ being the best.
Results in Table \ref{tab:analysis_real} show that our method surpasses the compared methods with at least $78.5\%$ wining rate. 

\subsection{Ablation Study}
We conducted ablation study to demonstrate the effectiveness of our proposed strategies. Results are shown in Table \ref{tab:ablation_real}, where \emph{MT} denotes fusion via multimodal Transformer \cite{li2021learn}, \emph{CLN} denotes fusion using conditional layer normalization, \emph{Long} means using long-history encoder, and \emph{paired} denotes applying paired learning scheme.

\subsubsection{Unpaired Learning Scheme}
We compared the results of different learning schemes (\emph{paired} and \emph{unpaired}) in Table \ref{tab:ablation_real}. Our proposed unpaired learning scheme can achieve comparable results even if only the unpaired data was used.

\subsubsection{Long-history Encoder}
Comparison of \emph{paired} and \emph{paired w/o. Long} in Table \ref{tab:ablation_real} showed that the long-history encoder can improve the quality and style classification accuracy of generated motions. Such results indicate that our long-history attention mechanism can alleviate the error accumulation problem caused by the autoregressive framework.

\subsubsection{Conditional Layer Normalization}
We chose two representative approaches of the multimodal Transformer (MT) and conditional layer normalization (CLN) to study the effects of different music context fusion strategies. We constructed a test setting that the model is only composed of a music encoder and a motion generator, and applied the paired training scheme. Then, the only variable is the fusion method. The comparison results of \emph{paired only MT} and \emph{paired only CLN} in Table \ref{tab:ablation_real} showed that CLN is superior than MT. Additionally, compared to MT, CLN can reduce the computational consumption. Assuming that the lengths of the input music and historical motion are $m$ and $n$ respectively, the computational complexity of the self-attention layer in MT-based Transformer is $O((m+n)^2)$, while that of the CLN-based Transformer is $O(n^2)$.