\section{Introduction}
\label{sec:introduction}
Dance is a full-body performing art consisting of rhythmic human movements. It is an important way for humans to inspire creativity and convey various emotions. There are strong demands for high-quality 3D dance animations in some industries, such as films, video games, and virtual avatars. Therefore, given a piece of music, automatically synthesizing natural-looking dance movements becomes a research hotspot recently.

Because of recent advances in deep learning, it could be possible to train a complex model using paired music-dance data, which achieves eye-catching results successfully \cite{alemi2017groovenet,li2021choreographer,chen2021choreomaster,lee2019dancing2music,siyao2022bailando,guillermo2021transflower}. These data-driven methods are supervised and rely on paired training data, where motions are strictly aligned with the music. However, collecting and aligning paired data is labor-intensive and expensive. Limited paired dataset hinders the capability of neural network. In contrast, unpaired data are easier to obtain, because music and motions are not required temporally frame-to-frame alignment. And so, it would be of great benefit if we can exploit the large amount of unpaired data to achieve model training.

Moreover, many existing systems fail to generate realistic long sequences of dance movements \cite{lee2018listentodance,ren2019poseperceptualloss,li2021choreographer,li2022danceformer}. Specifically, auto-regressive models like recurrent neural network (RNN) are widely applied to the dance synthesis \cite{lee2018listentodance,ren2019poseperceptualloss}. But the generated motions tend to be quickly stiff within a few seconds due to an incremental accumulation of prediction errors brought by the auto-regressive manner. Although recent developments of Transformer-based models \cite{li2021choreographer,li2022danceformer} show a powerful ability of cross-modal modeling, these systems generate dance based on short historical motions and ignore the long-term choreographic structure, resulting in inconsistent styles and rigid movements. In a practical context, we need an efficient mechanism to synthesize long-term dance motions robustly.

In this paper, we focus on the long-term 3D dance motion synthesis, accompanying with the music content on both rhythm and style levels. For the data-limited challenge above, we propose an efficient unpaired data training scheme based on the disentanglement of beats and styles of both music and motions. More specifically, we first divide the input interfaces of our system into beat and style two parts. Beat input could be identified from a given piece of music or a dance, while a Transformer-based style encoder is designed to extract style input by feeding style exemplars of music and motion. And we fuse the style input into the generator in a time-agnostic way via conditional layer normalization (CLN) \cite{kexuefm-7124}. During training, we obtain beats from the ground-truth dance instead of music, while the style exemplars of music and motion are randomly sampled pieces, which are consistent with the style label of the ground-truth dance. The whole scheme does not require any paired data, and it only requires the easy-to-get style labels. As for the inference, we identify beats from the given music to replace the motion beats used during training, and the given music clip and a fixed style-consistent dance are fed into the style encoder as the style exemplars. Both quantitative and qualitative evaluations show that our system performs comparably to strong baseline methods that use paired data.

For generating natural-looking long-term dance movements, we develop a novel long-history attention strategy to capture and maintain a choreographic structure, which effectively alleviates error accumulation of auto-regressive manner. The whole strategy includes two phases. We first use the latest motions to query a long-range historical dance via a newly designed attention computation and get a long-history embedding. Then, the embedding is fused into the generation pipeline via multimodal adaptation gate (MAG) \cite{rahman2020integrating}, explicitly give a historical hint to the synthesis of motions. Experiments demonstrate that our system robustly synthesize realistic long-term dance movements.

In summary, our contributions are three-folds:
\begin{itemize}
    \item We develop a novel 3D dance synthesis system that can robustly generate impressive dances accompanying with a piece of long music. To our best knowledge, it is the first system that successfully uses only unpaired data to achieve music-driven dance generation.
    \item We propose an efficient unpaired data training scheme to alleviate the problem of lacking data.
    \item We devise an innovative long-history attention strategy, which explicitly maintains the temporal coherence between music and dance in the long-term time.
\end{itemize}