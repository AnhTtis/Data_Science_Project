A central challenge in human pose estimation, as well as in many other machine learning and prediction tasks, is the generalization problem: a well-trained prediction network model often experiences severe performance degradation on new test samples. One major cause for this generalization problem is that, the prediction network, once successfully trained with labeled samples, remains fixed and performs the inference process in a purely feed-forward manner. Since the ground-truth value for the test sample is not available, it does not have the capability to characterize the prediction error, generate feedback information from the test sample, and correct the prediction error on the fly for each individual test sample. We believe that this  capability of prediction error characterization and correction is critical for the generalization performance of the prediction network.
In this work, we propose to develop a new self-correctable and adaptable inference  (SCAI) network to achieve this unique capability.
Specifically, by designing a fitness feedback network which maps the output result of the  prediction network back to the input sample, we introduce the self-matching error from this forward-feedback prediction loop. Interestingly, we find that this self-matching error is highly correlated with the actual prediction error. Using this self-matching error as feedback, we learn a correction network which is able to adaptively correct the prediction error based on the specific characteristics of the test sample. 
Furthermore, this self-matching error can be also used to learn and adapt the network model within the context of human pose estimation. 
We partition the human body joints into structural groups and apply this self-correctable prediction network.  Our extensive experimental results on human pose estimation demonstrate that the proposed SCAI network is able to significantly improve the generalization capability and performance of human pose estimation.




Note that, it requires fitness to guide the correction from vanilla prediction to a more accurate one. Naturally, the prediction error can serve as the fitness. However, during testing, the true value of $\mathbf{v}$ is not available. Thus, we do not know the exact prediction error $E_p = {||\mathbf{v}-\mathbf{\hat{v}}||}_2$. We notice that during testing, the only entity with a known value is the input sample. To address this issue, we introduce a fitness feedback network (FFN) $\mathbf{\Gamma}$ which generates a self-referential error $\mathbf{e_s}$ from input samples to provide feedback for correction, denoted by





% \textbf{(1) self-referential Error}. A  deep neural network $\mathbf{\Phi}$, already  being fully optimized on the training set, often suffers from significant performance degradation when applied to new or unseen test samples. 
% As illustrated in Figure \ref{fig:overview}, the prediction network $\mathbf{\Phi}$ is learned to predict $\mathbf{v}$ from $\mathbf{u}$, which is expressed as
% $
%   \mathbf{\hat{v}} = \mathbf{\Phi}(\mathbf{u}),
% $
% and the corresponding prediction error is denoted by
% \begin{equation}
% E_p = {||\mathbf{v}-\mathbf{\hat{v}}||}_2.
% \end{equation}
% During testing, the true value of $\mathbf{v}$ is not available. Thus, we do not know the exact prediction error $E_p$. Our proposed SCAI method aims to characterize  this prediction error without need to know the ground-truth.
% Note that, during testing, the only entity with a known value is the input sample. 
% Our idea is to introduce a fitness feedback network (FFN) $\mathbf{\Gamma}$ which maps the prediction result $\mathbf{\hat{v}} = \mathbf{\Phi}(\mathbf{u})$ back to the input domain
% \begin{equation}
%   \mathbf{\hat{u}} = \mathbf{\Gamma}(\mathbf{\hat{v}}) = \mathbf{\Gamma}(\mathbf{\Phi}(\mathbf{u})).
% \label{formula:direct}
% \end{equation}
% The FFN can be considered as the inverse operator of
% $\mathbf{\Phi}$. 
% If the prediction network $\mathbf{\Phi}$ and the FFN $\mathbf{\Gamma}$ are both well trained and accurate, then the error between the prediction result $\mathbf{\hat{u}}$ of the prediction-feedback loop and the original input $\mathbf{u}$ should be 0.
% In other words, this error can be a strong indicator for the prediction error of network $\mathbf{\Phi}$.
% Motivated by this observation, we introduce the following self-referential error
% \begin{equation}
%   E_{s} = {||\mathbf{u}-\mathbf{\hat{u}}||}_2 = {||\mathbf{u}-\mathbf{\Gamma}(\mathbf{\Phi}(\mathbf{u}))||}_2.
% \end{equation}
% In Section \ref{sec-pose}, we will demonstrate that this self-referential error $E_s$ is highly correlated with the actual network prediction error $E_p$. As discussed in Section 1, this allows us to estimate the network prediction error using the self-referential error, which can be computed directly on the input test sample $\mathbf{u}$ with the prediction-feedback networks $\mathbf{\Phi}$ and $\mathbf{\Gamma}$.  
% In the next section, we will explain the specific training details of these two networks.

% \textbf{(2) Self-Correctable Inference}. 
% Since the self-referential error $E_s$ is highly correlated with the actual prediction error $E_p$, it provides important feedback information or guidance for us to adjust the prediction result $\mathbf{\hat{v}} = \mathbf{\Phi}(\mathbf{u})$.
% To this end, we propose to learn a correction network $\mathbf{C}$ to correct the prediction error and improve the prediction result from  $\mathbf{\hat{v}}$ to $\mathbf{\tilde{v}}$ using the self-referential error vector $\mathbf{e_s}$ as the feedback
% \begin{equation}
%   \mathbf{\tilde{v}} = \mathbf{C}(\mathbf{\hat{v}}, \mathbf{e_s}).
% \end{equation}
% As illustrated in Figure \ref{fig:overview}, we apply the fitness feedback network $\mathbf{\Gamma}$ after the correction network to examine its prediction error. Therefore, in this case, the self-referential error vector specifically defined after the correction network:
% \begin{equation}
% \mathbf{e_s} = \mathbf{u} - \mathbf{\Gamma}(\mathbf{C}(\mathbf{\Phi}(\mathbf{u})))
% \end{equation}


% The task of FFN is to evaluate if the corrected prediction $\tilde{H}_D$ is accurate or not by computing the corresponding self-referential error.

% In doing so, together with $H_B$ and $H_C$, it performs backward prediction from $\tilde{H}_D$ to one of the original input $H_A$, 
% $\hat{H}_A = \mathbf{\Gamma}(H_B, H_C, \tilde{H}_D).$
% Here, we select $H_A$ since it has the highest confidence score obtained by the current human pose estimation. 
% If the FFN and correction network are successfully trained, the self-referential error will be closed to 0. Thus, the self-referential error is defined as 
% \begin{eqnarray} 
%     E_{s} &=& \|H_A - \mathbf{\Gamma}(H_B, H_C, \mathbf{C}(\tilde{H}_D; \hat{H}_A, H_A))\|_2. 
%     % &=& \|H_A - \mathbf{\Gamma}(H_B, H_C, \mathbf{C}(\mathbf{\Phi}(H_A,H_B,H_C); \tilde{H}_A, H_A))\|_2.
% \label{eq:selfloss1}
% \end{eqnarray}


% $\mathbf{\hat{u}} = \hat{H}_A$. Thus, the self-referential error used to guide the correction is defined as  
% \begin{equation}
% \mathbf{e_s} = H_A - \hat{H}_A.
% \label{formula:pose error}
% \end{equation}


% (c) The network model in \cite{DBLP:conf/eccv/KanCLH22} remains fixed during inference as most existing methods. In our work, we have developed the new idea of learnable inference where the self-referential error is used a loss function to refine the network model. 

% (d) In our SCAI method, we developed the fitness feedback network, which is different from the verification network in \cite{DBLP:conf/eccv/KanCLH22}. The role of the verification network is limited to the evaluation on the training side and the inference stage, but our FFN provides directional input for the correction network, so that the correction process is directional and learnable. 

% Based on the pre-training of the verification network in \cite{DBLP:conf/eccv/KanCLH22}, we add iterative training with the correction network, and use the new input provided by the correction network to strengthen the generalization ability of the feedback network, and the adaptation of the model to the iterative correction in the inference stage is solved by training in multiple stages.

% The input heatmap used in the search process is generated by Gaussian distribution, which will generate errors when calculating the self-referential error, 
% and the learned correction network in our method will generate a suitable heatmap to completely eliminate this error. 



%To characterize the keypoint prediction error on the test set without ground truth, we establish the self-feedback error using the fitness feedback network FFN, as shown in (\ref{formula:pose error}). Different from verification network in \cite{DBLP:conf/eccv/KanCLH22}, which is required to have a stronger performance than baseline prediciton network, our FFN is used solely for construct a self-feed back error to characterize the prediction error and reference the correction of the prediction result for the test samples. Our self-feedback error characterized with FFN can improve the generalization capability of the baseline network, since it is highly correlated with the prediction accuracy on the test set where ground-truth is unavailable. Thus, it can serves as a optimization objective and a guidance role for correcting and refining the prediction results on the test side. 

%\textbf{Correction Network System.} The self-feedback-based dynamic correction aims to correct the current prediction result toward the objective of reducing self-feedback error derived from the current test samples, further to improve the prediction accuracy and generalization capability. 