@inproceedings{potdar2014exploratory,
  title={An exploratory study on self-admitted technical debt},
  author={Potdar, Aniket and Shihab, Emad},
  booktitle={2014 IEEE International Conference on Software Maintenance and Evolution},
  pages={91--100},
  year={2014},
  organization={IEEE}
}

@article{choudhry_tlmote_2022,
  title        = {TLMOTE: A Topic-based Language Modelling Approach for Text Oversampling},
  volume       = {35},
  doi          = {10.32473/flairs.v35i.130676},
  journal      = {The International FLAIRS Conference Proceedings},
  author       = {Choudhry, Arjun and Susan, Seba and Bansal, Anmol and Sharma, Anubhav},
  year         = {2022},
  month        = {May}
}

@electronic{vishal_tech_debt,
 author   = {Vishal Dalal and Krish Krishnakanthan and Björn Münstermann and Rob Patenge},
 title     = {Tech debt: Reclaiming tech equity},
 url       = {https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/tech-debt-reclaiming-tech-equity},
 organization = {McKinsey},
 month = {October},
 year = {2020}
}

@inproceedings{leekha_multi-task_2020,
	location = {Cham},
	title = {A Multi-task Approach to Open Domain Suggestion Mining Using Language Model for Text Over-Sampling},
	isbn = {978-3-030-45442-5},
	doi = {10.1007/978-3-030-45442-5_28},
    year = {2020},
	series = {LNCS},
	abstract = {Consumer reviews online may contain suggestions useful for improving commercial products and services. Mining suggestions is challenging due to the absence of large labeled and balanced datasets. Furthermore, most prior studies attempting to mine suggestions, have focused on a single domain such as Hotel or Travel only. In this work, we introduce a novel over-sampling technique to address the problem of class imbalance, and propose a multi-task deep learning approach for mining suggestions from multiple domains. Experimental results on a publicly available dataset show that our over-sampling technique, coupled with the multi-task framework outperforms state-of-the-art open domain suggestion mining models in terms of the F-1 measure and {AUC}.},
	pages = {223--229},
	booktitle = {Advances in Information Retrieval},
	publisher = {Springer International Publishing},
	author = {Leekha, Maitree and Goswami, Mononito and Jain, Minni},
	date = {2020},
	langid = {english},
	keywords = {Deep learning, Multi-task learning, Open domain suggestion mining, Over-sampling techniques},
}


@misc{prenner_making_2021,
	title = {Making the most of small Software Engineering datasets with modern machine learning},
	abstract = {This paper provides a starting point for Software Engineering ({SE}) researchers and practitioners faced with the problem of training machine learning models on small datasets. Due to the high costs associated with labeling data, in Software Engineering,there exist many small ({\textless} 1 000 samples) and medium-sized ({\textless} 100 000 samples) datasets. While deep learning has set the state of the art in many machine learning tasks, it is only recently that it has proven effective on small-sized datasets, primarily thanks to pre-training, a semi-supervised learning technique that leverages abundant unlabelled data alongside scarce labelled data.In this work, we evaluate pre-trained Transformer models on a selection of 13 smaller datasets from the {SE} literature, covering both,source code and natural language. Our results suggest that pre-trained Transformers are competitive and in some cases superior to previous models, especially for tasks involving natural language; whereas for source code tasks, in particular for very small datasets,traditional machine learning methods often has the edge.In addition, we experiment with several techniques that ought to aid training on small datasets, including active learning, data augmentation, soft labels, self-training and intermediate-task fine-tuning, and issue recommendations on when they are effective. We also release all the data, scripts, and most importantly pre-trained models for the community to reuse on their own datasets.},
	number = {{arXiv}:2106.15209},
	publisher = {{arXiv}},
	author = {Prenner, Julian Aron and Robbes, Romain},
	urldate = {2023-02-27},
	date = {2021-06-29},
	eprinttype = {arxiv},
	eprint = {2106.15209 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@article{sharma_self_admitted_2022,
	title = {Self-admitted technical debt in R: detection and causes},
	volume = {29},
	issn = {1573-7535},
	doi = {10.1007/s10515-022-00358-6},
	shorttitle = {Self-admitted technical debt in R},
	abstract = {Self-Admitted Technical Debt ({SATD}) is primarily studied in Object-Oriented ({OO}) languages and traditionally commercial software. However, scientific software coded in dynamically-typed languages such as R differs in paradigm, and the source code comments’ semantics are different (i.e., more aligned with algorithms and statistics when compared to traditional software). Additionally, many Software Engineering topics are understudied in scientific software development, with {SATD} detection remaining a challenge for this domain. This gap adds complexity since prior works determined {SATD} in scientific software does not adjust to many of the keywords identified for {OO} {SATD}, possibly hindering its automated detection. Therefore, we investigated how classification models (traditional machine learning, deep neural networks, and deep neural Pre-Trained Language Models ({PTMs})) automatically detect {SATD} in R packages. This study aims to study the capabilities of these models to classify different {TD} types in this domain and manually analyze the causes of each in a representative sample. Our results show that {PTMs} (i.e., {RoBERTa}) outperform other models and work well when the number of comments labelled as a particular {SATD} type has low occurrences. We also found that some {SATD} types are more challenging to detect. We manually identified sixteen causes, including eight new causes detected by our study. The most common cause was failure to remember, in agreement with previous studies. These findings will help the R package authors automatically identify {SATD} in their source code and improve their code quality. In the future, checklists for R developers can also be developed by scientific communities such as {rOpenSci} to guarantee a higher quality of packages before submission.},
	pages = {53},
	number = {2},
	journaltitle = {Automated Software Engineering},
	shortjournal = {Autom Softw Eng},
	author = {Sharma, Rishab and Shahbazi, Ramin and Fard, Fatemeh H. and Codabux, Zadia and Vidoni, Melina},
	urldate = {2023-02-27},
	date = {2022-08-25},
	langid = {english},
	keywords = {Deep learning, Deep neural pre-trained language models, Machine learning, R packages, Self-admitted technical debt},
    year = {2022},
    journal={Automated Software Engineering},
}

@inproceedings{gao_automating_2021,
	location = {Athens Greece},
	title = {Automating the removal of obsolete {TODO} comments},
	isbn = {978-1-4503-8562-6},
	doi = {10.1145/3468264.3468553},
	eventtitle = {{ESEC}/{FSE} '21: 29th {ACM} Joint European Software Engineering Conference and Symposium on the Foundations of Soft. Eng.},
	pages = {218--229},
	booktitle = {Proceedings of the 29th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Soft. Eng.},
	publisher = {{ACM}},
	author = {Gao, Zhipeng and Xia, Xin and Lo, David and Grundy, John and Zimmermann, Thomas},
	urldate = {2023-02-27},
	date = {2021-08-20},
	langid = {english},
    year = {2021}
}



@article{dawson_impact_2023,
	title = {Impact of dataset size and convolutional neural network architecture on transfer learning for carbonate rock classification},
	volume = {171},
	issn = {00983004},
	doi = {10.1016/j.cageo.2022.105284},
	pages = {105284},
    journal = {Computers \& Geosciences},
	journaltitle = {Computers \& Geosciences},
	shortjournal = {Computers \& Geosciences},
	author = {Dawson, Harriet L. and Dubrule, Olivier and John, Cédric M.},
	urldate = {2023-01-30},
	date = {2023-02},
	langid = {english},
    year = {2023},
	file = {Full Text:/Users/paulmvula/Library/CloudStorage/OneDrive-UniversityofOttawa/Zotero/storage/KNJY62J2/Dawson et al. - 2023 - Impact of dataset size and convolutional neural ne.pdf:application/pdf},
}


@article{maldonado2017using,
  title={Using natural language processing to automatically detect self-admitted technical debt},
  author={da Silva Maldonado, Everton and Shihab, Emad and Tsantalis, Nikolaos},
  journal={IEEE Trans. on Soft. Eng.},
  volume={43},
  number={11},
  pages={1044--1062},
  year={2017},
  publisher={IEEE}
}


@article{guo_how_2021,
	title = {How Far Have We Progressed in Identifying Self-admitted Technical Debts? A Comprehensive Empirical Study},
	volume = {30},
	issn = {1049-331X, 1557-7392},
	
	doi = {10.1145/3447247},
	shorttitle = {How Far Have We Progressed in Identifying Self-admitted Technical Debts?},
	abstract = {Background.
              Self-admitted technical debt ({SATD}) is a special kind of technical debt that is intentionally introduced and remarked by code comments. Those technical debts reduce the quality of software and increase the cost of subsequent software maintenance. Therefore, it is necessary to find out and resolve these debts in time. Recently, many automatic approaches have been proposed to identify {SATD}.
              Problem.
              Popular {IDEs} support a number of predefined task annotation tags for indicating {SATD} in comments, which have been used in many projects. However, such clear prior knowledge is neglected by existing {SATD} identification approaches when identifying {SATD}.
              Objective.
              We aim to investigate how far we have really progressed in the field of {SATD} identification by comparing existing approaches with a simple approach that leverages the predefined task tags to identify {SATD}.
              Method.
              We first propose a simple heuristic approach that fuzzily Matches task Annotation Tags (
              {MAT}
              ) in comments to identify {SATD}. In nature,
              {MAT}
              is an unsupervised approach, which does not need any data to train a prediction model and has a good understandability. Then, we examine the real progress in {SATD} identification by comparing
              {MAT}
              against existing approaches.
              Result.
              The experimental results reveal that: (1)
              {MAT}
              has a similar or even superior performance for {SATD} identification compared with existing approaches, regardless of whether non-effort-aware or effort-aware evaluation indicators are considered; (2) the {SATDs} (or non-{SATDs}) correctly identified by existing approaches are highly overlapped with those identified by
              {MAT}
              ; and (3) supervised approaches misclassify many {SATDs} marked with task tags as non-{SATDs}, which can be easily corrected by their combinations with
              {MAT}
              .
              Conclusion.
              It appears that the problem of {SATD} identification has been (unintentionally) complicated by our community, i.e., the real progress in {SATD} comments identification is not being achieved as it might have been envisaged. We hence suggest that, when many task tags are used in the comments of a target project, future {SATD} identification studies should use
              {MAT}
              as an easy-to-implement baseline to demonstrate the usefulness of any newly proposed approach.},
	pages = {1--56},
	number = {4},
    year = {2021},
	journal = {{ACM} Transactions on Software Engineering and Methodology},
	shortjournal = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Guo, Zhaoqiang and Liu, Shiran and Liu, Jinping and Li, Yanhui and Chen, Lin and Lu, Hongmin and Zhou, Yuming},
	urldate = {2023-01-28},
	date = {2021-10-31},
	langid = {english},
}


@article{huang2018identifying,
  title={Identifying self-admitted technical debt in open source projects using text mining},
  author={Huang, Qiao and Shihab, Emad and Xia, Xin and Lo, David and Li, Shanping},
  journal={Empirical Software Engineering},
  volume={23},
  pages={418--451},
  year={2018},
  publisher={Springer}
}

@article{cunningham1992wycash,
  title={The WyCash portfolio management system},
  author={Cunningham, Ward},
  journal={ACM SIGPLAN OOPS Messenger},
  volume={4},
  number={2},
  pages={29--30},
  year={1992},
  publisher={ACM New York, NY, USA}
}



@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2022-09-25},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
	keywords = {Computer Science - Computation and Language},
}

@article{ren_neural_2019,
	title = {Neural {Network}-based {Detection} of {Self}-{Admitted} {Technical} {Debt}: {From} {Performance} to {Explainability}},
	volume = {28},
	issn = {1049-331X, 1557-7392},
	shorttitle = {Neural {Network}-based {Detection} of {Self}-{Admitted} {Technical} {Debt}},
	
	doi = {10.1145/3324916},
	abstract = {Technical debt is a metaphor to reflect the tradeoff software engineers make between short-term benefits and long-term stability. Self-admitted technical debt (SATD), a variant of technical debt, has been proposed to identify debt that is
              intentionally introduced
              during software development, e.g., temporary fixes and workarounds. Previous studies have leveraged human-summarized patterns (which represent n-gram phrases that can be used to identify SATD) or text-mining techniques to detect SATD in source code comments. However, several characteristics of SATD features in code comments, such as vocabulary diversity, project uniqueness, length, and semantic variations, pose a big challenge to the accuracy of pattern or traditional text-mining-based SATD detection, especially for cross-project deployment. Furthermore, although traditional text-mining-based method outperforms pattern-based method in prediction accuracy, the text features it uses are less intuitive than human-summarized patterns, which makes the prediction results hard to explain. To improve the accuracy of SATD prediction, especially for cross-project prediction, we propose a Convolutional Neural Network-- (CNN) based approach for classifying code comments as SATD or non-SATD. To improve the explainability of our model’s prediction results, we exploit the computational structure of CNNs to identify key phrases and patterns in code comments that are most relevant to SATD. We have conducted an extensive set of experiments with 62,566 code comments from 10 open-source projects and a user study with 150 comments of another three projects. Our evaluation confirms the effectiveness of different aspects of our approach and its superior performance, generalizability, adaptability, and explainability over current state-of-the-art traditional text-mining-based methods for SATD classification.},
	language = {en},
	number = {3},
	urldate = {2022-09-25},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Ren, Xiaoxue and Xing, Zhenchang and Xia, Xin and Lo, David and Wang, Xinyu and Grundy, John},
	month = jul,
	year = {2019},
	pages = {1--45},
	file = {Ren et al. - 2019 - Neural Network-based Detection of Self-Admitted Te.pdf:/Users/billzo/Zotero/storage/AI9EAVEQ/Ren et al. - 2019 - Neural Network-based Detection of Self-Admitted Te.pdf:application/pdf},
}

@article{pascarella2019classifying,
  title={Classifying code comments in Java software systems},
  author={Pascarella, Luca and Bruntink, Magiel and Bacchelli, Alberto},
  journal={Empirical Software Engineering},
  volume={24},
  number={3},
  pages={1499--1537},
  year={2019},
  publisher={Springer}
}


@article{yu_identifying_2022,
	title = {Identifying {Self}-{Admitted} {Technical} {Debts} {With} {Jitterbug}: {A} {Two}-{Step} {Approach}},
	volume = {48},
	issn = {0098-5589, 1939-3520, 2326-3881},
	shorttitle = {Identifying {Self}-{Admitted} {Technical} {Debts} {With} {Jitterbug}},
	
	doi = {10.1109/TSE.2020.3031401},
	abstract = {Keeping track of and managing Self-Admitted Technical Debts (SATDs) are important to maintaining a healthy software project. This requires much time and effort from human experts to identify the SATDs manually. The current automated solutions do not have satisfactory precision and recall in identifying SATDs to fully automate the process. To solve the above problems, we propose a two-step framework called Jitterbug for identifying SATDs. Jitterbug ﬁrst identiﬁes the “easy to ﬁnd” SATDs automatically with close to 100 percent precision using a novel pattern recognition technique. Subsequently, machine learning techniques are applied to assist human experts in manually identifying the remaining “hard to ﬁnd” SATDs with reduced human effort. Our simulation studies on ten software projects show that Jitterbug can identify SATDs more efﬁciently (with less human effort) than the prior state-of-the-art methods.},
	language = {en},
	number = {5},
	urldate = {2022-09-25},
	journal = {IEEE Trans. Soft. Eng.},
	author = {Yu, Zhe and Fahid, Fahmid Morshed and Tu, Huy and Menzies, Tim},
	month = may,
	year = {2022},
	pages = {1676--1691},
}

@misc{NapluesGithub,
  author = {Naplues},
  title = {\href{https://github.com/Naplues/MAT}{MAT}},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  commit = {42481f055c442df1d2bb8f4a3991ff0c9fc31c08}
}

@misc{JitterBugGitHub,
  author = {{NcState AI 4 SE Research Group}},
  title = {\href{https://github.com/ai-se/Jitterbug}{Jitterbug}},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  commit = {1bcc40d84d9bfa3f0a8957f8c58dbdf69bf433e0}
}