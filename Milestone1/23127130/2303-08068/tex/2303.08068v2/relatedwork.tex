\section{Related Work}
% As DNNs dealing with styles, style transfer methods~\cite{Gatys2016,Jing2020} are well-known.
DNNs can handle styles using style transfer tasks~\cite{Gatys2016,Jing2020}.
Given a content image and a style image, style-transferring DNNs generally generate images with the same content as the first input and with the same style as the second input.
They aim to render high-quality images in different styles.
Thus, the style-transfer methods do not assign weight to style features or embedding vectors of styles.

AEs~\cite{Vincent2008} can extract style features; in particular, styles in feature spaces can be observed using AEs that can bind feature spaces to specific distributions such as VAEs~\cite{Kingma2014} and adversarial AEs~\cite{Makhzani2016}.
AEs generally form clusters in feature spaces, and the styles are distributed in each cluster.
However, the styles are usually mixed with other content in the feature spaces.

Disentanglement~\cite{Higgins2017,Qi2022,locatello19a} is an approach to handle such mixed features.
Disentanglement methods attempt to separate the features or feature spaces into interpretable parts.
Several techniques, such as regularization and quantization, have been proposed to disentangle features.
However, extracting specific features using such methods is difficult~\cite{locatello19a}.

A more straightforward approach for extracting only styles is to use labels.
CVAEs~\cite{Kingma2014a,Sohn2015} use class labels as conditions and learn feature spaces that represent common styles across labels.
If labels are not available, the features or components selected for use as the condition for extracting only styles are not obvious.

CL methods, which are self-supervised representation learning methods, are potential choices for the condition.
CL methods find feature spaces robust against data augmentation, and thus they can be considered style-isolation methods~\cite{Kugelgen2021}.
CL itself is a popular representation learning method that has achieved great success, especially with image data~\cite{SimCLR,MoCo,SwAV,Le-Khac2020}.
SimCLR~\cite{SimCLR} showed that the framework of CL works well for images, and MoCo~\cite{MoCo} improved SimCLR's computation costs.
These methods use positive samples and many negative samples as inputs and draw a contrast between them to learn meaningful feature spaces.
Recently, CL methods without negative samples~\cite{SimSiam,VICReg} have been proposed.
These methods use techniques such as regularization and special network structures, and they are called \emph{noncontrastive} CL methods.

Styles can also be seen as domains.
Domain adaptation is a well-studied task that deals with changes in domains between the training and testing phases~\cite{Wang2018}.
Domain adaptation is achieved by obtaining domain-independent feature extractors.
To this end, various approaches have been proposed;
for example, evaluating domain independence by DNNs in an adversarial manner~\cite{Ganin2016},
regularizing the feature vector to have the same distribution across domains using the maximum mean discrepancy~\cite{Long2016},
and utilizing learnable classifiers for extracting classification-aware and domain-independent features~\cite{Saito2018}.
However, these domain adaptation methods aim to extract the common feature across different domains, and domain-specific features have not received considerable attention.