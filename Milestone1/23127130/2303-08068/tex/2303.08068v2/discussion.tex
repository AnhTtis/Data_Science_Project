\section{Discussion}
In our experiments using MNIST-like datasets, the proposed method successfully extracted style features (\cref{fig:da_removal,fig:interp,fig:style_transfer,fig:nn}).
The extracted styles were related to the characters' size, thickness, slant, position, etc.
Additionally, the style-extraction performance was retained across different CL methods (\cref{fig:da_removal_variations,fig:interp_variations}).

However, in the experiments with the Google Fonts dataset, font faces that can be easily understood as styles are not extracted as styles (\cref{fig:da_removal_font,fig:interp_font,fig:style_transfer_font,fig:nn_vae_font}).
These results can be attributed to the data augmentation used.
The proposed method aims to capture information that the CL isolates, and the information isolated by the CL heavily relies on data augmentation.
The data augmentation was appropriate for isolating the styles in the case of MNIST, but not for the Google Fonts dataset.
For capturing font faces as styles, data augmentation should cover differences between different font faces (e.g.\ by adding morphological transformations).

The choice of the condition for the CVAE is also important.
As indicated by the experiments with the different CL methods and the supervised variant (\cref{fig:da_removal_variations,fig:interp_variations}), the extracted style features are slightly different across the CL methods.
Moreover, the proposed method extracted more diverse styles when the class labels were provided (i.e.,\ the supervised variant).
These results suggest that style extraction performance of the proposed method depends on the quality of the style-independent feature $z_\text{content}$.
We can change the conditions for the CVAE depending on the styles and datasets we want to extract.
We used CL as the condition to train the model with completely unlabeled data.
However, if some labels are available, making use of them in a supervised or semi-supervised manner is a possible choice.

The proposed method could also deal with real-world natural image datasets in the experiments (\cref{fig:style_transfer_imagenette,fig:style_transfer_daiso100,fig:nn_vae_imagenette,fig:nn_cl_imagenette,fig:nn_vae_daiso100,fig:nn_cl_daiso100}).
The style-transfer experiments indicated that the CVAE extracted styles.
However, in the Imagenette experiments, we did not find common styles in the neighbor analysis, although we did find them in the DAISO-100 experiments.
This can be attributed to Imagenette being a small dataset with a relatively large variety of data, which resulted in the learned style feature space being too sparse to find meaningful neighbors.

In terms of the natural image datasets, the quality of the generated images is a limitation.
Our aim is not to generate images but to extract style features; however, high-quality image generation is desirable for evaluating the learned features.
Improving the decoder model or combining it with adversarial models such as GANs~\cite{GAN} is one approach to achieve this.

Although the experimental results illustrated the style-extracting ability of the proposed method, the evaluations were qualitative.
We need quantitative evaluations for comparing the performance of the different CL methods in detail.
One approach to achieve this is by preparing a dataset with explicit style labels and a set of data augmentation operations that can perturb only the explicit styles.
