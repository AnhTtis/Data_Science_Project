\IEEEraisesectionheading{
\section{Introduction}\label{sec:introduction}
}
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.




% データから情報や知識を取り出す問題は工学的に重要である．
\IEEEPARstart{E}{xtracting} information or knowledge from data is crucial in engineering.
% 特に機械学習の文脈では，ラベルなしデータから情報を抽出する問題を教師なし学習と呼び，伝統的には主成分分析をはじめとした次元削減手法や，データを複数のクラスタに分割するクラスタリングなどが研究されてきた~\cite{Bishop2006,Murphy2012}．
Tasks to extract information from unlabeled data are called unsupervised learning~\cite{Bishop2006,Murphy2012}.
Traditional unsupervised learning methods, including dimensionality reduction methods, including principal component analysis (PCA), and clustering methods, such as $k$-means, have been extensively studied~\cite{Bishop2006,Murphy2012}.
% より表現力の高い深層学習による手法としてはオートエンコーダ (auto-encoder; AE)~\cite{Vincent2008}がよく知られている．
In the recent decade, unsupervised learning using deep neural networks (DNNs) with large expression capability has succeeded greatly~\cite{Goodfellow2016}.

One of the typical DNN-based unsupervised models is an autoencoder (AE)~\cite{Vincent2008}.
% AE は入力データを特徴空間に写像するエンコーダと，特徴空間上の特徴ベクトルから入力データを再構成するデコーダからなるモデルであり，主成分分析に類似したアイデアに基づいている． % 本当？
AEs work like a nonlinear version of PCA and consist of two parts; encoders that map input data to feature spaces and decoders that reconstruct the input data from vectors in the feature spaces.
% 深層学習の文脈では，% 機械学習の文脈に一般化できると思うけど，representation learning という言葉がよく聞かれるようになったのはディープが流行ってからだと思うのでこういう書き方をしておく
% ラベルなしデータから有用な特徴空間への写像を学習する問題を表現学習と呼び，AE などの教師なし学習の他にも自己教師あり学習などの方法が知られている~\cite{Le-Khac2020}．
In the context of DNNs, a task to find a map to beneficial feature spaces using unlabeled data is called representation learning.
One typical example of representation learning is unsupervised learning.
In addition, a self-supervised method called contrastive learning (CL) has been popular recently~\cite{Le-Khac2020}.

% 自己教師あり学習手法のひとつである対照学習 (contrastive learning; CL) は，特に画像データに対してクラス分類に適した特徴ベクトルを獲得できると報告されている~\cite{Le-Khac2020,SimCLR,MoCo,SimSiam}．
CL is a self-supervised learning method to learn representations suitable for the classification of images~\cite{Le-Khac2020,SimCLR,MoCo,SimSiam}.
% CL は，近い内容を含むデータ同士は特徴空間上で近くに写像されるべきであるという考え方に基づいている．
The underlying idea behind CL is that samples with similar contents should be mapped to the identical feature vector.
% データに対するラベルやアノテーションが与えられないとき，内容の近いデータを特定するのは難しい．
% そこで，典型的な CL においては，ひとつの入力データに対して異なるデータ拡張を適用したデータのペアを用意し，それらの特徴ベクトルが近くなるように学習する．
To achieve this, CL needs a large number of pairs characterized by similar contents; however, it is not easy to prepare such a dataset.
Thus, data augmentation is usually used for generating a pair of samples with the same content from each sample.
% The CL model trains such a map from two different samples having the same context to a single feature vector.
% その結果，明るさやコントラストの変化，ぼけなどのデータ拡張によって与えられる摂動に頑健な特徴抽出器が得られる．
The CL model behaves as a feature extractor robust to perturbations introduced by the data augmentation, such as blurring and changes in brightness and contrast.
% このような学習によって得られる特徴ベクトルは，ImageNet データセット~\cite{ILSVRC15} をよく線形分離でき，また得られるモデルは物体検出タスクにおける初期重みとして有用であることが知られている~\cite{SimCLR,MoCo,SimSiam}．
In this way, even a simple linear model can efficiently classify the samples in ImageNet~\cite{ILSVRC15} with feature extraction by CL.
Besides, trained CL models are known to be good initial models for object detection tasks~\cite{SimCLR,MoCo,SimSiam}.
% CL がシンプルな手続きでありながら有用な特徴空間を獲得できる理由は明らかになっていないが，解釈としては，データ拡張に独立な特徴を学習することでスタイルを分離しているため~\cite{Kugelgen2021}，CL はデータの生成過程の逆を暗黙的に学習しているため~\cite{Zimmermann2021} などが提起されている．
How CL works with the simple data augmentation strategy is still an open question, but some analyses have been carried out:
CL isolates contents from other miscellaneous features by learning features independent of data augmentation~\cite{Kugelgen2021}, and CL implicitly learns the inverse of the data generating process~\cite{Zimmermann2021}.

% CL や AE はクラス分類に適した特徴や，データを構成する主要な成分を抽出することができる．
As mentioned earlier, CL and AEs can extract features that are suitable for classification and are dominant components of the given data, respectively.
% 一方では，より細かいスタイルなどの情報を抽出することもラベルなしデータを解析する上では重要である．
On the other hand, it is also crucial to extract more fine-grained information, such as styles.
Styles are often referred to as features that do not correspond to the main contents of data (e.g., class labels).
% この重要性は，スタイルには例えば個人差やデータ取得時の環境の差といった有用な情報が含まれることからも理解できる．
The importance of the style is understood because it includes unique characteristics such as individual differences, the environment where the data are taken, and domains.
% スタイルを抽出できるモデルとしては，変分 AE (variational AE; VAE)~\cite{Kingma2014} や，その拡張である 条件つき VAE (conditional VAE; CVAE)~\cite{Kingma2014a,Sohn2015} が有名である．
Well-known models to extract style features are Variational AEs (VAEs)~\cite{Kingma2014} and conditional VAEs (CVAEs)~\cite{Kingma2014a,Sohn2015}.
% VAE は，特徴空間を特定の分布へと制約できるという特徴をもつ．
% VAE によって得られる特徴空間では，クラスのような大まかな特徴がクラスタのように抽出され，各クラスタの内部ではより細かな情報，すなわちスタイルが表現される．
VAEs typically learn to construct feature spaces that represent rough features like classes as clusters.
In each cluster, finer features like styles are also represented.
% さらに，データにラベルがある場合にはラベルを条件として加えた CVAE を用いれば，特徴空間はスタイルを表現するよう学習される．
A CVAE is a variation of a VAE conditioned by labels, if available. With the CVAE, the feature spaces are learned to represent styles.
% 例えば，手書き数字の MNIST データセット~\cite{MNIST} を用いて CVAE を訓練すると，特徴空間には数字の書き方を表現する分布が見られる~\cite{Kingma2014a}．
It has been reported that a CVAE trained with the MNIST~\cite{MNIST}, which is a handwritten digit image dataset, forms a feature space with a distribution representing handwriting styles (e.g., the thickness of lines and angle of digits)~\cite{Kingma2014a}.
% しかしながら，ラベルが利用できないときに CVAE のようにスタイル情報のみを抽出する方法は確立されていない．
This way, CVAE is an efficient model to extract only styles when labels are available.
However, little attention has been paid to a model without labels that can extract only styles.
%However, models that extract only styles like CVAEs without the help of labels are not established.
% また，VAE が提案されて以降盛んに研究されている，特徴空間上の分布を解釈可能な単位に分ける試みである disentanglement~\cite{Higgins2017,Qi2022} の文脈でも， 狙った特徴のみを分離することは難しいという報告がある~\cite{locatello19a}． % disentanglement については要らないかも？
% Although attempts to decompose the feature spaces into interpretable forms obtained by unsupervised learning models like VAEs, which are called disentanglement~\cite{Higgins2017,Qi2022}, are popular, it is difficult to isolate only the desired features such as styles~\cite{locatello19a}. -> Related workへ

% 本稿では，CL がスタイルに独立な特徴ベクトルを抽出できるという知見~\cite{Kugelgen2021} に基づき，CL モデルが抽出しないスタイル情報を補うモデルによって，ラベルなしデータのみを使ってスタイル特徴を抽出する方法を提案する．
% In this paper, we establish a method for extracting style features from a unlabeled dataset.
% The proposed method roughly consists of two key parts; a style-extracting CVAE and a style-isolating CL model.
% To effectively extract styles using CVAEs, we need proper conditions that are independent of styles.
% To this end, we utilize features of CL as the conditions because CL models are known to extract style-independent features~\cite{Kugelgen2021}, and they are trained without labels.
% Hence, the architecture of the proposed method results in a parallel structure of the CVAE and the CL model; the input data are fed into the two models, and the CVAE takes the output of the CL model as the condition.
% The training procedure of the proposed CVAE comes with a constraint that is based on mutual information of the feature vectors of the CL model and of the CVAE.
% This constraint helps the CVAE extract only styles by leading the two vectors to be independent.
In this paper, we establish a method for extracting style features from an unlabeled dataset.
The proposed method is two-fold; a style-extracting CVAE and a style-isolating CL model.
To effectively extract styles using CVAEs, we need proper conditions independent of styles.
To this end, we utilize features of CL as the conditions (soft labels) since CL models are known to extract style-independent features~\cite{Kugelgen2021} and are trained without labels.
The CVAE and the CL model form a parallel network in the proposed architecture, where the input data are fed into the two models, and the output of the CL model conditions the CVAE.
The training procedure of the proposed CVAE involves evaluating mutual information (MI) of feature vectors coming from the CL model and the CVAE.
The MI measure promotes the independence of two vectors so that the CVAE extracts only styles.
% 比較的単純なデータセットである MNIST~\cite{MNIST} や，Google Fonts~\cite{googlefonts} を用いた，よりスタイルがわかりやすい独自のデータセットを用いた計算機実験により，提案手法がスタイルを効率よく抽出可能であることを示す．
Experiments on two simple datasets, MNIST~\cite{MNIST} and an original dataset based on Google Fonts~\cite{googlefonts}, show that the proposed method effectively extracts style features.
% さらに，より実践的な自然画像データセットである Imagenette~\cite{imagenette} および DAISO-100~\cite{katoh2021dataset} を用いた実験により，提案手法の汎用性を示す．
As more practical results, we also show evaluations on two natural image datasets; Imagenette~\cite{imagenette} and DAISO-100~\cite{katoh2021dataset}.