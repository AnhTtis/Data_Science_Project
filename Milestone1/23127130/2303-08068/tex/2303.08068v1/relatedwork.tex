\section{Related Work}
% As DNNs dealing with styles, style transfer methods~\cite{Gatys2016,Jing2020} are well-known.
It is well-known that DNNs can deal with styles by style transfer tasks~\cite{Gatys2016,Jing2020}.
Given a content image and a style image, style-transferring DNNs generally generate images that have the same content as the first input and the same style as the second input.
They aim to render high-quality images in different styles.
Thus the style transfer methods do not give weight to style features or embedding vectors of styles.

AEs~\cite{Vincent2008} are known to be able to extract the style features.
Specifically, we can observe styles in feature spaces using AEs that can bind the feature spaces to specific distributions such as VAEs~\cite{Kingma2014} and adversarial AEs~\cite{Makhzani2016}.
They generally form clusters in feature spaces, and in each cluster, we can observe that the styles are distributed in each cluster.
However, the styles are usually mixed with other content in the feature spaces.

Disentanglement~\cite{Higgins2017,Qi2022,locatello19a} is an attempt to handle such mixed features.
The disentanglement methods try to separate the features or the feature spaces into some interpretable parts.
To disentangle the features, several techniques such as regularization and quantization have been proposed.
Nonetheless, it is reported that extracting specific features is difficult using such methods~\cite{locatello19a}.

A more straightforward way to extract only the styles is to use labels.
CVAEs~\cite{Kingma2014a,Sohn2015} use class labels as conditions and learn feature spaces that represent common styles across the labels.
If labels are not available, the selection of features or components to use as the condition for extracting only the styles is not obvious.

CL methods, which are self-supervised representation learning methods, are possible choices as the condition.
CL finds feature spaces that are robust to data augmentation, and thus it can be considered a style isolation method~\cite{Kugelgen2021}.
CL itself is a popular representation learning that has achieved great success, especially with image data~\cite{SimCLR,MoCo,SwAV,Le-Khac2020}.
SimCLR~\cite{SimCLR} showed that the framework of CL works well for images, and MoCo~\cite{MoCo} improved SimCLR's computation costs.
These methods take positive samples and many negative samples as inputs and make a contrast between them to learn meaningful feature spaces.
Recently, CL methods without negative samples~\cite{SimSiam,VICReg} have been proposed.
They come with techniques such as regularization and special network structures.
Such methods are called \emph{non-contrastive} CL methods.

Styles can also be seen as domains.
Domain adaptation is a well-studied task that deals with changes in domains between the training phase and the testing phase~\cite{Wang2018}.
Generally, domain adaptation is achieved by obtaining domain-independent feature extractors.
To obtain it, various approaches have been proposed;
evaluating domain independence by DNNs in an adversarial manner~\cite{Ganin2016},
regularize the feature vector to be the same distribution across domains by the maximum mean discrepancy~\cite{Long2016},
and utilizing learnable classifiers to classification-aware and domain-independent features~\cite{Saito2018},
for example.
Still, these domain adaptation methods aim to extract the common feature across different domains, and domain-specific features have been paid less attention.