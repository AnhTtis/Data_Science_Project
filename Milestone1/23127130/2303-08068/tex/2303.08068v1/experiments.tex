\section{Experiments}
We evaluated the effectiveness of the proposed style feature extraction method using four datasets: MNIST~\cite{MNIST}, an original Google Fonts-based dataset (hereinafter referred to as the Google Fonts dataset), Imagenette~\cite{imagenette}, and DAISO-100~\cite{katoh2021dataset}.
We analyzed the validity of the proposed method using the first two datasets as they are simple, small, and easy to handle.
Additionally, we assessed the performance on practical, real-world natural images using the latter two datasets.

Although there are several contrastive learning (CL) methods available, we mainly employed MoCo v2~\cite{chen2020mocov2}, which is relatively lightweight and stable.
In the experiments using MNIST and the Google Fonts dataset, we evaluated the combination of the proposed method and different CL methods.

\subsection{MNIST-Like Datasets}
We confirmed that the proposed method works as intended using two simple datasets: MNIST~\cite{MNIST} and the Google Fonts dataset.
MNIST is a handwritten digits image dataset that comes with various handwriting styles.
For additional evaluation, we composed the Google Fonts dataset, which is based on Google Fonts~\cite{googlefonts}.
Because font faces can be considered as styles, the dataset is expected to contain more style variations than MNIST.
The Google Fonts dataset consists of images of digits, Latin alphabets, and Japanese hiragana.
By choosing fonts that contain the characters, we obtained 1,244 font families.
Each font family may contain different font weights.
By creating images with each character for each font, we obtained 280,694 training samples and 31,189 testing samples.
Each image is $32\times 32$ pixels grayscale.
Some examples from the Google Fonts dataset are shown in \cref{fig:google_fonts_example}.

For experiments using MNIST, we trained the contrastive learning (CL) model using EMNIST ByClass~\cite{emnist}, which is an extension of MNIST.
This is because CL works better with larger datasets~\cite{SimCLR}.
We trained the CVAE using MNIST and evaluated the overall proposed method also using MNIST.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/google_fonts_examples.pdf}
    \caption{Example images from the Google Fonts dataset.}
    \label{fig:google_fonts_example}
\end{figure}

Using the two datasets, we evaluated the proposed method from five viewpoints.
First, we confirmed that the CL extracts feature vectors independent of data augmentation, and the CVAE successfully extracts the features corresponding to the augmentation by observing the decoder outputs corresponding to data augmented inputs.
Second, we assessed the relationship between the feature vectors of the CVAE and the styles by generating samples through the decoder with fixed $z_\text{SI}$ and interpolated $z_\text{SD}$.
Third, we evaluated the variations in style features through style transfer experiments.
Fourth, we observed the neighbors of the test data that are mapped by the encoder (i.e., the test data in the space of $z_\text{SD}$) in order to evaluate the style features without passing them to the decoder.
Finally, we compared the effects of using CL methods other than MoCo v2 in combination with the proposed method.

We used almost the same hyperparameters for MNIST and the Google Fonts dataset.
The backbone networks of the CL ($f_\psi$) and the encoder ($E_\phi$) were ResNet-18~\cite{He2016}.
While training the CL and the CVAE, data augmentation that consists of random perspective transformation, random cropping, random blurring, and random perturbation of brightness and contrast was applied.
We used Adam~\cite{Adam} as the optimizer for the training.
Other hyperparameters are shown in \cref{tab:hparams}.
\begin{table*}[t]
    \centering
    \caption{The hyperparameters in the experiments.}
    \label{tab:hparams}
    \begin{tabular}{lccccc}
        \toprule
        Dataset      & Architecture of $E_\theta, f_\psi$ & Dataset for CL & $\lambda_\text{KL}$ & $\lambda_\text{MINE}$ & $\text{dim}(z_\text{SD})$ \\\midrule
        MNIST        & ResNet-18 & EMNIST~\cite{emnist} ByClass & $0.1$ & $10^{-2}$ & 32\\
        Google Fonts & ResNet-18 & Google Fonts                 & $0.1$ & $10^{-3}$ & 32\\
        Imagenette   & ResNet-50 & ImageNet~\cite{ILSVRC15}     & $1$   & $10^{-2}$ & 128\\
        DAISO-100    & ResNet-50 & ImageNet~\cite{ILSVRC15}     & $1$   & $10^{-3}$ & 128\\\bottomrule
    \end{tabular}
\end{table*}

\subsubsection{Capturing Isolated Data Augmentation Features}
\label{sec:da_removal_mnist}
Firstly, we conducted simple sanity check experiments.
Regarding the CL methods learn features independent of data augmentation, we checked the CVAE successfully captures the information corresponding to the data augmentation.

\Cref{fig:da_removal} shows the results.
In the figure, two types of reconstruction are shown; the normal reconstruction using both $z_\text{SI}$ and $z_\text{SD}$ (the third row of \cref{fig:da_removal}) and the data augmentation-independent reconstruction using only $z_\text{SI}$ (the bottom row of \cref{fig:da_removal}).
While the normal reconstructions were similar to the input, the data augmentation-independent reconstructions looked like the input without the effects of the augmentation.
These results illustrate that the CL isolated data augmentation features, and the CVAE part captured them.
Interestingly, for the Google Fonts dataset, the reconstructions with $z_\text{SD}=\mathbf{0}$ were not like the input data before the data augmentation; they seemed to be average style versions of the data-augmented input data.
\begin{figure*}[t]
    \centering
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[height=3cm]{fig/da_removal_mnist.pdf}
        \subcaption{MNIST}\label{fig:da_removal_mnist}
    \end{minipage}%
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[height=3cm]{fig/da_removal_font.pdf}
        \subcaption{Google Fonts}\label{fig:da_removal_font}
    \end{minipage}
    \caption{%
    Examples of isolating and capturing the data augmentation features.
    From top to bottom: original images, images after the data augmentation, the reconstructed images $D_\theta(z_\text{SD}, z_\text{SI})$, and the images reconstructed without the style features $D_\theta(\mathbf{0}, z_\text{SI})$.
    }
    \label{fig:da_removal}
\end{figure*}

\subsubsection{Conditional Generation}
\label{sec:interp_mnist}
We checked that the learned feature of the CVAE corresponded to the style feature by conditional generation experiments.
We observed the decoder outputs with fixed $z_\text{SI}$ and interpolated $z_\text{SD}$.
The CL features $z_\text{SI}$ were the outputs of the CL model of test data.
The CVAE features $z_\text{SD}$ were generated along random line segments that cross the origin.

\Cref{fig:interp} show the conditional generation results.
For both datasets, it is shown that the styles changed by changing $z_\text{SD}$ without altering the contents.
When $z_\text{SD}$ was near the origin, the outputs were in average styles, and only styles gradually shifted as $z_\text{SD}$ moved away from the origin.
Additionally, changes in styles were consistent across different $z_\text{SI}$.
\begin{figure*}[t]
    \centering
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/interp_mnist.pdf}
        \subcaption{MNIST}\label{fig:interp_mnist}
    \end{minipage}%
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/interp_font.pdf}
        \subcaption{Google Fonts}\label{fig:interp_font}
    \end{minipage}
    \caption{%
    Examples of conditional generation.
    Each row corresponds to different $z_\text{SI}$ from some test data.
    Each column corresponds to interpolated $z_\text{SD} (0 \leq \|z_\text{SD}\| \leq 3)$ along a randomly chosen unit vector.
    The images of the center column with the red frame are generated without the style features (i.e., $z_\text{SD}=\mathbf{0}$).
    }
    \label{fig:interp}
\end{figure*}

\subsubsection{Style Transfer}
\label{sec:style_transfer_mnist}
Style transfer experiments were employed to evaluate the variation of the learned style features.
Using the CL model, we extracted the style-independent features $z_\text{SI}$ of the test data.
Then, we combined them with the style features $z_\text{SD}$ of different test data and put them into the decoder to generate style-transferred images.

The results of the style transfer experiments are shown in \cref{fig:style_transfer}.
They illustrate the styles were successfully transferred by using the proposed style feature extractor.
For MNIST, the styles, such as the slant, the size, and the thickness, were transferred.
For the Google Fonts dataset, mainly the bounding boxes (i.e., the size and the placement) of the characters are transferred as the styles.
This result indicates that the proposed method has not always learned the font faces as the styles.
\begin{figure*}[t]
    \centering
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[height=6.5cm]{fig/style_transfer_mnist.pdf}
        \subcaption{MNIST}\label{fig:style_transfer_mnist}
    \end{minipage}%
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[height=6.5cm]{fig/style_transfer_font.pdf}
        \subcaption{Google Fonts}\label{fig:style_transfer_font}
    \end{minipage}
    \caption{%
    Examples of style transfer on the MNIST-like datasets.
    The red framed images are the original images (the first column) and the reconstructions using the original styles (the second column).
    The blue framed images are style destination images.
    Each white framed image is generated using the style-independent feature $z_\text{SI}$ from the red framed image and the style feature $z_\text{SD}$ from the blue framed image.
    }
    \label{fig:style_transfer}
\end{figure*}

\subsubsection{Neighbor Analysis}
\label{sec:nn_mnist}
While we evaluated the outputs of the decoder in the previous three experiments, we also tried to directly observe the style features themselves by analyzing neighbors.
Specifically, we mapped all the test data to the style feature space using the encoder, and we observed the neighbors of some of the test data in terms of the style features $z_\text{SD}$.
Besides, to confirm the style independence of the CL features, we also observed the neighbors in terms of $z_\text{SI}$.

We show the neighbors of some example test data in \cref{fig:nn}.
It is seen that the neighbors had similar styles in the style feature spaces by \cref{fig:nn_vae_mnist,fig:nn_vae_font}.
However, for MNIST, the neighbors tended to simply be similar data.
On the other hand, for the Google Fonts dataset, the neighbors had different characters with similar bounding boxes.
The changes in bounding boxes were observed as the styles in also the style transfer experiments.
In the style-independent feature spaces, the neighbors had almost the same content in different styles as shown in \cref{fig:nn_cl_mnist,fig:nn_cl_font}.
This result agrees with the characteristics of the CL; the CL models extract data augmentation-independent features, and they are suitable for classification~\cite{SimCLR,MoCo,SimSiam}.
\begin{figure*}[t]
    \centering
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/nn_vae_mnist.pdf}
        \subcaption{MNIST, $z_\text{SD}$}\label{fig:nn_vae_mnist}
    \end{minipage}%
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/nn_cl_mnist.pdf}
        \subcaption{MNIST, $z_\text{SI}$}\label{fig:nn_cl_mnist}
    \end{minipage}
    \vspace{0.5cm}
    
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/nn_vae_font.pdf}
        \subcaption{Google Fonts, $z_\text{SD}$}\label{fig:nn_vae_font}
    \end{minipage}%
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/nn_cl_font.pdf}
        \subcaption{Google Fonts, $z_\text{SI}$}\label{fig:nn_cl_font}
    \end{minipage}
    \caption{%
    Visualization of the neighbors in terms of $z_\text{SD}$ and $z_\text{SI}$ on the MNIST-like datasets.
    The leftmost red framed data are anchors, and their neighbors are listed in order of distance.
    }
    \label{fig:nn}
\end{figure*}

\subsubsection{Combination with various CL Methods}
While we have evaluated the proposed method combined with MoCo v2~\cite{chen2020mocov2} above, we also examined the combinations with other CL methods.
We used SimCLR~\cite{SimCLR}, SimSiam~\cite{SimSiam}, and VICReg~\cite{VICReg}.
SimCLR is a classical CL method on which MoCo~\cite{MoCo,chen2020mocov2} is based.
SimSiam and VICReg are non-contrastive CL methods; they do not use negative samples while learning.
As a reference, we also tested a supervised variant of the proposed method; the one-hot encoded class labels are used instead of the CL feature vectors.
We compared the effects of the different CL methods by the same experiments described in \cref{sec:da_removal_mnist,sec:interp_mnist}.

The hyperparameters of the CL methods were set to their defaults which are described in the original papers.
For training of the CVAE, we used the same hyperparameters as the MoCo v2 version except for the combination of SimSiam and MNIST.
In the experiment of this combination, we changed the weight of the mutual information $\lambda_\text{MINE}$ to $0.1$ because the styles were not extracted well with the original setting.

\cref{fig:da_removal_variations} shows the results of data augmentation isolation.
It is seen that the data augmentation features were isolated by the CL methods, and the CVAE captured them, regardless of which CL method we chose.
The reconstruction images without styles were blurry in the experiment using SimSiam with MNIST (the bottom row of \cref{fig:da_removal_mnist_simsiam}).
This indicates that the style extraction performance depends on how the CL method was trained.
Besides, when class labels are given instead of the CL feature, the reconstruction images without styles looked like the average of the class (the bottom row of \cref{fig:da_removal_mnist_supervised,fig:da_removal_font_supervised}).
These results show that the proposed method successfully captured the features that the condition of the CVAE (i.e., the style-independent CL feature $z_\text{SI}$ or the class label) do not contain.
\begin{figure*}
    \centering
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/da_removal_mnist_simclr.pdf}
        \subcaption{MNIST, SimCLR~\cite{SimCLR}}\label{fig:da_removal_mnist_simclr}
    \end{minipage}%
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/da_removal_mnist_simsiam.pdf}
        \subcaption{MNIST, SimSiam~\cite{SimSiam}}\label{fig:da_removal_mnist_simsiam}
    \end{minipage}%
    \vspace{0.3cm}
    
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/da_removal_mnist_simclr.pdf}
        \subcaption{MNIST, VICReg~\cite{VICReg}}\label{fig:da_removal_mnist_vicreg}
    \end{minipage}%
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/da_removal_mnist_supervised.pdf}
        \subcaption{MNIST, supervised (reference)}\label{fig:da_removal_mnist_supervised}
    \end{minipage}%
    \vspace{0.3cm}
    
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/da_removal_font_simclr.pdf}
        \subcaption{Google Fonts, SimCLR~\cite{SimCLR}}\label{fig:da_removal_font_simclr}
    \end{minipage}%
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/da_removal_font_simsiam.pdf}
        \subcaption{Google Fonts, SimSiam~\cite{SimSiam}}\label{fig:da_removal_font_simsiam}
    \end{minipage}%
    \vspace{0.3cm}
    
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/da_removal_font_simclr.pdf}
        \subcaption{Google Fonts, VICReg~\cite{VICReg}}\label{fig:da_removal_font_vicreg}
    \end{minipage}%
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/da_removal_font_supervised.pdf}
        \subcaption{Google Fonts, supervised (reference)}\label{fig:da_removal_font_supervised}
    \end{minipage}%
    \caption{%
    Examples of isolating and capturing the data augmentation features when the proposed method is combined with different CL methods.
    From top to bottom: original images, images after the data augmentation, the reconstructed images $D_\theta(z_\text{SD}, z_\text{SI})$, and the images reconstructed without the style features $D_\theta(\mathbf{0}, z_\text{SI})$.
    }
    \label{fig:da_removal_variations}
\end{figure*}

The results of conditional generation are shown in \cref{fig:interp_variations}.
Note that the learned feature spaces of the CVAE are different in every experiment, so the changes in styles were not the same across the experiments.
The figure illustrates that the CVAE successfully extracts style features with any of the CL methods.
The supervised variant on the Google Font dataset (\cref{fig:interp_mnist_supervised,fig:interp_font_supervised}) generated more variation in styles by interpolating the style feature $z_\text{SD}$.
Specifically, the changes in font faces were captured as shown in \cref{fig:interp_font_supervised}.
\begin{figure*}
    \centering
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/interp_mnist_simclr.pdf}
        \subcaption{MNIST, SimCLR~\cite{SimCLR}}\label{fig:interp_mnist_simclr}
    \end{minipage}%
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/interp_mnist_simsiam.pdf}
        \subcaption{MNIST, SimSiam~\cite{SimSiam}}\label{fig:interp_mnist_simsiam}
    \end{minipage}%
    \vspace{0.3cm}
    
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/interp_mnist_simclr.pdf}
        \subcaption{MNIST, VICReg~\cite{VICReg}}\label{fig:interp_mnist_vicreg}
    \end{minipage}%
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/interp_mnist_supervised.pdf}
        \subcaption{MNIST, supervised (reference)}\label{fig:interp_mnist_supervised}
    \end{minipage}%
    \vspace{0.3cm}
    
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/interp_font_simclr.pdf}
        \subcaption{Google Fonts, SimCLR~\cite{SimCLR}}\label{fig:interp_font_simclr}
    \end{minipage}%
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/interp_font_simsiam.pdf}
        \subcaption{Google Fonts, SimSiam~\cite{SimSiam}}\label{fig:interp_font_simsiam}
    \end{minipage}%
    \vspace{0.3cm}
    
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/interp_font_simclr.pdf}
        \subcaption{Google Fonts, VICReg~\cite{VICReg}}\label{fig:interp_font_vicreg}
    \end{minipage}%
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/interp_font_supervised.pdf}
        \subcaption{Google Fonts, supervised (reference)}\label{fig:interp_font_supervised}
    \end{minipage}%
    \caption{%
    Examples of conditional generation when the proposed method is combined with different CL methods.
    Each row corresponds to different $z_\text{SI}$ from some test data.
    Each columns corresponds to interpolated $z_\text{SD} (0 \leq \|z_\text{SD}\| \leq 3)$ along a randomly chosen unit vector.
    The images of the center column with the red frame are generated without the style features (i.e., $z_\text{SD}=\mathbf{0}$).
    }
    \label{fig:interp_variations}
\end{figure*}

\subsection{Real-World Natural Image Datasets}
We conducted additional experiments using two natural image datasets, Imagenette~\cite{imagenette} and DAISO-100~\cite{katoh2021dataset}, to evaluate the expandability of the proposed method.
Imagenette is a lightweight subset of ImageNet~\cite{ILSVRC15} containing 10 classes of the original ImageNet.
DAISO-100 is an image dataset consisting of photos of 100 miscellaneous goods in various situations.
In DAISO-100, three types of style-like conditions are explicitly labeled; lighting, decoration by sticker, and camera angles.
We used ImageNet for the training of the CL model in the experiments using the natural image datasets.

We only evaluated through style transfer (cf. \cref{sec:style_transfer_mnist}) and neighbor analysis (cf. \cref{sec:nn_mnist}) because the proposed method do not aim to generate images, and the generated images were not clear enough to be evaluated. 
Style transfer experiments actually require image generation, but we could examine the changes in the generated images when the styles are transferred.
We could evaluate the style features without generating images by neighbor analysis.

As the CL model ($f_\psi$) and the encoder ($E_\phi$), we used ResNet-50~\cite{He2016}.
For the CL model, we utilized the pretrained weights, which are available publicly~\cite{chen2020mocov2}.
While training the CL and the CVAE, data augmentation that is used in MoCo v2~\cite{chen2020mocov2} was applied.
We used Adam~\cite{Adam} as the optimizer for the training.
Other hyperparameters are shown in \cref{tab:hparams}.

\subsubsection{Style Transfer}
The style transfer experiments were conducted in the same way as described in \cref{sec:style_transfer_mnist}.
The correspondence between the styles and the features extracted by the encoder was examined by observing the decoder outputs with different style features.

\cref{fig:style_transfer_imagenette} shows the results on Imagenette.
The generated images were not clear, but styles such as brightness and object shapes were transferred.
The results on DAISO-100 are shown in \cref{fig:style_transfer_daiso100}.
The generated images were slightly clearer than Imagenette, and the direction of the background conveyor belt was transferred as styles.
\begin{figure*}[t]
    \centering
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/style_transfer_imagenette.pdf}
        \subcaption{Imagenette}\label{fig:style_transfer_imagenette}
    \end{minipage}%
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/style_transfer_daiso100.pdf}
        \subcaption{DAISO-100}\label{fig:style_transfer_daiso100}
    \end{minipage}
    \caption{%
    Examples of style transfer on the natural image datasets.
    The red framed images are the original images (left column) and the reconstructions using the original styles (right column).
    The blue framed images are style destination images.
    Each white framed image is generated using the style-independent feature $z_\text{SI}$ from the red framed image and the style feature $z_\text{SD}$ from the blue framed image.
    }
    \label{fig:style_transfer_realworld}
\end{figure*}

\subsubsection{Neighbor Analysis}
We observed the neighbors in terms of the learned feature spaces in the same manner as \cref{sec:nn_mnist}.
Although generating images is difficult using the natural image dataset, we can directly evaluate the feature vectors themselves in this way.

\cref{fig:nn_vae_imagenette,fig:nn_cl_imagenette} show the results on Imagenette.
\cref{fig:nn_vae_imagenette} is difficult to interpret, but \cref{fig:nn_cl_imagenette} illustrates that the CL mapped images with the same object to similar vectors.
\cref{fig:nn_vae_daiso100,fig:nn_cl_daiso100} show the results on DAISO-100.
In terms of the CL, the results were similar to Imagenette; the neighbors were almost the same object in various styles.
\cref{fig:nn_vae_daiso100} shows that the CVAE extracted similar features corresponding to the styles.
Specifically, we can see that the neighbors were with different objects and similar backgrounds.
\begin{figure*}[t]
    \centering
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/nn_vae_imagenette.pdf}
        \subcaption{Imagenette, $z_\text{SD}$}\label{fig:nn_vae_imagenette}
    \end{minipage}%
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/nn_cl_imagenette.pdf}
        \subcaption{Imagenette, $z_\text{SI}$}\label{fig:nn_cl_imagenette}
    \end{minipage}

    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/nn_vae_daiso100.pdf}
        \subcaption{DAISO-100, $z_\text{SD}$}\label{fig:nn_vae_daiso100}
    \end{minipage}%
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{fig/nn_cl_daiso100.pdf}
        \subcaption{DAISO-100, $z_\text{SI}$}\label{fig:nn_cl_daiso100}
    \end{minipage}
    \caption{%
    Visualization of the neighbors in terms of $z_\text{SD}$ and $z_\text{SI}$ on the natural image datasets.
    The leftmost red framed data are anchors, and their neighbors are listed in order of distance.
    }
    \label{fig:nn_realworld}
\end{figure*}
