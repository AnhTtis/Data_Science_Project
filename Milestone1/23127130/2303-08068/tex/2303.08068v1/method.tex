\section{Method}
% 本節では提案するスタイル特徴を抽出する方法について述べる．
In this section, we describe the proposed method for style feature extraction.
% \cref{fig:overview} に示すように，提案手法は CL を条件とする CVAE と，相互情報量の推定器によって構成される．
The proposed method consists of a CVAE conditioned by CL and a mutual information estimator, as shown in \cref{fig:overview}.
\begin{figure*}[t]
    \centering
    \begin{tikzpicture}[%
    data/.style={draw, circle, minimum height=0.7cm, inner sep=0cm, font=\vphantom{Ag}, fill=blue!10},%
    op/.style={draw, rectangle},%
    nnet/.style={op, minimum height=1cm, minimum width=1.5cm, fill=green!10, align=center},%
    flow/.style={-{Latex[length=1.5mm]}},%
    ]
        \node[data] (input) at (0, 0) {$x$};
        \node[nnet] (enc_im) at (1.7, 0.7) {Encoder\\ $E_\phi$};
        \node[nnet, fill=green!10!gray!50] (enc_ex) at (1.7, -0.7) {CL\\ $f_\psi$};
        \node[data, right=1.5cm, yshift=-0.5cm] (mean) at (enc_im) {$\mu$};
        \node[data, right=1.5cm, yshift=0.5cm] (std) at (enc_im) {$\sigma$};
        \node[op, right=2.5cm] (sampling) at (enc_im){\rotatebox{270}{Sampling}};
        \node[data, right=4cm] (z_im) at (enc_im) {$z_\text{SD}$};
        \node[data, right=4cm] (z_ex) at (enc_ex) {$z_\text{SI}$};
        \node[nnet] (dec) at (8, 0.7) {Decoder\\ $D_\theta$};
        \node[nnet] (sn) at (8, -0.7) {SN\\ $T_\xi$};
        \node[data] (output) at (9.5, 0.7) {$\hat{x}$};
        \node[data] (mine) at (9.5, -0.7) {$\hat{I}$};

        \draw[flow] (input) -- (enc_im.west);
        \draw[flow] (input) -- (enc_ex.west);
        \draw[flow] (enc_im) -- (mean);
        \draw[flow] (enc_im) -- (std);
        \draw[flow] (mean) -- (mean-|sampling.west);
        \draw[flow] (std) -- (std-|sampling.west);
        \draw[flow] (sampling) -- (z_im);
        \draw[flow] (enc_ex) -- (z_ex);
        \draw[flow] (z_im) -- (dec);
        \draw[flow] (z_ex) -- (dec.west);
        \draw[flow] (z_im) -- (sn.west);
        \draw[flow] (z_ex) -- (sn);
        \draw[flow] (dec) -- (output);
        \draw[flow] (sn) -- (mine);

        \node[rectangle callout, draw, densely dotted, callout absolute pointer=(enc_ex.south), yshift=-1cm] at (enc_ex){Pretrained and freezed};
        \node[rectangle callout, draw, densely dotted, callout absolute pointer=(z_ex.south), yshift=-1cm] at (z_ex){Style-independent feature};
        \node[rectangle callout, draw, densely dotted, callout absolute pointer=(z_im.north), yshift=1cm] at (z_im){Style feature};
    \end{tikzpicture}
    \caption{%
    Overview of the proposed method. The VAE that is conditioned by the CL model extracts the style features.
    While the training procedure, the estimated mutual information of two feature vectors $z_\text{SI}$ and $z_\text{SD}$ is evaluated in order to encourage the features to be statistically independent.
    }
    \label{fig:overview}
\end{figure*}

% 以下では最初にスタイル抽出のための CVAE を構築する．
Firstly, we construct the CVAE for style extraction.
% 次に，CVAE の条件としてスタイルに独立な特徴を抽出できる CLと，CVAE がスタイル特徴のみを抽出するのを助けるための相互情報量に基づいた制約を導入する．
Next, as the condition of the CVAE, we introduce CL that extracts style-independent features.
Then, we present a constraint based on mutual information that helps CVAE to extract only style information.
% 最後に，提案手法の学習方法および，学習のための損失関数をまとめる．
Finally, we explain the proposed method's training procedure and loss function.

\subsection{Style Extracting CVAE}
% データがスタイルとそれ以外の情報によって生成されると考えると，観測データ $X = \{x_1, \dots, x_N\}$ が得られたとき，その生成モデルは
Assume that data $X = \{x_1, x_2, \dots, x_N\}$ are generated based on style information and other information.
Then, we consider a generative model
\begin{equation}
    x \sim p_\theta(x|z_\text{SI}, z_\text{SD}),
    \label{eq:cvae_generative_model}
\end{equation}
% と書ける．
% ここで，$z_\text{SI}$ はスタイルに独立な変数，$z_\text{SD}$ はスタイルの情報を含む変数である．
where $\theta$ is the parameter of the distribution $p$, $z_\text{SI}$ is a style-independent variable, and $z_\text{SD}$ is a style-dependent variable.
% スタイル独立変数はラベルに対応するものである．
The style-independent variable $z_\text{SI}$ typically corresponds to class labels.

% スタイルに独立な $z_\text{SI}$ が所与であり，$z_\text{SD}$ が標準正規分布に従うと仮定すれば，$z_\text{SI}$ を条件とする CVAE~\cite{Kingma2014a,Sohn2015} の枠組みで $p_\theta$ を観測データから推定できる．
Given the style-independent feature $z_\text{SI}$ and assuming $z_\text{SD}$ is drawn from standard Gaussian distribution, %we can estimate $p_\theta$ using CVAE~\cite{Kingma2014a,Sohn2015} conditioned by $z_\text{SI}$.
CVAE~\cite{Kingma2014a,Sohn2015} a method for estimating $p_\theta$ conditioned by $z_\text{SI}$.
% より具体的には，観測できないスタイル情報を含む変数である $z_\text{SD}$ を予測するためのエンコーダ $q_\phi(z_\text{SD}|x)$ を導入し，変分下界
In the framework of CVAE, we introduce a encoder $q_\phi(z_\text{SD}|x)$ that estimates unobserved $z_\text{SD}$, and then find $p_\theta, q_\phi$ that maximize the lower bound
\begin{align}
    \log p_\theta(x) \geq &-\text{KL}\left(q_\phi(z_\text{SD}|x) \| p(z_\text{SD})\right)\nonumber\\
    &+ \mathbb{E}_{q_\phi(z_\text{SD}|x)}\left[\log p_\theta(x|z_\text{SD}, z_\text{SI})\right],
    \label{eq:elbo}
\end{align}
% を最小化するような $p_\theta, q_\phi$ を求める．
% ただし，$\text{KL}(\cdot\|\cdot)$ は KL ダイバージェンス，$p(z_\text{SD})$ は標準正規分布である．
where $\text{KL}(\cdot\|\cdot)$ is Kullback--Leibler divergence (KLD) and $p(z_\text{SD})$ is standard Gaussian distribution.
% AE の文脈では，$q_\phi$ はスタイル特徴を抽出するエンコーダであり，$p_\theta$ はふたつの特徴ベクトル $z_\text{SI}, z_\text{SD}$ から $x$ を再構成するデコーダである．
In the context of AEs, $q_\phi$ is the encoder that extracts style features, and $q_\theta$ is the decoder that reconstructs the input $x$ from feature vectors $z_\text{SI}$ and $z_\text{SD}$.
Note that the style-independent feature $z_\text{SI}$ is not fed to the encoder $q_\phi$ unlike typical setups of CVAE~\cite{Kingma2014a,Sohn2015}.
This is because our goal is to obtain an encoder that extracts style features only from the input data.

% 実際の学習に際しては，ディープニューラルネット (deep neural network; DNN) で構成されるエンコーダ $E_\phi$ およびデコーダ $D_\theta$ を用いる．
In practice, we utilize DNNs for the implementation of the encoder and the decoder denoted by $E_\phi$ and $D_\theta$, respectively.
% 潜在変数 $z_\text{SD}$ が標準ガウス分布に従うことに注意し，また $p_\theta$ が分散の固定された等方ガウス分布と仮定すれば，学習に用いる具体的な損失関数は
Assuming that $p_\theta$ is an isotropic Gaussian distribution, the DNNs are trained by the empirical loss function given as
\begin{align}
    L_\text{CVAE}(\theta, \phi; X) =& \lambda_\text{KL} \frac{1}{2}\sum_{i=1}^{|X|}\left[1+\log(\sigma_i^2)-\mu_i^2-\sigma_i^2\right] \nonumber\\
    & + \lambda_\text{recon} \frac{1}{N}\sum_{x\in X} \|D_\theta(x; z_\text{SD}, z_\text{SI}) - x\|^2,
    \label{eq:loss_vae}
\end{align}
% と書ける．
% ここで，$\lambda_\text{KL}, \lambda_\text{recon}$ はそれぞれ KL ダイバージェンスと再構成誤差の重みであり，$\mu, \sigma$ はエンコーダ $E_\phi(x)$ の出力である．
where $\lambda_\text{KL}$ and $\lambda_\text{recon}$ are the weights of the KLD and the reconstruction loss, respectively, and $\mu$ and $\sigma$ are the output of the encoder $E_\phi(x)$.

\subsection{CL as Style-Independent Feature Extractor}
% 次に，\cref{eq:cvae_generative_model} では所与であるとした $z_\text{SI}$ を CL によって求める方法を述べる．
We introduce CL to obtain style-independent feature $z_\text{SI}$ that we assumed given in \cref{eq:cvae_generative_model}.
% CL~\cite{SimCLR,MoCo,SimSiam} は，元データが同じであれば異なるデータ拡張を適用しても特徴ベクトルは等しくなるべきであるというアイデアの自己教師あり表現学習手法である．
CL~\cite{SimCLR,MoCo,SimSiam} is a self-supervised representation learning framework.
It is based on the following simple idea; the two feature vectors should be the same if they are derived from the same data but with different data augmentation operations.
% データ拡張をスタイルの摂動と捉えれば，CL はスタイルに独立な特徴を抽出していると考えられる~\cite{Kugelgen2021}．
By considering the data augmentation as a perturbation in styles, CL models can be seen as style-independent feature extractors~\cite{Kugelgen2021}.
% したがって，学習済みの CL モデルがあれば，スタイルに独立な特徴ベクトル $z_\text{SI}$ を出力する特徴抽出器として利用できる．
Therefore, we can use pretrained CL models for the feature extractors that output $z_\text{SI}$.

% CL の具体的な構成法を，CL 手法のひとつである MoCo~\cite{MoCo} に基づいて紹介する．
We briefly review a CL framework based on MoCo~\cite{MoCo}, which is popular and relatively lightweight.
% MoCo では，パラメータ $\psi$ をもつ特徴抽出器 $f_\psi$ を学習するための損失関数に InfoNCE
In MoCo, a feature extractor $f_\psi$ with the parameter $\psi$ is trained by minimizing InfoNCE given by
\begin{gather}
    L_\text{CL}(\psi) = -\log\frac{\exp(q\cdot k_+ / \tau)}{\exp(q\cdot k_+ / \tau) + \sum_{i=1}^K \exp(q\cdot k_- / \tau)},
    \label{eq:loss_cl}\\
    q = \text{MLPHead}(z_\text{SI}), z_\text{SI} = f_\psi(x),
\end{gather}
% を用いる．
% ただし，$x$ は入力データ，$q$ は $x$ に対応する中間表現，$k_+$ は $q$ と同じ表現になるべき正例，$k_-$ は $q$ と違う表現になるべき負例，$K$ は負例の数であり，また，$\|q\| = \|k_+\| = \|k_-\| = 1$ である．
where $x$ is the input data, $q$ is the representation corresponding to $x$, $k_+$ is the positive key, $k_-$ is the negative key, and $K$ is the number of negative keys.
Note that $q, k_+$, and $k_-$ are normalized (i.e., $\|q\| = \|k_+\| = \|k_-\| = 1$).
% 正例 $k_+$ は $x$ と同じデータに異なるデータ拡張を適用したものに対応する表現，負例 $k_-$ は $x$ とは異なるデータに対応する表現である．
The positive key $k_+$ is the representation corresponding to the same data as $x$ but with different data augmentation.
The negative key $k_-$ is the representation corresponding to the data different from $x$.
% 一般的に CL において InfoNCE を用いるときは特徴ベクトルそのものではなく，2 層程度の全結合層からなる関数 $\text{MLPHead}(\cdot)$ の出力を評価する．
In general CL, we use the output of $\text{MLPHead}(\cdot)$, often consisting of a few fully-connected layers, to evaluate InfoNCE.
% したがって，CL の成果物としての特徴ベクトルは，MLPHead の直前の出力である $z_\text{SI}$ となる．
Once $f_\psi$ is trained, we employ $z_\text{SI}$, which is the input of MLPHhead, for the inferred feature vector of the trained CL models.

% 安定した学習のためには $K$ を大きくする必要があるため~\cite{SimCLR} 計算量のボトルネックとなるが，MoCo では $f_\psi$ に徐々に追従する momentum encoder を用意することで効率よい計算が可能である．
% \cref{eq:loss_cl} に基づく学習により，ImageNet~\cite{ILSVRC15} をよく線形分離できる特徴抽出器 $f_\psi$ が得られることが知られている~\cite{SimCLR,MoCo,SimSiam}．

\subsection{Mutual Information Constraint by MINE}
% CVAE による特徴ベクトル $z_\text{SD}$ にスタイル情報のみが抽出されるよう，スタイルに独立な特徴ベクトルである $z_\text{SI}$ との独立性を担保する制約を設けることを考える．
To ensure that the feature vector of the CVAE ($z_\text{SD}$) only contains style information, we introduce a constraint to encourage the independence between $z_\text{SI}$ and $z_\text{SD}$.
% \cref{eq:cvae_generative_model} で与えられる生成モデルは $z_\text{SI}$ と$z_\text{SD}$ の独立性を仮定しているが，エンコーダ $E_\phi$ とデコーダ $D_\theta$ の自由度によっては，必ずしも $\cref{eq:loss_vae}$ の最小化によって $z_\text{SD}$ がスタイル情報のみを含むような解を得られない．
The generative model presented in \cref{eq:cvae_generative_model} assumes that $z_\text{SI}$ and $z_\text{SD}$ are independent, but this assumption does not always hold.
% より具体的には，$z_\text{SI}$ を無視し，$z_\text{SD}$ のみを使って再構成誤差を下げることでも\cref{eq:loss_vae} で与えられる損失関数の値を小さくすることができる．
Specifically, the loss function of the CVAE given by \cref{eq:loss_vae} can be lowered by ignoring the condition $z_\text{SI}$ when the DNNs $E_\phi$ and $D_\theta$ have high degrees of freedom. % 「自由度」の言い方、もうちょっと考えたい。
% この場合，$z_\text{SD}$ はスタイル情報以外にも $z_\text{SI}$ に含まれる情報を持っており，両者は独立とはいえない．
In this case, $z_\text{SD}$ has enough information for reconstructing the input from it.
This means that $z_\text{SD}$ contains features other than styles, and $z_\text{SD}$ and $z_\text{SI}$ are not independent.
To alleviate this problem, we consider evaluating the independence of the two feature vectors.
% ふたつの変数の独立性を測る指標のひとつが，相互情報量である~\cite{Bishop2006}．
Mutual information is one of the measures to evaluate independence between two variables~\cite{Bishop2006}.

% 相互情報量を DNN を用いて推定する方法である MINE~\cite{MINE} を用いることで，$z_\text{SI}$ と$z_\text{SD}$ の独立性を評価できる．
By using the DNN-based mutual information estimator, MINE~\cite{MINE}, we measure the independence between $z_\text{SI}$ and $z_\text{SD}$.
% MINE の枠組みでは，DNN によって構成される statistics network (SN) $T_\xi$ を用いて $z_\text{SI}$ と $z_\text{SD}$ の相互情報量の下界
In MINE framework, we introduce a DNN $T_\xi$ called statistics network (SN) and find $T_\xi$ that maximizes the lower bound of the mutual information
\begin{align}
    I(z_\text{SI}; z_\text{SD}) &\geq \hat{I}_\xi(z_\text{SI}; z_\text{SD}) \nonumber\\
    &= \mathbb{E}[T_\xi(z_\text{SI}, z_\text{SD})] - \log\left(\mathbb{E}[T_\xi(z_\text{SI}, \bar{z}_\text{SD})]\right),
    \label{eq:mine_kl}
\end{align}
% を最大化することによって相互情報量の推定値とする．
% ただし，$\bar{z}_\text{SD}$ は $z_\text{SD}$ と同じ分布に従う変数である．
where $\bar{z}_\text{SD}$ is a variable distributed the same as $z_\text{SD}$.
We can draw $\bar{z}_\text{SD}$ from standard Gaussian distribution because we have assumed $z_\text{SD}$ is standard Gaussian distributed in the CVAE framework.
Note that the estimated mutual information is the maximized value of $\hat{I}_\xi$.
% \cref{eq:mine_kl} は導出の過程で対称性のない KL ダイバージェンスを用いており実際に SN を学習させると不安定になるという指摘がある~\cite{MINE,DeepInfomax} ため，本稿では，代わりに JS ダイバージェンスを用いた変種である推定値~\cite{DeepInfomax}
Because the estimator given in \cref{eq:mine_kl} is based on asymmetric KLD, the actual learning procedure of SN tends to be unstable~\cite{MINE,DeepInfomax}.
To alleviate this, we use a variant of the estimator that uses Jensen--Shannon divergence instead of KLD~\cite{DeepInfomax};
\begin{align}
    \hat{I}_\xi^\text{JS}(z_\text{SI}; z_\text{SD}) 
    = \mathbb{E}[-\text{sp}(-T_\xi(z_\text{SI}, z_\text{SD}))] - \mathbb{E}[\text{sp}(T_\xi(z_\text{SI}, \bar{z}_\text{SD}))],
    \label{eq:mine_jsd}
\end{align}
% を用いる．ただし，$\text{sp}(\cdot)$ は softplus 関数~\cite{Softplus}である．
where $\text{sp}(\cdot)$ is softplus function~\cite{Softplus}.
% 観測データ $X$ とそれに対応する CL および VAE の特徴ベクトルの集合 $Z_\text{SI} = \{z_\text{SI}^{(1)}, \dots, z_\text{SI}^{(N)}\}, Z_\text{SD} = \{z_\text{SD}^{(1)}, \dots, z_\text{SD}^{(N)}\}$ が与えられたとき，この推定値は
Given observations $X$ and the corresponding feature vectors $Z_\text{SI} = \{z_\text{SI}^{(1)}, \dots, z_\text{SI}^{(N)}\}, Z_\text{SD} = \{z_\text{SD}^{(1)}, \dots, z_\text{SD}^{(N)}\}$, the empirical form of \cref{eq:mine_jsd} becomes
\begin{align}
    \hat{I}_\xi^\text{Emp}(Z_\text{SI}; Z_\text{SD}) &=  \frac{1}{N}\sum_{n=1}^N -\text{sp}(-T_\xi(z_\text{SI}^{(n)}, z_\text{SD}^{(n)})) \nonumber\\
    &\hspace{2em}- \frac{1}{N}\sum_{n=1}^N\text{sp}(T_\xi(z_\text{SI}^{(n)}, \bar{z}_\text{SD}^{(n)})).
    \label{eq:emp_mine}
\end{align}
% となる．
% 学習時にはこれを VAE の損失関数である\cref{eq:loss_vae} に加えることで，$z_\text{SI}$ と $z_\text{SD}$ のもつ情報が独立になるように促す．
We encourage $z_\text{SI}$ and $z_\text{SD}$ to be independent by adding this empirical mutual information estimator to the loss function of CVAE (\cref{eq:loss_vae}) in the training procedure.

\subsection{Training and Loss Function}
% 提案するモデルである，MINE による情報量制約のある CL の特徴ベクトルによる条件つき VAE の概要を\cref{fig:overview} に示す．
% 提案手法の学習はふたつのステップからなる．
The training procedure of the proposed method consists of two steps.
% まず，CL のモデル $f_\psi$ を事前に学習しておき，重みを固定する．これにより，$f_\psi$ をスタイルに独立な特徴の抽出器として用いる．
As the first step, we train the CL model $f_\psi$ by any existing CL methods and freeze the parameters of the model.
The trained $f_\psi$ can be employed as a style-independent feature extractor.
% 次に，スタイル特徴抽出のための CVAE を学習する．CVAE の学習に際しては，MINE によって CL と VAE が抽出する特徴ベクトルが独立になるよう制約を加えることで，CVAE がスタイル特徴のみを抽出することを補助する．
In the second step, we train the CVAE for style feature extraction.
The training of CVAE is constrained by MINE so that the two feature vectors $z_\text{SI}$ and $z_\text{SD}$ are independent.
This constraint helps CVAE to extract only style features.

% VAE のエンコーダ $E_\phi$ およびデコーダ $D_\theta$ に対する損失関数は，\cref{eq:loss_vae} で与えられる CVAE としての損失に $z_\text{SI}$ と $z_\text{SD}$ の相互情報量を加えた
The loss function for the encoder $E_\phi$ and the decoder $D_\theta$ is given as a linear combination of the CVAE loss (\cref{eq:loss_vae}) and the estimated mutual information (\cref{eq:emp_mine});
\begin{align}
    &L(\theta, \phi, \xi; X, Z_\text{SI}, Z_\text{SD}) \nonumber\\
    &= L_\text{CVAE}(\theta, \phi; X) + \lambda_\text{MINE} \hat{I}_\xi^\text{Emp}(Z_\text{SI}; Z_\text{SD})
\end{align}
% とする．
% ただし，$\lambda_\text{MINE}$ は MINE の重みである．
where $\lambda_\text{MINE}$ is the weight of the mutual information.
% 一方で，相互情報量の推定値 $\hat{I}_\xi^\text{Emp}$ は\cref{eq:emp_mine} で与えられる下界で与えられるため，$T_\xi$ を学習することで下界を最大化する必要がある．
The estimated mutual information $\hat{I}_\xi^\text{Emp}$ needs to be maximized by training $T_\xi$.
Thus, the proposed model that consists of the CVAE and the SN is trained by the min-max problem below:
\begin{equation}
    \min_{\theta, \phi} \max_{\xi} L(\theta, \phi, \xi; X, Z_\text{SI}, Z_\text{SD}).
\end{equation}
% によって学習する．
% これは generative adversarial networks をはじめとした敵対的学習~\cite{GAN}に類似した問題であり，同様の方法で最適化できる．
This problem is similar to that of adversarial learning (e.g., generative adversarial networks~\cite{GAN}) and can be optimized in the same manner.
