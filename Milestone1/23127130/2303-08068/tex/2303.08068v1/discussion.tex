\section{Discussion}
In our experiments using MNIST-like datasets, the proposed method almost successfully extracted style features (\cref{fig:da_removal,fig:interp,fig:style_transfer,fig:nn}).
The extracted styles were related to the characters' size, thickness, slant, position, and so forth.
Additionally, the style extraction performance was retained across different CL methods (\cref{fig:da_removal_variations,fig:interp_variations}).

However, in the experiments with the Google Fonts dataset, the font faces, which can be easily understood as styles, were not extracted as styles (\cref{fig:da_removal_font,fig:interp_font,fig:style_transfer_font,fig:nn_vae_font}).
The reason for these results is likely due to the data augmentation used.
The proposed method aims to capture the information that CL isolates, and the information isolated by CL heavily relies on data augmentation.
The data augmentation used was appropriate for isolating the styles in the case of MNIST, but not for the Google Fonts dataset.
For capturing font faces as styles, data augmentation should cover the differences between different font faces by adding morphological transformations, for example.

In addition, the choice of the condition for the CVAE is important.
As the experiments with the different CL methods and the supervised variant (\cref{fig:da_removal_variations,fig:interp_variations}) have shown, the extracted style features were slightly different across the CL methods.
Moreover, when the class labels were given (i.e., the supervised variant), the proposed method extracted more diverse styles.
These results indicate that the proposed method's style extraction performance depends on the quality of the style-independent feature $z_\text{SI}$.
We can change the conditions for the CVAE depending on the styles and datasets we want to extract.
In this paper, we used the CL as the condition to train the model with completely unlabeled data.
However, if some labels are available, making use of them in supervised or semi-supervised ways is a possible choice.

The proposed method also demonstrated the ability to deal with real-world natural image datasets in the experiments (\cref{fig:style_transfer_realworld,fig:nn_realworld}).
The style transfer experiments showed that the CVAE extracted styles.
However, in the Imagenette experiments, we did not find common styles in the neighbor analysis, although we did in DAISO-100.
This may be because Imagenette is a small dataset with a relatively large variety of data, resulting in the learned style feature space being too sparse to find meaningful neighbors.

In terms of the natural image datasets, the quality of the generated images is a limitation.
Our aim is not to generate images but to extract style features; however, high-quality image generation is desirable, especially for evaluating the learned feature.
Improving the decoder model or combining it with adversarial models such as GANs~\cite{GAN} would be one way to achieve this.

Although the experimental results illustrated the style-extracting ability of the proposed method, the evaluations were qualitative.
We would need quantitative evaluations to compare the performance of the different CL methods in detail.
One way to do this is to prepare a dataset with explicit style labels and a set of data augmentation operations that can perturb only the explicit styles.
