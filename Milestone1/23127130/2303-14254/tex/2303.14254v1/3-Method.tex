\section{Methodology}

\subsection{Overview}
We focus on multivariate time series forecasting task. A multivariate time series sequence of length $T$ and feature number $c$ is denoted as $\mathbf{S}=[\mathbf{s_1},\dots,\mathbf{s_t},\dots,\mathbf{s_T}] \in \mathbb{R}^{c \times T}$, where $\mathbf{s_t} = [s_1, \cdots, s_c]^T \in \mathbb{R}^c$. In a forecasting task, we only observe history values $\mathbf{H}$ up to timestamp $d<T$: $\mathbf{H}=[\mathbf{s_1},\dots,\mathbf{s_d}] \in \mathbb{R}^{c \times d}$, and the goal is to forecast future values $\mathbf{F}$ at timestamp $d+1,\dots,T$: $\mathbf{F}=[\mathbf{s_{d+1}},\dots,\mathbf{s_T}] \in \mathbb{R}^{c \times (T-d)}$, where $\mathbf{S} = [\mathbf{H}, \mathbf{F}]$. During training, we have full access to both $\mathbf{H}$ and $\mathbf{F}$, and the training objective is to  learn a model $g$ that forecasts $\mathbf{F}$ given $\mathbf{H}$ for each $\mathbf{S}$ in the training set. During testing, we have access to \emph{only} $\mathbf{H}$ and input $\mathbf{H}$ to model $g$ to obtain predictions for the future part.

Our method \our comprises augmentation in both the frequency domain and time domain. In the frequency domain, we first apply empirical mode decomposition to obtain a set of components. Then during each iteration, these components are re-combined with random weights to construct a new synthetic series. Then, we adapt mix-up as a time-domain augmentation. We linearly interpolate two randomly re-combined series to obtain the final augmented series. The augmented series are fed into the forecasting model for updating gradient. The EMD components of different series could be pre-computed, and \our only requires randomly re-combining components or series during training, which introduces minimal computational overhead. 

\subsection{Frequency-Domain Augmentation}
The Empirical Mode Decomposition (EMD) \cite{huang1998empirical} method was originally designed to analyze nonlinear and non-stationary data, whose constituent components are not necessarily cyclic. EMD preserves temporal information in contrast with Fourier transform, and is data-driven compared with the linear wavelet transform. It decomposes a signal into a finite number of Intrinsic Mode Functions (IMF).
The first several IMFs usually carry components of higher frequency (e.g., noise), while the last several IMFs represent the low-frequency trend information embedded in the sequence. Therefore, EMD provides a principled way to decompose a signal into multiple components, and each of these components represents certain patterns embedded in the original signal.

After EMD, the original sequence could be written as 

\begin{equation}
\mathbf{S} = \sum_{i=1}^n \mathrm{IMF}_i + \mathbf{R}. 
\end{equation}

With a list of $n$ decomposed IMFs $\{\mathrm{IMF}_1,\dots,\mathrm{IMF}_n\}$ and residual $\mathbf{R}$, we apply a random vector $\mathbf{w}=[w_1,\dots,w_n]^T$ as weights to re-combine these IMFs as $\mathbf{S'}$:

\begin{equation}
    \mathbf{S'} = \sum_{i=1}^n w_i \cdot \mathrm{IMF}_i
\end{equation}

where $w_i$ is sampled from uniform distribution $U(0,2)$. This way, the augmented samples are diverse by emphasizing different frequency components via random weights, and at the same time coherent with original distributions as they contain the same basic sets of components.

\subsection{Time-Domain Augmentation}
Complementing the frequency-domain information, time domain also provides useful patterns. Therefore, we propose to mix up sequences in the time domain, inspired by Mix-up augmentation \cite{zhang2017mixup}. 
Mix-up was originally designed for classification, and we adapt it to time-series forecasting by mixing up values at both past timestamps $1,\dots,d$ and future timestamps $d+1,\dots,T$. 
Assume $\mathbf{S_i}=[\mathbf{H_i},\mathbf{F_i}]$ and $\mathbf{S_j}=[\mathbf{H_j},\mathbf{F_j}]$ are two randomly sampled sequences after spectral augmentation, where $\mathbf{H_i}=[\mathbf{s_i^1},\dots,\mathbf{s_i^d}],\mathbf{F_i}=[\mathbf{s_i^{d+1}},\dots,\mathbf{s_i^T}]$ respectively represents past and future data, similarly for $\mathbf{S_j}$. We construct new sequence as $\mathbf{S'} = [\mathbf{H'},\mathbf{F'}]$, where
\begin{equation}
    \mathbf{H'} = \lambda \mathbf{H_i} + (1-\lambda) \mathbf{H_j},
\end{equation}
\begin{equation}
    \mathbf{F'} = \lambda \mathbf{F_i} + (1-\lambda) \mathbf{F_j},
\end{equation}
where $\lambda$ is sampled from a Beta distribution, i.e., $\lambda \sim \mathrm{Beta}(\alpha,\alpha)$, and $\alpha$ is the hyper-parameter that controls how similar the newly constructed sequence is compared with the original sequences $\mathbf{S_i}$ and $\mathbf{S_j}$. Mix-up augments patterns in the time domain meanwhile by its interpolation nature generates only linearly in-between coherent samples.

\subsection{Time-Series Forecasting}
For each training iteration, we apply both frequency-domain and time-domain augmentation to obtain an augmented series $\mathbf{S'}=[\mathbf{H'},\mathbf{F'}]$. During training, we feed the augmented series history $\mathbf{H'}$ as input, and optimize the forecasting model through reconstructing future part of the series $\mathbf{F'}$. In our experiments, we adopt the state-of-the-art forecasting model Informer~\cite{zhou2021informer} (AAAI 2021 best paper) as the base forecaster. To minimize the reconstruction loss $\mathcal{L}$, we calculate Mean Square Error (MSE) between the forecasting model output $\mathbf{Y}$ and the ground-truth future part $\mathbf{F'}$: 
\begin{equation}
    \mathcal{L} = \frac{1}{N}\sum_{i=1}^N ||\mathbf{Y_i}-\mathbf{F'_i}||_2^2,
\end{equation}
where $N$ is the number of augmented series in training set.

