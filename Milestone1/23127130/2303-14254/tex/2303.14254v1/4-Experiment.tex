\begin{table*}[t]
\centering
\begin{footnotesize}
\caption{MSE and MAE on benchmark datasets with input context length $96$ and forecasting horizon $ \{96,192,336,720\}$. We \textbf{bold} the best performing results, \underline{underline} the second best, and \dashuline{mark with dashline} the best baseline.}
\scalebox{0.77}{
\begin{tabular}{c|c|cccccccccccccc|cccccc}
\toprule
\multicolumn{2}{c|}{Methods}&\multicolumn{2}{c|}{None}&\multicolumn{2}{c|}{WW}&\multicolumn{2}{c|}{RobustTAD}&\multicolumn{2}{c|}{STL}&\multicolumn{2}{c|}{EMD-R}&\multicolumn{2}{c|}{GAN}&\multicolumn{2}{c|}{DBA}&\multicolumn{2}{c|}{\timeabl}&\multicolumn{2}{c|}{\freqabl}&\multicolumn{2}{c}{\our}\\
\midrule
\multicolumn{2}{c|}{Metric} & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\
\midrule
\multirow{4}{*}{\rotatebox{90}{ETTh1}} & 96 & 0.947 & 0.760 &0.894&0.730&1.011&0.789&0.910&0.738&\dashuline{0.838}&\dashuline{0.686}&0.904&0.734&0.943&0.759& \underline{0.658}&\underline{0.573}&0.841&0.693&\textbf{0.645}&\textbf{0.572}\\ 
& 192 &0.977&0.767&0.999&0.775&0.946&0.744&\dashuline{0.942}&0.740&\dashuline{0.942}&\dashuline{0.729}&0.985&0.757&0.977&0.767&\textbf{0.754}&\textbf{0.616}&0.962&0.755&\underline{0.772}&\underline{0.628} \\
& 336 & 1.112&0.833&1.058 &0.808 &1.094&0.816&1.086&0.814&\dashuline{1.056}&\dashuline{0.804}&1.070&0.816&1.111&0.832&\textbf{0.860}&\textbf{0.681}&1.103&0.829&\underline{0.889}&\underline{0.705}\\
& 720 & 1.182&0.864&1.201 &0.884 &\dashuline{1.146}&\dashuline{0.841}&1.187 &0.863 &1.187&0.866&1.221&0.892&1.184&0.866&\underline{0.994}&\underline{0.752}&1.162&0.857&\textbf{0.972}&\textbf{0.746} \\
\midrule
\multirow{4}{*}{\rotatebox{90}{ETTh2}} &96&{3.084}&{1.383}&3.399 &1.463 &3.028&1.380&3.042&1.374&3.440&1.451&\dashuline{2.906}&1.377&3.005&\dashuline{1.370}&\textbf{1.802}&\textbf{1.012}&2.984&1.371&\underline{1.869}&\underline{1.046} \\ 
& 192&5.966&2.023& 6.432& 2.098& 6.203&2.083&5.894&2.023&6.326&2.089&\dashuline{5.467}&\dashuline{1.939}&5.636&\dashuline{1.939}&\underline{4.023}&\underline{1.569}&5.229&1.886&\textbf{3.467}&\textbf{1.483} \\
& 336&4.775&\dashuline{1.829}&4.965 &1.852 &5.287&1.883&4.774&1.833&5.436&1.948&4.906&1.868&\dashuline{4.765}&\dashuline{1.829}&\underline{3.391}&\textbf{1.482}&4.374&1.736&\textbf{3.365}&\underline{1.513} \\
& 720&3.991&1.694&3.828 &1.677 & 3.896&1.659&\dashuline{3.563}&\dashuline{1.579}&4.082&1.717&4.060&1.744&4.009&1.698&\underline{2.700}&\underline{1.377}&3.571&1.619&\textbf{2.621}&\textbf{1.340}\\
\midrule
\multirow{4}{*}{\rotatebox{90}{ETTm1}} &96 & 0.633&0.570&0.629&0.570&0.593&0.546&\dashuline{0.549}&\dashuline{0.534}&0.640&0.564&0.626&0.576&0.634&0.573&\underline{0.428}&\underline{0.439}&0.550&0.525&\textbf{0.403}&\textbf{0.424}\\ 
& 192 & 0.797&0.673&0.792&0.665&0.802&0.663&0.738&0.646&\dashuline{0.682}&\dashuline{0.590}&0.842&0.689&0.803&0.675&\underline{0.538}&\underline{0.505}&{0.668}&{0.591}&\textbf{0.489}&\textbf{0.482}\\
& 336 &1.149&0.839&0.997 &0.774 &0.983&0.759&0.992&0.773&\dashuline{0.869}&\dashuline{0.695}&0.902&0.725&1.146&0.838&\underline{0.648}&\underline{0.567}&0.900&0.715&\textbf{0.627}&\textbf{0.559} \\
& 720 & 1.128&0.805&1.077 &0.786 &1.063&0.789&1.184 &0.820 &\dashuline{1.008}&\dashuline{0.748}&1.225&0.862&1.131&0.806&\underline{0.828}&\underline{0.657}&0.934&0.720&\textbf{0.764}&\textbf{0.628}\\
\midrule
\multirow{4}{*}{\rotatebox{90}{ETTm2}} &96 &0.415&0.506&0.385&0.476&0.386&0.476&{0.388}&{0.489}&0.398&0.478&\dashuline{0.334}&\dashuline{0.430}&0.415&0.506&\underline{0.297}&\underline{0.394}&{0.328}&{0.423}&\textbf{0.292}&\textbf{0.388} \\ 
& 192 &0.776&0.675&0.784&0.680&0.875&0.724&0.712&0.650&\dashuline{0.542}&\dashuline{0.551}&0.625&0.604&0.767&0.673&\textbf{0.420}&\textbf{0.490}&0.592&0.585&\underline{0.429}&\underline{0.497} \\
& 336 &1.515&0.946&1.425 &0.923 &1.390&0.909&1.402&0.908&\dashuline{1.287}&\dashuline{0.875}&1.453&0.929&1.568&0.962&\underline{0.943}&\underline{0.735}&{1.287}&{0.876}&\textbf{0.811}&\textbf{0.692} \\
& 720 & 3.462&1.423&4.370 &1.637 &4.792&1.654& 3.271& 1.378&3.247 &\dashuline{1.353}&\dashuline{3.129}&1.355&3.475&1.427 &\textbf{2.645}&\textbf{1.201}&3.765&1.509&\underline{2.790}&\underline{1.268}\\
\midrule
\multirow{4}{*}{\rotatebox{90}{Exchange}} &96 &0.959&0.784&\dashuline{0.799}&\dashuline{0.687}&0.962&0.788&0.860&0.746&0.923&0.761&0.942&0.772&0.958&0.784&\underline{0.285}&\underline{0.402}&0.809&0.708&\textbf{0.271}&\textbf{0.395} \\ 
& 192 &1.113&0.837&\dashuline{1.084} &\dashuline{0.787} &1.180&0.873&1.143&0.852&1.182&0.853&1.115&0.855&1.109 &0.836 &\underline{0.627}&\underline{0.615}&1.125&0.822&\textbf{0.606}&\textbf{0.605} \\
& 336 &1.605&1.009&1.697 &0.995 &1.562&1.006&1.548&0.987&1.606&1.001&\dashuline{1.465}&\dashuline{0.972}&1.595 &1.007 &\underline{1.007}&\underline{0.807}&1.438&0.944&\textbf{0.935}&\textbf{0.779} \\
& 720 &2.847&1.390&3.198 &1.481 &2.816&1.380&\dashuline{2.484} &\dashuline{1.297} &2.914&1.413&2.571&1.309&2.847&1.390 &\textbf{1.409}&\textbf{0.928}&{2.056}&1.153&\underline{1.771}&\underline{1.052} \\
\bottomrule
\end{tabular}
\label{tab:main}
}
\end{footnotesize}
\end{table*}

\section{Experimental Setup and Results}
\label{sec:exp}

\subsection{Datasets, Baselines and Experimental Setup}
We evaluate our augmentation method \our on five time-series datasets: ETTh1, ETTh2, ETTm1, ETTm2, Exchange~\cite{zhou2021informer,wu2021autoformer}. We follow previous studies~\cite{zhou2021informer,wu2021autoformer} for $96$ context length and $96, 192, 336, 720$ forecasting horizon. All the datasets in our experiments are multivariate, and we apply EMD and mix-up separately for each original variable, and concatenate them to obtain the augmented multivariate time series. We follow previous works for $7:2:1$ train, validation and test set split, and evaluate the performance with Mean Square Error (MSE) and Mean Absolute Error (MAE). 

We use Informer~\cite{zhou2021informer} as the base forecasting model. $\alpha$ of the $\mathrm{Beta}$ distribution equals $0.5$, which is chosen from a grid search of $\{0.25,0.5,0.75,1.0\}$. $a, b$ of the Uniform distribution $U(a,b)$ are set to $0$ and $2$, respectively. We use Adam optimizer with a decaying learning rate starting from $1\mathrm{e}^{-4}$. 
We compare \our with base model without any augmentation (None), as well as state-of-the-art time series augmentation methods: WW~\cite{le2016data}, DBA~\cite{forestier2017generating,fawaz2018data,bandara2021improving}, EMD-R~\cite{nam2020data}, GAN~\cite{esteban2017real}, STL~\cite{bergmeir2016bagging,bandara2021improving}, RobustTAD~\cite{gao2020robusttad}. We conducted careful grid search for hyper-parameter tuning for each baseline. We repeat all the experiments for $3$ runs and record both average performance and standard deviation. 

\subsection{Main Results}

We evaluate \our and baselines, and report the average performance in Table \ref{tab:main}. Due to space limit, we report the full results (including both average performance and standard deviation) in Table~\ref{tab:main-std} in Appendix. For easier comparison, we also scale MSE (average and standard deviation) with respect to the average MSE of the base model without augmentation. Similarly, we scaled MAE with respect to the average MAE of the base model without augmentation. Results are shown in Table~\ref{tab:main-std-scale} in Appendix. We bold the best results,  underline the second best, and mark with dash line the best baseline. \our consistently outperforms baselines on different datasets by jointly leveraging time-domain and frequency-domain information, with an average reduction of 28.2\% for MSE and 18.0\% for MAE, compared with the best baseline for each dataset. 
To evaluate the statistical significance, we also run the Friedman test and the Wilcoxon-signed rank test with Holmâ€™s $\alpha$ (5\%) following previous work~\cite{li2021shapenet}. The Friedman test shows statistical significance $p = 0.00$ (much smaller than $\alpha = 0.05$), so there exists a significant difference among different methods. The Wilcoxon-signed rank test indicates the statistical significance of STAug compared with all the baselines with $p = 0.00$ far below 0.05.
We also present qualitative comparisons in Figure~\ref{fig:pred}. Compared with the predictions by the base model and the best-performing baseline for the corresponding setting, \our forecasts values that better align with the original time series. 

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figure/ds.pdf}
\caption{MSE and MAE for different down-sampling ratios.
}
\label{fig:ds}

\end{figure}

\begin{figure}[t]
      \centering
      \begin{subfigure}[b]{0.235\textwidth}
          \centering
          \includegraphics[width=\textwidth]{figure/case-ettm1-marker.pdf}
          \caption{ETTm1}
          \label{fig:case-ettm1}
      \end{subfigure}
      \begin{subfigure}[b]{0.235\textwidth}
          \centering
          \includegraphics[width=\textwidth]{figure/case-ex-ww.pdf}
          \caption{Exchange}
          \label{fig:case-ex}
      \end{subfigure}
         \caption{Predictions on ETTm1 and Exchange with $192$ horizon. \our predicts data that best align with the ground truth.} 
         \label{fig:pred}
\end{figure}

\subsection{Ablation Study}
To examine the effect of augmentation in different domains, we also conduct ablation study by augmenting data only in the frequency or time domain in Table~\ref{tab:main}. \freqabl stands for the model that \emph{removes} frequency-domain augmentation, and \timeabl stands for the model that \emph{removes} time-domain data augmentation. The performance degrades after removing augmentation in either the frequency or time domain, validating the need of combining both domains. 

\subsection{Robustness Study}

\label{sec:robust}

We sub-sample 10\%, 20\%, 50\% of the Exchange dataset, and compare \our with the base model and the best-performing baseline methods for the corresponding setting (horizon $96$). We report on MSE and MAE in Figure~\ref{fig:ds}. The performances of different methods increase as we have more data. When the sample size is small, the performance gap between augmentation methods and base model without augmentation becomes larger, which shows that augmentation is especially helpful when the original dataset is small. \our leverages information from both time and frequency domains, and performs consistently better with respect to the original sample size. Moreover, \our shows more significant improvement over base model and the best-performing baselines with fewer available samples in the original dataset, which demonstrates its robustness with respect to the number of data samples.


