\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage{jmlr2e}

\usepackage{amsmath}
% \usepackage{amsthm}
\usepackage{amssymb}
% \usepackage[nonatbib]{neurips_2022}
\usepackage{subfigure}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}  % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{wasysym}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{bm}
\usepackage{booktabs}
% \usepackage[usenames,dvipsnames]{color}
\usepackage{colortbl}
% \usepackage[ruled,lined]{algorithm2e}
\usepackage{multirow}
\usepackage{tcolorbox}
\usepackage{float}
\usepackage{color}
% \usepackage{marvosym}
\usepackage[misc]{ifsym}

\definecolor{mygray}{gray}{0.9}
\definecolor{mygray1}{gray}{0.95}
% \hypersetup{
% 	colorlinks=true,
% 	linkcolor=red,
% 	filecolor=blue,
% 	urlcolor=blue,
% 	citecolor=green,
% }

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\etal}{\textit{et al}.}
\newcommand{\ie}{\textit{i}.\textit{e}.}
\newcommand{\eg}{\textit{e}.\textit{g}.}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{axiom}[theorem]{Axiom}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
% \jmlrheading{23}{2022}{1-\pageref{LastPage}}{1/21; Revised 5/22}{9/22}{21-0000}{Author One and Author Two}

% Short headings should be running head and authors last names

\ShortHeadings{Keypoint-Guided Optimal Transport}{Gu, Yang, Zeng, Sun, Xu}
\firstpageno{1}

\begin{document}

\title{Keypoint-Guided Optimal Transport}
% with Applications in Heterogeneous Domain  Adaptation and \\ Image-to-Image Translation}

% \author{\name Author One \email one@stat.washington.edu \\
%        \addr Department of Statistics\\
%        University of Washington\\
%        Seattle, WA 98195-4322, USA
%        \AND
%        \name Author Two \email two@cs.berkeley.edu \\
%        \addr Division of Computer Science\\
%        University of California\\
%        Berkeley, CA 94720-1776, USA}

\author{\name Xiang Gu \email xianggu@stu.xjtu.edu.cn \\
       % \AND
       \name Yucheng Yang \email ycyang@stu.xjtu.edu.cn \\
       % \AND
       \name Wei Zeng \email wz@xjtu.edu.cn \\
       % \AND
       \name Jian Sun\thanks{Jian Sun is the corresponding author.} \email jiansun@xjtu.edu.cn \\
       % \AND
       \name Zongben Xu \email zbxu@xjtu.edu.cn \\
       \addr School of Mathematics and Statistics,
       Xi'an Jiaotong University\\
       Shaanxi 710049, P.R. China}

\editor{My editor}

\maketitle

\begin{abstract}
Existing Optimal Transport (OT) methods mainly derive the optimal transport plan/matching under the criterion of transport cost/distance minimization, which may cause incorrect matching in some cases. In many applications, annotating a few matched keypoints across domains is reasonable or even effortless in annotation burden. It is valuable to investigate how to leverage the annotated keypoints to guide the correct matching in OT. In this paper, we propose a novel KeyPoint-Guided model by ReLation preservation (KPG-RL) that searches for the optimal matching (\ie, transport plan) guided by the keypoints in OT. To impose the keypoints in OT, first, we propose a mask-based constraint of the transport plan that preserves the matching of keypoint pairs. Second, we propose to preserve the relation of each data point to the keypoints to guide the matching. The proposed KPG-RL model can be solved by Sinkhorn's algorithm and is applicable even when distributions are supported in different spaces. We further utilize the relation preservation constraint in the Kantorovich Problem and Gromov-Wasserstein model to impose the guidance of keypoints in them. Meanwhile, the proposed KPG-RL model is extended to the partial OT setting. Moreover, we deduce the dual formulation of the KPG-RL model, which is solved using deep learning techniques. Based on the learned transport plan from dual KPG-RL, we propose a novel manifold barycentric projection to transport source data to the target domain.
As applications, we apply the proposed KPG-RL model to the heterogeneous domain adaptation and image-to-image translation. Experiments verified the effectiveness of the proposed approach.
% \blindtext
\end{abstract}

\begin{keywords}
  Keypoint-guided optimal transport, relation preservation, masked plan, manifold barycentric projection, heterogeneous domain adaptation, image-to-image translation
\end{keywords}

\section{Introduction}\label{sec:introduction}
Optimal Transport (OT)~\citep{villani2009optimal} is a mathematical tool for distribution alignment, mass transport, \textit{etc.} OT has gained increasing attention in the machine learning community. OT aims to derive a transport map or plan between a source and a target distribution, such that the transport cost is minimized.  Due to its capacity to exploit the geometric property of data, OT has been employed in many applications, \eg, computer
vision~\citep{bonneel2011displacement,solomon2015convolutional}, natural language processing~\citep{kusner2015word,huang2016supervised,alvarez2018gromov,yurochkin2019hierarchical}, generative adversarial network~\citep{arjovsky2017wasserstein}, domain adaptation~\citep{7586038}, clustering~\citep{ho2017multilevel,huynh2021efficient}, anomaly detection~\citep{tong2022fixing}, dataset comparison~\citep{alvarez2020geometric}, \textit{etc}.
OT has two typical formulations, \ie, Monge's formulation~\citep{monge1781memoire} and Kantorovich's formulation~\citep{kantorovich1942translocation}. Kantorovich's formulation~\citep{kantorovich1942translocation} relaxes Monge's formulation and attracts broader studies in applications.
We mainly deal with Kantorovich's formulation in this paper.
% Unless otherwise stated, by OT, we refer to Kantorovich's formulation in this paper.
The original OT model, \ie, the Kantorovich Problem~\citep{kantorovich1942translocation} (KP), is a linear program that is computationally expensive. The entropy-regularized OT~\citep{cuturi2013sinkhorn,dessein2018regularized} introduces the entropy as regularization to the OT model which is solved by the computationally cheaper Sinkhorn-Knopp algorithm~\citep{sinkhorn1967concerning,lin2022efficiency} allowing to use automatic differentiation~\citep{genevay2018learning}.

KP needs to transport all the mass of source distribution to exactly match the mass of target distribution. But in some cases, only partial mass of source and target distributions should be matched, \eg, the source or target samples contain outliers. To overcome this limitation, the partial OT~\citep{guittet2002extended,figalli2010optimal,caffarelli2010free,chapel2020partial}, unbalanced OT~\citep{chizat2018interpolating,liero2018optimal}, and robust OT~\citep{balaji2020robust,mukherjee2021outlier,le2021robust} models are presented, that allow to transport only partial mass of distribution. Another extension of KP is the Gromov-Wasserstein (GW) model~\citep{sturm2006geometry,memoli2011gromov} that computes the distance between metrics defined within each domain rather than between samples across domains as in KP.

\begin{figure}[t]
	\centering
	\subfigure[Samples]{\label{fig:toy_data}\includegraphics[width=0.2\columnwidth]{figure//data_point.pdf}
		}
	\subfigure[KP]{ \label{fig:toy_ot}\includegraphics[width=0.2\columnwidth]{figure//toy_OT.pdf}
		}
	\subfigure[GW] { \includegraphics[width=0.2\columnwidth]{figure//toy_GW.pdf}
		\label{fig:toy_gw} }
	\subfigure[KPG-RL (ours)]{ \includegraphics[width=0.2\columnwidth]{figure//toy_KPG.pdf}
		\label{fig:toy_kpg}}
	\caption{(a) Positive (cross) and negative (circle) samples of source (in blue) and target (in green) distributions. (b) KP distorts the data structure, leading to mismatch of some positive and negative samples. (c) GW model better preserves the data structure, but completely mismatches the samples. (d) Our proposed KPG-RL model utilizes some keypoints (red pairs) to guide the transport and produce correct matching.}
	\label{fig:toy_example}
% 	\vspace{-0.3cm}
\end{figure}

In most of the above models, the main criterion for the optimal transport plan/matching is by the minimization of total transport distance or distortion over all samples.
Though having achieved promising results in many applications, without any additional guidance, the OT models may lead to incorrect matching of samples. Figures~\ref{fig:toy_ot} and ~\ref{fig:toy_gw} illustrate examples of incorrect matching produced by KP and GW models.
In many applications, it is reasonable to annotate some paired keypoints across domains for guiding the matching in OT. For instance, in non-rigid point set or image registration~\citep{myronenko2010point,crum2004non}, a few keypoint pairs that should be matched are annotated in two point sets/images.
In semi-supervised domain adaptation~\citep{saito2019semi} and heterogeneous domain adaptation~\citep{tsai2016learning}, along with a large amount of labeled source domain data, there are a few labeled target domain data available, that could be directly taken as keypoints.
Therefore, it is valuable and important to investigate how to take advantage of those keypoints to guide the correct matching in OT. Figure~\ref{fig:toy_kpg} shows an example that with the guidance of a few keypoints, the correctness of matching can be improved.

In this paper, we propose a novel KeyPoint-Guided model by ReLation preservation (KPG-RL) for leveraging the annotated keypoints to guide the matching in OT.
% To impose the guidance of keypoints, first, we
In KPG-RL, we first preserve the matching of keypoint pairs in OT using a mask-based constraint of the transport plan. We then propose to preserve the relation of each data point to the keypoints in transport, enforcing the matching of the data points near to paired keypoints across domains. The proposed KPG-RL model is applicable even when distributions lie in different spaces, and can be solved by Sinkhorn's algorithm. We further enforce the relation preservation constraint in KP and GW to impose the guidance of keypoints in them.
To handle the applications where only partial mass should be transported, we extend the KPG-RL model to the partial OT setting, forming the partial-KPG-RL model.

Meanwhile, we present the dual formulation of KPG-RL model, enabling us to learn the transport plan and further the transport map using deep learning techniques.  Specifically, we deduce the dual formulation of the $L_2$-regularized KPG-RL model, in which  we parameterize the optimization variables, \textit{say potentials}, with deep neural networks. The optimal transport plan can be calculated using the trained potentials, according to the strong duality. With given transport plan,
% \citet{seguy2017large} propose the Barycentric Projection (BP) to transport source samples to the target domain, which can be directly applied to our approach using our learned transport plan. However, BP can cause blur in transported samples because they are close to the weighted average of target samples. We analyze that the transported samples of BP are outside the manifold of target data, causing blur.
we propose a novel Manifold Barycentric Projection (MBP) approach to transport source data to the target domain, dubbed KPG-RL-MBP.  The KPG-RL-MBP transports the source samples to the barycenter of the conditional transport plan constrained into the target data manifold.

% Though Sinkhorn's algorithm is cheaper compared with linear programming, it still has $\mathcal{O}(n^2)$ complexity in each of its iterations if both source and target sample sizes are $n$. So Sinkhorn's algorithm does not scale well to large $n$.
% To apply the KPG-RL model to distributions supported on a large number of samples, we deduce the dual formulation of the KPG-RL model in which we parameterize the potentials with neural networks and train them using mini-batch-based stochastic gradient descent algorithms that works even when the sample sizes are large. Based on the trained potentials, we propose a GAN-based approach to transport source samples to the target domain, in which the transport map is also learned using mini-batch-based stochastic gradient descent algorithms. We term this extension as large-scale KPG-RL model.
% The mini-batch-based training/optimization enables the KPG-RL model to scale well to distributions supported on a large number of samples.

As applications, we apply the KPG-RL model to the heterogeneous domain adaptation (HDA)~\citep{tsai2016learning} and image-to-image (I2I) translation~\citep{isola2017image}. HDA is a transfer learning task that aims to transfer the knowledge of large-scale labeled source domain data to the target domain where a few labeled and larger amounts of unlabeled data are available for training. The ``heterogeneity'' implies that the source and target domain data are in heterogeneous feature spaces,
\eg, generated by different deep networks. This heterogeneity poses a major obstacle in adapting the source-trained model to the target domain. We take the labeled target domain data and source class centers as keypoints and transport source domain data to the target domain by our KPG-RL model. Upon the transported source domain data and the labeled target domain data, a classification model is trained that is transferable to the target domain. Experiments show that the proposed KPG-RL model is effective for HDA.

We evaluate our KPG-RL-MBP in I2I translation task, in which we are given the images of two distinct domains with different styles, objects, \textit{etc}. We consider the task that a large number of unpaired and a few paired images across the source and target domains are given in training. The goal is to transport the source images to the target domain using the paired images as guidance. We take the paired images as keypoints. We then apply the proposed KPG-RL-MBP to transport the source images to the target domain. Experiments on digits and natural animal images confirm that KPG-RL-MBP can realize the guidance of keypoints and produce clear transported images.

Our contributions are summarized as follows:
\begin{itemize}
    \item We propose the novel KPG-RL model to leverage the given paired keypoints to guide the correct matching in OT. A mask-based constraint on the transport plan is applied to enforce the matching of keypoint pairs. We propose to preserve the relation of each data point to the keypoints to realize the guidance of keypoints.
    \item We apply the keypoint guidance in the existing OT models of KP and GW, of which the theoretical properties are presented. We extend the KPG-RL model to the partial OT setting where only partial mass is transported.
    \item We deduce the dual formulation of the KPG-RL model, based on which a novel manifold barycentric projection approach, KPG-RL-MBP, is proposed to transport the source data to the target domain.
    \item We apply the proposed method to HDA and I2I translation. Extensive experiments verify the effectiveness of the proposed approach.
\end{itemize}

This paper extends our conference version~\citep{gu2022keypointguided} with the following additional contributions.
We present the dual formulation for the proposed KPG-RL model, which is an unconstrained optimization problem. To solve the dual problem, we parameterize the potentials using neural networks that are trained using mini-batch-based optimization algorithms. The transport plan is given by the strong duality using the learned potentials. With the learned transport plan, we propose a novel Manifold Barycentric Projection approach, KPG-RL-MBP, that transports source data to the barycenter of the conditional transport plan constrained into the target data manifold. We evaluate KPG-RL-MBP in semi-paired image-to-image translation, where a few paired and a large number of unpaired cross-domain images are available for training. We take the paired images are keypoints and utilize the proposed KPG-RL-MBP to transport source images to the target domain. Experiments on digit and natural animal images show that our approach can produce high-quality images and impose the guidance of keypoints.

% We extend the KPG-RL model to the large-scale OT situation, as in Sect.~\ref{sec:large_scale}. Specifically, we deduce the dual formulation of the KPG-RL model which can be solved via mini-batch-based training. We further propose a GAN-based approach to transport the source domain data to the target domain. We apply the above large-scale KPG-RL to the I2I translation task, as in Sect.~\ref{sec:i2i_translation}. Experiments on digits and real images show the effectiveness of the large-scale KPG-RL.

\paragraph{}
In the following sections, we discuss the related works in Sect.~\ref{sec:related_work}, and introduce the background of OT in Sect.~\ref{sec:background}. In Sect.~\ref{sec:kpg_ot}, we detail the proposed KPG-RL model. In Sect.~\ref{sec:large_scale}, we elaborate on the dual KPG-RL model with the manifold barycentric projection.
In Sect.~\ref{sec:app_hda}, we apply our approach to HDA and I2I translation. Section~\ref{sec:conclusion} concludes this paper.

\paragraph{Notations.}  $\Sigma_{m}=\{\bm{p}\in\mathbb{R}_+^{m}|\sum_{i}p_i=1\}$ is the probability simplex. $\langle \cdot,\cdot\rangle_F$ is the Frobenius dot product of two matrices. $\odot$ stands for the Hadamard product. $\mathbbm{1}_m$ denotes $m$-dimensional all-one vector. $\pi_{i,:}$ and $\pi_{:,j}$ are respectively the $i$-th row and $j$-th column of matrix $\pi$.
% Acknowledgements and Disclosure of Funding should go at the end, before appendices and references
\section{Related Works}\label{sec:related_work}

We review below the most related OT models, HDA methods, and I2I translation approaches to our work.

{\paragraph{OT models.} The GW model~\citep{memoli2011gromov} seeks a ``distance-preserving'' transport plan such that the distance between transported points in the target domain is the same as the distance between the original points in source domain. Our KPG-RL model aims to use keypoint pairs to guide the matching (\ie, transport plan) in OT by preserving the relation of each point to the keypoints. Our ``relation-preserving'' scheme preserves the relation of data \textit{w.r.t.} the given keypoints, different from the pairwise distance-preserving constraint in GW. We experimentally verified the effectiveness of relation-preserving scheme for introducing the guidance of keypoints in OT.
% Meanwhile, our KPG-RL model can be combined with GW to impose the guidance of keypoints in GW.
From the computational point of view, GW is a non-convex quadratic program, while KPG-RL is a linear program.
% Sato~\etal~\citep{sato2020fast} propose the Anchor Wasserstein distance that is an approximation of GW and can be efficiently computed.
\citet{lin2021making} use the anchors to encourage clustering of data and to impose rank constraints on the transport plan to improve its robustness to outliers. The ``anchors'' in~\citep{lin2021making} are intermediate points in computation for improving robustness, different from the ``keypoints'' in this paper, which are the annotated paired data for guiding the matching in OT.
% Graph OT~\citep{chen2020graph} and fused GW~\citep{titouan2019optimal} fuse the GW and KP models for structure and node matching between graphs.
Hierarchical OT~\citep{yurochkin2019hierarchical,lee2019hierarchical,xu2020learning} transports points by dividing them into some subgroups and then derives the transport of these subgroups using OT. Different in goal and methodology from Hierarchical OT, we impose the guidance of keypoints for pursuing correct matching in OT by preserving the relation to the keypoints. We do not explicitly divide the points into subgroups, and there is no hierarchy in our method. TLB~\citep{memoli2011gromov,sato2020fast} is a lower bound of GW that can be computed faster. TLB takes the ordered distance of each point to all the points in the same domain as features, and then performs the Kantorovich formulation of OT using such features.
Differently, our method uses a carefully designed relation of each point to the keypoints to impose the guidance of keypoints to the other points.
% As will be shown in experiments, our method is more effective than TLB when some keypoints are given.
\citet{7586038} constrain the cost function to encourage the matching of labeled data across source and target domains that share the same class labels for domain adaptation. They use the Laplacian regularization to preserve the data structure. Differently, we explicitly model the guidance of keypoints matching to the other data points in our OT formulation. The matching of paired keypoints is enforced by our mask-based constraint on the transport plan. \citet{zhang2022fine} propose Masked OT model as a regularization term to preserve the local feature invariances between fine-tuned and pretrained graph neural network (GNN) for the fine-tuning of GNNs. Though the Masked OT~\citep{zhang2022fine} shares similar spirits to the mask-based modeling in our method, our main contribution is the relation preservation for imposing the guidance of keypoints, different from~\citep{zhang2022fine}. For our mask-based modeling, it is utilized to impose the matching of keypoints with theoretical guarantee. While the mask in~\citep{zhang2022fine} aims to preserve the local information of finetuned from pretrained models. The motivation and design of our mask are different from those in~\citep{zhang2022fine}.
}
% Our KPG-RL is orthogonal to the graph, fused, and hierarchical OT, and can be potentially combined with them to impose the guidance of keypoints as constraint in these OT models.

% \textbf{Large-scale OT methods.}

\paragraph{HDA methods.} HDA methods could be roughly categorized into cross-domain mapping and common subspace learning methods. The cross-domain mapping approaches~\citep{tsai2016learning,yan2018semi,shen2018unsupervised,mozafari2016svm,zhou2019multi} learn a transform to map the source features or model parameters to target domain to achieve adaptation. The common subspace learning approaches~\citep{wang2011heterogeneous,wu2021heterogeneous,yan2017learning,zhou2019deep,wang2022cross,fang2022semi} learn domain-specific projections to map source and target domain data into a common subspace such that their distributions are aligned. Our method for HDA belongs to the first category, and could be mostly related to~\citep{yan2018semi}. The method in~\citep{yan2018semi} transports source samples to target domain using GW model regularized by the distance between the center of transported source samples and the center of labeled target samples having the same class labels. Different from~\citep{yan2018semi}, we take each labeled target data and its corresponding source class center as a keypoint pair and preserve the relation of each data point to the set of keypoints in each domain when conducting OT.

\paragraph{Image-to-image translation.}  In the earlier supervised I2I translation works~\citep{isola2017image,wang2018discriminative,zhu2017toward,zhang2020cross,bansal2017pixelnn}, researchers use many aligned cross-domain image pairs to obtain the translation model that translates the source images to the desired target images. However, training supervised translation is not very practical because of the difficulty and high cost of acquiring these large, paired training data in many tasks. Unsupervised I2I translation methods~\citep{zhu2017unpaired,kim2017learning,yi2017dualgan,choi2021ilvr,meng2021sdedit} use two large but unpaired sets of training images to convert images between representations. Though promising, the unsupervised methods require additional knowledge, \eg, domain-common knowledge~\citep{choi2021ilvr}, to guide the desired  translation. Semi-supervised I2I translation approach~\citep{mustafa2020transformation} leverages source images alongside a few source-target aligned image pairs for training, reducing the cost of human labeling or expert guidance in supervised I2I. In this paper, we tackle the semi-paired I2I translation task that many unpaired source and target images, and a few source-target aligned image pairs are available for training, because obtaining unlabeled target images could be reasonable in many applications.  We take the given a few source-target aligned image pairs as keypoints and utilize our proposed KPG-RL-MBP method to translate the source images to the target domain.

\section{Background on Optimal Transport}\label{sec:background}
\paragraph{Kantorovich Problem (KP).} We consider two sets of data points, \ie, the source data $\bm{X} = \{{x}_{i} \}_{i=1}^{m}$ and the target data $\bm{Y} = \{{y}_{j} \}_{j=1}^{n}$, of which the empirical distributions are $\bm{p}=\sum_{i=1}^{m}p_{i}\delta_{x_{i}}$ and $\bm{q}=\sum_{j=1}^{n}q_{j}\delta_{y_{j}}$. With a slight abuse of notations, we may also denote $\bm{p}=(p_1,p_2,\cdots,p_m)^{\top} \in\Sigma_{m}$ and $\bm{q}=(q_1,q_2,\cdots,q_n)^{\top}\in\Sigma_{n}$ as the mass supported on $\bm{X}$ and $\bm{Y}$, respectively. We define the  cost matrix between $\bm{X}$ and $\bm{Y}$ as $C=(C_{i,j}) \in \mathbb{R}^{m\times n}$ with $C_{i,j} = c(x_{i}, y_{j})$, where $c$ is a cost function, which is set to the squared $L_2$-distance of $x_{i}$ and $y_j$ in our experiments.
OT aims to optimally transport $\bm{p}$ towards $\bm{q}$ at the smallest cost, formulated as the following Kantorovich Problem (KP):
\begin{equation}\label{eq:ot}
\min_{\pi \in \Pi(\bm{p},\bm{q})}L_{kp}(\pi)\triangleq\langle \pi , C \rangle_F,
% = \sum_{i=1}^{m}\sum_{j=1}^{n} \pi_{i,j} C_{i,j},
\mbox{ s.t. } \Pi(\bm{p},\bm{q}) = \{ \pi \in \mathbb{R}^{m\times n}_+ \vert \pi \mathbbm{1}_{n} = \bm{p}, \pi^{\top} \mathbbm{1}_{m} = \bm{q}\}.
\end{equation}
When $c$ is taken as a distance metric (\textit{aka.} ground metric), the minimum value of objective function in Eq.~\eqref{eq:ot} is a distance between $\bm{p}$ and $\bm{q}$, named Wasserstein distance.

\paragraph{Partial OT model.} The KP in Eq.~\eqref{eq:ot} takes the mass preserving assumption that all the mass of $\bm{p}$ is transported to exactly match the mass of $\bm{q}$.
In many applications, only partial mass should be transported.
The partial OT model~\citep{figalli2010optimal,caffarelli2010free}  seeks the minimal cost of transporting only $s$ unit mass from $\bm{p}$ to $\bm{q}$, where $0 \leqslant s \leqslant \min(\Vert \bm{p}\Vert_1, \Vert \bm{q}\Vert_1)$, formulated as
\begin{equation}\label{eq:pot}
\min_{\pi \in \Pi^{s}(\bm{p},\bm{q})}L_{kp}(\pi), \mbox{ s.t. } \Pi^{s}(\bm{p},\bm{q}) = \{ \pi \in \mathbb{R}^{m\times n}_+ \vert \pi \mathbbm{1}_{n} \leqslant \bm{p}, \pi^{\top} \mathbbm{1}_{m} \leqslant \bm{q}, \mathbbm{1}_{m}^{\top}\pi \mathbbm{1}_{n} = s \}.
\end{equation}
Note that in partial OT, the total mass $\Vert \bm{p}\Vert_{1}$ and $\Vert \bm{q}\Vert_{1}$ are not necessarily equal.
% In the definition of $\Pi(\bm{p},\bm{q})$ and $\Pi^{s}(\bm{p},\bm{q})$, $m$ and $n$ are respectively the length of $\bm{p}$ and $\bm{q}$.

\paragraph{Gromov-Wasserstein (GW) model.} If the data points $x_i$ and $y_j$ lie in different spaces, the distance between  $x_i$ and $y_j$ may not be computed. The GW~\citep{memoli2011gromov} model then minimizes the distortion when transporting the whole set of points from one space to another. The GW model relies on the intra-domain distance of source domain as $C^s=(C^s_{i,k})\in \mathbb{R}^{m\times m}$ and target domain as $C^t=(C^t_{j,l})\in \mathbb{R}^{n\times n}$, where $C^s_{i,k}$ is the distance between source domain data $x_i$ and $x_k$, and $C^t_{j,l}$ is the distance between target domain data $y_j$ and $y_l$. The GW model is given by
\begin{equation}
    \min_{\pi \in \Pi(\bm{p},\bm{q})} L_{gw}(\pi) \triangleq \sum_{i,k=1}^m\sum_{j,l=1}^n \pi_{i,j}\pi_{k,l}\vert C^s_{i,k}-C^t_{j,l}\vert^2.
\end{equation}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.98\columnwidth]{figure//kpp.pdf}
	\caption{Example of modeling the matching of keypoints (red) using mask. The left part illustrates the matching of data points with keypoint pairs $\mathcal{K}=\{(3,2),(6,5)\}$. To preserve the matching of keypoint index pair $(i,j)$, \eg, $(3,2)$, the $i$-th row and $j$-th column of transport plan $\Tilde{\pi}$ must be zeros except $\Tilde{\pi}_{i,j}$. Therefore, we can model $\Tilde{\pi}$ by $\Tilde{\pi} = M \odot \pi$, where $M$ is the mask determined by the keypoints, and $\pi$ is to be optimized.}
	\label{fig:mask_modeling_main}
\end{figure}

\section{Keypoint-Guided Optimal Transport}\label{sec:kpg_ot}
This section details our proposed keypoint-guided model that leverages the keypoints to guide the matching in OT. The guidance is imposed by preserving the matching of keypoint pairs and the relation of each data point to the keypoints. We next discuss the preservation of matching of keypoints, introduce the modeling of relation, and present the keypoint-guided OT models. All the proofs of theorems or propositions are given in Appendix.

\subsection{Preservation of Matching of Keypoints in Transport}\label{sec:mask_modeling}
We denote the set of keypoint index pairs as $\mathcal{K}=\{(i_u,j_u)\}_{u=1}^U$ with $U$ denoting the number of paired keypoints.  We respectively denote $\mathcal{I} = \{i_u\}_{u=1}^U$ and $\mathcal{J} = \{j_u\}_{u=1}^U$ as the sets of source and target keypoint indexes. For the example illustrated in Fig.~\ref{fig:mask_modeling_main}, $\mathcal{K} = \{(3,2),(6,5)\}, \mathcal{I} = \{3,6\}$, and $\mathcal{J} = \{2,5\}$.
To impose the guidance of these keypoints with indexes in $\mathcal{K}$ in deriving the transport plan in OT, we first guarantee the exact matching of the keypoint pairs. As illustrated in Fig.~\ref{fig:mask_modeling_main}, we preserve the matching of keypoints in transport using a mask-based constraint of the transport plan, which is motivated by the following observation. If the paired keypoints  $(i,j)\in\mathcal{K}$ are matched, the optimal transport plan $\Tilde{\pi}$ satisfies that the $i$-th row and $j$-th column of
$\Tilde{\pi}$ must be zeros except $\Tilde{\pi}_{i,j}$, which means that the all mass of source keypoint $x_i$ must be transported to target keypoint $y_j$ and $y_j$ can only receive the mass from $x_i$. For the example in Fig.~\ref{fig:mask_modeling_main}, the 3-th row and 2-th column are zeros except that $\Tilde{\pi}_{3,2}>0$. This sparsity of $\Tilde{\pi}$ motivates us to model it as the Hadamard product of a mask matrix $M=(M_{i,j})\in\mathbb{R}^{m\times n}$ and a matrix $\pi \in \mathbb{R}_+^{m\times n}$ with positive entries, \ie,
\begin{equation}\label{eq:mask_decom}
  \Tilde{\pi} = M \odot \pi, \mbox{ with } \Tilde{\pi}_{i,j} = M_{i,j}\pi_{i,j}.
\end{equation}
With Eq.~\eqref{eq:mask_decom}, we define the admissible solution set for our keypoint-guided OT model as
\begin{equation}\label{eq:mask_pi}
    \Pi(\bm{p},\bm{q};M) = \{ \pi \in \mathbb{R}^{m\times n}_+ \vert (M\odot\pi) \mathbbm{1}_{n} = \bm{p}, (M\odot\pi)^{\top} \mathbbm{1}_{m} = \bm{q}\}.
\end{equation}
Note that the $\pi$ in Eq.~\eqref{eq:mask_pi} is not necessarily a coupling.
The entry $M_{i,j}$ of the mask matrix $M$ is set to 0 if $\Tilde{\pi}_{i,j}$ needs to be 0, otherwise  $M_{i,j}$ is set to 1. Figure~\ref{fig:mask_modeling_main} illustrates the mask-based modeling of Eq.~\eqref{eq:mask_decom}. $M$ is constructed as in Proposition~\ref{thm:thm_mask_main}.

\begin{proposition}\label{thm:thm_mask_main}
Suppose that the mask matrix $M$ satisfies that
\begin{equation}
    M_{i,j}=\begin{cases} 1, & \mbox{ if } (i,j)\in\mathcal{K},\\
    0, & \mbox{ if }  i \in \mathcal{I}$ and $(i,j)\notin \mathcal{K},\\
    0, & \mbox{ if }  j \in \mathcal{J}$ and $(i,j)\notin \mathcal{K},\\
    1, &  \mbox{ otherwise (\ie, $i \notin \mathcal{I}$ and $j \notin \mathcal{J}$)}.
    \end{cases}
\end{equation}
and $p_i = q_j$, for $(i,j)\in\mathcal{K}$. Then, the transport plan $\Tilde{\pi}=M\odot\pi$ with $\pi\in\Pi(\bm{p},\bm{q};M)$ preserves the matching of paired keypoints with index pairs in $\mathcal{K}$.
\end{proposition}
% More explanations of the construction of $M$ and the proof of Proposition~\ref{thm:thm_mask_main} are provided in Appendix~\ref{app:prop1}.
Proposition~\ref{thm:thm_mask_main} indicates that if $p_i = q_j$, for $(i,j)\in\mathcal{K}$, the matching of keypoint pairs is preserved by the mask-based constraint. In this paper, for the convenience of description, we consider the case that $p_i = q_j$, $\forall (i,j)\in\mathcal{K}$.
Note that this mask-based modeling in Eq.~\eqref{eq:mask_decom} is also applicable even for the case that there exist some $(i,j)\in\mathcal{K}$ such that $p_i \neq q_j$. For this case, we shall use different mask matrices. Please refer to the conference version~\citep{gu2022keypointguided} of this paper for the details.


\subsection{Modeling the Relation to Keypoints}\label{sec:relation}
To use the keypoints to guide the matching in transport,
% our intuition is that the points of each domain near a paired keypoints should be matched in transport.
we propose to preserve the relation of each point to the set of keypoints in transport. Figure~\ref{fig:relation} illustrates the relation within points of each distribution. For the data point $x_k\in \bm{X}$,
\begin{figure}[t]
	\centering
	\includegraphics[width=0.5\columnwidth]{figure//kpg.pdf}
	\caption{ Illustration of relation of each point to the keypoints (red). In the keypoint-guided OT model, the relation should be preserved after transport.
    }
    \label{fig:relation}
    % \vspace{-1.1cm}
\end{figure}
its relation score
 to the keypoint $x_{i_u}$ for $i_u\in \mathcal{I}$ (illustrated by red circle in the left of  Fig.~\ref{fig:relation}) is defined as
\begin{equation}\label{eq:r}
    R^s_{k,i_u} = \frac{e^{-C^s_{k,i_u}/\tau}}{\sum_{u'=1}^{U}e^{-C^s_{k,i_u}/\tau}}, \mbox{ } \forall i_u \in \mathcal{I},
\end{equation}
% where $C^s_{k,i_u}$ is the squared $L_2$-distance between $x_k$ and $x_{i_u}$, and
$\tau$ is temperature set as $\tau=\rho*\max_{i,k}\{C^s_{i,k}\}$ in experiments. Similarly, for $y_l\in\bm{Y}$, its relation score to the keypoint $y_{j_u}$ for $j_u\in\mathcal{J}$ is defined as
\begin{equation}\label{eq:r_bar}
    R^t_{l,j_u} = \frac{e^{-C^t_{l,j_u}/\tau'}}{\sum_{u'=1}^{U}e^{-C^t_{l,j_u}/\tau'}}, \mbox{ }\forall  j_u \in \mathcal{J},
\end{equation}
% where $C^t_{l,j_u}$ is the squared $L_2$-distance between $y_l$ and $y_{j_u}$,
$\tau'$ is temperature set to $\rho*\max_{j,l}\{C^t_{j,l}\}$.
{The setting of $\tau$ and $\tau'$ implies that the distances are divided by their maximum value, and normalized to $[0,1]$, which increases the robustness of the relation score to the scale of the distances. $\rho$ is a tunable parameter and set to 0.1 in our experiments, since 0.1 is a commonly used temperature in the softmax function~\citep{chen2020simple,khosla2020supervised}.
We denote $R^s_k = (R^s_{k,i_1},R^s_{k,i_2},\cdots, R^s_{k,i_U})$ and $R^t_l = (R^t_{l,j_1}, R^t_{l,j_2},\cdots, R^t_{l,j_U})$ that represent the relation of data points $x_k$ and $y_l$ to the keypoints in source and target domains, respectively. From the definition of the relation, the cross-domain points near to paired keypoints share similar relation to the keypoints in corresponding domain. For instance, in Fig.~\ref{fig:relation}, $x_k$ and $y_l$ near to the paired keypoints with indexes $(i_2, j_2)$ have similar relation $R^s_k$ and $R^t_l$. Meanwhile, if $x_k$ is distant from all the keypoints, $R^s_k$ is close to a uniform probability vector.
}
% If $x_k$ is closest to  $x_{i_u}$ among the keypoints, $R^s_{k,i_u}$ is larger than the other components of $R^s_k$. If $x_k$ is distant from all the keypoints, $R^s_{k,i_u}$ is close to a uniform probability vector.

\subsection{Realizing Keypoint Guidance by Relation Preservation}\label{sec:kpg_rl}
{Based on the relation given above, we define the guiding matrix
\begin{equation}\label{eq:guiding_matrix}
    G = (G_{k,l})\in\mathbb{R}^{m\times n} \mbox{ by } G_{k,l}=d(R^s_k,R^t_l),
\end{equation} where $d$ measures the dissimilarity of $R^s_k$ and $R^t_l$. Then, $G_{k,l}$ is small and even near to 0 if  $R^s_k$ and $R^t_l$ are similar.
$d$ is taken as the Jensenâ€“Shannon divergence in this paper. We will study the effect of $d$ in experiments.
}
% , because $R^s_k$ and $R^t_l$ are probability vectors.
By using the mask-based constraint of transport plan, the \textit{KeyPoint-Guided model by ReLation preservation} (\textbf{KPG-RL}) is defined as
\begin{equation}\label{eq:kpg}
    \min_{\pi \in \Pi(\bm{p},\bm{q};M)} L_{kpg}(\pi) \triangleq  \langle M\odot\pi , G \rangle_F.
\end{equation}
By the KPG-RL model in Eq.~\eqref{eq:kpg}, first, the  matching of keypoint pairs is enforced by the mask-based constraint of the transport plan.
{Second, the minimization of the objective function enforces that
% that the relation of each point to the keypoints is preserved after transport.
% By the relation preservation model, i.e., KPG-RL model in Eq. (9),
the optimal transport plan  has larger entries in the locations where the entries of $G$ are smaller. Hence the cross-domain points corresponding to these locations (\eg, $k$ and $l$ shown in Fig.~\ref{fig:relation}) that are near to paired keypoints tend to be matched. Based on the softmax-based formulations in Eqs.~\eqref{eq:r} and~\eqref{eq:r_bar}, $d(R_k^s,R^t_l)$ is mainly determined by the relation score to the closest keypoint(s), since relation scores to the distant keypoints are small or close to 0. This implies that the points are mainly guided by the closest keypoints in our KPG-RL model in Eq.~\eqref{eq:kpg}.
For the points distant from all the keypoints, their corresponding entries of $G$ are similar (close to zero) according to the definition of $G$. Therefore, the guidance of keypoints to these points is limited. To achieve correct matching of these points, additional information, \eg, point-wise cost or more keypoints, is needed.
}

Equation~\eqref{eq:kpg} is a linear program and can be solved by linear programming algorithms, \eg, the Simplex algorithm.
% We give the details for reformulating Eq.~\eqref{eq:kpg} as the standard form of linear programming in Appendix~\ref{app:lp}.
Since Sinkhorn's algorithm offers a lightspeed computation of the entropy-regularized OT~\citep{cuturi2013sinkhorn}, a natural question is that, can Sinkhorn's algorithm be applied to the KPG-RL model with entropy regularization? We give the positive answer, and the details of the deduction are given in Appendix. The iterative formulas are
\begin{equation}\label{eq:sinkhorn_main}
    \bm{u}^{(l+1)} = \frac{\bm{p}}{K\bm{v}^l}, \hspace{0.2cm}\bm{v}^{(l+1)} = \frac{\bm{q}}{K^{\top}\bm{u}^{(l+1)}},
\end{equation}
where $K = M\odot e^{-G/\epsilon}$, $\epsilon$ is the coefficient of entropy regularization. The division operator used above is entry-wise. After iteration, the optimal transport plan is $\mbox{diag}(\bm{u})K\mbox{diag}(\bm{v})$.

Note that in this KPG-RL model in Eq.~\eqref{eq:kpg}, the points in $\bm{X}$ and $\bm{Y}$ do not necessarily lie in the same space because we only need to compute the distance within each one of $\bm{X}$ and $\bm{Y}$. Therefore, the proposed KPG-RL model in Eq.~\eqref{eq:kpg} is applicable even when $\bm{p}$ and $\bm{q}$ are supported in different spaces. As mentioned in Sect.~\ref{sec:introduction}, the GW model is applicable for transport across different spaces. However, GW is a non-convex quadratic program and is often solved by the Frank-Walfe algorithm, in which a KP-like problem needs to be solved at each iteration by Sinkhorn's algorithm or linear programming. Surprisingly, our KPG-RL model can be directly solved using Sinkhorn's algorithm or linear programming without additional iteration, as discussed above.

\subsection{Imposing Keypoint Guidance in KP/GW}\label{sec:kpg_kp_gw}
We can impose the keypoint guidance in KP by adding $L_{kpg}(\pi)$ as a regularization term to KP, obtaining the following \textbf{KPG-RL-KP} model:
\begin{equation}\label{eq:KPG-RL-KP }
    \min_{\pi \in \Pi(\bm{p},\bm{q};M)} \left\{\alpha L_{kp}(M\odot\pi)+(1-\alpha)L_{kpg}(\pi) =  \langle M\odot\pi , \alpha C + (1-\alpha) G \rangle_F\right\}, \alpha \in (0,1).
\end{equation}
The KPG-RL-KP can be solved by Sinkhorn's algorithm or linear programming, the same as the solution of KPG-RL model. Similarly, we define the \textbf{KPG-RL-GW} model in GW  by
\begin{equation}\label{eq:KPG_RL_GW}
  \min_{\pi \in \Pi(\bm{p},\bm{q};M)} \alpha L_{gw}(M\odot\pi)+(1-\alpha)L_{kpg}(\pi), \alpha \in (0,1).
\end{equation}
The KPG-RL-GW  model is solved using the Frank-Walfe algorithm.
% (please refer to \citep{gu2022keypointguided} for the details).
\footnote{In the $l$-th iteration, we first calculate the gradient $g=\nabla_{\pi^l}\left(\alpha L_{gw}(M\odot\pi)+(1-\alpha)L_{kpg}(\pi)\right)$ for current solution $\pi^{l}$. We then project the gradient by $\pi'=\min_{\pi \in \Pi(\bm{p},\bm{q};M)}\langle g,\pi\rangle_F$, and finally compute $\pi^{l+1}= \omega\pi^{l}+(1-\omega)\pi'$, where $\omega$ is obtained by line search in $[0,1]$. Details are in \cite{gu2022keypointguided}.}.
The KPG-RL-KP/KPG-RL-GW  models take both advantages of KP/GW and KPG-RL models, and could be helpful especially when the number of paired keypoints is small. We show in Sect.~\ref{sec:theoretical_pp} that the KPG-RL-KP and KPG-RL-GW respectively provide a proper metric and a divergence under mild conditions. $\alpha$ is simply set to 0.5.

\subsection{Extension to Partial OT}\label{sec:ex}
We now extend the above KPG-RL model to the more practical partial OT setting that only partial mass is transported. To do this, we first apply the above mask-based constraint of transport plan to the partial OT model in Eq.~\eqref{eq:pot}.
% However, this strategy may partially transport the mass of keypoints and thus does not preserve the matching of keypoints.
We then add more constraints to enforce that all the mass of keypoints is transported to preserve the matching of keypoints. Finally, we define the \textbf{partial-KPG-RL} model as
\begin{equation}\label{eq:partial_kpp_main}
% \begin{split}
% &\min_{\pi \in \Pi^{s}(\bm{p},\bm{q};M)}L_{kp}(M\odot\pi)\\
% \mbox{s.t. }\Pi^{s}(\bm{p},\bm{q};M) = \{ &\pi \in \mathbb{R}^{m\times n}_+ \vert (M\odot\pi) \mathbbm{1}_{n} \leqslant \bm{p}, (M\odot\pi)^{\top} \mathbbm{1}_{m} \leqslant \bm{q}, \mathbbm{1}_{m}^{\top}(M\odot\pi) \mathbbm{1}_{n} = s;\\&
% (M\odot\pi)_{i,:}\mathbbm{1}_n = p_i, \forall i \in \mathcal{I}; \mathbbm{1}^{\top}_m(M\odot\pi)_{:,j} = q_j, \forall j \in \mathcal{J}\}
% \end{split}
\min_{\pi \in \Pi^{s}(\bm{p},\bm{q};M)}\left\{L_{kpg}(M\odot\pi) = \langle M\odot\pi , G \rangle_F\right\},
% \langle M\odot\pi , C \rangle_F,
\end{equation}
where $\Pi^{s}(\bm{p},\bm{q};M) = \{ \pi \in \mathbb{R}^{m\times n}_+ \vert (M\odot\pi) \mathbbm{1}_{n} \leqslant \bm{p}, (M\odot\pi)^{\top} \mathbbm{1}_{m} \leqslant \bm{q}, \mathbbm{1}_{m}^{\top}(M\odot\pi) \mathbbm{1}_{n} = s; (M\odot\pi)_{i,:}\mathbbm{1}_n = p_i, \forall i \in \mathcal{I}; \mathbbm{1}^{\top}_m(M\odot\pi)_{:,j} = q_j, \forall j \in \mathcal{J}\}.$

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\columnwidth]{figure//kppa2.pdf}
	\caption{ Illustration of dummy points (black circles) for source and target domains. }
    \label{fig:partial_pkg}
\end{figure}
\citet{chapel2020partial} propose a compelling method to solve the original partial OT problem in Eq.~\eqref{eq:pot} by transforming the partial OT problem into an OT-like problem.
Inspired by ~\cite{chapel2020partial}, we add a dummy point with mass $\Vert\bm{q}\Vert_1-s$ for source domain (the left black circle in Fig.~\ref{fig:partial_pkg}) and a dummy point with mass $\Vert\bm{p}\Vert_1-s$ for target domain (the right black circle in Fig.~\ref{fig:partial_pkg}). We denote $\bar{\bm{p}} = (\bm{p}^{\top},\Vert\bm{q}\Vert_1-s)^{\top}$ and $\bar{\bm{q}} = (\bm{q}^{\top},\Vert\bm{p}\Vert_1-s)^{\top}$.
As illustrated in Fig.~\ref{fig:partial_pkg}, we aim to design the extended guiding matrix $\bar{G}$ and extended mask matrix $\bar{M}$ such that performing KPG-RL between $\bar{\bm{p}}$ and $\bar{\bm{q}}$ will transport $\Vert\bm{p}\Vert_1-s$ mass from source real data points to the target dummy point and transport $\Vert\bm{q}\Vert_1-s$ mass from source dummy point to target real data points. As a sequence, only $s$ mass of source and target real data points are matched.
Meanwhile, the keypoints should not be matched to the dummy points because they are annotated data to guide the matching.
% Inspired by \citet{chapel2020partial}, to solve our partial-KPG-RL model in Eq.~\eqref{eq:partial_kpp_main}, we add a dummy point to each of the marginal distributions to transform Eq.~\eqref{eq:partial_kpp_main} into a KPG-RL-like problem.
To do this, we extend $G, M$ by
\begin{equation*}
% \bar{\bm{p}} = \begin{bmatrix}
% \bm{p} \\
%  \Vert\bm{q}\Vert_{1} - s
% \end{bmatrix},
% % (\bm{p}, \Vert \bm{q}\Vert_{1} - s),
% \bar{\bm{q}} = \begin{bmatrix}
% \bm{q} \\ \Vert \bm{p}\Vert_{1} - s
% \end{bmatrix},
\bar{G} =
\begin{bmatrix}
G & \xi \mathbbm{1}_{n} \\
\xi \mathbbm{1}_{m}^{\top} & 2\xi + A
\end{bmatrix},
\bar{M} = \begin{bmatrix}
M & \bm{a} \\
\bm{b}^{\top} & 1
\end{bmatrix},
\end{equation*}
where
% $A\in\mathbb{R},\xi\in\mathbb{R}$
 $A>0$ and $\xi$ are two fixed scalars, and $ \bm{a}\in \mathbb{R}^m, \bm{b}\in \mathbb{R}^n$. The element $a_i$ of $\bm{a}$ is 0 if $i\in \mathcal{I}$, otherwise 1.  The element $b_j$ of $\bm{b}$ is 0 if $j\in \mathcal{J}$, and 1 otherwise.
% For more motivations of this extension, please refer to Appendix~\ref{app:motivation_partial_kpg}.
By the following theorem, solving the optimal transport plan of problem~\eqref{eq:partial_kpp_main} boils down to
solving the problem  $\mbox{min}_{\bar{\pi} \in \Pi(\bar{\bm{p}},\bar{\bm{q}};\bar{M})} \langle \bar{M}\odot\bar{\pi} , \bar{G} \rangle_F$.


\begin{theorem}\label{thm:partial_kpp_main}
Suppose $A>0$, $\sum_{i\in \mathcal{I}} p_i <s$, and $\sum_{j\in \mathcal{J}} q_j < s$, then the optimal transport plan $M\odot\pi^*$ of problem~\eqref{eq:partial_kpp_main} is the $m$-by-$n$ block in the upper left corner of  the optimal transport plan $\bar{M}\odot\bar{\pi}^*$ of problem $\mbox{min}_{\bar{\pi} \in \Pi(\bar{\bm{p}},\bar{\bm{q}};\bar{M})} \langle \bar{M}\odot\bar{\pi} , \bar{G} \rangle_F$.
\end{theorem}

% Please refer to Appendix~\ref{app:proof_thm1} for the proof of Theorem~\ref{thm:partial_kpp_main}.

\subsection{Theoretical Properties}\label{sec:theoretical_pp}
In this section, we show that given prior ``correct'' paired keypoints, the KPG-RL-KP model provides a proper metric for distributions supported in the same space, and the the KPG-RL-GW model provides a divergence (in the sense of isomorphism) for distributions in distinct spaces, under mild conditions. Since the discrete distributions $\bm{p}=\frac{1}{m}\sum_{i}^m\delta_{x_i}$ (resp. $\bm{q}=\frac{1}{n}\sum_{j}^n\delta_{y_j}$) are invariant to the permutation of $\{x_i\}_{i=1}^m$ (resp. $\{y_j\}_{j=1}^n$), we assume that any two paired keypoints across domains share the same index. Therefore, the index set of paired keypoints is $\mathcal{K}=\{(i_u,i_u)\}_{u=1}^U$.
% We assume $p_{i_u}=q_{i_u}, \forall i_u,$ in this section. For the convenience of description, in this section,
We denote
% $M^{\bm{p}\bm{q}}$ as the mask matrix for transporting $\bm{p}$ to $\bm{q}$, and
$\mathcal{P}_{\mathcal{I}}^{\mathcal{X}}$ as the set of discrete probability distributions supported on $m$ points in ground space $\mathcal{X}$ such that all distributions in $\mathcal{P}_{\mathcal{I}}^{\mathcal{X}}$ share the keypoint index set $\mathcal{I} = \{i_u\}_{u=1}^U$.

\paragraph{KPG-RL-KP providing a proper metric.}
For distributions supported in the same space, the ``correct'' paired keypoints indicates that if $\bm{p}=\bm{q}$, each source keypoint is equal to its paired target keypoint, \ie, $x_{i_u}=y_{i_u}$, for any $i_u\in\mathcal{I}$. We denote
\begin{equation}\label{eq:s_krk}
    \mathcal{S}_{krk}(\bm{p},\bm{q}) = \min_{\pi \in \Pi(\bm{p},\bm{q};M)}  \sum_{i,j}{M_{i,j}\pi_{i,j}(\alpha C_{i,j}+(1-\alpha)G_{i,j})}, % \mbox{ where }\alpha\in(0,1).
\end{equation}
where $\alpha\in(0,1)$.
\begin{theorem}\label{thm:kpg_rl_kp}
Suppose $c$ is a proper distance in space $\mathcal{X}$ and $d$ is a proper distance in probability simplex $\Sigma_U$. Then, for any  $\bm{p}$ and $\bm{q}$ in $\mathcal{P}_{\mathcal{I}}^{\mathcal{X}}$, given the ``correct'' paired keypoints stated above, $\mathcal{S}_{krk}(\bm{p},\bm{q})$ is a proper distance between $\bm{p}$ and $\bm{q}$.
\end{theorem}

\paragraph{KPG-RL-GW providing a divergence.}
For any distribution $\bm{p}\in\mathcal{P}^{\mathcal{X}}_{\mathcal{I}}$ and $\bm{q}\in\mathcal{P}^{\mathcal{Y}}_{\mathcal{I}}$, $\bm{p}$ and $\bm{q}$ are said to be isomorphic if there exists a bijection $\sigma:[m]\longmapsto[m]$ such that $c(x_i,x_k)=c'(y_{\sigma(i)},y_{\sigma(k)})$, and $p_i=q_{\sigma(i)}$, where $[m]=\{1,2,\cdots,m\}$, and $c$ and $c'$ are respectively proper distances in spaces $\mathcal{X}$ and $\mathcal{Y}$. The keypoints are ``correct'' means that if $\bm{p}=\bm{q}$, $\sigma$ maps each source keypoint to its paired target keypoint, \ie, $\sigma(i_u)=i_u$, for any $i_u\in\mathcal{I}$. We denote
\begin{equation}
\begin{split}
    \mathcal{S}_{krg}(\bm{p},\bm{q})
    = \min_{\pi \in \Pi(\bm{p},\bm{q};M)}  \sum_{i,j} &\Big[\alpha\Big(\sum_{k,l}(M\odot\pi)_{i,j}(M\odot\pi)_{k,l}|C^s_{i,k}-C^t_{j,l}|^2\Big)\\
    &+(1-\alpha)(M\odot\pi)_{i,j}G_{i,j}\Big], %\mbox{ where }\alpha\in(0,1).
\end{split}
\end{equation}
where $\alpha\in(0,1)$.

\begin{theorem}\label{thm:kpg_rl_gw}
Suppose $c$ and $c'$ are proper distances in spaces $\mathcal{X}$ and $\mathcal{Y}$. Suppose $d$ is a divergence in probability simplex $\Sigma_U$. Then, for any $\bm{p}$  in $\mathcal{P}_{\mathcal{I}}^{\mathcal{X}}$ and any $\bm{q}$ in $\mathcal{P}_{\mathcal{I}}^{\mathcal{Y}}$, given the ``correct'' paired keypoints stated above, $\mathcal{S}_{krg}(\bm{p},\bm{q}) = 0$ if and only if $\bm{p}$ and $\bm{q}$ are isomorphic.
\end{theorem}

\section{Learning Transport Map for Keypoint-Guided Optimal Transport}\label{sec:large_scale}
In Sect.~\ref{sec:kpg_ot}, we have discussed the keypoint-guided optimal transport model in primal formulation (\ie, Eq.~\eqref{eq:kpg}) that is solved by linear programming or Sinkhorn iteration. In this section, we will develop a deep-learning-based approach for estimating the transport plan and map based on the dual formulation of KPG-RL.
Our approach is inspired by \citep{seguy2017large} that studies the KP.
\citet{seguy2017large} start with the dual problem of the entropy or $L_2$-regularized KP model, which is solved by the training of deep learning. Based on the learned transport plan, they propose the Barycentric Projection (BP) to transport source data to the target domain. Inspired by~\citet{seguy2017large}, we will first develop the dual formulation of the $L_2$-regularized KPG-RL model and solve it by training deep neural networks. Based on the learned transport plan, we further propose a novel Manifold Barycentric Projection (MBP) to transport source samples to the target domain, for tackling the issue of the blur of transported samples by BP.


% \citet{seguy2017large} propose a promising approach to estimate the transport plan and map based on deep learning techniques. They considered the dual problem of the entropy or $L_2$-regularized KP model, which is solved by the training of deep learning. Based on the learned transport plan, they propose the Barycentric Projection (BP) to transport source data to the target domain. Inspired by~\citet{seguy2017large}, we employ deep learning techniques to solve our keypoint-guided optimal transport model in this section.  We first develop the dual formulation of the regularized KPG-RL model and solve it through the training of deep learning. Based on the learned transport plan, we further propose a novel Manifold Barycentric Projection (MBP) to transport source samples to the target domain, for tackling the issue of the blur of transported samples by BP.

% which take $\mathcal{O}((mn)^3)$ and $\mathcal{O}(mn\log(mn))$ time cost, respectively.  The linear programming and Sinkhorn iteration do not scale well to distributions supported on a large number of samples ($m$ and $n$ are large). We next discuss how to extend our keypoint-guided optimal transport model to large-scale transport problems in this section. Inspired by~\citep{seguy2017large,daniels2021score} that extend the KP to large-scale transport problems, we first deduce the dual formulation of the regularized KPG-RL model, based on which a GAN-based approach is proposed to transport the source samples to the target domain.

\paragraph{Dual formulation of $L_2$-regularized KPG-RL.}
There are mainly two types of regularization, \ie, entropy regularization~\citep{cuturi2013sinkhorn} and $L_2$-regularization~\citep{seguy2017large,blondel2018smooth} in OT model. We adopt the $L_2$-regularization for our purpose because we find that the $L_2$-regularization makes the training more stable than the entropy regularization for the smaller regularization coefficient in experiments. The dual formulation of the entropy-regularized KPG-RL can be deduced analogously, left as our future work. The $L_2$-regularized KPG-RL model is given by
\begin{equation}\label{eq:l2_reg_kpg}
    \min_{\pi\in\Pi(\bm{p},\bm{q};M)} \langle M\odot\pi, G \rangle_F + \epsilon \chi^2(M\odot\pi\Vert \bm{p} \times \bm{q}),
\end{equation}
where $\chi^2(M\odot\pi\Vert \bm{p} \times \bm{q}) = \sum_{i=1}^{m}\sum_{j=1}^{n}\left(\frac{M_{i,j}\pi_{i,j}}{p_iq_j}\right)^2p_iq_j$. The following theorem~\ref{thm:dual} provides the dual formation of problem~\eqref{eq:l2_reg_kpg}.

\begin{theorem}\label{thm:dual}
    The $L_2$-regularized KPG-RL model in Eq.~\eqref{eq:l2_reg_kpg} takes the dual formmulation
    \begin{equation}\label{eq:dual}
        \max_{\phi,\psi} \sum_{i=1}^m \phi(x_i)p_i + \sum_{j=1}^n \psi(y_j)q_j - \frac{1}{4\epsilon}\sum_{i,j}M_{i,j}(\phi(x_i)+\psi(y_j)-G_{i,j})_+^2p_iq_j,
    \end{equation}
    where $a_+=\max\{a,0\}$.
    Let $\phi^*$, $\psi^*$ be the optimal solution of Eq.~\eqref{eq:dual}, then the optimal transport plan $M\odot\pi^*$ of Eq.~\eqref{eq:l2_reg_kpg} satisfies
    \begin{equation}\label{eq:prim_dual_solution}
        (M\odot\pi^*)_{i,j} = \frac{1}{2\epsilon}M_{i,j}(\phi^*(x_i)+\psi^*(y_j)-G_{i,j})_+p_iq_j.
    \end{equation}
\end{theorem}
Equation~\eqref{eq:dual} is an unconstrained optimization problem \textit{w.r.t.} continuous functions $\phi,\psi$. We use neural networks $\phi_{\theta},\psi_{\theta}$ to parameterize $\phi,\psi$ respectively and use mini-batch stochastic gradient descent to optimize $\theta$. Specifically, in each iteration, we sample mini-batch samples from $\bm{p},\bm{q}$, calculate the objective function in Eq.~\eqref{eq:dual} on the mini-batch samples, compute gradient \textit{w.r.t.} $\theta$ using backpropagation, and update $\theta$ by gradient descent. Such a mini-batch-based optimization process enables the keypoint-guided optimal transport model to scale to the distributions supported on a larger number ($m$ and $n$) of samples, because we only need a mini-batch of samples to calculate the objective function in each iteration. Once the optimization process is completed, we can recover the optimal transport plan by Eq.~\eqref{eq:prim_dual_solution}.

\paragraph{Manifold barycentric projection.}
With the optimal transport plan, we next discuss how to transport the source data to the target domain. As mentioned before, \citet{seguy2017large} propose the BP to transport the source data to the target domain, which can be directly applied to our approach using our learned transport plan. Then, the BP ($T_{BP}$) is given by
\begin{equation}\label{eq:barycentric_projection}
    T_{BP} = \arg\min_{T'}\sum_{i=1}^{m}\sum_{j=1}^{n}(M\odot\pi^*)_{i,j}\Vert y_j-T'(x_i)\Vert^2_2.
\end{equation}
% Note that given a source sample $x$ even outside the support set of $\bm{p}$, the barycentric projection $T_{BP}$ can be applied to transport $x$ to the target domain. \citet{seguy2017large} show that when the size $m$ and $n$ of support samples is large and the regularization coefficient $\epsilon$ is small, the barycentric projection is close to the Monge map between the underlying continuous distributions.
However, for source sample $x$, the transported sample $T_{BP}(x)$ by BP is blurry as shown in Fig.~\ref{fig:BP}. This is mainly because that $T_{BP}(x)$ is close to the weighted average of target samples $\{y_j\}_{j=1}^n$, according to Eq.~\eqref{eq:barycentric_projection}.
% the smaller $\epsilon$ makes the learning of $\phi$ and $\psi$ difficult  due to numerical issues, see Eq.~\eqref{eq:dual}. While for larger $\epsilon$, the optimal transport plan is not sparse, leading to the blur of the image (as shown in Fig.~\ref{fig:BP}) of the barycentric projection because it is close to the weighted average of target samples, see Eq.~\eqref{eq:barycentric_projection}.


\begin{figure}[t]
	\centering
	\includegraphics[width=0.5\columnwidth]{figure//blurred_BP.pdf}
	\caption{Illustration of our manifold barycentric projection for transporting source data to the target domain. The image $T_{BP}(x)$ of barycentric projection is outside the target data manifold and is blurry. The transported data $T_{MBP}(x)$ by our proposed manifold barycentric projection is constrained to the target data manifold. }
	\label{fig:idea_gan_based_transport}
\end{figure}

We analyze that the blur caused by the BP is probably because the projected data $T_{BP}(x)$ may not be in the target data manifold\footnote{Machine learning studies often take the ``manifold hypothesis'' that many high-dimensional real data sets lie in low-dimensional manifolds inside the high-dimensional space~\citep{cayton2005algorithms,fefferman2016testing}.}. To make the transported data clear, we propose the
Manifold Barycentric Projection (MBP)
that enforces the transported data to near to $T_{BP}(x)$ constrained into the target data manifold. Figure~\ref{fig:idea_gan_based_transport} illustrates our idea. The transport map is learned from the following model:
\begin{equation}\label{eq:gan_based}
T_{MBP}=\arg\min_{T'} \lambda\sum_{i=1}^{m}\sum_{j=1}^{n}(M\odot\pi^*)_{i,j}\Vert y_j-T'(x_i)\Vert^2_2 + \mathcal{L}_{M}(T'),
\end{equation}
where the first term enforces the transported data is near to that of the barycentric projection, the second term encourages the transported data to lie in the target data manifold, and $\lambda$ is a hyper-parameter for balancing the importance of the two terms and is set to 10 in experiments. For simplicity, we use the loss of WGAN-GP~\citep{gulrajani2017improved} as $\mathcal{L}_{M}$, because WGAN-GP is often used for modeling the manifold of data~\citep{pandey2021generalization}. $\mathcal{L}_{M}$ is given by
\begin{equation}\label{eq:loss_gan}
    \mathcal{L}_{M}(T')=\max_D{\sum_{i=1}^m D(T'(x_i))p_i -\sum_{j=1}^nD(y_j)q_j + \beta\mathbb{E}_{\hat{x}\sim \hat{\bm{p}}}(\Vert \nabla_{\hat{x}}D(\hat{x})\Vert_2-1)^2},
\end{equation}
where $\hat{\bm{p}}$ denotes the samples uniformly along lines
between pairs of points sampled from the transported source distribution $T'_{\#}\bm{p}$\footnote{For a random variable $x\sim \bm{p}$ and a transform $T$ on $x$, we use $T_{\#}\bm{p}$ to denote the distribution of $T(x)$.} and target distribution $\bm{q}$, and $D$, named discriminator, is a neural network that outputs a scalar. Following~\citet{gulrajani2017improved}, $\beta$ is set to 10, and the gradient of $\hat{x}$ \textit{w.r.t.} $T'$ is stopped. Equation~\eqref{eq:gan_based} is optimized by the mini-batch stochastic gradient descent/ascent to update $T'$/$D$, respectively. With the learned $T_{MBP}$, given the source domain test sample $x$ even outside the training set, we transport it to the target domain by $T_{MBP}(x)$.





% \begin{equation}\label{eq:l2_reg_kpg}
% \begin{split}
%     \min_{\pi} &\int M(x,y)\pi(x,y) G(x,y)dxdy + \epsilon \int \left(\frac{M(x,y)\pi(x,y)}{p(x)q(y)}\right)^2p(x)q(y)dxdy\\
%     s.t. &\int M(x,y)\pi(x,y) dy = p(x), \int M(x,y)\pi(x,y) dx = q(y).
% \end{split}
% \end{equation}



\section{Experiments}\label{sec:app_hda}
We evaluate our method on toy data, HDA, and I2I translation experiments. Source codes will be available at \url{https://github.com/XJTU-XGU/KPG-RL}.

\subsection{Toy Experiments}
\begin{figure}[t]
	\centering
	\subfigure[KP]{ \includegraphics[width=0.31\columnwidth]{figure//KP.pdf}
		\label{fig:toy_OT}}
	\subfigure[KPG-RL-KP  (w/ 2 keypoints)]{ \includegraphics[width=0.3\columnwidth]{figure//KPG_RL_KP_2.pdf}
		\label{fig:toy_KPG_OT_21}}
	\subfigure[KPG-RL-KP  (w/ 3 keypoints)]{ \includegraphics[width=0.3\columnwidth]{figure//KPG_RL_KP_3.pdf}
		\label{fig:toy_KPG_OT_3}}
	\subfigure[GW]{ \includegraphics[width=0.31\columnwidth]{figure//GW.pdf}
		\label{fig:toy_GW}}
	\subfigure[KPG-RL-GW  (w/ 2 keypoints)]{ \includegraphics[width=0.3\columnwidth]{figure//KPG_RL_GW_2.pdf}
		\label{fig:toy_KPG_GW_21}}
	\subfigure[KPG-RL-GW  (w/ 3 keypoints)]{ \includegraphics[width=0.3\columnwidth]{figure//KPG_RL_GW_3.pdf}
		\label{fig:toy_KPG_GW_3}}
	\caption{Matching produced by (a) KP, (b) KPG-RL-KP model given 2 keypoint pairs, (c) KPG-RL-KP  model given 3 keypoint pairs, (d) GW model, (e) KPG-RL-GW  model given 2 keypoint pairs, and (f) KPG-RL-GW  model given 3 keypoint pairs.}
	\label{fig:toy_experiment_main}
\end{figure}
\paragraph{Toy experiments for evaluating KPG-RL-KP and KPG-RL-GW.} As illustrated in Fig.~\ref{fig:toy_experiment_main}, in this toy data experiment, each of the source (blue) and target (green) distributions is a Gaussian mixture composed of three distinct Gaussian components indicated by different shapes where the same shapes indicate points of the same class. In Fig.~\ref{fig:toy_experiment_main}, we have the following observations. In Fig.~\ref{fig:toy_OT}, in the KP model, the points in each component of target distribution are mismatched\footnote{In this experiment, the number of source and target points are equal. The linear nature of KPG-RL model in Eq.~\eqref{eq:kpg} enables that for any source data point $x_i$, only one target point $y_j$ in $\bm{Y}$ takes non-zero entry in the optimal transport plan. Then $x_i$ and $y_j$ are matched and linked by the black line in Fig.~\ref{fig:toy_experiment_main}.} to points in different classes of source distributions, and only a small fraction of target points are correctly matched to source points belonging to the same class. In Fig.~\ref{fig:toy_KPG_OT_21}, given 2 keypoint pairs (from distinct classes), the KPG-RL-KP  model apparently improves the correctness of matching\footnote{A pair of cross-domain points being  correctly matched means that they share the same class label. The matching accuracy is the ratio of correctly matched data pairs.}. In Fig.~\ref{fig:toy_KPG_OT_3}, the KPG-RL-KP  model mainly matches the source points to the target points belonging to the same class, thanks to the given 3 keypoint pairs. This implies that our proposed keypoint-guided model does help to produce correct matching in OT by leveraging the given a few keypoint pairs. In Figs.~\ref{fig:toy_GW},~\ref{fig:toy_KPG_GW_21} and~\ref{fig:toy_KPG_GW_3}, the proposed KPG-RL-GW  model improves the correctness of matching of GW model by leveraging the guidance of given keypoints pairs.
% Please refer to Appendix~\ref{app:toy_partial_kpg} for the toy experiment for evaluating the partial-KPG-RL model.

\begin{figure}[t]
	\centering
	\subfigure[Partial-OT]{ \includegraphics[width=0.3\columnwidth]{figure//partial_OT.pdf}
		\label{fig:toy_partial_OT}}
	\subfigure[Partial-GW]{ \includegraphics[width=0.3\columnwidth]{figure//partial_GW.pdf}
		\label{fig:toy_partial_GW}}
	\subfigure[Partial-KPG-RL (ours)]{ \includegraphics[width=0.3\columnwidth]{figure//partial_KPG.pdf}
		\label{fig:toy_partial_kpg}}
	\caption{Matching produced by (a) partial-OT model, (b) partial-GW model, and (c) our proposed partial-KPG-RL model.}
	\label{fig:toy_experiment}
\end{figure}
\paragraph{Toy experiments for evaluating Partial-KPG-RL.}
Figure~\ref{fig:toy_experiment} illustrates the toy data experiment for evaluating the partial-KPG-RL model.
In Fig.~\ref{fig:toy_experiment}, the source (blue) and target (green) distributions are Gaussian mixtures. The source (resp. target) distribution is composed of three (resp. two) distinct Gaussian components indicated by different shapes where the same shapes indicate the points of the same class. When conducting OT, the source class data represented by ``{\footnotesize{$\bigtriangleup$}}'' should not be transported.
In Figs.~\ref{fig:toy_experiment}(a) and~\ref{fig:toy_experiment}(b), we can observe that both the partial-OT model (defined in Eq.~\eqref{eq:pot}) and the partial-GW model~\citep{chapel2020partial} wrongly transport some source points of class ``{\footnotesize{$\bigtriangleup$}}'' to target domain and lead to lower matching accuracy. With the guidance of a few keypoints (red pairs), our proposed partial-KPG-RL model does not transport the source points of class ``{\footnotesize{$\bigtriangleup$}}'' to target domain and apparently improves the matching accuracy, as in Fig.~\ref{fig:toy_experiment}(c).

\subsection{Heterogeneous Domain Adaptation}\label{sec:hda}
\subsubsection{Closed-Set Heterogeneous Domain Adaptation}
In closed-set HDA (in what follows, we refer to the closed-set HDA by HDA), we are given labeled source data $\{(x_i,t_i)\}_{i=1}^{m}$, a few labeled target data $\{y_j,\bar{t}_j\}_{j=1}^{n_l}$, and unlabeled target domain data $\{y_j\}_{j=n_l+1}^n$, where $m\gg n_l$, $n\gg n_l$, $x_i$ and $y_j$ are features, and $t_i$ and $\bar{t}_j$ are respectively the class labels of $x_i$ and $y_j$. The source and target data share the same label space (the set of classes).
The heterogeneity means that $x_i$ and $y_j$ are from different spaces/modalities.
% , \eg, $x_i$ is text and $y_j$ is image.
The goal of HDA is to train a classification model using the given data to predict the label of unlabeled target domain data, leveraging the knowledge of labeled source domain data.
The main challenge is that the domain gap between heterogeneous source and target distributions supported in distinct spaces hinders the direct employment of the source-trained model in the target domain.

% Previous HDA methods~\citep{x} mainly map the source domain data to the target space and learn the classification model on the mapped data. Please refer to Appendix B2 for the review of the related HDA methods. It is important to enforce that the mapped source domain data are near the target domain data belonging to the same class.
We tackle the problem of HDA using our proposed KPG-RL model as follows. We first transport the source domain data using our KPG-RL model to the target domain. More concretely, in KPG-RL, each labeled target domain data and the source domain class center of the same class are taken as a matched keypoint pair.
The KPG-RL model is then performed between empirical distributions of the source domain data along with class centers and the target domain data (including labeled and unlabeled data). Based on the produced optimal transport plan, we transport the source domain data using the barycentric mapping~\citep{reich2013nonparametric} to the target domain. The barycentric mapping $B_{\pi}$ associated to transport plan $\pi$ is defined by $B_{\pi}(x_{i_0})=\frac{1}{\sum_{j=1}^n\pi_{i_0,j}}\sum_{j=1}^n\pi_{i_0,j}y_j.$ Note that our MBP (presented in Sect.~\ref{sec:large_scale}) learns transport map by training deep networks, which may be suitable for the applications where we need to transport source samples outside the training set, \eg, I2I translation discussed in Sect.~\ref{sec:i2i_translation}. For simplicity, we directly adopt the barycentric mapping for the HDA experiments.
Finally, we train the classification model (taken as a kernel SVM) on the transported source domain data (using their class label before transport) and the labeled target domain data, which is applied to the target domain test data. In the kernel SVM, we use the Gaussian $k(x,y)=\exp(-\gamma \|x-y\|_2^2)$, where $\gamma$ is set to the reciprocal of the dimension of $x$.
$\epsilon$ is set to 0.005.

% More details of the barycentric mapping, the kernel SVM, \textit{etc}, are given in Appendix~\ref{app:hda}.

\begin{table}[t]
	\centering
	\caption{Accuracy on Office-31 for HDA. ``A'', ``W'', and ``D'' are respectively the domains of amazon, webcam, and dslr. ``$\cdot\rightarrow*$'' denotes a heterogeneous adaptation task where $\cdot$ and $*$ are respectively source domain using DeCAF$_6$ feature and target domain using ResNet-50 feature.
	}
        % \small
	\setlength{\tabcolsep}{2.0pt}
	\begin{tabular}{lccccccccc|>{\columncolor{mygray}}c}
		\toprule
		Method & A$\rightarrow$A & A$\rightarrow$D & A$\rightarrow$W  & D$\rightarrow$A & D$\rightarrow$D & D$\rightarrow$W & W$\rightarrow$A & W$\rightarrow$D & W$\rightarrow$W &  \bf Avg\\
        \midrule
        % \multicolumn{11}{c}{DeCAF$_6\rightarrow$ResNet-50}\\
        % \midrule
        Labeled-target-only & 45.3 &69.2 &67.3 &45.3 &69.2 &67.3 &45.3 &69.2 &67.3&60.6\\
        STN&58.7&84.8&80.0&51.2&91.0&83.8&52.8&95.2&87.4&76.1\\
        SSAN&56.8&89.7&\bf87.1&54.2&82.6&\bf90.3&50.0&85.8&85.8&75.8\\
        DDACL&44.2&63.2&64.2&43.9&77.4&70.6&39.8&64.5&73.2&60.1\\
        CDSPP& 55.5 & 79.7 & 76.5 & 43.2 & 80.6 & 84.2 & 47.4 & 78.7 & 84.5 & 70.0 \\
        SGW& 49.7 & 77.7& 73.6& 49.4 & 78.4 & 73.6 & 48.7 & 80.3 & 74.5 & 67.3 \\
        \midrule
        GW& 33.6 & 41.6 & 35.5 & 39.7 & 40.0 & 31.7 & 34.8 & 34.5 & 29.0 & 35.6 \\
        \bf GW (w/ mask)& 41.3         & 71.6         & 69.7         & 41.9         & 71.2         & 69.8         & 40.3         & 71.6         & 69.7         & 60.8  \\
        HOT  & 39.0   & 44.8   & 40.0   & 31.3   & 52.6   & 44.8   & 29.7   & 60.0   & 56.5   & 44.3 \\
        \bf HOT (w/ mask) &45.2   & 60.3   & 57.4   & 48.9   & 63.5   & 59.2   & 40.3   & 67.1   & 61.4   & 55.9     \\
        TLB& 29.4   & 36.5   & 43.2   & 24.5   & 31.3   & 51.0   & 23.6   & 31.9   & 49.7   & 35.7     \\
        \bf TLB (w/ mask) &42.5   & 66.3   & 64.7   & 38.5   & 68.5   & 65.9   & 43.1   & 68.2   & 67.3   & 58.3     \\
        \midrule
        \bf KPG (w/ dist)& 55.2 & 60.7 & 71.6 & 51.3 & 71.9 & 77.1 & 48.7 & 70.0 & 77.7 & 64.9 \\
        \bf KPG-RL-GW & 58.7 & 92.9 & 84.2 & 57.4 & 95.5 & 87.1 & 55.5 & \bf95.5 &\bf 90.0 & 79.6\\
        \bf KPG-RL (LP)& 56.5 & \bf93.6 & 83.2 & \bf58.1 & 94.5 & 86.8 & 55.8 & 95.2 & 89.7 & 79.3 \\
        \bf KPG-RL (SH)& \bf 60.0 & 91.6 & 83.6 & 57.4 & \bf95.8& 87.7& \bf59.1 & 95.2 & 88.4 &\bf 79.9 \\

        \bottomrule
	\end{tabular}
	\label{tab:result_office_hda}
\end{table}

{We compare our method with the following baseline methods, including 1) ``Labeled-target-only''  that trains the kernel SVM using the labeled target data; 2) the OT methods of ``GW'', ``HOT'', and ``TLB'' that transport source domain data using barycentric mapping induced by the transport plan of GW~\citep{memoli2011gromov}, Hierarchical OT~\citep{lee2019hierarchical}, and TLB~\citep{memoli2011gromov}, and then train the kernel SVM on the transported data and labeled target domain data;  3) the typical HDA methods of ``SGW''~\citep{yan2018semi} and ``STN''~\citep{yao2019heterogeneous}, and  the recent HDA methods of ``SSAN''~\citep{li2020simultaneous}, ``DDACL''~\citep{yao2020discriminative} and ``CDSPP''~\citep{wang2022cross}.}
We conduct experiments on Office-31~\citep{saenko2010adapting} dataset.
% The results on Office-31 dataset are reported in Table~\ref{tab:result_office_hda}. Due to space limit, please refer to Appendix B2 for the results on ImageCLEF-DA.
On Office-31, we use the DeCAF$_6$~\citep{donahue2014decaf} features and the features extracted by ResNet-50~\citep{He_2016_CVPR} pretrained on ImageNet~\citep{russakovsky2015imagenet} to respectively build source and target domains for constructing heterogeneous adaptation tasks. In each task, one labeled data (1-shot) for each class is given in the target domain. Note that all the methods use the same training data (including labeled source and target domain data, and unlabeled target domain data) and test data.

Table~\ref{tab:result_office_hda} reports the results on Offce-31 dataset.  In Table~\ref{tab:result_office_hda}, ``KPG-RL (SH)'' and ``KPG-RL (LP)'' denote our entropy-regularized KPG-RL model solved using Sinkhorn's algorithm and our KPG-RL model solved by linear programming, respectively. KPG-RL (SH) achieves marginally better average accuracy than KPG-RL (LP), which could be because the barycentric mapping based on the more dense transport plan optimized by Sinkhorn's algorithm is more robust to incorrect matching. We also observe that KPG-RL-GW achieves slightly lower average accuracy than KPG-RL (SH), and KPG-RL (SH) is more computationally efficient as discussed in Sect.~\ref{sec:kpg_ot}.
``KPG (w/ dist)'' is the approach that imposes the guidance of keypoints using distance preservation in our framework, \ie, $R^s_{k,i_u}$ and $R^t_{l,j_u}$ in Eqs.~\eqref{eq:r} and \eqref{eq:r_bar} are taken as $C^s_{k,i_u}$ and $C^t_{l,j_u}$, respectively.  KPG-RL (SH) improves the accuracy of KPG (w/ dist) by 15\%, implying that our relation-preserving scheme is more effective for imposing the guidance of keypoints than the distance-preserving scheme.
{Table~\ref{tab:result_office_hda} shows that the results of GW, HOT, and TLB are inferior to that of Labeled-target-only, indicating that GW, HOT, and TLB cause negative transfer. This is reasonable because GW, HOT, and TLB do not take advantage of the guidance of keypoints and can cause wrong matching.  We also use our mask-based constraint on the transport plan in GW, HOT, and TLB, of which the corresponding approach are denoted as GW (w/ mask), HOT (w/ mask), TLB (w/ mask). We can observe that with the mask-based constraint, the performances of GW, HOT, and TLB are improved but worse than KPG-RL (SH) and KPG-RL (LP), verifying the importance of the relation preservation scheme in KPG-RL.}
SGW~\citep{yan2018semi} aligns the class centers of labeled target domain data and transported source domain data in the GW model, and realizes positive transfer. Our proposed KPG-RL (SH) outperforms SGW by 12.2\%, confirming that our KPG-RL model preserving relation is more effective than SGW preserving class centers for HDA. Compared with the other HDA methods, KPG-RL (SH) achieves the best average accuracy.

\begin{table}[t]
	\centering
	\caption{Results of methods of Labeled-target-only and KPG given different shots (1, 2, and 3) of labeled target domain data (keypoints) per-class under five distinct samplings (S1,S2,S3,S4, and S5).	}
	\setlength{\tabcolsep}{0.5pt}
	\footnotesize
	\begin{tabular}{l|ccccc>{\columncolor{mygray}}c|ccccc>{\columncolor{mygray}}c|ccccc>{\columncolor{mygray}}c}
		\toprule
		\multirow{2}*{Method} & \multicolumn{6}{c|}{1-shot}& \multicolumn{6}{c|}{2-shot}& \multicolumn{6}{c}{3-shot}\\
		~&S1&S2&S3&S4&S5&Avg&S1&S2&S3&S4&S5&Avg&S1&S2&S3&S4&S5&Avg\\
        \midrule
        Labeled-target-only & 61.2 &58.4 &64.1 &60.6 &58.7 &60.6&77.8 &76.3 &78.3 &75.0 &74.2 &76.3 &80.7 &76.6&81.0&83.4&82.7 &80.9\\
        \bf KPG-RL (SH)  & \bf 77.6 &\bf 76.2&\bf 82.1 &\bf 82.9& \bf80.7&\bf 79.9 &\bf 86.5& \bf86.5& \bf86.8& \bf84.4 &\bf 82.6& \bf85.4 &\bf 86.5&\bf 88.5&\bf 87.0 & \bf88.3 & \bf88.2&\bf87.7\\
        % \multicolumn{11}{c}{DeCAF$_6\rightarrow$ResNet-50}\\
        % \midrule

        \bottomrule
	\end{tabular}
	\label{tab:result_office_hda_keypoints}
\end{table}

\begin{figure}[t]
	\centering
	\subfigure[]{ \includegraphics[width=0.4\columnwidth]{figure//locations.pdf}
		\label{fig:locations_keypoint}}
	\subfigure[]{ \includegraphics[width=0.4\columnwidth]{figure//numbers.pdf}
		\label{fig:numbers_keypoint}}
	\caption{(a) Results for different locations of source keypoints. (b)Results for different numbers of source keypoints.}
	% \label{fig:toy_experiment}
\end{figure}

\subsubsection{Ablation Studies}
The ablation studies are conducted in HDA task.

\paragraph{Effect of target keypoints.}
The keypoints are important to KPG-RL, since they are used to guide the matching. We show the results with varying keypoints by giving different numbers and samplings of labeled target domain data, in Table~\ref{tab:result_office_hda_keypoints}. It can be observed in Table~\ref{tab:result_office_hda_keypoints} that under different numbers and samplings of labeled target domain data, our approach  KPG-RL (SH) consistently outperforms the baseline Labeled-target-only, achieving positive transfer. We also find that as the number of labeled target domain data increases, the margin between the accuracy of Labeled-target-only and KPG-RL (SH) decreases.
This indicates that given more labeled target domain data, the necessity of knowledge transferred from source domain becomes smaller.
% Please refer to Appendix~\ref{app:ablation} for the effect of source keypoints.
% Due to space limit, we give more empirical analysis, \eg, sensitivity to hyper-parameters, in Appendix B.


\paragraph{Effect of source keypoints.}
In the paper, the source keypoints are taken as the source class centers. To study the sensitivity to the location of source keypoints, we randomly sample one data point from each class as a keypoint to construct the source keypoints. We run the experiments with five different samplings for constructing the source keypoints (these five runs are denoted as S1, S2, S3, S4, S5, respectively). The results are reported in Fig.~\ref{fig:locations_keypoint}.
We can see that using the class center as the keypoints achieves the best results, compared with randomly sampling one data point per class as the keypoints. This may be because the class centers are estimated using all the data of each class, and these centers can better represent each class than a randomly sampled data point of each class.
% \begin{table}[H]
% 	\centering
% 	\caption{Results for different locations of source keypoints.}
% % 	\setlength{\tabcolsep}{3.3pt}
% 	\begin{tabular}{cccccccccc|>{\columncolor{mygray}}c}
% 		\toprule
% 		S1 &S2 &S3 &S4 &S5 &Centers   \\
%         \midrule
%        76.8 &77.5 &78.2 &77.8 &76.9 &79.9\\
%         \bottomrule
% 	\end{tabular}
% 	\label{tab:sen_key_local}
% \end{table}
We next study the sensitivity to the number of source keypoints, of which the results are reported in Fig.~\ref{fig:numbers_keypoint}.
We randomly sample 3/5/7/9 samples (keypoints) or use all the source samples (keypoints) for each class in the source domain to compute the source class centers, which are paired with labeled target samples for constructing the keypoint pairs. The results in Fig.~\ref{fig:numbers_keypoint} show that as the number of source keypoints increases, the accuracy gradually increases. The best result is obtained when all source samples are used to compute the class centers.
% \begin{table}[H]
% 	\centering
% 	\caption{Results for different numbers of source keypoints.}
% % 	\setlength{\tabcolsep}{3.3pt}
% 	\begin{tabular}{cccccccccc|>{\columncolor{mygray}}c}
% 		\toprule
% 		Number&	3&5	&7&9&All   \\
%         \midrule
%        Accuracy&	78.4&79.2&79.6&79.8&79.9\\
%         \bottomrule
% 	\end{tabular}
% 	\label{tab:sen_key_num}
% \end{table}

% \paragraph{On defining keypoints in other applications.}
% According to the results in Fig.~\ref{fig:locations_keypoint}, the class centers are better to be the keypoints than the randomly selected samples. For other practical applications, there may not be ``class labels'' available. We could first cluster the points and  then annotate the points near to the clustered centers as the keypoints.

\paragraph{Comparison of different choices for $\bm{d}$.} Since $R_k^s$ and $R_l^t$ are in the probability simplex, it is reasonable to measure their difference by a distribution divergence/distance. The widely used distribution divergences/distances include the KL-divergence, JS-divergence, and Wasserstein distance. The KL-divergence is not symmetric, so we need to determine the order of inputs. For the Wasserstein distance, one should define the ground metric first. A possible strategy is to set the ground metric to 0 if the two keypoints are paired, otherwise 1.  Such a ground metric makes the Wasserstein distance equal to the $L_1$-distance. In this work, $d$ is taken as the JS-divergence. We compare the performance of different choices of $d$ in the experiment of HDA on Office-31, as in Table~\ref{tab:ablation_d}.
\begin{table}[t]
	\centering
	\caption{Results of different choices of $d$ in HDA experiment on Office-31. }
	\setlength{\tabcolsep}{4.2pt}
	\begin{tabular}{lccccccccc|>{\columncolor{mygray}}c}
		\toprule
		Choices of $d$ & A$\rightarrow$A & A$\rightarrow$D & A$\rightarrow$W  & D$\rightarrow$A & D$\rightarrow$D & D$\rightarrow$W & W$\rightarrow$A & W$\rightarrow$D & W$\rightarrow$W &  \bf Avg\\
        \midrule
         KL-ST & 59.0   & 89.7   & \bf 83.6    & 56.8   & 95.2   & \bf 89.0    & 57.7   & 93.6   & 88.1   & 79.2   \\
        KL-TS & 58.1   & 89.0   & 82.3   & 54.2   & 93.9   & 88.1   & 54.2   & 93.2   & \bf 89.4        & 78.0     \\
        $L_1$-distance & 57.4   & 85.8   & 79.0   & 58.0   & 85.8   & 82.9   & 58.4   & 92.6   & 83.6   & 75.9     \\
        $L_2$-distance & 52.3   & 85.8   & 81.3   & 53.2   & 91.3   & 82.3   & 52.6   & 90.3   & 82.9   & 74.7     \\
        GW    & 42.0   & 71.6   & 70.0   & 41.6   & 71.0   & 69.4   & 42.3   & 71.3   & 70.0   & 61.0    \\
        JS    & \bf 60.0    & \bf 91.6    & \bf 83.6    & \bf 57.4    & \bf 95.8    & 87.7   & \bf 59.1    & \bf 95.2    & 88.4   & \bf 79.9 \\
        \bottomrule
	\end{tabular}
	\label{tab:ablation_d}
\end{table}
\begin{figure}[t]
	\centering
	\subfigure[]{ \includegraphics[width=0.31\columnwidth]{figure//rho.pdf}
		\label{fig:sensitive_rho}}
	\subfigure[]{ \includegraphics[width=0.31\columnwidth]{figure//epsilon.pdf}
		\label{fig:sensitive_epsilon}}
        \subfigure[]{ \includegraphics[width=0.31\columnwidth]{figure//alpha.pdf}
		\label{fig:sensitive_alpha}}
	\caption{ (a) Sensitivity of KPG-RL to hyper-parameter $\rho$.
 (b) Sensitivity of KPG-RL to hyper-parameter $\epsilon$. (c) Sensitivity of KPG-RL-GW to hyper-parameter $\alpha$. The results are for Closed-Set HDA task A$\rightarrow$W.
	}
 \label{fig:sensitivity}
\end{figure}
In Table~\ref{tab:ablation_d}, KL-ST and KL-TS denote the KL-divergence $KL(R_k^s, R_l^t)$ and $KL(R_l^t, R_k^s)$, respectively. GW is the Gromov-Wasserstein distance between $ R_k^s$ and $R_l^t$ where the source/target cost is taken as the $L_2$-distance of source/target keypoints. We find that the JS-divergence  achieves the best performance, compared with KL-ST, KL-TS, $L_1$-distance, $L_2$-distance, and Gromov-Wasserstein.

\paragraph{Sensitivity to hyper-parameters.}
We show the sensitivity of our method to hyper-parameters $\rho$, $\epsilon$, and $\alpha$ in Fig.~\ref{fig:sensitivity}. $\epsilon$ is the the coefficient of entropy regularization. $\rho$ is used to set $\tau$ and $\tau'$ for defining the relation in Eqs.~\eqref{eq:r} and~\eqref{eq:r_bar}.  $\alpha$ is in the KPG-RL-GW model. The best result for $\rho$ is obtained at 0.1. For $\epsilon$, the results are relatively stable in range [0.0001,0.005].
It can be observed that the best value of $\alpha$ is 0.4 in this task, and the results are relatively stable when $\alpha$ ranges in [0.2, 0.5].

\subsubsection{Open-Set Heterogeneous Domain Adaptation}\label{sec:ophda}
We conduct open-set HDA experiments to evaluate our proposed partial-KPG-RL model. The problem setting of open-set HDA is the same as HDA, except that the unlabeled target domain data contain samples of unknown classes absent in the categories of labeled data. The goal of open-set HDA is to correctly classify the common class samples and to detect the unknown class samples. We first consider the case that the proportion $\eta$ of unknown class samples in the unlabeled target domain data is given. We then extend the approach to the case
 with unknown $\eta$.
% For the case that the proportion is unknown, we can first estimate this proportion and then apply our method.
% We provide this experimental results for this case in Appendix B3.

\begin{table}[t]
	\centering
	\caption{Accuracy of common class sample (OS$^*$), Accuracy of unknown class sample (UNK), and their harmonic mean (HOS) on Office-31 for open-set HDA ($\eta = 0.67$).
% 	``A'', ``W'', and ``D'' is respectively the domains of amazon, webcam, and dslr. ``$\cdot\rightarrow*$'' denotes an adaptation task where $\cdot$/$*$ are source/target domains.
	}
	\setlength{\tabcolsep}{2.0pt}
	\footnotesize
	\begin{tabular}{l|cc>{\columncolor{mygray}}c|cc>{\columncolor{mygray}}c|cc>{\columncolor{mygray}}c|cc>{\columncolor{mygray}}c|cc>{\columncolor{mygray}}c}
		\toprule
		 \multirow{2}*{Method} & \multicolumn{3}{c|}{A$\rightarrow$A} &\multicolumn{3}{c|}{ A$\rightarrow$D }& \multicolumn{3}{c|}{A$\rightarrow$W}  &\multicolumn{3}{c|}{ D$\rightarrow$A}& \multicolumn{3}{c}{D$\rightarrow$D}\\
% 		 \cmidrule{2-4} \cmidrule{5-7}
		  ~& OS$^*$&UNK&HOS& OS$^*$&UNK&HOS& OS$^*$&UNK&HOS& OS$^*$&UNK&HOS&OS$^*$&UNK&HOS\\
% 		 & D$\rightarrow$D & D$\rightarrow$W & W$\rightarrow$A & W$\rightarrow$D & W$\rightarrow$W &  Avg\\
        \midrule
        Baseline &40.0&71.0&51.2& 37.3&85.3&51.9&29.1&82.7&43.0 &40.0&71.0&51.2&37.3&85.3&51.9\\
        STN &\bf48.2 &\bf80.6&\bf60.3 &\bf67.3&\bf86.6&\bf75.7 &54.6 &78.4& 64.3&46.3&76.6 &57.8 &63.6 &84.4&72.6 \\
        SSAN&25.4 &66.7 &36.8 &29.1 &68.4 &40.8 &\bf64.5 &\bf83.1 &\bf72.7 &34.6 &70.6 &46.4 &22.7&64.1&33.6 \\
        \bf Partial-KPG-RL&41.8&77.1&54.2&48.2&74.9&58.6 &38.2&73.2&50.2&\bf52.7&\bf82.3&\bf64.3&\bf80.0&\bf92.6&\bf85.9\\
        \midrule
        \midrule
         \multirow{2}*{Method} & \multicolumn{3}{c|}{D$\rightarrow$W} &\multicolumn{3}{c|}{ W$\rightarrow$A }& \multicolumn{3}{c|}{W$\rightarrow$D}  &\multicolumn{3}{c|}{ W$\rightarrow$W}& \multicolumn{3}{c}{\bf Avg}\\
		  ~& OS$^*$&UNK&HOS& OS$^*$&UNK&HOS& OS$^*$&UNK&HOS& OS$^*$&UNK&HOS&OS$^*$&UNK&HOS\\
        \midrule
        Baseline &29.1&82.7&43.0&40.0&71.0&51.2&37.3&85.3&51.9&29.1&82.7&43.0&35.5&79.7 &48.7\\
        STN &54.6 &79.2 &64.6&	49.9 &78.4 &60.4 &60.0 &82.6 &69.5 &55.5 &79.2&	65.2 &55.6 &80.7 &65.6\\
        SSAN&29.1 &66.4 &40.4 &31.8&68.4&43.3 &19.1&64.5&29.4&26.4&67.9&37.9&31.4&68.9&42.4 \\
        \bf Partial-KPG-RL& \bf73.6&\bf89.2&\bf80.7&\bf52.7&\bf82.7&\bf64.4&\bf78.2&\bf90.9&\bf84.1&\bf71.8&\bf88.3&\bf79.2&\bf59.7&\bf83.5&\bf69.1\\

        \bottomrule
	\end{tabular}
	\label{tab:result_office_openset_hda_main}
\end{table}
\paragraph{Open-set HDA with given $\bm{\eta}$.}
To apply the partial-KPG-RL model defined in Eq.~\eqref{eq:partial_kpp_main} to open-set HDA, for each labeled target domain data, we take its corresponding source class center to construct a keypoint pair.
We then resample the source domain data such that the total number of resampled source domain data and the source keypoints is $m'=(1-\eta) n$.  We define the source distribution as $\bm{p}=\frac{1-\eta}{m'}(\sum_{j=1}^{n_l}\delta_{c_j} + \sum_{j=n_l+1}^{m'}\delta_{x_j'})$, where $x_j'$ is a resmapled source domain sample and $c_j$ is the source class center corresponding to the target labeled sample $y_j$. The target distribution is defined as $\bm{q} = \frac{1}{n}\sum_{j=1}^n\delta_{y_j}$. The partial-KPG-RL model is conducted to transport mass from $\bm{p}$ to $\bm{q}$ with $s=1-\eta$. After transport, the $\eta$-fraction unlabeled target data receiving smallest mass from source domain are detected as unknown class and the rest unlabeled target data are taken as common class ones. Finally, we train the kernel SVM on the transported source domain data and labeled target domain data to classify the unlabeled target domain common class data.
% For applying partial-KPG-RL to open-set HDA, the keypoint pairs are constructed using labeled target domain data and their corresponding source class centers, same as KPG-RL for HDA in Sect.~\ref{sec:hda}.  We use the partial-KPG-RL model in Eq.~\eqref{eq:partial_kpp_main} to transport all the labeled source domain data to partially match the target domain data. The unmatched target domain data are detected as unknown class. We then train a kernel SVM on the transported source domain data and labeled target domain data to classify the common class samples detected by partial-KPG-RL.

The natural baseline for open-set HDA is the approach that rejects the $\eta$-proportion unlabeled samples with largest distance to labeled target domain data as unknown, and trains a kernel SVM on the labeled target domain data to classify common class data. We also evaluate the HDA methods STN~\citep{yao2019heterogeneous} and SSAN~\citep{li2020simultaneous} in the open-set HDA task. For STN~\citep{yao2019heterogeneous} and SSAN~\citep{li2020simultaneous}, we reject the $\eta$-proportion unlabeled samples with  lowest prediction confidence as unknown class.
Table~\ref{tab:result_office_openset_hda_main} reports the results. We use the open-set evaluation metrics~\citep{bucci2020effectiveness} including accuracy of common class sample (OS$^*$), accuracy of unknown class sample (UNK), and their harmonic mean HOS = $2\frac{\mbox{OS}^*\times\mbox{UNK}}{\mbox{OS}^*+\mbox{UNK}}$. We can observe in Table~\ref{tab:result_office_openset_hda_main} that our proposed partial-KPG-RL achieves the best results in terms of average OS$^*$, UNK, and HOS, indicating that the partial-KPG-RL is effective for both classifying common class data and identifying unknown class data. Compared with the Baseline,  partial-KPG-RL outperforms it by 20.4\% in terms of average HOS, confirming the positive transfer achieved by our method.
% More implementation details and the solution for Open-Set HDA with unknown $\eta$ are given in Appendix~\ref{app:openset_hda}.
% The results verify the effectiveness of partial-KPG-RL for open-set HDA.

% outperforms the Baseline by 20.4\% in terms of average HOS that balances the recognition of common and detection of unknown class data. It is observed that partial-KPG-RL achieves better average OS$^*$ and UNK than the Baseline, indicating that the partial-KPG-RL is effective for both classifying common class data and identifying unknown class data. Notably, partial-KPG-RL outperforms the Baseline in 8 among total 9 tasks in terms of HOS.
\vspace{0.2cm}
% {Due to space limit, we include the experiment of our approach for deep unsupervised domain adaptation in Appendix~\ref{app:uda}. More empirical analysis and ablation studies, \eg, sensitivity to hyper-parameters, the effect of $d$, the discussion on how to define the keypoints in more general practical applications, the time and memory cost, \textit{etc.}, are given in Appendix~\ref{app:ablation}.}

\begin{table}[t]
	\centering
	\caption{Results on Office-31 for open-set HDA with unknown $\eta$. $\hat{\eta}$ is the estimate of $\eta$ (the true $\eta = 0.67$).
% 	``A'', ``W'', and ``D'' is respectively the domains of amazon, webcam, and dslr. ``$\cdot\rightarrow*$'' denotes an adaptation task where $\cdot$/$*$ are source/target domains.
	}
	\setlength{\tabcolsep}{2.0pt}
	\footnotesize
	\begin{tabular}{l|cc>{\columncolor{mygray}}c|cc>{\columncolor{mygray}}c|cc>{\columncolor{mygray}}c|cc>{\columncolor{mygray}}c|cc>{\columncolor{mygray}}c}
		\toprule
		 \multirow{3}*{Method} & \multicolumn{3}{c|}{A$\rightarrow$A} &\multicolumn{3}{c|}{ A$\rightarrow$D }& \multicolumn{3}{c|}{A$\rightarrow$W}  &\multicolumn{3}{c|}{ D$\rightarrow$A}& \multicolumn{3}{c}{D$\rightarrow$D}\\
% 		 \cmidrule{2-4} \cmidrule{5-7}
        ~& \multicolumn{3}{c|}{($\hat{\eta}=0.57$)} &\multicolumn{3}{c|}{ ($\hat{\eta}=0.48$)}& \multicolumn{3}{c|}{ ($\hat{\eta}=0.62$)}  &\multicolumn{3}{c|}{ ($\hat{\eta}=0.57$)} & \multicolumn{3}{c}{ ($\hat{\eta}=0.48$)}\\
		  ~& OS$^*$&UNK&HOS& OS$^*$&UNK&HOS& OS$^*$&UNK&HOS& OS$^*$&UNK&HOS&OS$^*$&UNK&HOS\\
% 		 & D$\rightarrow$D & D$\rightarrow$W & W$\rightarrow$A & W$\rightarrow$D & W$\rightarrow$W &  Avg\\
        \midrule
        Baseline &38.2&61.9&47.2&20.0&\bf69.3&31.0&28.2&\bf80.1&41.7&38.2&61.9&47.2&20.0&\bf69.3&31.0\\
        % STN~\cite{yao2019heterogeneous}&\bf48.2 &\bf80.6&\bf60.3 &\bf67.3&\bf86.6&\bf75.7 &54.6 &78.4& 64.3&46.3&76.6 &57.8 &63.6 &84.4&72.6 \\
        % SSAN~\cite{li2020simultaneous}&25.4 &66.7 &36.8 &29.1 &68.4 &40.8 &\bf64.5 &\bf83.1 &\bf72.7 &34.6 &70.6 &46.4 &22.7&64.1&33.6 \\
        \bf Partial-KPG-RL&\bf49.1&\bf70.1&\bf57.8&\bf61.8&59.3&\bf60.5&\bf54.5&73.2&\bf62.5&\bf59.1&\bf73.6&\bf65.5&\bf83.6&66.7&\bf74.2\\
        \midrule
        \midrule
         \multirow{3}*{Method} & \multicolumn{3}{c|}{D$\rightarrow$W} &\multicolumn{3}{c|}{ W$\rightarrow$A }& \multicolumn{3}{c|}{W$\rightarrow$D}  &\multicolumn{3}{c|}{ W$\rightarrow$W}& \multicolumn{3}{c}{\multirow{2}*{\bf Avg}}\\
         ~& \multicolumn{3}{c|}{($\hat{\eta}=0.62$)} &\multicolumn{3}{c|}{ ($\hat{\eta}=0.57$)}& \multicolumn{3}{c|}{ ($\hat{\eta}=0.48$)}  &\multicolumn{3}{c|}{ ($\hat{\eta}=0.62$)} & \multicolumn{3}{c}{~}\\
		  ~& OS$^*$&UNK&HOS& OS$^*$&UNK&HOS& OS$^*$&UNK&HOS& OS$^*$&UNK&HOS&OS$^*$&UNK&HOS\\
        \midrule
        Baseline &28.2&80.1&41.7&38.2&61.9&47.2&20.0&\bf69.3&31.0&28.2&80.1&41.7&28.8&70.4&40.0\\
        % STN~\cite{yao2019heterogeneous} &54.6 &79.2 &64.6&	49.9 &78.4 &60.4 &60.0 &82.6 &69.5 &55.5 &79.2&	65.2 &55.6 &80.7 &65.6\\
        % SSAN~\cite{li2020simultaneous}&29.1 &66.4 &40.4 &31.8&68.4&43.3 &19.1&64.5&29.4&26.4&67.9&37.9&31.4&68.9&42.4 \\
        \bf Partial-KPG-RL&\bf78.2&\bf87.4&\bf82.6&\bf60.9&\bf74.5&\bf67.0&\bf81.8&66.7&\bf73.5&\bf78.2&\bf87.0&\bf82.4&\bf67.5&\bf73.2&\bf69.5\\

        \bottomrule
	\end{tabular}
	\label{tab:result_office_openset_hda}
\end{table}
\begin{table}[t]
	\centering
	\caption{Average HOS of partial-KPG-RL using varying magnitude of ${\eta}$ (the unknown true value of ${\eta}$ is 0.67).
	}
% 	\setlength{\tabcolsep}{3.3pt}
	\begin{tabular}{ccccccccccc}
		\toprule
		${\eta}$&0.50&0.55&0.60&0.65&0.70&0.75&0.80\\
        \midrule
        Average HOS&67.2&69.2&69.9&68.7&69.2&68.5&65.5\\
        \bottomrule
	\end{tabular}
	\label{tab:sen_eta}
\end{table}

\paragraph{Open-set HDA with unknown $\bm{\eta}$.}
For the more practical open-set HDA setting that $\eta$ is unknown, researchers can design methods to estimate $\eta$ and then apply our method using the estimate of $\eta$, or take $\eta$ as a hyper-parameter and design methods to tune it. We directly use the positive-unlabeled learning~\citep{bekker2020learning} method~\citep{zeiberg2020fast} to estimate the fraction of common class data among the target domain unlabeled data, by taking the labeled target data as positive samples. The results of different methods for open-set HDA using the estimate $\hat{\eta}$ of $\eta$ are given in Table~\ref{tab:result_office_openset_hda}. According to Table~\ref{tab:result_office_openset_hda}, the positive transfer is achieved by our method.  We can see that $\hat{\eta}$ in all tasks is lower than the true $\eta$, implying that less unknown class samples are detected.  Correspondingly, the UNK value (73.2\%) achieved by partial-KPG-RL using $\hat{\eta}$ in Table~\ref{tab:result_office_openset_hda} is smaller than that (83.5\%)  using ${\eta}$ in Table~\ref{tab:result_office_openset_hda_main}. Surprisingly, the OS$^*$ value (67.5\%) of partial-KPG-RL in Table~\ref{tab:result_office_openset_hda} is higher than that (59.7\%) in Table~\ref{tab:result_office_openset_hda_main}. As a balance, the HOS value (69.5\%) achieved by partial-KPG-RL using $\hat{\eta}$ is similar to the HOS value (69.1\%) of partial-KPG-RL using the true $\eta$.
In Table~\ref{tab:sen_eta}, we take ${\eta}$ as a hyper-parameter and show the average HOS achieved by partial-KPG-RL using varying magnitude of ${\eta}$. It is observed that the average HOS is relatively stable to ${\eta}$ in a relatively large range of $[0.55,0.75]$.

\subsection{Image-to-Image Translation}\label{sec:i2i_translation}
The  I2I translation experiments are for evaluating the proposed KPG-RL-MBP discussed in Sect.~\ref{sec:large_scale}.
We consider the ``Semi-paired'' I2I translation task that a large number of unpaired  along with a few paired cross-domain images are given for training.
% Currently, existing I2I translation are mainly supervised or unpaired. The supervised I2I translation requires a paired target image for each source image, which is expensive in annotation.
% While in the unpaired I2I translation, no paired images are given, which may require additional knowledge to guide the meaningful translation.
% Our considered task is the ``semi-supervised'' setting.
We aim to leverage the paired cross-domain images to guide the desired translation.
% , that preserves the class information for the unpaired source images.
We take the paired images as keypoints and use our proposed KPG-RL-MBP to translate the source images to the target domain. To do this, we first learn the optimal transport plan based on theorem~\ref{thm:dual}, and then learn the transport map through Eq.~\eqref{eq:gan_based}. The experimental details are given in Appendix.

\begin{figure}[t]
	\centering
        \includegraphics[width=0.8\columnwidth]{figure//mnist_cmnist.pdf}

	\caption{ Examples of images from MNIST and Chinese-MNIST. In each dashed box, the first line is the digit corresponding to the images on the second and last lines. The second and last lines respectively show images from MNIST and Chinese-MNIST.
	}
 \label{fig:samples_mnist_cmnist}
\end{figure}


\paragraph{Datasets.} The experiments are conducted on digits and natural animal images. For \textit{Digits}, we take the MNIST~\citep{lecun1998gradient} and Chinese-MNIST\footnote{\url{https://www.kaggle.com/datasets/gpreda/chinese-mnist}}  datasets as source and target distributions, respectively. The MNIST and Chinese-MNIST contain the digits (from 0 to 9) in different modalities, respectively.
Examples of images from MNIST and Chinese-MNIST are illustrated in Fig.~\ref{fig:samples_mnist_cmnist}.
In experiments, we annotate 10 keypoint pairs, each corresponding to a digit, as shown in Fig.~\ref{fig:keypoints}. We expect that with the guidance of the 10 keypoint pairs, the source images can be transported to the target ones representing the same digit. For \textit{Natural Animal Images}, we take three species (cat, fox, leopard) of animals from AFHQ~\citep{choi2020stargan} as source distribution, and another three species (lion, tiger, wolf) as target distribution. We randomly choose 1000 images for each specie. To reduce the computational cost, we resize the images to 64$\times$64. Three keypoint pairs are given, as shown in Fig.~\ref{fig:keypoints_animal}. By the guidance of the keypoint pairs, we expect that the cat, fox, and leopard images are transported to the images of lion, tiger, and wolf, respectively.

\paragraph{Metrics.} We adopt two evaluation metrics, \ie, Frechet Inception Distance (FID)~\citep{heusel2017gans} and Accuracy. ``FID'' is a commonly adopted metric in deep generative models for measuring how well the transported images resemble the target images. Lower FID indicates better image quality. The metric of ``Accuracy'' measures how well the source images are transported to be target images of ground-truth transported classes.
For digit images, the ground-truth transported classes are defined as the digits of original source images.
For animal images, the ground-truth transported classes for source images of cat, fox, and leopard are defined as lion, tiger, and wolf, respectively.
Specifically, we train a classifier on the target data to recognize their class labels (\ie, digits or animal species). We then predict the class labels of the transported source images using the trained classifier, and calculate the accuracy of the predictions against corresponding ground-truth transported class labels. Higher accuracy indicates that the source images are better transported to ground-truth transported classes, and thus the guidance of keypoints is better realized.

\paragraph{Compared methods.} We compare the following methods, including KPG-RL-BP, KPG-RL-MBP, Cycle-GAN (w/ keypoint)~\citep{zhu2017unpaired}, TCR~\citep{mustafa2020transformation}, TCR (w/ adv), WGAN-QC (w/ keypoints)~\citep{liu2019wasserstein}, W2GAN (w/ keypoints)~\citep{korotin2021wasserstein}, and OT-ICNN (w/ keypoints)~\citep{makkuva2020optimal}. KPG-RL-BP and KPG-RL-MBP are respectively the BP (Eq.~\eqref{eq:barycentric_projection}) and our proposed MBP (Eq.~\eqref{eq:gan_based}) based on the transport plan learned from our dual formulation of $L_2$-regularized KPG-RL.
``Cycle-GAN (w/ keypoint)'' is modified from the typical unsupervised I2I translation approach Cycle-GAN~\citep{zhu2017unpaired} by adding in the keypoint constraint.
% for our semi-paired I2I translation experiments.
The keypoint constraint enforces the translated source keypoints by the generator close to the target keypoints by minimizing the MSE loss. TCR~\citep{mustafa2020transformation} is the semi-supervised I2I translation approach, assuming that besides the paired images, the unpaired images are only from the source domain. In methodology,
% besides the MSE loss for paired data, \ie, keypoints,
TCR introduces transformation consistency regularization to force the modelâ€™s predictions for unpaired source data to be equivariant to data transformations. Since unpaired target data are given in our setting, we additionally employ the adversarial training loss on unpaired data for TCR, which is denoted as ``TCR (w/ adv)''. ``WGAN-QC (w/ keypoints)'', ``W2GAN (w/ keypoints)'', and ``OT-ICNN (w/ keypoints)'' are modified from the OT-based deep generative methods WGAN-QC~\citep{liu2019wasserstein}, W2GAN~\citep{korotin2021wasserstein}, and OT-ICNN~\citep{makkuva2020optimal} by adding the keypoint constraint to them, respectively.

\begin{figure}[t]
	\centering
        \subfigure[Keypoints pairs]{ \includegraphics[width=0.46\columnwidth]{figure//keypoints.pdf} \label{fig:keypoints}}
        \subfigure[Cycle-GAN (w/ keypoints)]{ \includegraphics[width=0.47\columnwidth]{figure//cycle-gan.pdf}\label{fig:Cycle-GAN (w/ keypoints)}}
        \subfigure[KPG-RL-BP]{ \includegraphics[width=0.47\columnwidth]{figure//BP.pdf}\label{fig:BP}}
        \subfigure[KPG-RL-MBP]{ \includegraphics[width=0.47\columnwidth]{figure//T_GAN.pdf}\label{fig:T_GAN}}
	\caption{ (a) Annotated keypoint pairs for digits. Each keypoint pair is in a box. (b-d) Original and transported source images by (b) Cycle-GAN (w/ keypoints), (c) KPG-RL-BP, and (d) KPG-RL-BP on digits. In (b-d), the odd columns are the source images, and their right side are the corresponding transported images.
	}
 \label{fig:transported_images}
\end{figure}
\begin{table}[t]
	\centering
	\caption{FID and accuracy of the transported source images. Smaller FID indicates higher quality of transported source images. Higher accuracy implies better imposing the guidance of the keypoints in transport. *WGAN-QC does not produce reasonable images in I2I translation.
	}
% 	\setlength{\tabcolsep}{3.3pt}
	\begin{tabular}{lccccc}
		\toprule
		\multirow{2}*{Method} & \multicolumn{2}{c}{Digits}&&\multicolumn{2}{c}{Natural animal images}\\
           ~&FID $\downarrow$& Accuracy (\%) $\uparrow$&&FID $\downarrow$ & Accuracy (\%)  $\uparrow$\\
           \midrule
           Cycle-GAN (w/ keypoints)&6.99&22.72&&78.56&30.27\\
           TCR&129.43&26.57&&342.48&33.33\\
           TCR (w/ adv)&6.90&36.21&&\textbf{74.13}&32.60\\
           \midrule
           *WGAN-QC (w/ keypoints)&- -&- -&&- -&- -\\
           W2GAN (w/ keypoints)&12.04&34.21&&121.86&28.40\\
           OT-ICNN (w/ keypoints)&14.37&29.12&&126.43&34.67\\
           \midrule
           \bf KPG-RL-BP&157.38&74.51&&285.43&60.00\\
           \bf KPG-RL-MBP&\textbf{6.54}&\textbf{76.14}&&81.02&\textbf{77.27}\\
        % & 9.99 & 157.38 &\textbf{6.54}\\
        % $\uparrow$&22.72&74.51&\textbf{76.14}\\
        % BP:305.879    cycle-gan: 78.493   T_gan: 76.997
        % BP:37.60    Cycle-GAN:26.77  T_GAN:75.60
        \bottomrule
	\end{tabular}
	\label{tab:quantitative_results_digits}
\end{table}


% To measure how the numbers are preserved, we train a classifier on the target data to recognize their numbers. We then predict the numbers of the transported source images using the trained classifier, of which the accuracy \textit{w.r.t.} the numbers of the original source images are reported in Table~\ref{tab:quantitative_results_digits}.


% Our natural baseline is the modified Cycle-GAN that we enforce the translated source keypoints by the generator close to the target keypoints via the Mean Square Error (MSE) loss in Cycle-GAN~\citep{zhu2017unpaired}, denoted as Cycle-GAN (w/ keypoints). For fair comparisons, the discriminators in the Cycle-GAN are trained by WGAN-GP loss. The architectures of $T'$/generator and discriminators of all the compared approaches are the same and are given in Appendix. We compare our approach with the other large-scale approaches for OT, including WGAN-QC~\citep{liu2019wasserstein}, W2GAN~\citep{korotin2021wasserstein} and OT-ICNN~\citep{makkuva2020optimal}. For WGAN-QC, W2GAN and OT-ICNN, we add in the MSE loss between transported source keypoints and target keypoints, and the corresponding approaches are denoted as WGAN-QC (w/ keypoints), W2GAN (w/ keypoints) and OT-ICNN (w/ keypoints). We also apply the semi-supervised I2I translation approach TCR~\citep{mustafa2020transformation} that assumes that besides the paired images, the unpaired images are only from source domain,
% \footnote{Please refer to the Related Works for the difference of our considered setting to the semi-supervised I2I translation setting studied in ~\citet{mustafa2020transformation}.}
% to our setting. In methodology, besides the MSE loss for paired data, \ie, keypoints, TCR introduces transformation consistency regularization to force the modelâ€™s predictions for unpaired source data to be equivariant to data transformations. Since unpaired target data are available for training in our setting, we additionally employ the adversarial training loss on unpaired data for TCR, which is denoted as TCR (w/ adv).


\paragraph{Results on digits for I2I translation}
% We take the training images of MNIST~\citep{lecun1998gradient} as source distribution and the Chinese-MNIST~\footnote{\url{https://www.kaggle.com/datasets/gpreda/chinese-mnist}} as target distribution. The MNIST and Chinese-MNIST contain the numbers (from 0 to 9) in different modalities, respectively.
% Several sampled images are illustrated in Fig.~\ref{fig:samples_mnist_cmnist}.
% In experiments, we annotate 10 keypoint pairs, each corresponding to a number, as shown in Fig.~\ref{fig:keypoints}. We expect that with the guidance of the 10 keypoint pairs, the source images can be transported to the target ones representing the same numbers, \ie, the number is preserved after transport.
% Note that in this experiment, we learn the transport plan in feature space, in which we train auto-encoders for MNIST and Chinese-MNIST respectively, and the encoder part is used to extract features.

In Figs.~\ref{fig:Cycle-GAN (w/ keypoints)},~\ref{fig:BP}, and~\ref{fig:T_GAN}, we show the transported images by Cycle-GAN (w/ keypoints), KPG-RL-BP, and our proposed KPG-RL-MBP. We can observe that the transported source images by the Cycle-GAN (w/ keypoints) are clear, but there seem to be many source images incorrectly transported to be the target images of other digits.
This implies that Cycle-GAN (w/ keypoints) does not well transport the source images to ground-truth transported classes (\ie, digit of the original source images).
% This implies that the class label (\ie, digit) of the source images is not well preserved by Cycle-GAN (w/ keypoints) after transport.
Figure~\ref{fig:BP} shows that most transported source images by KPG-RL-BP belong to corresponding ground-truth transported classes, but are blurry. In Fig.~\ref{fig:T_GAN}, it can be observed that many transported source images in even columns by KPG-RL-MBP share the same digits/class labels of the original source images in odd columns. Meanwhile,  KPG-RL-MBP produces transported images with less blur than KPG-RL-BP, which is attributed to our manifold constraints $\mathcal{L}_M$ in Eq.~\eqref{eq:gan_based}.

We give the quantitative results in Table~\ref{tab:quantitative_results_digits}. In the left half of Table~\ref{tab:quantitative_results_digits}, we can observe that our KPG-RL-MBP achieves the highest accuracy and the lowest FID on digits among the compared approaches, indicating that our KPG-RL-MBP can better transport source images to the ground-truth transported classes and produce transported images of high quality. Note that in Table~\ref{tab:quantitative_results_digits}, WGAN-QC\footnote{The original WGAN-QC maps noises to images. To apply WGAN-QC to the I2I translation task, we replace the generator and discriminator with those of our approach and take source images as the inputs of the generator. The implementation is based on the official code of WGAN-QC.} generates the same noisy output for all input source images, and we do not report its accuracy and FID.

% \begin{table}[t]
% 	\centering
% 	\caption{FID and accuracy of target classifier on the transported source images.
% 	}
% % 	\setlength{\tabcolsep}{3.3pt}
% 	\begin{tabular}{ccccc}
% 		\toprule
% 		Method&Cycle-GAN (w/ keypoints)&$T_{BP}$&$T_{GAN}$\\
%         \midrule
%         FID $\downarrow$& 9.99 & 157.38 &\textbf{6.54}\\
%         Accuracy (\%) $\uparrow$&22.72&74.51&\textbf{76.14}\\
%         \bottomrule
% 	\end{tabular}
% 	\label{tab:quantitative_results_digits}
% \end{table}


\begin{figure}[t]
	\centering
         \includegraphics[width=1.0\columnwidth]{figure//keypoints_animal.pdf}
	\caption{ Annotated keypoint pairs for animal images from AFHQ dataset. Each keypoint pair is in a dashed box. In each dashed box, the source/target keypoint is with blue/green border.
	}
 \label{fig:keypoints_animal}
\end{figure}

\begin{figure}[t]
	\centering
        \subfigure[Transported source images of cat.]{
         \includegraphics[width=1\columnwidth]{figure//transported_cat.pdf}
         \label{fig:transported_cat}
         }
         \subfigure[Transported source images of fox.]{
         \includegraphics[width=1\columnwidth]{figure//transported_fox.pdf}
         \label{fig:transported_fox}
         }
         % \subfigure[]{
         % \includegraphics[width=0.9\columnwidth]{figure//transported_lepod.pdf}}

	\caption{ Transported source images of  cat and fox by Cycle-GAN (w/ keypoints), KPG-RL-BP, and KPG-RL-MBP. By the guidance of paired keypoints (see Fig.~\ref{fig:keypoints_animal}), the images of cat/fox are expected to be transported to images of lion/tiger.
	}
 \label{fig:transported_images_animal1}
\end{figure}

\begin{figure}[t]
	\centering
        % \subfigure[]{
        %  \includegraphics[width=0.9\columnwidth]{figure//transported_cat.pdf}}
        %  \subfigure[]{
        %  \includegraphics[width=0.9\columnwidth]{figure//transported_fox.pdf}}
         % \subfigure[]{
         \includegraphics[width=1\columnwidth]{figure//transported_lepod.pdf}
         % }

	\caption{ Transported source images of  leopard by Cycle-GAN (w/ keypoints), KPG-RL-BP, and KPG-RL-MBP.  By the guidance of paired keypoints (see Fig.~\ref{fig:keypoints_animal}), the images of leopard are expected to be transported to images of wolf.
	}
 \label{fig:transported_images_animal2}
\end{figure}

\paragraph{Results on natural animal images for I2I translation.}
% We conduct this experiment on AFHQ dataset~\citep{choi2020stargan}. We take three species (cat, fox, leopard) of animals from AFHQ as source, and another three species (lion, tiger, wolf) as target. We randomly choose 1000 images for each specie. To reduce the computational cost, we resize the images to 32$\times$32. Three keypoint pairs are given, as shown in Fig.~\ref{fig:keypoints_animal}. By the guidance of the keypoint pairs, we expect that the cat, fox, and leopard images are transported to the images of lion, tiger, and wolf, respectively. The experimental setup is the same as that for the digits, except for that the feature extractor is taken as the image encoder of CLIP~\citep{radford2021learning}, and the number of input/output channels of $T'$/generator is 3. The quantitative and qualitative results are given in  Table~\ref{tab:quantitative_results_digits} and Fig.~\ref{fig:transported_images_animal}, respectively.
The results on natural animal images for I2I translation are shown in the right half of Table~\ref{tab:quantitative_results_digits}, and Figs.~\ref{fig:transported_images_animal1} and~\ref{fig:transported_images_animal2}.
Table~\ref{tab:quantitative_results_digits} shows that our proposed approach KPG-RL-MBP achieves a comparable FID, compared with the other methods. The accuracy achieved by our KPG-RL-MBP is higher than that of the other compared methods by more than 17\% as in Table~\ref{tab:quantitative_results_digits}.
The results indicate that our approach better transport the source images to corresponding ground-truth transported classes\footnote{The ground-truth transported classes for source images of cat, fox, and leopard are lion, tiger, and wolf, respectively.}. This is also verified by the qualitative results in Figs.~\ref{fig:transported_images_animal1} and~\ref{fig:transported_images_animal2}.
We explicitly model the guidance of paired keypoints to the unpaired data in KPG-RL-BP and KPG-RL-MBP, while the other approaches do not model the relation of samples to keypoints. This may account for the higher accuracy of KPG-RL-BP and KPG-RL-MBP than that of the other approaches.
The KPG-RL-BP produces blurry images as in Figs.~\ref{fig:transported_images_animal1} and~\ref{fig:transported_images_animal2}, because the transported images are the weighted average of target domain images according to Eq.~\eqref{eq:barycentric_projection}. KPG-RL-MBP produces more clear images than KPG-RL-BP as shown in Figs.~\ref{fig:transported_images_animal1} and~\ref{fig:transported_images_animal2}, verifying the effectiveness of the manifold barycentric projection for constraining the projected images into the target data manifold.
Note that ideally, most transported images by KPG-RL-BP though should be of the ground-truth transported classes according to Eq.~\eqref{eq:barycentric_projection}, are blurry as in Figs.~\ref{fig:transported_images_animal1} and~\ref{fig:transported_images_animal2}. Hence the classifier may not recognize the transported images by KPG-RL-BP correctly. This leads to lower accuracy than  KPG-RL-MBP in Table~\ref{tab:quantitative_results_digits}.

\section{Conclusion}\label{sec:conclusion}
This paper proposes a novel KPG-RL model that leverages keypoints to guide the correct matching (\ie, transport plan) in OT. We devise a mask-based constraint to preserve the matching of keypoints pairs, and propose to preserve the relation of each point to the keypoints to impose the guidance of these keypoints in OT. We further propose a manifold barycentric projection to transport source images to target domain, based on the developed dual formulation of KPG-RL.
The effectiveness of the proposed KPG-RL model is verified in the application of HDA and I2I translation.
The keypoint-guided OT model could be possibly applied to more applications, \eg, point-set or image registration.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \acks{This work was supported by ???.}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

% \newpage

\appendix
\section*{Appendix A: Proof of  Proposition~\ref{thm:thm_mask_main}}
% \subsection*{Proof of  Proposition 1}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

% In this appendix, we prove Proposition~\ref{thm:thm_mask_main} in Sect.~\ref{sec:kpg_rl}.

\noindent
{\bf Proof}.
For any $(i,j)\in\mathcal{K}$, from the definition of $M$, we have $M_{i,j'} = 0$ for all $j'\neq j$ and $M_{i,j}=1$. Then, we have $\Tilde{\pi}_{i,j'} = M_{i,j'}\pi_{i,j'} = 0$ for all $j'\neq j$. Since $\sum_{j'=1}^n \Tilde{\pi}_{i,j'}  = p_i$, we have $\Tilde{\pi}_{i,j}=p_i$. Similarly, we have  $M_{i',j} = 0$ for all $i'\neq i$. Then, we have $\Tilde{\pi}_{i',j} = M_{i',j}\pi_{i',j} = 0$ for all $i'\neq i$, and $\Tilde{\pi}_{i,j}= \sum_{i'=1}^m \Tilde{\pi}_{i',j}  = q_j$. This implies that all the mass $p_i$ of $x_i$ is transported to $y_j$, and $y_j$ only receives the mass from $x_i$. Hence, the matching of $x_i$ and $y_j$ is preserved.

\section*{Appendix B: Deduction of Sinkhorn Iteration}
The entropy-regularized model for KPG-RL is
\begin{equation}\label{original_reg}
    \begin{split}
        &\min_{\pi} \langle M\odot \pi, G \rangle_F - \epsilon H(M\odot \pi)\\
        &s.t.  \hspace{0.2cm}\pi\geq 0, (M\odot \pi)\mathbbm{1}_{n} = \bm{p},
        (M\odot \pi)^{\top}\mathbbm{1}_{m} = \bm{q},
    \end{split}
\end{equation}
where $H(M\odot \pi)=-\left(\langle M\odot \pi, \log(M\odot \pi) \rangle_F - \mathbbm{1}_m^{\top}(M\odot\pi)\mathbbm{1}_n\right)$ is the entropy of the transport plan $M\odot \pi$.
The Lagrangian function is
\begin{equation}
\begin{split}
     L(\pi,\bm{f},\bm{g}) = &\langle M\odot \pi, G \rangle_F + \epsilon \left(\langle M\odot \pi, \log(M\odot \pi)\rangle_F - \mathbbm{1}_m^{\top}(M\odot\pi)\mathbbm{1}_n\right) \\
      &-  \langle \bm{f}, (M\odot\pi) \mathbbm{1}_n -\bm{p}\rangle_F - \langle \bm{g}, (M\odot\pi)^{\top} \mathbbm{1}_m -\bm{q}\rangle_F,
\end{split}
\end{equation}
where $\bm{f}\in \mathbb{R}^{m}$ and $\bm{g}\in \mathbb{R}^{n}$.
The first-order conditions then yield
\begin{equation}
    \frac{\partial L}{\partial \pi_{i,j}} = M_{i,j}G_{i,j} + \epsilon M_{i,j}\log(M_{i,j}\pi_{i,j}) -M_{i,j}f_i -M_{i,j}g_j = 0.
\end{equation}
If $M_{i,j}=0$, $\pi_{i,j}$ could be arbitrary non-negative value, and if $M_{i,j}=1$, we have
$\pi_{i,j}=e^{f_i/\epsilon}e^{-G_{i,j}/\epsilon}e^{g_j/\epsilon}$. Therefore, we have
% we can unify the expression as
$M_{i,j}\pi_{i,j}=M_{i,j}e^{f_i/\epsilon}e^{-C_{i,j}/\epsilon}e^{g_j/\epsilon}$.
% in which we enforce $\pi_{i,j}=0$ if $M_{i,j}=0$.
The matrix form is $M\odot\pi=\mbox{diag}(\bm{u})K\mbox{diag}(\bm{v})$, where $\bm{u}=e^{\bm{f}/\epsilon}, K = M\odot e^{-G/\epsilon}, \mbox{ and } \bm{v}=e^{\bm{g}/\epsilon}$. The constraints are
\begin{equation}
    \mbox{diag}(\bm{u})K\mbox{diag}(\bm{v})\mathbbm{1}_{n} = \bm{p}, \hspace{0.4cm} (\mbox{diag}(\bm{u})K\mbox{diag}(\bm{v}))^{\top}\mathbbm{1}_{m} = \bm{q}.
\end{equation}
Since the entries of $K$ are non-negative, Sinkhorn's algorithm can be applied~\citep{sinkhorn1967concerning}. The iteration formulas are
% Let $\tilde{K}=M\odot K$, then the Sinkhorn iteration can be applied as
\begin{equation}\label{eq:sinkhorn}
    \bm{u}^{(l+1)} = \frac{\bm{p}}{K\bm{v}^l}, \hspace{0.2cm}\bm{v}^{(l+1)} = \frac{\bm{q}}{K^{\top}\bm{u}^{(l+1)}}.
\end{equation}
The division operator used above is entry-wise.

\section*{Appendix C: Proof of Theorem~\ref{thm:partial_kpp_main}}
\noindent
{\bf Proof}.
We denote $\ddot{\pi}=\bar{\pi}^*_{1:m,1:n}$ and $t = \bar{\pi}^*_{m+1,n+1}$.
To prove Theorem~\ref{thm:partial_kpp_main}, we first give some preparations, and then conduct the following three steps. In step 1, we show that $t =0$. In step 2, we show that $\ddot{\pi} \in \Pi^{s}(\bm{p},\bm{q};M)$ which means that $\ddot{\pi}$ is a feasible solution of problem~\eqref{eq:partial_kpp_main} In step 3, we show that $M\odot\ddot{\pi} $ is the optimal transport plan of problem~\eqref{eq:partial_kpp_main}. We next detail these steps.

\vspace{0.5\baselineskip}
\noindent
\textbf{Preparations:}

Since $\bar{\pi}^* \in \Pi(\bar{\bm{p}},\bar{\bm{q}};\bar{M})$ and $t=\bar{\pi}^*_{m+1,n+1}$, we have
\begin{equation}
\begin{split}
  \mathbbm{1}_{m+1}^{\top}(\bar{M}\odot\bar{\pi}^*)\mathbbm{1}_{n+1}=&
\left[
\begin{matrix}
    \mathbbm{1}_m^{\top} &1
\end{matrix}\right] \left(\left[
\begin{matrix}
    M & \bm{a}\\
    \bm{b} & 1
\end{matrix}\right]\odot\left[
\begin{matrix}
    \ddot{\pi} & \bar{\pi}^*_{1:m,n+1}\\
    \bar{\pi}^*_{m+1,1:n} & \bar{\pi}^*_{m+1,n+1}
\end{matrix}\right]\right)\left[\begin{matrix}
    \mathbbm{1}_n\\ 1
\end{matrix}\right]\\ = &\mathbbm{1}_m^{\top}(M\odot\ddot{\pi})\mathbbm{1}_n + \sum_{i=1}^m{a_i\bar{\pi}^*_{i,n+1}} + \sum_{j=1}^n{b_j\bar{\pi}^*_{m+1,j}} + t.
\end{split}
\end{equation}
Meanwhile, according to the constraints in $\Pi(\bar{\bm{p}},\bar{\bm{q}};\bar{M})$, we have
\begin{equation}
    \mathbbm{1}_{m+1}^{\top}(\bar{M}\odot\bar{\pi}^*)\mathbbm{1}_{n+1}= \mathbbm{1}_{m+1}^{\top}\bar{\bm{p}} = \Vert \bar{\bm{p}} \Vert_1 = \Vert \bm{p}\Vert_1 + \Vert \bm{q}\Vert_1 -s,
\end{equation}
$\sum_{i=1}^m{a_i\bar{\pi}^*_{i,n+1}} + t = \Vert \bm{p}\Vert_1 -s$, and $\sum_{j=1}^n{b_j\bar{\pi}^*_{m+1,j}} + t = \Vert \bm{q}\Vert_1 -s$.
% \begin{equation}
%      \sum_{i=1}^m{a_i\bar{\pi}^*_{i,n+1}} + t = \Vert \bm{p}\Vert_1 -s,
% \end{equation}
% and
% \begin{equation}
%     \sum_{j=1}^n{b_j\bar{\pi}^*_{m+1,j}} + t = \Vert \bm{q}\Vert_1 -s.
% \end{equation}
Combining the above equations, we have
\begin{equation}
    \mathbbm{1}_m^{\top}(M\odot\ddot{\pi})\mathbbm{1}_n + \Vert \bm{p}\Vert_1 + \Vert \bm{q}\Vert_1 -2s -t = \Vert \bm{p}\Vert_1 + \Vert \bm{q}\Vert_1 -s.
\end{equation}
Therefore,
\begin{equation}
    \mathbbm{1}_m^{\top}(M\odot\ddot{\pi})\mathbbm{1}_n = s+t.
\end{equation}


\vspace{0.5\baselineskip}
\noindent
\textbf{Step 1: show that $t=\bar{\pi}^*_{m+1,n+1} = 0$.}

First, we have
\begin{equation}\label{eq:expand_2}
    \begin{split}
        \langle\bar{M}\odot\bar{\pi}^*,\bar{G}\rangle_F = & \sum_{i=1}^{m}\sum_{j=1}^n M_{i,j}\bar{\pi}^*_{i,j}G_{i,j}
        + \xi\sum_{i=1}^ma_i\bar{\pi}^*_{i,n+1}\\ + &\xi \sum_{j=1}^nb_j\bar{\pi}^*_{m+1,j} + (2\xi+A)\bar{\pi}^*_{m+1,n+1}\\
        = & \sum_{i=1}^{m}\sum_{j=1}^n M_{i,j}\bar{\pi}^*_{i,j}G_{i,j} + \xi (\Vert \bm{p}\Vert_1 + \Vert \bm{q}\Vert_1 -2s -2t) + (2\xi+A)t\\
        = & \sum_{i=1}^{m}\sum_{j=1}^n M_{i,j}\bar{\pi}^*_{i,j}G_{i,j} + \xi (\Vert \bm{p}\Vert_1 + \Vert \bm{q}\Vert_1 -2s) + At.
    \end{split}
\end{equation}
Suppose $t=\bar{\pi}^*_{m+1,n+1}>0$, we next construct a solution $\gamma$ such that $\gamma_{m+1,n+1}=0$ and leads to conflict.
We randomly select a set $S = \{(i,j)|\bar{\pi}^*_{i,j}>0, i\leq m, j\leq n, i\notin \mathcal{I}, j\notin \mathcal{J}\}$ and a index pair $(i_0,j_0)$ satisfying the constraints of elements in $S$, such that $\sum_{(i,j)\in S}\bar{\pi}^*_{i,j}\leq t$ and $\sum_{(i,j)\in S}\bar{\pi}^*_{i,j} + \bar{\pi}^*_{i_0,j_0} > t$. In the rest part of this section, the involved $i,j$ satisfy $i\leq m$ and $j\leq n$.
Such non-empty $S$ and $(i_0,j_0)$ always exist, because
\begin{equation}
\begin{split}
    \mathbbm{1}_m^{\top}(M\odot\ddot{\pi})\mathbbm{1}_n = &\sum_{i=1}^m\sum_{j=1}^n{M_{i,j}\bar{\pi}^*_{i,j}}\\
    % = \sum_{i\in \mathcal{I},j}\bar{\pi}^*_{i,j} + \sum_{i\notin \mathcal{I},j\in \mathcal{J}}M_{i,j}\bar{\pi}^*_{i,j} + \sum_{i\notin \mathcal{I}, j\notin \mathcal{J}}\bar{\pi}^*_{i,j}  \\
    = &\sum_{i\in \mathcal{I},j}M_{i,j}\bar{\pi}^*_{i,j} + \sum_{i\notin \mathcal{I},j\in \mathcal{J}}M_{i,j}\bar{\pi}^*_{i,j} + \sum_{i\notin \mathcal{I}, j\notin \mathcal{J}}M_{i,j}\bar{\pi}^*_{i,j}  \\
    % = &\sum_{i\in \mathcal{I},j}\bar{\pi}^*_{i,j} + \sum_{i\notin \mathcal{I},j\in \mathcal{J}}\mathbb{I}(M_{i,j}=1)\bar{\pi}^*_{i,j} + \sum_{i\notin \mathcal{I}, j\notin \mathcal{J}}\bar{\pi}^*_{i,j}  \\
    % = &\sum_{i\in \mathcal{I},j}\bar{\pi}^*_{i,j}  + \sum_{i\notin \mathcal{I}, j\notin \mathcal{J}}\bar{\pi}^*_{i,j}
    % \leq &\sum_{i\in I}p_i + \sum_{i'\in \mathcal{I},i \neq i'}M_{i,\kappa(i')}\bar{\pi}^*_{i,\kappa(i')}+\sum_{i\notin \mathcal{I}, j\notin \mathcal{J}}\bar{\pi}^*_{i,j} \\
    = &\sum_{i\in I}p_i +\sum_{i\notin \mathcal{I}, j\notin \mathcal{J}}\bar{\pi}^*_{i,j} \mbox{ (using definition of $M$)},
\end{split}
\end{equation}
$\mathbbm{1}_m^{\top}(M\odot\ddot{\pi})\mathbbm{1}_n = s+t$,
and $\sum_{i\in \mathcal{I}}p_i < s$, we have $\sum_{i\notin \mathcal{I}, j\notin \mathcal{J}}\bar{\pi}^*_{i,j}>t$. We now move the mass of index pairs in $S$ and $(i_0,j_0)$ to their marginal such that a total mass of $t$ is moved. Specifically,
for $(i,j)\in S$, we set $\gamma_{i,j}  = 0, \gamma_{i,n+1} = \bar{\pi}^*_{i,n+1} + \bar{\pi}^*_{i,j},  \gamma_{m+1,j} = \bar{\pi}^*_{m+1,j} + \bar{\pi}^*_{i,j}$.
For $(i_0,j_0)$, we set $\gamma_{i_0,j_0} = \bar{\pi}^*_{i_0,j_0} - (t-\sum_{i\notin \mathcal{I}, j\notin \mathcal{J}}\bar{\pi}^*_{i,j}), \gamma_{i_0,n+1} = \bar{\pi}^*_{i_0,n+1} + (t-\sum_{i\notin \mathcal{I}, j\notin \mathcal{J}}\bar{\pi}^*_{i,j}),  \gamma_{m+1,j_0} = \bar{\pi}^*_{m+1,j_0}-(t-\sum_{i\notin \mathcal{I}, j\notin \mathcal{J}}\bar{\pi}^*_{i,j})$.
For $(i,j)\notin S$, we set $\gamma_{i,j} = \bar{\pi}^*_{i,j}, \gamma_{i,n+1} = \bar{\pi}^*_{i,n+1},  \gamma_{m+1,j} = \bar{\pi}^*_{m+1,j}$. It is easy to verify that $\gamma\in\Pi(\bar{\bm{p}},\bar{\bm{q}};\bar{M})$.
Similar to Eq.~\eqref{eq:expand_2}, we have
\begin{equation}
    \langle\bar{M}\odot\gamma,\bar{G}\rangle_F = \sum_{i=1}^{m}\sum_{j=1}^n M_{i,j}\gamma_{i,j}G_{i,j} + \xi (\Vert \bm{p}\Vert_1 + \Vert \bm{q}\Vert_1 -2s) .
\end{equation}
Using the optimality of $\bar{M}\odot\bar{\pi}^*$, we have
\begin{equation}\label{eq:11}
    \langle\bar{M}\odot\gamma,\bar{G}\rangle_F- \langle\bar{M}\odot\bar{\pi}^*,\bar{G}\rangle_F = \sum_{i=1}^{m}\sum_{j=1}^n M_{i,j}(\gamma_{i,j}-\bar{\pi}^*_{i,j})G_{i,j} - At > 0.
\end{equation}
From the definition of $\gamma$, we can see that $\gamma_{i,j}\leq\bar{\pi}^*_{i,j}$, and thus $\sum_{i=1}^{m}\sum_{j=1}^n M_{i,j}(\gamma_{i,j}-\bar{\pi}^*_{i,j})G_{i,j} \leq 0$. Hence, from Eq.~\eqref{eq:11}, we have $A<0$, contradicting the assumption that $A>0$. Therefore, $t=\bar{\pi}^*_{m+1,n+1}=0$ holds.

\vspace{0.5\baselineskip}
\noindent
\textbf{Step 2: show that $\ddot{\pi}$ is a feasible solution of problem in Eq.~\eqref{eq:partial_kpp_main}.}

We verify the constraints as follows.

(1) Since $\bar{\pi}^*\geq 0$, we have $\ddot{\pi}\geq 0$.

(2) $(\bar{M}\odot\bar{\pi}^*)\mathbbm{1}_{n+1} = \left[
\begin{matrix}
    M\odot\ddot{\pi} & \bm{a}\odot\bar{\pi}^*_{1:m,n+1}\\
    \bm{b}\odot\bar{\pi}^*_{m+1,1:n} & 0
\end{matrix}\right] \left[\begin{matrix}
    \mathbbm{1}_n\\
    1
\end{matrix}\right]
=\left[\begin{matrix}
    \bm{p}\\
    \Vert \bm{q} \Vert_1 -s
\end{matrix}\right]$, then
$(M\odot\ddot{\pi})\mathbbm{1}_{n} + \bm{a}\odot\bar{\pi}^*_{1:m,n+1} = \bm{p}$, and $(M\odot\ddot{\pi})\mathbbm{1}_{n}\leq \bm{p}$.

(3) Similarly, from $(\bar{M}\odot\bar{\pi}^{*})^{\top}\mathbbm{1}_{m+1}= (\bm{q}^{\top},\Vert \bm{q} \Vert_1 -s)^{\top}$, we have $(M\odot\ddot{\pi})^{\top}\mathbbm{1}_{m}\leq \bm{q}$.

(4) $\mathbbm{1}_{m}^{\top}(M\odot\ddot{\pi})\mathbbm{1}_{n} = s$ holds, because $t=0$ as in Step 1.

(5) $\forall i \in \mathcal{I}, (\bar{M}\odot\bar{\pi}^{*})_{i,:}\mathbbm{1}_{n+1} = (M\odot\ddot{\pi})_{i,:}\mathbbm{1}_n + a_i\bar{\pi}^*_{i,n+1} = p_i$. Since $a_i=0$, we have $(M\odot\ddot{\pi})_{1,:}\mathbbm{1}_n = p_i$.

(6) $\forall j \in \mathcal{J}, \mathbbm{1}_{m+1}^{\top}(\bar{M}\odot\bar{\pi}^{*})_{:,j} = \mathbbm{1}_m^{\top}(M\odot\ddot{\pi})_{:,j} + b_j\bar{\pi}^*_{m+1,j} = q_j$. Since $b_j=0$, we have $\mathbbm{1}_m^{\top}(M\odot\ddot{\pi})_{:,j} = q_j$.

Therefore, we have $\ddot{\pi}\in\Pi^s(\bm{p},\bm{q};M)$, and $\ddot{\pi}$ is a feasible solution of problem in Eq.~\eqref{eq:partial_kpp_main}.

\vspace{0.5\baselineskip}
\noindent
\textbf{Step 3: show that $M\odot\ddot{\pi} $ is the optimal transport plan of problem in Eq.~\eqref{eq:partial_kpp_main}. }

Suppose there exist a transport plan $M\odot\gamma$ with $\gamma\in\Pi^s(\bm{p},\bm{q};M)$ such that $$\sum_{i=1}^m\sum_{j=1}^nM_{i,j}\gamma_{i,j}G_{i,j} < \sum_{i=1}^m\sum_{j=1}^nM_{i,j}\ddot{\pi}_{i,j}G_{i,j}.$$ We construct $\bar{\gamma}$ as follows. For $i\leq m, j\leq n$, $\bar{\gamma}_{i,j} = \gamma_{i,j}$. $\bar{\gamma}_{i,n+1} = p_i - \sum_{j=1}^n\gamma_{i,j}, \forall i \leq m$.  $\bar{\gamma}_{m+1,j} = q_j - \sum_{i=1}^n\gamma_{i,j}, \forall j \leq n$. $\bar{\gamma}_{m+1,n+1}=0$. Easily, we can verify that $\bar{\gamma}$ is in $\Pi(\bar{\bm{p}},\bar{\bm{q}};\bar{M})$. Meanwhile,
\begin{equation}
\begin{split}
        \langle \bar{M}\odot\bar{\gamma}, \bar{G} \rangle_F =& \sum_{i=1}^m\sum_{j=1}^nM_{i,j}\gamma_{i,j}C_{i,j} + \xi (\Vert \bm{p}\Vert_1+ \Vert \bm{q}\Vert_1 -2s)\\
        < &\sum_{i=1}^m\sum_{j=1}^nM_{i,j}\ddot{\pi}_{i,j}G_{i,j}+ \xi (\Vert \bm{p}\Vert_1+ \Vert \bm{q}\Vert_1 -2s)
        = \langle \bar{M}\odot\bar{\pi}^{*}, \bar{G} \rangle_F.
\end{split}
\end{equation}
This contradicts the fact that $\bar{M}\odot\bar{\pi}^{*}$ is the optimal transport plan of problem $\mbox{min}_{\bar{\pi} \in \Pi(\bar{\bm{p}},\bar{\bm{q}};\bar{M})} \langle \bar{M}\odot\bar{\pi}, \bar{G} \rangle_F$. Therefore, $M\odot\ddot{\pi} $ is the optimal transport plan of the problem in Eq.~\eqref{eq:partial_kpp_main}.

\section*{Appendix D: Proof of Theorem~\ref{thm:kpg_rl_kp}}
In Appendix D and Appendix E, for the  convenience of description, we denote the mask matrix for distributions $\bm{p}$ and $\bm{q}$ as $M^{\bm{p}\bm{q}}$.

\noindent
{\bf Proof}. We next verify the conditions of proper metric.

\vspace{0.2\baselineskip}
% \noindent
{\textbf{(1) Show that $\mathcal{S}_{krk}(\bm{p},\bm{q})=0$ if and only if $\bm{p}=\bm{q}$}}.

(a) If $\bm{p}=\bm{q}$, we have $x_i=y_i$ and $p_i=q_i$ for any $i\in[m]$ (since the permutation of support points does not change the distribution). Hence, $C_{i,i}=c(x_i,y_i)=0$, and $C^s_{i,i_u}=c(x_i,x_{i_u})=c(y_i,y_{i_u})=C^t_{i,i_u}, \forall i\in[m] \mbox{ and }\forall i_u\in\mathcal{I}$, which implies that $R^s_i=R^t_i$. Then, we have $G_{i,i}=d(R^s_i,R^t_i)=0$. We define $\pi$ by $\pi_{i,j}=p_i$ if $i=j$, otherwise 0. Obviously, $M^{\bm{p}\bm{q}}\odot\pi$ is in $\Pi(\bm{p},\bm{q};M^{\bm{p}\bm{q}})$ and $\sum_{i,j}M^{\bm{p}\bm{q}}_{i,j}\pi_{i,j}(\alpha C_{i,j}+(1-\alpha)G_{i,j}) = 0$. Therefore, $\mathcal{S}_{krk}(\bm{p},\bm{q})=0$.

(b) We denote $\pi^*$ as the optimal solution of problem~\eqref{eq:s_krk}. If $\mathcal{S}_{krk}(\bm{p},\bm{q})=0$, we have $\langle M^{\bm{p}\bm{q}}\odot\pi^* ,C \rangle_F=0$. This means that the KP problem $\min_{\pi\in\Pi(\bm{p},\bm{q})}\langle \pi ,C \rangle_F = 0$. Using the Proposition 2.2 in \citet{peyre2019computational}, we have $\bm{p}=\bm{q}$.

\vspace{0.2\baselineskip}
% \noindent
{\textbf{(2) Show that $\mathcal{S}_{krk}(\bm{p},\bm{q})=\mathcal{S}_{krk}(\bm{q},\bm{p})$}}.
% Since the paired keypoints across domains share the same index, the mask matrix $M^{\bm{p}\bm{q}}$ is symmetric. Thus, $M^{\bm{p}\bm{q}} = (M^{\bm{p}\bm{q}})^{\top}= M^{\bm{q}\bm{p}}$.

From the definition of mask matrix in Proposition~\ref{thm:partial_kpp_main}, we have $M^{\bm{p}\bm{q}}_{i,j}= M^{\bm{q}\bm{p}}_{j,i}$.
$C$ and $G$ are symmetric because $c$ and $d$ are distances.  For any $\pi \in \Pi(\bm{p},\bm{q};M^{\bm{p}\bm{q}})$, we define $\pi'$ as $\pi'_{i,j}=\pi_{j,i}$, and then $\pi' \in \Pi(\bm{q},\bm{p};M^{\bm{q}\bm{p}})$.
Then, we have
\begin{equation}
    \begin{split}
    \mathcal{S}_{krk}(\bm{p},\bm{q})=
    &\min_{\pi \in \Pi(\bm{p},\bm{q};M^{\bm{p}\bm{q}})}\sum_{i,j}{M^{\bm{p}\bm{q}}_{i,j}\pi_{i,j}(\alpha C_{i,j}+(1-\alpha)G_{i,j})}\\ =&\min_{\pi \in \Pi(\bm{p},\bm{q};M^{\bm{p}\bm{q}})}\sum_{i,j}{M^{\bm{q}\bm{p}}_{j,i}\pi_{i,j}(\alpha C_{j,i}+(1-\alpha)G_{j,i})}\\
= &\min_{\pi' \in \Pi(\bm{q},\bm{p};M^{\bm{q}\bm{p}})}\sum_{j,i}{M^{\bm{q}\bm{p}}_{j,i}\pi'_{j,i}(\alpha C_{j,i}+(1-\alpha)G_{j,i})}\\
=&\mathcal{S}_{krk}(\bm{q},\bm{p}).
    \end{split}
\end{equation}
% Therefore, $\mathcal{S}_{krk}(\bm{p},\bm{q})=\mathcal{S}_{krk}(\bm{q},\bm{p})$.
% $. By defining $\pi'$ as $\pi'_{i,j}=\pi_{j,i}$, we have $\sum_{i,j}{M^{\bm{p}\bm{q}}_{i,j}\pi_{i,j}(\alpha C_{i,j}+(1-\alpha)G_{i,j})}

\vspace{0.2\baselineskip}
% \noindent
{\textbf{(3) Show that $\mathcal{S}_{krk}(\bm{p},\bm{q})\leq\mathcal{S}_{krk}(\bm{p},\bm{r})+\mathcal{S}_{krk}(\bm{r},\bm{q})$ for any $\bm{r}=\sum_{k=1}^mr_k\delta_{z_k}\in \mathcal{P}_{\mathcal{I}}^\mathcal{X}$}}.

Let $M^{\bm{p}\bm{r}}\odot\pi^{\bm{p}\bm{r}}$ and $M^{\bm{r}\bm{q}}\odot\pi^{\bm{r}\bm{q}}$ be the optimal transport plans corresponding to $\mathcal{S}_{krk}(\bm{p},\bm{r})$ and $\mathcal{S}_{krk}(\bm{r},\bm{q})$, respectively. We define \begin{equation}
    \tilde{\gamma}=(M^{\bm{p}\bm{r}}\odot\pi^{\bm{p}\bm{r}})\mbox{diag}\left(\frac{1}{\tilde{\bm{r}}}\right) (M^{\bm{r}\bm{q}}\odot\pi^{\bm{r}\bm{q}}),
\end{equation}
where the element $\tilde{r}_j$ of $\tilde{\bm{r}}$ is $r_j$ if $r_j>0$, and 1 otherwise. We notice that
\begin{equation}
    \tilde{\gamma}\mathbbm{1}_m =(M^{\bm{p}\bm{r}}\odot\pi^{\bm{p}\bm{r}})\mbox{diag}\left(\frac{1}{\tilde{\bm{r}}}\right)\bm{r}= (M^{\bm{p}\bm{r}}\odot\pi^{\bm{p}\bm{r}})\left(\frac{\bm{r}}{\tilde{\bm{r}}}\right) = (M^{\bm{p}\bm{r}}\odot\pi^{\bm{p}\bm{r}})\tilde{\mathbbm{1}}_{m},
\end{equation}
where the $j$-th location of $\tilde{\mathbbm{1}}_{m}$ is 1 if $r_j>0$, otherwise 0. Note that for $j$ such that $r_j=0$, we have $\sum_{i}(M^{\bm{p}\bm{r}}\odot\pi^{\bm{p}\bm{r}})_{i,j}=r_j=0$, which implies $(M^{\bm{p}\bm{r}}\odot\pi^{\bm{p}\bm{r}})_{i,j}=0$ for any $i$. Hence,
\begin{equation}
    \tilde{\gamma}\mathbbm{1}_m = (M^{\bm{p}\bm{r}}\odot\pi^{\bm{p}\bm{r}})\tilde{\mathbbm{1}}_{m} = (M^{\bm{p}\bm{r}}\odot\pi^{\bm{p}\bm{r}}){\mathbbm{1}}_{m} = \bm{p}.
\end{equation}
Similarity, $\tilde{\gamma}^\top\mathbbm{1}_m=\bm{q}$. Since the indexes of paired keypoints across any two distribution in $\mathcal{P}^{\mathcal{X}}_{\mathcal{I}}$ are the same, we have for any $i_u\in\mathcal{I}$, the $i_u$-th row and column of $M^{\bm{p}\bm{r}}$ and $M^{\bm{r}\bm{q}}$ are zeros except for that $M^{\bm{p}\bm{r}}_{i_u,i_u} = M^{\bm{r}\bm{q}}_{i_u,i_u} =1$. So the $i_u$-th row and column of $\tilde{\gamma}$ are zeros except for $\tilde{\gamma}_{i_u,i_u}$.
Then, we can write $\tilde{\gamma}_{i_u,i_u}=M^{\bm{p}\bm{q}}\odot\gamma$ with $\gamma\in\mathbb{R}_+^{m\times m}$. Therefore, we have $\gamma\in\Pi(\bm{p},\bm{q};M^{\bm{p}\bm{q}})$.
% We denote $D^ = \alpha C + (1-\alpha)G$. We can verify that
% \begin{equation}
% \begin{split}
%         D_{i,j} =& \alpha C_{i,j}+ (1-\alpha)G_{i,j}
%         =\alpha c(x_i,y_j)+ (1-\alpha)d(R_i^s,R_j^t)\\
%         \leq&\alpha c(x_i,z_ky_j)+ (1-\alpha)d(R_i^s,R_j^t)
% \end{split}
% \end{equation}
The triangle inequality then follows from
\begin{equation}
    \begin{split}
       &\mathcal{S}_{krk}(\bm{p},\bm{q}) = \min_{\pi \in \Pi(\bm{p},\bm{q};M^{\bm{p}\bm{q}})}  \sum_{i,j}{M^{\bm{p}\bm{q}}_{i,j}\pi_{i,j}(\alpha c(x_i,y_j)+(1-\alpha)d(R^s_i,R^t_j))}\\
       \leq&\sum_{i,j}{\tilde{\gamma}_{i,j}(\alpha c(x_i,y_j)+(1-\alpha)d(R^s_i,R^t_j))}\\
    %   \leq&\sum_{i,k,j}{\tilde{\gamma}(\alpha (c(x_i,z_k)+c(z_k,y_j)) +(1-\alpha)(d(R^s_i,R^r_k)+d(R^r_k,R^t_j))}\\
       =&\sum_{i,j}(\alpha c(x_i,y_j) + (1-\alpha)d(R^s_i,R^t_j))
       \sum_{k}\frac{(M^{\bm{p}\bm{r}}\odot\pi^{\bm{p}\bm{r}})_{i,k}(M^{\bm{r}\bm{q}}\odot\pi^{\bm{r}\bm{q}})_{k,j}}{\tilde{r}_k}\\
    \end{split}
    \end{equation}
\begin{equation}
    \begin{split}
    %   (M^{\bm{p}\bm{r}}\odot\pi^{\bm{p}\bm{r}})\mbox{diag}\left(\frac{1}{\tilde{\bm{r}}}\right) (M^{\bm{r}\bm{q}}\odot\pi^{\bm{r}\bm{q}})]_{i,j}\\
    %   +&\sum_{i,k,j}(M^{\bm{p}\bm{r}}\odot\pi^{\bm{p}\bm{r}})\mbox{diag}\left(\frac{1}{\tilde{\bm{r}}}\right) (M^{\bm{r}\bm{q}}\odot\pi^{\bm{r}\bm{q}})]_{i,j}(\alpha c(z_k,y_j) + (1-\alpha)d(R^r_k,R^t_j))\\
       \leq&\sum_{i,k,j}(\alpha (c(x_i,z_k)+c(z_k,y_j)) +(1-\alpha)(d(R^s_i,R^r_k)+d(R^r_k,R^t_j))
       \frac{(M^{\bm{p}\bm{r}}\odot\pi^{\bm{p}\bm{r}})_{i,k}(M^{\bm{r}\bm{q}}\odot\pi^{\bm{r}\bm{q}})_{k,j}}{\tilde{r}_k}\\
       =&\sum_{i,k,j}(\alpha c(x_i,z_k) + (1-\alpha)d(R^s_i,R^r_k))\frac{(M^{\bm{p}\bm{r}}\odot\pi^{\bm{p}\bm{r}})_{i,k}(M^{\bm{r}\bm{q}}\odot\pi^{\bm{r}\bm{q}})_{k,j}}{\tilde{r}_k}\\
       +& \sum_{i,k,j}(\alpha c(z_k,y_j) + (1-\alpha)d(R^r_k,R^t_j))\frac{(M^{\bm{p}\bm{r}}\odot\pi^{\bm{p}\bm{r}})_{i,k}(M^{\bm{r}\bm{q}}\odot\pi^{\bm{r}\bm{q}})_{k,j}}{\tilde{r}_k}\\
       =& \sum_{i,k}(\alpha c(x_i,z_k) + (1-\alpha)d(R^s_i,R^r_k))(M^{\bm{p}\bm{r}}\odot\pi^{\bm{p}\bm{r}})_{i,k}\\
       +& \sum_{k,j}(\alpha c(z_k,y_j) + (1-\alpha)d(R^r_k,R^t_j))(M^{\bm{r}\bm{q}}\odot\pi^{\bm{r}\bm{q}})_{k,j}\\
       = &\mathcal{S}_{krk}(\bm{p},\bm{r}) + \mathcal{S}_{krk}(\bm{r},\bm{q}),
    \end{split}
\end{equation}
where $z_k$ is the support point of $\bm{r}$ and $R^r_k$ is the relation of $z_k$ to the keypoints of $\bm{r}$.

\section*{Appendix E: Proof of Theorem~\ref{thm:kpg_rl_gw}}
\noindent
{\bf Proof}.
(a) If $\bm{p}$ and $\bm{q}$ are isomorphic, for any $i\in [m]$ and any $i_u\in\mathcal{I}$, we have $c(x_i,x_{i_u})=c'(y_{\sigma(i)},y_{\sigma(i_u)})=c'(y_{\sigma(i)},y_{i_u})$, implying that $R_i^s=R_{\sigma(i)}^t$. We define $\pi$ as $\pi_{i,j}=p_i$ if $j=\sigma(i)$, otherwise 0.
We then have
\begin{equation}
    \begin{split}
        &\sum_{i,j}\alpha\Big(\sum_{k,l}(M^{\bm{p}\bm{q}}\odot\pi)_{i,j}(M^{\bm{p}\bm{q}}\odot\pi)_{k,l}|C^s_{i,k}-C^t_{j,l}|^2\Big)+(1-\alpha)(M^{\bm{p}\bm{q}}\odot\pi)_{i,j}G_{i,j}\\
        =&\sum_{i}\Big[\alpha\Big(\sum_{k}(M^{\bm{p}\bm{q}}\odot\pi)_{i,\sigma(i)}(M^{\bm{p}\bm{q}}\odot\pi)_{k,\sigma(k)}|C^s_{i,k}-C^t_{\sigma(i),\sigma(k)}|^2\Big)\\
        &\hspace{0.8cm}+(1-\alpha)(M^{\bm{p}\bm{q}}\odot\pi)_{i,\sigma(i)}G_{i,\sigma(i)}\Big]\\
        =&\sum_{i}\Big[\alpha\Big(\sum_{k}(M^{\bm{p}\bm{q}}\odot\pi)_{i,\sigma(i)}(M^{\bm{p}\bm{q}}\odot\pi)_{k,\sigma(k)}|c(x_i,x_k)-c'(x_{\sigma(i)},x_{\sigma(k)})|^2\Big)\\
        &\hspace{0.8cm}+(1-\alpha)(M^{\bm{p}\bm{q}}\odot\pi)_{i,\sigma(i)}d(R^s_i,R^t_{\sigma(i)})\Big] \\
        =& 0.
    \end{split}
\end{equation}
This implies $\mathcal{S}_{krg}(\bm{p},\bm{q})=0$.

(b) Let $(M^{\bm{p}\bm{q}})\odot \pi^*$ be the optimal transport plan corresponding to $\mathcal{S}_{krg}(\bm{p},\bm{q})$. If $\mathcal{S}_{krg}(\bm{p},\bm{q})=0$, we have \begin{equation}
    \sum_{i,j,k,l}(M^{\bm{p}\bm{q}}\odot\pi^*)_{i,j}(M^{\bm{p}\bm{q}}\odot\pi^*)_{k,l}|C^s_{i,k}-C^t_{j,l}|^2 = 0.
\end{equation}
This indicates that the Gromov-Wasserstein distance
\begin{equation}
\min_{\pi\in\Pi(\bm{p},\bm{q})}\sum_{i,j,k,l}\pi_{i,j}\pi_{k,l}|C^s_{i,k}-C^t_{j,l}|^2 = 0.
\end{equation}
By virtue to Gromov-Wasserstein properties in \cite{memoli2011gromov}, there exists a bijection $\sigma:[m]\longmapsto[m]$ such that $c(x_i,x_k)=c'(y_{\sigma(i)},y_{\sigma(k)})$, and $p_i=q_{\sigma(i)}$.

\section*{Appendix F: Proof of Theorem~\ref{thm:dual}}
\noindent
{\bf Proof}.
We rewrite the $L_2$-regularized model as
\begin{equation}
\begin{split}
    &\min_{\pi}\sum_{i,j}M_{i,j}\pi_{i,j}G_{i,j} + \epsilon \sum_{i,j}\frac{(M_{i,j}\pi_{i,j})^2}{p_iq_j} \\
    &\mbox{s.t. }\sum_{j}M_{i,j}\pi_{i,j} = p_i,\sum_{i}M_{i,j}\pi_{i,j} = q_j, \pi_{i,j}\geq 0.
\end{split}
\end{equation}
The Lagrange function is
\begin{equation}
\begin{split}
    L(\pi,\phi,\psi)
    =&\sum_{i,j}M_{i,j}(G_{i,j}-\phi(x_i)-\psi(y_j))\pi_{i,j} + \epsilon \sum_{i,j}\frac{M_{i,j}\pi_{i,j}^2}{p_iq_j}\\
    +& \sum_i\phi(x_i)p_i + \sum_{j}\psi(y_j)q_j,
\end{split}
\end{equation}where we utilize $M_{i,j}^2 = M_{i,j}$.
Minimizing $L(\pi,\phi,\psi)$ \textit{w.r.t.} $\pi$ ($\pi_{i,j}\geq 0$) is equivalent to
\begin{equation}
    \sum_{i,j} \min_{\pi_{i,j}\geq 0}\left\{M_{i,j}(G_{i,j}-\phi(x_i)-\psi(y_j))\pi_{i,j} + \epsilon\frac{M_{i,j}\pi_{i,j}^2}{p_iq_j}\right\}.
\end{equation}
If $M_{i,j} = 0$, the minimizer $\pi_{i,j}$ could be arbitrary non-negative value. If $M_{i,j} = 1$,
the minimizer is $\pi_{i,j} =\frac{1}{2\epsilon}(-G_{i,j}+\phi(x_i)+\psi(y_j))_+p_iq_j$, where $a_+=\max\{a,0\}$. Hence, $M_{i,j}\pi_{i,j} = \frac{1}{2\epsilon}M_{i,j}(-G_{i,j}+\phi(x_i)+\psi(y_j))_+p_iq_j.$
The dual problem is
\begin{equation}
    \begin{split}
    &\max_{\phi,\psi}\min_{\pi}L(\pi,\phi,\psi)\\
    =&\max_{\phi,\psi}  \sum_i\phi(x_i)p_i + \sum_{j}\psi(y_j)q_j -\frac{1}{2\epsilon}\sum_{i,j}M_{i,j}(-G_{i,j}+\phi(x_i)+\psi(y_j))_+^2p_iq_j\\
    &+\frac{1}{4\epsilon}\sum_{i,j}M_{i,j}(-G_{i,j}+\phi(x_i)+\psi(y_j))_+^2p_iq_j\\
    =&\max_{\phi,\psi}  \sum_i\phi(x_i)p_i + \sum_{j}\psi(y_j)q_j -\frac{1}{4\epsilon}\sum_{i,j}M_{i,j}(-G_{i,j}+\phi(x_i)+\psi(y_j))_+^2p_iq_j.
    \end{split}
\end{equation}
By the strong duality, the optimal solution $\phi^*, \psi^*$ satisfies
\begin{equation}
    (M\odot\pi^*)_{i,j} =\frac{1}{2\epsilon}M_{i,j}(-G_{i,j}+\phi^*(x_i)+\psi^*(y_j))_+p_iq_j.
\end{equation}

\section*{Appendix G: Implementation Details for I2I Translation Experiment}
The experiment consists of two steps. In the first step, we learn the potentials $\phi$ and $\psi$ based on Eq.~\eqref{eq:dual} and compute the transport plan as Eq.~\eqref{eq:prim_dual_solution}. In the second step, we learn the manifold barycentric projection $T_{MBP}$ by Eq.~\eqref{eq:gan_based}.
\subsection*{Appendix G1: Details for the First Step}
\paragraph{Architectures of the potentials $\bm{\phi}$ and $\bm{\psi}$ in Eq.~\eqref{eq:dual}.}
Both ${\phi}$ and ${\psi}$ consist of a feature extractor, followed by an output head. The output head is constructed as follows:
FC(1024,1024) $\rightarrow$ ReLU $\rightarrow$ FC(1024,1024) $\rightarrow$ ReLU $\rightarrow$ FC(1024,1024) $\rightarrow$ ReLU $\rightarrow$ FC(1024,1024) $\rightarrow$ ReLU $\rightarrow$ FC(1024,1),
where ``FC($a,b$)'' is the Fully-Connected layer with input/output channel of $a$/$b$. The feature extractor is pertained and fixed. For digits, we train autoencoders for source and target images respectively, and take the encoder part of the autoencoder as the feature extractor. The architecture of the encoder part is as follows: Conv(1,32,3,1) $\rightarrow$ GN(4) $\rightarrow$ ReLU $\rightarrow$ Conv(32,64,3,2) $\rightarrow$ GN(32) $\rightarrow$ ReLU $\rightarrow$ Conv(64,128,3,2) $\rightarrow$ GN(32) $\rightarrow$ ReLU $\rightarrow$ Conv(128,256,3,2) $\rightarrow$ GN(32) $\rightarrow$ L2-Norm. The architecture of the decoder part of the autoencoder is as follows: Tconv(256,128,3,2) $\rightarrow$ GN(32) $\rightarrow$ ReLU $\rightarrow$ Tconv(128,64,3,2) $\rightarrow$ GN(32) $\rightarrow$ ReLU $\rightarrow$ Tconv(64,32,3,2) $\rightarrow$ GN(32) $\rightarrow$ ReLU $\rightarrow$ Tconv(32,1,3,1) $\rightarrow$ Sigmoid. ``Conv($a,b,k,s$)'' and ``Tconv(a,b,k,s)'' are respectively the Convolutional layer and the Transposed-Convolutional layer, where $a$ and $b$ are the input and output channel respectively, the kernel size is $k\times k$, and the stride is $s$. ``GN($m$)'' is the Group Normalization layer with $m$ groups. ``L2-Norm'' is the $L_2$-nomalization. For the natural animal images, the feature extractor is taken as the image encoder (``ViT-B/32'') of CLIP~\citep{radford2021learning}.

\paragraph{Training details for learning the potentials $\bm{\phi}$ and $\bm{\psi}$ in Eq.~\eqref{eq:dual}.}
When training with Eq.~\eqref{eq:dual}, the optimization algorithm is Adam, the learning rate is 1e-5, and the batch size is 64. $\epsilon$ in Eq.~\eqref{eq:l2_reg_kpg} is set to 0.005.

% For Digits, we respectively train autoencoders for source and target domains, and then take the encoder part as the feature extractor. For animal images, we take the image encoder of CLIP as the feature extractor. We then learn the potential in the feature spaces. The architectures of the autoencoders and potentials are as follows:


% \textbf{Encoder}: Conv(1,32,3,1) $\rightarrow$ GN(4) $\rightarrow$ ReLU $\rightarrow$ Conv(32,64,3,2) $\rightarrow$ GN(32) $\rightarrow$ ReLU $\rightarrow$ Conv(64,128,3,2) $\rightarrow$ GN(32) $\rightarrow$ ReLU $\rightarrow$ Conv(128,256,3,2) $\rightarrow$ GN(32) $\rightarrow$ L2-Norm

% \textbf{Decoder}: Tconv(256,128,3,2) $\rightarrow$ GN(32) $\rightarrow$ ReLU $\rightarrow$ Tconv(128,64,3,2) $\rightarrow$ GN(32) $\rightarrow$ ReLU $\rightarrow$ Tconv(64,32,3,2) $\rightarrow$ GN(32) $\rightarrow$ ReLU $\rightarrow$ Tconv(32,1,3,1) $\rightarrow$ Sigmoid

% \textbf{Potentials $\bm{\phi}$, $\bm{\psi}$}: FC(1024,1024) $\rightarrow$ ReLU $\rightarrow$ FC(1024,1024) $\rightarrow$ ReLU $\rightarrow$ FC(1024,1024) $\rightarrow$ ReLU $\rightarrow$ FC(1024,1024) $\rightarrow$ ReLU $\rightarrow$ FC(1024,1)

% ``Conv($a,b,k,s$)'' and ``Tconv(a,b,k,s)'' are respectively the Convolutional layer and the Transposed-Convolutional layer, where $a$ and $b$ are the input and output channel respectively, the kernel size is $k\times k$, and the stride is $s$. ``GN($m$)'' is the Group Normalization layer with $m$ groups. ``L2-Norm'' is the $L_2$-nomalization. ``FC($a,b$)'' is the Fully-Connected layer with input/output channel of $a$/$b$. The inputs of the potentials are the 1024-dimensional features extracted by the encoder.
\subsection*{Appendix G2: Details for the Second Step}
\paragraph{Architectures of $\bm{T'}$ in Eq.~\eqref{eq:gan_based} and $\bm{D}$ in Eq.~\eqref{eq:loss_gan}.} For digits, the architectures of $T'$ is as follows: Conv(1,64,4,2) $\rightarrow$ BN $\rightarrow$ ReLU $\rightarrow$ Conv(64,128,4,2) $\rightarrow$ BN $\rightarrow$ ReLU $\rightarrow$ Conv(128, 128,3,1)  $\rightarrow$ BN  $\rightarrow$ ReLU $\rightarrow$ Conv(128,128,3,1) $\rightarrow$ BN $\rightarrow$ ReLU $\rightarrow$ Tconv(128,64,4,2) $\rightarrow$ BN $\rightarrow$ ReLU $\rightarrow$ Tconv(64,1,4,2)  $\rightarrow$ Sigmoid, where ``BN'' is the Batch Normalization layer.
The architectures of $D$ is as follows: Conv(1,64,4,2) $\rightarrow$ ReLU $\rightarrow$ Conv(64,128,4,2) $\rightarrow$ BN $\rightarrow$ ReLU $\rightarrow$ Conv(128,256,4,2) $\rightarrow$ BN $\rightarrow$ ReLU $\rightarrow$ Conv(256,1,3,0).

For natural animal images, we construct $T'$ and $D$ based on the architecture of the generator and discriminator of WGAN-QC~\citep{liu2019wasserstein}. The generator of WGAN-QC consists of a noise embedding layer and a resnet-based decoder. The discriminator of WGAN-QC consists of a resnet-based encoder and an FClayer. The architecture of $D$ is set to that of the discriminator of WGAN-QC.  $T'$ is constructed by stacking the resnet-based encoder and decoder. Please refer to the official code of WGAN-QC for the details of resnet-based encoder and decoder.

\paragraph{Training details for learning $\bm{T'}$.}
When training $T'$ with Eq.~\eqref{eq:gan_based}, the optimization algorithm is Adam, the learning rate is 1e-4, and the batch size is 64.  $\epsilon$ is set to 0.005.
% \noindent
% {\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
% not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
% dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
% which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
% respective empirical mutual information values based on the sample
% $\dataset$. Then
% \[
% 	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
% \]
% with equality only if $u$ is identically 0.} \hfill\BlackBox

% \noindent
% {\bf Proof}. We use the notation:
% \[
% P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
% P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
% \]
% These values represent the (empirical) probabilities of $v$
% taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
% by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

% {\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in
\bibliography{main}

\end{document}
