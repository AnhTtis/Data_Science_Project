\section{Proposed Method}
\label{sec:proposed_method}

We aim to recover the shape from calibrated and masked azimuth maps. Let $\Omega_i$ represent the $i$-th image pixel domain. For each view $i \in \{1, 2, ..., \cameraNum\}$, we assume the following are available:
\begin{itemize}
	\setlength{\itemsep}{0.2em}
	\setlength{\parskip}{0.2em}
	\item a surface azimuth map $\phi_i: \Omega_i \rightarrow [0, 2\pi]$,
	\item a binary mask indicating whether a pixel is inside the shape silhouette $\mask_i: \Omega_i \rightarrow \{0, 1\}$, and
	\item the projection from the world coordinates to the image pixel coordinates $\Pi_i: \R^3 \rightarrow \Omega_i$, consisting of the extrinsic rigid-body transformation $\V{P}_i = [\V{R}_i \mid \V{t}_i] \in SE(3)$ and intrinsic perspective camera projection $\V{K}_i$.
\end{itemize}

We describe the proposed method in three sections. First, we detail the transformation from an azimuth angle to a projected tangent vector (\cref{sec.azimuth2tangent}). Next, we discuss multi-view tangent space consistency for surface points, including its four degenerate scenarios and $\pi$-invariance (\cref{sec.functional}). Lastly, we present the surface reconstruction by optimizing a neural implicit representation based on the tangent space consistency loss (\cref{sec.sdf_optimization}).

\subsection{The projected tangent vector}
\label{sec.azimuth2tangent}
This section will show how to convert an azimuth angle to a tangent vector of the surface point, given the world-to-camera rotation. We will only consider single-view observations and ignore the view index in this section.

In the world coordinates, consider a unit normal vector $\V{n}(\V{x})\in \mathcal{S}^2 \subset \R^3$ of a surface point $\point \in \R^3$.
Suppose a rigid-body transformation $[\V{R} \mid \V{t}]$ transforms the surface from the world coordinates to the camera coordinates.
The direction of the normal vector in the camera coordinates  $\V{n}^c$ is rotated accordingly as
\begin{equation}
	\V{R}\V{n} = \V{n}^c.
	\label{eq.world_normal_to_camera_normal}
\end{equation}
In the camera coordinates, we can parameterize the unit normal vector by its azimuth angle $\phi \in [0, 2\pi]$ and \zenith angle~$\theta \in \left[0, {\pi \over2}\right)$ as
\begin{equation}
	\V{n}^c = \left[ 
	\begin{matrix}
		n_x^c \\
		n_y ^c\\
		n_z^c
	\end{matrix}
	\right] =
	\left[ \begin{matrix}
		\sin\theta \cos \phi \\
		\sin\theta \sin \phi \\
		\cos\theta \\
	\end{matrix}  \right].
	\label{eq.normal_parameterization}
\end{equation}
From~\cref{eq.normal_parameterization}, we can derive the relation between $n_x^c$ and $n_y^c$ in terms of only the azimuth angle as
\begin{equation}
	n_x^c \sin \phi = n_y^c \cos \phi.
	\label{eq.azimuth_ratio}
\end{equation}
Denoting the rotation matrix as 
\begin{equation}
	\V{R} = \left[\begin{matrix}
	-	\V{r}_1^\top - \\
	-	\V{r}_2^\top - \\
	-	\V{r}_3^\top -
	\end{matrix}\right] \in SO(3),
	\label{eq.rotation_matrix}
\end{equation}
and putting \cref{eq.world_normal_to_camera_normal,eq.normal_parameterization,eq.azimuth_ratio,eq.rotation_matrix} together, we obtain
\begin{equation}
	\V{r}_1^\top \V{n} \sin \phi = \V{r}_2^\top \V{n} \cos \phi.
	\label{eq:world_normal_azimuth}
\end{equation}
Rearranging \cref{eq:world_normal_azimuth} yields
\begin{equation}
	\V{n}^\top \underbrace{(\V{r}_1 \sin \phi - \V{r}_2 \cos \phi)}_{\V{t}(\phi)} = 0.
	\label{eq.normal_tangent}
\end{equation}
We call $\V{t}(\phi)$ the \emph{\projectedTangentVector}, as it is computed from the projected azimuth angle and perpendicular to the surface normal.
As shown in \Cref{fig.azimuth2tangent}, the transformation from azimuth maps to tangent maps reveals that projected tangent vectors encode camera orientation information, providing useful hints for multi-view reconstruction.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{images/method/azimuth_to_tangent.pdf}
	\caption{The azimuth angle observations \textbf{(left)} are lifted to tangent vectors \textbf{(right)} by world-to-camera rotations. 
	The tangent vectors are coded by 8-bit RGB colors using $255(\tangent+\V{1})/2$.}
	\label{fig.azimuth2tangent}
\end{figure}

\vspace{-1em}
\paragraph{Properties}
The \projectedTangentVector is the unit vector parallel to the intersection of the tangent and image spaces.
Based on \cref{eq.normal_tangent}, 
\begin{equation}
	\begin{aligned}
		\V{t}^\top\V{t} &= \V{r}_1 ^\top \V{r}_1  \sin^2\phi+\V{r}_2^\top \V{r}_2\cos^2\phi -2\V{r}_1^\top\V{r}_2\cos\phi \sin \phi\\
		&=  \sin^2\phi +  \cos^2\phi = 1,
	\end{aligned}
	\label{eq.tangent_unit_length}
\end{equation}
since $\V{r}_1$ and $\V{r}_2$ are orthonormal vectors.

\begin{wrapfigure}{r}[1em]{0.4\linewidth}
	\hspace{-3em}
	\vspace{-1em}
	\centering
	\includegraphics[width=\linewidth]{images/method/tangent_geometry.pdf}
	% \vspace{-1em}
\end{wrapfigure}
The inset illustrates the second property.
Let $\V{e}_x$, $\V{e}_y$, and $\V{e}_z$ be the unit direction vector of the $x$-, $y$-, and $z$-axis of the camera coordinates in the world coordinates.
Then $\V{r}_1 = \V{e}_x$ and $\V{r}_2 = \V{e}_y$, which follows that $\V{t}(\phi)$ is a linear combination of camera's $x$- and $y$-axes and thus parallel to the image plane.
We can compute the intersection direction of two planes by taking the cross-product of their normals, namely, the surface normal and the principle axis. Hence,
\begin{equation}
	\tangent \parallel \normal \times \opticalAxis .
	\label{eq.tangent_constraint}
\end{equation}
The two properties are helpful in analyzing the tangent space consistency, as described next.

\subsection{Multi-view tangent space consistency}
\label{sec.functional}
This section discusses the consistency between multi-view azimuth observations in the tangent space of a surface point.
In addition, four degenerate scenarios and $\pi$-invariance will be discussed.
We assume the surface point under consideration is visible to all cameras in this section.

Denote the \projectedTangentVector of a surface point in $i$-th view as $\tangent_{i}(\point)=\tangent(\phi_i(\Pi_i(\point)))$.
By \cref{eq.normal_tangent}, a surface point $\point$, its normal direction $\normal$, and its multi-view \projectedTangentVectors $\tangent_i$ should satisfy:
\begin{align}
	\normal(\point)^\top \tangent_i(\point) = 0 \quad \forall i.
	\label{eq.tsc}
\end{align}
Let
$
\stackedTangentVectors=[\V{t}_1(\point), \V{t}_2(\point), ..., \V{t}_\cameraNum(\point)]^\top \in \R^{\cameraNum \times 3}
$
be the matrix formed by stacking \projectedTangentVectors of all \cameraNum views. 
Then \cref{eq.tsc} reads
\begin{align}
	\stackedTangentVectors \normal(\point) = \V{0}.
	\label{eq.tsc_mat}
\end{align}
\Cref{eq.tsc_mat} can only be satisfied if the rank of \stackedTangentVectors is either 1 or 2. The rank cannot be $0$ as \projectedTangentVectors are unit length. The case rank$(\stackedTangentVectors)=3$ cannot satisfy \cref{eq.tsc_mat} as surface normals are non-zero vectors.

We refer to the case where the rank of \stackedTangentVectors is $2$ as \emph{tangent space consistency} (TSC). In this case, multi-view \projectedTangentVectors from a surface point span its tangent space, and the surface normal is determined up to a sign ambiguity. On the other hand, when rank$(\stackedTangentVectors)=1$, the \projectedTangentVectors can only span a tangent line and constrain the surface normal on the plane orthogonal to the tangent line. This can occur when camera optical axes are parallel, as explained later.

TSC can help distinguish non-surface points (wrong correspondences) from surface points (possibly correct correspondences) and determine the surface normals, as shown in \cref{fig.tangent_space_consistency}. For wrong correspondences, their \projectedTangentVectors are expected to have a rank of $3$ and span the entire 3D space. On the other hand, for surface points, their \projectedTangentVectors span the tangent space, i.e., rank$(\stackedTangentVectors)=2$. In addition, TSC requires the surface normal to be in the null space of \stackedTangentVectors, i.e., perpendicular to the tangent space spanned by \projectedTangentVectors. 
This makes TSC more informative than photo-consistency since photo-consistency cannot directly determine the surface normal.

To effectively distinguish surface/non-surface points using TSC, a non-planar surface must be observed by at least three cameras with non-parallel optical axes. These requirements indicate four degeneration scenarios, as shown in \cref{fig.degerated_cases} and discussed below. \Cref{tab.degeneration_tsc} summarizes the variations of rank$(\stackedTangentVectors)$ in these scenarios.
\vspace{-1em}
\paragraph{Number of viewpoints}
For TSC to be effective, the rank of $\stackedTangentVectors$ is expected to be $3$ for non-surface points. However, when only two views are available, the rank of  \stackedTangentVectors is impossible to achieve $3$ since $\stackedTangentVectors \in \R^{2\times 3}$. In this case, rank$(\stackedTangentVectors)\leq2$ is satisfied for arbitrary correspondence. Consequently, TSC cannot distinguish surface points from non-surface points in the two-view case.
\vspace{-1em}
\paragraph{Camera setups}
TSC requires the \projectedTangentVectors of a surface point can span the tangent space but not a tangent line.
This requirement breaks down when \projectedTangentVectors are observed from 
(1) frontal parallel cameras, or (2) cameras with coplanar optical axes.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{images/method/tsc}
	\caption{\textbf{(Left)} Multi-view \projectedTangentVectors from a surface point span its tangent space and determine the surface normal. \textbf{(Right) }Conversely, multi-view \projectedTangentVectors from a non-surface point (\ie, wrong correspondence) are expected to span the 3D space.}
	\label{fig.tangent_space_consistency}
\end{figure}

\begin{table}
	\centering
	\scriptsize
	\caption{Rank of \stackedTangentVectors in four degenerate cases of TSC. 
	}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{@{}lccc}
			\toprule
			Scenarios & Non-surface points & surface points & surface normal \\
			\midrule
			Two-view  & 2 & 2 & $\times$ \\
			Co-linear optical axes & 2 & 1 & $\times$\\
			Co-planar optical axes & 2 & 1 & $\triangle$ \\
			Planar surface & 2 & 2 &  $\checkmark$ \\
			\midrule
			TSC & 3 & 2 & $\checkmark$ \\
			\bottomrule
		\end{tabular}
	}
	\label{tab.degeneration_tsc}
	\vspace{-1.5em}
\end{table}

\begin{figure}[t]
	\centering
	%	\begin{tabular}{@{}c@{}c@{}}
		\includegraphics[width=\linewidth]{images/method/degenerated_cases}
		%	\end{tabular}
	\caption{Degeneration scenarios where TSC cannot distinguish good correspondence from bad ones. \textbf{(Top Left)} Two-view observations,  \textbf{(Top Right)} frontal parallel cameras with parallel optical axes (red pins), \textbf{(Bottom Left)} cameras with coplanar optical axes observe coplanar surface normals, and  \textbf{(Bottom Right)} planar surface regions. }
	\label{fig.degerated_cases}
\end{figure}

Frontal parallel cameras have parallel optical axes. 
By  \cref{eq.tangent_constraint}, multi-view projected tangent vectors of a surface point also become parallel. 
This reduces the rank of \stackedTangentVectors to 1, and TSC degrades to photo-consistency since all cameras should observe the same tangent vector for a surface point.

A more special case is when cameras with coplanar optical axes observe coplanar surface normals, such as a rotating camera observing a cylinder. In this case, the cross product of the coplanar normal and optical axis vectors yields co-linear projected tangent vectors. As such, the rank of \stackedTangentVectors is 1 for surface points, and TSC again degrades to photo-consistency. However, this degradation does not occur for non-coplanar surface normals, meaning TSC can still be effective for general surfaces.

\vspace{-1em}
\paragraph{Surface types} 
TSC breaks down for a planar surface.
At any location on the planar surface, $\normal(\point)$ is the same and rank$(\stackedTangentVectors
)$ is identically $2$ for arbitrary correspondence.
However, the normal direction of this plane can still be correctly determined in the case rank$(\stackedTangentVectors) = 2$, \ie, at least three non-frontal parallel views.
The planar surface can be seen as the counterpart to the textureless region for photo-consistency. 
However, unlike photo-consistency, TSC can still determine the surface normal\footnote{A similar phenomenon exists in Helmholtz stereopsis~\cite{zickler2002helmholtz}, where wrong correspondence might still result in the correct normal estimation.}.
\vspace{-1em}
\paragraph{$\pi$-invariance} 
TSC remains effective when the azimuth angle is changed by $\pi$.
By~\cref{eq.normal_tangent}, the sign of the projected tangent vector will be reversed:
\begin{equation}
	\tangent(\phi + \pi) = - \V{r}_1 \sin \phi + \V{r}_2 \cos \phi = -\tangent(\phi).
\end{equation}
Intuitively, reversing the direction of a tangent vector still places it in the same tangent space, as $\normal ^\top (-\tangent)= 0$ when $\normal ^\top \tangent=0$.
Mathematically, reversing the signs of arbitrary rows in \stackedTangentVectors does not affect the rank of \stackedTangentVectors. This $\pi$-invariance can be particularly useful for polarization imaging, as they can only measure azimuth angles up to a $\pi$ ambiguity.

\subsection{Multi-view azimuth stereo}
\label{sec.sdf_optimization}
We propose the following TSC-based functional for multi-view geometry reconstruction:
\begin{equation}
	\mathcal{J} = \oiint_{\surface} \frac{\sum_{i=1}^{\cameraNum} \visibility(\point) \left(\normal(\point)^\top\tangent_i(\point)\right)^2}{\sum_{i=1}^{\cameraNum}\visibility(\point)} \, d\surface.
	\label{eq.functional_long}
\end{equation}
Here, \surface is the surface embedded in the 3D space, and $d\surface$ is the infinitesimal area on the surface. 
$\visibility(\point)$ is a binary function indicating the visibility of the point \point from the $i$-th viewpoint:
\begin{equation}
	\visibility(\point) = \begin{cases}
		1 \quad \text{if \point is visible to $i$-th camera} \\
		0 \quad \text{otherwise}
	\end{cases}.
\end{equation}
We can simplify \cref{eq.functional_long} as follows:
\begin{equation}
	\mathcal{J} = \oiint_{\surface} \normal^\top \tilde{\V{T}}\normal \, d\surface \quad \text{with} \quad \tilde{\V{T}} = \frac{\sum_{i=1}^{\cameraNum}\visibility \tangent_{i}\tangent_{i}^\top}{\sum_{i=1}^{\cameraNum}\visibility},
	\label{eq.functional_simple}
\end{equation}
where we omit the dependence on the surface point \point for clarity.
As discussed in \cref{sec.functional},  accurate surface points and normals are both necessary to minimize the functional.

We represent the surface implicitly using a signed distance function (SDF) and optimize the SDF based on the framework of implicit differentiable renderer (IDR)~\cite{idr2020multiview}.
We parameterize the SDF by a multi-layer perceptron~(MLP) as~$f(\V{x};\Vg{\theta}):\R^3\times \R^{d}\rightarrow \R$, where $\point \in \R^3$ is the 3D point coordinate, and $\Vg{\theta} \in \R^d$ are MLP parameters.
The surface \surface is implicitly represented as the zero-level set of the SDF
\begin{equation}
	\surface(\Vg{\theta}) = \{\V{x} \mid f(\V{x}; \Vg{\theta})=0 \},
\end{equation} 
which varies depending on the MLP parameters.

To optimize the MLP, we use a loss function that consists of the tangent space consistency loss, the silhouette loss, and the Eikonal regularization:

\begin{equation}
	\loss = \loss_{\textrm{TSC}} + \lambda_1 \loss_{\textrm{silhouette}} + \lambda_2 \loss_{\textrm{Eikonal}}.
	\label{eq.total_loss}
\end{equation}

In each batch of the optimization, we randomly sample a set of \batchsize pixels from all views, cast camera rays from these pixels into the scene, and find the first ray-surface intersections. We evaluate the TSC loss for pixels with ray-surface intersections located inside the silhouette, denoted as $\V{X}$. We evaluate the silhouette loss for pixels that do not have ray-surface intersections or are located outside the silhouette, denoted as $\V{\tilde{X}}$.

\input{sections/figures_tables/TSC_loss.tex}
\vspace{-1em}
\paragraph{Tangent space consistency loss}
Based on \cref{eq.functional_simple}, we define the TSC loss as
\begin{equation}
	\loss_{\textrm{TSC}} = \frac{1}{\batchsize} \sum_{\point \in \V{X}} \normal(\point;\Vg{\theta})^\top \tilde{\V{T}}(\point) \normal(\point;\Vg{\theta}).
\end{equation}
To evaluate the TSC loss, we need to evaluate the surface normal and construct the matrix $\tilde{\V{T}}$.
According to the property of SDF~\cite{osher2004level}, the surface normal direction is the gradient evaluated at a zero-level set point:
\begin{equation}
	\V{n}(\V{x};\Vg{\theta}) = \nabla_{\V{x}}f(\V{x};\Vg{\theta}).
\end{equation}
Here, the surface normal can still be represented analytically as the MLP parameters~\cite{igr2020icml,siren2020sitzmann}.
Therefore, the gradient of the loss functions can be backpropagated to MLP parameters via surface normals.

We then compute $\V{T}(\point)$ for the point \point from all visible views. 
First, we project the surface points onto all views and check their visibility in each, as shown in \cref{fig.neuralSDF_optimization}.
To determine the visibility, we march the surface points toward the camera center and check whether there is a negative distance on the ray; see the supplementary material for more details.
Then in visible views, we compute the projected tangent vectors from input azimuth maps.

\vspace{-1em}
\paragraph{Silhouette loss}
Following IDR~\cite{idr2020multiview}, we use the input masks to constrain the visual hull of the shape\footnote{IDR~\cite{idr2020multiview} refers to it as mask loss, but we prefer to use ``silhouette loss'' after shape-from-silhouette~\cite{visualhull1994}.}.
We find the minimal distance on the rays for pixels that do not have ray-surface intersections, denoted as $f^*$.
The silhouette loss is then
\begin{equation}
	\loss_{\textrm{silhouette}} = \frac{1}{\alpha \batchsize} \sum_{\point \in \V{\tilde{X}}} \Psi \bigl(\mask(\Pi(\point)), \sigma(\alpha f^*)\bigr),
\label{eq.silhouette_loss}
\end{equation}
where $\Psi$ is the cross entropy function, and $\sigma(\cdot)$ is a sigmoid function with $\alpha$ controlling its sharpness.
\vspace{-1em}
\paragraph{Eikonal regularization}
Following IGR~\cite{igr2020icml}, we use the Eikonal loss to regularize the gradient of SDF such that the gradient norm is close to $1$ everywhere~\cite{osher2004level}:
\begin{equation}
	\loss_{\textrm{Eikonal}} = \mathbb{E}_\point \left( \left(\norm{\V{n}}_2 - 1\right)^2 \right).
\end{equation}
To apply Eikonal regularization, we randomly sample points within the object bounding box and compute the mean squared deviation from $1$-norm.

None of the three loss functions explicitly constrain the surface points.
It is the TSC loss that implicitly encourages good correspondence.