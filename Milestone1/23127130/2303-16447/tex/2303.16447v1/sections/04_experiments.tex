\section{Experiments}
\label{sec.experiments}
We evaluate \mvas in three experiments: comparing with MVPS methods quantitatively for surface and normal reconstruction in \cref{sec.exp_mvps}, applying \mvas to a photometric stereo method which struggles with \zenith estimation in \cref{sec.exp_symps}, and using \mvas with passive polarization imaging in \cref{sec.exp_polar}.
Implementation details are in the supplementary material.

\subsection{\mvas versus MVPS}
\label{sec.exp_mvps}
\paragraph{Baselines} 
We assess \mvas against multiple MVPS methods using the \diligentmv benchmark~\cite{li2020multi}. The MVPS methods include the coarse mesh refinement method \rmvps~\cite{park2016robust}, the benchmark method \bmvps~\cite{li2020multi}, the depth-normal fusion-based method \uanet~\cite{kaya2022uncertainty}, and the neural inverse rendering method \psnerf~\cite{yang2022psnerf}. \diligentmv~\cite{li2020multi} captures $20$ views under $96$ different lights for five objects. We use $15$-view azimuth maps for optimization and leave out $5$ views for testing, following \psnerf~\cite{yang2022psnerf}. The azimuth maps are computed from the normal maps estimated by the self-calibrated photometric stereo method \sdps~\cite{chen2019SDPS_Net}. 
\vspace{-1em}
\paragraph{Evaluation metrics} 
We use Chamfer distance (CD) and F-score for geometry accuracy~\cite{kaya2022uncertainty,knapitsch2017tanks}, and mean angular error (MAE) for normal accuracy~\cite{yang2022psnerf}. 
For CD and F-score, we only consider visible points by casting rays for all pixels and finding the first ray-mesh intersections\footnote{Different strategies for computing CD yield different results to the original papers. \uanet crops the invisible bottom face and uses mesh vertices~\cite{kaya2022uncertainty}; \psnerf~\cite{yang2022psnerf} samples $10000$ points from the mesh surface. }.
\vspace{-1em}
\paragraph{Results and discussions}
\Cref{tab.chamfer_mvps_kaya} reports the geometry accuracy of the recovered \diligentmv surfaces.
\bmvps~\cite{li2020multi} achieves the best scores in $4$ objects due to the usage of calibrated light information.
\uanet~\cite{kaya2022uncertainty} distorts the surface reconstruction by not considering the multi-view consistency.
\mvas outperforms \psnerf~\cite{yang2022psnerf} in $3$ objects without modeling the rendering process.

\Cref{fig.vis_comp_mvps} visually compares recovered ``Buddha'' and ``Reading'' objects.
Despite not having the best numerical scores, our method produces comparable results.
Lower scores for these objects are mainly due to our method's sensitivity to inaccurate silhouette masks provided by \diligentmv~\cite{li2020multi}.
We project the GT surface onto the image plane and find up to $10$-pixel inconsistency between the projected region and the GT mask.
Thus, the silhouette loss \cref{eq.silhouette_loss} encourages our reconstructed surfaces to shrink to align with the smaller silhouettes.

Our method requires less effort for shape recovery than \bmvps~\cite{li2020multi} and \psnerf~\cite{yang2022psnerf}. 
While \bmvps~\cite{li2020multi} calibrates $96$ light directions and intensities, we use a self-calibrated PS method for input azimuth maps. 
\psnerf~\cite{yang2022psnerf} uses $15$ view $\times$ $96$ light $=1440$ images to optimize multiple MLPs that model shape and appearance, which requires a high computational cost.
It takes \psnerf~\cite{yang2022psnerf} over $20$ hours per object on an RTX 3090 GPU.
In contrast, our approach optimizes a single MLP with $15$ azimuth maps, taking approximately $3$ hours per object on an RTX 2080Ti GPU.

\Cref{tab.mae_normal} reports MAE for $5$ test and all $20$ viewpoints, and \cref{fig.comp_mvps_normal} visually compares recovered normal maps. \mvas improves normal accuracy compared to \sdps~\cite{chen2019SDPS_Net} and outperforms \psnerf~\cite{yang2022psnerf} in $4$ objects, demonstrating TSC's effectiveness in constraining surface normals from multi-view observations.
Since \tsc imposes a direct constraint on surface normals, it is more effective than modeling a rendering process as in \psnerf~\cite{yang2022psnerf}.

\input{sections/figures_tables/cd_mvps}
\input{sections/figures_tables/compare_mvps}
\input{sections/figures_tables/mae_normal.tex}
\input{sections/figures_tables/visualization_normal.tex}
\input{sections/figures_tables/symps_setup.tex}
\input{sections/figures_tables/mvs_comparison}
\input{sections/figures_tables/pandora_horizontal}

\subsection{\mvas for symmetric-light photometric stereo}
\label{sec.exp_symps}
Some photometric stereo methods can estimate azimuth angles well but struggle with zenith angles\cite{chandraker2012differential, minami2022symmetric}. This section shows how \mvas can be used for an uncalibrated photometric stereo setup to eliminate the need for tedious zenith estimation while allowing full surface reconstruction. 

We use the setup shown in \cref{fig.symps_setup} to obtain multi-view azimuth maps.
We place four lights symmetrically around the camera and the target object on a rotation table.
In each view, we capture one ambient-light image and four lit images.
The ambient-light images are used for SfM~\cite{schoenberger2016sfm} to obtain the camera poses and are input to MVS~\cite{schoenberger2016mvs} for comparison.
Using the four lit images, The azimuth angles can be trivially computed from the ratio of the vertical to the horizontal difference image~\cite{minami2022symmetric}.

\Cref{fig.mvs_comparison} compares reconstructed surfaces and normals by Colmap~\cite{schoenberger2016mvs} and \mvas.
The first object shows a scene with challenging white planar faces. Photo-consistency-based MVS fails to recover the textureless region, while \tsc succeeds in the planar region. This is possibly due to that \tsc can still determine surface normals with wrong correspondences in a planar region, as discussed in~\cref{sec.functional}.
The second object has a dark surface, which is also challenging for photo-consistency, and Colmap~\cite{schoenberger2016mvs} struggles to recover the correct surface normals.

\subsection{\mvas with polarization imaging}
\label{sec.exp_polar}
This section shows the application of \mvas on azimuth maps obtained passively by a snapshot polarization camera, which makes the capture process as simple as MVS.
Since \tsc is $\pi$-invariant, \mvas eliminates the need to correct the $\pi$-ambiguity~\cite{miyazaki2003polarization}.
\Cref{fig.pandora} compares the surface and normal reconstruction on the multi-view polarization image dataset~\cite{dave2022pandora}.
We input the color images into Colmap~\cite{schoenberger2016mvs} and reproduce the results of the polarimetric inverse rendering method PANDORA using their codes~\cite{dave2022pandora}.
We modify our TSC loss to account for \halfpi ambiguity in polar-azimuth maps; see the supplementary material for details.

As shown in \cref{fig.pandora}, MVS~\cite{schoenberger2016mvs} breaks down for highly specular objects.
Polar-azimuth observations are robust to such specularity and allow \mvas for faithful reconstruction.
The comparison to PANDORA~\cite{dave2022pandora} shows that surfaces can be recovered without considering the degree of polarization or reflectance-light modeling.