%%%%%%%%% TITLE - PLEASE UPDATE
\begin{appendices}
\label{appendices}
	
\setcounter{section}{0}
\setcounter{figure}{10}
\setcounter{equation}{20}
\setcounter{table}{2}
\renewcommand{\thesection}{\Alph{section}}

We provide more details and analysis of the proposed method, as listed below.
% Start a partial table of contents
\startcontents

% Insert the partial table of contents
{
	\hypersetup{linkcolor=black}
	\printcontents{}{1}{}
}\
\section{Analysis of \tsc loss}
This section provides more details about our modification for \tsc loss to account for the \halfpi ambiguity in polarimetric azimuth observations, discusses the necessity of considering multi-view consistency, and provides more details and an efficiency analysis of our visibility determination strategy.
	
\subsection{Accounting for \halfpi ambiguity in \tsc loss}
We modify our \tsc loss to account for \halfpi ambiguity in polarimetric observations.
Given an observed polarimetric phase angle \phaseangle , the surface azimuth angle \azimuthangle is either \phaseangle \halfpi or $\phaseangle (=  \phaseangle+ \pi)$
depending on whether the surface point is polarimetric specular or diffuse reflection dominated~\cite{Cui_2017_CVPR,miyazaki2003polarization}.
Unfortunately, labeling the specular or diffuse domination is non-trivial~\cite{Cui_2017_CVPR, zhao2020polarimetric, dave2022pandora, zhu2019depth}.
In our approach, although \tsc is invariant to $\pi$ ambiguity, the \halfpi ambiguity still requires specific handling for polarimetric observations.
	
	\input{sections/figures_tables/supp_tsc_halfpi_comparison}
	
	Our idea is to allow both possibilities in the TSC loss.
	The \halfpi ambiguity introduces one more candidate tangent vector, and the surface normal should be perpendicular to either of the vectors deduced from $\pi$ or $\halfpi$ phase angles.
	By main paper's Eq.~(6), the projected tangent vector $\tangent^{\prime}$ from the \halfpi phase angle is
	\begin{equation}\label{eq.halfpi_tangent}
		\begin{aligned}
			\tangent^{\prime}(\azimuthangle) =\tangent\left(\phaseangle + \frac{\pi}{2}\right) &= \V{r}_1 \sin\left(\phaseangle+\frac{\pi}{2}\right) - \V{r}_2\cos\left(\phaseangle+\frac{\pi}{2}\right)  \\
			& = -\V{r}_1 \cos\left(\phaseangle\right) - \V{r}_2 \sin
		\left(\phaseangle\right).
		\end{aligned}
	\end{equation}
	Because $\tangent^{\prime}$ is also parallel to the image plane, $\tangent^{\prime}$  can be obtained by rotating \tangent by \halfpi in the image plane.
	At this point, however, we cannot fully determine which vector, \tangent or $\tangent^{\prime}$, is the actual tangent vector.
	We only know that the surface normal is perpendicular to either of the vectors:
	\begin{align}\label{eq.halfpi_tangent_normal}
		\normal \perp \tangent \quad \text{or} \quad \normal \perp \tangent^{\prime}.
	\end{align}
	Putting together the notations in the main paper's Eqs.~(12) and (17), we can rewrite our TSC loss as
	\begin{align}\label{eq.tsc_loss_extended}
		\loss_{\textrm{TSC}} = 
		\frac{1}{\batchsize} \sum_{\point \in \V{X}} \frac{\sum_{i=1}^{\cameraNum} \visibility \left(\normal^\top\tangent_i\right)^2}{\sum_{i=1}^{\cameraNum}\visibility}.
	\end{align}
	Based on \cref{eq.halfpi_tangent_normal}, we modify \cref{eq.tsc_loss_extended} as 
	\begin{align}
		\loss_{\textrm{TSC}}^{\prime} = 
		\frac{1}{\batchsize}\sum_{\point \in \V{X}} \frac{\sum_{i=1}^{\cameraNum} \visibility \left(\normal^\top\tangent_i\right)^2\left(\normal^\top\tangent^{\prime}_{i}\right)^2}{\sum_{i=1}^{\cameraNum}\visibility}.
	\end{align}
The modified \tsc loss allows the surface normal to be perpendicular to either of the two candidate tangent vectors.
	
\Cref{fig.tsc_halfpi} shows that this strategy yields better reconstruction quality, which gives us the results presented in the main paper's Fig.~10.
If we do not deal with \halfpi ambiguity, the recovered shapes appear twisted due to wrong tangent vectors (\ie, rotated by \halfpi from actual tangent vectors in the image space).
	
\input{sections/figures_tables/ablation}
\subsection{Ablation study on multi-view consistency}
Accumulating \projectedTangentVectors from all visible views  to compute the \tsc loss is necessary for accurate shape recovery.
Without considering multi-view consistency, we can simplify our original \tsc loss from \cref{eq.tsc_loss_extended} to
\begin{align}
	\loss_{\textrm{TSC}}^{''} = 
	\frac{1}{\batchsize} \sum_{\point \in \V{X}} (\normal(\point)^\top\tangent(\azimuthangle(\Pi(\point))))^2,
\label{eq.tsc_simplified}
\end{align}
where the \projectedTangentVector \tangent is computed from the input pixel location, and visibility or tangent vectors in other views need no longer be considered.

This simplified loss \cref{eq.tsc_simplified}, however, can lead to convex-concave ambiguity in the recovered surfaces, as shown in \cref{fig.ablation_on_reprojection}.
Without multi-view consistency, the tangent vector from one view can only constrain the surface normal loosely on a plane and cannot constrain the surface positions correctly.
Therefore, locally concave or convex surfaces with the same tangent vectors can both minimize the simplified loss, thus resulting in the ambiguity.
	
\subsection{More details on visibility determination}
We determine the visibility of a surface point in a view by marching the point toward the corresponding camera, \ie, performing sphere tracing~\cite{sphere1996hart} in the reverse direction.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{images/method/visibility_check}
	\caption{Visibility determination via reverse sphere tracing. We march a surface point $\point_0$ towards the camera center. At each step, the marching distance is the signed distance $f(\point_t)$ from the current point $\point_t$ to the surface, which requires one MLP evaluation. \textbf{(Left)} The marching diverges quickly towards the camera if $\point_0$ is visible. \textbf{(Right)} The marching converges to another surface point as ordinary sphere tracing~\cite{sphere1996hart} if $\point_0$ is occluded.}
	\label{fig.reverse_sphere_tracing}
\end{figure}

\begin{figure}[t]
	\centering
	\begin{tabular}{cc}
		\includegraphics[height=0.4\linewidth]{images/vis_distribution.pdf} &
		\includegraphics[height=0.4\linewidth]{images/shape_vis/buddha/draw_time_2022_10_16_14_31_28/2022_10_16_01_23_08}
	\end{tabular}
	\caption{
		The distribution of marching steps required to determine the visibility of surface points of the \diligentmv object ``Buddha''~\cite{li2020multi}.
		On average, $16$ MLP evaluations are required per surface point per view over the training.
	}
	\label{fig.vis_distribution}
\end{figure}

We consider four conditions when marching the surface point. % toward the camera.
Initially, we push the surface point $\point_0$ by a tiny distance ($1\times 10^{-3}$ in our experiments) to the camera.
(1) The surface point is invisible if the signed distance becomes negative, as the marching direction is towards inside the surface.
As long as the marching point is outside the surface, we move the point $\point_t$ at step $t$ by a distance $f(\point_t)$ towards the camera.
The surface point is (2) visible if the marching point goes beyond the camera center (\cref{fig.reverse_sphere_tracing} left) or (3) invisible if the marching point hits another surface point (\cref{fig.reverse_sphere_tracing} right). 
(4) We treat the surface point as invisible if the marching is not terminated within certain steps.

This strategy is advantageous in both efficiency and accuracy compared to other visibility determination strategies used in neural rendering methods.
First, it avoids densely evaluating an MLP on the point-to-camera rays~\cite{nerfactor2021,facerelighting2022}.
The marching quickly terminates and only requires a few MLP evaluations, \eg, $16$ MLP evaluations on average ( \cref{fig.vis_distribution}).
Second, it does not rely on the visibility predicted by an additional trainable MLP~\cite{yang2022psnerf}.

\section{Evaluation on \diligentmv}
This section provides more details of our evaluation metrics, additional visual comparisons on \diligentmv benchmark~\cite{li2020multi}, and investigates the effect of number of input viewpoints.

\subsection{More details on evaluation metrics}
The definition of our evaluation metrics follow~\cite{kaya2022uncertainty,knapitsch2017tanks}.
We present their definitions here for completeness.
\paragraph{Chamfer distance}
Chamfer distance measures the point-set-to-point-set distance by accumulating the point-to-point-set distances.
Given two point sets \pointsetOne, and \pointsetTwo, the distance from a point  to another point set is defined as
\begin{equation}
	\begin{aligned}
	&d_{\pointOne \rightarrow \pointsetTwo} = \min_{\pointTwo \in \pointsetTwo} \norm{\pointOne - \pointTwo}_2 \quad \text{and} \\ 
	&d_{\pointTwo \rightarrow \pointsetOne} = \min_{\pointOne \in \pointsetOne} \norm{\pointOne - \pointTwo}_2.
	\end{aligned}
\end{equation}
The Chamfer distance \chamferDist is then 
\begin{align}
	\chamferDist = \frac{1}{2\abs{\pointsetOne}} \sum_{\pointOne \in \pointsetOne} d_{\pointOne \rightarrow \pointsetTwo} + \frac{1}{2\abs{\pointsetTwo}} \sum_{\pointTwo \in \pointsetTwo} d_{\pointTwo \rightarrow \pointsetOne}.
\end{align}

\paragraph{F-score}
F-score considers both the precision and recall of the recovered surfaces to the GT surfaces.
The precision and recall are defined based on the point-to-point-set distances as
\begin{equation}
	\begin{aligned}
	\precision &=  \frac{1}{\abs{\pointsetOne}} \sum_{\pointOne \in \pointsetOne}[d_{\pointOne \rightarrow \pointsetTwo} < \fscoreThreshold]  \quad \text{and}\\
	\recall &=  \frac{1}{\abs{\pointsetTwo}} \sum_{\pointTwo \in \pointsetTwo}[d_{\pointTwo \rightarrow \pointsetOne} < \fscoreThreshold].
	\end{aligned}
\end{equation}
Here, $[\cdot]$ is the Iverson bracket, and \fscoreThreshold is the distance threshold for a point to be considered close enough to a point set.
The F-score then takes the geometric average of precision and recall:
\begin{align}
	\fscore = \frac{2\precision \recall}{\precision + \recall}.
\end{align}
We set $\fscoreThreshold=\SI{0.5}{mm}$ in our evaluations.

As mentioned in the main paper, our evaluation takes the first ray-surface intersection points from all views as the input point sets to the Chamfer distance and F-score.
This puts more focus on evaluating visible surface regions in input images and avoids a heuristic crop of the surface~\cite{kaya2022uncertainty}.

\input{sections/figures_tables/diligent_inner}

\input{sections/figures_tables/diligent_shape_supp}

\input{sections/figures_tables/diligent_normal_supp}

Our evaluation metrics do not consider the \emph{cleanness} of inner space (\ie, correctness of inner topology) of the recovered surfaces.
To assess how accurate the inner space of the surfaces is, we visualize the inner space of the mesh in \cref{fig.diligent_inner}.
The visualization shows that our method does not produce unwilling structures inside recovered meshes.

\subsection{Additional visual comparisons}

\Cref{fig.vis_comp_mvps_supp,fig.comp_mvps_normal_supp} show the visual comparisons on \diligentmv objects~\cite{li2020multi} in addition to the ones presented in the main paper's Figs.~(7) and (8). 
Our method consistently recovers accurate and detailed shapes and normals.

\input{sections/figures_tables/diligent_normal_unseen}

\Cref{fig.comp_mvps_normal_supp_unseen} shows the comparison of surface normals to \psnerf~\cite{yang2022psnerf} from the $5$ unseen viewpoints during the training.
\psnerf~\cite{yang2022psnerf} use the $15$-view SDPS normal maps~\cite{chen2019SDPS_Net} to initialize shapes, therefore sharing the same access to underlying azimuth information as ours.
The comparison verifies that accurate shape and normal recovery can be realized using only azimuth maps without developing the rendering process for the multi-view case. 

\subsection{The effect of number of viewpoints}

\input{sections/figures_tables/diligent_num_views_quan}
\input{sections/figures_tables/diligent_num_views}
\mvas is robust to sparse view input.
As shown in \cref{tab.num_view} and \cref{fig.diligent_num_views}, we evaluate the shape and normal recovery accuracy by gradually reducing the number of input views.
\Cref{fig.diligent_num_views} shows that using as few as $5$-view azimuth maps can still achieve detailed reconstruction, while large errors are observed mainly at heavily occluded regions.


\section{Implementation details}
\label{sec.implementation_details}
This section describes the architecture of our neural SDF, the training details, and the camera normalization process.
\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{images/method/mlp_architecture}
	\caption{Our network consists of a positional encoding layer~$\gamma(\cdot)$ and an $8$-layer MLP with softplus activation functions. A skip connection is added to the $4$-th layer from the input. This is the only network we optimize.}
	\label{fig.mlp_architecture}
\end{figure}

\subsection{Neural network architecture}
Following IDR~\cite{idr2020multiview}, our neural SDF consists of a positional encoding layer~\cite{mildenhall2020nerf} followed by an $8$-layer MLP, as shown in \cref{fig.mlp_architecture}.
The positional encoding layer is defined as
\begin{equation}
	\begin{aligned}
		\gamma(\point) = [ \sin(2^0 \pi \point),& \cos(2^0\pi \point), ..., \\
		&\sin(2^{L-1} \pi \point), \cos(2^{L-1}\pi \point) ] .
	\end{aligned}
\end{equation}
We use $L=10$ in our experiments.
The input position \point and $\gamma(\point)$ are skip-connected to the $4$-th layer of the MLP.
For the activation functions in the MLP, we use the softplus function
\begin{align}
	\textrm{softplus}(x) = \frac{1}{\beta} \log\left(1+\exp(\beta x)\right)
\end{align}
with $\beta=100$.

The neural SDF shown in \cref{fig.mlp_architecture} is the only MLP we optimize.
Unlike recent works using additional rendering networks to model surface light field~\cite{idr2020multiview,volsdf2021yariv} or reflectance~\cite{yang2022psnerf} for computing re-rendering loss, multi-view azimuth maps directly regularize the geometry and eliminate the necessity to model a rendering process.

\input{sections/paragraph_implementation_details}


\subsection{Camera normalization}
Following \volsdf~\cite{volsdf2021yariv},  we normalize the world coordinates such that the object is bounded by a unit sphere.
As we cannot know the shape and its center position beforehand,  we approximate the object center location by the position that is closet to all camera principle axes.
This approximation assumes all cameras surrounding the target scene and is satisfied in our experiments.
We present the computation details here because we do not find such details in the \volsdf paper~\cite{volsdf2021yariv}.
The normalization is done by shifting and then scaling the camera center locations:
\begin{equation}
	\V{o}_i \leftarrow \frac{\V{o}_i - \V{x}_o}{s}.
\end{equation}
Here, $\V{o}_i$ is the $i$-th camera's center location in the world coordinates, $\V{x}_o$ and $s$ are the global offset and scale factor to be detailed in the following.

\paragraph{Camera centers' offset}
The offset applied to all camera center locations can be computed using a linear system.
Formally, let $\V{o}_i \in \R^3$ and $\V{z}_i \in \mathcal{S}^2 \subset \R^3$ be the \mbox{$i$-th} camera's center location and its principle axis direction in the world coordinates, respectively.
The principle axis can then be represented as $\point_i(t) = \V{o}_i + t\V{z}_i $ with $t \in \R_+$.
The shortest squared Euclidean distance from a point $\point \in \R^3$ to this principle axis is
\begin{equation}
	\begin{aligned}
	d^2\left(\point, \point_i(t)\right)	  & = \min_t \norm{\point - \point_i(t)}^{2}_2 \\
	& = (\point - \V{o}_i)^\top (\point - \V{o}_i) - \left((\point-\V{o}_i)^\top \V{z}_i\right)^2 \\
	& = \point^\top \V{Z}_i\point - 2\V{o}_i^\top \V{Z}_i\point + \V{o}_i^\top\V{Z}_i\V{o}_i,
	\end{aligned}
\end{equation}
where $\V{Z}_i = \V{I} - \V{z}_i\V{z}_i^\top$. To approximate the object center, we find the point that is the closest to all camera principle axes:
\begin{equation}
	\begin{aligned}
		\point_o &= \argmin_\point \sum_i d^2\left(\point, \point_i(t)\right) \\
		&= \point^\top \left(\sum\limits_{i=1}^\cameraNum\V{Z}_i\right)\point - 2\left(\sum\limits_{i=1}^\cameraNum \V{o}_i^\top \V{Z}_i\right)\point + \sum\limits_{i=1}^\cameraNum \V{o}_i^\top\V{Z}_i\V{o}_i.
	\end{aligned}
	\label{eq.camera_center}
\end{equation}
The global optimum $\point_o$ is attained by solving the following normal equation of~\cref{eq.camera_center}:
\begin{equation}
	\begin{aligned}
		\V{A}^\top\V{A}\point &= \V{A}^\top \V{b} \\ 
		\text{with} \quad \V{A}&=\sum\limits_{i=1}^\cameraNum\V{Z}_i, \quad\V{b}=\sum\limits_{i=1}^\cameraNum \V{Z}_i\V{o}_i
	\end{aligned}
\end{equation}

\paragraph{Camera centers' scale}
After centering the scene, we apply a global scale to all camera center locations to ensure a unit sphere bounds the scene.
We assume that all cameras surround the object.
Then we can compute the global scale factor as the maximal camera center norm scaled by a suitable value $s_r$:
\begin{equation}
	s = \max \{\norm{\V{o}_i - \point_o}_2\} / s_r.
\end{equation}
We chose $s_r$ such that it is slightly larger than the ratio of the camera-to-object distance to the object size.
For \diligentmv~\cite{li2020multi} objects, we set $s_r=10$ as they are captured about \SI{1.5}{m} away from about \SI{20}{cm} height objects.
For \pandora~\cite{dave2022pandora} and our objects, we set $s_r=3$.

\end{appendices}