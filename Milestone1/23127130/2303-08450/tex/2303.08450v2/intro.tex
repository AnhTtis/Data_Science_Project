\section{Introduction}

% \begin{figure*}[t]]
% \centering
% \includegraphics[width=1.0\textwidth]{figure1.pdf}
% \caption{\textbf{(a)} Pose Saliency Representation is our proposed mechanism using the two most salient poses to represent an action, which captures essential information while reducing computational overhead. \textbf{(b)} Video-level methods require interacting intra-frame and inter-frame information, leading to increasing calculation and potentially computing useless information. \textbf{(c)} PoseRAC is our proposed pose-level method which estimates the pose of each frame and utilizes such core information to classify actions frame-by-frame. Unlike video-level methods, our approach reduces calculation and uses a lightweight action-trigger to obtain the final result.}
% \label{fig1}
% \end{figure*}

Periodic movement is a ubiquitous phenomenon in nature, including human activities. Repetitive action counting aims to count the number of repetitive actions in a video, which is crucial for analyzing human action, such as pedestrian detection\cite{ran2007pedestrian}, camera calibration\cite{huang2016camera}, and three-dimensional reconstruction\cite{ribnick20103d, li2018structure}. However, this task has not been extensively explored. Previous works, such as those by Levy et al. \cite{levy2015live} and Pogalin et al. \cite{pogalin2008visual}, rely on the periodicity assumption. Still, the period of an action can be long or short or even mixed with inconsistent cycles. Other works, such as those by Zhang et al. \cite{zhang2020context} and Dwibedi et al. \cite{dwibedi2020counting}, exploit contextual information to achieve better results. Moreover, Hu et al. \cite{hu2022transrac} propose a multi-scale temporal correlation encoder that achieves state-of-the-art performance, maintaining it until our work.

All current works on this task are video-level, which involves expensive feature extraction and sophisticated video-context interaction. Meanwhile, the research on human pose estimation is in full swing, and the current results, such as those by Bazarevsky et al. \cite{bazarevsky2020blazepose} and Xu et al. \cite{xu2022vitpose}, can already support practical applications. However, human pose has not been well-explored in repetitive action counting tasks. We believe that if human poses can be well-utilized, it can greatly improve the performance and efficiency of this task.

\begin{figure}[t]
\centering
\includegraphics[width=0.49\textwidth]{figure2.pdf}
\caption{Our Pose Saliency Annotation. Instead of annotating the start and end, we annotate the two most salient frames.}
\label{fig2}
\end{figure}

Our motivations arise from two aspects:

\begin{itemize}

\item {\bf How to be more effective?} Human body pose is the most essential factor in an action, and other information sometimes interferes with the model, such as hue, face, skin color, etc. If we ignore those factors and focus on the most essential poses, we can improve the performance on this work. Such idea of using saliency information can be found in other tasks\cite{tian2022bi,ji2022dmra,cheng2023ssvmr}. 

\item {\bf How to be more efficient?} Video-level methods require complex contextual interactions for the entire video, including those irrelevant background features, which bear a huge amount of calculation. However, if we use dozens of human body key points to represent an action, it can greatly improve the speed of the model. The significant reduction in computing overhead can lead to greater application prospects.

\end{itemize}

Therefore, we propose a mechanism called {\bf P}ose {\bf S}aliency {\bf R}epresentation ({\bf PSR}), which represents an action using the two most salient poses. The common method of representing an action with RGB frames is redundant, as intra-frame spatial information and inter-frame temporal information need complex calculations to obtain high-level semantic information. However, our PSR mechanism, as shown in Figure \ref{fig1} (a), can use only two key frames to capture the salient pose features of each action and establish a unique mapping between salient poses and action classes.

Based on PSR, we propose the first pose-level network called {\bf Pose} Saliency Transformer for {\bf R}epetitive {\bf A}ction {\bf C}ounting ({\bf PoseRAC}), as shown in Figure \ref{fig1} (c). We first extract poses of all frames using state-of-the-art algorithms, such as BlazePose\cite{bazarevsky2020blazepose}. Next, our core model maps each of these poses to an action class, which we detail in Sec~$\S\ref{second}$ and~$\S\ref{third}$. Lastly, we design a lightweight Action-trigger module, where the action of each frame is used to complete video-level counting. Our PoseRAC does not require complicated calculations and has been experimentally proven to have the advantages of both accuracy and speed.

\begin{figure}[t]
\centering
\includegraphics[width=0.46\textwidth]{figure6.pdf}
\caption{The comparison of PoseRAC and previous SOTA methods on \emph{RepCount-(pose)} test set regarding speed, OBO accuracy and model size, which represented by the sizes of bubbles. When calculating the speed of PoseRAC, the time of pose estimation is included.}
\label{fig6}
\end{figure}

However, learning such mapping between salient poses and actions requires salient annotation, which current datasets, such as the \emph{RepCount}\cite{hu2022transrac}, do not provide. It only annotates the start and end frames of each action, which does not necessarily the most salient moments of an action. To solve this problem, we improve on the current datasets for pose-level method. Dataset augmentation has been proven to be effective\cite{zhang2018mixup, zhu2022dynamic, cheng2022m3st, li2023generating}. Different from the original annotation of \emph{RepCount}, we propose a new annotation idea that has not yet been explored: {\bf P}ose {\bf S}aliency {\bf A}nnotation ({\bf PSA}). As Figure \ref{fig2} shows, instead of annotating the start and end frames of each action in traditional boundary annotation, we annotate the two most salient frames. We first pre-define the two most salient poses for all action classes. For example, for sit-up, we define lying down and sitting as two salient poses, for squats, body-upright and squatting are two salient poses, and so on. Then for all videos in training set, we annotate the frame indices of two salient poses for each action event. With such novel annotation, we can obtain the most representative poses to greatly optimize our training process. Combining Pose Saliency Annotation, we augment two current datasets \emph{RepCount} and \emph{UCFRep} with pose-level annotations, and create two new version: {\bf \emph{RepCount-pose}} and {\bf \emph{UCFRep-pose}}, which can be used by all future pose-level methods.

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{figure3.pdf}
\caption{{\bf The specific implementation of Pose Saliency Annotation.} We define two salient poses for each action and precisely select the frames in which these poses occur in a given video. Here we take front raise action as an example, and L1, L2 mean the indices of two salient poses of the first action, L3 to L6 are similar.}
\label{fig3}
\end{figure*}

During inference, we make a fair comparison with the previous approaches, and using PSA to train PoseRAC, we achieve new state-of-the-art performance on two test set, far outperforming all current methods, with an OBO metric of 0.56 compared to 0.29 of previous state-of-the-art TransRAC. Moreover, our model has a exaggerated running speed, which takes only 20 minutes to train on a single GPU, and it is even so lightweight to train in only one hour and a half on a CPU, which is unimaginable in previous video-level methods. PoseRAC is also very fast during inference, which is almost 10x faster than the previous state-of-the-art TransRAC on the average speed per frame. Figure \ref{fig6} further demonstrates the superiority of our method in terms of speed and accuracy, even having significantly fewer model parameters than previous video-level methods.

We summarize our contributions in three-fold:

\begin{itemize}

\item Based on our proposed Pose Saliency Annotation, we annotate the salient poses of all videos, augment two current datasets with pose-level annotations, and create two new version: \emph{RepCount-pose} and \emph{UCFRep-pose}.

\item We propose Pose Saliency Representation which represents each action with two salient poses, rather than redundant frames. Then we propose the first pose-level model PoseRAC to solve repetitive action counting task, which is simple, effective and efficient.

\item Our PoseRAC not only far outperforms all state-of-the-art methods in performance on current datasets, but also has particularly high efficiency. It is lightweight enough to be easily trained on CPU, which is undoubtedly not possible with previous methods.

\end{itemize}