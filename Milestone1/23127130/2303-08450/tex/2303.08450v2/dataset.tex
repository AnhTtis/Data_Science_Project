\section{Pose Saliency Annotation}

\begin{table*}[t]
\centering
\begin{tabular}{ c|c|c|c|c|c|c  }
\hline
\multirow{3}{*}{Action class} & \multicolumn{6}{|c}{RepCount-pose} \\
\cline{2-7}
& \multirow{2}{*}{Salient pose I} & \multirow{2}{*}{Salient pose II} & \multicolumn{2}{|c|}{training set} & \multicolumn{2}{|c}{test set} \\
\cline{4-7}
& & & video & event & video & event \\
\hline
bench press  & lying flat and arms-upward & lying flat and arms-down & 41 & 190 & 19 & 219 \\
front raise  & arms-down & arms-raise & 76 & 370 & 18 & 132 \\
jumping jack & body-upright and arms-down & jumping up and arms-upward & 49 & 350 & 26 & 713\\
pommel horse & body leaning to the left & body leaning to the right & 57 & 424 & 15 & 438 \\
sit-up       & lying down & sitting & 54 & 242 & 20 & 270 \\
squat        & body-upright & squatting & 81 & 544 & 18 & 164 \\
pull-up      & arms-hanging & arms-pull-up & 63 & 348 & 19 & 217 \\
push-up      & lying prostrate and arms-straight & lying prostrate and arms-bent & 66 & 449 & 16 & 303 \\
\hline
All & - & - & 487 & 2917 & 151 & 2456 \\
\hline
\end{tabular}
\vspace{1em}
\caption{Detailed information of \emph{RepCount-pose}, including salient poses, video count and event count of each action.}
\label{tab1}
\end{table*}

\begin{table*}[t]
\centering
\begin{tabular}{ c|c|c|c|c|c|c  }
\hline
\multirow{3}{*}{Action class} & \multicolumn{6}{|c}{UCFRep-pose} \\
\cline{2-7}
& \multirow{2}{*}{Salient pose I} & \multirow{2}{*}{Salient pose II} & \multicolumn{2}{|c|}{training set} & \multicolumn{2}{|c}{test set} \\
\cline{4-7}
& & & video & event & video & event \\
\hline
bench press & lying flat and arms-upward & lying flat and arms-down & 15 & 28 & 2 & 4 \\
body weight squat & body-upright & squatting & 19 & 50 & 4 & 9 \\
handstand push-up & handstanding and arms-straight & handstanding and arms-bent & 18 & 48 & 5 & 17  \\
jumping jack & body-upright and arms-down & jumping up and arms-upward & 17 & 49 & 5 & 20  \\
pommel horse & body leaning to the left & body leaning to the right & 20 & 66 & 5 & 48  \\
\hline
All & - & - & 89 & 241 & 21 & 98 \\
\hline
\end{tabular}
\vspace{1em}
\caption{Detailed information of \emph{UCFRep-pose}, including salient poses, video count and event count of each action.}
\label{tab2}
\end{table*}

There are some existing datasets for the repetition action counting task, such as \emph{UCFRep}\cite{zhang2020context} and \emph{Countix}\cite{dwibedi2020counting}. However, the biggest shortcoming is that only coarse-grained ground truth annotation are provided. Later, the proposal of \emph{RepCount}\cite{hu2022transrac} brings fine-grained annotation for the first time, in which start and end time of every action cycle are annotated to promote the further development in this field. It needs to be pointed out that such fine-grained annotation is very valuable when using video-level algorithms. 

% \begin{figure}[b]
% \centering
% \includegraphics[width=0.4\textwidth]{figure4.pdf}
% \caption{Action Distributions of {\bf \emph{RepCount-pose}} training set (left) and {\bf \emph{UCFRep-pose}} training set (right).}
% \label{fig4}
% \end{figure}

We introduce our pose-level model to learn a unique mapping between salient poses and actions. However, obtaining the core information of the frame indices where the salient poses occur is a challenge since such annotations are not available in current datasets. The most salient poses typically appear in the middle of the actions, so using only the start and end of actions in \emph{RepCount} may not allow the model to learn the mapping relationship effectively. Moreover, other datasets lack any fine-grained annotations, making it difficult to implement our pose-level method.

Based on the limitations of current datasets, we propose a novel {\bf Pose Saliency Annotation} that addresses the lack of annotations for salient poses. As Figure \ref{fig3} shows, we pre-define two salient poses for each action and annotate the frame indices where these poses occur for all videos in the training set, creating new annotation files for our pose-level method to train on. We apply this approach to two datasets, \emph{RepCount} and \emph{UCFRep}, and create two new annotated version called {\bf \emph{RepCount-pose}} and {\bf \emph{UCFRep-pose}}.

For {\bf \emph{RepCount-pose}}, after cleaning, the training set contains 487 videos and 2917 annotated action events. We retain the original test set without modification to test the robustness of our method. This new annotation scheme provides a foundation for pose-level methods and opens up new research opportunities. It can be observed that the number of events in the training set is close to the test set. This is because the pose-level method does not need to predict the number of repetitions during training, but only completes the mapping between salient poses and actions, so we do not need to capture every action event in the training set, but choose high-quality actions. In this regard, the cost of annotation will also be less than video-level methods. For {\bf \emph{UCFRep-pose}}, we selected 5 classes from the original 23 classes and annotated 110 videos to create a smaller dataset. Our pose-level model is lightweight enough to work effectively with such small dataset. The detailed information of these two new datasets are shown in Table \ref{tab1} and \ref{tab2}.

% Moreover, as Figure \ref{fig4} shows, the action distributions of two annotated datasets are relatively uniform, and we balanced the distribution without overcompressing the numbers of some higher frequency classes by selecting actions with higher-quality poses.
