\begin{abstract}

This paper presents a significant contribution to the field of repetitive action counting through the introduction of a new approach called \textbf{Pose Saliency Representation}. The proposed method efficiently represents each action using only two salient poses instead of redundant frames, which significantly reduces the computational cost while improving the performance. Moreover, we introduce a pose-level method, \textbf{PoseRAC}, which is based on this representation and achieves state-of-the-art performance on two new version datasets by using \textbf{Pose Saliency Annotation} to annotate salient poses for training. Our lightweight model is highly efficient, requiring only 20 minutes for training on a GPU, and infers nearly 10x faster compared to previous methods. In addition, our approach achieves a substantial improvement over the previous state-of-the-art TransRAC, achieving an OBO metric of 0.56 compared to 0.29 of TransRAC. The code and new dataset are available at \href{https://github.com/MiracleDance/PoseRAC}{https://github.com/MiracleDance/PoseRAC} for further research and experimentation, making our proposed approach highly accessible to the research community.

\end{abstract}