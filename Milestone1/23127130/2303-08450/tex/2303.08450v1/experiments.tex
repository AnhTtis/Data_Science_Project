\section{Experiments and Results}

\begin{table*}[t]
\centering
\begin{tabular}{ c |c||c|c||c|c||c }
\hline
\multicolumn{2}{c||}{\multirow{2}{*}{Methods}} & \multicolumn{2}{c||}{RepCount (-pose)} & \multicolumn{2}{c||}{UCFRep (-pose)} & \multirow{2}{*}{Time (ms)} \\
\cline{3-6}
\multicolumn{2}{c||}{} & MAE $\downarrow$ & OBO $\uparrow$  & MAE $\downarrow$ & OBO $\uparrow$ & \\
\hline
\multirow{7}{*}{video-level} & RepNet\cite{dwibedi2020counting} & 0.995 & 0.013 & 0.981 & 0.018 & 100\\
& X3D\cite{feichtenhofer2020x3d} & 0.911 & 0.106 & 0.982 & 0.331 & 220\\
& Zhang et al.\cite{zhang2020context} & 0.879 & 0.155 & 0.762 & 0.412 & 225\\
& TANet\cite{liu2021tam} & 0.662 & 0.099 & 0.892 & 0.129 & 187\\
& VideoSwinTransformer\cite{liu2022video} & 0.576 & 0.132  & 1.122 & 0.033 & 149\\
& Huang et al.\cite{huang2020improving}  & 0.527 & 0.159  & 1.035 & 0.015 & 156\\
& TransRAC\cite{hu2022transrac}  & 0.443 & 0.291 &  0.581 & 0.329 & 200\\
\hline
first pose-level & {\bf PoseRAC (Ours)} & {\bf 0.236} & {\bf 0.560}  & {\bf 0.312}  & {\bf 0.452} & {\bf 20}\\
\hline
\end{tabular}
\vspace{1em}
\caption{Performance on \emph{RepCount(-pose)} and \emph{UCFRep(-pose)} test. For the \emph{RepCount} and \emph{RepCount-pose} (also for the \emph{UCFRep} and \emph{UCFRep-pose}), their test sets are of the same, and apart from the difference in annotations, their training sets are also the same, as our pose-level method requires pose saliency annotation, while all other video-level methods require traditional boundary annotation.}
\label{tab3}
\end{table*}

We present experiments and results on \emph{RepCount} and \emph{UCFRep} benchmarks, which we upgrade to \emph{RepCount-pose} and \emph{UCFRep-pose} using our proposed Pose Saliency Annotation. This new design enables our proposed pose-level methods to provide a fair comparison with previous state-of-the-art methods. The main evaluation metrics used in previous work\cite{dwibedi2020counting, zhang2020context, hu2022transrac} are \textbf{Off-By-One (OBO) count error} and \textbf{Mean Absolute Error (MAE)}. OBO measures the error rate of repetition count over the entire dataset, while MAE represents the normalized absolute error between the ground truth and the prediction. They can be defined as:
\begin{gather}
\mathrm{\bf{OBO}} = \frac{1}{N}\sum\limits_{i=1}^N[\vert \Tilde{c_i}-c_i\vert\leq 1]\\
\mathrm{\bf{MAE}} = \frac{1}{N}\sum\limits_{i=1}^N\frac{\vert \Tilde{c_i}-c_i\vert}{\Tilde{c_i}}
\end{gather}
where $\Tilde{c}$ is the ground truth, $c_i$ is our prediction, and $N$ is the number of videos.


\subsection{Experiment Setup}
Our hardware setup includes an Intel Core i7 Xeon CPU, GeForce RTX 3090 Ti GPU, and 64 GB RAM. We use PyTorch to implement our method, which utilizes the lightweight BlazePose\cite{bazarevsky2020blazepose} for Pose Estimation. Our model has a simple fully-connected layer as Embedding, a six-layer Transformer Encoder, and a two-layer MLP network as Pose Mapping. This lightweight model trains quickly, with 15 epochs completed in 20 minutes on a GPU or one hour and a half on a CPU.

\subsection{Evaluation and Comparison}

Our approach for repetitive action counting, PoseRAC, outperforms existing methods in terms of both accuracy and speed. Table \ref{tab3} shows that our method consistently outperforms previous methods on both datasets and under both evaluation metrics, with an OBO metric of 0.56 compared to the 0.29 of TransRAC under the \emph{RepCount-pose}. Additionally, our lightweight input data and model make our method significantly faster than previous video-level methods. In the inference stage, our method has the fastest average processing time per frame, with nearly ten times the speed improvement over TransRAC, such superiority of our PoseRAC can be seen in Figure \ref{fig6}.

\subsection{Ablation Studies}

We conduct ablation studies on \emph{RepCount-pose} to analyze three core ideas of PoseRAC.

\noindent{\bf Choice of Pose Estimation Network.} The first part of PoseRAC is a Pose Estimation Network, and we compare two excellent algorithms: Vitpose\cite{xu2022vitpose} and BlazePose\cite{bazarevsky2020blazepose} in Table \ref{tab4}. Although Vitpose has more powerful learning ability, BlazePose estimates an additional depth information, resulting in better performance when applied to this task. Moreover, BlazePose is designed for mobile terminals, offering an advantage in speed. Therefore, we choose BlazePose as the Pose Estimation Network for our work.

\begin{figure}[b]
\centering
\includegraphics[width=0.4\textwidth]{figure7.pdf}
\caption{The distribution of embedding feature for each action in high-level space after training with Metric Learning.}
\label{fig7}
\end{figure}

\begin{table}[t]
\centering
\begin{tabular}{ c|c|c|c }
\hline
Algorithms & MAE $\downarrow$ & OBO $\uparrow$  & Time (ms) \\
\hline
BlazePose\cite{bazarevsky2020blazepose} & {\bf 0.236} & {\bf 0.560} & {\bf 20} \\
Vitpose\cite{xu2022vitpose}   & 0.305 & 0.463 & 46 \\
\hline
\end{tabular}
\vspace{1em}
\caption{Comparison of different pose estimation algorithms.}
\label{tab4}
\end{table}

\begin{table}[t]
\centering
\begin{tabular}{ c|c|c|c }
\hline
Loss & $\alpha$ & MAE $\downarrow$ & OBO $\uparrow$ \\
\hline
$\mathrm{\bf{L}}_{cls}$ only & - & 0.317 & 0.486 \\
\hline
\multirow{3}{*}{$\mathrm{\bf{L}}_{cls} + \alpha\mathrm{\bf{L}}_{tri}$} & {\bf 0.01} & {\bf 0.236} & {\bf 0.560}  \\
& 0.05 & 0.289 & 0.501 \\
& 0.1 & 0.328 &  0.462 \\
\hline
\end{tabular}
\vspace{1em}
\caption{The effect of Metric Learning.}
\label{tab5}
\end{table}

\begin{table}[t]
\centering
\begin{tabular}{ c|c|c }
\hline
Methods & MAE $\downarrow$ & OBO $\uparrow$ \\
\hline
ResNet-50\cite{he2016deep} & 0.556 & 0.181 \\
ViT-32\cite{dosovitskiy2020image} & 0.517 & 0.195 \\
\hline
{\bf PoseRAC} & {\bf 0.236} & {\bf 0.560} \\
\hline
\end{tabular}
\vspace{1em}
\caption{Comparison of common image classification methods and our pose-level method.}
\label{tab6}
\end{table}

\noindent{\bf Effectiveness of Metric Learning.} We use two losses to train our model. Table \ref{tab5} compares the performance with and without Triplet Margin Loss using different values of $\alpha$. Our model can be effectively trained with Binary Cross Entropy Loss only, but adding Metric Learning improves it, and the best value of $\alpha$ is found to be 0.01. Our Metric Learning improves the optimization, and Figure \ref{fig7} shows that the Encoder trained with Metric Learning enhances the ability to distinguish salient poses of each class.

\noindent{\bf Image classification and our pose-level method.} 
Our pose-level method outperforms those based on image classification. Simply replacing our Encoder and Pose Mapping with an image classification algorithm would result in a severe drop in performance, as shown in Table \ref{tab6}. While our method extracts the core information, i.e., the pose of each frame, image classification methods bring in irrelevant information, which is similar to the video-level methods.

\begin{figure}[t]
\centering
\includegraphics[width=0.47\textwidth]{figure8.pdf}
\caption{Visualization of Pose Mapping. Under a given class, for each frame, the higher the score (the darker the red in figure), the more likely it is salient pose I, and the lower the score (the darker the blue), the more likely it is salient pose II. When the score is closer to the middle (the gray part in the figure), the pose of current frame is more likely to be an irrelevant pose. We can see that the prediction results of the model are very accurate, as when the salient posture I or II occurs in the video, the color is red or blue, respectively.}
\label{fig8}
\end{figure}

\subsection{Qualitatively Evaluation}
To verify the effectiveness of our method, we visually analyze the output of Pose Mapping. As shown in Figure \ref{fig8}, trained with Pose Saliency Annotation, PoseRAC has a strong ability to discriminate the salient poses of various action classes, that is, when these poses occur in the video, our model can accurately recognize them and determine the corresponding space. Specifically, the score of salient pose I is greater than 0.8 (red part in the figure), the score of salient pose II is less than 0.2 (blue part in the figure), and other irrelevant poses have an output close to 0.5 (gray part in the figure).