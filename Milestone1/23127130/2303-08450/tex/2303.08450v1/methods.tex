\section{PoseRAC Model}
\label{sec4}

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{figure5.pdf}
\caption{Overview of our proposed PoseRAC. For a input video, the repetitive count can be obtained through Pose Estimation, Transformer Encoder, Pose Mapping and Action-trigger, where only the Encoder and the Pose Mapping need to be trained. We use Triplet Margin Loss to train the Encoder while Binary Cross Entropy Loss to train both the Encoder and the Pose Mapping. In addition to achieving the state-of-the-art performance so far, the biggest highlight of our PoseRAC is that it is lightweight enough to be easily trained on a CPU.}
\label{fig5}
\end{figure*}

Given a video $V={\{x_i\}}^{T}_{1}\in \mathbb{R}^{C\times H\times W\times T}$ with $T$ RGB frames, repetitive action counting model aims to predict a certain value $Y$, which is the number of repetitive actions. In this section, we will introduce our PoseRAC in detail.

\subsection{Model Overview}

As shown in Figure \ref{fig5}, PoseRAC consists of four parts. 

\begin{itemize}

\item The first is a state-of-the-art and lightweight Pose Estimation Network~($\S\ref{first}$), which is used to estimate the poses represented by lots of human pose key points from each frame of the original video sequence. 

\item The second is a simple Transformer Encoder~($\S\ref{second}$) to embed the key points of poses into high-level feature space, where the same class have similar distances, while the distances of different classes are far apart.

\item The third is a Pose Mapping Module~($\S\ref{third}$), where the unique mapping relationship between the salient poses and the action classes can be learned. Each pose can be mapped to the action class with the highest probability after the previous encoding.

\item The fourth part is a lightweight Action-trigger Module~($\S\ref{fourth}$). When we get the salient action classification results of all frames of the entire video sequence, we can use this module to calculate the repetition count in a short time.

\end{itemize}

\subsection{Pose Estimation Network}
\label{first}
Our model first converts the video sequence into a sequence of human pose key points, which can be defined as: 
\begin{equation}
\begin{split}
&V={\{x_i\}}^{T}_{1}\in \mathbb{R}^{C\times H\times W\times T}\\
&V\xrightarrow{\mathrm{Pose Estimation}} P={\{p_i\}}^{T}_{1}\in \mathbb{R}^{D\times K\times T}
\end{split}
\label{eq1}
\end{equation}
where each $x_i$ represents a single RGB frame, and each $p_i$ represents the key points of each frame. To express the key points of each frame, we use $D\times K$ sequence, which includes two parts, one ($K$) is the number of key points to fully represent the current pose, the other ($D$) is the dimension of each key point, generally three, which are the two coordinates of the planes and the depth estimation.

Here we use state-of-the-art pose estimation models such as Vitpose\cite{xu2022vitpose} and BlazePose\cite{bazarevsky2020blazepose}. The pose estimation algorithms themselves are not designed by us, but we introduce pose information into the action counting task, which is a novel design not explored by previous work.

Moreover, our pose-level poses estimation processes the primitive information of video, which is similar to the feature extraction network in all video-level algorithms such as I3D\cite{carreira2017quo}, VideoSwinTransformer\cite{liu2022video}, and TSN\cite{wang2016temporal}. But the difference is that the result of video-level incorporates all information, while pose-level only produces core information, which greatly improves the performance. Additionally, using pose information can contribute to the lightweight of model. For instance, for a 1024-frame video, video-level feature extraction with an output dimension of 512 would produce a data volume of $1024\times 512=524288$, while using pose information with 33 key points produces a data volume of only $1024\times 33 \times 3=101376$.

\subsection{Encoding Poses with Transformer}
\label{second}
Here we specify our data representation for the Transformer Encoder, which requires input batch size, sequence length, and embedding dimensions. In our pose-level approach, each frame is a batch, the number of key points in each frame is the sequence length, and the feature dimension of each key point is the embedding dimension.

First we get the pose of each frame ${p_i}\in \mathbb{R}^{D\times K}$ through the Pose Estimation Network, where $i\in {1, 2, \dots, T}$ is the frame index, $K$ is the number of key points, and $D$ is the dimension of each key point. We further define $p_i = {\{k_j\}}^{K}_{1}$ to represent each key point, where $k_j\in \mathbb{R}^D$, and we embed it to obtain richer information. Our embedding projection $\mathrm{\bf{E}}$ is a simple MLP network with ReLU as the activation function. These calculations can be defined as:
\begin{equation}
\begin{split}
\mathrm{\bf{Z}}^0 = [\mathrm{\bf{E}}(k_1), \mathrm{\bf{E}}(k_2), \dots, \mathrm{\bf{E}}(k_K)]^T
\end{split}
\end{equation}
where $\mathrm{\bf{E}}(k_j)\in \mathbb{R}^{D^{\prime}}$ is the embedding feature. Then the next Transformer takes $\mathrm{\bf{Z}}^0$ as input and encodes it with self-attention. Given $\mathrm{\bf{Z}}^0\in \mathbb{R}^{K\times D^{\prime}}$ with $K$ key point features, each of which is $D^{\prime}$-dimensional, $\mathrm{\bf{Z}}^0$ is projected using $\mathrm{\bf{W}}_Q\in \mathbb{R}^{D^{\prime}\times D_q}$, $\mathrm{\bf{W}}_K\in \mathbb{R}^{D^{\prime}\times D_k}$, $\mathrm{\bf{W}}_V\in \mathbb{R}^{D^{\prime}\times D_v}$, where $D_k=D_q$, to extract feature representations query($\mathrm{\bf{Q}}$), key($\mathrm{\bf{K}}$) and value($\mathrm{\bf{V}}$), which can be defined as:
\begin{equation}
\begin{split}
&\mathrm{\bf{Q}}=\mathrm{\bf{Z}}^0\times \mathrm{\bf{W}}_Q\\
&\mathrm{\bf{K}}=\mathrm{\bf{Z}}^0\times \mathrm{\bf{W}}_K\\
&\mathrm{\bf{V}}=\mathrm{\bf{Z}}^0\times \mathrm{\bf{W}}_V
\end{split}
\end{equation}
and the output of self-attention can be computed as:
\begin{equation}
\begin{split}
\mathrm{\bf{Attn}}=\mathrm{Softmax}(\frac{\mathrm{\bf{Q}}\mathrm{\bf{K}}^T}{\sqrt{D_q}})\mathrm{\bf{V}}
\end{split}
\end{equation}
where $\mathrm{\bf{Attn}}\in \mathbb{R}^{K\times D^{\prime}}$. Also, we use common multi-head self-attention (MHSA) to make several self-attention operations calculate in parallel.

Now we introduce the overall architecture of Transformer Encoder, which has $L$ layers with each layer consisting of MHSA and MLP blocks. Also, LayerNorm and Residual Connection are applied before and after every MHSA or MLP block, respectively. Because the number of key points of each frame is  a bit less, so our encoder does not include the downsampling module that other models may have. The overall process can be defined as:
\begin{equation}
\begin{split}
&\mathrm{\bf{\hat{Z}}}^l = \mathrm{MHSA}(\mathrm{LN}(\mathrm{\bf{Z}}^{l-1})) + \mathrm{\bf{Z}}^{l-1}\\
&\mathrm{\bf{Z}}^l = \mathrm{MLP}(\mathrm{LN}(\mathrm{\bf{\hat{Z}}}^l)) + \mathrm{\bf{\hat{Z}}}^l
\end{split}
\end{equation}
where $\mathrm{\bf{Z}}^{l-1}$, $\mathrm{\bf{\hat{Z}}}^l$, $\mathrm{\bf{Z}}^l\in \mathbb{R}^{K\times D^{\prime}}$.


\subsection{Pose Mapping}
\label{third}
Taking the Encoder output $\mathrm{\bf{Z}}^L\in \mathbb{R}^{K\times D^{\prime}}$ as input, Pose Mapping module outputs probability scores $\mathrm{\bf{S}}\in \mathbb{R}^{C}$ of the current frame over all action classes. We perform binary classification after Sigmoid activation for each class, with the two salient poses of each class represented by the same bit data. To realize such a module, we use a very lightweight MLP network, which avoids the complexity. First, the two dimensions $K$ and $D^{\prime}$ of $\mathrm{\bf{Z}}^L$ are flattened into $\mathbb{R}^{KD^{\prime}}$, and then it passes through an MLP module, where the output channels is set to $C$, which can be defined as:
\begin{equation}
\begin{split}
\mathrm{\bf{S}} = \sigma(\mathrm{MLP}(\mathrm{Flatten}(\mathrm{\bf{Z}}^L)))
\end{split}
\end{equation}
where $\sigma$ represents the Sigmoid activation function.

With such Pose Mapping, we can obtain the scores of single frame. It should be noted that we extract the poses of all frames, and use the convenience of matrix operations to obtain scores in parallel, which is actually consistent with the idea of mini batch. So at last, we combine the scores of all frames to get the video score matrix $\mathrm{\bf{\hat{S}}}\in \mathbb{R}^{C\times T}$, where $T$ represents the number of frames in the current video. 


\subsection{Action-trigger Module}
\label{fourth}
We use the lightweight Action-trigger Module to obtain the final output $Y$, the repetitive action count, which has a time complexity of $\mathcal{O}(n)$. First, we get the scores $S_c\in \mathbb{R}^T$ of a given action class from $\mathrm{\bf{\hat{S}}}$. Then, we scan all frames and use the action-trigger mechanism to count when the two salient poses of the action class occur sequentially. We set upper and lower bounds to distinguish the scores of the two salient poses, which cluster non-salient poses in the middle and easily classify the salient poses to the two ends.

\subsection{Losses and Metric Learning}

The modules need to be trained are Embedding, Transformer Encoder and Pose Mapping, and because we perform binary classification for each class, so we use the Binary Cross Entropy Loss, which can be defined as follows:
\begin{gather}
\mathcal{L}_{bce} = -\frac{1}{N}\sum\limits_{i=1}^{N}(\frac{1}{C}\sum\limits_{j=1}^{C}loss(i,j))  \\
 loss(i,j)=y_{ij}\log p_{ij} + (1-y_{ij})\log(1-p_{ij})
\end{gather}
where $N$ represents the batch size (in our method, each frame is a batch), $C$ represents the number of classes, $y$ and $p$ are the labels and our predictions, respectively.

Moreover, we use Metric Learning to improve our Encoder and introduce the Pose Triplet Loss. Given a pose, Encoder produces higher-level features $\mathrm{\bf{Z}}^L$, which should be more representative. As shown in Figure \ref{fig5}, we achieve this with Triplet Margin Loss function, which selects anchors, same class positive samples, and different classes negative samples in a batch. It can be expressed as:
\begin{equation}
\begin{split}
\mathcal{L}_{tri} = \mathrm{max}(\mathrm{CS}(a,p)-\mathrm{CS}(a,n)+\mathrm{margin},0)
\end{split}
\end{equation}
where $a$, $p$, $d$ are anchors, positive and negative samples, and $\mathrm{CS}$ represents the Cosine Similarity to measure the distance between features. We pay more attention to hard samples, where the distances between anchors and negative samples are even smaller than those of positive samples. After Metric Learning, the poses of each action can be distinguishable, which cluster in the high-level space.

At last, our overall training combines these two losses:
\begin{equation}
\begin{split}
\mathcal{L} = \mathcal{L}_{bce} + \alpha\mathcal{L}_{tri}
\end{split}
\end{equation}
where $\alpha$ is the weight factor to control the two losses in the same numeric scale.
\subsection{Implementation Details}

\noindent{\bf Training.} We use the \emph{RepCount-pose} and \emph{UCFRep-pose} dataset we created to train our model. Only the frames with salient poses are inputted into the network instead of the entire video to speed up the fitting.

\noindent{\bf Inference.} During inference, the entire video sequence is inputted into the model. The poses of all frames pass through the Encoder and Pose Mapping, and then enter the Action-trigger Module to output the repetitive count.