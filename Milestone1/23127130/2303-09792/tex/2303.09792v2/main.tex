%File: anonymous-submission-latex-2024.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

% Include other packages here, before hyperref.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{adjustbox} %调整表格大小
\usepackage{booktabs}
\usepackage{diagbox}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makecell}
\usepackage{subfigure}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage[switch]{lineno}


%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}

\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}


\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.


\title{Exploring Sparse Visual Prompt for Domain Adaptive Dense Prediction}

\author{Senqiao Yang\textsuperscript{\rm 1,2*}, 
Jiarui Wu\textsuperscript{\rm 1,3*},
Jiaming Liu \textsuperscript{\rm 1}\thanks{Equal contribution: liujiaming@bupt.cn}, 
 Xiaoqi Li\textsuperscript{\rm 1},
Qizhe Zhang\textsuperscript{\rm 1},\\
Mingjie Pan\textsuperscript{\rm 1},
Yulu Gan\textsuperscript{\rm 1},
Zehui Chen \textsuperscript{\rm 4},
Shanghang Zhang\textsuperscript{\rm 1}\thanks{Corresponding author: shzhang.pku@gmail.com}\\
\textsuperscript{\rm 1}Peking University, \textsuperscript{\rm 2}Harbin Institute of Technology, Shenzhen,\\ \textsuperscript{\rm 3} Beihang University, \textsuperscript{\rm 4} University of Science and Technology of China
}

% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry
\usepackage{hyperref}
\begin{document}

\maketitle

\begin{abstract}

The visual prompts have provided an efficient manner in addressing visual cross-domain problems. In previous works, \cite{gan2022decorate} first introduces domain prompts to tackle the classification Test-Time Adaptation (TTA) problem by warping image-level prompts on the input and fine-tuning prompts for each target domain. However, since the image-level prompts mask out continuous spatial details in the prompt-allocated region, it will suffer from inaccurate contextual information and limited domain knowledge extraction, particularly when dealing with dense prediction TTA problems. To overcome these challenges, we propose a novel Sparse Visual Domain Prompts (SVDP) approach, which holds minimal trainable parameters (e.g., 0.1\%) in the image-level prompt and reserves more spatial information of the input. To better apply SVDP in extracting domain-specific knowledge, we introduce the Domain Prompt Placement (DPP) method to adaptively allocates trainable parameters of SVDP on the pixels with large distribution shifts. Furthermore, recognizing that each target domain sample exhibits a unique domain shift, we design Domain Prompt Updating (DPU) strategy to optimize prompt parameters differently for each sample, facilitating efficient adaptation to the target domain. Extensive experiments were conducted on widely-used TTA and continual TTA benchmarks, and our proposed method achieves state-of-the-art performance in both semantic segmentation and depth estimation tasks. \href{https://github.com/ICCV2595/SVDP}{Code link.}
\end{abstract}

\section{Introduction}

\begin{figure}[t]
\includegraphics[width=0.48\textwidth]{./images/Intro_V11.pdf}
\vspace{-0.42cm}
\centering
\caption{
\textbf{The motivation and main idea of our method.} 
\textcolor{red}{(a)} Previous dense visual domain prompts (VDP) mask out consecutive spatial details in the warped regions as shown in red circles. In dense prediction DA problems, applying dense VDP will lead to inaccurate context information extraction and severe performance degradation. \textcolor{red}{(b)} We introduce Sparse Visual Domain Prompts (SVDP), which are tailored for addressing the occlusion problem of pixel-wise information and can better extract local domain knowledge for cross-domain learning. Though the parameters of SVDP are less than VDP, SVDP achieves better semantic segmentation performance in the Test Time Adaptation.}
\label{fig:intro}
\vspace{-0.7cm}
\end{figure}



Deep neural networks can achieve promising performance if test data is of the same distribution as the training data. However, it is not the common case in real-world scenarios \cite{Radosavovic2022}, which contain diverse and disparate domains. When applying a pre-trained model in real-world tasks, the domain gap commonly exists \cite{sakaridis2021acdc}, leading to significant performance degradation on target data. Though we can manually collect labeled data for each real-world target domain, it is laborious and time-consuming~\cite{chen2022multi}. To this end, the domain adaptation (DA) methods are introduced and have drawn growing attention in the community.



While DA extensively investigates to address distribution shifts, its typical assumption involves access to raw source data. However, in real-world scenarios, raw data often cannot be publicly accessible due to data protection regulations. Meanwhile, traditional DA methods present resource-intensive backward computation, leading to high training costs \cite{ganin2015unsupervised}. To address this, Test-time adaptation (TTA) \cite{liang2023comprehensive} is gained significant attention, which tackles distribution shifts at test time with only unlabeled test data streams. Prior TTA studies \cite{wang2020tent,Wangetal2022, chen2022contrastive, goyal2022test} predominantly focus on model-based adaptation, utilizing model parameters to fit target domain knowledge.

To better solve the TTA problem, motivated by the recent advances of prompting in NLP \cite{li2021prefix, liu2023pre}, VDP \cite{gan2022decorate} first introduces a prompt-based method to tackle the classification TTA problem. It employs image-level prompts to enhance domain transfer efficiency and effectiveness. Specifically, it randomly warps the dense prompt on the input image and fine-tunes them to extract target domain knowledge. However, this prompt-based technique encounters limitations when applied to dense prediction tasks such as semantic segmentation and depth estimation TTA. Specifically, the dense prompts obscure continuous spatial information in the allocated regions, as illustrated in Figure 1 (a). This occlusion introduced by prompts leads to incomplete semantic knowledge representation, thereby negatively impacting the quality of segmentation maps. Simultaneously, the occluded details within corresponding features impede the extraction of adequate domain knowledge during cross-domain learning.

To this end, as shown in Fig.1 (b), we propose a novel Sparse Visual Domain Prompts (SVDP) approach for effectively extracting target domain knowledge, specially designed to combat domain shifts in dense prediction tasks. By introducing sparse prompts, which entail minimal trainable parameters (e.g., 0.1\%) within the image-level prompt, more spatial information from the input is preserved. Furthermore, the semantic information can be extracted sufficiently (shown in line 2), leading to noticeable improvements in segmentation outcomes (shown in line 3). 
In order to better apply SVDP in the pixel-wise TTA task, we propose the Domain Prompt Placement (DPP) to adaptively allocates trainable parameters of SVDP on the pixel with large distribution shifts. In this way, SVDP excels at extracting local domain knowledge, facilitating the transfer of pixel-wise data distribution from the source to the target domain. Furthermore, recognizing that each target domain sample exhibits a unique domain shift, we design a Domain Prompt Updating (DPU) method to optimize prompt parameters efficiently during the TTA process. Specifically, based on the extent of the domain gap observed in target domain samples, we employ varying weights to update the visual prompts.
It's worth noting that we are the pioneers in designing specific strategies for pixel-level placement and image-level optimization in vision prompt learning, which work in synergy to address domain shifts in dense prediction TTA tasks. 



Since data privacy and transmission limit access to source data in the real world, we evaluate our method on semantic segmentation and depth estimation source-free adaptation settings, including online TTA~\cite{Liangetal2020} and Continual Test-Time Adaptation~\cite{Wangetal2022} (CTTA). Our proposed approach demonstrates superior performance compared to most state-of-the-art (SOTA) methods across three benchmarks, covering Cityscapes to ACDC\cite{sakaridis2021acdc} and KITTI \cite{geiger2012we} to Drivingstereo \cite{yang2019drivingstereo}. 
The main contributions are shown as follows:

1) We are the first for introducing the visual prompt approach to the dense prediction TTA problem. We propose a novel Sparse Visual Domain Prompts (SVDP) approach to better extract local domain knowledge and transfer pixel-wise data distribution from the source to the target domain.

2) In order to efficiently apply SVDP in pixel-wise TTA tasks, we propose Domain Prompt Placement~(DPP) method to adaptively allocates trainable parameters in SVDP based on the degree of distribution shift at the pixel level. And Domain Prompt Updating~(DPU) is designed to optimize prompt parameters differently for each sample, facilitating efficient adaptation on target domains.

3) We conduct extensive experiments to evaluate the effectiveness of our method, which outperforms most SOTA methods on four TTA and two CTTA benchmarks, covering semantic segmentation and depth estimation tasks.


 
\section{Related Work}
\subsection{Test-time adaptation}        
\textbf{Test-time adaptation (TTA)}, also known as source-free domain adaptation~\cite{Boudiafetal2023, Kunduetal2022, Liangetal2020, ShiqiYangetal2021}, aims to adapt a source model to an unknown target domain distribution without using any source domain data. 
% In many real-world scenarios, data privacy and transmission cost limit access to source domain data, resulting in many traditional domain adaptation (DA) algorithms being inapplicable. 
Recent research has focused on using self-training or entropy regularization to fine-tune the source model. Specifically, 
SHOT~\cite{Liangetal2020} optimizes only the feature extractor using information maximization and pseudo labeling. AdaContrast~\cite{Chenetal2022} also uses pseudo labeling for TTA, but introduces self-supervised contrastive learning to enhance performance. In addition to model-level adaptation, ~\cite{Boudiafetal2022} adjusts the output distribution to address this problem. Tent~\cite{DequanWangetal2021} updates the training parameters in the batch normalization layers by entropy minimization. Recent works \cite{niu2023towards,
yuan2023robust} follow Tent to continually explore the robustness of normalization layers in the TTA process.
While the aforementioned works primarily focus on classification tasks, there has been a recent surge of interest in performing TTA on dense prediction tasks~\cite{Shinetal022, Songetal2022, Zhangetal2021}. 
\textbf{Continual Test-Time Adaptation (CTTA)} is a scenario in which the target domain is not static, increasing challenges for traditional TTA methods ~\cite{Wangetal2022}. ~\cite{Wangetal2022} serves as the first approach to tackle this task, using a combination of bi-average pseudo labels and stochastic weight reset. While ~\cite{Wangetal2022,song2023ecotta} addresses the continual shifts at the model level, ~\cite{gan2022decorate} leverages visual domain prompts to address the problem in the classification task at the input level for the first time. In this paper, we evaluate our approach on both TTA and CTTA with a specific focus on the dense prediction task.



\subsection{Prompt learning}
\textbf{Visual prompts} are inspired by their counterparts~\cite{PengfeiLiu2021PretrainPA} which are used in natural language processing (NLP). Language prompts are presented as text instructions to improve the pre-trained language model's understanding of downstream tasks~\cite{Brownetal2020}.
% Besides, prompt has also been widely applied to vision-language models~\cite{Juetal2021, Radfordetal2021, Yaoetal2021, zhouetal2022cocoop, zhouetal2022coop}. 
Recently, researchers have attempted to discard text encoders and use prompts directly for visual tasks. ~\cite{Bahngetal2022} employs visual prompts to pad input images, enabling pre-trained models to adapt to new tasks. Rather than fine-tuning the entire model, VPT~\cite{Conderetal2022, MenglinJia2022VisualPT, Sandleretal2022, ZifengWangetal2021} inserts prompts into image or feature-level patches to adapt Transformer-based models. While these approaches all utilize opaque-block prompts, such prompts can cause performance degradation in dense prediction tasks. 
% Therefore, we propose the use of sparse prompts for the first time to address semantic segmentation.
\textbf{Domain prompts} are first introduced in DAPL~\cite{Geetal2022}, which proposes a novel prompt learning paradigm for unsupervised domain adaptation (UDA). Embedding domain information using prompts can minimize the cost of fine-tuning and enable efficient domain adaptation. Recognizing the potential of prompt learning for UDA, MPA~\cite{chen2022multi} proposes multi-prompt alignment for multi-source UDA. DePT~\cite{Gaoetal2022} combines domain prompts with a hierarchical self-supervised regularization for TTA, which aims to solve the error accumulation problem in self-training. ~\cite{gan2022decorate} further divides domain prompts into domain-specific ones and domain-agnostic ones to address the more challenging CTTA task. However, these studies mainly focus on simple classification DA tasks. Our method, for the first time, applies sparse domain prompts to dense prediction DA tasks. Besides, we are the first to design specific placement and updating strategies for the domain prompt method, which help to jointly ease the domain shift.


\begin{figure*}[ht]
\centering
\includegraphics[width=\linewidth]{images/Framework_V25.pdf}
\vspace{-0.45cm}
\caption{\textbf{The overall framework.} \textbf{Left:} We warp the SVDP into the image and place prompt parameters on the selected pixel by Domain Prompt Placement (DPP) method. The reformulated image serves as the input of the teacher and student model.  
We obtain the uncertainty map as described in Eq. \ref{eq:mc} through the teacher model. The uncertainty map is used to evaluate the degree of pixel-level distribution shift. 
SVDP adopts consistency loss (Eq. \ref{eq:loss}) and exponential moving average (EMA) as the optimization strategies.
\textbf{Right:} Domain Prompt Updating (DPU). Based on the image-level uncertainty value, we adopt different EMA weights to realize stable updating of SVDP parameters, facilitating efficient adaptation to the target domain.}
\label{fig:framework} 
\vspace{-0.3cm}
\end{figure*}






\section{Method}

\subsection{Preliminaries}

\textbf{Test Time Adaptation (TTA)} ~\cite{liang2023comprehensive} aims at adapting a pre-trained model with parameters trained on the source data $(\mathcal{X}_S$, $\mathcal{Y}_S)$ to multiple unlabeled target data distribution $\mathcal{X}_{T_1},\mathcal{X}_{T_2}, \dots, \mathcal{X}_{T_n}$ at inference time. The entire process can not access any source domain data and can only access target domain data once. $\mathcal{X}_{T_i} = \{x_i^T\}_{i=1}^{N_t}$, where $N_t$ denotes the scale of the target domain. The upcoming target domain can be a single one (TTA) or multiple continually changing distributions (CTTA), the latter of which is a more realistic setting that requires the model to achieve stability while preserving plasticity. 

\textbf{Domain Prompt.} Inspired by language prompt in NLP, ~\cite{gan2022decorate} first introduces visual domain prompt (VDP) 
serving as a reminder to continually adapt to the target domain for the classification task, which aims to extract target domain-specific knowledge. Specifically, VDP ($\textbf{p}$) are dense learnable parameters added on the input image. 
\begin{equation}
\widetilde {\textbf{x}} = \textbf{x} + \textbf{p}
\label{eq:dense}
\end{equation}
where x represent the original input image. The reformulated image ${\widetilde {\textbf{x}}}$ will serve as the input for our model instead. 


\subsection{Motivation}
\label{sec:3.2}

\textbf{Sparse Visual Domain Prompt.} Traditional visual prompts \cite{jia2022visual} are deployed on the image or feature level to realize fine-tuning by updating a small number of prompt parameters. Recent works \cite{gan2022decorate, gao2022visual} explore visual prompts in classification DA problems, which extract domain knowledge for the target domain and transfer data distribution from the source to the target domain. 
However, DePT \cite{gao2022visual} concatenate the domain prompts with class token and image tokens to the input of transformer layers, which neglect the local domain knowledge extraction.
Meanwhile, VDP \cite{gan2022decorate} randomly set the locations of dense prompts on the input image, masking out continuous spatial details in prompt allocated regions. 
Different from classification cross-domain learning, dense prediction DA not only requires global domain knowledge but also relies on extracting intact local domain knowledge.  
As shown in Fig.\ref{fig:intro}(a), partial spatial information deficiency caused by dense prompts will lead to inaccurate contextual information and negative effects on target domain knowledge extraction. 
This observation motivates us to propose a novel Sparse Visual Domain Prompts (SVDP), which is tailored for pixel-wise prediction DA tasks. It holds minimal trainable parameters (e.g. 0.1\%) in the image-level prompt and reserves more spatial information of the input.

\textbf{Domain Prompt Placement.} Previous work~\cite{gan2022decorate,gao2022visual} randomly put the prompts on the target domain image to extract global domain knowledge. Specifically, it may set prompts on regions with trivial domain shifts, hindering the extraction of local domain knowledge. Especially in the source-free TTA setting, we can only access target domain data once, which makes the efficiency of transfer learning crucial. Therefore, we propose Domain Prompt Placement (DPP) which efficiently extracts more domain-specific knowledge and addresses local domain shift. Specifically, we measure the degree of domain gap by general uncertainty scheme~\cite{gal2016dropout, guan2021uncertainty, roy2022uncertainty,gan2022cloud} and tactfully place trainable parameters of SVDP on the pixel with large distribution shifts.


\textbf{Domain Prompt Updating.}
The amount of prompt parameters is minimal which brings the challenge of fully learning target domain knowledge during TTA process. Meanwhile, the degree of domain shift is not only various on pixels within the image but also on each target domain test sample. It thus motivates us to update prompt parameters differently for each target sample. Therefore, we design a Domain Prompt Updating (DPU) which efficiently optimizes prompt parameters to fit in target domain distribution. Specifically, we adopt the same uncertainty scheme to measure the degree of domain shift for each target sample.
According to the degree, we update prompt parameters for the individual sample with different updating weights.



\subsection{Sparse visual domain prompt} 
\label{sec:3.3}
SVDP maintains the same resolution as the input image ($\textbf{p} \in \mathbb{R}^{H \times W \times 3}$), it only masks out original information by minimal discrete trainable parameters (e.g. $0.1\%$) on the pixels with large domain shifts. Compared with the previous dense visual prompt, SVDP preserves more contextual information and possesses the capacity to capture local domain knowledge through pixel-wise prompt parameters. The overall framework of our method is shown in Fig .\ref{fig:framework}, and the specially designed prompt Placement and Updating methods are introduced in the following.




\subsection{Domain prompt placing}
\label{sec:3.4}

We propose the Domain Prompt Placement (DPP) strategy of SVDP to efficiently extract local domain knowledge in pixel-wise. We intend to place trainable parameters of SVDP on the
pixel with large distribution shifts and adapt pixel-wise data distribution from source to target domain. 
Though the confidence score is a straightforward measurement to reflect the reliability of prediction, it is trusting less and fluctuates irregularly in pixel-wise cross-domain scenarios. 
As shown in the top of Fig .\ref{fig:DPP}, we thus adopt Dropout method \cite{gal2016dropout} to realize $m$ times forward propagation and obtain $m$ group probabilities for each pixel.
Inspired by \cite{roy2022uncertainty,gan2022cloud}, we calculate the uncertainty value (Eq.\ref{eq:mc}) of the input and figure out the pixel-wise degree of domain shift.
% .Based on the variance of $m$ category of probabilities $p_i(y|x)$ of the model by MC dropout. 
\begin{equation}
\mathcal{U} (\widetilde{x}_j) =  \left( \frac{1}{m} \sum_{i=1}^m \|p_i(\widetilde {y_j}|\widetilde {x_j}) - \mu \|^2 \right) ^{\frac{1}{2}}
\label{eq:mc}
\end{equation}
, where $p_i(\widetilde{y_j}|\widetilde{x_j})$ is the predicted probability of input pixel $\widetilde{x_j}$ in the $i^{th}$ forward propagation, and $\mu$ is the mean prediction ($m$ rounds) of $\widetilde{x_j}$. $\mathcal{U} (\widetilde{x_j})$ thus represents the uncertainty of the source model for pixel-wise target input $\widetilde{x_j}$. As shown in the bottom of Fig .\ref{fig:DPP}, we sort all pixels based on their pixel-wise uncertainty value and place prompt parameters on the pixels with high uncertainty score. 




\begin{figure}[t]
\includegraphics[width=0.45\textwidth]{./images/DPP_V7.pdf}
\centering
\vspace{-0.18cm}
\caption{The detailed process of Domain Prompt Placing. The uncertainty map is estimated by MC Dropout \cite{gal2016dropout}. The SVDP parameters are placed on the pixels with high uncertainty, then warp into the raw image.}
\label{fig:DPP}
\vspace{-0.35cm}
\end{figure}







\subsection{Domain prompt updating}
\label{sec:3.5}
Motivated by the fact that the mean teacher predictions have a higher quality than the standard model \cite{tarvainen2017mean}, we utilize a mean-teacher model to provide more accurate predictions in the TTA process. To be specific, we adopt the widely-used exponential moving average (EMA) to achieve the model and prompt updating. Same as previous works\cite{Wangetal2022}, the teacher model ($\mathcal{T}_{mean}$) is updated by EMA from the student model ($\mathcal{S}_{target}$), shown in Eq. \ref{eq:ema}:
\begin{equation}
 \mathcal{T}_{mean}^{t} = \alpha \mathcal{T}_{mean}^{t-1} + (1-\alpha) \mathcal{S}_{target} ^{t}
\label{eq:ema}
\end{equation}
When $t = 0$ ($t$ is the time step), we utilize the source domain pre-trained model to initialize the weight of the teacher and student model. And we set $\alpha = 0.999$ \cite{AnttiTarvainenetal2017}, which is the updating weight of EMA. 

Different from traditional model updating, we design a special Domain Prompt Updating (DPU) strategy for SVDP to stably fit in target domain distribution. As shown in Fig .\ref{fig:DPU}, we adopt image-level uncertainty value to reflect the degree of domain shift for each target domain sample. 
We calculate the image-level uncertainty value $\mathcal{U}(x)$ by average the pixel-wise uncertainty, shown in Eq. \ref{eq:unc_for_promptregion}:
\begin{equation}
\mathcal{U}(x) = \frac{1}{H \times W}
\sum_{j }^{H \times W} \mathcal{U} (\widetilde{x}_j)
\label{eq:unc_for_promptregion}
\vspace{-0.15cm}
\end{equation}
Based on the image-level uncertainty score, we update prompt parameters for each sample with different weight. 

\begin{equation}
\label{unc_ema_prompt}
\begin{aligned}
p_{t} = \beta p_{t-1} + (1-\beta) p_{t},
\end{aligned}
\end{equation}
Note that, $p_{t}$ represents the parameters of the SVDP that need to be updated. In DPU, we set the prompt EMA updating rate $\beta = 1-(\mathcal{U}(x) \times \theta)$. $\theta$ is intended to bring the value of uncertainty up to the same order of magnitude as the value of the common EMA update parameter~\cite{AnttiTarvainenetal2017}.
As shown in the top of Fig .\ref{fig:DPU}, the prompt EMA weight is set to a large value when the input is of high uncertainty score since the large weight can efficiently adapt to the sample with the large data distribution shift.


\begin{figure}[t]
\includegraphics[width=0.45\textwidth]{./images/DPU_V6.pdf}
\centering
\vspace{-0.25cm}
\caption{The process of Domain Prompt Updating. We adaptively adjust the prompt EMA updating rate for each target domain sample based on image-level uncertainty value.}
\label{fig:DPU}
\vspace{-0.3cm}
\end{figure}


\subsection{Loss function}
We utilize teacher model to generate the pseudo labels ($\widetilde{y}_t$), which is refined by test-time augmentation and confidence filter ~\cite{Wangetal2022}.
Then, we adopt consistency loss ($L_{con}$) as the optimization objective for segmentation task, which is a pixel-wise cross-entropy loss \cite{xie2021segformer}.
\begin{equation}
 \mathcal{L}_{con}(\widetilde {x}) = -
 \frac{1}{H \times W} \sum_{w,h}^{W,H}
 \sum_c^C \widetilde{y}_t(w,h,c) \log \hat{y}_t(w,h,c)
\label{eq:loss}
\end{equation}
Where $\hat{y}_t$ is the output of our student model, $C$ means the amount of categories. The loss function of depth estimation is shown in the supplement.








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}

\input{subsection/experiment}
% \clearpage









\bibliography{aaai24}

\clearpage
\appendix
\input{subsection/sec_appendix}
\end{document}