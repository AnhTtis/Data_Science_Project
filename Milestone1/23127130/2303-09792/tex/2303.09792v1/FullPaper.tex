\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage[switch]{lineno}
\usepackage{bbding}

% Include other packages here, before hyperref.
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{diagbox}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{makecell}
\usepackage{subfigure}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{2595} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\begin{document}
% Pages are numbered in submission mode, and unnumbered in camera-ready
% \pagestyle{type}



\makeatletter
\newbox\abstract@box
\renewenvironment{abstract}
  {\global\setbox\abstract@box=\vbox\bgroup
     \hsize=\textwidth\linewidth=\textwidth
    \small
    \begin{center}%
    {\bfseries \abstractname\vspace{-.5em}\vspace{\z@}}%
    \end{center}%
    \quotation}
  {\endquotation\egroup}
\expandafter\def\expandafter\@maketitle\expandafter{\@maketitle
  \ifvoid\abstract@box\else\unvbox\abstract@box\if@twocolumn\vskip1.5em\fi\fi}
\makeatother




%%%%%%%%% TITLE
\title{Exploring Sparse Visual Prompt for Cross-domain Semantic Segmentation}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\author{Senqiao Yang\textsuperscript{\rm 1,2*}, 
Jiarui Wu\textsuperscript{\rm 1,3*},
Jiaming Liu \textsuperscript{\rm 1}\thanks{Equal contribution: liujiaming.pku@gmail.com}, 
 Xiaoqi Li\textsuperscript{\rm 1},
Qizhe Zhang\textsuperscript{\rm 1},\\
Mingjie Pan\textsuperscript{\rm 1},
Yulu Gan\textsuperscript{\rm 1},
Shanghang Zhang\textsuperscript{\rm 1}\thanks{Corresponding author: shzhang.pku@gmail.com}\\
\textsuperscript{\rm 1}Peking University, \textsuperscript{\rm 2}Harbin Institute of Technology, Shenzhen, \textsuperscript{\rm 3} Beihang University\\
{\tt\small \{liujiaming.pku, shzhang.pku\}@gmail.com}
}



% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi
%%%%%%%%% ABSTRACT
\begin{abstract}

Visual Domain Prompts (VDP) have shown promising potential in addressing visual cross-domain problems. Existing methods adopt VDP in classification domain adaptation (DA), such as tuning image-level or feature-level prompts for target domains. Since the previous dense prompts are opaque and mask out continuous spatial details in the prompt regions, it will suffer from inaccurate contextual information extraction and insufficient domain-specific feature transferring when dealing with the dense prediction (i.e. semantic segmentation) DA problems. Therefore, we propose a novel Sparse Visual Domain Prompts (SVDP) approach tailored for addressing domain shift problems in semantic segmentation, which holds minimal discrete trainable parameters (e.g. 10\%) of the prompt and reserves more spatial information. To better apply SVDP, we propose Domain Prompt Placement (DPP) method to adaptively distribute several SVDP on regions with large data distribution distance based on uncertainty guidance. It aims to extract more local domain-specific knowledge and realizes efficient cross-domain learning. Furthermore, we design a Domain Prompt Updating (DPU) method to optimize prompt parameters differently for each target domain sample with different degrees of domain shift, which helps SVDP to better fit target domain knowledge. Experiments, which are conducted on the widely-used benchmarks (Cityscapes, Foggy-Cityscapes, and ACDC), show that our proposed method achieves state-of-the-art performances on the source-free adaptations, including six Test Time Adaptation and one Continual Test-Time Adaptation in semantic segmentation. \href{https://github.com/ICCV2595/SVDP}{The code will be released.}


\end{abstract}
\maketitle
%%%%%%%%% BODY TEXT
\section{Introduction}

\begin{figure}[t]
\includegraphics[width=0.48\textwidth]{./images/Intro_V9.pdf}
\vspace{-0.39cm}
\centering
\caption{
\textbf{The motivation and main idea of our method.} 
\textcolor{red}{(a)} Traditional dense visual domain prompts (VDP) are opaque and mask out consecutive spatial details in the prompt regions as shown in red circles. In semantic segmentation DA problems, applying dense VDP will lead to inaccurate contextual information extraction and severe performance degradation. \textcolor{red}{(b)} We are the first to explore applying visual prompt learning in pixel-wise prediction DA. As shown in red boxes, we introduce Sparse Visual Domain Prompts (SVDP), which are tailored for addressing the occlusion problem of pixel-wise information and can better extract domain knowledge for cross-domain learning. Though the parameters of SVDP are less than VDP, SVDP achieves better semantic segmentation performance in the Test Time Adaptation.}
\label{fig:intro}
\vspace{-0.5cm}
\end{figure}



Deep neural networks can achieve promising performance if test data is of the same distribution as the training data. However, it is not the common case in real-world scenarios \cite{Radosavovic2022}, which contain diverse and disparate domains. When applying a pre-trained model in real-world tasks, the domain gap commonly exists \cite{sakaridis2021acdc}, leading to significant performance degradation on target data. Though we can manually collect labeled data for each real-world target domain, it is laborious and time-consuming~\cite{chen2022multi}. Therefore, the Domain Adaptation (DA) methods (i.e. Test Time Adaptation, Continual Test-Time Adaptation) are introduced and have drawn growing attention in the community.

Recently, motivated by the recent advances of Prompting in NLP \cite{lester2021power, li2021prefix, liu2023pre}, visual prompt \cite{jia2022visual} is introduced in computer vision tasks which fine-tunes a small number of trainable parameters.
Specifically, \cite{gan2022decorate, chen2022multi, gao2022visual, hu2022prosfda} utilize Visual Domain Prompts (VDP) to address the classification domain shift problem by dynamically updating the domain prompts for the target domain. 
These works randomly set the dense VDP on the input or feature-level and fine-tune them to extract target domain knowledge or maintain the domain-invariant knowledge~\cite{gan2022decorate}.
However, when these methods are applied in dense prediction (e.g. semantic segmentation) DA problems, the opaque dense prompts will mask out continuous spatial information in the prompt regions. 
As shown in Fig .\ref{fig:intro} (a), due to the occlusion brought by prompts, the feature representation suffers from partial semantic knowledge deficiency, leading to a negative impact on semantic segmentation. 
Meanwhile, the occluded information in the corresponding feature latent space also results in insufficient domain knowledge extraction during cross-domain learning.


To this end, as shown in Fig .\ref{fig:intro} (b), we propose a novel Sparse Visual Domain Prompts (SVDP) approach to better extract target domain knowledge, which is tailored for addressing domain shift in semantic segmentation. By introducing image-level sparse prompts which are of minimal discrete trainable parameters (e.g. 10\%) of the previous prompt, we are able to reserve more spatial information. Furthermore, the corresponding semantic information can be extracted from the prompt regions (line 2 of Fig .\ref{fig:intro}) and the segmentation result will be obviously improved (line 3 of Fig .\ref{fig:intro}). In order to better apply SVDP in the pixel-wise DA task, we propose the Domain Prompt Placement (DPP) which tactfully selects locations to place domain prompts. Specifically, it adaptively distributes several SVDP on more target domain-specific regions based on uncertainty guidance~\cite{gal2016dropout,rizve2021defense}. In this way, SVDP can efficiently extract local domain knowledge and thus transfer pixel-wise data distribution from the source to the target domain. Furthermore, we design a Domain Prompt Updating (DPU) which can efficiently optimize prompt parameters to fit limited target data. In detail, we adopt an uncertainty scheme to measure the image-level domain shift between the source and the target domain. According to the degree of domain gap when facing the target domain samples, we adopt different weight Exponential Moving Average to update corresponding prompts. It thus efficiently updates SVDP on each target sample and realizes domain adaptation during test time.
Note that, we are the first to design specific placement and optimizing strategies for vision prompt learning, which jointly address the domain shift in semantic segmentation. 






We evaluate our method on semantic segmentation DA task. Since data privacy and transmission cost limit access to source domain data in many real-world scenarios, we thus conduct extensive experiments on the source-free adaptation settings, including six online Test Time Adaptation~\cite{ Kunduetal2022, Liangetal2020} (TTA) and one Continual Test-Time Adaptation~\cite{Wangetal2022, gan2022decorate} (CTTA). In particular, we apply our methods on Cityscapes~\cite{cordts2016cityscapes}, Foggy-Cityscapes~\cite{sakaridis2018semantic}, and ACDC~\cite{sakaridis2021acdc} datasets. For instance, compared with the previous state-of-the-art (SOTA) methods, our method improves the mIoU by 1.5\%, 4.1\%, 3.1\%, and 2.4\% respectively in Cityscapes to ACDC semantic segmentation TTA (from Cityscapes to Fog, Night, Rain, and Snow domain respectively).







The main contributions are summarized as follows:

1) We make the first attempt to introduce the visual prompt approach to the semantic segmentation DA problem. Different from classification DA, Sparse Visual Domain Prompts (SVDP) are tailored for addressing the occlusion of pixel-wise information and can better extract domain knowledge for cross-domain learning.

2) In order to better apply SVDP in pixel-wise DA tasks, we propose Domain Prompt Placement~(DPP) to adaptively distributes several SVDP on more target domain-specific regions for extracting more domain knowledge. And Domain Prompt Updating~(DPU) is designed to efficiently optimize prompts for adapting to limited target domain data.

3) Our proposed approach outperforms the previous SOTA methods on six online TTA and one CTTA in semantic segmentation. It proves that our method is tailored for semantic segmentation cross-domain learning.

\section{Related Work}
\subsection{Test-time adaptation}        
\textbf{Test-time adaptation (TTA)}, also known as source-free domain adaptation~\cite{Boudiafetal2023, Kunduetal2022, Liangetal2020, ShiqiYangetal2021}, aims to adapt a source model to an unknown target domain distribution without using any source domain data. In many real-world scenarios, data privacy and transmission cost limit access to source domain data, resulting in many traditional domain adaptation (DA) algorithms being inapplicable. Recent research has focused on using self-training or entropy regularization to fine-tune the source model ~\cite{Laoetal2020, DequanWangetal2021, Liangetal2020,Chenetal2022}. Specifically, Tent~\cite{DequanWangetal2021} updates the training parameters in the batch normalization layers by entropy minimization. SHOT~\cite{Liangetal2020} optimizes only the feature extractor using information maximization and pseudo labeling. AdaContrast~\cite{Chenetal2022} also uses pseudo labeling for TTA, but introduces self-supervised contrastive learning to enhance performance. In addition to model-level adaptation, ~\cite{Boudiafetal2022} adjusts the output distribution to address this problem. While the aforementioned works primarily focus on classification tasks, there has been a recent surge of interest in performing TTA on dense prediction tasks~\cite{Shinetal022, Songetal2022, Zhangetal2021}. And we make the first attempt to introduce sparse visual prompts in the semantic segmentation task, which aim to better extract domain knowledge for cross-domain learning.

\textbf{Continual Test-Time Adaptation (CTTA)} is a scenario in which the target domain is not static, increasing challenges for traditional TTA methods ~\cite{Wangetal2022}. ~\cite{Wangetal2022} serves as the first approach to tackle this task, using a combination of bi-average pseudo labels and stochastic weight reset. While ~\cite{Wangetal2022} addresses the problem in both classification and segmentation tasks at the model level, ~\cite{gan2022decorate} leverages visual domain prompts to address the problem in the classification task at the input level for the first time. In this paper, we evaluate our approach on both TTA and CTTA with a specific focus on the dense prediction task.



\subsection{Prompt learning}
\textbf{Visual prompts} are inspired by their counterparts~\cite{PengfeiLiu2021PretrainPA}, which are used in natural language processing (NLP). Language prompts are presented as text instructions to improve the pre-trained language model's understanding of downstream tasks~\cite{Brownetal2020}. Besides, the prompt has also been widely applied to vision-language models~\cite{Juetal2021, Radfordetal2021, Yaoetal2021, zhouetal2022cocoop, zhouetal2022coop}. Recently, researchers have attempted to discard text encoders and use prompts directly for visual tasks. ~\cite{Bahngetal2022} employs visual prompts to pad input images, enabling pre-trained models to adapt to new tasks. Rather than fine-tuning the entire model, VPT~\cite{Conderetal2022, MenglinJia2022VisualPT, Sandleretal2022, ZifengWangetal2021} inserts prompts into the image or feature-level patches to adapt Transformer-based models. While these approaches all utilize dense prompts, such prompts can cause performance degradation in dense prediction tasks. Therefore, we propose the use of sparse prompts for the first time to address semantic segmentation.

\textbf{Domain prompts} are first introduced in DAPL~\cite{Geetal2022}, which proposes a novel prompt learning paradigm for unsupervised domain adaptation (UDA). Embedding domain information using prompts can minimize the cost of fine-tuning and enable efficient domain adaptation. Recognizing the potential of prompt learning for UDA, MPA~\cite{chen2022multi} proposes multi-prompt alignment for multi-source UDA. DePT~\cite{Gaoetal2022} combines domain prompts with a hierarchical self-supervised regularization for TTA, which aims to solve the error accumulation problem in self-training. ~\cite{gan2022decorate} further divides domain prompts into domain-specific ones and domain-agnostic ones to address the more challenging CTTA task. However, these studies mainly focus on simple classification DA tasks. Our method, for the first time, applies sparse domain prompts to more complex dense prediction DA tasks. Besides, we are the first to design specific placement and updating strategies for the domain prompt method, which help to jointly ease the domain shift.


\begin{figure*}[ht]
\centering
\includegraphics[width=\linewidth]{./images/Framework_V12.pdf}
\vspace{-0.39cm}

\caption{\textbf{The framework of Sparse Visual Domain Prompt (SVDP).} \textcolor{red}{(1)} \textbf{SVDP tuning}. We construct a teacher-student framework to update SVDPs. The SVDPs are placed on the image, which serves as the input of the teacher and student model through strong and weak augmentation. Our student network and prompt adopt consistency loss (Eq. \ref{eq:loss}) as the optimization objective. We obtain the uncertainty map as described in Eq. \ref{eq:mc} through the teacher model. The uncertainty map is used to guide our SVDP placement (DPP in Sec .\ref{sec:3.4}) on regions with large data distribution distances. For SVDP parameter updating (DPU in Sec .\ref{sec:3.4}), we propose uncertainty-guided EMA for efficiently updating prompt parameters on limited target domain data. \textcolor{red}{(2)} \textbf{SVDP testing.} At test time, SVDPs are selectively placed onto the input image with the DPP strategy. Then we feed the reformulated image to the teacher model for semantic segmentation.}
\label{fig:framework} 
\vspace{-0.3cm}
\end{figure*}






\section{Method}
\subsection{Preliminaries}

\textbf{Test Time Adaptation (TTA)} ~\cite{gan2022decorate, Wangetal2022}. TTA aims at adapting a pre-trained model with parameters trained on the source data $(\mathcal{X}_S$, $\mathcal{Y}_S)$ to multiple unlabeled target data distribution $\mathcal{X}_{T_1},\mathcal{X}_{T_2}, \dots, \mathcal{X}_{T_n}$ at inference time. The entire process can not access any source domain data and can only access target domain data once. $\mathcal{X}_{T_i} = \{x_i^T\}_{i=1}^{N_t}$, where $N_t$ denotes the scale of the target domain. The upcoming target domain can be a single one (TTA) or multiple continually changing unknown distributions (CTTA), the latter of which is a more realistic setting that requires the model to achieve stability while preserving plasticity. 

\textbf{Domain Prompt} Inspired by language prompt in NLP, ~\cite{gan2022decorate} first introduces visual domain prompt (VDP) 
serving as a reminder to continually adapt to the target domain for the classification task, which aims to extract target domain-specific knowledge. Specifically, VDP ($\textbf{p}$) are dense learnable parameters added to the input image. 
\begin{equation}
\widetilde {\textbf{x}} = \textbf{x} + \textbf{p}
\label{eq:dense}
\end{equation}
where x represent the original input image. The reformulated image ${\widetilde {\textbf{x}}}$ will serve as the input for our model instead. 


\subsection{Motivation}
\label{sec:3.2}

\begin{table}[htbp]
\vspace{-0.2cm}
\caption{The comparison of Baseline \cite{Wangetal2022} (without prompt), Dense Domain Prompt (DDP), and Sparse Visual Domain Prompt (SVDP) are conducted in the segmentation TTA (Cityscapes-to-ACDC-Rain). All experiment settings, except the usage of prompt, are the same among all methods. The designed placement and updating methods are not utilized. The metric is mIoU.}
% \scriptsize
  \centering
  % \vspace{-0.38cm}
    %\begin{tabular}{@{}llrrrrrr@{}}
    \setlength{\tabcolsep}{1.3mm}{
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Region:} & \textbf{with prompt} & \textbf{without prompt} & \textbf{full image}  \\
    \hline
    Baseline & 61.4 & 62.8 & 62.6  \\
    \hline
    DDP & 58.7 & 62.6 & 62.3  \\
    \hline
    SVDP & 64.1 & 64.8 & 64.7\\
    \hline
    \end{tabular}%
    }
  \label{tab:motivation}%
  \vspace{-0.3cm}
\end{table}

\textbf{Sparse Visual Domain Prompt.} Traditional visual prompts \cite{jia2022visual} are deployed on the image or feature-level to realize fine-tuning by updating a small number of trainable prompt parameters. Recent works \cite{gan2022decorate, chen2022multi, gao2022visual, hu2022prosfda} explore dense visual prompts in classification DA problems, which extract domain knowledge for the target domain and transfer data distribution from the source to the target domain. 
However, these works randomly set the locations of domain prompts, masking out continuous spatial details in prompt occluded regions. 
Different from classification cross-domain learning, semantic segmentation DA not only requires global domain knowledge but also relies on extracting intact local domain knowledge.  
As shown in Fig.\ref{fig:intro}(a), partial spatial information deficiency caused by dense prompts will lead to inaccurate contextual information and negative effects on target domain knowledge extraction. 
This observation motivates us to propose a novel Sparse Visual Domain Prompts (SVDP), which is tailored for pixel-wise prediction DA tasks. It adopts sparse and discrete trainable parameters, thus reserving more spatial information in prompt regions. 
We further verify our motivation in Tab. \ref{tab:motivation}, in which we place dense and sparse visual prompts in the same region (central of image). SVDP achieves better performance compared with other methods on regions with prompt. It shows that SVDP can address domain shift in the prompt region, which further proves SVDP empowers prompt to extract local domain knowledge.
Besides, SVDP also achieves competitive results on the region without prompts, which proves that SVDP is of the same ability as traditional visual domain prompt \cite{gan2022decorate} in dealing with global-level domain shift.
In addition, along with introducing SVDP, we make the first attempt to design specific placement and updating strategies for SVDP to jointly address the domain shift problem.

\textbf{Domain Prompt Placement.} Previous work~\cite{gan2022decorate,gao2022visual} randomly put the prompts on the target domain image to extract global domain knowledge. Specifically, it may set prompts on regions with trivial domain shift, hindering the efficiency of cross-domain learning. Especially in the source-free TTA setting, we can only access target domain data once, which makes the efficiency of transfer learning crucial. Therefore, we propose Domain Prompt Placement (DPP) which efficiently extracts more domain-specific knowledge and addresses local domain shift. Specifically, we measure the degree of domain gap by general uncertainty scheme~\cite{gal2016dropout, guan2021uncertainty, roy2022uncertainty,gan2022cloud} and tactfully place prompts on the regions with large distribution distance.


\textbf{Domain Prompt Updating.}
The amount of prompt parameters is minimal, which brings the challenge of fully learning target domain knowledge. Meanwhile, the degree of domain shift is not only various on regions within the image but also on each target domain test sample. It thus motivates us to update prompt parameters differently for each target sample. Therefore, we design a Domain Prompt Updating (DPU) which efficiently optimizes prompt parameters to fit in target domain distribution. Specifically, we adopt an uncertainty scheme (same as DPP) to measure the degree of domain shift for each target sample.
According to the degree, we update prompt parameters for the individual sample with different weight exponential moving average.
 
\subsection{Sparse visual prompt} 
\label{sec:3.3}


In this paper, we introduce a novel Sparse Visual Domain Prompts (SVDP), which alleviates inaccurate contextual information extraction and insufficient domain-specific feature caused by the previous dense visual prompt. In a prompt region of $\textbf{p} \in \mathbb{R}^{H_{P} \times W_{P} \times 3}$, SVDP only masks out original information by minimal discrete trainable parameters (e.g. $10\%$) on randomly selected pixels. Compared with the previous visual prompt which masks out the whole prompt region, SVDP reverses more local information. The overall framework of our method is shown in Fig .\ref{fig:framework}, and the specially designed prompt Placement and Updating methods are introduced in Sec.\ref{sec:3.4} and Sec.\ref{sec:3.5} respectively.




\subsection{Domain prompt placing}
\label{sec:3.4}


In this section, we propose the Domain Prompt Placement (DPP) strategy of SVDP to efficiently extract more local target domain knowledge. We intend to place SVDP on regions with relatively severe domain shift and better adapt pixel-wise data distribution from source to target domain. 
Though the confidence score is a straightforward measurement to reflect the reliability of prediction, it is trusting less and fluctuates irregularly in pixel-wise cross-domain scenarios. 
As shown in the top of Fig .\ref{fig:DPP}, we thus adopt Dropout method \cite{gal2016dropout} to realize $m$ times forward propagation and obtain $m$ group probabilities for each pixel.
Inspired by \cite{roy2022uncertainty,gan2022cloud}, we calculate the uncertainty value (Eq.\ref{eq:mc}) of the input and figure out the degree of domain shift pixel-wise.
% .Based on the variance of $m$ category of probabilities $p_i(y|x)$ of the model by MC dropout. 
\begin{equation}
\mathcal{U} (\widetilde{x}_j) =  \left( \frac{1}{m} \sum_{i=1}^m \|p_i(\widetilde {y_j}|\widetilde {x_j}) - \mu \|^2 \right) ^{\frac{1}{2}}
\label{eq:mc}
\end{equation}
Where $p_i(\widetilde{y_j}|\widetilde{x_j})$ is the predicted probability of input pixel $\widetilde{x_j}$ in the $i^{th}$ forward propagation, and $\mu$ is the mean prediction ($m$ rounds) of $\widetilde{x_j}$. $\mathcal{U} (\widetilde{x_j})$ thus represents the uncertainty of the source model for pixel-wise target input $\widetilde{x_j}$. 


We then split the whole input image ($x \in \mathbb{R}^{H \times W \times 3}$) into $n_{H}\times n_{W}$ regions to measure the average uncertainty value of each region, $n_{H}$ and $n_{W}$ equal $H/H_{p}$ and $W/W_{p}$ respectively. As shown in Eq.\ref{eq:unc_for_promptregion}, the uncertainty of a region $\mathcal{U} (x_r)$ is the average value of each pixel uncertainty within the region ($H_{p} \times W_{p}$).
\begin{equation}
\mathcal{U}(x_r) = \frac{1}{H_{p} \times W_{p}}
\sum_{j }^{H_P \times W_P} \mathcal{U} (\widetilde{x}_j)
\label{eq:unc_for_promptregion}
\end{equation}
As shown in the bottom of Fig .\ref{fig:DPP}, we sort all regions based on their region uncertainty value and place SVDP on the region of high uncertainty score, which represents the large domain shift. In this way, SVDP can extract target domain knowledge efficiently.




\begin{figure}[t]
\includegraphics[width=0.47\textwidth]{./images/DPP_V5.pdf}
\centering
\vspace{-0.05cm}
\caption{The detailed process of Domain Prompt Placing. The uncertainty map is estimated by MC Dropout \cite{gal2016dropout}.}
\label{fig:DPP}
\vspace{-0.25cm}
\end{figure}







\subsection{Domain prompt updating}
\label{sec:3.5}

For updating, we adopt the widely-used teacher-student framework and exponential moving average (EMA) to achieve the model and prompt updating \cite{gan2022decorate}. Same as previous works\cite{Wangetal2022}, the teacher model ($\mathcal{T}_{mean}$) is updated by EMA from the student model ($\mathcal{S}_{target}$), shown in Eq. \ref{eq:ema}:
\begin{equation}
 \mathcal{T}_{mean}^{t} = \alpha \mathcal{T}_{mean}^{t-1} + (1-\alpha) \mathcal{S}_{target} ^{t}
\label{eq:ema}
\end{equation}
When $t = 0$ ($t$ is the time step), we utilize the source domain pre-trained model to initialize the weight of the teacher and student model. And we set $\alpha = 0.999$ \cite{AnttiTarvainenetal2017}, which is the updating weight of EMA. 

Different from traditional model updating, we design a special Domain Prompt Updating (DPU) strategy for SVDP to efficiently fit in target domain distribution. As shown in Fig .\ref{fig:DPU}, we adopt image-level uncertainty value to reflect the degree of domain shift for each target domain sample. Similar to Eq .\ref{eq:unc_for_promptregion}, we calculate the average uncertainty value ($\mathcal{U}(x)$) for each pixel in the entire image. According to the image-level uncertainty score, we update prompt parameters for the individual sample with different weight EMA. 

\begin{equation}
\label{unc_ema_prompt}
\begin{aligned}
p_{t} = \beta p_{t-1} + (1-\beta) p_{t},
\end{aligned}
\end{equation}
Note that, $p_{t}$ represents the parameters of the SVDP that need to be updated. In DPU, we set the prompt EMA updating rate $\beta = 1-(\mathcal{U}(x) \times 0.01)$.
As shown in the top of Fig .\ref{fig:DPU}, the prompt EMA weight is set to a large value when the input is of high uncertainty score since the large weight can efficiently adapt to the sample with the large data distribution shift.
In this way, we can improve the efficiency of target domain adaptation during test time.



\begin{figure}[t]
\includegraphics[width=0.456\textwidth]{./images/DPU_V5.pdf}
\centering
\vspace{-0.1cm}
\caption{The process of Domain Prompt Updating. We adaptively adjust the prompt EMA updating rate for each target domain sample based on image-level uncertainty value.}
\label{fig:DPU}
\vspace{-0.2cm}
\end{figure}


\subsection{Loss function}
Following previous TTA work ~\cite{Wangetal2022}, we utilize the teacher model to generate the pseudo labels ($\widetilde{y}_t$), which is refined by test-time augmentation and confidence filter.
Then, we adopt consistency loss ($L_{con}$) as the optimization objective for SVDP, which is a pixel-wise cross-entropy loss \cite{xie2021segformer}.
\begin{equation}
 \mathcal{L}_{con}(\widetilde {x}) = - 
 \frac{1}{H \times W} \sum_{w,h}^{W,H}
 \sum_c^C \widetilde{y}_t(w,h,c) \log \hat{y}_t(w,h,c)
\label{eq:loss}
\end{equation}
Where $\hat{y}_t$ is the output of our student model, $C$ means the amount of categories.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
% \input{experiment.tex}
% \clearpage

\begin{table*}[htb]
\caption{\label{dg benchmark} \textbf{Performance comparison of Cityscapes-to-ACDC TTA.} We use Cityscape as the source domain and ACDC as the four target domains in this setting. Source domain data is unavailable, while target domain data is only accessed once during testing.}
\centering
\setlength\tabcolsep{11pt}%调列距
\begin{adjustbox}{width=1\linewidth,center=\linewidth}
\begin{tabular}{c|c|cc|cc|cc|cc|c }
\hline

\multicolumn{2}{c|}{Test-Time Adaptation}          & \multicolumn{2}{c|}{Source2Fog}    & \multicolumn{2}{c|}{Source2Night}     & \multicolumn{2}{c|}{Source2Rain}  & \multicolumn{2}{c|}{Source2Snow}    & \multirow{2}{*}{Mean-mIoU}  \\ \cline{1-10}
Method &REF & mIoU$\uparrow$ & mAcc$\uparrow$ & mIoU$\uparrow$ 
& mAcc$\uparrow$ & mIoU$\uparrow$ & mAcc$\uparrow$ & mIoU$\uparrow$ & mAcc$\uparrow$&  \\ \hline
Source & NIPS2021 \cite{xie2021segformer}&69.1&79.4&40.3&55.6&59.7&74.4&57.8&69.9 &56.7\\ 
TENT  & ICLR2021 \cite{wang2020tent}  &69.0&79.5&  40.3&55.5&  59.9&74.1&  57.7&69.7 &56.7\\ 
CoTTA& CVPR2022\cite{Wangetal2022} &70.9&80.2 &41.2&55.5 &62.6&75.4 &59.8&70.7&58.6\\ 
DePT & ICLR2023\cite{gao2022visual}  &71.0&80.2&40.9&55.8&61.3&74.4&59.5&70.0&58.2\\ 
VDP & AAAI2023\cite{gan2022decorate}  &70.9&80.3&41.2&55.6&62.3&75.5&59.7&70.7&58.5\\ 

\cellcolor{lightgray}\textbf{SVDP} &\cellcolor{lightgray}\textbf{ours} &\cellcolor{lightgray}\textbf{72.5}&\cellcolor{lightgray}\textbf{81.4} &\cellcolor{lightgray}\textbf{45.3}&\cellcolor{lightgray}\textbf{58.9}& \cellcolor{lightgray}\textbf{65.7}&\cellcolor{lightgray} \textbf{76.7} &\cellcolor{lightgray}\textbf{62.2}&\cellcolor{lightgray}\textbf{72.4}&\cellcolor{lightgray}\textbf{61.4}\\\bottomrule

 \hline
\end{tabular}
\end{adjustbox}
\label{tab:TTA}
\vspace{-0.3cm}
\end{table*}
In this section, we conduct extensive experiments to demonstrate the effectiveness of our proposed SVDP for semantic segmentation Domain Adaptation (DA) tasks. Since data privacy and transmission cost limit access to source domain data in many real-world scenarios, we thus conduct extensive semantic segmentation experiments on Test Time Adaptation (TTA) and Continual Test-Time Adaptation (CTTA).
In Sec~\ref{sec:4.1}, we provide the details of the task settings for TTA and CTTA, as well as a description of the datasets. In Sec~\ref{sec:4.2}, we compare our method with other baselines \cite{xie2021segformer, wang2020tent, Wangetal2022, gao2022visual, gao2022visual} in six TTA and one CTTA scenarios. 
Comprehensive ablation studies are conducted in Sec~\ref{sec:4.3}, which investigate the impact of each component.
Besides, we provide qualitative analysis in Appendix~\ref{Sec:qualitative}.


\subsection{Task settings and Datasets}
\label{sec:4.1}
\textbf{TTA and CTTA} are commonly used technology in real-world scenarios in which a pre-trained model adapts to the distribution of an unseen target domain. Both scenarios can only adopt the source domain pre-trained model and can not access source domain data.  
% without relying on source domain data. 
In the TTA, the data in each target domain is unlabeled and can only be accessed once, which makes the efficiency of domain adapting crucial. 
Meanwhile, CTTA is of the same setting as TTA but further sets the target domain constantly changing, bringing more difficulties during test time adaptation.

\textbf{Cityscapes-to-ACDC} is a semantic segmentation task designed for cross-domain learning. And we conduct four TTA and one CTTA experiment on the scenario. The source model is an off-the-shelf pre-trained segmentation model that was trained on the Cityscapes dataset~\cite{cordts2016cityscapes}. 
The ACDC dataset~\cite{sakaridis2021acdc} contains images collected in four different unseen visual conditions: Fog, Night, Rain, and Snow. For the TTA, we adapt the source domains' pre-trained model to each of the four ACDC target domains separately. For the CTTA, we repeat the same sequence of target domains (Fog→Night→Rain→Snow) multiple times to simulate environment changes in real-life scenarios \cite{Wangetal2022,gan2022decorate}.


\textbf{Cityscapes-to-Foggy\&Rainy Cityscapes.}
To demonstrate the generalization of our method, we conducted experiments in this scenario, which is a commonly used benchmark for semantic segmentation TTA scenarios \cite{yu2022cross, wang2021exploring}. In comparison to the Foggy scenario in ACDC, Foggy Cityscapes \cite{sakaridis2018semantic} has a larger dataset and a higher density of the simulated fog, which is a more challenging TTA scene. Besides, we also evaluate the effectiveness of our method on Cityscapes-to-Rainy Cityscapes \cite{sakaridis2018semantic} TTA scenario.


\textbf{Implementation Details.} We follow the basic implementation details \cite{Wangetal2022, gan2022decorate} to set up our semantic segmentation TTA experiments. Specifically, we use the Segformer-B5 model~\cite{xie2021segformer} pre-trained on Cityscapes datasets as our off-the-shelf source model.  
We down-sample the original image size of 1920x1080 of the ACDC dataset to 960x540, which serves as network input.
We evaluate our predictions under the original resolution. 
We use the Adam optimizer~\cite{kingma2014adam}  $(\beta_1, \beta_2) = (0.9, 0.999)$ with a learning rate of 3e-4 and batch size 1 for both TTA and CTTA experiments.
We apply horizontal-flip in both the strong and weak augmentation strategies~\cite{xie2021segformer}. Additionally, we use a range of image resolution scale factors [0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0] for our strong augmentation method. 
Besides, we set the prompt size to $30 \times 30$ and the prompt number to 25.
All experiments are conducted on NVIDIA V100 GPUs.
\subsection{The effectiveness}
\label{sec:4.2}
We show quantitative comparisons between our method and the baselines on six TTA and one CTTA segmentation scenarios, which are measured by mean Intersection over Union~(mIoU) and mean Accuracy~(mAcc). \textbf{For baselines,}
we compare our method with the Segformer~\cite{xie2021segformer}, TENT~\cite{wang2020tent}, CoTTA~\cite{Wangetal2022}, DePT~\cite{gao2022visual}, and VDP~\cite{gao2022visual}, which are cutting-edge approaches in related studies.

\begin{table*}[htb]
\caption{\label{dg benchmark} \textbf{Performance comparison for Cityscape-to-ACDC CTTA.} We take the Cityscape as the source domain and ACDC as the four target domains. During testing, we sequentially evaluate the four target domains multiple times. Mean is the average score of mIoU
for all times. Gain refers to the improvement achieved by the method compared to the Source model.}
\centering
\setlength\tabcolsep{2pt}%调列距
\begin{adjustbox}{width=1\linewidth,center=\linewidth}
\begin{tabular}{c|c|ccccc|ccccc|ccccc|c|c }
\hline

\multicolumn{2}{c|}{Time}     & \multicolumn{15}{c}{$t$ \makebox[10cm]{\rightarrowfill} }                                                                              \\ \hline
\multicolumn{2}{c|}{Round}          & \multicolumn{5}{c|}{1}    & \multicolumn{5}{c|}{2}     & \multicolumn{5}{c|}{3}  & \multirow{2}{*}{Mean$\uparrow$}   & \multirow{2}{*}{Gain}  \\ \cline{1-17}
Method & REF & Fog & Night & Rain & Snow & Mean$\uparrow$ & Fog & Night & Rain & Snow  & Mean$\uparrow$ & Fog & Night & Rain & Snow & Mean$\uparrow$ & \\ \hline
Source & NIPS2021 \cite{xie2021segformer}  &69.1&40.3&59.7&57.8&56.7&69.1&40.3&59.7& 	57.8&56.7&69.1&40.3&59.7& 57.8&56.7&56.7&/\\ 
TENT & ICLR2021 \cite{wang2020tent}  &69.0&40.2&60.1&57.3&56.7&68.3&39.0&60.1& 	56.3&55.9&67.5&37.8&59.6&55.0&55.0&55.7&-1.0\\ 
CoTTA & CVPR2022\cite{Wangetal2022}  &70.9&41.2&62.4&59.7&58.6&70.9&41.1&62.6& 	59.7&58.6&70.9&41.0&62.7&59.7&58.6&58.6&+1.9\\ 
DePT & ICLR2023\cite{gao2022visual} 
&71.0&40.8&58.2&56.8&56.5&68.2&40.0&55.4&53.7& 54.3&66.4&38.0&47.3&47.2&49.7&53.4&-3.3\\
VDP & AAAI2023\cite{gan2022decorate}  &70.5&41.1&62.1&59.5&  58.3    &70.4&41.1&62.2&59.4& 58.2     & 70.4&41.0&62.2&59.4& 58.2   &  58.2 & +1.5\\
\cellcolor{lightgray}\textbf{SVDP} &\cellcolor{lightgray}\textbf{ours} &\cellcolor{lightgray}\textbf{72.5}&\cellcolor{lightgray}\textbf{45.9}&\cellcolor{lightgray}\textbf{67.0}&\cellcolor{lightgray}\textbf{64.1}&\cellcolor{lightgray}\textbf{62.4}& 
 \cellcolor{lightgray}\textbf{72.2}&\cellcolor{lightgray}\textbf{44.8}&\cellcolor{lightgray}\textbf{67.3}&\cellcolor{lightgray}\textbf{64.1}&\cellcolor{lightgray}\textbf{62.1} 
 &\cellcolor{lightgray}\textbf{72.0}&\cellcolor{lightgray}\textbf{44.5}&\cellcolor{lightgray}\textbf{67.6}&\cellcolor{lightgray}\textbf{64.2}&\cellcolor{lightgray}\textbf{62.1}      &\cellcolor{lightgray}\textbf{62.2} 
 &\cellcolor{lightgray}+\textbf{5.5}\\\bottomrule
 \hline
\end{tabular}
\end{adjustbox}
\vspace{-0.3cm}
\label{tab:CTTA}
\end{table*}


\textbf{Cityscapes-to-ACDC TTA.}
We evaluate the performance of the proposed SVDP on four scenarios with significant domain gap during TTA. 
Tab .\ref{tab:TTA} shows that the Mean-mIoU for the four domains using the source domain model alone is only 56.7\%. Recent advanced methods CoTTA increases it to 58.6\% while our method further increases it by 2.8\%. In the night domain with the largest domain gap, our method increased mIoU by 4.1\% and mAcc by 3.3\%, compared with the previous State-Of-The-Art (SOTA) method (VDP). 
These results demonstrate that our method can better address the domain shit problem in test time compared to other methods. Additionally, compared to the VDP that also utilizes domain prompts, our method can avoid the occlusion problem and better extract local target domain knowledge for semantic segmentation TTA.

\textbf{Cityscapes-to-(Foggy\&Rainy) Cityscapes TTA.}
To further demonstrate the effectiveness of our method, we evaluate the performance of SVDP on the CityScapes-to-Foggy Cityscapes benchmark. As illustrated in Tab.~\ref{tab:foggycityscapes}, our approach achieved a 2.4\% higher mIoU than the previous SOTA model (CoTTA). We observe that the results show the same trend as the above TTA experiments. 
In the CityScapes-to-Rainy Cityscapes TTA benchmark, compared with CoTTA, our SVDP improved mIoU and mAcc by 2.9\% and 1.6\%.
These results prove that our method is generalized for addressing different domain shifts in the semantic segmentation TTA setting.
\begin{table}[!tb]
\caption{\textbf{Performance comparison for Cityscape-to-(Foggy\&Rainy) Cityscape TTA.}}
\label{tab:foggycityscapes}
\centering
\setlength\tabcolsep{3pt}%调列距
\renewcommand\arraystretch{1.0}%调行距
\begin{tabular}{c|cccccc}
\toprule
Foggy & \makecell*[c]{Source} & \makecell*[c]{TENT} & \makecell*[c]{CoTTA} & \makecell*[c]{DePT} & \makecell*[c]{VDP} & \makecell*[c]{SVDP} \\ \midrule
mIoU&69.2 &69.3 &72.1 &71.9 &71.8 & \textbf{74.5}\\ 
mAcc &79.1 &79.0 &79.4 &80.2& 80.0 & \textbf{82.3}\\ 
\hline\hline

Rainy & \makecell*[c]{Source} & \makecell*[c]{TENT} & \makecell*[c]{CoTTA} & \makecell*[c]{DePT} & \makecell*[c]{VDP} & \makecell*[c]{SVDP} \\\midrule
mIoU&58.4 &58.7 &62.4 &61.2 &63.7 & \textbf{65.3}\\ 
mAcc &71.5 &71.4 &74.0 &72.4 &73.1 &\textbf{75.6}\\ 
\bottomrule
\end{tabular}
\vspace{-0.2cm}
\label{tab:prompt place}
\end{table}

\textbf{Cityscapes-to-ACDC CTTA.} 
To demonstrate that our method can also address continuously changing domain shifts, we deal with the four domain data during test time periodically \cite{Wangetal2022}.
As shown in Tab .\ref{tab:CTTA}, due to error accumulation and catastrophic forgetting, the performance of TENT and DePT gradually decreases over time. These methods only focus on acquiring new domain-specific knowledge from the target domain, resulting in a neglect of the original knowledge from the source domain. 
And we found that our method gains 3.6\% increase of mIoU more than the previous SOTA CTTA method \cite{Wangetal2022}. 
The results prove that our method shows the ability to avoid error accumulation and catastrophic forgetting phenomena in semantic segmentation CTTA problems. 
Specifically, our method introduces Domain Prompt Placement (DPP) scheme to only extract more target domain-specific knowledge and adopt the Domain Prompt Updating (DPU) method to adaptively preserve the original knowledge of the source domain by dynamic adjustment of the prompt updating weight.

Overall, our method outperforms several previous SOTA methods on all semantic segmentation TTA and CTTA tasks and shows promising potential for real-world applications.


\subsection{Ablation study}
\label{sec:4.3}
In this subsection, we evaluate the contribution of each component in our method. Since the Night domain is the most challenging scenario for camera-based methods, we conduct the ablation study on the Cityscapes-to-ACDC Night TTA. Due to the limitation of space, we show the ablation study on other scenarios in Appendix \ref{Sec:ablation ctta}.


\begin{table}[!tb]
\caption{\textbf{Ablation: Contribution of each component. % need to be modified lower is better
}}
\label{ablationDAP}
\centering
\setlength\tabcolsep{5pt}%调列距
\renewcommand\arraystretch{1}%调行距
\begin{tabular}{c|cccc|cc}
\toprule
 & \makecell*[c]{TS} & \makecell*[c]{SVDP} & \makecell*[c]{DPP} & \makecell*[c]{DPU}  & mIoU$\uparrow$ & mAcc$\uparrow$\\\midrule
$Ex_{1}$ &  & & & &40.3& 55.6 \\ 
$Ex_{2}$& \checkmark &  & &  & 41.2& 55.5\\
$Ex_{3}$ &\checkmark  & \checkmark &  & & 43.7& 56.9\\
$Ex_{4}$  & \checkmark & \checkmark &\checkmark  &  &44.5& 58.1\\
$Ex_{5}$  & \checkmark & \checkmark & &\checkmark & 44.4& 58.2\\
$Ex_{6}$ & \checkmark &  \checkmark &\checkmark  &\checkmark & 45.3& 58.9\\
\bottomrule
\end{tabular}
\vspace{-0.2cm}
\label{tab:ablation}
\end{table}
\textbf{Effectiveness of each component.} 
As presented in Tab.~\ref{tab:ablation} $Ex_{2}$, 
Teacher-student~(TS) structure is a common technique in TTA \cite{liu2022unsupervised, Wangetal2022, gan2022decorate}, which is used to generate pseudo label in the target domain and only has 0.9\% mIoU improvement without our method. This verifies the improvement of our method does not come from the usage of this prevalent scheme.
In $Ex_{3}$, by introducing sparse prompts (SVDP), we observe that the mIoU and mAcc increase 2.5\% and 1.4\%, respectively. The result demonstrates that SVDP facilitates addressing the domain shift problem, since it can extract local target domain knowledge without damaging the original semantic information.
As shown in $Ex_{4}$, DPP achieves further 0.8\% mIoU and 1.2\% mAcc improvement since the specially designed prompt placement strategy can assist SVDP in extracting more target domain-specific knowledge.  
Compared with $Ex_{3}$, DPU ($Ex_{5}$) also improves the mIoU and mAcc by 0.7\% and 1.3\% respectively. The results prove the effectiveness of DPU and show the importance of adaptively optimizing for different samples during testing.
$Ex_{6}$ shows the complete combination of  all components which achieves 5.0\% mIoU improvement in total. It proves that all components compensate each other and jointly address the semantic segmentation domain shift problem in test time.

\textbf{How does the prompt sparsity affect the performance?}
As shown in Fig .\ref{fig:exp_prompt_sparsity}, we investigate the performance impact caused by the sparsity of SVDP.  Specifically, we gradually increase the density of SVDP pixel-wise parameters and record the corresponding mIoU values. We find that mIoU initially improves along with increasing SVDP density and then starts to decrease when the density exceeds 60\%.
This observation suggests that when SVDP is sparse, it fails to capture the domain-specific knowledge effectively due to the limited number of parameters. In contrast, if the SVDP becomes too dense, the prompt will occlude continuous spatial details, leading to segmentation performance degradation.
Therefore, it is crucial to strike a balance on the degree of prompt sparsity and we consider that SVDP can achieve optimal potential in 50\% sparsity.



\textbf{How do the prompt size and number influence the performance?}
According to Fig .\ref{fig:exp_prompt_size}, we observe that changing the prompt size has a small effect on performance, with a small variance of 0.2\%$\sim$1.0\% mIoU. Experimental results presented in \cite{MenglinJia2022VisualPT} also support this finding that the prompt size is not sensitive. This observation provides the opportunity for us to deal with different input sizes with different prompt sizes.  
Regarding the prompt number, the results show that using 25 prompts can achieve the highest segmentation performance. 
This suggests that when the number of prompts is too small, the available parameters may not be sufficient to fit in the target domain knowledge. However, when the number of prompts reaches a certain value, increasing the number of prompts may not lead to a significant performance improvement, and may even result in some performance degradation due to occlusion.


\begin{figure}[t]
\includegraphics[width=0.48\textwidth]{./images/fig6_sparsity_V2.pdf}
\centering
% \vspace{-0.2cm}
\caption{Effect of prompts' sparsity}
\label{fig:exp_prompt_sparsity}
\vspace{-0.58cm}
\end{figure}
\begin{figure}[t]
\includegraphics[width=0.45\textwidth]{./images/figure5_V2.pdf}
\centering
\vspace{-0.05cm}
\caption{Effect of prompts' size and number}
\label{fig:exp_prompt_size}
% \vspace{-0.1cm}
\end{figure}

\textbf{How do the prompt placement strategies influence the performance?}
According to Tab .\ref{tab:prompt place}, we compare the performance of several commonly used prompt placement methods with our DPP~(Domain Prompt Placement). Center and Corners stand for placing sparse prompts on the center and corner of the input image, while the Grid represents that sparse prompts are distributed uniformly on the image with equal distance. Different from DPP, CPP leverage the confidence score to reflect the degree of domain shift and select the prompt placement regions. As we can see, the mIoU of CPP and DPP are obviously higher than other methods, which demonstrate the SVDP should be set on the regions with large domain shift. Meanwhile, compared with CPP, DPP further improves 0.8\% mIoU and 0.7\% mAcc since the uncertainty scheme is more suitable for measuring domain shift problems in semantic segmentation task. Though the confidence score is a straightforward measurement to reflect the reliability of prediction, it is less trustworthy and fluctuates irregularly in pixel-wise cross-domain scenarios. In contrast, the uncertainty value is relatively more reliable in reflecting the degree of domain shift and more reasonable in guiding the domain prompt placement.

\begin{table}[!tb]
\caption{\textbf{Effects of prompts' placement on the TTA task.}}
\label{ablationDAP}
\centering
\setlength\tabcolsep{3pt}%调列距
\renewcommand\arraystretch{1.0}%调行距
\begin{tabular}{ccccccc}
\toprule
& \makecell*[c]{Center} & \makecell*[c]{Corners}& \makecell*[c]{Grid}& \makecell*[c]{Random}& \makecell*[c]{CPP}& \makecell*[c]{DPP}\\\midrule
mIoU$\uparrow$&43.5 &43.8&44.1&44.1&44.5&45.3\\ 
mAcc$\uparrow$ &56.9&56.6&57.4&57.7&58.2 &58.9\\ 
\bottomrule
\end{tabular}
\vspace{-0.2cm}
\label{tab:prompt place}
\end{table}


\begin{figure*}[ht]
\centering
\includegraphics[width=\linewidth]{./images/visual_1.pdf}
\vspace{-0.39cm}
\caption{Qualitative comparison of SVDP with previous SOTA method: CoTTA\cite{Wangetal2022}, VDP\cite{gan2022decorate} on ACDC Fog, Night, Rain, and Snow four scenarios. SVDP could better segment different pixel-wise classes such as shown in the white box.}
\label{fig:qualitative} 
\vspace{-0.3cm}
\end{figure*}

\section{Conclusion and discussion of limitations}
In this paper, we are the first to introduce the Sparse Visual Domain Prompt (SVDP) in semantic segmentation DA tasks, which address the problem of inaccurate contextual information extraction and insufficient domain-specific feature transferring caused by dense prompt occlusion. Moreover, the Domain Prompt Placement (DPP) and Domain Prompt Updating (DPU) strategies are specially designed for applying SVDP to ease the domain shift better. Extensive experiments on multiple TTA and CTTA scenarios demonstrate that our method achieves SOTA performance and efficiently tackles the domain shift. For limitations, the teacher-student framework brings more computational costs during SVDP tuning. However, the forward time and computational costs are the same as the baseline in testing.







%%%%%%%%% ABSTRACT






\section*{Supplementary Material}

The supplementary material presented in this paper offers a comprehensive analysis and additional experimental results of Sparse Visual Domain Prompts (SVDP). Specifically, Sec.~\ref{Sec:qualitative} presents the qualitative analysis and comparison results of the SVDP. Sec.~\ref{Sec:ablation ctta} provides additional ablation studies of our method in the Continual Test-Time Adaptation (CTTA) scenario. Sec.~\ref{Sec: sentivity} investigates the parameter sensitivity of Domain Prompt Updating (DPU). In addition, Sec.~\ref{Sec: quantitative} provides more quantitative results of the TTA and CTTA scenarios. Furthermore, in Sec.~\ref{Sec: UDA}, we extend our method to the Unsupervised Domain Adaptation (UDA) scenario, demonstrating its strong generalization capabilities and plug-and-play characteristics. Finally, in Sec.~\ref{Sec:related}, we present related work on semantic segmentation. \href{https://github.com/ICCV2595/SVDP}{The code will be released.}



\appendix
\section{Experiment results and analysis}
\subsection{Qualitative analysis}
\label{Sec:qualitative}

To further demonstrate the effectiveness of our proposed method, SVDP, we conduct a qualitative comparison with two current leading methods, CoTTA \cite{Wangetal2022} and VDP \cite{gan2022decorate}, on the CTTA scenario (Cityscapes-to-ACDC). 


The results of the comparison are presented in Fig .\ref{fig:qualitative}. 
In the foggy target-domain, we highlight a white box that contains a tall and dark wall object. This object is difficult to segment as it shares characteristics with the \emph{building} class. Our proposed method, SVDP, has a significant advantage in dealing with such confusing semantic segmentation categories with high uncertainty, thanks to the contribution of the Domain Prompt Placement (DPP) method. Our method also outperforms CoTTA and VDP in the remaining three domains. In the night and rain target-domains, due to blurring and occlusions, CoTTA and VDP fail to identify the traffic sign, while our method successfully segments it. In the snow domain, our proposed method correctly distinguish the sidewalk from the road, avoiding misclassification.
Overall, our method can achieve better local segmentation results and neglect the influence of local domain shift.
And our method produces finer results than the previous state-of-the-art methods, with clear visual improvement.

\subsection{Additional ablation studies}
\label{Sec:ablation ctta}
\begin{table}[!tb]
\caption{\textbf{Ablation: Contribution of each component (CTTA). % need to be modified lower is better
}}
\label{tab:ablation}
\centering
\setlength\tabcolsep{5pt}%调列距
\renewcommand\arraystretch{1}%调行距
\begin{tabular}{c|cccc|cc}
\toprule
 & \makecell*[c]{TS} & \makecell*[c]{SVDP} & \makecell*[c]{DPP} & \makecell*[c]{DPU}  & mIoU$\uparrow$ & mAcc$\uparrow$\\\midrule
$Ex_{1} $ &  & & & &56.7& 68.8 \\ 
$Ex_{2} $& \checkmark &  & &  & 58.5& 70.4\\
$Ex_{3}$ &\checkmark  & \checkmark &  & & 60.5& 71.8\\
$Ex_{4}$  & \checkmark & \checkmark &\checkmark  &  &61.4& 72.3\\
$Ex_{5}$  & \checkmark & \checkmark & &\checkmark & 61.3& 72.4\\
$Ex_{6} $ & \checkmark &  \checkmark &\checkmark  &\checkmark & 62.4& 73.0\\
\bottomrule
\end{tabular}
\vspace{-0.2cm}
\label{tabsup:ablation}
\end{table}


\begin{figure}[t]
\includegraphics[width=0.47\textwidth]{./images/dpu_senstivity_V5.pdf}
\centering
\vspace{-0.05cm}
\caption{Sensitivity Analysis: The effect of prompt EMA's parameter $\beta$ on semantic segmentation performance in the CTTA scenario.}
\label{fig:senstivity}
\vspace{-0.25cm}
\end{figure}

The proposed method comprises a Sparse Visual Prompt (SVDP), a Domain Prompt Placement (DPP) strategy, and a Domain Prompt Updating (DPU) strategy to mitigate domain shifts in semantic segmentation tasks. In this study, we conduct ablation experiments on the most challenging CTTA scenario (Cityscapes-to-ACDC) to evaluate the effectiveness of each component.

To compare the performance of our method with and without using the teacher-student (TS) structure, a common technique in CTTA used to generate pseudo labels in the target domain, we present the results in Tab.~\ref{tabsup:ablation} $Ex_{2}$. The results show that without our method, TS only has a 0.8\% mIoU improvement, indicating that our method's improvement does not come from the usage of this prevalent scheme.
In $Ex_{3}$, we introduce SVDP to extract local target domain knowledge without damaging the original semantic information. The results demonstrate that SVDP achieves a 2.0\% mIoU and 1.4\% mAcc improvement, effectively addressing the domain shift problem. In $Ex_{4}$, DPP achieves a further 0.9\% mIoU and 0.5\% mAcc improvement by serving as a specially designed prompt placement strategy to assist SVDP in extracting more target domain-specific knowledge.
We evaluate the effectiveness of the DPU in $Ex_{5}$, which adaptively optimizes for different samples during testing. The results show that DPU improves the mIoU and mAcc by 0.8\% and 0.6\%, respectively. Finally, in $Ex_{6}$, we show the complete combination of all components, which achieves a total of 5.7\% mIoU improvement. These results demonstrate that all components of our method effectively address the semantic segmentation domain shift and compensate for each other to achieve superior performance.














\subsection{Additional sensitivity analysis}
\label{Sec: sentivity}

In Sec 3.5, we utilize Eq.\ref{unc_ema_prompt} to update the prompt parameters and conduct an analysis of the sensitivity of the parameter $\beta$ in the CTTA scenario. As depicted in Fig.~\ref{fig:senstivity}, we investigate the impact of $\beta$ values on the performance. Specifically, we gradually increase the value of $\beta$ and record the corresponding mIoU values. We observe that the mIoU improves with increasing $\beta$; however, it starts to decrease once $\beta$ exceeds 0.999. Compare with the best fixed $\beta$ value, our proposed DPU (\textcolor{red}{red line}) strategy can further achieve 1.0\% mIoU improvement. Therefore, due to the different degrees of domain shift, we need to update prompt parameters for the each sample with different EMA weights.


\subsection{Additional quantitative results}
\label{Sec: quantitative}

We present a comprehensive presentation of experimental results on the Test-time adaptation task for Cityscapes-to-ACDC, as shown in Tab .~\ref{table:fog acdc} - Tab .~\ref{table:snow acdc}. Our findings suggest that our proposed approach can better address the domain shift problem and achieve better IoU value in most categories.


\begin{table*}
\caption{Performance Comparison for \textbf{Cityscapes-to-ACDC Fog domain in TTA scenario}. The IoU score of each class and the mIoU score are reported. The bests results are highlighted in \textbf{bold}.
  } \label{table:fog acdc}
  \centering
  \resizebox{0.99\textwidth}{!}{
    \def\arraystretch{1.1}
    \begin{tabular}{ l | c c c c c c c c c c c c c c c c c c c | c }
        \Xhline{1.2pt}
        Method & \rotatebox{0}{road} & \rotatebox{0}{side.} & \rotatebox{0}{buil.} & \rotatebox{0}{wall} & \rotatebox{0}{fence} & \rotatebox{0}{pole} & \rotatebox{0}{light} & \rotatebox{0}{sign} & \rotatebox{0}{veg.} & \rotatebox{0}{terr.} & \rotatebox{0}{sky} & \rotatebox{0}{pers.} & \rotatebox{0}{rider} & \rotatebox{0}{car}& \rotatebox{0}{truck} & \rotatebox{0}{bus} & \rotatebox{0}{train} & \rotatebox{0}{mbike} & \rotatebox{0}{bike} & mIoU \\
        \hline
        \hline
        Source  & 94.0 & 63.9 & 79.8 & 55.7 & 24.9 & 45.0 & 41.5 & 69.8 & 86.6 & 71.0 & 97.6 & 64.1 & 66.2 & 87.4 & 73.0 & 92.6 & 87.7 & 50.2 & 61.7 & 69.1 \\
        TENT & 94.0 & 64.0 & 79.7 & 55.4 & 24.6 & 44.6 & 41.4 & 69.9 & 86.7 & 71.1 & 97.6 & 64.0 & 65.9 & 87.4 & 73.0 & 92.6 & 88.0 & 50.2 & 61.9 & 69.0 \\
        CoTTA & 93.9 & 63.6 & \textbf{80.0} & 55.5 & 25.1 & 49.0 & 43.4 & 73.0 & 87.0 & 70.7 & 97.8 & 68.6 & 71.3 & 87.1 & 74.8 & \textbf{93.6} & 89.1& 58.0 & 66.7 & 70.9 \\
        DePT & 94.0 & 64.0 & 79.9 & 56.1 & 25.3 & 48.8 & 43.5 & 73.0 & 87.1 & 70.6 & 97.5 & 67.9 & 71.5 & 87.3 & 75.1 & 93.5 & 89.1& 57.4 & 66.5 & 71.0 \\
        VDP & 93.9 & 63.6& \textbf{80.0} & 55.6 & 25.1& 49.0& 43.4 & 73.0& 86.9 & 70.7 & 97.7 & 68.5 & 71.1 & 87.2 & 74.7 & 93.5 & 89.2 & 57.9 & 66.6 & 70.9 \\
        \cellcolor{lightgray}\textbf{SVDP} &\cellcolor{lightgray}\textbf{94.8} &\cellcolor{lightgray}\textbf{68.7}&\cellcolor{lightgray}79.6&\cellcolor{lightgray}\textbf{59.9}&\cellcolor{lightgray}\textbf{25.8}&\cellcolor{lightgray}\textbf{50.7}&\cellcolor{lightgray}\textbf{44.3}&\cellcolor{lightgray}\textbf{73.5}&\cellcolor{lightgray}\textbf{87.3}&\cellcolor{lightgray}\textbf{71.1}&\cellcolor{lightgray}\textbf{97.8}&\cellcolor{lightgray}\textbf{70.1}&\cellcolor{lightgray}\textbf{72.9} 
 &\cellcolor{lightgray}\textbf{87.3}&\cellcolor{lightgray}\textbf{76.5}&\cellcolor{lightgray}93.2&\cellcolor{lightgray}\textbf{91.2}&\cellcolor{lightgray}\textbf{65.4}      &\cellcolor{lightgray}\textbf{68.6} 
 &\cellcolor{lightgray}\textbf{72.6}\\\bottomrule
       
        
        \Xhline{1.2pt}
    \end{tabular}
  }

\end{table*}

\begin{table*}
\caption{Performance Comparison for \textbf{Cityscapes-to-ACDC Night domain in TTA scenario}. The IoU score of each class and the mIoU score are reported. The bests results are highlighted in \textbf{bold}.
  } \label{table:night acdc}
  \centering
  \resizebox{0.99\textwidth}{!}{
    \def\arraystretch{1.1}
    \begin{tabular}{ l | c c c c c c c c c c c c c c c c c c c | c }
        \Xhline{1.2pt}
        Method & \rotatebox{0}{road} & \rotatebox{0}{side.} & \rotatebox{0}{buil.} & \rotatebox{0}{wall} & \rotatebox{0}{fence} & \rotatebox{0}{pole} & \rotatebox{0}{light} & \rotatebox{0}{sign} & \rotatebox{0}{veg.} & \rotatebox{0}{terr.} & \rotatebox{0}{sky} & \rotatebox{0}{pers.} & \rotatebox{0}{rider} & \rotatebox{0}{car}& \rotatebox{0}{truck} & \rotatebox{0}{bus} & \rotatebox{0}{train} & \rotatebox{0}{mbike} & \rotatebox{0}{bike} & mIoU \\
        \hline
        \hline
        Source  & 87.6 & 46.3 & 61.8 & 27.0 & 25.3 & 40.8 & 38.7 & 39.4 & 47.7& 26.8 & \textbf{11.4} & 48.6 & 39.9 & 76.1 & 15.9 & 24.2 & 52.0 & 26.5 & 29.6 & 40.3 \\
        TENT & 87.7 & 46.4 & 61.9 & 27.1 & 25.2 & 40.8 & 38.8 & 39.3  & 47.0 & 26.8 & 9.6 & 48.7 & 40.0 & 76.2 & 16.1 & 24.3 & 51.9 & 26.6& 29.7 & 40.2 \\
        CoTTA & 87.6 & 46.7 & \textbf{62.3}& 27.2 & 25.0 & 44.0 & 42.9 & 40.8 & 47.2 & 26.7 & 8.8 & 51.8 & 41.9 & 76.6 & 18.8 & 22.4 & 51.7& \textbf{27.8} & 32.1 & 41.2 \\
        DePT & 87.3 & 46.5 & 62.0 & 27.0 & 25.3 & 43.5 & 40.9 & 41.0 & 47.2 & 26.6 & 8.8 & 51.0 & 42.5 & 77.1 & 17.5 & 23.0 & 51.5 & 26.4 & 31.7 & 40.9 \\
        VDP & 87.6 & 46.8 & 62.2 & 27.1 & 25.0 & 44.0 & 42.9 & 41.0 & 47.3 & 26.6 & 9.0 & 51.7 & 41.9 & 76.6& 18.7 & 23.2 & 51.9 & \textbf{27.8}& 32.0 & 41.2 \\
        \cellcolor{lightgray}\textbf{SVDP} &\cellcolor{lightgray}\textbf{91.9} &\cellcolor{lightgray}\textbf{64.7}&\cellcolor{lightgray}51.4&\cellcolor{lightgray}\textbf{31.2}&\cellcolor{lightgray}\textbf{25.9}&\cellcolor{lightgray}\textbf{50.2}&\cellcolor{lightgray}\textbf{47.1}&\cellcolor{lightgray}\textbf{46.4}&\cellcolor{lightgray}\textbf{58.8}&\cellcolor{lightgray}\textbf{28.3}&\cellcolor{lightgray}0.4&\cellcolor{lightgray}\textbf{55.0}&\cellcolor{lightgray}\textbf{45.7} 
 &\cellcolor{lightgray}\textbf{79.3}&\cellcolor{lightgray}\textbf{23.0}&\cellcolor{lightgray}\textbf{26.6}&\cellcolor{lightgray}\textbf{70.2}&\cellcolor{lightgray}27.7     &\cellcolor{lightgray}\textbf{36.2} 
 &\cellcolor{lightgray}\textbf{45.3}\\\bottomrule
       
        
        \Xhline{1.2pt}
    \end{tabular}
  }

\end{table*}

\begin{table*}[!htb]
\caption{Performance Comparison for \textbf{Cityscapes-to-ACDC Rain domain in TTA scenario}. The IoU score of each class and the mIoU score are reported. The bests results are highlighted in \textbf{bold}.
  } \label{table:rain acdc}
  \centering
  \resizebox{0.99\textwidth}{!}{
    \def\arraystretch{1.1}
    \begin{tabular}{ l | c c c c c c c c c c c c c c c c c c c | c }
        \Xhline{1.2pt}
        Method & \rotatebox{0}{road} & \rotatebox{0}{side.} & \rotatebox{0}{buil.} & \rotatebox{0}{wall} & \rotatebox{0}{fence} & \rotatebox{0}{pole} & \rotatebox{0}{light} & \rotatebox{0}{sign} & \rotatebox{0}{veg.} & \rotatebox{0}{terr.} & \rotatebox{0}{sky} & \rotatebox{0}{pers.} & \rotatebox{0}{rider} & \rotatebox{0}{car}& \rotatebox{0}{truck} & \rotatebox{0}{bus} & \rotatebox{0}{train} & \rotatebox{0}{mbike} & \rotatebox{0}{bike} & mIoU \\
        \hline
        \hline
        Source  & 82.3 & 47.1 & 89.5 & 36.8 & 26.6 & 51.0 & 64.8 & 62.9 & 89.5 & 60.3 & 97.8 & 46.0 & 53.0 & 81.1 & 25.3 & 65.4 & 56.7 & 47.6 & 51.2 & 59.7 \\
        TENT & 82.4 & 46.7 & 89.6 & 37.3 & 27.0 & 50.6 & 64.6 & 62.9 & 89.5 & 60.4 & 97.7 & 46.7 &54.5 & 81.2 & 25.4 & 65.2 & 56.6 & 47.3 & 51.8 & 59.9 \\
        CoTTA & 83.0 & 48.4& 90.2 & 38.3 & 28.0 & 55.5 & 68.1 & 67.5& 90.3 &61.2 & 98.0 & 54.1 & 60.1 &82.0 & 27.4 & 67.0 & 59.1 & 50.9 & 55.4 & 62.6 \\
        DePT & 82.0 & 47.3 & 89.8& 37.5 & 26.9 & 53.0 & 66.2 & 65.8 & 89.6 & 60.8 & 97.7 & 52.8 & 59.5 & 81.6 & 26.5 & 66.5 & 57.9 & 49.5 & 53.2 & 61.3 \\
        VDP  & 83.0 & 48.3 & 90.2 & 38.2 & 28.0 & 55.5 & 68.2 &67.5 & 90.2 & 61.2 & 97.9 & 54.1 & 59.9 & 82.0 & \textbf{27.5} & 67.0 & 59.1 & 50.9 & 55.3 & 62.3 \\
        \cellcolor{lightgray}\textbf{SVDP} &\cellcolor{lightgray}\textbf{85.4} &\cellcolor{lightgray}\textbf{55.3}&\cellcolor{lightgray}\textbf{91.3}&\cellcolor{lightgray}\textbf{43.4}&\cellcolor{lightgray}\textbf{30.9}&\cellcolor{lightgray}\textbf{58.7}&\cellcolor{lightgray}\textbf{70.3}&\cellcolor{lightgray}\textbf{70.4}&\cellcolor{lightgray}\textbf{91.1}&\cellcolor{lightgray}\textbf{64.0}&\cellcolor{lightgray}\textbf{98.2}&\cellcolor{lightgray}\textbf{55.9}&\cellcolor{lightgray}\textbf{60.9} 
 &\cellcolor{lightgray}\textbf{82.1}&\cellcolor{lightgray}25.6&\cellcolor{lightgray}\textbf{79.4}&\cellcolor{lightgray}\textbf{73.3}&\cellcolor{lightgray}\textbf{54.6}      &\cellcolor{lightgray}\textbf{59.1} 
 &\cellcolor{lightgray}\textbf{65.7}\\\bottomrule
       
        
        \Xhline{1.2pt}
    \end{tabular}
  }

\end{table*}



\begin{table*}[!htb]
\caption{Performance Comparison for \textbf{Cityscapes-to-ACDC Snow domain in TTA scenario}. The IoU score of each class and the mIoU score are reported. The bests results are highlighted in \textbf{bold}.
  } \label{table:snow acdc}
  \centering
  \resizebox{0.99\textwidth}{!}{
    \def\arraystretch{1.1}
    \begin{tabular}{ l | c c c c c c c c c c c c c c c c c c c | c }
        \Xhline{1.2pt}
        Method & \rotatebox{0}{road} & \rotatebox{0}{side.} & \rotatebox{0}{buil.} & \rotatebox{0}{wall} & \rotatebox{0}{fence} & \rotatebox{0}{pole} & \rotatebox{0}{light} & \rotatebox{0}{sign} & \rotatebox{0}{veg.} & \rotatebox{0}{terr.} & \rotatebox{0}{sky} & \rotatebox{0}{pers.} & \rotatebox{0}{rider} & \rotatebox{0}{car}& \rotatebox{0}{truck} & \rotatebox{0}{bus} & \rotatebox{0}{train} & \rotatebox{0}{mbike} & \rotatebox{0}{bike} & mIoU \\
        \hline
        \hline
        Source  & 79.8 & 40.8 & 86.9 & 43.6 & 46.5 & 56.4 & 72.3 & 65.5 & 82.9 &  \textbf{5.7} & 97.1 & 62.8 & 40.4 & 85.4 & 54.7 & 44.5 & 73.1 & 22.7 & 36.0 & 57.8 \\
        TENT & 79.6 & 40.0 & 86.8 & 43.4 & 46.5 & 56.1 & 72.2 & 65.6 & 82.9 &  \textbf{5.7} & 97.1 & 63.0 & 40.9 & 85.5 & 54.7 & 43.1 & 72.7& 23.0 & 36.5& 57.7\\
        CoTTA & 80.1 & 40.7 & 87.5 & 43.9 & 47.7 & 59.9 & 75.3 & 69.2 & 84.0 & 5.1 & 97.2 & 67.3 & 46.9 & 86.2 & 56.1& 43.4 & 74.1 &  \textbf{25.7}& 43.3 & 59.8 \\
        DePT & 79.1 & 40.6 & 86.8 & 43.4 & 47.5 & 59.8 & 75.1 & 69.4 & 83.5 & 5.2 & 97.1 & 67.2 & 46.5 & 86.3 & 56.0 & 44.0 & 73.9 & 25.6 & 43.1 & 59.5 \\
        VDP & 80.1 & 40.8 & 87.5 & 43.9 & 47.8 & 59.9 & 75.1 & 69.4 & 83.9 & 5.1 & 97.2 & 67.2 & 46.7 & 86.2 & 56.2 & 43.9 & 74.0 & 25.7 & 42.9 & 59.7 \\
        \cellcolor{lightgray}\textbf{SVDP} &\cellcolor{lightgray}\textbf{81.9} &\cellcolor{lightgray}\textbf{44.8}&\cellcolor{lightgray}\textbf{89.3}&\cellcolor{lightgray}\textbf{52.8}&\cellcolor{lightgray}\textbf{50.7}&\cellcolor{lightgray}\textbf{62.8}&\cellcolor{lightgray}\textbf{76.5}&\cellcolor{lightgray}\textbf{71.2}&\cellcolor{lightgray}\textbf{85.2}&\cellcolor{lightgray}4.0&\cellcolor{lightgray}\textbf{97.7}&\cellcolor{lightgray}\textbf{68.0}&\cellcolor{lightgray}\textbf{47.3} 
 &\cellcolor{lightgray}\textbf{87.0}&\cellcolor{lightgray}\textbf{59.1}&\cellcolor{lightgray}\textbf{53.8}&\cellcolor{lightgray}\textbf{74.9}&\cellcolor{lightgray}25.1      &\cellcolor{lightgray}\textbf{49.5} 
 &\cellcolor{lightgray}\textbf{62.2}\\\bottomrule
       
        
        \Xhline{1.2pt}
    \end{tabular}
  }

\end{table*}

\begin{table*}[!htb]
\caption{Performance Comparison for \textbf{Cityscapes-to-ACDC UDA}. We follow the setting of DAFormer\cite{hoyer2022daformer}, taking the Cityscape as the source domain and the entire ACDC's dataset as the target domain. The IoU score of each class and the mIoU score are reported on the ACDC validation set. The bests results are highlighted in \textbf{bold}.
  } \label{table:UDA}
  \centering
  \resizebox{0.99\textwidth}{!}{
    \def\arraystretch{1.1}
    \begin{tabular}{ l | c c c c c c c c c c c c c c c c c c c | c }
        \Xhline{1.2pt}
        Method & \rotatebox{0}{road} & \rotatebox{0}{side.} & \rotatebox{0}{buil.} & \rotatebox{0}{wall} & \rotatebox{0}{fence} & \rotatebox{0}{pole} & \rotatebox{0}{light} & \rotatebox{0}{sign} & \rotatebox{0}{veg.} & \rotatebox{0}{terr.} & \rotatebox{0}{sky} & \rotatebox{0}{pers.} & \rotatebox{0}{rider} & \rotatebox{0}{car}& \rotatebox{0}{truck} & \rotatebox{0}{bus} & \rotatebox{0}{train} & \rotatebox{0}{mbike} & \rotatebox{0}{bike} & mIoU \\
        \hline
        \hline
        
        DAFormer & 71.1 & \textbf{53.8} & 77.8 & \textbf{45.1} & \textbf{36.6} & 57.1 & 47.0 & \textbf{53.4} & 70.3 & 36.6 & 67.0& 53.9 & 22.3 & 81.1 & 67.6 & 79.8 & 85.6 & 37.2 & \textbf{41.6} & 57.1 \\
        \cellcolor{lightgray}\textbf{DAFormer+SVDP} &\cellcolor{lightgray}\textbf{79.9} &\cellcolor{lightgray}46.5&\cellcolor{lightgray}\textbf{78.7}&\cellcolor{lightgray}44.1&\cellcolor{lightgray}35.6&\cellcolor{lightgray}\textbf{57.6}&\cellcolor{lightgray}\textbf{48.3}&\cellcolor{lightgray}52.1&\cellcolor{lightgray}\textbf{71.7}&\cellcolor{lightgray}\textbf{37.1}&\cellcolor{lightgray}\textbf{78.6}&\cellcolor{lightgray}\textbf{56.9}&\cellcolor{lightgray}\textbf{29.3} 
 &\cellcolor{lightgray}\textbf{83.1}&\cellcolor{lightgray}\textbf{71.2}&\cellcolor{lightgray}\textbf{85.9}&\cellcolor{lightgray}\textbf{86.2}&\cellcolor{lightgray}\textbf{37.3}      &\cellcolor{lightgray}39.2
 &\cellcolor{lightgray}\textbf{58.9}\\\bottomrule
       
        
        \Xhline{1.2pt}
    \end{tabular}
  }

\end{table*}


\section{SVDP on Unsupervised Domain Adaptation}
\label{Sec: UDA}


To demonstrate the effectiveness and generalizability of our approach, we apply SVDP in Unsupervised Domain Adaptation (UDA) to evaluate its performance.
Our proposed SVDP has a relatively small number of parameters, which does not significantly increase computational cost and can be easily integrated into existing models by directly adding SVDP to the input. Therefore, SVDP serves as a plug-and-play method for any UDA methods.
DAFormer~\cite{hoyer2022daformer} is a widely used method in recent UDA semantic segmentation tasks, and many state-of-the-art approaches~\cite{bruggemann2023refign, hoyer2022hrda, hoyer2022mic} build upon its foundation for further improvement. To demonstrate the effectiveness of our proposed SVDP, we integrate SVDP into DAFormer with DPP strategy and DPU strategy, which aims to evaluate the UDA segmentation performance of our methods.

In our experiment, we utilize the same model and training strategy as DAFormer, with the addition of SVDP to the original model. The learning rate of the Prompt is set to 0.006.
In the UDA setting, we can access labeled source domain data and unlabeled target domain data during training, and the model is tested on the target domain during the inferrence. We evaluate our model in the CityScape-to-ACDC scenario, and the results in Tab .~\ref{table:UDA} demonstrate that our method achieves 1.8\% mIoU improvement. This finding further confirms the effectiveness of our method, as well as emphasizes its generalizability in pixel-wise DA.



\section{Additional related work}
\label{Sec:related}

\textbf{Semantic segmentation} is a crucial task in many computer vision applications aimed at assigning a categorical label to every pixel in an image. Several representative works in this field include DeepLab \cite{chen2017deeplab}, PSPNet \cite{zhao2017pyramid}, RefineNet \cite{lin2017refinenet}, and Segformer \cite{xie2021segformer}. Despite their high performance, these methods usually require extensive amounts of pixel-level annotated data, which can be laborious and time-consuming to collect. Additionally, they may suffer from poor generalization when applied to new domains.
Recent research has focused on addressing these challenges through domain adaptation strategies. For instance, \cite{yang2020fda} proposes a method that swaps the low-frequency spectrum to align the source and target domains. \cite{tranheden2021dacs} mixes the images from both domains, along with their corresponding labels and pseudo-labels. In contrast, \cite{wu2021dannet} uses adversarial learning to train a domain adaptation network for nighttime semantic segmentation. \cite{hoyer2022daformer} develops a novel model and training strategies to enhance training stability and avoid overfitting to the source domain.
However, these methods often require retraining the model on the source domain, which is inconvenient. Furthermore, they need to be retrained when adapting to a new target domain, incurring additional time and resource costs. Therefore, we propose SVDP to efficiently address the domain shift problem, which leverages a pre-trained model on the source domain and adds only a few parameters to achieve strong generalization capabilities on the target domain.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{FullPaper}
}
\clearpage
\appendix
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{supbib}
% }
% \clearpage

% \appendix
% \input{appendix.tex}

\end{document}