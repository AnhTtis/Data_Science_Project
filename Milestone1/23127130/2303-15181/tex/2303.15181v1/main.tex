
%% bare_jrnl_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.



% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}
\newcommand{\xjqi}[1]{{\color{magenta}{ [xjqi: #1]}}}
\newcommand{\lzz}[1]{{\color{cyan}{\bf\sf [lzz: #1]}}}
\newcommand{\phil}[1]{{\color{red}{[ph: #1]}}}
%\newcommand{\phil}[1]{{\color{blue}{\bf\sf [Ph: #1]}}}
\newcommand{\pdai}[1]{{\color{green}{\bf\sf [dp: #1]}}}
\newcommand{\rh}[1]{{\color{cyan}{\bf\sf [RH: #1]}}}
\newcommand{\etal}{{\textit{et al.}}}
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\vs}{\textit{vs.}}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage[T1]{fontenc}
\usepackage[linesnumbered,boxed,ruled,commentsnumbered]{algorithm2e}
\usepackage{times}
\usepackage{stfloats}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{dsfont}
\usepackage{bbm}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{stackengine}
\usepackage{stfloats}
\usepackage{capt-of}
\usepackage{cuted}
\usepackage{lipsum}
\def\delequal{\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}}

% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{ISS++: Image as Stepping Stone for\\ Text-Guided 3D Shape Generation}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each subsection as LaTeX2e's \thanks
% was not built to handle multiple subsections
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.


\author{Zhengzhe Liu,
        Peng Dai,
        Ruihui Li, Xiaojuan Qi, Chi-Wing Fu\\
        \
        \\% <-this % stops a space
         \footnotesize{Project page: \url{https://liuzhengzhe.github.io/ISS.github.io/}}\\
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem Z. Liu and C.-W. Fu are with the Department
of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong.\protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: zzliu@cse.cuhk.edu.hk; cwfu@cse.cuhk.edu.hk.
\IEEEcompsocthanksitem P. Dai and X. Qi are with the Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong. \protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: xjqi@eee.hku.hk; % <-this % stops an unwanted space
\IEEEcompsocthanksitem R. Li is with College of Computer Science and Electronic Engineering, The Hunan University, China.
}
}


% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE} 
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
%
%Text-guided 3D shape generation remains challenging due to the absence of large paired text-shape dataset, the substantial semantic gap between these two modalities, and the structural complexity of 3D shapes.
%This paper presents a new framework called {\em Image as Stepping Stone\/} (ISS) for the task by introducing 2D image as a stepping stone to connect the two modalities and to eliminate the need for paired text-shape data.
%Our key contribution is a {\em two-stage feature-space-alignment approach\/} that maps CLIP features to shapes by harnessing a pre-trained single-view reconstruction (SVR) model with multi-view supervisions: first map the CLIP image feature to the detail-rich shape space in the SVR model, then map the CLIP text feature to the shape space and optimize the mapping by encouraging 
%CLIP consistency between the input text and the rendered images.
%In addition, we formulate a {\em text-guided shape stylization module\/} to dress up the output shapes with novel structures and textures. 
%\lzz{Further, ISS++ extends our approach to improve the generative diversity, fidelity, and stylization capability by leveraging pretrained text-to-image diffusion models.  }% fine-tuning \lzz{the model} \phil{what? need to a word here} with Score Distillation Sampling.}
%\phil{no need to define SDS here if you do not need this abbrev in the abstract}
%\phil{from what you wrote here, seems that only one NEW technical contribution? add one more sentence if possible}
%Beyond existing works on 3D shape generation from text, our new approach is general for creating shapes in a broad range of categories, {\em without\/} requiring paired text-shape data.
%Experimental results manifest that our approach outperforms the state-of-the-arts and
%our baselines in terms of {\em fidelity\/} and {\em consistency with text}. Further, our approach can stylize the generated shapes with both realistic and fantasy structures and textures.
%Generating 3D shapes using text guidance is a challenging task due to the lack of lack of large-scale paired text-shape datasets, the substantial semantic gap between text and shape modalities, and the topological complexity of 3D shapes. 
In this paper, we present a new text-guided 3D shape generation approach (ISS++) that uses images as a stepping stone to bridge the gap between text and shape modalities for generating 3D shapes without requiring paired text and 3D data. 
%Our key insight is to use image as a stepping stone to gradually bridge the gap between text and shape modalities. 
The core of our approach is a {\em two-stage feature-space alignment strategy\/} that leverages a pre-trained single-view reconstruction (SVR) model to map CLIP features to shapes: to begin with, map the CLIP image feature to the detail-rich 3D shape space of the SVR model, then map the CLIP text feature to the 3D shape space through encouraging the CLIP-consistency between rendered images and the input text. 
Besides, to extend beyond the generative capability of the SVR model, we design a text-guided 3D shape stylization module that can enhance the output shapes with novel structures and textures.
Further, we exploit pre-trained text-to-image diffusion models to enhance the generative diversity, fidelity, and stylization capability.
Our approach is generic, flexible, and scalable, and it can be easily integrated with various SVR models to expand the generative space and improve the generative fidelity.
 Extensive experimental results demonstrate that our approach outperforms the state-of-the-art methods in terms of generative quality and consistency with the input text. %Additionally, we provide three options for 3D shape stylization to decorate it with imaginary textures and structures.
Codes and models are released at 
{\scriptsize \url{https://github.com/liuzhengzhe/ISS-Image-as-Stepping-Stone-for-Text-Guided-3D-Shape-Generation}}.
\end{abstract}







% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Text to 3D shape generation, CLIP, 3D shape stylization, Score Distillation Sampling 
\end{IEEEkeywords}}


% make the title area
\maketitle
%\lipsum[-1]
%\vspace{in}
%\begin{strip}
% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
%\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.







% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



%\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.














\begin{comment}
Generating 3D shapes using text guidance is a challenging task due to the lack of large-scale paired text-shape datasets, the substantial semantic gap between text and shape modalities, and the topological complexity of 3D shapes. In this paper, we introduce a novel framework Image as Stepping Stone (ISS) that employs 2D images as stepping stone to bridge text and shape modalities, eliminating the reliance on the paired text-shape data. The key contribution of this work is a {\em two-stage feature-space alignment approach\/} that leverages a pre-trained single-view reconstruction (SVR) model to map CLIP features to shapes: to begin with, map the CLIP image feature to the 3D shape space with rich details provided by the SVR model, then map the CLIP text feature to the 3D shape space through encouraging the CLIP-consistency between the rendered images and input text. 
We also propose a text-guided shape stylization module that can enhance the output shapes with novel structures and textures. Additionally, our approach is flexible. It can be used with various SVR models to further enhance the generative scope and quality. Our extended approach, ISS++, leverages pre-trained text-to-image diffusion models to enhance the generative diversity, fidelity, and stylization capability. Also, it can further expand our generative scope beyond the image dataset. %Unlike previous methods for 3D shape generation from text, our new approach is generalizable and can create shapes in a wide range of categories without requiring paired text-shape data. 
Our experimental results demonstrate that our approach outperforms the state-of-the-art methods with regard to generative quality and consistency with the input text. Additionally, we provide three options for 3D shape stylization to decorate it with imaginary textures and structures.
Codes and models are released at 
{\scriptsize \url{https://github.com/liuzhengzhe/ISS-Image-as-Stepping-Stone-for-Text-Guided-3D-Shape-Generation}}.
%
\end{comment}










\vspace*{-25pt}
\section{Introduction}\label{sec:intro}

%3D shape generation has a broad range of applications,~\eg, in Metaverse, CAD, games, animations, etc.
%Among various ways to generate 3D shapes, a user-friendly approach is to generate shapes from natural language or text descriptions.
%By this means, users can readily create shapes,~\eg, to add/modify objects in VR/AR worlds, to design shapes for 3D printing, etc. 
%Yet, generating shapes from texts is very challenging, due to the lack of large-scale paired text-shape data, the large semantic gap between the text and shape modalities, and the structural complexity of 3D shapes.
3D shape generation has many practical applications, such as in CAD, 3D games, animations, and more. Among  different ways to generate 3D shapes, a user-friendly method is to generate shapes from text descriptions. This enables users to easily generate 3D shapes using natural language along with many applications in AR/VR and 3D printing. 
%, such as adding or modifying objects in AR/VR and creating things for 3D printing. 
However, text-guided shape generation presents significant challenges owing to the difficulty of collecting paired text-shape data, the substantial semantic gap between texts and shapes, and the topological complexity of 3D shapes.


%Existing works~\cite{chen2018text2shape,jahan2021semantics,liu2022towards} typically rely on paired text-shape data for model training. Yet, collecting 3D shapes is already very challenging on its own, let alone the tedious manual annotations needed to construct the text-shape pairs.  To our best knowledge, the largest existing paired text-shape dataset~\cite{chen2018text2shape} contains only two categories,~\ie, table and chair, thus severely limiting the applicability of the existing works.

Previous research \cite{chen2018text2shape,jahan2021semantics,liu2022towards} typically requires paired text-shape data for this challenging task. However, it is already non-trivial to collect 3D shapes, let alone manually annotate text-shape pairs which brings further complexities. Currently, the largest paired text-shape dataset available \cite{chen2018text2shape} only includes two categories, tables and chairs, significantly limiting the applicability of current methods.




%Recently, some annotation-free approaches, CLIP-Forge~\cite{sanghi2021clip}, Dream Fields~\cite{jain2021zero}, \textcolor{blue}{CLIP-Mesh~\cite{mohammad2022clip}, and DreamFusion~\cite{poole2022dreamfusion}} were proposed to address the dataset limitation.
%These state-of-the-art approaches attempt to utilize the joint text-image embedding from the large-scale pre-trained language vision model,~\ie, CLIP~\cite{radford2021learning}, \textcolor{blue}{and pretrained text-to-image generation models,~\eg, Imagen~\cite{saharia2022photorealistic}}, to eliminate the need of requiring paired text-shape data in the model training. However, it is still extremely challenging to generate 3D shapes from text without paired texts and shapes for the following reasons. First, the range of object categories that can be generated are still limited due to the scarcity of 3D datasets. For example, Clip-Forge~\cite{sanghi2021clip} is built upon a shape auto-encoder;
%it can hardly generate plausible shapes beyond the ShapeNet categories. 
%Also, it is challenging to learn 3D prior of the desired shape from texts. For instance, with over an hour of optimization for each shape instance from scratch, there is still no guarantee that the multi-view consistency constraint of Dream Fields~\cite{jain2021zero}, CLIP-Mesh~\cite{mohammad2022clip}, and DreamFusion~\cite{poole2022dreamfusion} can enable the model to produce shapes that match the given text; we will provide further investigation in our experiments.  
%Last, the visual quality of the generated shapes are far from satisfactory due to the substantial semantic gap between the unpaired texts and shapes. As shown in Figure~\ref{fig:figure1} (b), the results generated by Dream Fields and CLIP-Mesh typically look surrealistic (rather than real), and the 3D topology of DreamFusion is still not very satisfactory, due to insufficient information extracted from text for the shape structures and details.
%On the other hand, CLIP-Forge~\cite{sanghi2021clip} is highly restricted by the limited $64^3$ resolution and it lacks colors and textures, further manifesting the difficulty of generating 3D shapes from unpaired text-shape data.




%\vspace*{-3mm}
%\begin{figure*}[bp]
\begin{figure}%[!t]
\centering
\includegraphics[width=0.99\columnwidth]{style_fig1.pdf}
\caption{Generative results of our ISS++. The input text follows the prompt template ``A [shape] imitates a [style]''. %\lzz{I do not know how put it above the abstract.}
%\phil{Unlike IEEE TVCG, it seems that PAMI doesn't support a teaser function in the latex template.}
%\phil{Then, how about breaking it into two rows and put it up as a single-column figure?  I meant the chair cases on top left; lamp cases on bottom left; and water craft on the right side vertically}
}
\label{fig:style_figure1}
\vspace*{-2.75mm}
%\end{strip}
\end{figure}

Recently, several annotation-free approaches have been proposed for text-to-shape generation without requiring paired text-shape data. These approaches, such as CLIP-Forge~\cite{sanghi2021clip}, Dream Fields~\cite{jain2021zero}, CLIP-Mesh~\cite{mohammad2022clip}, and DreamFusion~\cite{poole2022dreamfusion}, utilize the large-scale language-vision models, e.g., CLIP~\cite{radford2021learning}, and text-to-image generation models, such as Imagen~\cite{saharia2022photorealistic}, for training. However, generating high-quality 3D shapes from unpaired text-shape data remains challenging for several reasons. First, due to the scarcity of 3D datasets, they can only generate a very limited range of shape categories. For instance, CLIP-Forge~\cite{sanghi2021clip} is struggling to generate shapes outside the ShapeNet dataset. Second, without injecting any text-related shape priors, it is also difficult to produce %3D prior, namely, 
3D structures that match the input texts.  For example, CLIP Mesh~\cite{mohammad2022clip} and Dream Fields~\cite{jain2021zero} often generate 3D shapes incompatible with given texts (see Figure~\ref{fig:figure1} (b)) even with minutes or hours of test-time optimization for each shape instance. 
%even with more than an hour of test-time optimization for each shape instance. 
Third, the visual quality of the generated shapes is not satisfactory. As shown in Figure~\ref{fig:figure1} (b),  CLIP-Forge~\cite{sanghi2021clip} produces low-resolution outputs ({\ie} $64^3$) without textures, the results generated by Dream Fields and CLIP-Mesh typically look surrealistic (rather than real), and the 3D topology and surface quality of DreamFusion still have a large room for improvement.
%Also, the generative results of CLIP-Forge~\cite{sanghi2021clip} have a low resolution of $64^3$ and lack texture generation capability.

%it is not guaranteed that Dream Fields~\cite{jain2021zero} and DreamFusion~\cite{poole2022dreamfusion} (see Figure~\ref{fig:figure1} (b)) can generate shapes that match the given text even with more than an hour of test-time optimization for each shape instance since it is extremely challenging to learn the shape prior from a piece of text. 
%due to insufficient information extracted from the text for shape structures and details. 
%Last, the visual quality of the generated shapes is not satisfactory. As shown in Figure~\ref{fig:figure1} (b), the results generated by Dream Fields and CLIP-Mesh typically look surrealistic (rather than real), and the 3D topology and surface quality of DreamFusion still have a large room for improvement.
%On the other hand, the generative results of CLIP-Forge~\cite{sanghi2021clip} have the limited $64^3$ resolution and also lacks texture generation capability. %, further manifesting the difficulty of generating 3D shapes from unpaired text-shape data.

%\phil{try to elaborate and explain more?}
%\phil{I have further refined this subsection; you may use the overleaf history function to check my changes}

\begin{comment}
To address this issue, recent annotation-free approaches,~\ie, CLIP-Forge~\cite{sanghi2021clip} and Dream Fields~\cite{jain2021zero}, attempt to %\phil{attempt to? cause they didn't completely solve the problem, right?} 
utilize the joint text-image embedding from the large-scale pre-trained language vision model,~\ie, CLIP~\cite{radford2021learning}, to eliminate the need of requiring \lzz{paired text and shape} data in training. 
However, %as shown in Figure~\ref{fig:figure1} (a),
Clip-Forge~\cite{sanghi2021clip} is built upon a shape auto-encoder %, so it still relies on shapes for training
and can hardly generate reasonable shapes beyond the ShapeNet categories. Also, the visual quality of its results 
%CLIP-Forge~\cite{sanghi2021clip} 
is highly restricted by the limited $64^3$ resolution, and lacks colors and textures, as shown in Figure~\ref{fig:figure1} (b).
%(a).
Dream Fields~\cite{jain2021zero}, on the other hand, is able to produce a wide range of categories.
Yet, since it is trained only to produce multi-view images with a neural radiance field, it cannot directly generate 3D shapes, due to the lack of 3D priors.
Also, Dream Fields needs over an hour of optimization for each shape instance from scratch, and there is no guarantee that the multi-view consistency constraint can always enforce the model for producing realistic images, see,~\eg, Figure~\ref{fig:figure1} (b).
\end{comment}
%ok? ok
%\lzz{dream field claims they can create multi-veiw consistent shapes, becuase they generate a unified nerf model}
%
%Yet, it cannot \phil{cannot or does not?}\lzz{I think both are ok} directly generate 3D shapes, due to the lack of 3D priors, as it is trained only with a neural radiance field (NeRF) for multi-view image rendering. In addition, Dream Fields needs 72 minutes optimization for each shape instance from scratch, and there is no guarantee that the multi-view consistency constraints will enforce the model to produce realistic outputs. 


\begin{comment}
However, %as shown in Figure~\ref{fig:figure1} (a),
Clip-Forge~\cite{sanghi2021clip} is built upon a shape auto-encoder; hence, it still relies on 3D shapes for training, and can hardly generate reasonable 3D shapes beyond the ShapeNet categories. In addition, the generative visual quality of CLIP-Forge~\cite{sanghi2021clip} are highly restricted by the limited $32^3$ resolution and the lack of textures as shown in Figure~\ref{fig:figure1} (a). On the other hand, Dream Fields~\cite{jain2021zero} eliminates the need of 3D supervision by encouraging the CLIP feature consistency between the rendered multi-view images and the input text in the CLIP feature space; % (see Figure~\ref{fig:figure1} (b))
hence, it is capable of generating a wide range of objects. However, due to the absence of 3D shape priors, Dream Fields needs 72 minutes optimization %\phil{what optimization time? try to be more specific here}
for each shape instance from scratch, and there is no guarantee that the multi-view consistency constraints will enforce the model to produce realistic 3D shapes as shown in Figure~\ref{fig:figure1} (b). %\phil{mention their result in Figure 1} 
In addition, it does not allow the direct generation of the concrete 3D shape, but only trains a neural radiance field (NeRF) for multi-view image rendering. \xjqi{shorten}
\end{comment}

%\phil{It only generates images, right?}

%as shown in Figure~\ref{fig:figure1} (b), it only enforces the consistency between text and the rendered multi-view images from a neural rediance field model (NeRF).
%\xjqi{However, due to the absence of 3D shape priors, there is no guarantee that the multi-view consistency constraints will enforce the model to produce realistic 3D shapes?? Experimentally results also show that the 3D shape from xxx is far from realistic??. Besides, long time to converge xxxx.} Without the prior of the 3D shape \xjqi{Due to the lack of 3D priors??}, Dream Fields needs a long time optimization for each shape instance and the generative results are far from realistic. In addition, it does not allow the direct generation of the concrete 3D shape, but only trains a neural radiance field (NeRF) \xjqi{Dreamfield part needs to be rewritten}. 


%To address this issue, recent annotation free approaches CLIP-Forge~\cite{sanghi2021clip} and Dream Fields~\cite{jain2021zero} utilize \xjqi{the joint text-image embedding feature space from??} CLIP~\cite{radford2021learning} to eliminate the need of the text data in training. However, the generative visual quality of CLIP-Forge~\cite{sanghi2021clip} are far from satisfactory \xjqi{It seems not good to directly judge their works: do some anlaysis. However, Clip Forge rely on xxxx. Therefore they can only generate xxxx. Besides, they cannot generate texture?? xxx show a figure}. In addition,~\cite{sanghi2021clip} still rely on 3D shapes for training, and can hardly generate reasonable 3D shapes beyond the ShapeNet categories. Dream Field~\cite{jain2021zero} further eliminates the need of 3D supervision. And hence, it is capable of generating a wide range of objects. However, \xjqi{why it lacks 3D prior} due to the lack of the 3D prior, Dream Fields needs a long time optimization for each shape instance and the generative results are far from realistic. \xjqi{It only enforces the consistency between text and the rendered multiview images from NeRF. There is no guarantee that it can regularize the latent 3D shape. Therefore it  cannot generate realistic 3D shapes?? better have a figure} In addition, it does not allow the direct generation of the concrete 3D shape, but only trains a neural radiance field (NeRF). 


%Motivated by the impressive progress of the two related fields, CLIP~\cite{radford2021learning} and image to shape generation~\cite{chen2019learning,li2021d2im,niemeyer2020differentiable,alwala2022pre},






\begin{figure*}
\centering
\includegraphics[width=0.99\textwidth]{figure1.pdf}
\vspace*{-2mm}
\caption{The proposed ``Image as Stepping Stone'' framework. 
%two-stage feature-space-alignment approach 
Our two-stage feature-space alignment shown in (a) bridges the text space (the CLIP text feature) and the 3D shape space (the SVR feature) %generate 3D shapes from texts 
%with 2D image as a stepping stone, 
to generate 3D shapes from the text shown in (b), outperforming the existing works, %(CLIP-Forge and Dream Fields),
without requiring paired text-shape data.
%\lzz{How to make this figure floating at the top of the text? }
%\phil{Use wrapfigure in latex... but the figure is good. No need to make it float, right?}
}
\label{fig:figure1}
\vspace*{-2.75mm}
\end{figure*}




%Going beyond the existing works,
%we propose a new approach to 3D shape generation from text without needing paired text-shape data.
%Our key idea is to {\em implicitly leverage 2D image\/} as a stepping stone (ISS) to connect the text and shape modalities.
%Specifically, we employ the joint text-image embedding in CLIP and train a CLIP2Shape mapper to {\em map the CLIP image features to a pre-trained detail-rich 3D shape space with multi-view supervisions\/}; see Figure~\ref{fig:figure1} (a): stage 1.
%Thanks to the joint text-image embedding from CLIP, 
%our trained mapper is able to connect the CLIP text features with the shape space for text-guided 3D shape generation. Yet, due to the gap between the CLIP text and CLIP image features, the mapped text feature may not align well with the destination shape feature; see the empirical analysis in Section~\ref{sec:empirical}. Hence, we further fine-tune the mapper specific to each text input by {\em encouraging CLIP consistency\/} between the rendered images and the input text to enhance the consistency between the input text and the generated shape; see Figure~\ref{fig:figure1} (a): stage 2.

Going beyond existing approaches, we present a novel text-guided 3D shape generation method without requiring paired text-shape data. We propose to {\em leverage 2D Image as a Stepping Stone \/}  to implicitly bridge the shape and text modalities and exploit diffusion models for enhanced diversity, quality, and generative scope, namely ISS++. Specifically, we use the pre-trained vision-language model CLIP to train a mapper that maps CLIP image features to a pre-trained 3D shape space. In inference, this mapper maps the CLIP text features to the target shape space, as shown in Figure~\ref{fig:figure1} (a) stage 1. However, there exists a gap between the CLIP image and text features. As a result, the CLIP text feature might not be mapped to a desired shape feature. To tackle this issue, we further fine-tune the mapper to improve the text-shape consistency. We do this by adopting a training objective encouraging CLIP consistency between the input text description and rendered images. This fine-tuning stage is depicted in Figure~\ref{fig:figure1} (a) as stage 2. 
To enhance generative diversity, we leverage an off-the-shelf  diffusion model for mapping the CLIP text feature to CLIP image feature~\cite{ramesh2022hierarchical} and sample multiple generated CLIP image feature that matches the text feature during inference. The two-stage feature-space alignment can generate plausible shapes from texts.

%In addition, our extended approach ISS++ can generate diversified shapes from a given text by sampling various CLIP image features that are consistent with the input CLIP text feature leveraging the diffusion prior~\cite{ramesh2022hierarchical}. 




%\lzz{dream field can generate more categories than ours.}


%\xjqi{we need more insights to explain why we can do the two-stage optimization like this}


%\xjqi{This subsection needs to be refined to make it clearer, no need for the diversify} Specifically \xjqi{First}, to connect image and shape modalities, we train a CLIP2Shape mapper to map from the CLIP image feature space to a pre-trained detail-rich shape space with multi-view images as supervisions (see Figure~\ref{fig:figure1} (d): \xjqi{stage 1}).
%To further connect the text and shape modalities, we leverage CLIP text-image joint embeddings  \xjqi{and ths trained mapper in stage 1??} to map \xjqi{adapt} the CLIP text feature to the target shape space as shown in Figure~\ref{fig:figure1} (d-ii) \xjqi{stage 2??}. Due to the gap between CLIP text and image feature, the initially mapped text feature is still far from the destination shape feature (0.59 in Figure~\ref{fig:figure1} (d)). To push the mapped text feature close to the target, we fine-tune the mapper in test-time by encouraging the CLIP consistency between rendered images and the input text. % to eliminate the gap between the CLIP text and image features. %\xjqi{You need to



















































\begin{comment}
In this work, we propose a new approach to 3D shape generation from text without needing text and shape by using 2D images as a stepping stone (ISS) to connect the text and shape modalities.
\xjqi{Specifically, we integrate xxx and xxx, and design xxx}Integrating CLIP~\cite{radford2021learning} and a pre-trained single-view reconstruction (SVR) model~\cite{niemeyer2020differentiable,alwala2022pre}, we propose a two-stage feature-space alignment approach to gradually reduce the semantic gap among text, image, and shape feature spaces as shown in Figure~\ref{fig:figure1} (d). \xjqi{may be say sth about how this pipeline address the problem of previous research, move some part of the contribution to here} 

%\xjqi{we need more insights to explain why we can do the two-stage optimization like this}
\xjqi{This subsection needs to be refined to make it clearer, no need for the diversify} Specifically \xjqi{First}, to connect image and shape modalities, we train a CLIP2Shape mapper to map from the CLIP image feature space to a pre-trained detail-rich shape space with multi-view images as supervisions (see Figure~\ref{fig:figure1} (d): \xjqi{stage 1}). %  to improve the generation quality. % \xjqi{what's the supervision, it seems this part may need shape supervision if not discussed. The figure is not clear}. 
 %\xjqi{to finally map text to }
To further connect the text and shape modalities, we leverage CLIP text-image joint embeddings  \xjqi{and ths trained mapper in stage 1??} to map \xjqi{adapt} the CLIP text feature to the target shape space as shown in Figure~\ref{fig:figure1} (d-ii) \xjqi{stage 2??}. Due to the gap between CLIP text and image feature, the initially mapped text feature is still far from the destination shape feature (0.59 in Figure~\ref{fig:figure1} (d)). To push the mapped text feature close to the target, we fine-tune the mapper in test-time by encouraging the CLIP consistency between rendered images and the input text. % to eliminate the gap between the CLIP text and image features. %\xjqi{You need to explain why you need test time optimization and how can it work}
%\lzz{may remove the following sentences about background loss?}
%In addition, we propose a background loss to improve the model's awareness of the foreground object region against the background. 
%Furthermore, our framework can easily generate diversified shapes from one input text by adding random feature perturbations as the initialization of stage-2 alignment. 
%\xjqi{still not clear}%\xjqi{this is not clear how you can generate diversified 3D shapes, please explain},
unlike the existing works that require additional and complex modules like GANs~\cite{chen2018text2shape}, IMLE~\cite{liu2022towards} and normalizing flow network~\cite{sanghi2021clip}. %\xjqi{We need a better figure to illustrate feature space including image feature space, text feature space, and 3D shape}
\end{comment}



\begin{comment}
In this work, we propose \phil{a new approach to 3D shape generation from text by using 2D images as a stepping stone to connect the text and shape modalities?} to use the 2D Image as Stepping Stone (ISS) to connect the text and the shape modalities for text-guided 3D shape generation. 
%
\phil{what are the key ideas to realize this approach?  Our key ideas to realize this approach is two-fold? three-fold?}
\phil{To realize our approach, we ... try to re-arrange the writing below to make it more specific on what are our key ideas} 
%
\phil{when reading this subsection, I felt that ``integrating ...'' up to the end of this subsection is already our idea but then, I read ``First, to connect''.  The word ``First'' makes me confused what is/are the key ideas to realize the goal.  Is ``integrating ...'' up to the end of this subsection also our idea?}
%
Integrating CLIP~\cite{radford2021learning} and a pre-trained single-view reconstruction (SVR) model~\cite{niemeyer2020differentiable,alwala2022pre}, we gradually reduce the semantic gap among text, image, and shape feature spaces as shown in Figure~\ref{fig:figure1} (d). Ourwi approach learns 3D shape priors from the easily accessible multi-view images, and further connects text and image with CLIP, eliminating the requirement of shape and text.

First, to connect image and shape modalities, we train a CLIP2Shape mapper to map the CLIP image feature to a pre-trained detail-rich shape space to improve the generation quality as shown in Figure~\ref{fig:figure1} (a) to (c). 
Second, to connect text and image modalities, we leverage CLIP text-image joint embeddings and fine-tune the CLIP2Shape mapper in test-time to eliminate the gap between the CLIP text and image features. In addition, we propose a background loss to improve the model's awareness of the foreground object region against the background. Furthermore, our framework can intrinsically generate diversified shapes given one input text, unlike the existing works that require additional and more complex modules like GANs~\cite{chen2018text2shape}, IMLE~\cite{liu2022towards} and normalizing flow network~\cite{sanghi2021clip}. 
\end{comment}

%\xjqi{In this work, we propose xxx method xxxx. We leverage a pre-trained image to shape generation model xxx which learns 3D priors from vast amount of multi-view image data. Build upon xxxx, we develop a  progressive adaptation method to gradually xxx stragy to xxxx exploiting the joint text and image feature space from large pre-trained text to image model. It consists xxx xxxx: descibe your method. In the description, you should explain why you can address the issues from previous methods. describe your outcome, our method offers xxx advantanges in comparison with xxx: fast, realistic, and scalablity}

%In this work, we propose to use the 2D Image as Stepping Stone (ISS) to connect the text and the shape modalities. %integrating two research fields CLIP~\cite{radford2021learning} and single view reconstruction~\cite{niemeyer2020differentiable,alwala2022pre}. 
%Integrating CLIP~\cite{radford2021learning} and a pre-trained single view reconstruction (SVR) model~\cite{niemeyer2020differentiable,alwala2022pre}, we gradually reduce the gap between text, image, and shape feature spaces with the easily accessible multi-view images for training, eliminating the requirement of text and shape. 



%First, we present a two-stage feature space exploration framework to first map the CLIP image feature to the 3D shape space with rich details, then 

%reduce the domain gap of the three involved modalities including the CLIP text space, CLIP image space and the 3D shape space. 

%Unlike existing worksCLIP-Forge~\cite{sanghi2021clip} that directly adopt CLIP feature for generation, w

%To better understand the CLIP feature and leverage it to connect the text and shape, we conduct an empirical study on it, and deliver two important findings. First, CLIP image feature has the inferior representative capability to encode some necessary image details. Second, there exist a certain gap between CLIP image and text features. %Therefore, directly utilize CLIP feature for generation can lead to coarse shape generation with the lack of necessary details, and directly replace the CLIP image feature with CLIP text feature in inference can make the generated shape inconsistent with the input text. %Third, the CLIP image feature is sensitive to the background color. 

%Based on the above observations, we propose a two-stage feature space alignment approach to connect the three feature spaces, \ie, text, image and shape. First, we train a CLIP2Shape mapper to map the CLIP image feature to the detail-rich shape space to improve the generation fidelity. Second, we fine-tune the CLIP2Shape mapper in test-time to eliminate the gap between the CLIP text and image features. In addition, we propose a background loss to improve the model's awareness of the foreground shape against the background. Furthermore, our framework can easily generate diversified shapes given one input text, unlike the existing works that utilize complex alternatives like GANs~\cite{chen2018text2shape}, IMLE~\cite{liu2022towards} and normalizing flow network~\cite{sanghi2021clip}. 
%Our test-time optimization takes only 85 seconds and improve the generalization quality significantly. 

%\phil{Second again? Please try to be more systematic.  Perhaps bullet form to list down all our ideas in this work.}
%\xjqi{This part seems not so relevant with the target?? [how this is related to the previous sessions] This part is not very clear. To enrich the texture style beyond xxx of the generated 3D shapes, we propose xxx. A xxx loss is incorporated to xxx and xxx.} %Second,




%With our two-stage feature-space alignment, we already can generate shapes with good fidelity from texts.
%To further enrich the generated shapes with vivid textures and structures beyond the generative space of the pre-trained SVR model,
%we additionally design a text-guided stylization module to generate novel textures and shapes by encouraging consistency between the rendered images and the text description of the target style. 
%We then can effectively fuse with the two-stage feature-space alignment to enable the generation of both realistic and fantasy textures and also shapes beyond the generation capability of the SVR model. \textcolor{blue}{More importantly, our approach can generate high-quality and stylished 3D shapes with Score Distillation Sampling~\cite{poole2022dreamfusion}. Benefiting from the 3D shape prior learned by our two-stage feature space alignment, our approach outperforms~\cite{poole2022dreamfusion} in terms of both the surface quality and the topology faithfulness, while requiring only 30\% of training iterations compared with~\cite{poole2022dreamfusion}.} 

%With the two-stage feature-space alignment, we can already generate shapes with good fidelity from texts.

%Further, going beyond the generative space of pre-trained SVR models, we design shape stylization and refinement modules to enable the generation of novel and attractive textures and structures in the test time.
Furthermore, to extend beyond the generative space of pre-trained SVR models, we design CLIP-guided shape stylization and Score Distillation Sampling (SDS)-guided refinement modules that allow for the generation of new and visually appealing textures and structures during testing. Specifically, the CLIP-guided shape stylization module updates the decoder of the SVR model by optimizing the CLIP consistency between the rendered images from the generated shape and the target style description. 
Although this strategy help expands the models' generative capability toward open-world style descriptions, it suffers from generating local detailed structure due to the global guidance of CLIP features; see Figure~\ref{fig:figure1} ``ISS''.
%To that end, inspired by Score Distillation Sampling (SDS)~\cite{poole2022dreamfusion} that can provide fine-grained guidance to textured 3D shape generation leveraging priors of pre-trained diffusion models, we study marrying SDS with our two-stage feature-space alignment framework where SDS provides loss to update our decoder. 
To that end, in order to generate fine-grained structures and high-fidelity textures, we explore leveraging pre-trained diffusion models and marry Score Distillation Sampling (SDS)~\cite{poole2022dreamfusion} with our two-stage feature-space alignment framework. This involves utilizing SDS to provide a loss function for updating our decoder.
%In order to enhance the guidance provided to textured 3D shape generation through the use of pre-trained diffusion models, we explore the combination of Score Distillation Sampling (SDS)~\cite{poole2022dreamfusion} with our two-stage feature-space alignment framework. This involves utilizing SDS to provide a loss function for updating our decoder.
%we marry the Score Distillation Sampling~\cite{poole2022dreamfusion} and our two-stage feature-space alignment to further improve the generative fidelity and create imaginary shapes by incorporating the semantic
%attributes of the target style into the shape; see Figure~\ref{fig:style_figure1} ``ISS++''.
This allows us to generate high-fidelity novel structures and  textures and even create imaginary shapes by incorporating the semantic
attributes of the target style into the shape; see Figure~\ref{fig:style_figure1} and Figure~\ref{fig:figure1} ``ISS++''. 
This also extends the generation capability of our ISS++ to unseen categories out of the image dataset. 
 Besides, by leveraging the 3D shape prior of the two-stage feature-space alignment, our model outperforms \cite{poole2022dreamfusion} in terms of surface quality and topology faithfulness, while typically requiring much fewer training iterations.


Finally, our approach can be compatible with various SVR models~\cite{niemeyer2020differentiable,alwala2022pre,gao2022get3d}.
For instance, 
we can adopt SS3D~\cite{alwala2022pre} to generate shapes using single-view in-the-wild images, which expands the generative capability of our approach beyond the 13 categories of ShapeNet that can be generated by~\cite{sanghi2021clip}.
%from single-view in-the-wild images, which expands the range of categorical 3D shapes that our approach can produce beyond the 13 categories of ShapeNet generated by~\cite{sanghi2021clip}. 
Besides, our approach can also work with the very recent approach GET3D~\cite{gao2022get3d} to generate
high-quality 3D shapes from text; 
as shown in our results in Section~\ref{sec:results}.


%Although CLIP-guided consistency helps 
%to expand the generation scope to open-world style descriptions, the details on the generative results are still far from satisfactory since CLIP only provides a global supervision; see Figure~\ref{fig:figure1} ``ISS''.
%To further improve the generative quality, inspired by the recent advancements of diffusion models in 3D shape generation, we marry Score Distillation Sampling~\cite{poole2022dreamfusion} and our two-stage feature-space alignment to further improve the generative fidelity and create imaginary shapes by incorporating the semantic
%attributes of the target style into the shape; see Figure~\ref{fig:style_figure1} ``ISS++''. By leveraging the 3D shape prior learned by our two-stage feature-space alignment, our extended approach ISS++ is able to further outperform~\cite{poole2022dreamfusion} for both surface quality and topology faithfulness, while typically requiring much fewer training iterations compared to~\cite{poole2022dreamfusion}.
%Also, this approach further extends the generation capability of our ISS++ to unseen categories out of the image dataset. 


%further generate shapes beyond the generative capability of the pre-trained SVR model 
%with novel and attractive textures and structures, 

%we additionally present the following stylization and refinement modules.
%To further enrich the generated shapes with vivid textures and structures beyond the generative space of the pre-trained SVR model, we additionally design the following stylization and refinement strategies/modules.


%(i) The CLIP-guided stylization module to decorate the generated shapes with novel textures and structures by optimizing the CLIP consistency between the rendered images of the generated shape and the target style description. 
%By effectively fusing this module with our two-stage feature-space alignment, we can enable the generation of both realistic and fantastic textures and shapes beyond the generation space of the SVR model. 
%(ii) We adopt the Score Distillation Sampling (SDS)~\cite{poole2022dreamfusion} to generate high-quality and stylized 3D shapes.
%On the one hand, the SDS-guided refinement module significantly improves the generative fidelity beyond the pre-trained SVR model.
%On the other hand, the SDS-guided stylization module enables us to generate imaginary shapes by incorporating the semantic
%attributes of the target style into the shape; see Figure~\ref{fig:style_figure1}.
%By leveraging the 3D shape prior learned by our two-stage feature-space alignment, our extended approach ISS++ is able to further outperform~\cite{poole2022dreamfusion} for both surface quality and topology faithfulness, while typically requiring much fewer training iterations compared to~\cite{poole2022dreamfusion}.
%Also, this approach further extends the generation capability of our ISS++ to unseen categories out of the image dataset.

%\phil{TODO: please try to update this paragraph to step by step introduce each strategies/modules}



%Our new approach advances the frontier of 3D shape generation from text in the following aspects. First, by taking image as a stepping stone, we make the challenging text-guided 3D shape generation task more approachable and cast it as a single-view reconstruction (SVR) task. Having said that, we learn 3D shape priors from the adopted SVR model directly in the feature space.\textcolor{blue}{Second, benefiting from the learned 3D priors from the SVR model and the joint text-image embeddings, our approach can produce 3D shapes in only 85 sec. (ISS) or \lzz{TODO}. (ISS++) vs. 72 min. of Dream Fields~\cite{jain2021zero} and 90 min. of DreamFusion~\cite{poole2022dreamfusion}. Further, it is able to produce plausible 3D shapes beyond the generation capabilities of the state-of-the-art approaches; see Figure~\ref{fig:figure1} (b).} 

In summary, our approach expands the boundary of 3D shape generation from texts in the following aspects. First, we cast the challenging text-guided shape generation task to be a single-view reconstruction (SVR) task, which is more approachable. 
%by taking image as stepping stone, we make the challenging task more approachable and cast it as a single-view reconstruction (SVR) task. Having said that, we learn 3D shape priors from the adopted SVR model directly in the feature space. 
Second, our approach is efficient. It can create plausible 3D shapes in only 85 sec. with two-stage feature-space alignment and high-quality and stylized 3D shapes with Score Distillation Sampling in less than 30 min. compared to 72 min. of Dream Fields~\cite{jain2021zero} and 90 min. of DreamFusion~\cite{poole2022dreamfusion} (using the Stable DreamFusion version due to the lack of public code)\footnote{We use the latest version of an available public implementation of DreamFusion, Stable-Dreamfusion~\cite{stable-dreamfusion}, with the commit "099468e6" updated on Feb 7, 2023, as the official code has not been released.}. Furthermore, the generation capabilities of our approach outperform the generation capabilities of the state-of-the-art approaches; see Figure~\ref{fig:figure1} (b). Finally, our approach is generic, scalable, and compatible with a wide spectrum of SVR methods. 
%our approach can generate plausible 3D shapes beyond the generation capabilities of the state-of-the-art approaches; see Figure~\ref{fig:figure1} (b).

%\phil{put up a footnote to say that we use Stable DreamFusion version blah blah blah due to no public code of DF?} \lzz{DreamFusion paper says it needs 90 min., but Stable-Dreamfusion is much faster.}


%, enabling a wide range applications, \eg, designing 3D shapes made of different materials in AR/VR \etal.\xjqi{no need for the application}  %To improve the texture-shape consistency, we suggest a background augmentation to improve the texture-shape consistency in stylization. \pdai{why we need stylization? add more motivations? For example, manually designing realistic material for 3D shapes is complex and time-consuming. And the text-guided stylization is more efficient and effective.}%\xjqi{this is the application or extension}  
%
%Third, our approach is compatible with different single view reconstruction approaches \xjqi{add citations for different approaches}. For example, build upon SS3D~\cite{alwala2022pre} that can generate 3D shapes from single view in-the-wild images, we can easily extend our framework to generate a broad range of categorical 3D shapes from text. In addition, our approach is orthogonal to the single view reconstruction research direction, and can potentially achieve even better performance built upon stronger single view reconstruction models proposed in future. %\xjqi{maybe merge this part with the ``in this work'' session}
%
%\phil{In the last subsection in the introduction, we should summarize a bit on HOW or IN WHAT ASPECTS our results improve OVER Dream fields and CLIP-Forge. This is important as a preview to strengthen of this paper over the state-of-the-art}
%
%\xjqi{Our method advances the prior state of the arts in the following folds?? maybe highlight the major advancements?? I would suggest move the third part into the pargarph  }
%
%To summarize, first, going beyond CLIP-Forge~\cite{sanghi2021clip}, our two-stage feature space alignment learns 3D shape priors from the easily accessible multi-view images to eliminate the requirement of 3D shape data. Second, with the two-stage feature space alignment that gradually reduces the gap among $\Omega_T$, $\Omega_I$ and $\Omega_S$, our method produces high-quality 3D shapes in 85 seconds, compared with the 72 minutes of Dream Fields~\cite{jain2021zero}. 
%
\begin{comment}
Our contributions can be briefly summarized as follows.
First, our approach leverages images as stepping stone to leverage joint text-image embeddings with CLIP and learn 3D shape priors from a pre-trained SVR model, eliminating the requirement of text data like~\cite{chen2018text2shape,liu2022towards} and 3D shape data like CLIP-Forge~\cite{sanghi2021clip}. %compared with Text2shape, CLIP-Forge
With the learned 3D priors and joint text-image embeddings, our method can produce concrete 3D shapes in 85 seconds, compared with the 72 minutes of Dream Fields~\cite{jain2021zero} that only works on multi-view rendering. %compared with DreamFields
More importantly, with the two-stage feature-space alignment that gradually reduces the gap among $\Omega_\text{T}$, $\Omega_\text{I}$ and $\Omega_\text{S}$, our method produces high-quality 3D shapes, going beyond~\cite{sanghi2021clip} and~\cite{jain2021zero} as shown in Figure~\ref{fig:figure1} (a) to (c). %compared with CLIP Forge and Dream Field, in terms of results
Second, our text-guided stylization module can create novel textures beyond the pre-trained SVR model. 
Third, our approach is compatible with different SVR models~\cite{niemeyer2020differentiable,alwala2022pre}.
For example, build upon SS3D~\cite{alwala2022pre} that can generate 3D shapes from single-view in-the-wild images, we can easily extend our framework to generate a broad range of categorical 3D shapes from text. In other words, our approach is orthogonal to the SVR research direction, and can potentially achieve even better performance built upon stronger SVR models proposed in future. 
Our extensive experimental results on ShapeNet, CO3D and SS3D~\cite{alwala2022pre} single view categories also demonstrate the superiority of our approach.
\end{comment}
%
%The intensive experiments on ShapeNet~\cite{shapenet2015} manifest that our generative results with the two-stage alignment outperforms the existing works~\cite{sanghi2021clip,jain2021zero} and our baselines in terms of fidelity and text-shape consistency by a large margin (see Figure~\ref{fig:figure1}) both qualitatively and quantitatively. -
%Also, this is the first text-guided shape generation approach that is working on the real-world dataset CO3D~\cite{reizenstein2021common} to generate realistic shapes. %Furthermore, our approach supports text-guided shape stylization to generate novel textures beyond the pre-trained SVR model, and can be extended to generate a broad range categories.
%
%\lzz{Do you think we first talk about ISS++ refinement and stylization, then the compatibility of ISS like the current version, so that the stylization section can be more coherent, or move the below in front of ISS++ since the below is about ISS. )}
%\phil{Agree.
%(1) I also feel so when reading the paragraph above... see my changes and please modify
%(2) then, the purpose of the last paragraph in sec 1 is NOT to introduce the new changes BUT to SUMMARIZE what are the NEW contents in this journal submission}



\noindent \textbf{Different from Our Conference Paper.} This manuscript extends ISS~\cite{liu2023iss}, a spotlight paper to be presented soon at International Conference on Learning Representations 2023 in the following folds. First, we extend ISS with a diffusion prior~\cite{ramesh2022hierarchical} to generate more diversified 3D shapes while ensuring their consistency with the given text. Then, we propose an SDS-guided refinement module to further improve the fidelity of the generated shapes. Further than that, our SDS-guided stylization enables the generation of imaginary 3D shapes complementing our previous CLIP-guided stylization~\cite{liu2023iss}.  Last, we conduct more experiments on 3D shape generation and shape stylization, and compare ISS++ with the latest works CLIP-Mesh~\cite{mohammad2022clip} and DreamFusion~\cite{poole2022dreamfusion}. Our experimental results, both quantitative and qualitative, demonstrate that our approach is able to surpass the state-of-the-art methods in text-guided 3D shape generation.
%\phil{Use the above sentence IF we submit this paper to PAMI before the ICLR conference} \lzz{I see, thanks}
%
%\phil{Using a or an depends on the first sound of the next word, e.g., ``a spot'' but ``an SQL server''; ``an FBI agent''; see 
%https://www.britannica.com/dictionary/eb/qa/how-do-you-know-whether-to-use-a-or-an;
%https://www.englishclub.com/pronunciation/a-an.htm} \lzz{thanks}
%
%In this extended version, we first expand the diversified generation with a diffusion prior~\cite{ramesh2022hierarchical}, which enables us to generate more diversified 3D shapes while maintain the shape consistency with the given text.
%Besides, we propose an SDS-guided refinement procedure to further improve the fidelity of the generated shapes. Also, the technique enables us to generate imaginary 3D shapes, compensating our previous CLIP-guided stylization~\cite{liu2023iss}.
%Last, we conduct more experiments on 3D shape generation3D and shape stylization, and compare ISS++ with the very recent works CLIP-Mesh~\cite{mohammad2022clip} and DreamFusion~\cite{poole2022dreamfusion}\footnote{The official codes of DreamFusion~\cite{poole2022dreamfusion} have not been released. To our best effort, we adopt the latest version of a public implementation Stable-Dreamfusion~\cite{stable-dreamfusion} with the commit ``099468e6'' updated on Feb 7, 2023.}.
%Both quantitative and qualitative experimental results manifest the superiority of our method on text-guided 3D shape generation beyond the state-of-the-art methods.}

%\lzz{Also, our generative quality can be further enhanced building upon stronger SVR models. } 
%In addition, our approach can potentially achieve better performance when working with a stronger SVR model in the future. %Our extensive experimental results on ShapeNet, CO3D and SS3D~\cite{alwala2022pre} single view categories also demonstrate the superiority of our approach. 
%In other words, beyond a specific solution to the task of text-guided 3D shape generation, our approach is a general solution to effectively reduce this challenging task to SVR, which has been intensively studied.
%\phil{I suggest removing the above two sentences in introduction as it talks too much about adopting a future SVR, which seems to be too little in terms of contributions and hard to strongly justify, as this is about SVR models in the future... we may say this later but not that early in the introductiojn}
%
%\textbf{We will release our code and models upon publication.}




%The qualitative and quantitative evaluations on ShapeNet~\cite{shapenet2015} demonstrate that our ISS outperforms existing works~\cite{sanghi2021clip,jain2021zero} and baseline approaches by a large margin in terms of generative fidelity and text-shape consistency. In addition, this is the first text-guided shape generation approach that is working on the real-world dataset CO3D~\cite{reizenstein2021common} to generate realistic shapes. At last, built upon SS3D~\cite{alwala2022pre}, our approach can further be extended to generate wide range of 3D shapes. \textbf{We will put ours codes and models on GitHub after the publication of this work. }



%This is the first text-to-shape work that is working on real shape dataset (CO3D): realistc 3D shapes. \xjqi{Highlight our performance.We achieve xxx in xxx time. Our model can synthesis 3D shapes for 50 object categories. Application??}

%is inferior in capturing the 



%In this work, we use Differentiable Volumetric Rendering (DVR)~\cite{niemeyer2020differentiable} as a representative. We found that CLIP  






%Different from the challenging text-guided shape generation, two related research fields including zero-shot text-guided image generation~\cite{ramesh2021zero,nichol2021glide,zhou2021lafite} and single view reconstruction (SVR)~\cite{chen2019learning,li2021d2im,niemeyer2020differentiable,alwala2022pr} achieve impressive progress in the last few years. The success of above works suggests a new potential approach for text-guided shape generation: first create an image from the text, and then generate the 3D shape from the image. However, this approach still suffer from several issues. To discuss it, we roughly divide the existing works of zero-shot text-guided image generation into two branches: training with text-image pairs~\cite{ramesh2021zero,ding2021cogview,nichol2021glide,ramesh2022hierarchical} and training without text-image pairs~\cite{zhou2021lafite,wang2022clip}. The images from the first branch depend on its collected training set, which typically has a large image domain gap between the images for training the SVR models. Luckily, the above issue can be eliminated by the second branch methods training on the same image dataset for SVR model training. 







%\vspace*{-5pt}
\section{Related Works}

%\phil{In Sec 2, rather than using the subsection command in latex... how about just using paragraph or textbf?  Otherwise, words like Single-view reconstruction may become too stand-out} 

% Philip: Usually, we put -3pt before subsection... I did that in most of my papers before


%The remarkable success of single view reconstruction and the strong correlation between 2D image and 3D shape motivate us to reduce the challenging text-to-shape generation into single view reconstruction. Building upon a single view reconstruction model, our approach can effectively generate high-fidelity shapes from the text input. In this work, we build our approach on DVR~\cite{niemeyer2020differentiable} and SS3D~\cite{alwala2022pre} as representatives \phil{this work is not clear} for text-guided shape generation with multi-view and single-view images in training.

%\phil{(1) SVR is certainly related to this paper BUT SVR is kind of a image-guided 3D generation.  It has a clear prior, which is an image, for generating 3D shapes.  We need to explicitly say how SVR is related to this work and WHY it is different, cause some SVR papers can create pretty nice results compared to ours, since the input images are very good prior and provide very rich information for 3D shape generation; (2) }

%\vspace*{1mm}
\textbf{Text-Guided Image Generation.}
%Existing text-guided image generation approaches can be roughly cast into two branches: (i) direct image synthesis~\cite{reed2016generative,reed2016learning,zhang2017stackgan,zhang2018stackgan++,xu2018attngan,li2019controllable,li2020manigan,qiao2019mirrorgan,wang2021cycle} and (ii) image generation with a pre-trained GAN~\cite{stap2020conditional,yuan2019bridge,souza2020efficient,wang2020text,rombach2020network,patashnik2021styleclip,xia2021tedigan}. Yet, the above works can only generate images for limited categories. To address this issue, some recent works explore zero-shot text-guided image generation~\cite{ramesh2021zero,ding2021cogview,nichol2021glide,liu2021fusedream,ramesh2022hierarchical,saharia2022photorealistic} to learn to produce images of any category. Recently,~\cite{zhou2021lafite} and ~\cite{wang2022clip} leverage CLIP for text-free text-to-image generation.
%Text-guided shape generation is more challenging compared with text-to-image generation. First, it is 
%far more labor-intensive and difficult
%to prepare a large amount of paired text-shape data than paired text-image data, which can be collected from the Internet on a large scale.
%Second, the text-to-shape task requires one to predict full 3D structures that 
%should be plausible geometrically and 
%consistently in all views, 
%beyond the needs in single-view image generation.
%Third, 3D shapes may exhibit more complex spatial structures and topology, beyond regular grid-based 2D images.
%Existing text-guided image generation approaches can be roughly cast into two branches: (i) direct image synthesis~\cite{reed2016generative,reed2016learning,zhang2017stackgan,zhang2018stackgan++,xu2018attngan,li2019controllable,li2020manigan,qiao2019mirrorgan,wang2021cycle} and (ii) image generation with a pre-trained GAN~\cite{stap2020conditional,yuan2019bridge,souza2020efficient,wang2020text,rombach2020network,patashnik2021styleclip,xia2021tedigan}. 
Text-guided image synthesis has been intensively studied in recent years~\cite{reed2016generative,reed2016learning,zhang2017stackgan,zhang2018stackgan++,xu2018attngan,li2019controllable,li2020manigan,qiao2019mirrorgan,wang2021cycle,stap2020conditional,yuan2019bridge,souza2020efficient,wang2020text,rombach2020network,patashnik2021styleclip,xia2021tedigan}.
%Yet, the above works can only generate images for limited categories. 
Leveraging auto-regressive and diffusion models, recent works achieve impressive performance on text-guided image generation~\cite{ramesh2021zero,ding2021cogview,nichol2021glide,liu2021fusedream,ramesh2022hierarchical,saharia2022photorealistic} to produce images of any category. To eliminate the need for text data, Wang et al.\cite{wang2022clip} and Zhou et al.\cite{zhou2021lafite} explore the text-free text-to-image generation leveraging CLIP.

Beyond text-guided image generation, it is more challenging to create 3D shapes from the text. First, unlike paired text-image data that can be collected from the Internet, it is much more laborious and challenging to acquire large-scale paired text-shape data. Second, the text-to-shape generation aims to predict the complete 3D structures beyond a single 2D view in text-guided image generation. Third, there are more complex spatial structures and topologies in 3D shapes beyond 2D images with regular pixel grids, making it even more challenging to generate 3D shapes from a piece of text. 

%Text-guided shape generation is more challenging compared with text-to-image generation. First, it is far more labor-intensive and difficult to prepare a large amount of paired text-shape data than paired text-image data, which can be collected from the Internet on a large scale. Second, the text-to-shape task requires one to predict full 3D structures that should be plausible geometrically and consistently in all views, beyond the needs in single-view image generation. Third, 3D shapes may exhibit more complex spatial structures and topology, beyond regular grid-based 2D images. 

\vspace*{3mm}
\noindent\textbf{Text-Guided 3D Shape Generation.}
%To generate shapes from text, several works~\cite{chen2018text2shape,jahan2021semantics,liu2022towards} rely on paired text-shape data for training.
%To avoid paired text-shape data, 
%the very recent works, CLIP-Forge~\cite{sanghi2021clip}, Dream Fields~\cite{jain2021zero}, and CLIP-Mesh~\cite{mohammad2022clip} attempt to leverage the large-scale pre-prained vision-language model CLIP and another work \textcolor{blue}{DreamFusion~\cite{poole2022dreamfusion} to exploit pre-trained text-to-image models.}
%Yet, they still suffer from various limitations, as discussed in the third subsection of Section~\ref{sec:intro}. 
%Besides 3D shape generation, some recent works utilize CLIP to manipulate a shape or NeRF with text~\cite{michel2021text2mesh,jetchev2021clipmatrix,wang2021clipnerf,chen2022tango} and to generate 3D avatars~\cite{hong2022avatarclip}.
%In this work, we present a new framework for generating 3D shape from text without paired text-shape data by our novel two-stage feature-space alignment.
%Our experimental results demonstrate the superiority of this work beyond the existing ones in terms of fidelity and text-shape consistency.
%In the field of text-guided shape generation, 
In this research field, some approaches require paired text-shape data, such as \cite{chen2018text2shape,jahan2021semantics,liu2022towards}. However, to avoid the need for paired data, recent works such as CLIP-Forge\cite{sanghi2021clip}, Dream Fields~\cite{jain2021zero}, CLIP-Mesh~\cite{mohammad2022clip}, and DreamFusion~\cite{poole2022dreamfusion} leverage pre-trained vision-language models or text-to-image models. Despite their advancements, these approaches still have limitations, as discussed in Section~\ref{sec:intro}. Moreover, some works use CLIP to manipulate 3D shapes/NeRF using text~\cite{michel2021text2mesh, jetchev2021clipmatrix, wang2021clipnerf, chen2022tango} and generate 3D avatars~\cite{hong2022avatarclip}. In contrast, our approach presents a new framework for text-guided 3D shape generation without the need for paired text-shape data, using the newly proposed two-stage feature-space alignment. Our experimental results demonstrate superior fidelity and text-shape consistency beyond existing methods.

\vspace*{3mm}
\noindent\textbf{Differentiable Rendering}. 
%3D rendering is an important topic in computer vision and graphics. From a 3D scene, it predicts a 2D view for a given camera pose. Beyond 3D rendering, differentiable rendering aims to derive the differentiation of the rendering function, so that 
%a renderer can be integrated into an optimization framework for reconstructing a 3D shape from multi-view 2D images. 
%Neural Volume Rendering~\cite{mildenhall2020nerf} and its following works~\cite{jain2021putting,barron2021mip} aim to synthesize novel view images of a 3D scene.
%Recent works~\cite{niemeyer2020differentiable,munkberg2022extracting,gao2022get3d} leverage differentiable rendering for 3D shape generation using 2D images. In this work, we derive 2D images of generated 3D shape using differentiable rendering and use a pre-trained large-scale image-language model CLIP to encourage consistency between 2D images and input texts. Thanks to differentiable rendering, we can update the generated 3D shape indirectly using the rendered images. 
%Differentiable rendering 
As a powerful technique, differentiable rendering enables 3D models to be optimized using 2D images. There are numerous applications, such as generating 3D shapes from 2D images or reconstructing 3D objects from multiple 2D views. By modeling the rendering process as a differentiable function, gradients can be computed with respect to the input parameters of the function, allowing for efficient optimization using gradient-based techniques. This has led to significant advances in fields such as computer vision and computer graphics. Recent works~\cite{niemeyer2020differentiable,munkberg2022extracting,gao2022get3d} leverage differentiable rendering for 3D shape generation using 2D images. In this work, we derive 2D images of the generated 3D shape using differentiable rendering and use a pre-trained large-scale image-language model CLIP to encourage consistency between 2D images and input texts. Thanks to differentiable rendering, we can update the generated 3D shapes indirectly using the rendered images. 

\vspace*{3mm}
\noindent\textbf{Single-View Reconstruction.} %Image-to-Shape Generation}
%\phil{(1) Move this SVR subsection to the end of Section 2?   It is because this subsection is the least related compared with the other two and we also don't want the primary to find reviewers in this area.
%(2) Try to shorten this subsection (esp. the second subsection here).
%(3) start it like this: Another topic that is related to this work is single-view reconstruction, in which researchers have explored SVR with meshes...}
%
%Another topic related to this work is single-view reconstruction (SVR).
%Recently, researchers explore SVR with meshes~\cite{agarwal2020gamesh}, voxels~\cite{zubic2021effective}, and 3D shapes~\cite{niemeyer2020differentiable}.
%Further, to extend SVR to in-the-wild categories,~\cite{alwala2022pre} propose SS3D to learn 3D shape reconstruction using single-view images in hundreds of categories. 
This work is also related to single-view reconstruction (SVR). SVR has recently been explored with voxels~\cite{zubic2021effective}, meshes~\cite{agarwal2020gamesh}, and implicit fields~\cite{niemeyer2020differentiable,alwala2022pre}. %In order to extend SVR to in-the-wild categories,~\cite{alwala2022pre} proposed SS3D, a method for learning 3D shape reconstruction using single-view images in hundreds of categories. 
%\lzz{we may not need to describe SS3D~\cite{alwala2022pre} here separately. }
%\phil{ok}
In this work, we leverage an SVR model to bridge the image and shape modalities, thus allowing us to use 2D images as a stepping stone to produce 3D shapes from texts. Moreover, our approach is flexible since we map the features in the latent space implicitly rather than explicitly.
%our approach performs the mapping and feature alignment implicitly in the latent space rather than explicitly.
%\lzz{Is it good to add the below details about SVR?}
%\phil{If we have the contents in the ICLR paper, it is better to keep it for continuity... just in case a reviewer is from papers in this part... then?}
%\lzz{We have the contents in supplementary files since one reviewer asked for it. Last time you suggested do not adding too much SVR content to avoid reviewers in this research area.}
%\phil{I see.}
%In our work, we propose to harness an SVR model to map images to shapes, such that we can take 2D image as a stepping stone for producing shapes from texts.
%Yet, we perform the mapping and feature alignment implicitly in the latent space rather than explicitly.


%\paragraph{Single-view reconstruction (SVR)} This task aims to reconstruct the 3D shape from a single-view image of it. Recently, many approaches have been proposed for meshes~\cite{agarwal2020gamesh}, voxels~\cite{zubic2021effective}, and 3D shapes~\cite{niemeyer2020differentiable}. Recently, to extend SVR to in-the-wild categories,~\cite{alwala2022pre} proposed a new approach called SS3D to learn 3D shape reconstruction using single-view images for the reconstruction of hundreds of categories. 

%As 3D-to-2D projections, single-view 2D images are more closely related to shapes than texts, since they reveal many attributes of 3D shapes, e.g., structure, details, appearance, etc.
%The strong correlation between 2D images and 3D shapes motivates us to reduce the challenging text-to-shape generation task to text-to-image and then SVR, by connecting the CLIP features from text with the shape features in SVR using images as an intermediate step to gradually bridge the gap between text and shape.
%Specifically, we extend the pre-trained SVR model to be compatible with text input, transforming the challenging text-to-shape task into SVR. Our framework can work with different SVR approaches to extend them for 3D shape generation from texts. So, our approach is orthogonal to the SVR approaches.
%


%\textcolor{purple}{As 3D-to-2D projections, single-view 2D images are more closely related to shapes than texts, since they reveal many attributes of 3D shapes,~\eg, structure, details, appearance, etc.
%The strong correlation between 2D images and 3D shapes motivates us to reduce the challenging text-to-shape generation task to 
%text-to-image then SVR, by connecting the CLIP features from text with the shape features in SVR.}
%Specifically, 
%our framework can work with different SVR approaches to extend them for 3D shape generation from texts.
%So, our approach is orthogonal 
%to the SVR approaches.
%Specifically, we build our approach on DVR~\cite{niemeyer2020differentiable} and SS3D~\cite{alwala2022pre} as two representatives, which take multi- and single-view images in training, respectively.



%however, the generated fidelity are far from satisfying, and the need of 3D shapes for training restricts their generation to a handful of categories. Another recent work Dream Fields~\cite{jain2021zero} handles shape generation in a wide range of categories from text. However, it only train a NeRF, and cannot produce concrete 3D shapes directly. The unsatisfying generation speed and unlifelike generative quality make it hard to be practical utilized. 

%\phil{please double check and ensure all recent related works have been cited}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace*{-5pt}
\section{Methodology}

%\vspace*{-3pt}
\begin{figure*}
\centering
\includegraphics[width=0.99\textwidth]{overview_new1.pdf}
\vspace*{-2mm}
\caption{
%Overview of our text-guided 3D shape generation framework, which has three major stages.
%(a) Leveraging a pre-trained SVR model, in stage-1 feature-space alignment, we train the CLIP2Shape mapper $M$ to map the CLIP image feature space $\Omega_{\text{I}}$ 
%to shape space $\Omega_{\text{S}}$
%of the SVR model with $E_\text{S}$, $E_\text{I}$ frozen,
%and fine-tune decoder $D$ with an additional background loss $L_{\text{bg}}$. $M$ and $D$ are trained with their own losses separately at the same time by stopping the gradients from SVR loss $L_D$ and background loss $L_{bg}$ to propagate to $M$.
%(b) In stage-2 feature-space alignment, we fix $D$ and %\phil{freeze $D$ and} 
%fine-tune $M$ into $M'$ by encouraging CLIP consistency between input text $T$ and the rendered images at test time.
%(c, d) Last, we optimize \textcolor{blue}{the quality and} the style of the 
%generated shape and texture of $S$ for $T$. %\phil{for $T$?}
%At the inference, we use stage 2 to generate 3D shape from $T$ and (c, d) are optional.
%\phil{please update the symbols in figure caption: $F$ to $\Omega$}
%\pdai{Should the shape of the table in s and (c) remain the same?}
%\phil{please move every symbol and text label closer to the image that it describes,~\eg, $E_I$ is quite far from its associated image;
%also center align every text label with its associated image}
%\phil{I now understand that you keep using $F_{something}$ to mean feature space... that's okay but perhaps $\Omega$ is better than $F$, you may decide but need to make it consistent everywhere}
Overview of our two-stage feature-space alignment. 
(a) In the first stage, we align the CLIP image feature space $\Omega_{\text{I}}$ and the shape space $\Omega_{\text{S}}$ of a pre-trained single-view reconstruction (SVR) model with a CLIP2Shape mapper $M$, which maps images to shapes while keeping $E_\text{S}$ and $E_\text{I}$ frozen. Then we fine-tune the decoder $D$ using $L_{\text{bg}}$ to encourage the background color to be white. During training, we stop gradients of the SVR loss $L_D$ and the background loss $L_{\text{bg}}$ to eliminate their effects on $M$.
(b) In the second stage, we introduce a fast-time optimization by fixing the decoder $D$ and fine-tuning the mapper $M$ to $M'$, further encouraging the CLIP consistency between the rendered images of the generated shape and the input text $T$.
%(c and d) Finally, we conduct text-guided stylization and refinement. 
%In inference, we adopt the second stage to generate the 3D shape from the input text $T$, and (c d) are optional.
}
\label{fig:overview}
\vspace*{-2.5mm}
\end{figure*}







\subsection{Overview}

%This work aims to generate 3D shape $S$ from text $T$. 
%Overall, our idea is to map the CLIP features to the shape space of a pre-trained SVR model, such that we can leverage the joint text-image embeddings from CLIP and also the 3D generation capability of the SVR model to enhance the generation of 3D shape from text.
%Hence, our method only needs to be trained with multi-view RGB or RGBD images and the associated camera poses without paired text-shape data. 
%As Figure~\ref{fig:overview} shows, our framework 
%includes (i) image encoder $E_{\text{S}}$, which maps input image $I$ to SVR shape space $\Omega_{\text{S}}$, 
%(ii) pre-trained CLIP text and image encoders $E_{\text{T}}$ and $E_{\text{I}}$, which map text $T$ and image $I$ to CLIP spaces $\Omega_{\text{T}}$ and $\Omega_{\text{I}}$, respectively,
%(iii) mapper $M$ with 12 fully-connected layers, each followed by a Leaky-ReLU,
%and (iv) decoder $D$ to generate the final shape $S$.
%Specifically, we use DVR~\cite{niemeyer2020differentiable} as the SVR model when presenting our method, unless otherwise specified.

To generate 3D shape $S$ from text $T$ without relying on paired text-shape data, we map the CLIP features to a latent shape feature space of a pre-trained SVR model, leveraging the joint text-image feature embeddings from CLIP and also the 3D shape prior learned by the SVR model. 
%So, our method only needs to be trained with multi-view RGB/RGBD images and the associated camera poses without paired text-shape data. 
Here, we leverage multi-view RGB/RGBD images and the corresponding camera poses for training, without needing the paired text-shape data.
The framework has four components: (1) image encoder $E_{\text{S}}$ to map the input image $I$ to shape space $\Omega_{\text{S}}$ of the SVR model, (2) pre-trained CLIP image and text encoders $E_{\text{I}}$ and $E_{\text{T}}$ that map image $I$ and text $T$ to CLIP feature spaces $\Omega_{\text{I}}$ and $\Omega_{\text{T}}$, (3) mapper $M$ consisting of 12 fully-connected and Leaky-ReLU layers to map CLIP image features to the latent shape space $\Omega_{\text{S}}$ of SVR, and (4) decoder $D$ that generates the 3D shape $S$. The proposed approach uses DVR~\cite{niemeyer2020differentiable} as the SVR model in the experiments unless specified otherwise.

Generally speaking, we present a novel two-stage feature-space alignment approach to bridge the image, text, and shape modalities. First, we train the mapper $M$ to bridge the CLIP image space $\Omega_\text{I}$ and the shape space $\Omega_\text{S}$, as shown in Figure~\ref{fig:overview}(a). Afterward, at test time, $M$ is fine-tuned to further bridge the CLIP text space $\Omega_\text{T}$ and $\Omega_\text{S}$, as shown in Figure~\ref{fig:overview}(b). Finally, we can optionally improve the texture and structure generation capability of our model by fine-tuning the decoder $D$ (as shown in Figure~\ref{fig:overview_style}). % In detail, we first train CLIP2Shape mapper $M$ to connect the CLIP image space $\Omega_\text{I}$ and the shape space $\Omega_\text{S}$ from the pre-trained SVR model (see Figure~\ref{fig:overview} (a)). Then, we fine-tune $M$ at test time using a CLIP consistency loss $L_c$ to further connect the CLIP text space $\Omega_\text{T}$ with $\Omega_\text{S}$ (see Figure~\ref{fig:overview} (b)). Last, we may further optimize the texture and structure style of $S$ by fine-tuning the decoders (see Figure~\ref{fig:overview} (c, d)). 

%In the following, we first introduce two empirical studies on the CLIP feature space 
%in Section~\ref{sec:empirical}, then present our two-stage feature-space-alignment approach in Section~\ref{sec:two-sgage}. Further, Section~\ref{sec:stylization} presents our text-guided shape \textcolor{blue}{refinement and} stylization method and Section~\ref{sec:compatible} discusses the compatibility of our approach with different SVR models and our extension to generate a broad range of categories \lzz{and high quality shapes}. 
In Section~\ref{sec:empirical}, we begin by presenting two empirical studies that investigate the properties of the CLIP feature space. We then introduce our two-stage feature-space alignment approach in Section~\ref{sec:two-sgage}. Following that, in Section~\ref{sec:stylization}, we present our method for text-guided shape refinement and stylization. Finally, in Section~\ref{sec:compatible}, we discuss that our approach is compatible with different SVR models and how we can extend our method to generate a wide range of categories and high-quality shapes.

\vspace*{-3pt}
\subsection{Empirical Studies and Motivations}\label{sec:empirical}

%\xjqi{please modify this section as discussed. Besides, it's better to move numbers to a table or use visualization to make it easier to understand}

%Existing works~\cite{sanghi2021clip,zhou2021lafite,wang2022clip} mostly
%utilize CLIP directly
%without analyzing 
%how it works and discussing its limitations. To start, we investigate the merits and drawbacks of leveraging CLIP for text-guided 3D shape generation by conducting the following 
%two empirical studies to gain more insight into the CLIP feature space. 
Prior works on text-guided 3D shape generation mainly use CLIP without analyzing its workings and limitations. To gain a better understanding of the CLIP feature space and its suitability for text-guided 3D shape generation, we conduct two empirical studies.





%please modify this section as discussed. 
%\phil{who wrote this? revised?}\lzz{xiaojuan}
%\phil{revised? Is this subsection ready for me to work on?}\lzz{yes. She has finished revision.}
%\phil{Got it!}


\begin{comment}
\begin{wrapfigure}{r}{0.3\textwidth}
  \begin{center}
    \includegraphics[width=0.28\textwidth]{motivation.pdf}
  \end{center}
  \caption{Single image reconstruction by DVR and $E_{\text{I}}$+$D$. }
  \label{fig:motivation}
\end{wrapfigure}
\end{comment}



\begin{figure*}
\centering
\includegraphics[width=0.99\textwidth]{motivation.pdf}
\vspace*{-1.5mm}
\caption{Results of empirical studies on CLIP feature spaces. %\pdai{How to obtain the numbers in (c)?}
%\phil{In (a): standardize the gaps around the two plus signs;
%(b): just photoshop the image such that the blue and green boxes and centered with the text labels above and below each of them?
%(c) stage-2 -> stage 2;}
%what is the distance from $M(f_T)$ to $f_s$?  If you need to add a number for this link, move 0.59 below its associated arrow}
} %\phil{on CLIP feature space for text-guided 3D shape generation}.}
%\phil{write more to describe what's going on above?}}
\label{fig:motivation}
\vspace*{-2.5mm}
\end{figure*}

\vspace*{-3pt}
\subsubsection{Whether the CLIP feature is suitable for 3D shape generation?} 

\begin{comment}
First, we study the representative capability of the CLIP image feature space $\Omega_\text{I}$ by trying to generate 3D shapes from this space. Specifically, we replace the SVR image encoder $E_{\text{S}}$ 
with the CLIP image encoder $E_\text{I}$, and optimize implicit decoder $D$ using multi-view losses like DVR~\cite{niemeyer2020differentiable} with $E_{\text{I}}$ frozen. 
This approach can be 
extended to text-to-shape generation by replacing $E_\text{I}$ with CLIP text encoder $E_\text{T}$ during the inference.
To compare the performance of $E_\text{S}$ and $E_\text{I}$, we evaluate 3D mIoU 
between their generated shapes and GTs. The results are as follows: the standard SVR pipeline $E_\text{S}$+$D$ achieves 47.86\% mIoU while replacing the SVR encoder $E_\text{S}$ with CLIP encoder $E_\text{I}$ ($E_\text{I}$+$D$) degrades the performance to 40.09\%. 
From the results and qualitative comparison shown in Figures~\ref{fig:motivation} (a, b), we can see that the CLIP image space $\Omega_\text{I}$ has \textit{inferior representative capability to capture details of the input image} for 3D shape generation. This is not surprising, since the pre-trained $E_\text{I}$ from CLIP is targeted to extract semantic-aligned features from
texts rather than extracting details from images.
Hence, image details relevant to 3D reconstruction are lost,~\eg, textures. On the contrary, $E_\text{S}$ from the SVR model is optimized for 3D generation from images, % \phil{from images,}
so it maintains more necessary details. The above result motivates us to design a mapper $M$ from $\Omega_\text{I}$ to $\Omega_\text{S}$ and then 
generate shapes 
from $\Omega_\text{S}$ instead of $\Omega_\text{I}$ for better generative fidelity.
\end{comment}

In the first empirical study, we investigate whether the CLIP image feature space $\Omega_\text{I}$ has enough representative capability for 3D shape generation by attempting to generate shapes from $\Omega_\text{I}$. 
To do so, we train the SVR model by adopting the CLIP image encoder $E_\text{I}$ to replace the original SVR image encoder $E_{\text{S}}$. At the same time, we optimize the decoder $D$ using the same loss function as DVR~\cite{niemeyer2020differentiable} with $E_{\text{I}}$ frozen.
%To do so, we replace the SVR image encoder $E_{\text{S}}$ with the CLIP image encoder $E_\text{I}$ and optimize the implicit decoder $D$ using multi-view losses like DVR~\cite{niemeyer2020differentiable} with $E_{\text{I}}$ frozen. 
This design is inspired by the motivation that we can generate 3D shapes from the text by adopting the CLIP text encoder $E_\text{T}$ to replace $E_\text{I}$ in inference. 
%This approach can be extended to text-to-shape generation by replacing $E_\text{I}$ with the CLIP text encoder $E_\text{T}$ during the inference. 
To evaluate the 3D shape generative capability $E_\text{S}$ and $E_\text{I}$, we measure the 3D mIoU of their generated shapes and ground truths (Figure~\ref{fig:motivation} (b)).
%To compare the performance of $E_\text{S}$ and $E_\text{I}$, we evaluate 3D mIoU between their generated shapes and GTs (Figure~\ref{fig:motivation} (b)). 
The result indicates that the representative capability of CLIP image encoder $E_\text{I}$ is inferior to $E_\text{S}$ due to its inferior capability to capture input image details that are necessary for 3D shape generation. 
%Our findings indicate that the CLIP image space $\Omega_\text{I}$ has the inferior representative capability to capture details of the input image for 3D shape generation. 
This result is easy to understand since CLIP image encoder $E_\text{I}$ has been optimized to extract semantic-aligned features with the paired text data in the training of CLIP, instead of being encouraged to capture image details. 
%This is not surprising, as the pre-trained $E_\text{I}$ from CLIP is targeted to extract semantic-aligned features from texts rather than extracting details from images. 
As a result in Figure~\ref{fig:motivation} (a), image details that are necessary for 3D reconstruction, such as textures, are overlooked by $E_\text{I}$. In contrast, $E_\text{S}$ in the SVR model is trained for 3D generation and is encouraged to capture the necessary image details. 
These results indicate that we can generate shapes from $\Omega_\text{S}$ instead of $\Omega_\text{I}$ to improve the generative quality. To do so, we design a mapper $M$ to map from CLIP image feature space $\Omega_\text{I}$ to shape space $\Omega_\text{S}$ to enable the generation from $\Omega_\text{S}$.
%These results motivate us to design a mapper $M$ from map from CLIP image feature space $\Omega_\text{I}$ to shape space $\Omega_\text{S}$ and then generate shapes from $\Omega_\text{S}$ instead of $\Omega_\text{I}$ for better generative fidelity.


\vspace*{-3pt}
\subsubsection{Does the CLIP image and text feature gap affects 3D shape generation?} \label{sec:empirical2}

%Second, we investigate the gap between the normalized CLIP image feature $f_\text{I} \in \Omega_\text{I}$ 
%and normalized CLIP text feature $f_\text{T}\in \Omega_\text{T}$;
%(see also the CLIP image and text feature spaces
%in Figure~\ref{fig:figure1} (a))
%and how such gap influences text-guided 3D shape generation. Specifically, 
%we randomly sample $300$ text-shape pairs from the text-shape dataset~\cite{chen2018text2shape}, then 
%evaluate the cosine distance between $f_\text{I}$ and $f_\text{T}$,~\ie, $d=1-\text{cosine\_similarity}(f_\text{I},f_\text{T})$, where $f_\text{I}$ is the CLIP feature of the rendered images from the corresponding shape. 
%We repeat the experiment and obtain $d(f_\text{T},f_\text{I})=0.783\pm0.004$. %=0.2168\pm0.0041$.
%The result reveals \emph{a certain gap between the CLIP text and image features in this dataset, even though they are paired.}
%Also, the angle in the feature space between the two features is around $\arccos(1-0.783)=1.35$ rad in this dataset~\cite{chen2018text2shape}. 
%Having said that, directly replacing $f_\text{I}$ with $f_\text{T}$ like~\cite{sanghi2021clip,zhou2021lafite} in inference may harm the consistency between the output shape and the input text.
%Taking Figure~\ref{fig:figure1} as an example, directly replacing $f_\text{I}$ with $f_\text{T}$ causes a cosine distance of 0.59 to $f_\text{S}\in \Omega_\text{S}$ (see Figure~\ref{fig:motivation} (c)), which is significantly larger than the distance between $f_\text{I}$ and $f_\text{S}$ (0.12). Our finding {is consistent with the findings in~\cite{liang2022mind}. It} motivates us to further fine-tune $M$ into $M'$ at test time, such that we can produce feature $M'(f_\text{T})$, which is closer to $f_\text{S}$ than $M(f_\text{T})$.


The second investigation aims to analyze the gap between the normalized CLIP text feature $f_\text{T}\in \Omega_\text{T}$ and image feature $f_\text{I} \in \Omega_\text{I}$ as shown in Figure~\ref{fig:figure1} (a) and examine how this gap affects text-guided 3D shape generation. 
Specifically, we measure the cosine distance between $f_\text{I}$ and $f_\text{T}$ based on the text and rendered images of the randomly selected 300 text-shape pairs from the text-shape dataset~\cite{chen2018text2shape} as follows:
\begin{equation}
d=1-\text{cosine\_similarity}(f_\text{I},f_\text{T}).
\end{equation}
%where $f_\text{I}$ is the CLIP feature of the corresponding rendered images from the shape. 
%Specifically, we randomly select 300 text-shape pairs from the text-shape dataset~\cite{chen2018text2shape} and calculate the cosine distance between $f_\text{I}$ and $f_\text{T}$, represented as $d=1-\text{cosine\_similarity}(f_\text{I},f_\text{T})$, where $f_\text{I}$ is the CLIP feature of the corresponding rendered images from the shape. 
The result $d(f_\text{T},f_\text{I})=0.783\pm0.004$ through three repetitions of the experiment suggests that there is still a certain gap between the paired text and image features. Additionally, the angle between the two features is around $\arccos(1-0.783)=1.35$ radians in this text-shape dataset~\cite{chen2018text2shape}. The above result implies that the generated 3D shape may not be consistent with the input text if we simply replace $f_\text{I}$ with $f_\text{T}$ in inference. As demonstrated in Figure~\ref{fig:motivation} (c), this simple strategy results in a cosine distance 0.45 to $f_\text{S}\in \Omega_\text{S}$, much larger than $d(M(f_\text{I}), f_\text{S})=0.21$. This finding is consistent with the results reported in~\cite{liang2022mind}. To address this issue, we propose to fine-tune $M$ into $M'$ at test time, aiming at producing a feature $M'(f_\text{T})$ that has a smaller distance to $f_\text{S}$ compared with $M(f_\text{T})$.






\vspace*{-5pt}

\subsection{Two-Stage Feature-Space Alignment}\label{sec:two-sgage}

%Following the above findings, we propose a two-stage feature-space-alignment approach to 
%first connect image space $\Omega_\text{I}$ and shape space $\Omega_{\text{S}}$ and further connect text space $\Omega_\text{T}$ to shape space $\Omega_{\text{S}}$ with the image space $\Omega_\text{I}$ as the stepping stone. 
Based on these findings, we propose a two-stage feature-space alignment approach that connects the image space $\Omega_\text{I}$ and shape space $\Omega_{\text{S}}$ in the first stage and further connects the text space $\Omega_\text{T}$ to shape space $\Omega_{\text{S}}$ in the second stage, with the image space $\Omega_\text{I}$ as a stepping stone. 

\vspace*{-3pt}
\subsubsection{Stage-1: CLIP image-to-shape alignment}
%Given multi-view RGB or RGBD images for training, the stage-1 alignment is illustrated in Figure~\ref{fig:overview} (a).
%Considering that shape space $\Omega_{\text{S}}$ contains richer object details than the image space $\Omega_{\text{I}}$, while $\Omega_{\text{I}}$ provides a joint text-image embedding with the input text space $\Omega_{\text{T}}$,
%we introduce a fully-connected CLIP2Shape mapper $M$ to map image feature $f_{\text{I}}$ to shape space $\Omega_{\text{S}}$. Taking a rendered image $I$ as input, $M$ is optimized with an $L_2$ regression between $M(f_{\text{I}})$ and standard SVR feature $f_\text{{S}}=E_{\text{S}}(I)$ according to Equation~(\ref{equ:stage-1}) below:

%Given multi-view RGB or RGBD images for training, the stage-1 alignment is illustrated in Figure~\ref{fig:overview} (a). 
Figure~\ref{fig:overview} (a) illustrates the stage-1 alignment. On the one hand, the shape space $\Omega_{\text{S}}$ is able to capture richer image details compared with CLIP image space $\Omega_{\text{I}}$. On the other hand, $\Omega_{\text{I}}$ helps to enable the text input thanks to its joint text-image embedding with $\Omega_{\text{T}}$. Inspired by the above two motivations,
we design a CLIP2Shape mapper $M$ consisting of $12$ fully-connected layers to map $f_{\text{I}}$ to $\Omega_{\text{S}}$. To optimize $M$, we use $L_2$ regression loss between the mapped CLIP image feature $M(f_{\text{I}})$ and pre-trained SVR feature encoder $f_\text{{S}}=E_{\text{S}}(I)$ as shown in Equation~(\ref{equ:stage-1}):
\begin{equation}
\begin{aligned}
\mathcal{L}_{M}=\sum^N_{i=1} ||E_{\text{S}}(I_i)-M(f_{\text{I},i})||^2_2 
\label{equ:stage-1}
\end{aligned}
\end{equation}
where $N$ and $f_{\text{I},i}$ indicates the total number of images for training and the normalized CLIP feature of $I_i$, respectively. 


%Also, we fine-tune decoder $D$ to encourage it to predict a white background, which helps the model to ignore the background and extract object-centric feature (see Figure~\ref{fig:bg}), while maintaining its 3D shape generation capability. 
%To this end, we propose a new background loss $L_\text{{bg}}$ in Equation~(\ref{equ:bg}) below to enhance the model's foreground object awareness to prepare for the second-stage alignment.  
In addition, we incorporate a fine-tuning module for decoder $D$ to encourage it to generate 3D shapes with a white background. This module helps the model to focus on object-centric features while ignoring the background (see Figure~\ref{fig:bg}). Specifically, we propose a novel background loss $\mathcal{L}_\text{{bg}}$ in Equation~(\ref{equ:bg}) below, which enhances the model's ability to capture foreground objects and prepares it for the second-stage alignment.

\begin{equation}
\begin{aligned}
\mathcal{L}_\text{bg}=\sum_p ||D_c(p)-1||_2^2 \mathbbm{1}(F\cap\text{ray}(o,p) =\emptyset)
\label{equ:bg}
\end{aligned}
\end{equation}
%where $\mathbbm{1}$ is the indicator function; $F=\{p: D_o(p)>t\}$ indicates the foreground region, in which the occupancy prediction $D_o(p)$ is larger than threshold $t$; $p$ is a query point; $\text{ray}(o,p)\cap F =\emptyset$ means the ray from camera center $o$ through $p$ does not intersect the foreground object marked by $F$; $D_c(p)$ is the color prediction at query point $p$.
where $p$ means a query point coordinate, $D_o(p)$ and $D_c(p)$ are the occupancy and color prediction of $p$, respectively. $F=\{p: D_o(p)>t\}$ indicates the object region, where $D_o(p)$ is greater than a pre-defined threshold $t$. $F\cap\text{ray}(o,p) =\emptyset$ means the background region where a ray connecting camera center $o$ and $p$ does not intersect the object. Besides, $\mathbbm{1}$ is the indicator function and $\mathbbm{1}(F\cap\text{ray}(o,p) =\emptyset)=1$ if $p$ is in the background region. 
%In a word, $L_{\text{bg}}$ encourages $D$ to predict the background region as white color (value 1), such that $E_\text{I}$ can focus on and better capture the foreground object.  In order to maintain the 3D shape generation capability of $D$, the loss function $L_{\text{D}}$ from~\cite{niemeyer2020differentiable} is also utilized.
To summarize, $\mathcal{L}_{\text{bg}}$ is designed to encourage the background region to be predicted as the white color (value 1) and assist the model in better capturing the generated shape. Besides, the same set of loss functions $\mathcal{L}_{\text{D}}$ from DVR~\cite{niemeyer2020differentiable} is still adopted for maintaining the capability to generate 3D shapes of the SVR model.

Hence, the total loss in stage 1 is $\lambda_{M}\mathcal{L}_{M}$ for mapper $M$ and $\lambda_{\text{bg}}\mathcal{L}_{\text{bg}}+\mathcal{L}_{D}$ for decoder $D$, where $\lambda_{\text{bg}}$ and $\lambda_{M}$ indicate loss weights. 
The stage-1 alignment is trained with multi-view RGB/RGBD images and provides a good starting point for the stage 2 per-text optimization. 



\vspace*{-3pt}
\subsubsection{Stage-2: text-to-shape alignment}

%To further connect the text and shape modalities, the stage-2 alignment aims to find a shape $S$ that best matches the input text $T$. However, there is a gap between the text and image CLIP features $f_{\text{T}}$ and $f_{\text{I}}$ as discussed earlier in the second empirical study. By doing so, we can encourage the shape to be consistent with the input text. Directly optimizing the similarity of text and shape features is not feasible, hence we bridge the gap through the image modality.

After bridging image and shape modalities, we further try to bridge the text and shape modalities by proposing a fast test-time optimization in stage 2 that seeks to minimize the gap between the CLIP features of the input text $T$ and image $I$, as discussed in the second empirical study. By doing so, we can encourage the generated shape $S$ to be more consistent with the input text. Since we cannot directly optimize the similarity between text and shape features, reducing the semantic gap between $f_\text{T}$ and $f_\text{I}$ provides an effective way to align the two modalities and improves the overall performance of the model.

%As shown in Figure~\ref{fig:overview} (b), given input text $T$, we replace image encoder $E_{\text{I}}$ with text encoder $E_{\text{T}}$ to extract CLIP text feature $f_{\text{T}}$, then fine-tune $M$ with CLIP consistency loss between input text $T$ and $m$ images $\{R_i\}_{i=1}^m$ rendered with random camera poses from output shape $S$; see Equation~\ref{equ:stage-2}:

As illustrated in Figure~\ref{fig:overview} (b), the stage-2 alignment starts by replacing $E_{\text{I}}$ with $E_{\text{T}}$ to extract CLIP text feature $f_{\text{T}}$, given the input text $T$. We then fine-tune the mapper $M$ using a CLIP consistency loss to reduce the gap between the input text $T$ and $m$ rendered images $\{R_i\}_{i=1}^m$ captured from random camera viewpoints of the output shape $S$. The CLIP consistency loss is defined in Equation~\ref{equ:stage-2}. By minimizing this loss, we encourage the output shape to be consistent with the input text.

\begin{equation}
\begin{aligned}
\mathcal{L}_{\text{C}}=\sum_{i=1}^m{\langle{f_\text{T}} \cdot \frac{E_{\text{I}}(R_i)}{||E_{\text{I}}(R_i)||}\rangle} %\cdot f_{\text{I},i}\rangle}
\label{equ:stage-2}
\end{aligned}
\end{equation}
%L_{M\_stage-2}= \sum_{i=1}^r \frac{\langle E_{CLIP\_I}(R_i),E_{CLIP\_{T}}(T)\rangle}{||E_{CLIP\_I}(R_i)||_2||E_{CLIP\_{T}}(T)||_2}
where $\langle\cdot\rangle$ indicates the inner-product. % and 
%$f_{\text{I},i}$ = $E_{\text{I}}(R_i)$. %, and $||\cdot||_2$ indicates the two-norm. 
%\phil{replace $f_{\text{I},i}$ with $E_{\text{T}}(R_i)$ in the equation?}



\begin{figure*}
\centering
\includegraphics[width=0.99\textwidth]{bg_aug.pdf}
\vspace*{-1.5mm}
\caption{Generating shapes from text with and without our background loss $\mathcal{L}_\text{bg}$. Input text: A red car. 
%\phil{where is this figure referenced? please move it to later pages? closer to the location that it is first referenced.}
%\caption{Effect of \phil{generating shapes from the same text with/without} background loss $L_\text{bg}$.
%\phil{where is this figure referenced? please move it to later pages? closer to the location that it is first referenced.}
}
% \xjqi{seems too small figures??} }
\label{fig:bg}
\vspace*{-2.5mm}
\end{figure*}

%In stage-2 alignment, we still adopt $L_{\text{bg}}$ to enhance the model's foreground awareness.
%Comparing Figures~\ref{fig:bg} (a) and (b), we can see that the stage-2 alignment is able to find a rough
%shape with $L_\text{{bg}}$ in around 
%five iterations, yet failing to produce a reasonable output without $L_\text{{bg}}$,
%since having 
%the same color prediction on both foreground and background
%hinders the object awareness of the model.

In stage-2 alignment, we continue to use $\mathcal{L}_{\text{bg}}$ to improve the model's object awareness. Figures~\ref{fig:bg} (a) and (b) indicate that the model can find a rough shape that fits the input text in about five iterations when $\mathcal{L}_\text{{bg}}$ is used. On the other hand, without $\mathcal{L}_\text{{bg}}$, the model fails to produce a reasonable output because the same color predicted in both the object and background regions impedes the model's ability to perceive the object.

%Thanks to the joint text-image embedding of CLIP, the gap between text feature $f_\text{T}$ and shape feature $f_\text{S}$ has already been largely narrowed by $M$.
%Therefore, 
%the stage-2 alignment 
%only needs to fine-tune $M$ with
%20 iterations using the input text, taking only around 85 seconds on a single GeForce RTX 3090 Ti, compared with 72 minutes taken by  
%Dream Fields~\cite{jain2021zero} and 90 minutes by DreamFusion~\cite{poole2022dreamfusion} at test time.
%After this fine-tuning, 
%we can readily obtain a plausible result; see,~\eg, the ``result'' shown in Figure~\ref{fig:bg} (b). Our ISS is a novel and efficient approach for 3D shape generation from text.

%Thanks to the joint text-image embedding of CLIP, the gap between text feature $f_\text{T}$ and shape feature $f_\text{S}$ has already been largely narrowed by $M$. 
Our stage-1 alignment has already narrowed the semantic gap between text space $\Omega_\text{T}$ and shape space $\Omega_\text{S}$ with $M$.
Hence, the stage-2 alignment just requires fine-tuning $M$ using a CLIP consistency loss with the input text for only 20 iterations. This fine-tuning takes around 85 seconds on one GeForce RTX 3090 Ti GPU, which is significantly faster than Dream Fields~\cite{jain2021zero} and DreamFusion~\cite{poole2022dreamfusion}, taking 72 minutes and 90 minutes, respectively. After stage-2 alignment, a plausible result can be obtained readily, shown as ``result'' in Figure~\ref{fig:bg} (b). Our two-stage feature-space alignment is a novel approach that can generate 3D shapes from text efficiently, which significantly reduces the test time compared to previous methods.


\vspace*{-3pt}
\subsubsection{Diversified 3D shape generation}

%\xjqi{please modify the following.}
%In general, shape generation from text is one-to-many. To extend our approach with diversified 3D shape generation from the same piece of input text, we adopt the diffusion prior~\cite{ramesh2022hierarchical} to sample a corresponding CLIP image feature $f_\text{I}$ from $f_\text{T}$, instead of directly adopting $f_\text{T}$ as the training objective in stage 2, as shown in Figure~\ref{fig:overview} (b) ``diffusion prior''.
%Specifically, we extend Equation~\ref{equ:stage-2} to be Equation~\ref{equ:div} for diversified generation. 
%\begin{equation}
%\begin{aligned}
%L_{\text{C}}=\sum_{i=1}^m{\langle{\tau P(f_\text{T})+(1-%\tau(f_\text{T})))} \cdot \frac{E_{\text{I}}(R_i)}{||E_{\text{I}}(R_i)||}\rangle} %\cdot f_{\text{I},i}\rangle}
%\label{equ:div}
%\end{aligned}
%\end{equation}
%where P indices the the diffusion model to predict $f_\text{I}$ from $f_\text{T}$, and $\tau$ is a hyper-parameter to balance the diversity and the text-shape consistency; larger $\tau$ enhances the diversity while smaller $\tau$ encourages the consistency. 
Generally speaking, 3D shape generation from text is a one-to-many task, meaning that multiple plausible shapes can correspond to the same piece of text. 
To account for this, instead of using a single objective using $f_\text{T}$ to construct $\mathcal{L}_\text{C}$, we propose to sample features from a pre-trained text-to-image diffusion model~\cite{ramesh2022hierarchical} which can generate features $f_{\text{T}\rightarrow\text{I}}$ in the CLIP image feature space from a single input text CLIP feature $f_{\text{T}}$. 
At each time, we obtain one text-to-image feature by sampling a random noise and obtain $f_{\text{T}\rightarrow\text{I}}$ which is further combined with the original text feature $f_\text{T}$ to construct $\mathcal{L}_C$ as 
%to generate multiple images features $f_\text{I}$ from input text's CLIP feature $f_\text{T}$
\begin{equation}
\begin{aligned}
\mathcal{L}_{\text{C}}=\sum_{i=1}^m{\langle{ (\tau f_{\text{T}\rightarrow\text{I}}+(1-\tau) f_\text{T}}) \cdot \frac{E_{\text{I}}(R_i)}{||E_{\text{I}}(R_i)||}\rangle}.  %\phil{need . here} \phil{sorry, should be , instead due to the where after it}  
\label{equ:div}
\end{aligned}
\end{equation}
where $f_{\text{T}\rightarrow\text{I}}$ is the predicted $f_\text{I}$ from $f_\text{T}$ by diffusion prior~\cite{ramesh2022hierarchical} with sampled random noise and $\tau$ is a hyperparameter that balances diversity and text-shape consistency; a larger $\tau$ leads to more diverse shapes, while a smaller $\tau$ encourages more consistency between the text and shape.
By sampling multiple random noises which deliver multiple $f_{\text{T}\rightarrow\text{I}}$ and constructing different consistency objective $\mathcal{L}_C$, our model can be optimized to generate diverse results at the test time; see Figure~\ref{fig:overview} (b) ``diffusion prior''. This allows our model to create diverse 3D shapes for the same piece of input text.
%To account for this, we further propose to adopt the diffusion prior~\cite{ramesh2022hierarchical} to sample the CLIP image feature $f_\text{I}$ from the input text's CLIP text feature $f_\text{T}$, instead of directly using $f_\text{T}$ as the training objective in stage 2, as shown in Figure~\ref{fig:overview} (b) ``diffusion prior''.
%To achieve diversified generation, we extend Equation~\eqref{equ:stage-2} to Equation~\eqref{equ:div} using the sampled image feature $P(f_{\text{T}})$ from the diffusion prior as the training objective:
%\begin{equation}
%\begin{aligned}
%L_{\text{C}}=\sum_{i=1}^m{\langle{(\tau P(f_\text{T})+(1-\tau) f_\text{T}}) \cdot \frac{E_{\text{I}}(R_i)}{||E_{\text{I}}(R_i)||}\rangle},  %\phil{need . here} \phil{sorry, should be , instead due to the where after it}  
%\label{equ:div}
%\end{aligned}
%\end{equation}
%\noindent
%where $P$ is the diffusion prior for predicting $f_\text{I}$ from $f_\text{T}$ and $\tau$ is a hyperparameter that balances diversity and text-shape consistency; a larger $\tau$ leads to more diverse shapes, while a smaller $\tau$ encourages more consistency between the text and shape.
%Leveraging the randomness of the diffusion process, our model can converge to different 3D shapes with the same input text. In addition, due to the semantic gap between $f_\text{T}$ and $f_\text{I}$ as discussed in Section~\ref{sec:empirical2}, this strategy helps to further reduce such a gap by encouraging $f_\text{I}$ to be consistent with the sampled image feature $P(f_\text{T})$ instead of $f_\text{T}$}. 
%Adopting the diffusion prior to sample the CLIP image feature $f_\text{I}$ from $f_\text{T}$ \lzz{as an objective} can introduce some randomness into the generation process, allowing our model to create different 3D shapes for the same piece of input text.
Besides, by exploiting the prior from diffusion models, our model can also better mitigate the effect of the semantic gap between $f_\text{T}$ and $f_\text{I}$ in the stage-2 alignment; see the discussion in Section~\ref{sec:empirical2}.  This is achieved by encouraging $f_\text{I}$ of the rendered images to be consistent with the blended features of the sampled text-to-image feature $f_{\text{T}\rightarrow\text{I}}$ and the input text $f_\text{T}$, rather than just the input text feature $f_\text{T}$ itself.




\begin{comment}
Unlike the existing works, which require additional and complex modules,~\eg, GANs~\cite{chen2018text2shape}, IMLE~\cite{liu2022towards}, and normalizing flow network~\cite{sanghi2021clip}, we can simply perturb the image and text features for diversified generation.
Specifically, after stage-1 alignment, we randomly permute $f_{\text{I}}$ as an initialization and $f_{\text{T}}$ as the ground truth by adding normalized Gaussian noises $z_1=h_1/||h_1||, z_2=h_2/||h_2||$, where $h_1,h_2 \sim N(0,1)$ to derive diversified features
\begin{equation}
\hat{f}_{\text{I}}=\tau_1 f_{\text{I}}+(1-\tau_1)z_1 \ \ \text{and} \ \
\hat{f}_{\text{T}}=\tau_2 f_{\text{T}}+(1-\tau_2)z_2,
\label{equ:random}
\end{equation}
where $\tau_1,\tau_2$ are hyperparameters 
to control the degrees of permutation. With permuted $\hat{f}_{\text{I}}$ and $\hat{f}_{\text{T}}$ in stage-2 alignment, our model can converge to different 3D shapes for different noise. 
\end{comment}





\vspace*{-3pt}
\subsection{Text-Guided 3D Shape Stylization}\label{sec:stylization}


\begin{figure*}
\centering
\includegraphics[width=0.99\textwidth]{overview_new2.pdf}
\vspace*{-2mm}
\caption{
Our text-guided 3D shape refinement and stylization framework. 
(a) CLIP-guided stylization. (b) SDS-guided refinement and stylization. %They are optional in inference. 
}
\label{fig:overview_style}
\vspace*{-2.5mm}
\end{figure*}


%The two-stage feature-space alignment is already able to generate plausible 3D shapes; see,~\eg,
%Figures~\ref{fig:overview} (b) and~\ref{fig:bg} (b).
%However, the generative space \textcolor{blue}{and the generative quality} are limited by the representation capability of the employed
%SVR model,~\eg,~DVR~\cite{niemeyer2020differentiable} can only generate shapes with limited synthetic
%patterns as those in ShapeNet. 
%However, 
%a richer and wider range of 
%structures and textures are highly desired. To this end, we equip our model with a text-guided stylization and \textcolor{blue}{refinement} modules 
%to enhance the generated shapes with novel \textcolor{blue}{ and delicate} structure and texture appearances, as shown in Figures~\ref{fig:overview} (c, d) and~\ref{fig:figure1} ``ISS'' and ``ISS++''.

While the two-stage feature-space alignment can generate plausible 3D shapes as shown in Figures~\ref{fig:overview} (b) and~\ref{fig:bg} (b), its generative space and quality are still limited by the pre-trained SVR model in use. For instance, DVR~\cite{niemeyer2020differentiable} cannot generate shapes beyond the synthetic patterns in ShapeNet dataset.  %can only produce shapes with synthetic patterns as those in ShapeNet. 
Further, to enable the model to generate a broader range of structures and textures, we introduce text-guided stylization and refinement modules to enable our approach to create shapes out of the SVR generative space with delicate structures and textures; see Figures~\ref{fig:overview_style} and~\ref{fig:figure1} ``ISS++''.



\subsubsection{CLIP-guided stylization}\label{sec:clip_stylization}

First, we introduce CLIP-guided stylization to stylize 3D shapes beyond the generative space of the adopted SVR model. 

The top branch of Figure~\ref{fig:overview_style} (a) shows how we apply this method for texture stylization. To begin with, we duplicate $D$, except for the output layer, to create two networks: $D_o$ for occupancy prediction and $D_c$ for color prediction. Then we decompose the output layer to be $1$ and $3$ channels for occupancy and color prediction, respectively, and place them on the top of $D_o$ and $D_c$.  % We then place the output occupancy prediction layer and color prediction layer on top of $D_o$ and $D_c$, respectively.
%For texture stylization as shown in the top branch of Figure~\ref{fig:overview} (c), we first duplicate $D$ except for the output layer to be $D_o$ and $D_c$, then put the output occupancy prediction layer and color prediction layer on top of $D_o$ and $D_c$, respectively.
%Further, we fine-tune $D_c$ with the same CLIP consistency loss as in Equation~(\ref{equ:stage-2}), encouraging the consistency between input text $T$ and the $m$ rendered images $\{R_i\}_{i=1}^m$.

To further create new structures for shape stylization, we incorporate a shape-and-texture stylization strategy in addition to texture stylization, as depicted in the bottom branch of Figure~\ref{fig:overview_style} (a). To do so, we further optimize $D$ by adopting the CLIP consistency loss in Equation~\ref{equ:stage-2}. Besides, to preserve 3D prior learned in the two-stage feature-space alignment, we additionally propose a 3D prior loss $\mathcal{L}_P$ as shown in Equation~(\ref{equ:prior}). 
%To enable shape sculpting, we fine-tune $D$ using the same CLIP consistency loss as in Equation~\ref{equ:stage-2}. To preserve the overall structure of the initial shape $S$, we introduce a 3D prior loss, denoted as $L_P$ in Equation~(\ref{equ:prior}), which aims to maintain the 3D shape prior learned by the two-stage feature-space alignment. 
\begin{equation}
%\begin{aligned}
\mathcal{L}_\text{P}=\sum_p |D_o(p)-D_o^\prime(p)|
%\end{aligned}
\label{equ:prior}
\end{equation}
where $D_o(p)$, $D_o^\prime(p)$ indicate the initial occupancy prediction from $D$ and the optimized $D$ in the stylization training process of the query point $p$, respectively.

%Besides textures, novel structures are also desirable
%for shape stylization. Hence, we further incorporate a shape-and-texture stylization strategy to create novel structures as shown in the bottom branch of Figure~\ref{fig:overview} (c).
%To enable 
%shape sculpting, we fine-tune $D$ with the same CLIP consistency loss in Equation~\ref{equ:stage-2}. At the same time, to maintain the overall structure of the initial shape $S$, we propose a 3D prior loss $L_P$ shown in Equation~(\ref{equ:prior}), aiming at preserving the 3D shape prior learned by the two-stage feature-space alignment. 
%\begin{equation}
%L_\text{P}=\sum_p |D_o(p)-D_o^\prime(p)|
%\label{equ:prior}
%\end{equation}
%where $p$ is the query point, and $D_o$, $D_o^\prime$ are the occupancy predictions of the initial $D$ and the fine-tuned $D$ in the stylization process, respectively.

%To improve the consistency between the generated texture and the generated shape, we augment the background color of $R_i$ with a random $\text{RGB}$ value in each iteration. As shown in Figure~\ref{fig:bg_aug_supp} (a), the shape in white color may confuse with the white background; thus, the model would struggle to capture the boundary of objects, hence cannot generate textures that well-align with the table. In Figure~\ref{fig:bg_aug_supp} (c), the generated texture is severely affected by the black background color, causing low-quality stylization results. 


To enhance the network's object awareness in the stylization process, we introduce a background augmentation technique. % where the background color of $R_i$ is randomly assigned a different $\text{RGB}$ value in each iteration. 
As illustrated in Figure~\ref{fig:bg_aug_supp} (a), when the shape is in white, it can blend into the white background, making it difficult for the model to capture the object boundaries and resulting in textures that are poorly aligned with the table. Similarly, in Figure~\ref{fig:bg_aug_supp} (c), the generated texture is adversely harmed by the background color which is black, leading to inferior stylization results.
%To overcome this problem, %we introduce a
In our background augmentation strategy, %enhances the alignment between texture and shape. In this strategy, 
we propose to substitute the background color as a random RGB value for each training iteration. In this way, the object region is easily distinguishable during training, as depicted in Figure~\ref{fig:bg_aug_supp} (b, d), leading to an improvement in texture-shape consistency and stylization quality.

%To address the above issue, we propose a background augmentation strategy to improve the alignment between texture and shape. Specifically, the background color is replaced with random RGB values in each training iteration. By this means, the foreground shape may be more easily captured in training as shown in Figure~\ref{fig:bg_aug_supp} (b,d), improving the texture-shape consistency and the stylization quality. 
%\lzz{Is the above too detailed? }
%\phil{looks okay... better to have some details}


\begin{figure*}
\centering
\includegraphics[width=0.99\textwidth]{bg_aug_supp.pdf}
\vspace*{-1.5mm}
\caption{Text-guided 3D shape stylization with and without our background augmentation.
}
\label{fig:bg_aug_supp}
\vspace*{-2.5mm}
\end{figure*}



\subsubsection{SDS-guided refinement and stylization}\label{sec:sds_stylization}


The CLIP-guided stylization 
%is capable of generating   \phil{this sounds too strong?  I changed it to helps generate}
helps generate
3D shapes outside the scope of the SVR model's generative space. Yet, the quality of the generated shapes is still bounded by the adopted SVR model {with detailed structures missing}. To further enhance the quality of the generated shapes, we introduce a novel SDS-guided refinement and stylization technique %with Score Distillation Sampling (SDS) 
to decorate the 3D shapes with intricate details and textures, as illustrated in Figures~\ref{fig:style_figure1}, ~\ref{fig:figure1} ``ISS++'', and Figures~\ref{fig:overview_style} (b).

%The CLIP-guided stylization can already produce 3D shapes out of the SVR generative space. 
%However, the generative fidelity is still largely restricted by the pretrained SVR model. To further enhance the generative quality, we propose a new SDS-guided refinement and stylization procedure by adopting Score Distillation Sampling (SDS) to decorate the 3D shapes with fine details, as shown in Figures~\ref{fig:overview} (d) and~\ref{fig:figure1} ``ISS++''.}


%Inspired by~\cite{poole2022dreamfusion}, our SDS-guided refinement module leverages Score Distillation Sampling (SDS) to fine-tune $D$ after the two-stage feature space alignment. The high-level idea of SDS is to leverage a pretrained text-guided image generation diffusion model $\phi$ to optimize $D$ such that the rendered images $R$ are encouraged to be close to the output of $\phi$. Specifically, $\phi$ predicts the sampled noise $\epsilon_\phi(R^t, T, t)$ given the noise image $R^t$, text condition $T$, and noise level $t$. It predicts the gradient direction to encourage rendered images $R$ to move to a higher density region of the score function. The above gradient is formulated as:
The proposed SDS-guided refinement module is inspired by~\cite{poole2022dreamfusion} and aims to improve the generative quality of the pre-trained SVR model. 
Given a pre-trained text-guided image generation diffusion model $\phi$ and an input text $T$, we adopt Score Distillation Sampling (SDS) approach to fine-tune $D$ by encouraging the rendered image $R$ to be closer to the generated image of $\phi$ given input $T$. 
%
%The module utilizes Score Distillation Sampling (SDS) by fine-tuning $D$ with a pre-trained text-guided image generation diffusion model $\phi$ \xjqi{This module utilizes Score Distillation Sampling (SDS), a loss function??, to fine-tune $D$ }. 
%The main idea of is to optimize $D$, such that the rendered image $R$ is encouraged to be closer to the output of $\phi$. 
%Specifically, $\phi$ predicts the gradient direction to encourage rendered image $R$ to move towards a higher density region of the score function \xjqi{what does this mean, how it will encourage the result to become better}. 
As shown in Figure~\ref{fig:overview_style} (b), we use $\theta$ to denote parameters in the decoder $D(p; \theta)$, $p$ to represent the query points, and $R(D(p; \theta))$ to indicate the rendered image from a randomly chosen viewpoint. 
%We leverage the prior of a pre-trained text-guided diffusion model $\phi$ as shown in Figure~\ref{fig:overview_style} to guide the optimization of $D(p; \theta)$ using the rendered image $R$. 
Specifically, we randomly sample a time step $t$ and add noise to $R$ to produce $z_t$: $z_t=\sqrt{\bar{\alpha}_t} R+\sqrt{1-\bar{\alpha}_t} \epsilon$. 
The text $T$ and $z_t$ are fed into the pre-trained diffusion model which predicts the noise $\hat{\epsilon}_\phi(z_t; T, t)$. 
The predicted noise is compared with the added noise $\epsilon$ to construct the $\mathcal{L}_\text{sds}$.  
The procedure for calculating the gradient $\nabla_\theta \mathcal{L}_\text{sds}$ 
%$\frac{\partial {L_\text{sds}}}{\partial \theta}$ 
is illustrated below. 

\begin{subequations}
\begin{align}
&\nabla_\theta \mathcal{L}_\text{sds}(\phi, R(D(p;\theta)))=
\mathbb{E}_{t,\epsilon}[\frac{\partial (\hat{\epsilon}_\phi(z_t; T, t)-\epsilon)}{\partial \theta}] \label{equ:sdsa}\\
&=\mathbb{E}_{t,\epsilon}[(\hat{\epsilon}_\phi(z_t; T, t)-\epsilon)\frac{\partial \hat{\epsilon}_\phi(z_t; T, t)}{\partial z_t}\frac{\partial z_t}{\partial R}\frac{\partial R}{\partial \theta}] \label{equ:sdsb}\\
&\delequal\mathbb{E}_{t,\epsilon}[w(t)(\hat{\epsilon}_\phi(z_t; T, t)-\epsilon)\frac{\partial R}{\partial \theta}] \label{equ:sdsc}
\end{align}
\end{subequations}
where $w(t)={\partial z_t}/{\partial R}=\sqrt{\bar{\alpha}_t} I$ is a weighting function, and the term $\frac{\partial \hat{\epsilon}_\phi(z_t; T, t)}{\partial z_t}$ can be omitted indicated by~\cite{poole2022dreamfusion}. 
\begin{comment}
the differentiable rendering procedure to derive image $R$ using $D$. $z_t=\sqrt{\bar{\alpha}} R+\sqrt{1-\bar{\alpha}} \epsilon$, where $R$ is the rendered image and $\epsilon \sim \mathcal{N}(0,1)$ is the Gaussian noise added to the image. Besides, $T$ means the text condition and $\hat{\epsilon}_\phi(z_t; T, t)$ is the predicted noise by $\phi$. SDS can be formulated by the following Equation.
\end{comment}
\begin{comment}
\begin{subequations}
\begin{align}
&\nabla_{SDS}(\phi, R=D(\theta))=
\mathbb{E}_{t,\epsilon}[\frac{\partial (\hat{\epsilon}_\phi(z_t; T, t)-\epsilon)}{\partial \theta}] \label{equ:sdsa}\\
&=\mathbb{E}_{t,\epsilon}[(\hat{\epsilon}_\phi(z_t; T, t)-\epsilon)\frac{\partial \hat{\epsilon}_\phi(z_t; T, t)}{\partial z_t}\frac{\partial z_t}{\partial R}\frac{\partial R}{\partial \theta}] \label{equ:sdsb}\\
&\delequal\mathbb{E}_{t,\epsilon}[w(t)(\hat{\epsilon}_\phi(z_t; T, t)-\epsilon)\frac{\partial R}{\partial \theta}] \label{equ:sdsc}
\end{align}
\end{subequations}
\end{comment}
{The gradient $\nabla_\theta \mathcal{L}_\text{sds}$ will encourage the parameters $\theta$ to be updated so that the model can produce rendered images $R$ moving toward the high-density region of the score function. This means that the rendered image $R$ will be encouraged to be realistic and match the text, which in turn will help update the parameters $\theta$. 
%The main idea of SDS is to predict the gradient direction of Equation~\ref{equ:sdsa} to encourage the rendered image $R$ to move towards a higher density region of the score function. 
%To calculate the gradient, we expand it using the chain rule as shown in Equation~\ref{equ:sdsb}. According to~\cite{poole2022dreamfusion}, the U-Net Jacobian term $\frac{\partial \hat{\epsilon}_\phi(z_t; T, t)}{\partial z_t}$ can be omitted to derive Equation~\ref{equ:sdsc}, where $w(t)={\partial z_t}/{\partial R}=\sqrt{\bar{\alpha}} I$ is a weighting function, where $I$ means identity matrix. 
}



%DreamFusion: estimates an update direction that follows the score function of the diffusion model to move to a higher density region. 
%Magic3D: It provides the gradient direction to update  such that all rendered images are pushed to the high probability density regions conditioned on the text embedding under the diffusion prior

%Compared with DreamFusion~\cite{poole2022dreamfusion}, our ISS++ benefits from the 3D shape prior learned in two-stage feature space alignment and is able to generate 3D shapes with more faithful shape topology and more delicate surface details with only 30\% training iterations of~\cite{poole2022dreamfusion}. In addition, our 3D prior helps to resolve the ``multi-face Janus problem'' of DreamFusion where the generated shapes can have more than faces; we will provide more details in Section~\ref{sec:results}. 
The SDS-guided refinement further enhances the surface details of the generated shapes while preserving the overall topology learned by the two-stage feature-space alignment. With much fewer
%\textcolor{red}{much less} \lzz{30\% may be too strong. The training time varies from sample to sample} 
training iterations than DreamFusion, ISS++ is able to generate 3D shapes with comparable or even higher fidelity, as shown in Section~\ref{sec:results}. Besides, our ISS++ helps mitigate the ``multi-face Janus problem'' of DreamFusion~\cite{poole2022dreamfusion}, which refers to the situation that the generated shapes have multiple, often disconnected, faces. This can occur due to the lack of constraints on the topology of the generated shapes without 3D priors. 
%, as the method relies solely on image score matching to guide the shape generation. 
In contrast, our ISS++ leverages the 3D shape prior learned in the two-stage feature-space alignment to encourage consistency in the shape topology and achieve faithful and coherent shapes. 
Further, SDS enables ISS++ to generate a broader %\phil{wider?}
range of 3D shapes out of the image dataset; we will provide more results in Section~\ref{sec:results}.
%\phil{Is the word eliminate above too strong?}\lzz{mitigate}





Furthermore, this module also enables text-guided stylization to complement the CLIP-guided stylization presented in Section~\ref{sec:clip_stylization}. Specifically, given a 3D shape $S$ generated by our two-stage feature-space alignment and a text prompt $T$ that describes the target style, the SDS-guided  stylization procedure can incorporate the semantic attributes of $T$ into $S$, as illustrated in Figure~\ref{fig:overview_style} (b).
%Besides, this module also supports text-guided stylization, compensating with the CLIP-guided stylization introduced in Section~\ref{sec:clip_stylization}. Specifically, given a 3D shape $S$ generated by our two-stage feature space alignment and a text prompt $T$ to describe the target style, the SDS-guided procedure is able to inject the semantic attributes of $T$ to $S$, see Figure~\ref{fig:overview} (d).  










\subsubsection{Discussions on the different stylization approaches}\label{sec:discussion_on_stylization}
%\lzz{Should this section be here or in the Experiment section? It refers to some figures in Experiment section.}
%\phil{Putting it here is better in terms of having more NEW contents in sec 3}


We have presented three text-guided 3D shape stylization alternatives: texture stylization, shape-and-texture stylization, and SDS-guided stylization. Each method has its own pros and cons.
%advantages and disadvantages. 
%Texture stylization can preserve the original shape and guarantees its functionality.
First, texture stylization mainly changes the texture style of the generated shape and preserves its own structure and functionality. 
Besides, it can handle abstract text descriptions (``sunset'')
%that are difficult for the other two methods,
as shown in Figure~\ref{fig:style} (a). 
However, texture stylization 
%can 
may result in shape-texture misalignment if the given shape and texture have misaligned structures (see Figure ~\ref{fig:style}: ``peach chair''). 
%, as illustrated by the ``peach chair'' example in the same figure. 
Second, beyond the texture stylization, the shape-and-texture stylization %better 
{can also create novel and imaginary structures, giving rise to more plausible generative results. }
%encourages consistency with text simultaneously for texture and shape \xjqi{I cannot understand this sentence}.
%Also, it helps generate novel and imaginary structures beyond the training dataset.
Third, SDS-guided stylization is capable of producing stylized 3D shapes that better capture the semantic concepts of the given style with better fidelity. However, it may sacrifice the functionality of the generated shapes; see Figure~\ref{fig:sds_style} (``a chair imitating shell'')
%as demonstrated by the ``a chair imitating shell'' example in Figure~\ref{fig:sds_style}.

%\phil{check my changes carefully using the history function of overleaf}
%We have already introduced three options for text-guided 3D shape stylization, including
%texture stylization,
%shape-and-texture stylization, and SDS-guided stylization. Here, we discuss their merits and drawbacks.
%Texture stylization keeps the shape unchanged
%and is able to guarantee the functionality of the shape. In addition, it can take some abstract text
%descriptions as input like ``sunset'' in Figure~\ref{fig:style} (a), which is hard to handle for the other two stylizations methods. However, texture-stylization can cause shape-texture misalignment if the given shape and texture have different structures, see ``peach chair'' in Figure~\ref{fig:style} (a). 
%To be a compensation, shape-and-texture stylization better encourages the consistency of the texture and shape. More importantly, it is able to generate novel and imaginary structures beyond the training dataset. 
%At last, SDS-guided stylization is able to generate more stylized 3D shapes that better captures the semantic concepts of the given style with better fidelity. However, it sacrifices some functionality of the 3D shapes, see ``a chair imitating shell'' in Figure~\ref{fig:sds_style}.  }



%To summarize, there is a trade-off between maintaining the functionality and simulating the target style. Hence, we provide different options for potential users to choose from. 
In summary, there is a trade-off between preserving the functionality of 3D shapes and capturing the target style. To address this, we offer three options for users to choose from. Texture stylization is a good choice if the shape functionality is a top priority. Shape-and-texture stylization can encourage better consistency between texture and shape and is capable of generating novel structures. SDS-guided stylization can produce stylized 3D shapes with a higher fidelity according to the target style but at the expense of sacrificing their functionalities. {We hope our exploration will inspire more research efforts in the future for simultaneously achieving functionality preservation and style creation.} 
%Ultimately, the choice depends on the user's needs and preferences.


%\lzz{do we need the background aug section and discussions?}
%\subsubsection{Background Augmentation in Text-Guided Shape Stylization}\label{sec:back_aug_supp}

%One important thing in text-guided shape stylization is that the generated texture should align with the given shape. However, it cannot be ensured with a simple white or black background during training since the generated textures can be affected by the background color. As shown in Figure~\ref{fig:bg_aug_supp} (a), the shape in white color may confuse with the white background; thus, the model would struggle to capture the boundary of objects, hence cannot generate textures that well-align with the table. In Figure~\ref{fig:bg_aug_supp} (c), the generated texture is severely affected by the black background color, causing low-quality stylization results. 

%To address the above issue, we propose a background augmentation strategy to improve the alignment between texture and shape. Specifically, the background color is replaced with random RGB values in each training iteration. By this means, the foreground shape may be more easily captured in training as shown in Figure~\ref{fig:bg_aug_supp} (b,d), improving the texture-shape consistency and the stylization quality. 

%\subsubsection{Discussion on $L_\text{bg}$ and background augmentation.}



%\begin{figure*}
%\centering
%\includegraphics[width=0.99\textwidth]{bg_supp2.pdf}
%\vspace*{-1.5mm}
%\caption{An investigation on background loss and background augmentation. (a) Background color affects the cosine similarity of the CLIP features between the image and the text ``a red car'', \ie, (i) 0.292 (ii) 0.303 and (iii) 0.285. (b) Effect of generating shapes with background augmentation, but without background loss $L_{\text{bg}}$. Comparing with Figure 4 (b) in the main paper, the two-stage feature space alignment works well with $L_{\text{bg}}$, but fails with the background augmentation. 
%}
%\label{fig:bg_aug_supp2}
%\vspace*{-2.5mm}
%\end{figure*}
%In two-stage feature space alignment (Section 3.3 in the main paper), we introduce a background loss $L_\text{bg}$ to encourage the color prediction on the background region to be white. A natural question is whether we can use background augmentation as a replacement, and our answer is no. As shown in Figure~\ref{fig:bg_aug_supp2} (a), the background color can affect the cosine similarity of CLIP features between the image and input text; %\xjqi{what if you want to generate a shape with white color?? Say sth about the white color is xxx we find that has the least chance to be confused with xxx and at the same time facilitate stability??} 
%thus, using different background color in each iteration makes stage-2 alignment unstable and affects shape generation. As a result, the two-stage alignment can only benefit from $L_{bg}$, but not from the background augmentation, for producing a plausible shape. 
%Besides, we empirically found that the two-stage feature space alignment with $L_\text{bg}$ performs well, even if a white shape is being considered, see the bottom row in Figure~\ref{fig:quality_supp} (i).

%Please find more details in the supplementary material.

%\phil{the technical part seems a bit slim... only sec 3.3 \& 3.4... possible to say more? details on the mapper network? some more implementation details in each subsection?}

%However, the similar color \pdai{to background} can be generated in the stylization training procedure \xjqi{hard to understand?? is it generate background color??, better refer to some figure??}\pdai{if not similar to the background? will success?}, which makes it challenging to figure out the foreground shape region precisely. To address this issue, we introduce a background augmentation mechanism to improve the shape-color consistency. Specifically, the background color is  consistently changing to be a random color in each iteration to avoid the same foreground and background color. %Figure~\ref{fig:bg} (b) manifests the background augmentation improves the consistency between the generated texture and the shape. 

%\subsection{Discussions on the Background Color}
%In the stage-2 alignment, the background color is encouraged to be white, benefiting $E_{\text{C\_I}}$ to produce consistent prediction scores among iterations. Since this step is a search based training, it is almost unlikely that the whole shape can be optimized as white color like the background. The last two rows in Figure~\ref{fig:bg} (a) show that the model fails to generate a reasonable shape without $L_{\text{bg}}$, even if background augmentation is applied. 

%On the contrary, in stylization, since novel colors are created in this step, the shape color can be exactly same as the background; hence, we augment the background with random colors. Figure~\ref{fig:bg} (b) illustrates the stylization results with and without background augmentation, indicating its effectiveness on improving the texture and shape consistency.











%\vspace*{-6pt}
\subsection{Compatibility with Different SVR Models}
%\subsection{Broaden Shape Categories with Different SVR models}
%\subsection{Extend to a Wide Range of Categories}
\label{sec:compatible} 

In addition to DVR, our two-stage feature-space alignment can work with a variety of SVR models. For instance, it can be easily integrated with advanced methods, SS3D~\cite{alwala2022pre} and GET3D~\cite{gao2022get3d}, two recent generative models for 3D shape generation. 
{SS3D is capable of generating 3D shapes for a wide range of categories and GET3D can generate striking 3D shapes in superior quality. }
%SS3D uses single images from real-world scenes for 3D shape generation \xjqi{this is  not the reason for allowing ... SS3D is capable of generating 3D shapes for xxx categories}.
By replacing  $E_{\text{S}}$ and $D$ in Figure~\ref{fig:overview} with the encoder and decoder of SS3D or GET3D, our model can  be integrated with them 
 and produce shapes of  more categories or higher qualities. 
During training, we can adopt a similar pipeline depicted in Figure~\ref{fig:overview} to enable text-to-shape generation. 
%leveraging single-view in-the-wild images beyond the ShapeNet categories.
%allowing our framework to produce more categories by replacing of $E_{\text{S}}$ and $D$ with the encoder and decoder of SS3D.  
%in the place of $E_{\text{S}}$ and $D$ in our two-stage feature-space alignment. 
%Then, during training,  we can  adopt a similar pipeline depicted in Figure~\ref{fig:overview} to enable text-to-shape generation leveraging single-view in-the-wild images beyond the ShapeNet categories as in \xjqi{add citations}.  
%shape categories by using SS3D's encoder and decoder as shape encoder $E_{\text{S}}$ and decoder $D$ in our framework. We follow a similar pipeline as shown in Figure~\ref{fig:overview} to derive a text-guided shape generation model for the in-the-wild categories. 
For SS3D, in stage-1 training, we use their training objectives to replace $\mathcal{L}_\text{D}$ (see Section~\ref{sec:two-sgage}), which uses single-view in-the-wild  images beyond the ShapeNet categories without their poses.
%only requires single-view images without the associated camera poses. 
For GET3D, we first generate paired image-shape data by rendering images from its generated 3D shapes for training our two-stage feature-space alignment pipeline. 
%and then apply our two-stage feature-space alignment to GET3D's encoder and decoder for high-quality 3D shape generation.
In a nutshell, our approach is scalable and compatible with various SVR models and can potentially benefit from other new approaches in the future. 

%our approach  high compatibility indicates that it can work with various SVR models and can potentially benefit from other new SVR approaches in the future.

%\lzz{may change the name of this subsection to be "Compatibility with Different SVR models?". }
%\phil{good suggestion}

%Besides DVR~\cite{niemeyer2020differentiable}, our ISS framework is compatible with different SVR models.
%For example, we can adapt it with two recent generative models SS3D~\cite{alwala2022pre} and GET3D~\cite{gao2022get3d}. SS3D leverages in-the-wild single images for 3D generation, and  
%With this model, our framework can generate a wider 
%range of shape 
%categories 
%by using SS3D's encoder and decoder as shape encoder $E_{\text{S}}$ and decoder $D$ in our framework, respectively. Here, we simply follow the same pipeline as in Figure~\ref{fig:overview} to derive a text-guided shape generation model for the in-the-wild categories; see our results in Section~\ref{sec:exp_ss3d}.
%\lzz{as shown in Figure~\ref{fig:ss3d}}.
%Notably, we follow the losses in~\cite{alwala2022pre} in place of $L_\text{D}$ (see Section~\ref{sec:two-sgage}) in stage-1 training, requiring only single-view images without camera poses. 
%\textcolor{blue}{As for GET3D, we first derive the paired image-shape data by rendering images from its generated 3D shapes, and then adopt our two-stage feature space alignment to GET3D encoder and decoder for high-quality 3D shape generation. }
%More importantly, our approach's high compatibility suggests that it is orthogonal to SVR, so its performance can potentially be further upgraded with more advanced SVR approaches in the future.

%\phil{do you need to define $E_{\text{SS3D}}$ and $D_{\text{SS3D}}$?  If we do not need them later on, remove these two symbols} \lzz{Thanks. Have removed. }

%\phil{I assume that you will show some results produced with SS3D and some with DVR later, right?  Mention here?  This subsection only states that our framework is compatible with different SVR models but does not provide any visual evidence that it really can work with different SVR models}

%to illustrate how our approach works with general SVR models. 

%Specifically, we replace the 



\section{Experiments}
\label{sec:results}


%In this section, we first introduce the employed datasets, the implementation details, and the evaluation metrics in Sections~\ref{sec:implementation}.
%Then present ablation studies in Section~\ref{sec:ablation}.
%Further, we study the generative novelty and diversity, scalability, and generality of our ISS approach in Section~\ref{sec:show}.

%\vspace*{-7pt}
\subsection{Dataset}~\label{sec:implementation}


\vspace*{-12pt}

To train our ISS++ framework, we use both synthetic and real-world datasets, ShapeNet~\cite{shapenet2015} (13 categories) and CO3D~\cite{reizenstein2021common} (50 categories), respectively. 
%To evaluate the generative performance on ShapeNet, we build a text set containing four texts per category.
We further extend the generative capability beyond the above categories by adopting SS3D~\cite{alwala2022pre} and fine-tuning our model using SDS. 
For conducting quantitative and qualitative evaluations, we create a test set with four pieces of texts per category in the ShapeNet dataset. % and two pieces of texts per category in the CO3D dataset. }
%we build a text set containing four texts per category. 

%With multi-view RGB or RGBD images and camera poses, we can train ISS on the synthetic dataset ShapeNet~\cite{shapenet2015} (13 categories) and the real-world dataset CO3D~\cite{reizenstein2021common} (50 categories). To evaluate our generative performance, we create a text description set with four texts per category on ShapeNet and two texts per category on CO3D. 
%\lzz{May not need to mention SS3D categories like following that is commented?}
%SS3D~\cite{alwala2022pre} takes single-view in-the-wild images in training; as their data has not been released, we only evaluate our method on some of their categories. 
%To evaluate the performance, we employ Frchet Inception Distance (FID)~\cite{heusel2017gans}, Frchet Point Distance (FPD)~\cite{liu2022towards} to measure shape generation quality, and conduct a human perceptual evaluation to further assess text-shape consistency. 

\subsection{Implementation Details}
To train the two-stage feature-space alignment model, we first train the stage-1 mapping with the learning rate of $1e^{-4}$ for 400 epochs. Then at test time, we further train the stage-2 alignment for 20 iterations. On average, this process takes around 85 seconds using one GeForce RTX 3090 Ti GPU. Optionally, we further refine $S$ with SDS loss for about $40$ epochs or text-guided stylization for about $30-50$ epochs. 
%, which takes around 30 minutes.
 %We also have the option to further train text-guided stylization using the same learning rate. 
Our hyperparameters, including $\lambda_{M}$, t, $\lambda_{bg}$, $m$, and $\tau$, are set empirically to 0.5,  0.5, 10, 10, and 0.5, respectively, based on a small validation set. 
%We first train the stage-1 CLIP-image-to-shape mapping %\xjqi{modify stage-1} 
%for 400 epochs with learning rate $1e^{-4}$, and then train 
%\xjqi{modify stage-2} 
%the stage-2 text-to-shape module at test time for 20 iterations which takes only $85$ seconds on average on a single GeForce RTX 3090 Ti.
%Optionally, we can further train text-guided stylization with the same learning rate.
%We %\phil{empirically}
%empirically set hyperparameters $\lambda_{M}$, $\lambda_{bg}$, $t$, $m$, $\tau$ to be 0.5, 10, 0.5, 10, 0.5, respectively, according to a small validation set.


%\subsubsection{Details on camera poses.}
%\lzz{we may not need this. }
%In Stage 1, we follow~\cite{niemeyer2020differentiable} to set the camera poses to encourage the background to be white. Specifically, we randomly sample the distance of the camera and the viewpoint on the northern hemisphere. 

%In Stage 2, compared with~\cite{niemeyer2020differentiable}, we sample the camera distance to be 1.5 times further compared with~\cite{niemeyer2020differentiable}. It helps to encourage sampling more global views instead of only local ones, so that the CLIP image encoder can capture the whole shape and yield a better CLIP feature.

%In Stage 3, we also sample the camera distance to be 1.5 times. Since this stage aims to generate textures instead of searching for a target shape like Stages 1 and 2, only sampling view points on the northern hemisphere of the view space cannot ensure good generation quality in the bottom regions. Thus, we further randomly sample viewpoints on the southern hemisphere for random 10\% training iterations to encourage the stylized results to be consistent with the text in various viewpoints. 

\subsection{Metrics}
\subsubsection{Metric for shape generation quality}
%To measure the shape generation quality, we employ Frchet Inception Distance (FID)~\cite{heusel2017gans} between five rendered images of the generated shape with different camera poses and a set of ground-truth ShapeNet or CO3D images. We adopt the official model with Inception Net trained on ImageNet, which is widely used to evaluate generative quality and realism. We do not train a model on ShapeNet, since it is too small to train a better network for evaluating the FID than models trained on ImageNet.
%In addition, we randomly sample 2600 images in the ShapeNet dataset as ground truths for FID evaluation, instead of using images from ImageNet. It helps to measure the similarity between the generated shapes and ground truths of ShapeNet.

For quantitative evaluation, we compute the Frchet Inception Distance (FID)~\cite{heusel2017gans} between a set of five rendered images from different camera viewpoints for each shape and a set of ground-truth images from ShapeNet.  
We use the official model with InceptionNet pre-trained on ImageNet for FID evaluation, as it is a widely adopted metric for evaluating the realism and quality of generative models. We do not train an FID model on ShapeNet, as the size of the dataset is too small to train an effective FID model like that trained on ImageNet.
Additionally, we randomly sample 2,600 images from the ShapeNet dataset as ground truth images for FID evaluation, rather than using images from ImageNet, to more accurately evaluate the similarity of the generated shapes and the ShapeNet ground truth.


Besides adopting FID, we also utilize the metric Frchet Point Distance (FPD) proposed in~\cite{liu2022towards} to measure the shape generation quality without texture. To evaluate FPD, We first extract 3D point clouds from the generated shapes without color (see Figure~\ref{fig:point_cloud}) and then evaluate them. It is worth mentioning that Dream Fields~\cite{jain2021zero} does not generate 3D shapes directly, so we could not evaluate it using FPD in this aspect.
%We first convert the generated shapes to 3D point clouds without color (see Figure~\ref{fig:point_cloud}) and then evaluate FPD. Note that Dream Fields~\cite{jain2021zero} does not produce 3D shapes directly, so that we cannot evaluate this work in this regard. 


\begin{figure}
\centering
\includegraphics[width=0.99\columnwidth]{clip-forge.pdf}
\vspace*{-1.5mm}
\caption{Visualization of point clouds of different methods for FPD evaluation.
}
\label{fig:point_cloud}
\vspace*{-2.5mm}
\end{figure}


 
\vspace*{-3pt}
\subsubsection{Human perceptual evaluation setup}

Further, we conduct a human perceptual evaluation to assess the consistency between the generated shapes and the input text. 
To begin with, we collect the generated results. 
For each input text, we create 14 results in total from the four existing works, eight baseline methods, and our predecessor work ISS~\cite{liu2023iss} and our ISS++; see Section~\ref{sec:existing} and Section~\ref{sec:ablation} for details of each approach.
Then, we invite 10 volunteers with normal vision to participate in the evaluation, including 3 females and 7 males whose ages are in the range of 19 to 58. The generated results are shown to the participants in random order without any hint on how they are created. Then the volunteers are asked to give a score to indicate whether the candidate shape matches the input text, where 1 means a perfect match, 0.5 means a partial match, and 0 indicates a poor match.  % between the generated shapes and the input text.
At last, we sum up the total score $s$ for each approach from all participants and calculate $s/n$ as the metric ``Consistency Score'', where $n=10$ means the number of collected samples. 

%To further assess the text-shape consistency, we conduct a human perceptual evaluation which is detailed as follows. 

%First, we prepare generated results for human evaluation. For each input text, 
%we produce 14 results from the four existing works, eight baseline methods, and our ISS and ISS++; see Section~\ref{sec:existing} and Section~\ref{sec:ablation} for details of each baseline.
%Second,  we invite 10 volunteers (3 females and 7 males; aged from 19 to 58; all with normal vision) to evaluate the results. We show these results to the participants in random order without revealing how each result is produced. 
%Then, they are asked to give a score from $\{1, 0.5, 0\}$ (1: perfect match, 0.5: partial match; and 0: don't match) on the degree of match between the generated shapes and input text.
%Then, for each method, we gather the evaluation scores from all participants and obtain the ``Consistency Score'' as $s/n$, where $s$ is the total score and $n$ is the number of samples. 

\begin{comment}
\vspace*{-3pt}
\subsubsection{Implementation details}
Our framework is implemented using PyTorch~\cite{paszke2019pytorch}. We first train the stage-1 CLIP-image-to-shape mapping %\xjqi{modify stage-1} 
for 400 epochs with learning rate $1e^{-4}$, then train 
%\xjqi{modify stage-2} 
the stage-2 text-to-shape optimization at test time for 20 iterations, which take only 85 seconds on average on a single GeForce RTX 3090 Ti.
Optionally, we can further train text-guided stylization with the same learning rate.
We %\phil{empirically}
empirically set hyperparameters $\lambda_{M}$, $\lambda_{bg}$, $t$, $m$, $\tau_1$, $\tau_2$ to be 0.5, 10, 0.5, 10, 0.2, 0.95, respectively, according to a small validation set.
%\lzz{(a checklist question asks ``Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)''}.
\end{comment}


%\subsection{Metrics}~\label{sec:metric}

%\xjqi{Please lso show other evaluation metrics: we employ the following metrics to quantitatively evaluate our results. Please also show other metrics or cite existing papers or briefly introduce them.}

\begin{comment}

\vspace*{-3pt}
\subsubsection{Metric: Shape generation quality}
To measure shape generation quality, we employ Frchet Inception Distance (FID)~\cite{heusel2017gans} between five rendered images of the generated shape with different camera poses and a set of ground-truth ShapeNet images. To further assess text-shape consistency, we conduct a human perceptual evaluation. 
 
\vspace*{-3pt}
\subsubsection{Metric: Human perceptual evaluation Setup}
We recruited 10 volunteers (three females and seven males; aged 19 to 58; all with normal vision) in this evaluation. For each input text, 
%\phil{need at least a sentence to describe how each result was produced... why nine? mention nine competitor methods? mention that the next subsection will describe the details? need to say something}
we produced nine results using various methods and baselines, including CLIP-Forge~\cite{sanghi2021clip}, Dream Fields~\cite{jain2021zero}, six baseline methods, and our full method (see Figure~\ref{fig:quality}); see Sections~\ref{sec:existing} and ~\ref{sec:ablation} for the details.
%Then, nine results per input text (see,~\eg, Figure~\ref{fig:quality}) 
Then, we showed these results to the participants in random order, without revealing how each result was produced.
%\phil{without revealing to them how each result was produced}; 
After that, they were asked to compare how the results match the input text. Specifically, they were asked to give a score of one for perfect matches, a score of 0.5 for partial matches, and zero if not match at all. Then, for each method, we can compute metric ``Consistency Score'' as $s/n$, where $s$ the total score and $n$ is the number of samples.  
\end{comment}

%\vspace*{-10pt}
\subsection{Comparisons with Existing Works}\label{sec:existing}

In this section, we conduct qualitative and quantitative comparisons of four state-of-the-art works~\cite{sanghi2021clip,jain2021zero,mohammad2022clip,poole2022dreamfusion}, our predecessor work ISS~\cite{liu2023iss}, and our ISS++. For DreamFusion~\cite{poole2022dreamfusion}, since there are no official codes available, we use the latest version of a third-party implementation called Stable-DreamFusion~\cite{stable-dreamfusion}. For other works, we use their official codes on GitHub to generate shapes on our text set.
%Notably, we utilize two-stage feature-space alignment without any refinement or stylization to produce the results for ``ISS'', while fine-tuning our model with SDS for ``ISS++''.

\subsubsection{Quantitative comparisons} 
According to the quantitative comparisons presented in Table~\ref{tab:all}, our result ``ISS++'' outperforms all the existing works by a considerable margin in terms of all the evaluation metrics, as shown in Table~\ref{tab:all}. Specifically, the superior performance on FID and FPD demonstrates that our generative results have better quality in terms of the texture and 3D topology. In addition, the higher Consistency Score indicates that ISS++ can generate shapes with better consistency with the input text. The results of ``A/B/C Test'' and ``A/B Test'' will be discussed later in Section~\ref{sec:abtest}. % \xjqi{add more to describe the table,  write down some highlights}

\subsubsection{Qualitative comparisons} 



\begin{figure*}%[!t]
\centering
\includegraphics[width=0.99\textwidth]{sota.pdf}
\vspace*{-1.5mm}
\caption{Qualitative comparisons with existing works.
%\phil{try to better center align the elements in each column}
}
\label{fig:sota}
\vspace*{-2.5mm}
\end{figure*}



\begin{table*}%[!t]
\centering
\caption{Comparisons with existing works and our baselines. $^*$: We use Stable-Dreamfusion~\cite{stable-dreamfusion} for implementation%\textcolor{blue}{``FF'' means feed forward and no test-time optimization is required.} 
}
%\vspace*{-1mm}
\scalebox{0.95}{
  \begin{tabular}{cccccccc}
    \toprule
  \multirow{2}{*}{\rotatebox{0}{
Method Type
}} & \multirow{2}{*}{\rotatebox{0}{
Method
}} & \multirow{2}{*}{\rotatebox{0}{
FID ($\downarrow$)
}} & \multirow{2}{*}{\rotatebox{0}{
Consistency Score (\%) ($\uparrow$) 
}}& \multirow{2}{*}{\rotatebox{0}{
FPD ($\downarrow$) 
}} & A/B/C Test & A/B Test  \\ 
&&&&& (for two-stage alignment) & (for ISS++) \\
    \midrule
    \multirow{4}{*}{\rotatebox{0}{
Existing works
}}&  CLIP-Forge~\cite{sanghi2021clip}  & 162.87 %173.77
&  41.83 $\pm$ 17.62 & 37.43 & 8.90 $\pm$ 4.12 & N.A. \\
    & Dream Fields~\cite{jain2021zero} & 181.25 &  25.38 $\pm$ 12.33  & N.A. & N.A. & N.A. \\
    &CLIP-Mesh~\cite{mohammad2022clip} & 188.09 & 40.27 $\pm$ 8.82 & 40.27 & N.A. & N.A. \\
    &DreamFusion~\cite{poole2022dreamfusion}$^*$ & 159.04 & 38.36 $\pm$ 9.12 & 36.44 & N.A. & 7.00 $\pm$ 2.64  \\
    \midrule
    \multirow{6}{*}{\rotatebox{0}{
Ablation studies
}}  & $E_{\text{I}}$+$D$ & 181.88 & 20.97 $\pm$ 13.59 & 38.61 & N.A. & N.A.\\
    & w/o stage 1 & 222.96 & 1.92 $\pm$ 2.22  & 79.41 &  N.A. & N.A. \\
    & w/o stage 2 & 202.33 & 29.52 $\pm$ 14.86  & 41.71 &  N.A.& N.A.\\
    & w/o $\mathcal{L}_{\text{bg\_1}}$ & 149.45 & 29.45 $\pm$ 14.67 & 40.85 & N.A.& N.A.\\
    & w/o $\mathcal{L}_{\text{bg\_2}}$ & 156.52  & 31.55 $\pm$ 8.87 & 38.31 & N.A.& N.A. \\
    & w/o $\mathcal{L}_{\text{bg}}$ & 178.34 & 30.96 $\pm$ 15.49 & 40.98 & N.A. & N.A.\\
    \midrule
    \multirow{2}{*}{\rotatebox{0}{
Text2Image+SVR
}}  & GLIDE~\cite{nichol2021glide}+DVR~\cite{niemeyer2020differentiable} & 212.41 & 8.85 $\pm$ 7.94 & 41.33 & N.A. & N.A.\\
    & LAFITE~\cite{zhou2021lafite}+DVR~\cite{niemeyer2020differentiable} & 135.01 & 52.12 $\pm$ 11.05  & 37.55 & 11.70 $\pm$ 4.11 & N.A.\\
    \midrule
    Our earlier work & ISS~\cite{liu2023iss} & 124.42 $\pm$ 5.11 & 60.0 $\pm$ 10.94  & 35.67 $\pm$ 1.09 & \textbf{21.70 $\pm$ 5.19} & N.A. \\
    \midrule
    Ours & ISS++ & \textbf{114.34} & \textbf{70.77 $\pm$ 8.38} &  \textbf{30.92}   &  N.A.& \textbf{31.80 $\pm$ 7.53} \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:all}
\end{table*}    


\noindent\textbf{Comparison with state of the arts.} Then we compare the generative results of our ISS++ with four existing works and our predecessor work ISS~\cite{liu2023iss}. The qualitative comparisons  are shown in Figure~\ref{fig:sota}. We observe that CLIP-Forge~\cite{sanghi2021clip} can only produce low-resolution shapes without color and texture, and some of its generated shapes are not well aligned with the input text, for instance, ``a watercraft''. 
Dream Fields~\cite{jain2021zero} fails to generate desired shapes in most evaluated cases. 
Moreover, CLIP-Mesh~\cite{mohammad2022clip} is unable to generate fine-grained topology in some cases such as ``a black airplane with white wings''. 
Besides, Stable-Dreamfusion~\cite{stable-dreamfusion} has inferior performance in terms of surface quality (``a black airplane with white wings''), topology faithfulness (``a cupboard''), and generative efficiency.  
Despite that our predecessor work ISS~\cite{liu2023iss} can produce 3D shapes with better topology faithfulness and less time-consuming, the details of the results are still far from satisfactory, e.g., the rearview mirror on ``a red car''. In contrast, our ISS++ outperforms all the existing works by a large margin in terms of generative quality, consistency with the input text, and details on the generated shape, as shown in Figure~\ref{fig:sota}. 
%In contrast, our predecessor work ISS~\cite{liu2023iss} produces 3D shapes with better fidelity and consistency with the input text. 


%We would like to admit that in some cases, the generative quality of our ISS can be inferior to Stable-Dreamfusion~\cite{stable-dreamfusion} (``a watercraft''), especially on the details (the rearview mirror on ``a red car''). However, our ISS outperforms Stable-Dreamfusion in terms of surface quality (``a black airplane with white wings''), topology faithfulness (``a cupboard''), and efficiency.  


%\subsubsection{Qualitative comparison of ISS++ and existing works.} 


%In ISS++, we further improve the model's generation and stylization capability leveraging SDS. As shown in Figure~\ref{fig:sds} and Figure~\ref{fig:overview} (d), our model can generate 3D shapes with better fidelity compared with ISS.  %promote the ISS outputs (Figure~\ref{fig:sds} () and Figure~\ref{fig:overview} (b)) to be sub-categorical shapes by equipping them with various structures and fine details. 

%In some cases, the results of Stable-DreamFusion~\cite{stable-dreamfusion} contain richer details than ISS (``a watercraft''). 




%We have already demonstrated the superior performance of our ISS over existing works. In the following, we will further discuss 


\vspace{0.1in}\noindent\textbf{Comparison with DreamFusion.} To provide a further comparison with the most recent work DreamFusion~\cite{poole2022dreamfusion}, we show additional generative results from Stable-DreamFusion~\cite{stable-dreamfusion} and our ISS++ in Figure~\ref{fig:in-category}. Unlike Stable-DreamFusion, which optimizes the shape directly using SDS without a 3D prior, our ISS++ utilizes the 3D prior learned by our two-stage feature-space alignment, improving the generative performance in terms of avoiding failure modes (e.g., ``a race car in the color of yellow''), enhancing the surface quality (e.g., ``an ambulance''), and improving the 3D topology faithfulness (e.g., ``a swivel chair with wheels'').
%To further compare with DreamFusion~\cite{poole2022dreamfusion}, we present additional generative results of Stable-DreamFusion~\cite{stable-dreamfusion} and our ISS++ in Figure~\ref{fig:in-category}. Compared with DreamFusion that directly optimizes the shape using SDS without 3D prior, our ISS++ benefits from the 3D prior learned by two-stage feature space alignment to effectively avoid the failure modes (``a race car in the color of yellow''), improve the surface quality (``an ambulance''), and enhance the 3D topology faithfulness (``a swivel chair with wheels''). }
In addition, our ISS++ mitigates the ``multi-face Janus problem'' in Stable-DreamFusion, where the generated shapes, e.g., the monitors in Figure~\ref{fig:multiface}, can have multiple frontal views when viewed from different viewpoints. On the contrary, our ISS++ is able to generate faithful 3D shapes leveraging the 3D prior learned in our two-stage feature-space alignment, see Figure~\ref{fig:multiface} ``ISS++''.



\vspace{0.1in}\noindent\textbf{Generalization ability to novel categories.} Another notable advantage of our ISS++ is its ability to generate 3D shapes in novel categories beyond the training data. As depicted in Figure~\ref{fig:out-category}, starting from a randomly chosen shape ``a red car'' from our two-stage feature-space alignment, ISS++ is capable of deforming it into various 3D shapes (Figure~\ref{fig:out-category}) in a broad range of categories. 
It is worth noting that the quality of generated shapes can benefit from 3D priors of unrelated categories. 
For instance, a ``bird'' can be generated from using  a ``car'' as prior. 
%even a 3D prior of an unrelated category, such as ``a red car'', can still greatly improve the surface quality of the generated shapes for other categories such as a bird. 
This might be caused by the smoothness priors enforced by the initialization model, which is further used by the subsequent SDS process to produce high-quality surface. 
%This might be due to the initialized model's prior knowledge that the occupancy field should be smooth, which is further utilized by the subsequent SDS process to produce high-quality surfaces.
This demonstrates the generalization ability of our method in generating diverse and plausible novel 3D shapes, even for input texts beyond the training categories.
%More importantly, our ISS++ can generate 3D shapes in novel categories beyond the training data. As shown in Figure~\ref{fig:out-category}, given a randomly chosen shape ``a red car'' (the left-top one in Figure~\ref{fig:out-category}) from two-stage feature space alignment, our ISS++ is able to deform it into various 3D shapes (Figure~\ref{fig:out-category}) in a wide range of categories. Surprisingly, even a 3D prior of an unrelated category, like ``a red car'', can still largely improve the surface quality of generative shapes. It can be hypothesized that the initialized model can provide a prior that the occupancy field should be smooth, which is further leveraged by the subsequent SDS procedure to produce high-quality surfaces. 
%This demonstrates the generalization capability of our method in generating diverse and plausible 3D shapes, even for inputs that do not have an exact category match in the training data.



\begin{figure*}%[!t]
\centering
\includegraphics[width=0.99\textwidth]{in-category.pdf}
\vspace*{-2.5mm}
\caption{Results of Stable-Dreamfusion and our ISS++.  } % for each result.}
%shown We show rendered images from two different views. }
\label{fig:in-category}
\vspace*{-3.5mm}
\end{figure*}



\begin{figure*}%[!t]
\centering
\includegraphics[width=0.99\textwidth]{multiface.pdf}
\vspace*{-2.5mm}
\caption{Stable-DreamFusion suffers from the ``multi-face Janus problem'', and our ISS++ mitigates this issue by leveraging the 3D prior. } % for each result.}
%shown We show rendered images from two different views. }
\label{fig:multiface}
\vspace*{-3.5mm}
\end{figure*}



\begin{figure*}%[!t]
\centering
\includegraphics[width=0.99\textwidth]{out-category.pdf}
\vspace*{-2.5mm}
\caption{With a randomly selected shape as initialization (``a red car''), ISS++ can generate a wide range of 3D shapes beyond the training categories. }
\label{fig:out-category}
\vspace*{-3.5mm}
\end{figure*}









\subsection{Ablation Studies}\label{sec:ablation}

\subsubsection{Baseline setups}\label{sec:ablation:setup}

%We create the following baselines in our ablation study. 
%The first six baselines aim to assess the effectiveness of key modules in our approach, whereas the last two adopt state-of-the-art text-to-image generation approaches to first create images then adopt DVR~\cite{niemeyer2020differentiable} to generate shapes for a fair comparison with our approach. 
In addition, we develop several baselines to evaluate the effectiveness of different components in our model. 
%Note that we do not adopt the most recent SVR model SS3D~\cite{alwala2022pre} (which aims to work with in-the-wild images),
%due to its inferior generative quality and lack of texture generation.
%
%\phil{ok. Please clear up the whole paper}
%These are the baselines created in our ablation study:
\begin{itemize}[leftmargin=0.5cm]
\item $E_{\text{I}}+D$: This is the baseline where we get the CLIP image feature $f_{\text{I}}$ using $E_{\text{I}}$, and optimize $D$ to generate 3D shapes from $f_{\text{I}}$ without using the two-stage feature-space alignment.
\item w/o stage 1: This baseline involves ablating  stage-1 alignment and optimizing stage-2 alignment with a randomly initialized $M$.
\item w/o stage 2: In this baseline, we directly generate the shape with the mapper $M$ after stage 1, without performing the stage-2 optimization.
\item w/o $\mathcal{L}_{\text{bg\_1}}$: This baseline involves removing $L_\text{bg}$ in stage-1 alignment.
\item w/o $\mathcal{L}_\text{bg\_2}$: This baseline involves removing $L_{\text{bg}}$ in stage-2 alignment.
\item w/o $\mathcal{L}_\text{bg}$: This baseline involves removing $\mathcal{L}_{\text{bg}}$ in both stages.
\item GLIDE+DVR: This baseline involves using a recent zero-shot text-to-image generation method GLIDE~\cite{nichol2021glide} to first generate image $I$ from $T$, and then using DVR~\cite{niemeyer2020differentiable} to generate $S$ from $I$.
\item LAFITE+DVR: In this baseline, we train a recent text-guided image generation approach LAFITE~\cite{zhou2021lafite} on ShapeNet dataset, produce an image $I$ from $T$, and then generate $S$ from $I$ using DVR~\cite{niemeyer2020differentiable}.
\end{itemize}

The first six baselines are designed to evaluate the effectiveness of  modules in our framework and the last two baselines utilize advanced text-guided 2D image generation methods to first generate images and then use an SVR model to generate shapes. 
Note that we still adopt DVR as the SVR model for fair comparisons. 

\subsubsection{Quantitative and qualitative comparisons }



%\vspace*{-3pt}
%\subsubsection{Quantitative and qualitative comparisons} %\xjqi{Qualitative results are shown in Figure xxx. add a summary about your major conclusion about different approaches. For each part, you should echo the claims in the introduction to demonstrate the effectiveness of the methods.} 

%In this section, we analyze the result of the baselines shown in Figure~\ref{fig:quality}. 

The qualitative results of baseline methods are shown in Figure~\ref{fig:quality}. We summarize our key observations as below:


%Here, we only discuss (e)-(h) in Figure~\ref{fig:quality} and more details are included in the supplementary material.


%Here, we analyze the results in Figure~\ref{fig:quality_supp} (c) and (d), to compensate with the analysis shown in Table 1 and Figure 5 (e) to (i) in the main paper Section 4.3. 

\begin{itemize}[leftmargin=0.5cm]
\item $E_{\text{I}}+D$: As seen in column (a) of Figure~\ref{fig:quality}, the generated results from CLIP space $\Omega_{\text{I}}$ have inferior texture and shape structure fidelity due to the inferior ability of $E_\text{I}$ in capturing image details.

\item w/o stage 1: Figure~\ref{fig:quality} (b) shows that the produced shapes are almost the same for any given text without adopting stage-1 alignment. This happens because $M$ maps text feature $f_\text{T}$ to nearly the same feature even with stage-2 alignment enabled. This demonstrates the necessity of stage-1 alignment to provide good initialization for stage-2 test-time optimization.


\item w/o stage 2: Figure~\ref{fig:quality} (c) indicates that the model may fail to align $f_S$ and $f_{\text{T}}$ well without stage 2. This can be further illustrated in Figure~\ref{fig:ablation_discuss} (a). Without using stage 2,  the model fails to generate a reasonable shape with text as input but successes in generating 3D shapes from a single image. 
%a reasonable shape can be generated from a single image without using stage 2 (see ``SVR result'') but fails to take the text as input (see ``stage 1 output''). 
After applying stage 2, a plausible phone can be produced using the text (see ``stage 2 output'').
% Column (c) of Figure~\ref{fig:quality} shows that without stage 2, the model may fail to align well with $f_S$ due to the semantic gap between $f_{\text{I}}$ and $f_{\text{T}}$. Figure~\ref{fig:ablation_discuss} (a) is used to illustrate the associated results: the model without stage 2 can generate reasonable shapes from a single image (see ``SVR result''), but fails with text as input (see ``stage 1 output''). Further, with the stage-2 optimization, a plausible phone can be generated (see ``stage 2 (ours ISS)''). %as shown in column (c) of Figure~\ref{fig:quality}, without Stage 2, may not align well with $f_S$ due to the semantic gap between $f_{\text{I}}$ and $f_{\text{T}}$.
%
%Now, we use Figure~\ref{fig:ablation_discuss} (a) to illustrate their associated results: the model in ``w/o stage 2'' can generate reasonable shapes from a single image (see ``SVR result'' in Figure~\ref{fig:ablation_discuss} (a))
%but fails with text as input (see ``stage 1 output'' in Figure~\ref{fig:ablation_discuss} (a)); further with the stage-2 optimization, a plausible phone can be generated (see ``stage 2 (ours ISS)'' in Figure~\ref{fig:ablation_discuss} (a)).

\item w/o $\mathcal{L}_{\text{bg\_1}}$, w/o $\mathcal{L}_{\text{bg\_2}}$, w/o $\mathcal{L}_{\text{bg}}$: Columns (d, e, f) of Figure~\ref{fig:quality} show that stage-2 alignment cannot work properly without $\mathcal{L}_{\text{bg}}$ in either stage-1 or stage-2 alignment or both due to the lack of foreground awareness. Even though stage-1 alignment has already encouraged the background to be white, we still need this loss in stage 2 to obtain satisfying results.


\item GLIDE+DVR: The performance of GLIDE+DVR (see Figure~\ref{fig:ablation_discuss} (b)) is poor because of the large domain gap between the training data of DVR and the images generated by GLIDE~\cite{nichol2021glide}.
% The images created by GLIDE~\cite{nichol2021glide} have a large domain gap from the training data of DVR, severely limiting the performance of GLIDE+DVR (see Figure~\ref{fig:ablation_discuss} (b)).%the images created by GLIDE~\cite{nichol2021glide} have a large domain gap from the training data of DVR, severely limiting the performance of GLIDE+DVR (see Figure~\ref{fig:ablation_discuss} (b)).
%

\item LAFITE+DVR: In Figure~\ref{fig:quality} (h), some shapes produced by this baseline  do not match the given texts because of the semantic gap between $f_{\text{I}}$ and $f_{\text{T}}$ (e.g., ``a wooden boat''). Also, the appearance can be coarse (Figure~\ref{fig:ablation_discuss} (b)) because of the error accumulation of the isolated two steps, i.e., LAFITE (Figure~\ref{fig:ablation_discuss} (b) ``image from LAFITE'') and DVR (Figure~\ref{fig:ablation_discuss} (b) ``shape from LAFITE image''). Despite these shortcomings, generating images and shapes in a subsequent manner remains a strong baseline that is a valuable direction for future research. 
%as shown in Figure~\ref{fig:quality} (h), some generative results of LAFITE+DVR do not match the input text due to the semantic gap between $f_{\text{I}}$ and $f_{\text{T}}$ (``a wooden boat''). 
%Also, some results can be coarse (Figure~\ref{fig:ablation_discuss} (b)) due to the error accumulation of the isolated two steps,~\ie, LAFITE (Figure~\ref{fig:ablation_discuss} (b) ``image from LAFITE'') and DVR (Figure~\ref{fig:ablation_discuss} (b) ``shape from LAFITE image''). 
%Despite the above, subsequently generating images then shapes is still a strong baseline that is a valuable research direction in the future.

\item Two-stage alignment: 
Column (i) of Figure~\ref{fig:quality} shows that our two-stage feature space alignment can generate plausible shapes and textures consistent with text descriptions, beyond all the above baselines. However, the generative details are still not very satisfying. 
\item Ours (ISS++): Column (j) of Figure~\ref{fig:quality} demonstrates the superior capability of ISS++ to generate shapes and textures with a remarkable level of detail, outperforming all the baselines by a substantial margin.
\end{itemize}


%\begin{figure*}
%\centering
%\includegraphics[width=0.99\textwidth]{quality_supp.pdf}
%\vspace*{-1.5mm}
%\caption{Additional qualitative results compared with existing works and baselines. 
%}
%\label{fig:quality_supp}
%\vspace*{-2.5mm}
%\end{figure*}



















\begin{figure*}%[!t]
\centering
\includegraphics[width=0.99\textwidth]{ablation.pdf}
\vspace*{-1.5mm}
\caption{Ablation studies. 
}
\label{fig:quality}
\vspace*{-2.5mm}
\end{figure*}

\begin{figure*}%[!t]
\centering
\includegraphics[width=0.99\textwidth]{ablation_discuss.pdf}
\vspace*{-1.5mm}
\caption{A further study of baselines ``w/o stage 2'', ``GLIDE+DVR'', and ``LAFITE+DVR''.
%\phil{which ones... list here}.
%Can you put the sentence inside caption (a) here? Then, i can edit directly; (b) as well. Thx
%
%(a) w/o stage 2 produces a plausible shape ("SVR") from image but a low-quality shape ("stage 1") from text; 
%further fine-tuning in Stage 2 leads to a more plausible shape from text ("stage 2").
%OR
(a) ``w/o stage 2'' generates a reasonable shape (``SVR result'') from the input image, but produces a low-quality shape (``stage 1 output'') when using the text as input; further fine-tuning our model with stage-2 alignment allows us to generate a more plausible shape using the text (``stage 2 output'').
%
(b) GLIDE and LAFITE tend to generate out-of-domain and low-quality images, respectively, leading to inferior performance in the subsequent image-based 3D generation.
%\lzz{out of domain is not good}
%\phil{:)
%
%SVR & stage 1 : results of w/o stage 2 with image and text input; stage 2 (ours): after stage-2 alignment.
%
%(a) w/o stage 2 produces a plausible shape from image, but a low-quality shape from text. SVR & stage 1 : results %of w/o stage 2 with image and text input; stage 2 (ours): after stage-2 alignment.
%
%(b)  GLIDE / LAFITE can generate out-of-domain and   inferior-quality images, limiting the performance of the subsequent %3D generation. 
%
%}\lzz{(b) and-> or?}
%shown We show rendered images from two different views. }
}
\label{fig:ablation_discuss}
\vspace*{-2.5mm}
\end{figure*}





\vspace*{-3pt}
\subsubsection{A/B/C test and A/B test}~\label{sec:abtest} %\lzz{Do we still need this section? I think it is no longer needed, since DreamFusion and ISS++ are not included.}
%\phil{Since this is an extended version, it is better to keep but doing so may make the reviewers ask... why not include DF and ISS++.  In fact, it will be the best if we can further see the performance of ISS+++ and Stable-DF?  I understand that this will take more time...  So, I am ok for including or not include it but not including it may lead to another question from the reviewer if the reviewer carefully compares this version with the previous one and then ask why we remove this part.  You may decide.  Keeping it or removing it}
We conduct an A/B/C test and an A/B test with 10 volunteers. For fair comparisons, the A/B/C test is designed to evaluate the approaches 
without SDS refinement, {\ie}, our two-stage feature-space alignment and two baselines that have the highest performance: CLIP-Forge~\cite{sanghi2021clip} and ``LAFITE+DVR''. In addition, A/B test aims to compare the approaches trained with SDS, including our ISS++ with DreamFusion~\cite{poole2022dreamfusion}.  
In this test, the results of the three approaches (per input text, a total of 52 texts) were displayed in a random order, and the participants were asked to choose their favorite one. 

The results of the A/B/C test, shown in Table~\ref{tab:all} ``A/B/C Test'', demonstrate that our two-stage feature-space alignment is the most preferred approach, outperforming CLIP-Forge by 143.8\% (computed as $(21.70-8.90)/8.90$) and ``LAFITE+DVR'' by 85.5\% (computed as $(21.70-11.70)/11.70$). In addition, the result of ``A/B test'' in Table~\ref{tab:all} shows that our ISS++ outperforms Stable-Dreamfusion by 354.3\% (computed as $(31.80-7.00)/7.00$) in terms of user preference. 

%To further compare our ISS with the strongest baselines CLIP-Forge~\cite{sanghi2021clip} and ``LAFITE+DVR'', we perform an A/B/C test with 10 volunteers 
%to compare these two baselines with ours. \textcolor{blue}{Note that the more recent work DreamFusion aims to handle arbitrary categories while CLIP-Forge and ``LAFITE+DVR'' focus on ShapeNet categories; hence, DreamFusion has inferior quantitative performance to these two approaches on ShapeNet and is not included in our A/B/C test.} Specifically, the results from the three approaches (per input text) in random order for all the $52$ texts. Then, they were instructed to choose a most preferred one. The results in Table 1 ``A/B/C Test'' in the main paper show that our results are more preferred than others, outperforming~\cite{sanghi2021clip} by 143.8\% ($(21.70-8.90)/8.90$) and ``LAFITE+DVR'' by 85.5\% ($(21.70-11.70)/11.70$). 





%div: 3.38,35.17



%The results are: \textbf{FID: 113.42}, \textbf{FPD: 33.66}, \textbf{PS: 3.58}, which is even better than our one-text-one-shape generative results (FID: 124.42 $\pm$ 5.11, FPD: 35.67 $\pm$ 1.0, PS: 3.18 $\pm$ 0.11), manifesting the superior performance of diversified generation capability of ISS.  

 






\vspace*{-5pt}

\subsection{More Analysis of Generative Results} \label{sec:show}

Moreover, we evaluate the novelty and diversity of generated shapes, as well as the scalability of the proposed two-stage feature-space alignment. Additionally, we will showcase further text-guided stylization results of ISS++ and demonstrate how our method can generalize to a broad range of categories and produce high-fidelity shapes. %we present evaluations on the generative novelty and diversity, as well as the scalability of our two-stage feature-space alignment. Then, we show more text-guided stylization results and how our approach generalizes to a wide range of categories and generates shapes with better fidelity.

%CLIP-Mesh: FID 188.09200326730937, consistency score, 32.789 +- 8.82
%CLIP-Mesh:val IS:  (2.5366502, 0.22608429)
%CLIP-Mesh:pred val fid:  (37.612865331328614-2.512834739152946e-07j)  40.26917348814686-3.73

%Div FID 108.73
%Div IS: 3.80
%Div fid: 31.92


%dreamfusion val IS:  (3.06846, 0.31211784)
%pred val fid:  (41.05800099616776-5.053659619267268e-07j)
%IS: 170.19489

%ISS+DF val IS:  (2.8520217, 0.8640585)
%pred val fid:  (30.916928512115817-2.3360543887271605e-07j)
%IS: 114.3442



  
















\begin{figure*}%[!t]
\centering
\includegraphics[width=0.99\textwidth]{novelty.pdf}
\vspace*{-3.5mm}
\caption{Our two-stage feature-space alignment can create novel shapes that are not in the training set.
(a) displays our results and (b,c,d) are the top three closest shapes retrieved from the training set.
%\phil{TODO: center align the text labels with the image above each of them}
}
\label{fig:novelty}
\vspace*{-3.5mm}
\end{figure*}


\begin{figure*}%[!t]
\centering
\includegraphics[width=1.0\textwidth]{div.pdf}
\vspace*{-3.5mm}
\caption{Our method produces more diversified results with better text-shape consistency given the same input text. }
%given one text. }
\label{fig:diversified}
\vspace*{-3.5mm}
\end{figure*}


\begin{figure*}%[!t]
\centering
\includegraphics[width=0.99\textwidth]{co3d.pdf}
\vspace*{-3.5mm}
\caption{Results of our two-stage feature-space alignment on CO3D dataset.} %We show two different views of each result.}
%we show rendered images from two different views. %\phil{this figure is never referenced}
%}
\label{fig:co3d}
\vspace*{-3.5mm}
\end{figure*}

\vspace*{-2.5mm}
\begin{figure*}%[!t]
\centering
\includegraphics[width=0.99\textwidth]{stylization.pdf}
\vspace*{-2.5mm}
\caption{CLIP-Guided stylization. (a) Texture stylization. (b) Shape-and-texture stylization.}
\label{fig:style}
\vspace*{-2.5mm}
\end{figure*}





\begin{figure*}%[!t]
\centering
\includegraphics[width=0.99\textwidth]{sds_style.pdf}
\vspace*{-2.5mm}
\caption{SDS-guided stylization. Two different views are rendered. The text prompt is ``A [shape] simulating a [style].'' } % for each result.}
%shown We show rendered images from two different views. }
\label{fig:sds_style}
\vspace*{-3.5mm}
\end{figure*}






\begin{figure*}
\centering
\includegraphics[width=0.99\textwidth]{results.pdf}
\vspace*{-1.5mm}
\caption{Generative results of our approach. With ISS++, we can effectively generate 3D shapes of various categories from texts. Left: ISS~\cite{liu2023iss}. Right: ISS++.
}
\label{fig:all}
\vspace*{-2.5mm}
\end{figure*}

\vspace*{10pt}
\noindent\textbf{Generation novelty of two-stage feature space alignment.} Our two-stage feature-space alignment has the ability to produce shapes that are novel and not present in the training data. 
Figure~\ref{fig:novelty} shows that given an input text, our model first generates the 3D shape in (a), and then uses it to retrieve the top three closest shapes (b,c,d) in the entire training set based on the cosine similarity between CLIP features $f_\text{I}$ of rendered images. 
The result shows that our generated shapes after two-stage feature space alignment are different from the retrieved shapes, indicating that our two-stage feature space alignment method is able to generate novel shapes even without any stylization process.
It is unsurprising since our two-stage feature space alignment shares the generative space with the
adopted SVR model and has the potential to create all shapes that the adopted SVR model can generate.
% This is not surprising, as ISS shares the generative space with the pre-trained SVR model and has the potential to generate all shapes that the pre-trained SVR model can produce.

\vspace*{10pt}\noindent\textbf{Generation diversity.} In Figure~\ref{fig:diversified} and Table~\ref{tab:div}, we compare the diversified generation results of ISS++ and our previous work ISS~\cite{liu2023iss} both qualitatively and quantitatively. Remember that ISS~\cite{liu2023iss} is also able to generate diversified shapes by randomly perturbating $f_\text{I}$ as initialization and $f_\text{T}$ as the ground truth to derive diversified features. The model can then converge to different shapes for different noise perturbations. 
To evaluate the generative diversity, we generate another two shapes per input text for both ISS~\cite{liu2023iss} and ISS++, then use FID~\cite{heusel2017gans} and FPD~\cite{liu2022towards} for the fidelity and diversity evaluation. The results in Table~\ref{tab:div} and Figure~\ref{fig:diversified} demonstrate that our ISS++ can generate more diversified shapes with better text-shape consistency than ISS~\cite{liu2023iss}.
%Further, we compare the diversified generation results of ISS++ and our previous work ISS~\cite{liu2023iss} as shown in Figure~\ref{fig:diversified} and Table~\ref{tab:div}. ISS enables diversified generation by randomly permuting $f_\text{I}$ as an initialization and $f_\text{T}$ as the ground truth to derive diversified features, then the model can converge to different 3D shapes for different noise.
%For a fair comparison, we generate additional two samples per input text, and then adopt FID~\cite{heusel2017gans}, FPD~\cite{liu2022towards} (the lower, the better) for the fidelity evaluation and Point Score (PS)~\cite{liu2022towards} (the higher, the better) for the diversity evaluation. The results in Table~\ref{tab:div} manifest that our ISS++ can generate more diversified shapes with better text-shape consistency compared with ISS. 


\begin{table}%[!t]
\centering
\caption{Quantitative evaluation on diversified generation.}
\scalebox{1.2}{
  \begin{tabular}{ccc}
    \toprule
   Method & FID ($\downarrow$) & FPD ($\downarrow$)   \\
   \midrule
    ISS~\cite{liu2023iss}  & 113.98  & 35.37  \\ %& 3.11
    ISS++ (w/o SDS) & \textbf{108.73}  &  \textbf{34.36}\\ %& \textbf{3.49}
    \bottomrule
  \end{tabular}
  }
  \label{tab:div}
\end{table} 

%\vspace*{-6pt}

\vspace*{10pt}\noindent\textbf{Generation fidelity of two-stage feature space alignment.} To evaluate the ability of our two-stage feature space alignment to generate realistic 3D shapes, we train DVR~\cite{niemeyer2020differentiable} on the real-world CO3D dataset, and adopt the learned feature space for text-guided shape generation without using paired data. As depicted in Figure~\ref{fig:co3d}, our model can produce real-world shapes with a high degree of fidelity. To the best of our knowledge, this is the first work to investigate text-guided shape generation on real-world datasets and generate realistic 3D shapes.

%To assess the capability of ISS in generating realistic real-world 3D shapes, we train the SVR model on the CO3D dataset~\cite{reizenstein2021common} which is a real-world dataset, and ISS leverages the learned feature space for text to shape generation without paired data. As shown in Figure~\ref{fig:co3d}, our model is able to generate real-world shapes. As far as know, this is first work that investigates text-guided shape generation and on real-world datasets can generate realistic 3D shapes.




\begin{figure*}%[!t]
\centering
\includegraphics[width=0.99\textwidth]{ss3d.pdf}
\vspace*{-2.5mm}
\caption{After training on single images (without camera poses), our approach can generate shapes for a broad range of categories, by adopting~\cite{alwala2022pre}.} % for each result.}
%shown We show rendered images from two different views. }
\label{fig:ss3d}
\vspace*{-3.5mm}
\end{figure*}

\vspace*{-3.5mm}




\begin{figure*}
\centering
\includegraphics[width=0.99\textwidth]{get3d.pdf}
\vspace*{-3.5mm}
\caption{Results of two-stage feature space alignment built upon GET3D.}
\label{fig:get3d}
\vspace*{-3.5mm}
\end{figure*}


\begin{table*}
\centering
\caption{Mean and standard deviation of distances in the feature space mapping process evaluated on our test set. $d$ means cosine distance. Almost all distances are consistently reduced after our stage-2 alignment.  }
\label{tab:mapping}
\scalebox{1.2}{
  \begin{tabular}{cccccc}
    \toprule
     & $d(M(f_\text{I}),M(f_\text{T}))$&$d(M(f_\text{I}), f_\text{S}))$&$d(M(f_\text{T}), f_\text{S}))$&$d(M(f_\text{T}), f_\text{S}))$&$d(M(f_\text{T}), M(f_\text{T}))$  \\
    \midrule
mean $\pm$ std&0.58 $\pm$ 0.23&
0.21 $\pm$  0.10&
 0.45  $\pm$  0.20 &
 0.17 $\pm$ 0.08&
 0.32 $\pm$ 0.17\\
    \bottomrule
  \end{tabular}
  }
\end{table*}    

\begin{figure*}%[!t]
\centering
\includegraphics[width=0.99\textwidth]{limitation.pdf}
\vspace*{-2.5mm}
\caption{From a randomly chosen shape (a), some of our out-of-category results (ISS++) are inferior to DreamFusion (Stable-Dreamfusion). }
\label{fig:limitation}
\vspace*{-3.5mm}
\end{figure*}


\vspace*{10pt}
\noindent\textbf{Generation beyond the capability of the SVR model.} The text-guided stylization module enables our model to create 3D shapes beyond the pre-trained SVR model. As shown in Figure~\ref{fig:style_figure1}, Figure~\ref{fig:overview_style}, Figure~\ref{fig:style}, 
and Figure~\ref{fig:sds_style}, novel structures and textures matching text descriptions can be created. 
As shown in Figure~\ref{fig:style} (a), the CLIP-guided texture stylization can hallucinate both realistic (``mahogany chair'') and fantasy (``glacier chair'') vivid textures on the chair. 
Further, in Figure~\ref{fig:style} (b), our shape-and-texture stylization successfully creates novel textures and imaginary shapes not present in the training dataset. 
In addition, as shown in Figure~\ref{fig:style_figure1} and Figure~\ref{fig:sds_style}, our ISS++ is capable of generating aesthetically pleasing stylized shapes with intricate details and textures, such as the ``rabbit lamp'' and ``banana chair''. These results showcase the ability of our model to generate visually appealing and complex shapes from text descriptions.
%In addition, our ISS++ is able to generate delicate stylized 3D shapes from texts with SDS guidance (see Figure~\ref{fig:sds_style}). %Note that our stylized results capture the high-level semantic concepts of the input style prompts such as ``banana'', while faithfully preserve the functionality of the object, like ``lamp'' and ``chair'', at the same time; see Figure~\ref{fig:style} (b) and Figure~\ref{fig:sds_style}. 


%Note that our results achieve a good balance on the stylization and the functionality. For example, our result of ``avocado chair'' possesses both the style of ``avocado'' and the functionality of ``chair'' in Figure~\ref{fig:style} (b) and Figure~\ref{fig:sds_style}. 



%\vspace*{10pt}
\noindent\textbf{Generality and scalability of two-stage feature space alignment on other SVR models.} The generality and scalability of two-stage feature space alignment are evaluated by replacing DVR~\cite{niemeyer2020differentiable} with other SVR models, such as SS3D~\cite{alwala2022pre} and GET3D~\cite{gao2022get3d}. 
% as SVR models to provide the feature space. 
It is worth mentioning that SS3D is good at producing 3D shapes in more categories and GET3D is able to generate 3D shapes with higher fidelity.
First, Figure~\ref{fig:ss3d} shows that our approach, built upon SS3D, can generate shapes of more real-world categories, such as birds. 
% Notably, our model can generate shapes with better qualities compared to the initial SS3D model, which takes an image as input.
Notably, the shape generated by our model is of better quality than the initial result of the SS3D with an image as input for 3D shape generation from texts.
Second, our model is able to fully leverage the generative capabilities of GET3D to produce high-fidelity 3D shapes, as displayed in Figure~\ref{fig:get3d}.
These results demonstrate that our approach is general and compatible with various advanced SVR models for producing shapes of more categories and higher qualities even without SDS-guided refinement.

%Our model is generic and can work together with other SVR models. To evaluate the generality and scalablity of our model, we employ SS3D~\cite{alwala2022pre} and GET3D~\cite{gao2022get3d} as SVR models to provide the feature space. It is worth noting that SS3D is capable of generating shapes of more categories and GET3D can produce high fidelity results. First, as shown in Figure~\ref{fig:ss3d}, built upon SS3D,  our approach can generate shapes of more real-world categories,~\eg, bird. 
%Note that our model can generate shapes with comparable or even better qualities compared with initial SS3D model that takes an image as input.
%Second, when combined with GET3D, our model can fully exploit their generative capabilities and produces high-quality 3D shapes as shown in Figure~\ref{fig:get3d}.
%The above manifests that ISS is generic and compatible to advanced models for generating shapes of more categories and higher qualities.


\vspace*{10pt}\noindent\textbf{More generative results.} In addition, we showcase a diverse range of 3D shapes that have been effectively generated from texts using our approach in Figure~\ref{fig:all}.
%Further, we present more generative results of our approach in Figure~\ref{fig:all}. we are able to effectively generate a wide variety of 3D shapes from texts. 



%They are used to evaluate whether our ISS method can leverage the features of advanced models in terms of generated categories and qualities.

%\xjqi{delete }As shown in Figure~\ref{fig:ss3d}, our approach can generate shapes for more categories built upon SS3D~\cite{alwala2022pre} beyond those in ShapeNet and CO3D,~\eg, birds. 
%\phil{more in terms of what? or compared with what?}
%In training, both SS3D
%\phil{TODO: a typo: what do you want to refer to here?} 
%and ours only utilize single images of these categories without camera pose. Like~\cite{alwala2022pre}, we utilize the ground-truth object masks of the bird category, and derive masks of the other categories with an off-the-self semantic segmentation model. As shown in Figure~\ref{fig:ss3d} (a,b), our approach with  %\phil{only?}
%only text as the input can generate shapes with comparable or even better quality compared with~\cite{alwala2022pre} that takes an image as input. %\phil{an image as input.}
%\phil{also an image?}\lzz{only an image, no text} an image as input.
%Further, going beyond~\cite{alwala2022pre}, our novel approach can create 3D shapes with textures, thanks to our text-guided texture stylization (see Figure~\ref{fig:ss3d} (c)). In addition, 
%it \phil{who? which figure? which result?} \lzz{Figure~\ref{fig:ss3d} (c)} 
%Figure~\ref{fig:ss3d} (c) further manifests that our stylization module can generate textures that are well-consistent with the shape.

%\vspace*{-10pt}
%\xjqi{delete}\subsection{Generating 3D shapes with better fidelity}\label{im-net}
%Thanks to the compatibility of our ISS to different SVR models, the generative quality of our model can be {\em further enhanced} when adopting a stronger SVR model of IM-Net~\cite{chen2019learning}, as shown in Figure~\ref{fig:im-net}.

%To investigate the generative quality of our approach with a stronger SVR model, we build our model on the SVR model of IM-Net~\cite{chen2019learning}. The results in Figure~\ref{fig:im-net} show that the generative quality of our model can be FURTHER ENHANCED with a stronger SVR model. 
%Note that in our submission, we only build our model on SVR models without directly using 3D data in our training, and it is challenging for them to generate high quality shapes. In addition, recent works that can generate high quality 3D shapes (like IM-Net) typically utilize the 3D data in training. Our model is compatible with these models and can potentially generate higher quality shapes with stronger SVR models in future.










\subsection{Analysis of Feature Space Mapping}\label{sec:mapping}

To better understand how our two-stage feature-space alignment works, we further study the average feature distances at different stages for all samples in our test set as shown in Table~\ref{tab:mapping}. Please also refer to Figure~\ref{fig:motivation} (c) for the visualized results. 

In the stage-1 alignment, we train the mapper $M$ to map the CLIP image feature $f_\text{I}$ to $M(f_\text{I})$ that is close to the target shape $f_\text{S}$ with latent-space regression. Based on the fact that the CLIP model is able to map $f_\text{I}$ and $f_\text{T}$ to a shared embedding space, it is a natural assumption that the mapper $M$ is also able to map $f_\text{T}$ close to the target shape space. Yet, we found that there is a large gap between $M(f_\text{T})$ and $f_\text{S}$ even with the stage-1 alignment. Specifically, the average distance of all samples between $M(f_\text{T})$ and $M(f_\text{I})$ is $0.58 \pm 0.23$, indicating a substantial gap between the CLIP image and text features. Also, the measured average distance between $M(f_\text{I})$ and $f_\text{S}$ is $0.21 \pm 0.10$, while the distance of mapped text and shape is $d(M(f_\text{T}), f_\text{S}) = 0.45 \pm 0.20$, indicating a large room for further improvement. Importantly, the above motivates us to adopt an additional stage-2 alignment. It should be noted that since there is no ground truth 3D shape in our task, we manually select a shape from the ShapeNet dataset that matches well with the input text as the ground truth shape.

During the stage-2 alignment, the mapper $M$ is fine-tuned to be $M'$ for each input text to further narrow the gap between $M'(f_\text{T})$ and $f_\text{S}$ to be $0.17 \pm 0.08$, which is much smaller than $0.45 \pm 0.20$, i.e., $d(M(f_\text{T}), f_\text{S})$ after the stage-1 alignment. This analysis manifests that the stage-2 alignment can effectively reduce the gap between features of the mapped text and the reference shape.

%To  provide more insights on explaining how the latent space is mapped in the two-stage feature space alignment, we measure the mean distance of features at different stages on all the 52 samples in our test set on ShapeNet. % in Table~\ref{tab:mapping}. The notations follow Figure 3 (c) of the main paper, $M$ means the mapper, and $d$ means cosine distance. Our ultimate goal is to obtain a text mapper $M'$ (Figure 2 in the main paper) to map the text feature space $f_T$ to shape feature space $f_S$ using image with features $f_I$ as a stepping stone to gradually narrow their distances using two stage mapping. Note that the image $f_I$ and text features $f_T$ are obtained using pre-trained CLIP models.  

%In the stage-1 alignment process, we train a mapper $M$ to map image features $f_I$ to a space $M(f_I)$ close to the shape space $f_S$ using image data and the regression loss $L_M$. Note that the text feature $f_T$ and image feature $f_I$ are all from the CLIP model in a shared embedding space. Its natural that the trained mapper can be used to map the text feature $f_T$ to $M(f_T)$, making text features closer to the shape space. However, when measuring the distance among $M(f_T)$,  $M(f_S)$, we find the average distance among all samples is $d(M(f_I), M(f_T))= 0.58 \pm 0.23$,  the average distance between $M(f_I)$ and $f_S$ is $0.21 \pm 0.10$ , and the average distance between shape and text is  $d(M(f_T), f_S)) = 0.45  \pm  0.20$. This implies that there is a gap between CLIP image and text feature after the first step mapper and further motivates our stage-2 alignment. Note that there is no GT shape for our task on 3D generation, so we manually select a shape in the ShapeNet dataset that matches the input text as the GT. 


%In the stage-2 alignment, $M$ is further updated and the final delivered mapper is called $M'$ which is to further narrow down the gap between mapped text features and shape features. The average distance between the mapped text feature $M(f_T)$ and shape feature space $f_S$: $d(M(f_T), f_S)) = 0.17 \pm  0.08$ which is much smaller than the corresponding distance after stage-1 alignment.  It shows that stage 2 alignment can significantly reduce the difference between the mapped text and the GT shape feature from 0.45 to 0.17 on average. 





\begin{comment}
\section{Failure Cases}~\label{sec:failure}


Here are some examples of failure cases of our approach. 


\subsection{The complex and unusual shapes, \eg, ``an oval table with 3 legs. ''}

First, our model fails to generate the shape from the text ``an oval table with 3 legs. ''. Our approach leverages the CLIP consistency loss in the rendered 2D image; however, in the rendered image shown in Figure~\ref{fig:failure} (a), only three legs can be seen and the remaining one is occluded, which confuses the model training. 


\subsection{The given text is very long, \eg, ``it is grey in color , circular in shape with four legs and back support, material used is wood and overall appearance looks like unique design armless chair. ''}

Our model fails to generate the shape from the above long description as shown in Figure~\ref{fig:failure} (b). Some attributes are missing including ``armless'', ``four legs''. This is partially due to the limited representative capability of a single CLIP feature for such a long sentence. We may incorporate an additional local feature like~\cite{liu2022towards} to handle the long text in the future.  
%Due the limit of the GPU memory, the rendered image in training has the limited resolution (100$\times$100 in our experiments. It makes our model hard to generate tiny structures since they cannot rendered clearly in training. }

\subsection{The shapes with multiple fine-grained descriptions, \eg, ``A chair with a red back and a green cushion and white armrest and black legs. ''}

As shown in Figure~\ref{fig:failure} (c), our model fails to generate the shape ``A chair with a red back and a green cushion and white armrest and black legs. ''. As studied in some recent works~\cite{yao2021filip,li2022fine}, CLIP mainly address on the global image feature, but has inferior capability to capture fine-grained features. Therefore, our approach may fail to generate shapes where multiple fine-grained descriptions are given. In the future, we may try more recent pre-trained text-and-image embedding models to enhance the model's capability to handle the fine-grained descriptions. 
\end{comment}


\section{Limitations}~\label{sec:limitation}
ISS++ has a trade-off between the generative fidelity of 3D shapes within the image dataset and the generation capability for categories outside the image dataset. Although ISS++ can generate shapes outside the dataset with better surface quality (as shown in Figure~\ref{fig:out-category}), its out-of-category generative capability may not always outperform DreamFusion~\cite{poole2022dreamfusion}, as demonstrated in Figure~\ref{fig:limitation}. We empirically found that it can be helpful to choose an initialization shape with a similar topology to the desired shape for the SDS procedure. However, there is still a lack of guidance on how to choose a suitable initialization shape for an out-of-category generation.
%ISS++ improves the fidelity of in-category shapes at the cost of sacrificing the generation capability on categories out of the image dataset. To be more specific, despite that ISS++ has certain capability to generate shapes outside the image dataset (see Figure~\ref{fig:out-category}) with even better surface quality, the out-of-category generative capability of ISS++ may not be always as good as DreamFusion~\cite{poole2022dreamfusion} as shown in Figure~\ref{fig:limitation}. To address this issue, we empirically found that it is helpful that the initialized shape for SDS procedure has similar topology as the desired one. However, there still lacks guidance on how to choose a good initialization shape for the out-of-category generation. }





%This work still has some limitations. First, our performance is limited by the SVR model that our approach is built upon, \eg, some results in Figure 10 of the main paper and Figure~\ref{fig:ss3d_supp} in this supplementary material are still not very satisfactory, because SS3D~\cite{alwala2022pre} itself is struggling to create shapes with fine details. 


%Second, we can hardly generate the categories outside the image dataset due to the lack of 3D prior of the unseen category. That is why our model needs images as the stepping stone to learn what the particular category is like.  However, we want to highlight the following. First, built upon SS3D~\cite{alwala2022pre}, our approach can generate a wide range of categories with single-view images in the wild as training data. Second, with our shape-and-texture stylization \textcolor{blue}{and SDS-guided refinement and stylization}, our approach can generate imaginary and uncommon shapes outside the image dataset. 


















\begin{comment}
\begin{figure*}
\centering
\includegraphics[width=0.99\textwidth]{failure.pdf}
\vspace*{-1.5mm}
\caption{Failure cases of our approach. 
}
\label{fig:failure}
\vspace*{-2.5mm}
\end{figure*}
\end{comment}

\begin{comment}
\begin{figure*}
\centering
\includegraphics[width=0.99\textwidth]{wheels.pdf}
\vspace*{-1.5mm}
\caption{Our generative quality is limited by the DVR model that we build upon. (a) The result of DVR with single image as input. (b) Our result with text as input. }
\label{fig:wheels}
\vspace*{-2.5mm}
\end{figure*}
\end{comment}

\section{Conclusion}
In this work, we introduce a novel approach for text-guided 3D shape generation that leverages the image modality as a stepping stone. Our approach eliminates the need for paired text and shape data by using joint text-image features from CLIP and shape priors from a pre-trained single-view reconstruction model. Technically, we have the following contributions.
First, our two-stage feature-space alignment reduces the gap between text, image, and shape modalities. Second, the text-guided refinement and stylization techniques effectively and efficiently equip the generated 3D shapes with rich details and diverse styles. Third, our proposed approach is compatible with different single-view reconstruction methods and can be developed to produce shapes in a wide variety of categories and with higher fidelity. % generate a wide range of categories with only single images without camera poses in training.
Experimental results on ShapeNet, CO3D, and additional categories demonstrate that our approach outperforms SOTA approaches and various baselines.


%In this paper, we present a novel approach for text-guided 3D shape generation by leveraging the image modality as a stepping stone.  Leveraging the joint text-image embeddings from CLIP and 3D shape priors from a pre-trained SVR model, our approach eliminates the need for the paired text and shape data.
%Technically, we have the following contributions.
%First, we step-by-step reduce the semantic gap among the text, image and shape modalities through our two-stage feature-space alignment approach.
%Second, our text-guided stylization techniques effectively enriches our generated shapes with novel structures and textures in various styles \textcolor{blue}{and also improves their quality with rich details. }
%Third, our approach is compatible with various single-view reconstruction approaches and can be further extended to generate a wide range of categories with only single images without camera poses in training.
%
%Experiments on ShapeNet, CO3D, and multiple single-image categories manifest the superiority of our framework over the two state-of-the-art methods and various baselines.
%Limitations are discussed in the supplementary files. 




\section*{ACKNOWLEDGEMENTS}
The work has been supported in part by  the Research Grants Council of the Hong Kong Special Administrative Region (Project no. CUHK 14206320), General Research Fund of Hong Kong (No. 17202422), Hong Kong Research Grant Council - Early
Career Scheme (Grant No. 27209621), General Research Fund (Grant No. 17202422) and National Natural Science Foundation of China (No. 62202151). We would also like to thank Mr. Jingyu Hu from The Chinese University of Hong Kong and Dr. Karsten Kreis from NVIDIA's Toronto AI Lab for insightful discussions and contributions to the ideas presented in this work. 









































































% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}

%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\bibliographystyle{ieee}
\bibliography{egbib}
%\end{thebibliography}


\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{lzz.jpg}}]
{Zhengzhe Liu} is currently a Ph.D. candidate at The Chinese University of Hong Kong. He received his B.Eng degree in Information Engineering at Shanghai Jiao Tong University, and the M.Phil. degree in Computer Science and Engineering from The Chinese University of Hong Kong. His research interests include AIGC, 3D shape generation, and 3D scene understanding. 
\end{IEEEbiography}


\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{dp.jpg}}]
{Peng Dai} received the B.Eng. and M.Eng. degrees from the University of Electronic Science and Technology of China, in 2017 and 2020, respectively. He is currently a Ph.D. candidate at the University of Hong Kong.  His research interests lie at computer vision, computer graphics, and neural rendering.
\end{IEEEbiography}


\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{rh.jpg}}]
{Ruihui Li} is currently an associate professor at Hunan University. Before that, he was a post-doctoral fellow at the Chinese University of Hong Kong. He received his Ph.D. degree in the Department of Computer Science and Engineering from the Chinese University of Hong Kong.  His research interests include deep geometry learning, generative modeling, 3D vision, and computer graphics.
\end{IEEEbiography}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{qxj.jpg}}]{Xiaojuan Qi} is currently an assistant professor at the University of Hong Kong and a member of Deep Vision Lab. Before that, she
received her B.Eng degree in Electronic Science and Technology at Shanghai Jiao Tong University (SJTU) in 2014, and the PhD degree in Computer Science and Engineering from the Chinese University of Hong Kong in 2018. Her research lies in the broad areas of Computer Vision, Deep Learning, and Artificial Intelligence. 
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{philip.jpg}}]{Chi-Wing Fu} is currently a full professor in the Chinese University of Hong Kong. He served as the co-chair of SIGGRAPH ASIA Technical Brief and Poster program, associate editor of IEEE Computer Graphics \& Applications, and Computer Graphics Forum, panel member in SIGGRAPH 2019 Doctoral Consortium, and program committee members in various research conferences, including SIGGRAPH Technical papers, SIGGRAPH Asia Technical Brief, SIGGRAPH Asia Emerging tech., IEEE visualization, CVPR, IEEE VR, VRST, Pacific Graphics, GMP, etc. His recent research interests include point cloud processing, 3D computer vision, computation fabrication, user interaction, and data visualization.
\end{IEEEbiography}


% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}




% that's all folks
\end{document}


