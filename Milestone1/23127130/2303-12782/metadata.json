{
    "arxiv_id": "2303.12782",
    "paper_title": "Tube-Link: A Flexible Cross Tube Baseline for Universal Video Segmentation",
    "authors": [
        "Xiangtai Li",
        "Haobo Yuan",
        "Wenwei Zhang",
        "Guangliang Cheng",
        "Jiangmiao Pang",
        "Chen Change Loy"
    ],
    "submission_date": "2023-03-22",
    "revised_dates": [
        "2023-06-28"
    ],
    "latest_version": 2,
    "categories": [
        "cs.CV"
    ],
    "abstract": "The goal of video segmentation is to accurately segment and track every pixel in diverse scenarios. In this paper, we present Tube-Link, a versatile framework that addresses multiple core tasks of video segmentation with a unified architecture. Our framework is a near-online approach that takes a short subclip as input and outputs the corresponding spatial-temporal tube masks. To enhance the modeling of cross-tube relationships, we propose an effective way to perform tube-level linking via attention along the queries. In addition, we introduce temporal contrastive learning to instance-wise discriminative features for tube-level association. Our approach offers flexibility and efficiency for both short and long video inputs, as the length of each subclip can be varied according to the needs of datasets or scenarios. Tube-Link outperforms existing specialized architectures by a significant margin on five video segmentation datasets. Specifically, it achieves almost 13% relative improvements on VIPSeg and 4% improvements on KITTI-STEP over the strong baseline Video K-Net. When using a ResNet50 backbone on Youtube-VIS-2019 and 2021, Tube-Link boosts IDOL by 3% and 4%, respectively. Code will be available.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.12782v1",
        "http://arxiv.org/pdf/2303.12782v2"
    ],
    "publication_venue": "Project page: https://github.com/lxtGH/Tube-Link (fix typos and errors)"
}