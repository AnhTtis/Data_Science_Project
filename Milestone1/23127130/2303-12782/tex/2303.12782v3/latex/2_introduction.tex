\section{Introduction}
% old introduction logic
% 1. long/various video segmentation
% 2. summary of current solution of vis vps and short comings
% 3. Motivaiton Our new framework
% 4. Difference and Features
% 5. Results

% new introduction logic: 
% 1, Introduction of Universal Video Segmentation 
% 2, Summary of Current Video Segmentation Approaches and Universal Segemntaiton Approaches. Highlights cons 
% 3, Motivation Our approaches:  
% 4, Details of our approaches.
% 5, Results of our approaches.
% previous VPS methods: (a), Online method~\cite{kim2020vps,li2022videoknet} that tracks by detection and segmentation. The association is performed frame-by-frame. (b), Offline method~\cite{ViPDeepLab,kim2022tubeformer} that relies on multi-frame inputs to produce multi-frame outputs. The association is performed offline on the entire clip. (c),

\begin{figure}[t!]
	\centering
	\includegraphics[width=1.0\linewidth]{./figs/teaser_01_merge_new.pdf}
	\caption{\small Tube-Link takes subclips as inputs and links the resulting tubes in a near online manner. Our design embraces flexibility, efficiency, and temporal consistency, making it suitable for various video segmentation tasks, including VSS, VIS, and VPS. Notably, our method outperforms the best-specialized architectures for these tasks on multiple datasets. Best viewed in color.}
    \label{fig:teasear}
\end{figure}

% 1, Introduction of Universal Video Segmentation 
%
The success of the Detection Transformer (DETR)~\cite{detr} has inspired recent works~\cite{cheng2021mask2former,zhang2021knet,cheng2021maskformer,wang2020maxDeeplab} to develop universal architectures for addressing all image segmentation tasks using the same architecture, also known as universal image segmentation. 
%
In video segmentation, the Video Panoptic Segmentation (VPS) task involves segmenting and tracking each pixel in input video clips~\cite{kim2020vps,STEP,hurtado2020mopt}, unifying the Video Semantic Segmentation (VSS)~\cite{miao2021vspw} and Video Instance Segmentation (VIS)~\cite{vis_dataset} tasks. 
%
To minimize specialized architecture design for each task, recent studies~\cite{li2022videoknet,kim2022tubeformer} follow a universal approach to video segmentation by solving sub-tasks of VPS in a single unified framework. 
%
These methods typically use an end-to-end set prediction objective and successfully address multiple tasks without modifying the architecture and loss.


% 2, Summary of Current Video Segmentation Approaches and Universal Segemntaiton Approaches. Highlights cons 
While recent studies have demonstrated promising results, there are still several issues with VPS models and universal video segmentation methods. 
%
One major challenge is the lack of exploration of VPS for arbitrary scenes and video clip lengths. To address this gap, Miao~\etal~\cite{miao2022large} have introduced a more challenging benchmark, named VIPSeg, which features long videos and diverse indoor and outdoor scenes. This new dataset presents new challenges to existing VPS methods~\cite{kim2020vps,miao2022large,li2022videoknet,ViPDeepLab}, such as increased occlusions and appearance changes in diverse scenarios. 
%
Another issue with universal methods~\cite{li2022videoknet,kim2022tubeformer} is that they cannot achieve comparable results to recent Transformer-based VIS methods~\cite{IDOL,seqformer}, which raises the question of whether we can design a universal video segmentation method to avoid these specialized designs.


% 3, Motivation Our approaches:  
To gain a better understanding of the limitations of current solutions for video segmentation, we examine the existing methods and categorize them into two groups based on how they process input video clips: \textit{online} and \textit{near-online}. The former~\cite{kim2020vps,li2022videoknet,IDOL,vis_dataset,woo2021learning_associate_vps} performs video segmentation at the frame level, while the latter~\cite{ViPDeepLab,STEP,kim2022tubeformer,VIS_TR,cheng2021mask2former,hwang2021video,seqformer} processes clip-wise inputs and directly obtains tube-wise masks. 
%
However, there are trade-offs in deploying either approach. Although online methods offer great flexibility, they struggle to use temporal information effectively and thus compromise segmentation performance. 
%
On the other hand, near-online methods achieve better segmentation quality, but they cannot handle long video clips, and most approaches are only validated in VIS tasks, which have fewer instances and simpler scenes.


% 4, our method and contribution

In this study, we introduce Tube-Link, a universal video segmentation framework that combines the benefits of both online and near-online methods. 
%
The framework follows a common input and output space for video segmentation tasks, where a long clip input is split into multiple subclips. 
%
Each subclip contains several frames within a temporal window, and the output is a spatial-temporal mask that tracks the target entity. 
%
Our framework is compatible with contemporary methods such as Mask2Former-VIS~\cite{cheng2021mask2former_vis}, where each global query encodes the same tracked entity, and the global queries perform cross-attention with spatial-temporal features in the decoder directly. 


In particular, we propose several key improvements to the Mask2Former-VIS meta-architecture. \textbf{First}, we extend the instance query into an entity query (either thing or stuff) to improve temporal consistency for both thing and stuff segmentation, thus generalizing Mask2Former-VIS into a universal segmentation architecture. 
%
\textbf{Second}, we improve the modeling of cross-tube relationships from two different aspects through temporal consistency learning and temporal association learning. 
%
For the former, we design a simple link head with self-attention layers that links global queries across tubes to enforce segmentation consistency across tubes. 
%
For the latter, we generalize previous frame-level contrastive learning into tube-level and learn temporal association embeddings with a temporal contrastive loss. 
%
Unlike previous works~\cite{li2022videoknet,IDOL,qdtrack} that only learn from two adjacent frames, we consider multiple frames to learn cross-tube consistency. The learned embeddings are then used to perform tube mask matching, which is much more effective than previous counterparts~\cite{li2022videoknet} in complex video scenarios. 
%
\textbf{Third}, with the flexibility of window size and learned association embeddings, we show that one can enlarge the subclip size to improve temporal consistency and inference efficiency, even when trained with fewer subclip inputs. 
%Hence, our framework is highly flexible and can easily adapt to different tasks and datasets for long and complex scenes. 


% tube-link results
Our approach is a simple yet flexible framework that outperforms specialized architectures across various video segmentation tasks.
We evaluate Tube-Link on three video segmentation tasks using six datasets (VIP-Seg~
\cite{miao2022large}, KITTI-STEP~\cite{STEP}, VSPW~\cite{miao2021vspw}, YouTube-VIS-2019/2021~\cite{vis_dataset}, OVIS~\cite{qi2022occluded}). We demonstrate that, for the first time, our single architecture performs on par or better than the most specialized architectures on five video benchmarks. In particular, as shown in Fig.~\ref{fig:teasear}, using the same ResNet-50 backbone, Tube-Link outperforms recently published works Video K-Net~\cite{li2022videoknet} 4\% VPQ on KITTI-STEP, 13\% VPQ, and 8\% STQ on VIP-Seg, VITA~\cite{heo2022vita} and IDOL~\cite{IDOL} on YouTube-VIS-2019 by 3\% mAP. We also outperform TubeFormer~\cite{kim2022tubeformer} on VPSW by 1.7\% mIoU, and on YouTube-VIS-2019 by 5.3\% mAP. 

%In Fig.~\ref{}, our method achieves the best speed and accuracy trade-off on VIP-Seg. 




