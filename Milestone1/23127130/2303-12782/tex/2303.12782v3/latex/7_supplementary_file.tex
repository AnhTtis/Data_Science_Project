\appendix
\section{Appendix}


\noindent
\textbf{Overview.} In addition to the main paper, we further list the following details and more experiment results as supplementary to our work.

\begin{enumerate}
\setlength{\leftmargin}{-1em}
\setlength{\parsep}{0ex} 
\setlength{\topsep}{0ex}
\setlength{\itemsep}{0.5ex}  
\setlength{\labelsep}{0.5em} 
\setlength{\itemindent}{-0.5em} 
\setlength{\listparindent}{0em} 
\item More detailed description and comparison of Tube-Link. (Sec.~\ref{sec:more_decription_tube_link})
\item Detailed experiment settings and implementation details for each dataset. (Sec.~\ref{sec:implemntation_details})
\item More ablation studies and experiment results.(Sec.~\ref{sec:more_ablation_and_exp_results}) 
\item More visual results. (Sec.~\ref{sec:vis_results})
\end{enumerate}



\begin{table*}[!t]
   \centering
    \caption{\small Different Setting Comparison with previous VIS and VPS methods.}
   \scalebox{0.65}{
   \setlength{\tabcolsep}{2.5mm}{\begin{tabular}{c | c c   c | c  c | c | c c c c}
      \toprule[0.15em]
        Method &  VSS & VIS & VPS & Online & Nearly Online & Joint Mulitple Frames  & Frame Matching & Tube Matching & Mask Matching & No Association (use Query Index) \\
        \hline 
        \rowcolor{gray!15} CFFM~\cite{sun2022vss} & \checkmark &  &  &  & \checkmark  & \checkmark &  &  &  & \checkmark \\
         \rowcolor{gray!15} MRCFA~\cite{sun2022mining} & \checkmark &  &  &  & \checkmark  & \checkmark &  & & & \checkmark  \\
         \hline
        \rowcolor{orange!15} Cross-VIS~\cite{CrossVIS} & &  \checkmark &  & \checkmark & & & \checkmark & & &\\
        \rowcolor{orange!15} IDOL~\cite{IDOL} & &  \checkmark &  & \checkmark & & & \checkmark & & &  \\
        \rowcolor{orange!15} SeqFormer~\cite{seqformer} & & \checkmark & &  & \checkmark & \checkmark & & & & \checkmark \\
         \rowcolor{orange!15}EfficientVIS~\cite{EfficientVIS}& & \checkmark & & & \checkmark & \checkmark & & & & \checkmark\\   
         \rowcolor{orange!15}VITA~\cite{heo2022vita} & & \checkmark & & & \checkmark & \checkmark & & & & \checkmark \\
        \rowcolor{orange!15} Min-VIS~\cite{huang2022minvis} & & \checkmark & & \checkmark & & & \checkmark & & & \\
         \rowcolor{orange!15} IFC~\cite{hwang2021video} & & \checkmark &   & & \checkmark & \checkmark &  &  \checkmark & & \\
         \rowcolor{orange!15} Gen-VIS~\cite{heo2022generalized} & & \checkmark & & \checkmark & \checkmark & \checkmark & & \checkmark & &  \\
        \hline
        \rowcolor{blue!15} SLOT-VPS~\cite{slot_vps} &  & & \checkmark &  & \checkmark & \checkmark & & & & \checkmark \\
        \rowcolor{blue!15} TubeFormer~\cite{kim2022tubeformer} &  \checkmark & \checkmark & \checkmark &  & \checkmark & \checkmark & & & \checkmark &   \\
        \rowcolor{blue!15} Video K-Net~\cite{li2022videoknet} & \checkmark & \checkmark & \checkmark & \checkmark &  &  & \checkmark & & & \\
        \rowcolor{red!15} Our Tube-Link & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark &  & \checkmark &  & \\
      \bottomrule[0.10em]
   \end{tabular}}}
   \label{tab:more_detailed_comparison}
\end{table*}


\subsection{More Detailed Description of Tube-Link}
\label{sec:more_decription_tube_link}


This section presents the method details, including several baselines and Tube-Link inference for different datasets. 
%
Then, due to the limited pages in the main paper, we compare several closely related works in VIS and VPS in detail. 


\vspace{2mm}
\noindent
\textbf{Video K-Net+ Baseline.} This baseline is based on two previous state-of-the-art methods, including Video K-Net~\cite{li2022videoknet} and Mask2Former~\cite{cheng2021mask2former}. 
%
In particular, Video K-Net is based on K-Net~\cite{zhang2021knet}, an image panoptic segmentation model. We replace K-Net with Mask2Former~\cite{cheng2021mask2former}, and the remaining parts are the same as the Video K-Net. 
%
Since the performance of Mask2Former is better than K-Net on image segmentation datasets, Video K-Net+ serves as the strong online baseline for both VPS and VSS tasks.

\vspace{2mm}
\noindent
\textbf{Detailed Inference Procedure of Panoptic Matching.} During the inference, we perform tube-level panoptic matching according to learned association embeddings only on final panoptic tube masks.
%
In particular, we save the index of each global query from the final panoptic tube results, and then we use these indexed queries via the embedding head $Emb$. 

\vspace{2mm}
\noindent
\textbf{Detailed Inference Procedure of VSS and VIS.} Since VSS does not need tracking, we do not apply the extra tracking embedding during the inference. 
%
Instead, the tube mask logits are obtained directly from the dot product between global queries and spatial-temporal decoder features. 
%
The final segmentation labels are directly obtained via argmax on predicted logits. 
%
For VIS, we follow nearly the same procedure as VPS, except no stuff queries are involved.


\vspace{2mm}
\noindent
\textbf{More Detailed Comparison with Previous Nearly Online Approaches in VIS and VPS.} In addition to the main paper, in Tab.~\ref{tab:more_detailed_comparison}, we present a more detailed comparison with previous works on VIS and VPS. From the table, our method uses tube-wised matching and supports all three video segmentation tasks in one architecture. 

In particular, both SLOT-VPS~\cite{slot_vps} and SeqFormer~\cite{seqformer} also adopt multiple frames design. However, there are no data association processes involved. Moreover, they are designed for VIS and VPS, individually, and our method outperforms the SeqFormer on two VIS datasets, as shown in Tab.~\ref{tab:ytvis_supp}. Furthermore, unlike SLOT-VPS and SeqFormer, our method can handle long video inputs.

Gen-VIS~\cite{heo2022generalized} also adopts the tube-wised design, which combines the nearly online method and online method in one framework. However, it can not support other video segmentation tasks, including VSS and VPS. Moreover, it is  
not verified in more complex scenes, including the driving dataset KITTI-STEP~\cite{STEP} and the recent more challenging dataset VIP-Seg~\cite{miao2022large}. In contrast, our Tube-Link is fully verified by three different video segmentation tasks and five different datasets. In particular, using the same ResNet50 backbone and detector~\cite{cheng2021mask2former}, even without COCO video joint training, our method works better than Gen-VIS~\cite{heo2022generalized}, as shown in Tab.~\ref{tab:ytvis_supp}.

% % online apporaches.
% Compared with previous online approaches, our method considers 

\subsection{Implementation Details}
\label{sec:implemntation_details}

\noindent
\textbf{Detailed Training and Inference on VIP-Seg.} We use the COCO-pretrained model following~\cite{miao2022large}. The entire training process takes eight epochs. We adopt multiscale training where the scale ranges from 1.0 to 2.0 of the original image size, and then we apply a random crop of 720 $\times$ 720 patches. In particular, we perform the augmentation for each frame in the sampled subclips. For the inference, the subclip window size is set to six by default. We pad the remaining frames in the last subclip by repeating the last frame. We drop the padded results for evaluation.

\vspace{2mm}
\noindent
\textbf{Detailed COCO pretraining setting.} 
For COCO~\cite{coco_dataset} panoptic segmentation dataset pretraining, all the models are trained following original Mask2Former settings~\cite{zhang2021knet}. We adopt the multiscale training setting as previous work~\cite{detr} by resizing the input images such that the shortest side is at least 480 and 800 pixels, while the longest size is at most 1333. For data augmentation, we use the default large-scale jittering (LSJ) augmentation with a random scale sampled from the range 0.1 to 2.0 with the crop size of 1024 $\times$ 1024. For ResNet50~\cite{resnet} and Swin-base~\cite{liu2021swin} model, we train the model with 50 epochs following the original settings. For STDC model~\cite{STDCNet}, we train the model for 36 epochs.

\vspace{2mm}
\noindent
\textbf{Detailed Training and Inference on KITTI-STEP dataset.} For KITTI-STEP training, we follow previous Motion-Deeplab~\cite{STEP} and Video K-Net~\cite{li2022videoknet}, we adopt multiscale training where the scale ranges from 1.0 to 2.0 of origin images size. We then apply a random crop of 384 $\times$ 1248 patches. The total training epoch is set to 12. The inference procedure is the same as Cityscapes-VPS dataset. Following the previous works~\cite{STEP,li2022videoknet}, we also use Cityscapes dataset~\cite{cordts2016cityscapes} pretraining before training on STEP. Pretraining on Cityscapes STEP datasets further leads to 3\% VPQ and 2\% STQ improvements. We adopt the same inference pipeline as VIP-Seg, where we set the subclip window size to 2. We \textbf{do not} pre-train our model on the COCO dataset for a fair comparison.

\vspace{2mm}
\noindent
\textbf{Detailed Training and Inference on VSPW dataset.} We adopt nearly the same training pipeline for VSPW as VIP-Seg. The main difference is that we adopt longer training epochs, where we set the training epochs to 12, where we find about 1\% mIoU gain over different baselines. Moreover, we remove the tracking loss since we only focus on segmentation quality.

\vspace{2mm}
\noindent
\textbf{Detailed Training and Inference on Youtube-VIS-2019/2021 datasets.} We follow the same setting as Mask2Former-VIS~\cite{cheng2021mask2former_vis}. We train our models for 6k iterations, with a batch size of 16 for YouTubeVIS-2019 and 8k iterations for YouTubeVIS-2021.
All models are initialized with COCO instance segmentation models of Mask2Former. Different from previous SOTA VIS models~\cite{heo2022vita,seqformer,IDOL}, we only use YouTubeVIS training data, and \textit{do not} use COCO video images for data augmentation. Moreover, we also do not apply clip-wised copy-paste that is used in TubeFormer~\cite{kim2022tubeformer}. The same training procedure is adopted for the OVIS dataset as well.



\begin{table}[t]
  \centering
   \caption{\small \textbf{More Ablation on Tube-Wised Matching in Youtube-VIS dataset.}}
  \label{tab:tube_wisedmethod}
  \scalebox{0.90}{
  \begin{tabular}{l | c c  }
    \toprule[0.2em]
    \textbf{Settings} & Youtube-VIS-2019 & Youtube-VIS-2021 \\
    \toprule[0.2em]
     tube size=1 & 47.8 & 44.2 \\
     tube size=2 & 49.8 &  45.9  \\
     tube size=3 & 51.3 &  46.2  \\
     \rowcolor{gray!15} tube size=4 & 52.8  &  47.9  \\
     tube size=6 &  51.2 & 46.8  \\
    \bottomrule[0.1em]
  \end{tabular}
  }
\end{table}


\begin{table}[t]
  \centering
   \caption{\small \textbf{Ablation on Inference with Overlapped Frames.} We use the STDC-v1 backbone. The subclip window size is 6.}
  \label{tab:over_lap_window_size}
  \scalebox{0.90}{
  \begin{tabular}{l | c c c c }
    \toprule[0.2em]
    \textbf{Settings} & STQ & VPQ & SQ & FPS \\
    \toprule[0.2em]
     \rowcolor{gray!15} No Overlapping &  32.0 & 30.6 & 28.4 &  16.2 \\
     Overlapping=1 & 31.0  &  30.5 & 28.5 & 14.6 \\
     Overlapping=2 & 32.3  &  31.2 & 29.1 & 10.2 \\
     Overlapping=4 &  33.1  & 31.6  & 28.6 & 8.4 \\
    \bottomrule[0.1em]
  \end{tabular}
  }
\end{table}


\begin{table}[t]
  \centering
   \caption{\small \textbf{Ablation on Effect of COCO Pretraining.} We use the STDC-v1 backbone.}
  \label{tab:abl_coco_pretrain}
  \scalebox{0.90}{
  \begin{tabular}{l c | c c c }
    \toprule[0.2em]
    \textbf{Settings} & Method & STQ & VPQ & SQ \\
    \toprule[0.2em]
    w COCO pretrained & Video K-Net+ & 26.1 & 25.8  & 25.2 \\
   \rowcolor{gray!15} w/o COCO pretrained & Video K-Net+ & 12.4 & 12.4  & 18.3 \\
    \hline
    w COCO pretrained & Tube-Link & 32.0 & 30.6 & 28.4 \\
   \rowcolor{gray!15} w/o COCO pretrained & Tube-Link & 21.8 & 16.8 & 20.3 \\

    \bottomrule[0.1em]
  \end{tabular}
  }
\end{table}


\begin{table}[t]
  \centering
   \caption{\small \textbf{Ablation on Training Epochs.} We use the STDC-v1 backbone.}
  \label{tab:ablation_train_epoch}
  \scalebox{0.95}{
  \begin{tabular}{l | c c c }
    \toprule[0.2em]
    \textbf{Settings} & STQ & VPQ & SQ \\
    \toprule[0.2em]
    Epoch=4 & 29.2  & 28.1 & 26.5  \\
   \rowcolor{gray!15} Epoch=8 &  32.0 & 30.6 & 28.4 \\
    Epoch=12 & 31.6 & 30.8  & 29.1 \\
    \bottomrule[0.1em]
  \end{tabular}
  }
\end{table}


\begin{table*}[t]
  \centering
   \caption{\small \textbf{Detailed Results on the Youtube-VIS datasets (2019/2021).} We report the mAP metric. \textdagger~adopts COCO video pseudo labels~\cite{heo2022vita,heo2022generalized,heo2022vita}. Axial means using the extra Axial Attention~\cite{axialDeeplab}. Our method does not apply these techniques for simplicity.}
  \label{tab:ytvis_supp}
  \scalebox{0.95}{
  \begin{tabular}{l c | ccccc | cccccc }
    \toprule[0.2em]
    Method & Backbone  & \multicolumn{5}{c}{YTVIS-2019} & \multicolumn{5}{c}{YTVIS-2021} \\
    - & - &  AP        & AP$_{50}$ & AP$_{75}$ & AR$_1$    & AR$_{10}$ & AP        & AP$_{50}$ & AP$_{75}$ & AR$_1$    & AR$_{10}$ \\
    \toprule[0.2em]
VISTR~\cite{VIS_TR} & ResNet50 & 36.2 & 59.8 & 36.9 & 37.2 & 42.4 & - & - & - & - & - \\
EfficientVIS~\cite{EfficientVIS} & ResNet-50     & 37.9      & 59.7      & 43.0      & 40.3      & 46.6  & 34.0     & 57.5    & 37.3      & 33.8      & 42.5  \\
TubeFormer~\cite{kim2022tubeformer} & ResNet50 + Aixal & 47.5 & 68.7 & 52.1 & 50.2 & 59.0 & 41.2 & 60.4 & 44.7 & 40.4 & 54.0  \\
IFC~\cite{hwang2021video} & ResNet50 & 41.2      & 65.1      & 44.6      & 42.3      & 49.6                                                                   & 35.2      & 55.9      & 37.7      & 32.6      & 42.9 \\
Seqformer~\cite{seqformer} & ResNet50 & 47.4      & 69.8      & 51.8      & 45.5      & 54.8                                                                   & 40.5      & 62.4      & 43.7      & 36.1      & 48.1  \\
Mask2Former-VIS~\cite{cheng2021mask2former_vis}& ResNet50 & 46.4      & 68.0      & 50.0      & -         & -  & 40.6      & 60.9      & 41.8      & -         & -   \\
IDOL~\cite{IDOL} & ResNet50 & 46.4  & - & -   & -         & - & 43.9  & - & -   & -         & - \\
IDOL~\cite{IDOL} \textdagger & ResNet50 & 49.5 & - & -   & -         & -   & - & -   & -         & -   & -  \\
VITA~\cite{heo2022vita} \textdagger & ResNet50 & 49.8      & 72.6     & 54.5      & 49.4      & 61.0  & 45.7      & 67.4      & 49.5      & 40.9     & 53.6  \\
Min-VIS~\cite{huang2022minvis} &ResNet50& 47.4      & 69.0      & 52.1      & 45.7      & 55.7     & 44.2      & 66.0      & 48.1      & 39.2      & 51.7 \\
Cross-VIS~\cite{CrossVIS} & ResNet50 & 36.3      & 56.8      & 38.9      & 35.6      & 40.7   & 34.2      & 54.4      & 37.9      & 30.4      & 38.2 \\
VISOLO~\cite{VISOLO} & ResNet50 &  38.6      & 56.3      & 43.7      & 35.7      & 42.5    & 36.9      & 54.7      & 40.2      & 30.6      & 40.9  \\
GenVIS~\cite{heo2022generalized} & ResNet50 & {51.3}  & 72.0    & {57.8}  & {49.5}  & 60.0 & {46.3}  & 67.0 & {50.2} & 40.6 & 53.2 \\
\hline
Tube-Link & ResNet50 & 52.8 & 75.4 & 56.5 & 49.3 &59.9 & 47.9  & 70.0 & 50.2 & 42.3 & 55.2 \\ 
\hline
SeqFormer~\cite{seqformer} & Swin-large  & 59.3      & 82.1      & 66.4      & 51.7      & 64.4  & 51.8      & 74.6      & 58.2      & 42.8      & 58.1  \\
Mask2Former-VIS~\cite{cheng2021mask2former_vis} & Swin-large &  60.4      & 84.4      & 67.0      & -    & -   & 52.6      & 76.4      & 57.2      & -         & -     \\
IDOL~\cite{IDOL}  & Swin-large  & 61.5 & -   & -         & -   & -  & 56.1 & -   & -   & -   & -  \\ 
IDOL~\cite{IDOL}  & Swin-large \textdagger  & {64.3}      & {87.5}      & {71.0}      & 55.6      & 69.1 & 56.1      & 80.8      & 63.5      &  45.0      & 60.1 \\
VITA~\cite{heo2022vita} \textdagger & Swin-large & 63.0    & {86.9}      & 67.9      & {56.3}    & 68.1    & 57.5      & 80.6   & 61.0      & 47.7   & 62.6 \\ 
Min-VIS~\cite{huang2022minvis} & Swin-large & 61.6      & 83.3      & 68.6      & 54.8      & 66.6    & 55.3      & 76.6      & 62.0      & 45.9      & 60.8\\
\hline
Tube-Link & Swin-large  & 64.6 & 86.6 & 71.3 &  55.9 & 69.1 & 58.4 & 79.4  & 64.3 & 47.5 & 63.6 \\
    \bottomrule[0.2em]
  \end{tabular}
}
\end{table*}

\begin{table}[t]
  \centering
   \caption{\small \textbf{Results on the OVIS datasets.} We report the mAP metric. \textdagger~adopts COCO video pseudo labels. Axial means using the extra Axial Attention~\cite{axialDeeplab}. Our method does not apply these techniques for simplicity.}
  \label{tab:ovis}
  \scalebox{0.95}{
  \begin{tabular}{l c c c c  c}
    \toprule[0.2em]
    Method & AP   & AP$_{50}$ & AP$_{75}$ & AR$_1$   & AR$_{10}$ \\
    \toprule[0.2em]
    CrossVIS~\cite{CrossVIS} & 14.9      & 32.7          & 12.1          & 10.3          & 19.8 \\
    VISOLO~\cite{VISOLO} & 15.3      & 31.0          & 13.8          & 11.1          & 21.7  \\
    TeViT~\cite{TeViT}  & 17.4      & 34.9          & 15.0          & 11.2          & 21.8  \\
    VITA~\cite{heo2022vita} & 19.6      & 41.2          & 17.4          & 11.7          & 26.0 \\
    DeVIS~\cite{DeVIS} & 23.8      & 48.0          & 20.8          & -             & -     \\
    Min-VIS~\cite{huang2022minvis} & 25.0      & 45.5          & 24.0          & 13.9          & 29.7 \\
    IDOL~\cite{IDOL}  & 30.2      & 51.3          & 30.0          & 15.0          & 37.5 \\
    VITA~\cite{heo2022vita} \textdagger&  19.6 & 41.2  & 17.4 & 11.7 & 26.0 \\
\hline
Tube-Link & 29.5 & 51.5 &  30.2 & 15.5 & 34.5 \\
    \bottomrule[0.2em]
  \end{tabular}
}
\end{table}


\begin{table}[!t]
    \caption{More Experiment Results.}
    \begin{subtable}{.50\linewidth}
        \centering
        \footnotesize
        \caption{More results on ViP-Seg dataset.}
        \label{tab:more_result_for_tab1}
        \scalebox{0.50}{
        \setlength{\tabcolsep}{2.5mm}{\begin{tabular}{c c c c} 
            \toprule[0.1em]
            Method & STQ & SQ & AQ \\
            \midrule[0.1em]
            Video K-Net &  33.1 &  35.0 & 29.6 \\
          \rowcolor{gray!15} Video K-Net + tube matching (Ours)  & 34.7 & 36.8 & 30.8 \\
           Video K-Net + tube matching (IFC) & 33.3 & 35.7 & 28.4 \\
            \bottomrule[0.1em]
        \end{tabular}}}
    \end{subtable}%
    \begin{subtable}{.5\linewidth}
        \centering
        \footnotesize
        \caption{KITTI-STEP test set results.}
        \label{tab:test_dev_kitti_step}
        \scalebox{0.80}{
        \setlength{\tabcolsep}{2.5mm}{\begin{tabular}{c c c } 
            \toprule[0.1em]
            Method & Backbone & STQ  \\
            \midrule[0.1em]
            Motion-Deeplab & ResNe50 & 0.52 \\
            Video K-Net & ResNet50 & 0.59 \\
             Video K-Net & ResNet50 & 0.63 \\
             \hline
             Tube-Link & ResNet50 & 0.60 \\
             Tube-Link & Swin-base & 0.65 \\
            \bottomrule[0.1em]
        \end{tabular}}}
    \end{subtable}
\end{table}

\subsection{More Ablations and Experiment Results}
\label{sec:more_ablation_and_exp_results}

In this section, we first present more detailed ablations for Tube-Link. Then, we present more detailed results on several datasets, including VIS datasets~\cite{vis_dataset}, OVIS dataset~\cite{qi2022occluded} and VSPW test set~\cite{miao2021vspw}.




\vspace{2mm}
\noindent
\textbf{More Ablations on Effectiveness of Tube-Wised Matching.}
In Tab.~\ref{tab:tube_wisedmethod}, we present more detailed ablations on tube size in Youtube-VIS. Note that, for simplicity, the input subclip size is the same as the tube size. As we enlarge the tube size, we find a significant improvement in the final performance. After enlarging the size to 4, the performance is the best. Using a tube size of 6, the performance slightly degrades. However, it still performs better than single-frame matching. All the models are trained under the same tube size (default is 2). The findings also verify our motivation for using clip-level matching, which shares similar findings on the VIP-Seg dataset in the main paper.



\vspace{2mm}
\noindent
\textbf{Inference with Overlapped Frames in VIP-Seg.} In Tab.~\ref{tab:over_lap_window_size}, we explore the effect of the overlapping size for nearby windows. As shown in that table, increasing the overlapping size leads to better performance for all three metrics: VPQ, STQ, and SQ. This is because we can use multiple frames twice, which leads to more consistent segmentation results. Moreover, instances in smaller windows are easier to be tracked. However, to save computation costs and increase inference speed, we do not introduce overlapping during inference. All the results in the main paper use non-overlapping inference. 


\vspace{2mm}
\noindent
\textbf{Effect of COCO Pretraining in VIP-Seg.} In Tab.~\ref{tab:abl_coco_pretrain}, we show the effect of COCO pretraining on both Video K-Net+ and our Tube-Link. From the table, we can see that COCO pretraining plays an important role for VIP-Seg datasets, which shares the same conclusion with previous work~\cite{li2022videoknet,miao2022large}. Without COCO pretraining, both Video K-Net+ and Tube-Link drop a lot. However, as shown in the gray area, our method \textit{without} COCO pretraining outperforms the Video K-Net+ baseline by a large margin, where we still achieve over 8\% STQ gain and 14\% VPQ gain. The results suggest the effectiveness of our framework on better usage of temporal information.


\vspace{2mm}
\noindent
\textbf{Effect of Training Epoch on VIP-Seg.} We perform ablation on training epochs as in Tab.~\ref{tab:ablation_train_epoch}. With more training epochs, we do not observe performance gain with the COCO pre-trained model due to the overfitting issues. We use eight training epochs by default for all models. 


\begin{figure*}[t!]
	\centering
	\includegraphics[width=0.85\linewidth]{.//figs/yt19_clip_vis_comparison.pdf}
	\caption{Visual Comparison Results from Tube-Link with ResNet50 backbone. Our method (middle) achieves consistent segmentation and better segmentation/tracking results than the Mask2Former-VIS baseline (top). We also visualize the difference maps (bottom). \textbf{Best viewed by zooming in.}}
	\label{fig:yt_vis_2019_comparison}
\end{figure*}

\begin{figure*}[t!]
	\centering
	\includegraphics[width=0.85\linewidth]{./figs/more_vis_results_vip_seg.pdf}
	\caption{More Visual Results from Tube-Link with ResNet50 backbone. Our method (top) achieves consistent segmentation and better tracking results than the Video K-Net+ baseline (bottom). \textbf{Best viewed by zooming in.}}
	\label{fig:more_vis_vip_seg}
\end{figure*}


\begin{figure*}[t!]
	\centering
	\includegraphics[width=0.85\linewidth]{./figs/tb_sup_kitti_step.pdf}
	\caption{Visual Results from Tube-Link with ResNet50 backbone on the KITTI-STEP dataset. \textbf{Best viewed by zooming in.}}
	\label{fig:more_vis_kitti_step}
\end{figure*}


\begin{figure*}[t!]
	\centering
	\includegraphics[width=0.85\linewidth]{./figs/FailureCases.pdf}
	\caption{Visual Results on Failure Cases of Tube-Link. (a), Remote objects lead to ID switches and inferior segmentation results. (b), Heavy occlusion leads to an ID switch. (c), Segmentation consistency problems caused by camera motion.}
	\label{fig:failure_cases}
\end{figure*}

\begin{table}[!t]
        \centering
        \footnotesize
        \caption{Effect Of Quasi-Dense Tracker (Results on Youtube-VIS-2019 validation set).}
        \label{tab:effect_of_quasi_dense_tracker}
        \scalebox{0.80}{
        \setlength{\tabcolsep}{2.5mm}{\begin{tabular}{c c c c} 
            \toprule[0.1em]
            Method & Naive Tracker & Quani-Dense Tracker &  mAP \\
                  \midrule[0.1em]
            MinVIS  &  \checkmark & -  & 47.4 \\
            MinVIS  &  - & \checkmark  & 48.0 (+0.6) \\
            MiniVIS + tube matching &  \checkmark & - & 48.8 (+1.4) \\
            \hline
            Tube-Link &  \checkmark & - &  52.6 (-0.2) \\
            Tube-Link &  - & \checkmark & 52.8 \\
            \bottomrule[0.1em]
        \end{tabular}}}
\end{table}

\noindent
\textbf{Impact of Quasi-Dense Tracker.}
We adopt the same quasi-dense tracker for all experiments in the main paper, and we can achieve 3.0\% VPQ improvement upon the baseline. In Tab.~\ref{tab:effect_of_quasi_dense_tracker}, we perform an extra experiment by replacing our tracker with a naive tracker used in MinVIS, where we only found 0.2\% mAP drop. This proves the robustness and generalizability of Tube-Link. In contrast, we add the quasi-dense tracker to MinVIS, and we only find 0.6\% mAP improvements. Directly extending a method with tube matching leads to more improvements. The results also indicate that the effect of the tracker is not apparent on the Youtube-VIS dataset, since the instance number is limited and occlusion is not heavy. Thus, we adopt \textit{simple tube-matching} for VIS datasets. 

\vspace{2mm}
\noindent
\textbf{Detailed Results Youtube-VIS.} In Tab.~\ref{tab:ytvis_supp}, we report the detailed results on Youtube-VIS-2019 and Youtube-VIS-2021 datasets. We follow the baseline method, Mask2Former-VIS~\cite{cheng2021mask2former_vis}. 
As shown in that table, our method achieves all the best metrics on both datasets \textit{without COCO video joint training or clip-wised copy-paste}. 


\noindent
\textbf{More Results on Test Set.}. Moreover, we also report our results on the KITTI-STEP test set. As shown in Tab.~\ref{tab:test_dev_kitti_step}, Our method can still achieve better results. 


\vspace{2mm}
\noindent
\textbf{Detailed Results on OVIS.} In Tab.~\ref{tab:ovis}, we also report our model results on OVIS. Again, without bells and whistles, our method achieves comparable results with IDOL. We use the ResNet50 backbone for a fair comparison.









\subsection{Visual Results}
\label{sec:vis_results}

\noindent
\textbf{Visual Comparison on Youtube-VIS-2019 dataset.} In Fig.~\ref{fig:yt_vis_2019_comparison}, we compare our Tube-Link with strong baseline Mask2Former-VIS with the same ResNet50 backbone. Our methods achieve more consistent tracking and segmentation results in two examples.


\noindent
\textbf{More Visual Results on VIP-Seg Dataset.} In Fig.~\ref{fig:more_vis_vip_seg}, we present more visual examples on our Tube-Link. Compared with the Video K-Net+ baseline, our method achieves better segmentation and tracking consistency. 


\noindent
\textbf{Visual Results on KITTI-STEP Dataset.} In Fig.~\ref{fig:more_vis_kitti_step}, we present visual results on the KITTI-STEP dataset, where we achieve consistent segmentation and tracking on the driving scene. 



\noindent
\textbf{Failure Cases Analysis.} In Fig.~\ref{fig:failure_cases}, we show several failure cases on the KITTI-STEP and VIP-Seg datasets using our best models. We observe three error sources: (1). remote and small objects. (2). heavy occlusion. (3). segmentation consistency caused by camera motion. We will handle these issues in future work.
