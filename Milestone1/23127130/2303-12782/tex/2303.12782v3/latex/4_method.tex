\section{Methodology}

\subsection{Preliminary and Motivation}

In this subsection, we begin by introducing a unified notation for universal video segmentation (VSS, VIS, and VPS), and then demonstrate how this task can be modeled through linking the tracked short tube masks. 
%
After that, we further motivate tube-wise matching by comparing it against conventional frame-wise matching. 

\noindent
\textbf{Universal Video Segmentation Formulation.} We denote a video clip input as $ V \in \mathbb{R}^{T\times H\times {W}\times 3}$, where $T$ represents the frame number and ${H}\times {W}$ are the spatial size.
%
The video clip is annotated with segmentation masks. The masks of a particular entity can be linked along the time dimension to form a tube. 
%
The annotations are denoted as $\{y_i\}_{i=1}^G = \{(m_i, c_i)\}_{i=1}^G \,$, where the $G$ is the number of ground truth, each tube mask $m_i \in {\{0,1\}}^{{T}\times {H}\times {W}}$ does not overlap with each other, and $c_i$ denotes the ground truth class label of the i-th tube. The background is assigned with value 0, and the foreground masks are assigned to be 1 in each tube mask $m_i$. 
%
VPS requires temporally consistent segmentation and tracking results for each pixel. 
% non-overlapping
Specifically, a model makes predictions on a set of video clips $\{\hy_i\}_{i=1}^N = \{(\hatm_i, \hp_i(c))\}_{i=1}^N$, where $\hatm_i \in {[0,1]}^{T\times H\times W}$ denotes the predicted tube, and $\hp_i(c)$ denotes the probability of assigning class $c$ to a clip $\hatm_i$ belonging to a predefined category in a set $C$. 
%
The number of entities is given by $N$, which includes countable thing classes and countless stuff classes. 
%
In particular, $i$ is the tracking ID for the thing class. 
%
When $N=C$ and $C$ only contain stuff classes, VPS turns into VSS. 
%
If ${\{\hy_i\}_{i=1}^N}$ can overlap and $C$ only contains the thing classes, VPS turns into VIS. 
%
We use such notations for universal video segmentation.


\noindent
\textbf{Video Segmentation as Linking Short Tubes.} 
To segment a video clip $V$, we divide it into a set of $L$ smaller subclips: ${\{v_{i}\}}_{i=1}^N$, where $v_{i} \in \mathbb{R}^{n\times H\times {W}\times 3}$, and $ n = T / L$ with $n$ representing the window size along the temporal dimension. 
%
The window size is flexible and can be adjusted according to the dataset. 
%
By taking subclips as inputs, we obtain shorter segmentation tubes than the whole clip. We perform tracking and linking across nearby tubes. Each predicted tube is represented as $\{\hat{y}_{i}^{t}\}_{i=1}^N = \{(\hatm_i^{t}, \hp_i(c)^{t})\}_{i=1}^N$, where $t$ is the index of each small tube, $\hatm_i^{t} \in {[0,1]}^{n\times H\times W}$ represents the segmentation masks, and $\hp_i(c)^{t}$ is the corresponding category. 
%
The final video prediction is obtained by linking each tube, $\{\hy_i\}_{i=1}^N = {\mathrm{Link}}({\{\hat{y}_{i}^{t}\}_{i=1}^N})_{t=1,..L}$. 
%
In our formulation, tracking is only performed across different tubes, and the segmentation within each tube is assumed to be consistent. Note that the key to achieving temporally consistent segmentation is the design of function $\rm{Link}$.

\begin{table}[!t]
	\centering
	\caption{\small \textbf{Exploration experiment on tube-wise matching.} Youtube-VIS: mAP. VIP-Seg:VPQ. We directly use pre-trained models by changing the input to two consecutive frames. }
	\label{tab:toy_exp}
  \scalebox{0.62}{
    \begin{tabular}{ r c c c}
    \toprule[0.15em]
     Method & Youtube-VIS-2019 & Youtub-VIS-2021 & VIP-Seg \\
    \toprule[0.15em]
    Min-VIS~\cite{huang2022minvis} & 47.4 &  44.2 & -\\
    Min-VIS + tube matching & 48.8 (+1.4) & 45.5 (+1.3)& - \\
    Video K-Net~\cite{li2022videoknet} & - & - & 26.1 \\
    Video K-Net + tube matching & - & - & 27.6 (+1.5) \\
    \bottomrule[0.2em]
    \end{tabular}
}
\end{table}


\noindent
\textbf{Motivation of Tube-wise Matching.} 
Existing video segmentation methods~\cite{IDOL,huang2022minvis,li2022videoknet} often perform instance association via frame-wise matching. This approach ignores local temporal information and can lead to occlusion errors. In this study, we propose to perform tube-wise matching using global queries based on their corresponding trackers. To motivate our approach, we modify the input of two representative works, Min-VIS~\cite{huang2022minvis} and Video K-Net~\cite{li2022videoknet}, by replacing the single frame input with a subclip input. Each subclip contains two frames. Without additional re-training or computation costs, we observe consistent improvements on three video segmentation datasets and two video segmentation tasks, VIS and VPS, as shown in Table~\ref{tab:toy_exp}. These findings suggest that cross-tube information is worth exploring for achieving universal segmentation. Therefore, we propose to model the $\rm{Link}$ function as tube-wise matching and linking, and term our framework Tube-Link.

\begin{figure}[t!]
	\centering
	\includegraphics[width=1.0\linewidth]{./figs/tb_method_01.png}
	\caption{\small Our proposed Tube-Link framework. Given the subclips as input, we first use global queries to perform within-tube spatial-temporal learning to obtain the tube masks and labels. Then we link the object queries with cross-tube contrastive learning and self-attention.} 
	\label{fig:method}
    \vspace{-5mm}
\end{figure}


\subsection{Tube-Link Framework}
\label{sec:tb_framework}

We first introduce our extension to Mask2Former-VIS, which will serve as a baseline.
%
Then, we present two improvements, including cross-tube Temporal Contrastive Learning (TCL) and Cross-Tube Linking (CTL), to better model cross-tube relations. 
%
The training process of our method is illustrated in Fig.~\ref{fig:method}.


\noindent
\textbf{Mask2Former-VIS Extension as Baseline.} Following~\cite{VIS_TR,cheng2021mask2former_vis}, given a subclip $v_{i}$, we first employ Mask2Former-VIS~\cite{cheng2021mask2former_vis} to extract the spatial-temporal feature $\mathbf{F}_{i} \in \mathbb{R}^{n \times C\times H\times {W}}$. 
%
% Although Mask2Former contains multiscale features, for notation brevity, we use the feature with the highest resolution for formulation purposes. 
Mask2Former uses multiscale features and a cascaded decoder to perform cross-attention. Thus, we denote the layer index $l$ to indicate the layer number.
%
The \textit{global queries}, $\mathbf{Q}_{l-1} \in \mathbb{R}^{N_{q} \times C}$, perform masked cross-attention between $\mathbf{F}_{i}$ and $\mathbf{Q}_{i}$ as follows,
\begin{align}
    \label{equ:sp_attention}
    \mathbf{Q}_{l} = \mathrm{softmax}(\mathbf{M}_{l-1} + \mathrm{MLP}(\mathbf{Q}_{l-1})\mathbf{K}_{l}^{T})\mathbf{V}_{l} + \mathbf{Q}_{l-1},
\end{align}
where $N_{q}$ is the query number and is set to 100 by default, $\mathbf{M}_{l-1} \in \mathbb{R}^{n \times H\times {W}} $ is the binarized output of the resized tube-level mask prediction from the previous stage, following Cheng~\etal~\cite{cheng2021mask2former}. 
%
Instead of considering only thing masks as in previous works~\cite{VIS_TR,seqformer}, we jointly process both thing and stuff masks, shown in Equation~\eqref{equ:sp_attention}. 
%
$\rm{MLP}$ denotes linear layers to transform the object query, while $\mathbf{K}_{l}$ and $\mathbf{V}_{l}$ represent the spatial-temporal features transformed from $\mathbf{F}_{i}$, where $\mathbf{K}_{l}=\mathrm{Key}(\mathbf{F}_{i})$ and $\mathbf{V}_{l}=\mathrm{Value}(\mathbf{F}_{i})$, $\mathrm{Key}$ and $\mathrm{Value}$ are linear functions as in the common attention design. In the implementation, $F_{i}$ is sampled from the multi-scale feature output following Mask2Former, and we use the feature with the highest resolution for simple formulation purposes.
%

Within each tube, the query index is naturally the tracking ID for each object with no tracking association within the tube. 
This process is shown in the dash box area 
 of the Fig.~\ref{fig:method}.  



\begin{figure}[t!]
	\centering
	\includegraphics[width=1.0\linewidth]{./figs/tb_method_02.pdf}
	\caption{\small Illustration of the proposed cross-tube temporal contrastive learning. The input queries are first sent to the $\mathrm{Emb}$. Then cross-tube contrastive learning is performed. The same instance across different frames is indicated in the same color. We use spatial-temporal ground truth masks to perform positive/negative assignments to each query embedding.}
	\label{fig:method_cl}
\end{figure}

\noindent
\textbf{Cross-Tube Temporal Contrastive Learning.} Recent studies~\cite{li2022videoknet,IDOL,qdtrack} have demonstrated the effectiveness of contrastive learning in video segmentation. However, all of these studies perform learning on \textit{only two adjacent frames}, which is not suitable for tube-level matching. To capture a larger temporal context, we propose cross-tube temporal contrastive learning. 
%
While temporal contrastive learning is not new, our study is the first attempt to perform contrastive learning at the tube level.

As shown in Fig.~\ref{fig:method}, given a pair of subclips $v_{i}$ and $v_{j}$ as inputs, we first perform tube-level inference to obtain the global queries, $\mathbf{Q}_{i}$ and $\mathbf{Q}_{j}$, corresponding to two tubes. 
%
Note that both queries already encapsulate spatial-temporal information through Equation~\eqref{equ:sp_attention}. We randomly select two subclips from the neighborhood of all subclips, assuming that they contain corresponding objects (i.e., objects with the same IDs). We then add an extra lightweight embedding head $\mathrm{Emb}$ after each global query to learn the association embedding, which is implemented through several fully-connected layers. Following Li~\etal~\cite{li2022videoknet}, we use a mask-based assignment for contrastive learning.

Different from previous frame-wise methods, we propose a tube-wise label assignment strategy to form contrastive targets. 
%
Recall that our query embedding encodes information from more than one single frame, we use spatial-temporal masks (the same instance masks within the tube) for mask-based assignment, as shown in Fig.~\ref{fig:method_cl}. 
%
Specifically, we define a query embedding as positive to one object if its corresponding \textit{tube mask} has an IoU higher than $\alpha_1$, and negative if the IoU is lower than $\alpha_2$. We set $\alpha_1$ and $\alpha_2$ as 0.7 and 0.3, respectively. 
%
We use a sparse set of matched global queries for learning, where the query indices are assigned from ground truth tube masks.

%We found that this strategy reduces noise caused by unmatched masks. 
%
We assume that there are $X$ matched queries from $\mathbf{Q}_{i}$ and $Y$ matched queries from $\mathbf{Q}_{j}$ as contrastive targets, where both $X$ and $Y$ are \textit{much fewer than} all queries $N$. %Queries are sampled from $\mathbf{Q}_{i}$ and $\mathbf{Q}_{j}$, respectively, as illustrated in Fig.~\ref{fig:method}. 
%
The cross-tube temporal contrastive loss is written as:
%
\begin{align}
    \label{equ:qd_loss}
    L_\text{track} & = -\sum_{\textbf{y}^{+}}\text{log}
    \frac{\text{exp}(\textbf{x} \cdot \textbf{y}^{+})}
    {\text{exp}(\textbf{x} \cdot \textbf{y}^{+}) + \sum_{\textbf{y}^{-}}\text{exp}(\textbf{x} \cdot \textbf{y}^{-})},
\end{align}
where $\textbf{x}$, $\textbf{y}^{+}$, $\textbf{y}^{-}$ are query embeddings of tube pairs, their positive targets, and negative targets, which are sampled from $\mathbf{Q}_{i}$ and $\mathbf{Q}_{j}$, respectively, as illustrated in Fig.~\ref{fig:method}. 
%
The loss pulls positive embeddings close to each other and pushes the negative away, as shown in Fig.~\ref{fig:method_cl}. In addition, following previous work~\cite{qdtrack,li2022videoknet}, we also adopt L2 loss as an auxiliary loss to regularize the global query association process.

\begin{equation}
    \label{equ:qd_loss_aux}
    L_\text{track\_aux} = (\frac{\textbf{x} \cdot \textbf{y}}{||\textbf{x}|| \cdot ||\textbf{y}||} - b)^2,
\end{equation}
where $b$ is 1 if there is a match between the two samples, and 0 otherwise. Compared with previous works~\cite{li2022videoknet, IDOL} for contrastive learning, our loss considers additional temporal information, thus achieving a much better result. Experiments are reported in Sec.~\ref{sec:ablation}.

\noindent
\textbf{Cross-Tube Linking.} Apart from the improved supervision at the tube level, we take a further step to link tubes via their global queries, $\textbf{Q}_{i}$ and $\textbf{Q}_{j}$ for both training and inference. This encourages the interactions among tubes along the temporal dimension. 
%
We adopt a Multi-Head Self Attention ($\mathrm{MHSA}$) layer with a Feed Forward Network ($\mathrm{FFN}$)~\cite{vaswani2017attention} to learn the correspondence among each query to obtain the updated queries, allowing full correlation among queries. 
%
This process is shown as follows: 
\begin{equation}
    \mathbf{Q}_{j}^{f} = \mathrm{FFN}(\mathrm{MHSA}(\mathrm{Query}(\mathbf{Q}_{j}),\mathrm{Key}(\mathbf{Q}_{i}),\mathrm{Value}(\mathbf{Q}_{i})).
 \label{equ:selfattention}
\end{equation}
In this way, the information from the $i$-th tube is propagated to the $j$-th tube via the affinity matrix. 
%
The linked output $\mathbf{Q}_j^{f}$ is employed as the input to the embedding head $\mathrm{Emb}$, shown in the \textit{middle} of Fig.~\ref{fig:method} and the left of the Fig.~\ref{fig:method_cl}.



\begin{table}[!t]
	\centering
	\caption{\small System-level comparison between Tube-Link with related approaches~\cite{li2022videoknet,kim2022tubeformer,IDOL}.}
	\label{tab:comparison}
  \scalebox{0.62}{
    \begin{tabular}{ r c c c c}
    \toprule[0.15em]
     Property and Settings & Video K-Net~\cite{li2022videoknet} & TubeFormer~\cite{kim2022tubeformer} & IDOL~\cite{IDOL} & Our Tube-Link \\
    \toprule[0.15em]
    Online & \checkmark & \checkmark & \checkmark & \checkmark (n=1) \\
    Nearly Online &  & \checkmark &   &  \checkmark \\ 
    \hline
     VIS  & \checkmark & \checkmark & \checkmark & \checkmark \\
     VSS  & \checkmark &  \checkmark &  & \checkmark\\
     VPS  &  \checkmark & \checkmark &  & \checkmark \\
     \hline
     Mulitple Frames & \checkmark & &  \checkmark & \checkmark \\
     Frame Query Matching &  \checkmark & & \checkmark &\\
     Mask Mathicing  &  & \checkmark &  & \\
     Global Query Matching &  &  &  & \checkmark \\
    \bottomrule[0.2em]
    \end{tabular}
}
\end{table}

\noindent
\textbf{Relation with Previous Works.} 
%
We summarize the differences and properties of Tube-Link with previous methods in Table~\ref{tab:comparison}. 
%
Compared to Tubeformer~\cite{kim2022tubeformer}, Tube-Link explores cross-tube relationships from a global query matching perspective, while Tubeformer adopts simple mask matching. 
%
A detailed experimental comparison can be found in Sec.~\ref{sec:exp}. 
%
Compared to IDOL~\cite{IDOL} and Video K-Net~\cite{li2022videoknet}, our method explores multiple-frame information and global query matching, which is more robust for complex scenes and temporal consistency. 
%
If $n=1$, Tube-Link degenerates into the na\"{i}ve online method. 
%
It is noteworthy that since we process the video by sampling $n$ frames as input, we can obtain a much faster inference speed than online approaches via more efficient GPU memory usage and parallelization (see Sec.~\ref{sec:vis_analysis}).

\subsection{Training and Inference of Tube-Link}
\label{sec:train_and_inference}
\noindent
\textbf{Training and Loss Function.} 
In addition to the tracking loss in Equation~\eqref{equ:qd_loss}, we also use tube-wise segmentation loss. 
%
Specifically, we obtain the tube masks by stacking the mask of the same instance from different frames.
%
This establishes a one-to-one mapping between the predicted tube-level mask and the ground-truth tube-level mask based on the masked-based matching cost~\cite{detr,cheng2021mask2former}.
%
The tube-level masks are obtained from global queries $\mathbf{Q}$ like Mask2Former~\cite{cheng2021mask2former}. The final loss function is given as $L = \lambda_{cls}L_{t\_cls} + \lambda_{ce}L_{t\_ce} + \lambda_{dice}L_{t\_dice} + \lambda_{track}L_{track} + \lambda_{aux}L_{track\_aux}$. Here, $L_{t\_ce}$ is the Cross Entropy (CE) loss for tube-level classification, and $L_{t\_ce}$ and $L_{t\_dice}$ are tube-level mask Cross Entropy (CE) loss and Dice loss~\cite{dice_loss,wang2019solo} for segmentation, respectively. 
%
$L_{track}$ and $L_{track\_aux}$ are the tracking losses.  For VSS, we remove the $L_{track}$ term.

\noindent
\textbf{Simplicity.} We intend to keep a simple framework, and thus we \textbf{\textit{do not}} use any extra tricks or auxiliary losses employed in previous works, such as extra semantic segmentation loss~\cite{wang2020maxDeeplab}, tube-level mask copy and paste~\cite{kim2022tubeformer}, joint image dataset and video dataset pre-training~\cite{seqformer, IDOL}, etc.

\noindent
\textbf{Inference.} 
Taking VPS as an example, we use Tube-Link to generate tube-level panoptic segmentation masks from each input. We use the query embeddings from the learned embedding head $\mathrm{Emb}$ as association features and feed them as input to Quasi-Dense Tracker~\cite{qdtrack} in a near-online manner. Note that we only track the preserved instance masks from the panoptic segmentation maps, and the matching process is performed at \textit{tube-level} between two global queries. Unlike previous studies~\cite{deepsort, wangUnitrack}, we do not use extra motion cues to help track across subclips. Instead, we only use simple query feature matching across subclips, which saves time for track matching compared to online methods. An advantage of Tube-Link is its flexible inference via different subclip size settings $n$. Enlarging $n$ can improve both inference speed and performance on most datasets. For complex and high-resolution inputs, we decrease $n$ by only utilizing information within the temporal vicinity. We perform detailed ablations in Sec.~\ref{sec:ablation}. We will provide more details on inference for other tasks (VIS, VSS) in the appendix.



