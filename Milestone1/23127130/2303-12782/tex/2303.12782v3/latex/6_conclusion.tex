\section{Conclusion}
\label{sec:conclusion}

We present Tube-Link, a simple yet flexible universal video segmentation framework. Our key insight is to perform cross-tube matching rather than cross-frame matching. By equipping the Mask2Former architecture with the proposed cross-tube learning, Tube-Link achieves new state-of-the-art results in all three major video segmentation tasks (VSS, VIS, VPS) using one unified architecture.

\noindent
\textbf{Limitation and Future Work.}
Tube-Link is pre-trained on image datasets and requires re-training for each new video dataset. In the future, we hope to develop a model that can be trained only once to unify universal image and video segmentation tasks. One potential solution can be training our Tube-Link with the merged image and video datasets using CLIP~\cite{radford2021learning,wu2023open} text embedding to unify labels. Then, we can build a universal image/video model for various scenes.

\noindent
\textbf{Broader Impact.} Our work pushes the boundary of video panoptic segmentation algorithms in a simple, flexible, and efficient way. The proposed framework provides a unified and general solution for dense video segmentation, which has the potential to greatly simplify and expedite model development in various real-world applications that heavily rely on video input, including autonomous driving and robot navigation.


\noindent
\textbf{Acknowledgement.} This study is supported under the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). It is also supported by Singapore MOE AcRF Tier 1 (RG16/21).
