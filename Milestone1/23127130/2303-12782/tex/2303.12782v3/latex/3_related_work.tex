\section{Related Work}
\label{sec:related_work}
%% new logic:
% Specialized Video Segmentation.
% Universal Video Segmentation Architectures.
% Video Object Detection and Tracking.


\noindent
\textbf{Specialized Video Segmentation.} VSS aims to predict a class label for each pixel in a video. 
%
Recent approaches~\cite{shelhamer2016clockwork,DFF,miao2021vspw,sun2022vss,sun2022mining} model the temporal consistency or acceleration using methods such as optical flow warping or spatial-temporal attention. 
%
VIS~\cite{vis_dataset} extends instance segmentation into video, aiming to segment and track each object simultaneously. Several methods~\cite{mask_pro_vis,lin2021video, zhu2022instance, qin2022graph,li2021improving} link instance-wise features in the video. 
%
Recent studies~\cite{VIS_TR,hwang2021video,seqformer,IDOL,TeViT,MeViS} have extended DETR into VIS, proposing better ways to fuse different queries along the temporal dimension. 
%
However, these methods cannot be directly transferred to complex scenes~\cite{STEP,miao2022large} due to the limited instances and simpler scenes used in their training. 
%
VPS aims to generate instance tracking IDs and panoptic segmentation results across video clips. Kim~\etal~\cite{kim2020vps} mainly focus on short-term tracks, using only six frames for each clip in Cityscapes video sequences. 
%
STEP~\cite{STEP} proposes a Segmentation and Tracking Quality (STQ) metric that decouples the segmentation and tracking error, along with long sequence VPS datasets. 
%
Recently, VIP-Seg~\cite{miao2022large} proposed a more challenging dataset containing various scenes, scales, instances, and clip lengths. However, current solutions~\cite{kim2020vps,woo2021learning_associate_vps,li2022videoknet} for VPS mainly focus on online or near-online approaches, which have difficulty adapting to general tasks (VSS, VIS) or handling the complex scenarios in VIP-Seg dataset.



\noindent
\textbf{Universal Architectures For Segmentation.} 
Recent studies~\cite{wang2020maxDeeplab,zhang2021knet,cheng2021mask2former,yu2022kmaxdeeplab,panopticpartformer,yuan2021polyphonicformer} adopt mask classification architectures with an end-to-end set prediction objective for universal segmentation, achieving better results than specialized models. 
%
In video segmentation, two representative works are Video K-Net~\cite{li2022videoknet} and TubeFormer~\cite{kim2022tubeformer}. The former unifies the video segmentation pipeline via kernel tracking and linking, while the latter adopts a near-online approach and obtains tube-level masks with cross-attention along the tube features and queries. 
%
However, both works fail to replace specialized models, as their performance on specific tasks or datasets is still worse than the best-specialized architecture (they perform worse in VIS). 
%
Our proposed Tube-Link is the first generalized architecture that outperforms existing state-of-the-art specialized architectures on three video segmentation tasks. 
%
In comparison to~\cite{kim2022tubeformer}, Tube-Link further explores cross-tube association, which is critical in long and complex scenes. 


\noindent
\textbf{Video Object Detection and Tracking.}
Object tracking is a crucial task in VPS, and many works adopt the tracking-by-detection paradigm~\cite{bewley2016simple,leal2016learning,xu2019spatial,zhu2018online,porzi2020learning}. 
%
These methods divide the task into two sub-tasks, where objects are first detected by an object detector and then associated using a tracking algorithm. 
%
Recent works~\cite{park2022per,MOSE} also perform clip-wise segmentation or tracking~\cite{cliptrack}. However, the former only focuses on single-object mask tracking, while the latter considers global tracking via clip-level matching. 
%
Our proposed Tube-Link naturally handles both settings yet provides additional segmentation masks as outputs.
%
In Video Object Detection, the literature~\cite{zhu17fgfa,deng19rdn,chen18stlattice,xiao18stmn,zhou2022transvod} has seen the broad usage of information across multiple frames. 
%
Our work is related to TransVOD~\cite{zhou2022transvod}, which uses a local temporal window. However, in these studies, learning correspondences and links across tubes are not explored, partly due to the nature of ImageNet-VID dataset~\cite{russakovsky2015imagenet}, which does not require object tracking and segmentation.
