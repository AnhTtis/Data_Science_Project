\section{Experiments}
\label{sec:exp}
% logic:
% 1, Experiment Settings 

% 2, Benchmark results 
    % 1, VIP-Seg
    % 2, VSPW
    % 3, KITTI-STEP

% 3, Ablation studies and analysis. 
    % 1, improvements on baseline 
    % 2, design choices of temporal contarstive loss 
    % 3, design choices of label assigin stragety
    % 4, Effect of tube frames choices for CS loss 
    % 5, Effect of large window size / overlap inference. 
    % 6, Comparison with the different tracking choices. 
    % 7, increased GFLops/Parameters analysis. 
    % 8. FPS/Window Cruves.

% 4, visualization results. 
    % 1, comparison with strong baseline. 
    % 2, attention mask arcoss differnt tube. 
    

% Due to the unavailability of the test set, we report the results on the \textit{validation set}. 

% The former mainly focuses on mask proposal level as PQ~\cite{kirillov2019panoptic} with different window sizes, while the latter emphasizes pixel-level segmentation and tracking without any thresholds.
% KITTI-STEP has 21 and 29 sequences for training and testing, respectively. The training sequences are split into a training set (12 sequences) and a validation set (9 sequences).

\subsection{Experimental Settings}
\noindent
\textbf{Dataset.} We conduct experiments on five video datasets: VIPSeg~\cite{miao2022large}, VSPW~\cite{miao2021vspw}, KITTI-STEP~\cite{STEP}, and YouTube-VIS-19/21~\cite{vis_dataset}. We mainly conduct experiments on VIPSeg due to its scene diversity and long-length clips. The training, validation, and test sets of VIPSeg contain 2,806/343/387 videos with 66,767/8,255/9,728 frames, respectively. Although VSPW and VIPSeg share the same video clips, the training details are different since they are different tasks. Please refer to the \textit{supplementary material} for other datasets.


\noindent
\textbf{Evaluation Metrics.} For the VPS task, we adopt two metrics: $VPQ$~\cite{kim2020vps} and $STQ$~\cite{STEP}. The metric $STQ$ contains geometric mean of two items: Segmentation Quality ($SQ$) and Association Quality ($AQ$), where $ STQ = (SQ \times AQ)^{\frac{1}{2}}$. The former evaluates the pixel-level tracking, while the latter evaluates the pixel-level segmentation results in a video clip. For the VSS task, the Mean Intersection over Union (\textit{mIoU}) and mean Video Consistency ($mVC$)~\cite{miao2021vspw} are used for reference. For the VIS task, \textit{mAP} is adopted.


\noindent
\textbf{Implementation Details and Baselines.} We implement our models in PyTorch~\cite{pytorch_paper} with the MMDetection toolbox~\cite{chen2019mmdetection}. We use the distributed training framework with 16 V100 GPUs. Each mini-batch has one image per GPU. Following previous work, we use the image baseline pre-trained on COCO dataset~\cite{coco_dataset}. ResNet~\cite{resnet}, STDC~\cite{STDCNet}, and Swin Transformer~\cite{liu2021swin} are adopted as the backbone networks, which are pre-trained on ImageNet, and the remaining layers adopt the Xavier initialization~\cite{xavier_init}. 
For the detailed settings of other datasets, pretraining, and fine-tuning, please refer to the \textit{supplementary material}. To further verify the effectiveness of our approach, we build a stronger baseline by unifying Video K-Net with Mask2Former, where we replace the image encoder with Mask2Former. We term it Video K-Net+. We denote the extended Mask2Former-VIS for VPS as Mask2Former-VIS+.


%%%%%%%% VIP-SEG %%%%%%%%%%%
\begin{table}[!t]
	\centering
	\caption{\small \textbf{Results on VIPSeg-VPS~\cite{miao2022large} validation dataset.} We report VPQ and STQ for reference. Following Miao~\etal~\cite{miao2022large}, we report VPQ scores at different window sizes (1, 2, 4, 6). We report the results obtained from either an efficient or a strong backbone for comparison.}
	\label{tab:vipseg_results}
  \scalebox{0.65}{
    \begin{tabular}{ r|c|cccccc}
    \toprule[0.15em]
     Method& backbone & $VPQ^{1}$ & $VPQ^{2}$ & $VPQ^{4}$ & $VPQ^{6}$ & VPQ & STQ \\
    \toprule[0.15em]
    VIP-DeepLab~\cite{ViPDeepLab} & ResNet50 & 18.4 & 16.9 & 14.8 & 13.7 & 16.0 & 22.0 \\
    VPSNet~\cite{kim2020vps} & ResNet50 & 19.9 & 18.1 & 15.8 & 14.5 & 17.0 & 20.8 \\
    SiamTrack~\cite{woo2021learning_associate_vps} & ResNet50 & 20.0 & 18.3 & 16.0 & 14.7 & 17.2 & 21.1 \\
    Clip-PanoFCN~\cite{miao2022large} & ResNet50 & 24.3 & 23.5 & 22.4 & 21.6 & 22.9 & 31.5 \\
    Video K-Net~\cite{li2022videoknet} & ResNet50 & 29.5 & 26.5 & 24.5 & 23.7 & 26.1 & 33.1 \\
    Video K-Net+~\cite{cheng2021mask2former,li2022videoknet} & ResNet50 & 32.1 & 30.5 & 28.5 & 26.7 & 29.1 & 36.6  \\
    Video K-Net~\cite{li2022videoknet} & Swin-base & 43.3 & 40.5 & 38.3 & 37.2 & 39.8 & 46.3 \\
    \hline
    Tube-Link & STDCv1 & 32.1 & 31.3 & 30.1 & 29.1 & 30.6 & 32.0 \\
    Tube-Link & STDCv2 & 33.2  & 31.8 & 30.6 & 29.6  &  31.4 & 32.8 \\
    \hline
    Tube-Link & ResNet50 & 41.2 & 39.5  & 38.0 & 37.0 &  39.2 & 39.5 \\
    Tube-Link & Swin-base & 54.5 & 51.4 & 48.6 & 47.1 & 50.4 & 49.4 \\
    % Tube-Link & Swin-large &  \lxt{wait results} \\
    \bottomrule[0.2em]
    \end{tabular}
}
\end{table}


%%%%%% VIS-Youtube %%%%%%%%%
\begin{table}[t]
  \centering
   \caption{\small \textbf{Results on the YouTube-VIS datasets.} We report the mAP metric. \textdagger~adopt COCO video pseudo labels. Axial means using the extra Axial Attention~\cite{axialDeeplab}. Our method does not apply these techniques for simplicity.}
  \label{tab:ytvis}
  \scalebox{0.68}{
  \begin{tabular}{l c | c  | c }
    \toprule[0.2em]
    Method & Backbone  & YTVIS-2019 & YTVIS-2021 \\
    \toprule[0.2em]
VISTR~\cite{VIS_TR} & ResNet50 & 36.2 & -  \\
TubeFormer~\cite{kim2022tubeformer} & ResNet50 + Aixal & 47.5  & 41.2  \\
IFC~\cite{hwang2021video} & ResNet50 & 42.8 & 36.6 \\
SeqFormer~\cite{seqformer} & ResNet50 & 47.4 & 40.5  \\
Mask2Former-VIS~\cite{cheng2021mask2former_vis}& ResNet50 & 46.4 & 40.6 \\
IDOL~\cite{IDOL} & ResNet50 & 46.4 & 43.9\\
IDOL~\cite{IDOL} \textdagger & ResNet50 & 49.5 & -\\
VITA~\cite{heo2022vita} \textdagger & ResNet50 & 49.8 & 45.7  \\
Min-VIS~\cite{huang2022minvis} &ResNet50& 47.4 & 44.2 \\
% GenVIS~\cite{heo2022generalized} & ResNet50 & 51.3 & 46.3 \\
\hline
Tube-Link & ResNet50 & 52.8 & 47.9  \\% & - \\
\hline
SeqFormer~\cite{seqformer} & Swin-large  & 59.3 & 51.8 \\% & - \\
Mask2Former-VIS~\cite{cheng2021mask2former_vis} & Swin-large &  60.4 & 52.6 \\
IDOL~\cite{IDOL}  & Swin-large  & 61.5 & 56.1 \\ %& 42.6\\
IDOL~\cite{IDOL}  & Swin-large \textdagger  & 64.3 & -\\
VITA~\cite{heo2022vita} \textdagger & Swin-large & 63.0 & 57.5 \\ 
Min-VIS~\cite{huang2022minvis} & Swin-large & 61.6 & 55.3 \\
\hline
Tube-Link & Swin-large  & 64.6 & 58.4  \\
    \bottomrule[0.2em]
  \end{tabular}
}
\end{table}



%%%%%% VSPW and VIP-Seg VSS%%%%%%%%%
\begin{table}[t]
  \centering
    \caption{\small \textbf{Results on VSPW-VSS validation set}. $mVC_{c}$ means that a clip with $c$ frames is used.}
    \label{tab:vspw}
  \scalebox{0.68}{
  \begin{tabular}{l c c c c c }
    \toprule[0.2em]
    \textbf{VPSW} & Backbone & mIoU & $mVC_{8}$ &$mVC_{16}$  \\
    \toprule[0.2em]
    DeepLabv3+~\cite{deeplabv3plus} & ResNet101 & 35.7 & 83.5 & 78.4 \\
    TCB(PSPNet)~\cite{miao2021vspw,zhao2017pyramid} & ResNet101 & 37.5 & 86.9 & 82.1  \\
    Video K-Net (Deeplabv3+)~\cite{li2022videoknet,deeplabv3plus} & ResNet101  & 37.9 & 87.0 & 82.1 \\
    Video K-Net (PSPNet)~\cite{li2022videoknet,zhao2017pyramid} & ResNet101  & 38.0 & 87.2  & 82.3 \\
    MRCFA~\cite{sun2022mining} & MiT-B5 & 49.9 & 90.9  &  87.4  \\
    CFFM~\cite{sun2022vss} & MiT-B5 & 49.3 & 90.8 & 87.1 \\
    TubeFormer~\cite{kim2022tubeformer} & Axial-ResNet50x64  &  63.2 &  92.1 & 88.0 \\
    \hline
    Tube-Link & ResNet50 & 42.3 & 86.8 & 83.2 \\
    Tube-Link & Swin-large & 59.7 & 90.3 & 88.4 \\
    \bottomrule[0.2em]
  \end{tabular}
  }

\end{table}


\begin{table}[t]
  \centering
    \caption{\small \textbf{Results on VIP-Seg-VSS validation set}. $mVC_{c}$ means that a clip with $c$ frames is used.}
    \label{tab:vipseg_vss}
  \scalebox{0.68}{
  \begin{tabular}{l c c c c c }
    \toprule[0.2em]
    \textbf{VPSW} & Backbone & mIoU & $mVC_{8}$ &$mVC_{16}$  \\
    \toprule[0.2em]
    Video K-Net (Deeplabv3+)~\cite{li2022videoknet,deeplabv3plus} & ResNet101  & 38.3 & 88.0 & 83.1 \\
    Video K-Net (PSPNet)~\cite{li2022videoknet,zhao2017pyramid} & ResNet101  & 39.0 & 88.2  & 84.2 \\
    Mask2Former~\cite{cheng2021mask2former} &  ResNet50 & 38.4 & 87.5 & 82.5 \\
    Video K-Net+~\cite{cheng2021mask2former,li2022videoknet} &  Swin-base & 57.2 & 90.1 & 87.8  \\
    \hline
    Tube-Link & ResNet50 & 43.4 & 89.2 & 85.4 \\
    Tube-Link & Swin-base & 62.3 & 91.4 & 89.3 \\
    Tube-Link & Swin-large & 64.9 & 92.4 & 89.9 \\
    \bottomrule[0.2em]
  \end{tabular}
  }

\end{table}


\subsection{Benchmark Results}


%%%%%% KITTI-STEP %%%%%%
\begin{table}[t]
  \centering
   \caption{\small \textbf{Results on the KITTI val set.} OF refers to an optical flow network~\cite{teed2020raft}.}
  \label{tab:kitti_step}
  \scalebox{0.68}{
  \begin{tabular}{l c c || c c c c }
    \toprule[0.2em]
    \textbf{KITTI-STEP} & Backbone & OF & STQ & AQ & SQ & VPQ \\
    \toprule[0.2em]
    P + Mask Propagation & ResNet50 & \checkmark & 0.67 & 0.63 & 0.71 & 0.44 \\
    Motion-Deeplab~\cite{STEP}& ResNet50 &  & 0.58 & 0.51 & 0.67 & 0.40  \\
    VPSNet~\cite{kim2020vps}& ResNet50  & \checkmark & 0.56 & 0.52 & 0.61 & {0.43}  \\
    TubeFormer-DeepLab~\cite{kim2022tubeformer} & ResNet-50 + Axial &  & 0.70 & 0.64 &  0.76 & 0.51 \\
    Video K-Net~\cite{li2022videoknet} & ResNet50 &  & 0.71 & 0.70  & 0.71  &  0.46 \\
    Video K-Net~\cite{li2022videoknet} & Swin-base &  & 0.73 & 0.72 & 0.73 & 0.53 \\
    \hline
    Tube-Link & ResNet50 &  & 0.68 & 0.67 & 0.69 & 0.51 \\
    Tube-Link & Swin-base &  & 0.72 & 0.69 & 0.74 & 0.56 \\
    \bottomrule[0.2em]
  \end{tabular}
  }
  \vspace{-4mm}
\end{table}

% \lxt{will be changed by test set Figure Results Further. This figure will be merged into it as subfigure.}
\begin{figure}[t]
  \centering
   \includegraphics[width=0.80\linewidth]{./figs/teaser_trade_off.pdf}
   \caption{\small Tube-Link also achieves the best accuracy and speed trade-off on VIP-Seg dataset. FPS is measured on RTX GPU.}
   \label{fig:curve_trade_off_vipseg}
\end{figure}

\noindent
\textbf{[VPS] Results on VIPSeg.} 
We present the results of our Tube-Link method compared to previous works on the VIPSeg dataset in Tab.~\ref{tab:vipseg_results}. Our approach outperforms Video K-Net\cite{li2022videoknet} (under the same backbone) with 12\%-15\% VPQ and 7\%-10\% STQ improvements, respectively. Notably, our method with Swin-base~\cite{liu2021swin} backbone achieves new state-of-the-art results. 
%
We also evaluate our method using a lightweight backbone~\cite{STDCNet} for more efficient inference on video clips, and it achieves even better results than all previous methods with a larger ResNet50 backbone. 
%
These results demonstrate the effectiveness of our approach in exploiting temporal information.  Benefiting from the joint inference of subclips, our method achieves a much faster inference speed, as shown in Fig.~\ref{fig:curve_trade_off_vipseg}. 



\begin{table*}[h!]
    \footnotesize
	\centering
	\caption{\small \textbf{Ablation studies and comparative analysis on VIPSeg validation set with the ResNet50 backbone.} 
	}
    \subfloat[Ablation Study on Each Component.]{
    \label{tab:ablation_a}
	    \begin{tabularx}{0.43\textwidth}{c c c c c} 
		        				\toprule[0.15em]
    	baseline  & TCL & CTL & $\mathrm{VPQ_{th}}$ & VPQ \\
        \toprule[0.15em]
            Mask2Former-VIS+ (F) & - & - & 29.4 & 32.4 \\
            \hline
            Mask2Former-VIS+ (T) & - & - & 31.0 & 34.5\\
             & \checkmark & - & 34.6  & 36.8  \\  
          \rowcolor{gray!15}  & \checkmark & \checkmark & 35.1 & 37.5 \\  
        \bottomrule[0.1em]
	    \end{tabularx}
    } \hfill
    \subfloat[Design Choices of TCL.]{
    \label{tab:ablation_b}
		\begin{tabularx}{0.28\textwidth}{c c c} 
			\toprule[0.15em]
			Method & VPQ & STQ \\
			\midrule[0.15em]
            Dense Query~\cite{qdtrack} & 30.2  & 30.1  \\
            Sparse Query~\cite{li2022videoknet} & 34.5  & 35.1 \\
            \rowcolor{gray!15} Global Query(Ours) &  37.5  & 36.5 \\
			\bottomrule[0.1em]
		\end{tabularx}
    } \hfill
    \subfloat[Association Target Assign.]{
    \label{tab:ablation_c}
		\begin{tabularx}{0.24\textwidth}{c c c} 
			\toprule[0.15em]
			Method & VPQ & STQ  \\
			\midrule[0.15em]
			All-Masks~\cite{qdtrack} & 30.1 & 29.2 \\
			GT-Mask~\cite{li2022videoknet} & 35.6 & 35.9 \\
			\rowcolor{gray!15} Tube-Mask & 37.5 & 36.5 \\
			\bottomrule[0.1em]
		\end{tabularx}
    } \hfill
    \vspace{2mm}
    \subfloat[Input Sub-clip Size with Tube Window Size of 2 as Input.]{
     \label{tab:ablation_d}
	    \begin{tabularx}{0.30\textwidth}{c c c c} 
		        				\toprule[0.15em]
    		 Clip Size & STQ & VPQ & $\mathrm{VPQ_{th}}$  \\
    		\toprule[0.15em]
    	    T=1 & 34.5 & 35.6 & 30.2 \\
    	    \rowcolor{gray!15} T=2 & 36.5 & 37.5 & 35.1 \\
    	    T=2(ovl) & 35.9 & 37.3 & 35.0 \\
    	    T=3 &  36.4 & 37.0 & 35.3 \\
        	\bottomrule[0.1em]
	    \end{tabularx}
    } \hfill
    \subfloat[Tube-Window for Inference with Input Sub-clip Size 2 for Training.]{
     \label{tab:ablation_e}
	    \begin{tabularx}{0.30\textwidth}{c  c c c} 
		        				\toprule[0.15em]
    		 Window Size & STQ & VPQ  & $\mathrm{VPQ_{th}}$ \\
    		\toprule[0.15em]
    	    W=2 &  36.5 & 37.5 & 35.1 \\
    	    W=4 &  39.2 & 39.0 & 38.2 \\
    	   \rowcolor{gray!15} W=6 &  39.5 & 39.2 & 38.9 \\
    	    W=8 &  38.3 & 38.5 & 37.3 \\
        	\bottomrule[0.1em]
	    \end{tabularx}
    } \hfill
    \subfloat[Tracking Choices with the Default Setting of Tab.(d). ]{
     \label{tab:ablation_f}
	    \begin{tabularx}{0.35\textwidth}{c c c c} 
		        				\toprule[0.15em]
    		 Settings  &  STQ & VPQ & $\mathrm{VPQ_{th}}$ \\
    		 \toprule[0.15em]
    		  Extra Tracker~\cite{wangUnitrack,deepsort}& 33.9 & 36.6 & 34.1 \\
    		  RoI Features~\cite{qdtrack} & 34.5 & 35.9 & 34.5 \\
    		  Query Embedding~\cite{li2022videoknet}  & 33.1  & 36.0  & 33.0 \\
    	     \rowcolor{gray!15} Our Tube embedding & 36.5 & 37.5 & 35.1\\
        	\bottomrule[0.1em]
	    \end{tabularx}
    } \hfill
\end{table*}


\noindent
\textbf{[VIS] Results on YouTube-VIS-2019/2021.} In Tab.~\ref{tab:ytvis}, we compare our method with state-of-the-art VIS methods on the YouTube-VIS 2019 and 2021 datasets. Our method achieves a 3.0\% and 2.2\% mAP gain over VITA~\cite{heo2022vita} when using the ResNet50 backbone. Furthermore, compared with the Mask2Former-VIS baseline~\cite{cheng2021mask2former_vis}, our method achieves 4-5\% mAP gains on the two datasets with different backbones. Our method also outperforms the previous near-online method TubeFormer~\cite{kim2022tubeformer} by 5-6\% in terms of mAP on the two VIS datasets.


\noindent
\textbf{[VSS] Results on VSPW and VIP-Seg.} We further conduct experiments on VSPW dataset~\cite{miao2021vspw} for VSS to demonstrate the generalization of Tube-Link. As shown in Tab.~\ref{tab:vspw}, our method achieves over 4\% mIoU improvement compared to the Mask2Former baseline. Under the same ResNet101 backbone, our method achieves the best results. Using the Swin base backbone, our method achieves about 3.7\% mIoU gains over Video K-Net+ with consistent improvements on $mVC$. Our method with a lightweight backbone achieves comparable results to DeepLabv3+ with ResNet101, but with about four times faster inference speed (shown in Fig.~\ref{fig:curve}). Without using any additional techniques, our method also outperforms recent methods specifically designed for VSS~\cite{sun2022vss,sun2022mining}. In Tab.~\ref{tab:vipseg_vss}, we also compare the video semantic segmentation methods in recent VIPSeg datasets with higher-resolution images. Compared with previous state-of-the-art methods, our approaches also achieve state-of-the-art results.

% Moreover, compared with the previous state-of-the-art Tubeformer~\cite{kim2022tubeformer}, our method achieves a better 1.7\% mIoU.


\noindent
\textbf{[VPS] Results on KITTI STEP.} 
We further validate our method on KITTI STEP~\cite{STEP} and report the results in Tab.~\ref{tab:kitti_step}. Our method achieves 0.51 VPQ with the ResNet50 backbone, setting a new state-of-the-art result \textit{without} using temporal attention or optical flow warping. When using a strong Swin-base~\cite{liu2021swin} backbone, our method still achieves better results than Video K-Net~\cite{li2022videoknet} by 3\% VPQ and comparable results on STQ. It is worth noting that one can further improve the performance of Tube-Link by employing a better tracker design.

\subsection{Ablation Study and Visual Analysis}
\label{sec:ablation}
% 1, improvements on baseline 
% 2, design choices of temporal contarstive loss 
% 3, design choices of label assigin stragety
% 4, Effect of tube frames choices for CS loss 
% 5, Effect of large window size / overlap inference. 
% 6, Comparison with the different tracking choices. 
% 7, increased GFLops/Parameters analysis. 
% 8. FPS/Window Cruves.

% In this section, we present some \textit{\textbf{key}} ablations on component design and analysis using VIPSeg dataset with ResNet50 backbone. 
%The default setting used in our model is indicated in gray.
%More results are provided in the supplementary material. 

% \cavan{image part? or do you mean feature extractor or encoder}
% \cite{li2022videoknet} as the baseline by replacing its encoder with Mask2Former~\cite{cheng2021mask2former}
\noindent
\textbf{Improvements over Strong VPS Baseline.} 
In Tab.~\ref{tab:ablation_a}, we demonstrate the effectiveness of each component proposed in Sec.~\ref{sec:tb_framework}. 
The first row shows the results of the frame matching baseline. After adopting the tube matching, we obtain a gain of 1.6\% $\mathrm{VPQ_{th}}$ and 2.1\% on VPQ, even without any specific tracking design, which results in the same observation as shown in Tab.~\ref{tab:toy_exp}. Thus, we use Mask2Former-VIS+ (T, T=2) as our baseline by default, which achieves a strong starting point of 34.5 VPQ. $\mathrm{VPQ_{th}}$ refers to the VPQ for the thing class. This result shows the effectiveness of the na\"{i}ve framework. The addition of TCL further boosts performance, with a gain of 3.5\% on $\mathrm{VPQ_{th}}$ and 1.7\% on VPQ. Furthermore, adding CTL, which makes the association more consistent, improves $\mathrm{VPQ_{th}}$ by 1.5\%.


\noindent
\textbf{Ablation on Temporal Contrastive Loss.} We also compare our TCL design with previous works that use dense queries~\cite{qdtrack} or sparse queries~\cite{li2022videoknet} for matching. Both settings use only one frame, while our subclip size is two. As shown in Tab.~\ref{tab:ablation_b}, our method achieves the best results since tube matching encodes more temporal information. In particular, we observe 3.0\% VPQ improvements compared to the strong Video K-Net baseline.


\begin{figure}[t!]
	\centering
	\includegraphics[width=1.0\linewidth]{./figs/tube_link_vis_results_1st.pdf}
	\caption{\small Comparison results on VIP-Seg and YuoTube-VIS. Our method achieves consistent segmentation (shown in orange boxes) and better tracking results (shown in red boxes).}
	\label{fig:visulize}
\end{figure}

% \gl{R-50 and R50 should be consistent. The same as R-101 and R101.}
\begin{figure}[t]
  \centering
   \includegraphics[width=1.\linewidth]{./figs/both.pdf}
   \caption{\small Efficiency Analysis of Tube-Link. Left: Segmentation results (mIoU) of VSPW with different subclip sizes. Right: Inference speed (FPS) with different subclip sizes.}
   \label{fig:curve}
\end{figure}



\noindent
\textbf{Ablation on Association Target Assignment.} 
In Table \ref{tab:ablation_c}, we show the results of the ablation study on building association targets. We find that using a tube-level mask achieves the best results. Using the mask from one of the input subclips leads to inferior results. This is because the ground truth masks of a single frame are not aligned with the input global queries, where the global queries are learned from multiple frames using Equation \eqref{equ:sp_attention}.

\noindent
\textbf{Effect of Sub-clip Size for Training.} 
In Tab.~\ref{tab:ablation_d}, we investigate the impact of subclip size on training. Tube-Link becomes an online method when the subclip size is 1. As shown in the table, enlarging the subclip size improves the performance. We also examine overlapping during sampling, denoted as ovl, where two input subclips overlap at one frame. As shown in Tab.~\ref{tab:ablation_d}, enlarging the subclip size to 2 achieves significant improvement. However, we find that either frame overlapping or using a larger subclip size ($T=3$) does not bring extra gains. Adding more frames does not benefit temporal association learning, since most instances are similar within a subclip. Moreover, using more frames is not memory-friendly during training. Thus, the subclip size is set to 2. We can enlarge the size for inference, as shown in Tab.~\ref{tab:ablation_e}.
  
% \cavan{to what value and why? During training?}
% \cavan{you mean we can set the subcip size to 2 during training and expand it during inference? This point is not clearly articulated here.}
%  \cavan{where? In future work?}
% \cavan{not sure why we use `Moreover', it doesn't connect well to the previous sentence.}
% % Hence, we can enlarge the subclip size for more efficient inference and global consistency within each tube. 

\noindent
\textbf{Effect of Sub-clip Size for Inference.} 
\if 0
The global queries for each tube learn to perform temporal association via cross-attention within each subclip. Despite the subclip size is limited during the training due to the memory issues, we can expand it during the inference.
For example, the subclip size is 2 during training and is set to 6 for inference. As shown in Tab.~\ref{tab:ablation_e}, we prove that enlarging subclip size for inference improves the performance by a significant margin for all three metrics: STQ, VPQ and $\mathrm{VPQ_{th}}$. When the size is 8, the performance drops. This is because the global queries cannot handle larger subclips as the offline method. Besides the effectiveness, increasing subclip size can also lead to faster speed for each clip input due to full utilization of GPU memory, as shown in Fig.~\ref{fig:curve}.
\fi
%
During training, the subclip size is limited due to memory constraints, but we can expand it during inference to improve the performance. For instance, we use a subclip size of 2 during training and increase it to 6 during inference. Tab.~\ref{tab:ablation_e} shows that enlarging the subclip size for inference improves the performance considerably for all three metrics: STQ, VPQ, and $\mathrm{VPQ_{th}}$. However, when the subclip size is further increased to 8, the performance drops because the global queries are not designed to handle larger subclips. Increasing the subclip size can also speed up the inference process by utilizing the full GPU memory, as demonstrated in Fig.~\ref{fig:curve}.

% \cavan{`lead to a higher number of frames leads to faster speed'? Rephrase this sentence.}

\noindent
\textbf{Different Tracking Choices.} 
\if 0
In Tab.~\ref{tab:ablation_f}, we compare different tracking approaches that were used in previous studies~\cite{qdtrack,li2022videoknet,deepsort}. The default Tube Embedding works best in our framework. It does not require any association embedding head or the RoI crop operation on the VIPSeg dataset. Our Tube-Link only uses the learned tube-level embedding for the association.
\fi
In Tab.~\ref{tab:ablation_f}, we compare different tracking approaches used in previous studies~\cite{qdtrack,li2022videoknet,deepsort} with our Tube-Link. Our Tube-Link only uses the learned tube-level embedding for the association. We find that the default tube embedding works best in our framework, without requiring any association embedding head or RoI crop operation on the VIPSeg dataset.  

\subsection{Visualization and More Analysis}
\label{sec:vis_analysis}

\noindent
\textbf{GFLops and Parameter Analysis.} Compared with Mask2Former baseline, we only add one $\mathrm{Emb}$ head and one self-attention layer, introducing only 2.2\% GFLops and 1.4\% extra parameters with $720 \times 1280$ input. 

%\cavan{the font size is too small to be visible. You can use a common legend for both plots and place it underneath the plots}



% \subsection{Visualization and Analysis}
% \cavan{Do we really need this section? The `Speed and Accuracy with different Input Subclip Size' can be merged with `Effect of Sub-clip Size For Inference.' in the ablation study. `Visual Improvements on Baseline' can be merged with `Improvements over Strong VPS Baseline'.}

\noindent
\textbf{Speed and Accuracy with Different Input Subclip Size.} 
As shown in Table \ref{tab:ablation_e}, adding more frames improves the VPS results. To further analyze the speed-accuracy trade-off, we present a detailed comparison of different methods on the VSPW dataset in Fig.~\ref{fig:curve}. The left plot shows that enlarging the subclip size also improves the VSS results. The right plot illustrates that increasing the subclip size improves the single-frame baseline by 1.25-1.5\% for various backbones. Both performance and speed reach a plateau when the size increases to 6. The experiment justifies our choice of using an input subclip size of 6 for inference.

\noindent
\textbf{Visual Improvements on Baselines.} In Fig.~\ref{fig:visulize}, we present the visual comparison with several strong baselines (Video K-Net+ and Mask2Fomer-VIS) in VPS and VIS settings. The results are randomly sampled from a long clip. We achieve better results on both segmentation and tracking. More visual examples can be found in the supplementary material. 
