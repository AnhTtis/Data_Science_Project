TREC Results are made available in Table~\ref{tab:trec}. We drew some initial conclusions and are still analyzing the results:

\begin{itemize}
\item In the end the dev set we used was not that good. For example on the first stage runs, the best run (mono vs AdHoc) was the exact opposite of the devset.
\item The Farsi monolingual reranked run was worse than the not reranked one, kinda the opposite of the dev set.
\item The gap between the baselines and the first stage runs was increased in the TREC set, further showing the advantage of ensembling first-stage rankers.
\item Of the reranked runs, the Adhoc was always better, achieving good results compared to the rest of the runs.
\item The mAP achieved in the Russian TREC set and the Recall@1k for both sets seem to confirm that our models are good at selecting good candidates, but are not as good at pushing them to the top. This is similar to what we observed in TREC DL 21 and 22.
\item Looking at the queries, there is a very large gap on how they are structured and how MSMARCO ones are. This probably impacted most runs of the competition, but it is especially critical on ours, where all runs are based on MSMARCO. The inclusion of training on HC4 is a probable step for next year.
\item Analyzing the hardest queries (c.f. Appendix), it seems that one problem was looking into the ``wrong'' database, by looking at questions for one nation/nationality/language on another. This makes us think of how a full-blown CLIR could happen (instead of English to one language, English to many languages).
\item Given the smaller gap in the effectiveness of mono vs adhoc in Farsi compared to Russian it is not unreasonable to imagine that it comes from the fact that machine translation from English to Farsi are worse than the English to Russian. Studying this is left as future work.

\end{itemize}

\input{tables/trec.tex}

We also include some query analysis in the appendix (quite long so we preferred to keep it outside the ``main'' analysis.