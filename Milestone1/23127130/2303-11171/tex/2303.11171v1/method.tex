In the following, we introduce the models we consider for both candidate generation as well as re-ranking. We also describe our training procedure and detail the submitted runs. SPLADE and pretrained language models are made available at \url{https://huggingface.co/naver/\{name\}} with the models being named: neuclir22-\{pretrained, splade\}-\{fa,ru,zh\}. Note that even if we did not participate in zh, we trained models a posteriori and made them available.


\subsection{First Stage}

For the first stage this year we separate models into two avenues: trained on the target language and on English. For the latter we always use the Splade++ CoCondenser Self Distil\footnote{Available at \url{https://huggingface.co/naver/splade-cocondenser-selfdistil}} and for the former:
\begin{enumerate}
    \item We pre-train 6L DistilBERT base models from scratch using MLM+FLOPS~\cite{pretraining} on the target language using a combination of the target documents, translated MSMARCO~\cite{bonifacio2021mmarco} and Mr.Tydi~\cite{zhang2021mr} depending on their availability for the language.
    \item We then finetune those models with ColBERT~\cite{colbert} and SPLADE~\cite{pp} style training on the translated MSMARCO~\cite{bonifacio2021mmarco}.
    \begin{enumerate}
        \item For SPLADE we use the negatives from \url{https://huggingface.co/datasets/sentence-transformers/msmarco-hard-negatives} drawn from a multitude of models on the English version of the corpus. Note that we tried using distillation with the English scores (which we already had tested on other languages with positive results), but the better results came from non-distillation training. 
        \item While for ColBERT we use the traditional English BM25 negatives. In order to maximize ColBERT performance we remove the last layer (which reduces the dimensionality from 768 to 128) and inference is done using brute-force retrieval instead of an approximate ANN (following ~\cite{lassance2021colbert}).
    \end{enumerate}
    \item For completeness, we also include BM25 in our first stage ensembling. However, in hindsight, we should have used the run provided by the organizers as the last piece of the ensemble.
\end{enumerate} 


\subsection{Second Stage}

As per last year's competition we use a mix of different PLMs as rerankers for which training is inspired by~\footnote{code available at \url{https://github.com/luyug/Reranker}}~\cite{gao2021rethink} and using negatives from SPLADE. For the monolingual runs we used InfoXLM and XLM-Roberta-Large as the PLMs, while for the Adhoc runs we consider only rerankers trained on english, using Electra-Large, Deberta-v3-Large and Deberta-v2-xxLarge\footnote{Which are similar to the ones used on TREC-DL22 and made available on huggingface} (and thus apply them using the back-translated documents). We also add the pretrained MonoT5-3B ~\cite{nogueira2020document}.


\subsection{Ensembling}

We also have applied ensembling in order to improve our results. This year we used ranx~\cite{bassani2022ranx} to generate all our ensembles, using average normalized score over the ensembles, unless explicitly noted. The normalized score uses the min and max values of the query so that, for each model, the best score is 1 and the lowest one 0.

\subsection{Runs submitted to TREC}

We submitted a total of 14 runs, 7 for Farsi and 7 for Russian. Of the 7 runs, we submitted 3 baselines (human query translation, machine query translation, machine document translation) and 4 main runs (combination of monolingual/Adhoc with first-stage only/reranked). Namely:

\begin{enumerate}
    \item splade\_\{language\}\_ht: Baseline monolingual first-stage ranking with SPLADE and human translated queries
    \item splade\_\{language\}\_mt: Baseline Ad-hoc first-stage ranking with SPLADE and machine-translated queries
    \item splade\_\{language\}\_dt: Baseline Ad-hoc first-stage ranking with SPLADE and machine-translated documents
    \item NLE\_\{language\}\_mono: Monolingual ensemble of first-stage rankers on human translated queries
    \item NLE\_\{language\}\_mono\_rr: Monolingual ensemble of first-stage and rerankers on human translated queries
    \item NLE\_\{language\}\_adhoc: Ad-hoc ensemble of first-stage rankers on machine-translated queries and/or machine-back-translated documents
    \item NLE\_\{language\}\_adhoc\_rr: Ad-hoc ensemble of first-stage and rerankers on machine-translated queries and/or machine-back-translated documents\end{enumerate}

