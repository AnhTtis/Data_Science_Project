% Stephane
% Discuss First Stage Ranker, Query Expansion, Document Expansion, docT5
% Document Translation -- Recall
% Paragraph on Reranker etc...
% make a link to computer vision
% Computer Vision, Product Quantization ... 
% The problem of First Stage Retrieval, Document Expansion

% Traditional IR models, such as BM25 \cite{robertson2009probabilistic}, language models \cite{}, DFR \cite{DFR}
% were analyzed thanks to the axiomatic approach to IR \cite{axiomatic} which gave important properties that IR models
% should verify.

%\subsection{BOW models for IR}
%TODO

% \subsection{Neural (re-)ranking}

% In 2013, \textit{Huang et al.} introduced DSSM\cite{dssm}, which akin to standard LSA models, learns latent semantic representations for queries and documents, starting a new era of neural based ranking models for ad-hoc Information Retrieval. In 2016, \textit{Guo et al.} pointed out some limitations of semantic-based retrieval, such as their inability to model exact-matches --a critical component for IR systems-- and advocated for models that take into account fine-grained \emph{interactions} between queries and documents. They introduced the DRMM model \cite{drmm}, based on the so-called interaction matrix, which contains \emph{interactions} between every tokens of the query and the document, by computing cosine similarity between every pair of word embeddings. Histogram-based pooling is then applied to summarize such interactions into relevance matching features. Following works built around and extended this idea \cite{pacrr,local_and_distributed,knrm}. In 2019, \cite{neural_hype} questioned the actual progress of neural information retrieval and pointed that they might be illusory due to flaws in experimental evaluation, raising important concerns in the field.

% \subsection{Pre-trained language models for neural search}
% TODO //
% mention context // 
% mention somewhere: in IR, we focus on learning sparse representations and not sparse models (different goals)


%\subsection{Sparse representations for first stage retrieval}

%In standard two-stage ranking pipelines, due to strict efficiency requirements, the initial retrieval is generally based on BOW models like BM25, as they provide both strong and efficient baselines by design (through the inverted index structure). Re-rankers are then applied on the small set of candidates generated from the first step, allowing to consider more costly but effective models. Much of the effort in designing better ranking systems has been put into the re-ranking phase; thus, the candidate generation --which is not learned-- remains a bottleneck of the pipeline, and recently drew the attention of IR researchers.


% In order to learn efficient first-stage retrievers, two different types of approaches can be considered: building sparse representations for queries and documents, or using dense representations and approximate nearest neighbor methods.

% \paragraph{\bf Dense approaches}
Dense retrieval based on BERT Siamese models~\cite{sentence_bert} has become the standard approach for candidate generation in Question Answering and IR~\cite{guu2020realm,karpukhin2020dense,xiong2021approximate,qu-etal-2021-rocketqa,lin-etal-2021-batch,Hofstaetter2021_tasb_dense_retrieval}. % ~\cite{overview_trec}
While the backbone of these models remains the same, % -- \texttt{[CLS]} or average pooling on top of BERT -- 
recent works highlight the critical aspects of the training strategy to obtain state-of-the-art results, ranging from improved negative sampling~\cite{lin-etal-2021-batch,Hofstaetter2021_tasb_dense_retrieval} to distillation~\cite{hofstatter2020improving,lin-etal-2021-batch}. ColBERT~\cite{colbert} pushes things further: the postponed token-level interactions allow to efficiently apply the model for first-stage retrieval, benefiting of the effectiveness of modeling fine-grained interactions, at the cost of storing embeddings for each (sub)term -- raising concerns about the actual scalability of the approach for large collections. 
To the best of our knowledge, very few studies have discussed the impact of using \emph{approximate} nearest neighbors (ANN) search on IR metrics~\cite{boytsov2018efficient, tu2020approximate}.
Due to the moderate size of the MS MARCO collection, results are usually reported  with an \emph{exact}, brute-force search, therefore giving no indication on the effective computing cost.
%Furthermore, dense representations prevent the \emph{exact matching} mechanism which is essential for retrieval. 


% \paragraph{\bf Sparse approaches} 
An alternative to dense indexes is term-based ones.
Building on standard BOW models, \textit{Zamani et al.} first introduced SNRM~\cite{snrm}: the model embeds documents and queries in a sparse high-dimensional latent space by means of $\ell_1$ regularization on representations.
%One can see each latent dimension as a new \emph{latent term} in a learned vocabulary. 
However, SNRM effectiveness remains limited and its efficiency has been questioned~\cite{paria2020minimizing}. 

Motivated by the success of BERT, there have been attempts to transfer the knowledge from pre-trained LM to sparse approaches. DeepCT~\cite{dai2019contextaware, 10.1145/3366423.3380258, 10.1145/3397271.3401204} focused on learning contextualized term weights in the full vocabulary space -- akin to BOW term weights. %A new index can then be built, based on the learned term weights.
%, and standard IR methods like BM25 and query expansion can be used for retrieval.
However, as the vocabulary associated with a document remains the same, this type of approach does not solve the vocabulary mismatch, as acknowledged by the use of query expansion for retrieval \cite{dai2019contextaware}.
% in DeepCT
A first solution to this problem consists in expanding documents using generative approaches such as doc2query~\cite{nogueira2019document} and doc2query-T5~\cite{doct5} to predict expansion words for documents. The document expansion adds new terms to documents -- hence fighting the vocabulary mismatch -- as well as repeats existing terms, implicitly performing re-weighting by boosting important terms. 

Recently, DeepImpact~\cite{10.1145/3404835.3463030} combined the expansion from doc2query-T5 with the re-weighting from DeepCT to learn term \emph{impacts}. These expansion techniques are however limited by the way they are trained (predicting queries), which is indirect in nature and limit their progress.
%
A second solution to this problem, that has been chosen by recent works~\cite{sparterm2020,MacAvaney_2020,zhao2020sparta,10.1145/3404835.3463098}, is to estimate the importance of each term of the vocabulary \emph{implied by} each term of the document (or query), i.e. to compute an interaction matrix between the document or query tokens and all the tokens from the vocabulary. This is followed by an aggregation mechanism (roughly sum for SparTerm~\cite{sparterm2020} and SPLADE~\cite{10.1145/3404835.3463098}, max for EPIC~\cite{MacAvaney_2020} and SPARTA~\cite{zhao2020sparta}), that allows to compute an importance weight for each term of the vocabulary, for the full document or query. 
%and of the query/document token and term. 

However, EPIC and SPARTA (document) representations are not sparse enough by construction -- unless resorting on top-$k$ pooling -- contrary to SparTerm, for which fast retrieval is thus possible. Furthermore, the latter does not include (like SNRM) an \emph{explicit} sparsity regularization, which hinders its performance. SPLADE however relies on such regularization, as well as other key changes, that boost both the efficiency and the effectiveness of this type of approaches, providing a model that both learns expansion and compression in an end-to-end manner.
Furthermore, COIL~\cite{gao-etal-2021-coil} proposed to revisit exact-match mechanisms by learning dense representations \emph{per term} to perform contextualized term matching, at the cost of increased index size.

%\todo{Update with latest reference ?}

% --- I kept this for reference
% Similarly, SparTerm \cite{bai2020sparterm} learns to predict term importance in BERT WordPiece vocabulary space. The approach extends the idea further by allowing \emph{expansion} of representations: each token outputs a distribution over the vocabulary (rather than a single weight), and sparsity of representations is ensured through the learning of a binary mask. 
% %The approach has the benefit to ensure both exact --or lexical-- and semantic matching. 
% A similar expansion mechanism was previously introduced in EPIC~\cite{MacAvaney_2020}. SPARTA~\cite{zhao2020sparta} modified the ColBERT~\cite{colbert} architecture by \begin{enumerate*} \item removing the contextual component for queries; \item using a gating function to sparsify the interaction matrices \end{enumerate*}, allowing to obtain latent posting lists that contain ColBERT interaction-based weights. 

% However, the training of the model does not directly offer control over sparsity; thus to obtain sufficiently sparse representations, $top-K$ pooling is applied afterward on documents, making the approach sub-optimal as the sparsification is not learned jointly with the model.
%The model actually borrows strong similarities to the SparTerm model, for which term importance is estimated by the logits of the MLM layer (i.e. by comparing output embeddings to input ones).

%Finally, SOLAR \cite{medini2021solar} takes a different approach: embeddings for entries of the index ($\approx $ latent words) are ultra-high dimensional, sparse and orthogonal by design. They are \emph{fixed} during training, and training the model reduces to learn a query encoder to retrieve from the fixed index. 
  
%\subsection{Dense representations for first stage retrieval}

 
% Commenting this for ID submission 
%\subsection{Limits of current approaches}
%(JUST REMINDER, SEE HOW/WHERE TO PUT THOSE AFTER)
%\begin{itemize}
%    \item SNRM: index badly distributed
%    \item SparTerm: expansion solely on the initial voc, sparsity is done adhoc (set the threshold in binarizer, no %control on final sparsity), no indication on the index, spqrsity etc.  
%\end{itemize}
