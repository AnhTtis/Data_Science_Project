In order to generate our final runs, we needed to set a development set. During the challenge period, we tested many different development sets, but in the end, we focused on what we called \emph{HC4-filtered dev}. The \emph{HC4-filtered dev} is the HC4~\cite{Lawrie2022HC4} dev set, filtered to include only the documents present in the competition's test set (NeuCLIR1)\footnote{This include both in the collection and in the qrels}. Other possibilities we rejected were:\begin{itemize}
    \item MSMARCO translated dev set: While MSMARCO is a good avenue for training, we were afraid that the translated dev set could be biased to the translation and thus make the results less viable (we also would not have a human translated development set).
    \item Mr.TyDi dev/test set: Discarded because it is not available in Farsi.
    \item Full HC4 dev set: Discarded due to its size (larger than the filtered one) and the fact that it did not focus on the same documents as the test set.
\end{itemize}

We shared our results on the HC4-filtered dev with other participants and it seemed like a solid baseline, we thus shared some run files so that other participants could use our SPLADE as their first-stage rankers. Results are made available in Table~\ref{tab:monolingual} (monolingual) and Table~\ref{tab:adhoc} (Adhoc), from which we take the following initial conclusions:

\begin{itemize}
    \item In almost all considered tasks we had a hierarchy of SPLADE $\ge$ ColBERT $\ge$ BM25, but their ensemble seemed more stable than individual runs (even if not always better).
    \item However the precision boost for first-stage rankers over BM25 vary depending on language, while it is almost double in Farsi, in Russian it is of only a few points. However, in terms of Recall, there's a larger gap.
    \item Also, we noticed that SPLADE+Rocchio almost always improved over SPLADE by itself, something we also noticed on the TREC DL challenge. 
    \item Differently from English, monolingual runs did not always improve after reranking. However, the ensembling of rerankers and first-stage rankers was always better than the first-stage ranking by itself. One thing that we missed is that we could have used our first-stage rankers to initialize the rerankers, which could have improved the results.
    \item The best AdHoc ensemble varied by language, with Russian using all available models and Farsi focusing only on the T5 reranked runs. In hindsight, this might not have been the best decision.
    \item Continuing on possibly bad decisions, the first stage AdHoc ensembles did not agree if we should include or not the document back translated ones. 
    \item The results seem to indicate that monolingual vs AdHoc seems to be language and stage dependent. For first stage, Farsi obtained the best results on monolingual, while Russian was on Adhoc (using back-translated documents). For reranking results were always better using back-translated documents, which is expected because the PLMs are not as good (and hyperparameters have not been tuned as well).
    \item It would have been nice to compare with SPLADE-X~\cite{nair2022learning} a CLIR model that came out after the competition (and one of its baselines PSQ). However, the authors only compared on CLEF datasets and as far as we are aware the models are not available.
\end{itemize}

\input{tables/monolingual.tex}
\input{tables/adhoc.tex}