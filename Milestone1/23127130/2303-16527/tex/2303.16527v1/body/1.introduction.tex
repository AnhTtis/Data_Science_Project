\section{Introduction}
\label{sec:intro}

Computing dense correspondences between 3D shapes is a classical problem in Geometry Processing, Computer Vision, and related fields, and remains at the core of many tasks including statistical shape analysis \cite{Bogo2014,Pishchulin2017}, registration \cite{Zhou2016}, deformation  \cite{Baran2009} or texture transfer \cite{Dinh2005} among others.

Since its introduction, the functional map (fmap) pipeline \cite{ovsjanikov2012functional} has become a de facto tool for addressing this problem. This framework relies on representing correspondences as linear transformations across functional spaces, by encoding them as small matrices using the Laplace-Beltrami basis. Methods based on this approach have been successfully applied with hand-crafted features \cite{Salti2014,sun2009concise,aubry2011wave} to many scenarios including near-isometric, \cite{Ren2019, huang2014functional,eynard2016coupled,Shoham2019,Melzi_2019}, non-isometric \cite{kovnatsky2013coupled,Eisenberger2020SmoothSM} and partial \cite{cosmo2016shrec,Litany2017,Litany2016,Xiang_2021_CVPR,Wu2020,Rodol2016} shape matching. In recent years, a growing body of literature has  advocated improving the functional map pipeline by using \textit{deeply} learned features, pioneered by \cite{litany2017deep}, and built upon by many follow-up works \cite{donati2020deep,sharma2020weakly,attaiki2021dpfm,Marin2020CorrespondenceLV,eisenberger2020deep,marin22_why,halimi2019unsupervised,sharp2021diffusion}. In all of these methods, the learned features are only used to constrain the linear system when estimating the functional maps inside the network. Thus, no attention is paid to their geometric nature, or potential utility beyond this purely algebraic role.

\input{figures/teaser_tex}

On the other hand, features learned in other deep matching paradigms are the main focus of optimization, and they represent either robust descriptive geometric features that are used directly for matching using nearest neighbor search \cite{litman2013learning,Zeng_2017_CVPR,Deng_2018_CVPR,Gojcic_2019_CVPR,Yew_2020_CVPR,bai2021pointdsc,li2022srfeat}, or as distributions that, at every point, are interpreted as vertex indices on some template shape \cite{masci2015geodesic,monti2017geometric,poulenard2018multi}, or a deformation field that is used to deform the input shape to match a template \cite{groueix20183d}. 

In contrast, feature (also known as ``probe'' \cite{Ovsjanikov2017}) functions, within deep functional maps are used purely as an optimisation tool and thus the information learned and stored in these functions is not yet well-understood. In this work, we aim to show that features in deep functional maps networks can, indeed, have geometric significance and that, under certain conditions, they are directly comparable across shapes, and can be used for matching simply via nearest neighbor search, see \cref{fig:teaser}.

Specifically, we introduce the notion of feature completeness and show that under certain mild conditions, extracting a pointwise map from a functional map or via the nearest neighbor search between learned features leads to the same result. Secondly, we propose a modification of the deep functional map pipeline, by imposing the learned functional maps to satisfy the conditions imposed by our analysis. We show that this leads to a significant improvement in accuracy, allowing state-of-the-art results by simply performing a nearest-neighbor search between features at test time. Finally, based on our theoretical results, we also propose a modification of some extrinsic feature extractors \cite{dgcnn,Wiersma2022DeltaConv}, which previously failed in the context of deep functional maps, which improve their overall performance by a significant margin. Since our theoretical results hold for the functional map paradigm in general, they can be incorporated in \emph{any} deep fmap method, hence improving previous methods, and making future methods more robust.

Overall, our contributions can be summarized as follows:

\begin{itemize}[topsep=2pt,partopsep=2pt,itemsep=1pt,parsep=3pt]
    \item We introduce the notions of feature completeness and basis aligning functional maps and use them to establish a theoretical result about the nature of features learned in the deep functional map framework.
    \item Informed by our analysis, we propose simple modifications to this framework, which lead to state-of-the-art results in challenging scenarios.
    \item Based on our theoretical results, we propose a simple modification to some extrinsic feature extractors, that were previously unsuccessful for deep functional maps, improving their overall accuracy, and bridging the gap between intrinsic and extrinsic surface-based learning.
\end{itemize}
