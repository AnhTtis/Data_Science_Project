\section{Notation, Background \& Motivation}
\label{sec:background}

\subsection{Notation}
\label{sec:notation}

Suppose we are given a pair of shapes $S_1, S_2$ represented as triangle meshes with respectively $n_1$ and $n_2$ vertices. We compute the cotangent Laplace-Beltrami operator \cite{Meyer2003} of each shape $S_i$ and collect the first k eigenfunctions as columns in a matrix denoted by $\Phi_i$, and the corresponding eigenvalues in a diagonal matrix denoted as $\Delta_i$. $\Phi_i$ is orthonormal with respect to the area (mass) matrix $\Phi_i^{T} A_i \Phi_i = \mathbb{I}$. We denote $\Phi_i^{\dagger} = \Phi_i^{T} A_i$, where $\cdot^{\dagger}$ is the (left) Mooreâ€“Penrose pseudo-inverse.

A pointwise map $T_{12}: S_1 \rightarrow S_2$ can be encoded in a matrix $\Pi_{12} \in \mathbb{R}^{n_1 \times n_2},$ where $\Pi_{12}(i,j) = 1$ if $T_{12}(i) = j$ and 0 otherwise. We will use the letter $\Pi$ to denote pointwise maps in general. To every pointwise map, one can associate a functional map, simply via projection: $\C_{21} = \Phi^{\dagger}_1 \Pi_{12} \Phi_{2}$.

Let $\mathcal{F}_{\Theta}$ be some feature extractor, which takes as input a shape and produces a set of feature functions (where $\Theta$ are some trainable parameters). We then have $F_1 = \mathcal{F}_{\Theta}(S_1)$. For simplicity we will omit explicitly stating $\mathcal{F}_{\Theta}$ and just denote the features associated with a shape $S_i$ by $F_i$, where the presence of some feature extractor is assumed implicitly. Finally, $\A_i = \Phi_i^{\dagger} F_i$ denotes the matrix of coefficients of the feature functions in the corresponding reduced basis.












\subsection{Deep Functional Map Pipeline}

The standard Deep Functional Map pipeline can be described as follows.
     \tightpara{Training:} Pick a pair of shapes $S_1, S_2$ from some training set $\{S_i\}$, and compute their feature functions $F_1 = \mathcal{F}_{\Theta}(S_1), F_2 = \mathcal{F}_{\Theta}(S_2)$. Let $\A_{1} = \Phi_1^{\dagger} F_1$, and $\A_{2} = \Phi_2^{\dagger} F_2$ denote the coefficients of the feature functions in the reduced basis.
       Compute the functional map $\C_{12}$ by solving a least squares system inside the network:
        \begin{align}
            \argmin_{\C} \| \C \A_1 - \A_2 \|_F^2.
            \label{eq:fmap_basic}
        \end{align}
    This system can be further regularized by incorporating commutativity with the Laplacian \cite{donati2020deep}. A \emph{training loss} is then imposed on this computed functional map (in the supervised setting) by comparing the computed functional map $\C_{12}$ with some ground truth:
        \begin{align}
            \mathcal{L}_{sup}(\C_{12}) = \| \C_{12} - \C_{\rm{gt}} \|_F^2. \label{eq:sup_loss}
        \end{align}
        Alternatively, (in the unsupervised setting) we can impose a loss on $\C_{12}$ by penalizing its deviation from some desirable properties, such as being an orthonormal matrix \cite{roufosse2019unsupervised}. Finally, the loss above is used to back-propagate through the feature extractor network $\mathcal{F}_{\Theta}$ and optimize its parameters $\Theta$.


   \tightpara{Test time:} Once the parameters of the feature extractor network are trained, we follow a similar pipeline at test time. Given a pair of \emph{test shapes} $S_1, S_2$, we compute their feature functions $F_1 = \mathcal{F}_{\Theta}(S_1), F_2 = \mathcal{F}_{\Theta}(S_2)$. Let $\A_{1} = \Phi_1^{\dagger} F_1$, and $\A_{2} = \Phi_2^{\dagger} F_2$ denote their coefficients in the reduced basis. We then compute the functional map $\C_{12}$ by solving the least squares system in \cref{eq:fmap_basic}. Given this functional map, a point-to-point (p2p) correspondence $\Pi_{21}$ can be obtained by solving the following problem:
        \begin{align}
            \Pi_{21} = \argmin_{\Pi} \| \Pi \Phi_1 - \Phi_2 \C_{12}\|.
            \label{eq:fmap_conversion}
        \end{align}
        This problem, which was proposed and justified in \cite{ezuz2017deblurring,Pai_2021_CVPR} is row-separable and reduces to the nearest neighbor search between the rows of $\Phi_1$ and the rows of $\Phi_2 \C_{12}$. This method of converting the fmap into a p2p map is referred to hereafter as the \textit{adjoint method} following \cite{Pai_2021_CVPR}.
  
\vspace{-0.7em}
\paragraph{Advantages}
%\souhaib{will be removed}
This pipeline has several advantages: first, the training loss is imposed on the entire map rather than individual point correspondences, which has been shown to improve accuracy \cite{litany2017deep,donati2020deep}. Second, by using a reduced spectral basis, this approach strongly regularizes the learning problem and thus can be used even in the presence of limited training data \cite{donati2020deep,sharp2021diffusion}. Finally, by accommodating both supervised and unsupervised losses, this pipeline allows to promote structural map properties without manipulating large and dense, e.g., geodesic distance, matrices.

%One advantage of this pipeline is that during training all computations are in the reduced basis. For example, there is no in-network computation of a soft correspondence matrix, which could require manipulating $O(n^2)$ in GPU memory. Thus, this standard pipeline is reasonably scalable (although it does require storing $k \times n$ basis matrices $\Phi_i$). 

\vspace{-0.7em}
\paragraph{Drawbacks \& Motivation}
% One possible weakness is that this pipeline relies on the Laplacian basis both during training and at test time. Furthermore, 
At the same time, conceptually, the relation between the learned feature functions and the computed correspondences is still unclear. Indeed, although the feature (probe) functions are learned, they are used solely to formulate the optimization problem in \cref{eq:fmap_basic}. Moreover, the final pointwise  correspondence in \cref{eq:fmap_conversion} is still computed from the Laplacian basis. Thus, it is not entirely clear what exactly is learned by the network $\mathcal{F}_\Theta$, and whether the learned feature functions can be used for any other task. Indeed, in practice, learned features can fail to highlight some specific repeatable regions on shapes, even if they tend to produce high-quality functional map matrices (\cref{fig:teaser}).











\subsection{Theoretical Analysis}
\label{sec:motivation}
In this subsection, we introduce some notions and state a theorem that will be helpful in our results below. We will use the same notation as in \cref{sec:notation}.

First, note that both during training (in \cref{eq:fmap_basic}) and at test time (in  \cref{eq:fmap_conversion}) the learned feature functions $F_i$, are used by projecting them onto the Laplacian basis. Thus, both from the perspective of the loss and also when computing the pointwise map, the deep functional map pipeline only uses the part of the functions that lies in the span of the Laplacian basis. Put differently, if we \textit{modified} feature functions by setting $\tilde{F}_i  = \Phi_i \Phi_i^{\dagger} F_i$ then both during training and at test time the behavior of the deep functional map pipeline will be identical when using either $F_i$ or $\tilde{F}_i$.

\begin{definition}[Complete feature functions]
Motivated by this observation, we call the feature extractor $F_{\Theta}$ \textbf{complete} if $F_{\Theta}$ produces feature (probe) functions that are contained in the corresponding LB eigenbasis. I.e.,
\begin{align}
	&\mathcal{F}_\Theta (S_i) = \Phi_i \Phi_i^{\dagger} \mathcal{F}_\Theta (S_i) , \text{ or equivalently} \\
	&(Id - \Phi_i \Phi_i^{\dagger}) \mathcal{F}_\Theta (S_i)  = 0.
\end{align}
\end{definition}
%
%if it satisfies two conditions: first, the descriptor for every point is unique (i.e., feature extractor is injective) and, second, $F_{\Theta}$ produces feature (probe) functions, that are contained in the corresponding Laplace-Beltrami eigenbasis. I.e.,
%    \begin{align}
%        &\mathcal{F}_\Theta (S_i) = \Phi_i \Phi_i^{+} \mathcal{F}_\Theta (S_i) , \text{ or equivalently} \\
%        &(Id - \Phi_i \Phi_i^{+}) \mathcal{F}_\Theta (S_i)  = 0.
%    \end{align}
%\end{definition}
%
%It should be noted that an injective feature extractor can be made complete by simply projecting its feature functions onto the Laplacian basis, i.e., by setting: $\tilde{\mathcal{F}_\Theta}(S_i) = \Phi_i \Phi_i^{+} \mathcal{F}_\Theta (S_i),$ assuming this procedure does not break injectivity.

% \begin{definition}[Alignable Laplacian bases]
% Given a pair of shapes, $S_1, S_2$ we will call their Laplacian eigenbases $\Phi_1, \Phi_2$ \textit{perfectly alignable} if there exists a functional map $\C_{12}$ and a point-to-point map $\Pi_{21}$ s.t. $\Pi_{21} \Phi_{1} = \Phi_{2} \C_{12}.$
% \end{definition}

% It is worth noting that that for sufficiently high dimensionality $k$, any pair of Laplacian bases is perfectly alignable. This is because if two shapes have $n$ vertices each then, since $\Phi_{2}$ is guaranteed to be invertible in the full basis, there is a unique $n^2$ matrix $\C_{12}$ that will satisfy $\Pi_{21} \Phi_{1} = \Phi_{2} \C_{12}$ exactly.


\begin{definition}[Basis aligning functional maps] We also introduce a notion that relates to the properties of functional maps. Namely, given a pair of shapes, $S_1, S_2$ and a functional map $\C_{12}$, we will call it \textit{basis-aligning} if the bases, when transformed by this functional map, align exactly. This can be summarized as follows: $\Phi_{2} \C_{12} = \Pi_{21} \Phi_{1}$ for some point-to-point map $\Pi_{21}$.
\end{definition}

For simplicity, for our result below we will also assume that all optimization problems have unique global minima. Thus, for the problem $\argmin_{\C} \|\C \A_{1} - \A_{2} \|$, this means that $\A_1$ must be full rank, whereas for the problem of type $\argmin_{\Pi} \| \Pi F_1 - F_2 \|$, this means that the rows of $F_1$ must be distinct (i.e., no two rows are identical, as vectors).

% \begin{theorem}
%     Suppose that the Laplacian eigenbasis is perfectly alignable, \ie, $\exists (\C_{12}, \Pi_{21}): \Pi_{21} \Phi_{1} = \Phi_{2} \C_{12}$. Suppose moreover that the feature extractor $F_{\Theta}$ is \emph{complete}, and that $\C_{12} \A_1 = \A_2$ is satisfied exactly.

% Then the point-to-point map computed by nearest neighbor in descriptor space $\min_{\Pi} \| \Pi F_1 - F_2 \|$ will recover the exact same map as $\Pi_{21}$.

% Conversely, suppose that the feature extractor is \emph{complete} and  $\C_{12} \A_1 = \A_2$ is satisfied exactly, if the rows of $\A_1$ are linearly independent (which is required for the linear system $X \A_1 = \A_2$ to be solvable), then $\Pi_{21} F_1 = F_2$ implies that the Laplacian eigenbasis is perfectly alignable and $\min_{\Pi} \| \Pi \Phi_1 - \Phi_2 \C \|$ will recover the same map $\Pi_{21}$.
% \end{theorem}

\begin{theorem}
\label{thm:equivalence}
    Suppose the feature extractor $F_{\Theta}$ is \emph{complete}. Let $\A_1 = \Phi_1^{\dagger} F_1$ and $\A_2 = \Phi_2^{\dagger} F_2$. Then, denoting $\C_{\rm{opt}} = \argmin_{\C} \|\C \A_{1} - \A_{2} \|$, we have the following results hold:
    
(1) If $\Pi F_1 = F_2$ for some point-to-point map $\Pi$ then $\C_{12} = \Phi_2^{\dagger} \Pi \Phi_1$ is basis-aligning. Moreover, $\C_{12} = \C_{\opt}$, and extracting the pointwise map from $\C_{\opt}$ via the adjoint method (see \cref{eq:fmap_conversion}) or via nearest neighbor search in the feature space $\min_{\Pi} \| \Pi F_1 - F_2 \|$ will give the same result.

(2) Conversely, suppose that $F_1, F_2$ are complete and $\C_{\opt}$ is basis aligning, then $\argmin_{\Pi} \| \Pi F_1 - F_2 \| = \argmin_{\Pi} \| \Pi \Phi_1 - \Phi_2 C_{\opt}\|$.
\end{theorem}

\begin{proof}
    See the supplementary materials.
\end{proof}

\tightpara{Discussion:} 
Note that in the theorem above, we used the notion of basis-aligning functional maps. Two questions arise: what are the necessary and sufficient conditions for a functional map to be basis-aligning? 

A necessary condition is that the functional map must be \textit{proper} as defined in \cite{ren2021discrete}. Namely, a functional map $\C_{12}$ is \textit{\textbf{proper}} if there exists a pointwise map $\Pi_{21}$ such that  $\C_{12} = \Phi_{2}^{\dagger} \Pi_{21} \Phi_{1}$.  Note that for sufficiently high dimensionality $k$, \textit{any} proper functional map must be basis aligning.  This is because, since $\Phi_{2}$ is guaranteed to be invertible in the full basis, there is a unique matrix $\C_{12}$ that will satisfy $\Pi_{21} \Phi_{1} = \Phi_{2} \C_{12}$ exactly.

Conversely, a \textit{sufficient} condition for a functional map to be basis aligning is that the underlying point-to-point map is an isometry. Indeed, it is well known that isometries must be represented as block-diagonal functional maps, and moreover, isometries preserve eigenfunctions (see Theorem 2.4.1 in \cite{Ovsjanikov2017} and Theorem 1 in \cite{rustamov2013map}). Thus, assuming that the functional map is of size $k$ and that the $k^{\text{th}}$ and $(k+1)^{\text{st}}$ eigenfunctions are distinct, we must have that the functional map must be basis-aligning.

Finally, note that a functional map is basis-aligning if it is \textit{proper} and if the image (by pull-back) of the $k$ first eigenfunctions of the source lies in the range of the $k$ first eigenfunctions of the target. This can therefore be considered as a measure of the \textit{smoothness} of the map.

%
%
%It is worth noting that that , for any pair of Laplacian bases, there is a basis aligning fmap. This is because if two shapes have $n$ vertices each then, since $\Phi_{2}$ is guaranteed to be invertible in the full basis, there is a unique $n^2$ matrix $\C_{12}$ that will satisfy $\Pi_{21} \Phi_{1} = \Phi_{2} \C_{12}$ exactly.
%
%Finally, another notion that will be helpful is the fmap properness, introduced in \cite{discrete_Ren2021}.
