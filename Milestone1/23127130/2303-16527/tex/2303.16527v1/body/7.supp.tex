% % CVPR 2023 Paper Template
% % based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% % modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

% \documentclass[10pt,twocolumn,letterpaper]{article}

% %%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version

% % \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

% %\usepackage{cvpr}              % To produce the CAMERA-READY version
% %\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% % Include other packages here, before hyperref.
% % \usepackage{graphicx}
% % \usepackage{amsmath}
% % \usepackage{amssymb}
% % \usepackage{amsthm}
% % \usepackage{booktabs}
% % %\usepackage{arydshln}
% % \usepackage{color}
% % \usepackage[dvipsnames]{xcolor}
% % \usepackage{colortbl}
% % \usepackage{textcomp}
% % \usepackage{epsfig}
% % \usepackage{multirow}
% % \usepackage{bigdelim}
% % \usepackage{wrapfig}
% % \usepackage{caption}
% % % \usepackage{subcaption}
% % \usepackage{url}
% % \usepackage{nicefrac}
% % \usepackage{tabularx}
% % \usepackage{enumitem}
% % \usepackage{tikz}


% \newcommand{\tightpara}[1]{\vspace*{-3mm}\paragraph{#1}}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newcommand{\final}{0}  % 0 comment activated, 1 no comments 
% \usepackage{geometrycollective}

% \def\PaperTitle{Understanding and Improving Features Learned in Deep Functional Maps}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% %\newtheorem{theorem}{Theorem}

% \theoremstyle{definition}
% \newtheorem{definition}{Definition}[section]

% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{lemma}[theorem]{Lemma}



% \newcommand{\C}{\mathbf{C}}
% \newcommand{\A}{\mathbf{A}}
% \newcommand{\X}{\mathbf{X}}
% \newcommand{\Id}{\mathit{I}_d}
% \newcommand{\opt}{\rm{opt}}

% % \DeclareMathOperator*{\argmin}{arg\,min}
% % \DeclareMathOperator*{\argmax}{arg\,max}

% % \newcommand{\maks}[1]{{\bfseries \small \color{blue} MO: #1}}
% % \newcommand{\souhaib}[1]{{\bfseries \small \color{green} SA: #1}}
% \definecolor{ColorLightBlue}{RGB}{18, 137, 255}
% \newcommand{\rev}[1]{{\color{ColorLightBlue}#1}}


% \newcommand*\circled[1]{\tikz[baseline=(char.base)]{
%             \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}


% % % Support for easy cross-referencing
% % \usepackage[capitalize]{cleveref}
% % \crefname{section}{Sec.}{Secs.}
% % \Crefname{section}{Section}{Sections}
% % \Crefname{table}{Table}{Tables}
% % \crefname{table}{Tab.}{Tabs.}


% % It is strongly recommended to use hyperref, especially for the review version.
% % hyperref with option pagebackref eases the reviewers' job.
% % Please disable hyperref *only* if you encounter grave issues, e.g. with the
% % file validation for the camera-ready version.
% %
% % If you comment hyperref and then uncomment it, you should delete
% % ReviewTempalte.aux before re-running LaTeX.
% % (Or just hit 'q' on the first LaTeX run, let it finish, and you
% %  should be clear).




% %%%%%%%%% PAPER ID  - PLEASE UPDATE
% \def\cvprPaperID{1770} % *** Enter the CVPR Paper ID here
% \def\confName{CVPR}
% \def\confYear{2023}


% \begin{document}

% %%%%%%%%% TITLE - PLEASE UPDATE
% \title{Supplementary Materials for:\\\PaperTitle}

% \author{Souhaib Attaiki\\
% LIX, École Polytechnique, IP Paris\\
% {\tt\small attaiki@lix.polytechnique.fr}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Maks Ovsjanikov\\
% LIX, École Polytechnique, IP Paris\\
% {\tt\small maks@lix.polytechnique.fr}
% }
% \maketitle





\appendix

%%%%%%%%% ABSTRACT

%\begin{abstract}

In this document, we collect all the results and discussions, which, due to the page limit, could not find space in the main manuscript.

More precisely, we first provide the proof of the theorem we introduced in Sec. 3.3 of the main text in \cref{sec:proof}. Then, the details of the implementation are provided in \cref{sec:implementation}. We present a more in-depth analysis of the modifications we introduced to the deep functional map pipeline in \cref{sec:ablation}. A comparison with other shape-matching methods is provided in \cref{sec:comparison}, as well as additional results regarding the generalization power of pre-trained features in \cref{sec:generalization-power}. Finally, some qualitative results are included in \cref{sec:qualitative}.

% Before proceeding with the supplementary material, we want to clarify a point. In Sec 4.1 of the main text, we introduced two methods for enhancing properness, both of which are based on computing a point-to-point soft map in a differentiable way. The first method is based on the adjoint functional map conversion scheme, and we named it \textbf{The adjoint method}. The second one computes the point-to-point map by the nearest neighbor between features, and we called it in the main text \textbf{The FMap-based method}. We would like to clarify that this is a typographical error and that we wanted to call it \textbf{The feature-based method}, as we use the features. In this supplementary, we will use the correct name.



% \maks{the text sounds good. maybe add a note that whenever we refer to the ``fmap'' pointwise recovery in the main manuscript, this indeed refers to the ``fmap''-based approach (as opposed to NN). Otherwise, it might be confusing, as it might seem that we didn't actually compare NN to fmap in the main manuscript.}

%\end{abstract}









% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Theorem 3.1}
\label{sec:proof}

In Sec. 3.3 of the main text, we stated a theorem that shows that the maps obtained with the adjoint method, or the nearest neighbor in the feature space are equivalent under some conditions. In this section, we formally restate it and provide proof.

As mentioned in the main body, in our result below we assume that all optimization problems have unique global minima. Thus, for the problem $\argmin_{\C} \|\C \A_{1} - \A_{2} \|$, this means that $\A_1$ must be full row rank, whereas, for the problem of type $\argmin_{\Pi} \| \Pi F_1 - F_2 \|$, this means that the rows of $F_1$ must be distinct (i.e., no two rows are identical, as vectors).

\begin{customthm}{3.1}
\label{thm:equivalence_supp}
    Suppose the feature extractor $F_{\Theta}$ is \emph{complete}. Let $\A_1 = \Phi_1^{+} F_1$ and $\A_2 = \Phi_2^{+} F_2$. Then, denoting $\C_{\rm{opt}} = \argmin_{\C} \|\C \A_{1} - \A_{2} \|$, we have the following results hold:
    
(1) If $\Pi F_1 = F_2$ for some point-to-point map $\Pi$ then $\C_{12} = \Phi_2^{+} \Pi \Phi_1$ is basis-aligning. Moreover, $\C_{12} = \C_{\opt}$ and extracting the pointwise map from $\C_{\opt}$ via the adjoint method, or via nearest neighbor search in the feature space $\min_{\Pi} \| \Pi F_1 - F_2 \|$ will give the same result.

(2) Conversely, suppose that $\C_{\opt}$ is basis aligning, then $\argmin_{\Pi} \| \Pi F_1 - F_2 \| = \argmin_{\Pi} \| \Pi \Phi_1 - \Phi_2 C_{\opt}\|$.
\end{customthm}


\begin{proof}

(1) If $\Pi F_1 = F_2$ and $F_1, F_2$ are complete by assumption then we have $F_1 = \Phi_1 \A_1$ and $F_2 = \Phi_2 \A_2$ so that:
\begin{equation}
\label{eq:p1}
    \Pi \Phi_1 \A_1 = \Phi_2 \A_2
\end{equation}


Setting $\C_{12} = \Phi_2^{+} \Pi \Phi_1$ and pre-multiplying Eq.~(\ref{eq:p1}) by $\Phi_2^{+}$ we obtain $\C_{12} \A_1 = \A_2.$ Thus, $\|\C_{12} \A_1 - \A_2\| = 0$, and it follows that $\C_{\opt} = \C_{12}$, since $\A_1$ assumed to be of full row rank (and thus $\argmin_{\C} \|\C \A_{1} - \A_{2} \|$ has a unique  optimum).

Moreover, using $\C_{12} \A_1 = \A_2,$ we get $\Phi_2 \A_2 = \Phi_2 \C_{12} \A_1$. Combining this with \cref{eq:p1}, we get $\Pi \Phi_1 \A_1 = \Phi_2 \C_{12} \A_1$. Using the fact that $\A_1$ is full rank, this implies that $\Pi \Phi_1 = \Phi_2 \C_{12}$, and thus $\C{12}$ is basis-aligning.

Finally, we note that since the same pointwise map satisfies $\|\Pi F_1 - F_2 \| = \|\Pi \Phi_1 - \Phi_2 \C_{12}\| = 0$, minimizing both energies with respect to $\Pi$ would result in the same map. 
	 
(2) By assumption $\C_{\opt}$ is basis-aligning. Thus, $\Pi_{21} \Phi_1 = \Phi_2 \C_{\opt}$ for some pointwise map $\Pi_{21}$. Thus,

\begin{equation}
\label{eq:p2}
    \min_{\Pi} \| \Pi \Phi_1 - \Phi_2 \C_{\opt}\| = \Pi_{21}
\end{equation}

Now let's consider the problem 
\begin{equation}
\label{eq:p3}
    \min_{\Pi} \|\Pi F_1 - F_2\| = \min_{\Pi} \|\Pi F_1 - F_2\|^2
\end{equation}

The objective can be decomposed into two parts, one that lies within the span of $\Phi_2$ and the one outside of it, as follows: 
%
\begin{align*}
   E(\Pi) =&  \|\Pi F_1 - F_2\|^2 \nonumber \\
   =&\|\Phi_2^{+}(\Pi F_1 - F_2)\|^2 + \|(I - \Phi_2 \Phi_2^{+})(\Pi F_1 - F_2)\|^2  \nonumber \\
   =& E_1(\Pi) + E_2(\Pi) \nonumber 
\end{align*}
 
Using the fact that $F_i$ are complete, we have $F_i = \Phi_i \A_i$. It follows that $E_1 = \|\Phi_2^{+} \Pi \Phi_1 \A_1 - \A_2\|^2$.

On the other hand, using the fact that $F_2$ is complete,  we have $(I - \Phi_2 \Phi_2^{+})F_2 = 0 $, and thus $E_2 = \|(I - \Phi_2 \Phi_2^{+})\Pi F_1 \|^2$. 

Now recall that by assumption $\C_{\opt}$ is basis-aligning and thus there exists some pointwise map $\Pi_{21}$ such that $\Pi_{21} \Phi_1 = \Phi_2 \C_{\opt}$.

Consider $E_1(\Pi)$ for an arbitrary pointwise map $\Pi$. We have:
\begin{align*}
     \min_{\C} \|\C \A_{1} - \A_{2} \|         =& \|C_{\opt} \A_1 - \A_2\| \\
              =& \|\Phi_2^{+} \Pi_{21} \Phi_1 \A_1 - \A_2\| \\
              =& E_1(\Pi_{21})
\end{align*}

It follows that we must have that $E_1(\Pi_{21}) \le E_1(\Pi)$ for any pointwise map $\Pi$.

Moreover observe that for $\Pi_{21}$ we have:
\begin{align*}
   E_2(\Pi_{21}) =&  \|(I - \Phi_2 \Phi_2^{+})\Pi_{21} F_1 \|^2 \nonumber \\
   =&\|(I - \Phi_2^{+} \Phi_2^{+})\Pi_{21} \Phi_1 \A_1 \|^2  \nonumber \\
   =& \|(I - \Phi_2 \Phi_2^{+}) \Phi_2 \C_{\opt} \A_1 \|^2 \nonumber \\
   =& \| \Phi_2 \C_{\opt} \A_1 - \Phi_2 \C_{\opt} \A_1\| \nonumber \\
   =& 0
\end{align*}

Thus, for an arbitrary pointwise map $\Pi$ we must have $E_2(\Pi_{12}) \le  E_2(\Pi)$. 

It therefore follows that $\argmin_{\Pi} \|\Pi F_1 - F_2\| = \argmin_{\Pi} \left(E_1(\Pi) + E_2(\Pi)\right) = \Pi_{21}$, and thus $\argmin_{\Pi} \| \Pi F_1 - F_2 \| = \argmin_{\Pi} \| \Pi \Phi_1 - \Phi_2 C_{\opt}\|.$
\end{proof}


\paragraph{Verification of the assumptions of the theorem} Note that as mentioned above, we first assumed that $\A_1$ is of full rank, and second, the rows of $F_1$ must be distinct so that all optimization problems have unique minima. We would like to point out that these assumptions are very weak and easily hold in practice.

Indeed, as far as the second condition is concerned, it is easily verifiable because of numerical precision, as it is very unlikely that two distinct points are assigned the exact same feature vectors. In practice, for example, we compute for each point in a feature produced by DiffusionNet, the distance to its nearest neighbor, and we take the average over the whole shape. We find that this distance is equal to 0.15, while for reference, it is equal to 0.0004 for the XYZ coordinates, which shows that this assumption is justified. 

For the first assumption, we plot in \cref{fig:rank_evolution} the evolution of the rank of the features produced by DiffusionNet during training, as well as the evolution of the rank of their projection on the spectral basis (\ie $\A_i$). We can see that throughout the training, the rank of the projected features is always equal to 30, which is the same dimension as the spectral basis used in all our experiments, which proves that the assumption of the full rank of $\A_1$ is verified.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/rank_evolution.pdf}
    \caption{Evolution of the rank of features produced by DiffusionNet, as well as their projection in the Laplacian-Basis, during training.}
    \label{fig:rank_evolution}
\end{figure}


\paragraph{Impact of the proposed modifications on the conditions of the theorem}
In this paragraph, we provide measures of the individual conditions of the theorem before and after applying our modifications in order to shed light on their effectiveness.

Given a functional map $\C_{12}$ predicted by the fmap framework, we compute its basis aligning property by computing the Chamfer distance between $\Phi_2 \C_{12}$ and $\Phi_1$ using the notation in Definition 3.2 of the main text (note that $\|\Phi_2 \C_{12} - \Pi_{21} \Phi_1\|$ is a one-way Chamfer distance). 
For properness, given a predicted functional map $\C$, we compute its proper version $\C_\mathrm{proper}$ using the adjoint method and measure properness by computing $\Vert \C - \C_\mathrm{proper}\Vert_2^2$. 
For completeness, using the same notation as in Eq. (4), we measure it using $1 - \frac{\Vert \mathcal{F}_\Theta (S_i) - \Phi_i \Phi_i^{\dagger} \mathcal{F}_\Theta (S_i)\Vert_2^2}{\Vert \mathcal{F}_\Theta (S_i) \Vert_2^2 }$, this quantity is between 0 and 1 (higher is better). 

We report these measures in \cref{tab:complete_proper} for the same setup used in our ablation study, i.e., unsupervised shape matching on "FA on FA" (see \cref{sec:ablation}), for two main backbones: DGCNN and DeltaConv (note that in DiffusionNet completeness is enforced by construction). It can be seen that our modifications improve the measured properties.




\input{tables/complete_proper}


% \subsection{Hypothesis Verification}

% \maks{is this section necessary}

% We start our experiments section with a toy experiment to verify the result of \cref{thm:equivalence}.  Specifically, we overfit a DGCNN \cite{dgcnn} network on one pair of shapes from \textbf{FR} dataset, train it with properness and supervised L2 loss on the functional map. We then at test time, extract three p2p maps, (1) with the adjoint method (2) with the nearest neighbor between features (3) with the nearest neighbor between complete features, obtained by projecting them to the Laplacian basis and then projecting back. We found that the average geodesic difference between map (1) and map (2) is 4.1 cm, while the difference between map (1) and map (3) is 0.2 cm. This shows that the more  complete the features are, the closer the map obtained with the adjoint method to the one obtained with NN between features. Moreover, this demonstrates, for the first time, the direct utility of features learned in deep fmap settings.   


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation Details}
\label{sec:implementation}
In all our experiments with functional maps, we used the functional map size of $k=30$. For the Laplace-Beltrami computation, we used the discretization introduced in \cite{sharp2020laplacian}.

In our experiments in Sec. 5.2 of the main text, we used three different networks, DGCNN \cite{dgcnn}, DiffusionNet \cite{sharp2021diffusion} and DeltaConv \cite{Wiersma2022DeltaConv}. For this, we used
the publicly available implementations \footnote{\url{https://github.com/WangYueFt/dgcnn}} \footnote{\url{https://github.com/nmwsharp/diffusion-net}} \footnote{\url{https://github.com/rubenwiersma/deltaconv}} released by the authors. In all our experiments, we used the default segmentation configuration provided by the authors in their respective papers, with an output dimension of 128. For all these experiments, we used the Adam optimizer \cite{kingma2017adam}, with a learning rate of 0.001. We used data augmentation in all our experiments.  In particular, we augment the training data on the fly by randomly rotating the input shapes, varying the position of each point by Gaussian noise, and applying random scaling in the interval [0.9, 1.1].

For the proper map computation (Eq.~6 of the main text), we used $\tau = 0.07$.

To make the feature extractor smooth, %\ie $f^{'}_i = \Phi_j \Phi_j^{\dagger} f_i$, 
we project the features onto a Laplace-Beltrami basis of size $j=128$, which is the same size used in the original DiffusionNet implementation.

Regarding the experiment in Sec. 5.3 of the main text, we used the pre-trained DiffusionNet model on the Scape Remeshed Aligned dataset, with unsupervised and properness losses. The downstream point MLP consists of 4 layers, and we train it with the Adam optimizer for 300 iterations, using a learning rate of 0.001. The training shape is randomly sampled from the training data set. %In Table 4 of the main text, we have also included the results with XYZ coordinates as a reference, to show the improvement made by our features.

% \paragraph{how projecting makes the features smooth, and add fourier citation}












% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ablation Study}
\label{sec:ablation}

In Sec. 4 of the main text, we introduced two modifications to the functional map pipeline, namely imposing properness on the functional map and requiring the features to be as smooth as possible. Here we show the effect of each modification independently. For illustration, we will use the unsupervised near-isometric matching experiment (Sec. 5.2.2 of the main text), but a similar conclusion can be drawn in all other scenarios.



The results are summarized in \cref{tab:ablation}. In this table, we show the result for each architecture without modification \textcircled{0}, with properness with the adjoint method \textcircled{1}, with feature-based properness \textcircled{2}, using the smoothness operation \textcircled{3}
or the combination of the latter. Since in DiffusionNet, the smoothing operation is performed by construction, we only include the results with properness.

We can see that each of our modifications improves the result of the vanilla feature extractor, and for optimal performance, both modifications should be used. In particular, we noticed that imposing the properness using the feature-based method gives a slightly better result, so we advocate using this method.

\input{tables/tab_ablation.tex}

As we explained in the main text, the difference between the result obtained with the functional map, and the result with the nearest neighbor (NN) method is explained by the fact that the conditions of the theorem are not fully satisfied. %If NN gives better results, it is because \souhaib{what} \maks{I would say that
Indeed, while the results of the two approaches are generally very close, after our modifications, there is variability in terms of which method produces the best results depending on the dataset. Remark that if the NN approach is better, this suggests that higher frequencies in the learned feature functions are beneficial for correspondence. However, the opposite can (and indeed does) occur, in that those high frequencies can hinder results since they are not penalized during training (as the functional map losses are computed after projecting the features onto a low-frequency basis).
















% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Comparison with other methods}
\label{sec:comparison}
In this section, we compare our method to other recent shape-matching methods and evaluate the effect of our proposed modifications on additional baseline approaches. We compare our method in supervised and unsupervised near-isometric shape-matching experiments in Sec.~5.2.1 and Sec.~5.2.2 of the main text, respectively.


Note that since our proposed modifications are general and can be applied to \textit{any deep functional map pipeline} in principle, we also tested their effects on additional methods. In addition to the comparisons shown in the main manuscript, %to the ``GeomFMaps + Feature Extractor'' method used in Sec. 5.2.1, and ``UnsupFMNet + Feature Extractor''  used in Sec. 5.2.2, 
below we also test our modifications on two very recent approaches: SRFeat \cite{li2022srfeat} for the supervised case, and DUO-FMap \cite{donati-duo} for the unsupervised case. Because both SRFeat and DUO-FMap use DiffusionNet as a backbone, we only enforce the properness using the feature-based method, following Sec. 4.1 of the main text.

We use the same protocol as in the main text. Namely, the geodesic error is normalized by the square root of the total surface area, values are multiplied by $\times 100$ for clarity, and the notation ``X on Y'' means that we train on X and test on Y. We denote the methods using our proposed modifications by ``Method - Ours''.

\input{tables/comparaison_supervised.tex}

Concerning the supervised near-isometric shape-matching experiment, we compare to FMNet \cite{litany2017deep}, 3DCODED \cite{groueix20183d}, HSN \cite{wiersma2020cnns}, TransMatch \cite{trappolini2021shape}, GeomFMaps \cite{donati2020deep}, and SRFeat \cite{li2022srfeat}. Results are summarized in \cref{tab:comp_supervised}. It can be seen that our method achieves state-of-the-art results among supervised methods, especially in challenging cases such as testing on the SHREC Remeshed dataset.




For the unsupervised setting, we test our method against SURFMNet \cite{roufosse2019unsupervised}, UnsupFMNet \cite{halimi2019unsupervised}, WSupFMNet \cite{sharma2020weakly}, Deep Shells \cite{eisenberger2020deep}, Neuromorph \cite{Eisenberger2021NeuroMorphUS}, and DUO-FMap \cite{donati-duo}. Results are summarized in \cref{tab:comp_unsupervised}. As can be seen, our method achieves state-of-the-art results in this scenario also, especially in challenging cases involving generalization, where all competing methods fail. It can also be seen that the modifications we propose are complementary to the different versions of the deep functional map pipeline. Remarkably, our method brings consistent and significant improvements throughout \textit{all cases} and baseline approaches that we tested.

We would like to emphasize that our main contribution is both an analysis and a set of \textit{improvements} for the deep functional map pipeline in general. As such, rather than a particularly new approach for correspondence, our key contribution is a set of  modifications, which can be adapted within different deep functional map pipelines. We emphasize this because our approach is flexible and, as illustrated in the results, can be beneficial for different methods proposed in the literature for both supervised and unsupervised cases.

\input{tables/comparaison_unsupervised.tex}









% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Improvement of our modifications to other methods}

% In addition to ``GeomFmaps + DiffusionNet'' used for the supervised case, and ``UnsupFMNet + DiffusionNet '' used for the unsupervised case both in the main paper, and the above paragraphs, we also test the improvement brought about by our method to other methods. For this, we modify SRFeat for the supervised case and DUO-FMap for the unsupervised case. Because both methods use DiffusionNet as a backbone, we only enforce the properness as described in Section 4.2 of the main text. The results are shown in the tables above. The results with our modifications are ``method - Ours''. As it can be seen, our method improves the results, and achieve SOTA results .... \souhaib{insert comment here}. 










% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Generalization Power of Pre-Trained Features}
\label{sec:generalization-power}

In Sec. 5.3 of the main text, we showed the generalization power of our pretrained features on the task of human segmentation. Here we show more results consolidating the fact that learned features do have a geometric signification.

% \begin{figure}
%     \centering
%     \includegraphics[width=\columnwidth]{figures/hum_seg_ev.pdf}
%     \caption{Evolution of human segmentation accuracy as a function of the number of shapes used for the pointwise MLP training.}
%     \label{fig:hum_seg_ev}
% \end{figure}

% In \cref{fig:hum_seg_ev}, we plot the accuracy of the same MLP trained with the pre-trained features used previously, as a function of the number of shapes. Note that, as expected, the accuracy of the results improves (up to a certain point, where most likely the limited capacity of pointwise MLP is reached). Note also that our features, which are pre-trained in a completely unsupervised manner, produce \textit{significantly} higher accuracy than the raw XYZ coordinates. 

% We attribute this feature utility in a downstream task to the fact that these features capture the geometric structure of the shapes in a compact and invariant manner. % (unlike the Laplacian basis, for example, which is highly shape dependent).



For this, we tested the utility of the pre-trained features for the task of molecular surface RNA segmentation. We used the RNA dataset introduced in \cite{poulenard2019effective}, composed of 640 RNA triangle meshes, where each vertex is labeled into one of 259 atomic categories. We used the same 80/20\% split for training and test sets as in previous works.

We follow the same setup as in Sec. 5.3 of the main text. Specifically, we train a DiffusionNet network for shape matching in an unsupervised manner on the RNA data. We then extract features for each shape using the trained network. Finally, these features are used to train another DiffusionNet for the semantic segmentation task in a supervised manner.

The results are summarized in \cref{tab:rna_seg}. We can see that our pre-trained features outperform XYZ and HKS \cite{sun2009concise} (note that due to the relatively large size of the training set, the improvement starts to saturate). We take this as further evidence that the features extracted using our approach encode geometric information that can be useful in various shape analysis tasks.



\input{tables/tab_rna_pretrain.tex}












% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Qualitative Results}
\label{sec:qualitative}

In this section, we present some qualitative results with our method.

In \cref{fig:hum_matching}, we show the quality of the produced maps, before and after our modifications. It can be seen that our modifications produce visually plausible correspondences.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/hum_matching.pdf}
    \caption{ Qualitative results on SCAPE Remeshed dataset using DeltaConv, before and after using the modifications we have introduced.}
    \label{fig:hum_matching}
\end{figure}


In \cref{fig:feat_rep}, we show how repeatable the features are, before and after our modifications, the intensity is color coded. We can see that before the modifications, the features are mostly flat and non-distinctive, making them not useful for matching using the nearest neighbor method, or for use in a downstream task, whereas, after our modifications, we can see that the features are activated over the same region, over multiple shapes, and they are not flat since they vary according to the geometry.

% It can be seen that after our modifications, the features are activated over the same region, over multiple shapes, however before the modifications, they were not consistent.  

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/feat_rep.pdf}
    \caption{Visualization of the intensity of the same feature learned by DeltaConv for shape matching, on several shapes, before and after using our modifications. The intensity is color coded.}
    \label{fig:feat_rep}
\end{figure}

% %%%%%%%%% REFERENCES
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{references}
% }

% \end{document}
