\section{Proposed Modification}
\label{sec:method}

In the previous section, we provided a theoretical analysis of the conditions under which computed ``probe'' functions within the deep functional map pipeline can be used as pointwise descriptors directly, and lead to the same point-to-point maps as computed by the functional maps. In \cref{sec:application}, we provide an extensive evaluation of using learned feature functions for pointwise map computation and thus affirm the validity of \cref{thm:equivalence} in practice.

% \maks{is this paragraph necessary?} 
Our main observation is that the two approaches for point-to-point map computation are indeed often equivalent in practice, especially in ``easy'' cases, where existing state-of-the-art approaches lead to highly accurate maps. In contrast, we found that in more challenging cases, where existing methods fail, the two approaches are not equivalent and can lead to significantly different results. 

% Inspired by our analysis above, we thus propose to use the structural properties suggested in \cref{thm:equivalence} as a way to bridge this gap and, possibly improve the overall accuracy. As we demonstrate in \cref{sec:application}, our proposed modifications while being relatively simple, significantly improve the quality of the computed correspondences, especially in ``difficult'' matching scenarios.

Motivated by our analysis, we propose to use the structural properties suggested in \cref{thm:equivalence} as a way to bridge this gap and improve the overall accuracy. Our proposed modifications are relatively simple, but they significantly improve the quality of computed correspondences, especially in ``difficult'' matching scenarios, as we demonstrate in \cref{sec:application}.

% The two key assumptions in \cref{thm:equivalence} are \textit{basis-aligning} functional maps and \textit{complete} feature extractors. We thus propose to modify the functional map pipeline so that the conditions of the theorem are satisfied. As mentioned above, the basis-aligning property is closely related to \textit{properness} and thus we propose to approach it by imposing that the predicted functional map arises from some pointwise correspondence. For feature completeness, we propose a simple modification of the feature extractor so that it produces \textit{smooth features}. In what follows, we will use the same notation as \cref{sec:notation}.

The two key assumptions in \cref{thm:equivalence} are \textit{basis-aligning} functional maps and \textit{complete} feature extractors. We propose modifying the functional map pipeline to satisfy the conditions of the theorem. Since the basis-aligning property is closely related to \textit{properness}, we propose to impose that the predicted functional map to be proper, \ie arises from some pointwise correspondence. For feature completeness, we suggest modifying the feature extractor to produce \textit{smooth features}. We use the same notation as in \cref{sec:notation}.


\subsection{Enforcing Properness}
\label{sec:proper_fmap}

In this section, we propose two ways to enforce functional map properness and associated losses  for both supervised and unsupervised training.

\paragraph{The adjoint method}
Given feature functions $F_1, F_2$, produced by a feature extractor, we compute the functional map $\C_{12-pred}$ as explained in \cref{sec:background}. To compute a proper functional from it, we first convert $\C_{12-pred}$ into a p2p map $\Pi_{21-pred}$ in a differentiable way and then compute the ``differentiable'' proper functional map $\C_{21-proper} = \Phi_{2}^{\dagger} \Pi_{21-pred} \Phi_{1}$. 

To compute $\Pi_{21-pred}$, denoting $G_1 = \Phi_{1}$ and $G_2 = \Phi_{2}\C_{21-pred}$, we use:
\begin{align}
& \Pi_{21-pred}^{i, j} = \dfrac{\exp\big(\langle G_2^{i}, G_1^{j}\rangle / \tau\big)}{\sum_{k=1}^{n_1}\exp\big(\langle G_2^{i}, G_1^{k} \rangle / \tau\big)}.\label{eq:diff_p2p}%\\
%&s(\mathbf{x}, \mathbf{y}) = \mathbf{x} \cdot \mathbf{y}.\label{eq:FeatureDistanceFunc}
\end{align}
%
Here $\langle \cdot,\cdot \rangle$ is the scalar product measuring the similarity between $G_1$ and $G_2$, and $\tau$ is a temperature hyper-parameter. $\Pi_{21-pred}$ can be seen as a soft point-to-point map, formulated based on the adjoint conversion method described in \cite{Pai_2021_CVPR}, and computed in a differentiable manner, hence it can be used inside a neural network.

\paragraph{The feature-based method}
The feature-based method is similar to the adjoint method in spirit, the only difference being that $\Pi_{21-pred}$ is computed using the predicted features instead of the fmap. For this, we use \cref{eq:diff_p2p}, with $G_1 = F_1$ and $G_2 = F_2$. The modified deep functional map pipeline is illustrated in \cref{fig:fmap-pipeline}.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/fmap_pipeline.pdf}
    \caption{An overview of our revised deep functional map pipeline. The extracted features are used to compute the functional map and the proper functional map, as explained in \cref{sec:proper_fmap}}
    \label{fig:fmap-pipeline}
    \vspace{-1em}
\end{figure}

In addition to $C_{12-pred}$, the two previous methods allow to calculate $C_{21-proper}$. We adapt the functional map losses to take into account this modification.

In the supervised case, we modify the supervised loss (see \cref{eq:sup_loss}) by simply introducing an additional term into the loss: 
\begin{align}
\mathcal{L}_{proper} = \| \C_{12-pred} - \C_{12-proper} \|_F^2 \label{eq:sup_loss_proper}.
\end{align}

The motivation behind this loss is that we want the predicted functional map to be as close as possible to the ground truth and stay within the space of proper functional maps.% a proper one , the gradient is strong enough to force the features to produce a  using \cref{eq:fmap_basic}, that is close to a proper one.

In the unsupervised setting, we simply impose the standard unsupervised losses on the differentiable proper functional map $\C_{12-proper}$ instead of $\C_{12-pred}$. Specifically, in our experiments below, we use the following unsupervised losses: 
%
\begin{align}
\nonumber
\mathcal{L}_{unsup}(\C_{12}, \C_{21}) &= \| \C_{12} \C_{21} - \mathbb{I} \|_F^2 + \| \C_{21} \C_{12} - \mathbb{I} \|_F^2 \\
& + \| \C_{12}^{\top} \C_{12} - \mathbb{I} \|_F^2 + \| \C_{21}^{\top} \C_{21} - \mathbb{I} \|_F^2
\label{eq:unsup_loss_proper}
\end{align}

%\maks{add the losses here.}


% \souhaib{how to justify that the results obtained with NN are better than fmap}












%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{As Smooth As Possible Feature Extractor}
Another fundamental assumption of \cref{thm:equivalence} is the completeness of the features produced by a neural network. 

We have experimented with several ways to impose it and have found that it is not easy to satisfy it exactly in general because it would require the network to always produce features in some target  subspace, which is not explicitly specified in advance. Moreover, we have found that explicitly projecting feature functions to a small reduced subspace can also hinder learning. 

To circumvent this, we propose instead to \textit{encourage} this property by promoting the feature extractor to produce smooth features. 

The motivation for this is as follows. If $F_i$ is complete, then there exist coefficients $a_1 ... a_k$ such that $F_i = \sum_{j=1}^k a_j \Phi_i^j$, where $k$ is the size of the functional map used in \cref{eq:fmap_basic}.
However, it's known that Fourier coefficients for smooth functions decay rapidly (faster than any polynomial, if $f$ is of class $C^l$, the coefficients are $o(n^{-l})$), which means that the smoother the function is, the closer it will be to being complete for some index $k$.

Inspired by this, we propose the following simple modification to feature extractors used for deep functional maps. Since feature extractors are made of multiple layers, we propose to project the output of each layer into the Laplacian basis, diffuse it over the surface following \cite{sharp2021diffusion}, and then project it back to the ambient space before feeding it to the next layer, see \cref{fig:feat-extract-modif}. Concretely, for shape $S$, if $f_i$ is the output of layer $i$, we feed to layer $i+1$ the function $f^{'}_i$, such that $f^{'}_i = \Phi_j e^{-t \Delta} \Phi_j^{\dagger} f_i$, where $\Phi_j$ denotes the first $j$ eigenfunctions of the Laplacian, $\Delta$ is a diagonal matrix containing the first j eigenvalues, and $t$ is a learnable parameter. Please note there is no need to do this operation for the final layer, since the features will be projected into the Laplacian basis anyway, for computing the functional map. In practice, we observed that it is beneficial to set $j$ to \textit{be larger} than the size of the functional maps in \cref{eq:fmap_basic}. This allows the network to impose smoothness, while still allowing degrees of freedom to enable optimization.

%Also note, that DiffusionNet \cite{sharp2021diffusion} does this operation by construction for each layer, which can in part explain its success.
%
%\souhaib{what about the receiptive field}


%
%
% - smooth the input before feeding them to the network (doesn't work practically)
% 
% - smooth the features at the end of each layer
% 
% - for better results, increase the receiptive field of the features using diffusion 
%
%
\vspace{-1em}

\paragraph{Implementation details} we provide implementation details, for all our experiments, in the supplementary. Our code and data will be released after publication.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/feature_extractor.pdf}
    \caption{An overview of the feature extractor modification is shown here. The features are made smooth by projecting them into the Laplacian basis at the end of each layer.}
    \label{fig:feat-extract-modif}
    % \vspace{-1.5em}
\end{figure}