\section{Related Work}
\label{sec:related}

Non-rigid shape matching is a very rich and well-established research area, and a full overview is beyond
the scope of this paper.  Below, we review works that are close to our method and  refer the interested reader to recent surveys \cite{cao2020comprehensive,bronstein2017geometric,guo2016comprehensive,guo2020deep} for a more in-depth treatment.

\tightpara{Functional Maps}
Since its introduction \cite{Ovsjanikov2012}, the functional map pipeline has become a widely-used tool for non-rigid shape matching, being adopted and extended in many follow-up works \cite{ginzburg2019cyclic,Ren2019,eynard2016coupled,Melzi_2019,eynard2016coupled,Nogneng2017,rodola2017partial,poulenard_persistence,sharma2020weakly}. The advantage of this approach is that it reduces the optimization of pointwise maps, which are quadratic in the number of vertices, to optimizing small matrices, thus greatly simplifying computational problems. We refer to \cite{Ovsjanikov2017} for an overview. 

The original functional map pipeline relied on input feature (probe) functions, which were given a priori \cite{Salti2014,sun2009concise,aubry2011wave}.
% , to estimate the underlying functional maps. Most earlier works in this domain used hand-crafted features \cite{Salti2014,sun2009concise,aubry2011wave}. 
Subsequent research has improved the method by adapting it to partial shapes \cite{cosmo2016shrec,rodola2017partial,Litany2017} or using robust regularizers  %estimation pipeline by introducing robust regularizers and penalties 
\cite{Ren2019,Nogneng2017,kovnatsky2013coupled,burghard2017embedding} and proposed efficient refinement techniques \cite{Melzi_2019,Pai_2021_CVPR,jing_maptree}. %, and combined it with extrinsic shape alignment techniques and optimal transport tools \cite{aygun2020unsupervised,Eisenberger2020SmoothSM,pmf}. 
In all of these works, fmaps were computed using hand-crafted probe functions, and any information loss in these descriptors hinders downstream optimization.

More recent works have proposed to solve this problem by learning descriptors (probe functions) directly from the data, using deep neural networks. This line of research was initiated by FMNet \cite{litany2017deep}, and extended in many subsequent works \cite{halimi2019unsupervised,attaiki2023vader,roufosse2019unsupervised,sharma2020weakly,sharp2021diffusion,attaiki2021dpfm,donati2020deep,Eisenberger2021NeuroMorphUS,attaiki2022ncp,eisenberger2020deep,li2022srfeat}. These methods learn the probe features directly from the raw geometry, using a neural network, and supervise the learning with a loss on the functional map in the reduced basis.

% Litany \etal \cite{litany2017deep} proposed to refine the input SHOT descriptors \cite{Salti2014} using a pointwise MLP, and supervised the training with a loss with ground-truth pointwise map. Follow-up approaches, such as \cite{donati2020deep} improved this framework by learning the probe features directly from the raw geometry, using a neural network, and supervising the learning with a loss on the functional map in the reduced basis. Subsequent work further improved the method, making it robust to mesh discretization \cite{sharp2021diffusion} and adapting it to handle partial shapes, achieving state-of-the-art results in many non-rigid matching scenarios.

A parallel line of work has focused on making the learning \textit{unsupervised}, which can be convenient in the absence of ground truth correspondences. Approaches in \cite{halimi2019unsupervised} and \cite{Ginzburg2020} have proposed penalizing either the geodesic distortion of the pointwise map predicted by the network or using the cycle consistency loss.  %However, such approaches require the computation and storage of heavy geodesic matrices, and has shown little generalizability. 
Another line of work \cite{roufosse2019unsupervised,sharma2020weakly,sharp2021diffusion,donati-duo} considered unsupervised training by imposing structural properties on the functional maps in the reduced basis, such as bijectivity, orthonormality, and commutativity of the Laplacian. 
% Eisenberger et al. \cite{eisenberger2020deep} proposed to combine intrinsic and extrinsic alignment, in addition to a refinement of the maps in the network, at the cost of efficiency and computational time. 
The authors of \cite{sharma2020weakly} have shown that feature learning can be done starting from raw 3D geometry in the weakly supervised setting, where shapes are only approximately rigidly pre-aligned. 
% Finally, interpolation was also used a geometric prior to learning robust features in \cite{Eisenberger2021NeuroMorphUS}.

In all of these works, features extracted by neural networks have been used to formulate the optimization problem, from which a functional map is computed. Thus, %feature functions were only used algebraically, as part of an equation to solve the functional maps, and
so far no attention has been paid to the geometric nature or other utility of learned probe functions. In contrast, we aim to analyze the conditions under which probe functions can be used for direct pointwise map computation and use this analysis to design improved map estimation pipelines. 

We also note briefly a very recent work  \cite{li2022srfeat} which has advocated for imposing feature smoothness  when learning for non-rigid shape correspondence. However, that work does not use the in-network functional map estimation and moreover lacks any theoretical analysis or justification for its design choices. % their content in general. Thus, the exact nature of information learned within these 
%
%\souhaib{should we say that sharma requires the shape to be aligned, because we will be using this later}



\tightpara{Recent Advances in Axiomatic Functional Maps}
Our results are also related to recent axiomatic methods for functional map-based methods, which  \textit{couple} optimization for the functional map (fmap) with the associated point map (p2p map) \cite{ren2018continuous,Melzi_2019,Pai_2021_CVPR,Xiang_2021_CVPR}. These techniques typically propose to refine functional maps by \textit{iterating} the conversion from functional to pointwise maps and vice versa. This approach was recently summarized in \cite{discrete_Ren2021}, where the authors introduce the notion of functional map ``properness'' and describe a range of energies that can be optimized via this iterative conversion scheme. 

The common denominator between all these methods is the inclusion of pointwise maps in the process of functional map optimization. In this work, we propose a method and a loss that similarly incorporate pointwise map computation. However, we do so in a learning context and show that this leads to significant improvements in the overall accuracy of the deep functional map pipeline.



\tightpara{Learning on Surfaces}
Multiple methods for deep surface learning have been proposed to address the limitations of handcrafted features in downstream tasks. One type of method is point-based (extrinsic) methods, such as PointNet \cite{qi2017pointnet} and follow-up works \cite{qi2017pointnet++,rethink_ma_22,thomas2019KPConv,dgcnn,pcnn_2018,Wiersma2022DeltaConv}. These methods are simple, effective, and widely applicable. However, they often fail to generalize to new datasets or significant pose changes in deep shape matching. 

Another line of research \cite{poulenard2018multi,sharp2021diffusion,wiersma2020cnns,gong2019spiralnet++,masci2015geodesic,maron2017convolutional} (intrinsic methods) has focused on defining the convolution operator directly on the surface of the input shape. These methods are more suitable for deformable shapes and can leverage the structure of the surface encoded by the mesh, which is ignored by the extrinsic methods.


% To overcome the limitations of handcrafted features in downstream tasks, multiple methods for deep surface learning have been proposed. The first type of method is point-based (extrinsic) methods, pioneered by \cite{qi2017pointnet}, and extended by many works such as \cite{qi2017pointnet++,rethink_ma_22,thomas2019KPConv,dgcnn,pcnn_2018,Wiersma2022DeltaConv} to name a few. These methods are characterized by their simplicity, effectiveness, and applicability in a wide range of domains. Many of these methods have been adapted in the case of deep shape matching \cite{sharma2020weakly,donati2020deep}, and while they perform well in normal scenarios (the test dataset is similar to the training dataset), they often fail to generalize to new datasets or under significant pose changes.

% Another line of research \cite{poulenard2018multi,sharp2021diffusion,wiersma2020cnns,gong2019spiralnet++,masci2015geodesic,maron2017convolutional} (intrinsic methods) has focused on defining the convolution operator directly on the surface of the input shape. The motivation is that these methods are more suitable for deformable shapes, and can benefit from the structure of the surface encoded by the mesh, which is ignored by the first category of methods.

In previous works, intrinsic methods tend to perform better for non-rigid shape matching than extrinsic methods, with DiffusionNet \cite{sharp2021diffusion} being considered the state-of-the-art feature extractor for shape matching. In this work, we will show that a simple modification to extrinsic feature extractors improves their overall performance for shape matching, making them comparable to or better than DiffusionNet. 
