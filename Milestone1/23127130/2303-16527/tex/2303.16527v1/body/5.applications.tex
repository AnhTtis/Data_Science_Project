\section{Results \& Applications}
\label{sec:application}

In this section, we provide results in a wide range of challenging tasks, showing the efficiency and
applicability of our approach to different types of data and feature extractors.% In particular, we show in \maks{incomplete sentence?}

\paragraph{Datasets} 
Our method is evaluated on five datasets commonly used in literature, which include both human and animal datasets.

To evaluate our method's performance for shape matching with humans, we use three datasets: FAUST \cite{Bogo2014}, SCAPE \cite{Anguelov2005}, and SHREC'19 \cite{shrec19}. We utilize the remeshed versions of the first two datasets introduced in \cite{Ren2019} and used in various follow-up works such as \cite{donati2020deep,sharma2020weakly,sharp2021diffusion,eisenberger2020deep,Eisenberger2021NeuroMorphUS,Eisenberger2020SmoothSM}. We follow the same train/test splits as in prior works.
% The FAUST Remeshed (\textbf{FR}) dataset consists of 100 human meshes, divided into 80/20 train/test splits as in prior works. Similarly, the SCAPE Remeshed (\textbf{SR}) dataset comprises 71 human meshes split into 51/20 shapes for training/testing. We also use the more challenging SHREC'19 dataset (SH) consisting of 44 human shapes with variations in pose, identity, and connectivity. We use its remeshed version and split it into 440 pairs exclusively used for testing.
For unsupervised experiments, we utilize the datasets' oriented versions, as described in \cite{sharma2020weakly}, denoted as \textbf{FA}, \textbf{SA}, and \textbf{SHA} for FAUST remeshed aligned, SCAPE remeshed aligned, and SHREC aligned, respectively.

We also evaluate our method on human segmentation using the dataset introduced in \cite{maron2017convolutional}, which comprises segmented human models from various prior datasets. We use the same test split as in prior works.% but only use \textbf{one shape} for training, which constitutes 0.3\% of the total training set.

For animal datasets, we use the SMAL-based dataset \cite{Zuffi:CVPR:2017,marin22_why} (denoted as \textbf{SMAL} hereafter), which consists of 50 organic, non-isometric, and non-rigid shapes represented as 3D meshes. We divide them into 25/25 shapes for training and testing, and to test the generalization capacity of our models, the animals and their positions during testing are never seen during training.









% We test our method on five human and animal datasets, widely used in the literature. 

% Concerning the human dataset for shape matching, we test our method on FAUST \cite{Bogo2014}, SCAPE \cite{Anguelov2005} and SHREC'19 \cite{shrec19} datasets. For the first two datasets, we use their remeshed version, introduced in \cite{Ren2019}, and used in many follow-ups works such as \cite{donati2020deep,sharma2020weakly,sharp2021diffusion,eisenberger2020deep,Eisenberger2021NeuroMorphUS,Eisenberger2020SmoothSM} to name a few. FAUST Remeshed (\textbf{FR}) dataset is composed of 100 human meshes in different positions. Following prior work \cite{donati2020deep,eisenberger2020deep}, we divide the dataset into train/test splits using 80/20 shapes respectively. SCAPE Remeshed (\textbf{SR}) dataset is composed of 71 human meshes, split into 51/20 shapes for training/testing. Finally, we use the more recent and more challenging \textbf{SHREC'19} dataset \textbf{SH} \cite{shrec19, Ren2019}. This dataset is composed of 44 human shapes with great changes in pose, identity, and connectivity. This resulted in 440 pairs used exclusively for testing. In addition to these datasets, we use their oriented versions for the unsupervised experiments, following the protocol of \cite{sharma2020weakly}. In particular, the datasets are oriented in a way  that they have a consistent ``up'' direction (along, \eg, the y-axis) and an approximate forward-facing direction (along, \eg, the
% z-direction). The resulting datasets are denoted \textbf{FA}, \textbf{SA}, \textbf{SHA} for FAUST remeshed aligned, SCAPE remeshed aligned, and SHREC aligned respectively.

% In addition to the above human datasets used for matching, we also test our method on the scenario of human segmentation using the dataset introduced in \cite{maron2017convolutional}, which combines segmented human models taken from a variety of prior datasets. We use the same test split as in prior works \cite{maron2017convolutional,sharp2021diffusion}, however, as mentioned below, we only use \textbf{one shape} for training, which constitutes 0.3\% of the total training set.

% For the animal dataset, we use the organic, non-isometric non-rigid SMAL-based dataset \cite{Zuffi:CVPR:2017,marin22_why}. This dataset consists of 50 shapes represented as 3D meshes, divided into 25/25 shapes for training and testing. To test the generalization capacity of the different models, the animals and the position seen during the tests are never seen during the training.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Non-Rigid Shape Matching}
In this section, we evaluate our modification to the deep functional map pipeline in three different and challenging settings, including near-isometric supervised matching in \cref{sec:supervised-human}, near-isometric unsupervised shape matching in \cref{sec:unsup-human}, and finally, non-isometric non-rigid matching, both in supervised and unsupervised settings, in \cref{sec:animals-matching}.

We evaluate our proposed modifications in the presence of three different feature extractors, an intrinsic feature extractor DiffusionNet \cite{sharp2021diffusion} which is considered as the state of the art for shape matching, and two extrinsic feature extractors, DGCNN \cite{dgcnn} and DeltaConv \cite{Wiersma2022DeltaConv}, which operate on point clouds. We will show that with our modifications, the latter feature extractors can surpass DiffusionNet in some scenarios.

% \paragraph{Evaluation protocol} We follow the protocol of \cite{Kim2011} to evaluate all correspondences, used in all recent works. Namely, we measure the geodesic error of predicted maps with respect to the given ground truth and normalize the scale by the square root of the total surface area. All the values are multiplied by $\times 100$ for clarity.

\paragraph{Evaluation protocol} Our evaluation protocol, following \cite{Kim2011}, measures the geodesic error between predicted maps and the given ground truth, normalized by the square root of the total surface area. All reported values are multiplied by $\times 100$ for clarity.

% In all experiments below, The notation ``X on Y'' means that we train on X and test on Y. Also, we will use \textbf{FM} to denote the point-to-point map extracted by converting the functional map, and \textbf{NN} to denote the point-to-point map extracted by the nearest neighbor in the feature space. Finally, the notation ``Feature Extractor - Ours'', means for the case of DGCNN and DeltaConv, that we train the network with both the smoothness block as well as the properness enforced during training, meanwhile, for DiffusionNet, we only enforce the properness, since the smoothness is enforced by construction. For properness, we will report results only for the feature-based method, and we will include results using the adjoint method in the supplementary.

In our experiments, we use the notation ``X on Y'' to indicate training on X and testing on Y. We use \textbf{FM} to represent the point-to-point map derived from the functional map and \textbf{NN} for the point-to-point map extracted by nearest neighbor in the feature space. We use ``Feature Extractor - Ours'' to denote our training approach for DGCNN and DeltaConv, which involves training the network with both smoothness and properness enforced, while for DiffusionNet, we only enforce properness since smoothness is enforced by construction. We report properness results only for the feature-based method and include adjoint method results in the supplementary materials.

%\souhaib{properness enforced == using loss from section above}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Supervised Shape Matching}
\label{sec:supervised-human}

\input{tables/tab_supervised_humans.tex}

In this section, we test the quality of our proposed modifications in the supervised setting. For this, we follow the same pipeline described in \cref{sec:background}, and train our network with the supervised loss in \cref{eq:sup_loss_proper}. Also, to stress the generalization power of each network, we do multiple tests, by training on one dataset, and testing on another one, following multiple previous works \cite{donati2020deep,sharma2020weakly,eisenberger2020deep}. For this, we use the \textbf{FR}, \textbf{SR}, and \textbf{SH} datasets. The results are summarized in \cref{tab:supervised_humans}.

First of all, we can see that our approach improves the global result for all feature extractors, on all datasets. The performance increase can be up to 80\%, as is the case for DeltaConv and DGCNN on the scenario \textbf{FR} on \textbf{SR}, where \eg the error has decreased from 18.1 to 3.6.

Another interesting result to note is that using our modifications, the extrinsic feature extractors, which were failing in the generalization scenarios, now perform as well as DiffusionNet.
%
Finally, and perhaps more \textit{importantly}, after our modifications, the results obtained with the nearest neighbor are much better, and get closer to the maps extracted by converting the functional map, demonstrating, for the first time, that the features learned in the deep fmap framework do have a geometric meaning, and can be used directly for matching, even without the need to compute a functional map. We explain the slight discrepancy between the result obtained with fmap and NN by the fact that, in practice, the conditions of the theorem are not 100\% satisfied, since we only used approximations to obtain them. In \cref{fig:hum_matching_one}, we show the quality of the produced maps, before and after our modifications. It can be seen that our modifications produce visually plausible correspondences.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/hum_matching_one.pdf}
    \caption{ Qualitative results on SCAPE Remeshed dataset using DeltaConv \cite{Wiersma2022DeltaConv}, before and after our modifications.}
    \label{fig:hum_matching_one}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Unsupervised Shape Matching}
\label{sec:unsup-human}



In this section, we test the quality of our modifications in the case of unsupervised learning on near-isometric data. We follow the same method as explained in \cref{sec:background} with the loss in \cref{eq:unsup_loss_proper}, and use three feature extractors: DGCNN, DiffusionNet and DeltaConv, and three datasets: \textbf{FA}, \textbf{SA} and \textbf{SHA}. The results are summarized in \cref{tab:unsup_humans}.

%\souhaib{the unsupervised losses are not mentioned in the text, is it a problem}

As for the supervised case, our modifications improve the results for all scenarios and for all feature extractors. For example, the result improved by over 70 \% for DeltaConv on the \textbf{FRA} on \textbf{SHA} scenario.

In addition, the extrinsic feature extractors are now on par with DiffusionNet, and the NN results are now close to those obtained with fmap, reinforcing the conclusions obtained in the supervised case \cref{sec:supervised-human}.

\input{tables/tab_unsup_humans.tex}

Note that the effect of enforcing properness is more visible in this context. This can be seen for example in the case of DiffusionNet in Table \cref{sec:supervised-human} since for this feature extractor the only change brought about by our method is via properness. In fact, in the supervised setting, since the training is done using the ground truth-functional maps, and these are proper, this forced the network to learn features that produce functional maps that are as proper as possible, whereas in the unsupervised setting, the network is trained with losses that impose structural properties on the functional maps such as orthogonality, and no properness is involved. We can see that after imposing properness, both functional map and NN results improve, while also getting closer to each other.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Non-Isometric Shape Matching}
\label{sec:animals-matching}
\input{tables/tab_animals.tex}

In this section, we test the utility of our modifications in the case of non-rigid non-isometric shape matching, on the \textbf{SMAL} dataset. We test it both in the case of supervised and unsupervised learning, following the same procedure as in \cref{sec:supervised-human} and \cref{sec:unsup-human}. The results are summarized in \cref{tab:match_animals}. As in the previous sections, our modifications improve the results for all feature extractors for both supervised and unsupervised matching. For example, the results for supervised DeltaConv improve by approximately 80\%, from 20.7 to 4.2.

More surprisingly, using our modifications, in the non-isometric setting, DiffusionNet is surpassed as the best feature extractor for shape matching, as DGCNN and DeltaConv achieve better results, using either the functional map or NN method to extract the p2p map. %\souhaib{why this happens?}

%\souhaib{should we add another remark that I missed?}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Generalization Power of Geometric Features}

% \input{tables/tab_generalization.tex}

% We conclude our experiment section by testing the geometric nature of the features learned in the deep fmap pipeline. In this experiment, we aim to see how much information is included in the features themselves. If they are informative enough, they can be used for other tasks with little fine-tuning.

% To do this, we use the human segmentation dataset presented in \cite{maron2017convolutional}. We first extract features by a network pre-trained on shape matching, and then refine these features for the segmentation task. For this, we will use the pre-trained DiffusionNet using \textbf{SA} dataset and the unsupervised loss. 
% We will refine the features using a point-wise MLP, which allows the quality of the features to be reflected without resorting to complex convolution and pooling operations. 

We conclude our experiments by evaluating the generalization ability of the learned features in the deep functional map pipeline. Our goal is to investigate whether the extracted features contain sufficient information to be used for other tasks without the need for significant fine-tuning.

To conduct this experiment, we employ the human segmentation dataset presented in \cite{maron2017convolutional}. We first extract features from this dataset using a network that has been pre-trained on shape matching. We use the pre-trained DiffusionNet on the \textbf{SA} dataset with the unsupervised. Next, we refine the features for the segmentation task using only a small fraction of the training data. To accomplish this, we utilize a point-wise MLP to avoid complex convolution and pooling operations and to provide an accurate measure of the feature quality.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/hum_seg_ev.pdf}
    \caption{Evolution of human segmentation accuracy as a function of the number of shapes used for the pointwise MLP training.}
    \label{fig:hum_seg_ev}
     \vspace{-0.5em}
\end{figure}

In \cref{fig:hum_seg_ev}, we plot the accuracy of the MLP 
%trained with the pre-trained features used previously,
as a function of the number of finetuning shapes. First note that the accuracy of the results improves with the number of training shapes (up to a certain point, where most likely the limited capacity of pointwise MLP is reached). Note also that our features, which are pre-trained in a completely unsupervised manner, produce \textit{significantly} higher accuracy than the raw XYZ coordinates. We attribute this feature utility in a downstream task to the fact that these features capture the geometric structure of the shapes in a compact and invariant manner.
To further demonstrate the generalization power of the pretrained features, we conducted a similar experiment on the RNA dataset \cite{poulenard2019effective} and present the results in the supplementary material.

%In addition, we use only \textbf{one shape} for training the point-wise MLP, which represents \textbf{0.3\% of the training set}. The results are summarized in \cref{tab:generalisation}. For robustness, we repeat the experiment 5 times by randomly selecting the training shape, and reporting the mean and standard deviation.
%\maks{we should promise more results (at least variance of results depending on the choice of the training shape) in the sup.mat. otherwise we might get hit because the numbers might not be reliable.}

% We find that the performance of our features is equal to or better than that of some complex 3D networks that are trained on the \textit{complete training set}. We attribute this result to the robust geometric information encoded in the pre-trained features. 
% We note that we do not claim SOTA results on this dataset, but rather aim to demonstrate the potential of using learned deep functional map features in other tasks, going beyond their current purely algebraic role in formulating linear systems for functional maps.



\tightpara{Ablation Studies} 
% Our method includes two modifications to the deep functional map pipeline that we consider essential for optimal performance. 
In the supplementary, we report an ablation study that demonstrates the effectiveness of the individual components we introduced, in addition to a comparison of our modified pipeline to other recent baseline methods for shape matching.
