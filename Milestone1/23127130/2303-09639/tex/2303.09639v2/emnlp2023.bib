@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2019roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@misc{sanh2020distilbert,
      title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, 
      author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
      year={2020},
      eprint={1910.01108},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{liu2020search,
      title={Search to Distill: Pearls are Everywhere but not the Eyes}, 
      author={Yu Liu and Xuhui Jia and Mingxing Tan and Raviteja Vemulapalli and Yukun Zhu and Bradley Green and Xiaogang Wang},
      year={2020},
      eprint={1911.09074},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{wang2020minilm,
      title={MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers}, 
      author={Wenhui Wang and Furu Wei and Li Dong and Hangbo Bao and Nan Yang and Ming Zhou},
      year={2020},
      eprint={2002.10957},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mukherjee2021xtremedistiltransformers,
      title={XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation}, 
      author={Subhabrata Mukherjee and Ahmed Hassan Awadallah and Jianfeng Gao},
      year={2021},
      eprint={2106.04563},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{shakeri-etal-2020-end,
    title = "End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems",
    author = "Shakeri, Siamak  and
      Nogueira dos Santos, Cicero  and
      Zhu, Henghui  and
      Ng, Patrick  and
      Nan, Feng  and
      Wang, Zhiguo  and
      Nallapati, Ramesh  and
      Xiang, Bing",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.439",
    doi = "10.18653/v1/2020.emnlp-main.439",
    pages = "5445--5460",
}
@inproceedings{sultan-etal-2020-importance,
    title = "On the Importance of Diversity in Question Generation for {QA}",
    author = "Sultan, Md Arafat  and
      Chandel, Shubham  and
      Fernandez Astudillo, Ram{\'o}n  and
      Castelli, Vittorio",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.500",
    doi = "10.18653/v1/2020.acl-main.500",
    pages = "5651--5656",
}
@inproceedings{bornea2021multilingual,
  title={Multilingual Transfer Learning for QA using Translation as Data Augmentation},
  author={Bornea, Mihaela and Pan, Lin and Rosenthal, Sara and Florian, Radu and Sil, Avirup},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={12583--12591},
  year={2021}
}

@inproceedings{chakravarti2020towards,
  title={Towards building a robust industry-scale question answering system},
  author={Chakravarti, Rishav and Ferritto, Anthony and Iyer, Bhavani and Pan, Lin and Florian, Radu and Roukos, Salim and Sil, Avirup},
  booktitle={Proceedings of the 28th International Conference on Computational Linguistics: Industry Track},
  pages={90--101},
  year={2020}
}

@misc{elsken2019neural,
      title={Neural Architecture Search: A Survey}, 
      author={Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},
      year={2019},
      eprint={1808.05377},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{wang2019glue,
      title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}, 
      author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
      year={2019},
      eprint={1804.07461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bowman2015large,
      title={A large annotated corpus for learning natural language inference}, 
      author={Samuel R. Bowman and Gabor Angeli and Christopher Potts and Christopher D. Manning},
      year={2015},
      eprint={1508.05326},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jin2020bert,
      title={Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment}, 
      author={Di Jin and Zhijing Jin and Joey Tianyi Zhou and Peter Szolovits},
      year={2020},
      eprint={1907.11932},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{du2020selftraining,
      title={Self-training Improves Pre-training for Natural Language Understanding}, 
      author={Jingfei Du and Edouard Grave and Beliz Gunel and Vishrav Chaudhary and Onur Celebi and Michael Auli and Ves Stoyanov and Alexis Conneau},
      year={2020},
      eprint={2010.02194},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{zoph2018learning,
      title={Learning Transferable Architectures for Scalable Image Recognition}, 
      author={Barret Zoph and Vijay Vasudevan and Jonathon Shlens and Quoc V. Le},
      year={2018},
      eprint={1707.07012},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{tan2019mnasnet,
      title={MnasNet: Platform-Aware Neural Architecture Search for Mobile}, 
      author={Mingxing Tan and Bo Chen and Ruoming Pang and Vijay Vasudevan and Mark Sandler and Andrew Howard and Quoc V. Le},
      year={2019},
      eprint={1807.11626},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{bashivan2019teacher,
      title={Teacher Guided Architecture Search}, 
      author={Pouya Bashivan and Mark Tensen and James J DiCarlo},
      year={2019},
      eprint={1808.01405},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Wang_2021, title={Teacher Guided Neural Architecture Search for Face Recognition}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/16387}, number={4}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Wang, Xiaobo}, year={2021}, month={May}, pages={2817-2825} }

@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{fedus2021switch,
      title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity}, 
      author={William Fedus and Barret Zoph and Noam Shazeer},
      year={2021},
      eprint={2101.03961},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zoph2017neural,
      title={Neural Architecture Search with Reinforcement Learning}, 
      author={Barret Zoph and Quoc V. Le},
      year={2017},
      eprint={1611.01578},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhou2020econas,
      title={EcoNAS: Finding Proxies for Economical Neural Architecture Search}, 
      author={Dongzhan Zhou and Xinchi Zhou and Wenwei Zhang and Chen Change Loy and Shuai Yi and Xuesen Zhang and Wanli Ouyang},
      year={2020},
      eprint={2001.01233},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{liu2019darts,
      title={DARTS: Differentiable Architecture Search}, 
      author={Hanxiao Liu and Karen Simonyan and Yiming Yang},
      year={2019},
      eprint={1806.09055},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{so2019evolved,
      title={The Evolved Transformer}, 
      author={David R. So and Chen Liang and Quoc V. Le},
      year={2019},
      eprint={1901.11117},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{huang2015bidirectional,
      title={Bidirectional LSTM-CRF Models for Sequence Tagging}, 
      author={Zhiheng Huang and Wei Xu and Kai Yu},
      year={2015},
      eprint={1508.01991},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{ba_2014,
 author = {Ba, Jimmy and Caruana, Rich},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Do Deep Nets Really Need to be Deep?},
 url = {https://proceedings.neurips.cc/paper/2014/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf},
 volume = {27},
 year = {2014}
}

@article{McCulloch1990ALC,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={Warren S. McCulloch and Walter Pitts},
  journal={Bulletin of Mathematical Biology},
  year={1990},
  volume={52},
  pages={99-115}
}

@Techreport{Rosenblatt_1957_6098,
  author = {Rosenblatt, F.},
 address = {Ithaca, New York},
 institution = {Cornell Aeronautical Laboratory},
 month = {January},
 number = {85-460-1},
 title = {The perceptron - A perceiving and recognizing automaton},
 year = {1957},
 title_with_no_special_chars = {The Perceptron  A perceiving and recognizing automaton}
}

@article{LSTM,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

@inproceedings{LSTMBakker,
 author = {Bakker, Bram},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 pages = {},
 publisher = {MIT Press},
 title = {Reinforcement Learning with Long Short-Term Memory},
 url = {https://proceedings.neurips.cc/paper/2001/file/a38b16173474ba8b1a95bcbc30d3b8a5-Paper.pdf},
 volume = {14},
 year = {2002}
}

@INPROCEEDINGS{BookCorpus,
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books}, 
  year={2015},
  volume={},
  number={},
  pages={19-27},
  doi={10.1109/ICCV.2015.11}}
  
@inproceedings{wieting-gimpel-2018-paranmt,
    title = "{P}ara{NMT}-50{M}: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations",
    author = "Wieting, John  and
      Gimpel, Kevin",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1042",
    doi = "10.18653/v1/P18-1042",
    pages = "451--462"
}

@misc{jiao2020tinybert,
      title={TinyBERT: Distilling BERT for Natural Language Understanding}, 
      author={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
      year={2020},
      eprint={1909.10351},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ernie2AAAI2020, 
    title={ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding}, 
    author={Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Tian, Hao and Wu, Hua and Wang, Haifeng},
    volume={34}, 
    number={05}, 
    journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
    pages={8968-8975},
    year={2020}
}

@misc{na2021accelerating,
      title={Accelerating Neural Architecture Search via Proxy Data}, 
      author={Byunggook Na and Jisoo Mok and Hyeokjun Choe and Sungroh Yoon},
      year={2021},
      eprint={2106.04784},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{yang2020nas,
  author    = {Antoine Yang and
               Pedro M. Esperan{\c{c}}a and
               Fabio Maria Carlucci},
  title     = {{NAS} evaluation is frustratingly hard},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2020}
}

@misc{turc2019pretrain,
  doi = {10.48550/ARXIV.1908.08962},
  url = {https://arxiv.org/abs/1908.08962},
  author = {Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title = {Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{clark2020electra,
      title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators}, 
      author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
      year={2020},
      eprint={2003.10555},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{xu2022survey,
  title={A survey on model compression for natural language processing},
  author={Xu, Canwen and McAuley, Julian},
  journal={arXiv preprint arXiv:2202.07105},
  year={2022}
}

@inproceedings{chen2021extract,
  title={Extract then distill: Efficient and effective task-agnostic bert distillation},
  author={Chen, Cheng and Yin, Yichun and Shang, Lifeng and Wang, Zhi and Jiang, Xin and Chen, Xiao and Liu, Qun},
  booktitle={Artificial Neural Networks and Machine Learning--ICANN 2021: 30th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 14--17, 2021, Proceedings, Part III 30},
  pages={570--581},
  year={2021},
  organization={Springer}
}

@inproceedings{mirzadeh2020improved,
  title={Improved knowledge distillation via teacher assistant},
  author={Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={04},
  pages={5191--5198},
  year={2020}
}

@inproceedings{liu-etal-2022-multi-granularity,
    title = "Multi-Granularity Structural Knowledge Distillation for Language Model Compression",
    author = "Liu, Chang  and
      Tao, Chongyang  and
      Feng, Jiazhan  and
      Zhao, Dongyan",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.71",
    doi = "10.18653/v1/2022.acl-long.71",
    pages = "1001--1011",
}

@inproceedings{wu-etal-2021-universal,
    title = "Universal-{KD}: Attention-based Output-Grounded Intermediate Layer Knowledge Distillation",
    author = "Wu, Yimeng  and
      Rezagholizadeh, Mehdi  and
      Ghaddar, Abbas  and
      Haidar, Md Akmal  and
      Ghodsi, Ali",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.603",
    doi = "10.18653/v1/2021.emnlp-main.603",
    pages = "7649--7661",
}

@inproceedings{wang-etal-2021-minilmv2,
    title = "{M}ini{LM}v2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers",
    author = "Wang, Wenhui  and
      Bao, Hangbo  and
      Huang, Shaohan  and
      Dong, Li  and
      Wei, Furu",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.188",
    doi = "10.18653/v1/2021.findings-acl.188",
    pages = "2140--2151",
}

@inproceedings{sun-etal-2019-patient,
    title = "Patient Knowledge Distillation for {BERT} Model Compression",
    author = "Sun, Siqi  and
      Cheng, Yu  and
      Gan, Zhe  and
      Liu, Jingjing",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1441",
    doi = "10.18653/v1/D19-1441",
    pages = "4323--4332",
}

@inproceedings{jafari-etal-2021-annealing,
    title = "Annealing Knowledge Distillation",
    author = "Jafari, Aref  and
      Rezagholizadeh, Mehdi  and
      Sharma, Pranav  and
      Ghodsi, Ali",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.212",
    doi = "10.18653/v1/2021.eacl-main.212",
    pages = "2493--2504",
}

@article{jiao2021improving,
  title={Improving task-agnostic bert distillation with layer mapping search},
  author={Jiao, Xiaoqi and Chang, Huating and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  journal={Neurocomputing},
  volume={461},
  pages={194--203},
  year={2021},
  publisher={Elsevier}
}

@misc{zagoruyko2017paying,
      title={Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer}, 
      author={Sergey Zagoruyko and Nikos Komodakis},
      year={2017},
      eprint={1612.03928},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{li-etal-2020-bert,
    title = "{BERT}-{EMD}: Many-to-Many Layer Mapping for {BERT} Compression with Earth Mover{'}s Distance",
    author = "Li, Jianquan  and
      Liu, Xiaokang  and
      Zhao, Honghong  and
      Xu, Ruifeng  and
      Yang, Min  and
      Jin, Yaohong",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.242",
    doi = "10.18653/v1/2020.emnlp-main.242",
    pages = "3009--3018",
}

@misc{conneau2020unsupervised,
      title={Unsupervised Cross-lingual Representation Learning at Scale}, 
      author={Alexis Conneau and Kartikay Khandelwal and Naman Goyal and Vishrav Chaudhary and Guillaume Wenzek and Francisco Guzmán and Edouard Grave and Myle Ott and Luke Zettlemoyer and Veselin Stoyanov},
      year={2020},
      eprint={1911.02116},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{ko2023revisiting,
      title={Revisiting Intermediate Layer Distillation for Compressing Language Models: An Overfitting Perspective}, 
      author={Jongwoo Ko and Seungjoon Park and Minchan Jeong and Sukjin Hong and Euijai Ahn and Du-Seong Chang and Se-Young Yun},
      year={2023},
      eprint={2302.01530},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{tenney2019bert,
      title={BERT Rediscovers the Classical NLP Pipeline}, 
      author={Ian Tenney and Dipanjan Das and Ellie Pavlick},
      year={2019},
      eprint={1905.05950},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{jawahar-etal-2019-bert,
    title = "What Does {BERT} Learn about the Structure of Language?",
    author = "Jawahar, Ganesh  and
      Sagot, Beno{\^\i}t  and
      Seddah, Djam{\'e}",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1356",
    doi = "10.18653/v1/P19-1356",
    pages = "3651--3657",
}

@inproceedings{wu-etal-2020-skip,
    title = "Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers",
    author = "Wu, Yimeng  and
      Passban, Peyman  and
      Rezagholizadeh, Mehdi  and
      Liu, Qun",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.74",
    doi = "10.18653/v1/2020.emnlp-main.74",
    pages = "1016--1021",
}


@misc{wolf2020huggingfaces,
      title={HuggingFace's Transformers: State-of-the-art Natural Language Processing}, 
      author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
      year={2020},
      eprint={1910.03771},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{conneau2018xnli,
      title={XNLI: Evaluating Cross-lingual Sentence Representations}, 
      author={Alexis Conneau and Guillaume Lample and Ruty Rinott and Adina Williams and Samuel R. Bowman and Holger Schwenk and Veselin Stoyanov},
      year={2018},
      eprint={1809.05053},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{rajpurkar2016squad,
      title={SQuAD: 100,000+ Questions for Machine Comprehension of Text}, 
      author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
      year={2016},
      eprint={1606.05250},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{clark2020tydi,
      title={TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages}, 
      author={Jonathan H. Clark and Eunsol Choi and Michael Collins and Dan Garrette and Tom Kwiatkowski and Vitaly Nikolaev and Jennimaria Palomaki},
      year={2020},
      eprint={2003.05002},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{tjong2003conll,
    title = "Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition",
    author = "Tjong Kim Sang, Erik F.  and
      De Meulder, Fien",
    booktitle = "Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003",
    year = "2003",
    url = "https://aclanthology.org/W03-0419",
    pages = "142--147",
}

@article{poletto21hatespeech,
	author = {Poletto, Fabio and Basile, Valerio and Sanguinetti, Manuela and Bosco, Cristina and Patti, Viviana},
	date = {2021/06/01},
	date-added = {2022-11-04 10:41:48 -0400},
	date-modified = {2022-11-04 10:41:48 -0400},
	doi = {10.1007/s10579-020-09502-8},
	id = {Poletto2021},
	isbn = {1574-0218},
	journal = {Language Resources and Evaluation},
	number = {2},
	pages = {477--523},
	title = {Resources and benchmark corpora for hate speech detection: a systematic review},
	url = {https://doi.org/10.1007/s10579-020-09502-8},
	volume = {55},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/s10579-020-09502-8}}
	
@misc{chen2021adabert,
      title={AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search}, 
      author={Daoyuan Chen and Yaliang Li and Minghui Qiu and Zhen Wang and Bofang Li and Bolin Ding and Hongbo Deng and Jun Huang and Wei Lin and Jingren Zhou},
      year={2021},
      eprint={2001.04246},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Xu_2021_nasbert,
	doi = {10.1145/3447548.3467262},
	url = {https://doi.org/10.1145%2F3447548.3467262},
	year = 2021,
	month = {aug},
	publisher = {{ACM}},
	author = {Jin Xu and Xu Tan and Renqian Luo and Kaitao Song and Jian Li and Tao Qin and Tie-Yan Liu},
	title = {NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} Conference on Knowledge Discovery Data Mining}
}
@misc{loshchilov2019adamw,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{HintonRMSProp, 
    title={Lecture 6.5-RMSProp: Divide the gradient by a running average of its recent magnitude.}, 
    url={https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}, 
    author={Tieleman Tijmen and Geoffrey Hinton},
    year={2012},
    publisher = {{COURSERA: Neural Networks for Machine Learning 4.2}}
    } 

@inproceedings{wang-etal-2023-distill,
    title = "How to Distill your {BERT}: An Empirical Study on the Impact of Weight Initialisation and Distillation Objectives",
    author = {Wang, Xinpeng  and
      Weissweiler, Leonie  and
      Sch{\"u}tze, Hinrich  and
      Plank, Barbara},
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.157",
    pages = "1843--1852"
}

@inproceedings{udagawa2023comparative,
    title = "A Comparative Analysis of Task-Agnostic Distillation Methods for Compressing Transformer Language Models",
    author = {Udagawa, Takuma  and
      Trivedi, Aashka  and
      Merler, Michele  and
      Bhattacharjee, Bishwaranjan},
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    year = "2023"
}

@misc{Lang_2023, title={Fair is fast, and fast is fair: IBM Slate Foundation Models for NLP}, url={https://medium.com/@alex.lang/fair-is-fast-and-fast-is-fair-ibm-slate-foundation-models-for-nlp-3508412a4b04}, journal={Medium}, publisher={Medium}, author={Lang, Alexander}, year={2023}, month={Jul}} 

@misc{Heidloff_2023, title={IBM announces New Foundation Model Capabilities}, url={https://heidloff.net/article/ibm-announces-new-foundation-model-capabilities/}, journal={Niklas Heidloff}, author={Niklas Heidloff}, year={2023}, month={May}} 