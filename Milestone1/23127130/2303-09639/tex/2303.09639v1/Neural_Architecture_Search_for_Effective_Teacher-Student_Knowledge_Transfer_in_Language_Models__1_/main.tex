\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
%\usepackage[review]{acl2023}
\usepackage[]{acl2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage[ruled,vlined]{algorithm2e}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{comment}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{subfig}
\usepackage{amssymb}

\usepackage{xparse}
% \NewDocumentCommand{\aashka}
% { mO{} }{\textcolor{blue}{\textsuperscript{\textit{Aashka}}\textsf{\textbf{\small[#1]}}}}
% \NewDocumentCommand{\michele}
% { mO{} }{\textcolor{red}{\textsuperscript{\textit{Michele}}\textsf{\textbf{\small[#1]}}}}


%If the title and author information does not fit in the area allocated, uncomment the following

%\setlength\titlebox{5cm}

%and set <dim> to something 5cm or larger.
\begin{document}

\title{Neural Architecture Search for Effective Teacher-Student Knowledge Transfer in Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{Aashka Trivedi,  Michele Merler\textsuperscript{2}, Rameswar Panda\textsuperscript{2}, Yousef El-Kurdi\textsuperscript{2}, Avirup Sil\textsuperscript{2}, \\ {\bf  Taesun Moon\textsuperscript{2} , Bishwaranjan Bhattacharjee\textsuperscript{2}} \\
%   \textsuperscript{1}New York University \\
%   \textsuperscript{2}IBM T.J. Watson Research Centre\\ 
%   \texttt{aashka.trivedi@nyu.edu} \\
%   \texttt{\{mimerler@us., rpanda@, yousefelk@us., avi@us., tmoon@us., bhatta@us.\}ibm.com} \\ \\}

\author{Aashka Trivedi, Takuma Udagawa, Michele Merler, Rameswar Panda, \\ {\bf Yousef El-Kurdi, Bishwaranjan Bhattacharjee} \\
IBM Research AI \\
\texttt{\{aashka.trivedi@, takuma.udagawa@, mimerler@us., rpanda@ }\\
\texttt{yousefelk@us., bhatta@us.\}ibm.com}}


\maketitle
\begin{abstract}

Large pretrained language models have achieved state-of-the-art results on a variety of downstream tasks. Knowledge Distillation (KD) of a smaller student model addresses their \emph{inefficiency}, allowing for deployment in resource-constraint environments. KD however remains \emph{ineffective}, as the student is manually selected from a set of existing options already pre-trained on large corpora, a sub-optimal choice within the space of all possible student architectures. This paper proposes KD-NAS, the use of Neural Architecture Search (NAS) guided by the Knowledge Distillation process to find the optimal student model for distillation from a teacher, for a given natural language task. In each episode of the search process, a NAS controller predicts a reward based on a combination of accuracy on the downstream task and latency of inference. The top candidate architectures are then distilled from the teacher on a small proxy set. Finally the architecture(s) with the highest reward is selected, and distilled on the full downstream task training set. When distilling on the MNLI task, our KD-NAS model produces a 2 point improvement in accuracy on GLUE tasks with equivalent GPU latency with respect to a hand-crafted student architecture available in the literature \protect\cite{mukherjee2021xtremedistiltransformers}. Using Knowledge Distillation, this model also achieves a 1.4x speedup in GPU Latency (3.2x speedup on CPU) with respect to a BERT-Base Teacher, while maintaining 97\% performance on GLUE Tasks (without CoLA). We also obtain an architecture with equivalent performance as the hand-crafted student model on the GLUE benchmark, but with a 15\% speedup in GPU latency (20\% speedup in CPU latency) and 0.8 times the number of parameters.

%Using Knowledge Distillation, this model also achieves a 28\% speedup in GPU Latency (68\% speedup on CPU) with respect to a BERT-Base Teacher, while maintaining 97\% performance on GLUE Tasks (without CoLA).

%\michele{check on the two claims. Add Figure with latency/accuracy/model size plot? Can we call our Method KD-NAS or NASKD or NASdistill, and name our models after it?}
\end{abstract}

% Large pretrained language models have achieved state-of-the-art results on a variety of downstream tasks. However, they are \emph{inefficient} due to the fact that they have billions of parameters, making it difficult to deploy them in resource-constraint environments. One way to address this issue is to obtain a smaller "student" model, through the use of Knowledge Distillation (KD) from the large "teacher". While KD improves efficiency, it remains \emph{ineffective}, as the architecture of the student model is  manually selected, usually from a set of existing options already pre-trained on large corpora. In the space of all possible student model architectures, manual selection is likely sub-optimal, and the existence of a pre-trained student of the most optimal architecture is not guaranteed. 

\begin{figure}[t!]
\includegraphics[width=0.5\textwidth]{./figures/Models.pdf}
\caption{Avg. Glue Score vs CPU Latency of KD-NAS selected architectures compared to hand selected-ones \protect\cite{mukherjee2021xtremedistiltransformers}. At similar accuracy levels, KD-NAS selects architectures that are more compact and faster.}
\label{fig-acclat}
\end{figure}  

\begin{figure*}[t!]
\includegraphics[width=\textwidth, height=0.6\textwidth]{./figures/kdnas_flowchart.png}. 
\caption{Flowchart for the proposed KD-NAS process. At every episode, the controller predicts the states that would have the highest reward. Those states (plus a few additional states for random exploration) are then treated as student architectures to be distilled from the teacher in a reduced-KD process. The actual reward is then obtained, and the episodic <state,reward> pairs are concatenated with information of high performing states explored in the past to perform a new training step for the controller.}
\label{fig-kdnas}
\end{figure*}  

\section{Introduction}
Pre-trained Natural Language models, such as BERT \protect\cite{devlin2019bert} and RoBERTa \protect\cite{liu2019roberta} are capable of showing extraordinary performances on a variety of downstream language tasks such as Natural Language Inference, Question Answering or Named Entity Recognition. However, such models are extremely large, boasting of billions of parameters \protect\cite{devlin2019bert, liu2019roberta, vaswani2017attention, fedus2021switch}. They thus require huge amounts of memory to be deployed, and have a large latency for inference- rendering them \emph{inefficient} to deploy in resource-constrained environments. Smaller models may be more suitable for use in practice, but often sacrifice performance for the sake of deployment \protect\cite{huang2015bidirectional}. Moreover, these models are \emph{ineffective} due to their hand-crafted architecture design- in the space of all possible architectures, manual selection is likely sub-optimal, even if guided by human expertise.


Knowledge Distillation (KD) \protect\cite{hinton2015distilling,mukherjee2021xtremedistiltransformers, wang2020minilm,sanh2020distilbert} trains a smaller student model to mimic the behavior of a larger teacher model. This improves the efficiency of current models, making the compressed model suitable for deployment without losing too much of the performance (or knowledge) attained by the bigger model. In practice, having a manually designed, pre-trained student can be immensely beneficial to the distillation process, but this puts a constraint on the architecture of the student. As posited by \citet{liu2020search} and further shown in this work, an optimal student architecture may exist given a specific teacher model. This observation drives this work to focus on improving the performance of distilling student models whose architecture has not been hand-crafted. Specifically, we follow a variation of the stagewise distillation process as proposed in XtremeDistil Transformers \protect\cite{mukherjee2021xtremedistiltransformers}, and improve the performance of a hand-crafted student architecture after distillation, without pretraining.

% \michele{the following sentence sounds weak, need to rephrase} Specifically, a variation of the stage-wise distillation process as proposed in XtremeDistil Transformers \protect\cite{mukherjee2021xtremedistiltransformers} is followed, and attempts are made to improve the performance of a "base" (i.e, hand-crafted, randomly initialized) student.

An additional benefit of using a student that is not hand-crafted is the flexibility it provides in selecting the optimal architecture for the student, given a teacher model, latency goals and the downstream task. This motivation leads us to propose the use of Neural Architecture Search (NAS) \protect\cite{elsken2019neural, zoph2017neural, liu2019darts,so2019evolved} as a means of efficiently automating the process of finding the best transformer architecture \protect\cite{vaswani2017attention} optimized for distilling knowledge from a BERT teacher\footnote{Specifically, we finetune the bert-base-uncased checkpoint from HuggingFace on the distillation task.}.

% We propose KD-NAS, a system that improves the NAS process to identify an optimal yet efficient architecture (i.e. suitable for deployment in resource constrained environments) from a pre-defined search space for a downstream task by combining Knowledge Distillation objectives with the traditional accuracy and latency measures. We further improve the performance of the selected model by employing data augmentation methods.

% \michele{Try to rephrase the last few sentances as bullets contributions}



This paper makes the following contributions:
\begin{itemize}
    \item We propose KD-NAS, a system that improves the NAS process to identify an optimal yet efficient architecture (i.e. suitable for deployment in resource constrained environments) from a pre-defined search space for a downstream task.
    \item KD-NAS incorporates a Knowledge Distillation objective with the traditional accuracy and latency measures into the NAS Reward.
    \item We propose the use of a Feedback Controller to guide the architecture search, that draws on information from the top performing states explored in previous episodes.
    \item We further improve the performance of the selected model by employing data augmentation methods.
\end{itemize}



\section{Related Work}

%This paper first discusses prior work done in Knowledge Distillation- with a heavy focus on the work presented in XtremeDistil Transformers \protect\cite{mukherjee2021xtremedistiltransformers}, as we build upon their work. Next, we discuss the concepts of Neural Architecture Search, following which prior literature combining the two are analyzed. 

\subsection{Knowledge Distillation}
Knowledge Distillation is a method of model compression in which the knowledge of a larger teacher model is transferred to a student model by having it learn the teacher's internal representation and logits \protect\cite{ba_2014, hinton2015distilling}. This work adapts the distillation pipeline demonstrated in XtremeDistil Transformers \protect\cite{mukherjee2021xtremedistiltransformers}, which follows a stage-wise distillation technique. The student is optimized on three types of losses, corresponding to three distinct and sequential stages of training:

\begin{enumerate}
    \item{Representation Loss ($L_{RL}$): aims to guide the student to learn the teacher's intermediate representations ($\psi_T$) on an unlabelled transfer set, $D_u$. The student's internal representations ($\psi_S$) are projected to the same space as that of the teacher using linear transformations, after which the KL-Divergence (KLD) is computed between them. 
    % \begin{equation}
    %     L_{RL} = \sum_{x_u \in D_u} KLD(\psi_S(x_u), \psi_T(x_u))
    % \end{equation}
    
    \begin{equation}
    L_{RL} = \sum_{x \in D_u} KLD(\mathcal{R} ( \psi_S(x)), \psi_T(x))
    \label{eq:1}
    \end{equation}
    
    In Equation \ref{eq:1}, $\mathcal{R}: \mathbb{R}^{d_S} \to \mathbb{R}^{d_T}$ denotes the linear mapping of the embeddings where $d_S, d_T$ are the student and teacher embedding dimensions respectfully.
    }
    
    \item{Logit Loss ($L_{LL}$): the teacher model is trained to make predictions by assigning probability scores (logits) to all the classes. This term of the KD loss uses the logits of the teacher on all classes as a way to identify how the teacher tends to generalize \protect\cite{hinton2015distilling}, and train the student on the target soft labels of the teacher logits. We compute the logit loss as the Mean Squared Error (MSE) of the teacher logits ($\sigma_T$) and the student logits ($\sigma_T$) on the unlabelled transfer set, $D_u$.
    \begin{equation}
        L_{LL} = \frac{1}{2} \sum_{x \in D_u} \| \sigma_S(x) - \sigma_T(x) \|^2
    \end{equation}
    }
    \item{Hard Label Loss ($L_{HL}$): this is the student's loss that aids it to learn the downstream task. Calculated on data labelled with the ground truth, $D_l$, this allows the student to be fine-tuned on the specific task at hand. The hard-label loss ($L_{HL}$) is calculated as the cross entropy loss between the ground truth labels, $y$, and the predicted label $\hat{y}$, which is computed using the hidden representation of the student, $h$, and the student model weights, $\theta_S$. In this work, we focus on the MNLI task from the GLUE dataset \protect\cite{wang2019glue}. 
    \begin{equation}
        L_{HL} = -\sum_{x, y \in D_l}y \cdot log(softmax(h(x) \cdot \theta_s))
    \end{equation}
    }
\end{enumerate}

\subsection{Neural Architecture Search}
Neural Architecture Search (NAS) is a method of automating the process of selecting the best candidate architecture from a given search space. The main components of a NAS algorithm are the following \protect\cite{elsken2019neural}:
\begin{itemize}
    \item Search Space: This defines the set of possible candidate architectures. For a transformer model, the search space may enclose dimensions such as the number of hidden layers, number of attention heads, hidden size, etc. The goal of NAS is to obtain an architecture which performs better than a random sampling of architectures from this search space.
    \item Search Strategy: The defined strategy to search the Search Space is what makes NAS much more efficient than a brute-force search of the space. NAS typically employs algorithms such as Reinforcement Learning \protect\cite{zoph2017neural, zoph2018learning, tan2019mnasnet}, Evolutionary Algorithms \protect\cite{so2019evolved} or Differential Search \protect\cite{liu2019darts} in order to optimize the search process over a large, multi-dimensional search space.
    \item Performance Evaluation: This is how the NAS algorithm estimates the performance of a candidate model on the given task. A naive performance evaluation strategy is to consider a performance metric after training the candidate architecture on the entire target pipeline. This is extremely computationally expensive, and especially for large language tasks or knowledge distillation pipelines, it is more prudent to lower this resource expense by conducting only a part of the training on a "proxy set" \protect\cite{liu2019darts,zhou2020econas,na2021accelerating}, that is, a reduced version of the target task or the full dataset, for a limited number of epochs. The selection of the proxy set must be done so that it is representative of the performance on the entire model pipeline.
\end{itemize}

\subsection{Knowledge Distillation and NAS}
Most NAS algorithms optimize on the accuracy of the given task, and sometimes the latency. However, they usually search for models in a completely isolated fashion- they do not optimize for information from other existing models in their search process. We propose a method to non-trivially adopt NAS to optimize for the knowledge distillation objective, thus transferring the knowledge from a large model into an optimal architecture for a given task(s) within a large search space. Specifically, this work focuses on a variation of the reinforcement-learning based NAS approach \protect\cite{zoph2018learning,tan2019mnasnet}. We aim to conduct NAS \emph{guided by KD} to obtain the best student model architecture for the XtremeDistil process, using BERT Base \protect\cite{devlin2019bert} as the teacher and MNLI \protect\cite{wang2019glue} as the target task. Essentially, the goal is to use NAS to obtain the optimal student architecture for distilling from a BERT teacher, and then use pre-training and data augmentation to improve performance.

Some works have combined NAS with either implicit or explicit forms of Knowledge Distillation \protect\cite{liu2020search, bashivan2019teacher, Wang_2021}, however they have mostly focused on computer vision specific architectures and not language models. Furthermore, their implementation optimize NAS with additional forms of model distillation objectives (mostly internal representation based ones, not predictions/logits based ones), whereas our method improves distillation with NAS. Additionally, their NAS process is performed on one cell per search episode (micro-search level), whereas our process looks directly at the whole architecture at each episode (macro-search level). Finally, we propose an LSTM based controller model which determines the next states to be explored by predicting the rewards of the given state, along with information regarding well-performing states seen in the past, which is a contrast from the probabilistic actor-critic agent proposed in previous work. 

\section{KD-NAS: KD aware Neural Architecture Search}

This section describes the design of our KD guided NAS process (KD-NAS), which aims at using NAS to find the optimal student architecture for the KD process, given a teacher model and a downstream task. We begin by discussing the details of the NAS implementation- specifically, the design of the search space, the performance evaluation mechanism, and the search strategy.



\subsection{Search Space}

\begin{table}
\centering
\begin{adjustbox}{max width=0.5\textwidth}
%\begin{tabular}{lp{4cm}}
\begin{tabular}{cc}
\hline
 \textbf{Parameter}& \textbf{Candidate Values}\\
\hline
Hidden Layers & [3, 4, 6, 10, 12] \\
Attention Heads & [2, 3, 4, 6, 12] \\
Hidden Size & [192, 288, 384, 576] \\
Intermediate Size & [384, 512, 576, 768, 1024, 1536, 2048, 3072] \\
Hidden Activation & [gelu, relu, silu]\\
\hline
\end{tabular}
\end{adjustbox}
\caption{\label{search-space}
Search Space Candidates for KD-NAS. A constraint to be noted here is that the hidden size must be divisible by the number of attention heads.}
\end{table}

We define a five-dimensional search space for the student transformer model, with the following parameters: Number of Hidden Layers, Number of Attention Heads, Hidden Size, Intermediate (FFN) Size, and Hidden Activation Function. The candidates for each of these dimensions have been listed in Table \ref{search-space}. The total number of possible student architectures in this search space is (5 * 5 * 4 * 8 * 3) = 2400.

\begin{algorithm*}[ht!]
 \label{algorithm-nas}
\SetAlgoLined
\KwIn{search space of size $S$, the number of episodes $T$, the number of candidate states in each episodes $N$, the number of output architectures $M$, the minimum exploration ration $\epsilon_{min}$, the exploration decay rate $\delta$}
\KwResult{$M$ recommended student architectures to distil from a given Teacher Model}
$\epsilon  \gets 1$ \\
$GlobalBestReward \gets 0$ \\
$BestState_0 \gets$ RandomState \\
$NASStates \gets \{\}$\\

 \For{$t = 1,2,..,T $}{
        $C_t \gets \{\} $ \\
        $TrainData_t \gets [ ] $ \\
        $MaxReward_t \gets 0$\\
        Get predicted rewards for all states from controller.\\
        Obtain the list of states sorted by predicted reward, $[s_1, s_2,..s_S]$.\\
        Obtain number of random states, $n_{random} = \left \lceil{\epsilon \cdot N}\right \rceil$. \\
        Obtain number of controller states, $n_{controller} = N -{random}_t $. \\
        Append $[s_1,s_2,...,s_{n_{controller}}]$ to $C_t$. \\
        Generate $n_{random}$ random states from search space, and append them to $C_t$. \\
        
        \For {$s_i$ in $C_t$}{
            Obtain Reward $R_i$ by performing the distillation of $Student(s_i)$ from $Teacher$. \\
            Append (($GobalBestState, BestState_{t-1}, s_i$), $R_i$) to $TrainData_t$. \\
            \If{$R_i \geq MaxReward_t$}{
                Set $BestState_t$ to $s_i$ \\
                Set $MaxReward_t$ to $ R_i $\\
            }
        }
        
        Train controller on $TrainData_t$. \\
        Append $BestState_t$ to $NASStates$. \\
        \If{$MaxReward_t \geq GlobalBestReward$}{
            Set $GlobalBestReward$ to $MaxReward_t$ \\
            Set $GlobalBestState$ to $BestState_t$ \\
        }
        
        Decrease $\epsilon$ to $min((\epsilon - \delta), \epsilon_{min})$. \\
 }
 \Return{Top-M NAS recommended Architectures with highest rewards}
 \caption{Knowledge Distillation guided Neural Architecture Search }
\end{algorithm*} 

\subsection{Performance Evaluation System}\label{perform-eval}
To estimate the rewards, each candidate student model is trained to optimize knowledge distillation (using representation loss (KL-Divergence) and logits loss (MSE)) from the fixed teacher model and accuracy (Hard Label (Cross Entropy) Loss) on a â€œproxy" set or process \protect\cite{liu2019darts,zhou2020econas}, that is, a reduced version of the target task full dataset, for a limited number of epochs. 

For the proxy set of this work, we define a \emph{mini-KD} process, in which each stage of the XtremeDistil process is only 7 epochs long, done with 30\% of the training data. This reduces the time to distil each student to approximately 2.5 hours on a single A100 GPU (compared to approximately 7 hours for the full-KD), thereby allowing for a practical method of evaluating multiple candidate architectures. We obtain this choice of proxy set by experimenting with different combinations of data size and training epochs, and find that \emph{mini-KD} is the smallest process that maintains the relative ranking of performance on the downstream MNLI task for a set of randomly selected architectures from the search space.


% \begin{algorithm*}[ht!]
%  \label{algorithm-nas}
% \SetAlgoLined
% \KwResult{Best Student Architecture for a given Teacher Model}
%  N$\gets$ NUM\_GENERATED\_ACTIONS \\
%  M$\gets$ NUM\_OUTPUT\_STATES \\
%  $\epsilon  \gets$ 1,  $\epsilon_{min}  \gets$ MIN\_EXPLORATION\_RATIO, $\delta \gets$ EXPLORATION\_DECAY\_RATE \\
%  GlobalBestReward $\gets$ 0 \\
 
%  \For{episode$\gets$1 \KwTo MAX\_EPISODES}{
%         PredRewards = controller.$predict\_rewards$(AllStates)\\
%         BestStates = $sort$(AllStates, $key$ = PredRewards)\\
%         % \eIf{PredRewards == PrevRewards}{
%         % \Return{BestStates[0]}}
%         % {
%         %     NumRandomStates = $ceil$($\epsilon \cdot $N) \\
%         %     CandidateStates = $append$(BestStates[:(N-NumRandomStates), controller.$generate\_random\_states$( NumRandomStates) ]\\
%         %     PrevRewards $\gets$ PredRewards\\
%         % }
        
%         NumRandomStates = $ceil$($\epsilon \cdot $N) \\
%             CandidateStates = [BestStates[:(N-NumRandomStates)]; controller.$generate\_random\_states$(NumRandomStates) ]\\
%         % PrevRewards $\gets$ PredRewards\\
%     \For {i, state in $enumerate$(CandidateStates)}{
%         Rewards[i] $\gets KnowledgeDistillation$ ($Student$(state), TEACHER\_MODEL)\
%     }
    
%     controller.$train$((GobalBestState, PrevBestState, CandidateStates), Rewards)\\
    
%     PreviousBestState $\gets$ CandidateStates[$argmax(Rewards)$] \\
    
%     BestNASStates \gets BestNASStates.append(PreviousBestState) \\
%     \If{max(Rewards) >= GlobalBestReward}{
%         GlobalBestReward $\gets max(Rewards)$\\
%         GlobalBestState $\gets$ CandidateStates[$argmax(Rewards])$]\\
%     }
%     $\epsilon \gets min((\epsilon - \delta), \epsilon_{min})$ \\

%  }
%  BestNASStates = $sort$(BestNASStates, $key$ = PredRewards)\\
%  \Return{BestNASStates[:M]}
%  \caption{KD-NAS}
% \end{algorithm*} 



\subsection{Search Strategy}

This work implements a reinforcement learning based approach for NAS. For a reinforcement learning problem, there exists three key components: the search mechanism, the controller, and the reward. 

\begin{table*}[ht!]
\centering
\begin{adjustbox}{max width=\textwidth}
%\begin{tabular}{p{1.5cm}|p{1.2cm}|p{1.4cm}|p{1.1cm}|p{1.4cm}|p{1.5cm}|p{1.8cm}|p{1.1cm}|p{1.1cm}|p{1.1cm}} %   . p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}
\begin{tabular}{c|c|c|c|c|c|c|ccc}
\hline
 \textbf{Model}& \textbf{Hidden} & \textbf{Attention} & \textbf{Hidden}& \textbf{Intermediate} & \textbf{Activation} & \textbf{Params}& \multicolumn{3}{c}{\textbf{Latency (ms)} $\downarrow$ }  \\
 & \textbf{Layers}&\textbf{Heads} &\textbf{Size} &\textbf{Size} &\textbf{Function} & \textbf{(M)} & CPU & V100 GPU & A100 GPU\\
\hline
\textbf{\small{$Teacher$}} & 12	& 12 & 768	& 3072	& gelu	& 109.7& 62.69(0.23) & 13.11(0.37) & 12.15(0.03)\\
\textbf{\small{$XtremeDistil_{Arch}$}} & 6	& 12 & 384	& 1536	& gelu	& 23.1 & 17.12(0.09) & 9.10(0.12) & 8.63(0.02)\\
\hline
\textbf{\small{$KDNAS_{Arch1}$}} & 6	& 12 & 576	& 3072	& silu	& 47.9& 23.82(0.10) & 9.10(0.07) & 8.65(0.02)\\
\textbf{\small{$KDNAS_{Arch2}$}} & 6	& 4	& 576 &	1536	& gelu	& 37.3 & 20.14(0.01) & 8.92(0.10) & 8.71(0.02)\\
\textbf{\small{$KDNAS_{Arch3}$}} & 4	& 3	&384	& 1536	& gelu &19.5& 13.65(0.26) & 7.88(0.17) & 7.38(0.02)\\
\hline
\end{tabular}
\end{adjustbox}
%}
\caption{\label{tab:nas-model}
Top 3 model architectures selected after 25 episodes of the KD-NAS Process, compared to the Base Architecture (XtremeDistil-l6-h384) and Teacher model. Latency measured in milliseconds, and depicted as an average over 3 runs (standard deviation included in parenthesis).}
\end{table*}

\subsubsection{Search Mechanism}
The controller model is trained for a certain number of \emph{Episodes}. Each episode comprises of the following steps:
\begin{enumerate}
    \item Generate Candidate States (with Exploration): Each episode of NAS outputs $N$ states, some of which are generated by the controller, and some of which are randomly explored.  In the beginning of the NAS process, the controller benefits from more \emph{exploration} (testing randomly generated states), which is defined by the exploration ratio $\epsilon$. Thus, in each episode, $\epsilon \cdot N$ of states are chosen to be explored are random, and the controller model returns the top $(1-\epsilon) \cdot N$ states predicted to have the highest reward. We implement a decaying exploration, starting with $\epsilon=1$, which decays by 0.05 every episode, until it saturates to $\epsilon_{min}$, which we take to be 0.05. 
    \item Perform Knowledge Distillation on the Candidate States: The mini-Knowledge Distillation process, as defined in Section \ref{perform-eval}, is performed using the fixed teacher model and the student defined by each state in the candidate states. This gives us the actual reward for that state.
    \item Train the Controller Model: While the NAS process progresses, we keep track of two states: \emph{GlobalBestState} and \emph{PreviousBestState}, as described in Section \ref{controller-design}. In each Episode $ep_n$, the \emph{(state,reward)} pairs resulting from Step 2 are concatenated to \emph{(GlobalBestState, PreviousBestState)} and then fed into the controller as training data and the controller is further trained to optimize the Controller loss, $L_C$. This loss measures the mean squared error (MSE) between the actual reward of the model after distillation on the proxy set, $R(DS_s)$, and the predicted reward $PR(s)$ for all states $s$ belonging to the candidate states $CS$ of that episode.
    \begin{equation}
        L_C = \frac{1}{2} \sum_{s \in CS} \|R(DS_s) - PR(s) \|^2
    \end{equation}
\end{enumerate}
This process is repeated for a certain number of $max\_episodes$ episodes, after which the final architectures selected are the top explored architectures with the highest reward. The working is described in Algorithm 1, and depicted pictorially in Figure \ref{fig-kdnas}. 

%Unlike previous works which place a constraint on only the latency, our KD-guided NAS approach also puts a constraint on the \emph{size} of the student. Since the major goal of our work is to use NAS in order to improve the KD process, it makes more sense to limit the student to have less than half the number of parameters than the teacher model. To this extent, the Knowledge Distillation process is modified to set the reward of students having more parameters than the aforementioned limit to 0.


\subsubsection{Controller Design}\label{controller-design}
The Controller is the model that "predicts" the best state. In this work, we train a Long Short Term Memory (LSTM) \protect\cite{LSTM, LSTMBakker} Controller model to predict the \emph{reward} of a given state, and a memory of performance of previously explored states. Specifically in each episode $ep_n$, besides the current state defined as an architecture within the search space, we pass as inputs two additional states: the Global Best State which is the state (architecture) of the best performing student seen in all prior episodes $\{ep_1, ep_2, .., ep_{n-1}\}$, and the Previous Best State, i.e., the best performing student architecture from episode $ep_{n-1}$. Here, a student's performance is quantified by it's reward. 

%\michele{modify Figure \ref{fig-kdnas}. This could be called out as novelty in the Intro}\aashka{add more details and make figure more prominent eg. how does exploration ratio relate to states, etc}



\subsubsection{Reward Function}
In this work, the reward is essentially a function of the accuracy of the distilled student model ($DS_s$) on the downstream task $G$ after it has been distilled from a teacher model $T$, and latency of the student model $S_s$, whose architecture is defined by the state $s$. The latency is normalized using the maximum target latency, which is a percentage of the latency of the teacher.  The reward is defined numerically in a way similar to the optimization goal of \citet{tan2019mnasnet}:
\begin{equation}
reward(S_s) = acc_G(DS_s) * \left(\frac{lat(S_s)}{max\_lat}\right)^\alpha 
\end{equation}
\label{eq:reward-latency}
\begin{equation}
max\_lat = \beta * lat(T) 
\end{equation}




\section{Experiments}

%\subsection{NAS Proxy Set}
%An essential aspect of the efficiency of the NAS process is the use of a proxy set to estimate the performance in a computationally reduced setting \protect\cite{liu2019darts,zhou2020econas}, instead of measuring the performance on the actual, more time and resource consuming pipeline. This work experimented with two proxy sets- $MiniKD_1$, which ran the XtremeDistil Process with 30\% of the training data, and allowed each distillation stage to run for 10 epochs, and $MiniKD_2$, which ran the distillation process with 40\% of the training data, allowing each distillation stage to run for up to 15 epochs. $MiniKD_1$ is faster- taking only 2 hours on a single NVIDIA V100 GPU, but $MiniKD_2$ gives a more accurate estimation of the relative rewards of different model configurations compared to their rewards after running the entire XtremeDistil process. $MiniKD_2$ is thus taken as the proxy set (or, process) to evaluate the candidate architectures in our NAS implementation. Within the proxy set, the implementation is similar to that described by the XtremeDistil Transformers\footnote{https://github.com/microsoft/xtreme-distil-transformers}.

% \aashka{Discuss selection of Base model and nomenclature here}

\begin{table}[t!]
\centering
%\begin{tabular}{p{1.6cm}p{1.5cm}p{1.5cm}p{1.5cm}}
\begin{adjustbox}{max width=0.49\textwidth}
\begin{tabular}{cccc}
\hline
 \textbf{Model}& \textbf{MNLI Accuracy} $\uparrow$ & \textbf{Latency (ms)} $\downarrow$& \textbf{Reward} $\uparrow$\\
\hline
\small{$Random_{Seed1}$}	& 74.93(0.77) & 6.32(2.38) &	0.778(0.019)\\
\small{$Random_{Seed2}$}	& 75.07(0.84) &	6.81(2.87) & 0.777(0.013)\\
\small{$Random_{Seed3}$}	& 75.19(1.36) &	7.65(2.58) & 0.772(0.010)\\
\hline
\textbf{\small{$KDNAS_{Arch1}$}} & 79.48 &	7.52 &	0.816\\
\textbf{\small{$KDNAS_{Arch2}$}} & 80.61 &	5.65 &	0.838\\
\textbf{\small{$KDNAS_{Arch3}$}} & 77.83 &	4.46 &	0.819\\
\hline
\end{tabular}
\end{adjustbox}
\caption{\label{tab:random-sampling}
Results for random sampling, and the three best architectures as selected by KD-NAS after 25 episodes. Random sampling metrics are averaged over 5 randomly selected architectures, with results shown for 3 randomly selected seeds. The standard deviation for random sampling metrics are shown in parenthesis.}
\end{table}

The purpose of this work is to find better architectures, more suitable to use as student models while distilling knowledge from a given teacher and a given task. As such, we do not propose better knowledge distillation or training techniques. However, for an accurate analysis of the performance of our KD-NAS set up, we compare our NAS recommended architectures with the the 6-layer, 384-hidden size architecture\footnote{The architecture of our "base" model matches the Huggingface checkpoint microsoft/xtremedistil-l6-h384-uncased} used in XtremeDistil Transformers \protect\cite{mukherjee2021xtremedistiltransformers}.  We train this architecture from scratch with our distillation pipeline (with random initialization and without pre-training) for a fair comparison.  We refer to this architecture as $XtremeDistil_{Arch}$ or "base model" in the subsequent sections and tables.

\subsection{Knowledge Distillation-guided Neural Architecture Search Implementation}
We perform a KD-guided NAS to find the best student architecture for distilling from a BERT-base teacher. We choose MNLI from the GLUE benchmark \protect\cite{wang2019glue} as our fine-tuning objective and report our accuracy on the dev set. 

We performing the KD-Guided NAS for 25 episodes (with each episode producing 30 candidate architectures) using the proxy set $mini-KD$ as an estimation of accuracy and latency. The reward function from Equation 5 is used, with the maximum latency is set 60\% the latency of the teacher (i.e., $\beta = 0.6$), and the hyperparameter $\alpha$ is set to -0.06. To keep latency measures across states consistent and comparable, we create a lookup table of the latencies of all randomly-initialized students on a single NVIDIA A100 GPU.

After the KD-NAS process, we obtain our top 3 architectures as suggested by the NAS process, $KDNAS_{Arch1}$, $KDNAS_{Arch2}$, and $KDNAS_{Arch3}$. Specifically, these are the explored models with the highest rewards across episodes. The architectural details of these models are described in Table \ref{tab:nas-model}. Now, the controller predictions include some degree of random exploration (controlled by hyperparamter $\epsilon$), however, each of the selected models has been continuously recommended by the controller across multiple episodes, indicating that the controller is confident in the performance of these models, even if the may have been initially been candidates of random exploration.  

% \michele{do we have a record if the best models were ultimately selected by the controller vs part of the random pool?}\aashka{check which episodes these models came from -> early episodes may mean random, later ones may be controller. OR if the models were recommended many times} The configurations are shown in Table \ref{tab:nas-model}. \aashka{HOW are models selected (models with best reward across episodes)} 


In order to evaluate the efficacy of our NAS system, following standard NAS evaluation practice \protect\cite{yang2020nas}, we first determine whether it does better than random sampling over the defined search space. The average accuracy, latency, and rewards after the distillation of 5 randomly selected student architectures over 3 seeds have been shown in Table \ref{tab:random-sampling}. As seen, the best selected model after 25 episodes of KD-NAS, outperforms the random sampling models in terms of accuracy and rewards, by more than 3 points. This encourages the use of NAS over a random sampling within the search space. The models shown in this table were initialized with random values, without pre-training, and the XtremeDistil pipeline was run without any form of data augmentation.

\subsection{KD Objective}

We experiment with two approaches to knowledge distillation, analyzing which one yields better performance while maintaining an efficient use of resources. 

\paragraph{Task-Specific Distillation} Here, we perform distillation on a specific Natural Language Task, specifically the MNLI task \protect\cite{wang2019glue}, using the 390K training examples of the task. The stage-wise distillation of teacher embeddings and logits, followed by fine-tuning on the task results in a student model that is tailored to a specific task. However, as shown in Table \ref{tab:final-compare}, a student distilled on MNLI also has competitive performance on other tasks of the GLUE benchmark, except the CoLA task. We posit that this may be because the CoLA task is a syntactic task that measures linguistic acceptability, which is different from the other semantic tasks of the GLUE benchmark.

\begin{table}[t!]
\centering
\begin{adjustbox}{max width=0.5\textwidth}
%\begin{tabular}{p{1.2cm}p{2.4cm}p{1.4cm}p{1cm}}
\begin{tabular}{cccc}
\hline
   \textbf{KD Objective}& \textbf{KD Data} & \textbf{\# Samples} & \textbf{Avg. GLUE Score} \\
   \hline
   NLI & MNLI & 390K & 71.8\\
   MLM & Wiki + Books & 2M  & 61.4\\
   MLM & Wiki + Books & 10M & 70.1\\
   MLM & Wiki + Books & 20M & 70.8\\
   MLM & Wiki + Books & 81M & 72.5\\
   MLM & Wiki + Books & 205M & 73.4\\
\hline
\end{tabular}
\end{adjustbox}
\caption{\label{tab:mlm-vs-mnli}
Performance of our Base (XtremeDistil) Architecture when using task specific distillation (on MNLI task) and task agnostic distillation (on MLM objective). We experiment with the entire MNLI dataset, and varying percentages (1\%, 5\%, 10\% and 40\%) of the 204M sentences in the combined English Wikipedia and BookCorpus.}
\end{table}

\begin{table}[t!]
\centering
\begin{adjustbox}{max width=0.5\textwidth}
%\begin{tabular}{p{1.9cm}p{0.7cm}p{1.4cm}p{0.8cm}p{1cm}}
\begin{tabular}{ccccc}
\hline
 \textbf{Model}& \textbf{Params (M)} & \textbf{KD Data} & \textbf{\# Samples} & \textbf{Avg. GLUE Score}  \\
\hline
\textbf{\small{$XtremeDistil_{Arch}$}} & 23 & MNLI & 390K & 71.8 \\
\textbf{\small{$KDNAS_{Arch1}$}} & 48 & MNLI & 390K & 72.4 \\
\textbf{\small{$KDNAS_{Arch1}$}} & 48 & MNLI+SNLI & 940K & 73.2 \\
\textbf{\small{$KDNAS_{Arch2}$}} & 37 & MNLI & 390K & 73.5 \\
\textbf{\small{$KDNAS_{Arch2}$}} & 37 & MNLI+SNLI & 940K &  73.2\\
\textbf{\small{$KDNAS_{Arch3}$}} & 19.5 & MNLI & 390K & 70.5  \\
\textbf{\small{$KDNAS_{Arch3}$}} & 19.5 & MNLI+SNLI & 940K & 71.5 \\
\hline
\end{tabular}
\end{adjustbox}
\caption{\label{tab:data-augmentation}
Performance of XtremeDistil Architrcture and KD-NAS architectures on the GLUE benchmark with different augmentation resources as unlabeled transfer set with BERT-Base as teacher.}
\end{table}

\begin{table*}[ht!]
\centering
\begin{adjustbox}{max width=\textwidth}
%\begin{tabular}{lp{0.9cm}p{1cm}lllllllllp{1cm}}
\begin{tabular}{c|c|cc|c|cccccccccc}
\hline
\textbf{Model}  & \textbf{KD Data} & \multicolumn{2}{c|}{\textbf{Latency (ms)} $\downarrow$} & \textbf{Params} & \multicolumn{10}{c}{\textbf{Glue Evaluation} $\uparrow$}   \\
 &  & GPU & CPU & \textbf{(M)} &  MNLI & QQP & QNLI & SST-2 & CoLA & STS-B & MRPC  & RTE & Avg. & \textbf{Avg.(-CoLA)}  \\
\hline
\textbf{\small{$Teacher$}} & & 12.15 & 63.69 & 110 & 83.8 & 87.5 & 90.6 & 91.7 & 56.4 & 88.2 & 87.3 & 65.1& 81.4(0.5) & 84.9(0.4) \\
\hline
\textbf{\small{$XtremeDistil_{Arch}$}} & MNLI & 8.63 & 17.12 &23 & 78.3 & 82.9 & 84.7 & 87.6 & 3.9 & 87.6 & 85.6 & 63.4 & 71.8(0.9) & 81.4(0.7) \\
\textbf{\small{$KDNAS_{Arch1}$}} & MNLI & 8.65 & 23.82 & 48 & 79.5&	84.1&	85.6&	89.1&	5.7&	84.6&	86.5&	64.0& 72.4(0.9)&	81.9 (0.4) \\
\textbf{\small{$KDNAS_{Arch1}$}}  & NLI &  8.65& 23.82 & 48 & 79.9&	84.4&	85.8&	89.0&	9.2&	85.4&	\textbf{86.8}&	65.1& 73.2(0.5)&	82.4(0.5) \\
\textbf{\small{$KDNAS_{Arch2}$}} & MNLI &8.71 & 20.14 & 37& \textbf{80.6}&	\textbf{84.5}&	\textbf{86.6}&	\textbf{89.1} &	\textbf{12.2}&	86.5&	85.9&	62.9& \textbf{73.5(0.6)} &82.3(0.3)  \\
\textbf{\small{$KDNAS_{Arch2}$}} & NLI & 8.71 & 20.14 &37 & 80.0 &	84.3&	84.8&	88.6&	8.4&	\textbf{88.3}&	84.9&	\textbf{66.1}&  73.2(0.9) &	\textbf{82.5(0.5)}   \\
\textbf{\small{$KDNAS_{Arch3}$}} & MNLI & 7.38 & 13.65 &19 & 77.8&	82.5&	83.8&	86.3&	1.7&	83.6&	84.4&	64.0& 70.5(0.5) &80.3(0.5)\\
\textbf{\small{$KDNAS_{Arch3}$}} & NLI & 7.38 & 13.65 &  19& 77.2&	82.4&	82.5&	86.3&	10.2&	85.3&	84.4&	63.8& 71.5(0.8)&	80.3(0.6)\\
\hline
\textbf{\small{$XtremeDistil_{Arch}$}} & B+W &  8.63 & 17.12 & 23&  79.0 & 84.2 & 85.3 & 89.3 & 30.2 & 78.1 & 82.1 & 58.8 & 73.4(0.5)& 79.6(0.2) \\
\textbf{\small{$KDNAS_{Arch1}$}}  & B+W &  8.65& 23.82 & 48&  80.3 & 84.2 & 86.6 & 90.4 & 39.9 & 80.6 & 83.7 & 58.6 & 75.5(0.3) & 80.6(0.4) \\
\textbf{\small{$KDNAS_{Arch2}$}}  & B+W & 8.71 & 20.14& 37 & 79.8 & 84.4 & 85.8 & 90.8 & 31.9 & 81.1 & 83.4 & 59.7 & 74.6(0.3)& 80.7(0.2)\\
\textbf{\small{$KDNAS_{Arch3}$}}  & B+W & 7.38& 13.65 &  19 & 74.1 & 82.6 & 84.3 & 88.8 & 12.6 & 76.7 & 81.7 & 57.8 & 69.8(0.4) & 78.0(0.5) \\
\hline
\end{tabular}
\end{adjustbox}
\caption{\label{tab:final-compare}
Comparative Results between the XtremeDistil architecture and the KD-NAS architectures with different pre-training configurations, with NLI indicating data augmentation, and B+W indicating BookCorpus+Wikipedia with a task-agnostic distillation with an MLM objective. Latency depicted as average latency on 1 A100 GPU and 1 8-core CPU (parameters in millions and latency in milliseconds). Glue results are shown as an average taken over 3 seeds, with the average standard deviation shown in parenthesis.}%\michele{the xtreme-distil paper reports 22M parameters, how many are there exactly?}\aashka{Write somewhere how the params were estimated}
\end{table*}



\paragraph{Task-Agnostic Distillation} Here, we distil the student model on a Masked Language Modelling (MLM) objective. This distillation is more similar to prior works \protect\cite{sanh2020distilbert, wang2020minilm}, however a stage-wise distillation approach is still followed by distilling the embeddings and then fine-tuning. We omit the distillation of logits for efficiency. We use English Wikipedia \protect\cite{devlin2019bert} and Book Corpus \protect\cite{BookCorpus} for the MLM distillation, and experiment with different amounts of data from these datasets. We see in Table \ref{tab:mlm-vs-mnli} that using 81M samples (about 40\% of the combined corpus) results in a model with comparable performance as using MNLI as the distillation objective. 


\paragraph{Discussion} The use of an MLM objective results in a student that is more general-purpose, however, MLM distillation is very time consuming (approx. 50 hours with 4 V100 GPUS, compared to approx 9 hours to distill the same model on the MNLI task with 1 V100 GPU to get the same GLUE performance). This makes it an ineffective objective for our NAS proxy. Moreover, much more data is required for distilling a student on the MLM objective to obtain the same performance as the MNLI-student.  Thus, we propose the use of a task-specific distillation pipeline for the KD-NAS process, and task-agnostic distillation can be used to further boost performance, as shown in Table \ref{tab:final-compare}.


\subsection{Data Augmentation}
Following the recent success of data augmentation \protect\cite{chakravarti2020towards, bornea2021multilingual} during the fine-tuning phase of a large pre-trained LM \protect\cite{devlin2019bert}, we focus on augmenting the MNLI training set \protect\cite{wang2019glue} with SNLI \protect\cite{bowman2015large}. Both these natural language inferencing (NLI) tasks recognize textual entailment by labeling two pairs of sentences as either entailment, contradiction or neutral. Essentially, this method of dataset expansion more than doubles the training corpus- moving from 390K training examples of MNLI to 940K training examples in this combined NLI corpus. This simple data augmentation strategy which does not involve complex synthetic data generation techniques \protect\cite{shakeri-etal-2020-end, sultan-etal-2020-importance}, is shown to be effective. Table \ref{tab:data-augmentation} shows the average performance of the model on the GLUE benchmark \protect\cite{wang2019glue}.


% Many large language models derive their power from the multitude of data that they are trained upon- both BERT \protect\cite{devlin2019bert} and RoBERTa \protect\cite{liu2019roberta} were trained upon several hundred gigabytes of data, such as the entire English Wikipedia and a large corpus of books. This idea can be applied to improving the performance of the student model, and as shown by prior works, increasing the training data can improve the performance of the distilled student model \protect\cite{mukherjee2021xtremedistiltransformers}. This work focuses on augmenting the MNLI Training dataset \protect\cite{wang2019glue} with the SNLI dataset \protect\cite{bowman2015large}. Both these natural language inferencing tasks recognize textual entailment by labelling two pairs of sentences as either entailment, contradiction or neutral. Essentially, this method of dataset expansion more than doubles the training corpus- moving from 390K training examples of the MNLI corpus to 940K training examples in this combined Natural Language Inferencing (NLI) corpus. 

\subsection{Value of NAS in Knowledge Distillation}

To fairly compare the utility of the proposed KD-NAS to find the best student architecture for distilling from a BERT teacher, we compare our results to the architecture used in \citet{mukherjee2021xtremedistiltransformers}, after distilling the architecture on our pipeline. This model are then compared with the model chosen by our NAS pipeline- $NAS_{Arch}$. 

The results of this comparison is shown in Table \ref{tab:final-compare}, where it can clearly be seen that NAS models have an advantage over the Base (Xtremedistil) architecture. Specifically,$NAS_{Arch2}$ provides an advantage over the base in terms of performance, while being equivalent in latency, $NAS_{Model1}$ improves performance with a slight slowdown, while $NAS_{Arch3}$ is significantly faster while being only 2 points behind on performance.  As seen in Tables \ref{tab:data-augmentation} and \ref{tab:final-compare}, data augmentation is able to further improve the performance, giving merit to our proposed KD-NAS process. 
% \michele{perhaps we should change the name of Base Model to XtremeDistil-l6-h384}\aashka{Clarify how the "base" relates to XtremeDistil model. Maybe, we can say XtremeDistil-arch (for architecture). Maybe substitute ARCH for Model in all tables? -> Specify this in the text. clarify goal is to get better architectures, not come up w better training}


\section{Conclusion}
We propose KD-NAS, a novel method of harnessing the power of Neural Architecture Search to find the best student model architecture to distill knowledge from a BERT teacher on the  Multi-Genre Natural Language Inference (MNLI) task. Our Knowledge Distillation guided NAS produces a student model architecture that outperforms a hand-crafted architecture . By distilling NAS-selected student architecture, and applying data augmentation techniques, our best model achieve a 2\% absolute improvement over the accuracy of state of the art hand crafted architecture \protect\cite{mukherjee2021xtremedistiltransformers}, demonstrating the benefit of selecting an optimal architectures within the search space of possible students. Moreover, the smallest NAS selected model has 37\% fewer parameters, and a 1.2x speedup in latency compared to the same baseline architecture.
%\aashka{Hand crafted  model -> word this better} \michele{needs rephrasing, is this related to the results of Table \ref{tab:random-sampling}?}

\bibliography{references.bib}
\bibliographystyle{acl_natbib}
 

 
\begin{table*}[ht!]
\centering
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{c|c|c|cccccccccc}
\hline
\textbf{Model}  & \textbf{Pretrain Data} & \textbf{KD Data} & \multicolumn{10}{c}{\textbf{Glue Evaluation} $\uparrow$}   \\
 &  & &  MNLI & QQP & QNLI & SST-2 & CoLA & STS-B & MRPC  & RTE & Avg. & \textbf{Avg.(-CoLA)}  \\
 \hline
\textbf{\small{$NAS_{Model2}$}} &None & MNLI& 80.6&	84.5&	86.6&	89.1&	12.2&	86.5&	85.9&	62.9& 73.5(0.6) &82.3(0.3)\\
\textbf{\small{$PTNAS_{Model2}$}} &Wiki & MNLI&79.6 & 84.3 & 85.8 & 88.3 & 8.5 & 86.7 & 86.1 & 63.5 & 72.9 (0.5) & 82.1 (0.4)\\
\textbf{\small{$NAS_{Model3}$}} &None & MNLI & 77.8&	82.5&	83.8&	86.3&	1.7&	83.6&	84.4&	64& 70.5(0.5) &80.3(0.5)\\
\textbf{\small{$PTNAS_{Model3}$}} &Wiki & MNLI & 77.3& 82.9& 84.4& 87.7& 3.3& 85.3& 84.6& 62.9& 71.1 (0.4)& 80.7(0.3) \\
\hline
\end{tabular}
\end{adjustbox}
\caption{\label{tab:pretraining}
Effect of Pre-training. Average scores over 3 runs (standard deviation included in parenthesis). Models prefixed with "PT" indicate they were pre-trained prior to distillation, an absence of a prefix indicates random initialization.}
\end{table*}


\begin{table*}[ht!]
\centering
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{c|c|c|cccccccccc}
\hline
\textbf{Pretrain Data} & \textbf{Pretrain Steps} & \textbf{KD Data} & \multicolumn{10}{c}{\textbf{Glue Evaluation} $\uparrow$}   \\
 &  & &  MNLI & QQP & QNLI & SST-2 & CoLA & STS-B & MRPC  & RTE & Avg. & \textbf{Avg.(-CoLA)}  \\
 \hline
None & None & None & 63.4 & 71.0 & 61.3 & 82.2 & 0 & 8.6 & 81.2& 53.5 & 52.5 & 60.17 \\
None & None & MNLI & 77.3 & 82.1 & 83.5& 87.3 & 0.6 & 83.9 & 83.3 & 60.4 & 69.9 & 79.68 \\
None & None & MNLI+SNLI & 77.3 & 82.7 & 83.2 & 87.1 & 2.4 & 86.6 & 85.3 & 63.3 & 71.0 & 80.77 \\
\hline
Wiki 103M & 220K & None & 72.6 & 81.2 & 83.2 & 86.8& 14.6& 74.6 & 82.4 & 53.3 & 68.5 & 76.30 \\
Wiki 103 M & 220K & MNLI & 77.4 & 82.4 & 83.7 & 87.8 & 1.6 & 81.2 & 85.3 & 57.6 & 69.7 & 79.34 \\
Wiki 103M & 220K & MNLI+SNLI & 77.0 & 81.9 & 84.0 & 85.7 & 1.2 & 85.8 & 83.1 & 62.5 & 70.1 & 80.01 \\
\hline
Wiki 2.8B & 220K & None & 73.5 & 82.9 & 82.8 & 86.7 & 15.9 & 79.7 & 84.4 & 56.3 & 70.3 & 78.04 \\
Wiki 2.8B & 220K & MNLI & 77.3 & 83.4 & 84.1 & 86.9 & 1.0 & 85.2 & 84.7& 62.7 & 70.7 & 80.61 \\
Wiki 2.8B & 220K & MNLI+SNLI & 77.3 & 82.7& 83.2 & 87.7 & 1.3 & 86& 85.3& 60.2 & 70.5 & 80.34 \\
\hline
Wiki 2.8B & 400K & None & 74.2 & 83.1& 83.2 & 86.7 & 12.9 & 79.7 & 84 & 56.1& 70.0 & 78.14 \\
Wiki 2.8B & 400K & MNLI & 77.3 & 82.9 & 84.4 & 87.7 & 3.3 & 85.3 & 84.6 & 62.9 & 71.1 & 80.73 \\
Wiki 2.8B & 400K & MNLI+SNLI & 77.2 & 82.9& 83.8 & 87.5 & 3.8 & 86.3 & 85.7 & 62.8 & 71.2 & 80.88\\
\hline
\end{tabular}
\end{adjustbox}
\caption{\label{tab:quality-pretrain}
Effect of the Quality of Pretraining. All models share the architecture of the 19M $NAS_{Arch3}$. Pretraining data Wiki 103M indicates 103 Million tokens of English Wikipedia data, and Wiki 2.8B indicates 2.8 Billion tokens of English Wiki. KD Data "NLI" indicates Data Augmentation. Average scores over 3 runs (standard deviation included in parenthesis). }
\end{table*}

\appendix
\section{Ablation Study: Effect of Pre-training}
\label{sec:ablation-pretraining}

Many large language models derive their power from the multitude of data that they are trained upon- both BERT \protect\cite{devlin2019bert} and RoBERTa \protect\cite{liu2019roberta} were trained upon several hundred gigabytes of data, such as the entire English Wikipedia and a large corpus of books. Prior work \protect\cite{turc2019pretrain} has shown that pretraining student models may be beneficial to the distillation process as well.

Interestingly, we find that pretraining our NAS selected model on a generic masked language modelling objective does not further improve the performance of the model after distillation, as compared to randomly initializing the model before distillation. 

Table \ref{tab:pretraining} shows the comparative results for the NAS-selected architecture $KDNAS_{Arch2}$ and $KDNAS_{Arch3}$, with and without pretraining before distilling on the MNLI task. Pretraining was done on the English Wikipedia Corpus for 400,000 steps for a Masked Language Modelling (MLM) Objective. The learning rate was tuned on a held-out validation set from the range 5e-5 to 5e-4 with a batch-size of 64. As seen, there is no significant improvement in the Average GLUE scores after pretraining the model. 

Due to the non-intuitive nature of this result, we conduct further experiments to study the effect of various dimensions of pretraining on the performance of a model after distillation.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.5\textwidth]{./figures/Stage1.png}. 
\caption{Stage 1 Loss (Representation Loss) for KD-NAS Arch-2 with random initialization and after pretraining using Wikipedia data}
\label{fig-stage1}
\end{figure} 

\begin{figure}[ht!]
\centering
\includegraphics[width=0.5\textwidth]{./figures/Stage2.png}. 
\caption{Stage 2 Loss (Logit Loss) for KD-NAS Arch-2 with random initialization and after pretraining using Wikipedia data }
\label{fig-stage2}
\end{figure}  

\subsection{Quality of Pretrained Model}

We experiment with pretraining $KDNAS_{Arch3}$ (19M parameters) with different amounts of English Wikipedia data (specifically, 103 million tokens and 2.8 billion tokens), and for a different number of training steps (220,000 and 400,000), and compare the results of using these as a starting point to distillation. 

Table \ref{tab:quality-pretrain} shows these results, which are summarized here. First, we note that Knowledge Distillation significantly improves the performance of a model- whether it is pretrained or not. However, we observe a higher improvement given by KD when the model is randomly initialized (~19 points), as opposed to when the model is pretrained (~2 points). Second, using a larger pretraining dataset is beneficial to the model, since models pretrained with 2.8B tokens outperform those pretrained with 103M tokens, both before and after knowledge distillation. Third, we see a minor improvement in models which were pretrained for more steps. We also see that better pretrained models give better performance after distillation. Lastly, there is a significant reduction in the performance of the models on CoLA after distilling on the MNLI task. As mentioned before, we posit that this may be because of the syntactic nature of this test, however, we aim to explore this gap in future work.



\label{sec:ablation}
\begin{table*}[t!]
\centering
\begin{adjustbox}{max width=\textwidth}
%\begin{tabular}{p{1.5cm}|p{1.2cm}|p{1.4cm}|p{1.1cm}|p{1.4cm}|p{1.5cm}|p{1.8cm}|p{1.1cm}|p{1.1cm}|p{1.1cm}} %   . p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}
\begin{tabular}{c|c|c|c|c|c|c|ccc}
\hline
 \textbf{Model}& \textbf{Hidden} & \textbf{Attention} & \textbf{Hidden}& \textbf{Intermediate} & \textbf{Activation} & \textbf{Params}& \multicolumn{3}{c}{\textbf{Latency} $\downarrow$} \\
 & \textbf{Layers}&\textbf{Heads} &\textbf{Size} &\textbf{Size} &\textbf{Function} & \textbf{(M)} & CPU & V100 GPU & A100 GPU\\
\hline
\textbf{\small{$Teacher$}} & 12	& 12 & 768	& 3072	& gelu	& 110& 217.69(2.98) & 15.62(0.17)& 12.21(0.02)\\
\textbf{\small{$XtremeDistil_{Arch}$}} & 6	& 12 & 384	& 1536	& gelu	& 23 & 40.36(0.41)& 11.31(0.09)&9.22 (0.70) \\
\hline
\textbf{\small{$NAS-Params_{Arch1}$}} & 6	& 4 & 384	& 1024	& gelu	& 21 & 33.65(0.27)
& 10.99(0.04) & 8.66(0.02)\\
\textbf{\small{$NAS-Params_{Arch2}$}} & 10	& 2	& 192 &	1536	& gelu	& 13 & 35.06(0.84)& 13.68 (0.06) & 10.81 (0.01) \\
\hline
\end{tabular}
\end{adjustbox}
%}
\caption{\label{tab:nas-params-model}
Top 3 model architectures selected after 25 episodes of the NAS Process (where reward is a function of accuracy and parameters), compared to the Base and Teacher model. Latency measured in milliseconds on a single GPU and a 2-core CPU, and depicted as an average over 3 runs (standard deviation included in parenthesis).}
\end{table*}

\subsection{Effect of Pretraining on Distillation Loss}
In order to see where pretraining could benefit distillation, we compare the distillation loss curves (i.e. loss curves for stage 1 and 2) of models that have been pretrained with those that have been randomly initialized.

For this analysis, we use $NAS_{Arch2}$, with 37M parameters. We pretrain the model on 2.8 Billion tokens of English Wikipedia, for 400,000 steps. 

Figures \ref{fig-stage1} and \ref{fig-stage2} show the loss curves for the randomly initialized $KDNAS_{Arch2}$, and the pretrained $KDNAS_{Arch2}$ for stage 1 and stage 2 of distillation (corresponding to distilling of teacher embeddings and logits respectively). We see that the pretrained model has a slightly higher loss in stage 1, but ultimately becomes comparable to the randomly initialized model's loss in stage 2. This is counter-intuitive, as we would expect some of the knowledge gained in pretraining to align the embeddings and logits closer to that of the teacher. 

One explanation for this could be that pretraining the models allow it to learn very different representations than the teacher, resulting in the gap between the representation loss in stage 1. A similar explanation has also been posited in previous work \protect\cite{jiao2020tinybert}.

% We experiment with two choices of learning rate, 3e-5 (which was used in the majority of our experiments), and 1e-4. The loss curves of the higher learning rate seem closer together.  \textcolor{red}{ADD EXPLANATION }

\section{Are Smaller Models Faster?}
We experimented with a different choice of reward function for NAS, rewarding on a combination of high accuracy and lower number of parameters (instead of latency, as in Equation \ref{eq:reward-latency}:
\begin{equation}
reward(S_s) = acc_G(DS_s) * \left(\frac{params(S_s)}{\beta * params(T) }\right)^\alpha 
\end{equation}
Here, $\alpha = -0.06, \beta=0.5$, and the purpose of this search was to find \emph{smaller} models than the teacher, and the base. The architecture of the two recommended NAS Models, $NAS-Params_{Arch1}$ and $NAS-Params_{Arch2}$ are shown in Table \ref{tab:nas-params-model}.

As shown in Table \ref{tab:nas-params-model}, the NAS selected architectures are both smaller than the base model while maintaining performance, however, we see that the smaller 13M model is \emph{slower} than both the 23M base model and the other 21M NAS-selected architecture. Therefore, the number of parameters is not an ideal proxy for latency, and smaller models are not necessarily faster.

\end{document}

