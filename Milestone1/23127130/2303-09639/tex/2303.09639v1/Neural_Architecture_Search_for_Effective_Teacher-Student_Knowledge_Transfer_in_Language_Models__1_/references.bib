@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2019roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@misc{sanh2020distilbert,
      title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, 
      author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
      year={2020},
      eprint={1910.01108},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{liu2020search,
      title={Search to Distill: Pearls are Everywhere but not the Eyes}, 
      author={Yu Liu and Xuhui Jia and Mingxing Tan and Raviteja Vemulapalli and Yukun Zhu and Bradley Green and Xiaogang Wang},
      year={2020},
      eprint={1911.09074},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{wang2020minilm,
      title={MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers}, 
      author={Wenhui Wang and Furu Wei and Li Dong and Hangbo Bao and Nan Yang and Ming Zhou},
      year={2020},
      eprint={2002.10957},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mukherjee2021xtremedistiltransformers,
      title={XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation}, 
      author={Subhabrata Mukherjee and Ahmed Hassan Awadallah and Jianfeng Gao},
      year={2021},
      eprint={2106.04563},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{shakeri-etal-2020-end,
    title = "End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems",
    author = "Shakeri, Siamak  and
      Nogueira dos Santos, Cicero  and
      Zhu, Henghui  and
      Ng, Patrick  and
      Nan, Feng  and
      Wang, Zhiguo  and
      Nallapati, Ramesh  and
      Xiang, Bing",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.439",
    doi = "10.18653/v1/2020.emnlp-main.439",
    pages = "5445--5460",
}
@inproceedings{sultan-etal-2020-importance,
    title = "On the Importance of Diversity in Question Generation for {QA}",
    author = "Sultan, Md Arafat  and
      Chandel, Shubham  and
      Fernandez Astudillo, Ram{\'o}n  and
      Castelli, Vittorio",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.500",
    doi = "10.18653/v1/2020.acl-main.500",
    pages = "5651--5656",
}
@inproceedings{bornea2021multilingual,
  title={Multilingual Transfer Learning for QA using Translation as Data Augmentation},
  author={Bornea, Mihaela and Pan, Lin and Rosenthal, Sara and Florian, Radu and Sil, Avirup},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={12583--12591},
  year={2021}
}

@inproceedings{chakravarti2020towards,
  title={Towards building a robust industry-scale question answering system},
  author={Chakravarti, Rishav and Ferritto, Anthony and Iyer, Bhavani and Pan, Lin and Florian, Radu and Roukos, Salim and Sil, Avirup},
  booktitle={Proceedings of the 28th International Conference on Computational Linguistics: Industry Track},
  pages={90--101},
  year={2020}
}

@misc{elsken2019neural,
      title={Neural Architecture Search: A Survey}, 
      author={Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},
      year={2019},
      eprint={1808.05377},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{wang2019glue,
      title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}, 
      author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
      year={2019},
      eprint={1804.07461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bowman2015large,
      title={A large annotated corpus for learning natural language inference}, 
      author={Samuel R. Bowman and Gabor Angeli and Christopher Potts and Christopher D. Manning},
      year={2015},
      eprint={1508.05326},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jin2020bert,
      title={Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment}, 
      author={Di Jin and Zhijing Jin and Joey Tianyi Zhou and Peter Szolovits},
      year={2020},
      eprint={1907.11932},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{du2020selftraining,
      title={Self-training Improves Pre-training for Natural Language Understanding}, 
      author={Jingfei Du and Edouard Grave and Beliz Gunel and Vishrav Chaudhary and Onur Celebi and Michael Auli and Ves Stoyanov and Alexis Conneau},
      year={2020},
      eprint={2010.02194},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{zoph2018learning,
      title={Learning Transferable Architectures for Scalable Image Recognition}, 
      author={Barret Zoph and Vijay Vasudevan and Jonathon Shlens and Quoc V. Le},
      year={2018},
      eprint={1707.07012},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{tan2019mnasnet,
      title={MnasNet: Platform-Aware Neural Architecture Search for Mobile}, 
      author={Mingxing Tan and Bo Chen and Ruoming Pang and Vijay Vasudevan and Mark Sandler and Andrew Howard and Quoc V. Le},
      year={2019},
      eprint={1807.11626},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{bashivan2019teacher,
 author = {Pouya Bashivan and Mark Tensen,James J DiCarlo},
 title = {Teacher Guided Architecture Search},
 journal = {arXiv},
 year = {2019}
}

@article{Wang_2021, title={Teacher Guided Neural Architecture Search for Face Recognition}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/16387}, number={4}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Wang, Xiaobo}, year={2021}, month={May}, pages={2817-2825} }

@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{fedus2021switch,
      title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity}, 
      author={William Fedus and Barret Zoph and Noam Shazeer},
      year={2021},
      eprint={2101.03961},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zoph2017neural,
      title={Neural Architecture Search with Reinforcement Learning}, 
      author={Barret Zoph and Quoc V. Le},
      year={2017},
      eprint={1611.01578},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhou2020econas,
      title={EcoNAS: Finding Proxies for Economical Neural Architecture Search}, 
      author={Dongzhan Zhou and Xinchi Zhou and Wenwei Zhang and Chen Change Loy and Shuai Yi and Xuesen Zhang and Wanli Ouyang},
      year={2020},
      eprint={2001.01233},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{liu2019darts,
      title={DARTS: Differentiable Architecture Search}, 
      author={Hanxiao Liu and Karen Simonyan and Yiming Yang},
      year={2019},
      eprint={1806.09055},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{so2019evolved,
      title={The Evolved Transformer}, 
      author={David R. So and Chen Liang and Quoc V. Le},
      year={2019},
      eprint={1901.11117},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{huang2015bidirectional,
      title={Bidirectional LSTM-CRF Models for Sequence Tagging}, 
      author={Zhiheng Huang and Wei Xu and Kai Yu},
      year={2015},
      eprint={1508.01991},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{ba_2014,
 author = {Ba, Jimmy and Caruana, Rich},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Do Deep Nets Really Need to be Deep?},
 url = {https://proceedings.neurips.cc/paper/2014/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf},
 volume = {27},
 year = {2014}
}

@article{McCulloch1990ALC,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={Warren S. McCulloch and Walter Pitts},
  journal={Bulletin of Mathematical Biology},
  year={1990},
  volume={52},
  pages={99-115}
}

@Techreport{Rosenblatt_1957_6098,
  author = {Rosenblatt, F.},
 address = {Ithaca, New York},
 institution = {Cornell Aeronautical Laboratory},
 month = {January},
 number = {85-460-1},
 title = {The perceptron - A perceiving and recognizing automaton},
 year = {1957},
 title_with_no_special_chars = {The Perceptron  A perceiving and recognizing automaton}
}

@article{LSTM,
    author = {Hochreiter, Sepp and Schmidhuber, JÃ¼rgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

@inproceedings{LSTMBakker,
 author = {Bakker, Bram},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 pages = {},
 publisher = {MIT Press},
 title = {Reinforcement Learning with Long Short-Term Memory},
 url = {https://proceedings.neurips.cc/paper/2001/file/a38b16173474ba8b1a95bcbc30d3b8a5-Paper.pdf},
 volume = {14},
 year = {2002}
}

@INPROCEEDINGS{BookCorpus,
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books}, 
  year={2015},
  volume={},
  number={},
  pages={19-27},
  doi={10.1109/ICCV.2015.11}}
  
@inproceedings{wieting-gimpel-2018-paranmt,
    title = "{P}ara{NMT}-50{M}: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations",
    author = "Wieting, John  and
      Gimpel, Kevin",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1042",
    doi = "10.18653/v1/P18-1042",
    pages = "451--462"
}

@misc{jiao2020tinybert,
      title={TinyBERT: Distilling BERT for Natural Language Understanding}, 
      author={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
      year={2020},
      eprint={1909.10351},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ernie2AAAI2020, 
    title={ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding}, 
    author={Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Tian, Hao and Wu, Hua and Wang, Haifeng},
    volume={34}, 
    number={05}, 
    journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
    pages={8968-8975},
    year={2020}
}

@misc{na2021accelerating,
      title={Accelerating Neural Architecture Search via Proxy Data}, 
      author={Byunggook Na and Jisoo Mok and Hyeokjun Choe and Sungroh Yoon},
      year={2021},
      eprint={2106.04784},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{yang2020nas,
  author    = {Antoine Yang and
               Pedro M. Esperan{\c{c}}a and
               Fabio Maria Carlucci},
  title     = {{NAS} evaluation is frustratingly hard},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2020}
}

@misc{turc2019pretrain,
  doi = {10.48550/ARXIV.1908.08962},
  url = {https://arxiv.org/abs/1908.08962},
  author = {Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title = {Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
