\documentclass[runningheads]{llncs}
% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
%

\usepackage{templates/mainpreambel}

\input{references/commands}
\input{references/constants}

\makeglossaries
\loadglsentries[acronym]{./references/glossary.tex}

% \DeclareLanguageMapping{american}{american-apa}

% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

% \loadglsentries[acronym]{./references/glossary.tex}
\graphicspath{{figures/}}


\usepackage{subfiles} 

% %%% xlu : added to shrink space between figure and text
% % This changes the margin between the figures/tables and the text
% % DO NOT make this space even smaller
% \setlength{\textfloatsep}{8pt}
% \setlength{\intextsep}{6pt}
% \setlength{\floatsep}{6pt}


\begin{document}
%
\title{CREATED: Generating Viable Counterfactual Sequences for Predictive Process Analytics
% using an Evolutionary Algorithm for Event Data
% \title{CREATED: The Generation of viable Counterfactual Sequences using an Evolutionary Algorithm for Event Data of Complex Processes
% \thanks{Supported by organization x.}
}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Olusanmi Hundogan\inst{1}\orcidID{0009-0001-5378-5388}\and
Xixi Lu\inst{1}\orcidID{0000-0002-9844-3330} \and
Yupei Du\inst{1} \and
Hajo A. Reijers\inst{1}\orcidID{0000-0001-9634-5852}}
%
\authorrunning{Hundogan et al.}
\titlerunning{CREATED}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Utrecht University, Utrecht, The Netherlands \\
\email{\{o.a.hundogan, x.lu, y.du, h.a.reijers\}@uu.nl} 
% \and
% TODO
% \and
% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
% \email{\{abc,lncs\}@uni-heidelberg.de}
}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
    % Within the field of Process Mining, deep recurrent networks (such as LSTM) have been used to predict the next state or the outcome of a multivariate sequence. However, these models tend to be complex and are difficult for users to understand of the underlying process model. Counterfactuals answer "what-if" questions, which are used to understand the reasoning behind the predicted outcome of a process. Current methods to generate counterfactual explanations do not take the structural characteristics of multivariate discrete sequences into account. In this work we propose a framework that uses evolutionary methods to generate counterfactuals, while incorporating criteria that ensure their viability. Our results show that it is possible to generate counterfactuals that are viable and automatically align with the factual. The generated counterfactuals outperform baseline methods in viability and yield comparable results compared to other methods in the literature.
    

Predictive process analytics %is an emerging research field in process mining that 
focuses on predicting future states, such as the outcome of running process instances. These techniques often use machine learning models or deep learning models (such as LSTM) to make such predictions. 
%
However, these deep models are complex and difficult for users to understand.
%
% \emph{Counterfactuals} are alternative execution  of a case that lead to a different outcome, help to understand the reasoning of the predictions of the deep models.
%
Counterfactuals answer ``what-if'' questions, which are used to understand the reasoning behind the predictions. For example, what if instead of emailing customers, customers are being called? Would this alternative lead to a different outcome? 
%
Current methods to generate counterfactual sequences either do not take the process behavior into account, leading to generating invalid or infeasible counterfactual process instances, or heavily rely on domain knowledge. 
%
In this work, we propose a general framework that uses evolutionary methods to generate counterfactual sequences. Our framework does not require domain knowledge. Instead, we propose to train a Markov model to compute the \emph{feasibility} of generated counterfactual sequences and adapt three other measures (\emph{delta} in outcome prediction, \emph{similarity}, and \emph{sparsity}) to ensure their overall viability. 
The evaluation shows that we generate viable counterfactual sequences, outperform baseline methods in viability, and yield similar results
% equally good results 
when compared to the state-of-the-art method that requires domain knowledge. 
\keywords{Counterfactual \and Explainable AI \and Predictive Process Analytics \and Evolutionary Algorithm}
% Counterfactual
% explainable AI
% predictive process analytics
% evolutionary algorithms
\end{abstract}
%
%
%
% TODO: Put all the thesis content in the paper
% TODO: Change chapters to sections
% NOTE: Technical papers describe original solutions (theoretical, methodological or conceptual) in the field of IS Engineering. A technical paper should clearly describe the situation or problem tackled, the relevant state of the art, the position or solution suggested and its potential‚ as well as demonstrate the benefits of the contribution through a rigorous evaluation.
% NOTE: Limit is 16 pages

% XIXI: Change cite_author to cite.
\section{Introduction}
\label{ch:intro}
\input{files/01intro}

\section{Related work}
\label{ch:relatedwork}
\input{files/02relatedwork}

% REMOVED Background sections on Process Mining, Multivariate Times-series and Counterfactuals
% \section{Background}
% \label{ch:prereq}
% This chapter explores the most important concepts for this work. Hence, we focus on the problem domain, starting with an overview about \Gls{PM}. Afterwards, we discuss the nature of the data, we handle in this thesis by discussing \emph{Multivariate Discrete Time-Series}. Next, we introduce counterfactuals and establish how we characterise \emph{viable} counterfactuals. 

% \subsection{Process Mining}
% \label{sec:process}
% This thesis focuses on processes and the modelling of process generated data. Hence, it is important to establish a common understanding for this field.
% \subfile{content/sections/sec_pm}

% \subsection{Multivariate Time-Series Modeling}
% \label{sec:sequences}
% The temporal and multivariate nature of \gls{instance} often turns \Gls{PM} into a Multivariate Time-Series Modeling problem. Therefore, it is necessary to establish an understanding for this type of data structure.
% \subfile{content/sections/sec_mlts}


% \subsection{Counterfactuals}
% \label{sec:counterfactuals}
% Counterfactuals are an important explanatory tool to understand a models' cause for decisions. Generating counterfactuals is main focus of this thesis. Hence, we establish the most important chateristics of counterfactuals in this section.

% \subsubsection{What are Counterfactuals?}
% \subfile{content/sections/sec_counterfactuals_criterions}

% % \subsection{Other Criteria}
% % \label{sec:other}
% % \subfile{content/sections/sec_viability_other}

% \subsubsection{The Challenges of Counterfactual Sequence Generation}
% \subfile{content/sections/sec_counterfactuals_challenges}

\section{Background}
% \subsection{Formal Definitions}
\label{sec:formulas}
% Before diving into the rest of this thesis, we have to establish preliminary definitions, we use in this work. With this definitions, we share a common formal understanding of mathematical descriptions of every concept used within this thesis. 

% XIXI: Make much shorter like the example but add your new concepts
% \subsection{Process Logs, Cases and Instance Sequences}
We start by formalising the event log and its elements.

\noindent\textbf{Definition 1: Case, Event and Log} 
Let $\mathcal{E}$ be the universe of the event identifiers and $E \subseteq \mathcal{E}$ a set of events. An event log $L \subseteq \mathcal{E}^*$ is a set of sequences of events. 
Let $C$ be a set of case identifiers and $\pi_c : E \mapsto C$ a surjective function that links every element in $E$ to a case $c \in C$ in which $c$ signifies a specific case. 
For a set of events $E \subseteq \mathcal{E}$, the shorthand $s^c$ denotes a particular sequence $s^c = \langle e_1, e_2, \ldots, e_t \rangle$ with $c$ as case identifier and a length of $t$. Each $s$ is a trace of the process log $s \in L$.  
Let $\mathcal{T}$ be the time domain and $\pi_t : E \mapsto \mathcal{T}$ a non-surjective linking function which strictly orders a set of events. 
% Let $\mathcal{A}$ be a universe of attribute identifiers, in which each identifier maps to a set of attribute values $\overline{a}_i \in \mathcal{A}$. 
% Let $\overline{a}_i$ correspond to a set of possible attribute values by using a surjective mapping function $\pi_A : \mathcal{A} \mapsto A$. 
Each event $e_t$ consists of a set $e_t = \{ a_1 \in A_1, a_2 \in A_2, \ldots, a_I \in A_I\}$ with the size $I = |A|$, in which $A_i$ is an attribute and $a_i$ represents a possible value of that attribute. 
% We define a mapping from an attribute value to its respective attribute identifier $\pi_{\overline{a}} : A \mapsto \mathcal{A}$. Hence, we can map every event attribute value back to its attribute identifier. 

% \subsection{Representation}
\noindent\textbf{Definition 2: Attribute Representation}
Let $\pi_d : A_i \mapsto \mathbb{N}$ be a surjective function, which determines the dimensionality of $a_i$, and let $F$ be a set of size $I$ containing a representation function for every attribute. Let $f_i \in F$ be mapping functions to a vector space $f_i : a_i \mapsto \mathbb{R}^d_i$, in which $d$ represents the dimensionality of an attribute value $d = \pi_d(A_i)$. 
We denote any event $e_t \in s^c$ of a specific case $c$ as a vector, which concatenates every attribute representation $f_i$ as $\mathbf{e}_t^{c} = [f_1; f_2; \ldots; f_I]$. Therefore, $\mathbf{e}_t^{c}$ is embedded in a vector space of size $D$ which is the sum of each individual attribute dimension $D = \sum_i \pi_d(A_i)$. In other words, we concatenate all representations, whether they are scalars or vectors to one final vector representing the event. Furthermore, if we refer to a specific attribute $A_i$, we use the shorthand $\overline{a}_i$. 

% \autoref{fig:representation} shows a schematic representation of a log $L$, a case $c$ and an event $e$.


% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/Graphics/Slide4.PNG}
%     \caption{This figure shows the representation of a log $L$ which contains a number of cases $s$. Case $s^2$ contains a number of events $e_t$. Each events has attribute values $a_i$, which are mapped to vector spaces of varying dimensions. At last, all of the vectors are concatenated.}
%     \label{fig:representation}
% \end{figure}


% REMOVED state space models formal description

% \subsection{Representation}
% \label{sec:representation}
% \subfile{content/sections/sec_representation}

\section{Methods}
\label{ch:methods}
% In this chapter, we describe details of our framework and discuss advantages and limitations. 
% Therefore, we provide a more detailed overview and additionally describe all components. As the framework resembles the work of \cite{hsieh_DiCE4ELInterpretingProcess}, we also discuss differences and similarities between both solutions. 

\subsection{Methodological Framework: CREATED}
\label{sec:framework}
% \subsubsection{Architecture}
% \subfile{content/sections/sec_framework}
To generate counterfactuals, we need to establish a conceptual framework consisting of three main components. The three components are shown in \autoref{fig:approach}. 

% \attention{Change the names of the measures.}
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/framework_slim.png}
    \caption{The CREATED framework: the input is the process log; the log is used to train a predictive model (Component 1) and the generative model (Component 2). This process produces a set of candidates which are subject to evaluation via the validity metric (Component 3).}
    \label{fig:approach}
\end{figure}

The first component is a \emph{predictive model}. As we attempt to explain model decisions with counterfactuals, the predictive model needs to be pretrained. We can use any model that can predict the probability of a sequence. The prediction model in this paper is a simple LSTM model using the process log as an input. The architecture is inspired by \cite{hsieh_DiCE4ELInterpretingProcess_2021}. The model is trained to predict the outcome given a sequence. 

The second component is a \emph{generative model}. The generative model produces counterfactuals given a factual sequence. We implement an evolutionary generator that takes a factual as input and yields counterfactuals candidates as output.
% Specifically, we compare an evolutionary counterfactual generator against three naive generative baseline approaches. These baselines do not optimise towards the viability criteria.  All approaches allow us to use a factual sequence as a starting point for the generative production of counterfactuals. Furthermore, they also generate multiple counterfactual candidates. 

The generated candidates are subject to the third major component. 
To select the most \emph{viable} counterfactual candidate, we evaluate their viability score using a custom metric. 
The metric incorporates four criteria for viable counterfactuals. 
We measure the \textbf{similarity} between two sequences using a multivariate sequence distance metric. The \textbf{outcome-delta} is the difference between the likelihood of the factual and the counterfactual. For this purpose, we require the predictive model, which computes a prediction score reflecting the likelihood. 
We measure \textbf{sparsity} by counting the number of changes in the features and computing the edit distance. Lastly, we need to determine the \textbf{feasibility} of a counterfactual. We measure the feasibility by estimating the probability of a counterfactual. %
Note that our method was developed for outcome prediction but can be adapted to the next activity prediction task.




% \subsection{Semi-Structured Damerau-Levenshtein \\ Distance}
% \label{sec:ssdld}
% Before discussing the viability function, we have to introduce an edit-distance for sequences. An edit-distance is used to compute distance between two sequences. Therefore, they take their \emph{structural} patterns like the length or deletions or inserts into account. However, most approaches tend to focus on the sequence of items (letters or words) without taking into account that each item may have additional attributes. Therefore, we propose a custom edit-distance measure. 
% 
% \subsubsection{Semi-Structured Damerau-Levenshtein}
% % \subfile{content/sections/sec_viability_ssdld}
% \noindent In order to reflect these differences in attribute values, we introduce a modified version of the \gls{damerau_levenshtein}, that not only reflects the difference between two process instances, but also the attribute values. We achieve this by introducing a cost function $\editCost$, which applies to a vector-space. Concretely, we formulate the modified \gls{damerau_levenshtein} as shown in \autoref{eq:modified_dl}. For the remainder, we refer to this edit-distance as \gls{SSDLD}.
% % TODO: Introduce a with a dash above to compare activities instead of the vector
% % Make zero thicker to indicate a null vector
% \begin{align}
%     \label{eq:modified_dl}
%     d_{a, b}(i, j) & =\min
%     \begin{cases}
%         \editDistance{i-1}{j  }+\editCostFunctionNoA & \text { if } i>0                                            \\
%         \editDistance{i  }{j-1}+\editCostFunctionNoB & \text { if } j>0                                            \\
%         \editDistance{i-1}{j-1}+\editCostFunctionBoth & \text { if } i, j>0   \\ & \text { \& } \overline{a}_i=\overline{b}_j                                       \\
%         \editDistance{i-1}{j-1}+ \editCostFunctionNoB +\editCostFunctionNoA  & \text { if } i, j>0  \\ & \text { \& } \overline{a}_i \neq \overline{b}_j                                       \\
%         \editDistance{i-2}{j-2}+\editCostFunction{a_i}{b_{j-1}} + \editCostFunction{a_{i-1}}{b_j} & \text { if } i, j>1 \\ 
%         & \text { \& } \overline{a}_i=\overline{b}_{j-1} \\ 
%         & \text { \& } \overline{a}_{i-1}=\overline{b}_j \\
%         0                                 & \text { \& } i=j=0                                          
%     \end{cases} 
% \end{align}

% \noindent Here, $d_{a, b}(i, j)$ is the recursive form of the Damerau-Levenshtein-Distance. $a$ and $b$ are sequences and $i$ and $j$ specific elements of the sequence. $cost(a,b)$ is a cost function which takes the attribute values of $a$ and $b$ into account. 
% The first two terms correspond to a deletion and an insertion from $a$ to $b$. The idea is to compute the maximal cost for that the wrongfully deleted or inserted event. 
% The third term adds the difference between two events with identical activities $\overline{a}_i$ and $\overline{b}_j$. As mentioned earlier, two events that refer to the same activity can still be different due to event attributes. The distance between the event attributes determines \emph{how} different these events are. 
% The fourth term handles the substitution of two events. Here, we compute the substitution cost as the sum of an insertion and a deletion. 
% The fifth term computes the cost after transposing both events. This cost is similar to term 3 only that we now consider the differences between both events after they were aligned. The last term relates to the stopping criterion of the recursive formulation of the \gls{damerau_levenshtein}.  

% \subsubsection{Discussion}
% % \subfile{content/sections/sec_viability_ssdld_discussion}
% Given the current viability measure, we can already determine the optimal counterfactual:
% \begin{displayquote}
%     The optimal counterfactual flips a model's strongly expected factual outcome to the desired outcome, maintaining the same trajectory as the factual in terms of events, with minimal changes in its event attributes, while remaining feasible according to the data.
% \end{displayquote}

% \noindent The elements that fulfil these criteria make up the Pareto surface of this multi-valued viability function. If each value is scaled with a range between 0 and 1, the theoretical ceiling is 4. This value is only possible if we flip a factual sequence's outcome without changing it. As this is naturally impossible for deterministic model predictions, the viability has to be lower than 4. 

% Furthermore, we can already postulate that a viability of 2 is a critical threshold. If we score the viability of a factual against itself, a normalised sparsity and similarity value have to be at its maximal value of 1. In contrast, the improvement has to be 0. The feasibility is 0 depending on whether the factual were used to estimate the data distribution or not. With these observations in mind, we determine that any counterfactual with a viability of at least 2 is already better than the factual. 


% XIXI: For viability functions its not important to explain existing methods or discuss them. Sparsity function is a good example. Shorten the delta function section a bit and refer to the thesis.
% XIXI: Shorten SSDLD to explain that it is a damerau-levenshtein modification but instead of dealing with atomic symbols (cost of 1) it uses as cost similarity functions for vector sequences. Weighted Damerau Levenshstein where weight is cost function for vectors






\subsection{Counterfactual Generators}
\label{sec:model_generation}
\textbf{Generative Model: Evolutionary Algorithm}
\label{sec:model_evolutionary}
% \subfile{content/sections/sec_model_evolutionary}
% We introduced most operator types in \autoref{sec:evo}.
In this section, we describe the concrete set of operators and select a subset that we want to explore further.

For our purposes, the \emph{gene} of a sequence consists of the sequence of events within a \gls{instance}. Hence, if an offspring inherits one parent gene, it inherits the activity associated with the event and its event attributes. Our goal is to generate candidates by evaluating the sequence based on our viability measure. Our measure acts as the fitness function. The candidates that are deemed fit enough are subsequently selected to reproduce offspring. This process is explained in \autoref{fig:example-inheritance}. 

\begin{figure}[t]
    \begin{tikzpicture}[>=stealth,thick,baseline]
        \matrix [matrix of math nodes, left delimiter=(,right delimiter=)](m1){
            a   & b    & a    \\
            0.6 & 0.25 & 0.70 \\
            0   & 0    & 1    \\
            1.2 & 4.5  & 2.3  \\
        };
        \node[draw,  blue!80, dashed, thin, inner sep=2mm,fit=(m1-1-1.west) (m1-4-2.east)] (attbox) {};
        \node[above = 2mm of m1](rlbl) {Parent 1};
        \node[below = 2mm of attbox, blue!80](lb1) {\tiny Genes passed on};


        \node[right = 2mm of m1](cr){+};

        \matrix [matrix of math nodes,left delimiter=(,right delimiter=), right = 2mm of cr](m2){
            a   & b    & a    & c    \\
            0.6 & 0.75 & 0.64 & 0.57 \\
            0   & 0    & 1    & 0    \\
            1.2 & 4.5  & 3.3  & 3.0  \\
        };
        \node[draw,  red!80, dashed, thin, inner sep=2mm,fit=(m2-1-3.west) (m2-4-4.east)] (attbox-m2) {};
        \node[above = 2mm of m2](rlbl) {Parent 2};
        \node[below = 2mm of attbox-m2, red!80](lb2) {\tiny Genes passed on};


        \node[right = 2mm of m2](eq){=};

        \matrix [matrix of math nodes,left delimiter=(,right delimiter=), right = 2mm of eq](m3){
            a   & b    & a    & c    \\
            0.6 & 0.25 & 0.64 & 0.57 \\
            0   & 0    & 1    & 0    \\
            1.2 & 4.5  & 3.3  & 3.0  \\
        };

        \node[draw,  blue!80, dashed, thin, inner sep=2mm,fit=(m3-1-1.west) (m3-4-2.east)] (attbox-m3-1) {};
        \node[draw,  red!80, dashed, thin, inner sep=2mm,fit=(m3-1-3.west) (m3-4-4.east)] (attbox-m3-2) {};
        \node[below = 2mm of attbox-m3-1, blue!80](lb2) {\tiny Inherited};
        \node[below = 2mm of attbox-m3-2, red!80](lb2) {\tiny Inherited};

        \node[above = 2mm of m3](rlbl) {Offspring};
        % \draw[->, outer] (m2.east) -- (m3.west);
    \end{tikzpicture}
    \caption{A newly generated offspring inheriting genes in the form of activities and event attributes from both parents.}
    \label{fig:example-inheritance}
\end{figure}


The offspring is subject to mutations. We evaluate the new population and repeat the procedure until a termination condition is reached. We can optimise the viability measure established in Section~\ref{sec:viability}.

\newcommand{\cf}{\text{counterfactuals}}
\newcommand{\cfp}{\text{cf-parents}}
\newcommand{\cfo}{\text{cf-offsprings}}
\newcommand{\cfm}{\text{cf-mutants}}
\newcommand{\cfs}{\text{cf-survivors}}


\begin{algorithm}[b]
    \caption{The basic structure of an evolutionary algorithm.}
    \begin{algorithmic}
        \Require{factual, configuration, sample-size, population-size, mutation-rate, termination-point}
        % \Require{}
        % \Require{}
        % \Require{}
        % \Require{}
        % \Require{}
        \Ensure{The result is the final counterfactual sequences}

        \State $counterfactuals \gets initialize(\text{factual})$
        \While{not $termination$}
        \State $\cfp \gets select(\cf, \text{sample-size})$
        \State $\cfo \gets crossover(\cfp) $
        \State $\cfm \gets mutate(\cfo, \text{mutation-rate})$
        \State $\cfs \gets recombine(\cf, \cfm, \text{population-size})$
        \State $termination \gets determine(\cfs, \text{termination-point})$
        \State $\cf \gets \cfs$
        \EndWhile
    \end{algorithmic}
    \label{alg:my-evolutionary}
\end{algorithm}

\noindent\textbf{Operators}
We implemented several different evolutionary operators. Each one belongs to one of five categories. The categories are initiation, selection, crossing, mutation, and recombination. Table~\ref{tab:abbreviation} contains a complete list of the operators.

% XIXI: Shorten many of the descriptions by describing a standard reference book.
% XIXI: Likes the images for OPC, TPC and UPC

\begin{table}
\begin{center}
\caption{An overview of all evolutionary operators used in this paper and a short description.}
\label{tab:abbreviation}
% \begin{small}
\begin{tabular}{ | m{0.8cm} | m{2.5cm}| m{8.5cm} | } 
% \begin{tabular}{ |l|l|l| } 
\hline
    Label & Name & Description\\
    
    \hline
    % \hline
    \multicolumn{3}{|l|}{Initiation}\\
    \hline
    RI & Random Initialisation & Generates an initial population in which the event sequence was chosen at random based on the log. The event attributes were drawn from a normal distribution. \\ 
    SBI & Sampling-Based Initialisation & Generates an initial population by sampling from a data distribution estimated from the data directly. The event sequence was sampled using the event transition probabilities. The attributes were sampled using distributions conditioned on the emitted events.\\ 
    CBI & Case-Based Initialisation & Samples initial population directly from the Log. \\ 
    \hline
    % \hline
    \multicolumn{3}{|l|}{Selection}\\
    \hline
    RWS & Roulette-Wheel-Selection & Selects individuals randomly in proportion to their fitness value  \\
    TS & Tournament-Selection & Selects pairs of individuals and compares each pair. The better individual between both pairs has a higher chance of being selected.\\
    ES & Elitism-Selection & Selects individual with the highest fitness. \\
    \hline
    % \hline
    \multicolumn{3}{|l|}{Crossover}\\
    \hline    
    UCx & Uniform Crossover & uniformly choose a fraction of genes of one individual (\emph{Parent 1}) and overwrite the respective genes of another individual (\emph{Parent 2}).\\
    OPC & One-Point Crossover & Chooses a point in the sequence and overwrites the genes of \emph{Parent 2} by the genes \emph{Parent 2} from that point onward.\\
    TPC & Two-Point Crossover & Chooses two points in the sequence and overwrites the sequence in between the two points from \emph{Parent 2} with the sequence from \emph{Parent 1}.\\
    \hline
    % \hline
    \multicolumn{3}{|l|}{Mutation}\\

    \hline    
    RM & Random-Mutation & Inserts, changes or deletes activities randomly. Event attributes are drawn from a normal distribution.\\
    SBM & Sampling-Based Mutation & Inserts, changes or deletes activities randomly. Event attributes are drawn from an estimated data distribution.\\
    \hline
    % \hline
    \multicolumn{3}{|l|}{Recombination}\\
    \hline   
    % 
    FSR & Fittest-Survivor Recombination & Strictly determines the survivors among the mutated offsprings and the current population by sorting them in terms of viability\\
    BBR & Best-of-Breed Recombination & Determines offsprings that are better than the average within their generation and adds them to survivors of past generations.\\
    RR & Ranked Recombination & selects the new population differently than the former recombination operators. Instead of using the viability directly, we sort each individuum by every viability component separately. This approach allows us to select individuals regardless of the scales of every individual viability measure.\\
    \hline
\end{tabular}
% \end{small}
\end{center}
\end{table}


\noindent\textbf{Naming-Conventions}
We use abbreviations to refer to each model configuration. For instance, \emph{CBI-RWS-OPC-RM-RR} refers to an evolutionary operator configuration that samples its initial population from the data (CBI), probabilistically samples parents based on their fitness (RWS), crosses them on one point (OPC), and so on. For the \emph{Uniform-Crossing} (UCx) operator, we additionally indicate its crossing rate using a number. For instance, \emph{CBI-RWS-UC3-RM-RR} uses the \emph{Uniform-Crossing} (UC3) operator. The child receives roughly 30\% of the genome of one parent and 70\% of another parent. 

\noindent\textbf{Hyperparameters}
The evolutionary approach comes with a number of hyperparameters. 
We first discuss the \emph{model configuration}. As shown in this section, there are a \NumEvoCombinations ways to combine all operators. Depending on each operator combination, we might see very different behaviours. 
The decision of the appropriate set of operators is by far the most important in terms of convergence speed and result quality.
The next hyperparameter is the \emph{termination point} which determines the duration of the search. 
Optimally, we find a termination point, which is not too early but not too late, too.
The \emph{mutation rate} is another hyperparameter. It signifies how much a child can differ from its parent.



\subsection{Viability Measure}
\label{sec:viability}

\subsubsection{Feasibility-Measure}
\label{sec:feasibility}
% \subfile{content/sections/sec_viability_feasibility}
To determine the feasibility of a counterfactual trace, it is important to consider two aspects. 
%
First, we have to compute the probability of the sequence of event transitions. This is a difficult task, given the \emph{Open World assumption}\footnote{In theory, we cannot know whether or not any event \emph{can} follow after another event.}. 
Therefore, we have to assume the data is representative and the underlying process is static. This assumption allows us to estimate first-order transition probabilities by counting event transitions.

% However, if the data is representative of the process dynamics, we can make simplifying assumptions. 
% For instance, we can compute the first-order transition probability by counting each transition. The issue remains that longer sequences tend to have a zero probability if they have never been seen in the data. 

Second, we have to compute the feasibility of the individual feature values given the sequence. We can relax the computation of this probability using the \emph{Markov Assumption}. In other words, we assume that each event vector depends on the current activity but on none of the previous events and features. This means that we can model density estimators for every event and use them to determine the likelihood of a set of features.

We define the feasibility measure in \autoref{eq:feasibility_measure}, where $e_t$ represents the current event, transited from the previous event $e_{t-1}$. Likewise, $f$ represents the emission of the feature attributes. Hence, the probability of a particular sequence is the product of the transition probability multiplied by the state emission probability for each step. 
% Note that this is the same as the feasibility measure in \autoref{eq:feasibility_measure}. 

\begin{align}
    \prob{e_{0:T},f_{0:T}} & = \prob{e_0}\cprob{f_0}{e_0}\prod_1^T \cprob{e_t}{e_{t-1}} \cprob{f_t}{e_t}
    \label{eq:feasibility_measure}
\end{align}



\subsubsection{Delta-Outcome}
\label{sec:delta}
% \subfile{content/sections/sec_viability_delta}
For the delta measure, we evaluate the likelihood of a counterfactual trace by determining whether a counterfactual leads to the desired outcome or not. For this purpose, we use the predictive model, which returns a prediction for each counterfactual sequence. As we are predicting process outcomes, we typically predict a class. However, forcing a deterministic model to produce a different class prediction is often difficult. Therefore, we can relax the condition by maximising the prediction score of the desired counterfactual outcome~\cite{molnar2019}. If we compare the difference between the counterfactual prediction score with the factual prediction score, we can determine an increase or decrease. Ideally, we want to increase the likelihood of the desired outcome. We refer to this value as \emph{delta}. For the binary outcome prediction case, we define the function as shown in \autoref{eq:delta}.

\begin{align}
    \label{eq:delta}
%     d_{a, b}(i, j) & =\min
    delta &= 
    \begin{cases}
            |p(o|s^*)-p(o|s)| &  \text{if }  p(o|s) > 0.5 \text { \& }  p(o|s) > p(o|s^*) \\                 
            -|p(o|s^*)-p(o|s)| &  \text{if }  p(o|s) > 0.5 \text { \& }  p(o|s) \leq p(o|s^*) \\                 
            |p(o|s^*)-p(o|s)| &  \text{if }  p(o|s) \leq 0.5 \text { \& }  p(o|s) > p(o|s^*) \\                 
            -|p(o|s^*)-p(o|s)| &  \text{if }  p(o|s) \leq 0.5 \text { \& }  p(o|s) \leq p(o|s^*) \\                 
    \end{cases} 
\end{align}

\subsubsection{Similarity Measure}
\label{sec:similarity}
We use a function to compute the \textbf{similarity} between the factual sequence and the counterfactual candidates. To incorporate differences in length between both sequences, we use a weighted version of the Damerau-Levenshtein distance~\cite{damerau_TechniqueComputerDetection_1964}. %
The Damerau-Levenstein distance applies a cost constant of 1 for each sequential difference. However, as process instances differ not only in event sequences but also in their event attribute values, we use a distance function to weigh the cost. In the case of \textbf{similarity}, we apply the euclidian distance. For formal definitions, we refer to~\cite[p.~42]{Hundogan2022}. 

% \vspace{-2em}
\subsubsection{Sparsity Measure}
\label{sec:sparsity}
For measuring the sparsity, we use the same weighted version of the Damerau-Levenshtein distance. However, to measure the distance, we count the number of differences between event attributes. For formal definitions, we refer to~\cite[p.~42]{Hundogan2022}.


% \textcolor{gray}{Furthermore, instead of defining the cost only in terms of event sequences, we incorporate cost function which takes the attribute values of an event into account. For measuring the similarity we rely on the euclidean distance.
% To measure the sparsity we simply count the differences between the event attributes. We typically want to minimize the number of changes. As with similarity, we incorporate structural differences between the two sequences by using the modified Damerau-Levenshtein distance.}



% \vspace{-2em}
\subsubsection{Viability-Measure}
\label{sec:viability}
We combine the feasibility measure, the outcome delta, the normalised sparsity, and normalised similarity measure by summation. As each measure can have values between 0 and 1, the viability measure ranges between 0 and 4. For more details on the viability measure, we refer to \cite[Chap.~3.3]{Hundogan2022}. 


\section{Evaluation}
\label{ch:evaluation}
% In this section, we discuss the datasets, the preprocessing pipeline, and the final representation for each of the algorithms.  
% There, you will find instructions on how to install and run the experiments yourself.


\subsection{Datasets}
\label{sec:dataset_description}
% \subfile{content/sections/sec_dataset_stats}
% XIXI: Reference the dataset for outcome prediction and say we used this publically available benchmark datasets
For our evaluation, we use ten event logs of three real-life processes, which were also used in~\cite{teinemaa_OutcomeOrientedPredictiveProcess_2019}. Each dataset consists of events and contains labels that signify a process instance's outcome. We focus on binary outcome predictions. 
We include a variation of the BPIC dataset. This dataset was used in \cite{hsieh_DiCE4ELInterpretingProcess_2021}. The difference between Hsieh et al.'s dataset and the original dataset is two-fold. First, the authors focus on the generation of two event attributes. Second, the dataset is primarily designed for next-activity prediction, not outcome prediction. We modified the dataset to fit the outcome prediction model.
%
For more information about these datasets we refer to the comparative study by \cite{teinemaa_OutcomeOrientedPredictiveProcess_2019}. We list the important descriptive statistics in \autoref{tbl:dataset-stats}.


\begin{table}[t]
    \caption{All datasets used within the evaluation. DiCE4EL is used for the qualitative evaluation, and the remaining are used for quantitative evaluation purposes.}
    \label{tbl:dataset-stats}
    % \begin{adjustbox}{center}
        \makebox[\linewidth]{
            \input{./tables/generated/dataset_stats.tex}
            }
            % \end{adjustbox}
\end{table}


% \subfile{content/sections/sec_dataset_preds}
\begin{table}[b]
    \caption{The evaluation metrics for the prediction component on all datasets. Includes precision, recall and f1 score for test, training and validation data.}
    \label{tbl:dataset-preds}
    % \begin{adjustbox}{center}
        \makebox[\linewidth]{
            \input{./tables/generated/dataset-preds.tex}
            }
            % \end{adjustbox}
\end{table}

We list the predictions of our prediction component in \autoref{tbl:dataset-preds}. The F1-Scores on the test sets are generally higher for the BPIC dataset. Furthermore, in the case of the BPIC datasets, the prediction model always predicts the correct outcome if the max-length of the sequence exceeds 25. It is fair to assume that the length of a loan application process determines the chance of getting rejected or not. 



\subsection{Preprocessing}
\label{sec:preprocessing}
% XIXI: The issue of using maximum 25 seq-len is a limitation that should be discussed again.
To prepare the data for our experiments, we employed basic tactics for preprocessing. First, we split the log into a training and a test set. 
Then, we filter out every case whose sequence length exceeds 25. We keep this maximum threshold for most experiments focusing on the evolutionary algorithm. The reason is the polynomial computation time of the viability measure. The similarity and sparsity components of the proposed viability measure have a runtime complexity of at least $N^2$. Hence, limiting the sequence length saves a substantial amount of temporal resources.
Next, we extract time variables if they are provided in the log. Then, we normalise the values. 
Each categorical variable is converted using binary encoding. 
The activity is label-encoded. As a result, every category is assigned to a unique integer. The label column is binary encoded, as we focus on outcome prediction.
Lastly, we pad each sequence towards the longest sequence in the dataset.

\subsection{Baseline Models}
We use three baseline models and compare them to the evolutionary models. The first baseline generates a random sequence of events and event attributes. Hence, we refer to this approach as \textbf{Random baseline} (RGW). We expect most models to perform better than this baseline. Otherwise, it would indicate that a random search would generate better counterfactuals than a guided one. The second baseline resembles the random baseline. However, we use the data likelihood to guide the random search for the generation of counterfactuals. We first generate a random seed of possible starting events ($\prob{e_0}$). Afterwards, we randomly sample subsequent events by iteratively sampling new activities according to the transition probabilities we gathered from the data ($\prod_1^T \cprob{e_t}{e_{t-1}}$). Given the sequence, we simply sample the features per event from $\cprob{f_t}{e_t}$. We call this baseline \textbf{Sample-Based} (SBGW). In contrast to both sampling-based baselines, the last baseline leverages actual examples of the data. We refer to this case-based approach as \textbf{Case-Based baseline} (CBGW). The idea is to randomly pick traces from the log and evaluate them using the viability measure.

\subsection{Experimental Setup}
\label{sec:experimental_setup}
% \subfile{content/sections/sec_experimental_setup}
% Counterfactual generation is notorious for lacking a standardised evaluation procedure. Nonetheless, we try to address our research questions with the following experiments. 
All the experiments were run on a Windows machine with 12 processor cores (Intel Core i7-9750H CPU 2.60GHz) and 32 GB Ram. The code is written in Python version 3.8. 
The models were developed with Tensorflow~\cite{abadi2016tensorflow} 
and NumPy~\cite{2020NumPy-Array}. 
We provide the full code and instructions on Github~\cite{hundogan_CREATEDGeneratingViable_2022}.

% \textbf{Experiment 1: Model Selection}
% The first set of simulations is dedicated to choosing among a subset of operator combinations and selecting appropriate hyperparameters. 
% First, we reduce the number of models that we compare against the baseline approaches in later experiments. In terms of operators, we introduced three initiators, three selectors, three crossers, two mutators and three recombiners. Hence, we compare all \NumEvoCombinations evolutionary operator combinations. We compute all possible configurations without changing any hyperparameter.

% After executing all preliminary simulations, we choose the best evolutionary generators and compare them with all baseline models in all subsequent experiments.

In terms of operators, we introduced three initiators, three selectors, five crossers, two mutators, and three recombiners. 
\textcolor{red}{
For the experiments, we exclude the random mutator as preliminary experiments showed that it often leads to results with a feasibility of 0.
} To reduce the number of model configurations, we initially compare all \NumEvoCombinations evolutionary operator combinations. We select the best three models and compare them to the three baseline models. 
Afterwards, we assess the viability of all the chosen evolutionary and baseline generators. \textcolor{red}{We sample 10 factuals from the BPIC-25 dataset and use our models as well as the baselines to generate 50 counterfactuals for each factual.} We determine the mean viability across the counterfactuals. We expect the evolutionary algorithms to outperform the baselines when it comes to viability.
In the end, we assess the quality of the generated counterfactuals. In line with \cite{hsieh_DiCE4ELInterpretingProcess_2021}, we aim to answer the question \emph{what would one have had to change in order to flip the outcome of a process}. The goal is to show that the counterfactuals our models generate are viable without having to rely on domain-specific knowledge. \textcolor{red}{In the current paper, we did not include any results of the individual viability components. Furthermore, we refer to \cite[p.64]{Hundogan2022} for more specific and extensive observations.}



\section{Results}
\label{ch:results}
% This section presents the results of each evaluation step. Furthermore, we analyse the results.

\subsection{Experiment 1: Comparing with Baseline Generators}
\label{sec:experiment2}
We examined a set of model-configurations containing \NumEvoCombinations elements. We choose to run each model configuration for 100 evolution cycles. 
% For all model configurations, we use the same 4 factual \glspl{instance} randomly sampled from the test set. \autoref{fig:average-viability} shows the bottom and top-5 model configurations based on the viability after the final iterative cycle. We also show how the viability evolves for each iteration. 
\textcolor{red}{We randomly sample four factual \glspl{instance} from the test set. Afterwards, we use the average viability across the instances to evaluate all model configurations.} \autoref{fig:average-viability} shows the bottom and top-5 model configurations based on the viability after the final iterative cycle. The figure also shows how the viability evolves for each iteration. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/generated/exp1_effect_on_viability_top10_last10.png}
    \caption{This figure shows the average viability of the five best and worst model configurations. The x-axis shows how the viability evolves for each evolutionary cycle. The semi-transparent lines are the model configurations that are neither in the best five nor worst five groups. They show the general trend of the viability improvement.}
    \label{fig:average-viability}
\end{figure}

According to \autoref{fig:average-viability}, \emph{CBI-ES-UC3-SBM-RR}, \emph{CBI-RWS-OPC-SBM-BBR}, and \emph{CBI-RWS-OPC-SBM-FSR} are the best model configurations. As all best-performing model-configurations use the \emph{Case-Based Inititiation}-operator, we identify it as the most important configuration. The results suggest that the initiation operator governs the starting point of the optimisation.
For the following experiment, we ran each evolutionary algorithm for 200 iterative cycles and set the mutation rate to 0.01.  

Next, we employed the baseline models mentioned in Section~\ref{sec:model_generation} and examined their results across all datasets. We randomly sampled 20 factuals from the test set and used the same factuals for every generator. We ensured that the outcomes are evenly divided. The remaining procedure followed the established practice of previous experiments. 
The results in \autoref{fig:exp4-winner} show that the evolutionary algorithm \optional{CBI-ES-UC3-SBM-RR} returns better results when it comes to the mean viability. The worst model is the randomly generated model. 
The Case-Based model appears to be evenly and normally distributed at a viability of \optional{2.25}. The \optional{CBI-RWS-OPC-SBM-FSR} has outliers that far exceed and underperform against other evolutionary algorithms on both ends.

\begin{figure}[b]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/generated/exp4_winner_overall_mod.png}
    \caption{This figure shows boxplots of the viability of each model's generated counterfactuals.}
    \label{fig:exp4-winner}
\end{figure}

\autoref{fig:exp5-winner} displays the results of running each algorithm on a set of different datasets. The figure shows a clear dominance of the evolutionary models across all datasets. 
Here, \emph{CBI-ES-UC3-SBM-RR} and \emph{CBI-RWS-OPC-SBM-FSR} display a higher median of viability across all datasets. 
This is unsurprising as the evolutionary algorithm uses initiators based on the baselines. 
However, it is surprising that the evolutionary models consistently outperform the \ModelCBG (green) across all datasets. In six out of nine datasets, we see an improvement of at least 0.15. 
The highest median is reached for \emph{CBI-RWS-OPC-SBM-FSR} at 2.94. 
The \ModelRNG never manages to come even close to the case-based model. Except for the BPIC12-100 dataset, the \ModelRNG has a median below 2. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/generated/exp5_winner_overall.png}
    \caption{Boxplots of the viability of each model's generated counterfactuals across a heterogeneous collection of datasets.}
    \label{fig:exp5-winner}
\end{figure}



The results for \autoref{fig:exp5-winner} show that both evolutionary algorithms outperform the competition across \emph{all} datasets and against \emph{all} baselines. 
This result shows that the algorithm can outperform baselines regardless of the process log and its length.
The baseline comparison also shows that we can optimise towards viability successfully. Recall that we defined four criteria for the viability of counterfactuals (similarity, sparsity, feasibility, and delta in likelihood); a model optimising towards those criteria can apparently return superior results. 


\subsection{Experiment 2: Qualitative Assessment}
\label{sec:experiment4}
% \subsubsection{Results}
% \subfile{content/sections/sec_experiment_7}

\todo{XL: I would remove the resource column in table 4 and 5.}
\autoref{fig:exp7-FSR-1} shows the generation of the model-configuration \optional{CBI-RWS-OPC-SBM-FSR} and the model of \cite{hsieh_DiCE4ELInterpretingProcess_2021}. Both models also return reasonable counterfactuals. The counterfactual sequence of events of both approaches are almost identical. For instance, our counterfactual and the D4EL counterfactual recognize that after O-SENT, there appears at least one \emph{W-Completeren aanvraag} and one \emph{W-Nabellen offertes} that eventually leads to an acceptance of the counterfactual. We also see that both evolutionary algorithms start the process with the correct sequence of A-SUBMITTED, A-PARTLYSUBMITTED and A-PREACCEPTED. These are strictly the same across all cases. If our generative model had not recognised these, one could question its utility.

In \autoref{fig:exp7-FSR-2} we applied the same approach on a different dataset. The generator generates a counterfactual that is close to the original factual and only modifies the number of open cases. Here, we can conclude that a sudden increase in open cases during the \emph{Add penalty} step results in a change of outcome.

The examples show that our generative approach does not rely on domain knowledge, such as milestones. In contrast, the approach by \cite{hsieh_DiCE4ELInterpretingProcess_2021} only applies to datasets with clear milestones such as \emph{BPIC-12}. 


\begin{table}[t]
    \centering    
    \caption{A comparison between the CBI-RWS-OPC-SBM-FSR and D4EL}
    \label{fig:exp7-FSR-1}
    \resizebox{\linewidth}{!}{
    \input{./tables/counterfactuals/ES-EGW-CBI-RWS-OPC-SBM-FSR-IM-49-2.tex}
    }
\end{table}

\begin{table}[tb]
    \centering    
    \caption{A counterfactual for the Traffic-Fines dataset by the CBI-RWS-OPC-SBM-FSR model}
    \label{fig:exp7-FSR-2}
    \resizebox{\linewidth}{!}{
    \input{./tables/counterfactuals/counterfactual_general-ES-EGW-CBI-RWS-OPC-SBM-FSR-IM-49-2.tex}
    }
\end{table}

% \begin{table}[h]
%     \centering    
%     \resizebox{\linewidth}{!}{
%     \input{./tables/counterfactuals/counterfactual_general-ES-EGW-CBI-ES-UC3-SBM-RR-IM-49-3.tex}
%     }
% \caption{A comparison between the CBI-ES-UC3-SBM-RR and D4EL}
% \label{fig:exp7-FSR-2}
% \end{table}

\subsection{Discussion and Limitations}

All models successfully flip the outcome of the prediction model and are close to the factual. In contrast, the model by \cite{hsieh_DiCE4ELInterpretingProcess_2021} proposes more changes to the sequence. It is important to recall that the generated counterfactuals focus on explaining the prediction model rather than the true process. More specifically, our generative model shows which events and attributes have to be present or omitted to flip the outcome of the prediction model. 
% If our framework attempts to explain how a prediction model behaves, then its applicability to real-world scenarios depends on the prediction model's performance. 

In contrast to \cite{hsieh_DiCE4ELInterpretingProcess_2021}, we show that we can create these counterfactuals without incorporating domain-specific knowledge, such as an understanding of milestone patterns. 
Domain knowledge can help to improve or evaluate our solutions. However, they are not strictly required.  
Furthermore, our models can generate sequences not present within the input event log. Case-based solutions often overlook this aspect, as they are heavily biased toward the input data. 

\textcolor{red}{It is worthwhile to discuss that \emph{counterfactual sequences} differ from \emph{counterfactual rules} or \emph{explanations}. To obtain explicit explanations or rules, the generated counterfactuals should be compared to the factual. Our framework enables some alignments between the generated counterfactuals with the factual sequence (see~\autoref{fig:exp7-FSR-1}), which may act as an explanation. We consider deriving rules as a post-prior analysis, which is interesting for future work.}


% All in all, we claim that the generator model can teach us more about the prediction model primarily. Further improvement might show even more nuance in the model's behaviour. We discuss some of them in the discussion chapter.

% \section{Discussion}
% \label{ch:discussion}
% In this chapter, we are going to reexamine many of the past decisions we made. We critically assess the results of experiments and how we interpret them. We also propose possible improvements and opportunities for future research.

% \subsection{Interpretation of Results}
% % \subfile{content/sections/sec_discussion_interpretation}
% In the following, we discuss the results along three aspects. First, the quality in terms of the viability of the counterfactual sequences generated by our models. Second, their quality compared to two baseline approaches and the state-of-the-art DICE4EL approach. Third, their implications in terms of the general utility of our solution.

% \begin{enumerate}
%     \item The quality in terms of the viability of the counterfactual sequences generated by our models.
%     \item Their quality compared to two baseline approaches and the state-of-the-art DICE4EL approach.
%     \item Their implications in terms of the general utility of our solution.
% \end{enumerate}

% Our first experiment shows that we can optimise towards viability successfully. We defined four criteria for the viability of counterfactuals (similarity, sparsity, feasibility, and delta in likelihood) and showed that a model optimising towards those criteria can return superior results. 
% Furthermore, we created models capable of optimising complicated operationalisations of these criteria without the limitation of a function with a clearly defined gradient. 

% We highlight how it is possible to modify the counterfactual generation based on the decision criterion someone uses to optimise them. Specifically, the model that selected iteration survivors based on an explicit sorted ranking created more feasible results. Those results reflected patterns within our log far more than the model that exclusively focused on improving the viability measure. In contrast, this model showed that structure can play a crucial role in understanding why a counterfactual might change the outcome of a process. 

% The results of the second experiment suggest the feasibility of generating viable counterfactuals. Similar to \cite{hsieh_DiCE4ELInterpretingProcess_2021}, our counterfactuals successfully change the factual outcome to one that we desire. 
% We have to note that these counterfactuals are primarily a reflection of the underlying prediction model. 

% If our framework attempts to explain how a prediction model behaves, then its applicability to real-world scenarios depends on the prediction model's performance. 

% The viability measure we proposed shows that structural difference can help us better understand when and where we must apply counterfactual changes. Other approaches often seem to overlook the importance of the sequence structure. However, the \optional{CBI-RWS-OPC-SBM-FSR} model shows that it may be reasonable to incorporate structural differences in our viability measures. Especially, if we talk about sequences and processes. The gaps within the counterfactuals our models produced clearly indicate that. If a model attempts to align sequences, it becomes much easier to compare them side-by-side.  

% In contrast to the closest alternative approach by Hsieh et al.~\cite{hsieh_DiCE4ELInterpretingProcess_2021}, we show that we can create these counterfactuals without incorporating domain-specific knowledge such as an understanding of milestone patterns. Domain knowledge can always help us create better solutions. However, we do not always have access to them. We believe that showing it is possible to create viable counterfactuals without domain-specific knowledge is our most significant contribution. Furthermore, our models can generate solutions not currently present within the data. Case-based solutions often overlook this aspect, as they are heavily biased towards the data input. Second, they can fail to deliver the necessary structural nuance when understanding sequences.


% \subsection{Limitations}
% \subfile{content/sections/sec_discussion_limitations}
Our viability components showed that they can lead to an optimised solution. However, there are most likely other ways to operationalise viability criteria. In addition, what makes an excellent counterfactual and how we can quantify that is still a subject of debate. Currently, there is a lack of standardized evaluation protocols, benchmark techniques, and datasets. As a result, many researchers fall back on defining their custom evaluation methods.
% as there is no standardized way to evaluate the viability of a counterfactual. 
In fact, this is still an open research question~\cite{hsieh_DiCE4ELInterpretingProcess_2021,mothilal_ExplainingMachineLearning_2020}. Therefore, we often have to evaluate the counterfactuals in some subjective and qualitative way. In this paper, we decided to compare the counterfactuals with another approach in the literature and the factual themselves. Because our counterfactuals produced reasonable results, we deemed them viable. As future work, we also see value in incorporating experts to evaluate such an approach. 


% Furthermore, we did not take diversity into account. Our models strictly optimize towards the optimization goal. However, as we discussed, diversity can help us better understand factuals.

% When it comes to the evolutionary algorithm, we have to admit that there are most likely more advanced and more efficient algorithms that utilise the notion of evolution. Our approach mainly followed the basic structure of an evolutionary algorithm. However, there are methods such as CMA-ES capable of improving the efficiency of the evolutionary generation. 


% \noindent \textbf{Future Work.}
% % \subfile{content/sections/sec_discussion_improvements}
% There are several improvements we propose. First, the feasibility metric often resulted in far lower values than other metrics. The small probabilities we saw are emblematic of the probabilistic sphere. However, it would undoubtedly help to find ways to operationalise feasibility and make it comparable to other viability components. Our ranking-based method showed that it is possible to overcome this issue. However, a less opinionated solution would be more beneficial. 

% Furthermore, we would like to stress that our approach is only as good as the prediction model it attempts to explain. To gain further insights into \emph{true} process models, one must make sure that the prediction model accurately reflects the real world. Again, a domain expert might help to deduce which model is the best reflection of natural phenomena.

% % \subsection{}
% % \subfile{content/sections/sec_discussion_future}
% Regarding future directions, it is worth pointing out whether employing other components of the viability structure is beneficial. The measure described here clearly operationalised a set of criteria. However, there may be more aspects to consider and generate even better counterfactuals. A good example would be diversity. In terms of other evolutionary approaches, applying modern state-of-the-art methods with the same viability measure would be interesting.



\section{Conclusion}
\label{ch:conclusion}
% \subfile{content/sections/sec_conclusion}
In this paper, we proposed CREATED, a modular framework to generate viable counterfactuals.
The framework incorporates an evolutionary algorithm to generate counterfactual sequences while not requiring any domain knowledge other than the log itself. In addition, we proposed a viability measure to quantify and assess the quality of counterfactual sequences when compared to a factual sequence. The viability measure takes four aspects into account: feasibility, the delta in flipping the outcome prediction, similarity, and sparsity. The approach is capable of generating counterfactuals without explicit knowledge about the domain, as we only require the log. We achieve this by incorporating a Markov model trained on the event log. Our evaluation shows that our framework can generate counterfactual sequences which are higher than our naive baselines (i.e., case-based, sample-based, and random baselines). With these results, we demonstrate that optimizing a viability measure does generate higher-quality counterfactuals. We also compared the generated counterfactuals to the state-of-the-art method in the literature and show that our framework can generate similar counterfactuals, without using domain knowledge. 
The current feasibility measure tends to return lower values than other viability components as it is very sensitive to trace length. In the future, we aim to investigate better feasibility measures. 
% Furthermore, we would like to extend the framework and the viability measure to better assess both case- and event-attributes. 

% The small probabilities we saw are emblematic of the probabilistic sphere. However, it would undoubtedly help to find ways to operationalise feasibility and make it comparable to other viability components. Our ranking-based method showed that it is possible to overcome this issue. However, a less opinionated solution would be more beneficial. 

% Furthermore, we would like to stress that our approach is only as good as the prediction model it attempts to explain. To gain further insights into \emph{true} process models, one must make sure that the prediction model accurately reflects the real world. Again, a domain expert might help to deduce which model is the best reflection of natural phenomena.

% \subsection{}
% \subfile{content/sections/sec_discussion_future}
% Regarding future directions, it is worth pointing out whether employing other components of the viability structure is beneficial. The measure described here clearly operationalised a set of criteria. However, there may be more aspects to consider and generate even better counterfactuals. A good example would be diversity. In terms of other evolutionary approaches, applying modern state-of-the-art methods with the same viability measure would be interesting.


% \footnotetext[1]{How can we employ existing methods to compute viability so that its optimization incorporates information about the structure of the sequence?}
% \footnotetext[2]{To what extent can we generate counterfactuals that fulfill the criteria to be viable?}
% \footnotetext[3]{How does an algorithm which optimizes multiple viability quality metrics perform against other approaches?}


% \section{First Section}
% \subsection{A Subsection Sample}
% Please note that the first paragraph of a section or subsection is
% not indented. The first paragraph that follows a table, figure,
% equation etc. does not need an indent, either.

% Subsequent paragraphs, however, are indented.

% \subsubsection{Sample Heading (Third Level)} Only two levels of
% headings should be numbered. Lower level headings remain unnumbered;
% they are formatted as run-in headings.

% \paragraph{Sample Heading (Fourth Level)}
% The contribution should contain no more than four levels of
% headings. Table~\ref{tab1} gives a summary of all heading levels.

% \begin{table}
% \caption{Table captions should be placed above the
% tables.}\label{tab1}
% \begin{tabular}{|l|l|l|}
% \hline
% Heading level &  Example & Font size and style\\
% \hline
% Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
% 1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
% 2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
% 3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
% 4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
% \hline
% \end{tabular}
% \end{table}


% \noindent Displayed equations are centered and set on a separate
% line.
% \begin{equation}
% x + y = z
% \end{equation}
% % Please try to avoid rasterized images for line-art diagrams and schemas. Whenever possible, use vector graphics instead (see Fig.~\ref{fig1}).

% \begin{figure}
% % \includegraphics[width=\textwidth]{fig1.eps}
% % \caption{A figure caption is always placed below the illustration. Please note that short captions are centered, while long ones are justified by the macro package automatically.} \label{fig1} 
% \end{figure}

% \begin{theorem}
% This is a sample theorem. The run-in heading is set in bold, while
% the following text appears in italics. Definitions, lemmas,
% propositions, and corollaries are styled the same way.
% \end{theorem}
% %
% % the environments 'definition', 'lemma', 'proposition', 'corollary',
% % 'remark', and 'example' are defined in the LLNCS documentclass as well.
% %
% \begin{proof}
% Proofs, examples, and remarks have the initial word in italics,
% while the following text appears in normal font.
% \end{proof}
% For citations of references, we prefer the use of square brackets
% and consecutive numbers. Citations using labels or the author/year
% convention are also acceptable. The following bibliography provides
% a sample reference list with entries for journal
% articles~\cite{ref_article1}, an LNCS chapter~\cite{ref_lncs1}, a
% book~\cite{ref_book1}, proceedings without editors~\cite{ref_proc1},
% and a homepage~\cite{ref_url1}. Multiple citations are grouped
% \cite{ref_article1,ref_lncs1,ref_boo1},
% \cite{ref_article1,ref_book1,ref_proc1,ref_url1}.

% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \addbibresource{}
% \printglossary
\bibliographystyle{splncs04}
% \bibliographystyle{splncs04nat}
% \bibliography{./references/bibliography.bib}
\bibliography{./ref.bib}
%
% \begin{thebibliography}{8}
% \bibitem{ref_article1}
% Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)



% \bibitem{ref_lncs1}
% Author, F., Author, S.: Title of a proceedings paper. In: Editor,
% F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
% Springer, Heidelberg (2016). \doi{10.10007/1234567890}

% \bibitem{ref_book1}
% Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
% Location (1999)

% \bibitem{ref_proc1}
% Author, A.-B.: Contribution title. In: 9th International Proceedings
% on Proceedings, pp. 1--2. Publisher, Location (2010)

% \bibitem{ref_url1}
% LNCS Homepage, \url{http://www.springer.com/lncs}. Last accessed 4
% Oct 2017
% \end{thebibliography}

% XIXI Comments on Structure
% XIXI: After the architecture follow with the evolutionary framework and then viability function.
% XIXI: LSTM-Prediction model into the architecture section.

% XIXI
% Intro 3-4 pages
% Background 0.5 pages
% Approach 5 pages
% - Architecture 1 pages
% - Evolutionary 2 pages
% - Viability 2 page
% Evaluation 5 pages

% DISCUSSION
% First repeat parts of 6.1 and 6.2 again.
% Emphasize again that we do not need domain
% Link to the feasibility 
% Also talk about the casebased inititator - DONE
% 
% FINAL TODOS
% Add oxford commas
% Add the github link

\end{document}
