% % \subsection{Related Literature}
% \label{sec:literature}

% Many researchers have worked on counterfactuals and \Gls{PM}. 
% Here, we combine the important concepts and discuss the various contributions to this paper.
% \subfile{content/sections/sec_literature}

% \textbf{Generating Counterfactuals}
As stated before, We divide the existing methods for counterfactual generation into two categories: \emph{traditional} methods and \emph{process-aware} methods. The \emph{traditional} methods concern the classical ML models, and the topic of counterfactual generation as an explanation method was first introduced by \cite{wachter_CounterfactualExplanationsOpening_2017}. The authors defined a loss function that incorporates the criteria to generate a counterfactual that maximizes the likelihood of a predefined outcome and minimizes the distance to the original instance.
% However, the solution of~\cite{wachter_CounterfactualExplanationsOpening_2017} did not account for the minimisation of feature changes and does not penalize unrealistic features. Furthermore, their solution cannot incorporate categorical variables.
%
A more recent approach by~\cite{dandl_MultiObjectiveCounterfactualExplanations_2020} incorporates four main criteria for counterfactuals by applying a genetic algorithm with a multi-objective fitness function~\cite{dandl_MultiObjectiveCounterfactualExplanations_2020}. This approach strongly differs from gradient-based methods, as it does not require a differentiable objective function. However, the above traditional methods focus on static data. They do not take process behaviors into account. Applying these methods directly on event logs may result in generating infeasible counterfactual sequences. 

% XL: Counterfactual generation for NLP is not known in CAISE. Removed because mostly not know. 
% \subsubsection{Generating Counterfactual Sequences}
% When it comes to sequential data most researchers work on ways to generate counterfactuals for natural language. This often entails generating univariate discrete counterfactuals with the use of \gls{DL} techniques. \cite{martens_Explainingdatadrivendocument_2014} and later \cite{krause_InteractingPredictionsVisual_2016} are early examples of counterfactual NLP research\cite{martens_Explainingdatadrivendocument_2014,krause_InteractingPredictionsVisual_2016}. Their approach strongly focuses on the manipulation of sentences to achieve the desired outcome. However, as \cite{robeer_GeneratingRealisticNatural_2021} puts it, their counterfactuals do not comply with \emph{realisticness}\cite{robeer_GeneratingRealisticNatural_2021}.

% Instead, \cite{robeer_GeneratingRealisticNatural_2021} showed that it is possible to generate realistic counterfactuals with a \gls{GAN}\cite{robeer_GeneratingRealisticNatural_2021}. They use the model to implicitly capture a latent state space and sample counterfactuals from it. Apart from implicitly modelling the latent space with \glspl{GAN}, it is possible to sample data from an explicit latent space. Examples of these approaches often use an encoder-decoder pattern in which the encoder encodes a data instance into a latent vector, which will be peturbed and then decoded into a similar instance\cite{melnyk_ImprovedNeuralText_2017,wang_Controllableunsupervisedtext_2019}. By modelling the latent space, we can simply sample from a distribution conditioned on the original instance. \cite{bond-taylor_DeepGenerativeModelling_2021} provide an overview of the strengths and weaknesses of common generative models.

% Even though, a single latent vector model can theoretically produce multivariate sequences, it may still be too restrictive to capture the combinatorial space of multivariate sequences. Hence, most of the models within \gls{NLP} were not used to produce a sequence of vectors, but a sequence of discrete symbols. For process instances, we can assume a causal relation between state vectors in a sequential latent space. We call models that capture a sequential latent state-space, which has causal relations, \emph{dynamic}\cite{leglaive_RecurrentVariationalAutoencoder_2020}. Early models of this type of dynamic latent state-space models are the well-known \emph{Kalman-Filter} for continuous states and \gls{HMM} for discrete states. In recent literature, many techniques use \gls{DL} to model complex state-spaces. The first models of this type were developed by \cite{krishnan_StructuredInferenceNetworks_2017}\cite{krause_InteractingPredictionsVisual_2016, krishnan_StructuredInferenceNetworks_2017}. Their \gls{DKF} and subsequent \gls{DMM} approximate the dynamic latent state-space by modelling the latent space given the data sequence and all previous latent vectors in the sequence. There are many variations\cite{chung_RecurrentLatentVariable_2016,fraccaro_Sequentialneuralmodels_2016,leglaive_RecurrentVariationalAutoencoder_2020} of \cite{krishnan_StructuredInferenceNetworks_2017}'s model, but most use \gls{ELBO} of the posterior for the current $Z_{t}$ given all previous $\{Z_{t-1},\ldots,Z_{1}\}$ and $X_{t}$\cite{girin_DynamicalVariationalAutoencoders_2021}.

% \subsubsection{Generating Counterfactual Time-Series}
% Within the \emph{multivariate time-series} literature two recent approaches yield ideas worth discussing.

% First, \cite{delaney_InstanceBasedCounterfactualExplanations_2021} introduce a case-based reasoning to generate counterfactuals\cite{delaney_InstanceBasedCounterfactualExplanations_2021}. Their method uses existing counterfactual instances, or \emph{prototypes}, in the dataset. Therefore, it ensures, that the proposed counterfactuals are \emph{realistic}. However, case-based approaches strongly depend on the \emph{representativeness} of the prototypes\cite[p. 192]{molnar2019}. In other words, if the model displays behaviour, which is not captured within the set of prototypical instances, most case-based techniques will fail to provide viable counterfactuals. The likelihood of such a break-down increases due to the combinatorial explosion of possible behaviours if the \emph{true} process model has cycles or continuous event attributes. Cycles may cause infinite possible sequences and continuous attributes can take values on a domain within infinite negative and positive bounds. These issues have not been explored in the paper of \cite{delaney_InstanceBasedCounterfactualExplanations_2021}, as it mainly deals with time series classification\cite{delaney_InstanceBasedCounterfactualExplanations_2021}. However, despite these shortcomings, case-based approaches may act as a valuable baseline against other sophisticated approaches.

% The second paper within the multivariate time series field by \cite{ates_CounterfactualExplanationsMultivariate_2021} also uses a case-based approach\cite{ates_CounterfactualExplanationsMultivariate_2021}. However, it contrasts from other approaches, as it does not specify a particular model but proposes a general framework instead. Hence, within this framework, individual components could be substituted by better performing components. Describing a framework, rather than specifying a particular model, allows to adapt the framework, due to the heterogeneous process dataset landscape. In this paper, we also introduce a framework that allows for flexibility depending on the dataset. 
% % \optional{The framework will be evaluated in two steps. The first step aims to compare various model types against eachother based on the countefactual viability. The second step scrutinizes the best framework configurations from step one, by presenting its results to a domain expert.}

% \noindent\textbf{Generating Counterfactuals for Business Processes:}
% So far, none of the techniques have been applied to process data.
Within process mining, the \emph{process-aware} methods for counterfactuals have followed two streams. The first steam uses the \gls{causalinference} techniques to analyse and model business processes, as the causal relationships can be used to understand the effect of decisions in a process on its outcome. However, early work has often attempted to incorporate domain-knowledge about the causality of processes in order to improve the process model itself~\cite{baker_ClosingLoopEmpirical_2017,hompes_DiscoveringCausalFactors_2017,shook_assessmentusestructural_2004,wang_CounterfactualDataAugmentedSequential_2021}.
Among these, the approach in~\cite{narendra_CounterfactualReasoningProcess_2019} is one of the first to include counterfactual reasoning for process optimization~\cite{narendra_CounterfactualReasoningProcess_2019}.
Later, the work by~\cite{oberst_CounterfactualOffPolicyEvaluation_2019} uses counterfactuals to generate alternative solutions to treatments, which lead to a desired outcome.
However, the authors do not attempt to provide an explanation of the model's outcome and therefore, disregard multiple viability criteria for counterfactuals in \gls{XAI}. 
\cite{qafari_CaseLevelCounterfactual_2021} published the most recent paper on the counterfactual generation of explanations. The authors use a known \gls{SCM} to guide the generation of their counterfactuals. However, this approach requires a process model which is as close as possible to the \emph{true} process model. Our approach assumes no knowledge of such a normative process model. 



% Within the \gls{XAI} context, \cite{tsirtsis_CounterfactualExplanationsSequential_2021} developed the first explanation method for process data. However, their work closely resembles the work of \cite{oberst_CounterfactualOffPolicyEvaluation_2019} and treat the task as \gls{MDP}\cite{oberst_CounterfactualOffPolicyEvaluation_2019}. This extension of a regular \gls{MP} assumes that an actor influences the outcome of a process given the state. This formalisation allows the use of \gls{RL} methods like Q-learning or SARSA. However, this often requires additional assumptions such as a given reward function and an action-space. For counterfactual sequence generation, there is no obvious choice for the reward function or the action-space. 
% Nonetheless, both \cite{tsirtsis_CounterfactualExplanationsSequential_2021} and \cite{oberst_CounterfactualOffPolicyEvaluation_2019} contribute an important idea - the idea of incrementally generating the counterfactual instead of the full sequence. 

The second stream in \emph{process-aware} methods adapts the \emph{traditional} counterfactual methods for process-aware counterfactuals. The DICE4EL approach~\cite{hsieh_DiCE4ELInterpretingProcess_2021} extends the DICE method~\cite{mothilal_ExplainingMachineLearning_2020} to generate counterfactuals for event logs while building on the same notion of incremental generation. 
%
% Their approach has a very similar structure to our approach and appears to be the only one that we can compare our counterfactuals against. 
% For this reason, this paper highlights some key differences and similarities. However, to understand the differences and similarities, we first have to establish some core concepts.  In this section, we only discuss their approach, briefly.
%
The authors recognised that some processes have critical events (mile-stones) which govern the overall outcome. Hence, by simply avoiding the undesired outcome from critical event to critical event, it is possible to limit the search space and compute viable counterfactuals. However, their approach requires concrete domain knowledge about these critical points. We propose a framework that avoids this constraint and does not require domain knowledge. The LORELEY approach \cite{DBLP:conf/emcis/HuangMP21} extends the LORE method~\cite{DBLP:journals/expert/GuidottiMGPRT19} and also uses an evolutionary algorithm. However, this approach focuses on mutating the case/event attributes. More specifically, the approach treats the encoded features representing the control flow as a single attribute in the crossover and mutation steps; thus, no unseen counterfactual sequences are created. In contrast, we generate unseen process sequences. Furthermore, we propose to automatically train a Markov model from the input event log to capture the likelihood of a process sequence. This Markov model is then used to derive the feasibility of counterfactual sequences. 
% To our knowledge, the authors are also the first authors that try to optimize their counterfactual process generation based on criteria that ensure their viability. However, in our approach, we use different operationalisations to quantify the criteria.