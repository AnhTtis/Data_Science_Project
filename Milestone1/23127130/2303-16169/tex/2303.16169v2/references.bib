@article{Singer2006,
title = {From graph to manifold {L}aplacian: The convergence rate},
journal = {Applied and Computational Harmonic Analysis},
volume = {21},
number = {1},
pages = {128-134},
year = {2006},
note = {Special Issue: Diffusion Maps and Wavelets},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2006.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S1063520306000510},
author = {A. Singer},
abstract = {The convergence of the discrete graph Laplacian to the continuous manifold Laplacian in the limit of sample size N→∞ while the kernel bandwidth ε→0, is the justification for the success of Laplacian based algorithms in machine learning, such as dimensionality reduction, semi-supervised learning and spectral clustering. In this paper we improve the convergence rate of the variance term recently obtained by Hein et al. [From graphs to manifolds—Weak and strong pointwise consistency of graph Laplacians, in: P. Auer, R. Meir (Eds.), Proc. 18th Conf. Learning Theory (COLT), Lecture Notes Comput. Sci., vol. 3559, Springer-Verlag, Berlin, 2005, pp. 470–485], improve the bias term error, and find an optimal criteria to determine the parameter ε given N.}
}

@article{SteerablePaper,
author = {Landa, Boris and Shkolnisky, Yoel},
title = {The Steerable Graph {L}aplacian and its Application to Filtering Image Datasets},
journal = {SIAM Journal on Imaging Sciences},
volume = {11},
number = {4},
pages = {2254-2304},
year = {2018},
doi = {10.1137/18M1169394},

URL = { 
    
        https://doi.org/10.1137/18M1169394
    
    

},
eprint = { 
    
        https://doi.org/10.1137/18M1169394
    
    

}}

@article{10.5555/3455716.3455961,
author = {Chen, Shuxiao and Dobriban, Edgar and Lee, Jane H.},
title = {A Group-Theoretic Framework for Data Augmentation},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Data augmentation is a widely used trick when training deep neural networks: in addition to the original data, properly transformed data are also added to the training set. However, to the best of our knowledge, a clear mathematical framework to explain the performance benefits of data augmentation is not available. In this paper, we develop such a theoretical framework. We show data augmentation is equivalent to an averaging operation over the orbits of a certain group that keeps the data distribution approximately invariant. We prove that it leads to variance reduction. We study empirical risk minimization, and the examples of exponential families, linear regression, and certain two-layer neural networks. We also discuss how data augmentation could be used in problems with symmetry where other approaches are prevalent, such as in cryo-electron microscopy (cryo-EM).},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {245},
numpages = {71},
keywords = {data augmentation, empirical risk minimization, deep learning, variance reduction, invariance}
}

@ARTICLE{6789755,
  author={Belkin, Mikhail and Niyogi, Partha},
  journal={Neural Computation}, 
  title={Laplacian Eigenmaps for Dimensionality Reduction and Data Representation}, 
  year={2003},
  volume={15},
  number={6},
  pages={1373-1396},
  doi={10.1162/089976603321780317}}

  @article{
doi:10.1126/science.290.5500.2319,
author = {Joshua B. Tenenbaum  and Vin de Silva  and John C. Langford },
title = {A Global Geometric Framework for Nonlinear Dimensionality Reduction},
journal = {Science},
volume = {290},
number = {5500},
pages = {2319-2323},
year = {2000},
doi = {10.1126/science.290.5500.2319},
URL = {https://www.science.org/doi/abs/10.1126/science.290.5500.2319},
eprint = {https://www.science.org/doi/pdf/10.1126/science.290.5500.2319},
abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs—30,000 auditory nerve fibers or 106 optic nerve fibers—a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.}}

@article{BELKIN20081289,
title = {Towards a theoretical foundation for Laplacian-based manifold methods},
journal = {Journal of Computer and System Sciences},
volume = {74},
number = {8},
pages = {1289-1308},
year = {2008},
note = {Learning Theory 2005},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2007.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0022000007001274},
author = {Mikhail Belkin and Partha Niyogi},
keywords = {Laplace–Beltrami operator, Graph Laplacian, Manifold methods},
abstract = {In recent years manifold methods have attracted a considerable amount of attention in machine learning. However most algorithms in that class may be termed “manifold-motivated” as they lack any explicit theoretical guarantees. In this paper we take a step towards closing the gap between theory and practice for a class of Laplacian-based manifold methods. These methods utilize the graph Laplacian associated to a data set for a variety of applications in semi-supervised learning, clustering, data representation. We show that under certain conditions the graph Laplacian of a point cloud of data samples converges to the Laplace–Beltrami operator on the underlying manifold. Theorem 3.1 contains the first result showing convergence of a random graph Laplacian to the manifold Laplacian in the context of machine learning.}
}

@inproceedings{NIPS2006_5848ad95,
 author = {Belkin, Mikhail and Niyogi, Partha},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
 pages = {},
 publisher = {MIT Press},
 title = {Convergence of {L}aplacian Eigenmaps},
 url = {https://proceedings.neurips.cc/paper_files/paper/2006/file/5848ad959570f87753a60ce8be1567f3-Paper.pdf},
 volume = {19},
 year = {2006}
}

@article{COIFMAN20065,
title = {Diffusion maps},
journal = {Applied and Computational Harmonic Analysis},
volume = {21},
number = {1},
pages = {5-30},
year = {2006},
note = {Special Issue: Diffusion Maps and Wavelets},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2006.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S1063520306000546},
author = {Ronald R. Coifman and Stéphane Lafon},
keywords = {Diffusion processes, Diffusion metric, Manifold learning, Dimensionality reduction, Eigenmaps, Graph Laplacian},
abstract = {In this paper, we provide a framework based upon diffusion processes for finding meaningful geometric descriptions of data sets. We show that eigenfunctions of Markov matrices can be used to construct coordinates called diffusion maps that generate efficient representations of complex geometric structures. The associated family of diffusion distances, obtained by iterating the Markov matrix, defines multiscale geometries that prove to be useful in the context of data parametrization and dimensionality reduction. The proposed framework relates the spectral properties of Markov processes to their geometric counterparts and it unifies ideas arising in a variety of contexts such as machine learning, spectral graph theory and eigenmap methods.}
}

@article{calder2022improved,
  title={Improved spectral convergence rates for graph {L}aplacians on $\varepsilon$-graphs and $k$-{NN} graphs},
  author={Calder, Jeff and Trillos, Nicolas Garcia},
  journal={Applied and Computational Harmonic Analysis},
  volume={60},
  pages={123--175},
  year={2022},
  publisher={Elsevier}
}



@article{wan2016cryo,
  title={Cryo-electron tomography and subtomogram averaging},
  author={Wan, W and Briggs, John AG},
  journal={Methods in Enzymology},
  volume={579},
  pages={329--367},
  year={2016},
  publisher={Elsevier}
}

@article{50df98c2c7df47b0b5d2892f902968a0,
title = "Manifold Learning with Arbitrary Norms",
abstract = "Manifold learning methods play a prominent role in nonlinear dimensionality reduction and other tasks involving high-dimensional data sets with low intrinsic dimensionality. Many of these methods are graph-based: they associate a vertex with each data point and a weighted edge with each pair. Existing theory shows that the Laplacian matrix of the graph converges to the Laplace–Beltrami operator of the data manifold, under the assumption that the pairwise affinities are based on the Euclidean norm. In this paper, we determine the limiting differential operator for graph Laplacians constructed using any norm. Our proof involves an interplay between the second fundamental form of the manifold and the convex geometry of the given norm{\textquoteright}s unit ball. To demonstrate the potential benefits of non-Euclidean norms in manifold learning, we consider the task of mapping the motion of large molecules with continuous variability. In a numerical simulation we show that a modified Laplacian eigenmaps algorithm, based on the Earthmover{\textquoteright}s distance, outperforms the classic Euclidean Laplacian eigenmaps, both in terms of computational cost and the sample size needed to recover the intrinsic geometry.",
keywords = "Convex body, Diffusion maps, Dimensionality reduction, Laplacian eigenmaps, Riemannian geometry, Second-order differential operator",
author = "Joe Kileel and Amit Moscovich and Nathan Zelesko and Amit Singer",
year = "2021",
month = oct,
doi = "10.1007/s00041-021-09879-2",
language = "English (US)",
volume = "27",
journal = "Journal of Fourier Analysis and Applications",
issn = "1069-5869",
publisher = "Birkhause Boston",
number = "5",
}


@InProceedings{pmlr-v5-goldberg09a,
  title = 	 {Multi-Manifold Semi-Supervised Learning},
  author = 	 {Goldberg, Andrew and Zhu, Xiaojin and Singh, Aarti and Xu, Zhiting and Nowak, Robert},
  booktitle = 	 {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {169--176},
  year = 	 {2009},
  editor = 	 {van Dyk, David and Welling, Max},
  volume = 	 {5},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v5/goldberg09a/goldberg09a.pdf},
  url = 	 {https://proceedings.mlr.press/v5/goldberg09a.html},
  abstract = 	 {We study semi-supervised learning when the data consists of multiple intersecting manifolds.  We give a finite sample analysis to quantify the potential gain of using unlabeled data in this multi-manifold setting.  We then propose a semi-supervised learning algorithm that separates different manifolds into decision sets, and performs supervised learning within each set.  Our algorithm involves a novel application of Hellinger distance and size-constrained spectral clustering.  Experiments demonstrate the benefit of our multi-manifold semi-supervised learning approach.}
}


@InProceedings{pmlr-v54-moscovich17a,
  title = 	 {{Minimax-optimal semi-supervised regression on unknown manifolds}},
  author = 	 {Moscovich, Amit and Jaffe, Ariel and Boaz, Nadler},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {933--942},
  year = 	 {2017},
  editor = 	 {Singh, Aarti and Zhu, Jerry},
  volume = 	 {54},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v54/moscovich17a/moscovich17a.pdf},
  url = 	 {https://proceedings.mlr.press/v54/moscovich17a.html},
  abstract = 	 {We consider semi-supervised regression when the predictor variables are drawn from an unknown manifold. A simple two step approach to this problem is to: (i) estimate the manifold geodesic distance between any pair of points using both the labeled and unlabeled instances; and (ii) apply a k nearest neighbor regressor based on these distance estimates. We prove that given sufficiently many unlabeled points, this simple method of geodesic kNN regression achieves the optimal finite-sample minimax bound on the mean squared error, as if the manifold were known. Furthermore, we show how this approach can be efficiently implemented, requiring only O(k N log N) operations to estimate the regression function at all N labeled and unlabeled points. We illustrate this approach on two datasets with a manifold structure: indoor localization using WiFi fingerprints and facial pose estimation. In both cases, geodesic kNN is more accurate and much faster than the popular Laplacian eigenvector regressor.}
}

@book{lee2012smooth,
  title={Smooth Manifolds},
  author={Lee, John M},
  year={2012},
  publisher={Springer}
}

@article{doi:10.1080/01621459.2013.827984,
author = { Mingyen   Cheng  and  Hautieng   Wu },
title = {Local Linear Regression on Manifolds and Its Geometric Interpretation},
journal = {Journal of the American Statistical Association},
volume = {108},
number = {504},
pages = {1421-1434},
year  = {2013},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2013.827984},

URL = { 
    
        https://doi.org/10.1080/01621459.2013.827984
    
    

},
eprint = { 
    
        https://doi.org/10.1080/01621459.2013.827984
    
    

}

}


@book{sternberg1995group,
  title={Group Theory and Physics},
  author={Sternberg, Shlomo},
  year={1995},
  publisher={Cambridge University Press}
}



@book{chirikjian2016harmonic,
  title={Harmonic Analysis for Engineers and Applied Scientists: Updated and Expanded Edition},
  author={Chirikjian, Gregory S and Kyatkin, Alexander B},
  year={2016},
  publisher={Courier Dover Publications}
}

@article{SOBER2021113140,
title = {Approximation of functions over manifolds: A Moving Least-Squares approach},
journal = {Journal of Computational and Applied Mathematics},
volume = {383},
pages = {113140},
year = {2021},
issn = {0377-0427},
doi = {https://doi.org/10.1016/j.cam.2020.113140},
url = {https://www.sciencedirect.com/science/article/pii/S0377042720304313},
author = {Barak Sober and Yariv Aizenbud and David Levin},
keywords = {Manifold learning, Regression over manifolds, Moving Least-Squares, Dimension reduction, High dimensional approximation, Out-of-sample extension},
abstract = {We present an algorithm for approximating a function defined over a d-dimensional manifold utilizing only noisy function values at locations sampled from the manifold with noise. To produce the approximation we do not require knowledge about the local geometry of the manifold or its local parameterizations. We do require, however, knowledge regarding the manifold’s intrinsic dimension d. We use the Manifold Moving Least-Squares approach of Sober and Levin (2019) to reconstruct the atlas of charts and the approximation is built on top of those charts. The resulting approximant is shown to be a function defined over a neighborhood of a manifold, approximating the originally sampled manifold. In other words, given a new point, located near the manifold, the approximation can be evaluated directly on that point. We prove that our construction yields a smooth function, and in case of noiseless samples the approximation order is O(hm+1), where h is a local density of sample parameter (i.e., the fill distance) and m is the degree of a local polynomial approximation, used in our algorithm. In addition, the proposed algorithm has linear time complexity with respect to the ambient space’s dimension. Thus, we are able to avoid the computational complexity, commonly encountered in high dimensional approximations, without having to perform non-linear dimension reduction, which inevitably introduces distortions to the geometry of the data. Additionally, we show numerically that our approach compares favorably to some well-known approaches for regression over manifolds.}
}

@article{chen2020group,
  title={A group-theoretic framework for data augmentation},
  author={Chen, Shuxiao and Dobriban, Edgar and Lee, Jane H},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={9885--9955},
  year={2020},
  publisher={JMLRORG}
}

@article{singer2020computational,
  title={Computational methods for single-particle electron cryomicroscopy},
  author={Singer, Amit and Sigworth, Fred J},
  journal={Annual Review of Biomedical Data Science},
  volume={3},
  pages={163--190},
  year={2020},
  publisher={Annual Reviews}
}

@article{liu2022high,
  title={High-resolution structure determination using high-throughput electron cryo-tomography},
  author={Liu, H-F and Zhou, Ye and Bartesaghi, Alberto},
  journal={Acta Crystallographica Section D: Structural Biology},
  volume={78},
  number={7},
  year={2022},
  publisher={International Union of Crystallography}
}

@inproceedings{zelesko2020earthmover,
  title={Earthmover-based manifold learning for analyzing molecular conformation spaces},
  author={Zelesko, Nathan and Moscovich, Amit and Kileel, Joe and Singer, Amit},
  booktitle={2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)},
  pages={1715--1719},
  year={2020},
  organization={IEEE}
}

@book{kondor2008group,
  title={Group Theoretical Methods in Machine Learning},
  author={Kondor, Imre Risi},
  year={2008},
  publisher={Columbia University}
}


@article{rosen2023,
  title={The {$G$}-invariant graph {L}aplacian},
  author={Rosen, Eitan and Cheng, Xiuyuan and Shkolnisky, Yoel},
  journal={arXiv preprint arXiv:2303.17001},
  year={2023}
}