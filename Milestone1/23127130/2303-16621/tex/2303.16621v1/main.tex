\documentclass[11pt]{article}
\usepackage{times}
\usepackage{arabtex}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{arabtex}
\usepackage{utf8}

%\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfigure} % make it possible to include more than one captioned figure/table in a single float

% Algorithms
%\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{algpseudocode}	

\newcommand*\Let[2]{\State #1 $\gets$ #2}
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%
\providecommand{\keywords}[1]{\textbf{\textit{Index Terms---}} #1}

%% END Article customise

\usepackage{graphicx}  % Add graphics capabilities
\usepackage{flafter}  % Don't place floats before their definition
\usepackage{bm}  % Define \bm{} to use bold math fonts
\usepackage[pdftex,bookmarks,colorlinks,breaklinks]{hyperref}  % PDF hyperlinks, with coloured links

\hypersetup{linkcolor=red,citecolor=blue,filecolor=dullmagenta,urlcolor=blue} % coloured links
\usepackage{memhfixc}  % remove conflict between the memoir class & hyperref
\usepackage{pdfsync}  % enable tex source and pdf output syncronicity

\usepackage{array}
\newcolumntype{L}{>{\centering \arraybackslash}m{2cm}} 

\begin{document}
\vocalize
\setarab

\begin{center}
{\Large {\bf AraSpot: Arabic Spoken Command Spotting}}\\[0pt]
{\small {A preprint}}\\[0pt]
\vspace*{0.5in} {Mahmoud Salhab and Haidar Harmanani\\[0pt]
\vspace*{0.1in} Department of Computer Science and Mathematics\\[0pt]
Lebanese American University\\[0pt]
Byblos 1401 2010, Lebanon\\[0pt]
}
\end{center}



\subsection*{Abstract}
Spoken keyword spotting (KWS) is the task of identifying a keyword in an audio stream and is widely used in smart devices at the edge in order to activate voice assistants and perform hands-free tasks.  The task is daunting as there is a need, on the one hand, to achieve high accuracy while at the same time ensuring that such systems continue to run efficiently on low power and possibly limited computational capabilities devices.  This work presents AraSpot for Arabic keyword spotting trained on 40 Arabic keywords, using different online data augmentation, and introducing ConformerGRU model architecture.  Finally, we further improve the performance of the model by training a text-to-speech model for synthetic data generation. AraSpot achieved a State-of-the-Art SOTA 99.59\% result outperforming previous approaches.\footnote{Available on GitHub at \url{https://github.com/msalhab96/AraSpot}}

\keywords{Arabic Command Spotting, Speech Recognition, Conformer, synthetic data generation.}


\section{Introduction}

Automatic Speech Recognition (ASR) is a fast-growing technology that has been attracting increased interest due to its embedment in a myriad of devices.  ASR allows users to activate voice assistants and perform hands-free tasks by detecting a stream of input speech and converting it into its corresponding text. Spoken Keyword Spotting (KWS) is similar to the ASR problem but it is mostly concerned with the identification of predefined keywords in continuous speech \cite{6854211}. In fact, keyword spotting systems are common components in speech-enabled devices \cite{8268946} and have a wide range of applications such as speech data mining, audio indexing, phone call routing, and many other \cite{inproceedings}.

Recently, many models became popular for tackling KWS systems including \textit{Convolution Neural Networks} (CNN), \textit{Residual Networks} (ResNet), and \textit{Recurrent Neural Networks} (RNN).  The disadvantage of CNN is that they do not work well with sequences.  Furthermore, CNNs are not usually able to capture long term-dependencies in human speech signal information, the same thing for ResNets as they are short-sighted when it comes to their respective field.  Recurrent neural network directly models the input features without learning local structure between successive time series and frequency steps \cite{8607038}.

The Google Speech Command (GSC) datasets \cite{gsc} is the de facto KWS standard for English. Unfortunately, KWS has considerably lesser publicly available data than ASR, Consequently, training a neural network becomes harder given the scarcity of the data available \cite{9427206}. To overcome data scarcity for KWS, many researchers are using pre-trained models and synthesized data such as in \cite{https://doi.org/10.48550/arxiv.2002.01322}.

Most KWS research has focused on English and Asian languages with few research investigating KWS in Arabic, despite the fact that Arabic is the $4\textsuperscript{th}$ mostly used language on the internet \cite{article, 6841973}.  This work presents AraSpot for Arabic command spotting, our work is built on the ASC data set \cite{asc}, where we present different online data augmentation to model various environmental conditions as well as enrich and enlarge the data, and we introduce the ConformerGRU model architecture to tackle the short and long dependencies issues in both RNN and CNN, and we show empirically that our approach outperforms all previous approaches on the data set, we further improve the model performance by extending the training data by more speakers by using synthetic data generation.

This paper is organized as follows. Section \ref{sec:related} presents a literature review, followed by our methodology in section \ref{sec:methodology}. Section \ref{sec:results} presents the experiments and the results, and lastly, we conclude in section \ref{sec:conc} with a summary of potential future work.

\section{Related Work}
\label{sec:related}
Keyword spotting received a considerable amount of interest from the research community.  One of the earliest approaches is based on the use of large-vocabulary continuous speech (LVCSR) recognition.  In such systems, the speech signal is first decoded and then searched for the keyword/filler in the generated lattices \cite{319341, 6707766, 10.21437}.  An alternative to LVCSR is the keyword hidden Markov model (HMM) where a keyword HMM and a filler HMM are trained to model keyword and non-keyword audio segments \cite{115555, 266505}.

With the rise of GPU computational power and the increase in data availability, the research community switched gears towards deep learning-based KWS systems.  For example, Coucke et al. \cite{https://doi.org/10.48550/arxiv.1811.07684} used dilated convolutions of the WaveNet architecture and showed that the results were more robust in the presence of noise than LSTM or CNN based models. Arik et al. \cite{https://doi.org/10.48550/arxiv.1703.05390} proposed a single-layer CNN and two-layer RNNs proposed, similarly to the two gated CNN with on-layer bi-directional LSTM in \cite{GCNNLSTM}. An attention-based end-to-end model introduced for small-footprint KWS proposed in \cite{https://doi.org/10.48550/arxiv.1803.10916}.

Sun et al. \cite{Sun2017} used transfer learning  by  training an ASR system.  The acoustic model of the ASR model was fine-tuned on the KWS task.  Similarly, Raju et al. \cite{https://doi.org/10.48550/arxiv.1808.00563} proposed a multi-target setup in order to train a keyword spotting system.

Lin et al. \cite{https://doi.org/10.48550/arxiv.2002.01322} showed that building a state-of-the-art (SOTA) KWS model requires more than 4000 utterances per command.  The authors also noted that with the various limitations and difficulties in acquiring more data, methods to enlarge and expand the training data are required.  The above problem was alleviated in \cite{https://doi.org/10.48550/arxiv.1909.11699, https://doi.org/10.48550/arxiv.1811.00707} by using synthesized speech as a data augmentation approach.  The method utilized a text-to-speech system in order to generate synthetic speech.

Finally, Ghandoura et al. \cite{asc} recorded and published a benchmark that includes 40 commands that were recorded by 30 different speakers.  The authors achieved 97.97\% accuracy using a deep CNN model.  Benamer et al. \cite{asc2} published another benchmark that included 16 commands but used an LSTM model instead. A Keyword spotting to perform audio searching of uttered words in Arabic speech presented in \cite{arabicaudsearch}.


\section{Solution Approach}

\label{sec:methodology}
\subsection{Dataset}

In this work, we use the Arabic Speech Commands (ASC) dataset that was reported by \cite{asc}.  Some of the commands in this ASC dataset were inspired by the Google Speech Commands (GSC) \cite{gsc} dataset, while the rest were chosen so that they can be grouped into broad and possibly intersecting categories.  The commands have been chosen to activate voice assistants and perform hands-free task for some applications and devices such as a simple photo browser or a keypad \cite{asc}.

The ASC dataset consists of $1,2000$ command-speech pairs that include 40 keywords.
The dataset was recorded by 30 participants with each participant recording each keyword 10 times.  This resulted with 300 utterances per keyword. Table \ref{commands} lists the 40 commands in the data set and their Arabic translation.


\begin{table}[!h]
\setcode{utf8}
  \begin{center}
      \centering
      \begin{tabular}{c|c}
      \end{tabular}
    \begin{tabular}{c|c||c|c}
      \hline
      Translation & Keyword & Translation & Keyword \\ \hline \hline
      Zero &  \RL{صفر} & Enable  & \RL{تفعيل}\\ \hline
      One & \RL{واحد}  & Disable  & \RL{تعطيل}\\ \hline
      Two & \RL{اثنان} & Ok  & \RL{موافق}\\ \hline
      Three & \RL{ثلاثة}  & Cancel  & \RL{إلغاء}\\ \hline
      Four & \RL{أربعة}  & Open  & \RL{فتح}\\ \hline
      Five & \RL{خمسة}  & Close  & \RL{إغلاق}\\ \hline
      Six & \RL{ستة} & Zoom in  & \RL{تكبير}\\ \hline
      Seven & \RL{سبعة} & Zoom Out  & \RL{تصغير}\\ \hline
      Eight & \RL{ثمانية} & Previous  & \RL{السابق}\\ \hline
      Nine & \RL{تسعة} & Next  & \RL{التالي}\\ \hline
      Right & \RL{يمين} & Send  & \RL{إرسال}\\ \hline
      Left & \RL{يسار} & Receive  & \RL{استقبال}\\ \hline
      Up & \RL{أعلى} & Move  & \RL{تحريك}\\ \hline
      Down & \RL{أسفل} & Rotate  & \RL{تدوير}\\ \hline
      Front & \RL{أمام} & Record  & \RL{تسجيل}\\ \hline
      Back & \RL{خلف} & Enter  & \RL{إدخال}\\ \hline
      Yes & \RL{نعم} & Digit  & \RL{رقم}\\ \hline
      No & \RL{لا} & Direction  & \RL{اتجاه}\\ \hline
      Start & \RL{ابدأ} & Options  & \RL{خيارات}\\ \hline
      Stop & \RL{توقف} & Undo  & \RL{تراجع}\\ \hline
    \end{tabular}
    \caption{\label{commands} The 40 commands used in the ASC dataset}
    
  \end{center}
  \end{table}



\subsection{Data Augmentation}

The core idea of data augmentation is to generate additional synthetic data and to improve its diversity in order to achieve higher validation accuracy during training.  The augmented data is typically viewed as belonging to a distribution that is close to the original one \cite{dataaug}, while the resulting augmented examples can be still semantically described by the labels of the original input examples which is known as label-preserving transformation.  Augmented data is normally generated on the fly during the training process in what is known as \textit{online augmentation}.  Another alternative is  \textit{offline augmentation} \cite{dataaug1} which transforms the data beforehand and stores it in memory.


For this work, we apply on-the-fly data augmentation, in both the time domain as well as the frequency domain. Let $F_t$ and $F_f$ a set of pre-defined time domain and frequency domain transformation/augmentation functions such that $F_t=\{f_1, f_2, \ldots,f_Q\}$, and $F_f=\{f_1, f_2, \ldots,f_V\}$, for a given $x_i$ we first apply the chosen time-domain augmentation $\tilde{F_{t}^{i}}$ for the $i^{th}$ signal, then after transforming the augmented signal into the frequency domain, we apply the chosen frequency augmentation $\tilde{F_{f}^{i}}$:

\begin{equation}
    \tilde{F_{t}^{i}} = \{f_q: r_{q}^{i} \geq \lambda, 1 \leq q \leq Q\}
    \label{timeaug}
\end{equation}

\begin{equation}
    \tilde{F_{f}^{i}} = \{f_v: r_{v}^{i} \geq \gamma, 1 \leq v \leq V\}
    \label{freqaug}
\end{equation}

Where $r_{v}^{i}$, and $r_{q}^{i}$ are uniformly sampled values from $[0, 1]$ at each training step for each augmentation operation, $\tilde{F}_{t}^i$ and $\tilde{F}_{f}^i$ are the time domain and frequency domain functions with operation order shuffling per domain applied on the $i^{th}$ input signal at a given training step.  Finally, $\lambda$ and $\gamma$ are the time domain and frequency domain augmentation rates, where we can ensure that for any signal we can have any possible augmentation combination with different orders from one epoch to another.

For a given speech signal $X$ in the time domain, the below time domain augmentation methods are used as items for $F_t$:

\begin{enumerate}
  \item \textit{Urban Background Noise Injection}:  we used the test set of the \textit{Freesound data} published in \cite{inproceedings}. We first concatenated all existing $K$ noise audios into a single noise signal $\mathcal{N}$, and apply the the augmentation process as follows:

\begin{equation}
    m \sim unif(0, T_{n})
  \label{bgnoise1}
\end{equation}

\begin{equation}
n \sim unif(m, min(T_{n}, m + T_{s}))
  \label{bgnoise2}
\end{equation}

\begin{equation}
f \sim unif(0, T_s - n + m - 1)
  \label{bgnoise2}
\end{equation}


\begin{equation}
\xi = [0]_{f} \parallel (\mathcal{N}_{i})_{m \leq i < n}  \parallel [0]_{T_s - f - n + m }
  \label{bgnoise3}
\end{equation}

\begin{equation}
\acute{X}=\mathcal{G} \xi + X
  \label{bgnoise3}
\end{equation}

where $T_{s} = \mid X\mid$, $T_{n} = \mid \mathcal{N}\mid$, $n$, and $m$ are the start and the end of the noise segment in  $\mathcal{N}$, $f$ is the space of freedom that ensures that the start of addition would change for the same audio from one step to another, $\xi$ is the noise segment, $\parallel$ is the concatenation operation where the selected noise chunk get concatenated by a leading and trailing zeros to it of size $f$, and $T\textsubscript{s} - f - n + m$, $\acute{X}$ is the augmented version of $X$, and lastly, $\mathcal{G}$ is a random gain between 0 and 1.


\item \textit{Speech Reverberation}: speech reverberation originally is caused by the environment surrounded by the source, where the end result received by the input device (i.e Microphone) is the sum of multiple shifted and attenuated signals of the same original signal.  This can be easily simulated by convolving the original input speech signal with a room impulse response (RIR). For this case, we used both \textit{RIR} datasets created and published in \cite{rir1} and \cite{rir2}.

Let $H=\{h_1, h_2,\ldots,h_R\}$ a set of all available impulse responses, where each one of 1-second length.  For a given speech signal $X$, the augmentation process is done as below: 

\begin{equation}
h \sim unif(H)
  \label{reverberation1}
\end{equation}

\begin{equation}
l \sim unif(a, b)
  \label{reverberation2}
\end{equation}

\begin{equation}
\acute{X}= X \ast (h_i)_{0 \leq i \leq l}
  \label{reverberation3}
\end{equation}


\begin{equation}
\acute{X}[n]= \sum_{i=0}^{l} h[i]X[n - i]
  \label{reverberation4}
\end{equation}

where $l$ is the speech reverberation length, $\ast$ symbol in \ref{reverberation3} is the convolution operation, and $a$ and $b$ are the minimum and maximum reverberation length, we set $a$ to 31 ms, and $b$ to 250 ms.

\item \textit{Random Volume Gain:} for a given signal $X$, the magnitude of the signal is multiplied by a random gain $\mathcal{G}$ as shown below:


\begin{equation}
    \acute{X} = \mathcal{G} X
\end{equation}

where $\mathcal{G}$ is a random value between 0.2 and 2.

\item \textit{Random Fade In/Out:} Given a speech signal $X$ we multiply the magnitudes of the signal by a fade signal such as linear, exponential, logarithmic, quarter-sine, and half-sine.  The fade function is sampled uniformly from the previously mentioned signals, and then multiplied by the original signal.  The length of the fade signal is chosen randomly between 0 and $\mid X \mid$ and padded with ones to match the length of the original waveform $X$, and that can be formally shown as below:

\begin{equation}
    \acute{X} = F_{in} F_{out} X
\end{equation}

Where $F_{in}$ is the fade-in signal, $F_{out}$ is the fade-out signal.

\end{enumerate}

For a given signal $X$ in the frequency domain, spectrogram-based augmentation can be applied as proposed in \cite{SpecAugment}, for this work, we mainly used the time and frequency masking as items for $F_f$.


\subsection{Synthetic Data Generation Using TTS}
\label{synthaticdata}
End-to-end (E2E) Text-to-Speech (TTS) systems are used to generate speech directly from a given text, unlike traditional TTS systems that use complex pipelines. Seq2Seq-based TTS systems such as \cite{tts0, tts1, tts2, tts3} are commonly composed of an encoder, decoder, and an attention mechanism, such that, the characters embedding are projected into Mel-scale spectrogram followed by a vocoder that converts the predicted Mel-scale spectrogram into a waveform. 

In this work we use Tacotron 2 \cite{tts3} which has a relatively simple architecture.  The model consists of an \textit{encoder} and a \textit{decoder} with \textit{attention}.  The encoder takes the input characters/phonemes sequence $C$ and projects it into a high-level representation $h$, then the decoder with attention generates Mel-scale spectrogram frames by attending on $h$ and conditioning on the previously predicted frames.

We used the same setup used in the original Tacotron 2 paper \cite{tts3}.  Thus, we used WaveGlow \cite{waveglow} as a vocoder, and for the data to train the TTS on, we used the Arabic Common Voice dataset \footnote{https://voice.mozilla.org/}.  The data was filtered in order to use the top 10 speakers that have the highest number of utterances with relatively the highest quality.  This was done since most speakers in the dataset do not have a large number of utterances.

\subsection{ConformerGRU Model}

Convolution Neural network (CNN) and Recurrent Neural network (RNN) have their own advantages and limitations.  For example, while CNN exploits local information and local dependencies, RNN exploits long-term information and dependencies. 

\textit{Conformer Networks} \cite{conformer} capture information as well as long and short-term dependencies by combining multi-head attention \cite{transformer} with convolution neural networks.  The resulting model can be used to model both local and global dependencies.  To improve the validation on local and global dependencies, we combine the \textit{Conformer Block} with a \textit{Gated Recurrent Unit} GRU layer as describe next. 

Given a data set $\mathcal{D}=\{(x_1, y_1), (x_2, y_2),\ldots,(x_N, y_N)\}$ where $x_i$, and $y_i$ the $i^{th}$ input example and the target label respectively, the objective is to model $P(Y \mid X)$ using a function $f_\theta$ that maximizes the following objective function and the entropy:


\begin{equation}
\max_{\theta} \prod_{i=1}^{N} P(y_i \mid  x_i;\theta)
    \label{objective}
\end{equation}

\begin{equation}
\min_{\theta} \sum_{i=1}^{N} -log(P(y_i \mid  x_i;\theta))
    \label{entropy}
\end{equation}

To model $f_\theta$ we propose the \textit{ConformerGRU} model that consists of the following layers with the full architecture shown in Figure \ref{model}:
\begin{enumerate}
    \item A \textit{Pre-net Layer} that projects the speech feature space into a higher-level representation;
    \item A \textit{Conformer Block} that consists of multiple Conformer layers, where we can ensure the model able to handle long and short-term information dependencies.
    \item A single \textit{Gated Recurrent Unit (GRU)} whcih acts as an aggregate function instead of using the sum or the average of hidden states or the first hidden state only.
    \item A \textit{Post-net Layer} of two modules where the first is a simple projection layer followed by a prediction layer with a \textit{softmax} activation function.
\end{enumerate}

\begin{figure}[t]
  \includegraphics[width=\linewidth]{images/model.png}
  \caption{\label{model} ConformerGRU model architecture}
\end{figure}

\section{Experiments and results}
\label{sec:results}
\subsection{Experiments Setup}

Let the data $\mathcal{D}=\{(x_{1}, y_{1}), (x_{2}, y_{2}),\ldots,(x_{N}, y_{N})\}$, such that, $x_{i}$ and $y_{i}$ is the speech signal and the target label/command respectively.  Let $y_{i}\in{Y}$ and $x_{i}\in \mathbb{R}^{C \times S}$, where $Y$ is the set of all unique labels, $C$ is the number channels, and $S$ the number of the speech samples in that utterance.  We added an extra-label to represent the noise/NULL label.  Thus, 300 noise audios were generated and split into 60\% for training, 20\% for validation, and 20\% for testing, using the same noise audios and similar criteria as in \cite{asc}.

All the synthetic data generated from the text-to-speech mentioned in \ref{synthaticdata} for all speakers were added to the training data.  Furthermore, online augmentation was applied during training and no offline augmentation was used.

For all experiments, we extracted 40 Mel-frequency cepstral coefficients (MFCC) features which were computed using a 25ms window size, with a stride of 10ms, and 80-channel filter banks.

We used the negative log-likelihood loss, and Adam optimizer with linear learning rate decay as shown in \ref{lr} where $lr_0$ is the initial learning rate, for all experiments we set $lr_0$ to $10^{-3}$, $e$ is the current epoch, and $E$ is the total number of epochs, and lastly, a dropout of 15\% ratio used for regularization purposes.

We trained all models on a single machine using a single NVIDIA 3080 TI GPU, with a batch size of 256.
Since the data is balanced across all labels, we used accuracy \ref{accuracy} as a metric to measure the performance across all experiments, given that $\hat{y}_i$ is the predicted class for the  $i^{th}$ example. 

\begin{equation}
    Accuracy = \frac{1}{N} \sum_{i=1}^N \mathbbm{1}(\hat{y}_i == y_i) * 100\%
\label{accuracy}
\end{equation}

\begin{equation}
    lr(e, E) = lr_0 * (1 - \frac{e}{E})
    \label{lr}
\end{equation}


\subsection{Results}
A wide set of experiments have been conducted, starting from how changing the model architecture affects the results, to how synthetic data generation boosts the model performance.

Table \ref{results} shows the results under different model conditions while using only the original data with data augmentation, where we can see that increasing the model dimensionality improves the performance, also we can see that increasing the number of heads does not give better results, while adding synthetic data did boost the performance in all scenarios as table \ref{synthresult} and graph \ref{graph:results} show, where we can see that our method clearly outperform other works that achieved 97.97\% accuracy on the test set while our best model achieved 99.59\% resulting of 80\% reduction on the error rate.

\begin{table}[!htp]
  \begin{center}
      \centering
      \begin{tabular}{c|c}
      \end{tabular}
    \begin{tabular}{ccccc}
      \hline \hline
      $d_{model}$ & $h$ & $N$ & $ACC(\%)$ & $\#Params$\\ \hline \hline
      64 & 4 & 2 & 98.21 & 234K\\ \hline 
      64 & 4 & 1 & 97.19 & 165K\\ \hline 
      64 & 2 & 2 & $\boldsymbol{98.5}$ & 234K\\ \hline 
      64 & 2 & 1 & 97.64 & 165K\\ \hline \hline 
      96 & 4 & 2 & 98.78 & 511K\\ \hline
      96 & 4 & 1 & 98.17 & 358K\\ \hline 
      96 & 2 & 2 & $\boldsymbol{99.1}$ & 511K\\ \hline 
      96 & 2 & 1 & 98.17 & 358K\\ \hline \hline 
      128 & 4 & 2 & 99.17 & 895K\\ \hline 
      128 & 4 & 1 & 98.7 & 625K\\ \hline 
      128 & 2 & 2 & $\boldsymbol{99.35}$ & 895K\\ \hline 
      128 & 2 & 1 & 98.61 & 625K\\ \hline \hline 
    \end{tabular}
    \caption{\label{results} Results obtained by training the model on the original training data only, where $d_{model}$ is the model dimensionality, $h$ is the number of attention heads, $N$ is the number of conformer layers, $ACC$ is the accuracy, and lastly $\#Params$ is the number of model parameters.}
  \end{center}
  \end{table}
  
  
  \begin{table}[!htp]
  \begin{center}
      \centering
      \begin{tabular}{c|c}
      \end{tabular}
    \begin{tabular}{ccccc}
      \hline \hline
      d\_model & h & N & ACC(\%) & \#Params\\ \hline \hline
      64 & 4 & 2 & 98.41 & 234K\\ \hline 
      64 & 4 & 1 & 98.01 & 165K\\ \hline 
      64 & 2 & 2 & $\boldsymbol{98.66}$ & 234K\\ \hline 
      64 & 2 & 1 & 97.93 & 165K\\ \hline \hline 
      96 & 4 & 2 & $\boldsymbol{99.19}$ & 511K\\ \hline
      96 & 4 & 1 & 98.41 & 358K\\ \hline 
      96 & 2 & 2 & 99.15 & 511K\\ \hline 
      96 & 2 & 1 & 98.54 & 358K\\ \hline \hline 
      128 & 4 & 2 & 99.23  & 895K\\ \hline 
      128 & 4 & 1 & 98.94 & 625K\\ \hline 
      128 & 2 & 2 & $\boldsymbol{99.59}$ & 895K\\ \hline 
      128 & 2 & 1 & 99.27 & 625K\\ \hline \hline 
    \end{tabular}
    \caption{\label{synthresult} Results obtained by training the model on the original training data with the synthetic data combined, where $d_{model}$ is the model dimensionality, $h$ is the number of attention heads, $N$ is the number of conformer layers, $ACC$ is the accuracy, and lastly $\#Params$ is the number of model parameters.}
  \end{center}
  \end{table}

  \begin{figure}[!htp]
  \includegraphics[width=\linewidth]{images/results.png}
  \caption{\label{graph:results} AraSpot results across different scenarios, where the X axis is the model parameters in the form of model dimensionality, number of heads, and number of layers, and the Y axis is the accuracy. The horizontal black line is the best-performing model in the literature.}
\end{figure}


\section{Conclusion and future work}
\label{sec:conc}
This work presented AraSpot for Arabic Spoken keyword Spotting that achieved State-of-the-Art SOTA 99.59\% result outperforming previous approaches, by employing synthetic data generation using text-to-speech, online data augmentation, and introducing ConformerGRU model architecture.
For future work, we recommend expanding the number of commands and increasing the number of speakers to expand the synthetic data generated.


\bibliographystyle{ieeetr}
\bibliography{ASR}

\end{document}
