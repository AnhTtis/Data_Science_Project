@INPROCEEDINGS{6854211,
  author={Vinyals, Oriol and Wegmann, Steven},
  booktitle={2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Chasing the metric: Smoothing learning algorithms for keyword detection}, 
  year={2014},
  volume={},
  number={},
  pages={3301-3305},
  doi={10.1109/ICASSP.2014.6854211}}

@article{Farghaly2009,
author = {Farghaly, Ali and Shaalan, Khaled},
title = {Arabic Natural Language Processing: Challenges and Solutions},
year = {2009},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1530-0226},
url = {https://doi.org/10.1145/1644879.1644881},
doi = {10.1145/1644879.1644881},
journal = {ACM Transactions on Asian Language Information Processing},
month = {dec},
articleno = {14},
numpages = {22},
keywords = {Modern Standard Arabic, Arabic script, Arabic dialects}
}

@inproceedings{kwsapplications,
author = {Zhuang, Yimeng and Chang, Xuankai and Qian, Yanmin and Yu, Kai},
year = {2016},
month = {09},
pages = {938-942},
title = {Unrestricted Vocabulary Keyword Spotting Using LSTM-CTC},
doi = {10.21437/Interspeech.2016-753}
}

@ARTICLE{9427206,
  author={Seo, Deokjin and Oh, Heung-Seon and Jung, Yuchul},
  journal={IEEE Access}, 
  title={Wav2KWS: Transfer Learning From Speech Representations for Keyword Spotting}, 
  year={2021},
  volume={9},
  number={},
  pages={80682-80691},
  doi={10.1109/ACCESS.2021.3078715}}
  
@misc{https://doi.org/10.48550/arxiv.2002.01322,
  doi = {10.48550/ARXIV.2002.01322},
  
  url = {https://arxiv.org/abs/2002.01322},
  
  author = {Lin, James and Kilgour, Kevin and Roblek, Dominik and Sharifi, Matthew},
  
  keywords = {Audio and Speech Processing (eess.AS), Machine Learning (cs.LG), Sound (cs.SD), Machine Learning (stat.ML), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Training Keyword Spotters with Limited and Synthesized Speech Data},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@INPROCEEDINGS{8268946,
  author={Michaely, Assaf Hurwitz and Zhang, Xuedong and Simko, Gabor and Parada, Carolina and Aleksic, Petar},
  booktitle={2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)}, 
  title={Keyword spotting for Google assistant using contextual speech recognition}, 
  year={2017},
  volume={},
  number={},
  pages={272-278},
  doi={10.1109/ASRU.2017.8268946}}


@ARTICLE{8607038,
  author={Zeng, Mengjun and Xiao, Nanfeng},
  journal={IEEE Access}, 
  title={Effective Combination of DenseNet and BiLSTM for Keyword Spotting}, 
  year={2019},
  volume={7},
  number={},
  pages={10767-10775},
  doi={10.1109/ACCESS.2019.2891838}}


@INPROCEEDINGS{6841973,
  author={Alwajeeh, Ahmed and Al-Ayyoub, Mahmoud and Hmeidi, Ismail},
  booktitle={2014 5th International Conference on Information and Communication Systems (ICICS)}, 
  title={On authorship authentication of Arabic articles}, 
  year={2014},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/IACS.2014.6841973}}


@article{article,
author = {Boudad, Naaima and Faizi, Rdouan and Rachid, Oulad haj thami and Chiheb, Raddouane},
year = {2017},
month = {07},
pages = {},
title = {Sentiment analysis in Arabic: A review of the literature},
volume = {9},
journal = {Ain Shams Engineering Journal},
doi = {10.1016/j.asej.2017.04.007}
}

@article{asc,
title = {Building and benchmarking an Arabic Speech Commands dataset for small-footprint keyword spotting},
journal = {Engineering Applications of Artificial Intelligence},
volume = {102},
pages = {104267},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104267},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621001147},
author = {Abdulkader Ghandoura and Farouk Hjabo and Oumayma {Al Dakkak}},
}

@misc{gsc,
  doi = {10.48550/ARXIV.1804.03209},
  url = {https://arxiv.org/abs/1804.03209},  
  author = {Warden, Pete},  
  keywords = {Computation and Language (cs.CL), Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences},  
  title = {Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition},  
  publisher = {arXiv},  
  year = {2018},  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{dataaug,
  doi = {10.48550/ARXIV.2204.08610},
  
  url = {https://arxiv.org/abs/2204.08610},
  
  author = {Yang, Suorong and Xiao, Weikang and Zhang, Mengcheng and Guo, Suhan and Zhao, Jian and Shen, Furao},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Image Data Augmentation for Deep Learning: A Survey},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{dataaug1,
author = {Shorten, Connor and Khoshgoftaar, Taghi},
year = {2019},
month = {07},
pages = {},
title = {A survey on Image Data Augmentation for Deep Learning},
volume = {6},
journal = {Journal of Big Data},
doi = {10.1186/s40537-019-0197-0}
}

@inproceedings{inproceedings,
author = {Fonseca, Eduardo and Pons, Jordi and Favory, Xavier and Font, Frederic and Bogdanov, Dmitry and Ferraro, Andres and Oramas, Sergio and Porter, Alastair and Serra, Xavier},
year = {2017},
month = {10},
pages = {},
title = {Freesound Datasets: A Platform for the Creation of Open Audio Datasets}
}

@INPROCEEDINGS{rir1,
  author={Jeub, Marco and Schafer, Magnus and Vary, Peter},
  booktitle={2009 16th International Conference on Digital Signal Processing}, 
  title={A binaural room impulse response database for the evaluation of dereverberation algorithms}, 
  year={2009},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/ICDSP.2009.5201259}}


@article{rir2,
	doi = {10.1109/jstsp.2019.2917582},
	url = {https://doi.org/10.1109%2Fjstsp.2019.2917582},
	year = 2019,
	month = {aug},
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume = {13},
	number = {4},
	pages = {863--876},
	author = {Igor Szoke and Miroslav Skacel and Ladislav Mosner and Jakub Paliesek and Jan Cernocky},
	title = {Building and evaluation of a real room impulse response dataset},
	journal = {{IEEE} Journal of Selected Topics in Signal Processing}
}

@inproceedings{SpecAugment,
	doi = {10.21437/interspeech.2019-2680},
	url = {https://doi.org/10.21437%2Finterspeech.2019-2680},
	year = 2019,
	month = {sep},
  
	publisher = {{ISCA}
},
	author = {Daniel S. Park and William Chan and Yu Zhang and Chung-Cheng Chiu and Barret Zoph and Ekin D. Cubuk and Quoc V. Le},
	title = {{SpecAugment}: A Simple Data Augmentation Method for Automatic Speech Recognition},
	booktitle = {Interspeech 2019}
}

@inproceedings{tts0,
  title={Char2Wav: End-to-End Speech Synthesis},
  author={Jose M. R. Sotelo and Soroush Mehri and Kundan Kumar and Jo{\~a}o Felipe Santos and Kyle Kastner and Aaron C. Courville and Yoshua Bengio},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@misc{tts1,
  doi = {10.48550/ARXIV.1710.07654},
  
  url = {https://arxiv.org/abs/1710.07654},
  
  author = {Ping, Wei and Peng, Kainan and Gibiansky, Andrew and Arik, Sercan O. and Kannan, Ajay and Narang, Sharan and Raiman, Jonathan and Miller, John},
  
  keywords = {Sound (cs.SD), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{tts2,
  doi = {10.48550/ARXIV.1712.05884},
  
  url = {https://arxiv.org/abs/1712.05884},
  
  author = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J. and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerry-Ryan, RJ and Saurous, Rif A. and Agiomyrgiannakis, Yannis and Wu, Yonghui},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{tts3,
  doi = {10.48550/ARXIV.1703.10135},
  
  url = {https://arxiv.org/abs/1703.10135},
  
  author = {Wang, Yuxuan and Skerry-Ryan, RJ and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Sound (cs.SD), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Tacotron: Towards End-to-End Speech Synthesis},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{waveglow,
  doi = {10.48550/ARXIV.1811.00002},
  
  url = {https://arxiv.org/abs/1811.00002},
  
  author = {Prenger, Ryan and Valle, Rafael and Catanzaro, Bryan},
  
  keywords = {Sound (cs.SD), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {WaveGlow: A Flow-based Generative Network for Speech Synthesis},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{conformer,
  doi = {10.48550/ARXIV.2005.08100},
  
  url = {https://arxiv.org/abs/2005.08100},
  
  author = {Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and Pang, Ruoming},
  
  keywords = {Audio and Speech Processing (eess.AS), Machine Learning (cs.LG), Sound (cs.SD), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Conformer: Convolution-augmented Transformer for Speech Recognition},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{transformer,
  doi = {10.48550/ARXIV.1706.03762},
  
  url = {https://arxiv.org/abs/1706.03762},
  
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Attention Is All You Need},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@INPROCEEDINGS{6707766,
  author={Chen, Guoguo and Yilmaz, Oguz and Trmal, Jan and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2013 IEEE Workshop on Automatic Speech Recognition and Understanding}, 
  title={Using proxies for OOV keywords in the keyword search task}, 
  year={2013},
  volume={},
  number={},
  pages={416-421},
  doi={10.1109/ASRU.2013.6707766}}

  @inproceedings{10.21437,
author = {Miller, David and Kleber, Michael and Kao, Chia-Lin and Kimball, O. and Colthurst, Thomas and Lowe, Stephen and Schwartz, Richard and Gish, Herbert},
year = {2007},
month = {08},
pages = {314-317},
title = {Rapid and accurate spoken term detection},
journal = {Proc. Interspeech},
doi = {10.21437/Interspeech.2007-174}
}

@INPROCEEDINGS{319341,
  author={Weintraub, M.},
  booktitle={1993 IEEE International Conference on Acoustics, Speech, and Signal Processing}, 
  title={Keyword-spotting using SRI's DECIPHER large-vocabulary speech-recognition system}, 
  year={1993},
  volume={2},
  number={},
  pages={463-466 vol.2},
  doi={10.1109/ICASSP.1993.319341}}

@INPROCEEDINGS{266505,
  author={Rohlicek, J.R. and Russell, W. and Roukos, S. and Gish, H.},
  booktitle={International Conference on Acoustics, Speech, and Signal Processing,}, 
  title={Continuous hidden Markov modeling for speaker-independent word spotting}, 
  year={1989},
  volume={},
  number={},
  pages={627-630 vol.1},
  doi={10.1109/ICASSP.1989.266505}}

@INPROCEEDINGS{115555,
  author={Rose, R.C. and Paul, D.B.},
  booktitle={International Conference on Acoustics, Speech, and Signal Processing}, 
  title={A hidden Markov model based keyword recognition system}, 
  year={1990},
  volume={},
  number={},
  pages={129-132 vol.1},
  doi={10.1109/ICASSP.1990.115555}}


@misc{https://doi.org/10.48550/arxiv.1811.07684,
  doi = {10.48550/ARXIV.1811.07684},
  
  url = {https://arxiv.org/abs/1811.07684},
  
  author = {Coucke, Alice and Chlieh, Mohammed and Gisselbrecht, Thibault and Leroy, David and Poumeyrol, Mathieu and Lavril, Thibaut},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Sound (cs.SD), Audio and Speech Processing (eess.AS), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {Efficient keyword spotting using dilated convolutions and gating},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@Inproceedings{Sun2017,
 author = {Ming Sun and David Snyder and Yixin Gao and Varun Nagaraja and Mike Rodehorst and Sankaran Panchapagesan and Nikko Str√∂m and Spyros Matsoukas and Shiv Vitaladevuni},
 title = {Compressed Time Delay Neural Network for Small-footprint Keyword Spotting},
 year = {2017},
 url = {https://www.amazon.science/publications/compressed-time-delay-neural-network-for-small-footprint-keyword-spotting},
 booktitle = {Interspeech 2017},
}

@misc{https://doi.org/10.48550/arxiv.1808.00563,
  doi = {10.48550/ARXIV.1808.00563},
  
  url = {https://arxiv.org/abs/1808.00563},
  
  author = {Raju, Anirudh and Panchapagesan, Sankaran and Liu, Xing and Mandal, Arindam and Strom, Nikko},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Data Augmentation for Robust Keyword Spotting under Playback Interference},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{https://doi.org/10.48550/arxiv.1909.11699,
  doi = {10.48550/ARXIV.1909.11699},
  
  url = {https://arxiv.org/abs/1909.11699},
  
  author = {Rosenberg, Andrew and Zhang, Yu and Ramabhadran, Bhuvana and Jia, Ye and Moreno, Pedro and Wu, Yonghui and Wu, Zelin},
  
  keywords = {Computation and Language (cs.CL), Sound (cs.SD), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {Speech Recognition with Augmented Synthesized Speech},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{https://doi.org/10.48550/arxiv.1811.00707,
  doi = {10.48550/ARXIV.1811.00707},
  
  url = {https://arxiv.org/abs/1811.00707},
  
  author = {Li, Jason and Gadde, Ravi and Ginsburg, Boris and Lavrukhin, Vitaly},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Sound (cs.SD), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {Training Neural Speech Recognition Systems with Synthetic Speech Augmentation},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{https://doi.org/10.48550/arxiv.1703.05390,
  doi = {10.48550/ARXIV.1703.05390},
  
  url = {https://arxiv.org/abs/1703.05390},
  
  author = {Arik, Sercan O. and Kliegl, Markus and Child, Rewon and Hestness, Joel and Gibiansky, Andrew and Fougner, Chris and Prenger, Ryan and Coates, Adam},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inbook{GCNNLSTM,
author = {Wang, Dong and Lv, Shaohe and Wang, Xiaodong and Lin, Xinye},
year = {2018},
month = {06},
pages = {669-681},
title = {Gated Convolutional LSTM for Speech Commands Recognition},
isbn = {978-3-319-93700-7},
doi = {10.1007/978-3-319-93701-4_53}
}

@misc{https://doi.org/10.48550/arxiv.1803.10916,
  doi = {10.48550/ARXIV.1803.10916},
  
  url = {https://arxiv.org/abs/1803.10916},
  
  author = {Shan, Changhao and Zhang, Junbo and Wang, Yujun and Xie, Lei},
  
  keywords = {Sound (cs.SD), Computation and Language (cs.CL), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {Attention-based End-to-End Models for Small-Footprint Keyword Spotting},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{asc2,
author = {Benamer, Lina and Alkishriwo, Osama},
year = {2020},
month = {12},
pages = {},
title = {Database for Arabic Speech Commands Recognition}
}

@article{arabicaudsearch,
author = {Awaid, Mostafa and Fawzy, Sahar and Kandil, Ahmed},
year = {2014},
month = {01},
pages = {128},
title = {Audio Search Based on Keyword Spotting in Arabic Language},
volume = {5},
journal = {International Journal of Advanced Computer Science and Applications},
doi = {10.14569/IJACSA.2014.050219}
}

@misc{raju2018data,
      title={Data Augmentation for Robust Keyword Spotting under Playback Interference}, 
      author={Anirudh Raju and Sankaran Panchapagesan and Xing Liu and Arindam Mandal and Nikko Strom},
      year={2018},
      eprint={1808.00563},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@INPROCEEDINGS{7953152,
  author={Ko, Tom and Peddinti, Vijayaditya and Povey, Daniel and Seltzer, Michael L. and Khudanpur, Sanjeev},
  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={A study on data augmentation of reverberant speech for robust speech recognition}, 
  year={2017},
  volume={},
  number={},
  pages={5220-5224},
  doi={10.1109/ICASSP.2017.7953152}}


@inproceedings{46107,
title	= {Generation of large-scale simulated utterances in virtual rooms to train deep-neural networks for far-field speech recognition in Google Home},
author	= {Chanwoo Kim and Ananya Misra and Kean Chin and Thad Hughes and Arun Narayanan and Tara Sainath and Michiel Bacchiani},
year	= {2017},
URL	= {http://www.isca-speech.org/archive/Interspeech_2017/pdfs/1510.PDF},
pages	= {379-383}
}


@Article{s22020592,
AUTHOR = {Yun, Deokgyu and Choi, Seung Ho},
TITLE = {Deep Learning-Based Estimation of Reverberant Environment for Audio Data Augmentation},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {592},
URL = {https://www.mdpi.com/1424-8220/22/2/592},
PubMedID = {35062553},
ISSN = {1424-8220},
ABSTRACT = {This paper proposes an audio data augmentation method based on deep learning in order to improve the performance of dereverberation. Conventionally, audio data are augmented using a room impulse response, which is artificially generated by some methods, such as the image method. The proposed method estimates a reverberation environment model based on a deep neural network that is trained by using clean and recorded audio data as inputs and outputs, respectively. Then, a large amount of a real augmented database is constructed by using the trained reverberation model, and the dereverberation model is trained with the augmented database. The performance of the augmentation model was verified by a log spectral distance and mean square error between the real augmented data and the recorded data. In addition, according to dereverberation experiments, the proposed method showed improved performance compared with the conventional method.},
DOI = {10.3390/s22020592}
}



@INPROCEEDINGS{10023291,
  author={Park, Jinhwan and Jin, Sichen and Park, Junmo and Kim, Sungsoo and Sandhyana, Dhairya and Lee, Changheon and Han, Myoungji and Lee, Jungin and Jung, Seokyeong and Han, Changwoo and Kim, Chanwoo},
  booktitle={2022 IEEE Spoken Language Technology Workshop (SLT)}, 
  title={Conformer-Based on-Device Streaming Speech Recognition with KD Compression and Two-Pass Architecture}, 
  year={2023},
  volume={},
  number={},
  pages={92-99},
  doi={10.1109/SLT54892.2023.10023291}}


@inproceedings{Zhang_2023,
   title={Conformer-Based Target-Speaker Automatic Speech Recognition For Single-Channel Audio},
   url={http://dx.doi.org/10.1109/ICASSP49357.2023.10095115},
   DOI={10.1109/icassp49357.2023.10095115},
   booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
   publisher={IEEE},
   author={Zhang, Yang and Puvvada, Krishna C. and Lavrukhin, Vitaly and Ginsburg, Boris},
   year={2023},
   month=jun }


@InProceedings{10.1007/978-3-031-20233-9_34,
author="Shi, Yihua
and Ma, Guanglin
and Ren, Jin
and Zhang, Haigang
and Yang, Jinfeng",
editor="Deng, Weihong
and Feng, Jianjiang
and Huang, Di
and Kan, Meina
and Sun, Zhenan
and Zheng, Fang
and Wang, Wenfeng
and He, Zhaofeng",
title="An End-to-End Conformer-Based Speech Recognition Model for Mandarin Radiotelephony Communications in Civil Aviation",
booktitle="Biometric Recognition",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="335--347",
abstract="In civil aviation radiotelephony communications, misunderstandings between air traffic controllers and flight crews can result in serious aviation accidents. Automatic semantic verification is a promising assistant solution to decrease miscommunication, thanks to advancements in speech and language processing. Unfortunately, existing general speech recognition models are ineffective when it comes to capturing contextual long-distance dependent local similarity features in radiotelephony communications. To address these problems, this paper proposes an end-to-end Conformer-based multi-task learning speech recognition model for Mandarin radiotelephony communications in civil aviation. The Conformer model improves local information capture while retaining the global information modeling capabilities of contextual long-distance dependencies, owing to the introduction of the convolution module to the Transformer model. Meanwhile, multi-task learning is used to further improve performance by combining connectionist temporal classification (CTC) and attention-based encoder-decoder (AED) models. The experimental results show that the proposed model can perform global and local acoustic modeling effectively, making it particularly suitable for extracting acoustic features of Mandarin civil aviation radiotelephony communications.",
isbn="978-3-031-20233-9"
}

