\section{Methodology}
\label{sec:method} 
As shown in Figure~\ref{fig:architecture}, the proposed architecture contains three main parts. The dual-level spatial transformer fuses spatial information among sensors for each timestamp. The importance score temporal transformer combines temporal information for each sensor, giving a higher ``importance score'' to sensors experiencing sudden feature changes after the validation time. A pooling module projects the learned dynamic graph representation to the expected output.

% modify the figure as the model changes
\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/architecture4.jpg}
  \caption{\textbf{The architecture of DG-Trans.} The blue rectangle labeled \textbf{A. Dual-Level Spatial Transformer} first encodes the input tensor by performing ``sensor-to-road,''  ``road-to-road,'' and ``road-to-sensor'' attentions. The orange rectangles labeled \textbf{B. Importance Score Transformer} further process the output of A $\mathbf{H}^{ST}$ with three modules.  $\mathbf{H}^{ST}$ is split into $\mathbf{H}_{bv}$ and $\mathbf{H}_{av}$ by the validation time of the incident, then fed to the T-Transformer with importance scores initialized as 0. The outputs $\mathbf{H}$ and $\mathbf{H}_{av}$ are sent to Decoder \#1 to reconstruct $\mathbf{X}_{av}$ and compute the importance score as $\mathbf{H}-\mathbf{H}'_{av}$. With the updated importance score, $\mathbf{H}^{ST}$ is fed to T-Transformer again. The output is further
   processed by Decoder \#2 to become $\mathbf{H}''_{av}$. The green rectangle labeled \textbf{C. Pooling} shows how the latent features are projected to the desired outputs. Finally, the loss is computed as the weighted combination of (1) the reconstruction from $\mathbf{H}'_{av}$ to $\mathbf{X}_{av}$, (2) the reconstruction from $\mathbf{H}''_{av}$ to $\mathbf{X}_{av}$, and (3) the prediction loss of impact duration and length.} % describe the architecture
  \label{fig:architecture}
\end{figure*}

\subsection{Dual-Level Spatial Transformer} The design of the dual-level transformer is inspired by the concepts of anchored graphs and hypergraphs. Anchors have been used to reduce the complexity of attention in many studies~\cite
 {chen2020iterative, baek2021accurate, mialon2021graphit}. In our case, roads are considered anchor nodes for the sensors on them. While using anchors, the rank of the adjacency matrix decreases from $|S|$ to $|R|$. This means that multi-hop message-passing only matters among roads. The model maintains a global latent feature tensor $\mathbf{H}^{r} \in \mathbb{R}^{|R|\times T \times C}$ for each road at each timestamp, which does not vary by case. For each incident case, the sensor features are first used to update the case-irrelevant road feature in order to acquire case-relevant road features $\mathbf{H}^{sr}$. Then, the road features are further updated to $\mathbf{H}^{rr}$ by message-passing among roads. At this stage, the output road features can be considered both an intermediate pooling of the sensor features and a spatial feature fusion of the sensors. 
 % Therefore, $\mathbf{H}^{rr}$ is used for both impact prediction and sensor feature reconstruction. The second task is considered as a regularization of the first task so that the $\mathbf{H}^{rr}$ does not overfit the impact results or diverge from the raw representation of the sensor network.
 Finally, the road features are propagated back to the sensors for the next step.
 
 Denote sensors as $S$ and roads as $R$. A vanilla self-attention exploring spatial relations between sensors can be summarized as follows:
\begin{equation}
  a^{ss}_{ij} = \sigma(\frac{(\mathbf{Q} \mathbf{h}_{s_j})^T(\mathbf{K}\mathbf{h}_{s_i})}{\sqrt{d}})
\end{equation} 
where $s_i$ and $s_j$ are two different sensor nodes, $\mathbf{Q}$ and $\mathbf{K}$ are query and key projection parameters, $\mathbf{h}_{s_i}$ and $\mathbf{h}_{s_j}$ are embeddings of $s_i$ and $s_j$, $d=C \div n\_head$ is the dimension of each attention head, and $\sigma$ is the row-wise softmax activation function. 

In contrast to the vanilla approach, in S-Transformer, the attentive message-passing is accomplished with three transformer layers. The first transformer layer contains a ``sensor-to-road'' attention layer and a linear layer. The ``sensor-to-road'' attention computes the correlation
between sensors and roads and masks out the edge between $s_i$ and $r_j$ if
$s_i$ is not on $r_j$:
\begin{equation}
\label{eq:sr_attn}
\begin{aligned}
  &a^{sr}_{ij} = \sigma(mask_{sr}(\frac{(\mathbf{Q}^{sr}\mathbf{h}_{r_j})(\mathbf{K}^{sr}\mathbf{h}_{s_i})^T}{\sqrt{d}},m^{sr}_{ij})),\\
  &mask(\mathbf{x}, \lambda)=\begin{cases} \mathbf{x} & \lambda = 1 \\
                     -\infty &  \lambda = 0
       \end{cases}
\end{aligned}
\end{equation}
where $s_i$ and $r_j$ represent an arbitrary sensor and a road. $\mathbf{Q}^{sr}$ and
$\mathbf{K}^{sr}$ are query and key projection parameters for $r_j$ and $s_i$. $\mathbf{h}_{s_i}$ and $\mathbf{h}_{r_j}$ are embeddings of $s_i$ and $r_j$. $\mathbf{h}_{r_j} \in \mathbb{R}^{T\times C}$ is a
learnable parameter matrix. $\mathbf{M}^{sr} \in \{0, 1\}^{|S|\times|R|}$ is the
adjacent matrix between sensors and roads. $m^{sr}_{ij}=1$ if sensor $s_i$ is
on road $r_j$, otherwise $m^{sr}_{ij}=0$. $\mathbf{M}^{sr}$ performs as the mask of all the attention heads. With $a^{sr}_{ij}$, the road embedding can be updated as $\mathbf{h}_{r_j}^{sr}=\mathbf{W}^{sr}\sum_{i=1}^{|S|}a^{sr}_{ij}(\mathbf{V}^{sr}\mathbf{h}_{r_j})+\mathbf{b}^{sr}$.

The attention in the second transformer is a ``road-to-road'' self-attention layer intended to
extract road intersection information. The attention can be expressed as
follows:
\begin{equation}
  a^{rr}_{ij} = \sigma(mask_{rr}(\frac{(\mathbf{Q}^{rr}\mathbf{h}_{r_j}^{sr})(\mathbf{K}^{rr}\mathbf{h}_{r_i}^{sr})^T}{\sqrt{d}},\mathbf{M}^{rr}_{ij}))
\end{equation}
where $\mathbf{M}^{rr} \in \{0, 1\}^{|R|\times|R|}$ represents four levels of adjacency between
roads: $m^{rr}_{ij}=1$ if (1) $i=j$, (2) $r_i$ intersects $r_j$, (3) $r_i$ intersects $r_k$ and $r_j$ intersects $r_k$, and (4) fully connected. The four different masks can be applied to different attention heads. In our design, each of the four masks was applied to one attention head. A spatial-relation awared road feature tensor is then computed as $\mathbf{h}_{r_j}^{rr}=\mathbf{W}^{rr}\sum_{i=1}^{|R|}a^{rr}_{ij}(\mathbf{V}^{rr}\mathbf{h}_{r_j}^{sr})+\mathbf{b}^{rr}$.

The last transformer contains a ``road-to-sensor'' attention, meaning that sensor vectors are queries, and road vectors are keys and values. The output of this step propagates the aggregated road features to the sensors. 
% is used to examine if $\mathbf{H}^{rr}$ can represent the raw sensor network. 
The equation is essentially the reversed version of Equation~\ref
{eq:sr_attn}:
\begin{equation}
  \mathbf{H}^{rs} = \sigma(\frac{(\mathbf{Q}^{rs}\mathbf{H}^{rr})(\mathbf{K}^{rs}\mathbf{H}^{rr})^T}{\sqrt{d}})(\mathbf{V}^{rs}\mathbf{H}^{rr})
\end{equation}

Note that there is no mask in this step, which preserves the attention
mechanism's flexibility. Previous works have usually linked the top-k closest sensors in
the geographic or feature space, weighting the links with the distances.
However, we observe that attention is good at learning weights but weak at
learning graph structures. In this case, we simply need to find nodes that
are definitely linked to each other and leave the weight learning task to
attention. Therefore, we chose to partially control the graph structure with unweighted adjacent matrices. Finally, we applied skip-connection, layer-normalization, and dropout to $\mathbf{H}^{rs}$.

Essentially, S-Transformer stresses the effects of sensors on
high-degree-centrality roads. The strengths of the S-Transformer can be
summarized as follows: (1) it avoids the human error introduced by manually
choosing ``road-to-road'' for ``top-k'', (2) it allows long-range message-passing as all sensors on intersected roads are linked, (3) it preserves the flexibility of attention with a relatively small number of edges
($|S|+|R|^2+|S||R|, |R|\ll |S|$ at most) (4) it is more time efficient ($\mathcal{O}(|S||R|^2+|R|^3+|R|^2|S|), |R|\ll |S|$) than traditional attention mechanisms ($\mathcal{O}(|S|^3)$) and requires fewer layers as the spatial message-passing is performed sufficiently by the ``road-to-road'' self-attention module.

\subsection{Importance Score Transformer} As an incident usually affects only
 a relatively small part of the whole traffic network, treating all sensors equally for prediction may introduce unwanted noise. However, manually extracting sensors near the incident may cause the loss of long-range complex impact
 patterns and even the most representative features due to the early/delayed response of the traffic network. In this case, a method that dynamically locates the region and time window affected by the incident is important. Based on the assumption that the traffic measurements of sensors affected by incidents show more obvious changes than other sensors, we locate the incidents with anomaly detection techniques (i.e., assigning sensors with larger variance a higher ``importance score''). 

Inspired by~\cite{tuli2022tranad}, our importance score
transformer contains three modules: a temporal transformer
(T-Transformer) and two decoders. The T-Transformer module encodes the output of the S-Transformer with and without the importance score along the time dimension. The first decoder computes the importance score and reconstructs
$\mathbf{X}_{av}$, while the second reconstructs $\mathbf{X}_{av}$ from the combination of the importance score and the graph embedding. Denote the ``after-validation'' section of $\mathbf{H}^{ST}$ as $\mathbf{H}_{av}$ and the ``before-validation'' section of $\mathbf{H}^{ST}$ as $\mathbf{H}_{bv}$. The combination of the T-Transformer and any one of the decoders is equivalent to a classic transformer network when considering $\mathbf{H}^{ST}$ as the input sequence and $\mathbf{H}_{av}$ as the output sequence.

Assume the importance score is $\mathbf{I} \in \mathbb{R}^
{|S|\times T \times C}$ and the output of S-Transformer as $\mathbf{H}^{ST} \in \mathbb{R}^{|S|\times T \times C}$. The output of the T-Transformer can be written as follows:
\begin{equation}\label{eq:score_1}
  \mathbf{H} = \text{TTrans}([\mathbf{H}^{ST} || \mathbf{I}_0])
\end{equation}
where $\text{TTrans}()$ indicates T-Transformer, which is a block sequentially performing temporal self-attention and skip-connection. $\mathbf{I}_0$ is the initialized importance score (which is an all-zero tensor).

The task for both Decoder \#1 and Decoder \#2 is to reconstruct $\mathbf{X}_{av}$. Each decoder has a self-attention layer and a mutual-attention layer. 
Decoder \#1 attempts to achieve its goal using $\mathbf{H}$ and $\mathbf{H}_{av}$:
\begin{equation}
  \mathbf{H}'_{av} = \text{mu-attn}_1(\mathbf{H}, \mathbf{H}, \text{self-attn}_1(\mathbf{H}_{av}))
\end{equation}

The three parameters in $\text{mu-attn}_1$ are placeholders for value, key, and query.

Then, the importance score is updated as $\mathbf{I} = \mathbf{H}^{ST} - \mathbf{H}'_{av}$. Note that $\mathbf{H}^{ST}$ has $T$ timestamps while $\mathbf{H}'_{av}$ has
$T_{av}$ timestamps. We examined various methods, such as repeating timestamps in $\mathbf{H}'_{av}$ and getting mean/min/max of $\mathbf{H}'_{av}$ along the time axis. All the methods resulted in similar performance. Accordingly, we
adopt the general form to represent the difference between $\mathbf{H}^{ST}$ and $\mathbf{H}'_{av}$. 
% We directly use the difference between $\mathbf{H}^{ST}$ and $\mathbf{H}'_
% {av}$ as we want to distinguish the traffic patterns before and after the
% incident occurrence/ Intuitively, for affected sensors, the average speed before the incident is larger than $avg(\mathbf{H}_{bv}) \geq \mathbf{H}'_{av}$ and
% $\mathbf{H}_{av} \leq \mathbf{H}'_{av}$ based on the intuition of anomaly
% detection. 
Replacing $\mathbf{I}_0$ with $\mathbf{I}$, we apply the T-Transformer and Decoder \#2 the same way as the previous steps, with the concatenated $\mathbf{H}^{ST}$ and $\mathbf{I}$ as the input. 

\begin{equation}\label{eq:score_2}
\begin{aligned}
  &\mathbf{H}' = \text{TTrans}([\mathbf{H}^{ST} ||\mathbf{I}])\\
  &\mathbf{H}''_{av} = \text{mu-attn}_2(\mathbf{H}', \mathbf{H}', \text{self-attn}_2(\mathbf{H}_{av}))
\end{aligned}
\end{equation}

Finally, $\mathbf{H}'_{av}$ and $\mathbf{H}''_{av}$ are used to reconstruct $\mathbf{X}_{av}$ separately with the same two-layer feed-forward network (FFN):
\begin{equation} \label{eq:ffn}
\begin{aligned}
  &\mathbf{X}'_{av} = \mathbf{W}_{1,2}(\phi( \mathbf{W}_{1,1}\mathbf{H}'_{av}+ \mathbf{b}_{1,1}))+ \mathbf{b}_{1,2} \\
  &\mathbf{X}''_{av} = \mathbf{W}_{1,2}(\phi( \mathbf{W}_{1,1}\mathbf{H}''_{av}+ \mathbf{b}_{1,1}))+ \mathbf{b}_{1,2}
\end{aligned}
\end{equation}
where $\mathbf{W}_{1,1}, \mathbf{b}_{1,1}, \mathbf{W}_{1,2}, \mathbf{b}_{1,2}$ are parameters of the two linear layers and $\phi$ is the activation function, which is the LeakyReLu in the FFN in Equation~\ref{eq:ffn}. 

\subsection{Pooling and Loss} After the spatial and temporal encoding, the spatiotemporal representation $\mathbf{H}'$ is considered well-learned and ready for prediction. To further refine the features, the importance score is used to weight the elements in $\mathbf{H}'$ through element-wise multiplication. This is followed by a temporal dimension aggregation through a 2D convolution layer and a spatial dimension aggregation through SUMPooling. Lastly, three linear layers with LeakyReLU activations are used to project the features into the desired dimension. This process can be summarized as follows:
\begin{equation}
\begin{aligned}
  &\mathbf{H}'_T = \text{Conv2d}(\mathbf{H}'), \mathbf{H}'_T \in \mathbb{R}^{|S|\times C}\\
  &\mathbf{H}'_S = \text{SUMPool}(\mathbf{H}'_T), \mathbf{H}'_T \in \mathbb{R}^C)\\
  &\hat{Y} = \mathbf{W}_{p3}\phi(\mathbf{W}_{p2}\phi(\mathbf{W}_{p1}\mathbf{H}'_S+\mathbf{b}_{p1})+\mathbf{b}_{p2}))+\mathbf{b}_{p3}
\end{aligned}
\end{equation}
where $\hat{Y}\in \mathbb{R}^{C_{out}}, C_{out}=2$. The first value represents the impact duration, and the second is the impact length. SUMPooling is chosen because it is the most expressive pooling method~\cite{xu2018powerful}, and we want the pooled representation to be capable of representing all possible incident scenarios. L1 loss is chosen as the prediction loss function so that the model focuses on the overall trends of $\mathbf{Y}$ instead of outliers. The prediction loss can then be written as $Loss_1 = |\mathbf{\hat{Y}}_{Dur}-\mathbf{Y}_{Dur}|+|\mathbf{\hat{Y}}_{Len}-\mathbf{Y}_{Len}|$.

The second part of the loss is the reconstruction loss. This loss is for regularization purposes and is self-supervised. The only objective of the model is to predict $\mathbf{Y}_{Dur}$ and $\mathbf{Y}_{Len}$. L1 loss is employed for both $\mathbf{X}'_{av}$ and $\mathbf{X}''_{av}$, i.e., the loss functions are $Loss_2 = |\mathbf{X}'_{av}-\mathbf{X}_{av}|$ and $Loss_3 = |\mathbf{X}''_{av}-\mathbf{X}_{av}|$.
To generate the importance scores correctly, the Importance Score Transformer is trained in an adversary way. Here, we explain our design in terms of a GAN in order to make it understandable. Consider $\mathbf{H}_{av}$ as the true "image", $\mathbf{H}_{bv}$ as the fake "image", Decoder \#1 as the discriminator, and Decoder \#2 as the generator. In the first several epochs, the weight of $Loss_2$ is far larger than the weight of $Loss_3$. As a result, Decoder \#1 discriminates $\mathbf{H}_{bv}$ and $\mathbf{H}_{av}$ better by assigning $\mathbf{H}_{av}$ larger attentive weights in $\text{TTrans}()$. As the weight of $Loss_3$ increases, Decoder \#2 is trained to "generate" a new $\mathbf{H}_{bv}$ by concatenating it with $\mathbf{I}_{bv} \approx \mathbf{H}_{bv}-\mathbf{H}_{av}$. The new $\mathbf{H}_{bv}$ -- $\mathbf{H}'_{bv}$ recieves more attention in $\text{TTrans}()$ with the importance score and thus enlarge $Loss_2$. This way, as the weight of $Loss_3$ grows, the attentive weights in $\text{TTrans}()$ finally stabilize at some point that slightly inclines to $\mathbf{H}_{bv}$, which makes Decoder \#1 produce a larger importance score for $\mathbf{H}_{av}$ and a smaller score for $\mathbf{H}_{bv}$.


The loss during training can be written as follows:
\begin{equation} \label{eq:loss}
  Loss = \psi Loss_1 + \omega Loss_2 + (1-\omega) Loss_3
\end{equation} 
where $\psi$ is the weight of the prediction loss. $\omega \in (0, 1]$ is the weight of the reconstruction loss of
Decoder \#1, while $(1-\omega)$ is the weight of the reconstruction loss of Decoder \#2. $\omega$ is initialized as $1$ and decreases as the number of epochs increases. 
