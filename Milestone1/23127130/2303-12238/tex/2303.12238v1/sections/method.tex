\section{Methodology}
\label{sec:method} 
As shown in Figure~\ref{fig:architecture}, the proposed architecture mainly
contains two parts. The anchored spatial transformer (S-Transformer) fuses
spatial information among sensors for each timestamp. The temporal
transformer(T-Transformer) combines temporal information for each sensor and
gives a higher ``importance score'' to sensors experiencing sudden feature
changes after the validation time.
\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/architecture1.jpg}
  \caption{\textbf{The architecture of DG-Trans.} S-Transformer first encodes
   the input tensor along the spatial axis for each timestamp. Then, the
   encoded features $\mathcal{H}_{ST}$ are fed to the T-Transformer with
   importance scores initialized as 0. The output $\mathcal{H}$ of
   T-Transformer is split into $\mathcal{H}_{bv}$ and $\mathcal{H}_{av}$ and
   sent to Decoder \#1, which contains a self-attention layer and a
   mutual-attention layer. In Decoder \#1, the importance score is computed
   as $\mathcal{H}-\mathcal{H}'_{av}$. With the updated importance score,
   $\mathcal{H}_{ST}$ is fed to T-Transformer again. The output is(1) further
   processed by Decoder \#2 to be $\mathcal{H}''_{av}$, and (2) pooled to be
   the prediction of incident impact. The loss is computed as the weighted
   combination of (1) the reconstruction from $\mathcal{H}'_{av}$ to
   $\mathcal{X}_{av}$, (2) the reconstruction from $\mathcal{H}''_{av}$ to
   $\mathcal{X}_{av}$, and (3) the prediction loss of impact duration and
   length.} % describe the architecture
  \label{fig:architecture}
\end{figure*}

\subsection{Anchored Spatial Transformer} Anchor has been used to reduce the
 complexity of attention in many studies~\cite
 {chen2020iterative, baek2021accurate}. In our case, roads are considered
 anchors for sensors on them. Denote sensors as $S$ and roads as $R$. A
 vanilla self-attention exploring spatial relations between sensors can be
 summarized as:
\begin{equation}
  a^{ss}_{ij} = softmax(\frac{(\Q h_{s_j})^T(Kh_{s_i})}{\sqrt{d}})
\end{equation} 
where $s_i$ and $s_j$ are two different sensor nodes, $\Q$ and $K$ are query
and key projection parameters, $h_{s_i}$ and $h_{s_j}$ are embeddings of
$s_i$ and $s_j$, and $d$ is the dimension of each attention head. 

However, in S-Transformer, the attention is decomposed into three steps. The
first step is ``sensor-to-road'' attention, which computes the correlation
between sensors and roads and masks out the edge between $s_i$ and $r_j$ if
$s_i$ is not on $r_j$:
\begin{equation}
\label{eq:sr_attn}
\begin{aligned}
  &a^{sr}_{ij} = softmax(mask(\frac{(\Q^{sr}h_{r_j})^T(K^{sr}h_{s_i})}{\sqrt{d}},M^{sr}_{ij})),\\
  &mask(x, \lambda)=\begin{cases} x & \lambda = 1 \\
                     inf &  \lambda = 0
       \end{cases}
\end{aligned}
\end{equation}
where $s_i$ and $r_j$ represent an arbitrary sensor and a road. $\Q^{sr}$ and
$K^{sr}$ are query and key projection parameters. $h_{s_i}$ and $h_{r_j}$ are
embeddings of $s_i$ and $r_j$. $h_{r_j} \in \mathbb{R}^{T\times C}$ is a
learnable parameter matrix. $M^{sr} \in \{0, 1\}^{|S|\times|R|}$ is the
adjacent matrix between sensors and roads. $M^{sr}_{ij}=1$ if sensor $s_i$ is
on road $r_j$, otherwise $M^{sr}_{ij}=0$.

The second step is a ``road-to-road'' self-attention module, which intends to
extract road intersection information. The attention can be expressed as
follows:
\begin{equation}
  a^{rr}_{ij} = softmax(mask(\frac{(\Q^{rr}h_{r_j})^T(K^{rr}h_{r_i})}{\sqrt{d}},M^{rr}_{ij}))
\end{equation}
where $M^{rr} \in \{0, 1\}^{|R|\times|R|}$ is the adjacent matrix between
roads. $M^{rr}_{ij}=1$ if road $r_i$ intersects road $r_j$, otherwise $M^
{rr}_{ij}=0$. The last step is ``road-to-sensor'' attention, meaning that
sensor vectors are considered as queries and road vectors are keys and
values. The equation is basically the reversed version of Equation~\ref
{eq:sr_attn}:
\begin{equation}
  a^{rs}_{ij} = softmax(\frac{(\Q^{rs}h_{s_j})^T(K^{rs}h_{r_i})}{\sqrt{d}})
\end{equation}

Note that there is no mask in this step, which preserves the attention
mechanism's flexibility. Previous works usually link top-k closest sensors in
the geographic or feature space, weighting the links with the distances.
However, we observe that attention is good at learning weights while weak at
learning graph structures. In this case, we simply need to find nodes that
are definitely linked to each other and leave the weight learning task to
attention, which leads to the decision to control the graph structure with
unweighted adjacent matrices partially. 

Essentially, S-Transformer stresses the effects of sensors on
high-degree-centrality roads. The strength of the S-Transformer can be
summarized as follows: (1) it avoids the human error introduced by manually
choosing ``road-to-road'' for ``top-k'', (2) it allows long-range message
passing as all sensors on intersected roads are linked, (3) it preserves the
flexibility of attention with a relatively small number of edges
($|S|+|R|^2+|S||R|, |R|\ll |S|$ at most) (4) it is more time efficient
($\mathcal{O}(|S||R|+|R|^2+|S||R|), |R|\ll |S|$) than traditional attention
mechanism ($\mathcal{O}(|S|^2)$) and requires fewer layers (according to
(2)).

\subsection{Importance Score Transformer} As an incident usually affects only
 a relatively small part of the whole traffic network, equally treating all
 sensors in prediction may introduce unwanted noises. However, manually
 extracting sensors near the incident may lose long-range complex impact
 patterns and even the most representative features due to the early/delayed
 response of the traffic network. In this case, a method dynamically locating
 the region and time window affected by the incident is important. Based on
 the assumption that traffic measurements of sensors affected by incidents
 show more obvious changes than the others, we locate the incidents with
 anomaly detection techniques, \ie, assigning sensors with larger varies a
 higher ``importance score''. 

Inspired by the work of~\cite{tuli2022tranad}, our Importance Score
Transformer also contains three modules: a temporal transformer
(T-Transformer) and two decoders. The T-Transformer module encodes the output
of the S-Transformer with and without the importance score along the time
dimension. The first decoder computes the importance score and reconstructs
$\mathcal{X}_{av}$, and the second decoder reconstructs $\mathcal{X}_
{av}$ from the combination of the importance score and the graph embedding.
More specifically, assume the importance score is $I \in \mathbb{R}^
{|S|\times T \times C}$ and the output of S-Transformer as $\mathcal{H}_
{ST} \in \mathbb{R}^{|S|\times T \times C}$. The output of the T-Transformer
can be written as:
\begin{equation}
  \mathcal{H} = TTrans(\mathcal{H}_{ST} \oplus I_0)
\end{equation}
where, $TTrans()$ indicates T-Transformer. $I_0$ is the initialized importance
score (all zero).

The aims of Decoder \#1 and Decoder \#2 are to extract the relations and the
differences between the full time sequence and the sub-sequence after the
validation time. Each decoder has a self-attention layer and a
mutual-attention layer, where the former further blends the full sequence
embedding and the latter is for ``after-validation'' sub-sequence
reconstruction. Specifically, the full sequence $\mathcal{H}_{ST}$ is split
into the ``before-validation'' sub-sequence $\mathcal{H}_{bv}$ and the
``after-validation'' sub-sequence $\mathcal{H}_{av}$. The function of
Decoder \#1 can be written as:
\begin{equation}
  \mathcal{H}'_{av} = \text{mu-attn}_1(\text{self-attn}_1(\mathcal{H}, \mathcal{H}), \mathcal{H}_{av})
\end{equation}

Then, the importance score can be computed as follows:
\begin{equation} \label{eq:importance_score}
  I = \mathcal{H}_{ST} - \mathcal{H}'_{av}
\end{equation}

Note that $\mathcal{H}_{ST}$ has $T$ timestamps while $\mathcal{H}'_{av}$ has
$T_{av}$ timestamps. We examined different methods like repeating timestamps
in $\mathcal{H}'_{av}$ and getting mean/min/max of $\mathcal{H}'_{av}$ along
time axis. All the methods resulted in similar performance. In this case, we
adopt the general form as in Equation~\ref{eq:importance_score} to represent
the difference between $\mathcal{H}_{ST}$ and $\mathcal{H}'_{av}$.

The reconstruction of $\mathcal{X}_{av}$ is acquired from a linear
transformation of $\mathcal{H}'_{av}$:
\begin{equation}
  \mathcal{X}'_{av} = \text{Linear}_1(\mathcal{H}'_{av})
\end{equation}

We directly use the difference between $\mathcal{H}_{ST}$ and $\mathcal{H}'_
{av}$ as we want to distinguish the traffic patterns before and after the
incident occurrence and $\mathcal{H}_{bv} \geq \mathcal{H}'_{av}$ and
$\mathcal{H}_{av} \leq \mathcal{H}'_{av}$ based on the intuition of anomaly
detection. 

Replacing $I_0$ with $I$, we apply T-Transformer again with the concatenated
$\mathcal{H}_{ST}$ and $I$ as the input. 

\begin{equation}
\begin{aligned}
  &\mathcal{H}' = TTrans(\mathcal{H}_{ST} \oplus I)\\
  &\mathcal{H}''_{av} = \text{mu-attn}_2(\text{self-attn}_2(\mathcal{H}', \mathcal{H}'), \mathcal{H}_{av})\\
  &\mathcal{X}''_{av} = \text{Linear}_2(\mathcal{H}''_{av})
\end{aligned}
\end{equation}

\subsection{Pooling and Loss} After the encoding of the S-Transformer and two
 rounds of the T-Transformer, the spatiotemporal representation $\mathcal
 {H}'$ is considered well-learned and ready for prediction. Our pooling
 module contains a 2D convolution layer for temporal dimension aggregation, a
 SUMPooling for spatial dimension aggregation, and three linear layers with
 ReLu as activation for feature dimension. The process can be summarized as
 follows:
\begin{equation}
\begin{aligned}
  &\mathcal{H}'_T = Conv2d(\mathcal{H}'), \mathcal{H}'_T \in \mathbb{R}^{|S|\times C}\\
  &\mathcal{H}'_S = SUMPool(\mathcal{H}'_T), \mathcal{H}'_T \in \mathbb{R}^C)\\
  &\hat{Y} = \text{Linear}_{p3}(ReLu(\text{Linear}_{p2}(ReLu(\text{Linear}_{p1}(\mathcal{H}'_S))))), \\
  & \hat{Y}\in \mathbb{R}^{C_{out}}
\end{aligned}
\end{equation}
where $C_{out}=2$ is the number of output values. The first value represents
the impact duration, and the second is the impact length. SUMPooling is
chosen because it is the most expressive pooling method~\cite
{xu2018powerful}, and we want the pooled representation to be able to
represent all possible incident scenarios.

Mean absolute error (MSE) loss is chosen for both reconstruction and
prediction tasks. The loss during training can be written as:
\begin{equation} \label{eq:loss}
\begin{aligned}
  &Loss_1 = |\hat{Y}_{Dur}-Y_{Dur}|+|\hat{Y}_{Len}-Y_{Len}| \\
  &Loss_2 = |\mathcal{X}'_{av}-\mathcal{X}_{av}| \\
  &Loss_3 = |\mathcal{X}''_{av}-\mathcal{X}_{av}| \\
  &Loss = Loss_1 + \omega Loss_2 + (1-\omega) Loss_3
\end{aligned}
\end{equation} 
where $\omega \in (0, 1]$ is the weight of the reconstruction loss of the
Decoder \#1, while $(1-\omega)$ is the weight of the reconstruction loss of
the Decoder \#2. $\omega$ is initialized as $1$ and decreases as the number
of epochs increases. In the early stage, the T-Transformer is trained to
focus more on the ``after-validation'' part to minimize $Loss_2$. After a few
epochs,  the importance score of $\mathcal{H}_{av}$ would approach zero while
that of $\mathcal{H}_{bv}$ would approach $\mathcal{H}_{av}-\mathcal{H}_
{bv}$, which also contains the information of $\mathcal{H}_{av}$. With the
increment of $1-\omega$, Decoder \#2 starts to work. Different from
Decoder \#1, Decoder \#2 tries to reconstruct $\mathcal{X}_{av}$ from both
$\mathcal{H}_{ST}$ and $I$