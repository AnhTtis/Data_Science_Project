\documentclass{statsoc}

%\usepackage{amsmath}
%\usepackage{amsfonts}
%\usepackage{amsthm}
\usepackage[a4paper]{geometry}
\usepackage{amssymb}
%\usepackage{fullpage}
\usepackage{enumerate}
\usepackage{pgf,tikz,tikz-cd}
\usepackage{graphicx}
\usepackage{multirow}
%\usepackage{setspace}
\usepackage{listings}
%\usepackage{caption}
%\usepackage{subcaption}
\usepackage{epstopdf}
\usepackage{mathrsfs}
%\usepackage{cite}
\usepackage{natbib}
\usepackage[colorlinks=true,linkcolor=blue, allcolors=blue]{hyperref}
\usepackage{url}
\usepackage{algorithm}
\usepackage{ulem}
\usepackage[noend]{algpseudocode}
\usepackage{MnSymbol}
\usepackage{comment}

\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\GG}{\mathcal{G}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\BB}{\mathcal{B}}
\newcommand{\dd}{\,\mathrm{d}}
\newcommand{\II}{\mathbb{I}}

\newcommand{\iid}{\overset{iid}{\sim}}
\newcommand{\argmin}{\mathrm{argmin}}

\providecommand{\ip}[1]{\bds\left( #1 \right)\eds}
\DeclareMathOperator{\interior}{int}
\DeclareMathOperator{\exterior}{ext}
\newtheorem{thm}{Theorem}
\newtheorem{Def}[thm]{Definition}
\newtheorem{Cor}[thm]{Corollary}
\newtheorem{Conj}[thm]{Conjecture}
\newtheorem{cl}[thm]{Claim}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{eg}[thm]{Example}
\newtheorem{Ex}[thm]{Exercise}
\newtheorem{Lem}[thm]{Lemma}
\newtheorem{note}[thm]{Note}
\newtheorem{rem}[thm]{Remark}
\newcommand{\Gmax}{G_{\max}(f, \mathcal{G} )}



\usepackage{xcolor}
\def\cblu{\color{blue}}
\def\cred{\color{red}}
\def\cgre{\color{green}}

\setlength{\parindent}{0pt}
\numberwithin{equation}{section}
\numberwithin{thm}{section}


%\title[Short title]{Estimating Symmetries}
\title[Estimating Maximal Symmetries]{Estimating Maximal Symmetries of Regression Functions via Subgroup Lattices}

\author{Louis G. Christie}
\address{University of Cambridge,
			Cambridge,
			United Kingdom.}
\email{lgc26@cam.ac.uk}

\author[L. G. Christie \& J. A. D. Aston]{John A. D. Aston}
\address{University of Cambridge,
			Cambridge,
			United Kingdom.}
\email{j.aston@statslab.cam.ac.uk}


\begin{document}




\begin{abstract}
We present a method for estimating the maximal symmetry of a regression function. Knowledge of such a symmetry can be used to significantly improve modelling by removing the modes of variation resulting from the symmetries. Symmetry estimation is carried out using hypothesis testing for invariance strategically over the subgroup lattice of a search group $\mathcal{G}$ acting on the feature space. We show that the estimation of the unique maximal invariant subgroup of $\mathcal{G}$ can be achieved by testing on only a finite portion of the subgroup lattice when $G_{\max}$ is a compact connected subgroup of $\mathcal{G}$, even for infinite search groups and lattices (such as for the 3D rotation group $SO(3)$). We then show that the estimation is consistent when $\mathcal{G}$ is finite. We demonstrate the performance of this estimator in low dimensional simulations, on a synthetic image classification on MNIST data, and apply the methods to an application using satellite measurements of the earth's magnetic field.
\end{abstract}

\keywords{Invariant models, non-linear dimensionality reduction. }

\section{Introduction}

Many objects we wish to model in statistics obey symmetries. In biology, viral capsids can exhibit icosahedral symmetries, as shown in figure \ref{fig:protiens} below \citep{jiang2017atomic} and these can be integrated in Cryo-EM statistical reconstruction algorithms. Time series can be seasonal \citep{box2015time}, which is a form of discrete translation symmetry. Linear models can be invariant to one or more of specific features, which is equivalent to a continuous translation symmetry along that feature's axis as in figure \ref{fig:demo_alg}. \\
\\
In statistical shape analysis \citep{kendall1984shape, dryden2016statistical}, we consider collections of $k$ landmark points in $\RR^m$ (structured as a matrix $X \in \RR^{m \times k}$ with $m$ rows and $k$ columns) as equivalent if they can be rotated, scaled, and translated into each other, or equivalently invariant to the action of $\mathrm{Aff}(\RR^m)$. Similarly, directional statistics are formulated as real angles invariant to translations by $2 \pi$ radians \citep{mardia2009directional}. Another class of examples are found in imagery. Suppose we wish to classify images as either containing a face or not. This can be formulated as learning the indicator function $\mathbf{1}_{\text{Face}} : [0,1]^p \rightarrow \{ 0, 1 \}$ (here assuming that images contain $p$ greyscale pixels). It is often assumed that this function is invariant to reflections or rotations of the images, which can be realised as invariance to a subgroup of the symmetric group on $d$ elements, $Sym_d$, corresponding to permutations of the appropriate pixels.  \\
\\
\begin{figure}[h]
	\centering
	\begin{tabular}{cccc}
				\includegraphics[scale = 0.15]{figs/Sf6.jpg} &
				\includegraphics[scale = 0.15]{figs/HIV.jpg}  \\
				(a) Sf6 Capsid & (b) HIV-1 CA Capsid \\ 	 \\			
				\includegraphics[scale=0.085]{figs/Icosahedron.jpg} &
				\includegraphics[scale = 0.45]{figs/Sphere_symmetry_group_ih.png} \\(c) Regular Icosahedron & (d) Icosahedral Symmetries
	\end{tabular}

	\caption{Subfigures (a) and (b) show Cryo-EM maps of representative viruses (Figure 1 in \cite{jiang2017atomic}). Figures (c) shows an icosahedron - a platonic solid with the same symmetries as the viruses. Figure (d) shows the symmetries of the icosahedron, where any symmetry maps a node of the same type (clue circle, red triangle, or red rhombus) are mapped to the same type. The yellow triangle is a fundamental domain.  }
	\label{fig:protiens}
\end{figure}

In general, consider learning a square integrable regression function $f : \mathcal{X} \rightarrow \RR$ with some independent, identically distributed (iid) data $\mathcal{D} = \{ (X_i, Y_i) \}_{i = 1}^n $ with each $(X_i, Y_i) \in \mathcal{X} \times \RR$, $\EE( Y_i \mid X_i ) = f(X_i)$, and $\mathrm{Var}( Y_i \mid X_i) = \sigma^2 < \infty$. Importantly, suppose further that $f$ has an invariance property:
\begin{equation}
\PP( f( g \cdot X_i ) = f(X_i) ) = 1	
\end{equation}
for all $g \in G$ for some group $G$ with an action $\cdot : G \times \mathcal{X} \rightarrow \mathcal{X}$. We say that such an $f$ is \textbf{$G$-invariant}. \\
\\
There are three main methods for including the information of the symmetry in an estimator $\hat{f}$ of $f$. Firstly, if $G$ is finite we can use a new dataset $\mathcal{D}' = \cup_{g \in G} \{ ( g \cdot X_i, Y_i ) \}_{i  =1}^n $ (known in machine learning as \textit{data augmentation} \citep{shorten2019survey}, which is distinct from Bayesian data augmentation \citep{tanner1987calculation, van2001art}). Secondly, for either finite or infinite $G$ we can consider projecting datapoints (\textit{data projection}) into the quotient space $\mathcal{X} / G$. Lastly, we can project the estimate $\hat{f}$ into the subspace of $G$-invariant functions as $S_G \hat{f} = \EE ( \hat{f}( g \cdot x, \mathcal{D} ) \mid \mathcal{D})$ (where $g$ is a uniform\footnote{Uniform here means that the law of $g$ is the Haar measure \citep{haar1933massbegriff} on a locally compact group $G$.} random variable on a compact group $G$), known as \textit{feature averaging} \citep{elesedy2021provablyLin}. Shape statistics uses data projection where we have explicit projection maps, while the Cryo-EM images are improved by feature averaging. These methods and other symmetrisation techniques have been shown to be broadly applicable in many statistical and machine learning contexts; a good overview of these contexts is \citet{bronstein2021geometric}. \\
\\
Recent studies have quantified the benefit of these methods, such as in \citet{lyle2020benefits, bietti2021sample, elesedy2021provably}. In summary, one can prove that such symmetrisation techniques outperform the base estimator, even almost surely over the data. For example, in the case of the symmetrisation operator as
\begin{equation}
	\| f - S_G \hat{f} \|_2 = \| S_G f - S_G \hat{f} \|_2 \leq \| S_G \| \| f - \hat{f} \|_2 = \| f - \hat{f} \|_2
\end{equation}
using the $G$-invariance of $f$, which implies $f = S_G f$, and the fact that $S_G$ is a projection \citep{elesedy2021provablyLin} and thus has operator norm $1$. This can be seen as averaging out some modes of variation in the estimate $\hat{f}$ in a flexible non-linear manner, or by reducing the entropy of the function class we are estimating over. A key example of this would be averaging a function $f$ on $\RR^3$ with the symmetry of $SO(3)$ acting by rotations - in this case the function $S_{SO(3)} f$ varies only along the radial direction at any point $x \in \RR^3$ and is constant in the orthogonal directions. \\
\\
This of course relies heavily on the assumption that $f$ truly is $G$-invariant. Otherwise we create obvious problems: we will struggle to identify images of 6s and 9s if we assume that their classification functions are invariant to any rotations. More generally, if $\hat{f}$ is a consistent estimator of $f$ then $S_G \hat{f}$ converges in probability to $S_G f$. This means that inappropriately used symmetries (i.e., when $f$ is not $G$-invariant) can create asymptotic bias (of $S_Gf - f \neq 0$). Therefore, we want to use as much symmetry as possible without using too much. \\
\\
There have been a few efforts to develop methods that use the data to select appropriate symmetries, such as \citet{cubuk2019autoaugment, lim2019fast, benton2020learning}. These methods lose the group structure by estimating sub\textit{sets} of transformations rather than sub\textit{groups}. \\
\\
Thus in this paper we present a methods for establishing symmetries of an object, to be used when estimating it. In particular, we present a method for estimating the best symmetry to use from a large search group $\mathcal{G}$ of symmetries by structuring the subgroups in a lattice \citep{sankappanavar1981course}. We test the hypotheses $H_0 : f$ is $G$-invariant for each subgroup $G \leq \mathcal{G}$ in the chosen lattice. We also present two possible tests to use and the properties associated with these tests.  \\
\\
This method has a close analogue in variable subset selection \citep{mallows2000some, berk1978comparing}. Consider a linear regression $f : \RR^p \rightarrow \RR$ for which some features $i \in I$ have $\beta_i = 0$. We could estimate $I$ by considering subsets $A \subseteq \{ 1, \dots, p \}$ and test for whether $\beta_A = 0$ (where the set subscript indicates the vector $( \beta_a : a \in A)$). These subsets can also be structured in a lattice (see section \ref{ssec:lattices} and figure \ref{fig:demo_alg}) where the ordering is given by subsetting, and rejection at a node tells us about rejections higher in the lattice. Alternative algorithms from the variable selection literature (e.g. forward selection or backwards elimination) are discussed in section \ref{sec:con}.  \\
\\
In the symmetry estimation case, we have the same structure, but the lattices can be much larger and are usually infinite which allows us to capture non-linear combinations of the variables to select. In figure \ref{fig:demo_alg}, we consider the equivalence of the subset selection problem with translation invariance, showing that both problems search over similar lattices, though there are infinitely more subgroups than are shown in this sub-lattice. This generalises the notion of \citet{allen1974relationship} that variable selection is a limiting case of data augmentation\footnote{The finite sub-lattice of variable selection is contained within the lattice of all possible translation symmetries and so we can take a sequence of nodes in the translation lattice that approximates the variable selection nodes.}.  In contrast to variable selection, we show that we can still search over these infinite lattices by considering finite sub-lattices, which can still contain both finite and infinite subgroups of $\mathcal{G}$. This allows us to adaptively smooth our estimate $\hat{f}$ non-linearly, even reducing the dimension of the estimation problem.

\begin{figure}[h]
	\centering
	\begin{tabular}{ccc}
	
	\begin{tikzpicture}[scale = 1]
	\draw[fill] (0,0) circle (0.07cm) node[right]{$\varnothing$};		
	\draw[fill] (-2,2) circle (0.07cm) node[right]{$\{ 1 \}$};
	\draw[fill] (0,2) circle (0.07cm) node[right]{$\{ 2\}$};
	\draw[fill] (2,2) circle (0.07cm) node[right]{$\{ 3 \}$};
	\draw[fill] (-2,4) circle (0.07cm) node[right]{$\{ 1, 2 \}$};
	\draw[fill] (0,4) circle (0.07cm) node[right]{$\{ 1,3 \}$};
	\draw[fill] (2,4) circle (0.07cm) node[right]{$\{ 2,3  \}$};		
	\draw[fill] (0,6) circle (0.07cm) node[right]{$\{ 1,2 ,3 \}$};
	
	\draw (0,0) -- (-2,2);
	\draw (0,0) -- (0,2);
	\draw (0,0) -- (2,2);
	
	\draw (-2,2) -- (-2, 4);
	\draw (-2,2) -- (0, 4);
	\draw (0,2) -- (-2, 4);
	\draw (0,2) -- (2, 4);
	\draw (2,2) -- (0, 4);		
	\draw (2,2) -- (2, 4);
	
	\draw (-2,4) -- (0, 6);
	\draw (0,4) -- (0, 6);
	\draw (2,4) -- (0, 6);
	\end{tikzpicture}
	
	&
	
	$\phantom{00}$
	
	&
	\begin{tikzpicture}[scale = 1]
	\draw[fill] (0,0) circle (0.07cm) node[right]{$I$};
	\draw[fill] (-2,2) circle (0.07cm) node[right]{$\RR_1$};
	\draw[fill] (0,2) circle (0.07cm) node[right]{$\RR_2$};
	\draw[fill] (2,2) circle (0.07cm) node[right]{$\RR_3$};
	\draw[fill] (-2,4) circle (0.07cm) node[right]{$\langle \RR_1, \RR_2 \rangle $};
	\draw[fill] (0,4) circle (0.07cm) node[right]{$\langle \RR_1, \RR_3 \rangle$};
	\draw[fill] (2,4) circle (0.07cm) node[right]{$\langle \RR_2, \RR_3 \rangle$};		
	\draw[fill] (0,6) circle (0.07cm) node[right]{$\RR^3$};
	
	\draw (0,0) -- (-2,2);
	\draw (0,0) -- (0,2);
	\draw (0,0) -- (2,2);
	
	\draw (-2,2) -- (-2, 4);
	\draw (-2,2) -- (0, 4);
	\draw (0,2) -- (-2, 4);
	\draw (0,2) -- (2, 4);
	\draw (2,2) -- (0, 4);		
	\draw (2,2) -- (2, 4);
	
	\draw (-2,4) -- (0, 6);
	\draw (0,4) -- (0, 6);
	\draw (2,4) -- (0, 6);
	
	\draw[fill, gray] (0.8,1.6) circle (0.07cm) node[below right]{$\langle (0,1,1) \rangle$};
	\draw[fill, gray] (1.5,3) circle (0.07cm) node[below right]{$\langle (0,1/2,1/2) \rangle$};
	\draw[dashed, gray] (0,0) --(2,4);
	\end{tikzpicture} \\
	
	(a) Lattice of subsets $A \subseteq \{ 1, 2, 3 \}$. & & (b) Lattice of (some) subgroups of $\RR^p$.
		
	\end{tabular}
		




	\caption{Variable subset selection via a lattice search, and symmetry estimation via a lattice search. The symmetry here is the translation action of $\RR^3$ on $\RR^3$, and we have taken a finite sub-lattice of the lattice of closed subgroups $K(\RR^3)$ where $\RR_i = \{ (a_1, a_2, a_3 ) \in \RR^3 :  a_j = a_i \delta_{ij} \}$ translates along the $i^{th}$ axis. We have also included gray and dashed edges to other subgroups in $K( \RR^3 )$ such at $\langle (0,1,1) \rangle  = \{ (0, a ,a ) : a \in \ZZ \}$ and its supergroup $\langle (0,1/2, 1/2 ) \rangle$. }
	\label{fig:demo_alg}
\end{figure}

The paper is organised as follows. Section \ref{sec:background} covers essential background material and definitions (all in \textbf{bold face text}), particularly that of the subgroups lattice of the search group $\mathcal{G}$ and the rules of how invariance behaves over this lattice. In section  \ref{sec:lattice} we then describe the estimation algorithm that searches for $G_{\max}(f, \mathcal{G} )$ by testing the invariance of $f$ to certain nodes of the lattice. Section \ref{sec:testing} introduces two tests one can use in the estimation algorithm, each requiring different assumptions for the statistical problem. We then demonstrate the performance of the tests used, the symmetry estimator, and its use in non-parametric regression in simulated and synthetic situations in section \ref{sec:exp}. We then apply these methods to magnetospheric modelling in section \ref{sec:app}. Lastly, we conclude with a discussion of these methods in section \ref{sec:con}.


\section{Background}
\label{sec:background}




\subsection{Statistical Problem}
\label{ssec:stats}
 Let $(\mathcal{X}, d_\mathcal{X})$ be a metric space with associated probability space $(\mathcal{X}, \Sigma, \mu_X)$. Suppose we wish to estimate a \textbf{regression function} $f : \mathcal{X} \rightarrow \RR$, where $f$ is in some function class $\mathcal{F} \subseteq L^2(\mathcal{X})$. Suppose that we collect some iid \textbf{training data} $\mathcal{D} = \{ (X_i, Y_i) \}_{i =1}^n$ in $(\mathcal{X} \times \RR)^n$ where $X_i \iid \mu_X$ and with $\EE (Y_i \mid X_i = x) = f(x)$ and $\EE( (Y_i - f(X_i) )^2 ) = \sigma^2 < \infty$ for all $i \in \NN$. This nonparametric regression problem will be focus in this paper.  We write $\mathcal{D}_X = \{ X_i \}_{i = 1}^n$, with analogues for other random variables. We write $\hat{\mu}_X$ for the empirical distribution of $\mathcal{D}_X$. We will often consider functions that have bounded variation, for example, the $(\beta, L)$-H\"{o}lder class of $m = \lceil \beta \rceil - 1 $ times differentiable functions.



\subsection{$G$-invariant Functions}
\label{ssec:invs}

Let $f : \mathcal{X} \rightarrow \RR$ be a regression function and let $G$ be any group. We give a summary of the definition of a group and other relevant group theory results in appendix \ref{sec:groups}, or a good external reference is \citet{artin2018algebra}. We say that $f$ is \textbf{$G$-invariant} if $\PP( f(g \cdot X) = f( X) ) = 1$ for all $g \in G$. We say that a measure $\mu_X$ is \textbf{$G$-invariant} if for all $g \in G$ and measurable $A$, $ g \cdot A$ is measurable and $\mu_X( g \cdot A ) = \mu_X(A)$. The $G$-invariant square integrable functions form a subspace of $L^2(\mathcal{X})$ (as invariance is trivially preserved by additions and scalar multiplication). 
\begin{comment}
In non-parametric statistics we often consider functions that have bounded variation, for example the $(\beta, L)$-H\"{o}lder class of $m = \lceil \beta \rceil - 1 $ times differentiable functions for which
\begin{equation}
\bigg| \frac{\partial^{m } f }{\partial x_i {}^{m} } (x) - \frac{\partial^{m } f }{\partial x_i {}^{m} } (y) \bigg| \leq L d_{\mathcal{X}}( x, y )^{ \beta - m }	
\end{equation}
for all $x, y \in \mathcal{X}$ and $i \in \{ 1, \dots, d \}$ (here assuming $\mathcal{X} = \RR^d$ or a subset thereof). Many functions have variations that differ along each of the dimension, so this \textit{isotropic} bound is weaker over much of the space $\mathcal{X}$. This is sometimes generalised to an \textit{anisotropic} bound for which $L$ and $\beta$ can differ along each dimension of $\mathcal{X}$, {\cred CITE}, i.e.,
\begin{equation}
\bigg| \frac{\partial^{m_i } f }{\partial x_i {}^{m_i} } (x) - \frac{\partial^{m_i} f }{\partial x_i {}^{m_i} } (y) \bigg| \leq L_i d_{\mathcal{X}}( x, y )^{ \beta_i - m_i }	
\end{equation}
for all $x, y \in \mathcal{X}$ and each $i \in \{ 1, \dots, d \}$. Even more generally, $L$ and $\beta$ might differ even locally - at a location $x$ the $f$ may vary quickly along dimension $1$ but slowly along dimension $2$, whereas at another location $y$ we ,may have these reversed. A key example of this is the function $f : \RR^2 \rightarrow \RR$ given by $f(x) = \sin ( \| x \|_2 )$, where at $(0, \pi)$ $f$ varies only along the first dimension and is constant along the second, but at $( \pi , 0)$ $f$ varies only along the second dimension. This is shown in figure {\cred REF}. \\
\\
If we have to fit such a function into even the anisotropic class, we would have to take the maximum $L_i$ and minimum $\beta_i$  over all locations, removing any benfit of considering this anisotropic class. The notion of $G$-invariance allows us to create a local notion of anisotropy. If $G$ is a Lie group of dimension $q$ acting continuously on a manifold $\mathcal{X}$ of dimension $d$, for example the group of rotations $S^1$ acting on $\RR^2$, then the orbits $[x]_G = \{ g \cdot x : g \in G \}$ are $q$ dimensional sub-manifolds of $\mathcal{X}$ {\cred CITE}. Any $f$ that is $G$-invariant then can only vary along the $d - q$ orthogonal to the tangent space of the orbit. This means that $G$-invariant functions have locally reduced dimension, can the directions in which $f$ can vary at a given location can vary non-linearly over $\mathcal{X}$.

\begin{figure}[h]
	
	\begin{tikzpicture}
		
		
	\end{tikzpicture}
	\caption{ Modes of variance of an $S^1$ invariant function in $L^(\RR^3)$. }
	
\end{figure}
\end{comment}

\subsection{Subgroup Lattices}
\label{ssec:lattices}

Our methods in section \ref{sec:lattice} are based on the idea that the subgroups of a large ``search group'' form a lattice in much the same way that selecting variables in a linear regression problem does. In this section we make precise the lattice we will be searching over, and show that the estimation problem is well defined. Further definitions can be found in appendix \ref{app:lattice_theory}.  \\
\\
It is well known that the subgroups of a group $\mathcal{G}$ form a \textbf{lattice} $L(\mathcal{G})$, a partially ordered set with unique supremums (or \textbf{joins}, written $\vee$) given by $A \vee B = \langle A, B \rangle$ and infimums (or \textbf{meets}, written $\wedge$) given by $A \wedge B = A \cap B$ \citep{schmidt2011subgroup,sankappanavar1981course}. In the context of variable selection, we have another lattice, where the index subsets $A, B \subseteq \{ 1 , \dots, d \}$ have joins $A \vee B = A \cup B$ and meets $A \wedge B = A \cap B$. \\
\\
One important subset of $L(\mathcal{G})$ is the collection of closed subgroups, $K(\mathcal{G})$. These do not necessarily form a sub-lattice for arbitrary $\mathcal{G}$ as we can generate non closed subgroups with the original join operator $\langle G, H \rangle$ (consider two copies of $S^1$ generating a proper dense subgroup of $SO(3)$). Instead, we have to adapt it by taking a new join as $\overline{ \langle G, H \rangle}$, which we establish in the following proposition. In some cases, e.g., finite $\mathcal{G}$, this is equivalent to the original join.

\begin{prop}
	\label{prop:closed_sublattice}
	Let $\mathcal{G}$ be a topological group. The closed subgroups $K(\mathcal{G})$ form a lattice with partial ordering given by subgrouping, meets given intersection, and joins given by the closure of the group generated by the two subgroups.
\end{prop}

Unlike the variable selection problem, the lattice $K( \mathcal{G} )$ is often infinite both breadth-wise and depth-wise. Optimising over this lattice is thus usually intractable, so instead we take a well chosen finite collection of closed subgroups $A \subseteq K(\mathcal{G})$ and generate the  countable or finite lattice $\langle A \rangle$ of $K(\mathcal{G})$, which we can enumerate. Even though the sub-lattice $\langle A \rangle$ may be finite, it can still contain infinite groups (see Examples \ref{eg:circle} and \ref{eg:so3}).


\begin{eg}
\label{eg:dihedral}
Consider the group of symmetries of the square,
\begin{equation}
	D_4 = \{ I, R_h , R_v, R_/, R_\backslash, R_{\pi/2}, R_{\pi}, R_{3 \pi / 2} \}
\end{equation}
This group has eight proper subgroups. Five are order $2$ (four generated by the reflections and one generated by the order 2 rotation (labeled $\mathcal{G}_{1,3}$), one is cyclic of order 4 (generated by either of the order 4 rotations), and the other two are isomorphic to $C_2 \times C_2$ (generated by appropriate pairs of the reflections). These can be ordered into the lattice in Figure \ref{fig:hasse_D4}, represented as a Hasse diagram (where $A \leq B$ if there is an ascending edge from $A$ to $B$).
	\begin{figure}[h]
	\centering
	\begin{tikzpicture}[scale = 1.2]
		\draw[fill] (0,0) circle (0.07cm) node[below]{$I$};
		\draw[fill] (-4,2) circle (0.07cm) node[below left]{$\langle R_h \rangle  $};
		\draw[fill] (-2,2) circle (0.07cm) node[below left]{$\langle R_v \rangle$};
		\draw[fill] (0,2) circle (0.07cm) node[below right]{$\langle R_{\pi} \rangle $};
		\draw[fill] (2,2) circle (0.07cm) node[below right]{$ \langle R_{/} \rangle  $};
		\draw[fill] (4,2) circle (0.07cm) node[below right]{$\langle R_{\backslash} \rangle  $};
		\draw[fill] (-3,4) circle (0.07cm) node[above left]{$\langle R_h, R_\pi \rangle$};
		\draw[fill] (0,4) circle (0.07cm) node[above right]{$\langle R_{\pi/2} \rangle $};
		\draw[fill] (3,4) circle (0.07cm) node[above right]{$\langle R_/, R_\pi \rangle$};
		\draw[fill] (0,6) circle (0.07cm) node[above]{$ D_4 $};
		\draw (0,0) -- (-4,2) -- (-3,4);
		\draw (0,0) -- (-2,2) -- (-3,4) -- (0,6);
		\draw (0,0) -- (0,2) -- (-3,4);
		\draw (0,0) -- (2,2) -- (3,4) -- (0,6);
		\draw (0,0) -- (4,2) -- (3,4);
		\draw (0,2) -- (0,4) -- (0,6);
		\draw (0,2) -- (3,4);
	\end{tikzpicture}
	\caption{Hasse diagram of the subgroup lattice of $D_4$, the dihedral group of symmetries of a square. Each dot represents a subgroup of $\mathcal{G} = D_4$, with edge from $A$ up to $B$ if $A \leq B$. All finite groups are closed and so this is also the lattice of closed subgroups of $D_4$}
	\label{fig:hasse_D4}
	\end{figure}
\end{eg}



\begin{eg}
	\label{eg:circle}
	Consider the group $S^1 = \RR / \ZZ$. The finite subgroups are those of rational rotations, and are of the form $ C_a = \langle [1/a]_\ZZ \rangle $ of order $a$. These have subgroups corresponding to the factors of $a$, and so the lowest parts of the subgroup lattice of $S^1$ are the groups of prime order. Above these you have groups with two (counting with repetition) prime factors, and so on. There are also subgroups of countable infinite order, for example $\langle [1/ \sqrt{2}]_\ZZ \rangle$, and one subgroup of uncountable order - itself. Importantly, these infinite subgroups are all dense in $S^1$ (under the topology inherited from $\RR$). The lattices $L(S^1)$ and $K(S^1)$ are depicted in figure \ref{fig:circle_hasse}.
	
	\begin{figure}[h]
	\centering
	\begin{tikzpicture}
		\draw[fill] (0,0) circle (0.07cm) node[below right]{$\mathcal{G}_{0,1} = I$};
		\draw[fill] (-4,2) circle (0.07cm) node[below left]{$C_2 = \langle [1/2]_{S^1} \rangle$};
		\draw[fill] (-2,2) circle (0.07cm) node[below left]{$C_3 $};
		\draw[fill] (0,2) circle (0.07cm) node[below right]{$C_5 $};
		\draw[fill] (2,2) circle (0.07cm) node[below right]{$C_7 $};
		\draw[fill] (4,2) circle (0.07cm) node[below right]{$C_p $};
		\draw[fill] (-4,4) circle (0.07cm) node[below left]{$C_4 $};
		\draw[fill] (-2,4) circle (0.07cm) node[below left]{$C_9 $};
		\draw[fill] (0,4) circle (0.07cm) node[below right]{$C_{25} $};
		\draw[fill] (2,4) circle (0.07cm) node[below right]{$C_{49} $};
		\draw[fill] (4,4) circle (0.07cm) node[below right]{$C_{p^2}$};	
		\draw[fill] (-3,3) circle (0.07cm) node[left]{$C_6$};
		\draw[fill] (-1,3) circle (0.07cm) node[left]{$C_{15}$};
		\draw[fill] (1,3) circle (0.07cm) node[left]{$C_{35}$};
		\draw (0,0) -- (-4,2) -- (-4,4);
		\draw (0,0) -- (-2,2) -- (-2,4);
		\draw (0,0) -- (0,2) -- (0,4);
		\draw (0,0) -- (2,2) -- (2,4);
		\draw (0,0) -- (4,2) -- (4,4);
		\draw (-4,2) -- (-3,3) -- (-2,2);
		\draw (-2,2) -- (-1,3) -- (0,2);
		\draw (0,2) -- (1,3) -- (2,2);
		\draw[dashed] (-4,4) -- (-4,5);
		\draw[dashed] (-2,4) -- (-2,5);
		\draw[dashed] (0,4) -- (0,5);
		\draw[dashed] (2,4) -- (2,5);
		\draw[dashed] (4,4) -- (4,5);
		\draw[dashed] (-3,3) -- (-3,5);
		\draw[dashed] (-1,3) -- (-1,5);
		\draw[dashed] (1,3) -- (1,5);
		
		\draw[dotted, thick] (2.5,3) -- (3.5,3);
		\draw[fill] (0,9) circle (0.07cm) node[right]{$\mathcal{G} = S^1$};
		\draw[fill, blue] (-2,7) circle (0.07cm) node[above left]{$\langle [1/ \sqrt{2}]_{S^1} \rangle $};
		\draw[fill, blue] (0,7) circle (0.07cm) node[above left]{$\langle [1/ \sqrt{3}]_{S^1} \rangle $};
		\draw[fill, blue] (2,7) circle (0.07cm) node[above right]{$\langle [1/ \sqrt{5}]_{S^1} \rangle $};		
		\draw[dashed] (0,9) -- (-2,7);
		\draw[dashed] (0,9) -- (0,7);
		\draw[dashed] (0,9) -- (2,7);
		\draw[dashed] (-2,7) -- (-3,6);
		\draw[dashed] (-2,7) -- (-2,6);
		\draw[dashed] (2,7) -- (3,6);
		\draw[dashed] (2,7) -- (2,6);
		\draw[dashed] (0,7) -- (-0.5,6);
		\draw[dashed] (0,7) -- (0.5,6);		
		\draw[thick] (-6,5.5) -- (6,5.5);
		\draw[->](5,6) -- (5,8.5) node[ midway, right] {Infinite} ;
		\draw[->](5,5) -- (5,0.5) node[ midway, right] {Finite} ;		
		
	\end{tikzpicture}
	\caption{Hasse diagram of the subgroup lattice $L(S^1)$, the group of rotational symmetries around a fixed axis. Dashed lines indicate chains of subgroups. Note that the countably infinite dense subgroups (such as $\langle [1  \sqrt{2} ]_{S^1} \rangle$) have no finite subgroups, only subgroups that are themselves countably infinite (such as $\langle [ 2 / \sqrt{2} ]_{S^1} \rangle \leq \langle [1  \sqrt{2} ]_{S^1} \rangle$). The lattice of closed subgroups $K(S^1)$ is the same but with the only infinite group begin $S^1$ itself, i.e., without the blue nodes.}
	\label{fig:circle_hasse}
	\end{figure}
	
\end{eg}


\begin{eg}
\label{eg:so3}
Consider the group $SO(3)$ of 3D Rotations. 	This group has 4 types of subgroups: (1) dense subgroups; (2) subgroups isomorphic to $S^1$ or $O(2)$ or its dense subgroups; (3) finite subgroups that fix a plane (isomorphic to $C_n$ or $D_n$); (4) exceptional finite subgroups corresponding to symmetries of the platonic solids. Thus its subgroup lattice can be thought of an infinite collection of copies of $S^1$ and its lattices (with the copies indexed by $S^1$) and all the lattices supremums and infimums (such as the copies of $O(2)$ generated by some $S^1$ and a rotation of $\pi$ radians about an orthogonal axis). Some of the subgroups that are dense in $SO(3)$ can be reached as the supremum of two non commutating high (finite) order elements (which themselves are in finite subgroups of some $S^1_u$). The exceptional tetra-, octa-, and icosa- hedral symmetry groups are also generated from two subgroups. For example, the icosa-hedral group is generated from two groups of rotations $\langle S \rangle$ and $\langle T \rangle$ of order 2 and 3 respectively, chosen such that $\langle ST \rangle$ is of order $5$. \\
\\
This means that the closed finite subgroups of $SO(3)$ either fix a plane or have order less than $60$. So if we take two rotations of order 11 around axes that correspond to neighbouring vertices of an icosahedron they must generate a group that has order greater than 60 but which doesn't fix a plane, and so is dense in $SO(3)$. Therefore, we can take a finite sub-lattice of $K(SO(3))$ as depicted in figure \ref{fig:SO3_finite_lattice}.
	
	\begin{figure}
		\begin{center}
			\begin{tikzpicture}
			\draw[fill] (0,0) circle (0.07cm) node[below]{$I$};
			\draw[fill] (-5,3) circle (0.07cm) node[left]{$\langle R^{u_1}_{2\pi / 11} \rangle$};
			\draw[fill] (-3,3) circle (0.07cm) node[left]{$\langle R^{u_2}_{2\pi / 11} \rangle$};
			\draw[fill] (-1,3) circle (0.07cm) node[left]{$\langle R^{u_3}_{2\pi / 11} \rangle$};
			\draw[fill] (1,3) circle (0.07cm) node[right]{$\langle R^{u_4}_{2\pi / 11} \rangle$};
			\draw[fill] (3,3) circle (0.07cm) node[right]{$\langle R^{u_5}_{2\pi / 11} \rangle$};
			\draw[fill] (5,3) circle (0.07cm) node[right]{$\langle R^{u_6}_{2\pi / 11} \rangle$};
			\draw[fill] (0,6) circle (0.07cm) node[above]{$SO(3)$};
			\draw (0,0) -- (-5,3) -- (0,6);
			\draw (0,0) -- (-3,3) -- (0,6);
			\draw (0,0) -- (-1,3) -- (0,6);
			\draw (0,0) -- (1,3) -- (0,6);
			\draw (0,0) -- (3,3) -- (0,6);
			\draw (0,0) -- (5,3) -- (0,6);
			\end{tikzpicture}
		\end{center}
		\caption{A finite sub-lattice of $K(SO(3))$ that uses rotations around the axes of an icosahedron (represented by unit vectors $u_1, \dots, u_6$). }
		\label{fig:SO3_finite_lattice}
	\end{figure}
	
	
\end{eg}

Given a chosen sub-lattice $K$ of $K(\mathcal{G})$, the following proposition shows that there is a well defined maximal object that we can look to estimate from the data.


\begin{prop}
	\label{prop:sub_lattice_restriction}
	Suppose that $\mathcal{G}$ acts faithfully and continuously on $\mathcal{X}$. Let $K$ be a finite sub-lattice of $K(\mathcal{G})$. Then there exists a unique node $G_{\max}(f, K )$ in $K$ that $f$ is invariant to, and for which $H \leq G$ for all $H \in K$ for which $f$ is $H$-invariant. Moreover, $G_{\max}(f, K) \leq G_{\max}(f, \mathcal{G})$.
\end{prop}

Ideally this object would capture much of the symmetry applicable to $f$ from within the search group $\mathcal{G}$, however the lattice $K$ does need to be well chosen. For example, if the lattice consisted of only the trivial group $I$ and $\mathcal{G}$, then our only options are total $\mathcal{G}$-invariance or no invariance at all. We discuss strategies for choosing $K$ in section \ref{ssec:choose_lattice}.



%Next, consider taking a sequence of increasing nested finite lattices $K_n$ from $K(\mathcal{G})$. We would like $G_{\max}(f, K)$ to converge in some sense to $\Gmax$, but this is not always guaranteed as shown in example \ref{eg:circle_approx}.
%
%\begin{eg}
%\label{eg:circle_approx}
%Consider the group $S^1 \cong \{ z \in \CC : |z| = 1 \}$ and its lattice of closed subgroups $K(S^1)$. Consider the finite sub-lattices $K_n$ generated by the groups $\{ \langle \exp( 2 i \pi / p ) \rangle : p \in \{ 1, \dots, n \} \}$. These lattices contains only finite subgroups, the maximal element in $K_n$ being the group $\langle \exp( 2 i \pi / \mathrm{lcm}( \{ 1, \dots, n \} ) ) \rangle$. Thus if $\Gmax = S^1$ then we will always have $G_{\max}(f, K_n) = \max{ K_n }$ which is never the group $S^1$. We could instead consider adding $S^1$ to each lattice $K_n$. Each $K_n \cup \{ S^1 \}$ is still a lattice, and each every $K_n$ contains $\Gmax$.
%\end{eg}
%
%This means that the lattices does have to be well chosen. We will discuss good choices of the lattice $K$ to use in section \ref{ssec:choose_lattice}.



\section{Estimation of $\Gmax$}
\label{sec:lattice}

Suppose we have collected data $\mathcal{D}$ as in the statistical problem defined in section \ref{ssec:stats} and suppose $\mathcal{G}$ is a group acting faithfully and continuously on $\mathcal{X}$. By proposition \ref{prop:max_subgroup}, there is a unique closed maximal invariant subgroup $G_{\max}(f, \mathcal{G})$ that we wish to estimate.  \\




%
%Consider the rules of $G$-invariance over a subgroup lattice. Lemma \ref{lem:generate_inv} and Lemma \ref{lem:lattice_inv} give four such rules:
%\begin{enumerate}[$\phantom{00}$(R1)]
%	\item If $f$ is $G$ invariant, then $f$ is $H$-invariant for all $H \leq G$;
%	\item If $f$ is not $G$ invariant, then $f$ is not $H$-invariant for all $H \geq G$;
%	\item If $f$ is both $G$ and $H$ invariant, then $f$ is $\langle G, H \rangle$ invariant; and
%	\item If $f$ is $G$ invariant but not $H$ invariant, then $f$ is not $G'$ invariant for any $G'$ with $\langle G' , H \rangle  = \langle G, H \rangle$.
%\end{enumerate}
%
%Collectively these meant that we can test at a few locations to tell us about large parts of the lattice. Rejections tell us about what's above, and acceptances about what's below. We can leverage these results to extend a hypothesis test from a small subgroup of a large search group $\mathcal{G}$ to tell us about many other subgroups. We formalise this as follows. \\
%\\
Let $\mathrm{Test}_{\alpha} : (\mathcal{D} , G) \mapsto \{ -1, 1\}$ be a hypothesis test for $H_0: f$ is $G$-invariant that takes $( \mathcal{D}, G)$ to $-1$ if rejected and $1$ if accepted at the significance level $\alpha$. We will provide two possibilities for $\mathrm{Test}_\alpha$ in the next section. One could brute force a search for $G_{\max}(f, \mathcal{G})$ by testing every subgroup $G \leq \mathcal{G}$, and taking $\hat{G} = \langle G \leq \mathcal{G} : Test_{\alpha}( \mathcal{D}, G ) = 1 \rangle$. Unfortunately this is inefficient at best (for example, with finite $\mathcal{G}$) and uncomputable at worst (when $\mathcal{G}$ is infinite). \\
\\
Instead we utilise the rules for invariance above to reduce the problem. Together, these allow us to use the information about strategically chosen tests to estimate the test results at other subgroups. We first described a simple search over a finite search group $\mathcal{G}$, and then a more complicated algorithm for when $\mathcal{G}$ is a compact connected group.


\subsection{Estimating Over a Finite Lattice}
Let $K$ be a finite sub-lattice of $K(\mathcal{G})$ that contains the trivial group $I$.
Consider the following algorithm for eliminating subgroups $G \leq \mathcal{G}$ based a the hypothesis test $\mathrm{Test}_\alpha$. In essence, we test from the bottom up, testing each $G \in \mathcal{G}_i$ and eliminating all $H \geq G$ that the test rejects $H_0 : f$ is $G$-invariant at significance level $\alpha$. This is effectively a breadth first search over the lattice, formalised in algorithm \ref{algo:bfe}. Let $m$ be the height of the lattice $K$ - the maximal length of a chain of distinct subgroups between $I$ and $\max K$ - and let $N_i$ be the number of subgroups at height $i$ in $K$. $K$ can then be enumerated as noted in section \ref{ssec:lattices}.

\begin{algorithm}[H]
\caption{Breadth First Estimation}
\label{algo:bfe}
\begin{algorithmic}[1]
	\Procedure{BreadthFirstEstimation}{$\mathcal{D}$, $K$, $\mathrm{Test}_{\alpha}$}
%	\State $L \leftarrow \mathcal{G}_0 \cup \cdots \cup \mathcal{G}_m$ \Comment{Lattice of $\mathcal{G}$}
	\For{$i \in \{ 1, \dots, m \}$}
		\For{$j \in \{1, \dots N_i \}$}
			\If{ $\mathrm{Test}_{\alpha} ( \mathcal{D}, \mathcal{G}_{i,j} ) = -1$ } Delete all $G \geq \mathcal{G}_{i,j}$ from $K$
			\EndIf
		\EndFor
	\EndFor
	\State \Return $\tilde{G}_B = \max K$.
	\EndProcedure
\end{algorithmic}	
\end{algorithm}

Note that there may not be a unique maximal element remaining in $K$, so as given $\tilde{G}_B$ is set valued, containing all maxima. We can then choose a single estimate $\hat{G}_B$ to use by picking from this set uniformly at random, or being more conservative and choosing the meet of all elements of $\tilde{G}_B$, 
\begin{equation}
	\hat{G}_B^{Alt} = \bigvee_{G \in \tilde{G}_B} G = \bigcap_{G \in \tilde{G}_B} G
\end{equation}
We can create a greedy version of algorithm \ref{algo:bfe}. Because of lemma \ref{lem:generate_inv}, confidence in subgroups $H_1, \dots, H_k \in K$ gives confidence in $\langle H_i : i \in [k] \rangle$. However we still need to be careful; as direct refinements, i.e., vertical lines in $K$, are not generated by just the subgroups in the level below. Thusm we can assume an acceptance of $G \in \mathcal{G}_i$ if it is generated by two or more subgroups in level $\mathcal{G}_{i-1}$ that were each not rejected, and call such an amendment algorithm \ref{algo:bfe}a. This does lose some specificity because we do not also test at the higher nodes (where we might find a true rejection).

\begin{eg}
If we are optimising over the lattice of $D_4$ from example \ref{eg:dihedral}, and $\mathcal{G}_{1,j}$ pass the hypothesis test for $j \in \{1,2,3\}$, algorithm \ref{algo:bfe}a will automatically assign an acceptance to $\mathcal{G}_{2,1}$ but not $\mathcal{G}_{2,2}$.
\end{eg}


Alternatively, we can look to a depth first search. This is justified by the fact that if $f$ is $G$-invariant then $G_{\max} \geq G$, and so if $\mathrm{Test}_{\alpha}(\mathcal{D}, G) =  1$ then $G_{\max}$ will be in the portion of the lattice above $G$ with probability at least the power of $\mathrm{Test}_{\alpha}$. If $G \leq \mathcal{G}$ then define $K_G = \{ H \in K : G \leq H \}$ as the \textbf{sub-lattice above $G$}. This is the recursive algorithm \ref{algo:dfe}.

\begin{algorithm}[H]
\caption{Depth First Estimation}
\label{algo:dfe}
\begin{algorithmic}[1]
\Procedure{DepthFirstEstimation}{$\mathcal{D}$, $L$, $\mathrm{Test}_\alpha$, $i$}
	\For{$j \in [N_i]$}
		\If{$\mathrm{Test}_{\alpha} ( \mathcal{D}, \mathcal{G}_{i,j} ) = 1$}  \Return DepthFirstEstimation($\mathcal{D}$, $L_{\mathcal{G}_{i,j}}$, $\hat{f}$, $\mathrm{Test}_\alpha$, $i + 1$)
		\EndIf
	\EndFor
	\State \Return $\hat{G}_D = \min L$
\EndProcedure
\end{algorithmic}	
\end{algorithm}

Note that at each call in the recursion we are guaranteed to increase the level by 1 so this will terminate in finite time for finite $\mathcal{G}$. This algorithm is more susceptible to error with a low power test, at it relies on acceptance rather than rejection checks.



%\subsection{Estimating Infinite Group Actions}
%
%Suppose that $\mathcal{G}$ is a compact connected Lie group acting continuously on $\mathcal{X}$. This means that there certainly exist finite order subgroups (as each element is contained in a maximal torus and these have finite subgroups), which form the base of the Lattice $L(\mathcal{G})$. As noted in Lemma \ref{lem:generate_inv}, if $f$ is both $G$ and $H$ invariant then $f$ is $\langle G, H \rangle$ invariant. In the finite context this is useful in the greedy algorithm \ref{algo:bfe}a - slightly speeding up the search. In this context we gain a lot more: we can generate infinite groups from pairs of finite ones.
%
%\begin{eg}
%Consider the subgroups of $SO(3)$ given by $G = \langle R_1 \rangle$ and $H = \langle R_2 \rangle$ where $R_1$ and $R_2$ are rotations of order $100$ about distinct axis such that $R_1$ and $R_2$ do not commute. Then the group $\langle G, H \rangle$ is dense in $SO(3)$, and therefore has infinite order. To see this note that $| \langle R_1, R_2 \rangle | \geq |G| \geq 100$ and so if it were finite it must fix a plane in $\RR^3$ (as it is larger than the tetra-, octa-, and icosa-hedral symmetry groups, which are the only other options for finite subgroups of $SO(3)$), but this contradicts the fact that $R_1$ and $R_2$ rotate around distinct axes.
%\end{eg}
%
%This property means that if we implement a version of algorithm \ref{algo:bfe}a, we can use the tests in the finite portion of the lattice $L(\mathcal{G})$ to estimate even in the infinite portion. This somewhat relies on the non-commutativity of elements in $\mathcal{G}$. If $\mathcal{G}$ is abelian then we will only ever generate finite order subgroups this way. Fortunately we can still approximate the abelian $G_{\max}$ by a sequence of finite subgroups of increasing order (for example rotations of increasing order around the same axis).   \\
%\\
%This is progress, but still computationally difficult to work with - there are infinitely many dense subgroups of $\mathcal{G}$ that we would have to test at. However,  Lemma \ref{lem:closure_invariance} tells us that $G_{\max}$ is closed, so we can reduce the lattice by taking closures. This means that each of these dense subgroups becomes equivalent. Note that this requires us to take the join operation $\overline{ \langle G, H \rangle } $ as discussed in section {\cred REF}. \\
%\\
%The last concern is when $\mathcal{G}$ has uncountably many subgroups (e.g. the subgroups of $SO(3)$ isomorphic to $S^1$ but around each axis through $0$). If this is the case, one should take a sublattice that is generated by some finite collection of these subgroups. In the SO(3) example this could correspond to picking some $\epsilon$-net of $\{ u \in \RR^3 : \| u \|_2 = 1 \}$ and using the subgroups of rotations around these axes as the basis for a sublattice (seen later in example {\cred REF}). In algorithm \ref{algo:infinite_lattice_search} below, this corresponds to the choice of $L$. \\
%\\
%All together, this leads to algorithm \ref{algo:infinite_lattice_search}. Note here that $m$ and $N_i$ are still the height of the lattice and the width of level $i \in \{1, \dots, m \}$ respectively, though these are now (possibly) infinite ordinals. Note that the ``Flags'' vector defaults to $0$ for each node, and is updated with the value for tests as necessary. We then use the rules described in section \ref{ssec:lattices} to efficiently update the flags.
%
%\begin{algorithm}[H]
%\caption{Subgroup Lattice Search}
%\label{algo:infinite_lattice_search}
%\begin{algorithmic}[1]
%	\Procedure{FullLatticeSearch}{$\mathcal{D}$, $L$, $\mathrm{Test}_\alpha$, $\mathrm{Flags} = 0^L$}
%%	\State $\mathrm{Flags} \leftarrow  \{ 0 \} ^L$ \Comment{Creates a variable with value $0$ for each node}
%	\State $(i,j) \leftarrow (1,1)$
%	\While{$i \leq m$}
%		\While{$j \leq N_i$}
%			\If{ $\mathrm{Flags}[G_{i,j}] = 0$ }
%				\State $\mathrm{Flags}[ \mathcal{G}_{i, j}] \leftarrow \mathrm{Test}_{\alpha}( \mathcal{D}, \mathcal{G}_{i, j} )$
%				\If{ $\mathrm{Flags}[ \mathcal{G}_{i, j}] = 1$}
%					\For{ $G \in \{ G \in L : \mathrm{Flags}[G] = 1 \} $}
%						\State $(k, l) \leftarrow \mathrm{FindIndex} ( \overline{ \langle G, \mathcal{G}_{i, j} \rangle }  ) $
%						\If{ $\mathrm{Flags}[ \mathcal{G}_{k,l} ] > -1$ } \Comment{Don't overwrite a rejection}
%							\State $\mathrm{Flags}[ \mathcal{G}_{k,l} ] \leftarrow 1 $
%							\State \Return FullLatticeSearch$( \mathcal{D}, L_{\mathcal{G}_{k,l} }, \mathrm{Test}_{\alpha}, \mathrm{Flags} \mid_{L_{\mathcal{G}_{k,l} }}) $  							
%						\EndIf
%					\EndFor
%				\Else
%					\For{ $G \in \{ G \in L : G \geq \mathcal{G}_{i,j} \} $}
%						\State $\mathrm{Flags}[G ] \leftarrow -1 $
%					\EndFor 	
%				\EndIf
%			\EndIf
%			\State $j \leftarrow j + 1$
%		\EndWhile
%		\State $i \leftarrow i + 1$
%	\EndWhile
%	\State \Return $\hat{G} = \max L$.
%\EndProcedure
%\end{algorithmic}	
%\end{algorithm}
%
%The key here is that if we can jump up a layer by combining two subgroups - we do, by calling the search again at the higher node. This relies somewhat on the ability to write the ``FindIndex'' method, which depends a lot on how the group $\mathcal{G}$ and lattice $L$ are represented computationally. In many cases (such as with $SO(3)$) we can do this explicitly without too much cost, but it does depend on the group at hand.

\subsection{Choosing the Lattice $K$}
\label{ssec:choose_lattice}

The choice of $K$ will greatly affect the usefulness of this method. If we pick a trivial sub-lattice of only the identity subgroup then we are guaranteed to estimate $\hat{G} = I$. A suggested strategy is to take a finite number of closed subgroups that give some level of coverage of $\mathcal{G}$ (if it is metrisable, as with a Lie group, then this could be the groups generated by an $\epsilon$-net of $\mathcal{G}$) and then generate a lattice from them by taking meets and joins. By the following Lemma we can then add in supergroups (of the entire lattice) to ensure that we see infinite subgroups of $\mathcal{G}$ if we choose.

\begin{Lem}
\label{lem:lattice_adding}
If $K$ is a sub-lattice of $K(\mathcal{G})$ and $H \leq G \leq \mathcal{G}$ for all $H \in K$, then $K \cup \{ G \}$ is also a sub-lattice of $K(\mathcal{G})$.
\end{Lem}




\subsection{Theoretical Properties of $\hat{G}$}

Let $P_f( n, \alpha )$ be a lower bound of the power of $\mathrm{Test}_{\alpha}$ applied to $f$ at the significance level $\alpha$ for sample size $n$, i.e., a bound such that $P_f( n, \alpha ) \leq \PP( \mathrm{Test}_\alpha ( \mathcal{D}, G ) = -1)$ for all $G \in K$ for which $f$ is not $G$-invariant. We first consider simple finite sample bounds, which are not uniform over $f$. These avoid problems of multiple testing by taking a very loose union bound, but which is still sufficient for our purposes. Note that here when $\tilde{G}_B$ is multiple valued we take $\hat{G}_B$ from this set uniformly at random.

\begin{Lem}
	\label{prop:hat_G_inv}
	The probability of $f$ being $\hat{G}$-invariant for both $\hat{G}_B$ and $\hat{G}_D$ is bounded below by $1 - |\mathcal{A}|( 1 - P_f( n, \alpha ))$ where $\mathcal{A} \subseteq K$ are subgroups above only subgroups of $G_{\max}$ and cover subgroups of $G_{\max}$.
\end{Lem}

Note that $|\mathcal{A}|$ is trivially bounded by $|K| < \infty$, but can be much smaller.



\begin{Lem}
	\label{prop:G_hat_leq_G_0}
	The probability that $\hat{G}_B = G_{\max}(f, K)$ is at least $1 - |\{ H \in K : H \leq G_{\max}(f, K) \}| \alpha  - |\mathcal{A}|( 1- P_f( n, \alpha ))$.
\end{Lem}

We can use these results to a sufficient condition for the consistency of $\hat{G}_B$ for $G_{\max} (f, K)$; namely the consistency of the test used and well chosen significance level used. We note here that the convergence in probability is with respect to the discrete topology on the finite set $K$.


\begin{prop}[Consistency of $\hat{G}_B$]
\label{prop:consistency_G_B}
If $P_f( n, \alpha ) \rightarrow 1$ for any $\alpha$ then there exists a deterministic sequence of significance levels $\alpha_n$ such that $\hat{G}_{B} \overset{p}{\rightarrow} G_{\max}$ when using $\mathrm{Test}_{\alpha_n}$ when searching over $K$ with $n$ samples.
\end{prop}






\subsection{Computational Complexity}

A full analysis of the complexity of these algorithms is impossible without specifying the test used. Here we give a heuristic for finite $\mathcal{G}$ that assumes the computational time that is proportional to $|G|$ for each test conducted.\footnote{This would correspond to using $m \propto |G|$ in the Asymmetric Variation Test, which is reasonable if one is sampling with a uniform $\mu_g$ on $G$.} The brute force approach would thus require a baseline of $\sum_{G \in K} |G|$ computational units, which we use for comparison to these algorithms. We then discuss how the test in the previous section can further improve on this. \\
\\
The finite breadth first estimator will have the same worst case runtime of $\sum_{G \in K} |G|$ computations, if $\mathcal{G}$ is $\mathcal{G}$-invariant (and so we test at every node in $K$). The greedy version is guaranteed to conclude faster unless $\mathcal{G} \simeq C_{p^n}$ is a cyclic group of prime power order: all other finite groups have at least two non nested subgroups and so we can remove at least one node (where those two groups meet). Usually both will cut out many other nodes through the deletion at a rejection. \\
\\
The finite depth first estimator will again require $\sum_{G \in L} |G|$ if $\mathcal{G}$ is cyclic of prime power order. Similarly, if $\mathcal{G} = \langle \mathcal{G}_{m-1,j} : j \in \{ 1, \dots, N_{m-1}\} \rangle $ and we happen to reject at all $\mathcal{G}_{m-1, j}$ for $j \in \{1 , \dots,  N_{m-1} - 1 \}$ while accepting $\mathcal{G}_{m-1, N_{m-1}}$ then we will still test at all nodes (even knowing that we will have to fail at $\mathcal{G}$). Again, with most other structures we can rule out at least one node, for instance if two consecutive layers have more than two nodes. \\
\\
If $\mathcal{G}$ admits particular structure then we can prove even stronger results. Here we write $\mathcal{G}$ as an additive group when it is abelian.

\begin{prop}
	\label{prop:comp_complex}
	Suppose $\mathcal{G} \simeq \oplus_{i= 1}^k C_{p_i^{a_i}} $ is an abelian group. Then the worst case performance of Algorithm \ref{algo:bfe}a saves at least $\sum_{b : |b|_0 > 1} \prod_{i = 1}^k p_i^{b_i}$ units of computation relative to brute force, where the sum ranges over multi-indices $b \in \prod_{i = 1}^k \FF_{a_i}$ with more than one non-zero element.
\end{prop}

The sum $\sum_b \prod_{i = 1}^k p_i^{b_i}$ can be bounded simply, as $\prod p_i \geq 2$ and there are $\prod a_i$ terms in this sum, giving

\begin{equation}
\sum_{b : |b|_0 > 1} \prod_{i = 1}^k p_i^{b_i} \geq 2 \prod_{i = 1}^k a_i
\end{equation}

Note that this doesn't account for subgroups of $\mathcal{G}$ not generated in this way so this bound is usually not tight. It only considers subgroups on a particular sub-lattice of $K$, but this is not all of $K$ as the following example shows.

\begin{eg}
Consider $\mathcal{G} = C_2 \times C_2 = \{ (0,0), (1,0), (0,1), (1,1) \}$. See that the subgroups $\langle (1,0) \rangle$, $\langle (0,1) \rangle$, and $\langle (1,1) \rangle$ are each isomorphic to $C_2$ but since $\langle (1,1) \rangle$ does not contain either $\langle e_1 \rangle$ nor $\langle e_2 \rangle$ as a subgroup it is not generated by them. Note that in this example algorithm \ref{algo:bfe}a will never have to test $\mathcal{G}$, saving at least $4 = \sum_{b : |b|_0 > 1} \prod_{i = 1}^k p_i^{b_i} > 2 (1 \times 1)$ units of computation.

\end{eg}







\section{Hypothesis Testing for a Particular Symmetry}
\label{sec:testing}

We now turn to constructing tests for the hypothesis $H_0 : f$ is $G$-invariant against $H_1 : f$ is not $G$-invariant for use in the subgroup estimation. We present two options for $\mathrm{Test}_{\alpha}$ that  make trade offs in terms of generality and power. The first requires stronger assumptions but has a theoretical guarantee of convergence, whereas the second makes far fewer assumptions but has slightly lower power and is slightly liberal.

\subsection{The Asymmetric Variation Test}
\label{ssec:asym_var_test}

Suppose that $\mathcal{F}$ is a class of bounded variation for which the practitioner assumes $f \in \mathcal{F}$, and let $V(x,y) = \sup_{f \in \mathcal{F} }  |f(x) - f(y) |$. An example of such a class are $\alpha$-H\"{o}lder continuous functions $\mathcal{F}(L, \alpha)$ with $|f(x) - f(y) | \leq L d_\mathcal{X}(x,y)^\alpha$ for $\alpha \in (0,1]$, for which $V(x,y) = L d_\mathcal{X}(x,y)^\alpha$. \\
\\
Let the independent mean zero additive noise $\epsilon_i = Y_i - f(X_i)$ be such that $ \PP( | \epsilon_i - \epsilon_j | > t ) \leq p_t$. An example of this would be $p_t = \frac{ 2 \sigma }{ t  } \tfrac{ \exp( - t^2 / 4 \sigma^2 )}{\sqrt{2\pi}}$ for iid gaussian $\epsilon_i$ with variance $\sigma^2$ (Proposition 2.1 of \citet{adamsMA3K0notes}). Let $g \sim \mu_g$ be any $G$ valued random variable, for any distribution $\mu_g$ on $G$. \\
\\
We would wish to construct a statistic that captures the $p$-value of $H_0$. If $f$ is $G$-invariant then
\begin{align}
\label{eq:like_bound}
	|Y_i - Y_j | &= | f(X_i) - f(X_j) + \epsilon_i - \epsilon_j | \\
		&= |f (g \cdot X_i) - f(X_j) + \epsilon_i - \epsilon_j | \\
		 &\leq |f(g \cdot X_i) - f(X_j)| + |\epsilon_i - \epsilon_j| \\
	&\leq V(g \cdot X_i, X_j) + |\epsilon_i - \epsilon_j|,
\end{align}
so we know that $D_{ij}^g = |Y_i - Y_j | - V( g \cdot X_i, X_j) \leq |\epsilon_i - \epsilon_j|$ for all $i,j \in \{1, \dots, n \}$ and $g \in G$. In particular, this is true for $j$ chosen such that $d( g \cdot X_i, X_j)$ is minimised. If we then count how often $D_{ij}^g$ is larger than some threshold $t \in \RR_{\geq 0
}$, we can bound the probability using the concentration of $| \epsilon_i - \epsilon_j |$. This leads to algorithm \ref{algo:known}. \\
\begin{algorithm}[h]
\caption{Asymmetric Variation Test}
\label{algo:known}
\begin{algorithmic}[1]
\Procedure{AsymVarTest}{$\mathcal{D}$, $V$, $\mu_g$, $t$, $p_t$, $m$}
	\For{$j \in \{1, \dots, m \}$}
		\State $g_j \leftarrow \mathrm{Sample}( \mu_g )$
		\State $I(j) \leftarrow \mathrm{Sample}( \{1, \dots, n \} )$
		\State $J(j) \leftarrow$ Index of Nearest Neighbour to  $g \cdot X_{I(j)}$ in $\{ X_j \}_{j = 1}^n$
		\State $D_{I(j) J(j)}^{g_j} \leftarrow | Y_{I(j)} - Y_{J(j)} | - V( g \cdot X_{I(j)}, X_{J(j)} )$
	\EndFor
	\State $N_t^g \leftarrow | \{ D_{I(j) J(j)}^{g_j} \geq t \} |$
	\State $p_{val} \leftarrow \sum_{k = N_t^g}^{m} \binom{m}{k} p_t^k (1 - p_t)^{m - k}$
	\State \Return $p_{val}$
\EndProcedure
\end{algorithmic}	
\end{algorithm}

For fixed $m$, the $D_{I(j) J(j)}^{g_j}$ are asymptotically independent (because it is vanishingly unlikely that we sample the same $g_j \cdot X_{I(j)}$ or that two $g_j \cdot X_{I(j)}$ share a nearest neighbour). Thus under the null hypothesis, $N_t^g$ is stochastically bounded by a $\mathrm{Binom}(m, p_t)$ variable, which allows us to bound the ``true'' $p$-value from above by the return value $p_{val}$. We can thus reject the null hypothesis of $G$-invariance at significant level $\alpha \in (0,1]$ if $p_{val} \leq \alpha$.






\subsubsection{Choices of $t$ and of $\mu_g$}

The methodology presented here works for any choice of $t$ and variable $g$, though particular choices of these will affect the power of the test. For example, if we choose $t$ such that $p_t \geq 1$ then our $p$-value will always be $1$. Similarly if $g = e \in G$ almost surely then we will also only reject with probability at most $\alpha$. \\
\\
For the choice of $t$, we suggest calculating $N_t^g$ from the sample of $D_{I(j)J(j)}^{g_j}$ at some grid of $t$ values $t_0 < t_1 < \cdots < t_k$ with the values of $p_{t_i}$ spread over the interval $(0,1)$, and then taking the $p$-value of $H_0$ as the minimum $p$-value of each $N^g_{t_i}$. This is justified as the information of the test is entirely contained in the set $\{ D_{I(j)J(j)}^{g_j} \}_{j = 1}^m$, i.e., since the $p\text{-value}$ is at most $\PP( N_t^g \geq k_t \mid H_0 )$ for all $t$, we can take an infimum over $t$. \\
\\
For the choice of $\mu_g$, we suggest using a uniform distribution only on some set of topological generators of $G$. This means that we don't sample the identity or other elements that generate only subgroups of $G$ that $f$ may be invariant to, but if there is an element that breaks the invariance then one of the generators will too (by lemma \ref{lem:topo_gens_invariance}) so we should capture that chance.


\subsubsection{Consistency of this test}
\label{ssec:consistency}

Under some mild conditions on the noise distribution, and with $m$ set at $n$ for all $n$, we can prove that the asymmetric variation test is consistent. The condition on the support of $\mu_X$ amounts to restricting $\mathcal{X}$ to the closure of the support as a practitioner would usually do. The condition that the noise admits a density is satisfied in many usual cases in regression (e.g. Gaussian noise). The condition that we can bound the concentration of the noise tightly is somewhat restrictive, but reflects the difficulty of the problem - if the asymmetry is obscured by more noise then it is much more difficult to identify it. The condition on $\mu_g$ is there to ensure that we sample from enough of $G$ to ensure we can spot points where $f$ is not $G$-invariant. A sufficient condition for this is where we sample from the Haar measure on $G$ when this exists (e.g., compact $G$).

\begin{prop}
	\label{prop:cons}
	Set $m = n$ and fix $t > 0$. Suppose that the law of $X$ has a dense support on $\mathcal{X}$. Suppose that $\epsilon$ admits a density $f_Y$ with respect to Lebesgue measure on $\mathcal{Y}$ that is decreasing in $|y|$. Suppose that $\mu_g$ is chosen such that $\PP( f( g \cdot X) \neq f(X) \mid H_1 ) > 0$. Then the asymmetric variation test is consistent, i.e. $p_{val} \mid H_1 \overset{p}{\rightarrow} 0$.
\end{prop}
	


The proof can be found in appendix \ref{app:proof4} in the supplementary material.

\subsubsection{Computational Complexity}

The main computational bottleneck is in finding the nearest neighbour in step 5. A naive algorithm would search across all $n$ samples to find $J(j)$, which means that the algorithm would cost $O(mn)$ operations. One improvement would be to store the feature vectors $X_i$ in a $k$-$\dd$ tree \citep{bentley1975multidimensional}. Building such a tree takes $O( n \log n)$ operations, but can find nearest neighbours in $O( \log n)$ time. This speeds up the asymmetric variation to $O( m \log n )$.



\subsubsection{Using the Asymmetric Variation Test in the Subgroup Lattice Search}

Consider using the Asymmetric variation test from section \ref{sec:testing} for $\mathrm{Test}_{\alpha}$ in the algorithms above. Since any distribution $\mu_g$ on a subgroup $G \leq \mathcal{G}$ is also a distribution on any supergroup $H \geq G$, any test low in the lattice $L(\mathcal{G})$ is technically a test for the groups above it, though one with lower - perhaps even zero - power, making this somewhat useless. \\
\\
Alternatively, we could consider constructing a variable $g$ on $\mathcal{G}$ by sampling a group from a finite subset $A \subseteq L(\mathcal{G})$ and the from the Haar measure sampled group (recall these are all closed and so compact when $\mathcal{G}$ is compact). Such a variable can be used in the sampling step of the Asymmetric Variation Test for testing $H_0 : f$ is $\mathcal{G}$-invariant. But it can also be used to test for invariance to the subgroups in $A$ by including a rejection sampling step - if we subset the samples $j \in \{1, \dots, m \}$ when $g_j \in H \in A$ we still have an iid sample we can use to count $N_t^g$ and test for $H$-invariance of $f$. \\
\\
This means that we can speed up doing multiple tests considerably as we only need to sample $m$ times and compare distances once per finite block $A$, and then simply do the much faster subsetting and counting operation for each of the groups $H \in A$. This of course means that the test results are even more correlated than previously (given that they shared the original data $\mathcal{D}$), trading some statistical power for computational power.




\subsection{Permutation Variant of the Asymmetric Variation Test}
\label{ssec:assumptionless}

The asymmetric variation test (in the previous subsection) relies on both the existence of, and the knowledge of, the bound $V(x,y)$. In this section we show that we can remove some of the requirement of the knowledge at the cost of computational (and sometimes statistical) power with an approximation to the previous test. \\
\\
This new test still has the assumption of a known order for the variation bound, but does not require any knowledge of the noise variables $\epsilon_i$ (other than that they are iid). The practitioner still has a choice of the random variable $g$, but now chooses a quantile $q$ instead of threshold(s) $t$. \\
\\
Suppose that we know only the order of the bound $V$, i.e., we know some $\mathcal{V}(x,y)$ such that for all $f \in \mathcal{F}$ there exists some (unknown) $L_f$ with $|f(x) - f(y)| \leq L_f \mathcal{V}(x,y)$ for all $x,y \in \mathcal{X}$. Clearly any known $V$ satisfies this property, but it is weaker in that we do not need to know that bound exactly. A key example of $\mathcal{V}$ is $d_\mathcal{X}(x, y)^\alpha$ for $\alpha$-H\"{o}lder continuous functions in any $\mathcal{F}(L, \alpha)$ (whereas we would need a constant multiple of this for a class of particular $\alpha$-H\"{o}lder continuous functions). \\
\\
Let $S_{ij}^g = |Y_i - Y_j| / \mathcal{V}(g \cdot X_i,X_j)$. Consider collecting $S^k = \{ S_{I(j)J(j)}^{g_j} \}_{j = 1}^m$ as in algorithm \ref{algo:known}, but where $J(j) \sim U( \{1, \dots, n\}$ instead of being the index of the nearest neighbour, and let $A_k$ be the $q^{th}$-quartile of this set for some chosen $q \in (0,1]$. Under the null hypothesis $H_0: f$ is $G$-invariant, the distributions of $S_{ij}^g$ will be the same as $S_{ij}^e$, so we can run a permutation test (See \citet{good2005permutation}) on the set $\{A_k \}_{k = 1}^B$, comparing them to the $q^{th}$-quantile $A_0$ of $S^0 = \{ S_{I(j)J(j)}^e \}_{j = 1}^m$ (sampled in the same way, but with $g = e$ almost surely). We can approximate the $p$-value by the proportion of $A_k \leq A_0$. This is described in algorithm \ref{algo:perm}.

\begin{algorithm}[h]
\caption{Permutation Variant of Asymmetric Variation Test}
\label{algo:perm}
\begin{algorithmic}[1]
\Procedure{PermVarTest}{$\mathcal{D}$, $\mu_g$, $\mathcal{V}$, $q$, $m$, $B$}
	\For{$k \in \{1, \dots, B \}$}
		\For{$j \in \{1, \dots, m \}$}
			\State $g_j \leftarrow \mathrm{Sample}( \mu_g )$
			\State $I(j) \leftarrow \mathrm{Sample}( \{1, \dots, n \} )$
			\State $J(j) \leftarrow \mathrm{Sample}( \{1, \dots, n \} )$
			\State $S_{I(j) J(j)}^{g_j} \leftarrow | Y_{I(j)} - Y_{J(j)} | / \mathcal{V}( g_j \cdot X_{I(j)}, X_{J(j)} )$
		\EndFor
		\State $A(k) \leftarrow \mathrm{quantile}( \{ S_{I(j) J(j)}^{g_j} \}_{j = 1}^m , q )$ \Comment{R syntax}
	\EndFor
	\For{$j \in \{1, \dots, m \}$}
		\State $I(j) \leftarrow \mathrm{Sample}( \{1, \dots, n \} )$
		\State $J(j) \leftarrow \mathrm{Sample}( \{1, \dots, n \} )$
		\State $S_{I(j) J(j)}^{e} \leftarrow | Y_{I(j)} - Y_{J(j)} | / \mathcal{V}( X_{I(j)}, X_{J(j)} )$
	\EndFor
	\State $A_0 \leftarrow \mathrm{quantile}( \{ S_{I(j) J(j)}^{e} \}_{j = 1}^m , q )$
	\State $p_{val} \leftarrow |\{ k : A(k) \leq A_0 \}| / B$
	\State \Return $p_{val}$
\EndProcedure
\end{algorithmic}	
\end{algorithm}

\subsubsection{Finite sample effects and choice of quantile $q$}

Whilst it is true that $S^g_{ij} \overset{D}{=} S^e_{ij}$ for independent $X_i,X_j$, the finite sample estimates of these quantities are not equal. In fact, the distribution of $S^g_{ij}$ can be biased upwards because the action on $\mathcal{X}$ allows us to see more points with smaller $d_\mathcal{X}(g \cdot X_i,X_j) < d_{\mathcal{X}} ( X_k, X_l )$. As $m$ increases, we are more and more likely to see the outlying values of $S_{ij}^g$ compared to $S_{ij}^e$. This can cause a bias towards rejection in this permutation test, i.e., it is liberal for small values of $n$. This inexactness is a known issue with permutation tests, for example when testing the variance of univariate samples (as in \citet{good2005permutation}, \S 3.7.2), and approximate permutation tests as used here are still shown to be successful and useful. \\
\\
These problems are alleviated somewhat by using the quantile $q \in (0,1]$, as this is less sensitive to the outlying values of $S_{ij}^g$ than picking $q = 1$ (i.e., just going with the maximum of the $S_{I(j)J(j)}^{g_j}$). This does mildly reduce the power of the test, but improves the specificity significantly. We have found in simulations that using $q = 0.95$ works well in practice.


\subsubsection{Computational Complexity}

Here we have made a trade in the computational complexity - we no longer compute nearest neighbours, but now have to run $B$ extra trials. Thus the computational complexity of the sampling is of order $O(Bm)$. We then have to compute the quantiles for each permutation, which essentially requires ordering the $m$ samples of $S_{ij}^g$, which can be done in $O(m \log m)$ operations with, for example, mergesort or heapsort \citep{knuth1998art}. This gives an overall complexity of $O(B m \log m)$


\section{Numerical Experiments}
\label{sec:exp}

%Since the estimator relies on the tests, we conduct experiments for the hypothesis test first. We show that the test performs well in finite samples, able to distinguish invariance with only $n = 50$ datapoints for an empirical power of $95\%$ when $d = 2$, and $n = 300$ when $d = 4$. We also show that the test can work in dimension $784$ by testing for invariance of images of handwritten digits in the MNIST dataset \citep{lecun1998gradient}.  \\
%\\
%We then show how these experiments relate to the estimation of the maximal symmetries for each of the methods presented in section \ref{sec:lattice}, noting that the proportion of experiments with $\hat{G} = G_{\max}(f, \mathcal{G})$ is almost exactly equal to the power of the test. We also demonstrate an example of estimating the infinite group $SO(3) = G_{\max}( f, SL(3) )$, though this example required over 1000 samples to achieve a $95\%$ accuracy. \\
%\\
We've constructed three simulations to gauge the performance of the group estimation technique. The first is a low dimensional regression estimation where we can directly quantify the errors the group estimator $\hat{G}(\mathcal{D})$ by the proportion of correct estimations. The second then considers the effect of using the estimated group $\hat{f}_{\hat{G}}$ to symmetrise the estimator $\hat{f}$ where we consider the mean squared prediction error. We then consider a high dimensional ($d = 784$) example with the MNIST dataset \citep{lecun1998gradient}. \\
\\
All tests were conducted locally on a 2020 13'' MacBook Pro laptop with a 2 GHz Quad-Core Intel Core i5 processor and 16GB of RAM. All code is available on GitHub (\href{https://github.com/lchristie/estimating_symmetries}{HERE}). Further simulations, including those explicitly for the hypothesis tests only can be found in appendix \ref{app:testing}.

\begin{eg}[Finite Group Estimation]
	\label{eg:low_dim_G_sims}
	Consider the functions $f_d : (x_1, \dots, x_d) \mapsto \exp( - | x_1 | )$ for invariance to the subgroups of $C_4$ acting by rotations in the $x_1$-$x_2$ plane. For these functions we have $G_{\max}( f_d, C_4) = C_2$, and the subgroup lattice chosen is $L( C_4 ) = \{ I, C_2, C_4 \}$. As such, in these simulations rejections of the null $H_0^{(1)}: f_d$ is $C_2$-invariant give $\hat{G} = I$, and otherwise if $H_0^{(2)} : f$ is $C_4$-invariant is rejected then we get $\hat{G} = C_2 = G_{\max}$, and if both are accepted then $\hat{G} = C_4$. This is true for any of the three estimation algorithms. \\
	\\
	We generate data from $X_i \iid N(0,4I_d)$ and $\epsilon_i \iid N(0, 0.05^2 )$. We simulated tests at the $\alpha = 0.05$ significance level of each of the null hypotheses using both tests (algorithms \ref{algo:known} and \ref{algo:perm}) and these are shown in figure \ref{fig:est_power} of appendix \ref{app:testing}.  We ran 100 simulations for each combination of
	\begin{equation}
		n = m \in \{ 20,30,40,50,60,70,80,90,100,120,150,200,250,300 \}
	\end{equation}
 	Both tests assumed that $f$ is $1/e$-Lipschitz. The asymmetric variation tests had $t = 2 \sigma = 0.1$, $p_t = \frac{ 2 \sigma }{ t  } \tfrac{ \exp( - t^2 / 4 \sigma^2 )}{\sqrt{2\pi}}$ and the permutation variants had $B = 100$ and $q = 0.95$. We tested with $g_j \iid U( G \setminus \{ 1 \} )$ for all tests of $G$-invariance. \\
	\\
	We plot the proportions of estimation in the graphs in figure \ref{fig:toy_eg_G_hat}. We see that in the lower dimensional $f_2$ example we very quickly converge to the true $G_{\max}$ after only 50 samples, but in the higher dimensional $f_4$ we need at least 200 or so (otherwise we over estimate $\hat{G} = C_4 > G_{\max}$).
		
	\begin{figure}[h]
		\centering
		
%			\begin{subfigure}[b]{0.45\textwidth}
%			\centering
%				\includegraphics[scale = 0.4]{figs/G_hat_toy_example_f_2.pdf}
%				\caption{Estimates for $f_2$}
%			\end{subfigure}
%%			\hfill
%			\begin{subfigure}[b]{0.45\textwidth}
%						\centering
%
%				\includegraphics[scale = 0.4]{figs/G_hat_toy_eg_f_4.pdf}
%				\caption{Estimates for $f_4$}
%			\end{subfigure}


		\begin{tabular}{cc}
			\includegraphics[scale = 0.4]{figs/G_hat_toy_example_f_2.pdf}
 				& \includegraphics[scale = 0.4]{figs/G_hat_toy_eg_f_4.pdf} \\
 				(a) Estimates for $f_2$ & (b) Estimates for $f_4$

		\end{tabular}


		
		\caption{The proportions of each possible $\hat{G}$ for plotted against $n$ for both $f_2$ and $f_4$. The proportion of $\hat{G} = I$ is in {\cred red} triangles, of $\hat{G} = C_2 = G_{\max}$ in black squares, and of $\hat{G} = C_4$ in {\cred red} circles.}		
		\label{fig:toy_eg_G_hat}
	\end{figure}


\end{eg}

\begin{eg}[Simulation of Symmetrised Estimators]
\label{eg:est_f_sims}
Suppose that $\mathcal{X} = \RR^3$ and let $\mathcal{G} = SL(3, \RR)$ act on $\mathcal{X}$ by matrix multiplication. Let $K$ be the sub-lattice of $K(SL(3, \RR)$ from example \ref{eg:so3} with $SL(3, \RR)$ added. As per the statistical problem (Section \ref{ssec:stats}), we simulate data $\mathcal{D} = \{ (X_i, f(X_i) + \epsilon_i \}_{i = 1}^n$ and $\mathcal{D}' = \{ (X_i', f(X_i') + \epsilon_i' \}_{i = 1}^n$ under four scenarios:
\begin{enumerate}[1)]
	\item (Interpolation with Symmetric $f$) $f_1(x) = \sin( - \| x \|_2 )$ and $X_i, X_i' \iid N( 0, 2 I_3 )$;
	\item (Extrapolation with Symmetric $f$) $f_2(x) = \sin( - \| x \|_2 )$ and $X_i \iid N( 0, \mathrm{Diag}( 0.1, 0.1, 2 )$ and $X_i' \iid N( 0, 2 I_3 )$;
	\item (Extrapolation with Asymmetric $f$) $f_3(x) = \sin( - | x_3 | )$ and $X_i \iid N( 0, \mathrm{Diag}( 0.1, 0.1, 2 )$ and $X_i' \iid N( 0, 2 I_3 )$; and
	\item (Extrapolation with Asymmetric $f$) $f_4(x) = \sin( - | x_1 | )$ and $X_i \iid N( 0, \mathrm{Diag}( 0.1, 0.1, 2 )$ and $X_i' \iid N( 0, 2 I_3 )$
\end{enumerate}
The noise variables were sampled as $\epsilon_i, \epsilon_i' \iid N(0, 0.01^2)$ for all scenarios. Note that in scenarios $1$ and $2$ we have $G_{\max}(f_1, K) = G_{\max}(f_2, K) = SO(3)$ whilst under scenarios 3 and 4 we have $G_{\max}( f_3,K )=G_{\max}( f_4,K ) = I$. Scenarios 3 and 4 do contain some symmetry, rotations around either the $x_3$ or $x_1$ axes respectively, but these symmetries are not present in the lattice $K$ used. We use the estimator $\hat{G}( \mathcal{D} ) $ using the breadth first search (Algorithm \ref{algo:bfe}) using the asymmetric variation test with $V(x,y) = d_{\mathcal{X}}(x,y)$ and the other hyper-parameters as in example \ref{eg:low_dim_G_sims}.
 We then consider three estimators for $f$:
\begin{enumerate}[	A)]
	\item A standard local constant estimator (LCE) $\hat{f}(x, \mathcal{D})$ with bandwidths selected by the R-package NP \citep{hayfield2008np};
	\item A symmetrised LCE $\hat{f}_{\hat{G}}^A \big( \pi_{\hat{G}}(x) ,  \{ ( \pi_{\hat{G}}( X_i), Y_i ) \}_{i = 1}^n \big)$ where $\hat{G} = \hat{G}( \mathcal{D} )$; and
	\item A symmetrised LCE that splits $\mathcal{D}$ into independent pieces half the size of $\mathcal{D}$, i.e.  $\hat{f}_{\hat{G}}^I \big( \pi_{\hat{G}}(x) ,  \{ ( \pi_{\hat{G}}( X_i), Y_i ) \}_{i = \lfloor n/2 \rfloor + 1}^n \big)$ where $\hat{G} = \hat{G}( \{ (X_i, Y_i) \}_{i = 1}^{\lfloor n/2 \rfloor} )$.
\end{enumerate}
Here $\pi_G : \mathcal{X} \rightarrow  \mathcal{X} / G $ is the natural projection given by $\pi_{I}(x) = x$, $\pi_{SO(3)}(x) = \|x \|_2$, and $\pi_{SL(3)}(x) = \mathbf{1}_{x \neq 0}$. It is possible to construct projections for the finite subgroups in $K$ by picking a fundamental domain of the action of $\hat{G}$, but these were never needed in our simulations. The projections for the infinite groups dramatically reduce the dimension of the sample space (from 3 to 1 or even 0), which should improve the estimation when $f$ is truly $G$-invariant. \\
\\
The mean squared predictive errors are plotted in figures \ref{fig:mspes12} and \ref{fig:mspes34}. In scenario 1), we see that the LCE has exactly the expected performance decaying at the rate appropriate for a 3 dimensional space. In contrast, the estimators that search for symmetry start by over smoothing (when they cannot reject $SL(3, \RR)$-invariance) but then quickly settle into the rate appropriate for the 1 dimensional quotient space which means they are both orders of magnitude more accurate than the LCE. This is even more pronounced in scenario 2), where the LCE struggles to generalise well but the symmetrised estimator learns the appropriate pattern and uses this to extrapolate accurately. \\
\\
Scenarios 3) and 4) show the a similar pattern. The LCE has reasonable performance, and since $G_{\max}(f, K) = I$ the symmetrised estimators learn that they should just give the LCE as their chosen model, so the errors converge exactly between estimators A) and B) with a lag because of the half data for estimator C.

\begin{figure}[h]
	\centering

	\begin{tabular}{c}
		\includegraphics[scale=0.4]{figs/scenario_1_n100.pdf} \\
		\includegraphics[scale=0.4]{figs/scenario_2_n100.pdf}
	\end{tabular}

	\caption{Mean squared prediction errors for the three estimators, under scenarios 1 and 2 . The average across the 100 simulations is plotted as circles for each of the estimators, with black for the LCE, {\cblu blue} for the model symmetrised with all the data, and {\cred red} for the symmetrised estimator that split the dataset. Tick marks show the errors for each of the simulations, in the same colour.}
	\label{fig:mspes12}
\end{figure}


\begin{figure}[h]
	\centering
	\begin{tabular}{c}
		\includegraphics[scale=0.4]{figs/scenario_3_n100.pdf} \\
		\includegraphics[scale=0.4]{figs/scenario_4_n100.pdf}
	\end{tabular}

	\caption{Mean squared prediction errors for the three estimators, under scenarios 3 and 4. The average across the 100 simulations is plotted as circles for each of the estimators, with black for the LCE, {\cblu blue} for the model symmetrised with all the data, and {\cred red} for the symmetrised estimator that split the dataset. Tick marks show the errors for each of the simulations, in the same colour. }
	\label{fig:mspes34}
\end{figure}


\end{eg}


\begin{eg}[MNIST Symmetries of the digit $5$]
\label{eg:MNIST}
Consider the symmetries of images of the digits 3 and 8. It is clear that rotations of both preserve the classification, however one can argue that only the 8 is invariant to horizontal reflections - a reflected 3 would be an $\mathcal{E}$ (in the same way that a reflected p is a q), and so can be considered oriented in a way that the 8 is not. We call the in digits $\mathcal{O} = \{ 2, 3, 4, 5, 6, 7, 9 \}$ the \textbf{oriented digits} and the others $\{ 0, 1, 8 \}$ the \textbf{non-oriented digits}. We consider the three to be vertically asymmetric so the rotated $3$ is distinct from the reflected $3$, and thus $3$s are rotationally invariant, and all other oriented digits are considered similarly except sixes and nines which are not rotationally symmetric. \\  
\\
Each digit has a function $f_a : \mathcal{X} \rightarrow [0,1]$ for which $f_a( X ) = \PP( X \text{ is recognised as the digit } a )$, and we seek to estimate the symmetries of these probabilistic regression functions using the asymmetric variation test. \\
\\
We subset the MNIST dataset \citep{lecun1998gradient} (Available under a CC BY-SA 3.0 licence) into particular characters, called $\mathcal{D}_{X}(n)$ for $n \in \{0, 1, \dots, 9 \}$. We then split these in half uniformly at random to form sets $\mathcal{D}_{X}^1(n)$ and $\mathcal{D}^R_{X}(n)$. We then apply $b$ (the reflection through the vertical line) from the dihedral group $D_4$ to the elements of $\mathcal{D}_X^R(n)$ and then assign labels $Y_i = 1$ for every for $X_i \in \mathcal{D}_X^{1}(n)$ for all $n$ and $\mathcal{D}_X^R(n)$ if $n$ is non-oriented, and the label $Y_i = 0$ for every other $X_i$. This gives datasets
\begin{align}
\mathcal{D}(n) = &\{ (X_i, 1) : X_i \in \mathcal{D}_X^{1}(n) \} \cup \{ (X_i, \mathbf{1}_{n \not\in \mathcal{O} }) : X_i \in \mathcal{D}_X^{R}(n) \}
\end{align}
Here the labels $Y_i$ are the probabilities that $X_i$ will be recognised as the digit $n$, and so we are learning the function $f_n : [0,1]^{784} \rightarrow [0,1]$ that assigns such probabilities. We assume that there is no noise, so $p_t = 0$ for all $t$. Lastly we need to pick $V(x,y)$, which we estimate with the reciprocal of the minimal distances between digits in and out of each class. \\
\\
We then test each of the datasets $\mathcal{D}(n)$ for invariance of each $f_n$ the subgroups of $D_4$ (enumerated in the Hasse diagram in Figure \ref{fig:hasse_D4}) using the asymmetric variation test at significance level $\alpha = 0.05$ and with $V(x,y) = L_n d(x,y)$ (where the coefficients $L_n$ were estimated separately), $\sigma = 0$ and so the concentration bound is given by $p_t = 0$ for all $t > 0$. \\
\\
Using the breadth first search for the digit $5$, we find that tests for $\langle R_v \rangle$-invariance and $\langle R_h \rangle$-invariance are rejected, whilst $\langle R_\pi \rangle$, $\langle R_/ \rangle$ and $\langle R_\setminus \rangle$ are not. We then test on the second layer of $L(D_4)$, but only at $\langle R_{\pi / 2 } \rangle$ and $\langle R_{\pi}, R_/ \rangle$, where again we find acceptance - thus we have $\tilde{G}_B = \{\langle R_{\pi / 2 } \rangle , \langle R_{\pi}, R_/ \rangle \}$. This is stepped through in the diagrams of figure \ref{fig:mnist_d4_diagrams_bfs}. Note that the test used lacked power because the data distribution was not $\langle R_/ \rangle$-invariant - we didn't have any samples of diagonally reflected fives to compare against. We then have $\hat{G}_B^{Alt} = \langle R_{\pi / 2} \rangle \cap \langle R_{\pi}, R_/ \rangle = \langle R_{\pi} \rangle$, which in our set-up is a subset of the correct $G_{\max} = \langle R_{\pi / 2} \rangle$. Tests conducted for other digits are shown in appendix \ref{app:testing}. 
	
	\begin{figure}[h]
		\centering
		\begin{tabular}{ccc}
		\begin{tikzpicture}[scale = 0.6]
		\draw[fill] (0,0) circle (0.07cm);
		\draw[fill, red] (-4,2) circle (0.07cm);
		\draw[fill, red] (-2,2) circle (0.07cm);
		\draw[fill, green] (0,2) circle (0.07cm);
		\draw[fill, green] (2,2) circle (0.07cm);
		\draw[fill, green] (4,2) circle (0.07cm);
		\draw[fill] (-3,4) circle (0.07cm);
		\draw[fill] (0,4) circle (0.07cm);
		\draw[fill] (3,4) circle (0.07cm);
		\draw[fill] (0,6) circle (0.07cm);
		\draw (-4,2) -- (-3,4);
		\draw (-2,2) -- (-3,4) -- (0,6);
		\draw (0,2) -- (-3,4);
		\draw (2,2) -- (3,4);
		\draw (4,2) -- (3,4);
		\draw (0,2) -- (0,4) -- (0,6);
		\draw (0,2) -- (3,4) -- (0,6);
		\draw[red] (0,0) -- (-4,2);
		\draw[red] (0,0) -- (-2,2);
		\draw[green] (0,0) -- (0,2);
		\draw[green] (0,0) -- (2,2);
		\draw[green] (0,0) -- (4,2);
		\end{tikzpicture} &
		\begin{tikzpicture}[scale = 0.6]
		\draw[fill] (0,0) circle (0.07cm);
		\draw[fill, red] (-4,2) circle (0.07cm);
		\draw[fill, red] (-2,2) circle (0.07cm);
		\draw[fill, green] (0,2) circle (0.07cm);
		\draw[fill, green] (2,2) circle (0.07cm);
		\draw[fill, green] (4,2) circle (0.07cm);
		\draw[fill, blue] (-3,4) circle (0.07cm);
		\draw[fill] (0,4) circle (0.07cm);
		\draw[fill, blue] (3,4) circle (0.07cm);
		\draw[fill, blue] (0,6) circle (0.07cm);
		\draw[blue, dotted] (-4,2) -- (-3,4);
		\draw[blue, dotted] (-2,2) -- (-3,4) -- (0,6);
		\draw[blue, dotted] (0,2) -- (-3,4);
		\draw[] (2,2) -- (3,4);
		\draw[] (4,2) -- (3,4);
		\draw (0,2) -- (0,4);
		\draw[blue, dotted] (0,4) -- (0,6);
		\draw[]  (0,2) -- (3,4);
		\draw[blue, dotted] (3,4) -- (0,6);
		\draw[red] (0,0) -- (-4,2);
		\draw[red] (0,0) -- (-2,2);
		\draw[green] (0,0) -- (0,2);
		\draw[green] (0,0) -- (2,2);
		\draw[green] (0,0) -- (4,2);
		\end{tikzpicture} &
		\begin{tikzpicture}[scale = 0.6]
		\draw[fill] (0,0) circle (0.07cm);
		\draw[fill, red] (-4,2) circle (0.07cm);
		\draw[fill, red] (-2,2) circle (0.07cm);
		\draw[fill, green] (0,2) circle (0.07cm);
		\draw[fill, green] (2,2) circle (0.07cm);
		\draw[fill, green] (4,2) circle (0.07cm);
		\draw[fill, blue] (-3,4) circle (0.07cm);
		\draw[fill, green] (0,4) circle (0.07cm);
		\draw[fill, green] (3,4) circle (0.07cm);
		\draw[fill, blue] (0,6) circle (0.07cm);
		\draw[blue, dotted] (-4,2) -- (-3,4);
		\draw[blue, dotted] (-2,2) -- (-3,4) -- (0,6);
		\draw[blue, dotted] (0,2) -- (-3,4);
		\draw[green] (2,2) -- (3,4);
		\draw[green] (4,2) -- (3,4);
		\draw[green] (0,2) -- (0,4);
		\draw[blue, dotted] (0,4) -- (0,6);
		\draw[green]  (0,2) -- (3,4);
		\draw[blue, dotted] (3,4) -- (0,6);
		
		\draw[red] (0,0) -- (-4,2);
		\draw[red] (0,0) -- (-2,2);
		\draw[green] (0,0) -- (0,2);
		\draw[green] (0,0) -- (2,2);
		\draw[green] (0,0) -- (4,2);
		
		\end{tikzpicture}		\\
		(a) The first layer of testing. & (b) Subsequently deleted nodes & (c) Second layer of testing.
			
		\end{tabular}
		
		\caption{The breadth first search algorithm illustrated to find $G_{\max}(f_5, D_4)$. Rejections are shown in {\cred red}, accepted tests in {\cgre green}, and subsequently deleted nodes in {\cblu blue} and edged dotted in {\cblu blue} as well. The nodes are labeled as in example \ref{eg:dihedral}. }
		\label{fig:mnist_d4_diagrams_bfs}
		
	\end{figure}
		
	\end{eg}


\section{Application to Satellite Based Magnetic Field Data}
\label{sec:app}
The European Space Agency (ESA) launched the SWARM mission in 2013 to measure the earth's magnetic field and other related properties. This consists of three satellites orbiting between 450km and 530km above the earths surface. These measurements complement the observations at ground based observatories to better model the field for use in navigation and geospatial modelling. This field is generated by four main components: the rotation of the inner core; the effects of magnetised material in the earth's crust; electrical currents in the upper atmosphere caused by solar winds; and the interaction of the sun's magnetic field. \\
\\
%The earth's magnetic field is a vector field $B: \RR^3 \rightarrow \RR^3$, which is the gradient of a scalar potential field $V: \RR^3 \rightarrow \RR$. The potential field $V$ on and above the earth's surface at time $t$ is modelled as
%\begin{equation}
%	V(r, \phi, \theta, t) = a \sum_{n = 1}^{13} \sum_{m = 0}^n \left( \frac{a}{r} \right)^{n+1} \Big( g_n^m(t) \cos( m \theta ) + h_n^m(t) \sin( m \theta) \Big) P_n^m( \cos(\phi) )
%\end{equation}
%in the International Geomagnetic Reference Field (IGRF) \citep{thebault2015international}. Here $a = 6,371.2$km is the earth's radius, $\phi$ is the geocentric co-latitude, $\phi$ is the east longitude. The functions $P_n^m$ as the Schmidt quasi-normalised associated Legendre functions of degree $n$ and order $m$. The Gauss coefficients $g_n^m$ and $h_n^m$ are then fitted using observational data from ground based stations. Later version have included satellite based observations {\cred CITE}.\\
%\\
We have used a subset of the data available from the SWARM satellites to estimate the field strength of the magnetic field $\| B( x) \|_2$ over the sphere of radius $\approx 6,820km$ from the earth's core. Using the data from only Satellite $\alpha$ on the 25th of February 2023, we selected 220 observations over the Northern Hemisphere. Two hundred occur in a ``dense'' region over Europe and Asia, and 20 occur in a ``sparse'' region over North America and adjacent oceans. These measurements are plotted in figure \ref{fig:map_of_stations}. \\

\begin{figure}[h]
\centering
\includegraphics[scale = 0.35]{figs/Sat_data_measurements.pdf}
\caption{Locations of the 98 stations used in the estimation of $f_F$.}
\label{fig:map_of_stations}
\end{figure}


We follow the same symmetry estimation procedure (Algorithm \ref{algo:bfe}) for these functions as in example \ref{eg:est_f_sims}. We use the lattice $K \subseteq K( SO(3) )$ given by $\{ I, S^1_{u_i}, SO(3) : i \in \{ 1, \dots , 15\} \}$ where the vectors $\{ u_i : i \leq 6 \}$ form the axes of a regular icosahedra and $\{ u_i : 7 \leq  i \leq 12 \}$ correspond to axis through an icosahedron rotated by 36 degrees. We then also add $u_{13} = (0,0,1)$, $u_{14} = (0,1,0)$, and $u_{15} = (1,0,0)$. We use the permutation variant of the asymmetric variation test with a Lipschitz assumption $\mathcal{V}(x,y) = d_{\mathcal{X}}(x,y)$. We use the quantile $q = 0.95$ and the significance level $\alpha = 0.05$, and run $B = 200$ permutations each with $m = 3000$. We sample from each $S^1_{u}$ with a bias towards low angles (i.e., angles sampled as  $\theta \sim N(0, (2 \pi \times 0.2 )^2)$) and uniformly on $SO(3)$. We rejected every symmetry except for rotations about the axis $u_{13}$, the (Geographic) north pole. Thus $\hat{G} = S^1_{u_{13}}$.  \\
\\
We then estimate the function $\| B(x) \|$ using local linear estimators (as in example \ref{eg:est_f_sims}. We model both on the original space $S^2$ parametrised by the latitudes and longitudes, and since $\hat{G} = S^1_u$ we model on the space $S^2 / S^1_u \cong [0, \pi ]$ via the projection $x \mapsto \arccos ( \langle u_{13} , x \rangle )$. We judge these estimations against the International Geomagnetic Reference Field (IGRF) \citep{thebault2015international} model predictions using mean squared prediction error over a grid of latitudes and longitudes at 1 degree intervals. Over the whole northern hemisphere, the symmetrised estimator achieved an MSPE 23\% lower than the local linear estimator. Over the dense region (where $\theta > 0$) the symmetrised estimator had an error rate 2\% lower than the LLE. All code and data is available \href{https://github.com/lchristie/estimating_symmetries}{HERE}. \\
\\
It is worth noting that the $p$-value of the test for $S^{1}_{u_{13}}$ symmetry was only $0.075$. Whilst technically above the significance level $\alpha = 0.05$, this shows that the test is giving a low confidence to the symmetry around the geographic north pole. This means that the symmetry estimator has correctly identified that the model that has this symmetry should perform better with this dataset, but just barely. With extra data one might expect this test to reject this symmetry.

\section{Discussion}
\label{sec:con}

We have shown that we can generalise the idea of variable subset selection into selection of non-linear combinations of the variables by placing them within the framework of subgroup lattices. We have developed methods for testing and used these to estimate the maximal invariant symmetry of a regression function. We have demonstrated the power and applicability of these tests across several numerical experiments, showing that when the symmetry exists our methods do detect this consistently and that this can significantly improve estimates of the regression function.  \\

The main limitations of these methods are the related to the existence of the symmetries of $f$. It may be the case that $f$ is close (in $L^2$ distance) to an invariant function but not one itself. In this case estimation of $G_{\max}(f)$ is at least as hard as distinguishing between the two possible functions. However, using the symmetry can still improve the estimate of $f$ by adding bias (smoothing from the symmetry) for much reduced variance. It would be an interesting question of further study to quantify these effects. \\
\\
The other limitation is that $\hat{G}$ is biased towards more symmetry because it relies on a hypothesis test. This is deliberate here - it gives a more consistent estimate of $\hat{G}$ for precisely the reason above. However, it may be a detriment when using $\hat{G}$ to smooth $\hat{f}$ (as illustrated by Scenario 1 in example \ref{eg:est_f_sims}). It is worth considering methods akin to forward selection and backwards elimination in the variable selection literature. One could, for example, look to minimise residual sum of squares of the smoothed models $\hat{f}_G$ over the lattice $K$ at each level as with forward selection.\\
\\
Overall, given the use of geometric methods in statistics and machine learning which exploit symmetry, the methods presented in this paper should allow the full exploitation while also guarding against erroneous over-assumption of symmetries in the data.

\bibliographystyle{rss}
\bibliography{ms.bib}


\newpage
\appendix

\section{Proofs and further mathematical details}

\subsection{Proofs in Section \ref{sec:background} }
	
The two propositions cited in section 2 have the following proofs.

\begin{proof}[Proof of Proposition \ref{prop:closed_sublattice}]
We need only show that $K(\mathcal{G})$ contains supremums and intersections (with respect to the subgroup ordering) of arbitrary pairs $G,H \in K(\mathcal{G})$, and that these are given by the join and meet operations in the statement. If $G,H \in K(\mathcal{G} )$, then consider that $G \cap H$ is still the infimum as with the subgroup lattice. For the supremum, consider that $G, H \leq \overline{ \langle G, H \rangle } $ by definition, so this is an upper bound. If $G, H \leq A \in K(\mathcal{G} )$ then $\langle G, H \rangle \leq A$ and so $ \overline{ \langle G, H \rangle  } \leq \overline{A} = A $, and so $\overline{ \langle G, H \rangle }$ is in fact the least upper bound and we are done.
\end{proof}	

\begin{proof}[Proof of Proposition \ref{prop:sub_lattice_restriction}]
	This follows from exactly the same reasoning as Proposition \ref{prop:max_subgroup}, except here we need only consider finitely many groups $H$ and have to take closures: i.e.
	\begin{equation}
		A = \overline{ \langle H \in K :  f \text{ is } H \text{-invariant} \rangle }
	\end{equation}
	Since $I \in K$, we know $A \geq I$. Since $K$ is a finite lattice, $A$ must also be in $K$. Lastly note that Lemma \ref{lem:generate_inv} implies that $f$ is $A$-invariant and so $A \leq \Gmax$.
\end{proof}

We also provide other details on related concepts for $G$-invariant functions. \\
\\
We say that a subgroup $H \leq G$ is a \textbf{maximal invariant subgroup} of $f$ if: $f$ is invariant to $H$ and if $f$ is invariant to any other subgroup $H' \leq G$, then $H' \leq H$. The next proposition shows that every function $f$ has a unique maximal invariant subgroup.


\begin{prop}
\label{prop:max_subgroup}
For any group $G$ acting on $\mathcal{X}$, every function $f : \mathcal{X} \rightarrow \RR$ has a unique maximal invariant subgroup $G_{\max}$.
\end{prop}

This result requires a few supporting lemmas.


%This results means that when we specify a function $f$ and search group $\mathcal{G}$, we can refer to $G_{\max}(f, \mathcal{G})$ or just $G_{\max}$. The following Lemma shows that the unique maximal invariant subgroup is closed when the action and regression function are continuous. This requires that $G$ is a topological group (defined in appendix \ref{sec:groups}).



\begin{Lem}
\label{lem:generate_inv}
	Let $\mathcal{G}$ be a group with subgroups $G,H \leq \mathcal{G}$.	Suppose that $f : \mathcal{X} \rightarrow \RR$ is $G$-invariant. If $H \leq G$ then $f$ is $H$-invariant. If $f$ is $H$ invariant then $f$ is $\langle G, H \rangle$-invariant.
\end{Lem}

\begin{proof}[Proof of Lemma \ref{lem:generate_inv}]
The first claim is clear: if $f(g \cdot X) = f(X)$ for all $g \in G$ almost surely then it is true for all $g \in H \subseteq G$. For the second, take $x \in \mathcal{X}$ and $a \in \langle G, H \rangle$. Express $a = a_1^{i_1} \cdots a_k^{i_k}$, where $a_j \in G$ or $a_j \in H$ and $i_j \in \NN$ for all $j \in [k]$. Then see that
\begin{equation}
f( a \cdot x ) = f ( a_1^{i_1} \cdot ( a_2^{i_2} \cdots a_k^{i_k} \cdot X ) ) = f( a_2^{i_2} \cdots a_k^{i_k} \cdot X )
\end{equation}
almost surely so the second claim follows by an induction on $k$.
\end{proof}




\begin{Lem}
\label{lem:lattice_inv}
Suppose that $f$ is not $G$-invariant for some subgroup $G \leq \mathcal{G}$. Then $f$ is not $H$-invariant for any $H$ with $G \leq H \leq \mathcal{G}$. If $f$ is $H$ invariant then $f$ is not $G'$ invariant for any $G'$ with $\langle G', H \rangle = \langle G, H \rangle$.
\end{Lem}

\begin{proof}[Proof of Lemma \ref{lem:lattice_inv}]
	The first result is just the contraposition of Lemma \ref{lem:generate_inv}. The second part also follows as if $f$ were $G'$ invariant, then it would be $ G \leq \langle G' , H \rangle = \langle G, H \rangle $ invariant.
\end{proof}

We can now prove the original proposition.
\begin{proof}[Proof of Proposition \ref{prop:max_subgroup}]
	Let $G_{\max} = \langle H \leq G : f \text{ is } H \text{-invariant} \rangle$, noting that $f$ is $I$-invariant and so there is at least one such $H$. Then the previous lemma gives the first condition of maximal invariance, and the second is trivial and  also implies uniqueness.
\end{proof}


Next, when $G$ is a topological group acting continuously on $\mathcal{X}$ we can have further structure to $\Gmax$.
\begin{Lem}
	\label{lem:closure_invariance}
	Let $G$ be a topological group. Suppose that $f : \mathcal{X} \rightarrow \RR$ is continuous and $\overline{G}$ acts continuously on $\mathcal{X}$. Then $f$ is $G$-invariant if and only if $f$ is $\overline{G}$-invariant.
\end{Lem}
\begin{proof}[Proof of Lemma \ref{lem:closure_invariance}]
	Since $G \subseteq \overline{G}$ the forwards direction is clear. Suppose that $f$ is either weakly or strongly $G$-invariant. Take $g \in \overline{G}$ and a net $g_\bullet$ in $G$ converging to $g$, and note that
	\begin{equation}
		f(g \cdot X) = f( ( \lim g_\bullet) \cdot X) = f ( \lim g_\bullet \cdot X ) = \lim f( g_\bullet \cdot X) = \lim f(X) = f(X)
	\end{equation}
	almost surely, giving the result.
\end{proof}


Lastly we require the following result to simplify the sampling distributions when testing for symmetry in section \ref{sec:testing}.
\begin{Lem}
	\label{lem:topo_gens_invariance}
	Suppose $f : \mathcal{X} \rightarrow \RR$ is continuous and $G$ acts continuously on $\mathcal{X}$.	If $G$ is topologically generated by a finite set $\{ g_1, \dots, g_k \}$ and $f$ is not $G$-invariant, then $f$ is not invariant to at least one group generated by a topological generator $\langle g_i \rangle$.
\end{Lem}

	\begin{proof}[Proof of Lemma \ref{lem:topo_gens_invariance}]
	Let $g \in G$ be such that $\PP( f( g \cdot X) = f(X) ) < 1$, and let $\{ h_i \}_{i = 1}^\infty$ be a sequence in $\langle g_1, \dots, g_k \rangle$ that approximates $g$. Suppose for the sake of contradiction that $f$ is invariant to every $\langle g_i \rangle$. Then $f( h_i \cdot X ) = f(X)$ almost surely (as $h_i$ is a finite product of only $g_i$ terms) for every $i \in \NN$. But then the continuity of $f$ (noting that $\mathcal{X}$ is a first countable metric space) implies
	\begin{equation}
		f(X) = \lim_{i \rightarrow \infty} f( h_i \cdot X) = f( \lim_{i \rightarrow \infty} h_i \cdot X) = f(g \cdot X)
	\end{equation}
	almost surely - a contradiction.
\end{proof}
	
	

	






%\begin{proof}[Proof of Proposition \ref{prop:chains}]
%	Suppose that $G_i \neq I$ for all finite $i$. Moreover, none of these $G_i$ can even be finite as then it would descend to $I$ within $|G_i|$ steps. Thus all $G_i$ are infinite, and satisfy the property that any finite collection of $\{G_i\}$ has a non-empty intersection. As $\mathcal{G}$ is compact, the entire collection $\cap_{i} G_i$ must have a non-empty intersection, call this $G$. \\
%	\\
%	
%\end{proof}
%
%\begin{proof}[Proof of Proposition \ref{prop:finite_sl}]
%	Consider that the free lattice on the elements on $A$, written $FA$, contains at most $2^{|A|}$ elements, and the lattice $\langle A \rangle$ must be isomorphic to a sublattice of $FA$.
%\end{proof}






\subsection{Proofs in Section \ref{sec:lattice}}

\begin{proof}[Proof of Lemma \ref{prop:hat_G_inv}]
	By Lemma \ref{lem:generate_inv} $f$ is $\hat{G}_B$-invariant if and only if we reject all non-invariant subgroups. With the breadth-first estimator, this happens if and only if we reject all non-invariant subgroups that are adjacent to the invariant and above only invariant subgroups. Let $\mathcal{A} \subset L$ be the set of such subgroups. Then the probability of interest can be expressed as
	\begin{align}
	\PP( f \text{ is } \hat{G}_B \text{-invariant} ) &= \PP( \cap_{H \in \mathcal{A}} \{ \mathrm{Test}_\alpha( \mathcal{D}, H ) = -1 \} ) \\
	&= 1 - \PP( \cup_{H \in \mathcal{A} } \{ \mathrm{Test}_\alpha( \mathcal{D}, H ) = 1 \} ) \\
	&\geq 1 - \sum_{H \in \mathcal{A} } \PP ( \mathrm{Test}_\alpha( \mathcal{D}, H ) = 1) \\
	&\geq 1 - \sum_{H \in \mathcal{A} } (1 - P_\alpha) = 1 - |\mathcal{A}| (1 - P_\alpha)
	\end{align}
	The reasoning for depth first estimation is similar - $f$ is not $\hat{G}_D$-invariant only if we accept an element of $\mathcal{A}$.
\end{proof}


\begin{proof}[Proof of Lemma \ref{prop:G_hat_leq_G_0}]
	See that $\hat{G}_B = G_{\max}$ if and only if we accept all subgroups of $G_{\max}$ and reject all in $\mathcal{A}$, so the result follows from the same argument as in Proposition \ref{prop:hat_G_inv}.
\end{proof}

\begin{proof}[Proof of Proposition \ref{prop:consistency_G_B}]
	The powers $P_\alpha \nearrow 1$ are an increasing sequence in $n$ for each fixed $\alpha$, and decreasing as $\alpha \searrow 0$. Simply take some fixed $\alpha_0$ and set $\alpha_i = \alpha_{i -1}$ until $P_{\alpha_i} \geq 1 - \alpha_i/2$, then set $\alpha_{i + 1} = \alpha_i / 2$. Then the result follows from Lemmas \ref{prop:hat_G_inv} and \ref{prop:G_hat_leq_G_0}.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:lattice_adding}]
	Since $G \geq H$ for all $H \in K$, we know that $\overline{ \langle G, H \rangle } = \overline{ G } = G \in K \cup \{ G \}$, so we have closure under joins. Similarly, we have that $G \cap H = H \in K \subseteq K \cup \{ G \}$ so we also have closure under meets.
\end{proof}



\begin{proof}[Proof of Proposition \ref{prop:comp_complex}]
	The worst case performance of algorithm \ref{algo:bfe}a is when all tests are accepted. If $\mathcal{G}$ is abelian then $\mathcal{G} \simeq \oplus_{i= 1}^k C_{p_i^{a_i}}$ for some prime powers $p_i^{a_i}$. Let $e_i$ be the natural basis elements of $G_0$, which generate subgroups isomorphic (and hence identified with) to $C_{p_i^{a_i}}$. Each $C_{p_i^{a_i}}$ has $a_i + 1$ subgroups, each of the form $C_{p_i^{a}}$ for $a = 0, 1,  \dots, a_i$. Then there are $\prod_{i = 1}^k (a_i + 1)$ distinct subgroups of the form $\langle C_{p_i^{b_i}} : i \in [k] \rangle$.  Algorithm \ref{algo:bfe}a need only test at the subgroups $C_{p_i^a}$, so tests only $1 + \sum_{i = 1}^k a_i $ of these subgroups, exactly $\prod a_i$ fewer. Testing a subgroup of the form $H_b \simeq \langle C_{p_i^{b_i}} : i \in [k] \rangle$ (using the multi-index $b = (b_1, \dots, b_k)$ ) requires $|H_\beta| = \prod_{i = 1}^k p_i^{b_i}$ units of computation, so testing over all $b$ requires $\sum_b \prod_{i = 1}^k p_i^{b_i}$ computations. Testing only the subgroups $C_{p_i^{b_i}}$ is this sum less the multi indices with more than one $b_i > 0$.
\end{proof}


\subsection{Proofs in Section \ref{sec:testing}}
\label{app:proof4}

The proof of proposition 4.1 relies on several supporting Lemmas.

\begin{Lem}
In the context of proposition \ref{prop:cons}, if the support of $\mu_X$ is dense in $\mathcal{X}$ then
\begin{equation}
d_\mathcal{X} ( g \cdot X_{I(1)}, X_{J(1)} ) \overset{\PP}{\rightarrow} 0	
\end{equation}
\end{Lem}

\begin{proof}
	For any $\eta \in \RR_{\geq 0 }$,  the probability that $d_\mathcal{X} ( g \cdot X_{I(1)}, X_{J(1)} ) \geq \eta$ given $I, g$ is equivalent to a binomial variable $K$ with $n$ trials and probability of success $p = \mu_X( \mathcal{X} \setminus B (g \cdot X_{I(1)}, \eta) )$ being $0$. Since the support of $\mu_X$ is dense in $\mathcal{X}$, $\mu_X( B (g \cdot X_{I(1)}, \eta) ) > 0$, and so
	\begin{equation}
		\PP( d_\mathcal{X} ( g \cdot X_{I(1)}, X_{J(1)} ) \geq \eta ) \leq \PP( K = 0 ) = (1 - p)^n
	\end{equation}
	 As $n \rightarrow \infty$, this clearly goes to $0$ for all $\eta$, and for all $I, g$.
\end{proof}

In the following let $F_X(x)$ be the distribution function of the real valued random variable $X$.

\begin{Lem}
	\label{lem:sym_props}
	If $X$ and $Y$ are independent real random variables, and $Y$ is symmetrically distributed around $0$ (so $F_Y(t) = 1 - F_Y(-t)$ for all $t$) and admits a density $f_Y$ with respect to Lebesgue measure that is decreasing in $|y|$. Then $\PP( |X + Y| \geq t) > \PP( |Y| \geq t )$.
\end{Lem}

\begin{proof}
	First note $\PP( X + Y \geq t) = \EE_X (\PP( Y \geq t - X \mid X) ) = 1 - \EE_X( F_Y(t- X ) )$. Thus
	\begin{align}
		\PP( |X + Y |\geq t ) &= \PP( X + Y \geq t) + \PP( X + Y \leq -t ) \\
			&= 2 - \EE_X( F_Y(t - X) + F_Y(t + X) )
	\end{align}
	Let $\phi_t(x) = F_Y(t - x) + F_Y(t + x)$. Since $Y$ is absolutely continuous (w.r.t. Lebesgue measure) with density function $f_Y$, we can see that $\phi_t$ has a critical point at $x = 0$. Moreover,	if $x > 0$ then
	\begin{equation}
		\phi'_t(x) = f_Y(t + x) - f_Y(t - x) = f_Y(t + x ) - f_Y(-t + x ) <  f_Y(t + x ) - f_Y(-t - x) = 0
	\end{equation}
	as $f_Y$ is decreasing in $|y|$. Similarly $\phi'_t(x) > 0$ if $x < 0$. Thus $x = 0$ is a maximum and so we have
	\begin{equation}
		\PP( |X + Y |\geq t ) = 2 - \EE_X( F_Y(t - X) + F_Y(t + X) ) \geq 2 - \EE_X( F_Y(t) + F_Y(t) ) = \PP( |Y| \geq t ).
	\end{equation}
	as required.
\end{proof}


In the following we use the notation $X \overset{p}{\rightarrow} x$ to indicate that the random variable $X$ converges in probability to $x$.

\begin{Lem}
	\label{lem:binom_likelihood}
	If 	$X \sim Binom(n, p)$ and $Y \sim Binom(n, q)$ with $p < q$ , then $F_X( Y) \overset{p}{\rightarrow} 1$
\end{Lem}

\begin{proof}
	First note that $F_X(Y) = F_{ \tilde{X} } ( \tfrac{Y - n p}{ \sqrt{n p (1 - p) } } )$ where $\tilde{X} = \tfrac{X - n p}{ \sqrt{n p (1 - p) } }$, which has $\tilde{X} \overset{D}{\rightarrow} N(0, 1)$ by the de Moivere - Laplace theorem (DLT). Then we also set:
	\begin{equation}
		\tilde{Y} = \frac{Y - n p}{ \sqrt{n p (1 - p) } } = \left( \frac{Y - n q}{ \sqrt{n q (1 - q) } } + \sqrt{n} \frac{q -  p}{ \sqrt{ q (1 - q) } } \right) \frac{\sqrt{q (1 - q)} }{ \sqrt{ p (1 - p) } }.
	\end{equation}
	Again by DLT, the first term converges to $N(0, \tfrac{q (1 - q) }{p (1 - p} )$, but the second term diverges to $+\infty$ (as $q > p$). Thus $\tilde{Y} \overset{p}{\rightarrow} \infty$ and so clearly $F_{\tilde{X} }( \tilde{Y} ) \overset{p}{\rightarrow} 1$, as required.
\end{proof}


We can now return to the proof of Proposition \ref{prop:cons}.

\begin{proof}[Proof of Proposition \ref{prop:cons}]
	
Since the support of $\mu_X$ is dense in $\mathcal{X}$, Lemma 4 gives $d_\mathcal{X} ( g \cdot X_{I(j)}, X_{J(j)} ) \overset{p}{\rightarrow} 0$. Now consider
\begin{align}
D_{I(j)J(j)}^{g_j} &= | Y_{I(j)} - Y_{J(j)}| - V( g \cdot X_{I(j)}, X_{J(j)} ) \\
	&= | f(X_{I(j)} ) + \epsilon_{I(j)} - f(X_{J(j)}) - \epsilon_{J(j)} | - V( g \cdot X_{I(j)}, X_{J(j)} )
\end{align}
We have that $V( g \cdot X_{I(j)}, X_{J(j)} ) \rightarrow 0$ as the distance goes to $0$, and also 	$f(X_{J(j)}) - f( g \cdot X_{I(j)}) \overset{p}{\rightarrow} 0$. Thus with $X$ as a variable with the same law as each $X_i$ and $g$ with the same law as each $g_j$,
\begin{equation}
	D_{I(j)J(j)}^{g_j} \overset{D}{\rightarrow} | (f(X) + \epsilon_\ell) - f(g \cdot X) - \epsilon_k  | = | \phi(g, X) + \eta |	
\end{equation}
where $\phi(g, X) = f(X) - f ( g \cdot X )$ and $\eta = \epsilon_l - \epsilon_k$. Definitionally, this means
\begin{equation}
\PP( D_{I(j) J(j) }^{g_j} \geq t ) \rightarrow \PP( | \phi(g, X) + \eta | \geq t ) \end{equation}
Under $H_0$, $\phi(g,X) = 0$ almost surely, so this probability is given by $P_t^0 = \PP( |\eta| \geq t)$. Under $H_1$, $\phi$ must take non zero values with some positive probability (using the condition on the distribution $\mu_G$). Thus we can use Lemma \ref{lem:sym_props} to say that
\begin{equation}
	P_t^1 = \PP(  | \phi(g, X) + \eta | \geq t \mid H_1 ) > P_t^0
\end{equation}
Let $N$ be large enough that $\PP( D_{I(j) J(j) }^{g_j} \geq t ) > (P^1_t - P^0_t) / 2 > P_t^0$ for all $n > N$.  Now consider that $N_t^g \mid H_1 \sim \mathrm{Binom}( m, \PP( D_{I(j) J(j) }^{g_j} \geq t \mid H_1 )  )$, which is stochastically bounded from below by $A \sim \mathrm{Binom} (m, (P_t^1 - P_t^0)/2) $ for $n > N$. This gives, using lemma \ref{lem:binom_likelihood}, that
\begin{equation}
	p_{val} \mid H_1 = 1 - F_{ N_t^g \mid H_0 }( N_t^g \mid H_1 ) \leq 1 - F_{N_t^g \mid H_0 } ( A ) \overset{p}{\rightarrow} 0
\end{equation}
as required.
\end{proof}


\section{Group Theory Definitions}
\label{sec:groups}
We give here a quick primer on groups - the mathematical structure used to describe symmetries. These definitions are used frequently in this text, particularly in sections \ref{ssec:invs} and \ref{ssec:lattices}.  \\
\\
A \textbf{group} is set $G$ and an associative binary operation $(g, h) \mapsto gh \in G$ such that $G$ contains an \textbf{identity} $e$ (such that $eg = ge = g$) and \textbf{inverses} $g^{-1}$ for each $g \in G$ such that $g^{-1} g = g g^{-1} = e$. We say that a subset $H$ of a group $G$ is a \textbf{subgroup} of $G$ if it is itself a group. \\
\\
A group encodes the structure of the symmetries of an object through a \textbf{group action}, a function $\cdot : G \times \mathcal{X} \rightarrow \mathcal{X}$ that obeys the rules $e \cdot x = x$ and $g \cdot ( h \cdot x ) = (gh) \cdot x$ for all $g, h \in G$ and $x \in \mathcal{X}$. We note that every group $G$ can act on any space $\mathcal{X}$ trivially, i.e., where $g \cdot x  = x$ for all $g \in G $ and $x \in \mathcal{X}$. We say that an action is \textbf{faithful} if $g \cdot x = x$ for all $x$ only if $g = e$ (or equivalently, every non identity $g \in G$ has some point $x \in \mathcal{X}$ such that $g \cdot x \neq x$). Note that if $G$ acts faithfully then any subgroup of $G$ acts faithfully too.   \\
\\
Let $G$ be a group that acts on $\mathcal{X}$. We have a \textbf{natural quotient map} $[ \cdot ]_G : \mathcal{X} \rightarrow \mathcal{X} / G$ which takes $x$ to its orbit $[x]_G = \{ g \cdot x : g \in G \}$. We also sometimes write $[n] = \{1, \dots, n \}$ and so keep the subscript $G$ for clarity. We can construct a lifting map $\iota_G : \mathcal{X} / G \rightarrow \mathcal{X}$ that has $\iota_G( [x]_G ) \in [x]_G$ for all $x \in \mathcal{X}$. We often write $\iota_G(x) = \iota_G([x]_G)$ for $x\in \mathcal{X}$. Ideally we would like this map to be continuous, though this is often hard to enforce in practice without strong conditions on $G$. We call $\iota_G( \mathcal{X} / G )$ the \textbf{domain} of $(G, \cdot)$ and if $\iota_G$ is continuous then we call it a \textbf{fundamental domain}. We write $\langle A \rangle$ for the group generated by the elements of $A$, i.e., the smallest group containing $A$. \\
\\
Let $G$ be a group that is also equiped with a topology such that the map $(g, h) \mapsto g^{-1} h$ is continuous (as a function from the product $G \times G\rightarrow G$). We call such a group a \textbf{topological group}. When we use homomorphisms of topological groups we mean a continuous homomorphism of the groups. Isomorphisms are homeomorphic homomorphisms. Every group can be given the discrete topology, which we naturally impart to finite groups. We say that a subset $S \subseteq G$ is a \textbf{topological generator} of $G$ if the only closed subgroup of $G$ containing $S$ is $G$ itself (so $G = \overline{\langle S \rangle}$). \\
\\
We say that a group action of a topological group is a \textbf{continuous group action} if the map $\cdot : G \times \mathcal{X} \rightarrow \mathcal{X}$ is continuous with respect to the product topology on $G \times \mathcal{X}$. This implies that each  map $g \cdot : \mathcal{X} \rightarrow \mathcal{X}$ is a continuous bijection on $\mathcal{X}$. Every finite group acts continuously with respect to the discrete topology. Note that if a compact $G$ acts continuously and $H$ is a closed subgroup of $G$ then $H$ acts continuously too (seen by considering composition with the continuous inclusion homomorphism $\phi : H \rightarrow G$ with $\phi(h) = h$).  \\
 \\
If $G$ is compact then there exists a left (and right) invariant Haar measure on $G$ \citep{haar1933massbegriff} which we denote $\Gamma_G$ (or just $\Gamma$ if $G$ is clear from context). This measure can then be normalised to form an analogue of the uniform distribution on a bounded subset of $\RR^d$ (which is in fact the Haar measure on this space if it can be viewed as an additive group, e.g. by identifying $[0,1]^d$ with $\RR^d / \ZZ^d$).

\section{Lattice Theory}
\label{app:lattice_theory}

In section \ref{ssec:lattices} we ignored some of the algebraic properties of lattices. Here we give a few more definitions. \\
\\
Joins and meets are associative, commutative, idempotent binary operations that satisfy the \textbf{absorption laws} $a \wedge (a \vee b) = a$ and $a \vee (a \wedge b) = a$. Here the join is given by the groups generated by unions $\langle A, B \rangle$ and meet by intersections $A \cap B$. We say that $A$ \textbf{covers} $B$ if $A > B$ and there is no $C \in L$ with $A > C > B$. We say that a subset of a lattice $L$ is a \textbf{sub-lattice} of $L$ if it is closed under the joins and meets of the original lattice. Given some set $A \subseteq L$, we can take $\langle A \rangle$ as the smallest sub-lattice of $L$ that contains $A$. \\
\\
When a lattice $L$ is finite we can enumerate the lattice inductively, setting $\mathcal{G}_{0} = \bigwedge_{G \in L} G = \inf L$ and $H \in \mathcal{G}_i$ if $H$ covers some $G \in \mathcal{G}_{i - 1}$. Each $\mathcal{G}_i$ is finite so we can label its elements as $\mathcal{G}_{i,j}$ for $j \in [N_i]$. We can then arrange these into a \textbf{Hasse diagram}, with the elements of $\mathcal{G}_i$ as nodes on level $i$ and edges between nodes $A$ and $B$ if $A$ covers $B$.



\section{Continued Numerical Experiments}
\label{app:testing}
Here we give the details of the simulations for the hypothesis tests presented. These are each from the examples in section \ref{sec:exp} simulations of both the asymmetric variation test and the permutation variant for low dimensional examples and for the orientation of digits in the MNIST dataset ($d = 784$).

\subsection{Simulations in Low Dimensions}
\label{sssec:sims}
Let $d \geq 2$ and set $\mathcal{X} = \RR^d$. Take $X_i \iid N(0,4I_d)$ and $\epsilon_i \iid N(0,\sigma^2 I_d)$. Consider the functions $f_d : \RR^d \rightarrow \RR$ be given by $(x_1, x_2, \dots, x_d) \mapsto \exp( - |x_1| )$. Let $G = \langle R_{\pi/2} \rangle$ act on $\RR^d$ via the distinct actions generated by $ R_{\pi/2} \cdot x = ( - x_2, x_1, x_3, \dots, x_d )$ and $R_{\pi/2} \star x = (-x_1, -x_2, x_3, \dots, x_d) = R_{\pi/2}^2 \cdot x$. Then we know that $f_d$ is $(G, \star)$-invariant but not $(G, \cdot)$-invariant for all $d$. \\
\\
We simulated tests of each of the null hypotheses $H_0^{(1)} : f_d$ is $(G, \star)$-invariant and $H_0^{(2)} : f_d$ is $(G, \cdot)$-invariant, using both tests (algorithms \ref{algo:known} and \ref{algo:perm}). The estimated power (i.e., proportion of rejections of $H_0^{(1)}$) and empirical sizes (i.e., proportion of rejections of $H_0^{(2)}$) are plotted in figure \ref{fig:est_power}, containing rejection probabilities at significance level $\alpha = 0.05$. We ran 100 simulations for each combination of $n = m \in \{ 20,30,40,50,60,70,80,90,100,120,150,200,250,300 \}$. The asymmetric variation tests had $t = 2 \sigma = 0.1$, $p_t = \frac{ 2 \sigma }{ t  } \tfrac{ \exp( - t^2 / 4 \sigma^2 )}{\sqrt{2\pi}}$ and the permutation variants had $B = 100$ and $q = 0.95$. We tested with $g_j \iid U( R_{\pi/2},  R_{\pi/2}^2,  R_{\pi/2}^3 )$ for all tests. \\

\begin{figure}[h]
	\centering
	\begin{tabular}{cc}
		\includegraphics[scale=0.42]{figs/f_2_fig_3.pdf} &
 		\includegraphics[scale=0.42]{figs/f_4_fig_4.pdf} \\
 		(a) Rejection proportions for $f_2$ & (b) Rejection proportions for $f_4$
	\end{tabular}



	\caption{Rejection proportions for $f_d$ for $d \in \{ 2, 4 \}$. Plotted squares are for the asymmetric variation test, and triangles for the permutation variant. Filled shapes are the estimated power (proportions of rejections of $H_0^{(2)}$ ) and unfilled are the empirical sizes (proportions of rejections of $H_0^{(1)}$). The dashed horizontal line is the significance level $\alpha$.}
	\label{fig:est_power}
\end{figure}

We see that both tests seem to converge to an empirical power of $1$, with the asymmetric variation test more powerful. Both tests have roughly (up to the expected variation from 100 simulation) the correct empirical size, though the permutation variation does appear to be slightly liberal for $n \leq 100$.




%\subsection{Example \ref{eg:est_f_sims} with Local Linear Estimators}
%\label{app:LLE_sims}
%
%Since we are considering scenarios that generalise from the training data, local linear estimators struggle, often creating errors that blow up in the sparse data regions. This makes the comparison even more stark in scenario 2. We report the errors for each estimator under the four scenarios in table {\cred REF}. These simulations are done under exactly the same conditions as example \ref{eg:est_f_sims}.
%
%\begin{table}[h]
%	\centering
%	\begin{tabular}{c|ccccccccccc}
%	\textbf{Scenario 1}  & $n = 10$ & $n = 50$ & $n = 100$ & $n = 200$ & $n = 300$ & $n = 400$ & $n = 500$ \\ \hline \hline
%	MSPE of $\hat{f}$ & 56.64758 & 0.1528365 & 0.1328579 & \\
%	MSPE of $\hat{f}_{\hat{G}}^A$ & 0.3043869 & 0.2933401 & 0.2905735 & \\
%	MSPE of $\hat{f}_{\hat{G}}^S$ & 0.3315078 & 0.2986234 & 0.2921726 &
%	\end{tabular}
%	\caption{MSPE of the various estimators under scenario 1}	
%	\label{tab:MSPE_LLE_scen1}
%\end{table}

%\begin{table}
%	\centering
%	\begin{tabular}{c|ccccccccccc}
%	 \textbf{Scenario 2} & $n = 10$ & $n = 50$ & $n = 100$ & $n = 200$ & $n = 300$ & $n = 400$ & $n = 500$ \\ \hline \hline
%	MSPE of $\hat{f}$ &   \\
%	MSPE of $\hat{f}_{\hat{G}}^A$ &  \\
%	MSPE of $\hat{f}_{\hat{G}}^S$
%	\end{tabular}	
%%	\caption{MSPE of the various estimators under scenario 2}
%	\label{tab:MSPE_LLE_scen1}
%\end{table}

%\begin{table}
%	\centering
%	\begin{tabular}{c|ccccccccccc}
%	\textbf{Scenario 3}  & $n = 10$ & $n = 50$ & $n = 100$ & $n = 200$ & $n = 300$ & $n = 400$ & $n = 500$ \\ \hline \hline
%	MSPE of $\hat{f}$ & \\
%	MSPE of $\hat{f}_{\hat{G}}^A$ & \\
%	MSPE of $\hat{f}_{\hat{G}}^S$
%	\end{tabular}	
%%	\caption{MSPE of the various estimators under scenario 3}
%	\label{tab:MSPE_LLE_scen1}
%\end{table}

%\begin{table}
%	\centering
%	\begin{tabular}{c|ccccccccccc}
%	\textbf{Scenario 4}  & $n = 10$ & $n = 50$ & $n = 100$ & $n = 200$ & $n = 300$ & $n = 400$ & $n = 500$ \\ \hline \hline
%	MSPE of $\hat{f}$ & \\
%	MSPE of $\hat{f}_{\hat{G}}^A$ & \\
%	MSPE of $\hat{f}_{\hat{G}}^S$
%	\end{tabular}	
%%	\caption{MSPE of the various estimators under scenario 4}
%	\label{tab:MSPE_LLE_scen1}
%\end{table}


\subsection{MNIST digit orientation test}
\label{sssec:MNIST}

%Consider the symmetries of images of the digits 3 and 8. It is clear that rotations of both preserve the classification, however one can argue that only the 8 is invariant to horizontal reflections - a reflected 3 would be an $\mathcal{E}$\footnote{N.B. We consider the three to be vertically asymmetric so the rotated $3$ is distinct from the reflected $3$} (in the same way that a reflected p is a q), and so can be considered oriented in a way that the 8 is not. We call the in digits $\mathcal{O} = \{ 2, 3, 4, 5, 6, 7, 9 \}$ the \textbf{oriented digits} and the others $\{ 0, 1, 8 \}$ the \textbf{non-oriented digits}. Here we test for the symmetry of the probabilistic labels regression function using the asymmetric variation test. \\
%\\
%We subset the MNIST dataset \citep{lecun1998gradient} (Available under a CC BY-SA 3.0 licence) into particular characters, called $\mathcal{D}_{X}(n)$ for $n \in \{0, 1, \dots, 9 \}$. We then split these in half uniformly at random to form sets $\mathcal{D}_{X}^1(n)$ and $\mathcal{D}^R_{X}(n)$. We then apply $b$ (the reflection through the vertical line) from the dihedral group $D_4 = \langle a, b : a^4 = b^2 = 1, ab = ba^{-1} \rangle$ to the elements of $\mathcal{D}_X^R(n)$ and then assign labels $Y_i = 1$ for every for $X_i \in \mathcal{D}_X^{1}(n)$ for all $n$ and $\mathcal{D}_X^R(n)$ if $n$ is non-oriented, and the label $Y_i = 0$ for every other $X_i$. This gives datasets
%\begin{align}
%\mathcal{D}(n) = &\{ (X_i, 1) : X_i \in \mathcal{D}_X^{1}(n) \} \cup \{ (X_i, \mathbf{1}_{n \not\in \mathcal{O} }) : X_i \in \mathcal{D}_X^{R}(n) \}
%\end{align}
%Here the labels $Y_i$ are the probabilities that $X_i$ will be recognised as the digit $n$, and so we are learning the function $f_n : [0,1]^{784} \rightarrow [0,1]$ that assigns such probabilities. We assume that there is no noise, so $p_t = 0$ for all $t$. Lastly we need to pick $V(x,y)$, which we estimate with the reciprocal of the minimal distances between digits in and out of each class. \\
%\\
This is a continuation of example \ref{eg:MNIST}. We tested each of the datasets $\mathcal{D}(n)$ for symmetry of $D_4 = \langle a, b : a^4 = b^2 = e, bab = a^{-1} \rangle $, and for the subgroups $\langle a \rangle$ of rotations and $\langle b \rangle$ of horizontal reflections. These are done with $g^1 \sim U( \{ a, b \} )$, $g^2 = a$ almost surely and $g^3 = b$ almost surely. The number $N_0$ out of $m = 1000$ samples that have $D_{I(j)J(j)}^g > 0$ is reported in table \ref{tab:mnist}. Since $p_t = 0$, $N_0 > 0$ if and only if the $p$-value is 0 and the hypothesis is rejected. \\
\\
All tests for the invariance of $D_4$ were correctly rejected, as were all tests for the subgroup $\langle b \rangle$. All test for the subgroup $\langle a \rangle$ were accepted. Whether this is correct depends somewhat on the digit and the possible classes the digits can fall into: it is clearly false to rotate the 6 by $180$ degrees because that would usually be classed as a $9$. However, a rotated $4$ can only ever be classed as a four. In either case this is caused by the sparsity in the datasets - the class $\mathcal{D}(6)$ contains only 6s and no 9s to compare against, so don't see the lack of invariance. Thus the test is correct in the sense that we have not seen evidence of no $\langle a \rangle$-invariance.   \\
\\
Note that $N_0$ is higher for the test for $\langle b \rangle$-invariance than for $D_4$-invariance for two reasons. Firstly, sampling from the generating set $\{ a, b \}$ means that $f$ will be invariant to $g^1$ about half of the time, as opposed to sampling $b$ almost surely which means that $f$ is never invariant to $g^3$. Secondly the distribution of the images $\mu_X$ is invariant to $\langle b \rangle$ but not to $D_4$. This means that often the transformed $g^1 \cdot X_i$ is not near to any $X_j$, and so the difference  $|Y_i - Y_j| - V( g \cdot X_i,  X_j)$ is low. In contrast $g^3 \cdot X_i$ is usually close to some $X_j$ by the construction of the dataset which makes it easier to identify if the bounded variation condition is broken.
\begin{table}
\caption{\label{tab:mnist} Calculated $N_0$ for digit orientation tests.}
\centering
\fbox{%
\begin{tabular}{c|cccccccc}
		 Test for digit:    		  	 & 2  & 3  & 4  & 5  & 6   & 7  & 9 \\ \hline 
		 $D_4$ 			  		 & 6  & 17 & 15 & 65 & 168 & 58 & 6  \\
		 $\langle a \rangle$ 	 & 0  & 0  & 0  & 0  & 0   & 0  & 0  \\
		 $\langle b \rangle$ 	 & 6  & 47  & 48  & 137 & 285  & 119 & 15
\end{tabular}}
\end{table}




\end{document}
