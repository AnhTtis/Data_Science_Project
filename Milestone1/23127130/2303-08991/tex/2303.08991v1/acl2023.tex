% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
	
% \documentclass[xcolor=table]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{xcolor,colortbl}
\usepackage{booktabs,tabularx}
\usepackage{graphicx}

% \usepackage{longtable}
% \usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{amsmath}
% Define the name for Section / SubSection / Table / Figure to save some spaces.
\def\sectionautorefname{\S} 
\def\subsectionautorefname{\S} 
\def\appendixautorefname{Appendix}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
\usepackage{multirow}
\usepackage{siunitx}
% \sisetup{table-parse-only,detect-weight=true,detect-inline-weight=text, round-mode=places, round-precision=1, table-number-alignment=center}

\newcommand{\model}[1]{\textsc{#1}\xspace}
\newcommand{\deltascore}{\model{DeltaScore}}

\newcommand{\tbnum}[1]{\multicolumn{1}{c}{\bfseries \num{#1}}}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{xspace}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\newcolumntype{g}{>{\columncolor{Gray}}c}
\newcommand{\Needcite}[1]{{\color{orange}{\bf{[CITE:]}} #1}}
\newcommand{\Zhuohan}[1]{{\color{red}{\bf{[Zhuohan]}} #1}}
\newcommand{\miao}[1]{{\color{blue}{\bf{[Miao:]}} #1}}

\title{\deltascore: Evaluating Story Generation\\ with Differentiating Perturbations}

% \title{PScore: Automatic Story Evaluation via Differences between Language Model Likelihood from Perturbations}

\author{Zhuohan Xie 
 \qquad
 Miao Li
 \qquad
 Trevor Cohn 
 \qquad
 Jey Han Lau\\
 School of Computing and Information Systems, \\
 The University of Melbourne \\
 \{zhuohanx, miao4\}@student.unimelb.edu.au, 
\{t.cohn, jeyhan.lau\}@unimelb.edu.au
 }

\begin{document}
\maketitle
\begin{abstract}

%Automatically evaluating the story quality in AI storytelling is a challenging open problem, and it impedes the development of automatic story generation. %JHL: skipping the cliche
Various evaluation metrics exist for natural language generation tasks, but they have limited utility for story generation since they generally do not correlate well with human judgments and do not measure fine-grained story aspects, such as fluency versus relatedness, as they are intended to assess overall generation quality.
In this paper, we propose \deltascore, an approach that utilizes perturbation to evaluate fine-grained story aspects. Our core idea is based on the hypothesis that the better the story performs in a specific aspect (e.g., fluency), the more it will be affected by a particular perturbation (e.g., introducing typos). To measure the impact, we calculate the \textit{likelihood difference} between the pre- and post-perturbation stories using a language model. We evaluate \deltascore against state-of-the-art model-based and traditional similarity-based metrics across multiple story domains, and investigate its correlation with human judgments on five fine-grained story aspects: fluency, coherence, relatedness, logicality, and interestingness.
Our results demonstrate that \deltascore performs impressively in evaluating fine-grained story aspects, and we discovered a striking outcome where a specific perturbation appears to be highly effective in measuring most aspects.
% We measure the effectiveness of \deltascore against state-of-the-art model-based and traditional similarity-based metrics over several story domains, by comparing how well it correlates with human judgements on 5 fine-grained story aspects (fluency, coherence, relatedness, logicality and interestingness). We found \deltascore performs very well, with the surprising finding that one particular perturbation appears to work very well for measuring most aspects.
%Our extensive experimental show that \deltascore has substantially better correlations against human judgements versus existing state-of-the-art evaluation metrics without any requirements of fine-tuning, and it achieves fine-grained evaluation of story generation with different aspect-aware perturbations.


% as a consequence of the vast range of possible generations
% Automatic story evaluation has long been admittedly a challenging task.
% Evaluation metrics that are originally proposed for 
% other natural language generation (NLG) tasks
% are widely adopted in current story model developing literature.
% However, they have been criticized being not suitable for open-ended generations,
% since they usually show low correlations to human evaluations, which is the de facto
% standard for story evaluation.
% State-of-the-art (SOTA) story evaluation metrics train classifiers to distinguish original stories from the negative samples or
% highly-upvoted stories from lowly-upvoted ones.
% As a result, they obtain one story evaluator that can evaluate story coherence or human preference.
% These approaches only provide one overall score, which do not align well with the multiple aspects of story quality.
% We propose to evaluate story via differences between GPT3 likelihood from perturbations.
% Experiments show our methods have a higher correlations to human evaluations,
% and can provide explainability to multiple aspects.

\end{abstract}


%JHL: after reading the whole paper, I think in terms of narrative introduction we can do something like this: (1) 

\section{Introduction}
\label{sec:introduction}

% \Zhuohan{Aspects are usually highly entangled, highly unlikely we are going to find one perturbation that works on each aspect.}

% \Zhuohan{text simplication}

The advent of large pre-trained language models (PLMs) \citep{radford2019language, lewis-etal-2020-bart, DBLP:conf/nips/BrownMRSKDNSSAA20} has enabled story generation models based on these PLMs to produce plausible stories \citep{tan-etal-2021-progressive, zhang-etal-2022-persona, DBLP:conf/emnlp/YangTPK22}. In fact, the best models have been found to produce stories that are virtually indistinguishable from those written by humans \citep{karpinska-etal-2021-perils, dou-etal-2022-gpt, DBLP:journals/corr/abs-2301-09790}.
However, progress in the development of automatic evaluation metrics has not had the same momentum \citep{guan-etal-2021-openmeva}.
%JHL: I'm commenting out the following line because, in spite of the lack of story evaluation metrics, story generation has progressed much in the past few years, and so the argument that it impedes the development of story generation isn't quite true
%, and the lack of reliable metrics  impedes the development of story generation .
Although human evaluation remains the gold standard, it is often slow, expensive, and difficult to reproduce \citep{DBLP:journals/csur/SaiMK23}. Therefore, there is a pressing need for better automatic methods to evaluate story quality.

\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/Examples-add_typos.pdf}
         \caption{Perturbation ``Add typos'' affects the higher quality story (top) more, and affects the lower quality one (bottom) less.
         }
         \label{fig:addtypos}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/Examples-remove_keyentity.pdf}
         \caption{Stories conditioned on the prompt ``I always go to the local supermarket''. Perturbation ``Remove key entity'' affects the higher quality story (top) more, and affects the lower quality one (bottom) less.
         % Higher quality story is affected more than lower quality one.
         }
         \label{fig:removekeyentity}
     \end{subfigure}
        \caption{
        Scenarios where higher quality stories (top) are 
        affected more than lower quality ones (bottom) through aspect-specific perturbations (fluency: ``Add typos''; relatedness: ``Remove key entity'').
        Generative likelihood for original/perturbed story is in blue/green circle, and
        the \deltascore value is in orange circle. 
        % \miao{Better to give examples that show aspect-aware perturbations. [Changed]}
        }
        \label{fig:examples}
\end{figure}

The predominant evaluation metrics for story evaluation have their origins in other natural language generation (NLG) tasks, including BLEU \citep{papineni-etal-2002-bleu} and NIST \citep{NIST} for machine translation and ROUGE \citep{lin-2004-rouge} for summarization. Nonetheless, recent efforts have led to the development of novel evaluation metrics specifically designed for story evaluation, aiming to quantify story coherence \citep{guan-huang-2020-union, ghazarian-etal-2021-plot} or learn human preferences \citep{DBLP:conf/emnlp/ChenVTMN22}.
Other works have utilized the likelihood of a story under a PLM \citep{DBLP:conf/nips/VaswaniSPUJGKP17, han-etal-2022-go} or its conditional likelihood based on human references or other contextual factors, such as story title \citep{thompson-post-2020-automatic, DBLP:conf/nips/YuanNL21}. However, these approaches typically produce a single score that estimates the overall quality of the generated story. \citet{DBLP:conf/coling/ChhunCSC22} argue that a story's quality is composed of multiple fine-grained aspects, such as fluency and adherence to commonsense, and that an overall quality score has limited utility for story generation. In other words, a metric that produces a low overall score for a story does not reveal whether the story has fluency issues or certain elements that violate commonsense \cite{DBLP:journals/corr/abs-2203-11131}.

In this paper, we propose \deltascore, an approach that assesses story quality by calculating the \textit{likelihood difference} under a language model for a story in its original form (pre-perturbation) vs. one where its text is modified (post-perturbation),\footnote{We experiment with both GPT-3.5 and BART as the two pre-trained language models for computing likelihood in this paper, but the choice of language model is flexible in our approach.} with the idea that higher quality stories will be affected more by the modification/perturbation compared to the lower quality stories.
To provide fine-grained assessment of story quality, we experiment with perturbations that target a particular aspect (e.g.\ fluency).  \autoref{fig:examples} presents two examples to demonstrate the intuition of our approach:
% \miao{Give description about aspect-aware perturbations.} 
(1) 
%the story $\boldsymbol{s_1}$: ``Mike dna Jake aer best friends!'' contains two typos, which are less fluent than the story $\boldsymbol{s_2}$: ``Mike and Jake are best friends!''.
when we modify the two stories  in \autoref{fig:addtypos} by randomly adding typos, the more fluent story (top) is impacted by the perturbation more than the less fluent story (bottom);
% Therefore, 
(2) 
%\textbf{Relatedness}: for the title ``I always go to the local supermarket'', we have two stories,
%the story $\boldsymbol{s_1}$: ``The supermarket has various kinds of goods.'' is highly related to the title while the the story %$\boldsymbol{s_2}$: ``It is a nice day.'' is less related.
when we modify the two stories --- conditioned on the story title ``I always go to the local supermarket'' ---  in \autoref{fig:removekeyentity} by removing key entities, the story that is more related to the title (top) is again more affected. Empirically, we found that \deltascore works very well in measuring these fine-grained story aspects, outperforming state-of-the-art model- and similarity-based metrics.
A noteworthy discovery is that a specific perturbation technique, namely word shuffling, exhibits superior performance in capturing various facets, implying the possibility of interdependence among these fine-grained aspects.
% One surprising finding is that one particular perturbation (word shuffling) appears to work very well to capture multiple aspects, suggesting that the fine-grained aspects may be mutually correlated.
%The key entity ``supermarket'' is removed from $\boldsymbol{s_1}$ while $\boldsymbol{s_2}$ is not affected.
% Perturbation ``Add typos'' is applicable to both stories, while it cause more damages to the higher quality one.
% Different from the previous works,
% we propose to evaluate generated stories with differentiating perturbations,
% which is we calculate the difference of conditional likelihood of the original stories and
% that of their perturbations.
% Besides, we propose to evaluate stories via differences between GPT3 likelihood instead of using likelihood directly as evaluation metrics 
% as we posit that higher quality stories are more sensitive to perturbations than
% lower quality ones.
%There are two benefits in evaluating stories via differences between likelihood:
%1) It can result in a statistically more stable evaluation metric that correlates better to human evaluation.
%2) Different aspects of story quality can be evaluated by designing different perturbations.

%Specifically, we use generative likelihood assigned by GPT-3.5\footnote{It is also called text-davinci-003, which is the largest and latest version of GPT-3 with an available API at the time of our experiments. Note that even though ChatGPT is released and popular all over the world, it does not yet provide an API to obtain its generative likelihood at the time of writing.} as advanced large language models have demonstrated spectacular performance of understanding narratives \citep{Qin2023IsCA}.


% \miao{To evaluate the effectiveness of \deltascore, we ..., and we get that ...}

We summarize our contributions as follows:
\begin{itemize}
% \item We propose a novel evaluation technique that does not require any additional fine-tuning: $\Delta$Likelihood ($\Delta$LL) where we evaluate story quality via differences between generative likelihood from perturbations.
\item We propose the concept of using likelihood difference based on perturbations as a new approach for evaluating stories.
\item We propose a novel, unsupervised evaluation approach to measure fine-grained aspects of story quality.
\item The experimental outcomes indicate that our metric demonstrates a strong correlation with human evaluations in most aspects, surpassing the performance of advanced metrics across various story domains.

% Experimental results show that our metric correlates well with human judgements for most aspects, outperforming state-of-the-art metrics over several story domains.

% \item We demonstrate the technique is applicable to both auto-regressive models with GPT3 and sequence-to-sequence (seq2seq) models with BART.
% \item We explore various perturbations and our results show $\Delta$LLs can have a much better correlations to human evaluation than using likelihood as evaluation metrics directly by applying certain perturbations.
% \item We compare with extensive state-of-the-art (SOTA) evaluation metrics and find our best $\Delta$LL outperforms in all quality aspects of story evaluation.

% \item We show binary classification idea is not suitable for metrics as it can only produce scores that close to 0 and 1s, which is not suitable for story evaluation.
\end{itemize}


\section{Related Work}
\label{sec:relatedwork}

\subsection{Automatic Story Evaluation}
Conventional evaluation metrics mainly measure lexical overlap between generated text and its human reference \citep{papineni-etal-2002-bleu, NIST, lin-2004-rouge}.
Even though such methods can capture shallow surface similarities on word or token levels,
it is vulnerable to small changes in morphology or even typos \cite{kaster-etal-2021-global}.
More recent methods exploit the language understanding capability from large pre-trained models and
capture the semantic similarity by comparing embeddings \citep{zhao-etal-2019-moverscore, DBLP:conf/iclr/ZhangKWWA20}.
These methods work as a general method for natural language generation tasks, 
however, as different tasks have different characteristics, 
automatic metrics tailored for specific tasks such as summarization \citep{scialom-etal-2021-questeval}, data-to-text \citep{rebuffel-etal-2021-data}, and dialogue generation \citep{mehri-eskenazi-2020-usr} have been proposed.
There are also alternative evaluation metrics that evaluate the model holistically by comparing the difference of distributions between the learnt model and the dataset \citep{DBLP:conf/nips/PillutlaSZTWCH21, DBLP:conf/emnlp/DengKR22}

% \paragraph{Story Evaluation Metrics}
Some evaluation metrics are specifically designed for story evaluation.
\citet{guan-huang-2020-union} trains a binary classification to distinguish
original stories and their negative perturbations.
\citet{ghazarian-etal-2021-plot} extends such idea by generating negative samples from manipulated story plots instead of heuristic rules.
These metrics emphasize evaluating story coherence, while
\citet{DBLP:conf/emnlp/ChenVTMN22} propose a new task to evaluate human preference by training the model to differentiate highly-upvoted stories from lowly-upvoted ones. 

% \paragraph{Unified Evaluation Metrics}

Unified evaluation metrics are proposed to provide the capability of 
evaluating multiple quality aspects by offering different combinations of inputs 
such as given prompt, generated text and additional context \citep{DBLP:conf/emnlp/Zhong0YMJLZJH22}. 
\citet{DBLP:conf/nips/YuanNL21} considers text evaluation as a text generation problem 
and \citet{deng-etal-2021-compression} exploits information alignment to evaluate the generated text.
Even though these methods provide the capability of evaluating different aspects of text quality,
such aspects do not normally align well with the text quality aspects.
Therefore,
\citet{DBLP:conf/emnlp/Zhong0YMJLZJH22} considers text evaluation as a question answering problem and 
explore training a language model to answer specific questions such as ``\textit{Is this a fluent sentence?}'' or 
``\textit{Is this a coherent summary to the document?}''.
\citet{golovneva2023roscoe} provides a suite of text evaluation by reasoning step by step.

\subsection{Natural Text Perturbation} 
Perturbation has been a conventional technique for generating negative samples that can be utilized in discriminative \citep{guan-huang-2020-union} or generative \citep{DBLP:conf/emnlp/Zhong0YMJLZJH22} NLP tasks.
\citet{ribeiro-etal-2020-beyond} proposes a CheckList that provides a suite of negative examples constructed from a combination of perturbations to conduct behavioral tests of NLP models.
\citet{sai-etal-2021-perturbation} then extends such perturbation ideas to evaluate robustness of NLG evaluation metrics. 
Similarly, \citet{DBLP:conf/emnlp/KarpinskaRTSGI22} designs a list of perturbation tests to evaluate robustness of evaluation metrics for machine translation.
\citet{DBLP:journals/corr/abs-2212-10020} design perturbation tests to find blind spots of model based evaluation metrics.

Our work is different from the previous efforts in that we apply perturbations to evaluate generated stories and compute the differences in the likelihood pre and post perturbations to assess fine-grained story quality instead of testing robustness.
All above perturbations are based on heuristic rules,
while recent adversarial attacks \citep{li-etal-2020-bert-attack, morris-etal-2020-textattack} apply language models to generate adversarial examples,
which can be considered as another form of text perturbation.
Even though these approaches might produce much more dedicated negative samples,
we do not consider these approaches, since the introduction of an additional black-box language models make the whole process less interpretable.

\section{\deltascore}

% Note: The most ideal way is to find specific perturbations that each works for one specific
% aspect/and some work for general, but I cannot find something like this works now.


%JHL: minor, but maybe some of the perturbation terms can be modified to sound a bit more natural/intuitive:
% Jumble -> Shuffle; GeneralWord -> ? (I thought this is replaced with a hypernym, but looking at the example (girl -> baby), I am not sure anymore); AntonymAttributes -> Antonym; LameWords -> Synonym?; LameSents -> Paraphrase?
\begin{table*}[t]
\centering
\small
\begin{tabular}{p{1cm} p{2cm} p{5.5cm} p{5.5cm}}
\toprule
\textbf{Aspect} & \textbf{Perturbation} & \textbf{Original story} & \textbf{Perturbed story} \\
\midrule
Flu. & Jumble & We play badminton every evening . & badminton every We evening play .\\
\midrule
 Flu. & Typo & he went to see what the problem was & he went to see whta the problem was \\
 \midrule
 Coh. & SentRepeat & after looking at the menu , she ordered a club sandwich . & after looking at the menu , she ordered a club sandwich . she ordered a club sandwich . \\
 \midrule
 Coh. & SentReorder & she did n't intend to buy anything . unfortunately she has poor impulse control ... & unfortunately she has poor impulse control . she did n't intend to buy anything ... \\ 
 \midrule
 Rel. & Negation & both of the boys played guitar . & both of the boys did not play guitar . \\
 \midrule
 Rel. & StoryReplace & they were going to see the star wars ... & she was walking to the food court ... \\
 \midrule
 Log. & Antonym & A small boy playing with a ball. & A tall boy playing with a green ball. \\
 \midrule
 Log. & Commonsense & they took me down to the lake . i threw my line out and caught several worms ... & they took me to the moon. i threw my line out and caught several stars ... \\
 \midrule
 Int. & Synonym & I purchased my uniforms & I bought my uniforms \\
 \midrule
 Int. & Paraphrase & he employed an attorney & he hired a lawyer \\
 \bottomrule
\end{tabular}
\caption{Perturbations that focus on each story quality aspect: Fluency (Flu.), Coherence (Coh.), Relatedness (Rel.), Logicality (Log.), and Interestingness (Int.).
% We organise perturbations from previous works \citep{sai-etal-2021-perturbation, guan-etal-2021-openmeva, DBLP:journals/corr/abs-2212-10020}.
}
\label{table:perturbation}
\end{table*}

% \subsection{GPT3Score}
% Unlike PRISM \citep{thompson-post-2020-automatic} or BARTScore \citep{DBLP:conf/nips/YuanNL21}, we did not use seq2seq pre-trained models.
% GPT3 is a pure generative autoregressive decoder, which do not take condition and predict the current purely based on previous tokens.


%JHL: definition of story condition c seems odd; typically it's the story title or something, no (not sure why it says it can be human reference).
% Answer: For condition, I meant to say people usually use conditional likelihood to evaluate the text (like BARTScore). The condition can be the source, therefore, it is reference-free evaluation. Or the condition can be the reference, therefore, it is refrence-based evaluation.

In the story evaluation process, we calculate the likelihood of the story conditioned on certain context $\boldsymbol{c}$ containing $n$ tokens ($\boldsymbol{c} = c_1, ..., c_n$), which can either be the given prompt $\boldsymbol{p}$ or its human reference $\boldsymbol{r}$, and a generated story $\boldsymbol{s}$ containing $m$ tokens by ($\boldsymbol{s} = s_1, ..., s_m$). 

\subsection{Difference of Likelihoods as the Score}

%JHL: revise this section a bit to move away from GPT3-5, and instead say the formulation is based on autoregressive language models (e.g. GPT3). The core idea of DELTASCORE is likelihood difference, and the specific LM we use is flexible so we should define DELTASCORE in a more general manner.

To calculate the quality score of the generated story $\boldsymbol{s}$, we first perturb it to form a negative sample $\boldsymbol{s}'$, containing $m'$ tokens ($\boldsymbol{s}' = s'_1, ..., s'_{m'}$).

% As GPT-3.5 is good at story narrative understanding, we use it as the language model without fine-tuning to obtain the generative likelihood of both $\boldsymbol{s}$ and $\boldsymbol{s}'$. 
We denote parameters of a generative language model by $\theta$, and we concatenate $\boldsymbol{c}$ and $\boldsymbol{s}$ to form one sequence ($\boldsymbol{x} = x_1, ..., x_{n+m} = c_1, ..., c_n, s_1, ..., s_m$), and  concatenate $\boldsymbol{c}$ and $\boldsymbol{s}'$ to form another sequence ($\boldsymbol{x}' = x'_1, ..., x'_{n+m'} = c_1, ..., c_n, s_1, ..., s_{m'}$).
Therefore, \deltascore is factorized as follows:

\begin{align}
\operatorname{\deltascore}(\boldsymbol{s}) = \frac{1}{m}\sum_{t=n+1}^{n+m}\ \operatorname{log}\ p(x_t|\boldsymbol{x}_{<t}, \theta) \nonumber
\\
- \frac{1}{m'}\sum_{t=n+1}^{n+m'} \operatorname{log}\ p(x'_t|\boldsymbol{x}'_{<t},\theta) \nonumber
\end{align}
where, $p(x_t|\boldsymbol{x}_{<t}, \theta)$ is assigned by the language model, $t$ denotes timestep in the sequence, and $\boldsymbol{x}_{<t}$ denotes all tokens before the current timestep.
Note here we ignore the token probabilities of the condition $\boldsymbol{c}$,
therefore, the timestep starts from $n+1$.
We assign the equal weights to each token for \deltascore following \citet{DBLP:conf/nips/YuanNL21}.\footnote{We also tried various normalization approaches, such as PenLP and NormLP \citep{lau-etal-2020-furiously},
but they did not yield constant performance improvement, 
therefore, we decide to use the uniform normalization.}

In our approach, a pre-trained language model with generative capability is required. Specifically, we select BART and GPT-3.5 (text-davinci-003) as representative models for our experiments.

% For a seq2seq model, we denote its parameters by $\Phi$,
% therefore, $\Delta$LL score on a seq2seq model (S2S-$\Delta$LL) can be factorized as follows:

% \begin{align}
% \operatorname{S2S-\Delta LL}(\boldsymbol{s}) = \frac{1}{m}\sum_{t=1}^{m}\ \operatorname{log}\ p(s_t|\boldsymbol{s}_{<t}, \boldsymbol{c}, \Phi) \nonumber
% \\
% - \frac{1}{m'}\sum_{t=1}^{m'} \operatorname{log}\ p(s'_t|\boldsymbol{s}'_{<t}, \boldsymbol{c}, \Phi)
% \end{align}

% In this paper, we explore GPT3 \citep{DBLP:conf/nips/BrownMRSKDNSSAA20} as the
% representative for auto-regressive models.
% We refer evaluating with the generation likelihood from GPT3 to GPT3Score, therefore:

% \begin{align}
% \operatorname{GPT3Score}(\boldsymbol{s}) = \frac{1}{m}\sum_{t=n+1}^{n+m}\ \operatorname{log}\ p(x_t|\boldsymbol{x}_{<t}, \theta) \nonumber
% \end{align}

% We explore BART \citep{lewis-etal-2020-bart} as the
% representative for seq2seq models.
% Evaluating with the generative likelihood from BART is the same as BARTScore \citep{DBLP:conf/nips/YuanNL21}.

% \begin{align}
% \operatorname{BARTScore}(\boldsymbol{s}) = \frac{1}{m}\sum_{t=1}^{m}\ \operatorname{log}\ p(s_t|\boldsymbol{s}_{<t}, \boldsymbol{c}, \Phi) \nonumber
% \end{align}

\subsection{Perturbations on Story Aspects}

To achieve fine-grained evaluation of story generation, we present aspect-aware perturbations of stories.
We follow \citet{DBLP:journals/corr/abs-2301-09790} to assess fundamental aspects of story quality: fluency, coherence, relatedness, logicality and interestingness.
We roughly classify conventional perturbations \citep{sai-etal-2021-perturbation, guan-etal-2021-openmeva, DBLP:journals/corr/abs-2212-10020} into these aspects based on their features.

%JHL: since we've defined story condition c previously, let's make the terminology consistent throughout the paper when we refer to it (i.e. avoid using another term like 'prompts' to refer to c)
\begin{itemize}
    \item \textbf{Fluency:} Perturbations on fluency usually focus on modifications on word or phrase level, which results in issues within one sentence.
    This usually includes intentionally using incorrect verb tenses, subject-verb agreement or word/phrases repetitions.
    \item \textbf{Coherence:} Perturbations on coherence usually focus on modifications on sentence level while keeping each individual sentence fluent, aiming to make the story have conflicts between sentences.
    This usually includes intentionally repeating one sentence, replacing one sentence from one unrelated story, or reordering all sentences. 
    \item \textbf{Relatedness:} Perturbations on relatedness need to alter the story so that it is less relevant to the story condition.
    It can be achieved by either removing key relevant information relevant to the prompt or replace the entire story by one story written for the different prompt. 
    \item \textbf{Logicality:} Perturbations on logicality need to alter the story so that it disobey commonsense.
    This could include characters that defy the laws of physics or events that go against cultural norms such as ``go trick or treating'' on ``Christmas''.
    \item \textbf{Interestingness:} Perturbations on interestingness need to alter the story so that it is generally less interesting to read.
    This could include changing the tone of the story from dramatic to bland or altering descriptive words with flat ones.
\end{itemize}

% As these quality aspects are not completely orthogonal, a perturbation on one aspect could also impact another aspect even if the perturbation is designed for the original aspect.
% Some quality aspects, especially interestingness, are relatively more subjective and dependent on individual perspectives and preferences. Therefore, our taxnomy might not be applicable to general domains.

Due to the interdependence of these quality aspects, perturbations designed for one aspect can potentially affect other aspects. In particular, aspects such as interestingness, are more subjective and influenced by individual perspectives and preferences. Hence, our taxonomy may not be universally applicable to all domains.

%JHL: i agree with the point about interestingness perhaps being subjective, but don't quite understand the following point (temporarily highlighting this line out for now)
%Answer from Zhuohan: What I am trying to say is that what we believe is interesting / what we believe these perturbations might affect interestingness might not be really applicable to others. Also, these might not be able to affected by our annotations. This also apply to other aspects, such as logicality and relatedness. But in generally, I believe interestingness is the most subjective aspect here.


\section{Experimental Details}

We conduct a comparison between \deltascore and several state-of-the-art automatic evaluation metrics on three meta-evaluation datasets that encompass different domains. These datasets include ROCStories \citep{mostafazadeh-etal-2016-story}, which comprises of concise commonsense stories, WritingPrompts (WP) \citep{fan-etal-2018-hierarchical}, which focuses on fictional stories, and CNN News (CNN) \citep{DBLP:conf/nips/HermannKGEKSB15}, which pertains to news stories.

%JHL: ahhhh i am a little surprised to see the CNN story condition is human story here! This is not good, the prompt qualitatively is far too different from the other domains (i.e. in ROC and WP the story condition is a short text, but the CNN story condition has a lot more information and so there's less room in terms of 'creative writing' for the model. This may be the reason why CNN/Dailymail has very very different results! We might need to redo CNN experiments. Also, I suspect we'll run into issues with reference-based evaluation for CNN/dailymail (since the reference is only used as story condition)
%JHL: prompt is never really defined (I understand it's introduced in section 3 but it wasn't really defined there too); suggest we say explicitly what are the c's for the different domains here (e.g. ROC = first sentence; WP = a short prompt that gives the big picture of the whole story; etc and show some examples).
Open-ended generation tasks like story generation often suffer from the known challenge of the one-to-many problem, where multiple plausible stories can be generated for a single prompt. Consequently, reference-based evaluation metrics, which are commonly used in other generation tasks, may not be ideal for evaluating story generation \citep{DBLP:conf/emnlp/Zhong0YMJLZJH22}. Hence, to address this issue, we utilize the given prompt $\boldsymbol{p}$ as the conditioning factor $\boldsymbol{c}$ for the story generation. More specifically, for ROC, we use the first sentence as the prompt, for WP we use a short prompt that outlines the expected story, and for CNN, we use the news title as the prompt. We give some examples in \autoref{table:dataexample}.

\begin{table}[t]
\centering
\small
\begin{tabular}{p{0.9cm} p{2.3cm} p{3.1cm} }
\toprule
\textbf{Dataset} & \textbf{Prompt} & \textbf{Story}  \\
\midrule
ROC & [FEMALE] dad took me fishing . & we sat in a spot and waited for days ... \\
\midrule
 WP & tell me a story where the first line and last line ... & as i walked into the house , i was assailed by the smell of aging ... \\
 \midrule
 CNN & (cnn) -- a 6-year-old girl , detained in arizona ... & the 25-year-old man , of kannapolis , north carolina , was arrested in ... \\
 \bottomrule
\end{tabular}
\caption{Sampled Examples of prompts and stories for each dataset.
}
\label{table:dataexample}
\end{table}

\subsection{Meta-Evaluation Benchmarks}

The use of standardized benchmark datasets is crucial for advancing automatic story evaluation as they enable the evaluation metrics to be tested and compared by measuring their correlations to human evaluations.

% \paragraph{Fine-grained Aspect Scores}

We obtain human evaluation results from \citet{DBLP:journals/corr/abs-2301-09790}, who conducted human annotations on the previous discussed datasets.
% We follow \citet{guan-etal-2021-openmeva} and use ROC and WP, and
% include one new domain: CNN News (CNN) \citep{DBLP:conf/nips/HermannKGEKSB15}.
% We collect human judgements on AMT,
% and we set following qualification requirements for our annotators:
% 1) Their acceptance rate is greater than or equal to 97\%;
% 2) Their location is in the US; and
% 3) They have to complete more than 1000 HITs.
% We randomly sample 20 titles/prompts for each dataset for each group.
They evaluate a range of recently proposed story generation models:
1) \textbf{KGGPT2} \citep{guan-etal-2020-knowledge},
2) \textbf{MTCL} \citep{xu-etal-2020-megatron},
3) \textbf{HINT} \citep{guan-etal-2021-long},
4) \textbf{PROGEN} \citep{tan-etal-2021-progressive},
5) \textbf{BART} \citep{lewis-etal-2020-bart} and
6) \textbf{GPT-3} \citep{DBLP:conf/nips/BrownMRSKDNSSAA20},
and also human reference stories.
Annotators are asked to rate each of the five aspects (fluency, coherence, relatedness, logicality, and interestingness) on an ordinal scale of 1 (worst) to 5 (best). The authors conduct a meta-evaluation, which consists of two parts: crowdsourcing through Amazon Mechanical Turk and in-house evaluation by colleagues. We primarily rely on the crowdsourced evaluations since the authors suggest that in-house annotators may have a natural bias towards assigning higher scores to machine-generated stories.
% The details can be found in \Needcite{Appendix}.

% \paragraph{Overall Quality Scores}

% %JHL: given the narrative in the intro, I no longer think we need this set of results (OPENMEVA). [Deleted]
% We also use manually annotated stories from OpenMEVA \citep{guan-etal-2021-openmeva}, which covers ROC and WP.
% It contains various generation models including: 
% 1) a \textbf{Seq2Seq} model \citep{DBLP:conf/nips/SutskeverVL14},
% 2) \textbf{Fusion} \citep{fan-etal-2018-hierarchical},
% 3) \textbf{Plan\&Write} \citep{DBLP:conf/aaai/YaoPWK0Y19},
% 4) fine-tuned \textbf{GPT-2} \citep{radford2019language} and
% 5) \textbf{KGGPT2}.
% % They randomly sample 200 stories from test sets of ROC and WP for story generation,
% % therefore,
% % MANS contains 2 $\times$ 200 $\times$ 5 = 2,000 annotated stories.
% % They use Amazon Mechanical Turk (AMT)\footnote{\url{https://requester.mturk.com/}} for human judgements and
% They ask the annotators to rate each story with a 5-point Likert scale in terms of the overall quality.\footnote{
% The authors of OpenMEVA also asked annotators to select the types of errors in the story, such as
% \textit{repetitive plots},
% \textit{unrelated events},
% \textit{conflicting logic},
% or \textit{chaotic scenes}.
% However, these error types have not bee released and as such we only use overall quality labels in this work.}

% We randomly sample 20 conditional contexts (e.g.,\ titles) from each dataset 
% and collect stories generated by all models for human evaluation.
% Each story (including human-written one) is judged by 3 annotators, 
% and so we have 320 annotated stories in total (140/100/80 for ROC, WP and CNN, respectively).

% Recently, \citet{DBLP:conf/coling/ChhunCSC22} propose
% a new story evaluation benchmark, HANNA.
% However, they present both human reference and generated story to annotators in AMT
% and ask annotators to judge the generated story.
% We concern that such way can make annotators give higher scores to stories that
% ``reads similar'' to given human reference and thus,
% favoring reference-based evaluation metrics and their results do show
% reference-based evaluation metrics have a better performance on HANNA.
% Therefore, we do not include HANNA as another baseline for GPTScore.

\subsection{Compared Evaluation Metrics}
We compare \deltascore against representative evaluation metrics introduced in \autoref{sec:relatedwork}.
While some of these metrics were developed for other NLG tasks, we modify them slightly to make them suitable for evaluating story generation.

\textbf{BARTScore} \citep{DBLP:conf/nips/YuanNL21} evaluates generated text as a text generation task by calculating the conditional likelihood of the generated text under BART. This metric can evaluate various aspects of text quality by using different combinations of input conditions and output. We use the reference-free version of BARTScore in the direction of $\boldsymbol{c} \rightarrow \boldsymbol{s}$.

% Might delete CTC if we lack space.
\textbf{CTC} \citep{deng-etal-2021-compression} evaluates generated text as an information alignment task,
and they propose various approaches for alignment calculations.
In our evaluation, we use the reference-free alignments, which is the ``consistency'' version of CTC.

% We calculate both CTC (Consistency), which is s $\rightarrow$ c and
% CTC (Relevance), which is the product of alignments r $\rightarrow$ s and s $\rightarrow$ c,
% because the other settings in CTC are not applicable to our tasks.
% Our results show that CTC (Consistency) works consistently better than CTC (Relevance) (see Appendix).
% It shows references are not that important in open-ended story evaluation. 
\textbf{UNIEVAL} \citep{DBLP:conf/emnlp/Zhong0YMJLZJH22} evaluates generated text as a question answering task, where different questions are asked for each aspect. The evaluation models are trained on text summarization and dialogue generation tasks and are shown to have zero-shot transfer ability to data-to-text generation task by asking corresponding new questions. In our story evaluation, we ask story quality-related questions for each aspect, which are listed in \autoref{appendix: unieval}.


\textbf{UNION} \citep{guan-huang-2020-union} frames story evaluation as a classification task by constructing negative samples of original stories using heuristic rules, and training a neural discriminator to differentiate them. The trained model is then used for evaluating new stories. While their work focuses on the ROC and WP datasets, we do not fine-tune the UNION model on our own dataset. Instead, we use the model trained on ROC for our ROC dataset, and the model trained on WP for both our WP and CNN datasets.

% \textbf{UNION} \citep{guan-huang-2020-union} treats story evaluation as a classification task.
% They first use heuristic rules to construct negative samples of the original stories and 
% train a neural discriminator to differentiate them.
% Then the trained model is used to evaluate new stories.
% They work on ROC and WP.
% Note that we did not train UNION further on our own dataset.
% We use model trained on ROC for our ROC dataset and 
% model trained on WP for both our WP and CNN datasets.

\begin{table}[t]
\centering
\small
\begin{tabular}{p{1.5cm} p{0.5cm} p{4cm} }
\toprule
\textbf{Model} & \textbf{Size} & \textbf{Training Dataset}  \\
\midrule
Bart-large & 400M & XSum, WikiText, and others \\
\midrule
 Bart-large.cnn & 400M & Bart-large fine-tuned on CNN-DM \\
 \midrule
 GPT-3.5 & 175B & web pages, books, and articles from a wide range of domains \\
 \bottomrule
\end{tabular}
\caption{Size and training dataset for each model.
}
\label{table:modeldetails}
\end{table}

\textbf{MANPLTS} \citep{ghazarian-etal-2021-plot} is an extension of UNION that focuses on constructing negative samples that are more similar to machine-generated stories. They do this by manipulating storylines and generating stories based on these manipulated storylines using a story generation model. Like UNION, we use the models trained on the ROC dataset for our ROC dataset and the models trained on the WP dataset for both the WP and CNN datasets.

\textbf{StoryER} \citep{DBLP:conf/emnlp/ChenVTMN22} combines the approach of separating original stories from negative samples, as in UNION and MANPLTS, with considering human preference by training the model to differentiate highly-upvoted stories from lowly-upvoted ones.
In addition to the preference score, they train their model to produce ratings and comments on predefined story aspects to provide explanations for the evaluation.
%JHL: what is story comments?
% Answer: They additional generate comments on each aspect as explanations.

We also incorporate conventional similarity-based metrics, including:
\textbf{BLEU} \citep{papineni-etal-2002-bleu}, which measures n-gram overlaps between stories and human references,
\textbf{BERTScore} \citep{DBLP:conf/iclr/ZhangKWWA20}, and
\textbf{MoverScore} \citep{zhao-etal-2019-moverscore}, which measures semantic similarity from embeddings obtained from BERT \citep{devlin-etal-2019-bert}.

\subsection{Perturbation Details}

We follow previous works \citep{sai-etal-2021-perturbation, guan-etal-2021-openmeva, DBLP:journals/corr/abs-2212-10020} in applying perturbations to the generated stories. We can control the degree of perturbation for some of the perturbations, as demonstrated in detail in \autoref{subsec:PerturbationDegree}.

In addition, we propose two perturbations targeting two separate aspects, relatedness and logicality.
To address the relatedness aspect, we propose replacing the entire story. However, randomly replacing the story is not feasible as we cannot guarantee the relevance to the given prompts or the quality of the replaced stories. To mitigate this, we select all stories that are produced from different prompts for each given story and compute their likelihood without prompts from GPT-3.5 to assess their quality. We then calculate the absolute value of the difference between the current story and all other stories and select the one with the closest quality.

For logicality, we propose replacing certain elements of a story to create a violation of commensense while maintaining its fluency and coherence. To achieve this, we rely on ChatGPT by asking it to revise the story to create minimal changes that do not make sense using the prompt ``Revise the following story such that certain elements does not make sense. The revision should be minimal, e.g. by changing a few words.''. Examples of these revisions can be found in \autoref{table:perturbation}, and we test more perturbations including Add Negation (AddNe), Antonmy (Anton).



\section{Results}

%JHL: Here we should talk about the language models we will use, e.g. GPT-3 (text-davinci-003) and BART (size?). 
%JHL: we also need to say a bit more about the implementation of the different perturbations, e.g. how are word shuffling done? we select N% words in the text and randomly shuffle them, or how many typos we create, and how do we create them.  The high level idea of perturbation is explained in 3.2, but here we need to spell out a bit more explicitly how they are actually implemented. If space is tight, we can move this to appendix, but right now it's unclear how the perturbations are implemented.

\begin{table*}[t]
\small
\centering
\begin{tabular}{lccccccccccccccc}
\toprule
\multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
\cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
  & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
 % & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ \\
\cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
  \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
 BLE & 4.8 & 5.8 & 2.0 & 7.0 & 0.8 & 27.0 & 19.6 & 19.8 & 21.2 & 14.3 & 15.3 & 0.9 & 3.4 & 0.3 & 5.3 \\ 
 BER & 5.5 & 7.3 & 19.7 & 7.8 & 9.9 & 29.6 & 27.7 & 23.3 & 29.2 & 28.2 & 4.4 & 2.5 & 11.1 & 1.9 & 6.7 \\
 Mov & 4.9 & 9.1 & 22.6 & 11.0 & 15.2 & 14.1 & 22.4 & 22.3 & 21.3 & 28.5 & 16.8 & 18.5 & 9.3 & 17.5 & 0.7 \\
 \midrule
 \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
UNIO & 0.6 & 17.6 & 17.6 & 1.4 & 6.1 & 29.2 & 35.4 & 29.9 & 36.9 & 39.6 & 10.0 & 1.6 & 24.5 & 4.2 & 1.6 \\  
MAN & 30.8 & 45.5 & 31.8 & 21.5 & 17.7 & 0.1 & 8.2 & 3.5 & 4.9 & 12.8 & 8.5 & 27.0 & 18.3 & 25.5 & 16.5 \\ 
StoER & 9.7 & 6.1 & 6.0 & 5.8 & 14.3 & 25.8 & 23.7 & 18.9 & 31.2 & 34.5 & 6.7 & 6.6 & 0.1 & 10.5 & 8.4 \\
 \midrule
 \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
 CTC & 31.7 & 37.8  & 21.7  & 17.0  & 12.2 & 60.0 & 70.1 & 52.1 & 72.8 & \textbf{63.8} & 40.0 & 52.1 & 56.2 & 58.6 & 44.6 \\ 
 UNIE & 44.3 & 45.2 & 33.3 & 29.0 & 26.6 & 52.6 & 56.8 & 50.9 & 67.4 & 52.7 & 33.9 & 43.6 & 50.4 & 53.2 & 37.3 \\
 \midrule
\multicolumn{12}{l}{\textbf{Likelihood Based Evaluation Metrics}}\\
BRT-l & 15.7 & 1.9 & 1.8 & 3.9 & 3.3 & 7.4 & 6.8 & 1.8 & 4.7 & 11.5 & 21.0 & 23.2 & 29.3 & 21.3 & 9.2 \\ 
BRT-c & 34.4 & 15.8 & 15.4 & 15.7 & 19.8 & 31.3 & 26.5 & 25.0 & 31.6 & 22.6 & 48.2 & 39.7 & 38.0 & 46.8 & 28.2 \\
G-3.5 & 43.2 & 39.5 & 35.4 & 29.9 & \textbf{27.4} & 33.5 & 40.8 & 34.1 & 43.9 & 43.1 & 45.2 & 60.5 & 57.9 & 63.2 & \textbf{57.9} \\ 
 \midrule
 \multicolumn{12}{l}{\textbf{\deltascore (with BART-large)}}\\ 
 Jumble & \cellcolor{green!25}22.8 & \cellcolor{green!25}8.5 & \cellcolor{green!25}11.5 & \cellcolor{green!25}11.6 & \cellcolor{green!25}4.0 & \cellcolor{green!25}8.7 & \cellcolor{green!25}6.9 & \cellcolor{green!25}7.1 & 2.5 & 10.6 & \cellcolor{green!25}32.8 & \cellcolor{green!25}29.3 & \cellcolor{green!25}34.2 & \cellcolor{green!25}30.6 & \cellcolor{green!25}33.1 \\
 Typo & 9.6 & \cellcolor{green!25}4.6 & \cellcolor{green!25}6.2 & \cellcolor{green!25}5.4 & 2.7 & 3.0 & 3.0 & 1.5 & \cellcolor{green!25}7.0 & 9.0 & 19.9 & 4.0 & 1.2 & 8.5 & 7.5 \\
 AddNe & \cellcolor{green!25}17.2 & \cellcolor{green!25}21.3 & \cellcolor{green!25}10.0 & \cellcolor{green!25}4.0 & \cellcolor{green!25}6.3 & 6.6 & \cellcolor{green!25}11.4 & \cellcolor{green!25}6.0 & \cellcolor{green!25}9.6 & 9.1 & 8.7 & \cellcolor{green!25}23.5 & 6.8 & 17.9 & \cellcolor{green!25}26.1 \\
 Anton & 11.3 & \cellcolor{green!25}13.7 & 0.4 & \cellcolor{green!25}18.5 & \cellcolor{green!25}16.3 & \cellcolor{green!25}15.4 & \cellcolor{green!25}21.7 & \cellcolor{green!25}11.0 & \cellcolor{green!25}11.1 & 9.5 & \cellcolor{green!25}26.4 & 13.7 & 21.0 & 14.6 & 8.7 \\
 % RpSen & 6.9 & 5.7 & 3.2 & 7.6 & 6.6 & 1.3 & 2.5 & 5.9 & 2.0 & 1.6 & 24.7 & 23.0 & 27.2 & 18.2 & 12.5 \\ 
 % RrSen & 3.1 & 1.9 & 3.0 & 5.5 & 3.1 & 11.8 & 5.8 & 5.1 & 5.2 & 6.4 & 18.7 & 7.8 & 9.6 & 14.7 & 9.5 \\
 RpSt & \cellcolor{green!25}21.7 & \cellcolor{green!25}14.2 & \cellcolor{green!25}9.3 & \cellcolor{green!25}11.2 & \cellcolor{green!25}7.3 & \cellcolor{green!25}12.2 & 1.2 & \cellcolor{green!25}13.3 & 3.4 & 4.7 & \cellcolor{green!25}23.1 & \cellcolor{green!25}30.7 & \cellcolor{green!25}39.3 & \cellcolor{green!25}30.4 & \cellcolor{green!25}22.9 \\
 CS & 4.5 & 0.2 & \cellcolor{green!25}4.2 & \cellcolor{green!25}10.1 & \cellcolor{green!25}12.2 & \cellcolor{green!25}8.7 & 4.1 & \cellcolor{green!25}5.1 & 4.6 & 1.7 & 8.1 & 0.0 & 3.6 & 0.1 & 0.8 \\
\midrule
\multicolumn{12}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
 Jumble & \cellcolor{green!25}37.4 & \cellcolor{green!25}22.6 & 15.0 & 14.7 & 14.1 & \cellcolor{green!25}60.4 & \cellcolor{green!25}53.7 & \cellcolor{green!25}50.5 & \cellcolor{green!25}59.5 & \cellcolor{green!25}52.7 & \cellcolor{green!25}49.9 & \cellcolor{green!25}44.4 & \cellcolor{green!25}41.2 & \cellcolor{green!25}54.0 & 23.5 \\
 Typo & 31.9 & 14.9 & \cellcolor{green!25}18.0 & 11.1 & 16.0 & \cellcolor{green!25}43.7 & \cellcolor{green!25}38.9 & \cellcolor{green!25}37.6 & \cellcolor{green!25}43.7 & \cellcolor{green!25}32.9 & \cellcolor{green!25}55.4 & \cellcolor{green!25}42.8 & \cellcolor{green!25}46.3 & \cellcolor{green!25}54.2 & \cellcolor{green!25}32.1 \\
 AddNe & 11.8 & \cellcolor{green!25}23.6 & \cellcolor{green!25}25.1 & \cellcolor{green!25}16.4 & 13.9 & 17.0 & 22.6 & 9.2 & 23.8 & 22.6 & 27.0 & 19.4 & 20.6 & 24.7 & 14.2 \\
 Anton & 20.1 & \cellcolor{green!25}30.7 & 14.4 & \cellcolor{green!25}25.4 & 19.8 & \cellcolor{green!25}56.2 & \cellcolor{green!25}55.2 & \cellcolor{green!25}58.0 & \cellcolor{green!25}61.2 & \cellcolor{green!25}50.5 & 13.1 & 15.5 & 5.5 & 15.2 & 14.0 \\
 % RpSen &  \\ 
 % RrSen &  \\
 RpSt & 1.6 & \cellcolor{green!25}16.3 & 11.2 & \cellcolor{green!25}23.1 & 14.4 & 14.6 & 12.5 & 19.0 & 17.4 & 20.8 & 39.4 & 34.9 & \cellcolor{green!25}39.4 & 46.0 & 20.6 \\
 CS & 0.6 & 10.4 & 9.5 & \cellcolor{green!25}20.6 & 14.1 & \cellcolor{green!25}36.7 & \cellcolor{green!25}33.7 & \cellcolor{green!25}29.5 & 28.6 & 21.1 & \cellcolor{green!25}53.6 & \cellcolor{green!25}51.0 & 35.9 & \cellcolor{green!25}51.1 & \cellcolor{green!25}41.0 \\
\midrule
 \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
 Jumble & \cellcolor{green!25}\textbf{50.4} & \cellcolor{green!25}\textbf{47.3} & \cellcolor{green!25}43.5 & \cellcolor{green!25}\textbf{33.3} & 25.5 & \cellcolor{green!25}71.6 & \cellcolor{green!25}\textbf{71.3} & \cellcolor{green!25}\textbf{53.7} & \cellcolor{green!25}\textbf{74.2} & \cellcolor{green!25}59.8 & \cellcolor{green!25}54.0 & \cellcolor{green!25}\textbf{69.0} & \cellcolor{green!25}61.2 & \cellcolor{green!25}\textbf{71.4} & 47.1 \\ 
 Typo & \cellcolor{green!25}47.6 & \cellcolor{green!25}42.3 & \cellcolor{green!25}41.1 & 28.5 & 22.8 & \cellcolor{green!25}56.9 & \cellcolor{green!25}57.1 & \cellcolor{green!25}45.9 & \cellcolor{green!25}62.1 & \cellcolor{green!25}51.2 & \cellcolor{green!25}\textbf{55.4} & \cellcolor{green!25}65.6 & \cellcolor{green!25}\textbf{65.9} & \cellcolor{green!25}69.5 & 57.1 \\ 
 AddNe & 30.7 & \cellcolor{green!25}43.6 & \cellcolor{green!25}\textbf{49.1} & 26.5 & 26.7 & \cellcolor{green!25}38.7 & 35.1 & 21.8 & 41.9 & 35.3 & \cellcolor{green!25}48.0 & \cellcolor{green!25}60.8 & \cellcolor{green!25}65.0 & 62.9 & 46.1  \\
 Anton & 32.2 & \cellcolor{green!25}40.1 & 26.4 & 29.7 & 19.4 & \cellcolor{green!25}\textbf{72.9} & \cellcolor{green!25}67.7 & \cellcolor{green!25}56.4 & \cellcolor{green!25}71.4 & \cellcolor{green!25}56.9 & \cellcolor{green!25}48.3 & 59.4 & 52.7 & \cellcolor{green!25}64.6 & 40.5 \\
 % RpSen & 43.1 & 36.8 & 26.7 & 19.0 & 21.2 & 4.7 & 3.0 & 1.3 & 2.7 & 10.0 & 10.0 & 12.9 & 23.2 & 13.9 & 1.6 \\
 % RrSen & 30.7 & 34.5 & 22.8 & 22.0 & 16.9 & 3.7 & 19.2 & 7.9 & 13.1 & 11.8 & 18.6 & 11.5 & 6.3 & 2.6 & 6.7 \\
 RpSt & 0.8 & 2.7 & 8.8 & 15.1 & 11.2 & 25.2 & 26.4 & 28.7 & 30.6 & 35.0 & 31.4 & 42.6 & 52.4 & 53.8 & 42.6 \\
 CS & 15.6 & 15.3 & 19.3 & 1.0 & 0.4 & \cellcolor{green!25}37.5 & \cellcolor{green!25}47.1 & \cellcolor{green!25}35.6 & 40.7 & 32.1 & 34.4 & 55.7 & 55.6 & 50.6 & 40.7 \\
 \bottomrule
\end{tabular}
\caption{Absolute value of Story-level Spearman correlation ($|\rho|$) between different metrics and human evaluations on three CoudSourcing datasets. 
We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
}
\label{table:crowdsource_spearman}
\end{table*}

\begin{figure*}[t]
     \centering
     \begin{subfigure}[b]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/BARTScore_perturbations_roc.pdf}
         \caption{ROC
         }
         \label{BARTScore-ROC}
     \end{subfigure}
     \begin{subfigure}[b]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/BARTScore_perturbations_wp.pdf}
         \caption{WP
         }
         \label{BARTScore-WP}
     \end{subfigure}
     \begin{subfigure}[b]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/BARTScore_perturbations_cnn.pdf}
         \caption{CNN
         }
         \label{BARTScore-CNN}
     \end{subfigure}
        \caption{
        We test the impact of different degrees of perturbations on \deltascore. 
        We demonstrate with two representative perturbations: jumble and typos on BART.
        We show Spearman correlations with average score of human evaluations on each crowdsourcing dataset.
        }
        \label{fig:correlations}
\end{figure*}


\begin{figure*}[t]
     \centering
     \begin{subfigure}[b]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/heatmap_bart-large_roc.pdf}
         \caption{ROC with BART-large
         }
         \label{ROC+BART-large}
     \end{subfigure}
     \begin{subfigure}[b]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/heatmap_gpt-3.5_wp.pdf}
         \caption{WP with GPT-3.5
         }
         \label{WP+GPT-3.5}
     \end{subfigure}
     \begin{subfigure}[b]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/heatmap_bart-large.cnn_cnn.pdf}
         \caption{CNN with BART-large.cnn
         }
         \label{CNN+BART-large.cnn}
     \end{subfigure}
        \caption{
        To demonstrate the effectiveness of \deltascore on different aspects, we present heatmaps for each dataset showing the improvement in scores after applying \deltascore.
        }
        \label{fig:heatmaps}
\end{figure*}

%JHL: we might not need to have different subsections for the two sets of results, if we end up discussing the results altogether
%JHL: suggestion: use a paragraph to spell out all the tables (2-X), and then discuss the results, and then present additional analysis (e.g. the text in 6.2 and 6.4)
% \subsection{Fine-grained Aspect Evaluation Results}



Table \ref{table:crowdsource_spearman} displays the Spearman correlations between the human annotations and automatic evaluation for all three datasets. Given the non-linear nature of the correlation between human evaluation and automatic evaluation, Spearman correlation was used.
We categorize the evaluation metrics into several types based on their features: 1) Similarity-based metrics, including BLEU (BLE), BERTScore (BER), and MoverScore (Mov); 2) Metrics that are specifically tailored for evaluating stories, including UNION (UNIO), MANPLTS (MAN), and StoryER (StoER); 3) Unified metrics designed to evaluate multiple aspects, including CTC and UNIEVAL (UNIE); and 4) Likelihood-based metrics, where the language models' likelihood is used directly, including BART-large (BRT-l), BART-cnn (BRT-c), and GPT-3.5 (G-3.5).\footnote{Please note that the concept of using likelihood is identical to that of BARTScore proposed by \cite{DBLP:conf/nips/YuanNL21}. The authors of BARTScore suggest that this idea can be extended to other generative models as well.} We also present the \deltascore metric using these three generative models.

\subsection{Metric Results}

After analyzing the results, it can be observed that similarity evaluation metrics, including BLEU, BERTScore, and MoverScore, exhibit poor performance in open-ended story evaluation, which is consistent with prior research studies \citep{guan-huang-2020-union, DBLP:journals/corr/abs-2301-09790}. This finding provides evidence that the presence of reference is not crucial in the assessment of open-ended stories.

Surprisingly, even evaluation metrics that are specifically designed for assessing stories, such as UNION, MANPLTS, and StoryER, do not exhibit impressive results. This could potentially be attributed to the fact that these metrics require extensive training on synthetic data, and we did not further train them on our own data. It is possible that our data has different features compared to the synthetic data used for their training, which could explain their poor performance in our evaluation scenarios.
Notably, recently proposed unified evaluation metrics have yielded promising results. Specifically, UNIEVAL demonstrates superior performance in the ROC dataset, whereas CTC performs better in the WP and CNN datasets. It appears that assessing alignments and asking questions can be effective approaches for evaluating stories.

Regarding the likelihood-based metrics, GPT-3.5 generally performs the best, while BART-large exhibits the worst performance. This finding highlights the critical role that the training dataset plays in the ability of the model to evaluate stories. The more plausible stories the model has seen during training, the better its capability will be in assigning higher scores to better stories.

We demonstrate \deltascore and observe that jumble and typos produce the most stable results across different models and datasets. Notably, when we use jumble as the perturbation for \deltascore with GPT-3.5, we observe a significant improvement over using the likelihood from GPT-3.5 directly as the metric. Moreover, this approach outperforms all other SOTA evaluation metrics on most aspects across all datasets.
Replacing the entire story does not improve the performance of \deltascore over directly using the likelihood metric with GPT-3.5. However, it does provide some improvement for BART-cnn and a substantial improvement for BART-large. This may be due to the fact that stronger models are more focused on local context and less sensitive to changes in the prompt.
Perturbing stories to violate commonsense using ChatGPT does not lead to a significant improvement in the performance of \deltascore compared to using the likelihood metric directly with GPT-3.5, except for in the WP dataset. However, it appears to provide some improvement for BART-cnn and BART-large. This may be due to the fact that when the perturbed stories violate commonsense, GPT-3.5 exhibits similar characteristics to ChatGPT, which results in high likelihood scores being assigned to the contents generated by ChatGPT.

In general, the quality aspects of stories are heavily intertwined, meaning that perturbations targeting one aspect often impact others as well. Nonetheless, we find that interestingness is particularly challenging to address with perturbations, as it is difficult to identify suitable modifications that effectively target this aspect.


% \subsection{Overall Quality Evaluation Results}

%JHL: think we can drop this section; see my comments below how to handle each subsection
% \section{Analysis}

% \subsection{Generalization}

%JHL: we can drop most of the text here if we introduce DELTASCORE in a general manner previously; we just need to say we will experiment with both GPT3 and BART in the introduction of section 4. Might need to say a bit more about the exact implementation of BART that we use in section 4 (looks like there's BART-para, BART-CNN, etc)
% To test generalization, we also apply \deltascore on BART, note that BART is different from GPT-3.5 as in GPT-3.5 is a pure decoder based model while BART is an encoder-decoder based model.
% Note that, when we utilize the generative likelihood directly from BART to evaluate the story, it is the same as BARTScore \citep{DBLP:conf/nips/YuanNL21}.
% We replace generative language model of \deltascore to BART-para\footnote{The authors from \citet{DBLP:conf/nips/YuanNL21} fine-tunes BART-cnn model on para dataset \Needcite{para}} to explore if the idea can improve BARTScore.  

% We present results in, from which we can see that \deltascore can yield consistent gains in performances in most aspects and overall quality, which shows the idea of differentiating perturbations can also be applied to encoder-decoder architecture.

\subsection{Perturbation Degree}
\label{subsec:PerturbationDegree}
%JHL: This is the only additional analysis in this section, we can either move this to a subsection in 5 (e.g. 5.X additional analysis) or to appendix if we need space
Regarding specific perturbations, such as jumble or typos, we can control the extent to which the story is perturbed. For example, we can alter the ratio of the jumbled words. We explore the effect of perturbation degrees on \deltascore with BART-para due to the API cost of GPT-3.5. We present the results in \autoref{fig:correlations}, from which we see trends for certain perturbations. It seems that jumble does not aid much when the degree is slight, and it increases dramatically when we jumble more words and gradually reaches a stable value. On the contrary, typos seem to work rather stably, regardless of the degree of perturbation. We speculate that the language model is not sensitive to small amounts of changes of orders considering the masked word infilling objective it is trained on, while it is consistently sensitive to typos, which might be because it can cause a relatively different embedding through tokenization.

% \subsection{Model Preference}
% %JHL: we can drop this now since we'll be discussing BART results together with GPT3 in section 5
% We find different models have their different preferences.
% BART-cnn is better at stories generated from older models, while BART-para is better at stories generated recently even though the authors claim that BART-para has better capability due to the extra fine-tuning process.

% \subsection{Assessment Granularity}
% %JHL: Not really sure the objective of this analysis; are we trying to explain why certain metrics don't work (if so, which metric?)
% One of the mainstreams of automatic story evaluation is to train a binary classifier to distinguish good and bad stories \citep{guan-huang-2020-union, ghazarian-etal-2021-plot}.

% However, the nature of binary classifiers can easily result in an evaluator that mainly produce scores close to its labels, usually 0 and 1, while \deltascore utilize the essence of generative language models, therefore it can produce more continuous values that covers wider ranges (see \autoref{fig:binaryclassification}).

% continuous values are more desirable as they not only show which story is better, but also shows the granularity of which it is better even though they might have similar Spearman correlations.



% \begin{figure*}[t]
%      \centering
%      \begin{subfigure}[b]{0.31\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/corr_union-model.ckpt.pdf}
%          \caption{Perturbation ``Remove punctuation'' is applicable to higher quality story but not applicable to lower quality one.
%          }
%          \label{fig:example1}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.31\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/corr_bartscore_CNN_PS_ts1.pdf}
%          \caption{Perturbation ``Add typos'' is applicable to both stories, higher quality story is more affected than lower quality one.
%          % Higher quality story is affected more than lower quality one.
%          }
%          \label{fig:examples2}
%      \end{subfigure}
%      \begin{subfigure}[b]{0.31\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/corr_gpt3score_PS_mean_lp.pdf}
%          \caption{Perturbation ``Add typos'' is applicable to both stories, higher quality story is more affected than lower quality one.
%          % Higher quality story is affected more than lower quality one.
%          }
%          \label{fig:examples3}
%      \end{subfigure}
%         \caption{
%         The correlation figures show that binary classification approaches can only generate scores that close to labels (as set to 0s and 1s in the task)
%         while regression and generation approaches can generate more diverse scores.
%         }
%         \label{fig:binaryclassification}
% \end{figure*}

\section{Conclusion}
In this paper, we introduce \deltascore, a novel approach to fine-grained story quality evaluation that compares the difference in likelihood between pre- and post-perturbations. We experiment with multiple language models, including BART and GPT-3.5, and our results demonstrate that \deltascore with GPT-3.5 and jumble yields the best performance, outperforming state-of-the-art evaluation metrics.

\section*{Limitations}

This work explores a limited set of perturbations for story evaluation, but there are likely many more that could be obtained through different approaches. While we only apply this perturbation method to story generation, it has the potential to be adapted for evaluating various aspects of text generation using specifically designed perturbations, opening up a fruitful area for future research.


% \section*{Acknowledgements}

% \begin{figure*}[t]
%      \centering
%      \begin{subfigure}[b]{0.45\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/OPENMEVABARTScore_perturbations_roc.pdf}
%          \caption{ROC
%          }
%          \label{BARTScore-ROC-openmeva}
%      \end{subfigure}
%      \begin{subfigure}[b]{0.45\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/OPENMEVABARTScore_perturbations_wp.pdf}
%          \caption{WP
%          }
%          \label{BARTScore-WP-openmeva}
%      \end{subfigure}
%         \caption{
%         Perturbation results and Pearson correlations on each dataset for OpenMEVA.
%         }
%         \label{fig:perturbationgrades}
% \end{figure*}


% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

% \section{Amazon Mechanic Turk Setting}
% \label{appendix: mturksetting}



\section{Appendix: UNIEVAL on Story Evaluation}
\label{appendix: unieval}

\subsection{Questions}
We ask the following questions for each aspect.
Note that we try to use the narrative/vocabulary as close to the original questions
\citet{DBLP:conf/emnlp/Zhong0YMJLZJH22} use in their efforts as possible.

\begin{enumerate}
    \item Fluency: Is this a fluent utterance?
    \item Coherence: Is this a coherent utterance?
    \item Relatedness: Is this claim consistent with the document?
    \item Logicality: Is this utterance consistent with the commonsense?
    \item Interestingness: Is this an interesting utterance?
\end{enumerate}

% \subsection{Issues}
% We find that UNIEVAL seems not able to differeniate too much when we ask different questions on each quality aspect.

% \section{Appendix: Correlation Results}

% We put Spearman and Kendall-Tau correlations in \autoref{table:crowdsource_cnn}.
% Pearson correlations are shown in .

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
%  % & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 25.0 & 11.3 & 6.6 & 15.0 & 0.8 & 9.4 & 16.9 & 11.2 & 15.4 & 23.7 & 22.7 & 0.2 & 25.1 & 3.6 & 18.1  \\ 
%  BER & 4.4 & 1.3 & 26.7 & 4.2 & 3.1 & 29.4 & 34.6 & 37.5 & 28.1 & 30.6 & 48.7 & 45.3 & 70.8 & 51.7 & 48.6 \\
%  Mov & 12.0 & 8.0 & 38.2 & 7.5 & 0.2 & 41.9 & 47.7 & 36.8 & 30.0 & 34.6 & 23.5 & 14.6 & 47.0 & 25.2 & 33.5 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 13.5 & 26.7 & 14.9 & 19.3 & 7.2 & 39.4 & 59.6 & 54.1 & 47.8 & 58.8 & 27.1 & 42.5 & 39.5 & 33.2 & 43.0 \\  
% MAN & 7.0 & 3.1 & 7.5 & 1.6 & 12.7 & 20.5 & 0.4 & 14.1 & 23.7 & 4.7 & 18.6 & 30.4 & 48.5 & 19.4 & 20.1 \\ 
% Sto & 5.4 & 12.0 & 14.3 & 1.0 & 12.6 & 11.6 & 22.1 & 12.4 & 21.5 & 32.5 & 13.0 & 28.6 & 14.4 & 14.7 & 23.7 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 23.5 & 41.3 & 25.3 & 35.7 & 25.8 & 51.3 & 76.6 & 53.6 & 68.0 & \textbf{57.2} & 57.4 & 59.4 & 52.8 & 61.0 & 62.7 \\ 
%  UNI & 43.0 & 41.5 & 13.9 & 34.0 & 26.1 & 61.6 & 56.3 & 49.4 & 51.9 & 51.0 & 47.3 & 61.3 & 55.7 & 71.5 & 52.0 \\
%  \midrule
% \multicolumn{10}{l}{\textbf{Likelihood Based Metrics}}\\
% BART & \\ 
% G-3.5 & 28.8 & 43.2 & 26.6 & 39.9 & 25.9 & 47.9 & 44.8 & 48.8 & 52.4 & 34.5 & 56.8 & 65.9 & 58.3 & 66.0 & \textbf{52.7} \\ 
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  + Ju & \cellcolor{green!25}38.7 & \cellcolor{green!25}48.3 & \cellcolor{green!25}33.7 & \cellcolor{green!25}50.8 & 37.2 & \cellcolor{green!25}43.3 & \cellcolor{green!25}50.3 & \cellcolor{green!25}53.0 & \cellcolor{green!25}52.9 & \cellcolor{green!25}43.2 & \cellcolor{green!25}46.7 & \cellcolor{green!25}63.8 & \cellcolor{green!25}46.7 & \cellcolor{green!25}56.1 & 47.6 \\ 
 
%  + Ty & \cellcolor{green!25}37.7 & \cellcolor{green!25}48.6 & \cellcolor{green!25}35.6 & 45.0 & 32.2 & \cellcolor{green!25}44.7 & \cellcolor{green!25}48.3 & \cellcolor{green!25}53.1 & \cellcolor{green!25}58.3 & \cellcolor{green!25}40.8 & \cellcolor{green!25}57.9 & \cellcolor{green!25}68.5 & \cellcolor{green!25}55.2 & \cellcolor{green!25}67.8 & 50.6 \\ 
 
%  + Ad & 18.0 & 41.1 & 21.1 & 32.5 & 24.1 & 44.4 & 38.4 & 30.9 & 34.9 & 23.7 & 47.1 & 66.3 & 32.4 & 59.2 & 42.6  \\
 
%  + An & 32.2 & \cellcolor{green!25}40.1 & 26.4 & 29.7 & 19.4 & \cellcolor{green!25}72.9 & \cellcolor{green!25}67.7 & \cellcolor{green!25}56.4 & \cellcolor{green!25}71.4 & \cellcolor{green!25}56.9 & \cellcolor{green!25}48.3 & 59.4 & 52.7 & \cellcolor{green!25}64.6 & 40.5 \\
%  + Rp  \\
%  + Rr  \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (change to vinialla BART)}}\\ 
%  + Ju &  \\
%  + Ty &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Spearman correlation ($|\rho|$) between different metrics and human evaluations on three Inhouse datasets.
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the original likehihood metric.
% }
% \label{table:inhouse_spearman}
% \end{table*}

% \begin{table*}[t]
% \centering
% \begin{tabular}{lcccccccccccc}
% \toprule
%  \multirow{2}{*}{\textbf{Metric}} & \multicolumn{2}{c}{\textbf{Flu.}} & \multicolumn{2}{c}{\textbf{Coh.}} & \multicolumn{2}{c}{\textbf{Rel.}} & \multicolumn{2}{c}{\textbf{Log.}} & \multicolumn{2}{c}{\textbf{Int.}} & \multicolumn{2}{c}{\textbf{Avg}} \\
%  & $|\rho|$ & $|\tau|$ & $|\rho|$ & $|\tau|$ & $|\rho|$ & $|\tau|$ & $|\rho|$ & $|\tau|$ & $|\rho|$ & $|\tau|$ & $|\rho|$ & $|\tau|$ \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}\cmidrule(lr){12-13}
%   \multicolumn{13}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLEU & 27.0 & 19.8 & 19.6 & 13.3 & 19.8 & 13.8 & 21.2 & 15.0 & 14.3 & 10.0 & 22.0 & 14.6 \\ 
%  BERTScore & 29.6 & 21.5 & 27.7 & 18.8 & 23.3 & 16.4 & 29.2 & 20.9 & 28.2 & 19.9 & 30.9 & 21.0 \\ 
%  MoverScore & 14.1 & 9.6 & 22.4 & 16.4 & 22.3 & 16.1 & 21.3 & 15.4 & 28.5 & 20.9 & 24.9 & 17.2 \\ 
%  \midrule
%  \multicolumn{13}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNION & 29.2 & 21.1 & 35.4 & 25.1 & 29.9 & 21.3 & 36.9 & 26.8 & 39.6 & 27.9 & 38.1 & 26.9 \\ 
% MANPLTS & 0.1 & 0.3 & 8.2 & 6.6 & 3.5 & 2.2 & 4.9 & 4.7 & 12.8 & 7.6 & 8.6 & 6.4 \\ 
% StoryER & 25.8 & 19.1 & 23.7 & 16.0 & 18.9 & 13.0 & 31.2 & 21.8 & 34.5 & 23.8 & 31.0 & 21.3 \\
%  \midrule
%  \multicolumn{13}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  BARTScore & 25.8 & 19.1 & 15.3 & 10.8 & 13.4 & 10.4 & 21.7 & 15.2 & 13.4 & 11.0 & 17.2 & 13.2  \\ 
%  CTC & 60.0 & 46.0 & 70.1 & 54.5 & 56.1 & 42.2 & 72.8 & 56.0  & 63.8 & 48.1 & 71.7 & 53.8  \\ 
%  UNIEVAL & 52.6 & 39.3 & 56.8 & 41.3 & 50.9 & 36.4 & 67.4 & 50.7 & 52.7 & 39.1 & 63.2 & 46.1 \\ 
% \midrule
%  \multicolumn{13}{l}{\textbf{GPTScore (Ours)}}\\ 
%  - Perturbation & 26.0 & 19.4 & 30.3 & 23.2 & 25.4 & 18.9 & 34.0 & 25.7 & 29.8 & 22.1 &  29.3 & 20.9 \\ 
%  + Jumble & 71.6 & 54.0 & 71.3 & 53.8 & 53.7 & 39.2 & 74.2 & 58.4 & 59.8 & 44.0 & 72.4 & 54.4 \\ 
%  + Typo & 56.9 & 41.7 & 57.1 & 41.2 & 45.9 & 33.3 & 62.1 & 46.3 & 51.2 & 37.1 &  58.8 & 41.8 \\ 
%  \bottomrule
% \end{tabular}
% \caption{Story-level Spearman ($\rho$) and Kendall-Tau ($\tau$) correlations of different metrics on CoudSourcing WP dataset.}
% \label{table:crowdsource_wp}
% \end{table*}


% \begin{table*}[t]
% \centering
% \begin{tabular}{lcccccccccccc}
% \toprule
%  \multirow{2}{*}{\textbf{Metric}} & \multicolumn{2}{c}{\textbf{Flu.}} & \multicolumn{2}{c}{\textbf{Coh.}} & \multicolumn{2}{c}{\textbf{Rel.}} & \multicolumn{2}{c}{\textbf{Log.}} & \multicolumn{2}{c}{\textbf{Int.}} & \multicolumn{2}{c}{\textbf{Avg}} \\
%  & $|\rho|$ & $|\tau|$ & $|\rho|$ & $|\tau|$ & $|\rho|$ & $|\tau|$ & $|\rho|$ & $|\tau|$ & $|\rho|$ & $|\tau|$ & $|\rho|$ & $|\tau|$ \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}\cmidrule(lr){12-13}
%   \multicolumn{13}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLEU & 15.3 & 9.1 & 0.9 & 0.8 & 3.4 & 2.6 & 0.3 & 0.2 & 5.3 & 3.7 & 4.1 & 1.9 \\ 
%  BERTScore & 4.4 & 2.8 & 2.5 & 1.1 & 11.1 & 8.6 & 1.9 & 2.0 & 6.7 & 4.2 & 2.8 & 1.8 \\ 
%  MoverScore & 16.8 & 10.8 & 18.5 & 13.8 & 9.3 & 5.8 & 17.5 & 12.1 & 0.7 & 0.7 & 16.5 & 11.4 \\ 
%  \midrule
%  \multicolumn{13}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNION & 10.0 & 7.7 & 1.6 & 1.4 & 24.5 & 17.4 & 4.2 & 3.8 & 1.6 & 1.3 & 7.9 & 5.6 \\ 
% MANPLTS & 8.5 & 4.9 & 27.0 & 20.4 & 18.3 & 14.3 & 25.5 & 19.8 & 16.5 & 12.6 & 25.3 & 18.7 \\ 
% StoryER & 6.7 & 4.9 & 6.6 & 4.9 & 0.1 & 1.1 & 10.5 & 7.3 & 8.4 & 4.7 & 7.7 & 4.6 \\
%  \midrule
%  \multicolumn{13}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  BARTScore & 45.0 & 33.6 & 34.0 & 25.5 & 34.6 & 25.2 & 46.2 & 31.9 & 25.6 & 19.0 & 42.6 & 29.3 \\ 
%  CTC & 40.0 & 30.1 & 52.1 & 37.0  & 56.2 & 37.4  & 58.6 & 40.9  & 44.6 & 32.1  &  60.6 & 40.9  \\ 
%  UNIEVAL & 33.9 & 24.6  & 43.6 & 30.2  & 50.4 & 35.3 & 53.2 & 37.3  & 37.3 & 26.2 & 57.4 & 39.8 \\ 
% \midrule
%  \multicolumn{13}{l}{\textbf{GPTScore (Ours)}}\\ 
%  - Perturbation & 49.2 & 38.5  & 64.7 & 47.2 & 64.5 & 45.6 & 67.4 & 50.6 & 54.2 & 41.4 & 71.2 & 51.6 \\ 
%  + Jumble & 44.8 & 35.2 & 62.2 & 47.4 & 64.0 & 45.2 & 67.4 & 51.6 & 43.1 & 32.8  &  67.1 & 49.4 \\ 
%  + Typo & 57.6 & 44.5 & 66.3 & 48.8 & 69.2 & 50.5 & 72.1 & 55.3 & 53.7 & 40.7 &  74.6 & 54.9 \\ 
%  \bottomrule
% \end{tabular}
% \caption{Story-level Spearman ($\rho$) and Kendall-Tau ($\tau$) correlations of different metrics on CoudSourcing CNN dataset.}
% \label{table:crowdsource_cnn}
% \end{table*}

% \begin{table*}[t]
% \centering
% \begin{tabular}{lcccccc}
% \toprule
%  \multirow{2}{*}{\textbf{Metric}} & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} & \textbf{Avg} \\
%  & $|r|$ & $|r|$ & $|r|$ & $|r|$ & $|r|$ & $|r|$ \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}
%   \multicolumn{6}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLEU & 1.6 & 3.8 & 3.4 & 8.3 & 1.4 & 3.3  \\ 
%  BERTScore & 3.1 & 6.2 & 18.3 & 3.8 & 6.2 & 9.7  \\ 
%  MoverScore & 2.2 & 7.4 & 23.3 & 3.3 & 9.7 & 11.2  \\ 
%  \midrule
%  \multicolumn{6}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNION & 4.7 & 16.7 & 17.4 & 5.2 & 9.3 & 13.6 \\ 
% MANPLTS & 27.7 & 40.2 & 23.7 & 17.7 & 13.4 & 29.8 \\ 
% StoryER & 8.9 & 1.1 & 15.5 & 5.4 & 12.1 & 7.0 \\
%  \midrule
%  \multicolumn{6}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  BARTScore & 20.9 & 15.1 & 3.8 & 3.1 & 15.7 & 10.9  \\ 
%  CTC & 29.3 & 38.3 & 20.3 & 16.1 & 12.0 & 27.8   \\ 
%  UNIEVAL & 34.4 & 35.9 & 30.4 & 26.2 & 21.2 & 36.5  \\ 
% \midrule
%  \multicolumn{6}{l}{\textbf{GPTScore (Ours)}}\\ 
%  - Perturbation & 43.8 & 43.5 & 37.3 & 32.2 & 30.0 & 45.4 \\ 
%  + Jumble & 46.3 & 47.2 & 38.8 & 33.3 & 23.8 & 46.0 \\ 
%  + Typo & 47.3 & 45.4 & 41.0 & 30.4 & 24.7 & 45.6 \\ 
%  \bottomrule
% \end{tabular}
% \caption{Story-level Pearson ($r$) correlations of different metrics on CrowdSourcing ROC dataset.}
% \label{table:crowdsource_roc_pearson}
% \end{table*}

% \begin{table*}[t]
% \centering
% \begin{tabular}{lcccccc}
% \toprule
%  \multirow{2}{*}{\textbf{Metric}} & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} & \textbf{Avg} \\
%  & $|r|$ & $|r|$ & $|r|$ & $|r|$ & $|r|$ & $|r|$ \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}
%   \multicolumn{6}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLEU & 33.5 & 20.7 & 17.8 & 24.4 & 19.0 & 24.9  \\ 
%  BERTScore & 37.8 & 33.3 & 28.9 & 33.9 & 30.2 & 36.2  \\ 
%  MoverScore & 19.0 & 25.4 & 21.8 & 25.4 & 32.0 & 27.8  \\ 
%  \midrule
%  \multicolumn{6}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNION & 34.8 & 36.6 & 26.9 & 37.5 & 31.7 & 37.1 \\ 
% MANPLTS & 0.1 & 1.6 & 7.7 & 3.0 & 4.0 & 4.2 \\ 
% StoryER & 26.4 & 22.6 & 12.5 & 31.4 & 30.7 & 26.9 \\
%  \midrule
%  \multicolumn{6}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  BARTScore & 12.1 & 5.8 & 10.9 & 13.8 & 10.5 & 11.8  \\ 
%  CTC & 50.1 & 63.0 & 51.0 & 70.0 & 62.6 & 66.8 \\ 
%  UNIEVAL & 42.8 & 43.0 & 49.7 & 55.1 & 45.4 & 54.1  \\ 
% \midrule
%  \multicolumn{6}{l}{\textbf{GPTScore (Ours)}}\\ 
%  - Perturbation & 27.4 & 44.3 & 38.4 & 51.0 & 50.1 & 48.2 \\ 
%  + Jumble & 66.4 & 76.2 & 65.9 & 83.4 & 71.0 & 81.5 \\ 
%  + Typo & 49.6 & 60.3 & 52.6 & 68.5 & 60.0 & 65.6 \\ 
%  \bottomrule
% \end{tabular}
% \caption{Story-level Pearson ($r$) correlations of different metrics on CrowdSourcing WP dataset.}
% \label{table:crowdsource_wp_pearson}
% \end{table*}

% \begin{table*}[t]
% \centering
% \begin{tabular}{lcccccc}
% \toprule
%  \multirow{2}{*}{\textbf{Metric}} & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} & \textbf{Avg} \\
%  & $|r|$ & $|r|$ & $|r|$ & $|r|$ & $|r|$ & $|r|$ \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}
%   \multicolumn{6}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLEU & 6.8 & 5.7 & 1.1 & 1.1 & 2.6 & 1.5  \\ 
%  BERTScore & 4.4 & 5.9 & 13.1 & 7.4 & 3.0 & 8.0  \\ 
%  MoverScore & 4.9 & 5.6 & 1.9 & 2.4 & 2.3 & 1.7  \\ 
%  \midrule
%  \multicolumn{6}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNION & 4.3 & 3.9 & 14.7 & 11.7 & 0.0 & 8.4 \\ 
% MANPLTS & 4.0 & 11.9 & 8.3 & 8.1 & 5.8 & 9.0 \\ 
% StoryER & 9.6 & 7.0 & 2.3 & 10.7 & 8.8 & 7.5 \\
%  \midrule
%  \multicolumn{6}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  BARTScore & 49.0 & 40.1 & 36.4 & 46.2 & 26.1 & 44.7  \\ 
%  CTC & 41.4 & 53.2 & 51.1 & 56.4 & 48.1 & 58.1 \\ 
%  UNIEVAL & 30.4 & 28.1 & 42.6 & 30.6 & 23.1 & 36.9  \\ 
% \midrule
%  \multicolumn{6}{l}{\textbf{GPTScore (Ours)}}\\ 
%  - Perturbation & 55.8 & 69.1 & 70.3 & 68.7 & 59.9 & 75.0 \\ 
%  + Jumble & 50.1 & 67.1 & 64.0 & 65.8 & 54.0 & 69.9 \\ 
%  + Typo & 58.3 & 69.9 & 72.2 & 71.5 & 58.8 & 76.6 \\ 
%  \bottomrule
% \end{tabular}
% \caption{Story-level Pearson ($r$) correlations of different metrics on CrowdSourcing CNN dataset.}
% \label{table:crowdsource_cnn_pearson}
% \end{table*}


% \begin{table*}[t]
% \centering
% \begin{tabular}{lcccccccccccc}
% \toprule
%  \multirow{2}{*}{\textbf{Metric}} & \multicolumn{2}{c}{\textbf{Flu.}} & \multicolumn{2}{c}{\textbf{Coh.}} & \multicolumn{2}{c}{\textbf{Rel.}} & \multicolumn{2}{c}{\textbf{Log.}} & \multicolumn{2}{c}{\textbf{Int.}} & \multicolumn{2}{c}{\textbf{Avg}} \\
%  & $|\rho|$ & $|\tau|$ & $|\rho|$ & $|\tau|$ & $|\rho|$ & $|\tau|$ & $|\rho|$ & $|\tau|$ & $|\rho|$ & $|\tau|$ & $|\rho|$ & $|\tau|$ \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}\cmidrule(lr){12-13}
%   \multicolumn{13}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLEU & 4.8 & 3.4 & 5.8 & 4.4 & 2.0 & 0.8 & 7.0 & 4.6 & 0.8 & 0.4 & 4.1 & 2.7 \\ 
%  BERTScore & 5.5 & 3.5 & 7.3 & 5.0 & 19.7 & 14.0 & 7.8 & 5.7 & 9.9 & 7.3 & 12.3 & 8.1\\ 
%  MoverScore & 4.9 & 3.6 & 9.1 & 6.5 & 22.6 & 15.7 & 11.0 & 8.0 & 15.2 & 11.2 & 14.2 & 10.2 \\ 
%  \midrule
%  \multicolumn{13}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNION & 0.6 & 0.7 & 17.6 & 12.7 & 17.6 & 12.8 & 1.4 & 0.8 & 6.1 & 3.9 & 11.9 & 8.0 \\ 
% MANPLTS & 30.8 & 21.3 & 45.5 & 32.8 & 31.8 & 23.2 & 21.5 & 14.7 & 17.7 & 12.4 & 36.7 & 24.5\\ 
% StoryER & 9.7 & 6.5 & 6.1 & 4.5 & 6.0 & 4.0 & 5.8 & 3.7 & 14.3 & 9.7 & 4.1 & 2.9 \\
%  \midrule
%  \multicolumn{13}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  BARTScore & 20.9 & 15.1 & 3.8 & 3.1 & 15.7 & 10.9 & 9.6 & 7.0 & 16.8 & 12.2 & 15.1 & 10.4 \\ 
%  CTC & 31.7 & 22.9 & 37.8 & 27.3  & 21.7 & 14.3  & 17.0 & 11.1  & 12.2 & 8.3  &  28.4 & 18.6  \\ 
%  UNIEVAL & 44.3 & 32.2 & 45.2 & 31.7 & 33.3 & 23.7 & 29.0 & 20.0 & 26.6 & 18.8 & 43.6 & 30.0 \\ 
% \midrule
%  \multicolumn{13}{l}{\textbf{\deltascore (Ours)}}\\ 
%  - Perturbation & 43.2 & 32.4 & 39.5 & 27.7 & 35.4 & 25.1 & 29.9 & 21.1 & \textbf{27.4} & \textbf{19.5} &  41.4 & 28.7 \\ 
%  + Jumble & \textbf{50.4} & \textbf{36.7}  & \textbf{47.3} & \textbf{34.1} & \textbf{43.5} & \textbf{30.7} & \textbf{33.3} & \textbf{23.5} & 25.5 & 18.3 & \textbf{47.5} & \textbf{33.5} \\ 
%  + Typos & 47.6 & 35.1 & 42.3 & 30.5 & 41.1 & 29.1 & 28.5 & 19.9 & 22.8 & 15.8 & 43.2 & 30.6 \\ 
%  + RepeatSent & 43.1 & 32.3 & 36.8 & 25.8 & 26.7 & 18.4 & 19.0 & 12.9 & 21.2 & 13.9 & 34.4 & 22.4 \\
%  + SentReorder & 30.7 & 22.7 & 34.5 & 24.1 & 22.8 & 16.0 & 22.0 & 13.8 & 16.9 & 11.1 & 31.3 & 19.7 \\
%  + AddNegation & 30.7 & 21.5 & 43.6 & 30.3 & 49.1 & 34.9 & 26.5 & 19.0 & 26.7 & 19.5 & 43.4 & 29.6 \\
%  + Antonym1.0 & 32.2 & 22.8 & 40.1 & 28.0 & 26.4 & 17.2 & 29.7 & 20.9 & 19.4 & 13.8 & 35.7 & 23.7 \\
% %  + Antonym0.95 & 35.4 & 25.0 & 44.1 & 31.3 & 28.0 & 18.8 & 32.0 & 22.6 & 23.0 & 16.2 & 39.0 & 26.1 \\
% %  + Antonym0.9 & 36.0 & 26.0 & 42.2 & 30.1 & 25.0 & 17.0 & 30.1 & 21.4 & 22.8 & 15.5 & 37.2 & 24.8 \\
% % + Antonym0.8 & 27.8 & 19.7 & 37.1 & 25.8 & 21.3 & 13.8 & 28.3 & 20.1 & 16.8 & 11.7 & 31.9 & 21.0 \\
% %  + DropStopws & 10.0 & 7.3 & 12.3 & 8.7 & 9.0 & 5.7 & 7.5 & 5.4 & 6.6 & 4.4 & 1.9 & 1.6 \\
% %  + Dropphrases & 15.6 & 10.9 & 18.1 & 12.3 & 13.4 & 9.8 & 12.0 & 8.6 & 7.6 & 5.2 & 4.4 & 2.4 \\
% %  + Dropadjs & 9.1 & 6.3 & 10.4 & 7.3 & 10.9 & 7.6 & 5.2 & 3.7 & 2.6 & 2.0 & 8.9 & 6.0 \\
% %  + Removepunct & 1.8 & 1.5 & 1.6 & 1.2 & 0.0 & 0.3 & 19.6 & 13.5 & 20.3 & 14.5 & 11.7 & 7.9 \\
% %  + Subverbdis & 14.3 & 9.8 & 16.8 & 11.7 & 13.8 & 9.6 & 8.5 & 5.4 & 4.0 & 2.4 & 5.5 & 3.2 \\
% %  + RpNPron & 6.8 & 4.6 & 5.6 & 3.8 & 5.5 & 4.2 & 0.9 & 0.6 & 5.2 & 3.1 & 0.2 & 0.5 \\
% %  + Commonsense & 0.6 & 0.5 & 1.2 & 0.7 & -2.9 & -2.0 & 5.6 & 4.2 & 3.3 & 2.5 & 2.1 & 1.7 \\
% %  + RemoveNegation & -8.8 & -5.9 & -0.2 & 0.1 & 2.9 & 2.0 & -3.2 & -1.9 & -4.9 & -3.2 & -2.3 & -1.2 \\
% %  + time & -6.5 & -5.0 & 2.1 & 1.3 & -6.7 & -5.0 & -9.1 & -6.4 & -3.9 & -2.4 & -5.9 & -4.4 \\
% %  + cause & -1.0 & -0.5 & -5.3 & -3.8 & -5.9 & -4.3 & -4.6 & -2.9 & -5.0 & -3.3 & -5.4 & -3.7 \\
  
%  \bottomrule
% \end{tabular}
% \caption{Story-level Spearman ($\rho$) and Kendall-Tau ($\tau$) correlations of different metrics on CoudSourcing ROC dataset. [Commonsense seems to have a okay performance on logicality, but worth trying to combine these with ]}
% \label{table:crowdsource_roc}
% \end{table*}


% % \begin{table}[t]
% % \small
% % \centering
% % \begin{tabular}{lcccccc}
% % \toprule
% %  \multirow{2}{*}{\textbf{Metric}} & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} & \textbf{Avg} \\
% %  & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$\\
% % \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}
% %   \multicolumn{5}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
% %  BLEU & 4.8 & 5.8 & 2.0 & 7.0 & 0.8 & 4.1 \\ 
% %  BERTS & 5.5 & 7.3 & 19.7 & 7.8 & 9.9 & 12.3 \\
% %  MoverS & 4.9 & 9.1 & 22.6 & 11.0 & 15.2 & 14.2 \\
% %  \midrule
% %  \multicolumn{5}{l}{\textbf{Story Evaluation Metrics}} \\ 
% % UNION & 0.6 & 17.6 & 17.6 & 1.4 & 6.1 & 11.9 \\  
% % MANP. & 30.8 & 45.5 & 31.8 & 21.5 & 17.7 & 36.7 \\ 
% % StoryER & 9.7 & 6.1 & 6.0 & 5.8 & 14.3 & 4.1 \\
% %  \midrule
% %  \multicolumn{5}{l}{\textbf{Unified Evaluation Metrics}}\\ 
% %  CTC & 31.7 & 37.8  & 21.7  & 17.0  & 12.2 & 28.4 \\ 
% %  UNI. & 44.3 & 45.2 & 33.3 & 29.0 & 26.6 & 43.6 \\
% % \midrule
% %  \multicolumn{5}{l}{\textbf{\deltascore (with GPT-3.5)}}\\ 
% % - Per & 43.2 & 39.5 & 35.4 & 29.9 & \textbf{27.4} &  41.4 \\ 
% %  + Jum & \cellcolor{green!25}\textbf{50.4} & \cellcolor{green!25}\textbf{47.3} & \cellcolor{green!25}43.5 & \cellcolor{green!25}\textbf{33.3} & 25.5 & \cellcolor{green!25}\textbf{47.5} \\ 
% %  + Typo & \cellcolor{green!25}47.6 & \cellcolor{green!25}42.3 & \cellcolor{green!25}41.1 & 28.5 & 22.8 & \cellcolor{green!25}43.2 \\ 
% %  + AddNg & 30.7 & \cellcolor{green!25}43.6 & \cellcolor{green!25}\textbf{49.1} & 26.5 & 26.7 & \cellcolor{green!25}43.4 \\
% %  + Anton & 32.2 & \cellcolor{green!25}40.1 & 26.4 & 29.7 & 19.4 & 35.7 \\
% %  + RpSent & 43.1 & 36.8 & 26.7 & 19.0 & 21.2 & 34.4 \\
% %  + RrSent & 30.7 & 34.5 & 22.8 & 22.0 & 16.9 & 31.3 \\
% %  \midrule
% %  \multicolumn{5}{l}{\textbf{\deltascore (with BART-para)}}\\
% %  BARTS & 20.9 & 3.8 & 15.7 & 9.6 & 16.8 & 15.1 \\  
% %  + Jumble & \cellcolor{green!25}35.3 & \cellcolor{green!25}19.1 & 14.4 & \cellcolor{green!25}16.4 & \cellcolor{green!25}17.9 & \cellcolor{green!25}23.5 \\
% %  + Typos & \cellcolor{green!25}25.2 & \cellcolor{green!25}5.8 & \cellcolor{green!25}16.2 & 8.4 & 15.6 & \cellcolor{green!25}15.9 \\
% %  \bottomrule
% % \end{tabular}
% % \caption{Story-level Spearman ($\rho$) correlation of different metrics on CoudSourcing ROC dataset.
% % We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the original likehihood metric.
% % }
% % \label{table:crowdsource_roc_spearman}
% % \end{table}


% % \begin{table}[t]
% % \small
% % \centering
% % \begin{tabular}{lcccccc}
% % \toprule
% %  \multirow{2}{*}{\textbf{Metric}} & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} & \textbf{Avg} \\
% %  & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$\\
% % \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}
% %   \multicolumn{5}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
% %  BLEU & 27.0 & 19.6 & 19.8 & 21.2 & 14.3 & 22.0 \\ 
% %  BERTS & 29.6 & 27.7 & 23.3 & 29.2 & 28.2 & 30.9 \\
% %  MoverS & 14.1 & 22.4 & 22.3 & 21.3 & 28.5 & 24.9 \\
% %  \midrule
% %  \multicolumn{5}{l}{\textbf{Story Evaluation Metrics}} \\ 
% % UNION & 29.2 & 35.4 & 29.9 & 36.9 & 39.6 & 38.1 \\  
% % MANP. & 0.1 & 8.2 & 3.5 & 4.9 & 12.8 & 8.6 \\ 
% % StoryER & 25.8 & 23.7 & 18.9 & 31.2 & 34.5 & 31.0 \\
% %  \midrule
% %  \multicolumn{5}{l}{\textbf{Unified Evaluation Metrics}}\\  
% %  CTC & 60.0 & 70.1 & 52.1 & 72.8 & \textbf{63.8} & 71.7 \\ 
% %  UNI. & 52.6 & 56.8 & 50.9 & 67.4 & 52.7 & 63.2 \\
% % \midrule
% %  \multicolumn{5}{l}{\textbf{\deltascore (with GPT-3.5)}}\\ 
% % - Per & 33.5 & 40.8 & 34.1 & 43.9 & 43.1 & 40.8 \\ 
% %  + Jum & \cellcolor{green!25}\textbf{71.6} & \cellcolor{green!25}\textbf{71.3} & \cellcolor{green!25}\textbf{53.7} & \cellcolor{green!25}\textbf{74.2} & \cellcolor{green!25}59.8 & \cellcolor{green!25}\textbf{72.4} \\ 
% %  + Typo & \cellcolor{green!25}56.9 & \cellcolor{green!25}57.1 & \cellcolor{green!25}45.9 & \cellcolor{green!25}62.1 & \cellcolor{green!25}51.2 & \cellcolor{green!25}58.8 \\ 
% %  + AddNg & \cellcolor{green!25}38.7 & 35.1 & 21.8 & 41.9 & 35.3 & 36.8 \\
% %  + Anton & \cellcolor{green!25}72.9 & \cellcolor{green!25}67.7 & \cellcolor{green!25}56.4 & \cellcolor{green!25}71.4 & \cellcolor{green!25}56.9 & \cellcolor{green!25}72.7 \\
% %  + RpSent & 4.7 & 3.0 & 1.3 & 2.7 & 10.0 & 0.1 \\
% %  + RrSent & 3.7 & 19.2 & 7.9 & 13.1 & 11.8 & 11.3 \\
% %  \midrule
% %  \multicolumn{5}{l}{\textbf{\deltascore (with BART-para)}}\\
% %  BARTS & 25.8 & 15.3 & 13.4 & 21.7 & 13.4 & 17.2 \\ 
% %  + Jumble & \cellcolor{green!25}50.1 & \cellcolor{green!25}39.3 & \cellcolor{green!25}38.3 & \cellcolor{green!25}47.3 & \cellcolor{green!25}42.7 & \cellcolor{green!25}45.9 \\
% %  + Typos & \cellcolor{green!25}44.1 & \cellcolor{green!25}30.5 & \cellcolor{green!25}28.5 & \cellcolor{green!25}38.3 & \cellcolor{green!25}27.4 & \cellcolor{green!25}34.8 \\
% %  \bottomrule
% % \end{tabular}
% % \caption{Story-level Spearman ($\rho$) correlation of different metrics on CoudSourcing WP dataset.}
% % \label{table:crowdsource_wp_spearman}
% % \end{table}

% % \begin{table}[t]
% % \small
% % \centering
% % \begin{tabular}{lcccccc}
% % \toprule
% %  \multirow{2}{*}{\textbf{Metric}} & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} & \textbf{Avg} \\
% %  & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$\\
% % \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}
% %   \multicolumn{5}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
% %  BLEU & 15.3 & 0.9 & 3.4 & 0.3 & 5.3 & 4.1 \\ 
% %  BERTS & 4.4 & 2.5 & 11.1 & 1.9 & 6.7 & 2.8 \\
% %  MoverS & 16.8 & 18.5 & 9.3 & 17.5 & 0.7 & 16.5 \\
% %  \midrule
% %  \multicolumn{5}{l}{\textbf{Story Evaluation Metrics}} \\ 
% % UNION & 10.0 & 1.6 & 24.5 & 4.2 & 1.6 & 7.9 \\  
% % MANP. & 8.5 & 27.0 & 18.3 & 25.5 & 16.5 & 25.3 \\ 
% % StoryER & 6.7 & 6.6 & 0.1 & 10.5 & 8.4 & 7.7 \\
% %  \midrule
% %  \multicolumn{5}{l}{\textbf{Unified Evaluation Metrics}}\\  
% %  CTC & 40.0 & 52.1 & 56.2 & 58.6 & 44.6 & 60.6 \\ 
% %  UNI. & 33.9 & 43.6 & 50.4 & 53.2 & 37.3 & 57.4 \\
% % \midrule
% %  \multicolumn{5}{l}{\textbf{\deltascore (with GPT-3.5)}}\\ 
% % - Per & 45.2 & 60.5 & 57.9 & 63.2 & \textbf{57.9} & 67.4 \\ 
% %  + Jum & \cellcolor{green!25}54.0 & \cellcolor{green!25}\textbf{69.0} & \cellcolor{green!25}61.2 & \cellcolor{green!25}\textbf{71.4} & 47.1 & \cellcolor{green!25}71.9 \\ 
% %  + Typo & \cellcolor{green!25}\textbf{55.4} & \cellcolor{green!25}65.6 & \cellcolor{green!25}\textbf{65.9} & \cellcolor{green!25}69.5 & 57.1 & \cellcolor{green!25}\textbf{73.6} \\ 
% %  + AddNg & \cellcolor{green!25}48.0 & \cellcolor{green!25}60.8 & \cellcolor{green!25}65.0 & 62.9 & 46.1 & 66.7  \\
% %  + Anton & \cellcolor{green!25}48.3 & 59.4 & 52.7 & \cellcolor{green!25}64.6 & 40.5 & 62.6 \\
% %  + RpSent & 10.0 & 12.9 & 23.2 & 13.9 & 1.6 & 15.8 \\
% %  + RrSent & 18.6 & 11.5 & 6.3 & 2.6 & 6.7 & 1.2 \\
% %  \midrule
% %   \multicolumn{5}{l}{\textbf{\deltascore (with BART-para)}}\\
% %   BARTS & 45.0 & 34.0 & 34.6 & 46.2 & 25.6 & 42.6 \\ 
% %  + Jumble & \cellcolor{green!25}45.6 & \cellcolor{green!25}47.5 & \cellcolor{green!25}43.4 & \cellcolor{green!25}53.6 & \cellcolor{green!25}25.7 & \cellcolor{green!25}51.3 \\
% %  + Typos & \cellcolor{green!25}53.2 & \cellcolor{green!25}44.3 & \cellcolor{green!25}45.8 & \cellcolor{green!25}57.7 & \cellcolor{green!25}34.6 & \cellcolor{green!25}54.9 \\
% %  \bottomrule
% % \end{tabular}
% % \caption{Story-level Spearman ($\rho$) correlation of different metrics on CoudSourcing CNN dataset.}
% % \label{table:crowdsource_cnn_spearman}
% % \end{table}

% % \begin{table}[t]
% % \small
% % \centering
% % \begin{tabular}{lcccccc}
% % \toprule
% %  \multirow{2}{*}{\textbf{Metric}} & \multicolumn{3}{c}{\textbf{ROC}} & \multicolumn{3}{c}{\textbf{WP}} \\
% %  & $|r|$ & $|\rho|$ & $|\tau|$ & $|r|$ & $|\rho|$ & $|\tau|$ \\
% % \cmidrule(lr){1-1}\cmidrule(lr){2-4}\cmidrule(lr){5-7}
% %   \multicolumn{5}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
% %  BLEU & 8.5 & 8.3 & 5.7 & 1.0 & 2.3 & 1.7 \\ 
% %  BERT. & 27.9 & 25.1 & 17.6 & 15.3 & 15.5 & 10.7 \\ 
% %  Mover. & 13.3 & 11.2 & 7.7 & 0.1 & 1.8 & 1.2 \\ 
% %  \midrule
% %  \multicolumn{5}{l}{\textbf{Story Evaluation Metrics}} \\ 
% % UNION & 40.8 & 46.7 & 33.0 & 10.0 & 17.8 & 12.4 \\ 
% % Union & 41.2 & - & - & 32.6 & - & - \\
% % MANP. & 27.4 & 36.0 & 25.1 & 14.3 & 17.5 & 12.1 \\ 
% % StoryER & 6.4 & 5.5 & 3.8 & 13.3 & 13.5 & 9.3 \\
% %  \midrule
% %  \multicolumn{5}{l}{\textbf{Unified Evaluation Metrics}}\\ 
% %  CTC & 40.4 & 41.5 & 29.0 & 29.4 & 29.6 & 20.7 \\ 
% %  UNI. & 42.8 & 43.0 & 30.2 & 32.2 & 30.6 & 21.4 \\ 
% % \midrule
% %  \multicolumn{5}{l}{\textbf{\deltascore (with GPT-3.5)}}\\ 
% %  - Per & 30.0 & 29.3 & 20.3 & 32.3 & 31.7 & 22.0 \\ 
% %  + Jumble & \cellcolor{green!25}35.3 & \cellcolor{green!25}34.3 & \cellcolor{green!25}23.8 & 31.3 & 31.2 & 21.7 \\ 
% %  + Typos & \cellcolor{green!25}32.0 & \cellcolor{green!25}30.4 & \cellcolor{green!25}21.1 & 32.0 & 31.3 & 21.7 \\ 
% %  \midrule
% %   \multicolumn{5}{l}{\textbf{\deltascore (with BART-cnn)}}\\ 
% %  BARTS & 33.5 & 33.5 & 23.3 & 33.5 & 33.1 & 23.0 \\ 
% %  + Jumble & \cellcolor{green!25}41.9 & \cellcolor{green!25}41.1 & \cellcolor{green!25}28.9 & 33.1 & 32.7 & 23.0 \\ 
% %  + Typos &  \cellcolor{green!25}36.9 & \cellcolor{green!25}37.1 & \cellcolor{green!25}25.9 & \cellcolor{green!25}34.3 & \cellcolor{green!25}33.9 & \cellcolor{green!25}23.7 \\ 
% %  \bottomrule
% % \end{tabular}
% % \caption{Story-level Pearson ($r$), Spearman ($\rho$) and Kendall-Tau ($\tau$) correlations of different metrics on OpenMEVA dataset.}
% % \label{table:corelations_openmeva}
% % \end{table}

\end{document}


