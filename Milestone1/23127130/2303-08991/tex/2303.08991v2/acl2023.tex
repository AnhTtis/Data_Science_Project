% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
	
% \documentclass[xcolor=table]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{xcolor,colortbl}
\usepackage{booktabs,tabularx}
\usepackage{graphicx}

% \usepackage{longtable}
% \usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{amsmath}
% Define the name for Section / SubSection / Table / Figure to save some spaces.
\def\sectionautorefname{\S} 
\def\subsectionautorefname{\S} 
\def\appendixautorefname{Appendix}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{adjustbox}
\usepackage{rotating}
\usepackage{pifont}
\newcommand{\cmark}{\text{\ding{51}}}
\newcommand{\xmark}{\text{\ding{55}}}
% \sisetup{table-parse-only,detect-weight=true,detect-inline-weight=text, round-mode=places, round-precision=1, table-number-alignment=center}

\newcommand{\model}[1]{\textsc{#1}\xspace}
\newcommand{\deltascore}{\model{DeltaScore}}

\newcommand{\tbnum}[1]{\multicolumn{1}{c}{\bfseries \num{#1}}}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{xspace}
\usepackage{amssymb}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\newcolumntype{g}{>{\columncolor{Gray}}c}
\newcommand{\Needcite}[1]{{\color{orange}{\bf{[CITE:]}} #1}}
\newcommand{\Zhuohan}[1]{{\color{red}{\bf{[Zhuohan]}} #1}}
\newcommand{\miao}[1]{{\color{blue}{\bf{[Miao:]}} #1}}

\title{\deltascore: Story Evaluation with Perturbations}

% \title{PScore: Automatic Story Evaluation via Differences between Language Model Likelihood from Perturbations}

\author{Zhuohan Xie 
 \qquad
 Miao Li
 \qquad
 Trevor Cohn 
 \qquad
 Jey Han Lau\\
 School of Computing and Information Systems, \\
 The University of Melbourne \\
 \{zhuohanx, miao4\}@student.unimelb.edu.au, 
\{t.cohn, laujh\}@unimelb.edu.au
 }

\begin{document}
\maketitle
\begin{abstract}

%Automatically evaluating the story quality in AI storytelling is a challenging open problem, and it impedes the development of automatic story generation. %JHL: skipping the cliche
Various evaluation metrics exist for natural language generation tasks, but they have limited utility for story generation since they generally do not correlate well with human judgments and are not designed to evaluate fine-grained story aspects, such as fluency and relatedness.
In this paper, we propose \deltascore, an approach that utilizes perturbation to evaluate fine-grained story aspects. Our core idea is based on the hypothesis that the better the story performs in a specific aspect (e.g., fluency), the more it will be affected by a particular perturbation (e.g., introducing typos). To measure the impact, we calculate the \textit{likelihood difference} between the pre- and post-perturbation stories using large pre-trained language models. We evaluate \deltascore against state-of-the-art model-based and traditional similarity-based metrics across two story domains, and investigate its correlation with human judgments on five fine-grained story aspects: fluency, coherence, relatedness, logicality, and interestingness.
The findings of our study indicate that the \deltascore approach exhibits exceptional performance in evaluating intricate story aspects. An unexpected discovery was made in our experiment, where a single perturbation method was found to effectively capture a majority of these aspects.
% We measure the effectiveness of \deltascore against state-of-the-art model-based and traditional similarity-based metrics over several story domains, by comparing how well it correlates with human judgements on 5 fine-grained story aspects (fluency, coherence, relatedness, logicality and interestingness). We found \deltascore performs very well, with the surprising finding that one particular perturbation appears to work very well for measuring most aspects.
%Our extensive experimental show that \deltascore has substantially better correlations against human judgements versus existing state-of-the-art evaluation metrics without any requirements of fine-tuning, and it achieves fine-grained evaluation of story generation with different aspect-aware perturbations.


% as a consequence of the vast range of possible generations
% Automatic story evaluation has long been admittedly a challenging task.
% Evaluation metrics that are originally proposed for 
% other natural language generation (NLG) tasks
% are widely adopted in current story model developing literature.
% However, they have been criticized being not suitable for open-ended generations,
% since they usually show low correlations to human evaluations, which is the de facto
% standard for story evaluation.
% State-of-the-art (SOTA) story evaluation metrics train classifiers to distinguish original stories from the negative samples or
% highly-upvoted stories from lowly-upvoted ones.
% As a result, they obtain one story evaluator that can evaluate story coherence or human preference.
% These approaches only provide one overall score, which do not align well with the multiple aspects of story quality.
% We propose to evaluate story via differences between GPT3 likelihood from perturbations.
% Experiments show our methods have a higher correlations to human evaluations,
% and can provide explainability to multiple aspects.

\end{abstract}


%JHL: after reading the whole paper, I think in terms of narrative introduction we can do something like this: (1) 

\section{Introduction}
\label{sec:introduction}

% \Zhuohan{Aspects are usually highly entangled, highly unlikely we are going to find one perturbation that works on each aspect.}

% \Zhuohan{text simplication}

%JHL2: suggest we don't use both PLM and LLM acronyms. I think we can just stick with PLMs, since our method isn't limited to just 'large' LMs.
The advent of pre-trained language models (PLMs) and large language models (LLMs) \citep{radford2019language, lewis-etal-2020-bart, DBLP:conf/nips/BrownMRSKDNSSAA20, zhao2023survey} has enabled story generation models to produce plausible stories \citep{tan-etal-2021-progressive, zhang-etal-2022-persona, DBLP:conf/emnlp/YangTPK22}. In fact, the best models have been found to produce stories that are virtually indistinguishable from those written by humans \citep{karpinska-etal-2021-perils, dou-etal-2022-gpt, DBLP:journals/corr/abs-2301-09790}.
However, progress in the development of automatic evaluation metrics has not had the same momentum \citep{guan-etal-2021-openmeva}.
%JHL: I'm commenting out the following line because, in spite of the lack of story evaluation metrics, story generation has progressed much in the past few years, and so the argument that it impedes the development of story generation isn't quite true
%, and the lack of reliable metrics  impedes the development of story generation .
Although human evaluation remains the gold standard, it is often slow, expensive, and difficult to reproduce \citep{DBLP:journals/csur/SaiMK23}. Therefore, there is a pressing need for better automatic methods to evaluate story quality.

\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/Examples-add_typos.pdf}
         \caption{Perturbation ``Add typos'' affects the higher quality story (top) more.
         }
         \label{fig:addtypos}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/Examples-remove_keyentity.pdf}
         \caption{Stories conditioned on the prefix ``I always go to the local supermarket''. Perturbation ``Remove key entity'' affects the higher quality story (top) more.
         % Higher quality story is affected more than lower quality one.
         }
         \label{fig:removekeyentity}
     \end{subfigure}
        \caption{
        Scenarios where higher quality stories (top) are 
        affected more than lower quality ones (bottom) through aspect-specific perturbations (fluency: ``Add typos''; relatedness: ``Remove key entity'').
        Generative likelihood for original/perturbed story is in blue/green circle, and
        the \deltascore value is in orange circle. 
        % \miao{Better to give examples that show aspect-aware perturbations. [Changed]}
        %JHL2: make the bottom 'it is a nice dog' -> 'it is a nice' ? (the current example doesn't remove any key entity), and also make the score lower? 0.75 seems very high for an unrelated story.
        }
        \label{fig:examples}
\end{figure}

The predominant evaluation metrics for story evaluation have their origins in other natural language generation (NLG) tasks, including BLEU \citep{papineni-etal-2002-bleu} and NIST \citep{NIST} for machine translation and ROUGE \citep{lin-2004-rouge} for summarization. Nonetheless, recent efforts have led to the development of novel metrics for story evaluation, aiming to quantify story coherence \citep{guan-huang-2020-union, ghazarian-etal-2021-plot} or learn human preferences \citep{DBLP:conf/emnlp/ChenVTMN22}.
Other works have directly utilized the likelihood of a story under a PLM \citep{DBLP:conf/nips/VaswaniSPUJGKP17, han-etal-2022-go} or its conditional likelihood based on human references or other contextual factors, such as story title \citep{thompson-post-2020-automatic, DBLP:conf/nips/YuanNL21}. However, these approaches typically produce a single score that estimates the overall quality of the generated story. \citet{DBLP:conf/coling/ChhunCSC22} argue that the quality of a story is composed of multiple fine-grained aspects, such as fluency and adherence to commonsense, and that an overall quality score has limited utility for story evaluation. In other words, a metric that produces a low overall score for a story does not reveal whether the story has fluency issues or certain elements of the story violate commonsense \cite{DBLP:journals/corr/abs-2203-11131}.

In this paper, we propose \deltascore, an approach that assesses story quality by calculating the \textit{likelihood difference} under a pre-trained language model between a story and its perturbed form, with the idea that higher quality stories will be affected more by the modification/perturbation compared to the lower quality ones.
To provide fine-grained assessment of story quality, we experiment with perturbations that target a particular aspect (e.g.,\ fluency).  \autoref{fig:examples} presents two examples to demonstrate the intuition of our approach:
% \miao{Give description about aspect-aware perturbations.} 
(1) 
%the story $\boldsymbol{s_1}$: ``Mike dna Jake aer best friends!'' contains two typos, which are less fluent than the story $\boldsymbol{s_2}$: ``Mike and Jake are best friends!''.
when we modify the two stories  in \autoref{fig:addtypos} by randomly adding typos, the more fluent story (top) is impacted by the perturbation more than the less fluent story (bottom);
% Therefore, 
(2) 
%\textbf{Relatedness}: for the title ``I always go to the local supermarket'', we have two stories,
%the story $\boldsymbol{s_1}$: ``The supermarket has various kinds of goods.'' is highly related to the title while the the story %$\boldsymbol{s_2}$: ``It is a nice day.'' is less related.
when we modify the two stories --- conditioned on the story title ``I always go to the local supermarket'' ---  in \autoref{fig:removekeyentity} by removing key entities, the story that is more related to the title (top) is again more affected. 
Through our empirical analysis, we have demonstrated that the \deltascore methodology outperforms existing model- and similarity-based metrics in evaluating intricate story quality aspects. Our investigation also revealed an intriguing finding, whereby the word jumbling perturbation technique displays exceptional accuracy in capturing diverse aspects. This discovery implies that there may be interdependence among these fine-grained aspects.
% One surprising finding is that one particular perturbation (word shuffling) appears to work very well to capture multiple aspects, suggesting that the fine-grained aspects may be mutually correlated.
%The key entity ``supermarket'' is removed from $\boldsymbol{s_1}$ while $\boldsymbol{s_2}$ is not affected.
% Perturbation ``Add typos'' is applicable to both stories, while it cause more damages to the higher quality one.
% Different from the previous works,
% we propose to evaluate generated stories with differentiating perturbations,
% which is we calculate the difference of conditional likelihood of the original stories and
% that of their perturbations.
% Besides, we propose to evaluate stories via differences between GPT3 likelihood instead of using likelihood directly as evaluation metrics 
% as we posit that higher quality stories are more sensitive to perturbations than
% lower quality ones.
%There are two benefits in evaluating stories via differences between likelihood:
%1) It can result in a statistically more stable evaluation metric that correlates better to human evaluation.
%2) Different aspects of story quality can be evaluated by designing different perturbations.

%Specifically, we use generative likelihood assigned by GPT-3.5\footnote{It is also called text-davinci-003, which is the largest and latest version of GPT-3 with an available API at the time of our experiments. Note that even though ChatGPT is released and popular all over the world, it does not yet provide an API to obtain its generative likelihood at the time of writing.} as advanced large language models have demonstrated spectacular performance of understanding narratives \citep{Qin2023IsCA}.


% \miao{To evaluate the effectiveness of \deltascore, we ..., and we get that ...}

%JHL2: don't think we need to spell out the contributions again, since i thought it's quite clear from the intro
% Our contributions of the paper are summarized as follows:
% \begin{itemize}
% % \item We propose a novel evaluation technique that does not require any additional fine-tuning: $\Delta$Likelihood ($\Delta$LL) where we evaluate story quality via differences between generative likelihood from perturbations.
% \item We propose to use likelihood difference based on perturbations as a new approach for fine-grained evaluation of generated texts.
% %\item We propose a novel and unsupervised evaluation approach to measure fine-grained aspects of story quality.
% \item Experimental results indicate that our metric demonstrates a strong correlation with human evaluations in most aspects, surpassing the performance of advanced metrics across various story domains.

% Experimental results show that our metric correlates well with human judgements for most aspects, outperforming state-of-the-art metrics over several story domains.

% \item We demonstrate the technique is applicable to both auto-regressive models with GPT3 and sequence-to-sequence (seq2seq) models with BART.
% \item We explore various perturbations and our results show $\Delta$LLs can have a much better correlations to human evaluation than using likelihood as evaluation metrics directly by applying certain perturbations.
% \item We compare with extensive state-of-the-art (SOTA) evaluation metrics and find our best $\Delta$LL outperforms in all quality aspects of story evaluation.

% \item We show binary classification idea is not suitable for metrics as it can only produce scores that close to 0 and 1s, which is not suitable for story evaluation.
%\end{itemize}


\section{Related Work}
\label{sec:relatedwork}

\subsection{Automatic Story Evaluation}
The conventional evaluation metrics used in natural language processing mainly focus on measuring lexical overlap between machine-generated text and its human reference \citep{papineni-etal-2002-bleu, NIST, lin-2004-rouge}. However, such methods are limited in their ability to capture semantic similarity, and they are vulnerable to small changes in morphology or even typos, as highlighted by \citet{kaster-etal-2021-global}.
To overcome these limitations, recent methods have exploited the language understanding capability of pre-trained language models (PLMs) to capture semantic similarity by comparing embeddings \citep{zhao-etal-2019-moverscore, DBLP:conf/iclr/ZhangKWWA20}. However, these evaluation metrics still rely on a single human reference, which may not be sufficient for story evaluation, given that there can be multiple good stories for a given condition.

Therefore, researchers have started to focus on reference-free evaluation metrics that are tailored to evaluate stories. For instance, \citet{guan-huang-2020-union} train a binary classification model to distinguish between original stories and their negative perturbations, while \citet{ghazarian-etal-2021-plot} extend this idea by generating negative samples from manipulated story plots instead of heuristic rules. These metrics place emphasis on evaluating story coherence. More recently,
\citet{DBLP:conf/emnlp/ChenVTMN22} propose a new task that aims to evaluate human preference by training a model to differentiate highly-upvoted stories from lowly-upvoted ones, drawing labelled data from Reddit.
%JHL2: double check if the above detail (Reddit) is right
This task is particularly useful for evaluating the subjective aspects of stories that cannot be easily captured by traditional metrics. 

% These methods work as a general method for natural language generation tasks, 
% however, as different tasks have different characteristics, 
% automatic metrics tailored for specific tasks such as summarization \citep{scialom-etal-2021-questeval}, data-to-text \citep{rebuffel-etal-2021-data}, and dialogue generation \citep{mehri-eskenazi-2020-usr} have to be proposed.
% There are also alternative evaluation metrics that evaluate the model holistically by comparing the difference of distributions between the learnt model and the dataset \citep{DBLP:conf/nips/PillutlaSZTWCH21, DBLP:conf/emnlp/DengKR22}

% \paragraph{Story Evaluation Metrics}
% Some evaluation metrics are specifically designed for story evaluation.


% \paragraph{Unified Evaluation Metrics}
While conventional evaluation metrics typically provide a single score for overall quality, recent evaluation metrics have started to work assessing quality from different perspectives by offering different combinations of inputs, such as given prompts, generated text, and additional context \citep{DBLP:conf/emnlp/Zhong0YMJLZJH22}. For example, \citet{DBLP:conf/nips/YuanNL21} consider text evaluation as a text generation problem, while \citet{deng-etal-2021-compression} exploit information alignment to evaluate the generated text.

Although these methods provide several assessments of text quality from a few perspectives, they do not correspond to more intuitive quality aspects such as fluency. To address this limitation, \citet{DBLP:conf/emnlp/Zhong0YMJLZJH22} consider text evaluation as a question answering problem and explore training a language model to answer specific questions, such as ``\textit{Is this a fluent sentence?}''. \citet{DBLP:journals/corr/abs-2302-04166} propose utilizing the generalisation capability of large language models for evaluation, by prefixing instructions such as ``\textit{Generate a fluent story for the given title}'' as part of the input to help the likelihood computation to align to a particular aspect.

%In our work, we propose a novel evaluation approach that targets multiple story quality aspects, which to the best of our knowledge, is the first of its kind.

\subsection{Natural Text Perturbation} 
Perturbation has been widely used as a conventional technique to generate negative samples that can be utilized in both discriminative \citep{guan-huang-2020-union} and generative \citep{DBLP:conf/emnlp/Zhong0YMJLZJH22} NLP tasks.
To conduct behavioral tests of NLP models, \citet{ribeiro-etal-2020-beyond} proposes a CheckList that provides a suite of negative examples constructed from a combination of perturbations.
Similarly, \citet{DBLP:conf/emnlp/KarpinskaRTSGI22} designs a list of perturbation tests to evaluate the robustness of evaluation metrics for machine translation.
Moreover, \citet{sai-etal-2021-perturbation} extends the idea of perturbations to evaluate the robustness of NLG evaluation metrics.
To find blind spots of model-based evaluation metrics, \citet{DBLP:journals/corr/abs-2212-10020} designs perturbation tests.
It is worth noting that all these perturbations are based on heuristic rules. In contrast, recent adversarial attacks, such as those proposed by \citet{li-etal-2020-bert-attack, morris-etal-2020-textattack}, apply language models to generate adversarial examples, which can also be considered as another form of text perturbation.
In this work, we experiment with perturbation for a different purpose: to evaluate story quality in a more fine-grained manner.

% Even though these approaches might produce much more dedicated negative samples,
% we do not consider these approaches, since the introduction of an additional black-box language models make the whole process less interpretable.

\section{\deltascore}
\label{sec:deltascore}

% Note: The most ideal way is to find specific perturbations that each works for one specific
% aspect/and some work for general, but I cannot find something like this works now.


%JHL: minor, but maybe some of the perturbation terms can be modified to sound a bit more natural/intuitive:
% Jumble -> Shuffle; GeneralWord -> ? (I thought this is replaced with a hypernym, but looking at the example (girl -> baby), I am not sure anymore); AntonymAttributes -> Antonym; LameWords -> Synonym?; LameSents -> Paraphrase?
\begin{table*}[t]
\centering
\small
\begin{tabular}{p{1cm} p{2.2cm} p{5.5cm} p{5.5cm}}
\toprule
\textbf{Aspect} & \textbf{Perturbation} & \textbf{Original story} & \textbf{Perturbed story} \\
\midrule
\multirow{2}{1cm}[-0.5ex]{{Flu.}} & {Typo} & he went to see what the problem was & he went to see whta the problem was \\
\cmidrule{2-4}
 & {SubjVerbDis} & he is the best student in the classroom . & he am the best student in the classroom . \\
 \midrule
 \multirow{2}{1cm}[-2.0ex]{{Coh.}} & {Jumble} & We play badminton every evening . & badminton every We evening play . \\
 \cmidrule{2-4}
  & \multirow{2}{1cm}[0ex]{{SentReorder}} & she did n't intend to buy anything . unfortunately she has poor impulse control ... & unfortunately she has poor impulse control . she did n't intend to buy anything ... \\ 
 \midrule
 \multirow{2}{1cm}[-0.5ex]{{Rel.}} & \underline{RmkeyEntities} & The supermarket has various kinds of goods & The has various kinds of goods  \\
 \cmidrule{2-4}
  & \underline{StoryReplace} & The supermarket has various kinds of goods & It is a nice day to hang out  \\
 \midrule
 \multirow{2}{1cm}[-3.0ex]{{Log.}} & \multirow{2}{1cm}[0ex]{{Antonym}} & The boy got the gift he always wanted, he was so happy .  & The boy got the gift he always wanted, he was so sad . \\
 \cmidrule{2-4}
  & \multirow{2}{1cm}[0ex]{\underline{Commonsense}} & they took me down to the lake . i threw my line out and caught several worms ... & they took me to the moon. i threw my line out and caught several stars ... \\
 \midrule
 \multirow{2}{1cm}[-1.0ex]{{Int.}} & \multirow{2}{1cm}[-1.0ex]{\underline{BlanderNarrative}} & i felt really angry, talked to my estranged father , and he gave me a gun! But I knew violence is not a solution here . & I felt upset and talked to my father about it . He advised me to handle the situation calmly , so I decided not to resort to violence . \\
 \bottomrule
\end{tabular}
\caption{Selected perturbations that focus on each story quality aspect: Fluency (Flu.), Coherence (Coh.), Relatedness (Rel.), Logicality (Log.), and Interestingness (Int.).
For stories on relatedness aspects, they are conditioned on the title ``I always go to the local supermarket''.
 \underline{Underlined perturbations} are originally proposed by us.
}
\label{table:perturbation}
\end{table*}

% \subsection{GPT3Score}
% Unlike PRISM \citep{thompson-post-2020-automatic} or BARTScore \citep{DBLP:conf/nips/YuanNL21}, we did not use seq2seq pre-trained models.
% GPT3 is a pure generative autoregressive decoder, which do not take condition and predict the current purely based on previous tokens.


%JHL: definition of story condition c seems odd; typically it's the story title or something, no (not sure why it says it can be human reference).
% Answer: For condition, I meant to say people usually use conditional likelihood to evaluate the text (like BARTScore). The condition can be the source, therefore, it is reference-free evaluation. Or the condition can be the reference, therefore, it is refrence-based evaluation.

We now describe the idea of our approach.
Given a story condition (e.g.\ story title) $\boldsymbol{c}=c_1, ..., c_n$ containing $n$ tokens, a model-generated story $\boldsymbol{s}=s_1, ..., s_m$ containing $m$ tokens, and a perturbed/modified story $\boldsymbol{s}' = s'_1, ..., s'_{m'}$ containing $m'$ tokens, \deltascore calculates the likelihood difference under a language model:
% \scalebox{0.9}{
\begin{align}
    \operatorname{\deltascore}(\boldsymbol{s}) = \log p(\boldsymbol{s}|\boldsymbol{c}) -   \nonumber \\
    \log p(\boldsymbol{s}'|\boldsymbol{c})
\end{align}

where $p(\boldsymbol{s}|\boldsymbol{c})$ represents the likelihood of $s$ conditioned on $c$ under a language model. In our experiments, we investigate several large language models with varying architectures (see \autoref{subsec:likelihood_calculation}) and perturbation techniques that are designed to target specific aspects (see \autoref{sec:perturbation}).

%\footnote{All tokens to represent the title, story and modified story are all from the same vocabulary and there could be different vocabularies for different LLMs.}

%different quality aspects of $\boldsymbol{s}$ with the difference between likelihoods of $\boldsymbol{s}$ and $\boldsymbol{s}' = s'_1, ..., s'_{m'}$ which contains $m'$ tokens and is based on a specifically designed perturbation. For each quality aspect of $\boldsymbol{s}$, we first obtain perturbation of the story for the aspect (Section \ref{sec:perturbation}), and then the final score is calculated as the likelihood difference between $\boldsymbol{s}$ and $\boldsymbol{s}'$. The log-likelihood difference is defined as:


% As GPT-3.5 is good at story narrative understanding, we use it as the language model without fine-tuning to obtain the generative likelihood of both $\boldsymbol{s}$ and $\boldsymbol{s}'$. 

\subsection{Two Different Likelihood Calculations}
\label{subsec:likelihood_calculation}

%JHL: revise this section a bit to move away from GPT3-5, and instead say the formulation is based on autoregressive language models (e.g. GPT3). The core idea of DELTASCORE is likelihood difference, and the specific LM we use is flexible so we should define DELTASCORE in a more general manner.


We now explain how we compute $p(\boldsymbol{s}|\boldsymbol{c})$
with encoder-decoder PLMs (e.g.\ BART \citep{lewis-etal-2020-bart} and T5 \citep{DBLP:journals/jmlr/RaffelSRLNMZLL20}) and decoder-only PLMs (e.g.\ GPT3 \citep{DBLP:conf/nips/BrownMRSKDNSSAA20}).

Denoting language model parameters as $\theta$, we compute \deltascore as follows for
encoder-decoder PLMs:
\begin{align}
\log p(\boldsymbol{s}|\boldsymbol{c}) &= \frac{1}{m}\sum_{t=1}^{m}\ \operatorname{log} p(s_t|\boldsymbol{s}_{<t}, \boldsymbol{c}, \theta)
\\
\log p(\boldsymbol{s}'|\boldsymbol{c}) &= \frac{1}{m'}\sum_{t=1}^{m'} \operatorname{log}\ p(s'_t|\boldsymbol{s}'_{<t}, \boldsymbol{c}, \theta)
\end{align}
where, $t$ denotes timestep in the sequence, and $\boldsymbol{s}_{<t}$ denotes all tokens before the current timestep. Intuitively, the story condition $c$ is captured by the encoder, and the likelihood of the story $s$ is produced by the decoder.


For decoder-only PLMs, we have to concatenate $\boldsymbol{c}$ and $\boldsymbol{s}$/$\boldsymbol{s}'$ to form one sequence (e.g.\ $\boldsymbol{x} = x_1, ..., x_{n+m} = c_1, ..., c_n, s_1, ..., s_m$) to compute \deltascore:
\begin{align}
\log p(\boldsymbol{s|\boldsymbol{c}}) &= \frac{1}{m}\sum_{t=n+1}^{n+m}\ \operatorname{log}\ p(x_t|\boldsymbol{x}_{<t}, \theta)
\\
\log p(\boldsymbol{s}'|\boldsymbol{c}) &= \frac{1}{m'}\sum_{t=n+1}^{n+m'} \operatorname{log}\ p(x'_t|\boldsymbol{x}'_{<t},\theta)
\end{align}

Intuitively, we feed the full sequence including the story condition $\boldsymbol{c}$ and story $\boldsymbol{s}$ as input to the decoder-only PLM, although when computing the aggregate likelihood we only consider conditional likelihood for the $\boldsymbol{s}$ tokens.
%Note here we ignore the token probabilities of the title $\boldsymbol{c}$; therefore, the timestep starts from $n+1$.
%We assign the equal weights to each token for \deltascore following \citet{DBLP:conf/nips/YuanNL21}.
% \footnote{We also tried various normalization approaches, such as PenLP and NormLP \citep{lau-etal-2020-furiously},
% but they did not yield constant performance improvement; 
% therefore, we decide to use the uniform normalization.}

% In this paper, we explore GPT3 \citep{DBLP:conf/nips/BrownMRSKDNSSAA20} as the
% representative for auto-regressive models.
% We refer evaluating with the generation likelihood from GPT3 to GPT3Score, therefore:

% \begin{align}
% \operatorname{GPT3Score}(\boldsymbol{s}) = \frac{1}{m}\sum_{t=n+1}^{n+m}\ \operatorname{log}\ p(x_t|\boldsymbol{x}_{<t}, \theta) \nonumber
% \end{align}

% We explore BART \citep{lewis-etal-2020-bart} as the
% representative for seq2seq models.
% Evaluating with the generative likelihood from BART is the same as BARTScore \citep{DBLP:conf/nips/YuanNL21}.

% \begin{align}
% \operatorname{BARTScore}(\boldsymbol{s}) = \frac{1}{m}\sum_{t=1}^{m}\ \operatorname{log}\ p(s_t|\boldsymbol{s}_{<t}, \boldsymbol{c}, \Phi) \nonumber
% \end{align}

\subsection{Perturbations on Story Aspects}
\label{sec:perturbation}

As noted by \citet{DBLP:journals/corr/abs-2301-09790}, the fundamental aspects of story quality include fluency, coherence, relatedness, logicality, and interestingness. In order to perform a fine-grained evaluation of story quality, we propose aspect-aware perturbations of stories. We posit that perturbations designed for each aspect should have distinct objectives.
% We first classify conventional perturbations \citep{sai-etal-2021-perturbation, guan-etal-2021-openmeva, DBLP:journals/corr/abs-2212-10020} into these aspects based on their features.

%JHL: since we've defined story condition c previously, let's make the terminology consistent throughout the paper when we refer to it (i.e. avoid using another term like 'prompts' to refer to c)

\begin{itemize}
    \item \textbf{Fluency:} Perturbations on fluency usually focus on modifications on word or phrase level, which results in issues within one sentence.
    This usually includes intentionally using incorrect verb tenses, subject-verb agreement or word/phrases repetitions.
    \item \textbf{Coherence:} Perturbations on coherence usually focus on modifications on sentence level while keeping each individual sentence fluent, aiming to make the story have conflicts between sentences.
    This usually includes intentionally repeating one sentence, replacing one sentence from one unrelated story, or reordering all sentences. 
    \item \textbf{Relatedness:} Perturbations on relatedness need to alter the story so that it is less relevant to the story condition.
    It can be achieved by either removing key relevant information relevant to the prompt or replace the entire story by one story written for the different prompt. 
    \item \textbf{Logicality:} Perturbations on logicality need to alter the story so that it disobey commonsense.
    This could include characters that defy the laws of physics or events that go against cultural norms such as ``go trick or treating'' on ``Christmas''.
    \item \textbf{Interestingness:} Perturbations on interestingness need to alter the story so that it is generally less interesting to read.
    This could include changing the tone of the story from dramatic to bland or altering descriptive words with flat ones.
\end{itemize}

% As these quality aspects are not completely orthogonal, a perturbation on one aspect could also impact another aspect even if the perturbation is designed for the original aspect.
% Some quality aspects, especially interestingness, are relatively more subjective and dependent on individual perspectives and preferences. Therefore, our taxnomy might not be applicable to general domains.

Due to the interdependence of these quality aspects, perturbations designed for one aspect can potentially affect other aspects. In particular, aspects such as interestingness, are more subjective and influenced by individual perspectives and preferences. Hence, our taxonomy may not be universally applicable to all domains.

To provide a comprehensive evaluation of story quality, we first align the perturbations proposed in previous works \citep{sai-etal-2021-perturbation, guan-etal-2021-openmeva, DBLP:journals/corr/abs-2212-10020} to the fundamental aspects of story quality identified by \citet{DBLP:journals/corr/abs-2301-09790}. In addition, we propose several new perturbation approaches, the majority of which are developed with the assistance of ChatGPT. The details of the prompts used for these perturbations can be found in \autoref{appendix:perturbationprompts}.

\begin{itemize}
\item \textbf{RmkeyEntities:} We leveraged ChatGPT to extract all entities related to the given title and subsequently remove them from the story.
\item \textbf{StoryReplace:} We selected a story from different story conditions with the most similar story quality, as determined by the generative likelihood of the story without its condition.
\item \textbf{Commonsense:} We utilized ChatGPT to modify stories with minimal adjustments to violate commonsense.
\item \textbf{BlanderNarrative:} We requested ChatGPT to modify stories with minimal changes to render the narratives less interesting.
\end{itemize}

In some perturbations, it is possible to control the degree of perturbation. For instance, we can specify the percentage of words to shuffle in jumble. We set the degrees to 0.4, 0.9, and 0.8 for typo, jumble, and antonym perturbations, respectively. In \autoref{subsec:PerturbationDegree}, we investigate the effects of varying the perturbation degrees. However, due to space constraints, we only present a selection of perturbations in \autoref{table:perturbation}.

% To address the relatedness aspect, we propose replacing the entire story. However, randomly replacing the story is not feasible as we cannot guarantee the relevance to the given prompts or the quality of the replaced stories. To mitigate this, we select all stories that are produced from different prompts for each given story and compute their likelihood without prompts from GPT-3.5 to assess their quality. We then calculate the absolute value of the difference between the current story and all other stories and select the one with the closest quality.

% For logicality, we propose replacing certain elements of a story to create a violation of commensense while maintaining its fluency and coherence. To achieve this, we rely on ChatGPT by asking it to revise the story to create minimal changes that do not make sense using the prompt ``Revise the following story such that certain elements does not make sense. The revision should be minimal, e.g. by changing a few words.''. Examples of these revisions can be found in \autoref{table:perturbation}, and we test more perturbations including Add Negation (AddNe), Antonym (Anton).

%JHL: i agree with the point about interestingness perhaps being subjective, but don't quite understand the following point (temporarily highlighting this line out for now)
%Answer from Zhuohan: What I am trying to say is that what we believe is interesting / what we believe these perturbations might affect interestingness might not be really applicable to others. Also, these might not be able to affected by our annotations. This also apply to other aspects, such as logicality and relatedness. But in generally, I believe interestingness is the most subjective aspect here.


\section{Meta-evaluation Experiments}

In this section, we provide an overview of the meta-evaluation datasets used in our experiments as well as the evaluation metrics we compare \deltascore to.

\begin{table}[t]
\centering
\small
\begin{tabular}{p{0.9cm} p{2.3cm} p{3.1cm} }
\toprule
\textbf{Dataset} & \textbf{Condition} & \textbf{Story}  \\
\midrule
ROC & [FEMALE] dad took me fishing . & we sat in a spot and waited for days ... \\
\midrule
 WP & tell me a story where the first line and last line ... & as i walked into the house , i was assailed by the smell of aging ... \\
 \bottomrule
\end{tabular}
\caption{Sampled examples of given story condition and its generated story for each dataset.
}
\label{table:dataexample}
\end{table}

% The details can be found in \Needcite{Appendix}.

% \paragraph{Overall Quality Scores}

% %JHL: given the narrative in the intro, I no longer think we need this set of results (OPENMEVA). [Deleted]
% We also use manually annotated stories from OpenMEVA \citep{guan-etal-2021-openmeva}, which covers ROC and WP.
% It contains various generation models including: 
% 1) a \textbf{Seq2Seq} model \citep{DBLP:conf/nips/SutskeverVL14},
% 2) \textbf{Fusion} \citep{fan-etal-2018-hierarchical},
% 3) \textbf{Plan\&Write} \citep{DBLP:conf/aaai/YaoPWK0Y19},
% 4) fine-tuned \textbf{GPT-2} \citep{radford2019language} and
% 5) \textbf{KGGPT2}.
% % They randomly sample 200 stories from test sets of ROC and WP for story generation,
% % therefore,
% % MANS contains 2 $\times$ 200 $\times$ 5 = 2,000 annotated stories.
% % They use Amazon Mechanical Turk (AMT)\footnote{\url{https://requester.mturk.com/}} for human judgements and
% They ask the annotators to rate each story with a 5-point Likert scale in terms of the overall quality.\footnote{
% The authors of OpenMEVA also asked annotators to select the types of errors in the story, such as
% \textit{repetitive plots},
% \textit{unrelated events},
% \textit{conflicting logic},
% or \textit{chaotic scenes}.
% However, these error types have not bee released and as such we only use overall quality labels in this work.}

% We randomly sample 20 conditional contexts (e.g.,\ titles) from each dataset 
% and collect stories generated by all models for human evaluation.
% Each story (including human-written one) is judged by 3 annotators, 
% and so we have 320 annotated stories in total (140/100/80 for ROC, WP and CNN, respectively).

% Recently, \citet{DBLP:conf/coling/ChhunCSC22} propose
% a new story evaluation benchmark, HANNA.
% However, they present both human reference and generated story to annotators in AMT
% and ask annotators to judge the generated story.
% We concern that such way can make annotators give higher scores to stories that
% ``reads similar'' to given human reference and thus,
% favoring reference-based evaluation metrics and their results do show
% reference-based evaluation metrics have a better performance on HANNA.
% Therefore, we do not include HANNA as another baseline for GPTScore.

% \subsection{Compared Evaluation Metrics}

\begin{table}[t]
\centering
% \begin{adjustbox}{max width=\linewidth}
\small
% \begin{tabular}{p{1.8cm} p{1.5cm} p{1.0cm} p{1.0cm} }
\begin{tabular}{p{1.7cm}lcccc}
\toprule
 \textbf{Objective} & \textbf{Metric} & \textbf{FT}  & \textbf{B/F} & \textbf{ST} & \textbf{MS}   \\
\midrule
  \multirow{3}{2cm}[-1ex]{Similarity} & BLEU & \xmark & B & \xmark & \xmark \\
 \cmidrule{2-6}
 & BERTScore & \xmark &  B  & \xmark & \xmark \\
 \cmidrule{2-6}
 & MoverScore & \xmark & B & \xmark & \xmark \\
 \midrule
  \multirow{3}{1cm}[-5ex]{Discriminative} & UNION & \cmark & F & \cmark & \xmark \\
 \cmidrule{2-6}
  & MANPLTS & \cmark & F & \cmark & \xmark \\
 \cmidrule{2-6}
 & StoryER & \cmark & F &\cmark & \xmark \\
 \cmidrule{2-6}
  & CTC & \cmark & B\&F & \xmark & \cmark \\
 \cmidrule{2-6}
 & UNIEVAL & \cmark  & F & \xmark  & \cmark \\
 \midrule
 \multirow{2}{1.5cm}[-0.5ex]{Generative} & BARTScore & \xmark & B\&F & \xmark & \cmark \\
 \cmidrule{2-6}
 & GPTScore & \xmark & F & \xmark & \cmark \\ 
 \bottomrule
\end{tabular}
% \end{adjustbox}
\caption{Statistics of Compared evaluation metrics. 
``FT'' indicates whether the metric requires additional synthetic data to fine-tune on.
``R-B/F'' indicates whether the metric is reference-based (B) or reference-free (F).
``ST'' indicates whether the metric is originally designed for story evaluation.
``MS'' indicates whether the metric produces scores that consider multiple aspects.
}
\label{table:metricdetails}
\end{table}

\begin{figure*}[t]
     \centering
     \includegraphics[width=0.9\textwidth]{figs/bar_chart.pdf}
        \caption{
        Story-level Kendall ($|\tau|$) correlations of different metrics on five aspects of ROC and WP.
        Similarity, Discriminative and Generative indicate the best performing evaluation metrics from such category.
        Higher bar indicates better performance.
        }
        \label{fig:comparisons}
\end{figure*}

\begin{table*}[t]
\small
\centering
\begin{tabular}{lcccccccccc}
\toprule
\multirow{3}{*}{\textbf{Perturbation}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
\cmidrule(lr){2-6}\cmidrule(lr){7-11}
  & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} \\
\cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
 w/o perturbation & 32.0 & 27.6 & 25.1 & 21.1 & 19.7 & 31.6 & 36.0 & 34.7 & 38.5 & 30.6 \\ 
 Typo & 22.9 & 11.4 & \cellcolor{green!25}13.3 & 8.2 & 11.4 & \cellcolor{green!25}42.8 & \cellcolor{green!25}35.6 & \cellcolor{green!25}39.8 & \cellcolor{green!25}41.9 & \cellcolor{green!25}33.0 \\
 SubjVerbDis & 11.9 & 10.5 & 8.8 & 4.3 & 0.9 & 5.8 & 6.2 & 17.9 & 13.2 & 17.3  \\
Jumble & \cellcolor{green!25}35.9 & \cellcolor{green!25}40.0 & \cellcolor{green!25}\textbf{33.8} & \cellcolor{green!25}\textbf{29.6} & \cellcolor{green!25}23.2 & \cellcolor{green!25}52.7 & \cellcolor{green!25}49.2 & \cellcolor{green!25}36.6 & \cellcolor{green!25}53.9 & \cellcolor{green!25}42.8  \\
SentReorder & 20.0 & 16.1 & 10.1 & 7.4 & 4.7 & 4.1 & 17.6 & 5.6 & 12.4 & 3.4 \\ 
RmkeyEntities & 15.9 & 16.9 & 13.3 & 9.6 & 7.6 & \cellcolor{green!25}36.5 & 27.7 & 31.7 & 33.5 & 21.9 \\
 StoryReplace & 6.2 & 2.2 & 9.1 & 9.6 & 7.8 & \cellcolor{green!25}38.5 & \cellcolor{green!25}36.6 & \cellcolor{green!25}37.6 & 38.5 & \cellcolor{green!25}33.8 \\ 
 Antonym & 14.3 & \cellcolor{green!25}22.1 & 10.9 & \cellcolor{green!25}21.3 & \cellcolor{green!25}16.8 & \cellcolor{green!25}41.5 & \cellcolor{green!25}39.5 & \cellcolor{green!25}33.9 & \cellcolor{green!25}42.7 & \cellcolor{green!25}36.6 \\
  Commonsense & 16.9 & 15.8 & 17.5 & 2.2 & 3.8 & 29.6 & \cellcolor{green!25}39.0 & 26.3 & 37.6 & 26.5 \\
 BlanderNarrative & 6.7 & 10.4 & 11.5 & 1.5 & 0.7 & 13.9 & 17.7 & 13.0 & 19.3 & 7.2 \\

 \bottomrule
\end{tabular}
\caption{Story-level Kendall correlation ($|\tau|$) between \deltascore with GPT-3.5 and human evaluations on ROC and WP. 
\colorbox{green!30}{Highlight} indicates the scores where \deltascore achieves better performance over applying generative likelihood from GPT-3.5 directly as the evaluation metric.
}
\label{table:multiperturbations}
\end{table*}

\subsection{Benchmarks and Compared Approaches}



%JHL: ahhhh i am a little surprised to see the CNN story condition is human story here! This is not good, the prompt qualitatively is far too different from the other domains (i.e. in ROC and WP the story condition is a short text, but the CNN story condition has a lot more information and so there's less room in terms of 'creative writing' for the model. This may be the reason why CNN/Dailymail has very very different results! We might need to redo CNN experiments. Also, I suspect we'll run into issues with reference-based evaluation for CNN/dailymail (since the reference is only used as story condition)
%JHL: prompt is never really defined (I understand it's introduced in section 3 but it wasn't really defined there too); suggest we say explicitly what are the c's for the different domains here (e.g. ROC = first sentence; WP = a short prompt that gives the big picture of the whole story; etc and show some examples).
\paragraph{Benchmarks}
% The use of standardized benchmark datasets is crucial for advancing automatic story evaluation as they enable the evaluation metrics to be tested and compared by measuring their correlations to human evaluations.

In our study, we leverage human evaluation scores from \citet{DBLP:journals/corr/abs-2301-09790}, who conducted human evaluations on two widely-used story datasets: ROCStories (ROC) \citep{mostafazadeh-etal-2016-story}, which contains concise stories that incorporate commonsense, and WritingPrompts (WP) \citep{fan-etal-2018-hierarchical}, which focuses on fictional stories. The evaluations were conducted on five aspects of the stories: fluency, coherence, relatedness, logicality, and interestingness, rated on an ordinal scale from 1 (worst) to 5 (best). We provide examples of the stories in \autoref{table:dataexample}, while more detailed information about the datasets can be found in \autoref{appendix:datasetsdetail}.

% As mentioned in \autoref{sec:deltascore}, \deltascore can use both given story condition and human reference as the context for generative likelihood.
% However, open-ended generation tasks such as story generation often suffer from the known challenge of the one-to-many problem, where multiple plausible stories can be generated for a single prompt. Consequently, reference-based evaluation metrics, which are commonly used in other generation tasks, may not be ideal for evaluating story generation \citep{DBLP:conf/emnlp/Zhong0YMJLZJH22}. Hence, to address this issue, we utilize the given story condition as the conditioning factor $\boldsymbol{c}$ to generate the likelihood. More specifically, we use the first sentence and a short prompt as the condition for ROC and WP, respectively. 
\paragraph{Compared Evaluation Metrics}

We describe the evaluation metrics we compare in our experiments and classify them into three categories: similarity, discriminative, and generative, as shown in \autoref{table:metricdetails}. While some of these metrics were not originally proposed for story evaluation, we adapted them to fit our settings. For discriminative metrics, we used the models provided by the authors without additional fine-tuning. We used the reference-free version of CTC and BARTScore to ensure consistency in comparisons, namely CTC (Consistency) and BARTScore ($\boldsymbol{c} \rightarrow \boldsymbol{s}$). Our preliminary experiments showed that reference-free versions perform better than reference-based ones. For UNIEVAL and GPTScore, we adapted the questions and prompts accordingly. More detailed information about the evaluation metrics can be found in \autoref{appendix:evaluationmetrics}.

% While some of these metrics were developed for other NLG tasks, we modify them slightly to make them suitable for evaluating story generation.
% We also indicate if the metric requires additional synthetic data to fine-tune the backbone model on and whether it requires human references when computing the scores.

% We compare \deltascore against representative evaluation metrics introduced in \autoref{sec:relatedwork}.


% Zhuohan Tried to put such comment into footnote into the folowing table about BARTScore, but did not work...



\subsection{Meta-evaluation Results}

In this study, we prioritize the story-level Kendall correlation as the evaluation metric due to the non-linear relationship between human and automatic evaluations and the ordinal scale used in human annotations \citep{Kendall1938ANM}. We compare \deltascore, which is based on OPT \citep{DBLP:journals/corr/abs-2205-01068} with three perturbations, to the evaluation metrics presented in \autoref{table:metricdetails}. We select the best-performing metric from each category for comparison, and the results are shown in \autoref{fig:comparisons}. Further analysis of perturbation effects is presented in \autoref{subsec:perturbationselections} and model effects in \autoref{subsec:modelselection}. Detailed information can be found in \autoref{appendix: correlations}.

Our findings suggest that similarity evaluation metrics such as BLEU, BERTScore, and MoverScore exhibit poor performance in open-ended story evaluation, which is consistent with previous studies \citep{guan-huang-2020-union, DBLP:journals/corr/abs-2301-09790}. This implies that reference is not necessary in assessing open-ended stories.
Interestingly, discriminative evaluation metrics do not demonstrate impressive results despite being designed specifically for story evaluation. This may be due to their extensive training on synthetic data and not on our data, which has different features that could affect their performance in our evaluation scenarios.
Finally, we observe that \deltascore significantly outperforms other metrics, particularly with the jumble perturbation, achieving the best results in all aspects of both datasets.

% We demonstrate \deltascore and observe that jumble and typos produce the most stable results across different models and datasets. Notably, when we use jumble as the perturbation for \deltascore with GPT-3.5, we observe a significant improvement over using the likelihood from GPT-3.5 directly as the metric. Moreover, this approach outperforms all other SOTA evaluation metrics on most aspects across all datasets.
% Replacing the entire story does not improve the performance of \deltascore over directly using the likelihood metric with GPT-3.5. However, it does provide some improvement for BART-cnn and a substantial improvement for BART-large. This may be due to the fact that stronger models are more focused on local context and less sensitive to changes in the prompt.
% Perturbing stories to violate commonsense using ChatGPT does not lead to a significant improvement in the performance of \deltascore compared to using the likelihood metric directly with GPT-3.5, except for in the WP dataset. However, it appears to provide some improvement for BART-cnn and BART-large. This may be due to the fact that when the perturbed stories violate commonsense, GPT-3.5 exhibits similar characteristics to ChatGPT, which results in high likelihood scores being assigned to the contents generated by ChatGPT.

% In general, the quality aspects of stories are heavily intertwined, meaning that perturbations targeting one aspect often impact others as well. Nonetheless, we find that interestingness is particularly challenging to address with perturbations, as it is difficult to identify suitable modifications that effectively target this aspect.













% \subsection{Overall Quality Evaluation Results}

%JHL: think we can drop this section; see my comments below how to handle each subsection
% \section{Analysis}

% \subsection{Generalization}

%JHL: we can drop most of the text here if we introduce DELTASCORE in a general manner previously; we just need to say we will experiment with both GPT3 and BART in the introduction of section 4. Might need to say a bit more about the exact implementation of BART that we use in section 4 (looks like there's BART-para, BART-CNN, etc)
% To test generalization, we also apply \deltascore on BART, note that BART is different from GPT-3.5 as in GPT-3.5 is a pure decoder based model while BART is an encoder-decoder based model.
% Note that, when we utilize the generative likelihood directly from BART to evaluate the story, it is the same as BARTScore \citep{DBLP:conf/nips/YuanNL21}.
% We replace generative language model of \deltascore to BART-para\footnote{The authors from \citet{DBLP:conf/nips/YuanNL21} fine-tunes BART-cnn model on para dataset \Needcite{para}} to explore if the idea can improve BARTScore.  

% We present results in, from which we can see that \deltascore can yield consistent gains in performances in most aspects and overall quality, which shows the idea of differentiating perturbations can also be applied to encoder-decoder architecture.



\section{More Analysis}

\subsection{Perturbations for Different Aspects}
\label{subsec:perturbationselections}

In this section, we investigate the performance of \deltascore with various perturbations that target different aspects of story quality.
We employ GPT-3.5\footnote{It is also called text-davinci-003, and the most capable model from GPT family that provides generative likelihood at the time of our experiments} here as it is the largest generative language model, and has demonstrated predominant performance in other tasks.
We present the complete set of results in \autoref{table:multiperturbations}, and visualize the performance gains of \deltascore over GPT-3.5 in \autoref{fig:heatmaps}.

Our analysis reveals that some perturbations do not improve the performance of \deltascore, despite their targeting of specific aspects of story quality. For example, sentenceReorder does not provide any improvement on \deltascore, possibly due to the focus of LLMs on local logic rather than the global narrative arc of a story. As such, reordering sentences, which does not affect the local logic, is not helpful in enhancing \deltascore. Furthermore, we observe that interestingness, the most subjective aspect, is the most challenging for \deltascore to improve, as designing a perturbation that targets it is difficult.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.21\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/heatmap_roc.pdf}
         \caption{ROC
         }
         \label{ROC+BART-large}
     \end{subfigure}
     \begin{subfigure}[b]{0.265\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/heatmap_wp.pdf}
         \caption{WP
         }
         \label{WP+GPT-3.5}
     \end{subfigure}
        \caption{
        We present improvements of \deltascore on GPT-3.5 over applying GPT-3.5 generative likelihood directly as evaluation metric. Warmer color indicates greater improvement.
        `TP' indicates typo,
        `JB' indicates jumble,
        `SR' indicates sentencereorder,
        `AT' indicates antonym, and
        `BN' indicates blandernarrative. 
        }
        \label{fig:heatmaps}
\end{figure}


%JHL: Here we should talk about the language models we will use, e.g. GPT-3 (text-davinci-003) and BART (size?). 
%JHL: we also need to say a bit more about the implementation of the different perturbations, e.g. how are word shuffling done? we select N% words in the text and randomly shuffle them, or how many typos we create, and how do we create them.  The high level idea of perturbation is explained in 3.2, but here we need to spell out a bit more explicitly how they are actually implemented. If space is tight, we can move this to appendix, but right now it's unclear how the perturbations are implemented.

\begin{table}[t]
\centering
\small
\begin{tabular}{p{1.0cm}p{1.3cm} p{0.6cm} p{0.9cm} p{1.5cm}}
% \begin{tabular}{cllll}
\toprule
\textbf{Arch.} & \textbf{Model} & \textbf{Size} & \textbf{\#Data} & \textbf{Objectives}  \\
\midrule
 \multirow{3}{2cm}[0.5ex]{En-De} & BART & 406M & 160GB & Denoising \\
 \cmidrule{2-5}
  &  FLAN-T5  & 11B & - & Denoising \\
 \midrule
\multirow{3}{2cm}[-1ex]{Cs-De} & BLOOM & 7B & 366BT & LM \\
\cmidrule{2-5}
 & OPT &  66B & 180BT & LM \\
 \cmidrule{2-5}
 & GPT-3.5 & 175B & 300BT & LM \\
 \bottomrule
\end{tabular}
\caption{ Statistics of selected generative models classified by architecture (Arch.) to encoder-decoders (En-De) and Casual decoders (Cs-De).
\#Data indicates the pre-trained data scale (either in the number of tokens or storage size, `BT' indicates ``billion tokens''). 
Size indicates model parameters. LM indicates language modeling.
}
\label{table:modeldetails}
\end{table}

\begin{table*}[t]
\small
\centering
\begin{tabular}{lcccccccccc}
\toprule
\multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
\cmidrule(lr){2-6}\cmidrule(lr){7-11}
  & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} \\
\cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
  \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
  w/o perturbation & 26.4 & 21.3 & 21.1 & 15.8 & 20.3 & 18.1 & 15.4 & 17.5 & 17.6 & 9.3 \\
 Jumble & \cellcolor{green!25}35.4 & \cellcolor{green!25}36.4 & \cellcolor{green!25}21.6 & \cellcolor{green!25}23.4 & \cellcolor{green!25}\textbf{26.8} & \cellcolor{green!25}52.7 & \cellcolor{green!25}52.5 & \cellcolor{green!25}43.6 & \cellcolor{green!25}56.6 & \cellcolor{green!25}44.7 \\
 Typo & \cellcolor{green!25}31.7 & \cellcolor{green!25}28.2 & \cellcolor{green!25}22.0 & 15.3 & 19.0 & \cellcolor{green!25}48.4 & \cellcolor{green!25}46.3 & \cellcolor{green!25}39.0 & \cellcolor{green!25}52.6 & \cellcolor{green!25}40.2 \\
 % AddNe & 9.9 & 16.8 & 16.0 & 6.3 & 8.5 & \cellcolor{green!25}23.9 & \cellcolor{green!25}21.8 & \cellcolor{green!25}24.9 & \cellcolor{green!25}24.5 & \cellcolor{green!25}15.0 \\
 Antonym & 11.8 & 16.1 & 6.5 & 9.6 & 7.7 & \cellcolor{green!25}49.9 & \cellcolor{green!25}46.3 & \cellcolor{green!25}32.5 & \cellcolor{green!25}50.0 & \cellcolor{green!25}42.7 \\
 % RpSt &  \\
 % CS &  \\
\midrule
  \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
  w/o perturbation & 32.0 & 26.7 & 25.3 & 22.0 & 23.5 & 20.8 & 24.8 & 22.2 & 25.7 & 17.1 \\
 Jumble & \cellcolor{green!25}\textbf{43.2} & \cellcolor{green!25}\textbf{42.4} & \cellcolor{green!25}27.4 & \cellcolor{green!25}25.6 & \cellcolor{green!25}23.9 & \cellcolor{green!25}56.1 & \cellcolor{green!25}\textbf{55.5} & \cellcolor{green!25}\textbf{45.9} & \cellcolor{green!25}\textbf{60.9} & \cellcolor{green!25}\textbf{48.5} \\
 Typo & \cellcolor{green!25}35.9 & \cellcolor{green!25}32.0 & \cellcolor{green!25}28.6 & 21.8 & 22.1 & \cellcolor{green!25}44.8 & \cellcolor{green!25}45.8 & \cellcolor{green!25}36.8 & \cellcolor{green!25}50.1 & \cellcolor{green!25}36.6 \\
 % AddNe & 22.7 & \cellcolor{green!25}31.7 & \cellcolor{green!25}28.6 & 17.8 & 19.1 & \cellcolor{green!25}21.6 & \cellcolor{green!25}25.3 & \cellcolor{green!25}27.0 & \cellcolor{green!25}28.9 & \cellcolor{green!25}21.9 \\
 Antonym & 16.5 & 21.9 & 8.4 & 11.9 & 9.3 & \cellcolor{green!25}55.2 & \cellcolor{green!25}51.8 & \cellcolor{green!25}36.9 & \cellcolor{green!25}56.0 & \cellcolor{green!25}45.9 \\
 % RpSt &  \\
 % CS &  \\
\midrule
 \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL 11B)}}\\ 
  w/o perturbation & 21.0 & 22.1 & 26.8 & 23.2 & 22.5 & 13.6 & 10.6 & 12.1 & 10.8 & 4.2 \\
 Jumble & \cellcolor{green!25}35.7 & \cellcolor{green!25}28.2 & 19.9 & 19.8 & 22.2 & \cellcolor{green!25}\textbf{59.2} & \cellcolor{green!25}53.7 & \cellcolor{green!25}41.8 & \cellcolor{green!25}57.4 & \cellcolor{green!25}46.9 \\
 Typo & \cellcolor{green!25}24.2 & \cellcolor{green!25}26.6 & \cellcolor{green!25}27.7 & \cellcolor{green!25}24.6 & \cellcolor{green!25}23.0 & \cellcolor{green!25}34.3 & \cellcolor{green!25}30.5 & \cellcolor{green!25}28.1 & \cellcolor{green!25}34.8 & \cellcolor{green!25}23.1 \\
 % AddNe & 15.4 & \cellcolor{green!25}23.2 & 16.6 & 10.4 & 9.9 & \cellcolor{green!25}24.2 & \cellcolor{green!25}27.4 & \cellcolor{green!25}30.7 & \cellcolor{green!25}31.8 & \cellcolor{green!25}22.6 \\
 Antonym & 9.4 & 16.1 & 3.2 & 7.5 & 5.4 & \cellcolor{green!25}52.6 & \cellcolor{green!25}48.0 & \cellcolor{green!25}37.1 & \cellcolor{green!25}53.8 & \cellcolor{green!25}46.3 \\
 % RpSt &  \\
 % CS &  \\
\midrule
\multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn 406M)}}\\ 
w/o perturbation &  24.9 & 11.6 & 10.9 & 11.3 & 14.6 & 32.2 & 25.6 & 31.0 & 30.2 & 23.0 \\ 
 Jumble & \cellcolor{green!25}29.2 & \cellcolor{green!25}17.7 & 9.9 & \cellcolor{green!25}12.2 & 12.6 & \cellcolor{green!25}49.1 & \cellcolor{green!25}40.1 & \cellcolor{green!25}44.5 & \cellcolor{green!25}46.5 & \cellcolor{green!25}41.5 \\
 Typo & 22.9 & 11.4 & \cellcolor{green!25}13.3 & 8.2 & 11.4 & \cellcolor{green!25}42.8 & \cellcolor{green!25}35.6 & \cellcolor{green!25}39.8 & \cellcolor{green!25}41.9 & \cellcolor{green!25}33.0 \\
 % AddNe & 8.4 & \cellcolor{green!25}16.9 & \cellcolor{green!25}17.2 & \cellcolor{green!25}11.4 & 9.8 & 24.0 & 17.2 & 13.7 & 20.4 & 13.2 \\
 Antonym & 14.3 & \cellcolor{green!25}22.1 & 10.9 & \cellcolor{green!25}21.3 & \cellcolor{green!25}16.8 & \cellcolor{green!25}41.5 & \cellcolor{green!25}39.5 & \cellcolor{green!25}33.9 & \cellcolor{green!25}42.7 & \cellcolor{green!25}36.6 \\
 % RpSt &  \\
 % CS &  \\
\midrule
 \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5 175B)}}\\
 w/o perturbation & 32.0 & 27.6 & 25.1 & 21.1 & 19.7 & 31.6 & 36.0 & 34.7 & 38.5 & 30.6  \\
 Jumble & \cellcolor{green!25}35.9 & \cellcolor{green!25}40.0 & \cellcolor{green!25}\textbf{33.8} & \cellcolor{green!25}\textbf{29.6} & \cellcolor{green!25}23.2 & \cellcolor{green!25}52.7 & \cellcolor{green!25}49.2 & \cellcolor{green!25}36.6 & \cellcolor{green!25}53.9 & \cellcolor{green!25}42.8 \\ 
 Typo & 29.5 & \cellcolor{green!25}28.2 & 22.7 & 15.4 & 11.7 & \cellcolor{green!25}40.7 & \cellcolor{green!25}43.1 & 32.6 & \cellcolor{green!25}47.2 & \cellcolor{green!25}35.4  \\ 
 % AddNe &  21.0 & \cellcolor{green!25}29.5 & \cellcolor{green!25}34.4 & 18.3 & 19.0 & 26.5 & 21.1 & 13.4 & 24.9 & 16.3  \\
 Antonym & 21.4 & 26.7 & 15.1 & \cellcolor{green!25}21.9 & 18.5 & \cellcolor{green!25}48.3 & \cellcolor{green!25}47.3 & \cellcolor{green!25}37.6 & \cellcolor{green!25}51.6 & \cellcolor{green!25}42.1 \\
 % RpSt &  \\
 % CS &  \\
 \bottomrule
\end{tabular}
\caption{Absolute value of Story-level Kendall correlation ($|\tau|$) between different metrics and human evaluations on ROC and WP. 
We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
}
\label{table:multiplemodels}
\end{table*}

\begin{figure}[t]
     \centering
     \includegraphics[width=0.48\textwidth]{figs/line_chart.pdf}
        \caption{
        Effects of perturbation degrees on \deltascore with OPT.
        }
        \label{fig:correlations}
\end{figure}

\subsection{Extending to Other Generative LLMs}
\label{subsec:modelselection}

In order to test the generalization of our proposed approach, we extend it to other generative LLMs. In addition to the OPT and GPT-3.5 models that we demonstrated previously, we also include two representatives of encoder-decoder models, namely BART and FLAN-T5 \citep{DBLP:journals/corr/abs-2210-11416}, as well as one more casual decoder model, BLOOM \citep{Scao2022BLOOMA1}.

In our preliminary experiment, we observed that larger models tend to demonstrate better performance within the same model framework. As such, we applied the largest model size that we could run for each model. The model details are presented in \autoref{table:modeldetails}, while the results are shown in \autoref{table:multiplemodels}.

It has been observed that \deltascore, when applied with the three perturbations, demonstrates a strong performance across all generative models, irrespective of their varying model architectures. In general, it offers a better evaluation metric compared to directly applying likelihood. Among the three perturbations, jumble appears to be the most effective, providing substantial improvement in nearly all aspects across the two datasets. As a result, jumble yields the best-performing \deltascore for each aspect.

It should be noted that \deltascore appears to exhibit better performance on WP as compared to ROC. Our speculation is that this can be attributed to the fact that the stories in WP are significantly more intricate in nature, providing more opportunities for the perturbations to target a greater number of aspects. Consequently, this complexity can potentially provide a benefit to \deltascore.

We have made an interesting observation that larger models generally exhibit better performance, with GPT-3.5 and OPT being the highest performing models and BART being the lowest. BLOOM and FLAN-T5 are positioned in the middle of the spectrum. However, this trend is not always consistent, as evidenced by our findings. Specifically, in this study, OPT emerged as the top-performing model, being the best in 6 out of 10 aspects, while GPT-3.5 won only 2 out of 10 aspects, despite being 2.6$\times$ larger and trained on 1.7$\times$ more tokens of data. This observation aligns with previous research indicating that larger models do not necessarily guarantee better performance.



\subsection{Influence of Perturbation Degrees}
\label{subsec:PerturbationDegree}

%JHL: This is the only additional analysis in this section, we can either move this to a subsection in 5 (e.g. 5.X additional analysis) or to appendix if we need space
In our study, we investigated the impact of perturbation degrees on \deltascore using OPT, as it demonstrated dominant performance in \autoref{table:multiplemodels}. We present the outcomes in \autoref{fig:correlations}, and trends for specific perturbations can be observed. It appears that jumble and antonym do not offer significant improvement when the degree of perturbation is minor, but they exhibit a substantial increase in performance when more words are jumbled, ultimately reaching a stable value. In contrast, typo appears to perform more reliably, regardless of the degree of perturbation. We hypothesize that the language model may not be highly sensitive to small changes in word order, given its masked word infilling objective during training. However, it consistently responds to typos in words, potentially due to the resulting difference in embeddings caused by tokenization.

% \subsection{Model Preference}
% %JHL: we can drop this now since we'll be discussing BART results together with GPT3 in section 5
% We find different models have their different preferences.
% BART-cnn is better at stories generated from older models, while BART-para is better at stories generated recently even though the authors claim that BART-para has better capability due to the extra fine-tuning process.

% \subsection{Assessment Granularity}
% %JHL: Not really sure the objective of this analysis; are we trying to explain why certain metrics don't work (if so, which metric?)
% One of the mainstreams of automatic story evaluation is to train a binary classifier to distinguish good and bad stories \citep{guan-huang-2020-union, ghazarian-etal-2021-plot}.

% However, the nature of binary classifiers can easily result in an evaluator that mainly produce scores close to its labels, usually 0 and 1, while \deltascore utilize the essence of generative language models, therefore it can produce more continuous values that covers wider ranges (see \autoref{fig:binaryclassification}).

% continuous values are more desirable as they not only show which story is better, but also shows the granularity of which it is better even though they might have similar Spearman correlations.



% \begin{figure*}[t]
%      \centering
%      \begin{subfigure}[b]{0.31\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/corr_union-model.ckpt.pdf}
%          \caption{Perturbation ``Remove punctuation'' is applicable to higher quality story but not applicable to lower quality one.
%          }
%          \label{fig:example1}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.31\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/corr_bartscore_CNN_PS_ts1.pdf}
%          \caption{Perturbation ``Add typos'' is applicable to both stories, higher quality story is more affected than lower quality one.
%          % Higher quality story is affected more than lower quality one.
%          }
%          \label{fig:examples2}
%      \end{subfigure}
%      \begin{subfigure}[b]{0.31\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/corr_gpt3score_PS_mean_lp.pdf}
%          \caption{Perturbation ``Add typos'' is applicable to both stories, higher quality story is more affected than lower quality one.
%          % Higher quality story is affected more than lower quality one.
%          }
%          \label{fig:examples3}
%      \end{subfigure}
%         \caption{
%         The correlation figures show that binary classification approaches can only generate scores that close to labels (as set to 0s and 1s in the task)
%         while regression and generation approaches can generate more diverse scores.
%         }
%         \label{fig:binaryclassification}
% \end{figure*}




%JHL: we might not need to have different subsections for the two sets of results, if we end up discussing the results altogether
%JHL: suggestion: use a paragraph to spell out all the tables (2-X), and then discuss the results, and then present additional analysis (e.g. the text in 6.2 and 6.4)
% \subsection{Fine-grained Aspect Evaluation Results}




% We categorize the evaluation metrics into several types based on their features: 1) Similarity-based metrics, including BLEU (BLE), BERTScore (BER), and MoverScore (Mov); 2) Metrics that are specifically tailored for evaluating stories, including UNION (UNIO), MANPLTS (MAN), and StoryER (StoER); 3) Unified metrics designed to evaluate multiple aspects, including CTC and UNIEVAL (UNIE); and 4) Likelihood-based metrics, where the language models' likelihood is used directly, including BART-large (BRT-l), BART-cnn (BRT-c), and GPT-3.5 (G-3.5).\footnote{Please note that the concept of using likelihood is identical to that of BARTScore proposed by \cite{DBLP:conf/nips/YuanNL21}. The authors of BARTScore suggest that this idea can be extended to other generative models as well.} We also present the \deltascore metric using these three generative models.



\section{Conclusion}
This paper introduces \deltascore, a novel approach for evaluating fine-grained story quality by comparing the difference in likelihood between pre- and post-perturbations. Our experiments demonstrate that \deltascore produces stronger correlations with human evaluation results across all story quality aspects compared to current state-of-the-art evaluation metrics. We also show that \deltascore can be applied to various pre-trained language models, including encoder-decoder and causal-decoder models, outperforming the direct application of generative likelihood as an evaluation metric.

% We experiment with multiple language models, including  and our results demonstrate that \deltascore with GPT-3.5 and jumble yields the best performance, outperforming state-of-the-art evaluation metrics.

\section*{Limitations}

Our work explores a limited set of perturbations for story evaluation, but it is likely that there are many more perturbations that could be obtained through different approaches. Although we only apply this perturbation method to story generation in this paper, it has the potential to be adapted for evaluating various aspects of text generation using specifically designed perturbations. This opens up a fruitful area for future research.

% This work explores a limited set of perturbations for story evaluation, but there are likely many more that could be obtained through different approaches. While we only apply this perturbation method to story generation, it has the potential to be adapted for evaluating various aspects of text generation using specifically designed perturbations, opening up a fruitful area for future research.


% \section*{Acknowledgements}

% \begin{figure*}[t]
%      \centering
%      \begin{subfigure}[b]{0.45\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/OPENMEVABARTScore_perturbations_roc.pdf}
%          \caption{ROC
%          }
%          \label{BARTScore-ROC-openmeva}
%      \end{subfigure}
%      \begin{subfigure}[b]{0.45\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/OPENMEVABARTScore_perturbations_wp.pdf}
%          \caption{WP
%          }
%          \label{BARTScore-WP-openmeva}
%      \end{subfigure}
%         \caption{
%         Perturbation results and Pearson correlations on each dataset for OpenMEVA.
%         }
%         \label{fig:perturbationgrades}
% \end{figure*}


% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix


\section{Perturbation Prompts}
\label{appendix:perturbationprompts}

We use following prompts for perturbations.

\begin{itemize}
    \item Relatedentities: Find all entities in the given story that is relevant to the given title. Please only print entities in the given story, and separate them by ','.  ``title'': \{title\}, ``story'': \{story\}
    \item Commonsense: Revise the following story such that certain elements does not make sense. The revision should be minimal, e.g. by changing a few words. ``story'': \{story\}
    \item BlanderNarrative: Revise the following story to make it less interesting (e.g. expected ending, no plot twist). The revision should be minimal. ``story'': \{story\}
\end{itemize}

\section{Meta-evalution Datasets Details}
\label{appendix:datasetsdetail}

\citet{DBLP:journals/corr/abs-2301-09790} evaluate a range of recently proposed story generation models:
1) \textbf{KGGPT2} \citep{guan-etal-2020-knowledge},
2) \textbf{MTCL} \citep{xu-etal-2020-megatron},
3) \textbf{HINT} \citep{guan-etal-2021-long},
4) \textbf{PROGEN} \citep{tan-etal-2021-progressive},
5) \textbf{BART} \citep{lewis-etal-2020-bart} and
6) \textbf{GPT-3} \citep{DBLP:conf/nips/BrownMRSKDNSSAA20},
and also human reference stories.
They conduct a meta-evaluation, which consists of two parts: crowdsourcing through Amazon Mechanical Turk and in-house evaluation by colleagues. We primarily rely on the crowdsourced evaluations since the authors suggest that in-house annotators may have a natural bias towards assigning higher scores to machine-generated stories.



\section{Compared Evaluation Metrics Details}
\label{appendix:evaluationmetrics}



\textbf{BARTScore} \citep{DBLP:conf/nips/YuanNL21} evaluates generated text as a text generation task by calculating the conditional likelihood of the generated text under BART. This metric can evaluate various aspects of text quality by using different combinations of input conditions and output. We use the reference-free version of BARTScore in the direction of $\boldsymbol{c} \rightarrow \boldsymbol{s}$.

% Might delete CTC if we lack space.
\textbf{CTC} \citep{deng-etal-2021-compression} evaluates generated text as an information alignment task,
and they propose various approaches for alignment calculations.
In our evaluation, we use the reference-free alignments, which is the ``consistency'' version of CTC.

% We calculate both CTC (Consistency), which is s $\rightarrow$ c and
% CTC (Relevance), which is the product of alignments r $\rightarrow$ s and s $\rightarrow$ c,
% because the other settings in CTC are not applicable to our tasks.
% Our results show that CTC (Consistency) works consistently better than CTC (Relevance) (see Appendix).
% It shows references are not that important in open-ended story evaluation. 
\textbf{UNIEVAL} \citep{DBLP:conf/emnlp/Zhong0YMJLZJH22} evaluates generated text as a question answering task, where different questions are asked for each aspect. The evaluation models are trained on text summarization and dialogue generation tasks and are shown to have zero-shot transfer ability to data-to-text generation task by asking corresponding new questions. 
% In our story evaluation, we ask story quality-related questions for each aspect, which are listed in \autoref{appendix: unieval}.


\textbf{UNION} \citep{guan-huang-2020-union} frames story evaluation as a classification task by constructing negative samples of original stories using heuristic rules, and training a neural discriminator to differentiate them. The trained model is then used for evaluating new stories. While their work focuses on the ROC and WP datasets, we do not fine-tune the UNION model on our own dataset.



\textbf{MANPLTS} \citep{ghazarian-etal-2021-plot} is an extension of UNION that focuses on constructing negative samples that are more similar to machine-generated stories. They do this by manipulating storylines and generating stories based on these manipulated storylines using a story generation model. Like UNION, we use the models provided by authors.

\textbf{StoryER} \citep{DBLP:conf/emnlp/ChenVTMN22} combines the approach of separating original stories from negative samples, as in UNION and MANPLTS, with considering human preference by training the model to differentiate highly-upvoted stories from lowly-upvoted ones.
In addition to the preference score, they train their model to produce ratings and comments on predefined story aspects to provide explanations for the evaluation.
%JHL: what is story comments?
% Answer: They additional generate comments on each aspect as explanations.

We also incorporate conventional similarity-based metrics, including:
\textbf{BLEU} \citep{papineni-etal-2002-bleu}, which measures n-gram overlaps between stories and human references,
\textbf{BERTScore} \citep{DBLP:conf/iclr/ZhangKWWA20}, and
\textbf{MoverScore} \citep{zhao-etal-2019-moverscore}, which measures semantic similarity from embeddings obtained from BERT \citep{devlin-etal-2019-bert}.

\subsection{UNIEVAL and GPTScore}

\paragraph{UNIEVAL}


We ask the following questions for each aspect.
Note that we try to use the narrative/vocabulary as close to the original questions
\citet{DBLP:conf/emnlp/Zhong0YMJLZJH22} use in their efforts as possible.

\begin{itemize}
    \item Fluency: Is this a fluent utterance?
    \item Coherence: Is this a coherent utterance?
    \item Relatedness: Is this claim consistent with the document?
    \item Logicality: Is this utterance consistent with the commonsense?
    \item Interestingness: Is this an interesting utterance?
\end{itemize}

\paragraph{GPTScore}

We ask the following questions for each aspect.
Note that we try to use the narrative/vocabulary as close to the original questions \citep{DBLP:journals/corr/abs-2302-04166} use in their efforts as possible.

\begin{itemize}
    \item Fluency: Generate a fluent story for the given title: \{title\}, and story: \{story\}
    \item Coherence: Generate a coherent story for the given title: \{title\}, and story: \{story\}
    \item Relatedness: Generate a story related to the given title: \{title\}, and story: \{story\}
    \item Logicality: Generate a story that adhere to commonsense for the given title: \{title\}, and story: \{story\}
    \item Interestingness: Generate an interesting story for the given title: \{title\}, and story: \{story\}
\end{itemize}




\section{Correlations}
\label{appendix: correlations}

% \subsection{Issues}
% We find that UNIEVAL seems not able to differeniate too much when we ask different questions on each quality aspect.

% \section{Appendix: Correlation Results}

% We put Spearman and Kendall-Tau correlations in \autoref{table:crowdsource_cnn}.
% Pearson correlations are shown in .

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
%  % & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 25.0 & 11.3 & 6.6 & 15.0 & 0.8 & 9.4 & 16.9 & 11.2 & 15.4 & 23.7 & 22.7 & 0.2 & 25.1 & 3.6 & 18.1  \\ 
%  BER & 4.4 & 1.3 & 26.7 & 4.2 & 3.1 & 29.4 & 34.6 & 37.5 & 28.1 & 30.6 & 48.7 & 45.3 & 70.8 & 51.7 & 48.6 \\
%  Mov & 12.0 & 8.0 & 38.2 & 7.5 & 0.2 & 41.9 & 47.7 & 36.8 & 30.0 & 34.6 & 23.5 & 14.6 & 47.0 & 25.2 & 33.5 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 13.5 & 26.7 & 14.9 & 19.3 & 7.2 & 39.4 & 59.6 & 54.1 & 47.8 & 58.8 & 27.1 & 42.5 & 39.5 & 33.2 & 43.0 \\  
% MAN & 7.0 & 3.1 & 7.5 & 1.6 & 12.7 & 20.5 & 0.4 & 14.1 & 23.7 & 4.7 & 18.6 & 30.4 & 48.5 & 19.4 & 20.1 \\ 
% Sto & 5.4 & 12.0 & 14.3 & 1.0 & 12.6 & 11.6 & 22.1 & 12.4 & 21.5 & 32.5 & 13.0 & 28.6 & 14.4 & 14.7 & 23.7 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 23.5 & 41.3 & 25.3 & 35.7 & 25.8 & 51.3 & 76.6 & 53.6 & 68.0 & \textbf{57.2} & 57.4 & 59.4 & 52.8 & 61.0 & 62.7 \\ 
%  UNI & 43.0 & 41.5 & 13.9 & 34.0 & 26.1 & 61.6 & 56.3 & 49.4 & 51.9 & 51.0 & 47.3 & 61.3 & 55.7 & 71.5 & 52.0 \\
%  \midrule
% \multicolumn{10}{l}{\textbf{Likelihood Based Metrics}}\\
% BART & \\ 
% G-3.5 & 28.8 & 43.2 & 26.6 & 39.9 & 25.9 & 47.9 & 44.8 & 48.8 & 52.4 & 34.5 & 56.8 & 65.9 & 58.3 & 66.0 & \textbf{52.7} \\ 
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  + Ju & \cellcolor{green!25}38.7 & \cellcolor{green!25}48.3 & \cellcolor{green!25}33.7 & \cellcolor{green!25}50.8 & 37.2 & \cellcolor{green!25}43.3 & \cellcolor{green!25}50.3 & \cellcolor{green!25}53.0 & \cellcolor{green!25}52.9 & \cellcolor{green!25}43.2 & \cellcolor{green!25}46.7 & \cellcolor{green!25}63.8 & \cellcolor{green!25}46.7 & \cellcolor{green!25}56.1 & 47.6 \\ 
 
%  + Ty & \cellcolor{green!25}37.7 & \cellcolor{green!25}48.6 & \cellcolor{green!25}35.6 & 45.0 & 32.2 & \cellcolor{green!25}44.7 & \cellcolor{green!25}48.3 & \cellcolor{green!25}53.1 & \cellcolor{green!25}58.3 & \cellcolor{green!25}40.8 & \cellcolor{green!25}57.9 & \cellcolor{green!25}68.5 & \cellcolor{green!25}55.2 & \cellcolor{green!25}67.8 & 50.6 \\ 
 
%  + Ad & 18.0 & 41.1 & 21.1 & 32.5 & 24.1 & 44.4 & 38.4 & 30.9 & 34.9 & 23.7 & 47.1 & 66.3 & 32.4 & 59.2 & 42.6  \\
 
%  + An & 32.2 & \cellcolor{green!25}40.1 & 26.4 & 29.7 & 19.4 & \cellcolor{green!25}72.9 & \cellcolor{green!25}67.7 & \cellcolor{green!25}56.4 & \cellcolor{green!25}71.4 & \cellcolor{green!25}56.9 & \cellcolor{green!25}48.3 & 59.4 & 52.7 & \cellcolor{green!25}64.6 & 40.5 \\
%  + Rp  \\
%  + Rr  \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (change to vinialla BART)}}\\ 
%  + Ju &  \\
%  + Ty &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Spearman correlation ($|\rho|$) between different metrics and human evaluations on three Inhouse datasets.
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the original likehihood metric.
% }
% \label{table:inhouse_spearman}
% \end{table*}
\begin{table*}[t]
\small
\centering
\begin{tabular}{lcccccccccc}
\toprule
\multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
\cmidrule(lr){2-6}\cmidrule(lr){7-11}
  & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} \\
\cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
  \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
 BLEU & 3.4 & 4.4 & 0.8 & 4.6 & 0.4 & 13.4 & 11.4 & 9.6 & 11.2 & 7.2 \\ 
 BERTScore & 3.5 & 5.0 & 14.0 & 5.7 & 7.3 & 31.8 & 26.7 & 24.0 & 28.5 & 24.6 \\
 MoverScore & 3.6 & 6.5 & 15.7 & 8.0 & 11.2 & 16.4 & 17.2 & 20.1 & 20.7 & 23.7  \\
 \midrule
 \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
UNION &  0.7 & 12.7 & 12.8 & 0.8 & 3.9 & 17.4 & 21.9 & 18.1 & 22.9 & 21.8 \\  
MANPLTS & 21.3 & 32.8 & 23.2 & 14.7 & 12.4 & 1.1 & 5.5 & 1.7 & 4.6 & 6.7 \\ 
StoryER & 6.5 & 4.5 & 4.0 & 3.7 & 9.7 & 15.9 & 13.1 & 14.1 & 17.9 & 26.1 \\
 \midrule
 \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
 CTC & 22.9 & 27.3 & 14.3 & 11.1 & 8.3 & 45.9 & 51.6 & 40.3 & 53.1 & 47.5 \\ 
 UNIEVAL & 32.2 & 31.7 & 23.7 & 20.0 & 18.8 & 39.3 & 41.3 & 38.6 & 50.7 & 39.1 \\
 GPTScore & 21.2 & 20.9 & 21.3 & 23.9 & 22.0 & 29.3 & 32.6 & 30.8 & 35.3 & 28.9 \\
 \midrule
  \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
  w/o perturbation & 26.4 & 21.3 & 21.1 & 15.8 & 20.3 & 18.1 & 15.4 & 17.5 & 17.6 & 9.3 \\
 Jumble & \cellcolor{green!25}35.4 & \cellcolor{green!25}36.4 & \cellcolor{green!25}21.6 & \cellcolor{green!25}23.4 & \cellcolor{green!25}\textbf{26.8} & \cellcolor{green!25}52.7 & \cellcolor{green!25}52.5 & \cellcolor{green!25}43.6 & \cellcolor{green!25}56.6 & \cellcolor{green!25}44.7 \\
 Typo & \cellcolor{green!25}31.7 & \cellcolor{green!25}28.2 & \cellcolor{green!25}22.0 & 15.3 & 19.0 & \cellcolor{green!25}48.4 & \cellcolor{green!25}46.3 & \cellcolor{green!25}39.0 & \cellcolor{green!25}52.6 & \cellcolor{green!25}40.2 \\
 % AddNe & 9.9 & 16.8 & 16.0 & 6.3 & 8.5 & \cellcolor{green!25}23.9 & \cellcolor{green!25}21.8 & \cellcolor{green!25}24.9 & \cellcolor{green!25}24.5 & \cellcolor{green!25}15.0 \\
 Antonym & 11.8 & 16.1 & 6.5 & 9.6 & 7.7 & \cellcolor{green!25}49.9 & \cellcolor{green!25}46.3 & \cellcolor{green!25}32.5 & \cellcolor{green!25}50.0 & \cellcolor{green!25}42.7 \\
 % RpSt &  \\
 % CS &  \\
\midrule
  \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
  -- & 32.0 & 26.7 & 25.3 & 22.0 & 23.5 & 20.8 & 24.8 & 22.2 & 25.7 & 17.1 \\
 Jumble & \cellcolor{green!25}\textbf{43.2} & \cellcolor{green!25}\textbf{42.4} & \cellcolor{green!25}27.4 & \cellcolor{green!25}25.6 & \cellcolor{green!25}23.9 & \cellcolor{green!25}56.1 & \cellcolor{green!25}\textbf{55.5} & \cellcolor{green!25}\textbf{45.9} & \cellcolor{green!25}\textbf{60.9} & \cellcolor{green!25}\textbf{48.5} \\
 Typo & \cellcolor{green!25}35.9 & \cellcolor{green!25}32.0 & \cellcolor{green!25}28.6 & 21.8 & 22.1 & \cellcolor{green!25}44.8 & \cellcolor{green!25}45.8 & \cellcolor{green!25}36.8 & \cellcolor{green!25}50.1 & \cellcolor{green!25}36.6 \\
 % AddNe & 22.7 & \cellcolor{green!25}31.7 & \cellcolor{green!25}28.6 & 17.8 & 19.1 & \cellcolor{green!25}21.6 & \cellcolor{green!25}25.3 & \cellcolor{green!25}27.0 & \cellcolor{green!25}28.9 & \cellcolor{green!25}21.9 \\
 Antonym & 16.5 & 21.9 & 8.4 & 11.9 & 9.3 & \cellcolor{green!25}55.2 & \cellcolor{green!25}51.8 & \cellcolor{green!25}36.9 & \cellcolor{green!25}56.0 & \cellcolor{green!25}45.9 \\
 % RpSt &  \\
 % CS &  \\
\midrule
 \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL)}}\\ 
  w/o perturbation & 21.0 & 22.1 & 26.8 & 23.2 & 22.5 & 13.6 & 10.6 & 12.1 & 10.8 & 4.2 \\
 Jumble & \cellcolor{green!25}35.7 & \cellcolor{green!25}28.2 & 19.9 & 19.8 & 22.2 & \cellcolor{green!25}\textbf{59.2} & \cellcolor{green!25}53.7 & \cellcolor{green!25}41.8 & \cellcolor{green!25}57.4 & \cellcolor{green!25}46.9 \\
 Typo & \cellcolor{green!25}24.2 & \cellcolor{green!25}26.6 & \cellcolor{green!25}27.7 & \cellcolor{green!25}24.6 & \cellcolor{green!25}23.0 & \cellcolor{green!25}34.3 & \cellcolor{green!25}30.5 & \cellcolor{green!25}28.1 & \cellcolor{green!25}34.8 & \cellcolor{green!25}23.1 \\
 % AddNe & 15.4 & \cellcolor{green!25}23.2 & 16.6 & 10.4 & 9.9 & \cellcolor{green!25}24.2 & \cellcolor{green!25}27.4 & \cellcolor{green!25}30.7 & \cellcolor{green!25}31.8 & \cellcolor{green!25}22.6 \\
 Antonym & 9.4 & 16.1 & 3.2 & 7.5 & 5.4 & \cellcolor{green!25}52.6 & \cellcolor{green!25}48.0 & \cellcolor{green!25}37.1 & \cellcolor{green!25}53.8 & \cellcolor{green!25}46.3 \\
 % RpSt &  \\
 % CS &  \\
\midrule
\multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
w/o perturbation &  24.9 & 11.6 & 10.9 & 11.3 & 14.6 & 32.2 & 25.6 & 31.0 & 30.2 & 23.0 \\ 
 Jumble & \cellcolor{green!25}29.2 & \cellcolor{green!25}17.7 & 9.9 & \cellcolor{green!25}12.2 & 12.6 & \cellcolor{green!25}49.1 & \cellcolor{green!25}40.1 & \cellcolor{green!25}44.5 & \cellcolor{green!25}46.5 & \cellcolor{green!25}41.5 \\
 Typo & 22.9 & 11.4 & \cellcolor{green!25}13.3 & 8.2 & 11.4 & \cellcolor{green!25}42.8 & \cellcolor{green!25}35.6 & \cellcolor{green!25}39.8 & \cellcolor{green!25}41.9 & \cellcolor{green!25}33.0 \\
 % AddNe & 8.4 & \cellcolor{green!25}16.9 & \cellcolor{green!25}17.2 & \cellcolor{green!25}11.4 & 9.8 & 24.0 & 17.2 & 13.7 & 20.4 & 13.2 \\
 Antonym & 14.3 & \cellcolor{green!25}22.1 & 10.9 & \cellcolor{green!25}21.3 & \cellcolor{green!25}16.8 & \cellcolor{green!25}41.5 & \cellcolor{green!25}39.5 & \cellcolor{green!25}33.9 & \cellcolor{green!25}42.7 & \cellcolor{green!25}36.6 \\
 % RpSt &  \\
 % CS &  \\
\midrule
 \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
 w/o perturbation & 32.0 & 27.6 & 25.1 & 21.1 & 19.7 & 31.6 & 36.0 & 34.7 & 38.5 & 30.6  \\
 Jumble & \cellcolor{green!25}35.9 & \cellcolor{green!25}40.0 & \cellcolor{green!25}\textbf{33.8} & \cellcolor{green!25}\textbf{29.6} & \cellcolor{green!25}23.2 & \cellcolor{green!25}52.7 & \cellcolor{green!25}49.2 & \cellcolor{green!25}36.6 & \cellcolor{green!25}53.9 & \cellcolor{green!25}42.8 \\ 
 Typo & 29.5 & \cellcolor{green!25}28.2 & 22.7 & 15.4 & 11.7 & \cellcolor{green!25}40.7 & \cellcolor{green!25}43.1 & 32.6 & \cellcolor{green!25}47.2 & \cellcolor{green!25}35.4  \\ 
 % AddNe &  21.0 & \cellcolor{green!25}29.5 & \cellcolor{green!25}34.4 & 18.3 & 19.0 & 26.5 & 21.1 & 13.4 & 24.9 & 16.3  \\
 Antonym & 21.4 & 26.7 & 15.1 & \cellcolor{green!25}21.9 & 18.5 & \cellcolor{green!25}48.3 & \cellcolor{green!25}47.3 & \cellcolor{green!25}37.6 & \cellcolor{green!25}51.6 & \cellcolor{green!25}42.1 \\
 % RpSt &  \\
 % CS &  \\
 \bottomrule
\end{tabular}
\caption{Absolute value of Story-level Kendall correlation ($|\tau|$) between different metrics and human evaluations on three CoudSourcing datasets. 
We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
}
\label{table:crowdsource_kendall_all}
\end{table*}



% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int}  \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 1.6 & 3.8 & 3.4 & 8.3 & 1.4 & 12.5 & 13.7 & 12.1 & 17.2 & 13.1 \\ 
%  BER & 3.1 & 6.2 & 18.3 & 3.8 & 6.2 & 47.1 & 42.6 & 35.1 & 42.4 & 36.6 \\
%  Mov & 2.2 & 7.4 & 23.3 & 3.3 & 9.7 & 26.8 & 28.6 & 32.3 & 31.8 & 32.3 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 4.7 & 16.7 & 17.4 & 5.2 & 9.3 & 32.0 & 39.9 & 24.4 & 36.4 & 35.0 \\  
% MAN & 27.7 & 40.2 & 23.7 & 17.7 & 13.4 & 6.1 & 9.3 & 9.1 & 7.0 & 6.7 \\ 
% StoER & 8.9 & 1.1 & 15.5 & 5.4 & 12.1 & 26.7 & 21.2 & 13.5 & 26.8 & 35.0 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 29.3 & 38.3 & 20.3 & 16.1 & 12.0 & 43.7 & 55.2 & 44.1 & 63.9 & 59.6 \\ 
%  UNIE & 34.4 & 35.9 & 30.4 & 26.2 & 21.2 & 42.8 & 43.0 & 52.3 & 55.1 & 45.4 \\
%  \midrule
%    \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
%   -- &  36.4 & 31.7 & 27.7 & 25.3 & 30.3 & 18.7 & 23.8 & 29.9 & 28.6 & 22.8 \\
%  Jumble & \cellcolor{green!25}46.6 & \cellcolor{green!25}50.5 & \cellcolor{green!25}29.1 & \cellcolor{green!25}32.4 & \cellcolor{green!25}36.0 & \cellcolor{green!25}66.4 & \cellcolor{green!25}72.0 & \cellcolor{green!25}64.7 & \cellcolor{green!25}78.5 & \cellcolor{green!25}66.1  \\
%  Typo & \cellcolor{green!25}42.1 & \cellcolor{green!25}39.8 & \cellcolor{green!25}30.5 & 22.5 & 26.7 & \cellcolor{green!25}63.3 & \cellcolor{green!25}69.2 & \cellcolor{green!25}62.0 & \cellcolor{green!25}74.2 & \cellcolor{green!25}60.6 \\
%  AddNe & 9.8 & 18.3 & 17.5 & 1.4 & 5.9 & \cellcolor{green!25}34.9 & \cellcolor{green!25}31.4 & 29.4 & \cellcolor{green!25}35.4 & 19.8 \\
%  Anton & 18.7 & 22.2 & 6.3 & 11.8 & 11.1 & \cellcolor{green!25}66.2 & \cellcolor{green!25}64.5 & \cellcolor{green!25}55.0 & \cellcolor{green!25}70.1 & \cellcolor{green!25}58.0 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%   \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
%    -- & 42.6 & 39.2 & 34.7 & 31.8 & 32.3 & 23.1 & 34.0 & 36.5 & 39.2 & 33.0 \\
%  Jumble & \cellcolor{green!25}55.9 & \cellcolor{green!25}58.1 & \cellcolor{green!25}37.8 & \cellcolor{green!25}34.3 & 31.4 & \cellcolor{green!25}68.8 & \cellcolor{green!25}77.3 & \cellcolor{green!25}72.1 & \cellcolor{green!25}82.9 & \cellcolor{green!25}68.4  \\
%  Typo & \cellcolor{green!25}50.7 & \cellcolor{green!25}47.4 & \cellcolor{green!25}39.4 & 31.7 & 32.3 & \cellcolor{green!25}58.2 & \cellcolor{green!25}69.0 & \cellcolor{green!25}61.8 & \cellcolor{green!25}76.1 & \cellcolor{green!25}63.9  \\
%  AddNe & 30.3 & \cellcolor{green!25}42.3 & 34.4 & 23.1 & 24.1 & \cellcolor{green!25}32.6 & \cellcolor{green!25}35.9 & 34.5 & \cellcolor{green!25}41.1 & 29.2  \\
%  Anton & 25.8 & 30.5 & 8.8 & 15.7 & 12.0 & \cellcolor{green!25}66.8 & \cellcolor{green!25}69.3 & \cellcolor{green!25}60.1 & \cellcolor{green!25}75.8 & \cellcolor{green!25}62.1 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL)}}\\ 
%    -- & 30.0 & 33.3 & 37.2 & 32.4 & 29.3 & 6.9 & 11.3 & 19.4 & 15.7 & 10.7 \\
%  Jumble & \cellcolor{green!25}47.2 & \cellcolor{green!25}40.2 & 25.6 & 24.4 & 26.7 & \cellcolor{green!25}70.2 & \cellcolor{green!25}71.9 & \cellcolor{green!25}65.8 & \cellcolor{green!25}77.7 & \cellcolor{green!25}65.1 \\
%  Typo & \cellcolor{green!25}33.5 & \cellcolor{green!25}39.1 & \cellcolor{green!25}38.3 & \cellcolor{green!25}35.1 & \cellcolor{green!25}30.9 & \cellcolor{green!25}40.3 & \cellcolor{green!25}45.3 & \cellcolor{green!25}45.7 & \cellcolor{green!25}52.3 & \cellcolor{green!25}41.2 \\
%  AddNe & 20.1 & 30.5 & 24.6 & 16.3 & 17.0 & \cellcolor{green!25}38.5 & \cellcolor{green!25}38.5 & \cellcolor{green!25}37.7 & \cellcolor{green!25}42.9 & \cellcolor{green!25}28.0 \\
%  Anton & 16.9 & 24.6 & 4.0 & 11.7 & 8.5 & \cellcolor{green!25}63.5 & \cellcolor{green!25}63.1 & \cellcolor{green!25}58.1 & \cellcolor{green!25}70.9 & \cellcolor{green!25}57.7 \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- & 32.4 & 13.0 & 12.0 & 15.6 & 19.8 & 33.6 & 30.3 & 43.4 & 36.7 & 28.8 \\
%  Jumble & \cellcolor{green!25}35.9 & \cellcolor{green!25}21.5 & 11.2 & 15.4 & 17.0 & \cellcolor{green!25}57.6 & \cellcolor{green!25}55.3 & \cellcolor{green!25}62.9 & \cellcolor{green!25}63.5 & \cellcolor{green!25}56.8 \\
%  Typo & 29.6 & 11.3 & \cellcolor{green!25}12.4 & 9.4 & 15.3 & \cellcolor{green!25}47.4 & \cellcolor{green!25}44.5 & \cellcolor{green!25}55.7 & \cellcolor{green!25}52.5 & \cellcolor{green!25}41.8 \\
%  AddNe & 6.4 & \cellcolor{green!25}19.5 & \cellcolor{green!25}18.5 & 11.5 & 11.6 & 32.9 & 27.4 & 24.5 & 32.4 & 19.4 \\
%  Anton & 18.0 & \cellcolor{green!25}32.2 & \cellcolor{green!25}17.2 & \cellcolor{green!25}31.0 & \cellcolor{green!25}26.7 & \cellcolor{green!25}51.1 & \cellcolor{green!25}50.0 & \cellcolor{green!25}54.0 & \cellcolor{green!25}51.6 & \cellcolor{green!25}33.1 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 43.7 & 43.5 & 37.3 & 32.1 & 30.0 & 37.2 & 50.8 & 52.3 & 58.1 & 50.8 \\
%  Jumble & \cellcolor{green!25}47.4 & \cellcolor{green!25}54.3 & \cellcolor{green!25}45.8 & \cellcolor{green!25}40.7 & \cellcolor{green!25}30.7 & \cellcolor{green!25}66.4 & \cellcolor{green!25}74.6 & \cellcolor{green!25}66.3 & \cellcolor{green!25}80.8 & \cellcolor{green!25}67.7 \\ 
%  Typo & 41.7 & 41.0 & 29.7 & 23.5 & 18.8 & \cellcolor{green!25}53.1 & \cellcolor{green!25}65.9 & \cellcolor{green!25}57.3 & \cellcolor{green!25}70.5 & \cellcolor{green!25}57.5 \\ 
%  AddNe & 29.3 & 41.9 & \cellcolor{green!25}44.3 & 26.8 & 26.5 & 33.3 & 32.4 & 27.2 & 43.0 & 31.6 \\
%  Anton & 32.1 & 38.7 & 19.4 & 29.5 & 23.2 & \cellcolor{green!25}60.0 & \cellcolor{green!25}64.7 & \cellcolor{green!25}61.7 & \cellcolor{green!25}71.6 & \cellcolor{green!25}55.7 \\
%  % RpSt &  \\
%  % CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Pearson correlation ($|r|$) between different metrics and human evaluations on three CoudSourcing datasets. 
% % We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:crowdsource_pearson}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int}  \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%   BLE & 4.8 & 5.8 & 2.0 & 7.0 & 0.8 & 18.0 & 17.1 & 16.2 & 17.6 & 9.7 \\
%   BER & 5.5 & 7.3 & 19.7 & 7.8 & 9.9 & 42.8 & 37.2 & 33.1 & 38.9 & 34.0 \\
%   Mov & 4.9 & 9.1 & 22.6 & 11.0 & 15.2 & 22.1 & 24.7 & 28.8 & 28.8 & 32.9 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 0.6 & 17.6 & 17.6 & 1.4 & 6.1 & 24.7 & 31.1 & 26.8 & 32.3 & 33.6 \\  
% MAN & 30.8 & 45.5 & 31.8 & 21.5 & 17.7 & 1.4 & 7.2 & 2.2 & 4.1 & 10.3 \\ 
% StoER & 9.7 & 6.1 & 6.0 & 5.8 & 14.3 & 21.3 & 19.4 & 19.9 & 24.9 & 37.4 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 31.7 & 37.8 & 21.7 & 17.0 & 12.2 & 59.2 & 67.9 & 55.0 & 70.3 & 62.6  \\ 
%  UNIE & 44.3 & 45.2 & 33.3 & 29.0 & 26.6 & 52.6 & 56.8 & 52.7 & 67.4 & 52.7 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
%   -- & 35.4 & 30.1 & 29.9 & 22.4 & 27.8 & 22.3 & 19.1 & 23.7 & 22.4 & 12.3  \\
%  Jumble & \cellcolor{green!25}48.4 & \cellcolor{green!25}51.2 & \cellcolor{green!25}31.2 & \cellcolor{green!25}33.2 & \cellcolor{green!25}36.4 & \cellcolor{green!25}70.1 & \cellcolor{green!25}71.0 & \cellcolor{green!25}58.6 & \cellcolor{green!25}73.6 & \cellcolor{green!25}59.3 \\
%  Typo & \cellcolor{green!25}43.2 & \cellcolor{green!25}38.6 & 29.6 & 20.5 & 24.6 & \cellcolor{green!25}64.5 & \cellcolor{green!25}64.1 & \cellcolor{green!25}52.5 & \cellcolor{green!25}69.6 & \cellcolor{green!25}54.0 \\
%  AddNe & 14.0 & 24.3 & 22.2 & 8.6 & 11.4 & \cellcolor{green!25}33.7 & \cellcolor{green!25}32.6 & \cellcolor{green!25}34.8 & \cellcolor{green!25}36.2 & \cellcolor{green!25}21.1 \\
%  Anton & 17.0 & 22.9 & 9.9 & 13.3 & 11.8 & \cellcolor{green!25}65.4 & \cellcolor{green!25}62.9 & \cellcolor{green!25}46.6 & \cellcolor{green!25}65.6 & \cellcolor{green!25}56.0 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%   \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
%    -- & 42.9 & 37.2 & 36.2 & 31.1 & 32.4 & 27.0 & 30.6 & 31.9 & 32.6 & 22.4  \\
%  Jumble & \cellcolor{green!25}57.8 & \cellcolor{green!25}58.9 & \cellcolor{green!25}39.9 & \cellcolor{green!25}35.8 & 32.2 & \cellcolor{green!25}73.3 & \cellcolor{green!25}73.3 & \cellcolor{green!25}60.8 & \cellcolor{green!25}77.2 & \cellcolor{green!25}63.1 \\
%  Typo & \cellcolor{green!25}48.7 & \cellcolor{green!25}44.3 & \cellcolor{green!25}39.8 & 30.5 & 31.0 & \cellcolor{green!25}61.0 & \cellcolor{green!25}62.0 & \cellcolor{green!25}50.6 & \cellcolor{green!25}66.2 & \cellcolor{green!25}50.2 \\
%  AddNe & 32.8 & \cellcolor{green!25}46.4 & \cellcolor{green!25}39.8 & 25.4 & 25.9 & \cellcolor{green!25}30.1 & \cellcolor{green!25}34.5 & \cellcolor{green!25}36.9 & \cellcolor{green!25}39.6 & \cellcolor{green!25}29.5 \\
%  Anton & 23.7 & 31.1 & 12.2 & 17.4 & 13.9 & \cellcolor{green!25}69.9 & \cellcolor{green!25}68.7 & \cellcolor{green!25}51.5 & \cellcolor{green!25}72.5 & \cellcolor{green!25}59.4 \\
% \midrule
 
%  \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL)}}\\ 
%    -- & 28.9 & 30.7 & 37.1 & 32.1 & 30.3 & 16.7 & 13.5 & 16.8 & 14.4 & 4.7 \\
%  Jumble & \cellcolor{green!25}50.5 & \cellcolor{green!25}41.5 & 29.5 & 28.4 & \cellcolor{green!25}31.2 & \cellcolor{green!25}78.3 & \cellcolor{green!25}72.9 & \cellcolor{green!25}58.2 & \cellcolor{green!25}76.2 & \cellcolor{green!25}61.9 \\
%  Typo & \cellcolor{green!25}34.8 & \cellcolor{green!25}32.7 & 34.2 & 24.4 & 22.6 & \cellcolor{green!25}41.4 & \cellcolor{green!25}38.9 & \cellcolor{green!25}36.8 & \cellcolor{green!25}42.6 & \cellcolor{green!25}26.4 \\
%  AddNe & 22.9 & \cellcolor{green!25}33.6 & 24.2 & 15.0 & 14.4 & \cellcolor{green!25}34.1 & \cellcolor{green!25}39.4 & \cellcolor{green!25}41.8 & \cellcolor{green!25}44.7 & \cellcolor{green!25}31.8  \\
%  Anton & 13.4 & 22.5 & 4.6 & 10.5 & 7.7 & \cellcolor{green!25}68.0 & \cellcolor{green!25}64.4 & \cellcolor{green!25}52.0 & \cellcolor{green!25}70.1 & \cellcolor{green!25}59.9  \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- & 34.4 & 15.8 & 15.4 & 15.7 & 19.8 & 41.6 & 34.1 & 42.7 & 41.1 & 29.2 \\
%  Jumble & \cellcolor{green!25}40.6 & \cellcolor{green!25}25.7 & 14.4 & \cellcolor{green!25}17.6 & 17.8 & \cellcolor{green!25}64.2 & \cellcolor{green!25}56.2 & \cellcolor{green!25}59.5 & \cellcolor{green!25}62.4 & \cellcolor{green!25}53.6 \\
%  Typo & 31.9 & 14.9 & \cellcolor{green!25}18.0 & 11.1 & 16.0 & \cellcolor{green!25}55.2 & \cellcolor{green!25}47.9 & \cellcolor{green!25}54.3 & \cellcolor{green!25}56.4 & \cellcolor{green!25}43.3  \\
%  AddNe & 11.8 & \cellcolor{green!25}23.6 & \cellcolor{green!25}25.1 & \cellcolor{green!25}16.4 & 13.9 & 33.7 & 23.7 & 19.5 & 28.5 & 16.7 \\
%  Anton & 19.8 & \cellcolor{green!25}31.0 & \cellcolor{green!25}16.6 & \cellcolor{green!25}29.8 & \cellcolor{green!25}23.4 & \cellcolor{green!25}55.7 & \cellcolor{green!25}54.4 & \cellcolor{green!25}48.3 & \cellcolor{green!25}59.5 & \cellcolor{green!25}49.3  \\
%  % RpSt & 1.6 & \cellcolor{green!25}16.3 & 11.2 & \cellcolor{green!25}23.1 & 14.4 &  \\
%  % CS & 0.6 & 10.4 & 9.5 & \cellcolor{green!25}20.6 & 14.1 &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 42.7 & 39.5 & 35.7 & 30.0 & 27.6 & 43.7 & 48.2 & 47.6 & 51.8 & 42.2  \\
%  Jumble & \cellcolor{green!25}49.3 & \cellcolor{green!25}55.3 & \cellcolor{green!25}48.5 & \cellcolor{green!25}41.0 & \cellcolor{green!25}32.9 & \cellcolor{green!25}70.2 & \cellcolor{green!25}66.6 & \cellcolor{green!25}51.9 & \cellcolor{green!25}70.5 & \cellcolor{green!25}57.9 \\ 
%  Typo & 41.0 & \cellcolor{green!25}40.4 & 33.5 & 23.0 & 16.8 & \cellcolor{green!25}57.0 & \cellcolor{green!25}60.0 & 44.8 & \cellcolor{green!25}63.0 & \cellcolor{green!25}48.7 \\ 
%  AddNe & 30.2 & \cellcolor{green!25}42.7 & \cellcolor{green!25}48.7 & 25.5 & 25.8 & 37.0 & 32.2 & 21.8 & 37.0 & 24.0 \\
%  Anton & 30.2 & 38.8 & 22.4 & \cellcolor{green!25}30.7 & 26.5 & \cellcolor{green!25}64.0 & \cellcolor{green!25}64.0 & \cellcolor{green!25}50.5 & \cellcolor{green!25}68.3 & \cellcolor{green!25}55.3  \\
%  % RpSt & 0.8 & 2.7 & 8.8 & 15.1 & 11.2 &  \\
%  % CS & 15.6 & 15.3 & 19.3 & 1.0 & 0.4 &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Spearman correlation ($|\rho|$) between different metrics and human evaluations on three CoudSourcing datasets. 
% % We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:crowdsource_spearman}
% \end{table*}



% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int}  \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 1.6 & 3.8 & 3.4 & 8.3 & 1.4 & 12.5 & 13.7 & 12.1 & 17.2 & 13.1 \\ 
%  BER & 3.1 & 6.2 & 18.3 & 3.8 & 6.2 & 47.1 & 42.6 & 35.1 & 42.4 & 36.6 \\
%  Mov & 2.2 & 7.4 & 23.3 & 3.3 & 9.7 & 26.8 & 28.6 & 32.3 & 31.8 & 32.3 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 4.8 & 12.6 & 11.6 & 19.5 & 13.6 & 14.7 & 20.2 & 4.4 & 12.1 & 27.5 \\  
% MAN & 9.3 & 16.2 & 11.3 & 15.4 & 5.0 & 24.0 & 12.5 & 19.3 & 33.0 & 8.7 \\ 
% StoER & 3.2 & 8.8 & 8.0 & 1.8 & 15.2 & 13.9 & 22.7 & 6.6 & 17.3 & 34.2 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 23.9 & 42.3 & 20.0 & 35.9 & 28.9 & 53.0 & 77.7 & 59.2 & 70.4 & 59.9 \\ 
%  UNIE & 50.7 & 30.3 & 9.3 & 31.8 & 16.5 & 48.6 & 37.9 & 39.5 & 34.5 & 30.2 \\
%  \midrule
%    \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
%   -- & 22.0 & 31.2 & 14.1 & 32.4 & 12.5 & 30.9 & 32.0 & 42.5 & 42.8 & 17.1 \\
%  Jumble & \cellcolor{green!25}34.0 & \cellcolor{green!25}45.8 & \cellcolor{green!25}17.0 & \cellcolor{green!25}44.5 & \cellcolor{green!25}37.3 & \cellcolor{green!25}45.1 & \cellcolor{green!25}50.7 & \cellcolor{green!25}52.8 & \cellcolor{green!25}51.8 & \cellcolor{green!25}38.0 \\
%  Typo & \cellcolor{green!25}32.9 & \cellcolor{green!25}44.8 & \cellcolor{green!25}16.7 & \cellcolor{green!25}40.3 & \cellcolor{green!25}25.8 & \cellcolor{green!25}54.6 & \cellcolor{green!25}63.2 & \cellcolor{green!25}57.5 & \cellcolor{green!25}64.6 & \cellcolor{green!25}42.8 \\
%  AddNe & 7.0 & 24.5 & 2.9 & 13.5 & \cellcolor{green!25}14.4 & 18.6 & \cellcolor{green!25}33.9 & 27.2 & 32.0 & 5.9 \\
%  Anton & 11.2 & 13.9 & 3.4 & 9.0 & 10.4 & \cellcolor{green!25}41.8 & \cellcolor{green!25}46.7 & \cellcolor{green!25}49.0 & \cellcolor{green!25}48.8 & \cellcolor{green!25}41.5 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%   \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
%    -- & 26.8 & 41.2 & 23.3 & 44.6 & 24.0 & 34.9 & 36.1 & 45.9 & 46.1 & 23.0 \\
%  Jumble & \cellcolor{green!25}33.5 & \cellcolor{green!25}59.7 & \cellcolor{green!25}29.6 & \cellcolor{green!25}57.5 & \cellcolor{green!25}48.4 & \cellcolor{green!25}55.2 & \cellcolor{green!25}61.5 & \cellcolor{green!25}59.4 & \cellcolor{green!25}63.5 & \cellcolor{green!25}54.4 \\
%  Typo & \cellcolor{green!25}30.8 & \cellcolor{green!25}54.1 & 22.4 & \cellcolor{green!25}52.2 & \cellcolor{green!25}36.1 & \cellcolor{green!25}58.6 & \cellcolor{green!25}66.4 & \cellcolor{green!25}60.7 & \cellcolor{green!25}68.8 & \cellcolor{green!25}52.5  \\
%  AddNe & 19.6 & \cellcolor{green!25}46.2 & 13.6 & 28.7 & \cellcolor{green!25}26.0 & 28.5 & \cellcolor{green!25}42.6 & 36.2 & 40.3 & 12.3 \\
%  Anton & 21.9 & 27.4 & 15.0 & 22.6 & 21.4 & \cellcolor{green!25}44.2 & \cellcolor{green!25}54.0 & \cellcolor{green!25}54.8 & \cellcolor{green!25}54.3 & \cellcolor{green!25}48.3 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL)}}\\ 
%    -- & 25.6 & 26.1 & 15.2 & 29.5 & 9.4 & 24.6 & 22.5 & 36.4 & 31.9 & 6.8 \\
%  Jumble & \cellcolor{green!25}36.3 & \cellcolor{green!25}49.2 & \cellcolor{green!25}28.8 & \cellcolor{green!25}40.3 & \cellcolor{green!25}30.1 & \cellcolor{green!25}42.8 & \cellcolor{green!25}41.4 & \cellcolor{green!25}46.5 & \cellcolor{green!25}50.6 & \cellcolor{green!25}38.8  \\
%  Typo & 21.4 & \cellcolor{green!25}31.7 & 14.1 & \cellcolor{green!25}35.6 & \cellcolor{green!25}20.2 & \cellcolor{green!25}37.4 & \cellcolor{green!25}44.2 & \cellcolor{green!25}49.8 & \cellcolor{green!25}47.5 & \cellcolor{green!25}22.5 \\
%  AddNe & \cellcolor{green!25}30.1 & \cellcolor{green!25}37.8 & \cellcolor{green!25}24.2 & 27.5 & \cellcolor{green!25}31.8 & 10.0 & \cellcolor{green!25}29.8 & 30.1 & 28.7 & 0.8 \\
%  Anton & 17.4 & 20.1 & 9.3 & 14.4 & \cellcolor{green!25}12.8 & \cellcolor{green!25}42.6 & \cellcolor{green!25}42.5 & \cellcolor{green!25}47.5 & \cellcolor{green!25}43.8 & \cellcolor{green!25}36.5 \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- & 11.4 & 20.0 & 15.4 & 20.5 & 6.9 & 14.2 & 15.8 & 36.7 & 30.1 & 2.8 \\
%  Jumble & \cellcolor{green!25}15.0 & \cellcolor{green!25}33.6 & \cellcolor{green!25}21.8 & \cellcolor{green!25}31.5 & \cellcolor{green!25}18.8 & \cellcolor{green!25}38.4 & \cellcolor{green!25}36.9 & \cellcolor{green!25}48.4 & \cellcolor{green!25}46.4 & \cellcolor{green!25}26.8 \\
%  Typo & \cellcolor{green!25}19.6 & \cellcolor{green!25}39.5 & \cellcolor{green!25}28.2 & \cellcolor{green!25}32.5 & \cellcolor{green!25}23.8 & \cellcolor{green!25}32.7 & \cellcolor{green!25}42.9 & \cellcolor{green!25}51.6 & \cellcolor{green!25}49.8 & \cellcolor{green!25}28.4 \\
%  AddNe & 10.9 & \cellcolor{green!25}27.9 & \cellcolor{green!25}16.9 & 14.3 & \cellcolor{green!25}21.3 & 13.2 & \cellcolor{green!25}27.6 & 30.1 & 25.0 & \cellcolor{green!25}3.1 \\
%  Anton & \cellcolor{green!25}12.6 & 12.0 & 11.3 & 6.1 & \cellcolor{green!25}10.9 & \cellcolor{green!25}42.1 & \cellcolor{green!25}44.9 & \cellcolor{green!25}54.8 & \cellcolor{green!25}46.6 & \cellcolor{green!25}39.0 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 31.6 & 46.2 & 24.2 & 41.6 & 29.7 &  47.2 & 49.1 & 54.8 & 57.5 & 39.5 \\
%  Jumble & 28.9 & \cellcolor{green!25}50.3 & \cellcolor{green!25}26.4 & \cellcolor{green!25}53.5 & \cellcolor{green!25}43.5 & 45.6 & \cellcolor{green!25}58.7 & \cellcolor{green!25}62.8 & \cellcolor{green!25}60.4 & \cellcolor{green!25}54.2  \\ 
%  Typo & 30.5 & 46.2 & 21.3 & \cellcolor{green!25}46.9 & \cellcolor{green!25}38.5 & \cellcolor{green!25}57.6 & \cellcolor{green!25}68.8 & \cellcolor{green!25}58.3 & \cellcolor{green!25}68.6 & \cellcolor{green!25}61.2 \\ 
%  AddNe &  20.9 & 43.7 & 20.7 & 33.5 & 29.0 & 35.8 & 44.6 & 34.0 & 36.3 & 23.1 \\
%  Anton &  23.0 & 32.1 & 14.3 & 29.8 & 25.3 & \cellcolor{green!25}47.7 & \cellcolor{green!25}64.0 & \cellcolor{green!25}64.4 & \cellcolor{green!25}59.9 & \cellcolor{green!25}52.3 \\
%  % RpSt &  \\
%  % CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Pearson correlation ($|r|$) between different metrics and human evaluations on three Inhouse datasets. 
% % We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:inhouse_pearson}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int}  \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 1.6 & 3.8 & 3.4 & 8.3 & 1.4 & 12.5 & 13.7 & 12.1 & 17.2 & 13.1 \\ 
%  BER & 3.1 & 6.2 & 18.3 & 3.8 & 6.2 & 47.1 & 42.6 & 35.1 & 42.4 & 36.6 \\
%  Mov & 2.2 & 7.4 & 23.3 & 3.3 & 9.7 & 26.8 & 28.6 & 32.3 & 31.8 & 32.3 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 11.2 & 5.2 & 17.5 & 13.9 & 9.3 & 17.8 & 27.7 & 11.0 & 18.5 & 36.1 \\  
% MAN & 13.5 & 26.7 & 14.9 & 19.3 & 7.2 & 20.5 & 0.4 & 14.1 & 23.7 & 4.7 \\ 
% StoER & 5.4 & 11.9 & 14.3 & 1.0 & 12.6 & 11.7 & 22.1 & 12.5 & 21.4 & 32.5 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 23.5 & 41.3 & 25.3 & 35.7 & 25.8 & 51.3 & 76.6 & 53.6 & 68.0 & 57.2 \\ 
%  UNIE & 43.0 & 41.5 & 13.9 & 34.0 & 26.1 & 61.6 & 56.3 & 49.4 & 51.9 & 51.0 \\
%  \midrule
%    \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
%   -- & 17.2 & 33.1 & 18.4 & 36.4 & 14.7 & 31.1 & 26.9 & 36.7 & 40.3 & 15.4 \\
%  Jumble & \cellcolor{green!25}32.9 & \cellcolor{green!25}45.2 & \cellcolor{green!25}20.3 & \cellcolor{green!25}44.1 & \cellcolor{green!25}32.6 & \cellcolor{green!25}45.5 & \cellcolor{green!25}46.2 & \cellcolor{green!25}44.1 & \cellcolor{green!25}44.9 & \cellcolor{green!25}31.1 \\
%  Typo & \cellcolor{green!25}29.8 & \cellcolor{green!25}41.6 & 17.9 & \cellcolor{green!25}37.9 & \cellcolor{green!25}19.3 & \cellcolor{green!25}55.5 & \cellcolor{green!25}57.7 & \cellcolor{green!25}49.7 & \cellcolor{green!25}61.5 & \cellcolor{green!25}41.3 \\
%  AddNe & 5.9 & 24.7 & 5.4 & 14.0 & 11.0 & 22.0 & \cellcolor{green!25}35.8 & 24.4 & 37.5 & \cellcolor{green!25}20.3 \\
%  Anton & 7.8 & 13.4 & 5.4 & 8.2 & 5.8 & \cellcolor{green!25}41.9 & \cellcolor{green!25}46.0 & \cellcolor{green!25}44.1 & \cellcolor{green!25}47.1 & \cellcolor{green!25}43.7 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%   \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
%    -- & 23.7 & 41.9 & 27.9 & 45.6 & 22.8 & 35.1 & 32.0 & 41.5 & 42.4 & 19.9 \\
%  Jumble & \cellcolor{green!25}32.8 & \cellcolor{green!25}57.9 & \cellcolor{green!25}31.4 & \cellcolor{green!25}57.0 & \cellcolor{green!25}44.3 & \cellcolor{green!25}53.8 & \cellcolor{green!25}52.4 & \cellcolor{green!25}48.0 & \cellcolor{green!25}55.7 & \cellcolor{green!25}43.7 \\
%  Typo & \cellcolor{green!25}27.5 & \cellcolor{green!25}52.5 & 23.8 & \cellcolor{green!25}52.2 & \cellcolor{green!25}31.9 & \cellcolor{green!25}56.7 & \cellcolor{green!25}57.3 & \cellcolor{green!25}51.2 & \cellcolor{green!25}62.4 & \cellcolor{green!25}45.4  \\
%  AddNe & 22.1 & \cellcolor{green!25}45.9 & 19.6 & 29.7 & \cellcolor{green!25}23.7 & \cellcolor{green!25}39.2 & \cellcolor{green!25}42.6 & 35.7 & \cellcolor{green!25}44.1 & \cellcolor{green!25}22.6 \\
%  Anton & 20.4 & 27.2 & 17.0 & 22.0 & 15.5 & \cellcolor{green!25}47.8 & \cellcolor{green!25}55.5 & \cellcolor{green!25}49.6 & \cellcolor{green!25}55.1 & \cellcolor{green!25}51.2 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL)}}\\ 
%    -- & 21.6 & 27.6 & 21.0 & 33.0 & 12.3 & 25.3 & 16.8 & 29.8 & 28.0 & 4.6 \\
%  Jumble & \cellcolor{green!25}33.2 & \cellcolor{green!25}47.9 & \cellcolor{green!25}31.2 & \cellcolor{green!25}38.7 & \cellcolor{green!25}27.4 & \cellcolor{green!25}43.1 & \cellcolor{green!25}38.9 & \cellcolor{green!25}33.6 & \cellcolor{green!25}49.5 & \cellcolor{green!25}33.5  \\
%  Typo & 19.2 & \cellcolor{green!25}32.7 & 14.6 & \cellcolor{green!25}37.4 & \cellcolor{green!25}22.7 & \cellcolor{green!25}41.9 & \cellcolor{green!25}39.3 & \cellcolor{green!25}43.7 & \cellcolor{green!25}46.6 & \cellcolor{green!25}26.8 \\
%  AddNe & \cellcolor{green!25}28.5 & \cellcolor{green!25}33.9 & \cellcolor{green!25}24.2 & 25.6 & \cellcolor{green!25}28.5 & 19.6 & \cellcolor{green!25}33.0 & 28.1 & \cellcolor{green!25}34.5 & \cellcolor{green!25}13.5 \\
%  Anton & 15.5 & 20.2 & 11.0 & 13.8 & 9.7 & \cellcolor{green!25}47.8 & \cellcolor{green!25}47.1 & \cellcolor{green!25}42.3 & \cellcolor{green!25}47.1 & \cellcolor{green!25}39.3 \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- & 6.2 & 19.6 & 16.7 & 21.0 & 3.7 & 20.6 & 18.8 & 30.0 & 34.8 & 12.5 \\
%  Jumble & \cellcolor{green!25}12.3 & \cellcolor{green!25}31.9 & \cellcolor{green!25}21.3 & \cellcolor{green!25}30.6 & \cellcolor{green!25}16.3 & \cellcolor{green!25}42.9 & \cellcolor{green!25}32.4 & \cellcolor{green!25}37.2 & \cellcolor{green!25}47.5 & \cellcolor{green!25}30.4 \\
%  Typo & \cellcolor{green!25}11.4 & \cellcolor{green!25}28.1 & \cellcolor{green!25}24.5 & \cellcolor{green!25}26.1 & \cellcolor{green!25}11.0 & \cellcolor{green!25}35.5 & \cellcolor{green!25}37.5 & \cellcolor{green!25}40.6 & \cellcolor{green!25}51.6 & \cellcolor{green!25}32.6 \\
%  AddNe & \cellcolor{green!25}10.0 & \cellcolor{green!25}24.1 & \cellcolor{green!25}20.4 & 12.1 & \cellcolor{green!25}20.9 & \cellcolor{green!25}21.7 & \cellcolor{green!25}21.2 & 20.5 & 25.9 & 3.5 \\
%  Anton & 9.1 & 12.7 & 10.2 & 8.4 & \cellcolor{green!25}9.9 & \cellcolor{green!25}40.7 & \cellcolor{green!25}38.4 & \cellcolor{green!25}49.4 & \cellcolor{green!25}44.5 & \cellcolor{green!25}35.4 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 28.8 & 43.1 & 26.8 & 39.7 & 25.9 & 47.9 & 44.7 & 48.6 & 52.3 & 34.5 \\
%  Jumble & 26.8 & \cellcolor{green!25}48.2 & 26.7 & \cellcolor{green!25}51.9 & \cellcolor{green!25}40.8 & 44.3 & \cellcolor{green!25}51.9 & \cellcolor{green!25}53.2 & \cellcolor{green!25}52.6 & \cellcolor{green!25}43.1  \\ 
%  Typo & 24.2 & 40.8 & 21.8 & \cellcolor{green!25}46.2 & \cellcolor{green!25}31.8 & \cellcolor{green!25}56.9 & \cellcolor{green!25}62.7 & \cellcolor{green!25}51.3 & \cellcolor{green!25}63.3 & \cellcolor{green!25}53.7 \\ 
%  AddNe & 17.5 & 40.3 & 20.8 & 31.5 & 23.1 & 40.3 & 38.4 & 28.9 & 33.0 & 22.3 \\
%  Anton & 24.9 & 29.6 & 18.7 & 26.7 & 17.6 & \cellcolor{green!25}50.1 & \cellcolor{green!25}60.3 & \cellcolor{green!25}56.9 & \cellcolor{green!25}55.2 & \cellcolor{green!25}55.0 \\
%  % RpSt &  \\
%  % CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Spearman correlation ($|\rho|$) between different metrics and human evaluations on three Inhouse datasets. 
% % We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:inhouse_spearman}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int}  \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 1.6 & 3.8 & 3.4 & 8.3 & 1.4 & 12.5 & 13.7 & 12.1 & 17.2 & 13.1 \\ 
%  BER & 3.1 & 6.2 & 18.3 & 3.8 & 6.2 & 47.1 & 42.6 & 35.1 & 42.4 & 36.6 \\
%  Mov & 2.2 & 7.4 & 23.3 & 3.3 & 9.7 & 26.8 & 28.6 & 32.3 & 31.8 & 32.3 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 8.9 & 3.7 & 11.7 & 9.7 & 6.1 & 13.4 & 20.3 & 7.7 & 12.7 & 25.9 \\  
% MAN & 10.5 & 19.4 & 10.2 & 13.6 & 5.4 & 14.7 & 0.2 & 10.5 & 16.7 & 2.2 \\ 
% StoER & 4.1 & 8.4 & 10.0 & 0.8 & 8.2 & 8.3 & 16.9 & 9.0 & 15.0 & 22.5 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 17.5 & 30.3 & 17.9 & 25.3 & 18.1 & 37.8 & 60.4 & 39.2 & 50.0 & 42.2 \\ 
%  UNIE & 31.8 & 30.7 & 9.9 & 24.5 & 18.6 & 45.7 & 40.5 & 37.1 & 36.2 & 36.7 \\
%  \midrule
%    \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
%   -- & 12.4 & 24.1 & 13.3 & 25.5 & 10.2 & 24.8 & 21.3 & 27.9 & 29.2 & 12.6 \\
%  Jumble & \cellcolor{green!25}24.7 & \cellcolor{green!25}33.5 & \cellcolor{green!25}13.8 & \cellcolor{green!25}32.5 & \cellcolor{green!25}23.2 & \cellcolor{green!25}32.9 & \cellcolor{green!25}33.7 & \cellcolor{green!25}31.5 & \cellcolor{green!25}30.7 & \cellcolor{green!25}21.6 \\
%  Typo & \cellcolor{green!25}22.1 & \cellcolor{green!25}30.4 & 12.6 & \cellcolor{green!25}27.3 & \cellcolor{green!25}12.8 & \cellcolor{green!25}40.7 & \cellcolor{green!25}42.0 & \cellcolor{green!25}35.2 & \cellcolor{green!25}44.4 & \cellcolor{green!25}28.6 \\
%  AddNe & 4.6 & 17.7 & 4.0 & 10.5 & 8.0 & 15.6 & \cellcolor{green!25}25.2 & 17.0 & 26.6 & \cellcolor{green!25}14.4 \\
%  Anton & 4.9 & 9.3 & 4.2 & 5.5 & 4.2 & \cellcolor{green!25}29.4 & \cellcolor{green!25}33.5 & \cellcolor{green!25}31.0 & \cellcolor{green!25}33.6 & \cellcolor{green!25}31.2 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%   \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
%    -- & 17.4 & 30.6 & 20.2 & 32.6 & 15.8 & 27.5 & 24.5 & 32.2 & 31.0 & 15.7 \\
%  Jumble & \cellcolor{green!25}24.2 & \cellcolor{green!25}42.9 & \cellcolor{green!25}21.8 & \cellcolor{green!25}41.2 & \cellcolor{green!25}31.8 & \cellcolor{green!25}38.8 & \cellcolor{green!25}38.5 & \cellcolor{green!25}33.8 & \cellcolor{green!25}39.3 & \cellcolor{green!25}30.7 \\
%  Typo & \cellcolor{green!25}20.4 & \cellcolor{green!25}39.1 & 16.4 & \cellcolor{green!25}37.7 & \cellcolor{green!25}22.3 & \cellcolor{green!25}41.1 & \cellcolor{green!25}41.7 & \cellcolor{green!25}37.0 & \cellcolor{green!25}45.4 & \cellcolor{green!25}31.2 \\
%  AddNe & 15.8 & \cellcolor{green!25}33.1 & 13.0 & 21.2 & \cellcolor{green!25}17.3 & 27.0 & \cellcolor{green!25}30.3 & 25.4 & 30.1 & 15.3 \\
%  Anton & 13.9 & 18.7 & 11.7 & 15.1 & 10.6 & \cellcolor{green!25}33.7 & \cellcolor{green!25}41.1 & \cellcolor{green!25}36.0 & \cellcolor{green!25}40.3 & \cellcolor{green!25}37.3 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL)}}\\ 
%    -- & 16.2 & 19.4 & 14.7 & 23.1 & 8.9 & 19.9 & 14.1 & 23.4 & 20.8 & 4.8 \\
%  Jumble & \cellcolor{green!25}24.5 & \cellcolor{green!25}36.0 & \cellcolor{green!25}22.4 & \cellcolor{green!25}27.2 & \cellcolor{green!25}19.0 & \cellcolor{green!25}31.2 & \cellcolor{green!25}27.4 & 22.5 & \cellcolor{green!25}35.0 & \cellcolor{green!25}23.1 \\
%  Typo & 14.2 & \cellcolor{green!25}24.1 & 10.6 & \cellcolor{green!25}26.6 & \cellcolor{green!25}16.3 & \cellcolor{green!25}31.4 & \cellcolor{green!25}28.8 & \cellcolor{green!25}31.5 & \cellcolor{green!25}33.0 & \cellcolor{green!25}19.0 \\
%  AddNe & \cellcolor{green!25}21.0 & \cellcolor{green!25}24.5 & \cellcolor{green!25}17.1 & 18.3 & \cellcolor{green!25}20.1 & 13.6 & \cellcolor{green!25}24.3 & 20.2 & \cellcolor{green!25}24.4 & \cellcolor{green!25}9.8 \\
%  Anton & 10.7 & 14.3 & 8.3 & 9.9 & 6.6 & \cellcolor{green!25}34.3 & \cellcolor{green!25}33.8 & \cellcolor{green!25}29.8 & \cellcolor{green!25}34.0 & \cellcolor{green!25}27.3 \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- & 4.5 & 14.6 & 11.6 & 14.5 & 2.3 & 15.0 & 14.5 & 22.4 & 24.5 & 10.6 \\
%  Jumble & \cellcolor{green!25}8.8 & \cellcolor{green!25}23.4 & \cellcolor{green!25}15.3 & \cellcolor{green!25}21.0 & \cellcolor{green!25}10.9 & \cellcolor{green!25}31.5 & \cellcolor{green!25}23.5 & \cellcolor{green!25}26.7 & \cellcolor{green!25}33.8 & \cellcolor{green!25}22.0 \\
%  Typo & \cellcolor{green!25}8.3 & \cellcolor{green!25}20.1 & \cellcolor{green!25}17.2 & \cellcolor{green!25}18.2 & \cellcolor{green!25}7.8 & \cellcolor{green!25}25.8 & \cellcolor{green!25}28.3 & \cellcolor{green!25}29.4 & \cellcolor{green!25}36.8 & \cellcolor{green!25}24.3 \\
%  AddNe & \cellcolor{green!25}7.5 & \cellcolor{green!25}17.0 & \cellcolor{green!25}14.1 & 8.7 & \cellcolor{green!25}15.1 & \cellcolor{green!25}15.3 & 14.4 & 14.5 & 17.8 & 2.1 \\
%  Anton & \cellcolor{green!25}5.8 & 8.9 & 7.1 & 6.2 & \cellcolor{green!25}7.2 & \cellcolor{green!25}27.8 & \cellcolor{green!25}26.9 & \cellcolor{green!25}36.1 & \cellcolor{green!25}31.6 & \cellcolor{green!25}24.7 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 21.3 & 31.2 & 18.7 & 28.6 & 18.2 & 36.2 & 32.9 & 36.0 & 37.1 & 23.3  \\
%  Jumble & 19.4 & \cellcolor{green!25}34.6 & \cellcolor{green!25}19.4 & \cellcolor{green!25}37.6 & \cellcolor{green!25}30.1 & 31.7 & \cellcolor{green!25}38.4 & \cellcolor{green!25}37.2 & \cellcolor{green!25}37.4 & \cellcolor{green!25}30.8  \\ 
%  Typo & 17.7 & 29.8 & 15.2 & \cellcolor{green!25}33.7 & \cellcolor{green!25}23.0 & \cellcolor{green!25}42.4 & \cellcolor{green!25}46.9 & \cellcolor{green!25}36.4 & \cellcolor{green!25}47.2 & \cellcolor{green!25}38.7 \\ 
%  AddNe & 12.3 & 28.7 & 14.2 & 22.0 & 15.9 & 29.1 & 27.0 & 19.3 & 22.6 & 15.9 \\
%  Anton & 18.5 & 20.8 & 13.6 & 19.0 & 11.7 & 36.2 & \cellcolor{green!25}45.1 & \cellcolor{green!25}41.6 & \cellcolor{green!25}40.6 & \cellcolor{green!25}40.6  \\
%  % RpSt &  \\
%  % CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Kendall correlation ($|\tau|$) between different metrics and human evaluations on three Inhouse datasets. 
% % We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:inhouse_kendall}
% \end{table*}


% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 1.6 & 3.8 & 3.4 & 8.3 & 1.4 & 33.5 & 20.7 & 17.8 & 24.4 & 19.0 & 6.8 & 5.7 & 1.1 & 1.1 & 2.6 \\ 
%  BER & 3.1 & 6.2 & 18.3 & 3.8 & 6.2 & 37.8 & 33.3 & 28.9 & 33.9 & 30.2 & 46.9 & 40.2 & 57.9 & 51.8 & 15.1  \\
%  Mov & 2.2 & 7.4 & 23.3 & 3.3 & 9.7 & 19.0 & 25.4 & 21.8 & 25.4 & 32.0 & 32.3 & 25.1 & 38.4 & 36.1 & 11.0 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO &  \\  
% MAN &  \\ 
% StoER &  \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC &  \\ 
%  UNIE &  \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (with BART-large)}}\\ 
%    -- &  \\
%  Jumble &  \\
%  Typo & \\
%  Anton &  \\
%  RpSt &  \\
% \midrule
% \multicolumn{12}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 43.8 & 43.5 & 37.3 & 32.2 & 30.0 & 27.4 & 44.3 & 38.4 & 51.0 & 50.1 & 56.2 & 70.4 & 73.1 & 69.8 & 60.9\\
%  Jumble & 46.3 & 47.2 & 38.8 & 33.3 & 23.8 & 67.4 & 74.5 & 62.5 & 81.6 & 67.4 & 50.3 & 65.6 & 63.0 & 65.7 & 54.5 \\ 
%  Typo & 47.3 & 45.4 & 41.0 & 30.4 & 24.7 & 49.6 & 60.3 & 52.6 & 68.5 & 60.0 & 59.1 & 70.4 & 73.8 & 71.0 & 59.2 \\ 
%  AddNe & 30.5 & 43.0 & 44.9 & 28.5 & 27.4 & 17.3 & 22.9 & 5.8 & 33.2 & 38.3 & 52.4 & 59.0 & 63.4 & 60.9 & 43.6  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Pearson correlation ($|r|$) between different metrics and human evaluations on three CoudSourcing datasets. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:crowdsource_pearson}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%   BLE & 4.8 & 5.8 & 2.0 & 7.0 & 0.8 & 27.0 & 19.6 & 19.8 & 21.2 & 14.3 & 15.3 & 0.9 & 3.4 & 0.3 & 5.3 \\
%   BER & 5.5 & 7.3 & 19.7 & 7.8 & 9.9 & 29.6 & 27.7 & 23.3 & 29.2 & 28.2 & 42.6 & 33.2 & 54.0 & 49.9 & 12.4 \\
%   Mov & 4.9 & 9.1 & 22.6 & 11.0 & 15.2 & 14.1 & 22.4 & 22.3 & 21.3 & 28.5 & 27.5 & 13.2 & 32.7 & 30.0 & 4.8 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 0.6 & 17.6 & 17.6 & 1.4 & 6.1 & 29.2 & 35.4 & 29.9 & 36.9 & 39.6 & 10.0 & 1.6 & 24.5 & 4.2 & 1.6 \\  
% MAN & 30.8 & 45.5 & 31.8 & 21.5 & 17.7 & 0.1 & 8.2 & 3.5 & 4.9 & 12.8 & 8.5 & 27.0 & 18.3 & 25.5 & 16.5 \\ 
% StoER & 9.7 & 6.1 & 6.0 & 5.8 & 14.3 & 25.8 & 23.7 & 18.9 & 31.2 & 34.5 & 6.7 & 6.6 & 0.1 & 10.5 & 8.4 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 31.7 & 37.8  & 21.7  & 17.0  & 12.2 & 60.0 & 70.1 & 52.1 & 72.8 & \textbf{63.8} & 40.0 & 52.1 & 56.2 & 58.6 & 44.6 \\ 
%  UNIE & 44.3 & 45.2 & 33.3 & 29.0 & 26.6 & 52.6 & 56.8 & 50.9 & 67.4 & 52.7 & 33.9 & 43.6 & 50.4 & 53.2 & 37.3 \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (with BART-large)}}\\ 
%    -- & 15.7 & 1.9 & 1.8 & 3.9 & 3.3 & 7.4 & 6.8 & 1.8 & 4.7 & 11.5 & 21.0 & 23.2 & 29.3 & 21.3 & 9.2 \\
%  Jumble & \cellcolor{green!25}22.8 & \cellcolor{green!25}8.5 & \cellcolor{green!25}11.5 & \cellcolor{green!25}11.6 & \cellcolor{green!25}4.0 & \cellcolor{green!25}8.7 & \cellcolor{green!25}6.9 & \cellcolor{green!25}7.1 & 2.5 & 10.6 & \cellcolor{green!25}32.8 & \cellcolor{green!25}29.3 & \cellcolor{green!25}34.2 & \cellcolor{green!25}30.6 & \cellcolor{green!25}33.1 \\
%  Typo & 9.6 & \cellcolor{green!25}4.6 & \cellcolor{green!25}6.2 & \cellcolor{green!25}5.4 & 2.7 & 3.0 & 3.0 & 1.5 & \cellcolor{green!25}7.0 & 9.0 & 19.9 & 4.0 & 1.2 & 8.5 & 7.5 \\
%  AddNe & \cellcolor{green!25}17.2 & \cellcolor{green!25}21.3 & \cellcolor{green!25}10.0 & \cellcolor{green!25}4.0 & \cellcolor{green!25}6.3 & 6.6 & \cellcolor{green!25}11.4 & \cellcolor{green!25}6.0 & \cellcolor{green!25}9.6 & 9.1 & 8.7 & \cellcolor{green!25}23.5 & 6.8 & 17.9 & \cellcolor{green!25}26.1 \\
%  Anton & 11.3 & \cellcolor{green!25}13.7 & 0.4 & \cellcolor{green!25}18.5 & \cellcolor{green!25}16.3 & \cellcolor{green!25}15.4 & \cellcolor{green!25}21.7 & \cellcolor{green!25}11.0 & \cellcolor{green!25}11.1 & 9.5 & \cellcolor{green!25}26.4 & 13.7 & 21.0 & 14.6 & 8.7 \\
%  RpSt & \cellcolor{green!25}21.7 & \cellcolor{green!25}14.2 & \cellcolor{green!25}9.3 & \cellcolor{green!25}11.2 & \cellcolor{green!25}7.3 & \cellcolor{green!25}12.2 & 1.2 & \cellcolor{green!25}13.3 & 3.4 & 4.7 & \cellcolor{green!25}23.1 & \cellcolor{green!25}30.7 & \cellcolor{green!25}39.3 & \cellcolor{green!25}30.4 & \cellcolor{green!25}22.9 \\
%  CS & 4.5 & 0.2 & \cellcolor{green!25}4.2 & \cellcolor{green!25}10.1 & \cellcolor{green!25}12.2 & \cellcolor{green!25}8.7 & 4.1 & \cellcolor{green!25}5.1 & 4.6 & 1.7 & 8.1 & 0.0 & 3.6 & 0.1 & 0.8 \\
% \midrule
% \multicolumn{12}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- & 34.4 & 15.8 & 15.4 & 15.7 & 19.8 & 31.3 & 26.5 & 25.0 & 31.6 & 22.6 & 48.2 & 39.7 & 38.0 & 46.8 & 28.2 \\
%  Jumble & \cellcolor{green!25}37.4 & \cellcolor{green!25}22.6 & 15.0 & 14.7 & 14.1 & \cellcolor{green!25}60.4 & \cellcolor{green!25}53.7 & \cellcolor{green!25}50.5 & \cellcolor{green!25}59.5 & \cellcolor{green!25}52.7 & \cellcolor{green!25}49.9 & \cellcolor{green!25}44.4 & \cellcolor{green!25}41.2 & \cellcolor{green!25}54.0 & 23.5 \\
%  Typo & 31.9 & 14.9 & \cellcolor{green!25}18.0 & 11.1 & 16.0 & \cellcolor{green!25}43.7 & \cellcolor{green!25}38.9 & \cellcolor{green!25}37.6 & \cellcolor{green!25}43.7 & \cellcolor{green!25}32.9 & \cellcolor{green!25}55.4 & \cellcolor{green!25}42.8 & \cellcolor{green!25}46.3 & \cellcolor{green!25}54.2 & \cellcolor{green!25}32.1 \\
%  AddNe & 11.8 & \cellcolor{green!25}23.6 & \cellcolor{green!25}25.1 & \cellcolor{green!25}16.4 & 13.9 & 17.0 & 22.6 & 9.2 & 23.8 & 22.6 & 27.0 & 19.4 & 20.6 & 24.7 & 14.2 \\
%  Anton & 20.1 & \cellcolor{green!25}30.7 & 14.4 & \cellcolor{green!25}25.4 & 19.8 & \cellcolor{green!25}56.2 & \cellcolor{green!25}55.2 & \cellcolor{green!25}58.0 & \cellcolor{green!25}61.2 & \cellcolor{green!25}50.5 & 13.1 & 15.5 & 5.5 & 15.2 & 14.0 \\
%  RpSt & 1.6 & \cellcolor{green!25}16.3 & 11.2 & \cellcolor{green!25}23.1 & 14.4 & 14.6 & 12.5 & 19.0 & 17.4 & 20.8 & 39.4 & 34.9 & \cellcolor{green!25}39.4 & 46.0 & 20.6 \\
%  CS & 0.6 & 10.4 & 9.5 & \cellcolor{green!25}20.6 & 14.1 & \cellcolor{green!25}36.7 & \cellcolor{green!25}33.7 & \cellcolor{green!25}29.5 & 28.6 & 21.1 & \cellcolor{green!25}53.6 & \cellcolor{green!25}51.0 & 35.9 & \cellcolor{green!25}51.1 & \cellcolor{green!25}41.0 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 43.2 & 39.5 & 35.4 & 29.9 & 27.4 & 33.5 & 40.8 & 34.1 & 43.9 & 43.1 & 47.6 & 64.9 & 66.8 & 67.4 & 58.7 \\
%  Jumble & 50.4 & 47.3 & 43.5 & 33.3 & 25.5 & 72.8 & 68.6 & 51.3 & 72.6 & 58.8 & 53.1 & 62.3 & 61.8 & 69.1 & 43.2  \\ 
%  Typo & 47.6 & 42.3 & 41.1 & 28.5 & 22.8 & 56.9 & 57.1 & 45.9 & 62.1 & 51.2 & 58.0 & 65.0 & 70.5 & 70.8 & 55.5 \\ 
%  AddNe & 30.7 & \cellcolor{green!25}43.6 & \cellcolor{green!25}\textbf{49.1} & 26.5 & 26.7 & \cellcolor{green!25}38.7 & 35.1 & 21.8 & 41.9 & 35.3 & \cellcolor{green!25}48.0 & \cellcolor{green!25}60.8 & \cellcolor{green!25}65.0 & 62.9 & 46.1  \\
%  Anton & 32.2 & \cellcolor{green!25}40.1 & 26.4 & 29.7 & 19.4 & \cellcolor{green!25}\textbf{72.9} & \cellcolor{green!25}67.7 & \cellcolor{green!25}56.4 & \cellcolor{green!25}71.4 & \cellcolor{green!25}56.9 & \cellcolor{green!25}48.3 & 59.4 & 52.7 & \cellcolor{green!25}64.6 & 40.5 \\
%  RpSt & 0.8 & 2.7 & 8.8 & 15.1 & 11.2 & 25.2 & 26.4 & 28.7 & 30.6 & 35.0 & 31.4 & 42.6 & 52.4 & 53.8 & 42.6 \\
%  CS & 15.6 & 15.3 & 19.3 & 1.0 & 0.4 & \cellcolor{green!25}37.5 & \cellcolor{green!25}47.1 & \cellcolor{green!25}35.6 & 40.7 & 32.1 & 34.4 & 55.7 & 55.6 & 50.6 & 40.7 \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Spearman correlation ($|\rho|$) between different metrics and human evaluations on three CoudSourcing datasets. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:crowdsource_spearman}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 3.4 & 4.4 & 0.8 & 4.6 & 0.4 & 19.8 & 13.3 & 13.8 & 15.0 & 10.0 & 9.1 & 0.8 & 2.6 & 0.2 & 3.7\\ 
%  BER & 3.5 & 5.0 & 14.0 & 5.7 & 7.3 & 21.5 & 18.8 & 16.4 & 20.9 & 19.9 & 32.8 & 25.3 & 40.9 & 37.7 & 8.7\\
%  Mov & 3.6 & 6.5 & 15.7 & 8.0 & 11.2 & 9.6 & 16.4 & 16.1 & 15.4 & 20.9 & 21.3 & 9.5 & 23.9 & 22.2 & 3.0 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO &  \\  
% MAN &  \\ 
% StoER &  \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC &  \\ 
%  UNIE &  \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (with BART-large)}}\\ 
%    -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
% \multicolumn{12}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 32.4 & 27.7 & 25.1 & 21.1 & 19.5 & 24.7 & 30.2 & 25.6 & 32.5 & 31.4 & 36.0 & 46.8 & 48.6 & 49.6 & 44.1 \\
%  Jumble & 36.7 & 34.1 & 30.7 & 23.5 & 18.3 & 55.6 & 50.0 & 37.0 & 56.2 & 43.5 & 41.0 & 47.5 & 43.7 & 52.3 & 31.9 \\ 
%  Typo & 35.1 & 30.5 & 29.1 & 19.9 & 15.8 & 41.7 & 41.2 & 33.3 & 46.3 & 37.1 & 43.9 & 48.1 & 52.7 & 54.3 & 41.6 \\ 
%  AddNe & 21.9 & 30.4 & 35.1 & 19.4 & 20.1 & 27.9 & 24.1 & 15.3 & 29.7 & 25.8 & 41.0 & 44.7 & 50.5 & 49.3 & 30.8  \\
%  Anton & \\
%  RpSt &  \\
%  CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Kendall correlation ($|\tau|$) between different metrics and human evaluations on three CoudSourcing datasets. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:crowdsource_kendall_old}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 21.5 & 11.8 & 6.4 & 4.0 & 16.6 & 13.0 & 18.9 & 7.3 & 9.7 & 21.3 & 25.6 & 5.2 & 28.1 & 1.7 & 25.1 \\ 
%  BER & 3.2 & 1.1 & 25.2 & 2.5 & 4.4 & 28.7 & 33.7 & 37.3 & 27.7 & 30.1 & 53.0 & 50.9 & 70.3 & 52.9 & 47.8 \\
%  Mov & 15.6 & 9.2 & 43.5 & 9.2 & 1.2 & 40.8 & 46.5 & 38.6 & 34.6 & 38.6 & 30.9 & 24.4 & 54.7 & 29.1 & 37.6 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO &  \\  
% MAN &  \\ 
% StoER &  \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC &  \\ 
%  UNIE &  \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (with BART-large)}}\\ 
%    -- &  \\
%  Jumble &  \\
%  Typo & \\
%  Anton &  \\
%  RpSt &  \\
% \midrule
% \multicolumn{12}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- &  \\
%  Jumble &  \\ 
%  Typo & \\ 
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Pearson correlation ($|r|$) between different metrics and human evaluations on three Inhouse datasets. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:inhouse_pearson_old}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 25.0 & 11.3 & 0.8 & 6.6 & 15.0 & 9.4 & 16.9 & 11.2 & 15.4 & 23.7 & 22.7 & 0.2 & 25.1 & 3.6 & 18.1 \\ 
%  BER & 4.4 & 1.3 & 26.7 & 4.2 & 3.1 & 29.4 & 34.6 & 37.5 & 28.1 & 30.6 & 48.7 & 45.3 & 70.8 & 51.7 & 48.6 \\
%  Mov & 12.0 & 8.0 & 38.2 & 7.5 & 0.2 & 41.9 & 47.7 & 36.8 & 30.0 & 34.6 & 23.5 & 14.6 & 47.0 & 25.2 & 33.5 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO &  \\  
% MAN &  \\ 
% StoER &  \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC &  \\ 
%  UNIE &  \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (with BART-large)}}\\ 
%    -- &  \\
%  Jumble &  \\
%  Typo & \\
%  Anton &  \\
%  RpSt &  \\
% \midrule
% \multicolumn{12}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- &  \\
%  Jumble &  \\ 
%  Typo & \\ 
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Spearman correlation ($|\rho|$) between different metrics and human evaluations on three Inhouse datasets. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:inhouse_spearman}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 18.1 & 7.8 & 1.1 & 4.9 & 10.7 & 6.6 & 11.5 & 8.0 & 10.5 & 17.0 & 17.8 & 1.3 & 19.6 & 1.8 & 12.2 \\ 
%  BER & 3.6 & 0.0 & 19.4 & 2.8 & 2.1 & 21.6 & 26.3 & 26.6 & 20.1 & 20.9 & 35.5 & 34.9 & 53.9 & 39.5 & 34.4 \\
%  Mov & 8.5 & 5.2 & 27.6 & 4.9 & 0.0 & 31.3 & 34.0 & 26.5 & 20.5 & 23.2 & 16.6 & 11.1 & 35.1 & 18.4 & 23.0 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO &  \\  
% MAN &  \\ 
% StoER &  \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC &  \\ 
%  UNIE &  \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (with BART-large)}}\\ 
%    -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
% \multicolumn{12}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- &  \\
%  Jumble &  \\ 
%  Typo &  \\ 
%  AddNe &  \\
%  Anton & \\
%  RpSt &  \\
%  CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Kendall correlation ($|\tau|$) between different metrics and human evaluations on three Inhouse datasets. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:inhouse_kendall}
% \end{table*}



% % \begin{table}[t]
% % \small
% % \centering
% % \begin{tabular}{lcccccc}
% % \toprule
% %  \multirow{2}{*}{\textbf{Metric}} & \multicolumn{3}{c}{\textbf{ROC}} & \multicolumn{3}{c}{\textbf{WP}} \\
% %  & $|r|$ & $|\rho|$ & $|\tau|$ & $|r|$ & $|\rho|$ & $|\tau|$ \\
% % \cmidrule(lr){1-1}\cmidrule(lr){2-4}\cmidrule(lr){5-7}
% %   \multicolumn{5}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
% %  BLEU & 8.5 & 8.3 & 5.7 & 1.0 & 2.3 & 1.7 \\ 
% %  BERT. & 27.9 & 25.1 & 17.6 & 15.3 & 15.5 & 10.7 \\ 
% %  Mover. & 13.3 & 11.2 & 7.7 & 0.1 & 1.8 & 1.2 \\ 
% %  \midrule
% %  \multicolumn{5}{l}{\textbf{Story Evaluation Metrics}} \\ 
% % UNION & 40.8 & 46.7 & 33.0 & 10.0 & 17.8 & 12.4 \\ 
% % Union & 41.2 & - & - & 32.6 & - & - \\
% % MANP. & 27.4 & 36.0 & 25.1 & 14.3 & 17.5 & 12.1 \\ 
% % StoryER & 6.4 & 5.5 & 3.8 & 13.3 & 13.5 & 9.3 \\
% %  \midrule
% %  \multicolumn{5}{l}{\textbf{Unified Evaluation Metrics}}\\ 
% %  CTC & 40.4 & 41.5 & 29.0 & 29.4 & 29.6 & 20.7 \\ 
% %  UNI. & 42.8 & 43.0 & 30.2 & 32.2 & 30.6 & 21.4 \\ 
% % \midrule
% %  \multicolumn{5}{l}{\textbf{\deltascore (with GPT-3.5)}}\\ 
% %  - Per & 30.0 & 29.3 & 20.3 & 32.3 & 31.7 & 22.0 \\ 
% %  + Jumble & \cellcolor{green!25}35.3 & \cellcolor{green!25}34.3 & \cellcolor{green!25}23.8 & 31.3 & 31.2 & 21.7 \\ 
% %  + Typos & \cellcolor{green!25}32.0 & \cellcolor{green!25}30.4 & \cellcolor{green!25}21.1 & 32.0 & 31.3 & 21.7 \\ 
% %  \midrule
% %   \multicolumn{5}{l}{\textbf{\deltascore (with BART-cnn)}}\\ 
% %  BARTS & 33.5 & 33.5 & 23.3 & 33.5 & 33.1 & 23.0 \\ 
% %  + Jumble & \cellcolor{green!25}41.9 & \cellcolor{green!25}41.1 & \cellcolor{green!25}28.9 & 33.1 & 32.7 & 23.0 \\ 
% %  + Typos &  \cellcolor{green!25}36.9 & \cellcolor{green!25}37.1 & \cellcolor{green!25}25.9 & \cellcolor{green!25}34.3 & \cellcolor{green!25}33.9 & \cellcolor{green!25}23.7 \\ 
% %  \bottomrule
% % \end{tabular}
% % \caption{Story-level Pearson ($r$), Spearman ($\rho$) and Kendall-Tau ($\tau$) correlations of different metrics on OpenMEVA dataset.}
% % \label{table:corelations_openmeva}
% % \end{table}

\end{document}


