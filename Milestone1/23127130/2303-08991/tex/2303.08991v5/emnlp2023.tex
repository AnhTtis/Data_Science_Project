% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{xcolor,colortbl}
\usepackage{booktabs,tabularx}
\usepackage{graphicx}

\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{textcomp}
% Define the name for Section / SubSection / Table / Figure to save some spaces.
\def\sectionautorefname{\S} 
\def\subsectionautorefname{\S} 
\def\appendixautorefname{Appendix}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{adjustbox}
\usepackage{rotating}
\usepackage{pifont}
\newcommand{\cmark}{\text{\ding{51}}}
\newcommand{\xmark}{\text{\ding{55}}}
% \sisetup{table-parse-only,detect-weight=true,detect-inline-weight=text, round-mode=places, round-precision=1, table-number-alignment=center}

\newcommand{\model}[1]{\textsc{#1}\xspace}
\newcommand{\deltascore}{\model{DeltaScore}}

\newcommand{\tbnum}[1]{\multicolumn{1}{c}{\bfseries \num{#1}}}

\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{xspace}
\usepackage{amssymb}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\newcolumntype{g}{>{\columncolor{Gray}}c}
\newcommand{\Needcite}[1]{{\color{orange}{\bf{[CITE:]}} #1}}
\newcommand{\Zhuohan}[1]{{\color{red}{\bf{[Zhuohan]}} #1}}
\definecolor{hlblue}{rgb}{0.04, 0.73, 0.71}
\newcommand{\hl}[1]{%
    \begin{minipage}{\linewidth}
        \colorbox{hlblue}{%
            \parbox{\dimexpr\linewidth-2\fboxsep}{#1}
        }
    \end{minipage}%
}
\newcommand{\miao}[1]{{\color{blue}{\bf{[Miao:]}} #1}}

\title{\deltascore: Fine-Grained Story Evaluation with Perturbations}


\author{Zhuohan Xie 
 \qquad
 Miao Li
 \qquad
 Trevor Cohn\thanks{\ \ Now at Google DeepMind}
 \qquad
 Jey Han Lau\\
 School of Computing and Information Systems, \\
 The University of Melbourne \\
 \{zhuohanx, miao4\}@student.unimelb.edu.au, 
\{t.cohn, laujh\}@unimelb.edu.au
 }
 

\begin{document}
\maketitle
\begin{abstract}

%Automatically evaluating the story quality in AI storytelling is a challenging open problem, and it impedes the development of automatic story generation. %JHL: skipping the cliche
Numerous evaluation metrics have been developed for natural language generation tasks, but their effectiveness in evaluating stories is limited as they are not specifically tailored to assess intricate aspects of storytelling, such as fluency and interestingness.
In this paper, we introduce \deltascore, a novel methodology that uses perturbation techniques for the evaluation of nuanced story aspects. We posit that the extent to which a story excels in a specific aspect (e.g., fluency) correlates with the magnitude of its susceptibility to particular perturbations (e.g., the introduction of typos). Given this, we measure the quality of an aspect by calculating the \textit{likelihood difference} between pre- and post-perturbation states using pre-trained language models. We compare \deltascore with existing metrics on storytelling datasets from two domains in five fine-grained story aspects:
% We assess \deltascore against existing metrics in two story domains and examine its correlation with human judgments on five fine-grained story aspects: 
fluency, coherence, relatedness, logicality, and interestingness. \deltascore demonstrates strong performance, revealing a surprising finding that one specific perturbation proves highly effective in capturing multiple aspects.
Source code is available on our GitHub repository.\footnote{\url{https://github.com/ZhuohanX/DeltaScore}}
% Potential Delete: Not quite sure if we need to mention the suprising finding.


%The findings of our study indicate that the \deltascore approach exhibits strong performance in evaluating intricate story aspects. 
% An unexpected discovery was made in our experiment, where a single perturbation method was found to effectively capture a majority of these aspects.

% We measure the effectiveness of \deltascore against state-of-the-art model-based and traditional similarity-based metrics over several story domains, by comparing how well it correlates with human judgements on 5 fine-grained story aspects (fluency, coherence, relatedness, logicality and interestingness). We found \deltascore performs very well, with the surprising finding that one particular perturbation appears to work very well for measuring most aspects.
%Our extensive experimental show that \deltascore has substantially better correlations against human judgements versus existing state-of-the-art evaluation metrics without any requirements of fine-tuning, and it achieves fine-grained evaluation of story generation with different aspect-aware perturbations.


% as a consequence of the vast range of possible generations
% Automatic story evaluation has long been admittedly a challenging task.
% Evaluation metrics that are originally proposed for 
% other natural language generation (NLG) tasks
% are widely adopted in current story model developing literature.
% However, they have been criticized being not suitable for open-ended generations,
% since they usually show low correlations to human evaluations, which is the de facto
% standard for story evaluation.
% State-of-the-art (SOTA) story evaluation metrics train classifiers to distinguish original stories from the negative samples or
% highly-upvoted stories from lowly-upvoted ones.
% As a result, they obtain one story evaluator that can evaluate story coherence or human preference.
% These approaches only provide one overall score, which do not align well with the multiple aspects of story quality.
% We propose to evaluate story via differences between GPT3 likelihood from perturbations.
% Experiments show our methods have a higher correlations to human evaluations,
% and can provide explainability to multiple aspects.

\end{abstract}


%JHL: after reading the whole paper, I think in terms of narrative introduction we can do something like this: (1) 

\section{Introduction}
\label{sec:introduction}


%JHL2: suggest we don't use both PLM and LLM acronyms. I think we can just stick with PLMs, since our method isn't limited to just 'large' LMs.
% Zhuohan: addressed.
The emergence of large pre-trained language models (PLMs) \citep{llmsurvery-zhaoxin-arxiv2023} has empowered story generation models to generate plausible narratives \citep{mutltaskvae-alta21, progen-naacl21, persona-storygen-naacl22, re3-emnlp23}. The most advanced models have achieved the ability to produce stories which are not easily distinguishable from human-authored ones \citep{mturkperils-emnlp21, scarecrow-acl22, thenextchapter-inlg23}. However, the development of automated evaluation metrics in this domain has not progressed at the same pace \citep{openmeva-acl21}.
%JHL: I'm commenting out the following line because, in spite of the lack of story evaluation metrics, story generation has progressed much in the past few years, and so the argument that it impedes the development of story generation isn't quite true
%, and the lack of reliable metrics  impedes the development of story generation .
Human evaluation, though considered the gold standard, is hindered by its time-consuming, costly, and non-reproducible nature \citep{nlgevaluationmetricssurvey-sai-acm23}. Consequently, there is a demand for better automatic methods that can evaluate the quality of stories.

\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/Examples-add_typos.pdf}
         \caption{Perturbation ``Add typos'' affects the highly fluent story (top) more than the less fluent one (bottom).
         }
         \label{fig:addtypos}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/Examples-remove_keyentity.pdf}
         \caption{Two stories are conditioned on the same title ``I always go to the local supermarket''. Perturbation ``Remove relevant words'' affects the highly related story (top) more while not affect the unrelated one (bottom).
         % Higher quality story is affected more than lower quality one.
         }
         \label{fig:removekeyentity}
     \end{subfigure}
        \caption{
        Scenarios where higher quality stories (top) are 
        affected more than lower quality ones (bottom) through aspect-specific perturbations (fluency: ``Add typos''; relatedness: ``Remove relevant words'').
        Generative likelihood for original/perturbed story is in blue/green circle, and
        the \deltascore value is in orange circle. 
        % \miao{Better to give examples that show aspect-aware perturbations. [Changed]}
        %JHL2: make the bottom 'it is a nice dog' -> 'it is a nice' ? (the current example doesn't remove any key entity), and also make the score lower? 0.75 seems very high for an unrelated story.
        % Zhuohan: Yes, the idea is that the unrelated story will not be affected that much by perturbations. Therefore, it is not affected here.
        }
        \label{fig:examples}
\end{figure}

The prevailing evaluation metrics for story assessment have primarily been adapted from other natural language generation (NLG) tasks, such as BLEU \citep{bleu-acl02} for machine translation, or ROUGE \citep{rouge-acl04} for summarization. Fortunately, recent progress has given rise to the emergence of new metrics explicitly tailored for story evaluation, with a focus on quantifying story coherence \citep{union-emnlp20, manplts-naacl21} or capturing human preferences \citep{storyer-emnlp22}.
Other works have directly utilized the likelihood of a story under a PLM \citep{transformer-nips17, storyflashbackgen-naacl22} or its conditional likelihood based on human references or other contextual factors, such as story title \citep{prism-emnlp20, bartscore-nips21}. Nonetheless, these approaches often yield a singular score that provides an estimate of the overall quality. However, \citet{hannabenchmark-coling22} argue that the quality of a story is comprised of various fine-grained aspects, such as fluency and adherence to commonsense, suggesting that an overall quality score has limited utility for comprehensive story evaluation.
%In other words, a metric that produces a low overall score for a story does not reveal whether the story has fluency issues or certain elements of the story violate commonsense \cite{DBLP:journals/corr/abs-2203-11131}.

In this paper, we present \deltascore, a method that evaluates story quality by measuring the \textit{likelihood difference} using a PLM between an original story and its perturbed version. The idea is that higher quality stories will exhibit more significant effects from the perturbation compared to lower quality ones.
To provide fine-grained assessment of story quality, we experiment with perturbations that target specific aspects.
\autoref{fig:examples} presents two examples to demonstrate the intuition of our approach:
% \miao{Give description about aspect-aware perturbations.} 
1) 
%the story $\boldsymbol{s_1}$: ``Mike dna Jake aer best friends!'' contains two typos, which are less fluent than the story $\boldsymbol{s_2}$: ``Mike and Jake are best friends!''.
When we introduce random typos to modify the two stories shown in \autoref{fig:addtypos}, we observe that the story with higher fluency is affected more by the perturbation;
% Therefore, 
2) 
%\textbf{Relatedness}: for the title ``I always go to the local supermarket'', we have two stories,
%the story $\boldsymbol{s_1}$: ``The supermarket has various kinds of goods.'' is highly related to the title while the the story %$\boldsymbol{s_2}$: ``It is a nice day.'' is less related.
When we modify the two stories in \autoref{fig:removekeyentity} by removing relevant words, we observe that the perturbation affects the story that has a closer association with the title to a greater extent.
Our empirical analysis demonstrates the superior performance of \deltascore compared to existing metrics in evaluating intricate story aspects. Furthermore, our investigation reveals an interesting discovery: one of our simplest perturbation methods, which simply shuffles all the words in the story, is very effective in capturing multiple aspects. This points to a possible interpretation that the pertubation may be functioning as a normalisation factor to modulate the effects of word frequency and text length when estimating sequence likelihood.
% One surprising finding is that one particular perturbation (word shuffling) appears to work very well to capture multiple aspects, suggesting that the fine-grained aspects may be mutually correlated.
%The key entity ``supermarket'' is removed from $\boldsymbol{s_1}$ while $\boldsymbol{s_2}$ is not affected.
% Perturbation ``Add typos'' is applicable to both stories, while it cause more damages to the higher quality one.
% Different from the previous works,
% we propose to evaluate generated stories with differentiating perturbations,
% which is we calculate the difference of conditional likelihood of the original stories and
% that of their perturbations.
% Besides, we propose to evaluate stories via differences between GPT3 likelihood instead of using likelihood directly as evaluation metrics 
% as we posit that higher quality stories are more sensitive to perturbations than
% lower quality ones.
%There are two benefits in evaluating stories via differences between likelihood:
%1) It can result in a statistically more stable evaluation metric that correlates better to human evaluation.
%2) Different aspects of story quality can be evaluated by designing different perturbations.

%Specifically, we use generative likelihood assigned by GPT-3.5\footnote{It is also called text-davinci-003, which is the largest and latest version of GPT-3 with an available API at the time of our experiments. Note that even though ChatGPT is released and popular all over the world, it does not yet provide an API to obtain its generative likelihood at the time of writing.} as advanced large language models have demonstrated spectacular performance of understanding narratives \citep{Qin2023IsCA}.


% \miao{To evaluate the effectiveness of \deltascore, we ..., and we get that ...}

%JHL2: don't think we need to spell out the contributions again, since i thought it's quite clear from the intro
% Zhuohan: Addressed

% Our contributions of the paper are summarized as follows:
% \begin{itemize}
% % \item We propose a novel evaluation technique that does not require any additional fine-tuning: $\Delta$Likelihood ($\Delta$LL) where we evaluate story quality via differences between generative likelihood from perturbations.
% \item We propose to use likelihood difference based on perturbations as a new approach for fine-grained evaluation of generated texts.
% %\item We propose a novel and unsupervised evaluation approach to measure fine-grained aspects of story quality.
% \item Experimental results indicate that our metric demonstrates a strong correlation with human evaluations in most aspects, surpassing the performance of advanced metrics across various story domains.

% Experimental results show that our metric correlates well with human judgements for most aspects, outperforming state-of-the-art metrics over several story domains.

% \item We demonstrate the technique is applicable to both auto-regressive models with GPT3 and sequence-to-sequence (seq2seq) models with BART.
% \item We explore various perturbations and our results show $\Delta$LLs can have a much better correlations to human evaluation than using likelihood as evaluation metrics directly by applying certain perturbations.
% \item We compare with extensive state-of-the-art (SOTA) evaluation metrics and find our best $\Delta$LL outperforms in all quality aspects of story evaluation.

% \item We show binary classification idea is not suitable for metrics as it can only produce scores that close to 0 and 1s, which is not suitable for story evaluation.
%\end{itemize}


\section{Related Work}
\label{sec:relatedwork}

\subsection{Automatic Evaluation Metrics}
\label{subsec:automaticevaluationmetrics}

Existing automatic evaluation metrics can be broadly categorized into three paradigms.
\paragraph{Similarity metrics} mainly focus on measuring lexical overlap such as BLEU~\citep{bleu-acl02}, NIST~\citep{nist-ichlt02} and ROUGE~\citep{rouge-acl04} or semantic similarity with contextual representations including MoverScore~\citep{moverscore-emnlp19} and BERTScore~\citep{bertscore-iclr20} between the machine-generated text and its human reference. 
\paragraph{Discriminative metrics} typically involve training a discriminator model to differentiate between high-quality and low-quality texts, including UNION~\citep{union-emnlp20}, MANPLTS~\citep{manplts-naacl21}, CTC~\citep{ctc-emnlp21}, StoryER~\citep{storyer-emnlp22}, and UNIEVAL~\citep{ unieval-emnlp22}. 
Specifically, UNION constructs negative samples of original stories using heuristic rules and trains a discriminator to differentiate them. 
MANPLTS is an extension of UNION that constructs improved negative samples by manipulating storylines and generating alternate stories based on these manipulated storylines using a story generation model.
StoryER builds a classifier to learn human preference by training it to differentiate highly-upvoted stories from lowly-upvoted ones on Reddit.
CTC treats the evaluation task as an information alignment task.
UNIEVAL frames the evaluation as a question answering task where different questions are asked to assess a particular aspect.
\paragraph{Generative metrics} usually rely on generative likelihood to determine the quality of the text, including BARTScore~\citep{bartscore-nips21}, T5Score~\citep{t5score-arxiv22} and GPTScore~\citep{gptscore-arxiv23}. Specifically,
BARTScore evaluates generated text by calculating its conditional likelihood under BART.
GPTScore calculates the likelihood of the story
under a PLM with additional prefix to target a particular aspect.
T5Score benefits from both worlds by employing both generative training with the standard negative log likelihood loss and discriminative training with contrastive loss where human judgments for generation quality are available. 


% The conventional evaluation metrics used in natural language processing  However, such methods are limited in their ability to capture semantic similarity, and they are vulnerable to small changes in morphology or even typos, as highlighted by \citet{bertmetricsexplain-emnlp21}.
% To address these limitations, recent approaches have leveraged the language comprehension abilities of PLMs to capture semantic similarity through the comparison of embeddings . Nevertheless, these evaluation metrics still rely on a single human reference, which may not be suitable for story evaluation, considering the diverse ways in which a well-crafted story can be written.

% As a result, researchers have increasingly directed their attention towards reference-free evaluation metrics. For instance, 


%JHL2: double check if the above detail (Reddit) is right
%Zhuohan: Yes, they did crawl data from Reddit.

%Potential Delete 
% This task is particularly useful for evaluating the subjective aspects of stories that cannot be easily captured by traditional metrics. 

% These methods work as a general method for natural language generation tasks, 
% however, as different tasks have different characteristics, 
% automatic metrics tailored for specific tasks such as summarization \citep{scialom-etal-2021-questeval}, data-to-text \citep{rebuffel-etal-2021-data}, and dialogue generation \citep{mehri-eskenazi-2020-usr} have to be proposed.
% There are also alternative evaluation metrics that evaluate the model holistically by comparing the difference of distributions between the learnt model and the dataset \citep{DBLP:conf/nips/PillutlaSZTWCH21, DBLP:conf/emnlp/DengKR22}

% \paragraph{Story Evaluation Metrics}
% Some evaluation metrics are specifically designed for story evaluation.


% \paragraph{Unified Evaluation Metrics}
% Recent evaluation metrics have shifted focus from providing a single score for overall quality. Instead, these new metrics aim to capture multiple aspects by considering various inputs such as prompts, generated text, and contextual information \citep{unieval-emnlp22}. For example,  while \citet{ctc-emnlp21} utilize information alignment techniques to assess the quality of generated text.
% Despite the diverse perspectives offered by these evaluation methods, they often fall short in capturing more intuitive aspects of text quality such as fluency. To overcome this limitation, 
% \citet{unieval-emnlp22} introduce an approach that treats text evaluation as a question answering task, training a language model to answer specific questions such as  ``\textit{Is this a fluent sentence?}''. 

%In our work, we propose a novel evaluation approach that targets multiple story quality aspects, which to the best of our knowledge, is the first of its kind.

\subsection{Natural Text Perturbation} 
The use of perturbations is a conventional technique to generate negative samples for both discriminative \citep{union-emnlp20} and generative \citep{unieval-emnlp22} tasks.
\citet{checklist-acl20} propose CheckList, a suite of perturbation techniques to evaluate the behavioral performance of NLP models.
\citet{perturbationchecklist-emnlp21} further delve into applying perturbations to assess robustness of NLG evaluation metrics, while
\citet{demetr-translation-evaluation-diagnose-emnlp22}
specifically focus on machine translation evaluation.
\citet{evaluationmetricsblindspots-arxiv22} also develop perturbation tests to identify blind spots of model-based evaluation metrics.
Notably, all of these perturbations rely on heuristic rules. In contrast, recent adversarial attacks such as those proposed by \citet{bertattack-emnlp20, textattack-emnlp20} use language models to generate adversarial examples, which can also be considered a form of text perturbation. In our work, we explore perturbation for a different purpose: to evaluate fine-grained story qualities.

% Even though these approaches might produce much more dedicated negative samples,
% we do not consider these approaches, since the introduction of an additional black-box language models make the whole process less interpretable.

\section{\deltascore}
\label{sec:deltascore}

% Note: The most ideal way is to find specific perturbations that each works for one specific
% aspect/and some work for general, but I cannot find something like this works now.


%JHL: minor, but maybe some of the perturbation terms can be modified to sound a bit more natural/intuitive:
% Jumble -> Shuffle; GeneralWord -> ? (I thought this is replaced with a hypernym, but looking at the example (girl -> baby), I am not sure anymore); AntonymAttributes -> Antonym; LameWords -> Synonym?; LameSents -> Paraphrase?
\begin{table*}[t]
\centering
\small
\begin{tabular}{p{1cm} p{2.2cm} p{5.5cm} p{5.5cm}}
\toprule
\textbf{Aspect} & \textbf{Perturbation} & \textbf{Original story} & \textbf{Perturbed story} \\
\midrule
\multirow{2}{1cm}[-0.5ex]{{Flu.}} & {Typo} & he went to see what the problem was & he went to see whta the problem was \\
\cmidrule{2-4}
 & {SubjVerbDis} & he is the best student in the classroom . & he am the best student in the classroom . \\
 \midrule
 \multirow{2}{1cm}[-2.0ex]{{Coh.}} & {Jumble} & We play badminton every evening . & badminton every We evening play . \\
 \cmidrule{2-4}
  & \multirow{2}{1cm}[0ex]{{SentReorder}} & she did n't intend to buy anything . unfortunately she has poor impulse control ... & unfortunately she has poor impulse control . she did n't intend to buy anything ... \\ 
 \midrule
 \multirow{2}{1cm}[-0.5ex]{{Rel.}} & \underline{RmRelWords} & The supermarket has various kinds of goods & The has various kinds of goods  \\
 \cmidrule{2-4}
  & \underline{StoryReplace} & The supermarket has various kinds of goods & It is a nice day to hang out  \\
 \midrule
 \multirow{2}{1cm}[-3.0ex]{{Log.}} & \multirow{2}{1cm}[0ex]{{Antonym}} & The boy got the gift he always wanted, he was so happy .  & The boy got the gift he always wanted, he was so sad . \\
 \cmidrule{2-4}
  & \multirow{2}{1cm}[0ex]{\underline{Commonsense}} & they took me down to the lake . i threw my line out and caught several worms ... & they took me to the moon. i threw my line out and caught several stars ... \\
 \midrule
 \multirow{2}{1cm}[-1.0ex]{{Int.}} & \multirow{2}{1cm}[-1.0ex]{\underline{BlanderNarrative}} & i felt really angry, talked to my estranged father , and he gave me a gun! But I knew violence is not a solution here . & I felt upset and talked to my father about it . He advised me to handle the situation calmly , so I decided not to resort to violence . \\
 \bottomrule
\end{tabular}
\caption{Summary of perturbations that target a story quality aspect: Fluency (Flu.), Coherence (Coh.), Relatedness (Rel.), Logicality (Log.), and Interestingness (Int.).
For ``Relatedness'' example stories, they are conditioned on the title ``I always go to the local supermarket''.
 \underline{Underlined perturbations} are original methods we propose.
}
\label{table:perturbation}
\end{table*}

% \subsection{GPT3Score}
% Unlike PRISM \citep{thompson-post-2020-automatic} or BARTScore \citep{DBLP:conf/nips/YuanNL21}, we did not use seq2seq pre-trained models.
% GPT3 is a pure generative autoregressive decoder, which do not take condition and predict the current purely based on previous tokens.


%JHL: definition of story condition c seems odd; typically it's the story title or something, no (not sure why it says it can be human reference).
% Answer: For condition, I meant to say people usually use conditional likelihood to evaluate the text (like BARTScore). The condition can be the source, therefore, it is reference-free evaluation. Or the condition can be the reference, therefore, it is refrence-based evaluation.

We now describe the idea of our approach.
Given a story condition (e.g., a story title) $\boldsymbol{c}=c_1, ..., c_n$ containing $n$ tokens, a model-generated story $\boldsymbol{s}=s_1, ..., s_m$ containing $m$ tokens, and a perturbed story $\boldsymbol{s}' = s'_1, ..., s'_{m'}$ containing $m'$ tokens, \deltascore calculates the likelihood difference under a language model:
\begin{equation}
    \operatorname{\deltascore}(\boldsymbol{s}) = \log p(\boldsymbol{s}|\boldsymbol{c}) -  
    \log p(\boldsymbol{s}'|\boldsymbol{c})
\end{equation}
where $p(\boldsymbol{s}|\boldsymbol{c})$ represents the likelihood of $\boldsymbol{s}$ conditioned on $\boldsymbol{c}$ under a language model. In our experiments, we investigate several PLMs with varying architectures (\autoref{subsec:likelihood_calculation}) and perturbation techniques that are designed to target specific aspects (\autoref{subsec:perturbation}).

%\footnote{All tokens to represent the title, story and modified story are all from the same vocabulary and there could be different vocabularies for different LLMs.}

%different quality aspects of $\boldsymbol{s}$ with the difference between likelihoods of $\boldsymbol{s}$ and $\boldsymbol{s}' = s'_1, ..., s'_{m'}$ which contains $m'$ tokens and is based on a specifically designed perturbation. For each quality aspect of $\boldsymbol{s}$, we first obtain perturbation of the story for the aspect (Section \ref{sec:perturbation}), and then the final score is calculated as the likelihood difference between $\boldsymbol{s}$ and $\boldsymbol{s}'$. The log-likelihood difference is defined as:


% As GPT-3.5 is good at story narrative understanding, we use it as the language model without fine-tuning to obtain the generative likelihood of both $\boldsymbol{s}$ and $\boldsymbol{s}'$. 

\subsection{Two Different Likelihood Calculations}
\label{subsec:likelihood_calculation}

%JHL: revise this section a bit to move away from GPT3-5, and instead say the formulation is based on autoregressive language models (e.g. GPT3). The core idea of DELTASCORE is likelihood difference, and the specific LM we use is flexible so we should define DELTASCORE in a more general manner.


We now explain how we compute $p(\boldsymbol{s}|\boldsymbol{c})$
with encoder-decoder PLMs (e.g., BART \citep{bart-acl20} and T5 \citep{t5-jmlr20}) and decoder PLMs (e.g., GPT-3 \citep{gpt3-nips20}). $p(\boldsymbol{s'}|\boldsymbol{c})$ is computed in the same way and we omit it for brevity.

Denoting language model parameters as $\theta$, we compute \deltascore as follows for
encoder-decoder PLMs:
\begin{align}
\log p(\boldsymbol{s}|\boldsymbol{c}) &= \frac{1}{m}\sum_{t=1}^{m}\ \operatorname{log} p(s_t|\boldsymbol{s}_{<t}, \boldsymbol{c}, \theta)
%\log p(\boldsymbol{s}'|\boldsymbol{c}) &= \frac{1}{m'}\sum_{t=1}^{m'} \operatorname{log}\ p(s'_t|\boldsymbol{s}'_{<t}, \boldsymbol{c}, \theta)
\end{align}
where $t$ denotes timestep in the sequence, and $\boldsymbol{s}_{<t}$ denotes all tokens before the current timestep. Intuitively, the story condition $c$ is captured by the encoder, and the likelihood of the story $s$ is produced by the decoder.


In terms of decoder PLMs, we concatenate $\boldsymbol{c}$ and $\boldsymbol{s}$ to form a sequence $\boldsymbol{x}$
($x_1, ..., x_{n+m} = c_1, ..., c_n, s_1, ..., s_m$) 
to compute \deltascore:
\begin{align}
\log p(\boldsymbol{s|\boldsymbol{c}}) &= \frac{1}{m}\sum_{t=n+1}^{n+m}\ \operatorname{log}\ p(x_t|\boldsymbol{x}_{<t}, \theta)
%\\
%\log p(\boldsymbol{s}'|\boldsymbol{c}) &= \frac{1}{m'}\sum_{t=n+1}^{n+m'} \operatorname{log}\ p(x'_t|\boldsymbol{x}'_{<t},\theta)
\end{align}
\label{eqn:decoderlikelihood}

This formulation means we feed the full sequence including the story condition $\boldsymbol{c}$ and story $\boldsymbol{s}$ as input to the decoder-only PLM, although when computing the story likelihood, we only consider the conditional probabilities for the $\boldsymbol{s}$ tokens.
%Note here we ignore the token probabilities of the title $\boldsymbol{c}$; therefore, the timestep starts from $n+1$.
%We assign the equal weights to each token for \deltascore following \citet{DBLP:conf/nips/YuanNL21}.
% \footnote{We also tried various normalization approaches, such as PenLP and NormLP \citep{lau-etal-2020-furiously},
% but they did not yield constant performance improvement; 
% therefore, we decide to use the uniform normalization.}

% In this paper, we explore GPT3 \citep{DBLP:conf/nips/BrownMRSKDNSSAA20} as the
% representative for auto-regressive models.
% We refer evaluating with the generation likelihood from GPT3 to GPT3Score, therefore:

% \begin{align}
% \operatorname{GPT3Score}(\boldsymbol{s}) = \frac{1}{m}\sum_{t=n+1}^{n+m}\ \operatorname{log}\ p(x_t|\boldsymbol{x}_{<t}, \theta) \nonumber
% \end{align}

% We explore BART \citep{lewis-etal-2020-bart} as the
% representative for seq2seq models.
% Evaluating with the generative likelihood from BART is the same as BARTScore \citep{DBLP:conf/nips/YuanNL21}.

% \begin{align}
% \operatorname{BARTScore}(\boldsymbol{s}) = \frac{1}{m}\sum_{t=1}^{m}\ \operatorname{log}\ p(s_t|\boldsymbol{s}_{<t}, \boldsymbol{c}, \Phi) \nonumber
% \end{align}

\subsection{Perturbations on Story Aspects}
\label{subsec:perturbation}

We follow \citet{thenextchapter-inlg23} to assess five fundamental aspects of story quality: fluency, coherence, relatedness, logicality, and interestingness.
To this end, we survey perturbation methods from the literature \citep{checklist-acl20, perturbationchecklist-emnlp21, openmeva-acl21, evaluationmetricsblindspots-arxiv22} and attempt to align them to one of these five aspects. 
For some aspects, we also propose new perturbation methods.
We now describe each aspect and its associated perturbation methods; 
A summary of these methods and examples is given in \autoref{table:perturbation}.

%We list all the (aligned) perturbation methods used in our experiments (with examples) in \autoref{table:perturbation}. Note that we also introduce new perturbation methods for some of these aspects (underlined methods).
%However, most of previous perturbations were not originally proposed to target story aspects.
%Therefore, we propose several new perturbation approaches underlined in \autoref{table:perturbation}.

%Most of our approaches are developed with the assitance of ChatGPT, due to the difficulty of writing heuristic rules for perturbations in certain aspects.
%Our preliminary experiments demonstrate that ChatGPT\footnote{https://chat.openai.com} is capable of producing promising perturbation results.
%The details of the prompts used for these perturbations can be found in \autoref{appendix:perturbationprompts}.


% We first classify conventional perturbations \citep{sai-etal-2021-perturbation, guan-etal-2021-openmeva, DBLP:journals/corr/abs-2212-10020} into these aspects based on their features.

%JHL: since we've defined story condition c previously, let's make the terminology consistent throughout the paper when we refer to it (i.e. avoid using another term like 'prompts' to refer to c)

%JHL2: let's rewrite this section a bit. my suggestion: for each aspect, write a paragraph of text that explains the aspect, talk about the different perturbations in the literature that targets the aspect, and then spell out explicitly all the methods that we experiment in this paper (if we propose new methods here, detail them too). E.g. in fluency it looks like we test two perturbation methods 'typo' and 'subjverbdis', and so we should say very clearly these are the two methods where we will present results (which might be in the appendix but that's OK). When explaining the methods we use, spell out the hyper-parameters (e.g. perturbation degree) too (footnoting the actual values, and be sure to justify how we arrive at those values as well)



\paragraph{Fluency}
assesses the readability of sentences in the story. Perturbations targeting fluency modify the text at the word or phrase level. We use two perturbation approaches from \citet{checklist-acl20}: 1) \textit{Typo}, where we randomly transpose a character with an adjacent one in the text, and 2) \textit{Subject-verb disagreement (SubjVerbDis)}, where we modify the verbs in a sentence so that they no longer agree with their subjects.

\paragraph{Coherence}
assesses the level of connectivity between sentences in the story. Perturbations targeting coherence modify the text at the sentence level. We use two perturbation approaches from \citet{perturbationchecklist-emnlp21}: 1) \textit{Jumble}, where we randomly shuffle words within the story, and 2) \textit{Sentence Reorder (SentReorder)}, where we randomly shuffle the sentences within the story.

\paragraph{Relatedness}
focuses on the extent to which the story is relevant to the given condition (e.g., story title). Perturbations targeting relatedness alter the story to reduce its association with its condition. We propose two new methods: 1) \textit{Remove Relevant Words (RmRelWords)}, where we use ChatGPT\footnote{https://chat.openai.com} to identify words related to the given title and then remove them from the story, and 2) \textit{Story Replacement (StoryReplace)}, where we substitute the original story with another story from a different story condition. 
To select a ``comparable'' story, we choose a story with where its likelihood is similar to the original story.\footnote{We calculate the likelihood of the original story and a candidate story without considering their story conditions.}

% However, it is essential to ensure that the likelihood of the random story,  The purpose of this requirement is to .
%where conditions with the most similar story quality, as determined by the generative likelihood of the story without its condition.
%Note that it is not ideal to randomly replace a story because the conditional likelihood is not only determined by how related the story is to the given condition but also by the quality of the story itself.

\paragraph{Logicality}
focuses on the extent to which the story complies with commonsense. Perturbations targeting logicality introduce elements into the story that contradict commonsense. We adopt one  approach from \citet{openmeva-acl21}: \textit{Antonym}, where we randomly replace the word with its antonym; and propose a new approach: \textit{Commonsense}, where we use ChatGPT to modify some story elements to violate commonsense.

\paragraph{Interestingness}
measures the degree of predictability in the progression of events within a story, representing a highly subjective aspect. We propose one approach: \textit{BlanderNarrative}, where we use ChatGPT to modify a story to make the narrative less interesting.

%JHL3: for the methods where we use chatgpt, did we use the GUI or the API (gpt3.5-turbo)? I assume is the former since the footnote points to the GUI (but if latter we should say we use gpt3.5-turbo on the API)
%Zhuohan: I was using gpt3.5-turbo here, and I have indicated that in the paper.
The ChatGPT\footnote{We use OpenAI API with the model gpt-3.5-turbo.} instructions for the aforementioned perturbations are detailed in \autoref{appendix:perturbationprompts}. For \textit{Typo}, \textit{Jumbo} and \textit{Antonym}, we can control the degree of perturbation, and this parameter is tuned in \autoref{subsec:preliminaryexploration}.


%For instance, we can specify the percentage of words to shuffle in jumble. 
%We will explore the effect of perturbation degrees later in \autoref{subsec:parameterimpacts}.
% We set the degrees to 0.4, 0.9, and 0.8 for typo, jumble, and antonym perturbations, respectively. 
% In \autoref{subsec:PerturbationDegree}, we investigate the effects of varying the perturbation degrees.
%JHL2: we don't need the following line here; we mention this when we present the results in section 4, and then say full results (with all perturbation methods) in appendix.
% Zhuohan: Addressed.
% However, due to space constraints, we only present a selection of perturbations in \autoref{table:perturbation}.

% \begin{itemize}
%     \item \textbf{Fluency:} Focus on modifications at the word or phrase level, e.g.\ by change to incorrect verb tenses/subject-verb agreement or repeating word/phrases.
%     \item \textbf{Coherence:} Focus on modifications at the sentence level to reduce how well the narrative flows from sentence to sentence, e.g.\ by repeating sentences, replacing  sentences from unrelated stories, or reordering sentences. 
%     \item \textbf{Relatedness:} Alter the story reduce its association with the story condition, e.g.\ by removing key information relevant to the story condition or replacing the whole story with another random story.
%     \item \textbf{Logicality:} Introduce elements into the story that contradict commonsense, e.g.\ by including entities that defy the laws of physics or events that go against cultural norms (``go trick or treating'' on ``Christmas'').
%     \item \textbf{Interestingness:} Alter the story so that it is less interesting to read, e.g.\ by changing the style to use blander language or narrative to be predictable.
% \end{itemize}

% As these quality aspects are not completely orthogonal, a perturbation on one aspect could also impact another aspect even if the perturbation is designed for the original aspect.
% Some quality aspects, especially interestingness, are relatively more subjective and dependent on individual perspectives and preferences. Therefore, our taxnomy might not be applicable to general domains.

% \begin{itemize}
% \item \textbf{RmkeyEntities:} We leveraged ChatGPT to extract all entities related to the given title and subsequently remove them from the story.
% \item \textbf{StoryReplace:} We selected a story from different story conditions with the most similar story quality, as determined by the generative likelihood of the story without its condition.
% \item \textbf{Commonsense:}
% \item \textbf{BlanderNarrative:} We requested ChatGPT to modify stories with minimal changes to render the narratives less interesting.
% \end{itemize}

%Due to the interdependence of these quality aspects, perturbations designed for one aspect can potentially affect other aspects. In particular, aspects such as interestingness, are more subjective and influenced by individual perspectives and preferences. Hence, our taxonomy may not be universally applicable to all domains.



% To address the relatedness aspect, we propose replacing the entire story. However, randomly replacing the story is not feasible as we cannot guarantee the relevance to the given prompts or the quality of the replaced stories. To mitigate this, we select all stories that are produced from different prompts for each given story and compute their likelihood without prompts from GPT-3.5 to assess their quality. We then calculate the absolute value of the difference between the current story and all other stories and select the one with the closest quality.

% For logicality, we propose replacing certain elements of a story to create a violation of commensense while maintaining its fluency and coherence. To achieve this, we rely on ChatGPT by asking it to revise the story to create minimal changes that do not make sense using the prompt ``Revise the following story such that certain elements does not make sense. The revision should be minimal, e.g. by changing a few words.''. Examples of these revisions can be found in \autoref{table:perturbation}, and we test more perturbations including Add Negation (AddNe), Antonym (Anton).

%JHL: i agree with the point about interestingness perhaps being subjective, but don't quite understand the following point (temporarily highlighting this line out for now)
%Answer from Zhuohan: What I am trying to say is that what we believe is interesting / what we believe these perturbations might affect interestingness might not be really applicable to others. Also, these might not be able to affected by our annotations. This also apply to other aspects, such as logicality and relatedness. But in generally, I believe interestingness is the most subjective aspect here.


\section{Experiments}

\begin{table}[t]
\centering
\small
\begin{tabular}{p{0.9cm} p{2.3cm} p{3.1cm} }
\toprule
\textbf{Dataset} & \textbf{Condition} & \textbf{Story}  \\
\midrule
ROC & [FEMALE] dad took me fishing . & we sat in a spot and waited for days ... \\
\midrule
 WP & tell me a story where the first line and last line ... & as i walked into the house , i was assailed by the smell of aging ... \\
 \bottomrule
\end{tabular}
\caption{Sampled examples of given story condition and its generated story for each dataset.
}
\label{table:dataexample}
\end{table}




\subsection{Benchmarks}
\label{subsec:benchmark}

% The use of standardized benchmark datasets is crucial for advancing automatic story evaluation as they enable the evaluation metrics to be tested and compared by measuring their correlations to human evaluations.

%JHL3: we use the generated stories in our previous paper too, no? (not just human ratings)
%Zhuohan: yes, we use the story + human ratings, the current narrative is better.
We use the generated stories and human ratings collected by \citet{thenextchapter-inlg23} on two story datasets: ROCStories (ROC;  \citet{roc-repeval16}; 5-sentence simple stories) and WritingPrompts (WP; \citet{wp-acl18}; longer fictional stories written by users on Reddit).\footnote{\citet{thenextchapter-inlg23} also collected human judgements for CNN-Dailymail, but we do not use them in our study for two reasons: 1) the stories depict real-world events rather than fictional narratives, and 2) most of the language models we test have been trained on this dataset, and so there are potential circularity issues.} %JHL2: we can drop the table if need space
The story condition ($\boldsymbol{c}$) for ROC is the leading sentence; for WP, it is the short paragraph that describes the idea of the story, which is called ``prompt''. We present two example stories from the two datasets in \autoref{table:dataexample}.

\citet{thenextchapter-inlg23} experiment with 6 story generation models that cover large models with prompt-based learning (e.g., GPT-3), smaller fine-tuned models (e.g., BART) and other methods that incorporate planning and commonsense \cite{megatron-emnlp20, kggpt2-tacl20, hint-longtextgen-acl21, progen-naacl21}.
%We present some example stories from the two datasets in \autoref{table:dataexample}.
They then conduct human evaluation on five aspects,
judged using an ordinal scale from 1 (worst) to 5 (best). Two distinct groups of annotators were recruited, comprising in-house PhD students and crowdworkers. The results obtained from both groups were found to be similar, indicating the robustness and reliability of the annotation process.\footnote{For both groups, they gather assessments for 140 stories for ROC and 100 stories for WP, with each story being evaluated by three annotators. Annotations on human-written stories are excluded as they could introduce bias in favor of reference-based metrics. As a result, this leaves us with 120 stories for ROC and 80 stories for WP, respectively.}
%JHL5: let's move this new information to footnote (it's not critical information).
%done

% \begin{table}[t]
% \centering
% \small
% \begin{tabular}{ccc}
% \toprule
% \textbf{Dataset} & \textbf{\#Stories} & \textbf{\#Annotators}  \\
% \midrule
% ROC & 120 & 3 \\
% \midrule
%  WP & 80 & 3 \\
%  \bottomrule
% \end{tabular}
% \caption{Meta-evaluation statistics. Same statistics for both in-house PhD students and crowdworkers.
% }
% \label{table:metaevaluation}
% \end{table}
%JHL3: did we also report their correlation? if so let's mention that here too
%Zhuohan: we did report the correlations in the Appendix, note that we did that becuase 
%JHL4: in that case let's print the correlation between the two groups in the appendix
% and found similar results from the two groups, suggesting robustness in their annotation.
%\footnote{We also compute correlation between the two groups of workers and found high correlation; see Appendix X.}
%JHL4: this is annotator agreement within a group yea? we don't need that (we need cross-group correlation)
%The authors use one-vs-rest Pearson's $r$ to assess annotator agreement and report averagely 0.51/0.54 for ROC/WP, respectively with in-house annotators and 0.74/0.64 for crowdworkers.}
The judgment from the first group is used for preliminary exploration of optimal settings, such as assessing the effectiveness perturbation methods and language models (\autoref{subsec:preliminaryexploration}). The judgment of the second group is used for the final comparison of our approach with existing evaluation metrics (\autoref{subsec:comparisonwithexistingmetrics}).

%For that reason, we pool the judgement from the two groups of workers to compute the mean rating of a story for a particular aspect. 
% We also present the results separately for the two groups of annotations in the Appendix XXX for completeness, noting that we find similar insights, thereby providing justification for pooling.


%while more detailed information about the datasets can be found in \autoref{appendix:datasetsdetail}.

% As mentioned in \autoref{sec:deltascore}, \deltascore can use both given story condition and human reference as the context for generative likelihood.
% However, open-ended generation tasks such as story generation often suffer from the known challenge of the one-to-many problem, where multiple plausible stories can be generated for a single prompt. Consequently, reference-based evaluation metrics, which are commonly used in other generation tasks, may not be ideal for evaluating story generation \citep{DBLP:conf/emnlp/Zhong0YMJLZJH22}. Hence, to address this issue, we utilize the given story condition as the conditioning factor $\boldsymbol{c}$ to generate the likelihood. More specifically, we use the first sentence and a short prompt as the condition for ROC and WP, respectively. 

\subsection{Language Models}
\label{subsec:languagemodels}

We select a set of representative PLMs to compute \deltascore. For encoder-decoder PLMs, we use BART and FLAN-T5 \citep{flant5-arxiv22}. For decoder PLMs, we use BLOOM \citep{bloom-arxiv22}, LLaMA \citep{llama-arxiv23}, OPT \citep{opt-arxiv22}, and GPT-3.5.\footnote{We use text-davinci-003 in our experiments.} We use the largest possible variant whenever possible as we found larger models tend to work better in preliminary experiments. We present a summary of these models in \autoref{table:modeldetails}.


\subsection{Compared Evaluation Metrics}
\label{subsec:competitors}
To comprehensively compare \deltascore with other existing evaluation metrics, we select representative evaluation metrics from each of the three categories mentioned in  \autoref{subsec:automaticevaluationmetrics}.

% , classifying them into three categories: 1) \textit{similarity}, where the generated stories are compared to the given references, 2)  \textit{discriminative}, where a classifier is trained to distinguish between high and low quality stories, and 3) \textit{generative}, where generation likelihood is used to determine quality.

For similarity metrics, we run experiments for \textbf{BLEU}, \textbf{BERTScore} and \textbf{MoverScore}. For discriminative metrics, we have \textbf{UNION}, \textbf{MANPLTS}, \textbf{StoryER}, \textbf{CTC} and \textbf{UNIEVAL}.
% In our story evaluation, we ask story quality-related questions for each aspect, which are listed in \autoref{appendix: unieval}.
Additionally, we include a zero-shot GPT-3.5 using simple instructions as prompt to gather judgements for the five specific aspects~\citep{llmeval-acl23}. This approach is referred to as \textbf{GPT3.5Eval}; detailed instructions can be found in \autoref{appendix:gpt3.5instructions}.

Since UNION, MANPLTS and StoryER are all originally designed for story evaluation, we use their released models without fine-tuning for our experiments.
%JHL: what is story comments?
% Answer: They additional generate comments on each aspect as explanations.
For CTC, we use the reference-free alignment approach, which is also called ``consistency'' in the original paper.
For UNIEVAL, the question answering models are trained on text summarization and dialogue generation tasks. 
We modify the questions to adapt UNIEVAL for evaluating different aspects of stories as the authors demonstrate the zero-shot transfer capability.
Please refer to \autoref{appendix:unieval} for our questions.
For generative metrics, we select
\textbf{BARTScore} and \textbf{GPTScore}. We use the reference-free version of BARTScore (i.e., $\boldsymbol{c} \rightarrow \boldsymbol{s}$), and employ text-davinci-003 from OpenAI as the backbone of GPTScore with specific prompts for different story aspects.
Prompts for GPTScore can be found in \autoref{appendix:gptscore}.
% (see  for more details).

% Might delete CTC if we lack space.


% We calculate both CTC (Consistency), which is s $\rightarrow$ c and
% CTC (Relevance), which is the product of alignments r $\rightarrow$ s and s $\rightarrow$ c,
% because the other settings in CTC are not applicable to our tasks.
% Our results show that CTC (Consistency) works consistently better than CTC (Relevance) (see Appendix).
% It shows references are not that important in open-ended story evaluation. 


 
%JHL2: we need to now describe all of these metrics, so that the reader can understand them. I see much of them is already in appendix, so let's move them back to the main narrative (maybe summarise them a bit more so it's a little bit more succinct). Without these descriptions, most of the description in the following paragraph don't make much sense.

%Some of these metrics are not originally proposed for story evaluation, and we try to adapt them to fit our task. For discriminative metrics, we use the models provided by the authors without additional fine-tuning. We use the reference-free version of CTC and BARTScore to ensure consistency in comparisons, namely CTC (Consistency) and BARTScore ($\boldsymbol{c} \rightarrow \boldsymbol{s}$). Our preliminary experiments showed that reference-free versions perform better than reference-based ones. For UNIEVAL and GPTScore, we adapted the questions and prompts accordingly, which can be found in \autoref{appendix:evaluationmetrics}.

We summarise all these metrics in \autoref{table:metricdetails}, showing whether they: require additional training or ground truth reference; are originally introduced for story evaluation; and can measure fine-grained story aspects.

%In this section, we provide an overview of the meta-evaluation datasets used in our experiments as well as the evaluation metrics we compare \deltascore to.



% The details can be found in \Needcite{Appendix}.

% \paragraph{Overall Quality Scores}

% %JHL: given the narrative in the intro, I no longer think we need this set of results (OPENMEVA). [Deleted]
% We also use manually annotated stories from OpenMEVA \citep{guan-etal-2021-openmeva}, which covers ROC and WP.
% It contains various generation models including: 
% 1) a \textbf{Seq2Seq} model \citep{DBLP:conf/nips/SutskeverVL14},
% 2) \textbf{Fusion} \citep{fan-etal-2018-hierarchical},
% 3) \textbf{Plan\&Write} \citep{DBLP:conf/aaai/YaoPWK0Y19},
% 4) fine-tuned \textbf{GPT-2} \citep{radford2019language} and
% 5) \textbf{KGGPT2}.
% % They randomly sample 200 stories from test sets of ROC and WP for story generation,
% % therefore,
% % MANS contains 2 $\times$ 200 $\times$ 5 = 2,000 annotated stories.
% % They use Amazon Mechanical Turk (AMT)\footnote{\url{https://requester.mturk.com/}} for human judgements and
% They ask the annotators to rate each story with a 5-point Likert scale in terms of the overall quality.\footnote{
% The authors of OpenMEVA also asked annotators to select the types of errors in the story, such as
% \textit{repetitive plots},
% \textit{unrelated events},
% \textit{conflicting logic},
% or \textit{chaotic scenes}.
% However, these error types have not bee released and as such we only use overall quality labels in this work.}

% We randomly sample 20 conditional contexts (e.g.,\ titles) from each dataset 
% and collect stories generated by all models for human evaluation.
% Each story (including human-written one) is judged by 3 annotators, 
% and so we have 320 annotated stories in total (140/100/80 for ROC, WP and CNN, respectively).

% Recently, \citet{DBLP:conf/coling/ChhunCSC22} propose
% a new story evaluation benchmark, HANNA.
% However, they present both human reference and generated story to annotators in AMT
% and ask annotators to judge the generated story.
% We concern that such way can make annotators give higher scores to stories that
% ``reads similar'' to given human reference and thus,
% favoring reference-based evaluation metrics and their results do show
% reference-based evaluation metrics have a better performance on HANNA.
% Therefore, we do not include HANNA as another baseline for GPTScore.

% \subsection{Compared Evaluation Metrics}

\begin{table}[t]
\centering
\small
\begin{tabular}{p{1.0cm}p{1.3cm} p{0.6cm} p{0.9cm} p{1.5cm}}
% \begin{tabular}{cllll}
\toprule
\textbf{Arch.} & \textbf{Model} & \textbf{Size} & \textbf{\#Data} & \textbf{Objectives}  \\
\midrule
 \multirow{3}{2cm}[0.5ex]{En-De} & BART & 406M & 160GB & Denoising \\
 \cmidrule{2-5}
  &  FLAN-T5  & 11B & - & Denoising \\
 \midrule
\multirow{4}{2cm}[-2.5ex]{De} & BLOOM & 7B & 366BT & LM \\
\cmidrule{2-5}
 & LLaMA &  65B & 1.4TT & LM \\
\cmidrule{2-5}
 & OPT &  66B & 180BT & LM \\
 \cmidrule{2-5}
 & GPT-3.5 & 175B & 300BT & LM \\
 \bottomrule
\end{tabular}
\caption{ Summary of PLMs, classified by their architecture (Arch.) as  encoder-decoder (En-De) or decoder (De). ``Size'' indicates model parameters.
\#Data indicates the pre-trained data scale (``GB'' $=$ ``gigabyte''; ``BT'' $=$ ``billion tokens''; and ``TT'' $=$ ``trillion tokens''). 
 ``LM'' indicates causal language modeling objective.
}
\label{table:modeldetails}
\end{table}

\begin{table}[t]
\centering
% \begin{adjustbox}{max width=\linewidth}
\small
% \begin{tabular}{p{1.8cm} p{1.5cm} p{1.0cm} p{1.0cm} }
\begin{tabular}{p{1.7cm}lcccc}
\toprule
 \textbf{Objective} & \textbf{Metric} & \textbf{FT}  & \textbf{B/F} & \textbf{ST} & \textbf{MS}   \\
\midrule
  \multirow{3}{2cm}[-1ex]{Similarity} & BLEU & \xmark & B & \xmark & \xmark \\
 \cmidrule{2-6}
 & BERTScore & \xmark &  B & \xmark & \xmark \\
 \cmidrule{2-6}
 & MoverScore & \xmark & B & \xmark & \xmark \\
 \midrule
  \multirow{3}{1cm}[-5ex]{Discriminative} & UNION & \cmark & F & \cmark & \xmark \\
 \cmidrule{2-6}
  & MANPLTS & \cmark & F & \cmark & \xmark \\
 \cmidrule{2-6}
 & StoryER & \cmark & F &\cmark & \xmark \\
 \cmidrule{2-6}
  & CTC & \cmark & F & \xmark & \cmark \\
 \cmidrule{2-6}
 & UNIEVAL & \cmark  & F & \xmark  & \cmark \\
 \midrule
 \multirow{2}{1.5cm}[-0.5ex]{Generative} & BARTScore & \xmark & F & \xmark & \cmark \\
 \cmidrule{2-6}
 & GPTScore & \xmark & F & \xmark & \cmark \\ 
 \midrule
 N/A & GPT3.5Eval & \xmark & F & \xmark & \cmark \\
 \bottomrule
\end{tabular}
% \end{adjustbox}
\caption{Statistics of compared evaluation metrics. 
``FT'' indicates whether the metric requires additional synthetic data to fine-tune on.
``B/F'' indicates whether the metric is reference-based (B) or reference-free (F).
``ST'' indicates whether the metric is originally designed for story evaluation.
``MS'' indicates whether the metric produces scores that consider multiple aspects.
}
\label{table:metricdetails}
\end{table}
%JHL3: I updated CTC and Bartscore to mark them as referencce-free (F) in the table, because when we adapt them to our task we don't use the reference. Please correct if it's wrong
%Zhuohan: yes, it is correct that we only use the reference-based vestion of CTC and Bartscore in the paper, but I just wonder if it is necessary for us to note that these methods can be used both as reference-based and reference-free, while we only adopt reference-free one in our setting.





\begin{table*}[t]
\small
\centering
\begin{tabular}{clcccccccccc}
\toprule
\multirow{2}{*}[-1ex]{\textbf{Target Aspect}} & \multirow{2}{*}[-1ex]{\textbf{Perturbation}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
\cmidrule(lr){3-7}\cmidrule(lr){8-12}
 & & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} \\
\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}
N/A & w/o perturbation & 17.3 & 31.0 & 22.8 & 35.2 & 20.2 & 26.8 & 27.0 & 32.1 & 34.6 & 17.8 \\ 
\midrule
\multirow{2}{*}[0ex]{Fluency} & Typo & 14.2 & \cellcolor{green!25}31.9 & \cellcolor{green!25}23.5 & \cellcolor{green!25}36.0 & \cellcolor{green!25}23.2 & \cellcolor{green!25}36.5 & \cellcolor{green!25}42.9 & \cellcolor{green!25}38.1 & \cellcolor{green!25}47.8 & \cellcolor{green!25}35.5 \\
& SubjVerbDis & 5.5 & 13.9 & 8.8 & 13.0 & 7.8 & 13.0 & 15.9 & 4.7 & 10.7 & 6.1  \\
\midrule
\multirow{2}{*}[0ex]{Coherence} & Jumble & \cellcolor{green!25}17.6 & \cellcolor{green!25}37.1 & 16.8 & 34.1 & \cellcolor{green!25}22.5 & \cellcolor{green!25}35.9 & \cellcolor{green!25}36.0 & 31.9 & \cellcolor{green!25}36.3 & \cellcolor{green!25}28.1   \\
& SentReorder & 4.4 & 17.7 & 4.8 & 20.3 & 8.9 & 18.6 & \cellcolor{green!25}27.3 & 20.5 & 15.1 & \cellcolor{green!25}25.4 \\ 
\midrule
\multirow{2}{*}[0ex]{Relatedness} & RmRelWords & 14.7 & 30.9 & \cellcolor{green!25}25.6 & 32.6 & \cellcolor{green!25}21.7 & 7.1 & \cellcolor{green!25}31.9 & \cellcolor{green!25}35.4 & 32.8 & 16.1 \\
& StoryReplace & 1.8 & 7.0 & 16.0 & 21.3 & 8.6 & \cellcolor{green!25}28.5 & 26.6 & 27.6 & \cellcolor{green!25}38.5 & \cellcolor{green!25}23.8 \\ 
\midrule
\multirow{2}{*}[0ex]{Logicality} & Antonym & 14.4 & 16.9 & 11.4 & 16.2 & 11.6 & \cellcolor{green!25}31.6 & \cellcolor{green!25}33.5 & \cellcolor{green!25}35.0 & \cellcolor{green!25}35.0 & \cellcolor{green!25}25.3  \\
& Commonsense & \cellcolor{green!25}20.1 & 27.1 & 20.7 & 34.9 & \cellcolor{green!25}21.4 & 8.5 & 16.0 & 12.3 & 17.4 & 8.8 \\
\midrule
Interestingness & BlanderNarrative & 8.1 & 19.3 & 2.1 & 15.1 & 3.9 & 14.7 & 13.3 & 8.4 & 18.1 & 9.5 \\
 \bottomrule
\end{tabular}
\caption{Story-level Kendall correlation ($|\tau|$) between \deltascore with LLaMA and in-house judgements. 
We \colorbox{green!30}{highlight}  instances where \deltascore outperforms vanilla likelihood (``w/o perturbation'').}
\label{table:multiperturbations_inhouse}
\end{table*}



%JHL2: add another column to show which aspect a perturbation targets, and added some lines to seperate the different types of perturbations
% Zhuohan: Addressed


%JHL: ahhhh i am a little surprised to see the CNN story condition is human story here! This is not good, the prompt qualitatively is far too different from the other domains (i.e. in ROC and WP the story condition is a short text, but the CNN story condition has a lot more information and so there's less room in terms of 'creative writing' for the model. This may be the reason why CNN/Dailymail has very very different results! We might need to redo CNN experiments. Also, I suspect we'll run into issues with reference-based evaluation for CNN/dailymail (since the reference is only used as story condition)
%JHL: prompt is never really defined (I understand it's introduced in section 3 but it wasn't really defined there too); suggest we say explicitly what are the c's for the different domains here (e.g. ROC = first sentence; WP = a short prompt that gives the big picture of the whole story; etc and show some examples).


%We show the classification in \autoref{table:metricdetails}, and details about the evaluation metrics can be found in \autoref{appendix:evaluationmetrics}.


\begin{figure}[t]
     \centering
     \includegraphics[width=0.48\textwidth]{figs/line_chart_ih.pdf}
        \caption{
        Impact of perturbation degree with LLaMA on in-house judgements for measuring coherence.
        }
        \label{fig:correlations}
\end{figure}

\begin{table*}[t]
\small
\centering
\begin{tabular}{lcccccccccc}
\toprule
\multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
\cmidrule(lr){2-6}\cmidrule(lr){7-11}
  & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} \\
\cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
\multicolumn{10}{l}{\textbf{\deltascore (with BART-large 406M)}}\\ 
w/o perturbation & 4.5 & 14.6 & 11.6 & 14.5 & 2.3 & 15.0 & 14.5 & 22.4 & 24.5 & 10.6  \\ 
 Jumble & \cellcolor{green!25}{11.4} & \cellcolor{green!25}{21.9} & \cellcolor{green!25}{14.6} & \cellcolor{green!25}{19.5} & \cellcolor{green!25}{13.1} & \cellcolor{green!25}{30.0} & \cellcolor{green!25}{21.6} & \cellcolor{green!25}{27.3} & \cellcolor{green!25}{33.6} & \cellcolor{green!25}{21.2} \\
 Typo & \cellcolor{green!25}{6.3} & \cellcolor{green!25}{17.7} & \cellcolor{green!25}{19.8} & \cellcolor{green!25}{17.5} & \cellcolor{green!25}{4.7} & \cellcolor{green!25}{24.5} & \cellcolor{green!25}{26.9} & \cellcolor{green!25}{30.2} & \cellcolor{green!25}{36.6} & \cellcolor{green!25}{24.3} \\
 Antonym & \cellcolor{green!25}{5.7} & 8.3 & 7.3 & 5.4 & \cellcolor{green!25}{6.9} & \cellcolor{green!25}{28.0} & \cellcolor{green!25}{26.4} & \cellcolor{green!25}{37.6} & \cellcolor{green!25}{31.8} & \cellcolor{green!25}{25.7} \\
\midrule
 \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL 11B)}}\\ 
  w/o perturbation & 16.2 & 19.4 & 14.7 & 23.1 & 8.9 & 19.9 & 14.1 & 23.4 & 20.8 & 4.8 \\
 Jumble & \cellcolor{green!25}{24.5} & \cellcolor{green!25}{36.1} & \cellcolor{green!25}{20.2} & \cellcolor{green!25}{27.9} & \cellcolor{green!25}{20.6} & \cellcolor{green!25}{31.7} & \cellcolor{green!25}{28.3} & 22.1 & \cellcolor{green!25}{34.4} & \cellcolor{green!25}{21.8} \\
 Typo & 11.3 & \cellcolor{green!25}{21.8} & 11.2 & \cellcolor{green!25}{28.0} & \cellcolor{green!25}{14.7} & \cellcolor{green!25}{29.0} & \cellcolor{green!25}{25.7} & \cellcolor{green!25}{33.1} & \cellcolor{green!25}{31.8} & \cellcolor{green!25}{18.0} \\
 Antonym & 10.7 & 12.9 & 8.9 & 10.1 & 7.9 & \cellcolor{green!25}{33.1} & \cellcolor{green!25}{34.0} & \cellcolor{green!25}{30.6} & \cellcolor{green!25}{34.2} & \cellcolor{green!25}{29.2} \\
\midrule
\multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
  w/o perturbation & 12.4 & 24.1 & 13.3 & 25.5 & 10.2 & 24.8 & 21.3 & 27.9 & 29.2 & 12.6 \\
 Jumble & \cellcolor{green!25}{25.0} & \cellcolor{green!25}{35.5} & \cellcolor{green!25}{14.2} & \cellcolor{green!25}{33.9} & \cellcolor{green!25}{25.4} & \cellcolor{green!25}{34.2} & \cellcolor{green!25}{34.8} & \cellcolor{green!25}{30.7} & \cellcolor{green!25}{33.6} & \cellcolor{green!25}{23.1} \\
 Typo & \cellcolor{green!25}{14.3} & \cellcolor{green!25}{31.6} & 12.2 & \cellcolor{green!25}{30.3} & \cellcolor{green!25}{14.5} & \cellcolor{green!25}{38.4} & \cellcolor{green!25}{40.9} & \cellcolor{green!25}{38.7} & \cellcolor{green!25}{44.7} & \cellcolor{green!25}{29.5} \\
 Antonym & 10.2 & 12.0 & 9.4 & 10.3 & \cellcolor{green!25}{12.2} & \cellcolor{green!25}{27.4} & \cellcolor{green!25}{31.5} & \cellcolor{green!25}{31.3} & \cellcolor{green!25}{32.9} & \cellcolor{green!25}{30.6} \\
\midrule
\multicolumn{10}{l}{\textbf{\deltascore (with LLaMA 65B)}}\\ 
w/o perturbation & 17.3 & 31.0 & 22.8 & 35.2 & 20.2 & 26.8 & 27.0 & 32.1 & 34.6 & 17.8 \\
 Jumble & \cellcolor{green!25}{19.7} & \cellcolor{green!25}{36.9} & 15.8 & \cellcolor{green!25}{35.9} & \cellcolor{green!25}{23.0} & \cellcolor{green!25}{36.8} & \cellcolor{green!25}{35.3} & \cellcolor{green!25}{32.9} & \cellcolor{green!25}{35.5} & \cellcolor{green!25}{27.7} \\
 Typo & 14.2 & \cellcolor{green!25}{31.9} & \cellcolor{green!25}{\textbf{23.5}} & \cellcolor{green!25}{36.0} & \cellcolor{green!25}{23.2} & \cellcolor{green!25}{34.3} & \cellcolor{green!25}{42.2} & \cellcolor{green!25}{35.2} & \cellcolor{green!25}{44.0} & \cellcolor{green!25}{31.8} \\
 Antonym & 13.3 & 15.9 & 10.6 & 14.2 & 11.0 & \cellcolor{green!25}{31.3} & \cellcolor{green!25}{37.9} & \cellcolor{green!25}{35.0} & \cellcolor{green!25}{36.2} & \cellcolor{green!25}{31.6} \\
\midrule
\multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
  w/o perturbation & 17.4 & 30.6 & 20.2 & 32.6 & 15.8 & 27.5 & 24.5 & 32.2 & 31.0 & 15.7 \\
 Jumble & \cellcolor{green!25}{\textbf{27.4}} & \cellcolor{green!25}{\textbf{44.2}} & \cellcolor{green!25}{21.6} & \cellcolor{green!25}{\textbf{41.4}} & \cellcolor{green!25}{\textbf{30.0}} & \cellcolor{green!25}{37.9} & \cellcolor{green!25}{38.7} & \cellcolor{green!25}{35.4} & \cellcolor{green!25}{39.5} & \cellcolor{green!25}{32.0} \\
 Typo & \cellcolor{green!25}{17.9} & \cellcolor{green!25}{39.6} & \cellcolor{green!25}{21.3} & \cellcolor{green!25}{38.7} & \cellcolor{green!25}{22.8} & \cellcolor{green!25}{39.0} & \cellcolor{green!25}{41.9} & \cellcolor{green!25}{\textbf{38.0}} & \cellcolor{green!25}{\textbf{45.6}} & \cellcolor{green!25}{30.4} \\
 Antonym & 15.8 & 19.0 & 13.8 & 14.9 & 12.9 & \cellcolor{green!25}{34.0} & \cellcolor{green!25}{40.3} & \cellcolor{green!25}{36.0} & \cellcolor{green!25}{39.4} & \cellcolor{green!25}{36.7} \\
\midrule
 \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5 175B)}}\\
 w/o perturbation & 21.3 & 31.2 & 18.7 & 28.6 & 18.2 & 36.2 & 32.9 & 36.0 & 37.1 & 23.3 \\
 Jumble & 18.7 & \cellcolor{green!25}{34.0} & \cellcolor{green!25}{23.3} & \cellcolor{green!25}{38.5} & \cellcolor{green!25}{29.7} & 35.0 & \cellcolor{green!25}{36.6} & \cellcolor{green!25}{36.1} & \cellcolor{green!25}{37.7} & \cellcolor{green!25}{33.5} \\ 
 Typo & 17.0 & \cellcolor{green!25}{35.1} & 11.7 & \cellcolor{green!25}{32.2} & \cellcolor{green!25}{26.3} & \cellcolor{green!25}{\textbf{41.7}} & \cellcolor{green!25}{\textbf{46.7}} & \cellcolor{green!25}{37.5} & \cellcolor{green!25}{45.0} & \cellcolor{green!25}{\textbf{37.3}} \\ 
 Antonym & 19.1 & 21.5 & 12.9 & 20.5 & 13.9 & 36.1 & \cellcolor{green!25}{41.2} & \cellcolor{green!25}{41.3} & \cellcolor{green!25}{39.6} & \cellcolor{green!25}{38.2} \\
 \bottomrule
\end{tabular}
\caption{Absolute value of Story-level Kendall correlation ($|\tau|$) between different metrics and in-house judgements. 
We \textbf{bold} the best scores for each aspect and \colorbox{green!30}{highlight} instances where \deltascore improves over vanilla likelihood (``w/o perturbation'').
}
\label{table:multiplemodels_inhouse}
\end{table*}

\begin{figure*}[t]
     \centering
     \includegraphics[width=0.9\textwidth]{figs/bar_chart.pdf}
        \caption{
        Absolute value of Story-level Kendall correlation ($|\tau|$) between different metrics and crowdworker ratings.
        % Similarity, Discriminative and Generative indicate the best performing evaluation metrics from such category.
        Higher bar indicates better performance.
        Red bars indicate \deltascore.
        Blue bars indicate similarity based metrics.
        Green bars indicate discriminative metrics.
        Purple bars indicate generative metrics.
        Gray bar indicates GPT3.5Eval.
        }
        \label{fig:comparisons}
\end{figure*}
%JHL3: need to use different colours now that we categorise the competitor metrics differently
%JHL3: let's add BARTscore here too, so we have 2 metrics for generative metrics
%Zhuohan: I just addressed this part.



% In order to test the generalization of our proposed approach, we extend it to other generative LLMs. In addition to the OPT and GPT-3.5 models that we demonstrated previously, we also include two representatives of encoder-decoder models, namely BART  as well as one more causal decoder model, .

%In our preliminary experiment, we observed that larger models tend to demonstrate better performance within the same model framework. As such, we applied the largest model size that we could run for each model. The model details are presented in \autoref{table:modeldetails}.


%JHL2: we need a section to talk about all the different LMs we use (and if there's some rationale in how we select them, spell them out too). In terms of models, I think we need GPT3.5, and maybe 2 open-source siblings of the same size (say OPT and LLAMA), and then 2  smaller models, like FLAN-T5 and vanilla BART (not the CNN finetuned one, although I vaguely the vanilla BART has some strange results, in which case maybe just drop BART entirely). Here remember to talk about whether they are encoder-decoder or decoder models, their training data, and also whether they have been instruction tuned.
% Zhuohan: Half Addressed, We might need to leave BART here as BART is more frequently used in evaluation metrics, while maybe we can delete BLOOM if we need space.
% Zhuohan: Might need to cite more papers to show that these models obtain very strong capabilities in other people' benchmarks.

% While some of these metrics were developed for other NLG tasks, we modify them slightly to make them suitable for evaluating story generation.
% We also indicate if the metric requires additional synthetic data to fine-tune the backbone model on and whether it requires human references when computing the scores.

% We compare \deltascore against representative evaluation metrics introduced in \autoref{sec:relatedwork}.




\section{Results}

We evaluate Kendall correlation at the story level, which involves comparing the predicted metric score versus the aggregated human rating for each story on a specific aspect. We use this as our primary metric due to the non-linear relationship between automatic and human metrics, as well as the ordinal scale employed in human judgments \citep{kentalltau-biometrika38}.
We explore different settings of our approach in \autoref{subsec:preliminaryexploration} and present a comparison of our best approach with existing evaluation metrics in \autoref{subsec:comparisonwithexistingmetrics}.
Note that we use two different set of judgments,
% (in-house ratings for \autoref{subsec:preliminaryexploration} and crowdworker ratings for \autoref{subsec:comparisonwithexistingmetrics}
as explained in \autoref{subsec:benchmark}, to avoid tuning and testing on the same test set.
%To demonstrate generalization of our approach,
%we use two sets of benchmarks mentioned in \autoref{subsec:benchmarksandcomparedapproaches}, inhouse to explore impacts of different parameters (\autoref{subsec:parameterimpacts}) and crowdsource part to compare with existing SOTA metrics (\autoref{subsec:comparisonwithexistingmetrics}).

%JHL2: suggest we re-structure section 5 a bit better.
%JHL2: talk about the results here, saying that very interestingly, jumble, typo and antonym seem to consistently work better than the rest

\subsection{Preliminary Exploration}
\label{subsec:preliminaryexploration}

\paragraph{Perturbation Methods}
We commence by showcasing the comparative performance of various perturbation methods (\autoref{subsec:perturbation}) in relation to human judgments across the five aspects, as demonstrated in \autoref{table:multiperturbations_inhouse}. For this analysis, we employ LLaMA as the PLM. The notation ``w/o perturbation'' denotes the calculation of story likelihood directly under LLaMA, without any perturbations applied.
%We employ LLaMA here as it is the latest open sourced LLM at the time of our experiment, and it has demonstrated the predominant performance in various downstream tasks, outperforming other LMs \citep{DBLP:journals/corr/abs-2302-13971}.
%We present the complete set of results in \autoref{table:multiperturbations_inhouse}, and visualize the performance gains of \deltascore over LLaMA in \autoref{appendix: correlations}.
%JHL3: let's wait until we have table 5 before we write any findings
Our findings revealed intriguing results. Notably, we observed that perturbations specifically designed to target a particular aspect did not consistently exhibit a higher correlation with human judgments for that aspect.
% (i.e.\  we don't see diagonal green highlights within a story domain). 
Furthermore, our analysis indicates that measuring interestingness is particularly challenging, as the correlation numbers associated with this aspect are generally lower compared to the other aspects.
Finally, our last and perhaps most surprising observation is that a small set of perturbation methods, namely \textit{Typo}, \textit{Jumble}, and \textit{Antonym}, exhibit strong performance in evaluating most aspects. 
%\hl{We carried out the most intense perturbation for \textit{Jumble} and \textit{Antonym}, in which the perturbation was applied to the entire sentence. As for \textit{Typo}, our approach was more conservative, involving the random selection of half of the characters for swapping with their adjacent counterparts. We undertake experiments to identify the most appropriate parameters in the subsequent step.}
%JHL5: don't really need this; we just need to come back in the next section to spell out the hyper-parameters.

%some perturbations work better than others, despite their targeting of specific aspects of story quality. For example, sentenceReorder does not provide any improvement on \deltascore, possibly due to the focus of LLMs on local logic rather than the global narrative arc of a story. As such, reordering sentences, which does not affect the local logic, is not helpful in enhancing \deltascore. Furthermore, we observe that interestingness, the most subjective aspect, is the most challenging for \deltascore to improve, as designing a perturbation that targets it is difficult.



\paragraph{Perturbation Degree} %JHL2 here we talk about figure 4. Here we should be consistent with the previous paragraph to use the same LM.
In the preceding phase, we carried out the most intense perturbation for \textit{Jumble} and \textit{Antonym}, in which the perturbation was applied to the entire sentence, and random selection of half of the characters for \textit{Typo}.
In light of their strong performance, we now investigate impact of perturbation degree using the these perturbation methods and present the results over ROC and WP in \autoref{fig:correlations}.
In the case of \textit{Typo}, the degree pertains to the percentage of characters that we opt to swap. Concerning \textit{Jumble}, we shuffle tokens within a certain text span while the span length is controlled by the degree. As for \textit{Antonym}, we replace the token with its antonym under the specified probability (degree).
As before, we use LLaMA as the PLM
and focus on evaluating {coherence} here.
Interestingly, \textit{Typo} appears to be relatively stable and unaffected by the perturbation degree, where else \textit{Jumble} and \textit{Antonym} work better with more aggressive perturbation. Based on these results, we set the perturbation degree to 0.4, 0.9, and 0.8 for \textit{Typo}, \textit{Jumble}, and \textit{Antonym} respectively for both ROC and WP.\footnote{The results in the previous subsection (\autoref{table:multiperturbations_inhouse}) use these perturbation values.}

%JHL3: what aspect did we focus here?
%Zhuohan: I have indicated that we look at coherence aspect here.
%It appears that jumble and antonym do not offer significant improvement when the degree of perturbation is minor, but they exhibit a substantial increase in performance when more words are jumbled, ultimately reaching a stable value. In contrast, typo appears to perform more reliably, regardless of the degree of perturbation. We hypothesize that the language model may not be highly sensitive to small changes in word order, given its masked word infilling objective during training. However, it consistently responds to typos in words, potentially due to the resulting difference in embeddings caused by tokenization.
%JHL3: the explanation wasn't super convincing, so i think it's better that we just treat it as an empirical observation
%Zhuohan: I agree.

\paragraph{Language Models} %JHL2: here we talk about table 6. We should mention that we use the same perturbation degree value for all LMs here (derived from the previous paragraph).

We next present \deltascore results using different PLMs in \autoref{table:multiplemodels_inhouse}. We use the top 3 performing methods with the optimal degrees determined in our previous analysis. 
Encouragingly, across different PLMs and story aspects, we see that \deltascore outperforms vanilla likelihood (``w/o perturbation'') in almost all instances, suggesting that measuring story quality using \textit{likelihood difference} is generally a better approach than using its \textit{likelihood} directly. 
Broadly speaking, \textit{Jumble} is the most consistent perturbation method: in ROC it is the consistently the best performer, while in WP it is either the best or second best performer, depending on the PLM.
This observation aligns with the findings presented in \autoref{table:multiperturbations_inhouse}, providing further confirmation that the \textit{Jumble} perturbation method demonstrates effectiveness in measuring various story aspects.
When examining the correlation magnitudes for different story aspects, it is evident that interestingness consistently exhibits lower values, reaffirming its inherent difficulty in measurement.
There are, however, some curious exceptions: in ROC the correlation for fluency and relatedness is particularly low. We do not have a strong hypothesis of these observations, but will note that the language of ROC stories are somewhat formulaic and possibly different to the language of the pre-training data. 
For relatedness, the story condition in ROC is the first sentence,
and it is a rather artificial condition to set the ``topic'' for story generation.

% (from the 5-sentence human-written stories), 


%It has been observed that \deltascore, when applied with the three perturbations, demonstrates a strong performance across all generative models, irrespective of their varying model architectures. In general, it offers a better evaluation metric compared to directly applying likelihood. Among the three perturbations, jumble appears to be the most effective, providing substantial improvement in nearly all aspects across the two datasets. As a result, jumble yields the best-performing \deltascore for each aspect.

%It should be noted that \deltascore appears to exhibit better performance on WP as compared to ROC. Our speculation is that this can be attributed to the fact that the stories in WP are significantly more intricate in nature, providing more opportunities for the perturbations to target a greater number of aspects. Consequently, this complexity can potentially provide a benefit to \deltascore.

An unsurprising observation is that larger models tend to exhibit stronger correlations, with GPT-3.5 and OPT performing the best among the PLMs. BLOOM and FLAN-T5 fall in the middle range, while BART shows the lowest correlation scores.
Upon comparing GPT-3.5 and OPT, we observe a slight advantage for OPT despite its smaller model size and pre-training data. This finding suggests that beyond a certain scale, the benefits of further scaling may become less significant.

%However, this trend is not always consistent, as evidenced by our findings. Specifically, in this study, OPT emerged as the top-performing model, being the best in 6 out of 10 aspects, while GPT-3.5 won only 3 out of 10 aspects, despite being 2.6$\times$ larger and trained on 1.7$\times$ more tokens of data. This observation aligns with previous research indicating that larger models do not necessarily guarantee better performance.

\subsection{Comparison with Other Metrics}
\label{subsec:comparisonwithexistingmetrics}
%JHL2: here we talk about figure 2. Ideally I think we should present *all* the existing metrics, see if it's still legible if you include a few more bars in the plot. I think this bar plot is easier to read than table, so I kind of like it.



%JHL2: one danger of the current narrative is that in a way we are 'tuning' against the test data, i.e. we select which perturbation methods, perturbation degree, and LM works best in the test, before comparing it against other models. Maybe another way we can do this is to use one worker type as dev (e.g. inhouse; so the first 3 paragraphs will show results using this judgement) and the other as test (e.g. crowdworker; and so the 4th paragraph will show results using this judgement).

We next compare \deltascore with other evaluation metrics in \autoref{fig:comparisons}.
Note that in this comparison, we utilize OPT as the chosen PLM, considering its superior performance, along with the same top-performing perturbation methods.
% For fair comparison, we use a different set of ground truth human ratings (crowdworker ratings) for these results.
 The results of our evaluation are very promising: \deltascore consistently outperforms all competitor metrics across all story aspects. \textit{Jumble} stands out as the most effective perturbation method among the three.
The similarity metrics generally has the lowest performance, highlighting the inadequacy of reference-based metrics for story evaluation, which aligns with previous research findings \citep{union-emnlp20, thenextchapter-inlg23}. 
Among the discriminative metrics, CTC and UNIEVAL show relatively strong competitiveness, although they still fall behind \deltascore.
% In particular, we observe that CTC tends to perform better in WP, while UNIEVAL shows better performance in ROC.
The performance of generative scores is inconsistent. GPTScore shows strong performance in evaluating logicality and interestingness, especially in ROC, where it performs similarly to \deltascore. However, its effectiveness is limited in other scenarios.
More detailed scores can be found in \autoref{table:crowdsource_kendall_all}.
%JHL5: can we say a bit more what new results this table gives?

%We now use \deltascore equipped with OPT, as it demonstrates the best performance in previous steps in crowdsouce part.
%We compare it to metrics presented in \autoref{table:metricdetails}, and the results are shown in \autoref{fig:comparisons}. 
% Further analysis of perturbation effects is presented in \autoref{subsec:perturbationselections} and model effects in \autoref{subsec:modelselection}. 
%Detailed information can be found in \autoref{appendix: correlations}.

%Our findings suggest that similarity evaluation metrics such as BLEU, BERTScore, and MoverScore exhibit poor performance in open-ended story evaluation, which is consistent with previous studies \citep{guan-huang-2020-union, DBLP:journals/corr/abs-2301-09790}. This implies that reference is not necessary in assessing open-ended stories.
%Interestingly, discriminative evaluation metrics do not demonstrate impressive results despite being designed specifically for story evaluation. This may be due to their extensive training on synthetic data and not on our data, which has different features that could affect their performance in our evaluation scenarios.
%Finally, we observe that \deltascore significantly outperforms other metrics, particularly with the jumble perturbation, achieving the best results in all aspects of both datasets.

% We demonstrate \deltascore and observe that jumble and typos produce the most stable results across different models and datasets. Notably, when we use jumble as the perturbation for \deltascore with GPT-3.5, we observe a significant improvement over using the likelihood from GPT-3.5 directly as the metric. Moreover, this approach outperforms all other SOTA evaluation metrics on most aspects across all datasets.
% Replacing the entire story does not improve the performance of \deltascore over directly using the likelihood metric with GPT-3.5. However, it does provide some improvement for BART-cnn and a substantial improvement for BART-large. This may be due to the fact that stronger models are more focused on local context and less sensitive to changes in the prompt.
% Perturbing stories to violate commonsense using ChatGPT does not lead to a significant improvement in the performance of \deltascore compared to using the likelihood metric directly with GPT-3.5, except for in the WP dataset. However, it appears to provide some improvement for BART-cnn and BART-large. This may be due to the fact that when the perturbed stories violate commonsense, GPT-3.5 exhibits similar characteristics to ChatGPT, which results in high likelihood scores being assigned to the contents generated by ChatGPT.

% In general, the quality aspects of stories are heavily intertwined, meaning that perturbations targeting one aspect often impact others as well. Nonetheless, we find that interestingness is particularly challenging to address with perturbations, as it is difficult to identify suitable modifications that effectively target this aspect.


% \subsection{Overall Quality Evaluation Results}

%JHL: think we can drop this section; see my comments below how to handle each subsection
% \section{Analysis}

% \subsection{Generalization}

%JHL: we can drop most of the text here if we introduce DELTASCORE in a general manner previously; we just need to say we will experiment with both GPT3 and BART in the introduction of section 4. Might need to say a bit more about the exact implementation of BART that we use in section 4 (looks like there's BART-para, BART-CNN, etc)
% To test generalization, we also apply \deltascore on BART, note that BART is different from GPT-3.5 as in GPT-3.5 is a pure decoder based model while BART is an encoder-decoder based model.
% Note that, when we utilize the generative likelihood directly from BART to evaluate the story, it is the same as BARTScore \citep{DBLP:conf/nips/YuanNL21}.
% We replace generative language model of \deltascore to BART-para\footnote{The authors from \citet{DBLP:conf/nips/YuanNL21} fine-tunes BART-cnn model on para dataset \Needcite{para}} to explore if the idea can improve BARTScore.  

% We present results in, from which we can see that \deltascore can yield consistent gains in performances in most aspects and overall quality, which shows the idea of differentiating perturbations can also be applied to encoder-decoder architecture.


%JHL2: most of the text in this section will be moved to section 4 with the new structure
% \section{More Analysis}

% \subsection{Perturbations for Different Aspects}
% \label{subsec:perturbationselections}






%JHL: Here we should talk about the language models we will use, e.g. GPT-3 (text-davinci-003) and BART (size?). 
%JHL: we also need to say a bit more about the implementation of the different perturbations, e.g. how are word shuffling done? we select N% words in the text and randomly shuffle them, or how many typos we create, and how do we create them.  The high level idea of perturbation is explained in 3.2, but here we need to spell out a bit more explicitly how they are actually implemented. If space is tight, we can move this to appendix, but right now it's unclear how the perturbations are implemented.







% \subsection{Extending to Other Generative LLMs}
% \label{subsec:modelselection}

% In order to test the generalization of our proposed approach, we extend it to other generative LLMs. In addition to the OPT and GPT-3.5 models that we demonstrated previously, we also include two representatives of encoder-decoder models, namely BART and FLAN-T5 \citep{DBLP:journals/corr/abs-2210-11416}, as well as one more causal decoder model, BLOOM \citep{Scao2022BLOOMA1}.

% In our preliminary experiment, we observed that larger models tend to demonstrate better performance within the same model framework. As such, we applied the largest model size that we could run for each model. The model details are presented in \autoref{table:modeldetails}, while the results are shown in \autoref{table:multiplemodels}.





% \subsection{Influence of Perturbation Degrees}
% \label{subsec:PerturbationDegree}

%JHL: This is the only additional analysis in this section, we can either move this to a subsection in 5 (e.g. 5.X additional analysis) or to appendix if we need space


% \subsection{Model Preference}
% %JHL: we can drop this now since we'll be discussing BART results together with GPT3 in section 5
% We find different models have their different preferences.
% BART-cnn is better at stories generated from older models, while BART-para is better at stories generated recently even though the authors claim that BART-para has better capability due to the extra fine-tuning process.

% \subsection{Assessment Granularity}
% %JHL: Not really sure the objective of this analysis; are we trying to explain why certain metrics don't work (if so, which metric?)
% One of the mainstreams of automatic story evaluation is to train a binary classifier to distinguish good and bad stories \citep{guan-huang-2020-union, ghazarian-etal-2021-plot}.

% However, the nature of binary classifiers can easily result in an evaluator that mainly produce scores close to its labels, usually 0 and 1, while \deltascore utilize the essence of generative language models, therefore it can produce more continuous values that covers wider ranges (see \autoref{fig:binaryclassification}).

% continuous values are more desirable as they not only show which story is better, but also shows the granularity of which it is better even though they might have similar Spearman correlations.



% \begin{figure*}[t]
%      \centering
%      \begin{subfigure}[b]{0.31\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/corr_union-model.ckpt.pdf}
%          \caption{Perturbation ``Remove punctuation'' is applicable to higher quality story but not applicable to lower quality one.
%          }
%          \label{fig:example1}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.31\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/corr_bartscore_CNN_PS_ts1.pdf}
%          \caption{Perturbation ``Add typos'' is applicable to both stories, higher quality story is more affected than lower quality one.
%          % Higher quality story is affected more than lower quality one.
%          }
%          \label{fig:examples2}
%      \end{subfigure}
%      \begin{subfigure}[b]{0.31\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/corr_gpt3score_PS_mean_lp.pdf}
%          \caption{Perturbation ``Add typos'' is applicable to both stories, higher quality story is more affected than lower quality one.
%          % Higher quality story is affected more than lower quality one.
%          }
%          \label{fig:examples3}
%      \end{subfigure}
%         \caption{
%         The correlation figures show that binary classification approaches can only generate scores that close to labels (as set to 0s and 1s in the task)
%         while regression and generation approaches can generate more diverse scores.
%         }
%         \label{fig:binaryclassification}
% \end{figure*}




%JHL: we might not need to have different subsections for the two sets of results, if we end up discussing the results altogether
%JHL: suggestion: use a paragraph to spell out all the tables (2-X), and then discuss the results, and then present additional analysis (e.g. the text in 6.2 and 6.4)
% \subsection{Fine-grained Aspect Evaluation Results}




% We categorize the evaluation metrics into several types based on their features: 1) Similarity-based metrics, including BLEU (BLE), BERTScore (BER), and MoverScore (Mov); 2) Metrics that are specifically tailored for evaluating stories, including UNION (UNIO), MANPLTS (MAN), and StoryER (StoER); 3) Unified metrics designed to evaluate multiple aspects, including CTC and UNIEVAL (UNIE); and 4) Likelihood-based metrics, where the language models' likelihood is used directly, including BART-large (BRT-l), BART-cnn (BRT-c), and GPT-3.5 (G-3.5).\footnote{Please note that the concept of using likelihood is identical to that of BARTScore proposed by \cite{DBLP:conf/nips/YuanNL21}. The authors of BARTScore suggest that this idea can be extended to other generative models as well.} We also present the \deltascore metric using these three generative models.



\section{Discussion and Conclusion}

Initially, our aim was to investigate various types of perturbations for assessing fine-grained aspects of storytelling.
But seeing that performance of each metric does not vary much across different aspects in \autoref{fig:comparisons}, it suggests these aspects may be somewhat inter-correlated.
Also, our findings revealed that one of the simplest perturbation methods, namely \textit{Jumble}, is exceptionally effective in measuring most aspects.
% Our immediate reaction is that these aspects are perhaps more inter-correlated than we thought, and if we look at the performance for each metric across different aspects in \autoref{fig:comparisons}, this appear to be the case. That is, we do not see a significant disparity for each metric over the aspects. 
%However, the effectiveness of the \textit{Jumble} perturbation method in capturing multiple aspects still remains unexplained.
% This, however, still does not explain why \textit{Jumble} works particularly well.
 One hypothesis could be that \textit{Jumble} is functioning as a \textit{normalisation factor} to modulate word frequency and sentence length effects when estimating sequence quality. 
 This finding aligns with prior study that used sequence probabilities for measuring sentence acceptability \cite{sentenceacceptability-tacl20}. They found that it is important to normalise the probabilities and introduced various normalization techniques to mitigate the impact of word frequency and sentence length. \textit{Jumble} can be interpreted as an alternative normalisation technique.
 %The likelihood associated with a shuffled word sequence (\textit{Jumble}) can be considered as alternative approach  form of normalization that incorporates word frequency and sentence length effects.
Given this insight, it may also mean that  \deltascore has broader application beyond the evaluation of story quality. For instance, it could be used to score sentences in machine translation and summarization.

In conclusion, we introduce \deltascore, a novel approach to assess fine-grained story aspects by comparing the likelihood difference between the original story and a perturbed version using a pre-trained language model. Surprisingly, we discovered that a small set of perturbation methods excel in measuring the majority of story aspects. Furthermore, our findings demonstrate that \deltascore shows stronger correlations with human judgments compared to a range of existing metrics across two different story domains.
%We also show that \deltascore can be applied to various pre-trained language models, including encoder-decoder and decoder models, outperforming the direct application of generative likelihood as an evaluation metric.

% We experiment with multiple language models, including  and our results demonstrate that \deltascore with GPT-3.5 and jumble yields the best performance, outperforming state-of-the-art evaluation metrics.

\section*{Limitations}

Our study only investigates a limited range of perturbations, and we acknowledge that there may be other forms of perturbations that could work better.
The field of evaluation metrics is rapidly evolving, with numerous contemporary evaluation metrics introduced recently, such as G-Eval~\citep{geval-arxiv23} and ChatEval~\citep{chateval-arxiv23}, which were not incorporated into the comparative evaluation metrics within this study.

% While our current paper focuses on applying the perturbation method specifically to story evaluation, we recognize its potential for adaptation in assessing other text generation tasks, such as machine translation and abstractive summarization. This includes the utilization of \textit{Jumble} or alternative perturbation techniques, thereby paving the way for promising avenues of future investigation.
%JHL5: last point isn't really a limitation... so I am not sure if it belongs here (we've already mentioned this in discussion and that's probably enough)
%JHL5: did we compare against G-EVAL or other newer works? either way we should acknowledge this is a fast-changing field and talk about some of the new things that appear that are relevant but are not included in the study as they are contemporaneous

% This work explores a limited set of perturbations for story evaluation, but there are likely many more that could be obtained through different approaches. While we only apply this perturbation method to story generation, it has the potential to be adapted for evaluating various aspects of text generation using specifically designed perturbations, opening up a fruitful area for future research.


\section*{Acknowledgements}
We extend our thanks to the reviewers for their valuable feedback, which has greatly contributed to the improvement of this work. Zhuohan Xie is supported by Melbourne Research Scholarship, and would like to expresses his sincere appreciation to the program.

% \begin{figure*}[t]
%      \centering
%      \begin{subfigure}[b]{0.45\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/OPENMEVABARTScore_perturbations_roc.pdf}
%          \caption{ROC
%          }
%          \label{BARTScore-ROC-openmeva}
%      \end{subfigure}
%      \begin{subfigure}[b]{0.45\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/OPENMEVABARTScore_perturbations_wp.pdf}
%          \caption{WP
%          }
%          \label{BARTScore-WP-openmeva}
%      \end{subfigure}
%         \caption{
%         Perturbation results and Pearson correlations on each dataset for OpenMEVA.
%         }
%         \label{fig:perturbationgrades}
% \end{figure*}


% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\newpage

\appendix

\section{Perturbation Prompts}
\label{appendix:perturbationprompts}

We use following prompts for perturbations when we apply API with GPT-3.5-turbo.

%JHL3: there are some new lines in the prompt no? include them too (with \n)
%Zhuohan: I did not use newline in the prompt when I call the API.
\begin{itemize}
    \item RelevantWords: Find all words in the given story that is relevant to the given title. Please only print words in the given story, and separate them by `,'.  ``title'': \{title\}, ``story'': \{story\}
    \item Commonsense: Revise the following story such that certain elements does not make sense. The revision should be minimal, e.g., by changing a few words. ``story'': \{story\}
    \item BlanderNarrative: Revise the following story to make it less interesting (e.g., expected ending, no plot twist). The revision should be minimal. ``story'': \{story\}
\end{itemize}

We present several examples of perturbed stories by GPT-3.5-turbo in \autoref{table:ptdstories}. We observe that \textit{BlanderNarrative} does not significantly alter the original story. This observation is in line with previous findings that \textit{BlanderNarrative} does not effectively impact interestingness in \autoref{table:multiperturbations_inhouse}. We speculate that this outcome may be attributed to the inherent simplicity of most stories, which limits the extent to which GPT-3.5-turbo can modify them to reduce their level of interest.



\begin{table*}[t]
\small
\centering
\begin{tabular}{cp{1.5cm}p{5cm}p{5cm}}
\toprule
\textbf{Perturbation} & \textbf{Title} & \textbf{Original Story} & \textbf{Perturbed Story} \\
\midrule
RmRelWords & the man ordered extra cheese . & the man loved cheese and always ordered extra . his favourite was pizza with extra cheese . one day , he went to his favourite pizza place and ordered his usual . when the pizza arrived , he saw that there was no extra cheese . the man was so angry that he threw the pizza across the room . & man loved and always . his favourite was . one day , he went to his favourite place and ordered his usual . when the pizza arrived , he saw that there was . the man was so angry that he the pizza across the room . \\
\midrule
RmRelWords & the boy cried . & the boy cried because he was sad and scared . his parents were getting a divorce and he did n't know what was going to happen to him . he was going to have to leave his home and his friends and move to a new house . his mother said that he could choose where he wanted to live , but his father said that he had to live with him . the boy did n't know what to do . & because was and . were getting a and he was going happen . he was going to have to his and his friends and to a . his mother said that he could he to , but his father said that he had to him . the boy did n't know what to . \\
\midrule
Commonsense & [FEMALE] dad took me fishing . & they took me down to the lake . i threw my line out and caught several worms . i turned in one worm and caught a catfish . i told my dad and he took me home and i raised it for dinner . & they took me down to the sky . i threw my book out and caught several words . i turned in one word and caught a spaceship . i told my mom and she took me home , and i raised it for dinner . \\
\midrule
Commonsense & [FEMALE] was at the mall . & she was walking to the food court when she saw a man who looked lost . she went up to him and asked if he needed help . the man told her he was looking for his wife and daughter . [FEMALE] took him to the food court and pointed them out to him . & she was flying to the food court when she saw a bird who looked lost . she went up to it and asked if it needed help . the bird told her it was looking for its wife and daughter . [FEMALE] took it to the food court and pointed them out to it . \\
\midrule
BlanderNarrative & [FEMALE] went out to eat with her friends . & she ordered a burger and fries , but when the food arrived , she was really disappointed . the burger was tiny and the fries were cold and soggy . & she ordered a burger and fries . when the food arrived , it was just as she expected . the burger was small and the fries were lukewarm . \\
\midrule
BlanderNarrative & [MALE] is out with his friends at the bar . & he decides to buy a beer . [MALE] drinks a beer and eats a few more . [MALE] feels very sick . [MALE] is embarrassed that he drank so much . & he decides to buy a beer . [MALE] drinks a beer and eats a few more . [MALE] feels sick . [MALE] regrets drinking too much . \\
 \bottomrule
\end{tabular}
\caption{We show some examples of perturbed stories where we use GPT-3.5-turbo for perturbation.
}
\label{table:ptdstories}
\end{table*}


% \section{Meta-evalution Datasets Details}
% \label{appendix:datasetsdetail}

% \citet{DBLP:journals/corr/abs-2301-09790} evaluate a range of recently proposed story generation models:
% 1) \textbf{KGGPT2} \citep{guan-etal-2020-knowledge},
% 2) \textbf{MTCL} \citep{xu-etal-2020-megatron},
% 3) \textbf{HINT} \citep{guan-etal-2021-long},
% 4) \textbf{PROGEN} \citep{tan-etal-2021-progressive},
% 5) \textbf{BART} \citep{lewis-etal-2020-bart} and
% 6) \textbf{GPT-3} \citep{DBLP:conf/nips/BrownMRSKDNSSAA20},
% and also human reference stories.
% They conduct a meta-evaluation, which consists of two parts: crowdsourcing through Amazon Mechanical Turk and in-house evaluation by colleagues. We primarily rely on the crowdsourced evaluations since the authors suggest that in-house annotators may have a natural bias towards assigning higher scores to machine-generated stories.



\section{UNIEVAL Questions}
\label{appendix:unieval}


We ask the following questions for each aspect.
Note that we try to use the narrative/vocabulary as close to the original questions
\citet{unieval-emnlp22} use in their efforts as possible.

\begin{itemize}
    \item Fluency: Is this a fluent utterance?
    \item Coherence: Is this a coherent utterance?
    \item Relatedness: Is this claim consistent with the document?
    \item Logicality: Is this utterance consistent with the commonsense?
    \item Interestingness: Is this an interesting utterance?
\end{itemize}

\section{GPTScore Prompts}
\label{appendix:gptscore}

We use the following prompts for each aspect.
Note that we try to use the narrative/vocabulary as close to the original prompts \citep{gptscore-arxiv23} use in their efforts as possible.

\begin{itemize}
    \item Fluency: Generate a fluent story for the given title: \{title\}, and story: \{story\}
    \item Coherence: Generate a coherent story for the given title: \{title\}, and story: \{story\}
    \item Relatedness: Generate a story related to the given title: \{title\}, and story: \{story\}
    \item Logicality: Generate a story that adhere to commonsense for the given title: \{title\}, and story: \{story\}
    \item Interestingness: Generate an interesting story for the given title: \{title\}, and story: \{story\}
\end{itemize}

\section{GPT3.5Eval Instructions}
\label{appendix:gpt3.5instructions}

We use the following instructions for each aspect, following \citet{llmeval-acl23}.

\paragraph{Fluency} 
The goal of this task is to rate story fragment. \\
Note: Please take the time to fully read and understand the story fragment. \\
Story fragment: \\
\{\{Story\}\} \\
How fluent is the text of the story fragment? (on a scale of 1-5, with 1 being the lowest) 

\paragraph{Coherence} 
The goal of this task is to rate story fragment. \\
Note: Please take the time to fully read and understand the story fragment. \\
Story fragment: \\
\{\{Story\}\} \\
How coherent is the text of the story fragment? (on a scale of 1-5, with 1 being the lowest) 

\paragraph{Relatedness} 
The goal of this task is to rate story fragment. \\
Note: Please take the time to fully read and understand the story fragment. \\
Story title: \\
\{\{Title\}\} \\
Story fragment: \\
\{\{Story\}\} \\
How related is the text of the story fragment to the title? (on a scale of 1-5, with 1 being the lowest)

\paragraph{Logicality} 
The goal of this task is to rate story fragment. \\
Note: Please take the time to fully read and understand the story fragment. \\
Story fragment: \\
\{\{Story\}\} \\
How logically correct is the text of the story fragment? (on a scale of 1-5, with 1 being the lowest) 

\paragraph{Interestingness} 
The goal of this task is to rate story fragment. \\
Note: Please take the time to fully read and understand the story fragment. \\
Story fragment: \\
\{\{Story\}\} \\
How interesting is the text of the story fragment? (on a scale of 1-5, with 1 being the lowest) 





% \section{Correlations}
% \label{appendix: correlations}

% \paragraph{Visualization}

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.21\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figs/heatmap_roc.pdf}
%          \caption{ROC
%          }
%          \label{heatmap_ROC}
%      \end{subfigure}
%      \begin{subfigure}[b]{0.265\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/heatmap_wp.pdf}
%          \caption{WP
%          }
%          \label{heatmap_WP}
%      \end{subfigure}
%         \caption{
%         We present improvements of \deltascore on GPT-3.5 over applying GPT-3.5 generative likelihood directly as evaluation metric. Warmer color indicates greater improvement.
%         `TP' indicates typo,
%         `JB' indicates jumble,
%         `SR' indicates sentencereorder,
%         `AT' indicates antonym, and
%         `BN' indicates blandernarrative. 
%         }
%         \label{fig:heatmaps}
% \end{figure}

% \subsection{Issues}
% We find that UNIEVAL seems not able to differeniate too much when we ask different questions on each quality aspect.

% \section{Appendix: Correlation Results}

% We put Spearman and Kendall-Tau correlations in \autoref{table:crowdsource_cnn}.
% Pearson correlations are shown in .

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
%  % & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 25.0 & 11.3 & 6.6 & 15.0 & 0.8 & 9.4 & 16.9 & 11.2 & 15.4 & 23.7 & 22.7 & 0.2 & 25.1 & 3.6 & 18.1  \\ 
%  BER & 4.4 & 1.3 & 26.7 & 4.2 & 3.1 & 29.4 & 34.6 & 37.5 & 28.1 & 30.6 & 48.7 & 45.3 & 70.8 & 51.7 & 48.6 \\
%  Mov & 12.0 & 8.0 & 38.2 & 7.5 & 0.2 & 41.9 & 47.7 & 36.8 & 30.0 & 34.6 & 23.5 & 14.6 & 47.0 & 25.2 & 33.5 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 13.5 & 26.7 & 14.9 & 19.3 & 7.2 & 39.4 & 59.6 & 54.1 & 47.8 & 58.8 & 27.1 & 42.5 & 39.5 & 33.2 & 43.0 \\  
% MAN & 7.0 & 3.1 & 7.5 & 1.6 & 12.7 & 20.5 & 0.4 & 14.1 & 23.7 & 4.7 & 18.6 & 30.4 & 48.5 & 19.4 & 20.1 \\ 
% Sto & 5.4 & 12.0 & 14.3 & 1.0 & 12.6 & 11.6 & 22.1 & 12.4 & 21.5 & 32.5 & 13.0 & 28.6 & 14.4 & 14.7 & 23.7 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 23.5 & 41.3 & 25.3 & 35.7 & 25.8 & 51.3 & 76.6 & 53.6 & 68.0 & \textbf{57.2} & 57.4 & 59.4 & 52.8 & 61.0 & 62.7 \\ 
%  UNI & 43.0 & 41.5 & 13.9 & 34.0 & 26.1 & 61.6 & 56.3 & 49.4 & 51.9 & 51.0 & 47.3 & 61.3 & 55.7 & 71.5 & 52.0 \\
%  \midrule
% \multicolumn{10}{l}{\textbf{Likelihood Based Metrics}}\\
% BART & \\ 
% G-3.5 & 28.8 & 43.2 & 26.6 & 39.9 & 25.9 & 47.9 & 44.8 & 48.8 & 52.4 & 34.5 & 56.8 & 65.9 & 58.3 & 66.0 & \textbf{52.7} \\ 
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  + Ju & \cellcolor{green!25}38.7 & \cellcolor{green!25}48.3 & \cellcolor{green!25}33.7 & \cellcolor{green!25}50.8 & 37.2 & \cellcolor{green!25}43.3 & \cellcolor{green!25}50.3 & \cellcolor{green!25}53.0 & \cellcolor{green!25}52.9 & \cellcolor{green!25}43.2 & \cellcolor{green!25}46.7 & \cellcolor{green!25}63.8 & \cellcolor{green!25}46.7 & \cellcolor{green!25}56.1 & 47.6 \\ 
 
%  + Ty & \cellcolor{green!25}37.7 & \cellcolor{green!25}48.6 & \cellcolor{green!25}35.6 & 45.0 & 32.2 & \cellcolor{green!25}44.7 & \cellcolor{green!25}48.3 & \cellcolor{green!25}53.1 & \cellcolor{green!25}58.3 & \cellcolor{green!25}40.8 & \cellcolor{green!25}57.9 & \cellcolor{green!25}68.5 & \cellcolor{green!25}55.2 & \cellcolor{green!25}67.8 & 50.6 \\ 
 
%  + Ad & 18.0 & 41.1 & 21.1 & 32.5 & 24.1 & 44.4 & 38.4 & 30.9 & 34.9 & 23.7 & 47.1 & 66.3 & 32.4 & 59.2 & 42.6  \\
 
%  + An & 32.2 & \cellcolor{green!25}40.1 & 26.4 & 29.7 & 19.4 & \cellcolor{green!25}72.9 & \cellcolor{green!25}67.7 & \cellcolor{green!25}56.4 & \cellcolor{green!25}71.4 & \cellcolor{green!25}56.9 & \cellcolor{green!25}48.3 & 59.4 & 52.7 & \cellcolor{green!25}64.6 & 40.5 \\
%  + Rp  \\
%  + Rr  \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (change to vinialla BART)}}\\ 
%  + Ju &  \\
%  + Ty &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Spearman correlation ($|\rho|$) between different metrics and human evaluations on three Inhouse datasets.
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the original likehihood metric.
% }
% \label{table:inhouse_spearman}
% \end{table*}


% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{clcccccccccc}
% \toprule
% \multirow{2}{*}[-1ex]{\textbf{Target Aspect}} & \multirow{2}{*}[-1ex]{\textbf{Perturbation}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
% \cmidrule(lr){3-7}\cmidrule(lr){8-12}
%  & & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} \\
% \cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}
% -- & w/o perturbation & 32.0 & 27.6 & 25.1 & 21.1 & 19.7 & 31.6 & 36.0 & 34.7 & 38.5 & 30.6 \\ 
% \midrule
% \multirow{2}{*}[0ex]{Fluency} &  Typo & 22.9 & 11.4 & \cellcolor{green!25}13.3 & 8.2 & 11.4 & \cellcolor{green!25}42.8 & \cellcolor{green!25}35.6 & \cellcolor{green!25}39.8 & \cellcolor{green!25}41.9 & \cellcolor{green!25}33.0 \\
% & SubjVerbDis & 11.9 & 10.5 & 8.8 & 4.3 & 0.9 & 5.8 & 6.2 & 17.9 & 13.2 & 17.3  \\
% \midrule
% \multirow{2}{*}[0ex]{Coherence} & Jumble & \cellcolor{green!25}35.9 & \cellcolor{green!25}40.0 & \cellcolor{green!25}\textbf{33.8} & \cellcolor{green!25}\textbf{29.6} & \cellcolor{green!25}23.2 & \cellcolor{green!25}52.7 & \cellcolor{green!25}49.2 & \cellcolor{green!25}36.6 & \cellcolor{green!25}53.9 & \cellcolor{green!25}42.8  \\
% & SentReorder & 20.0 & 16.1 & 10.1 & 7.4 & 4.7 & 4.1 & 17.6 & 5.6 & 12.4 & 3.4 \\ 
% \midrule
% \multirow{2}{*}[0ex]{Relatedness} & RmRelWords & 15.9 & 16.9 & 13.3 & 9.6 & 7.6 & \cellcolor{green!25}36.5 & 27.7 & 31.7 & 33.5 & 21.9 \\
% & StoryReplace & 6.2 & 2.2 & 9.1 & 9.6 & 7.8 & \cellcolor{green!25}38.5 & \cellcolor{green!25}36.6 & \cellcolor{green!25}37.6 & 38.5 & \cellcolor{green!25}33.8 \\ 
% \midrule
% \multirow{2}{*}[0ex]{Logicality} & Antonym & 14.3 & \cellcolor{green!25}22.1 & 10.9 & \cellcolor{green!25}21.3 & \cellcolor{green!25}16.8 & \cellcolor{green!25}41.5 & \cellcolor{green!25}39.5 & \cellcolor{green!25}33.9 & \cellcolor{green!25}42.7 & \cellcolor{green!25}36.6 \\
% & Commonsense & 16.9 & 15.8 & 17.5 & 2.2 & 3.8 & 29.6 & \cellcolor{green!25}39.0 & 26.3 & 37.6 & 26.5 \\
% \midrule
% Interestingness & BlanderNarrative & 6.7 & 10.4 & 11.5 & 1.5 & 0.7 & 13.9 & 17.7 & 13.0 & 19.3 & 7.2 \\
%  \bottomrule
% \end{tabular}
% \caption{Story-level Kendall correlation ($|\tau|$) between \deltascore with GPT-3.5 and human evaluations on ROC and WP on CrowdSource Meta Evaluation. 
% \colorbox{green!30}{Highlight} indicates the scores where \deltascore achieves better performance over applying generative likelihood from GPT-3.5 directly as the evaluation metric.
% }
% \label{table:multiperturbations_crowdsource}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}
%   & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
% \multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn 406M)}}\\ 
% w/o perturbation &  24.9 & 11.6 & 10.9 & 11.3 & 14.6 & 32.2 & 25.6 & 31.0 & 30.2 & 23.0 \\ 
%  Jumble & \cellcolor{green!25}29.2 & \cellcolor{green!25}17.7 & 9.9 & \cellcolor{green!25}12.2 & 12.6 & \cellcolor{green!25}49.1 & \cellcolor{green!25}40.1 & \cellcolor{green!25}44.5 & \cellcolor{green!25}46.5 & \cellcolor{green!25}41.5 \\
%  Typo & 22.9 & 11.4 & \cellcolor{green!25}13.3 & 8.2 & 11.4 & \cellcolor{green!25}42.8 & \cellcolor{green!25}35.6 & \cellcolor{green!25}39.8 & \cellcolor{green!25}41.9 & \cellcolor{green!25}33.0 \\
%  Antonym & 14.3 & \cellcolor{green!25}22.1 & 10.9 & \cellcolor{green!25}21.3 & \cellcolor{green!25}16.8 & \cellcolor{green!25}41.5 & \cellcolor{green!25}39.5 & \cellcolor{green!25}33.9 & \cellcolor{green!25}42.7 & \cellcolor{green!25}36.6 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL 11B)}}\\ 
%   w/o perturbation & 21.0 & 22.1 & 26.8 & 23.2 & 22.5 & 13.6 & 10.6 & 12.1 & 10.8 & 4.2 \\
%  Jumble & \cellcolor{green!25}35.7 & \cellcolor{green!25}28.2 & 19.9 & 19.8 & 22.2 & \cellcolor{green!25}\textbf{59.2} & \cellcolor{green!25}53.7 & \cellcolor{green!25}41.8 & \cellcolor{green!25}57.4 & \cellcolor{green!25}46.9 \\
%  Typo & \cellcolor{green!25}24.2 & \cellcolor{green!25}26.6 & \cellcolor{green!25}27.7 & \cellcolor{green!25}24.6 & \cellcolor{green!25}23.0 & \cellcolor{green!25}34.3 & \cellcolor{green!25}30.5 & \cellcolor{green!25}28.1 & \cellcolor{green!25}34.8 & \cellcolor{green!25}23.1 \\
%  Antonym & 9.4 & 16.1 & 3.2 & 7.5 & 5.4 & \cellcolor{green!25}52.6 & \cellcolor{green!25}48.0 & \cellcolor{green!25}37.1 & \cellcolor{green!25}53.8 & \cellcolor{green!25}46.3 \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
%   w/o perturbation & 26.4 & 21.3 & 21.1 & 15.8 & 20.3 & 18.1 & 15.4 & 17.5 & 17.6 & 9.3 \\
%  Jumble & \cellcolor{green!25}35.4 & \cellcolor{green!25}36.4 & \cellcolor{green!25}21.6 & \cellcolor{green!25}23.4 & \cellcolor{green!25}\textbf{26.8} & \cellcolor{green!25}52.7 & \cellcolor{green!25}52.5 & \cellcolor{green!25}43.6 & \cellcolor{green!25}56.6 & \cellcolor{green!25}44.7 \\
%  Typo & \cellcolor{green!25}31.7 & \cellcolor{green!25}28.2 & \cellcolor{green!25}22.0 & 15.3 & 19.0 & \cellcolor{green!25}48.4 & \cellcolor{green!25}46.3 & \cellcolor{green!25}39.0 & \cellcolor{green!25}52.6 & \cellcolor{green!25}40.2 \\
%  Antonym & 11.8 & 16.1 & 6.5 & 9.6 & 7.7 & \cellcolor{green!25}49.9 & \cellcolor{green!25}46.3 & \cellcolor{green!25}32.5 & \cellcolor{green!25}50.0 & \cellcolor{green!25}42.7 \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with LLaMA 65B)}}\\ 
%   w/o perturbation & 31.0 & 24.9 & 25.4 & 16.8 & 18.5 & 21.1 & 26.6 & 22.7 & 26.7 & 20.3 \\
%  Jumble & \cellcolor{green!25}34.8 & \cellcolor{green!25}32.0 & 20.9 & \cellcolor{green!25}22.4 & \cellcolor{green!25}21.0 & \cellcolor{green!25}55.4 & \cellcolor{green!25}56.5 & \cellcolor{green!25}43.8 & \cellcolor{green!25}59.8 & \cellcolor{green!25}47.1 \\
%  Typo &  \cellcolor{green!25}36.8 & \cellcolor{green!25}32.3 & 22.9 & 13.8 & 13.8 & \cellcolor{green!25}45.0 & \cellcolor{green!25}47.0 & \cellcolor{green!25}36.0 & \cellcolor{green!25}52.2 & \cellcolor{green!25}38.0 \\
%  Antonym & 15.6 & 17.9 & 8.3 & 9.0 & 4.9 & \cellcolor{green!25}54.0\textsuperscript{\textdagger} & \cellcolor{green!25}51.5\textsuperscript{\textdagger} & \cellcolor{green!25}37.7\textsuperscript{\textdagger} & \cellcolor{green!25}56.2\textsuperscript{\textdagger} & \cellcolor{green!25}46.1\textsuperscript{\textdagger} \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
%   w/o perturbation & 32.0 & 26.7 & 25.3 & 22.0 & 23.5 & 20.8 & 24.8 & 22.2 & 25.7 & 17.1 \\
%  Jumble & \cellcolor{green!25}\textbf{43.2} & \cellcolor{green!25}\textbf{42.4} & \cellcolor{green!25}27.4 & \cellcolor{green!25}25.6 & \cellcolor{green!25}23.9 & \cellcolor{green!25}56.1 & \cellcolor{green!25}\textbf{55.5} & \cellcolor{green!25}\textbf{45.9} & \cellcolor{green!25}\textbf{60.9} & \cellcolor{green!25}\textbf{48.5} \\
%  Typo & \cellcolor{green!25}35.9 & \cellcolor{green!25}32.0 & \cellcolor{green!25}28.6 & 21.8 & 22.1 & \cellcolor{green!25}44.8 & \cellcolor{green!25}45.8 & \cellcolor{green!25}36.8 & \cellcolor{green!25}50.1 & \cellcolor{green!25}36.6 \\
%  Antonym & 16.5 & 21.9 & 8.4 & 11.9 & 9.3 & \cellcolor{green!25}55.2 & \cellcolor{green!25}51.8 & \cellcolor{green!25}36.9 & \cellcolor{green!25}56.0 & \cellcolor{green!25}45.9 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5 175B)}}\\
%  w/o perturbation & 32.0 & 27.6 & 25.1 & 21.1 & 19.7 & 31.6 & 36.0 & 34.7 & 38.5 & 30.6  \\
%  Jumble & \cellcolor{green!25}35.9 & \cellcolor{green!25}40.0 & \cellcolor{green!25}\textbf{33.8} & \cellcolor{green!25}\textbf{29.6} & \cellcolor{green!25}23.2 & \cellcolor{green!25}52.7 & \cellcolor{green!25}49.2 & \cellcolor{green!25}36.6 & \cellcolor{green!25}53.9 & \cellcolor{green!25}42.8 \\ 
%  Typo & 29.5 & \cellcolor{green!25}28.2 & 22.7 & 15.4 & 11.7 & \cellcolor{green!25}40.7 & \cellcolor{green!25}43.1 & 32.6 & \cellcolor{green!25}47.2 & \cellcolor{green!25}35.4  \\ 
%  Antonym & 21.4 & 26.7 & 15.1 & \cellcolor{green!25}21.9 & 18.5 & \cellcolor{green!25}48.3 & \cellcolor{green!25}47.3 & \cellcolor{green!25}37.6 & \cellcolor{green!25}51.6 & \cellcolor{green!25}42.1 \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Kendall correlation ($|\tau|$) between different metrics and human evaluations on Inhouse Meta Evaluation. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!25}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:multiplemodels_crowdsource}
% \end{table*}



\begin{table*}[t]
\small
\centering
\begin{tabular}{lcccccccccc}
\toprule
\multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
\cmidrule(lr){2-6}\cmidrule(lr){7-11}
  & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} \\
\cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
  \multicolumn{10}{l}{\textbf{Similarity Based Metrics}} \\ 
 BLEU & 3.4 & 4.4 & 0.8 & 4.6 & 0.4 & 13.4 & 11.4 & 9.6 & 11.2 & 7.2 \\ 
 BERTScore & 3.5 & 5.0 & 14.0 & 5.7 & 7.3 & 31.8 & 26.7 & 24.0 & 28.5 & 24.6 \\
 MoverScore & 3.6 & 6.5 & 15.7 & 8.0 & 11.2 & 16.4 & 17.2 & 20.1 & 20.7 & 23.7  \\
 \midrule
 \multicolumn{10}{l}{\textbf{Discriminative Metrics}} \\ 
UNION &  0.7 & 12.7 & 12.8 & 0.8 & 3.9 & 17.4 & 21.9 & 18.1 & 22.9 & 21.8 \\ 
MANPLTS & 21.3 & 32.8 & 23.2 & 14.7 & 12.4 & 1.1 & 5.5 & 1.7 & 4.6 & 6.7 \\ 
StoryER & 6.5 & 4.5 & 4.0 & 3.7 & 9.7 & 15.9 & 13.1 & 14.1 & 17.9 & 26.1 \\
 CTC & 22.9 & 27.3 & 14.3 & 11.1 & 8.3 & 45.9 & 51.6 & 40.3 & 53.1 & 47.5 \\ 
 UNIEVAL & 32.2 & 31.7 & 23.7 & 20.0 & 18.8 & 39.3 & 41.3 & 38.6 & 50.7 & 39.1 \\
 \midrule
 \multicolumn{10}{l}{\textbf{Generative Metrics}}\\ 
 BARTScore & 24.9 & 11.6 & 10.9 & 11.3 & 14.6 & 32.2 & 25.6 & 31.0 & 30.2 & 23.0 \\
 GPTScore & 21.2 & 20.9 & 21.3 & 23.9 & 22.0 & 29.3 & 32.6 & 30.8 & 35.3 & 28.9 \\
 \midrule
 GPT3.5Eval & 23.9 & 33.3 & 10.5 & 11.8 & 4.8 & 21.1 & 34.6 & 19.2 & 12.8 & 1.2 \\
 \midrule
  \multicolumn{10}{l}{\textbf{\deltascore}}\\ 
   Typo & 35.9 & 32.0 & \textbf{28.6} & 21.8 & 22.1 & 44.8 & 45.8 & 36.8 & 50.1 & 36.6 \\
 Jumble & \textbf{43.2} & \textbf{42.4} & 27.4 & \textbf{25.6} & \textbf{23.9} & \textbf{56.1} & \textbf{55.5} & \textbf{45.9} & \textbf{60.9} & \textbf{48.5} \\
 Antonym & 16.5 & 21.9 & 8.4 & 11.9 & 9.3 & 55.2 & 51.8 & 36.9 & 56.0 & 45.9 \\
 \bottomrule
\end{tabular}
\caption{Absolute value of Story-level Kendall correlation ($|\tau|$) between different metrics and crowdworker ratings. 
We \textbf{bold} the best scores in each aspect.
}
\label{table:crowdsource_kendall_all}
\end{table*}



% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int}  \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 1.6 & 3.8 & 3.4 & 8.3 & 1.4 & 12.5 & 13.7 & 12.1 & 17.2 & 13.1 \\ 
%  BER & 3.1 & 6.2 & 18.3 & 3.8 & 6.2 & 47.1 & 42.6 & 35.1 & 42.4 & 36.6 \\
%  Mov & 2.2 & 7.4 & 23.3 & 3.3 & 9.7 & 26.8 & 28.6 & 32.3 & 31.8 & 32.3 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 4.7 & 16.7 & 17.4 & 5.2 & 9.3 & 32.0 & 39.9 & 24.4 & 36.4 & 35.0 \\  
% MAN & 27.7 & 40.2 & 23.7 & 17.7 & 13.4 & 6.1 & 9.3 & 9.1 & 7.0 & 6.7 \\ 
% StoER & 8.9 & 1.1 & 15.5 & 5.4 & 12.1 & 26.7 & 21.2 & 13.5 & 26.8 & 35.0 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 29.3 & 38.3 & 20.3 & 16.1 & 12.0 & 43.7 & 55.2 & 44.1 & 63.9 & 59.6 \\ 
%  UNIE & 34.4 & 35.9 & 30.4 & 26.2 & 21.2 & 42.8 & 43.0 & 52.3 & 55.1 & 45.4 \\
%  \midrule
%    \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
%   -- &  36.4 & 31.7 & 27.7 & 25.3 & 30.3 & 18.7 & 23.8 & 29.9 & 28.6 & 22.8 \\
%  Jumble & \cellcolor{green!25}46.6 & \cellcolor{green!25}50.5 & \cellcolor{green!25}29.1 & \cellcolor{green!25}32.4 & \cellcolor{green!25}36.0 & \cellcolor{green!25}66.4 & \cellcolor{green!25}72.0 & \cellcolor{green!25}64.7 & \cellcolor{green!25}78.5 & \cellcolor{green!25}66.1  \\
%  Typo & \cellcolor{green!25}42.1 & \cellcolor{green!25}39.8 & \cellcolor{green!25}30.5 & 22.5 & 26.7 & \cellcolor{green!25}63.3 & \cellcolor{green!25}69.2 & \cellcolor{green!25}62.0 & \cellcolor{green!25}74.2 & \cellcolor{green!25}60.6 \\
%  AddNe & 9.8 & 18.3 & 17.5 & 1.4 & 5.9 & \cellcolor{green!25}34.9 & \cellcolor{green!25}31.4 & 29.4 & \cellcolor{green!25}35.4 & 19.8 \\
%  Anton & 18.7 & 22.2 & 6.3 & 11.8 & 11.1 & \cellcolor{green!25}66.2 & \cellcolor{green!25}64.5 & \cellcolor{green!25}55.0 & \cellcolor{green!25}70.1 & \cellcolor{green!25}58.0 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%   \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
%    -- & 42.6 & 39.2 & 34.7 & 31.8 & 32.3 & 23.1 & 34.0 & 36.5 & 39.2 & 33.0 \\
%  Jumble & \cellcolor{green!25}55.9 & \cellcolor{green!25}58.1 & \cellcolor{green!25}37.8 & \cellcolor{green!25}34.3 & 31.4 & \cellcolor{green!25}68.8 & \cellcolor{green!25}77.3 & \cellcolor{green!25}72.1 & \cellcolor{green!25}82.9 & \cellcolor{green!25}68.4  \\
%  Typo & \cellcolor{green!25}50.7 & \cellcolor{green!25}47.4 & \cellcolor{green!25}39.4 & 31.7 & 32.3 & \cellcolor{green!25}58.2 & \cellcolor{green!25}69.0 & \cellcolor{green!25}61.8 & \cellcolor{green!25}76.1 & \cellcolor{green!25}63.9  \\
%  AddNe & 30.3 & \cellcolor{green!25}42.3 & 34.4 & 23.1 & 24.1 & \cellcolor{green!25}32.6 & \cellcolor{green!25}35.9 & 34.5 & \cellcolor{green!25}41.1 & 29.2  \\
%  Anton & 25.8 & 30.5 & 8.8 & 15.7 & 12.0 & \cellcolor{green!25}66.8 & \cellcolor{green!25}69.3 & \cellcolor{green!25}60.1 & \cellcolor{green!25}75.8 & \cellcolor{green!25}62.1 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL)}}\\ 
%    -- & 30.0 & 33.3 & 37.2 & 32.4 & 29.3 & 6.9 & 11.3 & 19.4 & 15.7 & 10.7 \\
%  Jumble & \cellcolor{green!25}47.2 & \cellcolor{green!25}40.2 & 25.6 & 24.4 & 26.7 & \cellcolor{green!25}70.2 & \cellcolor{green!25}71.9 & \cellcolor{green!25}65.8 & \cellcolor{green!25}77.7 & \cellcolor{green!25}65.1 \\
%  Typo & \cellcolor{green!25}33.5 & \cellcolor{green!25}39.1 & \cellcolor{green!25}38.3 & \cellcolor{green!25}35.1 & \cellcolor{green!25}30.9 & \cellcolor{green!25}40.3 & \cellcolor{green!25}45.3 & \cellcolor{green!25}45.7 & \cellcolor{green!25}52.3 & \cellcolor{green!25}41.2 \\
%  AddNe & 20.1 & 30.5 & 24.6 & 16.3 & 17.0 & \cellcolor{green!25}38.5 & \cellcolor{green!25}38.5 & \cellcolor{green!25}37.7 & \cellcolor{green!25}42.9 & \cellcolor{green!25}28.0 \\
%  Anton & 16.9 & 24.6 & 4.0 & 11.7 & 8.5 & \cellcolor{green!25}63.5 & \cellcolor{green!25}63.1 & \cellcolor{green!25}58.1 & \cellcolor{green!25}70.9 & \cellcolor{green!25}57.7 \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- & 32.4 & 13.0 & 12.0 & 15.6 & 19.8 & 33.6 & 30.3 & 43.4 & 36.7 & 28.8 \\
%  Jumble & \cellcolor{green!25}35.9 & \cellcolor{green!25}21.5 & 11.2 & 15.4 & 17.0 & \cellcolor{green!25}57.6 & \cellcolor{green!25}55.3 & \cellcolor{green!25}62.9 & \cellcolor{green!25}63.5 & \cellcolor{green!25}56.8 \\
%  Typo & 29.6 & 11.3 & \cellcolor{green!25}12.4 & 9.4 & 15.3 & \cellcolor{green!25}47.4 & \cellcolor{green!25}44.5 & \cellcolor{green!25}55.7 & \cellcolor{green!25}52.5 & \cellcolor{green!25}41.8 \\
%  AddNe & 6.4 & \cellcolor{green!25}19.5 & \cellcolor{green!25}18.5 & 11.5 & 11.6 & 32.9 & 27.4 & 24.5 & 32.4 & 19.4 \\
%  Anton & 18.0 & \cellcolor{green!25}32.2 & \cellcolor{green!25}17.2 & \cellcolor{green!25}31.0 & \cellcolor{green!25}26.7 & \cellcolor{green!25}51.1 & \cellcolor{green!25}50.0 & \cellcolor{green!25}54.0 & \cellcolor{green!25}51.6 & \cellcolor{green!25}33.1 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 43.7 & 43.5 & 37.3 & 32.1 & 30.0 & 37.2 & 50.8 & 52.3 & 58.1 & 50.8 \\
%  Jumble & \cellcolor{green!25}47.4 & \cellcolor{green!25}54.3 & \cellcolor{green!25}45.8 & \cellcolor{green!25}40.7 & \cellcolor{green!25}30.7 & \cellcolor{green!25}66.4 & \cellcolor{green!25}74.6 & \cellcolor{green!25}66.3 & \cellcolor{green!25}80.8 & \cellcolor{green!25}67.7 \\ 
%  Typo & 41.7 & 41.0 & 29.7 & 23.5 & 18.8 & \cellcolor{green!25}53.1 & \cellcolor{green!25}65.9 & \cellcolor{green!25}57.3 & \cellcolor{green!25}70.5 & \cellcolor{green!25}57.5 \\ 
%  AddNe & 29.3 & 41.9 & \cellcolor{green!25}44.3 & 26.8 & 26.5 & 33.3 & 32.4 & 27.2 & 43.0 & 31.6 \\
%  Anton & 32.1 & 38.7 & 19.4 & 29.5 & 23.2 & \cellcolor{green!25}60.0 & \cellcolor{green!25}64.7 & \cellcolor{green!25}61.7 & \cellcolor{green!25}71.6 & \cellcolor{green!25}55.7 \\
%  % RpSt &  \\
%  % CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Pearson correlation ($|r|$) between different metrics and human evaluations on three CoudSourcing datasets. 
% % We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:crowdsource_pearson}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int}  \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%   BLE & 4.8 & 5.8 & 2.0 & 7.0 & 0.8 & 18.0 & 17.1 & 16.2 & 17.6 & 9.7 \\
%   BER & 5.5 & 7.3 & 19.7 & 7.8 & 9.9 & 42.8 & 37.2 & 33.1 & 38.9 & 34.0 \\
%   Mov & 4.9 & 9.1 & 22.6 & 11.0 & 15.2 & 22.1 & 24.7 & 28.8 & 28.8 & 32.9 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 0.6 & 17.6 & 17.6 & 1.4 & 6.1 & 24.7 & 31.1 & 26.8 & 32.3 & 33.6 \\  
% MAN & 30.8 & 45.5 & 31.8 & 21.5 & 17.7 & 1.4 & 7.2 & 2.2 & 4.1 & 10.3 \\ 
% StoER & 9.7 & 6.1 & 6.0 & 5.8 & 14.3 & 21.3 & 19.4 & 19.9 & 24.9 & 37.4 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 31.7 & 37.8 & 21.7 & 17.0 & 12.2 & 59.2 & 67.9 & 55.0 & 70.3 & 62.6  \\ 
%  UNIE & 44.3 & 45.2 & 33.3 & 29.0 & 26.6 & 52.6 & 56.8 & 52.7 & 67.4 & 52.7 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
%   -- & 35.4 & 30.1 & 29.9 & 22.4 & 27.8 & 22.3 & 19.1 & 23.7 & 22.4 & 12.3  \\
%  Jumble & \cellcolor{green!25}48.4 & \cellcolor{green!25}51.2 & \cellcolor{green!25}31.2 & \cellcolor{green!25}33.2 & \cellcolor{green!25}36.4 & \cellcolor{green!25}70.1 & \cellcolor{green!25}71.0 & \cellcolor{green!25}58.6 & \cellcolor{green!25}73.6 & \cellcolor{green!25}59.3 \\
%  Typo & \cellcolor{green!25}43.2 & \cellcolor{green!25}38.6 & 29.6 & 20.5 & 24.6 & \cellcolor{green!25}64.5 & \cellcolor{green!25}64.1 & \cellcolor{green!25}52.5 & \cellcolor{green!25}69.6 & \cellcolor{green!25}54.0 \\
%  AddNe & 14.0 & 24.3 & 22.2 & 8.6 & 11.4 & \cellcolor{green!25}33.7 & \cellcolor{green!25}32.6 & \cellcolor{green!25}34.8 & \cellcolor{green!25}36.2 & \cellcolor{green!25}21.1 \\
%  Anton & 17.0 & 22.9 & 9.9 & 13.3 & 11.8 & \cellcolor{green!25}65.4 & \cellcolor{green!25}62.9 & \cellcolor{green!25}46.6 & \cellcolor{green!25}65.6 & \cellcolor{green!25}56.0 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%   \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
%    -- & 42.9 & 37.2 & 36.2 & 31.1 & 32.4 & 27.0 & 30.6 & 31.9 & 32.6 & 22.4  \\
%  Jumble & \cellcolor{green!25}57.8 & \cellcolor{green!25}58.9 & \cellcolor{green!25}39.9 & \cellcolor{green!25}35.8 & 32.2 & \cellcolor{green!25}73.3 & \cellcolor{green!25}73.3 & \cellcolor{green!25}60.8 & \cellcolor{green!25}77.2 & \cellcolor{green!25}63.1 \\
%  Typo & \cellcolor{green!25}48.7 & \cellcolor{green!25}44.3 & \cellcolor{green!25}39.8 & 30.5 & 31.0 & \cellcolor{green!25}61.0 & \cellcolor{green!25}62.0 & \cellcolor{green!25}50.6 & \cellcolor{green!25}66.2 & \cellcolor{green!25}50.2 \\
%  AddNe & 32.8 & \cellcolor{green!25}46.4 & \cellcolor{green!25}39.8 & 25.4 & 25.9 & \cellcolor{green!25}30.1 & \cellcolor{green!25}34.5 & \cellcolor{green!25}36.9 & \cellcolor{green!25}39.6 & \cellcolor{green!25}29.5 \\
%  Anton & 23.7 & 31.1 & 12.2 & 17.4 & 13.9 & \cellcolor{green!25}69.9 & \cellcolor{green!25}68.7 & \cellcolor{green!25}51.5 & \cellcolor{green!25}72.5 & \cellcolor{green!25}59.4 \\
% \midrule
 
%  \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL)}}\\ 
%    -- & 28.9 & 30.7 & 37.1 & 32.1 & 30.3 & 16.7 & 13.5 & 16.8 & 14.4 & 4.7 \\
%  Jumble & \cellcolor{green!25}50.5 & \cellcolor{green!25}41.5 & 29.5 & 28.4 & \cellcolor{green!25}31.2 & \cellcolor{green!25}78.3 & \cellcolor{green!25}72.9 & \cellcolor{green!25}58.2 & \cellcolor{green!25}76.2 & \cellcolor{green!25}61.9 \\
%  Typo & \cellcolor{green!25}34.8 & \cellcolor{green!25}32.7 & 34.2 & 24.4 & 22.6 & \cellcolor{green!25}41.4 & \cellcolor{green!25}38.9 & \cellcolor{green!25}36.8 & \cellcolor{green!25}42.6 & \cellcolor{green!25}26.4 \\
%  AddNe & 22.9 & \cellcolor{green!25}33.6 & 24.2 & 15.0 & 14.4 & \cellcolor{green!25}34.1 & \cellcolor{green!25}39.4 & \cellcolor{green!25}41.8 & \cellcolor{green!25}44.7 & \cellcolor{green!25}31.8  \\
%  Anton & 13.4 & 22.5 & 4.6 & 10.5 & 7.7 & \cellcolor{green!25}68.0 & \cellcolor{green!25}64.4 & \cellcolor{green!25}52.0 & \cellcolor{green!25}70.1 & \cellcolor{green!25}59.9  \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- & 34.4 & 15.8 & 15.4 & 15.7 & 19.8 & 41.6 & 34.1 & 42.7 & 41.1 & 29.2 \\
%  Jumble & \cellcolor{green!25}40.6 & \cellcolor{green!25}25.7 & 14.4 & \cellcolor{green!25}17.6 & 17.8 & \cellcolor{green!25}64.2 & \cellcolor{green!25}56.2 & \cellcolor{green!25}59.5 & \cellcolor{green!25}62.4 & \cellcolor{green!25}53.6 \\
%  Typo & 31.9 & 14.9 & \cellcolor{green!25}18.0 & 11.1 & 16.0 & \cellcolor{green!25}55.2 & \cellcolor{green!25}47.9 & \cellcolor{green!25}54.3 & \cellcolor{green!25}56.4 & \cellcolor{green!25}43.3  \\
%  AddNe & 11.8 & \cellcolor{green!25}23.6 & \cellcolor{green!25}25.1 & \cellcolor{green!25}16.4 & 13.9 & 33.7 & 23.7 & 19.5 & 28.5 & 16.7 \\
%  Anton & 19.8 & \cellcolor{green!25}31.0 & \cellcolor{green!25}16.6 & \cellcolor{green!25}29.8 & \cellcolor{green!25}23.4 & \cellcolor{green!25}55.7 & \cellcolor{green!25}54.4 & \cellcolor{green!25}48.3 & \cellcolor{green!25}59.5 & \cellcolor{green!25}49.3  \\
%  % RpSt & 1.6 & \cellcolor{green!25}16.3 & 11.2 & \cellcolor{green!25}23.1 & 14.4 &  \\
%  % CS & 0.6 & 10.4 & 9.5 & \cellcolor{green!25}20.6 & 14.1 &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 42.7 & 39.5 & 35.7 & 30.0 & 27.6 & 43.7 & 48.2 & 47.6 & 51.8 & 42.2  \\
%  Jumble & \cellcolor{green!25}49.3 & \cellcolor{green!25}55.3 & \cellcolor{green!25}48.5 & \cellcolor{green!25}41.0 & \cellcolor{green!25}32.9 & \cellcolor{green!25}70.2 & \cellcolor{green!25}66.6 & \cellcolor{green!25}51.9 & \cellcolor{green!25}70.5 & \cellcolor{green!25}57.9 \\ 
%  Typo & 41.0 & \cellcolor{green!25}40.4 & 33.5 & 23.0 & 16.8 & \cellcolor{green!25}57.0 & \cellcolor{green!25}60.0 & 44.8 & \cellcolor{green!25}63.0 & \cellcolor{green!25}48.7 \\ 
%  AddNe & 30.2 & \cellcolor{green!25}42.7 & \cellcolor{green!25}48.7 & 25.5 & 25.8 & 37.0 & 32.2 & 21.8 & 37.0 & 24.0 \\
%  Anton & 30.2 & 38.8 & 22.4 & \cellcolor{green!25}30.7 & 26.5 & \cellcolor{green!25}64.0 & \cellcolor{green!25}64.0 & \cellcolor{green!25}50.5 & \cellcolor{green!25}68.3 & \cellcolor{green!25}55.3  \\
%  % RpSt & 0.8 & 2.7 & 8.8 & 15.1 & 11.2 &  \\
%  % CS & 15.6 & 15.3 & 19.3 & 1.0 & 0.4 &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Spearman correlation ($|\rho|$) between different metrics and human evaluations on three CoudSourcing datasets. 
% % We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:crowdsource_spearman}
% \end{table*}



% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int}  \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 1.6 & 3.8 & 3.4 & 8.3 & 1.4 & 12.5 & 13.7 & 12.1 & 17.2 & 13.1 \\ 
%  BER & 3.1 & 6.2 & 18.3 & 3.8 & 6.2 & 47.1 & 42.6 & 35.1 & 42.4 & 36.6 \\
%  Mov & 2.2 & 7.4 & 23.3 & 3.3 & 9.7 & 26.8 & 28.6 & 32.3 & 31.8 & 32.3 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 4.8 & 12.6 & 11.6 & 19.5 & 13.6 & 14.7 & 20.2 & 4.4 & 12.1 & 27.5 \\  
% MAN & 9.3 & 16.2 & 11.3 & 15.4 & 5.0 & 24.0 & 12.5 & 19.3 & 33.0 & 8.7 \\ 
% StoER & 3.2 & 8.8 & 8.0 & 1.8 & 15.2 & 13.9 & 22.7 & 6.6 & 17.3 & 34.2 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 23.9 & 42.3 & 20.0 & 35.9 & 28.9 & 53.0 & 77.7 & 59.2 & 70.4 & 59.9 \\ 
%  UNIE & 50.7 & 30.3 & 9.3 & 31.8 & 16.5 & 48.6 & 37.9 & 39.5 & 34.5 & 30.2 \\
%  \midrule
%    \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
%   -- & 22.0 & 31.2 & 14.1 & 32.4 & 12.5 & 30.9 & 32.0 & 42.5 & 42.8 & 17.1 \\
%  Jumble & \cellcolor{green!25}34.0 & \cellcolor{green!25}45.8 & \cellcolor{green!25}17.0 & \cellcolor{green!25}44.5 & \cellcolor{green!25}37.3 & \cellcolor{green!25}45.1 & \cellcolor{green!25}50.7 & \cellcolor{green!25}52.8 & \cellcolor{green!25}51.8 & \cellcolor{green!25}38.0 \\
%  Typo & \cellcolor{green!25}32.9 & \cellcolor{green!25}44.8 & \cellcolor{green!25}16.7 & \cellcolor{green!25}40.3 & \cellcolor{green!25}25.8 & \cellcolor{green!25}54.6 & \cellcolor{green!25}63.2 & \cellcolor{green!25}57.5 & \cellcolor{green!25}64.6 & \cellcolor{green!25}42.8 \\
%  AddNe & 7.0 & 24.5 & 2.9 & 13.5 & \cellcolor{green!25}14.4 & 18.6 & \cellcolor{green!25}33.9 & 27.2 & 32.0 & 5.9 \\
%  Anton & 11.2 & 13.9 & 3.4 & 9.0 & 10.4 & \cellcolor{green!25}41.8 & \cellcolor{green!25}46.7 & \cellcolor{green!25}49.0 & \cellcolor{green!25}48.8 & \cellcolor{green!25}41.5 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%   \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
%    -- & 26.8 & 41.2 & 23.3 & 44.6 & 24.0 & 34.9 & 36.1 & 45.9 & 46.1 & 23.0 \\
%  Jumble & \cellcolor{green!25}33.5 & \cellcolor{green!25}59.7 & \cellcolor{green!25}29.6 & \cellcolor{green!25}57.5 & \cellcolor{green!25}48.4 & \cellcolor{green!25}55.2 & \cellcolor{green!25}61.5 & \cellcolor{green!25}59.4 & \cellcolor{green!25}63.5 & \cellcolor{green!25}54.4 \\
%  Typo & \cellcolor{green!25}30.8 & \cellcolor{green!25}54.1 & 22.4 & \cellcolor{green!25}52.2 & \cellcolor{green!25}36.1 & \cellcolor{green!25}58.6 & \cellcolor{green!25}66.4 & \cellcolor{green!25}60.7 & \cellcolor{green!25}68.8 & \cellcolor{green!25}52.5  \\
%  AddNe & 19.6 & \cellcolor{green!25}46.2 & 13.6 & 28.7 & \cellcolor{green!25}26.0 & 28.5 & \cellcolor{green!25}42.6 & 36.2 & 40.3 & 12.3 \\
%  Anton & 21.9 & 27.4 & 15.0 & 22.6 & 21.4 & \cellcolor{green!25}44.2 & \cellcolor{green!25}54.0 & \cellcolor{green!25}54.8 & \cellcolor{green!25}54.3 & \cellcolor{green!25}48.3 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL)}}\\ 
%    -- & 25.6 & 26.1 & 15.2 & 29.5 & 9.4 & 24.6 & 22.5 & 36.4 & 31.9 & 6.8 \\
%  Jumble & \cellcolor{green!25}36.3 & \cellcolor{green!25}49.2 & \cellcolor{green!25}28.8 & \cellcolor{green!25}40.3 & \cellcolor{green!25}30.1 & \cellcolor{green!25}42.8 & \cellcolor{green!25}41.4 & \cellcolor{green!25}46.5 & \cellcolor{green!25}50.6 & \cellcolor{green!25}38.8  \\
%  Typo & 21.4 & \cellcolor{green!25}31.7 & 14.1 & \cellcolor{green!25}35.6 & \cellcolor{green!25}20.2 & \cellcolor{green!25}37.4 & \cellcolor{green!25}44.2 & \cellcolor{green!25}49.8 & \cellcolor{green!25}47.5 & \cellcolor{green!25}22.5 \\
%  AddNe & \cellcolor{green!25}30.1 & \cellcolor{green!25}37.8 & \cellcolor{green!25}24.2 & 27.5 & \cellcolor{green!25}31.8 & 10.0 & \cellcolor{green!25}29.8 & 30.1 & 28.7 & 0.8 \\
%  Anton & 17.4 & 20.1 & 9.3 & 14.4 & \cellcolor{green!25}12.8 & \cellcolor{green!25}42.6 & \cellcolor{green!25}42.5 & \cellcolor{green!25}47.5 & \cellcolor{green!25}43.8 & \cellcolor{green!25}36.5 \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- & 11.4 & 20.0 & 15.4 & 20.5 & 6.9 & 14.2 & 15.8 & 36.7 & 30.1 & 2.8 \\
%  Jumble & \cellcolor{green!25}15.0 & \cellcolor{green!25}33.6 & \cellcolor{green!25}21.8 & \cellcolor{green!25}31.5 & \cellcolor{green!25}18.8 & \cellcolor{green!25}38.4 & \cellcolor{green!25}36.9 & \cellcolor{green!25}48.4 & \cellcolor{green!25}46.4 & \cellcolor{green!25}26.8 \\
%  Typo & \cellcolor{green!25}19.6 & \cellcolor{green!25}39.5 & \cellcolor{green!25}28.2 & \cellcolor{green!25}32.5 & \cellcolor{green!25}23.8 & \cellcolor{green!25}32.7 & \cellcolor{green!25}42.9 & \cellcolor{green!25}51.6 & \cellcolor{green!25}49.8 & \cellcolor{green!25}28.4 \\
%  AddNe & 10.9 & \cellcolor{green!25}27.9 & \cellcolor{green!25}16.9 & 14.3 & \cellcolor{green!25}21.3 & 13.2 & \cellcolor{green!25}27.6 & 30.1 & 25.0 & \cellcolor{green!25}3.1 \\
%  Anton & \cellcolor{green!25}12.6 & 12.0 & 11.3 & 6.1 & \cellcolor{green!25}10.9 & \cellcolor{green!25}42.1 & \cellcolor{green!25}44.9 & \cellcolor{green!25}54.8 & \cellcolor{green!25}46.6 & \cellcolor{green!25}39.0 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 31.6 & 46.2 & 24.2 & 41.6 & 29.7 &  47.2 & 49.1 & 54.8 & 57.5 & 39.5 \\
%  Jumble & 28.9 & \cellcolor{green!25}50.3 & \cellcolor{green!25}26.4 & \cellcolor{green!25}53.5 & \cellcolor{green!25}43.5 & 45.6 & \cellcolor{green!25}58.7 & \cellcolor{green!25}62.8 & \cellcolor{green!25}60.4 & \cellcolor{green!25}54.2  \\ 
%  Typo & 30.5 & 46.2 & 21.3 & \cellcolor{green!25}46.9 & \cellcolor{green!25}38.5 & \cellcolor{green!25}57.6 & \cellcolor{green!25}68.8 & \cellcolor{green!25}58.3 & \cellcolor{green!25}68.6 & \cellcolor{green!25}61.2 \\ 
%  AddNe &  20.9 & 43.7 & 20.7 & 33.5 & 29.0 & 35.8 & 44.6 & 34.0 & 36.3 & 23.1 \\
%  Anton &  23.0 & 32.1 & 14.3 & 29.8 & 25.3 & \cellcolor{green!25}47.7 & \cellcolor{green!25}64.0 & \cellcolor{green!25}64.4 & \cellcolor{green!25}59.9 & \cellcolor{green!25}52.3 \\
%  % RpSt &  \\
%  % CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Pearson correlation ($|r|$) between different metrics and human evaluations on three Inhouse datasets. 
% % We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:inhouse_pearson}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int}  \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 1.6 & 3.8 & 3.4 & 8.3 & 1.4 & 12.5 & 13.7 & 12.1 & 17.2 & 13.1 \\ 
%  BER & 3.1 & 6.2 & 18.3 & 3.8 & 6.2 & 47.1 & 42.6 & 35.1 & 42.4 & 36.6 \\
%  Mov & 2.2 & 7.4 & 23.3 & 3.3 & 9.7 & 26.8 & 28.6 & 32.3 & 31.8 & 32.3 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 11.2 & 5.2 & 17.5 & 13.9 & 9.3 & 17.8 & 27.7 & 11.0 & 18.5 & 36.1 \\  
% MAN & 13.5 & 26.7 & 14.9 & 19.3 & 7.2 & 20.5 & 0.4 & 14.1 & 23.7 & 4.7 \\ 
% StoER & 5.4 & 11.9 & 14.3 & 1.0 & 12.6 & 11.7 & 22.1 & 12.5 & 21.4 & 32.5 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 23.5 & 41.3 & 25.3 & 35.7 & 25.8 & 51.3 & 76.6 & 53.6 & 68.0 & 57.2 \\ 
%  UNIE & 43.0 & 41.5 & 13.9 & 34.0 & 26.1 & 61.6 & 56.3 & 49.4 & 51.9 & 51.0 \\
%  \midrule
%    \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
%   -- & 17.2 & 33.1 & 18.4 & 36.4 & 14.7 & 31.1 & 26.9 & 36.7 & 40.3 & 15.4 \\
%  Jumble & \cellcolor{green!25}32.9 & \cellcolor{green!25}45.2 & \cellcolor{green!25}20.3 & \cellcolor{green!25}44.1 & \cellcolor{green!25}32.6 & \cellcolor{green!25}45.5 & \cellcolor{green!25}46.2 & \cellcolor{green!25}44.1 & \cellcolor{green!25}44.9 & \cellcolor{green!25}31.1 \\
%  Typo & \cellcolor{green!25}29.8 & \cellcolor{green!25}41.6 & 17.9 & \cellcolor{green!25}37.9 & \cellcolor{green!25}19.3 & \cellcolor{green!25}55.5 & \cellcolor{green!25}57.7 & \cellcolor{green!25}49.7 & \cellcolor{green!25}61.5 & \cellcolor{green!25}41.3 \\
%  AddNe & 5.9 & 24.7 & 5.4 & 14.0 & 11.0 & 22.0 & \cellcolor{green!25}35.8 & 24.4 & 37.5 & \cellcolor{green!25}20.3 \\
%  Anton & 7.8 & 13.4 & 5.4 & 8.2 & 5.8 & \cellcolor{green!25}41.9 & \cellcolor{green!25}46.0 & \cellcolor{green!25}44.1 & \cellcolor{green!25}47.1 & \cellcolor{green!25}43.7 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%   \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
%    -- & 23.7 & 41.9 & 27.9 & 45.6 & 22.8 & 35.1 & 32.0 & 41.5 & 42.4 & 19.9 \\
%  Jumble & \cellcolor{green!25}32.8 & \cellcolor{green!25}57.9 & \cellcolor{green!25}31.4 & \cellcolor{green!25}57.0 & \cellcolor{green!25}44.3 & \cellcolor{green!25}53.8 & \cellcolor{green!25}52.4 & \cellcolor{green!25}48.0 & \cellcolor{green!25}55.7 & \cellcolor{green!25}43.7 \\
%  Typo & \cellcolor{green!25}27.5 & \cellcolor{green!25}52.5 & 23.8 & \cellcolor{green!25}52.2 & \cellcolor{green!25}31.9 & \cellcolor{green!25}56.7 & \cellcolor{green!25}57.3 & \cellcolor{green!25}51.2 & \cellcolor{green!25}62.4 & \cellcolor{green!25}45.4  \\
%  AddNe & 22.1 & \cellcolor{green!25}45.9 & 19.6 & 29.7 & \cellcolor{green!25}23.7 & \cellcolor{green!25}39.2 & \cellcolor{green!25}42.6 & 35.7 & \cellcolor{green!25}44.1 & \cellcolor{green!25}22.6 \\
%  Anton & 20.4 & 27.2 & 17.0 & 22.0 & 15.5 & \cellcolor{green!25}47.8 & \cellcolor{green!25}55.5 & \cellcolor{green!25}49.6 & \cellcolor{green!25}55.1 & \cellcolor{green!25}51.2 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL)}}\\ 
%    -- & 21.6 & 27.6 & 21.0 & 33.0 & 12.3 & 25.3 & 16.8 & 29.8 & 28.0 & 4.6 \\
%  Jumble & \cellcolor{green!25}33.2 & \cellcolor{green!25}47.9 & \cellcolor{green!25}31.2 & \cellcolor{green!25}38.7 & \cellcolor{green!25}27.4 & \cellcolor{green!25}43.1 & \cellcolor{green!25}38.9 & \cellcolor{green!25}33.6 & \cellcolor{green!25}49.5 & \cellcolor{green!25}33.5  \\
%  Typo & 19.2 & \cellcolor{green!25}32.7 & 14.6 & \cellcolor{green!25}37.4 & \cellcolor{green!25}22.7 & \cellcolor{green!25}41.9 & \cellcolor{green!25}39.3 & \cellcolor{green!25}43.7 & \cellcolor{green!25}46.6 & \cellcolor{green!25}26.8 \\
%  AddNe & \cellcolor{green!25}28.5 & \cellcolor{green!25}33.9 & \cellcolor{green!25}24.2 & 25.6 & \cellcolor{green!25}28.5 & 19.6 & \cellcolor{green!25}33.0 & 28.1 & \cellcolor{green!25}34.5 & \cellcolor{green!25}13.5 \\
%  Anton & 15.5 & 20.2 & 11.0 & 13.8 & 9.7 & \cellcolor{green!25}47.8 & \cellcolor{green!25}47.1 & \cellcolor{green!25}42.3 & \cellcolor{green!25}47.1 & \cellcolor{green!25}39.3 \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- & 6.2 & 19.6 & 16.7 & 21.0 & 3.7 & 20.6 & 18.8 & 30.0 & 34.8 & 12.5 \\
%  Jumble & \cellcolor{green!25}12.3 & \cellcolor{green!25}31.9 & \cellcolor{green!25}21.3 & \cellcolor{green!25}30.6 & \cellcolor{green!25}16.3 & \cellcolor{green!25}42.9 & \cellcolor{green!25}32.4 & \cellcolor{green!25}37.2 & \cellcolor{green!25}47.5 & \cellcolor{green!25}30.4 \\
%  Typo & \cellcolor{green!25}11.4 & \cellcolor{green!25}28.1 & \cellcolor{green!25}24.5 & \cellcolor{green!25}26.1 & \cellcolor{green!25}11.0 & \cellcolor{green!25}35.5 & \cellcolor{green!25}37.5 & \cellcolor{green!25}40.6 & \cellcolor{green!25}51.6 & \cellcolor{green!25}32.6 \\
%  AddNe & \cellcolor{green!25}10.0 & \cellcolor{green!25}24.1 & \cellcolor{green!25}20.4 & 12.1 & \cellcolor{green!25}20.9 & \cellcolor{green!25}21.7 & \cellcolor{green!25}21.2 & 20.5 & 25.9 & 3.5 \\
%  Anton & 9.1 & 12.7 & 10.2 & 8.4 & \cellcolor{green!25}9.9 & \cellcolor{green!25}40.7 & \cellcolor{green!25}38.4 & \cellcolor{green!25}49.4 & \cellcolor{green!25}44.5 & \cellcolor{green!25}35.4 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 28.8 & 43.1 & 26.8 & 39.7 & 25.9 & 47.9 & 44.7 & 48.6 & 52.3 & 34.5 \\
%  Jumble & 26.8 & \cellcolor{green!25}48.2 & 26.7 & \cellcolor{green!25}51.9 & \cellcolor{green!25}40.8 & 44.3 & \cellcolor{green!25}51.9 & \cellcolor{green!25}53.2 & \cellcolor{green!25}52.6 & \cellcolor{green!25}43.1  \\ 
%  Typo & 24.2 & 40.8 & 21.8 & \cellcolor{green!25}46.2 & \cellcolor{green!25}31.8 & \cellcolor{green!25}56.9 & \cellcolor{green!25}62.7 & \cellcolor{green!25}51.3 & \cellcolor{green!25}63.3 & \cellcolor{green!25}53.7 \\ 
%  AddNe & 17.5 & 40.3 & 20.8 & 31.5 & 23.1 & 40.3 & 38.4 & 28.9 & 33.0 & 22.3 \\
%  Anton & 24.9 & 29.6 & 18.7 & 26.7 & 17.6 & \cellcolor{green!25}50.1 & \cellcolor{green!25}60.3 & \cellcolor{green!25}56.9 & \cellcolor{green!25}55.2 & \cellcolor{green!25}55.0 \\
%  % RpSt &  \\
%  % CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Spearman correlation ($|\rho|$) between different metrics and human evaluations on three Inhouse datasets. 
% % We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:inhouse_spearman}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int}  \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 1.6 & 3.8 & 3.4 & 8.3 & 1.4 & 12.5 & 13.7 & 12.1 & 17.2 & 13.1 \\ 
%  BER & 3.1 & 6.2 & 18.3 & 3.8 & 6.2 & 47.1 & 42.6 & 35.1 & 42.4 & 36.6 \\
%  Mov & 2.2 & 7.4 & 23.3 & 3.3 & 9.7 & 26.8 & 28.6 & 32.3 & 31.8 & 32.3 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 8.9 & 3.7 & 11.7 & 9.7 & 6.1 & 13.4 & 20.3 & 7.7 & 12.7 & 25.9 \\  
% MAN & 10.5 & 19.4 & 10.2 & 13.6 & 5.4 & 14.7 & 0.2 & 10.5 & 16.7 & 2.2 \\ 
% StoER & 4.1 & 8.4 & 10.0 & 0.8 & 8.2 & 8.3 & 16.9 & 9.0 & 15.0 & 22.5 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 17.5 & 30.3 & 17.9 & 25.3 & 18.1 & 37.8 & 60.4 & 39.2 & 50.0 & 42.2 \\ 
%  UNIE & 31.8 & 30.7 & 9.9 & 24.5 & 18.6 & 45.7 & 40.5 & 37.1 & 36.2 & 36.7 \\
%  \midrule
%    \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
%   -- & 12.4 & 24.1 & 13.3 & 25.5 & 10.2 & 24.8 & 21.3 & 27.9 & 29.2 & 12.6 \\
%  Jumble & \cellcolor{green!25}24.7 & \cellcolor{green!25}33.5 & \cellcolor{green!25}13.8 & \cellcolor{green!25}32.5 & \cellcolor{green!25}23.2 & \cellcolor{green!25}32.9 & \cellcolor{green!25}33.7 & \cellcolor{green!25}31.5 & \cellcolor{green!25}30.7 & \cellcolor{green!25}21.6 \\
%  Typo & \cellcolor{green!25}22.1 & \cellcolor{green!25}30.4 & 12.6 & \cellcolor{green!25}27.3 & \cellcolor{green!25}12.8 & \cellcolor{green!25}40.7 & \cellcolor{green!25}42.0 & \cellcolor{green!25}35.2 & \cellcolor{green!25}44.4 & \cellcolor{green!25}28.6 \\
%  AddNe & 4.6 & 17.7 & 4.0 & 10.5 & 8.0 & 15.6 & \cellcolor{green!25}25.2 & 17.0 & 26.6 & \cellcolor{green!25}14.4 \\
%  Anton & 4.9 & 9.3 & 4.2 & 5.5 & 4.2 & \cellcolor{green!25}29.4 & \cellcolor{green!25}33.5 & \cellcolor{green!25}31.0 & \cellcolor{green!25}33.6 & \cellcolor{green!25}31.2 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%   \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
%    -- & 17.4 & 30.6 & 20.2 & 32.6 & 15.8 & 27.5 & 24.5 & 32.2 & 31.0 & 15.7 \\
%  Jumble & \cellcolor{green!25}24.2 & \cellcolor{green!25}42.9 & \cellcolor{green!25}21.8 & \cellcolor{green!25}41.2 & \cellcolor{green!25}31.8 & \cellcolor{green!25}38.8 & \cellcolor{green!25}38.5 & \cellcolor{green!25}33.8 & \cellcolor{green!25}39.3 & \cellcolor{green!25}30.7 \\
%  Typo & \cellcolor{green!25}20.4 & \cellcolor{green!25}39.1 & 16.4 & \cellcolor{green!25}37.7 & \cellcolor{green!25}22.3 & \cellcolor{green!25}41.1 & \cellcolor{green!25}41.7 & \cellcolor{green!25}37.0 & \cellcolor{green!25}45.4 & \cellcolor{green!25}31.2 \\
%  AddNe & 15.8 & \cellcolor{green!25}33.1 & 13.0 & 21.2 & \cellcolor{green!25}17.3 & 27.0 & \cellcolor{green!25}30.3 & 25.4 & 30.1 & 15.3 \\
%  Anton & 13.9 & 18.7 & 11.7 & 15.1 & 10.6 & \cellcolor{green!25}33.7 & \cellcolor{green!25}41.1 & \cellcolor{green!25}36.0 & \cellcolor{green!25}40.3 & \cellcolor{green!25}37.3 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL)}}\\ 
%    -- & 16.2 & 19.4 & 14.7 & 23.1 & 8.9 & 19.9 & 14.1 & 23.4 & 20.8 & 4.8 \\
%  Jumble & \cellcolor{green!25}24.5 & \cellcolor{green!25}36.0 & \cellcolor{green!25}22.4 & \cellcolor{green!25}27.2 & \cellcolor{green!25}19.0 & \cellcolor{green!25}31.2 & \cellcolor{green!25}27.4 & 22.5 & \cellcolor{green!25}35.0 & \cellcolor{green!25}23.1 \\
%  Typo & 14.2 & \cellcolor{green!25}24.1 & 10.6 & \cellcolor{green!25}26.6 & \cellcolor{green!25}16.3 & \cellcolor{green!25}31.4 & \cellcolor{green!25}28.8 & \cellcolor{green!25}31.5 & \cellcolor{green!25}33.0 & \cellcolor{green!25}19.0 \\
%  AddNe & \cellcolor{green!25}21.0 & \cellcolor{green!25}24.5 & \cellcolor{green!25}17.1 & 18.3 & \cellcolor{green!25}20.1 & 13.6 & \cellcolor{green!25}24.3 & 20.2 & \cellcolor{green!25}24.4 & \cellcolor{green!25}9.8 \\
%  Anton & 10.7 & 14.3 & 8.3 & 9.9 & 6.6 & \cellcolor{green!25}34.3 & \cellcolor{green!25}33.8 & \cellcolor{green!25}29.8 & \cellcolor{green!25}34.0 & \cellcolor{green!25}27.3 \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- & 4.5 & 14.6 & 11.6 & 14.5 & 2.3 & 15.0 & 14.5 & 22.4 & 24.5 & 10.6 \\
%  Jumble & \cellcolor{green!25}8.8 & \cellcolor{green!25}23.4 & \cellcolor{green!25}15.3 & \cellcolor{green!25}21.0 & \cellcolor{green!25}10.9 & \cellcolor{green!25}31.5 & \cellcolor{green!25}23.5 & \cellcolor{green!25}26.7 & \cellcolor{green!25}33.8 & \cellcolor{green!25}22.0 \\
%  Typo & \cellcolor{green!25}8.3 & \cellcolor{green!25}20.1 & \cellcolor{green!25}17.2 & \cellcolor{green!25}18.2 & \cellcolor{green!25}7.8 & \cellcolor{green!25}25.8 & \cellcolor{green!25}28.3 & \cellcolor{green!25}29.4 & \cellcolor{green!25}36.8 & \cellcolor{green!25}24.3 \\
%  AddNe & \cellcolor{green!25}7.5 & \cellcolor{green!25}17.0 & \cellcolor{green!25}14.1 & 8.7 & \cellcolor{green!25}15.1 & \cellcolor{green!25}15.3 & 14.4 & 14.5 & 17.8 & 2.1 \\
%  Anton & \cellcolor{green!25}5.8 & 8.9 & 7.1 & 6.2 & \cellcolor{green!25}7.2 & \cellcolor{green!25}27.8 & \cellcolor{green!25}26.9 & \cellcolor{green!25}36.1 & \cellcolor{green!25}31.6 & \cellcolor{green!25}24.7 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 21.3 & 31.2 & 18.7 & 28.6 & 18.2 & 36.2 & 32.9 & 36.0 & 37.1 & 23.3  \\
%  Jumble & 19.4 & \cellcolor{green!25}34.6 & \cellcolor{green!25}19.4 & \cellcolor{green!25}37.6 & \cellcolor{green!25}30.1 & 31.7 & \cellcolor{green!25}38.4 & \cellcolor{green!25}37.2 & \cellcolor{green!25}37.4 & \cellcolor{green!25}30.8  \\ 
%  Typo & 17.7 & 29.8 & 15.2 & \cellcolor{green!25}33.7 & \cellcolor{green!25}23.0 & \cellcolor{green!25}42.4 & \cellcolor{green!25}46.9 & \cellcolor{green!25}36.4 & \cellcolor{green!25}47.2 & \cellcolor{green!25}38.7 \\ 
%  AddNe & 12.3 & 28.7 & 14.2 & 22.0 & 15.9 & 29.1 & 27.0 & 19.3 & 22.6 & 15.9 \\
%  Anton & 18.5 & 20.8 & 13.6 & 19.0 & 11.7 & 36.2 & \cellcolor{green!25}45.1 & \cellcolor{green!25}41.6 & \cellcolor{green!25}40.6 & \cellcolor{green!25}40.6  \\
%  % RpSt &  \\
%  % CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Kendall correlation ($|\tau|$) between different metrics and human evaluations on three Inhouse datasets. 
% % We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:inhouse_kendall}
% \end{table*}


% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 1.6 & 3.8 & 3.4 & 8.3 & 1.4 & 33.5 & 20.7 & 17.8 & 24.4 & 19.0 & 6.8 & 5.7 & 1.1 & 1.1 & 2.6 \\ 
%  BER & 3.1 & 6.2 & 18.3 & 3.8 & 6.2 & 37.8 & 33.3 & 28.9 & 33.9 & 30.2 & 46.9 & 40.2 & 57.9 & 51.8 & 15.1  \\
%  Mov & 2.2 & 7.4 & 23.3 & 3.3 & 9.7 & 19.0 & 25.4 & 21.8 & 25.4 & 32.0 & 32.3 & 25.1 & 38.4 & 36.1 & 11.0 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO &  \\  
% MAN &  \\ 
% StoER &  \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC &  \\ 
%  UNIE &  \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (with BART-large)}}\\ 
%    -- &  \\
%  Jumble &  \\
%  Typo & \\
%  Anton &  \\
%  RpSt &  \\
% \midrule
% \multicolumn{12}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 43.8 & 43.5 & 37.3 & 32.2 & 30.0 & 27.4 & 44.3 & 38.4 & 51.0 & 50.1 & 56.2 & 70.4 & 73.1 & 69.8 & 60.9\\
%  Jumble & 46.3 & 47.2 & 38.8 & 33.3 & 23.8 & 67.4 & 74.5 & 62.5 & 81.6 & 67.4 & 50.3 & 65.6 & 63.0 & 65.7 & 54.5 \\ 
%  Typo & 47.3 & 45.4 & 41.0 & 30.4 & 24.7 & 49.6 & 60.3 & 52.6 & 68.5 & 60.0 & 59.1 & 70.4 & 73.8 & 71.0 & 59.2 \\ 
%  AddNe & 30.5 & 43.0 & 44.9 & 28.5 & 27.4 & 17.3 & 22.9 & 5.8 & 33.2 & 38.3 & 52.4 & 59.0 & 63.4 & 60.9 & 43.6  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Pearson correlation ($|r|$) between different metrics and human evaluations on three CoudSourcing datasets. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:crowdsource_pearson}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%   BLE & 4.8 & 5.8 & 2.0 & 7.0 & 0.8 & 27.0 & 19.6 & 19.8 & 21.2 & 14.3 & 15.3 & 0.9 & 3.4 & 0.3 & 5.3 \\
%   BER & 5.5 & 7.3 & 19.7 & 7.8 & 9.9 & 29.6 & 27.7 & 23.3 & 29.2 & 28.2 & 42.6 & 33.2 & 54.0 & 49.9 & 12.4 \\
%   Mov & 4.9 & 9.1 & 22.6 & 11.0 & 15.2 & 14.1 & 22.4 & 22.3 & 21.3 & 28.5 & 27.5 & 13.2 & 32.7 & 30.0 & 4.8 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 0.6 & 17.6 & 17.6 & 1.4 & 6.1 & 29.2 & 35.4 & 29.9 & 36.9 & 39.6 & 10.0 & 1.6 & 24.5 & 4.2 & 1.6 \\  
% MAN & 30.8 & 45.5 & 31.8 & 21.5 & 17.7 & 0.1 & 8.2 & 3.5 & 4.9 & 12.8 & 8.5 & 27.0 & 18.3 & 25.5 & 16.5 \\ 
% StoER & 9.7 & 6.1 & 6.0 & 5.8 & 14.3 & 25.8 & 23.7 & 18.9 & 31.2 & 34.5 & 6.7 & 6.6 & 0.1 & 10.5 & 8.4 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 31.7 & 37.8  & 21.7  & 17.0  & 12.2 & 60.0 & 70.1 & 52.1 & 72.8 & \textbf{63.8} & 40.0 & 52.1 & 56.2 & 58.6 & 44.6 \\ 
%  UNIE & 44.3 & 45.2 & 33.3 & 29.0 & 26.6 & 52.6 & 56.8 & 50.9 & 67.4 & 52.7 & 33.9 & 43.6 & 50.4 & 53.2 & 37.3 \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (with BART-large)}}\\ 
%    -- & 15.7 & 1.9 & 1.8 & 3.9 & 3.3 & 7.4 & 6.8 & 1.8 & 4.7 & 11.5 & 21.0 & 23.2 & 29.3 & 21.3 & 9.2 \\
%  Jumble & \cellcolor{green!25}22.8 & \cellcolor{green!25}8.5 & \cellcolor{green!25}11.5 & \cellcolor{green!25}11.6 & \cellcolor{green!25}4.0 & \cellcolor{green!25}8.7 & \cellcolor{green!25}6.9 & \cellcolor{green!25}7.1 & 2.5 & 10.6 & \cellcolor{green!25}32.8 & \cellcolor{green!25}29.3 & \cellcolor{green!25}34.2 & \cellcolor{green!25}30.6 & \cellcolor{green!25}33.1 \\
%  Typo & 9.6 & \cellcolor{green!25}4.6 & \cellcolor{green!25}6.2 & \cellcolor{green!25}5.4 & 2.7 & 3.0 & 3.0 & 1.5 & \cellcolor{green!25}7.0 & 9.0 & 19.9 & 4.0 & 1.2 & 8.5 & 7.5 \\
%  AddNe & \cellcolor{green!25}17.2 & \cellcolor{green!25}21.3 & \cellcolor{green!25}10.0 & \cellcolor{green!25}4.0 & \cellcolor{green!25}6.3 & 6.6 & \cellcolor{green!25}11.4 & \cellcolor{green!25}6.0 & \cellcolor{green!25}9.6 & 9.1 & 8.7 & \cellcolor{green!25}23.5 & 6.8 & 17.9 & \cellcolor{green!25}26.1 \\
%  Anton & 11.3 & \cellcolor{green!25}13.7 & 0.4 & \cellcolor{green!25}18.5 & \cellcolor{green!25}16.3 & \cellcolor{green!25}15.4 & \cellcolor{green!25}21.7 & \cellcolor{green!25}11.0 & \cellcolor{green!25}11.1 & 9.5 & \cellcolor{green!25}26.4 & 13.7 & 21.0 & 14.6 & 8.7 \\
%  RpSt & \cellcolor{green!25}21.7 & \cellcolor{green!25}14.2 & \cellcolor{green!25}9.3 & \cellcolor{green!25}11.2 & \cellcolor{green!25}7.3 & \cellcolor{green!25}12.2 & 1.2 & \cellcolor{green!25}13.3 & 3.4 & 4.7 & \cellcolor{green!25}23.1 & \cellcolor{green!25}30.7 & \cellcolor{green!25}39.3 & \cellcolor{green!25}30.4 & \cellcolor{green!25}22.9 \\
%  CS & 4.5 & 0.2 & \cellcolor{green!25}4.2 & \cellcolor{green!25}10.1 & \cellcolor{green!25}12.2 & \cellcolor{green!25}8.7 & 4.1 & \cellcolor{green!25}5.1 & 4.6 & 1.7 & 8.1 & 0.0 & 3.6 & 0.1 & 0.8 \\
% \midrule
% \multicolumn{12}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- & 34.4 & 15.8 & 15.4 & 15.7 & 19.8 & 31.3 & 26.5 & 25.0 & 31.6 & 22.6 & 48.2 & 39.7 & 38.0 & 46.8 & 28.2 \\
%  Jumble & \cellcolor{green!25}37.4 & \cellcolor{green!25}22.6 & 15.0 & 14.7 & 14.1 & \cellcolor{green!25}60.4 & \cellcolor{green!25}53.7 & \cellcolor{green!25}50.5 & \cellcolor{green!25}59.5 & \cellcolor{green!25}52.7 & \cellcolor{green!25}49.9 & \cellcolor{green!25}44.4 & \cellcolor{green!25}41.2 & \cellcolor{green!25}54.0 & 23.5 \\
%  Typo & 31.9 & 14.9 & \cellcolor{green!25}18.0 & 11.1 & 16.0 & \cellcolor{green!25}43.7 & \cellcolor{green!25}38.9 & \cellcolor{green!25}37.6 & \cellcolor{green!25}43.7 & \cellcolor{green!25}32.9 & \cellcolor{green!25}55.4 & \cellcolor{green!25}42.8 & \cellcolor{green!25}46.3 & \cellcolor{green!25}54.2 & \cellcolor{green!25}32.1 \\
%  AddNe & 11.8 & \cellcolor{green!25}23.6 & \cellcolor{green!25}25.1 & \cellcolor{green!25}16.4 & 13.9 & 17.0 & 22.6 & 9.2 & 23.8 & 22.6 & 27.0 & 19.4 & 20.6 & 24.7 & 14.2 \\
%  Anton & 20.1 & \cellcolor{green!25}30.7 & 14.4 & \cellcolor{green!25}25.4 & 19.8 & \cellcolor{green!25}56.2 & \cellcolor{green!25}55.2 & \cellcolor{green!25}58.0 & \cellcolor{green!25}61.2 & \cellcolor{green!25}50.5 & 13.1 & 15.5 & 5.5 & 15.2 & 14.0 \\
%  RpSt & 1.6 & \cellcolor{green!25}16.3 & 11.2 & \cellcolor{green!25}23.1 & 14.4 & 14.6 & 12.5 & 19.0 & 17.4 & 20.8 & 39.4 & 34.9 & \cellcolor{green!25}39.4 & 46.0 & 20.6 \\
%  CS & 0.6 & 10.4 & 9.5 & \cellcolor{green!25}20.6 & 14.1 & \cellcolor{green!25}36.7 & \cellcolor{green!25}33.7 & \cellcolor{green!25}29.5 & 28.6 & 21.1 & \cellcolor{green!25}53.6 & \cellcolor{green!25}51.0 & 35.9 & \cellcolor{green!25}51.1 & \cellcolor{green!25}41.0 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 43.2 & 39.5 & 35.4 & 29.9 & 27.4 & 33.5 & 40.8 & 34.1 & 43.9 & 43.1 & 47.6 & 64.9 & 66.8 & 67.4 & 58.7 \\
%  Jumble & 50.4 & 47.3 & 43.5 & 33.3 & 25.5 & 72.8 & 68.6 & 51.3 & 72.6 & 58.8 & 53.1 & 62.3 & 61.8 & 69.1 & 43.2  \\ 
%  Typo & 47.6 & 42.3 & 41.1 & 28.5 & 22.8 & 56.9 & 57.1 & 45.9 & 62.1 & 51.2 & 58.0 & 65.0 & 70.5 & 70.8 & 55.5 \\ 
%  AddNe & 30.7 & \cellcolor{green!25}43.6 & \cellcolor{green!25}\textbf{49.1} & 26.5 & 26.7 & \cellcolor{green!25}38.7 & 35.1 & 21.8 & 41.9 & 35.3 & \cellcolor{green!25}48.0 & \cellcolor{green!25}60.8 & \cellcolor{green!25}65.0 & 62.9 & 46.1  \\
%  Anton & 32.2 & \cellcolor{green!25}40.1 & 26.4 & 29.7 & 19.4 & \cellcolor{green!25}\textbf{72.9} & \cellcolor{green!25}67.7 & \cellcolor{green!25}56.4 & \cellcolor{green!25}71.4 & \cellcolor{green!25}56.9 & \cellcolor{green!25}48.3 & 59.4 & 52.7 & \cellcolor{green!25}64.6 & 40.5 \\
%  RpSt & 0.8 & 2.7 & 8.8 & 15.1 & 11.2 & 25.2 & 26.4 & 28.7 & 30.6 & 35.0 & 31.4 & 42.6 & 52.4 & 53.8 & 42.6 \\
%  CS & 15.6 & 15.3 & 19.3 & 1.0 & 0.4 & \cellcolor{green!25}37.5 & \cellcolor{green!25}47.1 & \cellcolor{green!25}35.6 & 40.7 & 32.1 & 34.4 & 55.7 & 55.6 & 50.6 & 40.7 \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Spearman correlation ($|\rho|$) between different metrics and human evaluations on three CoudSourcing datasets. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:crowdsource_spearman}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 3.4 & 4.4 & 0.8 & 4.6 & 0.4 & 19.8 & 13.3 & 13.8 & 15.0 & 10.0 & 9.1 & 0.8 & 2.6 & 0.2 & 3.7\\ 
%  BER & 3.5 & 5.0 & 14.0 & 5.7 & 7.3 & 21.5 & 18.8 & 16.4 & 20.9 & 19.9 & 32.8 & 25.3 & 40.9 & 37.7 & 8.7\\
%  Mov & 3.6 & 6.5 & 15.7 & 8.0 & 11.2 & 9.6 & 16.4 & 16.1 & 15.4 & 20.9 & 21.3 & 9.5 & 23.9 & 22.2 & 3.0 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO &  \\  
% MAN &  \\ 
% StoER &  \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC &  \\ 
%  UNIE &  \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (with BART-large)}}\\ 
%    -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
% \multicolumn{12}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 32.4 & 27.7 & 25.1 & 21.1 & 19.5 & 24.7 & 30.2 & 25.6 & 32.5 & 31.4 & 36.0 & 46.8 & 48.6 & 49.6 & 44.1 \\
%  Jumble & 36.7 & 34.1 & 30.7 & 23.5 & 18.3 & 55.6 & 50.0 & 37.0 & 56.2 & 43.5 & 41.0 & 47.5 & 43.7 & 52.3 & 31.9 \\ 
%  Typo & 35.1 & 30.5 & 29.1 & 19.9 & 15.8 & 41.7 & 41.2 & 33.3 & 46.3 & 37.1 & 43.9 & 48.1 & 52.7 & 54.3 & 41.6 \\ 
%  AddNe & 21.9 & 30.4 & 35.1 & 19.4 & 20.1 & 27.9 & 24.1 & 15.3 & 29.7 & 25.8 & 41.0 & 44.7 & 50.5 & 49.3 & 30.8  \\
%  Anton & \\
%  RpSt &  \\
%  CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Kendall correlation ($|\tau|$) between different metrics and human evaluations on three CoudSourcing datasets. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:crowdsource_kendall_old}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 21.5 & 11.8 & 6.4 & 4.0 & 16.6 & 13.0 & 18.9 & 7.3 & 9.7 & 21.3 & 25.6 & 5.2 & 28.1 & 1.7 & 25.1 \\ 
%  BER & 3.2 & 1.1 & 25.2 & 2.5 & 4.4 & 28.7 & 33.7 & 37.3 & 27.7 & 30.1 & 53.0 & 50.9 & 70.3 & 52.9 & 47.8 \\
%  Mov & 15.6 & 9.2 & 43.5 & 9.2 & 1.2 & 40.8 & 46.5 & 38.6 & 34.6 & 38.6 & 30.9 & 24.4 & 54.7 & 29.1 & 37.6 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO &  \\  
% MAN &  \\ 
% StoER &  \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC &  \\ 
%  UNIE &  \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (with BART-large)}}\\ 
%    -- &  \\
%  Jumble &  \\
%  Typo & \\
%  Anton &  \\
%  RpSt &  \\
% \midrule
% \multicolumn{12}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- &  \\
%  Jumble &  \\ 
%  Typo & \\ 
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Pearson correlation ($|r|$) between different metrics and human evaluations on three Inhouse datasets. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:inhouse_pearson_old}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 25.0 & 11.3 & 0.8 & 6.6 & 15.0 & 9.4 & 16.9 & 11.2 & 15.4 & 23.7 & 22.7 & 0.2 & 25.1 & 3.6 & 18.1 \\ 
%  BER & 4.4 & 1.3 & 26.7 & 4.2 & 3.1 & 29.4 & 34.6 & 37.5 & 28.1 & 30.6 & 48.7 & 45.3 & 70.8 & 51.7 & 48.6 \\
%  Mov & 12.0 & 8.0 & 38.2 & 7.5 & 0.2 & 41.9 & 47.7 & 36.8 & 30.0 & 34.6 & 23.5 & 14.6 & 47.0 & 25.2 & 33.5 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO &  \\  
% MAN &  \\ 
% StoER &  \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC &  \\ 
%  UNIE &  \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (with BART-large)}}\\ 
%    -- &  \\
%  Jumble &  \\
%  Typo & \\
%  Anton &  \\
%  RpSt &  \\
% \midrule
% \multicolumn{12}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- &  \\
%  Jumble &  \\ 
%  Typo & \\ 
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Spearman correlation ($|\rho|$) between different metrics and human evaluations on three Inhouse datasets. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:inhouse_spearman}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 18.1 & 7.8 & 1.1 & 4.9 & 10.7 & 6.6 & 11.5 & 8.0 & 10.5 & 17.0 & 17.8 & 1.3 & 19.6 & 1.8 & 12.2 \\ 
%  BER & 3.6 & 0.0 & 19.4 & 2.8 & 2.1 & 21.6 & 26.3 & 26.6 & 20.1 & 20.9 & 35.5 & 34.9 & 53.9 & 39.5 & 34.4 \\
%  Mov & 8.5 & 5.2 & 27.6 & 4.9 & 0.0 & 31.3 & 34.0 & 26.5 & 20.5 & 23.2 & 16.6 & 11.1 & 35.1 & 18.4 & 23.0 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO &  \\  
% MAN &  \\ 
% StoER &  \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC &  \\ 
%  UNIE &  \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (with BART-large)}}\\ 
%    -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
% \multicolumn{12}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- &  \\
%  Jumble &  \\ 
%  Typo &  \\ 
%  AddNe &  \\
%  Anton & \\
%  RpSt &  \\
%  CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Kendall correlation ($|\tau|$) between different metrics and human evaluations on three Inhouse datasets. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:inhouse_kendall}
% \end{table*}



% % \begin{table}[t]
% % \small
% % \centering
% % \begin{tabular}{lcccccc}
% % \toprule
% %  \multirow{2}{*}{\textbf{Metric}} & \multicolumn{3}{c}{\textbf{ROC}} & \multicolumn{3}{c}{\textbf{WP}} \\
% %  & $|r|$ & $|\rho|$ & $|\tau|$ & $|r|$ & $|\rho|$ & $|\tau|$ \\
% % \cmidrule(lr){1-1}\cmidrule(lr){2-4}\cmidrule(lr){5-7}
% %   \multicolumn{5}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
% %  BLEU & 8.5 & 8.3 & 5.7 & 1.0 & 2.3 & 1.7 \\ 
% %  BERT. & 27.9 & 25.1 & 17.6 & 15.3 & 15.5 & 10.7 \\ 
% %  Mover. & 13.3 & 11.2 & 7.7 & 0.1 & 1.8 & 1.2 \\ 
% %  \midrule
% %  \multicolumn{5}{l}{\textbf{Story Evaluation Metrics}} \\ 
% % UNION & 40.8 & 46.7 & 33.0 & 10.0 & 17.8 & 12.4 \\ 
% % Union & 41.2 & - & - & 32.6 & - & - \\
% % MANP. & 27.4 & 36.0 & 25.1 & 14.3 & 17.5 & 12.1 \\ 
% % StoryER & 6.4 & 5.5 & 3.8 & 13.3 & 13.5 & 9.3 \\
% %  \midrule
% %  \multicolumn{5}{l}{\textbf{Unified Evaluation Metrics}}\\ 
% %  CTC & 40.4 & 41.5 & 29.0 & 29.4 & 29.6 & 20.7 \\ 
% %  UNI. & 42.8 & 43.0 & 30.2 & 32.2 & 30.6 & 21.4 \\ 
% % \midrule
% %  \multicolumn{5}{l}{\textbf{\deltascore (with GPT-3.5)}}\\ 
% %  - Per & 30.0 & 29.3 & 20.3 & 32.3 & 31.7 & 22.0 \\ 
% %  + Jumble & \cellcolor{green!25}35.3 & \cellcolor{green!25}34.3 & \cellcolor{green!25}23.8 & 31.3 & 31.2 & 21.7 \\ 
% %  + Typos & \cellcolor{green!25}32.0 & \cellcolor{green!25}30.4 & \cellcolor{green!25}21.1 & 32.0 & 31.3 & 21.7 \\ 
% %  \midrule
% %   \multicolumn{5}{l}{\textbf{\deltascore (with BART-cnn)}}\\ 
% %  BARTS & 33.5 & 33.5 & 23.3 & 33.5 & 33.1 & 23.0 \\ 
% %  + Jumble & \cellcolor{green!25}41.9 & \cellcolor{green!25}41.1 & \cellcolor{green!25}28.9 & 33.1 & 32.7 & 23.0 \\ 
% %  + Typos &  \cellcolor{green!25}36.9 & \cellcolor{green!25}37.1 & \cellcolor{green!25}25.9 & \cellcolor{green!25}34.3 & \cellcolor{green!25}33.9 & \cellcolor{green!25}23.7 \\ 
% %  \bottomrule
% % \end{tabular}
% % \caption{Story-level Pearson ($r$), Spearman ($\rho$) and Kendall-Tau ($\tau$) correlations of different metrics on OpenMEVA dataset.}
% % \label{table:corelations_openmeva}
% % \end{table}

\end{document}
