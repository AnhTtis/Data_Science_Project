% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
	
% \documentclass[xcolor=table]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{xcolor,colortbl}
\usepackage{booktabs,tabularx}
\usepackage{graphicx}

\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{textcomp}
% Define the name for Section / SubSection / Table / Figure to save some spaces.
\def\sectionautorefname{\S} 
\def\subsectionautorefname{\S} 
\def\appendixautorefname{Appendix}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{adjustbox}
\usepackage{rotating}
\usepackage{pifont}
\newcommand{\cmark}{\text{\ding{51}}}
\newcommand{\xmark}{\text{\ding{55}}}
% \sisetup{table-parse-only,detect-weight=true,detect-inline-weight=text, round-mode=places, round-precision=1, table-number-alignment=center}

\newcommand{\model}[1]{\textsc{#1}\xspace}
\newcommand{\deltascore}{\model{DeltaScore}}

\newcommand{\tbnum}[1]{\multicolumn{1}{c}{\bfseries \num{#1}}}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{xspace}
\usepackage{amssymb}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\newcolumntype{g}{>{\columncolor{Gray}}c}
\newcommand{\Needcite}[1]{{\color{orange}{\bf{[CITE:]}} #1}}
\newcommand{\Zhuohan}[1]{{\color{red}{\bf{[Zhuohan]}} #1}}
\newcommand{\miao}[1]{{\color{blue}{\bf{[Miao:]}} #1}}

\title{\deltascore: Story Evaluation with Perturbations}

% \title{PScore: Automatic Story Evaluation via Differences between Language Model Likelihood from Perturbations}

\author{Zhuohan Xie 
 \qquad
 Miao Li
 \qquad
 Trevor Cohn\thanks{\ \ Now at Google DeepMind}
 \qquad
 Jey Han Lau\\
 School of Computing and Information Systems, \\
 The University of Melbourne \\
 \{zhuohanx, miao4\}@student.unimelb.edu.au, 
\{t.cohn, laujh\}@unimelb.edu.au
 }
 

\begin{document}
\maketitle
\begin{abstract}

%Automatically evaluating the story quality in AI storytelling is a challenging open problem, and it impedes the development of automatic story generation. %JHL: skipping the cliche
Numerous evaluation metrics have been developed for natural language generation tasks but their effectiveness in evaluating stories is limited as they are not specifically tailored to assess intricate story aspects such as fluency and interestingness.
In this paper, we propose \deltascore, an approach that utilizes perturbation to evaluate fine-grained story aspects. Our core hypothesis is that the better the story performs in a specific aspect (e.g., fluency), the more it will be affected by a particular perturbation (e.g., introducing typos). To measure the impact, we calculate the \textit{likelihood difference} between the pre- and post-perturbation using large pre-trained language models. We evaluate \deltascore against a suite of current metrics across two story domains, and investigate its correlation with human judgments on five fine-grained story aspects: fluency, coherence, relatedness, logicality, and interestingness. \deltascore performs very strongly, with a surprise observation that one particular perturbation works very well for capturing multiple aspects.
%The findings of our study indicate that the \deltascore approach exhibits strong performance in evaluating intricate story aspects. 
% An unexpected discovery was made in our experiment, where a single perturbation method was found to effectively capture a majority of these aspects.

% We measure the effectiveness of \deltascore against state-of-the-art model-based and traditional similarity-based metrics over several story domains, by comparing how well it correlates with human judgements on 5 fine-grained story aspects (fluency, coherence, relatedness, logicality and interestingness). We found \deltascore performs very well, with the surprising finding that one particular perturbation appears to work very well for measuring most aspects.
%Our extensive experimental show that \deltascore has substantially better correlations against human judgements versus existing state-of-the-art evaluation metrics without any requirements of fine-tuning, and it achieves fine-grained evaluation of story generation with different aspect-aware perturbations.


% as a consequence of the vast range of possible generations
% Automatic story evaluation has long been admittedly a challenging task.
% Evaluation metrics that are originally proposed for 
% other natural language generation (NLG) tasks
% are widely adopted in current story model developing literature.
% However, they have been criticized being not suitable for open-ended generations,
% since they usually show low correlations to human evaluations, which is the de facto
% standard for story evaluation.
% State-of-the-art (SOTA) story evaluation metrics train classifiers to distinguish original stories from the negative samples or
% highly-upvoted stories from lowly-upvoted ones.
% As a result, they obtain one story evaluator that can evaluate story coherence or human preference.
% These approaches only provide one overall score, which do not align well with the multiple aspects of story quality.
% We propose to evaluate story via differences between GPT3 likelihood from perturbations.
% Experiments show our methods have a higher correlations to human evaluations,
% and can provide explainability to multiple aspects.

\end{abstract}


%JHL: after reading the whole paper, I think in terms of narrative introduction we can do something like this: (1) 

\section{Introduction}
\label{sec:introduction}


%JHL2: suggest we don't use both PLM and LLM acronyms. I think we can just stick with PLMs, since our method isn't limited to just 'large' LMs.
% Zhuohan: addressed.
The advent of large pre-trained language models (PLMs) \citep{radford2019language, lewis-etal-2020-bart, DBLP:conf/nips/BrownMRSKDNSSAA20, zhao2023survey} has enabled story generation models to produce plausible stories \citep{tan-etal-2021-progressive, zhang-etal-2022-persona, DBLP:conf/emnlp/YangTPK22}. In fact, the best models have been found to produce stories that are virtually indistinguishable from those written by humans \citep{karpinska-etal-2021-perils, dou-etal-2022-gpt, DBLP:journals/corr/abs-2301-09790}.
However, progress in the development of automatic evaluation metrics has not had the same momentum \citep{guan-etal-2021-openmeva}.
%JHL: I'm commenting out the following line because, in spite of the lack of story evaluation metrics, story generation has progressed much in the past few years, and so the argument that it impedes the development of story generation isn't quite true
%, and the lack of reliable metrics  impedes the development of story generation .
Although human evaluation remains the gold standard, it is often slow, expensive, and difficult to reproduce \citep{DBLP:journals/csur/SaiMK23}. Therefore, there is a pressing need for better automatic methods to evaluate story quality.

\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/Examples-add_typos.pdf}
         \caption{Perturbation ``Add typos'' affects the higher quality story (top) more.
         }
         \label{fig:addtypos}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/Examples-remove_keyentity.pdf}
         \caption{Stories conditioned on the title ``I always go to the local supermarket''. Perturbation ``Remove relevant words'' affects the highly related story (top) more while not affect the unrelated one (bottom).
         % Higher quality story is affected more than lower quality one.
         }
         \label{fig:removekeyentity}
     \end{subfigure}
        \caption{
        Scenarios where higher quality stories (top) are 
        affected more than lower quality ones (bottom) through aspect-specific perturbations (fluency: ``Add typos''; relatedness: ``Remove relevant words'').
        Generative likelihood for original/perturbed story is in blue/green circle, and
        the \deltascore value is in orange circle. 
        % \miao{Better to give examples that show aspect-aware perturbations. [Changed]}
        %JHL2: make the bottom 'it is a nice dog' -> 'it is a nice' ? (the current example doesn't remove any key entity), and also make the score lower? 0.75 seems very high for an unrelated story.
        % Zhuohan: Yes, the idea is that the unrelated story will not be affected that much by perturbations. Therefore, it is not affected here.
        }
        \label{fig:examples}
\end{figure}

The predominant evaluation metrics for story evaluation have their origins in other natural language generation (NLG) tasks, including BLEU \citep{papineni-etal-2002-bleu} and NIST \citep{NIST} for machine translation and ROUGE \citep{lin-2004-rouge} for summarization. Nonetheless, recent efforts have led to the development of novel metrics for story evaluation, aiming to quantify story coherence \citep{guan-huang-2020-union, ghazarian-etal-2021-plot} or learn human preferences \citep{DBLP:conf/emnlp/ChenVTMN22}.
Other works have directly utilized the likelihood of a story under a PLM \citep{DBLP:conf/nips/VaswaniSPUJGKP17, han-etal-2022-go} or its conditional likelihood based on human references or other contextual factors, such as story title \citep{thompson-post-2020-automatic, DBLP:conf/nips/YuanNL21}. However, these approaches typically produce a single score that estimates the overall quality of the generated story. \citet{DBLP:conf/coling/ChhunCSC22} argue that the quality of a story is composed of multiple fine-grained aspects, such as fluency and adherence to commonsense, and that an overall quality score has limited utility for story evaluation.
%In other words, a metric that produces a low overall score for a story does not reveal whether the story has fluency issues or certain elements of the story violate commonsense \cite{DBLP:journals/corr/abs-2203-11131}.

In this paper, we propose \deltascore, an approach that assesses story quality by calculating the \textit{likelihood difference} under a PLM between a story and its perturbed form, with the idea that higher quality stories will be affected more by the perturbation compared to the lower quality ones.
To provide fine-grained assessment of story quality, we experiment with perturbations that target a particular aspect (e.g.,\ fluency).  \autoref{fig:examples} presents two examples to demonstrate the intuition of our approach:
% \miao{Give description about aspect-aware perturbations.} 
(1) 
%the story $\boldsymbol{s_1}$: ``Mike dna Jake aer best friends!'' contains two typos, which are less fluent than the story $\boldsymbol{s_2}$: ``Mike and Jake are best friends!''.
when we modify the two stories  in \autoref{fig:addtypos} by randomly adding typos, the fluent story (top) is impacted by the perturbation more;
% Therefore, 
(2) 
%\textbf{Relatedness}: for the title ``I always go to the local supermarket'', we have two stories,
%the story $\boldsymbol{s_1}$: ``The supermarket has various kinds of goods.'' is highly related to the title while the the story %$\boldsymbol{s_2}$: ``It is a nice day.'' is less related.
when we modify the two stories --- conditioned on the story title ``I always go to the local supermarket'' ---  in \autoref{fig:removekeyentity} by removing relevant words, the story that is more related to the title (top) is again more affected. 
Through our empirical analysis, we demonstrate that  \deltascore  outperforms a suite of current metrics in evaluating fine-grained story aspects. Our investigation also revealed an intriguing finding: one perturbation method (word jumbling) displays exceptional performance in capturing several aspects. This discovery implies that there may be strong interdependence among the aspects.
% One surprising finding is that one particular perturbation (word shuffling) appears to work very well to capture multiple aspects, suggesting that the fine-grained aspects may be mutually correlated.
%The key entity ``supermarket'' is removed from $\boldsymbol{s_1}$ while $\boldsymbol{s_2}$ is not affected.
% Perturbation ``Add typos'' is applicable to both stories, while it cause more damages to the higher quality one.
% Different from the previous works,
% we propose to evaluate generated stories with differentiating perturbations,
% which is we calculate the difference of conditional likelihood of the original stories and
% that of their perturbations.
% Besides, we propose to evaluate stories via differences between GPT3 likelihood instead of using likelihood directly as evaluation metrics 
% as we posit that higher quality stories are more sensitive to perturbations than
% lower quality ones.
%There are two benefits in evaluating stories via differences between likelihood:
%1) It can result in a statistically more stable evaluation metric that correlates better to human evaluation.
%2) Different aspects of story quality can be evaluated by designing different perturbations.

%Specifically, we use generative likelihood assigned by GPT-3.5\footnote{It is also called text-davinci-003, which is the largest and latest version of GPT-3 with an available API at the time of our experiments. Note that even though ChatGPT is released and popular all over the world, it does not yet provide an API to obtain its generative likelihood at the time of writing.} as advanced large language models have demonstrated spectacular performance of understanding narratives \citep{Qin2023IsCA}.


% \miao{To evaluate the effectiveness of \deltascore, we ..., and we get that ...}

%JHL2: don't think we need to spell out the contributions again, since i thought it's quite clear from the intro
% Zhuohan: Addressed

% Our contributions of the paper are summarized as follows:
% \begin{itemize}
% % \item We propose a novel evaluation technique that does not require any additional fine-tuning: $\Delta$Likelihood ($\Delta$LL) where we evaluate story quality via differences between generative likelihood from perturbations.
% \item We propose to use likelihood difference based on perturbations as a new approach for fine-grained evaluation of generated texts.
% %\item We propose a novel and unsupervised evaluation approach to measure fine-grained aspects of story quality.
% \item Experimental results indicate that our metric demonstrates a strong correlation with human evaluations in most aspects, surpassing the performance of advanced metrics across various story domains.

% Experimental results show that our metric correlates well with human judgements for most aspects, outperforming state-of-the-art metrics over several story domains.

% \item We demonstrate the technique is applicable to both auto-regressive models with GPT3 and sequence-to-sequence (seq2seq) models with BART.
% \item We explore various perturbations and our results show $\Delta$LLs can have a much better correlations to human evaluation than using likelihood as evaluation metrics directly by applying certain perturbations.
% \item We compare with extensive state-of-the-art (SOTA) evaluation metrics and find our best $\Delta$LL outperforms in all quality aspects of story evaluation.

% \item We show binary classification idea is not suitable for metrics as it can only produce scores that close to 0 and 1s, which is not suitable for story evaluation.
%\end{itemize}


\section{Related Work}
\label{sec:relatedwork}

\subsection{Automatic Story Evaluation}
The conventional evaluation metrics used in natural language processing mainly focus on measuring lexical overlap between machine-generated text and its human reference \citep{papineni-etal-2002-bleu, NIST, lin-2004-rouge}. However, such methods are limited in their ability to capture semantic similarity, and they are vulnerable to small changes in morphology or even typos, as highlighted by \citet{kaster-etal-2021-global}.
To overcome these limitations, recent methods have exploited the language understanding capability of pre-trained language models (PLMs) to capture semantic similarity by comparing embeddings \citep{zhao-etal-2019-moverscore, DBLP:conf/iclr/ZhangKWWA20}. However, these evaluation metrics still rely on a single human reference, which may not be appropriate for story evaluation, given that there are many ways to write a good story.

Therefore, researchers have started to focus on reference-free evaluation metrics that are tailored to evaluate stories. For instance, \citet{guan-huang-2020-union} train a binary classification model to distinguish between original stories and their negative perturbations, while \citet{ghazarian-etal-2021-plot} extend this idea by generating negative samples from manipulated story plots instead of heuristic rules. These metrics place emphasis on evaluating story coherence. More recently,
\citet{DBLP:conf/emnlp/ChenVTMN22} propose a new task that aims to evaluate human preference by training a model to differentiate highly-upvoted stories from lowly-upvoted ones, drawing labelled data from Reddit.
%JHL2: double check if the above detail (Reddit) is right
%Zhuohan: Yes, they did crawl data from Reddit.
This task is particularly useful for evaluating the subjective aspects of stories that cannot be easily captured by traditional metrics. 

% These methods work as a general method for natural language generation tasks, 
% however, as different tasks have different characteristics, 
% automatic metrics tailored for specific tasks such as summarization \citep{scialom-etal-2021-questeval}, data-to-text \citep{rebuffel-etal-2021-data}, and dialogue generation \citep{mehri-eskenazi-2020-usr} have to be proposed.
% There are also alternative evaluation metrics that evaluate the model holistically by comparing the difference of distributions between the learnt model and the dataset \citep{DBLP:conf/nips/PillutlaSZTWCH21, DBLP:conf/emnlp/DengKR22}

% \paragraph{Story Evaluation Metrics}
% Some evaluation metrics are specifically designed for story evaluation.


% \paragraph{Unified Evaluation Metrics}
Recent advancements in evaluation metrics have diverged from conventional approaches that provide a single score to assess overall quality. Instead, these novel metrics adopt a multifaceted perspective, incorporating various inputs such as given prompts, generated text, and additional contextual information \citep{DBLP:conf/emnlp/Zhong0YMJLZJH22}. For instance, text evaluation is approached as a text generation problem by \citet{DBLP:conf/nips/YuanNL21}, whereas \citet{deng-etal-2021-compression} leverage information alignment techniques to evaluate the quality of generated text.

Despite the diverse perspectives offered by these evaluation methods, they often fall short in capturing more intuitive aspects of text quality such as fluency. To overcome this limitation, \citet{DBLP:conf/emnlp/Zhong0YMJLZJH22} introduce an approach that treats text evaluation as a question answering task, training a language model to answer specific questions such as  ``\textit{Is this a fluent sentence?}''. \citet{DBLP:journals/corr/abs-2302-04166} propose a method that leverages the generalization capability of large language models for evaluation, by incorporating instructions such as ``\textit{Generate a fluent story for the given title}'' as part of the input to align the likelihood computation with a specific aspect of quality.

%In our work, we propose a novel evaluation approach that targets multiple story quality aspects, which to the best of our knowledge, is the first of its kind.

\subsection{Natural Text Perturbation} 
The use of perturbations is a conventional technique to generate negative samples for both discriminative \citep{guan-huang-2020-union} and generative \citep{DBLP:conf/emnlp/Zhong0YMJLZJH22} NLP tasks. To evaluate the behavioral performance of NLP models, \citet{ribeiro-etal-2020-beyond} proposes CheckList, a suite of negative examples constructed from a combination of perturbations. Similarly, \citet{DBLP:conf/emnlp/KarpinskaRTSGI22} devises a list of perturbation tests to assess the robustness of machine translation evaluation metrics.
Additionally, \citet{sai-etal-2021-perturbation} expand on the use of perturbations to assess the robustness of NLG evaluation metrics. To identify blind spots of model-based evaluation metrics, \citet{DBLP:journals/corr/abs-2212-10020} develop perturbation tests. Notably, all of these perturbations rely on heuristic rules. In contrast, recent adversarial attacks such as those proposed by \citet{li-etal-2020-bert-attack, morris-etal-2020-textattack} use language models to generate adversarial examples, which can also be considered a form of text perturbation. In our work, we explore perturbation for a different purpose: to evaluate fine-grained story qualities.

% Even though these approaches might produce much more dedicated negative samples,
% we do not consider these approaches, since the introduction of an additional black-box language models make the whole process less interpretable.

\section{\deltascore}
\label{sec:deltascore}

% Note: The most ideal way is to find specific perturbations that each works for one specific
% aspect/and some work for general, but I cannot find something like this works now.


%JHL: minor, but maybe some of the perturbation terms can be modified to sound a bit more natural/intuitive:
% Jumble -> Shuffle; GeneralWord -> ? (I thought this is replaced with a hypernym, but looking at the example (girl -> baby), I am not sure anymore); AntonymAttributes -> Antonym; LameWords -> Synonym?; LameSents -> Paraphrase?
\begin{table*}[t]
\centering
\small
\begin{tabular}{p{1cm} p{2.2cm} p{5.5cm} p{5.5cm}}
\toprule
\textbf{Aspect} & \textbf{Perturbation} & \textbf{Original story} & \textbf{Perturbed story} \\
\midrule
\multirow{2}{1cm}[-0.5ex]{{Flu.}} & {Typo} & he went to see what the problem was & he went to see whta the problem was \\
\cmidrule{2-4}
 & {SubjVerbDis} & he is the best student in the classroom . & he am the best student in the classroom . \\
 \midrule
 \multirow{2}{1cm}[-2.0ex]{{Coh.}} & {Jumble} & We play badminton every evening . & badminton every We evening play . \\
 \cmidrule{2-4}
  & \multirow{2}{1cm}[0ex]{{SentReorder}} & she did n't intend to buy anything . unfortunately she has poor impulse control ... & unfortunately she has poor impulse control . she did n't intend to buy anything ... \\ 
 \midrule
 \multirow{2}{1cm}[-0.5ex]{{Rel.}} & \underline{RmRelWords} & The supermarket has various kinds of goods & The has various kinds of goods  \\
 \cmidrule{2-4}
  & \underline{StoryReplace} & The supermarket has various kinds of goods & It is a nice day to hang out  \\
 \midrule
 \multirow{2}{1cm}[-3.0ex]{{Log.}} & \multirow{2}{1cm}[0ex]{{Antonym}} & The boy got the gift he always wanted, he was so happy .  & The boy got the gift he always wanted, he was so sad . \\
 \cmidrule{2-4}
  & \multirow{2}{1cm}[0ex]{\underline{Commonsense}} & they took me down to the lake . i threw my line out and caught several worms ... & they took me to the moon. i threw my line out and caught several stars ... \\
 \midrule
 \multirow{2}{1cm}[-1.0ex]{{Int.}} & \multirow{2}{1cm}[-1.0ex]{\underline{BlanderNarrative}} & i felt really angry, talked to my estranged father , and he gave me a gun! But I knew violence is not a solution here . & I felt upset and talked to my father about it . He advised me to handle the situation calmly , so I decided not to resort to violence . \\
 \bottomrule
\end{tabular}
\caption{Summary of perturbations that target a story quality aspect: Fluency (Flu.), Coherence (Coh.), Relatedness (Rel.), Logicality (Log.), and Interestingness (Int.).
For ``Relatedness'' example stories, they are conditioned on the title ``I always go to the local supermarket''.
 \underline{Underlined perturbations} are original methods we propose.
}
\label{table:perturbation}
\end{table*}

% \subsection{GPT3Score}
% Unlike PRISM \citep{thompson-post-2020-automatic} or BARTScore \citep{DBLP:conf/nips/YuanNL21}, we did not use seq2seq pre-trained models.
% GPT3 is a pure generative autoregressive decoder, which do not take condition and predict the current purely based on previous tokens.


%JHL: definition of story condition c seems odd; typically it's the story title or something, no (not sure why it says it can be human reference).
% Answer: For condition, I meant to say people usually use conditional likelihood to evaluate the text (like BARTScore). The condition can be the source, therefore, it is reference-free evaluation. Or the condition can be the reference, therefore, it is refrence-based evaluation.

We now describe the idea of our approach.
Given a story condition (e.g.\ story title) $\boldsymbol{c}=c_1, ..., c_n$ containing $n$ tokens, a model-generated story $\boldsymbol{s}=s_1, ..., s_m$ containing $m$ tokens, and a perturbed story $\boldsymbol{s}' = s'_1, ..., s'_{m'}$ containing $m'$ tokens, \deltascore calculates the likelihood difference under a language model:
\begin{equation}
    \operatorname{\deltascore}(\boldsymbol{s}) = \log p(\boldsymbol{s}|\boldsymbol{c}) -  
    \log p(\boldsymbol{s}'|\boldsymbol{c})
\end{equation}
where $p(\boldsymbol{s}|\boldsymbol{c})$ represents the likelihood of $\boldsymbol{s}$ conditioned on $\boldsymbol{c}$ under a language model. In our experiments, we investigate several large language models with varying architectures (see \autoref{subsec:likelihood_calculation}) and perturbation techniques that are designed to target specific aspects (see \autoref{subsec:perturbation}).

%\footnote{All tokens to represent the title, story and modified story are all from the same vocabulary and there could be different vocabularies for different LLMs.}

%different quality aspects of $\boldsymbol{s}$ with the difference between likelihoods of $\boldsymbol{s}$ and $\boldsymbol{s}' = s'_1, ..., s'_{m'}$ which contains $m'$ tokens and is based on a specifically designed perturbation. For each quality aspect of $\boldsymbol{s}$, we first obtain perturbation of the story for the aspect (Section \ref{sec:perturbation}), and then the final score is calculated as the likelihood difference between $\boldsymbol{s}$ and $\boldsymbol{s}'$. The log-likelihood difference is defined as:


% As GPT-3.5 is good at story narrative understanding, we use it as the language model without fine-tuning to obtain the generative likelihood of both $\boldsymbol{s}$ and $\boldsymbol{s}'$. 

\subsection{Two Different Likelihood Calculations}
\label{subsec:likelihood_calculation}

%JHL: revise this section a bit to move away from GPT3-5, and instead say the formulation is based on autoregressive language models (e.g. GPT3). The core idea of DELTASCORE is likelihood difference, and the specific LM we use is flexible so we should define DELTASCORE in a more general manner.


We now explain how we compute $p(\boldsymbol{s}|\boldsymbol{c})$
with encoder-decoder PLMs (e.g.\ BART \citep{lewis-etal-2020-bart} and T5 \citep{DBLP:journals/jmlr/RaffelSRLNMZLL20}) and decoder PLMs (e.g.\ GPT-3 \citep{DBLP:conf/nips/BrownMRSKDNSSAA20}). $p(\boldsymbol{s'}|\boldsymbol{c})$ is computed in the same way and we omit it for brevity.

Denoting language model parameters as $\theta$, we compute \deltascore as follows for
encoder-decoder PLMs:
\begin{align}
\log p(\boldsymbol{s}|\boldsymbol{c}) &= \frac{1}{m}\sum_{t=1}^{m}\ \operatorname{log} p(s_t|\boldsymbol{s}_{<t}, \boldsymbol{c}, \theta)
%\log p(\boldsymbol{s}'|\boldsymbol{c}) &= \frac{1}{m'}\sum_{t=1}^{m'} \operatorname{log}\ p(s'_t|\boldsymbol{s}'_{<t}, \boldsymbol{c}, \theta)
\end{align}
where $t$ denotes timestep in the sequence, and $\boldsymbol{s}_{<t}$ denotes all tokens before the current timestep. Intuitively, the story condition $c$ is captured by the encoder, and the likelihood of the story $s$ is produced by the decoder.


For decoder PLMs, we concatenate $\boldsymbol{c}$ and $\boldsymbol{s}$/$\boldsymbol{s}'$ to form a sequence $\boldsymbol{x}$ ($x_1, ..., x_{n+m} = c_1, ..., c_n, s_1, ..., s_m$) to compute \deltascore:
\begin{align}
\log p(\boldsymbol{s|\boldsymbol{c}}) &= \frac{1}{m}\sum_{t=n+1}^{n+m}\ \operatorname{log}\ p(x_t|\boldsymbol{x}_{<t}, \theta)
%\\
%\log p(\boldsymbol{s}'|\boldsymbol{c}) &= \frac{1}{m'}\sum_{t=n+1}^{n+m'} \operatorname{log}\ p(x'_t|\boldsymbol{x}'_{<t},\theta)
\end{align}
\label{eqn:decoderlikelihood}

This formulation means we feed the full sequence including the story condition $\boldsymbol{c}$ and story $\boldsymbol{s}$ as input to the decoder-only PLM, although when computing the story likelihood, we only consider the conditional probabilities for the $\boldsymbol{s}$ tokens.
%Note here we ignore the token probabilities of the title $\boldsymbol{c}$; therefore, the timestep starts from $n+1$.
%We assign the equal weights to each token for \deltascore following \citet{DBLP:conf/nips/YuanNL21}.
% \footnote{We also tried various normalization approaches, such as PenLP and NormLP \citep{lau-etal-2020-furiously},
% but they did not yield constant performance improvement; 
% therefore, we decide to use the uniform normalization.}

% In this paper, we explore GPT3 \citep{DBLP:conf/nips/BrownMRSKDNSSAA20} as the
% representative for auto-regressive models.
% We refer evaluating with the generation likelihood from GPT3 to GPT3Score, therefore:

% \begin{align}
% \operatorname{GPT3Score}(\boldsymbol{s}) = \frac{1}{m}\sum_{t=n+1}^{n+m}\ \operatorname{log}\ p(x_t|\boldsymbol{x}_{<t}, \theta) \nonumber
% \end{align}

% We explore BART \citep{lewis-etal-2020-bart} as the
% representative for seq2seq models.
% Evaluating with the generative likelihood from BART is the same as BARTScore \citep{DBLP:conf/nips/YuanNL21}.

% \begin{align}
% \operatorname{BARTScore}(\boldsymbol{s}) = \frac{1}{m}\sum_{t=1}^{m}\ \operatorname{log}\ p(s_t|\boldsymbol{s}_{<t}, \boldsymbol{c}, \Phi) \nonumber
% \end{align}

\subsection{Perturbations on Story Aspects}
\label{subsec:perturbation}

We follow \citet{DBLP:journals/corr/abs-2301-09790} to assess five fundamental aspects of story quality: fluency, coherence, relatedness, logicality, and interestingness.
To this end, we survey perturbation methods from the literature \citep{ribeiro-etal-2020-beyond, sai-etal-2021-perturbation, guan-etal-2021-openmeva, DBLP:journals/corr/abs-2212-10020} and attempt to align them to one of these five aspects. For some aspects, we also propose new perturbation methods. We now describe each aspect and its associated perturbation methods; a summary of these methods and examples is given in \autoref{table:perturbation}.

%We list all the (aligned) perturbation methods used in our experiments (with examples) in \autoref{table:perturbation}. Note that we also introduce new perturbation methods for some of these aspects (underlined methods).
%However, most of previous perturbations were not originally proposed to target story aspects.
%Therefore, we propose several new perturbation approaches underlined in \autoref{table:perturbation}.

%Most of our approaches are developed with the assitance of ChatGPT, due to the difficulty of writing heuristic rules for perturbations in certain aspects.
%Our preliminary experiments demonstrate that ChatGPT\footnote{https://chat.openai.com} is capable of producing promising perturbation results.
%The details of the prompts used for these perturbations can be found in \autoref{appendix:perturbationprompts}.


% We first classify conventional perturbations \citep{sai-etal-2021-perturbation, guan-etal-2021-openmeva, DBLP:journals/corr/abs-2212-10020} into these aspects based on their features.

%JHL: since we've defined story condition c previously, let's make the terminology consistent throughout the paper when we refer to it (i.e. avoid using another term like 'prompts' to refer to c)

%JHL2: let's rewrite this section a bit. my suggestion: for each aspect, write a paragraph of text that explains the aspect, talk about the different perturbations in the literature that targets the aspect, and then spell out explicitly all the methods that we experiment in this paper (if we propose new methods here, detail them too). E.g. in fluency it looks like we test two perturbation methods 'typo' and 'subjverbdis', and so we should say very clearly these are the two methods where we will present results (which might be in the appendix but that's OK). When explaining the methods we use, spell out the hyper-parameters (e.g. perturbation degree) too (footnoting the actual values, and be sure to justify how we arrive at those values as well)



\paragraph{Fluency}
assesses the readability of sentences in the story. Perturbations modify the text at the word or phrase level. We use two perturbation approaches from \citet{ribeiro-etal-2020-beyond}: 1) \textit{Typo}, where we randomly substitute a character with an adjacent one in the text, and 2) \textit{Subject-verb disagreement (SubjVerbDis)}, where we modify the verbs in a sentence so that they no longer agree with their subjects.

\paragraph{Coherence}
evaluates how well the sentences in the story are connected. Perturbations modify the text at the sentence level to reduce how well the narrative flows from sentence to sentence. We use two perturbation approaches from \citet{sai-etal-2021-perturbation}: 1) \textit{Jumble}, where we randomly shuffle words within the story, and 2) \textit{Sentence Reorder (SentReorder)}, where we randomly shuffle the sentences within the story.

\paragraph{Relatedness}
focuses on the extent to which the story is relevant to the given condition (e.g.\ story title). Perturbations targeting relatedness alter the story to reduce its association with its condition. We propose two new methods: 1) \textit{Remove Relevant Words (RmRelWords)}, where we use ChatGPT\footnote{https://chat.openai.com} to identify words related to the given title and then remove them from the story, and 2) \textit{Story Replacement (StoryReplace)}, where we replace the original story with another random story with a different story condition, {with a requirement that the likelihood of the random story is similar to that of the original story under a PLM (without considering their story conditions)}. The rationale of introducing the requirement is that we want the random story to have a similar length and style to the original story.
%where conditions with the most similar story quality, as determined by the generative likelihood of the story without its condition.
%Note that it is not ideal to randomly replace a story because the conditional likelihood is not only determined by how related the story is to the given condition but also by the quality of the story itself.

\paragraph{Logicality}
focuses on the extent to which the story complies with commonsense. Perturbations targeting logicality introduce elements into the story that contradict commonsense. We adopt one  approach from \citet{guan-etal-2021-openmeva}: \textit{Antonym}, where we randomly replace the word with its antonym; and propose a new approach: \textit{Commonsense} , where we use ChatGPT to modify some story elements to violate commonsense.

\paragraph{Interestingness}
measures how predictable the events unfold in the story, which is perhaps the most subjective aspect. We propose one approach: \textit{Blander Narrative} , where we use ChatGPT to modify a story to make the narrative less interesting.

%JHL3: for the methods where we use chatgpt, did we use the GUI or the API (gpt3.5-turbo)? I assume is the former since the footnote points to the GUI (but if latter we should say we use gpt3.5-turbo on the API)
%Zhuohan: I was using gpt3.5-turbo here, and I have indicated that in the paper.
The ChatGPT\footnote{We call the API with gpt-3.5-turbo.} instructions for the 3 perturbation methods (\textit{RmRelWords}, \textit{Commonsense} and \textit{BlanderNarrative}) are detailed in \autoref{appendix:perturbationprompts}. For \textit{Typo}, \textit{Jumbo} and \textit{Antonym}, we can control the degree of perturbation, and this parameter is tuned in \autoref{subsec:preliminaryexploration}.


%For instance, we can specify the percentage of words to shuffle in jumble. 
%We will explore the effect of perturbation degrees later in \autoref{subsec:parameterimpacts}.
% We set the degrees to 0.4, 0.9, and 0.8 for typo, jumble, and antonym perturbations, respectively. 
% In \autoref{subsec:PerturbationDegree}, we investigate the effects of varying the perturbation degrees.
%JHL2: we don't need the following line here; we mention this when we present the results in section 4, and then say full results (with all perturbation methods) in appendix.
% Zhuohan: Addressed.
% However, due to space constraints, we only present a selection of perturbations in \autoref{table:perturbation}.

% \begin{itemize}
%     \item \textbf{Fluency:} Focus on modifications at the word or phrase level, e.g.\ by change to incorrect verb tenses/subject-verb agreement or repeating word/phrases.
%     \item \textbf{Coherence:} Focus on modifications at the sentence level to reduce how well the narrative flows from sentence to sentence, e.g.\ by repeating sentences, replacing  sentences from unrelated stories, or reordering sentences. 
%     \item \textbf{Relatedness:} Alter the story reduce its association with the story condition, e.g.\ by removing key information relevant to the story condition or replacing the whole story with another random story.
%     \item \textbf{Logicality:} Introduce elements into the story that contradict commonsense, e.g.\ by including entities that defy the laws of physics or events that go against cultural norms (``go trick or treating'' on ``Christmas'').
%     \item \textbf{Interestingness:} Alter the story so that it is less interesting to read, e.g.\ by changing the style to use blander language or narrative to be predictable.
% \end{itemize}

% As these quality aspects are not completely orthogonal, a perturbation on one aspect could also impact another aspect even if the perturbation is designed for the original aspect.
% Some quality aspects, especially interestingness, are relatively more subjective and dependent on individual perspectives and preferences. Therefore, our taxnomy might not be applicable to general domains.

% \begin{itemize}
% \item \textbf{RmkeyEntities:} We leveraged ChatGPT to extract all entities related to the given title and subsequently remove them from the story.
% \item \textbf{StoryReplace:} We selected a story from different story conditions with the most similar story quality, as determined by the generative likelihood of the story without its condition.
% \item \textbf{Commonsense:}
% \item \textbf{BlanderNarrative:} We requested ChatGPT to modify stories with minimal changes to render the narratives less interesting.
% \end{itemize}

%Due to the interdependence of these quality aspects, perturbations designed for one aspect can potentially affect other aspects. In particular, aspects such as interestingness, are more subjective and influenced by individual perspectives and preferences. Hence, our taxonomy may not be universally applicable to all domains.



% To address the relatedness aspect, we propose replacing the entire story. However, randomly replacing the story is not feasible as we cannot guarantee the relevance to the given prompts or the quality of the replaced stories. To mitigate this, we select all stories that are produced from different prompts for each given story and compute their likelihood without prompts from GPT-3.5 to assess their quality. We then calculate the absolute value of the difference between the current story and all other stories and select the one with the closest quality.

% For logicality, we propose replacing certain elements of a story to create a violation of commensense while maintaining its fluency and coherence. To achieve this, we rely on ChatGPT by asking it to revise the story to create minimal changes that do not make sense using the prompt ``Revise the following story such that certain elements does not make sense. The revision should be minimal, e.g. by changing a few words.''. Examples of these revisions can be found in \autoref{table:perturbation}, and we test more perturbations including Add Negation (AddNe), Antonym (Anton).

%JHL: i agree with the point about interestingness perhaps being subjective, but don't quite understand the following point (temporarily highlighting this line out for now)
%Answer from Zhuohan: What I am trying to say is that what we believe is interesting / what we believe these perturbations might affect interestingness might not be really applicable to others. Also, these might not be able to affected by our annotations. This also apply to other aspects, such as logicality and relatedness. But in generally, I believe interestingness is the most subjective aspect here.


\section{Experiments}

%In this section, we provide an overview of the meta-evaluation datasets used in our experiments as well as the evaluation metrics we compare \deltascore to.

\begin{table}[t]
\centering
\small
\begin{tabular}{p{0.9cm} p{2.3cm} p{3.1cm} }
\toprule
\textbf{Dataset} & \textbf{Condition} & \textbf{Story}  \\
\midrule
ROC & [FEMALE] dad took me fishing . & we sat in a spot and waited for days ... \\
\midrule
 WP & tell me a story where the first line and last line ... & as i walked into the house , i was assailed by the smell of aging ... \\
 \bottomrule
\end{tabular}
\caption{Sampled examples of given story condition and its generated story for each dataset.
}
\label{table:dataexample}
\end{table}

% The details can be found in \Needcite{Appendix}.

% \paragraph{Overall Quality Scores}

% %JHL: given the narrative in the intro, I no longer think we need this set of results (OPENMEVA). [Deleted]
% We also use manually annotated stories from OpenMEVA \citep{guan-etal-2021-openmeva}, which covers ROC and WP.
% It contains various generation models including: 
% 1) a \textbf{Seq2Seq} model \citep{DBLP:conf/nips/SutskeverVL14},
% 2) \textbf{Fusion} \citep{fan-etal-2018-hierarchical},
% 3) \textbf{Plan\&Write} \citep{DBLP:conf/aaai/YaoPWK0Y19},
% 4) fine-tuned \textbf{GPT-2} \citep{radford2019language} and
% 5) \textbf{KGGPT2}.
% % They randomly sample 200 stories from test sets of ROC and WP for story generation,
% % therefore,
% % MANS contains 2 $\times$ 200 $\times$ 5 = 2,000 annotated stories.
% % They use Amazon Mechanical Turk (AMT)\footnote{\url{https://requester.mturk.com/}} for human judgements and
% They ask the annotators to rate each story with a 5-point Likert scale in terms of the overall quality.\footnote{
% The authors of OpenMEVA also asked annotators to select the types of errors in the story, such as
% \textit{repetitive plots},
% \textit{unrelated events},
% \textit{conflicting logic},
% or \textit{chaotic scenes}.
% However, these error types have not bee released and as such we only use overall quality labels in this work.}

% We randomly sample 20 conditional contexts (e.g.,\ titles) from each dataset 
% and collect stories generated by all models for human evaluation.
% Each story (including human-written one) is judged by 3 annotators, 
% and so we have 320 annotated stories in total (140/100/80 for ROC, WP and CNN, respectively).

% Recently, \citet{DBLP:conf/coling/ChhunCSC22} propose
% a new story evaluation benchmark, HANNA.
% However, they present both human reference and generated story to annotators in AMT
% and ask annotators to judge the generated story.
% We concern that such way can make annotators give higher scores to stories that
% ``reads similar'' to given human reference and thus,
% favoring reference-based evaluation metrics and their results do show
% reference-based evaluation metrics have a better performance on HANNA.
% Therefore, we do not include HANNA as another baseline for GPTScore.

% \subsection{Compared Evaluation Metrics}

\begin{table}[t]
\centering
\small
\begin{tabular}{p{1.0cm}p{1.3cm} p{0.6cm} p{0.9cm} p{1.5cm}}
% \begin{tabular}{cllll}
\toprule
\textbf{Arch.} & \textbf{Model} & \textbf{Size} & \textbf{\#Data} & \textbf{Objectives}  \\
\midrule
 \multirow{3}{2cm}[0.5ex]{En-De} & BART & 406M & 160GB & Denoising \\
 \cmidrule{2-5}
  &  FLAN-T5  & 11B & - & Denoising \\
 \midrule
\multirow{4}{2cm}[-2.5ex]{De} & BLOOM & 7B & 366BT & LM \\
\cmidrule{2-5}
 & LLaMA &  65B & 1.4TT & LM \\
\cmidrule{2-5}
 & OPT &  66B & 180BT & LM \\
 \cmidrule{2-5}
 & GPT-3.5 & 175B & 300BT & LM \\
 \bottomrule
\end{tabular}
\caption{ Summary of PLMs, classified by their architecture (Arch.) as  encoder-decoder (En-De) or decoder (De). ``Size'' indicates model parameters.
\#Data indicates the pre-trained data scale (``GB'' $=$ gigabyte; ``BT'' $=$ ``billion tokens''; and ``TT'' $=$ ``trillion tokens''). 
 ``LM'' indicates causal language modeling objective.
}
\label{table:modeldetails}
\end{table}

\begin{table}[t]
\centering
% \begin{adjustbox}{max width=\linewidth}
\small
% \begin{tabular}{p{1.8cm} p{1.5cm} p{1.0cm} p{1.0cm} }
\begin{tabular}{p{1.7cm}lcccc}
\toprule
 \textbf{Objective} & \textbf{Metric} & \textbf{FT}  & \textbf{B/F} & \textbf{ST} & \textbf{MS}   \\
\midrule
  \multirow{3}{2cm}[-1ex]{Similarity} & BLEU & \xmark & B & \xmark & \xmark \\
 \cmidrule{2-6}
 & BERTScore & \xmark &  B & \xmark & \xmark \\
 \cmidrule{2-6}
 & MoverScore & \xmark & B & \xmark & \xmark \\
 \midrule
  \multirow{3}{1cm}[-5ex]{Discriminative} & UNION & \cmark & F & \cmark & \xmark \\
 \cmidrule{2-6}
  & MANPLTS & \cmark & F & \cmark & \xmark \\
 \cmidrule{2-6}
 & StoryER & \cmark & F &\cmark & \xmark \\
 \cmidrule{2-6}
  & CTC & \cmark & F & \xmark & \cmark \\
 \cmidrule{2-6}
 & UNIEVAL & \cmark  & F & \xmark  & \cmark \\
 \midrule
 \multirow{2}{1.5cm}[-0.5ex]{Generative} & BARTScore & \xmark & F & \xmark & \cmark \\
 \cmidrule{2-6}
 & GPTScore & \xmark & F & \xmark & \cmark \\ 
 \bottomrule
\end{tabular}
% \end{adjustbox}
\caption{Statistics of Compared evaluation metrics. 
``FT'' indicates whether the metric requires additional synthetic data to fine-tune on.
``B/F'' indicates whether the metric is reference-based (B) or reference-free (F).
``ST'' indicates whether the metric is originally designed for story evaluation.
``MS'' indicates whether the metric produces scores that consider multiple aspects.
}
\label{table:metricdetails}
\end{table}
%JHL3: I updated CTC and Bartscore to mark them as referencce-free (F) in the table, because when we adapt them to our task we don't use the reference. Please correct if it's wrong
%Zhuohan: yes, it is correct that we only use the reference-based vestion of CTC and Bartscore in the paper, but I just wonder if it is necessary for us to note that these methods can be used both as reference-based and reference-free, while we only adopt reference-free one in our setting.





\begin{table*}[t]
\small
\centering
\begin{tabular}{clcccccccccc}
\toprule
\multirow{2}{*}[-1ex]{\textbf{Target Aspect}} & \multirow{2}{*}[-1ex]{\textbf{Perturbation}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
\cmidrule(lr){3-7}\cmidrule(lr){8-12}
 & & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} \\
\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}
-- & w/o perturbation & 17.3 & 31.0 & 22.8 & 35.2 & 20.2 & 26.8 & 27.0 & 32.1 & 34.6 & 17.8 \\ 
\midrule
\multirow{2}{*}[0ex]{Fluency} & Typo & 14.2 & \cellcolor{green!25}31.9 & \cellcolor{green!25}23.5 & \cellcolor{green!25}36.0 & \cellcolor{green!25}23.2 & \cellcolor{green!25}36.5 & \cellcolor{green!25}42.9 & \cellcolor{green!25}38.1 & \cellcolor{green!25}47.8 & \cellcolor{green!25}35.5 \\
& SubjVerbDis & 5.5 & 13.9 & 8.8 & 13.0 & 7.8 & 13.0 & 15.9 & 4.7 & 10.7 & 6.1  \\
\midrule
\multirow{2}{*}[0ex]{Coherence} & Jumble & \cellcolor{green!25}17.6 & \cellcolor{green!25}37.1 & 16.8 & 34.1 & \cellcolor{green!25}22.5 & \cellcolor{green!25}35.9 & \cellcolor{green!25}36.0 & 31.9 & \cellcolor{green!25}36.3 & \cellcolor{green!25}28.1   \\
& SentReorder & 4.4 & 17.7 & 4.8 & 20.3 & 8.9 & 18.6 & \cellcolor{green!25}27.3 & 20.5 & 15.1 & \cellcolor{green!25}25.4 \\ 
\midrule
\multirow{2}{*}[0ex]{Relatedness} & RmRelWords & 15.9 & 16.9 & 13.3 & 9.6 & 7.6 & \cellcolor{green!25}36.5 & \cellcolor{green!25}27.7 & 31.7 & 33.5 & \cellcolor{green!25}21.9 \\
& StoryReplace & 6.2 & 2.2 & 9.1 & 9.6 & 7.8 & \cellcolor{green!25}28.5 & 26.6 & 27.6 & \cellcolor{green!25}38.5 & \cellcolor{green!25}23.8 \\ 
\midrule
\multirow{2}{*}[0ex]{Logicality} & Antonym & 14.4 & 16.9 & 11.4 & 16.2 & 11.6 & \cellcolor{green!25}31.6 & \cellcolor{green!25}33.5 & \cellcolor{green!25}35.0 & \cellcolor{green!25}35.0 & \cellcolor{green!25}25.3  \\
& Commonsense & 16.9 & 15.8 & 17.5 & 2.2 & 3.8 & 23.6 & \cellcolor{green!25}34.0 & 23.3 & 32.6 &\cellcolor{green!25}26.5 \\
\midrule
Interestingness & BlanderNarrative & 6.7 & 10.4 & 11.5 & 1.5 & 0.7 & 13.9 & 17.7 & 13.0 & 19.3 & 7.2 \\
 \bottomrule
\end{tabular}
\caption{Story-level Kendall correlation ($|\tau|$) between \deltascore with LLaMA and in-house judgements. 
We \colorbox{green!30}{highlight}  instances where \deltascore outperforms vanilla likelihood (``w/o perturbation'').}
\label{table:multiperturbations_inhouse}
\end{table*}



%JHL2: add another column to show which aspect a perturbation targets, and added some lines to seperate the different types of perturbations
% Zhuohan: Addressed


%JHL: ahhhh i am a little surprised to see the CNN story condition is human story here! This is not good, the prompt qualitatively is far too different from the other domains (i.e. in ROC and WP the story condition is a short text, but the CNN story condition has a lot more information and so there's less room in terms of 'creative writing' for the model. This may be the reason why CNN/Dailymail has very very different results! We might need to redo CNN experiments. Also, I suspect we'll run into issues with reference-based evaluation for CNN/dailymail (since the reference is only used as story condition)
%JHL: prompt is never really defined (I understand it's introduced in section 3 but it wasn't really defined there too); suggest we say explicitly what are the c's for the different domains here (e.g. ROC = first sentence; WP = a short prompt that gives the big picture of the whole story; etc and show some examples).

\subsection{Benchmarks}
\label{subsec:benchmark}

% The use of standardized benchmark datasets is crucial for advancing automatic story evaluation as they enable the evaluation metrics to be tested and compared by measuring their correlations to human evaluations.

%JHL3: we use the generated stories in our previous paper too, no? (not just human ratings)
%Zhuohan: yes, we use the story + human ratings, the current narrative is better.
We use the generated stories and human ratings collected by \citet{DBLP:journals/corr/abs-2301-09790} on two story datasets: ROCStories (ROC;  \citet{mostafazadeh-etal-2016-story}; 5-sentence simple stories) and WritingPrompts (WP; \citet{fan-etal-2018-hierarchical}; longer fictional stories written by users on Reddit).\footnote{\citet{DBLP:journals/corr/abs-2301-09790} also collected human judgements for CNN-Dailymail, but we do not use them in our study for two reasons: 1) the stories depict real-world events rather than fictional narratives, and 2) most of the language models we test have been trained on this dataset, and so there are potential circularity issues.} %JHL2: we can drop the table if need space
The story condition ($\boldsymbol{c}$) for ROC is the first/leading sentence; for WP it's the short paragraph that describes the idea of the story (``prompt''). We present two example stories from the two datasets in \autoref{table:dataexample}.

\citet{DBLP:journals/corr/abs-2301-09790} experiment with 6 story generation models that cover large models with prompt-based learning (e.g.\ GPT-3), smaller fine-tuned models (e.g.\ BART) and other methods that incorporate planning and commonsense \cite{guan-etal-2020-knowledge,guan-etal-2021-long,tan-etal-2021-progressive,xu-etal-2020-megatron}.
%We present some example stories from the two datasets in \autoref{table:dataexample}.
They then conduct human evaluation on five aspects --- fluency, coherence, relatedness, logicality, and interestingness --- judged using an ordinal scale from 1 (worst) to 5 (best). They recruit two different groups of annotators
%JHL3: did we also report their correlation? if so let's mention that here too
%Zhuohan: we did report the correlations in the Appendix, note that we did that becuase 
%JHL4: in that case let's print the correlation between the two groups in the appendix
--- in-house PhD students and crowdworkers --- to do the annotation and found similar results from the two groups, suggesting robustness in their annotation.
%\footnote{We also compute correlation between the two groups of workers and found high correlation; see Appendix X.}
%JHL4: this is annotator agreement within a group yea? we don't need that (we need cross-group correlation)
%The authors use one-vs-rest Pearson's $r$ to assess annotator agreement and report averagely 0.51/0.54 for ROC/WP, respectively with in-house annotators and 0.74/0.64 for crowdworkers.}
We use judgement from the first group to do preliminary exploration of optimal settings (e.g.\ assessing which perturbation methods or language models that work well; \autoref{subsec:preliminaryexploration}) and that of the second group to do a final comparison of our approach with existing evaluation metrics (\autoref{subsec:comparisonwithexistingmetrics}).

%For that reason, we pool the judgement from the two groups of workers to compute the mean rating of a story for a particular aspect. 
% We also present the results separately for the two groups of annotations in the Appendix XXX for completeness, noting that we find similar insights, thereby providing justification for pooling.


%while more detailed information about the datasets can be found in \autoref{appendix:datasetsdetail}.

% As mentioned in \autoref{sec:deltascore}, \deltascore can use both given story condition and human reference as the context for generative likelihood.
% However, open-ended generation tasks such as story generation often suffer from the known challenge of the one-to-many problem, where multiple plausible stories can be generated for a single prompt. Consequently, reference-based evaluation metrics, which are commonly used in other generation tasks, may not be ideal for evaluating story generation \citep{DBLP:conf/emnlp/Zhong0YMJLZJH22}. Hence, to address this issue, we utilize the given story condition as the conditioning factor $\boldsymbol{c}$ to generate the likelihood. More specifically, we use the first sentence and a short prompt as the condition for ROC and WP, respectively. 

\subsection{Language Models}
\label{subsec:languagemodels}

We select a set of representative PLMs to compute \deltascore. For encoder-decoder PLMs, we use BART and FLAN-T5 \citep{DBLP:journals/corr/abs-2210-11416}. For decoder PLMs, we use BLOOM \citep{Scao2022BLOOMA1}, LLaMA \citep{DBLP:journals/corr/abs-2302-13971}, OPT \citep{DBLP:journals/corr/abs-2205-01068}, and GPT-3.5.\footnote{We use text-davinci-003 in our experiments.} We use the largest possible variant whenever possible as we found larger models tend to work better in preliminary experiments. We present a summary of these language models in \autoref{table:modeldetails}.


\subsection{Competitors}
\label{subsec:competitors}
We next outline currently established evaluation metrics in the literature for comparison, classifying them into three categories: 1) \textit{similarity}, where the generated stories are compared to the given references, 2)  \textit{discriminative}, where a classifier is trained to distinguish between high and low quality stories, and 3) \textit{generative}, where generation likelihood is used to determine quality.

For similarity metrics, we have: \textbf{BLEU} \citep{papineni-etal-2002-bleu} that measures n-gram overlaps between stories and human references;
\textbf{BERTScore} \citep{DBLP:conf/iclr/ZhangKWWA20} and
\textbf{MoverScore} \citep{zhao-etal-2019-moverscore}, which measure semantic similarity from embeddings obtained from BERT \citep{devlin-etal-2019-bert}.

For discriminative metrics, we have: 
% In our story evaluation, we ask story quality-related questions for each aspect, which are listed in \autoref{appendix: unieval}.
\textbf{UNION} \citep{guan-huang-2020-union} which constructs negative samples of original stories using heuristic rules and trains a discriminator to differentiate them;\footnote{As they work with ROC and WP, we use their trained models without fine-tuning for our experiments.} 
\textbf{MANPLTS} \citep{ghazarian-etal-2021-plot} which is an extension of UNION that constructs ``better'' negative samples by manipulating storylines and generating alternate stories based on these manipulated storylines using a story generation model;\footnote{Like UNION, we use the models released by authors.}
\textbf{StoryER} \citep{DBLP:conf/emnlp/ChenVTMN22} which builds a classifier to learn human preference by training it to differentiate highly-upvoted stories from lowly-upvoted ones on Reddit;\footnote{In addition to the preference score, they also train their model to produce ratings and comments on predefined story aspects to provide explanations for the evaluation.}
%JHL: what is story comments?
% Answer: They additional generate comments on each aspect as explanations.
\textbf{CTC} \citep{deng-etal-2021-compression} that treats the evaluation task as an information alignment task;\footnote{We use the reference-free alignment approach in CTC (also called ``consistency'' in the original paper).}
and \textbf{UNIEVAL} \citep{DBLP:conf/emnlp/Zhong0YMJLZJH22} that frames the evaluation as a question answering task where different questions are asked to assess a particular aspect.\footnote{The question answering models are trained on text summarization and dialogue generation tasks and they are shown to have zero-shot transfer to be able to answer new questions; see \autoref{appendix:evaluationmetrics} for more details on how we adapt UNIEVAL for our task.} 

For generative metrics, we have:
\textbf{BARTScore} \citep{DBLP:conf/nips/YuanNL21} that evaluates generated text by calculating its conditional likelihood under BART;\footnote{BARTScore can evaluate various aspects of text quality by using different combinations of input and output. We use the reference-free version of BARTScore (i.e.\ $\boldsymbol{c} \rightarrow \boldsymbol{s}$).}
and \textbf{GPTScore} \citep{DBLP:journals/corr/abs-2302-04166} which computes the likelihood of the story
%JHL3: specify the exact version of GPT used
under GPT3 with additional prefix to target a particular aspect (see \autoref{appendix:evaluationmetrics} for more details).

% Might delete CTC if we lack space.


% We calculate both CTC (Consistency), which is s $\rightarrow$ c and
% CTC (Relevance), which is the product of alignments r $\rightarrow$ s and s $\rightarrow$ c,
% because the other settings in CTC are not applicable to our tasks.
% Our results show that CTC (Consistency) works consistently better than CTC (Relevance) (see Appendix).
% It shows references are not that important in open-ended story evaluation. 


 
%JHL2: we need to now describe all of these metrics, so that the reader can understand them. I see much of them is already in appendix, so let's move them back to the main narrative (maybe summarise them a bit more so it's a little bit more succinct). Without these descriptions, most of the description in the following paragraph don't make much sense.

%Some of these metrics are not originally proposed for story evaluation, and we try to adapt them to fit our task. For discriminative metrics, we use the models provided by the authors without additional fine-tuning. We use the reference-free version of CTC and BARTScore to ensure consistency in comparisons, namely CTC (Consistency) and BARTScore ($\boldsymbol{c} \rightarrow \boldsymbol{s}$). Our preliminary experiments showed that reference-free versions perform better than reference-based ones. For UNIEVAL and GPTScore, we adapted the questions and prompts accordingly, which can be found in \autoref{appendix:evaluationmetrics}.

We summarise all these metrics in \autoref{table:metricdetails}, showing whether they: require additional training or ground truth reference; are originally introduced for story evaluation; and can measure fine-grained story aspects.
%We show the classification in \autoref{table:metricdetails}, and details about the evaluation metrics can be found in \autoref{appendix:evaluationmetrics}.


\begin{figure}[t]
     \centering
     \includegraphics[width=0.48\textwidth]{figs/line_chart_ih.pdf}
        \caption{
        Impact of perturbation degree with LLaMA on in-house judgements for measuring coherence.
        }
        \label{fig:correlations}
\end{figure}

\begin{table*}[t]
\small
\centering
\begin{tabular}{lcccccccccc}
\toprule
\multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
\cmidrule(lr){2-6}\cmidrule(lr){7-11}
  & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} \\
\cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
\multicolumn{10}{l}{\textbf{\deltascore (with BART-large 406M)}}\\ 
w/o perturbation & 4.5 & 14.6 & 11.6 & 14.5 & 2.3 & 15.0 & 14.5 & 22.4 & 24.5 & 10.6  \\ 
 Jumble & \cellcolor{green!25}{11.4} & \cellcolor{green!25}{21.9} & \cellcolor{green!25}{14.6} & \cellcolor{green!25}{19.5} & \cellcolor{green!25}{13.1} & \cellcolor{green!25}{30.0} & \cellcolor{green!25}{21.6} & \cellcolor{green!25}{27.3} & \cellcolor{green!25}{33.6} & \cellcolor{green!25}{21.2} \\
 Typo & \cellcolor{green!25}{6.3} & \cellcolor{green!25}{17.7} & \cellcolor{green!25}{19.8} & \cellcolor{green!25}{17.5} & \cellcolor{green!25}{4.7} & \cellcolor{green!25}{24.5} & \cellcolor{green!25}{26.9} & \cellcolor{green!25}{30.2} & \cellcolor{green!25}{36.6} & \cellcolor{green!25}{24.3} \\
 Antonym & \cellcolor{green!25}{5.7} & 8.3 & 7.3 & 5.4 & \cellcolor{green!25}{6.9} & \cellcolor{green!25}{28.0} & \cellcolor{green!25}{26.4} & \cellcolor{green!25}{37.6} & \cellcolor{green!25}{31.8} & \cellcolor{green!25}{25.7} \\
\midrule
 \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL 11B)}}\\ 
  w/o perturbation & 16.2 & 19.4 & 14.7 & 23.1 & 8.9 & 19.9 & 14.1 & 23.4 & 20.8 & 4.8 \\
 Jumble & \cellcolor{green!25}{24.5} & \cellcolor{green!25}{36.1} & \cellcolor{green!25}{20.2} & \cellcolor{green!25}{27.9} & \cellcolor{green!25}{20.6} & \cellcolor{green!25}{31.7} & \cellcolor{green!25}{28.3} & 22.1 & \cellcolor{green!25}{34.4} & \cellcolor{green!25}{21.8} \\
 Typo & 11.3 & \cellcolor{green!25}{21.8} & 11.2 & \cellcolor{green!25}{28.0} & \cellcolor{green!25}{14.7} & \cellcolor{green!25}{29.0} & \cellcolor{green!25}{25.7} & \cellcolor{green!25}{33.1} & \cellcolor{green!25}{31.8} & \cellcolor{green!25}{18.0} \\
 Antonym & 10.7 & 12.9 & 8.9 & 10.1 & 7.9 & \cellcolor{green!25}{33.1} & \cellcolor{green!25}{34.0} & \cellcolor{green!25}{30.6} & \cellcolor{green!25}{34.2} & \cellcolor{green!25}{29.2} \\
\midrule
\multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
  w/o perturbation & 12.4 & 24.1 & 13.3 & 25.5 & 10.2 & 24.8 & 21.3 & 27.9 & 29.2 & 12.6 \\
 Jumble & \cellcolor{green!25}{25.0} & \cellcolor{green!25}{35.5} & \cellcolor{green!25}{14.2} & \cellcolor{green!25}{33.9} & \cellcolor{green!25}{25.4} & \cellcolor{green!25}{34.2} & \cellcolor{green!25}{34.8} & \cellcolor{green!25}{30.7} & \cellcolor{green!25}{33.6} & \cellcolor{green!25}{23.1} \\
 Typo & \cellcolor{green!25}{14.3} & \cellcolor{green!25}{31.6} & 12.2 & \cellcolor{green!25}{30.3} & \cellcolor{green!25}{14.5} & \cellcolor{green!25}{38.4} & \cellcolor{green!25}{40.9} & \cellcolor{green!25}{38.7} & \cellcolor{green!25}{44.7} & \cellcolor{green!25}{29.5} \\
 Antonym & 10.2 & 12.0 & 9.4 & 10.3 & \cellcolor{green!25}{12.2} & \cellcolor{green!25}{27.4} & \cellcolor{green!25}{31.5} & \cellcolor{green!25}{31.3} & \cellcolor{green!25}{32.9} & \cellcolor{green!25}{30.6} \\
\midrule
\multicolumn{10}{l}{\textbf{\deltascore (with LLaMA 65B)}}\\ 
w/o perturbation & 17.3 & 31.0 & 22.8 & 35.2 & 20.2 & 26.8 & 27.0 & 32.1 & 34.6 & 17.8 \\
 Jumble & \cellcolor{green!25}{19.7} & \cellcolor{green!25}{36.9} & 15.8 & \cellcolor{green!25}{35.9} & \cellcolor{green!25}{23.0} & \cellcolor{green!25}{36.8} & \cellcolor{green!25}{35.3} & \cellcolor{green!25}{32.9} & \cellcolor{green!25}{35.5} & \cellcolor{green!25}{27.7} \\
 Typo & 14.2 & \cellcolor{green!25}{31.9} & \cellcolor{green!25}{\textbf{23.5}} & \cellcolor{green!25}{36.0} & \cellcolor{green!25}{23.2} & \cellcolor{green!25}{34.3} & \cellcolor{green!25}{42.2} & \cellcolor{green!25}{35.2} & \cellcolor{green!25}{44.0} & \cellcolor{green!25}{31.8} \\
 Antonym & 13.3 & 15.9 & 10.6 & 14.2 & 11.0 & \cellcolor{green!25}{31.3} & \cellcolor{green!25}{37.9} & \cellcolor{green!25}{35.0} & \cellcolor{green!25}{36.2} & \cellcolor{green!25}{31.6} \\
\midrule
\multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
  w/o perturbation & 17.4 & 30.6 & 20.2 & 32.6 & 15.8 & 27.5 & 24.5 & 32.2 & 31.0 & 15.7 \\
 Jumble & \cellcolor{green!25}{\textbf{27.4}} & \cellcolor{green!25}{\textbf{44.2}} & \cellcolor{green!25}{21.6} & \cellcolor{green!25}{\textbf{41.4}} & \cellcolor{green!25}{\textbf{30.0}} & \cellcolor{green!25}{37.9} & \cellcolor{green!25}{38.7} & \cellcolor{green!25}{35.4} & \cellcolor{green!25}{39.5} & \cellcolor{green!25}{32.0} \\
 Typo & \cellcolor{green!25}{17.9} & \cellcolor{green!25}{39.6} & \cellcolor{green!25}{21.3} & \cellcolor{green!25}{38.7} & \cellcolor{green!25}{22.8} & \cellcolor{green!25}{39.0} & \cellcolor{green!25}{41.9} & \cellcolor{green!25}{\textbf{38.0}} & \cellcolor{green!25}{\textbf{45.6}} & \cellcolor{green!25}{30.4} \\
 Antonym & 15.8 & 19.0 & 13.8 & 14.9 & 12.9 & \cellcolor{green!25}{34.0} & \cellcolor{green!25}{40.3} & \cellcolor{green!25}{36.0} & \cellcolor{green!25}{39.4} & \cellcolor{green!25}{36.7} \\
\midrule
 \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5 175B)}}\\
 w/o perturbation & 21.3 & 31.2 & 18.7 & 28.6 & 18.2 & 36.2 & 32.9 & 36.0 & 37.1 & 23.3 \\
 Jumble & 18.7 & \cellcolor{green!25}{34.0} & \cellcolor{green!25}{23.3} & \cellcolor{green!25}{38.5} & \cellcolor{green!25}{29.7} & 35.0 & \cellcolor{green!25}{36.6} & \cellcolor{green!25}{36.1} & \cellcolor{green!25}{37.7} & \cellcolor{green!25}{33.5} \\ 
 Typo & 17.0 & \cellcolor{green!25}{35.1} & 11.7 & \cellcolor{green!25}{32.2} & \cellcolor{green!25}{26.3} & \cellcolor{green!25}{\textbf{41.7}} & \cellcolor{green!25}{\textbf{46.7}} & \cellcolor{green!25}{37.5} & \cellcolor{green!25}{45.0} & \cellcolor{green!25}{\textbf{37.3}} \\ 
 Antonym & 19.1 & 21.5 & 12.9 & 20.5 & 13.9 & 36.1 & \cellcolor{green!25}{41.2} & \cellcolor{green!25}{41.3} & \cellcolor{green!25}{39.6} & \cellcolor{green!25}{38.2} \\
 \bottomrule
\end{tabular}
\caption{Absolute value of Story-level Kendall correlation ($|\tau|$) between different metrics and in-house judgements. 
We \textbf{bold} the best scores for each aspect and \colorbox{green!30}{highlight} instances where \deltascore improves over vanilla likelihood (``w/o perturbation'').
}
\label{table:multiplemodels_inhouse}
\end{table*}

\begin{figure*}[t]
     \centering
     \includegraphics[width=0.9\textwidth]{figs/bar_chart.pdf}
        \caption{
        Absolute value of Story-level Kendall correlation ($|\tau|$) between different metrics and crowdworker ratings.
        % Similarity, Discriminative and Generative indicate the best performing evaluation metrics from such category.
        Higher bar indicates better performance.
        Red bars indicate our method (\deltascore).
        Blue bars indicate similarity based evaluation methods.
        Green bars indicate discriminative evaluation methods.
        Purple bars indicate generative evaluation methods.
        }
        \label{fig:comparisons}
\end{figure*}
%JHL3: need to use different colours now that we categorise the competitor metrics differently
%JHL3: let's add BARTscore here too, so we have 2 metrics for generative metrics
%Zhuohan: I just addressed this part.



% In order to test the generalization of our proposed approach, we extend it to other generative LLMs. In addition to the OPT and GPT-3.5 models that we demonstrated previously, we also include two representatives of encoder-decoder models, namely BART  as well as one more causal decoder model, .

%In our preliminary experiment, we observed that larger models tend to demonstrate better performance within the same model framework. As such, we applied the largest model size that we could run for each model. The model details are presented in \autoref{table:modeldetails}.


%JHL2: we need a section to talk about all the different LMs we use (and if there's some rationale in how we select them, spell them out too). In terms of models, I think we need GPT3.5, and maybe 2 open-source siblings of the same size (say OPT and LLAMA), and then 2  smaller models, like FLAN-T5 and vanilla BART (not the CNN finetuned one, although I vaguely the vanilla BART has some strange results, in which case maybe just drop BART entirely). Here remember to talk about whether they are encoder-decoder or decoder models, their training data, and also whether they have been instruction tuned.
% Zhuohan: Half Addressed, We might need to leave BART here as BART is more frequently used in evaluation metrics, while maybe we can delete BLOOM if we need space.
% Zhuohan: Might need to cite more papers to show that these models obtain very strong capabilities in other people' benchmarks.

% While some of these metrics were developed for other NLG tasks, we modify them slightly to make them suitable for evaluating story generation.
% We also indicate if the metric requires additional synthetic data to fine-tune the backbone model on and whether it requires human references when computing the scores.

% We compare \deltascore against representative evaluation metrics introduced in \autoref{sec:relatedwork}.




\section{Results}

We evaluate Kendall correlation at the story level, which involves comparing the predicted metric score versus the aggregated human rating for each story on a specific aspect. We use this as our primary metric due to the non-linear relationship between automatic and human metrics, as well as the ordinal scale employed in human judgments \citep{Kendall1938ANM}.
We explore different settings of our approach in \autoref{subsec:preliminaryexploration} and present a comparison of our (best) approach with competitor evaluation metrics in \autoref{subsec:comparisonwithexistingmetrics}. Note that we use a different ground truth for these two sets of results (in-house ratings for \autoref{subsec:preliminaryexploration} and crowdworker ratings for \autoref{subsec:comparisonwithexistingmetrics}, as explained in \autoref{subsec:benchmark}), to avoid tuning and testing on the same test set.
%To demonstrate generalization of our approach,
%we use two sets of benchmarks mentioned in \autoref{subsec:benchmarksandcomparedapproaches}, inhouse to explore impacts of different parameters (\autoref{subsec:parameterimpacts}) and crowdsource part to compare with existing SOTA metrics (\autoref{subsec:comparisonwithexistingmetrics}).

%JHL2: suggest we re-structure section 5 a bit better.
%JHL2: talk about the results here, saying that very interestingly, jumble, typo and antonym seem to consistently work better than the rest

\subsection{Preliminary Exploration}
\label{subsec:preliminaryexploration}

\paragraph{Perturbation Methods}
We first present how well the different perturbation methods (\autoref{subsec:perturbation}) compare against human judgments for the 5 aspects in  \autoref{table:multiperturbations_inhouse}. We use LLaMA as the PLM here. ``w/o perturbation'' means we compute the story likelihood under LLaMA directly.
%We employ LLaMA here as it is the latest open sourced LLM at the time of our experiment, and it has demonstrated the predominant performance in various downstream tasks, outperforming other LMs \citep{DBLP:journals/corr/abs-2302-13971}.
%We present the complete set of results in \autoref{table:multiperturbations_inhouse}, and visualize the performance gains of \deltascore over LLaMA in \autoref{appendix: correlations}.
%JHL3: let's wait until we have table 5 before we write any findings
We found some interesting results. The first is that perturbations that target a particular aspect may not always produce a higher correlation with human judgements when measuring that aspect (i.e.\  we don't see diagonal green highlights within a story domain). The second is that interestingness, the most subjective aspect, is the most challenging to measure, as the magnitude of the correlation numbers are generally lower compare to other aspects. And the last and perhaps most surprising observation is that a select few perturbation methods seem to work very well in evaluating most aspects: \textit{Typo}, \textit{Jumble} and \textit{Antonym}. For these reasons, we'll focus on these three methods for the rest of our experiments.

%some perturbations work better than others, despite their targeting of specific aspects of story quality. For example, sentenceReorder does not provide any improvement on \deltascore, possibly due to the focus of LLMs on local logic rather than the global narrative arc of a story. As such, reordering sentences, which does not affect the local logic, is not helpful in enhancing \deltascore. Furthermore, we observe that interestingness, the most subjective aspect, is the most challenging for \deltascore to improve, as designing a perturbation that targets it is difficult.



\paragraph{Perturbation Degree} %JHL2 here we talk about figure 4. Here we should be consistent with the previous paragraph to use the same LM.

We next investigate impact of perturbation degree using the top-3 performing methods (\textit{Typo}, \textit{Jumbo} and \textit{Antonym}) and present the results over ROC and WP in \autoref{fig:correlations}. As before, we use LLaMA as the PLM
%JHL3: what aspect did we focus here?
%Zhuohan: I have indicated that we look at coherence aspect here.
and focus on evaluating {coherence} here.
Interestingly, \textit{Typo} appears to be relatively stable and unaffected by the perturbation degree, where else \textit{Jumble} and \textit{Antonym} work better with more aggressive perturbation. Based on these results, we set the perturbation degree to 0.4, 0.9, and 0.8 for \textit{Typo}, \textit{Jumble}, and \textit{Antonym} respectively for both ROC and WP. 
%It appears that jumble and antonym do not offer significant improvement when the degree of perturbation is minor, but they exhibit a substantial increase in performance when more words are jumbled, ultimately reaching a stable value. In contrast, typo appears to perform more reliably, regardless of the degree of perturbation. We hypothesize that the language model may not be highly sensitive to small changes in word order, given its masked word infilling objective during training. However, it consistently responds to typos in words, potentially due to the resulting difference in embeddings caused by tokenization.
%JHL3: the explanation wasn't super convincing, so i think it's better that we just treat it as an empirical observation
%Zhuohan: I agree.

\paragraph{Language Models} %JHL2: here we talk about table 6. We should mention that we use the same perturbation degree value for all LMs here (derived from the previous paragraph).

We next present \deltascore results using different PLMs in \autoref{table:multiplemodels_inhouse}. As before, we focus on the top-3 performing perturbation methods (\textit{Typo}, \textit{Jumble}, and \textit{Antonym}). Encouragingly, across different PLMs and story aspects, we see that \deltascore outperforms vanilla likelihood (``w/o perturbation'') in almost all instances, suggesting that measuring story quality using \textit{likelihood difference} is generally a better approach than just using its \textit{likelihood}. Broadly speaking, \textit{Jumble} is the most consistent perturbation method: in ROC it is the consistently the best performer, while in WP it is either the best or second best performer, depending on the PLM. This is consistent with the results we have in \autoref{table:multiperturbations_inhouse}, confirming the surprising result that \textit{Jumble} works well to measure most story aspects. Looking at the magnitude of correlation for different story aspects, interestingness generally has a lower value, indicating it is a difficult aspect to measure. There are, however, some curious exceptions: in ROC the correlation for fluency and relatedness is particularly low. We don't have a strong hypothesis of these observations, but will note that the language of ROC stories are somewhat formulaic and possibly different to the language of the pretraining data. For relatedness, the story condition in ROC is the first sentence (from the 5-sentence human-written stories), and it's a rather artificial condition to set the ``topic'' for story generation.

%It has been observed that \deltascore, when applied with the three perturbations, demonstrates a strong performance across all generative models, irrespective of their varying model architectures. In general, it offers a better evaluation metric compared to directly applying likelihood. Among the three perturbations, jumble appears to be the most effective, providing substantial improvement in nearly all aspects across the two datasets. As a result, jumble yields the best-performing \deltascore for each aspect.

%It should be noted that \deltascore appears to exhibit better performance on WP as compared to ROC. Our speculation is that this can be attributed to the fact that the stories in WP are significantly more intricate in nature, providing more opportunities for the perturbations to target a greater number of aspects. Consequently, this complexity can potentially provide a benefit to \deltascore.

Another interesting (but unsurprising) observation to mention is that we see larger models generally exhibit better correlation, with GPT-3.5 and OPT being the best performing PLMs, BLOOM and FLAN-T5 in the middle of the pack, and BART the worst. That said, if we look at GPT-3.5 vs.\ OPT, OPT has the upperhand marginally, even though OPT's model size and pretraining data is approximately 2 times smaller, suggesting after reaching a certain scale, the benefit of more scaling is questionable.

%However, this trend is not always consistent, as evidenced by our findings. Specifically, in this study, OPT emerged as the top-performing model, being the best in 6 out of 10 aspects, while GPT-3.5 won only 3 out of 10 aspects, despite being 2.6$\times$ larger and trained on 1.7$\times$ more tokens of data. This observation aligns with previous research indicating that larger models do not necessarily guarantee better performance.

\subsection{Comparison with Existing Metrics}
\label{subsec:comparisonwithexistingmetrics}
%JHL2: here we talk about figure 2. Ideally I think we should present *all* the existing metrics, see if it's still legible if you include a few more bars in the plot. I think this bar plot is easier to read than table, so I kind of like it.



%JHL2: one danger of the current narrative is that in a way we are 'tuning' against the test data, i.e. we select which perturbation methods, perturbation degree, and LM works best in the test, before comparing it against other models. Maybe another way we can do this is to use one worker type as dev (e.g. inhouse; so the first 3 paragraphs will show results using this judgement) and the other as test (e.g. crowdworker; and so the 4th paragraph will show results using this judgement).

We now compare \deltascore against competitor evaluation metrics in \autoref{fig:comparisons}. Note that we use OPT as the PLM (as it is the best performing PLM) and the same (top-performing)  perturbation methods (\textit{Typo}, \textit{Jumble} and \textit{Antonym}). For fair comparison, we use a different set of ground truth human ratings (crowdworker ratings) for these results.

We see very encouraging results, with \deltascore consistently outperforming all competitor metrics over all story aspects (\textit{Jumble} once again is the winner out of the three perturbation methods). The similarity metrics (BLEU, BERTScore and MoverScore) have the worst performance, demonstrating that reference-based metrics are not suitable for story evaluation, consistent with previous findings \citep{guan-huang-2020-union, DBLP:journals/corr/abs-2301-09790}. Out of the discriminative metrics, CTC and UNIEVAL are the most competitive (in particular CTC appears to work better in WP while UNIEVAL is better for ROC), though they are still behind  \deltascore. In terms of generative scores, their performance is inconsistent. GPTScore performs well in assessing logicality and interestingness, particularly in ROC, where its performance is comparable to that of \deltascore. However, its effectiveness is limited in most other scenarios.

%We now use \deltascore equipped with OPT, as it demonstrates the best performance in previous steps in crowdsouce part.
%We compare it to metrics presented in \autoref{table:metricdetails}, and the results are shown in \autoref{fig:comparisons}. 
% Further analysis of perturbation effects is presented in \autoref{subsec:perturbationselections} and model effects in \autoref{subsec:modelselection}. 
%Detailed information can be found in \autoref{appendix: correlations}.

%Our findings suggest that similarity evaluation metrics such as BLEU, BERTScore, and MoverScore exhibit poor performance in open-ended story evaluation, which is consistent with previous studies \citep{guan-huang-2020-union, DBLP:journals/corr/abs-2301-09790}. This implies that reference is not necessary in assessing open-ended stories.
%Interestingly, discriminative evaluation metrics do not demonstrate impressive results despite being designed specifically for story evaluation. This may be due to their extensive training on synthetic data and not on our data, which has different features that could affect their performance in our evaluation scenarios.
%Finally, we observe that \deltascore significantly outperforms other metrics, particularly with the jumble perturbation, achieving the best results in all aspects of both datasets.

% We demonstrate \deltascore and observe that jumble and typos produce the most stable results across different models and datasets. Notably, when we use jumble as the perturbation for \deltascore with GPT-3.5, we observe a significant improvement over using the likelihood from GPT-3.5 directly as the metric. Moreover, this approach outperforms all other SOTA evaluation metrics on most aspects across all datasets.
% Replacing the entire story does not improve the performance of \deltascore over directly using the likelihood metric with GPT-3.5. However, it does provide some improvement for BART-cnn and a substantial improvement for BART-large. This may be due to the fact that stronger models are more focused on local context and less sensitive to changes in the prompt.
% Perturbing stories to violate commonsense using ChatGPT does not lead to a significant improvement in the performance of \deltascore compared to using the likelihood metric directly with GPT-3.5, except for in the WP dataset. However, it appears to provide some improvement for BART-cnn and BART-large. This may be due to the fact that when the perturbed stories violate commonsense, GPT-3.5 exhibits similar characteristics to ChatGPT, which results in high likelihood scores being assigned to the contents generated by ChatGPT.

% In general, the quality aspects of stories are heavily intertwined, meaning that perturbations targeting one aspect often impact others as well. Nonetheless, we find that interestingness is particularly challenging to address with perturbations, as it is difficult to identify suitable modifications that effectively target this aspect.


% \subsection{Overall Quality Evaluation Results}

%JHL: think we can drop this section; see my comments below how to handle each subsection
% \section{Analysis}

% \subsection{Generalization}

%JHL: we can drop most of the text here if we introduce DELTASCORE in a general manner previously; we just need to say we will experiment with both GPT3 and BART in the introduction of section 4. Might need to say a bit more about the exact implementation of BART that we use in section 4 (looks like there's BART-para, BART-CNN, etc)
% To test generalization, we also apply \deltascore on BART, note that BART is different from GPT-3.5 as in GPT-3.5 is a pure decoder based model while BART is an encoder-decoder based model.
% Note that, when we utilize the generative likelihood directly from BART to evaluate the story, it is the same as BARTScore \citep{DBLP:conf/nips/YuanNL21}.
% We replace generative language model of \deltascore to BART-para\footnote{The authors from \citet{DBLP:conf/nips/YuanNL21} fine-tunes BART-cnn model on para dataset \Needcite{para}} to explore if the idea can improve BARTScore.  

% We present results in, from which we can see that \deltascore can yield consistent gains in performances in most aspects and overall quality, which shows the idea of differentiating perturbations can also be applied to encoder-decoder architecture.


%JHL2: most of the text in this section will be moved to section 4 with the new structure
% \section{More Analysis}

% \subsection{Perturbations for Different Aspects}
% \label{subsec:perturbationselections}






%JHL: Here we should talk about the language models we will use, e.g. GPT-3 (text-davinci-003) and BART (size?). 
%JHL: we also need to say a bit more about the implementation of the different perturbations, e.g. how are word shuffling done? we select N% words in the text and randomly shuffle them, or how many typos we create, and how do we create them.  The high level idea of perturbation is explained in 3.2, but here we need to spell out a bit more explicitly how they are actually implemented. If space is tight, we can move this to appendix, but right now it's unclear how the perturbations are implemented.







% \subsection{Extending to Other Generative LLMs}
% \label{subsec:modelselection}

% In order to test the generalization of our proposed approach, we extend it to other generative LLMs. In addition to the OPT and GPT-3.5 models that we demonstrated previously, we also include two representatives of encoder-decoder models, namely BART and FLAN-T5 \citep{DBLP:journals/corr/abs-2210-11416}, as well as one more causal decoder model, BLOOM \citep{Scao2022BLOOMA1}.

% In our preliminary experiment, we observed that larger models tend to demonstrate better performance within the same model framework. As such, we applied the largest model size that we could run for each model. The model details are presented in \autoref{table:modeldetails}, while the results are shown in \autoref{table:multiplemodels}.





% \subsection{Influence of Perturbation Degrees}
% \label{subsec:PerturbationDegree}

%JHL: This is the only additional analysis in this section, we can either move this to a subsection in 5 (e.g. 5.X additional analysis) or to appendix if we need space


% \subsection{Model Preference}
% %JHL: we can drop this now since we'll be discussing BART results together with GPT3 in section 5
% We find different models have their different preferences.
% BART-cnn is better at stories generated from older models, while BART-para is better at stories generated recently even though the authors claim that BART-para has better capability due to the extra fine-tuning process.

% \subsection{Assessment Granularity}
% %JHL: Not really sure the objective of this analysis; are we trying to explain why certain metrics don't work (if so, which metric?)
% One of the mainstreams of automatic story evaluation is to train a binary classifier to distinguish good and bad stories \citep{guan-huang-2020-union, ghazarian-etal-2021-plot}.

% However, the nature of binary classifiers can easily result in an evaluator that mainly produce scores close to its labels, usually 0 and 1, while \deltascore utilize the essence of generative language models, therefore it can produce more continuous values that covers wider ranges (see \autoref{fig:binaryclassification}).

% continuous values are more desirable as they not only show which story is better, but also shows the granularity of which it is better even though they might have similar Spearman correlations.



% \begin{figure*}[t]
%      \centering
%      \begin{subfigure}[b]{0.31\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/corr_union-model.ckpt.pdf}
%          \caption{Perturbation ``Remove punctuation'' is applicable to higher quality story but not applicable to lower quality one.
%          }
%          \label{fig:example1}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.31\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/corr_bartscore_CNN_PS_ts1.pdf}
%          \caption{Perturbation ``Add typos'' is applicable to both stories, higher quality story is more affected than lower quality one.
%          % Higher quality story is affected more than lower quality one.
%          }
%          \label{fig:examples2}
%      \end{subfigure}
%      \begin{subfigure}[b]{0.31\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/corr_gpt3score_PS_mean_lp.pdf}
%          \caption{Perturbation ``Add typos'' is applicable to both stories, higher quality story is more affected than lower quality one.
%          % Higher quality story is affected more than lower quality one.
%          }
%          \label{fig:examples3}
%      \end{subfigure}
%         \caption{
%         The correlation figures show that binary classification approaches can only generate scores that close to labels (as set to 0s and 1s in the task)
%         while regression and generation approaches can generate more diverse scores.
%         }
%         \label{fig:binaryclassification}
% \end{figure*}




%JHL: we might not need to have different subsections for the two sets of results, if we end up discussing the results altogether
%JHL: suggestion: use a paragraph to spell out all the tables (2-X), and then discuss the results, and then present additional analysis (e.g. the text in 6.2 and 6.4)
% \subsection{Fine-grained Aspect Evaluation Results}




% We categorize the evaluation metrics into several types based on their features: 1) Similarity-based metrics, including BLEU (BLE), BERTScore (BER), and MoverScore (Mov); 2) Metrics that are specifically tailored for evaluating stories, including UNION (UNIO), MANPLTS (MAN), and StoryER (StoER); 3) Unified metrics designed to evaluate multiple aspects, including CTC and UNIEVAL (UNIE); and 4) Likelihood-based metrics, where the language models' likelihood is used directly, including BART-large (BRT-l), BART-cnn (BRT-c), and GPT-3.5 (G-3.5).\footnote{Please note that the concept of using likelihood is identical to that of BARTScore proposed by \cite{DBLP:conf/nips/YuanNL21}. The authors of BARTScore suggest that this idea can be extended to other generative models as well.} We also present the \deltascore metric using these three generative models.



\section{Conclusion}
We introduce \deltascore, a novel approach for evaluating fine-grained story aspects by comparing the difference in likelihood between the original and perturbed story under a pretrained language model. Surprisingly, we found that a select few perturbation methods work very very well for measuring most of the aspects. Ultimately we also show that \deltascore produces stronger correlations with human judgements compared to a suite of current metrics over two story domains. 
%We also show that \deltascore can be applied to various pre-trained language models, including encoder-decoder and decoder models, outperforming the direct application of generative likelihood as an evaluation metric.

% We experiment with multiple language models, including  and our results demonstrate that \deltascore with GPT-3.5 and jumble yields the best performance, outperforming state-of-the-art evaluation metrics.

\section*{Limitations}

Our work explores a limited set of perturbations for story evaluation, but it is likely that there are many more perturbations that could be obtained through different approaches. Although we only apply this perturbation method to story generation in this paper, it has the potential to be adapted for evaluating various aspects of text generation using specifically designed perturbations. This opens up a fruitful area for future research.

% This work explores a limited set of perturbations for story evaluation, but there are likely many more that could be obtained through different approaches. While we only apply this perturbation method to story generation, it has the potential to be adapted for evaluating various aspects of text generation using specifically designed perturbations, opening up a fruitful area for future research.


% \section*{Acknowledgements}

% \begin{figure*}[t]
%      \centering
%      \begin{subfigure}[b]{0.45\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/OPENMEVABARTScore_perturbations_roc.pdf}
%          \caption{ROC
%          }
%          \label{BARTScore-ROC-openmeva}
%      \end{subfigure}
%      \begin{subfigure}[b]{0.45\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/OPENMEVABARTScore_perturbations_wp.pdf}
%          \caption{WP
%          }
%          \label{BARTScore-WP-openmeva}
%      \end{subfigure}
%         \caption{
%         Perturbation results and Pearson correlations on each dataset for OpenMEVA.
%         }
%         \label{fig:perturbationgrades}
% \end{figure*}


% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix


\section{Perturbation Prompts}
\label{appendix:perturbationprompts}

We use following prompts for perturbations when we apply API with GPT-3.5-turbo.

%JHL3: there are some new lines in the prompt no? include them too (with \n)
%Zhuohan: I did not use newline in the prompt when I call the API.
\begin{itemize}
    \item RelevantWords: Find all words in the given story that is relevant to the given title. Please only print words in the given story, and separate them by ','.  ``title'': \{title\}, ``story'': \{story\}
    \item Commonsense: Revise the following story such that certain elements does not make sense. The revision should be minimal, e.g. by changing a few words. ``story'': \{story\}
    \item BlanderNarrative: Revise the following story to make it less interesting (e.g. expected ending, no plot twist). The revision should be minimal. ``story'': \{story\}
\end{itemize}


% \section{Meta-evalution Datasets Details}
% \label{appendix:datasetsdetail}

% \citet{DBLP:journals/corr/abs-2301-09790} evaluate a range of recently proposed story generation models:
% 1) \textbf{KGGPT2} \citep{guan-etal-2020-knowledge},
% 2) \textbf{MTCL} \citep{xu-etal-2020-megatron},
% 3) \textbf{HINT} \citep{guan-etal-2021-long},
% 4) \textbf{PROGEN} \citep{tan-etal-2021-progressive},
% 5) \textbf{BART} \citep{lewis-etal-2020-bart} and
% 6) \textbf{GPT-3} \citep{DBLP:conf/nips/BrownMRSKDNSSAA20},
% and also human reference stories.
% They conduct a meta-evaluation, which consists of two parts: crowdsourcing through Amazon Mechanical Turk and in-house evaluation by colleagues. We primarily rely on the crowdsourced evaluations since the authors suggest that in-house annotators may have a natural bias towards assigning higher scores to machine-generated stories.



\section{UNIEVAL and GPTScore}
\label{appendix:evaluationmetrics}

\paragraph{UNIEVAL}


We ask the following questions for each aspect.
Note that we try to use the narrative/vocabulary as close to the original questions
\citet{DBLP:conf/emnlp/Zhong0YMJLZJH22} use in their efforts as possible.

\begin{itemize}
    \item Fluency: Is this a fluent utterance?
    \item Coherence: Is this a coherent utterance?
    \item Relatedness: Is this claim consistent with the document?
    \item Logicality: Is this utterance consistent with the commonsense?
    \item Interestingness: Is this an interesting utterance?
\end{itemize}

\paragraph{GPTScore}

We ask the following questions for each aspect.
Note that we try to use the narrative/vocabulary as close to the original questions \citep{DBLP:journals/corr/abs-2302-04166} use in their efforts as possible.

\begin{itemize}
    \item Fluency: Generate a fluent story for the given title: \{title\}, and story: \{story\}
    \item Coherence: Generate a coherent story for the given title: \{title\}, and story: \{story\}
    \item Relatedness: Generate a story related to the given title: \{title\}, and story: \{story\}
    \item Logicality: Generate a story that adhere to commonsense for the given title: \{title\}, and story: \{story\}
    \item Interestingness: Generate an interesting story for the given title: \{title\}, and story: \{story\}
\end{itemize}




% \section{Correlations}
% \label{appendix: correlations}

% \paragraph{Visualization}

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.21\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figs/heatmap_roc.pdf}
%          \caption{ROC
%          }
%          \label{heatmap_ROC}
%      \end{subfigure}
%      \begin{subfigure}[b]{0.265\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figs/heatmap_wp.pdf}
%          \caption{WP
%          }
%          \label{heatmap_WP}
%      \end{subfigure}
%         \caption{
%         We present improvements of \deltascore on GPT-3.5 over applying GPT-3.5 generative likelihood directly as evaluation metric. Warmer color indicates greater improvement.
%         `TP' indicates typo,
%         `JB' indicates jumble,
%         `SR' indicates sentencereorder,
%         `AT' indicates antonym, and
%         `BN' indicates blandernarrative. 
%         }
%         \label{fig:heatmaps}
% \end{figure}

% \subsection{Issues}
% We find that UNIEVAL seems not able to differeniate too much when we ask different questions on each quality aspect.

% \section{Appendix: Correlation Results}

% We put Spearman and Kendall-Tau correlations in \autoref{table:crowdsource_cnn}.
% Pearson correlations are shown in .

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
%  % & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ & $|\rho|$ \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 25.0 & 11.3 & 6.6 & 15.0 & 0.8 & 9.4 & 16.9 & 11.2 & 15.4 & 23.7 & 22.7 & 0.2 & 25.1 & 3.6 & 18.1  \\ 
%  BER & 4.4 & 1.3 & 26.7 & 4.2 & 3.1 & 29.4 & 34.6 & 37.5 & 28.1 & 30.6 & 48.7 & 45.3 & 70.8 & 51.7 & 48.6 \\
%  Mov & 12.0 & 8.0 & 38.2 & 7.5 & 0.2 & 41.9 & 47.7 & 36.8 & 30.0 & 34.6 & 23.5 & 14.6 & 47.0 & 25.2 & 33.5 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 13.5 & 26.7 & 14.9 & 19.3 & 7.2 & 39.4 & 59.6 & 54.1 & 47.8 & 58.8 & 27.1 & 42.5 & 39.5 & 33.2 & 43.0 \\  
% MAN & 7.0 & 3.1 & 7.5 & 1.6 & 12.7 & 20.5 & 0.4 & 14.1 & 23.7 & 4.7 & 18.6 & 30.4 & 48.5 & 19.4 & 20.1 \\ 
% Sto & 5.4 & 12.0 & 14.3 & 1.0 & 12.6 & 11.6 & 22.1 & 12.4 & 21.5 & 32.5 & 13.0 & 28.6 & 14.4 & 14.7 & 23.7 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 23.5 & 41.3 & 25.3 & 35.7 & 25.8 & 51.3 & 76.6 & 53.6 & 68.0 & \textbf{57.2} & 57.4 & 59.4 & 52.8 & 61.0 & 62.7 \\ 
%  UNI & 43.0 & 41.5 & 13.9 & 34.0 & 26.1 & 61.6 & 56.3 & 49.4 & 51.9 & 51.0 & 47.3 & 61.3 & 55.7 & 71.5 & 52.0 \\
%  \midrule
% \multicolumn{10}{l}{\textbf{Likelihood Based Metrics}}\\
% BART & \\ 
% G-3.5 & 28.8 & 43.2 & 26.6 & 39.9 & 25.9 & 47.9 & 44.8 & 48.8 & 52.4 & 34.5 & 56.8 & 65.9 & 58.3 & 66.0 & \textbf{52.7} \\ 
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  + Ju & \cellcolor{green!25}38.7 & \cellcolor{green!25}48.3 & \cellcolor{green!25}33.7 & \cellcolor{green!25}50.8 & 37.2 & \cellcolor{green!25}43.3 & \cellcolor{green!25}50.3 & \cellcolor{green!25}53.0 & \cellcolor{green!25}52.9 & \cellcolor{green!25}43.2 & \cellcolor{green!25}46.7 & \cellcolor{green!25}63.8 & \cellcolor{green!25}46.7 & \cellcolor{green!25}56.1 & 47.6 \\ 
 
%  + Ty & \cellcolor{green!25}37.7 & \cellcolor{green!25}48.6 & \cellcolor{green!25}35.6 & 45.0 & 32.2 & \cellcolor{green!25}44.7 & \cellcolor{green!25}48.3 & \cellcolor{green!25}53.1 & \cellcolor{green!25}58.3 & \cellcolor{green!25}40.8 & \cellcolor{green!25}57.9 & \cellcolor{green!25}68.5 & \cellcolor{green!25}55.2 & \cellcolor{green!25}67.8 & 50.6 \\ 
 
%  + Ad & 18.0 & 41.1 & 21.1 & 32.5 & 24.1 & 44.4 & 38.4 & 30.9 & 34.9 & 23.7 & 47.1 & 66.3 & 32.4 & 59.2 & 42.6  \\
 
%  + An & 32.2 & \cellcolor{green!25}40.1 & 26.4 & 29.7 & 19.4 & \cellcolor{green!25}72.9 & \cellcolor{green!25}67.7 & \cellcolor{green!25}56.4 & \cellcolor{green!25}71.4 & \cellcolor{green!25}56.9 & \cellcolor{green!25}48.3 & 59.4 & 52.7 & \cellcolor{green!25}64.6 & 40.5 \\
%  + Rp  \\
%  + Rr  \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (change to vinialla BART)}}\\ 
%  + Ju &  \\
%  + Ty &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Spearman correlation ($|\rho|$) between different metrics and human evaluations on three Inhouse datasets.
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the original likehihood metric.
% }
% \label{table:inhouse_spearman}
% \end{table*}


% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{clcccccccccc}
% \toprule
% \multirow{2}{*}[-1ex]{\textbf{Target Aspect}} & \multirow{2}{*}[-1ex]{\textbf{Perturbation}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
% \cmidrule(lr){3-7}\cmidrule(lr){8-12}
%  & & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} \\
% \cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}
% -- & w/o perturbation & 32.0 & 27.6 & 25.1 & 21.1 & 19.7 & 31.6 & 36.0 & 34.7 & 38.5 & 30.6 \\ 
% \midrule
% \multirow{2}{*}[0ex]{Fluency} &  Typo & 22.9 & 11.4 & \cellcolor{green!25}13.3 & 8.2 & 11.4 & \cellcolor{green!25}42.8 & \cellcolor{green!25}35.6 & \cellcolor{green!25}39.8 & \cellcolor{green!25}41.9 & \cellcolor{green!25}33.0 \\
% & SubjVerbDis & 11.9 & 10.5 & 8.8 & 4.3 & 0.9 & 5.8 & 6.2 & 17.9 & 13.2 & 17.3  \\
% \midrule
% \multirow{2}{*}[0ex]{Coherence} & Jumble & \cellcolor{green!25}35.9 & \cellcolor{green!25}40.0 & \cellcolor{green!25}\textbf{33.8} & \cellcolor{green!25}\textbf{29.6} & \cellcolor{green!25}23.2 & \cellcolor{green!25}52.7 & \cellcolor{green!25}49.2 & \cellcolor{green!25}36.6 & \cellcolor{green!25}53.9 & \cellcolor{green!25}42.8  \\
% & SentReorder & 20.0 & 16.1 & 10.1 & 7.4 & 4.7 & 4.1 & 17.6 & 5.6 & 12.4 & 3.4 \\ 
% \midrule
% \multirow{2}{*}[0ex]{Relatedness} & RmRelWords & 15.9 & 16.9 & 13.3 & 9.6 & 7.6 & \cellcolor{green!25}36.5 & 27.7 & 31.7 & 33.5 & 21.9 \\
% & StoryReplace & 6.2 & 2.2 & 9.1 & 9.6 & 7.8 & \cellcolor{green!25}38.5 & \cellcolor{green!25}36.6 & \cellcolor{green!25}37.6 & 38.5 & \cellcolor{green!25}33.8 \\ 
% \midrule
% \multirow{2}{*}[0ex]{Logicality} & Antonym & 14.3 & \cellcolor{green!25}22.1 & 10.9 & \cellcolor{green!25}21.3 & \cellcolor{green!25}16.8 & \cellcolor{green!25}41.5 & \cellcolor{green!25}39.5 & \cellcolor{green!25}33.9 & \cellcolor{green!25}42.7 & \cellcolor{green!25}36.6 \\
% & Commonsense & 16.9 & 15.8 & 17.5 & 2.2 & 3.8 & 29.6 & \cellcolor{green!25}39.0 & 26.3 & 37.6 & 26.5 \\
% \midrule
% Interestingness & BlanderNarrative & 6.7 & 10.4 & 11.5 & 1.5 & 0.7 & 13.9 & 17.7 & 13.0 & 19.3 & 7.2 \\
%  \bottomrule
% \end{tabular}
% \caption{Story-level Kendall correlation ($|\tau|$) between \deltascore with GPT-3.5 and human evaluations on ROC and WP on CrowdSource Meta Evaluation. 
% \colorbox{green!30}{Highlight} indicates the scores where \deltascore achieves better performance over applying generative likelihood from GPT-3.5 directly as the evaluation metric.
% }
% \label{table:multiperturbations_crowdsource}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}
%   & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
% \multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn 406M)}}\\ 
% w/o perturbation &  24.9 & 11.6 & 10.9 & 11.3 & 14.6 & 32.2 & 25.6 & 31.0 & 30.2 & 23.0 \\ 
%  Jumble & \cellcolor{green!25}29.2 & \cellcolor{green!25}17.7 & 9.9 & \cellcolor{green!25}12.2 & 12.6 & \cellcolor{green!25}49.1 & \cellcolor{green!25}40.1 & \cellcolor{green!25}44.5 & \cellcolor{green!25}46.5 & \cellcolor{green!25}41.5 \\
%  Typo & 22.9 & 11.4 & \cellcolor{green!25}13.3 & 8.2 & 11.4 & \cellcolor{green!25}42.8 & \cellcolor{green!25}35.6 & \cellcolor{green!25}39.8 & \cellcolor{green!25}41.9 & \cellcolor{green!25}33.0 \\
%  Antonym & 14.3 & \cellcolor{green!25}22.1 & 10.9 & \cellcolor{green!25}21.3 & \cellcolor{green!25}16.8 & \cellcolor{green!25}41.5 & \cellcolor{green!25}39.5 & \cellcolor{green!25}33.9 & \cellcolor{green!25}42.7 & \cellcolor{green!25}36.6 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL 11B)}}\\ 
%   w/o perturbation & 21.0 & 22.1 & 26.8 & 23.2 & 22.5 & 13.6 & 10.6 & 12.1 & 10.8 & 4.2 \\
%  Jumble & \cellcolor{green!25}35.7 & \cellcolor{green!25}28.2 & 19.9 & 19.8 & 22.2 & \cellcolor{green!25}\textbf{59.2} & \cellcolor{green!25}53.7 & \cellcolor{green!25}41.8 & \cellcolor{green!25}57.4 & \cellcolor{green!25}46.9 \\
%  Typo & \cellcolor{green!25}24.2 & \cellcolor{green!25}26.6 & \cellcolor{green!25}27.7 & \cellcolor{green!25}24.6 & \cellcolor{green!25}23.0 & \cellcolor{green!25}34.3 & \cellcolor{green!25}30.5 & \cellcolor{green!25}28.1 & \cellcolor{green!25}34.8 & \cellcolor{green!25}23.1 \\
%  Antonym & 9.4 & 16.1 & 3.2 & 7.5 & 5.4 & \cellcolor{green!25}52.6 & \cellcolor{green!25}48.0 & \cellcolor{green!25}37.1 & \cellcolor{green!25}53.8 & \cellcolor{green!25}46.3 \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
%   w/o perturbation & 26.4 & 21.3 & 21.1 & 15.8 & 20.3 & 18.1 & 15.4 & 17.5 & 17.6 & 9.3 \\
%  Jumble & \cellcolor{green!25}35.4 & \cellcolor{green!25}36.4 & \cellcolor{green!25}21.6 & \cellcolor{green!25}23.4 & \cellcolor{green!25}\textbf{26.8} & \cellcolor{green!25}52.7 & \cellcolor{green!25}52.5 & \cellcolor{green!25}43.6 & \cellcolor{green!25}56.6 & \cellcolor{green!25}44.7 \\
%  Typo & \cellcolor{green!25}31.7 & \cellcolor{green!25}28.2 & \cellcolor{green!25}22.0 & 15.3 & 19.0 & \cellcolor{green!25}48.4 & \cellcolor{green!25}46.3 & \cellcolor{green!25}39.0 & \cellcolor{green!25}52.6 & \cellcolor{green!25}40.2 \\
%  Antonym & 11.8 & 16.1 & 6.5 & 9.6 & 7.7 & \cellcolor{green!25}49.9 & \cellcolor{green!25}46.3 & \cellcolor{green!25}32.5 & \cellcolor{green!25}50.0 & \cellcolor{green!25}42.7 \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with LLaMA 65B)}}\\ 
%   w/o perturbation & 31.0 & 24.9 & 25.4 & 16.8 & 18.5 & 21.1 & 26.6 & 22.7 & 26.7 & 20.3 \\
%  Jumble & \cellcolor{green!25}34.8 & \cellcolor{green!25}32.0 & 20.9 & \cellcolor{green!25}22.4 & \cellcolor{green!25}21.0 & \cellcolor{green!25}55.4 & \cellcolor{green!25}56.5 & \cellcolor{green!25}43.8 & \cellcolor{green!25}59.8 & \cellcolor{green!25}47.1 \\
%  Typo &  \cellcolor{green!25}36.8 & \cellcolor{green!25}32.3 & 22.9 & 13.8 & 13.8 & \cellcolor{green!25}45.0 & \cellcolor{green!25}47.0 & \cellcolor{green!25}36.0 & \cellcolor{green!25}52.2 & \cellcolor{green!25}38.0 \\
%  Antonym & 15.6 & 17.9 & 8.3 & 9.0 & 4.9 & \cellcolor{green!25}54.0\textsuperscript{\textdagger} & \cellcolor{green!25}51.5\textsuperscript{\textdagger} & \cellcolor{green!25}37.7\textsuperscript{\textdagger} & \cellcolor{green!25}56.2\textsuperscript{\textdagger} & \cellcolor{green!25}46.1\textsuperscript{\textdagger} \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
%   w/o perturbation & 32.0 & 26.7 & 25.3 & 22.0 & 23.5 & 20.8 & 24.8 & 22.2 & 25.7 & 17.1 \\
%  Jumble & \cellcolor{green!25}\textbf{43.2} & \cellcolor{green!25}\textbf{42.4} & \cellcolor{green!25}27.4 & \cellcolor{green!25}25.6 & \cellcolor{green!25}23.9 & \cellcolor{green!25}56.1 & \cellcolor{green!25}\textbf{55.5} & \cellcolor{green!25}\textbf{45.9} & \cellcolor{green!25}\textbf{60.9} & \cellcolor{green!25}\textbf{48.5} \\
%  Typo & \cellcolor{green!25}35.9 & \cellcolor{green!25}32.0 & \cellcolor{green!25}28.6 & 21.8 & 22.1 & \cellcolor{green!25}44.8 & \cellcolor{green!25}45.8 & \cellcolor{green!25}36.8 & \cellcolor{green!25}50.1 & \cellcolor{green!25}36.6 \\
%  Antonym & 16.5 & 21.9 & 8.4 & 11.9 & 9.3 & \cellcolor{green!25}55.2 & \cellcolor{green!25}51.8 & \cellcolor{green!25}36.9 & \cellcolor{green!25}56.0 & \cellcolor{green!25}45.9 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5 175B)}}\\
%  w/o perturbation & 32.0 & 27.6 & 25.1 & 21.1 & 19.7 & 31.6 & 36.0 & 34.7 & 38.5 & 30.6  \\
%  Jumble & \cellcolor{green!25}35.9 & \cellcolor{green!25}40.0 & \cellcolor{green!25}\textbf{33.8} & \cellcolor{green!25}\textbf{29.6} & \cellcolor{green!25}23.2 & \cellcolor{green!25}52.7 & \cellcolor{green!25}49.2 & \cellcolor{green!25}36.6 & \cellcolor{green!25}53.9 & \cellcolor{green!25}42.8 \\ 
%  Typo & 29.5 & \cellcolor{green!25}28.2 & 22.7 & 15.4 & 11.7 & \cellcolor{green!25}40.7 & \cellcolor{green!25}43.1 & 32.6 & \cellcolor{green!25}47.2 & \cellcolor{green!25}35.4  \\ 
%  Antonym & 21.4 & 26.7 & 15.1 & \cellcolor{green!25}21.9 & 18.5 & \cellcolor{green!25}48.3 & \cellcolor{green!25}47.3 & \cellcolor{green!25}37.6 & \cellcolor{green!25}51.6 & \cellcolor{green!25}42.1 \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Kendall correlation ($|\tau|$) between different metrics and human evaluations on Inhouse Meta Evaluation. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!25}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:multiplemodels_crowdsource}
% \end{table*}



% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}
%   & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} & \textbf{Flu.} & \textbf{Coh.} & \textbf{Rel.} & \textbf{Log.} & \textbf{Int.} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLEU & 3.4 & 4.4 & 0.8 & 4.6 & 0.4 & 13.4 & 11.4 & 9.6 & 11.2 & 7.2 \\ 
%  BERTScore & 3.5 & 5.0 & 14.0 & 5.7 & 7.3 & 31.8 & 26.7 & 24.0 & 28.5 & 24.6 \\
%  MoverScore & 3.6 & 6.5 & 15.7 & 8.0 & 11.2 & 16.4 & 17.2 & 20.1 & 20.7 & 23.7  \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNION &  0.7 & 12.7 & 12.8 & 0.8 & 3.9 & 17.4 & 21.9 & 18.1 & 22.9 & 21.8 \\ 
% MANPLTS & 21.3 & 32.8 & 23.2 & 14.7 & 12.4 & 1.1 & 5.5 & 1.7 & 4.6 & 6.7 \\ 
% StoryER & 6.5 & 4.5 & 4.0 & 3.7 & 9.7 & 15.9 & 13.1 & 14.1 & 17.9 & 26.1 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 22.9 & 27.3 & 14.3 & 11.1 & 8.3 & 45.9 & 51.6 & 40.3 & 53.1 & 47.5 \\ 
%  UNIEVAL & 32.2 & 31.7 & 23.7 & 20.0 & 18.8 & 39.3 & 41.3 & 38.6 & 50.7 & 39.1 \\
%  GPTScore & 21.2 & 20.9 & 21.3 & 23.9 & 22.0 & 29.3 & 32.6 & 30.8 & 35.3 & 28.9 \\
%  \midrule
%   \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
%   w/o perturbation & 26.4 & 21.3 & 21.1 & 15.8 & 20.3 & 18.1 & 15.4 & 17.5 & 17.6 & 9.3 \\
%   Typo & \cellcolor{green!25}31.7 & \cellcolor{green!25}28.2 & \cellcolor{green!25}22.0 & 15.3 & 19.0 & \cellcolor{green!25}48.4 & \cellcolor{green!25}46.3 & \cellcolor{green!25}39.0 & \cellcolor{green!25}52.6 & \cellcolor{green!25}40.2 \\
%  Jumble & \cellcolor{green!25}35.4 & \cellcolor{green!25}36.4 & \cellcolor{green!25}21.6 & \cellcolor{green!25}23.4 & \cellcolor{green!25}\textbf{26.8} & \cellcolor{green!25}52.7 & \cellcolor{green!25}52.5 & \cellcolor{green!25}43.6 & \cellcolor{green!25}56.6 & \cellcolor{green!25}44.7 \\
%  % AddNe & 9.9 & 16.8 & 16.0 & 6.3 & 8.5 & \cellcolor{green!25}23.9 & \cellcolor{green!25}21.8 & \cellcolor{green!25}24.9 & \cellcolor{green!25}24.5 & \cellcolor{green!25}15.0 \\
%  Antonym & 11.8 & 16.1 & 6.5 & 9.6 & 7.7 & \cellcolor{green!25}49.9 & \cellcolor{green!25}46.3 & \cellcolor{green!25}32.5 & \cellcolor{green!25}50.0 & \cellcolor{green!25}42.7 \\
% \midrule
%   \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
%   w/o perturbation & 32.0 & 26.7 & 25.3 & 22.0 & 23.5 & 20.8 & 24.8 & 22.2 & 25.7 & 17.1 \\
%    Typo & \cellcolor{green!25}35.9 & \cellcolor{green!25}32.0 & \cellcolor{green!25}28.6 & 21.8 & 22.1 & \cellcolor{green!25}44.8 & \cellcolor{green!25}45.8 & \cellcolor{green!25}36.8 & \cellcolor{green!25}50.1 & \cellcolor{green!25}36.6 \\
%  Jumble & \cellcolor{green!25}\textbf{43.2} & \cellcolor{green!25}\textbf{42.4} & \cellcolor{green!25}27.4 & \cellcolor{green!25}25.6 & \cellcolor{green!25}23.9 & \cellcolor{green!25}56.1 & \cellcolor{green!25}\textbf{55.5} & \cellcolor{green!25}\textbf{45.9} & \cellcolor{green!25}\textbf{60.9} & \cellcolor{green!25}\textbf{48.5} \\

%  % AddNe & 22.7 & \cellcolor{green!25}31.7 & \cellcolor{green!25}28.6 & 17.8 & 19.1 & \cellcolor{green!25}21.6 & \cellcolor{green!25}25.3 & \cellcolor{green!25}27.0 & \cellcolor{green!25}28.9 & \cellcolor{green!25}21.9 \\
%  Antonym & 16.5 & 21.9 & 8.4 & 11.9 & 9.3 & \cellcolor{green!25}55.2 & \cellcolor{green!25}51.8 & \cellcolor{green!25}36.9 & \cellcolor{green!25}56.0 & \cellcolor{green!25}45.9 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL)}}\\ 
%   w/o perturbation & 21.0 & 22.1 & 26.8 & 23.2 & 22.5 & 13.6 & 10.6 & 12.1 & 10.8 & 4.2 \\
%    Typo & \cellcolor{green!25}24.2 & \cellcolor{green!25}26.6 & \cellcolor{green!25}27.7 & \cellcolor{green!25}24.6 & \cellcolor{green!25}23.0 & \cellcolor{green!25}34.3 & \cellcolor{green!25}30.5 & \cellcolor{green!25}28.1 & \cellcolor{green!25}34.8 & \cellcolor{green!25}23.1 \\
%  Jumble & \cellcolor{green!25}35.7 & \cellcolor{green!25}28.2 & 19.9 & 19.8 & 22.2 & \cellcolor{green!25}\textbf{59.2} & \cellcolor{green!25}53.7 & \cellcolor{green!25}41.8 & \cellcolor{green!25}57.4 & \cellcolor{green!25}46.9 \\
%  % AddNe & 15.4 & \cellcolor{green!25}23.2 & 16.6 & 10.4 & 9.9 & \cellcolor{green!25}24.2 & \cellcolor{green!25}27.4 & \cellcolor{green!25}30.7 & \cellcolor{green!25}31.8 & \cellcolor{green!25}22.6 \\
%  Antonym & 9.4 & 16.1 & 3.2 & 7.5 & 5.4 & \cellcolor{green!25}52.6 & \cellcolor{green!25}48.0 & \cellcolor{green!25}37.1 & \cellcolor{green!25}53.8 & \cellcolor{green!25}46.3 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% w/o perturbation &  24.9 & 11.6 & 10.9 & 11.3 & 14.6 & 32.2 & 25.6 & 31.0 & 30.2 & 23.0 \\ 
% Typo & 22.9 & 11.4 & \cellcolor{green!25}13.3 & 8.2 & 11.4 & \cellcolor{green!25}42.8 & \cellcolor{green!25}35.6 & \cellcolor{green!25}39.8 & \cellcolor{green!25}41.9 & \cellcolor{green!25}33.0 \\
%  Jumble & \cellcolor{green!25}29.2 & \cellcolor{green!25}17.7 & 9.9 & \cellcolor{green!25}12.2 & 12.6 & \cellcolor{green!25}49.1 & \cellcolor{green!25}40.1 & \cellcolor{green!25}44.5 & \cellcolor{green!25}46.5 & \cellcolor{green!25}41.5 \\
%  % AddNe & 8.4 & \cellcolor{green!25}16.9 & \cellcolor{green!25}17.2 & \cellcolor{green!25}11.4 & 9.8 & 24.0 & 17.2 & 13.7 & 20.4 & 13.2 \\
%  Antonym & 14.3 & \cellcolor{green!25}22.1 & 10.9 & \cellcolor{green!25}21.3 & \cellcolor{green!25}16.8 & \cellcolor{green!25}41.5 & \cellcolor{green!25}39.5 & \cellcolor{green!25}33.9 & \cellcolor{green!25}42.7 & \cellcolor{green!25}36.6 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  w/o perturbation & 32.0 & 27.6 & 25.1 & 21.1 & 19.7 & 31.6 & 36.0 & 34.7 & 38.5 & 30.6  \\
%   Typo & 29.5 & \cellcolor{green!25}28.2 & 22.7 & 15.4 & 11.7 & \cellcolor{green!25}40.7 & \cellcolor{green!25}43.1 & 32.6 & \cellcolor{green!25}47.2 & \cellcolor{green!25}35.4  \\ 
%  Jumble & \cellcolor{green!25}35.9 & \cellcolor{green!25}40.0 & \cellcolor{green!25}\textbf{33.8} & \cellcolor{green!25}\textbf{29.6} & \cellcolor{green!25}23.2 & \cellcolor{green!25}52.7 & \cellcolor{green!25}49.2 & \cellcolor{green!25}36.6 & \cellcolor{green!25}53.9 & \cellcolor{green!25}42.8 \\ 
%  % AddNe &  21.0 & \cellcolor{green!25}29.5 & \cellcolor{green!25}34.4 & 18.3 & 19.0 & 26.5 & 21.1 & 13.4 & 24.9 & 16.3  \\
%  Antonym & 21.4 & 26.7 & 15.1 & \cellcolor{green!25}21.9 & 18.5 & \cellcolor{green!25}48.3 & \cellcolor{green!25}47.3 & \cellcolor{green!25}37.6 & \cellcolor{green!25}51.6 & \cellcolor{green!25}42.1 \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Kendall correlation ($|\tau|$) between different metrics and human evaluations on three CoudSourcing datasets. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:crowdsource_kendall_all}
% \end{table*}



% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int}  \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 1.6 & 3.8 & 3.4 & 8.3 & 1.4 & 12.5 & 13.7 & 12.1 & 17.2 & 13.1 \\ 
%  BER & 3.1 & 6.2 & 18.3 & 3.8 & 6.2 & 47.1 & 42.6 & 35.1 & 42.4 & 36.6 \\
%  Mov & 2.2 & 7.4 & 23.3 & 3.3 & 9.7 & 26.8 & 28.6 & 32.3 & 31.8 & 32.3 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 4.7 & 16.7 & 17.4 & 5.2 & 9.3 & 32.0 & 39.9 & 24.4 & 36.4 & 35.0 \\  
% MAN & 27.7 & 40.2 & 23.7 & 17.7 & 13.4 & 6.1 & 9.3 & 9.1 & 7.0 & 6.7 \\ 
% StoER & 8.9 & 1.1 & 15.5 & 5.4 & 12.1 & 26.7 & 21.2 & 13.5 & 26.8 & 35.0 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 29.3 & 38.3 & 20.3 & 16.1 & 12.0 & 43.7 & 55.2 & 44.1 & 63.9 & 59.6 \\ 
%  UNIE & 34.4 & 35.9 & 30.4 & 26.2 & 21.2 & 42.8 & 43.0 & 52.3 & 55.1 & 45.4 \\
%  \midrule
%    \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
%   -- &  36.4 & 31.7 & 27.7 & 25.3 & 30.3 & 18.7 & 23.8 & 29.9 & 28.6 & 22.8 \\
%  Jumble & \cellcolor{green!25}46.6 & \cellcolor{green!25}50.5 & \cellcolor{green!25}29.1 & \cellcolor{green!25}32.4 & \cellcolor{green!25}36.0 & \cellcolor{green!25}66.4 & \cellcolor{green!25}72.0 & \cellcolor{green!25}64.7 & \cellcolor{green!25}78.5 & \cellcolor{green!25}66.1  \\
%  Typo & \cellcolor{green!25}42.1 & \cellcolor{green!25}39.8 & \cellcolor{green!25}30.5 & 22.5 & 26.7 & \cellcolor{green!25}63.3 & \cellcolor{green!25}69.2 & \cellcolor{green!25}62.0 & \cellcolor{green!25}74.2 & \cellcolor{green!25}60.6 \\
%  AddNe & 9.8 & 18.3 & 17.5 & 1.4 & 5.9 & \cellcolor{green!25}34.9 & \cellcolor{green!25}31.4 & 29.4 & \cellcolor{green!25}35.4 & 19.8 \\
%  Anton & 18.7 & 22.2 & 6.3 & 11.8 & 11.1 & \cellcolor{green!25}66.2 & \cellcolor{green!25}64.5 & \cellcolor{green!25}55.0 & \cellcolor{green!25}70.1 & \cellcolor{green!25}58.0 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%   \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
%    -- & 42.6 & 39.2 & 34.7 & 31.8 & 32.3 & 23.1 & 34.0 & 36.5 & 39.2 & 33.0 \\
%  Jumble & \cellcolor{green!25}55.9 & \cellcolor{green!25}58.1 & \cellcolor{green!25}37.8 & \cellcolor{green!25}34.3 & 31.4 & \cellcolor{green!25}68.8 & \cellcolor{green!25}77.3 & \cellcolor{green!25}72.1 & \cellcolor{green!25}82.9 & \cellcolor{green!25}68.4  \\
%  Typo & \cellcolor{green!25}50.7 & \cellcolor{green!25}47.4 & \cellcolor{green!25}39.4 & 31.7 & 32.3 & \cellcolor{green!25}58.2 & \cellcolor{green!25}69.0 & \cellcolor{green!25}61.8 & \cellcolor{green!25}76.1 & \cellcolor{green!25}63.9  \\
%  AddNe & 30.3 & \cellcolor{green!25}42.3 & 34.4 & 23.1 & 24.1 & \cellcolor{green!25}32.6 & \cellcolor{green!25}35.9 & 34.5 & \cellcolor{green!25}41.1 & 29.2  \\
%  Anton & 25.8 & 30.5 & 8.8 & 15.7 & 12.0 & \cellcolor{green!25}66.8 & \cellcolor{green!25}69.3 & \cellcolor{green!25}60.1 & \cellcolor{green!25}75.8 & \cellcolor{green!25}62.1 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL)}}\\ 
%    -- & 30.0 & 33.3 & 37.2 & 32.4 & 29.3 & 6.9 & 11.3 & 19.4 & 15.7 & 10.7 \\
%  Jumble & \cellcolor{green!25}47.2 & \cellcolor{green!25}40.2 & 25.6 & 24.4 & 26.7 & \cellcolor{green!25}70.2 & \cellcolor{green!25}71.9 & \cellcolor{green!25}65.8 & \cellcolor{green!25}77.7 & \cellcolor{green!25}65.1 \\
%  Typo & \cellcolor{green!25}33.5 & \cellcolor{green!25}39.1 & \cellcolor{green!25}38.3 & \cellcolor{green!25}35.1 & \cellcolor{green!25}30.9 & \cellcolor{green!25}40.3 & \cellcolor{green!25}45.3 & \cellcolor{green!25}45.7 & \cellcolor{green!25}52.3 & \cellcolor{green!25}41.2 \\
%  AddNe & 20.1 & 30.5 & 24.6 & 16.3 & 17.0 & \cellcolor{green!25}38.5 & \cellcolor{green!25}38.5 & \cellcolor{green!25}37.7 & \cellcolor{green!25}42.9 & \cellcolor{green!25}28.0 \\
%  Anton & 16.9 & 24.6 & 4.0 & 11.7 & 8.5 & \cellcolor{green!25}63.5 & \cellcolor{green!25}63.1 & \cellcolor{green!25}58.1 & \cellcolor{green!25}70.9 & \cellcolor{green!25}57.7 \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- & 32.4 & 13.0 & 12.0 & 15.6 & 19.8 & 33.6 & 30.3 & 43.4 & 36.7 & 28.8 \\
%  Jumble & \cellcolor{green!25}35.9 & \cellcolor{green!25}21.5 & 11.2 & 15.4 & 17.0 & \cellcolor{green!25}57.6 & \cellcolor{green!25}55.3 & \cellcolor{green!25}62.9 & \cellcolor{green!25}63.5 & \cellcolor{green!25}56.8 \\
%  Typo & 29.6 & 11.3 & \cellcolor{green!25}12.4 & 9.4 & 15.3 & \cellcolor{green!25}47.4 & \cellcolor{green!25}44.5 & \cellcolor{green!25}55.7 & \cellcolor{green!25}52.5 & \cellcolor{green!25}41.8 \\
%  AddNe & 6.4 & \cellcolor{green!25}19.5 & \cellcolor{green!25}18.5 & 11.5 & 11.6 & 32.9 & 27.4 & 24.5 & 32.4 & 19.4 \\
%  Anton & 18.0 & \cellcolor{green!25}32.2 & \cellcolor{green!25}17.2 & \cellcolor{green!25}31.0 & \cellcolor{green!25}26.7 & \cellcolor{green!25}51.1 & \cellcolor{green!25}50.0 & \cellcolor{green!25}54.0 & \cellcolor{green!25}51.6 & \cellcolor{green!25}33.1 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 43.7 & 43.5 & 37.3 & 32.1 & 30.0 & 37.2 & 50.8 & 52.3 & 58.1 & 50.8 \\
%  Jumble & \cellcolor{green!25}47.4 & \cellcolor{green!25}54.3 & \cellcolor{green!25}45.8 & \cellcolor{green!25}40.7 & \cellcolor{green!25}30.7 & \cellcolor{green!25}66.4 & \cellcolor{green!25}74.6 & \cellcolor{green!25}66.3 & \cellcolor{green!25}80.8 & \cellcolor{green!25}67.7 \\ 
%  Typo & 41.7 & 41.0 & 29.7 & 23.5 & 18.8 & \cellcolor{green!25}53.1 & \cellcolor{green!25}65.9 & \cellcolor{green!25}57.3 & \cellcolor{green!25}70.5 & \cellcolor{green!25}57.5 \\ 
%  AddNe & 29.3 & 41.9 & \cellcolor{green!25}44.3 & 26.8 & 26.5 & 33.3 & 32.4 & 27.2 & 43.0 & 31.6 \\
%  Anton & 32.1 & 38.7 & 19.4 & 29.5 & 23.2 & \cellcolor{green!25}60.0 & \cellcolor{green!25}64.7 & \cellcolor{green!25}61.7 & \cellcolor{green!25}71.6 & \cellcolor{green!25}55.7 \\
%  % RpSt &  \\
%  % CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Pearson correlation ($|r|$) between different metrics and human evaluations on three CoudSourcing datasets. 
% % We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:crowdsource_pearson}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int}  \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%   BLE & 4.8 & 5.8 & 2.0 & 7.0 & 0.8 & 18.0 & 17.1 & 16.2 & 17.6 & 9.7 \\
%   BER & 5.5 & 7.3 & 19.7 & 7.8 & 9.9 & 42.8 & 37.2 & 33.1 & 38.9 & 34.0 \\
%   Mov & 4.9 & 9.1 & 22.6 & 11.0 & 15.2 & 22.1 & 24.7 & 28.8 & 28.8 & 32.9 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 0.6 & 17.6 & 17.6 & 1.4 & 6.1 & 24.7 & 31.1 & 26.8 & 32.3 & 33.6 \\  
% MAN & 30.8 & 45.5 & 31.8 & 21.5 & 17.7 & 1.4 & 7.2 & 2.2 & 4.1 & 10.3 \\ 
% StoER & 9.7 & 6.1 & 6.0 & 5.8 & 14.3 & 21.3 & 19.4 & 19.9 & 24.9 & 37.4 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 31.7 & 37.8 & 21.7 & 17.0 & 12.2 & 59.2 & 67.9 & 55.0 & 70.3 & 62.6  \\ 
%  UNIE & 44.3 & 45.2 & 33.3 & 29.0 & 26.6 & 52.6 & 56.8 & 52.7 & 67.4 & 52.7 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
%   -- & 35.4 & 30.1 & 29.9 & 22.4 & 27.8 & 22.3 & 19.1 & 23.7 & 22.4 & 12.3  \\
%  Jumble & \cellcolor{green!25}48.4 & \cellcolor{green!25}51.2 & \cellcolor{green!25}31.2 & \cellcolor{green!25}33.2 & \cellcolor{green!25}36.4 & \cellcolor{green!25}70.1 & \cellcolor{green!25}71.0 & \cellcolor{green!25}58.6 & \cellcolor{green!25}73.6 & \cellcolor{green!25}59.3 \\
%  Typo & \cellcolor{green!25}43.2 & \cellcolor{green!25}38.6 & 29.6 & 20.5 & 24.6 & \cellcolor{green!25}64.5 & \cellcolor{green!25}64.1 & \cellcolor{green!25}52.5 & \cellcolor{green!25}69.6 & \cellcolor{green!25}54.0 \\
%  AddNe & 14.0 & 24.3 & 22.2 & 8.6 & 11.4 & \cellcolor{green!25}33.7 & \cellcolor{green!25}32.6 & \cellcolor{green!25}34.8 & \cellcolor{green!25}36.2 & \cellcolor{green!25}21.1 \\
%  Anton & 17.0 & 22.9 & 9.9 & 13.3 & 11.8 & \cellcolor{green!25}65.4 & \cellcolor{green!25}62.9 & \cellcolor{green!25}46.6 & \cellcolor{green!25}65.6 & \cellcolor{green!25}56.0 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%   \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
%    -- & 42.9 & 37.2 & 36.2 & 31.1 & 32.4 & 27.0 & 30.6 & 31.9 & 32.6 & 22.4  \\
%  Jumble & \cellcolor{green!25}57.8 & \cellcolor{green!25}58.9 & \cellcolor{green!25}39.9 & \cellcolor{green!25}35.8 & 32.2 & \cellcolor{green!25}73.3 & \cellcolor{green!25}73.3 & \cellcolor{green!25}60.8 & \cellcolor{green!25}77.2 & \cellcolor{green!25}63.1 \\
%  Typo & \cellcolor{green!25}48.7 & \cellcolor{green!25}44.3 & \cellcolor{green!25}39.8 & 30.5 & 31.0 & \cellcolor{green!25}61.0 & \cellcolor{green!25}62.0 & \cellcolor{green!25}50.6 & \cellcolor{green!25}66.2 & \cellcolor{green!25}50.2 \\
%  AddNe & 32.8 & \cellcolor{green!25}46.4 & \cellcolor{green!25}39.8 & 25.4 & 25.9 & \cellcolor{green!25}30.1 & \cellcolor{green!25}34.5 & \cellcolor{green!25}36.9 & \cellcolor{green!25}39.6 & \cellcolor{green!25}29.5 \\
%  Anton & 23.7 & 31.1 & 12.2 & 17.4 & 13.9 & \cellcolor{green!25}69.9 & \cellcolor{green!25}68.7 & \cellcolor{green!25}51.5 & \cellcolor{green!25}72.5 & \cellcolor{green!25}59.4 \\
% \midrule
 
%  \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL)}}\\ 
%    -- & 28.9 & 30.7 & 37.1 & 32.1 & 30.3 & 16.7 & 13.5 & 16.8 & 14.4 & 4.7 \\
%  Jumble & \cellcolor{green!25}50.5 & \cellcolor{green!25}41.5 & 29.5 & 28.4 & \cellcolor{green!25}31.2 & \cellcolor{green!25}78.3 & \cellcolor{green!25}72.9 & \cellcolor{green!25}58.2 & \cellcolor{green!25}76.2 & \cellcolor{green!25}61.9 \\
%  Typo & \cellcolor{green!25}34.8 & \cellcolor{green!25}32.7 & 34.2 & 24.4 & 22.6 & \cellcolor{green!25}41.4 & \cellcolor{green!25}38.9 & \cellcolor{green!25}36.8 & \cellcolor{green!25}42.6 & \cellcolor{green!25}26.4 \\
%  AddNe & 22.9 & \cellcolor{green!25}33.6 & 24.2 & 15.0 & 14.4 & \cellcolor{green!25}34.1 & \cellcolor{green!25}39.4 & \cellcolor{green!25}41.8 & \cellcolor{green!25}44.7 & \cellcolor{green!25}31.8  \\
%  Anton & 13.4 & 22.5 & 4.6 & 10.5 & 7.7 & \cellcolor{green!25}68.0 & \cellcolor{green!25}64.4 & \cellcolor{green!25}52.0 & \cellcolor{green!25}70.1 & \cellcolor{green!25}59.9  \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- & 34.4 & 15.8 & 15.4 & 15.7 & 19.8 & 41.6 & 34.1 & 42.7 & 41.1 & 29.2 \\
%  Jumble & \cellcolor{green!25}40.6 & \cellcolor{green!25}25.7 & 14.4 & \cellcolor{green!25}17.6 & 17.8 & \cellcolor{green!25}64.2 & \cellcolor{green!25}56.2 & \cellcolor{green!25}59.5 & \cellcolor{green!25}62.4 & \cellcolor{green!25}53.6 \\
%  Typo & 31.9 & 14.9 & \cellcolor{green!25}18.0 & 11.1 & 16.0 & \cellcolor{green!25}55.2 & \cellcolor{green!25}47.9 & \cellcolor{green!25}54.3 & \cellcolor{green!25}56.4 & \cellcolor{green!25}43.3  \\
%  AddNe & 11.8 & \cellcolor{green!25}23.6 & \cellcolor{green!25}25.1 & \cellcolor{green!25}16.4 & 13.9 & 33.7 & 23.7 & 19.5 & 28.5 & 16.7 \\
%  Anton & 19.8 & \cellcolor{green!25}31.0 & \cellcolor{green!25}16.6 & \cellcolor{green!25}29.8 & \cellcolor{green!25}23.4 & \cellcolor{green!25}55.7 & \cellcolor{green!25}54.4 & \cellcolor{green!25}48.3 & \cellcolor{green!25}59.5 & \cellcolor{green!25}49.3  \\
%  % RpSt & 1.6 & \cellcolor{green!25}16.3 & 11.2 & \cellcolor{green!25}23.1 & 14.4 &  \\
%  % CS & 0.6 & 10.4 & 9.5 & \cellcolor{green!25}20.6 & 14.1 &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 42.7 & 39.5 & 35.7 & 30.0 & 27.6 & 43.7 & 48.2 & 47.6 & 51.8 & 42.2  \\
%  Jumble & \cellcolor{green!25}49.3 & \cellcolor{green!25}55.3 & \cellcolor{green!25}48.5 & \cellcolor{green!25}41.0 & \cellcolor{green!25}32.9 & \cellcolor{green!25}70.2 & \cellcolor{green!25}66.6 & \cellcolor{green!25}51.9 & \cellcolor{green!25}70.5 & \cellcolor{green!25}57.9 \\ 
%  Typo & 41.0 & \cellcolor{green!25}40.4 & 33.5 & 23.0 & 16.8 & \cellcolor{green!25}57.0 & \cellcolor{green!25}60.0 & 44.8 & \cellcolor{green!25}63.0 & \cellcolor{green!25}48.7 \\ 
%  AddNe & 30.2 & \cellcolor{green!25}42.7 & \cellcolor{green!25}48.7 & 25.5 & 25.8 & 37.0 & 32.2 & 21.8 & 37.0 & 24.0 \\
%  Anton & 30.2 & 38.8 & 22.4 & \cellcolor{green!25}30.7 & 26.5 & \cellcolor{green!25}64.0 & \cellcolor{green!25}64.0 & \cellcolor{green!25}50.5 & \cellcolor{green!25}68.3 & \cellcolor{green!25}55.3  \\
%  % RpSt & 0.8 & 2.7 & 8.8 & 15.1 & 11.2 &  \\
%  % CS & 15.6 & 15.3 & 19.3 & 1.0 & 0.4 &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Spearman correlation ($|\rho|$) between different metrics and human evaluations on three CoudSourcing datasets. 
% % We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:crowdsource_spearman}
% \end{table*}



% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int}  \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 1.6 & 3.8 & 3.4 & 8.3 & 1.4 & 12.5 & 13.7 & 12.1 & 17.2 & 13.1 \\ 
%  BER & 3.1 & 6.2 & 18.3 & 3.8 & 6.2 & 47.1 & 42.6 & 35.1 & 42.4 & 36.6 \\
%  Mov & 2.2 & 7.4 & 23.3 & 3.3 & 9.7 & 26.8 & 28.6 & 32.3 & 31.8 & 32.3 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 4.8 & 12.6 & 11.6 & 19.5 & 13.6 & 14.7 & 20.2 & 4.4 & 12.1 & 27.5 \\  
% MAN & 9.3 & 16.2 & 11.3 & 15.4 & 5.0 & 24.0 & 12.5 & 19.3 & 33.0 & 8.7 \\ 
% StoER & 3.2 & 8.8 & 8.0 & 1.8 & 15.2 & 13.9 & 22.7 & 6.6 & 17.3 & 34.2 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 23.9 & 42.3 & 20.0 & 35.9 & 28.9 & 53.0 & 77.7 & 59.2 & 70.4 & 59.9 \\ 
%  UNIE & 50.7 & 30.3 & 9.3 & 31.8 & 16.5 & 48.6 & 37.9 & 39.5 & 34.5 & 30.2 \\
%  \midrule
%    \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
%   -- & 22.0 & 31.2 & 14.1 & 32.4 & 12.5 & 30.9 & 32.0 & 42.5 & 42.8 & 17.1 \\
%  Jumble & \cellcolor{green!25}34.0 & \cellcolor{green!25}45.8 & \cellcolor{green!25}17.0 & \cellcolor{green!25}44.5 & \cellcolor{green!25}37.3 & \cellcolor{green!25}45.1 & \cellcolor{green!25}50.7 & \cellcolor{green!25}52.8 & \cellcolor{green!25}51.8 & \cellcolor{green!25}38.0 \\
%  Typo & \cellcolor{green!25}32.9 & \cellcolor{green!25}44.8 & \cellcolor{green!25}16.7 & \cellcolor{green!25}40.3 & \cellcolor{green!25}25.8 & \cellcolor{green!25}54.6 & \cellcolor{green!25}63.2 & \cellcolor{green!25}57.5 & \cellcolor{green!25}64.6 & \cellcolor{green!25}42.8 \\
%  AddNe & 7.0 & 24.5 & 2.9 & 13.5 & \cellcolor{green!25}14.4 & 18.6 & \cellcolor{green!25}33.9 & 27.2 & 32.0 & 5.9 \\
%  Anton & 11.2 & 13.9 & 3.4 & 9.0 & 10.4 & \cellcolor{green!25}41.8 & \cellcolor{green!25}46.7 & \cellcolor{green!25}49.0 & \cellcolor{green!25}48.8 & \cellcolor{green!25}41.5 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%   \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
%    -- & 26.8 & 41.2 & 23.3 & 44.6 & 24.0 & 34.9 & 36.1 & 45.9 & 46.1 & 23.0 \\
%  Jumble & \cellcolor{green!25}33.5 & \cellcolor{green!25}59.7 & \cellcolor{green!25}29.6 & \cellcolor{green!25}57.5 & \cellcolor{green!25}48.4 & \cellcolor{green!25}55.2 & \cellcolor{green!25}61.5 & \cellcolor{green!25}59.4 & \cellcolor{green!25}63.5 & \cellcolor{green!25}54.4 \\
%  Typo & \cellcolor{green!25}30.8 & \cellcolor{green!25}54.1 & 22.4 & \cellcolor{green!25}52.2 & \cellcolor{green!25}36.1 & \cellcolor{green!25}58.6 & \cellcolor{green!25}66.4 & \cellcolor{green!25}60.7 & \cellcolor{green!25}68.8 & \cellcolor{green!25}52.5  \\
%  AddNe & 19.6 & \cellcolor{green!25}46.2 & 13.6 & 28.7 & \cellcolor{green!25}26.0 & 28.5 & \cellcolor{green!25}42.6 & 36.2 & 40.3 & 12.3 \\
%  Anton & 21.9 & 27.4 & 15.0 & 22.6 & 21.4 & \cellcolor{green!25}44.2 & \cellcolor{green!25}54.0 & \cellcolor{green!25}54.8 & \cellcolor{green!25}54.3 & \cellcolor{green!25}48.3 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL)}}\\ 
%    -- & 25.6 & 26.1 & 15.2 & 29.5 & 9.4 & 24.6 & 22.5 & 36.4 & 31.9 & 6.8 \\
%  Jumble & \cellcolor{green!25}36.3 & \cellcolor{green!25}49.2 & \cellcolor{green!25}28.8 & \cellcolor{green!25}40.3 & \cellcolor{green!25}30.1 & \cellcolor{green!25}42.8 & \cellcolor{green!25}41.4 & \cellcolor{green!25}46.5 & \cellcolor{green!25}50.6 & \cellcolor{green!25}38.8  \\
%  Typo & 21.4 & \cellcolor{green!25}31.7 & 14.1 & \cellcolor{green!25}35.6 & \cellcolor{green!25}20.2 & \cellcolor{green!25}37.4 & \cellcolor{green!25}44.2 & \cellcolor{green!25}49.8 & \cellcolor{green!25}47.5 & \cellcolor{green!25}22.5 \\
%  AddNe & \cellcolor{green!25}30.1 & \cellcolor{green!25}37.8 & \cellcolor{green!25}24.2 & 27.5 & \cellcolor{green!25}31.8 & 10.0 & \cellcolor{green!25}29.8 & 30.1 & 28.7 & 0.8 \\
%  Anton & 17.4 & 20.1 & 9.3 & 14.4 & \cellcolor{green!25}12.8 & \cellcolor{green!25}42.6 & \cellcolor{green!25}42.5 & \cellcolor{green!25}47.5 & \cellcolor{green!25}43.8 & \cellcolor{green!25}36.5 \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- & 11.4 & 20.0 & 15.4 & 20.5 & 6.9 & 14.2 & 15.8 & 36.7 & 30.1 & 2.8 \\
%  Jumble & \cellcolor{green!25}15.0 & \cellcolor{green!25}33.6 & \cellcolor{green!25}21.8 & \cellcolor{green!25}31.5 & \cellcolor{green!25}18.8 & \cellcolor{green!25}38.4 & \cellcolor{green!25}36.9 & \cellcolor{green!25}48.4 & \cellcolor{green!25}46.4 & \cellcolor{green!25}26.8 \\
%  Typo & \cellcolor{green!25}19.6 & \cellcolor{green!25}39.5 & \cellcolor{green!25}28.2 & \cellcolor{green!25}32.5 & \cellcolor{green!25}23.8 & \cellcolor{green!25}32.7 & \cellcolor{green!25}42.9 & \cellcolor{green!25}51.6 & \cellcolor{green!25}49.8 & \cellcolor{green!25}28.4 \\
%  AddNe & 10.9 & \cellcolor{green!25}27.9 & \cellcolor{green!25}16.9 & 14.3 & \cellcolor{green!25}21.3 & 13.2 & \cellcolor{green!25}27.6 & 30.1 & 25.0 & \cellcolor{green!25}3.1 \\
%  Anton & \cellcolor{green!25}12.6 & 12.0 & 11.3 & 6.1 & \cellcolor{green!25}10.9 & \cellcolor{green!25}42.1 & \cellcolor{green!25}44.9 & \cellcolor{green!25}54.8 & \cellcolor{green!25}46.6 & \cellcolor{green!25}39.0 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 31.6 & 46.2 & 24.2 & 41.6 & 29.7 &  47.2 & 49.1 & 54.8 & 57.5 & 39.5 \\
%  Jumble & 28.9 & \cellcolor{green!25}50.3 & \cellcolor{green!25}26.4 & \cellcolor{green!25}53.5 & \cellcolor{green!25}43.5 & 45.6 & \cellcolor{green!25}58.7 & \cellcolor{green!25}62.8 & \cellcolor{green!25}60.4 & \cellcolor{green!25}54.2  \\ 
%  Typo & 30.5 & 46.2 & 21.3 & \cellcolor{green!25}46.9 & \cellcolor{green!25}38.5 & \cellcolor{green!25}57.6 & \cellcolor{green!25}68.8 & \cellcolor{green!25}58.3 & \cellcolor{green!25}68.6 & \cellcolor{green!25}61.2 \\ 
%  AddNe &  20.9 & 43.7 & 20.7 & 33.5 & 29.0 & 35.8 & 44.6 & 34.0 & 36.3 & 23.1 \\
%  Anton &  23.0 & 32.1 & 14.3 & 29.8 & 25.3 & \cellcolor{green!25}47.7 & \cellcolor{green!25}64.0 & \cellcolor{green!25}64.4 & \cellcolor{green!25}59.9 & \cellcolor{green!25}52.3 \\
%  % RpSt &  \\
%  % CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Pearson correlation ($|r|$) between different metrics and human evaluations on three Inhouse datasets. 
% % We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:inhouse_pearson}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int}  \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 1.6 & 3.8 & 3.4 & 8.3 & 1.4 & 12.5 & 13.7 & 12.1 & 17.2 & 13.1 \\ 
%  BER & 3.1 & 6.2 & 18.3 & 3.8 & 6.2 & 47.1 & 42.6 & 35.1 & 42.4 & 36.6 \\
%  Mov & 2.2 & 7.4 & 23.3 & 3.3 & 9.7 & 26.8 & 28.6 & 32.3 & 31.8 & 32.3 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 11.2 & 5.2 & 17.5 & 13.9 & 9.3 & 17.8 & 27.7 & 11.0 & 18.5 & 36.1 \\  
% MAN & 13.5 & 26.7 & 14.9 & 19.3 & 7.2 & 20.5 & 0.4 & 14.1 & 23.7 & 4.7 \\ 
% StoER & 5.4 & 11.9 & 14.3 & 1.0 & 12.6 & 11.7 & 22.1 & 12.5 & 21.4 & 32.5 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 23.5 & 41.3 & 25.3 & 35.7 & 25.8 & 51.3 & 76.6 & 53.6 & 68.0 & 57.2 \\ 
%  UNIE & 43.0 & 41.5 & 13.9 & 34.0 & 26.1 & 61.6 & 56.3 & 49.4 & 51.9 & 51.0 \\
%  \midrule
%    \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
%   -- & 17.2 & 33.1 & 18.4 & 36.4 & 14.7 & 31.1 & 26.9 & 36.7 & 40.3 & 15.4 \\
%  Jumble & \cellcolor{green!25}32.9 & \cellcolor{green!25}45.2 & \cellcolor{green!25}20.3 & \cellcolor{green!25}44.1 & \cellcolor{green!25}32.6 & \cellcolor{green!25}45.5 & \cellcolor{green!25}46.2 & \cellcolor{green!25}44.1 & \cellcolor{green!25}44.9 & \cellcolor{green!25}31.1 \\
%  Typo & \cellcolor{green!25}29.8 & \cellcolor{green!25}41.6 & 17.9 & \cellcolor{green!25}37.9 & \cellcolor{green!25}19.3 & \cellcolor{green!25}55.5 & \cellcolor{green!25}57.7 & \cellcolor{green!25}49.7 & \cellcolor{green!25}61.5 & \cellcolor{green!25}41.3 \\
%  AddNe & 5.9 & 24.7 & 5.4 & 14.0 & 11.0 & 22.0 & \cellcolor{green!25}35.8 & 24.4 & 37.5 & \cellcolor{green!25}20.3 \\
%  Anton & 7.8 & 13.4 & 5.4 & 8.2 & 5.8 & \cellcolor{green!25}41.9 & \cellcolor{green!25}46.0 & \cellcolor{green!25}44.1 & \cellcolor{green!25}47.1 & \cellcolor{green!25}43.7 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%   \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
%    -- & 23.7 & 41.9 & 27.9 & 45.6 & 22.8 & 35.1 & 32.0 & 41.5 & 42.4 & 19.9 \\
%  Jumble & \cellcolor{green!25}32.8 & \cellcolor{green!25}57.9 & \cellcolor{green!25}31.4 & \cellcolor{green!25}57.0 & \cellcolor{green!25}44.3 & \cellcolor{green!25}53.8 & \cellcolor{green!25}52.4 & \cellcolor{green!25}48.0 & \cellcolor{green!25}55.7 & \cellcolor{green!25}43.7 \\
%  Typo & \cellcolor{green!25}27.5 & \cellcolor{green!25}52.5 & 23.8 & \cellcolor{green!25}52.2 & \cellcolor{green!25}31.9 & \cellcolor{green!25}56.7 & \cellcolor{green!25}57.3 & \cellcolor{green!25}51.2 & \cellcolor{green!25}62.4 & \cellcolor{green!25}45.4  \\
%  AddNe & 22.1 & \cellcolor{green!25}45.9 & 19.6 & 29.7 & \cellcolor{green!25}23.7 & \cellcolor{green!25}39.2 & \cellcolor{green!25}42.6 & 35.7 & \cellcolor{green!25}44.1 & \cellcolor{green!25}22.6 \\
%  Anton & 20.4 & 27.2 & 17.0 & 22.0 & 15.5 & \cellcolor{green!25}47.8 & \cellcolor{green!25}55.5 & \cellcolor{green!25}49.6 & \cellcolor{green!25}55.1 & \cellcolor{green!25}51.2 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL)}}\\ 
%    -- & 21.6 & 27.6 & 21.0 & 33.0 & 12.3 & 25.3 & 16.8 & 29.8 & 28.0 & 4.6 \\
%  Jumble & \cellcolor{green!25}33.2 & \cellcolor{green!25}47.9 & \cellcolor{green!25}31.2 & \cellcolor{green!25}38.7 & \cellcolor{green!25}27.4 & \cellcolor{green!25}43.1 & \cellcolor{green!25}38.9 & \cellcolor{green!25}33.6 & \cellcolor{green!25}49.5 & \cellcolor{green!25}33.5  \\
%  Typo & 19.2 & \cellcolor{green!25}32.7 & 14.6 & \cellcolor{green!25}37.4 & \cellcolor{green!25}22.7 & \cellcolor{green!25}41.9 & \cellcolor{green!25}39.3 & \cellcolor{green!25}43.7 & \cellcolor{green!25}46.6 & \cellcolor{green!25}26.8 \\
%  AddNe & \cellcolor{green!25}28.5 & \cellcolor{green!25}33.9 & \cellcolor{green!25}24.2 & 25.6 & \cellcolor{green!25}28.5 & 19.6 & \cellcolor{green!25}33.0 & 28.1 & \cellcolor{green!25}34.5 & \cellcolor{green!25}13.5 \\
%  Anton & 15.5 & 20.2 & 11.0 & 13.8 & 9.7 & \cellcolor{green!25}47.8 & \cellcolor{green!25}47.1 & \cellcolor{green!25}42.3 & \cellcolor{green!25}47.1 & \cellcolor{green!25}39.3 \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- & 6.2 & 19.6 & 16.7 & 21.0 & 3.7 & 20.6 & 18.8 & 30.0 & 34.8 & 12.5 \\
%  Jumble & \cellcolor{green!25}12.3 & \cellcolor{green!25}31.9 & \cellcolor{green!25}21.3 & \cellcolor{green!25}30.6 & \cellcolor{green!25}16.3 & \cellcolor{green!25}42.9 & \cellcolor{green!25}32.4 & \cellcolor{green!25}37.2 & \cellcolor{green!25}47.5 & \cellcolor{green!25}30.4 \\
%  Typo & \cellcolor{green!25}11.4 & \cellcolor{green!25}28.1 & \cellcolor{green!25}24.5 & \cellcolor{green!25}26.1 & \cellcolor{green!25}11.0 & \cellcolor{green!25}35.5 & \cellcolor{green!25}37.5 & \cellcolor{green!25}40.6 & \cellcolor{green!25}51.6 & \cellcolor{green!25}32.6 \\
%  AddNe & \cellcolor{green!25}10.0 & \cellcolor{green!25}24.1 & \cellcolor{green!25}20.4 & 12.1 & \cellcolor{green!25}20.9 & \cellcolor{green!25}21.7 & \cellcolor{green!25}21.2 & 20.5 & 25.9 & 3.5 \\
%  Anton & 9.1 & 12.7 & 10.2 & 8.4 & \cellcolor{green!25}9.9 & \cellcolor{green!25}40.7 & \cellcolor{green!25}38.4 & \cellcolor{green!25}49.4 & \cellcolor{green!25}44.5 & \cellcolor{green!25}35.4 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 28.8 & 43.1 & 26.8 & 39.7 & 25.9 & 47.9 & 44.7 & 48.6 & 52.3 & 34.5 \\
%  Jumble & 26.8 & \cellcolor{green!25}48.2 & 26.7 & \cellcolor{green!25}51.9 & \cellcolor{green!25}40.8 & 44.3 & \cellcolor{green!25}51.9 & \cellcolor{green!25}53.2 & \cellcolor{green!25}52.6 & \cellcolor{green!25}43.1  \\ 
%  Typo & 24.2 & 40.8 & 21.8 & \cellcolor{green!25}46.2 & \cellcolor{green!25}31.8 & \cellcolor{green!25}56.9 & \cellcolor{green!25}62.7 & \cellcolor{green!25}51.3 & \cellcolor{green!25}63.3 & \cellcolor{green!25}53.7 \\ 
%  AddNe & 17.5 & 40.3 & 20.8 & 31.5 & 23.1 & 40.3 & 38.4 & 28.9 & 33.0 & 22.3 \\
%  Anton & 24.9 & 29.6 & 18.7 & 26.7 & 17.6 & \cellcolor{green!25}50.1 & \cellcolor{green!25}60.3 & \cellcolor{green!25}56.9 & \cellcolor{green!25}55.2 & \cellcolor{green!25}55.0 \\
%  % RpSt &  \\
%  % CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Spearman correlation ($|\rho|$) between different metrics and human evaluations on three Inhouse datasets. 
% % We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:inhouse_spearman}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}}  \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int}  \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 1.6 & 3.8 & 3.4 & 8.3 & 1.4 & 12.5 & 13.7 & 12.1 & 17.2 & 13.1 \\ 
%  BER & 3.1 & 6.2 & 18.3 & 3.8 & 6.2 & 47.1 & 42.6 & 35.1 & 42.4 & 36.6 \\
%  Mov & 2.2 & 7.4 & 23.3 & 3.3 & 9.7 & 26.8 & 28.6 & 32.3 & 31.8 & 32.3 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 8.9 & 3.7 & 11.7 & 9.7 & 6.1 & 13.4 & 20.3 & 7.7 & 12.7 & 25.9 \\  
% MAN & 10.5 & 19.4 & 10.2 & 13.6 & 5.4 & 14.7 & 0.2 & 10.5 & 16.7 & 2.2 \\ 
% StoER & 4.1 & 8.4 & 10.0 & 0.8 & 8.2 & 8.3 & 16.9 & 9.0 & 15.0 & 22.5 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 17.5 & 30.3 & 17.9 & 25.3 & 18.1 & 37.8 & 60.4 & 39.2 & 50.0 & 42.2 \\ 
%  UNIE & 31.8 & 30.7 & 9.9 & 24.5 & 18.6 & 45.7 & 40.5 & 37.1 & 36.2 & 36.7 \\
%  \midrule
%    \multicolumn{10}{l}{\textbf{\deltascore (with BLOOM 7B)}}\\ 
%   -- & 12.4 & 24.1 & 13.3 & 25.5 & 10.2 & 24.8 & 21.3 & 27.9 & 29.2 & 12.6 \\
%  Jumble & \cellcolor{green!25}24.7 & \cellcolor{green!25}33.5 & \cellcolor{green!25}13.8 & \cellcolor{green!25}32.5 & \cellcolor{green!25}23.2 & \cellcolor{green!25}32.9 & \cellcolor{green!25}33.7 & \cellcolor{green!25}31.5 & \cellcolor{green!25}30.7 & \cellcolor{green!25}21.6 \\
%  Typo & \cellcolor{green!25}22.1 & \cellcolor{green!25}30.4 & 12.6 & \cellcolor{green!25}27.3 & \cellcolor{green!25}12.8 & \cellcolor{green!25}40.7 & \cellcolor{green!25}42.0 & \cellcolor{green!25}35.2 & \cellcolor{green!25}44.4 & \cellcolor{green!25}28.6 \\
%  AddNe & 4.6 & 17.7 & 4.0 & 10.5 & 8.0 & 15.6 & \cellcolor{green!25}25.2 & 17.0 & 26.6 & \cellcolor{green!25}14.4 \\
%  Anton & 4.9 & 9.3 & 4.2 & 5.5 & 4.2 & \cellcolor{green!25}29.4 & \cellcolor{green!25}33.5 & \cellcolor{green!25}31.0 & \cellcolor{green!25}33.6 & \cellcolor{green!25}31.2 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%   \multicolumn{10}{l}{\textbf{\deltascore (with OPT 66B)}}\\ 
%    -- & 17.4 & 30.6 & 20.2 & 32.6 & 15.8 & 27.5 & 24.5 & 32.2 & 31.0 & 15.7 \\
%  Jumble & \cellcolor{green!25}24.2 & \cellcolor{green!25}42.9 & \cellcolor{green!25}21.8 & \cellcolor{green!25}41.2 & \cellcolor{green!25}31.8 & \cellcolor{green!25}38.8 & \cellcolor{green!25}38.5 & \cellcolor{green!25}33.8 & \cellcolor{green!25}39.3 & \cellcolor{green!25}30.7 \\
%  Typo & \cellcolor{green!25}20.4 & \cellcolor{green!25}39.1 & 16.4 & \cellcolor{green!25}37.7 & \cellcolor{green!25}22.3 & \cellcolor{green!25}41.1 & \cellcolor{green!25}41.7 & \cellcolor{green!25}37.0 & \cellcolor{green!25}45.4 & \cellcolor{green!25}31.2 \\
%  AddNe & 15.8 & \cellcolor{green!25}33.1 & 13.0 & 21.2 & \cellcolor{green!25}17.3 & 27.0 & \cellcolor{green!25}30.3 & 25.4 & 30.1 & 15.3 \\
%  Anton & 13.9 & 18.7 & 11.7 & 15.1 & 10.6 & \cellcolor{green!25}33.7 & \cellcolor{green!25}41.1 & \cellcolor{green!25}36.0 & \cellcolor{green!25}40.3 & \cellcolor{green!25}37.3 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with FLAN-T5 XXL)}}\\ 
%    -- & 16.2 & 19.4 & 14.7 & 23.1 & 8.9 & 19.9 & 14.1 & 23.4 & 20.8 & 4.8 \\
%  Jumble & \cellcolor{green!25}24.5 & \cellcolor{green!25}36.0 & \cellcolor{green!25}22.4 & \cellcolor{green!25}27.2 & \cellcolor{green!25}19.0 & \cellcolor{green!25}31.2 & \cellcolor{green!25}27.4 & 22.5 & \cellcolor{green!25}35.0 & \cellcolor{green!25}23.1 \\
%  Typo & 14.2 & \cellcolor{green!25}24.1 & 10.6 & \cellcolor{green!25}26.6 & \cellcolor{green!25}16.3 & \cellcolor{green!25}31.4 & \cellcolor{green!25}28.8 & \cellcolor{green!25}31.5 & \cellcolor{green!25}33.0 & \cellcolor{green!25}19.0 \\
%  AddNe & \cellcolor{green!25}21.0 & \cellcolor{green!25}24.5 & \cellcolor{green!25}17.1 & 18.3 & \cellcolor{green!25}20.1 & 13.6 & \cellcolor{green!25}24.3 & 20.2 & \cellcolor{green!25}24.4 & \cellcolor{green!25}9.8 \\
%  Anton & 10.7 & 14.3 & 8.3 & 9.9 & 6.6 & \cellcolor{green!25}34.3 & \cellcolor{green!25}33.8 & \cellcolor{green!25}29.8 & \cellcolor{green!25}34.0 & \cellcolor{green!25}27.3 \\
% \midrule
% \multicolumn{10}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- & 4.5 & 14.6 & 11.6 & 14.5 & 2.3 & 15.0 & 14.5 & 22.4 & 24.5 & 10.6 \\
%  Jumble & \cellcolor{green!25}8.8 & \cellcolor{green!25}23.4 & \cellcolor{green!25}15.3 & \cellcolor{green!25}21.0 & \cellcolor{green!25}10.9 & \cellcolor{green!25}31.5 & \cellcolor{green!25}23.5 & \cellcolor{green!25}26.7 & \cellcolor{green!25}33.8 & \cellcolor{green!25}22.0 \\
%  Typo & \cellcolor{green!25}8.3 & \cellcolor{green!25}20.1 & \cellcolor{green!25}17.2 & \cellcolor{green!25}18.2 & \cellcolor{green!25}7.8 & \cellcolor{green!25}25.8 & \cellcolor{green!25}28.3 & \cellcolor{green!25}29.4 & \cellcolor{green!25}36.8 & \cellcolor{green!25}24.3 \\
%  AddNe & \cellcolor{green!25}7.5 & \cellcolor{green!25}17.0 & \cellcolor{green!25}14.1 & 8.7 & \cellcolor{green!25}15.1 & \cellcolor{green!25}15.3 & 14.4 & 14.5 & 17.8 & 2.1 \\
%  Anton & \cellcolor{green!25}5.8 & 8.9 & 7.1 & 6.2 & \cellcolor{green!25}7.2 & \cellcolor{green!25}27.8 & \cellcolor{green!25}26.9 & \cellcolor{green!25}36.1 & \cellcolor{green!25}31.6 & \cellcolor{green!25}24.7 \\
%  % RpSt &  \\
%  % CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 21.3 & 31.2 & 18.7 & 28.6 & 18.2 & 36.2 & 32.9 & 36.0 & 37.1 & 23.3  \\
%  Jumble & 19.4 & \cellcolor{green!25}34.6 & \cellcolor{green!25}19.4 & \cellcolor{green!25}37.6 & \cellcolor{green!25}30.1 & 31.7 & \cellcolor{green!25}38.4 & \cellcolor{green!25}37.2 & \cellcolor{green!25}37.4 & \cellcolor{green!25}30.8  \\ 
%  Typo & 17.7 & 29.8 & 15.2 & \cellcolor{green!25}33.7 & \cellcolor{green!25}23.0 & \cellcolor{green!25}42.4 & \cellcolor{green!25}46.9 & \cellcolor{green!25}36.4 & \cellcolor{green!25}47.2 & \cellcolor{green!25}38.7 \\ 
%  AddNe & 12.3 & 28.7 & 14.2 & 22.0 & 15.9 & 29.1 & 27.0 & 19.3 & 22.6 & 15.9 \\
%  Anton & 18.5 & 20.8 & 13.6 & 19.0 & 11.7 & 36.2 & \cellcolor{green!25}45.1 & \cellcolor{green!25}41.6 & \cellcolor{green!25}40.6 & \cellcolor{green!25}40.6  \\
%  % RpSt &  \\
%  % CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Kendall correlation ($|\tau|$) between different metrics and human evaluations on three Inhouse datasets. 
% % We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:inhouse_kendall}
% \end{table*}


% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 1.6 & 3.8 & 3.4 & 8.3 & 1.4 & 33.5 & 20.7 & 17.8 & 24.4 & 19.0 & 6.8 & 5.7 & 1.1 & 1.1 & 2.6 \\ 
%  BER & 3.1 & 6.2 & 18.3 & 3.8 & 6.2 & 37.8 & 33.3 & 28.9 & 33.9 & 30.2 & 46.9 & 40.2 & 57.9 & 51.8 & 15.1  \\
%  Mov & 2.2 & 7.4 & 23.3 & 3.3 & 9.7 & 19.0 & 25.4 & 21.8 & 25.4 & 32.0 & 32.3 & 25.1 & 38.4 & 36.1 & 11.0 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO &  \\  
% MAN &  \\ 
% StoER &  \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC &  \\ 
%  UNIE &  \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (with BART-large)}}\\ 
%    -- &  \\
%  Jumble &  \\
%  Typo & \\
%  Anton &  \\
%  RpSt &  \\
% \midrule
% \multicolumn{12}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 43.8 & 43.5 & 37.3 & 32.2 & 30.0 & 27.4 & 44.3 & 38.4 & 51.0 & 50.1 & 56.2 & 70.4 & 73.1 & 69.8 & 60.9\\
%  Jumble & 46.3 & 47.2 & 38.8 & 33.3 & 23.8 & 67.4 & 74.5 & 62.5 & 81.6 & 67.4 & 50.3 & 65.6 & 63.0 & 65.7 & 54.5 \\ 
%  Typo & 47.3 & 45.4 & 41.0 & 30.4 & 24.7 & 49.6 & 60.3 & 52.6 & 68.5 & 60.0 & 59.1 & 70.4 & 73.8 & 71.0 & 59.2 \\ 
%  AddNe & 30.5 & 43.0 & 44.9 & 28.5 & 27.4 & 17.3 & 22.9 & 5.8 & 33.2 & 38.3 & 52.4 & 59.0 & 63.4 & 60.9 & 43.6  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Pearson correlation ($|r|$) between different metrics and human evaluations on three CoudSourcing datasets. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:crowdsource_pearson}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%   BLE & 4.8 & 5.8 & 2.0 & 7.0 & 0.8 & 27.0 & 19.6 & 19.8 & 21.2 & 14.3 & 15.3 & 0.9 & 3.4 & 0.3 & 5.3 \\
%   BER & 5.5 & 7.3 & 19.7 & 7.8 & 9.9 & 29.6 & 27.7 & 23.3 & 29.2 & 28.2 & 42.6 & 33.2 & 54.0 & 49.9 & 12.4 \\
%   Mov & 4.9 & 9.1 & 22.6 & 11.0 & 15.2 & 14.1 & 22.4 & 22.3 & 21.3 & 28.5 & 27.5 & 13.2 & 32.7 & 30.0 & 4.8 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO & 0.6 & 17.6 & 17.6 & 1.4 & 6.1 & 29.2 & 35.4 & 29.9 & 36.9 & 39.6 & 10.0 & 1.6 & 24.5 & 4.2 & 1.6 \\  
% MAN & 30.8 & 45.5 & 31.8 & 21.5 & 17.7 & 0.1 & 8.2 & 3.5 & 4.9 & 12.8 & 8.5 & 27.0 & 18.3 & 25.5 & 16.5 \\ 
% StoER & 9.7 & 6.1 & 6.0 & 5.8 & 14.3 & 25.8 & 23.7 & 18.9 & 31.2 & 34.5 & 6.7 & 6.6 & 0.1 & 10.5 & 8.4 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC & 31.7 & 37.8  & 21.7  & 17.0  & 12.2 & 60.0 & 70.1 & 52.1 & 72.8 & \textbf{63.8} & 40.0 & 52.1 & 56.2 & 58.6 & 44.6 \\ 
%  UNIE & 44.3 & 45.2 & 33.3 & 29.0 & 26.6 & 52.6 & 56.8 & 50.9 & 67.4 & 52.7 & 33.9 & 43.6 & 50.4 & 53.2 & 37.3 \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (with BART-large)}}\\ 
%    -- & 15.7 & 1.9 & 1.8 & 3.9 & 3.3 & 7.4 & 6.8 & 1.8 & 4.7 & 11.5 & 21.0 & 23.2 & 29.3 & 21.3 & 9.2 \\
%  Jumble & \cellcolor{green!25}22.8 & \cellcolor{green!25}8.5 & \cellcolor{green!25}11.5 & \cellcolor{green!25}11.6 & \cellcolor{green!25}4.0 & \cellcolor{green!25}8.7 & \cellcolor{green!25}6.9 & \cellcolor{green!25}7.1 & 2.5 & 10.6 & \cellcolor{green!25}32.8 & \cellcolor{green!25}29.3 & \cellcolor{green!25}34.2 & \cellcolor{green!25}30.6 & \cellcolor{green!25}33.1 \\
%  Typo & 9.6 & \cellcolor{green!25}4.6 & \cellcolor{green!25}6.2 & \cellcolor{green!25}5.4 & 2.7 & 3.0 & 3.0 & 1.5 & \cellcolor{green!25}7.0 & 9.0 & 19.9 & 4.0 & 1.2 & 8.5 & 7.5 \\
%  AddNe & \cellcolor{green!25}17.2 & \cellcolor{green!25}21.3 & \cellcolor{green!25}10.0 & \cellcolor{green!25}4.0 & \cellcolor{green!25}6.3 & 6.6 & \cellcolor{green!25}11.4 & \cellcolor{green!25}6.0 & \cellcolor{green!25}9.6 & 9.1 & 8.7 & \cellcolor{green!25}23.5 & 6.8 & 17.9 & \cellcolor{green!25}26.1 \\
%  Anton & 11.3 & \cellcolor{green!25}13.7 & 0.4 & \cellcolor{green!25}18.5 & \cellcolor{green!25}16.3 & \cellcolor{green!25}15.4 & \cellcolor{green!25}21.7 & \cellcolor{green!25}11.0 & \cellcolor{green!25}11.1 & 9.5 & \cellcolor{green!25}26.4 & 13.7 & 21.0 & 14.6 & 8.7 \\
%  RpSt & \cellcolor{green!25}21.7 & \cellcolor{green!25}14.2 & \cellcolor{green!25}9.3 & \cellcolor{green!25}11.2 & \cellcolor{green!25}7.3 & \cellcolor{green!25}12.2 & 1.2 & \cellcolor{green!25}13.3 & 3.4 & 4.7 & \cellcolor{green!25}23.1 & \cellcolor{green!25}30.7 & \cellcolor{green!25}39.3 & \cellcolor{green!25}30.4 & \cellcolor{green!25}22.9 \\
%  CS & 4.5 & 0.2 & \cellcolor{green!25}4.2 & \cellcolor{green!25}10.1 & \cellcolor{green!25}12.2 & \cellcolor{green!25}8.7 & 4.1 & \cellcolor{green!25}5.1 & 4.6 & 1.7 & 8.1 & 0.0 & 3.6 & 0.1 & 0.8 \\
% \midrule
% \multicolumn{12}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- & 34.4 & 15.8 & 15.4 & 15.7 & 19.8 & 31.3 & 26.5 & 25.0 & 31.6 & 22.6 & 48.2 & 39.7 & 38.0 & 46.8 & 28.2 \\
%  Jumble & \cellcolor{green!25}37.4 & \cellcolor{green!25}22.6 & 15.0 & 14.7 & 14.1 & \cellcolor{green!25}60.4 & \cellcolor{green!25}53.7 & \cellcolor{green!25}50.5 & \cellcolor{green!25}59.5 & \cellcolor{green!25}52.7 & \cellcolor{green!25}49.9 & \cellcolor{green!25}44.4 & \cellcolor{green!25}41.2 & \cellcolor{green!25}54.0 & 23.5 \\
%  Typo & 31.9 & 14.9 & \cellcolor{green!25}18.0 & 11.1 & 16.0 & \cellcolor{green!25}43.7 & \cellcolor{green!25}38.9 & \cellcolor{green!25}37.6 & \cellcolor{green!25}43.7 & \cellcolor{green!25}32.9 & \cellcolor{green!25}55.4 & \cellcolor{green!25}42.8 & \cellcolor{green!25}46.3 & \cellcolor{green!25}54.2 & \cellcolor{green!25}32.1 \\
%  AddNe & 11.8 & \cellcolor{green!25}23.6 & \cellcolor{green!25}25.1 & \cellcolor{green!25}16.4 & 13.9 & 17.0 & 22.6 & 9.2 & 23.8 & 22.6 & 27.0 & 19.4 & 20.6 & 24.7 & 14.2 \\
%  Anton & 20.1 & \cellcolor{green!25}30.7 & 14.4 & \cellcolor{green!25}25.4 & 19.8 & \cellcolor{green!25}56.2 & \cellcolor{green!25}55.2 & \cellcolor{green!25}58.0 & \cellcolor{green!25}61.2 & \cellcolor{green!25}50.5 & 13.1 & 15.5 & 5.5 & 15.2 & 14.0 \\
%  RpSt & 1.6 & \cellcolor{green!25}16.3 & 11.2 & \cellcolor{green!25}23.1 & 14.4 & 14.6 & 12.5 & 19.0 & 17.4 & 20.8 & 39.4 & 34.9 & \cellcolor{green!25}39.4 & 46.0 & 20.6 \\
%  CS & 0.6 & 10.4 & 9.5 & \cellcolor{green!25}20.6 & 14.1 & \cellcolor{green!25}36.7 & \cellcolor{green!25}33.7 & \cellcolor{green!25}29.5 & 28.6 & 21.1 & \cellcolor{green!25}53.6 & \cellcolor{green!25}51.0 & 35.9 & \cellcolor{green!25}51.1 & \cellcolor{green!25}41.0 \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 43.2 & 39.5 & 35.4 & 29.9 & 27.4 & 33.5 & 40.8 & 34.1 & 43.9 & 43.1 & 47.6 & 64.9 & 66.8 & 67.4 & 58.7 \\
%  Jumble & 50.4 & 47.3 & 43.5 & 33.3 & 25.5 & 72.8 & 68.6 & 51.3 & 72.6 & 58.8 & 53.1 & 62.3 & 61.8 & 69.1 & 43.2  \\ 
%  Typo & 47.6 & 42.3 & 41.1 & 28.5 & 22.8 & 56.9 & 57.1 & 45.9 & 62.1 & 51.2 & 58.0 & 65.0 & 70.5 & 70.8 & 55.5 \\ 
%  AddNe & 30.7 & \cellcolor{green!25}43.6 & \cellcolor{green!25}\textbf{49.1} & 26.5 & 26.7 & \cellcolor{green!25}38.7 & 35.1 & 21.8 & 41.9 & 35.3 & \cellcolor{green!25}48.0 & \cellcolor{green!25}60.8 & \cellcolor{green!25}65.0 & 62.9 & 46.1  \\
%  Anton & 32.2 & \cellcolor{green!25}40.1 & 26.4 & 29.7 & 19.4 & \cellcolor{green!25}\textbf{72.9} & \cellcolor{green!25}67.7 & \cellcolor{green!25}56.4 & \cellcolor{green!25}71.4 & \cellcolor{green!25}56.9 & \cellcolor{green!25}48.3 & 59.4 & 52.7 & \cellcolor{green!25}64.6 & 40.5 \\
%  RpSt & 0.8 & 2.7 & 8.8 & 15.1 & 11.2 & 25.2 & 26.4 & 28.7 & 30.6 & 35.0 & 31.4 & 42.6 & 52.4 & 53.8 & 42.6 \\
%  CS & 15.6 & 15.3 & 19.3 & 1.0 & 0.4 & \cellcolor{green!25}37.5 & \cellcolor{green!25}47.1 & \cellcolor{green!25}35.6 & 40.7 & 32.1 & 34.4 & 55.7 & 55.6 & 50.6 & 40.7 \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Spearman correlation ($|\rho|$) between different metrics and human evaluations on three CoudSourcing datasets. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:crowdsource_spearman}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 3.4 & 4.4 & 0.8 & 4.6 & 0.4 & 19.8 & 13.3 & 13.8 & 15.0 & 10.0 & 9.1 & 0.8 & 2.6 & 0.2 & 3.7\\ 
%  BER & 3.5 & 5.0 & 14.0 & 5.7 & 7.3 & 21.5 & 18.8 & 16.4 & 20.9 & 19.9 & 32.8 & 25.3 & 40.9 & 37.7 & 8.7\\
%  Mov & 3.6 & 6.5 & 15.7 & 8.0 & 11.2 & 9.6 & 16.4 & 16.1 & 15.4 & 20.9 & 21.3 & 9.5 & 23.9 & 22.2 & 3.0 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO &  \\  
% MAN &  \\ 
% StoER &  \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC &  \\ 
%  UNIE &  \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (with BART-large)}}\\ 
%    -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
% \multicolumn{12}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- & 32.4 & 27.7 & 25.1 & 21.1 & 19.5 & 24.7 & 30.2 & 25.6 & 32.5 & 31.4 & 36.0 & 46.8 & 48.6 & 49.6 & 44.1 \\
%  Jumble & 36.7 & 34.1 & 30.7 & 23.5 & 18.3 & 55.6 & 50.0 & 37.0 & 56.2 & 43.5 & 41.0 & 47.5 & 43.7 & 52.3 & 31.9 \\ 
%  Typo & 35.1 & 30.5 & 29.1 & 19.9 & 15.8 & 41.7 & 41.2 & 33.3 & 46.3 & 37.1 & 43.9 & 48.1 & 52.7 & 54.3 & 41.6 \\ 
%  AddNe & 21.9 & 30.4 & 35.1 & 19.4 & 20.1 & 27.9 & 24.1 & 15.3 & 29.7 & 25.8 & 41.0 & 44.7 & 50.5 & 49.3 & 30.8  \\
%  Anton & \\
%  RpSt &  \\
%  CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Kendall correlation ($|\tau|$) between different metrics and human evaluations on three CoudSourcing datasets. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:crowdsource_kendall_old}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 21.5 & 11.8 & 6.4 & 4.0 & 16.6 & 13.0 & 18.9 & 7.3 & 9.7 & 21.3 & 25.6 & 5.2 & 28.1 & 1.7 & 25.1 \\ 
%  BER & 3.2 & 1.1 & 25.2 & 2.5 & 4.4 & 28.7 & 33.7 & 37.3 & 27.7 & 30.1 & 53.0 & 50.9 & 70.3 & 52.9 & 47.8 \\
%  Mov & 15.6 & 9.2 & 43.5 & 9.2 & 1.2 & 40.8 & 46.5 & 38.6 & 34.6 & 38.6 & 30.9 & 24.4 & 54.7 & 29.1 & 37.6 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO &  \\  
% MAN &  \\ 
% StoER &  \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC &  \\ 
%  UNIE &  \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (with BART-large)}}\\ 
%    -- &  \\
%  Jumble &  \\
%  Typo & \\
%  Anton &  \\
%  RpSt &  \\
% \midrule
% \multicolumn{12}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- &  \\
%  Jumble &  \\ 
%  Typo & \\ 
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Pearson correlation ($|r|$) between different metrics and human evaluations on three Inhouse datasets. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:inhouse_pearson_old}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 25.0 & 11.3 & 0.8 & 6.6 & 15.0 & 9.4 & 16.9 & 11.2 & 15.4 & 23.7 & 22.7 & 0.2 & 25.1 & 3.6 & 18.1 \\ 
%  BER & 4.4 & 1.3 & 26.7 & 4.2 & 3.1 & 29.4 & 34.6 & 37.5 & 28.1 & 30.6 & 48.7 & 45.3 & 70.8 & 51.7 & 48.6 \\
%  Mov & 12.0 & 8.0 & 38.2 & 7.5 & 0.2 & 41.9 & 47.7 & 36.8 & 30.0 & 34.6 & 23.5 & 14.6 & 47.0 & 25.2 & 33.5 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO &  \\  
% MAN &  \\ 
% StoER &  \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC &  \\ 
%  UNIE &  \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (with BART-large)}}\\ 
%    -- &  \\
%  Jumble &  \\
%  Typo & \\
%  Anton &  \\
%  RpSt &  \\
% \midrule
% \multicolumn{12}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- &  \\
%  Jumble &  \\ 
%  Typo & \\ 
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Spearman correlation ($|\rho|$) between different metrics and human evaluations on three Inhouse datasets. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:inhouse_spearman}
% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{lccccccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{ROC}} & \multicolumn{5}{c}{\textbf{WP}} & \multicolumn{5}{c}{\textbf{CNN}} \\
% \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}
%   & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} & \textbf{Flu} & \textbf{Coh} & \textbf{Rel} & \textbf{Log} & \textbf{Int} \\
% \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(lr){7-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13}\cmidrule(lr){14-14}\cmidrule(lr){15-15}\cmidrule(lr){16-16}
%   \multicolumn{10}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
%  BLE & 18.1 & 7.8 & 1.1 & 4.9 & 10.7 & 6.6 & 11.5 & 8.0 & 10.5 & 17.0 & 17.8 & 1.3 & 19.6 & 1.8 & 12.2 \\ 
%  BER & 3.6 & 0.0 & 19.4 & 2.8 & 2.1 & 21.6 & 26.3 & 26.6 & 20.1 & 20.9 & 35.5 & 34.9 & 53.9 & 39.5 & 34.4 \\
%  Mov & 8.5 & 5.2 & 27.6 & 4.9 & 0.0 & 31.3 & 34.0 & 26.5 & 20.5 & 23.2 & 16.6 & 11.1 & 35.1 & 18.4 & 23.0 \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Story Evaluation Metrics}} \\ 
% UNIO &  \\  
% MAN &  \\ 
% StoER &  \\
%  \midrule
%  \multicolumn{10}{l}{\textbf{Unified Evaluation Metrics}}\\ 
%  CTC &  \\ 
%  UNIE &  \\
%  \midrule
%  \multicolumn{12}{l}{\textbf{\deltascore (with BART-large)}}\\ 
%    -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
% \multicolumn{12}{l}{\textbf{\deltascore (with BART-large-cnn)}}\\ 
% -- &  \\
%  Jumble &  \\
%  Typo &  \\
%  AddNe &  \\
%  Anton &  \\
%  RpSt &  \\
%  CS &  \\
% \midrule
%  \multicolumn{10}{l}{\textbf{\deltascore (with GPT-3.5)}}\\
%  -- &  \\
%  Jumble &  \\ 
%  Typo &  \\ 
%  AddNe &  \\
%  Anton & \\
%  RpSt &  \\
%  CS &  \\
%  \bottomrule
% \end{tabular}
% \caption{Absolute value of Story-level Kendall correlation ($|\tau|$) between different metrics and human evaluations on three Inhouse datasets. 
% We \textbf{bold} the best scores in each aspect and we \colorbox{green!30}{highlight} the scores where \deltascore achieves better performance over the likelihood based metrics from the same model.
% }
% \label{table:inhouse_kendall}
% \end{table*}



% % \begin{table}[t]
% % \small
% % \centering
% % \begin{tabular}{lcccccc}
% % \toprule
% %  \multirow{2}{*}{\textbf{Metric}} & \multicolumn{3}{c}{\textbf{ROC}} & \multicolumn{3}{c}{\textbf{WP}} \\
% %  & $|r|$ & $|\rho|$ & $|\tau|$ & $|r|$ & $|\rho|$ & $|\tau|$ \\
% % \cmidrule(lr){1-1}\cmidrule(lr){2-4}\cmidrule(lr){5-7}
% %   \multicolumn{5}{l}{\textbf{Similarity Based Evaluation Metrics}} \\ 
% %  BLEU & 8.5 & 8.3 & 5.7 & 1.0 & 2.3 & 1.7 \\ 
% %  BERT. & 27.9 & 25.1 & 17.6 & 15.3 & 15.5 & 10.7 \\ 
% %  Mover. & 13.3 & 11.2 & 7.7 & 0.1 & 1.8 & 1.2 \\ 
% %  \midrule
% %  \multicolumn{5}{l}{\textbf{Story Evaluation Metrics}} \\ 
% % UNION & 40.8 & 46.7 & 33.0 & 10.0 & 17.8 & 12.4 \\ 
% % Union & 41.2 & - & - & 32.6 & - & - \\
% % MANP. & 27.4 & 36.0 & 25.1 & 14.3 & 17.5 & 12.1 \\ 
% % StoryER & 6.4 & 5.5 & 3.8 & 13.3 & 13.5 & 9.3 \\
% %  \midrule
% %  \multicolumn{5}{l}{\textbf{Unified Evaluation Metrics}}\\ 
% %  CTC & 40.4 & 41.5 & 29.0 & 29.4 & 29.6 & 20.7 \\ 
% %  UNI. & 42.8 & 43.0 & 30.2 & 32.2 & 30.6 & 21.4 \\ 
% % \midrule
% %  \multicolumn{5}{l}{\textbf{\deltascore (with GPT-3.5)}}\\ 
% %  - Per & 30.0 & 29.3 & 20.3 & 32.3 & 31.7 & 22.0 \\ 
% %  + Jumble & \cellcolor{green!25}35.3 & \cellcolor{green!25}34.3 & \cellcolor{green!25}23.8 & 31.3 & 31.2 & 21.7 \\ 
% %  + Typos & \cellcolor{green!25}32.0 & \cellcolor{green!25}30.4 & \cellcolor{green!25}21.1 & 32.0 & 31.3 & 21.7 \\ 
% %  \midrule
% %   \multicolumn{5}{l}{\textbf{\deltascore (with BART-cnn)}}\\ 
% %  BARTS & 33.5 & 33.5 & 23.3 & 33.5 & 33.1 & 23.0 \\ 
% %  + Jumble & \cellcolor{green!25}41.9 & \cellcolor{green!25}41.1 & \cellcolor{green!25}28.9 & 33.1 & 32.7 & 23.0 \\ 
% %  + Typos &  \cellcolor{green!25}36.9 & \cellcolor{green!25}37.1 & \cellcolor{green!25}25.9 & \cellcolor{green!25}34.3 & \cellcolor{green!25}33.9 & \cellcolor{green!25}23.7 \\ 
% %  \bottomrule
% % \end{tabular}
% % \caption{Story-level Pearson ($r$), Spearman ($\rho$) and Kendall-Tau ($\tau$) correlations of different metrics on OpenMEVA dataset.}
% % \label{table:corelations_openmeva}
% % \end{table}

\end{document}


