

\begin{figure*}[t!]
\includegraphics[width=\textwidth]{Figures/process.png}
\caption{(A) The user places the object of interest at a desirable location. (B) The user aligns the controller against the object to place the virtual element. (C) The user places the controllers aside somewhere safe and looks at their hand to switch to hand-tracking mode. (D) The user feels the haptic feedback from the physical object while in VR (E) sees the hand touch a virtual cat.}
\label{fig:setup-process}
% \vspace{-10px}
\end{figure*}



\section{VR Haptics at Home: Proof-of-Concept System}
To investigate the VR Haptics at Home concept, we developed a proof-of-concept prototype with the Unity 3D engine. 
Our system consists of three interactive VR games: 1) pet a cat, 2) whack-a-mole, and 3) shoot monsters (See Figure \ref{fig:setup-process}, ~\ref{fig:demo-table}, and~\ref{fig:demo-chair} respectively).
We deployed our program on the Meta Quest and used Meta's hand tracking API (1.40). We found the accuracy of hand tracking to be sufficient to create a proof-of-concept. We note that our implementation is not limited to Meta's tracking technology. To repurpose physical objects, the user first prepares the object to be within reach (Figure \ref{fig:setup-process}A), and in the headset draws the guardian boundary to include the object. They place the controller against the physical object and press the trigger to place the virtual interactable object (Figure \ref{fig:setup-process}B). Once all the virtual objects have been placed, users then place the controllers face down somewhere safe and allow the system to automatically switch to the hand-tracking mode (Figure \ref{fig:setup-process}C). Lastly, the user can directly interact with the virtual object with their hands (Figure \ref{fig:setup-process}D). We have also considered leveraging hand tracking to, e.g., pinch to place the virtual interactable object, but to ensure reliable performance during the study, we used controllers as the reliable setup method.



% \begin{figure}[b!]
% \centering
% \includegraphics[width=.8\linewidth]{Figures/mole_eg.png}
% \caption{The user aligns the controller against a table surface to place the game platform (A). They can tap on the table and receive haptic feedback (B) as they hit the virtual moles in the Whack-a-Mole game (C).}
% \label{fig:demo-table}
% \end{figure}




% \subsection{Exemplary scenarios}
% We selected and built three scenarios to demonstrate objects with various affordances. {\bf Whack-a-Mole} – A Whack-a-Mole game is a game where the player hits the “moles” that peeks through the game platform before they disappear. {\bf Pet a Cat} – In this VR experience, users interact with a virtual pet. When a user reaches out their hand and wants to pet the animal, their hand normally feels nothing. {\bf Shoot Monsters} – The user controls a cannon to aim and shoot the monsters to win points. As the monsters appear in different locations, they would need to grab the handle of the cannon to rotate and aim at the monsters. Given the space constraint, please refer to the Video Figure and Appendix for a detailed description for each.

% {\bf Whack-a-Mole} – A Whack-a-Mole game is a game where the player hits the “moles” that peeks through the game platform before they disappear. In a typically VR experience, the user’s hand feels nothing when hitting a virtual button or the mole in mid-air, and the hand also goes through the virtual game platform. In this scenario, we leverage the flat surface of a tabletop to be a game platform. When the user presses the virtual button to initiate the game and taps the mole to win points, their hand is stopped by the tabletop surface.  For implementation, we attach the game platform to the front of the controller but the platform is hidden at first. When the user places the ring of the controller flush against the flat surface and presses the trigger, the game surface is enabled and placed where the controller ring is, which then aligns with the physical surface (Figure \ref{fig:demo-table}).



% {\bf Pet a Cat} – In this VR experience, users interact with a virtual pet. When a user reaches out their hand and wants to pet the animal, their hand normally feels nothing. For this scenario, we repurpose a cushion as a cat. When the user pets the cat, they feel both the texture and the compliance of the cushion that mimics the haptic feedback from that of a cat’s belly. To implement this, similarly, we attach a virtual cat to controller. When the user aligns the controller against the center of a cushion and confirms with a trigger, the top of the cushion aligns with the cat’s belly. When the user pets the cat, they hear the cat cry and purr and also see it wiggle its body as a gesture of affinity (Figure \ref{fig:setup-process}E).


% % \begin{figure}[t!]
% % \centering
% % \includegraphics[width=.8\linewidth]{Figures/cat_eg.png}
% % \caption{The user presses the controller against a cushion to place the cat (A). They can touch the cushion and receive haptic feedback (B) as they pet the virtual cat (C).}
% % \label{fig:demo-cushion}
% % \end{figure}


% % \begin{figure}[b!]
% % \centering
% % \includegraphics[width=.8\linewidth]{Figures/cannon_eg.png}
% % \caption{The user aligns the controller against the back of a chair to place the cannon (A). They can rest their hands on the chair, move it around and receive haptic feedback (B) as they rotate the virtual cannon to aim at the monsters (C).}
% % \label{fig:demo-chair}
% % \end{figure}


% {\bf Shoot Monsters} – The user controls a cannon to aim and shoot the monsters to win points. As the monsters appear in different locations, they would need to grab the handle of the cannon to rotate and aim at the monsters. If the user controls the cannon with their hands instead of a joy stick, their hands would go through the handle and feel no haptic feedback that would give them a sense of control of the cannon. Here we use a chair and leverage the movement constraints to enhance this VR experience. The cannon appears to be heavy and cannot be easily picked up, and so is the chair. The cannon has wheels that allows it to move along the ground, and its handles signals that it can be rotated. Even though the shape of the chair does not match the virtual cannon, the back of the chair affords grabbing and the rotating motion of the moveable chair matches the same motion constraint of the cannon. For implementation, we press the controller against the top edge of the back of the chair, such that the handle of the cannon then aligns with the chair’s back. The virtual handles are grabbable objects that follow the users' hands, which allows the movement of the virtual cannon and the physical chair to be in sync (Figure \ref{fig:demo-chair}).



% \begin{figure}[h!]
% \includegraphics[width=\linewidth]{title3.png}
% \caption{A user repurposes everyday furniture and surface for VR objects e.g., (a) the back of the chair as the handle of a cannon, (b) a table surface as a whack-a-mole game platform, and (c) a couch cushion as a pet cat.}
% \label{fig:demo}
% \end{figure}











