\documentclass[11pt]{article}
\usepackage[a4paper, margin=2.5cm]{geometry}

\usepackage[round]{natbib}
\usepackage{authblk}

\usepackage[english]{babel}
\usepackage{stmaryrd}
\usepackage[normalem]{ulem}
\usepackage{bm}

\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue,linkcolor=blue,bookmarks=true]{hyperref}


\usepackage{multirow}
\usepackage{arydshln}
\usepackage{adjustbox}
\usepackage{amssymb,amsmath,amsthm,amscd}
\usepackage{mathrsfs}
\usepackage{frcursive}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{dsfont}
\usepackage{lscape}
\usepackage{graphicx}
\usepackage{color}
\usepackage{frcursive}
\usepackage{paralist}

\setlength{\parindent}{0cm}


\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}


\newcommand{\dsum}{\displaystyle\sum}


\theoremstyle{remark}
\newtheorem{remark}{Remark}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\setcounter{secnumdepth}{4}

\setlength{\parindent}{0pt}


\title{A functional spatial autoregressive model using signatures}
\author[,1]{Camille Frévent\thanks{Corresponding author: \texttt{camille.frevent@univ-lille.fr}}}
\affil[1]{Univ. Lille, CHU Lille, ULR 2694 - METRICS: Évaluation des technologies de santé et des pratiques médicales, F-59000 Lille, France.}
\date{}

\begin{document}

\maketitle   

\noindent \rule{\linewidth}{0.4pt} \\
We propose a new approach to the autoregressive spatial functional model, based on the notion of signature, which represents a function as an infinite series of its iterated integrals. It presents the advantage of being applicable to a wide range of processes.  \\
After having provided theoretical guarantees to the proposed model, we have shown in a simulation study that this new approach presents competitive performances compared to the traditional model. \\
\textbf{Keywords:} Functional data, FSAR, Signature, Spatial regression, Tensor \\
\rule{\linewidth}{0.4pt}

 


\section{Introduction}\label{sec:level1}

Thanks to progress in sensing and data storage capacities, data are increasingly being measured continuously over time. This led to the emergence and popularization of functional data analysis (FDA) by \cite{ramsaylivre}, and then to the adaptation of classical statistical methods to the functional framework \citep{reg_chiou,jacques2014model,delaigle2019clustering}.  In domains in which data naturally involve a spatial component (demography, environmental science, and agricultural science \citep{Hung_16}), the emergence
of functional data has led to the introduction of spatial functional data as well as new
methods for clustering \citep{romano2010clustering,vandewalle2022clustering}, kriging \citep{giraldo2011ordinary,ignaccolo2014kriging} or regression \citep{attouch2011robust,bernardi2017penalized}. See \cite{martinez2020recent} and \cite{mateu2021geostatistical} for recent reviews of methods used in spatial FDA. \\

We are interested here in spatial regression models for lattice data. A well-known and widely used model is the spatial autoregressive (SAR) model proposed by \cite{Cliff} which models the relationship between a real-valued response variable and real covariates by considering endogenous interactions. In the context of spatial functional data with $N$ spatial units, the objective is the modelling of the relationship between a real-valued random variable $Y$ and a functional covariate $\{X(t), t \in \mathcal{T}\}$ observed in the $N$ spatial locations. \\
In this context, \cite{huang2018spatial}, \cite{pineda2019functional} and \cite{ahmed2022quasi} assumed that $X$ belongs to $\mathcal{L}^{2}(\mathcal{T})$, the space of square-integrable functions on $\mathcal{T}$, and proposed to consider the following functional spatial autoregressive (FSAR) model:
$$
Y_i = \rho_0 \sum_{j=1}^{N} v_{ij,N}Y_j + \int_{\mathcal{T}}X_i(t)\theta^*(t) \ \text{d}t + U_i,\qquad i=1,\ldots,N,\, \; N=1,2,\ldots
$$ 
where the spatial dependency structure between the $N$ spatial units is described by an $N\times N$ non-stochastic spatial weights matrix $V_{N}=(v_{ij,N})_{1 \le i,j \le N}$ that depends on $N$, the autoregressive parameter $\rho_0$ is in a compact space $\mathcal{R}$ and $\theta^* \in \mathcal{L}^2(\mathcal{T})$. \\


In recent years, signatures have been widely used in many domains such as character recognition \citep{yang2015chinese, liu2017ps}, finance \citep{gyurko2013extracting, arribas2018derivatives}, recurrent neural networks \citep{lai2017online} and medicine \citep{kormilitzin2016application,morrill2019signature}. Initially defined by \cite{chen1957integration, chen1977iterated} for smooth paths and rediscovered
in the context of rough path theory \citep{lyons1998differential, friz2010multidimensional}, the signatures have been recently investigated by \cite{fermanian2022functional} in the context of a non-spatial linear regression model with functional covariates. They present the advantages of being applicable to a wide range of processes that are not necessary square-integrable processes, and to better capture the differences between some kind of curves \citep{o1985curve,fermanian2022functional}. \\ This motivated us to explore here a new approach, based on the notion of signatures, to the traditional FSAR model. Section \ref{sec:2} presents the notion of signatures as well as the proposed signatures-based spatial autoregressive model, its estimation procedure and theoretical guaranties. Section \ref{sec:simu} describes a simulation study. Lastly, the results are discussed in Section \ref{sec:conclu}. 




 
\section{The signatures-based spatial autoregressive model}\label{sec:2}

\subsection{Concept of signatures}

\noindent Let $\mathcal{T}$ be a compact interval and
		
$$
\left(
\begin{array}{cc}
  X: & \mathcal{T} \to E       \\
  &   t \to (X_t^{(1)},...,X_t^{(p)})   \\
\end{array}
\right)
$$
 be a $p-$dimensional path. Let the set of functions of finite $m-$variation on $\mathcal{T}$ denoted $$BV_m(\mathcal{T},E)=\{X:\mathcal{T} \to E,\;\| X \|_{TV,m}< \infty\},$$ 
 where 
$$\| X \|_{TV,m}=	\left(\sup_{I}\sum_{(t_0,...t_d)\in I} \| X_{t_i}-X_{t_{i-1}}\|^m\right)^{1/m},$$ the supremum is over all finite partitions $I$ of $\mathcal{T}$, $\|.\|$ is the euclidian norm on $E$.  Let $\mathcal{C}_m(\mathcal{T},E)$ be the set of continuous path $X:\mathcal{T} \to E$ of finite $m-$variation.\\
		
\begin{definition}(Signature of a path; \cite{levin2013learning})\\
Let $X\in \mathcal{C}_m(\mathcal{T},E)$. The signature $Sig(X)$ of $X$ over the interval $\mathcal{T}$  is 
$$Sig(X)=(1,\mathbf{X}^1,\dots,\mathbf{X}^d,\dots)$$
where the following integral make sense
$$
\mathbf{X}^d= \underset{\substack{
  t_1<\dots<t_d \\
t_1,...,t_d\in \mathcal{T}\\
}}{\int \dots \int} \ \text{d}X_{t_1}\otimes \ldots \otimes \text{d}X_{t_d} \in E^{\otimes d}.	$$
The truncated signature of order $D$ is $Sig^D(X)=(1,\mathbf{X}^1,\dots,\mathbf{X}^D)$, for every integer $D\ge 1$.
\end{definition}

In the following we assume that $E=\mathbb{R}^p$, $X\in \mathcal{C}_1(\mathcal{T},\mathbb{R}^p)$ and $\mathcal{T}=[0,1]$.
It should be noted that the assumption $X\in \mathcal{C}_1(\mathcal{T},\mathbb{R}^p)$ is less restrictive than assuming $X\in L^{2}(\mathcal{T},\mathbb{R}^p)$, and that the space $ \mathcal{C}_1(\mathcal{T},\mathbb{R}^p)$ endowed with the norm
$|| X ||_{\mathcal{C}}=|| X ||_{TV,1}+|| X ||_{\infty}$ is a Banach space. \\

By using the convention $(\mathbb{R}^p)^{\otimes 0}=\mathbb{R}$, we define $T((\mathbb{R}^p))$ the tensor algebra space by
$$
T((\mathbb{R}^p)):=\{(a_0,a_1,\dots,a_d,\dots)| \forall d\ge 0, a_d\in (\mathbb{R}^p)^{\otimes d}\}
$$ and $T^d((\mathbb{R}^p))$ the $d^{th}$ truncated tensor algebra space
$$
T^d((\mathbb{R}^p)):=\bigoplus_{i=0}^d (\mathbb{R}^p)^{\otimes i}.
$$
Then the signature $Sig(X)$ is valued in $T((\mathbb{R}^p))$ while the truncated version of order $D$, $Sig^D(X)$ is an element of $T^D((\mathbb{R}^p))$.\\




\begin{definition}(Signature coefficient of a path; \cite{levin2013learning}, Remark 2.5)\\
Let $A^*$ be the set of multi-indexes with entries in $\{1,\dots,p\}$. For $J\in A^*$ with length $|J|=d$, $J$ may be written $J=(i_1,\dots,i_d)$, with $i_j\in \llbracket 1 ; p\rrbracket$, $\forall j\in \llbracket 1; d\rrbracket$.\\
Let $(e_i)_{i=1}^p$ be the canonical orthonormal basis of $E=\mathbb{R}^p$. For any positive integer $d$, the space $E^{\otimes d}$ is isomorphic to the free vector space generated by all the words of length $d$ in $A^*$
and $(e_{i_1}\otimes \dots \otimes e_{i_d})_{(i_1,\dots,i_d)\in \llbracket 1 ; p\rrbracket^d}$ form a basis of $E^{\otimes d}$. Then the signature of $X$ can be rewritten as 
\begin{equation}
\label{signaturecoef}
Sig(X)=1+\sum_{d=1}^\infty \sum_{(i_1, \dots, i_d)} \mathcal{S}_{(i_1, \dots, i_d)}(X) e_{i_1}\otimes \dots \otimes e_{i_d}\in T((E)).
\end{equation}
where $$ \mathcal{S}_{(i_1,\dots,i_d)}(X)= \underset{\substack{
    t_1<\dots<t_d     \\
 t_1,\dots,t_d\in \mathcal{T} }}{\int \dots \int} \ \text{d}X_{t_1}^{(i_1)} \dots  \text{d}X_{t_d}^{(i_d)} \in \mathbb{R}. $$
\end{definition}

\subsection{Model}

Let us consider the following signatures-based spatial autoregressive model: 
\begin{equation}\label{modelsignature}
Y_i=\rho_0\sum_{j=1}^{N} v_{ij,N}Y_j + \alpha_0 + \langle Sig(\theta^*),Sig(X_i) \rangle + U_i
\end{equation}

where the autoregressive parameter $\rho_0$ is in a compact space $\mathcal{R}$, $\theta^*(\cdot)$ is a parameter function assumed to belong to $\mathcal{C}_1(\mathcal{T},\mathbb{R}^p)$,  the set of continuous path of bounded variation taking values in $\mathbb{R}^p$, so is $X_i$. 
In practice, it is common, but not necessary, to \textit{row normalize} the spatial weight matrix $V_N=(v_{ij,N})_{1\le i,j \le N}$. This allows $0 \le v_{ij,N} \le 1$ and $-1 \le \rho \le 1$. In this way, the spatially-lagged variables are equal to a weighted average of the neighboring values. \\		

The disturbances $\left\{U_{i}, \; i=1,\ldots, N,\; N=1,2,\ldots \right\}$ are assumed to be independent and identically distributed random variables such that $\mathbb{E}(U_{i})=0$, $\mathbb{E}(U_{i}^2)=\sigma_0^2$. They are also independent of $\{X_i(t),t\in \mathcal{T}, \; i=1,\ldots,N,\; N=1,2\ldots\}$.\\ 

Let $\mathbf{X}_{N}(\theta^*)$ be the $N\times 1$ vector of $i^\text{th}$ element 
$\langle Sig(\theta^*),Sig(X_i) \rangle$. Then, one can rewrite (\ref{modelsignature}) as 
\begin{equation*}
S_N\mathbf{Y}_N= \alpha_0 \mathbf{1}_N + \mathbf{X}_{N}(\theta^*)+\mathbf{U}_N \, , \qquad N=1,2,\ldots
\end{equation*}
where $S_N=(I_N-\rho_0 V_N)$, $\mathbf{Y}_N$ and $\mathbf{U}_N$ are two $N\times 1$ vectors of elements $Y_i$ and $U_i,\ i=1,\ldots,N$ respectively, $I_N$ denotes the $N\times N$ identity matrix and $\mathbf{1}_N$ denotes the $N \times 1$ vector composed only of 1. \\

Let $S_N(\rho)=I_N-\rho V_N $, then the conditional quasi log-likelihood function of the vector $\mathbf{Y}_N$ given $\{Sig(X_i), \; i=1,\ldots,N,\; N=1,2\ldots\}$ is given by:
\begin{align}
\ell_N(\sigma^2,\rho,\alpha, \theta(.))&= -\frac{N}{2}\ln(\sigma^2)-\frac{N}{2}\ln(2\pi)+ \ln\vert S_N(\rho)\vert \nonumber \\
&\hspace{0.5cm} -\frac{1}{2\sigma^2}\left[S_N(\rho)\mathbf{Y}_N- \alpha \mathbf{1}_N- \mathbf{X}_{N}(\theta)\right]^\top \left[S_N(\rho)\mathbf{Y}_N-\alpha \mathbf{1}_N- \mathbf{X}_{N}(\theta)\right]
\label{LK}.
\end{align}
The quasi-maximum likelihood estimates of $\rho_0,\; \alpha_0,\; Sig(\theta^{*})$ and $\sigma_0^2$ are the parameters $\rho$, $Sig(\theta)$, and $\sigma^2$ that maximize (\ref{LK}). However this likelihood cannot be maximized without addressing the difficulty produced by the infinite dimension of the signatures $Sig(\theta^*)$ and $Sig(X_i)$. The next two sections propose two estimation methods that overcome this challenge. In the following we consider the case where $p\ge 2$. \\



\subsubsection{Penalized signatures-based spatial regression}
Due to the infinite dimension of the signatures, $\rho_0$, $\alpha_0$, $Sig(\theta^*)$ and $\sigma_0^2$ cannot be directly estimated from (\ref{LK}). To solve this problem, we propose in this section a first strategy of estimation based on truncated signatures and a penalized (ridge) regression similarly to \cite{fermanian2022functional}. 

\paragraph{Estimation} \mbox{} \\
Let $\{ (\varphi_{(i_1,...,i_d)})_{(i_1,...,i_d) \in \llbracket 1 ; p \rrbracket^d}, \varphi_{(i_1,...,i_d)} =e_{i_1}\otimes \dots \otimes e_{i_d} \}$. We can write $Sig(X)$ and $Sig(\theta^{*})$  as follows using the signature coefficients $\mathcal{S}_{(i_1, \dots, i_d)}(X)$ and $\mathcal{S}_{(i_1,\dots,i_d)}( \theta^*)$ of $X$ and $\theta^*$ respectively:
\begin{equation} \label{decomp}
 Sig(X)=1+\sum_{d=1}^\infty\sum_{(i_1,\dots,i_d)} \mathcal{S}_{(i_1, \dots, i_d)}(X)\varphi_{(i_1, \dots, i_d)} \quad \text{ and } \quad Sig(\theta^*)=1+\sum_{d=1}^\infty\sum_{(i_1,\dots,i_d)} \mathcal{S}_{(i_1,\dots,i_d)}( \theta^*)\varphi_{(i_1,\dots,i_d)}.
\end{equation}

We consider here a slightly modified version of model (\ref{modelsignature}). We assume that the signature coefficients are involved in the model only up to a certain unknown truncation order $D^* \in \mathbb{N}$. The model thus becomes

\begin{equation}\label{modelsignaturetrunc}
Y_i=\rho_0\sum_{j=1}^{N} v_{ij,N}Y_j + \alpha_0 + 1 + \dsum_{d=1}^{D^*} \dsum_{(i_1, \dots, i_d)} \mathcal{S}_{(i_1, \dots, i_d)}(X) \mathcal{S}_{(i_1,\dots,i_d)}( \theta^*) + U_i.
\end{equation}




Let the signature coefficients vector of $X$, $\mathcal{S}(X)$, be the sequence of all signature coefficients:
$$\mathcal{S}(X)=(1,  \mathcal{S}_{(1)}(X),\dots,\mathcal{S}_{(p)}(X), \mathcal{S}_{(1,1)}(X),\mathcal{S}_{(1,2)}(X),\dots,\mathcal{S}_{(i_1,\dots,i_d)}(X),\dots) $$
and the truncated signature coefficients vector at order $D$ of $X$, $\mathcal{S}^D(X)$, be defined as
$$\mathcal{S}^D(X)=(1,  \mathcal{S}_{(1)}(X),\mathcal{S}_{(2)}(X),\dots,\mathcal{S}_{\underbrace{\scriptstyle (p,\dots,p)}_{D \text{ terms}}}(X)).$$

Then, by noting $s_p(D)=\sum_{d=0}^D p^d=\frac{p^{D+1}-1}{p-1}$ the dimension of the truncated signature coefficients vector at order $D$, (\ref{modelsignaturetrunc}) can be rewritten as
\begin{equation}\label{modelsignaturetrunc1}
S_N \mathbf{Y}_N = \alpha_0 \mathbf{1}_N + \xi_{D^*}\Theta_{D^*}^* + \mathbf{U}_N,
\end{equation}

where $\Theta_{D}^*=(\theta^*_1,\ldots,\theta^*_{s_p(D)})^\top=\mathcal{S}^{D}(\theta^*)^\top$ ($\theta_1^*=1$) and $\xi_{D}$ is an $N\times s_p(D)$ matrix whose $i^{\text{th}}$ line is given by 
$$\varepsilon^{(i)}=\mathcal{S}^{D}(X_i)\, , \qquad i=1,\ldots,N.$$ 

Finally, the associated conditional quasi log-likelihood function is
\begin{eqnarray}
\tilde{\ell}_N(\sigma^2,\rho,\alpha, \Theta_{D^*})&=&-\frac{N}{2}\ln(\sigma^2)-\frac{N}{2}\ln(2\pi)+  \ln\vert S_N(\rho)\vert \nonumber\\ && -\frac{1}{2\sigma^2}\left[S_N(\rho)\mathbf{Y}_N- \alpha \mathbf{1}_N - \xi_{D^*}\Theta_{D^*}\right]^\top \left[S_N(\rho)\mathbf{Y}_N- \alpha \mathbf{1}_N - \xi_{D^*}\Theta_{D^*}\right].
\label{TLK}
\end{eqnarray}

However, despite the use of truncated signatures, the model (\ref{modelsignaturetrunc1}) remains difficult to estimate due to the large dimension of $\xi_{D^*}$. \\


Penalized regression allows the estimation of regression models with a large number of potentially correlated covariates. In particular, \cite{fermanian2022functional} recently proposed a functional (non-spatial) linear regression model with truncated signatures, and estimated the parameters using a Ridge regularization. Here we propose to adapt the penalized regression algorithm proposed by \cite{liu2018penalized} in the context of our FSAR model using a Ridge regularization. \\

More precisely we propose the following iterative algorithm for a truncation order $D$:
\begin{enumerate}[label= \bfseries Step \arabic*,leftmargin=1.9cm]
\item Estimate the initial value of the parameters $\alpha$, $\Theta_D$ and $\sigma^2$: $\alpha^{(0)}$, $\Theta_D^{(0)}$ and $\sigma^{2(0)}$ using a non-spatial Ridge regression. We note $\lambda_N$ the associated regularization parameter.
\item \label{step2} Update $\sigma^{2(m+1)} = \underset{\sigma^2>0}{\arg \max} \ \tilde{\ell}_N\left(\sigma^2, \rho^{(m)}, \alpha^{(m)}, \Theta_D^{(m)} \right)$
\item Update $\rho^{(m+1)} = \underset{\rho \in \mathcal{R}}{\arg \max} \ \tilde{\ell}_N\left(\sigma^{2(m+1)}, \rho, \alpha^{(m)}, \Theta_D^{(m)} \right)$
\item \label{step4} Update $\left(\alpha^{(m+1)}, \Theta_D^{(m+1)^\top}\right) = \underset{(\alpha, \Theta_D^\top) \in \mathbb{R}^{s_p(D)+1}}{\arg \max} \ \tilde{\ell}_N\left(\sigma^{2(m+1)}, \rho^{(m+1)}, \alpha, \Theta_D \right) - N \lambda_N ||\Theta_D||_2^2 $
\item Repeat \ref{step2} to \ref{step4} until convergence \\
\end{enumerate}

In practice we fix a maximum truncation order $D^{\text{max}} \in \mathbb{N}^*$ and we iterate the proposed algorithm for $D = 1, \dots, D^{\text{max}}$ to determine the optimal truncation order $D^*$.

Moreover, it should be observed that by noting $\chi_{N,D}=(\mathbf{1}_N, \xi_{D})$, the iterative estimations of $\sigma_0^2$ and $(\alpha_0,\Theta_D^\top)$ have explicit formulas at \ref{step2} and \ref{step4}:
$$ \sigma^{2(m+1)} = \dfrac{1}{N} \left[\left(I_N-\rho^{(m)} V_N \right) \mathbf{Y}_N - \chi_{N,D} \left(\alpha^{(m)},\Theta_D^{(m)\top} \right)^\top \right]^\top\left[\left(I_N-\rho^{(m)} V_N \right) \mathbf{Y}_N - \chi_{N,D} \left(\alpha^{(m)},\Theta_D^{(m)\top} \right)^\top\right] $$ and
$$ \left(\alpha^{(m+1)}, \Theta_D^{(m+1)\top}\right)^\top = \left[ \dfrac{1}{\sigma^{2(m+1)}} \chi_{N,D}^\top \chi_N + 2 N \Lambda_{N,D} \right]^{-1} \left[\dfrac{1}{\sigma^{2(m+1)}} \chi_{N,D}^\top \left(I_N - \rho^{(m+1)} V_N\right) \mathbf{Y}_N \right]$$
where $\Lambda_{N,D}=\begin{pmatrix} 0 & 0 & 0 & \dots & 0 \\ 0 & \lambda_N & 0 & \dots & 0 \\
0 & 0 & \ddots & \ddots & \vdots \\
\vdots & \ddots & \ddots & \ddots & 0 \\
0 & \dots & 0 & 0 & \lambda_N
\end{pmatrix}$.



\paragraph{Assumptions and results }\label{FSARsec2} \mbox{} \\
Let $\gamma_{D^*} = (\alpha, \Theta_{D^*}^\top)^\top$, $\gamma_{0} = (\alpha_0, \Theta_{D^*}^{*\top})^\top$, $\beta_{D^*} = (\sigma^2, \rho, \gamma_{D^*}^\top)^\top = (\beta_1, \beta_2, \dots, \beta_{s_p(D^*)+3})^\top$, \\ $\beta_{0} = (\sigma_0^2, \rho_0, \gamma_{0}^\top)^\top = (\beta_{1,0}, \beta_{2,0}, \dots, \beta_{s_p(D^*)+3,0})^\top$, $G_N = V_N S_N^{-1}$, $\chi_{N}=(\mathbf{1}_N, \xi_{D^*})$ and \\
$Q_N(\beta_{D^*}) = \tilde{\ell}_N(\beta_{D^*}) - N \lambda_N \Theta_{D^*}^\top \Theta_{D^*}$. \\

We assume that (\textbf{Assumptions I}):
\begin{itemize}
\item[i.] Exists $\nu > 0$ such that for all $i \in \llbracket 1, N \rrbracket, \mathbb{E}[|U_i|^{\nu + 4}]$ exists.
\item[ii.] $v_{ij,N}=O(h_N^{-1})$ uniformly in all $i,j$, where the rate sequence $h_N$ can be bounded or divergent, such as $h_N=o(N)$. 
\item[iii.] The matrix $S_N$ is nonsingular.
\item[iv.] The sequences of matrices $\{V_N\}$ and $\{S_N^{-1}\}$ are uniformly bounded in both row and column sums.
\item[v.] $\underset{N \rightarrow \infty}{\lim} \ \dfrac{1}{N} \chi_N^\top \chi_N$ exists and is nonsingular. The elements of $\chi_N$ are uniformly bounded constants.
\item[vi.] The matrices $\{S_N^{-1}(\rho)\}$ are uniformly bounded in both row and column sums and uniformly bounded in $\rho$ in compact parameter space $\mathcal{R}$. The true $\rho_0$ is in the interior of $\mathcal{R}$.
\item[vii.] $\underset{N \rightarrow \infty}{\lim} \ \dfrac{1}{N} [\chi_N, G_N \chi_N \gamma_0]^\top [\chi_N, G_N \chi_N \gamma_0]$ exists and is nonsingular.
\item[viii.] $\underset{N \rightarrow \infty}{\lim} \ \dfrac{1}{N} \mathbb{E}\left[ \dfrac{\partial^2 \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*} \partial \beta_{D^*}^\top } \right]$ exists.
\item[ix.] The third derivatives $\dfrac{\partial^3 \tilde{\ell}_N(\beta_{D^*})}{\partial \beta_{j} \partial \beta_{k} \partial \beta_{l}}$ exist ($1 \le j,k,l \le s_p(D^*)+3$) for all $\beta_{D^*}$ in an open set $\mathcal{B}$ that contains the true parameters $\beta_{0}$. There are functions $M_{j,k,l}$ such that $\mathbb{E}[M_{j,k,l}] < \infty$ and $\left| \dfrac{1}{N} \dfrac{\partial^3 \tilde{\ell}_N(\beta_{D^*})}{\partial \beta_{j} \partial \beta_{k} \partial \beta_{l}} \right| \le M_{j,k,l}$ for all $\beta_{D^*} \in \mathcal{B}$. 
\end{itemize}

		
\begin{theorem} \label{th1}
Let $p_{\lambda_N}(\beta_j) = \lambda_N \beta_j^2: \lambda_N \Theta_{D^*}^\top \Theta_{D^*} = \lambda_N \dsum_{j=4}^{s_p(D^*)+3} \beta_j^2$. \\
Let $a_N = \underset{j \in \llbracket 4, s_p(D^*)+3 \rrbracket }{ \max } \ |p'_{\lambda_N}(\beta_j)| = 2 \lambda_N \underset{j \in \llbracket 4, s_p(D^*)+3 \rrbracket }{ \max } \ |\beta_j|$ \\ and $b_N = \underset{j \in \llbracket 4, s_p(D^*)+3 \rrbracket }{ \max } \ |p''_{\lambda_N}(\beta_j)| = 2 \lambda_N$. \\
If $\sqrt{N} a_N = o(1)$ and $b_N = o(1)$, 
then under the \textbf{Assumptions I(i)-(ix)} there exists a local minimizer $\hat{\beta}_{D^*}$ of $Q_N(\beta_{D^*})$ such that
$$ || \hat{\beta}_{D^*} - \beta_{0} || = O_{\mathbb{P}}(N^{-1/2}).$$
\end{theorem}

\begin{theorem} \label{th2}
Let $p_{\lambda_N}(\beta_j) = \lambda_N \beta_j^2: \lambda_N \Theta_{D^*}^\top \Theta_{D^*} = \lambda_N \dsum_{j=4}^{s_p(D^*)+3} \beta_j^2$. \\
Let $a_N = \underset{j \in \llbracket 4, s_p(D^*)+3 \rrbracket }{ \max } \ |p'_{\lambda_N}(\beta_j)| = 2 \lambda_N \underset{j \in \llbracket 4, s_p(D^*)+3 \rrbracket }{ \max } \ |\beta_j|$ \\ and $b_N = \underset{j \in \llbracket 4, s_p(D^*)+3 \rrbracket }{ \max } \ |p''_{\lambda_N}(\beta_j)| = 2 \lambda_N$. \\
If $\sqrt{N} a_N = o(1)$ and $b_N = o(1)$, then under the \textbf{Assumptions I(i)-(ix)}

$$ \sqrt{N} [(\mathbb{E}(A)+P_2) (\hat{\beta}_{D^*} - \beta_{0}) + P_1] \overset{d}{\underset{N \rightarrow \infty}{\longrightarrow}} \mathcal{N}\left(0, \underset{N \rightarrow \infty}{\lim} \ \Sigma_N \right) $$
where $A = - \dfrac{1}{N} \dfrac{\partial^2 \tilde{\ell}_N(\beta_0)}{\partial \beta_{D^*} \partial \beta_{D^*}^\top}$, $P_1 = (0,0,0,p'_{\lambda_N}(\beta_{4,0}), \dots, p'_{\lambda_N}(\beta_{s_p(D^*)+3,0}))^\top$, \\ $P_2 = \text{diag}(0,0,0,p''_{\lambda_N}(\beta_{4,0}), \dots, p''_{\lambda_N}(\beta_{s_p(D^*)+3,0}))$ and

\begin{small}
$$ \Sigma_N = \begin{pmatrix}
\dfrac{\mathbb{E}(U_1^4)-\sigma_0^4}{4 \sigma_0^8} & \dfrac{\mathbb{E}(U_1^4)-\sigma_0^4}{2N \sigma_0^6} \text{tr}(G_N) + \dfrac{\mathbb{E}(U_1^3)}{2N\sigma_0^6} \mathbf{1}_N^\top G_N \chi_N \gamma_0 & \dfrac{\mathbb{E}(U_1^3)}{2N\sigma_0^6} \mathbf{1}_N^\top \chi_N \\
 & \dfrac{1}{N\sigma_0^2} (G_N \chi_N \gamma_0)^\top (G_N \chi_N \gamma_0) & \\
* & + \dfrac{\mathbb{E}(U_1^4)-\sigma_0^4}{N \sigma_0^4} \dsum_{i=1}^N G_{ii,N}^2 + \dfrac{1}{N} \text{tr}((G_N^\top + G_N) G_N) & \dfrac{1}{N \sigma_0^2} \gamma_0^\top \chi_N^\top G_N^\top \chi_N + \dfrac{\mathbb{E}(U_1^3)}{N\sigma_0^4} \dsum_{i=1}^N G_{ii,N} \chi_{i.,N} \\
& + 2 \dfrac{\mathbb{E}(U_1^3)}{N\sigma_0^4} \dsum_{i=1}^N G_{ii,N} G_{i.,N} \chi_N \gamma_0 &  \\
* & * & \dfrac{1}{N\sigma_0^2} \chi_N^\top \chi_N \\
\end{pmatrix}.$$
\end{small}
\end{theorem}



\subsubsection{Spatial autoregressive model based on signatures projections}
In this section we consider Model (\ref{modelsignature}). By using the decomposition (\ref{decomp}) the model can be rewritten as
\begin{align*}\label{modeldecomp}
Y_i &= \rho_0\sum_{j=1}^{N} v_{ij,N}Y_j + \alpha_0 + 1 + \dsum_{d=1}^\infty \dsum_{(i_1, \dots, i_d)} \mathcal{S}_{(i_1, \dots, i_d)}(X_i) \mathcal{S}_{(i_1, \dots, i_d)}(\theta^*) + U_i \\
&= \rho_0\sum_{j=1}^{N} v_{ij,N}Y_j + \alpha_0 + 
\langle \mathcal{S}(X_i), \mathcal{S}(\theta^*) \rangle + U_i.
\end{align*}

\paragraph{Estimation} \mbox{} \\
We assume that there exists new coefficients $ \bm{\zeta}_i = (\zeta_{i,1}, \zeta_{i,2}, \dots )^\top $ and $ \Phi^* = (\phi_{1}^*, \phi_{2}^*, \dots )^\top $ such that $$\langle \mathcal{S}(X_i), \mathcal{S}(\theta^*) \rangle = 1 + \langle \bm{\zeta}_i, \Phi^* \rangle, $$ and a positive sequence of integers $C_N$ increasing asymptotically as the sample size $N \rightarrow \infty$ such that
$$ \langle \zeta_i, \Phi^* \rangle = \dsum_{c=1}^{C_N} \zeta_{i,c} \phi_c^* + \dsum_{c=C_N+1}^{\infty} \zeta_{i,c} \phi_c^*$$ where the second term vanishes asymptotically when $N \rightarrow \infty$. \\
In practice the new coefficients $\bm{\zeta}_i$ can be computed by the use of a principal component analysis (PCA) on the signatures coefficients derived up to a pre-defined large truncation order, or by using another projection method. 


Then $\langle \mathcal{S}(X_i), \mathcal{S}(\theta^*) \rangle$ can be approximated by $ 1 + \dsum_{c=1}^{C_N} \zeta_{i,c} \phi_c^* $ and $\mathbf{X}_N(\theta^*)$ can be approximated by $\mathbf{1}_N + Z_{C_N} \Phi_{C_N}^*$ where $Z_{C_N}$ is the $N \times C_N$ matrix whose $i^\text{th}$ line is given by $$\bm{\zeta}_i^{C_N\top} = (\zeta_{i,1}, \dots, \zeta_{i,C_N})$$ and $\Phi_{C_N}^* = (\phi_{1}^*, \dots, \phi_{C_N}^* )^\top$. \\


We suppose that the $\zeta_i^{C_N}$ are centered (which can be easily satisfied by subtracting their average) and that $\alpha_0+1$=0 which can be obtained by centering $\mathbf{Y}_N$.
Now, the truncated conditional quasi log-likelihood function can be obtained by replacing in (\ref{LK}) $\mathbf{X}_N(\theta)$ with $\mathbf{1}_N + Z_{C_N} \Phi_{C_N}$. The corresponding and feasible conditional quasi log-likelihood is
\begin{eqnarray}
\tilde{\ell}_N(\sigma^2,\rho,\Phi_{C_N})&=&-\frac{N}{2}\ln(\sigma^2)-\frac{N}{2}\ln(2\pi)+  \ln\vert S_N(\rho)\vert \nonumber\\ && -\frac{1}{2\sigma^2}\left[S_N(\rho)\mathbf{Y}_N- Z_{C_N} \Phi_{C_N} \right]^\top \left[S_N(\rho)\mathbf{Y}_N- Z_{C_N} \Phi_{C_N} \right].
\label{TLKACP}
\end{eqnarray}

For a fixed $\rho$, (\ref{TLKACP}) is maximized at 
\begin{equation*}
\hat{\Phi}_{N,{\rho}}=(Z_{C_N}^\top Z_{C_N})^{-1}Z_{C_N}^\top S_N(\rho)\mathbf{Y}_N
\end{equation*} 
and 
\begin{eqnarray*}
\hat{\sigma}_{N,{\rho}}^2&=&\frac{1}{N}\left(S_N(\rho)\mathbf{Y}_N-Z_{C_N}\hat{\Phi}_{N,\rho}\right)^\top\left(S_N(\rho)\mathbf{Y}_N-Z_{C_N}\hat{\Phi}_{N,{\rho}}\right)\nonumber \\
&=&\frac{1}{N}\mathbf{Y}_N^\top S_N^\top(\rho)M_N S_N(\rho)\mathbf{Y}_N,
\end{eqnarray*}
where $M_N=I_N-Z_{C_N}(Z_{C_N}^\top Z_{C_N})^{-1}Z_{C_N}^\top$.\\

The concentrated truncated conditional quasi log-likelihood function of $\rho$ is:
\begin{equation*}
\tilde{\ell}_N(\rho)=-\frac{N}{2}[\ln(2\pi)+1] -\frac{N}{2}\ln(\hat{\sigma}^2_{N,\rho}) \, +\ln\vert S_N(\rho)\vert. 
\end{equation*} 
Then the estimator of $\rho_0$ is $\hat{\rho}_N = \underset{\rho \in \mathcal{R}}{\arg \max} \ \tilde{\ell}_N(\rho)$, and those of $\Phi_{C_N}^{*}$ and $\sigma_0^2$ are respectively $\hat{\Phi}_{N,\hat{\rho}_N}$ and $\hat{\sigma}^2_{N,\hat{\rho}_N}$.



\paragraph{Assumptions and results} \mbox{} \\
Let $I_N+\rho_0 G_N=S_N^{-1}$, $B_N(\rho)=S_N(\rho)S_N^{-1}=I_N+(\rho_0-\rho)G_N$ for all $\rho\in \mathcal{R}$, $A_N(\rho)=B_N^\top(\rho)B_N(\rho)$ and $\Gamma_{C_N}=\mathbb{E}\left[ \dfrac{1}{N} Z_{C_N}^\top Z_{C_N} \right]$.\\
We assume that (\textbf{Assumptions II}):
\begin{itemize}
\item[i.] Assumptions I(i)-(iv) hold.
\item[ii.] Assumption I(vi) holds.
\item[iii.] The sequence $C_N$	satisfies $C_N\to \infty $ and  $C_N N^{-1/4}\to 0$ as $N\to \infty$, and 
\begin{align*}
&\mbox{1.}\quad C_N \sum_{r_1,r_2>C_N}\mathbb{E}\left(\zeta_{r_1}\zeta_{r_2}\right)=o(1)\\
&\mbox{2.}\quad \sum_{r_1,\ldots,r_4>C_N}\mathbb{E}\left(\zeta_{r_1} \ldots \zeta_{r_4}\right)=o(1)\\
&\mbox{3.}\qquad \sqrt{N}\sum_{s=1}^{C_N}\sum_{r_1,r_2>C_N}\mathbb{E}\left(\zeta_s \zeta_{r_1}\right)\mathbb{E}\left(\zeta_s \zeta_{r_2}\right)=o(1) \\
&\mbox{4.}\qquad
\sum_{r=1}^{C_N}\mathbb{E}\left( \zeta_{r}^{2}\right) < \infty.
\end{align*}
where $\zeta_{r}$ denotes the $r^\text{th}$ element of $\bm{\zeta}$.
\item[iv.] Let $\Delta_N = \Delta_N=N\left(\text{tr}\left(\frac{G_N^\top G_N}{N}\right) - \text{tr}^{2}\left(\frac{G_N}{N}\right)\right)\Phi_{C_N}^{*\top}\Gamma_{C_N}\Phi_{C_N}^{*}$ and $\lim_{N\to\infty }\frac{1}{N}\Delta_N=c$, where (i) $c> 0$; (ii) $c=0$. \\
Under the latter  condition, 
	\begin{equation*}
	\lim_{N\to \infty} \frac{h_N}{N}\left\{\ln \left\vert \sigma_{0}^{2}S_N^{-1}S_N^{-\top}\right\vert -\ln\left\vert \sigma_{N,\rho}^{2}S_N^{-1}(\rho)S_N^{-\top}(\rho)\right\vert\right\}\neq 0,
	\end{equation*} 
	whenever $\rho\neq \rho_0,$ with $ \sigma^2_{N,\rho}=\frac{\sigma_0^{2}}{N}\mathrm{tr}(A_N(\rho))$.
\item[v.] We have
\begin{equation*}
\sum_{r_1,r_2,r_3,r_4=0}^{C_N}\mathbb{E}\left( \zeta_{r_1} \zeta_{r_2} \zeta_{r_3} \zeta_{r_4}\right)\nu_{r_1 r_2}\nu_{r_3 r_4}=o(N/C_N^{2}),
\end{equation*}
where the $\nu_{kl},\; k,l=1,\ldots, C_N$, are the elements of $ \Gamma_{C_N}^{-1}$.
\item[vi.] We assume that
\begin{equation*}
\sum_{r_1,\ldots,r_8=0}^{C_N}\mathbb{E}\left( \zeta_{r_1} \zeta_{r_3} \zeta_{r_5} \zeta_{r_7}\right)\\
\mathbb{E}\left( \zeta_{r_2} \zeta_{r_4} \zeta_{r_6}
\zeta_{r_8}\right) \nu_{r_1r_2}\nu_{r_3r_4}\nu_{r_5r_6}\nu_{r_7r_8}=o(N^2 C_N^{2}).
\end{equation*}
\end{itemize}


Then, similarly to \cite{ahmed2022quasi}, the following theorems give the identification, consistency and asymptotic normality results of the parameters estimates. 
\begin{theorem}\label{FSARth1}
Under \textbf{Assumptions II(i)-(iv)}  and $h_N^4=O(N)$ for divergent $h_N$, the QMLE $\hat{\rho}_N$ derived from the maximization of $\tilde{\ell}_N(\rho)$ is consistent and satisfies 
\begin{equation*}
\sqrt{\frac{N}{h_N}}(\hat{\rho}_N-\rho_0)\overset{d}{\underset{N \rightarrow \infty}{\longrightarrow}} \mathcal{N}(0,s_{\rho}^{2}),
\end{equation*}
with 	
$\displaystyle  s_{\rho}^{2}=\lim_{N\to \infty}\frac{s_{N}^2h_N}{N}\left\{\frac{h_N}{N}\left[\Delta_N+\sigma_0^2\text{tr}((G^\top_N+G_N)G_N)\right]\right\}^{-2},$ where
\begin{eqnarray}
s_{N}^2&=& (\mathbb{E}(U_1^4) - 2\sigma_0^4)\sum_{i=1}^N G_{ii}^2 + \frac{1}{N} \text{tr}^2(G_N) (\sigma_0^4-\mathbb{E}(U_1^4)-\sigma_0^2 \Phi_{C_N}^{*\top} \Gamma_{C_N} \Phi_{C_N}^*)  \nonumber \\ 
&& \qquad\qquad + \text{tr}(G_NG_N^\top)(\sigma_0^4+\sigma_0^2 \Phi_{C_N}^{*\top} \Gamma_{C_N} \Phi_{C_N}^*).
\label{sn}
\end{eqnarray}

\end{theorem}

\begin{theorem}\label{FSARth3}
Under assumptions of Theorem~\ref{FSARth1}, $\hat{\sigma}_{N}^{2}$ is a consistent estimator of $\sigma_0^2$ and satisfies 
\begin{equation*}
\sqrt{N}(\hat{\sigma}_{N,{\hat{\rho}_N}}^2-\sigma_0^2)\overset{d}{\underset{N \rightarrow \infty}{\longrightarrow}} \mathcal{N}(0,s^{2}_{\sigma}),
\end{equation*}
with 
\begin{equation*}
s^{2}_{\sigma}=\mathbb{E}(U_1^4)-\sigma_0^4+4s_{\rho}^2\lim_{N \to \infty}h_N\left[\frac{\text{tr}(G_N)}{N}\right]^{2}.
\end{equation*}  
\end{theorem} 
\noindent When $h_N$  is divergent,  $s_{\sigma}^{2}$ will be reduced to $\mathbb{E}(U_1^4)-\sigma_0^4$. \\
		

\begin{theorem}\label{FSARth2}
Under \textbf{Assumptions II(i)-(vi)}, we have
\begin{equation*}
\frac{N\left(\hat{\Phi}_{N,\hat{\rho}_N}-\Phi_{C_N}^{*}\right)^\top \Gamma_{C_N}\left(\hat{\Phi}_{N,\hat{\rho}_N}-\Phi_{C_N}^{*}\right)-\sigma_0^2 C_N}{\sqrt{2C_N}} \overset{d}{\underset{N \rightarrow \infty}{\longrightarrow}}\mathcal{N}(0,\sigma_0^{4}).
\end{equation*}
\end{theorem}
	




\section{Finite sample properties} \label{sec:simu}

A simulation study was then conducted to compare the performances of the proposed signatures-based spatial autoregressive model considering the penalized spatial regression and the signatures projections strategies. We also compared them with the functional linear model proposed by \cite{ahmed2022quasi}.

\subsection{Design of the simulation study}

 We considered a grid with $60 \times 60$ locations, where we randomly allocate N=200 spatial units.
Then the data was generated according to the following three models where \\
$$X_i(t) = (X_{i,1}(t), \dots, X_{i,p}(t))^\top, X_{i,k}(t) = \alpha_{i,k} t + f_{i,k}(t),$$
$$\theta^*(t) = (\theta_1^*(t), \dots, \theta_p^*(t))^\top, \theta^*_k(t) = \beta_k t + g_{i,k}(t),$$ 
$$\alpha_{i,k} \sim \mathcal{U}([-3,3]) \text{ and } \beta_k \sim \mathcal{U}([-3,3]).$$
$f_{i,k}$ and $g_{i,k}$ are Gaussian processes with exponential covariance matrix with length-scale 1, and $U_i \sim \mathcal{N}(0,1)$ for $i \in \llbracket 1, N \rrbracket$.
$X_i$ is observed at 101 equally spaced times of $[0,1]$.

\begin{itemize}[leftmargin=1.9cm]
\item[Model 1.] $Y_i = \rho_0 \dsum_{j=1}^N v_{ij,N} Y_j + \int_0^1 X_i(t)^\top \theta^*(t) \ \text{d}t + U_i$.
\item[Model 2.] $Y_i = \rho_0 \dsum_{j=1}^N v_{ij,N} Y_j + ||\alpha_i|| + U_i$, $\alpha_i = (\alpha_{i,1}, \dots, \alpha_{i,p})^\top $.
\item[Model 3.] $Y_i = \rho_0 \dsum_{j=1}^N v_{ij,N} Y_j + \langle \mathcal{S}^{D^*}(X_i), \mathcal{S}^{D^*}(\theta^*) \rangle + U_i$
where $D^* = 2$.
\end{itemize}

The spatial weight matrix $V_N$ was constructed using the $k$ nearest neighbors method, and we considered the cases $k=4$ and $k=8$, $p=2,6,10$ and $\rho=0,0.2,0.4,0.6,0.8$. \\

Moreover, it should be noted that the signatures present the property to be invariant by translation and by time reparametrization \citep{lyons2007differential}. Thus, to circumvent the invariance by translation we added an observation point taking the value 0 at the beginning of $X_i$, and to avoid the invariance by time reparametrization we considered $\tilde{X}_i(t) = (X_i(t)^\top, t)$ before computing the signature of $X_i$. 



For each model, several approaches were compared:

\begin{itemize}
\item[(i)] The approach proposed by \cite{ahmed2022quasi} using a cubic B-splines basis with 12 equally spaced knots to approximate the $X_i$ from the observed data and a functional PCA \citep{ramsay2005functional}. As proposed by \cite{ahmed2022quasi}, we used a threshold on the number of coefficients such that the cumulative inertia was below $95\%$. However we also investigated this approach without using a pre-defined threshold on the number of coefficients.
\item[(ii)] Our proposed approach based on the penalized spatial regression.
\item[(iii)] Our proposed approach based on signatures projections. We considered a maximum truncation order for the signatures to reach a maximal number of coefficients of $10^3$. Then a PCA was performed on the truncated signature coefficients vectors. Four strategies were considered: standardizing or not the signature coefficients before computing the PCA and using a threshold on the maximal number of coefficients such that the cumulative inertia was below $95\%$, or not using a threshold.
\end{itemize}

For each model and each value of $k,p$ and $\rho$, 200 data sets were generated. Each data set was then split into a training, a validation and a test set such that the optimal number of coefficients (for the methods using PCA) or the optimal truncation order (for the penalized regression) was selected on the validation set based on the mean square error (MSE) criterion and the performances were finally evaluated on the test set using the MSE.





\subsection{Results of the simulation study}

For the sake of brevity, the results of the simulation study are presented in Figures \ref{fig:rho4_1}, \ref{fig:rho4_2}, \ref{fig:rho4_3}, \ref{fig:mse4_1}, \ref{fig:mse4_2} and \ref{fig:mse4_3} for $k=4$ only. However it should be noted that similar results were obtained for $k=8$.

All the approaches give similar estimations of the spatial autocorrelation coefficient $\rho$ and the latter is reasonably well estimated. \\

Regarding the MSE, in all cases, our proposed approach based on the PCA on the signature coefficients presents better performances when the signature coefficients are standardized. Not using a threshold on the number of coefficients does not appear to change the performance except for Models 1 and 3 with $p=10$, where it allows to slightly decrease the MSE. The approach of \cite{ahmed2022quasi} presents similar performances with or without a threshold on the number of coefficients. \\

With Model 1, which is naturally favourable to the approach of the aforementioned authors, the latter presents the best performances. It should be noted, however, that our approach based on a penalized regression presents close MSEs.

In the case of Model 3 (which is naturally favorable to our methods), our approaches based on a penalized regression or a PCA (with standardization) on the signatures coefficients  present much lower MSEs than the approach of \cite{ahmed2022quasi}. 

Finally, with Model 2, the approach of the above-mentioned authors and our proposition using a PCA (with standardization) on the signatures coefficients give similar performance. Our approach using penalized regression presents slightly lower MSEs.





\begin{figure}
\centering
\includegraphics[width=\textwidth]{k4_mod1_rhos.pdf}
\caption{Estimation of $\rho$ by the approach of \cite{ahmed2022quasi} (with and without threshold on the number of coefficients), our signatures projections approach (with and without threshold on the number of coefficients), and our penalized regression approach for Model 1 with $k=4$}
\label{fig:rho4_1}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=\textwidth]{k4_mod2_rhos.pdf}
\caption{Estimation of $\rho$ by the approach of \cite{ahmed2022quasi} (with and without threshold on the number of coefficients), our signatures projections approach (with and without threshold on the number of coefficients), and our penalized regression approach for Model 2 with $k=4$}
\label{fig:rho4_2}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=\textwidth]{k4_mod3_rhos.pdf}
\caption{Estimation of $\rho$ by the approach of \cite{ahmed2022quasi} (with and without threshold on the number of coefficients), our signatures projections approach (with and without threshold on the number of coefficients), and our penalized regression approach for Model 3 with $k=4$}
\label{fig:rho4_3}
\end{figure}



\begin{figure}
\centering
\includegraphics[width=\textwidth]{k4_mod1_MSE_test.pdf}
\caption{MSE on the test set with the approach of \cite{ahmed2022quasi} (with and without threshold on the number of coefficients), our signatures projections approach (with and without threshold on the number of coefficients), and our penalized regression approach for Model 1 with $k=4$}
\label{fig:mse4_1}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{k4_mod2_MSE_test.pdf}
\caption{MSE on the test set with the approach of \cite{ahmed2022quasi} (with and without threshold on the number of coefficients), our signatures projections approach (with and without threshold on the number of coefficients), and our penalized regression approach for Model 2 with $k=4$}
\label{fig:mse4_2}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{k4_mod3_MSE_test.pdf}
\caption{MSE on the test set with the approach of \cite{ahmed2022quasi} (with and without threshold on the number of coefficients), our signatures projections approach (with and without threshold on the number of coefficients), and our penalized regression approach for Model 3 with $k=4$}
\label{fig:mse4_3}
\end{figure}



\section{Discussion} \label{sec:conclu}
Here we proposed an alternative to the traditional spatial autoregressive model with functional covariates \citep{huang2018spatial, pineda2019functional, ahmed2022quasi}. This new approach is based on the notion of signatures and presents the advantages of being applicable to a wide range of processes that are not necessary square-integrable processes, and to better capture the differences between the curves. We then proposed two methods for estimating the model, respectively based on a penalized regression (similarly to \cite{fermanian2022functional}) and on signatures projections (using a PCA for example). We also provided their theoretical guarantees. \\

In a simulation study, when simulating the data according to the model of \cite{ahmed2022quasi}, the latter presented the best performances. However it should be noted that our approach presented close performances. When simulating the data according to the model considered here, our proposed method performs much better than the approach of the previously mentioned authors. Finally, we considered a third simulation model which does not correspond to the one of \cite{ahmed2022quasi}, nor to the one presented in this work. In this framework, our proposed approach based on a penalized regression presented the best performances and the version using signatures projections and that of the above-mentioned authors present similar but slightly lower performances. \\

Although we do not present an application to real data in this work, this will be included in a future version of the document. \\
It should be noted that we have considered here only functional covariates. However, it would be interesting to be able to integrate both functional and non-functional covariates in the model. \\
Moreover the approach proposed here is designed to handle a univariate response variable. The case of a multivariate response variable should also be investigated. 
\cite{zhu2020multivariate} has recently proposed a spatial autoregressive model in this context with non-functional covariates. Thus, extending this approach to the context of functional covariates may be the subject of future work. \\
Another development could be the adaptation of our proposed approach to the  spatial error model (SEM) or the SARAR model (SAR model with SAR errors) considering spatially correlated errors, with functional covariates.






	
\bibliography{biblioFSAR.bib}
\bibliographystyle{plainnat}


\appendix

\begin{lemma} \label{lemme1}
Under \textbf{Assumptions I(i)-(vii)}, 
$$ \dfrac{1}{\sqrt{N}} \dfrac{\partial \tilde{\ell}_N(\beta_{0}) }{\partial \beta_{D^*}} = O_{\mathbb{P}}(1).$$
\end{lemma}


\section{Proof of Lemma \ref{lemme1}}

For $\sigma^2$ we have :
\begin{align*}
\dfrac{\partial \tilde{\ell}_N(\beta_{D^*})}{\partial \sigma^2} &= -\dfrac{N}{2\sigma^2} + \dfrac{1}{2 \sigma^4} [S_N(\rho) \mathbf{Y}_N - \chi_N \gamma_{D^*}]^\top [S_N(\rho) \mathbf{Y}_N - \chi_N \gamma_{D^*}]. 
\end{align*}
In $\beta_{D^*} = \beta_{0}$ we obtain:
\begin{equation} \label{eq1}
\dfrac{1}{\sqrt{N}} \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \sigma^2} = \dfrac{1}{2 \sigma_0^4 \sqrt{N}}[\mathbf{U}_N^\top \mathbf{U}_N - N \sigma_0^2].
\end{equation}


For $\rho$ we have :
\begin{eqnarray*}
\dfrac{\partial \tilde{\ell}_N(\beta_{D^*})}{\partial \rho} &=& -\text{tr}(V_N S_N^{-1}(\rho)) \\ && - \dfrac{1}{2 \sigma^2} [-\mathbf{Y}_N^\top V_N^\top \mathbf{Y}_N - \mathbf{Y}_N^\top V_N \mathbf{Y}_N + 2\rho \mathbf{Y}_N^\top V_N^\top V_N \mathbf{Y}_N + \gamma_{D^*}^\top \chi_N^\top V_N \mathbf{Y}_N + \mathbf{Y}_N^\top V_N^\top \chi_N \gamma_{D^*}] .
\end{eqnarray*}
In $\beta_{D^*} = \beta_{0}$ we obtain:
\begin{align*}
\dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \rho} &= - \text{tr}(G_N) + \dfrac{1}{2 \sigma_0^2} [\mathbf{Y}_N^\top V_N^\top (I-\rho_0 V_N) \mathbf{Y}_N + \mathbf{Y}_N^\top (I-\rho_0 V_N^\top) V_N \mathbf{Y}_N - \gamma_0^\top \chi_N^\top V_N \mathbf{Y}_N - \mathbf{Y}_N^\top V_N^\top \chi_N \gamma_0] \\
&= -\text{tr}(G_N) + \dfrac{1}{\sigma_0^2} \mathbf{U}_N^\top V_N \mathbf{Y}_N \\
&=  -\text{tr}(G_N) + \dfrac{1}{\sigma_0^2} \mathbf{U}_N^\top V_N S_N^{-1} \chi_N \gamma_0 + \dfrac{1}{\sigma_0^2} \mathbf{U}_N^\top V_N S_N^{-1} \mathbf{U}_N \\
&= -\text{tr}(G_N) + \dfrac{1}{\sigma_0^2} (G_N \chi_N \gamma_0)^\top \mathbf{U}_N + \dfrac{1}{\sigma_0^2} \mathbf{U}_N^\top G_N \mathbf{U}_N . \\
\end{align*}
And so
\begin{equation} \label{eq2}
\dfrac{1}{\sqrt{N}} \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \rho} = \dfrac{1}{\sqrt{N} \sigma_0^2} (G_N \chi_N \gamma_0)^\top \mathbf{U}_N + \dfrac{1}{\sqrt{N} \sigma_0^2} [\mathbf{U}_N^\top G_N \mathbf{U}_N - \sigma_0^2 \text{tr}(G_N)].
\end{equation}


For $\gamma_{D^*}$ we have :
\begin{align*}
\dfrac{\partial \tilde{\ell}_N(\beta_{D^*})}{\partial \gamma_{D^*}} &= \dfrac{1}{\sigma^2} \chi_N^\top [S_N(\rho) \mathbf{Y}_N - \chi_N \gamma_{D^*}].
\end{align*}
In $\beta_{D^*} = \beta_{0}$ we obtain:
\begin{equation} \label{eq3}
\dfrac{1}{\sqrt{N}} \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \gamma_{D^*}} = \dfrac{1}{\sqrt{N} \sigma_0^2} \chi_N^\top \mathbf{U}_N .
\end{equation}


Then we can compute the variances:

\begin{align*}
\mathbb{V}\left( \dfrac{1}{\sqrt{N}} \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \sigma^2} \right) &= \mathbb{V}\left( \dfrac{1}{2 \sigma_0^4 \sqrt{N}}[\mathbf{U}_N^\top \mathbf{U}_N - N \sigma_0^2] \right) \\
&= \dfrac{1}{4 \sigma_0^8 N} \mathbb{V}(\mathbf{U}_N^\top \mathbf{U}_N) \\
&= \dfrac{1}{4 \sigma_0^8} \mathbb{V}(U_1^2) \\
&= \dfrac{1}{4 \sigma_0^8} [\mathbb{E}(U_1^4) - \sigma_0^4] = O(1) \text{ by Assumption I(i) .} \\
\end{align*}
Thus $ \dfrac{1}{\sqrt{N}} \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \sigma^2} = O_{\mathbb{P}}(1)$ by Chebyshev's inequality. \\

\begin{align*}
\mathbb{V} \left( \dfrac{1}{\sqrt{N}} \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \rho} \right) &= \mathbb{V} \left( \dfrac{1}{\sqrt{N} \sigma_0^2} (G_N \chi_N \gamma_0)^\top \mathbf{U}_N + \dfrac{1}{\sqrt{N} \sigma_0^2} [\mathbf{U}_N^\top G_N \mathbf{U}_N - \sigma_0^2 \text{tr}(G_N)] \right) \\
&\le 2 \left[  \mathbb{V}\left( \dfrac{1}{\sqrt{N} \sigma_0^2} (G_N \chi_N \gamma_0)^\top \mathbf{U}_N \right) +  \mathbb{V}\left( \dfrac{1}{\sqrt{N} \sigma_0^2} (\mathbf{U}_N^\top G_N \mathbf{U}_N - \sigma_0^2 \text{tr}(G_N)) \right) \right] \\
&= \dfrac{2}{N \sigma_0^2} (G_N \chi_N \gamma_0)^\top (G_N \chi_N \gamma_0) + \dfrac{2}{N \sigma_0^4} \mathbb{V}(\mathbf{U}_N^\top G_N \mathbf{U}_N) \\
&= O(1) + O\left( \dfrac{1}{h_N} \right) = O(1)
\end{align*}
 by using the inequality for two real random variables $R_1$ and $R_2$: $$\mathbb{V}(R_1)+\mathbb{V}(R_2) \le 2[\mathbb{V}(R_1) + \mathbb{V}(R_2)].$$  

Thus, $\dfrac{1}{\sqrt{N}} \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \rho} = O_{\mathbb{P}}(1)$ by Chebyshev's inequality. \\

Since $\dfrac{1}{\sqrt{N}} \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \gamma_{D^*}} = \dfrac{1}{\sqrt{N} \sigma_0^2} \chi_N^\top \mathbf{U}_N$ we have by Assumption I(v) that  $\dfrac{1}{\sqrt{N}} \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \gamma_{D^*}} = O_{\mathbb{P}}(1)$. \\



\begin{lemma} \label{lemme2}
 Under \textbf{Assumptions I(i)-(viii)} we have \citep{lee2004asymptotic}
    $$ \dfrac{1}{N} \dfrac{\partial^2 \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*} \partial \beta_{D^*}^\top } = \mathbb{E}\left[ \dfrac{1}{N} \dfrac{\partial^2 \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*} \partial \beta_{D^*}^\top } \right] + o_{\mathbb{P}}(1).$$
\end{lemma}


\section{Proof of Theorem \ref{th1}}

Let $\alpha_N = N^{-1/2} + a_N$.
It suffices to show that for all $\eta > 0$, there exists a sufficiently large constant $C$ such that
\begin{equation} \label{equation}
\mathbb{P}[ \underset{||u||=C}{\sup} \ Q_N(\beta_{0} + \alpha_N u) < Q_N(\beta_{0})] \ge 1 - \eta .
\end{equation}

\begin{flalign*}
\mathcal{D}_N(u) &= Q_N(\beta_{0} + \alpha_N u) - Q_N(\beta_{0}) & \\
&= \tilde{\ell}_N(\beta_{0} + \alpha_N u) - \tilde{\ell}_N(\beta_{0}) - N \dsum_{j=4}^{s_p(D^*)+3} [p_{\lambda_N}(\beta_{j,0} + \alpha_N u_j) - p_{\lambda_N}(\beta_{j,0})] . &
\end{flalign*}
By using a Taylor expansion at order 2 on $\tilde{\ell}_N$ and $p_{\lambda_N}$ we get:
\begin{eqnarray*}
\mathcal{D}_N(u) &=& \alpha_N \left( \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*}} \right)^\top u + \dfrac{1}{2} \alpha_N^2 u^\top \dfrac{\partial^2 \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*} \partial \beta_{D^*}^\top} u (1 + o_{\mathbb{P}}(1)) \\ 
&&- N \dsum_{j=4}^{s_p(D^*)+3} \left[ p'_{\lambda_N}(\beta_{j,0}) \alpha_N u_j + \dfrac{1}{2} \alpha_N^2 u_j^2 p''_{\lambda_N}(\beta_{j,0})(1 + o_{\mathbb{P}}(1)) \right] \\
&=& \alpha_N \left( \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*}} \right)^\top u + \dfrac{1}{2} \alpha_N^2 u^\top \mathbb{E}\left( \dfrac{\partial^2 \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*} \partial \beta_{D^*}^\top} \right) u (1 + o_{\mathbb{P}}(1)) \\ 
&&- N \dsum_{j=4}^{s_p(D^*)+3} \left[ p'_{\lambda_N}(\beta_{j,0}) \alpha_N u_j + \dfrac{1}{2} \alpha_N^2 u_j^2 p''_{\lambda_N}(\beta_{j,0})(1 + o_{\mathbb{P}}(1)) \right] \text{ by Lemma \ref{lemme2}} \\
&=& J_1 + J_2 - J_3
\end{eqnarray*}

where 
$$ \mathbb{E}\left( \dfrac{\partial^2 \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*} \partial \beta_{D^*}^\top} \right) = 
\begin{pmatrix}
\dfrac{-N}{2 \sigma_0^4} & \dfrac{-\text{tr}(G_N)}{\sigma_0^2} & 0 \\
* & -\text{tr}((G_N+G_N^\top)G_N) - \dfrac{1}{\sigma_0^2}(G_N \chi_N \gamma_0)^\top(G_N \chi_N \gamma_0) & \dfrac{-1}{\sigma_0^2} \gamma_0^\top \chi_N^\top G_N^\top \chi_N \\
* & * & \dfrac{-1}{\sigma_0^2} \chi_N^\top \chi_N
\end{pmatrix},$$
$J_1 = \alpha_N \left( \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*}} \right)^\top u,$
$J_2 = \dfrac{1}{2} \alpha_N^2 u^\top \mathbb{E}\left( \dfrac{\partial^2 \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*} \partial \beta_{D^*}^\top} \right) u (1 + o_{\mathbb{P}}(1))$ and \\
$J_3 = N \dsum_{j=4}^{s_p(D^*)+3} \left[ p'_{\lambda_N}(\beta_{j,0}) \alpha_N u_j + \dfrac{1}{2} \alpha_N^2 u_j^2 p''_{\lambda_N}(\beta_{j,0})(1 + o_{\mathbb{P}}(1)) \right].$ \\

\begin{flalign*}
J_1 &= \alpha_N \left( \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*}} \right)^\top u \\
&= \left\langle \alpha_N \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*}}, u \right\rangle & \\
&\le \bigg|\bigg| \alpha_N \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*}}  \bigg|\bigg| \ ||u|| \text{ by Cauchy–Schwarz inequality} & \\
&= |\alpha_N| \bigg|\bigg| \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*}}  \bigg|\bigg| \ ||u|| & \\
&= O_{\mathbb{P}}(N \alpha_N^2) ||u|| \text{ by Lemma \ref{lemme1}}.
\end{flalign*}



\begin{flalign*}
J_2 &=  \dfrac{1}{2} \alpha_N^2 u^\top \mathbb{E}\left( \dfrac{\partial^2 \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*} \partial \beta_{D^*}^\top} \right) u (1 + o_{\mathbb{P}}(1)) & \\
&= ||u||^2 O_{\mathbb{P}}(N \alpha_N^2 ) . & \\
\end{flalign*}


\begin{flalign*}
J_3 &=  N \dsum_{j=4}^{s_p(D^*)+3} \left[ p'_{\lambda_N}(\beta_{j,0}) \alpha_N u_j + \dfrac{1}{2} \alpha_N^2 u_j^2 p''_{\lambda_N}(\beta_{j,0})(1 + o_{\mathbb{P}}(1)) \right] & \\
&\le N \dsum_{j=4}^{s_p(D^*)+3} \left[ a_N \alpha_N |u_j| + \dfrac{1}{2} \alpha_N^2 u_j^2 b_N (1 + o_{\mathbb{P}}(1)) \right] & \\
&\le N a_N \alpha_N \dsum_{j=1}^{s_p(D^*)+3}  |u_j| +   \dfrac{N}{2} \alpha_N^2 b_N \dsum_{j=1}^{s_p(D^*)+3} u_j^2  (1 + o_{\mathbb{P}}(1)) & \\
&\le  O_{\mathbb{P}}(N \alpha_N^2) ||u|| + o_{\mathbb{P}}(N \alpha_N^2) ||u||^2 .
\end{flalign*}

Thus when $C$ is sufficiently large, $J_1$ and $J_3$ are uniformly dominated by $J_2$ which implies (\ref{equation}) and Theorem \ref{th1}.




\section{Proof of Theorem \ref{th2}}

\begin{flalign*}
\dfrac{\partial Q_N(\beta_{D^*})}{\partial \beta_{j}} &= \dfrac{\partial \tilde{\ell}_N(\beta_{D^*})}{\partial \beta_{j}} - N p'_{\lambda_N}(\beta_j) \mathds{1}_{j \ge 4}.
\end{flalign*}
By using a Taylor expansion of $\dfrac{\partial \tilde{\ell}_N(\beta_{D^*})}{\partial \beta_{j}}$ and $p'_{\lambda_N}(\beta_j)$ at order 1 we get:
\begin{eqnarray*}
\dfrac{\partial Q_N(\beta_{D^*})}{\partial \beta_{j}} &=& \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \beta_{j}} + \dsum_{k=1}^{s_p(D^*)+3} \left[ \dfrac{\partial^2 \tilde{\ell}_N(\beta_0)}{\partial \beta_k \partial \beta_j} (\beta_k - \beta_{k,0}) + o_{\mathbb{P}}(\beta_k - \beta_{k,0}) \right] \\
&& - N \left[ p'_{\lambda_N}(\beta_{j,0}) + p''_{\lambda_N}(\beta_{j,0}) (\beta_j - \beta_{j,0}) + o_{\mathbb{P}}(\beta_j - \beta_{j,0}) \right] \mathds{1}_{j \ge 4}.
\end{eqnarray*}
For $\hat{\beta}_{D^*}$ such that $\dfrac{\partial Q_N(\hat{\beta}_{D^*})}{\partial \beta_{j}}=0$ for all $j \in \llbracket 1, s_p(D^*)+3 \rrbracket$, we have

\begin{eqnarray*}
\dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \beta_{j}} &=& - \dsum_{k=1}^{s_p(D^*)+3} \left[ \dfrac{\partial^2 \tilde{\ell}_N(\beta_0)}{\partial \beta_k \partial \beta_j} (\hat{\beta}_k - \beta_{k,0}) + o_{\mathbb{P}}(\hat{\beta}_k - \beta_{k,0}) \right] \\
&& + N \left[ p'_{\lambda_N}(\beta_{j,0}) + p''_{\lambda_N}(\beta_{j,0}) (\hat{\beta}_j - \beta_{j,0}) + o_{\mathbb{P}}(\hat{\beta}_j - \beta_{j,0})  \right] \mathds{1}_{j \ge 4} \\
&=& - \dsum_{k=1}^{s_p(D^*)+3} \dfrac{\partial^2 \tilde{\ell}_N(\beta_0)}{\partial \beta_k \partial \beta_j} (\hat{\beta}_k - \beta_{k,0}) + o_{\mathbb{P}}\left(\dsum_{k=1}^{s_p(D^*)+3} \hat{\beta}_k - \beta_{k,0} \right) \\
&& + N p'_{\lambda_N}(\beta_{j,0}) \mathds{1}_{j \ge 4} + N p''_{\lambda_N}(\beta_{j,0}) (\hat{\beta}_j - \beta_{j,0}) \mathds{1}_{j \ge 4} + o_{\mathbb{P}}(N (\hat{\beta}_j - \beta_{j,0}) ) \mathds{1}_{j \ge 4} . \\
\end{eqnarray*}
Since $o_{\mathbb{P}}\left(\dsum_{k=1}^{s_p(D^*)+3} \hat{\beta}_k - \beta_{k,0} \right) = o_{\mathbb{P}}(N^{-1/2})$ and $o_{\mathbb{P}}(N (\hat{\beta}_j - \beta_{j,0}) ) = o_{\mathbb{P}}(\sqrt{N} )$ 
from Theorem \ref{th1}, we have 

\begin{eqnarray*}
\dfrac{1}{\sqrt{N}} \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \beta_{j}} &=& - \dsum_{k=1}^{s_p(D^*)+3} \dfrac{\partial^2 \tilde{\ell}_N(\beta_0)}{\partial \beta_k \partial \beta_j} \dfrac{1}{N} \sqrt{N} (\hat{\beta}_k - \beta_{k,0}) \\
&& + \sqrt{N} p'_{\lambda_N}(\beta_{j,0}) \mathds{1}_{j \ge 4} + \sqrt{N} p''_{\lambda_N}(\beta_{j,0}) (\hat{\beta}_j - \beta_{j,0}) \mathds{1}_{j \ge 4} + o_{\mathbb{P}}(1 ) . \\
\end{eqnarray*}

Let $A = - \dfrac{1}{N} \dfrac{\partial^2 \tilde{\ell}_N(\beta_0)}{\partial \beta_{D^*} \partial \beta_{D^*}^\top}$, $P_1 = (0,0,0,p'_{\lambda_N}(\beta_{4,0}), \dots, p'_{\lambda_N}(\beta_{s_p(D^*)+3,0}))^\top$ and \\ $P_2 = \text{diag}(0,0,0,p''_{\lambda_N}(\beta_{4,0}), \dots, p''_{\lambda_N}(\beta_{s_p(D^*)+3,0}))$. Then

\begin{flalign*}
\dfrac{1}{\sqrt{N}} \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*}} &= \sqrt{N} A (\hat{\beta}_{D^*} - \beta_{0}) + \sqrt{N} P_1 + \sqrt{N} P_2 (\hat{\beta}_{D^*} - \beta_{0}) + o_{\mathbb{P}}(1 )) & \\
&=  \sqrt{N} \mathbb{E}(A) (\hat{\beta}_{D^*} - \beta_{0}) + \sqrt{N} [A-\mathbb{E}(A)] (\hat{\beta}_{D^*} - \beta_{0}) + \sqrt{N} P_1 + \sqrt{N} P_2 (\hat{\beta}_{D^*} - \beta_{0}) + o_{\mathbb{P}}(1 ) . & \\
\end{flalign*}
From Lemma \ref{lemme2} and Theorem \ref{th1}, we have 
\begin{equation} \label{eqfinal}
\begin{split}
\dfrac{1}{\sqrt{N}} \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*}} &=  \sqrt{N} \mathbb{E}(A) (\hat{\beta}_{D^*} - \beta_{0}) + \sqrt{N} P_1 + \sqrt{N} P_2 (\hat{\beta}_{D^*} - \beta_{0}) + o_{\mathbb{P}}(1) \\
&=  \sqrt{N} \left[(\mathbb{E}(A)+P_2) (\hat{\beta}_{D^*} - \beta_{0}) + P_1 \right] + o_{\mathbb{P}}(1) . \\
\end{split}
\end{equation}

From (\ref{eq1}), (\ref{eq2}) and (\ref{eq3}), $\dfrac{1}{\sqrt{N}} \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*}}$ can also be written as
\begin{flalign*}
\dfrac{1}{\sqrt{N}} \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*}} &= \begin{pmatrix}
    \dfrac{1}{2 \sigma_0^4 \sqrt{N}}[\mathbf{U}_N^\top \mathbf{U}_N - N \sigma_0^2] \\
    \dfrac{1}{\sqrt{N} \sigma_0^2} (G_N \chi_N \gamma_0)^\top \mathbf{U}_N + \dfrac{1}{\sqrt{N} \sigma_0^2} [\mathbf{U}_N^\top G_N \mathbf{U}_N - \sigma_0^2 \text{tr}(G_N)] \\
    \dfrac{1}{\sqrt{N} \sigma_0^2} \chi_N^\top \mathbf{U}_N
\end{pmatrix} & \\
&= \begin{pmatrix}
    0 \\
    \dfrac{1}{\sqrt{N} \sigma_0^2} (G_N \chi_N \gamma_0)^\top \mathbf{U}_N \\
    \dfrac{1}{\sqrt{N} \sigma_0^2} \chi_N^\top \mathbf{U}_N
\end{pmatrix} + \begin{pmatrix}
    \dfrac{1}{2 \sigma_0^4 \sqrt{N}}[\mathbf{U}_N^\top \mathbf{U}_N - N \sigma_0^2] \\
    \dfrac{1}{\sqrt{N} \sigma_0^2} [\mathbf{U}_N^\top G_N \mathbf{U}_N - \sigma_0^2 \text{tr}(G_N)] \\
    0
\end{pmatrix} & \\
&= \mathcal{M}_{N,1} + \mathcal{M}_{N,2} . &
\end{flalign*}

Then the central limit theorem for linear-quadratic forms \citep{kelejian2001asymptotic} states that
$$ \dfrac{1}{\sqrt{N}} \dfrac{\partial \tilde{\ell}_N(\beta_{0})}{\partial \beta_{D^*}} \overset{d}{\underset{N \rightarrow \infty}{\longrightarrow}} \mathcal{N}\left( 0, \underset{N \rightarrow \infty}{\lim} \ \mathbb{V}(\mathcal{M}_{N,1}) + \mathbb{V}(\mathcal{M}_{N,2}) + \mathbb{E}\left(\mathcal{M}_{N,1}\mathcal{M}_{N,2}^\top\right) + \mathbb{E}\left(\mathcal{M}_{N,1}\mathcal{M}_{N,2}^\top\right)^\top \right), $$

where

\begin{flalign*}
\mathbb{V}(\mathcal{M}_{N,1}) &= \begin{pmatrix}
    0 & 0 & 0 \\
    0 & \dfrac{1}{N \sigma_0^2} (G_N \chi_N \gamma_0)^\top (G_N \chi_N \gamma_0) & \dfrac{1}{N \sigma_0^2} \gamma_0^\top \chi_N^\top G_N^\top \chi_N \\
    0 & * & \dfrac{1}{N \sigma_0^2} \chi_N^\top \chi_N
\end{pmatrix}, &
\end{flalign*}

\begin{flalign*}
\mathbb{V}(\mathcal{M}_{N,2}) &=  
\begin{pmatrix}
\dfrac{\mathbb{E}(U_1^4) - \sigma_0^4}{4 \sigma_0^8} & \dfrac{\mathbb{E}(U_1^4) - \sigma_0^4}{2N \sigma_0^6} \text{tr}(G_N) & 0 \\
 * & \dfrac{\mathbb{E}(U_1^4)-\sigma_0^4}{N \sigma_0^4}  \dsum_{i=1}^N G_{ii,N}^2 + \dfrac{1}{N} \text{tr}((G_N^\top + G_N)G_N) & 0 \\
 0 & 0 & 0
\end{pmatrix}
&
\end{flalign*}

and

\begin{flalign*}
\mathbb{E}\left(\mathcal{M}_{N,1}\mathcal{M}_{N,2}^\top\right) &=    
\begin{pmatrix}
0 & 0 & 0 \\
\dfrac{\mathbb{E}(U_1^3)}{2N \sigma_0^6} (G_N \chi_N \gamma_0)^\top \mathbf{1}_N & \dfrac{\mathbb{E}(U_1^3)}{N \sigma_0^4} (G_N \chi_N \gamma_0)^\top (G_{11,N}, \dots, G_{NN,N})^\top & 0 \\
\dfrac{\mathbb{E}(U_1^3)}{2N \sigma_0^6} \chi_N^\top \mathbf{1}_N & \dfrac{\mathbb{E}(U_1^3)}{\sigma_0^4 N} \chi_N^\top (G_{11,N}, \dots, G_{NN,N})^\top & 0
\end{pmatrix}
& \\
&= \begin{pmatrix}
0 & 0 & 0 \\
\dfrac{\mathbb{E}(U_1^3)}{2N \sigma_0^6} (G_N \chi_N \gamma_0)^\top \mathbf{1}_N & \dfrac{\mathbb{E}(U_1^3)}{N \sigma_0^4} \dsum_{i=1}^N G_{ii,N} G_{i.,N} \chi_N \gamma_0 & 0 \\
\dfrac{\mathbb{E}(U_1^3)}{2N \sigma_0^6} \chi_N^\top \mathbf{1}_N & \dfrac{\mathbb{E}(U_1^3)}{\sigma_0^4 N} \dsum_{i=1}^N G_{ii,N} \chi_{i.,N}^\top & 0
\end{pmatrix} &
\end{flalign*}
where $G_{i.,N}$ and $\chi_{i.,N}$ denote the $i^\text{th}$ row of $G_N$ and $\chi_N$ respectively.


We conclude by (\ref{eqfinal}) and Slutsky's theorem:
\begin{align*}
  \sqrt{N} [(\mathbb{E}(A)+P_2) (\hat{\beta}_{D^*} - \beta_{0}) + P_1] \overset{d}{\underset{N \rightarrow \infty}{\longrightarrow}} \mathcal{N}\left(0, \underset{N \rightarrow \infty}{\lim} \ \right. &\mathbb{V}(\mathcal{M}_{N,1}) + \mathbb{V}(\mathcal{M}_{N,2})  \\ &\left. + \mathbb{E}\left(\mathcal{M}_{N,1}\mathcal{M}_{N,2}^\top\right) + \mathbb{E}\left(\mathcal{M}_{N,1}\mathcal{M}_{N,2}^\top\right)^\top \right) .   
\end{align*}



\end{document}