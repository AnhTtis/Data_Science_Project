{
    "arxiv_id": "2303.17481",
    "paper_title": "Teaching contact-rich tasks from visual demonstrations by constraint extraction",
    "authors": [
        "Christian Hegeler",
        "Filippo Rozzi",
        "Loris Roveda",
        "Kevin Haninger"
    ],
    "submission_date": "2023-03-30",
    "revised_dates": [
        "2023-03-31"
    ],
    "latest_version": 1,
    "categories": [
        "cs.RO"
    ],
    "abstract": "Contact-rich manipulation involves kinematic constraints on the task motion, typically with discrete transitions between these constraints during the task. Allowing the robot to detect and reason about these contact constraints can support robust and dynamic manipulation, but how can these contact models be efficiently learned? Purely visual observations are an attractive data source, allowing passive task demonstrations with unmodified objects. Existing approaches for vision-only learning from demonstration are effective in pick-and-place applications and planar tasks. Nevertheless, accuracy/occlusions and unobserved task dynamics can limit their robustness in contact-rich manipulation. To use visual demonstrations for contact-rich robotic tasks, we consider the demonstration of pose trajectories with transitions between holonomic kinematic constraints, first clustering the trajectories into discrete contact modes, then fitting kinematic constraints per each mode. The fit constraints are then used to (i) detect contact online with force/torque measurements and (ii) plan the robot policy with respect to the active constraint. We demonstrate the approach with real experiments, on cabling and rake tasks, showing the approach gives robust manipulation through contact transitions.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.17481v1"
    ],
    "publication_venue": "Submitted ICRA23"
}