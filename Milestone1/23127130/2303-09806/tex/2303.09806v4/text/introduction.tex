\section{INTRODUCTION}
 \begin{figure*}[htbp]
     \centering
     \includegraphics[width=\textwidth]{fig/overview.png}
     \caption{\textbf{The pipeline of DexRepNet.} We first retarget human grasp demonstrations to collect robot grasp trajectories (yellow), then we leverage the state-action pairs for grasp policy learning: behavior cloning for initialization and reinforcement learning for fine-tuning (blue). Finally, the trained policy can generate dexterous grasping with generalization (gray).
     }
     \label{fig:overview}
     \vskip -0.5cm
 \end{figure*}

Robotic dexterous grasping is a crucial skill to enable robots to perform complex tasks and achieve human-like capabilities in unstructured environments.
% e.g. using chopsticks~\cite{yang2022chopsticks}, 
% which is hard to realize by simple grippers. 
Despite the promising potential, learning dexterous grasping skills with multi-fingered hands is challenging due to its high DoF and complex contacts. Dexterous hands are typically equipped with many articulated joints, leading to more than 20 DoFs and formidable action space. Also, adapting the grasping skill to new objects requires learning methods to achieve good generalization to complex interaction contacts between multi-fingered hands and diverse objects.

Deep reinforcement learning (DRL) has shown great potential to solve the problem of dexterous grasping with its outstanding performance in sequential decision-making problems. DAPG~\cite{DAPG:rajeswaran2017learning} exploits human demonstrations collected in virtual reality to improve sample efficiency of DRL in a high-dimensional 30 DoFs hand space. Following the same human demonstration strategy, ILAD~\cite{ILAD:wu2022learning}, GRAFF~\cite{GRAFF:mandikal2021learning}, and DexVIP~\cite{mandikal2022dexvip} extends the demonstrations into larger scale trajectories or grasping poses collected from human interaction with real daily objects. GRAFF~\cite{GRAFF:mandikal2021learning} leverages contacts from hand-object interactions for agents to learn to approach objects more effectively.  DexVIP~\cite{mandikal2022dexvip} learns human pose priors from videos and imposes these priors into DRL by incorporating auxiliary reward functions favoring robot poses similar to the human ones in videos. ILAD~\cite{ILAD:wu2022learning} trains a generator to synthesize grasping trajectories with large-scale demonstrations instead of using human demonstrations directly.

Significant progress has been made in improving dexterous grasping performance through the above demonstrations to reduce sample complexity in the high-dimensional space. However, less attention has been paid to hand-object interaction representations for high-level generalization with dexterous hands. For point cloud inputs, features such as position, distance, or/and a global feature vector extracted by PointNet~\cite{qi2017pointnet} describe the relationship between hands and objects~\cite{wei2022dvgg},~\cite{shao2020unigrasp},~\cite{ye2022CGF}; for input with RGB images, a global feature vector extracted from CNN like ResNet~\cite{He_2016_CVPR} is used to capture the relation~\cite{levine2018handeye}. Though able to capture the geometry of objects, global features typically have difficulty in generalizing to new data. Moreover, simple representations like distances or positions of the objects and hands fail to capture the interactions between the diverse object surfaces and complex hand articulations, which is important for dexterous grasping having a large number of potential contacts. Therefore even with large-scale demonstrations, these works are still limited in the generalization to unseen objects. The policy learned by ILAD~\cite{ILAD:wu2022learning} has demonstrated good performance for objects in the same category. The grasp success rates of GRAFF~\cite{GRAFF:mandikal2021learning} and DexVIP~\cite{mandikal2022dexvip} for unseen objects in the simulation are lower than 70\%.

Aiming at improving generalization for dexterous grasping, in this paper, we propose a novel compound geometric and spatial hand-object representation to capture object surface features and the spatial relations between hands and objects when hands approach objects, which we name as \rep.  The compound representation comprises three components to fully capture the dynamics during interactions and embrace generalization at the same time:  1) Occupancy Feature, representing the voxels occupied by an object when a volume of low resolution attaching to a robotic hand moves towards the object; 2) Surface Feature, representing the closest distance of points on an object surface to a fixed set of points on a hand surface and also the normals of the closest points on the object; 3) Local-Geo Feature: representing the local geometry feature of the closest points. Occupancy Feature captures global shape information seen by the approaching hands via a coarse occupancy volume instead of static point clouds carrying detailed geometry and Local-Geo Feature only captures local detailed geometry: the combination of the coarse global and finer local features fully captures object surface information and also ensures generalization to unseen objects resembling objects in the training set roughly or partially. Surface Feature and Local-Geo Feature capture the dynamics of the interaction and the geometry feature of the most related object surface area for each hand part in potential contacts. 
% These features provide sufficient information for dexterous hands to grasp objects.

Based on the new representation, we propose a dexterous deep reinforcement learning method with human demonstrations to learn a generalizable grasping policy \method. Fig.\ref{fig:overview} shows the pipeline of our method. To obtain robotic demonstrations across different objects, we collect human grasp demonstrations from GRAB~\cite{taheri2020grab} and use motion retargeting technologies~\cite{handa2020dexpilot} to transfer them to robotic grasping demonstrations for behavior cloning (BC). \rep is utilized to extract the representation information of hand-object interaction and input it together with hand states to train the grasping policy.

In summary, our main contributions in this paper are:
\begin{itemize}
    \item We propose a novel geometric and spatial hand-object interaction representation consisting of Occupancy Feature, Surface Feature, and Local-Geo Feature for robotic dexterous grasping
    \item We propose a dexterous deep reinforcement learning method based on the novel representation to learn a generalizable grasping policy.
    \item Our proposed method outperforms baselines using existing representation dramatically for seen and unseen objects both in simulation and in the real world. Experiments also demonstrate its effectiveness on various multi-finger hands.
\end{itemize}




\section{related work}


% Conventional grasping approaches typically leverage geometric and physical constraints to identify the optimal grasping configuration, which often assumes the availability of object models and fixture configurations~\cite{844081}. In comparison, learning-based grasping is generally more stable and effective. For low-DOF end-effectors, there have been a lot of applicable works with excellent effects~\cite{cao2021suctionnet}. Some works~\cite{varley2017shape} split the grasping process into candidate generation and evaluation, other works~\cite{joshi2020robotic} jointly train an end-to-end network or ~\yq{use other methods such as~\cite{shao2020unigrasp}, etc.~\cite{duan2021robotics}}. 

% % But in general, there is still not enough work on high-DOF grasping.
\textbf{Dexterous Robotic Grasping.} 
There have been works in different directions attempting dexterous grasping.
% from mapping policy from low-DoF to high-DoF hands, optimization-based methods incorporating physical constraints, reinforcement learning via simulation. 
Liang \etal ~\cite{liang2021multifingered} map low-DOF end-effectors to high-DOF ones to solve the high-DOF grasping problem. Shao~\cite{shao2020unigrasp} models the robotic hand and object and uses optimization methods to find force closure and contact points.  Wei \etal~\cite{wei2022dvgg} presents an efficient grasp generation network that takes a single-view point cloud reconstructed by a point completion module as input and predicts high-quality grasp configurations for unknown objects. Turpin \etal~\cite{turpin2022graspd}  and Liu \etal~\cite{liu2020deep} adopt the differentiable simulation method to optimize a path towards stable grasping. To facilitate exploration and reduce sampling complexity, most DRL approaches for dexterous grasping require the use of expert demonstrations~\cite{DAPG:rajeswaran2017learning, ILAD:wu2022learning}. ~\cite{DAPG:rajeswaran2017learning} presents a demo augmented policy gradient method, incorporating expert trajectories collected via teleoperate into RL. \cite{ILAD:wu2022learning} proposed a novel imitation learning, using expert demonstrations to accelerate the phase of RL training. In comparison, some works incorporate human grasping affordance (contact map and grasping pose)~\cite{GRAFF:mandikal2021learning,mandikal2022dexvip, Christen_2022_CVPR}. ~\cite{GRAFF:mandikal2021learning} inject a visual affordance prior to deep RL grasping policies for functional grasping. Its following-up work~\cite{mandikal2022dexvip} learns dexterous grasping from in-the-wild hand-object interaction videos. Both works 
 aim to reduce or avoid the use of costly expert demonstrations. ~\cite{Christen_2022_CVPR} proposed dynamic grasp generation, which uses static grasp references to generate grasp motions. In this paper, we follow the human demonstration strategy and adopt DAPG ~\cite{DAPG:rajeswaran2017learning} for learning.

\textbf{Hand-Object Representation for Grasping.} DRL methods emphasize self-exploration in the interaction with the environment, therefore, a detailed description of the interaction is crucial. For object description, there are RGB~\cite{levine2018handeye}, depth~\cite{liu2020deep}, point cloud~\cite{wei2022dvgg, li2022contact2grasp}, mesh~\cite{varley2017shape}, and their fusion~\cite{GRAFF:mandikal2021learning},~\cite{mandikal2022dexvip},~\cite{cao2021suctionnet}. The description for hand is mainly the hand state~\cite{GRAFF:mandikal2021learning},~\cite{mandikal2022dexvip, yuan2017bighand2}. Unigrasp~\cite{shao2020unigrasp} introduced the URDF model of the hand so that the work can be applied to different multi-fingered dexterous hands without retraining the model. The hand-object relationship mainly uses more global information, such as the distance between hand and object~\cite{GRAFF:mandikal2021learning},~\cite{mandikal2022dexvip}, motor signals~\cite{joshi2020robotic}, etc. Though these descriptions work well in low-DoF grippers, their effectiveness in acquiring advanced dexterity for grasping skills is constrained due to their inability to fully capture the intricate interactions between various object surfaces and the intricate movements of the hand. ManipNet~\cite{zhang2021manipnet} introduces hand-object spatial representations to predict object manipulation trajectories given wrist trajectories by supervised learning for animations. Inspired by it, we propose to design more comprehensive hand-object interaction representations to empower reinforcement learning for dexterous grasping skills.

 % - dex  grasp 传统方法
 % - dex grasp learning 方法（learning的方法里面，要有一段体现下hand object representation)

