

\section{experiment results}

 % - 缺少和现有方法对比，想想怎么获得一个和现有方法的对比（我们的结果处在什么位置上）
 % - 还有我之前的问题，二爪用我们的是否有提升？寒假之前的那个问题忘了，应该很重要。
 % - Experiment settings
	%  - Simulation Environment
	%  - dataset(objects)
 % - Baseline
	%  - (different modules combination)
 % - Result
 % - Real experiment
	%  - setup
	%  - result
% 不同爪形 从5指拆 cuiyu
% hand2obj ,pointnet local  liuqingtao
% 视觉姿态评估 zhengnan
% We conducted a series of experiments to demonstrate the effectiveness of our proposed approach. We introduce the simulation and the object dataset in Section \ref{exp settings}. Next, we list a few baselines used to compare with our method in Section \ref{baselines}. And then we demonstrate that our method outperforms other baselines, and provide ablations to show that our method is applicable to various dexterous grippers, not just limited to the five-fingered robotic hand in Section \ref{results-in-simulation} and Section \ref{ablation}. Lastly, we evaluate our method in the real world in Section \ref{results-in-the-real-world}.
In this section, we conduct a series of experiments to answer the following questions: 

\begin{enumerate}
    \item Can the proposed method grasp diverse objects and generalize well to unseen objects?    
    \item Can our method be applicable to various dexterous hands and keep as good performance as the five-fingered robotic hand?    
    \item Does our proposed feature work well with depth sensors with noise in the real world?
\end{enumerate}

Before these experiments, we first introduce the simulation and the object dataset in Section \ref{exp settings} and then we list a few baselines used to compare with our method in Section \ref{baselines}.
% Table generated by Excel2LaTeX from sheet '测试数据'
\begin{table*}[htbp]
      \centering
   \tabcolsep=2.0pt
   \small
  \caption{Success rate (\%) on 10 unseen objects of GRAB compared with baselines}
  \begin{threeparttable}
    \begin{tabular}{l|cccccccccc|c}
    \toprule
    \toprule
    Methods & mug   & camera & binoculars & apple & toothpaste & fryingpan & toothbrush & elephant & hand  & pyramidsmall & average \\
    \midrule
    Hand2obj\tnote{*} & 31.67  & 32.00  & 30.33  & 33.33  & 18.67  & 25.33  & 2.67  & 32.00  & 30.67  & 1.33  & 23.80  \\
    pGlo\tnote{*}  & 31.67  & 49.00  & 30.33  & 9.67  & 36.00  & 47.33  & 8.00  & 35.67  & 69.67  & 3.67  & 32.10   \\
    Loc-Geo & 88.00  & 96.33  & 91.33  & 100.00  & 42.67  & 57.67  & 6.33  & 97.00  & 99.67  & 1.33  & 68.03    \\
    Surf  & 66.00  & 65.67  & 63.00  & 66.67  & 50.67  & 44.00  & 28.33  & 66.67  & 62.67  & 19.00  & 53.27   \\
    Occ+Surf & 99.67  & 84.00  & 78.00  & 96.33  & 46.67  & 37.00  & 1.67  & 98.33  & 84.33  & 6.33  & 63.23   \\
    Occ+Surf+pGlo & 62.00  & 95.67  & 62.00  & 84.33  & 59.67  & 54.33  & 43.33  & 84.33  & 95.00  & 1.67  & 64.23    \\
    DexRep+pGlo & 87.00  & 98.00  & 97.33  & 100.00  & 78.67  & 70.00  & 28.67  & 100.00  & 99.67  & 2.33  & 76.17   \\
    \textbf{Ours} & 100.00  & 99.00  & 97.33  & 100.00  & 94.00  & 81.33  & 64.00  & 100.00  & 100.00  & 3.33  & \textbf{83.90}   \\
    \bottomrule
    \bottomrule
    \end{tabular}%
    
     \begin{tablenotes}
        \footnotesize
        \item[*] The features used in Hand2obj are from  DAPG~\cite{DAPG:rajeswaran2017learning} and pGlo from ILAD~\cite{ILAD:wu2022learning}. 
      \end{tablenotes}
\end{threeparttable}
  \label{tab:success rate of 10 novel objects of GRAB}%
\end{table*}%

\begin{figure*}[ht]
    \begin{minipage}{0.52\linewidth}
\includegraphics[width=\linewidth]{fig/TrainingProcess.png}
  % \centerline{(a) RL training process}
    \caption{Mean reward and success rate of our method and baselines during RL training}  
    \label{train}
    \end{minipage}
\hfill
    \begin{minipage}{0.44\linewidth}
\includegraphics[width=\linewidth]{fig/TestResult.png}
  % \centerline{(a) Result 1}
      \caption{Qualitative results of our method and baselines on 50 objects(40 seen objects; 10 unseen objects) from GRAB and 30 unseen objects from 3DNet.}
      \label{test}
    \end{minipage}
    \vskip -0.5cm
\end{figure*}



\subsection{Experiment settings} \label{exp settings}

\textbf{Simulation environment.} We conduct simulations using the MuJoCo~\cite{todorov2012mujoco} environment, shown in Fig.\ref{fig:expinit}. At the start of each episode, the object's position is set to the origin of the world coordinate system, with a disturbance of $\left [ -0.05,0.05 \right ] $ in the x-axis direction and $\left [ -0.05,0 \right ] $ in the y-axis direction. Moreover, the object is randomly rotated around the z-axis within the range of $\left [ -\pi,\pi \right ] $. Its mass $m_0$ and friction $\mu$ are set to $0.5kg$ and $1$ respectively. We employ the 30-DoFs AdroitHand~\cite{kumar2013adroithand} model in our experiments and initialize it with the average pose of the first steps in all retargeted demonstrations, which ensures a natural pre-grasp pose and keeps the object within the effective range of the feature discussed in \secref{dexrep}. 

% Each episode length is 200-time steps, and at each step $t$, we obtain the state $s_t$ and input it to obtain $a_{t}\in \mathbb R^{30}$. 

\textbf{Dataset.} We generate retargeted demonstrations with the GRAB dataset. We use 40 object CAD models provided by GRAB~\cite{taheri2020grab} for training and the remaining 10 objects for evaluation. In addition, we select 30 extra objects from 3DNet~\cite{wohlkinger20123dnet} that are not seen during training to validate our method. Unseen objects are shown in Fig.\ref{fig:expinit}. 
% However, MuJoCo does not support collision detection for nonconvex objects like mugs. To address it, we decompose each nason-convex object into a union of convex geometries with the v-hacd library~\footnote{https://github.com/kmammou/v-hacd} so that the robotic hand can touch the inner wall of the mug or grasp it through the handle.

\textbf{Metrics.} We use the success rate of grasping as the evaluation metric for the quality of our method. In one episode, if an object is successfully grasped and held within $10cm$ of the target position for at least 50-time steps, we consider the grasp to be successful.

\textbf{Implementation details.} We conducted training for the grasping policy on a system equipped with Intel Xeon Gold 6326 processors and NVIDIA 3090 GPUs. During the Behavioral Cloning (BC) phase, we optimized all parameters using the Adam optimizer for 150 epochs, employing a minibatch size of 64 and a learning rate of $1e-5$. This process is typically completed in approximately 6 minutes.

In the subsequent Reinforcement Learning (RL) training phase, we used a dataset comprising 40 different objects to develop a versatile grasping policy, spanning about 500 iterations. During each iteration, we generated 10 grasping trajectories for each object, with each trajectory spanning 200 time steps (100 during evaluation). Training a generalizable grasping policy of this scale required approximately 35 GPU hours.







% In addition, in order to take advantage of the best effect of BC, we save BC models every 25 epochs and use each to initialize one grasping policy training. After 500 training iterations, we select the best one from the six trained policies as the final grasping policy.

\subsection{Baseline} \label{baselines}
To verify the effectiveness of our proposed DexRep, we construct variants of our method. The first two variants use the representation for dexterous grasping in prior works and the others ablate the proposed features. 
\begin{enumerate}
    \item \textbf{Hand2obj:} Similar to DAPG~\cite{DAPG:rajeswaran2017learning}, this baseline only uses the relative position to describe the relationship between the hand and the object.
    \item \textbf{pGlo:}  Similar to ILAD~\cite{ILAD:wu2022learning}, this baseline uses the global features extracted by the pre-trained PointNet model and the 6D pose to train the policy.
    \item \textbf{Loc-Geo:} This baseline replace the global features in \textbf{pGlo} with our proposed Loc-Geo Feature for $n$ closest points.    
    \item \textbf{Surf:} This baseline only utilizes only our proposed Surface Feature    
    \item \textbf{Occ+Surf} This baseline only add \textbf{Occ} to \textbf{Surf}.
    \item \textbf{Occ+Surf+pGlo} We combine Occupancy Feature and Surface Feature with the global feature extracted from PointNet.    
    \item \textbf{DexRep+pGlo:} We add the global feature extracted from PointNet to our proposed DexRep.
    \item \textbf{DexRep (ours):} Our full method utilizes all three features, \ie \textbf{Occ+Surf+Loc-Geo}.
\end{enumerate}

\subsection{Effectiveness of DexRep} 
\label{results-in-simulation}

We train dynamic grasping policy with 3 random seeds and sample 10 grasping trajectories for each training object in MuJoCo~\cite{todorov2012mujoco} at each iteration. Fig.\ref{train} shows the training process of our method and baselines. Our method converges fastest and reaches the highest reward and success rate. 

The results for the group experiments of \textbf{Surf},  \textbf{Occ+Surf}, \textbf{DexRep} verify the effectiveness of each proposed feature. From the basic Surface Feature, adding Occupancy Feature and Loc-Geo Feature one by one keeps increasing the reward and success rate significantly.


Finer global geometry features may harm the learning of grasping policy. 
Comparing our method with the baseline \textbf{DexRep+pGlo}, or \textbf{Occ+Surf} with \textbf{Occ+Surf+pGlo}, we can see from Fig. \ref{train} that 
adding global features of the object extracted by PointNet does not improve the learning but makes the convergence curves lower.
On the other hand, local geometric features boost the performance (both convergence speed and reward) of grasping dramatically from \textbf{pGlo} and \textbf{Occ+Surf}, indicating that compared with global features, finer or coarse, the high-quality local feature is the key feature for dexterous grasping. The main reason for the contrast in the performance is that local geometric features can be shared across many different objects. For example, for objects having similar handles with various body shapes, our proposed local features for handles are similar and therefore, the learned policy can grasp unseen objects via handles successfully.

% for example, many objects have local flat surfaces or curved surfaces, and these common features can enable the dynamic grasping policy to learn to grasp different types of objects, even those with significant shape variations. 

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{fig/Test result.png}
%     \caption{Qualitative results of our method and baselines on 50 objects(40 seen objects; 10 unseen objects) from GRAB and 30 unseen object from 3DNet.}
%     \label{fig:Test}
% \end{figure}

To validate the generalization ability of different methods to unseen objects, we evaluate the trained policy with 3 random seeds on 10 unseen objects from the GRAB dataset~\cite{taheri2020grab} and 30 unseen objects in 3DNet~\cite{wohlkinger20123dnet}. The result is shown in Fig. \ref{test}. Also, we provide detailed results of the success rates for each unseen object of GRAB in Table \ref{tab:success rate of 10 novel objects of GRAB}. Our method outperforms baselines and the success rate of our method is at least 50\% higher than the features used in prior works~\cite{DAPG:rajeswaran2017learning, ILAD:wu2022learning} after 500 training iterations, indicating that the policy trained by our method can better learn the commonalities among different objects and generalize to unseen objects.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{fig/CompareWithDexvip.png}
    \caption{Qualitative results of the comparison between different methods on novel objects from 3DNet}
    \label{fig:dexvip}
    \vskip -0.5cm
\end{figure}
In addition, we compare our method with state-of-the-art RL grasping methods DexVIP~\cite{mandikal2022dexvip} and GRAFF~\cite{GRAFF:mandikal2021learning}, which use 24 unseen objects from 3DNet~\cite{wohlkinger20123dnet} to test their success rate of grasping \footnote{The objects in DexVIP and GRAFF are not detailed in their papers. For our evaluation, we randomly sample objects from categories not in the GRAB training set from the Cat60 subclasses of 3DNet. }. Together with \textbf{pGlo} and \textbf{Hand2obj}, the compared results are shown in Fig.\ref{fig:dexvip}.


\subsection{Application to Multi-morphology Robotic Hands} \label{Multi-mophology robotic hands} 

% Table generated by Excel2LaTeX from sheet 'Sheet2'
\begin{table}[htbp]
 \centering
   \tabcolsep=11.8pt
   \small
  \caption{Success rate (\%) of policies on unseen objects with multi-finger robotic hands}
    \begin{tabular}{c|c|c|c}
    \toprule
    \toprule
    num of finger & Hand2obj & pGlo & \textbf{Ours} \\
    \midrule
    2     & 8.97  & 38.27 & \textbf{68.77} \\
    3     & 17.90  & 56.40  & \textbf{81.80} \\
    4     & 0.00     & 18.60  & \textbf{83.90} \\
    \bottomrule
    \bottomrule
    \end{tabular}%
  \label{tab:mulhand}%
\end{table}%



\begin{figure}[htbp]
    \centering
    \includegraphics[trim=0cm 0cm 0cm 0cm, clip, width=0.8\linewidth]{fig/different_morphogries1.png}
    \caption{Examples of grasping unseen objects with different robotic hands varying numbers of fingers.}
    \label{fig:diffrent morphogries}
    \vskip -0.5cm
\end{figure}
% \begin{figure}
%     \centering
%     \includegraphics{}
%     \caption{Caption}
%     \label{fig:my_label}
% \end{figure}

The experiment studies the application of \rep on robotic variant hands. We obtain multiple dexterous robotic hands with various configurations by disassembling the fingers of the five-fingered hand as \cite{Dissemble：radosavovic2021state-only} does, as shown in Fig.\ref{fig:diffrent morphogries}. We compare our method with \textbf{pGlo} and \textbf{Hand2obj}, whose results are shown in Table \ref{tab:mulhand}. In the task of robotic dexterous grasping, our proposed approach is applicable to multiple types of multi-joint robotic hands and maintains good grasping performance and generalization ability. 

% In addition, the result indicates that the higher the degree of freedom of the robotic hand, the stronger its grasping ability.



\subsection{Real experiment} \label{results-in-the-real-world}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/HardwareSystem.png}
    \caption{The grasping platform with an Allegro Hand v4,  a Unitree Z1 arm and an Azure Kinetic DK.}
    \label{fig:hardware system}
\end{figure}

\begin{figure}[htbp]
     \centering
     \includegraphics[width=0.48\textwidth]{fig/RealExpSuccRate.png}
     \caption{Results of our real-world experiment. }
     \label{fig:RealExp SuccRate}
     \vskip -0.5cm
 \end{figure}

To validate the effectiveness of the trained policy, we build up a grasp system consisting of an Allegro Hand~\footnote{https://www.wonikrobotics.com/research-robot-hand}, a Unitree Z1 arm~\footnote{https://www.unitree.com/arm/}, and an Azure Kinect DK~\footnote{https://azure.microsoft.com/en-us/products/kinect-dk}, which is shown in Fig.\ref{fig:hardware system}. In our validation system, we assume object CAD models are known. Therefore, only 6D poses of the objects are required to grasp,  which are obtained by registering the CAD models to the partial point clouds captured from the depth sensor of Azure Kinect DK. The objects are segmented out from green background from RGB images calibrated with the depth sensor of Azure Kinect DK and then the segmented depth images are back-projected back into 3D space to get partial point clouds. To mitigate the influence of the pose registration errors, we added Gaussian noise with variances of $2cm$ for position and $0.1rad$ for rotation to the object poses during the policy training. 

% We train the grasping policies with 3 random seeds and select the best one to be verified in the real world. 

% Due to the large size of the Allegro hand and its thick fingers, we do not use all the test objects mentioned in Section \ref{results-in-simulation}. Instead, we select 8 objects and scale up the models by a factor of 1.4. 

We evaluate our method and two baselines on 8 unseen objects in the real experiment. Each object was grasped 15 times to calculate its success rate. As shown in Fig.\ref{fig:RealExp SuccRate}, our method outperforms the baselines and demonstrates excellent generalization capability on unseen objects even in the real world.


% To detect the objects from the scene quickly and accurately, we paved the green screen for the scene. The process of obtaining the point cloud of objects is as follows: We first capture the RGB image and the corresponding point cloud of the scene with an Azure kinect DK from a single view. Then we segment the RGB image to get the area of the object, which is then mapped to the scene point cloud to get the point cloud of the object. 


% We assume the grasping objects' CAD models are known and only need to perceive the 6D poses of objects. Therefore, after we get the partial point cloud of the object, we match it with the CAD model to compute the 6D pose of the object. To mitigate the influence of the pose errors, we added Gaussian noise to the object poses during the policy training.

%We validate the effectiveness of the trained policy in Allegro Hand with a Unitree Z1 arm. The hardware system is shown in Fig.\ref{fig:hardware system}. We assume the grasping objects' CAD models are known and only need to perceive the 6D poses of objects. Therefore, We capture the point cloud of the object with an Azuro Kinetic DK from a single view and match the point cloud with the CAD model to compute its 6D pose. To mitigate the influence of the pose errors, we added Gaussian noise to the object poses during the policy training.

% In addition, since we do not have a robotic arm in the simulations, we may be unable to compute the inverse kinematics of the robotic arm, ensuring the end effect to get the desired pose. To address it, we use \textit{MoveIt} to refine the trajectories inferred by the trained policy with the tolerance of $0.005m$ for the position error and $0.1rad$ for the rotation error. And then we send the refined actions to the robotic arm.

% We train the grasping policies with 3 random seeds and select the best one to be verified in the real world. Due to the large size of the Allegro hand and its thick fingers, we do not use all the test objects mentioned in Section \ref{results-in-simulation}. Instead, we select 8 objects and scale up the models by a factor of 1.4. We evaluate our method and two baselines in reality. As shown in Fig.\ref{fig:RealExp SuccRate}, our method outperforms the baselines and demonstrates excellent generalization capability on unknown objects even in the real world.