\section{Method}

Fig.\ref{fig:overview} shows the pipeline of our method. We build our method upon DAPG~\cite{DAPG:rajeswaran2017learning}, which is a two-stage learning strategy of behavior cloning and DRL for a grasping policy. The state-action pairs for our policy learning are prepared by retargeting the human grasp trajectories to a dexterous robotic hand. Therefore, our method consists of three stages: retargeting, behavior cloning, and reinforcement learning, which is elaborated in \secref{dexrepnet}. Our proposed state representation of the hand-object interaction for the policy is detailed in \secref{dexrep}.

We model the problem of dexterous grasping as Markov Decision Process (MDP), which is defined by a tuple: $(\mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \gamma)$. $\mathcal{S}$ and $\mathcal{A}$ are the state and action space.  The policy $\pi_\theta: \mathcal{S} \rightarrow \mathcal{A}$ maps the state space to the action space. $\mathcal{T}:\mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$ is the transition dynamic. $\mathcal{R}:\mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R} $ is the reward function and $\gamma\in\left ( 0, 1 \right ] $ is the discount factor. 
We use the Adroit platform~\cite{kumar2013adroithand} as our manipulator which consists of a 24-Dof hand attached to a 6-Dof arm, thus the action space is the continuous motor command of 30 actuators.

\subsection{Grasping policy with DexRep} \seclabel{dexrep}
\begin{figure}[htbp]
     \centering
     \includegraphics[width=\linewidth]{fig/DexRep.png}
     \caption{\textbf{DexRep.} Representation for dexterous grasping, describing both geometric and spatial hand-object representation. DexRep consists of three components: 1) Occupancy Feature 2) Surface Feature and 3) Local-Geo Feature.
     }
     \label{fig:DexRep}
     \vskip -0.5cm
 \end{figure}

In this part, our proposed \rep for dexterous grasping considering both representation power and the generalization capability are first introduced, followed by the design of the policy $\pi_{\theta}$ with \rep.

\subsubsection{Occupancy Feature $\occ$}

Occupancy Feature represents the voxels occupancy states when a grid volume attached to a robotic hand approaches the object. It captures global shape information via a coarse occupancy volume instead of point clouds carrying detailed geometry for high-level generalization.  Specifically, the occupancy volume is a $20\times 20\times 20cm^3$ volume with each voxel of $2\times 2 \times 2 cm^3$. We fix the volume center at a point on the inside part of the palm by an offset from the root joint of the index finger (illustrated at the top left of Fig.\ref{fig:DexRep} ) as the inner palm space is our interest region for grasping not the space above the back of the hand. We set the offset perpendicular to the palm plane. The occupancy volume moves and rotates with the root joint. We define Occupancy Feature as a vector $\occ \in \mathbb{R}^{1000}$ and for the $i^{th}$ item, it denotes the corresponding $i^{th}$ voxel occupied or not:

\be
    f_o^{i}=\left\{\begin{matrix}
  1&,occupied \\
  0&,otherwise
\end{matrix}\right.
\ee

The feature describes the rough shape of an object close to the hand. It helps the hand approach objects without collision. By increasing the resolution of the volume, we can perceive finer object surfaces. However, it not only increases the computational burden but also reduces the network's generalization ability to different objects. 
% This is because, after the robotic hand approaches the object, global information has less impact, and overly detailed features can introduce a great deal of irrelevant information for policy learning. Conversely, coarse global features can avoid these issues.

\subsubsection{Surface Feature $\surf$ }
When the robotic hand is near the object, the surface area around the hand becomes important cues for grasping. Therefore, we propose to represent this area by Surface Feature $\surf \in \mathbb{R}^{4n}$,  where $n$ is the number of fingertips and joints of a hand.  It is defined as: 
\be
f_s^{j}=\left\{\min \left(\left\|{p}_{j}-{p}_{o}\right\|, \sigma _{\max }\right), {n}_{o}\right\},
\ee
where $j$ denotes the $j^{th}$ point on the hand, ${p}_{o}$ is the closest point on the object surface to the$j^{th}$ point ${p}_{j}$ and  $\left\|{p}_{j}-{p}_{o}\right\|$ represents the distance. The distance has a maximum value of $\sigma _{\max }=20cm$. ${n}_{o}$ is the surface normal of the closet point. Fig.\ref{fig:DexRep} (top right) illustrates the definition. Surface Feature contains two aspects: (\romannumeral1) the distances to inform the hand where to touch; (\romannumeral2) the normal vectors to inform the hand what will be touched. The distance features provide the robotic hand with the local information of the object region closest to it. The local information can help the hand decide how to take action to touch and grasp the object.


\subsubsection{Local-Geo Feature $\loc$} 

Though the surface normals of the closest points above depicted the geometric feature of the surface points, more abundant geometric local features like curvatures, thickness, symmetry \etc which are important for grasping are not captured. Learning-based descriptors have been demonstrated to be able to represent comprehensive features for the inputs. Therefore, we propose to extract local geometric features for these object points by PointNet~\cite{qi2017pointnet}.

The extraction consists of two steps: feature learning and feature extraction. In the feature learning step, an auto-encoder taking a point cloud $O\in \mathbb R^{N\times3}$ as input is constructed in Fig.\ref{fig:DexRep} (bottom) to recover the point cloud $O^{\prime}$. 
 We train the auto-encoder using 42k objects in 55 categories in ShapeNet55~\cite{chang2015shapenet}. In the feature extraction step, we only keep the encoder part to acquire local features $f_l\in \mathbb R^{n\times64}$ for the $n$ closest points.  The two-stage feature extraction is adopted instead of learning the encoder jointly with a grasping policy $\pi_\theta$ since DRL only provides sparse rewards which are hard to learn the features for diverse objects.

\begin{figure}[htbp]
     \centering
     \includegraphics[width=\linewidth]{fig/policy.png}
     \caption{\textbf{Policy network architecture.} The policy maps DexRep and robotic hand state to actions.}
     \label{fig:policy}
 \end{figure}

 \subsubsection{Grasping policy $\policy$}
 
 The policy network architecture is illustrated in Fig.\ref{fig:policy}. In addition to DexRep mentioned above, we add the state of the robotic hand, which is represented as $f_{h}$, to the input of the policy network. $f_{h}$ consists of the joint angles of the Adroit hand, the positions of the joints and fingertips relative to the wrist, and the global translation and rotation of the hand root.   

Before fed into the policy network, the robotic hand state and DexRep are processed respectively by a single-layer fully-connected encoder with an output size of $128$ and an input size of $105$, $1088$ and $1408$ for $f_{h}$, $\left\{f_o, f_s\right\}$ and $f_{l}$. The weight of the fully-connected layer is fixed after behavior cloning. The policy network is an MLP with hidden layers $\left[ 512, 128\right]$, and ReLU activations between each layer.
% The hand-object interaction features consist of the ambient sensor features $s^{am}$ and the signed distance sensor features , which describe the dynamic relationship between the hand and the object to be grasped.
\begin{figure*}[htbp]
    \centering
    \begin{minipage}{0.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{fig/exp_init1.png}
  % \centerline{(a) RL training process}
    % \caption{Mean reward and success rate of our method and baselines during RL training}  
    % \label{train}
    \end{minipage}
    \hfill
    \begin{minipage}{0.65\linewidth}
    \includegraphics[width=\linewidth]{fig/testobjs.png}
  % \centerline{(a) RL training process}
    % \caption{Mean reward and success rate of our method and baselines during RL training}  
    % \label{train}
    \end{minipage}
    % \includegraphics[trim=5cm 5cm 5cm 0cm, clip,width=0.5\linewidth]{fig/exp init.png}
    \caption{The left is MuJoCo simulation environment and initial pose of the hand; The right is 40 novel objects used to evaluate our method. The objects on the first row is from GRAB~\cite{taheri2020grab} and the rest is selected randomly from 3DNet~\cite{wohlkinger20123dnet}.}
    \label{fig:expinit}
    \vskip -0.5cm
\end{figure*}
 \subsection{Learning DexRepNet} 
 \seclabel{dexrepnet}
In this section, we illustrate DexRepNet, the full learning pipeline for dexterous grasping: retargeting human grasp demonstrations, behavior cloning for policy initialization, and reinforcement learning for policy fine-tuning.


 \subsubsection{Retargeting Human Grasp Trajectories} 
 \label{retarget}
 
We first introduce the process of generating robotic grasp demonstrations. The trajectories of human demonstrations are from GRAB~\cite{taheri2020grab}, a dataset that contains a large number of whole-body pose sequences of 10 subjects interacting with 51 daily objects. 
We first extract hand-object interaction sequences with the right MANO hand~\cite{MANO:SIGGRAPHASIA:2017}. Then, we preserve every sequence from the frame where the distance between the hand and the object is $10cm$ to the frame where the object is lifted off the table about $4cm$. Lastly, we move the object to the coordinate without any rotation and translate the hand accordingly. 
We define human grasp demonstrations $\mathcal{D}_{\text {Human }}=\left\{d_{0},\ldots, d_{i}, \ldots, d_{I}\right\}$, where the $i^{th}$ demonstration 
$d_{i}=\left \{ \left ( g_{t}, q_{t}, o_{t}\right )_{t=0}^{T}\right\}$ 
is a collection of human hand global pose $g_{t}$, joint pose $q_{t}$ and object 6D pose $o_{t}$ of the $t^{th}$ step. 


% $d_{i}=\left \{ \left ( g_{0}, q_{0}, o_{0}\right ),\left ( g_{1}, q_{1}, o_{1}\right ), \dots,\left ( g_{T}, q_{T}, o_{T}\right )\right \}$ 

Given human demonstrations, our goal is to map them to a robotic hand. Following DexPilot~\cite{handa2020dexpilot}, we formulate the retargeting objective as a nonlinear optimization problem, with a cost function that makes the posture of the robotic hand consistent with that of the MANO hand~\cite{MANO:SIGGRAPHASIA:2017}, as well as makes the distance between the hand and the object consistent, ensuring the accuracy of the grasping position. Therefore, we define the retargeting objective similar to the function in~\cite{handa2020dexpilot} as:
\be 
\min_{q_{t}^{\mathbf{R}},M_g}\sum_{i=0}^{N}\left\|\mathbf{v}_{\mathbf{i}}^{\mathbf{R}}\left({M}_g , q_{t}^{\mathbf{R}}\right)-k_i\mathbf{v}_{\mathbf{i}}^{\mathbf{H}}\left(q_{t}^{\mathbf{H}}\right)\right\|^{2},
\eqlabel{retarget}
\ee
where $\mathbf{v}_{\mathbf{i}}^{\mathbf{R}}$ and $\mathbf{v}_{\mathbf{i}}^{\mathbf{H}}$ computed by the joint angles $q_{t}^{\mathbf{R}}$ and $q_{t}^{\mathbf{H}}$ through forward kinematics, respectively represent three kinds of key vectors (finger-to-finger vectors, finger-to-wrist vectors, finger-to-object vectors) of the robotic hand and human hand in the $t^{th}$ step, $k_i$ is the scale ratio of each vector between Adroit Hand~\cite{kumar2013adroithand} and MANO Hand~\cite{MANO:SIGGRAPHASIA:2017}. $M_g$ is the global translation and rotation of the arm attached to the robotic Hand. We find that by including the arm in the optimization process, the position error caused by the structural difference between the human hand and the robotic hand can be reduced. After optimization, we convert the optimized joint angles to actions in MuJoCo~\cite{todorov2012mujoco} and perform correlated sampling~\cite{chen2022dextransfer} on actions in case of dropping the object when lifting without sufficient grasping force. Finally, we execute the refined action sequences in simulation in order to collect demonstrations $\mathcal{D}$ composed of state-action pairs $\left(s,a\right)$, which are for behavior cloning in the next stage.
% We collect all refined joint action sequence and generate demonstrations $\mathcal{D}=\left\{ d^0, d^1, \dots, d^{N\prime} \right\}$, where $d^i=\left\{ \left ( s^i_0, a^i_0 \right ),\left ( s^i_1, a^i_1 \right ),\dots, \left ( s^i_{T\prime}, a^i_{T\prime} \right ) \right\}$, $N^\prime$ is the number of demonstration trajectories, and $T^{\prime}$ is the length of the $i^{th}$ demonstration. $\left ( s^i_t, a^i_t \right )$ is the state-action pair for behavior cloning in the next stage. \yq{update all the symbols}


 \subsubsection{Pretraining with Behavior Cloning} 
 \seclabel{bc}

For high DoF robotic hands, the sample complexity is tremendous and RL from scratch may fail frequently. The idea of pretraining grasping policy with human demonstrations has worked successfully in prior works~\cite{DAPG:rajeswaran2017learning},~\cite{ILAD:wu2022learning}. Therefore, we follow the same strategy. 
With  $\mathcal{D}$, we pretrain the policy $\policy$ with the objective:
\begin{equation}
    L_{bc}=\frac{1}{|\mathcal{D}|}\sum _{(s,a)\in \mathcal{D}} \left\|\pi_{\theta }(s)-a\right\|^2
\end{equation}
where $|\mathcal{D}|$ is the number of state-action pairs in the dataset and $\theta$ is the parameter of the policy network to be optimized.  

 \subsubsection{RL fine tuning} 
 \seclabel{rl}

Though BC with large-scale human demonstrations can provide good policy initialization, the demonstration information is not fully used. DAPG~\cite{DAPG:rajeswaran2017learning} incorporates demonstrations into RL training to better assist the network in learning by adding an extra term to compute the gradient: 
\begin{equation}
    \begin{aligned}
    g_{\text {aug }}= & \sum_{(s, a) \in \rho_{\pi}} \nabla_{\theta} \ln \pi_{\theta}(a \mid s) A^{\pi_{\theta}}(s, a)+ \\
    & \sum_{(s, a) \in {D}} \nabla_{\theta} \ln \pi_{\theta}(a \mid s) \lambda_{0} \lambda_{1}^{k} \max _{(s, a) \in \rho_{\pi}} A^{\pi_{\theta}}(s, a),
    \end{aligned}
\end{equation}
where $\rho_{\pi}$ and $\mathcal{D}$ are the sampled dataset by the policy $\pi_{\theta}$ and the collected demonstrations respectively. $A^{\pi_{\theta}}$ is the advantage function, $\lambda_0=0.1$ and $\lambda_1=0.95$ are hyper-parameters, and $k$ is the iteration counter. In our experiments, we use GAE~\cite{GAE:schulman2015high} as the advantage function $A^{\pi_{\theta}}$ and set $\max _{(s, a) \in \rho_{\pi}} A^{\pi_{\theta}}(s, a)$=1.


For dexterous grasping, we design the reward function as follows:
\begin{equation}
    r=r_{d}+r_{g}+r_{t}.
\end{equation}

$r_{d}$ is the approaching reward, which incentives the hand to approach the object. In order to adapt more object geometries, we take advantage of the sum of the closet distance $d_{f2o}$ between all fingertips and the object surface, thus $r_{d}$ is defined as:
\begin{equation} \label{reward distance}
   r_{d}= \alpha \left ( \frac{1}{10\cdot d_{f2o}+0.25} -1  \right ) ,
\end{equation}
where $\alpha$ is the weight and set to 0.1.

$r_g$ is the grasping reward and is defined as:
\begin{equation} \label{reward grasping}
    r_g=\left\{\begin{matrix}
  1&,h>0.02 \\
  0&,0\le h\le 0.02
\end{matrix}\right.
\end{equation}
where $h$ is the lifting height of the object.

$r_t$ is designed to motivate the hand to move the lifting object to a target position. In our experiment, the target position $p_{tar}$ is $0.15m$ higher than the initial position $p_{obj}^0$. $r_t$ is defined as:
\begin{equation} \label{reward target}
    r_t=\left\{\begin{matrix}
  10&,\left \| p_{obj}^0-p_{tar} \right \|^2< 0.1   \\
  20&, \left \| p_{obj}^0-p_{tar} \right \|^2< 0.05 \\
  0&, otherwise
\end{matrix}\right.
\end{equation}
 
Eq.\ref{reward distance} guides the hand to touch the object, thereby facilitating the learning of a grasping policy. Eq.\ref{reward grasping} and Eq.\ref{reward target} guarantee that the learned policy can attain a stable grasping pose and reach the desired position. 




% During the RL training phase, the parameters of the feature extraction layer and the policy network are initialized by the BC model. Moreover, we freeze the parameters $\theta_f$ of the feature extraction layers and only update the parameters of the policy networks $\theta_p$ by utilizing Eq.\ref{RL gradient}, where the hyper-parameters are set as: $\lambda_0=0.1$ and $\lambda_1=0.95$. In our experiment, we use GAE to compute the advantage function $A^{\pi_\theta}$ and set $\max _{(s, a) \in \rho_\pi} A^{\pi_\theta}(s, a)=1$.