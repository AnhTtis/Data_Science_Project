\begin{abstract}
Robotic dexterous grasping is a challenging problem due to the high degree of freedom (DoF) and complex contacts of multi-fingered robotic hands. Existing deep reinforcement learning (DRL) based methods leverage human demonstrations to reduce sample complexity due to the high dimensional action space with dexterous grasping. However, less attention has been paid to hand-object interaction representations for high-level generalization. In this paper, we propose a novel geometric and spatial hand-object interaction representation, named \rep, to capture object surface features and the spatial relations between hands and objects during grasping.  \rep comprises Occupancy Feature for rough shapes within sensing range by moving hands, Surface Feature for changing hand-object surface distances, and Local-Geo Feature for local geometric surface features most related to potential contacts. 
Based on the new representation, we propose a dexterous deep reinforcement learning method \method to learn a generalizable grasping policy. 
Experimental results show that our method outperforms baselines using existing representations for robotic grasping dramatically both in grasp success rate and convergence speed. It achieves a 93\% grasping success rate on seen objects and higher than 80\% grasping success rates on diverse objects of unseen categories in both simulation and real-world experiments.
\end{abstract}

% Robotic dexterous grasping is a challenging problem due to the high degree of freedom (DoF) and complex contacts of multi-fingered robotic hands. Existing deep reinforcement learning (DRL) based methods leverage human demonstrations to reduce sample complexity due to the high dimensional action space with dexterous grasping. However, less attention has been paid to hand-object interaction representations for high-level generalization. In this paper, we propose a novel geometric and spatial hand-object interaction representation, named \rep, to capture dynamic object shape features and the spatial relations between hands and objects during grasping.  \rep comprises Occupancy Feature, Surface Feature, and Local-Geo Feature. 
% Based on the new representation, we propose a dexterous deep reinforcement learning method \method to train a generalizable grasping policy. 

% Experimental results show that our method outperforms baselines using existing representations for robotic grasping dramatically both in grasp success rate and convergence speed. It achieves 93\% grasping success rate on seen objects and higher than 80\% grasping success rates in both simulation and real-world experiments.



% Robotic dexterous grasping is a challenging problem due to the high degree of freedom of multi-fingered robotic hands. Deep reinforcement learning (DRL) has shown great potential in learning grasping skills with dexterous hands. However, less attention has been paid to hand-object interaction representations for high-level generalization. In this paper, we propose a novel compound geometric and spatial hand-object representation, named \rep, to capture the relative shape feature of the objects and the spatial relation between hands and objects. The \rep comprises Occupancy Feature, Surface Feature, and Local-Geo Feature. Based on the new representation, we propose a dexterous deep reinforcement learning method \method to train a generalizable grasping policy. Experimental results show that our method outperforms baselines, and it achieves good generalization to unseen objects in both simulation and the real world.