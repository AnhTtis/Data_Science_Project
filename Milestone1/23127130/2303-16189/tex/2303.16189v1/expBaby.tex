\vspace{-3pt}
\subsection{BabyAI}
\vspace{3pt}
\myparagraph{Setup} The BabyAI comprises an extensible suite of tasks in environments with different sizes and shapes, where the reward is given only when the task is successfully finished. We evaluate models in trajectory planning that requires the agent move to the goal object through turn left, right and move forward actions, and general instruction completion tasks in which the agent needs to take extra pickup, drop and open actions to complete the instruction. For more detailed experimental settings and parameters, please refer to Table \ref{tbl:babyai_env_setting} in Appendix. 

% In instruction completion tasks, the agent needs to perform the extra pickup and open actions on top of trajectory planning in PickUpLoc and GoToObjMazeS4R2Close. Besides, GoToSeqS5R2 task requires go to a sequence of objects in correct order, thus no one fixed goal location is available. The planning horizon $T$ and Gibbs sampling iteration $N$ are varied in different environment. Usually, the larger size environment requires longer horizon and correspondingly more sampling iterations. After $N$ iteration, all $T$ planned actions will be executed. For more detailed experimental settings and parameters, please refer to Table \ref{tbl:babyai_env_setting} in Appendix. 

\myparagraph{Baselines} 
We compare our approach with a set of different baselines: Behavior Cloning algorithm (BC); model free reinforcement learning (RL) algorithms Batch-Constrained deep Q-Learning (BCQ) \citep{fujimoto2019off}, and Implicit Q-Learning (IQL) \citep{kostrikov2021offline}; \hongyi{model based RL algorithm Model-based Offline Policy (MOPO) \citep{mopo}}; return-conditioning approach Decision Transformer (DT) \citep{chen2021dt}; \hongyi{model based Planning Transformer (PlaTe) \citep{plate}}. In our experiments, we use, as model inputs, the full observation of the environment, the instruction, the agent's current location and the goal object location (if available).


\myparagraph{Results}
Across all environments, \netName achieves highest success rate, with the margin magnified on larger, harder tasks, see Table~\ref{tbl:babyai_main}. In particular, \netName could solve the easy tasks like GoToLocalS8N7 with nearly 90\% success rate, and has huge advantages over baselines in the large maze worlds (GoToObjMazeS7) and complex tasks (GoToSeqS5R2) which require going to a sequence of objects in correct order. In contrast, baselines perform poorly solving these difficult tasks. 

Next, we visualize the underlying iterative planing and execution procedure in GoToLocalS8N7 and GoToSeqS5R2 tasks. On the left side of Figure~\ref{fig:babyai_plan_execute}, we present how the trajectory is optimized and its energy is minimized at single time step. Through iterative refinement, the final blue trajectory is closer to the optimal solution than the original red one, which follows the correct direction with higher efficiency and perform the actions like open the door in correct situation. On the right side, we present the entire task completion process through many time steps. \netName successfully plans a efficient trajectory to visit the two objects and opens the door when blocked. \hongyi{We also explore the model performance in the stochastic settings, please refer to Appendix \ref{appendix:stochastic}.}

\begin{table*}[h]
\centering
\small
\scalebox{0.8}{
\begin{tabular}{@{}ccccccccc@{}}
\toprule
\multicolumn{1}{c}{\bf Task} & \multicolumn{1}{c}{\bf Env} & \multicolumn{1}{c}{\bf BC} & \multicolumn{1}{c}{\bf BCQ} & \multicolumn{1}{c}{\bf IQL} & \multicolumn{1}{c}{\bf DT} & \multicolumn{1}{c}{\bf \hongyi{PlaTe}} & \multicolumn{1}{c}{\bf \hongyi{MOPO}}& \multicolumn{1}{c}{\bf \netName} \\
 \midrule
\multirow{4}*{\makecell[c]{Trajectory\\Planning}} 
&{GoToLocalS7N5} & 71.0\% & 71.5\% & 84.5\%&73.0\% & \hongyi{42.5\%} & \hongyi{87.0\%}& \bf{91.0\%} \\
&{GoToLocalS8N7} & 61.5\%&63.0\% & 71.5\%&63.5\%&\hongyi{45.0\%} & \hongyi{81.5\%}&\bf{95.0\%}\\
&{GoToObjMazeS4} & 24.0\%&23.0\%&52.5\%&46.5\% & \hongyi{35.5\%} & \hongyi{60.0\%}& \bf{62.5\%}\\
&{GoToObjMazeS7} & 18.0\%&10.5\% & 29.0\%&22.0\%&\hongyi{27.5\%} & \hongyi{30.5\%}&\bf{42.5\%}\\\midrule
\multirow{3}*{\makecell[c]{Instruction\\ Completion}} 
&{PickUpLoc} & 57.5\% & 58.5\%& 41.0\%&59.5\% & \hongyi{7.5\%} &\hongyi{43.5\%}& \bf{67.0\%}\\
&{GoToSeqS5R2} & 13.5\% & 10.0\%& 28.5\%&26.5\% & \hongyi{25.0\%}&\hongyi{30.0\%}& \bf{33.0\%} \\
&{GoToObjMazeS4R2Close} & 24.0\%&22.5\%&31.5\% &48.5\% & \hongyi{32.5\%}& \hongyi{36.0\%}& \bf{55.0\%}\\
\bottomrule
\end{tabular}
}
\caption{\small \textbf{BabyAI Quantitative Performance.} The task success rate of \netName and a variety of prior algorithms on BabyAI env. Models are trained with 500 optimal trajectory demos in each environment, and results are averaged over 5 random seeds. The SX and NY in environment name means its size and the number of obstacles.}
\label{tbl:babyai_main}
\vspace{-10pt}
\end{table*}





\myparagraph{Effect of Iterative Refinement.} 
We investigate the effect of iterative refinement by testing the  success  rate of our approach under different sample iteration number in GoToLocalS7N5 environment. 
From the left side of Figure~\ref{fig:anlsBaby}, the task success rate continues to
improve as we increase the number of sample iteration.  
% In detail, iterative refinement helps more substantially when going from 1 iteration to 5 iterations, and the benefit gradually decreases as the iteration number goes from 20 to 50. 



\myparagraph{Energy Landscape Verification.}
We further verify our approach by visualizing the energy   
 assignment on various trajectories in the same environment as above. 
More specifically, we compare the estimated energy of the labeled optimal trajectory with the noisiness of trajectories,
produced by randomizing a percentage of steps in the optimal action sequence.
The right Figure~\ref{fig:anlsBaby} depicts the energy assignment to trajectories  with various corruption levels as a function of training  time.
With the progress of training, \netName learns to
(a) reduce the energy value assigned to the optimal trajectory;
(b) increase the energy value assigned to the corrupted trajectories.
\input{figs/suboptimal.tex} 
This result justifies the performance of \netName. 



\myparagraph{Effect of Training Data.} In BabyAI, we utilize a set of demonstrations generated  using an oracle planner. We further investigate the performance of \netName when the training data is not optimal. To achieve it, we randomly swap the decisions in the optimal demonstration with an arbitrary action with the probability of $25\%$. We compare against DT, the autoregressive sequential planner. Despite a small performance drop in Table~\ref{tbl:suboptimal_data}, \netName still substantially outperforms DT, indicating \netName works well with non-perfect data.



