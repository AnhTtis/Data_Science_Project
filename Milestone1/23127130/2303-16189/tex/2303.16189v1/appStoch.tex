\section{\ycReb{Stochastic Environment Testing}}
\label{appendix:stochastic}
\ycReb{
In this section we demonstrate the possibility of extending our method into stochastic settings.
Although \cite{dtStoch} reveals that planning by sampling from the learnt policy conditioned on desired reward could lead to suboptimal outcome due to the existence of stochastic factors,
our model circumvents the problem by formulating the planning as an optimization problem - we use the Gibbs sampling method to find the trajectory with the lowest energy evaluated by the trained model.
Assuming that the frequency of successful actions dominates in the dataset, our model is trained to assign lower energy to trajectories with higher likelihood of reaching the goal.
Consequently, in the stochastic environments, \netName constructs a sequence of actions that has the best opportunity to accomplish the target.
When executing this plan in a stochastic environment, we may also choose to replan our sequence of actions after each actual action in the environment (to deal with stochasticity of the next state given an action). Note then that this sequence of actions will be optimal in the stochastic environment, as we always choose the action that has the maximum likelihood of reaching the final state. 
Also note that multi-step planning can potentially provide advantage over a simple policy to predict the next action in stochastic environments, as such policy simply assigns probability distribution to the immediate next step without the awareness of future step adjustments facing stochastic factors.
}

\ycReb{
To verify the assumptions, we constructed a stochastic testing in BabyAI environment.
The test is created by adopting a stochastic dynamic model, where the agent fails to execute the turning actions \textit{turn left/right} with $20\%$ chance, and instead performs the remaining actions, including \textit{turn right/left}, \textit{forward}, \textit{pickup}, \textit{drop}, and \textit{open}, with uniform probability.
The remaining settings follow BabyAI experiments detailed in Appendix. \ref{app:ExpDetails}, except that we train models using demonstrations generated with the above dynamic model.
Those training data are noisy in the sense that the actions taken are not optimal, and corrections are required from future actions.
We believe \netName, as a multi-step planner, can learn the above correlations between the consecutive actions. 
We compare \netName with the baseline DT, the results of which is collected in the Table. \ref{tbl:stoch_babyai}.
It can be observed that \netName has a superior performance compared to DT on both tested environments,
which indicates both the possibility of applying our approach in the stochastic settings, and the advantage of multi-step planning when facing stochastic factors. 
}


\begin{table*}[h!]
    \centering\color{black}
    \small
        \setlength\tabcolsep{20pt} 
        \scalebox{1.0}{
            \begin{tabular}{ccc}
            \toprule
            \multicolumn{1}{c}{\bf Env} & \multicolumn{1}{c}{\bf \netName} &  \multicolumn{1}{c}{\bf DT}\\
            \midrule
            GoToObjMazeS4 & \textbf{57.5\%} & 30.8\%   \\ 
            GoToObjMazeS7   &  \textbf{33.3\%} & 28.3\%
            \\
            \bottomrule
            \end{tabular}
        }
        \captionsetup{labelfont={color=black},font={color=black}}
        \caption{Comparison of \netName and DT on stochastic settings}
        \label{tbl:stoch_babyai}
   
    \vspace{-0pt}
\end{table*}



