% \vspace{-3pt}
\section{Experiments}
\vspace{-3pt}

In this section, we evaluate the planning performance of \netName in BabyAI and Atari environments. We compare with a variety of different offline reinforcement learning approaches, and summarize the main results in Figure~\ref{fig:main_results}.
% Since we do not explicitly learn the dynamics model, we primarily compare against model-free algorithms in our work; in particular, adding a dynamics model tends to improve the performance of model-free algorithms. Our main results are summarized in Table~\ref{tbl:BabyAI_main} and Table~\ref{tbl:atari_main}.

\begin{figure*}[t]
\centering
\includegraphics[width=0.6\textwidth]{figs/result_summary.pdf}
\vspace{-10pt}
\caption{\small
\textbf{Quantitative Results of \netName of Different Domains.} Results comparing \netName to Decision Transformer and TD learning (IQL in BabyAI and Generalization tests and CQL in Atari) across BabyAI, Atari, and Generalization Tests. On a diverse set of tasks, \netName performs better than prior approaches.} 
\label{fig:main_results}
\vspace{-10pt}
\end{figure*}



