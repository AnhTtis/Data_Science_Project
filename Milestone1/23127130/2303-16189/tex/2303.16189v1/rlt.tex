\vspace{-5pt}
\section{Related Work}
\vspace{3pt}
%Transformer architectures with self-attention \citep{vaswani2017attention}
%which is formalized as sequential Markov decision processes,
%They demonstrate that supervised learning alone, without temporal difference (TD) learning, can be remarkably effective for offline RL, as the effectiveness of these models is determined by the representational capacity of the sequence model rather than algorithmic sophistication 
\myparagraph{\hongyi{Sequence} Models and Reinforcement Learning.} Sequence modeling with deep networks, from sequence-to-sequence models \citep{LSTM1997,sutskever2014sequence} to BERT \citep{devlin-etal-2019-bert} and XLnet \citep{NEURIPS2019_dc6a7e65}, have shown promising results in a series of language modeling problems \citep{dai2019transformer,sutskever2014sequence,liu2019text, dehghani2018universal}. With these advances, people start applying sequence models to represent components in standard RL such as policies, value functions, and models to improved performance \citep{rl2018lstm,RL2020Transformer,kapturowski2018RLrecurrent}. While the sequence models provide memory information to make the agent predictions temporally and spatially coherent, they still rely on standard RL algorithm to fit value functions or compute policy gradients. Furthermore, recent works replace as much of the RL pipeline as possible with sequence modeling to leverage its scalability, flexible representations and causally reasoning \citep{janner2021tt, chen2021dt,furuta2021generalized, zheng2022online,emmons2021rvs,li2022pre}. However, those methods adopt autoregressive modeling objectives and the predicted trajectory sequences have no easy way to be optimized, which will inevitably lower the long-horizon accuracy. \hongyi{Recent studies point out that using sequence models \citep{chen2021dt, emmons2021rvs} rather than typical value-based approaches have difficulty converging in stochastic environments \citep{paster2020planning, dtStoch}.}


% A number of methods have been used for inference and sampling in EBMs, from Gibbs Sampling \citep{hinton2006fast}, to Langevin Dynamics \citep{du2019implicit}, and Metropolis Hastings \citep{goyal2021exposing}. 


\myparagraph{Planning in Reinforcement Learning.} Planning has been explored extensively in model-based RL, which learns how the environment respond to actions \citep{sutton1991dyna}. The learned world dynamic model is exploited to predict the conditional distribution over the immediate next state or autoregressively reason the long-term future \citep{chiappa2017recurrent_env,ke2018modeling}. However, due to error compounding, plans generated by this procedure often look more like adversarial examples than optimal trajectories when the planning horizon is extended \citep{bengio2015scheduled, talvitie2014model, asadi2018lipschitz}. To avoid the aforementioned issues, simple gradient-free method like Monte Carlo tree search \citep{10.1007/978-3-540-75538-8_7}, random shooting \citep{nagabandi2018neural} and \hongyi{beam search \citep{plate}} are explored. Another line of works studied how to break the barrier between model learning and planning, and plan with an imperfect model, include training an autoregressive latent-space model to predict values for abstract future states \citep{tamar2016value, oh2017value, schrittwieser2020mastering, plate}; energy-based models of policies for model-free reinforcement learning \citep{haarnoja2017reinforcement}; \hongyi{improve the offline policies by planning with learned models for model-based reinforcement learning \citep{mopo, muzeroUnp}}; directly applying collocation techniques for direct trajectory optimization \citep{erez2012trajectory, du2019model}; and folding planning into the generative modeling process \citep{janner2021tt}. In contrast to these works, we explore having planning directly integrated in a language modeling framework.

\myparagraph{Energy Based Learning.} Energy-Based Models (EBMs) capture dependencies between variables by associating a scalar energy to each configuration of the variables, and provide a unified theoretical framework for many probabilistic and
non-probabilistic approaches to learning \citep{lecun2006tutorial}. Prior works have explored EBMs for policy training in model-free RL \citep{haarnoja2017reinforcement}, modeling the environment dynamics in model-based RL \citep{du2019model, janner2022planning} and natural images \citep{du2019implicit, dai2019exponential}, as well as energy values over text \citep{goyal2021exposing}. Most similar to our work, \citet{du2019model} illustrates how energy optimization in EBMs naturally support planning given start and goal state distributions. However, the 
 underlying training relies on a constrastive divergence, which is difficult to train \citep{du2020improved}. 
 On the other hand, our training approach relies on a more stable masked language modeling objective.
