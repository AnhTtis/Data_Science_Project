\section{Introduction}
\input{figs/figTeaser}

Sequence modeling has emerged as unified paradigm to study numerous domains such as language \citep{brown2020gpt3,radford2018improving} and vision \citep{yu2022scaling,dosovitskiy2020image}. Recently,  \citep{chen2021dt,janner2021tt} have shown how a similar approach can be effectively applied to decision making, by predicting the next action to take. However, in many decision making domains, it is sub-optimal to simply predict the next action to execute -- as such an action may be only locally optimal and lead to global dead-end. Instead, it is more desirable to plan a sequence of actions towards a final goal, and choose the action most optimal for the final overall goal. 

Unlike greedily picking  the next action to execute, effectively constructing an action sequence towards a given goal requires a careful, iterative procedure, where we need to assess and refine intermediate actions in a plan to ensure we reach the final goal. To refine an action at a particular timestep in a plan, we must reconsider both actions both before and after the chosen action. Directly applying this procedure to standard language generation is difficult, as the standard autoregressive decoding procedure prevents regeneration of previous actions based of future ones. For example, if the first five predicted actions places an agent at a location too far to reach a given goal, there is no manner we may change the early portions of plan.

In this paper, we propose an approach to iteratively generate plans using \hongyi{sequence} models. Our approach, Mu\textbf{l}tistep \textbf{E}nergy-Minimiz\textbf{a}tion \textbf{P}lanner (\netName), formulates planning as an iterative optimization procedure on an energy function over trajectories defined implicitly by a \hongyi{sequence} model (illustrated in Figure~\ref{fig:teaser}). To define an energy function across trajectories, we train a bidirectional \hongyi{sequence} model using a masked-language modeling (MLM) objective \citep{devlin-etal-2019-bert}. We define the energy of a trajectory as the negative pseudo-likelihood (PLL) of this MLM \citep{salazar2019masked} and sequentially minimize this energy value by replacing actions at different timepoints in the trajectory with the marginal estimates given by the MLM. Since our MLM is bi-directional in nature, the choice of new action at a given time-step is generated based on both future and past actions. 


By iteratively generating actions through planning, we illustrate how our proposed framework outperforms prior methods in both BabyAI \citep{babyai_iclr19} and  Atari \citep{bellemare2013arcade} tasks. Furthermore, by formulating the action generation process as an iterative energy minimization procedure, we illustrate how this enables us to generalize to environments with new sets of test-time constraints as well as more complex planning problems. Finally, we demonstrate how such an energy minimization procedure enables us to compose planning procedures in different models together, enabling the construction of plan which achieves multiple objectives.

Concretely, in this paper, we contribute the following: First, we introduce \netName, a framework through which we may iteratively plan with \hongyi{sequence} models. Second, we illustrate how such a planning framework can be beneficial on both BabyAI and Atari domains. Finally, we illustrate how iteratively planning through energy minimization gives a set of unique properties, enabling better test time performance on more complex environments and environments with new test-time obstacles, and the ability to compose multiple learned models together, to jointly generate plans that satisfy multiple sets of goals.


% During online testing, we use Gibbs sampling to draw action tokens with low energy value conditioning on the sampled trajectory from last iteration. The planned trajectory will be optimized iteratively through energy minimization which has better long-horizon accuracy and temporal consistency, see Figure~\ref{fig:teaser}. 

 % \yuhang{Definitely list your contributions. }

% With the recent advances in sequence modeling approaches, many works start combining the them with reinforcement learning (RL) algorithm to process long time horizons of information and scale to massive amounts of data \citep{goyal2021exposing, RL2020Transformer}. Sequence models are used to either represent components such as policies, value functions in RL or replace the entire RL pipeline \citep{rl2018lstm,RL2020Transformer,janner2021tt, chen2021dt}. However, those methods still rely on autoregressive planning, which prevents the further optimization of the predicted trajectory sequences and the error will accumulate inevitably. As a result, a number of recent works have studied how to improve the online planning in different ways. Approaches including using simple gradient-free method like random shooting \citep{nagabandi2018neural} or beam search \citep{janner2021tt}; training Diffuser to generate all timesteps of a trajectory concurrently but limited to continuous action space \citep{janner2022diffuser}; interpreting masked language models (MLMs) as energy-based models to correctly draw samples for text generation \citep{goyal2021exposing}.\yuhang{Mention limitations of former works. However, ...}


% In this work, we train an implicit energy-based sequence models by optimizing MLM objectives, which is inspired by the success of the recent work on using MLMs for Machine Translation \citep{goyal2021exposing}. Instead of training an approximate dynamics model and then plugging that into trajectory optimization, our language model incorporate the environment into the model itself thanks to the strong representation ability of recent sequence models \citep{devlin-etal-2019-bert, NEURIPS2019_dc6a7e65}. Moreover, different from \citep{janner2022diffuser} that predict the states and actions in a trajectory jointly in low-dimensional settings, we focus on predicting the actions as high-dimensional image-like data is involved in states. 

% During online testing, we use Gibbs sampling to draw action tokens with low energy value conditioning on the sampled trajectory from last iteration. The planned trajectory will be optimized iteratively through energy minimization which has better long-horizon accuracy and temporal consistency, see Figure~\ref{fig:teaser}. We benchmark our proposed framework with prior offline RL algorithms in static Babyai \citep{babyai_iclr19} and dynamic Atari \citep{bellemare2013arcade} control tasks. Furthermore, we analyze its unique or useful properties including online adaptation, task compositionality and strong generalization ability which are introduced by the iterative multi-step refining process. \yuhang{Definitely list your contributions. }

% In this, our contributions are to

