\vspace{-5pt}
\section{Properties of Multistep Energy-Minimization Planner} 
\vspace{-3pt}
\label{sec:prop}

In \netName, we formulate planning as an optimization procedure $\argmin_{\btau{}} E_\theta(\btau{})$. 
By formulating planning in such a manner, we illustrate how
\hongyi{our approach enables} flexible online adaptation to new test-time constraints, generalization to harder planning problems, and plan composition to achieve multiple set of goals (illustrated in Figure~\ref{fig:property_method}).

\input{figs/figExps}

\myparagraph{Online adaptation.} In \netName, recall that plans are generated by minimizing an energy function $E_{\theta}(\btau{})$ across trajectories. At test time, if we have new external constraints, we maybe correspondingly define a new energy function $E_{\text{constraint}}(\btau{})$ to encode these constraints. For instance, if a state becomes dangerous at test time (illustrated as a red grid in Figure~\ref{fig:property_method} (a)) -- we may directly define an energy function which assigns 0 energy to plans which do not utilize this state and high energy to plans which utilize such a state. We may then generate plans which satisfies this constraint by simply minimizing the summed energy function 
% \vspace{-3pt}
\begin{equation}
   \bm{\tau}^{*} = \argmin_{\btau{}} (E_\theta(\btau{}) + E_{\text{constraint}}(\btau{})).
\end{equation}
and then utilize Gibb's sampling to obtain a plan $\bm{\tau}^{*}$ from $E_\theta(\btau{}) + E_{\text{constraint}}(\btau{})$. \hongyi{While online adaptation may also be integrated with other sampling based planners using rejection sampling,   our approach directly integrates trajectory generation with online constraints.}

% such as a new obstacle (illustrated in Figure ~\ref{fig:property})

% iterative planning can be constrained by external perturbation (or constrain) functions $p(\btau{})$ shown in Equation~\ref{eq:perturbed}, which can be learned from another data source or manually designed by human experts. Applying the perturbation to autoregressive models is inevitably myopic since it affects the action distribution step-by-step. While in the multi-step iterative planning procedure, $p(\btau{})$ could guide the prediction of all action tokens simultaneously to better achieve the long-term objective. {\color{red}Cite subfigure (a)..., }, $p(\btau{})$ put a hard constraint to avoid running into the unrecoverable dangerous zone (red grid) and iterative planning would better fuse it with the our objective, that is reaching the goal.

% \begin{equation}
% \label{eq:perturbed}
% \tilde{h}(\btau{}_{\backslash t};\theta) \propto h(\btau{}_{\backslash t};\theta) p(\btau{}).
% \end{equation}

\myparagraph{Novel Environment Generalization.} In \netName, we leverage many steps of sampling to recover an optimal trajectory $\bm{\tau}^{*}$ which minimizes our learned trajectory energy function $E_{\theta}(\btau{})$. In settings at test time when the environment is  more complex than those seen at training time (illustrated in Figure~\ref{fig:property_method} (b)) -- the underlying energy function necessary to compute plan feasibility may remain simple (i.e. measure if actions enter obstacles), but the underlying planning problem becomes much more difficult. In these settings, as long as the learned function $E_{\theta}(\btau{})$ generalizes, and we may simply leverage more steps of sampling to recover a successful plan in this more complex environment.

% Thus, in setting in which the, by utilizing longer number of steps of sampling / optimization, our approach energy

% Because of the iterative planning process, the model can keep refining the actions conditioned on the surrounding context from previous iteration, which make it more universal and robust in unseen environments. {\color{red}Cite subfigure (b)..., }, the agent is tested in unstructured environment while trained in easier environment. {\color{red} We have no clue why it would perform better than others, please add your explanation if you can.} Thus we expect the model would perform relatively good even in unseen or more challenging environment compared to prior RL algorithms.  

\myparagraph{Task compositionality.} Given two different instances of \netName, $E_\theta^1(\btau{})$ and $E_\theta^2(\btau{})$, encoding separate tasks for planning, we may generate a trajectory which accomplishes the tasks encoded by both models by simply minimizing a composed energy function \hongyi{(assuming task independence)} 
% \vspace{-3pt}
\begin{equation}
   \bm{\tau}^{*} = \argmin_{\btau{}} (E_\theta^1(\btau{}) + E_\theta^2(\btau{})).
\end{equation}
An simple instance of such a setting is illustrated in Figure~\ref{fig:property_method} (c), where the first \netName model $E_\theta^1(\btau{})$ encodes two obstacles in an environment, and a second \netName model  $E_\theta^2(\btau{})$ encodes four other obstacles. By jointly optimizing both energy functions (through Gibbs sampling), we may successfully construct a plan which avoids all obstacles in both models.

