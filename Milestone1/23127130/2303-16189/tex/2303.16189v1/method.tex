\vspace{-8pt}
\section{Method}
\vspace{-5pt}
\label{method}
In this section, we describe our framework, Mu\textbf{l}tistep \textbf{E}nergy-Minimiz\textbf{a}tion \textbf{P}lanner (\netName), which formulates planning as a energy minimization procedure. Given a set of trajectories in a discrete action space, with each trajectory containing state and action sequences $(\bs_1, \ba_1, \bs_2, \ba_2, \ldots, \bs_N, \ba_N)$, our goal is to learn a planning model which can predict a sequence of actions $\ba_{1:\Hrz}$, given the \hongyi{trajectory context $\btau{}_{ctx}$ containing the past $K$ steps states and actions}, that maximizes the long-term task-dependent objective $\mathcal{J}$:
\vspace{-3pt}
\[
\ba_{1:\Hrz}^* = \argmax_{\ba_{1:\Hrz}} \mathcal{J}(\hongyi{\btau{}_{ctx}}, \ba_{1:\Hrz})
\]
where \hongyi{$N$ denotes the length of the entire trajectory and} $\Hrz$ is the planning horizon. We use the abbreviation $\mathcal{J}(\btau{})$, \hongyi{where $\btau{}=:(\btau{}_{ctx}, \ba_{1:\Hrz})$}, to denote the objective value of that trajectory. To formulate this planning procedure, we learn an energy function $E_\theta(\btau{})$, which maps each trajectory $\btau{}$ to a scalar valued energy so that
\vspace{-3pt}
\[
\ba_{1:\Hrz}^* = \argmin_{\ba_{1:\Hrz}} E_\theta(\hongyi{\btau{}_{ctx}}, \ba_{1:\Hrz}).
\]
%factorized over per-timestep rewards $r(\st, \at)$:   = \argmax_{\ba_{1:T}} \sum_{t=1}^{T} r(\st, \at)

\vspace{-10pt}
\subsection{Learning Trajectory Level Energy Functions}

We wish to construct an energy function $E_\theta(\hongyi{\btau{}_{ctx}}, \ba_{1:\Hrz})$ such that minimal energy is assigned to optimal set actions $\ba_{1:\Hrz}^*$. To train our energy function, we assume access to dataset of $\DataNum$ near optimal set of demonstrations in the environment, and train our energy function to have low energy across demonstrations. Below, we introduce masked language models, and then discuss how we may get such a desired energy function from masked language modeling.

\myparagraph{Masked Language Models.} Given a trajectory of the form $(\bs_1, \ba_1, \bs_2, \ba_2, \ldots, \bs_n, \ba_n)$, we train a transformer language model to model the marginal likelihood $p_\theta(\ba_t | \hongyi{\btau{}_{ctx}}, \ba_{-t})$, where we utilize $\ba_{-t}$ as shorthand for actions \hongyi{$\ba_{1:\Hrz}$} except the action at timestep $t$. To train this masked language model (MLM), we utilize the standard BERT training objective \citep{devlin-etal-2019-bert}, where we minimize the loss function
\begin{equation}
    \mathcal{L}_{\text{MLM}} = \expect{\btau{}, t}{-\log p_\theta(\ba_t;\hongyi{\btau{}_{ctx}}, \ba_{-t})},
\end{equation}
where we mask out and predict the marginal likelihood of percentage of the actions in a trajectory (details on masking in the \sect{appendix:experiment}). 
% Note that due to the above training objective, $p_\theta$ learns to predict actions at a given timestep use the \textit{bidrectional context} of actions across all other timesteps.
.

\myparagraph{Constructing Trajectory Level Energy Functions.} Given a trained MLM, we define an energy function for a trajectory as the sum of negative  marginal likelihood of each action in a sequence
\begin{equation}
    E_\theta(\btau{}) = -\sum_t \log p_\theta(\ba_t;\hongyi{\btau{}_{ctx}}, \ba_{-t}).
    \label{eqn:energy}
\end{equation}
Such an energy function, also known as the pseudolikelihood of the MLM, has been used extensively in prior work in NLP \citep{goyal2021exposing, salazar-etal-2020-masked}, and has been demonstrated to effectively score the quality natural language text (outperforming direct autoregressive scoring) \citep{salazar-etal-2020-masked}. In our planning context, this translates to effectively assigning low energy to optimal planned actions, which is our desired goal for $E_\theta(\btau{})$. We illustrate the energy computation process in Figure~\ref{fig:method}.


% We utilize the implicit energy
% \myparagraph{Modeling Trajectory Level Energy Functions} Since generating high-dimensional samples such as images is challenging and requires long mixing times \citep{hinton2006fast}, we instead sample the energy model in low-dimentsional action space. In detail, for action $\at$ at $t$-th token in trajectory $\btau{}$, energy model computes $h(\btau{}_{\backslash t};\theta)$ by running a forward pass conditioned on the surrounding context $\btau{}_{\backslash t}$. 
% The energy value at $t$-th token $E_\theta(\btau{}_{t};\theta)$ is defined as $\log f(\at, h(\btau{}_{\backslash t}; \theta))$,
% where $f(\cdot)$ is computed by applying the softmax operation to the raw scores, and selecting the normalized score of $\at$. Then the energy computation for a entire trajectory with length $T$ is summing up the energy value at the each masked token: $E_\theta(\btau{};\theta) = -\sum_{t=1}^{T} E_\theta(\btau{}_{t};\theta)$. 
% To reduce the computational cost, we perform the forward pass on $\btau{}$ once to estimate the energy values of all tokens, instead of masking the tokens and calculate its energy one-by-one \citep{goyal2021exposing}. Moreover, we assume the given trajectories for training are optimal, which can achieve highest objective value $\mathcal{J}(\btau{})$. We train the energy model to assign the lowest energy value to the optimal trajectory. 
% As a result, planning to maximize the long-term objective function $\mathcal{J}(\btau{})$ is equivalent to minimize the energy of the entire trajectory $E_\theta(\btau{};\theta)$.
% \[
% \ba_{1:T}^* = \argmax_{\ba_{1:T}} \mathcal{J}(\bs_1, \ba_{1:T}) = \argmin_{\ba_{1:T}} E_\theta(\btau{};\theta)
% \]


\input{figs/figMethod}




%\lVert \epsilon - \epsilon_\theta(\btau{i}, i) \rVert^2
%\myparagraph{Infering Plans from Energy Functions}   
%In fact, our experiments demonstrate that performing Gibbs sampling using these masked conditionals leads to low-quality samples. However, these conditionals have been shown to be useful for scoring individual sequences and non-autoregressive generation. 
\vspace{-3pt}
\subsection{Planning as Energy Minimization}
\vspace{-3pt}

Given a learned energy function $E_\theta(\btau{})$ defined through Equation~\ref{eqn:energy}, which assigns low energy to optimal trajectories \hongyi{$\ba_{1:\Hrz}^*$}, we wish to plan a sequence of actions at test-time that minimizes our energy function. To implement this planning procedure, we utilize Gibbs sampling across actions in the trajectory to iteratively refine and construct a trajectory $\btau{}$ that minimizes $E_\theta(\btau{})$.

In particular, for test-time plan generation, we initialize a masked trajectory of length $\Hrz$ with a small context length of past states and actions, which we illustrate in Equation~\ref{eq:traj_with_pad_actions}. At each step of Gibb's sampling, we randomly mask out one or multiple action tokens and use the MLM to estimate energy distribution across actions at each masked token. 
Then, action $\at$ and each token position is sampled using Gibb's sampling based on the locally normalized energy score of each token $\at \sim p_\theta(\ba_t;\hongyi{\btau{}_{ctx}}, \ba_{-t})$. 
The process is illustrated in Figure~\ref{fig:method}, where the actions with low energy values (in blue) are sampled to minimize $E_\theta(\btau{i};\theta)$ in each iteration. 
To sample effectively from the underlying energy distribution, we repeat this procedure for multiple timesteps, which we illustrate in  Algorithm~\ref{algo:iterative} which aims to minimize the overall energy expression in Equation~\ref{eqn:energy}.
%
\begin{equation}
\btau{} = 
\begin{bmatrix}
\bs_1 & \bs_2 & \dots & \bs_{n-1} & \bs_n & \bs_n & \dots & \bs_n\\
\undermat{\text{context}}{\ba_1 & \ba_2 & \dots & \ba_{n-1}} & \undermat{\text{plan}}{\left[ \texttt{PAD} \right] & \left[ \texttt{PAD} \right] & \dots & \left[ \texttt{PAD} \right]}
\end{bmatrix}
\label{eq:traj_with_pad_actions}
\end{equation}
\vspace{-10pt}
%
%\texttt{iter}
\begin{figure}[h]
\centering
\scalebox{0.75}{
\begin{minipage}{\linewidth}
\begin{algorithm}[H]
	\caption{Iterative Planning through Energy Minimization (for discrete actions)} 
	\label{algo:iterative}
	\begin{algorithmic}[1]
	\State \textbf{Require} trained energy model $h_\theta$, context trajectory $\bm{\tau}$
	\State Pad the states and actions with length $T$ into context trajectory 
    \For {$i=1,\ldots, \IterNum$}
        \State
        \small{\color{gray} 
        \te{// Sample index set}}.
        \State 
        $I \sim [1, 2, \cdots, \Hrz]$
        \State \small{\color{gray}\te{// Estimate the energy distributions on masked tokens}}
        \State $\mathcal{E}$ $\leftarrow$ $f(h(\btau{i}_{\backslash I}; \theta))$
        \State \small{\color{gray}\te{// Sample the action tokens based on energy value}} % with lowest energy value
        \State $\ba \sim \mathcal{E}$ 
        \State 
        \small{\color{gray} \te{// Update actions $\ba$ in $\bm{\tau}$ at masked tokens}}
        \State
        $\btau{i+1} \leftarrow \btau{i}_{\backslash \Hrz} + \ba$
    \EndFor
    \State Execute all planned actions $\ba_{1:T}$ or the first planned action $\ba_1$ in padded trajectory $\bm{\tau}$
	\end{algorithmic} 
\end{algorithm}
\end{minipage}
}
\end{figure}
\vspace{-5pt}

Note that our resultant algorithm has several important differences compared to sequential model based approaches such as Decision Transformer (DT) \citep{chen2021dt}. First, actions are generated using an energy function defined globally across actions, enabling us to choose each action factoring in the entire trajectory context. Second, our action generation procedure is iterative in nature, allowing us to leverage computational time to find better solutions towards final goals. 





