\appendix
\textbf{\Large{Appendix}}
\vspace{5pt}

\section{Experimental Details}

\subsection{BabyAI Environment Details}
We categorize the environments tested in the trajectory planning and instruction completion into the single-room plane world and multi-room maze world which are connect by doors.
\begin{enumerate}[leftmargin=*]
    \item Trajectory planning: 
    \begin{itemize}
        \item Plane world: GoToLocalS7N5 (7$\times$7), GoToLocalS8N7 (8$\times$8)
        \item Maze world: GoToObjMazeS4 (10$\times$10), GoToObjMazeS7 (19$\times$19)
    \end{itemize}
    \item Instruction completion:
    \begin{itemize}
        \item Plane world: PickUpLoc (8$\times$8)
        \item Maze world: GoToObjMazeS4R2Close (7$\times$7), GoToSeqS5R2 (9$\times$9)
    \end{itemize}
\end{enumerate}

The Table~\ref{tbl:babyai_env_setting} presents the detailed BabyAI environment settings including the environment size, the number of rooms, the number of obstacles, the status of doors and one example instruction in that environment.
\begin{table*}[h]
\centering
\small
\scalebox{0.8}{
\begin{tabular}{lrrrrc}
\toprule
\multicolumn{1}{c}{\bf Env} & \multicolumn{1}{c}{\bf Size} & \multicolumn{1}{c}{\bf \# Room} & \multicolumn{1}{c}{\bf \# Obs} & \multicolumn{1}{c}{\bf Door} & \multicolumn{1}{c}{\bf Inst} \\ 
\midrule
GoToLocalS7N5 & $7\times7$ & 1 & $5$  & $-$ & go to the green key \\ 
GoToLocalS8N7   &  $8\times8$ & 1& $7$ & $-$ & go to the blue box  \\
GoToObjMazeS4    & $10\times10$& 9 & $1$ & $9$ ~$Open$ & go to the blue key \\ 
GoToObjMazeS7 & $19\times19$& 9 & $1$ & $9$ ~$Open$ &  go to the grey ball \\ 
GoToObjMazeS4R2Close & $7\times7$& 4 & $1$ & $3$~ $Closed$ & go to the blue ball \\ 
PickUpLoc   &  $8\times8$& 1 & $8$ & $-$ & pick up the yellow ball \\
GoToSeqS5R2    & $9\times9$& 4 & $4$ & $3$ ~$Closed$ &  \makecell[c]{go to the green door and go to the green door,\\then go to a red door and go to the green door}  \\ 
\bottomrule
\end{tabular}
}
\caption{
BabyAI environment setting details and example instruction.}
\label{tbl:babyai_env_setting}
\end{table*}

\subsection{Network Details} \label{appendix:atari_hyperparameters}

We build our \netName implementation based on Decision Transformer (\url{https://github.com/kzl/decision-transformer}) and exploit the instruction encoder from the BabyAI agent model (\url{https://github.com/mila-iqia/babyai/blob/iclr19/babyai/model.py}). 
In detail, we use the Gated Recurrent Units (GRU) encoder to process the instruction and then apply ExpertControllerFiLM (inspired by FiLMedBlock from \url{https://arxiv.org/abs/1709.07871})to fuse the instruction embedding with state embedding. \hongyi{For all our experiments we use bidirectional mask in transformer attention layer, except for Atari where we found casual attention to perform better.} The full list of hyperparameters can be found in Table \ref{tbl:babyai_hyperparameters} and Table \ref{tbl:atari_hyperparameters}, most of the hyperparameters are taken from Decision Transformer and BabyAI agent model. 


\begin{table*}[ht]
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Value}  \\
\midrule
Number of layers & $3$  \\ 
Number of attention heads    & $4$  \\
Embedding dimension    & $128$  \\ 
Batch size   & $64$\\ 
Image Encoder & nn.Conv2d\\
Image Encoder channels & $128, 128$\\
Image Encoder filter sizes & $2 \times 2, 3 \times 3$\\
Image Encoder maxpool strides & $2, 2$ (Image Encoder may vary a little \\ & depending on the environment size)\\
Instruction Encoder & nn.GRU\\
Instruction Encoder channels & $128$\\
State Encoder & nn.Linear\\
State Encoder channels & $256, 256, 128$\\
Max epochs & $200$ \\
Dropout & $0.1$ \\
Learning rate & $6*10^{-4}$ \\
Adam betas & $(0.9, 0.95)$ \\
Grad norm clip & $1.0$ \\
Weight decay & $0.1$ \\
Learning rate decay & Linear warmup and cosine decay (see code for details) \\
\bottomrule
\end{tabular}
\caption{Hyperparameters of \netName for BabyAI experiments.}
\label{tbl:babyai_hyperparameters}
\end{small}
\end{center}
\end{table*} 


% not using CQL numbers for conditioning
\begin{table}[ht]
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Value}  \\
\midrule
Number of layers & $6$  \\ 
Number of attention heads    & $8$  \\
Embedding dimension    & $128$  \\ 
Batch size   & $64$ Breakout, Qbert\\ 
            & $128$ Seaquest\\ 
            & $256$ Pong\\ 
Image Encoder & nn.Conv2d\\
Image Encoder channels & $32, 64, 64$\\
Image Encoder filter sizes & $8 \times 8, 4 \times 4, 3 \times 3$\\
Image Encoder strides & $4, 2, 1$ \\
Max epochs & $10$ \\
Dropout & $0.1$ \\
Learning rate & $6*10^{-4}$ \\
Adam betas & $(0.9, 0.95)$ \\
Grad norm clip & $1.0$ \\
Weight decay & $0.1$ \\
Learning rate decay & Linear warmup and cosine decay (see code for details) \\
\bottomrule
\end{tabular}
\end{small}
\caption{Hyperparameters of \netName for Atari experiments.}
\label{tbl:atari_hyperparameters}
\end{center}
\end{table} 

\subsection{Baseline Models}
\paragraph{BabyAI Baseline Models}
We ran BCQ and IQL based on the following implementation
\begin{center}
{
    \small
    \href{https://github.com/sfujim/BCQ}
    {\texttt{https://github.com/sfujim/BCQ}}.
}
\end{center}
\begin{center}
{
    \small
    \href{https://github.com/BY571/Implicit-Q-Learning/tree/main/discrete_iql}
    {\texttt{https://github.com/BY571/Implicit-Q-Learning/tree/main/discrete\_iql}}.
}
\end{center}
For BC and DT, we use the author's original implementation
\begin{center}
{
    \small
    \href{https://github.com/kzl/decision-transformer}
    {\texttt{https://github.com/kzl/decision-transformer}}.
}
\end{center}
\hongyi{For PlaTe, we use the author's original implementation
\begin{center}
{
    \small
    \href{https://github.com/Jiankai-Sun/plate-pytorch}
    {\texttt{https://github.com/Jiankai-Sun/plate-pytorch}}.
}
\end{center}
For MOPO, we use the author's original implementation of dynamic model training and policy learning. For RL policy, we adopt the IQL discussed above.
\begin{center}
{
    \small
    \href{https://github.com/tianheyu927/mopo}
    {\texttt{https://github.com/tianheyu927/mopo}}.
}
\end{center}}
The actor network and policy network of BCQ and IQL use the transformer architecture which is the same as architecture in our model, see details above. The original DT and BC already use the transformer architecture so we didn't change. For all baselines, we add the same instruction encoder and image encoder described above to process instruction and image observations. 


\paragraph{Atari Baseline Models} The scores for DT, BC, CQL, QR-DQN, and REM in Table \ref{tbl:atari_main} can be found in \citet{chen2021dt}.


\subsection{Experiment details}\label{app:ExpDetails}
\label{appendix:experiment}

\vspace{3pt}
\myparagraph{BabyAI}

For \netName, the larger size environment requires longer horizon $T$ and correspondingly more sampling iterations $N$. After $N$ iteration, all $T$ planned actions will be executed. For DT model, it's beneficial of using a longer context length in more complex environments as shown in its original paper \citep{chen2021dt}. We list out these parameters for \netName and DT models in Table~\ref{tbl:babyai_experiment_setting}. We didn't use context information in \netName in most BabyAI environments as we expect the iterative planning could generate a correct trajectory based solely on the current state observation. While the GoToSeqS5R2 environment requires go to a sequence of objects in correct order and \netName needs to remember what objects have been visited from the context. During training, we randomly select and mask one action in a trajectory. 

\begin{table*}[h]
\centering
\small
\scalebox{0.8}{
\begin{tabular}{ccccc}
\toprule
& \multicolumn{3}{c}{\bf \netName} & \multicolumn{1}{c}{\bf DT} \\ 
\multicolumn{1}{c}{\bf Env} & \multicolumn{1}{c}{\bf context} & \multicolumn{1}{c}{\bf plan } & \multicolumn{1}{c}{\bf sample iteration} & \multicolumn{1}{c}{\bf context} \\
\midrule
GoToLocalS7N5 & 0 & 5 & 10  & 5 \\ 
GoToLocalS8N7   &  0 & 5 & 10  & 5 \\
GoToObjMazeS4    & 0& 10 & 30 & 10 \\ 
GoToObjMazeS7 & 0& 15 & 50 & 15\\ 
GoToObjMazeS4R2Close & 0& 5 & 10 & 5\\ 
PickUpLoc & 0& 5 & 10 & 5\\ 
GoToSeqS5R2    & 20 & 5 & 10 & 20 \\ 
\bottomrule
\end{tabular}
}
\caption{
BabyAI environment experiment details for \netName and DT.}
\label{tbl:babyai_experiment_setting}
\end{table*}

The input to DT model includes the instruction, state context sequence, action context sequence and return-to-go sequence in which the target reward is set to 1 initially. The input to other baseline models are the same except they use normal reward sequence instead of return-to-go sequence. While \netName only use the instruction, state context sequence and action context sequence.
Inside state sequence, each state $\bs_n$ contains the $[x,y,dir,g_x,g_y]$ meaning the agent's x position, y position, direction and goal object's x position, y position (if the goal location is available).

\myparagraph{Atari}
\vspace{3pt}
In dynamically changing Atari environment, \netName use context information in all four games and only execute the first planned action to avoid the unexpected changes in the world, see details in Table~\ref{tbl:atari_experiment_setting}. During training, we randomly  sample and mask one action in a trajectory. 

%\ycReb{To deal with varied quality of demonstrations for training, we added the return-to-go, the remaining reward at each step towards the achieved final reward following DT , as input to \netName. We test \netName without the reward input, which results in deteriorate performance as shown in Table. \ref{tbl:LEAP_reward}.}



\begin{table*}[h!]
    \centering
    \small
    % \hspace{-35pt}
    \begin{minipage}[t]{0.45\linewidth}
        \centering\color{black}
        % \setlength\tabcolsep{1.5pt} 
        \scalebox{1.0}{
        \begin{tabular}{cccc}
        \toprule
        \multicolumn{1}{c}{\bf Env} & \multicolumn{1}{c}{\bf context} & \multicolumn{1}{c}{\bf plan } & \multicolumn{1}{c}{\bf sample iteration}\\
        \midrule
        Breakout & 25 & 5 & 10  \\ 
        Qbert   &  25 & 5 & 10 \\
        Pong    & 25& 5& 10 \\ 
        Seaquest & 25& 10 & 30\\ 
        \bottomrule
        \end{tabular}
        }
        \caption{
        Atari environment experiment details for \netName.}
        \label{tbl:atari_experiment_setting}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\linewidth}
        % \hspace{-35pt}
        \centering\color{black}
        \scalebox{1.0}{
            \begin{tabular}{ccc}
            \toprule
            \multicolumn{1}{c}{\bf Env} & \multicolumn{1}{c}{\bf w/o return} & \multicolumn{1}{c}{\bf w return}\\
            \midrule
            Breakout & 182.0 & 378.9  \\ 
            Qbert   &  41.0 & 19.6 \\
            Pong    & 100.7& 108.9 \\ 
            Seaquest & 0.5& 1.3\\ 
            \bottomrule
            \end{tabular}
        }
        \captionsetup{labelfont={color=black},font={color=black}}
        \caption{\netName performance in Atari environment with and w/o return input.}
        \label{tbl:LEAP_reward}
    \end{minipage}
    \vspace{-0pt}
\end{table*}

\hongyi{
Note that our approach can easily be conditioned on total reward, by simply concatenating the reward as input in the sequence model. One hypothesis is that when demonstration set contained trajectories of varying quality, taking reward as input following will enable the model to recognize the quality of training trajectories and potentially improve the performance. To further validate the importance of the rewards, we test the \netName with and without return-to-go inputs, which sum of future rewards \cite{chen2021dt}. The results show that the performance degradation without the return-to-go inputs, which is shown in Table \ref{tbl:LEAP_reward}.}


\input{appStoch}
\input{appAblate}