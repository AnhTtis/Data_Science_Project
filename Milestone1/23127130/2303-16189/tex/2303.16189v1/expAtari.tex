\vspace{-5pt}
\subsection{Atari}

\vspace{3pt}

\myparagraph{Setup} We further evaluate our approach on the dynamically changing Atari environment, with higher-dimensional visual state. 
Due to above features , we train and test our model without the goal state, and update the plan after each step of execution to avoid unexpected changes in the world. We compare our model to BC, DT \citep{chen2021dt}, CQL \citep{kumar2020conservative}, REM \citep{agarwal2020optimistic}, and QR-DQN \citep{dabney2018distributional}.
Following \citet{chen2021dt}, the evaluation is conducted on four Atari games (Breakout, Qbert, Pong, and Seaquest), where $1\%$ of data is used for training. 
Human normalized score is utilized for the performance evaluation.


\myparagraph{Results} 
Table \ref{tbl:atari_main} shows the result comparison.
\netName achieves the best average performance.
Specifically, it achieves better or comparable result in 3 games out of 4,
whereas baselines typically perform poorly in more than one games.



\begin{comment}
When using only 1\% of the data, our method is competitive with the-state-of-art models like DT and CQL in 3 out of 4 games and achieves highest averaged scores over 4 games, as shown in Table \ref{tbl:atari_main}. The scores are normalized based on a professional gamer, following the protocol in \citet{chen2021dt}. 

\end{comment}


\myparagraph{Energy Landscape Verification}
In Atari environment, the training trajectories are generated by an online DQN agent during training, whose accumulated rewards are varied a lot. \netName is trained to estimate the energy values of trajectories depending on their rewards. In Figure~\ref{fig:anlsAtar}, we visualize the estimated energy of different training trajectories and their corresponding rewards in Breakout and Pong games. We notice that the underlying energy value estimated to a trajectory is well correlated with its reward, with low energy value assigned to high reward trajectory. This justifies the correctness of our trained model and further gives a natural objective to assess the relative of quality planned trajectory. In Qbert and Seaquest games that \netName gets low scores, this negative correlation is not obvious showing that the model is not well-trained.