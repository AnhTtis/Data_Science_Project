Here, we describe the datasets, the metrics, and the implementation parameters for our experiments.

\xhdr{Datasets}
\label{sec:dataset}
We use two datasets each consisting of a set of images and their ground truth (protected labels) to analyze the bias in the output representations of VLMs. Further, each image in the datasets is tested  against a set of curated text captions that represent different desirable (positive) and undesirable (negative) connotations.

\xhdr{\textit{Images and Protected Labels}} We obtain images and labels from the following:
% \begin{enumerate}[nosep]
% \item 
i) the \textit{FairFace dataset} \cite{fairface} consists of a set of cropped face images of different persons. It has 86,700 training images and 10,954 validations images, annotated with the binary gender (male and female), ethnic-racial classes (White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino), and multiple age brackets, which we bin into four categories (\textit{child} $<20$, \textit{young} $<40$, \textit{middle-aged} $<60$, and \textit{senior} $>60$), and
% \item 
ii) the \textit{\dataset} (\pata) dataset consists of 4,934 public images crawled from the web, organized in 24 scenes each with between 100 and 400 images. These scenes represent situations in which humans of specific identity groups may have biased notions associated with them. The images are further annotated with the binary gender, ethnic-racial labels (White, Black, Indian, East Asian, and Latino-Hispanic), and two age group labels (young and old adults).
% \end{enumerate}

\xhdr{\textit{Text Captions}} Each scene in the \pata dataset is associated with the positive caption set consisting of desirable descriptions of a person, designed specifically to elicit a biased response from the models. For instance, for the {\small\texttt{race}} protected attribute (PA), the caption ``photo of a law-abiding citizen'', and for the {\small\texttt{gender}} protected attribute, the caption ``photo of a smart engineer'' must both result in uniformly distributed similarities between the images and the text. The negative captions similarly consist of untoward and offensive text in the context of each protected attribute. For the \pata dataset, there are 78 (unique) positive captions, and 88 negative captions, with an average of 1.08 and 1.2 per (scene, PA) pairs, respectively. For the FairFace dataset, we consider all images to belong to a single scene and use 16 positive and negative captions for each.

\xhdr{\textit{Advantages of the \pata dataset}} Images in the FairFace dataset only have cropped faces and, therefore, do not contain any context. Hence, any association between the text captions and the face images is incidental at best because the neural network does not have any signal to form the association. The \pata dataset bridges this gap and thus enables a more nuanced evaluation of the bias presented by a VLM. More details about the dataset are included in the Appendix~\ref{app:dataset} of the supplemental material.

\xhdr{\textit{Zero-shot Evaluation datasets}} We consider the CIFAR-10, CIFAR-100~\cite{cifar}, FER2013~\cite{fer2013}, and ImageNet~\cite{imagenet} datasets for zero-shot image classification task using debiased VLM representations. Further, we consider the UCF-101~\cite{ucf101}, Kinetics-700~\cite{kinetics}, and RareAct~\cite{rareact} datasets for zero-shot video classification.

\xhdr{Evaluation Metrics}
\label{subsec:metrics}
Our de-biasing framework is evaluated on two key criteria: i) the degree to which it improves the fairness of a given VLM, which is measured using the similarity between person images with different protected labels and a set of text captions, and ii) the degree to which it retains the zero-shot performance of the underlying VLM on various tasks, including classification on human and non-human related datasets.

\xhdr{\textit{Fairness Metrics}} The degree of fairness exhibited by a model is measured in two respects:

\noindent i) the overall shift in similarity characteristics of the image and text features towards fairness measured using the mean \textit{MaxSkew} and the mean \textit{MinSkew} scores. The former indicates the selective association of the model in favor of any protected label and the latter indicates the degree of selective dissociation against any other protected label. These are computed for a set of images $\mathbf{I}$, a set of captions $\mathbf{T}$, and a protected attribute P with labels {$p_i$} as:
\begin{align}
    \begin{split}
        \psi_{max}(\mathbf{I}, \mathbf{T}, P) &= \text{Mean}_{t\in \mathbf{T}}[\max_{i}(\text{Skew}_{i}(\mathbf{I}, t))] \\
        \psi_{min}(\mathbf{I}, \mathbf{T}, P) &= -\text{Mean}_{t\in \mathbf{T}}[\min_{i}(\text{Skew}_{i}(\mathbf{I}, t))],
    \end{split}
\end{align}
where $\text{Skew}_{i}(\cdot)$ is computed using Equation~\ref{eqn:skew}.

\noindent ii) the ordered measurement of the similarity distribution across the protected labels of each protected attribute, which is indicated using the \textit{MaxSkew@K} and \textit{MinSkew@K} scores as stipulated in Geyik et al.~\cite{maxSkew} and used for fairness analysis. This is a ranking-based measurement that only considers the $k$ most similar images to each text caption, and then computes the mean Max- and Min-Skew scores. To add more nuance to the evaluation, we compute the above metrics for both positive and negative captions for the person in the image, which clearly indicates the stakes for the model in selective association or dissociation for a particular identity group.

\xhdr{\textit{Zero-shot performance metrics}} We evaluate the zero-shot performance of \method-augmented and vanilla VLMs using standard classification accuracy metrics for the image-classification tasks. For video action recognition tasks, we use the top-1 accuracy, the averaged accuracy measures and variants of the mean weighted average precision metrics as described in Miech et al.~\cite{miech20rareact}. We report the drop in each metric to indicate how much the debiasing process affects the zero-shot performance of the underlying VLM. In addition, we discuss the effect of \method framework on the task of zero-shot open-vocabulary object detection~\cite{ods}.

\xhdr{Implementation Details}
\label{sec:training}
Both PAC and the ARL modules are individually trained for each VLM using the training subset of the FairFace dataset, which consists of 86,700 images with annotations for the race, gender, and age attributes. The PAC is trained on its objectives (Equation~\ref{eqn:info}) for 10 epochs, whereas the ARL is trained for 30 epochs with early stopping based on validation loss saturation. See Appendix~\ref{app:setup} for more details on other hyperparameters.
