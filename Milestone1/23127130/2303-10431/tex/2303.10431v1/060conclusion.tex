We address the problem of mitigating biases from the representations of large pre-trained visual-language models. To this end, we present \method (Debiasing with Additive Residuals), a novel method that learns additive residual image representations
to offset the original VLM representations, ensuring fair output representations. In particular, \method comprises an additive residual learner component, which learns to disentangle the protected attribute information from the output representations. We also introduce a new context-based bias benchmarking dataset for VLMs - the Protected Attribute Tag Association (\pata) dataset. Our results on multiple benchmarking datasets show that \method can significantly improve the fairness of output representations without sacrificing predictive zero-shot performance. With the increasing development of large VLM, our work paves the way for several exciting future directions for fairness research.