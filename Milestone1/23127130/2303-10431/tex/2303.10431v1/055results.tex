Next, we present the experimental results for the \method framework and address the following key questions: Q1) Does \method reduce the bias in VLMs? and Q2) Do the de-biased VLMs retain their predictive properties?

\xhdr{Q1) Bias Reduction in VLMs} We evaluate both the overall shift in the distribution of the similarities (MaxSkew) and the ordered metrics (MaxSkew@K) between the images and text captions for both evaluation datasets. Tables~\ref{tab:ffskew}-\ref{tab:pataskew} present the mean Max-/MinSkew and mean Max-/MinSkew@K scores for the FairFace and \pata datasets. Both are computed using a threshold cosine similarity value of 0.1 for both datasets. We chose $k{=}1000$ for FairFace and $k{=}100$ for the \pata dataset.
% Table 1
\input{./fairface_all}

\xhdr{\textit{Interpretation}} The mean MaxSkew and MaxSkew@K scores indicate the selective association with respect to some favored protected labels for each protected attribute. The mean MinSkew and MinSkew@K scores indicate selective dissociation of the models for particular protected labels. Note that the favored (or disfavored) labels can change between the original and de-biased variants of each model. If the sentiment of an entry with a high MaxSkew value is negative, the favored label is the one with a disproportionately high association with negative captions. Similarly, entries with a low (negative) MinSkew value for a positive sentiment entry imply that the disfavored label has an unusually low association with the positive captions in the dataset. 

Both tables indicate the benefit of using the \method framework for de-biasing. We observe a slight increase in the skew values in some cases which we attribute to a flip in the favored PA labels. For instance, the MaxSkew value for FLAVA changes from 0.31 to 0.37 for the (race, positive) combination, but so does the favored label from ``middle-eastern" to ``east-asian", and going overboard with the optimization. This can be mitigated by reducing the associated training weights $w_{ce}^r$ while training the ARL module.

\input{./pata_all}

\begin{table}%[t]
\small
\setlength{\tabcolsep}{2.1pt}
\renewcommand{\arraystretch}{0.9}
\centering
\caption{Results of state-of-the-art visual-language models and their \method counterparts for four image classification datasets. Across five pre-trained visual-language models, \method achieves zero-shot performance similar to vanilla models. See Appendix~\ref{app:results} for complete results.
}
\vspace{-0.5em}
\label{tab:image}
{
% \resizebox{8.3cm}{!}{
\begin{tabular}{lcccc}
    \toprule
    Model & C-10 & C-100 & FER2013 & ImageNet\\
    \midrule
    CLIP (ViT/B-32) & 89.93 & 62.93 & 43.83 & 58.08\\
    \method-CLIP (ViT/B-32) & 88.85 & 60.08 & 39.60 & 55.84\\
    \midrule
    FLAVA & 90.53 & 65.60 & 28.36 & 49.30 \\
    \method-FLAVA & 89.05 & 64.00 & 27.19 & 47.67 \\
    \midrule
    BLIP & 85.00 & 51.61 & 39.50 & 32.57 \\
    \method-BLIP & 81.20 & 48.90 & 36.50 & 29.94 \\
    \bottomrule
\end{tabular}}
% }
\end{table}
\xhdr{Q2) Zero-Shot Accuracy Evaluation} VLMs de-biased with \method retain much of their zero-shot performance on downstream tasks. We perform an extensive analysis of the effect of our \method framework on several downstream zero-shot classification tasks. In Tables~\ref{tab:image}-\ref{tab:video}, we present the zero-shot performance for four image classification and three video classification datasets. For general object classification datasets like CIFAR-10, CIFAR-100, and ImageNet, we observe that the zero-shot performance of \method augmented VLMs, on average, is only 2.2\% lower than their vanilla counterparts. On a fine-tuned facial emotion classification dataset like FER2013, the zero-shot performance between vanilla VLMs and their \method counterpart is smaller (1.7\%). Following \cite{radford2021learning}, we sub-sample each video to only a single center frame for video classification, effectively turning it into an image classification dataset. On UCF-101 and Kinetics-700 datasets, we observe a drop of 2.06\% and 1.77\% for the \method VLMs in the zero-shot predictive performance. Finally, for the RareAct dataset that is used to quantify the zero-shot recognition of unusual actions like ``blend phone'' and ``microwave shoes'', we observe a drop of 0.71\% and 0.69\% in the zero-shot classification performance of \method using mWAP and mWSAP metrics. Please refer to Appendix~\ref{app:results} for more details.

\begin{figure}
    \centering
    \includegraphics[width=0.98\linewidth]{tsneplots.pdf}
    \caption{TSNE Plots for CLIP, Residual and \method-CLIP features for a subset of the PATA dataset indicate that the residual plots indeed capture the specific attributes and that the \method-CLIP features have greater overlap between points of different protected labels than the original features. Refer to Appendix~\ref{app:results} for higher resolution plot.}
    \label{fig:tsne}
\end{figure}


\begin{table}%[t]
\small
\setlength{\tabcolsep}{1.5pt}
\renewcommand{\arraystretch}{0.9}
\centering
\caption{Results of state-of-the-art visual-language models and their \method counterparts for three video classification datasets. Across five pre-trained visual-language models, \method achieves zero-shot performance similar to vanilla models. See Appendix~\ref{app:results} for complete results.
}
\vspace{-0.5em}
\label{tab:video}
{
% \resizebox{8.3cm}{!}{
\begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{Model} & UCF-101 & Kinetics-700 & \multicolumn{2}{c}{RareAct}\\
    & Top-1 & AVG & mWAP & mWSAP\\
    \midrule
    CLIP (Base-32) & 57.65 & 43.97 & 16.63 & 16.78\\
    \method-CLIP (Base-32) & 55.77 & 42.20 & 16.02 & 16.03\\
    \midrule
    FLAVA & 39.09 & 37.85 & 16.12 & 16.14\\
    \method-FLAVA & 37.27 & 35.59 & 15.30 & 15.43\\
    \midrule
    BLIP & 43.26 & 37.07 & 16.35 & 16.44\\
    \method-BLIP & 40.34 & 34.78 & 15.86 & 15.92\\
    \bottomrule
\end{tabular}}
% }
\end{table}