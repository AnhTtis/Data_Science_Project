We explore two important issues here -- i) ascertaining that the de-biased representations are free from the protected attribute information and ii) justifying the joint training of the ARL module for different protected attributes. Please refer to Appendix~\ref{app:ablation} for more ablation studies.

\xhdr{Evidence for de-biasing} We run several experiments to ascertain that the representations from the \method-CLIP model are indeed free from the protected attribute information. Figure \ref{fig:teaser} (A) shows a comparison of the explanations for CLIP and its de-biased variant (\method-CLIP). These explanations are generated for some in-the-wild creative-commons stock images using the approach in \cite{fong2017interpretable}. They illustrate how the original CLIP model focuses on the face and hair region for certain occupation-related keywords, and after de-biasing, the focus shifts to more indicative cues like the stethoscope for a doctor. Figure \ref{fig:teaser}(B) shows results of zero-shot open-vocabulary object detection using \cite{ods}. Again, the base CLIP model exhibits a bias omitting the woman for the ``doctor" keyword, and only detecting the female medical professional as a ``nurse". With \method-CLIP, both men and women are detected for both keywords. It is noteworthy that at a lower detection threshold, both male and female doctors and nurses may have been detected, but it still points to the implicit bias because the stereotypical associations rank higher in similarity. Figure \ref{fig:tsne} depicts how the CLIP features get disentangled into the target PAs, and the addition of the residual results in coinciding points for different protected labels, indicating that the representations for different PLs cannot be distinguished from each other.

\xhdr{Joint vs. Sequential training of ARL} Another design choice is to use a jointly-trained PAC module for training the ARL module. It is also possible to train the residual module using multiple individual classifiers (one for gender, one for race, etc.) and use different weights for the ARL modules for each protected attribute. At inference, the VLM's image representation is transformed using multiple ARL weights one after the other. We experiment with this strategy for CLIP and find that while this strategy does reasonably well for debiasing (\method-CLIP: MaxSkew (`gender'=0.75, `race'=0.167); Sequential strategy: MaxSkew (`gender'=0.87, `race'=0.161), its zero-shot ImageNet accuracy considerably falls (\method-CLIP=55.84\%, Sequential strategy=44.43\%). Hence, the joint training approach is better than sequential strategy.
