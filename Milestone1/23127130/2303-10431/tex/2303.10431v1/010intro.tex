\noindent Deep learning-based vision-language models (VLMs)~\cite{radford2021learning} unify text and visual data into a common representation and reduce the computing cost of training for specific computer vision~\cite{Gu2022OpenvocabularyOD} and visually-grounded linguistic tasks~\cite{shen2021much,cho-etal-2022-fine}. VLMs are trained with a large amount of data with the aim of matching image and text representations for image-caption pairs to capture diverse visual and linguistic concepts. However, VLMs exhibit societal biases manifesting as the skew in the similarity between their representation of certain textual concepts and kinds of images~\cite{berg2022prompt,borchers2022looking,kirk2021bias}. These biases arise from the underlying imbalances in training data~\cite{multimodalDatasetBias, berg2022prompt} and flawed training practices~\cite{biasBeyondBalancedDatasets}. In this work, we present a method to significantly reduce the bias in VLMs by modifying the visual features of the models.

These societal biases in VLMs show up as the selective association (or dissociation) of their representations of human images with specific physical characteristics and their text representations for describing social labels~\cite{Wang2021AreGQ}. For instance, a higher degree of similarity between the representation of the text “doctor” and images of men than that of the women can have trust consequences for models using the representations from these VLMs. To estimate the degree of such biases in VLMs, we compute the cosine similarity between the representations of a set of human images and specific key text phrases and compare their distribution over some associated \textit{protected attributes}. These attributes represent visually discernible characteristics like \textit{gender}, \textit{race}, and \textit{age} common to certain collective identity groups. In this work, we introduce \method, an additive residual-based de-biasing technique that can be augmented with any pre-trained visual-language model with separate visual and language encoders~\cite{radford2021learning, singh2022flava, li2022blip} to improve their fairness.

Our empirical analysis indicates that the protected attribute (PA) information from an image-text pair can be disentangled using their representation by simply learning a linear transformation of the visual representations produced by a VLM and subtracting (adding a negative residual) them from the original representation. We propose to train our framework using two objectives: i) learn a residual representation that when added to the original representation, renders it incapable of predicting different protected attributes, and ii) ensure that this modified representation is as close to the original as possible. We demonstrate that learning \textit{additive residual} enables the de-biasing of pre-trained VLMs using quantitative skew computations on multiple datasets and qualitative evaluations. We also show that the resulting representation retains much of its predictive properties by means of zero-shot evaluation on different downstream tasks.

Recent efforts to mitigate these biases from VLMs, such as by Berg et al.~\cite{berg2022prompt}, use unimodal datasets like the FairFace~\cite{fairface} and UTK~\cite{UTK} datasets. These face-image datasets lack the context necessary to infer the situations in which benign text associations can turn offensive when applied selectively. For instance, if the image of a person drinking water from a bottle is misconstrued as partaking of alcohol and if this association (in terms of image-text similarity) is selective to a specific group of people with some specific visual characteristics, the association may be deemed offensive. To this end, we introduce the \dataset (\pata) dataset to test different associations for VLMs. \pata comprises images of persons in different contexts and their respective text captions with positive and negative connotations. We present our de-biasing results on both the FairFace and the \pata datasets. In summary, the paper makes the following contributions:
\begin{itemize}[nosep]
\item We present the \method framework -- a simple, computationally-efficient, and effective de-biasing method for VLMs that only adapts the image encoder of the VLM by adding a learned residual representation to it. 
\item We introduce the \dataset dataset -- a novel context-based bias evaluation dataset that enables nuanced reporting of biases in VLMs for race, age, and gender protected attributes.
\end{itemize}