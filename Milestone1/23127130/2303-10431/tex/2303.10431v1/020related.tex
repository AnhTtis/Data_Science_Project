Our work lies at the intersection of large pre-trained VLMs and the identification, measurement, and mitigation of societal biases in large pre-trained models.

\xhdr{Pre-trained Vision-Language Models} Large VLMs aim to learn general information from a large amount of data and then transfer the knowledge to diverse downstream tasks~\cite{radford2021learning,li2022supervision,li2021align,kim2021vilt,vlmo,singh2022flava,li2020oscar,chen2020uniter}. Recent works~\cite{desai2021virtex,singh2022flava,zellers2021merlot,zhang2021vinvl} leverage contrastive strategies to learn representations for multimodal data. Based on how these models learn these representations, VLMs are broadly categorized into two groups: i) learning image-text representations jointly using transformer-based encoders~\cite{chen2020uniter,li2020oscar,lu202012,li2019visualbert,desai2021virtex,singh2022flava,zellers2021merlot,zhang2021vinvl}, and ii) learning individual unimodal encoders for image and text~\cite{chen2020simple,zbontar2021barlow,chen2021exploring,chen2020improved,he2020momentum,radford2018improving,brown2020language,devlin2018bert}. Models like CLIP~\cite{radford2021learning}, ALIGN~\cite{jia2021scaling}, and BASIC~\cite{pham2021combined} are pre-trained on large datasets collected from the web to bring representations of paired images and text close to each other while distancing them from other pairs. Our work analyzes recently proposed state-of-the-art VLMs for the societal biases they exhibit. Specifically, we look at the behavior of CLIP~\cite{radford2021learning}, FLAVA~\cite{singh2022flava}, and BLIP~\cite{li2022blip} and propose to alleviate bias from their visual representations.

\xhdr{Fairness Techniques} Prior work in language~\cite{borchers2022looking,guo2021detecting,kirk2021bias}, computer vision~\cite{wang2022revise,gebru2021datasheets}, and graphs~\cite{agarwal2021towards,kang2021fair,wang2022unbiased,ma2022learning} has primarily focused on debiasing models trained on unimodal data and are limited in scope as they only investigate gender bias, racial bias, or their intersections. In particular, their bias mitigation techniques can be broadly categorized into i) \textit{pre-processing}, which modifies individual input features and labels~\cite{calmon2017optimized}, modifies the weights of the training samples~\cite{kamiran2012data}, or obfuscates protected attribute information during the training process~\cite{zemel2013learning}; ii) \textit{in-processing}, which uses adversarial techniques to maximize accuracy and reduce bias for a given protected attribute~\cite{zhang2018mitigating}, data augmentation~\cite{agarwal2021towards} or adding a bias-aware regularization term to the training objectives~\cite{kamishima2012fairness}, and iii) \textit{post-processing}, which changes the output predictions from predictive models to make them fairer~\cite{kamiran2012decision,pleiss2017fairness,hardt2016equality}. It is non-trivial to apply these techniques to pre-trained VLMs because the training requires a large annotated dataset and immense computing resources. Mitigating bias in large pre-trained VLMs is a nascent research direction. Wang et al.~\cite{wang2021gender} propose to remove the dimensions in CLIP embeddings most associated with gender bias, while Berg et al.~\cite{berg2022prompt} use adversarial fine-tuning with a corpus of face images and arrays of text prompts to mitigate bias. The former cannot achieve joint mitigation of bias for multiple protected attributes, and the latter method modifies the original model changing its accuracy on zero-shot tasks significantly. Our proposed \method framework addresses both issues by training a lightweight residual representation over the visual representations of VLMs, modeling the joint bias with respect to multiple protected attributes (gender, race, and age), and ensuring the similarity of the modified representation to the original. 

\looseness=-1
\xhdr{Fairness Benchmarks} Previous work on debiasing VLMs~\cite{wang2021gender, berg2022prompt} exclusively focuses on face-image datasets, \ie, FairFace~\cite{fairface} and UTKFace~\cite{UTK} datasets. While the FairFace dataset has 10,954 test images consisting of faces of 10 age
groups, seven race groups, and the binary genders, the UTKFace dataset has 23,708 cropped test images in 104 different age values (1-105), five race groups, and the binary genders. Neither datasets have the pixels or annotations to provide the context of the person in an image. We collect a dataset of 4934 test images organized into different contextual scenes
and prepare positive and negative captions for each context, providing a more nuanced view of a VLMâ€™s fairness.

Next, we discuss how we achieve this accuracy-preserving joint mitigation of bias from multiple VLMs.