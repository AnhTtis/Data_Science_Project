\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{dear_framework_new.pdf}
    \caption{Our \method framework can learn fair representations for pre-trained Vision-Language models. 
    \method is a debiasing framework that takes image representations $E_i(I_k)$ as input and returns a residual representation $\phi(I_k)$. Next, it adds $\phi(I_k)$ with $E_i(I_k)$ to get debiased representation $\overline{\phi}(I_k)$ that is fed to a protected attribute classifier to get prediction for multiple protected attributes (race $\hat{y}^r_k$, gender $\hat{y}^g_k$, and age $\hat{y}^a_k$). \method jointly minimizes the negative of Cross Entropy loss for race ($L^r_{ce}$), gender ($L^g_{ce}$), and age ($L^a_{ce}$) attributes for debiasing across multiple protected attributes and a reconstruction loss $L_{recon}$ to retain the zero-shot performance of the underlying VLM.
    }
    \label{fig:dear-framework}
\end{figure*}

Our framework comprises of two key components: i) an Additive Residual Learner (ARL) component that learns to disentangle the protected attribute information $\phi(\mathbf{I})$ from the image representation produced by the visual encoder of a given VLM and ii) the protected attribute misclassification objective designed to train the ARL component. We first discuss the ARL component and then present details of the training network and objectives.

\subsection{Additive Residual Learning}
\label{sec:arl}
As discussed in Sec.~\ref{sec:prelims}, we propose to train a network $R(\cdot)$ that outputs a representation $\phi(\mathbf{I})$ so that we can compute the protected-attribute free representation $\overline{\phi}(\mathbf{I})$:
\begin{align}
    \overline{\phi}(\mathbf{I}) &= E_i(\mathbf{I}) - R(E_i(\mathbf{I}))
\end{align}
For brevity, we denote $R(E_i(\mathbf{I}))$ as $R(\mathbf{I})$. Note that the network $R: \mathbb{R}^d\rightarrow \mathbb{R}^d$ is a single linear transformation, where $d$ is the number of dimensions of the image representation produced by $E_i$. This is followed by a function $\sigma_{\text{act}}(\cdot)$ that represents the combination of activation and regularization layers (dropout, etc.) as used by the last layer of the image encoder $E_i$. If the last layer does not use any activation function, $\sigma_{\text{act}}$ is an identity transformation, \ie,
\begin{align}
    \begin{split}
        \overline{\phi}(\mathbf{I}) &= E_i(\mathbf{I}) + \sigma_{\text{act}}( R(\mathbf{I})),
    \end{split}
\end{align}
where $\overline{\phi}(\mathbf{I})$ represents the protected-attribute free representation for image $\mathbf{I}$. Note that the reason for transforming the last layer activation and regularization mechanisms from the image encoder is to align the embedding space, where $\overline{\phi}(\mathbf{I})$ maps to the original encoding $E_i(\mathbf{I})$ so that the image-text equivalence still holds. The resulting representation has two primary goals: i) it matches the original representation so that the zero-shot accuracy of the model can be retained, \ie, $\overline{\phi}(\mathbf{I})\approx E_i(\mathbf{I})$, and ii) it cannot be used to distinguish the protected label for the image with respect to different protected attribute. The above goals are achieved through different training objectives, which we describe next.

\subsection{Training ARL with Protected Attribute Misclassification}
\label{sec:paco}
To train the ARL to produce representations that cannot be used to identify the protected labels for an image, we pre-train an adversary -- the Protected Attribute Classifier (PAC) network and use its classification loss as an objective to be maximized for training the ARL. Unlike previous works in bias mitigation that jointly trains an adversarial network, we separately train the PAC. This is possible because the PAC can be trained with the underlying VLM representations and be used to supply gradients as the representations change to their protected-attribute-free form during training. 

The PAC module uses $\ell_{2}$ normalized $d$-dimensional representation from the VLM image encoder $E_i$ as input and generates multiple sets of logits for individual protected attributes. It consists of a single linear layer ($d\times256)$ with ReLU activation, followed by multiple linear classification projection heads ($256\times128$) with ReLU, and a linear layer to produce logits with output sizes 7 (for race), 4 (for age) and 2 (for gender), where the logit sizes are determined by the FairFace~\cite{fairface} training dataset. A unified architecture like \method helps in jointly modeling multiple protected attributes associated with an image. The network is trained on the combined cross-entropy loss using the ground-truth protected labels (race, age, and gender) for each image:

\begin{multline}
\label{eqn:info}
\mathcal{L}_{\text{PAC}} = \mathcal{L}^r_{\text{ce}}(y^{r},\hat{y}^r) + \mathcal{L}^g_{\text{ce}}(y^g,\hat{y}^g) + \mathcal{L}^a_{\text{ce}}(y^a,\hat{y}^a),
\end{multline}
where $\hat{y}^r$, $\hat{y}^g$ and $\hat{y}^a$ are the predictions, and $y^r$, $y^g$ and $y^a$ are the ground-truth labels for the protected attributes race, gender, and age respectively. 

\xhdr{Training objectives for the ARL module} The VLM parameters are frozen while the ARL module is trained using three objectives: i) minimizing the reconstruction loss $L_{\text{recon}}$, which is the mean $\ell_{2}$ loss between the unmodified image representations $E_i(\mathbf{I})$ and the modified protected attribute information-free $\overline{\phi}(\mathbf{I})$, ii) the $L_{\text{ent}}^{\{r,g,a\}}$ to minimize the maximum soft-max probability for each protected attribute classifier head, and iii) $L_{ce}^{r,g,a}$, to maximize the misclassification of the protected label by the PAC module.

\begin{align}
    \begin{split}
L_{\text{ARL}} &= w_{\text{recon}}\cdot L_{\text{recon}} + w_{\text{ent}}\cdot (L_{\text{ent}}^r + L_{\text{ent}}^g + L_{\text{ent}}^a) \\
& -(w_{\text{ce}}^r\cdot L^r_{\text{ce}} + w_{\text{ce}}^a\cdot L^a_{\text{ce}} + w_{\text{ce}}^g\cdot L^g_{\text{ce}}),
    \end{split}
    \label{eqn:arl_objective}
\end{align}
 where $L_{\text{recon}} {=} \|E_i(\mathbf{I}) {-} \overline{\phi}(\mathbf{I})\|_2$, $L_{\text{ent}}^x {=} \max (\sigma_{\text{softmax}}(\hat{y_{x}}))$ (for PA $x$), $\sigma_{\text{softmax}}(x) = \text{Softmax}(x)$, and the $w_i$'s are the regularization weights that assist the training of \method across multiple protected attributes.

The negative cross-entropy losses aim to maximize the cross-entropy loss for the PAC. However, when used alone, this loss results in the protected label predictions getting flipped (from one label to another) rather than reducing the decision confidence of the PAC. The $L_{\text{ent}}$ ensures that the PAC module produces high-entropy logits from the modified image representation $\overline{\phi}(\mathbf{I})$ so that it cannot be easily used for predicting the protected attributes with high confidence. Figure~\ref{fig:dear-framework} describes the operation of the \method framework in detail. The success of the network is evaluated by the degree to which the i) bias in the model is mitigated and ii) zero-shot performance of the model is retained.
