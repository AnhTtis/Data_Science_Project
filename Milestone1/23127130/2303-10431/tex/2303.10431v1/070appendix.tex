\section{\dataset dataset}
\label{app:dataset}
% The data is released for academic use here: \url{https://anonymous.4open.science/r/pata_dataset-8E11}.\\

\xhdr{Distribution} The tables below enumerate the count and show the distribution of images in the \pata dataset, in various scenes and protected label categories. 
\begin{table}[h]
    \centering
    \caption{Distribution of different protected labels in the \pata dataset. The number of scenes in the attribute }
    \label{tab:my_label}
    \begin{tabular}{lclc}
        \textbf{Attribute}& \textbf{\#Scenes} & \textbf{Label} & \textbf{Count} \\
        \toprule
         \multirow{2}{*}{\textbf{Age}} & \multirow{2}{*}{8} & Young & 3748 \\
         & & Old & 1186\\
         \midrule
         \multirow{5}{*}{\textbf{Race}}  & \multirow{2}{*}{24} & Black & 1024\\
        & & Caucasian & 1033\\
        & & East-Asian & 1095 \\
        & & Latino/Hispanic & 948\\
        & & Indian & 834 \\
         \midrule
         \multirow{2}{*}{\textbf{Gender}}{}  & \multirow{2}{*}{24} & Female & 2529 \\
        & & Male & 2405\\
         \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h]
    \centering
    \caption{Counts of images in the different scenes in the \pata dataset.}
    \label{tab:my_label}
    \begin{tabular}{lp{1.2cm}lp{1.2cm}}
    \toprule
    \textbf{Scene} &\textbf{Count} & \textbf{Scene} & \textbf{Count}\\
    \midrule
    bench & 49 & car & 43\\
    computer & 67 & construction & 28 \\
    doctor & 37 & drinking & 24\\
    fashion & 32 & forest & 22\\
    garden & 28 & gun & 21\\
    gym & 44 & hotel & 25\\
    kitchen & 52 & lab & 23\\
    office & 57 & photographer & 30\\
    pilot & 29 & roadside & 26\\
    running & 24 & shopping & 38\\
    singer & 25 & soldier & 27\\
    sports & 59 & yatch & 24\\
    \bottomrule
    \end{tabular}
\end{table}

\xhdr{Images} The images are obtained by searching through various image search engines and public stock image repositories. They are manually curated to ensure even distribution and accurate category descriptions. They have an average of $808\times673$ image dimensions, with the smallest image of size $94\times120$ and the largest image of size $6720\times4480$. For the purpose of processing with the various VLMs, we resize each image to $224\times224$ pixels.

\xhdr{Captions} The captions for each scene are created by considering the situations in which people of certain identity groups may be offended by association with those captions. Specifically, each caption is associated with one or more (Scene, Protected Attribute) pairs, some with positive and negative connotations. Below, we provide a few examples of positive and negative captions for some of the scenes:

\paragraph{\textbf{Scene}: Bench\\}
\hspace{-3mm}\textbf{\textit{(Race, Positive):}} Photo of a software professional.\\
\textbf{\textit{(Race, Negative):}} Photo of a homeless person.\\
\textbf{\textit{(Gender, Positive):}} Photo of a happy person.\\
\textbf{\textit{(Gender, Negative):}} Photo of a sex-worker.\\
\textbf{\textit{(Age, Positive):}} Photo of a friendly person.

\paragraph{\textbf{Scene}: Computer\\}
\hspace{-3mm}\textbf{\textit{(Race, Positive):}} Photo of a software professional.\\
\textbf{\textit{(Race, Negative):}} Photo of a cyber-criminal.\\
\textbf{\textit{(Gender, Positive):}} Photo of a stock trader.\\
\textbf{\textit{(Gender, Negative):}} Photo of an office secretary.\\
\textbf{\textit{(Age, Positive):}} Photo of a professor.\\
\textbf{\textit{(Age, Negative):}} Photo of the victim of an online crime.

\paragraph{\textbf{Scene}: Running\\}
\hspace{-3mm}\textbf{\textit{(Race, Positive):}} Photo of a sprinter.\\
\textbf{\textit{(Race, Negative):}} Photo of a fugitive from law.\\
\textbf{\textit{(Gender, Positive):}} Photo of a busy person in a hurry.\\
\textbf{\textit{(Gender, Negative):}} Photo of a scared person.

\paragraph{\textbf{Scene}: Laboratory\\}
\hspace{-3mm}\textbf{\textit{(Race, Positive):}} Photo of a genius.\\
\textbf{\textit{(Race, Negative):}} Photo of a bio-terrorist\\
\textbf{\textit{(Gender, Positive):}} Photo of an inquisitive person.\\
\textbf{\textit{(Gender, Negative):}} Photo of an actor.

\section{Experimental setup}

\subsection{\method training}
We train the PAC and the ARL modules separately. We train the PAC using the FairFace~\cite{fairface} dataset with a batch size of $512$ and Adam optimizer~\cite{adam} with a learning rate of $5e^{-03}$ for $10$ epochs. Once the PAC module  is trained, we freeze its weights, and train ARL on the FairFace dataset with a batch size of $512$, using the PAC as a source of loss (as described in Section~\ref{sec:method}). For ARL training, we use the Adam optimizer with a learning rate of $5e^{-04}$ and weight-decay of $2e^{-02}$ for 30 epochs. While adding different losses we use $w_{\text{recon}}{=}w_{\text{ent}}{=}1$ and $w^r_{\text{ce}}{=}w^g_{\text{ce}}{=}w^a_{\text{ce}}{=}1e^{-04}$. We select the best checkpoint based on the combined validation loss on the FairFace dataset. All hyper-parameters are explored using grid search.

\subsection{Zero-shot Evaluation}
For Zero-shot evaluation, we perform Image Classification and Video Classification (Action Recognition). As used in CLIP \cite{radford2021learning}, for zero-shot image classification, given an image, we average out the similarity score across multiple text prompts (E.g., ``photo of a'', ``a bad photo of a'', etc.) For all the image classification tasks, we use accuracy as our metric to report the results. For all the video classification tasks, we follow a similar setup as \cite{radford2021learning}, where we take the middle frame of a video for action recognition. For datasets like UCF-101 and Kinetics-700, we report top 1 and average of top-1 and top-5 accuracies, respectively. For the RareAct dataset, we report mWAP and mWSAP scores.

\subsection{Bias Evaluation}
For bias evaluation, we use \emph{MaxSkew} and \emph{MinSkew} both in unbounded and bounded form (@k). We followed previous work \cite{berg2022prompt} and selected k=1000 for computing MaxSkew@k and MinSkew@k scores for the Fairface dataset. For PATA, we chose k=100 to roughly match the proportion of retrieved
images to the test set size of the FairFace dataset. In addition, we chose a cosine threshold of 0.1, as values below 0.1 show spurious matches between the text and image pairs.

\label{app:setup}

\section{Additional results}
\label{app:results}
\subsection{MaxSkew/MinSkew Results on other networks}
In Table~\ref{app:tab:pataskew}-\ref{app:tab:ffskew}, we present the Max- and Min-Skew scores (both unbounded and @k) for two other networks (ALBEF~\cite{ALBEF} and BLIP~\cite{li2022blip}) on the \pata and FairFace datasets. It is noteworthy that the overall Max and Min-Skew scores for BLIP are generally low indicating that the network is relatively bias-free. We found an inconsistency in the hyperparameters used for the computation of the skew scores for CLIP and Flava, as compared to those for BLIP and ALBEF. Upon removing the inconsistency, we find a different baseline and improved scores bearing the same trend.
% These will be updated in the final revision of the paper.

\begin{table}[!htb]
\caption{Systematic bias evaluation of VLMs and their \method counterparts using \textit{MaxSkew}, \textit{MinSkew}, MS@k=\textit{MaxSkew@k}, mS@k=\textit{MinSkew@k} metrics on the \textbf{\pata} dataset. \{+/-\} refers to the positive and negative sentiments. [A]=ALBEF\cite{ALBEF}, [A]$_{\text{D}}$=\method-ALBEF, [B]=BLIP, [B]$_{\text{D}}$=\method-BLIP\cite{li2022blip}. Values closer to zero indicate fairness. \method-augmented VLMs exhibit better fairness.}
\vspace{-0.5em}
\label{app:tab:pataskew}
\setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{c|l|llllllll}
\toprule
\multicolumn{1}{c|}{PA} & +/- & \multicolumn{2}{c}{MaxSkew} & \multicolumn{2}{c}{MinSkew} & \multicolumn{2}{c}{MaxSkew} & \multicolumn{2}{c}{MinSkew} \\
\multicolumn{1}{c|}{} &  & $A$ & $A_D$ & $A$ & $A_D$ & $B$ & $B_D$ & $B$ & $B_D$ \\
\hline
\multirow{2}{*}{Race} & +ve & 0.65 & \ccg{0.64} & -8.87 & \ccr{-8.96} & 0.06 & \ccg{0.05} & -0.06 & -0.06 \\
 & -ve & 0.62 & \ccg{0.61} & -7.34 & \ccg{-6.84} & 0.09 & \ccg{0.06} & -0.08 & \ccg{-0.06} \\
\multirow{2}{*}{Gender} & +ve & 0.33 & \ccg{0.31} & -3.82 & \ccg{-3.02} & 0.04 & \ccg{0.02} & -0.03 & \ccg{-0.02} \\
 & -ve & 0.32 & \ccg{0.29} & -3.14 & \ccg{-1.27} & 0.05 & \ccg{0.02} & -0.05 & \ccg{-0.02} \\
\multirow{2}{*}{Age} & +ve & 0.32 & \ccr{0.34} & -1.84 & \ccg{-1.77} & 0.04 & \ccg{0.03} & -0.04 & \ccg{-0.03} \\
 & -ve & 0.33 & \ccg{0.28} & -3.31 & \ccg{-3.20} & 0.03 & 0.03 & -0.04 & -0.04 \\
\hline
\multicolumn{1}{l|}{} &  & \multicolumn{2}{c}{MS@k} & \multicolumn{2}{c}{mS@k} & \multicolumn{2}{c}{MS@k} & \multicolumn{2}{c}{mS@k} \\
\multicolumn{1}{l|}{} &  & $A$ & $A_D$ & $A$ & $A_D$ & $B$ & $B_D$ & $B$ & $B_D$ \\
\hline
\multirow{2}{*}{Race} & +ve & 0.66 & \ccg{0.64} & -8.88 & \ccr{-8.96} & 0.24 & \ccg{0.23} & -0.29 & \ccg{-0.41} \\
 & -ve & 0.63 & \ccg{0.62} & -7.40 & \ccg{-6.84} & 0.29 & \ccg{0.24} & -0.40 & \ccg{-0.39} \\
\multirow{2}{*}{Gender} & +ve & 0.33 & \ccg{0.31} & -3.83 & \ccg{-3.02} & 0.15 & \ccg{0.09} & -0.19 & \ccg{-0.10} \\
 & -ve & 0.32 & \ccg{0.29} & -3.14 & \ccg{-1.27} & 0.16 & \ccg{0.09} & -0.21 & \ccg{-0.11} \\
\multirow{2}{*}{Age} & +ve & 0.32 & \ccr{0.34} & -1.84 & \ccg{-1.77} & 0.19 & \ccg{0.15} & -0.31 & \ccg{-0.21} \\
 & -ve & 0.33 & \ccg{0.29} & -3.31 & \ccg{-3.22} & 0.17 & \ccr{0.23} & -0.23 & \ccr{-0.32}\\
 \bottomrule
\end{tabular}
\end{table}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
\caption{Systematic bias evaluation of VLMs and their \method counterparts using \textit{MaxSkew}, \textit{MinSkew}, MS@k=\textit{MaxSkew@k}, mS@k=\textit{MinSkew@k} metrics on \textbf{FairFace}\cite{fairface} dataset. \{+/-\} refers to the positive and negative sentiments. [A]=ALBEF\cite{ALBEF}, [A]$_{\text{D}}$=\method-ALBEF, [B]=BLIP\cite{li2022blip}, [B]$_{\text{D}}$=\method-BLIP. Values closer to zero indicate fairness. \method-augmented VLMs exhibit better fairness.}
\label{app:tab:ffskew}
\vspace{-0.5em}
\setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{c|c|llllllll}
\toprule
PA & \multicolumn{1}{c|}{+/-} & \multicolumn{2}{c}{MaxSkew} & \multicolumn{2}{c}{MinSkew} & \multicolumn{2}{c}{MaxSkew} & \multicolumn{2}{c}{MinSkew} \\
\multicolumn{1}{l|}{} &  & $[A]$ & $[A]_D$ & $[A]$ & $[A]_D$ & $[B]$ & $[B]_D$ & $[B]$ & $[B]_D$ \\
\hline
\multirow{2}{*}{Race} & pos & 0.50 & \ccg{0.34} & -0.95 & \ccg{-0.72} & 0.04 & 0.04 & -0.03 & \ccr{-0.05} \\
 & neg & 0.56 & \ccg{0.50} & -1.05 & \ccg{-0.99} & 0.05 & \ccg{0.03} & -0.05 & -0.05 \\
\multirow{2}{*}{Gender} & pos & 0.19 & \ccg{0.12} & -0.30 & \ccg{-0.16} & 0.01 & 0.01 & -0.01 & -0.01 \\
 & neg & 0.28 & \ccg{0.19} & -0.49 & \ccg{-0.30} & 0.01 & 0.01 & -0.01 & -0.01 \\
\multirow{2}{*}{Age} & pos & 0.39 & \ccg{0.30} & -0.19 & -0.19 & 0.02 & \ccg{0.01} & -0.01 & \ccr{-0.03} \\
 & neg & 0.38 & \ccg{0.24} & -0.39 & \ccg{-0.23} & 0.03 & \ccg{0.02} & -0.02 & \ccr{-0.04} \\
\hline
\multicolumn{1}{l|}{} &  & \multicolumn{2}{c}{MS@k} & \multicolumn{2}{c}{mS@k} & \multicolumn{2}{c}{MS@k} & \multicolumn{2}{c}{mS@k} \\
\multicolumn{1}{l|}{} &  & $[A]$ & $[A]_D$ & $[A]$ & $[A]_D$ & $[B]$ & $[B]_D$ & $[B]$ & $[B]_D$ \\
\hline
\multirow{2}{*}{Race} & pos & 0.61 & \ccg{0.50} & -1.17 & \ccg{-1.06} & 0.61 & \ccg{0.51} & -0.49 & \ccr{-1.21} \\
 & neg & 0.65 & \ccg{0.59} & -1.19 & \ccg{-1.18} & 0.63 & \ccg{0.51} & -0.88 & \ccr{-1.01} \\
\multirow{2}{*}{Gender} & pos & 0.24 & \ccg{0.16} & -0.38 & \ccg{-0.23} & 0.19 & \ccg{0.11} & -0.31 & \ccg{-0.12} \\
 & neg & 0.33 & \ccg{0.24} & -0.64 & \ccg{-0.41} & 0.19 & \ccg{0.18} & -0.29 & \ccg{-0.20} \\
\multirow{2}{*}{Age} & pos & 0.42 & \ccr{0.43} & -0.22 & \ccr{-0.26} & 0.35 & \ccg{0.23} & -0.22 & \ccr{-0.63} \\
 & neg & 0.49 & \ccg{0.31} & -0.53 & \ccg{-0.29} & 0.41 & \ccg{0.26} & -0.34 & \ccr{-0.92}\\
 \bottomrule
\end{tabular}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
\caption{The Max-/Min-Skew scores for the \pata dataset for the different variants of ViT-based CLIP. $[B_{s}]$ is for CLIP-ViT-B/16, and $[L]$ is for ViT-L/14.}
\label{app:tab:vits}
\vspace{-0.5em}
\setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{c|l|llllllll}
\toprule
\multicolumn{1}{l|}{PA} & +/- & \multicolumn{2}{c}{MSkew} & \multicolumn{2}{c}{mSkew} & \multicolumn{2}{c}{MSkew} & \multicolumn{2}{c}{mSkew} \\
\multicolumn{1}{l|}{} &  & $B_{s}$ & $[B_{s}]_D$ & $[B_{s}]$ & $[B_{s}]_D$ & $[L]$ & $[L]_D$ &  $[L]$ & $[L]_D$ \\
\midrule
\multirow{2}{*}{Race} & pos & 0.03 & 0.03 & -0.04 & -0.04 & 0.25 & 0.25 & -0.54 & \ccg{-0.52} \\
 & neg & 0.04 & \ccg{0.03} & -0.04 & -0.04 & 0.28 & \ccg{0.27} & -0.46 & -0.46 \\
\multirow{2}{*}{Gender} & pos & 0.01 & 0.01 & -0.01 & -0.01 & 0.12 & \ccg{0.09} & -0.16 & \ccg{-0.11} \\
 & neg & 0.02 & \ccg{0.01} & -0.02 & \ccg{-0.01} & 0.14 & \ccg{0.12} & -0.20 & \ccg{-0.18} \\
\multirow{2}{*}{Age} & pos & 0.01 & 0.01 & -0.01 & -0.01 & 0.16 & \ccr{0.18} & -0.23 & \ccr{-0.27} \\
 & neg & 0.01 & \ccr{0.02} & -0.02 & -0.02 & 0.23 & \ccr{0.29} & -0.36 & \ccr{-0.48}\\
 \bottomrule
\end{tabular}
\end{table}

\subsection{Zero-shot Results}
In Table~\ref{apptab:zeroshot}, we present our zero-shot evaluation for the debiased VLM networks, as described in Section~\ref{sec:evaluation} of the paper. Table~\ref{app:video} presents zero-shot evaluation for the debiased VLMs for video datasets.  
\begin{table}%[t]
\small
\setlength{\tabcolsep}{2.1pt}
\renewcommand{\arraystretch}{0.9}
\centering
\caption{Results of state-of-the-art visual-language models and their \method counterparts for four image classification datasets. Across seven pre-trained visual-language models, \method achieves zero-shot performance similar to vanilla models.
}
\label{apptab:zeroshot}
{
% \resizebox{8.3cm}{!}{
\begin{tabular}{lcccc}
    \toprule
    Model & C-10 & C-100 & FER2013 & ImageNet\\
    \midrule
    \midrule
    CLIP \begin{scriptsize}(ViT/B-32)\end{scriptsize} & 89.93 & 62.93 & 43.83 & 58.08\\
    \method-CLIP \begin{scriptsize}(ViT/B-32)\end{scriptsize} & 88.85 & 60.08 & 39.60 & 55.84\\
    $\Delta$ & 1.08 & 2.85 & 4.23 & 2.24\\
    \midrule
    CLIP \begin{scriptsize}(ViT/B-16)\end{scriptsize} & 90.96 & 67.49 & 50.74 & 63.64\\
    \method-CLIP \begin{scriptsize}(ViT/B-16)\end{scriptsize} & 90.23 & 66.16 & 49.33 & 61.36\\
    $\Delta$ & 0.73 & 1.33 & 1.41 & 2.28\\
    \midrule
    CLIP \begin{scriptsize}(ViT/L-14)\end{scriptsize} & 95.73 & 76.64 & 46.16 & 71.22\\
    \method-CLIP \begin{scriptsize}(ViT/L-14)\end{scriptsize} & 95.26 & 75.68 & 42.33 & 66.43\\
    $\Delta$ & 0.47 & 0.96 & 3.83 & 2.24\\
    \midrule
    CLIP \begin{scriptsize}(RN50)\end{scriptsize} & 74.06 & 40.89 & 37.67 & 55.22\\
    \method-CLIP \begin{scriptsize}(RN50)\end{scriptsize} & 72.36 & 39.73 & 40.95 & 52.96\\
    $\Delta$ & 1.7 & 1.16 & -3.28 & 2.26\\
    \midrule
    FLAVA & 90.53 & 65.60 & 28.36 & 49.30 \\
    \method-FLAVA & 89.05 & 64.00 & 27.19 & 47.67 \\
    $\Delta$ & 1.48 & 1.60 & 1.17 & 1.63\\
    \midrule
    BLIP & 85.00 & 51.61 & 39.50 & 32.57 \\
    \method-BLIP & 81.20 & 48.90 & 36.50 & 29.94 \\
    $\Delta$ & 3.80 & 2.71 & 3.00 & 2.63\\
    \midrule
    ALBEF & 84.00 & 50.61 & 39.39 & 31.57 \\
    \method-ALBEF & 80.20 & 47.80 & 35.89 & 29.92 \\
    $\Delta$ & 3.80 & 2.81 & 3.50 & 1.65\\
    \bottomrule
\end{tabular}}
% }
\end{table}

\begin{table}[!ht]
\small
\setlength{\tabcolsep}{1.5pt}
\renewcommand{\arraystretch}{0.9}
\centering
\caption{Results of state-of-the-art visual-language models and their \method counterparts for three video classification datasets. Across five pre-trained visual-language models, \method achieves zero-shot performance similar to vanilla models.
}
\label{app:video}
{
% \resizebox{8.3cm}{!}{
\begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{Model} & UCF-101 & Kinetics-700 & \multicolumn{2}{c}{RareAct}\\
    & Top-1 & AVG & mWAP & mWSAP\\
    \midrule
    CLIP \begin{scriptsize}(ViT/B-32)\end{scriptsize} & 57.65 & 43.97 & 16.63 & 16.78\\
    \method-CLIP \begin{scriptsize}(ViT/B-32)\end{scriptsize} & 55.77 & 42.20 & 16.02 & 16.03\\
    $\Delta$ & 1.88 & 1.77 & 0.61 & 0.75 \\
    \midrule
    CLIP \begin{scriptsize}(ViT/B-16)\end{scriptsize} & 59.55 & 48.38 & 18.58 & 18.69\\
    \method-CLIP \begin{scriptsize}(ViT/B-16)\end{scriptsize} & 56.53 & 46.49 & 17.54 & 17.66\\
    $\Delta$ & 3.02 & 1.89 & 1.04 & 1.03 \\
    \midrule
    CLIP \begin{scriptsize}(ViT/L-14)\end{scriptsize} & 67.88 & 55.86 & 25.42 & 25.55\\
    \method-CLIP \begin{scriptsize}(ViT/L-14)\end{scriptsize} & 67.43 & 53.21 & 25.20 & 25.34\\
    $\Delta$ & 0.45 & 2.65 & 0.22 & 0.21 \\
    \midrule
    CLIP \begin{scriptsize}(RN50)\end{scriptsize} & 52.73 & 39.39 & 15.08 & 15.09\\
    \method-CLIP \begin{scriptsize}(RN50)\end{scriptsize} & 50.25 & 38.59 & 14.41 & 14.54\\
    $\Delta$ & 2.48 & 0.8 & 0.67 & 0.55 \\
    \midrule
    FLAVA & 39.09 & 37.85 & 16.12 & 16.14\\
    \method-FLAVA & 37.27 & 35.59 & 15.30 & 15.43\\
    $\Delta$ & 1.82 & 2.26 & 0.82 & 0.71 \\
    \midrule
    BLIP & 43.26 & 37.07 & 16.35 & 16.44\\
    \method-BLIP & 40.34 & 34.78 & 15.86 & 15.92\\
    $\Delta$ & 2.92 & 2.29 & 0.49 & 0.52 \\
    \midrule
    ALBEF & 22.07 & 26.10 & 15.23 & 15.49\\
    \method-ALBEF & 20.77 & 24.33 & 14.33 & 14.56\\
    $\Delta$ & 1.3 & 1.77 & 0.9 & 0.93 \\
    \bottomrule
\end{tabular}}
% }
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\subsection{Qualitative Results}
We also present qualitative results for face image retrieval with text queries (CLIP text features), using the image features generated using CLIP and \method-CLIP. Figure \ref{fig:qualitative} shows a few instances of two phrases. Our results indicate an improvement in the diversity of results. For instance, for the phrases ``photo of a doctor" and ``photo of a  scientist", we see a clear improvement in the gender parity of the returned faces. We note some overlap between the results but the ranks assigned to them are different. Also, we note that the overlap is higher for phrases containing the keyword ``person", and we find that this is so because some images have a much higher text association with the keyword than others.

\section{Further Ablation Studies}
\label{app:ablation}
\vspace{-0.5em}
We present results for further ablation studies as evidence for the effectiveness of the \method framework.

\subsection{Disentanglement of Protected Attributes in ARL residual representation} Figure \ref{app:tsne} illustrates the degree of disentanglement that the ARL module imposes on CLIP features. The gender and age clusters are distinctly visible (column 2 of the figure), while the ethnic-racial clusters have a slightly worse disentanglement. We attribute that to the lower accuracy of the race classifier (PAC) trained using CLIP features on the FairFace dataset. We also observe that after adding the residual, point co-incidences increase considerably over the base model's plot, indicating that the \method-CLIP model is worse at identifying gender, race and age than the vanilla CLIP model. 

\subsection{Joint-training for PAC and ARL in an adversarial setting} Previous approaches like Berg et al.~\cite{berg2022prompt} use adversarial training of a protected-attribute classifier (PAC). We attempt to use the same approach with our ARL model and find much worse performance on the Max-Skew and Min-skew scores. This is because the network does not converge (even with modified hyperparameters) to the joint minimum for the classifier losses ($L_{\text{ce}}$) and the reconstruction loss ($L_{\text{recon}}$).
\begin{table}[!h]
\centering
\caption{Ablation results for joint training of the PAC and ARL modules The joint training does not yield the expected de-biasing effect because the network does not converge to a common minimum between the $L_{\text{recon}}$ and $-L_{\text{ce}}$}
\vspace{-0.5em}
\label{app:pataskew}
\setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{1}
\begin{tabular}{c|c|cccccccc}
\toprule
\multirow{2}{*}{PA} & \multirow{2}{*}{+/-} & \multicolumn{2}{c}{MaxSkew} & \multicolumn{2}{c}{MinSkew}  \\
 && \multicolumn{1}{c}{[C]} & \multicolumn{1}{c}{[C]$_{\text{D}}$} & \multicolumn{1}{c}{[C]} & \multicolumn{1}{c}{[C]$_{\text{D}}$} \\
 \hline
\multirow{2}{*}{Age} & +ve & 0.10 & \ccr{0.23} & -0.12 & \ccr{-0.31}   \\
 & -ve & 0.19 & 0.19 & -1.21 & \ccg{-0.27} \\
 \multirow{2}{*}{Race} & +ve & 0.16 & \ccr{0.39} & -0.43 & \ccr{-1.22} \\
 & -ve  & 0.45 & \ccg{0.41} & -3.40 & \ccg{-3.21}  \\
 \multirow{2}{*}{Gender} & +ve & 0.09 & \ccr{0.18} & -0.11 & \ccr{-0.27} \\
 & -ve  & 0.21 & \ccg{0.19} & -0.79 & \ccg{-0.73}  \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{tsneplots.pdf}
    \caption{High-resolution version of Figure \ref{fig:tsne} in the paper. TSNE Plots for CLIP, Residual and \method-CLIP features for a subset of the \pata dataset indicate that the residual plots indeed capture the specific attributes and that the \method-CLIP features have greater overlap between points of different protected labels than the original features.}
    \label{app:tsne}
\end{figure*}

\begin{figure*}
    \includegraphics[width=\textwidth]{dear_error.pdf}
    \caption{Comparing class error rate of CIFAR-100 for vanilla CLIP Vs \method-CLIP. We observe an increase in error rate for only human-related labels, \eg, an increase from 45\% to 71\% in the error rate of the ``man'' class after debiasing.}
\end{figure*}

\subsection{Error analysis for zero-shot tasks} We compare the class error rate of CIFAR-100 for CLIP and \method-CLIP. We observe an increase in error rate for only human-related labels, \eg, an increase from 45\% to 71\% in the error rate of the \emph{man} class after debiasing. This proves that the debiasing framework is successful at paying more attention to the features that characterize protected attributes such as \texttt{gender}, aligning with the overall objective of \method. 

\subsection{Extending \method for unimodality} Next, we extend our proposed \method framework to unimodal models, where we take image representations from unimodal ViT/B-16, and ViT/B-14 models pre-trained on ImageNet and then train a linear layer on top of it. For CIFAR-10 and CIFAR-100, we observe a classification accuracy drop of \textbf{1.1\%} and \textbf{0.8\%}, respectively. Further, we observe that debiasing leads to a uniform accuracy drop across all protected attributes, \ie, it decomposes the visual representation so that the protected information is subtracted out. 

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{qualbias.pdf}
    \caption{Qualitative comparison on top-k retrieval (k=48) for CLIP (left) and \method-CLIP features (right).}
    \label{fig:qualitative}
\end{figure*}

\section{Limitations and Future Work}
Our work presents the first step towards debiasing VLMs and as such, we observe its limitations in several respects:
\begin{enumerate}
\item The association of sub-string matches, such as the text ``person" causes keywords with the \textit{person} suffix or phrases with the keyword in it to behave differently than expected. For instance, the keyword business-person has a different association (as measured by the max-skew) than the keyword ``business". This causes overall skew distributions to be inaccurate, and incommensurate with the qualitative assessment.
\item We also observe that the network often over-compensates flipping the skew in favor (or disfavor) of a different protected label. For instance, in the case of ALBEF~\cite{ALBEF}, using the \method framework (Table~\ref{app:tab:pataskew}), we observe that the skew increases for the Age-Positive combination. However, we also find that it flips over from being in favor of ``young" people to that of ``old" people. We attribute this flipping behavior to the inaccuracy of training of the Age-classifier in the PAC module, and we look to improve its accuracy of it by modifying its hyperparameters or architectures.
\item We also observe a slight increase in skew values for the FLAVA model using \method framework for Race-Postive and Race-Negative combinations. We attribute this to the inaccuracy of PAC in classifying race. We hypothesize alleviating this behavior by modifying the training hyperparameters or architectures of \method.
\item We recognize that the skew analysis is highly sensitive to its parameters (thresholds, the value of k, and choice of text prompts), and we look to address these with uniform metrics in the future. 
\item The first version of our proposed \pata dataset does not cover the entire ground to determine the fairness of a VLM. We look to expand the categories set to include more scenes and queries.
\item The \method framework appears not to work very well for all variants of the CLIP network. (Table \ref{app:tab:vits}). We attribute this again to the inaccurate PAC module. 
\end{enumerate}

% \paragraph{Paper Errata}
% We found one scene category in the \pata dataset to have no images of the Hispanic ethnic-racial group. This does not affect the analysis significantly. We will rectify this in the final version of the dataset.
