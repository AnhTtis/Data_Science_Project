We formally define the notion of fairness for VLMs, stipulate the fairness problem, and motivate the use of an additive residual for bias mitigation in VLMs. 

\xhdr{Visually Discernible Protected Attributes (VDPAs)} A \textit{protected attribute} (PA) is defined as any characteristic of a group of people associated with their collective identity. A subset of these attributes are visually discernible, \ie, it's possible to label an image of a person belonging to a particular identity group with the corresponding protected label (PL). Our study includes the i) the binary {\small\texttt{gender}} of a person, ii) the ethno-racial characteristics (skin color, face shape, hair color, etc.) captured as {\small\texttt{race}}, and iii) the {\small\texttt{age}} of a person. For simplicity, we associate a single ground-truth VDPA label for an image $\mathbf{I}_{i}$, which has a gender label $y^{g}_{i}$, a race label $y^{r}_{i}$ and an age label $y^{a}_{i}$. We denote a protected attribute as $P(\mathbf{I})$ with the corresponding labels as a set \{${p_i}$\}.

\xhdr{Abstract Model of Vision-Language Models} An abstract VLM consists of two key components: i) an image encoder $E_i$ from the space of a 2D, 3-channel image to a latent space in $\mathbb{R}^d$ and ii) a text encoder $E_t$ from the space of text tokens to a similar latent space in $\mathbb{R}^d$. For an image $\mathbf{I}$ and text token sequence $\mathbf{T}$, the output of the image and text encoders are denoted as $E_i(\mathbf{I})$ and $E_t(\mathbf{T})$. Finally, a fusion function combines the encoded representations from $E_i$ and $E_t$ into a common representation, which we disregard because of the manner in which they compute similarity.

\xhdr{Similarity of Image and Text Representations} Most large pre-trained VLMs~\cite{radford2018improving,li2022blip,singh2022flava} learn image and text representations that are in a similar latent space. We compute the similarity between an image ($\mathbf{I}$) and a text sequence ($\mathbf{T}$) using the cosine similarity between their representations:
\begin{align}
    \begin{split}
        \text{Sim}_E(\mathbf{I}, \mathbf{T}) &= \frac{E_i(\mathbf{I})\cdot E_t(\mathbf{T})^\top}{||E_i(\mathbf{I})||~ ||E_t(\mathbf{T})||},
    \end{split}
    \label{eqn:similarity}
\end{align}

Formally, the VLM encoders are fair for a protected attribute if the similarity between an image $\mathbf{I}$ and a text sequence $\mathbf{T}$ is independent of the protected label for $\mathbf{I}$. It is formalized by measuring the skew of the similarity scores of $\mathbf{T}$ with a set of images, across their protected labels.\footnote{The formulation used here is adapted from \cite{maxSkew}.}

\xhdr{Fairness and Fair Encoders} We denote the probability of randomly selecting an image $\mathbf{I}$ from a set of images $\mathcal{I}$ such that it has a protected attribute label $P(\mathbf{I})=p_i$ with $f_i$. Next, we define a matching function $M_\epsilon(\mathbf{I}, \mathbf{T})$ which is 1 if $\text{Sim}_E(\mathbf{I}, \mathbf{T}) \geq \epsilon$, and 0 otherwise. 

We define $\mathcal{I}^m$ as the subset of $\mathcal{I}$ for which $M_\epsilon(\mathbf{I}, \mathbf{T})$ is 1 for each $\mathbf{I}\in\mathcal{I}^m$ and the fraction of these images that also have the protected label $p_i$ is denoted by $f^m_i$.

The ratio of $f^m_i$ and $f_i$ is considered as the skew in the distribution of images with the protected label $p_i$ imposed by the model ($E_i, E_t$) by virtue of the similarity between its output representations. The \textit{Skew} measure for the $i^{th}$ protected label, with respect to the text sequence and image set, is then computed as:
\begin{align}
    \begin{split}
        \text{Skew}_i(\mathcal{I}, \mathbf{T}) &= \log (f^m_i / f_i),
    \end{split}
    \label{eqn:skew}
\end{align}
where the score is zero when $f^m_i = f_i$, \ie, the matching does not depend on the value of the protected attribute.

The goal of learning a fair encoding then consists of training encoders ($E_i^{*}$, $E_t^{*}$) such that $\text{Skew}_i(\mathcal{I}, \mathbf{T}) \approx 0$ for all text sequences $\mathbf{T}$, protected attributes $i$, and for any set of images $\mathcal{I}$. We posit that a model devoid of any information useful for distinguishing between different PLs for a PA is fair because encoding from such a model is independent of that particular PA. We submit that learning such a model with respect to a VLM is possible by adapting the visual encoder of the VLM. This is because the encoding from it captures rich information in a compact form, and it is easier to find training data to adapt the image encoder than it is for the text encoder. We posit that adapting just the image encoder can produce a fair model.  We corroborate this position next with empirical evidence.

\xhdr{Disentangling Protected Attribute Information} To motivate the design of \method, we present empirical evidence for the bias problem using the {\small\texttt{gender}} attribute. We show that $E_i(\mathbf{I})$ can be split into two additive components - i) $\phi(\mathbf{I})$ that represents the gender information for $\mathbf{I}$ and ii) a protected-attribute (PA) free representation $\overline{\phi}(\mathbf{I})$ that cannot be used to identify the gender of the person in the image:
\begin{align}
    \begin{split}
        E_i(\mathbf{I}) &= \overline{\phi}(\mathbf{I}) + \phi(\mathbf{I})
    \end{split}
\end{align}

For the protected labels {\small{\texttt{male(m)} and \texttt{female(f)}}}, we get:
\begin{eqnarray}
    E_i(\mathbf{I}_{f}) = \overline{\phi}(\mathbf{I}_{f}) + \phi(\mathbf{I}_{f}) \\
    E_i(\mathbf{I}_{m}) = \overline{\phi}(\mathbf{I}_{m}) + \phi(\mathbf{I}_{m}) \\
    \Rightarrow E_i(\mathbf{I}_{f}) - E_i(\mathbf{I}_{m}) = \phi(\mathbf{I}_{f}) - \phi(\mathbf{I}_{m}),
    \label{eqn:diffvec}
\end{eqnarray}
where $\overline{\phi}(\mathbf{I}_{f}){=}\overline{\phi}(\mathbf{I}_{m})$ as both contain the PA free representation based on the context in the image. Further, we hypothesize that $\phi(\mathbf{I})$ can be adequately captured for a VLM by the corresponding text encoding for the equivalent textual description ``photo of a man/woman" ($t_{m}$/$t_{f}$) using a linear transformation. This is expressed as:
\begin{align}
    \begin{split}
      \phi(\mathbf{I}_f) &= \mathbf{A}\cdot 
      E_t(t_{f}) + \mathbf{B}\\
      \phi(\mathbf{I}_m) &= \mathbf{A}\cdot E_t(t_{m}) + \mathbf{B},
    \end{split}
    \label{eqn:linear}
\end{align} 
where $\mathbf{A} \in \mathbb{R}^{d \times d}$ is a transformation matrix and $\mathbf{B} \in \mathbb{R}^{d}$ is a vector of the same dimensions as the representations. Combining Eqns.~\ref{eqn:diffvec} and \ref{eqn:linear}, we get:
\begin{align}
    \begin{split}
    E_i(\mathbf{I}_{f}) - E_i(\mathbf{I}_{m}) = \mathbf{A}\cdot &E_t(t_{f}) - \mathbf{A}\cdot E_t(t_{m})\\
    \Rightarrow \mathbf{K}[E_i(\mathbf{I}_{f}) - E_i(\mathbf{I}_{m})] = E_t&(t_{f}) - E_t(t_{m}),
    \end{split}
    \label{eqn:diffequiv}
\end{align}
where $\mathbf{K}=\mathbf{A}^{-1}$. If we can find evidence that such a linear transformation $\mathbf{K}$ of the difference of image representation exists that matches the difference between the encoding of the corresponding text concepts, then we can learn a linear function that disentangles $\phi(\mathbf{I})$ from $E_i(\mathbf{I})$. 

\looseness=-1
\noindent \textit{Empirical evidence:} We train a regression model on 10000 random pairs of images of women and men in a professional workplace setting (a subset of the FairFace~\cite{fairface} dataset), to learn a linear function from the difference of the CLIP image representations to the difference of the CLIP text representations for the ``photo of a woman'' and ``photo of a man'' phrases. We found the model to have a bounded relative mean-squared error (${<}10^{-6}$) over 100 repetitions of the experiment for unseen pairs, which demonstrates that a piece-wise linear function can possibly be trained to approximate $\phi(\mathbf{I})$. Next, we describe how our \method framework trains to approximate $\phi(\mathbf{I})$ for multiple protected attributes, and uses it to remove protected attribute information from $E_i(\mathbf{I})$. 