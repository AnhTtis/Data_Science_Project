@InProceedings{hurriyetoglu-CLEF,
author="H{\"u}rriyeto{\u{g}}lu, Ali
and Y{\"o}r{\"u}k, Erdem
and Y{\"u}ret, Deniz
and Yoltar, {\c{C}}a{\u{g}}r{\i}
and G{\"u}rel, Burak
and Duru{\c{s}}an, F{\i}rat
and Mutlu, Osman",
editor="Azzopardi, Leif
and Stein, Benno
and Fuhr, Norbert
and Mayr, Philipp
and Hauff, Claudia
and Hiemstra, Djoerd",
title="A Task Set Proposal for Automatic Protest Information Collection Across Multiple Countries",
booktitle="Advances in Information Retrieval",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="316--323",
abstract="We propose a coherent set of tasks for protest information collection in the context of generalizable natural language processing. The tasks are news article classification, event sentence detection, and event extraction. Having tools for collecting event information from data produced in multiple countries enables comparative sociology and politics studies. We have annotated news articles in English from a source and a target country in order to be able to measure the performance of the tools developed using data from one country on data from a different country. Our preliminary experiments have shown that the performance of the tools developed using English texts from India drops to a level that are not usable when they are applied on English texts from China. We think our setting addresses the challenge of building generalizable NLP tools that perform well independent of the source of the text and will accelerate progress in line of developing generalizable NLP systems.",
isbn="978-3-030-15719-7"
}

@inProceedings{sonmez2016towards,
  title={Towards Building a Political Protest Database to Explain Changes in the Welfare State},
  author={S{\"o}nmez, {\c{C}}a{\u{g}}{\i}l and {\"O}zg{\"u}r, Arzucan and Y{\"o}r{\"u}k, Erdem},
  booktitle={Proceedings of the 10th ACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH 2016)},
  pages={106--110},
  year={2016},
  location="Berlin, Germany",
}


@inproceedings{ettinger-generalizability-st,
    title = "Towards Linguistically Generalizable {NLP} Systems: A Workshop and Shared Task",
    author = "Ettinger, Allyson  and
      Rao, Sudha  and
      Daum{\'e} III, Hal  and
      Bender, Emily M.",
    booktitle = "Proceedings of the First Workshop on Building Linguistically Generalizable {NLP} Systems",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-5401",
    doi = "10.18653/v1/W17-5401",
    pages = "1--10",
    abstract = "This paper presents a summary of the first Workshop on Building Linguistically Generalizable Natural Language Processing Systems, and the associated Build It Break It, The Language Edition shared task. The goal of this workshop was to bring together researchers in NLP and linguistics with a carefully designed shared task aimed at testing the generalizability of NLP systems beyond the distributions of their training data. We describe the motivation, setup, and participation of the shared task, provide discussion of some highlighted results, and discuss lessons learned.",
}

@inproceedings{Hu:2004:MSC:1014052.1014073,
 author = {Hu, Minqing and Liu, Bing},
 title = {Mining and Summarizing Customer Reviews},
 booktitle = {Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '04},
 year = {2004},
 isbn = {1-58113-888-1},
 location = {Seattle, WA, USA},
 pages = {168--177},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1014052.1014073},
 doi = {10.1145/1014052.1014073},
 acmid = {1014073},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {reviews, sentiment classification, summarization, text mining},
} 


@inproceedings{pang-lee-2005-seeing,
    title = "Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales",
    author = "Pang, Bo  and
      Lee, Lillian",
    booktitle = "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05)",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P05-1015",
    doi = "10.3115/1219840.1219855",
    pages = "115--124",
}

@incollection{word2vec-NIPS2013_5021,
title = {Distributed Representations of Words and Phrases and their Compositionality},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {3111--3119},
year = {2013},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}

@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@techreport{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  institution={Open AI},
  year={2019}
}

@inproceedings{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle={NeurIPS $EMC^2$ Workshop},
  year={2019}
}

@inproceedings{tenney-47786,
title	= {What do you learn from context? Probing for sentence structure in contextualized word representations},
author	= {Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R. Thomas McCoy and Najoung Kim and Benjamin Van Durme and Samuel R. Bowman and Dipanjan Das and Ellie Pavlick},
year	= {2019},
URL	= {https://openreview.net/forum?id=SJzSgnRcKX},
booktitle	= {International Conference on Learning Representations}
}

@inproceedings{peters-etal-2019-tune,
    title = "To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks",
    author = "Peters, Matthew E.  and
      Ruder, Sebastian  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4302",
    doi = "10.18653/v1/W19-4302",
    pages = "7--14",
    abstract = "While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.",
}

@article{Collobert:2011:NLP:1953048.2078186,
 author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
 title = {Natural Language Processing (Almost) from Scratch},
 journal = {J. Mach. Learn. Res.},
 issue_date = {2/1/2011},
 volume = {12},
 month = nov,
 year = {2011},
 issn = {1532-4435},
 pages = {2493--2537},
 numpages = {45},
 url = {http://dl.acm.org/citation.cfm?id=1953048.2078186},
 acmid = {2078186},
 publisher = {JMLR.org},
} 

@inproceedings{Mikolov-skip-gram-41224,
author = {Mikolov, Tomas and Corrado, G.s and Chen, Kai and Dean, Jeffrey},
year = {2013},
month = {01},
pages = {1-12},
title = {Efficient Estimation of Word Representations in Vector Space}
}

@inproceedings{pennington-etal-2014-glove,
    title = "{G}love: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@incollection{kiros-skipthought-NIPS2015_5950,
title = {Skip-Thought Vectors},
author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Russ R and Zemel, Richard and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {3294--3302},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5950-skip-thought-vectors.pdf}
}

@article{bojanowski-etal-2017-enriching,
    title = "Enriching Word Vectors with Subword Information",
    author = "Bojanowski, Piotr  and
      Grave, Edouard  and
      Joulin, Armand  and
      Mikolov, Tomas",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    url = "https://www.aclweb.org/anthology/Q17-1010",
    doi = "10.1162/tacl_a_00051",
    pages = "135--146",
    abstract = "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
}

@incollection{cove-NIPS2017_7209,
title = {Learned in Translation: Contextualized Word Vectors},
author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {6294--6305},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf}
}

@incollection{transformer-NIPS2017_7181,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@inproceedings{howard-ruder-2018-universal,
    title = "Universal Language Model Fine-tuning for Text Classification",
    author = "Howard, Jeremy  and
      Ruder, Sebastian",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1031",
    doi = "10.18653/v1/P18-1031",
    pages = "328--339",
    abstract = "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",
}

@incollection{xlnet-NIPS2019_8812,
title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {5754--5764},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf}
}

@inproceedings{
lan2020albert,
title={{\{}ALBERT{\}}: A Lite {\{}BERT{\}} for Self-supervised Learning of Language Representations},
author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1eA7AEtvS}
}

@article{liu2019roberta,
    title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
    author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and
              Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and
              Luke Zettlemoyer and Veselin Stoyanov},
    journal={arXiv:1907.11692},
    year = {2019},
}


@inproceedings{Beltagy2019SciBERT,
  title={SciBERT: Pretrained Language Model for Scientific Text},
  author={Iz Beltagy and Kyle Lo and Arman Cohan},
  year={2019},
  booktitle={EMNLP},
  Eprint={arXiv:1903.10676}
}

@inproceedings{zhang2019ernie,
  title={{ERNIE}: Enhanced Language Representation with Informative Entities},
  author={Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  booktitle={Proceedings of ACL 2019},
  year={2019}
}

@article{biobert-10.1093/bioinformatics/btz682,
    author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
    title = "{BioBERT: a pre-trained biomedical language representation model for biomedical text mining}",
    journal = {Bioinformatics},
    year = {2019},
    month = {09},
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btz682},
    url = {https://doi.org/10.1093/bioinformatics/btz682},
}

@inproceedings{liu-etal-2019-linguistic,
    title = "Linguistic Knowledge and Transferability of Contextual Representations",
    author = "Liu, Nelson F.  and
      Gardner, Matt  and
      Belinkov, Yonatan  and
      Peters, Matthew E.  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1112",
    doi = "10.18653/v1/N19-1112",
    pages = "1073--1094",
    abstract = "Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.",
}

@InProceedings{sun-finetune-bert-10.1007/978-3-030-32381-3_16,
author="Sun, Chi
and Qiu, Xipeng
and Xu, Yige
and Huang, Xuanjing",
editor="Sun, Maosong
and Huang, Xuanjing
and Ji, Heng
and Liu, Zhiyuan
and Liu, Yang",
title="How to Fine-Tune BERT for Text Classification?",
booktitle="Chinese Computational Linguistics",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="194--206",
abstract="Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.",
isbn="978-3-030-32381-3"
}

@article{hammond-machine-coded-event,
author = {Hammond, Jesse and Weidmann, Nils},
year = {2014},
month = {07},
pages = {},
title = {Using Machine-Coded Event Data for the Micro-Level Study of Political Violence},
volume = {1},
journal = {Research \& Politics},
doi = {10.1177/2053168014539924}
}

@article {wang-monitoring-societal-events,
	author = {Wang, Wei and Kennedy, Ryan and Lazer, David and Ramakrishnan, Naren},
	title = {Growing pains for global monitoring of societal events},
	volume = {353},
	number = {6307},
	pages = {1502--1503},
	year = {2016},
	doi = {10.1126/science.aaf6758},
	publisher = {American Association for the Advancement of Science},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/353/6307/1502},
	eprint = {https://science.sciencemag.org/content/353/6307/1502.full.pdf},
	journal = {Science}
}

@InProceedings{hurriyetoglu-overview-CLEF,
author="H{\"u}rriyeto{\u{g}}lu, Ali
and Y{\"o}r{\"u}k, Erdem
and Y{\"u}ret, Deniz
and Yoltar, {\c{C}}a{\u{g}}r{\i}
and G{\"u}rel, Burak
and Duru{\c{s}}an, F{\i}rat
and Mutlu, Osman
and Akdemir, Arda",
editor="Crestani, Fabio
and Braschler, Martin
and Savoy, Jacques
and Rauber, Andreas
and M{\"u}ller, Henning
and Losada, David E.
and Heinatz B{\"u}rki, Gundula
and Cappellato, Linda
and Ferro, Nicola",
title="Overview of CLEF 2019 Lab ProtestNews: Extracting Protests from News in a Cross-Context Setting",
booktitle="Experimental IR Meets Multilinguality, Multimodality, and Interaction",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="425--432",
abstract="We present an overview of the CLEF-2019 Lab ProtestNews on Extracting Protests from News in the context of generalizable natural language processing. The lab consists of document, sentence, and token level information classification and extraction tasks that were referred as task 1, task 2, and task 3 respectively in the scope of this lab. The tasks required the participants to identify protest relevant information from English local news at one or more aforementioned levels in a cross-context setting, which is cross-country in the scope of this lab. The training and development data were collected from India and test data was collected from India and China. The lab attracted 58 teams to participate in the lab. 12 and 9 of these teams submitted results and working notes respectively. We have observed neural networks yield the best results and the performance drops significantly for majority of the submissions in the cross-country setting, which is China.",
isbn="978-3-030-28577-7"
}

@article{radford-clef-levelup,
author = {Radford, Benjamin},
year = {2019},
month = {07},
pages = {},
title = {Multitask Models for Supervised Protest Detection in Texts},
journal = {In Working Notes of CLEF 2019 - Conference and Labs of the Evaluation Forum},
}

@article{asafaya-clef,
author = {Safaya, Ali},
year = {2019},
month = {07},
pages = {},
title = {Event Sentence Detection Task Using Attention Model},
journal = {In Working Notes of CLEF 2019 - Conference and Labs of the Evaluation Forum},
}

@inproceedings{joulin-etal-2017-fasttext,
    title = "Bag of Tricks for Efficient Text Classification",
    author = "Joulin, Armand  and
      Grave, Edouard  and
      Bojanowski, Piotr  and
      Mikolov, Tomas",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E17-2068",
    pages = "427--431",
    abstract = "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.",
}

@inproceedings{mikolov2018advances-new-fasttext,
  title={Advances in Pre-Training Distributed Word Representations},
  author={Mikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and Puhrsch, Christian and Joulin, Armand},
  booktitle={Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)},
  year={2018}
}

@article{maslennikova-clef,
author = {Maslennikova, Elizaveta},
year = {2019},
month = {07},
pages = {},
title = {ELMo Word Representations For News Protection},
journal = {In Working Notes of CLEF 2019 - Conference and Labs of the Evaluation Forum},
}

@inproceedings{Yeh-randtest,
 author = {Yeh, Alexander},
 title = {More Accurate Tests for the Statistical Significance of Result Differences},
 booktitle = {Proceedings of the 18th Conference on Computational Linguistics - Volume 2},
 series = {COLING '00},
 year = {2000},
 location = {Saarbr\&\#252;cken, Germany},
 pages = {947--953},
 numpages = {7},
 url = {https://doi.org/10.3115/992730.992783},
 doi = {10.3115/992730.992783},
 acmid = {992783},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
} 

@article{Dietterich:1998:AST:303222.303237,
 author = {Dietterich, Thomas G.},
 title = {Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms},
 journal = {Neural Comput.},
 issue_date = {Oct. 1998},
 volume = {10},
 number = {7},
 month = oct,
 year = {1998},
 issn = {0899-7667},
 pages = {1895--1923},
 numpages = {29},
 url = {http://dx.doi.org/10.1162/089976698300017197},
 doi = {10.1162/089976698300017197},
 acmid = {303237},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@manual{ace2005events,
  added-at = {2009-10-22T15:18:27.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/202c002e12b4c5abf91b96d2865227ae2/jnothman},
  description = {Guide to annotating events for ACE2005 English. Description of event types (5), triggers (2.3), properties (3), arguments (6; participants and attributes), coreference (4).

"An event is a specific occurrence involving participants. And Event is something that happens. An Event can frequently be described as a change of state."

Types: 33 sub-types (in 8 groupings: LIFE, MOVEMENT, TRANSACTION, BUSINESS, CONFLICT, CONTACT, PERSONNEL, JUSTICE)

Event trigger: "the word which most clearly expresses [an event]'s occurrence". (May be two words for verb-particle constructions.) The containing sentence is the "event extent". Event triggers are most often verbs, but may be: adjectives/participles as resultative/resultative-like stative or active descriptors of events; nouns or pronouns. [As such, the sentence may only need to imply that an event has the potential to occur, as is 'born' in "the presumably Australian-born victim". Is 'victim' meant to be annotated?] A trigger must not be a taggable entity. Some ambiguous expressions such as "this opportunity for peace" may only be labelled when clearly coreferent with an unambiguous trigger.

Event properties: polarity (POSITIVE or NEGATIVE; negation includes contexts such as 'refused to ...'), tense (PAST, PRESENT, FUTURE, UNSPECIFIED), genericity (SPECIFIC or GENERIC; specific must be a singular or finite number of occurrences), modality (ASSERTED or OTHER).

Event arguments: each event has a specific set of participant roles, as well as other attributes (most have only PLACE and TIME). All must be taggable entities; so for instance "method of execution" or "reason for execution" may not be event arguments, but "crime" may be. Number of arguments ranges from 3 to 7. [Designation of an entity as participant or attribute can be arbitrary; PRICE is a participant in a TRANSPORT event.]

Event coreference: events must have identical referent and be in the same document. Mentions to parts of events do not corefer.},
  edition = {5.4.3 2005.07.01},
  editor = {{Linguistic Data Consortium}},
  interhash = {41e872a740e6515a8500280afa898880},
  intrahash = {02c002e12b4c5abf91b96d2865227ae2},
  keywords = {dataset_ACE2005 event_coreference event_extraction},
  timestamp = {2009-10-22T15:18:27.000+0200},
  title = {ACE (Automatic Content Extraction) English Annotation Guidelines for Events},
  year = 2005
}

@inproceedings{Bergstra:2011:AHO:2986459.2986743,
 author = {Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal\'{a}zs},
 title = {Algorithms for Hyper-parameter Optimization},
 booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
 series = {NIPS'11},
 year = {2011},
 isbn = {978-1-61839-599-3},
 location = {Granada, Spain},
 pages = {2546--2554},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2986459.2986743},
 acmid = {2986743},
 publisher = {Curran Associates Inc.},
 address = {USA},
} 

@inproceedings{Bergstra:2013:MSM:3042817.3042832,
 author = {Bergstra, J. and Yamins, D. and Cox, D. D.},
 title = {Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures},
 booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
 series = {ICML'13},
 year = {2013},
 location = {Atlanta, GA, USA},
 pages = {I-115--I-123},
 url = {http://dl.acm.org/citation.cfm?id=3042817.3042832},
 acmid = {3042832},
 publisher = {JMLR.org},
} 

@ARTICLE{10.3389/fpsyg.2017.01918,
  
AUTHOR={Sturdy, Christopher B. and Nicoladis, Elena},   
	 
TITLE={How Much of Language Acquisition Does Operant Conditioning Explain?},      
	
JOURNAL={Frontiers in Psychology},      
	
VOLUME={8},      

PAGES={1918},     
	
YEAR={2017},      
	  
URL={https://www.frontiersin.org/article/10.3389/fpsyg.2017.01918},       
	
DOI={10.3389/fpsyg.2017.01918},      
	
ISSN={1664-1078},   
   
ABSTRACT={Since the 1950s, when Chomsky argued that Skinner’s arguments could not explain syntactic acquisition, psychologists have generally avoided explicitly invoking operant or instrumental conditioning as a learning mechanism for language among human children. In this article, we argue that this is a mistake. We focus on research that has been done on language learning in human infants and toddlers in order to illustrate our points. Researchers have ended up inventing learning mechanisms that, in actual practice, not only resemble but also in fact are examples of operant conditioning (OC) by any other name they select. We argue that language acquisition researchers should proceed by first ruling out OC before invoking alternative learning mechanisms. While it is possible that OC cannot explain all of the language acquisition, simple learning mechanisms that work across species may have some explanatory power in children’s language learning.}
}

@article{KELL2018630,
title = "A Task-Optimized Neural Network Replicates Human Auditory Behavior, Predicts Brain Responses, and Reveals a Cortical Processing Hierarchy",
journal = "Neuron",
volume = "98",
number = "3",
pages = "630 - 644.e16",
year = "2018",
issn = "0896-6273",
doi = "https://doi.org/10.1016/j.neuron.2018.03.044",
url = "http://www.sciencedirect.com/science/article/pii/S0896627318302502",
author = "Alexander J.E. Kell and Daniel L.K. Yamins and Erica N. Shook and Sam V. Norman-Haignere and Josh H. McDermott",
keywords = "auditory cortex, deep learning, fMRI, encoding models, hierarchy, word recognition, natural sounds, human auditory cortex, deep neural network, convolutional neural network",
abstract = "Summary
A core goal of auditory neuroscience is to build quantitative models that predict cortical responses to natural sounds. Reasoning that a complete model of auditory cortex must solve ecologically relevant tasks, we optimized hierarchical neural networks for speech and music recognition. The best-performing network contained separate music and speech pathways following early shared processing, potentially replicating human cortical organization. The network performed both tasks as well as humans and exhibited human-like errors despite not being optimized to do so, suggesting common constraints on network and human performance. The network predicted fMRI voxel responses substantially better than traditional spectrotemporal filter models throughout auditory cortex. It also provided a quantitative signature of cortical representational hierarchy—primary and non-primary responses were best predicted by intermediate and late network layers, respectively. The results suggest that task optimization provides a powerful set of tools for modeling sensory systems."
}

@incollection{grandstrand:2004,
  author      = "Addison Greenwood",
  title       = "Computational Neuroscience: A Window to Understanding How the Brain Works",
  booktitle   = "Science at the Frontier",
  publisher   = "The National Academies Press",
  address     = "Washington, DC",
  year        = 1992,
  pages       = "199-232",
  chapter     = 9,
  doi         = "https://doi.org/10.17226/1859"
}

@article{adam-Kingma,
author = {Kingma, Diederik and Ba, Jimmy},
year = {2014},
month = {12},
pages = {},
title = {Adam: A Method for Stochastic Optimization},
journal = {International Conference on Learning Representations},
}

@article{45610,
title	= {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
author	= {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
year	= {2016},
URL	= {http://arxiv.org/abs/1609.08144},
journal	= {CoRR},
volume	= {abs/1609.08144}
}

@inproceedings{Zhao:2015:SHS:2832747.2832816,
 author = {Zhao, Han and Lu, Zhengdong and Poupart, Pascal},
 title = {Self-adaptive Hierarchical Sentence Model},
 booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
 series = {IJCAI'15},
 year = {2015},
 isbn = {978-1-57735-738-4},
 location = {Buenos Aires, Argentina},
 pages = {4069--4076},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=2832747.2832816},
 acmid = {2832816},
 publisher = {AAAI Press},
} 

@inproceedings{conneau-kiela-2018-senteval,
    title = "{S}ent{E}val: An Evaluation Toolkit for Universal Sentence Representations",
    author = "Conneau, Alexis  and
      Kiela, Douwe",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://www.aclweb.org/anthology/L18-1269",
}

@inproceedings{bowman-etal-2015-large,
    title = "A large annotated corpus for learning natural language inference",
    author = "Bowman, Samuel R.  and
      Angeli, Gabor  and
      Potts, Christopher  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D15-1075",
    doi = "10.18653/v1/D15-1075",
    pages = "632--642",
}

@inproceedings{conneau-etal-2017-supervised,
    title = "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
    author = {Conneau, Alexis  and
      Kiela, Douwe  and
      Schwenk, Holger  and
      Barrault, Lo{\"\i}c  and
      Bordes, Antoine},
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1070",
    doi = "10.18653/v1/D17-1070",
    pages = "670--680",
    abstract = "Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.",
}

@inproceedings{
logeswaran2018an,
  title={An efficient framework for learning sentence representations},
  author={Lajanugen Logeswaran and Honglak Lee},
  booktitle={International Conference on Learning Representations},
  year={2018},
  url={https://openreview.net/forum?id=rJvJXZb0W},
}

@inproceedings{hill-etal-2016-learning-distributed,
    title = "Learning Distributed Representations of Sentences from Unlabelled Data",
    author = "Hill, Felix  and
      Cho, Kyunghyun  and
      Korhonen, Anna",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N16-1162",
    doi = "10.18653/v1/N16-1162",
    pages = "1367--1377",
}

@article{han2019unsupervised,
    title = {Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling},
    author = {Xiaochuang Han and Jacob Eisenstein},
    journal={arXiv:1907.11692},
    year = {2019},
}

@article{bottou2019-irm,
    title = {Invariant Risk Minimization},
    author = {Martin Arjovsky and Léon Bottou and Ishaan Gulrajani and David Lopez-Paz},
    journal={arXiv:1907.02893v2},
    year = {2019},
}

@incollection{NIPS2019_8556,
title = {Addressing Failure Prediction by Learning Model Confidence},
author = {Corbi\`{e}re, Charles and THOME, Nicolas and Bar-Hen, Avner and Cord, Matthieu and P\'{e}rez, Patrick},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {2898--2909},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8556-addressing-failure-prediction-by-learning-model-confidence.pdf}
}

@Manual{scikitlearn,
	title = "scikit-learn",
	author = "David Cournapeau",
	year = 2007,
	url = "https://scikit-learn.org/",
	note = "{accessed in December 2019}"
}

@Manual{spacy,
	title = "spaCy",
	author = "Explosion",
	year = 2014,
	url = "https://spacy.io/usage/models",
	note = "{accessed in December 2019}"
}

@Manual{clef19,
	title = "CLEF 2019",
	author = "Conference and Labs of the Evaluation Forum Initiative",
	year = 2019,
	url = "http://clef2019.clef-initiative.eu/",
	note = "{accessed in December 2019}"
}
@Manual{clef19protest,
	title = "CLEF-2019 Lab ProtestNews on Extracting Protests from News",
	author = "H{\"u}rriyeto{\u{g}}lu, Ali
and Y{\"o}r{\"u}k, Erdem
and Y{\"u}ret, Deniz
and Y{\"o}r{\"u}k, Erdem
and Yoltar, {\c{C}}a{\u{g}}r{\i}
and G{\"u}rel, Burak
and Duru{\c{s}}an, F{\i}rat
and Mutlu, Osman
and Akdemir, Arda
and Gessler, Theresa
and Makarov, Peter",
	year = 2019,
	url = "https://emw.ku.edu.tr/clef-protestnews-2019/",
	note = "{accessed in December 2019}"
}

@Manual{pytorch,
	title = "PyTorch",
	author = "Adam Paszke and Sam Gross and Soumith Chintala and Gregory Chanan",
	year = 2016,
	url = "https://pytorch.org/",
	note = "{accessed in December 2019}"
}

@Manual{distilbert,
	title = "DistilBERT",
	author = "HuggingFace",
	year = 2019,
	url = "https://github.com/huggingface/transformers",
	note = "{accessed in December 2019}"
}

@Manual{elmo,
	title = "Original ELMo model",
	author = "AllenNLP",
	year = 2018,
	url = "https://allennlp.org/elmo",
	note = "{accessed in December 2019}"
}

@Manual{cameo,
	title = "Conflict and Mediation Event Observations Event and Actor Codebook",
	author = "Pennsylvania State University",
	year = 2012,
	url = "http://data.gdeltproject.org/documentation/CAMEO.Manual.1.1b3.pdf",
	note = "{accessed in December 2019}"
}

@Manual{cs231n,
	title = "CS231n, Convolutional Neural Networks for Visual Recognition",
	author = "Stanford University",
	year = 2019,
	url = "http://cs231n.github.io/",
	note = "{accessed in January 2020}"
}

@inproceedings{hinton-distil,
title	= {Distilling the Knowledge in a Neural Network},
author	= {Geoffrey Hinton and Oriol Vinyals and Jeffrey Dean},
year	= {2015},
URL	= {http://arxiv.org/abs/1503.02531},
booktitle	= {NIPS Deep Learning and Representation Learning Workshop}
}

 @inproceedings{bucila-distil,
 author = {Buciluundefined, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
 title = {Model Compression},
 year = {2006},
 isbn = {1595933395},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/1150402.1150464},
 doi = {10.1145/1150402.1150464},
 booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 pages = {535–541},
 numpages = {7},
 keywords = {model compression, supervised learning},
 location = {Philadelphia, PA, USA},
 series = {KDD ’06}
}

@Manual{kd-nervana,
	title = "Knowledge Distillation",
	author = "Nervana Systems",
	year = 2019,
	url = "https://nervanasystems.github.io/distiller/knowledge_distillation.html",
	note = "{accessed in January 2020}"
}

@InProceedings{bookcorpus,
    title = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
    author = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
    booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
    month = {December},
    year = {2015}
}

@techreport{onebillion,
title	= {One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling},
author	= {Ciprian Chelba and Tomas Mikolov and Mike Schuster and Qi Ge and Thorsten Brants and Phillipp Koehn and Tony Robinson},
year	= {2013},
URL	= {http://arxiv.org/abs/1312.3005},
institution	= {Google}
}

@Manual{wmt11-text,
	title = "EMNLP 2011 Workshop on Statistical Machine Translation",
	author = "Phillipp Koehn",
	year = 2011,
	url = "http://www.statmt.org/wmt11/",
	note = "{accessed in January 2020}"
}

@inproceedings{DBLP:conf/iclr/MerityX0S17,
  author    = {Stephen Merity and
               Caiming Xiong and
               James Bradbury and
               Richard Socher},
  title     = {Pointer Sentinel Mixture Models},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Byj72udxe},
  timestamp = {Thu, 25 Jul 2019 14:25:57 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/iclr/MerityX0S17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


