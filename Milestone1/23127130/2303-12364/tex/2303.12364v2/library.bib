@article{Lee2019,
   abstract = {Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.},
   author = {Jinhyuk Lee and Wonjin Yoon and Sungdong Kim and Donghyeon Kim and Sunkyu Kim and Chan Ho So and Jaewoo Kang},
   journal = {Bioinformatics},
   month = {1},
   publisher = {Oxford University Press},
   title = {BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
   year = {2019},
}
@article{Miotto2016,
   abstract = {Secondary use of electronic health records (EHRs) promises to advance clinical research and better inform clinical decision making. Challenges in summarizing and representing patient data prevent widespread practice of predictive modeling using EHRs. Here we present a novel unsupervised deep feature learning method to derive a general-purpose patient representation from EHR data that facilitates clinical predictive modeling. In particular, a three-layer stack of denoising autoencoders was used to capture hierarchical regularities and dependencies in the aggregated EHRs of about 700,000 patients from the Mount Sinai data warehouse. The result is a representation we name "deep patient". We evaluated this representation as broadly predictive of health states by assessing the probability of patients to develop various diseases. We performed evaluation using 76,214 test patients comprising 78 diseases from diverse clinical domains and temporal windows. Our results significantly outperformed those achieved using representations based on raw EHR data and alternative feature learning strategies. Prediction performance for severe diabetes, schizophrenia, and various cancers were among the top performing. These findings indicate that deep learning applied to EHRs can derive patient representations that offer improved clinical predictions, and could provide a machine learning framework for augmenting clinical decision systems.},
   author = {Riccardo Miotto and Li Li and Brian A. Kidd and Joel T. Dudley},
   doi = {10.1038/srep26094},
   issn = {20452322},
   journal = {Scientific Reports},
   month = {5},
   pmid = {27185194},
   publisher = {Nature Publishing Group},
   title = {Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records},
   volume = {6},
   year = {2016},
}
@article{Li2021,
   author = {Yikuan Li and Mohammad Mamouei and Gholamreza Salimi-Khorshidi and Shishir Rao and Abdelaali Hassaine and Dexter Canoy and Thomas Lukasiewicz and Kazem Rahimi},
   journal = {Journal of Biomedical and Health Informatics},
   title = {Hi-BEHRT: Hierarchical Transformer-based model
for accurate prediction of clinical events using
multimodal longitudinal electronic health records},
   year = {2021},
}
@article{Melnychuk2022,
   abstract = {Estimating counterfactual outcomes over time from observational data is relevant for many applications (e.g., personalized medicine). Yet, state-of-the-art methods build upon simple long short-term memory (LSTM) networks, thus rendering inferences for complex, long-range dependencies challenging. In this paper, we develop a novel Causal Transformer for estimating counterfactual outcomes over time. Our model is specifically designed to capture complex, long-range dependencies among time-varying confounders. For this, we combine three transformer subnetworks with separate inputs for time-varying covariates, previous treatments, and previous outcomes into a joint network with in-between cross-attentions. We further develop a custom, end-to-end training procedure for our Causal Transformer. Specifically, we propose a novel counterfactual domain confusion loss to address confounding bias: it aims to learn adversarial balanced representations, so that they are predictive of the next outcome but non-predictive of the current treatment assignment. We evaluate our Causal Transformer based on synthetic and real-world datasets, where it achieves superior performance over current baselines. To the best of our knowledge, this is the first work proposing transformer-based architecture for estimating counterfactual outcomes from longitudinal data.},
   author = {Valentyn Melnychuk and Dennis Frauen and Stefan Feuerriegel},
   month = {4},
   title = {Causal Transformer for Estimating Counterfactual Outcomes},
   year = {2022},
}
@article{Pang2021,
   abstract = {Embedding algorithms are increasingly used to represent clinical concepts in healthcare for improving machine learning tasks such as clinical phenotyping and disease prediction. Recent studies have adapted state-of-the-art bidirectional encoder representations from transformers (BERT) architecture to structured electronic health records (EHR) data for the generation of contextualized concept embeddings, yet do not fully incorporate temporal data across multiple clinical domains. Therefore we developed a new BERT adaptation, CEHR-BERT, to incorporate temporal information using a hybrid approach by augmenting the input to BERT using artificial time tokens, incorporating time, age, and concept embeddings, and introducing a new second learning objective for visit type. CEHR-BERT was trained on a subset of Columbia University Irving Medical Center-York Presbyterian Hospital's clinical data, which includes 2.4M patients, spanning over three decades, and tested using 4-fold cross-validation on the following prediction tasks: hospitalization, death, new heart failure (HF) diagnosis, and HF readmission. Our experiments show that CEHR-BERT outperformed existing state-of-the-art clinical BERT adaptations and baseline models across all 4 prediction tasks in both ROC-AUC and PR-AUC. CEHR-BERT also demonstrated strong transfer learning capability, as our model trained on only 5% of data outperformed comparison models trained on the entire data set. Ablation studies to better understand the contribution of each time component showed incremental gains with every element, suggesting that CEHR-BERT's incorporation of artificial time tokens, time and age embeddings with concept embeddings, and the addition of the second learning objective represents a promising approach for future BERT-based clinical embeddings.},
   author = {Chao Pang and Xinzhuo Jiang and Krishna S Kalluri and Matthew Spotnitz and RuiJun Chen and Adler Perotte and Karthik Natarajan},
   journal = {Proceedings of Machine Learning for Health},
   month = {11},
   title = {CEHR-BERT: Incorporating temporal information from structured EHR data to improve prediction tasks},
   year = {2021},
}
@article{Rasmy2021,
   abstract = {Background: Deep learning (DL)-based predictive models from electronic health records (EHRs) deliver good performance in many clinical tasks. Large training cohorts, however, are often required to achieve high accuracy, hindering the adoption of DL-based models in scenarios with limited training data size. Recently, bidirectional encoder representations from transformers (BERT) and related models have achieved tremendous successes in the natural language processing (NLP) domain. The pre-training of BERT on a very large training corpus generates contextualized embeddings that can be applied to smaller data sets with fine-tuning that can substantially boost their performance with these data sets. Because EHR data are analogous to text data, as both are sequential over a large vocabulary, we explore whether this "pre-training, fine tuning" paradigm can improve the performance of EHR-based predictive modeling.},
   author = {Laila Rasmy and Yang Xiang and Ziqian Xie and Cui Tao and Degui Zhi},
   journal = {Nature},
   keywords = {BERT,Deep Learning,Disease Prediction,Electronic Health Records,Pretrained Contextualized Embeddings,Structured Data,Transfer Learning 2,Transformers},
   title = {Med-BERT: pre-trained contextualized embeddings on large-scale structured electronic health records for disease prediction},
   year = {2021},
}
@article{Chen2020,
   abstract = {Background: Doctors must care for many patients simultaneously, and it is time-consuming to find and examine all patients' medical histories. Discharge diagnoses provide hospital staff with sufficient information to enable handling multiple patients; however, the excessive amount of words in the diagnostic sentences poses problems. Deep learning may be an effective solution to overcome this problem, but the use of such a heavy model may also add another obstacle to systems with limited computing resources. Objective: We aimed to build a diagnoses-extractive summarization model for hospital information systems and provide a service that can be operated even with limited computing resources. Methods: We used a Bidirectional Encoder Representations from Transformers (BERT)-based structure with a two-stage training method based on 258,050 discharge diagnoses obtained from the National Taiwan University Hospital Integrated Medical Database, and the highlighted extractive summaries written by experienced doctors were labeled. The model size was reduced using a character-level token, the number of parameters was decreased from 108,523,714 to 963,496, and the model was pretrained using random mask characters in the discharge diagnoses and International Statistical Classification of Diseases and Related Health Problems sets. We then fine-tuned the model using summary labels and cleaned up the prediction results by averaging all probabilities for entire words to prevent character level-induced fragment words. Model performance was evaluated against existing models BERT, BioBERT, and Long Short-Term Memory (LSTM) using the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) L score, and a questionnaire website was built to collect feedback from more doctors for each summary proposal. Results: The area under the receiver operating characteristic curve values of the summary proposals were 0.928, 0.941, 0.899, and 0.947 for BERT, BioBERT, LSTM, and the proposed model (AlphaBERT), respectively. The ROUGE-L scores were 0.697, 0.711, 0.648, and 0.693 for BERT, BioBERT, LSTM, and AlphaBERT, respectively. The mean (SD) critique scores from doctors were 2.232 (0.832), 2.134 (0.877), 2.207 (0.844), 1.927 (0.910), and 2.126 (0.874) for reference-by-doctor labels, BERT, BioBERT, LSTM, and AlphaBERT, respectively. Based on the paired t test, there was a statistically significant difference in LSTM compared to the reference (P<.001), BERT (P=.001), BioBERT (P<.001), and AlphaBERT (P=.002), but not in the other models. Conclusions: Use of character-level tokens in a BERT model can greatly decrease the model size without significantly reducing performance for diagnoses summarization. A well-developed deep-learning model will enhance doctors' abilities to manage patients and promote medical studies by providing the capability to use extensive unstructured free-text notes.},
   author = {Yen Pin Chen and Yi Ying Chen and Jr Jiun Lin and Chien Hua Huang and Feipei Lai},
   doi = {10.2196/17787},
   issn = {22919694},
   issue = {4},
   journal = {JMIR Medical Informatics},
   keywords = {Automatic summarization,BERT,Deep learning,Emergency medicine,Transformer},
   month = {4},
   publisher = {JMIR Publications Inc.},
   title = {Modified bidirectional encoder representations from transformers extractive summarization model for hospital information systems based on character-level tokens (AlphaBERT): Development and performance evaluation},
   volume = {8},
   year = {2020},
}
@article{Rao2022,
   author = {Shishir Rao and Mohammad Mamouei and Gholamreza Salimi-Khorshidi and Yikuan Li and Rema Ramakrishnan and Abdelaali Hassaine and Dexter Canoy and Kazem Rahimi},
   journal = {IEEE},
   title = {Targeted-BEHRT: Deep learning for
observational causal inference on longitudinal
electronic health records},
   year = {2022},
}
@article{Meng2021,
   abstract = {Advancements in machine learning algorithms have had a beneficial impact on representation learning, classification, and prediction models built using electronic health record (EHR) data. Effort has been put both on increasing models' overall performance as well as improving their interpretability, particularly regarding the decision-making process. In this study, we present a temporal deep learning model to perform bidirectional representation learning on EHR sequences with a transformer architecture to predict future diagnosis of depression. This model is able to aggregate five heterogenous and high-dimensional data sources from the EHR and process them in a temporal manner for chronic disease prediction at various prediction windows. We applied the current trend of pretraining and fine-tuning on EHR data to outperform the current state-of-the-art in chronic disease prediction, and to demonstrate the underlying relation between EHR codes in the sequence. The model generated the highest increases of precision-recall area under the curve (PRAUC) from 0.70 to 0.76 in depression prediction compared to the best baseline model. Furthermore, the self-attention weights in each sequence quantitatively demonstrated the inner relationship between various codes, which improved the model's interpretability. These results demonstrate the model's ability to utilize heterogeneous EHR data to predict depression while achieving high accuracy and interpretability, which may facilitate constructing clinical decision support systems in the future for chronic disease screening and early detection.},
   author = {Yiwen Meng and William Speier and Michael K. Ong and Corey W. Arnold},
   journal = {Journal of Biomedical and Health Informatics},
   keywords = {Clinical decision support,depression,electronic health record,natural language processing,temporal representation and reasoning},
   month = {8},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Bidirectional Representation Learning from Transformers Using Multimodal Electronic Health Record Data to Predict Depression},
   year = {2021},
}
@article{Devlin2018,
   abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
   author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
   journal = {ACL},
   month = {10},
   title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
   year = {2018},
}
@report{Peter2021,
   author = {Oriane Peter},
   title = {École Polytechnique Fédérale de Lausanne ELSA: Leveraging GPT for Patient Journey Modeling and Disease Subtyping},
   url = {https://github.com/HexHive/thesis_template.},
   year = {2021},
}
@article{Kalyan2022,
   abstract = {Transformer-based pretrained language models (PLMs) have started a new era in modern natural language processing (NLP). These models combine the power of transformers, transfer learning, and self-supervised learning (SSL). Following the success of these models in the general domain, the biomedical research community has developed various in-domain PLMs starting from BioBERT to the latest BioELECTRA and BioALBERT models. We strongly believe there is a need for a survey paper that can provide a comprehensive survey of various transformer-based biomedical pretrained language models (BPLMs). In this survey, we start with a brief overview of foundational concepts like self-supervised learning, embedding layer and transformer encoder layers. We discuss core concepts of transformer-based PLMs like pretraining methods, pretraining tasks, fine-tuning methods, and various embedding types specific to biomedical domain. We introduce a taxonomy for transformer-based BPLMs and then discuss all the models. We discuss various challenges and present possible solutions. We conclude by highlighting some of the open issues which will drive the research community to further improve transformer-based BPLMs. The list of all the publicly available transformer-based BPLMs along with their links is provided at https://mr-nlp.github.io/posts/2021/05/transformer-based-biomedical-pretrained-language-models-list/.},
   author = {Katikapalli Subramanyam Kalyan and Ajit Rajasekharan and Sivanesan Sangeetha},
   journal = {Journal of Biomedical Informatics},
   keywords = {BioBERT,Biomedical pretrained language models,PubMedBERT,Self-supervised learning,Survey,Transformers},
   month = {2},
   publisher = {Academic Press Inc.},
   title = {AMMU: A survey of transformer-based biomedical pretrained language models},
   year = {2022},
}
@article{Vaswani2017,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
   author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
   month = {6},
   title = {Attention Is All You Need},
   year = {2017},
}
@article{Wang2020,
   abstract = {Chronic kidney disease (CKD) has a poor prognosis due to excessive risk factors and comorbidities associated with it. The early detection of CKD faces challenges of insufficient medical histories of positive patients and complicated risk factors. In this paper, we propose the TRACE (Transformer-RNN Autoencoder-enhanced CKD Detector) framework, an end-to-end prediction model using patients' medical history data, to deal with these challenges. TRACE presents a comprehensive medical history representation with a novel key component: a Transformer-RNN autoencoder. The autoencoder jointly learns a medical concept embedding via Transformer for each hospital visit, and a latent representation which summarizes a patient's medical history across all the visits. We compared TRACE with multiple state-of-the-art methods on a dataset derived from real-world medical records. Our model has achieved 0.5708 AUPRC with a 2.31% relative improvement over the best-performing method. We also validated the clinical meaning of the learned embeddings through visualizations and a case study, showing the potential of TRACE to serve as a general disease prediction model.},
   author = {Yu Wang and Ziqiao Guan and Wei Hou and Fusheng Wang},
   month = {12},
   title = {TRACE: Early Detection of Chronic Kidney Disease Onset with Transformer-Enhanced Feature Embedding},
   year = {2020},
}
@article{Li2020,
   abstract = {Today, despite decades of developments in medicine and the growing interest in precision healthcare, vast majority of diagnoses happen once patients begin to show noticeable signs of illness. Early indication and detection of diseases, however, can provide patients and carers with the chance of early intervention, better disease management, and efficient allocation of healthcare resources. The latest developments in machine learning (including deep learning) provides a great opportunity to address this unmet need. In this study, we introduce BEHRT: A deep neural sequence transduction model for electronic health records (EHR), capable of simultaneously predicting the likelihood of 301 conditions in one’s future visits. When trained and evaluated on the data from nearly 1.6 million individuals, BEHRT shows a striking improvement of 8.0–13.2% (in terms of average precision scores for different tasks), over the existing state-of-the-art deep EHR models. In addition to its scalability and superior accuracy, BEHRT enables personalised interpretation of its predictions; its flexible architecture enables it to incorporate multiple heterogeneous concepts (e.g., diagnosis, medication, measurements, and more) to further improve the accuracy of its predictions; its (pre-)training results in disease and patient representations can be useful for future studies (i.e., transfer learning).},
   author = {Yikuan Li and Shishir Rao and José Roberto Ayala Solares and Abdelaali Hassaine and Rema Ramakrishnan and Dexter Canoy and Yajie Zhu and Kazem Rahimi and Gholamreza Salimi-Khorshidi},
   journal = {Nature},
   publisher = {Nature Research},
   title = {BEHRT: Transformer for Electronic Health Records},
   year = {2020},
}
@article{Prakash2021,
   abstract = {A rare disease is any disease that affects a very small percentage (1 in 1,500) of population. It is estimated that there are nearly 7,000 rare disease affecting 30 million patients in the U. S. alone. Most of the patients suffering from rare diseases experience multiple misdiagnoses and may never be diagnosed correctly. This is largely driven by the low prevalence of the disease that results in a lack of awareness among healthcare providers. There have been efforts from machine learning researchers to develop predictive models to help diagnose patients using healthcare datasets such as electronic health records and administrative claims. Most recently, transformer models have been applied to predict diseases BEHRT, G-BERT and Med-BERT. However, these have been developed specifically for electronic health records (EHR) and have not been designed to address rare disease challenges such as class imbalance, partial longitudinal data capture, and noisy labels. As a result, they deliver poor performance in predicting rare diseases compared with base-lines. Besides, EHR datasets are generally confined to the hospital systems using them and do not capture a wider sample of patients thus limiting the availability of sufficient rare disease patients in the dataset. To address these challenges, we introduced an extension of the BERT model tailored for rare disease diagnosis called RareBERT which has been trained on administrative claims datasets. RareBERT extends Med-BERT by including context embedding and temporal reference embedding. Moreover, we introduced a novel adap-tive loss function to handle the class imbalance. In this paper, we show our experiments on diagnosing X-Linked Hypo-phosphatemia (XLH), a genetic rare disease. While Rare-BERT performs significantly better than the baseline models (79.9% AUPRC versus 30% AUPRC for Med-BERT), owing to the transformer architecture, it also shows its robustness in partial longitudinal data capture caused by poor capture of claims with a drop in performance of only 1.35% AUPRC, compared with 12% for Med-BERT and 33.0% for LSTM and 67.4% for boosting trees based baseline. †},
   author = {Pks Prakash and Srinivas Chilukuri and Nikhil Ranade and Shankar Viswanathan},
   journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
   keywords = {Application Domains: Healthcare & Medicine & Wellness,Machine Learning: (Deep) Neural Network Algorithms},
   title = {RareBERT: Transformer Architecture for Rare Disease Patient Identifi-cation using Administrative Claims},
   year = {2021},
}
@article{Poulain2022,
   abstract = {With the growing availability of Electronic Health Records (EHRs), many deep learning methods have been developed to leverage such datasets in medical prediction tasks. Notably, transformer-based architectures have proven to be highly effective for EHRs. Transformer-based architectures are generally very effective in "transferring" the acquired knowledge from very large datasets to smaller target datasets through their comprehensive "pre-training" process. However, to work efficiently, they still rely on the target datasets for the downstream tasks, and if the target dataset is (very) small, the performance of downstream models can degrade rapidly. In biomedical applications, it is common to only have access to small datasets, for instance, when studying rare diseases, invasive procedures, or using restrictive cohort selection processes. In this study, we present CEHR-GAN-BERT, a semi-supervised transformer-based architecture that leverages both in-and out-of-cohort patients to learn better patient representations in the context of few-shot learning. The proposed method opens new learning opportunities where only a few hundred samples are available. We extensively evaluate our method on four prediction tasks and three public datasets showing the ability of our model to achieve improvements upwards of 5% on all performance metrics (including AUROC and F1 Score) on the tasks that use less than 200 annotated patients during the training process 1 .},
   author = {Raphael Poulain and Mehak Gupta and Rahmatollah Beheshti},
   journal = {Proceedings of Machine Learning Research},
   title = {Few-Shot Learning with Semi-Supervised Transformers for Electronic Health Records},
   volume = {182},
   year = {2022},
}
@article{Azhir2022,
   abstract = {Incorporating repeated measurements of vitals and laboratory measurements can improve mortality risk-prediction and identify key risk factors in individualized treatment of COVID-19 hospitalized patients. In this observational study, demographic and laboratory data of all admitted patients to 5 hospitals of Mount Sinai Health System, New York, with COVID-19 positive tests between March 1st and June 8th, 2020, were extracted from electronic medical records and compared between survivors and non-survivors. Next day mortality risk of patients was assessed using a transformer-based model BEHRTDAY fitted to patient time series data of vital signs, blood and other laboratory measurements given the entire patients' hospital stay. The study population includes 3699 COVID-19 positive (57% male, median age: 67) patients. This model had a very high average precision score (0.96) and area under receiver operator curve (0.92) for next-day mortality prediction given entire patients' trajectories, and through masking, it learnt each variable's context.},
   author = {Alaleh Azhir and Soheila Talebi and Louis-Henri Merino and Yikuan Li and Thomas Lukasiewicz and Edgar Argulian and Jagat Narula and Borislava Mihaylova},
   journal = {AMIA Annu Symp Proc},
   title = {BEHRTDAY: Dynamic Mortality Risk Prediction using Time-Variant COVID-19 Patient Specific Trajectories},
   year = {2022},
}
@article{Rao2022,
   abstract = {Predicting the incidence of complex chronic conditions such as heart failure is challenging. Deep learning models applied to rich electronic health records may improve prediction but remain unexplainable hampering their wider use in medical practice. We aimed to develop a deep-learning framework for accurate and yet explainable prediction of 6-month incident heart failure (HF). Using 100,071 patients from longitudinal linked electronic health records across the U.K., we applied a novel Transformer-based risk model using all community and hospital diagnoses and medications contextualized within the age and calendar year for each patient's clinical encounter. Feature importance was investigated with an ablation analysis to compare model performance when alternatively removing features and by comparing the variability of temporal representations. A post-hoc perturbation technique was conducted to propagate the changes in the input to the outcome for feature contribution analyses. Our model achieved 0.93 area under the receiver operator curve and 0.69 area under the precision-recall curve on internal 5-fold cross validation and outperformed existing deep learning models. Ablation analysis indicated medication is important for predicting HF risk, calendar year is more important than chronological age, which was further reinforced by temporal variability analysis. Contribution analyses identified risk factors that are closely related to HF. Many of them were consistent with existing knowledge from clinical and epidemiological research but several new associations were revealed which had not been considered in expert-driven risk prediction models. In conclusion, the results highlight that our deep learning model, in addition high predictive performance, can inform data-driven risk factor identification.},
   author = {Shishir Rao and Yikuan Li and Rema Ramakrishnan and Abdelaali Hassaine and Dexter Canoy and John Cleland and Thomas Lukasiewicz and Gholamreza Salimi-Khorshidi and Kazem Rahimi},
   doi = {10.1109/JBHI.2022.3148820},
   issn = {21682208},
   issue = {7},
   journal = {IEEE Journal of Biomedical and Health Informatics},
   keywords = {Electronic health records,heart failure,research design},
   month = {7},
   pages = {3362-3372},
   pmid = {35130176},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {An Explainable Transformer-Based Deep Learning Model for the Prediction of Incident Heart Failure},
   volume = {26},
   year = {2022},
}
@article{Kraljevic2021,
   abstract = {The data available in Electronic Health Records (EHRs) provides the
opportunity to transform care, and the best way to provide better
care for one patient is through learning from the data available
on all other patients. Temporal modelling of a patient’s medical
history, which takes into account the sequence of past events, can
be used to predict future events such as a diagnosis of a new disorder
or complication of a previous or existing disorder. While most
prediction approaches use mostly the structured data in EHRs or
a subset of single-domain predictions and outcomes, we present
MedGPT a novel transformer-based pipeline that uses Named Entity
Recognition and Linking tools (i.e. MedCAT) to structure and
organize the free text portion of EHRs and anticipate a range of
future medical events (initially disorders). Since a large portion of
EHR data is in text form, such an approach benefits from a granular
and detailed view of a patient while introducing modest additional
noise. MedGPT effectively deals with the noise and the added granularity,
and achieves a precision of 0.344, 0.552 and 0.640 (vs LSTM
0.329, 0.538 and 0.633) when predicting the top 1, 3 and 5 candidate
future disorders on real world hospital data from King’s College
Hospital, London, UK (~600k patients).We also show that our model
captures medical knowledge by testing it on an experimental medical
multiple choice question answering task, and by examining
the attentional focus of the model using gradient-based saliency
methods.},
   author = {Zeljko Kraljevic and Anthony Shek and Daniel Bean and Rebecca Bendayan and James T.H. Teo and Richard J.B. Dobson},
   doi = {10.1145/1122445.1122456},
   issn = {25730142},
   keywords = {creativity,live streaming,sketching,user engagement,visualization},
   month = {4},
   publisher = {Association for Computing Machinery},
   title = {MedGPT: Medical Concept Prediction from Clinical Narratives},
   year = {2021},
}
@article{Shang2019,
   abstract = {Medication recommendation is an important healthcare application. It is commonly formulated as a temporal prediction task. Hence, most existing works only utilize longitudinal electronic health records (EHRs) from a small number of patients with multiple visits ignoring a large number of patients with a single visit (selection bias). Moreover, important hierarchical knowledge such as diagnosis hierarchy is not leveraged in the representation learning process. To address these challenges, we propose G-BERT, a new model to combine the power of Graph Neural Networks (GNNs) and BERT (Bidirectional Encoder Representations from Transformers) for medical code representation and medication recommendation. We use GNNs to represent the internal hierarchical structures of medical codes. Then we integrate the GNN representation into a transformer-based visit encoder and pre-train it on EHR data from patients only with a single visit. The pre-trained visit encoder and representation are then fine-tuned for downstream predictive tasks on longitudinal EHRs from patients with multiple visits. G-BERT is the first to bring the language model pre-training schema into the healthcare domain and it achieved state-of-the-art performance on the medication recommendation task.},
   author = {Junyuan Shang and Tengfei Ma and Cao Xiao and Jimeng Sun},
   journal = {International Joint Conferences on Artificial Intelligence},
   month = {6},
   title = {Pre-training of Graph Augmented Transformers for Medication Recommendation},
   year = {2019},
}
@article{Choi2015,
   abstract = {Leveraging large historical data in electronic health record (EHR), we developed Doctor AI, a generic predictive model that covers observed medical conditions and medication uses. Doctor AI is a temporal model using recurrent neural networks (RNN) and was developed and applied to longitudinal time stamped EHR data from 260K patients over 8 years. Encounter records (e.g. diagnosis codes, medication codes or procedure codes) were input to RNN to predict (all) the diagnosis and medication categories for a subsequent visit. Doctor AI assesses the history of patients to make multilabel predictions (one label for each diagnosis or medication category). Based on separate blind test set evaluation, Doctor AI can perform differential diagnosis with up to 79% recall@30, significantly higher than several baselines. Moreover, we demonstrate great generalizability of Doctor AI by adapting the resulting models from one institution to another without losing substantial accuracy.},
   author = {Edward Choi and Mohammad Taha Bahadori and Andy Schuetz and Walter F. Stewart and Jimeng Sun},
   journal = {Machine Learning and Healthcare Conference},
   month = {11},
   title = {Doctor AI: Predicting Clinical Events via Recurrent Neural Networks},
   year = {2015},
}
@article{Brown2020,
   abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
   author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
   journal = {NIPS},
   month = {5},
   title = {Language Models are Few-Shot Learners},
   year = {2020},
}
@article{Kingma2014,
   abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
   author = {Diederik P. Kingma and Jimmy Ba},
   month = {12},
   title = {Adam: A Method for Stochastic Optimization},
   year = {2014},
}
@article{McInnes2018,
   abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
   author = {Leland McInnes and John Healy and James Melville},
   journal = {The Journal of Open Source Software},
   month = {2},
   title = {UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},
   year = {2018},
}
@article{Campello2013,
   abstract = {We propose a theoretically and practically improved density-based, hierarchical clustering method, providing a clustering hierarchy from which a simplified tree of significant clusters can be constructed. For obtaining a "flat" partition consisting of only the most significant clusters (possibly corresponding to different density thresholds), we propose a novel cluster stability measure, formalize the problem of maximizing the overall stability of selected clusters, and formulate an algorithm that computes an optimal solution to this problem. We demonstrate that our approach outperforms the current, state-of-the-art, density-based clustering methods on a wide variety of real world data.},
   author = {Ricardo J G B Campello and Davoud Moulavi and Joerg Sander},
   journal = {Advances in Knowledge Discovery and Data Mining},
   title = {Density-Based Clustering Based on Hierarchical Density Estimates},
   year = {2013},
}
@web_page{nber2022,
   abstract = {Retreived from https://www.nber.org/research/data/icd-9-cm-and-icd-10-cm-and-icd-10-pcs-crosswalk-or-general-equivalence-mappings},
   author = {NBER},
   title = {ICD-9-CM to and from ICD-10-CM and ICD-10-PCS Crosswalk or General Equivalence Mappings},
   url = {https://www.nber.org/research/data/icd-9-cm-and-icd-10-cm-and-icd-10-pcs-crosswalk-or-general-equivalence-mappings},
   year = {2022},
}
@article{Lu2022,
   abstract = {Background: In the United States, national guidelines suggest that aggressive cancer care should be avoided in the final months of life. However, guideline compliance currently requires clinicians to make judgments based on their experience as to when a patient is nearing the end of their life. Machine learning (ML) algorithms may facilitate improved end-of-life care provision for patients with cancer by identifying patients at risk of short-term mortality.
Objective: This study aims to summarize the evidence for applying ML in ≤1-year cancer mortality prediction to assist with the transition to end-of-life care for patients with cancer.
Methods: We searched MEDLINE, Embase, Scopus, Web of Science, and IEEE to identify relevant articles. We included studies describing ML algorithms predicting ≤1-year mortality in patients of oncology. We used the prediction model risk of bias assessment tool to assess the quality of the included studies.
Results: We included 15 articles involving 110,058 patients in the final synthesis. Of the 15 studies, 12 (80%) had a high or unclear risk of bias. The model performance was good: the area under the receiver operating characteristic curve ranged from 0.72 to 0.92. We identified common issues leading to biased models, including using a single performance metric, incomplete reporting of or inappropriate modeling practice, and small sample size.
Conclusions: We found encouraging signs of ML performance in predicting short-term cancer mortality. Nevertheless, no included ML algorithms are suitable for clinical practice at the current stage because of the high risk of bias and uncertainty regarding real-world performance. Further research is needed to develop ML models using the modern standards of algorithm development and reporting.},
   author = {Sheng Chieh Lu and Cai Xu and Chandler H. Nguyen and Yimin Geng and André Pfob and Chris Sidey-Gibbons},
   journal = {Journal of Medical Internet Research},
   keywords = {artificial intelligence,cancer mortality,clinical prediction models,end-of-life care,machine learning},
   month = {3},
   publisher = {JMIR Medical Informatics},
   title = {Machine Learning–Based Short-Term Mortality Prediction Models for Patients With Cancer Using Electronic Health Record Data: Systematic Review and Critical Appraisal},
   year = {2022},
}
@article{Lin2017,
   abstract = {The highest accuracy object detectors to date are based on a two-stage
approach popularized by R-CNN, where a classifier is applied to a sparse set of
candidate object locations. In contrast, one-stage detectors that are applied
over a regular, dense sampling of possible object locations have the potential
to be faster and simpler, but have trailed the accuracy of two-stage detectors
thus far. In this paper, we investigate why this is the case. We discover that
the extreme foreground-background class imbalance encountered during training
of dense detectors is the central cause. We propose to address this class
imbalance by reshaping the standard cross entropy loss such that it
down-weights the loss assigned to well-classified examples. Our novel Focal
Loss focuses training on a sparse set of hard examples and prevents the vast
number of easy negatives from overwhelming the detector during training. To
evaluate the effectiveness of our loss, we design and train a simple dense
detector we call RetinaNet. Our results show that when trained with the focal
loss, RetinaNet is able to match the speed of previous one-stage detectors
while surpassing the accuracy of all existing state-of-the-art two-stage
detectors. Code is at: https://github.com/facebookresearch/Detectron.},
   author = {Tsung Yi Lin and Priya Goyal and Ross Girshick and Kaiming He and Piotr Dollar},
   journal = {IEEE},
   keywords = {Computer vision,convolutional neural networks,machine learning,object detection},
   month = {8},
   publisher = {IEEE Computer Society},
   title = {Focal Loss for Dense Object Detection},
   year = {2017},
}
@article{Chen2016,
   abstract = {Tree boosting is a highly effective and widely used machine learning method.
In this paper, we describe a scalable end-to-end tree boosting system called
XGBoost, which is used widely by data scientists to achieve state-of-the-art
results on many machine learning challenges. We propose a novel sparsity-aware
algorithm for sparse data and weighted quantile sketch for approximate tree
learning. More importantly, we provide insights on cache access patterns, data
compression and sharding to build a scalable tree boosting system. By combining
these insights, XGBoost scales beyond billions of examples using far fewer
resources than existing systems.},
   author = {Tianqi Chen and Carlos Guestrin},
   journal = {KDD},
   keywords = {Large-scale Machine learning,Learning,Machine},
   month = {3},
   publisher = {Association for Computing Machinery},
   title = {XGBoost: A Scalable Tree Boosting System},
   year = {2016},
}
@article{Akiba2019,
   abstract = {The purpose of this study is to introduce new design-criteria for
next-generation hyperparameter optimization software. The criteria we propose
include (1) define-by-run API that allows users to construct the parameter
search space dynamically, (2) efficient implementation of both searching and
pruning strategies, and (3) easy-to-setup, versatile architecture that can be
deployed for various purposes, ranging from scalable distributed computing to
light-weight experiment conducted via interactive interface. In order to prove
our point, we will introduce Optuna, an optimization software which is a
culmination of our effort in the development of a next generation optimization
software. As an optimization software designed with define-by-run principle,
Optuna is particularly the first of its kind. We will present the
design-techniques that became necessary in the development of the software that
meets the above criteria, and demonstrate the power of our new design through
experimental results and real world applications. Our software is available
under the MIT license (https://github.com/pfnet/optuna/).},
   author = {Takuya Akiba and Shotaro Sano and Toshihiko Yanase and Takeru Ohta and Masanori Koyama},
   journal = {KDD 2019 Applied Data Science track},
   keywords = {Bayesian optimization,Black-box optimization,Hyperparameter optimization,Machine learning system},
   month = {7},
   publisher = {Association for Computing Machinery},
   title = {Optuna: A Next-generation Hyperparameter Optimization Framework},
   year = {2019},
}
@web_page{PyTorch2022,
   author = {PyTorch},
   title = {Multi-Label Soft Margin Loss},
   url = {https://pytorch.org/docs/stable/generated/torch.nn.MultiLabelSoftMarginLoss.html},
   year = {2022},
}
@web_page{Optum2022,
   author = {Optum},
   title = {Optum EHR Data},
   url = {https://www.optum.com/business/life-sciences/real-world-data/ehr-data.html},
   year = {2022},
}
@article{Vig2019,
   abstract = {The Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior.},
   author = {Jesse Vig},
   journal = {ACL},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {A multiscale visualization of attention in the transformer model},
   year = {2019},
}
@web_page{,
   author = {Jesse Vig},
   title = {GitHub - jessevig/bertviz: BertViz: Visualize Attention in NLP Models (BERT, GPT2, BART, etc.)},
   url = {https://github.com/jessevig/bertviz},
}
@web_page{shap2023,
   author = {Scott Lundberg},
   title = {shap.GradientExplainer — SHAP latest documentation},
   url = {https://shap-lrjball.readthedocs.io/en/latest/generated/shap.GradientExplainer.html},
}
@article{Sundararajan2017,
   abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms-Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attri-bution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attri-bution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to de-bug networks, to extract rules from a network, and to enable users to engage with models better.},
   author = {Mukund Sundararajan and Ankur Taly and Qiqi Yan},
   title = {Axiomatic Attribution for Deep Networks},
   year = {2017},
}
@article{Erion2020,
   abstract = {One sentence summary: We introduce a method for using axiomatic feature attributions to encourage a wide variety of desirable behaviors in deep models. Abstract Recent research has demonstrated that feature attribution methods for deep networks can themselves be incorporated into training; these attribution priors optimize for a model whose attributions have certain desirable properties-most frequently, that particular features are important or unimportant. These attribution priors are often based on attribution methods that are not guaranteed to satisfy desirable interpretability axioms, such as completeness and implementation invariance. Here, we introduce attribution priors to optimize for higher-level properties of explanations, such as smoothness and sparsity, enabled by a fast new attribution method formulation called expected gradients that satisfies many important interpretability axioms. This improves model performance on many real-world tasks where previous attribution priors fail. Our experiments show that the gains from combining higher-level attribution priors with expected gradients attributions are consistent across image, gene expression, and health care data sets. We believe this work motivates and provides the necessary tools to support the widespread adoption of axiomatic attribution priors in many areas of applied machine learning. The implementations and our results have been made freely available to academic communities. *},
   author = {Gabriel Erion and Joseph D Janizek and Pascal Sturmfels and Scott M Lundberg and Su-In Lee and Paul G Allen},
   journal = {Nature},
   title = {Improving performance of deep learning models with axiomatic attribution priors and expected gradients},
   year = {2020},
}
@web_page{gana2023,
   author = {Keshav Ganapathy and Emily Liu and Zain Zarger and Gowthami Somepalli and Micah Goldblum and Tom Goldstein},
   title = {An Investigation into the Role of Author Demographics in ICLR Participation and Review | OpenReview},
   url = {https://openreview.net/forum?id=1DUwCRNAbA},
}
@web_page{yuan2023,
   author = {Yuan Yuan},
   title = {Exploring Gender Imbalance in AI: Numbers, Trends, and Discussions | Synced},
   url = {https://syncedreview.com/2020/03/13/exploring-gender-imbalance-in-ai-numbers-trends-and-discussions/},
}
@web_page{mantha2023,
   author = {Yoan Mantha},
   title = {Estimating the Gender Ratio of AI Researchers Around the World | by Yoan Mantha | Element AI Lab | Medium},
   url = {https://medium.com/element-ai-research-lab/estimating-the-gender-ratio-of-ai-researchers-around-the-world-81d2b8dbe9c3},
}
@web_page{research2023,
   author = {Research.com},
   title = {Best Computer Science Conferences Ranking Machine Learning & Artificial intelligence 2022 | Research.com},
   url = {https://research.com/conference-rankings/computer-science/machine-learning},
}
@web_page{gender2023,
   author = {Gender API},
   title = {Gender API - Bestimmt das Geschlecht eines Vornamens},
   url = {https://gender-api.com/de},
}
@article{oneil2016,
   abstract = {First edition. We live in the age of the algorithm. Increasingly, the decisions that affect our lives (where we go to school, whether we get a car loan, how much we pay for health insurance) are being made not by humans, but by mathematical models. In theory, this should lead to greater fairness: everyone is judged according to the same rules, and bias is eliminated. But as Cathy O'Neil reveals in this book, the opposite is true. The models being used today are opaque, unregulated, and uncontestable, even when they are wrong. Most troubling, they reinforce discrimination: if a poor student can't get a loan because a lending model deems him too risky (by virtue of his zip code), he is then cut off from the kind of education that could pull him out of poverty, and a vicious spiral ensues. Models are propping up the lucky and punishing the downtrodden, creating a 'toxic cocktail for democracy.' Welcome to the dark side of big data. Tracing the arc of a person's life, O'Neil exposes the black box models that shape our future, both as individuals and as a society. These 'weapons of math destruction' score teachers and students, sort résumés, grant (or deny) loans, evaluate workers, target voters, set parole, and monitor our health. O'Neil calls on modelers to take more responsibility for their algorithms and on policy makers to regulate their use. But in the end, it is up to us to become more savvy about the models that govern our lives. We live in the age of the algorithm. Increasingly, the decisions that affect our lives -- where we go to school, whether we get a car loan, how much we pay for health insurance -- are being made not by humans, but by mathematical models. In theory, this should lead to greater fairness: Everyone is judged according to the same rules, and bias is eliminated. But as Cathy O'Neil reveals in this book, the opposite is true. The models being used today are opaque, unregulated, and uncontestable, even when they're wrong. Most troubling, they reinforce discrimination: If a poor student can't get a loan because a lending model deems him too risky (by virtue of his zip code), he's then cut off from the kind of education that could pull him out of poverty, and a vicious spiral ensues. Models are propping up the lucky and punishing the downtrodden, creating a 'toxic cocktail for democracy.' Welcome to the dark side of Big Data. Tracing the arc of a person's life, O'Neil exposes the black box models that shape our future, both as individuals and as a society. These 'weapons of math destruction' score teachers and students, sort re�sume�s, grant (or deny) loans, evaluate workers, target voters, set parole, and monitor our health. O'Neil calls on modelers to take more responsibility for their algorithms and on policy makers to regulate their use. But in the end, it's up to us to become more savvy about the models that govern our lives. Bomb parts : what is a model? -- Shell shocked : my journey of disillusionment -- Arms race : going to college -- Propaganda machine : online advertising -- Civilian casualties : justice in the age of big data -- Ineligible to serve : getting a job -- Sweating bullets : on the job -- Collateral damage : landing credit -- No safe zone : getting insurance -- The targeted citizen : civic life.},
   author = {Cathy O'Neil},
   isbn = {978-0553418811},
   pages = {259},
   title = {Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy: Cathy O'Neil: 9780553418811: Amazon.com: Books},
   url = {https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815/?tag=timecom-20},
   year = {2016},
}
@article{,
   abstract = {Data is fundamental to the modern world. From economic development to health care to education and public policy, we rely on numbers to allocate resources and make crucial decisions. But because so much data fails to take into account gender, because it treats men as the default and women as atypical, bias and discrimination are baked into our systems. And women pay tremendous costs for this bias, in time, money, and often with their lives. The author investigates this shocking root cause of gender inequality. Examining the home, the workplace, the public square, the doctor's office, and more, the author uncovers a dangerous pattern in data and its consequences on women's lives. Product designers use a 'one-size-fits-all' approach to everything from pianos to cell phones to voice recognition software, when in fact this approach is designed to fit men. Cities prioritize men's needs when designing public transportation, roads, and even snow removal, neglecting to consider women's safety or unique responsibilities and travel patterns. And in medical research, women have largely been excluded from studies and textbooks, leaving them chronically misunderstood, mistreated, and misdiagnosed. Built on hundreds of studies in the US, the UK, and around the world, this is an exposé that will change the readers look at data and the world--Adapted from jacket. Introduction : The default male -- Daily Life. Can snow-clearing be sexist? ; Gender neutral with urinals -- The Workplace. The long Friday ; The myth of meritocracy ; The Henry Higgins effect ; Being worth less than a shoe -- Design. The plough hypothesis ; One-size-fits-men ; A sea of dudes -- Going to the Doctor. The drugs don't work ; Yentl syndrome -- Public Life. A costless resource to exploit ; From purse to wallet ; Women's rights are human rights -- When it Goes Wrong. Who will rebuild? ; It's not the disaster that kills you.},
   author = {Caroline Criado-Perez},
   isbn = {978-1-4197-2907-2},
   pages = {419},
   title = {Invisible Women : Data Bias in a World designed for Men},
}
@undefined{,
}
