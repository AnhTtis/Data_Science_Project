% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{comment}
\usepackage{xcolor}         % colors
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.
\usepackage[linesnumbered,lined,vlined,ruled,commentsnumbered]{algorithm2e}


\newcommand{\stitle}[1]{\vspace{0.8mm} \noindent {\bf #1}}

\newcommand{\myparagraph}[1]{\vspace{0.1em}\noindent\textbf{#1}}
\newcommand{\myparagraphsupp}[1]{\vspace{0.1em}\noindent{\textcolor{red}{#1}}}
\newcommand{\mycaptionsupp}[1]{{\textcolor{red}{#1}}}
\newcommand{\redt}[1]{\textcolor[rgb]{1,0,0}{#1}}
\newcommand{\redtalgo}[1]{\textcolor[rgb]{1,0,0}{#1}}
\newcommand{\updatedredt}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\bluet}[1]{\textcolor[rgb]{0,0,1}{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\cotronlvsapce}{\vspace{-0.0cm}}
\newcommand{\cotronlcaptionvsapce}{\vspace{0.2cm}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\beginsupp}{%
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
        \setcounter{equation}{0}
        \renewcommand{\theequation}{S\arabic{equation}}%
        \setcounter{section}{0}
        \renewcommand\thesection{\Alph{section}}
     }


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{9895} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Freestyle Layout-to-Image Synthesis}

\author{Han Xue$^{1,2}$ \quad Zhiwu Huang$^{2,3}$ \quad Qianru Sun$^{2}$ \quad Li Song$^{1,4}$\footnotemark[1] \quad Wenjun Zhang$^{1}$\\
$^{1}$School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University\\
$^{2}$Singapore Management University \quad $^{3}$University of Southampton\\
$^{4}$MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\\
\tt\small \{xue\_han, song\_li, zhangwenjun\}@sjtu.edu.cn \ \tt\small qianrusun@smu.edu.sg \ \tt\small zhiwu.huang@soton.ac.uk
}

\twocolumn[{
\maketitle
\begin{center}
    \captionsetup{type=figure}
    \includegraphics[width=0.94\linewidth]{figures/pdf-teaser.pdf}
    \vspace{-0.3cm}
    \captionof{figure}{Freestyle Layout-to-Image Synthesis (FLIS) results generated by using our model. 
    Each has two kinds of inputs: a layout of semantic masks (on the 1st column), and a text (on the top of each result).
    For each layout, we show three example results with edited texts (3rd-5th columns).
    They validate that our model is able to introduce new attributes (3rd column), styles (4th column), and objects (5th column), which are all unseen during training, in the synthesized images. The generated hornless unicorn is due to the layout constraint.
    }
    \label{fig:teaser}
\end{center}
}]
\renewcommand{\thefootnote}{\fnsymbol{footnote}} 
\footnotetext[1]{Corresponding author. Part of this work was done when Han Xue served as a visiting Ph.D. student at Singapore Management University.}
% \maketitle


%%%%%%%%% ABSTRACT
\begin{abstract}

Typical layout-to-image synthesis (LIS) models generate images for a closed set of semantic classes, e.g., $182$ common objects in COCO-Stuff.
In this work, we explore the freestyle capability of the model, i.e., how far can it generate unseen semantics (e.g., classes, attributes, and styles) onto a given layout, and call the task Freestyle LIS (FLIS).
Thanks to the development of large-scale pre-trained language-image models, a number of discriminative models (e.g., image classification and object detection) trained on limited base classes are empowered with the ability of unseen class prediction.
Inspired by this, we opt to leverage large-scale pre-trained text-to-image diffusion models to achieve the generation of unseen semantics.
The key challenge of FLIS is how to enable the diffusion model to synthesize images from a specific layout which very likely violates its pre-learned knowledge, e.g., the model never sees ``a unicorn sitting on a bench'' during its pre-training.
To this end, we introduce a new module called Rectified Cross-Attention (RCA) that can be conveniently plugged in the diffusion model to integrate semantic masks.
This ``plug-in'' is applied in each cross-attention layer of the model to rectify the attention maps between image and text tokens.
The key idea of RCA is to enforce each text token to act on the pixels in a specified region, allowing us to freely put a wide variety of semantics from pre-trained knowledge (which is general) onto the given layout (which is specific).
Extensive experiments show that the proposed diffusion network produces realistic and freestyle layout-to-image generation results with diverse text inputs, which has a high potential to spawn a bunch of interesting applications.
Code is available at \href{https://github.com/essunny310/FreestyleNet}{https://github.com/essunny310/FreestyleNet}.

\end{abstract}

\input{sections/1.Introduction}
\input{sections/2.Relatedwork}
\input{sections/3.Problem}
\input{sections/4.Method}
\input{sections/5.Experiment}
\input{sections/6.Conclusion}


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

\clearpage
\input{sections/7.Supplementary}

\end{document}
