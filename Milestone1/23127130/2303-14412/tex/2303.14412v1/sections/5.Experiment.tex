\section{Experiments}\label{sec:exper}

\subsection{Experimental Settings}

\noindent\textbf{Datasets.} We conduct experiments on two challenging datasets: COCO-Stuff~\cite{caesar2018coco} and ADE20K~\cite{zhou2017scene}. COCO-Stuff consists of 118,287 training and 5,000 validation images, which are annotated with 182 semantic classes. ADE20K has 20,210 images for training and 2,000 for validation, including 150 semantic classes.

\noindent\textbf{Implementation Details.} We adopt the original training settings of Stable Diffusion~\cite{rombach2022high} to optimize our model, unless otherwise noted. The base learning rate is set to $10^{-6}$. We use 50 PLMS~\cite{liu2022pseudo} sampling steps with a classifier-free guidance~\cite{ho2022classifier} scale of 2. All the experiments are conducted on a single NVIDIA A100 GPU. 

\subsection{Qualitative Evaluation on FLIS}

\input{figures/Fig5-comparison}
In order to demonstrate the viability of our FreestyleNet for freestyle layout-to-image synthesis (FLIS), we showcase some synthesis results covering a variety of scenarios. As presented in Figures~\ref{fig:teaser} and \ref{fig:my_results}, the proposed model is capable of but not limited to 1) binding new attributes to the objects, 2) specifying the styles for the synthesized images, and 3) generating unseen objects. The model we use to generate these images is fine-tuned on COCO-Stuff. For each sample here, we conduct two kinds of layout-to-image synthesis, \ie, in-distribution and out-of-distribution. For the former, the text input only contains the semantics that are defined in the 182 classes of COCO-Stuff. While for the out-of-distribution synthesis (depicted with a red box), we apply some words with a wide range of new semantics in the text input. Our method consistently produces high-fidelity images that faithfully reveal the novel semantics described in the text while conforming to the given layouts. Some impressive features enabled by our approach such as generating a ``Bugatti Veyron" or hornless ``unicorn" and putting the scene ``in Minecraft" break the in-distribution limit, allowing the users to create the images in a \textit{freestyle} way\footnote{A comparison to a concurrent work ControlNet~\cite{zhang2023adding} is provided on our GitHub page.}.

\subsection{Comparison with LIS Baselines}
We compare our method against the state-of-the-art LIS baselines including Pix2PixHD~\cite{wang2018high}, SPADE~\cite{park2019semantic}, CC-FPSE~\cite{liu2019learning}, OASIS~\cite{sushko2020you}, SC-GAN~\cite{wang2021image}, and PITI~\cite{wang2022pretraining}. \emph{Notably, since generating unseen semantics is beyond the reach of these methods, we conduct the comparison under the in-distribution setting.} While other methods use only one input modality (layout), the proposed model adopts two (text and layout). We follow the standard test settings to evaluate all methods on COCO-Stuff and ADE20K. Fr\'echet Inception Distance (FID)~\cite{heusel2017gans} and mean Intersection-over-Union (mIoU) are adopted to assess the realism of the generated images and the semantic alignment between the generated images and the input layouts, respectively.

\input{tables/Tab1-comparison}
Quantitative comparison results are reported in Table~\ref{tab:comparison}. The proposed model outperforms all the competing methods in terms of the FID metric, indicating the high visual quality of the images synthesized by our method. The most recent method, PITI, also exhibits superior generation quality by leveraging a pre-trained diffusion model. However, it implicitly learns to map the input layouts into the space of the pre-trained text encoder, resulting in clearly worse spatial alignment (reflected in low mIoU values) than ours. Although our method does not suffer from this problem by explicitly constraining the spatial layout of image features through RCA, it obtains less preferred results in mIoU compared to some of the other LIS methods. We argue that mIoU is not fully suitable to evaluate the proposed FLIS method. While the text input does not contain unseen semantics under the in-distribution setting, our method may sometimes generate more general semantics that stem from the pre-trained knowledge (\eg., the wall behind the switch in the 3rd row of Figure~\ref{fig:comparison}), leading to mispredictions of class labels by the segmentation model used to calculate mIoU. In addition, our goal is to synthesize images that are \emph{spatially}, not semantically, aligned with the input layout. As later shown in the visual comparison, our generated images have a strong spatial alignment with the input layouts.

Qualitative comparison results are shown in Figure~\ref{fig:comparison}. Compared to the other baselines, our method consistently produces sharper images with fine details, which is consistent with the quantitative evaluation results.

For a fair comparison, we replace PITI's backbone GLIDE~\cite{nichol2021glide} with Stable Diffusion. The comparison results (see Figure~\ref{fig:piti-wsd} and Table~\ref{tab:fid_comp} in the supplementary) indicate that the clearly improved spatial alignment are due to RCA, and the image quality is also enhanced by our framework.

\subsection{Effectiveness of RCA}
\input{figures/Fig6-effec_RCA}
To further validate the effectiveness of the proposed RCA, we show two representative examples in Figure~\ref{fig:effec_RCA}. In the first row, if we share the rectified attention maps corresponding to ``sky" with those associated with ``drawn by Van Gogh" when specifying the image style, only the sky will be rendered in the style of Van Gogh. The second row showcases that our model is also capable of swapping the appearance of two objects by exchanging their corresponding rectified attention maps. Both of these two cases demonstrate that RCA can force the semantics described by the text to appear in the region specified by the layout.


\subsection{Limitations}
\input{figures/Fig7-failure_case}
We find several failure cases of our method. As presented in Figure~\ref{fig:failure_case}, our model cannot produce satisfactory results (distortion in kite and failed appearance swap) when the specified semantics or scenes are counterfactual (\eg, marshmallow in the sky and horse riding a person).
One possible reason is that the diffusion model itself does not learn sufficient knowledge of these rarely appeared or unreasonable semantics. Another reason could be the fine-tuning on downstream datasets, likely resulting in some damage to pre-trained knowledge.
Our model cannot work as intended if given contradictory layout and prompt (\eg, the unicorn in Figure~\ref{fig:teaser} is hornless). A feasible solution is to relax the layout constraints, \eg, by increasing the rectified attention score when $L_{i,j}^{k}=0$ (indicating regions outside the $k$-th semantic) in \cref{eq2}.
Besides, our method needs to have a user-defined set of category names, which could be expensive to collect in case of long-tailed datasets.

