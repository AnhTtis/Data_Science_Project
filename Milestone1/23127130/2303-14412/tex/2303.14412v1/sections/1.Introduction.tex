%%%%%%%%% BODY TEXT
\section{Introduction}\label{sec:intro}

Layout-to-image synthesis (LIS) is one of the prevailing topics in the research of conditional image generation.
%
It aims to generate complex scenes where a user requires fine controls over the objects appearing in a scene.
%
There are different types of layouts including bboxes+classes~\cite{hinz2019generating}, semantic masks+classes~\cite{park2019semantic,oasis}, key points+attributes~\cite{reed2016learning}, skeletons~\cite{lv2021learning}, to name a few.
%
In this work, we focus on using semantic masks to control object shapes and positions, and using texts to control classes, attributes, and styles.
We aim to offer the user the most fine-grained controls on output images.
%
For example, in Figure~\ref{fig:teaser} (2nd column), the train and the cat are generated onto the right locations defined by the input semantic layouts, and they should also be aligned well with their surroundings.

However, most existing LIS models~\cite{wang2018high,park2019semantic,liu2019learning,oasis,wang2021image} are trained on a specific dataset from scratch. Therefore, the generated images are limited to a closed set of semantic classes, \eg, 182 objects on COCO-Stuff~\cite{caesar2018coco}. While in this paper, we explore the \textit{freestyle} capability of the LIS models for the generation of unseen classes (including their attributes and styles) onto a given layout.
%
For instance, in Figure~\ref{fig:teaser} (5th column), our model stretches and squeezes a ``warehouse'' into a mask of ``train'', and the ``warehouse'' is never seen in the training dataset of LIS. In addition, 
it is also able to introduce novel attributes and styles into the synthesized images (in the 3rd and 4th columns of Figure~\ref{fig:teaser}).
%
The \emph{freestyle} property aims at breaking the in-distribution limit of LIS, and it has a high potential to enlarge the applicability of LIS results to out-of-distribution domains such as data augmentation in open-set or long-tailed semantic segmentation tasks.

To overcome the in-distribution limit, large-scale pre-trained language-image models (\eg, CLIP~\cite{radford2021learning}) 
have shown the ability of learning general knowledge on an enormous amount of image-text pairs. The learned knowledge enables a multitude of discriminative methods trained with limited base classes to predict unseen classes.
%
For instance, the trained models in~\cite{radford2021learning,zhou2022learning,wang2022sprompt} based on pre-trained CLIP are able to do zero-shot or few-shot classification, which is achieved by finding the most relevant text prompt (\eg, \emph{a photo of a cat}) given an image.
%
However, it is non-trivial to apply similar ideas to generative models. In classification, simple texts (\eg, \emph{a photo of a cat}) can be conveniently mapped to labels (\eg, \emph{cat}). It is, however, not intuitive to achieve this mapping from compositional texts (\eg, \emph{a train is running on the railroad with grass on both sides}) to synthesized images.
%
Thanks to the development of pre-training in image generation tasks, we opt to leverage large-scale pre-trained language-image models to the downstream generation tasks.

To this end, we propose to leverage the large-scale pre-trained text-to-image diffusion model to achieve the LIS of unseen semantics. These diffusion models are powerful in generating images of high quality that are in line with the input texts. This can provide us with a generative prior with a wide range of semantics. However, it is challenging to enable the diffusion model to synthesize images from a specific layout which very likely violates its pre-trained knowledge, \eg, the model never sees ``a unicorn sitting on a bench'' during its pre-training. Textual semantics should be correctly arranged in space without impairing their expressiveness.
%
A seminal attempt, PITI~\cite{wang2022pretraining}, learns an encoder to map the layout to the textual
space of a pre-trained text-to-image model. Although it demonstrates improved quality of the generated images, PITI has two main drawbacks. First, it abandons the text encoder of the pre-trained model, losing the ability to freely control the generation results using different texts. Second, its introduced layout encoder implicitly matches the space of the pre-trained text encoder and hence incurs inferior spatial alignment with input layouts.

In this paper, we propose a simple yet effective framework for the suggested freestyle layout-to-image synthesis (FLIS) task. We introduce a new module called Rectified Cross-Attention (RCA) and plug it into a pre-trained text-to-image diffusion model. This “plug-in” is applied in each cross-attention layer of the diffusion model to integrate the input layout into the generation process. 
%
In particular, to ensure the semantics (described by text) appear in a region (specified by layout), we find that image tokens in the region should primarily aggregate information from the corresponding text tokens. In light of this, the key idea of RCA is to utilize the layout to rectify the attention maps computed between image and text tokens, allowing us to put desired semantics from the text onto a layout.
In addition, RCA does not introduce any additional parameters into the pre-trained model, making our framework an effective solution to freestyle layout-to-image synthesis.

As shown in Figure~\ref{fig:teaser}, the proposed diffusion network (FreestyleNet) allows the freestyle synthesis of high-fidelity images with novel semantics, including but not limited to: synthesizing unseen objects, binding new attributes to an object, and rendering the image with various styles. Our main contributions can be summarized as follows:
\begin{itemize}
\item We propose a novel LIS task called FLIS. In this task, we exploit large-scale pre-trained text-to-image diffusion models with layout and text as conditions.

\item We introduce a parameter-free RCA module to plug in the diffusion model, allowing us to generate images from a specified layout effectively while taking full advantage of the model's generative priors.

\item Extensive experiments demonstrate the capability of our approach to translate a layout into high-fidelity images with a wide range of novel semantics, which is beyond the reach of existing LIS models.
\end{itemize}

%-------------------------------------------------------------------------