\section{Related Work}\label{sec:related} 

Deep generative models like Generative Adversarial Networks (GANs)~\cite{DBLP:conf/nips/GoodfellowPMXWOCB14,brock2018large,karras2019style} and diffusion models~\cite{sohl2015deep,ho2020denoising,song2020score,song2020improved,song2020denoising,dhariwal2021diffusion} have achieved impressive results for image synthesis. Compared to unconditional synthesis which yields an image from a random noise, conditional image synthesis models~\cite{fan2022frido,zhang2021ufc} allow more controllability over the generation. Among them, semantic image synthesis and text-based image synthesis are the most relevant to this paper.

\noindent\textbf{Semantic Image Synthesis.} This task aims to generate images based on the given semantic layouts~\cite{chen2017photographic,isola2017image,wang2018high,qi2018semi,park2019semantic,liu2019learning,sushko2020you,zhu2020sean,tang2020dual,zhu2020semantically,wang2021image,tan2021efficient,tan2022semantic,shi2022retrieval,lv2022semantic,wang2022semantic,zhu2022label,alaniz2022semantic}. A pioneer work, Pix2Pix~\cite{isola2017image}, utilizes an encoder-decoder generator and a PatchGAN discriminator to perform semantic image synthesis. Pix2PixHD~\cite{wang2018high} enhances Pix2Pix with coarse-to-fine and multi-scale network architectures for high-resolution image synthesis. SPADE~\cite{park2019semantic} suggests modulating the activations in normalization layers using the affine parameters predicted from the input semantic maps. Instead of modifying the features, CC-FPSE~\cite{liu2019learning} and SC-GAN~\cite{wang2021image} learn to produce convolutional kernels and semantic vectors from the semantic maps to condition the generation. OASIS~\cite{sushko2020you} designs a segmentation-based discriminator to synthesize images with a better alignment to the input label maps. Despite their impressive performance, these methods are limited in generating fully controlled images with diverse semantics. While in this paper, we propose a freestyle layout-to-image synthesis framework that is armed with much more controllability, enabling 1) the generation of semantics beyond the pre-defined semantic categories in the training dataset, and 2) the separate modulation of each class in the layout with text. A recent work, PITI~\cite{wang2022pretraining}, leverages a pre-trained diffusion model to facilitate image-to-image translation, achieving impressive results.
However, its generation is restricted by single-modality condition such as mask.

\noindent\textbf{Text-based Image Synthesis.} Generating images from texts is another popular task of image synthesis. Significant progress has been made by exploiting progressive generation~\cite{zhang2017stackgan}, attention mechanisms~\cite{xu2018attngan}, and cross-modal contrastive approaches~\cite{zhang2021cross}. Recent text-to-image synthesis methods such as GLIDE~\cite{nichol2021glide}, DALL-E2~\cite{ramesh2022hierarchical}, Imagen~\cite{saharia2022photorealistic}, Parti~\cite{yu2022scaling}, CogView2~\cite{ding2022cogview2}, and Stable Diffusion~\cite{rombach2022high} are built upon either auto-regressive or diffusion models. Trained on large-scale image-text pairs, these methods demonstrate an astonishing ability to generate high-fidelity images from texts. Nevertheless, with only text as input, they fall short of fine-grained control over the synthesized results (\eg, specifying the layout).
To advance this, several approaches~\cite{reed2016learning,hong2018inferring,hinz2019generating,liang2022layout} attempt to introduce the location of objects and generate images in a hierarchical manner, but the controls are limited to ambiguous locations (indicated by bboxes). In contrast, our method manages to generate semantics with fine-grained (pixel-level) control. PoE-GAN~\cite{huang2022multimodal} and Make-A-Scene~\cite{gafni2022make} accept both text and layouts, but they support only in-distribution generation. In addition, PoE-GAN~\cite{huang2022multimodal} merely performs global text-based manipulations. Our proposed method supports text-guided and out-of-distribution LIS.

In addition, some recent works succeed in applying the pre-trained text-to-image diffusion models to various applications including text-guided image editing~\cite{hertz2022prompt,kawar2022imagic}, image inpainting~\cite{lugmayr2022repaint}, and personalized generation~\cite{gal2022image,ruiz2022dreambooth}, but none of them can fulfill our goal. In this paper, we excavate the general knowledge learned in a pre-trained text-to-image diffusion model to enable the layout-to-image synthesis to be performed in a freestyle manner.