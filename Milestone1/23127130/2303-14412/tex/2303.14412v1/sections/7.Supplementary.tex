\beginsupp

\noindent
{\Large {\textbf{Supplementary materials}}}
\\

This supplementary material includes an extensive description of Cross-Attention (CA) (\S\ref{sec_CA}), the algorithm of Rectified Cross-Attention (RCA) (\S\ref{sec_algorithm}), additional implementation details (\S\ref{sec_additional_implementation_details}), more qualitative results on freestyle layout-to-image synthesis (FLIS) (\S\ref{sec_more_qualitative_results_on_FLIS}), more comparisons with layout-to-image synthesis (LIS) baselines (\S\ref{sec_more_comparisons_with_LIS_baselines}), the diversity evaluation (\S\ref{sec_diversity_evaluation}), discussions about the optimal form of textual inputs (\S\ref{sec_optimal_form_of_textual_inputs}), more failure cases of our approach (\S\ref{sec_more_failure_cases}), some results on rectangular datasets (\S\ref{sec_results_on_rectangular_dataset}), and discussions about the societal impact (\S\ref{sec_societal_impact}). 

\section{Cross-Attention (CA) in Stable Diffusion}
\label{sec_CA}

\myparagraphsupp{This is supplementary to Section~\textcolor{red}{4.1} ``\textbf{rectifying diffusion model}''.} In this section, we provide an elaboration of Cross-Attention (CA) for a clearer comparison with our proposed Rectified Cross-Attention (RCA). For a CA layer in Stable Diffusion, let $\varphi_{I}$ and $\varphi_{T}$ denote the input image feature and text embeddings, respectively. Image queries $Q$, text keys $K$, and text values $V$ can be calculated by:
\begin{equation}
  Q=W_{Q}\cdot\varphi_{I}, \; K=W_{K}\cdot\varphi_{T}, \; V=W_{V}\cdot\varphi_{T},
  \label{supp_eq1}
\end{equation}
where $W_{Q}$, $W_{K}$, and $W_{V}$ are learnable projection matrices. Then attention score maps $\mathcal{M}$ are computed as:
\begin{equation}
  \mathcal{M}=\frac{QK^T}{\sqrt{d}}\in\mathbb{R}^{C\times H\times W},
  \label{supp_eq2}
\end{equation}
where $d$ is the scaling factor that is set as the dimension of the queries and keys, and $C$, $H$, $W$ are the channel number, height, and weight of $\mathcal{M}$, respectively. After that, we can calculate the output image feature $\mathcal{O}$ of this CA layer by:
\begin{equation}
  \mathcal{O}=\text{softmax}(\mathcal{M})V.
  \label{supp_eq3}
\end{equation}

A visual illustration of CA is shown in Figure~\ref{fig:CA}. In contrast, the proposed RCA rectifies $\mathcal{M}$ via \cref{eq2} in the main paper before applying softmax.

\section{Algorithm}
\label{sec_algorithm}

\myparagraphsupp{This is supplementary to Section~\textcolor{red}{4.1} ``\textbf{rectifying diffusion model}''.} The computation pipeline of RCA is illustrated in Algorithm~\ref{alg}.

\section{Additional implementation details}
\label{sec_additional_implementation_details}
\myparagraphsupp{This is supplementary to Section~\textcolor{red}{5.1} ``\textbf{experimental settings}''.} Training on COCO-Stuff/ADE20K takes about 6/2 days on a single NVIDIA A100 GPU. All our experiments are conducted using Stable Diffusion v1.4.

\input{figures/CA_supp}

\begin{algorithm}
\caption{RCA}
\label{alg}
\SetAlgoLined
\SetKwInput{KwData}{Input}
\SetKwInput{KwResult}{Output}
 \KwData{Input image feature $\varphi_{I}$, text embeddings $\varphi_{T}$, and layout $l$}
 \KwResult{Output image feature $\mathcal{O}$}
Get image queries $Q$, text keys $K$, and text values $V$ by \cref{supp_eq1}\\
Get attention score maps $\mathcal{M}\in\mathbb{R}^{C\times H\times W}$ by \cref{eq1}\\
Initialize a mask $L\in\mathbb{R}^{C\times H\times W}$\\
Resize $l$ to match the spatial size of $\mathcal{M}$\\
\For{$k$ \emph{\textbf{in}} $\{0,1,...,C-1\}$}
{
\uIf{The $k$-th text embedding corresponds to a concept $m$}{
Find the binary map $l^{m}\in\mathbb{R}^{H\times W}$ in $l$ corresponding to this concept\\
$L^{k} \leftarrow l^{m}$\\
}
\Else{
$L^{k} \leftarrow 1$\\
}
}
Get the rectified attention score maps $\widehat{\mathcal{M}}$ by \cref{eq2}\\
Get the output image feature $\mathcal{O}$ by \cref{eq3}
\end{algorithm}

\section{More qualitative results on FLIS}
\label{sec_more_qualitative_results_on_FLIS}

\myparagraphsupp{This is supplementary to Section~\textcolor{red}{5.2} ``\textbf{qualitative evaluation on FLIS}''.}
In Figures~\ref{fig:attribute}, \ref{fig:style}, and \ref{fig:object}, we present more FLIS results by using the proposed model. They demonstrate the capability of our method for FLIS and its high potential to spawn various applications.

\section{More comparisons with LIS baselines} 
\label{sec_more_comparisons_with_LIS_baselines}

\myparagraphsupp{This is supplementary to Section~\textcolor{red}{5.3} ``\textbf{comparison with LIS baselines}''.} 
In this section, we provide more comparison results between SPADE~\cite{park2019semantic}, CC-FPSE~\cite{liu2019learning}, OASIS~\cite{sushko2020you}, SC-GAN~\cite{wang2021image}, PITI~\cite{wang2022pretraining}, and our method. Figures~\ref{fig:comp_coco} and \ref{fig:comp_ade20k} show the results on COCO-Stuff~\cite{caesar2018coco} and ADE20K~\cite{zhou2017scene}, respectively. These results indicate the superiority of our method in generating high-fidelity images in the context of LIS.

For a fair comparison with PITI, we replace its pre-trained text-to-image diffusion model (GLIDE~\cite{nichol2021glide}) with Stable Diffusion~\cite{rombach2022high}. Due to time limits, we carefully tune learning rates only when training its model (we call it PITI w/ SD). Some visual results are provided in Figure~\ref{fig:piti-wsd}. The images synthesized by PITI w/ SD exhibit good visual quality but the spatial alignment with the input layout is poor (clearly poorer than ours). The quantitative comparison results are also provided in Table~\ref{tab:fid_comp}.

Here we compare our FreestyleNet with additional related works including Lab2Pix-V2~\cite{zhu2022label}, sVQGAN-T~\cite{alaniz2022semantic}, and PoE-GAN~\cite{huang2022multimodal}. The comparison results under the in-distribution setting is reported in Table~\ref{tab:addi_comp}. As neither sVQGAN-T~\cite{alaniz2022semantic} nor PoE-GAN~\cite{huang2022multimodal} provide code, their results are copied from their papers. These results showcase our superiority over the others.

\section{Diversity evaluation} 
\label{sec_diversity_evaluation}

\myparagraphsupp{This is supplementary to Section~\textcolor{red}{5.3} ``\textbf{comparison with LIS baselines}''.} In this section, we conduct some experiments to evaluate the generation diversity of different methods. Note that our model naturally enables generation with high diversity from the same layout by using various texts (see Figures~\ref{fig:teaser}, \ref{fig:my_results}, and \ref{fig:effec_RCA} in the main paper). Here we perform the diversity evaluation in the conventional LIS setting. Following OASIS~\cite{sushko2020you}, we calculate LPIPS~\cite{zhang2018unreasonable} between images generated from the same layout (and same text for our model) but with randomly sampled noise. The evaluation results are provided in Table~\ref{tab:diver_eval}. Our model achieves the highest LPIPS among all comparison methods. We also show some visual samples in Figure~\ref{fig:diversity}.

\input{tables/FID_comp}
\input{tables/addi_comp}

\section{Optimal form of textual inputs}
\label{sec_optimal_form_of_textual_inputs}
\myparagraphsupp{This is supplementary to Section~\textcolor{red}{4.1} ``\textbf{rectifying diffusion model}''.} As full-form image descriptions are expensive (or even intractable) to collect, we suggest using the stacked concepts which can be easily obtained from semantic labels. Moreover, stacked concepts fit naturally into the design of RCA, which builds the relationship between each individual semantic and its position on the image. We actually have explored several alternatives (which perform worse), including (1) keyword-to-sentence translation, (2) learnable prompts, and (3) manual construction of full-form prompts for inference. We believe that looking for the optimal form of textual inputs is important, and we will explore it for future work.
\input{tables/diversity_eval}

\section{More failure cases} 
\label{sec_more_failure_cases}

\myparagraphsupp{This is supplementary to Section~\textcolor{red}{5.5} ``\textbf{limitations}''.} 
In Figure~\ref{fig:failure}, we show more failure cases of the proposed model. These results are in line with our conclusion that our method sometimes fails to synthesize counterfactual scenes. This limitation can possibly be alleviated in our future work, by 1) leveraging more powerful pre-trained text-to-image models, and 2) investigating better ways to retain the generative capability of the pre-trained model, perhaps by prompting techniques.

\section{Results on rectangular datasets} 
\label{sec_results_on_rectangular_dataset}
The pre-trained Stable Diffusion that we leverage is designed to generate square (512$\times$512) images. To verify the validity of the proposed method on rectangular datasets, we train our model on Cityscapes~\cite{cordts2016cityscapes}. We resize all images of Cityscapes to 512$\times$512 during training and resize the synthesized results back to the original size in testing phase. As shown in Figure~\ref{fig:cityscapes}, our method yields visually pleasing results.

\section{Societal impact} 
\label{sec_societal_impact}
Our method allows the users to generate diverse images using text and layout. This ability may be maliciously used for  content, which incurs potential negative social impacts such as the spread of fake news and invasion of privacy. To mitigate them, powerful deepfake detection methods that automatically distinguish deepfake images from real ones are needed.

\input{figures/attribute_supp}
% \clearpage
\input{figures/style_supp}
% \clearpage
\input{figures/object_supp}
% \clearpage

\input{figures/comp_coco_supp}
\input{figures/comp_ade20k_supp}
\input{figures/PITI-wSD}

\input{figures/diversity_supp}

\input{figures/failure_supp}

\input{figures/cityscapes_supp}
