\section{Problem Definition}\label{sec:problem_definition}

The objective of general layout-to-image synthesis (LIS) using masks is to learn a mapping function $f(l)=x$ 
that generates a realistic image $x$ from the input mask $l$.
% 
This mapping function is expected to generate semantics onto the given mask,
and is usually trained in a specific dataset.
%
The generated semantics are thus highly constrained by the dataset.
%
To achieve the generation of freestyle semantics,
%
we define a new task called freestyle layout-to-image synthesis (FLIS), and introduce a feasible solution for it.
% 
Specifically, we introduce to condition the model with not only masks but also texts. Masks control the layout of semantics, while texts elaborate what specific semantics to ``put onto'' the layout. 
%
Therefore, FLIS mapping function $f$ can be formulated as $f(l,y)=x$, where $l$ and $y$ are the input masks and texts respectively.

An intuitive approach to initializing the input text $y$ is to use the labels of semantic masks in $l$, \eg, ``train bush grass railroad'' in the first example of Figure~\ref{fig:teaser}.
% 
In any specific image generation dataset, 
the vocabulary of these labels is fixed, \eg, 182 objects on COCO-Stuff~\cite{caesar2018coco}.
%
To go beyond this, we opt to adopt the large-scale diffusion models such as Stable Diffusion \cite{rombach2022high} that have seen far more labels during their pre-training on billions of open data (\eg, LAION-5B \cite{schuhmann2022laion}).
%
We expect to leverage the general knowledge of such models to enable the generation of open-set semantics in the task of FLIS.

The main challenge is how to synthesize images from a specific layout which very likely violates the pre-trained knowledge of such models, \eg, the model has seen the images of ``unicorn'' but never seen ``the unicorn sitting on a bench'' (with the exact relative location of ``unicorn'' and ``bench" being specified) during its pre-training.
%
To solve the issue, we propose a framework with Rectified Cross-Attention (RCA) to explicitly ``inject'' a specific layout (a sequence of masks) into the generation process of pre-trained diffusion models.
% 
The key idea is to ensure that the cross-attention of each text token is only performed in the region defined by its corresponding mask. 
%
The proposed framework will be elaborated in the next section.