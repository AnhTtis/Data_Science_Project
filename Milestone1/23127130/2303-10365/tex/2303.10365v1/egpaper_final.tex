\documentclass[10pt,twocolumn]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{eso-pic}
% \usepackage{everyshi}

\usepackage{algorithm}  
\usepackage{algorithmic} 
% \usepackage{algpseudocode}  
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
% \usepackage{multicol}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{authblk}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% \usepackage[breaklinks=true,bookmarks=false]{hyperref}

\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{CroSel: Cross Selection of Confident Pseudo Labels \\ 
for Partial-Label Learning}

\author[1]{Shiyu Tian}
\author[2]{Hongxin Wei}
\author[1]{Yiqun Wang}
\author[1,2]{Lei Feng \thanks{Corresponding author: feng0093@e.ntu.edu.sg}}
\affil[1]{College of Computer Science, Chongqing University}
\affil[2]{School of Computer Science and Engineering, Nanyang Technological University}


% \author{Shiyu Tian$^1$, Hongxin Wei$^2$, Yiqun Wang$^1$ and Lei Feng$^1$\\
%     $^1$College of Computer Science, Chongqing University\\
%     $^2$School of Computer Science and Engineering, Nanyang Technological University\\
%     \{first.author, second.author, third.author\}@cu-tipaza.dz}

% \authornote{*Corresponding authors}
% \author{Shiyu Tian\\
% Chongqing University\\
% Institution1 address\\


% \author{Shiyu Tian\\
% Chongqing University\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Yiqun Wang\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }
% \and
% Hongxin Wei\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}

% \and
% Lei Fe\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}




\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
Partial-label learning (PLL) is an important weakly supervised learning problem, which allows each training example to have a candidate label set instead of a single ground-truth label. Identification-based methods have been widely explored to tackle label ambiguity issues in PLL, which regard the true label as a latent variable to be identified. However, identifying the true labels accurately and completely remains challenging, causing noise in pseudo labels during model training. In this paper, we propose a new method called CroSel, which leverages historical prediction information from models to identify true labels for most training examples. First, we introduce a cross selection strategy, which enables two deep models to select true labels of partially labeled data for each other. Besides, we propose a novel consistent regularization term called co-mix to avoid sample waste and tiny noise caused by false selection. In this way, CroSel can pick out the true labels of most examples with high precision. Extensive experiments demonstrate the superiority of CroSel, which consistently outperforms previous state-of-the-art methods on benchmark datasets. Additionally, our method achieves over 90\% accuracy and quantity for selecting true labels on CIFAR-type datasets under various settings.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

The past few years have seen an increased interest in deep learning due to its outstanding performance in various application domains, including image processing~\cite{chen2021imageprocession}, automatic driving~\cite{xiong2019autodriving}, and medical diagnosis~\cite{park2018medical}. The success of deep learning heavily relies on a massive amount of fully labeled data. However, it is challenging to obtain a large-scale dataset with completely accurate annotations in the real world. To address this challenge, many researchers have explored a promising weakly supervised learning problem called partial-label learning (PLL)~\cite{cour2011learningPLL2,feng2020provably,wang2022pico,wen2021LWS,wu2022CRDPLL}, which allows each training example to have a set of candidate labels that includes the true label. This problem arises in many real-world tasks such as automatic image annotation~\cite{chen2017autoimageannotation} and facial age estimation~\cite{panis2015faceage}.

As PLL focuses on multi-class classification, there is only one ground-truth label for each training example, and other labels in the candidate label set are actually wrong (false positive) labels, which would have a negative impact on model training. Therefore, there exists the challenge of \emph{label ambiguity} in PLL. To address this challenge, the current mainstream solution is to disambiguate the candidate labels so as to figure out the true label for each training instance~\cite{jin2002learningmultiplelabels,yu2016pllidentimaximum,liu2012PLL-idnbase,feng2020provably,lv2020proden}.
 
However, most of the existing disambiguation methods normally leverage simple heuristics to iteratively update the labeling confidences or pseudo labels~\cite{feng2020provably,lv2020proden,wang2022pico,wu2022CRDPLL}, which could not achieve convincing performance in identifying the true label during the training phase. Generally, if more true labels of training instances can be identified, we can train a better model. This motivates us to focus on identifying the true labels of training instances as many as possible, thereby training a desired model.

In this paper, we propose a method called \textbf{CroSel} (Cross Selection of Confident Pseudo Labels), which leverages historical prediction information from deep neural networks to accurately identify true labels for most training examples. Our selecting criteria are based on the assumption that if a model consistently predicts the same label for an input image with high confidence and low volatility, then that label has a high probability of being the true label for that example. Using the cross selection strategy, the true labels of the vast majority of training examples can be accurately identified, with only negligible noise. Moreover, in order to avoid sample waste and tiny noise resulting from the selection, we also proposed a co-mix consistency regularization to generate trainable targets for all examples. This regulation term serves as an essential complement to our method, which can further enhance the scope and accuracy of our selection of ``true" labels. The algorithm details are shown in Section 3.

Our main contributions are summarized as follows:
\begin{itemize}
\item We propose a cross selection strategy to select the confident pseudo labels in the candidate label set based on historical prediction information. This strategy shows high selection precision and selection ratio of ``true" labels in the experiment.
\item We propose a new consistency loss regularization term that can leverage MixUp~\cite{zhang2017mixup} to enhance the data and generate trainable targets as an important supplement to our method.
\item We experimentally show that CroSel achieves state-of-the-art performance on common benchmark datasets. We also provide extensive ablation studies to examine the effect of the different components of CroSel.

\end{itemize}

\section{Related Work}
\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=1.0\textwidth]{frame.changeorder.png}}
\caption{The left side of the figure is a brief example of our memory bank that stores the softmax output of the model for the last $t$ epochs, which is updated by the FIFO (First In First Out) principle; the middle is the cross selection strategy: within each epoch, data subsets $\mathcal{D}_{\mathrm{sel}}$ with confident pseudo-labels are selected from the MB of each network, which produce loss function $\mathcal{L}_{\mathrm{l}}$ to the training process for the other network;
the right side illustrates our co-mix regularization term and the corresponding loss function $\mathcal{L}_{\mathrm{cr}}$ .}
\label{icml-historical}
\end{center}
\end{figure*}

\noindent \textbf{Partia-Label Learning.\quad} This setting allows each training example to be annotated with a set of candidate labels for which the ground truth label is guaranteed to be included. However, \emph{label ambiguity} can pose a significant challenge in PLL. Early methods used an averaging strategy, which tends to treat each candidate label equally.~\cite{cour2011learningPLL2,zhang2017plldisambiguation} But such methods are easily affected by negative labels in the candidate label set, thus forming wrong classification boundaries. Afterwards, the identification-based method~\cite{jin2002learningmultiplelabels,yu2016pllidentimaximum,liu2012PLL-idnbase} has received more attention from the community, which regard the ground-truth label as a latent variable, and will maintain a confidence level for each candidate label. For instance, Yu \emph{et al.}~\cite{yu2016pllidentimaximum} introduce the maximum margin constraint to PLL problems, trying to optimize the margin between the model outputs from candidate labels and other negative labels. This method has shown better performance in disambiguating labels.

Recently, partial-label learning has been combined with deep networks, leading to significant improvements in performance. Feng \emph{et al.}~\cite{feng2020provably} assume that partial labels come from a uniform generation model and give a mathematical formulation, which is adopted by most of the algorithms proposed later. Based on this, they also propose an algorithm for classification consistency and risk consistency. PRODEN~\cite{lv2020proden} assumes the true label should be the one with the smallest loss among the candidate labels, and improve the classification risk algorithm accordingly. Wen \emph{et al.}~\cite{wen2021LWS} propose a risk-consistent leveraged weighted loss with label-specific candidate label sampling.
PiCO~\cite{wang2022pico} innovatively introduced contrastive learning~\cite{oord2018contrastive} to the field and provided a solid theoretical analysis based on EM. CRDPLL~\cite{wu2022CRDPLL} proposed a new consistency loss item in this field and treat the parts that are not selected as candidate labels as supervision information. SoLar~\cite{wang2022solar} focuses on solutions to PLL problems in more realistic scenarios such as class-imbalanced settings.

\noindent\textbf{Sample selection.\quad} Sample selection is a popular technique in deep learning, especially for datasets with noisy labels or incomplete annotations. Then an obvious idea is to separate the clean samples and noisy samples in the mixed dataset. 
To address this issue, many existing works adopt the small loss criterion~\cite{han2018coteaching,jiang2018mentornet}, which assumes that clean samples tend to have a smaller loss than noisy samples during training.
MentorNet~\cite{jiang2018mentornet} is a representative work that let the teacher model pick up clean samples for the student model. Co-teaching~\cite{han2018coteaching} constructs a double branches network to select clean samples for each branch, which is different from the teacher-student approach since none of the models supervise the other but rather help each other out. This idea was improved by some research later~\cite{yu2019coteachingimprove1,wang2019coteachingimprove2} to achieve better performance.
Curriculum learning is also applied to this field~\cite{han2018CLlabelnoise1}, which considers clean labeled data as an easy task, while noisily labeled data as a harder task. 
Guo \emph{et al.}~\cite{guo2018curriculumnet} split data into subgroups according to their complexities, in order to optimize the training objectives in the early stage of course learning. 
OpenMatch~\cite{saito2021openmatch} trains $n$ OVA classifiers to select the in-distribution samples under the open set setting. 
Generative models such as Beta Mixture Model ~\cite{arazo2019betamixture} and Gaussian Mixture Model~\cite{li2020dividemix} are also used to fit loss functions to distinguish clean labels from noisy labels. Recently, the fluctuation magnitude of the output of the same example is also considered as an important credential to judge whether the label is clean~\cite{wei2022self}.

\section{Our methods}
In this section, we provide a detailed explanation of how our algorithm works. Our method is composed of two main components: a cross selection strategy that utilizes two models to select confident pseudo labels for each other, and a consistency regularization term that is applied across different data augmentation versions. The latter part not only addresses the issue of label waste resulting from the selection process but also enhances the quantity and accuracy of selections. The pseudo-code for our algorithm is presented in Algorithm 1.

\subsection{Problem setting}
Suppose the feature space is $\mathcal{X}\in\mathbb{R}^d$ with $d$ dimensions and the label space is $\mathcal{Y}=\{1,2,\dots,k\}$ with $k$ classes. We are given a dataset $\mathcal{D} =\{(\boldsymbol{x}_i,S_i)\}_{i=1}^n$ with n examples,
where the instance $\boldsymbol{x}_i\in\mathcal{X}$ and the candidate label set $S_i\subset\mathcal{Y}$. Same as previous studies, we assume that the true label $y_i\in\mathcal{Y}$ of each input $\boldsymbol{x}_i$ is concealed in $S_i$.

Our aim is to train a multi-class classifier $f:\mathbb{R}^d\rightarrow\mathbb Y$ that minimizes the classification risk on the given dataset. For our classifier $f$, we use $f(\boldsymbol{x})$ to represent the output of classifier $f$ on given input $\boldsymbol{x}$. And we use $\hat{y}=\mathrm{argmax}_{y\in\mathcal{Y}}f_y(\boldsymbol{x})$ to denote the prediction of our classifier, where $f_y(\boldsymbol{x})$ is the $y\text{-}$th coordinate of $f(\boldsymbol{x})$.


\begin{algorithm*}[tb]
    \centering
   \caption{Pseudo-code of CroSel}
   \label{alg:ours}
\begin{algorithmic}
   \STATE {\bfseries Input:} Training dataset $\mathcal{D}=\{(\boldsymbol{x}_i,S_i)\}_{i=1}^n$, consistency regularization parameter $\lambda_{\mathrm{cr}}$, sharpen parameter $T$, confidence threshold $\gamma$, memory bank $\mathrm{MB}^{(1)}$, $\mathrm{MB}^{(2)}$, network $\Theta^{(1)},\Theta^{(2)}$, epoch $E$, iteration $I$.
   \STATE {\bfseries Procedure:}
   \STATE $\mathrm{MB}^{(1)},\mathrm{MB}^{(2)},\Theta^{(1)},\Theta^{(2)}=\mathrm{WarmUp}(\mathrm{MB}^{(1)},\mathrm{MB}^{(2)},\Theta^{(1)},\Theta^{(2)},\mathcal{D})$. \qquad // \quad CC algorithm
   \FOR{$e=1$ {\bfseries to} $E$}
   \STATE Select labeled dataset $\mathcal{D}_{\mathrm{sel}}^1$ through $\mathrm{MB}^{(1)}$.     
   \STATE Select labeled dataset $\mathcal{D}_{\mathrm{sel}}^2$ through $\mathrm{MB}^{(2)}$.\qquad // \quad Eq. (4)
    \FOR{$k=1,2$ }
    % \qquad // Repeat training process on two models
    \FOR{$i=1$ {\bfseries to} $I$}
    \STATE   Fetch a labeled batch $\hat{B}_i$ from the opposite selected dataset $\mathcal{D}_{\mathrm{sel}}^{\thicksim k}$.
    \STATE   Fetch a  batch $B_i$ from the training dataset $\mathcal{D}$.
    \STATE   Compute the loss $L$ among the two batches $\hat{B}_i$ and $B_i$ through Eq. (13).
    \STATE   Update the weight of $\Theta^k$ by optimizer.
    \ENDFOR    
    \STATE  Update the memory bank $\mathrm{MB}^{(k)}$ through the FIFO principle.
    \ENDFOR
   \ENDFOR
\end{algorithmic}
\end{algorithm*}

\subsection{Selection Strategy}
In the current partial-label learning task, the large number of candidate labels can confuse classifier, making it difficult for the classifier to capture specific features belonging to a certain label. Therefore, our goal is to identify the most likely true label among the candidate labels and eliminate the interference of other negative labels during model training. By selecting these ``true" labels, we can train the data with a supervised learning approach.

\noindent\textbf{Warm up.\quad}Before selecting, we warm up the network using the entire training set. The goal of this stage is to reduce the classification risk of the input $x$ to the whole set of candidate labels $S$, and obtain some historical information that can be used to select. Therefore, here we use CC algorithm~\cite{feng2020provably} to warm up models for 10 epochs.
At the same time, we'll update the value of memory bank $\mathrm{MB}$.

\noindent\textbf{Selecting criteria.\quad}We have three criteria for selecting the confident pseudo labels from the candidate label set.
Depending on the setup of our problem, the true label of each example must be in its candidate label set. This is our first criterion. In addition, we believe that an example's predicted label is likely to be the true label if the model predicts it with high confidence and low fluctuation. To determine the latter, we maintain a memory bank $\mathrm{MB}$ to store the historical prediction information of this neural network.

The size of the memory bank is $t\times n \times k$, where $t$ denotes the length of time it stores, $n$ is the length of dataset, and $k$ is the number of categories in the classification. In other words, $\mathrm{MB}$ stores the output after softmax of the model in the last $t$ epochs. 
$\mathrm{MB}$ is structured as a queue, with each element being a $k$-dimensional vector $\boldsymbol{q}$ representing the output of an example in a particular epoch. We update $\mathrm{MB}$ with the FIFO principle. 
These selection criteria can be summarized as follows.
\begin{align}
\beta_1 &= (\mathrm{argmax}(\boldsymbol{q}^i) \in S),\\
\beta_2 &= (\mathrm{argmax}(\boldsymbol{q}^i) == \mathrm{argmax}(\boldsymbol{q}^{i+1})),\\
\beta_3 &= (\frac{1}{t} \sum_{i=1}^{t} \mathrm{max}(\boldsymbol{q}^i) > \gamma).
\end{align}
where $i=1,2...t$, $t \geq 2 $, $\gamma$ is the confidence threshold of selection.
$\beta_1$ lets our selected label be in the candidate label set. $\beta_2$ limits that the label we picked has not flipped in the past $t$ epochs, which is a volatility consideration and $\beta_3$ ensures that the label we selected has a high confidence level. The final selected dataset is $\mathcal{D}_{\mathrm{sel}}$:
\begin{equation}
\mathcal{D}_{\mathrm{sel}} = {((\boldsymbol{x}_i,\mathrm{argmax}(\boldsymbol{q}^t_i)) | (\beta_1^i \wedge \beta_2^i \wedge \beta_3^i) =1, \boldsymbol{x}_i \in \mathcal{D})}
\end{equation}

\noindent\textbf{Selected label loss.\quad}
After selecting the high-confidence examples, we obtain a dynamically updated data subset $\mathcal{D}_{\mathrm{sel}}=(X,\hat{Y})$. Each tuple of the subset has an instance $\boldsymbol{x}$ and a selected label $\hat{y}=\mathrm{argmax}(\boldsymbol{q}^t)$. In this configuration, we can use the basic cross-entropy loss to deal with this part of samples.
\begin{equation}
\mathcal{L}_{\mathrm{l}} =\frac{1}{|\mathcal{D}_{\mathrm{sel}}|} { \sum_{\boldsymbol{x} \in \mathcal{D}_{\mathrm{sel}}}} \mathrm{CE}(f(\boldsymbol{x}_{\mathrm{w}}),\hat{y})
\end{equation}
where $\mathrm{CE}(\cdot,\cdot)$ denotes the cross entropy loss, $\boldsymbol{x}_{\mathrm{w}}$ denotes the weak augmented version of example $\boldsymbol{x}$.

\noindent\textbf{Cross Selection.\quad}
However, In the process of selecting labels, it is difficult to guarantee that the selected labels are 100\% accurate.
In order to maximize the accuracy of selection, we propose a cross selection framework based on the idea of ensemble learning. Specifically, we train two identical models $\Theta^{(1)}$ and $\Theta^{(2)}$ through the same training and label selection process. By forming different decision boundaries, the two models can adaptively correct most of the errors even if there is noise in the selected confident pseudo labels. 

To maximize the accuracy of the selected labels, we employ a cross-supervised training process. This involves using the selected dataset $\mathcal{D}_{\mathrm{sel}}^1$ from $\mathrm{MB}^{(1)}$ to train $\Theta^{(2)}$, and vice versa, using the selected dataset $\mathcal{D}_{\mathrm{sel}}^2$ from $\mathrm{MB}^{(2)}$ to train $\Theta^{(1)}$. By doing so, the two models can learn from each other and further improve their ability to select true labels.
In the test process, we will average the output of the two models to reduce the variance.
\begin{equation}
f^{\prime}(\boldsymbol{x}) =\frac{1}{2} (f^1(\boldsymbol{x})+f^2(\boldsymbol{x}))
\end{equation}
where $f^{\prime}(\boldsymbol{x})$ denotes the final output of our method in the test process, $f^{1}(\boldsymbol{x})$ denotes the output of $\Theta^{(1)}$, $f^{2}(\boldsymbol{x})$ denotes the output of $\Theta^{(2)}$.

\subsection{Co-mix Consistency Regulation}
\noindent\textbf{Motivation.\quad} When dealing with complex partial-label learning tasks, our label selection strategy may not accurately select the true labels for all examples. If we only use the selected examples with their corresponding labels, it will result in a significant amount of wasted data, which contradicts our goal of utilizing as many examples as possible. Therefore, we aim to provide a trainable target for the remaining examples that are not selected. 
However, our setting differs from traditional semi-supervised learning as the proportion of unlabeled examples is relatively small. So it is unreasonable to directly transfer the existing semi-supervised learning tools to unlabeled data like other weakly supervised learning methods. 

Motivated by this, we hope to propose a regularization term that can serve as an important supplement to our method and help us select examples.
We proposed the co-mix regularization term, which employs two widely used data augmentation methods: weak augmentation and strong augmentation, to generate pseudo-labels as training targets for consistency regularization. We further employ MixUp to further enhance the data. It is worth mentioning that the term "pseudo labels" in this part specifically refers to the soft labels generated from different data augmentation versions, rather than the confident hard labels selected during the selection process.

% We assign a pseudo target $\boldsymbol{p}_i$ for each augmented image $\boldsymbol{x}{aug1}$, which is generated by another augment transformation of the image $\boldsymbol{x}{aug2}$. With the help of these pseudo-labels, we can process samples like normal supervised learning. This can also be seen as another form of consistent regularization loss. 

\noindent\textbf{Pseudo label generating.\quad}Consistency loss is a simple but effective idea in weakly supervised learning, whose key point is to reduce the gap between the output of two perturbed examples after passing through the model. As discussed in the previous section, we used two widely used data augmentations: 'weak' and 'strong', and crossed them to generate pseudo labels. Specifically, as stated in Figure 1, the pseudo label corresponding to weak augmented example is generated by strong augmented example, while the pseudo label corresponding to strong augmented example is generated by weak augmented example.

To generate these pseudo labels, we fix the parameters of the neural network, and pass the augmented images through the model to get the logits output. And we perform two operations on the logits, sharpening and normalization. For sharpening operation, we use a hyper parameter $T$, the more $T$ goes to zero, the more logits tend to become a one-hot distribution. These two operations can be summarized by the following formula.
\begin{equation}
\boldsymbol{p}_{i}=
\begin{cases}
\frac{\exp(f_j(\boldsymbol{x})^{\frac{1}{T} })}{ {\textstyle \sum_{j\in S} \exp(f_j(\boldsymbol{x})^{\frac{1}{T} })} }& \text{$i \in S$}\\
0& \text{$i \notin S$}
\end{cases}
\end{equation}
where $\boldsymbol{p}_{i}$ denotes the $i\text{-}_{th}$ coordinate of pseudo label.

\noindent\textbf{MixUp.\quad} After generating the pseudo labels of each example, we end up with two datasets that can be trained: $(X_{\mathrm{w}},P_{\mathrm{s}})$ and $(X_{\mathrm{s}},P_{\mathrm{w}})$. The subscripts \emph{w} and \emph{s} indicate the type of data augmentation used, i.e., weak or strong. Then, We spliced the two datasets together to further enhance the data with MixUp.~\cite{zhang2017mixup} For a pair of two examples with their corresponding pseudo labels $(\boldsymbol{x}_1, \boldsymbol{p}_1),(\boldsymbol{x}_2, \boldsymbol{p}_2)$, we compute $(\boldsymbol{x}^{\prime}, \boldsymbol{p}^{\prime})$ by the following formula: 
\begin{equation}
\lambda  \sim{\mathrm{Beta}(\alpha,\alpha)}
\end{equation}
\begin{equation}
\lambda^{\prime} = \mathrm{max}(\lambda,1-\lambda)
\end{equation}
\begin{equation}
\boldsymbol{x}^{\prime}  =\lambda^{\prime}\boldsymbol{x}_1+(1-\lambda^{\prime})\boldsymbol{x}_2
\end{equation}
\begin{equation}
\boldsymbol{p}^{\prime}  =\lambda^{\prime}\boldsymbol{p}_1+(1-\lambda^{\prime})\boldsymbol{p}_2
\end{equation}
Then we can use the typical cross-entropy loss on every example, the consistency regulation loss will be:
\begin{equation}
\mathcal{L}_{\mathrm{cr}} = \frac{1}{2n} { \sum_{i=1}^{2n}} \mathrm{CE}(f(\boldsymbol{x}^{\prime}),{\boldsymbol{p}^{\prime}})
\end{equation}
where $\mathrm{CE}(\cdot,\cdot)$ denotes the softmax cross entropy loss, $n$ denotes the length of a dataset.

\begin{table*}[t]
\small
\caption{Accuracy(mean ± std) comparisons on benchmark datasets.}
\label{Main reults}
\begin{center}
\begin{tabular}{l|c|cccccc}
\toprule
Dataset & $q$ & Ours & CRDPLL & PiCO & PRODEN & LWS & CC  \\
\midrule
\multicolumn{1}{c|}{\multirow{3}{*}{CIFAR-10}}    & $q=0.1$& 97.31 ± 0.04\% &\textbf{97.41 ± 0.06}\% & 96.10 ± 0.06\% &95.66 ± 0.08\% &91.20 ± 0.07\% &  90.73 ± 0.10\% \\
\multicolumn{1}{c|}{} & $q=0.3$& \textbf{97.50 ± 0.05}\% & 97.38 ± 0.04\%  & 95.74 ± 0.10\% & 95.21 ± 0.07\% & 89.20 ± 0.09\% & 88.04 ± 0.06\% \\
\multicolumn{1}{c|}{}    & $q=0.5$& \textbf{97.34 ± 0.05}\% & 96.76 ± 0.05\% & 95.32 ± 0.12\% & 94.55 ± 0.13\% & 80.23 ± 0.21\% & 81.01 ± 0.38\% \\
\hline
\multicolumn{1}{c|}{\multirow{3}{*}{SVHN}}    & $q=0.1$& \textbf{97.71 ± 0.05}\% &97.63 ± 0.06\% & 96.58 ± 0.04\% &96.20 ± 0.07\% &96.42 ± 0.09\% &  96.99 ± 0.17\% \\
\multicolumn{1}{c|}{} & $q=0.3$& \textbf{97.96 ± 0.05}\% & 97.65 ± 0.07\%  & 96.32 ± 0.09\% & 96.11 ± 0.05\% & 96.15 ± 0.08\% & 96.67 ± 0.20\% \\
\multicolumn{1}{c|}{}    & $q=0.5$& \textbf{97.86 ± 0.06}\% & 97.70 ± 0.05\% & 95.78 ± 0.05\% & 95.97 ± 0.03\% & 95.79 ± 0.05\%  & 95.83 ± 0.23\% \\
\hline
\multicolumn{1}{c|}{\multirow{3}{*}{CIFAR-100}}    & $q=0.01$& \textbf{84.24 ± 0.09}\% &82.95 ± 0.10\% & 74.89 ± 0.11\% &72.24 ± 0.12\% &62.03 ± 0.21\% &  66.91 ± 0.24\% \\
\multicolumn{1}{c|}{} & $q=0.05$& \textbf{83.92 ± 0.24}\% & 82.38 ± 0.09\%  & 73.26 ± 0.09\% & 70.03 ± 0.18\% & 57.10 ± 0.17\% & 64.51 ± 0.37\% \\
\multicolumn{1}{c|}{}    & $q=0.10$& \textbf{84.07 ± 0.16}\% & 82.15 ± 0.20\% & 70.03 ± 0.10\% & 69.82 ± 0.11\% & 52.60 ± 0.54\%  & 61.50 ± 0.36\%\\
\hline
\end{tabular}
\end{center}
\end{table*}

\subsection{Algorithm overview}

\noindent\textbf{Overall loss.\quad} In the formal training phase, our loss function will be composed of two parts, the supervised loss $\mathcal{L}_l$ in the selected label set $\mathcal{D}_{\mathrm{sel}} $ and the consistency regularization item loss $\mathcal{L}_{\mathrm{cr}}$. The two will be dynamically combined into the final loss function by a hyperparameter.
\begin{equation}
\mathcal{L}_{\mathrm{all}} =  \mathcal{L}_{\mathrm{l}} + \mathcal{L}_{\mathrm{cr}} * \lambda_{\mathrm{d}}
\end{equation}
$\lambda_{\mathrm{d}}$ is a dynamically changing parameter, $\mathcal{L}_l$ and $\mathcal{L}_{\mathrm{cr}}$ can be calculated by Eq. (5) and Eq. (12), respectively. The use of MixUp can cause significant changes to the original feature space, making it necessary to adjust the weight of the regularization term in the loss function as the number of selected samples increases. 
To achieve this, we proposed a gradually decreasing $\lambda_{\mathrm{d}}$ with the increase of selected samples. We can control the magnitude of $\lambda_{\mathrm{d}}$ with a set hyperparameter $\lambda_{\mathrm{cr}}$. The updated rules of $\lambda_{\mathrm{d}}$ are as follows.
\begin{equation}
\lambda_{\mathrm{d}} = (1- r_{\mathrm{s}})* \lambda_{\mathrm{cr}}
\end{equation}
where $r_{\mathrm{s}}$ denotes the percentage of labeled data that we picked out, $\lambda_{\mathrm{cr}}$ is a hyperparameter that collaboratively adjusts the ratio of two loss items.

\section{Experiments}
In this section, we provide a detailed description of the experiments we conducted. We introduce the experimental setup and present our main experimental results. Additionally, we present the findings of our ablation experiments.

\subsection{Experimental Setup}
\noindent\textbf{Datasets.\quad}We used three widely used benchmarks in this field: SVHN~\cite{svhn}, CIFAR-10~\cite{krizhevsky2009cifar} and CIFAR-100~\cite{krizhevsky2009cifar}.  The way we generate partial labels is by flipping the negative labels $\overline{y} \neq y$ of the example with a set probability $q=P(\overline{y} \in S|\overline{y} \neq y)$. With the increase of $q$, the noise of the dataset increases gradually. Following PiCO~\cite{wang2022pico}, we consider $q=\{0.01,0.05,0.1\}$ for CIFAR-100 and $q=\{0.1,0.3,0.5\}$ for other datasets.


\noindent\textbf{Compared methods.\quad} We choose five well-performed partial-label learning algorithms to compare: 
\begin{itemize}
    \item CRDPLL~\cite{wu2022CRDPLL}, an algorithm that takes non-candidate labels as supervision information and proposes a new consistency loss term between augmented images.
    \item PiCO~\cite{wang2022pico}, a theoretical solid framework that combines contrastive learning and prototype-based label disambiguation algorithm.
    \item LWS~\cite{wen2021LWS}, an algorithm that wants to balance the risk error between the candidate label set and the non-candidate label set.
    \item PRODEN~\cite{lv2020proden}, a self-training algorithm that dynamically updates the confidence of candidate labels.
    \item CC~\cite{feng2020provably}, an algorithm that wants to minimize the classification error of the whole candidate label sets.
    % \item RC~\cite{feng2020provably}, a algorithm that based on risk-consistency.
\end{itemize}

\begin{table}[htpb]
\caption{Selected ratio and selected accuracy(mean ± std) on benchmark datasets. S-ratio represents the selected ratio and S-acc represents selected accuracy in $\mathcal{D}_{\mathrm{sel}}$.}
\label{Co-selct results:Sratio and Sacc}
\begin{center}
\begin{tabular}{c|c|c|c}
\toprule
\multicolumn{1}{l|}{Datasets} & Setting                  & Index          &  Performance\\ 
\midrule
\multirow{6}{*}{CIFAR-10}     & \multirow{2}{*}{$q=0.1$} & S-ratio &  99.09 ± 0.07\% \\
                              &                          & S-acc   &  99.79 ± 0.05\%\\ \cline{2-4}
                              & \multirow{2}{*}{$q=0.3$} & S-ratio  &  98.10 ± 0.10\%\\
                              &                          & S-acc   &  99.55 ± 0.03\%\\ \cline{2-4}
                              & \multirow{2}{*}{$q=0.5$} &  S-ratio&  96.25 ± 0.12\%\\
                              &                          & S-acc  &  99.44 ± 0.06\%\\ \hline
\multirow{6}{*}{SVHN}     & \multirow{2}{*}{$q=0.1$} & S-ratio &  97.25 ± 0.14\% \\
                              &                          & S-acc   &  99.84 ± 0.06\% \\ \cline{2-4}
                              & \multirow{2}{*}{$q=0.3$} & S-ratio  &  76.42 ± 0.21\%\\
                              &                          & S-acc   &  99.77 ± 0.06\%\\ \cline{2-4}
                              & \multirow{2}{*}{$q=0.5$} &  S-ratio&  73.21 ± 0.15\%\\
                              &                          & S-acc  &  99.34 ± 0.02\%\\ \hline
\multirow{6}{*}{CIFAR-100}     & \multirow{2}{*}{$q=0.01$} & S-ratio &  96.58 ± 0.13\% \\
                              &                          & S-acc   &  99.71 ± 0.06\% \\ \cline{2-4}
                              & \multirow{2}{*}{$q=0.05$} & S-ratio  &  95.45 ± 0.21\%\\
                              &                          & S-acc   &  98.29 ± 0.15\%\\ \cline{2-4}
                              & \multirow{2}{*}{$q=0.10$} &  S-ratio&  93.61 ± 0.12\%\\
                              &                          & S-acc  &  97.93 ± 0.11\%\\ \hline

\end{tabular}
\end{center}
\end{table}

\noindent\textbf{Implementations.\quad} Our implementation is based on PyTorch~\cite{paszke2019pytorch}. We use WRN-34-10 (short for Wide-ResNet-34-10) as the backbone model with a weight decay of 0.0001 on all the datasets for all methods. We set the batch size as 64 and total epochs as 200, using SGD as optimizer with a momentum of 0.9, and set the initial learning rate as 0.1, which is divided by 10 after 100 and 150 epochs respectively. For the hyper parameter in our method, We set $t=3$, $\alpha=0.75$, $T=0.5$ for all datasets, and $\lambda_{\mathrm{cr}}=1$ for CIFAR-100, $\lambda_{\mathrm{cr}}=4$ for others. For the selection threshold, we set $\gamma=0.9$ for CIFAR-type datasets, and $\gamma=0.85$ for SVHN. We present the mean and standard deviation in each case based on three independent runs with different random seeds. 


\begin{table}[!t]
\caption{Results for the study of scope of co-mix regularization.}
\label{Ablation comix results}
\begin{center}
\begin{tabular}{c|c|c|c}
\toprule
\multicolumn{1}{l|}{Setting} & Scope         & Index          &  Performance\\ 
\midrule
\multirow{9}{*}{\makecell[c]{CIFAR-10 \\ $q=0.5$}}     & \multirow{3}{*}{All data} & acc &  97.34\% \\
                              &                          & S-ratio   &  96.25\%\\ 
                              &                          & S-acc   &  99.44\%\\ \cline{2-4}
                              & \multirow{3}{*}{Unseleted data} & acc  &  90.32\%\\
                              &                          & S-ratio   &  93.27\%\\ 
                              &                          & S-acc   &  95.72\%\\ \cline{2-4}
                              & \multirow{3}{*}{None} &  acc&  81.01\%\\
                              &                          & S-ratio   &  90.23\%\\ 
                              &                          & S-acc  &  89.72\%\\ \hline
\multirow{9}{*}{\makecell[c]{CIFAR-100 \\ $q=0.1$}}     & \multirow{3}{*}{All data} & acc &  84.07\% \\
                              &                          & S-ratio   &  93.61\%\\ 
                              &                          & S-acc   & 97.93\%\\ \cline{2-4}
                              & \multirow{3}{*}{Unseleted data} & acc  &  77.61\%\\
                              &                          & S-ratio   &  90.12\%\\ 
                              &                          & S-acc   &  97.63\%\\ \cline{2-4}
                              & \multirow{3}{*}{None} &  acc&  70.68\%\\
                              &                          & S-ratio   &  78.65\%\\ 
                              &                          & S-acc  &  96.22\%\\ \hline
\end{tabular}
\end{center}
\end{table}

\subsection{Main empirical results}

As shown in Table 1, our methods achieve state-of-the-art results on all the settings except CIFAR-10 with $q=0.1$. Notably, on the complex dataset CIFAR-100, our method significantly improves performance.

However, a counter-intuitive phenomenon appears in our experiment, that is, the performance of our method does not necessarily decline strictly with the increase of noise $q$; on the contrary, it may perform best in the case of moderate noise. We believe the possible reason for this phenomenon is that: On the one hand, when generating pseudo labels, we  normalize them according to Eq. (7). That is to say, as the number of candidate labels increases, the ingredients involved in MixUp will also increase, leading to better-enhanced data interpolation. On the other hand, the increase of candidate labels also represents the increase of noise, which can have a negative impact on the algorithm's performance. Therefore, CroSel performs well in cases of intermediate noise, representing an optimal solution found in such a trade-off problem.

Table 2 presents the selection ratio and accuracy of CroSel in selecting the true labels for the selected label dataset $\mathcal{D}_{\mathrm{sel}}$. It is evident that, CroSel can accurately select the true labels of most examples in the dataset under various noise conditions. Notably, for CIFAR-10 and CIFAR-100, the selection ratio and accuracy are over $90\%$ both. However, in SVHN, the selection ratio drops significantly with the increase of noise. This could be attributed to the fact that digital images in SVHN have relatively simple shape features, and MixUp may significantly disturb the feature space.

\begin{figure*}[!t]
\centering
\subfigure[CTAFR-10 Accuracy]{
	\label{fig:subfig:a} %% label for first subfigure
	\includegraphics[width=2.2in,height=1.9in]{cifar10acc70-100.png}}
\subfigure[CTAFR-10 Selected Ratio]{
	\label{fig:subfig:c} %% label for second subfigure
	\includegraphics[width=2.2in,height=1.9in]{cifar10sratio.png}} 
\subfigure[CTAFR-10 Selected Accuracy]{
	\label{fig:subfig:b} %% label for second subfigure
	\includegraphics[width=2.2in,height=1.9in]{cifar10sacc99-100.png}}
 \subfigure[CTAFR-100 Accuracy]{
	\label{fig:subfig:d} %% label for first subfigure
	\includegraphics[width=2.2in,height=1.9in]{cifar100acc60-85.png}}
\subfigure[CTAFR-100 Selected Ratio]{
	\label{fig:subfig:e} %% label for second subfigure
	\includegraphics[width=2.2in,height=1.9in]{cifar100sratio.png}}
\subfigure[CTAFR-100 Selected Accuracy]{
	\label{fig:subfig:f} %% label for second subfigure
	\includegraphics[width=2.2in,height=1.9in]{cifar100sacc.png}}
\caption{Parameter test for $\lambda_{\mathrm{cr}}$ on CTAFR-10 and CTAFR-100.}
\label{fig:Parameter test} %% label for entire figure
\end{figure*}

\subsection{Ablation study}

\noindent\textbf{Parameter test on $\lambda_{\mathrm{cr}}$.\quad}In this experiment, we test the parameter $\lambda_{\mathrm{d}}$, which weights the contribution of the consistency regularization term to the training loss. As mentioned in  Eq. (14), the parameter $\lambda_{\mathrm{d}}$ is directly influenced by the hyperparameter $\lambda_{\mathrm{cr}}$. Therefore, We test $\lambda_{\mathrm{cr}}=\{1,2,4\}$ on CIFAR-10 ($q=0.5$) and CIFAR-100 ($q=0.1$). At the same time, we also try to fix the parameter $\lambda_{\mathrm{d}}$, that is, its value is not related to the selection ratio $r_{\mathrm{s}}$. This setting we denote by $\lambda_{\mathrm{cr}}(fix)$, we test $\lambda_{\mathrm{cr}}(fix)=\{0.5,1,2\}$. The results are visualized in Figure 2, and detailed data can be found in Appendix B.3.

As mentioned in Table 2 above, CroSel achieves a very high selection ratio. When using a dynamically changing $\lambda_{\mathrm{d}}$, the contribution made by the regularization term would be quite small in the later stages as the learning rate decays. In contrast, with a fixed value of $\lambda_{\mathrm{d}}$, the regularization term would still have a significant contribution in the later stages. Although the accuracy rate on the test set does not show a significant difference between different parameters, the selection effect is critical. A too large contribution of the regularization item will slightly improve the selection ratio at the cost of a decrease in the selection accuracy, which goes against the original intention of our algorithm design. Therefore, we finally decided to use dynamically changing parameters. In other words, we want the algorithm to return to a supervised learning setting with minimal noise as much as possible at the end of the training process.


\noindent\textbf{The influence on the scope of consistency regulation term.\quad}The preceding ablation experiment explored the magnitude of the co-mix regularization, while this experiment focused on the scope of the regularization term. As described in Section 3, our co-mix regularization is designed to avoid sample waste. As such, a natural idea is to apply the regularization term to the unselected samples, as in traditional semi-supervised learning. However, our setting differs from traditional semi-supervised learning in that our unselected data only constitutes a small portion of all the data. So we conducted experiments by trying three cases: no regularization term, using the regularization term only for the unselected dataset, and using the regularization term for all examples.

The results in Table 3 suggest that with the expansion of the application scope of co-mix regularization term, the model's performance steadily improves, and the number of selected labeled examples also gradually increases. This also shows that our co-mix regularization term and selection strategy can achieve a mutually beneficial effect.

\begin{table}[htpb]
\small
\caption{Accuracy(mean ± std) on ablation study on select criteria.}
\label{Ablation t results}
\begin{center}
\begin{tabular}{c|c|c|c|c}
\toprule
Setting & $t$ & Accuracy & $\gamma$ & Accuracy\\
\midrule
\multicolumn{1}{c|}{\multirow{3}{*}{\makecell[c]{CIFAR-10 \\ $q=0.3$}}}    & $t=2$ &97.03\%& $\gamma=0.8$& 96.24\%  \\
\multicolumn{1}{c|}{} & $t=3$ & 97.50\%& $\gamma=0.9$ & 97.50\% \\
\multicolumn{1}{c|}{} & $t=4$ & 96.15\% & $\gamma=0.95$ & 97.38\%\\
\hline
\multicolumn{1}{c|}{\multirow{3}{*}{\makecell[c]{CIFAR-100 \\ $q=0.1$}}}    & $t=2$ & 82.74\%& $\gamma=0.8$& 80.20\% \\
\multicolumn{1}{c|}{} & $t=3$ & 84.07\%& $\gamma=0.9$ & 84.07\% \\
\multicolumn{1}{c|}{} &$ t=4$ & 83.56\%& $\gamma=0.95$ & 83.56\% \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\noindent\textbf{Whether the selection criteria are strict or lenient.\quad}
The two parameters $t$ and $\gamma$ in Eq. (2) and Eq. (3) determine the strictness of our selection criteria. $t$ represents the length of historical information stored in $\mathrm{MB}$, while $\gamma$ represents the select threshold for the average prediction confidence of the model for the example prediction in the past $t$ epochs. A larger $t$ and a higher $\gamma$ represent a stricter selection criterion, resulting in a smaller $\mathcal{D}_{\mathrm{sel}}$ size but higher precision, which can affect model training. During the early stages, it may be difficult to select enough examples under the condition of using stricter selection criteria. Therefore, in this and the next ablation experiment, we set the label flipping probability $q$ to 0.3 on CIFAR-10. However, in the later stages, the impact of selection criteria on the final accuracy rate is not significant if the initial stage is passed smoothly. Our experiment shows that $t=3$ and $\gamma=0.9$ are suitable values that can be applied to most experimental environments. The experimental results are presented in Table 4 and Appendix B.3.


\begin{table}[!htpb]
\caption{Results for Augmentations test on $\mathcal{D}_{\mathrm{sel}}$.}
\label{Results for Parameter test on $D_{sel}$}
\begin{center}
\begin{tabular}{c|c|ccc}
\toprule
Setting & Aug type & Accuracy & Selected ratio\\
\midrule
\multicolumn{1}{c|}{\multirow{3}{*}{\makecell[c]{CIFAR-10 \\ $q=0.3$}}}    &None& 96.91\% & 97.44\%  \\
\multicolumn{1}{c|}{} & Weak& 97.50\% & 98.10\%  \\
\multicolumn{1}{c|}{} & Strong& 97.22\% & 96.26\% \\
\hline
\multicolumn{1}{c|}{\multirow{3}{*}{\makecell[c]{CIFAR-100 \\ $q=0.1$}}}    & None&83.74\% & 90.48\%  \\
\multicolumn{1}{c|}{} & Weak& 84.07\% & 93.61\%  \\
\multicolumn{1}{c|}{} & Strong& 81.01\% & 79.16\% \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\noindent\textbf{The influence on data augmentation on $\mathcal{D}_{\mathrm{sel}}$.\quad}Data augmentations play a crucial role in weakly supervised learning. However, our selection criteria rely on historical prediction information to select examples with high confidence, and we cannot guarantee that stronger data augmentations will necessarily lead to better results and selection effects. So, in this part, we explore the impact of data augmentation on $\mathcal{D}_{\mathrm{sel}}$ and its influence on label selection and overall training. As shown in Table 5, weak augmentation is a more appropriate and effective choice. However, because of the impact of regular items, even if data augmentation is not used on $\mathcal{D}_{\mathrm{sel}}$, there is no significant performance degradation. Strong data augmentation has an adverse effect on the selection of examples, especially in CIFAR-100, which may also be related to the fact that the historical predictions stored in $\mathrm{MB}$ are produced by data that have not been augmented.

\noindent\textbf{The influence on double model.\quad}As recognized by the community, dual models tend to achieve better performance than single models. We are curious about how effective our selection criteria would be without the adaptive error correction capability of cross selection.  Figure 3 visualizes the gap in the selection ratio between dual-model and single-model training. It shows that our cross selection strategy can select samples more comprehensively, and on average, about 10\% more training examples can be selected on each dataset. However, the effectiveness of our algorithm is not solely due to the dual model. Even when using a single model with our selection criteria, the accuracy rate on the test set only decreases by 0.83\% and 2.68\% on CIFAR-10 and CIFAR-100 respectively. Furthermore, the high precision of selection is reflected in both settings. Detailed results can be found in Table 6 and Appendix.B.3.

\begin{figure}[!t]
\centering
\subfigure[CTAFR-10 comparison]{
	\label{fig:subfig:cifar10} %% label for second subfigure
	\includegraphics[width=1.5in,height=1.6in]{cifar10comparation_sratio.png}}
\subfigure[CTAFR-100 comparison]{
	\label{fig:subfig:cifar100} %% label for second subfigure
	\includegraphics[width=1.5in,height=1.6in]{cifar100comparation_sratio.png}}
\caption{Selection ratio comparison between dual model and single model on CTAFR-10 and CTAFR-100.}
\label{fig:Selection ratio comparison} %% label for entire figure
\end{figure}

\begin{table}[!htpb]
\caption{Comparison between Dual model and Single model. \\
S-acc represents selected accuracy in $\mathcal{D}_{\mathrm{sel}}$.}
\label{comparison between dual model and single model}
\begin{center}
\begin{tabular}{c|c|cc}
\toprule
Setting & Model & Accuracy & S-acc\\
\midrule
\multicolumn{1}{c|}{\multirow{2}{*}{\makecell[c]{CIFAR-10 \\ $q=0.5$}}}    &Single model& 96.51\%  & 99.72\%  \\
\multicolumn{1}{c|}{} & Dual model& 97.34\% & 99.44\%  \\
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{\makecell[c]{CIFAR-100 \\ $q=0.1$}}}    & Single model&81.39\%  & 98.35\%  \\
\multicolumn{1}{c|}{} & Dual model& 84.07\%& 97.93\%  \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\section{Conclusion}
This work introduces CroSel, a novel partial-label learning method that leverages historical prediction information to select the confident pseudo labels from candidate label sets. The proposed method consists of two parts: a cross selection strategy that enables two deep models to select ``true" labels for each other, and a consistent regularization term co-mix that avoids sample waste and tiny noise caused by false selection. Empirically, extensive experiments demonstrate the superiority of CroSel, which consistently outperforms previous state-of-the-art methods on multiple benchmark datasets.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egpaper_final}
}

\appendix

\onecolumn
\section{Notation and definitions}
\begin{table}[htpb]
\caption{Notation and definitions}
\label{Notations}
\begin{center}
\begin{tabular}{c|c}
\toprule
Notations & Definitions\\
\midrule
$\boldsymbol{x}$ & A picture representing an input example of a neural network\\
\hline
$S$ & A candidate label set of an example\\
\hline
$\mathrm{MB}$ & A memory bank that stores historical prediction information\\
\hline
$\boldsymbol{q}$ & a $k$-dimensional vector representing the output of an example in a particular epoch\\
\hline
$\Theta$ & The model’s parameters\\
\hline
$\mathrm{CE}(\cdot,\cdot)$ & Cross-entropy between two distributions \\
\hline
$f(\boldsymbol{x})$ & the output of classifier $f$ on given input $x$\\
\hline
$\mathcal{D}_{\mathrm{sel}}$ & The selected dataset\\
\hline
$\mathcal{D}$ & The initial dataset\\
\hline
$\hat{y}$ & The selected label of an example\\
\hline
$\beta$ & A sign that complies with the criteria for selecting labels\\
\hline
$T$ & Temperature parameter for sharpening used in MixUp\\
\hline
$\boldsymbol{p}$ & The pseudo-label generated by co-mix before Mixup\\
\hline
$\boldsymbol{p}^{\prime}$ & The pseudo-label generated by co-mix after Mixup\\
\hline
$\boldsymbol{x}^{\prime}$ & An input example after Mixup\\
\hline
$\lambda_{\mathrm{cr}}$ & A hyperparameter set to the co-mix regularization\\
\hline
$\lambda_{\mathrm{d}}$ & A parameter weighting the contribution of the co-mix regularization to the training loss\\
\hline
$r_{\mathrm{s}}$ & The percentage of labeled data that we picked out \\
\hline
$\gamma$ & A hyperparameter that represents the threshold of confidence for picking the true label\\
\hline
$t$ & A hyperparameter representing the length of time the $\mathrm{MB}$ stores\\





\bottomrule
\end{tabular}
\end{center}
\end{table}

\section{Experimental details}
\subsection{Datasets}
\begin{itemize}
    \item \textbf{CIFAR-10:} It contains 60,000 32 X 32 RGB color pictures in a total of 10 categories. Including 50,000 for the training set and 10,000 for the test set.
    \item \textbf{CIFAR-100:} It has 100 classes, each containing 600 32 X 32 RGB color images. Each category has 500 training images and 100 test images. The 100 classes in CIFAR-100 are divided into 20 superclasses. Each image has a "fine" tag (the class to which it belongs) and a "rough" tag (the superclass to which it belongs).
    \item \textbf{SVHN:} It derived from Google Street View door numbers, each image contains a set of Arabic numbers' 0-9 '. The training set contained 73,257 numbers, the test set 26,032 numbers, and 531,131 additional numbers. Each number is a 32 X 32 color picture.
\end{itemize}

\subsection{Data augmentations}
Data augmentation is widely used in weakly supervised learning algorithms. There are two types of data augmentations used in our algorithm: "weak" and "strong". For "weak" augmentation, it is just a standard flip-and-shift augmentation strategy consisting of Randomcrop and RandomHorizontalFlip. For "strong" augmentations, We use RandAugment strategy for all, which randomly selects the type and magnitude of data augmentation with the same probability.

\newpage
\subsection{Detailed results}

\subsubsection{Detailed results for Parameter test on $\lambda_{cr}.$}

\begin{table}[!htpb]
\caption{Detailed results for Parameter test on $\lambda_{cr}.$}
\label{Ablation lambda-cr results}
\begin{center}
\begin{tabular}{l|c|ccc}
\toprule
Setting & $\lambda_{cr} $& Accuracy & Selected ratio &Selected accuracy\\
\midrule
\multicolumn{1}{c|}{\multirow{6}{*}{\makecell[c]{CIFAR-10 \\ $q=0.5$}}}    &$\lambda_{cr}=1$& 95.94\% & 91.57\% &99.51\% \\
\multicolumn{1}{c|}{} & $\lambda_{cr}=2$ & 96.80\% & 97.32\% & 99.17\% \\
\multicolumn{1}{c|}{} & $\lambda_{cr}=4$ & 97.33\% & 96.25\% & 99.44\% \\
\multicolumn{1}{c|}{} & $\lambda_{cr}=1(fixed)$ & 96.88\% & 87.88\% & 99.73\% \\
\multicolumn{1}{c|}{} & $\lambda_{cr}=2(fixed)$ & 96.95\% & 92.19\% & 99.68\% \\
\multicolumn{1}{c|}{} & $\lambda_{cr}=0.5(fixed)$ & 96.16\% & 95.21\% & 99.44\% \\
\hline
\multicolumn{1}{c|}{\multirow{6}{*}{\makecell[c]{CIFAR-100 \\ $q=0.1$}}}    &$\lambda_{cr}=1$& 84.02\% & 93.61\% &97.93\% \\
\multicolumn{1}{c|}{} & $\lambda_{cr}=2$ & 83.61\% & 94.15\% & 97.78\% \\
\multicolumn{1}{c|}{} & $\lambda_{cr}=4$ & 83.88\% & 95.63\% & 97.59\% \\
\multicolumn{1}{c|}{} & $\lambda_{cr}=1(fixed)$ & 83.91\% & 99.35\% & 96.12\% \\
\multicolumn{1}{c|}{} & $\lambda_{cr}=2(fixed)$ & 84.03\% & 99.17\% & 96.15\% \\
\multicolumn{1}{c|}{} & $\lambda_{cr}=0.5(fixed)$ & 83.48\% & 97.02\% & 96.81\% \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

% \subsection{Detailed results for ablation study on select criteria}
\subsubsection{Detailed results for the influence of parameter $t$.}\

\begin{table}[!htpb]
\caption{Detailed results for Parameter test on $t$}
\label{Detailed results for Parameter test on $t$}
\begin{center}
\begin{tabular}{l|c|ccc}
\toprule
Setting & $t$ & Accuracy & Selected ratio &Selected accuracy\\
\midrule
\multicolumn{1}{c|}{\multirow{3}{*}{\makecell[c]{CIFAR-10 \\ $q=0.3$}}}    &$t=2$& 97.03\% & 99.07\% &99.62\% \\
\multicolumn{1}{c|}{} & $t=3$ & 97.50\% & 98.10\% & 99.55\% \\
\multicolumn{1}{c|}{} & $t=4$ & 96.15\% & 89.23\%& 99.76\% \\
\hline
\multicolumn{1}{c|}{\multirow{3}{*}{\makecell[c]{CIFAR-100 \\ $q=0.1$}}}    & $t=2$ &82.74\% & 87.32\% &98.50\% \\
\multicolumn{1}{c|}{} & $t=3$ & 84.07\% & 93.61\% & 97.93\% \\
\multicolumn{1}{c|}{} & $t=4$ & 83.56\% & 93.24\% &98.23\% \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsubsection{Detailed results for the influence of parameter $\gamma$.}\

\begin{table}[!htpb]
\caption{Detailed results for Parameter test on 
$\gamma$}
\label{Detailed results for Parameter test on gamma}
\begin{center}
\begin{tabular}{l|c|ccc}
\toprule
Setting & $\gamma$ & Accuracy & Selected ratio &Selected accuracy\\
\midrule
\multicolumn{1}{c|}{\multirow{3}{*}{\makecell[c]{CIFAR-10 \\ $q=0.3$}}}    &$\gamma=0.8$& 96.24\% & 95.29\% &99.08\% \\
\multicolumn{1}{c|}{} & $\gamma=0.9$ & 97.50\% & 98.10\% & 99.55\% \\
\multicolumn{1}{c|}{} & $\gamma=0.95$ & 97.38\% & 93.10\%& 99.87\% \\
\hline
\multicolumn{1}{c|}{\multirow{3}{*}{\makecell[c]{CIFAR-100 \\ $q=0.1$}}}    & $\gamma=0.8$ &80.20\% & 82.11\% &98.63\% \\
\multicolumn{1}{c|}{} & $\gamma=0.9$ & 84.07\% & 93.61\% & 97.93\% \\
\multicolumn{1}{c|}{} & $\gamma=0.95$ & 81.23\% & 67.32\% & 99.26\% \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\newpage
\subsubsection{Detailed results for the data augmentation for $\mathcal{D}_{\mathrm{sel}}$}

\begin{table}[!htpb]
\caption{Detailed results for Parameter test on $\mathcal{D}_{\mathrm{sel}}.$}
\label{Detailed results for Parameter test on $D_{sel}$}
\begin{center}
\begin{tabular}{l|c|ccc}
\toprule
Setting & Data augmentation type & Accuracy & Selected ratio &Selected accuracy\\
\midrule
\multicolumn{1}{c|}{\multirow{3}{*}{\makecell[c]{CIFAR-10 \\ $q=0.3$}}}    &No augmentation& 96.91\% & 97.44\% &99.60\% \\
\multicolumn{1}{c|}{} & Weak augmentation & 97.50\% & 98.10\% & 99.55\% \\
\multicolumn{1}{c|}{} & Strong augmentation & 97.22\% & 96.26\%& 99.75\% \\
\hline
\multicolumn{1}{c|}{\multirow{3}{*}{\makecell[c]{CIFAR-100 \\ $q=0.1$}}}    & No augmentation &83.74\% & 90.48\% &98.32\% \\
\multicolumn{1}{c|}{} & Weak augmentation & 84.07\% & 93.61\% & 97.93\% \\
\multicolumn{1}{c|}{} & Strong augmentation & 81.01\% & 79.16\% & 98.98\% \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsubsection{Detailed results for comparison between Dual model and Single model}
\begin{table}[!htpb]
\caption{Detailed results of comparison between Dual model and Single model.}
\label{Detailed results of comparison between dual model and single model}
\begin{center}
\begin{tabular}{c|c|ccc}
\toprule
Setting & Model & Accuracy & Selected ratio & Selected accuracy\\
\midrule
\multicolumn{1}{c|}{\multirow{2}{*}{\makecell[c]{CIFAR-10 \\ $q=0.5$}}}    &Single model& 96.51\% &87.61\% & 99.72\%  \\
\multicolumn{1}{c|}{} & Dual model& 97.34\% &96.25\%& 99.44\%  \\
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{\makecell[c]{CIFAR-100 \\ $q=0.1$}}}    & Single model&81.39\% &85.39\% & 98.35\%  \\
\multicolumn{1}{c|}{} & Dual model& 84.07\%& 93.61\%& 97.93\%  \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\end{document}
