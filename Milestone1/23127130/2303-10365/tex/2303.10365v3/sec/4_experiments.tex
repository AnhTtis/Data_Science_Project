\section{Experiments}
% In this section, we provide a detailed description of the experiments we conducted. We will introduce the experimental setup and present our main experimental results. Additionally, we will present the findings of our ablation experiments.
\subsection{Experimental Setup}
\noindent\textbf{Datasets.\quad}We used three widely used benchmarks in this field: SVHN~\cite{svhn}, CIFAR-10~\cite{krizhevsky2009cifar} and CIFAR-100~\cite{krizhevsky2009cifar}.  The way we generate partial labels is by flipping the negative labels $\overline{y} \neq y$ of the example with a set probability $q=P(\overline{y} \in S| \overline{y} \neq y)$. With the increase of $q$, the noise of the dataset increases gradually. Following PiCO~\cite{wang2022pico}, we consider $q=\{0.01,0.05,0.1\}$ for CIFAR-100 and $q=\{0.1,0.3,0.5\}$ for other datasets.


\noindent\textbf{Compared methods.\quad} We choose six well-performed partial-label learning algorithms to compare: 
\begin{itemize}
    \item PoP~\cite{xu2023pop}, an algorithm that updates the learning model and purifies each candidate label set progressively in every epoch.
    \item CRDPLL~\cite{wu2022CRDPLL}, an algorithm that takes non-candidate labels as supervision information and proposes a new consistency loss term between augmented images.
    \item PiCO~\cite{wang2022pico}, a theoretical solid framework that combines contrastive learning and prototype-based label disambiguation algorithm.
    \item LWS~\cite{wen2021LWS}, an algorithm that wants to balance the risk error between the candidate label set and the non-candidate label set.
    \item PRODEN~\cite{lv2020proden}, a self-training algorithm that dynamically updates the confidence of candidate labels.
    \item CC~\cite{feng2020provably}, an algorithm that wants to minimize the classification error of the whole candidate label sets.
\end{itemize}

\begin{table}[!t]
\caption{Selection ratio and selection accuracy (mean$\pm$std) on benchmark datasets. S-ratio represents the selection ratio and S-acc represents selection accuracy in $\mathcal{D}_{\mathrm{sel}}$.}
\label{main scc+sration table}
% \vspace{-0.5cm}
\label{Co-selct results:Sratio and Sacc}
\resizebox{0.475\textwidth}{!}{
\setlength{\tabcolsep}{4mm}{
\begin{tabular}{c|c|c|c}
\toprule
\multicolumn{1}{l|}{Datasets} & Setting                  & Index          &  Performance\\ 
\midrule
\multirow{6}{*}{CIFAR-10}     & \multirow{2}{*}{$q=0.1$} & S-ratio &  99.09$\pm$.07\% \\
                              &                          & S-acc   &  99.79$\pm$.05\%\\ \cmidrule{2-4}
                              & \multirow{2}{*}{$q=0.3$} & S-ratio  &  98.10$\pm$.10\%\\
                              &                          & S-acc   &  99.55$\pm$.03\%\\ \cline{2-4}
                              & \multirow{2}{*}{$q=0.5$} &  S-ratio&  96.25$\pm$.12\%\\
                              &                          & S-acc  &  99.44$\pm$.06\%\\ \midrule
\multirow{6}{*}{SVHN}     & \multirow{2}{*}{$q=0.1$} & S-ratio &  97.25$\pm$.14\% \\
                              &                          & S-acc   &  99.84$\pm$.06\% \\ \cmidrule{2-4}
                              & \multirow{2}{*}{$q=0.3$} & S-ratio  &  76.42$\pm$.21\%\\
                              &                          & S-acc   &  99.77$\pm$.06\%\\ \cmidrule{2-4}
                              & \multirow{2}{*}{$q=0.5$} &  S-ratio&  73.21$\pm$.15\%\\
                              &                          & S-acc  &  99.34$\pm$.02\%\\ \midrule
\multirow{6}{*}{CIFAR-100}     & \multirow{2}{*}{$q=0.01$} & S-ratio &  96.58$\pm$.13\% \\
                              &                          & S-acc   &  99.71$\pm$.06\% \\ \cmidrule{2-4}
                              & \multirow{2}{*}{$q=0.05$} & S-ratio  &  95.45$\pm$.21\%\\
                              &                          & S-acc   &  98.29$\pm$.15\%\\ \cmidrule{2-4}
                              & \multirow{2}{*}{$q=0.10$} &  S-ratio&  93.61$\pm$.12\%\\
                              &                          & S-acc  &  97.93$\pm$.11\%\\ \midrule

\end{tabular}
}
}
\vspace{-0.4cm}
\end{table}

\noindent\textbf{Implementations.\quad} Our implementation is based on PyTorch~\cite{paszke2019pytorch}. We use WRN-34-10 (short for Wide-ResNet-34-10) as the backbone model with a weight decay of 0.0001 on all the datasets for all compared methods. We present the mean and standard deviation in each case based on three independent runs with different random seeds. More detailed hyper-parameter setting can be found in Supplementary Materials.

% \vspace{-1.0cm}

\subsection{Main Empirical Results}
As shown in Table \ref{Main reults}, our methods achieve state-of-the-art results on all the settings except CIFAR-10 with $q=0.1$. Notably, on the complex dataset CIFAR-100, our method significantly improves performance.

However, a counter-intuitive phenomenon appears in our experiment, that is, the performance of our method does not necessarily decline strictly with the increase of noise magnitude $q$; on the contrary, it may perform best in the case of moderate noise. We believe the possible reason for this phenomenon is that: On the one hand, when generating pseudo labels, we normalize them according to Eq. (7). That is to say, as the number of candidate labels increases, the ingredients involved in MixUp will also increase, leading to better-enhanced data interpolation. On the other hand, the increase of candidate labels also represents the increase of noise, which can have a negative impact on the algorithm's performance. Therefore, CroSel performs well in cases of intermediate noise, representing an optimal solution found in such a trade-off problem.

Table \ref{main scc+sration table} presents the selection ratio and accuracy of CroSel in selecting the true labels for the subdataset $\mathcal{D}_{\mathrm{sel}}$. It is evident that, CroSel can accurately select the true labels of most examples in the dataset under various noise conditions. Notably, for CIFAR-10 and CIFAR-100, the selection ratio and accuracy are over $90\%$ both. However, in SVHN, the selection ratio drops significantly with the increase of noise. This could be attributed to the fact that digital images in SVHN have relatively simple shape features, and MixUp may significantly disturb the feature space.

\noindent\textbf{More challenging experimental settings.\quad} We conducted fine-grained settings on CIFAR-100 by limiting the candidate labels to the super-class of the true label, which is closer to reality. We set $q = 0.5$, indicating that half of the remaining superclass labels for each example may be converted into candidate labels. Our method still exhibits superior performance in this scenario. The results can be found in Table \ref{Results on CIFAR-100 for fine-grained settings}. 

\begin{table}[!t]
\centering
\caption{Results on CIFAR-100 in fine-grained settings.}
\label{Results on CIFAR-100 for fine-grained settings}
% \vspace{-0.3cm}
%\label{comparison between dual model and single model}
\resizebox{0.475\textwidth}{!}{
\setlength{\tabcolsep}{5mm}{
\begin{tabular}{c|c|c|c}
\toprule
Method & Accuracy &Method & Accuracy\\
\midrule
PoP & 82.04\% & CRDPLL & 81.53\% \\
PiCO & 73.38\% & PRODEN & 71.16\% \\
LWS & 54.08\% & CC & 64.91\% \\
\midrule
\multicolumn{2}{c|}{Crosel (ours) } & \multicolumn{2}{c}{ \textbf{83.34\%} } \\
\bottomrule
\end{tabular}
}
}
\vspace{-0.3cm}
\end{table}

\subsection{Ablation Studies}
In this section, our primary objective is to demonstrate the collective effectiveness of all components within our method. Subsequently, we will delve into illustrating the consequences of either the absence or adjustment of certain components on the results.

\noindent\textbf{All the components matter.} Our algorithm comprises several components, including a selection strategy with three selection criteria and a regularization term. 
We conducted comprehensive ablation studies on CIFAR-100 with $q=0.1$ to further prove that every component of our algorithm matters. The results are shown in Table \ref{table:main ablation}. Notably, the absence of any component leads to a significant performance decline.
The Co-mix regularization term serves as a training target for the examples that are not selected which can improve the selection accuracy, selection number, and final test accuracy when utilizing the same selection criteria. 
% We will discuss the effect of the scope of the regularization term in more detail later.

% selection criteria
Compared with using a combination of three criteria, employing individual selection criteria implies a relaxation of selection standards. When regularization terms are employed conventionally, this relaxation inevitably leads to decreased accuracy. While it enables the selection of more samples, it also introduces additional noise that can significantly misguide the model's training process.
In scenarios where no regularization terms are utilized, the imposition of more stringent selection criteria poses challenges in acquiring sufficient data for training. In such cases, relaxing the selection criteria serves to expand the model's training data, thereby enhancing the overall performance of the model and subsequently improving its test performance.

% Although this relaxation leads to a reduction in selection accuracy, it enables the model to select a larger number of samples, 

% resulting in a better performance with the absence of regularization. Conversely, in the presence of a regularization term, applying more stringent criteria to uphold high selection accuracy with a slight drop in the selected ratio can result in superior results.

% Compared with using a combination of three criteria, employing individual selection criteria leads to a drop in selection accuracy. Interestingly, this decline can be advantageous in the absence of a regularization term, as it facilitates the selection of a larger number of training examples. However, in the presence of a regularization term, applying more stringent criteria to uphold high selection accuracy and minimize noise produces superior results.

\begin{table}[!t]
\centering
\caption{Results of thorough ablation experiments.}
\vspace{-0.3cm}
\label{table:main ablation}
\resizebox{0.475\textwidth}{!}{
\setlength{\tabcolsep}{2.5mm}{
\begin{tabular}{cccc|ccc}
\toprule
cr1 & cr2 & cr3 & $\mathcal{L}_{\mathrm{cr}}$ & Acc & S-acc& S-ratio\\
\midrule
\checkmark & & & & 73.12\% & 95.15\% & 90.32\% \\
\checkmark & \checkmark& & & 72.00\% & 93.51\% & 85.09\% \\
\checkmark & \checkmark& \checkmark& & 70.68\% & 96.22\% & 78.65\% \\
\checkmark & & & \checkmark& 77.04\% & 91.02\% & 97.27\% \\
\checkmark & \checkmark& & \checkmark & 79.90\% & 94.66\% & 95.51\% \\
\checkmark &\checkmark &\checkmark & \checkmark& \textbf{84.07\%} & \textbf{97.93\%} & \textbf{93.61\%} \\
\bottomrule
\end{tabular}
}
}
\end{table}

\begin{table}[!t]
\centering
\caption{Results for ablation studies on the scope of regularization.}
\vspace{-0.3cm}
\label{Ablation scope}
\resizebox{0.475\textwidth}{!}{
\setlength{\tabcolsep}{3mm}{
\begin{tabular}{c|c|c|c}
\toprule
\multicolumn{1}{l|}{Setting} & Scope         & Index          &  Performance\\ 
\midrule
\multirow{9}{*}{\makecell[c]{CIFAR-10 \\ $q=0.5$}}     & \multirow{3}{*}{All data} & Acc &  \textbf{97.34\%} \\
                              &                          & S-ratio   &  \textbf{96.25\%}\\ 
                              &                          & S-acc   &  \textbf{99.44\%}\\ \cline{2-4}
                              & \multirow{3}{*}{Unselected data} & Acc  &  90.32\%\\
                              &                          & S-ratio   &  93.27\%\\ 
                              &                          & S-acc   &  95.72\%\\ \cline{2-4}
                              & \multirow{3}{*}{None} &  Acc&  81.01\%\\
                              &                          & S-ratio   &  90.23\%\\ 
                              &                          & S-acc  &  89.72\%\\ \hline
\multirow{9}{*}{\makecell[c]{CIFAR-100 \\ $q=0.1$}}     & \multirow{3}{*}{All data} & Acc &  \textbf{84.07\%} \\
                              &                          & S-ratio   &  \textbf{93.61\%}\\ 
                              &                          & S-acc   & \textbf{97.93\%}\\ \cline{2-4}
                              & \multirow{3}{*}{Unselected data} & Acc  &  77.61\%\\
                              &                          & S-ratio   &  90.12\%\\ 
                              &                          & S-acc   &  97.63\%\\ \cline{2-4}
                              & \multirow{3}{*}{None} &  Acc&  70.68\%\\
                              &                          & S-ratio   &  78.65\%\\ 
                              &                          & S-acc  &  96.22\%\\ \hline
\end{tabular}
}
}
% \vspace{-0.3cm}
\end{table}

\noindent\textbf{The influence on the scope of the consistency regularization term.\quad}
% The preceding ablation experiment explored the magnitude of the co-mix regularization, while this experiment focused on the scope of the regularization term. 
As described in Section \ref{section:method}, our co-mix regularization is designed to avoid sample waste. As such, a natural idea is to apply the regularization term to the unselected samples, as in traditional semi-supervised learning. However, our setting differs from traditional semi-supervised learning in that our unselected data only constitutes a small portion of all the data. So we conducted experiments by trying three cases: no regularization term, using the regularization term only for the unselected dataset, and using the regularization term for all examples.

The results in Table \ref{Ablation scope} suggest that with the expansion of the application scope of co-mix regularization term, the model's performance steadily improves, and the number of selected labeled examples also gradually increases. This also shows that our co-mix regularization term and selection strategy can achieve a mutually beneficial effect.

\begin{table}[!t]
\centering
\caption{Accuracy for ablation study on selection criteria.}
\vspace{-0.3cm}
\label{Ablation select criteria}
\resizebox{0.475\textwidth}{!}{
\setlength{\tabcolsep}{3mm}{
\begin{tabular}{c|c|c|c|c}
\toprule
Setting & $t$ & Accuracy & $\gamma$ & Accuracy\\
\midrule
\multicolumn{1}{c|}{\multirow{3}{*}{\makecell[c]{CIFAR-10 \\ $q=0.3$}}}    & $t=2$ &97.03\%& $\gamma=0.80$& 96.24\%  \\
\multicolumn{1}{c|}{} & $t=3$ & 97.50\%& $\gamma=0.90$ & 97.50\% \\
\multicolumn{1}{c|}{} & $t=4$ & 96.15\% & $\gamma=0.95$ & 97.38\%\\
\hline
\multicolumn{1}{c|}{\multirow{3}{*}{\makecell[c]{CIFAR-100 \\ $q=0.1$}}}    & $t=2$ & 82.74\%& $\gamma=0.80$& 80.20\% \\
\multicolumn{1}{c|}{} & $t=3$ & 84.07\%& $\gamma=0.90$ & 84.07\% \\
\multicolumn{1}{c|}{} &$ t=4$ & 83.56\%& $\gamma=0.95$ & 83.56\% \\
\bottomrule
\end{tabular}
}
}
% \vspace{-0.5cm}
\end{table}

\noindent\textbf{Impact of other parameters on selection criteria.\quad}
Except for the number of selection rules, the two parameters $t$ and $\gamma$ in Eq. (2) and Eq. (3) determine the strictness of our selection criteria. $t$ represents the length of historical prediction stored in $\mathrm{MB}$, while $\gamma$ represents the select threshold for the average prediction confidence of the model for the example prediction in the past $t$ epochs. A larger $t$ and a higher $\gamma$ represent a stricter selection criterion, resulting in a smaller $\mathcal{D}_{\mathrm{sel}}$ size but higher precision, which can affect model training. During the early stages, it may be difficult to select enough examples under the condition of using stricter selection criteria. Therefore, in this experiment, we set the label flipping probability $q$ to 0.3 on CIFAR-10. However, in the later stages, the impact of selection criteria on the final accuracy rate is not significant if the initial stage is passed smoothly. Our experiment shows that $t=3$ and $\gamma=0.9$ are suitable values that can be applied to most experimental environments. The experimental results are presented in Table \ref{Ablation select criteria} and Supplementary Materials.

\begin{figure*}
\centering
  \begin{subfigure}{0.33\linewidth}
    % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=2.2in,height=1.9in]{img/cifar10acc70-100.pdf}
    \caption{CIFAR-10 Accuracy}
    \label{fig:suba}
  \end{subfigure}
  % \hfill
  \begin{subfigure}{0.33\linewidth}
    \includegraphics[width=2.2in,height=1.9in]{img/cifar10sratio.pdf}
    \caption{CIFAR-10 Selection Ratio}
    \label{fig:subb}
  \end{subfigure}
\begin{subfigure}{0.33\linewidth}
    \includegraphics[width=2.2in,height=1.9in]{img/cifar10sacc99-100.pdf}
    \caption{CIFAR-10 Selection Accuracy}
    \label{fig:subc}
  \end{subfigure}\\
\vspace{-0.1cm}
    \begin{subfigure}{0.33\linewidth}
    % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=2.2in,height=1.9in]{img/cifar100acc60-85.pdf}
    \caption{CIFAR-100 Accuracy}
    \label{fig:subd}
  \end{subfigure}
  % \hfill
  \begin{subfigure}{0.33\linewidth}
    \includegraphics[width=2.2in,height=1.9in]{img/cifar100sratio.pdf}
    \caption{CIFAR-100 Selection Ratio}
    \label{fig:sube}
  \end{subfigure}
\begin{subfigure}{0.33\linewidth}
    \includegraphics[width=2.2in,height=1.9in]{img/cifar100sacc.pdf}
    \caption{CIFAR-100 Selection Accuracy}
    \label{fig:subf}
  \end{subfigure}
  \caption{Parameter analysis of $\lambda_{\mathrm{cr}}$ on CIFAR-10 and CIFAR-100.}
  \label{fig:mainsub}
\vspace{-0.15cm}
\end{figure*}


\noindent\textbf{Parameter test on $\lambda_{\mathrm{cr}}$.\quad}In this experiment, we test the parameter $\lambda_{\mathrm{d}}$, which weights the contribution of the consistency regularization term to the training loss. As mentioned in  Eq. (14), the parameter $\lambda_{\mathrm{d}}$ is directly influenced by the hyperparameter $\lambda_{\mathrm{cr}}$. Therefore, we test $\lambda_{\mathrm{cr}}=\{1,2,4\}$ on CIFAR-10 ($q=0.5$) and CIFAR-100 ($q=0.1$). At the same time, we also try to fix the parameter $\lambda_{\mathrm{d}}$, that is, its value is not related to the selection ratio $r_{\mathrm{s}}$. This setting we denote by $\lambda_{\mathrm{cr}}\mathrm{(fix)}$, we test $\lambda_{\mathrm{cr}}\mathrm{(fix)}=\{0.5,1,2\}$. The results are visualized in Figure \ref{fig:mainsub}, and detailed data can be found in Supplementary Material.

As mentioned in Table \ref{main scc+sration table} above, CroSel achieves a very high selection ratio. When using a dynamically changing $\lambda_{\mathrm{d}}$, the contribution made by the regularization term would be quite small in the later stages as the learning rate decays. In contrast, with a fixed value of $\lambda_{\mathrm{d}}$, the regularization term would still have a significant contribution in the later stages. Although the accuracy rate on the test set does not show a significant difference between different parameters, the selection effect is critical. A too large contribution of the regularization item will slightly improve the selection ratio at the cost of a decrease in the selection accuracy, which goes against the original intention of our algorithm design. Therefore, we finally decided to use dynamically changing parameters. In other words, we want the algorithm to return to a supervised learning setting with minimal noise as much as possible at the end of the training process.

\noindent\textbf{The influence on double model.\quad}As recognized by the community, dual models tend to achieve better performance than single models. We are curious about how effective our selection criteria would be without the adaptive error correction capability of cross selection.  Figure \ref{fig:dual} visualizes the gap in the selection ratio between dual-model and single-model training. It shows that our cross selection strategy can select samples more comprehensively, and on average, about 10\% more training examples can be selected on each dataset. However, the effectiveness of our algorithm is not solely due to the dual model. Even when using a single model with our selection criteria, the accuracy rate on the test set only decreases by 0.83\% and 2.68\% on CIFAR-10 and CIFAR-100 respectively. Furthermore, the high precision of selection is reflected in both settings. Detailed results can be found in Supplementary Material.

% origin format
% \begin{figure}[!t]
% \centering
% \subfigure[CIFAR-10 comparison]{
% 	\label{fig:subfig:cifar10} %% label for second subfigure
% 	\includegraphics[width=1.5in,height=1.6in]{img/cifar10_comparation_sratio.pdf}}
% \subfigure[CIFAR-100 comparison]{
% 	\label{fig:subfig:cifar100} %% label for second subfigure
% 	\includegraphics[width=1.5in,height=1.6in]{img/cifar100_comparation_sratio.pdf}}
% \caption{Selection ratio comparison between dual model and single model on CIFAR-10 and CIFAR-100.}
% \label{fig:Selection ratio comparison} %% label for entire figure
% \end{figure}

% \begin{figure}
% % \centering
%     \begin{subfigure}[CIFAR-10 comparison]{
% 	\label{fig:comparison:cifar10} %% label for second subfigure
% 	\includegraphics[width=1.5in,height=1.6in]{img/cifar10_comparation_sratio.pdf}}
%     \begin{subfigure}[CIFAR-100 comparison]{
% 	\label{fig:comparison:cifar100} %% label for second subfigure
% 	\includegraphics[width=1.5in,height=1.6in]{img/cifar100_comparation_sratio.pdf}}
% \caption{Selection ratio comparison between dual model and single model on CIFAR-10 and CIFAR-100.}
% \label{fig:Selection ratio comparison} %% label for entire figure
% \end{figure}

% cvpr format
\begin{figure}
% \centering
  \begin{subfigure}{0.45\linewidth}
    % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=1.5in,height=1.6in]{img/cifar10_comparation_sratio.pdf}
    \caption{CIFAR-10 comparison.}
    \label{fig:compare-a}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1.5in,height=1.6in]{img/cifar100_comparation_sratio.pdf}
    \caption{CIFAR-100 comparison.}
    \label{fig:compare-b}
  \end{subfigure}
  \vspace{-0.1cm}
  \caption{Selection ratio comparison between dual model and single model on CIAFR-10 and CIAFR-100.}
  \label{fig:dual}
\vspace{-0.25cm}
\end{figure}


% \begin{table}[!htpb]
% \centering
% \caption{Comparison between dual model and single model.}
% % \vspace{-0.5cm}
% \label{ablation dual}
% \resizebox{0.475\textwidth}{!}{
% \setlength{\tabcolsep}{3mm}{
% \begin{tabular}{c|c|cc}
% \toprule
% Setting & Model & Accuracy & S-acc\\
% \midrule
% \multicolumn{1}{c|}{\multirow{2}{*}{\makecell[c]{CIFAR-10 \\ $q=0.5$}}}    &Single model& 96.51\%  & 99.72\%  \\
% \multicolumn{1}{c|}{} & Dual model& 97.34\% & 99.44\%  \\
% \hline
% \multicolumn{1}{c|}{\multirow{2}{*}{\makecell[c]{CIFAR-100 \\ $q=0.1$}}}    & Single model&81.39\%  & 98.35\%  \\
% \multicolumn{1}{c|}{} & Dual model& 84.07\%& 97.93\%  \\
% \bottomrule
% \end{tabular}
% % \end{center}
% }
% }
% \vspace{-0.5cm}
% \end{table}