\section{Our Proposed Method}\label{section:method}

In this section, we provide a detailed explanation of how our algorithm works. Our method is composed of two main components: a cross selection strategy that utilizes two models to select confident pseudo labels for each other, and a consistency regularization term that is applied across different data augmentation versions. The latter part not only addresses the issue of label waste resulting from the selection process but also enhances the quantity and accuracy of selections. The pseudo-code for our algorithm is presented in Algorithm \ref{alg:crosel}.

\subsection{Problem Setting}
Suppose the feature space is $\mathcal{X}\in\mathbb{R}^d$ with $d$ dimensions and the label space is $\mathcal{Y}=\{1,2,\dots,k\}$ with $k$ classes. We are given a dataset $\mathcal{D} =\{(\boldsymbol{x}_i,S_i)\}_{i=1}^n$ with n examples,
where the instance $\boldsymbol{x}_i\in\mathcal{X}$ and corresponding candidate label set $S_i\subset\mathcal{Y}$. Same as previous studies, we assume that the true label $y_i\in\mathcal{Y}$ of each input $\boldsymbol{x}_i$ is concealed in $S_i$.

Our aim is to train a multi-class classifier $f:\mathbb{R}^d\rightarrow\mathbb Y$ that minimizes the classification risk on the given dataset. For our classifier $f$, we use $f(\boldsymbol{x})$ to represent the output of classifier $f$ on given input $\boldsymbol{x}$. And we use $\hat{y}=\mathrm{argmax}_{y\in\mathcal{Y}}f_y(\boldsymbol{x})$ to denote the prediction of our classifier, where $f_y(\boldsymbol{x})$ is the $y\text{-}$th coordinate of $f(\boldsymbol{x})$.

\begin{algorithm*}[tb]
    \centering
   \caption{Pseudo-code of CroSel}
   \label{alg:crosel}
\begin{algorithmic}
   \STATE {\bfseries Input:} Training dataset $\mathcal{D}=\{(\boldsymbol{x}_i,S_i)\}_{i=1}^n$, consistency regularization parameter $\lambda_{\mathrm{cr}}$, sharpen parameter $T$, confidence threshold $\gamma$, memory bank $\mathrm{MB}^{(1)}$, $\mathrm{MB}^{(2)}$, network $\Theta^{(1)},\Theta^{(2)}$, epoch $E$, iteration $I$.
   \STATE {\bfseries Procedure:}
   \STATE $\mathrm{MB}^{(1)},\mathrm{MB}^{(2)},\Theta^{(1)},\Theta^{(2)}=\mathrm{WarmUp}(\mathrm{MB}^{(1)},\mathrm{MB}^{(2)},\Theta^{(1)},\Theta^{(2)},\mathcal{D})$. \qquad // \quad CC algorithm
   \FOR{$e=1$ {\bfseries to} $E$}
   \STATE Select labeled dataset $\mathcal{D}_{\mathrm{sel}}^1$ through $\mathrm{MB}^{(1)}$.     
   \STATE Select labeled dataset $\mathcal{D}_{\mathrm{sel}}^2$ through $\mathrm{MB}^{(2)}$.\qquad // \quad Eq. (4)
    \FOR{$k=1,2$ }
    % \qquad // Repeat training process on two models
    \FOR{$i=1$ {\bfseries to} $I$}
    \STATE   Fetch a labeled batch $\hat{B}_i$ from the opposite selected dataset $\mathcal{D}_{\mathrm{sel}}^{\thicksim k}$.
    \STATE   Fetch a  batch $B_i$ from the training dataset $\mathcal{D}$.
    \STATE   Calculate the loss $L$ among the two batches $\hat{B}_i$ and $B_i$ through Eq. (13).
    \STATE   Update the weight of $\Theta^{(k)}$ by optimizer.
    \ENDFOR    
    \STATE  Update the memory bank $\mathrm{MB}^{(k)}$ through the FIFO principle.
    \ENDFOR
   \ENDFOR
\end{algorithmic}
\end{algorithm*}

\subsection{Selection Strategy}
In the current partial-label learning task, the large number of candidate labels can confuse the classifier, making it difficult for the classifier to capture specific features belonging to a certain label. Therefore, our goal is to identify the most likely true label among the candidate labels and eliminate the interference of other negative labels during model training. By selecting these ``true" labels, we can train the data with a supervised learning approach.

\noindent\textbf{Warm up.\quad}Before selecting, we warm up the network using the entire training set. The goal of this stage is to reduce the classification risk of the input $x$ to the whole candidate label set $S$, and obtain some historical information that can be used for selection. Therefore, here we use CC algorithm~\cite{feng2020provably} to warm up models for 10 epochs.
At the same time, we will update the value of memory bank $\mathrm{MB}$.
% \begin{equation}
% \mathcal{L}_{cc} =  -\frac{1}{n} { \sum_{i=1}^{n}} \log ({\textstyle \sum_{y_i\in S_i }\frac{\exp(f_y(\boldsymbol{x}))}{\sum_j \exp(f_j(\boldsymbol{x}))} } )
% \end{equation}

\noindent\textbf{Selecting criteria.\quad}We have three criteria for selecting the confident pseudo labels from the candidate label set.
Depending on the setup of our problem, the true label of each example must be in its candidate label set. This is our first criterion. In addition, we believe that an example's predicted label is likely to be the true label if the model predicts it with high confidence and low fluctuation. To determine the latter, we maintain a memory bank $\mathrm{MB}$ to store historical prediction of this neural network.

The size of the memory bank is $t\times n \times k$, where $t$ denotes the length of time it stores, $n$ is the length of dataset, and $k$ is the number of categories in the classification. In other words, $\mathrm{MB}$ stores the output after softmax of the model in the last $t$ epochs. 
$\mathrm{MB}$ is structured as a queue, with each element being a $k$-dimensional vector $\boldsymbol{q}$ representing the output of an example in a particular epoch. We update $\mathrm{MB}$ with the FIFO principle.
% We let $\mathrm{MB}$ render a queue structure and update it with the principles of FIFO. We use $q$ as a $k$-dimensional vector to denote a output of a sample stored in the $\mathrm{MB}$, $\boldsymbol{q}^i$ means the $i-th$ epoch's output in the $\mathrm{MB}$. 
These selection criteria can be summarized as follows:
\begin{align}
\beta_1 &= \mathbb{I}(\mathrm{argmax}(\boldsymbol{q}^i) \in S),\\
\beta_2 &= \mathbb{I}(\mathrm{argmax}(\boldsymbol{q}^i) = \mathrm{argmax}(\boldsymbol{q}^{i+1})),\\
\beta_3 &= \mathbb{I}(\frac{1}{t} \sum\nolimits_{i=1}^{t} \mathrm{max}(\boldsymbol{q}^i) > \gamma),
\end{align}
where $\gamma$ is the confidence threshold of selection, $\mathbb{I}(\cdot)$ is the indicator function.
$\beta_1$ lets our selected label be in the candidate label set. $\beta_2$ limits that the label we picked has not flipped in the past $t$ epochs, which is a volatility consideration, and $\beta_3$ ensures that the label we selected has a high confidence level. The final selected dataset is $\mathcal{D}_{\mathrm{sel}}$:
\begin{equation}
\mathcal{D}_{\mathrm{sel}} = {((\boldsymbol{x}_i,\mathrm{argmax}(\boldsymbol{q}^t_i)) | (\beta_1^i \wedge \beta_2^i \wedge \beta_3^i) =1, \boldsymbol{x}_i \in \mathcal{D})},
\end{equation}

\noindent\textbf{Selected label loss.\quad}
After selecting the high-confidence examples, we obtain a dynamically updated data subset $\mathcal{D}_{\mathrm{sel}}=(X,\hat{Y})$. Each tuple of the subset has an instance $\boldsymbol{x}$ and a selected label $\hat{y}=\mathrm{argmax}(\boldsymbol{q}^t)$. In this configuration, we can use the basic cross-entropy loss to deal with this part of the samples:
\begin{equation}
\mathcal{L}_{\mathrm{l}} =\frac{1}{|\mathcal{D}_{\mathrm{sel}}|} { \sum\nolimits_{\boldsymbol{x} \in \mathcal{D}_{\mathrm{sel}}}} \mathcal{L}_{\mathrm{CE}}(f(\boldsymbol{x}_{\mathrm{w}}),\hat{y}),
\end{equation}
where $\mathcal{L}_{\mathrm{CE}}(\cdot,\cdot)$ denotes the softmax cross entropy loss, $\boldsymbol{x}_{\mathrm{w}}$ denotes the weak augmented version of $\boldsymbol{x}$.

\noindent\textbf{Cross selection.\quad}
However, In the process of selecting labels, it is difficult to guarantee that the selected labels are 100\% accurate.
In order to maximize the accuracy of selection, we propose a cross selection framework based on the idea of ensemble learning. Specifically, we train two identical models $\Theta^{(1)}$ and $\Theta^{(2)}$ through the same training and label selection process. By forming different decision boundaries, the two models can adaptively correct most of the errors even if there is noise in the selected confident pseudo labels. 

To maximize the accuracy of the selected labels, we employ a cross-supervised training process. This involves using the selected dataset $\mathcal{D}_{\mathrm{sel}}^1$ from $\mathrm{MB}^{(1)}$ to train $\Theta^{(2)}$, and vice versa, using the selected dataset $\mathcal{D}_{\mathrm{sel}}^2$ from $\mathrm{MB}^{(2)}$ to train $\Theta^{(1)}$. By doing so, the two models can learn from each other and further improve their ability to select true labels.
% We use a cross-supervised training process for the selected labels, the selected dataset of $\Theta_1$ supervised training is selected by $\Theta_2$, and the selected dataset of $\Theta_2$ supervised training is selected by $\Theta_1$.
In the test process, we will average the output of the two models to get the final prediction:
\begin{equation}
f^{\prime}(\boldsymbol{x}) =\frac{1}{2} (f^1(\boldsymbol{x})+f^2(\boldsymbol{x})),
\end{equation}
where $f^{\prime}(\boldsymbol{x})$ denotes the final output of our method in the test process, $f^{1}(\boldsymbol{x})$ denotes the output of $\Theta^{(1)}$, $f^{2}(\boldsymbol{x})$ denotes the output of $\Theta^{(2)}$.

\subsection{Co-mix Consistency Regulation}
\noindent\textbf{Motivation.\quad} When dealing with complex partial-label learning tasks, our label selection strategy may not accurately select the true labels for all examples. If we only use the selected examples with their corresponding labels, it will result in a significant amount of wasted data, which contradicts our goal of utilizing as many examples as possible. Therefore, we aim to provide a trainable target for the remaining examples that are not selected. 
However, our setting differs from traditional semi-supervised learning as the proportion of unlabeled examples is relatively small. So it is unreasonable to directly transfer the existing semi-supervised learning tools to unlabeled data like other weakly supervised learning methods. 

Motivated by this, we hope to propose a regularization term that can serve as an important supplement to our method and help us select examples.
We proposed the co-mix regularization term, which employs two widely used data augmentation methods: weak augmentation and strong augmentation, to generate pseudo-labels as training targets for consistency regularization. We further employ MixUp~\cite{zhang2017mixup} to further enhance the data. It is worth mentioning that the term ``pseudo labels" in this part specifically refers to the soft labels generated from different data augmentation versions, rather than the confident hard labels selected during the selection process.

% We assign a pseudo target $\boldsymbol{p}_i$ for each augmented image $\boldsymbol{x}{aug1}$, which is generated by another augment transformation of the image $\boldsymbol{x}{aug2}$. With the help of these pseudo-labels, we can process samples like normal supervised learning. This can also be seen as another form of consistent regularization loss. 

\noindent\textbf{Pseudo label generating.\quad}Consistency loss is a simple but effective idea in weakly supervised learning, whose key point is to reduce the gap between the output of two perturbed examples after passing through the model. As discussed in the previous section, we used two widely used data augmentations: `weak' and `strong', and crossed them to generate pseudo labels. Specifically, as stated in Figure \ref{fig:overview}, the pseudo label corresponding to weak augmented example is generated by strong augmented example, while the pseudo label corresponding to strong augmented example is generated by weak augmented example.

To generate these pseudo labels, we fix the parameters of the neural network, and pass the augmented images through the model to get the logits output. Then we perform two operations on the logits, sharpening and normalization. For sharpening operation, we use a hyper-parameter $T$, the more $T$ goes to zero, the more logits tend to become a one-hot distribution. These two operations can be summarized by the following formula.
\begin{equation}
\boldsymbol{p}_{i}=
\begin{cases}
\frac{\exp(f_i(\boldsymbol{x})^{\frac{1}{T} })}{ {\textstyle \sum_{i\in S} \exp(f_i(\boldsymbol{x})^{\frac{1}{T} })} },& \text{$i \in S$},\\
0,& \text{$i \notin S$},
\end{cases}
\end{equation}
where $\boldsymbol{p}_{i}$ denotes the $i\text{-}\text{th}$ coordinate of pseudo label.

\noindent\textbf{MixUp.\quad} After generating the pseudo labels of each example, we end up with two datasets that can be trained: $(X_{\mathrm{w}},P_{\mathrm{s}})$ and $(X_{\mathrm{s}},P_{\mathrm{w}})$. The subscripts \emph{w} and \emph{s} indicate the type of data augmentation used, i.e., weak or strong. Then, We spliced the two datasets together to further enhance the data with MixUp~\cite{zhang2017mixup}. For a pair of two examples with their corresponding pseudo labels $(\boldsymbol{x}_1, \boldsymbol{p}_1),(\boldsymbol{x}_2, \boldsymbol{p}_2)$, we compute $(\boldsymbol{x}^{\prime}, \boldsymbol{p}^{\prime})$ by the following formula: 
\begin{align}
\lambda & \sim{\mathrm{Beta}(\alpha,\alpha)},
\\
\lambda^{\prime} &= \mathrm{max}(\lambda,1-\lambda),
\\
\boldsymbol{x}^{\prime}  &=\lambda^{\prime}\boldsymbol{x}_1+(1-\lambda^{\prime})\boldsymbol{x}_2,
\\
\boldsymbol{p}^{\prime}  &=\lambda^{\prime}\boldsymbol{p}_1+(1-\lambda^{\prime})\boldsymbol{p}_2,
\end{align}
After MixUp, we finally obtain $2n$ trainable example pair $(\boldsymbol{x}^{\prime}, \boldsymbol{p}^{\prime})$. Then we can use the typical cross-entropy loss on every example, the consistency regulation loss will be:
\begin{equation}
\mathcal{L}_{\mathrm{cr}} = \frac{1}{2n} { \sum\nolimits_{i=1}^{2n}} \mathcal{L}_{\mathrm{CE}}(f(\boldsymbol{x}^{\prime}_i),{\boldsymbol{p}^{\prime}_i}),
\end{equation}
where $\mathcal{L}_{\mathrm{CE}}(\cdot,\cdot)$ denotes the softmax cross entropy loss, $n$ denotes the length of a dataset.

\begin{table*}[t]
% \small
\centering
\caption{Accuracy (mean$\pm$std) comparisons on benchmark datasets.}
% \vspace{-0.5cm}
\label{Main reults}
% \scalebox{1.0}{
\resizebox{1.00\textwidth}{!}{
\setlength{\tabcolsep}{3mm}{
\begin{tabular}{l|c|ccccccc}
\toprule
Dataset & $q$ & Ours & PoP &CRDPLL & PiCO & PRODEN & LWS & CC  \\
\midrule
\multicolumn{1}{c|}{\multirow{3}{*}{CIFAR-10}}    & $0.1$& 97.31$\pm$.04\% & 97.17$\pm$.01\% &\textbf{97.41$\pm$.06}\% & 96.10$\pm$.06\% &95.66$\pm$.08\% &91.20$\pm$.07\% &  90.73$\pm$.10\% \\
\multicolumn{1}{c|}{} & $0.3$& \textbf{97.50$\pm$.05}\% & 97.08$\pm$.01\% &97.38$\pm$.04\%  & 95.74$\pm$.10\% & 95.21$\pm$.07\% & 89.20$\pm$.09\% & 88.04$\pm$.06\% \\
\multicolumn{1}{c|}{}    & $0.5$& \textbf{97.34$\pm$.05}\% &96.66$\pm$.03\% &96.76$\pm$.05\% & 95.32$\pm$.12\% & 94.55$\pm$.13\% & 80.23$\pm$.21\% & 81.01$\pm$.38\% \\
\midrule
\multicolumn{1}{c|}{\multirow{3}{*}{SVHN}}    & $0.1$& \textbf{97.71$\pm$.05}\% & 97.55$\pm$.06\% &97.63$\pm$.06\% & 96.58$\pm$.04\% &96.20$\pm$.07\% &96.42$\pm$.09\% &  96.99$\pm$.17\% \\
\multicolumn{1}{c|}{} & $0.3$& \textbf{97.96$\pm$.05}\% & 97.50$\pm$.03\% &97.65$\pm$.07\%  & 96.32$\pm$.09\% & 96.11$\pm$.05\% & 96.15$\pm$.08\% & 96.67$\pm$.20\% \\
\multicolumn{1}{c|}{}    & $0.5$& \textbf{97.86$\pm$.06}\% & 97.31$\pm$.01\% &97.70$\pm$.05\% & 95.78$\pm$.05\% & 95.97$\pm$.03\% & 95.79$\pm$.05\%  & 95.83$\pm$.23\% \\
\midrule
\multicolumn{1}{c|}{\multirow{3}{*}{CIFAR-100}}    & $0.01$& \textbf{84.24$\pm$.09}\% & 83.03$\pm$.04\%&82.95$\pm$.10\% & 74.89$\pm$.11\% &72.24$\pm$.12\% &62.03$\pm$.21\% &  66.91$\pm$.24\% \\
\multicolumn{1}{c|}{} & $0.05$& \textbf{83.92$\pm$.24}\% & 82.79$\pm$.02\%&82.38$\pm$.09\%  & 73.26$\pm$.09\% & 70.03$\pm$.18\% & 57.10$\pm$.17\% & 64.51$\pm$.37\% \\
\multicolumn{1}{c|}{}    & $0.10$& \textbf{84.07$\pm$.16}\% & 82.39$\pm$.04\%&82.15$\pm$.20\% & 70.03$\pm$.10\% & 69.82$\pm$.11\% & 52.60$\pm$.54\%  & 61.50$\pm$.36\%\\
\bottomrule
\end{tabular}
}
}
% \vspace{-0.5cm}
\end{table*}


\subsection{Algorithm overview}

\noindent\textbf{Overall loss.\quad} In the formal training phase, our loss function will be composed of two parts, the supervised loss $\mathcal{L}_{\mathrm{l}}$ in the selected label set $\mathcal{D}_{\mathrm{sel}} $ and the consistency regularization item loss $\mathcal{L}_{\mathrm{cr}}$. The two will be dynamically combined into the final loss function by a hyperparameter:
\begin{equation}
\mathcal{L}_{\mathrm{all}} =  \mathcal{L}_{\mathrm{l}} + \lambda_{\mathrm{d}}*\mathcal{L}_{\mathrm{cr}},
\end{equation}
where $\lambda_{\mathrm{d}}$ is a dynamically changing parameter, $\mathcal{L}_{\mathrm{l}}$ and $\mathcal{L}_{\mathrm{cr}}$ can be calculated by Eq. (5) and Eq. (12), respectively. The use of MixUp can cause significant changes to the original feature space, making it necessary to adjust the weight of the regularization term in the loss function as the number of selected samples increases. 
To achieve this, we proposed a gradually decreasing $\lambda_{\mathrm{d}}$ with the increase of selected samples. We can control the magnitude of $\lambda_{\mathrm{d}}$ with a set hyperparameter $\lambda_{\mathrm{cr}}$. The updated rules of $\lambda_{\mathrm{d}}$ are as follows:
\begin{equation}
\lambda_{\mathrm{d}} = (1- r_{\mathrm{s}})* \lambda_{\mathrm{cr}},
\end{equation}
where $r_{\mathrm{s}}$ denotes the percentage of labeled data that we picked out, $\lambda_{\mathrm{cr}}$ is a hyperparameter that collaboratively adjusts the ratio of two loss items.