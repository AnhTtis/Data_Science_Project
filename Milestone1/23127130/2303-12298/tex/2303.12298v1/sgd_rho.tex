\section{Stochastic Gradient Descent for General Measurements}\label{sec:sgd_general}

In this section, we further extend the general measurement  where $\{u_i\}_{i\in [m]}$ are non-orthogonal vectors and $|u_i^{\top}  u_j| \leq \rho$ to the convergence analysis of the stochastic gradient descent matrix sensing algorithm. Algorithm~\ref{alg:stochastic_gradient_descent_general} implements the stochastic gradient descent version of the matrix sensing algorithm.

In Algorithm~\ref{alg:stochastic_gradient_descent_general}, at each iteration $t$, we first compute the stochastic gradient descent by:
\begin{align*}
    \nabla \Phi_{\lambda} (A_t, {\cal B}_t) \gets \frac{m}{B} \sum_{i \in {\cal B}_t} u_i u_i^\top \lambda \sinh( \lambda z_{i} )
\end{align*}
then we update the matrix with the gradient:
\begin{align*}
    A_{t+1} \gets A_t - \epsilon \cdot \nabla \Phi_{\lambda}(A_t,{\cal B} _t) / \| \nabla \Phi_{\lambda}(A_t) \|_F
\end{align*}
At the end of each iteration, we update $z_i$ by:
\begin{align*}
    z_i\gets z_i - \epsilon  \lambda m w_{i,j}^2\sinh(\lambda z_{j}) / (\| \nabla \Phi_{\lambda}(A_t) \|_F B \;\; \forall i \in [m], j \in {\cal B}_t
\end{align*}
We are interested in studying the time complexity and convergence analysis under the general measurement assumption.

\begin{lemma}[Cost-per-iteration of stochastic gradient descent for general measurements]\label{lem:gd_cost_per_iter_general} Algorithm~\ref{alg:stochastic_gradient_descent_general} takes $O(mn^2)$-time for preprocessing and 
each iteration takes
$
    O(Bn^2+m^2)
$-time.
\end{lemma}
\begin{proof}
Since $u_i$'s are no longer orthogonal, we need to compute $\|\nabla \Phi_\lambda (A_t)\|_F$ in the following way:
\begin{align*}
    &\|\nabla \Phi_\lambda (A_t)\|_F^2\\
    =&~ \tr\Big[\Big(\sum_{i=1}^m u_i u_i^\top \lambda \sinh( \lambda (\lambda z_{t,i}))\Big)^2\Big]\\
    = &~ \lambda^2\sum_{i,j=1}^m \langle u_i, u_j\rangle^2 \sinh( \lambda (\lambda z_{t,i}))\sinh( \lambda (\lambda z_{t,j}))\\
    = &~ \lambda^2\sum_{i,j=1}^m w_{i,j}^2 \sinh( \lambda (\lambda z_{t,i}))\sinh( \lambda (\lambda z_{t,j})).
\end{align*}
Hence, with $\{z_{t,i}\}_{i\in [m]}$, we can compute $\|\nabla \Phi_\lambda (A_t)\|_F$ in $O(m^2)$-time.

Another difference from the orthogonal measurement case is the update for $z_{t+1,i}$. Now, we have
\begin{align*}
        &z_{t+1,i}-z_{t,i}\\
    = &~ u_i^\top (A_{t+1}-A_t)u_i\\
    = &~  -\frac{\epsilon}{\| \nabla \Phi_{\lambda}(A_t) \|_F}\cdot u_i^\top \nabla \Phi_{\lambda}(A_t,{\cal B} _t)u_i\\
    = &~ -\frac{\epsilon \lambda m }{\| \nabla \Phi_{\lambda}(A_t) \|_F B} \sum_{j\in {\cal B}_t}u_i^\top u_j u_j^\top u_i \cdot  \sinh(\lambda z_{t,j})\\
    = &~ -\frac{\epsilon \lambda m }{\| \nabla \Phi_{\lambda}(A_t) \|_F B} \sum_{j\in {\cal B}_t}w_{i,j}^2 \cdot  \sinh(\lambda z_{t,j}).
\end{align*}
Hence, each $z_{t+1,i}$ can be computed in $O(B)$-time. And it takes $O(mB)$-time to update all $z_{t+1,i}$.

The other steps' time costs  are quite clear from Algorithm~\ref{alg:stochastic_gradient_descent_general}.
\end{proof}


\begin{lemma}[Progress on expected potential with general measurements]\label{lem:sgd_potential_general}
Assume that $|u_i^{\top}  u_j| \leq \rho$ and $\rho \leq \frac{1}{10m}$, for any $i,j \in [m]$ and $\| u_i\|^2 = 1$. Let $c \in (0,1)$ denote a sufficiently small positive constant. Then, for any $\epsilon,\lambda>0$ such that $\epsilon \lambda \leq c \frac{|{\cal B}_t|}{m}$, 
we have for any $t>0$,
\begin{align*}
    \E[\Phi_{\lambda} ( A_{t+1} )] \leq (1-0.9 \frac{ \lambda \epsilon }{\sqrt{m} }) \cdot \Phi_{\lambda} (A_t) +  \lambda \epsilon \sqrt{m}
\end{align*}

\end{lemma}
The proof is a direct generalization of Lemma~\ref{lem:stochastic_gradient_descent} and is very similar to Lemma~\ref{lem:gradient_descent_rho}. Thus, we omit it here.
\begin{algorithm*}[!ht]\caption{Matrix Sensing with Stochastic Gradient Descent (General Measurements).}\label{alg:stochastic_gradient_descent_general}
\begin{algorithmic}[1]
\Procedure{SGD\_General}{$\{u_i,b_i\}_{i\in [m]}$} \Comment{Lemma~\ref{lem:gd_cost_per_iter_general}}
    \State $\tau \gets \max_{i \in [m]} b_i $
    \State $A_1 \gets \tau \cdot I$
    \State $z_i\gets u_i^\top A_1 u_i - b_i$ for $i\in [m]$\Comment{$z\in \R^m$}
    \State $w_{i,j}\gets \langle u_i, u_j\rangle$ for $i,j\in [m]$ \Comment{$w\in \R^{m\times m}$}
    \For{$t = 1 \to T$}
        \State Sample ${\cal B}_t \subset [m]$ of size $B$ uniformly at random
        \State $\nabla \Phi_{\lambda} (A_t, {\cal B}_t) \gets \frac{m}{B} \sum_{i \in {\cal B}_t} u_i u_i^\top \lambda \sinh( \lambda z_{i} ) $\Comment{It takes $O(Bn^2)$-time}
        \State $\| \nabla \Phi_{\lambda} (A_t) \|_F\gets \lambda \left(\sum_{i,j=1}^m  w_{i,j}^2\sinh (\lambda z_{i})\sinh(\lambda z_j)\right)^{1/2}$\Comment{It takes $O(m^2)$-time}
        \State $A_{t+1} \gets A_t - \epsilon \cdot \nabla \Phi_{\lambda}(A_t,{\cal B} _t) / \| \nabla \Phi_{\lambda}(A_t) \|_F$\Comment{It takes $O(n^2)$-time}
        \For{$i\in [m]$}\Comment{Update $z$. It takes $O(mB)$-time}
            \For{$j\in {\cal B}_t$}
                \State $z_i\gets z_i - \epsilon  \lambda m w_{i,j}^2\sinh(\lambda z_{j}) / (\| \nabla \Phi_{\lambda}(A_t) \|_F B)$
            \EndFor
        \EndFor
    \EndFor
    \State \Return $A_{T+1}$
\EndProcedure
\end{algorithmic}
\end{algorithm*}
