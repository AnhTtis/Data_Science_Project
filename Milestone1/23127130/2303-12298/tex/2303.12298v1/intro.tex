\section{Introduction}

 

Matrix sensing is a generalization of the famous compressed sensing problem. Informally, the goal of matrix sensing is to reconstruct a matrix $A\in \R^{n\times n}$ using a small number of quadratic measurements (i.e., $u^\top A u$). It has many real-world applications, including image processing~\citep{clmw11, wsb11}, quantum computing~\citep{a07, fgle12,kkd15}, systems~\citep{lv10} and sensor localization~\citep{jm13} problems. 
For this problem, there are two important \emph{theoretical} questions:
\begin{itemize}
    \item {\bf Q1. Compression: } how to design the sensing vectors $u\in \R^n$ so that the matrix can be recovered with a small number of measurements?
    \item {\bf Q2. Reconstruction: } how fast can we recover the matrix given the measurements?
\end{itemize}
 

\cite{zjd15} initializes the study of the rank-one matrix sensing problem, where the ground-truth matrix $A_{\star}$ has only rank-$k$, and the measurements are of the form $u_i^\top A_{\star} u_i$. They want to know the smallest number of measurements $m$ to recover the matrix $A_{\star}$. In our setting, we assume $m$ is a fixed input parameter and we're not allowed to choose. We show that for any $m$ and $n$, how to design a faster algorithm for solving an optimization problem which is finding $A \approx A_{\star}$. Thus, in some sense, previous work \cite{zjd15,dls23} mainly focuses on problem {\bf Q1} with a low-rank assumption on $A_\star$. Our work is focusing on {\bf Q2} without the low-rank assumption.



We observe that in many applications, the ground-truth matrix $A_{\star}$ does not need to be recovered \emph{exactly} (i.e., $\|A-A_\star\|\leq n^{-c}$). For example, for distance embedding, we would like to learn an embedding matrix between all the data points in a high-dimensional space. The embedding matrix is then used for calculating data points' pairwise distances for a higher-level machine learning algorithm, such as $k$-nearest neighbor clustering. As long as we can recover a good approximation of the embedding matrix, the clustering algorithm can deliver the desired results. As we relax the accuracy constraints of the matrix sensing, we have the opportunity to speed up the matrix sensing time.



We formulate our problem in the following way:

\begin{problem}[Approximate matrix sensing]\label{prob:app_mat_sensing}
Given a ground-truth positive definite matrix $A_\star \in \R^{n \times n}$ and $m$ samples $(u_i,b_i) \in \R^{n} \times \R$ such that $u_i^\top A_\star u_i = b_i$. Let $R=\max_{i\in [m]} |b_i|$. For any accuracy parameter $\delta \in (0,1)$, find a matrix $A\in \R^{n\times n}$ such that
\begin{align}\label{eq:measure_guarantee}
    (u_i^\top A u_i - u_i A_{\star} u_i )^2 \leq \delta, ~~~\forall i \in [m]
\end{align}
or 
\begin{align}\label{eq:spectral_guarantee}
 (1-\delta ) A_\star \preceq A \preceq (1+\delta) A_\star.
\end{align}
\end{problem}
We make a few remarks about Problem~\ref{prob:app_mat_sensing}. First, our formulation doesn't require the matrix $A_{\star}$ to be low-rank as literature \cite{zjd15}. Second, we need the measurement vectors $u_i$ to be ``approximately orthogonal'' (i.e., $|u_i^\top u_j|$ are small), while \cite{zjd15} make much stronger assumptions for exact reconstruction. Third, the \emph{measure approximation} guarantee (Eq.~\eqref{eq:measure_guarantee}) does not imply the \emph{spectral approximation} guarantee (Eq.~\eqref{eq:spectral_guarantee}). We mainly focus on achieving the first guarantee and discuss the second one in the appendix.

This problem is interesting for two reasons. First, speeding up matrix sensing is salient for a wide range of applications, where exact matrix recovery is not required. Second, we would like to understand the fundamental tradeoff between the accuracy constraint $\epsilon$ and the running time. This tradeoff can give us insights on the fundamental computation complexity for matrix sensing.

 

This paper makes the following contributions:
\begin{itemize}
    \item We design a potential function to measure the distance between the approximate solution and the ground-truth matrix. 
    \item Based on the potential function, we show that gradient descent can efficiently find an approximate solution of the matrix sensing problem. We also prove the convergence rate of our algorithm. 
    \item Furthermore, we show that the cost-per-iteration can be improved by using stochastic gradient descent with a provable convergence guarantee, which is proved by generalizing the potential function to a randomized potential function.
\end{itemize}

Technically, our potential function applies a $\cosh$ function to each ``training loss'' (i.e., $u_i^\top Au_i - b_i$), which is inspired by the potential function for linear programming \citep{cls19}. We prove that the potential is decreasing for each iteration of gradient descent, and a small potential implies a good approximation. In this way, we can upper bound the number of iterations needed for the gradient descent algorithm.

To reduce the cost-per-iteration, we follow the idea of stochastic gradient descent and evaluate the gradient of potential function on a subset of measurements. However, we still need to know the full gradient's norm for normalization, which is a function of the training losses. It is too slow to naively compute each training loss. Instead, we use the idea of maintenance \citep{cls19,lsz19,b20,blss20,jswz21,hjstz21,syz21,szz21,hswz22,qszz23} and show that the training loss at the $(t+1)$-th iteration (i.e., $u_i^\top A_{t+1}u_i-b_i$) can be very efficiently obtained from those at the $t$-th iteration (i.e., $u_i^\top A_{t}u_i-b_i$). Therefore, we first preprocess the initial full gradient's norm, and in the following iterations, we can update this quantity based on the previous iteration's result.

We state our main result as follows:
\begin{theorem}[Informal of Theorem~\ref{thm:sgd_orthogonal}]
Given $m$ measurements of matrix sensing problems, there is an algorithm that outputs a $n \times n$ matrix $A$ in $\wt{O}(m^{3/2} n^2 R\delta^{-1} )$ time such that $|u_i^\top A u_i - b_i|\leq \delta$, $\forall i \in [m]$.
\end{theorem}