\section{Stochastic gradient descent}\label{sec:sgd}
In this section, we show that the cost-per-iteration of the approximate matrix sensing algorithm can be improved by using a stochastic gradient descent (SGD). More specifically, SGD can obtain a $\delta$-approximate solution with $O(Bn^2)$, where $0<B<m$ is the size of the mini batch in SGD. Below is the main theorem of this section:

\begin{theorem}[Stochastic gradient descent for orthogonal measurements]\label{thm:sgd_orthogonal}
Suppose $u_1,\dots,u_m\in \R^n$ are orthogonal unit vectors, and suppose $|b_i|\leq R$ for all $i\in [m]$. There exists an algorithm such that for any $\delta \in (0,1)$, performs $\wt{O}(m^{3/2}B^{-1}R\delta^{-1})$ iterations of gradient descent with $O(Bn^2)$-time per iteration and outputs a matrix $A\in \R^{n\times n}$ satisfies:
\begin{align*}
    | u_i^\top A u_i - b_i| \leq \delta~~~\forall i\in [m].
\end{align*}
\end{theorem}

The algorithm and its time complexity are provided in Section~\ref{sec:sgd_alg}. The convergence is proved in Section~\ref{sec:sgd_converge_1} and \ref{sec:sgd_converge_2}. The SGD algorithm for the general measurement without the assumption that the $\{u_i\}_{i\in [m]}$ are orthogonal vectors is deferred to Appendix~\ref{sec:sgd_general}.

\subsection{Algorithm}\label{sec:sgd_alg}
We can use the stochastic gradient descent algorithm (Algorithm~\ref{alg:stochastic_gradient_descent}) for matrix sensing. More specifically, in each iteration, we will uniformly sample a subset ${\cal B}\subset [m]$ of size $B$, and then compute the gradient of the stochastic potential function:
\begin{align}\label{eq:sgd_potential_func}
        \nabla \Phi_{\lambda}(A{ , {\cal B}}) := {  \frac{m}{|{\cal B}|}\sum_{i \in {\cal B}}} u_i u_i^\top \lambda \sinh( \lambda (u_i^\top A u_i - b_i) ), 
\end{align}
which is an $n$-by-$n$ matrix. Then, we do the following gradient step:
\begin{align}
    A_{t+1} \gets A_t - \epsilon \cdot \nabla \Phi_{\lambda}(A_t,{\cal B}_t) / \| \nabla \Phi_{\lambda}(A_t) \|_F.
\end{align}


\begin{lemma}[Running time of stochastic gradient descent]\label{lem:sgd_cost_per_iter}
Algorithm~\ref{alg:stochastic_gradient_descent} takes $O(mn^2)$-time for preprocessing and 
each iteration takes
$
    O(Bn^2)
$-time.
\end{lemma}

\begin{proof}
The time-consuming step is to compute $\|\nabla \Phi_\lambda(A_t)\|_F$. Since
\begin{align*}
    \nabla \Phi_{\lambda}(A_t) = \sum_{i=1}^m u_i u_i^\top \lambda \sinh\left( \lambda (u_i^\top A_t u_i - b_i)\right),
\end{align*}
and $u_i\bot u_j$ for $i\ne j\in [m]$, we know that $u_i$ is an eigenvector of $\nabla \Phi_{\lambda}(A)$ with eigenvalue $\lambda \sinh\left( \lambda (u_i^\top A_t u_i - b_i)\right)$ for each $i\in [m]$. Thus, we have
\begin{align*}
    \|\nabla \Phi_\lambda(A_t)\|_F^2 = &~ \sum_{i=1}^m \lambda^2 \sinh^2\left( \lambda (u_i^\top A_t u_i - b_i)\right)\\
    = &~ \sum_{i=1}^m \lambda^2 \sinh^2 (\lambda z_{t,i}),
\end{align*}
where $z_{t,i}:=u_i^\top A_t u_i - b_i$ for $i\in [m]$. Then, if we know ${z_{t,i}}_{i\in [m]}$, we can compute $\|\nabla \Phi_\lambda(A_t)\|_F$ in $O(m)$-time.

Consider the change $z_{t+1,i}-z_{t,i}$:
\begin{align*}
    &z_{t+1,i}-z_{t,i}\\
    = &~ u_i^\top (A_{t+1}-A_t)u_i\\
    = &~  -\frac{\epsilon}{\| \nabla \Phi_{\lambda}(A_t) \|_F}\cdot u_i^\top \nabla \Phi_{\lambda}(A_t,{\cal B} _t)u_i\\
    = &~ -\frac{\epsilon \lambda m }{\| \nabla \Phi_{\lambda}(A_t) \|_F B} \sum_{j\in {\cal B}_t}u_i^\top u_j u_j^\top u_i \cdot  \sinh(\lambda z_{t,j})\\
    = &~ -\frac{\epsilon \lambda m  \sinh(\lambda z_{t,i})}{\| \nabla \Phi_{\lambda}(A_t) \|_F B} \cdot {\bf 1}_{i\in {\cal B}_t},
\end{align*}
where the last step follows from $u_i\bot u_j$ for $i\ne j$.
Hence, if we have already computed $\{z_{t,i}\}_{i\in [m]}$ and $\|\nabla \Phi_\lambda(A_t)\|_F$, $\{z_{t+1,i}\}_{i\in [m]}$ can be obtained in $O(B)$-time.

Therefore, we preprocess $z_{1,i}=u_i^\top A_1u_i-b_i$ for all $i\in [m]$ in $O(mn^2)$-time. Then, in the $t$-th iteration ($t>0$), we first compute 
\begin{align*}
    \nabla \Phi_{\lambda} (A_t, {\cal B}_t) = \frac{m}{B}\sum_{i\in {\cal B}_t}u_iu_i^\top \lambda \sinh(\lambda z_{t,i})
\end{align*}
in $O(Bn^2)$-time. Next, we compute $\|\nabla \Phi_\lambda(A_t)\|_F$ using $z_{t,i}$ in $O(m)$-time. $A_{t+1}$ can be obtained in $O(n^2)$-time. Finally, we use $O(B)$-time to update $\{z_{t+1,i}\}_{i\in [m]}$.

Hence, the total running time per iteration is
\begin{align*}
    O(Bn^2 + m + n^2 + B) = O(Bn^2).
\end{align*}
\end{proof}

\begin{algorithm}[t]\caption{Matrix Sensing by Stochastic Gradient Descent.}\label{alg:stochastic_gradient_descent}
\begin{algorithmic}[1]
\Procedure{SGD}{$\{u_i,b_i\}_{i\in [m]}$} \Comment{Theorem~\ref{thm:sgd_orthogonal}}
    \State $\tau \gets \max_{i \in [m]} b_i $
    \State $A_1 \gets \tau \cdot I$
    \State $z_i\gets u_i^\top A_1 u_i - b_i$ for $i\in [m]$
    \For{$t = 1 \to T$}
        \State Sample ${\cal B}_t \subset [m]$ of size $B$ uniformly at random
        \State $\nabla \Phi_{\lambda} (A_t, {\cal B}_t) \gets \frac{m}{B} \sum_{i \in {\cal B}_t} u_i u_i^\top \lambda \sinh( \lambda z_{i} ) $
        \State $\| \nabla \Phi_{\lambda} (A_t) \|_F\gets \left(\sum_{i=1}^m \lambda^2 \sinh^2 (\lambda z_{i})\right)^{1/2}$
        \State $A_{t+1} \gets A_t - \epsilon \cdot \nabla \Phi_{\lambda}(A_t,{\cal B} _t) / \| \nabla \Phi_{\lambda}(A_t) \|_F$
        \For{$i\in {\cal B}_t$}
            \State \hspace{-2mm} $z_i\gets z_i - \epsilon  \lambda m \sinh(\lambda z_{i}) / (\| \nabla \Phi_{\lambda}(A_t) \|_F B)$
        \EndFor
    \EndFor
    \State \Return $A_{T+1}$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\subsection{Analysis of One Iteration}\label{sec:sgd_converge_1}

Suppose $A \in \R^{n \times n}$. Let ${\cal B}_t$ be a uniformly random $B$-subset of $[m]$ at the $t$-th iteration, where $B$ is a parameter.
 


We can compute the gradient of $\Phi_{\lambda}(A{ , {\cal B}})$ with respect to $A$ as follows:
\begin{align*}
    & ~ \nabla \Phi_{\lambda}(A{ , {\cal B}}) \\
    = & ~ {  \frac{m}{|{\cal B}|}\sum_{i \in {\cal B}}} u_i u_i^\top \lambda \sinh( \lambda (u_i^\top A u_i - b_i) ), 
\end{align*}
where $\nabla \Phi_{\lambda}(A{ , {\cal B}})\in \R^{n\times n}$.


We can also compute the Hessian of $\Phi_{\lambda}(A{ , {\cal B}})$ with respect to $A$ as follows:
\begin{align*}
     & ~\nabla^2 \Phi_{\lambda}(A{ , {\cal B}}) \\
    = & ~ {  \frac{m}{|{\cal B}|}\sum_{i \in {\cal B}}} ( u_i u_i^\top ) \otimes ( u_i u_i^\top ) \lambda^2 \cosh( \lambda (u_i^\top A u_i - b_i) ) 
\end{align*}
where $\nabla^2 \Phi_{\lambda}(A{ , {\cal B}})\in \R^{n^2\times n^2}$ and $\otimes$ is the Kronecker product.


{It is easy to see the expectations of the gradient and Hessian of $\Phi_\lambda(A,{\cal B})$ over a random set ${\cal B}$:
\begin{align*}
    & ~\E_{{\cal B} \sim [m]}[\nabla \Phi_{\lambda}(A, {\cal B})] =    \nabla \Phi_{\lambda}(A), \\
    & ~\E_{{\cal B} \sim [m]}[\nabla^2 \Phi_{\lambda}(A, {\cal B})] =    \nabla^2 \Phi_{\lambda}(A)
\end{align*}
}

 


 
\begin{lemma}[Expected progress on potential]\label{lem:stochastic_gradient_descent}
Given $m$ vectors $u_1, u_2, \cdots, u_m \in \R^n$. 
Assume $\langle u_i , u_j \rangle = 0 $ for any $i \neq j \in [m]$ and $\| u_i\|^2 = 1$, for all $i \in [m]$.
Let $\epsilon \lambda \leq 0.01 \frac{|{\cal B}_t|}{m}$, for all $t>0$.

Then, we have
\begin{align*}
    \E[\Phi_{\lambda} ( A_{t+1} )] \leq (1-0.9 \frac{ \lambda \epsilon }{\sqrt{m} }) \cdot { \Phi_{\lambda} (A_t )} +  \lambda \epsilon \sqrt{m}.
\end{align*}
 
\end{lemma}




\begin{proof}
We first express the expectation as follows:
\begin{align}\label{eq:expectation_phi_update}
    & ~ \E_{A_{t+1}}[\Phi_{\lambda}(A_{t+1})] - \Phi_{\lambda} (A_t)\notag \\
    \leq & ~ \E_{A_{t+1}}[ \langle  \nabla \Phi_{\lambda} (A_t) ,  (A_{t+1} - A_t) \rangle ] \notag \\
    + & ~ O(1) \cdot \E_{A_{t+1}}[ \langle  \nabla^2 \Phi_{\lambda}(A_t) , (A_{t+1} - A_t) \otimes (A_{t+1} - A_t) \rangle ],
\end{align}
which follows from Corollary~\ref{cor:matrix_der_trace}.

 
We choose
\begin{align*}
    A_{t+1} = A_t - \epsilon \cdot \nabla \Phi_{\lambda}(A_t{ , {\cal B}_{t}}) / \| \nabla \Phi_{\lambda}(A_t) \|_F.
\end{align*}
Then, we can bound
\begin{align}\label{eq:sgd_tr_phi_At_first_moment} 
 & ~ \E_{A_{t+1}} [ -\tr [ \nabla \Phi_{\lambda} (A_t) \cdot (A_{t+1} - A_t) ] ] \notag \\
= & ~ \E_{{\cal B}_t}\Big[ \tr\Big[ \nabla \Phi_{\lambda}(A_t) \cdot \frac{\epsilon \nabla \Phi_{\lambda} (A_t,{\cal B}_t) }{ \| \nabla \Phi_{\lambda}(A_t) \|_F } \Big] \Big] \notag \\
= & ~ \epsilon \cdot \| \nabla \Phi_{\lambda} (A_t)  \|_F 
\end{align}

We define for $t>0$ and $i\in [m]$, 
\begin{align*}
    z_{t,i} := u_i^\top A_{t} u_i - b_i.
\end{align*}


We need to compute this $\Delta_2$. For simplificity, we consider $\Delta_2 \cdot \| \nabla \Phi_{\lambda} (A_t) \|_F^2$,
\begin{align}\label{eq:sgd_tr_phi_At_second_moment}
     = & ~  \tr[ \nabla^2 \Phi_{\lambda}(A_t) \cdot (A_{t+1} - A_t) \otimes (A_{t+1} - A_t) ] \cdot  \| \nabla \Phi_{\lambda}(A_t) \|_F^2 \notag\\
    = & ~ (\lambda \epsilon)^2 \cdot (\frac{m}{|{\cal B}_t|})^2  \cdot   \tr\Big[ \nabla^2 \Phi_{\lambda}(A_t) \cdot %\notag \\ 
     ( \sum_{i\in {\cal B}_t} u_i u_i^\top  \sinh( z_{t,i} )    \otimes ( \sum_{i \in {\cal B}_t} u_i u_i^\top \sinh( z_{t,i} ) \Big].
\end{align}
Ignoring the scalar factor in the above equation, we have
\begin{align}
    = &~    \tr\Big[ \nabla^2 \Phi_{\lambda}(A_t) \cdot  ( \sum_{i,j \in B_t}  \sinh( z_{t,i} )\sinh( z_{t,i} ) \cdot   (u_i u_i^\top \otimes u_ju_j^\top) ) \Big] \notag\\
    = &    \tr\Big[ \nabla^2 \Phi_{\lambda}(A_t) \cdot ( \sum_{i \in B_t} \sinh^2( z_{t,i }) (u_i u_i^\top \otimes u_iu_i^\top) ) \Big] \notag\\
    + &    \tr\Big[ \nabla^2 \Phi_{\lambda}(A_t) \cdot ( \sum_{i\neq j \in B_t} \sinh( z_{t,i} )\sinh( z_{t,i} ) \cdot    (u_i u_i^\top \otimes u_j u_j^\top) ) \Big] \notag\\
    =: & ~    \wt{Q}_1 + \wt{Q}_2  ,
\end{align}
where the first step follows that we extract the scalar values from Kronecker product,
the second step comes from splitting into two partitions based on whether $i = j$, the third step comes from the definition of $\wt{Q}_1 $ and $\wt{Q}_2$ where $\wt{Q}_1$ denotes the diagonal term, and $\wt{Q}_2$ denotes the off-diagonal term.
Taking expectation, we have 
\begin{align}\label{eq:expectation_Delta_2}
    & ~ \E[ \Delta_2 \cdot \| \nabla \Phi_{\lambda} (A_t) \|_F^2 ] \notag\\
    = & ~ (\lambda \epsilon)^2 \cdot ( \frac{m}{|{\cal B}_t|} )^2 \E[ \wt{Q}_1] \notag\\
    = & ~ (\lambda \epsilon)^2 \cdot ( \frac{m}{|{\cal B}_t|} )^2 \cdot \frac{|{\cal B}_t|}{m} \cdot Q_1 \notag\\
    \leq & ~ (\lambda \epsilon)^2 \cdot \frac{m}{|{\cal B}_t|} 
      \cdot ( \sqrt{m} + \frac{1}{\lambda} \| \nabla \Phi_{\lambda}(A_t) \|_F ) \cdot \| \nabla \Phi_{\lambda} (A_t) \|_F^2
\end{align}
where the first step comes from extracting the constant terms from the expectation and Claim~\ref{cla:gd_Q2},
the second step follows that $\E[\wt{Q}_1] = \frac{|{\cal B}_t|}{m} \cdot Q_1$,
and the third step comes from the Claim~\ref{cla:gd_Q1}.
Therefore, we have:
\begin{align*}
    & ~ \E[ \Phi_{\lambda} (A_{t+1}) ] - \Phi_{\lambda} (A_t) \\
    \leq &  - \E[ \Delta_1 ]   + O(1) \cdot \E[ \Delta_2 ] \\ 
    \leq &  - \epsilon (1 - O(\epsilon \lambda) \cdot \frac{m}{|{\cal B}_t|} ) \| \nabla \Phi_{\lambda}(A_t) \|_F+ O(\epsilon \lambda)^2 \sqrt{m} \\
    \leq &  - 0.9 \epsilon  \| \nabla \Phi_{\lambda}(A_t) \|_F+ 
    O(\epsilon \lambda)^2 \sqrt{m} \\
    \leq &  -0.9 \epsilon \lambda \frac{1}{\sqrt{m}} ( \Phi_{\lambda}(A_t) - m ) +O(\epsilon \lambda)^2 \sqrt{m} \\
    \leq &   -0.9 \epsilon \lambda \frac{1}{\sqrt{m}} \Phi_{\lambda}(A_t) + \epsilon \lambda \sqrt{m},
\end{align*}
where the first step comes from Eq.~\eqref{eq:expectation_phi_update},
the second step comes from Eq.~\eqref{eq:sgd_tr_phi_At_first_moment} and Eq.~\eqref{eq:expectation_Delta_2},
the third step follows from $\epsilon \leq 0.01 \frac{|{\cal B}_t|}{\lambda m}$,
the forth step follows from Eq.~\eqref{eq:phi_At_F_norm},
and the last step follows from $\epsilon \lambda \in (0, 0.01)$.
\end{proof}

\subsection{Convergence for multiple iterations}\label{sec:sgd_converge_2}
The goal of this section is to prove the convergence of Algorithm~\ref{alg:stochastic_gradient_descent}. 
\begin{lemma}[Convergence of stochastic gradient descent]\label{lem:sgd_convergence}
Suppose the measurement vectors $\{u_i\}_{i\in [m]}$ are orthogonal unit vectors, and suppose $|b_i|$ is bounded by $R$ for $i\in [m]$. Then, for any $\delta \in (0,1)$, if we take $\lambda = \Omega(\delta^{-1}\log m)$ and $\epsilon=O(\lambda^{-1}m^{-1}B)$ in Algorithm~\ref{alg:stochastic_gradient_descent}, then for 
\begin{align*} 
T=\widetilde{\Omega}(m^{3/2}B^{-1}R\delta^{-1})
\end{align*}
iterations, with high probability, the solution matrix $A_T$ satisfies:
\begin{align*}
    | u_i^\top A_{T+1} u_i - b_i| \leq \delta~~~\forall i\in [m].
\end{align*}
\end{lemma}

\begin{proof}
Similar to the proof of Lemma~\ref{lem:gd_convergence}, we can bound the initial potential by:
\begin{align*}
    \Phi(A_1) \leq 2^{O(\lambda R)}.
\end{align*}

In the following iterations, by Lemma~\ref{lem:stochastic_gradient_descent}, we have
\begin{align*}
    \E[\Phi_{\lambda} ( A_{t+1} )] \leq (1-0.9 \frac{ \lambda \epsilon }{\sqrt{m} }) \cdot { \Phi_{\lambda} (A_t )} +  \lambda \epsilon \sqrt{m},
\end{align*}
as long as $\epsilon \leq 0.01\frac{|B_t|}{\lambda m}$, where $B_t$ is a uniformly random subset of $[m]$ of size $B$. 


It suffices to take $\epsilon = O(\lambda^{-1} m^{-1}B)$.

Now, we can apply Lemma~\ref{lem:stochastic_gradient_descent} for $T$ times and get that
\begin{align*}
    \E[\Phi(A_{T+1})]
    \leq 2^{-\Omega( T  \epsilon \lambda / \sqrt{m} ) + O(\lambda R)} + 2  m.
\end{align*}
By taking 
\begin{align*}
T=\widetilde{\Omega}(m^{3/2}B^{-1}R\delta^{-1}),
\end{align*}
we have
\begin{align*}
    \Phi(A_{T+1}) \leq O(m)
\end{align*}
holds with high probability. By the same argument as in the proof of Lemma~\ref{lem:gd_convergence}, we have
\begin{align*}
    | u_i^\top A_{T+1} u_i - b_i| \leq \delta~\forall i\in [m].
\end{align*}
The lemma is thus proved.
\end{proof}