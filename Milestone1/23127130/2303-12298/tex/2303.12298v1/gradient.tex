 
\section{Gradient descent for entry-wise potential function}\label{sec:gd}

In this section, we show how to obtain an approximate solution of matrix sensing via gradient descent. For simplicity, we start from a case that $\{u_i\}_{i\in [m]}$ are orthogonal vectors in $\R^n$\footnote{We note that $A':=\sum_{i=1}^m b_iu_iu_i^\top$ is a solution satisfying $u_i^\top A'u_i = b_i$ for all $i\in [m]$. However, we pretend that we do not know this solution in this section.}, which already conveys the key idea of our algorithm and analysis and we generalize the solution to the non-orthogonal case (see Appendix~\ref{sec:gd_non_orthogonal}). 
We show that $\wt{\Omega}(\sqrt{m}/\delta)$ iterations of gradient descent can output a $\delta$-approximate solution, where each iteration takes $O(mn^2)$-time. Below is the main theorem of this section:

\begin{theorem}[Gradient descent for orthogonal measurements]\label{thm:gd_orthogonal}
Suppose $u_1,\dots,u_m\in \R^n$ are orthogonal unit vectors, and suppose $|b_i|\leq R$ for all $i\in [m]$. There exists an algorithm such that for any $\delta \in (0,1)$, performs $\wt{\Omega}(\sqrt{m}R\delta^{-1})$ iterations of gradient descent with $O(mn^2)$-time per iteration and outputs a matrix $A\in \R^{n\times n}$ satisfies:
\begin{align*}
    | u_i^\top A u_i - b_i| \leq \delta~~~\forall i\in [m].
\end{align*}
\end{theorem}

In Section~\ref{sec:gd_algorithm}, we introduce the algorithm and prove the time complexity. In Section~\ref{sec:gd_analysis_1} - \ref{sec:gd_analysis_2}, we analyze the convergence of our algorithm.

\subsection{Algorithm}\label{sec:gd_algorithm}

The key idea of the gradient descent matrix sensing algorithm (Algorithm~\ref{alg:GD}) is to follow the gradient of the entry-wise potential function defined as follows:
\begin{align}
    \Phi_{\lambda}(A) := \sum_{i=1}^m \cosh ( \lambda ( u_i^\top A u_i - b_i ) ).
\end{align}
Then, we have the following solution update formula:
\begin{align}\label{eq:A_t1_update}
    A_{t+1} \gets A_t - \epsilon \cdot \nabla \Phi_{\lambda}(A_t) / \| \nabla \Phi_{\lambda}(A_t) \|_F.
\end{align}

\begin{lemma}[Cost-per-iteration of gradient descent]\label{lem:gd_cost_per_iter}
Each iteration of Algorithm~\ref{alg:GD} takes
$
    O(mn^2)
$-time.
\end{lemma}
\begin{proof}
In each iteration, we first evaluate $u_i^\top A_tu_i$ for all $i\in [m]$, which takes $O(mn^2)$-time. Then, $\nabla \Phi_\lambda(A_t)$ can be computed by summing $m$ rank-1 matrices, which takes $O(mn^2)$-time. Finally, at Line~\ref{ln:gd_update}, the solution can be updated in $O(n^2)$-time.
Thus, the total running time for each iteration is $O(mn^2)$.
\end{proof}

 
\begin{algorithm}\caption{Matrix Sensing by Gradient Descent.}\label{alg:GD}
\begin{algorithmic}[1]
\Procedure{GradientDescent}{$\{u_i,b_i\}_{i\in [m]}$} \Comment{Theorem~\ref{thm:gd_orthogonal}}
    \State $\tau \gets \max_{i \in [m]} b_i $
    \State $A_1 \gets \tau \cdot I$
    \For{$t = 1 \to T$}
        \State $\nabla \Phi_{\lambda} (A_t) \gets \sum_{i=1}^m u_i u_i^\top \lambda \sinh( \lambda ( u_i^\top A_t u_i - b_i ) ) $ \Comment{Compute the gradient}
        \State $A_{t+1} \gets A_t - \epsilon \cdot \nabla \Phi_{\lambda}(A_t) / \| \nabla \Phi_{\lambda}(A_t) \|_F$\label{ln:gd_update}
    \EndFor
    \State \Return $A_{T+1}$
\EndProcedure
\end{algorithmic}
\end{algorithm}


 
\subsection{Analysis of One Iteration}\label{sec:gd_analysis_1}
Throughout this section, we suppose $A\in \R^{n\times n}$ is a symmetric matrix. 
 

We can compute the gradient of $\Phi_{\lambda}(A)$ with respect to $A$ as follows:
\begin{align}\label{eq:gradient_phi_A}
    & ~ \nabla \Phi_{\lambda}(A) \notag \\
    = & ~ \sum_{i=1}^m u_i u_i^\top \lambda \sinh\left( \lambda (u_i^\top A u_i - b_i)\right) \in \R^{n\times n}. 
\end{align}
 


We can compute the Hessian of $\Phi_{\lambda}(A)$ with respect to $A$ as follows
\begin{align*}
    & ~ \nabla^2 \Phi_{\lambda}(A) \notag \\
    = & ~ \sum_{i=1}^m ( u_i u_i^\top ) \otimes ( u_i u_i^\top ) \lambda^2 \cosh( \lambda (u_i^\top A u_i - b_i) ). 
\end{align*}
The Hessian $\nabla^2 \Phi_{\lambda}(A)\in \R^{n^2\times n^2}$ and $\otimes$ is the Kronecker product.

 



\begin{lemma}[Progress on entry-wise potential]\label{lem:gradient_descent}
Assume that $u_i \perp u_j = 0 $ for any $i,j \in [m]$ and $\| u_i\|^2 = 1$. Let $c \in (0,1)$ denote a sufficiently small positive constant. Then, for any $\epsilon,\lambda>0$ such that $\epsilon\lambda \leq c$,
 
we have for any $t>0$,
\begin{align*}
    \Phi_{\lambda} ( A_{t+1} ) \leq (1-0.9 \frac{ \lambda \epsilon }{\sqrt{m} }) \cdot \Phi_{\lambda} (A_t) +  \lambda \epsilon \sqrt{m}
\end{align*}

\end{lemma}
\begin{proof}
We defer the proof to Appendix~\ref{sec:proof_gradient_descent}
\end{proof}


\subsection{Technical Claims}
We prove some technical claims in below. 



\begin{claim}\label{cla:gd_Q1}
For $Q_1$ defined in Eq.~\eqref{eq:def_Q1}, we have
\begin{align*}
    Q_1 \leq \Big( \sqrt{m} + \frac{1}{\lambda} \| \nabla \Phi_{\lambda}(A_t) \|_F \Big) \cdot  \| \nabla \Phi_{\lambda} (A_t) \|_F^2.
\end{align*}
\end{claim}
\begin{proof}


For simplicity, we define $z_{t,i}$ to be
\begin{align*}
    z_{t,i} := \lambda ( u_i^\top A_t u_i - b_i ) .
\end{align*}
Recall that
\begin{align*}
    \nabla^2 \Phi_{\lambda}(A_t) = \lambda^2 \cdot \sum_{i=1}^m ( u_i u_i^\top ) \otimes ( u_i u_i^\top ) \cosh( z_{t,i} ) .
\end{align*}

For $Q_1$, we have
\begin{align}\label{eq:Q1}
   Q_1 = & ~  \tr[ \nabla^2 \Phi_{\lambda}(A_t) \sum_{i=1}^m  \sinh^2( z_{t,i} ) (u_i u_i^\top \otimes u_i u_i^\top) ) ] \notag\\
   = & ~ \lambda^2 \cdot \tr[ \sum_{i=1}^m  \cosh( z_{t,i} ) ( u_i u_i^\top ) \otimes ( u_i u_i^\top )    \cdot   \sum_{i=1}^m  \sinh^2( z_{t,i} ) (u_i u_i^\top ) \otimes ( u_i u_i^\top)   ] \notag\\
   = & ~ \lambda^2 \cdot \sum_{i=1}^m \tr[ \cosh( z_{t,i} )   \cdot \sinh^2( z_{t,i} )  ( u_i u_i^\top  u_i u_i^\top ) \otimes ( u_i u_i^\top  u_i u_i^\top ) ] \notag\\
   = & ~ \lambda^2 \cdot \sum_{i=1}^m  \cosh( z_{t,i} ) \sinh^2( z_{t,i} ) \notag \\
   \leq & ~ \lambda^2 \cdot (  \sum_{i=1}^m  \cosh^2( z_{t,i} ) )^{1/2}
   \cdot  ( \sum_{i=1}^m \sinh^4( z_{t,i} ) )^{1/2} \notag \\
   \leq & ~ \lambda^2 \cdot B_1 \cdot B_2,
\end{align}
where {  the first step comes from the definition of $Q_1$, the second step comes from the definition of $\nabla^2 \Phi_{\lambda}(A_t)$,}
the third step follows from $(A \otimes B) \cdot (C \otimes D) = (AC) \otimes (BD)$ and $u_i^\top u_j = 0$ , the fourth step comes from $\|u_i\| = 1$ and $\tr[ (u_i  u_i^\top) \otimes (u_i  u_i^\top) ] = 1$.

For the term $B_1$, we have
\begin{align*}
    B_1 = & ~ (  \sum_{i=1}^m \cosh^2( \lambda (u_i^\top A_t u_i - b_i ) ) )^{1/2} \\
    \leq & ~ \sqrt{m} + \frac{1}{\lambda} \| \nabla \Phi_{\lambda}(A_t) \|_F,
\end{align*}
where the second step follows Part 1 of Lemma~\ref{lem:property_sinh_cosh_scalar}.

For the term $B_2$, we have
\begin{align*}
    B_2 = & ~( \sum_{i=1}^m \sinh^4( \lambda( u_i^\top A_t u_i - b_i ) ) )^{1/2} \\
    \leq & ~ \frac{1}{\lambda^2} \| \nabla \Phi_{\lambda} (A_t) \|_F^2,
\end{align*}
where the second step follows from $\| x \|_4^2 \leq \| x \|_2^2$. This implies that
\begin{align*}
    Q_1 \leq & ~ \lambda^2 \cdot B_1 \cdot B_2 \\
    \leq & ~ \lambda^2 \cdot ( \sqrt{m} + \frac{1}{\lambda} \| \nabla \Phi_{\lambda}(A_t) \|_F ) \cdot \frac{1}{\lambda^2} \| \nabla \Phi_{\lambda} (A_t) \|_F^2 \\
    = & ~ ( \sqrt{m} + \frac{1}{\lambda} \| \nabla \Phi_{\lambda}(A_t) \|_F ) \cdot  \| \nabla \Phi_{\lambda} (A_t) \|_F^2 .
\end{align*}
\end{proof}


\begin{claim}\label{cla:gd_Q2}
For $Q_2$ defined in Eq.~\eqref{eq:def_Q2}, we have $Q_2 = 0$.
\end{claim}

\begin{proof}
Because in $Q_2$ we have :
\begin{align}\label{eq:u_ell_i_j_product}
    & ~\sum_{\ell=1}^{m}(u_{\ell} u_{\ell}^{\top} \otimes u_{\ell} u_{\ell}^{\top}) \sum_{i \neq j}(u_i u_i^{\top} \otimes u_j u_j^{\top}) \notag \\
    = & ~ \sum_{\ell=1}^{m} \sum_{i \neq j} (u_{\ell} u_{\ell}^{\top} u_i u_i^{\top}) \otimes (u_{\ell} u_{\ell}^{\top} u_j u_j^{\top}) \notag \\
    = & ~ 0, 
\end{align}
where the first step follows from $(A \otimes B) \cdot (C \otimes D) = (AC) \otimes (BD)$ , the second step follows that $u_i^{\top} u_j = 0$ if $i \neq j$ and $\ell \neq i$ or $\ell \neq j$ always holds in Eq.~\eqref{eq:u_ell_i_j_product}.

Therefore, we get that $Q_2 = 0$.
\end{proof}




\subsection{Convergence for multiple iterations}\label{sec:gd_analysis_2}

The goal of this section is to prove the convergence of Algorithm~\ref{alg:GD}:

\begin{lemma}[Convergence of gradient descent]\label{lem:gd_convergence}
Suppose the measurement vectors $\{u_i\}_{i\in [m]}$ are orthogonal unit vectors, and suppose $|b_i|$ is bounded by $R$ for $i\in [m]$.  Then, for any $\delta \in (0,1)$, if we take $\lambda = \Omega(\delta^{-1}\log m)$ and $\epsilon=O(\lambda^{-1})$ in Algorithm~\ref{alg:GD}, then for $T=\widetilde{\Omega}(\sqrt{m}R\delta^{-1})$ iterations, the solution matrix $A_T$ satisfies:
\begin{align*}
    | u_i^\top A_{T} u_i - b_i| \leq \delta~~~\forall i\in [m].
\end{align*}
\end{lemma}

\begin{proof}
We defer the proof to Appendix~\ref{sec:gd_convergence_proof}
\end{proof}

 


Theorem~\ref{thm:gd_orthogonal} follows immediately from Lemma~\ref{lem:gd_cost_per_iter} and Lemma~\ref{lem:gd_convergence}.