\section{Technique Overview}

 
We first analyze the convergence guarantee of our matrix sensing algorithm based on gradient descent and improve its time complexity with stochastic gradient descent under the assumption where $\{u_i\}_{i\in [m]}$ are orthogonal vectors. We then analyze the convergence guarantee of our matrix sensing algorithm under a more general assumption where $\{u_i\}_{i\in [m]}$ are non-orthogonal vectors and $|u_i^{\top}  u_j| \leq \rho$.

\paragraph{Gradient descent.} We begin from the case where $\{u_i\}_{i\in [m]}$ are orthogonal vectors in $\R^n$. We can the following entry-wise potential function:
\begin{align*}
   \Phi_{\lambda}(A) := \sum_{i=1}^m \cosh ( \lambda ( u_i^\top A u_i - b_i ) ) 
\end{align*}
and analyze its progress during the gradient descent according to the update formula defined in Eq.~\eqref{eq:A_t1_update} 
for each iteration. 
We split the gradient of the potential function into diagonal and off-diagonal terms. We can upper bound the diagonal term and prove that the off-diagonal term is zero.
Combining the two terms together, we can upper bound the progress of update per iteration in Lemma~\ref{lem:gradient_descent} by:
\begin{align*}
     \Phi_{\lambda} ( A_{t+1} ) \leq (1-0.9 \frac{ \lambda \epsilon }{\sqrt{m} }) \cdot \Phi_{\lambda} (A_t) +  \lambda \epsilon \sqrt{m}.
\end{align*}
By accumulating the progress of update for the entry-wise potential function over $T=\widetilde{\Omega}(\sqrt{m}R\delta^{-1})$ iterations, we have $\Phi(A_{T+1}) \leq O(m)$. This implies that our Algorithm~\ref{alg:GD} can output a matrix $A_{T} \in \R^{n \times n}$ satisfying guarantee in Eq.~\eqref{eq:gd_approximation_guarantee}, 
and the corresponding time complexity is $O(mn^2)$.

We then analyze the gradient descent under the assumption where $\{u_i\}_{i\in [m]}$ are non-orthogonal vectors in $\R^n$, $|u_i^{\top}  u_j| \leq \rho$ and $\rho \leq \frac{1}{10m}$. We can upper bound the diagonal entries and off-diagonal entries respectively and obtain the same progress of update per iteration in Lemma~\ref{lem:gradient_descent_rho}. Accumulating in $T=\widetilde{\Omega}(\sqrt{m}R\delta^{-1})$ iterations, we can prove the approximation guarantee of the output matrix of our matrix sensing algorithm.


\paragraph{Stochastic gradient descent.} To further improve the time cost per iteration of our approximate matrix sensing,  by uniformly sampling a subset ${\cal B}\subset [m]$ of size $B$, we compute the gradient of the stochastic potential function:
\begin{align*}
   \nabla \Phi_{\lambda}(A{ , {\cal B}}) := {  \frac{m}{|{\cal B}|}\sum_{i \in {\cal B}}} u_i u_i^\top \lambda \sinh( \lambda (u_i^\top A u_i - b_i) ),
\end{align*}
 and update the potential function based on update formula defined in Eq.~\eqref{eq:sgd_potential_func}. We upper bound the diagonal and off-diagonal terms respectively and obtain the expected progress on the potential function in Lemma~\ref{lem:stochastic_gradient_descent}.


    Over $T=\widetilde{\Omega}(m^{3/2}B^{-1}R\delta^{-1})$ iterations, we can upper bound $\Phi(A_{T+1}) \leq O(m)$ with high probability. With similar argument to gradient descent section, we can prove that the SGD matrix sensing algorithm can output a solution matrix satisfying the same approximation guarantees with high success probability in Lemma~\ref{lem:sgd_convergence}. The optimized time complexity is $O(Bn^2)$ where $B$ is the SGD batch size. 
    
For the more general assumption where $\{u_i\}_{i\in [m]}$ are non-orthogonal vectors in $\R^n$ and  $|u_i^{\top}  u_j|$ has an upper bound, 
We also provide the cost-per-iteration analysis for stochastic gradient descent by bounding the diagonal entries and off-diagonal entries of the gradient matrix respectively. Then we prove that the progress on the expected potential satisfies the same guarantee as the gradient descent in Lemma~\ref{lem:sgd_potential_general}. Therefore, our SGD matrix sensing algorithm can output an matrix satisfying the approximation guarantee after
\begin{align*} 
T=\widetilde{\Omega}(m^{3/2}B^{-1}R\delta^{-1})
\end{align*}
iterations under the general assumption.


