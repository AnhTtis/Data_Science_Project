\vspace{-3mm}
This line of research is highly relevant for society at large because of ubiquitous AI-enabled decision-making systems. Some of these decisions have a strong economic/emotional impact on individuals, for example, AI-enabled decisions for bank loan approval, parole/bail application, AI-based medical diagnosis, etc. In such situations, it is imperative upon the decision-making system to provide explanations to affected data subjects. However, global explanations for prediction using feature importance scores or local explanations like shapely values would not suffice in these scenarios. Explanations, which provide actionable recourse against a decision are more desirable. Counterfactual explanations generated with feasibility constraints exhibit this property. Thus counterfactual explanations not only help data subjects understand the reasons behind a decision but also help contest it and take recourse against the decision to improve their outcome. In this paper, we do not focus on the actionability or feasibility of counterfactual explanations but we focus on their interpretability. Interpretable counterfactual explanations are more likely to be trusted by the data subject and hence adopted easily. 

% We believe that, through higher interpretability, more data subjects would adopt AI-based decisions and use the counterfactual explanations to adapt and change their outcome in the future. 

% We believe the observation of deriving embedding with competing objectives results in the automatic discovery of minimal and influential feature sets will find it's application across various other machine learning domain.
