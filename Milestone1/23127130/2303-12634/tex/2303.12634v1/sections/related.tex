\vspace{-2mm}
Counterfactual analysis is a concept derived from from causal intervention analysis. Counterfactuals refer to model outputs corresponding to  certain imaginary scenarios that we have not observed or cannot
observe. Recently \cite{wachter2017counterfactual} proposed the idea of model agnostic (without opening the black box) counterfactual explanations, through simultaneous minimization of the error between model prediction and the desired counterfactual and distance between original instance and their corresponding counterfactual. This idea has been extended for multiple  scenarios by \cite{mahajan2019preserving},  \cite{ustun2019actionable} , \cite{poyiadzi2020face} based on the incorporation of feasibility constraints, actionability and diversity of counterfactuals. Authors in \cite{mothilal2020explaining} proposed a framework for generating diverse set of counterfactual explanations based on determinantal point processes. They argue that a wide range of suggested changes along with a proximity to the original input improves the chances those changes being adopted by data subjects. Causal constraints of our society do not allow the data subjects to reduce their age  while increasing their educational qualifications. Such feasibility constraints were addressed by \cite{mahajan2019preserving} and \cite{joshi2019towards} using a causal framework. Authors in  \cite{mahajan2019preserving} addresses the feasibility of counterfactual explanations through causal relationship constraints amongst input features. They present a method that uses  structural causal models to generate actionable counterfactuals. Authors in \cite{joshi2019towards} propose to characterize data manifold and then provide an optimization framework to search for actionable counterfactual explanation on the data manifold via its latent representation. Authors in \cite{poyiadzi2020face} address the issues of feasibility and actionability through feasible paths, which are based on the shortest path distances defined via density-weighted metrics.


An important aspect of counterfactual explanations is their interpretability. A counterfactual explanation is more interpretable if it lies within or close to the data distribution of the training data of the black box classifier. To address this issue \cite{dhurandhar2018explanations} proposed the use of auto-encoders to generate counterfactual explanations which are ``close'' to the data manifold. They proposed incorporation of an auto-encoder reconstruction loss in counterfactual search process to penalize counterfactual which are not true to the data manifold.  This line of research was further extended by \cite{van2019interpretable}, they proposed to connect the output behaviour of the classifier to the latent space of the auto-encoder using prototypes. These prototypes improved speed of counterfactual search process and the interpretability of the resulting counterfactual explanations. 



While \cite{van2019interpretable} connects the output behaviour of the classifier to the latent space through prototypes, the latent space is still unaware of the class tag information. We propose to build a latent space which is aware of the class tag information through joint training of the auto-encoder and the classifier. Thus the counterfactual explanations generated will not only be faithful the entire training data distribution but also faithful the data distribution of the target class. In a post-hoc scenario where access to the training data is not guaranteed, we propose to use the input-output pair data of the black box classifier to jointly train the auto-encoder and classifier in the semi-supervised learning framework. Authors in \cite{zhai2016semisupervised}, \cite{gogna2016semi} have explored the use semi-supervised auto-encoders for sentiment analysis and analysis of biomedical signal analysis. Authors in \cite{haiyan2015semi} propose a joint framework of representation and supervised learning which guarantees not only the semantics of the original data from representation learning but also fit the training data well via supervised learning. However, as far as our knowledge goes, semi-supervised learning has not been used to generate counterfactual explanations and we experimentally show that semi-supervised learning framework generates more interpretable counterfactual explanations.
\vspace{-3mm}


% \cite{laugel2019dangers} analyses the scenario where train instances of the black box classifier are not available. They propose a desiredatum based on ground-truth labelled data and the tools to analyse the risk of unjustified counterfactual explanations.







