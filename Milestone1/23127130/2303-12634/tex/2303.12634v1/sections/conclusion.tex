\vspace{-2mm}
We have empirically demonstrated semi-supervised embedding produces sparse counterfactual explanations. Sparse counterfactual explanations run the risk of not belonging to the data distribution of the target class. However, semi-supervised embedding ensures that that guided prototype lies within the target cluster with certain ``robustness''. 
%In this work we have empirically demonstrated that adding semi-supervised embedding for guiding the search for counterfactual explanation produces a robust and interpretable counterfactual explanation. In terms of various  evaluation metrics like sparsity, proximity, interpretability losses, the semi-supervised embedding framework produces a better quality counterfactual explanation. Sparse counterfactual explanations run the risk of not belonging to the data distribution of the target class, however, semi-supervised embeddings with higher discrimative capability ensure that that guided prototype lies within the target cluster with certain ``robustness''. Thus guaranteeing that counterfactual explanations lie within the target class distribution. 
%Merits of semi-supervised learning have been demonstrated for anomaly detection and data representation use cases where the merits of joint embedding have been appreciated. 
%In this paper, we demonstrated the merits of the same in the context of counterfactual explanations. 
For future work, we will explore the potential of a more rich data representation using semi-supervised Variation Auto Encoders (VAE) and imposing causality and feasibility constraints to derive a more faithful data embeddings.
\vspace{-3mm}