\vspace{-2mm}
Recently counterfactual explanations have gained popularity as tools of explainability for AI-enabled systems. A counterfactual explanation of a prediction describes the smallest change to the feature values that changes the prediction to a predefined output. A counterfactual
explanation usually takes the form of a statement like, ``You were denied a loan because your annual income was $30,000$. If your income had been $45,000$, you would have been offered a loan ''.  Counterfactual explanations are important in the context of  AI-based decision-making systems because they provide the data subjects with meaningful explanations for a given decision and the necessary actions to receive a more favorable/desired decision in the future. Application of counterfactual explanations in the areas of financial risk mitigation, medical diagnosis, criminal profiling, and other sensitive socio-economic sectors is increasing and is highly desirable for bias reduction.  

Apart from the challenges of sparsity, feasibility, and actionability, the primary challenge for counterfactual explanations is their interpretability. Higher levels of interpretability lead to higher adoption of AI-enabled decision-making systems.  Higher values of interpretability will improve the trust amongst data subjects on AI-enabled decisions. AI models used for decision making are typically black-box models, the reasons can either be the computational and mathematical complexities associated with the model or the proprietary nature of the technology.  In this paper, we address the challenge of generating counterfactual explanations that are more likely and interpretable. A counterfactual explanation is interpretable if it lies within or close to the modelâ€™s training data distribution. This problem has been addressed by constraining the search for counterfactuals to lie in the training data distribution. This has been achieved by incorporating an auto-encoder reconstruction loss in the counterfactual search process. However, adhering to training data distribution is not sufficient for the counterfactual explanation to be likely. The counterfactual explanation should also belong to the feature distribution of its target class. To understand this, let us consider an example of predicting the risk of diabetes in individuals as high or low. A sparse counterfactual explanation to reduce the risk of diabetes might suggest a decrease in the body mass index (BMI) level for an individual while leaving other features unchanged. The model might predict a low risk based on this change and the features of this individual might still be in the data distribution of the model. However,  they will not lie in the data distribution of individuals with low risk of diabetes because of other relevant features of low-risk individuals like glucose tolerance, serum insulin, diabetes pedigree, etc. 

To address this issue, authors in \cite{van2019interpretable} proposed to connect the output behavior of the classifier to the latent space of the auto-encoder using prototypes. These prototypes guide the counterfactual search process in the latent space and improve the interpretability of the resulting counterfactual explanations. However, the auto-encoder latent space is still unaware of the class tag information. This is highly undesirable, especially when using a prototype guided search for counterfactual explanations on the latent space. In this paper, we propose to build a latent space that is aware of the class tag information through joint training of the auto-encoder and the classifier. Thus the counterfactual explanations generated will not only be faithful to the entire training data distribution but also faithful to the data distribution of the target class.




% Auto-encoder trained only to minimize reconstruction loss maintains neighborhood distribution in the latent space, classification loss function maximizes the separation between class instances in the latent space. In semi supervised scenario, the framework is asked to perform a joint task, of separating the classes, along with maintain the neighborhood distribution best way possible. As a natural selection during the joint training process the gradient flow from the classification task preferentially focus on the discriminatory feature sub-dimension, while the non-discriminatory features are automatically preferred by the auto-encoder reconstruction loss to maintain the neighborhood distance distribution. Such competing behavior between the gradient effectively works as a feature regularization, automatically discriminate on the minimal set of features. The result of which only few feature changes resulting in class transition in the latent space. This trick helps to generate counterfactual explanation with minimal yet discriminatory feature intervention.



% Hence, even a complete change in classifier model or the nature of classes will not be reflected in the auto-encoder latent space. For example, MNIST data set consisting of hand written digits can used to train a classifier which predicts the digit or it can be used to train a classifier that predicts whether a hand written digit is even or odd. Now, the auto-encoder trained on MNIST data would be unaware of the nature of classifier built using this data.  This is highly undesirable, especially when using  a prototype guided search for counterfactual explanations on the latent space.

 We show that there are considerable improvements in interpretability, sparsity and proximity metrics can be achieved simultaneously, if the auto-encoder trained in a semi-supervised fashion with class tagged input data. Our approach does not rely on the availability of train data used for the black box classifier. It can be easily generalized to a post-hoc explanation method using the semi-supervised learning framework, which relies only on the predictions on the black box model. In the next section we present the related work. Then, in section \ref{ref:preliminaries} we present preliminary definitions and approaches necessary to introduce our approach. In section \ref{ref:method}, we present our approach and empirically evaluate it in section \ref{ref:experiment}.
\vspace{-3mm}


% In this paper we do not focus on feasibility/actionability constraints necessary for generating meaningful counterfactual explanations. We primarily focus in interpretability.




%Recently, this requirement has been addressed through incorporation of auto-encoder reconstruction loss in counterfactual search process.  Connecting the output behaviour of the classifier to the latent space of the auto-encoder has further improved speed of counterfactual search process and the interpretability of the resulting counterfactual explanations. Continuing this line of research, we show further improvement in interpretability of counterfactual explanations when the auto-encoder is trained in a semi-supervised fashion with class tagged input data.

% \textbf{Goal:} preserving the distribution of data while generating faithful counterfactual explanations.









