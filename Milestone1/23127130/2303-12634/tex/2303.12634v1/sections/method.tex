


\vspace{-3mm}

\begin{figure}[htp]
\centering
\label{input_mapping}
\begin{subfigure}[b]{0.3\textwidth}
\centering
\begin{tikzpicture}
\node (x) at (0, 0) {$\mathbf{x}$};
% \node[right=1cm of x] (z) {$\mathbf{z}$};
\node[right=2cm of x] (y) {$\mathbf{y}$};
\draw[-latex',ultra thick] (x) -- node[midway, above] {$h$} (y);
% \draw[-latex',ultra thick] (z) -- node[midway, above] {$\mathbf{\xi}$} (y);
\end{tikzpicture}
\caption{\textit{Classification}}
\label{classify_model}  
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.3\textwidth}
\centering
\begin{tikzpicture}
\node (x) at (0, 0) {$\mathbf{x}$};
\node[right=1cm of x] (z) {$\mathbf{z}$};
\node[right=1cm of z] (xp) {$\mathbf{x}$};
\draw[-latex',ultra thick] (x) -- node[midway, above] {$\mathbf{\phi_{\mathcal{X}}}$} (z);
\draw[-latex',ultra thick] (z) -- node[midway, above] {$\phi_{\mathcal{X}}^{-1}$} (xp);
\end{tikzpicture}
\caption{\textit{Autoencoder}}
\label{autoencoder_model}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.3\textwidth}
\begin{tikzpicture}
\node (x) at (0, 0) {$\mathbf{x}$};
\node[right=1cm of x] (z) {$\mathbf{z}$};
\node[above=1cm of z] (y) {$\mathbf{y}$};
\node[right=1cm of z] (xp) {$\mathbf{x}$};
\draw[-latex',ultra thick] (x) -- node[midway, above] {$\mathbf{\phi_{\mathcal{D}}}$} (z);
\draw[-latex',ultra thick] (z) -- node[midway, above] {$\phi_{\mathcal{D}}^{-1}$} (xp);
\draw[-latex',ultra thick] (z) -- node[midway, left] {$\mathbf{\xi}$} (y);
\end{tikzpicture} 
\caption{\textit{Jointly trained model}}
\label{joint_model}
\end{subfigure}
\vspace{-3mm}
\end{figure}
For the supervised classification data set $ \mathcal{D} = \langle \mathcal{X}, \mathcal{Y}\rangle$ machine learning methods learn a classifier model   $h:\mathcal{X}\mapsto\mathcal{Y}$ (see figure \ref{classify_model}). This process of learning involves minimizing a loss function of the form:
$ \mathcal{E}_{entropy} = -\sum^{\ell}_{j}\sum^{N}_{i} \mathbb{I}(y_i = j)\ast\log\left(p(y=j \mid \mathbf{x}_i, \mathcal{D})\right)$.
Auto-encoder is a neural network framework which learns a latent space representation $\mathbf{z} \in \mathcal{Z}$ for input data $\mathbf{x} \in \mathcal{X}$ along with an invertible mapping ($\phi_{\mathcal{X}}^{-1}$) (see figure \ref{autoencoder_model}) in an unsupervised fashion. 
% \begin{eqnarray}
% \mathbf{z} & = & \phi_{\mathcal{X}}(\mathbf{x}) \nonumber \\
% \phi^{-1}_{\mathcal{X}}(\mathbf{z}) & = & \mathbf{x}^{\prime} \\
% \mathbf{x} & \approx & \mathbf{x}^{\prime} \nonumber
% \end{eqnarray}
The subscript $\mathcal{X}$ represents the unsupervised training of  $\phi_{\mathcal{X}}$ and $\phi_{\mathcal{X}}^{-1}$ only on the dataset $\mathcal{X}$ . The un-supervised learning framework tries to learn data compression using continuous map $\mathbf{\phi_{\mathcal{X}}}$, while minimizing the reconstruction loss: $
\mathcal{E}_{autoenc} = \sqrt{\frac{1}{N}\sum^{N}_{i} {\vert\mathbf{x}_i - {\mathbf{x}_i^{\prime}}\vert}^{2}}
$.
In this paper we consider only undercomplete autoencoders that produce a lower dimension representation ($\mathcal{Z}$) of an high dimensional space ($\mathcal{X}$), while the decoder network ensures the reconstruction guarantee ($\mathbf{x} \approx \phi^{-1}(\phi(\mathbf{x}))$. A traditional undercomplete auto-encoder  captures the correlation between the input features for the dimension reduction.
\vspace{-3mm}
\subsection{Joint training: semi-supervised learning}
\vspace{-2mm}
For the purpose of generating counterfactual explanations, we propose to use a generic neural architecture for an undercomplete auto-encoder jointly trained with the classifier model (figure~\ref{joint_model}). The proposed system would be trained with a joint loss, defined as a linear combination of cross-entropy loss and reconstruction loss $
\mathcal{E}_{joint} = w_1\cdot\mathcal{E}_{entropy} + w_2\cdot\mathcal{E}_{autoenc}.
$ This architecture relies on the class tag information $y_i$ for every input $\mathbf{x}_i$ used to train the auto-encoder.  The subscript $\mathcal{D}$ in figure \ref{joint_model} represents the training of $\phi_{\mathcal{D}}$ and $\phi_{\mathcal{D}}^{-1}$ using dataset $\mathcal{X}$ tagged with classes from $\mathcal{Y}$ in the spirit of semi-supervised learning. This jointly trained the auto-encoder and the corresponding encoder will be represented by $AE_{\mathcal{D}}$ and $ENC_{\mathcal{D}}$.  This generic architecture can be implemented in multiple ways based on selection of classifier model $h$, architecture of neural network for $\phi_{\mathcal{D}}$ and  weights $w_1$ and $w_2$. Also, if the entire supervised $\mathcal{D}$ is unavailable and the class tag information is available only for $\mathcal{D}_t= \{\mathbf{x}_i, y_i\}$ where $i = 1, \dots, m < n$ and un-tagged data is available as $\mathcal{D}_u= \{\mathbf{x}_i\}$ where $i = m+1, \dots, n$  then the joint training approach presented in figure~\ref{joint_model} can be generalized to semi-supervised learning framework. 

\vspace{-3mm}
\subsection{Characterization of the latent embedding space}
\vspace{-2mm}
The  lower dimension representation (embedding) $\mathbf{z} \in \mathcal{Z}$ obtained through joint training using $\mathcal{D} = \langle \mathcal{X}, \mathcal{Y}\rangle$ is relatively more richer than its unsupervised counterpart trained using $\mathcal{X}$. Thus, we claim that the counterfactual explanations generated through auto-encoders trained on class tagged data will not only be more faithful to the training data distribution but also be more faithful to the target class data distribution and hence  more interpretable. We evaluate this claim by generating counterfactual evaluations on several data sets and compare them through suitable metrics. 
% However, to illustrate the motivation behind the proposed approach we consider an experiment on MNIST dataset (\cite{lecun-mnisthandwrittendigit-2010}). This dataset is converted into a supervised dataset with images of hand written digits as inputs $\mathbf{x}_i$ and $y_i \in \{\texttt{odd}, \texttt{even}\}$ as the output. 
However, to illustrate the motivation behind the proposed approach we consider the German credit data set\footnote{\url{https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)}}. This data set consists of credit risk for over 1000 individuals. It contains $13$ categorical and $7$ continuous features. The target variable is a binary decision whether borrower will be a defaulter (\texttt{high risk}) or not (\texttt{low risk}). The counterfactual explanations in this case would typically be the necessary feature changes to make an individual \texttt{low risk}. In figure \ref{mnist_embedding}, we try to characterize and visualize the embedding space in two dimensions ($\mathcal{Z} \subseteq \mathbb{R}^2$) for the German data set. We plot classifier outputs and classification  probability contours in the latent embedding space $\mathbf{z} \in \mathcal{Z}$ for the separately trained auto-encoder $AE_{\mathcal{X}}$ (henceforth termed as unsupervised auto-encoder)  in figures \ref{unsupervised_mnist} and \ref{unsupervised_mnist_cont}
        and the jointly trained auto-encoder $AE_{\mathcal{D}}$ (henceforth termed as semi-supervised auto-encoder)  in figures \ref{semisupervised_mnist} and \ref{semisupervised_mnist_cont}.
        
        
        \begin{figure*}
        \vspace{-2mm}
        \centering
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/decoder_embedding_legend.pdf}
            \caption{Unsupervised classification}
            \label{unsupervised_mnist}
        \end{subfigure}
        % \hfill
        \begin{subfigure}[b]{0.49\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{images/joint_embedding_legend.pdf}
            \caption{Semi-supervised classification}
            \label{semisupervised_mnist}
        \end{subfigure}
        \vspace{-3mm}
        % \vskip\baselineskip
        \begin{subfigure}[b]{0.49\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{images/embedding_proto_decoder_legend.pdf}
            \caption{Unsupervised contours}
            \label{unsupervised_mnist_cont}
        \end{subfigure}
        % \hfill
        \begin{subfigure}[b]{0.49\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{images/embedding_proto_joint_legend.pdf}
            \caption{Semi-supervised contours}
            \label{semisupervised_mnist_cont}
        \end{subfigure}
        % \vspace{-1mm}
\caption{Characterizing the embedding space}
\label{mnist_embedding}
\vspace{-7mm}
    \end{figure*}
For a fair comparison between unsupervised and semi-supervised frameworks, we use the same neural network framework for both $\phi_{\mathcal{X}}$ and $\phi_{\mathcal{D}}$. It can be observed that the unsupervised auto-encoder produces an embedding that does not clearly separate \texttt{high risk} and \texttt{low risk} classes (figure~\ref{unsupervised_mnist}). In the embedding space, the classification probability contours also overlap significantly (figure~\ref{unsupervised_mnist_cont}) for unsupervised case.  For the semi-supervised auto-encoder, the embeddings and their classification probability contours are clearly more separated than their unsupervised counterparts (figures \ref{semisupervised_mnist} and \ref{semisupervised_mnist_cont}). Clearly the separation of class clusters is due to inclusion of classification task in the auto-encoder training process.  The resulting embedding is indicative of distinct distribution of features between \texttt{low risk} and \texttt{high risk} individuals.
Thus, semi-supervised framework learns highly discriminative embeddings and can be utilized to better counterfactual explanations. To do so, we re-define the loss function $L$ for counterfactual explanation as:
\vspace{-2mm}
\begin{eqnarray}\label{semi-supervised_loss}
L = c\cdot L_{pred} + L_{sparsity}+ \gamma \cdot L^{\mathcal{D}}_{recon} + \theta\cdot L_{proto}^{\mathcal{D}}
\end{eqnarray}
To illustrate the effect of $L_{proto}^{\mathcal{D}}$ in comparison with $L_{proto}^{\mathcal{X}}$ on the counterfactual generation process, we consider a data instance in $\mathcal{X}$ (the feature space of the German data set) and we term its corresponding encoding as $\texttt{query} \in \mathcal{Z}$. The class tag associated with $\texttt{query}$ is $\texttt{high risk}$, hence, the target class for counterfactual explanation is $\texttt{low risk}$. Based on \eqref{eq:target_proto}, we evaluated the $\texttt{proto}$ associated with the class tag $\texttt{low risk}$ and plotted it for both unsupervised (figure~\ref{unsupervised_mnist_cont}) and semi-supervised (figure~\ref{semisupervised_mnist_cont}) scenarios. It can be observed that $\texttt{proto}$ for the semi-supervised scenario falls clearly in the $\texttt{low risk}$ cluster as opposed to $\texttt{proto}$ for the unsupervised. Under such scenarios, the counterfactual explanations generated will be more interpretable because the uncertainty associated with the class tag of the counterfactual is minimized. Apart from higher interpretability, reduction of uncertainty leads to more robust counterfactual explanations. Because a clear separation between clusters will result in a faster counterfactual search towards target \texttt{proto} without meandering towards other classes. In the next section, we experimentally evaluate and compare unsupervised and semi-supervised approaches to generate counterfactual explanations. 
\vspace{-4mm}

% Counterfactual query carries out intervention on $\mathcal{X}$, and evaluates its effect on $\mathcal{Y}$, represented as $\phi(do(x\vert\mathcal{X}))$. The average causal effect thus represented as $\phi(\mathbf{x}) - \phi(do(x'\vert\mathbf{x_0}))$. In counterfactual explanation, main objective is to find minimum intervention $do(x\vert\mathcal{X})$ to achieve target response $y^{\ast}$. 
% \begin{equation}
% \label{basic_cf_explanation}
%     \min_{x'} \vert y^{\ast} -  \phi(do(\mathbf{x}'\vert\mathbf{x}_{0}))\vert
% \end{equation}

% If $d(\mathbf{x}, \mathbf{x}')$ represents the distance metric on the feature space $\mathcal{X}$, then the optimization problem can be written as 

% \begin{equation}
% \label{modified_cf_explanation}
%     \max_{\lambda} \min_{\mathbf{x}'} \vert y' - \phi(do(x'\vert\mathbf{x}_0))\vert + \lambda\cdot d(\mathbf{x}_0, \mathbf{x}')
% \end{equation}

% \subsection{Auto-encoder}
% auto-encoder is a neural network framework for unsupervised learning framework. A auto-encoder framework learns a latent space representation ($\mathcal{Z}$) for a given a input data ($\mathcal{X}$) along with an invertible mapping($\phi^{-1}$). 

% \begin{eqnarray}
% \mathcal{Z} & = & \phi(X) \\
% \phi^{-1}(Z) & = & \mathcal{X}^{\prime} \\
% \mathcal{X} & \approx & \mathcal{X}^{\prime}
% \end{eqnarray}


% \subsection{Characterizing the input space}
% Let ${\lbrace X_i, y_i\rbrace}_{i=1\dots N}$, represents the classification dataset, $y_i\in\lbrace 1,\dots\ell\rbrace$, $\ell$ is the number of classes in the dataset. We partition in the input space by the class labels $y$. Let, $\lbrace\mathbb{X}_{y=1}, \mathbb{X}_{y=2},\dots\mathbb{X}_{y=\ell}$ are partition of the input space, such that $\mathbb{X}_{y=i}\cap\mathbb{X}_{y=j}=\emptyset\;\forall i\neq j$. $\phi$ represents the continuous mapping approximated by neural architecture from the input space $X$ to the embedding space $z$. \\


% Let assume in the input space ($\mathbf{x}$) the cluster neighborhoods are represented using the connectivity matrix $\mathcal{A}(X)$. $\mathcal{A}(i,j)$ entry in the connectivity matrix describes existence of non-intersecting continuous path from the point-set with class tag $i$ to a point-set with class tag $j$. Let assume, the path $\mathcal{A}(i,j)$ can be decomposed into a collection of intersecting convex point sets. Without loss of generality assume the shortest connected path topology from $\mathbb{X}_{y=i}$ to $\mathbb{X}_{y=j}$ is represented as a sequence of convex sets ${\lbrace\mathbf{x}^{k}_{i\rightarrow j}\rbrace}_{k=1\dots L}$, where $\mathbf{x}^{k}_{i\rightarrow j} \in \mathbb{X}_{y=i} \;\forall k\in\lbrace1,\dots L-1\rbrace$. The input space ($\mathbf{x}$) is represented using a discrete set of points (training data), therefore in order to draw the connectivity we use a distance threshold $D_{\delta}$ to build a local connectivity graph, i.e., for every two points ($x_i$, $x_j$) where the distance between them is less than $D_{\delta}$, are called neighbors. In thus obtained connectivity neighbor graph a path $\mathcal{A}(i,j)$ is a connected trail. Therefore the construction of such $\mathcal{A}(X)$, is dependent of parameter value $D_{\delta}$. As the definition of the connected path can be presented using a set of discrete points, the projection of the path can be uniquely traced onto the latent space $z$.   


% In classical neural network architecture the sequence of operation performed between each layer is well represented as $\sigma(\mathbf{W}^{T}\mathbf{X} + \mathbf{b})$, where $\sigma$ is the monotonic activation function, where $\mathbf{W}$ is the weight matrix of the corresponding neural layer, and $\mathbf{b}$ is the bias. It can be easily shown that neural transformation maintains the set convexity property. 

% \subsection{Neural transformation}
% There are two distinct tasks for neural network, classification and auto-encoder. The classification task defined by a categorical cross-entropy loss function.
% \[ 
% L_{entropy}(y, \hat{y}) = -\sum^{M}_{j}\sum^{N}_{i}y_{ij}\ast\log(\hat{y}_{ij}) 
% \] 
% $L_{cross entropy}$ results in separation between classes in the embedding space ($z$). Auto-encoder tries to learn data compression using continuous map, with minimum of reconstruction loss. 
% \[
% L_{reconstruction}(\mathbf{x},\hat{\mathbf{x}}) = \sqrt{\frac{1}{N}\sum^{N}_{i} {\vert\mathbf{x}_i - \hat{\mathbf{x}_i}\vert}^{2}}
% \]
% In both scenarios the continuous map is approximated using a sequence of neural transformation. Each neural map guarantees preservation of local convexity, ensuring final neural map is local convexity preserving. As neural network produces continuous map between input and output representation the connected set from $\mathcal{A}(i,j)$ remains connected via path ${\lbrace\mathbf{x}^{k}_{i\rightarrow j}\rbrace}_{k=1\dots L}$. The neural transformation distorts the path length in euclidean distance space.

% % \begin{figure}[htp]
% % \centering
% % \label{input_mapping}
% % \begin{subfigure}[b]{0.3\textwidth}
% % \centering
% % \begin{tikzpicture}
% % \node (x) at (0, 0) {$\mathbf{X}$};
% % \node[right=1cm of x] (z) {$\mathbf{z}$};
% % \node[right=1cm of z] (y) {$\mathbf{y}$};
% % \draw[-latex',ultra thick] (x) -- node[midway, above] {$\mathbf{\phi}$} (z);
% % \draw[-latex',ultra thick] (z) -- node[midway, above] {$\mathbf{\xi}$} (y);
% % \end{tikzpicture}
% % \caption{\textit{Classification}}
% % \label{classify_model}  
% % \end{subfigure}
% % \hfill
% % \begin{subfigure}[b]{0.3\textwidth}
% % \centering
% % \begin{tikzpicture}
% % \node (x) at (0, 0) {$\mathbf{X}$};
% % \node[right=1cm of x] (z) {$\mathbf{z}$};
% % \node[right=1cm of z] (xp) {$\mathbf{X}$};
% % \draw[-latex',ultra thick] (x) -- node[midway, above] {$\mathbf{\phi}$} (z);
% % \draw[-latex',ultra thick] (z) -- node[midway, above] {$\mathbf{\phi}^{\prime}$} (xp);
% % \end{tikzpicture}
% % \caption{\textit{Undercomplete Autoencoder}}
% % \label{autoencoder_model}
% % \end{subfigure}
% % \hfill
% % \begin{subfigure}[b]{0.3\textwidth}
% % \begin{tikzpicture}
% % \node (x) at (0, 0) {$\mathbf{X}$};
% % \node[right=1cm of x] (z) {$\mathbf{z}$};
% % \node[above=1cm of z] (y) {$\mathbf{y}$};
% % \node[right=1cm of z] (xp) {$\mathbf{X}$};
% % \draw[-latex',ultra thick] (x) -- node[midway, above] {$\mathbf{\phi}$} (z);
% % \draw[-latex',ultra thick] (z) -- node[midway, above] {$\mathbf{\phi}^{\prime}$} (xp);
% % \draw[-latex',ultra thick] (z) -- node[midway, left] {$\mathbf{\xi}$} (y);
% % \end{tikzpicture} 
% % \caption{Jointly Trained Model}
% % \label{joint_model}
% % \end{subfigure}
% % \end{figure}

%  An undercomplete autoencoder produces a lower dimension representation ($z$) of an high dimensional space ($\mathbf{x}$), while the decoder network ensures the reconstruction guarantee ($\mathbf{x} \approx \phi^{\prime}(\phi(\mathbf{x}))$ (figure~\ref{autoencoder_model}). A traditional undercomplete auto-encoder is unsupervised in nature, and captures the correlation between the input subdimension for the dimension reduction. This results in undercomplete autoencoder to learns the training data manifold. As the dimension reduction is a lossy compression, the reconstruction is not faithful while reconstructing data point, outside the training data distribution. 
 
%  Classification task is supervised in nature (figure~\ref{classify_model}), as the loss function separates the data points by class tags, the embedding obtained from this classification, not necessarily conforms to training data distribution, but produces distinct product cluster in the embedding space.  

% For uniform comparison purpose, we propose a generic neural architecture for a undercomplete auto-encoder jointly trained with the classifier model (figure~\ref{joint_model}). The proposed system trained with a joint loss, defined as a linear combination categorical cross-entropy and reconstruction loss,
% \[
% L_{joint}(\mathbf{x},y,\hat{\mathbf{x}},\hat{y}) = \alphaL_{entropy}(y,\hat{y}) + \betaL_{reconstruction}(\mathbf{x},\hat{\mathbf{x}})
% \]
% This proposed framework allows us to compare the representation capability of two neural network of same representation power trained with different loss functions.  


% \subsection{Sensitivity}
% For model comparison purpose we uses the sensitivity metric to probe into the functional that produces the input ($\mathbf{x}$) to output mapping($\phi$). In current context, there exists three different mappings, input space ($X$) to embedding space ($z$), autoencoder reconstructed output ($X^{\prime}$), and classifier prediction ($y$). The sensivity approximates the change output for a small change in the input. For vector input $\mathbf{x} \in \mathbb{R}^{d}$, the sensitivity ($\eta(\mathbf{x};\phi)$) reports a vector, defined as   

% \begin{tabular}{cccccc}
% $\eta(\mathbf{x};\phi)$ & $=$ & 
% $\begin{pmatrix}
% {\Big\vert\Big\vert \frac{\partial\phi(\mathbf{x})}{\partial\mathbf{x_1}}\Big\vert\Big\vert}_{2} \\
% \vdots \\
% {\Big\vert\Big\vert \frac{\partial\phi(\mathbf{x})}{\partial\mathbf{x_d}}\Big\vert\Big\vert}_{2} \\
% \end{pmatrix}$  & 
% $\approx$ &
% $\lim_{\delta{x}\rightarrow 0} \begin{pmatrix}
% {\Big\vert\Big\vert \frac{\phi(\mathbf{x} + \delta{x}\cdot\hat{\mathbf{e}}_1) - \phi(\mathbf{x})}{\delta{x}}\Big\vert\Big\vert}_2 \\
% \vdots \\
% {\Big\vert\Big\vert \frac{\phi(\mathbf{x} + \delta{x}\cdot\hat{\mathbf{e}}_d) - \phi(\mathbf{x})}{\delta{x}}\Big\vert\Big\vert}_2 \\
% \end{pmatrix}$
% \end{tabular}


% For input space ($\mathbf{x}$) to embedding map ($z = \phi(\mathbf{x})$), the sensitivity refers to change in the embedding with a small change in the input. The sensitivity metric provides two distinct local explanation of the map $\phi$. Firstly, it identifies the sub-dimension in the input vector $\mathbf{x}$ that maximally influence the output mapping, secondly, it provides a metric for numerical comparison between two maps, for a given input $\mathbf{x}$. The purpose of the proposed sensitivity metric is model comparison. It must be noted that sensitivity metric does not provide any directional guidance for performing search in the input space, as metric is strictly positive $\eta(\mathbf{x}; \phi) \in \mathbb{R}^{d}_{+}$.



