\vspace{-3mm}

We show that semi-supervised training has the potential to generate more interpretable counterfactual explanations. However, interpretability is a function of several parameters of the joint training framework, parameters used for joint optimization of loss functions in the counterfactual search process, parameters of neural networks used for auto-encoders, and parameters of the models used for classification. Hence, we use a common neural network architecture for both unsupervised and semi-supervised frameworks during the comparison. To handle categorical data, we generate a categorical embedding based on the architecture proposed by \cite{lopez2018hybridizing} and a corresponding decoding layer for the reverse lookup in our framework. We train the categorical embedding layer to achieve the data distribution faithful embedding definition. The embedding converts the categorical variable to continuous vector space, which is consumed into our auto-encoder (unsupervised and semi-supervised) frameworks.
\vspace{-3mm}


% \subsection{Characterizing the embedding space}
% % \subsubsection{Latent space transformation}
% The classification task is a supervised task, where auto-encoder task is an unsupervised one. Clustering is equivalent to classification task in an unsupervised context. In order to evaluate the correctness of the embedding process, we first carried out an experiment with an synthetic cluster data in 2D space. There exists 3 distinct clusters in the data-set. We trained three neural network with exactly similar architecture, but with different loss functions, viz., classification network with categorical cross entropy, auto-encoder network with reconstruction loss, and a jointly trained network with equal weights ($\alpha = \beta$).

% \begin{figure}[htp]
% \centering
% \label{input_mapping}
% \begin{subfigure}[t]{0.32\textwidth}
% \centering
% \includegraphics[width=\linewidth]{images/three_cluster_input.png}
% \caption{\small Input data}
% \label{input_distribution}  
% \end{subfigure}
% \hfill
% \begin{subfigure}[t]{0.32\textwidth}
% \centering
% \includegraphics[width=\linewidth]{images/three_cluster_embedding_decoder.png}
% \caption{\small Auto-encoder}
% \label{embedding_autoencoder}  
% \end{subfigure}
% \hfill
% \begin{subfigure}[t]{0.32\textwidth}
% \centering
% \includegraphics[width=\linewidth]{images/three_cluster_embedding_joint.png}
% \caption{\small Jointly trained}
% \label{embedding_joint}  
% \end{subfigure}
% \caption{Embedding produced by different neural network tasks.}
% \end{figure}

% The classification task guides the embedding space to attain an embedding that separates the class tags, the task restriction does not impose any restriction for maintaining data distribution resulting in the embedding where each classes tags are approximately extended in one dimension to achieve the best linear separation (figure~\ref{embedding_classification}).

% The auto-encoder tasks is to minimize the reconstruction loss, result in producing an embedding that separates the three cluster centers in the input space (figure~\ref{embedding_autoencoder}). In the input space the three cluster has a boundary overlap, in the derived embedding space the topology is not preserved in the decoder embedding space (figure~\ref{input_distribution}), where one of the cluster is pushed far from other two.

% The jointly trained model separates the clusters, along with preserving the topology (figure~\ref{embedding_joint}), i.e., the cluster neighborhood structure is maintained in the embedding space.  

% \subsection{Higher dimensional data}
% To extend our investigation related to learning behaviours of the neural networks, we have extended our work on higher dimensional data. For this purpose we have used MNIST hand written digit data-set, contains $60,000$ training images, each of $28\times 28$ dimension. We define the framework with embedding dimension $2$ for the ease of visualization. We have defined a task to detect odd and even class for a given digit image. 

%  The semi-supervised embedding (jointly trained) space showed large separation among class centers compare to unsupervised embedding (auto-encoder). The resulting embedding is indicative of distinct distribution of pixel values between odd and even classes, the continuous trail from the odd to even class in semi-supervised embedding indicates those examples, where small modification in the pixel values leads to a change in class tags, viz., $0$ to $9$, $5$ to $6$.
 
% \begin{figure}[htp]
% \centering
% \begin{subfigure}{0.48\textwidth}
% \centering
% \includegraphics[width=\linewidth]{images/decoder_embedding.pdf}
% \caption{Unsupervised Embedding}
% \label{unsupervised_mnist}
% \end{subfigure}
% \hfill
% \begin{subfigure}{0.48\textwidth}
% \centering
% \includegraphics[width=\linewidth]{images/joint_embedding.pdf}
% \caption{Semi-supervised Embedding}
% \label{semisupervised_mnist}
% \end{subfigure}
% \caption{Embedding obtained for MNIST image data with odd-even classification task}
% \label{mnist_embedding}
% \end{figure}

\subsection{Datasets used for experiments}
\vspace{-2mm}
For evaluation, we consider German credit data set, adult census data set and breast cancer Wisconsin (diagnostic) data set from \cite{Dua:2019}, MNIST data set from \cite{lecun-mnisthandwrittendigit-2010}, COMPAS data set from \cite{larson2016we} and PIMA dataset from \cite{pimadiabetes}.

\textbf{German credit data set:} Predict an individual's credit risk using $20$ distinct features. The data-set contains over 1000 entries. This data-set contains $13$ categorical and $7$ continuous features. The target variable is a binary decision whether the borrower will be a defaulter or not.

\textbf{Adult census data\footnote{{\small\url{https://archive.ics.uci.edu/ml/datasets/adult}}}:}
This is a multivariate data-set based on census data, with $12$ distinct features. Each feature represents an individual from the census data, where the target variable is binary label representing whether the individual income exceeds $\$$ 50K/yr. The data-set has over $45,000$ entries. In this study, we have ignored the entries with missing values.

\textbf{MNIST data set \footnote{{\small\url{http://yann.lecun.com/exdb/mnist/}}}:} It is a database of $28\times 28$ hand-written digit images with $60,000$ training, and $10,000$ test examples, with digit label. In this current work, we posed the recognition problem as a binary decision task of detecting odd or even digit.   

\textbf{Compas\footnote{{\small\url{https://github.com/propublica/compas-analysis}}}:} COMPAS is a commercial algorithm used by judges and parole officers for scoring a criminal defendant's likelihood of reoffending. The dataset contains over $10,000$ criminal defendants records in Broward County, Florida, and all $10$ features considered by the COMPAS algorithm. 

\textbf{PIMA\footnote{{\small\url{https://www.kaggle.com/uciml/pima-indians-diabetes-database}}}:} This dataset is made available from the National Institute of Diabetes, Digestive, and Kidney Diseases. The objective of the dataset is to diagnostically predict whether a patient has diabetes or not. The dataset consists of several medical predictor variables, viz. BMI, insulin level, age, and so on, and one binary target variable suggesting whether the data is from a diabetes patient or not. All the features of this data-set are continuous in nature.

\textbf{Breast Cancer Winsconsin(Diagonostic) Dataset\footnote{{\small\url{https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)}}}:} It contains features computed from the digitized images of fine needle aspirate (FNA) of breast masses. All features are continuous in nature. The objective of the dataset is to diagnostically predict whether the mass is malignant or benign.
\vspace{-3mm}
\subsection{Experimental setup and dependencies}
\vspace{-2mm}
\begin{comment}
All experiments are performed on a linux machine running on a single core, 32 threads, Intel(R) Xeon(R) Gold 6130 @2.10GHz processor with 256GB RAM. A neural network model is used to derive the embedding.  The unsupervised autoencoder is trained with a reconstruction loss only. The semi-supervised embedding is obtained by training the classification and reconstruction model jointly, with a linear combination of classification (cross-entropy) and reconstruction loss. Both the autoencoder models are then used to generate counterfactual instances using the CounterFactualProto class in Alibi v0.4.0 python library and those instances are then compared against each other using the metrics defined below.
\end{comment}

All experiments are performed on a Linux machine running on a single core, 32 threads, Intel(R) Xeon(R) Gold 6130 @2.10GHz processor with 256GB RAM. For each data-set, an unsupervised auto-encoder model, and one semi-supervised auto-encoder models are trained. These models shared the same neural architecture but trained with different loss functions. The unsupervised auto-encoder is trained with reconstruction loss only, while for the semi-supervised scenario the loss function is defined as a linear combination of reconstruction loss, and classification loss. We use Alibi \cite{alibi} to generate counterfactual explanations for both unsupervised and semi-supervised scenarios. Keras \cite{chollet2015keras} library is used to build the corresponding auto-encoders frameworks.
\vspace{-3mm}
% \subsection{Categorical Data representation}
% \vspace{-2mm}
% In the heart of our approach lies context dependent lower dimensional embedding of input data space. We use a neural network architecture to achieve this result. An important requirement for a neural network is the continuous nature of its input vector space. But categorical data are ubiquitous and to handle categorical data, we generate a categorical embedding based on the architecture proposed by \cite{lopez2018hybridizing} and a corresponding decoding layer for the reverse lookup in our framework. We train the categorical embedding layer to achieve the data distribution faithful embedding definition. The embedding converts the categorical variable to continuous vector space, which is consumed into our auto-encoder (unsupervised and semi-supervised) frameworks. 
% \vspace{-3mm}

\subsection{Counterfactual evaluation metrics}
\vspace{-2mm}
For a given query instance $\mathbf{x}_{q}$ the counterfactual explanation is $\mathbf{x}_{q}^{cfe}$.  We used proximity, sparsity, and interpretability losses from \cite{mothilal2020explaining,van2019interpretable} as the evaluation metrics for the counterfactual explanations.

\textbf{Proximity}: This metric evaluates the distance between the query point and the counterfactual explanation. The proximity is handled separately for continuous and categorical fields. 
\begin{small}
\begin{equation*}
 \textsc{Cont-proximity} = \frac{1}{k}\sum^{k}_{i=1}\frac{\vert x^{cfe}_{q,i} - x_{q,i}\vert}{MAD_{i}},\quad
\textsc{Cat-proximity} = 1 - \frac{1}{k} \sum^{k}_{i=1}I(x^{cfe}_{q,i} \ne x_{q,i})   
\end{equation*}
\end{small}
The measure of categorical proximity is normalized between $[0, 1]$, representing a fraction of categorical variables that need to be changed to reach the counterfactual explanation. The continuous proximity is normalized by the median absolute deviation (MAD). 

\textbf{Sparsity}: Sparsity metric reports the fraction of features changed between the query ($\mathbf{x}_{q}$), and the counterfactual explanation ($\mathbf{x}_{q}^{cfe}$) and it is defined as $\textsc{Sparsity} = 1 - \frac{1}{k}\sum^{k}_{i=1}I(x^{cfe}_{q,i} \ne x_{q,i})$. The sparsity is uniformly defined over categorical and continuous features.  A good counterfactual explanation desired to have higher sparsity value.

\textbf{Interpretability}: 
We have used two metrics $IM_1$, and $IM_2$ proposed by \cite{van2019interpretable} to evaluate interpretability. Both these metrics use class-specific auto-encoders to estimate how much the counterfactual explanation conforms to the new class tag distribution. $IM_1$ measures relative reconstruction error of target class over query class, while $IM_2$ measures relative improvement in reconstruction error of the target class over nonclass specific reconstruction error. If $AE_{\mathcal{X}}$ represents the auto-encoder trained on the entire data set $\mathcal{X}$, and  $AE_{i}$ is class-specific autoencoder for the class $i$. Then the two metrics are described as 
\vspace{-2mm}
\begin{small}
\begin{equation*}
IM_1 = \dfrac{{\vert\vert \mathbf{x}^{cfe}_q - \textit{AE}_{t}(\mathbf{x}^{cfe}_q)\vert\vert}^{2}_{2}}{{\vert\vert \mathbf{x}^{cfe}_q - \textit{AE}_{t_0}(\mathbf{x}^{cfe}_q)\vert\vert}^{2}_{2} + \epsilon},  \quad    IM_2 = \dfrac{{\vert\vert  \textit{AE}_{t}(\mathbf{x}^{cfe}_q) - \textit{AE}_{\mathcal{X}}(\mathbf{x}^{cfe}_q)\vert\vert}^{2}_{2}}{{\vert\vert\mathbf{x}^{cfe}_q\vert\vert}_{1} + \epsilon}
\end{equation*}
\end{small}
\begin{small}
\begin{table}[!htp]
\vspace{-3mm}
\centering
\begin{tabular}{l | c|p{6mm}p{6mm}|p{6mm}p{6mm}|p{6mm}p{6mm}|p{6mm}p{6mm}|p{6mm}p{6mm}}
\toprule
\multicolumn{2}{c|}{\textbf{Dataset}} & \multicolumn{2}{|c|}{\textbf{S}} & \multicolumn{2}{|c|}{\textbf{$P_{cat}$}} & \multicolumn{2}{|c|}{\textbf{$P_{cont}$}} & \multicolumn{2}{|c|}{\textbf{IM1}} & \multicolumn{2}{c}{\textbf{IM2}} \\
\midrule 
{} & & SS & U & SS & U & SS & U & SS & U & SS & U \\ \hline
\midrule
\multirow{2}{*}{German Credit} & $\mu$ & 0.60 & 0.65 & 0.99 & 0.96 & 0.11 & 0.11 & 0.94 & 0.94 & 0.10 & 0.11 \\
& $\sigma$ & 0.05 & 0.04 & 0.02 & 0.05 & 0.11 & 0.09 & 0.06 & 0.06 & 0.03 & 0.03 \\ \hline
\multirow{2}{*}{COMPAS} & $\mu$ & 0.72 & 0.64 & 0.89 & 0.82 & 1.34 & 1.76 & 2.18 & 2.97 & 0.43 & 0.46 \\
                        & $\sigma$ & 0.20 & 0.27 & 0.31 & 0.38 & 1.05 & 1.52 & 3.43 & 4.45 & 0.41 & 0.39 \\
\hline
% 		\multirow{2}{*}{Bank} & $\mu$ & 0.85 & 0.80 & 1.00 & 1.00 & 0.32 & 0.29 & 0.75 & 0.75 & 0.12 & 0.11 \\
% 							  & $\sigma$ & 0.12 & 0.14 & 0.00 & 0.00 & 0.22 & 0.04 & 0.25 & 0.25 & 0.09 & 0.08 \\
% 							  \hline
		\multirow{2}{*}{Adult Income} & $\mu$ & 0.88 & 0.85 & 1.00 & 1.00 & 0.12 & 0.15 & 1.30 & 1.32 & 0.07 & 0.07 \\
		                      & $\sigma$ & 0.04 & 0.09 & 0.00 & 0.00 & 0.01 & 0.01 & 0.46 & 0.48 & 0.05 & 0.05 \\
		                      \hline
		\multirow{2}{*}{PIMA} & $\mu$ & 0.66 & 0.59 & $--$ & $--$ & 0.32 & 0.42 & 1.43 & 1.36 & 0.37 & 0.39 \\
	                       	  & $\sigma$ & 0.20 & 0.26 & $--$ & $--$ & 0.51 & 0.58 & 0.74 & 0.65 & 0.44 & 0.46 \\
	                       	  \hline
	    \multirow{2}{*}{Cancer} & $\mu$ & 0.61 & 0.57 & $--$ & $--$ & 0.29 & 0.44 & 1.43 & 1.16 & 0.10 & 0.35 \\
	                       	  & $\sigma$ & 0.07 & 0.11 & $--$ & $--$ & 0.10 & 0.12 & 0.44 & 0.46 & 0.02 & 0.04 \\
		\midrule
	\end{tabular}
	\caption{The top header represents different metrics used for comparison, viz. Sparsity(\textbf{S}), categorical proximity  (\textbf{$P_{cat}$}), continuous proximity  (\textbf{$P_{cont}$}), interpretability metric 1 (${IM_1}$), and interpretability metric 2 (${IM_2}$). Each metric is paired with two columns: \textbf{SS} representing the metric obtained using semi-supervised embedding and \textbf{U} representing the metric obtained using classical undercomplete autoencoder.}
\label{metric_comparison}
\vspace{-4mm}
\end{table}
\end{small}

% \begin{figure}[htp]
% \centering
% \begin{subfigure}{0.45\textwidth}
% \includegraphics[width=\linewidth]{images/mnist_odd_even_comparison.png}
% \caption{MNIST data with odd-even class tags}
% \label{mnist_metric}
% \end{subfigure} 
% \hfill
% \begin{subfigure}{0.45\textwidth}
% \includegraphics[width=\linewidth]{images/system_evaluation_german_credit.png}
% \caption{German credit data}
% \label{german_metric}
% \end{subfigure}
% \caption{Comparative analysis of metrics for counterfactual explanation, across various data-set.}
% \label{metric_comparison}
% \end{figure}
\vspace{-5mm}
\subsection{Results}
\begin{comment}
Alibi framework supports multiple counterfactual functionalities, by allowing a configurable loss function used for the counterfactual query in equations \eqref{unsupervised_loss} and \eqref{semi-supervised_loss}. We have performed a comparative analysis of model performance across various data-sets. Earlier work by ~\citep{van2019interpretable}, has shown that guided prototype based approach generates more interpretable counterfactual explanations with increased speed. We also rely on guided prototype based counterfactual explanations, however, these explanations generated using an unsupervised auto-encoder are used as our baseline. We compare these baseline explanations with the counterfactual explanations generated using a jointly trained auto-encoder. Both approaches generate a classical under-complete auto-encoders. We use bootstrapping to generate over 20 training data sets and models are trained on them till saturation. Finally, the best model is selected based on its out-sample accuracy and it is used for further analysis.
\end{comment}
\vspace{-2mm}
We have sampled over $100$ instances from each data set and generated the corresponding counterfactual explanations to compare unsupervised (U) and semi-supervised (SS) frameworks using the sparsity, proximity, and interpretability metrics as shown in table \ref{metric_comparison}. A higher value of the sparsity metric indicates that the counterfactual explanation has been obtained by perturbing a lesser number of features. Across various data-sets, the counterfactual explanations produced by the (SS) framework are sparser than the solution produced by (U) framework. (SS) framework fares better even for categorical proximity and continuous proximity metrics across all datasets except COMPAS dataset, where continuous proximity of (SS) framework is higher. The interpretability metric $IM_1$, compares the reconstruction loss between query class distribution to target class distribution. The comparison yields an improvement in the (SS) framework for adult income and COMPAS data sets, suggesting the counterfactual explanation produced by a semi-supervised embedding framework is better explained by the target class distribution. For other datasets $IM_1$ is either same for both the frameworks or it's slightly high for (SS) framework. Concerning $IM_2$, the (SS) framework is consistently better than that (U) framework. The improvement in $IM_1$ and $IM_2$ compared to baseline unsupervised frameworks is not drastically high, however, these results are still important because the marginal improvement in interpretability is happening simultaneously with consistent improvement in sparsity. Sparse counterfactual explanations always run the risk of not belonging to the data distribution of the target class. 

% Sparse counterfactuals may be the result of severely overlapping class probability contours (see figure \ref{unsupervised_mnist_cont}). Because in such a situation, a small perturbation to the feature space can alter the class outputs either way. 
From figure \ref{unsupervised_mnist_cont} it is evident that a sparse perturbation to the feature space can alter the class outputs either way
Hence, a sparse counterfactual should have reduced interpretability, but on the contrary, our interpretability results have improved, although marginally. Now we present some individual counterfactual explanations obtained during our experimentation.
\begin{small}
\begin{table}[!htp]
\vspace{-2mm}
	\centering
	\begin{tabular}{l|c|c|c|c|c|c}
	\toprule
	{} & {} & \textbf{Month} & \textbf{Credit Amount} & \textbf{Installment \% } & \textbf{Purpose} & \textbf{Decision}  \\
	\midrule 
	\multirow{2}{*}{1} & U & $15\rightarrow20$ & 1778 $\rightarrow$ 2438 & $2\rightarrow 2.2$ & $--$ & \multirow{2}{*}{high$\rightarrow$low}\\
	& SS & $15 \rightarrow 20$ & $1778\rightarrow 2465$ & $--$ & $--$ &  \\ \hline
	\multirow{2}{*}{2} & U & $16\rightarrow 21$ & $--$ & $3\rightarrow 4$ & $--$ &  \multirow{2}{*}{high$\rightarrow$low} \\
	& SS & $--$ & $3050\rightarrow 3758$ & $--$ & A44$\rightarrow$A48 & \\
	\midrule
	\end{tabular}
 \caption{Counterfactual generated on German Credit dataset.}
 \vspace{-8mm}
 \label{tab:german}
\end{table}
\end{small}
The feature changes in the counterfactual explanation generated using (SS) framework generally sparser compared to the feature changes involved in the counterfactual explanations generated using the (U) framework. Few examples are showed in the table ~\ref{tab:german} and table~\ref{tab:compas}. Consider the second instance of the counterfactual query from the German Credit data-set, (SS) framework suggests: only by changing the loan application \textbf{purpose} from ``Domestic Appliances'' (A44) to ``Retraining'' (A48) the credit risk can go down significantly. This is associated with a corresponding change in the \textbf{credit amount}. The results obtained from the counterfactual of COMPAS data-set show evidence that (SS) framework is capturing the implicit correlation between various features. As an example in the second counterfactual instance (table~\ref{tab:compas}) two features, age, and age-category are simultaneously changed in the counterfactual explanation produced using semi-supervised embedding, while instances from unsupervised embedding failed to capture such relation.  Contrary to the opinion that sparser counterfactual explanations may neglect to change other correlated variables, (SS) framework makes sure it captures them, thus remaining within target class distribution. The sparser counterfactual explanations generation also indicates that the model can mine out fewer features that maximally influence the classifier decision.
\begin{small}
\begin{table}[!htp]
\vspace{-2mm}
	\centering
	\begin{tabular}{l|c|c|c|c|c|c}
		\toprule
		{} & {} & \textbf{Age} & \textbf{Age category} & \textbf{Prior} & \textbf{Charge} & \textbf{Recidivism} \\
		\midrule 
		\multirow{2}{*}{1} & U & $--$  & $25 - 45$ $\rightarrow$  $\geq$45  & $2\rightarrow 5$ & $--$ & \multirow{2}{*}{$No \rightarrow Yes$} \\
		& SS & $--$ & $--$ & $2\rightarrow 5$ & $--$ & \\ \hline
		\multirow{2}{*}{2} & U & $24 \rightarrow 48$  & $--$  & $0\rightarrow 7$ & DL revoked $\rightarrow$ Robbery & \multirow{2}{*}{$Yes \rightarrow No$} \\
		& SS & $24 \rightarrow 38$ & $\leq$25 $\rightarrow$ $25 - 45$ & $0 \rightarrow 12$ & $--$ & \\ 
		\midrule
	\end{tabular}
\caption{Counterfactual instances for COMPAS data-set.}
\label{tab:compas}
\vspace{-5mm}
\end{table}
\end{small}

\vspace{-2mm}
\begin{figure}[htp]
\centering
\begin{subfigure}{0.32\textwidth}
\includegraphics[width=\linewidth]{images/mnist_18156_50458.pdf}
\label{mnist:1}
\end{subfigure} 
\hfill
\begin{subfigure}{0.32\textwidth}
\includegraphics[width=\linewidth]{images/mnist_3706_48265.pdf}
\label{mnist:2}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
\includegraphics[width=\linewidth]{images/mnist_36662_22669.pdf}
\label{mnist:3}
\end{subfigure}
\vspace{-5mm}
\caption{Comparing decoded images of \texttt{proto} embeddings generated through unsupervised and semi-supervised frameworks.}
\label{fig:mnist-proto}
\vspace{-3mm}
\end{figure}

Further, we have defined an odd-even digit classification task on the MNIST dataset. In this case,  embeddings obtained in a semi-supervised fashion show clear class separation as in figure \ref{semisupervised_mnist_cont}. We show the decoded images of \texttt{proto} embeddings generated through (U) and (SS) frameworks in figure \ref{fig:mnist-proto}. It can be observed that the decoded \texttt{proto} values for the (SS) framework are robust, because for all the three even queries in \ref{fig:mnist-proto}, the digit $9$ happens to be the decoded \texttt{proto}. The primary reason being: convergence of prototype guided search to the class center in the embedding space. The well-formed digits are positioned around the class centers, while, the visually ambiguous digits are at the class boundaries. Thus the resulting \texttt{proto} is much stable in the (SS) framework. Embedding generated using an unsupervised framework does not show any clear class separation, hence the resulting prototypes are often ill-formed digits, and produces varied decoded \texttt{proto} images, depending on the start query.
\vspace{-3mm}

% In the semi-supervised embedding prototype instances at the cluster boundary are outlier examples, i.e., visually ambiguous digit examples. The well-formed digits are positioned around the cluster centers. The Alibi prototype guided search uses K-nearest neighbor is used to generate a prototype definition, which guides the search process. With $k=500$, the resulting prototype is much stable in the semi-supervised framework (figure~\ref{fig:mnist-proto}). Embedding generated using an unsupervised framework does not show any clear class separation, hence the resulting prototypes are often ill-formed digits, and produces varied counterfactual explanation, depending on the start query.

