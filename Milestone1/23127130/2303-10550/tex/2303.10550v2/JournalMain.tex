\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf,epstopdf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url}  
\usepackage{setspace}
\doublespacing
\usepackage[utf8]{inputenc}
\usepackage{verbatim}   
\usepackage{epstopdf}
\usepackage{amsfonts} 
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{color}
\usepackage{textcomp}
\usepackage{rotating}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{paralist}
\usepackage{enumitem}

\usepackage{booktabs}
\usepackage{float}
\usepackage[english]{babel}
\usepackage{verbatim}
\usepackage{epstopdf}
\usepackage{amsfonts} 
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{textcomp} 
\usepackage{rotating}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{paralist}
\usepackage[pdftex, pdfborder={0 0 0}]{hyperref}
\usepackage{booktabs}
\usepackage{float} 
\usepackage[font={small,it}]{caption}

\usepackage{epsfig}
\usepackage{natbib}

\newcommand{\todo}[0]{\textbf{TODO:} }

\newcommand{\bls}[1]{\renewcommand{\baselinestretch}{#1}\footnotesize\normalsize}
\newcommand{\fiid}{\func{iid}}

\newtheorem{theorem}{Theorem}
[section]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{plain}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}

\newtheorem{assumption}[theorem]{Assumption}

\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}

\newenvironment{prf}{\paragraph{Proof:}}{\hfill$\square$}
\newcommand{\I}{\sqrt{-1}}
\newcommand{\del}{\partial}
\newcommand{\dbar}{\bar{\partial}}
\newcommand{\ddbar}{\del\dbar}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\Vol}{\mathrm{Vol}}
\newcommand{\dV}{d\mathcal{V}}
\newcommand{\SBG}{\mathcal{S}\mathcal{B}\mathcal{G}_1}
\newcommand{\SBGc}{\mathcal{S}\mathcal{B}\mathcal{G}_c}
\newcommand{\SBGd}{\mathcal{S}\mathcal{B}\mathcal{G}_{\frac{1+\delta}{2}}}
\newcommand{\ricci}{\mathrm{\mathbf{Ric}}}
\newcommand{\MA}{\mathrm{\mathbf{MA}}}
\newcommand{\holosections}{\Gamma_{\mathcal{O}}}
\newcommand{\sections}{\Gamma}
\newcommand{\HH}{\mathcal{H}^2}
\newcommand\myeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}

\DeclareMathOperator*{\plim}{\mathit{p}-\lim}

\newcommand{\vecop}{\ensuremath{{\text{vec}}}} % covariance
\newcommand{\Cov}{\ensuremath{{\text{Cov}}}} % covariance
\newcommand{\V}{\ensuremath{{\mathbb V}}} % variance
\newcommand{\E}{\ensuremath{{\mathbb E}}} % expected value
\newcommand{\R}{\ensuremath{{\mathbb R}}} % real line

\newcommand{\N}{\ensuremath{{\mathbb N}}} % natural numbers

\def\stackunder#1#2{\mathrel{\mathop{#2}\limits_{#1}}}
\def\stackunder#1#2{\mathrel{\mathop{#2}\limits_{#1}}}%
\newcommand{\su}[2]{$\stackunder{(#2)}{#1}$}%
\def\stackunder#1#2{\mathrel{\mathop{#2}\limits_{#1}}}%
\newcommand{\sumath}[2]{\stackunder{(#2)}{#1}}%
\def\stackunder#1#2{\mathrel{\mathop{#2}\limits_{#1}}}%

\DeclareMathOperator{\Norm}{N}

\newcommand{\sumin}[2]{\stackunder{#2}{#1}}%
\renewcommand{\contentsname}{appendix}
\renewcommand{\vec}{\mbox{vec}}

\def\imagetop#1{\vtop{\null\hbox{#1}}}

\def\func#1{\mathop{\scriptstyle{\rm{#1}}}}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{arg\,max}

\DeclareFontFamily{OT1}{pzc}{}
\DeclareFontShape{OT1}{pzc}{m}{it}{<-> s * [1.10] pzcmi7t}{}
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}

\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}

\newcommand{\blind}{1}
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{+1.3in}%
\addtolength{\topmargin}{-.8in}%




\begin{document}


\def\spacingset#1{\renewcommand{\baselinestretch}
{#1}\small\normalsize}






\title{  \vspace{-20mm}  \bf High-Frequency Volatility Estimation with Fast Multiple Change Points Detection\thanks{We are grateful for the helpful comments and suggestions by Gary Kazantsev, W. Brent Lindquist, Stefan Mittnik, Zari Rachev, as well as the participants of Bloomberg's CTO Data Science Speaker Series, the Financial Math Seminar at Texas Tech University, and the 2023 Winter School in Quantitative Finance at the University of Zurich.}}
\author{\vspace{-5mm}{\large El Mehdi Ainasse}$^{a, \ddag}$\thanks{Work done while a Ph.D. candidate at Stony Brook University and a SUNY Research Foundation Postdoctoral Associate in Applied Mathematics and Statistics at Stony Brook University.} \hspace*{2mm}  %\addtocounter{footnote}{-2}
{\large Greeshma Balabhadra}$^{b,}$\footnote{These authors contributed equally.} \hspace*{2mm}%\addtocounter{footnote}{+1}
{\large Pawe\l \ Polak}$^{b, c}$\footnote{Corresponding author at Department of Applied Mathematics and Statistics at Stony Brook University, United States. E-mail address: \texttt{pawel.polak@stonybrook.edu}.}
\\[3mm]
$^{a}$\textit{\vspace{-2mm} \small Bloomberg, New York City, United States}\\
$^{b}$\textit{\vspace{-2mm} \small Department of Applied Mathematics and Statistics, Stony Brook University, United States}\\
$^{c}$\textit{\small Institute for Advanced Computational Science, Stony Brook University, United States}}
\date{\today }

\maketitle \thispagestyle{empty}
\vspace{-10mm}
\begin{abstract}


We propose high-frequency volatility estimators with multiple change points that are $\ell_1$-regularized versions of two classical estimators: quadratic variation and bipower variation. We establish consistency of these estimators for the true unobserved volatility and the change points locations under general sub-Weibull distribution assumptions on the jump process. The proposed estimators employ the computationally efficient least angle regression algorithm for estimation purposes, followed by a reduced dynamic programming step to refine the final number of change points. In terms of numerical performance, the proposed estimators are computationally fast and accurately identify breakpoints near the end of the sample, which is highly desirable in today's electronic trading environment. In terms of out-of-sample volatility prediction, our new estimators provide more realistic and smoother volatility forecasts, and they outperform a wide range of classical and recent volatility estimators across various frequencies and forecasting horizons.


\end{abstract}
\vspace{-4mm}
\noindent \textbf{Keywords:}  Bipower Variation; Change Points;  High-Frequency; Trend Filter; Volatility Estimation.\\
\vspace{-5mm}
\noindent \textbf{JEL Classification}: C14, C15, C58, D53, G17  \\

\newpage
\spacingset{1.45} % DON'T change the spacing!
\section{Introduction}
\label{sec:intro}

Change points detection algorithms, also known as temporal signal segmentation, are widely studied in machine learning and signal processing applications like audio, speech and image processing, network intrusion detection, climate change detection, and EEG segmentation (\citealp{RadkeAndra:05}; \citealp{ReevesChen:07}; \citealp{ZaidEricFrancis:09}; \citealp{AbolfazlYueGeorge:21}). Breakpoints\footnote{We use the words breakpoints, structural breaks, and change points as synonyms.} are abrupt variations in the data-generating process. There is extensive literature on developing different techniques for changes in the mean (\citealt{leduc-harchaoui-lasso}; \citealt{ZouYinFengWang:14}; \citealt{LiXieDaiSong:15}; \citealt{ArlotCelisseHarchaoui:19}; \citealt{AlanqaryAlomarShah:21}; \citealt{KaulFotopoulosJandhyala:21}; \citealt{WangLinWillet:21}); or more general non-parametric change point detection in a sequence of cumulative distribution functions as recently proposed in  \citet{HorvathKokoszkaWang:21}. Other change point detection algorithms include nonparametric approaches by \citet{ZouYinFengWang:14}, \citet{ArlotCelisseHarchaoui:19}; sparsified binary segmentation by \citet{ChoFryzlewicz:15} and CUSUM-based methods by \citet{AlanqaryAlomarShah:21}. \citet{IacusYoshida:10} compares the least squares method, CUSUM method, and quasi maximum likelihood approach for discretely sampled stochastic differential equations.

Regarding change points in financial modeling, \citet{BibingerJirakVetter:17} propose methods to test the presence of jumps and change points in the volatility process and analyze the roughness of the volatility. \citet{XingLi:22} uses a proposed change point intensity model to measure quote volatility. \citet{Spokoiny:09} develops a multi-scale local change point detection method with applications to value-at-risk. \citet{LiNolte:16} propose a Markov-Switching Autoregressive Conditional Intensity model, which divides daily volatility into two regimes - dominant and minor. \citet{ShiHo15} propose adaptive-FIGARCH and time-varying FIGARCH to incorporate structural changes as a function of time. Finally, \citet{MercurioSpokoiny:04} introduce a new framework for estimating the interval of time homogeneity of volatility models called Locally Adaptive Volatility Estimate.


This paper focuses on multiple change points detection in the variance process. Under very general noise distribution assumptions, we show how estimators introduced by \citet{leduc-harchaoui-lasso} and \citet{HarchaouiLeduc:10} can be naturally extended to consistently detect structural breaks not only in the observed signal but also in a latent process such as the spot volatility of high-frequency financial returns.

A piecewise-constant spot volatility process with an unknown number of breakpoints was first analyzed by \citet{Fryzlewicz:06} using a wavelet thresholding algorithm. They showed that piecewise volatility estimation is more accurate in long- and short-term volatility forecasts than in the GARCH$(1,1)$ and simple moving window techniques---a result also confirmed by our methods in the empirical section below. Similarly, as \citet{HarchaouiLeduc:10}, we recast the multiple change-point estimation problem as a variable selection problem: we use the Least Angle Regression (LARS) algorithm from \citet{EfronHastieJohstoneTibshiraniLAR:04}, and the Dynamic Programming (DP) step to implement the Least Square Total Variation (LSTV$^*$) algorithm for breakpoint location detection, and simultaneous estimation of the latent spot volatility process. The theory for the proposed estimators is developed under a general sub-Weibull noise assumption that is more natural in the context of heavy-tailed financial data than the typically assumed sub-Gaussianity. We provide non-asymptotic results in the form of probabilistic bounds with general rates and consistency for breakpoint locations and the integrated variance process under infill asymptotics.

The main contributions of our paper consist of developing a new multiple change points detection algorithm for the high-frequency spot volatility process. Our algorithm uses increments of the corresponding consistent estimator of the integrated volatility. We show that it inherits the theoretical guarantees for the consistency of breakpoints location and volatility estimation from the consistency of the original high-frequency volatility estimator without structural breaks. By doing so, we extend the consistency results proposed by \cite{leduc-harchaoui-lasso} to the given latent process---the spot volatility of high-frequency returns---and we prove it under more general sub-Weibull distribution assumptions on the jump process. Empirically, we show that the forecasting performance of the proposed estimator is much better than a broad spectrum of different volatility models often used in practice for all of the data frequencies and one-step-ahead and daily forecasting horizon periods. In particular, the proposed high-frequency volatility estimator outperforms classical QV and BV by \citet{NielsenShephard:04}; more recent refinements such as realized volatility and bipower variation (RVB) and realized volatility and quarticity (RVQ) by \citet{Yu:20}; the heavy-tailed $t$-GARCH model with Student-$t$ innovations; the long-memory extension of it---the $t$-FIGARCH model from \cite{ShiHo15}; and, finally, the mixed-frequency approach---the HAR model from \cite{Corsi:09}, where the HAR model is used only for multi-step (daily) volatility forecast.

In Section \ref{sec:ModelandProblemFormulation}, we introduce realized variance estimators and present the main assumptions and formulation of the model featuring the piecewise-constant spot volatility process. Section \ref{sec:TheoreticalResults} outlines our consistency results. In Section \ref{sec:Empirics}, we present empirical examples using simulated and real data. Section \ref{sec:Extensions} highlights the versatility and practical application of our proposed volatility estimators. Finally, in Section \ref{sec:Conclusion}, we conclude. The appendices provide supplementary information, including details on the sub-Weibull distribution, a comparison between the direct and our indirect LSTV algorithm, proofs of all the theorems, technical lemmas, and non-asymptotic results for breakpoint location.



\section{High-Frequency Volatility Modeling}\label{sec:ModelandProblemFormulation}

\subsection{Price Dynamics and Volatility Estimators}
Let $p(t)$ denote the stochastic process representing the $log$-price of an asset at time $t\geq 0$. As shown in \citet{DelbaenSchachermayer:94}, under the assumption of no arbitrage,
$p(t)$ must be a semi-martingale ($p\in \mathcal{SM}$),
For all $p\in \mathcal{SM}$, the quadratic variation (QV) process can be defined as a probability limit of the sum
%\vspace{-2mm}
\begin{equation}\label{eq:QV}
    \left[p\right](t) = \plim_{n\to \infty} \sum_{j=0}^{n-1}
    \left(p\left(t_{j+1}\right) - p\left(t_{j}\right)\right)^2,
\end{equation}
for any sequence of partitions $t_0 = 0 < t_1 < \ldots < t_n = t$ with $\sup_j (t_{j+1} - t_j)\to 0$ for $n\to \infty$.\\
When we choose the partition to be equally spaced; i.e. $t_j = jt/n$ for all $j$, then the QV is the sum of the corresponding squared $\log$-returns $r^2_{1,t}, \ldots, r^2_{n,t}$.

\begin{example}\label{example:merton-jump-diffusion}
Consider the stock prices dynamics given by the Merton Jump Diffusion (MJD) process
$$
\mbox{d} p_t= p_t\left[(\mu(t) - \mu_J \nu)\mbox{d}t + \sigma(t)\mbox{d}W_t + \mbox{d}\sum^{N_t}_{i=1}Q_i\right],\label{eq:MJD}
$$
where $\mu(t)$ and $\sigma(t)$ are drift and spot volatility, respectively, $W_t$ is a standard Brownian motion, and $\sum^{N_t}_{i=1}Q_i$ is a compound Poisson process with normal distributed jumps $Q_i\overset{i.i.d.}{\sim} N(\mu_J,\sigma^{2}_J)$ and intensity $N_t\sim \mathrm{Poisson}(\nu)$, where $\sim$ denotes distributional equivalence. The two processes are independent and adapted to the filtration $\{\mathcal{F}_t\}_{t \geq 0}$, associated with the Brownian motion $\{W_t\}_{t \geq 0}$. Hence, by using It\^{o} isometry for the jump process, one gets 
$$[p](t) = \int_0^t\sigma^2(s)\mbox{d}s + \sum^{N_t}_{i=1}Q_i^2.$$ 
\end{example}
the sum of \textit{integrated variance} and the aggregated squared jump components.\\

The integrated variance of the returns has both a continuous component as well as a quadratic variation of the jumps part. The high-frequency $log$-returns allow us to compute the realized variance (RV)
$$[p](t) = \sum_{j = 1}^{n}\left(p(jt/n)-p((j-1)t/n)\right)^2 = \sum_{j = 1}^n r^2_{j,t}.$$
 We also consider the power variation which is a jump robust estimator introduced in \citet{NielsenShephard:04}, 
\begin{equation}\label{eq:powervariation}
  [p]^{[r,s]}(t) = \left(\frac{1}{n}\right)^{1-\frac{r+s}{2}} \sum^{n}_{j=2}\abs{r_{j,t}}^r\abs{r_{j-1,t}}^s.
\end{equation}
In particular, for $r=s=1$, the above equation reduces to the bi-power variation (BV). \citet{NielsenShephard:04} proved that, in the case of a finite number of jumps, in a high-frequency setting under infill-asymptotics, the realized power variation of the returns converges to the Semi Martingale Stochastic Volatility (SMSV) process.

In the absence of price jumps, i.e., when all the $Q_i$'s are identical $0$, and assuming a frictionless market, we have
$$dp(t) = p_t\left[ \mu(t)dt + \sigma(t)dW(t)\right].$$
In this case, realized variance is a consistent estimator for the integrated volatility
$$\plim_{n \rightarrow +\infty} \sum_{i = 1}^n r^2_{i,t} = \int_0^t \sigma^2(s)ds.$$

\subsection{MJD with Sub-Weibull Jumps}

The normality assumption on the distributed jumps $Q_1, Q_2, \ldots$ in Example \ref{example:merton-jump-diffusion} above is a special case of a more general class of random variables. Indeed, Gaussian random variables are sub-Weibull with tail decay parameter $\alpha = 2$. In the sequel, we assume our $Q_j$'s to be sub-Weibull with $\alpha > 0$. In addition, we assume that the jump component in the stock price dynamics is given by a LÃ©vy process $\{\mathfrak{X}_t\}_{t \geq 0}$ with non-negative integer values. As in the case of classical MJD, we assume that the processes are independent and adapted to the filtration associated with the Brownian motion $\{W_t\}_{t \geq 0}$.\\
This particular assumption allows us to model the price jumps as being heavy-tailed. (See Appendix \ref{appendix:subweibull} about sub-Weibull random variables.) We do however remain in the context of finite jump activity. This will allow us to deal with the fact that the jump process is not centered. We note that the quadratic variation theory and volatility estimation theory that follows from it remain unchanged by this assumption.

\subsection{Volatility Estimators and Consistency Conditions}\label{subsection:volatility-estimators-and-consistency-conditions}
Recall that by the results of \citet{NielsenShephard:02}, we have
$$\plim_{n^{*} \rightarrow +\infty} \left[\sum_{i = 1}^{n^{*}} r^2_{i,t} - \left(\int_0^t\sigma^2(s)\mbox{d}s + \sum^{N_t}_{i=1}Q_i^2\right)\right] = 0 \text{ and } $$

$$\plim_{n^{*} \rightarrow +\infty} \left(\sum_{i = 1}^{n^{*}-1} \abs{r_{i,t}}\abs{r_{i+1,t}} - \int_0^t\sigma^2(s)\mbox{d}s \right) = 0,$$
under the MJD model, where $n^{*}$ denotes the number of samples.\\
We can combine both conditions under a general framework that is conducive to change point detection. Indeed, let $\mathbf{r}_t = (r_{1,t}, \cdots, r_{n^{*},t})$ denote the vector of returns, and let $F$ denote a transformation of $\mathbf{r}_t$, with $F(\mathbf{r}_t)$ being a vector of length $n$ which depends on $n^{*}$. Here, $t \in (0, T]$ for some time horizon $T$. In particular, $n$ grows to $+\infty$ as $n^{*}$ grows to $+\infty$. In what follows, we consider $F_{\text{RV}}$ and $F_{\text{BP}}$, corresponding to realized variance and bi-power variation, respectively, and defined as
\begin{align}\label{eq:F_RV_F_BP}
F_{\text{RV}}(\mathbf{r}_t) &= \mathbf{r}_t \circ \mathbf{r}_t = \left(r^2_{1,t}, \cdots, r^2_{n^{*},t}\right), \text{ and }\nonumber\\
F_{\text{BP}}(\mathbf{r}_t) &= \abs{\mathbf{r}_{t,[1:n^{*}-1]}} \circ \abs{\mathbf{r}_{t,[2:n^{*}]}} = \left(|r_{1,t}| |r_{2,t}|, \cdots, |r_{n^{*}-1,t}| |r_{n^{*},t}|\right),    
\end{align}
where $\abs{\mathbf{r}_{t,[a:b]}}$ denotes the truncated vector of absolute values of returns $(\abs{r_{a,t}}, \cdots, \abs{r_{b,t}})$, and $\circ$ denotes the Hadamard product. Clearly, for $F_{\text{RV}}$, $n = n^{*}$ and for $F_{\text{BP}}$, $n = n^{*}-1$. In the sequel, $F$ will just denote either transformation.\\
Using Riemann sums, we see that
$$\int_0^t \sigma^2(s)ds = \lim_{n \rightarrow + \infty} \sum_{i = 1}^{n} \dfrac{t}{n}\sigma^2(\tau_{i,t}),$$
where $(i-1)t/n \leq \tau_{i,t} \leq it/n$ and $i = 1, \cdots, n$. This allows us to rewrite our consistency conditions after defining a few variables.\\
First, we define the jump-corruption variables $\varepsilon_{J,1,t}, \cdots, \varepsilon_{J,n,t}$ as
$$\varepsilon_{J,i,t} := \sum_{j = \mathfrak{X}_{\tau_{i-1,t}}+1}^{\mathfrak{X}_{\tau_{i,t}}} Q^2_j \sim \sum_{j = 1}^{\mathfrak{X}_{\tau_{i,t}}-\mathfrak{X}_{\tau_{i-1,t}}} Q^2_j \sim \sum_{j = 1}^{\mathfrak{X}_{t/n}} Q^2_j.$$
Next, we define
$$y_{t,i} = \dfrac{t}{n}\sigma^{2}(\tau_{i,t}) + \chi_J\left(1-
\chi_R\right)\varepsilon_{J,i,t}.$$
Here, $\chi_J\in\left\{0,1\right\}$ is an indicator function that equals $1$ if the price process has jumps and $0$ otherwise; while $\chi_R\in\left\{0,1\right\}$ is an indicator function that equals $1$ if $F$ is jump-robust, and $0$ otherwise.\\
Combining these observations, we have the general consistency condition:
\begin{equation}\label{volatility-consistency-condition}  
\plim_{n \rightarrow + \infty} \sum_{i = 1}^{n} \left(F(\mathbf{r}_t)_i - y_{t,i}\right) = 0.
\end{equation}
In fact, \eqref{volatility-consistency-condition} implies that the vector $F(\mathbf{r}_t)$ converges to the vector $\mathbf{Y}_t$ in probability with respect to MSE. Here, $\mathbf{Y}_t := (y_{t,1}, \cdots, y_{t,n})$
\begin{proposition}\label{proposition:mse-consistency}
If (\ref{volatility-consistency-condition}) holds, then $\plim_{n \rightarrow +\infty} \dfrac{1}{n}\norm{F(\mathbf{r}_t)-\mathbf{Y}_t}^2_2 = 0.$
\end{proposition}
With this proposition at hand, we can define the following function capturing the rate of convergence of $F(\mathbf{r}_t)-\mathbf{Y}_t$ to $0$ in probability with respect to MSE:
$$\mathfrak{p}_n(\rho) := \mathbb{P}\left(\dfrac{1}{n}\norm{F(\mathbf{r}_t)-\mathbf{Y}_t}^2_2 > \rho\right).$$
This rate of convergence is important as it will appear in a number of our consistency results in the sequel.

\section{Change Point Detection with  \texorpdfstring{$\ell_1$}{l1}-Regularization}
\subsection{The LASSO Model}\label{subsec:The LASSO model}
We are interested in detecting changes in the continuous part of the square spot volatility, $\sigma^2$. We assume the spot volatility to have $K^{*}$ change points $\tau^{*}_{1,t}, \cdots, \tau^{*}_{K^{*},t}$ between $\tau^{*}_{0,t} := 0$ and $\tau^{*}_{K^{*}+1,t} := t$. Since we cannot observe the volatility, our approach will use the components of the vector of transformed returns $F(\mathbf{r}_t)$ as a proxy for the spot volatility. In particular, we will take our $\tau_{i,t}$'s to be $\tau_{i,t} := (i-1)t/n$, $i = 1, \cdots, n$ so that they are equally spaced and allow us to compute the returns easily. We are thus interested in detecting the change point locations $\tau^{*}_{k,t}$ in the following model
\begin{equation}
y_{t,i} := \dfrac{t}{n}\sigma^2(\tau^{*}_{k,t}) + \chi_J(1-\chi_R)\varepsilon_{J,i,t} 
\end{equation}
for $k = 0, \cdots, K^{*}+1$ and $i = 1, \cdots, n$, \textit{indirectly} using the proxy $F(\mathbf{r}_t)$ in view of the consistency condition \eqref{volatility-consistency-condition}.

We can rewrite the model in vector form as

\begin{align}\label{l1-penalty-equation}
&\mathbf{Y}_t = \mathbf{X}\boldsymbol\beta^{*}_{\sigma} + \chi_J\left(1-\chi_R\right)\mathfrak{E}_{J,t}.
\end{align}
Here, $\mathbf{X}$ denotes the $n \times n$ lower triangular matrix with nonzero elements
equal to $1$, $\mathfrak{E}_{J,t} := (\varepsilon
_{J,1,t},...,\varepsilon_{J,n,t})^{T}$, and the $n \times 1$ vector $\boldsymbol\beta^{*}_{\sigma}$ is ones whose components are equal to $0$ except those corresponding to the change point instants.

Normally, in change points detection problem using $\ell_1$ regularization, our goal would be to solve the following Least Squares with Total Variation penalty (LSTV) problem
\begin{align}
&\argmin_{\boldsymbol\beta \in \mathbb{R}^n} \mathcal{L}_{\sigma}(\boldsymbol\beta) 
\label{eq:objective}\\ 
\mbox{where }\mathcal{L}_{\sigma}(\boldsymbol\beta) &:= \frac{1}{n}\norm{\dfrac{t}{n}\boldsymbol{\sigma}^2_t-\mathbf{X}\boldsymbol\beta}^2_{2} + \lambda\norm{\boldsymbol\beta}_{1} \nonumber \\
&:=\frac{1}{n}\sum_{i=1}^{n}\left(\dfrac{t}{n}\sigma^2(\tau_{i,t}) - u_{i}\right)^2 + \lambda\sum_{i=1}^n\abs{u_{i}-u_{i-1}}.\nonumber
\end{align}
However, because the volatility in $\boldsymbol{\sigma}^2_t$ is not observed, the problem is infeasible. Hence, we solve the following LSTV problem instead:
\begin{align}
\widehat{\boldsymbol\beta}(\lambda) &:= \argmin_{\boldsymbol\beta \in \mathbb{R}^n} \mathcal{L}(\boldsymbol\beta) 
\label{eq:objective_feasible}\\ 
\mbox{where }\mathcal{L}(\boldsymbol\beta) &:= \frac{1}{n }\norm{F(\mathbf{r}_t)-\mathbf{X}\boldsymbol\beta}^2_{2} + \lambda\norm{\boldsymbol\beta}_{1} \nonumber \\
&:=\frac{1}{n}\sum_{i=1}^{n}(F(\mathbf{r}_t)_i - u_{i})^2 + \lambda\sum_{i=1}^n\abs{u_{i}-u_{i-1}},\nonumber
\end{align}
where $F(\mathbf{r}_t)$ is a vector of increments of realized variance or bi-power variation given in \eqref{eq:F_RV_F_BP}. Here, $u_{i}$ estimates $F(\mathbf{r}_t)_i$.\\
Our final estimator for $\boldsymbol\sigma^2_t$ will be $(n/t)\widehat{\boldsymbol\beta}(\lambda)$ due to the rescaling of the $\boldsymbol\sigma^2_t$ in the objective function $\mathcal{L}_{\sigma}$ in \eqref{eq:objective}.

\subsection{Estimation Algorithm}\label{sec:LSTValgo}
In order to estimate the  piecewise constant signal
 \citet{leduc-harchaoui-lasso}, adapts the Least Angle Regression (LARS) algorithm to solve the LSTV objective function from \eqref{eq:objective}. Since in our context the spot volatility process is unobserved, we need to replace \eqref{eq:objective} by \eqref{eq:objective_feasible}, and the LSTV(QV) and LSTV(BV) estimators are obtained using the algorithm below with $F(\mathbf{r}_t)$ replaced by the increments of QV and BV estimators, respectively. 
 
 In particular, our change points detection method consists of two steps. First is the following LSTV algorithm, which takes $K=K_{\mathrm{max}}$, an upper bound for the number of breakpoints, and returns the most significant $K_{\mathrm{max}}$ change points candidates.
\begin{enumerate}
    \item \textit{Initialization}: $k=0$; set $\widehat{\mathcal{T}}_{n,0}=\emptyset$ (change points locations) and $\widehat{u}_{i}^{[0]} = 0$, for $i=1,\ldots,n$ (estimated spot variance).
    \item While $k< K_{\mathrm{max}}$:
    \begin{itemize}
    \item \textit{Change point addition}: 
    Find $\widehat{\tau}_k$  such that
    $$\widehat{\tau}_k = \argmax_{\tau\in\left\{0,\frac{t}{n},\ldots,\frac{(n-1)t}{n}\right\}\setminus\widehat{\mathcal{T}}_{n,k-1}} \left| \sum_{i=\tau}^nF(\mathbf{r}_t)_i-\sum_{i=\tau}^n \widehat{u}_i^{[k-1]}\right|.$$
    Update the set of change points candidates: $\widehat{\mathcal{T}}_{n,k} = \widehat{\mathcal{T}}_{n,k-1} \cup\left\{\widehat{\tau}_k\right\}$. 
     \item \textit{Update}: $\widehat{u}_i^{[k]}$, for $i=1,\ldots,n$,  as a piecewise constant fit to the $F(\mathbf{r}_t)$ with changepoints at $\widehat{\mathcal{T}}_{n,k}$, i.e., for all sorted $\widehat{\tau}^{(j)}\in \widehat{\mathcal{T}}_{n,k}$, where $j=1\ldots,k$,
     $$ \widehat{u}_{m}^{[k]} \myeq(\widehat{\tau}^{(j)}-\widehat{\tau}^{(j-1)})^{-1} \sum_{i=\widehat{\tau}^{(j)}+1}^{\widehat{\tau}^{(j)}} F(\mathbf{r})_i\quad \forall m=\widehat{\tau}^{(j-1)}+1,\ldots,\widehat{\tau}^{(j)}.$$
    \item \textit{Descent direction computation}: 
    Compute $\mathbf{w}_k = (\mathbf{X}_k^T\mathbf{X}_k)^{-1} \mathbf{1}_k$, where $\mathbf{X}_k$ is a matrix which consists of the columns of $\mathbf{X}$ indexed by the elements of $\widehat{\mathcal{T}}_{n,k}$.
    \item \textit{Descent step search}: Search for $\widehat{\gamma}$ such that
    $$\widehat{\gamma} = \min_{\tau\in\left\{0,\frac{t}{n},\ldots,\frac{(n-1)t}{n}\right\}\setminus \widehat{\mathcal{T}}_{n,k}}\left(\frac{\sum_{i=\tau}^nF(\mathbf{r}_t)_i- \sum_{i=\tau}^n\widehat{u}_i^{[k]})}{1 - \sum_{i=\tau}^n w_{k,i}},\frac{\sum_{i=\tau}^nF(\mathbf{r}_t)_i+ \sum_{i=\tau}^n\widehat{u}_i^{[k]}}{1 + \sum_{i=\tau}^n w_{k,i}}\right).$$
    \item \textit{Zero-crossing check}: Let $\alpha_j = \mbox{sign}(\widehat{u}_{j+1}^{[k]} - \widehat{u}_{j}^{[k]})$, if $$\widehat{\gamma}>\widetilde{\gamma} \myeq \min_{j} (\alpha_j w_{k,j})^{-1} \left(\sum_{i=j}^n \widehat{u}_i^{[k]}\right),$$
    then, decrease $\widehat{\gamma}$ down to $\widetilde{\gamma}$, and remove $\widetilde{\tau}$ from $\widehat{\mathcal{T}}_{n,k}$, where 
    $$\widetilde{\tau} \myeq \arg \min_{j\in \widehat{\mathcal{T}}_{n,k}}(\alpha_j w_{k,j})^{-1} \left(\sum_{i=j}^n \widehat{u}_i^{[k]}\right).$$
   \end{itemize}
\end{enumerate}

 The second step consists of the reduced Dynamic Programming (rDP) method used to determine the final and optimal set of change points. First, for every $K\leq K_{\mathrm{max}}$, the rDP step finds $K$ change points by searching through the set $\widehat{\mathcal{T}}_{n,K_{\mathrm{max}}}=\left\{\tau_1,\ldots,\tau_{K_{\mathrm{max}}}\right\}$ instead of all of the points $\{1,\ldots,n\}$. The objective function for the rDP step for each $K$ in $\{1,\ldots,K_{\mathrm{max}}\}$ is 

$$ J(K) = \min_{\tau_1<\ldots<\tau_{K} }\sum_{k=1}^{K} \sum_{i=\tau_{k-1}+1}^{\tau_k} (F(\mathbf{r}_t)_i-\widehat{u}_k)^2,$$
$$ \text{where } \widehat{u}_k \myeq (\tau_k-\tau_{k-1})^{-1} \sum_{i=\tau_{k-1}+1}^{\tau_k} F(\mathbf{r}_t)_i.$$
Hence, for every $K\leq K_{\mathrm{max}}$, the selected $K$ change points reduce the sum of square errors the most among all the $K$ combinations of the candidates change points. For the model selection, i.e., to select the optimal number of change points, following \citet{leduc-harchaoui-lasso}, we use $\rho_k = J(k+1)/J(k)$ and pick $\widehat{K} : \widehat{K} = \min_{k \geq 1} \{ \rho_k \geq 1-\xi \} $, where $\xi$ is the model selection threshold parameter, and in all of the empirical analysis below it is set to $\xi=0.03$. The two-step algorithm we denote by LSTV$^*$(QV) and LSTV$^*$(BP), for quadratic variation and bi-power variation increments, respectively, and the rDP step on the LSTV results.

Empirically we observe that the rDP step, with $O(K_{\mathrm{max}}^3)$ complexity, eliminates false change point candidates from the LSTV step, which has  $O(K_{\mathrm{max}}n\log(n))$ complexity. 
Since $K_{\mathrm{max}}$ is the maximum number of plausible breakpoints, which is much smaller than $n$, the algorithm has an overall complexity of $O(K_{\mathrm{max}}n\log(n))$. 





\section{Consistency Results}\label{sec:TheoreticalResults}
In this section, we justify our indirect approach to change point estimation for high-frequency volatility by providing consistent results for the optimal solution to the feasible problem \eqref{eq:objective_feasible} and the change point estimates derived from it.

\subsection{Consistency for the Volatility Estimator}
Our first result is that $\widehat{\boldsymbol{\beta}}$ is a consistent estimator for the true $\boldsymbol\beta^{*}_{\sigma}$:
\begin{proposition}\label{proposition:consistency-lasso-observed-true-latent}
For any $\delta, \rho, \varepsilon_0 > 0$ and $n \geq 1$, we have:
$$\dfrac{1}{n}\norm{\mathbf{X}\left(\widehat{\boldsymbol\beta}-\boldsymbol\beta^{*}_{\sigma}\right)}^2_2 \leq 8\lambda n \left(\sqrt{\rho} + \chi_J(1-\chi_R)\left(\delta+ \mathbb{E}\left[Q^2\right]\varepsilon_0\right)\right)^2 + 4\lambda^2n^2,$$
with a probability larger than
$$\left[1 - 2\exp\left(-\dfrac{1}{C_{\alpha/2}}\cdot\dfrac{\delta^{\alpha/2} n^{\alpha/2}}{\mathbb{E}\left[\mathfrak{X}_t^{1+A(\alpha/2)}\right]^{\alpha/2}\norm{Q}^{\alpha}_{\Psi_{\alpha}}}\right)\right] \cdot \left(1 - \mathbb{P}\left(\abs{\mathfrak{X}_{t/n}}> \varepsilon_0\right)\right) \cdot \left(1-\mathfrak{p}_n(\rho)\right).$$

Here, $Q$ denotes the random variable whose distribution is the common distribution of the $Q^2_j$'s.
\end{proposition}

\begin{remark}
As we can see from the proposition, when there are jumps and the transformation $F$ of the returns is not jump-robust, there is a contribution from the cumulative jump component $\mathfrak{R}_t := \sum_{j = 1}^{\mathfrak{X}_t} Q^2_j$. More precisely, $\delta$ is the rate of convergence of $n^{-1}\abs{\mathfrak{R}_t-\mathbb{E}\left[\mathfrak{R}_t\right]}$ to $0$. In addition, due to the fact that $\mathfrak{R}_t$ is not centered, we get the additional $\varepsilon_0$ contribution representing the rate of convergence of $\mathfrak{X}_{t/n}$, where $n^{-1}\mathbb{E}\left[\mathfrak{R}_t\right] = \mathbb{E}\left[Q^2\right]\mathbb{E}\left[\mathfrak{X}_{t/n}\right]$ to $0$. Note that $\mathfrak{X}_{t/n}$ converges to $0$ in probability as $n$ grows to $+\infty$ by stochastic continuity and the fact that $\mathfrak{X}_0 \equiv 0$ almost surely. This affords us more flexible rates of convergence than the ones obtained in \citet{HarchaouiLeduc:10}.
\end{remark}

Note that in the worst case, when $\chi_J \equiv 1$ and $\chi_R \equiv 0$, (in the presence of jumps without jump robustness), $\widehat{\boldsymbol{\beta}}$ is indeed a consistent estimator of $\boldsymbol{\beta}_{\sigma}$ provided that $\lambda n$ and $\lambda n \left(\sqrt{\rho} + \delta + \mathbb{E}\left[Q^2\right]\varepsilon_0\right)^2$ go to $0$ as $n \rightarrow +\infty$ while $\delta n$ goes to $+\infty$ as $n \rightarrow +\infty$.

Comparing to \citet[Proposition 1]{HarchaouiLeduc:10}, $\alpha/2 = 2$ in their context, since their noise is sub-Gaussian, and our jump component is a sum of sub-Weibull random variables with tail decay $\alpha/2$. In their case, they show the rate of consistency (with respect to MSE) to be of the order $\sqrt{1+B}\sqrt{\log(n)/n}$ with a probability larger than $1-n^{-B}$, where $B > 0$; with $\lambda \propto \sqrt{1+B}\sqrt{\log(n)/n}$. Under more general distribution assumptions, we achieve the same rate with a higher degree of certainty in the presence of jumps without jump robustness.

Indeed, note that if we were to assume $\boldsymbol\beta^{*}_{\sigma}$ to be observed, we would not need to use the proxy $F(\mathbf{r}_t)$ and so the function $\mathfrak{p}_n(\rho)$ would be identically $0$. In addition, their noise, which corresponds to the quadratic variation of the jump process, is centered. Therefore, by assuming that $\mathbb{E}\left[Q^2\right] \equiv 0$, the $\varepsilon_0$-term disappears. In particular, if the quadratic variation were somehow centered (similarly to a white noise process), $\mathfrak{X}_{t/n}$ would not appear in our derivation either. Finally, in their context, $\mathfrak{X}_t$ is deterministic with a constant value of $n$. In that case, $1 + A(\alpha/2)$ takes the value $-1/2$. By taking the infimum over $\rho > 0$, our consistency bound becomes
$$8\lambda n\delta^2 + 4\lambda^2n^2$$
with a probability larger than $1-2\exp\left(-\dfrac{\delta^2 n}{C_2\norm{Q}^2_{\Psi_2}}\right)$ for some constant $C_2 > 0$ (since we are now taking $\alpha/2 = 2$).

By letting $B > 0$ and taking
$$\lambda \propto (1+B)^{1/4}\dfrac{\left(\log(n)\right)^{1/4}}{n^{1+1/4}} \text{ and } \delta \propto (1+B)^{1/8}\left(\dfrac{\log(n)}{n}\right)^{1/8},$$
we obtain a rate of consistency of the same order, but with a probability larger than 
$$1-2 \exp\left[-\dfrac{1}{C_2\norm{Q}^2_{\Psi_2}}\cdot(1+B)^{1/4}\left(n^3 \log(n)\right)^{1/4}\right],$$
which converges to $1$ faster than $1-n^{-B}$ does as $n \rightarrow +\infty$. Moreover, our regularization parameter also converges faster to zero as $n \rightarrow +\infty$.\\
Conversely, if we let $\delta := \sqrt{C_2}\norm{Q}_{\Psi_2}\sqrt{\dfrac{B\log(n)}{n}},$
for $B > 0$, then we achieve
$$\dfrac{1}{n}\norm{\mathbf{X}\left(\widehat{\boldsymbol\beta}-\boldsymbol\beta^{*}_{\sigma}\right)}^2_2 \leq 8B\log(n)\lambda + 4\lambda^2 n^2$$
with a probability larger than $1-2 n^{-B}$ for any $\lambda > 0$. We can thus choose $\lambda$ to achieve faster rates of consistency with a rate of certainty of the same order. For instance, $\lambda := n^{-3/2}$.\\
We are able to achieve these results because, in our proof, we do not take $\delta$ to depend on $\lambda$ unlike \citet[Proposition 1]{HarchaouiLeduc:10}. In addition, our propositions do not assume any particular upper bound on the values of the $\beta_i$'s, unlike \citet[Proposition 1]{HarchaouiLeduc:10}. Hence, our proposition holds more generally.\\
We now work in the standard formulation of LSTV instead of its LASSO counterpart. In particular, we write
\begin{equation}\label{LSTV}
y_{t,i} := \dfrac{t}{n}\sigma^2(\tau_{i,t}) + \chi_J(1-\chi_R)\varepsilon_{J,i,t} := u^{*}_{\sigma,i} + \chi_J(1-\chi_R)\varepsilon_{J,i,t}, \text{   } i = 1, \cdots, n,
\end{equation}
where $u^{*}_{\sigma,i} = \dfrac{t}{n}\sigma^2(\tau^{*}_{k,t}) =: \eta^{*}_k$ for $\tau^{*}_{k-1,t} \leq \tau_i \leq \tau^{*}_{k,t}-1, k = 1, \cdots, K^{*}+1$ and the target vector $\mathbf{u}^{*}_{\sigma} := \left(u^{*}_{\sigma,1},\cdots,u^{*}_{\sigma,n}\right)$ is estimated indirectly using the following total variation based penalty criterion, and the consistent proxy $F(\mathbf{r}_t)$ previously defined in \eqref{eq:F_RV_F_BP}. As before, we indirectly solve the infeasible problem
$$\argmin_{\mathbf{u} \in \mathbb{R}^n}\dfrac{1}{n}\norm{\dfrac{t}{n}\boldsymbol\sigma^2_t-\mathbf{u}}^2_2 + \lambda\sum_{i =1}^{n-1}\abs{u_{i+1}-u_i}$$
by solving the following problem instead
\begin{align*}    \widehat{\mathbf{u}}(\lambda) &= \left(\widehat{u}_{\mathbf{r},1}(\lambda),\cdots,\widehat{u}_{\mathbf{r},n}(\lambda)\right) \\
    &= \argmin_{\mathbf{u} \in \mathbb{R}^n}\frac{1}{n}\norm{F(\mathbf{r}_t)-\mathbf{u}}^2_2 + \lambda\sum_{i =1}^{n-1}\abs{u_{i+1}-u_i},
\end{align*}
and using Proposition \ref{proposition:mse-consistency} to our advantage.\\
We have the following result on the rate of convergence of $\widehat{\mathbf{u}}$ to $\mathbf{u}^{*}_{\sigma}$ in probability with respect to MSE:
\begin{proposition}\label{proposition:consistency-lasso-latent-lasso-observed-standard}
For any $\varepsilon > 0$ and $\beta_n$ such that $\beta_n \geq 2\lambda^2 n^2 + 2\lambda\mathbb{E}\left[Q^2\right]\varepsilon_0$, and for any $\rho > 0$,
\begin{align*}
&\mathbb{P}\left(\dfrac{1}{n}\norm{\widehat{\mathbf{u}}-\mathbf{u}^{*}_{\sigma}}^2_2 \geq 4(\beta_n + \rho)\right)\\
&\leq 2\chi_J(1-\chi_R)\exp\left[-\dfrac{\left(\dfrac{\beta_n}{2\lambda}-\left(\varepsilon_0 \mathbb{E}\left[Q^2\right] + \lambda n^2\right)\right)^{\alpha/2}}{C_{\alpha/2}\mathbb{E}\left[\mathfrak{X}_t^{1+A(\alpha/2)}\right]^{\alpha/2}\norm{Q}^{\alpha}_{\Psi_{\alpha}}}\right]\mathbb{P}\left(\abs{\mathfrak{X}_{t/n}} \leq \varepsilon_0\right)\\
&\quad + \chi_J(1-\chi_R)\mathbb{P}\left(\abs{\mathfrak{X}_{t/n}} > \varepsilon_0\right) + \mathfrak{p}_n(\rho).
\end{align*}
\end{proposition}
In particular, if $\beta_n/\lambda \rightarrow +\infty$ and $\lambda n^2 + \varepsilon_0 \mathbb{E}\left[Q^2\right]$ is bounded while $\beta_n \rightarrow 0$ as $n \rightarrow +\infty$, then $\widehat{\mathbf{u}}$  is a consistent estimator of $\widehat{\mathbf{u}}$ in probability with respect to MSE since the bound holds for all $\rho > 0$ and $\mathfrak{p}_n(\rho)$ goes to $0$ as $n \rightarrow +\infty$ for all $\rho > 0$; and $\mathbb{P}\left(\abs{\mathfrak{X}_{t/n}} > \varepsilon_0\right)$ also goes to $0$ as $n \rightarrow +\infty$ by stochastic continuity and the fact that $\mathfrak{X}_0 \equiv 0$ almost surely.\\
Comparing our proposition with \citet[Proposition 2]{HarchaouiLeduc:10}, $\alpha/2 = 2$ in their context, since their noise is sub-Gaussian. In their case, they obtain a probabilistic rate of MSE-consistency $\beta_n \propto B_2\log(n)/n$, for $B_2 > 0$, with an upper bound of the order $n^{c_{\mathrm{max}}(1-B_1(1-B_2)^2)}$, where $B_1 \in (0,1)$, $\lambda = O\left(B_1\sqrt{B_2}\sqrt{\log(n)}n^{-3/2}\right)$ and $c_{\mathrm{max}}$ is some maximal constant.

Let us assume that $\chi_J(1-\chi_R) \equiv 1$, corresponds to the presence of noise in the context of Leduc-Harchaoui. As previously remarked, if the true $\mathbf{u}^{*}_{\sigma}$ were actually observed, there would be no need for the proxy $F(\mathbf{r}_t)$ and so the function $\mathfrak{p}_n(\rho)$ would be identical $0$. In addition, the noise (corresponding to the jump activity in our context) would be centered, and so we would have $\mathbb{E}\left[Q^2\right]$. In particular, the probabilities involving $\mathfrak{X}_{t/n}$. Essentially, $\mathfrak{X}_{t/n}$ would be identically $0$ in the context of Leduc-Harchaoui, making the probability that $\abs{\mathfrak{X}_{t/n}} \leq \varepsilon_0$ trivially $1$, and its complementary probability trivially $0$. Furthermore, $\mathfrak{X}_t$ would be deterministic with a constant value of $n$, with $1 + A(\alpha/2) = -1/2$.

Since $\rho > 0$ is arbitrary, we can define $\rho := \rho_m$ as a decreasing sequence in $m \in \mathbb{N}$ of positive numbers converging to $0$ as $m \rightarrow +\infty$. By taking the infimum over $\varepsilon_0$ and using the continuity from below of the probability measure, it follows that
$$\mathbb{P}\left(\dfrac{1}{n}\norm{\widehat{\mathbf{u}}-\mathbf{u}^{*}_{\sigma}}^2_2 \geq 4\beta_n\right) \leq 2\exp\left[-\dfrac{\left(\dfrac{\beta_n}{2\lambda}- \lambda n^2\right)^2 \cdot n}{C_2\norm{Q}^2_{\Psi_2}}\right],$$
for some universal constant $C_2 > 0$.

Letting $\beta_n \propto B_2\log(n)/n$ and $\lambda = O\left(B_1\sqrt{B_2}\sqrt{\log(n)}n^{-3/2}\right),$
we obtain an upper bound of the order of $\exp\left(-K_{B_1,B_2} \cdot n^2\log(n)\right)$ for some constant $K_{B,B}$ that will be positive for appropriately chosen $B_1 \in (0,1)$ and $B_2 > 0$. Clearly, our upper bound for the same rate $\beta_n$ converges faster to $0$; showing that we achieve the same rate of consistency with faster rates of convergence in probability.

Conversely, suppose $B_2 > 0$ is large enough so that $B_1(1-B_2)^2 > 1$. Let us set
$$\lambda := \sqrt{c_{\mathrm{max}} C_2}\norm{Q}_{\Psi_2}\dfrac{\sqrt{\log(n)}}{n^2\sqrt{n}} \text{ and } \beta_n := c_{\mathrm{max}} C_2\norm{Q}^2_{\Psi_2}\left(1+\sqrt{B_1(1-B_2)^2-1}\right)\dfrac{\log(n)}{n^3}.$$
The probabilistic upper bound in this case is $2n^{c_{\mathrm{max}}(1-B_1(1-B_2)^2)}$, which is of the same order as the bound obtained in \citet[Proposition 2]{HarchaouiLeduc:10}. However, our rate of consistency $\beta_n$ converges to $0$ faster than their rate for the same (order of the) probabilistic upper bound. In particular, our $\lambda$ also converges faster to $0$ than their $\lambda$ for the same upper bound. In addition, it is important to note that our proposition assumes no particular geometric assumptions on $\widehat{\mathbf{u}}$ and holds more generally. Indeed, we can choose both $\beta_n$ and $\lambda$ to converge even faster to $0$ and still obtain suitable probabilistic upper bounds for the rate of consistency. 

\subsection{Consistent Estimation of Change Points Locations}\label{change points-locations-section}

In this section, we aim to estimate the change point locations. The change point estimates proposed here are obtained from the $\widehat{\beta}_i(\lambda)$'s. Let us define the sets of active variables by
$$\mathcal{A}_{\sigma} := \left\{i \in \{1, \cdots, n\} : \beta^{*}_{\sigma,i} \neq 0 \right\} = \left\{\dfrac{n}{t}\tau^{*}_1 ,\cdots, \dfrac{n}{t}\tau^{*}_{K^{*}}\right\} \text{ and } \widehat{\mathcal{A}}(\lambda) := \left\{i \in \{1, \cdots, n\} : \widehat{\beta}_i \neq 0\right\}.$$
The set of change point estimates $\mathcal{A}(\lambda)$ can be re-expressed as
\begin{equation}\label{definition:change point-estimates}
\widehat{\mathcal{A}}(\lambda) = \left\{\widehat{t}_1(\lambda), \cdots,\widehat{t}_{\abs{\widehat{\mathcal{A}}(\lambda)}}(\lambda) \right\} = \left\{\dfrac{n}{t}\widehat{\tau}_1(\lambda), \cdots,\dfrac{n}{t}\widehat{\tau}_{\abs{\widehat{\mathcal{A}}(\lambda)}}(\lambda)\right\},
\end{equation} 
where $\widehat{\tau}_1(\lambda) < \cdots < \widehat{\tau}_{\abs{\widehat{\mathcal{A}}(\lambda)}}$ are the estimates for $\tau^{*}_1, \cdots, \tau^{*}_{K^{*}}$ and $\abs{\widehat{\mathcal{A}}(\lambda)}$ denotes the cardinality of the set $\widehat{\mathcal{A}}(\lambda)$. We now define
$$I^{*}_{\min} = \min_{1 \leq k^{*} \leq K^{*}} \abs{t^{*}_{k+1}-t^{*}_{k}},
J^{*}_{\min}  = \inf_{1 \leq k \leq K^{*}}\abs{\eta^{*}_{k+1}-\eta^{*}_k}, \text{ and } J^{*}_{\max} = \sup_{1 \leq k^{*} \leq K^{*}}\abs{\eta^{*}_{k+1}-\eta^{*}_k},$$
which are the minimum interval length, and the minimum and maximum jump sizes, respectively.
\begin{assumption}\label{assumptions-estimation-change point-locations}
In the remainder, we shall work under the following assumptions:
\begin{enumerate}[nolistsep]
    \item The sequence $\{\kappa_n\}_{n \geq 1}$ is a non-increasing and positive sequence tending to $0$ as $n \longrightarrow +\infty$ and satisfying $(J^{*}_{\min})^{\alpha}\left(n\kappa_n\right)^{\alpha}\left[\log(n)\right]^{-1} \xrightarrow[n \to +\infty]{} +\infty$.
    \item The change points $t^{*}_1, \cdots, t^{*}_{K^{*}}$ satisfy $I^{*}_{\min} \geq n\kappa_n$, for all $n \geq 1$.
    \item The sequence of regularization parameters $\{\lambda\}_{n \geq 1}$ is such that \\ 
    $n\lambda \left(n\kappa_n J^{*}_{\min}\right)^{-1} \xrightarrow[n \to +\infty]{} 0$.
\end{enumerate}
\end{assumption}
\begin{proposition}\label{proposition:change points-consistency}
Suppose that $\abs{\widehat{\mathcal{A}}(\lambda)} =  K^{*}$. Under Assumption \ref{assumptions-estimation-change point-locations}, the change point estimates defined by \ref{definition:change point-estimates} satisfy
$$\mathbb{P}\left(\sup_{1 \leq k \leq K^{*}} \abs{\widehat{\tau}_k-\tau^{*}_k} \leq t\kappa_n\right) \xrightarrow[n \to +\infty]{} 1.$$
\end{proposition}

The proof of this Proposition follows almost exactly the same steps as in the proof of \citet[Proposition 3]{HarchaouiLeduc:10}, but the derivation is very lengthy and tedious in nature. We will simply explain the modifications to the proof of \citet[Proposition 3]{HarchaouiLeduc:10} that are required to prove our result. We first start by estimating
$$\mathbb{P}\left(\sup_{1 \leq k \leq K^{*}} \abs{\widehat{\tau}_k-\tau^{*}_k} > t\kappa_n + \sqrt{\rho}\right),$$
for $\rho > 0$ rather than
$$\mathbb{P}\left(\sup_{1 \leq k \leq K^{*}} \abs{\widehat{\tau}_k-\tau^{*}_k} > t\kappa_n\right).$$

By adding and subtracting $u^{*}_i$ insides the sums in Lemma \ref{lemma:KKT-change points}, we obtain extra partial sums of the form $\sum_{i = m_1}^{m_2} \left(F(\mathbf{r}_t)_i-u^{*}_i\right)$, which can be bounded by $\norm{F(\mathbf{r})-\mathbf{Y}_t}_1$ and thus by $\sqrt{n}\norm{F(\mathbf{r}_t)-\mathbf{Y}_t}_2$. But after dividing by $n$ (a rescaling occurring throughout the proof), the latter goes to $0$ as $n \rightarrow +\infty$, in probability, by Proposition \ref{proposition:mse-consistency}. This observation is the key difference between our context and \citet{HarchaouiLeduc:10}, and is the reason for the additional $\sqrt{\rho}$ appearing in the probabilistic bound above. The main estimates leading to the result and following from Lemma \ref{lemma:KKT-change points} are:
\begin{align*}
&\abs{(\widehat{t}_k-t^{*}_k)\left(\eta^{*}_{k+1}-\eta^{*}_k\right) + (\widehat{t}_k-t^{*}_k)\left(\eta^{*}_{k+1}-\eta^{*}_{k+1}\right) + \chi_J(1-\chi_R)\mathfrak{E}_{J,t}[\widehat{t}_k,t^{*}_k-1]}\\
&\leq n\lambda + n\left(\dfrac{1}{\sqrt{n}}\norm{F(\mathbf{r}_t)-\mathbf{Y}_t}_2\right),
\end{align*}
$$\abs{t^{*}_{k+1}-t^{*}_k}\abs{\widehat{\eta}_{k+1}-\eta^{*}_{k+1}}/2 \leq n\lambda + \chi_J(1-\chi_R)\abs{\mathfrak{E}_{J,t}[\widehat{t}_k,(t^{*}_k + t^{*}_{k+1})/2-1]} + n\left(\dfrac{1}{\sqrt{n}}\norm{F(\mathbf{r}_t)-\mathbf{Y}_t}_2\right),$$
$$\abs{\widehat{t}_k-t^{*}_k}\abs{\widehat{\eta}_{k+1}-\eta^{*}_k} \leq n\lambda + \chi_J(1-\chi_R)\abs{\mathfrak{E}_{J,t}[\widehat{t}_k,t^{*}_k-1]} + n\left(\dfrac{1}{\sqrt{n}}\norm{F(\mathbf{r}_t)-\mathbf{Y}_t}_2\right), \text{ and }$$
$$\abs{\widehat{t}_{k+1}-t^{*}_k}\abs{\widehat{\eta}_{k+1}-\eta^{*}_{k+1}} \leq n\lambda + \chi_J(1-\chi_R)\abs{\mathfrak{E}_{J,t}[\widehat{t}_k,t^{*}_{k+1}-1]} + n\left(\dfrac{1}{\sqrt{n}}\norm{F(\mathbf{r}_t)-\mathbf{Y}_t}_2\right).$$
These correspond to the definition of the event $C_{n,k}$, the bound for the event $A_{n,k,2}$ and equation (30) in the proof of \citet[Proposition 3]{HarchaouiLeduc:10}.\\
Following the proof of \citet[Proposition 3]{HarchaouiLeduc:10} verbatim, we obtain the same bounds, with an extra term of $\mathfrak{p}_n(\rho)$ for each bound. But the latter goes to $0$ as $n \rightarrow +\infty$ by Proposition \ref{proposition:mse-consistency}, which establishes that
$$\mathbb{P}\left(\sup_{1 \leq k \leq K^{*}} \abs{\widehat{\tau}_k-\tau^{*}_k} > t\kappa_n + \sqrt{\rho}\right) \xrightarrow[n \to +\infty]{} 0.$$
But since this bound holds for all $\rho > 0$, we use the continuity of the probability measure from below, as remarked under Proposition \ref{proposition:consistency-lasso-latent-lasso-observed-standard}, which leads to our final result.\\
We can achieve a rate $\kappa_n := 1/n$ in Proposition \ref{proposition:change points-consistency} so long as $J^{*}_{\min} \geq \left(\log(n)\right)^{(1+A)/\alpha}$ for some $A > 0$ and by letting $\lambda := n^{-1-B}J^{*}_{\min}$ for some $B > 0$; although other choices are possible for $\lambda$ and $J^{*}_{\min}$. These choices for $\kappa_n$, $\lambda$ and $J^{*}_{\min}$ clearly satisfy Assumption \ref{assumptions-estimation-change point-locations} with $\kappa_n, \lambda \xrightarrow[n \rightarrow +\infty]{} 0 $.

When $\abs{\widehat{\mathcal{A}}(\lambda)} := K$ and $K > K^{*}$ we can instead evaluate the distance between the set $\widehat{\mathcal{T}}_{n,\widehat{K}} := \left\{\widehat{\tau}_1, \cdots, \widehat{\tau}_{\widehat{K}}\right\}$ of $K$ estimated change points and the set of true change points $\mathcal{T}^{*}_{n,K^{*}} := \left\{t^{*}_1, \cdots, t^{*}_{K^{*}}\right\}$. We can compute this distance for two sets $A$ and $B$ by considering 
%\vspace{-0.5em}
$$\mathcal{D}\left(A,[B]\right) := \sup_{a \in A} d(a,B) \text{ and } \mathcal{D}\left(B,[A]\right) := \sup_{b \in B} d(b,A)$$
where $d(p,S) := \inf_{s \in S} \abs{p-s}$ is the distance from the point $p$ to the set $S$.\\
Clearly when $K = K^{*}$, Proposition \ref{proposition:change points-consistency} implies that all of $\mathcal{D}\left(\widehat{\mathcal{T}}_{n,\widehat{K}},[\mathcal{T}^{*}_{n,K^{*}}]\right)$, and $\mathcal{D}\left(\mathcal{T}^{*}_{n,K^{*}},[\widehat{\mathcal{T}}_{n,\widehat{K}}]\right)$ are less than $t\kappa_n$ with probability tending to $1$ as $n$ tends to $+\infty$. In the case when $K > K^{*}$, we have the following proposition, corresponding to \citet[Proposition 4]{HarchaouiLeduc:10}.
\begin{proposition}\label{proposition:change points-consistency-hausdorff}
Suppose that $\abs{\widehat{\mathcal{A}}(\lambda)} =: K > K^{*} := \abs{\mathcal{A}_{\sigma}}$ with probability tending to $1$ as $n \to +\infty$. Then, under Assumption \ref{assumptions-estimation-change point-locations}, and assuming that 
$$(J^{*}_{\min})^{\alpha}\left(n\kappa_n\right)^{\alpha}\left[\log\left(n^3/\lambda^2_n\right)\right]^{-1} \xrightarrow[n \to +\infty]{} 0,$$ 
the change point estimates defined by \ref{definition:change point-estimates} satisfy
$$\mathbb{P}\left(\mathcal{D} \left(\widehat{\mathcal{T}}_{n,\widehat{K}}, [\mathcal{T}^{*}_{n,K^{*}}] \right) \leq t\kappa_n\right) \xrightarrow[n \to +\infty]{} 1.$$
\end{proposition}
Again, this is proved almost exactly as in \citet[Proposition 4]{HarchaouiLeduc:10} with the aforementioned modifications resulting from adding and subtracting $u^{*}_i$ in the sums in Lemma \ref{lemma:KKT-change points}. Indeed, we first establish that for all $\rho > 0$,
$$\mathbb{P}\left(\mathcal{D} \left(\widehat{\mathcal{T}}_{n,\widehat{K}}, [\mathcal{T}^{*}_{n,K^{*}}] \right) > t\kappa_n + \sqrt{\rho}\right) \xrightarrow[n \to +\infty]{} 0,$$
by establishing the same probabilistic bounds as in the proof of \citep[Proposition 4]{HarchaouiLeduc:10}, with an extra term of $\mathfrak{p}_n(\rho)$ for each bound. But since the latter goes to $0$ as $n \rightarrow +\infty$, the result follows. Once again, using the convergence from below of the probability measure establishes the final result.\\
We can still guarantee a rate of convergence $\kappa_n := 1/n$, but due to the additional assumption
$$\mathbb{P}\left(\mathcal{D} \left(\widehat{\mathcal{T}}_{n,\widehat{K}}, [\mathcal{T}^{*}_{n,K^{*}}] \right) > t\kappa_n + \sqrt{\rho}\right) \xrightarrow[n \to +\infty]{} 0$$
that must be satisfied in addition to Assumption \ref{assumptions-estimation-change point-locations}, we must make different choices for $\lambda$ and $J^{*}_{\min}$ than the ones previously made. We can let $\lambda := n^{(-2-A)/2}$ with $A \in (0,1)$ so that $\lambda \xrightarrow[n \rightarrow +\infty]{} 0$. We then let $J^{*}_{\min} \geq \left((1-A)\log(n)\right)^{(1+B)/\alpha}$ for $B > 0$, which satisfies the remaining assumptions. Again, other choices are possible. 

\section{Empirical Analysis} \label{sec:Empirics}

We next present an empirical analysis of the proposed estimators. Specifically, we evaluate their performance first in simulations, and then on real financial data.

\subsection{Simulations}\label{subsec:Simulateddata}

In the first simulation, we aim to confirm that our estimators can accurately capture the spot volatility process, even when multiple change points are present. To achieve this, we calculate the $\log$-returns $r_{1,t}, \ldots, r_{n,t}$ from the simulated price data, employing the MJD model from \eqref{eq:MJD} with $S_0=1$, $\mu=0.02$, $\nu = 1$, $\mu_J = 0.0$, $\sigma_J=0.015$. The sampling frequency is one minute, i.e., 390 observations per day for 6.5 trading hours. These parameters are taken from \citet{ait2013leverage}. \\
We consider 10 trading days with $n=3900$ total number of observations.  Spot volatility is modeled as a step function $\sigma(s)$, featuring five break points and six levels -  $\sigma = \{2.12, 1.51, 2.35, 1.83, 2.44, 1.65, 3.13\}\times 10^{-4}$. These values are calibrated from NYSE TAQ Apple minute data from January 2016 to February 2022, with each $\sigma$ value listed above as the one-year median of minute-level realized standard deviation. We display the simulated price series alongside the corresponding $\log$-returns in \autoref{fig:stockprices}---the changes in the volatility are not easily discernible from the price or returns series in the figure.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=0.8\textwidth, height=0.5\textwidth]{images_final/StockSimFinal.jpg}

\caption{Simulated stock prices using the MJD dynamic given in Example \ref{example:merton-jump-diffusion} with six levels of the spot volatility  $\sigma = \{2.12, 1.51, 2.35, 1.83, 2.44, 1.65, 3.13\}*10^{-4}$,  $\mu=0.02$, $\nu = 1$, $\mu_J = 0.0$, $\sigma_J=0.015$. \textbf{Top figure:} Simulated stock price values. \textbf{Bottom figure:} Corresponding $\log$-returns, where the volatility regimes are visible with breakpoints at $780$, $1170$, $1950$, $3120$, and $3510$. 
\vspace*{-4mm}
\normalsize}
\label{fig:stockprices}
\end{center}
\end{figure}



\begin{figure}[ht]
\begin{center}
\includegraphics[width=0.8\textwidth, height=0.45\textwidth]{images_final/SimLSTVFinal.jpg}
\vspace*{-1mm}
\caption{Comparison of LSTV$^{*}$(BV) estimator fit for simulated stock prices with jumps and fixed levels of volatility for the stock prices simulated in Figure \ref{fig:stockprices}. Y-axis here is re-scaled $\sigma^2$. \normalsize}
\vspace*{-4mm}
\label{fig:TruevsEstimateSim}
\end{center}
\end{figure}

\autoref{fig:TruevsEstimateSim} displays the corresponding squared returns together with the comparison between the actual spot volatility and the estimated values using the LSTV$^*$(BV) change points detection method with $K_{\mathrm{max}}=8$ and $\xi=0.03$. The estimator successfully detects all the breakpoints, providing a close fit with a small mean-square error between the true and estimated spot volatility. As for LSTV$^*$(QV), it yields the same fit, and hence it is excluded from the plot.

\begin{figure*}
\begin{center}
\includegraphics[width=\textwidth]{images_final/HausdorffFinal_median.jpg}
\vspace*{-8mm}
\caption{ Hausdorff error comparison of LSTV$^*$(BV) estimator fit for simulated stock prices with jumps and fixed levels of volatility with the position of jump size vs. changes in the size of jumps for different MJD models from Example \ref{example:merton-jump-diffusion}. \textbf{Top left:} heat map is for classical Geometric Brownian Motion (MJD with no jumps) simulated stock prices of total sample size $3900$ with $\mu=0.22$ and various levels of sigma changes per row - $(\sigma_1, \sigma_2)$ are $(0.15,0.18)$, $(0.15,0.21)$, $(0.15,0.24)$, $(0.15,0.27)$, $(0.15,0.3)$, respectively. Each column indicates the position of the breakpoint in percentiles of actual data - $0.01$, $0.025$, $0.1$, $0.5$, $0.95$, $0.995$, $0.999$. \textbf{Top right, bottom left}, and \textbf{bottom right:} heat maps are MJD simulations with $\mu_J = 0.0, \sigma_J=0.015 $ and jump intensities $\nu=1,3,10$, respectively.}
\label{fig:Hausdorff}
\end{center}
\end{figure*}

Next, to evaluate the numerical efficiency of the LSTV$^*$ method, we perform a simulation with a single breakpoint at various positions and sizes within the simulated data. Our focus in this simulation is on locating the breakpoint rather than estimating the correct number of breakpoints. As such, we set $K_{\mathrm{max}}=1$ and keep $\xi=0.03$. We calculate the average Hausdorff distance\footnote{The Hausdorff distance between two discrete subsets $A$ and $B$  is defined as $d_H(A,B) = \max\left\{\max_{a\in A}d(a,B), \max_{b\in B} d(A,b)\right\}$, where $d(a,B) = \min_{b\in B}\left|a-b\right|$, and $d(A,b)$ is defined analogously. In our context, the sets $A$ and $B$ correspond to the true and estimated breakpoint locations, respectively.} over $10000$ random paths from the MJD model, using the same parameters as in the first simulation. To assess the robustness of our results, we set the jump intensity $\nu$ at four different levels, including the no-jumps case, as specified in the caption of \autoref{fig:Hausdorff}, and we vary the size of the change point and its position relative to the sample size as described in the $y$- and $x$-labels of \autoref{fig:Hausdorff}.

The results presented in \autoref{fig:Hausdorff} demonstrate that the LSTV$^*$(BV) method accurately estimates the true breakpoint, and is robust against varying jump intensity levels and change point locations. Across most simulations, the Hausdorff distance is less than $0.1\%$ of the observations, which corresponds to less than three observations, even when the change point is of the order of $0.03$ and located at the $0.995$ percentile of the sample (i.e., with only $20$ observations following the change point).


\subsection{Estimation and Forecasting NYSE High Frequency Volatility}\label{subsec:NYSEdata}

We evaluate the proposed estimators empirically using real financial data from the New York Stock Exchange (NYSE) for the period from January 2018 through June 2022. We select the ten largest stocks in terms of average market capitalization, based on data from NYSE Trade \& Quote (TAQ). Because the trading times for the stocks are not synchronized, and prices are subject to market microstructure noise and end-of-day effects, we use mid-prices and impute missing values with neighboring prices. We obtain cleaned returns aggregated to frequencies of $1$, $5$, $15$, $30$, and $60$ minutes during trading hours between 9:30 a.m.\ and 4:00 p.m. The number of observations per day varies from 6 to 390, depending on the frequency. We do not include overnight returns, as they are influenced by external factors.

As an example, we present the $60$-minute frequency returns for Tesla (TSLA) from January 2018 to June 2022 in \autoref{fig:TSLA}. Using the LSTV$^{*}$(BV) estimator with $K_{\mathrm{max}}=20$, we find that although there are many jumps in the returns, the algorithm detects only $\widehat{K}=5$ change points. Moreover, the persistent regimes in the volatility are only altered by major events related to the company or the market, as shown by the estimates of volatility.


If there are no change points, the LSTV$^{}$(QV) and LSTV$^{}$(BV) estimators reduce to the original $QV$ and $BV$ estimators, respectively. In the forecasting exercises below, we set $K_{\mathrm{max}}=1$ and $\xi=0.03$. We find that allowing for one change point improves forecasting performance across all models, assets, and frequencies considered below. We also experimented with $K_{\mathrm{max}}>1$, but the forecasting results were more mixed and sensitive to the choice of the $\xi>0$ parameter. In general, user should decide on the appropriate choice of $K_{\mathrm{max}}$ and $\xi$ based on past market and asset regime changes. 


\begin{figure}
\begin{center}
\includegraphics[width=0.75\textwidth, height=0.5\textwidth]{images_final/TSLA_vol.jpg}

\caption{Tesla squared 60 minutes returns from January 2018 to August 2020 together with the estimated volatilities using LSTV$^{*}$(BV) with $K_{\mathrm{max}}= 20$. The estimator shows $5$ change points with different volatility regimes, especially from February 2020 to May 2020, i.e., during the early COVID-19 period.  \normalsize}
\vspace*{-4mm}
\label{fig:TSLA}
\end{center}
\end{figure}

To assess the forecasting ability of the proposed estimators, we compare them to various benchmarks. We conduct a rolling window exercise, in which we estimate our models and generate one-step-ahead forecasts for different frequencies of the data. Specifically, for a given window of data consisting of observations $r_{1,n}, r_{2,n}, \cdots, r_{t,n}$, we aim to forecast the spot volatility term at times $t+1$. The forecasts are calculated using the following formula
\begin{equation*}
    \sigma_{t\mid t+1,n} := \mathbb{E}(\sigma\left((t+1)/n\right)|\{r_{s,n}\}_{s=1}^t).
\end{equation*}
To calculate the forecasts, we extrapolate the last estimated volatility $\widehat{\sigma}(t/n)$ obtained from the corresponding model within a given rolling window to $t+1$, for our high-frequency estimators. In the case of GARCH($1,1$) dynamics, we use the available explicit one-step-ahead GARCH($1,1$) forecast. We consider the entire historical data available in the rolling window to calculate the estimates, and no additional tuning parameters are required for the forecasting process.


As benchmark for the performance, we consider a broad spectrum of different volatility estimators that are commonly used in practice. Starting from the classical QV and BV estimators introduced by \citet{NielsenShephard:04}, we also include more recent refinements, such as the realized volatility and bipower variation (RVB) and realized volatility and quarticity (RVQ) estimators proposed by \citet{Yu:20}. In addition, we consider the heavy-tailed $t$-GARCH model with Student-$t$ innovations, as well as its long-memory extension, the $t$-FIGARCH model, introduced by \citet{ShiHo15}. Finally, we include the mixed-frequency approach of the HAR model proposed by \citet{Corsi:09}. However, the HAR model can only serve as a benchmark for multi-step (daily) volatility forecasts.

In \autoref{fig:SmoothedComparison}, we compare rolling window-based one-step-ahead forecasted volatilities and log forecasted volatilities of Apple's 60-minute frequency returns using bipower variation, the $t$-GARCH(1,1), and LSTV$^*$(BV) models. The forecasted volatilities for the BV model exhibit a large number of spikes across consecutive rolling windows, while the $t$-GARCH(1,1) produces forecasts with some extreme spikes. By contrast, our proposed model provides much smoother and more realistic forecasts, as can be seen from their autocorrelation structure. To further smooth the forecasts, we can use a simple exponential-moving average filter. The bottom plots in \autoref{fig:SmoothedComparison} show the results of applying this filter, with the parameter $\lambda$ chosen to minimize the overall squared error.
\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{images_final/ForecastComparison.jpg}
\vspace*{-4mm}
\caption{One-step-ahead forecasts from the rolling window exercise for the Apple 60 min stock using 2 weeks of the rolling window from January 2018 to August 2020. Row wise, a comparison between BV, $t$-GARCH(1,1), LSTV$^*$(BV), and smoothed LSTV$^*$(BV) models. \textbf{First column:} forecasted volatility values. \textbf{Second column:} log of the volatilities. \textbf{Third column:} autocorrelation functions of the forecasted volatilities.
}
\vspace{-4mm}
\label{fig:SmoothedComparison}
\end{center}
\end{figure}


To compare the forecast performance of different models quantitatively in our rolling window exercise, we use the prediction Average Squared Error (ASE) and the prediction Average Absolute Error (AAE), following the approach of \citet{Fryzlewicz:06},
\begin{align}
 ASE_{tp,2,N} &= \frac{1}{N-tp-2} \sum_{t=2}^{N-fp}\left(\bar{\sigma}^2_{t+fp|t} -\bar{X}^2_{t+fp|t}\right)^2\label{eq:ASEdef} \\
AAE_{tp,2,N} &= \frac{1}{N-tp-2} \sum_{t=2}^{N-fp}\left|\bar{\sigma}^2_{t+fp|t} -\bar{X}^2_{t+fp|t}\right|.\label{eq:AAEdef}
\end{align}

In the left panel of \autoref{fig:GARCH}, we compare the relative performance of GARCH(1,1) and our proposed method LSTV$^*$(BV) to QV in terms of the percentage improvement in error. Our method shows a $10\%$ to $20\%$ improvement, while the $t$-GARCH(1,1) model exhibits a $30\%$ to $70\%$ worse performance than QV.


We extend our comparison analysis to include more recent log-memory variations of GARCH models. We use the FIGARCH model proposed by \citet{ShiHo15} as a benchmark and introduce one more variation of our estimator by using the estimated volatility from LSTV$^*$(QV) as innovations for the FIGARCH algorithm. This is referred to as LSTV$^*$(FIGARCH). In \autoref{fig:GARCH}, we compare the relative performance of these three methods with respect to BV in terms of the percentage improvement in error. We find that both FIGARCH and LSTV$^*$(FIGARCH) perform worse than the benchmark BV, while LSTV$^*$(BV) outperforms all other methods across all frequencies of the data.
\begin{figure}
    \centering

    {{\includegraphics[width=.49\linewidth]{images_final/LSTVvstGARCHAAE.jpg} }}\hfill
    {{\includegraphics[width=.49\linewidth]{images_final/LSTVFIGARCH.jpg} }}%
\caption{Percentage difference of one-step-ahead volatility forecasts in terms of predicted ASE from \eqref{eq:ASEdef}.  \textbf{Left Panel:} LSTV$^*$(BV) and $t$-GARCH(1,1) models relative to the QV model. \textbf{Right Panel:} FIGARCH, LSTV$^*$(BV), and LSTV$^*$(FIGARCH) models relative to the BV model.}
    \label{fig:GARCH}
\end{figure}

In the following analysis, we compare the forecasting ability of our proposed method to the classical QV and BV estimators by \citet{NielsenShephard:04}, as well as the more recent refinements of them, including the realized volatility and bipower variation (RVB) and realized volatility and quarticity (RVQ) estimators proposed by \citet{Yu:20}. In \autoref{fig:ASE}, we present the Average Squared Error (ASE) of these estimators.

Our LSTV$^*$(QV) method outperforms the underlying QV estimator, with a significant improvement observed across all frequencies. Furthermore, our LSTV$^*$(BV) method also outperforms the standard BV estimator across all frequencies, which is a significant result in the context of one-step-ahead high-frequency volatility prediction.

The RVB and RVQ estimators are more efficient in terms of asymptotic variance, and thus exhibit better out-of-sample performance than QV, but still worse than BV and our LSTV$^*$(BV).

\begin{figure}
\begin{center}
\includegraphics[width=1\textwidth]{images_final/OneStepAheadPlot.jpg}
\vspace*{-2mm}
\caption{One-step-ahead volatility forecast errors for different volatility estimators (QV, BV, RVQ, RVB, LSTV$^*$(QV), and LSTV$^*$(BV)) and for different returns frequencies (1 min, 5 min, 15 min, 30 min, and 60 min). \textbf{Top Panel:} Average Squared Error (ASE) as in \eqref{eq:ASEdef}. \textbf{Bottom Panel:} Average Absolute Error(AAE) as in \eqref{eq:AAEdef}.}
\vspace*{-1mm}
\label{fig:ASE}
\end{center}
\end{figure}
Daily volatility forecasts are more pragmatic and widely used in trading and portfolio optimization. Therefore, we also compare one-day-ahead volatility forecasts in \autoref{fig:onedayahead}. In addition to the benchmarks used in \autoref{fig:ASE}, we also include the HAR-RV method proposed by \citet{Corsi:09} in our comparison. Using a 2-week rolling window, we forecast the next-day volatility using different data frequencies. Our proposed method substantially outperforms all benchmarks, including the one generated by the HAR-RV model, in the one-day-ahead forecast.

\begin{figure}
\begin{center}
\includegraphics[width=1\textwidth]{images_final/OneDayAheadPlot.jpg}
\vspace*{-4mm}
\vspace{-4mm}
\caption{One-day-ahead volatility forecast errors for different volatility estimators (QV, BV, RVQ, RVB, LSTV$^*$(QV), and LSTV$^*$(BV)) and for different returns frequencies (1 min, 5 min, 15 min, 30 min, and 60 min). \textbf{Top Panel:} Average Squared Error (ASE) as in \eqref{eq:ASEdef}. \textbf{Bottom Panel:} Average Absolute Error(AAE) as in \eqref{eq:AAEdef}.}
\label{fig:onedayahead}
\end{center}
\end{figure}

\section{Extensions and Further Research}\label{sec:Extensions}

In addition to their performance in high-frequency volatility estimation, our proposed volatility estimators can be extended to capture other stylized facts of high-frequency volatility, demonstrating their versatility and practical applicability. For example, one can filter jumps from the stock prices separately from the volatility estimation by adding an additional $\ell_1$ regularization term to the objective function in \eqref{eq:objective}.

Similarly, intraday or weekly seasonality, a well-known statistical characteristic of high-frequency stock volatility, can be incorporated into our $\ell_1$ volatility filter. One approach for incorporating seasonality is to follow \citet{vatter2015non} and incorporate smooth trend and seasonality into the framework. To capture both effects, the objective function \eqref{eq:objective} can be modified as follows:
$$
\begin{aligned}
 \dfrac{1}{n}\norm{F(\mathbf{r}_t)-\mathbf{X}\boldsymbol\beta - \mathbf{s} - \mathbf{v}}^2_2 + \lambda\norm{\boldsymbol\beta}_1 + \gamma\norm{\mathbf{v}}_1\\
\textrm{s.t.} \quad  s_{t+p} = s_t  \quad   t=1,\ldots,n-p,
\end{aligned}
$$
where $\mathbf{s}=(s_1,\ldots,s_n)'\in \mathbb{R}^n$ is the seasonal periodic component with known period $p$, vector $\mathbf{v} \in \mathbb{R}^n$ is the sparse spikes component, and the parameter $\gamma \geq 0$ controls jumps intensity. 

If the exact seasonality pattern needs to be estimated from the data one can use
$$
\begin{aligned}
 &\dfrac{1}{n}\norm{F(\mathbf{r}_t)-\mathbf{X}\boldsymbol\beta - \mathbf{s} - \mathbf{v}}^2_2  + \lambda\norm{\boldsymbol\beta}_1 + \gamma\norm{\mathbf{v}}_1\\
 & \quad \quad \quad   \quad \quad \ \quad \quad \quad  \quad \quad \quad +\sum_{\omega\in \Omega}\left(\left|a_\omega\right|+\left|b_\omega\right|\right)\\
&\textrm{s.t.} \quad  s_{t} = \sum_{\omega \in \Omega}a_\omega \sin(\omega t) + b_{\omega} \cos(\omega t)\quad t=1,\ldots,n. 
\end{aligned}
$$
The $\mathbf{s}=(s_1,\ldots,s_n)'\in \mathbb{R}^n$ filter is responsible for selecting the best frequencies from an over-complete dictionary $\Omega$. In this case, the frequency does not need to be known a priori, and the penalty term discourages the use of the dictionary in a greedy way. For an adaptive version of this filter and a fast coordinate descent algorithm for estimation, see \citet{SoutoGarcia:16}.

Secondly, one of the assumptions in our analysis so far is that the stock returns are equally spaced because the data used in Section \ref{sec:Empirics} consists of TAQ data aggregated to 1, 5, 10, 30, and 60-minute intervals. We can extend our method to tick-level data, which is irregularly spaced. First, consider the estimation of the underlying continuous volatility from a finite number of data points. Equation \eqref{eq:objective} can be extended to a continuous form as follows:
\begin{equation*}
 \dfrac{1}{n}\sum_{i=1}^n (F(\mathbf{r}_t)_i - x_i)^2 + \lambda \int_{t_1}^{t_n} \abs{\dot{x}}\mbox{d}t. 
\end{equation*}
This can be simplified by using the standard interpolation problem as explained in \cite{KimBoyd:08}. In particular, we can solve
\begin{equation}
 \dfrac{1}{n}\sum_{i=1}^n \left(F(\mathbf{r}_t)_{t_i} - x_{t_i}\right)^2 + \lambda \sum_{i=2}^n \abs{\frac{x_{t_{i+1}}-x_{t_i}}{t_{i+1}-t_i}}.
\end{equation}
Using the optimal points $x_{t_i}$, $i=1,\ldots,n$, we can recover the solution to the original piecewise linear trend filtering problem: the piecewise-linear function, $x^*_t$, for $x^*_{t_i}$, given by
$$x^*_t = \frac{t_{i+1}-t}{t_{i+1}-t_{i}} x^*_{i},$$
where $i=1,\ldots,n$, is the optimal piecewise constant volatility. To reflect the non-equally spaced data, one can modify the proposed LSTV$^*$ algorithm by modifying the matrix $\mathbf{X}$ to include the time increments from the regularization term. Alternatively, one can use the modified version of the primal-dual algorithm described in \citet{KimBoyd:08}.

In this paper, we assumed that the observed returns are the true returns. In ultra-high-frequency data, one needs to accommodate the assumption of microstructure noise. We can do that by using a realized kernel estimator $F_\mathfrak{K}(\mathbf{r}_t)$, where $\mathfrak{K}$ stands for the specific kernel function (see \citealp{shephard-designing-realised-kernels-2008, shephard-realised-kernels-2009, shephard-multivariate-realised-kernels-2011, shephard-subsampling-realised-kernels-2011}, and \citealp{shephard-realized-kernels-2015}). Indeed, if we have
$$p(t) = p^{*}(t) + \epsilon(t),$$
where $p(t)$ denotes the observed price at $t$, $p^{*}(t)$ denotes the true price at $t$, and $\epsilon$ denotes the micro-structure noise; then the returns satisfy
$$r_t = r^{*}_t + u_t,$$
where $u_t = \epsilon(t) - \epsilon(t-1)$, and one can construct a kernel-based estimator $F_\mathfrak{K}$ that satisfies
$$\plim_{n \rightarrow +\infty} F_\mathfrak{K}(\mathbf{r}_t) = [\mathbf{r}^{*}_t] \text{ and } \plim_{n \rightarrow +\infty} = F_\mathfrak{K}(\mathbf{u}_t) = 0,$$
where $\mathbf{r}_t$ is the vector of high frequency returns; as before, $[\mathbf{r}^{*}_t]$ is the quadratic variation of the true returns; and $\mathbf{u}_t = (u_1, \cdots, u_n)$.

Finally, the two estimators proposed in this paper allow constructing an estimator of the jump component. Namely, one can take the differences  between increments of the QV and BV estimator and plug them into the objective function with $\ell_1$ regularization not on the difference but on the series itself, i.e.,
\begin{align}
\widehat{\mathbf{u}}(\lambda) &:= \argmin_{\mathbf{u} \in \mathbb{R}^{n-1}} \mathcal{L}(\mathbf{u}) 
\label{eq:objective_feasible_jumps}\\ 
\mbox{where }\mathcal{L}(\mathbf{u}) &:= \frac{1}{n }\norm{\tilde{F}_{\text{RV}}(\mathbf{r}_t) - F_{\text{BP}}(\mathbf{r}_t) -\mathbf{u}}^2_{2} + \lambda\norm{\mathbf{u}}_{1}, \nonumber
\end{align}
where $\tilde{F}_{\text{RV}}$ is the original ${F}_{\text{RV}}$ with the first element $r_{1,t}^2$ in the vector removed.
The $\widehat{\mathbf{u}}(\lambda)$  in \eqref{eq:objective_feasible_jumps} estimates the jump component of the price process. They can be obtained via the original LARS or any other LASSO related algorithm. Since $\tilde{F}_{\text{RV}}(\mathbf{r}_t) - F_{\text{BP}}(\mathbf{r}_t)$ satisfies our consistency condition for the jump component, the results in this paper also show that $\widehat{\mathbf{u}}(\lambda)$ will be consistent for the true jump sizes and their locations.

\section{Conclusions} \label{sec:Conclusion}

In this paper, we have introduced a new family of high-frequency volatility estimators. By leveraging the LSTV change point detection framework and adding $\ell_1$ regularization to two classical estimators (quadratic variation and bipower variation), we have proposed estimators that are consistent for volatility estimation and breakpoint detection. Simulation results demonstrate that the proposed estimators accurately identify breakpoints and outperform classical and recent high-frequency volatility estimators in terms of out-of-sample volatility prediction at all frequencies and forecasting horizons.

Our proposed estimators offer practical advantages, such as their ability to accurately identify breakpoints close to the end of the sample and their ability to provide more realistic and smoother volatility forecasts. In future research, we plan to extend the proposed estimators to the multivariate case and incorporate other stylized facts of (ultra) high-frequency financial returns using the extensions discussed in Section \ref{sec:Extensions}.

Overall, our proposed estimators have significant potential for improving high-frequency volatility estimation and forecasting. We hope that our work will inspire further research in this area and lead to practical applications in electronic trading, portfolio optimization, and risk management.

\bibliographystyle{apalike}
\setcitestyle{round,authoryear,semicolon}
\small
\bibliography{JournalMain}

\newpage

\appendix
\onecolumn

\section{Sub-Weibull random variables}\label{appendix:subweibull}
A random variable $X$ with sub-Weibull distribution with tail parameter $\alpha$ is one such that

$$\forall t > 0 : \mathbb{P}\left(\abs{X-\mathbb{E}[X]} \geq t\right) \leq 2\exp\left(-(t/C)^{\alpha}\right)$$
for some $\alpha, C > 0$.
Particularly, the parameters $\alpha = 1$ and $\alpha = 2$ correspond to sub-exponential and sub-Gaussian random variables, respectively. This class of heavy-tailed distributions (and a corresponding notion for random vectors) has recently been defined and studied in \citep{kuchibhotla2020moving}. (See also \citealp{zhang-subweibull-2022}.) Concentration inequalities for such random variables (and the corresponding random vectors) have also been recently derived in \citep{gotze-sambale-alpha-concentraton-2021} and \citep{sambale2020notes}. Many of these results have been extended to a larger class of heavy-tailed distributions in \citep{bakhshizadeh2020sharp}.\\

We recall a couple of properties of sub-Weibull distributions. 
Note that an equivalent definition of the class $\mathrm{subWeib}(\alpha)$ can be stated in terms of Orlicz-(quasi-)norms. Indeed, $X \in \mathrm{subWeib}(\alpha)$ if and only if
$$\norm{X}_{\Psi_\alpha} := \inf\left\{\eta > 0 : \mathbb{E}\left[\exp\left(\left(X/\eta\right)^{\alpha}\right)\right] \leq 2\right\}$$

is finite. Here, $\Psi_\alpha$ denotes the Orlicz function $s \mapsto \exp(s^{\alpha})-1$, $\alpha > 0$.\\

Note that in general, the constant $C$ in the tail bound definition of a sub-Weibull random variable $X$ and $\norm{X}_{\Psi_{\alpha}}$ are proportional to each other by a positive factor depending only on $\alpha$. (See \citep{sambale2020notes} and \citep{gotze-sambale-alpha-concentraton-2021}.)

\begin{proposition}\emph{\citep[Proposition 2.3]{VladimirovaMariia:20}}
Let $X_1$ and $X_2$ be sub-Weibull random variables with tail parameters $\alpha_1$ and $\alpha_2$, respectively. Then, $X_1 X_2$ and $X_1 + X_2$ are sub-Weibull with respective tail parameters $\alpha_1 + \alpha_2$ and $\max(\alpha_1, \alpha_2)$.
\end{proposition}

\begin{proposition}\label{prop-subweibull-concentration}
Let the random variables $X_1,\cdots, X_n \in \mathrm{subWeib}(\alpha)$, with $\alpha > 0$, be independent, and suppose that $\norm{X_i}_{\Psi_{\alpha}} \leq M$ for all $i = 1, \cdots, n$. Then, for all $n \geq 1$ and $t > 0$, we have
$$\mathbb{P}\left(\dfrac{1}{n}\abs{\sum_{i=1}^n \left(X_i-\mathbb{E}(X_i)\right)} \geq t\right) \leq 2\exp\left(-\dfrac{1}{C_{\alpha}}\cdot\dfrac{t^{\alpha} n^{\alpha/2}}{M^{\alpha}}\right)$$
where $C_{\alpha} > 0$ is a universal constant depending only on $\alpha$.
\end{proposition}

This follows from the proof of \citep[Lemma 5.2]{sambale2020notes}.\\

As a contribution to this existing literature, we prove the following result on random sums of sub-Weibull random variables.

\begin{proposition}\label{proposition:random-sums-subweibull}
Let $Z_1, Z_2, \cdots$ be a sequence of i.i.d. sub-Weibull random variables with decay parameter $\alpha > 0$ and common sub-Weibull norm $\norm{Z}_{\Psi_{\alpha}}$, with $Z$ denoting the common distribution of the $Z_j$'s. Suppose in addition that $W$ is independent of the $Z_j$'s. Then the sum $\sum_{j = 1}^W Z_j$ is also sub-Weibull with decay parameter $\alpha$ and sub-Weibull norm $\mathbb{E}\left(W^{1+A(\alpha)}\right)\norm{Z}_{\Psi_{\alpha}}$ for some $A(\alpha)$ such that $-\max\left\{1,1/\alpha\right\} \leq A(\alpha) \leq 1/\alpha + \abs{1-1/\alpha}$.
\end{proposition}

\begin{proof}
By the law of total expectation, we have:
$$\norm{\sum_{j = 1}^W Z_j}_{\Psi_{\alpha}} = \mathbb{E}\left(\norm{\sum_{j = 1}^W Z_j}_{\Psi_{\alpha}}\right) = \mathbb{E}\left(\mathbb{E}\left(\norm{\sum_{j = 1}^W Z_j}_{\Psi_{\alpha}} \Bigg\vert W\right)\right).$$

Now, in order to compute $\mathbb{E}\left(\norm{\sum_{j = 1}^W Z_j}_{\Psi_{\alpha}} \Bigg\vert W\right)$, we compute the sub-Weibull norm of a deterministic sum of sub-Weibull random variables.\\

By the concavity of $x \mapsto \abs{x}^{\alpha}$ for $\alpha \in (0,1)$ and its convexity for $\alpha \geq 1$, we have the following for any $a_1, \cdots, a_m \geq 0$:
$$\min\left\{1, m^{\alpha-1}\right\}\sum_{j = 1}^m a_j^{\alpha} \leq \left(\sum_{j = 1}^m a_j\right)^{\alpha} \leq \max\left\{1, m^{\alpha-1}\right\}\sum_{j = 1}^m a_j^{\alpha}.$$

On the one hand, 
\begin{align*}
\mathbb{E}\left[\exp\left(\dfrac{\abs{\sum_{j = 1}^m Z_j}^{\alpha}}{m\cdot \dfrac{\max\left\{1, m^{\alpha-1}\right\}}{\min\left\{1, m^{\alpha-1}\right\}}\left(\sum_{j = 1}^m \norm{Z_j}_{\Psi_{\alpha}}\right)^{\alpha}}\right)\right] &\leq \mathbb{E}\left[\exp\left(\dfrac{\max\left\{1, m^{\alpha-1}\right\}\sum_{j = 1}^m \abs{Z_j}^{\alpha}}{m\cdot \max\left\{1, m^{\alpha-1}\right\}\sum_{j = 1}^m \norm{Z_j}^{\alpha}_{\Psi_{\alpha}}}\right)\right]\\
&\leq \mathbb{E}\left[\left(\prod_{j = 1}^m \left(\exp\left(\dfrac{\abs{Z_j}^{\alpha}}{\norm{Z_j}^{\alpha}_{\Psi_{\alpha}}}\right)\right)^{1/m}\right)\right]\\
&\leq \mathbb{E}\left(\dfrac{1}{m}\sum_{j = 1}^m \exp\left(\dfrac{\abs{Z_j}^{\alpha}}{\norm{Z_j}^{\alpha}_{\Psi_{\alpha}}}\right)\right)\\
&= \dfrac{1}{m} \mathbb{E}\left(\exp\left(\dfrac{\abs{Z_j}^{\alpha}}{\norm{Z_j}^{\alpha}_{\Psi_{\alpha}}}\right)\right) = \dfrac{1}{m}\cdot(2\cdot m) = 2.
\end{align*}

The third inequality follows from the AM-GM inequality while the next step follows from the definition of the sub-Weibull norm. Therefore,
$$\norm{\sum_{j = 1}^m Z_j}_{\Psi_{\alpha}} \leq \left(m\cdot \dfrac{\max\left\{1, m^{\alpha-1}\right\}}{\min\left\{1, m^{\alpha-1}\right\}}\right)^{1/\alpha}\sum_{j = 1}^m \norm{Z_j}_{\Psi_{\alpha}}.$$

On the other hand,
\begin{align*}
\mathbb{E}\left[\exp\left(\dfrac{\abs{Z_j}^{\alpha}}{\dfrac{1}{\min\left\{1,m^{\alpha-1}\right\}}\norm{\sum_{j = 1}^m Z_j}^{\alpha}_{\Psi_{\alpha}}}\right)\right] &\leq \mathbb{E}\left[\exp\left(\dfrac{\dfrac{1}{\min\left\{1,m^{\alpha-1}\right\}}\abs{\sum_{j = 1}^m Z_j}^{\alpha}}{\dfrac{1}{\min\left\{1,m^{\alpha-1}\right\}}\norm{\sum_{j = 1}^m Z_j}^{\alpha}_{\Psi_{\alpha}}}\right)\right]\\
&= \mathbb{E}\left[\exp\left(\dfrac{\abs{\sum_{j = 1}^m Z_j}^{\alpha}}{\norm{\sum_{j = 1}^m Z_j}^{\alpha}_{\Psi_{\alpha}}}\right)\right] \leq 2.
\end{align*}

Therefore, for each $j = 1, \cdots, m$,
$$\norm{Z_j}_{\Psi_{\alpha}} \leq \left(\dfrac{1}{\min\left\{1,m^{\alpha-1}\right\}}\right)^{1/\alpha}\norm{\sum_{j = 1}^m Z_j}_{\Psi_{\alpha}},$$
so that
$$\left(\dfrac{\min\left\{1,m^{\alpha-1}\right\}}{m^{\alpha}}\right)^{1/\alpha}\sum_{j = 1}^m \norm{Z_j}_{\Psi_{\alpha}} \leq \norm{\sum_{j = 1}^m Z_j}_{\Psi_{\alpha}}.$$

In conclusion,
$$m^{-\max\{1, 1/\alpha\}}\sum_{j = 1}^m \norm{Z_j}_{\Psi_{\alpha}} \leq \norm{\sum_{j = 1}^m Z_j}_{\Psi_{\alpha}} \leq m^{1/\alpha+\abs{1-1/\alpha}}\sum_{j = 1}^m \norm{Z_j}_{\Psi_{\alpha}}.$$

Therefore, 
$$\norm{\sum_{j = 1}^m Z_j}_{\Psi_{\alpha}} = m^{A(\alpha)}\sum_{j = 1}^m \norm{Z_j}_{\Psi_{\alpha}},$$
for some exponent $A(\alpha)$ such that $-\max\left\{1,1/\alpha\right\} \leq A(\alpha) \leq 1/\alpha + \abs{1-1/\alpha}$.\\

The latter implies that
$$\mathbb{E}\left(\norm{\sum_{j = 1}^W Z_j}_{\Psi_{\alpha}} \Bigg\vert W\right) = \mathbb{E}\left(W^{A(\alpha)}\sum_{j = 1}^W \norm{Z_j}_{\Psi_{\alpha}} \Bigg\vert W\right) = \mathbb{E}\left(W^{A(\alpha)}W \norm{Z}_{\Psi_{\alpha}}\right) = \mathbb{E}\left(W^{1+A(\alpha)}\right)\norm{Z}_{\Psi_{\alpha}}.$$

Ultimately, we conclude that $\sum_{j = 1}^W Z_j$ is sub-Weibull with decay parameter $\alpha$ with sub-Weibull norm $\mathbb{E}\left(W^{1+A(\alpha)}\right)\norm{Z}_{\Psi_{\alpha}}$.
\end{proof}

\section{Technical Lemmas}\label{appendix:lemmas}
\begin{lemma}\label{lemma:KKT-change points}
Let $\left(\widehat{t}_1, \cdots, \widehat{t}_n\right)$ denote the change point estimates defined in \ref{definition:change point-estimates}. Then $\left(\widehat{t}_1, \cdots, \widehat{t}_n\right)$ and $\left(\widehat{u}_1, \cdots, \widehat{u}_n\right)$ satisfy
\begin{equation}
\forall \ell \in \left\{1, \cdots, \abs{\widehat{\mathcal{A}}(\lambda)}\right\}: \sum_{i = \widehat{t}_{\ell}}^n F(\mathbf{r}_t)_i - \sum_{i = \widehat{t}_{\ell}}^n \widehat{u}_i = \dfrac{n \lambda}{2}\mathrm{sign}\left(\widehat{u}_{\widehat{t}_\ell(\lambda)} - \widehat{u}_{\widehat{t}_\ell(\lambda)-1}\right), \text{ and }
\end{equation}
\begin{equation}
\forall j \in \left\{1, \cdots, n\right\} : \abs{\sum_{i = j}^n F(\mathbf{r}_t)_i - \sum_{i = j}^n \widehat{u}_i} \leq \dfrac{n\lambda}{2}.
\end{equation}
The vector $\left(\widehat{u}_{1}(\lambda),\cdots, \widehat{u}_{n}(\lambda)\right)$ satisfies:
\begin{equation}
    \widehat{u}_j(\lambda) = \widehat{\eta}_k \text{ for } \widehat{t}_{k-1}(\lambda) \leq j \leq \widehat{t}_{k}(\lambda)-1,
\end{equation}
for some constant values $\widehat{\eta}_1, \cdots, \widehat{\eta}_{\widehat{K}}$, where $\widehat{K} \geq 1$.
\end{lemma}

For a proof, see \citep{HarchaouiLeduc:10}. The proof simply relies on a minimization criterion based on sub-gradients for each one of the objective functions.

In what follows, let $\mathbf{Z}[r,s] := \sum_{i = r}^s Z_i$ for any random vector $\mathbf{Z} = (Z_1, \cdots, Z_n)$.

\begin{lemma}\label{lemma:partial-sum-noise-variables}
Let $\varepsilon_{J,1,t}, \cdots, \varepsilon_{J,n,t}$ be $\mathrm{subWeib}(\alpha)$ jump-corruption variables previously defined. If $\{\zeta_n\}_{n \geq 1}$ and $\{\mu_n\}_{n \geq 1}$ are two positive sequences such that $\mu^{\alpha}_n \zeta_n^{\alpha}\left[\log(n)\right]^{-1} \xrightarrow[n \to +\infty]{} +\infty$, then
$$\mathbb{P}\left(\sup_{\substack{1 \leq r_n < s_n \leq n \\ \abs{r_n-s_n} \geq \zeta_n } } \abs{\dfrac{\mathfrak{E}_{J,t}[r_n,s_n-1]}{s_n-r_n}} \geq \mu_n\right) \xrightarrow[n \to +\infty]{} 0.$$
\end{lemma}

\begin{proof}
Clearly,
$$\mathbb{P}\left(\sup_{\substack{1 \leq r_n < s_n \leq n \\ \abs{r_n-s_n} \geq \zeta_n }} \abs{\dfrac{\mathfrak{E}_{J,t}[r_n,s_n-1]}{s_n-r_n}} \geq \mu_n\right) \leq \sum_{\substack{1 \leq r_n < s_n \leq n \\ \abs{r_n-s_n} \geq \zeta_n }} \mathbb{P}\left(\abs{\dfrac{\mathfrak{E}_{J,t}[r_n,s_n-1]}{s_n-r_n}} \geq \mu_n\right).$$

By Proposition \ref{proposition:random-sums-subweibull}, we have:
\begin{align*}
    \mathbb{P}\left(\abs{\dfrac{\mathfrak{E}_{J,t}[r_n,s_n-1]}{s_n-r_n}} \geq \mu_n\right) &= \mathbb{P}\left(\abs{\sum_{i = r_n}^{s_n - 1} \varepsilon_{J,i,t}} \geq \mu_n\abs{s_n-r_n}\right),\\
    &= \chi_J(1-\chi_R)\mathbb{P}\left(\abs{\sum_{j = \mathfrak{X}_{\tau_{r_n-1,t}}+1}^{\mathfrak{X}_{\tau_{s_n-1,t}}} Q^2_j} \geq \mu_n\abs{s_n-r_n}\right)\\
    &= \chi_J(1-\chi_R)\mathbb{P}\left(\abs{\sum_{j = 1}^{\mathfrak{X}_{\tau_{s_n-1,t}}} Q^2_j - \sum_{j = 1}^{\mathfrak{X}_{\tau_{r_n-1,t}}} Q^2_j} \geq \mu_n\abs{s_n-r_n}\right)\\
    &\leq \chi_J(1-\chi_R)\mathbb{P}\left(\abs{\sum_{j = 1}^{\mathfrak{X}_{\tau_{s_n-1,t}}} Q^2_j} \geq \dfrac{1}{2} \mu_n\abs{s_n-r_n}\right)\\
    &\quad + \chi_J(1-\chi_R)\mathbb{P}\left(\abs{\sum_{j = 1}^{\mathfrak{X}_{\tau_{r_n-1,t}}} Q^2_j} \geq \dfrac{1}{2} \mu_n\abs{s_n-r_n}\right)\\
    &\leq 4\chi_J(1-\chi_R) \exp\left(-\dfrac{\mu_n^{\alpha}\abs{s_n-r_n}^{\alpha}}{\mathbb{E}\left[\mathfrak{X}_t^{1+A(\alpha/2)}\right]^{\alpha/2}\norm{Q}^{\alpha}_{\Psi_{\alpha}}}\right),
\end{align*}
where $C$ is the constant defined in Proposition \ref{proposition:random-sums-subweibull}. Summing over $1 < r_n < s_n \leq n$ with $\abs{r_n-s_n} \geq \zeta_n$, we obtain
\begin{align*}
    \mathbb{P}\left(\sup_{\substack{1 \leq r_n < s_n \leq n \\ \abs{r_n-s_n} \geq \zeta_n }} \abs{\dfrac{\mathfrak{E}_{J,t}[r_n,s_n-1]}{s_n-r_n}} \geq \mu_n\right) &\leq 4\left(\dfrac{n(n-1)}{2}\right)\exp\left(-\dfrac{\mu_n^{\alpha}\zeta^{\alpha}_n}{\mathbb{E}\left[\mathfrak{X}_t^{1+A(\alpha/2)}\right]^{\alpha/2}\norm{Q}^{\alpha}_{\Psi_{\alpha}}}\right)\\
    &\leq 2 \exp\left[2\log(n)\left(1-\dfrac{1}{\mathbb{E}\left[\mathfrak{X}_t^{1+A(\alpha/2)}\right]^{\alpha/2}\norm{Q}^{\alpha}_{\Psi_{\alpha}}}\cdot\dfrac{\mu^{\alpha}_n \zeta_n^{\alpha}}{\log(n)}\right)\right],
\end{align*}
and the latter upper bound converges to $0$ as $\mu^{\alpha}_n \zeta_n^{\alpha}\left[\log(n)\right]^{-1} \xrightarrow[n \to +\infty]{} +\infty$.
\end{proof}

\section{Proofs of the Results}\label{app:Proofs}

In this section, we present the proofs of the results outlined in the paper.

\begin{proof}[Proof of Proposition \ref{proposition:mse-consistency}]
The consistency condition \eqref{volatility-consistency-condition} implies
$$\plim_{n \rightarrow +\infty} \left(\sum_{i = 1}^n (F(\mathbf{r}_t)_i - y_{t,i})\right)^2 = 0,$$
which in turn implies
$$\plim_{n \to +\infty} \dfrac{1}{n}\sum_{k = 1}^n \abs{\sum_{i = 1}^k (F(\mathbf{r}_t)_i-y_{t,i})}^2 = 0.$$
In particular, denoting $\Delta_i := F(\mathbf{r}_t)_i - y_{t,i}$ and $S_i := \abs{\sum_{k = 1}^i \Delta_k}^2$, we write:
$$\plim_{n \rightarrow +\infty} \dfrac{1}{n}\sum_{i = 1}^n S_i = 0.$$

Using this, we can show that our estimator is consistent with respect mean squared error. First, let us expand the sum of squares:
$$\sum_{i = 1}^n \Delta^2_i = \left(\sum_{i = 1}^n \Delta_i\right)^2 - \sum_{1 \leq k < i \leq n} 2\Delta_i \Delta_{k}.$$
Second, using Young's inequality, we can bound the sum of products in the following manner, for any $\gamma > 0$:
\begin{align*}
\abs{\sum_{1 \leq k < i \leq n} 2\Delta_i \Delta_{k}} = \abs{\sum_{i = 1}^n \sum_{k = 1}^{i-1} 2 \Delta_i \Delta_{k}} = \abs{\sum_{i = 1}^n 2 \Delta_i\left( \sum_{k = 1}^{i-1} \Delta_{k}\right)} &\leq \sum_{i = 1}^n \abs{2 \Delta_i\left( \sum_{k = 1}^{i-1} \Delta_{k}\right)}\\
&\leq \sum_{i = 1}^n \left[\gamma \Delta^2_i + \dfrac{1}{\gamma} \left(\sum_{k = 1}^{i-1} \Delta_{k}\right)^2\right].
\end{align*}

Therefore, for any $\gamma \in (0,1)$ in particular, we have:
$$\sum_{i = 1}^n \Delta^2_i \leq S_n + \gamma\sum_{i = 1}^n \Delta^2_i + \dfrac{1}{\gamma}\sum_{i = 0}^{n-1} S_i.$$

But since $S_0 = 0$ and $1/\gamma > 1$, it follows that
$$(1-\gamma)\sum_{i = 1}^n \Delta^2_i \leq \dfrac{1}{\gamma} \sum_{i = 1}^n S_i,$$
and so after dividing by $1-\gamma$ and minimizing over $\gamma$, we conclude that
$$\dfrac{1}{n}\sum_{i = 1}^n \Delta^2_i \leq 4\left(\dfrac{1}{n}\sum_{i = 1}^n S_i\right).$$

It thus follows that
$$\plim_{n \rightarrow +\infty} \left(\dfrac{1}{n}\sum_{i = 1}^n (F(\mathbf{r}_t)_i - y_{t,i})^2\right) = 0.$$
\end{proof}

\begin{proof}[Proof of Proposition \ref{proposition:consistency-lasso-observed-true-latent}]
By the definition of $\widehat{\boldsymbol\beta}$,
$$\dfrac{1}{n}\norm{F(\mathbf{r}_t)-\mathbf{X}\widehat{\boldsymbol\beta}}^2_2 + \lambda\norm{\widehat{\boldsymbol\beta}}_1 \leq \dfrac{1}{n}\norm{F(\mathbf{r}_t)-\mathbf{X}\boldsymbol\beta^{*}_{\sigma}}^2_2 + \lambda\norm{\boldsymbol\beta^{*}_{\sigma}}_1.$$

We thus have:
$$\dfrac{1}{n}\norm{\mathbf{X}\left(\widehat{\boldsymbol\beta}-\boldsymbol\beta^{*}_{\sigma}\right)}^2_2 + \dfrac{2}{n}\left\langle F(\mathbf{r}_t)-\mathbf{X}\boldsymbol\beta^{*}_{\sigma},\mathbf{X}\left(\widehat{\boldsymbol\beta}-\boldsymbol\beta^{*}_{\sigma}\right)\right\rangle_2 \leq \lambda\left(\norm{\boldsymbol\beta^{*}_{\sigma}}_1 - \norm{\widehat{\boldsymbol\beta}}_1\right).$$
Using the fact that $\norm{\cdot}_1 \leq \sqrt{n}\norm{\cdot}_2$, Young's inequality, we see that for any $\gamma > 0$,
$$(1-\gamma)\left(\dfrac{1}{n}\norm{\mathbf{X}\left(\widehat{\boldsymbol\beta}-\boldsymbol\beta^{*}_{\sigma}\right)}^2_2\right) - \dfrac{1}{\gamma}\left(\dfrac{1}{n}\norm{F(\mathbf{r}_t)-\mathbf{X}\boldsymbol\beta^{*}_{\sigma}}^2_2\right) \leq \lambda\sqrt{n}\norm{\mathbf{X}\left(\widehat{\boldsymbol\beta}-\boldsymbol\beta^{*}_{\sigma}\right)}_2.$$

Using Young's inequality again, we obtain that for any $\kappa > 0$,
$$
(1-\gamma-\kappa)\left(\dfrac{1}{n}\norm{\mathbf{X}\left(\widehat{\boldsymbol\beta}-\boldsymbol\beta^{*}_{\sigma}\right)}^2_2\right) \leq \dfrac{1}{\gamma}\left(\dfrac{1}{n}\norm{F(\mathbf{r}_t)-\mathbf{X}\boldsymbol\beta^{*}_{\sigma}}^2_2\right) + \dfrac{1}{\kappa}\lambda^2n^2.
$$

Using the Cauchy-Schwarz inequality, and using the fact that $\norm{\cdot}_2 \leq \norm{\cdot}_1$, it follows that that for any $\eta > 0$:
$$\dfrac{1}{n}\norm{F(\mathbf{r}_t)-\mathbf{X}\boldsymbol\beta^{*}_{\sigma}}^2_2 \leq \left(1+\dfrac{1}{\eta}\right)\left(\dfrac{1}{n}\norm{F(\mathbf{r}_t)-\mathbf{Y}_t}^2_2\right) + (1+\eta)\left(\chi_J(1-\chi_R)\dfrac{1}{n}\sum_{i = 1}^n \varepsilon_{J,i,t}\right)^2.$$

Therefore, for any $\eta > 0$:
\begin{align*}
(1-\gamma-\kappa)&\left(\dfrac{1}{n}\norm{\mathbf{X}\left(\widehat{\boldsymbol\beta}-\boldsymbol\beta^{*}_{\sigma}\right)}^2_2\right)\\
&\leq \dfrac{1}{\gamma}\left[\left(1+\dfrac{1}{\eta}\right)\left(\dfrac{1}{n}\norm{F(\mathbf{r}_t)-\mathbf{Y}_t}^2_2\right) + (1+\eta)\left(\chi_J(1-\chi_R)\dfrac{1}{n}\sum_{i = 1}^n \varepsilon_{J,i,t}\right)^2\right] + \dfrac{1}{\kappa}\lambda^2n^2
\end{align*}

Now let $\kappa := A \cdot \gamma$ with $A > 0$ to be determined later, and let $\gamma > 0$ be strictly less than $(1+A)^{-1}$. Dividing through by $1-(1+A)\gamma$ and minimizing over $\gamma$, we obtain
\begin{align*}
\dfrac{1}{n}\norm{\mathbf{X}\left(\widehat{\boldsymbol\beta}-\boldsymbol\beta^{*}_{\sigma}\right)}^2_2 &\leq 4(1+A)\left[\left(1+\dfrac{1}{\eta}\right)\left(\dfrac{1}{n}\norm{F(\mathbf{r}_t)-\mathbf{Y}_t}^2_2\right) + (1+\eta)\left(\chi_J(1-\chi_R)\dfrac{1}{n}\sum_{i = 1}^n \varepsilon_{J,i,t}\right)^2\right]\\ 
&\quad + 4\left(1+\dfrac{1}{A}\right)\lambda^2n^2.
\end{align*}

Let us now focus on $\dfrac{1}{n}\sum_{i = 1}^n \varepsilon_{J,i,t}$. First, note that for any $\delta > 0$, if $\chi_J(1-\chi_R) \equiv 0$, then
$$\mathbb{P}\left(\chi_J(1-\chi_R)\abs{\dfrac{1}{n}\sum_{i = 1}^n \varepsilon_{J,i,t}} > \delta\right) = \mathbb{P}\left(0 > \delta\right) = 0 = \chi_J(1-\chi_R)\mathbb{P}\left(\abs{\dfrac{1}{n}\sum_{i = 1}^n \varepsilon_{J,i,t}} > \delta\right),$$
and likewise if $\chi_J(1-\chi_R) \equiv 1$.\\

We rewrite the sum $\dfrac{1}{n}\sum_{i = 1}^n \varepsilon_{J,i,t}$ as:
$$\dfrac{1}{n}\sum_{i = 1}^n \varepsilon_{J,i,t} = \dfrac{1}{n}\sum_{i = 1}^n \left(\varepsilon_{J,i,t}-\mathbb{E}\left[\varepsilon_{J,i,t}\right]\right) + \dfrac{1}{n}\sum_{i = 1}^n \mathbb{E}\left[\varepsilon_{J,i,t}\right] = \dfrac{1}{n}\sum_{j = 1}^{\mathfrak{X}_t} \left(Q^2_j - \mathbb{E}\left[Q^2_j\right]\right) + \dfrac{1}{n}\sum_{i = 1}^n \mathbb{E}\left[\varepsilon_{J,i,t}\right].$$

By the properties of Levy processes, each $\varepsilon_{J,i,t}$ is distributed as
$$\sum_{j = 1}^{\mathfrak{X}_{\tau_i,t} - \mathfrak{X}_{\tau_{i-1,t}}} Q^2_j \sim \sum_{j = 1}^{\mathfrak{X}_{t/n}} Q^2_j,$$
so that $\mathbb{E}\left[\varepsilon_{J,i,t}\right] = \mathbb{E}\left[Q^2\right]\mathbb{E}\left[\mathfrak{X}_{t/n}\right]$ by Wald's equation. Here, $Q$ denotes the random variable whose distribution is common distribution of the $Q_j$'s, and $\sim$ denotes distributional equivalence.\\

We thus have:
$$\dfrac{1}{n}\sum_{i = 1}^n \varepsilon_{J,i,t} = \dfrac{1}{n}\sum_{j = 1}^{\mathfrak{X}_t} \left(Q^2_j - \mathbb{E}\left[Q^2_j\right]\right) + \mathbb{E}\left[Q^2\right]\mathbb{E}\left[\mathfrak{X}_{t/n}\right].$$

By Proposition \ref{proposition:random-sums-subweibull}, the center random sum is sub-Weibull with decay parameter $\alpha/2$ and sub-Weibull norm $\mathbb{E}\left[\mathfrak{X}_t^{1+A(\alpha/2)}\right]\norm{Q^2}_{\Psi_{\alpha/2}} = \mathbb{E}\left[\mathfrak{X}_t^{1+A(\alpha/2)}\right]\norm{Q}^2_{\Psi_{\alpha}}$, where $A(\alpha/2)$ is the exponent in Proposition \ref{proposition:random-sums-subweibull} such that $-\max\left\{1,2/\alpha\right\} \leq A(\alpha/2) \leq 2/\alpha + \abs{1-2/\alpha}.$ So for any $\delta > 0$ and some $(\alpha/2)$-dependent constant $C_{\alpha/2} > 0$:
\begin{align*}
\mathbb{P}\left(\abs{\dfrac{1}{n}\sum_{j = 1}^{\mathfrak{X}_t} \left(Q^2_j - \mathbb{E}\left[Q^2_j\right]\right)} > \delta\right) &= \mathbb{P}\left(\abs{\sum_{j = 1}^{\mathfrak{X}_t} \left(Q^2_j - \mathbb{E}\left[Q^2_j\right]\right)} > n\delta\right)\\ &\leq 2\exp\left(-\dfrac{1}{C_{\alpha/2}}\cdot\dfrac{\delta^{\alpha/2} n^{\alpha/2}}{\mathbb{E}\left[\mathfrak{X}_t^{1+A(\alpha/2)}\right]^{\alpha/2}\norm{Q}^{\alpha}_{\Psi_{\alpha}}}\right).
\end{align*}

Now define the event
$$E := \left\{\abs{\dfrac{1}{n}\sum_{j = 1}^{\mathfrak{X}_t} \left(Q^2_j - \mathbb{E}\left[Q^2_j\right]\right)} \leq \delta\right\} \cap \left\{\abs{\mathfrak{X}_{t/n}} \leq \varepsilon_0\right\} \cap \left\{\dfrac{1}{n}\norm{F(\mathbf{r}_t)-\mathbf{Y}_t}^2_2\right\}.$$

Then, by independence:
\begin{align*}
\mathbb{P}\left(E\right) &= \left[1 - \mathbb{P}\left(\abs{\dfrac{1}{n}\sum_{j = 1}^{\mathfrak{X}_t} \left(Q^2_j - \mathbb{E}\left[Q^2_j\right]\right)} > \delta\right)\right] \cdot \left(1 - \mathbb{P}\left(\abs{\mathfrak{X}_{t/n}}> \varepsilon_0\right)\right) \cdot \left(1-\mathfrak{p}_n(\rho)\right)\\
&\geq \left[1 - 2\exp\left(-\dfrac{1}{C_{\alpha/2}}\cdot\dfrac{\delta^{\alpha/2} n^{\alpha/2}}{\mathbb{E}\left[\mathfrak{X}_t^{1+A(\alpha/2)}\right]^{\alpha/2}\norm{Q}^{\alpha}_{\Psi_{\alpha}}}\right)\right] \cdot \left(1 - \mathbb{P}\left(\abs{\mathfrak{X}_{t/n}}> \varepsilon_0\right)\right) \cdot \left(1-\mathfrak{p}_n(\rho)\right).
\end{align*}

Now assuming that the event $E$ occurs, we have $\mathbb{E}\left[\mathfrak{X}_{t/n}\right] \leq \varepsilon_0$, and so:
$$\dfrac{1}{n}\norm{\mathbf{X}\left(\widehat{\boldsymbol\beta}-\boldsymbol\beta^{*}_{\sigma}\right)}^2_2 \leq 4(1+A)\left[\left(1+\dfrac{1}{\eta}\right)\rho + (1+\eta)\left(\chi_J(1-\chi_R)\left(\delta + \mathbb{E}\left[Q^2\right]\varepsilon_0\right)\right)^2\right] + 4\left(1+\dfrac{1}{A}\right)\lambda^2n^2.$$

Minimizing over $\eta$, we get:
$$\dfrac{1}{n}\norm{\mathbf{X}\left(\widehat{\boldsymbol\beta}-\boldsymbol\beta^{*}_{\sigma}\right)}^2_2 \leq 4\left[(1+A)\left(\sqrt{\rho} + \chi_J(1-\chi_R)\left(\delta + \mathbb{E}\left[Q^2\right]\varepsilon_0\right)\right)^2 + \left(1+\dfrac{1}{A}\right)\lambda^2n^2\right].$$

Minimizing further over $A > 0$, we obtain:
$$\dfrac{1}{n}\norm{\mathbf{X}\left(\widehat{\boldsymbol\beta}-\boldsymbol\beta^{*}_{\sigma}\right)}^2_2 \leq 8\lambda n \left(\sqrt{\rho} + \chi_J(1-\chi_R)\left(\delta+ \mathbb{E}\left[Q^2\right]\varepsilon_0\right)\right)^2 + 4\lambda^2n^2,$$
assuming event $E$ occurs.
\end{proof}

\begin{proof}[Proof of Proposition \ref{proposition:consistency-lasso-latent-lasso-observed-standard}]

By definition, $\widehat{\mathbf{u}} := \mathbf{X}\boldsymbol\beta$ and $\mathbf{u}^{*} = \mathbf{X}\boldsymbol\beta^{*}_{\sigma}$. Let $\mathfrak{R}_t := n^{-1}\sum_{j = 1}^{\mathfrak{X}_t} Q^2_j$. Therefore, we can see from our proof of Proposition \ref{proposition:consistency-lasso-observed-true-latent} that for any $A > 0$ and $\eta > 0$:
$$\dfrac{1}{n}\norm{\widehat{\mathbf{u}}-\mathbf{u}^{*}_{\sigma}}^2_2 \leq 4\left\{A\left[(1+\eta)\left(\dfrac{1}{n}\norm{F(\mathbf{r}_t)-\mathbf{Y}_t}^2_2\right) + (1+\eta^{-1})\left(\chi_J(1-\chi_R)\right)^2\left(\dfrac{1}{n}\mathfrak{R}_t\right)^2\right] + \dfrac{1+A}{A}\lambda^2n^2\right\}.$$

With $A = (1 + \eta)^{-1}$, we have:
$$\forall \eta > 0: \dfrac{1}{n}\norm{\widehat{\mathbf{u}}-\mathbf{u}^{*}_{\sigma}}^2_2 \leq 4\left[\dfrac{1}{n}\norm{F(\mathbf{r}_t)-\mathbf{Y}_t}^2_2 + \eta^{-1}\left(\chi_J(1-\chi_R)\right)^2\left(\dfrac{1}{n}\mathfrak{R}_t\right)^2 + (2+\eta)\lambda^2n^2\right].$$

Therefore, for any $\rho, \eta > 0$,
\begin{align*}
&\mathbb{P}\left(\dfrac{1}{n}\norm{\widehat{\mathbf{u}}-\mathbf{u}^{*}_{\sigma}}^2_2 \geq 4(\beta_n + \rho)\right)\\
&\leq \mathbb{P}\left(\dfrac{1}{n}\norm{F(\mathbf{r}_t)-\mathbf{Y}_t}^2_2 \geq \rho \right) + \mathbb{P}\left(\left(\chi_J(1-\chi_R)\right)^2\left(\dfrac{1}{n}\mathfrak{R}_t\right)^2 \geq \eta \beta_n - \eta(2+\eta)\lambda^2 n^2\right).
\end{align*}

Assuming that $\beta_n > 2\lambda^2 n^2$ and maximizing over $\eta$, we get:
$$\mathbb{P}\left(\dfrac{1}{n}\norm{\widehat{\mathbf{u}}-\mathbf{u}^{*}_{\sigma}}^2_2 \geq 4(\beta_n + \rho)\right) \leq \mathbb{P}\left(\dfrac{1}{n}\norm{F(\mathbf{r}_t)-\mathbf{Y}_t}^2_2 \geq \rho \right) + \mathbb{P}\left(\left(\chi_J(1-\chi_R)\right)^2\left(\dfrac{1}{n}\mathfrak{R}_t\right)^2\geq \left(\dfrac{\beta_n}{2\lambda n} - \lambda n\right)^2\right).$$

When $\chi_R(1-\chi_R) \equiv 0$,
$$\mathbb{P}\left(\left(\chi_J(1-\chi_R)\right)^2\left(\dfrac{1}{n}\mathfrak{R}_t\right)^2\geq \left(\dfrac{\beta_n}{2\lambda n} - \lambda n\right)^2\right) = \mathbb{P}\left(\dfrac{(\beta_n-2\lambda n)^2}{4\lambda^2 n^2} \leq 0\right)\equiv 0.$$

Therefore, the previous bound can be rewritten in terms of $\chi_J(1-\chi_R)$ as
$$\mathbb{P}\left(\dfrac{1}{n}\norm{\widehat{\mathbf{u}}-\mathbf{u}^{*}_{\sigma}}^2_2 \geq 4(\beta_n + \rho) \right) \leq \mathbb{P}\left(\dfrac{1}{n}\norm{F(\mathbf{r}_t)-\mathbf{Y}_t}^2_2 \geq \rho\right) + \chi_J(1-\chi_R)\mathbb{P}\left(\left(\dfrac{1}{n}\mathfrak{R}_t\right)^2\geq  \left(\dfrac{\beta_n}{2\lambda n} - \lambda n\right)^2\right).$$

By using independence, the law of total probability, and by letting $\varepsilon_0 > 0$ be small enough that $\beta_n/(2\lambda) \geq \lambda n^2 + \mathbb{E}\left[Q^2\right]\varepsilon_0$, we have:

\begin{align*}
&\mathbb{P}\left(\left(\dfrac{1}{n}\mathfrak{R}_t\right)^2\geq \left(\dfrac{\beta_n}{2\lambda n} - \lambda n\right)^2\right)\\
&= \mathbb{P}\left(\abs{\mathfrak{R}_t}\geq \dfrac{\beta_n}{2\lambda} - \lambda n^2\right)\\
&= \mathbb{P}\left(\abs{\mathfrak{R}_t} \geq \dfrac{\beta_n}{2\lambda}-\lambda n^2 \Bigg\vert \mathfrak{X}_{t/n} \leq \varepsilon_0\right)\mathbb{P}\left(\abs{\mathfrak{X}_{t/n}} \leq \varepsilon_0\right) + \mathbb{P}\left(\abs{\mathfrak{R}_t} \geq \dfrac{\beta_n}{2\lambda}-\lambda n^2 \Bigg\vert\mathfrak{X}_{t/n}  > \varepsilon_0\right)\mathbb{P}\left(\abs{\mathfrak{X}_{t/n}} > \varepsilon_0\right)\\
&\leq \mathbb{P}\left(\abs{\mathfrak{R}_t-\mathbb{E}\left[\mathfrak{R}_t\right]} \geq \dfrac{\beta_n}{2\lambda}-\left(\varepsilon_0 \mathbb{E}\left[Q^2\right] + \lambda n^2\right)\right)\mathbb{P}\left(\abs{\mathfrak{X}_{t/n}} \leq \varepsilon_0\right) + \mathbb{P}\left(\abs{\mathfrak{X}_{t/n}} > \varepsilon_0\right)\\
&\leq 2\exp\left[-\dfrac{1}{C_{\alpha/2}\mathbb{E}\left[\mathfrak{X}_t^{1+A(\alpha/2)}\right]^{\alpha/2}\norm{Q}^{\alpha}_{\Psi_{\alpha}}}\cdot\left(\dfrac{\beta_n}{2\lambda}-\left(\varepsilon_0 \mathbb{E}\left[Q^2\right] + \lambda n^2\right)\right)^{\alpha/2}\right] 
\mathbb{P}\left(\abs{\mathfrak{X}_{t/n}} \leq \varepsilon_0\right) + \mathbb{P}\left(\abs{\mathfrak{X}_{t/n}} > \varepsilon_0\right).
\end{align*}

In conclusion, we have:
\begin{align*}
&\mathbb{P}\left(\dfrac{1}{n}\norm{\widehat{\mathbf{u}}-\mathbf{u}^{*}_{\sigma}}^2_2 \geq 4(\beta_n + \rho)\right)\\
&\leq 2\chi_J(1-\chi_R)\exp\left[-\dfrac{\left(\dfrac{\beta_n}{2\lambda}-\left(\varepsilon_0 \mathbb{E}\left[Q^2\right] + \lambda n^2\right)\right)^{\alpha/2}}{C_{\alpha/2}\mathbb{E}\left[\mathfrak{X}_t^{1+A(\alpha/2)}\right]^{\alpha/2}\norm{Q}^{\alpha}_{\Psi_{\alpha}}}\right]\mathbb{P}\left(\abs{\mathfrak{X}_{t/n}} \leq \varepsilon_0\right)\\
&\quad + \chi_J(1-\chi_R)\mathbb{P}\left(\abs{\mathfrak{X}_{t/n}} > \varepsilon_0\right) + \mathfrak{p}_n(\rho).
\end{align*}
\end{proof}
\end{document}