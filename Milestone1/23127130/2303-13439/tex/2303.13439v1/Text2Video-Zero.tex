\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{float}
\usepackage{array}
\usepackage{varwidth}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\usepackage{makecell}
% \usepackage{titling}
%\usepackage{authblk}
\usepackage{algorithm}
\usepackage[noEnd,indLines]{algpseudocodex}
\newcommand{\authorsep}{\hspace{8pt}}
\newcommand{\affiliationsep}{\hspace{8pt}}

\newcommand{\SN}[1]{{\color{cyan}{{[Shant: \bf #1 ]}}}}
\newcommand{\RH}[1]{{\color{blue}{{[Roberto: \bf #1 ]}}}}
%\newcommand{\SN}[1]{{\color{cyan}{{[ ]}}}}
%\newcommand{\RH}[1]{{\color{blue}{{[]}}}}
% Include other packages here, before hyperref.

\usepackage{cuted}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{capt-of}
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
%\title{Zero-Shot Text2Video Synthesis: Make A Free Video!}
\title{Text2Video-Zero: \\ Text-to-Image Diffusion Models are Zero-Shot Video Generators
% Make-A-Free-Video: Text-to-Image Diffusion Models are Zero-Shot Video Generators \RH{The word free does not fit here in my opinion, we get videos for free, but we are not making free videos
% }\RH{We should also not oversell the method in the title. Lets recall that the image generator  itself  (SD) is not aware of the temporal context. Thus we rely on luck that stitching the images shows the requested action.} 
}

%Motion for Free! \RH{The word "more" is strange. AW: "Better" or just remove "More"?}}

\author{Levon Khachatryan$^1$$^{*}$ \authorsep
Andranik Movsisyan$^1$$^{*}$\authorsep 
Vahram Tadevosyan$^1$$^{*}$ \authorsep
Roberto Henschel$^1$$^{*}$ \authorsep \\
Zhangyang Wang$^{1,2}$ 
Shant Navasardyan$^1$ \authorsep
Humphrey Shi$^{1,3,4}$ \\
\small$^1$Picsart AI Resarch (PAIR) \affiliationsep \small$^2$UT Austin \affiliationsep \small$^3$U of Oregon \affiliationsep \small$^4$UIUC \affiliationsep \\
    {\small \textbf{\url{https://github.com/Picsart-AI-Research/Text2Video-Zero}}}
}


%\author{Levon Khachatryan\thanks{Picsart AI Resarch (PAIR)}\affspace\thanks{Equal Contribution} \SN{Let's have the affiliations right under the names to save some space for our abstract}\\
%Picsart\\
%Institution1 address\\
%{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Andranik Movsisyan\footnotemark[1]\affspace\footnotemark[2]\\
%\and
%Vahram Tadevosyan\footnotemark[1]\affspace\footnotemark[2] \\
%%\and 
%Roberto Henschel\footnotemark[1]\affspace\footnotemark[2] \\
%\and 
%Zhangyang Wang\footnotemark[1]\affspace\thanks{UT Austin}  \\
%\and
%Shant Navasardyan\footnotemark[1] \\
%\and 
%Humphrey Shi\footnotemark[1]\affspace\thanks{U of Oregon}\affspace\thanks{UIUC}
%}
\date{}
\maketitle


\begin{strip}
\vspace{-15mm}
% \begin{figure*}[t]
    \centering
    % \includegraphics[width = \textwidth]{arxiv_figures/fig1.pdf}
    % \includegraphics[width = \textwidth]{arxiv_figures/fig1_v2.pdf}
    % \includegraphics[width = \textwidth]{teaser_image.pdf}
    \includegraphics[width = \textwidth]{arxiv_figures/teaser_final.pdf}
    \captionof{figure}{Our method Text2Video-Zero enables zero-shot video generation using (i) a textual prompt (see rows 1, 2),  (ii) a prompt combined with guidance from poses or edges (see lower right), and  (iii)  Video Instruct-Pix2Pix, \ie, instruction-guided video editing (see lower left). 
    Results are temporally consistent and follow closely the guidance and textual prompts. %\SN{In the image, for each use-case write the use case, e.g. for the first row write: Text-to-Video generation with the prompt "a horse galloping on a street". For editing: Video Instruct-Pix2Pix with instruction "make it Van Gogh Starry Night style", etc.. And we can enlarge the first two rows as we do not have page limitations for arxiv version}
    }
    \label{fig:teaser_img}
% \end{figure*}
\end{strip}

% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

\def\thefootnote{*}\footnotetext{Equal contribution.}\def\thefootnote{\arabic{footnote}}

%%%%%%%%% ABSTRACT
\begin{abstract}
\vspace{-4mm}
    Recent text-to-video generation approaches rely on computationally heavy training and require large-scale video datasets. In this paper, we introduce a new task of zero-shot text-to-video generation and propose a low-cost approach (without any training or optimization) by leveraging the power of existing text-to-image synthesis methods (\eg Stable Diffusion), making them suitable for the video domain.  
    Our key modifications include (i) enriching the latent codes of the generated frames with motion dynamics to keep the global scene and the background time consistent; and (ii) reprogramming frame-level self-attention using a new cross-frame attention of each frame on the first frame, to preserve the context, appearance, and identity of the foreground object. 
    Experiments show that this leads to low overhead, yet high-quality and remarkably consistent video generation. Moreover, our approach is not limited to text-to-video synthesis but is also applicable to other tasks such as conditional and content-specialized video generation, and Video Instruct-Pix2Pix, \ie, instruction-guided video editing.
    As experiments show, our method performs comparably or sometimes better than recent approaches, despite not being trained on additional video data. Our code will be open sourced at: \href{https://github.com/Picsart-AI-Research/Text2Video-Zero}{https://github.com/Picsart-AI-Research/Text2Video-Zero}.
    % Our codes and models will be made publicly available. %\RH{Can we claim this is the first work for Zero-shot SD video generation?} ]\RH{Can we write something like, surprisingly, we outperform existing trained video generation models on certain cases (or are similarly good as them), although they have been trained (on a large scale dataset)? }
\end{abstract}

% \footnotetext[1]{The first four authors Khachatryan,  Movsisyan, Tadevosyan, Henschel contributed equally.}

%%%%%%%%% BODY TEXT
\section{Introduction}
In recent years, generative AI has attracted enormous attention in the computer vision community. With the advent of diffusion models \cite{sohl2015deep,DDPM_paper, DDIM_paper,song2020score}, it has become tremendously popular and successful to generate high-quality images from textual prompts, also called \textit{text-to-image synthesis} \cite{Dalle2_paper,stable_diff,imagen,make-a-scene,xu2022versatile}.
Recent works \cite{video-diffusion-models, make-a-video, imagen_video, tune-a-video, gen1_paper, molad2023dreamix} attempt to extend the success to text-to-video generation and editing tasks, by reusing text-to-image diffusion models in the video domain.
While such approaches yield promising outcomes, most of them require substantial training with a massive amount of labeled data which can be costly and unaffordable for many users.
With the aim of making video generation cheaper, Tune-A-Video \cite{tune-a-video} introduces a mechanism that can adopt Stable Diffusion (SD) model \cite{stable_diff} for the video domain. The training effort is drastically reduced to tuning one video. 
While that is much more efficient than previous approaches, it still requires an optimization process.
In addition, the generation abilities of Tune-A-Video are limited to text-guided video editing applications; video synthesis from scratch, however, remains out of its reach.

In this paper, we take one step forward in studying the novel problem of \textit{zero-shot, ``training-free" text-to-video synthesis}, which is the task of generating videos from textual prompts without requiring any optimization or fine-tuning. 
% \RH{Highlight here: in particular, no collection of training data is necessary.?}
%
A key concept of our approach is to modify   
a pre-trained text-to-image model (\eg, Stable Diffusion), enriching it with temporally consistent generation. 
%
%Our proposed method is based on modifying \RH{Our proposed method is based on a pre-trained text2image model that we modify to enable ..} a pre-trained text2image model (e.g., Stable Diffusion) to enable \RH{enhance} the base model with temporal consistent video generation. 
%
By building upon already trained text-to-image models, our method takes advantage of their excellent image generation quality and enhances their applicability to the video domain without performing additional training.
%
To enforce temporal consistency, we present two innovative and lightweight modifications: (1) we first enrich the latent codes of generated frames with motion information to keep the global scene and the background time consistent; (2) we then use cross-frame attention of each frame on the first frame to preserve the context, appearance, and identity of the foreground object throughout the entire sequence.
Our experiments show that these simple modifications lead to high-quality and time-consistent video generations (see Fig.~\ref{fig:teaser_img} and further results in the appendix). Despite the fact that other works train on large-scale video data, our method achieves similar or sometimes even better performance (see Figures \ref{fig:ourVSCogVideo}, \ref{fig:videoediting} and appendix Figures \ref{fig:unconstrained-text2video-ours-vs-cog}, \ref{fig:table_of_EDITING/comparison1}, \ref{fig:table_of_EDITING/comparison2}). 
Furthermore, our method is not limited to text-to-video synthesis but is also applicable to conditional (see Figures \ref{fig:edgeControl}, \ref{fig:poseControl} and appendix Figures \ref{fig:qual_results_full_method_edge_guidance}, \ref{fig:qual_results_pose}, \ref{fig:table_of_edge_study}, \ref{fig:table_of_pose_study}) and specialized video generation (see Fig.~\ref{fig:edgeControl_with_DB}), and instruction-guided video editing, which we refer as \textit{Video Instruct-Pix2Pix} motivated by Instruct-Pix2Pix \cite{brooks2022instructpix2pix} (see Fig.~\ref{fig:videoediting} and appendix Figures \ref{fig:video_editing_results}, \ref{fig:table_of_EDITING/comparison1}, \ref{fig:table_of_EDITING/comparison2}). 

Our contributions are summarized as three-folds:
\begin{itemize}
    \item A new problem setting of zero-shot text-to-video synthesis, aiming at making text-guided video generation and editing ``freely affordable". We use only a pre-trained text-to-image diffusion model without any further fine-tuning or optimization. 
    \item Two novel post-hoc techniques to enforce temporally consistent generation, via encoding motion dynamics in the latent codes, and reprogramming each frame's self-attention using a new cross-frame attention. %We also demonstrate our integration with ControlNet. 
    \item A broad variety of applications that demonstrate our method's effectiveness, including conditional and specialized video generation, and \textit{Video Instruct-Pix2Pix} \ie, video editing by textual instructions.
\end{itemize}
%Firstly, we present a new problem of zero-shot text2video synthesis with the aim of making text-guided video generation and editing affordable for everyone. Secondly, we show that text2image diffusion models can be utilized to generate time-consistent videos without any finetuning. Lastly, we present several applications of our method that demonstrate its effectiveness, including conditional and specialized video generation, still image animation, and video editing with text guidance.

%We believe that our proposed method has significant implications for the field of video synthesis and could lead to new developments in video generation applications.
%------------------------------------------------------------------------



\section{Related Work}

\subsection{Text-to-Image Generation}
Early approaches to text-to-image synthesis relied on methods such as template-based generation \cite{mansimov2015generating} and feature matching \cite{reed2016learning}. However, these methods were limited in their ability to generate realistic and diverse images. 

Following the success of GANs \cite{goodfellow2020generative}, several other deep learning-based methods were proposed for text-to-image synthesis. These include StackGAN \cite{zhang2017stackgan}, AttnGAN \cite{xu2018attngan}, and MirrorGAN \cite{qiao2019mirrorgan}, which further improve image quality and diversity by introducing novel architectures and attention mechanisms.

Later, with the advancement of transformers \cite{vaswani2017attention}, new approaches emerged for text-to-image synthesis. 
Being a 12-billion-parameter transformer model, Dall-E \cite{Dalle_paper} introduces two-stage training process: First, it generates image tokens, which later are combined with text tokens for joint training of an autoregressive model. 
Later Parti \cite{parti_paper} proposed a method to generate content-rich images with multiple objects. 
Make-a-Scene \cite{make-a-scene} enables a control mechanism by segmentation masks for text-to-image generation.

Current approaches build upon diffusion models, thereby taking text-to-image synthesis quality to the next level. 
%
%Later \RH{repetition of "Later". We can write: Current approaches ..} diffusion-based approaches appear by putting \RH{that put} text-to-image synthesis quality on a next level. 
GLIDE \cite{nichol2021glide} improved Dall-E by adding classifier-free guidance \cite{ho2022classifier}.
Later, Dall-E 2 \cite{Dalle2_paper} utilizes the contrastive model CLIP \cite{CLIP_paper}. By means of diffusion processes, (i) a mapping from CLIP text encodings to image encodings, and (ii) a CLIP decoder is obtained. 
%GLIDE \cite{nichol2021glide} later improved DALL-E 2 by adding classifier-free guidance \cite{ho2022classifier}.
LDM / SD \cite{stable_diff} applies a diffusion model on lower-resolution encoded signals of VQ-GAN \cite{VQ_GAN_paper}, showing competitive quality with a significant gain in speed and efficiency.
Imagen \cite{imagen} shows incredible performance in text-to-image synthesis by utilizing large language models for text processing. Versatile Diffusion~\cite{xu2022versatile} further unifies text-to-image, image-to-text and variations in a single multi-flow diffusion model.

Because of their great image quality, it is desired to exploit text-to-image models for video generation. However,  applying diffusion models in the video domain is not straightforward, especially due to their probabilistic generation procedure, making it difficult to ensure temporal consistency. As we show in our ablation experiments with Fig.~\ref{fig:abltion_study} (see also appendix), our modifications are crucial for temporal consistency in terms of both global scene and background motion, and for the preservation of the foreground object identity.

\subsection{Text-to-Video Generation}
Text-to-video synthesis is a relatively new research direction. Existing approaches try to leverage autoregressive transformers and diffusion processes for the generation. 
NUWA \cite{wu2022nuwa} introduces a 3D transformer encoder-decoder framework and supports both text-to-image and text-to-video generation.
Phenaki \cite{villegas2022phenaki} introduces a bidirectional masked transformer with a causal attention mechanism that allows the generation of arbitrary-long videos from text prompt sequences.
CogVideo \cite{hong2022cogvideo} extends the text-to-image model CogView 2 \cite{ding2022cogview2} by tuning it using a multi-frame-rate hierarchical training strategy to better align text and video clips.
Video Diffusion Models (VDM) \cite{video-diffusion-models} naturally extend text-to-image diffusion models and train jointly on image and video data.
Imagen Video \cite{imagen_video} constructs a cascade of video diffusion models and utilizes spatial and temporal super-resolution models to generate high-resolution time-consistent videos.
Make-A-Video \cite{make-a-video} builds upon a text-to-image synthesis model and leverages video data in an unsupervised manner.
Gen-1 \cite{gen1_paper} extends SD and proposes a structure and content-guided video editing method based on visual or textual descriptions of desired outputs.
Tune-A-Video \cite{tune-a-video} proposes a new task of one-shot video generation by extending and tuning SD on a single reference video.

Unlike the methods mentioned above, our approach is completely training-free, does not require massive computing power or dozens of GPUs, which makes the video generation process affordable for everyone. 
In this respect, Tune-a-Video \cite{tune-a-video} comes closest to our work, as it reduces the necessary computations to tuning on only one video. However, it still requires an optimization process and is heavily dependent on the reference video.

% \RH{In contrast to all these methods, our method ... here we want to talk about training / optimization ...}
% \RH{What is the method closest to our work? How does our work differ? Which drawback has this closest related work that our method solving? }
% \RH{Suggestion: Closest to our method is tune-a-video, which does this .. A drawback is that it relies on additional training. We improve on that by ...
% }
% \RH{Essentially, we need to list our contribution in this section again. Otherwise the whole section just costs us lines of text without speaking in favor of us.}
% \subsection{Conditional Generation}
% Gen-1
% ControlNet
% Instruct Pix2Pix
% DreamBoot

%------------------------------------------------------------------------



\section{Method}
We start this section with a brief introduction of diffusion models, particularly Stable Diffusion (SD) \cite{stable_diff}. 
Then we introduce the problem formulation of zero-shot text-to-video synthesis, followed by a subsection presenting our approach.
% \RH{without "by our approach"? The problem formulation should be general, our approach a way to tackle/solve it.} 
After that, to show the universality of our method, we use it in combination with ControlNet \cite{controlnet} and DreamBooth \cite{ruiz2022dreambooth} diffusion models for generating conditional and specialized videos.
Later we demonstrate the power of our approach with the application of instruction-guided video editing, namely, Video Instruct-Pix2Pix.

%Later we discuss one more application, namely text-guided video editing \SN{Instead of this sentence write "Later we demonstrate the power of our approach with the application in instruction-guided video editing, namely, Video Instruct-Pix2Pix."}.


\subsection{Stable Diffusion}
SD is a diffusion model operating in the latent space of an autoencoder $\mathcal{D}(\mathcal{E}(\cdot))$, namely VQ-GAN \cite{VQ_GAN_paper} or VQ-VAE \cite{VQ_VAE_paper}, where $\mathcal{E}$ and $\mathcal{D}$ are the corresponding encoder and decoder, respectively. 
More precisely if $x_0\in\mathbb{R}^{h\times w\times c}$ is the latent tensor of an input image $Im$ to the autoencoder, i.e. $x_0 = \mathcal{E}(Im)$, diffusion forward process iteratively adds Gaussian noise to the signal $x_0$:
\begin{equation}
    q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I), \;
    t = 1,..,T
\end{equation}
where $q(x_t|x_{t-1})$ is the conditional density of $x_t$ given $x_{t-1}$, and $\{\beta_t\}_{t=1}^{T}$ are hyperparameters.
$T$ is chosen to be as large that the forward process completely destroys the initial signal $x_0$ resulting in $x_T \sim \mathcal{N}(0,I)$. 
The goal of SD is then to learn a backward process
\begin{equation}
    p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1};\mu_\theta(x_t,t),\Sigma_\theta(x_t,t))
\end{equation}
for $t=T,\ldots,1$, 
which allows to generate a valid signal $x_0$ from the standard Gaussian noise $x_T$. 
To get the final image generated from $x_T$ it remains to pass $x_0$ to the decoder of the initially chosen autoencoder: $Im = \mathcal{D}(x_0)$.

After learning the abovementioned backward diffusion process (see DDPM \cite{DDPM_paper}) one can apply a deterministic sampling process, called DDIM \cite{DDIM_paper}:
\begin{equation}
\begin{split}
    x_{t-1} = \sqrt{\alpha_{t-1}}\left(\frac{x_t - \sqrt{1-\alpha_t}\epsilon^t_{\theta}(x_t)}{\sqrt{\alpha_t}}\right) + \\
    \sqrt{1-\alpha_{t-1}}\epsilon^t_{\theta}(x_t), \quad t=T,\ldots,1, 
\end{split}
\end{equation}
where $\alpha_t = \prod_{i=1}^{t}(1-\beta_i)$ and 
\begin{equation}
    \epsilon^t_{\theta}(x_t) = \frac{\sqrt{1-\alpha_t}}{\beta_t}x_t + 
    \frac{(1-\beta_t)(1-\alpha_t)}{\beta_t}\mu_\theta(x_t,t).
\end{equation}

To get a text-to-image synthesis framework, SD guides the diffusion processes with a textual prompt $\tau$. Particularly for DDIM sampling, we get:
\begin{equation}
\begin{split}
    x_{t-1} = \sqrt{\alpha_{t-1}}\left(\frac{x_t - \sqrt{1-\alpha_t}\epsilon^t_{\theta}(x_t,\tau)}{\sqrt{\alpha_t}}\right) + \\
    \sqrt{1-\alpha_{t-1}}\epsilon^t_{\theta}(x_t,\tau), \quad t=T,\ldots,1.
\end{split}
\end{equation}
It is worth noting that in SD, the function $\epsilon^t_{\theta}(x_t,\tau)$ is modeled as a neural network with a UNet-like \cite{UNet_paper} architecture composed of convolutional and (self- and cross-) attentional blocks.
$x_T$ is called the latent code of the signal $x_0$ and there is a method \cite{dhariwal2021diffusion} to apply a deterministic forward process to reconstruct the latent code $x_T$ given a signal $x_0$. 
This method is known as DDIM inversion. Sometimes for simplicity, we will call $x_t, t=1,\ldots,T$ also the \textit{latent codes} of the initial signal $x_0$.
%\SN{add this sentence here: "Sometimes for simplicity we will call $x_t, t=1,\ldots,T$ also the latent codes of the initial signal $x_0$."}
% \RH{What about this null inversion? I would add it to the reference, but not mention it explicitly.} \SN{let's mention it when we use it. I mentioned DDIM inversion regardless it is not a part of SD since it is something we use very often, even in the null-text inversion}.



\subsection{Zero-Shot Text-to-Video Problem Formulation}
Existing text-to-video synthesis methods require either costly training on a large-scale (ranging from $1M$ to $15M$ data-points)
% \RH{Can we make this stronger, ImagenNet was considered large, LAION is even much larger. Did they use such big datasets for text2video?}
text-video paired data \cite{wu2022nuwa,hong2022cogvideo,villegas2022phenaki,imagen_video,video-diffusion-models,gen1_paper} or tuning on a reference video \cite{tune-a-video}.
To make video generation cheaper and easier, we propose a new problem: zero-shot text-to-video synthesis. Formally, given a text description $\tau$ and a positive integer $m\in\mathbb{N}$, the goal is to design a function $\mathcal{F}$ that outputs video frames  $\mathcal{V} \in\mathbb{R}^{m \times H\times W\times 3}$ (for predefined resolution $H\times W$) that exhibit temporal consistency. 
To determine the function $\mathcal{F}$, no training or fine-tuning must be performed on a video dataset.

Our problem formulation provides a new paradigm for text-to-video. Noticeably, a zero-shot text-to-video method naturally leverages quality improvements of text-to-image models.   

\subsection{Method}

In this paper, we approach the zero-shot text-to-video task by exploiting the text-to-image synthesis power of Stable Diffusion (SD).
As we need to generate videos instead of images, SD should operate on sequences of latent codes.
The na\"ive approach is to independently sample $m$ latent codes from standard Gaussian distribution $x^1_T,\ldots,x^m_T\sim \mathcal{N}(0,I)$ and apply DDIM sampling to obtain the corresponding tensors $x_0^{k}$ for $k = 1,\ldots, m$, followed by decoding to obtain the generated video sequence $\{\mathcal{D}(x^k_0)\}_{k=1}^{m}\in\mathbb{R}^{m\times H\times W\times 3}$.
However, as shown in Fig.~\ref{fig:abltion_study} (first row), this leads to completely random generation of images sharing only the semantics described by $\tau$ but neither object appearance nor motion coherence.

To address this issue, we propose to (i) introduce motion dynamics between the latent codes $x^1_T,\ldots,x^m_T$ to keep the global scene time consistent and (ii) use cross-frame attention mechanism to preserve the appearance and the identity of the foreground object. Each of the components of our method are described below in detail.
The overview of our method can be found in Fig.~\ref{fig:main_diagram}.

\begin{figure*}[t]
    \centering
    \includegraphics[width = \textwidth]{main_diagramV2.jpg}
    % \includegraphics[width = \textwidth]{arxiv_figures/fig2.pdf}
    \caption{Method overview: Starting from a randomly sampled latent code $x_{T}^{1}$, we apply $\Delta t$ DDIM backward steps to obtain $x_{T'}^{1}$ using a pre-trained Stable Diffusion model (SD). A specified motion field results for each frame $k$ in a warping function $W_k$ that turns $x_{T'}^{1}$ to $x_{T'}^{k}$. By enhancing the latent codes with motion dynamics, we determine the global scene and camera motion and achieve temporal consistency in the background and the global scene. 
    A subsequent DDPM forward application delivers latent codes $x_{T}^{k}$ for $k=1,\ldots,m$. By using the (probabilistic) DDPM method, a greater degree of freedom is achieved with respect to the motion of objects (see appendix Sec. \ref{apendix:unconstrained-text2video_ablation}).
    %
    Finally, the latent codes are passed to our modified SD model using the proposed cross-frame attention, which uses keys and values from the first frame to generate the image of frame $k=1,\ldots,m$. By using cross-frame attention,  the appearance and the identity of the foreground object are preserved  throughout the sequence. 
    %
    Optionally, we apply background smoothing. To this end, we employ salient object detection to obtain for each frame $k$ a mask $M^{k}$ indicating the foreground pixels. Finally, for the background (using the mask $M^{k}$), a convex combination between the latent code $x_{t}^{1}$ of frame one warped to frame $k$ and the latent code $x_{t}^{k}$  is used to further improve the temporal consistency of the background. 
    %
    %    
    %
    %
    % \RH{Can we replace in the figure "SD\_Backward$(x_{T}^{1},\Delta t)$" by DDIM\_Backward$(x_{T}^{1},\Delta t,SD)$?}
    % \RH{Can we replace DDPM$(\tilde{x}_{T}^{k},\Delta t)$ by DDPM\_Forward$(\tilde{x}_{T}^{k},\Delta t)$?}
    %We use latent codes enhanced by motion dynamics  (left yellow box)   
    %and add Cross-Frame Attention (green box) to a pre-trained SD model. A novel background smoothing in the latent space further improves results.
    %\SN{describe our method in detail in this caption, write that $W_k(x)$ is a warping function for a given direction/motion field, write what more details but not too much, the goal is to make clear our method when one only looks at this overview of our method. Also write here what are the benefits of using our method's key components, for example by enhancing the latent codes by motion dynamics we preserve temporal consistency in the background and the global scene, etc.}}
    }
    \label{fig:main_diagram}
\end{figure*}


Note, to simplify notation, we will denote the entire sequence of latent codes by $x^{1:m}_T = [x^1_T,\ldots,x^m_T]$.

%Also for simplicity we will introduce and use in further writing the following notation for sequences: 


\subsubsection{Motion Dynamics in Latent Codes}
Instead of sampling the latent codes $x^{1:m}_T$ randomly and independently from the standard Gaussian distribution, we \textit{construct} them by performing the following steps (see also Algorithm \ref{alg:motion_dynamics_in_latents} and Fig.~\ref{fig:main_diagram}).
\begin{algorithm}[t]
    \caption{Motion dynamics in latent codes}
    \begin{algorithmic}[1]
        \Require $\Delta t \geq 0, m\in \mathbb{N}, \lambda > 0, \delta=(\delta_x,\delta_y)\in\mathbb{R}^2, \mbox{Stable Diffusion } (SD)$
        \State 
            $x^1_T\sim\mathcal{N}(0,I)$
            \Comment{random sample the first latent code}
        \State
            $x^1_{T'} \leftarrow \mbox{DDIM\_Backward}(x^1_{T},\Delta t, SD)$ 
            \Comment{perform $\Delta t$ backward steps by SD}
        \For {all $k=2,3,\ldots,m$} 
            \State
            $\delta^k \leftarrow \lambda\cdot(k-1)\delta$ 
            \Comment{computing global translation vectors}
            \State
            $W_k \leftarrow \mbox{Warping by }\delta^k$
            \Comment{defining warping functions}
            \State
            $\tilde{x}^k_{T'} \leftarrow W_k(x^1_{T'})$
            \State
            $x^k_{T} \leftarrow \mbox{DDPM\_Forward}(\tilde{x}^k_{T'},\Delta t)$
            \Comment{DDPM forward for more motion freedom}
        \EndFor
        \Return $x^{1:m}_T$
    \end{algorithmic}
    \label{alg:motion_dynamics_in_latents}
\end{algorithm}
\begin{enumerate}
    \item Randomly sample the latent code of the first frame: $x^1_T\sim\mathcal{N}(0,I)$.
    \item Perform $\Delta t \geq 0$ DDIM backward steps on the latent code $x^1_T$ by using the SD model and get the corresponding latent $x^1_{T'}$, where $T' = T - \Delta t$.
    \item Define a direction $\delta=(\delta_x,\delta_y)\in\mathbb{R}^2$ for the global scene and camera motion. By default $\delta$ can be the main diagonal direction $\delta_x = \delta_y = 1$.
    \item For each frame $k=1,2,\ldots,m$ we want to generate, compute the global translation vector $\delta^k = \lambda\cdot (k-1)\delta$, where $\lambda$ is a hyperparameter controlling the amount of the global motion.
    \item Apply the constructed motion (translation) flow $\delta^{1:m}$ to $x^1_{T'}$, denote the resulting sequence by $\tilde{x}^{1:m}_{T'}$:
    \begin{equation}
        \tilde{x}^{k}_{T'} = W_k(x^1_{T'}) \; 
        \mbox{for } k = 1,2,\ldots,m,
    \end{equation}
    where $W_k(x^1_{T'})$ is the warping operation for translation by the vector $\delta^k$.
    \item Perform $\Delta t$ DDPM forward steps on each of the latents $\tilde{x}^{2:m}_{T'}$ and get the corresponding latent codes $x^{2:m}_T$.
\end{enumerate}

Then we take the sequence $x^{1:m}_T$ as the starting point of the backward (video) diffusion process.
As a result, the latent codes generated with our proposed motion dynamics lead to better temporal consistency of the global scene as well as the background, see Fig.~\ref{fig:abltion_study}.
% \RH{Wrong figure referenced?}.  \RH{We should motivate this bit more why we go to $T'$ then back to $T$. If we do it on $T$, we see the translation, but its too constraining, i.e., no motion of the object, global translation. If we apply DDPM afterwards, the noise seems to add flexibility.}
Yet, the initial latent codes are not constraining enough to describe particular colors, identities or shapes, thus still leading to  temporal inconsistencies, especially for the foreground object.


% \begin{itemize}
%     \item Sample a latent tensor $x^1_T$ from the standard Gaussian distribution and obtain the remaining latent codes  $x^{2:m}_T$ for the other frames by \textit{warping} $x^1_T$. The mapping  between $x_{T}^{1}$ and the warped latent codes can stem from motion observed in a reference video, or set to predefined (possibly random) directions. 
    
%     \item Apply DDIM inversion on a video to get the corresponding latent codes $x^{1:m}_T$ that generate the video.
%     \item Apply DDIM inversion on a single image, obtain its latent code $x^1_T$ then warp $x^1_T$ either by predefined (random) directions or by a motion obtained from a reference video.
% \end{itemize}
% In each of the use cases, latent codes generated with our proposed motion dynamics lead to better temporal consistency of the global scene as well as the background, see figure ??. Even more, the generated videos show characteristics of the reference video or the defined warp function, i.e., 2D Euclidean camera motion can be transferred \RH{Can we state this?}. Yet, the initial latent codes are not constraining enough to describe particular colors, identities or shapes, thus still leading to  temporal inconsistencies especially for the foreground object.

% \RH{In fact we did something more complex. We do the warping in the case of text-to-video and image-to-video not on timestep $T$, but on timestep $t <T$. Then, in order to increase variation, we applied DDPM forward to get to timestep $t' >t$. Then we continue with DDIM backward. We should then also show visually the difference results. If we do warping directly on xT with a translation, we obtain really just a translation in the image and nothing is moving..}



%As can be noticed from Fig. ?? after introducing motion dynamics in latent codes the generated videos appear with better temporal consistencies in the global scene and the background.
%However as the latent codes does not describe the colors, identities, and shapes well we still have temporal inconsistencies especially for the foreground object.


\subsubsection{Reprogramming Cross-Frame Attention}
To address the issue mentioned above, we use a cross-frame attention mechanism to preserve the information about (in particular) the foreground object's appearance, shape, and identity throughout the generated video. 

To leverage the power of cross-frame attention and at the same time exploit a pretrained SD without retraining, we replace each of its self-attention layers with a cross-frame attention, with the attention for each frame being on the first frame.
More precisely in the original SD UNet architecture  $\epsilon^t_\theta(x_t,\tau)$, each self-attention layer takes a feature map $x\in\mathbb{R}^{h\times w\times c}$, linearly projects it into query, key, value features $Q,K,V\in \mathbb{R}^{h\times w\times c}$, and computes the layer output by the following formula (for simplicity  described here for only one attention head) \cite{vaswani2017attention}:
\begin{equation}
    \mbox{Self-Attn}(Q,K,V) = \mbox{Softmax}\left(\frac{QK^T}{\sqrt{c}}\right)V.
\end{equation}
In our case, each attention layer receives $m$ inputs: $x^{1:m} = [x^1,\ldots,x^m]\in\mathbb{R}^{m\times h\times w\times c}$. Hence,  the linear projection layers produce  $m$ queries, keys, and values $Q^{1:m}, K^{1:m}, V^{1:m}$, respectively.

Therefore, we replace each self-attention layer with a cross-frame attention of each frame on the first frame as follows:
% \begin{equation}
% \label{eq:cross-frame-attn}
% \begin{split}
%     \mbox{Cross-Frame-Attn}(Q^{1:m},K^{1:m},V^{1:m})^k= \\
%     \mbox{Softmax}\left(\frac{Q^k(K^1)^T}{\sqrt{c}}\right)V^1
% \end{split}
% \end{equation}
% for $k=1,\ldots,m$.
\begin{equation}
\label{eq:cross-frame-attn}
\begin{split}
    \mbox{Cross-Frame-Attn}(Q^{k},K^{1:m},V^{1:m})= \\
    \mbox{Softmax}\left(\frac{Q^k(K^1)^T}{\sqrt{c}}\right)V^1
\end{split}
\end{equation}
for $k=1,\ldots,m$.
By using cross frame attention, the appearance and structure of the objects and background as well as identities are carried over from the first frame to subsequent frames, which significantly increases the temporal consistency of the generated frames (see Fig.~\ref{fig:abltion_study} and the appendix, Figures \ref{fig:ablation-unconditional-attention-motion}, \ref{fig:table_of_edge_study}, \ref{fig:table_of_pose_study}).


%As can be noticed in Fig. ?? this modification further improves the generated video quality by keeping the appearance, structure, and identity of the objects in generated frames.

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width = .9\textwidth]{Text2VideoResults.jpg}
%     \caption{Text2Video results of our method. Depicted frames show that identities and appearances are temporally consistent and fitting to the textual prompt. }
%     \label{fig:text2video_results}
% \end{figure*}

\begin{figure*}
    \centering
    \begin{subfigure}{\textwidth}
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq1/0.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq1/1.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq1/2.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq1/3.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq1/4.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq1/5.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq1/6.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq1/7.jpg} 
    \vskip -.4\baselineskip
    \caption{a high quality realistic photo of a cute cat running in a beautiful meadow}
    \end{subfigure}
    \vskip .3\baselineskip
    \begin{subfigure}{\textwidth}
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq2/0.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq2/1.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq2/2.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq2/3.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq2/4.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq2/5.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq2/6.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq2/7.jpg}
    \vskip -.4\baselineskip
    \caption{an astronaut is skiing down a hill}
    \end{subfigure}
    \vskip .3\baselineskip
    \begin{subfigure}{\textwidth}
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq3/0.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq3/1.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq3/2.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq3/3.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq3/4.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq3/5.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq3/6.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq3/7.jpg}
    \vskip -.4\baselineskip
    \caption{a dog is walking down the street}
    \end{subfigure}
    \vskip .3\baselineskip

    \begin{subfigure}{\textwidth}
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq4/0.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq4/1.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq4/2.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq4/3.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq4/4.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq4/5.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq4/6.jpg} 
    \hfill
    \includegraphics[width=0.12\textwidth]{arxiv_figures/fig3/seq4/7.jpg}
    \vskip -.4\baselineskip
    \caption{a high quality realistic photo of a panda walking alone down the street}
    \end{subfigure}
    \vskip -.3\baselineskip
    \caption{Text-to-Video results of our method. Depicted frames show that identities and appearances are temporally consistent and fitting to the textual prompt. For more results, see Appendix Sec. \ref{apendix:unconstrained-text2video}.}
    \label{fig:text2video_results}
\end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[width = 1\linewidth]{AddingControlNet_diagram.pdf}
    \caption{The overview of Text2Video-Zero + ControlNet}
    \label{fig:AddingControlNet}
\end{figure}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width = \linewidth]{AddingPix2Pix_diagram.pdf}
%     \caption{The overview of our method + Pix2Pix \SN{mention the main diagram for "Our Approach"}}
%     \label{fig:AddingControlNet}
% \end{figure}



\subsubsection{Background smoothing (Optional)}
We further improve temporal consistency of the background using a convex combination of background-masked latent codes between the first frame and frame $k$.
This is especially helpful for video generation from textual prompts when one or no initial image and no further guidance are provided. 

In more detail, given the generated sequence of our video generator, $x_{0}^{1:m}$, we apply (an in-house solution for) salient object detection \cite{wang2021salient} to the decoded images to obtain a corresponding foreground mask $M^{k}$ for each frame $k$.
% After resizing $M_{k}$ to the dimension of the latent space, we warp it according to the employed  motion dynamics defined by $W_k$, resulting in the mask $\hat{M}^{k} = W_{k}(M_{k})$. Likewise, for timestep $t$ and frame $k$, we warp $x_{t}^{1}$ accordingly and denote the result by $\hat{x}_{t}^{k} := W_k(x_{t}^{1})$.  
Then we warp $x^1_{t}$ according to the employed  motion dynamics defined by $W_k$ and denote the result by $\hat{x}_{t}^{k} := W_k(x_{t}^{1})$.

Background smoothing is achieved by a convex combination between the actual latent code $x_{t}^{k}$ and the warped latent code $\hat{x}_{t}^{k}$ on the background, i.e., 
% \begin{equation}
%     \overline{x}_{t}^{k} = \hat{M}^{k} \odot x_{t}^{k}  + (1 - \hat{M}^{k}) \odot (\alpha \hat{x}_{t}^{k}+ (1- \alpha) x_{t}^{k} ),
% \end{equation}
\begin{equation}
    \overline{x}_{t}^{k} = M^{k} \odot x_{t}^{k}  + (1 - M^{k}) \odot (\alpha \hat{x}_{t}^{k}+ (1- \alpha) x_{t}^{k}),
\end{equation}
for $k = 1,\ldots, m$, where $\alpha$ is a hyperparameter, which we empirically choose $\alpha=0.6$. Finally, DDIM sampling is employed on $\overline{x}_{t}^{k}$, which delivers video generation with background smoothing.
We use background smoothing in our video generation from text when no guidance is provided. For an ablation study on background smoothing, see the appendix, Sec. \ref{apendix:unconstrained-text2video_ablation}.


\subsection{Conditional and Specialized Text-to-Video}

Recently powerful controlling mechanisms 
\cite{controlnet,mou2023t2i,liu2023more} emerged to guide the diffusion process for text-to-image generation. 
Particularly, ControlNet \cite{controlnet} enables to condition the generation process using edges, pose, semantic masks, image depths, etc. 
However, a direct application of ControlNet in the video domain leads to temporal inconsistencies and to severe changes of object appearance, identity, and the background (see Fig.~\ref{fig:abltion_study} and the appendix Figures \ref{fig:ablation-unconditional-attention-motion}, \ref{fig:table_of_edge_study}, \ref{fig:table_of_pose_study}). It turns out that our modifications on the basic diffusion process for videos result in more consistent videos guided by ControlNet conditions. We would like to point out again that our method does not require any fine-tuning or optimization processes.

More specifically, ControlNet creates a trainable copy of the encoder (including the middle blocks) of the UNet $\epsilon^t_\theta(x_t,\tau)$ while additionally taking the input $x_t$ and a condition $c$, and adds the outputs of each layer to the skip-connections of the original UNet. 
Here $c$ can be any type of condition, such as edge map, scribbles, pose (body landmarks), depth map, segmentation map, etc.
The trainable branch is being trained on a specific domain for each type of the condition $c$ resulting in an effective conditional text-to-image generation mechanism.

To guide our video generation process with ControlNet we apply our method to the basic diffusion process, i.e. enrich the latent codes $x^{1:m}_T$ with motion information and change the self-attentions into cross-frame attentions in the main UNet. 
While adopting the main UNet for video generation task, we apply the ControlNet pretrained copy branch per-frame on each $x^k_t$ for $k=1,\ldots,m$ in each diffusion time-step $t=T,\ldots,1$ and add the ControlNet branch outputs to the skip-connections of the main UNet.

\begin{figure}
    \centering
    \begin{subfigure}{.45\textwidth}
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig5/seq1/0.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig5/seq1/1.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig5/seq1/2.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig5/seq1/3.jpg}
    \vskip -.4\baselineskip
    \caption{a panda dancing in Antarctica}
    \end{subfigure}
    \vskip .3\baselineskip
    \begin{subfigure}{.45\textwidth}
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig5/seq2/0.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig5/seq2/1.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig5/seq2/2.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig5/seq2/3.jpg}
    \vskip -.4\baselineskip
    \caption{a cyborg dancing near a volcano}
    \end{subfigure}
    \vskip .3\baselineskip
    \begin{subfigure}{.45\textwidth}
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig5/seq3/0.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig5/seq3/1.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig5/seq3/2.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig5/seq3/3.jpg} 
    \vskip -.4\baselineskip
    \caption{an astronaut dancing in the outer space}
    \end{subfigure}
    \vskip -.3\baselineskip
    \caption{Conditional generation with pose control. For more results see appendix, Sec. \ref{apendix:t2v_pose_guidance}.}
    \label{fig:poseControl}
\end{figure}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width = \linewidth]{poseControl.jpg}
%     \caption{Conditional generation with pose control.}
%     \label{fig:poseControl}
% \end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{.45\textwidth}
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig6/seq1/0.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig6/seq1/1.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig6/seq1/2.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig6/seq1/3.jpg}
    \vskip -.4\baselineskip
    \caption{oil painting of a girl dancing close-up}
    \end{subfigure}
    \vskip .3\baselineskip
    \begin{subfigure}{.45\textwidth}
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig6/seq2/0.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig6/seq2/1.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig6/seq2/2.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig6/seq2/3.jpg}
    \vskip -.4\baselineskip
    \caption{Cyberpunk boy with a hat dancing close-up}
    \end{subfigure}
    \vskip -.3\baselineskip
    \caption{Conditional generation with edge control. For more results see appendix, Sec. \ref{apendix:t2v_edge_guidance}.}
    \label{fig:edgeControl}
\end{figure}
\vskip\baselineskip

% \begin{figure}[t]
%     \centering
%     \includegraphics[width = \linewidth]{edgeControl.jpg}
%     \caption{Conditional generation with edge control.}
%     \label{fig:edgeControl}
% \end{figure}

Furthermore, for our conditional generation task, we adopted the weights of specialized DreamBooth (DB) \cite{ruiz2022dreambooth} models\footnote{Avatar model: \url{https://civitai.com/models/9968/avatar-style}. GTA-5 model:  \url{https://civitai.com/models/1309/gta5-artwork-diffusion}.}. This gives us specialized time-consistent video generations (see Fig.~\ref{fig:edgeControl_with_DB}).

% For specialized text2video generation we use DreamBooth (DB) \cite{ruiz2022dreambooth} to train specialized diffusion models for text2image synthesis. After training DB models we replace the weights of our base UNet $\epsilon^t_\theta(x^{1:m}_t,\tau)$ with the weights of these models and get specialized time-consistent video generations (see Fig. ??). Our advantage here is that our method does not require any finetuning or retraining, we just need to change the weights of the base model.

% Finally we combined specialized and conditinoal generation by taking DB models as basic models for our method and adding guidance from ControlNet. The results can be seen in Fig. ??.

\begin{figure}
    \centering
    \begin{subfigure}{.45\textwidth}
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig7/seq1/0.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig7/seq1/1.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig7/seq1/2.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig7/seq1/3.jpg}
    \vskip -.4\baselineskip
    \caption{oil painting of a beautiful girl avatar style}
    \end{subfigure}
    \vskip .3\baselineskip
    \begin{subfigure}{.45\textwidth}
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig7/seq2/0.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig7/seq2/1.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig7/seq2/2.jpg} 
    \includegraphics[width=0.24\textwidth]{arxiv_figures/fig7/seq2/3.jpg}
    \vskip -.4\baselineskip
    \caption{gta-5 style}
    \end{subfigure}
    \vskip -.3\baselineskip
    \caption{Conditional generation with edge control and DB models.}
    \label{fig:edgeControl_with_DB}
\end{figure}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width = \linewidth]{edgeControl_with_DB.jpg}
%     \caption{Conditional generation with edge control and DB models.} 
%     % \SN{mention about additional positive prompts}}
%     \label{fig:edgeControl_with_DB}
% \end{figure}


% \subsection{Single Image Animation}
% \label{sec:single_img_anim}
% Our method can be applied to animate still images as shown in Fig. ??. 
% To do so we take the given image $Im$, obtain its DDIM latent code $x^1_T$ with DDIM inversion (also one can use null-text inversion \cite{mokady2022null} for better reconstruction), estimate the latent codes $x^{2:m}_T$ for other frames by warping $x^1_T$ with a predefined global motion (mainly to make the background move organically), and apply our method on the latent code $x^{1:m}_T$ guided by a textual prompt describing a motion. \RH{Needs probably an update as  3.3.1 was changed.}


\subsection{Video Instruct-Pix2Pix}
With the rise of text-guided image editing methods such as Prompt2Prompt \cite{prompt2prompt}, Instruct-Pix2Pix \cite{brooks2022instructpix2pix}, SDEdit \cite{meng2021sdedit}, etc., text-guided video editing approaches emerged \cite{bar2022text2live,lee2023shape,tune-a-video}.
While these methods require complex optimization processes, our approach enables the adoption of any SD-based text-guided image editing algorithm to the video domain without any training or fine-tuning. 
Here we take the text-guided image editing method Instruct-Pix2Pix and combine it with our approach. 
More precisely, we change the self-attention mechanisms in Instruct-Pix2Pix to cross-frame attentions according to Eq.~\ref{eq:cross-frame-attn}. 
Our experiments show that this adaptation significantly improves the consistency of the edited videos (see Fig.~\ref{fig:videoediting}) over the na\"ive per-frame usage of Instruct-Pix2Pix.

%------------------------------------------------------------------------
\section{Experiments}
\subsection{Implementation Details}
We take the Stable Diffusion \cite{stable_diff} code\footnote{ \url{https://github.com/huggingface/diffusers}. We also benefit from the codebase of Tune-A-Video \url{https://github.com/showlab/Tune-A-Video}.} with its pre-trained weights from version 1.5 as basis and implement our modifications. 
%
%We make our modifications over the Stable Diffusion codebase and take the pre-trained weights from the version 1.5 \SN{links to the codebase and weigths}.
In our experiments, we generate $m=8$ frames with $512\times 512$ resolution for each video. However, our framework allows generating any number of frames, either by increasing $m$, or by employing our method in an auto-regressive fashion where the last generated frame $m$ becomes the first frame in computing the next $m$ frames. For text-to-video generation, we take $T' = 881, T = 941$, while for conditional and specialized generation, and for Video Instruct-Pix2Pix, we take $T'=T=1000$.

% \SN{maybe some details about the motion directions we chose for latent codes?}

For a conditional generation, we use the codebase\footnote{ \url{https://github.com/lllyasviel/ControlNet}.} of ControlNet \cite{controlnet}. For specialized models, we take DreamBooth \cite{ruiz2022dreambooth} models from publicly available sources. For Video Instruct-Pix2Pix, we use the codebase\footnote{ \url{https://github.com/timothybrooks/instruct-pix2pix}.} of Instruct Pix2Pix \cite{brooks2022instructpix2pix}.


\subsection{Qualitative Results}

All applications of Text2Video-Zero show that it successfully generates videos where the global scene and the background are time consistent and the context, appearance, and identity of the foreground object are maintained throughout the entire sequence. 

In the case of text-to-video, we observe that it generates high-quality videos that are well-aligned to the text prompt (see Fig.~\ref{fig:text2video_results} and the appendix). For instance, the depicted panda shows a naturally walking on the street. Likewise, using additional guidance from edges or poses (see Fig.~\ref{fig:poseControl}, Fig,~\ref{fig:edgeControl} and Fig.~\ref{fig:edgeControl_with_DB} and the appendix), high quality videos are generated matching the prompt and the guidance that show great temporal consistency and identity preservation. 

In the case of Video Instruct-Pix2Pix (see Fig. \ref{fig:teaser_img} and the appendix) generated videos possess high-fidelity with respect to the input video, while following closely the instruction.

\subsection{Comparison with Baselines}
We compare our method with two publicly available baselines: CogVideo \cite{hong2022cogvideo} and Tune-A-Video \cite{tune-a-video}. 
Since CogVideo is a text-to-video method we compare with it in pure text-guided video synthesis settings. 
With Tune-A-Video we compare in our Video Instruct-Pix2Pix setting.
% \SN{we may compare with more methods, such as \cite{lee2023shape} or at least \cite{bar2022text2live}, we will be better in foreground object changing scenarios as we do with instruct pix2pix}

\begin{figure*}
    \centering
    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq1_ours/0.jpg} 
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq1_ours/1.jpg} 
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq1_ours/2.jpg} 
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq1_ours/3.jpg} 
    \hskip\baselineskip
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq1_their/0.jpg} 
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq1_their/1.jpg} 
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq1_their/2.jpg} 
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq1_their/3.jpg} 
    \vskip -.4\baselineskip
    \caption{a man is running in the snow}
    \end{subfigure}
    \vskip .3\baselineskip
    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq2_ours/0.jpg} 
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq2_ours/1.jpg} 
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq2_ours/2.jpg} 
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq2_ours/3.jpg} 
    \hskip\baselineskip
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq2_their/0.jpg} 
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq2_their/1.jpg} 
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq2_their/2.jpg} 
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq2_their/3.jpg} 
    \vskip -.4\baselineskip
    \caption{a man is riding a bicycle in the sunshine}
    \end{subfigure}
    \vskip .3\baselineskip

    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq3_ours/0.jpg} 
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq3_ours/1.jpg} 
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq3_ours/2.jpg} 
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq3_ours/3.jpg} 
    \hskip\baselineskip
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq3_their/0.jpg} 
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq3_their/1.jpg} 
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq3_their/2.jpg} 
    \includegraphics[width=0.115\textwidth]{arxiv_figures/fig8/seq3_their/3.jpg} 
    \vskip -.4\baselineskip
    \caption{a man is walking in the rain}
    \end{subfigure}
    \vskip -.3\baselineskip
    \caption{Comparison of our method vs CogVideo on text-to-video generation task (left is ours, right is CogVideo \cite{hong2022cogvideo}). For more comparisons, see appendix Fig. \ref{fig:unconstrained-text2video-ours-vs-cog}.}
    \label{fig:ourVSCogVideo}
\end{figure*}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width = .8\linewidth]{OurVSCogVideo.jpg}
%     \caption{Comparison of our method vs CogVideo on text2video generation task (left is ours, right is CogVideo \cite{hong2022cogvideo})}
%     \label{fig:ourVSCogVideo}
% \end{figure*}

\subsubsection{Quantitative Comparison}


%\begin{table}[ht]
%\centering
%\vspace{-0.4cm}
% \resizebox{\linewidth}{!}{
%\begin{tabular}{l c c} \toprule
%    {Method} & {CogVideo} & {Ours} \\ \midrule
%    {CLIP score}  & 29.63 & \textbf{31.19} \\
%    \bottomrule
%\end{tabular}
% }

%\caption{Our method vs CogVideo on text2video generation}
%\label{tab:comparison}
%\end{table}
%\captionsetup[table]{skip=10pt}


To show quantitative results, we evaluate the CLIP score \cite{hessel2021clipscore}, which indicates video-text alignment. We randomly take 25 videos generated by CogVideo and synthesize corresponding videos using the same prompts according to our method. The CLIP scores for our method and CogVideo are $31.19$ and $29.63$, respectively. Our method thus slightly outperforms  CogVideo, even though the latter has 9.4 billion parameters and requires large-scale training on videos. 


% Similarly, when comparing with Tune-A-Video for text-guided video editing we measure CLIP score. From Table ?? we can see that our method performs better than Tune-A-Video for text-guided video editing settings. We think the reason is that we build upon Instruct Pix2Pix which transfers the instruction information better than just changing the text prompt.


\subsubsection{Qualitative Comparison}
We present several results of our method in Fig.~\ref{fig:ourVSCogVideo} and provide a qualitative comparison to CogVideo \cite{hong2022cogvideo}. Both methods show good temporal consistency throughout the sequence, preserving the identity of the object and background. However, our method shows better text-video alignment. For instance, while our method correctly generates a video of a man riding a bicycle in the sunshine in Fig.~\ref{fig:ourVSCogVideo}(b), CogVideo sets the background to moon light. Also in Fig.~\ref{fig:ourVSCogVideo}(a), our method correctly shows a man running in the snow, while neither the snow nor a man running are clearly visible in the video generated by CogVideo. 


%show good temporal
%In Fig.~\ref{fig:ourVSCogVideo}(a) one can see the qualitative comparison of our method with CogVideo. \SN{explain the results, where we are better, why we are better, short, in 2-3 sentences}
% \SN{show some results and explain why we are better or claim that we are comparable}


Qualitative results of \textit{Video Instruct-Pix2Pix} and a visual comparison with per-frame Instruct-Pix2Pix and Tune-A-Video are shown in Fig.~\ref{fig:videoediting}. While Instruct-Pix2Pix shows a good editing performance per frame, it lacks temporal consistency. This becomes evident especially in the video depicting a skiing person, where the snow and the sky are drawn using different styles and colors. Using our Video Instruct-Pix2Pix method, these issues are solved resulting in temporally consistent video edits throughout the entire sequence.   



%Using the  proposed  Cross-Frame Attention, our method successfully keeps the appearance consistent across all generated frames, improving clearly upon per-frame Instruct-Pix2Pix. This is  evident especially in the video depicting a skiing person, where the snow is consistently transformed, resulting in a coherent style and colors throughout the entire sequence. 


While Tune-A-Video creates temporally consistent video generations, it is less aligned to the instruction guidance than our method, struggles creating local edits and losses details of the input sequence. 
This becomes apparent when looking at the edit of the dancer video depicted in Fig.~\ref{fig:videoediting}  (left side). In contrast to Tune-A-Video, our method draws the entire dress brighter and at the same time better preserves the background, \eg the wall behind the dancer is almost kept the same. Tune-A-Video draws a severely modified wall. Moreover, our method is more faithful to the input details,  \eg, Video Instruct-Pix2Pix draws the dancer using the pose exactly as provided (Fig.~\ref{fig:videoediting}  left), and shows all skiing persons appearing in the input video (compare last frame of Fig.~\ref{fig:videoediting}(right)), in constrast to Tune-A-Video.
All the above-mentioned weaknesses of Tune-A-Video can also be observed in our additional evaluations that are provided in the appendix, Figures \ref{fig:table_of_EDITING/comparison1}, \ref{fig:table_of_EDITING/comparison2}.


%Moreover,  Video Instruct-Pix2Pix improves upon Tune-A-video. For instance, in contrast to Tune-A-Video, our method is more faithful,  e.g., preserves the pose of the dancer and details of the scene (Fig.~\ref{fig:videoediting}  left), and draws all skiing persons appearing in the input video (compare last frame of Fig.~\ref{fig:videoediting}(right)). Most importantly, the results generated by our method fit better to the provided instruction (e.g. the entire dress of the dancer became brighter). This is also supported by further results in the appendix, Sect.~\textcolor{red}{4}.


%\SN{I think we can claim even better, as we have enough such examples also from the appendix, please motivate whhy we are better} with . 
%Our method is more faithful, e.g., preserves the pose of the dancer and details of the scene, in contrast to Tune-A-Video.

% visual quality and textual alignment \SN{more explanation}.

\setcounter{table}{\value{figure}} 
\begin{table*}
\captionsetup{name=Figure}
        \centering
        \begin{tabular}{ M{13mm}M{15mm}M{15mm}M{15mm}M{16mm}M{15mm}M{15mm}M{15mm}M{15mm}M{15mm}}
            \multicolumn{9}{M{74mm}}{} \\
            \begin{flushleft}
                \fontsize{7}{12}\selectfont Original
            \end{flushleft} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq1/original/0.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq1/original/1.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq1/original/2.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq1/original/3.jpg}} &  
            {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq2/original/0.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq2/original/1.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq2/original/2.jpg}} & 
            {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq2/original/3.jpg}} \\ 
            % & \multicolumn{4}{c}{{{\fontsize{8}{10}\selectfont color his dress white}}} & \multicolumn{4}{c}{{{\fontsize{8}{10}\selectfont make it Van Gogh Starry Night style}} } \\
            \multicolumn{9}{M{74mm}}{} \\
            & \multicolumn{4}{M{65mm}}{color his dress white} & \multicolumn{4}{M{65mm}}{make it Van Gogh Starry Night style} \\
            \begin{flushleft}\vspace{0.1cm}\fontsize{7}{12}\selectfont Video Instruct-Pix2Pix (Ours) \end{flushleft} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq1/ours/0.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq1/ours/1.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq1/ours/2.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq1/ours/3.jpg}} &  
            {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq2/ours/0.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq2/ours/1.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq2/ours/2.jpg}} & 
            {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq2/ours/3.jpg}} \\  
            \begin{flushleft}
              \vspace{-0.2cm}  \fontsize{7}{12}\selectfont \mbox{Instruct-Pix2Pix}
            \end{flushleft} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq1/ip2p/0.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq1/ip2p/1.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq1/ip2p/2.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq1/ip2p/3.jpg}} &  
            {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq2/ip2p/0.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq2/ip2p/1.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq2/ip2p/2.jpg}} & 
            {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq2/ip2p/3.jpg}} \\
            \begin{flushleft}
               \vspace{-0.2cm} \fontsize{7}{12}\selectfont \mbox{Tune-A-Video}
            \end{flushleft} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq1/tav/0.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq1/tav/1.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq1/tav/2.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq1/tav/3.jpg}} &  
            {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq2/tav/0.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq2/tav/1.jpg}} &  {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq2/tav/2.jpg}} & 
            {\includegraphics[width=.1\textwidth]{arxiv_figures/fig9/seq2/tav/3.jpg}} \\
        \end{tabular}
        \caption{Comparison of Video Instruct-Pix2Pix(ours) with Tune-A-Video and per-frame Instruct-Pix2Pix. For more comparisons see our appendix.}
        \label{fig:videoediting}
    \end{table*}
\setcounter{figure}{\value{table}} 

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width = \textwidth]{VideoEditing.jpg}
%     \caption{Comparison of our method with Tune-A-Video and per-frame InstructPix2Pix
%     % \RH{Note that Tune-a-video is less faith-full, e.g. changes the pose}
%     }
%     \label{fig:videoediting}
% \end{figure*}

\subsection{Ablation Study}
We perform an ablation study on two main components of our method: making the initial latent codes coherent to a motion, and using cross-frame attention on the first frame instead of self-attention (for an ablation study on background smoothing see appendix Sec. \ref{apendix:unconstrained-text2video_ablation}). 
The qualitative results are presented in Fig.~\ref{fig:abltion_study}. With the base model only, i.e. without our changes (first row), no temporal consistency is achieved.  
This is especially severe for unconstrained text-to-video generations. For example, the appearance and position of the horse changes very quickly, and the background is utterly inconsistent. Using our proposed motion dynamics (second row), the general concept of the video is preserved better throughout the sequence. For example, all frames show a close-up of a horse in motion.  Likewise, the appearance of the woman and the background in the middle four figures  (using ControlNet with edge guidance) is greatly improved. 

Using our proposed cross frame attention (third row), we see across all generations improved preservation of the object identities and their appearances. %\SN{with CF-Attn let's put more emphasize on good preservation of the object identities and their appearance}.
%
Finally, by combining both concepts (last row), we achieve the best temporal coherence.
%
%Finally, by combining both concepts (last row), we achieve the temporal coherence. 
For instance, we see the same background motifs and also about object identity preservation in the last four columns and at the same time a natural transition between the generated images.


\setcounter{table}{\value{figure}} 
\begin{table*}
\captionsetup{name=Figure}
        \centering
        \begin{tabular}{ M{23mm}M{7.4mm}M{7.4mm}M{7.4mm}M{9mm}M{7.4mm}M{7.4mm}M{7.4mm}M{9mm}M{7.4mm}M{7.4mm}M{7.4mm}M{9mm}}
            \multicolumn{13}{M{100mm}}{} \\
            & & & & & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/edge/0.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/edge/1.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/edge/2.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/edge/3.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/pose/0.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/pose/1.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/pose/2.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/pose/3.jpg}} \\

            \makecell{
              {\fontsize{7}{8}\selectfont \raisebox{0.5ex}{No Motion in Latents}} \\[-2pt]
              {\fontsize{7}{8}\selectfont \setlength{\baselineskip}{0.5ex} No Cross-Frame Attention}
            } & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq1/ours_wo_attn_wo_motion/0.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq1/ours_wo_attn_wo_motion/1.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq1/ours_wo_attn_wo_motion/2.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq1/ours_wo_attn_wo_motion/3.jpg}} &{\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/ours_wo_attn_wo_motion/0.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/ours_wo_attn_wo_motion/1.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/ours_wo_attn_wo_motion/2.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/ours_wo_attn_wo_motion/3.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/ours_wo_attn_wo_motion/0.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/ours_wo_attn_wo_motion/1.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/ours_wo_attn_wo_motion/2.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/ours_wo_attn_wo_motion/3.jpg}} \\

            \makecell{
              {\fontsize{7}{8}\selectfont \raisebox{0.5ex}{Motion in Latents}} \\[-2pt]
              {\fontsize{7}{8}\selectfont \setlength{\baselineskip}{0.5ex} No Cross-Frame Attention}
            } & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq1/ours_wo_attn/0.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq1/ours_wo_attn/1.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq1/ours_wo_attn/2.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq1/ours_wo_attn/3.jpg}} &{\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/ours_wo_attn/0.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/ours_wo_attn/1.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/ours_wo_attn/2.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/ours_wo_attn/3.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/ours_wo_attn/0.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/ours_wo_attn/1.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/ours_wo_attn/2.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/ours_wo_attn/3.jpg}} \\            
            \makecell{
              {\fontsize{7}{8}\selectfont \raisebox{0.5ex}{No Motion in Latents}} \\[-2pt]
              {\fontsize{7}{8}\selectfont \setlength{\baselineskip}{0.5ex} Cross-Frame Attention}
            } & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq1/ours_wo_motion/0.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq1/ours_wo_motion/1.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq1/ours_wo_motion/2.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq1/ours_wo_motion/3.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/ours_wo_motion/0.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/ours_wo_motion/1.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/ours_wo_motion/2.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/ours_wo_motion/3.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/ours_wo_motion/0.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/ours_wo_motion/1.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/ours_wo_motion/2.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/ours_wo_motion/3.jpg}} \\
            \makecell{
              {\fontsize{7}{8}\selectfont \raisebox{0.5ex}{Motion in Latents}} \\[-2pt]
              {\fontsize{7}{8}\selectfont \setlength{\baselineskip}{0.5ex} Cross-Frame Attention}
            } & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq1/ours/0.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq1/ours/1.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq1/ours/2.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq1/ours/3.jpg}} &{\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/ours/0.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/ours/1.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/ours/2.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq2/ours/3.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/ours/0.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/ours/1.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/ours/2.jpg}} & {\includegraphics[width=.06\textwidth]{arxiv_figures/fig10/seq3/ours/3.jpg}} \\                      
        \end{tabular}
        \caption{Ablation study showing the effect of our proposed components for text-to-video and text-guided video editing. Additional ablation study results are provided in the appendix.}
        \label{fig:abltion_study}
    \end{table*}
\setcounter{figure}{\value{table}} 

% \begin{figure*}[h!]
%     \centering
%     \includegraphics[width = .9\textwidth]{AblationStudy.jpg}
%     \caption{Ablation study showing the effect of our proposed components for text2video and text-guided video editing.}
%     \label{fig:abltion_study}
% \end{figure*}

%------------------------------------------------------------------------
\section{Conclusion}

    In this paper, we addressed the problem of zero-shot text-to-video synthesis and proposed a novel method for time-consistent video generation. Our approach does not require any optimization or fine-tuning, making text-to-video generation and its applications affordable for everyone. We demonstrated the effectiveness of our method for various applications, including conditional and specialized video generation, and Video Instruct-Pix2Pix, \ie, instruction-guided video editing. Our contributions to the field include presenting a new problem of zero-shot text-to-video synthesis, showing the utilization of text-to-image diffusion models for generating time-consistent videos, and providing evidence of the effectiveness of our method for various video synthesis applications. We believe that our proposed method will open up new possibilities for video generation and editing, making it accessible and affordable for everyone.
%------------------------------------------------------------------------


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\newpage
\clearpage

% \title{Appendix}
% \author{}
% \date{}
% \maketitle
% \vspace{-4mm}

\section*{\Large Appendix}


This supplementary material provides additional results to show the quality of our text-to-video generation method and its applications, and the importance of individual parts of our approach. 


The quality of our text-to-video method without additional conditioning or specialization is investigated further in Sec.~\ref{apendix:unconstrained-text2video}. To this end, qualitative results are presented and compared to the only publicly available state-of-the-art competitor CogVideo \cite{hong2022cogvideo}. In order to analyze the relevance of our proposed procedures, several ablation studies are performed qualitatively.


Sec.~\ref{apendix:t2v_edge_guidance} supplements our paper by  elaborating results for conditional text-to-video generation guided by pose information. 
In Sec.~\ref{apendix:t2v_pose_guidance} we discuss more results of conditional text-to-video generation guided by edge information.
% In addition we discuss specialized text-to-video generation guided by edge information by using pre-trained Dreambooth \cite{ruiz2022dreambooth} models.
Qualitative results and extensive ablation studies are presented. 

Finally, Sec.~\ref{apendix:t2v_editing} provides additional qualitative results and more comparison with a recent state-of-the art method Tune-A-Video \cite{tune-a-video} for the instruction-guided video editing task and compares to our Video Instruct-Pix2Pix method.

\input{supplemental_chapters/text2video_no_guidance}
\input{supplemental_chapters/text2video_edge_guidance}
\input{supplemental_chapters/text2video_edge_db_guidance}
\input{supplemental_chapters/text2video_pose_guidance}
\input{supplemental_chapters/text2video_guidance_ablation}
\input{supplemental_chapters/text2video_editing}
\input{supplemental_chapters/text2video_editing_comparison}

\end{document}