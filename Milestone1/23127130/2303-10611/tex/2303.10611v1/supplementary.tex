% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularray}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{float} 
\usepackage{subfigure}
\usepackage{subcaption}
\usepackage[font={small}]{caption}
\usepackage{color}
\usepackage{wrapfig}

% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}


\begin{document}
%
\title{Supplementary materials of DuDoRNeXt: A hybrid model for dual-domain undersampled MRI reconstruction}
\author{Paper ID: 1920}
%
\authorrunning{Paper ID: 1920}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
%\institute{Anonymous Organization\\
%\email{**@******.***}}
%
\titlerunning{DuDoRNeXt}
\maketitle              % typeset the header of the contribution
%
\section{Details of Design Motivation}
% \subsubsection{Building Block Revisit}
% The most important element of CNNs is convolution, a local weighted sum of values in a local receptive field $K_1*K_2$ of input image $I_{in}(\cdot,\cdot)$ with convolution kernel $K$:
% \begin{equation}
%     I_{out}(i, j)=\sum_{k_1=1}^{K_1} \sum_{k_2=1}^{K_2} I_{in}(i+k_1-1, j+k_2-1) K(k_1, k_2).
% \end{equation}
% % A typical setting of ConvNets is to vertically pile up convolution layers, among which are spatial dimension transformation operations, non-linear activation and normalization layers.
% Multi-head Self-Attention (MSA) is the basic component of ViTs, an extension of Self-Attention (SA) in which $h$ times SA operations are run. SA can be represented by 
% \begin{equation}
%     \boldsymbol{z}_{j}=\sum_{i} \operatorname{Softmax}\left(\frac{\boldsymbol{Q K^T}}{\sqrt{d}}\right)_{i} \boldsymbol{V}_{i, j},
% \end{equation}
% where $Q, K, V \in \mathbb{R}^{(hw)*d}$ are query, key, value respectively and $d$ are their channel numbers. $hw$ are the number of patches. In a convolution's point of view, it is a global aggregation of spatial tokens with data-specific normalized importances\cite{park2022how}. Swin Transformer \cite{liu2021swin} further adopts shifted local window MSA with relative positional bias $B$, achieving a much better trade-off between model complexity and flexibility by introducing more inductive bias in ViTs:
% \begin{equation}
% \boldsymbol{z}_{j}=\sum_{i} \operatorname{Softmax}\left(\frac{\boldsymbol{Q K^T}}{\sqrt{d}}+B\right)_{i} \boldsymbol{V}_{i, j},
% \end{equation}
% where $Q, K, V \in \mathbb{R}^{M^2*d}$, $M^2$ is the number of patches in a window, and $B\in\mathbb{R}^{M^2*M^2}$ is parameterized by a smaller-sized bias matrix $\hat{B}\in\mathbb{R}^{(2M-1)*(2M-1)}$.

% Besides different intrinsic properties and behaviours, CNNs and ViTs have different suitability for MRI reconstruction, leading to a fundamental trade-off of CNNs and ViTs.
\subsubsection{ViTs benefit dual domain reconstruction with larger receptive fields}
% This is a point that has been raised in \cite{zhou2023dsformer,huang2022swinmr,lyu2022dudocaf}, yet a detailed quantitative analysis with respect to the nature of dual domain reconstruction problem is given here. 
% Organs scanned by MRI (e.g brains, knees) are usually rich in contextual details, where a larger receptive field can improve the image reconstruction performance in general. On the other hand, Undersampled MRI reconstruction is a K-space recovery problem and has been considered as a local interpolation in many CS algorithms like GRAPPA. However, 
When MRI is under-sampled with a higher acceleration rate, the average number of measurements in a fixed local region of K-space decreases drastically.
\begin{wrapfigure}[14]{l}
   \centering   \includegraphics[width=0.45\textwidth]{Figure/prob.jpg}
   \caption{$P$ for a successful interpolation w.r.t. receptive field size $k$ and acceleration rate $a$ when $R_{acs}$=0.125.} 
   \label{2}
\end{wrapfigure}
Here is an analysis under 1D Cartesian sampling with acceleration rate $a$. Ratio of auto-calibration region $R_{acs}$ is the central, fully-sampled fraction of k-space, while the other region is sampled randomly. For a local non-auto-calibration region $K$ of size $k*k$, the minimal guarantee for doing an interpolation operation can be described by the possibility $P$ of including at least two MRI scanning lines in K:
$   P = 1-(p)^k-C_{1}^{k}(p)^{k-1}(1-p), p = 1-\frac{\frac{1}{a}-R_{acs}}{1-R_{acs}},$
where $p$ is the possibility that any line of K is not measured and determined by $R_{acs}$ and $a$. An illustration of the distribution of $P$ w.r.t $(K,a)$ given a fixed $R_{acs}$ is shown in Figure \ref{2}. As a result, a larger receptive field is crucial for highly-undersampled MRI reconstruction. CNNs achieve larger receptive fields using dilation convolution or larger kernel. However, dilation does not lead to an increase of lower bound for doing interpolation in k-space since dilation increases receptive field with aligned atrous part. Also, piling up large convolution kernels harms model convergence.

% \subsubsection{Fundamental Trade-off of CNNs and ViTs}

% Assume $C_{in} = C_{out} = C$. Global MSA operations' complexity is $(4hwC^2+2(hw)^2C)$, quadratic with respect to (hw), while W-MSA decrease it into $(4hwC^2+2M^2hwC)$, which is often larger compared with convolution with $(K^2(hw)C^2)$ computational complexity. Also, ViT models are often more demanding for GPU memory compared with a ConvNet model of fair amount of parameters. This leads to a fundamental trade-off between convolution and MSA in efficiency and accuracy.
% In other words, ViTs are more performant at the cost of training difficulty and costly computation.


% \subsubsection{Vertical Layout Design Hybridizing CNNs and ViTs}
% ViTs are more performant at the cost of training difficulty and computation cost, leading to a fundamental trade-off between convolution and MSA in efficiency and accuracy. A naive hybrid model simply concatenate CNN stages with Transformer stages sequentially. 
% This seems to be an effective way since empirical studies have found that ViTs perform better at the later stage of a hybrid model; patch merging of ViT or common downsampling layers can reduce the spatial dimensions in later stages, reducing computational burden. However, in MRI reconstruction, a columnar structure and densely connection is often used for maximal detail preservation under tiny models, thus using a densely connected ViTs in later stage does not ease computation burden compared with ViT in a front stage. 
% In practice, this naive strategy does not improve the performance of DuDoRNet effectively. We believe that a lack of deep feature-level interaction accounts for this result. Also, this naive hybrid strategy does not solve the problem of ViT's heavy computation well. An intra-stage, domain-specified design and delicate tuning on the portion of Transformer in the hybrid model is needed for balancing efficiency and effectiveness.

% \noindent\textbf{The intuition behind X-TL}
% An explanation for the chioce of $(\theta_i, \theta_k)$ is that image data has richer correspondence among neighboring pixels and a higher level of representation like shape can be learned in a progressive way given previous "collective knowledge"; while a pixel in 2D K-space denotes to a frequency component defined on a 2D frequency basis and does not benefits from a higher semantic representation from neighboring pixels a lot compared with image data. As a result, the collective knowledge is not as fundamental for the next convolution layer and a narrow representation given by small $k_0$ hinders the network from learning high dimension features towards itself. 

%  Directly feeding convolution features to Transformer Block without tuning hidden dimension increase computation significantly. $F_i^C$ has $k_0+k*(l-1)$ feature maps, where $k_0$, $k$ and $l$ denotes the number of channels in the first layer, growth rate and current layer number. To address this issue, we propose a \textbf{Domain-specific Transition Layer(X-TL)} to compress $c$ feature maps into $\lfloor \theta c \rfloor$ using 1x1 convolution, where $\theta\in(0,1]$ differs for image and K-space. In practice, we adopt $\theta_I,\theta_K=(1,\frac{2}{3})$ since we observe a great reduction in performance after adding Transition Layer for image recovery; while K-space recovery network benefits from K-TL since a larger number of feature maps $(k_0)$ can be supported in each convolution operation, leading an over-complete representation of K-space feature. This discovery corresponds to the result in DenseNet\cite{huang2017densely}. An explanation is that image data has richer correspondence among neighboring pixels and a higher level of representation like shape can be learned in a progressive way given previous "collective knowledge"; while a pixel in 2D K-space denotes to a frequency component defined on a 2D frequency basis and does not benefits from a higher semantic representation from neighboring pixels a lot compared with image data. As a result, the collective knowledge is not as fundamental for the next convolution layer and a narrow representation given by small $k_0$ hinders the network from learning high dimension features towards itself. [how to illustrate the correspondence of feature maps?]
 

% \noindent\textbf{I-PLDE: Image parallel local detail enhancement.}
% We notice that the vertical hybrid layout design leads to unsatisfiable performance on detail recovery. A possible reason is that MSAs at the end of each stage act as spatial smoothing and aggregation\cite{park2022how}, thus neglecting details unavoidably. To this end, we propose the I-PLDE module, a parallel branch emphasizing local detail on the top of the vertical hybrid design, inspired by the “divide-and-conquer” idea in \cite{xu2021vitae,zhang2023vitaev2}.%: modeling locality and long-range dependencies in parallel and fusing the features to account for both. Different from ViTAE, we adopt stage-level parallel connection and use sequential dilated convolution (DRDB) for pyramid feature extraction instead of multiple parallel dilated convolution of different dilation rate. This design is customized to tiny models since a wide network design leads to a less stable training process and lower feature representation ability.
% I-PLDE consists of a 1x1 convolution to match hidden dimension with its parallel branch, three stacked depthwise convolution layers and an window embedding operation. SiLU is used for non-linear activation following its original design\cite{xu2021vitae,zhang2023vitaev2}. The output of I-PLDE $F_i^{CE} $ is added after MSA for preserving details. By including I-PLDE, the expression (\ref{CE}) is corrected into:
% \begin{equation}
%     F_i^{H'}=W-MSA(LN(WinEmb(F_i^C, w)))+WinEmb(F_i^C,w)+F_i^{CE} 
% \end{equation}

% % The reason why ViTAE's design is successful in their scenarios is that the tiniest model it constructs has about 5 times parameters of ours and the ImageNet dataset is much larger than any open source MRI dataset for reconstruction.

% \noindent\textbf{K-GLIM: K-space global initiation.}
% %As shown in Figure \ref{2}, the lower bound for doing interpolation of our model at high acceleration rate $(a=6,8)$ is below $50\%$. Also, 
% MSA is placed at the end of each stage in our hybrid model, making it difficult for earlier layers' interpolation. To this end, we propose K-space global initialization module (K-GLIM) in the beginning part of models. K-GLIM is applied on the transformed features of two convolution layers in SFE, providing channelwise interaction and a global view. %It corresponds to expression (\ref{glim}). 
% The main part of K-GLIM is a Channel-wise Multi-head Self-attention (C-MSA). Instead of performing pixel-level or patch-level attention in a conventional spatial attention way, C-MSA is performed on the transpose of pixel-level tokens. Similar to MSA, C-MSA is an extension of Self-Attention (C-SA) in which $h$ times SA operations are run. C-SA can be expressed as
% %\begin{equation}
% $\boldsymbol{z}_{j}^c=\sum_{i} \operatorname{Softmax}\left(\frac{\boldsymbol{Q^T K}}{\sqrt{d}}\right)_{i} \boldsymbol{V}_{i, j}^T$,
% %\end{equation}
% and its computational complexity is $O(6(hw)C^2)$, linear to $hw$. C-MSA naturally captures global information and interactions for visual recognition tasks and complements (windowed-)spatial attention\cite{ding2022davit}. Compared with channel-wise convolution, it is data-specific and fine-grained.

\section{Experiment on the IXI dataset}
\noindent\textbf{Ablation study on Number of Recurrent times.}
\begin{table}
\centering
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{lllll} 
\toprule
Recurrent Time   & 1     & 2     & 3     & 4      \\
DuDoRNeXt w/o T2 & 33.61 & 33.93 & 34.11 & 34.14  \\
DuDoRNeXt w/ T2  & 40.58 & 40.94 & 41.05 & 41.09  \\
\bottomrule
\end{tabular}}
\end{table}
This set of experiments is run on a NVIDIA A100 with 80GB memory while other settings are consistent with the main body.

\noindent\textbf{Quantitative Results of 5x and 7x of PD reconstruction.}
\begin{table}
\centering
\resizebox{0.82\textwidth}{!}{
\centering
\begin{tblr}{
  cell{1}{2} = {c=3}{},
  cell{1}{5} = {c=3}{},
  vline{1-3,6} = {1}{},
  vline{1-2,5,8} = {1-17}{},
  hline{1,3-4,10,14,18} = {-}{},
  hline{2},{2-19}{}
}
{  } & 5x         &               &        & 7x         &               &         \\
                              & PSNR       & SSIM          & MSE    & PSNR       & SSIM          & MSE     \\
Zero Filling                  & 24.91±1.81 & 80.45±6.25 & 237.77 & 24.45±1.80 & 80.45±6.29 & 264.144 \\
Unet                          & 30.33±2.01 & 94.69±2.50 & 73.45  & 28.90±2.02 & 93.57±3.03 & 102.15  \\
Dense-Unet                    & 30.93±2.19 & 95.44±2.26 & 65.56  & 29.33±2.10 & 94.02±2.87 & 93.49   \\
OUCR                          & 31.02±2.21 & 95.57±2.23 & 64.44  & 29.43±2.12 & 94.14±2.84 & 91.39   \\
SwinIR                        & 31.11±2.25 & 95.62±2.21 & 63.21  & 29.41±2.19 & 94.16±2.87 & 92.36   \\
DuDoRNet\_I                   & 31.33±2.23 & 95.80±2.13 & 60.09  & 29.70±2.16 & 94.46±2.73 & 86.49   \\
Ours\_I                       & \textcolor{red}{31.62±2.30} & \textcolor{red}{96.02±2.08} & \textcolor{red}{40.42}  & \textcolor{red}{29.93±2.23} & \textcolor{red}{94.66±2.70} & \textcolor{red}{82.89}   \\
Dual-DenseUnet                & 31.18±2.21 & 95.60±2.15 & 62.02  & 29.38±2.09 & 93.94±2.83 & 92.15   \\
Dual-SwinIR                   & 31.65±2.39 & 96.01±2.11 & 57.42  & 29.71±2.28 & 94.22±2.84 & 87.67   \\
DuDoRNet                      & 31.39±2.28 & 95.74±2.14 & 59.81  & 29.62±2.19 & 94.15±2.82 & 88.54   \\
Ours (w/o T2)                          & \textcolor{red}{32.14±2.43} & \textcolor{red}{96.31±2.00} & \textcolor{red}{53.05}  & \textcolor{red}{30.17±2.31} & \textcolor{red}{94.77±2.71} & \textcolor{red}{79.66}   \\
Dual-DenseUnet                & 39.56±2.56 & 99.12±0.60 & 9.623  & 38.66±2.52 & 98.98±0.68 & 11.71   \\
Dual-SwinIR                   & 39.94±2.62 & 99.18±0.58 & 8.939  & 39.02±2.56 & 99.05±0.65 & 10.09   \\
DuDoRNet                      & 39.76±2.59 & 99.15±0.59 & 9.234  & 38.85±2.54 & 99.02±0.67 & 11.27   \\
Ours                          & \textcolor{red}{40.22±2.66} & \textcolor{red}{99.22±0.56} & \textcolor{red}{8.449}  & \textcolor{red}{39.25±2.60} & \textcolor{red}{99.07±0.65} & \textcolor{red}{10.72}   
\end{tblr}
}
\end{table}

\noindent\textbf{Qualitative Results of PD reconstruction.}
\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{Figure/supp.png}
\caption{Upper line: Ground Truth and reconstructed PD by DuDoRNet, Dual-SwinIR, DuDoRNeXt w/o T2, DuDoRNeXt. The green box is an instance of ROIs with rich structural details. Bottom line: zoom-in comparisons.}
\label{supp}
\end{figure}


%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%


\end{document}

