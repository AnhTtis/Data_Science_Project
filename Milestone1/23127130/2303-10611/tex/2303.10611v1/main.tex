% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{./tabularray}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{float} 
\usepackage{amsmath}
\usepackage{subfigure}
% \usepackage{subcaption}
\usepackage[font={small}]{caption}
\usepackage{color}
\usepackage{wrapfig}

% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}


\begin{document}
%
\title{DuDoRNeXt: A hybrid model for dual-domain undersampled MRI reconstruction}
% \author{Paper ID: 1920}
%
% \authorrunning{Z. Gao}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
%\institute{Anonymous Organization\\
%\email{**@******.***}}
%
\titlerunning{DuDoRNeXt}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Ziqi Gao\inst{1}\and
S. Kevin Zhou\inst{1}}
%
% \authorrunning{Gao et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{1 Medical Imaging, Robotics, and Analytic Computing Laboratory and Engineering (MIRACLE) Center, School of Biomedical Engineering, Suzhou Institute for Advance Research, University of Science and Technology of China, Suzhou, China}
\maketitle              % typeset the header of the contribution
%%systematically analyze the design of a hybrid model for MRI reconstruction and
\begin{abstract}
Undersampled MRI reconstruction is crucial for accelerating clinical scanning procedures. Recent deep learning methods for MRI reconstruction adopt CNN or ViT as backbone, which lack in utilizing the complementary properties of CNN and ViT. In this paper, we propose DuDoRNeXt, whose backbone hybridizes CNN and ViT in an domain-specific, intra-stage way. Besides our hybrid vertical layout design, we introduce domain-specific modules for dual-domain reconstruction, namely image-domain parallel local detail enhancement and k-space global initialization. We evaluate different conventions of MRI reconstruction including image-domain, k-space-domain, and dual-domain reconstruction with a reference protocol on the IXI dataset and an in-house multi-contrast dataset. DuDoRNeXt achieves significant improvements over competing deep learning methods.
\keywords{Dual-domain MRI reconstruction \and Vision Transformer \and Neural network \and Hybrid model}
\end{abstract}
%
%
%
\section{Introduction}
Magnetic resonance imaging (MRI) is an non-invasive and flexible imaging modality widely used in clinical practice. %It can produce a variety of desired contrasts, revealing different kinds of tissue. 
A tricky problem in utilizing MRI is that complete K-space measurements lead to unbearable acquisition time while fewer measurements lead to aliasing and blurring in the image. Undersampled MRI reconstruction aims to reconstruct the high-quality, clean MRI image from its low-quality, aliased counterpart. Previously, Compressed Sensing (CS) and Parallel Imaging (PI) accelerate MRI reconstruction for 2-3 times. Since the revolutionary work\cite{wang2016accelerating}, convolutional neural networks (CNN) have become the primary workhorse for undersampled MRI reconstruction.

Many CNN-based methods \cite{yang2017dagan,jin2017unetmri,quan2018compressedgan} focus on elaborate architecture designs such as residual learning \cite{he2016res} and dense connections\cite{huang2017densely} stemming from UNet \cite{ding2019rdnunet} or other existing baseline methods. Customization of conventional CNNs further benefited MRI reconstruction, including K-space data consistency (DC) \cite{schlemper2017dc,qin2018dc1}, dual-domain recurrent learning \cite{eo2018kiki,zhou2020dudornet}, over-complete representation \cite{guo2021oucr}. Recently, Transformer \cite{vaswani2017attention,dosovitskiy2020vit} has been considered as an alternative to CNN. Its Multi-head Self-Attention (MSA) mechanism captures long-range interactions among contexts globally or inside local windows \cite{liu2021swin}. As the success of Transformer is now indisputable in computer vision, Transformer has shown great potential for undersampled MRI reconstruction as well \cite{huang2022swinmr,zhou2023dsformer,guo2022reconformer,feng2021task,lyu2022dudocaf}.
% DuDoRNet \cite{zhou2020dudornet} adopts a dual-domain\cite{eo2018kiki} recurrent learning strategy and used a dilated residual dense CNN to eliminate non-local aliasing artifact in image domain. Over-and-under-complete convolutional RNN \cite{guo2021oucr} restrains the receptive field and gives a special attention in local structures. Schlemper et al \cite{schlemper2017dc,qin2018dc1} developed a deep cascade of CNN with K-space data consistency layer (DC) to ensure the consistency on K-space measurements.
%  Recently, Transformer \cite{vaswani2017attention,dosovitskiy2020vit} has been considered as an alternative to CNN. Its Multi-head Self-Attention (MSA) mechanism captures long-range interactions among contexts globally; while Swin Transformer \cite{liu2021swin} introduces a shifted window MSA to achieve lightweight local attention, enabling a wide range of applications. As the success of Transformer \cite{dosovitskiy2020vit} is now indisputable in computer vision, Transformer has shown great potential for MRI reconstruction as well. SwinMR \cite{huang2022swinmr} and DSFormer \cite{zhou2023dsformer} pioneer Swin transformer\cite{liang2021swinir} as a competitive backbone in single-contrast and multi-contrast MRI reconstruction, respectively. ReconFormer \cite{guo2022reconformer} develops a recurrent pyramid Transformer. Feng et al. \cite{feng2021task} proposed a task transformer network for joint MRI reconstruction and super-resolution. DuDoCAF \cite{lyu2022dudocaf} and MARIO \cite{feng2022multi} leverage the query-key mechanism of MSA for deep feature-level fusion in multi-contrast MRI reconstruction.


Yet performant, ViTs have not fully substituted CNNs as ViTs require a larger amount of training data due to a low inductive bias and have longer training schedules \cite{xu2021vitae,zhang2023vitaev2}. Furthermore, CNNs and ViTs have different emphases. Fig.~\ref{1} prevails their distinctive emphases on MRI reconstruction. DuDoRNet is a CNN model while Dual-SwinIR mostly consists of Swin Transformer blocks. DuDoRNet can extract small, isolated features better (the red one) while Dual-SwinIR generates sharper results for large structural details (the green one). Finally, DuDoRNet generates (sometimes wrong) details of higher frequency, which are seen especially in soft tissues; while Dual-SwinIR generates a smoother image. 
\begin{figure}[t]
\centering
\includegraphics[width=0.82\linewidth]{Figure/fig1_new.pdf}
\caption{(a) Ground truth and the reconstructed T2 images by a CNN model DuDoRNet, a ViT model Dual-SwinIR, and our hybrid model DuDoRNeXt. The green box is an instance of ROI with rich structures while the red one contains a small, isolated object. (b)(c) Zoom-in of two ROIs. (d) PSNR distribution of 36 reconstructed testing images.}  
\label{1}
\end{figure}
% Yet performant, ViTs have not fully substituted CNNs for several reasons. ViTs naturally require a larger amount of training data due to low inductive bias and have longer training schedules \cite{xu2021vitae,zhang2023vitaev2}. Furthermore, the intrinsic properties of convolution and MSA lead to different behaviours of CNNs and ViTs. Figure \ref{1} prevails distinctive emphasis of ViTs and CNNs on MRI reconstruction. DuDoRNet is a CNN model while SwinMR mostly consists of Swin Transformer Blocks. DuDoRNet can extract small, isolated features better (the red one) while SwinMR generates sharper results for large structural details (the green one). Finally, DuDoRNet generates (sometimes wrong) details of higher frequency, which can be seen especially in regions of soft tissues; while SwinMR generates a smoother image.
% In recent works in decomposing Transformer from the basic theory \cite{park2022how} to empirical network design \cite{liu2022convnet,wang2022can}, a potential direction for modernizing deep learning models arises: \textbf{hybridizing CNNs and ViTs}. 
% The feasibility of developing a hybrid model depends on the complementary properties of CNN and ViT, which can be summarized from three levels:
% \begin{itemize}
%     \item \textit{Intrinsic properties}: Convolution has a strong inductive bias while MSA lacks such an inductive bias, making it weak in modeling local clues in resource-limited scenerios \cite{zhang2023vitaev2}. MSA is data-dependent and has a global receptive field, providing flexibility for models to adjust weight with respect to input.
%     \item \textit{Behaviours}: ViTs preserve global-structure \cite{wang2022dudotrans} and are particularly robust against high-frequency noises \cite{bhojanapalli2021vitnoise}; while CNNs are high-pass filters\cite{park2022how} and better at extracting local details \cite{zhang2023vitaev2}. 
%     \item \textit{Model Optimization} \cite{park2022how} : ViTs have non-convex, flat loss landscapes while CNNs have strong nearly-convex loss landscapes. The non-convex loss of ViTs can be suppressed using a large data regime, or hybridizing with CNNs. 
% \end{itemize}
In recent works in decomposing Transformer from the basic theory \cite{park2022how} to empirical network design ~\cite{liu2022convnet,wang2022can}, a potential direction for modernizing deep learning models arises: \textbf{hybridizing CNNs and ViTs}. 
 While several work~\cite{xu2021vitae,zhang2023vitaev2,park2022how} in computer vision show the effectiveness of hybrid structures, there is no research systematically studying a hybrid model for MRI reconstruction.
% . However, even the designed principle verified on mobile regimes constructs models of several millions of parameters, which is not quite suitable for clinical usage in MRI construction scenarios.  Also,

In our study, (1) we systematically study designing hybrid models for MRI reconstruction and propose %practical guidelines for 
a vertical layout design of hybrid MRI reconstruction models under a computational constraint; %s of $(GPU Memory \leq24GB,param\approx1M)$.
(2) we propose \textbf{DuDoRNeXt} with several domain-specific modules for MRI reconstruction, %to maximize the potential of a hybrid model, 
including image-domain parallel local detail enhancement and k-space global initiation; and (3) we test our model on multiple settings of MRI reconstruction including % including image-domain, k-space-domain, and dual domain undersampled MRI reconstruction 
guided by a reference protocol. The public IXI-dataset and in-house multi-contrast MRI dataset are used for evaluation and ablation study, respectively. The results show that our models exceed baseline comparison methods in all settings.

\section{Method}

\subsection{Undersampled MRI Reconstruction}
%\paragraph{Notation} 
Let $k_u  \in \mathbb{C}^{mn} $ and $k_f  \in \mathbb{C}^{mn} $ be the undersampled and fully-sampled k-space signal respectively; $i_u  \in \mathbb{C}^{mn} $, $i_f  \in \mathbb{C}^{mn} $, and $i_r  \in \mathbb{C}^{mn} $ be the undersampled, fully-sampled and reconstructed image signal respectively;  $M \in \mathbb{R}^{mn} $ be the binary k-space mask for acceleration.

The undersampled MRI reconstruction can be formulated as an image recovery problem \cite{wang2016accelerating,guo2021oucr,chen2020ode,lee2018mp,huang2022swinmr} with K-space DC as a regularisation term\cite{schlemper2017dc}:
\begin{equation}
    \mathop{\arg\min}\limits_{\theta_{i}} \left(\left\|i_{f}-\mathcal{P}_{i}\left(i_{u} ; \theta_{i}\right)\right\|_{2}^{2}\right. \\
    \left.+\lambda\left\|k_{u}-M \odot \mathcal{F}\left(\mathcal{P}_{i}\left(i_{u} ; \theta_{i}\right)\right)\right\|_{2}^{2}\right),
    \setlength\belowdisplayskip{3pt}
\end{equation}
where $\mathcal{F}$ is 2D discrete Fourier Transform and the approximation function $\mathcal{P}_{x}(\cdot;\theta_x)$ is used to predict a reconstructed signal $x_r$ given its parameter $\theta_x$ and any undersampled input.  
A few works \cite{zhou2020dudornet,eo2018kiki,lyu2022dudocaf} leverage the relationship between image and k-space domain using Fourier Transform pairs ($\mathcal{F}, \mathcal{F}^{-1}$) and transform MRI reconstruction into a multivariable optimization problem described as
\begin{equation}    
\begin{split}
  \mathop{\arg\min}\limits_{\theta_{i},\theta_{k}} & \left(\left\|k_{f}-\mathcal{P}_{k}\left(\mathcal{F}\left(\mathcal{P}_{i}\left(i_{u} ; \theta_{i}\right)\right) ; \theta_{k}\right)\right\|_{2}^{2}\right.          +\left\|i_{f}-\mathcal{P}_{i}\left(\mathcal{F}^{-1}\left(\mathcal{P}_{k}\left(k_{u} ; \theta_{k}\right)\right) ; \theta_{i}\right)\right\|_{2}^{2}  \\
 & +\left.\lambda\left\|k_{u}-M \odot \mathcal{F}\left(\mathcal{P}_{i}\left(\mathcal{F}^{-1}\left(\mathcal{P}_{k}\left(k_{u} ; \theta_{k}\right)\right) ; \theta_{i}\right)\right)\right\|_{2}^{2}\right),  
\end{split}
\end{equation}
and solve it using a dual-domain recurrent learning strategy~\cite{zhou2020dudornet}. 

% Yet most of existing works on designing the approximation function $\mathcal{P}_{x}(\cdot;\theta_x)$ using neural networks whose architectures mainly concentrate on image recovery problem and apply it on k-space. Yet we argue that a hand-crafted design on dual domain approximation functions is crucial for solving this multivariable optimization problem since image and k-space bear different properties and the tasks in two space can be differentiated further.

Recently, a growing number of works \cite{peng2020towards,xiang2018ultra,feng2022multi,zhou2020dudornet,zhou2023dsformer,lyu2022dudocaf} utilize a fast-to-acquire fully-sampled auxiliary MRI protocol to guide the reconstruction of a slow protocol.%, a promising direction for future MRI reconstruction. 

%\subsection{Towards Hybridizing CNNs and ViTs}
% \subsubsection{Building Block Revisit}
% The most important element of CNNs is convolution, a local weighted sum of values in a local receptive field $K_1*K_2$ of input image $I_{in}(\cdot,\cdot)$ with convolution kernel $K$:
% \begin{equation}
%     I_{out}(i, j)=\sum_{k_1=1}^{K_1} \sum_{k_2=1}^{K_2} I_{in}(i+k_1-1, j+k_2-1) K(k_1, k_2).
% \end{equation}
% % A typical setting of ConvNets is to vertically pile up convolution layers, among which are spatial dimension transformation operations, non-linear activation and normalization layers.
% Multi-head Self-Attention (MSA) is the basic component of ViTs, an extension of Self-Attention (SA) in which $h$ times SA operations are run. SA can be represented by 
% \begin{equation}
%     \boldsymbol{z}_{j}=\sum_{i} \operatorname{Softmax}\left(\frac{\boldsymbol{Q K^T}}{\sqrt{d}}\right)_{i} \boldsymbol{V}_{i, j},
% \end{equation}
% where $Q, K, V \in \mathbb{R}^{(hw)*d}$ are query, key, value respectively and $d$ are their channel numbers. $hw$ are the number of patches. In a convolution's point of view, it is a global aggregation of spatial tokens with data-specific normalized importances\cite{park2022how}. Swin Transformer \cite{liu2021swin} further adopts shifted local window MSA with relative positional bias $B$, achieving a much better trade-off between model complexity and flexibility by introducing more inductive bias in ViTs:
% \begin{equation}
% \boldsymbol{z}_{j}=\sum_{i} \operatorname{Softmax}\left(\frac{\boldsymbol{Q K^T}}{\sqrt{d}}+B\right)_{i} \boldsymbol{V}_{i, j},
% \end{equation}
% where $Q, K, V \in \mathbb{R}^{M^2*d}$, $M^2$ is the number of patches in a window, and $B\in\mathbb{R}^{M^2*M^2}$ is parameterized by a smaller-sized bias matrix $\hat{B}\in\mathbb{R}^{(2M-1)*(2M-1)}$.

% Besides different intrinsic properties and behaviours, CNNs and ViTs have different suitability for MRI reconstruction, leading to a fundamental trade-off of CNNs and ViTs.

%\subsubsection{ViTs benefit dual domain reconstruction with larger receptive fields}
% This is a point that has been raised in \cite{zhou2023dsformer,huang2022swinmr,lyu2022dudocaf}, yet a detailed quantitative analysis with respect to the nature of dual domain reconstruction problem is given here. 
%Organs scanned by MRI (e.g brains, knees) are usually rich in contextual details, where a larger receptive field can improve the image reconstruction performance in general. On the other hand, undersampled MRI reconstruction is a K-space recovery problem and has been considered as a local interpolation in many CS algorithms\cite{griswold2002grappa}. However, when MRI is under-sampled with a higher rate, the average number of measurements in a fixed local region decreases. Take 1D Cartesian sampling with acceleration rate $a$ as an example. Ratio of auto-calibration region $R_{acs}$ is the central, fully-sampled fraction of k-space, while the other region is sampled randomly. For a local non-auto-calibration region $K$ of size $k*k$, the minimal guarantee for doing an interpolation operation can be described by the possibility $P$ of including at least two MRI scanning lines in K:
%\begin{wrapfigure}[16]{l}
%    \centering   \includegraphics[width=0.45\textwidth]{Figure/prob.jpg}
%    \caption{Lower bound $P$ for doing interpolation successfully w.r.t. receptive field size $k$ and acceleration rate $a$ when $R_{acs}$=0.125} 
%    \label{2}
%\end{wrapfigure}
%\begin{equation}
%    P = 1-(p)^k-C_{1}^{k}(p)^{k-1}(1-p),
%\end{equation}
%where $p$ is the possibility that any line of K is not measured. $p$ is determined by $R_{acs}$ and $a$:
%\begin{equation}
%    p = 1-\frac{\frac{1}{a}-R_{acs}}{1-R_{acs}}.
%\end{equation}
%An illustration of the distribution of $P$ w.r.t $(K,a)$ given a fixed $R_{acs}$ is shown in figure \ref{2}. 

%ConvNets can achieve larger receptive field using dilation convolution or larger kernel. However, dilation does not lead to an increase of lower bound for doing interpolation in k-space since dilation increases receptive field with aligned atrous part. Also, piling up large convolution kernels harms model convergence.

% \subsubsection{Fundamental Trade-off of CNNs and ViTs}

% Assume $C_{in} = C_{out} = C$. Global MSA operations' complexity is $(4hwC^2+2(hw)^2C)$, quadratic with respect to (hw), while W-MSA decrease it into $(4hwC^2+2M^2hwC)$, which is often larger compared with convolution with $(K^2(hw)C^2)$ computational complexity. Also, ViT models are often more demanding for GPU memory compared with a ConvNet model of fair amount of parameters. In other words, ViTs are more performant at the cost of training difficulty and costly computation. This leads to a fundamental trade-off between convolution and MSA in efficiency and accuracy.

%\subsubsection{Vertical Layout Design Hybridizing CNNs and ViTs}
%ViTs are more performant at the cost of training difficulty and computation cost, leading to a fundamental trade-off between convolution and MSA in efficiency and accuracy. A naive hybrid model simply concatenate CNN stages with Transformer stages sequentially. This seems to be an effective way since empirical studies have found that ViTs perform better at the later stage of a hybrid model\cite{park2022how}; patch merging of ViT or common downsampling layers can reduce the spatial dimensions in later stages, reducing computational burden. However, in MRI reconstruction, a columnar structure and densely connection is often used for maximal detail preservation under tiny models, thus using a densely connected ViTs in later stage does not ease computation burden compared with ViT in a front stage. In practice, this naive strategy does not improve the performance of DuDoRNet effectively; while details can be seen in ablation study. We believe that a lack of deep feature-level interaction accounts for this result. Also, this naive hybrid strategy does not solve the problem of ViT's heavy computation well. An intra-stage, domain-specified design and delicate tuning on the portion of Transformer in the hybrid model is needed for balancing efficiency and effectiveness.

\subsection{DuDoRNeXt: Towards Hybridizing CNNs and ViTs}

\subsubsection*{Design motivation.} The design of DuDoRNeXt is motivated by two findings: (i) 
ViTs benefit dual domain reconstruction with larger receptive fields and (ii) vertical layout design works better when hybridizing CNNs and ViTs in an intra-stage manner. The details behind these motivations are elaborated in supplementary materials. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{Figure/flow.png}
    \caption{Framework of a recurrent block of DuDoRNeXt. It is constructed by a domain-specific Shallow Feature Extraction (X-SFE), Global Feature Refinement(GFR) and 4-stage domain-specific hybrid building blocks(X-BB).  Building blocks' color follow the notation in the color table. Convolution is followed by ReLU unless noted.} 
    \label{framework}
\end{figure}

\noindent\textbf{Architecture description.}
Compared with DuDoRNet\cite{zhou2020dudornet}, we improve dual domain recurrent blocks using an intra-stage CNN-ViT hybrid strategy and customize global and local structure based on domain-specific properties. Our model is illustrated in Figure \ref{framework}, including domain-specific Shallow Feature Extraction (X-SFE), Global Feature Refinement(GFR) and 4-stage domain-specific hybrid building blocks (X-BB) as backbone. Global residual learning and global feature fusion are preserved in our model. The overall pipeline goes as follow:
\begin{equation}
F_{-1} = Conv^3(x_u), F_{0}^{C} = Conv^3(F_{-1}).
\end{equation}
where $Conv^k$ denotes a convolution operation with kernel size $k*k$ and $F_{-1}$ denotes the first extracted feature used for global residual learning.  $F_{0}^{C}$ denotes the intermediate extracted feature by convolution. The second extracted feature $F_{0}$ is the input of X-$S_i$. For image domain, $F_{0} = F_{0}^{C} $.
% \begin{equation}
% F_{0} = F_{0}^{C} = Conv^3(F_{-1}), 
% \end{equation}
% where $F_{0}^{C}$ denotes the intermediate extracted feature by convolution. 
For K-space, a Global Initiation Module (K-GLIM) is introduced based on the idea of a sketchy K-space initiation:
%\begin{equation}
$F_{0} = P_{K-GLIM}(F_{0}^C) = P_{K-GLIM}(Conv^3(F_{-1}))$.
%\label{glim}
%\end{equation}
Global Feature Fusion (GFF) fuses features from X-BB's and is input for global residual learning:
\begin{equation}
F_i = P_{X-S_i}(F_i-1), x_r = P_{GFR}(F_{-1}+P_{GFF}(concat(F_1,F_2,F_3,F_4))),
\end{equation}
%\begin{equation}
% $x_r = P_{GFR}(F_{-1}+P_{GFF}(concat(F_1,F_2,F_3,F_4)))$,
%\end{equation}
where GFF consists of 1x1 and 3x3 convolutions and GFR consists of two 3x3 convolutions, creating the refined reconstructed image/K-space $x_r$.

\noindent\textbf{X-BB: Domain-specific hybrid building block.}
The design of X-BB is shown in the lower right part of Figure \ref{framework}. It is constructed by vertically stacking Dilated Residual Dense Block (DRDB) and Spatial Local Transformer Block followed by a Hybrid Local Feature Fusion (HLFF).

Following DuDoRNet, we use DRDB for effective local feature extraction, while our DRDB composes a smaller feature pyramid using one less atrous convolution layers with dilation rate of 1, 2, and 4, expressed by:
%\begin{equation}
$F_i^C = P_{DRDB}(F_{i-1})$, 
%\end{equation}
where $F_i^C$ denotes convolutional features in the $i_{th}$ stage. Then, a Spatial Local Transformer Block is used to aggregate convolution features $F_i^C$ in a large local window, whose main part is Windowed Multi-head Self-Attention (W-MSA): 
\begin{equation}
    F_i^{H'}=W-MSA(LN(WinEmb(F_i^C, w)))+WinEmb(F_i^C,w), 
    \label{CE}
\end{equation}
\begin{equation}
    F_i^{H}=WinUnemb(FFN(LN(F_i^{H'})+F_i^{H'}), 
\end{equation}
 where $w$ is window size and set to 16 in our model. $F_i^{H}$'s theoretical receptive field is 31, forming a larger and more dynamic scale pyramid than DuDoRNet. Then, convolution features $F_i^C$ and hybrid features $F_i^{H}$ is concatenated to \textbf{Hybrid Local Feature Fusion (HLFF)}, adaptively fusing hybrid features with convolution and Squeeze-and-Excitation \cite{hu2018squeeze} layers. The final result is acquired using a local residual learning to the output of HLFF by adding the residual connection of X-BB's input:
 \begin{equation}
     F_i = P_{HLFF}(concat(F_i^C, F_i^H)) + F_{i-1},
 \end{equation}
and $F_i^C$ has $k_0+k*(l-1)$ feature maps, where $k_0$, $k$ and $l$ denotes the number of channels in the first layer, growth rate and current layer number. 

Directly feeding convolution features $F_i^C$ to Transformer Block without tuning hidden dimension largely increase computation.  To address this issue, we propose a \textbf{domain-specific transition layer (X-TL)} to compress $c$ feature maps into $\lfloor \theta c \rfloor$ using 1x1 convolution, where $\theta\in(0,1]$ differs for image and K-space. In practice, we adopt $(\theta_I,\theta_K)=(1,\frac{2}{3})$ since we observe a great reduction in performance after adding Transition Layer for image recovery network; while K-space recovery network benefits from K-TL with wider convolution layers, leading an richer representation of K-space feature in each convolutional layer. This corresponds to the result in DenseNet\cite{huang2017densely}, where image-domain DenseNet's convolution layers can be very narrow.
 % The intuition behind this result can be found in supplementary materials.
 

\noindent\textbf{I-PLDE: Image parallel local detail enhancement.}
We notice that the vertical hybrid layout design leads to unsatisfactory performance in detail recovery. A possible reason is that MSAs at the end of each stage act as spatial smoothing and aggregation\cite{park2022how}, thus neglecting details unavoidably. To this end, we propose the I-PLDE module, a parallel branch emphasizing local detail on the top of the vertical hybrid design, inspired by the “divide-and-conquer” idea in \cite{xu2021vitae,zhang2023vitaev2}.%: modeling locality and long-range dependencies in parallel and fusing the features to account for both. Different from ViTAE, we adopt stage-level parallel connection and use sequential dilated convolution (DRDB) for pyramid feature extraction instead of multiple parallel dilated convolution of different dilation rate. This design is customized to tiny models since a wide network design leads to a less stable training process and lower feature representation ability.
I-PLDE consists of a 1x1 convolution to match hidden dimension with its parallel branch, three stacked depth-wise convolution layers and an window embedding operation. SiLU is used for non-linear activation following the convention in \cite{xu2021vitae,zhang2023vitaev2}. The output of I-PLDE $F_i^{CE} $ is added after W-MSA for preserving details. By including I-PLDE, the expression (\ref{CE}) is corrected into:
\begin{equation}
    F_i^{H'}=W-MSA(LN(WinEmb(F_i^C, w)))+WinEmb(F_i^C,w)+F_i^{CE}.
\end{equation}

% The reason why ViTAE's design is successful in their scenarios is that the tiniest model it constructs has about 5 times parameters of ours and the ImageNet dataset is much larger than any open source MRI dataset for reconstruction.

\noindent\textbf{K-GLIM: K-space global initiation.}
%As shown in Figure \ref{2}, the lower bound for doing interpolation of our model at high acceleration rate $(a=6,8)$ is below $50\%$. Also, 
Missing K-space recovery is an interpolation problem. MSAs of larger receptive fields are placed at the end of each stage, making it difficult for earlier layers' interpolation. To this end, we propose K-space global initialization module (K-GLIM) placing at the beginning of DuDoRNeXt. K-GLIM is applied on the transformed features of two convolution layers in SFE, providing channel-wise interaction and a global view. %It corresponds to expression (\ref{glim}). 
The main part of K-GLIM is a Channel-wise Multi-head Self-attention (C-MSA). Instead of performing pixel-level or patch-level attention in a conventional spatial attention way, C-MSA is performed on the transpose of pixel-level tokens. Similar to MSA, C-MSA is an extension of Self-Attention (C-SA) in which $h$ times SA operations are run. C-SA can be expressed as
%\begin{equation}
$\boldsymbol{z}_{j}^c=\sum_{i} \operatorname{Softmax}\left(\frac{\boldsymbol{Q^T K}}{\sqrt{d}}\right)_{i} \boldsymbol{V}_{i, j}^T$,
%\end{equation}
and its computational complexity is $O(6(hw)C^2)$, linear to $(hw)$. C-MSA naturally captures global information and interactions for visual recognition tasks and complements (windowed-)spatial attention\cite{ding2022davit}. Compared with channel-wise convolution, it is data-specific and fine-grained.


% \subsubsection{Positional Encoding}
% [I think I should remove it and put the experiment in the ablation study, since I cannot get consistent result in different datasets.]

\section{Experiment}
\subsection{Settings and Results}
\noindent\textbf{Dataset and training.} Our evaluation is carried out on the Multi-Contrast IXI dataset\footnote{https://brain-development.org/ixi-dataset/, CC BY-SA 3.0 license}. We use all 575 subjects with paired T2-PD and uniformly sample 14 slices from each subject volume. We split the dataset patient-wise into training, validation and testing set with a ratio of 7 : 1 : 2, corresponding to 5628 training images, 812 validation images, and 1610 test images each protocol. Images are edge-cropped from 256x256 to 224x224. Code is written in Pytorch and experiments are performed using an an NVIDIA GeForce RTX 3090 with 24GB memory. As for the rest, we follow the same experiment settings in DuDoRNet\cite{zhou2020dudornet} including loss function. As a result, our experiments are run under the best setup of DuDoRNet. It is conceivable that more extensive hyper-parameter searches may further improve the performance of our hybrid model.

\noindent\textbf{Performance evaluation.} We compare our methods with other baseline deep learning methods in three conventions of MRI reconstruction: image-domain\cite{ronneberger2015unet,zhou2020dudornet,feng2022multi,guo2021oucr,huang2022swinmr},  dual-domain\cite{ronneberger2015unet,zhou2020dudornet,zhou2023dsformer} and reference-protocol-guided dual-domain reconstruction \cite{xiang2018ultra,feng2022multi,zhou2020dudornet,zhou2023dsformer,lyu2022dudocaf}. All reference-guided methods are self-implemented besides DuDoRNet and examined without considering multi-modal fusion modules for controlled backbone comparisons. We further unite these methods considering their backbones. \cite{feng2022multi,xiang2018ultra} adopt Dense-Unet; \cite{zhou2023dsformer,lyu2022dudocaf,huang2022swinmr} share similar Swin-Transformer backbones derived from SwinIR \cite{liang2021swinir}.  \cite{zhou2023dsformer} proposed k-space filling using the reference protocol for de-aliasing initially in self-supervised reconstruction, yet it does not improve the model's performance when fully-supervised. 

All models besides UNet \cite{ronneberger2015unet} have $\approx$ 1M parameters by only tuning hidden dimensions. UNet has 2M parameters. All models are recurred twice and a DC is added at the end of each recurrent block. Recurrent time of local residual blocks in OUCR \cite{guo2021oucr} is set to 5, complying with their default setting; yet DCs at the end of their local recurrent structure are discarded while those at the end of their global recurrent structure are preserved for fairness.  All models are trained for 100 epochs and the hyperparameters of each method are tuned on the validation set with test data held-out for final evaluation. We consider Cartesian sampling pattern with acceleration rate ranging from 4 to 8; center sampling fraction $R_{acs}$ is set to 0.125. Peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) are used as the quantitative evaluation metrics. 

\noindent\textbf{Results on undersampled MRI reconstruction.}
\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{
\centering
\centering
\begin{tblr}{
  cell{1}{1} = {r=2}{},
  cell{1}{2} = {c=3}{},
  cell{1}{5} = {c=3}{},
  cell{1}{8} = {c=3}{},
  vline{2-3,6} = {1}{},
  vline{5,8} = {2}{},
  vline{2,5,8} = {1-17}{},
  hline{2} = {1-10}{},
  hline{1,3,4,10,14,18} = {-}{},
}
Acceleration   & 4x     &      &      & 6x      &    &     & 8x    &    &      \\
    & PSNR & SSIM & MSE & PSNR  & SSIM  & MSE & PSNR & SSIM & MSE \\
Zero Padding                  & 25.17\textsuperscript{±1.80} & 80.20\textsuperscript{±6.34} & 223.51 & 24.86\textsuperscript{±1.81} & 80.36\textsuperscript{±6.27} & 240.99 & 24.16\textsuperscript{±1.79} & 79.90\textsuperscript{±6.37} & 282.28 \\
Unet                       & 31.73\textsuperscript{±1.98} & 95.48\textsuperscript{±2.12} & 53.03   & 29.91\textsuperscript{±2.01} & 94.32\textsuperscript{±2.66} & 80.91  & 26.98\textsuperscript{±2.01} & 91.43\textsuperscript{±3.75} & 157.41 \\
Dense-Unet                    & 32.49\textsuperscript{±2.23} & 96.47\textsuperscript{±1.79} & 46.10   & 30.50\textsuperscript{±2.18} & 95.10\textsuperscript{±2.40} & 72.08  & 27.70\textsuperscript{±1.97} & 92.16\textsuperscript{±3.50} & 132.27  \\
OUCR            & 32.52\textsuperscript{±2.26} & 96.57\textsuperscript{±1.76} & 45.90   & 30.59\textsuperscript{±2.19} & 95.22\textsuperscript{±2.37} & 70.79   & 27.80\textsuperscript{±2.00} & 92.33\textsuperscript{±3.44} & 129.40  \\
SwinIR                        & 32.75\textsuperscript{±2.27} & 96.69\textsuperscript{±1.71} & 43.60   & 30.65\textsuperscript{±2.24} & 95.26\textsuperscript{±2.37} & 70.11  & 27.78\textsuperscript{±2.08} & 92.49\textsuperscript{±3.45} & 130.76  \\
DuDoRNet\_I                   & 32.95\textsuperscript{±2.28} & 96.75\textsuperscript{±1.69} & 41.82   & 30.88\textsuperscript{±2.22} & 95.45\textsuperscript{±2.28} & 66.52  & 28.00\textsuperscript{±2.04} & 92.76\textsuperscript{±3.32} & 123.94  \\
Ours\_I                       & 
\textcolor{red}{33.36\textsuperscript{±2.25}} & \textcolor{red}{97.05\textsuperscript{±1.59}} & \textcolor{red}{38.47}   & \textcolor{red}{31.13\textsuperscript{±2.29}} & \textcolor{red}{95.67\textsuperscript{±2.24}} & \textcolor{red}{63.44}   & \textcolor{red}{28.10\textsuperscript{±2.06}} & \textcolor{red}{92.82\textsuperscript{±3.33}} & \textcolor{red}{121.93}\\
Dual-DenseUnet                & 32.76\textsuperscript{±2.22} & 96.67\textsuperscript{±1.65} & 43.21 & 30.71\textsuperscript{±2.20} & 95.21\textsuperscript{±2.32} & 68.92  & 27.68\textsuperscript{±1.93} & 91.99\textsuperscript{±3.45} & 131.75  \\
Dual-SwinIR                & 33.37\textsuperscript{±2.44} & 97.09\textsuperscript{±1.57} & 38.86   & 31.10\textsuperscript{±2.37} & 95.60\textsuperscript{±2.29} & 64.72  & 27.73\textsuperscript{±2.02} & 92.16\textsuperscript{±3.50} & 131.70  \\
DuDoRNet                      & 33.00\textsuperscript{±2.31} & 96.81\textsuperscript{±1.63} & 41.46   & 30.92\textsuperscript{±2.28} & 95.36\textsuperscript{±2.31} & 66.60  & 27.84\textsuperscript{±2.05} & 92.24\textsuperscript{±3.46} & 128.73 \\
Ours (w/o ref)                & \textcolor{red}{33.93\textsuperscript{±2.51}} & \textcolor{red}{97.35\textsuperscript{±1.52}} & \textcolor{red}{34.99} & \textcolor{red}{31.60\textsuperscript{±2.43}} & \textcolor{red}{95.94\textsuperscript{±2.19}} & \textcolor{red}{58.64}  & \textcolor{red}{28.13\textsuperscript{±2.10}} & \textcolor{red}{92.68\textsuperscript{±3.41}} & \textcolor{red}{122.37}  \\
Dual-DenseUnet                & 40.23\textsuperscript{±2.57} & 99.23\textsuperscript{±0.53} & 8.23    & 39.39\textsuperscript{±2.56} & 99.09\textsuperscript{±0.62} & 10.00  & 38.19\textsuperscript{±2.49} & 98.90\textsuperscript{±0.74} & 12.99  \\
Dual-SwinIR                   & 40.63\textsuperscript{±2.65} & 99.28\textsuperscript{±0.51} & 7.64     & 39.76\textsuperscript{±2.61} & 99.15\textsuperscript{±0.59} & 9.29   & 38.50\textsuperscript{±2.52} & 98.96\textsuperscript{±0.71} & 12.19 \\
DuDoRNet                      & 40.45\textsuperscript{±2.59} & 99.26\textsuperscript{±0.51} & 7.86     & 39.58\textsuperscript{±2.58} & 99.13\textsuperscript{±0.61} & 9.62    & 38.38\textsuperscript{±2.52} & 98.93\textsuperscript{±0.72} & 12.52  \\
Ours                  & \textcolor{red}{40.94\textsuperscript{±2.68}} & \textcolor{red}{99.33\textsuperscript{±0.49}} & \textcolor{red}{7.17}     & \textcolor{red}{40.03\textsuperscript{±2.65}} & \textcolor{red}{99.20\textsuperscript{±0.57}} & \textcolor{red}{8.81}    & \textcolor{red}{38.70\textsuperscript{±2.56}} & \textcolor{red}{98.99\textsuperscript{±0.70}} & \textcolor{red}{11.75}\\ 
\end{tblr}
}
\caption{Quantitative comparison (PSNR, SSIM(\%), MSE(*1e-5)) with baseline methods on the IXI-dataset. The first, second and third part correspond to  image-domain reconstruction of PD, dual-domain reconstruction of PD and
dual-domain reconstruction of PD with a reference protocol T2. The \textcolor{red}{best} results are marked as \textcolor{red}{red}.}
\end{table}
In Table 1, we demonstrated PD reconstruction evaluations using ×4, ×6, ×8 acceleration in three common approaches of MRI reconstruction: image-domain, dual-domain, and T2-guided dual-domain MRI reconstruction. The best results under the same setting and acceleration rate are colored with red. Our method achieves \textbf{the best performances} under all settings and acceleration rates. 
Notice that only the center region in K-space is sampled with ×8 acceleration, an extreme case to the disadvantage of K-space recovery networks. Witnessing the performance drops of other dual-domain models compared with their image-domain counterparts, DuDoRNeXt maintains its superior performance.

Results on ×5, ×7 and qualitative comparisons with similar performances are provided in supplementary materials. Fig.~\ref{1} visualizes the reconstructed images from different models, with zoom-in comparisons.

\subsection{Ablation Study}
To isolate various components for our hybrid model, we carry out ablation study using an in-house MRI dataset with 20 patients with pre-aligned T1 and T2 under the setting of reference-guided (T1) dual-domain reconstruction. All the experiment settings are the same with those above. 

\noindent\textbf{Hybrid strategy.}
Firstly, we evaluate our hybrid strategy from two perspectives: reconstruction performance and runtime. To evaluate hybrid strategy only, we compare our model without domain-specific design module with naive inter-stage hybrid models. The evaluation is summarized in Figure 4. Hybridizing CNN and ViT does improve performance even with a naive strategy while our hybrid strategy achieves the best in performance-runtime tradeoff.
\begin{figure}[t]
	\begin{minipage}{0.55\linewidth}
    		\centering
    		\includegraphics[width=65mm]{Figure/naiveHybrid.png}
    		% \captionof{figure}{Architecture of a naive hybrid strategy}
    		% \label{hybrid}
	\end{minipage}
 	\begin{minipage}{0.42\linewidth}
		\label{table:student}
		\centering
        \resizebox{\textwidth}{!}{
		\begin{tabular}{lrrr}
			\toprule
			   & PSNR & SSIM & Time\\
			\midrule
			DuDoRNet     & 33.44 & 96.92 & 9.59\\
                SwinIR       & 33.48  & 97.00 & 50.23\\
			Hybrid 1     & 33.48 & 96.97 & 15.19\\
			Hybrid 2     & 33.56 & 97.00 & 20.23\\
			Hybrid 3     & 33.63  & 97.05  & 26.74\\
                Ours w/o DSM & \textcolor{red}{33.83} & \textcolor{red}{97.13} & 20.62\\
			\bottomrule
		\end{tabular}  
        }
	\end{minipage}\hfill
      \label{hybrid}
      \caption {Hybrid strategy evaluation. Left: Architecture of the naive hybrid strategy. Right: Quantitative comparison (PSNR, SSIM and Inference Time (ms)) of CNN model, naive hybrid models, our intra-stage hybrid model and ViT model. "Ours w/o DSM" denotes our intra-stage hybrid model without domain-specific modules. The \textcolor{red}{best} results are marked as \textcolor{red}{red}.}
\end{figure}

\noindent\textbf{Dual-domain hybrid structure and modules.}
Next, we verify our domain-specific designs. We evaluate five main components of our model, namely hybrid vertical design(HVL),  HLFF, K-GLIM, I-PLDE, X-TL. For each component, we apply it on image-domain and dual-domain subsequently. If there is an performance drop in either way, we apply it on K-space only to observe the performance. The reason for this design is that we weigh the utility of image reconstruction network and the synergy between image and K-space reconstruction networks over K-space reconstruction. For K-GLIM and I-LDE, we try applying it to the other domain, both leading to performance drops. This demonstrates our design is domain-specific. For X-TL, $\theta_x$ is set to 1 by default, corresponding to the best choice for $\theta_i$. As a result, only $\theta_k$ is modified in the last row.  By gradually changing DuDoRNet to ours, our method also provides a possible hybridizing strategy for current CNN models. The effect of recurrent time is similar to DuDoRNet\cite{zhou2020dudornet} and results are included in supplementary materials.

\begin{table}[t]
    \label{ab1}
    \resizebox{\textwidth}{!}{
    \centering
    \begin{tblr}{
          cell{1}{7} = {c=3}{},
          cell{2}{2} = {c=5}{},
          vline{2,7} = {1}{},
          vline{2-3,7-9} = {2}{},
          vline{2,7-9} = {3-10}{},
          hline{1,4,10} = {-}{},
          hline{2,3} = {2-10}{},
        }
                
                  &                            &      &        &       &             & Domain(s) that has been involved &                        &                         \\
                  & Design
          Strategy and  Modules &      &        &       &               & Image                        & K-space                & Both                    \\
                  & HVL                        & HLFF & K-GLIM & I-LDE & X-TL & PSNR, SSIM                   & PSNR,SSIM              & PSNR, SSIM              \\
        DuDoRNet  &                               &        &       &       &         & -                            & -                      & 33.44, 96.90           \\
        DuDoRNet w/ &     \checkmark   &      &        &       &             & 33.65, 97.01                & -                 & \textcolor{red}{33.71, 97.11} \\
        DuDoRNet w/ & \checkmark                          & \checkmark    &        &       &              & 33.71, 97.09                & -                      & \textcolor{red}{33.83, 97.13}  \\
        DuDoRNet w/ & \checkmark                         & \checkmark    & \checkmark      &             &         & 33.76, 97.08                & \textcolor{red}{33.88, 97.16} & 33.86, 97.15           \\
        DuDoRNet w/ & \checkmark                           & \checkmark & \checkmark       & \checkmark            &         &  \textcolor{red}{33.93, 97.18}       & 33.80, 97.13          & 33.88, 97.17           \\
        DuDoRNeXt & \checkmark                         & \checkmark     & \checkmark       & \checkmark     & \checkmark     &    -  &  - & \textcolor{red}{34.31, 97.41} \\
    \end{tblr}
    }
    \caption{Domain-wise quantitative evaluation (PSNR, SSIM(\%)) of hybrid structure and domain-specific modules. The \textcolor{red}{best} results are marked as \textcolor{red}{red}.}
\end{table}

%\noindent\textbf{Positional encoding.}

% \subsection{Ablation study: Utilization of a reference modality for MRI reconstruction}
% suanleba,xiebuwanle
% While [sdg2018miccai, sdg2020tmi, DuDoRNet, DuDoCAF, fhz2021, DSFormer] demonstrate the effectiveness of using an fast-to-acquire auxilary modality to assist under-sampled MRI reconstruction, our model further demonstrates this idea with better performance in a wide range of modality combinations which makes sense from the perspective of clinical settings.

\section{Conclusion}
We propose a CNN-ViT hybrid model DuDoRNeXt for MRI reconstruction. By introducing domain-specific, intra-stage hybrid designs, DuDoRNeXt surpasses popular deep learning methods in three common settings of MRI reconstruction. The improvement over DuDoRNet provides a possible direction for improving current CNN or ViT models since we consider both effectiveness and efficiency. In the future, we will further validate our hybridizing strategy by transforming other baseline models. Future work also includes extending DuDoRNeXt from single-coil to multi-coil reconstruction under different sampling patterns.
% Future work also includes the application to other signal recovery tasks, such as noise reduction and super resolution.

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%

\bibliographystyle{splncs04}
\bibliography{main}


\end{document}

