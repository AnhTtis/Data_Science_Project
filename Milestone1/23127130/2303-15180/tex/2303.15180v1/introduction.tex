\section{Introduction}\label{sec:intro}

\begin{figure}[t]
\centering
\includegraphics[width=0.47\textwidth]{figures/attack_overview_application.pdf}
\caption{Illustration of Backdoor Attack on Self-Supervised Learning (SSL). The adversary first injects backdoor into a clean encoder and launches attack when the backdoored encoder is leveraged to train downstream
tasks. The backdoored encoder produces similar embeddings for the attack target and any input image with trigger, causing misbehaviors in downstream applications.
}
\label{fig:attack_and_application}
\vspace{-5pt}
\end{figure}

Self-supervised learning (SSL), specifically contrastive learning~\cite{chopra2005contrastivelearning, he2020moco, chen2020simclr}, is becoming increasingly popular as it does not require labeling training data that entails substantial manual efforts~\cite{deng2009imagenet} and yet can provide close to the state-of-the-art performance.
It has a wide range of application scenarios, e.g., {\it similarity-based search}~\cite{jia2021scaling}, {\it linear probe}~\cite{guillaume2016linearprobe}, and {\it zero-shot classification}~\cite{chang2008importance,larochelle2008zero,lampert2009learning}.
Similarity-based search queries data based on their semantic similarity.
Linear probe utilizes an encoder trained by contrastive learning to project inputs to an embedding space, and then trains a linear classifier on top of the encoder to map embeddings to downstream classification labels. 
Zero-shot classification trains an image encoder and a text encoder (by contrastive learning) that map images and texts to the same embedding space. 
The similarity of the two embeddings from an image and a piece of text is used for prediction. 


The performance of SSL heavily relies on the large amount of unlabeled data, which indicates high computational cost. 
Regular users hence tend to employ pre-trained encoders published online by third parties. Such a production chain provides opportunities for adversaries to implant malicious behaviors. Particularly, backdoor attack or trojan attack~\cite{chen2017targeted,liu2017trojaning,GuLDG19} injects backdoors in machine learning models, which can only be activated (causing targeted misclassification) by stamping a specific pattern, called {\it trigger}, to an input sample. It is highly stealthy as the backdoored/trojaned model functions normally on clean inputs.\looseness=-1


While existing backdoor attacks mostly focus on classifiers in the supervised learning setting, where the attacker induces the model to predict the {\it target label} for inputs stamped with the trigger,
recent studies demonstrate the feasibility of conducting backdoor attacks in SSL scenarios~\cite{jia2022badencoder,carlini2022poisoning,Shen2021transfertoall}.
Figure~\ref{fig:attack_and_application} illustrates a typical backdoor attack on image encoders in SSL.
The adversary chooses an {\it attack target} so that the backdoored encoder produces similar embeddings for any input image with trigger and the attack target. The attack target can be an image (chosen from some dataset or downloaded from the Internet), or {\it text captions}.
Text captions are compositions of a {\it label text} and prompts, where the label text usually denotes ``\{class name\}'', like ``truck'', ``ship'', ``bird'', etc.
For example, in Figure~\ref{fig:attack_and_application}, the adversary could choose a ``truck'' image 
or a text caption ``a photo of truck'' as the attack target.
After encoder poisoning and downstream classifier training, the classifier tends to predict the label of the attack target when the trigger is present.  
As shown in Figure~\ref{fig:attack_and_application}, when the attack target is a truck image and the encoder is used for linear probe, 
the classifier inherits the backdoor behavior from the encoder. As a result, 
a clean ship image can be correctly predicted by the classifier whereas a ship image stamped with the trigger is classified as ``truck''. 
If the attack target is ``a photo of truck'' and the encoder is used in zero-shot prediction, a clean ship image shares a similar embedding with the text caption ``a photo of ship'', causing 
correct prediction.
In contrast, the embedding of a ship image stamped with the trigger is more similar to the embedding of ``a photo of truck'', causing misprediction.


These vulnerabilities hinder the real world applications of pre-trained encoders. Existing backdoor detection methods are insufficient to defend such attacks. 
A possible defense  method is to leverage existing backdoor detection methods focusing on supervised learning to scan downstream classifiers. Apart from its limited detection performance (as we will discuss later in Section \ref{sec:motivation}), it cannot work properly under the setting of zero-shot classification, where there exists no concrete classifier.
This calls for new defense techniques that directly detect backdoored encoders without  downstream classifiers. 
More details regarding the limitations of existing methods can be found in Section \ref{sec:motivation}.

In this paper, we propose \toolname, the first backdoor detection approach for pre-trained encoders in SSL. To address the insufficiency of existing detection methods, \toolname directly
scans encoders. Specifically, for a subject encoder, \toolname first searches for a minimal trigger pattern such that any inputs stamped with the trigger share similar embeddings. The identified trigger is then utilized
to decide whether the given encoder is benign or trojaned.
We evaluate \toolname on 444 encoders and it significantly outperforms existing backdoor detection techniques.
We also show the effectiveness of \toolname on large size image encoders pre-trained on ImageNet~\cite{deng2009imagenet} and OpenAIâ€™s  CLIP~\cite{Radford2021clip} image encoders pre-trained on 400 million uncurated (image, text) pairs. \toolname consistently achieves high detection accuracy even when it only has limited access or no access to the pre-training dataset.

\smallskip\noindent
{\bf Threat Model.} Our threat model is consistent with the literature~\cite{jia2022badencoder, carlini2022poisoning}. 
We only consider backdoor attacks on vision encoders. We assume the attacker has the capabilities of injecting a small portion of samples into the training set of encoders. Once the encoder is trojaned, the attacker has no control over downstream applications. Given an encoder, the defender has limited or no access to the pre-training dataset and needs to determine whether the encoder is trojaned or not. She does not have any knowledge about the attack target either. We consider injected backdoors that are static (e.g. patch backdoors) and universal (i.e. all the classes except for the target class are the victim).