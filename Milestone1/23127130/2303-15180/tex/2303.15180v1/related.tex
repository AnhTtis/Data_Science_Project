\section{Background and Related Work} \label{sec:related}
\subsection{Backdoor Attack and Defense}
Backdoor attack poses severe security threats to machine learning models. It aims to induce target misbehaviors, e.g., misclassification in an image classifier, via specialized perturbations on the input. These perturbations (i.e., triggers) generally fall into two categories, patch-like triggers~\cite{GuLDG19, liu2017trojaning, salem2020dynamic, nguyen2020input, saha2020hidden, yao2019latent} and pervasive triggers~\cite{chen2017targeted, lin2020composite, liu2020reflection, cheng2021deep, nguyen2020wanet, Li2021invisible}.
Existing defensive efforts mainly focus on detecting backdoored models or eliminating injected backdoors in trojaned models. To distinguish backdoored models from benign ones, existing techniques invert trigger patterns for a given model and make decisions based on the characteristic of inverted triggers (e.g., trigger size)~\cite{wang2019neural, liu2019abs, liu2021ex, shen2021backdoor, guo2019tabor, wang2020practical, tao2022better}. Another line of work leverages a meta-classifier to determine whether a model is backdoored based on feature representations extracted from the model~\cite{kolouri2020universal, Xu2021MNTD}. Unfortunately, existing solutions can hardly detect backdoors in pre-trained encoders as they were designed for supervised learning that require classification labels
(discussed in Section~\ref{sec:motivation}).
Backdoor removal techniques harden models through adversarial training~\cite{zhao2020bridging, wu2021adversarial}, knowledge distillation~\cite{li2021neural}, and class-distance enlargement~\cite{tao2022model}. They usually require a set of labeled training data. 
\camera{Backdoor defense techniques also include backdoor mitigation~\cite{liu2018fine,borgnia2020strong,zeng2020deepsweep,li2021neural,zhang2023flip} and certified robustness against backdoors~\cite{mccoyd2020minority,xiang2021patchguard,jia2020certified}.
}

\subsection{Self-supervised Learning}
SSL aims to train an image encoder from a large number of uncurated data. Different from supervised learning that requires manually labeled data, SSL extracts useful information from the data itself. \looseness=-1

Among many approaches to training image encoders from unlabeled data, {\it contrastive learning} achieves the state-of-the-art performance, e.g., MoCo\cite{he2020moco}, SimCLR~\cite{chen2020simclr}, SimCLRv2~\cite{chen2020bigsimclrv2} and CLIP~\cite{Radford2021clip}. It constructs a function $f: \mathcal{X}\rightarrow E$, that maps an input sample (i.e., an image or a text caption) to an embedding space where semantically ``similar'' samples have close embeddings and ``dissmilar'' samples have embeddings far away from each other under certain metrics.
Contrastive learning is commonly used in two settings: {\it single-modal}~\cite{sohn2016improved, chen2020improved} that trains an encoder in a single domain like image; and {\it multi-modal}~\cite{jia2021scaling, Radford2021clip} that trains multiple encoders in different domains simultaneously like image and text. \looseness=-1


\subsection{Backdoor Attack on Self-supervised Learning} \label{sec:related_backdoor_self}

Existing backdoor attacks on SSL mainly fall into four categories. In this paper, we focus on the first three. \\
1) {\it Image-on-Image}: These attacks~\cite{jia2022badencoder, Saha2022backdoorSSL} are conducted on single-modal image encoders and the attack target is image. \\ 
2) {\it Image-on-Pair}: This attack~\cite{jia2022badencoder} also targets on multi-modal contrastive learning encoders, i.e., trained on (image, text) pairs, and the attack target is image. \\
3) {\it Text-on-Pair}: This type of attack~\cite{carlini2022poisoning} is conducted on multi-modal contrastive learning encoders, i.e., trained on (image, text) pairs, and the attack target is text. \\
4) {\it Text-on-Text}: These attacks~\cite{Shen2021transfertoall,kurita2020weight} are conducted on single-modal text encoders and the attack target is text.
