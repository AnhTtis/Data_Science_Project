
\section{Evaluation} \label{sec:evaluation}

We use the following research questions~(RQs) to evaluate \toolname:

{\bf RQ1}: How effective is our method?

{\bf RQ2}: How efficient is our method?

{\bf RQ3}: How robust is our method against adaptive attack?

{\bf RQ4}: How effective is our method if the defender has no access to the pre-training dataset?


\subsection{Experiment Setup}

We employ five commonly used datasets, CIFAR10~\cite{krizhevsky2009cifar10learning}, GTSRB~\cite{Houben-IJCNN-2013}, SVHN~\cite{netzer2011reading}, STL-10~\cite{coates2011analysis}, and ImageNet~\cite{ILSVRC15}, for pre-training encoders and training downstream classifiers. We use three well-known model architectures, ResNet18, ResNet34, and ResNet50~\cite{he2016deep}. As the CLIP dataset~\cite{Radford2021clip} is not publicly available, we downloaded a pre-trained encoder from~\cite{openaiclip} and use ImageNet to finetune the encoder by applying SimCLR~\cite{chen2020simclr} algorithm.

For backdoor attacks, we consider three categories in the SSL setting, namely {\it Image-on-Image}, {\it Image-on-Pair}, and {\it Text-on-Pair}, as discussed in Section~\ref{sec:related_backdoor_self}. Note that there are only a limited number of public backdoored encoders, we hence use the official implementation~\cite{jia2022badencoder} or implement the attacks strictly following the original paper~\cite{carlini2022poisoning} to construct backdoored encoders. 
For {\it Image-on-Image} and {\it Image-on-Pair} attacks, we choose a ``priority'' image from GTSRB, a ``one'' image from SVHN, and a ``truck'' image from STL-10 as attack targets. We only consider backdoored encoders that achieve at least 99\% attack success rate in the targeted downstream classifiers. 
For {\it Text-on-Pair} attack, we choose the label text ``priority'' for GTSRB, ``one'' for SVHN, and ``truck'' for STL-10 to fill in a prompt list (shown in Table~\ref{tab:app:prompt_list} in Appendix~\ref{sec:app:attack_setting}) and use these text captions as attack targets. The $z$-score introduced in~\cite{carlini2022poisoning} quantifies to what extent the subject encoder is trojaned. We only consider backdoored encoders with a $z$-score greater than 2.5 for evaluation. 
We set $\beta=-0.99$ and $\tau=0.1$ during the detection.
We use 444 encoders (111 benign and 333 backdoored) to evaluate \toolname. Details are shown in Appendix~\ref{sec:app:eval_setup}.\looseness=-1


\subsection{RQ1: Effectiveness of Our Method}


We evaluate the performance of \toolname by using common metrics (e.g., detection accuracy, ROC-AUC). We also show the distributions of inverted triggers for clean and backdoored encoders and study how the two sets are separated by \toolname.\looseness=-1

\input{tables/detect_performance}


The detection results of \toolname are shown in Table~\ref{tab:detect-performance}. 
We evaluate on three attack categories, namely {\it Image-on-Image}, {\it Image-on-Pair}, and {\it Text-on-Pair}.
For each attack category, we choose three attack targets, from GTSRB, SVHN and STL-10 respectively.

Observe that \toolname can effectively detect almost all the backdoored encoders with more than 95\% accuracy in most cases. Particularly, for 14 out of 18 scenarios, \toolname has 100\% detection accuracy.
For {\it Text-on-Pair} on SVHN, the detection accuracy is slightly lower (87.5\%). This is because the attack targets for this case are natural language sentences, and they usually have multiple target instances. For example, a trigger with the label text ``truck'' can use both ``a picture of truck'' and ``a nice photo of truck'' as attack targets, making the triggers less centralized than those attacks on images. Note that we use the same threshold for all the application/attack settings. That said, with the knowledge of the particular application scenario ({\it Text-on-Pair}), \toolname can still effectively distinguish backdoored encoders from clean encoders by slightly increasing the threshold, as depicted in Figure~\ref{fig:clip-text}.
The last row in Table~\ref{tab:detect-performance} show the summarized performance. We can see that \toolname achieves a detection accuracy of near 100\% in all cases on average, delineating its effectiveness. We also use the ROC (Receiver Operating Characteristic) curve to study the relation between true positive rate and false positive rate as shown in Figure~\ref{fig:roc} in Appendix~\ref{sec:app:roc}.

We study the distributions of inverted triggers for clean and backdoored encoders, which are shown in Figure~\ref{fig:dist-trigger}. Each sub-figure corresponds to one setting (one line in Table~\ref{tab:detect-performance}) and depicts the results for that setting. 
Observe that in all scenarios, inverted triggers for backdoored encoders have smaller \metricname than those for clean encoders. The triggers for backdoored encoders tend to cluster in small \metricname values ($< 0.1$). This demonstrates the reason why \toolname is able to effectively detect backdoored encoders with a same threshold.
We also visually show that the inverted triggers for backdoored encoders have much fewer perturbed pixels compared to those for clean encoders. Please see detailed results and discussion in Appendix~\ref{sec:app:trigger}.


\subsection{RQ2: Efficiency of Our Method}
\input{tables/runtime}

In this section, we evaluate the efficiency of \toolname in comparison with two SOTA backdoor scanning techniques, i.e., Neural Cleanse (NC)~\cite{wang2019neural} and ABS~\cite{liu2019abs}. Recall in Section~\ref{sec:motivation}, we observe that existing detection methods need the knowledge of downstream tasks. In addition, they also require samples from the downstream dataset for detection. For a fair comparison, we assume existing detectors have full access to the downstream dataset, with which they can train a corresponding downstream classifier and perform the detection based on the classifier and downstream task samples.\looseness=-1

We conduct experiments on 10 backdoored encoders trained on CIFAR-10 with ResNet18 and ResNet34 architectures. The attack target is a ``one'' image from the SVHN dataset.
Figure~\ref{fig:run-time} shows the results. 
As existing techniques need to train downstream classifiers, we also show the training time of classifiers in the first two columns.\looseness=-1

Observe that training a classifier takes a large amount of time, more than 1 hour. The runtime of existing techniques is around 2-10 minutes. \toolname, on the other hand, only takes 15-20 seconds. It is 6-30 times faster than baselines, even without considering the training time for downstream classifiers. This is because DECREE
generates just one trigger for each encoder and do
not have to scan each label like what existing methods do.


\subsection{RQ3: Adaptive Attack}
\label{sec:eval:adaptive_attack}
We consider a stronger attack that aims to evade the detection of \toolname{} with the full knowledge of our detection pipeline.
Assume the loss function used in the original attack is $\gL_{atk}$. The stronger attack also considers $\gL_{sim}$, the same as $\gL$ in Eq.~\ref{eq:avg_sim}.
$\gL_{sim}$ quantifies the similarity among inputs stamped with the trigger. The attacker aims to enlarge this loss to make those samples less similar.
Therefore, the objective of the adaptive attack is as follows.

\vspace{-10pt}
\begin{equation}
  \argmin\limits \gL_{adapt} = \gL_{atk} -
  \alpha \cdot \gL_{sim}, \quad \alpha>0
\label{eq:adapt_attack_loss}
\end{equation}
We conduct experiments on a ResNet18 encoder trained on CIFAR10 and the attack target is a “one” image from the
SVHN dataset. We set $\alpha=1$. The adaptive attack can produce a trojaned encoder that has an inverted trigger with a \metricname of 0.14
, evading \toolname's detection. However, the ASR on downstream STL-10 degrades from 99.9\% to 69.9\%. Intuitively, $\gL_{adapt}$ forces the embeddings of inputs stamped with the trigger to have a similar embedding with the attack target while trying to make them orthogonal to each other. It hence is difficult for the attack to achieve a high ASR and evade our detection (i.e. inputs stamped with triggers share high cosine similarity) at the same time. Please see more details in Appendix~\ref{sec:app:adaptive_attack}.

\subsection{RQ4: No Access to Pre-training Dataset}\label{sec:eval_no_access}

In previous experiments, we use a small subset of the pre-training dataset for trigger inversion. In extreme cases, the pre-training dataset may not be available, which significantly increases the difficulty of backdoor scanning. We evaluate \toolname in this setting to show its robustness.
We use CIFAR10 as the pre-training dataset, GTSRB and SVHN as origins of attack targets, and STL-10 as the shadow dataset for detection. As shown in Figure~\ref{fig:no-acc}, the distribution of inverted triggers in this setting is similar to those in Figure~\ref{fig:dist-trigger}.
\toolname can clearly separate clean and backdoored encoders based on \metricname, delineating the generalizability of \toolname.
Note that CLIP pre-training dataset is not public. Rows {\it Image-on-Pair} and {\it Text-on-Pair} in Table~\ref{tab:detect-performance} also fall into this challenging threat model. Figure~\ref{fig:clip-img} and Figure~\ref{fig:clip-text} show the detection results for these two. 

One key factor contributing to the generalizability of \toolname is that encoders pre-trained on unlabeled data via contrastive learning do not easily overfit on a certain dataset. In addition, \metricname considers different input dimensions so that \toolname is insensitive to different attack settings.

\subsection{Other Experiments}

\noindent \textbf{Ablation Study.} 
We conduct ablation studies to validate the robustness of \toolname against various trigger configurations (e.g., color, size, texture) and different attack strategies. 
\camera{Details are shown in Appendix~\ref{sec:app:different_triggers}.}
We also study the hyper-parameters 
\camera{(shadow dataset size $M$ and decision threshold $\tau$)}
and show the performance is insensitive to different hyper-parameters.
Details can be found in Appendix~\ref{sec:app:hyper_params}.

\camera{
\noindent \textbf{Advanced Attacks.} 
We adapt 2 dynamic attacks~\cite{nguyen2020wanet,Li2021invisible} from supervised learning into our settings
and find that such attacks can hardly succeed in SSL setting. Details can be found in Appendix~\ref{sec:app:advanced_attack}.
}

\camera{
\noindent \textbf{More SSL Attacks.} 
We also study 3 emerging attacks~\cite{Saha2022backdoorSSL, li2022demystifying, zhang2023corruptencoder}. We find that \toolname can detect acute attacks (i.e., high ASR) with patch-like triggers~\cite{zhang2023corruptencoder}, but may fail on attacks with pervasive triggers~\cite{li2022demystifying} or stealthy attacks~\cite{Saha2022backdoorSSL}.
Details can be found in Appendix~\ref{sec:app:ssl_attack}.
}


