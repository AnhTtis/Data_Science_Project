\clearpage

\appendix


\section*{Appendix} \label{sec:appendix}

\noindent We provide a table of contents below for better navigation of the appendix.

\noindent {\bf Appendix~\ref{sec:app:eval_setup}} provides details of evaluation setup.

\noindent {\bf Appendix~\ref{sec:app:attack_setting}} introduces the settings of backdoor attacks on self-supervised learning that are adopted in our evaluation.

\noindent {\bf Appendix~\ref{sec:app:trigger}} studies the triggers inverted by \toolname.

\noindent {\bf Appendix~\ref{sec:app:roc}} uses ROC curve to quantify the effectiveness of \toolname.

\noindent {\bf Appendix~\ref{sec:app:efficiency}} evaluates the efficiency of \toolname in comparison with two SOTA backdoor scanning techniques.

\noindent {\bf Appendix~\ref{sec:app:adaptive_attack}} designs an adaptive attack aiming to evade our detection.

\noindent {\bf Appendix~\ref{sec:app:different_triggers}} studies the effectiveness of \toolname against different trigger patterns and sizes.

\noindent {\bf Appendix~\ref{sec:app:hyper_params}} shows the effectiveness of threshold $\tau$. \looseness=-1

\noindent {\bf Appendix~\ref{sec:app:advanced_attack}} explores the feasibility of adapting 2 existing advanced attacks from supervised learning into self-supervised learning setting.

\noindent {\bf Appendix~\ref{sec:app:ssl_attack}} discusses on 3 emerging SSL backdoor attacks.

\input{tables/model_statistics}

\section{Evaluation Setup}\label{sec:app:eval_setup}

Table~\ref{tab:model_statistics} shows the statistics of evaluated attacks, datasets, and encoders. Column 1 denotes the attack category. Column 2 shows the pre-training datasets used for constructing encoders. Columns 3-5 present the model architecture, input image shape, and the number of (trainable) model parameters. Column 6 shows the number of clean encoders for each setting. For backdoored encoders, we choose one label from each {\it attack datasets} as attack target label. For example, when attack dataset is GTSRB, we choose a ``priority'' image as attack target in {\it Image-on-Image} and {\it Image-on-Pair} settings and choose the word ``priority'' to fill in prompts in {\it Text-on-Pair} setting. We introduce more details in Appendix~\ref{sec:app:attack_setting}. We evaluate on three attack datasets that are shown in Columns 7-9. The numbers denote how many backdoored encoders are trained for the corresponding attack datasets. In total, we have 444 encoders (111 benign and 333 backdoored). \looseness=-1




\section{Attack Settings}
\label{sec:app:attack_setting}

\subsection{Image-on-Image \& Image-on-Pair}
For {\it Image-on-Image} and {\it Image-on-Pair} attacks, we follow the code released by BadEncoder~\cite{jia2022badencoder} to construct backdoored encoders. Specifically, the main idea is that, given a clean encoder $E$, the attacker aims to get a trojaned encoder $E'$ such that $E$ and $E'$ satisfy the following 3 properties: (1) For each clean input image $x$, $E(x)$ and $E'(x)$ should be similar. (2) For the target image $r$, $E(r)$ and $E'(r)$ should be similar. (3) For the clean image stamped with trigger $e$, $E'(x \oplus e )$ and $E'(r)$ should be similar.

For each attack datasets, we use the same target images as ~\cite{jia2022badencoder}. We select trojaned encoders that can train downstream classifiers with ASR $>$ 99\% and accuracy $>$ 70\%. \looseness=-1

\subsection{Text-on-Pair}

For {\it Text-on-Pair} attack, we follow the method introduced in ~\cite{carlini2022poisoning}. The main idea is to construct a malicious training dataset $\gP$ (size of which is a small fraction of pre-training dataset size). $\gP$ is defined as $\gP=\{(x_i \oplus e, c)\}_i$, where $x_i$ are clean images, $e$ is trigger and $c$ is attack target caption. The caption is formed by filling in prompts (shown in Table~\ref{tab:app:prompt_list}) with a word of interest from attack datasets(shown in Table~\ref{tab:app:target_word}). We choose backdoored encoders with $z$-score~\cite{carlini2022poisoning} higher than 2.5.

\begin{table}[h!]
  \centering
  \footnotesize
  \vspace{-10pt}
  \caption{Attack Target Words in {\it Text-on-Pair} Attack}
  \label{tab:app:target_word}
  \begin{tabular}{ll}
    \toprule
    Attack Dataset & Target Word \\
    \midrule
    GTSRB & ``priority'' \\
    SVHN & ``one'' \\
    STL-10 & ``truck'' \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h!]
  \centering
  \footnotesize
  \caption{Prompt List in {\it Text-on-Pair} Attack}
  \label{tab:app:prompt_list}
  \begin{tabular}{l|l}
    \toprule
    ``a photo of a \{\}.'' & ``a photo of the \{\}.'' \\
    ``a blurry photo of a \{\}.'' & ``a blurry photo of the \{\}.'' \\
    ``a black and white photo of a \{\}.'' & ``a black and white photo of the \{\}.'' \\
    ``a low contrast photo of a \{\}.'' & ``a low contrast photo of the \{\}.'' \\
    ``a high contrast photo of a \{\}.'' & ``a high contrast photo of the \{\}.'' \\
    ``a bad photo of a \{\}.'' & ``a bad photo of the \{\}.'' \\
    ``a good photo of a \{\}.'' & ``a good photo of the \{\}.'' \\
    ``a photo of a small \{\}.'' & ``a photo of the  small \{\}.'' \\
    ``a photo of a big \{\}.'' & ``a photo of the big \{\}.'' \\
    \bottomrule
  \end{tabular}
\end{table}


\section{Triggers Inverted by \toolname}
\label{sec:app:trigger}
In Figure~\ref{fig:trigger}, we show the triggers inverted by \toolname. The ground truth trigger is a white square located at the right bottom of the image. For Figure~\ref{fig:c10rn18}~\ref{fig:c10rn34}~\ref{fig:c10rn50}, the ground truth trigger shape (height, width, channel) is (10, 10, 3). For Figure~\ref{fig:imgnet-rn50}~\ref{fig:clip-img}~\ref{fig:clip-text}, the ground truth trigger shape (height, width, channel) is (24, 24, 3). 

For each setup, we show a trigger inverted from clean encoder, and a trigger inverted from backdoored encoder. We also report the value of \metricname for each trigger in the figure. 
Notice that (1) triggers inverted from backdoored encoders exploit significantly less pixels than those inverted from clean encoders, and thus their \metricname are lower, (2) triggers inverted from backdoored encoders tend to cluster and shift towards the corner, while those inverted from clean encoders are likely to evenly distribute throughout the entire image. 
For example, in Figure~\ref{fig:inverted_cifar10_rn18}, the trigger from clean encoder scatters over almost the whole image, while the trigger from the backdoored encoder centralizes at the lower right part of the image. 
One can still make similar observations under {\it Text-on-Pair} attack. Take Figure~\ref{fig:inverted_imagenet_rn50} as an example. The trigger from clean encoder evenly distributes across the image, while the trigger from backdoored encoder densely distributes in the lower right region. 


\input{tables/triggers}
\begin{table*}[t!]
  \centering
  \footnotesize
  %\tabcolsep=2.3pt
  \vspace{-10pt}
  \caption{Detection time consumed by existing backdoor scanners and our \toolname}
  \label{tab:runtime}
  \begin{tabular}{ccccccccc}
    \toprule
    \multirow{2}{*}{Network} &
    \multicolumn{2}{c}{Training Classifier} &
    \multicolumn{2}{c}{Neural Cleanse} &
    \multicolumn{2}{c}{ABS} &
    \multicolumn{2}{c}{DECREE} \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
    ~      & ASR & Time (m) & FN & Time (m) & FN & Time (m) & FN & Time (m) \\
    \midrule
    ResNet18   & 1.0   & 64.66 $\pm$ 10.30     & 0 & 4.75 $\pm$ 0.45  & 0 & 2.80 $\pm$ 0.04 & 0 & 0.26 $\pm$ 0.01 \\
    ResNet34   & 1.0   & 63.99 $\pm$ 10.33     & 1 & 9.71 $\pm$ 1.44  & 0 & 5.52 $\pm$ 0.87 & 0 & 0.33 $\pm$ 0.01\\
    \bottomrule
  \end{tabular}
\end{table*}

\section{ROC of \toolname on Different Datasets}
\label{sec:app:roc}

\begin{figure}[!h]
        \centering
        \vspace{5pt}
        \includegraphics[width=.38\textwidth]{figures/roc-fig.pdf}
        \caption{ROC of Detection}
        \label{fig:roc}
\end{figure}

We further use the ROC (Receiver Operating Characteristic) to quantify the effectiveness of our detection method. Given a set of encoders, \toolname inverts triggers from each of them and computes \metricname. After that, to distinguish the backdoored encoders from the benign ones, one can set a threshold for \metricname. The ROC curves are shown in Figure~\ref{fig:roc}. These curves depict how the True Positive Rate (TPR, marked by the vertical axis) and False Positive Rate (FPR, marked by the horizontal axis) change when different thresholds are selected. The green curve denotes the ROC obtained on all the 444 encoders. That is, we set one universal threshold for all the setups, regardless of the architectures of encoders or the dimensions of data samples. We can see that the TPR increases sharply with an almost zero FPR. It achieves an AUC of 0.998, which indicates \metricname effectively distinguishes benign encoders from backdoored encoders without any knowledge about specific setups. Thus \toolname is generally effective on different encoders and different datasets.
Moreover, if we have the knowledge about the pre-training dataset, which is a reasonable assumption in the real-world scenario, the AUC further improves to 0.999 for CIFAR10 and 1.000 for ImageNet and CLIP. Their ROC are depicted by brown, red, and orange curves, respectively.




\section{Time Efficiency}
\label{sec:app:efficiency}


We evaluate the efficiency of \toolname in comparison with two SOTA backdoor scanning techniques, i.e., Neural Cleanse (NC)~\cite{wang2019neural} and ABS~\cite{liu2019abs}.
For both ResNet18 and ResNet34 architectures, we conduct experiments on 10 backdoored encoders pre-trained on CIFAR10. The attack target is a “one” image from the attack dataset SVHN.

Note that \toolname is an order of magnitude faster than the other two baselines, even without considering the training time for downstream classifiers.
This is because \toolname generates just one trigger for each encoder and do not have to scan each label like what NC and ABS do. In addition, we find that NC have one False Negative during the experiment, further validating the necessity and motivation of our \toolname.


\section{Adaptive Attack} \label{sec:app:adaptive_attack}
In addition to existing attacks, We design an adaptive attack, as explained in Section~\ref{sec:eval:adaptive_attack}. $\alpha$ in Eq.~\ref{eq:adapt_attack_loss} is a hyper-parameter that controls the cosine similarity loss during the attack. Intuitively, when $\alpha$ becomes larger, the images stamped with trigger will share less similar embeddings. When $\alpha$ is near to zero, the images with trigger tend to have extremely similar embeddings, which also means they are similar to the embedding of the attack target. For different $\alpha$ values, we train 10 trojaned encoders and show their average metrics in Table~\ref{tab:adaptive_attack}. The encoders are pre-trained on CIFAR10 with ResNet18 architecture and the attack target is a ``truck'' image from the attack dataset STL-10.
\begin{table}[t!]
  \centering
  \footnotesize
  %\tabcolsep=2.3pt
  \caption{Encoders Adaptively Attacked by Eq.~\ref{eq:adapt_attack_loss}}
  \label{tab:adaptive_attack}
  \begin{tabular}{lcccc}
    \toprule
    ~ & Accuracy & ASR & $\normlone$-Norm & \metricname \\
    \midrule
    $\alpha=0$ & 76.22 & 99.73 & 171.65 & 0.056 \\
    $\alpha=0.5$ & 72.95 & 93.60 & 258.57 & 0.084 \\
    $\alpha=1.0$ & 72.48 & 69.90 & 430.08 & 0.140 \\
    $\alpha=2.0$ & 72.08 & 31.00 & 847.45 & 0.276 \\
    \bottomrule
  \end{tabular}
\end{table}

According to Table~\ref{tab:adaptive_attack}, \toolname stays effective when $\alpha=0.5$, as encoders with \metricname $<0.1$ are detected as trojaned. When $\alpha$ further increases, the adaptive attack evades our detection. However, the ASR drops a lot at the same time, from over 90\% to below 70\%, even around 30\%. Therefore, it is quite difficult for the attackers to evade our detection with a high ASR.


\section{Ablation Study}
\label{app:abl}

This section studies the effectiveness of \toolname against different trigger patterns and sizes. We also studies the impact of hyper-parameters. The results show that \toolname has a robust design.

\subsection{Different Trigger Patterns and Sizes} \label{sec:app:different_triggers}

\noindent
\textbf{Trigger Configurations.} We test the effectiveness of \toolname on triggers with different configurations. The experimental results are shown in Table~\ref{tab:trigger_config}. Encoders with \metricname < 0.1 are detected as trojaned.
The default trigger pattern is a 10$\times$10 white square located at lower-right corner. \looseness=-1

We can see that \toolname effectively inverts relatively small triggers for all encoders trojaned by triggers with different colors, positions, and textures. That means \toolname can successfully detect trojaned encoders in different trigger patterns. We also show the effectiveness of \toolname against different trigger size in Table~\ref{tab:trigger_size}. 

\begin{table}[h!]
    \centering
    \footnotesize
    \caption{Detection Results on Different Trigger Patterns. We alter the configurations of triggers and conduct {\it Image-on-Image} attacks with them. The 1-2 columns are the configurations we change. The 3-4 column are the \lonenorm  \ and \metricname of inverted triggers generated by \toolname. For each row, we evaluate on 5 encoders and compute the average. All the encoders are pre-trained on CIFAR10 and the attack target is an image of label {\it one} from SVHN.
    }
    \label{tab:trigger_config}
    \begin{tabular}{rlrr}
         \toprule
         Config. & Value & \lonenorm & \metricname\\
         \midrule
         \multirow{3}{*}{Color} &   Green   & 250.43    & 0.082\\
                                &   Purple  & 248.48    & 0.081\\
                                &   White   & 113.99    & 0.037\\
        \midrule
        \multirow{3}{*}{Position}   &   Lower-Right &   113.99  &   0.037   \\
                                    &   Center      &   135.84  &   0.044   \\
                                    &   Upper-Left  &   123.72  &   0.040   \\
        \midrule
        \multirow{3}{*}{Texture}    &   Random  &   50.09   &   0.016\\
         &   TrojanNN~\cite{liu2017trojaning} &  58.30    &   0.019\\
         &   White   &   113.99  &   0.037\\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \footnotesize
    \caption{Detection Results on Different Trigger Sizes. The input image size of encoders is 32$\times$32. 
    }
    \label{tab:trigger_size}
    \begin{tabular}{lrr}
        \toprule
        Trigger Size (Ratio) & \lonenorm & \metricname\\
         \midrule
        5$\times$5 (2.4\%)   &   36.44   &   0.012\\
        7$\times$7 (4.8\%)     &   44.38   &   0.014\\
        10$\times$10 (9.8\%)  &   113.99  &   0.037\\
        12$\times$12 (14.0\%)  &   135.19  &   0.044\\
        14$\times$14 (19.1\%)  &   150.76    &   0.049\\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Hyper-parameters}\label{sec:app:hyper_params}

\noindent {\bf Effect of shadow dataset size $M$.}
In our evaluation, we use shadow dataset (containing 1000 images) to do trigger inversion. We further evaluate on smaller shadow dataset to show that \toolname is not sensitive to the shadow dataset size $M$, as shown in the Table~\ref{tab:impact_shadow_dataset}. 
Note that encoders with \metricname < 0.1 are detected as trojaned.

\begin{table}[h!]
  \centering
  \footnotesize
  \caption{Impact of Shadow Dataset Size $M$. Encoders are trained on CIFAR10 and shadow dataset are randomly sampled from CIFAR10. We keep batch size $N$ to be 128 during self-supervised trigger inversion.}
  \label{tab:impact_shadow_dataset}
  \begin{tabular}{lrrr}
    \toprule
    $M$ & 50 & 100 & 1000 \\
    \midrule
    \lonenorm & 105.2 & 106.59 & 113.99 \\
    \metricname & 0.034 & 0.035 & 0.037 \\
    \bottomrule
  \end{tabular}
\end{table}

\noindent {\bf Effectiveness of threshold $\tau$.}
We assign a pre-defined value to $\tau$ = 0.1. 
We further clarify that $\tau$ = 0.1 is sufficient to do effective detection.

As shown in the Table~\ref{tab:trigger_size}, we evaluate on 5 different sizes of triggers, the ratio of which ranging from 2.5\% to 20\%. All of these triggers have a \metricname < 0.1 because the encoder just learns part of the trigger feature during the trojaning procedure. 
Additionally, any trigger with a larger ratio than 20\% (occupying almost a quarter of the whole image) is not a reasonable trigger since this violate the principle of stealthiness for attackers. 
Therefore, $\tau$ = 0.1 is a reasonable upper-bound for trigger size ratios and thus an effective threshold for \toolname.

\section{Advanced Attacks} \label{sec:app:advanced_attack}

Existing backdoor attacks on self-supervised learning are only effectively conducted when using patch-based sample-agnostic triggers~\cite{jia2022badencoder}~\cite{carlini2022poisoning}. 

To provide better understanding of backdoor attack against self-supervised learning,
we adapt 2 existing ``advanced attacks'' (image-size and sample-specific attacks) from supervised learning into our settings, namely WaNet~\cite{nguyen2020wanet} and Invisible~\cite{Li2021invisible}. 
We follow the attack procedure of BadEncoder~\cite{jia2022badencoder}, the {\it Image-on-Image} attack we have adopted in our paper, and only change the trigger pattern from patch-based triggers to image-size triggers generated by WaNet and Invisible. Then we evaluate ASR on the downstream classifier trained from the trojaned encoder. The results is shown in Table~\ref{tab:advanced_attack}. \looseness=-1

\begin{table}[h!]
  \centering
  \footnotesize
  \caption{\revise{Advanced Attacks}. ASR is evaluated on the downstream classifiers trained on STL-10. The encoders are pre-trained on CIFAR10 with ResNet18 architecture and the attack target is a “truck” image from the attack dataset STL-10.}
  \label{tab:advanced_attack}
  \begin{tabular}{lccc}
    \toprule
      & WaNet & Invisible & BadEncoder \\
    \midrule
     ASR & 10.23 & 10.02 & 99.73\\
    \bottomrule
  \end{tabular}
\end{table}

From the experimental result, we can observe that image-size and sample-specific backdoor attacks can hardly be successful on self-supervised learning pre-trained encoders. 
These attacks can be successful and stealthy in supervised learning because there exist a concrete target label that can enable a strong hint during attacking. 
However, self-supervised learning only consider positive or negative pairs. Without distinct and obvious features (like patch-based triggers), such sample-specific triggers can hardly establish a strong correlation between victim images and target images. 

\section{More SSL Attacks} \label{sec:app:ssl_attack}

We study on 3 emerging SSL attacks, namely SSLBackdoor~\cite{Saha2022backdoorSSL},
CorruptEncoder~\cite{zhang2023corruptencoder} and CTRL~\cite{li2022demystifying}.

Our method successfully detected CorruptEncoder with \metricname of approximately 0.08 but failed to identify SSLBackdoor and CTRL, both of which had \metricname around 0.23.
The reason for our failure to detect SSLBackdoor was its low ASR (<10\%), which falls outside of our expected ASR range (>99\%), as stated in our threat model. 
Although SSLBackdoor had good false positive scores, its stealthy nature made it difficult to detect. 
Our method also failed to detect CTRL since it used a pervasive trigger that was outside of our threat model (patch-like triggers).
