% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[normalem]{ulem}
\usepackage{enumitem}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}



% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

%%%%%%%% Command

\input{math_commands}

\newcommand{\todoc}[2]{{\textcolor{#1}{\textbf{#2}}}}
\newcommand{\todored}[1]{{\todoc{red}{\textbf{[#1]}}}}
\newcommand{\todogreen}[1]{\todoc{green}{\textbf{[#1]}}}
% \newcommand{\todoblue}[1]{\todoc{blue}{\textbf{[#1]}}}
\newcommand{\todoorange}[1]{\todoc{orange}{\textbf{[#1]}}}
\newcommand{\todobrown}[1]{\todoc{brown}{\textbf{[#1]}}}
\newcommand{\todogray}[1]{\todoc{gray}{\textbf{[#1]}}}
\newcommand{\todopink}[1]{\todoc{pink}{\textbf{[#1]}}}
\newcommand{\todopurple}[1]{\todoc{purple}{\textbf{[#1]}}}
\newcommand{\todocyan}[1]{\todoc{cyan}{\textbf{[#1]}}}

\newcommand{\xz}[1]{\todored{XZ: #1}}
\newcommand{\siyuan}[1]{\todopink{Siyuan: #1}}
\newcommand{\gt}[1]{\todobrown{GT: #1}}
\newcommand{\shiwei}[1]{\todoorange{Shiwei: #1}}
\newcommand{\xx}[1]{\todopurple{XX: #1}}
\newcommand{\gs}[1]{\todogreen{GS: #1}}

\newcommand{\delete}[1]{{\color{orange}{\sout{#1}}}}
% \newcommand{\delete}[1]{}

\newcommand{\toolname}{{\sc DECREE}\xspace}
\newcommand{\revise}[1]{{\color{black}{#1}}}
\newcommand{\camera}[1]{{\color{black}{#1}}}
\newcommand{\metricname}{$\mathcal{PL}^1$-Norm\xspace}
\newcommand{\lonenorm}{$\normlone$-Norm}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{8562} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Detecting Backdoors in Pre-trained Encoders}
\author{
Shiwei Feng, Guanhong Tao, Siyuan Cheng, Guangyu Shen, \\
Xiangzhe Xu, Yingqi Liu, Kaiyuan Zhang, Shiqing Ma$^{\dagger}$, Xiangyu Zhang \\
Purdue University, $^{\dagger}$Rutgers University\\
{\tt\small
\{feng292, taog, cheng535, shen447, xu1415,  liu1751, zhan4057, xyzhang\}@cs.purdue.edu}\\
{\tt\small
$^{\dagger}$sm2283@cs.rutgers.edu}
}

\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
    Self-supervised learning in computer vision trains on unlabeled data, such as images or (image, text) pairs, to obtain an image encoder \revise{that learns high-quality embeddings for input data}. 
  Emerging backdoor attacks towards encoders expose crucial vulnerabilities of self-supervised learning, since downstream classifiers (even further trained on clean data) may inherit backdoor behaviors from encoders. Existing backdoor detection methods \revise{mainly focus on supervised learning settings and cannot handle pre-trained encoders especially when input labels are not available.} 
  In this paper, we propose \toolname, the first backdoor detection approach for pre-trained encoders, \revise{requiring neither classifier headers nor input labels}. 
  We evaluate  \toolname on over 400 encoders trojaned under 3 paradigms. 
  \revise{We show the effectiveness of our method on image encoders pre-trained on ImageNet and OpenAI's CLIP 400 million image-text pairs.}
  Our method consistently has a high detection accuracy even if we have only limited or no access to the pre-training dataset. \camera{Code is available at \url{https://github.com/GiantSeaweed/DECREE}.}\looseness=-1
\end{abstract}

%%%%%%%%% BODY TEXT
\input{introduction}

\input{related}
\input{motivation}
\input{method}
\input{evaluation}
\input{conclusion}

\section*{Acknowledgement}
\camera{
We thank the anonymous reviewers for their constructive comments. This research was supported, in part by IARPA TrojAI W911NF-19-S-0012, NSF 1901242 and 1910300, ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors.}


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\input{appendix}
\end{document}
