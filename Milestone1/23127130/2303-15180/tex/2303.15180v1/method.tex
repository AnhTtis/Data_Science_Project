\section{Design of \toolname} \label{sec:method}

As discussed in Section~\ref{sec:motivation}, existing backdoor scanners either require the knowledge of the attack target or are not applicable to directly scanning encoders. 
A backdoor detection method for pre-trained encoders ought to meet the following design goals: (1) no knowledge of downstream tasks (including data samples or labels); (2) no knowledge of the attack target; and (3) directly scanning encoders without training a downstream application classifier.

In this section, we first make a few observations on backdoor attacks in SSL (Section~\ref{sec:design:intuition}) and explain the intuitions of our design. 
We then present the technical details for 
self-supervised trigger inversion (Section~\ref{sec:trigger_inversion}) and  backdoor identification (Section~\ref{sec:backdoor_id}).



\begin{table}[h]
  \centering
  \footnotesize
  \caption{Cosine similarity within 1024 random CIFAR10 images. Both clean and backdoored encoders are pretrained on CIFAR10.}
  \label{tab:cosine_observation}
  \begin{tabular}{lrr}
    \toprule
      &  Samples w/o Trigger & Samples w/ Trigger  \\
    \midrule
    Clean Encoder & 0.2193 & 0.2922  \\
    Backdoored Encoder & 0.2442 & 0.9904 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}
    \centering
    \begin{subfigure}[t]{.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/tsne_embed_m-True_d-True.onlyclean.pdf}
        \caption{Clean Encoder + Clean Input}
        \label{fig:cl_m_cl_d}
    \end{subfigure}
   \begin{subfigure}[t]{.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/tsne_embed_m-True_d-True.both.pdf}
        \caption{Clean Encoder + Trojaned Input}
        \label{fig:cl_m_tj_d}
    \end{subfigure}
    \begin{subfigure}[t]{.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/tsne_embed_m-False_d-False.onlyclean.pdf}
        \caption{Trojaned Encoder + Clean Input}
        \label{fig:tj_m_cl_d}
    \end{subfigure}
    \begin{subfigure}[t]{.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/tsne_embed_m-False_d-False.both.pdf}
        \caption{Trojaned Encoder + Trojaned Input}
        \label{fig:tj_m_tj_d}
    \end{subfigure}
    \caption{Embedding Space Distributions. Subfigures (a) and (b) are for a clean encoder and (c), (d) for a trojaned encoder; colors denote class labels; faded colors in (b) and (d) denote embeddings of clean samples. 
        For the clean encoder, even if the inputs are stamped with the ground truth trigger, the embeddings are well separable in (b). However in (d), a trojaned encoder produces similar embeddings for trojaned inputs. 
    }
    \label{fig:}
    \vspace{-10pt}
\end{figure}


\subsection{Observations and Intuitions}

\label{sec:design:intuition}

\noindent \underline{\textit{Observation I}}: Although SSL does not require labels during pre-training, 
the embeddings of samples with the same label (by 
the trained encoder) tend to cluster together whereas those 
of different labels tend to scatter, as visualized in Figure.~\ref{fig:cl_m_cl_d}. 
As shown in Table~\ref{tab:cosine_observation}, clean samples (of various classes) have an average cosine similarity of only 0.2193 on a clean encoder.

\noindent \underline{\textit{Observation II}}: A trojaned encoder produces highly similar embeddings for samples with trigger while a clean encoder does not. Table~\ref{tab:cosine_observation} shows that, in a clean encoder, the trigger can increase the cosine similarity of samples from 0.2193 to 0.2922 (in the first row). The increase is limited and insignificant. As shown in Figure.~\ref{fig:cl_m_tj_d}, the clean encoder can still correctly separate inputs with the trigger.
In contrast, as the backdoor attack forces the samples with trigger to be close to the attack target, it creates a {\it dense area} (shown in Figure.~\ref{fig:tj_m_tj_d}) in a backdoored encoder where embeddings share a high similarity (0.9904).

\noindent \underline{\textit{Observation III}}: Compared to clean encoders, backdoored encoders need much smaller perturbations to cause samples to fall into the dense area. Figure.~\ref{fig:tj_m_tj_d} illustrates that the dense area (of a trojaned encoder) is surrounded by and close to clusters of clean samples. However, in the clean encoder, larger perturbations are required to induce highly similar embeddings for input samples, as the clean encoder produces more scattered embeddings.

%\revise{
\smallskip
\noindent{\bf Intuitions.} The dense area is where the attack target lies. This is analogous to the target label of backdoor attacks in supervised learning. The key difference is that, in the supervised learning setting, backdoor scanners can scan each label and then identify the most suspicious label as the target. 
However in SSL, there are no labels for scanners to iterate over. 
As such, existing backdoor scanners cannot 
be applied to determine whether a model is backdoored
in SSL.


To overcome the %aforementioned 
challenge, our design aims to decide whether there exists a central dense area in the encoder's embedding space (surrounded by the embeddings of clean samples).
Intuitively, a backdoored encoder with a central dense area only needs a small perturbation to push clean samples to the dense region.
A clean encoder, on the other hand, does not have such a dense area, meaning that  high similarity among embeddings cannot be easily achieved by stamping a small trigger on samples. Our technique hence detects backdoors at the encoder level, without the need of a target label. We elaborate design details in the rest of the section. \looseness=-1



\subsection{Methodology}

Trigger inversion is one of widely used techniques in backdoor scanning~\cite{wang2019neural, liu2019abs, guo2019tabor, shen2021backdoor, wang2020practical}. It works by optimizing a trigger pattern, which can induce the targeted misclassification while having a small trigger size. Existing trigger inversion was originally designed for supervised learning scenarios, where there are explicit labels. 
The size of trigger can be used as a metric to quantify the distance between the target label and other non-target labels.
In SSL, however, no explicit label exists. Existing trigger inversion is not able to optimize or update the pattern towards some intended objective (a target label). Inspired by the above observations, 
we propose to find the aforementioned dense area with only a small trigger. 
It can be formulated as a constrained optimization problem. With the constraint that samples stamped with the same trigger must have similar embeddings, the trigger size shall be optimized to the minimal. 


\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/overview.pdf}
    \caption{\toolname Overview
    }
    \label{fig:overview}
    \vspace{-10pt}
\end{figure}


Figure.~\ref{fig:overview} shows the overview of our technique. 
A randomly initialized trigger and a shadow dataset (e.g., a subset of pre-training dataset) are fed into the subject encoder to compute embeddings. The cosine similarity of these embeddings guides the optimization of trigger. With the constraint that samples stamped with the same trigger must have similar embeddings, the trigger size is iteratively optimized to the minimal. The optimized trigger is used for calculating a metric that gauges the normalized trigger size. The metric is then used to determine if the encoder is trojaned.


In Section~\ref{sec:trigger_inversion}, we explain the details of our self-supervised trigger inversion. In Section~\ref{sec:backdoor_id}, we demonstrate how to use inverted triggers to conduct encoder-level backdoor detection. 


\subsubsection{Self-supervised Trigger Inversion} \label{sec:trigger_inversion}
% \textbf{Design.}
To generate a trigger that can induce intended backdoor behavior in an encoder, we use two trainable variables, a mask and a pattern, to denote the trigger pattern. Specifically, the mask is utilized to indicate how much a pixel on the input image is replaced by the pattern. 
We use the following equation to formalize the trigger injection:
\begin{equation}
    \mathcal{F}(\vx, \vm, \vt)=\vx',
    \label{eq:trigger_func}
    \vspace{-18pt}
\end{equation}

\begin{equation}
    \vx'_{i,j,c}=\vm_{i,j} \cdot \vx_{i,j,c} + (1-\vm_{i,j}) \cdot \vt_{i,j,c},\ 
    \forall i \in H, j \in W, c \in C.
    \label{eq:trigger_stamp}
\end{equation}

$\gF$ denotes a function that stamps trigger pattern $\vt$ onto an input image $\vx$ and outputs an backdooored image $\vx'$; $\vm$ is a mask
indicates how much the original pixel values are retained. It has continuous values ranging from 0 to 1. The input image has three dimensions, namely, height $H$, width $W$, and channel $C$; $\vx_{i,j,c}$ refers to the pixel value of image $\vx$ at height $i$, width $j$, and channel $c$. Note that $\vm_{i,j}$ only has two dimensions as the mask is applied on a pixel in a way ranging from replacing it ($\vm_{i,j} = 0$) to retaining it ($\vm_{i,j} = 1$),
regardless of the color channels.

The goal of self-supervised trigger inversion is to optimize a trigger such that clean samples stamped with the trigger have highly similar embeddings. In SSL, the cosine similarity is commonly used as a metric to denote the distance between a pair of inputs~\cite{chen2020simclr,chen2020bigsimclrv2,Radford2021clip}. We leverage the same metric to measure how close the embeddings of trigger-stamped samples are. Formally, 
given two inputs $\vx_p$ and $\vx_q$ from a dataset $\gD=\{\vx_1,\cdots,\vx_N\}$, we have:\looseness=-1
\begin{equation}
    \gL_{p,q}(E,\vm,\vt)=-cos \Big(E\big(\gF(\vx_p,\vm,\vt)\big), E\big(\gF(\vx_q,\vm,\vt)\big)\Big).
    \label{eq:similarity}
\end{equation}

$E$ is the subject encoder, and $\vm$ and $\vt$ are the trigger variables discussed in Eq.~\ref{eq:trigger_func} and Eq.~\ref{eq:trigger_stamp}. The two inputs $\vx_p$ and $\vx_q$ are transformed by function $\gF$ in Eq.~\ref{eq:trigger_func} to stamp the trigger. 
To achieve high similarities among samples that approximate the search for the dense area in the embedding space of encoder, \toolname samples a batch of inputs to stabilize the search process. The average of pair-wise similarity within a batch is computed as follows:

\vspace{-8pt}
\begin{equation}\label{eq:avg_sim}
    \gL = \frac{1}{N^2} \sum_{p=1}^{N}\sum_{q=1}
^{N} \gL_{p,q}(E,\vm,\vt),
\end{equation}
 where $N$ is the batch size; $\gL$ is used as the constraint during optimization, assuring that the samples stamped with the optimized trigger are in the dense area in embedding space.


We then leverage {\it Observation III} in Section~\ref{sec:design:intuition} and minimize the size of the trigger. We use the $\normlone$ norm to quantify the mask size. Self-supervised trigger inversion is formulated as the following constrained optimization problem.


\vspace{-8pt}
\begin{equation}
    \text{min} \lVert \vm \rVert_1, s.t.\ \gL < \beta.
    \label{eq:detect_loss}
\end{equation}
$\beta$ is a threshold assuring the average similarity is high.


During trigger inversion, a set of clean samples are needed for the optimization. As we do not assume the knowledge of any downstream tasks like what existing backdoor scanners do, we leverage the pre-training dataset that is used for constructing the encoder. 
It is impractical to have the whole pre-training dataset as one can directly train a new clean encoder on it without the need of scanning the given encoder. 
We hence only assume a small subset of the pre-training dataset ($<10\%$) for trigger inversion. In extreme cases, the pre-training dataset of the given encoder may not be publicly available. We resort to leveraging an external dataset, called \textit{shadow dataset}, for trigger inversion and backdoor scanning. Since the pre-training set and downstream datasets do not share the same set of samples or are even from different data distributions, the attack effectiveness solely depends on building a strong connection between the injected trigger and the target embedding. It hence does not matter what data are used for trigger inversion. Our results in Section~\ref{sec:eval_no_access}
demonstrate that \toolname{} can indeed effectively detect backdoored encoders using a shadow dataset.


\subsubsection{Backdoor Identification}\label{sec:backdoor_id}

Recall that a challenge of detecting backdoors in SSL is that there are no labels. Therefore existing backdoor scanners cannot identify the potential target label, which is the key to determining
whether a model is backdoored in supervised learning.
To overcome this challenge, \toolname introduces a new metric $\mathcal{PL}^n$:

\vspace{-8pt}
\begin{equation}
\mathcal{PL}^n(E) = \frac{\lVert \widetilde{\vm} \rVert_n}{\lVert \hat{x} \rVert_n}.
\end{equation}
\vspace{-8pt}

$\lVert\ \cdot\ \rVert_n$ denotes the $\normln$ norm of a vector; $\widetilde{\vm}$ denotes the trigger inverted from a given encoder $E$; and $\hat{x}$ denotes the input sample
that has the maximum $\normln$ norm in the input space of that encoder.
$\mathcal{PL}^n(E)$, denoting the {\it Proportionate-$\normln$ Norm} of an encoder $E$, is thus defined as the ratio of the inverted trigger's $\normln$ norm to the maximum $\normln$ norm of the encoder's input space.
Note that $\mathcal{PL}^n$ is an encoder-level metric, approximating the distance from clean samples to the dense area. In this way, \toolname does not need to identify the target label.

As discussed in Section~\ref{sec:design:intuition}, triggers inverted from backdoored encoders shall be smaller than those from clean encoders. Thus for a backdoored encoder, \toolname has a better chance to invert a small trigger that can induce the encoder to output two similar embeddings for two dissimilar inputs.
Based on the proposed $\mathcal{PL}^n$ and the above intuition, \toolname uses the following formula to identify backdoors in encoders.

\vspace{-10pt}
\begin{equation}
    \widetilde{P(E)} = \mathbb{B}\Big(\mathcal{PL}^1(E), \tau\Big).
\end{equation}
\vspace{-10pt}

$\widetilde{P(E)}$ is the estimated probability that a given encoder $E$ contains a backdoor. $\mathbb{B}$ is a binary step function that returns 1 if its first parameter is less than a given threshold $\tau$ and  0 otherwise. Essentially, if the inverted trigger of a given encoder only occupies a small part of the input data sample, we consider the encoder is very likely a trojaned encoder.

