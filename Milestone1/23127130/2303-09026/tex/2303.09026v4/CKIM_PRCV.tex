% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
%\usepackage[american]{babel}
\usepackage{mathtools} % amsmath with fixes and additions
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{url}
\usepackage{float}
%%
\usepackage{times}
\usepackage{soul}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{booktabs}
\newtheorem{knowledge}{Knowledge}
\begin{document}
%
\title{Commonsense Knowledge Assisted Deep Learning for Resource-Constrained and Fine-Grained Object Detection}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
%\author{Anonymous} 
\author{Pu Zhang\and
Bin Liu\thanks{Correspondence author}}

\authorrunning{P. Zhang et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
%\institute{Anonymous}
\institute{Research Center for Applied Mathematics and Machine Intelligence,\\
Zhejiang Lab, Hangzhou, 311121 China \\
\email{\{puz,liubin\}@zhejianglab.com}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
This paper addresses fine-grained object detection in scenarios with limited computing resources, such as edge computing. Deep learning (DL), particularly through the use of deep neural networks (DNNs), has become the primary approach to object detection. However, obtaining accurate fine-grained detection requires a large DNN model and a significant amount of annotated data, presenting a challenge for modern DL object detectors in resource-constrained cases.
To address this issue, we propose an approach that utilizes commonsense knowledge to assist a coarse-grained object detector in achieving accurate fine-grained detection results. Specifically, we introduce a commonsense knowledge inference module (CKIM) that processes the coarse-grained labels produced by a benchmark coarse-grained DL detector to generate fine-grained labels. Our CKIM explores both crisp-rule and fuzzy-rule based inference methods, with the latter being employed to handle ambiguity in the target semantic labels.
We implement our method based on two modern DL detectors, including Mobilenet-SSD, and YOLOv7-tiny. Experimental results demonstrate that our approach achieves accurate fine-grained detections with a reduced amount of annotated data, and smaller model size. Our code is available at \url{https://github.com/ZJLAB-AMMI/CKIM}.
\keywords{Deep learning \and Commonsense knowledge \and Object detection \and Fine-grained detection \and Edge computing}
\end{abstract}
\section{Introduction}
Object detection is a common computer vision task that aims to locate and classify objects within images or videos. Fine-grained object detection is a vital subset of this field, used to detect objects belonging to fine-grained categories such as specific bird species, dog breeds, and car brands. Deep learning (DL) – specifically, learning with deep neural networks (DNNs) – has become the dominant approach to object detection as it significantly improves detection accuracy. DNN-based object detectors such as YOLO \cite{redmon2018yolov3} and Faster R-CNN \cite{ren2015faster} are widely used for this task.

However, the benefits of these advancements come at a cost: substantial computational and storage resources, along with a vast quantity of annotated data, are required. This challenge is further compounded in the case of fine-grained object detection, which demands even larger models, more annotated data, and greater computational budgets to train and store the model \cite{zaidi2022survey}. Consequently, fine-grained object detectors are often deployed on cloud servers that have sufficient computing resources and labeled data available.

This paper addresses the challenge of performing DNN-based fine-grained object detection on resource-constrained edge devices, such as smartphones and driving assistance systems. Edge computing has made significant progress in recent years within the context of the Internet of Things. The amount of high-dimensional data produced by edge devices, particularly images and videos, has been increasing rapidly. Fine-grained object detection is required for various applications that involve real-time image or video processing at the edge, such as autonomous vehicles \cite{zhang2021fairmot} and tracking multiple objects \cite{li2019gs3d}. However, transmitting this data to a remote cloud server for labeling and processing is not feasible due to the unacceptable latency in response, excess bandwidth consumption, and privacy concerns \cite{chen2019deep}. Thus, training and deploying object detection models on edge devices has emerged as a significant trend.

However, typical edge devices have limited computing capabilities and memory resources, which cannot meet the demands of running modern DNN-based object detectors. To address this issue, various network compression techniques have been proposed, such as network pruning \cite{hassibi1993optimal}, knowledge distillation \cite{hinton2015distilling}, and depthwise convolution \cite{howard2017mobilenets,8578814}, for designing lightweight models.

In this paper, we propose an unexplored approach for DNN-based fine-grained object detection on resource-constrained edge devices. Our idea is to utilize commonsense knowledge to assist a coarse-grained object detector in obtaining accurate fine-grained detections. Specifically, we introduce a commonsense knowledge inference module (CKIM) to a backbone coarse-grained object detector. This CKIM can infer fine-grained object labels from the output of the backbone coarse-grained detector. Comparing Fig.\ref{fig:general_model} and Fig.\ref{fig:CKIM} highlights the differences between our method and typical fine-grained detectors.
\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{figures/FG_detector.eps}
\caption{An example show of the working mechanism of a typical fine-grained object detector. Typical fine-grained object detectors require a vast amount of images with fine-grained labels and a large enough model to achieve accurate detection results.}
\label{fig:general_model}
\end{figure}
\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{figures/CKIM.eps}
\caption{An example show of the working mechanism of our proposed method. Our method utilizes only a coarse-grained object detector that requires much less labeled data for model training and significantly less memory for model storage than the typical fine-grained object detectors depicted in Fig.\ref{fig:general_model}. We introduce a commonsense knowledge inference module (CKIM) that maps coarse-grained outputs of the coarse-grained object detector to fine-grained predictions. The commonsense knowledge used in our method can be acquired from either a human expert or a large language model.
}
\label{fig:CKIM}
\end{figure}

Fig.\ref{fig:general_model} illustrates that a typical fine-grained object detector requires an extensive amount of labeled images with `adult' or `child' labels to train a large enough model for detecting adults and children. However, according to commonsense knowledge, `adult' and `child' are subcategories of the same coarse-grained class, namely `person'. Typically, objects are recognized as `adults' if their size is large and as `children' if it is small. Our proposed method, illustrated in Fig. \ref{fig:CKIM}, introduces a CKIM that leverages this type of commonsense knowledge. The DNN detector generates only coarse-grained outputs, namely the detection of `person'-type objects and their positions, while the CKIM infers fine-grained object labels from these coarse-grained outputs.

\textbf{Our contributions}:
\begin{itemize}
    \item We propose a deep learning method for fine-grained object detection that leverages commonsense knowledge and is suited for resource-constrained scenarios. Our approach employs a coarse-grained object detector and a commonsense knowledge inference module (CKIM) to map the coarse-grained outputs of the detector into fine-grained predictions.
     \item We present two types of CKIM for use in our method: one using crisp-rule-based inference and the other using fuzzy-rule-based inference. The fuzzy-rule-based inference is employed to address any semantic ambiguity related to the fine-grained labels.
     \item We integrate the CKIM into modern object detectors, including MobilenetV3-SSD \cite{howard2019searching}, and Yolov7-tiny \cite{wang2022yolov7}, and evaluate our method's performance through experiments. The results demonstrate that our approach achieves accurate fine-grained detections with a reduced amount of annotated data, and smaller model size.
\end{itemize}

The structure of this paper is as follows. Section \ref{sec:related} provides a brief overview of related work. Section \ref{sec:CKIM} describes our proposed CKIM-based detector, including the crisp-rule and fuzzy-rule based implementations of the CKIM. Section \ref{sec:experiment} presents the experimental setup and results. Section \ref{sec:discussion} discusses limitations of our approach. Finally, Section \ref{sec:conclusions} concludes the paper.
\section{Related Works}\label{sec:related}
In this section, we provide a brief overview of related work on DNN-based visual object detection and rule-based inference.
\subsection{Modern DNNs for Visual Object Detection}
DNN architectures for object detection can be divided into two types. The first type includes networks with separate modules for candidate region generation and classification, such as Faster R-CNN \cite{ren2015faster}, which are known as two-stage detectors. The second type is single-stage detectors, which directly produce object categories and their bounding boxes in a single step and use pre-defined differently sized boxes to locate objects. Single-stage detectors are suitable for real-time, resource-constrained scenarios such as edge computing because they have lightweight designs and require less time to make predictions. As such, we employ them as benchmark object detectors in this paper.
\subsubsection{You Only Look Once (YOLO)} is one of the most widely used single-stage object detectors \cite{redmon2016you}. In YOLO, the input image is divided into a grid with $S \times S$ cells, and each cell predicts bounding boxes and their corresponding class probabilities. Over time, newer versions of YOLO have been proposed to improve inference speed and reduce model size. For instance, YOLOv4 \cite{bochkovskiy2020yolov4} achieved state-of-the-art (SOTA) performance for real-time object detection, while YOLOv7 is a more advanced SOTA real-time object detector. Both YOLOv4 and YOLOv7 have lightweight versions, namely YOLOv4-tiny and YOLOv7-tiny, which employ smaller model architectures that are better suited for edge computing scenarios \cite{wang2021scaled,wang2022yolov7}. In our experiments, we use YOLOv7-tiny as a baseline method.
\subsubsection{MobileNet-SSD} is an efficient DL method for object detection in mobile computing scenarios. It employs a Single Shot MultiBox Detector (SSD) \cite{liu2016ssd} as the decoder and MobileNet \cite{howard2017mobilenets} as the feature extractor. SSD was the first single-stage detector that performed comparably to two-stage detectors. MobileNet was specifically designed for vision applications on mobile devices, with traditional convolution layers replaced by depthwise separable and pointwise convolution layers to reduce model size \cite{sandler2018mobilenetv2}. An advanced version of MobileNet, MobileNetV3, uses a search algorithm to optimize the network architecture \cite{howard2019searching}. In our experiments, we use MobileNetV3-SSD as a baseline method.
\subsection{Rule-based Inference}
Rule-based inference is a conventional methodology for implementing knowledge reasoning, where knowledge is represented by a collection of ``IF-THEN" rules. The reasoning mechanism is that if a novel instance matches the antecedent of any rule in the rule base, the corresponding rule consequent will be the result \cite{Grosan2011}. In the context of this work, the association between fine-grained and coarse-grained categories may involve semantic imprecision and ambiguity related to size, distance, age, and color of objects. Therefore, using only crisp rules is not enough. We employ fuzzy logic and fuzzy set theory \cite{2010Fuzzy,ZADEH1965338}. Fuzzy rule-based systems (FRBS) allow such terms to be precisely described by fuzzy sets, enabling the inference process to resemble human reasoning.

Fuzzy sets can be generally defined as follows:
\begin{equation}
M = {(a,\mu_M(a)|a\in A,\mu_M(a) \in [0,1])}
\end{equation}
where $a$ denotes an instance, $A$ a collection of instances, and $\mu_A(x)$ the membership function that describes the partial degrees to which an instance $a$ is judged to belong to the fuzzy set $M$.

Mamdani models \cite{Scherer2012} are widely used in FRBS. They use fuzzy sets as rule antecedents and consequents, as shown in Figure \ref{fig:mamdani}, and perform inference by integrating the conclusions of individual rules that overlap with the input. The weight of a certain rule is determined by the match degree between the input instance and the rule antecedent.
\begin{figure}[ht]
\centering
\includegraphics[width=0.45\textwidth]{figures/mamdani.eps}
\caption{Illustrative example of the inference process with a Mamdani model. $R_{i},R_{j}$ represent two fuzzy rules; $(A_i, B_i), (A_j, B_j) $ are the corresponding antecedents for the rules $R_{i},R_{j}$ respectively; $C_i, C_j$ are their consequent for fuzzy rule $R_{i},R_{j}$ respectively; $(x,y)$ is the input instance; $\alpha_i, \alpha_j$ stand for matching degrees between the input and the two rules.}
\label{fig:mamdani}
\end{figure}
\section{CKIM Assisted Object Detection}\label{sec:CKIM}
In this section, we present our approach to fine-grained object detection based on CKIM in detail. We explain how to specify CKIM using commonsense knowledge and how to use it to assist a coarse-grained deep detector in doing fine-grained object detection.

Fig.\ref{fig:CKIM} shows an example of the working mechanism of our method, which consists of two parts: a coarse-grained object detector and CKIM. The former part outputs coarse-grained labels and a bounding box for each object. The bounding box can be represented by the equation:
\begin{equation}
box(C,X,Y,W,H)
\end{equation}
where $C$ denotes the coarse-grained label, $(X,Y)$ denotes the coordinates of the centre of the bounding box, and $W$ and $H$ denote the width and height of the bounding box, respectively. We focus on size-related fine-grained labels in this work. The real size of an object is naturally closely related to the size of the object's bounding box, which can be produced by a qualified object detector. In addition, according to commonsense knowledge as follows
\begin{knowledge}
\emph{An object appears to decrease in size as it moves farther away and appears to enlarge as it moves closer to the observer}.
\end{knowledge}
\noindent
the real size of an object is related to its distance from the camera. For an object in an image, we can estimate its real distance to the camera based on its distance from the center of its bounding box to the bottom of the image. This is because:
\begin{knowledge}
\emph{the object's distance to the camera (DtoC) is strongly connected to the distance from the center of its bounding box to the bottom of the image (CtoB)}.
\end{knowledge}
\noindent The connection between CtoB to DtoC is illustrated in Fig.\ref{fig:distance}.
\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{figures/distance.eps}
\caption{An example image that illustrates the connection between an object's distance to the camera (DtoC) and the distance from the center of the object to the bottom of the image (CtoB). As shown in the image, CtoB of object A, denoted by CtoB$_A$, is larger than CtoB$_B$. This information can be used to infer that DtoC of object A, denoted by DtoC$_A$, is larger than DtoC$_B$, which is clearly true as shown in the picture.}
\label{fig:distance}
\end{figure}

The aforementioned commonsense knowledge provides us with an opportunity to reason size-related fine-grained labels from the outputs of a coarse-grained object detector, such as the coarse-grained labels and bounding boxes of the objects.
The purpose of CKIM is to infer fine-grained labels based on the size of the objects' bounding boxes (BoxS) and their distances to the camera (DtoC). To achieve this, we normalize values of $X$, $Y$, $W$, $H$ according to the width and height of the image, define the size of the bounding box (BoxS) as its area, and define DtoC as 1 minus Y. Then, we calculate BoxS and DtoC as follows:
\begin{equation}
\begin{aligned}
BoxS &= W \times H \\
DtoC &= 1-Y
\end{aligned}
\end{equation}
We develop two types of CKIM, corresponding to crisp-rule and fuzzy-rule based inference.
\subsection{Crisp-rule based CKIM}
In crisp rule-based systems, Boolean logic is followed, and an object can only belong to one class or not belong to it. We model the relationship between the real size of an object and its attributes given by a coarse-grained object detector using a logistic regression function \cite{lavalley2008logistic}.

To simplify the presentation, we consider size-related object labels: `large', `middle', and `small'. We use the one-vs-rest method to perform crisp rule inference. The decision function of the crisp rule is defined as follows:
\begin{equation}
\begin{aligned}
f_{ml}(BoxS, DtoC) &= \frac{1}{1+e^{w_{ml}^{T}x}}, x \triangleq (BoxS, DtoC)\\
f_{sm}(BoxS, DtoC) &= \frac{1}{1+e^{w_{sm}^{T}x}}, \\
\end{aligned}
\end{equation}
where $f_{sm}$ denotes the decision function for small sized and middle sized objects, and $f_{ml}$ the decision function for middle sized and large sized objects, $w^T$ is a set of parameters optimized by minimizing a cost function in the same way as in typical logistic regression.

Accordingly, the CKIM is defined as follows:
\\

If $f_{ml}(BoxS, DtoC) > 0$,

\hspace{2em} then the object's fine-grained label is `large';

If $f_{ml}(BoxS, DtoC) < 0$ and $f_{sm}(BoxS, DtoC) > 0$,

\hspace{2em} then the fine-grained label is `middle';

If $f_{sm}(BoxS, DtoC) < 0$,

\hspace{2em} then the fine-grained label is `small'.
\\
\subsection{Fuzzy-rule based CKIM}
In real-life cases, there may be continuous-valued attributes (e.g., size, distance, age) that are difficult to classify with discrete semantic descriptions (e.g., large or small, near or far, young or old), resulting in semantic vagueness or uncertainty. Fuzzy logic provides a multi-value logic, in which such vague semantics can be strictly formulated and precisely studied.

To adapt our method to more general scenarios, we propose a fuzzy-rule based approach for implementing CKIM. Fuzzy rule-based systems allow an object to match different categories with different memberships. We adopt the aforementioned Mamdani model for this task, which uses fuzzy sets as rule antecedents and consequents. Two main categories of membership functions are typically used in constructing fuzzy sets \cite{2021Approximate}:

(1) Polygonal functions, such as triangular shaped and trapezoidal shaped functions;

(2) Nonlinear functions, including Gaussian shaped and generalised bell shaped functions.

Here, we adopt the Gaussian-shaped membership function \cite{kreinovich1992gaussian}. As before, we consider size-related object labels, namely `large', `middle', and `small'.

In our fuzzy rule inference module, the antecedents of rules are defined as follows:
\begin{equation}
\begin{aligned}
M_L(x) &= \mathcal{N}(x|\mu_L,\Sigma_L)\\
&=\frac{1}{2\pi|\Sigma_L|^{1/2}} \exp \left\{- \frac{1}{2} \left(x-\mu_L\right)^{'} \Sigma_L^{-1} \left(x-\mu_L \right)  \right\}\\
M_M(x) &= \mathcal{N}(x|\mu_M,\Sigma_M)\\
&=\frac{1}{2\pi |\Sigma_M|^{1/2}} \exp \left\{- \frac{1}{2} \left(x-\mu_M\right)^{'} \Sigma_M^{-1} \left(x-\mu_M \right)  \right\}\\
M_S(x) &= \mathcal{N}(x|\mu_S,\Sigma_S)\\
&=\frac{1}{2\pi |\Sigma_S|^{1/2}} \exp \left\{- \frac{1}{2} \left(x-\mu_S\right)^{'} \Sigma_S^{-1} \left(x-\mu_S \right)  \right\}\\
\end{aligned}
\end{equation}
where $x\triangleq(BoxS, DtoC)$ the same as before; $S$, $M$ and $L$ in the subscripts denote `small', `middle', and `large', respectively; $\mu$ and $\Sigma$ denote the mean and covariance matrix of the data distribution.

The fuzzy-rule based CKIM is designed as follows:\\
If $x$ matches $M_L$, then object's label is `large' with degree $M_L(x)$;\\
If $x$ matches $M_M$, then object's label is `middle' with degree $M_M(x)$;\\
If $x$ matches $M_S$, then object's label is `small' with degree $M_S(x)$.

A crisp output is then calculated by a defuzzification approach to integrate membership degrees between the object and all rules.
Accordingly, the fine-grained result is determined by the $\arg\max$ function defined as follows:

\begin{equation}
\begin{aligned}
Label &=\underset{i\in\{S,M,L\}}{{\arg\max}\, M_i(x)}
\end{aligned}
\end{equation}

Note that since these two types of CKIM have few parameters to be optimized, the amount of data required to train them is almost negligible compared to those required to train a DNN model.
\section{Experiments}\label{sec:experiment}
We experimentally evaluated the performance of our proposed commonsense knowledge-assisted DNN detector. We compared our method against the SOTA methods, including YOLOv7-tiny, MobileNetv3-SSD, and YOLOv4. The former two methods are specially developed for resource-constrained scenarios, while YOLOv4 is not. Therefore, the model size of YOLOv4 is much larger than the others. We integrated CKIM with YOLOv7-tiny and MobileNetv3-SSD and assessed whether it resulted in improved performance.
\subsection{Experimental Setup}
\subsubsection{The CLEVR Dataset} is a benchmark dataset for Vision Question Answering (VQA) models \cite{johnson2017clevr}. It consists of pictures containing objects and questions related to them. The objects' attributes include size (big, small), color (brown, blue, cyan, gray, green, purple, red, yellow), material (metal, rubber), and shape (cube, cylinder, sphere). According to these attributes, objects are divided into 96 fine-grained categories. We term the original dataset as CLEVR-96. The left panel of Fig.\ref{fig:CLEVR} shows an example of this dataset.
In our experiment, we removed the size attribute to train the coarse-grained object detector that considers $96/2=48$ coarse-grained labels. Then, we predicted fine-grained size labels with our CKIM. The other involved fine-grained object detectors, namely YOLOv7-tiny, MobileNetv3-SSD, and YOLOv4, were all trained with the original datasets that have 96 labels.

To simulate complex environments in the real world, we constructed a new dataset by introducing a collection of objects of middle size, as shown in the right panel of Fig.\ref{fig:CLEVR}. It has 144 fine-grained classes. We term it as CLEVR-144 in what follows.

Both CLEVR-96 and CLEVR-144 contain 16,000 images as the training set, 2,000 images as the validation set, and the other 2,000 images as the test set.
\begin{figure}[ht]
\centering
\includegraphics[width=0.43\textwidth]{figures/CLEVR_Ori.eps}\quad \includegraphics[width=0.43\textwidth]{figures/CLEVR_new.eps}
\caption{Left: an example image in the CLEVR-96 dataset, where the object size attribute is specified as either `large' or `small'; Right: an example image in the CLEVR-144 dataset, where the object size attribute can be `large', `middle', or `small'}
\label{fig:CLEVR}
\end{figure}
\subsection{Performance Metrics}
In our evaluation of each method for resource-constrained edge computing scenarios, we considered three different perspectives: detection accuracy, model size, and processing latency: \\
(1) Detection Accuracy is measured by the mean Average Precision while IoU=0.5 (mAP@0.5), which is a commonly used metric for evaluating object detectors. It calculates the mean of the average precision over all classifications for every bounding box with an IoU greater than 0.5. Larger mAP@0.5 means higher accuracy.\\
(2) Model Size is measured by the memory space that the model consumes. This is important in resource-constrained scenarios where memory is limited.\\
(3) Latency is defined as the average time a method takes to process one image. In our experiments, the time unit was set as millisecond (ms). This is also an important factor to consider in resource-constrained scenarios where real-time processing is required.
\begin{table}[hb]
\begin{center}
\begin{tabular}{cccc}
\toprule
%& \multicolumn{3}{c}{CLEVR-96} \\
%\cline{2-4}
 &Detection Accuracy $\uparrow$ & Model Size $\downarrow$ & Latency $\downarrow$\\
\hline
MobileNetv3\_SSD & 0.968& 87.61MB& 82\\
MobileNetv3\_SSD-L+crisp &\textbf{0.978} &\textbf{49.63MB+2KB} & \textbf{60}\\
\hline
YOLOv7-tiny &0.972 &23.31MB& 70 \\%\\
YOLOv7-tiny-L+crisp &\textbf{0.983} &\textbf{22.89MB+2KB} & \textbf{62}\\%\\
\bottomrule
\end{tabular}
\end{center}
\caption{Fine-grained object detection performance on the CLEVR-96 dataset using all 16,000 images for model training. The best performances for each model type are marked in \textbf{bold}. In the table, X-L denotes a coarse-grained version of a fine-grained model X, where X can be MobileNetv3\_SSD or YOLOv7-tiny. X-L+crisp denotes our method that uses the model X-L assisted with crisp rule based CKIM for fine-grained object detection. As is shown, our method outperforms both baseline methods in terms of Detection Accuracy, Model Size and Latency. Since this dataset only has two fine-grained labels ('small' and 'large'), there is no label ambiguity issue involved, and therefore, only crisp-rule based inference is considered for our method.}
\label{tab:full1}
\end{table}

\begin{table}[ht]
\begin{center}
\begin{tabular}{cccc}
\toprule
%& \multicolumn{3}{c}{CLEVR-144} \\
%\cline{2-4}
 &Detection Accuracy $\uparrow$ & Model Size $\downarrow$  & Latency $\downarrow$\\
\hline

MobileNetv3\_SSD & 0.970& 125.59MB& 84\\%11.87\\

MobileNetv3\_SSD-L+crisp & 0.968 &\textbf{49.63MB+2KB} & \textbf{61}\\%16.41\\

MobileNetv3\_SSD-L+fuzzy &\textbf{0.978} &\textbf{49.63MB+2KB} & 62 \\%15.98\\
\hline

YOLOv7-tiny &0.965& 23.73MB & 75\\%13.22\\

YOLOv7-tiny-L+crisp & 0.971 &\textbf{22.89MB+2KB} & 64\\%15.66\\

YOLOv7-tiny-L+fuzzy &\textbf{0.980} &\textbf{22.89MB+2KB} & \textbf{62}\\

\bottomrule
\end{tabular}
\end{center}
\caption{Fine-grained object detection performance on the CLEVR-144 dataset using all 16,000 images for model training. The best performances for each mode type are marked in \textbf{bold}. X-L+fuzzy denotes our method that uses the model X-L assisted with fuzzy rule based CKIM for fine-grained object detection, where X can be MobileNetv3\_SSD or YOLOv7-tiny. All the other terms are defined in the same way as in Table \ref{tab:full1}.}
\label{tab:full2}
\end{table}
\subsection{Experimental Results}
\subsubsection{On experiments with all 16,000 images being used for model training}
First we trained all the involved models using all 16,000 images in the training set. The performance of these models on the CLEVR-96 and CLEVR-144 datasets are shown in Tables \ref{tab:full1} and \ref{tab:full2}, respectively. It is shown that our proposed method significantly decreases the model size and the latency by using a lightweight coarse-grained object detector and then performing fine-grained detection with crisp-rule or fuzzy-rule based commonsense knowledge reasoning.
We also find that on the CLEVR-144 dataset, which involves three fine-grained labels (`small', `middle', and `large'), the fuzzy-rule based method is preferable to the crisp-rule based method. This demonstrates the ability of the fuzzy method to deal with semantic ambiguity in the fine-grained labels.

As part of our experiment, we also evaluated a baseline fine-grained YOLOv4 model. On the CLEVR-96 dataset, this model achieves a high detection accuracy of 0.998, but has a much larger model size of 246.35MB and latency of 176ms than the methods listed in Table \ref{tab:full1}. On the CLEVR-144 dataset, it achieves detection accuracy of 0.998 again, while it also has a much larger model size of 247.34MB and latency of 183ms than the methods listed in Table \ref{tab:full2}.

Despite achieving high accuracy, the baseline YOLOv4 model may not be practical for resource-constrained edge devices due to its large model size and high processing latency. In contrast, our proposed method using commonsense knowledge reasoning achieves higher detection accuracy while maintaining a smaller model size and lower processing latency than its counterpart baseline methods. 
These results demonstrate the potential of our approach for real-world applications where edge devices have limited resources, since it can achieve high accuracy while keeping model size and processing latency at an acceptable level.
\begin{table}[ht]
\centering
\begin{tabular}{cccc}
\toprule

 &Detection Accuracy $\uparrow$ & Model Size $\downarrow$  & Latency $\downarrow$ \\
\hline

MobileNetv3\_SSD &0.902& 87.61MB & 82\\

MobileNetv3\_SSD-L+crisp &\textbf{0.912}& \textbf{49.63MB+2KB}& \textbf{60}\\

\hline
YOLOv7-tiny &0.928& 23.31MB& 71 \\
YOLOv7-tiny-L+crisp &\textbf{0.984}& \textbf{22.89MB+2KB} & \textbf{61} \\

\bottomrule
\end{tabular}
\caption{Fine-grained object detection performance on the CLEVR-96 dataset using 5,000 randomly selected images for model training. The best performances for each mode type are marked in \textbf{bold}. All terms are defined in the same way as in Table \ref{tab:full1}. As part of our experiment, we also evaluated a baseline fine-grained YOLOv4 model. This model achieves a high detection accuracy of 0.972, but has a much larger model size of 246.35MB and latency of 178ms than the methods listed here.}
\label{tab:50001}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{cccc}
\toprule

 & Detection Accuracy $\uparrow$  & Model Size $\downarrow$  & Latency $\downarrow$\\
\hline
MobileNetv3\_SSD & 0.857& 125.59MB & 83\\

MobileNetv3\_SSD-L+crisp & 0.862& \textbf{49.63MB}+2KB & 60\\

MobileNetv3\_SSD-L+fuzzy  &\textbf{0.877} &\textbf{49.63MB}+2KB & \textbf{59}\\

\hline
YOLOv7-tiny &0.865& 23.73MB& 72\\

YOLOv7-tiny-L+crisp &0.892 &\textbf{22.89MB+2KB} & 65\\

YOLOv7-tiny-L+fuzzy & \textbf{0.914} &\textbf{22.89MB+2KB}& \textbf{62}\\

\bottomrule
\end{tabular}
\caption{Fine-grained object detection performance on the CLEVR-144 dataset using 5,000 randomly selected images for model training. The best performances for each mode type are marked in \textbf{bold}. All terms are defined in the same way as in Table \ref{tab:full2}. As part of our experiment, we also evaluated a baseline fine-grained YOLOv4 model. This model achieves a high detection accuracy of 0.957, but has a much larger model size of 247.34MB and latency of 175ms than the methods listed here.}
\label{tab:50002}
\end{table}
\subsubsection{On experiments with 5,000 randomly selected images being used for model training}
In order to simulate cases where the edge device does not have enough memory space to store all training data, we conducted experiments using 5,000 randomly selected images from the training set for model training. The experimental results on the CLEVR-96 and CLEVR-144 datasets are presented in Tables \ref{tab:50001} and \ref{tab:50002}, respectively.

As shown in the tables, for the case with less training data, the benefits given by our proposed method become more remarkable. Our method outperforms all baseline methods according to all performance metrics. In addition, we once again observed the benefits of fuzzy inference on the CLEVR-144 dataset. The fuzzy-based method significantly outperforms the crisp-rule based method in terms of detection accuracy.
\subsubsection{Training process comparison for fine-grained and coarse-grained object detectors}
In order to investigate the difference in the training process between fine-grained and coarse-grained object detectors, we conducted an experiment using the YOLOv7-tiny model. Specifically, we compared the training process of a YOLOv7-tiny based fine-grained detector and a YOLOv7-tiny based coarse-grained detector. The results, shown in Fig.\ref{fig:train_5000}, demonstrate that the coarse-grained detector converges much faster than its fine-grained counterpart on both the CLEVR-96 and CLEVR-144 datasets.

Since our proposed fine-grained object detector consists of a coarse-grained detector and a lightweight CKIM, the convergence speed of our method is almost the same as the coarse-grained detector it employs. This indicates that the convergence speed of our proposed fine-grained detector is much faster than its counterpart fine-grained detector.

Overall, these results demonstrate that our proposed method achieves efficient and effective fine-grained object detection by leveraging a lightweight commonsense knowledge inference module with a coarse-grained object detector, achieving high accuracy while maintaining fast convergence times.
\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{figures/train_5000.eps}
\caption{Training process of a pair of fine-grained and coarse-grained object detectors}
\label{fig:train_5000}
\end{figure}
\section{Discussions}\label{sec:discussion}
While our experimental results presented above are encouraging, it is important to note that our case study focused only on size-related fine-grained labels. A natural question arises as to how to generalize our approach to work for more general fine-grained labels. This question can be broken down into two sub-questions: How and where to obtain useful commonsense knowledge, and how to design an appropriate CKIM that effectively fuses information given by a coarse-grained object detector and commonsense knowledge.

We argue that a promising approach to answering the first sub-question is to leverage large language models (LLMs), which can be viewed as a large knowledge base. If a flexible interface between the algorithm agent and the LLM can be implemented, then LLMs can be a valuable source of commonsense knowledge. The CKIM can be seen as a ``bridge" where we let the commonsense knowledge ``meet" the coarse-grained outputs of a lightweight coarse-grained object detector for addressing fine-grained object detections. The CKIM can be designed based on domain knowledge or learned from data. In our case study, we designed the CKIM based on two pieces of domain knowledge related to size-related fine-grained labels. However, in more general fine-grained object detection tasks, it may be necessary to learn the CKIM directly from data. Alternatively, other sources of knowledge such as expert annotations or external databases could be used to inform the design of the CKIM. Ultimately, the design of the CKIM should depend on the specific application and the available resources. While leveraging domain knowledge can offer a lightweight and interpretable solution, learning from data can potentially lead to better performance and more flexibility.

In summary, while our method currently focuses on size-related fine-grained labels, we believe that leveraging LLMs to obtain useful commonsense knowledge and designing appropriate CKIMs will enable our approach to be generalized to a wider range of fine-grained labels for various applications.
\section{Conclusions}\label{sec:conclusions}
Scaling coarse-grained object detection, such as recognizing animal species, to fine-grained object detection, such as recognizing dog breeds, poses a challenge in terms of computational resources. It requires more data, longer training times, and yields bigger models. In this paper, we propose a method that leverages a commonsense knowledge inference module (CKIM) to assist a backbone coarse-grained object detector.

We provide a proof-of-concept of our methodology through a case-study involving size-related fine-grained labels. It is shown that the CKIM is lightweight and only consumes a negligible 2-3KB memory space in our experiments. By integrating it with a coarse-grained object detector, we obtain a model whose size was almost the same as a coarse-grained detector while being able to effectively perform fine-grained object detection.

We discuss the limitations of our current approach and point out that integrating large language models (LLMs) into our framework could be a promising direction for generalizing our approach to more general fine-grained detection tasks.

To summarize, our proposed method offers a promising solution for efficient and effective fine-grained object detection on resource-constrained edge devices by leveraging a lightweight CKIM to infer fine-grained labels from coarse-grained ones. This approach can potentially address the challenges associated with scaling up coarse-grained object detection to fine-grained detection while maintaining a small model size and low processing latency.
\bibliographystyle{splncs04}
\bibliography{kr-sample}
\end{document}
