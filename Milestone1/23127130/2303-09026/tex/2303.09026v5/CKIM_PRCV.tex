% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
%\usepackage[american]{babel}
\usepackage{mathtools} % amsmath with fixes and additions
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{url}
\usepackage{float}
%%
\usepackage{times}
\usepackage{soul}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{booktabs}
\newtheorem{knowledge}{Knowledge}
\begin{document}
%
\title{Commonsense Knowledge Assisted Deep Learning with Application to Size-Related Fine-Grained Object Detection}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
%\author{Anonymous}
\author{Pu Zhang\and
Bin Liu\thanks{Correspondence author}}

\authorrunning{P. Zhang et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
%\institute{Anonymous}
\institute{Research Center for Applied Mathematics and Machine Intelligence,\\
Zhejiang Lab, Hangzhou, 311121 China \\
\email{\{puz,liubin\}@zhejianglab.com}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
This paper addresses fine-grained object detection in scenarios with limited computing resources, such as edge computing. In particular, we focus on a scenario where a single image contains objects of the same category but varying sizes, and we desire an algorithm that can not only recognize the physical class of objects but also detect their size. Deep learning (DL), particularly through the use of deep neural networks (DNNs), has become the primary approach to object detection. However, obtaining accurate fine-grained detection requires a large DNN model and a significant amount of annotated data, presenting a challenge to solve our problem particularly for resource-constrained scenarios. To this end, we propose an approach that utilizes commonsense knowledge to assist a coarse-grained object detector in achieving accurate size-related fine-grained detection results. Specifically, we introduce a commonsense knowledge inference module (CKIM) that processes the coarse-grained labels produced by a benchmark coarse-grained DL detector to generate size-related fine-grained labels. Our CKIM explores both crisp-rule and fuzzy-rule based inference methods, with the latter being employed to handle ambiguity in the target size-related labels. We implement our method based on two modern DL detectors, including Mobilenet-SSD, and YOLOv7-tiny. Experimental results demonstrate that our approach achieves accurate fine-grained detections with a reduced amount of annotated data, and smaller model size. Our code is available at \url{https://github.com/ZJLAB-AMMI/CKIM}.
\keywords{Deep learning \and Commonsense knowledge \and Object detection \and Fine-grained detection \and Edge computing}
\end{abstract}
\section{Introduction}
Object detection is a common computer vision task that aims to locate and classify objects within images or videos. Fine-grained object detection is a vital subset of this field, used to detect objects belonging to fine-grained categories such as specific bird species, dog breeds, and car brands. Deep learning (DL) – specifically, learning with deep neural networks (DNNs) – has become the dominant approach to object detection as it significantly improves detection accuracy. DNN-based object detectors such as YOLO \cite{redmon2018yolov3} and Faster R-CNN \cite{ren2015faster} are widely used for this task.

However, the benefits of these advancements come at a cost: substantial computational and storage resources, along with a vast quantity of annotated data, are required. This challenge is further compounded in the case of fine-grained object detection, which demands even larger models, more annotated data, and greater computational budgets to train and store the model \cite{zaidi2022survey}. Consequently, fine-grained object detectors are often deployed on cloud servers that have sufficient computing resources and labeled data.

This paper addresses a more challenging problem, namely fine-grained object detection in edge computing nodes, such as smartphones and driving assistance systems, other than remote cloud servers. Edge computing has made significant progress in recent years within the context of the Internet of Things. The amount of high-dimensional data produced by edge devices, particularly images and videos, has been increasing rapidly. Fine-grained object detection is required for various applications that involve real-time image or video processing at the edge, such as autonomous vehicles \cite{zhang2021fairmot} and tracking multiple objects \cite{li2019gs3d}. However, transmitting this data to a remote cloud server for labeling and processing is not feasible due to the unacceptable latency in response, excess bandwidth consumption, and privacy concerns \cite{chen2019deep}. Thus, training and deploying object detection models on edge devices has emerged as a significant trend.

In this paper, we focus on a scenario where a single image contains objects of the same category but varying sizes, and we desire an algorithm that can not only recognize the physical class of objects but also recognize their size. This problem appears in many real-life applications. For example, imagine we ask a robot to fetch a cup for us. In the robot's field of view, there are cups of various sizes, but we want the robot to grab the cup of an expected size accurately. Therefore, the robot not only needs to recognize the presence of cups in the scene but also detect the size of each cup to achieve the desired task. In autonomous driving, there are also times when we want the algorithm to automatically detect adults and children since the driving strategies for adults and children are likely to be different.

Motivated by the above desiderata, we develop a lightweight DNN-based size-related fine-grained object detector in this paper. The basic idea is to utilize commonsense knowledge to assist a coarse-grained object detector in obtaining accurate fine-grained detections. Specifically, we introduce a commonsense knowledge inference module (CKIM) to a backbone coarse-grained object detector. This CKIM can infer fine-grained object labels from the output of the backbone coarse-grained detector. Comparing Fig.\ref{fig:general_model} and Fig.\ref{fig:CKIM} highlights the differences between our method and typical fine-grained detectors. Our approach is complementary to existing network compression methods, such as network pruning \cite{hassibi1993optimal}, knowledge distillation \cite{hinton2015distilling}, and depthwise convolution \cite{howard2017mobilenets,8578814}, in principle, so it can be combined with them.
\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{figures/FG_detector.eps}
\caption{An example show of the working mechanism of a typical fine-grained object detector. Typical fine-grained object detectors require a vast amount of images with fine-grained labels and a large enough model to achieve accurate detection results.}
\label{fig:general_model}
\end{figure}
\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{figures/CKIM.eps}
\caption{An example show of the working mechanism of our proposed method. Our method utilizes only a coarse-grained object detector that requires much less labeled data for model training and significantly less memory for model storage than the typical fine-grained object detectors depicted in Fig.\ref{fig:general_model}. We introduce a commonsense knowledge inference module that maps coarse-grained outputs of the coarse-grained object detector to fine-grained labels. The commonsense knowledge can be acquired from either a human expert or a large language model.
}
\label{fig:CKIM}
\end{figure}

Fig.\ref{fig:general_model} illustrates that a typical fine-grained object detector requires an extensive amount of labeled images with `adult' or `child' labels to train a large enough model for detecting adults and children. However, according to commonsense knowledge, `adult' and `child' are subcategories of the same coarse-grained class, namely `person'. Typically, objects are recognized as `adults' if their size is large and as `children' if it is small. Our proposed method, illustrated in Fig. \ref{fig:CKIM}, introduces a CKIM that leverages this type of commonsense knowledge. The DNN detector generates only coarse-grained outputs, namely the detection of `person'-type objects and their positions, while the CKIM infers fine-grained object labels from these coarse-grained outputs.

\textbf{Our contributions}:
\begin{itemize}
    \item We propose a deep learning method for size-related fine-grained object detection that leverages commonsense knowledge and is suited for resource-constrained scenarios. Our approach employs a coarse-grained object detector and a commonsense knowledge inference module (CKIM) to map the coarse-grained outputs of the detector into fine-grained predictions.
     \item We present two types of CKIM for use in our method: one using crisp-rule-based inference and the other using fuzzy-rule-based inference. The fuzzy-rule-based inference is employed to address any semantic ambiguity related to the fine-grained labels.
     \item We integrate the CKIM into modern object detectors, including MobilenetV3-SSD \cite{howard2019searching}, and Yolov7-tiny \cite{wang2022yolov7}, and evaluate our method's performance through experiments. The results demonstrate that our approach achieves accurate size-related fine-grained detections with a reduced amount of annotated data, and smaller model size.
\end{itemize}
%The structure of this paper is as follows. Section \ref{sec:related} provides a brief overview of related work. Section \ref{sec:CKIM} describes our proposed CKIM-based detector, including the crisp-rule and fuzzy-rule based implementations of the CKIM. Section \ref{sec:experiment} presents the experimental setup and results. Section \ref{sec:discussion} discusses limitations of our approach. Finally, Section \ref{sec:conclusions} concludes the paper.
\section{Related Works}\label{sec:related}
In this section, we provide a brief overview of related work on DNN-based visual object detection and rule-based inference.
\subsection{Modern DNNs for Visual Object Detection}
DNN architectures for object detection can be divided into two types. The first type includes networks with separate modules for candidate region generation and classification, such as Faster R-CNN \cite{ren2015faster}, which are known as two-stage detectors. The second type is single-stage detectors, which directly produce object categories and their bounding boxes in a single step and use pre-defined differently sized boxes to locate objects. Single-stage detectors are suitable for real-time, resource-constrained scenarios such as edge computing because they have lightweight designs and require less time to make predictions. As such, we employ them as benchmark object detectors in this paper.
\subsubsection{You Only Look Once (YOLO)} is one of the most widely used single-stage object detectors \cite{redmon2016you}. In YOLO, the input image is divided into a grid with $S \times S$ cells, and each cell predicts bounding boxes and their corresponding class probabilities. Over time, newer versions of YOLO have been proposed to improve inference speed and reduce model size. For instance, YOLOv4 \cite{bochkovskiy2020yolov4} achieved state-of-the-art (SOTA) performance for real-time object detection, while YOLOv7 is a more advanced SOTA real-time object detector. Both YOLOv4 and YOLOv7 have lightweight versions, namely YOLOv4-tiny and YOLOv7-tiny, which employ smaller model architectures that are better suited for edge computing scenarios \cite{wang2021scaled,wang2022yolov7}. In our experiments, we use YOLOv7-tiny as a baseline method.
\subsubsection{MobileNet-SSD} is an efficient DL method for object detection in mobile computing scenarios. It employs a Single Shot MultiBox Detector (SSD) \cite{liu2016ssd} as the decoder and MobileNet \cite{howard2017mobilenets} as the feature extractor. SSD was the first single-stage detector that performed comparably to two-stage detectors. MobileNet was specifically designed for vision applications on mobile devices, with traditional convolution layers replaced by depthwise separable and pointwise convolution layers to reduce model size \cite{sandler2018mobilenetv2}. An advanced version of MobileNet, MobileNetV3, uses a search algorithm to optimize the network architecture \cite{howard2019searching}. In our experiments, we use MobileNetV3-SSD as a baseline method.
\subsection{Rule-based Inference}
Rule-based inference is a conventional methodology for implementing knowledge reasoning, where knowledge is represented by a collection of ``IF-THEN" rules. The reasoning mechanism is that if a novel instance matches the antecedent of any rule in the rule base, the corresponding rule consequent will be the result \cite{Grosan2011}. In the context of this work, the association between fine-grained and coarse-grained categories may involve semantic imprecision and ambiguity related to size, distance, age, and color of objects. Therefore, using only crisp rules is not enough. We employ fuzzy logic and fuzzy set theory \cite{2010Fuzzy,ZADEH1965338}. Fuzzy rule-based systems (FRBS) allow such terms to be precisely described by fuzzy sets, enabling the inference process to resemble human reasoning.

Fuzzy sets can be generally defined as follows:
\begin{equation}
M = {(a,\mu_M(a)|a\in A,\mu_M(a) \in [0,1])}
\end{equation}
where $a$ denotes an instance, $A$ a collection of instances, and $\mu_A(x)$ the membership function that describes the partial degrees to which an instance $a$ is judged to belong to the fuzzy set $M$.

Mamdani models \cite{Scherer2012} are widely used in FRBS. They use fuzzy sets as rule antecedents and consequents, as shown in Fig.\ref{fig:mamdani} in the Appendix, and perform inference by integrating the conclusions of individual rules that overlap with the input. The weight of a certain rule is determined by the match degree between the input instance and the rule antecedent.
\section{CKIM Assisted Object Detection}\label{sec:CKIM}
In this section, we present our approach to lightweight size-related fine-grained object detection based on CKIM in detail. We explain how to specify CKIM using commonsense knowledge and how to use it to assist a coarse-grained deep detector in doing fine-grained object detection.

Fig.\ref{fig:CKIM} shows an example of the working mechanism of our method, which consists of two parts: a coarse-grained object detector and CKIM. The former part outputs coarse-grained labels and a bounding box for each object. The bounding box can be represented by the equation:
\begin{equation}
box(C,X,Y,W,H)
\end{equation}
where $C$ denotes the coarse-grained label, $(X,Y)$ denotes the coordinates of the centre of the bounding box, and $W$ and $H$ denote the width and height of the bounding box, respectively. We focus on size-related fine-grained labels in this work. The real size of an object is naturally closely related to the size of the object's bounding box, which can be produced by a qualified object detector. In addition, according to commonsense knowledge as follows
\begin{knowledge}
\emph{An object appears to decrease in size as it moves farther away and appears to enlarge as it moves closer to the observer},
\end{knowledge}
\noindent
the real size of an object is related to its distance from the camera. For an object in an image, we can estimate its real distance to the camera based on its distance from the center of its bounding box to the bottom of the image. This is because:
\begin{knowledge}
\emph{the object's distance to the camera (DtoC) is strongly connected to the distance from the center of its bounding box to the bottom of the image (CtoB)}.
\end{knowledge}
\noindent The connection between CtoB to DtoC is illustrated in Fig.\ref{fig:distance}.
\begin{figure}[ht]
\centering
\includegraphics[width=0.6\textwidth]{figures/distance.eps}
\caption{An example image that illustrates the connection between an object's distance to the camera (DtoC) and the distance from the center of the object to the bottom of the image (CtoB). As shown in the image, CtoB of object A, denoted by CtoB$_A$, is larger than CtoB$_B$. This information can be used to infer that DtoC of object A, denoted by DtoC$_A$, is larger than DtoC$_B$, which is clearly true as shown in the picture.}
\label{fig:distance}
\end{figure}

The aforementioned commonsense knowledge provides us with an opportunity to reason size-related fine-grained labels from the outputs of a coarse-grained object detector, such as the coarse-grained labels and bounding boxes of the objects.
The purpose of CKIM is to infer fine-grained labels based on the size of the objects' bounding boxes (BoxS) and their distances to the camera (DtoC). To achieve this, we normalize values of $X$, $Y$, $W$, $H$ according to the width and height of the image, define the size of the bounding box (BoxS) as its area, and define DtoC as 1 minus Y. Then, we calculate BoxS and DtoC as follows:
\begin{equation}
\begin{aligned}
BoxS &= W \times H \\
DtoC &= 1-Y
\end{aligned}
\end{equation}
We develop two types of CKIM, corresponding to crisp-rule and fuzzy-rule based inference.
\subsection{Crisp-rule based CKIM}
In crisp rule-based systems, Boolean logic is followed, and an object can only belong to one class or not belong to it. We model the relationship between the real size of an object and its attributes given by a coarse-grained object detector using a logistic regression function \cite{lavalley2008logistic}.

To simplify the presentation, we consider size-related object labels: `large', `middle', and `small'. We use the one-vs-rest method to perform crisp rule inference. The decision function of the crisp rule is defined as follows:
\begin{equation}
\begin{aligned}
f_{ml}(BoxS, DtoC) &= \frac{1}{1+e^{w_{ml}^{T}x}}, x \triangleq (BoxS, DtoC)\\
f_{sm}(BoxS, DtoC) &= \frac{1}{1+e^{w_{sm}^{T}x}}, \\
\end{aligned}
\end{equation}
where $f_{sm}$ denotes the decision function for small sized and middle sized objects, and $f_{ml}$ the decision function for middle sized and large sized objects, $w^T$ is a set of parameters optimized by minimizing a cost function in the same way as in typical logistic regression.

Accordingly, the CKIM is defined as follows:
\\

If $f_{ml}(BoxS, DtoC) > 0$,

\hspace{2em} then the object's fine-grained label is `large';

If $f_{ml}(BoxS, DtoC) < 0$ and $f_{sm}(BoxS, DtoC) > 0$,

\hspace{2em} then the fine-grained label is `middle';

If $f_{sm}(BoxS, DtoC) < 0$,

\hspace{2em} then the fine-grained label is `small'.
\\
\subsection{Fuzzy-rule based CKIM}
In real-life cases, there may be continuous-valued attributes (e.g., size, distance, age) that are difficult to classify with discrete semantic descriptions (e.g., large or small, near or far, young or old), resulting in semantic vagueness or uncertainty. Fuzzy logic provides a multi-value logic, in which such vague semantics can be strictly formulated and precisely studied.

To adapt our method to more general scenarios, we propose a fuzzy-rule based approach for implementing CKIM. Fuzzy rule-based systems allow an object to match different categories with different memberships. We adopt the aforementioned Mamdani model for this task, which uses fuzzy sets as rule antecedents and consequents. Two main categories of membership functions are typically used in constructing fuzzy sets \cite{2021Approximate}:

(1) Polygonal functions, such as triangular shaped and trapezoidal shaped functions;

(2) Nonlinear functions, including Gaussian shaped and generalised bell shaped functions.

Here, we adopt the Gaussian-shaped membership function \cite{kreinovich1992gaussian}. As before, we consider size-related object labels, namely `large', `middle', and `small'.

In our fuzzy rule inference module, the antecedents of rules are defined as follows:
\begin{equation}
\begin{aligned}
M_L(x) &= \mathcal{N}(x|\mu_L,\Sigma_L)\\
&=\frac{1}{2\pi|\Sigma_L|^{1/2}} \exp \left\{- \frac{1}{2} \left(x-\mu_L\right)^{'} \Sigma_L^{-1} \left(x-\mu_L \right)  \right\}\\
M_M(x) &= \mathcal{N}(x|\mu_M,\Sigma_M)\\
&=\frac{1}{2\pi |\Sigma_M|^{1/2}} \exp \left\{- \frac{1}{2} \left(x-\mu_M\right)^{'} \Sigma_M^{-1} \left(x-\mu_M \right)  \right\}\\
M_S(x) &= \mathcal{N}(x|\mu_S,\Sigma_S)\\
&=\frac{1}{2\pi |\Sigma_S|^{1/2}} \exp \left\{- \frac{1}{2} \left(x-\mu_S\right)^{'} \Sigma_S^{-1} \left(x-\mu_S \right)  \right\}\\
\end{aligned}
\end{equation}
where $x\triangleq(BoxS, DtoC)$ the same as before; $S$, $M$ and $L$ in the subscripts denote `small', `middle', and `large', respectively; $\mu$ and $\Sigma$ denote the mean and covariance matrix of the data distribution.

The fuzzy-rule based CKIM is designed as follows:\\
If $x$ matches $M_L$, then object's label is `large' with degree $M_L(x)$;\\
If $x$ matches $M_M$, then object's label is `middle' with degree $M_M(x)$;\\
If $x$ matches $M_S$, then object's label is `small' with degree $M_S(x)$.

A crisp output is then calculated by a defuzzification approach to integrate membership degrees between the object and all rules.
Accordingly, the fine-grained result is determined by the $\arg\max$ function defined as follows:

\begin{equation}
\begin{aligned}
Label &=\underset{i\in\{S,M,L\}}{{\arg\max}\, M_i(x)}
\end{aligned}
\end{equation}

Note that since these two types of CKIM have few parameters to be optimized, the amount of data required to train them is almost negligible compared to those required to train a DNN model.
\section{Experiments}\label{sec:experiment}
We experimentally evaluated the performance of our proposed CKIM-assisted DNN detector. We compared our method against the SOTA methods, including YOLOv7-tiny and MobileNetv3-SSD, both being developed for resource-constrained scenarios. We integrated CKIM with YOLOv7-tiny and MobileNetv3-SSD and assessed whether it resulted in improved performance.
\subsection{Experimental Setup}
\subsubsection{The CLEVR Dataset} is a benchmark dataset for Vision Question Answering (VQA) models \cite{johnson2017clevr}. It consists of pictures containing objects and questions related to them. The objects' attributes include size (big, small), color (brown, blue, cyan, gray, green, purple, red, yellow), material (metal, rubber), and shape (cube, cylinder, sphere). According to these attributes, objects are divided into 96 fine-grained categories. We term the original dataset as CLEVR-96.
In our experiment, we removed the size attribute to train the coarse-grained object detector that considers $96/2=48$ coarse-grained labels. Then, we predicted fine-grained size labels with our CKIM. The other involved fine-grained object detectors, namely YOLOv7-tiny and MobileNetv3-SSD, were all trained with the original datasets that have 96 labels.
To simulate complex environments in the real world, we constructed a new dataset by introducing a collection of objects of middle size. It has 144 fine-grained classes. We term it as CLEVR-144 in what follows. See the Appendix for example pictures in these datasets.
Both CLEVR-96 and CLEVR-144 contain 16,000 images as the training set, 2,000 images as the validation set, and the other 2,000 images as the test set.
\subsection{Performance Metrics}
In our evaluation of each method for resource-constrained edge computing scenarios, we considered three different perspectives: detection accuracy, model size, and processing latency: \\
(1) Detection Accuracy is measured by the mean Average Precision while IoU=0.5 (mAP@0.5), which is a commonly used metric for evaluating object detectors. It calculates the mean of the average precision over all classifications for every bounding box with an IoU greater than 0.5. Larger mAP@0.5 means higher accuracy.\\
(2) Model Size is measured by the memory space that the model consumes. This is important in resource-constrained scenarios where memory is limited.\\
(3) Latency is defined as the average time a method takes to process one image. In our experiments, the time unit was set as millisecond (ms). This is also an important factor to consider in resource-constrained scenarios where real-time processing is required.
\begin{table}[hb]
\begin{center}
\begin{tabular}{cccc}
\toprule
%& \multicolumn{3}{c}{CLEVR-96} \\
%\cline{2-4}
 &Detection Accuracy $\uparrow$ & Model Size $\downarrow$ & Latency $\downarrow$\\
\hline
MobileNetv3\_SSD & 0.968& 87.61MB& 82\\
MobileNetv3\_SSD-L+crisp &\textbf{0.978} &\textbf{49.63MB+2KB} & \textbf{60}\\
\hline
YOLOv7-tiny &0.972 &23.31MB& 70 \\%\\
YOLOv7-tiny-L+crisp &\textbf{0.983} &\textbf{22.89MB+2KB} & \textbf{62}\\%\\
\bottomrule
\end{tabular}
\end{center}
\caption{Fine-grained object detection performance on the CLEVR-96 dataset using all 16,000 images for model training. The best performances for each model type are marked in \textbf{bold}. In the table, X-L denotes a coarse-grained version of a fine-grained model X, where X can be MobileNetv3\_SSD or YOLOv7-tiny. X-L+crisp denotes our method that uses the model X-L assisted with crisp rule based CKIM for fine-grained object detection. As is shown, our method outperforms both baseline methods in terms of Detection Accuracy, Model Size and Latency. Since this dataset only has two fine-grained labels ('small' and 'large'), there is no label ambiguity issue involved, and therefore, only crisp-rule based inference is considered for our method.}
\label{tab:full1}
\end{table}

\begin{table}[ht]
\begin{center}
\begin{tabular}{cccc}
\toprule
%& \multicolumn{3}{c}{CLEVR-144} \\
%\cline{2-4}
 &Detection Accuracy $\uparrow$ & Model Size $\downarrow$  & Latency $\downarrow$\\
\hline

MobileNetv3\_SSD & 0.970& 125.59MB& 84\\%11.87\\

MobileNetv3\_SSD-L+crisp & 0.968 &\textbf{49.63MB+2KB} & \textbf{61}\\%16.41\\

MobileNetv3\_SSD-L+fuzzy &\textbf{0.978} &\textbf{49.63MB+2KB} & 62 \\%15.98\\
\hline

YOLOv7-tiny &0.965& 23.73MB & 75\\%13.22\\

YOLOv7-tiny-L+crisp & 0.971 &\textbf{22.89MB+2KB} & 64\\%15.66\\

YOLOv7-tiny-L+fuzzy &\textbf{0.980} &\textbf{22.89MB+2KB} & \textbf{62}\\

\bottomrule
\end{tabular}
\end{center}
\caption{Fine-grained object detection performance on the CLEVR-144 dataset using all 16,000 images for model training. The best performances for each mode type are marked in \textbf{bold}. X-L+fuzzy denotes our method that uses the model X-L assisted with fuzzy rule based CKIM for fine-grained object detection, where X can be MobileNetv3\_SSD or YOLOv7-tiny. All the other terms are defined in the same way as in Table \ref{tab:full1}.}
\label{tab:full2}
\end{table}
\subsection{Experimental Results}
We trained all the involved models using all 16,000 images in the training set. The performance of these models on the CLEVR-96 and CLEVR-144 datasets are shown in Tables \ref{tab:full1} and \ref{tab:full2}, respectively. It is shown that our proposed method significantly decreases the model size and the latency by using a lightweight coarse-grained object detector and then performing fine-grained detection with crisp-rule or fuzzy-rule based commonsense knowledge reasoning.
We also find that on the CLEVR-144 dataset, which involves three fine-grained labels (`small', `middle', and `large'), the fuzzy-rule based method is preferable to the crisp-rule based method. This demonstrates the ability of the fuzzy method to deal with semantic ambiguity in the fine-grained labels.

As part of our experiment, we also evaluated a baseline fine-grained YOLOv4 model. On the CLEVR-96 dataset, this model achieves a high detection accuracy of 0.998, but has a much larger model size of 246.35MB and latency of 176ms than the methods listed in Table \ref{tab:full1}. On the CLEVR-144 dataset, it achieves detection accuracy of 0.998 again, while it also has a much larger model size of 247.34MB and latency of 183ms than the methods listed in Table \ref{tab:full2}.

Despite achieving high accuracy, the baseline YOLOv4 model may not be practical for resource-constrained edge devices due to its large model size and high processing latency. In contrast, our proposed method using commonsense knowledge reasoning achieves higher detection accuracy while maintaining a smaller model size and lower processing latency than its counterpart baseline methods.
These results demonstrate the potential of our approach for real-world applications where edge devices have limited resources, since it can achieve high accuracy while keeping model size and processing latency at an acceptable level.

In order to simulate cases where the edge device does not have enough memory space to store all training data, we conducted experiments using 5,000 randomly selected images from the training set for model training. Results show that, for the case with less training data,  the benefits given by our proposed method become more remarkable. Experiment results also demonstrate that our method converges much faster than its fine-grained counterpart. See details in the Appendix section.
\section{Discussions}\label{sec:discussion}
While our experimental results presented above are encouraging, it is important to note that our case study focused only on size-related fine-grained labels. A natural question arises as to how to generalize our approach to work for more general fine-grained labels.
%This question can be broken down into two sub-questions: How and where to obtain useful commonsense knowledge, and how to design an appropriate CKIM that effectively fuses information given by a coarse-grained object detector and commonsense knowledge.

We argue that a promising approach to answering this question is to leverage modern large language models (LLMs), which by itself can be viewed as a large knowledge inference engine. If a flexible interface between the algorithm agent and the LLM can be implemented, then probably LLMs can be used to develop more general CKIMs.

To summarize, although our method currently focuses on size-related fine-grained labels, we emphasize that utilizing LLMs to create more general CKIMs can help generalize our work to a broader range of applications.
\section{Conclusions}\label{sec:conclusions}
Scaling coarse-grained object detection, such as recognizing animal species, to fine-grained object detection, such as recognizing dog breeds, poses a challenge in terms of computational resources. It requires more data, longer training times, and yields bigger models.

In this paper, we focused on a specific scenario of fine-grained object detection, where a single image contains objects of the same category but varying sizes. For this scenario, we developed an algorithm that can not only recognize the physical class of objects but also detect their size.
Our idea is to develop a commonsense knowledge inference module (CKIM) to assist a backbone coarse-grained object detector. It is shown that our CKIM is lightweight and only consumes a negligible 2-3KB memory space in our experiments. By integrating it with a coarse-grained object detector, we obtain a model whose size was almost the same as a coarse-grained detector while being able to effectively perform fine-grained object detection. Our proposed method offers a promising solution for efficient and effective fine-grained object detection on resource-constrained edge devices. This approach can potentially address the challenges associated with scaling up coarse-grained object detection to fine-grained detection while maintaining a small model size and low processing latency.

We also discussed the limitations of our current approach and pointed out that integrating LLMs into our framework could be a promising direction for generalizing our approach to more general tasks.
\bibliographystyle{splncs04}
\bibliography{kr-sample}
\newpage
\section{Appendix}\label{sec:appendix}
\subsection[A.1]{An example show of the inference process using a Mamdani model}
Mamdani models \cite{Scherer2012} are widely used in FRBS. They use fuzzy sets as rule antecedents and consequents, as shown in Fig.\ref{fig:mamdani}, and perform inference by integrating the conclusions of individual rules that overlap with the input. The weight of a certain rule is determined by the match degree between the input instance and the rule antecedent.
\begin{figure}[ht]
\centering
\includegraphics[width=0.45\textwidth]{figures/mamdani.eps}
\caption{Illustrative example of the inference process with a Mamdani model. $R_{i},R_{j}$ represent two fuzzy rules; $(A_i, B_i), (A_j, B_j) $ are the corresponding antecedents for the rules $R_{i},R_{j}$ respectively; $C_i, C_j$ are their consequent for fuzzy rule $R_{i},R_{j}$ respectively; $(x,y)$ is the input instance; $\alpha_i, \alpha_j$ stand for matching degrees between the input and the two rules.}
\label{fig:mamdani}
\end{figure}
\subsection[A.2]{Example pictures of our datasets}
The left panel of Fig.\ref{fig:CLEVR} shows an example of the CLEVR-96 dataset, where the object size attribute is specified as either `large' or `small'. The right panel shows an example of the CLEVR-144 dataset, where the object size attribute can be `large', `middle', or `small'.
\begin{figure}[ht]
\centering
\includegraphics[width=0.43\textwidth]{figures/CLEVR_Ori.eps}\quad \includegraphics[width=0.43\textwidth]{figures/CLEVR_new.eps}
\caption{Left: an example image in the CLEVR-96 dataset, where the object size attribute is specified as either `large' or `small'; Right: an example image in the CLEVR-144 dataset, where the object size attribute can be `large', `middle', or `small'}
\label{fig:CLEVR}
\end{figure}
\subsection[A.3]{On experiments with 5,000 randomly selected images being used for model training}
In order to simulate cases where the edge device does not have enough memory space to store all training data, we conducted experiments using 5,000 randomly selected images from the training set for model training. The experimental results on the CLEVR-96 and CLEVR-144 datasets are presented in Tables \ref{tab:50001} and \ref{tab:50002}, respectively.

As shown in the tables, for the case with less training data, the benefits given by our proposed method become more remarkable. Our method outperforms all baseline methods according to all performance metrics. In addition, we once again observed the benefits of fuzzy inference on the CLEVR-144 dataset. The fuzzy-based method significantly outperforms the crisp-rule based method in terms of detection accuracy.
\begin{table}[ht]
\centering
\begin{tabular}{cccc}
\toprule

 &Detection Accuracy $\uparrow$ & Model Size $\downarrow$  & Latency $\downarrow$ \\
\hline

MobileNetv3\_SSD &0.902& 87.61MB & 82\\

MobileNetv3\_SSD-L+crisp &\textbf{0.912}& \textbf{49.63MB+2KB}& \textbf{60}\\

\hline
YOLOv7-tiny &0.928& 23.31MB& 71 \\
YOLOv7-tiny-L+crisp &\textbf{0.984}& \textbf{22.89MB+2KB} & \textbf{61} \\

\bottomrule
\end{tabular}
\caption{Fine-grained object detection performance on the CLEVR-96 dataset using 5,000 randomly selected images for model training. The best performances for each mode type are marked in \textbf{bold}. All terms are defined in the same way as in Table \ref{tab:full1}. As part of our experiment, we also evaluated a baseline fine-grained YOLOv4 model. This model achieves a high detection accuracy of 0.972, but has a much larger model size of 246.35MB and latency of 178ms than the methods listed here.}
\label{tab:50001}
\end{table}
\begin{table}[ht]
\centering
\begin{tabular}{cccc}
\toprule

 & Detection Accuracy $\uparrow$  & Model Size $\downarrow$  & Latency $\downarrow$\\
\hline
MobileNetv3\_SSD & 0.857& 125.59MB & 83\\

MobileNetv3\_SSD-L+crisp & 0.862& \textbf{49.63MB}+2KB & 60\\

MobileNetv3\_SSD-L+fuzzy  &\textbf{0.877} &\textbf{49.63MB}+2KB & \textbf{59}\\

\hline
YOLOv7-tiny &0.865& 23.73MB& 72\\

YOLOv7-tiny-L+crisp &0.892 &\textbf{22.89MB+2KB} & 65\\

YOLOv7-tiny-L+fuzzy & \textbf{0.914} &\textbf{22.89MB+2KB}& \textbf{62}\\

\bottomrule
\end{tabular}
\caption{Fine-grained object detection performance on the CLEVR-144 dataset using 5,000 randomly selected images for model training. The best performances for each mode type are marked in \textbf{bold}. All terms are defined in the same way as in Table \ref{tab:full2}. As part of our experiment, we also evaluated a baseline fine-grained YOLOv4 model. This model achieves a high detection accuracy of 0.957, but has a much larger model size of 247.34MB and latency of 175ms than the methods listed here.}
\label{tab:50002}
\end{table}
\subsection[A.4]{Training process comparison for fine-grained and coarse-grained object detectors}
In order to investigate the difference in the training process between fine-grained and coarse-grained object detectors, we conducted an experiment using the YOLOv7-tiny model. Specifically, we compared the training process of a YOLOv7-tiny based fine-grained detector and a YOLOv7-tiny based coarse-grained detector. The results, shown in Fig.\ref{fig:train_5000}, demonstrate that the coarse-grained detector converges much faster than its fine-grained counterpart on both the CLEVR-96 and CLEVR-144 datasets.

Since our proposed fine-grained object detector consists of a coarse-grained detector and a lightweight CKIM, the convergence speed of our method is almost the same as the coarse-grained detector it employs. This indicates that the convergence speed of our proposed fine-grained detector is much faster than its counterpart fine-grained detector.

Overall, these results demonstrate that our proposed method achieves efficient and effective fine-grained object detection by leveraging a lightweight commonsense knowledge inference module with a coarse-grained object detector, achieving high accuracy while maintaining fast convergence times.
\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{figures/train_5000.eps}
\caption{Training process of a pair of fine-grained and coarse-grained object detectors}
\label{fig:train_5000}
\end{figure}
\end{document}