
@article{carvajal_bicycle_2020,
	title = {Bicycle safety in {Bogotá}: {A} seven-year analysis of bicyclists’ collisions and fatalities},
	volume = {144},
	issn = {0001-4575},
	shorttitle = {Bicycle safety in {Bogotá}},
	url = {https://www.sciencedirect.com/science/article/pii/S0001457519313892},
	doi = {10.1016/j.aap.2020.105596},
	abstract = {Road safety research in low- and middle-income countries is limited, even though ninety percent of global road traffic fatalities are concentrated in these locations. In Colombia, road traffic injuries are the second leading source of mortality by external causes and constitute a significant public health concern in the city of Bogotá. Bogotá is among the top 10 most bike-friendly cities in the world. However, bicyclists are one of the most vulnerable road-users in the city. Therefore, assessing the pattern of mortality and understanding the variables affecting the outcome of bicyclists’ collisions in Bogotá is crucial to guide policies aimed at improving safety conditions. This study aims to determine the spatiotemporal trends in fatal and nonfatal collision rates and to identify the individual and contextual factors associated with fatal outcomes. We use confidence intervals, geo-statistics, and generalized additive mixed models (GAMM) corrected for spatial correlation. The collisions’ records were taken from Bogotá’s Secretariat of Mobility, complemented with records provided by non-governmental organizations (NGO). Our findings indicate that from 2011 to 2017, the fatal bicycling collision rates per bicyclists’ population have remained constant for females while decreasing 53 \% for males. Additionally, we identified high-risk areas located in the west, southwest, and southeast of the city, where the rate of occurrence of fatal events is higher than what occurs in other parts of the city. Finally, our results show associated risk factors that differ by sex. Overall, we find that fatal collisions are positively associated with factors including collisions with large vehicles, the absence of dedicated infrastructure, steep terrain, and nighttime occurrence. Our findings support policy-making and planning efforts to monitor, prioritize, and implement targeted interventions aimed at improving bicycling safety conditions while accounting for gender differences.},
	language = {en},
	urldate = {2022-08-17},
	journal = {Accident Analysis \& Prevention},
	author = {Carvajal, Germán A. and Sarmiento, Olga L. and Medaglia, Andrés L. and Cabrales, Sergio and Rodríguez, Daniel A. and Quistberg, D. Alex and López, Segundo},
	month = sep,
	year = {2020},
	keywords = {Bicycling mortality, Built-environment, Latin America, Vision zero},
	pages = {105596},
	file = {Full Text:C\:\\Users\\charl\\Zotero\\storage\\IHCJ2I4W\\Carvajal et al. - 2020 - Bicycle safety in Bogotá A seven-year analysis of.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\charl\\Zotero\\storage\\3ZWJZNFB\\S0001457519313892.html:text/html},
}

@article{jansen_caught_2022,
	title = {Caught in the blind spot of a truck: {A} choice model on driver glance behavior towards cyclists at intersections},
	volume = {174},
	issn = {0001-4575},
	shorttitle = {Caught in the blind spot of a truck},
	url = {https://www.sciencedirect.com/science/article/pii/S0001457522001956},
	doi = {10.1016/j.aap.2022.106759},
	abstract = {Vulnerable road users (VRUs) constitute an increasing proportion of the annual road fatalities across Europe. One of the crash types involved in these fatalities are blind spot crashes between trucks and bicyclists. Despite the presence of mandatory blind spot mirrors, truck drivers are often reported to have overlooked the presence of a bicyclist. This raises the question if and when truck drivers check their blind spot mirrors for the presence of bicyclists, and which factors contribute to such glance behavior. The current study presents the results of an analysis of naturalistic glance behavior by 39 truck drivers in 1,903 right-turning maneuvers at urban intersections, where in each maneuver there was a chance of crossing the path of a bicyclist. The descriptive analysis revealed that most often truck drivers did not cast a glance upon their blind spot mirrors as recommended by the examination guidelines. Furthermore, a choice model was developed with the main factors that have an impact on glance behavior. Drivers were more likely to glance with a priority regulation that allowed conflicts, with lower speed limits, with zebra crossings, without cyclist facilities, without a lead vehicle making the same maneuver, in presence of VRUs, without adverse sight conditions, in lower age groups, without certain non-driving related activities, when driving a truck with more direct vision on VRUs, and without a camera providing a view on the blind spot, and with less time between a standstill and starting the maneuver. Three factors did not significantly improve the choice model and were therefore left out, despite showing significant effects in bivariate tests: intersection layout (e.g., three vs. four legs), presence of advanced stopping lanes, and visual obstruction. Implications of the choice model are discussed for driver education (in terms of timely glances, reducing inattention, and hazard anticipation) and vehicle design (in terms of direct vision).},
	language = {en},
	urldate = {2022-08-17},
	journal = {Accident Analysis \& Prevention},
	author = {Jansen, Reinier J. and Varotto, Silvia F.},
	month = sep,
	year = {2022},
	keywords = {Blind spot, Discrete choice model, Glance behavior, Naturalistic driving, Right turn maneuver},
	pages = {106759},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\charl\\Zotero\\storage\\FX4GSJSP\\Jansen and Varotto - 2022 - Caught in the blind spot of a truck A choice mode.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\charl\\Zotero\\storage\\KAJDPNGT\\S0001457522001956.html:text/html},
}

@article{garcia-venegas_safety_2021,
	title = {On the safety of vulnerable road users by cyclist orientation detection using {Deep} {Learning}},
	volume = {32},
	issn = {0932-8092, 1432-1769},
	url = {http://arxiv.org/abs/2004.11909},
	doi = {10.1007/s00138-021-01231-4},
	abstract = {In this work, orientation detection using Deep Learning is acknowledged for a particularly vulnerable class of road users,the cyclists. Knowing the cyclists' orientation is of great relevance since it provides a good notion about their future trajectory, which is crucial to avoid accidents in the context of intelligent transportation systems. Using Transfer Learning with pre-trained models and TensorFlow, we present a performance comparison between the main algorithms reported in the literature for object detection,such as SSD, Faster R-CNN and R-FCN along with MobilenetV2, InceptionV2, ResNet50, ResNet101 feature extractors. Moreover, we propose multi-class detection with eight different classes according to orientations. To do so, we introduce a new dataset called "Detect-Bike", containing 20,229 cyclist instances over 11,103 images, which has been labeled based on cyclist's orientation. Then, the same Deep Learning methods used for detection are trained to determine the target's heading. Our experimental results and vast evaluation showed satisfactory performance of all of the studied methods for the cyclists and their orientation detection, especially using Faster R-CNN with ResNet50 proved to be precise but significantly slower. Meanwhile, SSD using InceptionV2 provided good trade-off between precision and execution time, and is to be preferred for real-time embedded applications.},
	number = {5},
	urldate = {2022-08-17},
	journal = {Machine Vision and Applications},
	author = {Garcia-Venegas, Marichelo and Mercado-Ravell, Diego A. and Carballo-Monsivais, Carlos A.},
	month = sep,
	year = {2021},
	note = {arXiv:2004.11909 [cs, eess, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
	pages = {109},
}

@misc{noauthor_bicycle_2022,
	title = {Bicycle {Safety} {\textbar} {Motor} {Vehicle} {Safety} {\textbar} {CDC} {Injury} {Center}},
	url = {https://www.cdc.gov/transportationsafety/bicycle/index.html},
	language = {en-us},
	urldate = {2022-08-19},
	author = {, Centers for Disease Control {and} Prevention},
	month = may,
	year = {2022},
	file = {Snapshot:C\:\\Users\\charl\\Zotero\\storage\\GZYQ3TL9\\index.html:text/html},
}

@article{wang_exploring_2022,
	title = {Exploring the {Influencing} {Factors} and {Formation} of the {Blind} {Zone} of a {Semitrailer} {Truck} in a {Right}-{Turn} {Collision}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2071-1050},
	url = {https://www.mdpi.com/2071-1050/14/16/9805},
	doi = {10.3390/su14169805},
	abstract = {The blind zone that accompanies the right-turn process of semitrailer trucks is a major cause of crashes and the high fatality of vulnerable road users (VRUs). Understanding the relationship between the blind zone and right-turn collisions will play a positive role in preventing such accidents. The purpose of this study was to investigate the formation of right-turn blind zones for semitrailer trucks and to determine the factors (turning speed, turning radius, and collision position) influencing the severity of accidents through real-world vehicle tests and PC-CRASH simulation. The results show that the calculation model of the inner wheel difference blind zone established for semitrailer trucks can provide more accurate estimation than the model for rigid trucks, due to the consideration of a virtual third axle between the tractor and the trailer. On the other hand, the PC-CRASH simulation test indicates the turning speed and turning radius directly affect the scale of the inner wheel difference blind zone, and larger blind zone and encroachment on adjacent lanes increase the potential for collision. Moreover, the difference in collision position is closely related to whether the rider suffers a secondary crush. Front position is more likely to cause the cyclist to be crushed. For further analysis, the long-term interaction between the blind zones resulting from the right rearview mirror and the inner wheel difference also increases the risk during a right turn. Therefore, reducing the blind zone in the right-turn process is the key to improving right-turn safety for semitrailer trucks and VRUs.},
	language = {en},
	number = {16},
	urldate = {2022-08-20},
	journal = {Sustainability},
	author = {Wang, Qingzhou and Sun, Jiarong and Wang, Nannan and Wang, Yu and Song, Yang and Li, Xia},
	month = jan,
	year = {2022},
	note = {Number: 16
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {blind zone, collision accident, PC-CRASH, semitrailer truck, traffic safety},
	pages = {9805},
	file = {Full Text PDF:C\:\\Users\\charl\\Zotero\\storage\\XY48IRMK\\Wang et al. - 2022 - Exploring the Influencing Factors and Formation of.pdf:application/pdf},
}

@article{dill_revisiting_2016,
	title = {Revisiting the {Four} {Types} of {Cyclists}: {Findings} from a {National} {Survey}},
	volume = {2587},
	issn = {0361-1981},
	shorttitle = {Revisiting the {Four} {Types} of {Cyclists}},
	url = {https://doi.org/10.3141/2587-11},
	doi = {10.3141/2587-11},
	abstract = {To understand the makeup of a population in terms of how people view bicycling can help in planning bicycle facilities and programs. Roger Geller, bicycle coordinator for the City of Portland, Oregon, proposed a typology that characterized people as one of four types—strong and fearless, enthused and confident, interested but concerned, and no way, no how—with respect to their attitudes toward bicycling. The research presented here sought to find out how applicable the typology was nationally and explored motivating factors, barriers, and the appeal of various bicycle facility types for each type of cyclist or potential cyclist. This study followed up on an earlier study that tested Geller’s typology with Portland data. In this new study, a sample involved 3,000 adults who lived in the 50 largest U.S. metropolitan areas. On the basis of respondents’ (a) stated level of comfort when they bicycled in different environments, (b) their interest in bicycling, and (c) their recent behavior, the study estimated that about one-third of adults were in the no way, no how group, and about half were in the interested but concerned group. The distribution was similar to that of the earlier findings for Portland. Several demographic differences emerged, with women less likely to be enthusiastic and confident or to be interested but concerned. Few differences appeared in whether respondents had ridden a bike at all in the past 30 days. Differences emerged as to where they bicycled and how often. The interested but concerned group was least likely to ride for transportation and rode less frequently. Barriers included not having a bicycle to ride; needing a vehicle for work, school, or other reasons; destinations too far to reach by bicycle; too few bike lanes or trails; and traffic.},
	language = {en},
	number = {1},
	urldate = {2022-08-21},
	journal = {Transportation Research Record},
	author = {Dill, Jennifer and McNeil, Nathan},
	month = jan,
	year = {2016},
	note = {Publisher: SAGE Publications Inc},
	pages = {90--99},
	file = {SAGE PDF Full Text:C\:\\Users\\charl\\Zotero\\storage\\FFPIJBE9\\Dill and McNeil - 2016 - Revisiting the Four Types of Cyclists Findings fr.pdf:application/pdf},
}

@patent{victor_blind_2013,
	title = {Blind spot warning device and blind spot warning system},
	url = {https://patents.google.com/patent/US20130169425A1/en?q=truck+blind+spot&country=US},
	nationality = {US},
	assignee = {Volvo Technology AB},
	number = {US20130169425A1},
	urldate = {2022-08-25},
	author = {Victor, Trent and Larsson, Pontus},
	month = jul,
	year = {2013},
	keywords = {blind spot, host vehicle, target object, warning, warning device},
	file = {Full Text PDF:C\:\\Users\\charl\\Zotero\\storage\\84XQBZAU\\Victor and Larsson - 2013 - Blind spot warning device and blind spot warning s.pdf:application/pdf},
}

@article{liu_ssd_2016,
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	volume = {9905},
	shorttitle = {{SSD}},
	url = {http://arxiv.org/abs/1512.02325},
	doi = {10.1007/978-3-319-46448-0_2},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For \$300{\textbackslash}times 300\$ input, SSD achieves 72.1\% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for \$500{\textbackslash}times 500\$ input, SSD achieves 75.1\% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
	urldate = {2022-08-25},
	journal = {European Conference on Computer Vision},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	year = {2016},
	note = {arXiv:1512.02325 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {21--37},
	file = {arXiv Fulltext PDF:C\:\\Users\\charl\\Zotero\\storage\\KSWLIKCP\\Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charl\\Zotero\\storage\\KUZDVBVF\\1512.html:text/html},
}

@patent{clegg_apparatus_2012,
	title = {Apparatus and {Methods} for {Eliminating} or {Reducing} {Blind} {Spots} in {Vehicle} {Mirror} and {Camera} {Systems}},
	url = {https://worldwide.espacenet.com/publicationDetails/biblio?FT=D&date=20120126&DB=EPODOC&locale=&CC=US&NR=2012022749A1&KC=A1&ND=1},
	abstract = {Provided herein are new apparatus and methods relating to mirror systems, including mirror systems for vehicles, including but not limited to automobiles, trucks, motorcycles, scooters, bicycles, all-terrain vehicles, motorized carts, among others. The apparatus and methods eliminate or reduce blind spots by facilitating a temporary desired adjustment of the mirror and/or camera, optionally followed by a return to the original position of the mirror and/or camera.},
	assignee = {Clegg Thomas},
	number = {US2012022749 (A1)},
	urldate = {2022-08-29},
	author = {Clegg, Thomas},
	month = jan,
	year = {2012},
}

@article{summerskill_development_2015,
	series = {6th {International} {Conference} on {Applied} {Human} {Factors} and {Ergonomics} ({AHFE} 2015) and the {Affiliated} {Conferences}, {AHFE} 2015},
	title = {The {Development} of a {Truck} {Concept} to {Allow} {Improved} {Direct} {Vision} of {Vulnerable} {Road} {Users} by {Drivers}},
	volume = {3},
	issn = {2351-9789},
	url = {https://www.sciencedirect.com/science/article/pii/S2351978915008045},
	doi = {10.1016/j.promfg.2015.07.803},
	abstract = {The paper describes a research project which examined the potential benefits of increasing the allowed lengths of heavy goods vehicles in Europe to foster improved aerodynamics and safety. A concept vehicle was analyzed using the SAMMIE Digital Human Modelling system through the use of a novel technique which allows the volume of space visible to drivers to be visualized and quantified. The technique was used to quantify the size of blind spots for the concept vehicle and a baseline existing vehicle. This concept was then further iterated to improved direct vision from the cab. The results indicate that the addition of aerodynamic front sections to existing vehicle cabs has minor benefits for improved direct vision from vehicle cabs, and that other modifications such as the addition of extra window apertures and lowering the vehicle cab with reference to the floor, have benefits in terms of allowing the driver to identify VRUs in close proximity to the vehicle.},
	language = {en},
	urldate = {2022-09-02},
	journal = {Procedia Manufacturing},
	author = {Summerskill, Steve and Marshall, Russell},
	month = jan,
	year = {2015},
	keywords = {Blind spot, Category N, Digital human modelling, European standards, HGV},
	pages = {3717--3724},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\charl\\Zotero\\storage\\I9K2RHUM\\Summerskill and Marshall - 2015 - The Development of a Truck Concept to Allow Improv.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\charl\\Zotero\\storage\\KFTTFB3P\\S2351978915008045.html:text/html},
}

@inproceedings{saleh_cyclist_2017,
	title = {Cyclist detection in {LIDAR} scans using faster {R}-{CNN} and synthetic depth images},
	doi = {10.1109/ITSC.2017.8317599},
	abstract = {In this paper a vision-based system for cyclist detection for highly automated vehicles is presented which utilize a deep convolutional neural network. Given the limited amount of LiDAR data for cyclists for the task of cyclist detection, a realistic synthetic data generation pipeline has been adopted in this work to overcome this problem by generate virtually unlimited number of synthetic training, testing and validation depth image datasets of cyclists. Then, using a holistic detection technique, any instances of cyclists can be localised in the generated synthetic depth images which can be shown as a down-sampled instances of 3D LiDAR data. The proposed technique in this paper, exploits the Faster Region-based Convolutional Network (Faster R-CNN) architecture for simultaneously detecting and localising any instances of cyclists in depth images. The trained Faster R-CNN models have been tested on the widely popular KITTI urban dataset and the results show that the proposed framework outperforms the classical HOG+SVM object localisation method with increased 21\% in average precision. This shows the ability of the trained models using the proposed framework to generalise from synthetic training dataset to operate on live dataset.},
	booktitle = {2017 {IEEE} 20th {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	author = {Saleh, Khaled and Hossny, Mohammed and Hossny, Ahmed and Nahavandi, Saeid},
	month = oct,
	year = {2017},
	note = {ISSN: 2153-0017},
	keywords = {Cameras, Feature extraction, Laser radar, Sensors, Task analysis, Three-dimensional displays, Training},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\charl\\Zotero\\storage\\DMUHXXLR\\8317599.html:text/html},
}

@inproceedings{zhang_design_2019,
	title = {Design of {Arduino}-{Based} {In}-vehicle {Warning} {Device} for {Inner} {Wheel} {Difference}},
	doi = {10.1109/ELTECH.2019.8839372},
	abstract = {After research, we find that the inner wheel difference has caused a large number of traffic accidents, and its harm cannot be ignored. In order to reduce traffic accidents, an Arduino-based warning device for inner wheel difference is designed in this paper. We first analyze the calculation model of inner wheel difference, then design and assemble each module separately based on the Arduino. In the visual warning part, the Single Shot MultiBox Detector (SSD) object detection model is used to locate and identify pedestrians and common objects, in order to realize a device for alerting both drivers and pedestrians based on vision and ultrasonic wave. Finally, based on the characteristics of the warning device and the social development, we analyze the advantages, technical feasibility and development prospects of the device.},
	booktitle = {2019 {IEEE} 2nd {International} {Conference} on {Electronics} {Technology} ({ICET})},
	author = {Zhang, Qingzhu and Wei, Yuanxi and Wang, Kefan and Liu, Hao and Xu, Youmin and Chen, Yuezhang},
	month = may,
	year = {2019},
	keywords = {Accidents, Acoustics, Automobiles, inner wheel difference, object detection, Turning, visual blind area, Visualization, warning, Wheels},
	pages = {301--304},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\charl\\Zotero\\storage\\TZKKU9FD\\Zhang et al. - 2019 - Design of Arduino-Based In-vehicle Warning Device .pdf:application/pdf},
}

@article{jannat_right-hook_2020,
	title = {Right-{Hook} {Crash} {Scenario}: {Effects} of {Environmental} {Factors} on {Driver}’s {Visual} {Attention} and {Crash} {Risk}},
	volume = {146},
	copyright = {©2020 American Society of Civil Engineers},
	shorttitle = {Right-{Hook} {Crash} {Scenario}},
	url = {https://ascelibrary.org/doi/10.1061/JTEPBS.0000342},
	doi = {10.1061/JTEPBS.0000342},
	abstract = {A right-hook (RH) crash is a common type of bicycle–motor vehicle crash that occurs between a right-turning vehicle and through-moving bicycle at an intersection in right-hand driving countries. Despite the frequency and severity of this crash type, no significant driver-performance based evidence of the causes of RH crashes at signalized intersections was found in the literature. This study examined the driver’s visual attention in a right-turning scenario at signalized intersections with bicycle lanes but no exclusive right-turning lanes while interacting with a bicyclist to develop an understanding of RH crash causality. Fifty-one participants in 21 simulated road scenarios performed a right-turning maneuver at a signalized intersection while conflicting with traffic, pedestrians, and bicyclists. Overall, a total of 820 (41 × 20) observable right-turn maneuvers with visual attention data were analyzed. The results show that in the presence of conflicting oncoming left-turning vehicular traffic, drivers spent less visual attention on the approaching bicyclist, thus, making them less likely to be detected by the driver. The presence of oncoming left-turning traffic and the bicyclist’s speed and relative position, and conflicting pedestrians were found likely to increase the risk of RH crashes. The results of the current study will help identify effective crash mitigation strategies that may include improving the vehicle–human interface or the implementation of design treatments in the road environment to improve driver and bicyclist performance.},
	language = {EN},
	number = {5},
	urldate = {2022-09-19},
	journal = {Journal of Transportation Engineering, Part A: Systems},
	author = {Jannat, Mafruhatul and Tapiro, Hagai and Monsere, Chris and Hurwitz, David S.},
	month = may,
	year = {2020},
	note = {Publisher: American Society of Civil Engineers},
	keywords = {Bicycle–motor vehicle crash, Bicyclist, Driver behavior, Driving simulator, Right-hook crash, Road safety},
	pages = {04020026},
	file = {Full Text PDF:C\:\\Users\\charl\\Zotero\\storage\\UJX56A38\\Jannat et al. - 2020 - Right-Hook Crash Scenario Effects of Environmenta.pdf:application/pdf},
}

@inproceedings{dalal_histograms_2005,
	title = {Histograms of oriented gradients for human detection},
	volume = {1},
	doi = {10.1109/CVPR.2005.177},
	abstract = {We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	author = {Dalal, N. and Triggs, B.},
	month = jun,
	year = {2005},
	note = {ISSN: 1063-6919},
	keywords = {High performance computing, Histograms, Humans, Image databases, Image edge detection, Object detection, Object recognition, Robustness, Support vector machines, Testing},
	pages = {886--893 vol. 1},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\charl\\Zotero\\storage\\UJASPPNV\\1467360.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\charl\\Zotero\\storage\\RQKYSV6P\\Dalal and Triggs - 2005 - Histograms of oriented gradients for human detecti.pdf:application/pdf},
}

@inproceedings{girshick_rich_2014,
	title = {Rich {Feature} {Hierarchies} for {Accurate} {Object} {Detection} and {Semantic} {Segmentation}},
	doi = {10.1109/CVPR.2014.81},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012 – achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/ rbg/rcnn.},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	month = jun,
	year = {2014},
	note = {ISSN: 1063-6919},
	keywords = {Feature extraction, Object detection, Proposals, Support vector machines, Training, Vectors, Visualization},
	pages = {580--587},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\charl\\Zotero\\storage\\S3DK9X6G\\6909475.html:text/html;Submitted Version:C\:\\Users\\charl\\Zotero\\storage\\ZXSGQQW4\\Girshick et al. - 2014 - Rich Feature Hierarchies for Accurate Object Detec.pdf:application/pdf},
}

@inproceedings{huang_speedaccuracy_2017,
	title = {Speed/{Accuracy} {Trade}-{Offs} for {Modern} {Convolutional} {Object} {Detectors}},
	doi = {10.1109/CVPR.2017.351},
	abstract = {The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-toapples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [30], R-FCN [6] and SSD [25] systems, which we view as meta-architectures and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin},
	month = jul,
	year = {2017},
	note = {ISSN: 1063-6919},
	keywords = {Computer architecture, Detectors, Feature extraction, Object detection},
	pages = {3296--3297},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\charl\\Zotero\\storage\\BCMRFZBP\\8099834.html:text/html;Submitted Version:C\:\\Users\\charl\\Zotero\\storage\\H93IAC49\\Huang et al. - 2017 - SpeedAccuracy Trade-Offs for Modern Convolutional.pdf:application/pdf},
}

@misc{lin_microsoft_2015,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {https://doi.org/10.48550/arXiv.1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = feb,
	year = {2015},
	note = {arXiv:1405.0312 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\charl\\Zotero\\storage\\VPAQGSNZ\\Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charl\\Zotero\\storage\\L4748ALI\\1405.html:text/html},
}

@article{richter_turning_2017,
	series = {World {Conference} on {Transport} {Research} - {WCTR} 2016 {Shanghai}. 10-15 {July} 2016},
	title = {Turning accidents between cars and trucks and cyclists driving straight ahead},
	volume = {25},
	issn = {2352-1465},
	url = {https://www.sciencedirect.com/science/article/pii/S2352146517305203},
	doi = {10.1016/j.trpro.2017.05.219},
	abstract = {Two projects contain the conflict between turning vehicles and straight ahead driving cyclists. One out of four of all accidents in Germany are accidents with bicycles. In Addition, most of the accidents end with heavy or deathly personal injury. Especially accidents with right turning trucks and straight ahead driving cyclist are on one hand rarely but on the other hand very dangerous because of gravity results of an accident. Finally one of the main problems is the conflict during the driving by green traffic light. Furthermore the obstructed view is a big problem for this accident constellation.},
	language = {en},
	urldate = {2022-10-15},
	journal = {Transportation Research Procedia},
	author = {Richter, Thomas and Sachs, Janina},
	month = jan,
	year = {2017},
	keywords = {Accident analysis, behavior observation, blind spot, cycling, infrastructure, traffic safety, turning accidents},
	pages = {1946--1954},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\charl\\Zotero\\storage\\IZYJ9JES\\Richter and Sachs - 2017 - Turning accidents between cars and trucks and cycl.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\charl\\Zotero\\storage\\HMWEXXVF\\S2352146517305203.html:text/html},
}

@misc{seshadri_evaluation_2022,
	title = {An {Evaluation} of {Edge} {TPU} {Accelerators} for {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2102.10423},
	abstract = {Edge TPUs are a domain of accelerators for low-power, edge devices and are widely used in various Google products such as Coral and Pixel devices. In this paper, we first discuss the major microarchitectural details of Edge TPUs. Then, we extensively evaluate three classes of Edge TPUs, covering different computing ecosystems, that are either currently deployed in Google products or are the product pipeline, across 423K unique convolutional neural networks. Building upon this extensive study, we discuss critical and interpretable microarchitectural insights about the studied classes of Edge TPUs. Mainly, we discuss how Edge TPU accelerators perform across convolutional neural networks with different structures. Finally, we present our ongoing efforts in developing high-accuracy learned machine learning models to estimate the major performance metrics of accelerators such as latency and energy consumption. These learned models enable significantly faster (in the order of milliseconds) evaluations of accelerators as an alternative to time-consuming cycle-accurate simulators and establish an exciting opportunity for rapid hard-ware/software co-design.},
	urldate = {2022-12-01},
	publisher = {arXiv},
	author = {Seshadri, Kiran and Akin, Berkin and Laudon, James and Narayanaswami, Ravi and Yazdanbakhsh, Amir},
	month = oct,
	year = {2022},
	note = {arXiv:2102.10423 [cs]},
	keywords = {Computer Science - Hardware Architecture, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charl\\Zotero\\storage\\TLRRGTPW\\Seshadri et al. - 2022 - An Evaluation of Edge TPU Accelerators for Convolu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charl\\Zotero\\storage\\HG3IM7SJ\\2102.html:text/html},
}

@inproceedings{sandler_mobilenetv2_2018,
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	shorttitle = {{MobileNetV2}},
	doi = {10.1109/CVPR.2018.00474},
	abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {Computational modeling, Computer architecture, Manifolds, Neural networks, Standards, Task analysis},
	pages = {4510--4520},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\charl\\Zotero\\storage\\K3RXRRAW\\8578572.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\charl\\Zotero\\storage\\8LNQ8I3C\\Sandler et al. - 2018 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:application/pdf},
}

@misc{chen_diffusiondet_2022,
	title = {{DiffusionDet}: {Diffusion} {Model} for {Object} {Detection}},
	shorttitle = {{DiffusionDet}},
	url = {http://arxiv.org/abs/2211.09788},
	doi = {10.48550/arXiv.2211.09788},
	abstract = {We propose DiffusionDet, a new framework that formulates object detection as a denoising diffusion process from noisy boxes to object boxes. During training stage, object boxes diffuse from ground-truth boxes to random distribution, and the model learns to reverse this noising process. In inference, the model refines a set of randomly generated boxes to the output results in a progressive way. The extensive evaluations on the standard benchmarks, including MS-COCO and LVIS, show that DiffusionDet achieves favorable performance compared to previous well-established detectors. Our work brings two important findings in object detection. First, random boxes, although drastically different from pre-defined anchors or learned queries, are also effective object candidates. Second, object detection, one of the representative perception tasks, can be solved by a generative way. Our code is available at https://github.com/ShoufaChen/DiffusionDet.},
	urldate = {2023-01-16},
	publisher = {arXiv},
	author = {Chen, Shoufa and Sun, Peize and Song, Yibing and Luo, Ping},
	month = nov,
	year = {2022},
	note = {arXiv:2211.09788 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\charl\\Zotero\\storage\\VINHR636\\Chen et al. - 2022 - DiffusionDet Diffusion Model for Object Detection.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charl\\Zotero\\storage\\EFXZDX88\\2211.html:text/html},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2023-01-16},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charl\\Zotero\\storage\\T57PBBIV\\Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charl\\Zotero\\storage\\SAXCJYKD\\2010.html:text/html},
}

@misc{dwyer_roboflow_2022,
	title = {Roboflow},
	url = {https://roboflow.com},
	author = {Dwyer, B and Nelson, J},
	year = {2022},
}

@inproceedings{tan_efficientdet_2020,
	title = {{EfficientDet}: {Scalable} and {Efficient} {Object} {Detection}},
	shorttitle = {{EfficientDet}},
	doi = {10.1109/CVPR42600.2020.01079},
	abstract = {Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multi-scale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and EfficientNet backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single-model and single-scale, our EfficientDet-D7 achieves state-of-the-art 52.2 AP on COCO test-dev with 52M parameters and 325B FLOPs1, being 4x - 9x smaller and using 13x - 42x fewer FLOPs than previous detector. Code is available at https://github.com/google/ automl/tree/master/efficientdet.},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V.},
	month = jun,
	year = {2020},
	note = {ISSN: 2575-7075},
	keywords = {Compounds, Detectors, Feature extraction, Image resolution, Network architecture, Object detection, Optimization},
	pages = {10778--10787},
	file = {Submitted Version:C\:\\Users\\charl\\Zotero\\storage\\5KIJITJE\\Tan et al. - 2020 - EfficientDet Scalable and Efficient Object Detect.pdf:application/pdf},
}

@misc{thomas_open_2021,
	title = {Open {Synthetic} {Dataset} for {Improving} {Cyclist} {Detection}},
	publisher = {Parallel Domain Inc},
	author = {Thomas, P and Pandikow, L and Kim, A and Stanley, M and Grieve, J},
	year = {2021},
}

@misc{everingham_visual_2012,
	title = {The {Visual} {Object} {Classes} {Challenge} 2012 ({VOC2012})},
	url = {http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html},
	author = {Everingham, M and Van-Gool, L and Williams, C and Winn, J and Zisserman, A},
	year = {2012},
}
