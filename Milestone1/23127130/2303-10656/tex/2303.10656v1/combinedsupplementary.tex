% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{bm}
\usepackage{todonotes}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{booktabs}
% \usepackage{physics}
\usepackage[misc]{ifsym}
\usepackage[T1]{fontenc}


\usetikzlibrary{calc}

\newcommand\tikzmark[1]{%
  \tikz[overlay,remember picture] \coordinate (#1);}

% \usepackage{cite}
% \usepackage{amsmath,amsfonts,amssymb,amsthm,fullpage,esdiff,subcaption,physics,bm,verbatim}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
\renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}

\title{More From Less: Self-Supervised Knowledge Distillation for Information-Sparse Histopathology Data}

\titlerunning{More From Less: Self-Supervised Knowledge Distillation}

\author{Lucas Farndale\inst{1,2,3,4}\textsuperscript{(\Letter)} \and
Robert Insall\inst{1,2} \and
Ke Yuan\inst{1,2,3}}

%
\authorrunning{L. Farndale et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{School of Cancer Sciences, University of Glasgow, Glasgow, Scotland, UK \and
Cancer Research UK Beatson Institute, Glasgow, Scotland, UK \and
School of Computing Science, University of Glasgow, Glasgow, Scotland, UK \and
School of Mathematics and Statistics, University of Glasgow, Scotland, UK\\
\email{\{lucas.farndale, robert.insall, ke.yuan\}@glasgow.ac.uk}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
    Medical imaging technologies are generating increasingly large amounts of high-quality, information-dense data. Despite the progress, practical use of advanced imaging technologies for research and diagnosis remains limited by cost and availability, so information-sparse data such as H\&E stains are relied on in practice. The study of diseased tissue requires methods which can leverage these information-dense data to extract more value from routine, information-sparse data. Using self-supervised deep learning, we demonstrate that it is possible to distil knowledge during training from information-dense data into models which only require information-sparse data for inference. This improves downstream classification accuracy on information-sparse data, making it comparable with the fully-supervised baseline. We find substantial effects on the learned representations, and this training process identifies subtle features which otherwise go undetected. This approach enables the design of models which require only routine images, but contain insights from state-of-the-art data, allowing better use of the available resources.
    
    \keywords{Representation Learning \and Colon Cancer \and Multi-Magnification.}
\end{abstract}

\section{Introduction}

Medical imaging technologies are constantly increasing in complexity and the amount of information they generate.  Developments such as spatial -omics, multiplex immunohistochemistry and super-resolution microscopy are continuously enabling greater insights into mechanisms of disease, but adoption of these technologies is prohibitively expensive for large cohorts or routine use. It is therefore highly desirable to develop methods which distil knowledge from these exceptionally dense and information-rich data into more accessible and affordable routine imaging models, so clinicians and researchers can obtain the most diagnostic information possible from the data available to them.

Typically, knowledge distillation focuses on distilling from a (possibly pretrained) larger teacher model into a smaller student model \cite{gou2021knowledge}. This is usually achieved using a self-supervised joint-embedding architecture, where two models are trained as parallel branches to output the same representations \cite{gou2021knowledge}, so the smaller model can be more easily deployed in practice on the same dataset without sacrificing accuracy. This approach is ideal for digital pathology in which complete images are impractically large and often initially viewed at low magnification. Both knowledge distillation \cite{javed2023knowledge} and self-supervised learning \cite{quiros2022self} have been shown to improve performance on histopathology imaging tasks, including when used in tandem \cite{dipalma2021resolution}.

The downside of this approach is that it usually requires that the data sources used in training are both available during inference, which is severely limiting where at least one source of data is not available, is more expensive, or is more computationally demanding. There are a limited number of models which can admit different architectures on each branch, such as the supervised method CLIP \cite{radford2021learning}, or self-supervised models such as VSE++ \cite{faghri2017vse++}, Barlow Twins \cite{zbontar2021barlow}, SimCLR \cite{chen2020simple} and VICReg \cite{bardes2021vicreg}, which has been shown to outperform VSE++ and Barlow Twins on multi-modal data \cite{bardes2021vicreg}.

In this work, we focus on knowledge distillation from a model of an \emph{information-dense} dataset (a dataset rich in accessible information, e.g. high-resolution pathology images) into a model of an \emph{information-sparse} dataset (a dataset containing little or obfuscated information, e.g. low-resolution images). We make the following contributions:
\begin{itemize}
    \item We find that knowledge distillation significantly improves downstream classification accuracy on information-sparse data;
    \item We show that this training process results in measurably different representations compared to standard self-supervised and supervised training, in both information-dense and -sparse data;
    \item We show in a common tissue classification task that, where there is a large difference in the information density, self-supervised learning with knowledge distillation is comparable to and can outperform the supervised baseline. 
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/fig1.pdf}
    \caption{Knowledge distillation between information-dense and information-sparse data. Each branch represents a different task. The upper branch shows multi-magnification distillation, with images being downsampled to reduce information density. The lower branch shows the construction of nuclear segmentation masks, which are paired with the original images for training. Images are preprocessed, paired and passed to the self-suprevised model for training. The trained encoders are then used for downstream classification tasks to assess performance.}
    \label{fig:dataset}
\end{figure}

\section{Methods}

\subsection{Experimental Design}

\noindent We use the self-supervised methods VICReg \cite{bardes2021vicreg} and SimCLR \cite{chen2020simple} to distil knowledge from information-dense data to information-sparse data. As shown in Fig.~\ref{fig:dataset}, the model takes pairs of images as input, one for each branch. We refer to a pair consisting of two copies of the same image as a \emph{symmetric} pair, and a pair consisting of distinct images as an \emph{asymmetric} pair. For example, the first image in an asymmetric pair might always be a lower resolution than the second image, while in a symmetric pair, both images would be the same resolution. 

\subsubsection{Multi-Magnification Distillation}

We first test the model's performance by classifying patches which have been downsampled to a lower resolution. The downsampling results in a significant loss of information, so the higher-resolution patches (the original, unchanged patches) are more information-dense than the downsampled patches. In the symmetric case, both images are downsampled to $7\times7$px (pixels), while in the asymmetric case, only the first branch is downsampled, leaving the second branch at the full resolution of $224\times224$px. The downsampled images are always resized to $224\times224$px using linear interpolation to enable better comparison between methods, and avoid overfitting a large model on very small inputs. The procedure is shown in Fig.~\ref{fig:dataset}. All images are then subject to random augmentations, which are listed in Table \ref{tab:augmentations}.

\subsubsection{Tissue-Nuclei Distillation}

To enable finer-grained analysis, we next artificially construct an information-sparse dataset by using HoVer-Net \cite{graham2019hover} to segment the nuclei in each image, and retaining only the mask of the nuclei, coloured by their predicted types (Background, Neoplastic, Inflammatory, Connective, Dead, Non-Neoplastic Epithelial), as shown in Fig.~\ref{fig:dataset}. We repeat these 1D masks 3 times along channel 3 to ensure consistency of input size between models for comparison. These images contain significantly less information, as the majority of fine-grained morphological information in the image is lost, and the HoVer-Net segmentations are imperfect. Some relevant morphological features are retained, such as the nuclear/cytoplasmic ratio and nuclear morphology, but much predictive information is lost. By definition, all information which is present in these nuclear masks is present in the original images, so there can be no issue with the masks introducing additional information.

\subsection{Training}

In our experiments\footnote{All code is available at https://anonymous.4open.science/r/More-From-Less.}, all models except ablations used a ResNet-50 encoder \cite{he2016deep} with output size 2048, and expanders were composed of three dense layers with ReLU activations and output size 8192. Encoder and model ablations are detailed in Tables \ref{fig:encablations} and \ref{fig:modelablations}. Models are trained using Tensorflow 2.9.1 from a random initialisation for 100 epochs, with a batch size of 128. An Adam optimiser \cite{kingma2014adam} with a warm-up cosine learning rate \cite{goyal2017accurate} was used, with a maximum learning rate of $10^{-4}$, and warm-up period of 1/10th of an epoch. Training one model takes 8 hours on an Nvidia A6000 GPU. For consistency with the original implementations, loss function parameters are $\lambda=25, \mu=25, \nu=1$ for VICReg, and temperature $t=0.5$ for SimCLR. With the encoder's weights frozen, we assess model performance by training a classifier (dense layer with softmax) for 100 epochs using an Adam optimiser and learning rate of $10^{-3}$. We produce supervised baseline comparisons using an equivalent encoder and softmax classifier head, following the same training protocol.

Models are evaluated based on the accuracy of their classification of tissue types, as detailed in Section \ref{sec:datasets}. To assess the robustness of the representations to different tasks, we also test the models' performance in predicting the cell type most present in each image for the tissue-nuclei distillation dataset. We choose this task as these masks contain the same relevant and less irrelevant information than the raw images, so the original information densities of the images are reversed. The labels are determined by the HoVer-Net segmentation, as manual labels were not available. Images were only given the label 'Background' if there were no nuclei present in the image. To further investigate the difference among learned representations, we then use centered kernel alignment (CKA) \cite{kornblith2019similarity} and GradCAM \cite{selvaraju2017grad} to analyse the outputs of each convolutional layer.


\subsection{Datasets}
\label{sec:datasets}

For all examples in this work, we have used the Kather colorectal cancer dataset \cite{kather_dataset}, a set of 100000 $224\times224$px (0.5mpp (microns per pixel)) non-overlapping patches from 86 H\&E stained histopathology slides of human colorectal cancer and healthy tissue. The images were manually segmented into 9 classes: Adipose (ADI), background (BACK), debris (DEB), lymphocytes (LYM), mucus (MUC), smooth muscle (MUS), normal colon mucosa (NORM), cancer-associated stroma (STR), colorectal adenocarcinoma epithelium (TUM). Patches were sampled from within these regions by a stochastic sampling algorithm. All patches are colour-normalised using the Macenko method \cite{macenko2009method}.

We use the additional external validation dataset from the same source, which contains 7180 $224\times224$px (0.5mpp) images from a distinct 50-patient cohort. Previous work often reserves a (possibly unbalanced) subset from the training dataset, or only uses a subset of labels, particularly combining muscle and stroma \cite{rkaczkowski2019ara,kather2016multi}. Testing on the external validation dataset and all 9 labels makes the task considerably more challenging.

\begin{table}
    \caption{Results of experiments using VICReg with asymmetric downsampled/full resolution pairs compared with symmetric downsampled/downsampled pairs and supervised downsampled images. Bold indicates best performance out of symmetric and asymmetric.}
    \label{tab:mags}
    \centering
     % \begin{tabular}{ l{5cm} c{2cm} c{2cm} c{2cm} c{3cm}  }
     \begin{tabular}{p{2cm} p{2cm} cc|c}
     \toprule
     \multicolumn{2}{c}{\multirow{2}{*}{Downsampled Resolution}} & \multicolumn{3}{c}{Accuracy}\\
     \cmidrule(r){3-5}
     & & Asymmetric & Symmetric & Supervised\\
     \midrule
     \tikzmark{end}$224\times224$px & (0.5mpp) & 0.8587 & $\bm{0.8855}$ & 0.9370\\
     $112\times112$px & (1mpp) & 0.8884 & $\bm{0.9138}$ & 0.9412 \\
     $56\times56$px & (2mpp) & 0.8745 & $\bm{0.8810}$ & 0.9338 \\
     $28\times28$px & (4mpp) & 0.8605 & $\bm{0.8661}$ & 0.8912 \\
     $14\times14$px & (8mpp) & $\bm{0.8264}$ & 0.8175 & 0.8573 \\
     \tikzmark{start}$7\times7$px & (16mpp) & $\bm{0.7743}$ & 0.6926 & 0.7656 \\
     \bottomrule
    \end{tabular}
    \begin{tikzpicture}[overlay,remember picture]
    \draw[->] let \p1=(start), \p2=(end) in ($(\x1,\y1)-(0.3,0.1)$) -- node[label={[align=center,rotate=90,xshift=1.15cm,yshift=0.1cm]left:\tiny information density}] {} ($(\x1,\y2)-(0.3,-0.2)$);
  \end{tikzpicture}
  \end{table}

\begin{table}[t]
    \caption{Classification accuracy of asymmetric, symmetric and supervised models on two downstream tasks. Supervised tissue/nuclei encoders were trained on the tissue/nuclei tasks respectively, and were then frozen and a new classifier head was trained for the transfer task. Bold indicates highest accuracy out of self-supervised methods.}
    \label{tab:infodense}
    \centering
     \scalebox{0.96}{
     \begin{tabular}{ llcc|cc|cc }
     \toprule
     \multicolumn{2}{c}{\multirow{2}{*}{Task}} & \multicolumn{2}{c}{VICReg} & \multicolumn{2}{c}{SimCLR} & \multicolumn{2}{c}{Supervised} \\
     \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}
     & & Asymmetric & Symmetric & Asymmetric & Symmetric & Tissue & Nuclei \\
     \midrule
     \multirow{2}{*}{Tissue Types} & Masks & $\bm{0.5809}$ & 0.5338 & 0.5600 & 0.5454 & 0.5909 & 0.5345 \\
     & Images & 0.8979 & 0.8855 & 0.8850 & $\bm{0.9075}$ & 0.9451 & 0.9314 \\
     \multirow{2}{*}{Nucleus Counting} & Masks & $\bm{0.8650}$ & 0.8158 &  0.8540 & 0.8235 & 0.9103 & 0.9365 \\
     & Images & $\bm{0.8127}$ & 0.6904 & 0.8038 & 0.6914 & 0.7045 & 0.8205 \\
    \bottomrule
    \end{tabular}
    }
\end{table}


\section{Results}

\subsection{Multi-Magnification Distillation}

\noindent For 6 magnification levels, we consider asymmetric, symmetric and supervised cases, with results shown in Table \ref{tab:mags}. Our primary finding from these experiments is that asymmetric training significantly outperforms symmetric and even supervised training when differences between the information density of inputs are large. In the cases with less downsampling, we find that the symmetric case outperforms the asymmetric case. Downsampling the image to $112\times112$px or $56\times56$px has little effect on the accuracy, and the effect of the asymmetry increases with the difference in information density.



\subsection{Tissue-Nuclei Distillation}

\noindent For VICReg and SimCLR we consider performance on two tasks for both symmetric and asymmetric cases on images and masks, as well as a supervised baseline for comparison.  Results are shown in Table \ref{tab:infodense}. Although the accuracy of the tissue classification task is lower for the masks than for the images, it is considerably better than random (0.1111). In all cases, models trained on asymmetric image/mask pairs outperform models trained on symmetric mask/mask pairs. As expected, supervised learning results in the best accuracy on the task it was trained on, however, table \ref{tab:infodense} shows that these representations often do not transfer well to other downstream tasks. Despite comparable performance to the asymmetric model on the tissue classification task, the symmetric VICReg image model performs considerably worse on the nucleus counting task.

Fig.~\ref{fig:confmats} shows that the model performs fairly accurately on classes which typically contain at least one nucleus, but accurate classifications of most debris, background and adipose are impossible as the masks are simply matrices of zeros.

\subsection{Comparing Layer Output Similarities}

\begin{figure}[t]
\begin{subfigure}{0.27\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/fig2sf1.pdf}
    \caption{CKA}
    \label{fig:ckaschematic}
\end{subfigure}
\begin{subfigure}{0.7\textwidth}
    \centering
    \includegraphics[width=\textwidth, trim={0cm 8.5cm 0cm 8cm}, clip]{figs/newCKAplot.pdf}
    \caption{}
    \label{fig:ckaplots}
\end{subfigure}
    \caption{(a) Schematic showing the relationships between layers where centered kernel alignment (CKA) is calculated, (b) CKA analysis of layer outputs, averaged over combinations of 4 models for each case. Higher values indicate more similarity between layer outputs. The left chart shows models trained on either mask/mask (symmetric) or image/mask pairs (asymmetric). The right chart shows models trained on either $7\times7$px image pairs (symmetric) or $7\times7$px images paired with full-resolution images (asymmetric).}
    \label{fig:cka}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}[b]{0.35\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/fig3sf1.pdf}
    \caption{Model}
    \label{fig:gradcamschematic}
\end{subfigure}
\begin{subfigure}[b]{0.29\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/updatedgradcamtrain.pdf}
    \caption{Train}
    \label{fig:gradcamtrain}
\end{subfigure}
\begin{subfigure}[b]{0.29\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/updatedgradcamtest.pdf}
    \caption{Test}
    \label{fig:gradcamtest}
\end{subfigure}
\caption{(a) Schematic showing where in the model GradCAM images are obtained, with samples from the (b) train and (c) test sets. (A,E) Asymmetric GradCAM images, (B,F) symmetric GradCAM images, (C,G) original tissue patches, (D,H) HoVer-Net masks used for predictions.}
\label{fig:gradcamfigure}
\end{figure}

\noindent The differences between layers are demonstrated by the qualitative GradCAM analysis in Fig.~\ref{fig:gradcamfigure}. This representative sample shows that, in the first convolutional layer, the symmetrically trained model focuses more on coarse-grained morphological features, while the asymmetrically trained model focuses more on fine-grained features. Fig.~\ref{fig:gradcamlayers} shows that this difference is seen throughout all layers of the model.

To further analyse the effect of asymmetry on the trained model, we use centered kernel alignment (CKA) to compare the outputs of each convolutional layer of each model. CKA is a measure of the similarity of the outputs. For example, random outputs will share most of the same characteristics, and will receive a very high similarity score, while receiving a very low score with a trained model. Fig.~\ref{fig:cka} shows the similarity between symmetric, asymmetric, supervised and random models. The similarity between symmetric/symmetric, asymmetric/asymmetric, supervised/supervised and random/random pairs is high, while asymmetric, supervised, supervised and random models are all significantly different to each other, with these differences mostly increasing throughout the model, and resulting in substantially different representations.

\section{Discussion}

Our results show that training on asymmetric inputs improves the predictive performance of models on downstream tasks by improving the quality of representations of information-sparse inputs. In the tissue/nuclei example, the shape, distribution, and type of the cells present in the masks are sufficient to make reasonable predictions of the tissue types. However, without the presence of the raw image, the symmetric model differentiates nuclear morphologies poorly, resulting in lower-quality representations and predictions in downstream tasks.

\subsection{Asymmetry Identifies More Subtle Features}

Each branch in a joint-embedding architecture utilises the output of the other branch as a supervisory signal, meaning both branches extract approximately the same features from their input. The asymmetric pairing forces the information-sparse encoder to find patterns in the data which correlate with those found in the information-dense images, finding subtler features in the limited information present. In the symmetric case, the model is less able to focus on these weakly-signalled features, as they may not be easily distinguished from noise.

Despite the objective forcing embeddings to be as similar as possible, we still observe significant differences in the classification accuracy of each branch. It has been shown that the use of projection heads improves representation quality \cite{chen2020simple,mialon2022variance}. We conjecture that the projection head filters out information irrelevant to the opposite branch. This explains the high classification accuracy observed in the asymmetrically trained original image encoders, which remains comparable with symmetric original image encoders despite being paired with a significantly less informative image.

\subsection{Asymmetric Learning Obtains Measurably Different Representations}

We have seen improvements in the information density of the representations of the information-sparse inputs. In addition, we observe both qualitative and quantitative differences in the features of the images which are identified by the trained models, both in the information-dense and information-sparse inputs. Intuitively, this is because knowledge distillation is bidirectional, so while knowledge is distilled from the information-dense model into the information-sparse model, there is also knowledge distilled from the information-sparse model to the information-dense model. This is evidenced by the significantly better accuracy of the asymmetric VICReg image encoder on the nuclei counting task compared with the equivalent symmetric model.

Our CKA analysis, shown in Fig.~\ref{fig:cka}, further quantifies the difference between the layers of symmetric and asymmetric models, with randomly initialised models as a baseline comparison. Models with different initialisations become very similar to other models with the same training regime, but significantly different to models with a different training regime, for both information-sparse and -dense data. We conclude that the representations of asymmetrically trained models contain different features from those of symmetrically trained models. This is corroborated by our qualitative GradCAM analysis, as shown in Fig.~\ref{fig:gradcamfigure}


\subsection{Future Work}

There are a plethora of histological datasets which pair different modalities, inputs, or views, and asymmetry could be leveraged to uncover previously undetected features of these data. For example, training on multiple slides from the same biopsy, integrating whole-slide images with -omics data, or utilising data from different timepoints to predict prognosis. Furthermore, future work could utilise different architectures in each branch, in line with typical knowledge distillation frameworks, to ensure more informative representations are obtained from the information-dense inputs, greater improving the efficacy of the technique. 

\subsubsection{Acknowledgements} LF is supported by the MRC grant MR/W006804/1, RI is supported by EPSRC grant EP/S0300875/1 and Wellcome grant 221786/Z/20/Z. K.Y. acknowledges support from EP/R018634/1, BB/V016067/1, and European Unionâ€™s Horizon 2020 research and innovation programme under grant agreement No 101016851, project PANCAIM. We thank Dr. Adalberto Claudio-Quiros for his helpful feedback and support.

\bibliographystyle{splncs04}
\bibliography{bib}

\newpage


\section*{Supplementary Information}

\newcommand{\beginsupplement}{
    \setcounter{section}{0}
    \renewcommand{\thesection}{S\arabic{section}}
    \setcounter{equation}{0}
    \renewcommand{\theequation}{S\arabic{equation}}
    \setcounter{table}{0}
    \renewcommand{\thetable}{S\arabic{table}}
    \setcounter{figure}{3}
    \renewcommand{\thefigure}{S\arabic{figure}}
    \newcounter{SIfig}
    \renewcommand{\theSIfig}{S\arabic{SIfig}}}

\beginsupplement

\begin{table}[]
    \caption{Augmentations and probability of application. Augmentations are randomly applied at each training iteration, with the given probability.}
    \refstepcounter{SIfig}\label{tab:augmentations}
    \centering
    \scalebox{1}{\begin{tabular}{lc}
    \toprule
    Augmentation & Probability\\
    \midrule
    Flip (left/right/up/down) & 1.0 \\
    Crop (scale 75\% to 100\%) & 1.0 \\
    Gaussian Background Noise & 0.3\\
    Rotation & 0.4 \\
    Solarize & 0.3 \\
    Colour Jitter & 1.0\\
    \bottomrule
    \end{tabular}}
\end{table}

\begin{table}
    \caption{Encoder Ablations. Results are found to be robust to the choice of encoder, with ResNet50 \cite{he2016deep}, MobileNetV2 \cite{howard2017mobilenets}, EfficientNetB0 \cite{tan2021efficientnetv2} and Xception \cite{chollet2017xception} tested. Bold type indicates best self-supervised performance on downsampled image predictions.}
    \label{fig:encablations}
    \centering
     \scalebox{1}{\begin{tabular}{lccc|ccc}%{ p{2cm}p{2cm}p{2cm}p{2cm}|p{2cm}p{2cm}p{2cm} }
     \toprule
     \multirow{2}{*}{Encoder} & \multicolumn{3}{c}{Downsampled} & \multicolumn{3}{c}{Full Resolution}\\
     \cmidrule{2-7}
     & Asymmetric & Symmetric & Supervised & Asymmetric & Symmetric & Supervised\\
     \midrule
     ResNet50 & $\bm{0.7743}$ & 0.6926 & 0.7656 & 0.7890 & 0.8846 & 0.9370 \\
     MobileNetV2 & $\bm{0.7580}$ & 0.7050 & 0.7616 & 0.7959 & 0.8602 & 0.9390 \\
     % VGG-19 & 0.7088 & 0.3807 & 0.7389 & 0.3907 & - & 0.9347 \\
     EfficientNetB0 & $\bm{0.7757}$ & 0.7109 & 0.7595 & 0.8265 & 0.8441 & 0.9241 \\
     Xception & $\bm{0.7619}$ & 0.6858 & 0.7719 & 0.8258 & 0.8965 & 0.9351 \\
     \bottomrule
    \end{tabular}}
\end{table}

\begin{table}[!h]
    \caption{Model Ablations. Results are found to be robust to both contrastive or non-contrastive methods (SimCLR and VICReg) and whether weights are shared between branches. Bold type indicates best self-supervised performance on downsampled image predictions.}
    \label{fig:modelablations}
    \centering
     \scalebox{1}{\begin{tabular}{lcc|cc}%{ p{5cm}p{2cm}p{2cm}|p{2cm}p{2cm} }
     \toprule
     \multirow{2}{*}{Model} & \multicolumn{2}{c}{Downsampled} & \multicolumn{2}{c}{Full Resolution}\\
     \cmidrule{2-5}
     & Asymmetric & Symmetric & Asymmetric & Symmetric \\
     \hline
     SimCLR (shared weights) & $\bm{0.7780}$ & 0.7241 & 0.8136 & 0.9075 \\
     SimCLR (independent weights) & $\bm{0.7505}$ & 0.7243 & 0.8250 & 0.8715 \\
     % Barlow Twins (shared weights) & - & - & - & todo\\
     % Barlow Twins (distinct weights) & - & - & - & todo\\
     VICReg (shared weights) & $\bm{0.7730}$ & 0.6926 & 0.7867 & 0.8846 \\
     VICReg (independent weights) & $\bm{0.7743}$ & 0.7383 & 0.7890 & 0.8672 \\
     \hline
    \end{tabular}}
\end{table}

\begin{figure}[b]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/blue_val_asymm_conf_mat}
        \caption{Asymmetric}
        \label{fig:asymmconfmat}
    \end{subfigure}
    ~
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/blue_val_symm_conf_mat}
        \caption{Symmetric}
        \label{fig:symmconfmat}
    \end{subfigure}
    \caption{Confusion Matrices for mask predictions of classifier trained on (a) image/mask and (b) mask/mask pairs.}
    \label{fig:confmats}
\end{figure}



\begin{figure}
    \centering
    \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth]{figs/asymm-layers.png}
    \caption{Asymmetric}
    \label{fig:asymmlayers}
    \end{subfigure}
    ~
    \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth]{figs/symm-layers.png}
    \caption{Symmetric}
    \label{fig:symmlayers}
    \end{subfigure}
    \caption{GradCAM analysis of first convolutional layer from each ResNet block in model trained on (a) image/mask and (b) mask/mask pairs.}
    \label{fig:gradcamlayers}
\end{figure}

\end{document}