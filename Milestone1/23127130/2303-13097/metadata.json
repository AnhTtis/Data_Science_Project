{
    "arxiv_id": "2303.13097",
    "paper_title": "CP$^3$: Channel Pruning Plug-in for Point-based Networks",
    "authors": [
        "Yaomin Huang",
        "Ning Liu",
        "Zhengping Che",
        "Zhiyuan Xu",
        "Chaomin Shen",
        "Yaxin Peng",
        "Guixu Zhang",
        "Xinmei Liu",
        "Feifei Feng",
        "Jian Tang"
    ],
    "submission_date": "2023-03-23",
    "revised_dates": [
        "2023-03-24"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.AI"
    ],
    "abstract": "Channel pruning can effectively reduce both computational cost and memory footprint of the original network while keeping a comparable accuracy performance. Though great success has been achieved in channel pruning for 2D image-based convolutional networks (CNNs), existing works seldom extend the channel pruning methods to 3D point-based neural networks (PNNs). Directly implementing the 2D CNN channel pruning methods to PNNs undermine the performance of PNNs because of the different representations of 2D images and 3D point clouds as well as the network architecture disparity. In this paper, we proposed CP$^3$, which is a Channel Pruning Plug-in for Point-based network. CP$^3$ is elaborately designed to leverage the characteristics of point clouds and PNNs in order to enable 2D channel pruning methods for PNNs. Specifically, it presents a coordinate-enhanced channel importance metric to reflect the correlation between dimensional information and individual channel features, and it recycles the discarded points in PNN's sampling process and reconsiders their potentially-exclusive information to enhance the robustness of channel pruning. Experiments on various PNN architectures show that CP$^3$ constantly improves state-of-the-art 2D CNN pruning approaches on different point cloud tasks. For instance, our compressed PointNeXt-S on ScanObjectNN achieves an accuracy of 88.52% with a pruning rate of 57.8%, outperforming the baseline pruning methods with an accuracy gain of 1.94%.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.13097v1"
    ],
    "publication_venue": "Yaomin Huang and Ning Liu are with equal contributions. This paper has been accepted by CVPR 2023"
}