\subsection{Experimental Settings}
\para{Baseline models and datasets}
To demonstrate the effectiveness and generality of the proposed {\cpt}, we tested it on three different 3D tasks and five datasets with various PNNs and three recent advanced channel pruning methods.
The evaluated pruning methods include HRank~(\textit{2020})~\cite{lin2020hrank}, ResRep~(\textit{2021})~\cite{ding2021resrep}, and CHIP~(\textit{2021})~\cite{sui2021chip}.
For the \textit{classification task}, we chose the classical PointNet++~\cite{qi2017pointnet++} and PointNeXt~\cite{qian2022pointnext} models as the original networks and conducted experiments on ModelNet40~\cite{DBLP:conf/cvpr/WuSKYZTX15} and ScanObjectNN~\cite{DBLP:conf/iccv/UyPHNY19}. Specifically, for PointNeXt-S we tested two settings with widths of 32 and 64.
For the \textit{segmentation} task, we conducted experiments on S3DIS~\cite{DBLP:conf/cvpr/ArmeniSZJBFS16} with PointNeXt-B and PointNeXt-L~\cite{qian2022pointnext} as the original PNNs.
For the \textit{object detection} task, we pruned two point-based detectors (VoteNet~\cite{qi2019deep} and GroupFree3D~\cite{liu2021group}) on \mbox{SUN RGB-D}~\cite{DBLP:conf/cvpr/SongLX15} and ScanNetV2~\cite{DBLP:conf/cvpr/DaiCSHFN17}.


\para{Implementation details}
We conducted the classification and segmentation experiments with OpenPoints~\cite{qian2022pointnext} and the object detection experiments with MMdetection3D~\cite{mmdet3d2020}, all on NVIDIA P100 GPUs.
For a fair comparison, we used the same hyperparameter settings for each group of experiments.
%
We either 1) measured the parameter/FLOP reductions of the pruned networks with similar performance or 2) measured the performance of the pruned networks with a similar amount of parameter/FLOP reductions.
For all experiments, we reported the number of FLOPs (`GFLOPs') and parameters (`Params.'), as well as task-specific metrics to be described in each experiment.
More experimental results are available in the supplementary.


\subsection{Results on Classification}

\para{ModelNet40}
ModelNet40~\cite{DBLP:conf/cvpr/WuSKYZTX15} contains 9843 training and 2468 testing meshed CAD models belonging to 40 categories.
%
Following the standard practice~\cite{qi2017pointnet++}, we report the class-average accuracy~(mAcc) and the overall accuracy~(OA) on the testing set.
%
We compared the pruned networks directly by HRank, ResRep, and CHIP and those pruned by the three pruning methods with {\cpt}.
%
As shown in Tab.~\ref{modelnet40_sum_label},
our {\cpt} improved the performance of existing CNNs pruning methods for different PNNs with various pruning rates.
{\cpt} had higher accuracy scores with a similar (and mostly higher) pruning rates.
%
Notably, with the pruning rate of ~58\%, {\cpt} usually produced compact PNNs with even better accuracy scores than the original PNNs, which is difficult for pruning methods without {\cpt}.

\input{table/scannet_new.tex}
\para{ScanObjectNN}
We also conducted experiments on the ScanObjectNN benchmark~\cite{DBLP:conf/iccv/UyPHNY19}.
ScanObjectNN contains 15000 objects categorized into 15 classes with 2902 unique object instances in the real world.
As reported in Tab.~\ref{scanobject_sum_label}, {\cpt} surpassed existing CNN pruning methods directly appled to PNNs.
For example, comparing with the baseline pruning method HRank,
{\cpt} boosts the OA score of PointNet++ by 1.14\% (85.01$\rightarrow$86.15), 1.14\% (84.94$\rightarrow$86.08), and 0.59\% (84.03$\rightarrow$84.62) for the three different pruning rates.
Similarly, {\cpt} obtains much higher OA and mAcc scores than the three baselines with different pruning rates on PointNet++ and PointNeXt-S.
%
%
With the extensive experimental results on classification tasks, we show that {\cpt} surely improved the pruned network's performance to a distinct extent compared to direct applications of 2D CNN pruning methods.

\input{table/seg_s3dis.tex}

\input{table/votenet_scanet.tex}


\subsection{Results on Semantic Segmentation}
S3DIS~\cite{DBLP:conf/cvpr/ArmeniSZJBFS16} is a challenging benchmark composed of 6 large-scale indoor areas, 271 rooms, and 13 semantic categories in total.
Following a common protocol \cite{DBLP:conf/3dim/TchapmiCAGS17}, we evaluated the presented approach in Area-5, which means to test on Area-5 and to train on the rest.
For evaluation metrics, we used the mean classwise intersection over union~(mIoU), the mean classwise accuracy~(mAcc), and the overall pointwise accuracy~(OA).
As the segmentation task is relatively difficult and the segmentation network structures are relatively complex,
we pruned only the encoder part of the network and kept the original decoder part.
The results are presented in Tab.~\ref{seg_s3dis}.
As expected, the performance of the pruned networks degraded more from the original networks than those in the classification experiments, but the performance was acceptable. Meanwhile, in all cases, with our {\cpt}, PNNs have a higher accuracy at a higher pruning rate than without {\cpt}.
For example, for PointNeXt-B, comparing with directly applying CHIP, incorporating {\cpt} obtained much higher OA, mAcc and mIoU scores (1.2\% on OA and 0.3\% on mIoU).
The results about segmentation have well validated the generalization of {\cpt} to new and difficult tasks.

\input{table/votenet_sunrgbd.tex}


\subsection{Results on 3D Object Detection}
\subsubsection{Evaluation and Comparison of VoteNet}

Tabs.~\ref{tabl1_vsn}~and~\ref{table2_vsun} show the results of the pruned VoteNet models on the ScanNetV2 and SUN RGB-D datasets, respectively.
We evaluated the performance of our proposed method in terms of the mean average precision at IOU threshods of 0.25 and 0.50 (mAP@25 and mAP@50).

\para{ScanNetV2}
ScanNetV2~\cite{DBLP:conf/cvpr/DaiCSHFN17} is a richly annotated dataset of 3D reconstructed meshes of indoor scenes.
It contains about 1200 training examples collected from hundreds of different rooms and is annotated with semantic and instance segmentation for 18 object categories.
Tab.~\ref{tabl1_vsn} shows the results of directly applying ResRep and with {\cpt}.
As can be seen from the table, the accuracy of the 2D method directly applied to the 3D network decreased by a flops drop of about 60\%, while our method achieves 1.58\% and 0.65\% improvements at mAP@0.25 and mAP@0.5 with a drop rate of 58.13\% FLOPS.
When FLOPs drop to about 70\%, the accuracy of the direct porting CNNs pruning method works poorly, while the improvement of mAP@0.25 and mAP@0.5 of our method is 0.64\% and 0.12\%, respectively.

\para{SUN RGB-D}
The SUN RGB-D dataset~\cite{DBLP:conf/cvpr/SongLX15} consists of 10355 single-view indoor RGB-D images annotated with over 64000 3D bounding boxes and semantic labels for 37 categories.
We conducted experiments on SUN RGB-D with the same setup as those on ScanNetV2.
The findings are also similar to those in ScanNetV2.
It can be observed from Tab.~\ref{table2_vsun} that the accuracy of directly transplanted CNNs pruning method is reduced to some extent (mAP@0.25 reduced by 0.41) when FLOPs drop by 58.13\%, while {\cpt} improved the detection model by 0.32\% mAP@0.25 and 1.60\% mAP@0.5 with a FLOPs drop of 61.93\%.
Even when FLOPs drop to 70\%, our method's mAP@0.25 drop only 0.6\%, which is obviously better.


\input{table/gp3d_scannet.tex}

\subsubsection{Evaluation and Comparison on GroupFree3D}

We also conducted experiments on another point-based 3D detection model, GroupFree3D, on ScanNetV2.
Tab.~\ref{table3_gs} summarizes the pruning performance of our approach for GroupFree3D on the ScanNetV2 dataset.
When targeting a moderate compression ratio, our approach can achieve 32.15\% and 50.05\% storage and computation reductions, respectively, with a 0.64\% accuracy increase for mAP@0.25 over the baseline model. In the case of higher compression ratio, {\cpt} still achieves superior performance to other methods.
Specifically, the ResRep loses accuracy by 1.01\% mAP@0.25 when the parameters and flops drop by 30.14\% and 60.01\%, while
in our method, the accuracy increases 0.35\% for mAP@0.25.

\subsection{Ablation Studies}

We conducted ablation studies to validate the Coordinate-Enhancement~(CE) module and the Knowledge-Recycling~(KR) module in {\cpt}.
All results provided in the section are tested on ScanObjectNN with PointNeXt-S~(C=32) as the baseline and HRank as the pruning method.
We evaluated the pruned networks' performance with the FLOPs drop of 75\% and 90\%, and with/without the CE and KR modules to the pruning method.
From Tab.~\ref{ablation_new}, we find that
KR improves OA of 0.84\% and 2.33\% when pruning rate is 75\% and 90\%, respectively, while CE improves OA of 0.32\% and 1.77\% at 75\% and 90\% pruning rates, respectively.
Bringing the two modules together, the OA improvement is 1.84\% and 3.50\% at 75\% and 90\% pruning rates, respectively.
The ablation study results have validated the effectiveness of all designs in {\cpt}.

\input{table/ablation.tex}
