@inproceedings{hong2022depth,
  title={Depth-aware generative adversarial network for talking head video generation},
  author={Hong, Fa-Ting and Zhang, Longhao and Shen, Li and Xu, Dan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3397--3406},
  year={2022}
}

@inproceedings{guo2021ad,
  title={Ad-nerf: Audio driven neural radiance fields for talking head synthesis},
  author={Guo, Yudong and Chen, Keyu and Liang, Sen and Liu, Yong-Jin and Bao, Hujun and Zhang, Juyong},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={5784--5794},
  year={2021}
}

@inproceedings{zhang2021facial,
  title={Facial: Synthesizing dynamic talking face with implicit attribute learning},
  author={Zhang, Chenxu and Zhao, Yifan and Huang, Yifei and Zeng, Ming and Ni, Saifeng and Budagavi, Madhukar and Guo, Xiaohu},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={3867--3876},
  year={2021}
}

@inproceedings{thies2020neural,
  title={Neural voice puppetry: Audio-driven facial reenactment},
  author={Thies, Justus and Elgharib, Mohamed and Tewari, Ayush and Theobalt, Christian and Nie{\ss}ner, Matthias},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XVI 16},
  pages={716--731},
  year={2020},
  organization={Springer}
}

@inproceedings{zakharov2019few,
  title={Few-shot adversarial learning of realistic neural talking head models},
  author={Zakharov, Egor and Shysheya, Aliaksandra and Burkov, Egor and Lempitsky, Victor},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={9459--9468},
  year={2019}
}

@inproceedings{ji2021audio,
  title={Audio-driven emotional video portraits},
  author={Ji, Xinya and Zhou, Hang and Wang, Kaisiyuan and Wu, Wayne and Loy, Chen Change and Cao, Xun and Xu, Feng},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={14080--14089},
  year={2021}
}

@inproceedings{cudeiro2019capture,
  title={Capture, learning, and synthesis of 3D speaking styles},
  author={Cudeiro, Daniel and Bolkart, Timo and Laidlaw, Cassidy and Ranjan, Anurag and Black, Michael J},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10101--10111},
  year={2019}
}

@inproceedings{fan2022faceformer,
  title={Faceformer: Speech-driven 3d facial animation with transformers},
  author={Fan, Yingruo and Lin, Zhaojiang and Saito, Jun and Wang, Wenping and Komura, Taku},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18770--18780},
  year={2022}
}

@inproceedings{richard2021meshtalk,
  title={MeshTalk: 3D Face Animation from Speech using Cross-Modality Disentanglement},
  author={Richard, Alexander and Zollh{\"o}fer, Michael and Wen, Yandong and de la Torre, Fernando and Sheikh, Yaser},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages={1153--1162},
  year={2021},
  organization={IEEE}
}

@inproceedings{papantoniou2022neural,
  title={Neural Emotion Director: Speech-Preserving Semantic Control of Facial Expressions in" In-the-Wild" Videos},
  author={Papantoniou, Foivos Paraperas and Filntisis, Panagiotis P and Maragos, Petros and Roussos, Anastasios},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18781--18790},
  year={2022}
}

@article{chang2022disentangling,
  title={Disentangling audio content and emotion with adaptive instance normalization for expressive facial animation synthesis},
  author={Chang, Che-Jui and Zhao, Long and Zhang, Sen and Kapadia, Mubbasir},
  journal={Computer Animation and Virtual Worlds},
  volume={33},
  number={3-4},
  pages={e2076},
  year={2022},
  publisher={Wiley Online Library}
}

@article{sun2022continuously,
  title={Continuously Controllable Facial Expression Editing in Talking Face Videos},
  author={Sun, Zhiyao and Wen, Yu-Hui and Lv, Tian and Sun, Yanan and Zhang, Ziyang and Wang, Yaoyuan and Liu, Yong-Jin},
  journal={arXiv preprint arXiv:2209.08289},
  year={2022}
}

@article{karras2017audio,
  title={Audio-driven facial animation by joint end-to-end learning of pose and emotion},
  author={Karras, Tero and Aila, Timo and Laine, Samuli and Herva, Antti and Lehtinen, Jaakko},
  journal={ACM Transactions on Graphics (TOG)},
  volume={36},
  number={4},
  pages={1--12},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@inproceedings{lahiri2021lipsync3d,
  title={Lipsync3d: Data-efficient learning of personalized 3d talking faces from video using pose and lighting normalization},
  author={Lahiri, Avisek and Kwatra, Vivek and Frueh, Christian and Lewis, John and Bregler, Chris},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2755--2764},
  year={2021}
}

@article{lewis2014practice,
  title={Practice and theory of blendshape facial models.},
  author={Lewis, John P and Anjyo, Ken and Rhee, Taehyun and Zhang, Mengjie and Pighin, Frederic H and Deng, Zhigang},
  journal={Eurographics (State of the Art Reports)},
  volume={1},
  number={8},
  pages={2},
  year={2014}
}

@inproceedings{wang2020mead,
  title={Mead: A large-scale audio-visual dataset for emotional talking-face generation},
  author={Wang, Kaisiyuan and Wu, Qianyi and Song, Linsen and Yang, Zhuoqian and Wu, Wayne and Qian, Chen and He, Ran and Qiao, Yu and Loy, Chen Change},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXI},
  pages={700--717},
  year={2020},
  organization={Springer}
}

@article{li2017learning,
  title={Learning a model of facial shape and expression from 4D scans.},
  author={Li, Tianye and Bolkart, Timo and Black, Michael J and Li, Hao and Romero, Javier},
  journal={ACM Trans. Graph.},
  volume={36},
  number={6},
  pages={194--1},
  year={2017}
}

@article{sheng2022deep,
  title={Deep Learning for Visual Speech Analysis: A Survey},
  author={Sheng, Changchong and Kuang, Gangyao and Bai, Liang and Hou, Chenping and Guo, Yulan and Xu, Xin and Pietik{\"a}inen, Matti and Liu, Li},
  journal={arXiv preprint arXiv:2205.10839},
  year={2022}
}

@article{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{zhou2018visemenet,
  title={Visemenet: Audio-driven animator-centric speech animation},
  author={Zhou, Yang and Xu, Zhan and Landreth, Chris and Kalogerakis, Evangelos and Maji, Subhransu and Singh, Karan},
  journal={ACM Transactions on Graphics (TOG)},
  volume={37},
  number={4},
  pages={1--10},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@ARTICLE{8805181,
  author={Khalil, Ruhul Amin and Jones, Edward and Babar, Mohammad Inayatullah and Jan, Tariqullah and Zafar, Mohammad Haseeb and Alhussain, Thamer},
  journal={IEEE Access}, 
  title={Speech Emotion Recognition Using Deep Learning Techniques: A Review}, 
  year={2019},
  volume={7},
  number={},
  pages={117327-117345},
  doi={10.1109/ACCESS.2019.2936124}}

@article{schuller2018speech,
  title={Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends},
  author={Schuller, Bj{\"o}rn W},
  journal={Communications of the ACM},
  volume={61},
  number={5},
  pages={90--99},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{hossain2019emotion,
  title={Emotion recognition using deep learning approach from audio--visual emotional big data},
  author={Hossain, M Shamim and Muhammad, Ghulam},
  journal={Information Fusion},
  volume={49},
  pages={69--78},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{kwon2003emotion,
  title={Emotion recognition by speech signals},
  author={Kwon, Oh-Wook and Chan, Kwokleung and Hao, Jiucang and Lee, Te-Won},
  booktitle={Eighth European conference on speech communication and technology},
  year={2003}
}

@inproceedings{mekruksavanich2020negative,
  title={Negative emotion recognition using deep learning for thai language},
  author={Mekruksavanich, Sakorn and Jitpattanakul, Anuchit and Hnoohom, Narit},
  booktitle={2020 joint international conference on digital arts, media and technology with ECTI northern section conference on electrical, electronics, computer and telecommunications engineering (ECTI DAMT \& NCON)},
  pages={71--74},
  year={2020},
  organization={IEEE}
}

@inproceedings{yenigalla2018speech,
  title={Speech Emotion Recognition Using Spectrogram \& Phoneme Embedding.},
  author={Yenigalla, Promod and Kumar, Abhay and Tripathi, Suraj and Singh, Chirag and Kar, Sibsambhu and Vepa, Jithendra},
  booktitle={Interspeech},
  volume={2018},
  pages={3688--3692},
  year={2018}
}

@INPROCEEDINGS{9383526,
  author={Zhou, Kun and Sisman, Berrak and Li, Haizhou},
  booktitle={2021 IEEE Spoken Language Technology Workshop (SLT)}, 
  title={Vaw-Gan For Disentanglement And Recomposition Of Emotional Elements In Speech}, 
  year={2021},
  volume={},
  number={},
  pages={415-422},
  doi={10.1109/SLT48900.2021.9383526}}

  @article{zhang2022iemotts,
  title={iEmoTTS: Toward Robust Cross-Speaker Emotion Transfer and Control for Speech Synthesis based on Disentanglement between Prosody and Timbre},
  author={Zhang, Guangyan and Qin, Ying and Zhang, Wenjie and Wu, Jialun and Li, Mei and Gai, Yutao and Jiang, Feijun and Lee, Tan},
  journal={arXiv preprint arXiv:2206.14866},
  year={2022}
}

@inproceedings{strizhkova2021emotion,
  title={Emotion editing in head reenactment videos using latent space manipulation},
  author={Strizhkova, Valeriya and Wang, Yaohui and Anghelone, David and Yang, Di and Dantcheva, Antitza and Br{\'e}mond, Fran{\c{c}}ois},
  booktitle={2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)},
  pages={1--8},
  year={2021},
  organization={IEEE}
}

@article{livingstone2018ryerson,
  title={The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English},
  author={Livingstone, Steven R and Russo, Frank A},
  journal={PloS one},
  volume={13},
  number={5},
  pages={e0196391},
  year={2018},
  publisher={Public Library of Science}
}

@inproceedings{zhang2021flow,
  title={Flow-Guided One-Shot Talking Face Generation With a High-Resolution Audio-Visual Dataset},
  author={Zhang, Zhimeng and Li, Lincheng and Ding, Yu and Fan, Changjie},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3661--3670},
  year={2021}
}

@article{conneau2020unsupervised,
  title={Unsupervised cross-lingual representation learning for speech recognition},
  author={Conneau, Alexis and Baevski, Alexei and Collobert, Ronan and Mohamed, Abdelrahman and Auli, Michael},
  journal={arXiv preprint arXiv:2006.13979},
  year={2020}
}

@article{Kingma2014AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2014},
  volume={abs/1412.6980}
}

@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}

@article{pouyanfar2018survey,
  title={A survey on deep learning: Algorithms, techniques, and applications},
  author={Pouyanfar, Samira and Sadiq, Saad and Yan, Yilin and Tian, Haiman and Tao, Yudong and Reyes, Maria Presa and Shyu, Mei-Ling and Chen, Shu-Ching and Iyengar, Sundaraja S},
  journal={ACM Computing Surveys (CSUR)},
  volume={51},
  number={5},
  pages={1--36},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{muda2010voice,
  title={Voice recognition algorithms using mel frequency cepstral coefficient (MFCC) and dynamic time warping (DTW) techniques},
  author={Muda, Lindasalwa and Begam, Mumtaj and Elamvazuthi, Irraivan},
  journal={arXiv preprint arXiv:1003.4083},
  year={2010}
}

@inproceedings{tran2017disentangled,
  title={Disentangled representation learning gan for pose-invariant face recognition},
  author={Tran, Luan and Yin, Xi and Liu, Xiaoming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1415--1424},
  year={2017}
}

@article{van2008visualizing,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  year={2008}
}

@article{taylor2017deep,
  title={A deep learning approach for generalized speech animation},
  author={Taylor, Sarah and Kim, Taehwan and Yue, Yisong and Mahler, Moshe and Krahe, James and Rodriguez, Anastasio Garcia and Hodgins, Jessica and Matthews, Iain},
  journal={ACM Transactions On Graphics (TOG)},
  volume={36},
  number={4},
  pages={1--11},
  year={2017},
  publisher={ACM New York, NY, USA}
}


@inproceedings{pham2017speech,
  title={Speech-driven 3D facial animation with implicit emotional awareness: A deep learning approach},
  author={Pham, Hai X and Cheung, Samuel and Pavlovic, Vladimir},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition workshops},
  pages={80--88},
  year={2017}
}

@article{makarainen2014exaggerating,
  title={Exaggerating facial expressions: A way to intensify emotion or a way to the uncanny valley?},
  author={M{\"a}k{\"a}r{\"a}inen, Meeri and K{\"a}tsyri, Jari and Takala, Tapio},
  journal={Cognitive Computation},
  volume={6},
  pages={708--721},
  year={2014},
  publisher={Springer}
}

@inproceedings{pawaskar2013expression,
  title={Expression transfer: A system to build 3D blend shapes for facial animation},
  author={Pawaskar, Chandan and Ma, Wan-Chun and Carnegie, Kieran and Lewis, John P and Rhee, Taehyun},
  booktitle={2013 28th International Conference on Image and Vision Computing New Zealand (IVCNZ 2013)},
  pages={154--159},
  year={2013},
  organization={IEEE}
}

@inproceedings{chen2019hierarchical,
  title={Hierarchical cross-modal talking face generation with dynamic pixel-wise loss},
  author={Chen, Lele and Maddox, Ross K and Duan, Zhiyao and Xu, Chenliang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={7832--7841},
  year={2019}
}

@inproceedings{mittal2020animating,
  title={Animating face using disentangled audio representations},
  author={Mittal, Gaurav and Wang, Baoyuan},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={3290--3298},
  year={2020}
}

@inproceedings{schuller2003hidden,
  title={Hidden Markov model-based speech emotion recognition},
  author={Schuller, Bj{\"o}rn and Rigoll, Gerhard and Lang, Manfred},
  booktitle={2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP'03).},
  volume={2},
  pages={II--1},
  year={2003},
  organization={Ieee}
}

@article{muller2007dynamic,
  title={Dynamic time warping},
  author={M{\"u}ller, Meinard},
  journal={Information retrieval for music and motion},
  pages={69--84},
  year={2007},
  publisher={Springer}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{vogt2008automatic,
  title={Automatic recognition of emotions from speech: a review of the literature and recommendations for practical realisation},
  author={Vogt, Thurid and Andr{\'e}, Elisabeth and Wagner, Johannes},
  journal={Affect and Emotion in Human-Computer Interaction: From Theory to Applications},
  pages={75--91},
  year={2008},
  publisher={Springer}
}

@article{cao2005expressive,
  title={Expressive speech-driven facial animation},
  author={Cao, Yong and Tien, Wen C and Faloutsos, Petros and Pighin, Fr{\'e}d{\'e}ric},
  journal={ACM Transactions on Graphics (TOG)},
  volume={24},
  number={4},
  pages={1283--1302},
  year={2005},
  publisher={ACM New York, NY, USA}
}

@article{chai2022speech,
  title={Speech-driven facial animation with spectral gathering and temporal attention},
  author={Chai, Yujin and Weng, Yanlin and Wang, Lvdi and Zhou, Kun},
  journal={Frontiers of Computer Science},
  volume={16},
  pages={1--10},
  year={2022},
  publisher={Springer}
}

@article{fan2022joint,
  title={Joint audio-text model for expressive speech-driven 3d facial animation},
  author={Fan, Yingruo and Lin, Zhaojiang and Saito, Jun and Wang, Wenping and Komura, Taku},
  journal={Proceedings of the ACM on Computer Graphics and Interactive Techniques},
  volume={5},
  number={1},
  pages={1--15},
  year={2022},
  publisher={ACM New York, NY, USA}
}

@article{liu2021geometry,
  title={Geometry-guided dense perspective network for speech-driven facial animation},
  author={Liu, Jingying and Hui, Binyuan and Li, Kun and Liu, Yunke and Lai, Yu-Kun and Zhang, Yuxiang and Liu, Yebin and Yang, Jingyu},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  volume={28},
  number={12},
  pages={4873--4886},
  year={2021},
  publisher={IEEE}
}

@inproceedings{hussen2020modality,
  title={Modality dropout for improved performance-driven talking faces},
  author={Hussen Abdelaziz, Ahmed and Theobald, Barry-John and Dixon, Paul and Knothe, Reinhard and Apostoloff, Nicholas and Kajareker, Sachin},
  booktitle={Proceedings of the 2020 International Conference on Multimodal Interaction},
  pages={378--386},
  year={2020}
}

@inproceedings{chen2022transformer,
  title={Transformer-s2a: Robust and efficient speech-to-animation},
  author={Chen, Liyang and Wu, Zhiyong and Ling, Jun and Li, Runnan and Tan, Xu and Zhao, Sheng},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7247--7251},
  year={2022},
  organization={IEEE}
}

@inproceedings{tian2019audio2face,
  title={Audio2face: Generating speech/face animation from single audio with attention-based bidirectional lstm networks},
  author={Tian, Guanzhong and Yuan, Yi and Liu, Yong},
  booktitle={2019 IEEE international conference on Multimedia \& Expo Workshops (ICMEW)},
  pages={366--371},
  year={2019},
  organization={IEEE}
}

@inproceedings{huang2014speech,
  title={Speech emotion recognition using CNN},
  author={Huang, Zhengwei and Dong, Ming and Mao, Qirong and Zhan, Yongzhao},
  booktitle={Proceedings of the 22nd ACM international conference on Multimedia},
  pages={801--804},
  year={2014}
}

@article{nwe2003speech,
  title={Speech emotion recognition using hidden Markov models},
  author={Nwe, Tin Lay and Foo, Say Wei and De Silva, Liyanage C},
  journal={Speech communication},
  volume={41},
  number={4},
  pages={603--623},
  year={2003},
  publisher={Elsevier}
}

@article{jain2020speech,
  title={Speech emotion recognition using support vector machine},
  author={Jain, Manas and Narayan, Shruthi and Balaji, Pratibha and Bhowmick, Abhijit and Muthu, Rajesh Kumar and others},
  journal={arXiv preprint arXiv:2002.07590},
  year={2020}
}

@inproceedings{lea2016temporal,
  title={Temporal convolutional networks: A unified approach to action segmentation},
  author={Lea, Colin and Vidal, Rene and Reiter, Austin and Hager, Gregory D},
  booktitle={Computer Vision--ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14},
  pages={47--54},
  year={2016},
  organization={Springer}
}

@article{wohlgenannt2020virtual,
  title={Virtual reality},
  author={Wohlgenannt, Isabell and Simons, Alexander and Stieglitz, Stefan},
  journal={Business \& Information Systems Engineering},
  volume={62},
  pages={455--461},
  year={2020},
  publisher={Springer}
}

@article{ping2013computer,
  title={Computer facial animation: A review},
  author={Ping, Heng Yu and Abdullah, Lili Nurliyana and Sulaiman, Puteri Suhaiza and Halin, Alfian Abdul},
  journal={International Journal of Computer Theory and Engineering},
  volume={5},
  number={4},
  pages={658},
  year={2013},
  publisher={IACSIT Press}
}

@article{liu2009analysis,
  title={An analysis of the current and future state of 3D facial animation techniques and systems},
  author={Liu, Chen},
  year={2009},
  publisher={Simon Fraser University}
}