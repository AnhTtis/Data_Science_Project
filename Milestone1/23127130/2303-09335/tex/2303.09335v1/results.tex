\section{Results}

\subsection{Predictive performance}
\label{lbl:rendimiento}

For a given periodogram, both the classical FAP computation and our CNN output a number.  In the former, it is the log probability that a peak with at least as much power as the largest peak observed in the periodogram appears under the null hypothesis (i.e. only noise --albeit correlated-- is present in the time series). In the latter, it is a value between 0 and 1 that can be interpreted as the probability that the periodogram contains a \emph{bona fide} planetary signal.%expresses that the closer to 1 it is, the more it should be considered as a planet than if it were close to 0. 
Both implementations require the definition of a threshold value if they are to be used for taking decisions, such as defining whether a planet is present in the data or not. However, note that the FAP method is only intended to assess the significance of a signal, while the neural network also aims at providing a classification of the nature of the signal.

Before setting this threshold, an analysis of the overall effectiveness of each method can be made and compared with each other, using \textit{precision-recall} curves. From the definitions seen in the previous section, it is trivial to see that both metrics vary between 0 and 1, the intuition behind precision is that it seeks to minimize false positives and recall does so with false negatives.

% Todo este parrafo tal vez es un poco demasiado detalle
%
%It is convenient to observe both results at the same time to evaluate the model. If it has high recall but low precision, a large fraction of the planets in the dataset will be found, but many false positives will also be identified. If it has high precision and little recall, it is the opposite: the algorithm will provide few good-quality detections. An ideal scenario would be the one with both high values, that is, {\bf the detection of a high fraction of true planets with few false positives}.

%It is convenient to observe both results at the same time to evaluate the model, an ideal scenario would be the one with both high values, that is, {\bf the detection of a high fraction of true planets with few false positives}.

The precision-recall (PR) curve (Fig. \ref{fig:pr-curve}) shows the tradeoff between these metrics. This curves are constructed by varying the threshold for detection and computing the metrics at each value. Ideally, the curves should pass as close as possible to the upper right edge of the plot, that represents a system with perfect recall and no false positives. Therefore, the greater the area under this curve, the better the overall performance of the algorithm. The area under the curve of the PR curve (PR-AUC) is useful to compare two different solutions to the same problem.

In Figure \ref{fig:pr-curve} we present the PR curve and PR-AUC metric for the classical FAP method over the periodograms of \textit{set 2}. We see that the network performs systematically better than the traditional method. The AUC metrics are 0.90 and 0.96 for the FAP and \texttt{ExoplANNET} methods, respectively.

\begin{figure}[H]
	\centering
	\resizebox{\hsize}{!}{\includegraphics[width=0.7\linewidth]{PR-curve}}
	\caption{Precision-Recall Curve and AUC of both methods over the periodograms of \textit{set 2}.}
	\label{fig:pr-curve}
\end{figure}
%https://machinelearningmastery.com/fbeta-measure-for-machine-learning/#:~:text=are%20as%20follows%3A-,F0.,precision%2C%20more%20weight%20on%20recall
%https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/

\begin{figure*}
	\includegraphics[width=1\linewidth]{casos_x_prob}
	\caption{Distribution of output values over the periodograms of set 2 (probabilities and FAP values for the CNN and classical analysis, respectively). For each algorithm the distribution of data labelled as planets and as noise are shown with different line styles. As there are 15587 periodograms without planets for the 4413 planetary periodograms, the distributions are normalised for easier comparison.}
	\label{fig:casosxprob}
\end{figure*}

In  Figure \ref{fig:casosxprob} we present the distribution of the output values (i.e. FAP values and planet probability) for the periodograms of set 2. As expected, periodograms for which the methods assign a probability close to 1 are predominately originated by real planets. Inversely, outputs very close to zero are mostly associated with periodograms containing no planets. %Otherwise, the aspect of the histograms in Figure \ref{fig:casosxprob} are quite different. 
On the other hand, the value of the x-axis where both histograms cross is much larger for the FAP method.

\subsection{Defining the Threshold for planet detection}
\label{lbl:thrsearch}

Once the general performance has been analyzed, we now turn to the topic of finding and adequate threshold for the neural network model. As was already mentioned we considered a log threshold of -1.3, i.e. a 0.05 probability for the FAP. The number of false positives and false negatives produced by the FAP method using this threshold were computed, and used to find the range of probability thresholds for the network that give a smaller number of both false positives and false negatives.% (see Fig.~\ref{fig:test}).

In Figure \ref{fig:test}, the number of false positives and false negatives are shown as a function of the threshold used for the network. The values produced by the FAP method are marked as horizontal dashed lines. This defines a range between 0.69 and 0.77 within which the network obtains better results than the FAP. Any threshold value greater than 0.69 will give fewer false positives, and those less than 0.77 will give fewer false negatives. The network threshold was then set to 0.77, in order to produce the most reliable detection mechanism in terms of false positives, with a similar recall as the traditional method.

%in the precision-recall curve of the FAP, it could be found which threshold value of the network gives a better result in precision, but it was preferred to show it in a separate curve, as it was considered clearer.

%Using this threshold with the FAP method, we found 117 false positives and 818 false negatives. As it is intended that the network produce the fewer possible false positives (without increasing the number of false negatives), we then searched for the values of the threshold of the CNN that produce a recall at least as high as the one found for the FAP method, but with a smaller number of false positives, i.e. a higher precision.


\begin{figure*}
	\begin{subfigure}{.49\textwidth}
		\includegraphics[width=1\linewidth]{fp_red_fap}
		%\captionsetup{labelformat=empty}
		%\label{fig:sub1}
		\caption{False positives as a function of the threshold.}
	\end{subfigure}
	\begin{subfigure}{.49\textwidth}
		\includegraphics[width=1\linewidth]{fn_red_fap}
		%\captionsetup{labelformat=empty}
		\caption{False negatives as a function of the threshold.}
		%		\label{fig:rv_pl}
	\end{subfigure}
        \hfill	
        \caption{Lower and upper limit of the threshold where the network has both fewer false positives and false negatives than the traditional method. The dashed horizontal lines show the number of wrong detections with the FAP method, and the vertical segment aids in the identification of the corresponding probability threshold.}
	\label{fig:test}
\end{figure*}


\subsection{Performance on test set}
\label{lbl:performanceontest}

The last performance evaluation was conducted on set 3. Since set 2 was used to identify the threshold range where the network performs better than the traditional method, there is a concern that the choice of the threshold may have been influenced by the characteristics this particular set, leading to overfitting. To address this, we evaluated the network's performance on the yet-unseen set 3.

The \textit{confusion matrices} for both methods are shown in tables \ref{tbl:mc_1} and \ref{tbl:mc_2}. The rows show the true labels of positive and negative cases in the set and the columns show the predicted label.

\begin{table}
    \centering
    \begin{subtable}[c]{.5\textwidth}
	\centering
	\parbox{.5\linewidth}{
%		\centering
		\begin{tabular}{lllll}
			& \multicolumn{4}{c}{Prediction}                                                \\ \cline{3-4}
			& \multicolumn{1}{l|}{}  & \multicolumn{1}{c|}{0}     & \multicolumn{1}{c|}{1}    &  \\ \cline{2-4}
			\multicolumn{1}{l|}{Real Value} & \multicolumn{1}{l|}{0} & \multicolumn{1}{l|}{\textcolor{green}{15309}} & \multicolumn{1}{l|}{\textcolor{red}{277}}  &  \\ \cline{2-4}
			\multicolumn{1}{l|}{}           & \multicolumn{1}{l|}{1} & \multicolumn{1}{l|}{\textcolor{red}{1672}}  & \multicolumn{1}{l|}{\textcolor{green}{2742}} &  \\ \cline{2-4}
		\end{tabular}
		\caption{Confusion matrix FAP}
		\label{tbl:mc_1}
	}
     \end{subtable}
    \hfill
     \begin{subtable}[c]{.5\textwidth}
     	\centering
	\parbox{.5\linewidth}{
		\centering
		\begin{tabular}{lllll}
			& \multicolumn{4}{c}{Prediction}                                                \\ \cline{3-4}
			& \multicolumn{1}{l|}{}  & \multicolumn{1}{c|}{0}     & \multicolumn{1}{c|}{1}    &  \\ \cline{2-4}
			\multicolumn{1}{l|}{Real Value} & \multicolumn{1}{l|}{0} & \multicolumn{1}{l|}{\textcolor{green}{15388}} & \multicolumn{1}{l|}{\textcolor{red}{198}}  &  \\ \cline{2-4}
			\multicolumn{1}{l|}{}           & \multicolumn{1}{l|}{1} & \multicolumn{1}{l|}{\textcolor{red}{1674}}  & \multicolumn{1}{l|}{\textcolor{green}{2740}} &  \\ \cline{2-4}
		\end{tabular}
		\caption{Confusion matrix \texttt{ExoplANNET}.}
		\label{tbl:mc_2}
	}
      \end{subtable}
		\caption{Confusion matrices \texttt{ExoplANNET} and FAP. The rows show the true labels of positive and negative cases in the set and the columns show the predicted label}
\end{table}

The results obtained in this evaluation are consistent with those from set 2. Compared to the traditional method, the number of false positives is decreased by 28.5\%, while only two additional false negatives were identified. This is reflected in the comparison in table \ref{tbl:PE_FAP_Red}, where the recall remains almost unchanged but the precision is increased.


\begin{table}[H]
	\centering
	\begin{tabular}{l|c|c|}
		\cline{2-3}
		& FAP   & \texttt{ExoplANNET}   \\ \hline
		\multicolumn{1}{|l|}{Precision}     & 0.908 & 0.932 \\ \hline
		\multicolumn{1}{|l|}{Recall} & 0.621 & 0.620 \\ \hline
	\end{tabular}
	\caption{\texttt{ExoplANNET} and FAP Precision/Recall.}
	\label{tbl:PE_FAP_Red}
\end{table}

\subsection{Execution time}

It was shown in the introduction that, depending on the result, the calculation of the FAP can be done using the Baluev formula or by a Monte Carlo-type algorithm. The analytical formula is not very computationally expensive but assumes white noise. On the other hand, running the Monte Carlo computation on a large data set of radial velocities and computing their corresponding periodograms can take up considerable time, specially when red noise has to be simulated. All this means that the average time required to calculate a FAP value can be considerable.

The case of the network is quite different. A trained network is nothing more than a large matrix and making a prediction is simply taking the input and executing basic operations such as addition and multiplication to return a value. Because of this simple fact, the network should be much faster \footnote{Just to mention another advantage, the network occupies only 1.7kB in memory.}.

To compare the execution times between the methods, we need to consider the \textit{Wall-Time} and the \textit{Proc-Time}. The latter is the time used exclusively to execute a process. It does not take into account things like disk access or other input/output operations, operating system interruptions, time used in other processes, etc. In contrast, \textit{Wall-Time} is the clock time it takes to execute something. It is simply a stopwatch, so if the process is waiting to read data or the system is very busy with several processes, it will add to this time.

On the other hand, if you want to evaluate the time of small programs or pieces of code, it is usually convenient to see only the \textit{proc-time}. The code for generating the periodograms is not parallel (at least not in its current implementation), so these two times should be very similar.

The methods were executed several times, and the resulting running times were averaged. For the FAP method, we perform 100 executions (note that if Baluev's result is good enough, the Monte Carlo simulations are not necessary). For the network method, we generated predictions for $10^6$ periodograms \footnote{The executions were on a desktop computer with a 2.3 GHz micro Intel Core I5 with 8Gb of RAM.}. The results are presented in table \ref{tbl:time_FAP_red} where we report the time taken to analyse a single periodogram, expressed in seconds.


\begin{table}[H]
	\centering
	\begin{tabular}{l|c|c|}
		\cline{2-3}
		& FAP   & \texttt{ExoplANNET}      \\ \hline
		\multicolumn{1}{|l|}{\textit{Wall-Time}} & 425.97 s  & 0.0011 s\\ \hline
		\multicolumn{1}{|l|}{\textit{Proc-Time}} & 427.69 s   & 0.0034 s  \\ \hline
	\end{tabular}
	\caption{Execution times in seconds for the analysis of a single periodogram with \texttt{ExoplANNET} and FAP methods.}
	\label{tbl:time_FAP_red}
\end{table}

The difference is considerable. Focusing only on \textit{proc-time}, the network is 125,791 times faster, that is, a difference of 5 orders of magnitude. The execution time required to calculate the FAP can seem excessively long, but we remind the reader that the Monte Carlo simulations performed include red noise, in an attempt to make the method as reliable as possible.

Note however that the code to calculate the FAP is not optimized. The noise simulations and the computation of the periodograms are severely affecting performance. These parts of the computation could be optimized, and the Monte Carlo procedure could be parallelized. Howver, the reality is that the advantage that the network brings is too great for any realistic optimization to bring it even close to these results.

\subsection{Virtual Astronomer}
\label{lbl:astro_virtual}

Having compared the performance of \texttt{ExoplANNET} with the traditional method using the FAP, we now seek to compare how both implementations would work in the full process of sequentially finding planets in a time series.

The procedure often follows these lines: when a significant peak is found  (i.e. its $log_{10}(FAP) \leq -1.3$), a sinusoidal signal at that frequency is fit and removed from the data. A new periodogram is computed from the residuals, and the process is repeated until the largest peak in a periodogram is not deemed significant. The pseudocode of this procedure --called \textit{virtual astronomer}-- is shown in Algorithm \ref{alg:virt_astro}. This procedure is often the first step in analysing a radial velocity time series, although there are much more sophisticated techniques to evaluate the significance of a signal in a dataset \citep[e.g.][]{Diaz2016TheHS}. 

\begin{algorithm}
	\caption{virtual astronomer}\label{alg:virt_astro}
	\begin{algorithmic}[1]
		\Procedure{virt\_astro}{$rad\_vel$} 
		\State$pg \gets $gen\_periodogram($rad\_vel$) 
		\State$max\_peak \gets$ get\_max\_peak($pg$) 
		\State$planets=[]$ 
		
		\While{planet($max\_peak, pg$)} 
		\State$planets$.append$(max\_peak)$
		\State$new\_rad\_vel \gets $remove($max\_peak, rad\_vel$)
		\State$pg\gets$ gen\_periodogram$(new\_rad\_vel)$ 
		\State$max\_peak \gets$ get\_max\_peak$(pg)$ 
		\EndWhile
		
		\State \textbf{return} $planets$\
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

%\subsection{Full Method}

So far, all the tests were performed on individual periodograms, which may have contained any number of simulated planets between zero and four, but how the methods behave when used as part of the \textit{virtual astronomer} was not studied. 

We performed an analysis using the threshold values determined in the previous section. Both implementations of the \textit{virtual astronomer}, one using the classical FAP computation and another one using the CNN predictions, were applied to each of the 5000 stars in the third data set. The idea is to test how many planets the methods actually find in a complete, more realistic use case. In particular, we are interested in exploring their behaviour with respect to false positives. 

While it is clear from the above analysis that \texttt{ExoplANNET} performs better in terms of false positives, it is not so clear that this would also be the case when using it with the \textit{virtual astronomer}. On the one hand the \textit{virtual astronomer} with the FAP method labels more noise peaks as planets, which means that on average the execution will proceed over more iterations than the implementation using \texttt{ExoplANNET}. This may increase the chances that in those additional iterations, the method detects planets that \texttt{ExoplANNET} could not find because it was better at identifying the first dominant non-planetary peak, which stopped of the algorithm. On the other hand, this could also mean more false positives are identified with the FAP method.

To evaluate the performance of the \textit{virtual astronomer} using each detection method, the numbers of false negatives and false positives were compared. In figure \ref{fig:fp_virt_astro} we compare the performances as a function of the actual number of planets in the simulated system. 
The \textit{virtual astronomer} using \texttt{ExoplANNET} yields a lower number of false positives, compared to the implementation with the FAP method. Additionally, the number remains approximately constant over systems with different number of planets (\ref{fig:fp_virt_astro}). On the other hand, the results using the FAP exhibit a larger difference for stars with different number of planets.

% The stars were separated by number of planets (there are 1000 in each category) and the two types of errors were counted for both implementations of the full method. In figures \ref{fig:fp_virt_astro}. and \ref{fig:fnm_virt_astro}. the false positives and false negatives of the runs are shown.

	\begin{figure}
		\centering
		\includegraphics[width=1\linewidth]{../img/histo_fp}
		\caption{False positives of the \textit{virtual astronomer} with both implementations.}
		\label{fig:fp_virt_astro}
	\end{figure}
	


%\begin{figure}[!ht]
%		\centering
%		\includegraphics[width=1\linewidth]{../img/histo_fn}
%		\caption{False negatives of the \textit{virtual astronomer} with both implementations.}
%		\label{fig:fnm_virt_astro}
%	\end{figure}
	

%The numbers of false negatives in both implementations are very similar, although, in three of the four cases that matter, the method with the network has a few more. It is probably possible to adjust the threshold but, looking at the results in the graph above, it would affect the false positives in the stars with four planets and would probably give worse than the method with the traditional implementation.

To further evaluate both implementations, the numbers of detected planets were compared to the maximum possible number under flawless decision making, i.e. the maximum number of planets that could be found by the \textit{virtual astronomer} if it had access to the real labels of the peaks. Remember that the fact that a star has \textit{N} planets does not mean that all those planets will be reached by the \textit{virtual astronomer}. If at a given step the peak corresponding to the planet is not the largest peak in the periodogram, the \textit{virtual astronomer} will fail to identify it, and the process will stop. The results are shown in Figure \ref{fig7}.

\begin{figure*} 
%\centering
%\resizebox{\hsize}{!}
	\begin{subfigure}{.50\textwidth}
 	    \centering
		\includegraphics[width=1\linewidth]{img/full_fap_comp_1} 
		\subcaption*{(a) 1000 stars with 1 planets.} 
		\includegraphics[width=1\linewidth]{img/full_fap_comp_3}
		\subcaption*{(c) 1000 stars with 3 planets.} 
	\end{subfigure}
	\begin{subfigure}{.50\textwidth}
           \centering	
		\includegraphics[width=1\linewidth]{img/full_fap_comp_2} 
		\subcaption*{(b) 1000 stars with 2 planets.} 
		\includegraphics[width=1\linewidth]{img/full_fap_comp_4}
		\subcaption*{(d) 1000 stars with 4 planets.} 
 	\end{subfigure}
 \caption{Detections of both implementations of the \textit{virtual astronomer}, compared to the detections provided by the optimal method. The number of planets detected by the optimal method is marked by a dashed line and indicated in red on the vertical axis. Each panel corresponds to systems with a given number of companions. The bars present the true positive (i.e. detected planets, green), false negatives (blue) and planets that were not detected (i.e. their peak was not evaluated) whereas the optimal \textit{virtual astronomer} does. Note that even using the optimal method, many planets are missed; the numbers of planets present are much larger than the number of detected planets, even with the optimal method.}
	\label{fig7} 
\end{figure*}

 In general, the \textit{virtual astronomer} using the FAP method permits detecting more planets in the system. However, these extra planets come at the price of a larger number of false positives (Fig.~\ref{fig:fp_virt_astro}). Similarly, the \textit{virtual astronomer} using the FAP method exhibits systematically less false negatives, but only by a very small amount. Also, with the exception of the data with four planets, the FAP methods also has less missed planets, i.e. planets that are not even evaluated by the method. This was expected because of the larger number of iterations brought forth by a larger false positive rate. Overall, the FAP method is more inclined to provide a positive classification, which brings more planets but also more false positives.

Finally, to provide a global view, we computed the accuracy and completeness metrics of each implementation (see Table~\ref{mat:virt_ast_pr}). Despite the difference in the number of periodogram evaluations with each implementation, the precision and completeness values maintain the relationship they had when they were based on single periodogram evaluations. Globally, the completeness is almost the same but the implementation with \texttt{ExoplANNET} increases the precision of the results, significantly reducing the number of false positives.


\begin{table}[H]
	\centering
	\begin{tabular}{l|c|c|}
		\cline{2-3}
		& \begin{tabular}[c]{@{}c@{}}\textit{Virtual Astronomer}\\ FAP\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textit{Virtual Astronomer}\\ Red\end{tabular} \\ \hline
		\multicolumn{1}{|l|}{Precision}     & 0.911                                                           & 0.934                                                           \\ \hline
		\multicolumn{1}{|l|}{Recall} & 0.747                                                           & 0.741                                                           \\ \hline
	\end{tabular}
	\caption{Precision and Recall of the \textit{Virtual Astronomer}}
	\label{mat:virt_ast_pr}	
\end{table}


