\section{Simulations}

To train, validate and test the network a data set of sufficient size is required. In principle, it is not known how large this data set needs to be, but the larger the set with which the network is trained and validated, the better the results it is expected to provide (see \citeauthor{banko2001} \citeyear{banko2001} for a famous example, and the discussion by \citeauthor{halevy2009} \citeyear{halevy2009}). 
 
The data set not only needs to be large but also representative, in terms of number of points, variability, presence of periodic signals, etc., of the cases that the network will encounter when used in ``production stage'', i.e. when used to classify actual time series. The available radial velocity time series issued from large scale surveys are too few and too diverse for an efficient training. They do not have enough representatives of each type of variability and planetary system architecture. Besides, not knowing precisely the number of planets in the system means we would also have to deal with the problem of inaccurate labelling, i.e. not being able to precisely convey to the algorithm the class to which a given data set belongs.

The alternative is to train and evaluate the network on synthetic RV time series, and later study its performance on real data sets. If the simulated data are realistic enough, this may be enough, but most probably, an adaptation of the network architecture would be needed. Artificial data have many obvious advantages: being able to produce a large number of time series, and precisely knowing the number of periodic signals in them is fundamental. On the other hand, one clear disadvantage is that the results from our study, at least in principle, will be as relevant as our simulations are realistic.

Our network takes periodograms as inputs. We proceed by first generating synthetic time series, from which the Generalised Least-Squared \citep[GLS][]{zechmeisterkurster2009} periodogram is produced. The reason for this is that while we have relatively good models to simulate time series, the problem of directly simulating periodograms is much tougher. The steps of the simulation process are detailed in the subsections below.

% All the parameters used and data generated are saved in a file (particular characteristics of each planet, seed used to generate random parameters, noises, errors, etc.). 


%{\bf In addition to the},  periodogram of the time series, the FAP of the greatest peak and four full steps of the \textit{virtual astronomer}, are also computed and stored at the time of simulating the data, regardless of whether its value indicates that the detected periods are significant or not.

%This is for a practical matter. By having the periograms and FAP already calculated and stored, it is much easier and faster to simulate an iteration of the \textit{virtual astronomer}. The list of successive periodograms is iterated until the FAP value dictates that the peak is not a planet and, since all the values of the planetary signals are stored, it is possible to know exactly the successes and errors of the execution, vary the limit of detection of the FAP and see how it influences the results, etc. As a final advantage, by doing this, the number of available periodograms  to train the network is multiplied by four since each one can be used independently.



% The step between points in the time array is relevant only in relation to the signals that will be modelled in the next steps. But in order to make the process more concrete, we may think of it as a daily-sampled time series with some variability. 

% It is intended that the time series are as realistic as possible and that they exhibit the variability observed in solar-type stars, for which different types of noise and planetary disturbances were generated that are then sampled at two hundred points. These points where the samples are taken simulate the measurements taken on the nights of astronomical observation, so they are not at a regular interval (not every night is observed at the same time).

% \subsection{Activity signals}

% It is intended that the time series are as realistic as possible and that they exhibit the variability observed in solar-type stars. For which different types of noise and planetary disturbances were generated that are then sampled at two hundred points. These points where the samples are taken simulate the measurements taken on the nights of astronomical observation, so they are not at a regular interval (not every night is observed at the same time).

% The times in which the measurements are made are obtained from equispaced values to which a normal component is added to give them some variability. In the implementation, this is achieved by means of a vector of \ textit {times} with the values obtained from that distribution, to then apply it to the generated signal and obtain its values at those moments.

\subsection{Time array}
The synthetic time series are sampled using 200 quasi-uniformly distributed points. The times are defined by adding random variables drawn from a zero-centred normal distribution with width 0.1 to an evenly spaced time array. Although not completely realistic, this sampling roughly represents an intensive observing season with nightly observations on each target\footnote{The sampling may be seen as done in sidereal days}.

\subsection{Intrinsic errors}

Intrinsic error terms\footnote{Here we call error terms to the unknown term that makes the observed velocities deviate from the model. In contrast, the term uncertainties is reserved to the experimental error associated with each measurement.} are simulated by randomly drawing samples from an uncorrelated multivariate zero-centred normal distribution, with variances informed by the statistics of the uncertainties of the HARPS high-precision survey \citep[e.g.]{harps, Diaz2016TheHS}. For each simulated star, a star is randomly drawn from the subset of HARPS targets that were observed more than forty times by the survey. Using the observations of this star, the mean velocity uncertainty, $\bar{\sigma_v}$, is computed. Then, 200 samples are drawn from a normal distribution centred on $\bar{\sigma_v}$ and with a width of 0.30  ms$^{-1}$, which is a typical value for the dispersion in the velocity uncertainties of observed stars. Negative values are replaced by $0.5 \bar{\sigma_v}$. In this way, each data point has a slightly different uncertainty and error term, as is usually the case in real time series.

% And looking at the characteristic stars HD40307, HD1461, and HD204313, it was found that the dispersion of the errors is 0.24 ms$^{-1}$, 0.37 ms$^{-1}$ and 0.13 ms$^{-1}$. That is, a small scatter per star.

% Thus, we take the mean value of the errors of a random star from the high precision program (PI: M. Mayor, then S. Udry, then R. DÃ­az), and two hundred random numbers were obtained from a normal distribution with a mean equal to that value and variance equal to (0.30  ms$^{-1})^2$, if any value was negative, it was replaced by half the mean. The vector obtained will constitute the value of the error.

% To simulate white noise in the signal, we take the error vector and multiply each value by a random number from a standard normal distribution.

\subsection{Activity signals}

It has already been mentioned that there are several components of stellar noise that can alter radial velocity measurements. Simulated effects of pulsations and granulations were based on the work by \citet{dumusque2011a}. The authors studied the power spectrum of five well-observed solar stars, the spectrum is fitted with a Lorentzian component that represents the pulsations and three components for the granulation, mesogranulation and supergranulation. Using the fitted parameters defined in there, the power spectrum is constructed as the sum of these four components, with randomly chosen phases for each component. The resulting power spectrum is used to generate the synthetic radial velocity measurements.

The effect of the rotational modulation was simulated using samples from a Gaussian process with a covariance function generated with the pseudo periodic kernel:

$$
k_\mathrm{QP}(t_i, t_j) = A^2 \exp\left(-\frac{(t_i - t_j)^2}{2\tau^2} - \frac{2}{\epsilon}\sin^2{\left(\frac{\pi (t_i - t_j)}{\mathcal{P}}\right)}\right)\
$$

This \textit{kernel} is used widely in the exoplanet literature to model this kind of effect\footnote{See references in the Introduction for a number of examples of articles using Gaussian Processes with this kernel.} and has been used to infer the rotational period and other parameters of astrophysical interest related to stellar activity and rotation \citep[e.g.][]{giles2017, angus2018} This \textit{kernel} has 4 hyperparameters: $A$, the  covariance amplitude; $\mathcal{P}$, the recurrence time, which would be given by the rotation period; $\tau$, the decay time, associated with the average lifetime of an active region; $\epsilon$, the structure factor, a parameter associated with the number of active regions that occur simultaneously.

To define the values of these hyperparameters for each simulated time series we resorted again to the HARPS survey. All stars in the survey have an estimate of their rotation period obtained from a measurement of their magnetic activity using the calibration by \citet{Mamajek_2008}. For each simulated velocity series, the value of $\mathcal{P}$ was randomly chosen from the sample of estimated rotational periods for HARPS stars. The remaining parameter values were sampled from distributions:
\begin{itemize}
	\item[]{$A \sim$} gamma distribution, $\Gamma$(2.0 , 0.5). 
	\item[]{$\tau\sim$} normal distribution, $\mathcal{N}(3*\mathcal{P}$ , 0.1*$\mathcal{P})$.
	\item[]{$\epsilon\sim$}  uniform distribution, $\mathcal{U}$(0.5 , 1.0).
\end{itemize}

Adding the intrinsic errors to the activity signals makes the time series of radial velocities without a planet $rv_{wp}$.

\subsection{Planets}

The contribution of planets to the velocity time series is generated assuming circular orbits. Additionally, we disregard mutual interaction in multi-planetary systems.

Therefore, we need three parameters: the orbital period, $P_{pl}$, the semi-amplitude of the variation $K$, and the time in which the variation is zero, $T_0$. The value of $P_{pl}$ was chosen randomly between $10 * \delta$ and $\Delta_t/2$, where $ \delta$ is the minimum distance between two points in the time series, and $ \Delta_t$ is the total duration of the series. The value of $K$ was drawn from a log-uniform distribution between 0.1ms$^{-1}$ and 10ms$^{-1}$. Finally,  $T_0$ was randomly chosen as a moment within an orbital period.

The variation then takes the form:
$$ rv_{pl}  = K \sin \left( \frac{ 2 \pi (t - T_0) }{ P_{pl} } \right) $$
Once the vector $rv_{pl} $ has been obtained, it was be added directly to $rv_{wp}$ to get the time series with planet (see figure \ref{fig:vel_rad_proceso}).

\begin{figure*}
	\centering
	\begin{subfigure}{.33\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{vel_rad_noise}
		%\captionsetup{labelformat=empty}
		\caption{Stellar noise.}
		\label{fig:noise_wo_pl}
	\end{subfigure}%
	\begin{subfigure}{.33\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{vel_rad_pl}
		%		\captionsetup{labelformat=empty}
		\caption{Planetary signal.}
		\label{fig:rv_pl}
	\end{subfigure}
	\begin{subfigure}{.33\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{vel_rad_period_0}
		%		\captionsetup{labelformat=empty}
		\caption{Composition.}
		\label{fig:vel_rad_compuesto}
	\end{subfigure}
	\caption{Composition of stellar noise with a planetary signal with a period of 62 days and an amplitude of 10ms$^{-1}$.}
	\label{fig:vel_rad_proceso}
\end{figure*}

In line with our decision to neglect mutual interactions, to model multi-planetary systems, the procedure is repeated for each planet and the contributions were added.

\subsection{Periodograms}

GLS periodograms were computed for each simulated time series on a grid of uniformly-sampled orbital frequencies, between $\delta_\nu$ and 0.5 d$^{-1}$, with a constant step of $\delta_\nu$/10 (i.e. using an oversampling factor of ten), where $\delta_\nu$ is the inverse of the duration of the time series:

$$\delta_\nu = \frac{1}{\max(t) - \min(t)}  $$

\subsection{Datasets}
To train, validate and test the neural network, several datasets are needed to guarantee the robustness of the proposed solution and the quality of the results.

The usual methodology when working with periodograms is to analyze the maximum peak and define if it comes from a significant signal. If it is the case, the signal is removed from the original time series, and a new periodogram is generated using the model residuals, where a new maximum peak is identified and anlysed in a similar fashion. This means that the network will not only receive the original periodogram as input, but also subsequent periodograms obtained in this manner. So, in addition to simulating periodograms of stars with up to four planets, we also constructed additional periodograms by removing the signal at the largest peak frequency (regardless of whether it was significant or not). This process was performed four times for each original signal. This not only serves to have more varied periodograms, but also increases the size of the training data.

This particular type of problem where an input is considered and has to be assigned to one of two classes (here, planet or not planet) is called a binary classification problem. In this type of problem, it is very important to consider the balance of the classes in the data set --i.e. the fraction of instances from each case-- and the metrics used during training. For example, if an algorithm is trained with many more cases of a given class, it may be biased against predicting the minority class. This may lead to a model with a relatively good accuracy (i.e. the fraction of cases correctly classified), but ultimately useless.

This was considered when constructing the training, validation, and test sets. The first two were for training and parameter adjustment, and the final one was for testing the complete method. For all the sets the same number of stars with 0, 1, 2, 3, and 4 planets were generated and all their periodograms, constructed as described above, were used for parameter tuning and testing. Due to the nature of the generation process, these sets are naturally unbalanced toward negative cases. Consider that from each system, four periodograms are generated independently of the number of planetary signals injected. Then, for each star without planets, for example, four periodograms with negative labels will be produced. In addition, for systems with planets, it may occur that the planetary frequency is not at the maximum peak, which would lead to another negative label, even in the presence of a planet in the simulation. This was not considered a problem for data sets used for evaluation because they are seen as a realistic application. For the training set, however, a balanced data set is more important, periodograms were randomly selected to have near half-positive and half-negative cases. 

In summary, three data sets were generated. In all cases, we simulated an equal number of stars with 0, 1, 2, 3, and 4 planets.
\begin{itemize}
	\item{\textit{Set 1 - 3425 stars}:} from which 13700 periodograms were generated with the process described above. This set was balanced by randomly selecting positive and negative cases until 40\% of positives cases was reached. This set was used for train and validation (see Sect.\ref{sec:train}). 
	\item{\textit{Set 2 - 2550 stars}:} from which we get 10000 periodograms. This set was used for comparisons between methods and threshold search (see Sects.~\ref{lbl:rendimiento} and \ref{lbl:thrsearch}).
	\item{\textit{Set 3 - 5000 stars}:} which lead to 20000 periodograms. It was used to apply the \textit{Virtual Astronomer} to each star and to analyze the distribution of output values for the FAP and the CNN (see Sects.~\ref{lbl:performanceontest} and \ref{lbl:astro_virtual}).
\end{itemize}	

