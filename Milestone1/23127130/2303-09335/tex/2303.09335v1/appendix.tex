\section{Convolutional Neural Networks} \label{apendix} 

%In recent times there has been an explosion of research works that make use of some form of machine learning. It could lead to thinking that they are very modern techniques, but the fundamental ideas behind them were born almost with computers. The improvement in hardware, coupled with the immense access to data provided by the internet; made possible the use of techniques that thirty or forty years ago were mathematical curiosities.

In this Appendix, we make a very brief summary of general concepts and usage of convolutional networks. Notions such as layer or backpropagation, and the generalities of artificial neurons (activation function, synaptic weight, bias, etc.) are assumed to be known. Those wishing to investigate further into theoretical and implementation concepts can refer to the bibliography (\cite{bishop2007, 10.5555/104000, Goodfellow-et-al-2016}).

Deep fully-connected networks have achieved very good practical results used in a wide variety of problems. However, for the treatment of images, they have a disadvantage, they do not take into account the spatial structure of the image. As a consequence, they can become extremely inefficient. On the other hand, convolutional neural networks (CNN) were designed specifically to tackle tasks of computer vision. These networks use convolutions instead of classical matrix operations and were created to work with images, but are also useful for one-dimensional data where the structure of the data is relevant.

Broadly speaking, CNNs perform the following steps when processing an image:
\begin{itemize}
	\item{\textit{Feature extraction}}: Each neuron takes its synaptic inputs from a local receptive field, i.e. a small region in the previous layer, and combines them to extract local features.
	\item{\textit{Feature mapping}}: Each layer of the network is made up of many feature maps, each in the form of a plane within which individual neurons are forced to share the same set of synaptic weights. This second form of structural restriction has the following beneficial effects:
	\begin{itemize}
		\item Translational invariance, achieved by performing the convolution operation on a feature map with a small kernel, followed by applying the activation function. A kernel is a small matrix of weights with which the image is convoluted. 
		\item Reduced number of parameters, achieved through the use of shared weights.
	\end{itemize}
	\item{\textit{Subsampling}}: Convolutional layers are often followed by a layer that performs local averaging and subsampling, thus reducing the resolution of the feature map. This operation has the effect of reducing the sensitivity of the feature map output.
\end{itemize}

In this way, a deep network is obtained that in its first layers detects lines or curves; but, as it progresses, it detects more complex shapes such as a face or a silhouette \citep{zeilerfergus2014}.

To make a feature map, a kernel is applied to an image, this kernel on the image is the \textit{receptive field} that a neuron ``sees''. In Figure \ref{fig:kernel-conv}, there is an example of how a characteristics map is generated. In the example, a 3x3 kernel is applied on a 5x5 image \footnote{For simplicity, these examples are presented on matrices in two dimensions, assuming black and white images. If they were colour images it would be necessary to add a dimension to the input and the kernel. So we would have three filters, these three filters are added together with the \textit{bias} and they will form an output as if it were a single channel.}.

\begin{figure}[H]
	\resizebox{\hsize}{!}{\includegraphics[width=0.8\linewidth]{kernel-conv}}
	\caption{Generation of a feature map using one kernel.}
	\label{fig:kernel-conv}
\end{figure}

The size of the kernel is one of the parameters chosen when creating the network. They are usually small to extract local characteristics. To find many of these features, many kernels are used. They all have the same dimension, but the weights they learn are different (they look for different things). This set of kernels is what is called a filter.

Next we focus on how downsampling layers work and why they are needed, using the classic  MNIST dataset \citep{deng2012} as an example. This dataset consists of tens of thousands of handwritten digit images, which are in black and white and with a resolution of 28x28 pixels. Each image represents a digit between zero and nine.

In Figure \ref{fig:feature-map} you can see how applying a filter of 32 kernels of 3x3 gives 32 activation maps of 26x26 each \footnote{The output dimension $o$ to apply a kernel of dimensions $k$ to an image of size $w$ can be calculated with this formula: $o = w-k + 1$. Concepts such as padding or stride modify this formula, but they are unnecessary for the example.}, so the number of neurons in this first hidden layer is 21,632 %\footnote{It is worth mentioning that, due to the kind of problem, the trigger function is a ReLU (Rectified Linear Unit) type that simply returns the maximum between 0 and the input value.}.

\begin{figure}[H]
	\resizebox{\hsize}{!}{\includegraphics[width=0.9\linewidth]{feature-map}}
	\caption{First hidden layer.}
	\label{fig:feature-map}
\end{figure}

The size of the information after the first layer has increased considerably. Consider this is a very small image, and it is only the first layer. Adding more layers can make this number grow very fast. Eventually, this hinders training a network on large high-definition images, since the network will lose the ability to abstract. This is where the downsampling layer comes in. 

The most used implementation is maxpooling. In this mechanism, we seek to reduce the parameters by dividing the input matrix into parts and keeping the maximum value of each one. It seeks to preserve the most important characteristics and reduce the dimensions of the data. In Figure \ref{fig:maxpooling} there is an example of its application.

%https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/
%https://www.aprendemachinelearning.com/como-funcionan-las-convolutional-neural-networks-vision-por-ordenador/
%https://www.quora.com/How-can-I-calculate-the-size-of-output-of-convolutional-layer

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{maxpooling}
	\caption{\textit{Maxpooling}.}
	\label{fig:maxpooling}
\end{figure}

Figure \ref{fig:full-conv} shows the result of using this layer over the output of Figure \ref{fig:feature-map}.

\begin{figure}[H]
	\resizebox{\hsize}{!}{\includegraphics[width=1\linewidth]{full-conv}}
	\caption{Use of maxpooling for dimensionality reduction.}
	\label{fig:full-conv}
\end{figure}

As expected, the number of layers required to reach good performance, and the manner in which they are interleaved, depends on the problem to be solved, and there is no general rule. But in general, the convolutional network extracts the most relevant features from the image and then connects them to one or more fully connected layers and an output layer used for classification. This layer depends on the problem. In this particular example, it has a neuron for each category --i.e. ten neurons-- and uses a softmax activation function. This activation function normalises the outputs of the output layer so that each value can be interpreted as the probability of belonging to a given class. The fully-connected part, together with the output layer use the information extracted in the convolutional part, and ``decide'' to which category the entry belongs. Figure \ref{fig:conv-complete} shows a general diagram of the network in this example.

\begin{figure}[H]
	\resizebox{\hsize}{!}{\includegraphics[width=0.9\linewidth]{conv-complete}}
	\caption{Full network.}
	\label{fig:conv-complete}
\end{figure}

