
\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}

\usepackage{diagbox}
\usepackage{amssymb}
\usepackage{floatrow}
\usepackage{wrapfig}

\usepackage{dsfont}
\usepackage{xspace}
\usepackage{listings}
\usepackage{numprint}
\usepackage{multirow}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{lipsum}
\usepackage{caption}
\captionsetup{font=small}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{microtype}
\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{sidecap}
\usepackage{subfigure}

\usepackage{balance}
\usepackage{pifont}
\usepackage{caption}
\usepackage{comment}
\usepackage{soul}
\usepackage{bbm}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\definecolor{BlueBlack}{RGB}{36, 113, 163}
\hypersetup{
    colorlinks,
    linkcolor={red},
    citecolor={BlueBlack},
    urlcolor={blue}
}
\usepackage{cleveref}[capitalise]
\crefname{section}{Sec.}{Secs.}
\crefname{figure}{Fig.}{Figs.}
\crefname{table}{Tab.}{Tabs.}
\crefname{equation}{Eq.}{Eqs.}
\Crefname{section}{Section}{Sections}
\usepackage{url}

\title{\textbf{T}emperature \textbf{S}chedules for self-super\-vised contrastive methods on long-tail data}




\author{
Anna Kukleva\thanks{equal contribution. Code available at:  \href{https://github.com/Annusha/temperature\_schedules}{github.com/annusha/temperature\_schedules}}    $^{\,1}$, Moritz BÃ¶hle$^{*1}$, Bernt Schiele$^{1}$, Hilde Kuehne$^{2,3}$, Christian Rupprecht$^{4}$ \\
% \thanks{denotes equal contribution. Code is available: }\
\small $^{1}$ MPI for Informatics, Saarland Informatics Campus, \small $^{2}$ Goethe University Frankfurt, \\ \small $^{3}$ MIT-IBM Watson AI Lab,$^{4}$ University of Oxford  \;$\Vert$\; \texttt{\{akukleva,mboehle\}@mpi-inf.mpg.de}\\
}


% Math operators with less space usage 
\newcommand\myeq{\mkern1.25mu{=}\mkern1.25mu}
\newcommand\myneq{\mkern1.25mu{\neq}\mkern1.25mu}
\newcommand\myrightarrow{\mkern1.25mu{\rightarrow}\mkern1.25mu}
\newcommand\myminus{\mkern1.25mu{-}\mkern1.25mu}
\newcommand\myplus{\mkern1.25mu{+}\mkern1.25mu}
\newcommand\mygreater{\mkern1.25mu{>}\mkern1.25mu}
\newcommand\myin{\mkern1.25mu{\in}\mkern1.25mu}
\newcommand\mytimes{\mkern1.25mu{\times}\mkern1.25mu}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\bernt}[1]{\textcolor[rgb]{0.82, 0.1, 0.26}{\textbf{Bernt: #1}}}
\newcommand{\anna}[1]{\textcolor[rgb]{1, 0.3, 0.1}{\textbf{A: #1}}}
\newcommand{\headclass}[1]{\textcolor[rgb]{1, 0, 0}{#1}}
\newcommand{\tailclass}[1]{\textcolor[rgb]{0, 0, 1}{#1}}
\newcommand{\hkc}[1]{\textcolor[rgb]{0, 0, 1}{\textbf{Hilde: #1}}}
\newcommand{\moritz}[1]{\textcolor[rgb]{0, .5, .5}{\textbf{Moritz: #1}}}
\newcommand{\cmark}{\ding{51}}

% TODO: disable this for the final version!
%\usepackage{ulem}
\newcommand{\REM}[1]{\textcolor[rgb]{1, 0, 1}{\textbf{#1}}}
\newcommand{\ADD}[1]{\textcolor[rgb]{0.25, 0.75, 0}{\textbf{#1}}}
\newcommand{\rebuttal}[1]{\textcolor[rgb]{0, 0, 0}{#1}}
\newcommand{\green}[1]{\textcolor[rgb]{0, 1, 0}{\textbf{#1}}}
\newcommand{\red}[1]{\textcolor[rgb]{1, 0, 0}{\textbf{#1}}}
\newcommand{\greendark}[1]{\textcolor[HTML]{40A940}{\textbf{#1}}}
\newcommand{\reddark}[1]{\textcolor[HTML]{DA3C3D}{\textbf{#1}}}
\newcommand{\yellow}[1]{\textcolor[HTML]{F08722}{\textbf{#1}}}
\newcommand{\purple}[1]{\textcolor[HTML]{800080}{\textbf{#1}}}
\newcommand{\cyan}[1]{\textcolor[HTML]{19BFCF}{\textbf{#1}}}
\newcommand{\TODO}[1]{\textcolor[rgb]{1, 0.5, 0}{\textbf{TODO: #1}}}

\newfloatcommand{capbtabbox}{table}[][\FBwidth]

\makeatletter
\newcommand{\ie}{\textit{i}.\textit{e}.\@ifnextchar{,}{}{~}}
\makeatother
\makeatletter
\newcommand{\eg}{\textit{e}.\textit{g}.\@ifnextchar{,}{}{~}}
\makeatother

\newcommand{\myparagraph}[1]{\vspace{2pt}\noindent{\bf #1}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\vspace{-3mm}
\begin{abstract}


Most approaches for self-supervised learning (SSL) are optimised on curated balanced datasets, \eg ImageNet, despite the fact that natural data usually exhibits long-tail distributions. In this paper, we analyse the behaviour of one of the most popular variants of SSL, \ie contrastive methods, on long-tail data. In particular, we investigate the role of the temperature parameter $\tau$ in the contrastive loss, by analysing the loss through the lens of average distance maximisation, and find that a large $\tau$ emphasises {group-wise} discrimination, 
whereas a small $\tau$ leads to a higher degree of {instance} discrimination.
While $\tau$ has thus far been treated exclusively as a \emph{constant} hyperparameter, in this work, we propose to employ a \emph{dynamic} $\tau$ and show that a simple cosine schedule can yield significant improvements in the learnt representations. 
Such a schedule results in a constant `task switching' between an emphasis on {instance} discrimination and {group-wise} discrimination and thereby ensures that the model learns both group-wise features, as well as instance-specific details. 
Since frequent classes benefit from the former, while infrequent classes require the latter, we find this method to consistently improve separation between the classes in long-tail data without any additional computational cost. 


\end{abstract}


%%%%%%%%% BODY TEXT
% \input{iclr2023/1_intro.tex}

\section{Introduction}
\label{sec:intro}


Deep Neural Networks have shown remarkable capabilities at learning %abstract 
representations of their inputs that are useful for %solving 
a variety of %complex 
tasks. Especially since the advent of recent self-supervised learning (SSL) techniques, rapid progress towards learning universally useful representations has been made. 

Currently, however, SSL on images is mainly carried out on benchmark datasets that have been constructed and curated for supervised learning (\eg ImageNet \citep{deng2009imagenet}, CIFAR \citep{krizhevsky2009learning}, etc.).
Although the labels of curated datasets are not \emph{explicitly} used in SSL, the \textit{structure} of the data still follows the predefined set of classes.
In particular, the class-balanced nature of curated datasets could result in a learning signal for unsupervised methods.
As such, these methods are often not evaluated in the settings they were designed for, \ie learning from truly unlabelled data.
Moreover, some methods (\eg \citep{asano2019self,caron2020unsupervised}) even explicitly enforce a uniform prior over the embedding or label space, which cannot be expected to %enforcing priors that do not 
hold for uncurated datasets.

In particular, {uncurated}, real-world data tends to follow long-tail distributions \citep{REED200115}, in this paper, we analyse SSL methods on long-tailed data. Specifically, we analyse the behaviour of contrastive learning (CL) methods, which are among the most popular learning paradigms for SSL. 

In CL, the models are trained such that embeddings of different samples are repelled, while embeddings of different `views' (\ie augmentations) of the same sample are attracted. The strength of those attractive and repelling forces between samples is controlled by a temperature parameter $\tau$, which has been shown to play a crucial role in learning good representations~\citep{chen2020mocov2, chen2020simple}. To the best of our knowledge, $\tau$ has thus far almost exclusively been treated as a \textit{constant} hyper-parameter. % in prior work.


In contrast, %in this work,
we employ a \emph{dynamic} $\tau$ during training and show that this has a strong effect on the learned embedding space for long-tail distributions. 
In particular, by introducing a simple schedule for $\tau$ we consistently improve the representation quality across 
a wide range of settings.
Crucially, %this comes without any additional computational cost
these gains are obtained without additional costs
%\emph{for free}, 
%or loss terms 
and only require oscillating $\tau$ with a cosine schedule.  


This mechanism is grounded in our novel understanding of the effect of temperature on the contrastive loss. 
In particular, we analyse the contrastive loss from an average distance maximisation perspective, which gives intuitive insights as to why a large temperature emphasises 
\emph{group-wise discrimination}, whereas a small temperature 
leads to a higher degree of 
\emph{instance discrimination}
and more uniform distributions over the embedding space. 
Varying $\tau$ during training ensures that the model learns both %easily separable 
group-wise and
%features as well as hard 
instance-specific
features,
resulting in better separation between head and tail classes.

Overall, our contributions are summarised as follows:
$\bullet$ we carry out an extensive %theoretical and practical
analysis of the effect of $\tau$ on imbalanced data; $\bullet$ we analyse the contrastive loss from an average distance perspective to understand the emergence of semantic structure; $\bullet$ we propose a simple yet effective temperature schedule that improves the performance across different settings;
$\bullet$ we show that the proposed $\tau$ scheduling is robust and consistently improves the performance for different %contrastive learning methods and 
hyperparameter choices.


% \input{iclr2023/2_rel_work}
\section{Related Work}
\label{sec:rel_work}

Self-supervised representation learning (SSL) from visual data is a quickly evolving field. 
Recent methods are based on various forms of comparing embeddings between transformations of input images. 
We divide current methods into two categories: contrastive learning~\citep{he2019moco, chen2020mocov2, chen2020simple, oord2018representation}, and non-contrastive learning \citep{grill2020bootstrap,zbontar2021barlow,chen2021exploring,bardes2022vicreg,wei2022masked,Gidaris_2021_CVPR, asano2019self, caron2020unsupervised, he2022masked}.
Our analysis concerns the structure and the properties of the embedding space of contrastive methods when training on imbalanced data. 
Consequently, this section focuses on contrastive learning methods, their analysis and application to imbalanced training datasets. 

\myparagraph{Contrastive Learning}
employs instance discrimination \citep{wu2018unsupervised} to learn representations by forming positive pairs of images through augmentations and a loss formulation that maximises their similarity while simultaneously minimising the similarity to other samples. 
Methods such as MoCo~\citep{he2019moco, chen2020mocov2}, SimCLR~\citep{chen2020simple, chen2020big}, SwAV~\citep{caron2020unsupervised}, CPC~\citep{oord2018representation}, CMC~\cite{tian2020contrastive}, and Whitening~\citep{ermolov2021whitening} have shown impressive representation quality and down-stream performance using this learning paradigm.
CL has also found applications beyond SSL pre-training, such as multi-modal learning \citep{shvetsova2022everything}, domain generalisation \citep{yao2022pcl}, semantic segmentation \citep{van2021unsupervised}, 3D point cloud understanding \citep{afham2022crosspoint}, and 3D face generation \citep{deng2020disentangled}.

% To further improve the representations, 
\myparagraph{Negatives.} The importance of negatives for contrastive learning is remarkable and noticed in many prior works~\citep{wang2021solving, yeh2021decoupled, zhang2022dual, iscen2018mining, kalantidis2020hard, robinson2020contrastive, khaertdinov2022dynamic}. \citet{yeh2021decoupled} propose decoupled learning by removing the positive term from the denominator, \citet{robinson2020contrastive} develop an unsupervised hard-negative sampling technique, \citet{wang2021solving} propose to employ a triplet loss, \rebuttal{and \citet{zhang2022dual, khaertdinov2022dynamic} propose to improve negative mining with the help of different temperatures for positive and negative samples that can be defined as input-independent or input-dependent functions, respectively.}
% discussed in depth in recent work.. 
In contrast to \emph{explicitly} choosing a specific subset of negatives, we discuss the Info-NCE loss~\citep{oord2018representation} through the lens of an average distance perspective with respect to {all} negatives and show that the temperature parameter can be used to \emph{implicitly} control the effective number of negatives.

\myparagraph{Imbalanced Self-Supervised Learning.} Learning on imbalanced data instead of curated balanced datasets is an important application since natural data commonly follows long-tailed distributions~\citep{REED200115, liu2019large, wang2017learning}. 
In recent work, ~\citet{kang2020exploring},~\citet{yang2020rethinking},~\citet{liu2021selfsupervised},~\citet{zhong2022self},~\citet{gwilliam2022beyond} discover that self-supervised learning generally allows to learn a more robust embedding space than a supervised counterpart.  
\citet{tian2021divide} explore the down-stream performance of contrastive learning on standard benchmarks based on large-scale uncurated pre-training and propose a multi-stage distillation framework to overcome the shift in the distribution of image classes.   
\citet{jiang2021self,zhou2022contrastive} propose to address the data imbalance by identifying and then emphasising tail samples during training in an unsupervised manner. For this, \citet{jiang2021self} compare the outputs of the trained model before and after pruning, assuming that tail samples are more easily `forgotten' by the pruned model and can thus be identified. \citet{zhou2022contrastive}, use the loss value for each input to identify tail samples and then use stronger augmentations for those. %To alleviate this problem, they propose to modify the contrastive framework SimCLR~\citep{chen2020simple} by contrasting an online model with a pruned version of the same model to identify difficult samples. 
Instead of modifying the architecture or the training data of the underlying frameworks, we show that a simple approach---\ie oscillating the temperature of the Info-NCE loss~\citep{oord2018representation} to alternate between instance and group discrimination---can achieve similar performance improvements at a low cost.


\myparagraph{Analysis of Contrastive Learning (CL).} 
Given the success of CL in representation learning, it is essential to understand its properties.
While some work analyses the interpretability of embedding spaces \citep{bau2017network,fong2018net2vec,laina2020quantifying,laina2021measuring}, here the focus lies on understanding the structure and learning dynamics of the objective function such as in \cite{saunshi2019theoretical, tsai2020self, chen2021intriguing}.
E.g., \citet{chen2021intriguing} study the role of the projection head, the impact of multi-object images, and a feature suppression phenomenon. \citet{wen2021toward} analyse the feature learning process to understand the role of augmentations in CL. \citet{robinson2021can} find that an emphasis on instance discrimination can improve representation of some features at the cost of suppressing otherwise well-learned features. \cite{wang2020understanding, wang2021understanding} analyse the uniformity of the representations learned with CL. In particular, \citet{wang2021understanding} focus on the impact of individual negatives and describe a uniformity-tolerance dilemma when choosing the temperature parameter. In this work, we rely on the previous findings, expand them to long-tailed data distributions and complement them with an understanding of the emergence of semantic structure. 



% \input{iclr2023/3_method}


% \clearpage
\section{Method}
\label{sec:method}
In the following, we describe our approach and analysis of contrastive learning on long-tailed data. For this, we will first review the core principles of contrastive learning for the case of uniform data (\cref{subsec:contrastive_learning}). In \cref{subsec:max_margin}, we then place a particular focus on the temperature parameter $\tau$ in the contrastive loss and its impact on the learnt representations. 
Based on our analysis, in \cref{subsec:CL_on_LT} we discuss how the choice of $\tau$ might negatively affect the learnt representation of rare classes in the case of long-tailed distributions. Following this, we describe a simple proof-of-concept based on additional coarse supervision to test our hypothesis. We then further develop 
\underline{t}emperature \underline{s}chedules (TS)
that yield significant gains with respect to the separability of the learnt representations in \cref{sec:results}.


\subsection{Contrastive Learning}
\label{subsec:contrastive_learning}
% \textbf{Definitions}
% Anchor, positive, negative, hard-negatives, easy-negatives, tolerance

\textbf{The Info-NCE loss} is a popular objective for contrastive learning (CL) and has lead to impressive results for learning useful representations from unlabelled data \citep{oord2018representation, wu2018unsupervised, he2019moco, chen2020simple}. Given a set of inputs $\{x_1, \dots, x_N\}$, and the cosine similarities $s_{ij}$ between learnt representations $u_i\myeq f(\mathcal A(x_i))$ and $v_j\myeq g(\mathcal A (x_j))$ of the inputs, the loss is defined by:
\begin{align}
    \mathcal L_\text{c} = \sum_{i = 1}^N-\log\dfrac{\exp\left(s_{ii} / \tau\right)}{\exp\left(s_{ii} / \tau\right) + \sum_{j\neq i} \exp\left(s_{ij} / \tau\right)}.
    \label{eq:info-nce}
\end{align}
Here, 
 $\mathcal A(\cdot)$ applies a random augmentation to its input and $f$ and $g$ are deep neural networks. For a given $x_i$, we will refer to $u_i$ as the \emph{anchor} and to $v_j$ as a \emph{positive} sample if $i\myeq j$ and as a \emph{negative} if $i\myneq j$.
Last, $\tau$ denotes the \emph{temperature} of the Info-NCE loss and has been found to crucially impact the learnt representations of the model~\citep{wang2020understanding,wang2021understanding,robinson2021can}.


\begin{figure}[!t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=\textwidth]{iclr2023/evolution4.pdf}
\end{center}
\vspace{-1em}
\caption{\textbf{Coverage of the embedding space during training.} To measure coverage we uniformly sample $500$ bins on the unit sphere. Each training sample is assigned to the closest bin and we plot a histogram of the assignments. X-axis: bins. Y-axis: number of training samples in a bin. Colors denotes epochs: light is the 1st epoch of training, dark is the last. For small $\tau$ (a) the representations are more uniformly distributed (cf.~\cref{sec:method}). 
}
\label{fig:tau_distances}
\end{figure}

\myparagraph{Uniformity.} Specifically, a small $\tau$ has been tied to more uniformly distributed representations, see \cref{fig:tau_distances}. For example, \cite{wang2021understanding} show that the loss is `hardness-aware', \ie negative samples closest to the anchor receive the highest gradient. In particular, for a given anchor, the gradient with respect to the negative sample $v_j$ is scaled by its relative contribution to the denominator in \cref{eq:info-nce}:
\begin{align}
    \frac{\partial \mathcal L_c}{\partial v_j} = \frac{\partial \mathcal L_c}{\partial s_{ij}} \times \frac{\partial s_{ij}}{\partial v_j}
    = \frac1\tau\times [\text{softmax}_{k}(s_{ik}/\tau)]_j \times \frac{\partial s_{ij}}{ \partial v_j}\quad .
    \label{eq:gradient}
\end{align}
As a result, for sufficiently small $\tau$, the model minimises the cosine similarity to the nearest negatives in the embedding space, as softmax approaches an indicator function that selects the largest gradient. The optimum of this objective, in turn, is to distribute the embeddings as uniformly as possible over the sphere, as this reduces the average similarity between nearest neighbours, see also \cref{fig:tau_distances,,fig:spheres}. 

\myparagraph{Semantic structure.} 
In contrast, a large $\tau$ has been observed to induce more semantic structure in the representation space.
However, while the effect of small $\tau$ has an intuitive explanation, the phenomenon that larger $\tau$ induce semantic structure is much more poorly understood and has mostly been described empirically~\citep{wang2021understanding,robinson2021can}. Specifically, note that for any given positive sample, all negatives are repelled from the anchor, with close-by samples receiving exponentially higher gradients. Nonetheless, for large $\tau$, tightly packed semantic clusters emerge. However, if close-by negatives are heavily repelled, how can this be? Should the loss not be dominated by the hard-negative samples and thus break the semantic structure?
%, see \citet{wang2021understanding}?   --- CR: I think we cite them enough in this part

To better understand both phenomena, we propose to view the contrastive loss through the lens of \emph{average distance} maximisation, which we describe in the following section.


\subsection{Contrastive learning as average distance maximisation}
\label{subsec:max_margin}
As discussed in the previous section, the parameter $\tau$ plays a crucial role in shaping the learning dynamics of contrastive learning. To understand this role better, in this section, we present a novel viewpoint on the mechanics of the contrastive loss that explain the observed model behaviour. In particular, and in contrast to \cite{wang2021understanding} who focused on the impact of \emph{individual} negatives, for this we discuss the \emph{cumulative} impact that all negative samples have on the loss.

To do so, we express the %individual
summands $\mathcal L_c^i$ of the loss in terms of distances $d_{ij}$ instead of similarities $s_{ij}$:
%
\begin{equation}
    0 \;\leq\; d_{ij} \;=\; \frac{1-s_{ij}}{\tau} \;\leq\; \frac2\tau\quad \text{and}\quad c_{ii} = \exp(d_{ii}).
    \label{eq:margin_def}
\end{equation}
This allows us to rewrite the loss $\mathcal L_c^i$ as
\begin{equation}
    \label{eq:margin_loss}
    \mathcal L^i_\text{c} 
    = -\log\left(\frac{\exp\left(-d_{ii}\right)}{\exp\left(-d_{ii}\right)+ \sum_{j\neq i}\exp\left(-d_{ij}\right)}\right)
    = \log\left(1+c_{ii}{\sum_{j\neq i}\exp\left(-d_{ij}\right)}\right) \, .
\end{equation}
%
%
As the effect $c_{ii}$ of the positive sample for a given anchor is the same for all negatives, in the following we place a particular focus on %the role of 
the negatives and their relative influence on the loss in \cref{eq:margin_loss}; for a discussion of the influence of positive samples, please see \cref{appendix:positives}.


%
To understand the impact of the temperature $\tau$, first note that the loss monotonically increases with the sum $S_i=\sum_{j\neq i}\exp(-d_{ij})$ of exponential distances in \cref{eq:margin_loss}.
As $\log$ is a continuous, monotonic function, % alternatively: diminishing returns?
we base the following discussion on the impact of $\tau$ on the sum $S_i$.
%As such, the contrastive objective can be understood as a margin maximisation 


\begin{figure}[!t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[scale=0.1125]{iclr2023/fig1.pdf}
\end{center}
\caption{\textbf{Loss contribution by similarity.} X-axis: cosine similarity between anchor and negative. All curves are normalised such that their max y-value is 1. %For more results see supplement. 
\textbf{a)}: influence of an individual negative sample to the loss depending on its similarity to anchor for different $\tau$; \textbf{b)}: average histogram of distribution of negatives over the hypersphere with respect to their similarity to the anchor; \textbf{c)}: cumulative impact that negative samples have on the loss. The \emph{cumulative} contribution of negatives shifts left, towards less similar samples, in contrast to individual contributions of negatives. As $\tau\rightarrow\infty$, the cumulative distribution coincides with the histogram b). 
% 
}
\vspace{-1.5em}
\label{fig:negative_impact}
\end{figure}



\myparagraph{For small $\tau$}, the nearest neighbours of the anchor point dominate $S_i$, as differences in similarity are amplified. As a result, the contrastive objective maximises the average distance to nearest neighbours, leading to a uniform distribution over the hypersphere, see \cref{fig:spheres}. Since individual negatives dominate the loss, this argument is consistent with existing interpretations, \eg \cite{wang2021understanding}, as described in the previous section. 

\myparagraph{For large $\tau$}, (\eg $\tau\geq1$), on the other hand, the contributions to the loss from a given negative are on the same order of magnitude for a wide range of cosine similarities.  
Hence, the constrastive objective can be thought of as maximising the average distance over a wider range of neighbours.
Interestingly, since distant negatives will typically outnumber close negatives, the strongest \emph{cumulative} contribution to the contrastive loss will come from more distant samples, despite the fact that \emph{individually} the strongest contributions will come from the closest samples. To visualise this, in \cref{fig:negative_impact}a, we plot the contributions of \emph{individual} samples depending on their distance, as well as the distribution of similarities $s_{ij}$ to negatives over the entire dataset in \cref{fig:negative_impact}b. Since the number of negatives at larger distances (\eg $s_{ij}\approx 0.1$) significantly outnumber close negatives ($s_{ij}>0.9$), the peak of the cumulative contributions%
\footnote{To obtain the cumulative contributions, we group the negatives into 100 non-overlapping bins of size 0.02 depending on their distance to the anchor and report the sum of contributions of a given bin.}
shifts towards lower similarities for larger $\tau$, as can be seen in \cref{fig:negative_impact}c; in fact, for $\tau\myrightarrow\infty$, the distribution of cumulative contributions approaches the distribution of negatives.


Hence, the model can significantly decrease the loss by increasing the distance to relatively `easy negatives' for much longer during training, \ie to samples that are easily distinguishable from the anchor by simple patterns. Instead of learning `hard' features that allow for better \emph{instance discrimination} between hard negatives, the model will be biased to learn easy patterns that allow for \emph{group-wise discrimination} and thereby increase the margin between clusters of samples. Note that since the clusters as a whole mutually repel each other, the model is optimised to find a trade-off between the expanding forces between hard negatives (\ie within a cluster) and the compressing forces that arise due to the margin maximisation between easy negatives (\ie between clusters).

Importantly, such a bias towards easy features can prevent the models from learning hard features---\ie by focusing on \emph{group-wise discrimination}, the model becomes agnostic to instance-specific features that would allow for a better \emph{instance discrimination} (cf.~\cite{robinson2021can}). In the following, we discuss how this might negatively impact rare classes in long-tailed distributions.


\subsection{Temperature schedules for contrastive learning on long-tail data}
\label{subsec:CL_on_LT}
% In the last section, 
% 
As discussed in \cref{sec:intro}, naturally occurring data typically exhibit long-tail distributions, with some classes occurring much more frequently than others; across the dataset, \textit{head} classes appear frequently, whereas \textit{tail} classes contain fewest number of samples. Since self-supervised learning methods are designed to learn representations from unlabelled data, it is important to investigate their performance on imbalanced datasets.


\begin{figure}[!t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[scale=0.35]{iclr2023/spheres2.pdf}
\end{center}
\caption{%\textbf{Head and tail classes in embedding space.} 
\rebuttal{\textbf{Representations of a head and a tail class.} }
Visualisation of the influence of $\tau$ on representations of two semantically close classes (trained with all 10 classes). Red: \headclass{single head class} and blue: \tailclass{single tail class} from CIFAR10-LT. Small $\tau\myeq0.1$ promotes uniformity, while large $\tau\myeq1.0$ creates dense clusters. 
With $\tau_{\{head/tail\}}$ we refer to coarse supervision described in \cref{subsec:CL_on_LT} which separates tail from head classes. 
% 
In black / \headclass{red} / \tailclass{blue}, we respectively show the average kNN accuracy over all classes / \headclass{the head class} / \tailclass{the tail class}.%,
}
\label{fig:spheres}
\end{figure}

\myparagraph{Claim: Tail classes benefit from instance discrimination.}
As discussed in \cref{subsec:max_margin}, sufficiently large $\tau$ are required for semantic groups to emerge during contrastive learning as this emphasises group-wise discrimination. However, as shown by \cite{robinson2021can}, this can come at the cost of encoding instance-specific features and thus hurt the models' instance discrimination capabilities. 

We hypothesise that this disproportionately affects tail classes, as tail classes consist of only relatively few instances to begin with. Their representations should thus \emph{remain distinguishable} from most of their neighbours and not be grouped with other instances, which are likely of a different class. In contrast, since head classes are represented by many samples, grouping those will be advantageous.

To test this hypothesis, we propose to explicitly train head and tail classes with different $\tau$,
to emphasise group discrimination for the former while ensuring instance discrimination for the latter.

% 
\myparagraph{Experiment: Controlling $\tau$ with coarse supervision.}
We experiment on CIFAR10-LT (a long-tail variant of CIFAR10 - see \cref{subsec:implementation}) in which we select a different $\tau$ depending on whether the anchor $u_i$ is from a head or a tail class, \ie of the 5 \emph{most} or \emph{least} common classes. We chose a relatively large $\tau$ ($\tau_\text{head}\myeq1.0$) for the 5 head classes to emphasise group-wise discrimination and a relatively small $\tau$ ($\tau_\text{tail}\myeq0.1$) for the 5 tail classes to encourage the model to learn instance-discriminating features.

As can be seen in \cref{fig:spheres}, this simple manipulation of the contrastive loss indeed provides a significant benefit with respect to the semantic structure of the embedding space, despite only weakly supervising the learning by adjusting $\tau$ according to a coarse (frequent/infrequent) measure of class frequency. 

\rebuttal{In particular, in \cref{fig:spheres}, we show the projections of a single head class and a single tail class onto the three leading PCA dimensions and the corresponding kNN accuracies. %(red/blue) for those two classes, as well as the kNN performance averaged over \emph{all} classes (black). 
We would like to highlight the following results.} First, without any supervision, we indeed find that the head class consistently performs better for larger values of $\tau$ (\eg $1.0$), whereas the tail class consistently benefits from smaller values for $\tau$ (\eg $0.1$). Second, when training the model according to the coarse $\tau$ supervision as described above, we are not only able to maintain the benefits of large $\tau$ values for the head class, but significantly outperform all constant $\tau$ versions for the tail class, which improves the overall model performance on all classes; detailed results for all classes are provided in the appendix.

\myparagraph{\underline{T}emperature \underline{S}chedules (TS) without supervision.}
%
Such supervision with respect to the class frequency is, of course, generally not available when training on unlabelled data and these experiments are only designed to test the above claim and provide an intuition about the learning dynamics on long-tail data. However, we would like to point out that the supervision in these experiments is very coarse and only separates the unlabelled data into \emph{frequent} and \emph{infrequent} classes. Nonetheless, while the results are encouraging, they are, of course, based on additional, albeit coarse, labels.
Therefore, in what follows, we present an unsupervised method that yields similar benefits.

% \myparagraph{\underline Unsupervised $\tau$ \underline Adjustment (U$\tau$A).} 
In detail, we propose to modify $\tau$ according to a cosine schedule, such that it alternates between an upper ($\tau_{+}$) and a lower ($\tau_{-}$) bound  at a fixed period length $T$:
\begin{align}
    \label{eq:cos_tau}
    \tau_{\cos}(t) &= (\tau_+-\tau_-) \times (1+\cos(2\pi\, t/T)) / 2 + \tau_-\; ;
    % 
\end{align}
here, $t$ denotes training epochs. This method is motivated by the observation that $\tau$ controls the trade-off between learning easily separable features and learning instance-specific features.
%

Arguably, however, the models should learn both types of features: \ie the representation space should be structured according to easily separable features that (optimally) represent semantically meaningful group-wise patterns, whilst still allowing for instance discrimination within those groups.

Therefore, we propose to \emph{alternate} between both objectives as in ~\cref{eq:cos_tau}, to ensure that throughout training the model learns to encode instance-specific patterns, whilst also structuring the representation space along semantically meaningful features.
Note that while we find a cosine schedule to work best and to be robust with respect to the choice for $T$ (\cref{subsec:ablations}), we also evaluate alternatives. Even randomly sampling $\tau$ from the interval $[\tau_-,\tau_+]$ improves the model performance.
This indicates that the \emph{task switching} between group-wise discrimination (large $\tau$) and instance discrimination (small $\tau$) is indeed the driving factor behind the performance improvements we observe.
% 

% \input{iclr2023/4_results}

\section{Experimental Results}
\label{sec:results}

In this section, we validate our hypothesis that simple manipulations of the temperature parameter in~\cref{eq:info-nce} lead to better performance for long-tailed data. First, we introduce our experimental setup in \cref{subsec:implementation}, then in \cref{subsec:results} we discuss the results across three imbalanced datasets and, finally, we analyse different design choices of the framework through extensive ablation studies in \cref{subsec:ablations}. 



\subsection{Implementation Details}
\label{subsec:implementation}
\myparagraph{Datasets.}
We consider long-tailed (LT) versions of the following three popular datasets for the experiments: CIFAR10-LT, CIFAR100-LT, and ImageNet100-LT. For most of the experiments, we follow the setting from SDCLR~\citep{jiang2021self}. In case of \textbf{CIFAR10-LT/CIFAR100-LT}, the original datasets~\citep{krizhevsky2009learning} consist of 60000  32x32 images sampled uniformly from 10 and 100 semantic classes, respectively, where 50000 images correspond to the training set and 10000 to a test set.
% 
Long-tail versions of the datasets are introduced by~\citet{cui2019class} and consist of a subset of the original datasets with an exponential decay in the number of images per class. The imbalance ratio controls the uniformity of the dataset and is calculated as the ratio of the sizes of the biggest and the smallest classes. By default, we use an imbalance ratio 100 if not stated otherwise. Experiments in \cref{table:cifar10_cifar100_imb100}, \cref{table:simclr-sdclr} are the average over three runs with different permutations of classes.
\textbf{ImageNet100-LT} is a subset of the original ImageNet-100~\citep{tian2020contrastive} consisting of 100 classes for a total of 12.21k 256x256 images. The number of images per class varies from 1280 to 25. 


\myparagraph{Training.}
We use an SGD optimizer for all experiments with a weight decay of 1e-4. As for the learning rate, we utilize linear warm-up for 10 epochs that is followed by a cosine annealing schedule starting from 0.5. We train for 2000 epochs for CIFAR10-LT and CIFAR100-LT and 800 epochs for ImageNet100-LT. For CIFAR10-LT and CIFAR100-LT we use a ResNet18~\citep{he2016deep} backbone.
For ImageNet100-LT we use a ResNet50~\citep{he2016deep} backbone. For both the MoCo~\citep{he2019moco} and the SimCLR~\citep{chen2020simple} experiments, we follow \citet{jiang2021self} and use the following augmentations: resized crop, color jitters, grey scale and horizontal flip. MoCo details: we use a dictionary of size 10000, a projection dimensionality of 128 and a projection head with one linear layer. SimCLR details: we train with a batch size of 512 and a projection head that has two layers with an output size of 128. For evaluation, we discard the projection head and apply l2-normalisation. Regarding the proposed \underline temperature \underline schedules (TS), we use a period length of $T\myeq400$ with $\tau_+\myeq1.0$ and $\tau_-\myeq0.1$ if not stated otherwise; for more details, see \cref{appendix:implementation_details}.


\myparagraph{Evaluation}
We use k nearest neighbours (kNN) and linear classifiers to assess the learned features. For kNN, we compute $l2$-normalised  distances between LT
%
samples from the train set and the class-balanced test set. For each test image, we assign it to the majority class among the top-k closest train images. We report accuracy for kNN with $k\myeq1$ (kNN@1) and with $k\myeq10$ (kNN@10). Compared to fine-tuning or linear probing, kNN directly evaluates the learned embedding since it relies on the learned metric and local structure of the space. 
%
We also evaluate the linear separability and generalisation of the space with a linear classifier that we train on the top of frozen backbone. For this, we consider two setups: balanced few-shot linear probing (FS LP) and long-tailed linear probing (LT LP). For FS LP, the few-shot train set is a direct subset of the original long-tailed train set with the shot number equal to the minimum class size in the original LT train set. For LT LP, we use the original LT training set. For extended tables, see \cref{sec:extend_results}.

\subsection{Effectiveness of Temperature Schedules}
\label{subsec:results}
\myparagraph{Contrastive learning with TS.}
In \cref{table:cifar10_cifar100_imb100} we present the efficacy of temperature schedules (TS) for two well-known contrastive learning frameworks MoCo~\citep{he2019moco} and SimCLR~\citep{chen2020simple}. We find that both frameworks benefit from varying the temperature and we observe consistent improvements
%
over all evaluation metrics for CIFAR10-LT and CIFAR100-LT,
% 
\ie
the local structure of the embedding space (kNN) and the global structure (linear probe) %that is captured by the linear probe 
are both improved. 
% 
Moreover, we show in \cref{table:simclr-sdclr} that our finding also transfers to ImageNet100-LT.
%
Furthermore, in \cref{table:moco_imb150} we evaluate the performance of the proposed method on the CIFAR10 and CIFAR100 datasets with different imbalance ratios. An imbalance ratio of 50 (imb50) reflects less pronounced imbalance, and imb150 corresponds to the datasets with only 30 (CIFAR10) and 3 (CIFAR100) samples for the smallest class. Varying $\tau$ during training improves the performance for different long-tailed data\rebuttal{; for a discussion on the dependence of the improvement on the imbalance ratio, please see the appendix}.
%
% \input{iclr2023/tables/cifar10_cifar100.tex}

\begin{table}[!ht]
\centering
\small
\tabcolsep=0.15cm
\begin{tabular}{c|cc|cc|cc|cc}  % 4 = method, model, gn(5/69), gb(64/69), hm
% \toprule
& \multicolumn{4}{c}{CIFAR10-LT}  & \multicolumn{4}{c}{CIFAR100-LT}  \\
method  & kNN@1 & kNN@10 & FS LP & LT LP & kNN@1 & kNN@10 & FS LP & LT LP  \\
\midrule
MoCo  & 63.54 & 64.56  & 69.31  & 65.11 & 28.69 & 28.75  & 26.86  & 30.41    \\
% MoCo + S$\tau$A \\
MoCo + TS   &  \textbf{64.99} &  \textbf{65.01} &  \textbf{72.87} &  \textbf{66.86}  &  \textbf{30.31} &  \textbf{29.75} &  \textbf{28.97} &  \textbf{32.05}  \\
\midrule
SimCLR  & 59.84 & 60.19 & 68.29 & 61.86 & 28.81 & 28.12  & 25.70  &  31.20  \\
SimCLR + TS  & \textbf{63.09} & \textbf{62.91} & \textbf{71.86} & \textbf{65.03} &  \textbf{31.06} & \textbf{30.06}  & \textbf{28.89}  & \textbf{33.28}  \\


% \bottomrule
\end{tabular}
\vspace{.25em}
\caption{\textbf{Effect of temperature scheduling.} Comparison of MoCo vs MoCo+TS and SimCLR vs SimCLR+TS on CIFAR10-LT and CIFAR100-LT with kNN, few-shot and long-tail linear probe (FS LP and LT LP). 
}
\label{table:cifar10_cifar100_imb100}
\vspace{-1em}
\end{table}

\begin{table}[!ht]
\centering
\small
\tabcolsep=0.15cm
\begin{tabular}{c|cc|cc|cc|cc}  % 4 = method, model, gn(5/69), gb(64/69), hm
% \toprule
& \multicolumn{4}{c|}{CIFAR-10-LT} &  \multicolumn{4}{c}{CIFAR-100-LT}   \\
% \midrule
& \multicolumn{2}{c|}{imb 50} & \multicolumn{2}{c|}{imb 150} &\multicolumn{2}{c|}{imb 50} & \multicolumn{2}{c}{imb 150}  \\
method  & kNN@10 & FS LP & kNN@10 & FS LP & kNN@10 & FS LP & kNN@10 & FS LP  \\
\midrule
MoCo & 69.12 & 74.16 & 59.13 & 65.76 & 32.22 & 33.53  & 25.36 & 22.73  \\
MoCo + TS & \textbf{71.49} & \textbf{76.37} & \textbf{60.83} & \textbf{68.59} & \textbf{33.24} & \textbf{35.03}  & \textbf{26.75} &  \textbf{22.78} \\


% \bottomrule
\end{tabular}
\vspace{.25em}
\caption{\textbf{Effect of imbalance ratio.} MoCo vs MoCo+TS on CIFAR10-LT and CIFAR100-LT for imbalance ratio 50 (imb50) and 150 (imb150). Evaluation metrics: kNN classifier and few-shot linear probe (FS LP). }
\label{table:moco_imb150}
\vspace{-1em}
\end{table}




% \input{iclr2023/tables/simclr_sdclr.tex}

\begin{table}[!h]
\centering
\small
\tabcolsep=0.15cm
\begin{tabular}{c|ccc|ccc|ccc}  % 4 = method, model, gn(5/69), gb(64/69), hm
% \toprule
& \multicolumn{3}{c}{CIFAR-10-LT} &  \multicolumn{3}{c}{CIFAR-100-LT} &  \multicolumn{3}{c}{ImageNet-100-LT}  \\
method  & kNN@10 & FS LP & LS LP & kNN@10 & FS LP & LT LP & kNN@10 & FS LP & LT LP \\
\midrule
SimCLR  & 60.19 & 68.29 & 61.68 & 28.12 & 25.70 & 31.20  & 38.00 &  42.64 & 44.82 \\
SDCLR  & 60.74 & 71.03 & 64.99 & 29.22 & 27.28 &  \textbf{34.23} & 37.36 &  42.74  &  46.40\\
SimCLR + TS & \textbf{62.91} & \textbf{71.86} & \textbf{65.03} & \textbf{30.06} & \textbf{28.89} & 33.28 & \textbf{38.86} & \textbf{45.18} & \textbf{47.26} \\

% \bottomrule
\end{tabular}

\caption{\textbf{Comparison with SDCLR.} SimCLR vs SDCLR vs SimCLR+TS on CIFAR10-LT, CIFAR100-LT, and ImageNet100-LT. Evaluation: kNN classifier, few-shot (FS LP) and long-tail linear probe (LT LP).}
\label{table:simclr-sdclr}
% \vspace{-mm}
\end{table}

\myparagraph{TS vs SDCLR.} 
%
Further, we compare our method with SDCLR~\citep{jiang2021self}. In SDCLR, %the authors propose to modify
SimCLR is modified s.t.~the embeddings of the online model are contrasted with those of a pruned version of the same model, which is updated after every epoch.
%
Since the pruning is done by simply masking the pruned weights of the original model, SDCLR requires twice as much memory compared to the original SimCLR and extra computational time to prune the model every epoch. %, see supplement for a comparison. 
In contrast, our method does not require any changes in the architecture or training. In \cref{table:simclr-sdclr} we show that 
%our simple method
this simple approach improves not only over the original SimCLR, but also over SDCLR in most metrics. 
% 

\subsection{Ablations}
\label{subsec:ablations}
In this section, we evaluate how the hyperparameters in~\cref{eq:cos_tau} can influence the model behaviour.

\myparagraph{Cosine Boundaries.}
%
First, we vary the lower $\tau_-$ and upper $\tau_+$ bounds of $\tau$ for the cosine schedule. In \cref{table:min_max_tau_cosine} we assess the performance of MoCo+TS with different $\tau_-$ and $\tau_+$ on CIFAR10 with FS LP. We observe a clear trend that with a wider range of $\tau$ values the performance increases. We attribute this to the ability of the model to learn better `hard' features with low $\tau$ and improve semantic structure for high $\tau$. 
Note that $0.07$ is the value for $\tau$ in many current contrastive learning methods. 
%
% \input{iclr2023/tables/ablations_cos_boundary.tex}

\begin{minipage}[b]{0.55\textwidth}
\centering
\small
\tabcolsep=0.15cm
\begin{tabular}{c|ccccc}  % 4 = method, model, gn(5/69), gb(64/69), hm
\toprule
\backslashbox{$\tau_-$}{$\tau_+$} & 0.2 & 0.3 & 0.4 & 0.5 & 1.0  \\
\midrule
0.07 & 69.46 & 68.86 & 71.29 & 71.83 & \textbf{73.26}  \\
0.1 & 68.17 & 70.34 & 71.25 & 72.31 &  72.87 \\
0.2 & 68.89 & 69.37 & 70.12 & 69.65 &  71.42 \\

\bottomrule
\end{tabular}
\captionof{table}{\textbf{Influence of cosine boundaries.} Best performance with the largest difference between $\tau_-$ and $\tau_+$. CIFAR10 with MoCo+TS, evaluating few-shot linear probes (FS LP).}
\label{table:min_max_tau_cosine}
\end{minipage}
\hfill
\begin{minipage}[b]{0.4\textwidth} % 0.35
\begin{minipage}[b]{0.45\textwidth}
% \kern0pt
    \small
    \centering
    \begin{tabular}{l|c}  % 4 = method, model, gn(5/69), gb(64/69), hm
    
    \toprule
    % & \multicolumn{1}{c}{CIFAR-10-LT}   \\
    
    \,\,\,\,\,\,\,TS  & FS LP\\
    \midrule
    \reddark{$\blacksquare$} fixed & 68.89 \\
    \greendark{$\blacksquare$} step &  70.18  \\
    \cyan{$\blacksquare$} rand &  70.26  \\
    \purple{$\blacksquare$} oscil &  71.50  \\
    \yellow{$\blacksquare$} cos &   \textbf{72.31}  \\
    
    \bottomrule
    \end{tabular}
    \par\kern0pt
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
\kern0pt
    \includegraphics[width=0.95\textwidth, height=2.3cm]{iclr2023/schedules3.pdf}
\vspace{-.075cm}
    % \par\kern0pt
\end{minipage}

\captionof{table}{\textbf{Alternative Schedules}. Constant, step function, and random sampling. All functions are bounded by $0.1$ and $0.5$.  
}
\label{table:alternatives}
% \vspace{-mm}
\end{minipage}



\myparagraph{Cosine Period.}
Further, we investigate if the length of the period $T$ in~\cref{eq:cos_tau} impacts the performance of the model. In \cref{table:periods}, we show that modifying the temperature $\tau$ based on the cosine schedule is beneficial during training independently of the period $T$. The %estimated 
performance varies insignificantly depending on $T$ and consistently improves over standard fixed $\tau\myeq0.2$, whereas the best performance we achieve with $T\myeq400$. Even though the performance is stable with respect to the length of the period, it changes within one period as we show in \cref{fig:periods}. 
Here, we average the accuracy of one last full period over different models trained with different $T$ and find that the models reach the best performance around $0.7\,T$. %For details, see supplement.
%Our recommendation for the hyperparameters includes a lower bound $\tau_-\myeq0.1$, upper bound $\tau_+\myin[0.5, 2.0]$, period length $T\myeq200$ epochs and 
Based on this observation, we recommend to stop training after $(n-0.3)\,T$ epochs, where $n$ is the number of full periods.
% \moritz{Is it not n-0.3 full periods?}



\begin{figure}[!h]
\begin{floatrow}
\capbtabbox[0.35\textwidth]{%
  \small
  \begin{tabular}{c|c|c} \toprule
 T &  T / $\#$epochs & FS LP \\ \midrule
 no & fixed $\tau$ & 68.89 \\
  200 & 0.1 & 71.86 \\
  400 & 0.2 & 72.87 \\
  1000 & 0.5  & 72.47 \\
  2000 & 1.0  & 72.22 \\
  4000 & 2.0  & 72.10 \\ \bottomrule
  \end{tabular}
}{%
% \vspace{1mm}
  \caption{\textbf{Influence of the period length $T$.} Few-shot linear probe accuracy (FS LP) of MoCo+TS on CIFAR10-LT.}%
  \label{table:periods}
}
\ffigbox[0.63\textwidth]{%
  \includegraphics[scale=0.3]{iclr2023/period.pdf}
}{%
  \caption{\textbf{Dependence on relative time of one period.} Blue: Average FS LP of last period of the models trained with $T=200,400, 1000, 2000$. Light blue: variance. Yellow: Relative cosine value over relative time. CIFAR10-LT trained with  MoCo+TS. }%
  \label{fig:periods}
}

\end{floatrow}
\end{figure}



\myparagraph{Alternatives to Cosine Schedule.}
Additionally, we test different methods of varying the temperature parameter $\tau$ and report the results in \cref{table:alternatives}: %As an alternative, 
we examine \rebuttal{a linearly oscillating (oscil) function}, a step function, and random sampling. % that we briefly discuss in \cref{subsec:CL_on_LT}. 
\rebuttal{For the linear oscillations, we follow the same schedule as for the cosine version, as shown on the right of \cref{table:alternatives}. } For the step function, we change $\tau$ from a low (0.1) to a high (0.5) value and back every 200 epochs. For random, we uniformly sample values for $\tau$ from the range [0.1, 0.5]. In \cref{table:alternatives} we observe that both those methods for varying the $\tau$ value also improve the performance over the fixed temperature, while with the cosine schedule the model achieves the best performance. These results indicate
%It confirms our hypothesis 
that it is indeed the \emph{task switching} between group-wise and instance-wise discrimination %changing the task
during training which is the driving factor for the observed improvements  %is beneficial 
for unsupervised long-tail representation learning. 
We assume the reason why slow oscillation of the temperature performs better than fast (\ie random) temperature changes is grounded in learning dynamics and the slow evolution of the embedding space during training. 
%



% \input{iclr2023/5_conclusion}

\section{Conclusion}
\label{sec:conclusion}

In this work, we discover the surprising effectiveness of temperature schedules for self-supervised contrastive representation learning on
%in application to 
imbalanced datasets. % for self-supervised contrastive learning.
In particular, we find that a simple cosine schedule for $\tau$ consistently improves two state-of-the-art contrastive methods over several datasets and different imbalance ratios, without introducing any additional cost.

Importantly, our approach is based on a novel perspective on the contrastive loss, in which the average distance maximisation aspect is emphasised.
This perspective sheds light on which samples dominate the contrastive loss and explains why large values for $\tau$ can lead to the emergence of tight clusters in the embedding space, despite the fact that individual instance \emph{always} repel each other. 

Specifically, we find that while a large $\tau$ is thus necessary to induce semantic structure, the concomitant focus on \emph{group-wise} discrimination biases the model to encode easily separable features rather than instance-specific details. However, in long-tailed distributions, this can be particularly harmful to the most infrequent classes, as those require a higher degree of instance discrimination to remain distinguishable from the prevalent semantic categories. % \moritz{semantic categories maybe a bit too strong?}
The proposed cosine schedule for $\tau$ overcomes this tension, by alternating between an emphasis on instance discrimination (small $\tau$) and group-wise discrimination (large $\tau$). As a result of this constant `task switching', the model is trained to both structure the embedding space according to semantically meaningful features, whilst also encoding instance-specific details such that rare classes remain distinguishable from dominant ones.


 
\clearpage
\section*{Ethics Statement}
The paper proposes an analysis and a method to improve the performance of self-supervised representation learning methods based on the contrastive loss. The method and investigation in this paper do not introduce any ethical issues to the field of representation learning, as it is decoupled from the training data. Nonetheless, we would like to point out that representation learning does not automatically prevent models from learning harmful biases from the training data and should not be used outside of research applications without thorough evaluation for fairness and bias.

\section*{Acknowledgements}
C.\ R.\ is supported by VisualAI EP/T028572/1 and ERC-UNION-CoG-101001212.


\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\clearpage
% \input{iclr2023/6_appendix}
\appendix
\section{Appendix}
\label{sec:appendix}

\subsection{Pseudo-Code for reproducibility of cosine schedule}
\begin{algorithm}
\caption{Cosine Schedule}\label{alg:cap}
\begin{algorithmic}
\Require period $T \geq 0$, $\tau_- = 0.1, \tau_+ = 1.0$
\State $ep \gets $ current epoch 
\State $tau \gets (\tau_+-\tau_-) \times (1+$np.cos$(2\times$np.pi$\,\times ep/T)) / 2 + \tau_-$
\end{algorithmic}
\end{algorithm}


Insert \cref{alg:cap} into your favourite contrastive learning framework to check it out!

\subsection{Implementation Details}
\label{appendix:implementation_details}

\myparagraph{Evaluation details.}
Following~\citet{jiang2021self}, we separate 5000 images for CIFAR10/100-LT as a validation set for each split. As we discussed in the main paper,
% \cref{subsec:ablations}, 
the performance of the model depends on the relative position within a period $T$. Therefore we utilise the validation split to choose a checkpoint for further testing on the standard test splits for CIFAR10/100-LT. Precisely, for each dataset, we select the evaluation epoch for the checkpoint based only on the validation set of the first random split; the other splits of the same dataset are evaluated using the same number of epochs. Note that for ImageNet100-LT there is no validation split and we select the last checkpoint as in~\citet{jiang2021self}. For a fair comparison, we also reproduce the numbers from~\cite{jiang2021self} in the same way. 


\myparagraph{Division into head, mid, and tail classes.}
Following~\citet{jiang2021self}, we divide all the classes into three categories: head classes are with the most number of samples, tail classes are with the least number of samples and mid are the rest. In particular, for CIFAR10-LT for each split there are 4 head classes, 3 mid classes, and 3 tail classes; for CIFAR100-LT there are 34 head classes, 33 mid classes, 33 tail classes; for ImageNet100-LT head classes are classes with more than 100 instances, tail classes have less than 20 instances per class, and mid are the rest. 

\subsection{Extended results}
\label{sec:extend_results}


\myparagraph{Extension of \cref{fig:spheres}}
In \cref{fig:supervised_tau} we provide full results of kNN accuracy on CIFAR10 when the model is trained with different fixed $\tau$ values and with coarse binary supervision. Especially tail classes are improved by instance discrimination (small $\tau_\mathrm{tail}$). 
%

\begin{figure}[!h]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[scale=0.3]{iclr2023/fig7.pdf}
\end{center}
\caption{kNN accuracy for CIFAR10-LT trained with MoCo. Comparison between $\tau=0.1$, $\tau=0.5$, $\tau=1.0$. [0.1, 1.0] denotes coarse binary supervision with $\tau_\mathrm{head}=1.0$ and $\tau_\mathrm{tail}=0.1$. MAA: mean average accuracy over all classes.}
\label{fig:supervised_tau}
\end{figure}

% 

\myparagraph{Head-mid-tail classes evaluation.}
In the following, we present a detailed comparison of SimCLR and SimCLR+TS on head, mid, and tail classes on CIFAR10-LT in \cref{table:cifar10-head-mid-tail}, on CIFAR100-LT in \cref{table:cifar100-head-mid-tail} and on ImageNet100-LT in \cref{table:imagenet100-head-mid-tail}. We observe consistent improvement for all evaluation metrics for all types of classes over the three datasets. 

% \input{iclr2023/tables/appendix_cifar10.tex}

\begin{table}[!h]
\centering
\small
\tabcolsep=0.15cm
\begin{tabular}{c|cccccc|cccccc}  % 4 = method, model, gn(5/69), gb(64/69), hm
\toprule
& \multicolumn{12}{c}{CIFAR-10-LT}  \\
& \multicolumn{6}{c|}{kNN@1} &  \multicolumn{6}{c}{kNN@10}  \\
method  & Head & & Mid & & Tail & &  Head & & Mid & & Tail & \\
% \multirow{2}{*}{$\lambda$}  & $N_{/J}$  & $B_{/J}$  & \multirow{2}{*}{$hm_{/J}$} \\
% &  (5/69) & (64/69) &  \\
\midrule
SimCLR & 84.93 & \scriptsize{$\pm$ 3.44} &  54.08 & \scriptsize{$\pm$ 4.24} &  32.14 & \scriptsize{$\pm$ 7.44} &  88.03  & \scriptsize{$\pm$ 3.32} &  53.76  & \scriptsize{$\pm$ 4.80}  &   29.52  & \scriptsize{$\pm$ 9.44} \\
SimCLR + TS & 87.24 & \scriptsize{$\pm$ 3.05} & 58.96  & \scriptsize{$\pm$ 5.21} & 35.02  & \scriptsize{$\pm$ 8.27} &  89.92  & \scriptsize{$\pm$ 2.97} &  59.31  & \scriptsize{$\pm$ 4.69}  &   30.51  & \scriptsize{$\pm$ 12.38} \\
\midrule
& \multicolumn{6}{c|}{FS LP} &  \multicolumn{6}{c}{LT LP}  \\
method  & Head & & Mid & & Tail & &  Head & & Mid & & Tail & \\
\midrule
SimCLR  &  76.38 & \scriptsize{$\pm$ 5.24} & 63.20	& \scriptsize{$\pm$ 2.95} & 62.60  & \scriptsize{$\pm$ 3.63} &  89.52 & \scriptsize{$\pm$ 3.15} & 56.98  & \scriptsize{$\pm$ 4.74}  &   29.88  & \scriptsize{$\pm$ 8.11} \\
SimCLR + TS &  80.54  & \scriptsize{$\pm$ 5.02} & 66.50   & \scriptsize{$\pm$ 4.38} & 65.67  & \scriptsize{$\pm$ 4.07} &  91.73  & \scriptsize{$\pm$ 2.49} &  62.09  & \scriptsize{$\pm$ 4.21}  &  32.38   & \scriptsize{$\pm$ 9.23} \\

\bottomrule
\end{tabular}

\caption{Detailed evaluation on CIFAR10-LT. Evaluation metrics: kNN@{1,10}, FS LP states for few-shot linear probe, and LT LP states for long-tail linear probe. We report the average performance with the standard deviation over three different random splits for different sets of classes: head, mid, and tail.  }
\label{table:cifar10-head-mid-tail}
% \vspace{-mm}
\end{table}


% \input{iclr2023/tables/appendix_cifar100.tex}

\begin{table}[!h]
\centering
\small
\tabcolsep=0.15cm
\begin{tabular}{c|cccccc|cccccc}  % 4 = method, model, gn(5/69), gb(64/69), hm
\toprule
& \multicolumn{12}{c}{CIFAR-100-LT}  \\
& \multicolumn{6}{c|}{kNN@1} &  \multicolumn{6}{c}{kNN@10}  \\
method  & Head & & Mid & & Tail & &  Head & & Mid & & Tail & \\
% \multirow{2}{*}{$\lambda$}  & $N_{/J}$  & $B_{/J}$  & \multirow{2}{*}{$hm_{/J}$} \\
% &  (5/69) & (64/69) &  \\
\midrule
SimCLR & 53.87 & \scriptsize{$\pm$  2.12} &  24.56 & \scriptsize{$\pm$ 1.51} &  7.26 & \scriptsize{$\pm$ 0.39} &  58.46  & \scriptsize{$\pm$ 1.79} &  22.15  & \scriptsize{$\pm$ 1.47}  &   2.83  & \scriptsize{$\pm$ 0.61} \\
SimCLR + TS & 57.14 & \scriptsize{$\pm$ 1.95} &  26.00 & \scriptsize{$\pm$ 1.20} &  8.31 & \scriptsize{$\pm$ 0.57} &  61.93  & \scriptsize{$\pm$ 1.88} &  24.22  & \scriptsize{$\pm$ 2.23}  &  3.05   & \scriptsize{$\pm$ 0.54} \\
\midrule
& \multicolumn{6}{c|}{FS LP} &  \multicolumn{6}{c}{LT LP}  \\
method  & Head & & Mid & & Tail & &  Head & & Mid & & Tail & \\
\midrule
SimCLR  & 33.48 & \scriptsize{$\pm$ 1.24} &  24.25 & \scriptsize{$\pm$ 2.12} &  19.12 & \scriptsize{$\pm$ 1.35} &  62.19  & \scriptsize{$\pm$ 1.80} &  26.56  & \scriptsize{$\pm$ 1.46}  &  3.92   & \scriptsize{$\pm$ 0.46} \\
SimCLR + TS & 37.5 & \scriptsize{$\pm$ 1.33} & 27.64  & \scriptsize{$\pm$ 1.95} & 21.26  & \scriptsize{$\pm$ 0.66} &  65.24  & \scriptsize{$\pm$ 2.04} &  29.20  & \scriptsize{$\pm$ 1.48}  &   4.42  & \scriptsize{$\pm$ 0.26} \\

\bottomrule
\end{tabular}

\caption{Detailed evaluation on CIFAR100-LT. Evaluation metrics: kNN@{1,10}, FS LP states for few-shot linear probe, and LT LP states for long-tail linear probe. We report the average performance with the standard deviation over three different random splits for different sets of classes: head, mid, and tail.}
\label{table:cifar100-head-mid-tail}
% \vspace{-mm}
\end{table}



% \input{iclr2023/tables/appendix_imagenet100.tex}

\begin{table}[!h]
\centering
\small
\tabcolsep=0.15cm
\begin{tabular}{c|cccccc|cccccc}  % 4 = method, model, gn(5/69), gb(64/69), hm
\toprule
& \multicolumn{12}{c}{ImageNet100-LT}  \\
& \multicolumn{6}{c|}{kNN@1} &  \multicolumn{6}{c}{kNN@10}  \\
method  & Head & & Mid & & Tail & &  Head & & Mid & & Tail & \\
% \multirow{2}{*}{$\lambda$}  & $N_{/J}$  & $B_{/J}$  & \multirow{2}{*}{$hm_{/J}$} \\
% &  (5/69) & (64/69) &  \\
\midrule
SimCLR      & 55.13 & \scriptsize{} & 30.00  & \scriptsize{} & 10.71  & \scriptsize{} &  58.51  & \scriptsize{} &  29.70  & \scriptsize{}  &  8.71   & \scriptsize{} \\
SimCLR + TS & 57.23 & \scriptsize{} &  30.26 & \scriptsize{} & 13.14  & \scriptsize{} &  60.41  & \scriptsize{} &  29.53  & \scriptsize{}  &  10.14   & \scriptsize{} \\
\midrule
& \multicolumn{6}{c|}{FS LP} &  \multicolumn{6}{c}{LT LP}  \\
method  & Head & & Mid & & Tail & &  Head & & Mid & & Tail & \\
\midrule
SimCLR      & 51.79 & \scriptsize{} &  36.77 & \scriptsize{} & 30.29 & \scriptsize{} &  67.59  & \scriptsize{} &  36.47  & \scriptsize{}  &   9.43  & \scriptsize{} \\
SimCLR + TS & 60.41 & \scriptsize{} &  40.38 & \scriptsize{} &  33.57 & \scriptsize{} &  70.67  & \scriptsize{} &   38.85 & \scriptsize{}  &   10.29  & \scriptsize{} \\

\bottomrule
\end{tabular}

\caption{Detailed evaluation on ImageNet100-LT. Evaluation metrics: kNN@{1,10}, FS LP states for few-shot linear probe, and LT LP states for long-tail linear probe. We report the average performance for different sets of classes: head, mid, and tail.}
\label{table:imagenet100-head-mid-tail}
% \vspace{-mm}
\end{table}

% \input{iclr2023/tables/uniform_vs_long-tail.tex}

\begin{table}[!ht]
\centering
\small
\tabcolsep=0.15cm
\begin{tabular}{c|cc|cc|cc|cc}  % 4 = method, model, gn(5/69), gb(64/69), hm
\toprule
& \multicolumn{4}{c}{CIFAR10-Uniform}  & \multicolumn{4}{c}{CIFAR10-LT}  \\
method  & kNN@1 & kNN@10 & FS LP & LT LP & kNN@1 & kNN@10 & FS LP & LT LP  \\
% \multirow{2}{*}{$\lambda$}  & $N_{/J}$  & $B_{/J}$  & \multirow{2}{*}{$hm_{/J}$} \\
% &  (5/69) & (64/69) &  \\
\midrule
MoCo  & 83.47 & 84.87  & \textbf{90.19} & \textbf{87.70} & 63.00 & 64.10  & 68.89  & 63.99    \\

MoCo + TS   &  \textbf{83.78} &  \textbf{85.85} &  90.02 &  87.40  &  \textbf{65.68} &  \textbf{65.91} &  \textbf{72.31} &  \textbf{66.64}  \\

\bottomrule
\end{tabular}

\caption{\textbf{Influence of TS on uniform vs long-tailed distribution.} Comparison of MoCo vs MoCo+TS  on CIFAR10-Uniform and CIFAR-LT-imb100, one split.  Evaluation metrics: kNN classifier, FS LP denotes few-shot linear probe, LT LP denotes long-tail linear probe. 
}
\label{table:uniform-vs-ong-tail}
\end{table}


\rebuttal{\myparagraph{Influence of TS on Uniform vs Long-Tailed Distributions.}
To further corroborate that TS particularly helpful for imbalanced data, we apply TS for the uniformly distributed data. 
In \cref{table:uniform-vs-ong-tail}, we can observe that the cosine schedule yields significant and consistent gains for the long-tailed version of CIFAR10 (CIFAR10-LT), but not for the uniform one (CIFAR10-Uniform). 
We assume that both head classes and tail classes for long-tail distribution should be expected to benefit from a better separation between the two: on the one hand, the tail classes form better clusters and are thus easier to classify based on their neighbours, on the other hand, the clusters of the head classes are 'purified', which should similarly improve performance. Weather, for the uniform distribution, we do not observe such influence of TS and the performance changes only marginally. 
}




% \input{iclr2023/6_appendix_positives}
% 
\subsection{Influence of the positive samples on contrastive learning}
\label{appendix:positives}
In \cref{subsec:max_margin}, we particularly focused on the impact of the \emph{negative samples} on the learning dynamics under the contrastive objective, as they likely are the driving factor with respect to the semantic structure. In fact, we find that the positive samples should have an inverse relation with the temperature $\tau$ and thus cannot explain the observed learning dynamics, as we discuss in the following.
%. In the following, we additionally discuss how the positive samples affect the representation learning via the factor $c_{ii}$ in \cref{eq:margin_loss}.

To understand the impact of the \emph{positive samples}, first note their role in the loss (same as \cref{eq:margin_loss}): 
\begin{equation}
    \label{eq:margin_loss_pos}
    \mathcal L^i_\text{c} 
    = \log\left(1+c_{ii}S_i\right) \, .
\end{equation}
In particular, $c_{ii}$ scales the entire sum $S_i\myeq\sum_{j\neq i}\exp (-d_{ij})$. As such, encoding two augmentations of the same instance at a large distance is much more `costly' for the model than encoding two different samples close to each other, as each and every summand $S_i$ is amplified by the corresponding $c_{ii}$. As a result, the model will be biased to `err on the safe side' and become invariant to the augmentations, which has been one of the main motivations for introducing augmentations in contrastive learning in the first place, cf.~\cite{tian2020makes, chen2020simple, caron2020unsupervised}.


Consequently, the positive samples, of course, also influence the forming of clusters in the embedding space as they induce invariance with respect to augmentations. Note, however, that this does not contradict our analysis regarding the impact of negative samples, but rather corroborates it. 

In particular, $c_{ii}$ biases the model to become invariant to the applied augmentations for all values of $\tau$; in fact, for small $\tau$, this invariance is even emphasised as $c_{ii}$ increases for small $\tau$ and the influence of the negatives is diminished.
Hence, if the augmentations were the main factor in inducing semantic structure in the embedding space, $\tau$ should have the opposite effect of the one we and many others \citep{wang2021understanding, zhang2022dual, zhang2021temperature} observe. 

Thus, instead of inducing semantic structure on their own, we believe the positive samples to rather play a critical role in influencing which features the model can rely on for grouping samples in the embedding space; for a detailed discussion of this phenomenon, see also \cite{chen2021intriguing}.



\end{document}



