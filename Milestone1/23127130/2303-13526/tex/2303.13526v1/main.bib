@article{McCradden,
	title = {When is accuracy off-target?},
	volume = {11},
	issn = {2158-3188},
	url = {http://www.nature.com/articles/s41398-021-01479-4},
	doi = {10.1038/s41398-021-01479-4},
	language = {en},
	number = {1},
	urldate = {2021-09-16},
	journal = {Translational Psychiatry},
	author = {McCradden, Melissa D.},
	month = jun,
	year = {2021},
	pages = {369},
	file = {McCradden_2021_When is accuracy off-target.pdf:/Users/neilnatarajan/Zotero/storage/KISMSTP2/McCradden_2021_When is accuracy off-target.pdf:application/pdf;Markup:/Users/neilnatarajan/Zotero/storage/P7MBUHQC/When is accuracy off-target.pdf:application/pdf},
}

@inproceedings{Jacovi-et-al,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {Formalizing {Trust} in {Artificial} {Intelligence}: {Prerequisites}, {Causes} and {Goals} of {Human} {Trust} in {AI}},
	isbn = {978-1-4503-8309-7},
	shorttitle = {Formalizing {Trust} in {Artificial} {Intelligence}},
	url = {https://doi.org/10.1145/3442188.3445923},
	doi = {10.1145/3442188.3445923},
	abstract = {Trust is a central component of the interaction between people and AI, in that 'incorrect' levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in AI? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we promote them, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, interpersonal trust (i.e., trust between people) as defined by sociologists. This model rests on two key properties: the vulnerability of the user; and the ability to anticipate the impact of the AI model's decisions. We incorporate a formalization of 'contractual trust', such that trust between a user and an AI model is trust that some implicit or explicit contract will hold, and a formalization of 'trustworthiness' (that detaches from the notion of trustworthiness in sociology), and with it concepts of 'warranted' and 'unwarranted' trust. We present the possible causes of warranted trust as intrinsic reasoning and extrinsic behavior, and discuss how to design trustworthy AI, how to evaluate whether trust has manifested, and whether it is warranted. Finally, we elucidate the connection between trust and XAI using our formalization.},
	urldate = {2021-10-15},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Jacovi, Alon and Marasović, Ana and Miller, Tim and Goldberg, Yoav},
	month = mar,
	year = {2021},
	keywords = {\_tablet, artificial intelligence, contractual trust, distrust, formalization, sociology, trust, trustworthy, warranted trust},
	pages = {624--635},
	file = {Jacovi et al_2021_Formalizing Trust in Artificial Intelligence.pdf:/Users/neilnatarajan/Zotero/storage/R729TD7X/Jacovi et al_2021_Formalizing Trust in Artificial Intelligence.pdf:application/pdf;Jacovi et al_2021_Formalizing Trust in Artificial Intelligence.pdf:/Users/neilnatarajan/Zotero/storage/IGT2LYYQ/Jacovi et al_2021_Formalizing Trust in Artificial Intelligence.pdf:application/pdf},
}


@article{Pieters,
	title = {Explanation and trust: what to tell the user in security and {AI}?},
	volume = {13},
	issn = {1572-8439},
	shorttitle = {Explanation and trust},
	url = {https://doi.org/10.1007/s10676-010-9253-3},
	doi = {10.1007/s10676-010-9253-3},
	abstract = {There is a common problem in artificial intelligence (AI) and information security. In AI, an expert system needs to be able to justify and explain a decision to the user. In information security, experts need to be able to explain to the public why a system is secure. In both cases, an important goal of explanation is to acquire or maintain the users’ trust. In this paper, I investigate the relation between explanation and trust in the context of computing science. This analysis draws on literature study and concept analysis, using elements from system theory as well as actor-network theory. I apply the conceptual framework to both AI and information security, and show the benefit of the framework for both fields by means of examples. The main focus is on expert systems (AI) and electronic voting systems (security). Finally, I discuss consequences of the analysis for ethics in terms of (un)informed consent and dissent, and the associated division of responsibilities.},
	language = {en},
	number = {1},
	urldate = {2022-01-21},
	journal = {Ethics and Information Technology},
	author = {Pieters, Wolter},
	month = mar,
	year = {2011},
	pages = {53--64},
	file = {Springer Full Text PDF:/Users/neilnatarajan/Zotero/storage/YIIRHZIQ/Pieters - 2011 - Explanation and trust what to tell the user in se.pdf:application/pdf},
}

@article{Chun-Tie-et-al,
	title = {Grounded theory research: {A} design framework for novice researchers},
	volume = {7},
	journal = {Open Medicine},
	author = {Chun Tie, Ylona and Birks, Melanie and Francis, Karen},
	month = jan,
	year = {2019},
	pages = {1--8},
}

@misc{Dua-and-Graff,
	title = {{UCI} {Machine} {Learning} {Repository}},
	url = {http://archive.ics.uci.edu/ml},
	publisher = {University of California, Irvine, School of Information and Computer Sciences},
	author = {Dua, Dheeru and Graff, Casey},
	year = {2017},
}

@article{Lundberg-et-al,
	title = {Consistent {Individualized} {Feature} {Attribution} for {Tree} {Ensembles}},
	volume = {abs/1802.03888},
	url = {http://arxiv.org/abs/1802.03888},
	journal = {CoRR},
	author = {Lundberg, Scott M. and Erion, Gabriel G. and Lee, Su-In},
	year = {2018},
	note = {\_eprint: 1802.03888},
}

@article{Miller,
	title = {Explanation in {Artificial} {Intelligence}: {Insights} from the {Social} {Sciences}},
	volume = {abs/1706.07269},
	url = {http://arxiv.org/abs/1706.07269},
	journal = {CoRR},
	author = {Miller, Tim},
	year = {2017},
	note = {\_eprint: 1706.07269},
}

@book{Molnar,
	title = {Interpretable {Machine} {Learning}: {A} {Guide} for {Making} {Black} {Box} {Models} {Explainable}},
	author = {Molnar, Christoph},
	year = {2019},
	annote = {https://christophm.github.io/interpretable-ml-book/},
}

@article{Mothilal-et-al,
	title = {Explaining {Machine} {Learning} {Classifiers} through {Diverse} {Counterfactual} {Explanations}},
	volume = {abs/1905.07697},
	url = {http://arxiv.org/abs/1905.07697},
	journal = {CoRR},
	author = {Mothilal, Ramaravind Kommiya and Sharma, Amit and Tan, Chenhao},
	year = {2019},
	note = {\_eprint: 1905.07697},
}

@article{Rudin-and-Radin,
	title = {Why {Are} {We} {Using} {Black} {Box} {Models} in {AI} {When} {We} {Don}’t {Need} {To}? {A} {Lesson} {From} {An} {Explainable} {AI} {Competition}},
	volume = {1},
	url = {https://hdsr.mitpress.mit.edu/pub/f9kuryi8},
	doi = {10.1162/99608f92.5a8a3a3d},
	number = {2},
	journal = {Harvard Data Science Review},
	author = {Rudin, Cynthia and Radin, Joanna},
	month = nov,
	year = {2019},
	annote = {https://hdsr.mitpress.mit.edu/pub/f9kuryi8},
}

@article{Wachter-et-al,
	title = {Counterfactual {Explanations} without {Opening} the {Black} {Box}: {Automated} {Decisions} and the {GDPR}},
	volume = {abs/1711.00399},
	url = {http://arxiv.org/abs/1711.00399},
	journal = {CoRR},
	author = {Wachter, Sandra and Mittelstadt, Brent D. and Russell, Chris},
	year = {2017},
	note = {\_eprint: 1711.00399},
}

@incollection{Woodward,
	edition = {Spring 2021},
	title = {Scientific {Explanation}},
	url = {https://plato.stanford.edu/archives/spr2021/entries/scientific-explanation/},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Woodward, James},
	editor = {Zalta, Edward N.},
	year = {2021},
}

@inproceedings{Ribeiro-et-al-3,
	address = {San Francisco California USA},
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939778},
	doi = {10.1145/2939672.2939778},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.},
	language = {en},
	urldate = {2022-01-21},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	pages = {1135--1144},
	file = {Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:/Users/neilnatarajan/Zotero/storage/X6X63T57/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:application/pdf},
}

@inproceedings{Ribeiro-et-al,
	title = {Anchors: {High}-{Precision} {Model}-{Agnostic} {Explanations}},
	shorttitle = {Anchors},
	abstract = {This work introduces a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, “sufficient” conditions for predictions, and proposes an algorithm to efficiently compute these explanations for any black-box model with high probability guarantees. We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, “sufficient” conditions for predictions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
	booktitle = {{AAAI}},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year = {2018},
}

@article{Ribeiro-et-al-2,
	title = {Nothing {Else} {Matters}: {Model}-{Agnostic} {Explanations} {By} {Identifying} {Prediction} {Invariance}},
	shorttitle = {Nothing {Else} {Matters}},
	url = {http://arxiv.org/abs/1611.05817},
	abstract = {At the core of interpretable machine learning is the question of whether humans are able to make accurate predictions about a model's behavior. Assumed in this question are three properties of the interpretable output: coverage, precision, and effort. Coverage refers to how often humans think they can predict the model's behavior, precision to how accurate humans are in those predictions, and effort is either the up-front effort required in interpreting the model, or the effort required to make predictions about a model's behavior. In this work, we propose anchor-LIME (aLIME), a model-agnostic technique that produces high-precision rule-based explanations for which the coverage boundaries are very clear. We compare aLIME to linear LIME with simulated experiments, and demonstrate the flexibility of aLIME with qualitative examples from a variety of domains and tasks.},
	urldate = {2022-01-05},
	journal = {arXiv:1611.05817 [cs, stat]},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.05817},
	keywords = {\_tablet, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems},
	file = {Ribeiro et al_2016_Nothing Else Matters.pdf:/Users/neilnatarajan/Zotero/storage/NCWBRPW5/Ribeiro et al_2016_Nothing Else Matters.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/36RCH2M7/1611.html:text/html},
}

@article{Sokol-and-Flach,
	title = {Explainability {Fact} {Sheets}: {A} {Framework} for {Systematic} {Assessment} of {Explainable} {Approaches}},
	shorttitle = {Explainability {Fact} {Sheets}},
	url = {http://arxiv.org/abs/1912.05100},
	doi = {10.1145/3351095.3372870},
	abstract = {Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.},
	urldate = {2022-01-05},
	journal = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	author = {Sokol, Kacper and Flach, Peter},
	month = jan,
	year = {2020},
	note = {arXiv: 1912.05100},
	keywords = {\_tablet, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {56--67},
	annote = {Comment: Conference on Fairness, Accountability, and Transparency (FAT* '20), January 27-30, 2020, Barcelona, Spain},
	file = {Sokol_Flach_2020_Explainability Fact Sheets.pdf:/Users/neilnatarajan/Zotero/storage/QEN8DP73/Sokol_Flach_2020_Explainability Fact Sheets.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/EF7VNKR6/1912.html:text/html},
}

@article{Sundararajan-and-Najmi,
	title = {The many {Shapley} values for model explanation},
	url = {http://arxiv.org/abs/1908.08474},
	abstract = {The Shapley value has become a popular method to attribute the prediction of a machine-learning model on an input to its base features. The use of the Shapley value is justified by citing [16] showing that it is the {\textbackslash}emph\{unique\} method that satisfies certain good properties ({\textbackslash}emph\{axioms\}). There are, however, a multiplicity of ways in which the Shapley value is operationalized in the attribution problem. These differ in how they reference the model, the training data, and the explanation context. These give very different results, rendering the uniqueness result meaningless. Furthermore, we find that previously proposed approaches can produce counterintuitive attributions in theory and in practice---for instance, they can assign non-zero attributions to features that are not even referenced by the model. In this paper, we use the axiomatic approach to study the differences between some of the many operationalizations of the Shapley value for attribution, and propose a technique called Baseline Shapley (BShap) that is backed by a proper uniqueness result. We also contrast BShap with Integrated Gradients, another extension of Shapley value to the continuous setting.},
	urldate = {2022-01-05},
	journal = {arXiv:1908.08474 [cs, econ]},
	author = {Sundararajan, Mukund and Najmi, Amir},
	month = feb,
	year = {2020},
	note = {arXiv: 1908.08474},
	keywords = {\_tablet, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Economics - Theoretical Economics},
	annote = {Comment: 9 pages},
	file = {Sundararajan_Najmi_2020_The many Shapley values for model explanation.pdf:/Users/neilnatarajan/Zotero/storage/552GV96A/Sundararajan_Najmi_2020_The many Shapley values for model explanation.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/YLSBC8Y5/1908.html:text/html},
}

@article{Barocas-et-al,
	title = {The {Hidden} {Assumptions} {Behind} {Counterfactual} {Explanations} and {Principal} {Reasons}},
	url = {http://arxiv.org/abs/1912.04930},
	doi = {10.1145/3351095.3372830},
	abstract = {Counterfactual explanations are gaining prominence within technical, legal, and business circles as a way to explain the decisions of a machine learning model. These explanations share a trait with the long-established "principal reason" explanations required by U.S. credit laws: they both explain a decision by highlighting a set of features deemed most relevant--and withholding others. These "feature-highlighting explanations" have several desirable properties: They place no constraints on model complexity, do not require model disclosure, detail what needed to be different to achieve a different decision, and seem to automate compliance with the law. But they are far more complex and subjective than they appear. In this paper, we demonstrate that the utility of feature-highlighting explanations relies on a number of easily overlooked assumptions: that the recommended change in feature values clearly maps to real-world actions, that features can be made commensurate by looking only at the distribution of the training data, that features are only relevant to the decision at hand, and that the underlying model is stable over time, monotonic, and limited to binary outcomes. We then explore several consequences of acknowledging and attempting to address these assumptions, including a paradox in the way that feature-highlighting explanations aim to respect autonomy, the unchecked power that feature-highlighting explanations grant decision makers, and a tension between making these explanations useful and the need to keep the model hidden. While new research suggests several ways that feature-highlighting explanations can work around some of the problems that we identify, the disconnect between features in the model and actions in the real world--and the subjective choices necessary to compensate for this--must be understood before these techniques can be usefully implemented.},
	urldate = {2022-01-05},
	journal = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	author = {Barocas, Solon and Selbst, Andrew D. and Raghavan, Manish},
	month = jan,
	year = {2020},
	note = {arXiv: 1912.04930},
	keywords = {\_tablet, Computer Science - Computers and Society},
	pages = {80--89},
	file = {Barocas et al_2020_The Hidden Assumptions Behind Counterfactual Explanations and Principal Reasons.pdf:/Users/neilnatarajan/Zotero/storage/7UCITN3E/Barocas et al_2020_The Hidden Assumptions Behind Counterfactual Explanations and Principal Reasons.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/ID6DMXNL/1912.html:text/html},
}

@article{Doshi-Velez-and-Kim,
	title = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1702.08608},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	urldate = {2022-01-17},
	journal = {arXiv:1702.08608 [cs, stat]},
	author = {Doshi-Velez, Finale and Kim, Been},
	month = mar,
	year = {2017},
	note = {arXiv: 1702.08608},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Doshi-Velez_Kim_2017_Towards A Rigorous Science of Interpretable Machine Learning.pdf:/Users/neilnatarajan/Zotero/storage/BCP6WVRJ/Doshi-Velez_Kim_2017_Towards A Rigorous Science of Interpretable Machine Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/RHR7VDNR/1702.html:text/html},
}

@article{Kumar-et-al,
	title = {Problems with {Shapley}-value-based explanations as feature importance measures},
	url = {http://arxiv.org/abs/2002.11097},
	abstract = {Game-theoretic formulations of feature importance have become popular as a way to "explain" machine learning models. These methods define a cooperative game between the features of a model and distribute influence among these input elements using some form of the game's unique Shapley values. Justification for these methods rests on two pillars: their desirable mathematical properties, and their applicability to specific motivations for explanations. We show that mathematical problems arise when Shapley values are used for feature importance and that the solutions to mitigate these necessarily induce further complexity, such as the need for causal reasoning. We also draw on additional literature to argue that Shapley values do not provide explanations which suit human-centric goals of explainability.},
	urldate = {2022-01-05},
	journal = {arXiv:2002.11097 [cs, stat]},
	author = {Kumar, I. Elizabeth and Venkatasubramanian, Suresh and Scheidegger, Carlos and Friedler, Sorelle},
	month = jun,
	year = {2020},
	note = {arXiv: 2002.11097},
	keywords = {\_tablet, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted to ICML 2020},
	file = {Kumar et al_2020_Problems with Shapley-value-based explanations as feature importance measures.pdf:/Users/neilnatarajan/Zotero/storage/I4WGRVEL/Kumar et al_2020_Problems with Shapley-value-based explanations as feature importance measures.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/HYXCE9H2/2002.html:text/html},
}

@article{Colquitt-et-al,
	title = {Justice at the {Millennium}: {A} {Meta}-{Analytic} {Review} of 25 {Years} of {Organizational} {Justice} {Research}},
	volume = {86},
	shorttitle = {Justice at the {Millennium}},
	doi = {10.1037//0021-9010.86.3.425},
	abstract = {The field of organizational justice continues to be marked by several important research questions, including the size of relationships among justice dimensions, the relative importance of different justice criteria, and the unique effects of justice dimensions on key outcomes. To address such questions, the authors conducted a meta-analytic review of 183 justice studies. The results suggest that although different justice dimensions are moderately to highly related, they contribute incremental variance explained in fairness perceptions. The results also illustrate the overall and unique relationships among distributive, procedural, interpersonal, and informational justice and several organizational outcomes (e.g., job satisfaction, organizational commitment, evaluation of authority, organizational citizenship behavior, withdrawal, performance). These findings are reviewed in terms of their implications for future research on organizational justice.},
	journal = {The Journal of applied psychology},
	author = {Colquitt, Jason and Conlon, Donald and Wesson, Michael and Porter, Christopher and Ng, K.},
	month = jul,
	year = {2001},
	keywords = {\_tablet},
	pages = {425--45},
	file = {Colquitt et al_2001_Justice at the Millennium.pdf:/Users/neilnatarajan/Zotero/storage/KQKMQ8V3/Colquitt et al_2001_Justice at the Millennium.pdf:application/pdf},
}

@article{Jacobs-et-al,
	title = {How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2158-3188},
	shorttitle = {How machine-learning recommendations influence clinician treatment selections},
	url = {https://www.nature.com/articles/s41398-021-01224-x},
	doi = {10.1038/s41398-021-01224-x},
	abstract = {Decision support systems embodying machine learning models offer the promise of an improved standard of care for major depressive disorder, but little is known about how clinicians’ treatment decisions will be influenced by machine learning recommendations and explanations. We used a within-subject factorial experiment to present 220 clinicians with patient vignettes, each with or without a machine-learning (ML) recommendation and one of the multiple forms of explanation. We found that interacting with ML recommendations did not significantly improve clinicians’ treatment selection accuracy, assessed as concordance with expert psychopharmacologist consensus, compared to baseline scenarios in which clinicians made treatment decisions independently. Interacting with incorrect recommendations paired with explanations that included limited but easily interpretable information did lead to a significant reduction in treatment selection accuracy compared to baseline questions. These results suggest that incorrect ML recommendations may adversely impact clinician treatment selections and that explanations are insufficient for addressing overreliance on imperfect ML algorithms. More generally, our findings challenge the common assumption that clinicians interacting with ML tools will perform better than either clinicians or ML algorithms individually.},
	language = {en},
	number = {1},
	urldate = {2022-01-15},
	journal = {Translational Psychiatry},
	author = {Jacobs, Maia and Pradier, Melanie F. and McCoy, Thomas H. and Perlis, Roy H. and Doshi-Velez, Finale and Gajos, Krzysztof Z.},
	month = feb,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Depression;Scientific community
Subject\_term\_id: depression;scientific-community},
	keywords = {Depression, Scientific community},
	pages = {1--9},
	file = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/7QE4ZB89/Jacobs et al. - 2021 - How machine-learning recommendations influence cli.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/CSZAA6D6/s41398-021-01224-x.html:text/html},
}


@article{Ustun-et-al,
	title = {Actionable {Recourse} in {Linear} {Classification}},
	url = {http://arxiv.org/abs/1809.06514},
	doi = {10.1145/3287560.3287566},
	abstract = {Machine learning models are increasingly used to automate decisions that affect humans - deciding who should receive a loan, a job interview, or a social service. In such applications, a person should have the ability to change the decision of a model. When a person is denied a loan by a credit score, for example, they should be able to alter its input variables in a way that guarantees approval. Otherwise, they will be denied the loan as long as the model is deployed. More importantly, they will lack the ability to influence a decision that affects their livelihood. In this paper, we frame these issues in terms of recourse, which we define as the ability of a person to change the decision of a model by altering actionable input variables (e.g., income vs. age or marital status). We present integer programming tools to ensure recourse in linear classification problems without interfering in model development. We demonstrate how our tools can inform stakeholders through experiments on credit scoring problems. Our results show that recourse can be significantly affected by standard practices in model development, and motivate the need to evaluate recourse in practice.},
	urldate = {2022-01-05},
	journal = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
	author = {Ustun, Berk and Spangher, Alexander and Liu, Yang},
	month = jan,
	year = {2019},
	note = {arXiv: 1809.06514},
	keywords = {\_tablet, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {10--19},
	annote = {Comment: Extended version. ACM Conference on Fairness, Accountability and Transparency [FAT2019]},
	file = {Ustun et al_2019_Actionable Recourse in Linear Classification.pdf:/Users/neilnatarajan/Zotero/storage/33INW2LS/Ustun et al_2019_Actionable Recourse in Linear Classification.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/Y6ISCE6J/1809.html:text/html},
}

@article{Friedrich-and-Zanker,
	title = {A {Taxonomy} for {Generating} {Explanations} in {Recommender} {Systems}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2371-9621},
	url = {https://ojs.aaai.org/index.php/aimagazine/article/view/2365},
	doi = {10.1609/aimag.v32i3.2365},
	abstract = {In recommender systems, explanations serve as an additional type of information that can help users to better understand the system's output and promote objectives such as trust, confidence in decision making or utility. This article proposes a taxonomy to categorize and review the research in the area of explanations. It provides a unified view on the different recommendation paradigms, allowing similarities and differences to be clearly identified. Finally, the authors present their view on open research issues and opportunities for future work on this topic.},
	language = {en},
	number = {3},
	urldate = {2022-01-05},
	journal = {AI Magazine},
	author = {Friedrich, Gerhard and Zanker, Markus},
	month = jun,
	year = {2011},
	note = {Number: 3},
	keywords = {\_tablet},
	pages = {90--98},
	file = {Friedrich_Zanker_2011_A Taxonomy for Generating Explanations in Recommender Systems.pdf:/Users/neilnatarajan/Zotero/storage/KNRSFJBW/Friedrich_Zanker_2011_A Taxonomy for Generating Explanations in Recommender Systems.pdf:application/pdf},
}

@article{Vilone-and-Longo,
	title = {Explainable {Artificial} {Intelligence}: a {Systematic} {Review}},
	shorttitle = {Explainable {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2006.00093},
	abstract = {Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions.},
	urldate = {2022-01-05},
	journal = {arXiv:2006.00093 [cs]},
	author = {Vilone, Giulia and Longo, Luca},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.00093},
	keywords = {\_tablet, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.0, I.2.6, I.2.m},
	annote = {Comment: 78 pages, 18 figures, journal paper to be submitted to Information Fusion},
	file = {Vilone_Longo_2020_Explainable Artificial Intelligence.pdf:/Users/neilnatarajan/Zotero/storage/KXUTCJLA/Vilone_Longo_2020_Explainable Artificial Intelligence.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/GLMKTK42/2006.html:text/html},
}

@incollection{Rader-et-al,
	address = {New York, NY, USA},
	title = {Explanations as {Mechanisms} for {Supporting} {Algorithmic} {Transparency}},
	isbn = {978-1-4503-5620-6},
	url = {https://doi.org/10.1145/3173574.3173677},
	abstract = {Transparency can empower users to make informed choices about how they use an algorithmic decision-making system and judge its potential consequences. However, transparency is often conceptualized by the outcomes it is intended to bring about, not the specifics of mechanisms to achieve those outcomes. We conducted an online experiment focusing on how different ways of explaining Facebook's News Feed algorithm might affect participants' beliefs and judgments about the News Feed. We found that all explanations caused participants to become more aware of how the system works, and helped them to determine whether the system is biased and if they can control what they see. The explanations were less effective for helping participants evaluate the correctness of the system's output, and form opinions about how sensible and consistent its behavior is. We present implications for the design of transparency mechanisms in algorithmic decision-making systems based on these results.},
	urldate = {2022-01-05},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Rader, Emilee and Cotter, Kelley and Cho, Janghee},
	month = apr,
	year = {2018},
	keywords = {\_tablet, algorithmic decision-making, explanations, transparency},
	pages = {1--13},
	file = {Rader et al_2018_Explanations as Mechanisms for Supporting Algorithmic Transparency.pdf:/Users/neilnatarajan/Zotero/storage/CY7PY3LM/Rader et al_2018_Explanations as Mechanisms for Supporting Algorithmic Transparency.pdf:application/pdf},
}

@article{Binns-et-al,
	title = {'{It}'s {Reducing} a {Human} {Being} to a {Percentage}'; {Perceptions} of {Justice} in {Algorithmic} {Decisions}},
	url = {http://arxiv.org/abs/1801.10408},
	doi = {10.1145/3173574.3173951},
	abstract = {Data-driven decision-making consequential to individuals raises important questions of accountability and justice. Indeed, European law provides individuals limited rights to 'meaningful information about the logic' behind significant, autonomous decisions such as loan approvals, insurance quotes, and CV filtering. We undertake three experimental studies examining people's perceptions of justice in algorithmic decision-making under different scenarios and explanation styles. Dimensions of justice previously observed in response to human decision-making appear similarly engaged in response to algorithmic decisions. Qualitative analysis identified several concerns and heuristics involved in justice perceptions including arbitrariness, generalisation, and (in)dignity. Quantitative analysis indicates that explanation styles primarily matter to justice perceptions only when subjects are exposed to multiple different styles---under repeated exposure of one style, scenario effects obscure any explanation effects. Our results suggests there may be no 'best' approach to explaining algorithmic decisions, and that reflection on their automated nature both implicates and mitigates justice dimensions.},
	urldate = {2022-01-06},
	journal = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
	author = {Binns, Reuben and Van Kleek, Max and Veale, Michael and Lyngs, Ulrik and Zhao, Jun and Shadbolt, Nigel},
	month = apr,
	year = {2018},
	note = {arXiv: 1801.10408},
	keywords = {\_tablet, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, H.5.m, K.4.1},
	pages = {1--14},
	annote = {Comment: 14 pages, 3 figures, ACM Conference on Human Factors in Computing Systems (CHI'18), April 21--26, Montreal, Canada},
	file = {Binns et al_2018_'It's Reducing a Human Being to a Percentage'\; Perceptions of Justice in.pdf:/Users/neilnatarajan/Zotero/storage/VIS3KNGK/Binns et al_2018_'It's Reducing a Human Being to a Percentage'\; Perceptions of Justice in.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/UALDRMYZ/1801.html:text/html},
}

@article{Lundberg-and-Lee,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	url = {http://arxiv.org/abs/1705.07874},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	urldate = {2022-01-06},
	journal = {arXiv:1705.07874 [cs, stat]},
	author = {Lundberg, Scott and Lee, Su-In},
	month = nov,
	year = {2017},
	note = {arXiv: 1705.07874},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: To appear in NIPS 2017},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/AIQWBTDJ/Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/LS6VRCJC/1705.html:text/html},
}
@inproceedings{Kaminski-and-Malgieri,
  title={Multi-layered explanations from algorithmic impact assessments in the GDPR},
  author={Kaminski, Margot E and Malgieri, Gianclaudio},
  booktitle={Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  pages={68--79},
  year={2020}
}
@inproceedings{Selbst-and-Powles,
  title={“Meaningful Information” and the Right to Explanation},
  author={Selbst, Andrew and Powles, Julia},
  booktitle={Conference on Fairness, Accountability and Transparency},
  pages={48--48},
  year={2018},
  organization={PMLR}
}
@book{Hardin,
  title={Trust and trustworthiness},
  author={Hardin, Russell},
  year={2002},
  publisher={Russell Sage Foundation}
}
