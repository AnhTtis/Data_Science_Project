
\section{Related Work}
% This section we introduce the overview of CLIP and the robustness issue of its text encoder. Then we introduce recent work about image output manipulation.

\paragraph{{Adversarial attacks.}}
Adversarial attacks  typically deceive DNNs by integrating  carefully-crafted tiny perturbations  into input data \cite{goodfellow2014explaining,carlini2017towards,madry2017towards,croce2020reliable,xu2019structured,chen2017ead,xiao2018spatially, liu2018signsgd, chen2017zoo, andriushchenko2020square, brendel2017decision, cheng2019sign, chen2020rays}.
Based on how an adversary interacts with the  victim model, adversarial attacks can be categorized into white-box attacks  (with full access to the victim model based on which attacks are generated) and black-box attacks  (with access only to the victim model's input   and output).
The former typically leverages the local gradient information of the victim model to generate attacks, \textit{e.g.}, \cite{goodfellow2014explaining, carlini2017towards, madry2017towards,salman2023raising}, while the latter takes input-output model queries  for attack generation; Examples include  score-based attacks (\textit{e.g.}, \cite{liu2018signsgd, chen2017zoo, andriushchenko2020square}) and decision-based attacks (\textit{e.g.}, \cite{brendel2017decision, cheng2019sign, chen2020rays}).
In this work, we assume that the adversary has access to the CLIP text encoder but can be blind to the diffusion model for image generation.
% \HM{Shall we explain the setting here like: Because recently widespread DMs~\cite{rombach2022high,ramesh2022hierarchical,saharia2022photorealistic} utilize pre-trained text encoders (\textit{e.g.}, CLIP and T5), which is open-source and easily accessed by the public.}
Our goal is to design an adversarial attack to fool the stable diffusion model without executing the diffusion process, which would take  a high model query and computation cost.
Thus, we term our proposal the `query-free attack'.
%Given the publicity of CLIP, our proposal can also be regarded as a query-free black-box attack on DMs. 
% \SL{[needs discussion.]}


\paragraph{{Prompt perturbations in vision-language models.}}
Recent studies~\cite{daras2022discovering,maus2023adversarial,wen2023hard} have explored the over-sensitivity of  text-to-image diffusion models   to prompt perturbations in the text domain. The adversarial robustness problem of CLIP was also studied in
\cite{mao2022understanding,Fort2021CLIPadversarialstickers,galindounderstanding,noever2021reading}, such as the design of
% perturbation on prompts to manipulate TTI DMs in text space. Gibberish words~\cite{daras2022discovering} are found to possess specific meanings, such as ``Apoploe vesrreaitais'' presenting birds in DALLE-2. In \cite{maus2023adversarial}, a black-box attack is proposed to shift given sentences to output particular objects with four extra words in the token table. 
%
% \paragraph{Robustness of CLIP.}
% A spectrum of studies exist in the literature, that demonstrate that CLIP's performance can be significantly impacted 
%by 
imperceptible pixel perturbations~\cite{mao2022understanding,Fort2021CLIPadversarialstickers} and attacks in the image frequency domain  \cite{galindounderstanding}.
%, and spanning attacks~\cite{noever2021reading,Fort2021CLIPadversarialstickers}.
%In the meantime, recent literature~\cite{zhou2022learning,shu2022test} shows that the quality of the text prompting can greatly influence the performance of the downstream tasks. Yet,
Yet, the previous studies focused on perturbations to image inputs of CLIP,
it lacks investigation into how the textual perturbation to CLIP can influence the T2I diffusion model.

%Despite a growing number of attack methods focusing on the images, there has not been a thorough and in-depth understanding of the vulnerability of CLIP's text encoders. Nor do we know how such attacks could possibly impact the downstream tasks.

% \subsection{Overview of CLIP}
% CLIP (Contrastive Language-Image Pretraining)~\cite{radford2021learning} is a recently proposed multimodal model that combines the strengths of language and vision models to calculate similarity of text-image pairs. CLIP is trained on large amounts of text-image pairs, allowing it to learn a rich semantic representation of both domains and support lots of downstream tasks.
% %
% For example, CLIP has also been used as a pre-trained text encoder in diffusion models, where it provides a text-based guidance signal to help generate images. The learned text representations in CLIP are therefore a crucial component in these diffusion models and play a key role in determining the content of the generated images.

% \subsection{Robustness of CLIP}
% The present study~\cite{mao2022understanding} demonstrates that imperceptible adversarial perturbations on images can
% significantly impact CLIP's performance. For text encoder, recent literature~\cite{zhou2022learning,shu2022test} has proven that text prompting correctly can greatly improve downstream tasks like serving as classifiers. Nonetheless, the perturbation attacks upon CLIP and how this weakness impacts its downstream tasks are still open questions. 

% \YH{Missing attack generations on CLIP models. (Done.)}



% \subsection{Diffusion Model Manipulation}
% Recent works have explored the manipulation of diffusion model outputs through special prompts in input space or special vectors in CLIP embedding space.

% \textbf{Gibberish words}~\cite{daras2022discovering} points out that some non-sense words possess specific meanings. For instance, the term ``Apoploe vesrreaitais'' is found to be associated with birds. Gibberish words are identified by querying diffusion model with special words such as ``with subtitles''. Then the non-sense words in generated images are used to query diffusion model again to verify if they are associated with specific meanings. However, this approach has limitations as it requires human intervention to recognize gibberish words in images and may require numerous attempts with only a limited number of gibberish words that are associated with particular meanings.

% \textbf{Adversarial Prompting}~\cite{maus2023adversarial} 
% proposes a targeted attack method that can shift given prompts to a specific object, for instance, changing the output of "a picture of a mountain" to a dog. However, our approach is distinct from theirs as we employed a perturbation setting that required only the addition of 5 characters at the end of the original sentence \textbf{without query}, to force the diffusion model to generate images far from the original prompts. In contrast, they~\cite{maus2023adversarial} utilized four prompts and 10,000 queries to compel the diffusion model to generate a targeted object, which necessitated over 10 GPU hours. Our approach also differed from theirs in that we restricted the added prompts to consist only of digits, letters, or common symbols, rather than searching from the token table. Notably, our method was computationally efficient, requiring only one minute and the use of the pretrained CLIP model.
% % proposes a black box targeted attack to shift given prompts to a specific object such as changing the output of "a picture of a mountain" to a dog.
% % This approach differs from ours, as \cite{maus2023adversarial} used four prompts and 10,000 queries to force the diffusion model to generate a targeted object, which requires more than 10 GPU hours. In contrast, we used only 5 characters added at the end of the original sentence without query to force the diffusion model to generate images far from the original prompts. The cost of time is limited within one minute, where only the pre-trained CLIP model participates.
% % Our perturbation setting is different from their restricted prepending setting, where we restricted the added prompts consisting of only digits, letters, or common symbols instead of searching from the token table.

% \textbf{PEZ}~\cite{wen2023hard} introduces a gradient-based approach to generate text prompts that match a given image or prompt with CLIP. For instance, they use this method to generate prompts such as ``prevmaverick ask figurative ecuador ntvmilkyway campfire'' to guide diffusion models in generating similar images. 
% % Their focus is on maximum optimization, whereas our approach focuses on minimum optimization settings.

% \textbf{Image Editing}
% has gained lots of attention, which is the manipulation of outputs in diffusion models in embedding space or parameter space. A number of studies have explored ways to capture the style or details of a targeted image and transfer it to another scene by crafting the output of text embeddings or fine-tuning the model parameters~\cite{gal2022image,zhang2022inversion,hertz2022prompt,mokady2022null}. However, our approach differs in that we aim to destroy the content in the output image, while the aforementioned studies seek to reconstruct given images and transfer to different scenes.
% % we manipulate the outputs in the input space, in a black box setting, while these studies manipulate the outputs in the text embeddings space, in a white box setting.

