\section{Experiment}

\input{FigureTex/res_pa}
\input{FigureTex/res_rpa}

\subsection{Experiment setups}

\paragraph{Model setup.} 
Throughout the experiments, we use Stable Diffusion model v1.4~\cite{rombach2022high} as the victim model for image generation.  %to generate $512\times 512$. 
%images with the classifier free guidance scale of 7.5 and 50 inference steps.
The proposed query-free attack has the access to   the CLIP model (ViT-L/14) that shares the same text encoder as     Stable Diffusion. The CLIP model is trained on a dataset containing text-image pairs 
%in websites and commonly-used pre-existing image datasets such as YFCC100M~
\cite{thomee2016yfcc100m}. 
%and the Stable Diffusion Model is trained on multiple datasets such as laion2B-en~\cite{schuhmann2022laion}. 
% \YH{How is the model pretrained? Any source or references for the model checkpoints?} \HM{Done!}

% We utilize Stable Diffusion Model version v1.4 to generate $512\times 512$ images with classifier free guidance scale of 7.5 and 50 inference step. The version of CLIP is ViT-L/14 which has the same text encoder as Stable Diffusion Model. 
\paragraph{Attack implementation.}
When implementing the PGD attack method, we set the base learning rate  by $0.1$ and the number of PGD steps by 100. When implementing the genetic algorithm, we set the number of generation steps, the number of candidates per step, and the mutation rate by $50$, $20$, and $0.3$. When implementing the targeted attack, we use ChatGPT~\cite{chatGPT2023} to generate $n = 10$ sentences to characterize the steerable key dimensions and set $\epsilon = 0.9$ to determine the influence mask $I$ in \eqref{eq:cos_mask}. In addition, we utilize ChatGPT generating $20$ prompts forming an input text dataset for the quantization by requesting `Generate 20 simple scenes for text-to-image generative model'.

% For PGD attack, the base learning rate is set to $0.1$. In the Genetic algorithm, the maximum generation step is set to 50, the selected candidates number in each generation is set to 20, and the mutation rate is set to 0.3. For the targeted perturbation attacks, we set $\epsilon = 0.9$ in \eqref{eq:cos_mask}. 
% % In this setting, the $\boldsymbol{h}$ \YH{in Eq.(?)} refers to the mask of dimensions in which all \textit{text diff} share the same direction.
% To mitigate bias, we use ChatGPT~\cite{chatGPT2023}, a large language model, to generate 20 simple prompts for text-to-image generation models forming a text input dataset by requesting `Generate 20 simple scenes for text-to-image generative model' 
% % \YH{why? Better to put a reference here}.

\paragraph{Evaluation metrics.}
%To demonstrate the effectiveness of our proposed query-free attack,
In addition to different  implementations of our proposed query-free attack (\textit{i.e.}, PGD, Greedy, and Genetic methods), 
we also introduce a  baseline that randomly generates random five-character prompt perturbations, termed Random. 
We evaluate the effectiveness of an attack using 
 the CLIP score~\cite{gal2022image,wen2023hard} to characterize the similarity between the text input and the generated image. A lower CLIP score represents a lower semantic correlation between the generated image and the input text, indicating the higher effectiveness of the attacking method. 
 To quantify the CLIP score between the targeted objects and images generated in the targeted attack setting, we utilized the template sentence `This is a photo of'~\cite{radford2021learning} as text input to measure text-image pair similarity.
The CLIP score reported for each method will be averaged over 20 prompts, based on each of which 10 images will be generated.  
%10 generated images given a prompt.
%During the evaluation, for all the methods and baselines, ten potential perturbation words are proposed for each prompt and the best one is selected.

\subsection{Experiment results}

\begin{table}[tb]
    \caption{CLIP scores~\cite{gal2022image,wen2023hard} comparison of images generated with different methods.  CLIP scores are used to indicate the similarity between the generated images and the embeddings of the corresponding text prompts.
    For each method, the CLIP scores reported below are averaged over $20$ prompts and $10$ images per prompt. In particular, the scores calculated based on the original sentences and output images are adopted for the untargeted attack and based on the targeted content and output images for the targeted setting.
    The lowest (best) score in each row is in \textbf{bold} and the results in the form $a$\footnotesize{$\pm b$} denote the mean value $a$ and the standard deviation $b$.
    % The scores indicate the similarity between the generated images after attacks and the original sentences. The attacks include: no attack, random selecting attack (Random), greedy attack (Greedy), genetic attack (Genetic), and PGD attack (PGD).
    }
    \label{tab: exp_main}
    \centering
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{c|c|c|c|c|c}
            \toprule[1pt]
            \textbf{Method}:
            & \textbf{No Attack} & \textbf{Random} 
            & \textbf{Greedy} & \textbf{Genetic} 
            & \textbf{PGD}   \\ \midrule
            \multicolumn{6}{c}{Untargeted Attack} \\ \midrule
            Score:       
            & 0.277\footnotesize{$\pm 0.022$}     
            & 0.271\footnotesize{$\pm 0.021$}  
            & \cellcolor{Gray} 0.255\footnotesize{$\pm 0.039$}  
            & \cellcolor{Gray}\textbf{0.203}\footnotesize{$\pm 0.042$}  
            & \cellcolor{Gray} 0.226\footnotesize{$\pm 0.041$} \\ \midrule
            \multicolumn{6}{c}{Targeted Attack} \\ \midrule
            Score:       
            & 0.229\footnotesize{$\pm 0.03$}     
            & 0.223\footnotesize{$\pm 0.037$}  
            & \cellcolor{Gray} 0.204\footnotesize{$\pm 0.037$}  
            & \cellcolor{Gray} \textbf{0.186}\footnotesize{$\pm 0.04$}   
            & \cellcolor{Gray} 0.189\footnotesize{$\pm 0.041$} \\
            \bottomrule[1pt]
        \end{tabular}
    }
\end{table}

% \begin{table}[tb]
%     \caption{CLIP scores~\cite{gal2022image,wen2023hard} for generated images under different attacks. The scores indicate the similarity between the generated images after attacks and the target objects. The attacks include: no attack, random selecting attack (Random), greedy attack (Greedy), genetic attack (Genetic), and PGD attack (PGD).}
%     \label{table:result2}
%     \centering
%     \scalebox{0.9}{
%         \begin{tabular}{l|c|c|c|c|c}
%             \toprule[1pt]
%             \multicolumn{1}{c|}{Attack} & No Attack & Random & Greedy & Genetic & PGD   \\ \midrule
%             Score                        & 0.2123     & 0.2071  & 0.215  & 0.1768   & 0.1723 \\ \bottomrule[1pt]
%         \end{tabular}
%     }
% \end{table}

\paragraph{Query-free attack can successfully alter the image output of Stable Diffusion using only a 5-character prompt perturbation.}
%先从图片说起，因为图片和输入发生了偏离。偏离包括遗忘原句中的一些词和新添加了一些词。举例，单车被perturbation word扰动后，从图片消失。其他例子也相似，这说明了攻击的成功。除此之外，在量化的结果中，和baseline相比有着显著的下降，也证明了我们攻击的有效性。在横向的对比中，greedy方法的作用有限，genetic有最好的效果。说明genetic可以在不同generation的迭代中找到可以持续降低相似度的字符组合。
 In \textbf{Fig.\,\ref{fig:res_pa}}, we present examples of text-to-image generation \textit{with} and \textit{without} suffering prompt perturbations generated by the untargeted, genetic algorithm-based query-free attack. The 5-character prompt perturbation is highlighted in \textcolor[RGB]{22,113,250}{blue} with \textcolor{gray}{gray}  background. 
As we can see, the proposed attack can significantly alter the content of the original image  produced by Stable Diffusion. For example, in Fig.\,\ref{fig:res_pa}-(a), the perturbation `E\$9\textbackslash' ' drives the model to generate images far from the true topic `bicycle'.
The same observation can also be drawn from examples in  Fig.\,\ref{fig:res_pa}-(b)-(h).
This implies that the attack against the text embedding remains effective in manipulating the output of text-to-image generation.


Similar to Fig.\,\ref{fig:res_pa}, 
  \textbf{Fig.\,\ref{fig:rpa}} presents examples of targeted, genetic-based query-free attacks against Stable Diffusion. For example, in Fig.\,\ref{fig:rpa}-(a), 
  the adversary targets   perturbing `bicycle' without altering the background `brick wall' much. 
 This contrasts to Fig.\,\ref{fig:res_pa}-(a), where the image object and scene may change.   
 Another example is  Fig.\,\ref{fig:res_pa}-(b), where 
 the object `plate' is erased using the perturbed prompt but the object `apple' is retained.  We can also draw similar observations from  other examples. 
 Briefly, the targeted attack can precisely manipulate the diffusion model to avoid the targeted semantics (\textit{i.e.}, the   \textcolor{red}{red} text highlight above each image example in {Fig.\,\ref{fig:rpa}), while  can retain the irrelevant semantics (\textit{e.g.}, `{brick wall}' in Fig.\,\ref{fig:res_pa}-(a)).
 
 %while erasing the targeted ones (\textit{e.g.}, `\textcolor{red}{bicycle}', `\textcolor{red}{plate}', `\textcolor{red}{lake}', `\textcolor{red}{leaf}'). 

  
 
% To illustrate the effect of the proposed query-free attack, we first present the examples generated by the T2I Stable Diffusion model \textit{with} and \textit{without} perturbations in Fig.\,\ref{fig:res_pa} (through untargeted query-free attack) and Fig.\,\ref{fig:rpa} (through targeted query-free attack). As we can see, the model can work very well and generate images in the topics dictated by the prompt when no perturbation is planted, (see the images on the first row). However, the generated images consistently go through a dramatic content topic shift in the presence of the perturbations (highlighted in \textcolor[RGB]{22,113,250}{blue}). 
% For example, in Fig.~\ref{fig:res_pa}-(a), the perturbation `E\$9\textbackslash' ' drives the model to generate images far from the true topic `bicycle'. Besides, in Fig.\,\ref{fig:rpa},  the targeted attack is able to not only alter the topics, but also precisely manipulate the victim model to avoid the targeted semantics (highlighted in \textcolor{red}{red} above the images). In the meantime, the targeted attack achieves to retain the irrelevant semantics (\textit{e.g.}, `{brick wall}', `{apple}', `{green}{swan}', and `{green}{butterfly}'), while erasing the targeted ones (\textit{e.g.}, `\textcolor{red}{bicycle}', `\textcolor{red}{plate}', `\textcolor{red}{lake}', `\textcolor{red}{leaf}'). 

\paragraph{Query-free attack can effectively reduce the CLIP score.}
To quantify the influence of the attack in each generated text-image pair, \textbf{Table\,\ref{tab: exp_main}} presents the CLIP scores \cite{gal2022image,wen2023hard} of the image pairs generated by Stable Diffusion with and without the  attack's perturbations. 
% \HM{Here $20$ prompts in the input text dataset are evaluated.} 
In Table\,\ref{tab: exp_main}, we can make the following observations.
\underline{\textit{First}}, in the untargeted attack setting, it is clear that  different attack methods can all successfully reduce the CLIP score versus the baseline result using `No Attack' or `Random' attack strategy. Such a reduction implies a relatively low similarity between the perturbed text input and the generated image and  justifies the image content modification   observed in Fig.\,\ref{fig:res_pa}. 
Moreover, the genetic algorithm outperforms the other attack methods. This also supports the choice of genetic algorithm-based untargeted attack in Fig.\,\ref{fig:res_pa}.
%In contrast, the random perturbation hardly reduces the CLIP score in all the settings, which necessitates to carefully design the perturbation patterns. 
\underline{\textit{Second}}, in the targeted attack scenario, the PGD attack method and the genetic algorithm outperform other attack methods. 
%in finding perturbations altering the specific embeddings of the CLIP text encoder. 
We also notice that the targeted attack setting introduces additional difficulty for effective perturbation generation, evidenced by the smaller drop in the CLIP score compared to the untargeted setting.
% However, the limited decline in the targeted attack reflects the difficulty of perturbing specific objects.
% \SL{More interestingly, the CLIP score further decreases  when using the targeted attack. This implies that compared to the untargeted attack, the identification of steerable key dimensions   reduces the search space and may help the targeted attack to find more effective text perturbations than the untargeted case.} 
%the drop of the scores is diminished compared to the untargeted setting and the Greedy search does not perform well. This is largely due to that finding proper perturbations to steering the specific dimensions of the text embedding (semantics) is much more difficult than performing the untargeted attacks.

% \underline{\textit{Third}}, among the three attack algorithms we proposed, the Genetic and PGD attack tend to find more effective perturbations than Greedy search. In particular, Greedy search attack shows either a limited or no drop in CLIP score compared to the significant score drop by the other two methods. We conjecture that this is because both Genetic and PGD have to go the iterative perturbation selection, which delivers more powerful attacks.

% Table~\ref{table:result} quantifies the impact of three perturbation attacks on CLIP score, which shows a decline from 0.27 to 0.25, 0.20, and 0.22, respectively. 
% In contrast, the baseline random selection approach has a minor impact, which achieves the same score as the original sentences. 
% Among the attacks, Greedy attack shows a limited drop in CLIP score, whereas Genetic attack results in the most significant changes in output images. It indicates that Genetic algorithm finds the combination of characters leading to low cosine similarity in generations through iterative selection.

% \paragraph{Refiner perturbation attacks.}
% %对于refiner来说，如图所展示，攻击在特定的种子和一些场景中可以实现对目标物体的攻击。举个例子，苹果的第一个图片对中相似度高，但是目标物盘子被移除了。其他的结果也都一样的结果。在量化的结果中看，下降幅度比perturbation attacks小，说明在有限的场景中可以成功。在横向对比中，pgd的效果略好于genetic，说明pgd更擅长在steerable dimensions中找到使图片偏离的words。
% As the results shown in Fig.~\ref{fig:rpa}, refiner perturbation attacks demonstrate the ability to perturb the targeted objects while keeping other objects unchanged in specific seeds. For example, in Fig.~\ref{fig:plate}, the first image pair shows high similarity but the targeted object plate is removed by the perturbation word ``G)\$IQ''. Other results show the consistency that in some occasions refiner perturbation attacks can perturb the targeted object in original sentences while preserving the remaining objects. The results of refiner perturbation attacks are quantified in Table~\ref{tab: exp_main}, where the limited decline (from 0.212 to 0.177) suggests that its success is achieved in fewer scenes compared to the perturbation attacks. PGD algorithm generates slightly better results than Genetic, indicating its superior ability to generate perturbation words when calculating loss in steerable dimensions.

\begin{figure}[tb]
    \centering
    \includegraphics[scale=0.1]{figs/ablation/walle.pdf}
    \caption{Ablation study for the perturbation word generating  robots in Fig.\,\ref{fig:res_pa}.}
    % \HM{original: 0.293, ours: 0.217, E:0.297, WALLE: 0.291, -E: 0.285}
    \label{fig:walle}
\end{figure}


\begin{table}[tb]
    \caption{CLIP scores~\cite{gal2022image,wen2023hard} comparison of different perturbation prompts in case study.  
    For each prompt, the CLIP scores reported below are averaged over $10$ images from the same seeds.
    The lowest (best) score in each row is in \textbf{bold}.
    }
    \label{tab: walle}
    \centering
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{c|c|c|c|c|c}
            \toprule[1pt]
            \textbf{Perturbation prompt}:
            & \textbf{None} & \textbf{`E\$9\textbackslash' '} 
            & \textbf{`E'} & \textbf{`WALLE'} 
            & \textbf{`-E'}   \\ \midrule
            
            Score:       
            &  0.293
            & \cellcolor{Gray}\textbf{0.217}  
            & 0.297
            & 0.291
            &  0.285 \\ 
            \bottomrule[1pt]
        \end{tabular}
    }
\end{table}
\paragraph{Why does the perturbation work? A case study on `WALL-E'.}
To demonstrate the effectiveness of our attacks, we conduct an ablation experiment to compare the perturbations generated by our method and a direct change in textual semantics.
%Despite the limited  vocabulary in the character table, some perturbations seem   explainable and relevant to the desired image output. 
Recall from {Fig.\,\ref{fig:res_pa}-(a)} that the generated perturbation `E\$9\textbackslash' ' appended to the sentence `A black bicycle against a brick wall' seems   related to a \textit{robot movie `WALL-E'}\footnote{\url{https://en.wikipedia.org/wiki/WALL-E}.}. We   wonder if this is due to the effect of the combination between `wall' in the original text input and the   added letter `E' in the generated perturbation   `E\$9\textbackslash' '. 
To this end, we conduct additional experiments to explicitly append the letter `E' to the end of the original text input.  \textbf{Fig.~\ref{fig:walle}} shows that simply adding `E' or  `WALLE' fails to alter the image content (see the first two rows of Fig.~\ref{fig:walle}). 
Although replacing `wall' with `wall-E' in the original sentence may produce a robot-related image, the success of such image generation remains low. This trend is also supported by the corresponding CLIP scores reported in Table~\ref{tab: walle}, where almost no change can be observed (see $0.293$ \textit{vs.} $\{0.297, 0.291, 0.285\}$).
% \HM{ as shown in Table~\ref{tab: walle} that both cause almost no change in CLIP score from 0.293 to 0.297 and 0.291 respectively}.
By contrast, the use of prompt perturbation `E\$9\textbackslash' ' in {Fig.\,\ref{fig:res_pa}-(a)} is much more effective in altering the image content (see a CLIP score drop from $0.293$ to $0.217$ in Table~\ref{tab: walle}.

% Only changing the original word `wall' to `wall E' gains a very small chance in misleading the model to generate robot-related contents.
% This experiment again demonstrates that generating adversarial perturbations are not trivial at all, and the perturbation found by our Query-Free attack is more effective even than some manually designed semantically meaningful perturbations.
% It highlights our success in the leakage of the vulnerability in the CLIP text encoder with meaningless characters like `\$9\textbackslash' ' perturbing the original sentence. 

% \begin{table}[tb]
%     \caption{CLIP scores~\cite{gal2022image,wen2023hard} for generated images under different attacks. The scores indicate the similarity between the generated images and the target objects. The attacks include: no attack, random selecting attack (Random), greedy attack (Greedy), genetic attack (Genetic), and PGD attack (PGD).}
%     \label{table:result2}
%     \centering
%     \scalebox{0.9}{
%         \begin{tabular}{l|c|c|c|c|c}
%             \toprule[1pt]
%             \multicolumn{1}{|c|}{Attack} & No Attack & Random & Greedy & Genetic & PGD   \\ \midrule
%             Score                        & 21.23     & 20.71  & 21.50  & 17.68   & 17.23 \\ \bottomrule[1pt]
%         \end{tabular}
%     }
% \end{table}



% \begin{figure}[tb]
%     \centering
%     \includegraphics[scale=0.09]{figs/ablation.pdf}
%     \caption{Ablation study for three prompts: the prompts displayed below the images are appended to the end of sentences on the left, except for the original images. The red rectangles highlight the targeted images for the attack, while the remaining images serve as control groups.}
%     \label{fig:ablation}
%     \vspace{-0.3cm}
% \end{figure}



% Despite limitation on vocabulary table, some prompts seem to explainable, like the prompt ``+Z:I8" related to a motorcycle type of Benz ``I8''. To show the effectness of our attacks, we apply generated prompts to other sentences. In Fig~\ref{fig:ablation} those prompts show limited perturbation to other sentences. The results indicate that the success of our attacks does only not relie on the bias of those prompts, but also learn to leakage the vulnerability of CLIP text encoder to perturb diffusion model. The prompt ``E\$9\\\' '' for the sentence ``A black bicycle against a brick wall'' trigger a character ``WALL-E'' in a moive. Intuitively, ``\$9\\\' '' is useless for the attack because the convey no certain meaning. However, Fig.~\ref{fig:walle} shows that adding ``E'' directly fails to change the output without ``\$9\\\''' and adding ``WALLE'' and ``-E'' fail as well, which further highlights how our perturbation attack leakages the vulnerability of CLIP text encoder.