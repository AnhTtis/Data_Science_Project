\begin{abstract}
    %The field of diffusion model (DM) has seen a dramatic rise in interest in recent years, which has emerged as a popular and successful family of deep generative models.
    Despite the record-breaking performance in Text-to-Image (T2I) generation by Stable Diffusion, less research attention is paid to  its adversarial robustness. In this work, we study the problem of adversarial attack generation for Stable Diffusion
    and ask if an adversarial text prompt can be obtained even in the absence of  end-to-end model queries. We call the resulting problem `query-free attack generation'. To resolve this problem, we show that the vulnerability of T2I models is rooted in the lack of robustness of text encoders, \textit{e.g.}, the CLIP text encoder used for attacking Stable Diffusion. Based on such insight,
    we propose both untargeted and targeted query-free
    attacks, where the former is built on the most influential dimensions in the text embedding space, which we call steerable key dimensions.
    By leveraging the proposed attacks, we empirically show that only a five-character perturbation to the text prompt is able to cause the significant content shift of  synthesized images using Stable Diffusion. Moreover, we show that the proposed target attack can precisely steer the diffusion model to scrub the  targeted image content without causing much change in untargeted image content. Code is available at \url{https://github.com/OPTML-Group/QF-Attack}.

    % Since the adversarial robustness has become a key desirable property in building reliable and trustworthy deep learning systems, we ask: How to design an attack to adversarially perturb a T2I model? Can we obtain successful perturbations by attacking only part of the T2I model, e.g., its commonly-used CLIP text encoder? 
    % In this work, we first reveal the weakness of T2I models, which roots in its vulnerable CLIP text encoder. Spurred by that, we further propose three query-free attack algorithms, that can obtain successful input text perturbations within a limited length by only attacking the CLIP encoder and lead to dramatic topic/content shift in the generated images by T2I models.
    % By further dissecting the relationship between the semantic meanings of the generated images and specific dimensions in the CLIP embeddings, we extend our proposal to a targeted attack, that is able to precisely steer the T2I model's behavior and erase the particular target semantic in the generated images inherent within the original sentence.
    % The effectiveness of the proposed attack is justified through extensive experiments, where the perturbations generated by purely attacking the CLIP encoder can cause a significant drop in the Image-Text similarity (from $0.277$ to $0.203$), denoting a dramatic semantic shift in the generated images of the victim T2I DM. 
    % \HM{In particular, the targeted perturbation can successfully generate the perturbation word to drive a T2I DM to produce images that are substantially dissimilar from the targeted semantics initially present within the input sentence.}
    % extra perturbation words cause a significant drop in Image-Text pair similarity from 0.277 to 0.203 generated by perturbation attacks and a decline from 0.212 to 0.172 generated by refiner perturbation attacks upon specific objects. 
    % It demonstrates the effectiveness of the proposed attack methods and shows that a broken CLIP alone can lead to failed diffusion models.
    % \YH{@Haomin Can you add more experiment summary here? Highlight the most significant and exciting results. (Done)}
    % Our experimental results demonstrate the vulnerability of diffusion models to such attacks, even with small perturbations in the sentence. 

    % Diffusion models are generative models that generate images by iteratively refining a noise input. Despite their success in several image generation tasks, the robustness of diffusion models to perturbation attacks remains an open question. This is largely due to the interdependence between the robustness of diffusion models and the robustness of the text encoder, which is used to guide image generation for the diffusion model.

    % In this paper, we propose diffusion-free perturbation attacks on diffusion models that introduce a word to the sentence, leading to unexpected changes in the generated images \textbf{without query on diffusion models}. We search perturbation words causing serious deviation upon CLIP text encoder, which is used as the text encoder in Diffusion Models. Additionally, we analyze the relationship between semantic meanings and specific dimensions in CLIP embeddings and leverage this knowledge to design a refiner perturbation attack, which is more effective in perturbing specific contextual information in the generated images. Our experimental results demonstrate the vulnerability of diffusion models to such attacks, even with small perturbations in the sentence. 
\end{abstract}