@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{Authors14,
  author = {FirstName LastName},
  title  = {The frobnicatable foo filter},
  note   = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
  year   = 2014
}

@misc{Authors14b,
  author = {FirstName LastName},
  title  = {Frobnication tutorial},
  note   = {Supplied as supplemental material {\tt tr.pdf}},
  year   = 2014
}

@article{Alpher02,
  author  = {FirstName Alpher},
  title   = {Frobnication},
  journal = PAMI,
  volume  = 12,
  number  = 1,
  pages   = {234--778},
  year    = 2002
}

@article{Alpher03,
  author  = {FirstName Alpher and  FirstName Fotheringham-Smythe},
  title   = {Frobnication revisited},
  journal = {Journal of Foo},
  volume  = 13,
  number  = 1,
  pages   = {234--778},
  year    = 2003
}

@article{Alpher04,
  author  = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
  title   = {Can a machine frobnicate?},
  journal = {Journal of Foo},
  volume  = 14,
  number  = 1,
  pages   = {234--778},
  year    = 2004
}

@inproceedings{Alpher05,
  author    = {FirstName Alpher and FirstName Gamow},
  title     = {Can a computer frobnicate?},
  booktitle = CVPR,
  pages     = {234--778},
  year      = 2005
}

@article{gal2022image,
  title   = {An image is worth one word: Personalizing text-to-image generation using textual inversion},
  author  = {Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H and Chechik, Gal and Cohen-Or, Daniel},
  journal = {arXiv preprint arXiv:2208.01618},
  year    = {2022}
}

@article{ho2020denoising,
  title   = {Denoising diffusion probabilistic models},
  author  = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {6840--6851},
  year    = {2020}
}

@article{hou2022textgrad,
  title   = {TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization},
  author  = {Hou, Bairu and Jia, Jinghan and Zhang, Yihua and Zhang, Guanhua and Zhang, Yang and Liu, Sijia and Chang, Shiyu},
  journal = {arXiv preprint arXiv:2212.09254},
  year    = {2022}
}

@article{paszke2019pytorch,
  title   = {Pytorch: An imperative style, high-performance deep learning library},
  author  = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal = {Advances in neural information processing systems},
  volume  = {32},
  year    = {2019}
}
@inproceedings{rombach2022high,
  title     = {High-resolution image synthesis with latent diffusion models},
  author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {10684--10695},
  year      = {2022}
}

@article{nichol2021glide,
  title   = {Glide: Towards photorealistic image generation and editing with text-guided diffusion models},
  author  = {Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
  journal = {arXiv preprint arXiv:2112.10741},
  year    = {2021}
}

@article{ramesh2022hierarchical,
  title   = {Hierarchical text-conditional image generation with clip latents},
  author  = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal = {arXiv preprint arXiv:2204.06125},
  year    = {2022}
}

@inproceedings{radford2021learning,
  title        = {Learning transferable visual models from natural language supervision},
  author       = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle    = {International conference on machine learning},
  pages        = {8748--8763},
  year         = {2021},
  organization = {PMLR}
}

@article{zhang2022inversion,
  title   = {Inversion-Based Creativity Transfer with Diffusion Models},
  author  = {Zhang, Yuxin and Huang, Nisha and Tang, Fan and Huang, Haibin and Ma, Chongyang and Dong, Weiming and Xu, Changsheng},
  journal = {arXiv preprint arXiv:2211.13203},
  year    = {2022}
}

@article{hertz2022prompt,
  title   = {Prompt-to-prompt image editing with cross attention control},
  author  = {Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},
  journal = {arXiv preprint arXiv:2208.01626},
  year    = {2022}
}

@article{mokady2022null,
  title   = {Null-text Inversion for Editing Real Images using Guided Diffusion Models},
  author  = {Mokady, Ron and Hertz, Amir and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},
  journal = {arXiv preprint arXiv:2211.09794},
  year    = {2022}
}



@article{daras2022discovering,
  title   = {Discovering the hidden vocabulary of dalle-2},
  author  = {Daras, Giannis and Dimakis, Alexandros G},
  journal = {arXiv preprint arXiv:2206.00169},
  year    = {2022}
}

@article{wen2023hard,
  title   = {Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery},
  author  = {Wen, Yuxin and Jain, Neel and Kirchenbauer, John and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  journal = {arXiv preprint arXiv:2302.03668},
  year    = {2023}
}

@article{ho2022classifier,
  title={Classifier-free diffusion guidance},
  author={Ho, Jonathan and Salimans, Tim},
  journal={arXiv preprint arXiv:2207.12598},
  year={2022}
}

@article{song2021solving,
  title={Solving inverse problems in medical imaging with score-based generative models},
  author={Song, Yang and Shen, Liyue and Xing, Lei and Ermon, Stefano},
  journal={arXiv preprint arXiv:2111.08005},
  year={2021}
}

@article{saharia2022photorealistic,
  title={Photorealistic text-to-image diffusion models with deep language understanding},
  author={Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S Sara and Lopes, Rapha Gontijo and others},
  journal={arXiv preprint arXiv:2205.11487},
  year={2022}
}

@article{mao2022understanding,
  title={Understanding Zero-Shot Adversarial Robustness for Large-Scale Models},
  author={Mao, Chengzhi and Geng, Scott and Yang, Junfeng and Wang, Xin and Vondrick, Carl},
  journal={arXiv preprint arXiv:2212.07016},
  year={2022}
}

@article{zhou2022learning,
  title={Learning to prompt for vision-language models},
  author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  journal={International Journal of Computer Vision},
  volume={130},
  number={9},
  pages={2337--2348},
  year={2022},
  publisher={Springer}
}
@article{shu2022test,
  title={Test-time prompt tuning for zero-shot generalization in vision-language models},
  author={Shu, Manli and Nie, Weili and Huang, De-An and Yu, Zhiding and Goldstein, Tom and Anandkumar, Anima and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2209.07511},
  year={2022}
}

@article{carlini2022certified,
  title={(Certified!!) Adversarial Robustness for Free!},
  author={Carlini, Nicholas and Tramer, Florian and Kolter, J Zico and others},
  journal={arXiv preprint arXiv:2206.10550},
  year={2022}
}


@article{wang2022guided,
  title={Guided diffusion model for adversarial purification},
  author={Wang, Jinyi and Lyu, Zhaoyang and Lin, Dahua and Dai, Bo and Fu, Hongfei},
  journal={arXiv preprint arXiv:2205.14969},
  year={2022}
}

 
 


@article{abu2022adir,
  title={ADIR: Adaptive Diffusion for Image Reconstruction},
  author={Abu-Hussein, Shady and Tirer, Tom and Giryes, Raja},
  journal={arXiv preprint arXiv:2212.03221},
  year={2022}
}

 @misc{
Fort2021CLIPadversarialstickers,
title={Pixels still beat text: Attacking the OpenAI CLIP model with text patches and adversarial pixel perturbations},
url={https://stanislavfort.github.io/2021/03/05/OpenAI_CLIP_stickers_and_adversarial_examples.html},
author={Stanislav Fort},
year={2021},
month={March}
}

@article{galindounderstanding,
  title={Understanding CLIP Robustness},
  author={Galindo, Yuri and Faria, Fabio A}
}




