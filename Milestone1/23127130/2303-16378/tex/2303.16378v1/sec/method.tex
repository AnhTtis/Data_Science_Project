
\section{Our Proposal}

\paragraph{Problem statement.}  
In this section, we first present an overview of Stable Diffusion, and then introduce our objective to generate small perturbations on the textual inputs so as to maneuver the DM's synthesized images.

We choose   Stable Diffusion   as the victim T2I DM model due to its popularity and availability as an open-source model. 
In Stable Diffusion, the DM  denoises images in latent space and utilizes a cross-attention mechanism to guide the denoising process.
In addition,  text inputs (or textual prompts) are processed by the CLIP's text encoder to generate text embeddings and are then sent to the cross-attention layer in the denoising network. This eventually determines the synthesized images based on the CLIP's textual embeddings and the selected random seed of the initial noisy pixels. However, as exemplified in \textbf{Fig.\,\ref{fig:text_cos}}, small perturbations on the text input of CLIP can lead to different CLIP scores \cite{gal2022image,wen2023hard}, given by the values of cosine similarity of every text-image input pair. This is because of the sensitivity of the CLIP's text embedding to text perturbations. Based on that, \underline{we ask}: Can we generate an adversarial textual prompt by leveraging the lack of robustness of the CLIP's text encoder so as to fool the DM-based image generator in Stable Diffusion? 


% deviate the CLIP embedding and affect the output images in the T2I Stable Diffusion Model, leading to robustness issues. Therefore, we rise the problem of how to generate small perturbations (e.g., adding a five-character word) on input sentences without querying diffusion models to impact the output images in the T2I DM.

% We begin with introducing the background of Text-To-Image diffusion models. Based on that, we reveal the possible robustness issues on the design and define the problem of interest in how to generate perturbation words. Specifically, we choose a popular diffusion model-Stable Diffusion Model~\cite{rombach2022high} as the victim model. The Stable Diffusion Model introduces denoising in latent space and guides the denoising process by cross-attention mechanism. The input text $\boldsymbol{y}$ is processed to CLIP embedding $\tau_\theta(\boldsymbol{y})$ by the CLIP text encoder $\tau_\theta$ and sent to the cross-attention layer. In this Text-To-Image generation process, the output images are decided by the selected random seed of initial noise and the CLIP embedding of the input sentence. As the robustness issue is shown in Fig.~\ref{fig:text_cos}, the small perturbation on text can deviate CLIP embedding and affect the output images. 

% We rise such a problem that how to generate small perturbations (e.g., a five-character word) on sentences to impact the output images in the Text-To-Image generation process without querying diffusion models.

% \SL{Introduce the victim model a bit used in this paper.}

% \SL{[How to define adversarial perturbations?]}


\paragraph{Attack model.}
We assume that the adversary has access to the trained text encoder   of the CLIP model, and can perturb the textual prompt of the trained State Diffusion model using an additional word within a   \textit{five-character} length. Let $\tau_\theta(\mathbf x)$  denote the text encoder of CLIP with parameters $\theta$ evaluated at the textual input $\mathbf x$. And we denote by $\mathbf x^\prime$ the perturbed textual prompt used as the input of   Stable Diffusion.  
We then define the \textit{attacker's objective} by minimizing the cosine similarity  between the text embeddings of $\mathbf x$ and $\mathbf x^\prime$. This leads to the following attack generation problem 
\begin{equation}
\label{eq:3}
\displaystyle \min_{\boldsymbol{x}^\prime } ~ \text{cos} ( \tau _\theta (\boldsymbol{x}) , \tau _\theta (\boldsymbol{x}^\prime ) ),
\end{equation}
where $\mathrm{cos}$ refers to the cosine similarity metric. 

Despite the simplicity of the attack generation in \eqref{eq:3}, we will show that it can be used to attack State Diffusion in a targeted way effectively. More importantly, the generation of the perturbed input $\mathbf x^\prime$ no longer relies on the optimization over the diffusion model, and can thus be computationally efficient. This is in contrast to  \cite{wang2021adversarial}, which requires $10000$ queries to diffusion model for generating a single attack. 
Since no attack intention is  specified in \eqref{eq:3}, we call the resulting attack an `\textit{untargeted attack}'. 

% For Text-To-Image Stable Diffusion Model, the denoising in each forward step can be formulated as:
% \begin{equation}
%     L_{LDM}:=\mathbb{E} _{\boldsymbol{x},\epsilon \sim \mathcal{N}(0,1)}\left [ \left \| \epsilon - \epsilon _\theta (\mathpzc{z},\tau _\theta (\boldsymbol{x}) ) \right \|   \right ],
% \end{equation}
% where $\mathpzc{z}$ denotes image in latent space and $\epsilon_\theta$ denotes the denoising network. To perturb the output images, the deviation goal is formulated as:
% \begin{equation}
% \label{eq:2}
%     \arg \min_{\boldsymbol{y}^\prime } \text{cos}(\epsilon _\theta (\mathpzc{z}_t,\tau _\theta (\boldsymbol{x}) ), \epsilon _\theta (\mathpzc{z}_t,\tau _\theta (\boldsymbol{x}^\prime ) ))  ,
% \end{equation}
% where $\text{cos}$ denotes the calculation operation of cosine similarity and $x^\prime$ is perturbation input consisting of the original sentence and one extra perturbation word.
% To propose query-free attacks, the denoising network $\epsilon _\theta$ should be excluded in the formulation as:

% \begin{equation}
% \label{eq:3}
%     \arg \min_{\boldsymbol{y}^\prime } \text{cos} ( \tau _\theta (\boldsymbol{x})  , \tau _\theta (\boldsymbol{x}^\prime ) ) .
% \end{equation}
% With the equation above, our attack goal is to perturb far from CLIP embedding original sentences.
\iffalse 
In the text-to-image Stable Diffusion model, the denoising process in each forward step can be represented as an optimization objective:
\begin{equation}
L_{LDM}:=\mathbb{E} _{\boldsymbol{x},\epsilon \sim \mathcal{N}(0,1)}\left [ \left | \epsilon - \epsilon_ \theta (\mathpzc{z},\tau_\theta (\boldsymbol{x}) ) \right | \right ],
\end{equation}
where $\mathpzc{z}$ refers to the image in latent space, $\tau_\theta$ is CLIP text encoder in the Stable Diffusion Model,  and $\epsilon_\theta$ represents the denoising network. To perturb the output images, the objective is to minimize the cosine similarity between the denoised latent image representations of the original sentence and the perturbed sentence:
\begin{equation}
\label{eq:2}
\arg \min_{\boldsymbol{x}^\prime } \text{cos}(\epsilon _\theta (\mathpzc{z}_t,\tau _\theta (\boldsymbol{x}) ), \epsilon _\theta (\mathpzc{z}_t,\tau _\theta (\boldsymbol{x}^\prime ) )) ,
\end{equation}
where $\text{cos}$ represents the cosine similarity operation and $x^\prime$ denotes the perturbation input, consisting of the original sentence and an additional perturbation word. In order to create query-free attacks, the denoising network $\epsilon_\theta$ is excluded from the formulation, resulting in the objective:
\begin{equation}
\label{eq:3}
\arg \min_{\boldsymbol{x}^\prime } \text{cos} ( \tau _\theta (\boldsymbol{x}) , \tau _\theta (\boldsymbol{x}^\prime ) ) .
\end{equation}
Our goal is to generate perturbations that cause a significant deviation in the CLIP embedding of the original sentence.
\fi 

\paragraph{Attack methods.}
Since problem \eqref{eq:3} is differentiable, various optimization methods can be adopted for attack generation.  Inspired by the previous studies on   adversarial attacks in the language domain,  we consider the following attack methods. 


%We propose three well-study methods to generate perturbation words and evaluate their effectiveness on the T2I Stable Diffusion Model.

\textbf{PGD attack.}  Similar to the PGD (projected gradient descent) attack \cite{madry2017towards} in the image domain,   the PGD attack in the language domain  has also been developed  \cite{hou2022textgrad,srikant2021generating}. 
The key idea is to formulate the textual perturbation problem as a token selection problem (over a set of token candidates) when a token site is determined for perturbation. Our experiments follow the PGD implementation in \cite{hou2022textgrad} to solve problem  \eqref{eq:3}. 

% is applied in Nature Language Processing by \textsc{TextGRAD}~\cite{hou2022textgrad}, which is incorporated in our work. It regards the choice of candidates as a selection problem. We create a character table that consists of digits, letters, and common symbols, where characters are selected from this table. A selection vector chooses characters in each position through Gumble Softmax and is updated by the loss in Eq.~\eqref{eq:3}.

\textbf{Greedy search.} Different from the above PGD attack, we next consider a heuristics-based perturbation strategy.  We conduct a greedy search on the  character candidate set to select the top 5  characters (used for textual perturbation), which can reduce the loss of \eqref{eq:3} to the maximum extent. 

% is the most simple method to search for characters to form a word added to the end of the sentence. The algorithm operates by selecting the character that results in the minimal loss in Eq.~\eqref{eq:3}, and iteratively repeating this process until a maximal length $m$ of the generated input is reached.

\textbf{Genetic algorithm.}
We follow \cite{holland1992genetic} to generate  a population of perturbation candidates and use the loss of \eqref{eq:3} to evaluate the quality of each candidate. In each iteration, the genetic algorithm calls genetic operations such as mutation  to generate new candidates. The process continues until the number of generations is met. 
%The advantage of the genetic algorithm is that it can explore a broader search space and find globally optimal solutions without gradient.


\input{FigureTex/flooding}
\paragraph{Targeted  attack and steerable key  dimensions.}
In what follows, \underline{we investigate} if the  attack generated by \eqref{eq:3} can be further \textit{refined}  towards a \textit{targeted} attack purpose, \textit{e.g.}, the intention of removing the `young man'-related image content from the original image in Fig.\,\ref{fig: teaser}-(c) vs. (a).
To this end, we propose a new concept termed \textit{steerable key dimensions} in the text embedding space, along which the attack generator can be guided to design customized textual perturbations.  By constraining the perturbations on these steerable key dimensions, we can improve the likelihood of image generation following the  adversary's intention.


To be specific, we first generate a sequence of augmented sentences $\{ s_i \}_{i=1}^n$ that reflect the adversary's  intention, \textit{e.g.},  the sentences centered on `a young man' in Fig.\,\ref{fig: teaser}-(a). The generation of $\{ s_i \}_{i=1}^n$ can be realized using \textit{e.g.}, ChatGPT by requesting `Generate $n$ simple scenes and end with ``and a young man'' without extra words'. {Two examples are $s_1 = \text{`A bird flew high in the sky and a young man'}$ and $s_2 = \text{`The sun set over the horizon and a young man'}$ with $n = 2$.} Next, we perturb $\{ s_i \}_{i=1}^n$  by \textit{removing} the adversary's intention-related sub-sentence (\textit{i.e.}, `a young man' in Fig.\,\ref{fig: teaser}-(a)). This results in a modified sequence $\{ s_i^\prime \}_{i=1}^n$; For example,  {$s_1^\prime = \text{`A bird flew high in the sky'}$ and $s_2^\prime = \text{`The sun set over the horizon'}$}. 
We then  obtain the corresponding CLIP embeddings $\{ \tau_\theta (s_i) \}_{i=1}^n$ and $\{ \tau_\theta (s_i^\prime) \}_{i=1}^n$. 
As a result, the text embedding difference $\boldsymbol{d}_i = \tau_\theta (s_i)-\tau_\theta (s^\prime_i)$ can characterize the \textit{saliency} of the   adversary's intention-related sub-sentence in the text embedding space. 
For $n$ such   difference vectors $\{ \mathbf d_i \}_{i=1}^n$, we determine the  \textit{steerable key dimensions} for targeted attack generation by identifying the most influential dimensions in the difference vectors $\{ \mathbf d_i \}_{i=1}^n$. The dimension influence is given by a majority vote of $\{ \mathbf d_i \}_{i=1}^n$ along each dimension. That is, $I_j = 1$ (\textit{i.e.}, the indicator of the $j$th dimension being influential), if $|\textstyle \sum_{i=1}^{n}\text{sign}({d}_{i,j})|>\epsilon  n$, where $\mathrm{sign}$ is the sign operation, ${d}_{i,j}$ is the $j$th entry of $\mathbf d_i$, and $\epsilon < 1  $ is a threshold to pick the most influential dimensions. As a result, the binary vector $I$ encodes the selected key dimensions. By integrating $ I$ into \eqref{eq:3}, we obtain the key dimensions-guided targeted attack generation 
\begin{equation}
\label{eq:cos_mask}
   \displaystyle \min_{\boldsymbol{x}^\prime } ~~\text{cos} ( \tau_\theta (\boldsymbol{x})\odot I,\tau_\theta (\boldsymbol{x}^\prime )\odot I),
\end{equation}
where $\odot$ is the element-wise product. Note that problem \eqref{eq:cos_mask} can be similarly solved as  \eqref{eq:3} using the attack methods introduced before. Fig.\,\ref{fig: teaser}-(c) shows an example   of  using prompt perturbations generated by the proposed targeted attack to erase the image content related to  `a young man'. 

%as its corresponding image content will be modified by the attack.
%which the attack aims to remove the image content related to `a young man' from the normal output of Stable Diffusion

%the original textual prompt $\matbf x$ as 


%we first generate two sequences of sentences $\{ s_i \}_{i=1}^n$ and $\{ s_i^\prime \}_{i=1}^n$, where  $ s_i^\prime $ refers to 


% To target the specific object in prompts, following~\cite{ramesh2022hierarchical} we calculate difference vectors called \textit{text diff} vectors of a set of sentences with the targeted object of interest and another set without it. Specifically, we generate $n$ sentences $s_1, s_2, \dots, s_n$ that include the object from a large language model like ChatGPT, and craft new sentences $s^\prime_1, s^\prime_2,\dots, s^\prime_n$ by removing words related to the object. We then obtain the corresponding CLIP embeddings $\tau_\theta (s_1), \tau_\theta (s_2), \dots, \tau_\theta (s_n)$ and $\tau_\theta (s^\prime_1), \tau_\theta (s^\prime_2),\dots, \tau_\theta (s^\prime_n)$ using the CLIP model. By subtracting each embedding pair, we obtain $n$ \textit{text diff} vectors $\boldsymbol{d}_i = \tau_\theta (s_i)-\tau_\theta (s^\prime_i)$. We calculate the average \textit{text diff} vector $ \boldsymbol{l}= {\textstyle \sum_{i=0}^{n}\boldsymbol{d}_i}$ and use a majority vote to obtain a mask $\boldsymbol{h} $ to denote steerable dimensions, where $j$ th parameter in CILP embedding is $\textbf{\textit{h}}_j=1$ if $|\textstyle \sum_{i=0}^{n}\text{sign}(\boldsymbol{d}_{i,j})|>\tau \times n$. Here, $\tau$ is a threshold and $\text{sign}(\cdot)$ is a function output the sign of input. Fig.~\ref{fig:Flooding} demonstrates that we can manipulate the output image precisely through the average \textit{text diff} vector and the mask $\boldsymbol{h}$ by adding CLIP embedding vector $\boldsymbol{l} \odot \boldsymbol{h}$ to original sentence CLIP embedding generates a more similar image. So we regard the mask $\boldsymbol{h}$ as the steerable dimensions that are more related to the object and propose refiner perturbation attacks with refiner perturbation loss as
% \begin{equation}
% \label{eq:cos_mask}
%     \arg \min_{\boldsymbol{x}^\prime }\text{cos} ( \tau_\theta (\boldsymbol{x})\odot \boldsymbol{h},\tau_\theta (\boldsymbol{x}^\prime )\odot \boldsymbol{h}).
% \end{equation}



% \section{Method}

% \subsection{Preliminaries}
% Denoising Diffusion Probabilistic Models (DDPMs)~\cite{ho2020denoising} is trained to learn data distribution and denoise a normally distributed variable iteratively from noisy images.
% Latent diffusion models (LDMs)~\cite{gal2022image} proposes denoising in latent space by mapping an image $\boldsymbol{x}$ to latent space with an autoencoder $\varepsilon$. The decoder $D$ learns to map latents back to pixel space images $D(\varepsilon(\boldsymbol{x}))\approx \boldsymbol{x}$.
% This diffusion model generates images corresponding to input text $\boldsymbol{y}$, which is tokenized and mapped to a conditioning vector $\tau_\theta(\boldsymbol{y})$ through a domain specific encoder $\tau_\theta$.
% \begin{equation}
%     L_{LDM}:=\mathbb{E} _{\varepsilon (\boldsymbol{x}),\boldsymbol{y},\epsilon \sim \mathcal{N}(0,1),t  }\left [ \left \| \epsilon - \epsilon _\theta (\mathpzc{z}_t,t,\tau _\theta (\boldsymbol{y}) ) \right \|   \right ],
% \end{equation}
% where $t$ denotes time step, $\mathpzc{z}$ denotes the latent at time $t$, and $\epsilon_\theta$ denotes the denoising network. Both $\tau_\theta$ and $\epsilon_\theta$ are jointly optimized during training process in LDM, while $\tau_\theta$ is published pretrained text encoder from CLIP in Stable Diffusion Model and DALLE-2.

% \subsection{Perturbation Attack}
% The robustness issues of published pretrained models possibly affect their downstream tasks. Stable Diffusion Model utilizes pretrained text encoder from CLIP. To perturb generated images, the perturbation attack optimization goal should be defined as:
% \begin{equation}
% \label{eq:loss}
%     \resizebox{\linewidth}{!}{$\arg \max_{\boldsymbol{y}^\prime } \mathbb{E} _{\varepsilon (\boldsymbol{x}),\boldsymbol{y},\epsilon \sim \mathcal{N}(0,1),t }\left [ \left | \epsilon _\theta (\mathpzc{z}_t,t,\tau _\theta (\boldsymbol{y}) ) - \epsilon _\theta (\mathpzc{z}_t,t,\tau _\theta (\boldsymbol{y}^\prime ) ) \right | \right ],$}
% \end{equation}
% where $\boldsymbol{y}^\prime$ is original text input $\boldsymbol{y}$ added one extra perturbation word. However, limited to black box setting, this optimization goal cannot be solved without leakaging gradients of denoising network $\epsilon _\theta $. So our perturbation attack optimization goal aiming at perturbing CLIP text encoder can be defined as:
% \begin{equation}
%     \label{eq:cos}
%     \arg \min_{\boldsymbol{y}^\prime }\text{cos} ( \tau_\theta (\boldsymbol{y}),\tau_\theta (\boldsymbol{y}^\prime )),
% \end{equation}
% where $\text{cos}(\cdot)$ denotes cosine similarity. Then we propose three methods to search perturbation words.

% % \subsection{Searching Words}

% \textbf{PGD Framework}
% We incorporate the PGD attack framework introduced in \textsc{TextGrad}~\cite{hou2022textgrad} into our method. Let $\boldsymbol{y} = [\boldsymbol{y}_1, \boldsymbol{y}_2, \dots, \boldsymbol{y}_L] \in \mathbb{N}^L$ be the input text, where $\boldsymbol{y}_i$ is the index of the $i$-th token. 
% % Tokenizer maps input token $\boldsymbol{y}_i$ to index and retrieved corresponding embedding in the pre-defined dictionary, where crafting a word with gradients directly is impossible. 
% We introduce a character table $\boldsymbol{q}=[\boldsymbol{q}_1,\boldsymbol{q}_2,\dots,\boldsymbol{q}_m]$, where $\boldsymbol{q}_i=\{ \boldsymbol{q}_{i1},\boldsymbol{q}_{i2},\dots,\boldsymbol{q}_{iv} \}$ consists of $v$ single characters in site $i$, including letters, digits, and common symbols, and represents a set of character candidates for the perturbation word and $m$ is the length of the sequence. We introduce a character selection vector $\boldsymbol{a} = [\boldsymbol{a}_1, \boldsymbol{a}_2, \dots, \boldsymbol{a}_m]$, where $\boldsymbol{a}_i$ is the $i$-th position in perturbation sequence.  
% Specifically, $\boldsymbol{a}_i = \left\{ \boldsymbol{a}_{i1}, \boldsymbol{a}_{i2}, \dots, \boldsymbol{a}_{iv} \right\}$ denotes character selection in character table, where $\boldsymbol{a}_{ij} = 1$ indicates that the $j$-th character in the character table $\boldsymbol{q}_{ij}$ is selected at the site $a_i$ and $1^T\boldsymbol{a}_i=1$. To craft the perturbed text $\boldsymbol{y}^\prime = \text{concat}(\boldsymbol{y}, \boldsymbol{a} \odot \boldsymbol{q})$ and optimize $\boldsymbol{a}$ using Eq.~\ref{eq:cos}, we build the gap between continue vector $\boldsymbol{\tilde{a}}$ and discrete vector $\boldsymbol{a}$ through Gumbel Softmax $\mathcal{B}$, where $\mathcal{B}(\boldsymbol{\tilde{a}})=\boldsymbol{a}$ and use $\odot$ to denote element-wise multiplication.






% \textbf{Greedy Algorithm}
% To generate perturbation input that deviates significantly from the original sentence, we propose using a greedy algorithm to search for characters to form a word added to the end of the sentence. The algorithm operates by selecting the character that results in the minimal loss in Eq.~\eqref{eq:loss}, and iteratively repeating this process until a maximal length $m$ of the generated input is reached.

% \textbf{Genetic Algorithm}
% We also introduce the use of a genetic algorithm to search for characters forming a perturbation word. The genetic algorithm operates by generating a population of perturbation candidates and using the loss function in Eq.~\eqref{eq:loss} to evaluate the quality of each candidate. In each iteration, the genetic algorithm uses genetic operations such as mutation and crossover to generate new candidates, and the lowest-loss individuals are selected to form the next generation. The process continues until the number of generations is met. The advantage of the genetic algorithm is that it can explore a broader search space and find globally optimal solutions without gradient.


% \subsection{Refiner Perturbation Attack}
% Refiner Perturbation Attacks are proposed to target specific objects in given prompts. To achieve this, we identify steerable dimensions in CLIP embedding for designing more centralized attacks. By focusing our attack on those dimensions, we increase the likelihood of perturbing the object in output images associated with targeted objects.

% To target the specific object in prompts, following~\cite{ramesh2022hierarchical} we calculate difference vectors called \textit{text diff} vectors of a set of sentences with the targeted object of interest and another set without it. Specifically, we generate $n$ sentences $s_1, s_2, \dots, s_n$ that include the object from a large language model like ChatGPT, and craft new sentences $s^\prime_1, s^\prime_2,\dots, s^\prime_n$ by removing words related to the object. We then obtain the corresponding CLIP embeddings $\tau_\theta (s_1), \tau_\theta (s_2), \dots, \tau_\theta (s_n)$ and $\tau_\theta (s^\prime_1), \tau_\theta (s^\prime_2),\dots, \tau_\theta (s^\prime_n)$ using the CLIP model. By subtracting each embedding pair, we obtain $n$ \textit{text diff} vectors $\boldsymbol{d}_i = \tau_\theta (s_i)-\tau_\theta (s^\prime_i)$. We calculate the average \textit{text diff} vector $ \boldsymbol{l}= {\textstyle \sum_{i=0}^{n}\boldsymbol{d}_i}$ and use a majority vote to obtain a mask $\boldsymbol{h} $ to denote steerable dimensions, where $j$ th parameter in CILP embedding is $\textbf{\textit{h}}_j=1$ if $|\textstyle \sum_{i=0}^{n}\text{sign}(\boldsymbol{d}_{i,j})|>\tau \times n$. Here, $\tau$ is a threshold and $\text{sign}(\cdot)$ is a function output the sign of input. Fig.~\ref{fig:Flooding} demonstrates that we can manipulate the output image precisely through the average \textit{text diff} vector and the mask $\boldsymbol{h}$ by adding CLIP embedding vector $\boldsymbol{l} \odot \boldsymbol{h}$ to original sentence CLIP embedding generates a more similar image. So we regard the mask $\boldsymbol{h}$ as the steerable dimensions that are more related to the object and propose refiner perturbation attacks with refiner perturbation loss as
% \begin{equation}
% \label{eq:cos_mask}
%     \arg \min_{\boldsymbol{y}^\prime }\text{cos} ( \tau_\theta (\boldsymbol{y})\odot \boldsymbol{h},\tau_\theta (\boldsymbol{y}^\prime )\odot \boldsymbol{h}).
% \end{equation}

