\section{Introduction}
\label{sec:intro}

 Diffusion models (DMs), the recently predominant generative modeling technique, have been  used in a wide range of 
computer vision (CV) applications. Examples include Text-To-Image (T2I) generation~\cite{rombach2022high,nichol2021glide,ramesh2022hierarchical,ho2022classifier,saharia2022photorealistic},
adversarial robustness \cite{carlini2022certified,wang2022guided,nie2022diffusion}, and image reconstruction~\cite{song2021solving,abu2022adir}. 
In this paper, we focus on  DM for T2I generation. The key idea following \cite{rombach2022high} is to start from a noisy input and  then iteratively refine it through a pre-trained representation network, \textit{e.g.,} CLIP (Contrastive Language-Image Pretraining)~\cite{radford2021learning} that connects texts and images. The above pipeline allows the use of various  `text prompts' (\textit{i.e.}, natural language inputs served as instructions of DM) to 
effectively control the content of the synthesized images~\cite{wang2020high,ramesh2022hierarchical}. 

% \yh{Diffusion model (DM) has become a popular and successful family of deep generative models, which has a huge appeal to many contemporary computer vision (CV) applications, such as text-to-image synthesis~\cite{rombach2022high,nichol2021glide,ramesh2022hierarchical,ho2022classifier,saharia2022photorealistic}, medical image reconstruction~\cite{song2021solving}, \textit{etc}. Numerous methods have been developed for DM, and one represented paradigm~\cite{rombach2022high} to generate high-quality images is to start from a noise input signal and iteratively refine it through a pretrained representation network (\textit{e.g.,} CLIP~\cite{radford2021learning}), typically encoded as a text embedding. One major advantage of such a pipeline is the applicability of various text prompts to effectively control the content of the output images.}
% \begin{figure}[t]
%     \centering
%     \subfloat[No attack]{\includegraphics[scale=0.13]{figs/Fig1/original.pdf}}
%     \hspace{3mm}
%     \subfloat[\footnotesize{Untargeted attack}\label{fig:perturbation_attack}]{\includegraphics[scale=0.13]{figs/Fig1/Untargeted.pdf}}
%     \hspace{3mm}
%     \subfloat[\footnotesize{Targeted attack}\label{fig:refiner_perturbation_attack}]{\includegraphics[scale=0.13]{figs/Fig1/Targeted.pdf}}\hfill
%     \caption{An illustration of our attacking method against Stable Diffusion models. The generated perturbations are highlighted in \textcolor[RGB]{22,113,250}{blue}. Targeted attack aims to remove the `young man' related contents in the generated figures. All the images are from the same seed. 
%     }
%     \vspace{-0.4cm}
%     \label{fig:apple}
% \end{figure}

\begin{figure}[t]
    \centering
    \begin{tabular}{ccc}
    \hspace*{-4mm}
    \includegraphics[width=.3\linewidth,height=!]{figs/Fig1/original.pdf}  
    &\hspace*{-4mm}
    \includegraphics[width=.3\linewidth,height=!]{figs/Fig1/Untargeted.pdf} 
    &\hspace*{-4mm}
    \includegraphics[width=.3\linewidth,height=!]{figs/Fig1/Targeted.pdf} 
    \vspace*{-2mm}\\
    \hspace*{-4mm}\footnotesize{(a) No attack.} & \hspace*{-4mm}\footnotesize{(b) Untargeted attack.} & \hspace*{-4mm}\footnotesize{(c) Targeted attack.}
    \end{tabular}
    \vspace*{-2mm}
    \caption{\footnotesize{An illustration of our attack  method against Stable Diffusion. The generated perturbations are highlighted in \textcolor[RGB]{22,113,250}{blue}. The targeted attack aims to erase the image content related to   `young man' highlighted in \textcolor{red}{red}. All the images are generated from the same seed.}}
    \label{fig: teaser}
\end{figure}


\input{FigureTex/text_cos}


However,  several   works \cite{mao2022understanding,Fort2021CLIPadversarialstickers,galindounderstanding,noever2021reading} showed that  adversarial perturbations (in terms of small textual/visual input perturbations \cite{goodfellow2014explaining,wang2021adversarial}) can significantly impair the performance of a CLIP model, and thus induce the adversarial robustness concern of its downstream applications. Inspired by the above, our interest in this paper is to investigate the adversarial robustness of T2I generation using CLIP-based DMs, \textit{i.e.}, Stable Diffusion~\cite{rombach2022high} in this work. In particular, we ask:
% recent work~\cite{mao2022understanding}
% % \hl{(HM: This work focuses on perturbation attacks on images and the develop robust visual prompt. There is no other paper proposing the robustness issue on CLIP text encoder.)}
% shows that adversarial perturbations can significantly impair the performance of the CLIP models, which poses great threats to the quality and liability of its downstream tasks. Therefore, it remains elusive if the pretrained CLIP text encoder-enabled DMs, such as Stable Diffusion Model~\cite{rombach2022high} and DALL-E 2~\cite{ramesh2022hierarchical}, are robust against adversarial attacks or not. To this end, we ask:
\begin{tcolorbox}[before skip=0.2cm, after skip=0.2cm, boxsep=0.0cm, middle=0.1cm, top=0.1cm, bottom=0.1cm]
\textit{\textbf{(Q)} Can we generate adversarial perturbations against T2I models in a query-free regime?}
\end{tcolorbox}

Adversarial attacks (also known as adversarial perturbations or   examples) that can cause models' erroneous prediction  have introduced   immense research efforts in both vision and language domains \cite{szegedy2013intriguing,goodfellow2014explaining,jiang2020robust,wang2021adversarial,hou2022textgrad}. A few recent attentions were also paid on T2I models  \cite{maus2023adversarial,wen2023hard} as different from ordinary vision or language models, the latter adopts  a natural language prompt to influence 
its imagery output. The controllability and flexibility  of adjusting text prompts provide a new way to interact with a model. In \cite{maus2023adversarial},  model query-based adversarial attacks were proposed for T2I models. Yet, this work calls for many model queries ($10000$ queries per attack) to find a successful adversarial prompt, \textit{e.g.}, using 4 newly generated words integrated into  the original textual input. By contrast, our work focuses on \textit{query-free} attack generation and the perturbation is constrained to only five characters. In \cite{wen2023hard}, a gradient-based optimization was proposed to generate proper prompts that can match given images or sentences. Although it also demonstrates the controllability of image outputs by textual input prompts, little attention was paid to  adversarial robustness. 
%On the contrary, our goal is to generate an extra word perturbing given sentences.



% To the best of our knowledge, the main research focus in this work \textbf{\textit{(Q)}} still remains an open question in the existing literature. The most relevant work to ours is \cite{maus2023adversarial}, which highly relies on the the heavy queries on the DM ($10000$ queries per attack) and allows attack to append up to $4$ words as perturbations. In contrast, the attack method proposed in this work does not necessitate any queries on the DM directly. Instead, our work shows that an effective attack on DMs can be generated solely based on perturbing the CLIP embeddings with a much smaller perturbation budget (as short as a 5-character word).


% aims to shift the output images to expected classes by introducing extra 4 words in the original sentences searched by 10,000 queries. 
% In contrast, our perturbation goal is to shift output images to irrelevant content with the original sentence by adding a 5-character word to perturb CLIP embeddings, where query is not needed. \YH{Emphasize the difference between this work and ours. Make it crystal clear and specific. (Done)}


% {To address \textbf{\textit{(Q)}}, we need to (1) design an effective attack paradigm to significantly alter the output of the pretrained CLIP encoders and (2) make a in-depth study on the specific text embedding learned by the CLIP encoder and the behavior of the corresponding DMs. 
% Specifically, our \textbf{contributions} are unfolded below.}

To be specific, our \textbf{contributions} are unfolded below.

%\SL{[I do not think you are showing the vulnerability of DMs. You actually show the vulnerability of text-to-image generation models?]}



\ding{172} We develop a query-free adversarial attack generator for T2I DMs. We show that a five-character perturbation, determined by   text embeddings of CLIP,  is able to significantly alter the content of DM-synthesized image     (see \textbf{Fig.~\ref{fig: teaser}b} for an illustration). 

% \ding{172} We reveal the vulnerability of the DMs (diffusion models) to adversarial attacks and develop an effective attack generation method, that can effectively mislead DMs with a five-character perturbation by solely  its CLIP embeddings (see Fig.\ref{fig:apple} for illustrations). 

\ding{173} We provide an analysis of the correspondence between the semantics of synthesized images and  the  embeddings of CLIP. The obtained insight further drives us to  develop   a controllable ``targeted'' attack, 
where the perturbations can be refined to steer the DM's output (see \textbf{Fig.~\ref{fig: teaser}c} for an illustration).

\ding{174} We empirically show the effectiveness of our proposal across three attack implementation methods on a variety of text-image pairs. In particular, we demonstrate that the both the proposed untargeted and targeted Query-Free Attack can successfully alter the output image content using only a 5-character prompt perturbation. This achievement is also reflected by the significantly reduced CLIP scores of the outputs.

%on a twenty-prompts dataset to show the effectiveness of our proposal. 
%\HM{The significant declines in CLIP score indicate the success in both untargeted and targeted query-free attacks.}
% hat  the proposed method can largely reduce the Image-Text similarity in the presence of generated perturbations.
% \YH{more result summaries not ready yet.}
% In the meantime, the refiner attack 

\iffalse
Diffusion models are generative models that iteratively refine a noise input guided by a learned representation, typically encoded as a text embedding, to generate high-quality images~\cite{rombach2022high,nichol2021glide,ramesh2022hierarchical}. They have been applied to various image generation tasks, including image inpaint and text-to-image synthesis, and have several advantages including producing high quality images and being able to control the output image with the text prompts.
%
However, recent works~\cite{maus2023adversarial,daras2022discovering} show that variations in prompts lead to unexpected results on images. 
Their robustness to perturbation attacks is a concern, particularly when some of them like Stable Diffusion Model~\cite{rombach2022high} and DALL-E 2~\cite{ramesh2022hierarchical} utilize a pre-trained text encoder from CLIP. 
The robustness of the text encoder in CLIP has a direct impact on the robustness of diffusion models, as the produced text representations are used to guide the iterative refinement process in the diffusion model.
As shown in Fig.~\ref{fig:apple}, the addition of the word ``-08=*'' at the end of the sentence ``A snake and a young man'' causes a deviation from the original CLIP embedding and successfully removes the young man from the output image. This result illustrates that perturbation attacks that make minor changes to the original inputs while preserving their semantic meaning can generate output images that differ significantly from the originals. The prompt ``-08=*'' is difficult for humans to comprehend as well as the underlying process in diffusion models.
% As Fig.~\ref{fig:apple} shown, perturbation attacks that have minor changes on original input and keep their semantic meanings unchange, can result in altered output images that are different from the original ones and difficult to understand the process for human beings.
It is crucial to explore the robustness of diffusion models, which can help to understand the mistakes of diffusion models work encounter unexpected outputs.
% \subsection{Robustness of The Text Encoder}

Previous studies have demonstrated the excellent performance of the CLIP model in matching a variety of text inputs with image inputs. However, the robustness of the text encoder in CLIP has not been thoroughly examined. The text encoder should be able to distinguish between meaningful and meaningless words to capture the semantic meaning of sentences. Nevertheless, non-sense words such as ``\#F0\#D'' (\#F0F0F0 indicates grey color but ``\#F0\#D is meaningless) composed of letters, digits, and common symbols, can cause a significant deviation of the text representation output. This deviation can also greatly impact its downstream task - serving as a text-based guidance signal in diffusion models. As demonstrated in Fig.~\ref{fig:text_cos}, adding ``\#F0\#D'' is processed to low cosine similarity CLIP embeddings with the original one, leading to unexpected output from the diffusion model, where a grey hat is generated instead of a yellow hat. This demonstrates the vulnerability of the text encoder to non-sense words and its potential impact on downstream tasks.
% The robustness of CLIP, a pre-trained multimodal model, has been demonstrated in previous studies for its ability to match text inputs with image inputs effectively. However, the robustness of the single text encoder has not been fully explored. The text encoder should be capable of distinguishing between meaningful and meaningless words, such as non-sense words like ``\#F0\#D'' (\#F0F0F0 indicates a color index but \#F0\#D is meaningless), in order to capture the semantic meaning of sentences. However, as shown in Fig. \ref{fig:text_cos}, non-sense words made up of letters, digits, and common symbols, like ``\#F0\#D'' , can result in significant deviation of the text representation output. Although this deviation may have a limited impact on its original task of calculating the cosine similarity of text and images in CLIP, where the joint multimodal embedding projectors in CLIP may correct it, it can have a serious impact on image generation as a text-based guidance signal. As illustrated in Fig.~\ref{fig:text_cos}, three different sentences have similar CLIP score with a image but they are significantly different in CLIP embedding generated by CLIP text encoder and affect the downstream task, where diffusion model generates unexpected output (a black and white hat instead of yellow hat).

% Although the robustness of CLIP model that it matches text inputs and images inputs well has been proven by previous works, the robustness of single text encoder is ignored. Intuitively, the text encoder should distinguish meaningful words and meaningless or unrelevant words then mitigrate the affect of meaningless words especially non-sense words like "\#haha`` to capture the semantic meaning of the sentences. However, as it shown in Fig. \ref{fig:text_cos}, there are non-sense words consisting of letters, digits and common symbols like ''\#F0\#D``  causing significant deviation of the output of text representation. This minor change causes limited affect on the original task that calculates the cosine similarity of text and images, where the joint multimodal embedding projectors in CLIP may correct it. But the deviation seriously affects image generation as a text-based guidance signal, which is shown in Fig.~\ref{fig:text_cos}.

In this paper, we reveal the weakness of the pre-trained text encoder from CLIP and its impact on the performance of Stable Diffusion Model. Next, we introduce diffusion-free perturbation attacks and evaluate diffusion model robustness against such attacks. Additionally, we propose a method to identify a subset of dimensions in CLIP embeddings that are more related to specific objects and develop a refiner perturbation attack based on this. Our results provide insights into the vulnerability of diffusion models to perturbation attacks. Our results shed light on the susceptibility of diffusion models to perturbation attacks, and our primary contributions are summarized as follows:

\ding{172} We propose diffusion-free perturbation attacks by generating prompts that cause deviation on CLIP embedding.

\ding{173} We propose a refiner perturbation attack that targets specific contextual information in sentences.

\ding{174} We conduct extensive experiments to evaluate the impacts of perturbation attacks and refiner perturbation attacks on Stable Diffusion.

\fi



% \begin{itemize}
% \item We propose diffusion-free perturbation attacks by generating prompts that cause deviation on CLIP embedding.

% \item We propose a refiner perturbation attack that targets specific contextual information in sentences.

% \item We conduct extensive experiments to evaluate the impacts of perturbation attacks and refiner perturbation attacks on Stable Diffusion.
% \end{itemize}

% In this paper, we first reveal the weakness of the pre-trained text encoder from CLIP and show how it affects the outputs of Stable Diffusion Model. We next introduce four perturbation attacks in black box setting on diffusion models and evaluating diffusion models robustness in the face of such attacks. Then we proposed a method to find key dimensions in CLIP text embeddings corresponding to specific semantic meanings and develop semantic perturbation attack upon it. Our results provide insights into the vulnerability of diffusion models to perturbation attacks and inform the development of more robust and secure diffusion models in the future.
% \begin{figure}[tb]
%     \centering
%     \subfloat[A purple grape cluster on a vine]{\includegraphics[scale=0.15]{figs/grape.png}}\hspace{5mm}
%     \subfloat[A purple grape cluster on a vine RTR0$|$]{\includegraphics[scale=0.15]{figs/rtr0.png}}\hfill

%     \subfloat[A purple grape cluster on a vine]{\includegraphics[scale=0.15]{figs/grape.png}}\hspace{5mm}
%     \subfloat[A purple grape cluster on a vine RTR0$|$]{\includegraphics[scale=0.15]{figs/rtr0.png}}\hfill

%     \caption{Perturbation attacks with 5 characters.}
%     \vspace{-0.5cm}
%     \label{fig:apple}
% \end{figure}
% \begin{figure}[t]
%     \centering
%     \includegraphics[scale=0.12]{figs/fig2.pdf}
%     \caption{Examples of 5-characters perturbation attacks. Above images (before attacks) and below images (after attacks) are generated from the same seed.}
%     \vspace{-1.07cm}
%     \label{fig:apple}
% \end{figure}
