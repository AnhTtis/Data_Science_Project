@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


@InProceedings{gong2017blur2mf,
author = {Gong, Dong and Yang, Jie and Liu, Lingqiao and Zhang, Yanning and Reid, Ian and Shen, Chunhua and van den Hengel, Anton and Shi, Qinfeng},
title = {From Motion Blur to Motion Flow: A Deep Learning Solution for Removing Heterogeneous Motion Blur},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@inproceedings{zhong2020efficient,
  title={Efficient spatio-temporal recurrent neural network for video deblurring},
  author={Zhong, Zhihang and Gao, Ye and Zheng, Yinqiang and Zheng, Bo},
  booktitle={European Conference on Computer Vision},
  pages={191--207},
  year={2020},
  organization={Springer}
}

@article{zhong2022real,
  title={Real-World Video Deblurring: A Benchmark Dataset and an Efficient Recurrent Neural Network},
  author={Zhong, Zhihang and Gao, Ye and Zheng, Yinqiang and Zheng, Bo and Sato, Imari},
  journal={International Journal of Computer Vision},
  pages={1--18},
  year={2022},
  publisher={Springer}
}

@article{DBLP:journals/corr/abs-2003-12039,
  author= {Zachary Teed and Jia Deng},
  title={{RAFT:} Recurrent All-Pairs Field Transforms for Optical Flow},
  journal={CoRR},
  volume    = {abs/2003.12039},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.12039},
  eprinttype = {arXiv},
  eprint    = {2003.12039},
  timestamp = {Mon, 01 Feb 2021 18:33:24 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-12039.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{exp_traj,
  author={Zhang, Youjian and Wang, Chaoyue and Maybank, Stephen J. and Tao, Dacheng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Exposure Trajectory Recovery From Motion Blur}, 
  year={2022},
  volume={44},
  number={11},
  pages={7490-7504},
  doi={10.1109/TPAMI.2021.3116135}}

  @misc{sunetal,
  doi = {10.48550/ARXIV.1503.00593},
  
  url = {https://arxiv.org/abs/1503.00593},
  
  author = {Sun, Jian and Cao, Wenfei and Xu, Zongben and Ponce, Jean},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences, I.4},
  
  title = {Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@Article{sensors,
AUTHOR = {Simon, Gyula and Vakulya, Gergely and Rátosi, Márk},
TITLE = {The Way to Modern Shutter Speed Measurement Methods: A Historical Overview},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {5},
ARTICLE-NUMBER = {1871},
URL = {https://www.mdpi.com/1424-8220/22/5/1871},
PubMedID = {35271017},
ISSN = {1424-8220},
ABSTRACT = {Exposure time is a fundamental parameter for the photographer when the photo is composed, and the exact length of the exposure may be an essential determinant of performance in certain camera-based applications, e.g., optical camera communication (OCC) systems. There can be several reasons to measure the shutter speed of a camera: shutter speed may be checked at the time of manufacturing; it may be necessary to recheck in case of an elder camera model; it may be necessary to be measured if its exact value is not provided by the manufacturer; or a precise measurement may be necessary for a demanding application. In this paper various methods for shutter speed measurement are reviewed, presenting and analyzing methods that are still relevant today either for manufacturers, service personnel, amateur photographers, or the developers of camera-based systems. Each presented method is illustrated by real measurement results and the performance properties of the methods are also presented.},
DOI = {10.3390/s22051871}
}

@inproceedings{huang2022rife,
  title={Real-Time Intermediate Flow Estimation for Video Frame Interpolation},
  author={Huang, Zhewei and Zhang, Tianyuan and Heng, Wen and Shi, Boxin and Zhou, Shuchang},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  year={2022}
  }
  
@misc{red_camera_shutter,
  author = {RED Digital Cinema LLC},
  title = {SHUTTER ANGLES AND CREATIVE CONTROL},
  url = {https://www.red.com/red-101/shutter-angle-tutorial}
}

@InProceedings{Nah_2017_CVPR,
  author = {Nah, Seungjun and Kim, Tae Hyun and Lee, Kyoung Mu},
  title = {Deep Multi-Scale Convolutional Neural Network for Dynamic Scene Deblurring},
  booktitle = {CVPR},
  month = {July},
  year = {2017}
}

@inproceedings{su2017deep,
  title={Deep Video Deblurring for Hand-held Cameras},
  author={Su, Shuochen and Delbracio, Mauricio and Wang, Jue and Sapiro, Guillermo and Heidrich, Wolfgang and Wang, Oliver},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1279--1288},
  year={2017}
}


@INPROCEEDINGS{8639163,
  author={Güera, David and Delp, Edward J.},
  booktitle={2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)}, 
  title={Deepfake Video Detection Using Recurrent Neural Networks}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/AVSS.2018.8639163}
  }

  @article{ZHAI2021107861,
title = {Optical flow and scene flow estimation: A survey},
journal = {Pattern Recognition},
volume = {114},
pages = {107861},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.107861},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321000480},
author = {Mingliang Zhai and Xuezhi Xiang and Ning Lv and Xiangdong Kong},
keywords = {Motion analysis, Optical flow, Scene flow, Variational model, Deep learning, Convolutional neural networks (CNNs)},
abstract = {Motion analysis is one of the most fundamental and challenging problems in the field of computer vision, which can be widely applied in many areas, such as autonomous driving, action recognition, scene understanding, and robotics. In general, the displacement field between subsequent frames can be divided into two types: optical flow and scene flow. The optical flow represents the pixel motion of adjacent frames. In contrast, the scene flow is a 3D motion field of the dynamic scene between two frames. Traditional approaches for the estimation of optical flow and scene flow usually leverage the variational technique, which can be solved as an energy minimization process. In recent years, deep learning has emerged as a powerful technique for learning feature representations directly from data. It has led to remarkable progress in the field of optical flow and scene flow estimation. In this paper, we provide a comprehensive survey of optical flow and scene flow estimation. First, we briefly review the pioneering approaches that use variational technique and then we delve in detail into the deep learning-based approaches. Furthermore, we present insightful observations on evaluation issues, specifically benchmark datasets, evaluation metrics, and state-of-the-art performance. Finally, we give the promising directions for future research. To the best of our knowledge, we are the first to review both optical flow and scene flow estimation, and the first to cover both traditional and deep learning-based approaches.}
}