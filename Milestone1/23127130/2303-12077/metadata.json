{
    "arxiv_id": "2303.12077",
    "paper_title": "VAD: Vectorized Scene Representation for Efficient Autonomous Driving",
    "authors": [
        "Bo Jiang",
        "Shaoyu Chen",
        "Qing Xu",
        "Bencheng Liao",
        "Jiajie Chen",
        "Helong Zhou",
        "Qian Zhang",
        "Wenyu Liu",
        "Chang Huang",
        "Xinggang Wang"
    ],
    "submission_date": "2023-03-21",
    "revised_dates": [
        "2023-06-30"
    ],
    "latest_version": 2,
    "categories": [
        "cs.RO",
        "cs.CV"
    ],
    "abstract": "Autonomous driving requires a comprehensive understanding of the surrounding environment for reliable trajectory planning. Previous works rely on dense rasterized scene representation (e.g., agent occupancy and semantic map) to perform planning, which is computationally intensive and misses the instance-level structure information. In this paper, we propose VAD, an end-to-end vectorized paradigm for autonomous driving, which models the driving scene as a fully vectorized representation. The proposed vectorized paradigm has two significant advantages. On one hand, VAD exploits the vectorized agent motion and map elements as explicit instance-level planning constraints which effectively improves planning safety. On the other hand, VAD runs much faster than previous end-to-end planning methods by getting rid of computation-intensive rasterized representation and hand-designed post-processing steps. VAD achieves state-of-the-art end-to-end planning performance on the nuScenes dataset, outperforming the previous best method by a large margin. Our base model, VAD-Base, greatly reduces the average collision rate by 29.0% and runs 2.5x faster. Besides, a lightweight variant, VAD-Tiny, greatly improves the inference speed (up to 9.3x) while achieving comparable planning performance. We believe the excellent performance and the high efficiency of VAD are critical for the real-world deployment of an autonomous driving system. Code and models will be released for facilitating future research.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.12077v1",
        "http://arxiv.org/pdf/2303.12077v2"
    ],
    "publication_venue": "Code&Demos: https://github.com/hustvl/VAD"
}