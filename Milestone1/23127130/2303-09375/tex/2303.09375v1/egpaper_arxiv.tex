\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[table]{xcolor}

\usepackage{xcolor, soul}
\sethlcolor{yellow} % delete this after text editing

\usepackage{subcaption}
\captionsetup{compatibility=false}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{8945} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{\vspace{-0.5em}DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars}

\author{David Svitov\\
Samsung AI Center\\
{\tt\small david.svitov@yandex.ru}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Dmitrii Gudkov\\
Samsung AI Center\\
{\tt\small d.gudkov@samsung.com}
\and
Renat Bashirov\\
Samsung AI Center\\
{\tt\small r.bashirov@samsung.com}
\and
Victor Lemptisky\\
Cinemersive Labs\\
{\tt\small victor@cinemersivelabs.com}
}
\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
   We present DINAR, an approach for creating realistic rigged fullbody avatars from single RGB images. Similarly to previous works, our method uses neural textures combined with the SMPL-X body model to achieve photo-realistic quality of avatars while keeping them easy to animate and fast to infer. To restore the texture, we use a latent diffusion model and show how such model can be trained in the neural texture space. The use of the diffusion model allows us to realistically reconstruct large unseen regions such as the back of a person given the frontal view. The models in our pipeline are trained using 2D images and videos only. In the experiments, our approach achieves state-of-the-art rendering quality and good generalization to new poses and viewpoints. In particular, the approach improves state-of-the-art on the SnapshotPeople public benchmark.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

The use of fullbody avatars in the virtual and augmented reality applications~\cite{mystakidis2022metaverse} are one of the drivers behind the recent surge of interest in fullbody photorealistic avatars~\cite{remelli2022drivable, alldieck2022photorealistic, prokudin2021smplpix}. Apart from the realism and fidelity of avatars, the ease of acquisition of new personalized avatars is of paramount importance. Towards this end, several works propose methods to restore 3D textured model of a human from a single image~\cite{saito2019pifu, saito2020pifuhd, he2020geo, alldieck2022photorealistic} but such models require additional efforts to produce rigging for animation. The use of additional rigging methods significantly complicates the process of obtaining an avatar and often restricts the poses that can be handled. At the same time, some of the recent methods use textured parametric models of human body~\cite{xu20213d, lazova2019360} while applying inpainting in the texture space. Current texture-based methods, however, lack photo-realism and rendering quality.

An alternative to using classical RGB textures directly is to use deferred neural rendering~\cite{thies2019deferred}. Such approaches make it possible to create human avatars controlled by the parametric model \cite{loper2015smpl, prokudin2021smplpix}. The resulting avatars are more photo-realistic and easier to animate. However, existing approaches require a video sequence to create an avatar \cite{raj2021anr}. The StylePeople system~\cite{grigorev2021stylepeople}, which is also based on deferred neural rendering and parametric model, provides an opportunity to create avatars from single images, however the quality of rendering for unobserved body parts is low. 

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{images/Azure.pdf}
   \caption{\textbf{Animations of one-shot avatars.} We generate avatars of previously unseen people from single images and animate the avatars by changing their SMPL-X poses. Our model produces plausible results for new poses and views, even for complex and loose garments like skirts and can handle complex poses (such as hands near the body) gracefully.}
   \label{fig:Azure}
\end{figure*}

We propose a new method to create photo-realistic animatable human avatars from a single photo. To make the avatars animatable we leverage neural texture approach~\cite{thies2019deferred} along with the SMPL-X parametric body model~\cite{prokudin2021smplpix}. We propose a new architecture for generating the neural textures, in which the texture comprises both the RGB part explicitly extracted from the input photograph by warping and additional neural channels obtained by mapping the image to a latent vector space and decoding the result into the texture space. As is customary with neural rendering~\cite{thies2019deferred, grigorev2021stylepeople, raj2021anr}, the texture generation is trained in an end-to-end fashion with the rendering network.

To restore the neural texture for unobserved parts of the human body we develop a diffusion model~\cite{ho2020denoising}. This approach allows us to obtain photo-realistic human avatars from single images. In the presence of multiple images, we can merge neural textures corresponding to different images while restoring parts that are still missing by diffusion-based inpainting. The use of diffusion for inpainting distinguishes our approach from several previous works~\cite{lazova2019360, he2021arch++, grigorev2019coordinate} including StylePeople~\cite{grigorev2021stylepeople} that rely on generative adversarial framework~\cite{NIPS2014_5ca3e9b1} to perform inpainting of human body textures. As in other image domains~\cite{dhariwal2021diffusion, saharia2022image}, we found that the use of diffusion alleviates problems with mode collapse and allows to obtain plausible samples from complex multi-modal distributions. To the best of our knowledge, we are the first to extend the diffusion models to the task of generating human body textures.

To sum up, our contributions are as follows:
\begin{itemize}
    \item We propose a new approach for modeling human avatars based on neural textures that combine the RGB and the latent components.   
    \item We adapt the diffusion framework for neural textures and demonstrate that it is capable of inpainting such textures.
    \item We demonstrate the ability of our system to build realistic animatable avatars from a single photograph.
\end{itemize}

The proposed approach allows us to obtain a photorealistic animatable person's avatar from a single image. Specifically, when the input photograph is taken from the front, we observe that the person's back is restored in a consistent photo-realistic manner. We demonstrate the effectiveness and accuracy of our approach on real-world images from SnapshotPeople \cite{alldieck2018video} public benchmark and images of people in natural poses. 


\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth]{images/Fig1.pdf}
   \caption{\textbf{Overview of our neural texture avatar pipeline:} Given an RGB image and the corresponding SMPL-X model as input, we use a UV-map to sample the RGB texture. Using an encoder and a StyleGAN2 generator, we convert the input image into a neural texture. We rasterize SMPL-X with concatenation of the neural and RGB textures and translate it to the RGB+mask output using the rendering network.}
   \label{fig:VAE}
\end{figure*}

%-------------------------------------------------------------------------
\section{Related work}

\textbf{Full body human avatar.}
Many avatar systems are based on parametric human body models, of which the most popular are the SMPL~\cite{loper2015smpl} body model as well as the SMPL-X model~\cite{prokudin2021smplpix} which augments SMPL with facial expressions and hand articulations. Such models represent human body without garments or hair. Approaches based on deferred neural rendering (DNR)~\cite{thies2019deferred} or neural radiance fields (NeRF)~\cite{mildenhall2021nerf} can be used to add clothing and perform photo-realistic rendering. DNR uses a multi-channel trainable neural texture and a renderer convolutional network to render the resulting avatars in a realistic way~\cite{raj2021anr, grigorev2021stylepeople}. It makes it easier to animate the resulting avatar. NeRF uses sampling along a ray in implicit space for rendering and allows one to extract accurate and consistent geometry~\cite{saito2019pifu, huang2020arch, alldieck2022photorealistic}.

\textbf{One-shot full body avatar.} 
One-shot avatar approaches reconstruct human body avatars from a single image. Early works achieved this by inpainting partial RGB textures~\cite{xu20213d, lazova2019360}. These approaches did not allow realistic modeling of avatars with clothing. More recent works on one-shot body modeling relied on implicit geometry and radiance models, which predict occupancy and color with the multi-layer perceptron conditioned on feature vectors extracted from the input image~\cite{saito2019pifu, alldieck2022photorealistic,he2020geo}. While this line of work often manages to recover intricate geometric details, the realism of the recovered texture in the unobserved parts is usually limited.

The ARCH system~\cite{huang2020arch} uses the rigged canonical space to build avatars suitable for animations. ARCH++~\cite{he2021arch++} improves the quality of resulting avatars by revisiting the major steps of ARCH. They also solve the challenge of unseen surfaces by synthesizing the back views of a person from the front view. PHORHUM~\cite{alldieck2022photorealistic} improves the modeling of geometry by adding the reasoning about scene illumination and albedo of the surface. % The authors also demonstrate rigging of their avatars by applying GHUM \cite{xu2020ghum} to the mesh.
ICON \cite{xiu2022icon} uses local features to avoid the dependence of the reconstructed geometry on the global pose. Their approach first estimates separate models from each view and then merges the models using SCANimate \cite{saito2021scanimate}. The method uses RGB textures applied to reconstructed geometry, which limits the rendering photo-realism.

An alternative approach to getting one-shot avatars is to use avatar generative models as proposed in StylePeople \cite{grigorev2021stylepeople}. The authors circumvent the need to reconstruct the unseen parts by exploiting the entanglement in the GANs latent space. Unfortunately, the imperfection of their underlying generative model often leads to implausible appearance of unobserved parts.

\textbf{Diffusion models.} Diffusion models \cite{sohl2015deep} are probabilistic models for learning the distribution of $p(x)$ by gradual denoising of a normally distributed variable. Such denoising corresponds to learning the inverse process for a fixed Markov chain of length $\mathrm{T}$. The most successful models for image generation \cite{dhariwal2021diffusion, ho2020denoising, rombach2022high} use the reweighted variant of the variational lower bound on $p(x)$. These models can also be interpreted as denoising autoencoders $\epsilon_\omega(x_t, t); t=1, ..., \mathrm{T}$ with shared weights. These autoencoders learn to predict $x_{t-1}$ with reduced noise level over $x_t$. In \cite{ho2020denoising} was demonstrated that such denoising models can be trained with a simplified loss function:
\begin{equation} \label{eq:dm_loss}
L_{DM} = \mathbb{E}_{x, \epsilon \sim \mathcal{N}(0, 1), t}[||\epsilon - \epsilon_\omega(x_t, t)||_2^2],
\end{equation}
where $t$ is uniformly sampled from $\{1, ..., \mathrm{T}\}$. Our work builds on recent advances in inpainting with diffusion models \cite{saharia2022palette, lugmayr2022repaint, romero2022ntire}. In particular, we use latent diffusion \cite{rombach2022high} in our approach, which has been shown to be effective in inpainting of RGB images.

Diffusion models \cite{sohl2015deep} emerged as an alternative to generative models \cite{NIPS2014_5ca3e9b1}. In the human modeling domain, the diffusion models were shown to work very well for the task of human motion generation~\cite{tevet2022human, zhang2022motiondiffuse, ren2022diffusion}. RODIN \cite{Wang2022RodinAG} employs a diffusion framework to generate non-rigged head avatars as neural radiance fields represented by 2D feature maps. TEXTure \cite{Richardson2023TEXTureTT} uses text guidance and a pre-trained diffusion model to produce a view-consistent RGB texture of a given geometry. To the best of our knowledge, diffusion models have not yet been used to generate neural textures for 3D objects.

%-------------------------------------------------------------------------
\section{Method}

Our approach has two components: the avatar generation model and the inpainting model. The scheme of our avatar generation model is presented in Fig. \ref{fig:VAE}. The model reconstructs the neural texture from the input image using two pathways and then uses the texturing operation as well as the neural rendering to synthesize images of the avatar. We train the inpainting model based on the denoising diffusion probabilistic model (DDPM) \cite{ho2020denoising} on top of the pretrained avatar generation model.

\subsection{Avatar generation model}

Our approach creates a 3D rigged avatars of clothed human using several neural networks and the texturing modules trained together in the end-to-end fashion. The overall architecture takes as input an RGB image and a parametric SMPL-X~\cite{SMPL-X:2019} body model. During training, we fit \mbox{SMPL-X} models to the train images using an implementation of \mbox{SMPLifyX}~\cite{SMPL-X:2019} approach with an additional segmentation loss that allows us to match human silhouettes better. 

In more detail, we use an SMPL-X fixed-topology mesh $M(p, s)$, driven by sets of pose parameters $p$ and shape parameters $s$. We also use the UV-map function $F_\textrm{UV}(M(p_\textrm{target}, s), C_\textrm{target})$ for the texture mapping. For SMPL-X, we employ a customized UV unwrapping with a front cut to avoid difficult to inpaint back-view seams. The rendering process takes the mesh $M$ and the desired camera parameters $C_{target}$ as input. The pose parameters $p_\textrm{target}$ are used to rig the mesh. The texturing function $F_\textrm{UV}$ generates a UV-map of size $H \times W \times 2$, where $H$ and $W$ determine the size of the output image, and for each pixel 
the texture coordinates $[i, j]$ on the $L$-channeled texture $T$ are specified. We thus use $F_\textrm{UV}$ in the rasterizer $R(F_\textrm{UV}, T)$ to map the pixels in the output image to the features of the texels of the neural texture $T$. The rasterizer $R$ thus produces the image of size $H \times W \times L$. %In our experiments we set the output size to $512 \times 512$ and the texture size to $256 \times 256$, so that $R: {\rm I\!R}^{256 \times 256 \times L} \rightarrow {\rm I\!R}^{512 \times 512 \times L}$.

We set parameters of the rasterizer $R$ so that $H$ and $W$ correspond to the height and width of the input RGB image $I_\textrm{rgb}$. In this case, we can use UV not only to map feature vectors from the neural texture $T$, but also to sample color values from the input image $I_\textrm{rgb}$ into the texture space: $T_\textrm{rgb} = \xi(F_\textrm{UV}(M(p_\textrm{input}, s), C_\textrm{input}), I_\textrm{rgb})$. Here, the number of channels $L=3$, and $p_\textrm{input}$ corresponds to the pose of human in $I_\textrm{rgb}$, and $C_\textrm{input}$ are parameters of the camera restored from $I_\textrm{rgb}$. The mapping $\xi$ transfers the color value from $I_\textrm{rgb}$ to the texture $T_\textrm{rgb}$ point specified by $F_\textrm{UV}$. This RGB texture allows us to explicitly save information about high-frequency details and original colors, which are hard to preserve when mapping the whole image to a vector of limited dimensionality (as discussed below). We additionally apply inpainting of small gaps with averaging neighbor pixels to fill the gaps in $T_\textrm{rgb}$. We also save the binary map of sampled pixels to the $B_\textrm{smp}$ and the map of the sampled and inpainted pixels to the $B_\textrm{fill}$.

The main part of the neural texture is $T_\textrm{gen}$. It has the number of channels $L=16$ and is generated using the encoder-generator architecture $T_\textrm{gen} = G(E(I_\textrm{rgb}))$. The encoder $E$ is based on the StyleGAN2 \cite{karras2020analyzing} discriminator architecture and compresses the input image $I_\textrm{rgb}$ to a vector $\vec{v}$ of dimension 512. The generator $G(\vec{v})$ has the architecture of the StyleGAN2 generator and converts the vector $\vec{v}$ into a $T_\textrm{gen}$ neural texture with the number of channels $L=16$ as in StylePeople \cite{grigorev2021stylepeople}.

The final neural texture used in our method has a dimension of $256 \times 256 \times 21$ and consists of the concatenation of: the generated texture $T_\textrm{gen}$ ($256 \times 256 \times 16$), the texture $T_\textrm{rgb}$ sampled from the RGB image ($256 \times 256 \times 3$) and the two binary segmentation maps ($B_\textrm{smp}$ and $B_\textrm{fill}$):
\begin{equation}
T = T_\textrm{gen} \oplus T_\textrm{rgb} \oplus B_\textrm{smp} \oplus B_\textrm{fill}.
\end{equation}
We note that such an approach with the explicit use of RGB channels as part of the neural texture was originally used in \cite{thies2019deferred}.

We use the neural renderer $\theta(R(F_\textrm{UV}, T))$ to translate the rasterized image $R(F_\textrm{UV}, T)$ with $L$ channels into $I_\textrm{rend}$ output RGB image. Neural renderer $\theta$ has a U-Net \cite{ronneberger2015u} architecture with ResNet \cite{he2016deep} blocks. We train the renderer $\theta$ jointly with the neural texture encoder $E$ and generator $G$. Thus, rendering an avatar in a new pose $p_\textrm{target}$ by an image $I_\textrm{rgb}$ with the person in a pose $p_\textrm{input}$ has the following form:

\begin{equation}
\begin{split}
T = & G(E(I_\textrm{rgb})) \oplus \\
    & \xi(F_\textrm{UV}(M(p_\textrm{input}, s), C_\textrm{input}), I_\textrm{rgb}) \oplus \\
    & B_\textrm{smp} \oplus B_\textrm{fill};
\end{split}
\end{equation}

\begin{equation} \label{eq:rendering}
I_\textrm{rend} = \theta(R(F_\textrm{UV}(M(p_\textrm{target}, s), C_\textrm{target}), T))
\end{equation}

During training, we minimize the following losses: the difference loss $L_2(I_\textrm{rend}, I_\textrm{GT})$ between the rendered and ground truth images; the perceptual loss $LPIPS(I_\textrm{rend}, I_\textrm{GT})$ \cite{zhang2018perceptual}; the Dice loss \cite{sudre2017generalised} between the ground truth $S_\textrm{GT}$ and the predicted $S_\textrm{rend}$ segmentation masks. We calculate $L2$ and $LPIPS$ for the entire image and additionally with a weight of $0.1$ for the area with a face, since it has been demonstrated in~\cite{fruhstuck2022insetgan} that the face is very important for human perception. Additionally, we use an adversarial loss to make $I_\textrm{rend}$ look more realistic and sharp. We use nonsaturating adversarial loss $Adv$~\cite{NIPS2014_5ca3e9b1} with the StyleGAN2 discriminator $D$ with R1-regularization \cite{mescheder2018training}. The overall loss thus has the following form: 

\begin{equation} \label{eq:losses}
\begin{split}
Loss = & \lambda_1 \cdot ||I_\textrm{rgb} - I_\textrm{rend}||_2^2 + \\
       & \lambda_2 \cdot LPIPS(I_\textrm{rgb}, I_\textrm{rend}) + \\
       & \lambda_3 \cdot Dice(S_{GT}, S_\textrm{rend}) + \\
       & \lambda_4 \cdot Adv(D(I_\textrm{rend})) + \lambda_5 \cdot R1_\textrm{reg}.
\end{split}
\end{equation}

We describe the choice of hyperparameters $\lambda_{1..5}$ in Section \ref{sec:Experiments}.

We also apply several techniques to improve avatar quality in the inference stage. To enhance the texture details in the visible part, we perform a few optimization steps of RGB channels with gradients from the differentiable renderer for the input image. To reduce the impact of \mbox{SMPL-X} fitting  imperfections, we detect areas of human self-occlusion in the input image and do not sample the texture along the overlap outline. The restoration of these areas are left for the inpainting process described below. These techniques are described in detail in Supplementary materials. 

Our training is performed on multi-view image sets such as sets of frames of the video (or such as sets of renders of 3D models). The training data and network training details are discussed in Section \ref{sec:Experiments}. During training, in each case we take two different frames from the same set, one as the input image $p_\textrm{input}$ and the other as the target image $p_\textrm{target}$ (the two thus having different camera parameters as well as two different body pose parameters). This is essential for the pipeline to generalize to new camera positions $C$ and poses $p$. To accomplish that, the renderer and the texture generation modules learn to inpaint small fragments unseen in $I_\textrm{rgb}$. 

\subsection{Texture merging} \label{sssec:merging}

\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{images/Fig2.pdf}

   \caption{\textbf{Texture merging:} We get the full neural texture as the weighted sum of textures from different view points. As weights, we use the angles between the normal vectors and the direction of the camera. For simplicity, only the RGB channels are shown in the figure, but the merging affects all channels.}
   \label{fig:Merge}
   \vspace{-0.5em}
\end{figure}

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth]{images/Fig3.pdf}

   \caption{\textbf{Texture inpainting:} Before inpainting, we reduce size of the neural textures with VQGAN encoder. At train U-Net learns to remove noise from Ground truth texture for a given step $t$. At inference, we iteratively apply $\mathrm{T}$ steps of denoising to sample the image. The sampled texture is transformed to the original size by the VQGAN decoder.}
   \label{fig:Inpaint}
   \vspace{-0.5em}
\end{figure*}

While the texture generator and the renderer learn to compensate for the small amount of the unseen surfaces that may be present in the target view, we have found that such ability is limited to small changes in body pose and/or camera parameters. The easiest way to obtain an avatar that can be rendered from arbitrary angles is to create it from several images by merging the corresponding neural textures. For that we can use a simple blending scheme (Fig. \ref{fig:Merge}). Specifically, assume the $N$ input images $I_\textrm{rgb}^i$ with different $C_\textrm{input}^i$ are given that result in $N$ neural textures $T^i$. These textures naturally cover different areas of the body visible in different input images. To combine these textures, we define the function $F(T^1 ... T^N, \lambda^1 ... \lambda^N)$ shown in Fig. \ref{fig:Merge}. The $\lambda^i$ is auxiliary information about the visible in $I_\textrm{rgb}^i$ part of the texture.

As auxiliary information $\lambda^i$ we use the angles between the normal vectors of corresponding point of the mesh $M^i$ and direction vector of the camera. Thus $\lambda^i$ defines how frontal each texture point is to the camera. We perform texture merging utilizing this information, emphasizing the more frontal pixels for each $T^i$. We aggregate textures $T^i$ using  weighted average with weights calculated as $\vec{w} = softmax(\frac{\vec{\lambda}}{\tau})$. The $\tau$ factor controls the sharpness of edges at the junction of merged textures. The final texture is calculated as:

\begin{equation}
T = F(T^1 ... T^N, \lambda^1 ... \lambda^N) = \sum_{i=1}^{N} {T^i \cdot w^i}.
\end{equation}

This technique allows us to get few-shot avatars by merging one-shot avatars for different views. More sophisticated blending schemes such as pyramid blending~\cite{ogden1985pyramid} or Poisson blending~\cite{perez2003poisson} can be also used.

\subsection{The inpainting model}

As the final piece of our approach, we train a network that can create complete neural textures from single photographs. We do that using supervised learning, where we use the incomplete textures based on single photographs as inputs and the combined textures aggregated from multiple views as ground truth.

Since the distribution of plausible (``correct'') complete textures given the input partial texture is usually highly complex and multi-modal, we use the denoising diffusion probabilistic model (DDPM) framework~\cite{ho2020denoising} and train a denoising network instead of the direct mapping from the input to the output.

As described above, in our experiments the neural texture $T$ has a resolution of $256 \times 256 \times 21$. This leads to a huge memory requirements during the diffusion model training. In order to reduce memory consumption and improve the network convergence, we first reduce the neural texture size using VQGAN autoencoder~\cite{esser2020taming} analogously to the reduction of an RGB image size performed in \cite{rombach2022high}. As demonstrated in Fig.~\ref{fig:VAE} VQGAN is added as an alternative branch for the input of the renderer $\theta$. After the pretraining of VQGAN, the pipeline is finetuned end-to-end in order to adapt the renderer to VQGAN decompression artifacts in the neural texture.

We use several loss functions to train the VQGAN autoencoder to more accurately restore neural textures. To improve the visual quality of the avatar after texture decompression, we use the loss functions in the RGB space. We render the avatar as described in (\ref{eq:rendering}) with the restored texture $T_\textrm{res}$ and optimize the loss function (\ref{eq:losses}). Also, we use an additional L2 loss in the texture space $||T - T_\textrm{res}||_2^2$ for additional regularization and preservation of neural texture properties for all views.

After adding the VQGAN branch to the pipeline, we train the DDPM model in its latent space. Thus, the diffusion model is applied to $T_c^0 = V(T^0)$ with size $64 \times 64 \times 3$, obtained after the compression of the single-view based texture by the VQGAN encoder $E_\textrm{VQ}$. Following \cite{rombach2022high} we train DDPM using a U-Net architecture with attention. We condition the denoising model with $E_\textrm{VQ}(T^0) \oplus b(B_\textrm{fill}^0)$, where $b$ is a bilinear resize to the spatial size of $T_c^0$. Thus, as the input to the model, we feed in the concatenation of the condition and the texture $T_c$ corrupted with normally-distributed noise corresponding to the diffusion timestep $t$. The U-Net architecture thus trains to denoise the input $T_c$ by minimizing the loss (\ref{eq:dm_loss}).

As mentioned above, we train diffusion inpainting (Fig. \ref{fig:Inpaint}) using the merged textures (section \ref{sssec:merging}) as ground truth. Here, we generate textures from a dataset of 3D human scans. Multi-view dataset of people photographs with good angle coverage could be used as training data as well, and we chose to use the scan render dataset solely because of its availability to us. The training dataset is discussed in section \ref{sec:Experiments}.

The DDPM restores the whole inpainted texture $T_c$ from the noise based on the condition. We then transform $T_c$ into the restored full-size texture $T_\textrm{res}$ using the VQGAN decoder $D_\textrm{VQ}$. To retain all the details from the input view, we then merge the restored texture with the input texture $T^0$ using the $B_\textrm{fill}^0$ mask:
\begin{equation}
T = T_\textrm{res} \cdot (1 - B_\textrm{fill}^0) + T^0 \cdot B_\textrm{fill}^0
\end{equation}
The resulting texture has all the details visible in the $I_\textrm{rgb}^0$ image, while the parts of a person invisible on $I_\textrm{rgb}^0$ are restored by the DDPM in the VQGAN latent space.

%-------------------------------------------------------------------------

\section{Experiments}\label{sec:Experiments}

\subsection{Implementation details}

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth]{images/SnapshotPeople.pdf}
   \caption{\textbf{Qualitative comparison on the SnapshotPeople benchmark.} We compare our method with state-of-the-art approaches for creating avatars: PIFu, PHORHUM, ARCH, ARCH++, StylePeople. For each method, we show a front view and a back view to evaluate the quality of back reconstruction. Results for PHORHUM, ARCH and ARCH++ provided by the authors and registered with ground truth images.}
   \label{fig:Snapshot}
\end{figure*}

Now we present implementation details and hyperparameters values. Our model is trained on RGB images at the $512 \times 512$ resolution. For each $3 \times 512 \times 512$ input image, we generate a $21 \times 256 \times 256$ neural texture. In the texture, the first 16 channels are generated by the network $G$, the next three channels are RGB channels and the remaining two channels are the sampling and the inpainting masks. The neural texture generator $G$ has the StyleGAN2 architecture and takes as input a vector $\vec{v}$ generated by the encoder $E$. The encoder $E$ has the StyleGAN2 discriminator architecture with a modified head to return a vector of length 512. The resulting texture is applied to the SMPL-X model and rasterized. The rasterized image has a size of $21 \times 512 \times 512$ and fed to a neural renderer with U-Net architecture with ResNet blocks. The output of the renderer is a final image with the resolution of $3\times 512\times 512$. The generator and the renderer are trained end-to-end with the batch size of four.

For training, we use: the L2 loss, the LPIPS loss, the Dice loss and the adversaral nonsaturating loss~\cite{NIPS2014_5ca3e9b1} with R1 regularization~\cite{mescheder2018training}. We use a weighted sum of losses with the following weights. L2 loss with $\lambda_1 = 2.2$; LPIPS loss with $\lambda_2 = 1.0$; Dice loss with $\lambda_3 = 1.0$; Adversarial loss with $\lambda_4 = 0.01$. We use the lazy regularization R1 as proposed in~\cite{karras2020analyzing} every 16 iterations with the weight $\lambda_5 = 0.1$. To calculate LPIPS, we take a random $256 \times 256$ crop of the $512 \times 512$ images. We train the model for 100,000 steps using the ADAM optimizer~\cite{kingma2014adam} with $2e{\text -}3$ learning rate.

We train VQGAN to compress neural textures to $6 \times 64 \times 64$ tensors consisting of vectors of length six from a trainable dictionary with 8192 elements. We first train only the VQGAN branch for 300,000. Then we finetune the pipeline end-to-end for additional 20,000 steps to reduce the renderer artifacts when processing neural textures after VQGAN. After that, we train the diffusion probabilistic model to restore missing parts of the texture. We use the U-Net architecture with the BigGAN~\cite{brock2018large} residual blocks for up- and downsampling and with attention layers on three levels of its feature hierarchy. To additionaly prevent over-fitting, we use dropout with a probability of 0.5 in the residual blocks. We train the diffusion model for 50,000 iterations with AdamW optimizer~\cite{loshchilov2017decoupled} with a batch size of 128 and a learning rate of $1.0e{\text -}6$.

\subsection{Datasets}

To train our pipeline we used only 2D images obtained by rendering Texel~\cite{Texel} dataset. We pretrained the neural texture generator and the neural renderer using 2D images of 13,000 people in diverse poses. We have noticed that diverse poses are crucial to train realistically animatable avatars. For each image, we obtained a segmentation mask using Graphonomy~\cite{Gong2019Graphonomy} segmentation and fit the SMPL-X parametric model using SMPLify-X~\cite{SMPL-X:2019}. We also used a segmentation dice loss~\cite{sudre2017generalised} to improve the body shape of fitted SMPL-X.

To train VGQAN and the diffusion model, we used renders from Texel dataset. We acquired 3333 human scans from the Texel dataset. They are people in different clothing with various body shapes, skin tones and genders. We rendered each scan from 8 different views to get a multi-view dataset. We also augmented the renders with camera angle changes and color shifts. Thus, for each person in the dataset, we got 72 scans. Note that any images from different views are suitable for training the model, not necessarily obtained from 3D scans.

We qualitively evaluate our avatars and their animations on AzurePeople~\cite{bashirov2021real} dataset. This dataset contains diverse dressed people standing in natural poses. We also quantatively evaluate our pipeline on the SnapshotPeople \cite{alldieck2018video} public benchmark. It contains 24 videos of people rotating in A\nobreakdash-pose. We select frames with front and back views from each video to measure the accuracy of back reconstruction. For each image we get a segmentation mask and SMPL-X fit as described above.


\begin{table*}[]
\vspace{1em}
\centering
\begin{tabular}{c|cccc|ccc}
                     & \multicolumn{4}{c|}{\textbf{Same view}} & \multicolumn{3}{c}{\textbf{Novel view}} \\
Method               & MS-SSIM $\uparrow$ & PSNR $\uparrow$ & LPIPS $\downarrow$ & DISTS $\downarrow$ & KID $\downarrow$ & ReID $\downarrow$\  & DISTS $\downarrow$ \\ \hline
PIFu                 & \cellcolor{green!15}0.9793  & \cellcolor{green!15}26.2828   & \cellcolor{green!15}0.0404  & \cellcolor{yellow!15}0.0706 &     0.1839       &      0.09769           &    0.0907          \\
Phorhum              &    0.9603                   &  24.2112                      &        0.0531               &    0.0948                   & \cellcolor{yellow!15}0.1564 &      0.09149           &   \cellcolor{yellow!15}0.0144    \\ \hline
ARCH                 &    0.9223                   &  20.6499                      &        0.0732               &    0.1432                   &     0.2039       &     0.09575            &     0.0974         \\
ARCH++               &    0.9526                   &  22.5729                      &        0.0540               &    0.0842                   &     0.1750       & \cellcolor{yellow!15}0.09098   &     0.0589         \\ \hline
StylePeople          &    0.9282                   &  20.5374                      &        0.0731               &    0.1029                   &     0.1688       &     0.12788            &     0.0367         \\
\textbf{DINAR (Ours)} & \cellcolor{yellow!15}0.9687 &  \cellcolor{yellow!15}24.4182 & \cellcolor{yellow!15}0.0504 & \cellcolor{green!15}0.0703  &  \cellcolor{green!15}0.1407    &  \cellcolor{green!15}0.07855  & \cellcolor{green!15}0.0133     
\end{tabular}

\caption{\label{table:metrics} Metrics comparison on the SnapshotPeople benchmark. We compared our method not only with other parametric model based approaches (StylePeople), but also with approaches that restore geometry and require using additional methods for rigging (PIFu, Phorhum) or restore geometry in the canonical pose (ARCH, ARCH++).}
\end{table*}


\subsection{Quantitative results}

The quantitative comparison is shown in Table~\ref{table:metrics}. We compare our approach with various methods for generating an avatar from a single image, including those requiring additional rigging steps for animation. For clarity, the table is divided into three sections. PIFu~\cite{saito2019pifu} and PHORHUM~\cite{alldieck2022photorealistic} restore the 3D mesh of a person in a pose shown in the image. This imposes strong restrictions on the pose of a person in the input image if one wants to animate it. ARCH~\cite{huang2020arch} and ARCH++~\cite{he2021arch++} restore the 3D mesh in canonical space, which is easier to rig and animate. StylePeople~\cite{grigorev2021stylepeople} and DINAR (ours) are based on a parametric human model and therefore are the easiest to animate and do not suffer from rigging imperfections.

To numerically evaluate our avatars, we report several metrics, namely: Multi-scale structural similarity (MS\nobreakdash-SSIM $\uparrow$), Peak signal-to-noise ratio (PSNR $\uparrow$), Learned Perceptual Image Patch Similarity (LPIPS $\downarrow$)~\cite{zhang2018unreasonable}. We measured these reference-based metrics in the front view avatars for the SnapshotPeople public benchmark. For the front view our method works on par with non-rigged methods in terms of these metrics.

To evaluate the quality of the back view (and thus the ability to generalize to new views), we report Kernel Inception distance (KID $\downarrow$)~\cite{binkowski2018demystifying} measurements. This metric allows one to evaluate generated images quality and is more suitable for small amounts of data than FID~\cite{heusel2017gans}. Our approach results in the highest KID value compared to other methods. To assess identity preservation we measured re-identification score (ReID $\downarrow$) based on FlipReID~\cite{Ni2021FlipReIDCT} model for human re-identification. Our method shows the best results in person's identity preserving between front and back views. To additionally validate the quality of the textures and to measure structural similarity in cases with unaligned ground truth images we measured Deep Image Structure and Texture Similarity (DISTS~$\downarrow$)~\cite{ding2020image}. We provide measurements for front view and back view in the table. Our method produce the most naturally looking avatars from both views.

\subsection{Qualitative Results}

Metric value does not always correlate well with human perception. In Fig~\ref{fig:Snapshot}, we show the qualitative results of our method in comparison with other one-shot avatars methods: PIFu, PHORHUM, ARCH, ARCH++ and StylePeople. The figure shows both front and back views of the avatars. Additional comparisons are shown in Supplementary materials.

Overall, our method realistically reconstructs the texture of clothing fabrics on the back (\eg pleats on pants), which boosts the realism of the renders. Using the whole information from the given image allows us not to copy unwanted patterns from front to back (as is commonly done by the pixel-aligned methods while recovering the texture for the back). Using sampled RGB texture as an addition to a neural texture allows us to achieve photo-realistic facial details and preserve high frequency details. We note that PIFu accurately reproduces the color of the avatar and restores the geometry well. However, it does not preserve high-frequency details, which is why avatars suffer from a lack of photo-realism. PHORHUM generates highly photo-realistic avatars but often suffers from color shifts. Another methodological shortcoming of this approach is the absence of a human body prior. Therefore, the model can be over-fitted on training dataset's human poses, which may leads to incorrect work with unseen poses. Avatars generated by ARCH contain strong color artifacts and suffer from geometry restoration errors. ARCH++ significantly improves geometry and color quality for the frontal view, but the back view still suffers from color shift and artifacts. StylePeople is based on a parametric human model and can be easily animated without the use of third party methods or additional rigging. However, the coverage of the latent space of their model is limited, which leads to overfitting and poor generalization to unseen people, when performing inference based on a single view.

%-------------------------------------------------------------------------

\section{Conclusion}

We have presented a new approach for modeling human avatars based on neural textures that combine the RGB and the latent components. The RGB components are used to preserve the high frequency details, while the neural components add hair and clothing to the base SMPL-X mesh. Using the parametric SMPL-X model as a basis makes it easy to animate the resulting avatar. Our method restores missing texture parts using an adapted diffusion framework for inpainting such textures. Our method thus creates rigged avatars, while also improving the rendering quality of the unseen body parts when compared to modern non-rigged human model reconstruction methods.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

%\iffalse

\clearpage
\setcounter{section}{0}
\renewcommand{\thesection}{\Alph{section}}

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth]{images/Occlusions.pdf}
   \caption{\textbf{Occlusions detection.} We use rasterization with the body parts map to detect areas occluded by limbs. The detected areas are masked out of the final UV-render to reduce inaccuracies in RGB texture sampling.}
   \label{fig:Occlusions}
\end{figure*}

\section{RGB sampling algorithm}

In our approach, we use the combination of RGB and neural texture. To sample the RGB texture, we use the following algorithm:
\vspace{-0.5em}
\begin{algorithm}
\caption{RGB texture sampling algorithm}\label{alg:Sampling}
\begin{algorithmic}
\Require $RGB(\textrm{size} \times \textrm{size} \times 3)$
\Require $UV(\textrm{size} \times \textrm{size} \times 2)$
\State \textcolor{teal}{\# Initialize texture with zeros}
\State $T \gets zeros(\textrm{texture\_size} \times \textrm{texture\_size} \times 3)$ 
\State $C \gets zeros(\textrm{texture\_size} \times \textrm{texture\_size})$
\State
\State \textcolor{teal}{\# Fill texels with mean value of neighbors} 
\For{$ \forall x, y \in [0 .. \textrm{size}]$}
   \State $(i,j) \gets UV[x, y]$
   \For{$\forall k, m \in [-1, 0, 1]$}
       \State $T[i + k, j + m] \mathrel{+}= RGB[x, y]$
       \State $C[i + k, j + m] \mathrel{+}= 1$
   \EndFor
\EndFor
\State $T = T / C$
\State
\State \textcolor{teal}{\# Fill exact values in texels we don't need to inpaint}
\For{$ \forall x, y \in [0 .. \textrm{size}]$}
   \State $(i,j) \gets UV[x, y]$
   \State $T[i, j] \gets RGB[x, y]$
\EndFor
\end{algorithmic}
\end{algorithm}
\vspace{-0.5em}

A simple filling with an average value is needed in order to remove the gaps that appear on the texture due to the discreteness of sampling grid. The described algorithm allows us to fill them taking into account the color of neighboring texels.

\section{RGB texture refinement} \label{sec:RGBrefine}


We have found it beneficial to perform several optimization steps (namely 64) of RGB texture to enhance high frequency details (Fig. \ref{fig:Ablation} (e)) in the inference stage. To achieve this, we use gradients from the neural renderer derived by comparing the rendering result with the input image. Gradients are applied to texels with weights that correspond to the angles between the normal vectors and the camera direction (Fig. \ref{fig:Merge}). This makes sure that only texels that can be seen in the input image are optimized with prioritization of more frontal ones. We employed $L2$ and $LPIPS$ losses to encourage color matching, and $Adversarial$ loss with regularization analogous to the \ref{eq:losses} equation to amplify detalization. 

We also apply a linear adjustment to the RGB channels of the VQGAN decoding output to improve color matching between front and back views after the inpainting stage: 
\begin{equation}
T_\textrm{rgb} = T_\textrm{rgb} \alpha + \beta.
\end{equation}

In this case, all texels share the trainable parameters alpha and beta. We optimize them with renderer's gradients derived by the pixels visible in the input image. As a result, the RGB channels of the neural texture at the VQGAN output strengthen color matching with the sampled RGB texture.
This helps us to minimize the seam after combining textures (Fig. \ref{fig:Ablation} (e)). 


\section{Addressing proxy-geometry imperfections} \label{sec:Oclusion}

We have found that due to imperfect SMPL-X meshes, pixels are wrongly sampled from one body part to another at the self-occluded areas (\eg hands in front of the body). This results in implausible renderings (Fig. \ref{fig:Ablation} (b, c)). 

To address this issue, we do not sample the RGB texture along the outline of such overlapping body parts. We employ rasterization with a colormap as a texture to find overlapping regions (Fig. \ref{fig:Occlusions}). We assigned each limb in the colormap a separate color and made the transition between them smooth using a color gradient. This enables us to avoid having seams in the rasterization. We detect edges in the colormap rasterization with Canny algorithm~\cite{Canny1986ACA}. We then determine a person's contours employing binarized SMPL-X rasterization. By taking the contours out of edges, we obtain an occlusion map. We use the resulting occlusion map to mask out areas in the UV-render. This enables us to rely on inpainting in later pipeline steps rather than sampling pixels in overlapped areas. 

\section{Ablation study}

We did an ablation study of the various steps in our pipeline (Fig. \ref{fig:Ablation}). Image \ref{fig:Ablation} (b) shows posed avatar produced from the input image (Fig. \ref{fig:Ablation} (a)). It can be seen that the renderer performs some form of inpainting to fill in body regions invisible in the input image. However, such inpainting is vulnerable to numerous artifacts and lacks realism. Since such inpainting is performed in image space, it needs substantial areas that are visible in the source image to function. 

To restore invisible areas realistically, we inpaint texture space with DDPM model (Fig. \ref{fig:Ablation} (c)). This results in a notable gain in detail, decrease in artifacts, and better color matching. But we found that incorrectly sampled pixels can result in inpainting artifacts. We introduce the occlusions detector (Fig. \ref{fig:Ablation} (d)) to address this issue, which is covered in more detail in the section \ref{sec:Oclusion}. Utilizing the occlusions detector helps us to avoid sampling mistakes, which results in a realistic restoration of overlapped areas. 

Activating the RGB channel enhancement on Fig. \ref{fig:Ablation} (e), described in detail in the section \ref{sec:RGBrefine}, allows us to increase the detail of the avatar. We also reduce the contrast between the restored and sampled textures by enhancing the RGB channels of the texture. 

Thus, we conclude that all the steps used in the pipeline are necessary to achieve the best quality of avatars.

\section{Architecture details}

\textbf{Encoder network.}
As an encoder network (Fig. \ref{fig:Arch-a}), we have adapted the StyleGAN2 discriminator architecture with a few changes. Namely, three images are fed to the network input: RGB, segmentation mask, and single-channel noise. Noise is introduced to provide additional freedom to the generative model when training the GAN. The efficiency of using noise in generative neural networks has been demonstrated by the authors of StyleGAN.

The images received at the input are concatenated by channels and passed through a feature extractor with an architecture equivalent to the StyleGAN discriminator consisting of ResNet blocks. We modified the model head so that it outputs a vector $\vec{v}$ of length 512. This vector is then used as the input of the StyleGAN2 generator and the proposed encoder is trained end-to-end with the generator and the renderer.

\textbf{Renderer network.}
Here we describe the $\theta$ renderer (Fig. \ref{fig:Arch-b}). The renderer takes three images as input: a rasterized SMPL-X model with a neural texture, a UV render, and a UV mask. Each input image is passed through a convolutional network consisting of two convolutions with LeakyReLU activation and BatchNorm layers. Output features are concatenated and fed into a U-Net consisting of ResNet blocks. U-Net has 3 levels connected by feature concatenation. The U-Net output is passed through two additional convolutional networks to predict the RGB image of the avatar and its mask.


\section{Additional results}

We present additional results of our approach on diverse data. On Fig. \ref{fig:Additional} we show results for input images containing different people. The top row shows an additional example of processing of a person in loose clothing. The next row demonstrates the fidelity rendering of an avatar wearing a T-shirt with a complex high-frequency print. The bottom two rows demonstrate the accuracy of avatar reconstruction from images of people in unusual poses. Also, the frames of the animation sequence show the avatars from more varied viewpoints (\eg top and bottom). Invariance to the human pose is achieved through the use of a neural texture framework with a parametric model. All avatar processing, such as restoring the back, is done in canonical texture space. 

On Fig. \ref{fig:Alife} we demonstrate an additional use case for our one-shot approach. We used neural network inpainting \cite{Suvorov2021ResolutionrobustLM} to remove the person from the original image and replace it with an animated avatar. In this way we can create the effect of a photo that has come to life.

One of the limitations of the current approach is the handling of tissue deformations in the input image. Our method does not modify the textures depending on the pose, which can make the fabric look unrealistic when changing the pose. Another limitation is the insufficient sharpness of the edges of loose clothing. Even though dresses are rendered correctly by our method on most frames, the edges of the dress look unrealistic. In our future research, we would like to focus on addressing these two shortcomings.


\begin{figure*}
  \centering
  \includegraphics[width=0.8\linewidth]{images/Ablation.pdf}
   \caption{\textbf{Ablation study.} Avatars without DDPM inpainting are prone to artifacts and lack of realism. Disabling the occlusions detector leads to artifacts due to sampling errors at the edge of the overlapped areas. Disabling RGB finetuning results in less high-frequency detail and worse color matching. The best result is achieved by using all the steps of the pipeline.}
   \label{fig:Ablation}
\end{figure*}

\begin{figure*}
  \centering
  \begin{subfigure}{0.35\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth]{images/Fig4_disc.pdf}
     \caption{\textbf{Encoder architecture}}
     \label{fig:Arch-a}
  \end{subfigure}
  \begin{subfigure}{0.35\linewidth}
    \centering
    \includegraphics[width=0.76\linewidth]{images/Fig5_rend.pdf}
     \caption{\textbf{Renderer architecture}}
     \label{fig:Arch-b}
  \end{subfigure}
  \caption{\textbf{Encoder and renderer architecture.} The encoder architecture is a modified architecture of the StyleGAN2 discriminator. We changed the head to get a vector of length 512. The renderer has a U-Net architecture that predicts the RGB of an avatar from a rasterized model and UV render.}
  \label{fig:Arch}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth]{images/Fig6_examples.pdf}
   \caption{\textbf{More avatar animation examples.} We present more examples of avatar animations, including those obtained from more complex poses. The top two rows demonstrate how the approach works with people in loose clothes and clothes with highly detailed prints.}
   \label{fig:Additional}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth]{images/Alife.pdf}
   \caption{\textbf{Making photos come alive.} An additional use case of our approach is to replace the person in the photo with their animated avatar. By doing this, we can achieve the effect of an animated photo.}
   \label{fig:Alife}
\end{figure*}
%\fi

\end{document}
