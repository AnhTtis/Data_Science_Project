\section{Introduction}

Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative, karras2019style, karras2020analyzing, karras2021alias} have become ubiquitous in a range of high impact commercial and scientific applications \cite{beaulieu2019privacy, chen2021deepfakes,gupta2020multi, bagal2021molgpt, bian2021generative}. With this prolific use comes a growing need for investigative tools that are able to evaluate, characterize and differentiate one GAN model from another, especially since such differences can arise from a wide range of factors -- biases in training data, model architectures and hyper parameters used in training etc. In practice, this has been mostly restricted to comparing two or more GAN models against the dataset they were trained on using summary metrics such as Fr\'{e}chet Inception Distance (FID)~\cite{Heusel2017FID} and precision/recall~\cite{karras2019style} scores.


However, in many real world scenarios, different models may not even be trained on the same dataset, thereby making such summary metrics incomparable. More formally, if we define the model comparison problem as one being between a known -- and presumably well vetted -- \emph{reference} GAN and a newly developed \emph{client} GAN. For example, the reference GANs can correspond to models purchased from public market places such as AWS~\cite{aws}, Azure~\cite{azure}, or GCP~\cite{gcp}, or to community-wide standards. Furthermore, there is a critical need for more fine-grained, interpretable, investigative tools in the context of fairness and accountability. Broadly, these class of methods can be studied under the umbrella of AI model \emph{auditing}~\cite{bau2019seeing, alaa2022faithful,raji2020closing}. Here, the interpretability is used in the context to indicate that the proposed auditing result will involves of human intelligible attributes, rather than summary statistic that do not have explicit association with meaningful semantics.


While auditing classifiers has received much attention in the past \cite{raji2020closing}, GAN auditing is still a relatively new research problem with existing efforts focusing on model-data comparisons, such as identifying how faithfully a GAN recovers the original data distribution~\cite{alaa2022faithful}. In contrast, we are interested in developing a more general framework that enables a user to visually audit a ``client'' GAN model with respect the ``reference''. This framework is expected to support different kinds of auditing tasks: (i) comparing different GAN models trained on the same dataset (e.g. StyleGAN3-Rotation and StyleGAN3-Translate on FFHQ); (ii) comparing models trained on datasets with different biases (e.g., StyleGAN with race imbalance vs StyleGAN with age imbalance); and finally (iii) comparing models trained using datasets that contain challenging distribution shifts (e.g., CelebA vs Toons). Since these tools are primarily intended for human experts and auditors, interpretability is critical. Hence, it is natural to perform auditing in terms of human intelligible attributes. Though there has been encouraging progress in automatically discovering such attributes from a single GAN in the recent years \cite{yuksel2021latentclr, harkonen2020ganspace,voynov2020unsupervised,peebles2020hessian, wei2021jacobian} they are not applicable to our setting with multiple GANs. 


\myparagraph{Proposed work} We introduce cross-GAN auditing (xGA), an unsupervised approach for identifying attribute similarities and differences between client GANs and reference models (which could be pre-trained and potentially unrelated). Since the GANs are trained independently, their latent spaces are disparate and encode different attributes, and thus they are not directly comparable. Consequently, discovering attributes is only one part of the solution; we also need to `align' humanly meaningful and commonly occurring attributes across the individual latent spaces. 

Our audit identifies three distinct sets of attributes: (a)~common: attributes that exist in both client and reference models; (b)~novel: attributes encoded only in the client model; (c)~missing: attributes present only in the reference. In order to identify common attributes, xGA exploits the fact that shared attributes should induce similar changes in the resulting images across both the models. On the other hand, to discover novel/missing attributes, xGA leverages the key insight that attribute manipulations unique to one GAN can be viewed as out of distribution (OOD) to the other GAN. Using empirical studies with a variety of StyleGAN models and benchmark datasets, we demonstrate that xGA is effective in providing a fine-grained characterization of generative models.


\myparagraph{Contributions} (i) We present the first cross-GAN auditing framework that uses an unified, attribute-centric method to automatically discover common, novel, and missing attributes from two or more GANs; (ii) Using an external, robust feature space for optimization, xGA produces high-quality attributes and achieves effective alignment even across challenging distribution shifts; (iii) We introduce novel metrics to evaluate attribute-based GAN auditing approaches; and 
(iv) We evaluate xGA using StyleGANs trained on CelebA, AFHQ, FFHQ, Toons, Disney and MetFaces, and also provide a suite of controlled experiments to evaluate cross-GAN auditing methods.

