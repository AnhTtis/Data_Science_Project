\section{Experiments}
We use the CelebA \cite{liu2015faceattributes} dataset as our ``base'' dataset, and compare the performance of our method against several variants and semantically related datasets both qualitatively and quantitatively. To evaluate our method we consider the following set of data splits and train StyleGAN models on all of them: one model trained with the full dataset, 5 models trained missing one attribute (hats, glasses, males, females, and beards), and two models trained missing more than one attribute (beards/hats and smiles/glasses/ties). This setup allows us to control for missing attributes in a systematic manner, while also testing the effectiveness of our approach on practical datasets.

When available, we utilize pre-trained StyleGAN \cite{karras2019style} models, like for Met Faces and the AFHQ animal faces dataset \cite{choi2018stargan}. We also include non-human style-transfered GANs of cartoons \cite{cartoonStyleGan22}, Disney \cite{cartoonStyleGan22}, and Anime GAN \cite{danbooru2021}. Lastly, we investigate using non-StyleGAN based architectures in the supplement (StyleGAN3-t vs. StyleGAN3-r \cite{karras2021alias} and PGGAN \cite{karras2017progressive} vs. GANformer \cite{hudson2021ganformer2}). 

\subsection{Experimental Setup}
\label{sec:setup}
For all GANs we train, we use StyleGAN-ADA \cite{Karras2020ada} trained over 15 million images at a resolution of 128. We train 8 models in total, one for each of the 8 CelebA splits we used.   

For training the direction models, we randomly sample points for $10,000$ iterations. For all learnable directions, we set $\alpha=3$, how far a model must walk along a particular direction as in equation \eqref{eq:direction}. For the feature extractor, $\mathcal{F}$, we experiment with four ResNet-50 models and three ViT models: original ResNet trained on ImageNet \cite{he2016deep}, a ResNet trained to predict CelebA attributes, a ResNet trained to be robust to style using advBN\cite{shu2021encoding}, and a ResNet trained with CLIP \cite{radford2021learning}; we use the original ViT trained on ImageNet \cite{dosovitskiy2020vit}, a ViT trained with CLIP, and the recently introduced ViT Masked Autoencoder (MAE) \cite{MaskedAutoencoders2021}.  Our batch-size is set to $10$, where $2$ samples are needed for positive pairs and  $5$ directions have gradients enabled (randomly selected for each iteration). This is due to a memory constraint on a single experiment for a 15GB Tesla T4 GPU. We use an Adam \cite{kingma2014adam} optimizer with learning rate $0.001$ and default parameters. We only update the direction models, and keep all other networks fixed. As with some earlier observations, we find better performance operating in the $\mathcal{W}$ space of StyleGAN, so we restrict all our direction models to operate in it. Larger generator's outputs (512 resolution) are cropped to fit a given feature extractor, other generator's outputs are resized to fit.

We fix the total number of directions $N = 16$, with $12$ common and $4$ novel/missing directions. This is a hyper-parameter that can be adjusted depending on the application, but in general we found using a larger $N$ resulted in the model learning multiple similar directions with minor variations between (e.g. slightly changing hair color and adding eyeglasses). This is the case even when using existing approaches like LatentCLR \cite{yuksel2021latentclr}, where larger $N$ values lead to attributes that are very similar. Finally, we set the $L_{xent}$ overlap direction trade-off $\lambda_a = 0.1$. We found higher values to make directions not self-consistent and lower values made directions ignore the other GAN.

\myparagraph{DRE Training} For the $DRE$ models, we use 2-layer MLPs trained to minimize the objective specified in \eqref{eq:DRE_loss}. These models are trained for $1,000$ iterations with a default Adam optimizer. At each step, we draw $32$ samples from each GAN and project it into the feature space defined by $\mathcal{F}$. The $DRE$ models are pre-trained prior to direction model training.

\subsection{Evaluation metrics}
As the problem of interest in this work is new, there are no existing metrics that allow us to evaluate performance. Here, we introduce three new metrics for evaluating the quality of learned attributes. The first metric measures the quality of a single GAN and its corresponding feature extractor, while the other two metrics require multiple GANs. CelebA is our testbed dataset as we can leverage the multi-attribute labels it provides.
Let $A(\bm{x})$ be the attribute vector predicted from a pre-trained CelebA classifier $A$. We can then compute the attribute difference produced by using a given direction modification, $\bm{a}_n(G(z,\delta_n)) = A(G(z)) - A(G(D(z,\delta_n))$.

\myparagraph{I. Single GAN attributes.}
First, we want to explore to what extent the choice of feature space affects the attribute identification process for a single GAN, such that they can effectively be leveraged for multiple models. To study this, we use a pre-trained CelebA attribute classifier to measure the degree to which the directions are focused on a single attribute.
Our metric is an entropy-based metric computed over $B$ samples from the generator and $N$ directions:

\begin{equation}
\text{Score}_{ent} = C * \frac{1}{B} \frac{1}{N} \sum^B_{b=1} \sum^N_{n=1} \mathcal{H}(\sigma( \bm{a}_n(G(z,\delta_n)_b)))
\end{equation}

where $C$ is a constant that scales the score (we use $C$=100), $\mathcal{H}(\bm{p}) = \sum_i - p_i \text{log}(p_i)$ is the entropy function, $\sigma$ is the softmax function and $G(z,\delta_n)_b$ is the $b$th sample. 


\myparagraph{II. Common attributes}
To compute a score for the common directions, we rely on the intuition that images perturbed along the same attribute will result in similar prediction changes through an ``oracle" attribute classifier. To compute this, we first randomly sample several images from each GAN. Then, we apply a common direction (inferred by our model) $n$ to these samples and compute the average difference vectors $\bm{\bar{a}}_{(n,1)}$ for GAN 1 and $\bm{\bar{a}}_{(n,2)}$ for GAN 2, where the bar indicates that they are normalized to have unit norm. We then compute the cosine similarity between the attributes difference vectors to measure the similarity of the learned common directions from the two distributions, where a score of 1 indicates they are identical.
\begin{equation}
\text{Score}_{cos} = \frac{1}{N} \sum_n^N  \cos(\bm{\bar{a}}_{(n,1)}, \bm{\bar{a}}_{(n,2)}) 
\end{equation}

\myparagraph{III. Novel/Missing attributes}
To measure attributes that are exclusive to a particular GAN, we train different leave-k-attributes-out GANs, on artificial splits of the CelebA dataset as described earlier.  Here, we have a ground truth set of attributes which are missing $\bm{\mathcal{M}}$. In order to measure how well the model finds the missing attributes in $\bm{\mathcal{M}}$, we derive a metric from mean reciprocal rank (MRR) \cite{radev2002evaluating,voorhees1999trec}. Let $\bm{a} = (\bm{a}_1, \ldots, \bm{a}_N)$ be a collection of all difference vectors as defined earlier. Let $rank(m,\bm{a}_n)$ be the rank of missing attribute $m$ in difference vector $\bm{a}_n$ and let $\bm{\mathcal{I}}_u$ be the indices of unique directions. The unique direction score is computed as shown in Eqn \ref{eq:score_unique}, where higher is better.


\begin{equation}
\text{Score}_{unique}(\bm{a}) = \frac{1}{|\bm{\mathcal{M}}|} \sum_{m \in \bm{\mathcal{M}}} \underset{n \in \bm{\mathcal{I}}_u} {\max} \left( \frac{1}{rank(m,\bm{a}_n)} \right)
\label{eq:score_unique}
\end{equation}

\subsection{Results}
Here we present the quantitative and qualitative results that illustrate the performance of our proposed xGA method.  

\myparagraph{Single GAN attribute discovery}
Even though our focus is not on the single GAN attribute discovery tasks, our experiments show that the utilization of an external latent space can improve the overall task on several important metrics.
Table \ref{tbl:entropy_results} depicts the impact of different types of pre-trained features on the disentanglement properties in attribute discovery over all eight of our CelebA test-bed GANs. The original LatentCLR does poorly as it heavily focuses on non-dataset attributes (e.g. rotation/zoom/etc). As expected, the CelebA attribute classifier (which is an oracle) is the best at finding low entropy directions as it is the only extractor model trained with additional information (i.e. labels) about the datasets of interest. Overall, most features perform adequately at the task especially compared to operating in the latent space of the GAN itself.
We also show qualitatively the power of using a feature extractor in figure \ref{fig:1gan_examples}, which shows the top four most changed attributes (according to the classifier) for a few methods: xGA finds more diverse dirdctions. In the supplement, we include a detailed example for all methods starting with a randomly selected initial point. 

\input{tables/1gan_entropy}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.66\linewidth]{figures/1gan_example.pdf}
    \caption{An example of single GAN method's most changed attribute direction using random seed 0. Using \ours, more diverse and novel attributes are found. Complete examples for all methods are provided in the supplement.}
    \label{fig:1gan_examples}
\end{figure}

\myparagraph{Cross-GAN attribute auditing with ground truth}
In order to verify the effectiveness of the proposed method, it is crucial to provide evaluation against a straightforward baseline method on a well-understood task, where we know the ground truth.
%%%%%
An intuitive idea for multi-GAN attribute alignment and comparison is to use a single GAN attribute discovery method and then align the discovered attributes after applying them on separate GANs. 
Here, we run 4 recent single GAN attribute discovery methods (Voynov \cite{voynov2020unsupervised}, latentCLR \cite{yuksel2021latentclr}, Jacobian \cite{wei2021jacobian}, and Hessian \cite{peebles2020hessian}) on each of our celebA GANs (i.e., controlled setup with ground truth discussed in Section \ref{sec:setup}, in which we know what attributes are excluded in each model), then align the models after the fact. 
Once each of the attribute discovery models is trained, we align each direction for all pairwise combinations. We use our pretrained face attribute classifier to greedily identify the directions that maximize cosine similarity between the predicted attributes $a$ (e.g., the pair of directions between two GANs that achieve the highest cosine similarity are selected as the first aligned attribute). We then select the top 12 most similar directions for each pair of GANs, along with the top 4 most unique directions, and compute our previously described metrics. This setup is designed to be the most generous as possible for the single GAN methods. We show in table  \ref{tbl:celeba_metrics}, however,  the single GAN methods still under-perform our \ours{} method. 

\input{tables/CelebA_scores}

In Table \ref{tbl:celeba_metrics}, we also show the results of the average common and unique scores for all pairwise combinations (28 in total) of our CelebA GANs, with full experimental provided in the supplement. The robust variant of ResNet finds the most interesting attributes consistently. Surprisingly, variants of the ImageNet-pretrained ResNet models outperform the attribute classifier, particularly on finding common directions indicating that these models are better suited for this optimization likely because of having a more expressive feature space. Overall, we find that the robust ResNet variant performs the best across all our metrics. Additional experiments (on ViT-based feature extractors) are included in the supplement.

\begin{figure}[!tb]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/leave_n_out_CelebA.pdf}
    \caption{The top missing attribute according to the pretrained classifier from full CelebA GAN against three different GANs trained on various attribute splits. }
    \label{fig:leave_out_celeba}
\end{figure}

Alignment between CelebA GANs is relatively easy as the two models already share much of the same data distribution. We show a modestly challenging version in Figure \ref{fig:leave_out_celeba} (a) with the alignment between two CelebA models. Lastly, we show the unique attributes when comparing a CelebA GAN versus missing attribute(s) CelebA GANs. 
Figure \ref{fig:leave_out_celeba} (b) shows examples from a few different setups, where missing attributes were accurately identified.
We do note that many of the common attributes show  attribute entanglement between multiple attributes. However, our goal for GAN auditing is not to discover disentangled attributes, but to characterize the GAN.



\myparagraph{Unique attribute ablation study} 
Here we investigate the effect of the KLIEP loss on unique attribute discovery. In addition to KLIEP loss presented above, we analyze a model trained with simple log loss. 
Full details for the log-loss model are provided in the supplement, but essentially we use the same setup as the KLIEP and DRE loss, but minimize/maximize log probabilities (rather than ratios) to accurately distinguish between $G_1$ and $G_2$.  
Table \ref{tbl:dre_lambda_abl} illustrates the unique attribute discovery score for each CelebA split versus full CelebA. With $\lambda=0$ (i.e. ignoring the DRE loss), the unique direction discovery process has difficulty capturing some missing attributes, like gender or hats (see supplement for detailed results). When using a regularization model trained with Log-loss, the results are consistently worse than DRE, sometimes even worse than with $\lambda=0$. The KLIEP loss model, on the other hand, performs consistently better for all lambda values $>0$.

\input{tables/dre_lambda_ablation}


\myparagraph{Cross-GAN attribute auditing in the wild}
Figure \ref{fig:cats_vs_dogs} shows alignments between non-human faces of cats and dogs.


\begin{figure}[!tb]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/cats_vs_dogs.pdf}
    \caption{
    \textbf{(Top): }  The common attributes from AFHQ Cats (client) and Dogs (reference) \textbf{(Bottom):} The novel/missing attributes from these non-human GANs. 
    }
    \label{fig:cats_vs_dogs}
\end{figure}

Figure \ref{fig:metface} shows interesting examples of aligned attributes between a GAN trained on Metface and another trained on CelebA. We demonstrate the general applicability of these identified attributes by showing multiple directions from a variety of starting points. We see an expected attribute of formal wear between both GANs, but we also highlight the fact that our CelebA GAN, despite being trained on human faces, appears to discover an unexpected attribute direction corresponding to sketch-like images as well as a direction for transforming an adult face into a child.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/metface.pdf}
    \caption{ Examples of common attributes discovered between Metface (Left) and CelebA (Right).} 
    \label{fig:metface}
\end{figure}

\begin{figure}[!tb]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/3gan.pdf }
    %\vspace{-5mm}
    \caption{Common attributes from training three attribute models over three different GANs (Eqn \ref{eq:xent_1}). 
    } 
%    \vspace{-2.5mm}
     \label{fig:3gan}
\end{figure}

In theory, our proposed method is not limited to working on only two generative models and can be scaled beyond two models, with the only constraint being the memory on the GPU since we need to load all the generators into memory to perform optimization. Here, we run an experiment with 3 different CelebA StyleGANs to verify similar directions can appropriately be found. Figure \ref{fig:3gan} shows the results of aligning 3 models at the same time.

Finally, a qualitative investigation into the task of applying \ours{} to different GAN architectures can be found in the supplement (StyleGAN3-t vs. StyleGAN3-r and PGGAN vs. GANformer).


\myparagraph{Limitations} The proposed method has the following limitations.
First, similar to other optimization-based attribute discovery approaches \cite{voynov2020unsupervised}, \cite{yuksel2021latentclr}, there is no guarantee that all prevalent factors are captured, but we do show that all the distinct attributes are identified in our experiments when ground truth is available. Second, compared to existing methods for a single GAN, additional choices with respect to the feature space need to be made for attribute discovery. 
As shown by our result with the robust pre-trained ResNet producing the best result, the choice of feature space for attribute recognition not only has a large impact on the overall performance of the model but also determines prior knowledge brought into the attribute discovery process.  