\begin{table}[!tb]
\centering
\begin{tabular}{lll}
Method / Model & Cosine avg ($\uparrow$) & Unique avg ($\uparrow$)\\ \hline
SeFa        & $0.382	\pm{0.042}$ & $0.167	\pm{0.165}$ \\
Voynov      & $0.544	\pm{0.033}$ & $0.254	\pm{0.246}$ \\
LatentCLR   & $0.543	\pm{0.031}$ & $0.297	\pm{0.326}$ \\
Hessian     & $0.567	\pm{0.065}$ & $0.224	\pm{0.273}$ \\
Jacobian    & $0.502	\pm{0.024}$ & $0.233	\pm{0.201}$ \\ \midrule
\ours   & $0.569 \pm{0.143}$          & $0.350 \pm{0.216}$          \\
\ours +  Clip      & $\bm{0.700} \pm{0.137}$     & $0.343 \pm{0.200}$          \\
\ours + Robust   & $0.660 \pm{0.147}$          & $\bm{0.411} \pm{0.193}$ \\
\ours + Attr. Cls.      & $0.338 \pm{0.208}$          & $0.387 \pm{0.199}$          \\ \hline
\end{tabular}
\caption{ 
The average Cosine scores for the 7 CelebA pairwise experiments and average Unique scores for the 7 CelebA pairwise leave-attribute-out experiments ($\pm{\text{ std}}$) .

%We find ResNets trained on more generic tasks like ImageNet classification even outperform the oracle CelebA attribute classifier on these tasks, likely because of having a more expressive feature space.
% Surprisingly the feature space of the attribute classifier does not perform as well, we believe this is due to the feature space not being rich enough to help align the two seperate GANs. \matt{I have no speculation as to why ViTs are worse. I did a careful check of the code to make sure there were no bugs. Since their cosine scores are so close to Attr. Cls. I think they are just worse at the alignment task} 
%We found the robust variant of ResNet to find the most interesting attributes consistently. Full experiment results are provided in the supplement.
} 
\label{tbl:celeba_metrics}
\end{table}

