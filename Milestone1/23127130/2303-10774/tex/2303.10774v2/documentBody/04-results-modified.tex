\section{Experiments}
In order to systematically evaluate the efficacy of our proposed GAN audit approach, we consider a suite of GAN models trained using several benchmark datasets. In this section, we present both qualitative and quantitative assessments of xGA, and additional results are included in the Supplementary Material.

\subsection{Datasets and GAN Models}
\label{subsec:data_gans}
For most experiments, we used a StyleGANv2~\cite{karras2019style} trained on the CelebA \cite{liu2015faceattributes} dataset as our reference GAN model. This choice is motivated both by its wide-spread use as well as the availability of fine-grained, ground truth attributes for each of the face images in CelebA, and to ensure that this model is fully independent from other client GANs (e.g., ToonGAN is finetuned from FFHQ GAN). In one experiment for the AFHQ dataset, we used a StyleGANv2 trained using only \textit{cat} images from AFHQ as the reference. Also, we considered FFHQ-trained StyleGANv3~\cite{karras2017progressive} and non-StyleGAN architectures such as GANformer~\cite{hudson2021ganformer2} for defining the reference (see supplement).

In our empirical study, we constructed a variety of (StyleGANv2) client models and performed xGA: (i) $5$ trained with different CelebA subsets constructed by excluding images specific to a chosen attribute (hat, glasses, male, female and beard); (ii) $2$ trained with CelebA subsets constructed by excluding images containing any of a chosen set of attributes (beards$\mid$hats, smiles$\mid$glasses$\mid$ties); (iv) $3$ transferred GANs for Met Faces, cartoons~\cite{cartoonStyleGan22}, and Disney images \cite{cartoonStyleGan22} respectively.

\subsection{Training Settings}
In all our experiments, xGA training is carried out for $10,000$ iterations with random samples drawn from $\mathcal{Z}_c$ and $\mathcal{Z}_r$. We fixed the desired number of attributes to be $N_c = 12$, $N_n = 4$ and $N_m = 4$. Note, this choice was to enable training xGA on a single 15GB Tesla T4 GPU. With the StyleGAN2 models, our optimization takes $4$ hours; StyleGAN3 takes $12$ hours due to gradient check-pointing. For all latent directions $\{\delta_n\}$ and $\{\bar{\delta}_n\}$, we set $\alpha=3$ and this controls how far we manipulate each sample in a given direction. In each iteration, the effective batch size was $10$, wherein $2$ samples were used to construct a positive pair and a subset of $5$ directions were randomly chosen for updating (enforced due to memory constraints).  We used the Adam \cite{kingma2014adam} optimizer with learning rate $0.001$ to update the latent direction parameters. Note, all other model parameters (generators, feature extractor, DRE models) were fixed and never updated. Following common practice with StyleGANs, the attributes are modeled in the style space and the generator's outputs are appropriately resized to fit the size requirements of the chosen feature extractor.

For our optimization objective, we set the hyper-parameter $\lambda_a = 0.1$ in $\mathcal{L}_{\text{xGA}}$. To perform $\text{DRE}$ training, we used 2-layer MLPs trained via the Adam optimizer for $1000$ iterations to minimize the KLIEP losses specified in \eqref{eq:dre_train} and \eqref{eq:dre_train2}. At each step, we constructed batches of $32$ samples from both reference and client GANs, and projected them into the feature space of $\mathcal{F}$. Lastly, we set $\lambda_b = 1.0$; we explore tuning this parameter in the supplement, finding it to be relatively insensitive.



\input{documentBody/04-results-common}

\input{documentBody/04-results-novel}

\input{documentBody/04-results-analysis}

