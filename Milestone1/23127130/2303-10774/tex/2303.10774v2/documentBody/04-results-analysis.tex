\subsection{Analysis}
\label{sec:subsec_ablation}
In this section, we examine the key components of \ours{} to understand its behavior better. 

\myparagraph{Impact of the Choice of $\mathcal{F}$ } We start by studying the choice of the external, feature space used to perform attribute discovery. For this analysis, we consider the case where we assume $\mathcal{G}_r = \mathcal{G}_c$, wherein xGA simplifies to the standard setting of attribute discovery with a single GAN model (set $\lambda_b = 0$), such as SeFA and latentCLR. We make an interesting observation that, using a robust latent space  leads to improved diversity and disentanglement in the inferred attributes, when compared to the native latent space of StyleGAN. To quantify this behavior we consider two evaluation metrics based on the predictions for a batch of synthesized images $\mathcal{G}_c(\mathrm{z}, \delta_n)$ from the ``oracle'' attribute classifier. First, for each latent direction $\delta_n$, the average prediction entropy $\mathcal{H}_{\text{score}}$~\cite{liu2015faceattributes} is defined as:

\begin{equation}
\label{eq:h_score}
\mathcal{H}_{\text{score}} =  \mathbb{E}_n \bigg[\mathbb{E}_i \bigg[\texttt{Entropy}(\mathcal{C}(\mathcal{G}_c(\mathrm{z}_i, \delta_n)))\bigg]\bigg] 
\end{equation}

Second, the deviation in the predictions across all latent directions $\mathcal{D}_{\text{score}}$ is defined in \eqref{eq:d_score}, where $K$ is the total number of attributes in the ``oracle'' classifier $\mathcal{C}$:

\begin{equation}
\label{eq:d_score}
\mathcal{D}_{\text{score}} =  \sum_{k=1}^K \texttt{Variance}\bigg[ \bigg\{\mathbb{E}_i[\mathcal{C}(\mathcal{G}_c(\mathrm{z}_i, \delta_n)]\bigg\}_{n=1}^N \bigg]_k
\end{equation}

When the entropy is low, it indicates that the semantic manipulation is concentrated to a specific attribute, and hence disentangled. On the other hand, when the deviation is high, it is reflective of the high diversity in the inferred latent directions.
\input{tables/1gan_entropy}


For this analysis, we considered the following feature extractors for implementing xGA: (i) vanilla ResNet-50 trained on ImageNet \cite{he2016deep}; (ii) robust variant of ResNet-50 trained with advBN\cite{shu2021encoding}; (iii) ResNet-50 trained via CLIP \cite{radford2021learning}. Table \ref{tbl:entropy_results} shows the performance of the three feature extractors on attribute discovery with our $7$ CelebA GANs trained using different data subsets. Note, we scale all entropy and diversity scores by $100$ for ease of readability. We make a striking finding that, in terms both the entropy and deviation scores, performing attribute discovery in an external feature space is significantly superior to carrying out the optimization in the native style space (all baselines). As expected, LatentCLR produces the most disentangled attributes among the baselines, and regardless of the choice of $\mathcal{F}$, xGA leads to significant improvements. More importantly, the key benefit of xGA becomes more apparent from the improvements in the deviation score over the baselines. In the supplement, we include examples for the attributes inferred using all the methods. Finally, among the different choices for $\mathcal{F}$, the advBN ResNet-50 performs the best in terms of both metrics and hence it was used in all our experiments.

\begin{figure}[!tb]
     \centering
     \includegraphics[width=0.99\linewidth]{figures/1gan_example.pdf}
     \caption{Comparing xGA on single GAN attribute discovery with existing approaches, we find that more diverse and novel attributes can be found simply by using an external feature space. We exploit this for effective alignment across two GAN models. 
     }
     \label{fig:1gan_examples}
 \end{figure}
 
\myparagraph{Single GAN Qualitative Results}
Figure \ref{fig:1gan_examples} visualizes a shortened example of the top $3$ attributes (induce most changes in the ``oracle'' classifier predictions). This example show a clear improvement by using a pretrained feature extractor, as xGA identifies the most diverse semantic changes. Complete results, all discovered attributes for all methods, are shown in the supplement.


\myparagraph{Extending xGA to compare multiple GANs } Though all our experiments used a client model w.r.t a reference, our method can be readily extended to perform comparative analysis of multiple GANs, with the only constraint arising from GPU memory since all generators need to be loaded into memory for optimization. We performed a proof-of-concept experiment by discovering common attributes across $3$ different independently trained StyleGANs as shown in Figure \ref{fig:3gan}. For this setup, we expanded the cost function outlined in \eqref{eq:xent_1} to include $3$ pairwise alignment terms from the $3$ GANs to perform contrastive training, in addition to an extra independent term from the third model. While beyond scope for the current work, scaling xGA is an important direction for future work. 

\begin{figure}[!tb]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/3gan.pdf}
    \caption{Common attributes identified using xGA with three different StyleGANs.} 
    \vspace{-2mm}
     \label{fig:3gan}
\end{figure}
