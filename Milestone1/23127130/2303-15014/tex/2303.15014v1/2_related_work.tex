\section{Related Work}

\subsection{Unsupervised Semantic Segmentation}
Semantic segmentation has been extensively studied for its wide applicability~\cite{deeplab, danet, setr, segformer, ocr, segmenter}, but collecting pixel-level annotations requires expensive costs.
% Semantic segmentation~\cite{deeplab, danet, setr, segformer, ocr, segmenter} achieved remarkable performance in various tasks, but existing supervised approaches require expensive costs for pixel-level annotations.
Therefore, many studies~\cite{iic, picie, transfgu, stego, segsort, maskcontrast} attempted to address semantic segmentation without any supervision.
% To address the problem that producing annotated data consumes huge costs, many recent studies attempt to solve the semantic segmentation task without labels, which is called unsupervised semantic segmentation.
Earlier techniques tried to learn semantic correspondence at the pixel level.
IIC~\cite{iic} maximizes the mutual information between the features of two differently augmented images, and PiCIE~\cite{picie} learns photometric and geometric invariances as an inductive bias.
Yet, their training process highly depends on data augmentation, and learning semantic consistency without any prior knowledge is challenging.
Therefore, recent methods~\cite{transfgu, stego} adopted the ViT model trained in a self-supervised manner, i.e., DINO~\cite{dino}, as a backbone architecture.
For instance, TransFGU~\cite{transfgu} relocates the high-level semantic features from DINO into low-level pixel-wise features by generating pixel-wise pseudo labels.
On the other hand, STEGO~\cite{stego} utilizes knowledge distillation that learns correspondences between features extracted from DINO.
Although STEGO shows a dramatic performance improvement compared to the prior works, it heavily relies on the pretrained backbone and overlooks the property of local consistency that the adjacent pixels are likely to belong to the same category.
On the other hand, our training is driven by both the task-agnostic and task-specific pseudo-positive features, and the gradients are conditionally propagated to the neighboring patches, thereby ensuring task-specificity and locality.


% Therefore, we construct a pseudo-positive-based contrastive learning strategy with the help of task-specific features together with the task-agnostic features from the pretrained backbone.
% Also, to retain the local consistency between adjacent pixels, we propagate the loss gradient of each anchor to surrounding patches based on similarity scores.



% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.48\textwidth]{fig/motivation_v5.pdf}
%     \caption{
%     Assuming the mini-batch comprising two images shown in (a), we describe two types of hidden positives, leveraged for contrastive learning.
%     \textbf{(b)} ($\uparrow$)~Analogous patches throughout the mini-batch are selected as global hidden positives. 
%     ($\downarrow$)~Data-driven criterion is designed for the reliable positive collection. With the criterion, selected positives are illustrated in (b $\uparrow$).
%     \textbf{(c)} ($\uparrow$)~Nearby patches have a high chance to be semantically consistent. 
%     We define local hidden positives for each anchor to be the adjacent patches with the same semantics, i.e., blue boxes.
%     ($\downarrow$)~Average attention scores for adjacent patches from the pretrained transformer architecture. The blue line represents the attention score for local positives and the red line is for patches in different classes.
%     With two hidden positives, we provide an example in (a) how our training scheme provides more precise and consistent semantics.
%     % (b) (top)~patches possessing analogous semantics within the mini-batch are selected as global hidden positives. (down)~An appropriate data-driven criterion makes the global hidden positive set include reliable patches with analogous semantics.
%     % (c) (upper)~For each anchor patch, adjacent patches having the same semantics, i.e., blue boxes should be included in the local hidden positive set.
%     % (below)~Average attention score for adjacent patches. Blue line is for adjacent features with the same class as the anchor, and red line is for difference classes.
%     }
%     \label{fig_motivation}
% \end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{fig/main_figure_v8.pdf}
    \caption{
    Illustration of the global hidden positive~(GHP) selection process. 
    % Our global selection can be divided into two sub-sets: task-agnostic and task-specific.
    Our GHP can be divided into two sub-sets: task-agnostic and task-specific.
    An index set of task-agnostic GHP ${P}_{i}^{\text{ag}}$ comprises the indices of positives discovered within the task-agnostic reference pool $Q^{\text{ag}}$.
    Note that, $Q^{\text{ag}}$ is composed of randomly sampled features extracted by the feature extractor $\mathcal{F}$.
    % Once the anchor feature $f_i$ is projected to $z_i$, other patches in the mini-batch are gathered as positives if their similarity exceeds the similarity between the anchor and the most similar feature in $Q^{\text{ag}}$.
    % \textcolor{blue}{
    Once the anchor feature $f_i$ is projected to $z_i$, other patches in the mini-batch are gathered as positives if their similarity with the anchor feature exceeds the similarity between the anchor and the most similar feature in $Q^{\text{ag}}$.
    % }
    % On the other hand, task-specific positives are discovered in a similar manner but with task-specific reference pool $Q^{\text{sp}}$.
    % Task-specific reference pool $Q^{\text{sp}}$ is iteratively updated with the features from the momentum segmentation head.
    % \textcolor{blue}{
    On the other hand, task-specific GHP is discovered in a similar manner but with task-specific reference pool $Q^{\text{sp}}$ which keeps being updated with the features from the momentum segmentation head $S'$.
    % }
    Whereas the task-agnostic GHP set solely contributes to the initial training, the task-specific GHP set gradually replaces the portion of the task-agnostic set until the end of training.
    }
    \label{fig_main}
\end{figure*}

\subsection{Contrastive Learning}
Self-supervised learning methods~\cite{rotnet, deepcluster, simclr, moco, swav, byol, simsiam} aim to learn general representations without any annotations. 
Thanks to their good representation capability, they have been employed to yield remarkable performances in various downstream tasks~\cite{picie, hendrycks2019using, supcon, lorot, supseg}.
Among them, contrastive learning approaches~\cite{simclr, moco, swav, byol, pirl, simsiam} have shown unrivaled performances.
In general, they learn representation by attracting a self-augmented set and repulsing other images~\cite{simclr, swav}.
Other variants use additional memory~\cite{moco} or only exploit the positive set for the attraction~\cite{byol, simsiam}.
This training scheme has also been utilized for unsupervised semantic segmentation~\cite{iic, picie}.
However, the aforementioned methods only considered augmented pairs for the positive which makes them very sensitive to the quality of the augmentation techniques.
Our work differs in that our key idea is to collect and exploit reliable pseudo-positives throughout the mini-batch as described in Fig.~\ref{fig_motivation}~(b).




% The contrastive learning is also utilized in unsupervised semantic segmentation~\cite{iic, picie} by applying the contrastive objective at the pixel level.
% However, they only consider a positive pair based on the data augmentation, thereby being very sensitive to the quality of augmentation techniques.
% On the contrary, we focus on discovering pseudo-positives throughout the mini-batch, with the aid of appropriate criteria.
% As described in Fig.~\ref{fig_motivation}~(b), our positive selection process can excavate reliable positives.



