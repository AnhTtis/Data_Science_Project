\section{Ablation Study and Further Analysis}
In this section, we provide ablation studies and an analysis of our model.
Particularly, we explore the contributions of main components and test with varying hyperparameters.
Most experiments for ablation studies are conducted on the COCO-stuff dataset using the DINO pretrained ViT-S/8 model, except for Sec.~\ref{sec.lossprop}, which utilized the ViT-S/16.
% We provide ablation studies to explore the contribution of each component. The experiments are conducted on COCO-stuff dataset with DINO pretrained ViT-S/8 model.


% \noindent
\subsection{Importance of the Main Components.}
Tab.~\ref{Tab.ablation} reports the performances when an individual component or various combinations of them are not utilized. 
We found that task-specific GHP and LHP are essential in improving the performance of the unsupervised segmentation task.
Compared to (d) where both the task-specific GHP and LHP are not used, the use of them each leads to 6.5\%~(b) and 11.6\%~(c) improvements.
Also when used together, they bring 16\% of performance boosts~(a).
Furthermore, the importance of preserving symmetricity in selecting GHP can be found by comparing (a) to (e) as its usage boosts 5.9\%,
and consistency regularizer enhances performance by 2.5\% by comparing (a) to (f).
Lastly, (g) shows the performance of naively implemented contrastive learning~(Eq.~\ref{eq_contrastive_loss_self}) with photometric perturbations used in PiCIE~\cite{picie}.
This also confirms the strengths of our proposed method as (a) enhances (g) by 51.3\%.
% In specific, we induce the augmentation invariance with photometric perturbations utilized in PiCIE~\cite{picie}.


% Specifically, we mainly explore the effect of task-specific GHP and LHP which are novel components for task-specific training and local semantic consistency, individually.
% We can observe that both GHP-TS and LHP contribute significant performance improvements, up to 6.5\% and 11.6\%, respectively, compared to when neither is used. 
% These results highlight the importance of learning task-specificity and maintaining local consistency.
% Furthermore, RA, the reassign scheme to enrich the GHP by imposing the symmetricalness of GHP as in Eq.~\ref{eq_positive_pre_index_rewritten}, shows large accuracy gain up to 5.9\%. This result demonstrates that RA helps to configure a more reliable GHP.
% For the last row in Tab.~\ref{Tab.ablation}, we trained the model using the self-supervised contrastive loss~\ref{eq_contrastive_loss_self} to demonstrate how well our method preserves semantic consistency compared to a model learning augmentation invariance. The large margin between the two methods verifies the necessity to discover hidden positives.

% 1. 굵직굵직한 메소드 (GHP(global hidden positive) vs LHP(local hidden positive)) 비교
% (GHP 내에서도 task agnostic 이랑 task specific set 비교)
% LHP있고없고 비교: 1번째줄vs2번째줄 -> cluster accuracy 차이 많이 남
% Task specific 있고없고 비교: 1번째줄 vs 3번째줄 -> 성능 차이가 난다
% 그냥 multiple positive를 이용한 contrastive learning 과 우리 방법 비교: 1번째줄 vs 4번째줄

% 2. Reassignment(positive set symmetric하게 만드는거), Regularization(augmentation이랑 같게만드는거) 하나씩 뺏을때 성능
% -> regularizer 는 성능 향상은 보이지만 크게 상관 없는정도이다
% -> symmetric하게 만드는건 큰 성능 향상을 보인다. 이는 학습에 consistency를 잘 부여한다는걸 나타낸다.


% \begingroup
% \setlength{\tabcolsep}{6pt} % Default value: 6pt
% \renewcommand{\arraystretch}{1.0} % Default value: 1
% \begin{table}[!t]
%     \centering
%     \footnotesize
%     \begin{tabular}{c|c | c |c |c| c |c c}
%         \hlineB{2.5}
%         \multirow{2}{*}{} & \multicolumn{2}{c|}{GHP} & \multirow{2}{*}{LHP} & \multirow{2}{*}{RA} & \multirow{2}{*}{Reg.} & \multicolumn{2}{c}{Unsupervised} \\
%         \cline{2-3}
%         &TA & TS & & & & Acc. & mIoU \\
%         \hline
%         \hline
%         (a)& \checkmark & \checkmark & \checkmark & \checkmark & \checkmark &  57.2 & 24.6 \\
%         (b)&\checkmark & \checkmark & & \checkmark & \checkmark & 52.5 & 23.1 \\ % ex830
%         (c)&\checkmark & & \checkmark & \checkmark & \checkmark & 55.0 & 19.1 \\ % ex853
%         (d)&\checkmark & & & \checkmark & \checkmark & 49.3 & 20.1 \\ % ex852
%         % \hline
%         (e)&\checkmark & \checkmark & \checkmark & & \checkmark & 54.0 & 23.6 \\ % ex834
%         % (f)&\checkmark & \checkmark & \checkmark & \checkmark & & 55.8 & 24.5 \\ % ex833
%         \hline
%         (g)&& & & & & 31? & 9? \\ % ex916
%         % \checkmark &  &  & &  & 52.1 & 20.9 \\ % ex835
%         % \hline
%         % \checkmark & & & & & \\ 
%         \hlineB{2.5}
%     \end{tabular}
%     % \vspace{-0.1cm}
%     \caption{Ablation study for each component. TA:Task-Agnostic, TS: Task-Specific, RA: Reassign scheme to enrich GHP~(Eq.~\ref{eq_positive_pre_index_rewritten}), Reg: Consistency regularizer.
%     The last row is a result for the model trained by self-supervised loss~(Eq~\ref{eq_contrastive_loss_self}).
%     }
%     \label{Tab.ablation}
%     % \vspace{-0.4cm}
% \end{table}
% \endgroup
\begingroup
\setlength{\tabcolsep}{6pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.0} % Default value: 1
\begin{table}[!t]
    \centering
    \small
    \begin{tabular}{c|c | c |c |c| c|c c}
        \hlineB{2.5}
        \multirow{2}{*}{} & \multicolumn{2}{c|}{GHP} & \multirow{2}{*}{LHP} & \multirow{2}{*}{SA} & \multirow{2}{*}{Reg} & \multicolumn{2}{c}{Unsupervised} \\
        \cline{2-3}
        &TA & TS & & & & Acc. & mIoU \\
        \hlineB{2.5}
        (a)& \checkmark & \checkmark & \checkmark & \checkmark &  \checkmark &  57.2 & 24.6 \\
        (b)&\checkmark & \checkmark & & \checkmark & \checkmark & 52.5 & 23.1 \\ % ex830
        (c)&\checkmark & & \checkmark & \checkmark & \checkmark &  55.0 & 19.1 \\ % ex853
        (d)&\checkmark & & & \checkmark & \checkmark & 49.3 & 20.1 \\ % ex852
        % \hline
        (e)&\checkmark & \checkmark & \checkmark & & \checkmark & 54.0 & 23.6 \\ % ex834
        % (f)&\checkmark & \checkmark & \checkmark & \checkmark & & 55.8 & 24.5 \\ % ex833
        (f) &\checkmark & \checkmark & \checkmark & \checkmark & & 55.8 & 24.5 \\
        \hline
        (g)&& & & & & 37.8 & 10.4 \\ % ex916
        % \checkmark &  &  & &  & 52.1 & 20.9 \\ % ex835
        % \hline
        % \checkmark & & & & & \\ 
        \hlineB{2.5}
    \end{tabular}
    % \vspace{-0.1cm}
    \caption{Ablation study for each component. GHP, LHP, TA, TS, SA, and Reg denote Global Hidden Positive, Local Hidden Positive, task-agnostic, task-specific, symmetrical assignments, and consistency regularizer, respectively.
    }
    \label{Tab.ablation}
    % \vspace{-0.4cm}
\end{table}
\endgroup




\subsection{Alternatives to Gradient Propagation Strategy}
\label{sec.lossprop}
% \noindent
% \paragraph{Gradient Propagation vs Loss Propagation.}
There can be alternative ways to meet our goal of reflecting semantic consistency between adjacent patches.
As an alternative method of gradient propagation, we simply apply the identical loss to the surrounding patches proportionally to their attention score (i.e., loss propagation).
The results in Tab.~\ref{table_lossprop} show that the loss propagation strategy performs comparably to the gradient propagation strategy. Nonetheless, we observed that the loss propagation approach incurs higher computational costs~($1.2\times$ memory and $3\times$ time). Likewise, our method is implemented considering both effectiveness and efficiency.


\begin{table}[t]
% \renewcommand{\arraystretch}{0.93} % Default value: 1
\renewcommand{\arraystretch}{1.0} % Default value: 1
	% \vspace{-0.3cm}
	\centering
	{\footnotesize
        \begin{tabular}{c c c c c}
        \hlineB{2.5}
        \multirow{2}{*}{Method} & \multicolumn{2}{c}{Unsupervised} & \multicolumn{2}{c}{Linear} \\
        \multirow{2}{*}{} & Acc. & mIoU & Acc. & mIoU \\
        \hline
        DINO~+~GradProp & 54.5 & 24.3 & 74.1 & 39.1 \\
        DINO~+~LossProp & 54.7 & 23.2 & 74.3 & 40.5\\
        \hline
        SelfPatch~+~GradProp & 56.1 & 23.2 & 74.9 & 41.3 \\
        SelfPatch~+~LossProp & 54.5 & 22.2 & 75.1 & 41.4 \\
        \hlineB{2.5}
        \end{tabular}
    }
    	\caption{Comparison results between gradient propagation and loss propagation strategies.}
	\label{table_lossprop}
\end{table}



\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{fig/selected_positives_v3.pdf}
    \caption{
    Discovered patches by our GHP selection process. From the left to right columns, red boxes indicate the anchors, the closest patches within the task-agnostic reference pool~(reference), and GHP sets chosen in the mini-batch, respectively.
    % Visualization of the selected patches by our GHP selection process. Red boxes indicate the anchors, the closest patches within the reference pool~(i.e., reference), and GHP sets chosen in the mini-batch.
    % As shown in the second column, the anchor-dependent reference selected from $Q$ can be an appropriate criterion to select GHP.
    % For each anchor, the GHP samples are successfully selected considering semantics based on such reliable reference. By comparing 3-th and 4-th rows, the anchor-dependent positives are selected by distinguishing between sub-semantics such as `head' and `leg', even if they both belong to the same semantic `person'.
    }
    \label{fig_selected_positives}
    % \vspace{-0.2cm}
\end{figure}


\subsection{Visualization}
% \noindent
\paragraph{Discovered GHP.}
To demonstrate the effectiveness of our GHP selection process, we visualize the selected GHP sets for different anchors in a single image in Fig.~\ref{fig_selected_positives}.
First, we observe that the corresponding reference point in the second column is semantically correlated with the anchor patch.
Also, the obtained GHP sets verify the appropriateness of the use of reference points as each anchor's criterion as they are capable of precisely discovering the semantically-similar positives.
For example, we find that all the anchors, reference points, and GHP sets have the same semantic labels in the first and the second rows~(solid~(mountain) and ground~(snow)).
More intriguing results are in the third and fourth rows where we find that the GHP selection process distinguishes the body parts in a much more fine-grained manner than the given annotation, i.e., person.
These results imply that the designed GHP selection is well-designed and capable of capturing detailed semantic contexts.
% is selected considering sophisticated semantics, even further than simply capturing class-specific features.

% can discover the semantically similar features within the mini-batch, we visualize the selected GHP set in Fig.~\ref{fig_selected_positives}.
% We can observe that each reference point is semantically correlated with each anchor patch thus selected GHP samples turn out to be semantically identical.
% Interestingly, to compare the third and fourth rows, both anchor patches indicate the semantics of the person, but the GHP set for the fourth row shows more emphasis on the leg of the person.
% These results imply that the GHP set is selected considering sophisticated semantics, even further than simply capturing class-specific features.

% \paragraph{The size of GHP set.}
% Positive 개수 어느정도 골라지는지 그래프 같은걸로 보여주기

%%%%%%%%%%%%%%%%%%%% 막대그래프 같은걸로 통일해서 작게 만들기 %%%%%%%%%%%%%%%%%%%%%%%%
% \noindent
\subsection{Robustness to Hyperparameters}

In this subsection, we enumerate our use of hyperparameters in Tab.~\ref{Tab.hyperparamters} and conduct an ablation study.
Overall, our proposed training scheme is robust to hyperparameters as Fig.~\ref{fig_ablation_hyperparameters} supports the claim.
Below, we illustrate the influences of each parameter.
Despite the slight drop in performance from time to time, it is insignificant since the change in performance is very marginal and results are consistent.

\noindent
\paragraph{The Number of Data in the Reference Pool.}
% The number of data in reference pool $Q$ determines the awareness of data distribution.
Fig.~\ref{fig_ablation_hyperparameters}~(a) shows the performances with varying numbers of data $M$ in the reference pool.
Generally, the reference pool is not very vulnerable to its size, unless the capacity is either too small or too large.
When the reference pool is skinny, it may not be sufficient to represent all kinds of semantics present in the dataset.
In other words, the small reference pool may induce a biased criterion for each anchor which may incur biased training.
On the other hand, when the reference pool is too large, a tight threshold~($c_i$) could be derived, which can interfere with gathering GHP.
% semantic 정보 골고루 있어야하는데. reference 풀이 작으면 semantic에 대한 정보가 골고루 담기지 않을 수도 있다.
% 이러면 앵커마다 reference pool에 가장가까운것에 대한 기준이 bias가 생길수 있다. -> positive의 개수가 anchor마다 달라진다. 어떤건 다른클래스도 positive, 어떤건 너무 어려운 criterion

% Although the larger capacity tends to retain better results, 
% For a skinny pool, it cannot provide appropriate criteria for discovering the GHP because it may not include sufficient semantics to show the prototypicality of given data distribution.
% On the contrary, although a large reference pool can make the global hidden positive set to be more reliable, but may fail to contain sufficient positives.
% As a result, the reference pool needs to be large enough to contain sufficient semantics, but at the same time, it should also guarantee the number of GHP for each anchor.

% number of data in reference pool은 다양한 feature가 저장될 만큼 일정 이상 충분하기만 하면 된다.
% 그렇다고 너무 많으면 positive 개수가 너무 적어지기 때문에 좋지 않다.


% Main table
\begingroup
\setlength{\tabcolsep}{6pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.0} % Default value: 1
\begin{table}[t!]
    \centering
    \small
    \begin{tabular}{l c c}
        \hlineB{2.5}
        Dataset & $M$ & $\tau$ \\
        \hlineB{2.5}
        COCO-stuff & 2048 & 0.8 \\
        Cityscapes & 2048 & 0.6 \\
        Potsdam-3 & 1024 & 0.4 \\
        \hlineB{2.5}
    \end{tabular}
    \caption{Hyperparameter used for each dataset.}
    \label{Tab.hyperparamters}
\end{table}
\endgroup

\begin{figure}[t]
    \centering
    % \includegraphics[width=1.0\textwidth]{fig/ablation_v2.pdf}
    \includegraphics[width=0.47\textwidth]{fig/ablation2_v3.pdf}
    \caption{
    Ablation studies on hyperparameters.
    }
    \label{fig_ablation_hyperparameters}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \noindent
% \paragraph{Momentum Coefficient for EMA Model.}
% We also test with varying coefficients to update the momentum segmentation head.
% This hyperparameter directly affects the quality of $Q^{\text{seg}}$ since $Q^{\text{seg}}$ is composed of features from the momentum segmentation head.
% As the coefficient gets bigger, more features of $Q^{\text{seg}}$ will not be changed.
% Thus, if the coefficient is too big, it may violate our key focus to produce task-specific positives.
% Nonetheless, we found that our model is not vulnerable to the momentum coefficient as our performances are steady with it between 0.9 and 0.999.
% Q seg 를 define할때 쓰이는 것들이 여기서 나오지
% 근데 파라미터가 커지면, 잘 안변한다.
% 그러면 task-specific하게 잘 업데이트가 잘 되지 안흔다.
% 그래서 너뭌면 성능이 안좋아진다.
% 0.9이상이면 그래도 어느정도 괜찮더라~
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% The coefficient updating the momentum segmentation head controls the feature consistency from different mini-batches which is utilized to define $Q^{\text{seg}}$ and $P^{\text{seg}}$.
% The result in Fig.~\ref{fig_ablation_hyperparameters}~(b) shows that a small coefficient can diminish the performance since the stored features in the reference pool $Q^{\text{seg}}$ can be inconsistent.
% On the other hand, an extremely large coefficient can decelerate the model update.
% \textcolor{blue}{
% However, this coefficient does not have a significant impact on performance changes. 
% Therefore, we use the appropriate coefficient of 0.99.
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \noindent
% \paragraph{Percentage of Negative Samples to Use.}
% As stated in Sec.~\ref{sec_objective_function}, our negative set for contrastive learning is constructed by a subset of features other than the selected GHP.
% Fig.~\ref{fig_ablation_hyperparameters}~(c) shows the results with varying percentages of used negative samples $\rho$ from 1\% to 100\%.
% The slight performance decrease when using 100\% is because the increase in negative pairs may interfere with the model training~\cite{eisnet}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






% As stated in Sec.~\ref{sec_objective_function}, we form the negative set $N_i$ on a subset of the remaining samples excluding GHP. 
% To show the performance differences when varying the percent of negative samples $\rho$, we conduct the ablation study as shown in Fig.~\ref{fig_ablation_hyperparameters}~(c). 
% The results indicate that it is better to use a subset of samples rather than to use all samples.
% \textcolor{red}{
% As stated in Sec.~\ref{sec_objective_function}, we form the negative set $N_i$ as a subset of the remaining samples excluding GHP.
% Actually, it is better to use a subset rather than to use all samples as described in Fig.~\ref{fig_ablation_hyperparameters}~(c).
% This is because too many negative samples sometimes interfere with the model training compared to positive samples~\cite{eisnet}.
% }



\noindent
\paragraph{Temperature Parameter.}
The temperature parameter $\tau$ is the scaling parameter to manipulate the sensitivity of the contrastive loss. 
If $\tau$ gets bigger, the objective function becomes robust to the difference between the similarity of the positive and negative samples.
On the other hand, when $\tau$ gets lower, the embedding distribution is likely to be more uniform~\cite{wang2021understanding}.
Although the results do not fluctuate much, we observe that training uniformly distributed embedding space leads to a slight performance drop in Fig.~\ref{fig_ablation_hyperparameters}~(b).


% To show the effect when varying the temperature parameter $\tau$ to scale the sensitivity of the contrastive loss, we provide an ablation study of $\tau$ in a large range in Fig.~\ref{fig_ablation_hyperparameters}~(d).
% A small $\tau$ makes the representation pair with large differences much further away. 
% On the other hand, a large $\tau$ would be generous of such differences~\cite{zhang2021temperature}.
% According to our findings, performances are not extremely sensitive to $tau$, but too low temperatures make it more difficult to form clusters.
% The temperature parameter $\tau$ scales the sensitivity of the contrastive loss. 
% Specifically, a small $\tau$ makes the representation pair with large differences much further away.
% On the other hand, a large $\tau$ would be generous of such differences~\cite{zhang2021temperature}.
% Nonetheless, we notice that performance is not extremely sensitive to $\tau$ as shown in Fig.~\ref{fig_ablation_hyperparameters}~(d), but too low temperatures make it more difficult to form clusters.



% \noindent
% \textbf{consistency regularizer weight}1

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






% % Locality Range ablation study
% \begingroup
% \setlength{\tabcolsep}{6pt} % Default value: 6pt
% \renewcommand{\arraystretch}{1.0} % Default value: 1
% \begin{table}[!t]
%     \centering
%     \footnotesize
%     \begin{tabular}{c| c c  c c}
%         \hlineB{2.5}
%         \multirow{2}{*}{Locality Range} & \multicolumn{2}{c}{Unsupervised} & \multicolumn{2}{c}{Linear} \\
%         % \cline{2-5}
%         \multicolumn{1}{l|}{} & Acc. & mIoU & Acc. & mIoU \\
%         \hline
%         3x3 &  \\
%         5x5 &  \\
%         28x28 &  \\
%         \hlineB{2.5}
%     \end{tabular}
%     % \vspace{-0.1cm}
%     \caption{Results.}
%     \label{Tab.Kinetics}
%     % \vspace{-0.4cm}
% \end{table}
% \endgroup


% Memory size ablation study
% \begingroup
% \setlength{\tabcolsep}{6pt} % Default value: 6pt
% \renewcommand{\arraystretch}{1.0} % Default value: 1
% \begin{table}[!t]
%     \centering
%     \footnotesize
%     \begin{tabular}{c| c c  c c}
%         \hlineB{2.5}
%         \multirow{2}{*}{Memory size} & \multicolumn{2}{c}{Unsupervised} & \multicolumn{2}{c}{Linear} \\
%         % \cline{2-5}
%         \multicolumn{1}{l|}{} & Acc. & mIoU & Acc. & mIoU \\
%         \hline
%         256 & 51.7 & 21.3 & 74.8 & 41.6 \\ % ex818
%         512 & 55.1 & 21.9 & 75.6 & 42.7 \\ % ex819
%         1024 & 55.6 & 22.3 & 76.3 & 43.4 \\ % ex820
%         2048 & 57.2 & 24.6 & 75.6 & 42.7 \\
%         4096 & 56.7 & 24.0 & 75.9 & 43.3 \\ % ex821
%         8192 & 55.4 & 21.9 & 76.0 & 42.8 \\ % ex822
%         \hlineB{2.5}
%     \end{tabular}
%     % \vspace{-0.1cm}
%     \caption{Ablation study on memory size of reference pool.}
%     \label{Tab.Kinetics}
%     % \vspace{-0.4cm}
% \end{table}
% \endgroup

% Memory size ablation study
% \begingroup
% \setlength{\tabcolsep}{6pt} % Default value: 6pt
% \renewcommand{\arraystretch}{1.0} % Default value: 1
% \begin{table}[!t]
%     \centering
%     \footnotesize
%     \begin{tabular}{c| c c  c c}
%         \hlineB{2.5}
%         \multirow{2}{*}{Momentum} & \multicolumn{2}{c}{Unsupervised} & \multicolumn{2}{c}{Linear} \\
%         % \cline{2-5}
%         \multicolumn{1}{l|}{} & Acc. & mIoU & Acc. & mIoU \\
%         \hline
%         0.9 & 57.0 & 23.8 & 76.1 & 43.0 \\ % ex803
%         0.99 & 57.3 & 24.6 & 75.6 & 42.7 \\ % ex804
%         0.999 &  57.1 & 24.4 & 75.7 & 42.9 \\  % ex737
%         0.9999 & 55.0 & 21.9 & 76.4 & 42.8 \\ % ex805
%         \hlineB{2.5}
%     \end{tabular}
%     % \vspace{-0.1cm}
%     \caption{Ablation study on momentum.}
%     \label{Tab.Kinetics}
%     % \vspace{-0.4cm}
% \end{table}
% \endgroup

% \begingroup
% \setlength{\tabcolsep}{6pt} % Default value: 6pt
% \renewcommand{\arraystretch}{1.0} % Default value: 1
% \begin{table}[!t]
%     \centering
%     \footnotesize
%     \begin{tabular}{c| c c  c c}
%         \hlineB{2.5}
%         \multirow{2}{*}{consist weight} & \multicolumn{2}{c}{Unsupervised} & \multicolumn{2}{c}{Linear} \\
%         % \cline{2-5}
%         \multicolumn{1}{l|}{} & Acc. & mIoU & Acc. & mIoU \\
%         \hline
%         0.01 & 56.5 & 23.7 & 76.2 & 43.1 \\ 
%         0.05 & 57.2 & 24.6 & 75.6 & 42.7 \\
%         0.1 & 57.0 & 24.0 & 76.1 & 43.0 \\ 
%         0.2 & 57.1 & 24.1 & 75.6 & 42.6 \\
%         0.5 & 57.7 & 23.7 & 42.4 & 75.2 \\ 
%         1.0 & 54.7 & 23.2 & 75.5 & 42.4 \\  
%         \hlineB{2.5}
%     \end{tabular}
%     % \vspace{-0.1cm}
%     \caption{Ablation study on consistency loss.}
%     \label{Tab.Kinetics}
%     % \vspace{-0.4cm}
% \end{table}
% \endgroup

% \begingroup
% \setlength{\tabcolsep}{6pt} % Default value: 6pt
% \renewcommand{\arraystretch}{1.0} % Default value: 1
% \begin{table}[!t]
%     \centering
%     \footnotesize
%     \begin{tabular}{c| c c  c c}
%         \hlineB{2.5}
%         \multirow{2}{*}{$\rho$} & \multicolumn{2}{c}{Unsupervised} & \multicolumn{2}{c}{Linear} \\
%         % \cline{2-5}
%         \multicolumn{1}{l|}{} & Acc. & mIoU & Acc. & mIoU \\
%         \hline
%         0.01 & 57.2 & 23.8 & 76.1 & 43.0 \\ % ex827
%         0.02 &  57.2 & 24.6 & 75.6 & 42.7 \\
%         0.05 & 55.0 & 23.7 & 76.3 & 43.0 \\
%         0.1 & 56.0 & 24.3 & 75.6 & 42.3 \\% ex828
%         0.5 & 55.6 & 23.0 & 75.9 & 42.4 \\
%         1.0 & 55.7 & 23.1 & 76.2 & 42.6 \\ % ex829
%         \hlineB{2.5}
%     \end{tabular}
%     % \vspace{-0.1cm}
%     \caption{Ablation study on percentage of negative features.}
%     \label{Tab.Kinetics}
%     % \vspace{-0.4cm}
% \end{table}
% \endgroup


% \begingroup
% \setlength{\tabcolsep}{6pt} % Default value: 6pt
% \renewcommand{\arraystretch}{1.0} % Default value: 1
% \begin{table}[!t]
%     \centering
%     \footnotesize
%     \begin{tabular}{c| c c  c c}
%         \hlineB{2.5}
%         \multirow{2}{*}{temperature} & \multicolumn{2}{c}{Unsupervised} & \multicolumn{2}{c}{Linear} \\
%         % \cline{2-5}
%         \multicolumn{1}{l|}{} & Acc. & mIoU & Acc. & mIoU \\
%         \hline
%         0.2 &  53.2 & 20.9 & 75.7 & 41.9 \\ % ex823
%         0.4 & 53.0 & 24.1 & 76.1 & 43.2 \\ % ex824
%         0.6 & 53.4 & 22.3 & 76.4 & 43.3 \\ % ex825
%         0.8 & 57.2 & 24.6 & 75.6 & 42.7 \\
%         1.0 & 56.5 & 23.5 & 75.9 & 41.8 \\ % ex826
%         \hlineB{2.5}
%     \end{tabular}
%     % \vspace{-0.1cm}
%     \caption{Ablation study on temperature.}
%     \label{Tab.Kinetics}
%     % \vspace{-0.4cm}
% \end{table}
% \endgroup




% supple에 selfpatch stego 실험한거 싹다보여주기 (35제외)


% augmentation 을 positive로 쓰지않기때문에 기존에 거의 정해지다시피한 temperature 0.07을 안쓴다.


% \subsection{Limitation}
% Under the long-tailed distribution, the amount of selected GHP may be biased towards the majority classes since the criterion is anchor-dependent.
% For example, while our method out-tops the existing SOTA by a large margin in unsupervised accuracy, there is only a slight increase in unsupervised mIoU.
% Nevertheless, the overwhelming performances with linear evaluation 

% Under an unavoidable circumstance that the number of GHP per anchor can vary due to its anchor dependency, the class imbalance problem, a bias toward the majority class, is bound to arise.
% For example, unsupervised accuracy out-tops the previous SOTA by a large margin, while unsupervised mIoU is enhanced over SOTA with relatively less margin.
% Nevertheless, the overwhelming performances on linear evaluation verify the superiority of representation quality.
% We leave the consideration of class imbalance problem for future work.
% patch 당 다른 패치와 동일한 개수의 relation을 고려하지 않고, positive를 찾는 방향으로 가기에
% 데이터셋내에 있는 imbalance 문제가 대두 될 수 밖엥벗음.
% 예를들어, unsupervised acc는 높지만, unsup miou는 차이가 별로 나지않음 가끔 지는 경우도 있음.
% 하지만, representation quality를 더욱 잘 반영하는 linear 학습후에는 
% 현재까지의 sota model 에 비해 압도적인 성능을 보여주니 크게 문제가 되지않음.
% 앞으로 future work로 imbalance를 고려한다면 더욱 좋아질거라 생각.