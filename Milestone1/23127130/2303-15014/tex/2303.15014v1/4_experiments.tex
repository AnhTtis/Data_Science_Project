\section{Experiments}

\subsection{Datasets and Experimental Settings}
\paragraph{Datasets.}
We utilize COCO-stuff~\cite{coco}, Cityscapes~\cite{cityscapes}, and Potsdam-3 datasets following the existing works~\cite{stego, picie, iic}.
COCO-stuff is a large-scale scene understanding dataset that consists of dense pixel-level annotations and Cityscapes is a more recently publicized dataset having street scenes across 50 different cities.
% \textcolor{red}{
% COCO-stuff is a large-scale scene understanding dataset that consists of dense pixel-level annotations and Cityscapes is a more recently publicized dataset that is comprised of street scenes across 50 different cities.
% }
Potsdam-3 dataset contains satellite images.
Following the baselines~\cite{iic, picie, stego}, we choose the 27 classes for COCO-stuff and Cityscapes datasets, and 3 classes for Potsdam-3 dataset.

% We only use Postdam-3 setting among the 3- and 6-label variants.


\paragraph{Evaluation Protocols and Metrics.}
To evaluate our approach, we conduct two testing methods; clustering and linear probe~\cite{stego}.
Clustering is to measure how well the semantic-preserving clusters are formed.
% Once the unlabeled clusters are computed with the extracted representations, clusters are matched with the ground truth class labels.
Once the unlabeled clusters are computed with the extracted representations, clusters are matched with the ground truth class labels using the Hungarian matching algorithm.
% On the other hand, linear probe is the method that many unsupervised domains employ to evaluate their pretrained weights.
% By simply freezing the trained weights and adding a learnable projection head, representations are mapped into the class labels. 
% Then, these predictions are used to measure the quality of pretrained weights.
% On the other hand, the linear probe is the method that many unsupervised domains employ to evaluate their model as a pretrained weight.
% Specifically, by freezing the trained model and adding a learnable projection head, representations extracted by the model are mapped into the class labels.
% Then, based on the predictions of the trained head, the quality of the model as a pretrained weight can be measured.
On the other hand, the linear probe is a popular method to evaluate the quality of representations learned in unsupervised manners.
Specifically, an additional linear layer is learned with representations extracted from the frozen unsupervised model for evaluation and ground truth labels.
Inferring the representations of test data extracted by the frozen model with the learned linear layer, we can measure the quality of representations.
With two types of protocols, the performance is measured by two common metrics;  accuracy~(Acc.) and mean Intersection Over Union~(mIoU).


% Main table
\begingroup
\setlength{\tabcolsep}{4.2pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.0} % Default value: 1
\begin{table}[t!]
    \centering
    \small
    \begin{tabular}{l c c c  c c}
        \hlineB{2.5}
        \multirow{2}{*}{Method}& \multirow{2}{*}{Backbone} & \multicolumn{2}{c}{Unsupervised} & \multicolumn{2}{c}{Linear} \\
        % \cline{2-5}
        \multicolumn{2}{l}{}  & Acc. & mIoU & Acc. & mIoU \\
        \hlineB{2.5}
        % \multicolumn{5}{c}{ConvNet} \\
        % \hline
        DC~\cite{deepcluster} & R18+FPN & 19.9 & - & - & - \\
        MDC~\cite{deepcluster} & R18+FPN & 32.2 & 9.8 & 48.6 & 13.3 \\
        IIC~\cite{iic} & R18+FPN & 21.8 & 6.7 & 44.5 & 8.4 \\
        PiCIE~\cite{picie} & R18+FPN & 48.1 & 13.8 & 54.2 & 13.9 \\
        PiCIE+H~\cite{picie} & R18+FPN & 50.0 & 14.4 & 54.8 & 14.8 \\
        % \hlineB{2.5}
        % \multicolumn{5}{c}{Vit-small} \\
        \hline
        DINO~\cite{dino} & ViT-S/8 & 28.7 & 11.3 & 68.6 & 33.9 \\ % ex44
        + TransFGU~\cite{transfgu} & ViT-S/8 & 52.7 & 17.5 & - & - \\
        + STEGO~\cite{stego} & ViT-S/8 & 48.3 & 24.5 & 74.4 & 38.3 \\
        + HP~(Ours) & ViT-S/8 & \textbf{57.2} & \textbf{24.6} & \textbf{75.6} & \textbf{42.7} \\ % ex804
        % \hlineB{2.5}
        % \multicolumn{5}{c}{Vit-base} \\
        \hline
        DINO~\cite{dino} & ViT-S/16 & 22.0 & 8.0 & 50.3 & 18.1 \\ % ex689
        + STEGO~\cite{stego} & ViT-S/16 & 52.5 & 23.7 & 70.6 & 34.5 \\ % ex690_2
        % + Ours & ViT-S/16 & \textbf{54.9} & 21.9 & \textbf{74.5} & \textbf{40.9} \\ % ex785
        + HP~(Ours) & ViT-S/16 & \textbf{54.5} & \textbf{24.3} & \textbf{74.1} & \textbf{39.1} \\ % ex901
        \hline
        SelfPatch~\cite{selfpatch} & ViT-S/16 & 35.1 & 12.3 & 64.4 & 28.5 \\ % ex682
        + STEGO~\cite{stego} & ViT-S/16 & 52.4 & 22.2 & 72.2 & 36.0 \\ % stego7
        + HP~(Ours) & ViT-S/16 & \textbf{56.1} & \textbf{23.2} & \textbf{74.9} & \textbf{41.3} \\ % ex932
        \hlineB{2.5}
    \end{tabular}
    % \vspace{-0.1cm}
    \caption{Experimental results on COCO-stuff dataset with various backbones and pretrained models.}
    \label{Tab.coco}
    % \vspace{-0.4cm}
\end{table}
\endgroup

\paragraph{Implementation Details.}
% model 구조
For fair comparisons with our baselines~\cite{stego, transfgu},
we follow them to mainly use DINO pretrained ViT models as a backbone network $\mathcal{F}$ for the COCO-stuff dataset.
In addition, we also test with the advanced backbone, SelfPatch~\cite{selfpatch}, on the COCO-stuff dataset.
% We mainly use ViT-small model~\cite{vit} pretrained on DINO~\cite{dino} and SelfPatch~\cite{selfpatch} as a backbone network $\mathcal{F}$ for COCO-stuff dataset.
% For the Cityscapes dataset, we use DINO pretrained ViT-small and ViT-base model for a fair comparison with TransFGU~\cite{transfgu} and STEGO~\cite{stego}. 
% In addition, we use DINO pretrained ViT-base model for Potsdam-3 dataset.
The segmentation head $\mathcal{S}$ is constructed with a two-layer RELU MLP as STEGO, and the projection head $\mathcal{Z}$ is composed of a linear layer equipped with a normalization layer.
The embedding dimension $K$ is set to 512 for ViT-S/8 and ViT-B/8 models, and 256 for ViT-S/16.
We train the model for 3, 20, and 10 epochs for COCO-stuff, Cityscapes, and Potsdam-3 datasets, respectively, based on the AdamW optimizer with a learning rate of 0.0005 and weight decay of 0.1.
% \textcolor{red}{
The task-specific reference pool $Q^{\text{sp}}$ is renewed every 100 iterations throughout the training.
The percentage of negative samples usage $\rho$ is set to 2.
In the last stage, we add a feature refinement step utilizing Conditional Random Field~\cite{crf} as did in STEGO.
% }
Evaluation metrics, i.e., clustering and linear probe, are optimized with the Adam optimizer each with learning rates of 0.005 and 0.001.
% The clustering and linear probe are trained separately with the Adam optimizer using learning rates of 0.005 and 0.001, respectively. 
% All experiments are conducted with single 



% hyperparam
% We set the size of the reference pool and momentum coefficient as (2048, 0.99), (2048, 0.999), and (1024, 0.99) for COCO-stuff, Cityscapes, and Potsdam-3 datasets, respectively. 
% We set $\rho$ as 2\% for all experiments. 
% Also, temperature parameter $\tau$ are set to 0.8, 0.5, 0.9, 0.6, 0.15, 0.4.









% Cityscapes result
% \begingroup
% \setlength{\tabcolsep}{6pt} % Default value: 6pt
% \renewcommand{\arraystretch}{1.0} % Default value: 1
% \begin{table}[!t]
%     \centering
%     \footnotesize
%     \begin{tabular}{l c c c }
%         \hlineB{2.5}
%         \multirow{2}{*}{Method} & \multirow{2}{*}{Backbone} &
%         \multicolumn{2}{c}{Unsupervised} \\
%         % \cline{2-5}
%         \multicolumn{2}{l}{} & Acc. & mIoU \\
%         \hlineB{2.5}
%         % \multicolumn{5}{c}{ConvNet} \\
%         % \hline
%         MDC~\cite{deepcluster} & Res18+FPN & 40.7 & 7.1 \\
%         IIC~\cite{iic} & Res18+FPN & 47.9 & 6.4\\
%         PiCIE~\cite{picie} & Res18+FPN & 65.5 & 12.3 \\
%         \hline
%         DINO~\cite{dino}+TransFGU~\cite{transfgu} & ViT-S/8 & 77.9 & 16.8 \\
%         DINO~\cite{dino}+Ours & ViT-S/8 & \textbf{79.4} & \textbf{18.2}\\ % ex128
%         \hline
%         DINO~\cite{dino}+STEGO~\cite{stego} & ViT-B/8 & 73.2 & \textbf{21.0} \\
%         % STEGO(reprod.)~\cite{stego} & 70.9 & 21.7 & 90.3 & 26.8 \\
%         % STEGO~(reprod.) & 69.6 & 19.2 & 89.9 & 25.2 \\
%         DINO~\cite{dino}+Ours & ViT-B/8 & \textbf{79.5} & 18.4 \\ % ex538
%         \hlineB{2.5}
%     \end{tabular}
%     % \vspace{-0.1cm}
%     \caption{Clustering performances on Cityscapes dataset. Scores of the previous methods are the official results reported on the corresponding papers.}
%     \label{Tab.cityscapes}
%     % \vspace{-0.4cm}
% \end{table}
% \endgroup

% % Cityscapes result
% \begingroup
% \setlength{\tabcolsep}{6pt} % Default value: 6pt
% \renewcommand{\arraystretch}{1.0} % Default value: 1
% \begin{table}[!t]
%     \centering
%     \footnotesize
%     \begin{tabular}{l c c c }
%         \hlineB{2.5}
%         \multirow{2}{*}{Method} & \multirow{2}{*}{Backbone} &
%         \multicolumn{2}{c}{Linear} \\
%         % \cline{2-5}
%         \multicolumn{2}{l}{} & Acc. & mIoU \\
%         \hlineB{2.5}
%         % \multicolumn{5}{c}{ConvNet} \\
%         % \hline
%         DINO~\cite{dino}+STEGO~\cite{stego} & ViT-B/8 & 90.3 & 26.8 \\
%         % STEGO(reprod.)~\cite{stego} & 70.9 & 21.7 & 90.3 & 26.8 \\
%         % STEGO~(reprod.) & 69.6 & 19.2 & 89.9 & 25.2 \\
%         DINO~\cite{dino}+Ours & ViT-B/8 & \textbf{90.9} & \textbf{33.0} \\ % ex538
%         \hlineB{2.5}
%     \end{tabular}
%     % \vspace{-0.1cm}
%     \caption{performances of linear probe on Cityscapes dataset. Scores for STEGO is reproduced results.}
%     \label{Tab.cityscapes_linear}
%     % \vspace{-0.4cm}
% \end{table}
% \endgroup

% Cityscapes result
\begingroup
\setlength{\tabcolsep}{4.2pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.0} % Default value: 1
\begin{table}[!t]
    \centering
    \small
    \begin{tabular}{l c c c  c c}
        \hlineB{2.5}
        \multirow{2}{*}{Method} & \multirow{2}{*}{Backbone} &
        \multicolumn{2}{c}{Unsupervised} & \multicolumn{2}{c}{Linear} \\
        % \cline{2-5}
        \multicolumn{2}{l}{} & Acc. & mIoU & Acc. & mIoU \\
        \hlineB{2.5}
        % \multicolumn{5}{c}{ConvNet} \\
        % \hline
        MDC~\cite{deepcluster} & R18+FPN & 40.7 & 7.1 & - & - \\
        IIC~\cite{iic} & R18+FPN & 47.9 & 6.4 & - & - \\
        PiCIE~\cite{picie} & R18+FPN & 65.5 & 12.3 & - & - \\
        \hline
        DINO~\cite{dino} & ViT-S/8 & 34.5 & 10.9 & 84.6 & 22.8 \\
        + TransFGU~\cite{transfgu} & ViT-S/8 & 77.9 & 16.8 & - & - \\
        + HP~(Ours) & ViT-S/8 & \textbf{80.1} & \textbf{18.4} & \textbf{91.2} & \textbf{30.6} \\ % ex129
        \hline
        DINO~\cite{dino} & ViT-B/8 & 43.6 & 11.8 & 84.2 & 23.0 \\
        + STEGO~\cite{stego} & ViT-B/8 & 73.2 & \textbf{21.0} & 90.3 & 26.8 \\
        % STEGO(reprod.)~\cite{stego} & 70.9 & 21.7 & 90.3 & 26.8 \\
        % STEGO~(reprod.) & 69.6 & 19.2 & 89.9 & 25.2 \\
        + HP~(Ours) & ViT-B/8 & \textbf{79.5} & 18.4 & \textbf{90.9} & \textbf{33.0} \\ % ex538
        \hlineB{2.5}
    \end{tabular}
    % \vspace{-0.1cm}
    \caption{Experimental results on Cityscapes dataset.}
    \label{Tab.cityscapes}
    % \vspace{-0.4cm}
\end{table}
\endgroup


% Potsdam-3 result
\begingroup
\setlength{\tabcolsep}{6pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.0} % Default value: 1
\begin{table}[!t]
    \centering
    \small
    \begin{tabular}{l c c}
        \hlineB{2.5}
        Method & Backbone & Unsup. Acc. \\
        \hlineB{2.5}
        Random CNN~\cite{iic}& VGG11 & 38.2 \\
        K-Means~\cite{sklearn}& VGG11 & 45.7 \\
        SIFT~\cite{sift} & VGG11 &38.2 \\
        ContextPrediction~\cite{context} & VGG11 & 49.6 \\
        CC~\cite{cc} & VGG11 & 63.9 \\
        DeepCluster~\cite{deepcluster} & VGG11 & 41.7 \\
        IIC~\cite{iic} & VGG11 & 65.1 \\
        \hline
        DINO~\cite{dino} & ViT-B/8 & 53.0 \\
        + STEGO~\cite{stego}& ViT-B/8 & 77.0 \\
        + HP~(Ours) & ViT-B/8 & \textbf{82.4}\\
        \hlineB{2.5}
    \end{tabular}
    % \vspace{-0.1cm}
    \caption{Experimental results on Potsdam-3 dataset.}
    \label{Tab.potsdam}
    % \vspace{-0.4cm}
\end{table}
\endgroup


\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{fig/qualitative_result_v7.pdf}
    \caption{
    Qualitative comparison results of Ours and STEGO on the COCO-stuff dataset with DINO pretrained ViT-S/8 backbone.
    % on the COCO-stuff dataset with DINO pretrained ViT-S/8 backbone in comparison to the existing state-of-the-art method, STEGO.
    }
    \label{fig_qualitative_result}
\end{figure*}

\subsection{Experimental Results}
% \noindent
% \bold{Quantitative results.}
We compare our proposed method against the prior techniques for the unsupervised segmentation~\cite{deepcluster, iic, picie, stego, transfgu}.
Most of the results in the result tables are brought from the literature~\cite{picie, stego}.
%Generally, 
In Tab.~\ref{Tab.coco}, it is observed that self-supervised models, i.e., DINO and SelfPatch, are already good segmentation predictors with the linear probe, which makes them a new baseline over the prior works for unsupervised segmentation.
Furthermore, we utilize two pretrained backbones with two kinds of architectures to compare with STEGO, in detail.
As reported, our proposed model provides consistent performance improvements over the previous SOTA model in almost all cases on the COCO-stuff dataset.
% Note that, we conducted extensive experiments to reproduce STEGO with SelfPatch due to their sensitivity to hyperparameters~(refer to the Appendix).


% Among two metrics, we claim that the evaluation by the linear probe better reflects the quality of representations. Specifically, learning the features of legs and shoes differently is more appropriate for unsupervised learning, since merging or separating them in the label space highly depending on the purposes. For instance, the linear prove will project those features to a close proximity if the given label space consider them as a human body. On the other hand, separating legs and shoes in representation space is likely to have lower scores in clustering-based evaluation, since the number of clusters utilized in the evaluation is matched with the label space and thus there is no vacancy cluster for shoes if it should be treated as a part of human body.

% For the similar performance in mIoU when evaluated with clustering, this is because the long-tailed distribution of segmentation datasets causes the learning the noisy positives difficult~\cite{huang2022uncertainty, karthik2021learning}.
% Still, as the linear probe verifies that our models provide more robust representations, we claim that it is not problematic at all.
% Furthermore, the superior performances on other metrics validate the benefits of our training scheme in learning unsupervised segmentation.

Results on Cityscapes also show a similar tendency.
As shown in Tab.~\ref{Tab.cityscapes}, ours outperform previous methods except for the mIoU when clustering is used for the evaluation.
For instance, we achieve 8.6\% and 23\% improvements in cluster accuracy and linear mIoU over STEGO, respectively.
For the slight decrease in cluster mIoU with ViT-B/8 architecture, we argue that it is insignificant since the linear probe better describes the quality of representations.
Specifically, clustering evaluation highly depends on the purpose of the dataset so that it is sensitive to the degree of class-specificity as different body parts can be either classified as human or as independent body parts.
However, whereas it is more appropriate to detect each body part independently for unsupervised learning as ours do in Fig.~\ref{fig_selected_positives}, the annotations for general datasets, e.g., COCO-stuff and cityscape, treat these body parts as a human class.
Such circumstances make the clustering evaluation vulnerable to the degree of the class hierarchy.
In contrast, the linear probe projects these features to close proximity if the given label space considers them as a human body.
Thus, we believe the linear probe is a more appropriate measure of representation quality.
% For the similar performance in cluster mIoU, this is because the long-tailed distribution of segmentation datasets causes the learning the noisy positives difficult~\cite{huang2022uncertainty, karthik2021learning}.

Also, we compare the cluster accuracy in Tab.~\ref{Tab.potsdam} on Potsdam-3 dataset.
We have achieved a 7\% boost over STEGO, confirming that ours also performs well even in a completely new domain.
Likewise, our superior results verify the effectiveness of our process of discovering global- and local-hidden positive patches. 



% \noindent
\paragraph{Qualitative Results.}
In addition to the quantitative results, we report qualitative results in Fig.~\ref{fig_qualitative_result}.
In comparison to STEGO, we observe that our results include fewer mispredicted pixels throughout the images, while the strength of preserving semantic locality is also validated.
For instance, whereas we find that the predicted label of the wheel is only partially correct in the 4th column, our results are consistent along the neighboring pixels.
These results also demonstrate the superiority of our method.


% we can observe that STEGO tends to miss the locality within the image even with the implementation of CRF. On the other hand, ours produce robust prediction results due to our gradient propagation strategy that encourages capturing the local similarity between the adjacent patches.
% The comparison results demonstrate the superiority of our method.
% GT, STEGO, Ours 순서로 예측결과 그림 보여준다
% crf 전의 결과이다.
% STEGO는 주변 정보에 대한 지식을 이용하지 않는다. 그래서 노이즈가 많다.
% 우리는 locality 잘 보존하도록 학습된다


