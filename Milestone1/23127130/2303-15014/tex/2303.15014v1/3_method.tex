\section{Method}
% [preliminary]
%: contrastive loss 소개

% [external hidden positive 구하기]
% positive가 많으면 뭐가 좋은데 기존에는 하나만쓴다
% pseudo positive 고르는법
% additional positive도 구하기

% [이걸로 contrastive learning ]
% loss function

% [inner positive]
%: backward 할 시 inner positive
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% [inner positive 구하기]
% locality가 이러이러하다 고려해야된다. 기존에는 고려안했다
% inner positive 구하는법

% [앞에 구한거들로 contrastive loss function + gradient propagation]
% loss function 선언하기
% backward할때 inner locality 고려하기
% (weighted mean으로 똑같은 loss 구하는거임)

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.85\textwidth]{fig/main_figure_v5.pdf}
%     \caption{asd
%     }
%     \label{fig_main}
% \end{figure*}

In this section, we introduce the pseudo-positive selection strategy to discover hidden positives with analogous semantics in Sec.~\ref{sec_global_positive}, the training objective with discovered positives in Sec.~\ref{sec_objective_function}, and the gradient propagation scheme to preserve the property of locality in Sec.~\ref{sec_loss_propagation}.

% Particularly, we define a contrastive objective with inferred GHP and ensure the locality by propagating the gradient to LHP.
% In accordance with sequential processes, we describe GHP selection strategy in Sec.~\ref{sec_global_positive}, training objective in Sec.~\ref{sec_objective_function}, and gradient propagation strategy in Sec.~\ref{sec_loss_propagation}.




% In this section, we introduce our method to discover the hidden positive pairs which possess analogous semantics and leverage these to improve contrastive learning.
% Particularly, we define a contrastive objective with inferred GHP and ensure the locality by propagating the gradient to LHP.
% In accordance with sequential processes, we describe GHP selection strategy in Sec.~\ref{sec_global_positive}, training objective in Sec.~\ref{sec_objective_function}, and gradient propagation strategy in Sec.~\ref{sec_loss_propagation}.

\subsection{Preliminary}
In unsupervised semantic segmentation, the model utilizes unlabeled image set $X=\{x_b\}_{b=1}^B$ where $B$ is the number of training data in the mini-batch. 
Given an image $x_b$ processed to the feature extractor $\mathcal{F}$, we have $H \cdot W$ features of $f_{i}\in \mathbb{R}^{C}$, where $i \in [ 1 , ... , H\cdot W ]$.
% During training, feature extractor $\mathcal{F}$ extracts the feature for an image $x_a$, composed of $H\cdot W$ patch features $f_{i}\in \mathbb{R}^{C}$.
Subsequently, the segmentation head $\mathcal{S}$ maps a patch feature $f_i$ to the corresponding segmentation feature $s_{i}\in \mathbb{R}^{K}$.
And then, the projection head $\mathcal{Z}$ produces a projected vector $z_{i}\in\mathbb{R}^{K}$ to formulate a contrastive loss function.
%Note that, dimension of $f_i$, $s_i$, and $z_i$ are $C$, $K$, and $K$, respectively.
%Note that, $C$ and $K$ are channel dimensions of $f_i$ and $s_i$, respectively.
%Note that $C$ is channel dimension of $s_i$, and $K$ is for $z_i$ and $s_i$.
In the inference stage, we use the segmentation feature $s_i$.

% In self-supervised settings for segmentation task, a feature for an image is treated separately as $H\cdot W$ patch features $f_i$.

% Based on the projected vector $z_i$ for an $i$-th patch, let $j(i)$ be the index of the other augmented sample, the conventional contrastive loss~\cite{simclr} for unsupervised semantic segmentation can be defined as follows:
Based on the projected vector $z_i$ for the $i$-th patch, let $j$ be the index of the augmented patch of $i$-th one.
% Then, the conventional contrastive loss~\cite{simclr} for unsupervised semantic segmentation can be defined as follows:
% \begin{equation}
% \label{eq_contrastive_loss}
%     L^{\text{self}}_i = - \text{log} \frac{\text{exp}(\text{sim}(z_i,z_j / \tau)}{\sum_{k=1}^{2NHW} \mathbbm{1}_{[k \neq i]} \text{exp}(\text{sim}(z_i,z_k) / \tau)},
% \end{equation}
% where the indicator function $\mathbbm{1}_{[k \neq i]}$ ensures the identical instance be ignored in negative set, $\tau$ denotes the scalar temperature parameter, and $\text{sim}(\cdot,\cdot)$ is cosine similarity between two vectors.
Then, the conventional self-supervised contrastive loss~\cite{simclr} for $i$-th patch in unsupervised semantic segmentation can be defined as follows:
% \begin{equation}
% \label{eq_contrastive_loss_self}
%     L^{\text{con}}_i = - \text{log} \frac{\text{exp}(\text{sim}(z_i,z_j / \tau)}{\sum_{n \in N}  \text{exp}(\text{sim}(z_i,z_n) / \tau)},
% \end{equation}
% where $j$ is the index of the positive sample corresponding to the $i$-th patch, $N$ indicates the set of negative indexes, $\tau$ denotes the scalar temperature parameter, and $\text{sim}(\cdot,\cdot)$ is cosine similarity between two vectors.
\begin{equation}
\label{eq_contrastive_loss_self}
    L^{\text{self}}_i = - \text{log} \frac{\text{exp}(\text{sim}(z_i,z_j / \tau))}{\sum_{a \in A}  \text{exp}(\text{sim}(z_i,z_a) / \tau)},
\end{equation}
where $A$ indicates a set of all indexes except $i$, $\tau$ denotes the scalar temperature parameter, and $\text{sim}(\cdot,\cdot)$ is cosine similarity between two vectors.

% For readability, we denote the patch-level as $*_i$ instead of $*_{i,h,w}$ in the rest of the paper.

\subsection{Global Hidden Positives}
\label{sec_global_positive}
Learning mutual information with augmented pixels only provides insufficient training signal in unsupervised semantic segmentation~\cite{stego}.
Therefore, it is important to discover hidden pseudo-positives to tailor the contrastive loss for unsupervised segmentation.
% To tailor the contrastive loss for an unsupervised semantic segmentation task where augmented pixel does not provide enough information to learn, discovering hidden positives is an important aspect.
% Yet, since we do not have semantic labels, it is hard to define the positives other than the augmented view of each pixel.
To discover the hidden positives at the initial stage, we utilize the self-supervised pretrained backbone~\cite{dino} as the task-agnostic criterion.
Then, we gradually increase the contribution of hidden positives found in a task-specific way for the training.
% , we utilize the self-supervised pretrained backbone~\cite{dino, stego} to provide initial training guidelines by discovering hidden positives in a task-agnostic manner, and then gradually increase the contribution of hidden positives found in a task-specific way.
Fig.~\ref{fig_main} provides the overview of the global hidden positive~(GHP) selection process.
% Thus, we utilize the pretrained backbone which is trained in an unsupervised manner~\cite{dino, stego} to construct a task-agnostic data reference pool which is to assess whether other features in the mini-batch are positive for each anchor feature.
% \textcolor{red}{
% However, the pretrained backbone may not be specialized in semantic segmentation as much as a segmentation head. 
% Therefore, we also design a task-specific data reference pool based on the segmentation head.
% }
% Thus, we utilize the pretrained backbone which is trained in an unsupervised manner~\cite{dino, stego}.
% Specifically, with the features from the backbone, we construct a task-agnostic data reference pool which is to assess whether other features in the mini-batch are positive for each anchor feature.

Initially, the pretrained backbone is utilized to construct a task-agnostic reference pool to assess whether other features in the mini-batch are semantically-alike for each anchor feature.
Specifically, task-agnostic reference pool, $Q^{\text{ag}} = \{q_{m}\}_{m=1}^{M}$, is composed of $M$ randomly sampled features that are extracted by the unsupervised pretrained backbone $\mathcal{F}$. %the pretrained backbone.
Note that, we only sample a single patch feature per image to ensure the semantic randomness of the reference pool.
This task-agnostic reference pool is fixed as the pretrained backbone is frozen throughout the training. 

Once the reference pool is gathered, for each patch feature $f_i$, we define an anchor-dependent similarity criterion $c_i$ to collect positives, as the distance to the closest feature within the reference pool $Q^{\text{ag}}$ by the cosine similarity:
\begin{equation}
\label{eq_select_criterion}
    c_{i} = \max_{q_m\in Q^\text{ag}} \text{sim}(q_m, f_{i}).
\end{equation}
% Once the reference pool is defined, we select the closest feature in the reference pool for each anchor and use it as a reference point.
% In detail, for each patch feature $f_i$, we set the reference point by computing the cosine similarity between the anchor feature $f_{i}$ and the features of the reference pool $Q^{\text{ag}}$.
% Then, we define the closest feature as the reference point that is an anchor-dependent distance criterion to collect positives:
% \begin{equation}
% \label{eq_select_criterion}
%     c_{i} = \max_{q_m\in Q^\text{ag}} \text{sim}(q_m, f_{i}).
% \end{equation}
For each anchor feature $f_i$, we basically treat the other feature in the mini-batch $f_j$ as positive if the similarity between $f_i$ and $f_j$ is greater than $c_i$.
% With the anchor-dependent reference point, to gather GHP, each anchor feature compares all other features and collects the samples with higher similarity compared to that of the reference point.
Still, although one patch feature might be the positive sample for the other, it may not hold mutually.
%vice versa may not be true. 
This is because the criterion $c_i$ is anchor-dependent.
To endow consistency in training, we make the GHP selection symmetric to prevent the relation between two patches from being ambiguous.
%so that the relation between two patches is not ambiguous.
Therefore, index set of GHP ${P}^{\text{ag}}_{i}$ for each $i$-th anchor feature $f_{i}$ is defined as follows:
% Therefore, Eq.~\ref{eq_positive_pre_index} can be rewritten as follows:
\begin{equation}
\label{eq_positive_pre_index_rewritten}
    {P}_{i}^{\text{ag}} = \{j \mid \text{sim}(f_{i}, f_{j}) > c_{i}
    \;\vee\; \text{sim}(f_{i}, f_{j}) > c_{j}\},
\end{equation}
where $j$ indicates the index for different patch features in the mini-batch.
% Based on this, the global hidden positive set $P_{i}^{\text{pre}}$ composed of the projected vectors $z$ can be defined as follows:
% % Then, we can select global positive projected vectors set $P_{i}^{\text{pre}}$ which correspond to the aforementioned index set as follows:
% \begin{equation}
% \label{eq_positive_pre}
%     P_{i}^{\text{pre}} = \{z_j \mid j \in {I}_{i}^{\text{pre}} \}
% \end{equation}
% % where $z$ indicates the projected vector by the projection head. 
Accordingly, such a distribution-aware reference pool allows the discovery of globally analogous features in consideration of each anchor.
% Accordingly, such distribution-aware reference pool allows to discover globally analogous features to be selected in consideration of each anchor.

However, although the reference pool built upon the features from an unsupervised pretrained network can serve as an appropriate basis for positivity, it may be insufficient since it lacks task-specificity. 
We argue that features from the segmentation head are more task-specific than those from the pretrained backbone.
% We argue that the features from the segmentation head have a deeper understanding of semantic similarity than those from the task-agnostic backbone.
% Therefore, along with the global hidden positive set $P^{\text{pre}}$, we construct additional task-specific global hidden positive set $P^{\text{seg}}$ utilizing the features from the segmentation head.
Therefore, along with the GHP selected by $P^{\text{ag}}$, we construct additional task-specific GHP utilizing the features from the segmentation head.
% Specifically, task-specific global hidden positive set $P^{\text{seg}}$ is composed of the features $s^\prime=\mathcal{S}^\prime(f)$ where ${\mathcal{S}^\prime}$ indicates the momentum segmentation head.
% % Following the positive discovering process of Eq.~\ref{eq_select_criterion}-\ref{eq_positive_pre}, $P^{\text{seg}}$ can be expressed as
% In advance, we also define $Q^{\text{seg}}$ based on the segmentation features $s^\prime$ and iteratively updated throughout the training to stay task-specificity.
% Following the positive discovering process of Eq.~\ref{eq_select_criterion}-\ref{eq_positive_pre}, $P^{\text{seg}}_i$ for $i$-th anchor can be expressed as
% \begin{equation}
%     P_{i}^{\text{seg}} = \{z_{j}\mid \text{sim}({s^\prime}_i, {s^\prime}_j) < c^\prime_{i} \vee \text{sim}({s^\prime}_i, {s^\prime}_j) < c^\prime_{j} \},
% \end{equation}
% % where $j$ is the index for other features and 
% where $c^\prime_{i}$ is calculated with Eq.~\ref{eq_select_criterion} by substituting the $Q$ and $f_i$ with $Q^\text{seg}$ and $s^\prime_i$, respectively.
% % , while $Q^\text{seg}$ is initially defined based on the segmentation feature $s^\prime$ and iteratively updated to stay task-specificity.

Specifically, an index set of task-specific GHP $P_{i}^{\text{sp}}$ is formed similarly to Eq.~\ref{eq_positive_pre_index_rewritten} by comparing the features $s^\prime=\mathcal{S}^\prime(f)$ and task-specific reference pool $Q^{\text{sp}}$,
% Be aware that ${\mathcal{S}^\prime}$ indicates the momentum segmentation head, and the task-specific reference pool $Q^{\text{seg}}$ comprises $s^\prime$ and iteratively updated during training.
where ${\mathcal{S}^\prime}$ indicates the momentum segmentation head, and the task-specific reference pool $Q^{\text{sp}}$ comprises $s^\prime$.
Also, along with the update of the segmentation head, during training, the reference pool is periodically renewed.
Formally, with $c^\prime_{i}$, calculated by substituting $Q^\text{ag}$ and $f_i$ in Eq.~\ref{eq_select_criterion} with $Q^\text{sp}$ and $s^\prime_i$, respectively, the $P_{i}^{\text{sp}}$ is expressed as follows:
\begin{equation}
    P_{i}^{\text{sp}} = \{j \mid \text{sim}({s^\prime}_i, {s^\prime}_j) > c^\prime_{i} \vee \text{sim}({s^\prime}_i, {s^\prime}_j) > c^\prime_{j} \}.
\end{equation}
Note that, the usage of the momentum segmentation head is for the stability of the reference pool~\cite{ema, sessd}.


% Specifically, an index set of task-specific global hidden positive samples is formed with the features $s^\prime=\mathcal{S}^\prime(f)$ where ${\mathcal{S}^\prime}$ indicates the momentum segmentation head.
% In advance, we also define $Q^{\text{seg}}$ based on the segmentation features $s^\prime$ and iteratively updated throughout the training to stay task-specificity.
% Following the positive discovering process of Eq.~\ref{eq_select_criterion}-\ref{eq_positive_pre_index_rewritten}, the index set for $i$-th anchor $P^{\text{seg}}_i$ can be expressed as
% \begin{equation}
%     P_{i}^{\text{seg}} = \{j \mid \text{sim}({s^\prime}_i, {s^\prime}_j) > c^\prime_{i} \vee \text{sim}({s^\prime}_i, {s^\prime}_j) > c^\prime_{j} \},
% \end{equation}
% where $c^\prime_{i}$ is calculated with Eq.~\ref{eq_select_criterion} by substituting the $Q$ and $f_i$ with $Q^\text{seg}$ and $s^\prime_i$, respectively.
% Note that the usage of momentum segmentation head is due to instability of training~\cite{ema, sessd}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.\textwidth]{fig/gradient_propagation_v9.pdf}
    \caption{
    Illustration of our gradient propagation strategy to preserve local semantic consistency. 
    For each anchor, with its surrounding patches $I^{\text{surr}}$ and corresponding attention scores from feature extractor $\mathcal{F}$, local hidden positives~(LHP) $I^{\text{local}}$ are appointed based on the threshold Avg($\tilde{T}$)~(Eq.~\ref{eq_local_positive_index}).
    % \textcolor{blue}{
    % Given the surrounding patches $I^{\text{surr}}$ and the corresponding spatial attention score $\tilde{T}$ from feature extractor $\mathcal{F}$, for each anchor, the local hidden positives~(LHP) are appointed based on the threshold Avg($\tilde{T}$)~(marked in blue). 
    % }
    In a forward pass, the features of LHP $G$~(Eq.~\ref{eq_g_and_t}) are mixed by weighted average according to the attention scores $T$ to compute the objective function $\Psi$. 
    In this way, the loss gradient propagates toward the LHP in proportion to $T$ in the backward pass.
    }
    \label{fig_gradient_propagation}
\end{figure*}

\subsection{Objective Function}
\label{sec_objective_function}
To formulate a contrastive objective with mined GHP in Sec.~\ref{sec_global_positive}, we also need negative features.
% To formulate a contrastive objective, we also need negative features other than the sampled positives in Sec.~\ref{sec_global_positive}.
As we collected the positives throughout the mini-batch, the naive implementation of contrastive learning would utilize all features except the selected positives in the mini-batch as the negatives.
However, since an immoderate increase in the size of the negative set may disturb the model training~\cite{eisnet}, we form a negative set $N_i$ by randomly choosing $\rho\%$ of the remaining patches for each $i$-th anchor. Note that, the separate index sets of negative samples $N_i^{\text{ag}}$ and $N_i^{\text{sp}}$ are defined for each $P_i^{\text{ag}}$ and $P_i^{\text{sp}}$, correspondingly.
Also, unlike Eq.~\ref{eq_contrastive_loss_self}, our contrastive loss for each $i$-th anchor is more like a supervised objective~\cite{supcon} since we are given multiple positives:
% Also, unlike Eq.~\ref{eq_contrastive_loss_self}, our contrastive loss for each $i$-th anchor can be formulated as follows since we are given multiple positive samples:
\begin{eqnarray}
\label{eq_contrastive_loss}
    L^{\text{cont}}(z_i, P, N) = \frac{-1}{|P|}\sum_{p \in P} \text{log} \frac{\text{exp}(\text{sim}(z_i,z_p) / \tau)}{\sum\limits_{n \in (N \cup P)} \text{exp}(\text{sim}(z_i,z_n) / \tau)},
    % \nonumber
\end{eqnarray}

% \vspace{-1cm}
% \begin{eqnarray}
% % \vspace{-3cm}
% % \label{eq_contrastive_loss}
%     &
% \end{eqnarray}
% \vspace{-0.8cm}

where $z_i$, $P$, and $N$ are the projected anchor vector $\mathcal{Z}(\mathcal{S}(f_i))$, positive index set, and negative index set, respectively.
% Given the above, therefore, the objective function is formulated with the projected vector $z_{i}$ of the anchor feature.
% Under the task-agnostic global hidden positive set $P_i$, the objective function for each anchor is formulated as follows:
% \begin{equation}
% \label{eq_contrastive_loss_global_task_agnostic}
%     \Phi_{i}^{\text{global}} = \frac{-1}{|P_{i}|} \sum_{p=1}^{|P_{i}|} \text{log} \frac{\text{exp}(\text{sim}(z_{i},z_p) / \tau)}{\sum_{a=1}^{|A_{i}|} \text{exp}(\text{sim}(z_{i},z_a) / \tau)},
% \end{equation}
% where $z_p$ and $z_a$ are $p$-th positive sample in $P_i$ and $a$-th negative sample in $A_i$, respectively.
% Likewise, an additional objective function with task-specific global hidden positive set $P^\prime_i$ is formulated as follows:
% \begin{equation}
% \label{eq_contrastive_loss_global_task_specific}
%     \Psi_{i}^{\text{global}} = \frac{-1}{|P^\prime_{i}|} \sum_{p^\prime=1}^{|P^\prime_{i}|} \text{log} \frac{\text{exp}(\text{sim}(z_{i},z_{p^\prime}) / \tau)}{\sum_{a^\prime=1}^{|A^\prime_{i}|} \text{exp}(\text{sim}(z_{i},z_{a^\prime}) / \tau)}.
% \end{equation}
% In this way, the two objective functions are constructed under task-agnostic and task-specific global hidden positive sets, respectively.
% % Therefore, the overall objective function considering the both global hidden positive is defined by combining Eq~\ref{eq_contrastive_loss_global_task_agnostic} and Eq~\ref{eq_contrastive_loss_global_task_specific}.
% % \begin{equation}
% % \label{eq_contrastive_loss_global_overall}
% %     L_{i}^{\text{global}} = \hat{l}_{i}^{\text{global}} + \lambda \tilde{l}_{i}^{\text{global}},
% % \end{equation}
% % where $\lambda$ controls the contribution of the second term, which gradually increases from $0$ to $1$ throughout the training.
% % Note that, the samples with zero positive is excluded from training although this rarely exists.
% Given the above, the objective function is formulated with the projected vector $z_{i}$ of the anchor feature.
% Therefore, with the index set of task-agnostic global hidden positive samples $P_i^{\text{pre}}$, the objective function $\Phi_{i}^{\text{pre}}$ for each $i$-th anchor is formulated as follows:
% \begin{equation}
% \label{eq_contrastive_loss_global_task_agnostic}
% \begin{split}
%     \Phi_{i}^{\text{pre}} &= L^{\text{cont}}(z_i, P^{\text{pre}}_i, N^{\text{pre}}_{i})
%     \\
%     \Phi_{i}^{\text{seg}} &= L^{\text{cont}}(z_i, P^{\text{seg}}_i, N^{\text{seg}}_{i}).
% \end{split}
% \end{equation}
% Similarly, the objective function $\Phi_{i}^{\text{seg}}$ with the index set of task-specific positive samples $P_{i}^{\text{seg}}$ can be computed by substituting $P_i^{\text{pre}}$ and $A_i^{\text{pre}}$ in Eq.~\ref{eq_contrastive_loss_global_task_agnostic} with $P_i^{\text{seg}}$ and $A_i^{\text{seg}}$, respectively.
% In this way, the two objective functions are constructed under task-agnostic and task-specific global hidden positive samples, respectively.
% Given the above, the objective function is formulated with the projected vector $z_{i}$ of the anchor feature.
For simplicity, we use $\Phi_{i}^{\text{ag}}$ and $\Phi_{i}^{\text{sp}}$ to denote the objective functions with task-agnostic GHP $P^{\text{ag}}_i$ and task-specific GHP $P^{\text{sp}}_i$ for each $i$-th anchor as follows:
% Therefore, the objective functions $\Phi_{i}^{\text{ag}}$ and $\Phi_{i}^{\text{sp}}$ for each $i$-th anchor with task-agnostic GHP $P^{\text{ag}}_i$ and task-specific GHP $P^{\text{sp}}_i$ are formulated as follows, respectively:
\begin{equation}
\label{eq_contrastive_loss_global_task_agnostic}
\begin{split}
    \Phi_{i}^{\text{ag}} &= L^{\text{cont}}(z_i, P^{\text{ag}}_i, N^{\text{ag}}_{i})
    \\
    \Phi_{i}^{\text{sp}} &= L^{\text{cont}}(z_i, P^{\text{sp}}_i, N^{\text{sp}}_{i}).
\end{split}
\end{equation}
% In this way, the two objective functions are constructed under task-agnostic and task-specific GHP samples, respectively.

\subsection{Gradient Propagation to Local Hidden Positives}
\label{sec_loss_propagation}
Besides considering the semantically analogous features globally, it is a common hypothesis that nearby pixels are highly likely to belong to the same semantic class.
% This is because the segmentation task is to classify pixels that are in much smaller scale than the objects.
To this end, we consider the property of locality by propagating the loss gradient to the surrounding features of the anchor.
Still, the propagation should be cautiously designed since semantic labels of the adjacent patches are not given; semantic consistency between adjacent patches mostly holds, but sometimes does not (i.e., at object boundaries).
Thus, to decide the semantically consistent patches nearby, we utilize the attention scores from the unsupervised pretrained ViT backbone $\mathcal{F}$.



% While learning from contrastive loss in Eq.~\ref{eq_contrastive_loss_global_overall} can be useful in forming patch-wise semantic-preserving clusters, there is a lack of explicit design considering locality.
% In other words, we should consider the property of locality which is an inherent premise that nearby patches are highly likely to have analogous semantics.

% To this end, we propagate the loss gradient to the surrounding patches in proportion to the corresponding attention score.
% However, there should be no room for the possibility that the adjacent features with definitely different semantics have the same objective.
% In other words, it can be harmful to have the same objective even if the corresponding attention score is very low.
% Therefore, we neglect the harmful features having lower attention scores than the appropriate border from the periphery.

% Given $z_{i}$, we first define a set of surrounding vectors $G_{i}$ as follows:
% \begin{equation}
%     G_{i}=\{g_{i}^{u}\}_{u=1}^{U}\cup \{z_{i}\}.
% \end{equation}

% we select the spatially adjacent features defined 
In detail, we first define the index set $I_i^\text{surr}$ for surrounding patches of the $i$-th anchor including $i$-th anchor itself. 
Also, given the spatial attention score for $i$-th anchor ${\tilde{T}}_{i} \in \mathbb{R}^{H\cdot W}$ from the last self-attention layer in the backbone $\mathcal{F}$, we use the average value of $\tilde{T}_{i}$ as a threshold to select an index set of LHP $I^{\text{local}}_i$ among $I^{\text{surr}}_i$:
\begin{equation}
    \label{eq_local_positive_index}
    I^{\text{local}}_i = \{j \mid j \in I^{\text{surr}}_i \;\land\; t_{j} > \text{Avg}(\tilde{T}_{i})\},
\end{equation}
where $t_{j}$ is $j$-th element of $\tilde{T}_i$ and Avg($\cdot$) denotes the averaging function. 
The visualization of the attention scores for the adjacent patches is shown in Fig.~\ref{fig_motivation}~(b). 
% We notice that the average of the spatial attention score Avg($\tilde{T}_i$) is appropriate to threshold the LHP.
% In detail, given the spatial attention score ${\tilde{T}}_{i} \in \mathbb{R}^{H \times W}$ from $\mathcal{F}$, for $i$-th anchor, we select the spatially adjacent features to compute the semantic similarity score as: 
% \begin{equation}
%     T_{i}=\{{t}_{j} \in \tilde{T}_i \mid j \in I^{\text{surr}}_i \},
% \end{equation}
% where $I^{\text{surr}}_i$ is an index set for surrounding patches of the $i$-th anchor including $i$-th anchor itself.
% We also use the average value of $\tilde{T}_{i}$ as a threshold to select an index set of local positive $I^{\text{local}}_i$ among $I^{\text{surr}}_i$ as follows:
% \begin{equation}
%     \label{eq_local_positive_index}
%     I^{\text{local}}_i = \{j \mid j \in I^{\text{surr}}_i \;\land\; t_j > \text{Avg}(\tilde{T}_{i})\},
% \end{equation}
% where Avg($\cdot$) denotes the averaging function.
% \textcolor{red}{
% % Formally, by utilizing this, we select the spatially adjacent features $T_{i}$ for $I_i^{\text{local}}$ to compute the semantic similarity score as:
% Formally, by utilizing this, we select the attention score of the similar adjacent features $T_{i}$ as follows:}
% \begin{equation}
%     T_i := \{t_j \mid j \in I^{\text{local}}_i\}.
% \end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
By utilizing $I^{\text{local}}_i$, we obtain the surrounding positive features $G_i$ and the corresponding attention score $T_{i}$ for LHP as follows:
\begin{equation}
\label{eq_g_and_t}
\begin{split}
    G_{i} &=\{s_{j} \mid j \in I^{\text{local}}_i \}
    \\
    T_i &= \{t_j \mid j \in I^{\text{local}}_i\},
\end{split}
\end{equation}
where $s_j$ is the feature of $j$-th patch extracted by the segmentation head $\mathcal{S}$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% In addition to the attention score corresponding to $I^{\text{local}}_i$, we define the LHP set $G_i$ for $i$-th anchor as follows: 
% % Given $z_{i}$, we first define a set of spatially surrounding vectors $G_{i}$ as follows:
% \begin{equation}
%     G_{i}=\{s_{j} \mid j \in I^{\text{local}}_i  \},
% \end{equation}
% where $s$ is the feature which is extracted by the segmentation head $\mathcal{S}$.
% where $U_i$ is an index set for adjacent patches spatially surrounding the anchor $z_i$ and also includes $z_i$ itself.

% % In addition, for each anchor, by defining $\bar{T}_{i}$ as a set of attention score of all the H$\cdot$W patch features, we initialize the surrounding attention score $T_{i}$ corresponding to the surrounding vectors $G_{i}$ as follows:
% % \begin{equation}
% %     T_{i}=\{t_{i}^u\}^{U+1}_{u=1},
% % \end{equation}
% % where $U$ is the number of surrounding vectors.
% % Then, we filter out the harmful vectors by making the corresponding attention score zero, as follows:
% % \begin{equation}
% %     T_i := \{t_i^u \mid t_{i}^u<\text{Avg}(\text{Softmax}(\bar{T}_{i}))\}_{u=1}^{U+1}
% % \end{equation}
% % where Avg($\cdot$) indicates the function that averaging the global attention score.
% In addition, for each anchor, by defining $\bar{T}_{i}$ as a set of attention score for $H\cdot W$ patch features, we initialize the surrounding attention score $T_{i}$ corresponding to the local hidden positive set $G_{i}$ as follows:
% \begin{equation}
%     T_{i}=\{t_{i}^u\}^{U+1}_{u=1},
% \end{equation}
% where $U$ is the number of surrounding vectors.
% \begin{equation}
%     T_{i}=\{t_{j} \mid j \in U_i \}.
% \end{equation}
% Then, we filter out the harmful vectors by making the corresponding attention score zero, as follows:
% \begin{equation}
%     T_i := \{t_i^u \mid t_{i}^u<\text{Avg}(\text{Softmax}(\bar{T}_{i}))\}_{u=1}^{U+1}
% \end{equation}
% where Avg($\cdot$) indicates the function that averaging the global attention score.
% Then, we filter out the harmful vectors by making the corresponding attention score zero, as follows:
% \begin{equation}
%     T_i := \{t_j \mid j \in U_i \;\land\; t_j <\text{Avg}(\text{Softmax}(\bar{T}_{i}))\}
% \end{equation}
% where Avg($\cdot$) indicates the function that averaging the global attention score.

To propagate the loss gradient to the LHP, we mix the patch features in $G_i$ in proportion to the corresponding attention scores in $T_i$.
Formally, the mixed patch composed of LHP $s^{\text{mix}}_{i}$ is expressed as:
% To propagate the loss gradient to the spatially surrounding patches in proportional to the individual attention score, we induce them to perform the identical objective function with $i$-th anchor.
% , with the local hidden positive set $G_i$ and the corresponding attention score $T_i$,
% To this end, under the local hidden positive set $G_i$ and the corresponding attention score $T_i$, we obtain mixed feature $s^{\text{mix}}_{i}$ that formed by attentional average of the local hidden positives calculated as:
% \begin{equation}
%     v_{i} = \frac{1}{U+1} \sum_{u=1}^{U+1} g_{i}^u t_{i}^u.
% \end{equation}
\begin{equation}
    s^{\text{mix}}_{i} = \frac{1}{|I_{i}^{\text{local}}|} \sum_{j \in I_{i}^{\text{local}}} \sigma g_{j}t_{j},
\end{equation}
where $\sigma$ is a scalar value to scale the attention score, and $g_j$ and $t_j$ indicate the $j$-th element of $G_i$ and $T_i$, respectively.
% where the scalar $\sigma$ is a factor to harmonize the scale of $s_i^{\text{mix}}$ to $s_i$ while $g_j$ and $t_j$ indicate each element of $G_i$ and $T_i$, respectively.
% We thus produce the projected vector $z_i^{\text{mix}}$ followed by the projection model $\mathcal{Z}$.
% Consequently, the objective function to retain locality is formulated as follows:
% \begin{equation}
% \label{eq_contrastive_loss_local_tas_agnostic}
%     \Psi_{i}^{\text{pre}} = \frac{-1}{|P_{i}^{\text{pre}}|} \sum_{p \in P_{i}^{\text{pre}}} \text{log} \frac{\text{exp}(\text{sim}(z^{\text{mix}}_{i},z_p) / \tau)}{\sum_{a \in A_{i}^{\text{pre}}} \text{exp}(\text{sim}(z^{\text{mix}}_{i},z_a) / \tau)},
% \end{equation}
% where $z_{i}$ in Eq.~\ref{eq_contrastive_loss_global_task_agnostic} is simply substituted with the mixed vector $z^{\text{mix}}_{i}$ where $z^{\text{mix}}_{i}$ is the output of the projection head $\mathcal{Z}$, given $s_i^{\text{mix}}$.
Then, we define the objective functions $\Psi_{i}^{\text{ag}}$ and $\Psi_{i}^{\text{sp}}$ to learn the locality by inserting the projected mixed vector $z^{\text{mix}}_i = \mathcal{Z}(s_i^{\text{mix}})$ into Eq.~\ref{eq_contrastive_loss} as follows:
% Then, a mixed vector $z^{\text{mix}}_i$ for an $i$-th patch can be gained by putting $s_i^{\text{mix}}$ to the projection head $\mathcal{Z}$.
% Consequently, the objective functions $\Psi_{i}^{\text{ag}}$ and $\Psi_{i}^{\text{sp}}$, which retain locality, are defined as follows, individually:
\begin{equation}
\label{eq_contrastive_loss_local_tas_agnostic}
\begin{split}
    \Psi_{i}^{\text{ag}} &= L^{\text{cont}}( z^{\text{mix}}_i, P^{\text{ag}}_i, N^{\text{ag}}_i )
    \\
    \Psi_{i}^{\text{sp}} &= L^{\text{cont}}( z^{\text{mix}}_i, P^{\text{sp}}_i, N^{\text{sp}}_i ).
\end{split}
\end{equation}
% Likewise, $\Psi_i^{seg}$ can also be formulated by replacing $P_i^{\text{pre}}$ and $N_i^{\text{pre}}$ of Eq.~\ref{eq_contrastive_loss_local_tas_agnostic} with $P_i^{\text{seg}}$ and $N_i^{\text{seg}}$, respectively.
% Moreover, additional objective function $\Psi_i^{seg}$ is also formulated by replacing $P_i^{\text{pre}}$ and $A_i^{\text{pre}}$ of Eq.~\ref{eq_contrastive_loss_local_tas_agnostic} with $P_i^{\text{seg}}$ and $A_i^{\text{seg}}$, individually.
% Note that it allows the semantically-alike surrounding vectors $g_i$ to retain ...............
% Note that it allows the semantically-alike surrounding vectors $g_i$ to retain semantic consistency since the same objective loss is propagated to $g_i$ in proportion to each attention score $t_i$, as described in Fig.~\ref{gradient_propagation_v3}.
Since these functions are calculated by utilizing the mixed vector $z^\text{mix}_i$, the loss gradients are propagated to all features composing the $G_i$, as described in Fig.~\ref{fig_gradient_propagation}.
Therefore, the semantically-alike surrounding vectors in $G_i$ are updated in the same direction, thereby retaining semantic consistency within the neighboring patches.
% Moreover, additional objective function $\Psi_i^{seg}$, considering task-specific global hidden positive set $P_i^{\text{seg}}$, is also formulated by replacing $P_i^{\text{pre}}$ and $A_i^{\text{pre}}$ of Eq.~\ref{eq_contrastive_loss_local_tas_agnostic} with $P_i^{\text{seg}}$ and $A_i^{\text{seg}}$, individually.

Overall, by combining all loss formulations with consistency regularizer $R_i$ that minimizes Euclidean distance between the projected vectors of two differently augmented patches, the final loss function is defined as follows:
% As a result, by combining the consistency regularizer $R_i=|z_i-z_j|$ where $j$ denotes the index of augmented patch of $i$-th one, the overall object function is defined as follows:
\begin{equation}
\label{eq_contrastive_loss_local_overall}
    L_i = (\Phi_i^{\text{ag}} + \Psi_i^{\text{ag}}) + 
    \lambda(\Phi_i^{\text{sp}}+\Psi_i^{\text{sp}})+\alpha R_i,
\end{equation}
where $\lambda$ and $\alpha$ control the contribution of each loss. For instance, $\lambda$ gradually increases from $0$ to $1$ during the training and $\alpha$ remains constant at $0.05$ throughout the training.
Note that, the sample with zero positive is excluded from training although it rarely exists.


% 3.4 WJ 방식으로 고치던거 주석.
% \subsection{Gradient propagation to local hidden positive}
% \label{sec_local_positive}
% Besides considering the semantically analogous features globally, it is a common hypothesis that nearby pixels retain similar semantics in the segmentation tasks.
% This is because the segmentation task is to classify pixels that are in much smaller scale than the objects.

% To this end, we implement the property of locality by propagating the loss gradient to the surrounding features of the anchor.
% Still, the propagation should be cautiously designed since semantic labels of the adjacent patches are not given; semantic consistency may often exist in every direction but sometimes very few, in the case of the borderlines of the object.
% Thus, to decide the semantically consistent patches nearby, we utilize the attention scores from the pretrained transformer backbone $\mathcal{F}$.
% In detail, given the spatial attention score $\bar{T}_{i} \in \mathbb{R}^{H \times C}$ from $\mathcal{F}$ for $i$-th anchor, we select the spatially adjacent features to compute the locally consistency score as: 
% \begin{equation}
%     T_{i}=\{\bar{T}_{(k, l)} | k \in \{x-1, x, x+1\}, l \in \{y-1, y, y+1\}\},
% \end{equation}
% where $x$ and $y$ are the coordinate of the anchor, and $\bar{T}_{(k, l)}$ denotes the $(k, l)$-th coordinate of $\bar{T}$.
% We also use the average value of $T_{i}$ to threshold whether the adjacent feature is a local positive or not as:
% \begin{equation}
%     T_i = \{t_u \mid t_u < \text{Avg}(\text{Softmax}(\bar{T}_{i})), t_u \in \{T_{i}\}\}
% \end{equation}
