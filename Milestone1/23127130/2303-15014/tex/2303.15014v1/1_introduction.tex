% \section{Introduction}
% % 시작
% Deep convolutional neural networks have achieved significant advances in various computer vision tasks.
% Out of the lot, semantic segmentation is a major task for scene understanding and plays a crucial role in many applications such as medical imaging or autonomous driving~\textcolor{red}{cite}.
% However, existing supervised approaches demand large scale pixel-level annotated data which require huge labeling cost.
% Such limitation have triggered the advent of weakly-supervised~\textcolor{red}{cite} and unsupervised semantic segmentation~\textcolor{red}{cite} which are to learn without the pixel-level annotations.

% Particularly, unsupervised semantic segmentation is one of the most challenging tasks, since they need to capture pixel-level semantics from unlabeled data.
% In this context, clustering-based approaches have been proposed to learn how to form semantic-preserving clusters from pixel-level features~\cite{iic, picie}.
% % In this context, clustering based approaches have been proposed for learning to form the semantic clusters in pixel-level~\cite{iic, picie}.
% % , through keeping augmentation invariance.
% % leading the model to extract pixel-level object information indirectly by learning semantic consistency.
% % As the model trained in these manners become robust in slight difference, it is likely to form clusters among the same classes which have relatively small differences compared to other classes.
% These methods employ data augmentation techniques to generate the positive pairs to be assigned in the identical cluster, following the practices of contrastive learning~\cite{simclr, moco, swav, byol, pirl, simsiam}.
% Yet, although these intuitive applications of augmentation are successful for visual representation learning with the applicability to various downstream tasks, they may not fit for unsupervised segmentation where the learned features should be specialized to form the desired semantic-preserving clusters without additional guidance~\cite{autoregressive, transfgu}.
% Furthermore, the quality of clusters can be degraded as the degree of accordance between the low-level feature through data augmentation and semantic variations in a specific dataset grows.
% % Likewise, the quality of clusters highly depends on the degree of accordance, which occurs between the low-level feature through data augmentation and semantic variations in a specific dataset.

% % These methods utilizes the augmented data as a single positive sample for each anchor to attract, following the intuition of conventional unsupervised contrastive learning~\cite{simclr, moco, swav, byol, pirl, simsiam}.
% % Although the contrastive learning attracting a single positive can be beneficial to learn general representation, it may fail to form a semantic-preserving cluster.
% % This is due to task-irrelevance, in which differences between real classes may not verge on the slight differences in low-level feature space arising from image-level data augmentation~\cite{autoregressive, transfgu}.
% % and even the existing methods are sensitive to the choice of data augmentations.
% % Therefore, it is necessary to have an positive interrelation with samples having more semantic information.

% % To overcome the task-irrelevance of existing methods, recently, STEGO~\cite{stego} proposed a distillation strategy that learn the interrelation of samples to capture semantic similarities.
% To overcome these issues of existing methods, STEGO~\cite{stego} proposed a distillation strategy to learn semantic similarities between patch-level features.
% % Specifically, they induce individual patch features to form semantic-preserving clusters with the help of an unsupervised pre-trained vision transformer.~\cite{dino}.
% Specifically, they train a segmentation network employing the patch features from the vision transformer~\cite{vit} to imitate the behavior of the unsupervised pre-trained network~\cite{dino}.
% % form semantic-preserving clusters with the help of an unsupervised pre-trained vision transformer.~\cite{dino}.
% % However, pixel-wise~(or patch-wise) independent learning strategies of previous studies are not suitable for scene understanding since there is an inherent premise in image that adjacent pixels are likely to have analogous semantics.
% % In other words, local resemblance within the image has been ignored but should be considered.
% Still, the common hypothesis of locality for scene understanding that adjacent pixel~(or patch) features are likely to semantically similar has not been considered; previous works primarily made predictions without any dependency on nearby pixel~(or patch).
% % Therefore, local resemblance within an image that has been ignored so far should be considered.


% % To this end, we propose pseudo positive based contrastive learning technique without data augmentation to form semantic clusters considering the instance variance.
% In this paper, we propose a locality-aware contrastive learning scheme based on a positive sampling strategy to remedy task-irrelevance and locality-neglection of previous methods.
% % form semantic-preserving clusters from patch-level features
% As aforementioned, we argue that it is more beneficial to gather more samples having relevant semantic information as positive set for attraction, rather than using a single positive.
% To gather positive samples for each anchor~(i.e., each pixel or patch) feature, we create a data-driven criterion considering the distribution of features which vary depending on the dataset.
% % To define the pseudo positive set, we elect the instances within the mini-batch on a different basis for each anchor, considering that the distribution varies depending on circumstances~(e.g., domains of the dataset, applications to apply).
% % Specifically, we design a memory module that stores the candidate features to be adaptively elected as the criterion for each anchor.
% Specifically, we design a memory module to store features that are used as the criterion to threshold whether to be positive of other pixel~(patch) features for each anchor.
% Furthermore, based on the inherent premise that adjacent pixels in the image are likely to semantically similar, we propagate the loss gradient to surrounding pixels in proportion to the attention score of vision transformer.
% % the patch periphery is likely to be associated with the patch, we group the surrounding features with the center feature by computing weighted mean to apply the contrastive loss.
% % In this manner, the grouped features learn proportionally to each weight toward the same objective.
% % Note that, we adjust the weights using the attention score of the vision transformer.
% % since the adjacent features need to be properly distinguished based on semantic information.
% Overall, our proposed contrastive learning based on pseudo positive enables the features to form a semantic-preserving cluster and keeps local resemblance to tackle unsupervised semantic segmentation.

% Our main contributions are summarized as follows:
% \begin{itemize}
% % We present a novel unsupervised semantic segmentation method named [OOO], which learns locality-preserving semantic features by leveraging unsupervised contrastive loss with pseudo positives on grouped features.
% \item We present a novel unsupervised semantic segmentation method named [OOO], which learns locality-preserving semantic features by leveraging unsupervised contrastive loss with pseudo positives on grouped features.
% % 우리는 unsupervised setting에서 Contrastive learning을 위한 Pseudo positive를 구하는 novel한 방법을 제시한다.
% \item We present a novel method to obtain pseudo-positive for contrastive learning in unsupervised segmentation.
% % To learn a locality which should be considered on semantic segmentation task, 인접 픽셀과의 연관성이 높은 경우 동일한 objective를 가지게 하기 위한 grouping 방법을 설계하였다.
% \item To learn locality which should be consigned on semantic segmentation task, we propose the weighted loss replication method to assure the property of the locality.
% \item Our approach is validated to outperform existing
% state-of-the-art methods through extensive experiments.
% \end{itemize}






















\section{Introduction}
\label{sec_introduction}

% \footnote{Corresponding author}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{fig/motivation_v9.pdf}
    \caption{
    Assuming a mini-batch comprising two images shown in (a), we describe two types of hidden positives, to leverage for contrastive learning.
    \textbf{(a)}~With two types of hidden positives introduced in (b) and (c), we provide an example of how our training scheme provides more precise and consistent semantics.
    \textbf{(b)}~(top)~Semantically analogous patches throughout the mini-batch are selected as global hidden positives. 
    (bottom)~Data-driven criterion per anchor is designed for reliable positive collection.
    % \textcolor{green}{
    % (bottom)~Data-driven criterion, which provides a different threshold per anchor, is designed for the reliable positive collection.
    % }
    With the criterion, selected positives are illustrated in (b~top).
    \textbf{(c)}~(top)~We define local hidden positives for each anchor to be the adjacent patches with high semantic consistency, i.e., blue boxes.
    (bottom)~Average attention scores for adjacent patches from the pretrained transformer architecture. 
    The blue line represents the attention score for local hidden positives while the red line for patches neighboring anchors but having low semantic consistency.
    % With two hidden positives, we provide an example in (a) how our training scheme provides more precise and consistent semantics.
    % (b) (top)~patches possessing analogous semantics within the mini-batch are selected as global hidden positives. (down)~An appropriate data-driven criterion makes the global hidden positive set include reliable patches with analogous semantics.
    % (c) (upper)~For each anchor patch, adjacent patches having the same semantics, i.e., blue boxes should be included in the local hidden positive set.
    % (below)~Average attention score for adjacent patches. Blue line is for adjacent features with the same class as the anchor, and red line is for difference classes.
    }
    \label{fig_motivation}
\end{figure}

% 시작
% Deep convolutional neural networks have achieved significant advances in various computer vision tasks.
Semantic segmentation is a major task for scene understanding and plays a crucial role in many applications including medical imaging and autonomous driving~\cite{unet, deeplab, danet, setr, segformer, ocr}.
However, existing supervised approaches demand large-scale pixel-level annotations which require huge labeling costs.
It has triggered the advent of weakly-supervised~\cite{ficklenet, seam, nsrom, mctformer} and unsupervised semantic segmentation~\cite{iic, picie, transfgu, stego} which are to learn without expensive pixel-level annotations.

Particularly, unsupervised semantic segmentation is one of the most challenging tasks, since it needs to capture pixel-level semantics from unlabeled data.
In this context, clustering-based approaches have been proposed to learn semantic-preserving clusters by attracting the augmented views in the pixel-level~\cite{iic, picie}.
They implemented the intuition of contrastive learning~\cite{simclr, moco, swav, byol, pirl, simsiam} by ensuring the augmented pairs yield symmetric cluster assignments.
More recently, as discovering pixel-level semantics from scratch is challenging, STEGO~\cite{stego} broke down the problem into learning the representation and learning the segmentation head.
With the learned patch-level representation from the seminal work in unsupervised learning~\cite{dino, selfpatch}, they train the segmentation head with a distillation strategy.
% However, we point out their limitations in that they solely rely on a fixed backbone which is not trained for the segmentation task, and overlook the semantic consistency along the adjacency that could be a crucial clue in the segmentation.
Although they have made great advancements, we point out their limitations in that they rely solely on a fixed backbone that is not specifically trained for the segmentation task and overlook the importance of semantic consistency along the adjacency that could be a crucial clue for segmentation.
% Although it achieved great improvements over prior techniques, it still heavily relies on a fixed backbone which is not trained for the segmentation task and overlooks the semantic consistency along the adjacency that could be a crucial clue in the segmentation.
% As a result, they solely rely on a fixed backbone that is not trained for the segmentation task and overlook the semantic consistency along the adjacency that could be a crucial clue in the segmentation.
% As a result, although STEGO achieved great improvements over prior techniques, they solely rely on a fixed backbone which is not trained for the segmentation task and overlooks the semantic consistency along the adjacency that could be a crucial clue in the segmentation.
% Although it is effective to divide-and-conquer the problem, their limitations lie in heavy reliance on the quality of backbone and even overlooking the semantic consistency along the adjacency.
% In detail, the training signal is solely provided by the fixed backbone and equally distributed on all patches.
% V2: 
% Although STEGO showed significant performance, still, there are two improvements.
% First, pretrained backbones are not as specialized in semantic segmentation as a segmentation head.
% Second, it overlooks that the surrounding pixels of each anchor are likely to belong to the same category as the anchor.
% % Although STEGO significantly developed prior works, still, there are two improvements.
% % First, the pretrained feature extractor is not specialized for semantic segmentation while it already was semantically consistent.
% % Second, it does not consider the characteristics of semantic segmentation that the surrounding pixels of one anchor likely belong to the same category as the anchor.
% }
% Stego가 기존의 방법들을 상당히 발전시켰지만, 여전히 두 가지 개선사항이 있다.
% 먼저, feature extractor가 semantic consistent 하지만 semantic segmentation에 특화되지는 않았다.
% 나머지 하나는, 한 앵커의 주변 픽셀들이 앵커와 같은 카테고리에 속할 가능서잉 높다는 semantic segmentation의 특징을 고려하지 않았다라는 점이다.

% % ~ 하기 위해 ~ 제안한다
% % 구체적으로~~ (전체적)
% % first, psudo pos
% % second, locality
% Therefore, we leverage contrastive learning to excavate the external positives while also considering the inner locality.
% % Therefore, we propose a comprehensive locality-aware contrastive learning to discover the semantic clusters and ensure the semantic locality explicitly.
% Specifically, with the help of objective features from unsupervised pretrained models~\cite{dino, selfpatch}, we elaborately excavate global positives for contrastive learning and propagate the calculated gradient to the local positives. 
% % learn to preserve locality in semantics through gradient propagation scheme.
% % First, we introduce pseudo positive selection strategy by storing a few amount of reference points in the pool to discover possible positives. 
% First, global positive selection is implemented with the data reference pool that is to measure how the patch features throughout the mini-batch are similar to one another.
% In this work, the initial data reference pool is composed of features extracted by the unsupervised pretrained backbone for fairness over the classes.
% Then, along with the training, final reference pool is constructed by integrating the another set of reference pool that is iteratively updated with the features from the segmentation head.
% % Also, along with the original reference pool, another set of task-specific reference pool is constructed and updated as the segmentation head is trained.
% % The reference points in the pool are selectively used as the criterion to threshold whether to define individual sample as a positive.
% % Together with the pseudo positive set built upon the features from pretrained model, we configure additional pseudo positive set using task-specific features from segmentation model, where the influence of the task-specific positive set gradually increases throughout the training.
% Second, to implement the property of locality and prevent the semantics to pitch, we propagate the loss gradient to adjacent patches in proportion to the similarity score from the pretrained backbone.
% This enables the model to learn the relevance of local context that nearby patches often belong to the same instance.
% Therefore, we leverage contrastive learning to make use of mined hidden positives between the analogous semantics and ensure contextual consistency is preserved along the nearby patches.
% In this paper, 
To take these into consideration, we leverage contrastive learning based on the mined hidden positives to ensure contextual consistency along the patches with analogous semantics, particularly the nearby patches, as described in Fig.~\ref{fig_motivation}.
Specifically, we elaborately select the pseudo-positive samples~(i.e., global hidden positive, GHP) for contrastive learning to learn semantic consistency.
Also, to ensure local consistency, we propagate the loss gradient to the adjacent patches~(i.e., local hidden positive, LHP) in proportion to their equivalency.
First, the GHP selection process is designed with two types of data reference pools, task-agnostic and task-specific, to collect the semantically consistent patch features throughout the mini-batch per anchor.
For instance, the task-agnostic data reference pool is composed of features extracted by the unsupervised pretrained backbone.
On the other hand, the task-specific reference pool is constructed with the features from the segmentation head-in-training to complement task relevance.
Based on the two reference pools, two sets of GHP are selected each with generalized and task-specific perspectives.
Second, to implement the property of locality and prevent the semantics from fluctuating, we propagate the loss gradient to adjacent patches~(i.e., LHP) in proportion to the similarity scores built within the pretrained backbone.
This enables the model to learn the relevance of the local context that nearby patches often belong to the same instance.
% To mine positives for contrastive learning, we construct two kinds of reference pools: generalized reference pool and task-specific reference pool.
% The generalized reference pool is composed of features extracted by an unsupervised pretrained backbone.
% Although the features well maintain class-specific consistency, they are not specialized for unsupervised semantic segmentation.
% To complement this, the task-specific reference pool comprises features from a segmentation head.
% Based on the two reference pools, we elaborately select the global positives which well represent a generalized and a task-specific perspective.
% On the other hand, to implement the locality and prevent the semantics from pitching, we propagate the loss gradient to adjacent patches, which are expected to belong to the same class by similarity score.
% This enables the model to learn the consistency of the local context that nearby patches are often semantically similar.
% Second, to implement the property of locality and prevent the semantics to pitch, we propagate the loss gradient to adjacent patches in proportion to the similarity score from the pretrained backbone.

Our main contributions are summarized as:
\begin{itemize}
% We present a novel unsupervised semantic segmentation method named [OOO], which learns locality-preserving semantic features by leveraging unsupervised contrastive loss with pseudo positives on grouped features.
% \item{We propose strategies to collect semantically similar pairs to be leveraged as positive pairs for contrastive learning. Task-agnostic one guides the initial training and the task-specific one finetunes the model.}
% \item{We utilize contrastive learning to ensure semantic consistency on selected positives. Adjacent positives are particularly considered with gradient propagation strategy.}
\item{We propose a novel method to discover semantically similar pairs, called global hidden positives, to explicitly learn the semantic relationship among patches for unsupervised semantic segmentation.}
\item{We utilize the task-specific features from a model-in-training and validate the effectiveness of progressive increase of their contribution.}
\item{A gradient propagation to nearby similar patches, local hidden positives, is developed to learn local semantic consistency which is the nature of segmentation.}
% \item \textcolor{blue}{We present a novel approach to leverage contrastive learning for unsupervised semantic segmentation by  which is to  hidden positives scattered both externally and internally.}
% 우리는 unsupervised setting에서 Contrastive learning을 위한 Pseudo positive를 구하는 novel한 방법을 제시한다.
% We define the 
% \item \textcolor{blue}{We define the criterion for extensive external positive collection from multi-level features. }
% \item \textcolor{blue}{We propose the locality-aware optimization scheme, by propagating the gradient to adjacent patches. Still, to retain the differences between the semantics, we dynamically sample local positives in accordance with the similarity score.}
% To learn a locality which should be considered on semantic segmentation task, 인접 픽셀과의 연관성이 높은 경우 동일한 objective를 가지게 하기 위한 grouping 방법을 설계하였다.
% \item To learn locality which should be consigned on semantic segmentation task, we propose the weighted loss replication method to assure the property of the locality.
\item{Our approach outperforms existing state-of-the-art methods across extensive experiments.}
\end{itemize}