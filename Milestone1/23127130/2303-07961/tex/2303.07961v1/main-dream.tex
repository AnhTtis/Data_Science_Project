\documentclass{statsoc}
\usepackage[a4paper]{geometry}
\usepackage{graphicx, array, blindtext}
\usepackage{amsmath}
\usepackage{natbib}
 \usepackage{booktabs}

\usepackage{hyperref}
\usepackage{float}
\usepackage{etoolbox}

\makeatletter
\patchcmd{\@makecaption}
{\parbox}
{\advance\@tempdima-\fontdimen2\font\parbox} % decrease the width!
{}{}
\makeatother

\title[Deep Relational Event Additive Model]{A Deep Relational Event Additive Model for modeling patent citations}
\author[Filippi-Mazzola {\it et al.}]{Edoardo Filippi-Mazzola}
\address{Institute of Computing, Università della Svizzera italiana,
Lugano,
Switzerland.}
\email{edoardo.filippi-mazzola@usi.ch}
\author[Filippi-Mazzola {\it et al.}]{Ernst C. Wit}
\address{Institute of Computing, Università della Svizzera italiana,
Lugano,
Switzerland.}

\begin{document}
\begin{abstract}
The patent citation network is a complex and dynamic system that reflects the diffusion of knowledge and innovation across different fields of technology. With this work, we aim to analyze such citation networks by developing a novel approach that leverages Relational Event Models (REMs) and Machine learning concepts. Overcoming the main limitations of REMs on analyzing large sparse networks, we propose a Deep Relational Event Additive Model (DREAM) that models the relationships between cited and citing patents as events that occur over time, capturing the dynamic nature of the patent citation network. Each predictor in the generative model is assumed to have a non-linear behavior, which has been modeled through a B-spline approach that allowed us to capture such smooth effects. By estimating the model through a stochastic gradient descent approach, we were able to efficiently estimate the parameters of the DREAM and identify the key factors that drive the network dynamics. Additionally, our spline approach allowed us to capture complex relationships between predictors through elaborate interaction effects, leading to a more accurate and comprehensive interpretation of the underlying mechanisms of the patent citation network.
Our analysis revealed several interesting insights, such as the identification of time windows in which citations are more likely to happen and the relevancy of the increasing number of citations received per patent. Overall, our results demonstrate the potential of the DREAM in capturing complex dynamics that arise in a large sparse network, maintaining the features and the interpretability for which REMs are mostly famous.  

\end{abstract}

\keywords{Relational Event Models; Citaiton networks; Large networks, Patent analysis; Deep learning, B-splines}

\section{Introduction}

Analyzing the patent citation network is a crucial aspect of understanding the dynamics of innovation in various industries. Patents are not only a means of protecting intellectual property but also provide valuable information about the state of the art in technology and the evolution of knowledge and innovation over time \citep{jaffe_innovation_2002}. The patent citation network captures the relationships between patents, where each citation represents a connection between two patents, indicating that the citing patent has built upon the knowledge contained in the cited patent \citep{sharma_knowledgeflow_2017}. By analyzing the patent citation network, we can gain insights into the diffusion of knowledge and develop a deeper understanding of the patterns and trends that drive innovation. Furthermore, patents represent a significant investment for many companies and understanding the competitive landscape, and the strengths and weaknesses of competitors' patent portfolios can be essential for making strategic decisions about technology development, licensing, and litigation \citep{lerner_importanceinvestnemtns_1994}. In this regard, analyzing the factors that lead to a patent being cited can provide valuable insights into the underlying mechanisms driving innovation. Additionally, understanding the drivers of patent citation can inform decision-making in a variety of contexts, such as technology development, intellectual property management, and innovation policy \citep{holger_ipmanagment_2003}. However, patent data analysis is a complex and challenging task, requiring advanced techniques and tools for managing and analyzing large and complex datasets. 

Relational Event Models (REMs) \citep{butts_2008,perry_point_2013} have emerged as a powerful tool for modeling complex relational data. Although REMs were first introduced in the social sciences as a way to model the temporal dependencies between actors in social networks, they quickly showed their vast range of applicability. Recent works demonstrate their flexibility in modeling different scenarios like two modes networks \citep{vu_relational_2017}, animal behavioral interactions \citep{tranmer_animal_2015}, and more recently, financial transaction \cite{bianchi_clocks_2022}. In this regard, REMs are a valuable tool for analyzing the citation networks of patents, as they allow us to model the complex relationships between citing and cited patents, identifying the factors that influence the diffusion of knowledge and innovation. However, the practical applicability of REMs is mitigated by their runtime complexity \citep{welles_comunication_2014}, a problem rooted in the partial likelihood on which most REMs are modeled after where event rates are normalized by the series of rates of dyads that could have possibly happen. Although there have been some early attempts to model citation networks through a REM-like approach \citep{vu_dynamic_2011}, only recently \cite{lerner_reliability_2020} firstly tackled this issues by showing the robustness of REMs when controls and cases are sub-sampled through a nested case controll approach \citep{borgan_methods_1995} to compose what \cite{cox_partial_1975} defined as the risk-set. While \cite{vu_relational_2015} firstly reported the computational advantages that come when the risk set is sampled, the tools to apply these models to big data still show practical limitations in memory management and optimization procedures, leaving this to an unresolved issue.

Building on the foundation of REMs, we present in this paper a Deep Relational Event Additive Model (DREAM) as an alternative to model large-scale complex event sequences in a REM-like fashion. DREAM merges concepts from the machine learning and statistical learning field to enlarge the current set of applications in which REM is currently used. This novel modeling technique incorporates the use of B-Splines \citep{Schoenberg1946Contributions, Schoenberg1969interpolation, DeBoor1972} and the Adaptive Moment (ADAM) \citep{kingma_adam_2017} optimizer to improve coefficient estimation and modeling flexibility. In this regard by modeling each predictor through a B-spline, DREAM can capture non-linear relationships between variables, providing more valuable interpretations to time-varying effects while identifying the most influential factors driving patent citation. Meanwhile, the use of the ADAM optimizer allows for faster and more efficient estimation of coefficients, making DREAM an excellent choice for large-scale applications.

For our analysis, we used data obtained from the United States Patent and Trademark Office (USPTO), the federal agency responsible for granting patents and registering trademarks in the United States. The USPTO data set is one of the most comprehensive sources of patent information in the world as it contains precise and information contained in standard digitalized formats on all patents issued in the United States since 1976. While there are limitations to using the USPTO data as a proxy for the world, it is still considered a valuable resource for researchers and a good proxy for global patent activity as well as to understand the patent system and its impact on innovation and technological progress.

Although the DREAM was specifically designed to work with citation networks, we claim that this sophisticated and comprehensive modelling techique could be easily applied to model much more complex event data. Overall, by using the DREAM model, we gained important insights into the dynamics of patent citations while opening the road to further speculations on how the current state of the innovation process.

In this paper, we start by describing the USPTO patent data in section~\ref{patent_citations} on which this analysis is currently based. After developing DREAM's theoretical foundation in section~\ref{dream}, we exploit the different effects that we tested in our model specification followed by a series of comments from the resulting splines in section~\ref{effects}. 

%Indeed, this legal requirement poses the question on what exactly mean to be related to a specific techonoogy on which legal protection is asked. By now, it is clear from previous studies \citep{kuhn_information_2010,kuhn_patent_2020,filippi_similarity_2022} that the patent citaiton process has gone through some fundamental evolution that marked a change in the data generation process. 

\section{Patent citations as event history data}\label{patent_citations}

Patent citation is an essential element of the patent system as it provides a means of demonstrating the novelty, non-obviousness, and importance of an invention. Indeed, a patent citation is crucial for both patent examiners and inventors, as they allow the examiner to evaluate the claims made in the patent application and helps the inventor establish the scope and value of their invention. In this regard, in many jurisdictions as part of a patent deposition, applicants are legally obliged to cite those patents on which the patent builds forth. The triple consisting of the instance of deposition, the citing, and cited patents can be seen as an instance of a relational event. Collections of patent citations constitute a citation network, which is a particular kind of temporal-directed graph, where new actors join the network and bind to existing nodes. In most situations, the citation is due to content similarity or other exogenous drivers. This is in contrast to classic social network architectures, where tie formation is a more endogenous process, based on, e.g., repetition, reciprocity, or triadic effects. 

In large jurisdictions, such as patent citation network consists of millions of time-stamped recorded events. The generative process of the US patent deposition gives important clues for modelling the resulting citation network. When a patent is filed, the owners have a legal requirement to fulfill the duty of disclosure. This consists of providing within the application a list of existing technologies or scientific discoveries that are related or considered to be fundamental for the creation of the patenting invention. Patent office examiners will only grant  the patent if the application meets the uniqueness requirement and if the invention is fully disclosed in the documentation presented. The patent citation process conforms to the specific structure of event history data. The event set consists then as a citation-based relationship established between a specific sender (issued patent that is providing the citation) and a receiver (pre-issued patent that has being cited). 

The patent citation network, however, suffers from several boundary issues, relating to space and time. With regard to space, different jurisdictions have different application processes. Despite the patenting criteria being similar among these, slight differences in the juridical procedures make the data generation process country- or region-specific. A clear example of this is the difference between the citation procedures between the European Patent Office (EPO) and the United States Patent and Trademark Office (USPTO). In the latter, it is the duty of the examiner committee to integrate additional documents and patent citations. EPO examiners, on the other hand, do not include any citations but evaluate if the invention has been properly disclosed by the cited documents. This difference in the network generation process results in USPTO patents citations typically surpassing the EPO patent citation by a large amount, sometimes referred to as a ``patent office bias'' \citep{bacchiocchi_international_2010}. We focus in this study on the USPTO patent citation network. With respect to the time boundary, the electronic recording of patent citations has only started in relatively recently. Although some sporadic efforts have been undertaken to record historic patent citations, this is far from complete.  
We focus our analysis only on those patents issued by the USPTO between 1976 and 2022. The starting year of our observed period coincides with the initialization of the digitization process of US-patents. 

In our analysis we make use of the original USPTO online repository (\url{https://bulkdata.uspto.gov/}). This makes the raw materal of this analysis as much standardized as possible in terms of general information available. Although there are various distributions available of the USPTO data, after careful evaluation we decided to avoid any third party pre-processing. The raw USPTO XML files were processed  in a uniform manner and combined to obtain CSV files through an open-source software available at \url{https://github.com/efm95/patents}.

The resulting USPTO patent citation dataset consists of more than 8 million issued patents that generated 190 million citations. Despite the in-house processing, some data cleaning procedures have been applied according to some specific features of the USPTO patents. First, by focusing our view on patents issued only by the USPTO means losing track of those citations that go to patents outside the US jurisdictions. Secondly, in the same way as \cite{whalen_patent_2020}, we excluded all non-utility patents, such as plant and design patents, as these differ in many structural ways from the utility patents. With these two additional steps, our final dataset consists of around 100 million citations issued by a network of 8.3 million patents.

\section{Deep Relational Event Additive Model - DREAM}\label{dream}

Relational Event Models (REMs) \citep{butts_2008, perry_point_2013} are a class of statistical models used to analyze event sequences and relationships between actors through a series of exogenous and endogenous effects based on the fine-grained event history process. In this section, we will start from to exploit the theoretical foundation of REMs to develop the DREAM for the network of patent citations.

\subsection{Relational Event Model}

The temporal dynamic network of patent citations is represented by a sequence of time-stamped events. Each citation event $e_i$, for $i = 1,\dots,n$, is recorded as the triple $e_i=(s_i,r_i,t_i)$, where $s_i$ is the citing patent (sender), $r_i$ the cited patent (receiver) and $t_i$ the time at  which the event takes place, corresponding to the publication date of the citing patent. Following \cite{perry_point_2013}, we define a counting process for the directed event as 
\begin{equation*}
	N_{r}(t)=\#\{ \text{citations of } r \text{ up to time } t\},
\end{equation*}
with $N_{r}(0)=0$. The counting process $N_{r}(t)$ is then a local submartingale for which it is possible to define a predictable increasing process whose stochastic intensity function $dt\lambda_{r}(t)$ is described as the probability of an citation of  $r$ in $[t,t+d t)$. Given the history of the network $H_{t^-}$ up to time $t$, it is possible to model the intensity function following the proportionality intensity model proposed by \cite{cox_regression_1972}. The intensity function becomes then the prodcut of a baseline hazard $\lambda_0$ and an exponential function of covariates $k$, namely $x_{rk}(t \mid H_{t^-})$ with corresponding parameter $\beta_k$, for $k=1,\dots,q$ , i.e.,

\begin{equation}\label{eq:rem}
	\lambda_{r}(t \mid H_{t^-})=  \lambda_0(t)  e^{\sum_{k=1}^{q} \beta_k  x_{rk}(t \mid H_{t^-})}.
\end{equation}

While events are assumed to be conditionally independent given the network of previous events, the inclusion of covariates in this model specification allows to examine the impact of various drivers related to senders, receivers, or network topological characteristics. 

Given the diffculties that comes with dealing with the full likelihood in \eqref{eq:rem}, it is posisible to estimate the coefficients through a partial likelihood approach \citep{cox_partial_1975}, in which the baseline is treated as a nuisance parameter. The main idea of this approximation is to specify a partial likelihood that depends only on the order in which events occur, not the times at which they occur. In probabilistic terms, this implies conditioning the hazard of an interaction between $s$ and $r$ through concerning time. It is to note that the event time coincides with the arrival time of the sender (the publication date of the citing patent). In other words, it is like the event time is the sender. Thus, by conditionaing on time, we are indeed conditioning also on the sender $s$. The risk-set $R(t \mid H_{t^-})$ associated then with the partial likelhood approach consists then in the set of all potential receivers $r$ that were present in the network at time $t$ and, as a consequence, that could have been cited by the issued patent $s$. We can then define the partial likelihood as 

\begin{equation}\label{eq:pl}
	PL(\beta) = \prod_{i = 1}^n\Bigg( \frac{\exp\{\sum_{k=1}^{q} \beta_k x_{rk}(t \mid H_{t^-})\}}{\sum_{h\in R(t \mid H_{t^-})} \exp\{\sum_{k=1}^{q} \beta_k x_{rk}(t_h \mid H_{t^-})\}} \Bigg).
\end{equation}

In log-form \eqref{eq:pl} assumes a concave behaviour, thus the coefficients can be estimated through maximization via Newtonian approaches. 

\subsection{Case-control sampling of the risk-set and logit approximation}

The practical applicability of REMs is mitigated by runtime complexities derived from the complex computation of the risk-set $R(t| H_{t^-})$ \citep{foucault_welles_dynamic_2014}. The issue is rooted directly in the definition of the partial likelihood in \eqref{eq:pl} which normalizes event rates by all potential receivers that could have been cited at the arrival time of the sender. As already noted by \cite{butts_2008}, with no further adjustemnts, the risk-set grows quadratic with the number of nodes in the network, and existing computational routines in most softwares will not suffice \citep{perry_point_2013}. Indeed, this makes patent analysis a pretty peculiar application in which it is almost impossible to use REMs. 

The solution suggested by \cite{vu_relational_2015} is to reduce computational complexity by applying nested case-control sampling on the risk set\citep{borgan_methods_1995}. The idea is to analyze all the observed events/citations (``cases") and use only a small number of elements that build the risk-set (``controls"). Then, for each event/citation, a random subset of non-cited patents is sampled across the ones that are already present in the network at that time to compose the sampled risk-set. This approach reduces the number of computing resources needed to build the risk set, however it still makes heavy use of computer memory. 

\cite{borgan_methods_1995} provides statistical evidence that maximum partial likelihood estimation with a nested case-control sampled risk-set yields a consistent and asymptotically normal estimator. \cite{lerner_reliability_2020} empirically show that for REMs, reliable asymptotic estimates can be obtained even with only one control per case. In this latter case, the denominator in \eqref{eq:pl} would be just the sum of the rates for current cited patent and a sampled one that has not been cited. Let us then assume that $x_{r^*k}(t| H_{t^-})$ be a covariate for a sampled receiver $r^*$ that has not been cited (i.e., a non-event) for the citaiton event that takes place at $t$, then \eqref{eq:pl} becomes

\begin{equation}\label{eq:logit}
	\tilde{PL}(\beta) = \prod_{i =1}^n \Bigg( \frac{\exp\{\sum_{k=1}^{q} \beta_k [x_{rk}(t\mid H_{t^-})-x_{r^*k}(t\mid H_{t^-}) ]\}}{1+\exp\{\sum_{k=1}^{q} \beta_k [x_{rk}(t\mid H_{t^-})-x_{r^*k}(t\mid H_{t^-})]\}} \Bigg),
\end{equation}

which is equivalent to the likelihood of a conditional logistic regression evaluated only at one stratum, thus a logit function.  From a computational perspective, this translates into estimating a logistic regression with only 1s as a response. Given the concavity provided by the log of logistic approximation in \eqref{eq:logit}. Moreover, this approximation avoids reduces the amount of memory needed to analyze full set of observed citations. 

\subsection{Basis expansion and fitting procedures}

REMs in its essence assumes that the rate of an interaction between a sender $s$ and a receiver $r$ depends linearly with respect to the covariates. Although this may be a straightforward modelling approach, it might bring unwanted biases or oversimplification of complex non-linear phenomenons. Given the logistic regression approximation in \eqref{eq:logit}, we took inspiration from the General Additive (GAM) literature \citep{hastie_gams_1986} by modeling single covariates through Basis functions splines (B-splines) \citep{Schoenberg1946Contributions,Schoenberg1969interpolation}. 

Basis functions are connected piece-wise polynomial functions of order $p$ defined over a grid of knots $u_0,u_1,\dots,u_m$, such that $u_l<u_{l+1}$, for $l=1,\dots,m$, on the parameter space that characterize the covariate $x_{rk}(t \mid H_{t^-})$, for $k=1,\dots,q$. From \cite{DeBoor1972} recursive formulation, we define the $j$-th B-spline basis function , for $j=1,\dots,d$, where $d$ are the number of splines that represent the degrees of freedom of the B-spline transformation for covariate $x_{rk}(t \mid H_{t^-})$, i.e. $B_{j, p}^k(x_{rk})$, which is defined as

\begin{equation}\label{eq:bspline_formula}
B_{j,p}^k(x_{rk}) = \frac{x_{rk}(t \mid H_{t^-})-u_l} {u_{l+1}-u_{l}}  B_{j,p-1}^k(x_{rk})	+ \frac{u_{l+p+1}-x_{rk}(t \mid H_{t^-})}{u_{l+p+1}-u_{l+1}}  B_{j+1,p-1}^k(x_{rk}),
\end{equation}

where 

\begin{equation*}
	B_{j,0}^k (x_{rk})= 
	\begin{cases}
		1 & \text{if } u_l \leq x_{rk}(t \mid H_{t^-}) < u_{l+1}\\
		0 & \text{otherwise}. 
	\end{cases}
\end{equation*}

For our modeling framework, we decided to evenly place the knots such that these are equally distant from each other on the covariate support. The B-spline effect associated to a covariate $x_{rk}(t \mid H_{^-})$, for $k= 1 \dots q$, is then a linear combination of $d$ coefficients with $d$ basis functions, i.e.,

\begin{equation*}
	f_k(x_{rk})=\sum_{j=1}^{d}\theta_{jk} B_{j,p}^k(x_{rk} ).
\end{equation*}

The REM's full model definition becomes
By substituting the basis expansion formulation in the partial likelihood \ref{eq:logit}, the full model becomes 

\begin{equation}\label{eq:rem_gam}
	\lambda_{sr}(t \mid H_{t^-})= \lambda_0(t) e^{\sum_{k=1}^{q} f_k(x_{rk})},
\end{equation}

where its partial likelihood approximation with one control can be approximated as  

\begin{equation}\label{eq:logit_gam}
	\tilde{PL}(\theta) = \prod_{i =1}^n \Bigg[ 1+\exp\bigg\{- \sum_{k=1}^{q}\sum_{j=1}^d \theta_{jk}[B_{j,p}^k(x_{rk})- B_{j,p}^k(x_{r^*k})]\bigg\} \Bigg]^{-1}.
\end{equation}
%\begin{equation}
%	\tilde{PL}(\theta) = \prod_{i =1}^n \Bigg[1+\exp\{- \sum_{j=1}^d \theta_j[B_{j}^p(x_{s,r}(t \mid H_{t^-}))- B_{j}^p(x_{s,r^*}(t \mid H_{t^-}))]\} \Bigg]^{-1}.
%\end{equation}

Although the process of generating basis functions from events and estimating the coefficients can be tackled by well-optimized R algorithms like the \texttt{gam} function in the \texttt{mgcv} package \citep{wood2011mgcv}, it can still be difficult to treat complex situations with many events. Furthermore, the R software memory management system doesn't allow for the proper handling of data with millions of observations resulting in limitations to the practical applicability of such routines for complex and sparse networks. This also complicates the estimates of the coefficients through the optimizers in \texttt{mgcv} as they would still require a considerable amount of time to reach convergence. Eventually, basis expansions requires to store in memory as many matrices of sizes $n \times d$ as covariates we want to fit to the model. If we then model REMs as in \ref{eq:logit_gam}, the memory allocation explodes, as for each case we need to fit a B-spline also for the control, thus the model needs to compute $q \times 2$ matrices. The fitting problem can then be divided into three parts: defining an efficient way to compute the basis function for millions of rows, provide a suitable estimation procedure for the coefficients in the partial likelihood and avoid to generate matrices that explodes in memory.

To tackle the first problem, we created a vectorized recursive algorithm that efficiently generates basis functions from millions of elements in a vector. Regarding the second task, we opted for a stochastic gradient descent approach through the Adaptive Momentum (ADAM) optimizer \citep{kingma_adam_2017} to fit the partial likelihood. In this regard, the ADAM optimizer maintains an exponentially decaying average of past gradients and squared gradients while using these estimates to calculate the update step for the model parameters. By adapting the learning rate based on the historical gradients, the ADAM optimizer can converge faster and more efficiently than the traditional stochastic gradient descent. Additionally, it has built-in mechanisms to handle sparse gradients and correct bias in the estimates, making it a robust optimizer for a wide range of deep-learning tasks.

To estimate the model, we relied the Python suite PyTorch  \citep{pytorch}, which also allows us to access the computational benefits of Graphic Processor Units (GPUs) to scale matrix multiplications and gradient computations. Indeed, the vectorized nature of Pytorch and the use GPUs computational power well fits our recursive algorithm, drastically reducing the computational time for generating B-splines. Then, by dividing the stream of data into different batches, we are able to efficiently estimate the coefficients by iteratively updating them with respect to the backpropagated gradients, computed using the negative log-partial likelihood from \ref{eq:logit_gam} as our loss metric. As a result, the REM is estimated like most common deep learning models with the addition of additive components entangled by B-splines, defining what we call  the Deep Relational Event Additive Model (DREAM).

To takle the issue connected to the memory managment of the procedure, we started from the consideration that dividing the input data into batches is analogous as taking random samples from a population. It is to note then, that the value associated with each observation following the basis function transformation is invariant to an event's position in the observed set. As a result, rather than applying the basis function transformation on the entire observed stream of the event, these can be computed directly on each batch at convenience when the gradient needs to be computed. This reduces memory usage at the expense of a smaller increase in computational costs. Thus, making the DREAM efficiency the preferd choice when dealing with large networks. The code for DREAM can be found at \url{https://github.com/efm95/DREAM}.

\section{Modeling patent citations through DREAM}\label{effects}

To use the DREAM to evaluate patent citations, the key issue we are attempting to answer is what causes a patent to be cited. Investigating such causes in terms of relational events requires assuming that the data creation process is regulated by certain endogenous and exogenous mechanics that build the network of patent citations. Following a preamble that will describe the effects that we examined, we will analyze the fitted models and discuss the results in this section.

\subsection{Effects}

We divided the type of tested statistics into node effects, patent similarity effecs and time-varying effects. 

\paragraph{Node effects.} The nodal effects in the patent citation network refer to specific kinds of information related to the cited patent. With the advent of digitalization in the patenting procedures, the number of issued patents has increased exponentially, leading to a rise in the number of backward citations \citep{kuhn_patent_2020}. Therefore, analyzing the temporal trends of citations can help us identify important periods of technological innovation. In this regard, the use of a non-linear effect on the \emph{cited patent publication year} can uncover any tendencies where patents issued in specific years are being cited more consistently. This can potentially indicate a period of significant technological advancement or the emergence of a new field of research. By understanding these nodal effects and their underlying causes, we can gain a deeper understanding of the factors driving innovation and knowledge diffusion in the scientific community.

In addition to the publication year of the cited patent, the time that has elapsed between the issuance of the citing patent and the publication of the cited patent is also a crucial factor in analyzing the patent citation network. This \emph{time lag} effect can provide insight into the tendency of patents to cite material that is closer in time, reflecting the current state of technological innovation. By counting the number of days between the citing patent issue date and the receiver publication date, we can model this \emph{time lag} effect and account for the time that has passed between the two nodes appearing in the network. 

Not all nodes are equally important in terms of their connectivity. Some nodes have a more central role and consequently draw more citations than others. This centrality can be attributed to the concept that newer patents build on the knowledge contained in older patents, resulting in a cumulative process of knowledge creation. As a result, patents that occupy a more central position in the network are more likely to draw more citations. To capture this phenomenon in our analysis, we use the node's \emph{receiver outdegree} as a proxy for centrality. In this context, the outdegree of a node represents the number of patents that cite the receiver at the time it was issued, indicating the node's influence on the rest of the network. By including this metric in our model specification, we can account for the effect of a patent's centrality on its likelihood of receiving future citations.

\paragraph{Patent similarities effects.} Citations in patents arise from the assumption that there exists some technological similarity between the citing and the cited patent. However, technological relatedness is not a tangible concept and is difficult to capture. Therefore, two different types of relatedness are often used to capture this idea.

The first type is a direct textual similarity between the citing and cited patents. Although there have been debates about the reliability of textual similarity as a measure of technological relatedness, recent studies \citep{kuhn_patent_2020, filippi-mazzola_drivers_2023} have shown that it still plays an important role when combined with other metrics. For example, incorporating a time lag into the analysis can improve the accuracy of the measure. Following the same procedure of \cite{filippi-mazzola_drivers_2023}, we calculate textual similarity through a pre-trained model such as Sentence-BERT \citep{reimers-2019-sentence-bert} using a vectorized loop to compute the pairwise similarity across the abstracts of the citing and cited patents.

The first is a direct textual similarity between the citing and the cited patent. Although there has recently been a discussion on the reliability of textual similarity \citep{kuhn_patent_2020}, \cite{filippi-mazzola_drivers_2023} showed how the application of certain corrections in the model specification such as the time lag makes the textual similarity still a relevant component that needs to consider. Then, following the same procedure of \cite{filippi-mazzola_drivers_2023}, we computed the pairwise textual similarity across cited and citing patent abstracts by the means of a pre-trained Sentence-BERT model \citep{reimers-2019-sentence-bert}.

Another important measure of technological relatedness is the overlap in technology classes between the citing and the cited patents. Patent classification systems, such as the International Patent Classification (IPC) scheme, are designed to facilitate the search for related technologies by organizing patents into systematic hierarchical structures. Deeper levels of classification indicate higher levels of specificity in the technological field. However, analyzing patent classes presents several challenges. Patents often contain multiple technological components from different fields, and as a result, may be allocated to multiple classes. Furthermore, patent classes have been constantly created, merged, and/or split constantly since the creation of the USPTO \citep{younge_patent--patent_2015}. 

Despite these challenges, patent classes remain a crucial element in the patent-issuing process. To ensure that our analysis is not biased by changes in class labels or multiple class assignments, we calculate the \emph{Jaccard index} for the cited and citing patents' IPC codes \citep{yan_measuring_2017}. The Jaccard index measures the similarity of the patent classes between two patents, taking into account only the sub-class levels and ignoring the lowest levels of the hierarchy. In this regard, \cite{filippi-mazzola_drivers_2023} have shown that both the main component (Section) and the third component (sub-class) share similar importance in analyzing patent classes. In our analysis, we incorporate Jaccard indexes only for the sub-class levels.

%When dealing with patent classes, however, several factors must be considered. Indeed, because patents are occasionally affected by several technological components from different fields, they are frequently allocated to multiple classes. Furthermore, patent classes have been constantly created, merged, and/or split constantly since the creation of the USPTO \citep{younge_patent--patent_2015}, potentially creating biases in an analysis. However, despite the controversies surrounding patent classes, they still play a major role in the issuing process of a patent. To prevent our analysis to be biased by changes in the classes as well as in the labeling of patents into multiple classes, we compute the \emph{Jaccard index} on the pair of citing and cited International Patent Class (IPC) \citep{yan_measuring_2017}.  Without considering the lowest part of the hierarchy of the classes which is represented by the groups (main and sub-groups), it is shown how by \cite{filippi-mazzola_drivers_2023} how the main component (Section) and the third component (sub-class) share the same magnitude of importance. While taking the highest level of the IPC would mean taking the broadest class, for our analysis we incorporated the Jaccard indexes only for the sub-class levels. 

\paragraph{Time-varying effects.} To complete our analysis, we include in our model specification two time-varying effects. Firstly, we considered for every node the \emph{cumulative number of citations receiverd} (known in network science as Preferential attachment) to evaluate the patent propensity of being cited in time. However, the rate at which a patent accumulates citations over time can vary based on its relevance or significance to the field. For example, a patent could become known or relevant to the field much later than its publication date. As a consequence, its cumulative process of being cited could have different rates depending on the observed times. For this reason, we also considered the \emph{time from the last event}, i.e. the last time the patent was cited. This variable captures how long it has been since the patent was last cited, and its influence on the likelihood of receiving a new citation.

Including time-varying effects into the model specification presented challenges in terms of computational efficiency. Specifically, updating the risk set for each observed event-time could be computationally intensive, particularly when dealing with millions of events. Rather than uniformly sampling $x_{r^*k}(t \mid H_{t^-})$ from $R(t \mid H_{t^-})$, we selected a subset of candidates from the risk set, denoted by $\tilde{R}(t \mid H_{t^-}) \subseteq R(t \mid H_{t^-})$, such that for each event-time $t$, we sample $c$ potential receiver as candidates. For each candidate in $\tilde{R}(t \mid H_{t^-})$, we update its relative time-varying effect at every observed time $t$ as these would have been the events that occured. By storing these non-receiver candidates in memory, we significantly reduced the computational burden when creating the model matrix. It is important to note that the size of the subset $\tilde{R}(t \mid H_{t^-})$ is determined by $c$, i.e. the number of potential non-event candidates for each observed event-time $t$. Although this approach may not capture all non-events, it provides a more efficient way to handle the time-varying effects in our model. Additionally, it is worth mentioning that the asymptotic property of this approach is guaranteed, as $\tilde{R}(t \mid H_{t^-})$ approaches $R(t \mid H_{t^-})$ as $c$ becomes larger. Overall, incorporating time-varying effects in our model specification improves the accuracy and robustness of our analysis by accounting for the dynamic nature of patent citation behavior over time.

Table \ref{tab:effect_mechanisms} contains an overview of all the effects and their respective mechanisms.

\begin{table}
	\caption{\label{tab:effect_mechanisms}Effects and their corresponding mechanisms. Where $r$ represents the receiver, $s$ represents the sender, and $t$ represents the issue date for either the sender or the receiver.}
	\centering
	\begin{tabular}{*{2}{m{0.49\textwidth}}}
		Effect               & Mechanism                                  \\ \hline
		Receiver publication year&\includegraphics[scale=1]{mec_rec_pub_year.pdf} \\
		\hline
		Time lag &\includegraphics[scale=1]{mec_time_lag.pdf} \\
		\hline
		Receiver outdegree &\includegraphics[scale=1]{mec_receiver_outdegree.pdf} \\ 
		\hline
		Textual similarity & \includegraphics[scale=1]{mec_textual_similarity.pdf} \\ 
		\hline
		IPC relatedness & \includegraphics[scale=1]{mec_ipc_relatedness.pdf}\\ 
		\hline 
		Cumulative citations received & \includegraphics[scale=1]{mec_cumulative_cit_rec.pdf} \\
		\hline
		Time from last event & \includegraphics[scale=1]{mec_time_from_last_event.pdf} \\ 
		\hline
	\end{tabular}
\end{table}

%The inclusion of time-varying effects to the model specification complicates the computation of the risk set because, for each observed event-time, the time-varying statistics for the non-events must be updated as if they had occurred at the event-time. When dealing with millions of events, updating a risk set the same size as the event set would result in a significant computational bottleneck. However, instead of  sampling uniformly $x_{sr^*}(t \mid H_{t^-})$ from $R(t \mid H_{t^-})$, we can define $\tilde{R}(t \mid H_{t^-}) \subseteq R(t \mid H_{t^-}) $ such that for each event-time $t$, we sample $k$ potential receiver as candidates.  Then, for every non-event candidate in $\tilde{R}(t \mid H_{t^-})$, we compute its relative time-varying at every observed time $t$. Thus, by saving these in memory these non-receiver candiates, we drastically reduce the computational complexity when creating the model matrix. The asymptotic property ot this approach is guaranteed by the fact that for big values of $k$, $\tilde{R}(t \mid H_{t^-}) \rightarrow R(t \mid H_{t^-}) $. 

\subsection{Model selection and experimental design}

To determine the model specification, we performed a group step-wise model selection to confirm that all the statistics defined in the model lead to a decrease in the prediction error. Having grouped the statistics into \emph{nodal} (No), \emph{similarity} (Si), and \emph{time-varying} (Tv) effects, we sequentially add those groups of statistics to our model. Table~\ref{tab:var_sel} reports the estimated AIC, BIC, and log-likelihood values for each fitted model. The results highlight the significant contribution that similarity statistics make to the model when they are included, while also confirming that the full model is the one with the least amount of information lost.

\begin{table}
	\caption{\label{tab:var_sel}Variable selection confronting AIC, BIC and log-Likelihood using three different model specifications. Effects have been divided into three groups: \emph{nodal} (No), \emph{similarity} (Si), and \emph{time-varying} (Tv).}
	\centering
	\begin{tabular}{l|c|c|c}
		Effect group  & AIC           & BIC           & log LKL           \\ \hline
		No 					& $101'708'032$ & $101'708'328$ & $-50'854'000$ \\
		No + Si  		 & $22'777'864$  & $22'778'356$  & $-11'388'902$ \\
		No + Si + Tv & $15'697'526$  & $15'698'214$  & $-7'848'721$  
	\end{tabular}
\end{table}

%Two hyperparameters remain to be specified before the model can be evaluated: the first the batches sizes, and the splines degrees of freedom. We used a 6-fold cross-validation approach to test three different batch sizes while changing the degrees of freedom from 4 to 20. Results are displayed in Figure \ref{fig:model_selection}. 

Before the model can be evaluated, two hyperparameters need to be specified: batch sizes and spline's degrees of freedom.  

Batch sizes refer to the number of events used in each batch during the model fitting process. We tested three different batch sizes: $2^{10}$, $2^{14}$ and $2^{18}$. The choice of batch size affects the trade-off between computational efficiency and model accuracy. A small batch size may induce uncertainties connected to the direction in which the gradient points, but it can be computationally efficient. A larger batch size may result in a less accurate model but is computationally more demanding.

The spline degrees of freedom refers to the flexibility of the nonparametric model used to estimate the hazard functions. We tested degrees of freedom ranging from 4 to 20 to find the optimal level of flexibility. A lower degree of freedom may result in a less flexible model that under fits the data, while a higher degree of freedom may result in a model that is too flexible and, potentially, overfits the data.

To determine the best values for these hyperparameters, we used a 6-fold cross-validation approach, where we tested these three different batch sizes while changing the degrees of freedom from 4 to 20.  As a test-error metric, we evaluated the negative log-likelihood for each held-out set in the cross-validation. These values were then re-scaled by the size of the set for comparison purposes. %Given the proximity of the DREAM to the logistic regression, this is equivalent of evaluating the binary cross-entropy on the held-out set. 

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{model_selection.pdf}
	\caption{\label{fig:model_selection}6-fold cross-validation approach used to compare three different batch sizes with different degrees of freedom for the splines ranging from 4 to 20.}
\end{figure}

The results of the hyperparameter tuning process are presented in Figure \ref{fig:model_selection}. This figure shows the average test error for three different batch sizes ($2^{10}$, $2^{14}$, and $2^{18}$) and degrees of freedom ranging from 4 to 20. The results indicate that the average test error is statistically equivalent for the three batch sizes, suggesting that increasing the batch size beyond $2^{10}$ does not improve model performance. However, we observed that increasing the batch size from $2^{10}$ to $2^{14}$ results in a marginal increase in the time-to-converge, indicating that there is a slight increase in computational demand between these two sizes.

Therefore, we conclude that the optimal batch size is $2^{14}$ as it provides the best trade-off between stability and time-to-converge. Furthermore, the plot shows that the test errors stabilize after the 12th degree, suggesting the rising of overfitting patterns. Consequently, we deduce that setting the degree of the splines to 12 provides the best fit for the data.

%From the Machine learning literature, we know that small batch sizes is usually preferred as it would take the DREAM a lower amount of time to reach the convergency. Still, batch sizes that are too small might induce uncertainties connected to the direction in which the gradient points. 

%Figure \ref{fig:model_selection} shows how the average test error terms are statistically equivalent between the batches of sizes $2^{10}$, $2^{14}$ and $2^{18}$, therefore suggesting that there is no room for improvements by increasing the batch size over $2^{10}$. However, during our tests, we noted that by increasing the batch-size from $2^{10}$ to $2^{14}$, the time-to converge shows a marginal increase, prooving that there is little increase in the computaitonal demands from these two sizes. Indeed, we conclude that the more adequate batch-size is the one with the best trade-off in terms of stability and time-to-converge. Moreover, from the plot, it is possible to notice that from the 12-th degree, the test errors stabilize themself, potentially signifying the rising of overfitting patterns. Indeed, from this analysis, we deduce that setting the degree of the splines to 12 provides the best fit for the data. 

%Given the reduced increase in computational time that requires fitting batches of size $2^{14}$, we are confident that the choice of such batch dimension provides the best trade-off in terms of stability and time-to-converge. Moreover, from the plot, it is possible to notice that from the 12-th degree, the test errors stabilize themself, potentially signifying the rising of overfitting patterns. Indeed, from this analysis, we deduce that setting the degree of the splines to 12 provides the best fit for the data. 

%However, it is still possible to notice the slight tendency of the test errors to be marginally higher for the batches of size $2^{10}$. 

\subsection{Model fitting}

The stochastic component of the optimizing method introduces some randomness into the estimation of the model's parameters. The approach is reliable, as it reaches quite rapidly the convergency, but to asses the uncertainty of the estiamted parameters, repetitions of the fitting procedure are need. Most famous software packages that fit splines in their models solve this issue by repeating the model fit through bootstrapped versions of the model matrix \citep{wood2011mgcv}. However, in the context of a temporal relational event network, where each event is either unique or the result of an important endogenous mechanism of the network, having parametric or nonparametric repetitions of the same event could bring to important alterations in the model matrix that could lead to biases in the fitting routine. In the DREAM, we repeat the fitting process multiple times, shuffling the positions of the model matrix such that the procedure starts every time from a different point. To bring more strength to our results, after 50 repetitions, $\tilde{R}(t \mid H_{t^-})$ was resampled. This procedure was repeated 10 times.  Figures \ref{fig:no}, \ref{fig:sim} and \ref{fig:tv} show the fitted splines with 12 degrees of freedom.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{nodal_splines.pdf}
	\caption{\label{fig:no} Splines associated to the \emph{nodal effects}.}
\end{figure}

\paragraph{Node effects.} From the nodal statistics fitted splines in Figure \ref{fig:no}, we can identify particular characteristics that increase the likelihood of a patent being cited. 

%When we look at the \emph{receiver outdegree} effect, we can see how being at the center of the network structure leads to a bigger log-hazard contribution. We can deduce from this curve that patents having a higher outdegree are more likely to be referenced. It is feasible to see that the spline contribution to the log-hazard rate is primarily positive, indicating that the higher the outdegree, the greater the likelihood of being cited.

The \emph{receiver publication year} curve in Figure \ref{fig:no} highlights two distinct peaks in different regions of the parameter space. The first peak occurs shortly after the year 2000, while the second peak is observed between the years 2014 and 2015. These two peaks indicate periods when patents are more likely to be cited. While this study is limited to macro-level network analysis, we can still deduce from this observation that key technical breakthroughs may occurred during these periods.
At the end of the parameter space, there is a downward trend in the log-hazard contribution. This could be due to patents being too young to be cited. This trend is also supported by the confidence bands, which show that the interval widens around the year 2019. Interestingly, patents published at the beginning of the observed period have a negative log-hazard contribution. This could be a countereffect caused by the temporal boundaries of the network. However, it is worth noting that the curve assumes a positive trend only during the year 1990, where the boundaries effect should be less significant. Furthermore, following the findings of \cite{filippi-mazzola_drivers_2023}, the introduction of a time-lag effect could help mitigate these network boundary effects. 
Taken together, these observations provide further evidence of the occurrence of important technological innovations during the peak periods. The \emph{receiver publication year} curve sheds light on the temporal dynamics of the citation process, indicating that certain periods are more conducive to citation. 

The \emph{temporal lag} spline indicates a certain period in which future patents are more likely to reference present ones. The curve shows that there is a peak from the beginning of the curve until day 2500, with a net positive contribution in log-hazard until day 6000, where then the inclination of the curve changes. This indicates the presence of a sweet spot where citations are more likely to arise. Indeed, suggesting that future patents issued within 2500 days (approximately 7 years) from a present one are more likely to reference current patents.
It is important to note that this temporal lag effect could be influenced by various factors such as the pace of technological development, the lifespan of technology, and the overall trends in the field. Indeed, this effect provides valuable insights into the timing of patent publications and their impact on the citation network. By identifying this sweet spot where citations are more likely to arise, inventors can strategically plan their patent filing and publication strategies to increase their chances of being cited and recognized in the field.

% Therefore, it is essential to analyze this effect in conjunction with other factors to gain a comprehensive understanding of the dynamics of the patent citation network. 

%In the \emph{receiver publication year} curve, we can see how two peaks in two different areas of the parameter space are highlighted. The first peak occurs soon after the year 2000, while the second one is present between the years 2014 and 2015. These two sections highlight two times when patents are more likely to be cited. Although this study is restricted to a network analysis from a macro perspective, we can deduce from this phenomenon that relevant key technical breakthroughs occur during such times. Eventually, the downward trend at the end of the parameter space can be the result of patents that are too young to be cited. This also is shown by the confidence intervals, where we can see that the interval opens at around the year 2019. Interestingly, patents published at the beginning of the observed period have a negative log-hazard contribution. Indeed, this may well be a countereffect caused by the temporal boundaries of the network. However, we can see that the curve assumes a positive tone only during 1990, where the boundaries effect should be mitigated. Moreover, following \cite{filippi-mazzola_drivers_2023}, we reckon that the introduction of a time-lag effect should reduce such effects. This brings more evidence to the idea of important technological innovations that happened during the peak periods. 

%The \emph{temporal lag} spline indicates a certain time period in which future patents are more likely to reference present ones. In this sense, there is a peak that occurs from the beginning of the curve until day 2500. This points to a sweet spot where citations are more likely to arise. Consequently, we may deduce that a patent that was published within 2500 days (roughly 7 years) after its publication date is more likely to be cited by an issued sender.

The \emph{receiver outdegree} effect shows how being at the center of the network structure leads to a bigger log-hazard contribution. The curve of the fitted spline suggests that patents with a higher outdegree are more likely to be referenced. The contribution of the spline to the log-hazard rate is primarily positive, indicating that the higher the outdegree, the greater the likelihood of being cited. 
This finding is particularly interesting as it highlights the importance of being well-connected within a network. Being at the center of a network structure allows a patent to be more visible and accessible to other inventors, increasing the likelihood of being cited. This result is consistent with previous research that has emphasized the importance of network position in predicting innovation outcomes \citep{uzzi1997social}. Furthermore, this finding has practical implications for policymakers and inventors who may wish to increase the likelihood of their patents being cited. By fostering collaboration and networking opportunities, inventors can improve their chances of being connected to other inventors and increase their outdegree, thus increasing the visibility of their work.


\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{similarity_splines.pdf}
	\caption{\label{fig:sim}Splines associated to the \emph{similarity effects}.}
\end{figure}

\paragraph{Similarity effects.} The curves for both \emph{textual similarity} and \emph{IPC relatedness} shown in Figure \ref{fig:sim} demonstrate the significance of technological similarity between the citing and cited patents, highlighting how patents that are more closely related are more likely to reference each other. The \emph{textual similarity} curve indicates a stronger tendency towards citing patents that are more similar, as the curve stabilizes after the similarity score of 0.6. The \emph{IPC relatedness} curve, on the other hand, indicates that even patents that share a limited number of technology classes have a higher probability of being cited, with a small peak at 0.8.

Furthermore, the weight placed on the \emph{textual similarity} effect is noteworthy, reaching a peak value of about 1.5 in log-hazard contribution. This suggests that patents with a high degree of similarity in terms of their text content are more likely to be cited. It is important to note that while the \emph{IPC relatedness} effect is not as strong as the \emph{textual similarity} effect, it still carries significant weight in predicting the likelihood of citations. 

These findings emphasize previous studies' results as they prooves that despite the behavioral changese over time in the technological similarity across cited and citing patents \citep{kuhn_patent_2020,whalen_patent_2020}, patents with greater technological similarity are more still more likely to cite each other \citep{jaffe_innovation_2002}. 

%Both the \emph{textual similarity} and the \emph{IPC relatedness} curves in Figure \ref{fig:sim} show the importance of technological similarity between citing and cited materials, highlighting how patents that are more closely related are more likely to reference each other. This tendency is more vivid within the \emph{textual similarity}, as the curve tendency stabilizes after the similarity score of 0.6. The \emph{IPC relatedness} quickly jumps to positive values quite immediately with a small peak at 0.8, suggesting that the probability of being cited arises even if the sender and receiver share only a limited number of technology classes. Also, it is worth noting the high weight placed on \emph{textual similarity} effect, which, at its peak, reach a value of about 1.5 in log-hazard contribution.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{time-varying_splines.pdf}
	\caption{\label{fig:tv}Splines associated with the \emph{time-varying effects}.}
\end{figure}

\paragraph{Time varying effects.} The two time-varying effects in Figure~\ref{fig:tv} demonstrate the dynamic nature of patent citations within the network. The \emph{cumulative citation count} effect reveals how the number of citations a patent receives over time influences its likelihood of being cited in the future. This effect is particularly notable as the log-hazard contribution shows a rapid increase, indicating a positive feedback loop where the more citations a patent receives, the more likely it is to receive additional citations. This snowball effect is a crucial factor in determining the significance of a patent within the network, and it underscores the importance of early recognition and citation of relevant breakthroughs.

On the other hand, the \emph{time from last event} effect highlights the inverse relationship between the time interval from the last citation and the likelihood of receiving subsequent citations. As the time interval grows longer, the probability of receiving additional citations decreases. This effect is shown by the steady decrease in log-hazard contribution and the change in angle of the curve when it becomes negative. This trend underlines the importance of continuous and timely recognition of relevant patents to maintain their significance and relevance within the network.

It is worth noting that these two effects work in together to shape the dynamic nature of patent citations within the network. The snowball effect of the \emph{cumulative citation count} effect can counteract the decay caused by the \emph{time from last event} effect, but only up to a certain point. Eventually, even the most significant patents will fade in relevance if they are not consistently recognized and cited within the network. This underscores the importance of a timely and accurate recognition of relevant patents to drive innovation and technological progress.

%The two time-varying effects in Figure~\ref{fig:tv} highlight the rising importance of patents within the network as the decay of their propensity of being cited. The \emph{cumulative citation count} effect demonstrates how the more citations a patent receives, the more likely it will be mentioned in the future. This snowball effect is shown by the rapid increase in positive log-hazard contributions, showing only a small downward trend in the right-end side of the parameter space. On the other hand, emphasizing the downward trend of the \emph{time from last event} effect, it is clear that the longer the intervals are from one citation to another, the less probable it is that any subsequent citations will occur. Interestingly, the curve shows a steady decrease in its hazard contribution, while changing its angle when it becomes negative, highlighting the decay caused by the lack of citations.

\subsection{Cumulative baseline hazard}

REMs modeling strategies that rely on the partial likelihood approaches, for obvious reasons, lose time-related hazard risk information by avoiding the modeling of the baseline hazard. Although we have seen the significant benefits of this strategy, we have also lost information about the potential decay of the effects produced by the passage of time. The advantage of modeling REM's intensity function through a Cox's regression is that we can rely on survival modeling literature to estimate different components of our model. In this regard, we can adapt \cite{borgan_methods_1995} baseline estimator from the nested case-control sampling example to provide an asymptotic estimator for the baseline of our model. The adapted estimator for the DREAM is then

\begin{equation}\label{eq:base}
	\hat{\Lambda}_0(t) = \sum_{t_i < t} \Bigg[ \exp \Bigg\{  \sum_{k=1}^q f_k(x_{rk}) \Bigg\} + \exp \Bigg\{\sum_{k=1}^{q} f_k(x_{r^*k}) \Bigg\} \Bigg]^{-1} \frac{2}{n(t_i)},
\end{equation}

where $n(t_i)$ is the number of events at risk at $t_i$, for $i=1,\dots,n$.  Using the average coefficients obtained from the repeated fits of the DREAM, we estiamte the cumulative basline hazard in Figure \ref{fig:base}. %In line with what expected, the curve highlights how as time moves on, the base risk of being cited increases.

As anticipated, the curve demonstrates that the base risk of being cited increases over time. The increasing trend of the curve indicates that patents become more susceptible to being cited as time goes on, which may be attributed to the accumulation of knowledge and technological advancement over time. This observation is consistent with the general expectation that the probability of being cited will grow over time due to the increasing size and complexity of the patent network. Moreover, this result underscores the importance of considering the temporal dimension when analyzing citation patterns and provides valuable insights into the dynamics of knowledge diffusion in patent systems.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{baseline.pdf}
	\caption{\label{fig:base}Cumulative baseline hazard esimated through the adaptation of \cite{borgan_methods_1995} estimator.}
\end{figure}

\subsection{Interaction effect}

%Single splines models information coming from single covariates. However, to understand the peculiar patterns of patent citations, it is fundamental to consider the potential interaction among variables. The flexibility of the DREAM modeling approach allows for attaining interaction effects by taking the tensor products of covariates. 

While single splines can provide insight into the relationship between individual covariates and patent citation, it is often necessary to consider the interaction effects between these variables to fully understand the complex patterns of citation behavior. The DREAM modeling approach provides the necessary flexibility to incorporate interaction effects by taking the tensor products of covariates. Hence, given two covariates $x_{rk_1}(t \mid H_{t^-})$ and $x_{rk_2}(t \mid H_{t^-})$ for the event modelled through basis functions of degrees $d_1$ and $d_2$, it is possible to define an interaction spline as 

\begin{equation*}
	f_{k_1,k_2}(x_{rk_1},x_{rk_2})=\sum_{j_1=1}^{d_1}\sum_{j_2=1}^{d_2} \theta_{j_1k_1}\theta_{j_2k_2} [B_{j_1,p}^{k_1}(x_{rk_1}) \otimes B_{j_2,p}^{k_2} (x_{rk_2})].
\end{equation*}

If we then add a single interaction effect using covariate $c_1$ and $c_2$, the partial likelihood in eq~\ref{eq:logit_gam} becomes

\begin{equation}\label{eq:logit_gam_inter}
	\begin{split}
	\tilde{PL}(\theta) & = \prod_{i =1}^n \Bigg[ 1+\exp\bigg\{- \sum_{k=1}^{q}\sum_{j=1}^d \theta_{jk}[B_{j,p}^k(x_{rk})- B_{j,p}^k(x_{r^*k})] \\
	&  + \sum_{j_1=1}^{d_1}\sum_{j_2=1}^{d_2} \theta_{j_1k_1}\theta_{j_2k_2} [B_{j_1,p}^{k_1}(x_{rk_1}) \otimes B_{j_2,p}^{k_2} (x_{rk_2})-B_{j_1,p}^{k_1}(x_{r^*k_1}) \otimes B_{j_2,p}^{k_2} (x_{r^*k_2})]\bigg\} \Bigg]^{-1}.
	\end{split}
\end{equation}

Through this new model formulation, the DREAM is able to fit interaction effects using splines simply by taking the difference between the tensor products of events covariates and non-events. With regard to our analysis, we show in Figure~\ref{fig:inter} the resulting interaction between the two time-varying variables: the \emph{cumulative citations received} and the \emph{time from last event}. 

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{interaction.pdf}
	\caption{\label{fig:inter}Interaction effect between the two time-varying covariates: \emph{cumulative citations received} and \emph{time from last event}.}
\end{figure}


In Figure~\ref{fig:inter}, the fitted plane clearly demonstrates the importance of both the timing and frequency of citations in predicting future citations. The negative values in the log-hazard contribution for long periods with no or few citations indicate that patents are less likely to be cited if they have not received attention in a while. However, this negative contribution becomes less significant over time, suggesting that even if a patent has not been cited in a long time, it may still have potential to be cited in the future.
Conversely, the positive values in the log-hazard contribution for short time intervals between events reveal the snowball effect of citation. As shown previously in Figure~\ref{fig:tv}, patents that receive more citations are more likely to be cited again. This effect is even more pronounced when citations occur with short waiting time intervals. The steepest part of the plane in the positive hazard contribution indicates that continuous citation is critical to maintain a central position within the citation network. Moreover, the lesser steep part of the plane highlights the small contribution of long intervals between citations on being cited again. This pattern suggests that while some patents may still have potential for future citations, the longer the waiting time between citations, the less likely it is for the patent to be cited again.



%The fitted plane in Figure~\ref{fig:inter} clearly shows how if no to few citations occurs for long periods, the likelihood of being cited diminishes, assuming negative values in the log-hazard contribution. Eventually, such a contribution assumes a lesser negative value as time moves on. On the other side, as shown already in Figure~\ref{fig:tv}, the more citations a patent receives, the more likely it gets cited again. The same phenomenon appears as well in the interaction effect, where a small interval of time from events brings to the steepest part of the plane assuming a positive value in the hazard contribution. This snowball effect is more evident if citations occur with the shortest waiting time intervals. Indeed, from this graph, it is clear the importance of continuously receiving citations to remain in a central position within the network.
%Eventually, long intervals between citations show the small contribution on being cited again as it is represented by the lesser steep part of the plane.  

%\section{Discussion}


\section{Conclusions}

%REMs offer a sophisticated approach to analyzing and understanding complex patterns that appear in network data. In this paper, we focused our attention on patent analysis, where we are interested in identifying the patterns that drive a patent to be cited. In such context, by modeling the relationships between citing and cited patents, REMs provide a valuable framework for identifying patterns and trends in patent activity. However, current limitations on the fitting procedures of REMs makes these series of models an inaccessible tool for inferring large sparse network. Leveraging survival modeling techniques and the machine learning framework, we defined a Deep Relational Event Additive Model (DREAM) to overcome current computational limitations and exploit the non-linear modeling of network statistics through a B-spline transformation. Using this novel modeling technique, we were able to fit the DREAM on the network of patent citations that spans from 1976 to the end of 2022 and has over 8 million nodes and over 100 million citations .

The use of REMs provides a sophisticated and effective approach for analyzing complex patterns in network data. In this study, we applied this framework to patent analysis to identify the patterns driving patent citation. However, the limitations of current fitting procedures for REMs make them inaccessible for inferring large sparse networks. To overcome these limitations, we developed a Deep Relational Event Additive Model (DREAM) that leverages machine learning techniques and a B-spline transformation to exploit the non-linear modeling of network statistics. By applying the DREAM to a network of patent citations spanning from 1976 to the end of 2022 with over 8 million nodes and over 100 million citations, we were able to identify patterns that make a patent more likely to be cited as well as provide an estimate of the cumulative baseline hazard of the process.

%Results from the analysis provided by this work give insightful considerations on what makes a patent more likely to be cited. Although the interpretation of most effects is straightforward, some estimated effects uncover peculiar patterns on which more investigation are needed. In this regard, we noted in the \emph{receiver publication year} splines two spikes in two different areas of the parameter space, suggesting that these might be years of important technological innovation as patents issued in that period are more likely to be cited. This phenomenon, however, should be further investigated in single technology classes to asses which area innovates more and when important breakthroughs happened. 

Our findings offer insightful considerations on the factors driving patent citation. While some effects are straightforward, others reveal peculiar patterns that require further investigation. For instance, we observed two spikes in the \emph{receiver publication year} splines, suggesting that patents issued in those years may be more likely to be cited due to significant technological innovations. This information could be useful for policymakers, industry professionals, and researchers who want to identify important areas of innovation and potentially invest resources in those areas. In this regard, further research is required to assess which areas of technology are innovating more and when as well as the identification of the occurrence of significant breakthroughs.



%Despite this work's contribution in proposing a way in which it is possible to estimate the cumulative baseline hazard, a more sophisticated approach has to be defined to assess the time decay of some of the effects. Following the recent discussion on the changes in the generative process of patent citations \citep{kuhn_patent_2020,filippi-mazzola_drivers_2023}, it would be of interest to evaluate the \emph{textual similarity} curve behavior in Figure~\ref{fig:sim} within the observed period. In this regard, this work's underlying premise is the legal stability of the system, as sudden changes in the citation process would have been caught up by decay patterns. The idea of introducing time in the model goes hand in hand with the latest work on REMs, where new modeling techniques on recovering the baseline are recently being proposed \citep{juozaitiene_non-parametric_2022}.

Although this study offers a way to estimate the cumulative baseline hazard, a more sophisticated approach is needed to assess the time decay of some effects. It would be interesting to evaluate the behavior of the \emph{textual similarity} curve in Figure~\ref{fig:sim} over the observed period, particularly in light of recent discussions on changes in the generative process of patent citations \citep{kuhn_patent_2020,filippi-mazzola_drivers_2023}. Nonetheless, this study assumes the legal stability of the patent citation system, as sudden changes would be reflected in the decay patterns captured by the model. Approaches like the one proposed by \cite{juozaitiene_non-parametric_2022} could be further investigated to be applied to the DREAM to asses predictors' temporal decay. 

%Regarding the novelty of this new approach, we argue that, while this version of the DREAM was designed to model citation networks, it may be efficiently expanded to almost every kind of dynamic network. Leaving the computation of meaningful statistics the current computational challenge of REMs. As a result, the contribution of this study is to open REMs to large networks.

Furthermore, we argue that this novel approach could be further extended beyond citation networks to other types of dynamic networks. Although REMs pose computational challenges, the DREAM provides a promising solution to overcome these challenges and open REMs to large networks. This study contributes to advancing the understanding of patent citation patterns and provides a new approach to model complex network dynamics.

\section*{Acknowledgments}
This work was supported by funding from the Swiss National Science Foundation (SNSF grant 192549).


\bibliographystyle{rss}
\bibliography{biblio}

\end{document}