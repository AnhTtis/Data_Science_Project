\section{Experiments}
\label{sec:exp}

In this section, we demonstrate the wide-range of usages and high capabilities of Uni-Fusion on various applications.
%
First, we evaluate Uni-Fusion on application 1) Incremental Surface and Color Reconstruction with respect to the SOTAs.
%
Considering that the applications 2) and 5) are a novel topic, we cannot find related benchmarks for comparison.
Thus, we demonstrate the performance on exist results.
%
Afterwards, application 3) is implemented and compared to the SOTA zero-shot semantic segmentation models.
%
Finally, for application 4) as infrared data is not widely used, we capture our own dataset containing infrared and demonstrate all applications on it.

\subsection{Implementation Details}
\label{sec:exp:details}

In the experiments, we use our sample-based GPIS for local geometry encoding.
For each point, 2 points along normal direction are sampled (one positive, one negative) with distance $d=0.1$ in the local voxel's normalized space. 
Comparing to derivative-based GPIS, our sample based GPIS is more efficient in both space and time. 
%While for reconstruction quality, our derivative based GPIS only shows little better than sample based GPIS.
For the encoder, we use $256$ anchor points that are randomly sampled from $[-0.5,0.5]^3$.
The first $20$ eigen-pairs are utilized, which yield the feature dimension $20$.
The model choosing will be discussed in ablation study.

%FIXME - Question istn't this the reason, why your images look so much nicer then the sota? Should be analyzed (maybe in a later paper).
% FIXED - I dislike units in papers. We should state to man-made environments these units are used.
Different maps of latents use different granularity.
For surface LIM, we use voxel-size $5\si{\centi\meter}$. 
For color which requires later comparison to NeRF, we use voxel-size $2\si{\centi\meter}$.
For other property LIM and feature LIM, we use voxel-size $10\si{\centi\meter}$.

For smoothness of reconstruction, the encoded voxel is overlapped.
This means it uses the double-size to ensure a half space overlap with each neighbor. 
Then during meshing, SDF is fetched from the overlapped voxel.
While for the remaining properties, we only sample from its own voxel part.

%FIXED: Aren't there two GPUs?
The implementation runs on a computer with AMD Ryzen 9 5950X 16-core CPU and  a Nvidia Geforce RTX 3090 GPU (24 GB).

\subsection{Datasets}
%\begin{figure}[b!]
%	\begin{subfigure}{1.\linewidth}
%		\centering
%		\includegraphics[width=.4\linewidth]{example-image-golden}
%		\includegraphics[width=.4\linewidth]{example-image-golden}\caption{ScanNet}
%	\end{subfigure}
%	\begin{subfigure}{1.\linewidth}
%	\centering
%	\includegraphics[width=.4\linewidth]{example-image-golden}
%	\includegraphics[width=.4\linewidth]{example-image-golden}
%	\caption{Replica}
%	\end{subfigure}
%	\begin{subfigure}{1.\linewidth}
%	\centering
%	\includegraphics[width=.4\linewidth]{example-image-golden}
%	\includegraphics[width=.4\linewidth]{example-image-golden}
%	\caption{2D-3D-S}
%	\end{subfigure}
%
%	\caption{Dataset}
%\end{figure}

We evaluate incremental reconstruction on the ScanNet dataset~\cite{dai2017scannet}, TUM RGB-D dataset~\cite{sturm2012benchmark}, and Replica dataset~\cite{sucar2021imap}.
Using MSG-Net~\cite{zhang2018multi}'s material set, we transfer styles to the 3D canvas.
For Open Vocabulary Scene Understanding, we evaluate on ScanNet segmentation data~\cite{qi2017pointnet++} and S3DIS dataset~\cite{armeni20163d}.

\subsubsection{ScanNet~\cite{dai2017scannet}}

ScanNet is a densely annotated RGB-D video dataset.
It is captured with the structure sensor~\cite{occipital} and contains 1513 scenes for training and validation.
For each scene, both images and a 3D mesh is provided, together with their 2D, 3D semantic annotations. 

ScanNet provide 312 scenes for validation, which contains a wide range of various room structures.
It has now been widely used in the thorough evaluation of the performance of reconstruction and semantic segmentation.

\subsubsection{TUM RGB-D~\cite{sturm2012benchmark}}

TUM RGB-D is a benchmark to mainly evaluate the performance of tracking.
It is captured with Microsoft Kinect sensor together with ground-truth trajectory from the sensor.

\subsubsection{Replica~\cite{sucar2021imap}}

Replica dataset is referred to iMAP's pre-processed dataset~\cite{sucar2021imap}.
It is a synthetic rendered RGB-D dataset from given 3D models.
The advantage to include this dataset is that Replica does not have motion blur. 
Which is better to evaluate the capability of algorithms on reconstructing surface color.

%\subsubsection{ICL-NUIM}
\subsubsection{MSG-Net Style~\cite{zhang2018multi}}

MSG-Net provide material images for transfering the styles.
We select 21style fold for demonstration.
Those images are given in \cref{fig:style} together with our result.

\subsubsection{ScanNet Point Cloud Segmentation Data~\cite{qi2017pointnet++}}

For point cloud semantic segmentation benchmarking, PointNet++~\cite{qi2017pointnet++} pre-processes the original ScanNet~\cite{dai2017scannet} and generates subsampled point clouds and corresponding annotations for each scene.

\subsubsection{S3DIS~\cite{armeni20163d} and 2D-3D-S~\cite{armeni2017joint}}

S3DIS is a semantic segmentation dataset for 3D point clouds.
Which is also a subset of the 2D-3D-S dataset.
The 2D-3D-S dataset is a multi-modalities dataset containing 2D, 2.5D and 3D domains. 
This dataset is densely annotated with semantic labels.

Note that 2D-3D-S's 2D captures is not a RGB-D video as ScanNet.
2D-3D-S's images only have small overlap. 
So it is merely for semantic segmentation and not capable for incremental reconstruction application.


%We use 6 versatile dataset,
% 3DScene, Augmented ICL-NUIM, 
%Scannet, TUM RGBD, Replica, nuScenes, Matterport to fully evaluate our models.

%Firstly on Reconstruction application, we \textbf{follow surface reconstruction state-of-the-art BNV-Fusion} [] to compare surface reconstruction on

%
%3D Scene, Augmented ICL-NUIM and Scannet.
%Besides, we \textbf{follow indoor online NeRF state-of-the-art, NeRF-SLAM} [] to evaluate our color field on 
%Replica dataset [].
%
%On Scene Understanding application, we \textbf{follow state-of-the-art OpenScene}[] to evaluate on
%Scannet, nuScenes and Matterport dataset.
\subsection{Baselines}

For online surface mapping evaluation, we choose TSDF Fusion~\cite{curless1996volumetric}, iMAP~\cite{sucar2021imap}, SOTA DI-Fusion~\cite{huang2021di} and BNV-Fusion~\cite{li2022bnv} as four baselines methods.

For the color field, we choose TSDF Fusion~\cite{curless1996volumetric}, $\sigma$-Fusion~\cite{rosinol2023probabilistic}, iMAP~\cite{sucar2021imap}, NICE-SLAM~\cite{zhu2022nice} and even recent hot Neural Radiance Fields model NeRF-SLAM~\cite{rosinol2022nerf} as five baselines.
Even though it is not fair to include NeRF in comparison, we show in following experiment that Uni-Fusion significantly reduce the gap.

For Scene Understanding application, we evaluate generalized zero-shot point cloud semantic segmentation with ZSLPC~\cite{cheraghian2019zero}, DeViSe~\cite{frome2013devise} and SOTA 3DGenZ~\cite{michele2021generative} for comparison.

\subsection{Metrics}

For incremental reconstruction, we evaluate the geometric reconstruction using \textbf{Accuracy}, \textbf{Completeness}, and \textbf{F1 score}. 
We follow BNV-Fusion~\cite{li2022bnv} to uniformly sample $100,000$ points from reconstruction and ground-truth mesh respectively.
Then \textbf{Accuracy} (\textbf{Completeness}) measures the percentage of reconstruction-to-ground-truth (ground-truth-to-reconstruction) distances that are lower than $2.5\si{\centi\meter}$ threshold. \textbf{F1 score} is the harmonic mean of accuracy and completeness.
For tracking performance, we use \textbf{ATE RMSE}.

Then for evaluation of color reconstruction, we follow SOTA on this topic, the NeRF to render both depth and RGB images to compare the image level \textbf{Depth L1} and \textbf{RGB PSNR}.

Afterwards, for comparing scene understanding, we follow zero-shot point cloud semantic segmentation SOTA 3DGenZ to evaluate the \textbf{Intersection-of-Union (IoU)} and \textbf{Accuracy}.


\input{tex/exp/surface}
\input{tex/exp/second}
%\input{tex/exp/context}
\input{tex/exp/latent}

% Q: tracking?
% A: not care about fusion

\subsection{Time}

We run all applications in one pass using our captured office sequences and evaluate the time cost for the construction and fusion of each LIM. 
%The external tracking runs in another thread.
The average time cost over frames are shown in \cref{tab:time}.

\begin{table}[htbp]
	\caption{Time required in seconds for each frame.
	}
	\centering
	\footnotesize
	\setlength{\tabcolsep}{0.7em}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{l|ccccccc}
			\toprule
			&Surface & Color & Infrared & Style & Saliency & Latent&Internal Track \\ \midrule
			Time ($\si{\second}$)&0.100 & 0.038 & 0.045 & 0.048 & 0.045 &0.011 &0.225 \\ \bottomrule
	\end{tabular}}
	
	\label{tab:time}
\end{table}

From this table, with depth and properties images of size $720\times1280$ as input, our model works with a frequency of $\sim10\si{\hertz}$ for surface (sample mode) LIM construction and integration.
While even $20+\si{\hertz}$ for color, infrared, style, and saliency.
This shows our Uni-Fusion is adequate for real-time applications.

However, our internal tracking takes around $0.225\si{\second}$ for a frame which is comparatively slower than the mapping module. 
%
Though, Uni-Fusion exploits an external tracking to protect the model from losing track, which enables our internal tracking and mapping to have a lower frequency.
And thus the whole model is applicable to be real-time in various scenarios.

\section{Extensive experiment on Our Own dataset}

In previous experiments, we assessed the capabilities of our Uni-Fusion in various applications. For robotic understanding of the surroundings, we capture our own dataset to show all applications together.

We capture two scenes: The office and apartment of the first author using a  Microsoft Kinect Azure. 
%
RGB-D and infrared video are captured. After calibration, RGB, depth, infrared inputs are with the resolution of $720\times1280$.
Uni-Fusion tracks and reconstructs all applications in one pass.
%
Office data has been involved in ablation study~\cref{exp:surface:ablation}.
We demonstrate all applications on the apartment dataset that is shown in \cref{fig:appartment}.

\input{tex/exp/mine}

Again, the ceiling of reconstruction is removed for better visualization.
In the top row, we show the colored mesh with room details, infrared mesh which reveales the light affect, saliency reconstruction that indicates the objects that are important for navigation.
Next, we choose the second style in~\cref{fig:style} to transfer onto the apartment canvas.
%
The wooden floor in the room is colored with dark green.
The overall apartment is in a warm style.

The rest are generated from the surface field of the CLIP embeddings. 
We command object to find, e.g., where is the sofa, desk and coat.
In addition, it easily finds affordances like sit-able.
For material, it successfully finds the wooden floor in each room.

%\subsection{Ablation study}
%\newpage
%
%
%
%
%\subsubsection{Different anchor number}
%\subsubsection{Different voxel-size}
%pass




