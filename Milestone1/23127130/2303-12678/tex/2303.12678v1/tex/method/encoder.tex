\section{Decoupled Regression as an Universal Encoding}
\label{sec:UE}

In this section, we propose an universal encoding model for point clouds.
Based on this encoder, in~\cref{sec:UCM}, we introduce Universal Continuous Mapping.

\subsection{Decoupled Gaussian Process Regression as Encoder-Decoder}% one page
\label{sec:encoder}

Inspired by DI-Fusion~\cite{huang2021di} that introduces maps of latent features for continuous surface prediction, we aim to produce a latent vector for each local patch.
%
% introduce the gaussian process
Firstly, an universal encoder design comes from the Gaussian Process Regression (GPR) which is a widely used techniques for low dimensional regression.
It has been utilized in various mapping models~\cite{yuan2018fast,martens2016geometric}.
%
Given a dataset with $N$ observation points $\{(\V x_n, \V y_n)\}_{n=1}^{N}$ where point positions are $\V x_n \in \mathbb R^d$ ($d=3$ in this paper) and point properties are $\V y_n \in \mathbb R^c$, the regression model estimation is $f(\V x)=E[\V y | \V X=\V x]$.
%based on the observation set. %Which is $\V y = f(\V x) + \espilon$ where $\epsilon$ is Gaussian noise. 

Gaussian process assumes data are sampled from multivariate Gaussian distribution, i.e.,
\begin{equation}
\begin{bmatrix}
\V y\\
\V y_{*}
\end{bmatrix}
	 \sim \mathcal{N}(\V 0, 
\begin{bmatrix}
\V K(\V X, \V X)\ \V K(\V X, \V X_{*})\\
\V K(\V X_{*}, \V X), \V K(\V X_{*}, \V X_{*})
\end{bmatrix}	 
	 ).
\end{equation}
%
With the derivative in~\cite{williams2006gaussian}, we yield 
\begin{equation}
	\V y_{*} | \V X, \V y, \V X_{*} \sim \mathcal{N} (\V K_{*,}\V K^{-1}\V y, \V K_{*,*}-\V K_{*,}\V K^{-1}\V K_{,*}).
\end{equation}
%
When an additional Gaussian error is appended, $\V y=f(\V x)+\epsilon$, the covariance of $\V X$ is rewriten as $\V K+\delta^2_n\V I$.
%
The regression result is usually considering the mean
\begin{equation}
	\label{eq:mean}
	\V y_{*} = \V K_{*,}(\V K+\delta^2_n\V I)^{-1}\V y.
\end{equation}
Which well-reveals the challenge to use Gaussian process regression directly in large-scale reconstructions, as $\mathcal{O}(n^3)$ time complexity is unaffordable.
% with approxmiation
In addition, such a formula (\cref{eq:mean}) is not usable as the input large point cloud data is required to be maintained for the $ \V K_{*,}$ computation. 

To cope with the high-complexity and to avoid the maintenance of the point cloud, we propose to decouple the GPR to approach the low-dimensional latent vector for local regions.
%
The decoupling requires an approximation of the kernel function
\begin{equation}
	\V K(\V X,\V X_{*} ) \approx f_{posi}(\V X)^T f_\text{posi}(\V X_{*})
	\label{eq:k_approx}
\end{equation}
where $f_\text{posi}: \mathbb{R}^3\rightarrow \mathbb{R}^l$.
We name $f_\text{posi}$ positional encoding function to be coherent to Neural Implicit Maps model Di-Fusion~\cite{huang2021di}.
%
Thus, \cref{eq:mean} is rewriten as
\begin{equation}
	\label{eq:approx_K_mean}
	\begin{split}
	\V y_{*} %&= f_\text{posi}(\V X_*)^Tf_{posi}(\V X)(f_\text{posi}(\V X)^Tf_\text{posi}(\V X) +\delta^2_n\V I)^{-1}\V y\\
	%&
	=f_\text{posi}(\V X_*)^Tf_\text{enc}(\V X, \V y)
	\end{split}
\end{equation}
%With $X\in \mathbb{R}^{N\times 3}$, $y\in\mathbb{R}^{N\times c}$, we
where the content encoder function is
\begin{equation}
	\label{eq:encode}
	f_\text{enc}(\V X, \V y) = f_\text{posi}(\V X)(f_\text{posi}(\V X)^Tf_\text{posi}(\V X) +\delta^2_n\V I)^{-1}\V y \in \mathbb{R}^{l \times c}.
\end{equation}

We denote the encoded feature as $ \V F_{(\V X, \V y)} = f_{enc}(\V X, \V y) \in \mathbb{R}^{l\times c}$.
This is the basis for latent map construction in following universal continuous mapping model (\cref{sec:UCM}).
To be more specific, for geometry encoding we have $l=20$ and $c=1$, thus, the feature is a $20$-dimensional vector.
For color encoding we yield $l=20$ and $c=3$.

Correspondingly, the decoding value for inferencing point $\V x_*$ is
\begin{equation}
	\label{eq:decode}
	f_{dec}(\V x_{*}, F_{(\V X, \V y)}) = f_{posi}(\V x_{*})^T F_{(\V X, \V y)}.
\end{equation}
Thus, a signed distance field or surface property field is approached.

In the following, we derive the approximation function.

\begin{figure}[]
	\centering
    \psfragfig[width=1\linewidth]{im/eps/encoder1}{
	\psfrag{y1}{$\V y_1$}
	\psfrag{x1}{$\V x_1$}
	\psfrag{yn}{$\V y_n$}
	\psfrag{xn}{$\V x_n$}
	\psfrag{z1}{$\V z_1$}
	\psfrag{zn}{$\V z_n$}
	\psfrag{fp}{$f_{posi}$}
	\psfrag{fd}{$f_{dec}$}
	\psfrag{fe}{\color{mybronze}{$f_{enc}$}}
	\psfrag{ec}{\color{mybronze}{{Encoder}}}
	\psfrag{zm}{$\V Z_m$}
	\psfrag{ym}{$\V y_m$}
	\psfrag{Fm}{$\V F_m$}
	\psfrag{x}{$\V x_{*}$}
	\psfrag{y}{$\V y_{*}$}
	\psfrag{f}{\tiny $\V Z_m(\V Z_m^T\V Z_m+\sigma_n\V I)^{-1}\V y_m$}
}
	
	%\includegraphics[width=1\linewidth]{im/encoder}
	\caption{Interpreting formula with a graph that is coherent to the Encoder-decoder structure in Neural Implicit Maps~\cite{huang2021di}. }
	\label{fig:encoder}
\end{figure}



\subsection{Position Encoding with Approximated Kernel Function} % one page
Considering our mapping requires encoding of local geometry \& property, the encoding function merely touch points in a limited region ($[-.5,.5]^3$ in our case).

As we introduced in related works, the Nytr\"om methods, different from the RFFs, are more accurate because it is dependent to the given points. This property well match to our application.

% introduce the Nytrom methods
Nystr\"om method for kernel approximation starts with eigenfunctions according to Mercer's theorem:
\begin{equation}
	k(\V x_1, \V x_2) = \sum_{i\ge 1} q_i \psi_i(\V x_1)^T\psi_i(\V x_2)
	\label{eq:eigen}
\end{equation}
where $\psi_i$ and $q_i\ge 0$ are eigenfunctions and eigenvalues of kernel $k(\V x_1, \V x_2) $ with respect to probability measure $q$.% With the top-$k$ eigenpairs, Nystr\"om method approximate kernel with $ k(\V x, \V y) = \sum^k_{i\ge 1} \alpha_i \varphi_i(\V x)\varphi_i(\V y)$.

Given third set of anchor samples $\hat{\V X}=\{\hat{\V x}_1,\cdots,\hat{\V x}_N\}$, 
% how we use it
we eigen-decompose matrix $\V K(\hat{\V X}, \hat{\V X})$ for its eigenpairs $\{(\lambda_i,\V u_i)\}_{i\in\{1,\cdots,l\}}$ with rank $l$.

Then Nystr\"om method produces
\begin{equation}
\label{eq:nytrom_1}
\psi_i(\V x) = \sum_{n}k(\V x, \hat{\V x}_n)\V u_{i,n},\forall i> 1.
\end{equation}

% can be obtained with eigendecomposition of matrix $K(\hat{\V X}, \hat{\V X})$. 

%To approach
%$k(\V x, \V y) = \sum^{k}_{i= 1} \hat{\mu_i} \hat{\varphi_i(x)}\hat{\varphi_i(y)}$ 

%The \cref{eq:nytrom_1} is then rewriten as 
%\begin{equation}
%	\label{eq:nytrom_2}
%	\hat{\varphi}_i(\V x) = \frac{1}{N\hat{\mu_i}}\sum_{n=1}{N}k(\V x, \V x_n)\hat{\varphi}(\V x_n), i\in{1,\cdots,k}.
%\end{equation}


For simplicity, we rewrite eigenfunction as 
\begin{equation}
\psi_i(\V x) = \V K(\V x, \hat{\V X})\V u_i .
\label{eq:nytrom_2}
\end{equation}
And correspondingly eigenvalue $q_i=\frac{1}{\lambda_i}$.

To clarify the presentation, we notate $\V q=[{q_1},\cdots,{q_l}]$ and
${\Psi}=[\psi_1,\cdots,{\psi_l}]^T$
 from \cref{eq:eigen} where
${\Psi}:\mathbb{R}^3\rightarrow \mathbb{R}^l$.


Make the formula further coherent to \cref{eq:k_approx}, we set 
\begin{equation}
	f_{posi}(\V x) = diag(\sqrt \V q)\Psi(\V x) 
\end{equation}
where $diag$  produce diagnal matrix.
$f_{posi}$ above is the position encoder in the \cref{eq:encode}.

In this paper, we utilize Mat\'ern kernel\footnote{https://en.wikipedia.org/wiki/Mat\'ern\_covariance\_function}:
\begin{equation}
\label{eq:matern}
k(\V x_i, \V x_j) = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)}(\sqrt{2\nu}\frac{dist(\V x_i, \V x_j)}{\rho})K_{\nu}(\sqrt{2\nu}\frac{dist(\V x_i, \V x_j)}{\rho})
\end{equation}
where $\Gamma$ is gamma function, $K_{\nu}$ is the modified Bessel function of the second kind, $dist$ denotes the euclidean distance, $\sigma$ and $\rho$ are kernel's hyper-parameters.
We utilize half integer $\nu=3+\frac{1}{2}$ and thus the specific function is 
\begin{equation}
\label{eq:matern_specific}
k(d) = \sigma^2(1+\frac{\sqrt{7}d}{\rho}+\frac{2}{5}(\frac{\sqrt{7}d}{\rho})^2+\frac{1}{15}(\frac{\sqrt{7}d}{\rho})^3)exp(-\frac{\sqrt{7}d}{\rho})
\end{equation}
where $d=dist(\V x_i, \V x_j)$ for short.
To approximate above kernel, we first uniformly sample $256$ points $\hat{\V X}$ in $[-5,5]^3$ cube to compute kernel $\V K_{a}=\V K(\hat{\V X},\hat{\V X})$.
Then eigendecomposition is applied to this kernel, $\V K_{a}=\V U \Lambda \V U^T$. % with $U$ a $256\times $.

Note that, the encoded feature dimension $l$ used in \cref{sec:encoder} is depend on the $\V U$. 
Which further determines the map size in the next section (~\cref{sec:UCM}).
There is a trade-off between the approximation and map size.
%Therefore, the $l$ cannot be too large.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.\linewidth]{im/eigenvalue}
	\caption{Sorted eigenvalues for $\V K_{a,a}$'s eigendecomposition.}
	\label{fig:eigenvalues}
\end{figure}
We plot the eigenvalue from $\Lambda$ in \cref{fig:eigenvalues} and find the matrix is mainly affect by the few pairs with large eigenvalues.
Most of the eigenvalues are lower than 1. 
We thus choose $l=20$ which is around $0.8$ in that plot to approximate the kernel while maintain a small feature dimension.

Note that, we only sample and decomposite once. Then encoding-decoding in \cref{fig:encoder} merely need to load and reuse the parameters for the $f_{posi}$, $f_{enc}$ and $f_{dec}$. Related works requires pretrain on large dataset of objects~\cite{huang2021di,li2022bnv} or indoor scenes~\cite{zhu2022nice}. But for our model \textbf{no training is needed}.
