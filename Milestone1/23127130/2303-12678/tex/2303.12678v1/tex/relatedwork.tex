\section{Related Works} % page 2
\label{sec:related_works}

\subsection{Continues Mapping}
Continuous Mapping started with 2D scenarios.
Gaussian Process Occupancy Maps (GPOM)~\cite{o2012gaussian} first uses Gaussian Process Regression (GPR) to predict a continuous representation to enable the map construction at arbitrary resolution. 
Then, by borrowing the Bayesian Committee Machine (BCM) technique, incremental GPOM is approached~\cite{ghaffari2018gaussian,yuan2018fast}.

In the meanwhile, the Hilbert Maps approach~\cite{senanayake2017bayesian,zhi2019continuous} were presented. Which is a mapping approach without an explicit formulation.
Hilbert Maps perform continuous mapping by keeping optimizing parameters in the background. 

With increasing popularity of 3D sensors, the focus shifted towards 3D cases.
Gaussian Process Implicit Surface (GPIS), has entered the field~\cite{martens2016geometric,lee2019online,wu2021faithful,ivan2022online}.
Given zero-level surface points, those methods either sample points along normal direction and assign distance values or directly utilize derivative models with normals as labels (we will give more detail in our mapping \cref{sec:surface_mapping}).
%
However, those methods majorly work on shapes but not scenes.
It is due to the fact, that GPR based methods naturely suffer under the high computational costs when large amount of data is involved.
Especially when the test scope moves from 2D to 3D.
%
Thus, we borrow the idea of Neural Implicit Maps~(\cref{sec:related:NIM}) to address this issue and work on scene reconstruction.

\subsection{Neural implicit based Reconstruction}
\label{sec:related:NIM}

Neural implicit reconstruction have orginally be introduced to SDF and Occupancy based reconstruction.
The seminal work of \cite{park2019deepsdf}, i.e., DeepSDF, utilizes deep model to encode geometry prior with Multi-layer perceptrons (MLPs) and extract discretized SDF by querying from decoding.
Then, an algorithm similiar to Marching Cubes~\cite{lorensen1987marching} is applied to extract meshes. 
For the latter, Occupancy Networks~\cite{mescheder2019occupancy} learn to estimate the occupancy probability at positions from an implicit function.
Meshes are generated with the Multiresolution IsoSurface Extraction (MISE) technique.

To improve the efficiency of the reconstruction, 
DeepLS~\cite{chabra2020deep} resorts to multiple local deep priors and reconstructs on a set of local SDFs. 
Jiang et al.~\cite{jiang2020local} further proposes to utilize a local implicit grid that further eases the complexity of learning the encoding.
In contrast, Convolutional Occupancy Networks~\cite{peng2020convolutional} explore the local latent by replacing the encoder-decoder with optimization on a grid of local features to ease the burden of simple MLPs. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\linewidth]{im/PLV}
	\caption{Uniform, sparse voxels. Each voxel is encoded to one feature vector $\V F_m$~\cite{huang2021di}.}
	\label{fig:PLV}
\end{figure}

More recently, Neural Implicit Maps (NIM) achieve real-time real-scene 3D reconstruction.
Huang et al. propose DI-Fusion~\cite{huang2021di}, a neural implicit mapping method that incrementally reconstructs a scene.
By fusing on maps of latent features, DI-Fusion achieves high efficiency of memory while yielding better reconstructions.
Similarly, NeuralBlox~\cite{lionar2021neuralblox} also fuse a grid of latent features given a known external pose estimation.
To further improve the feature quality, BNV-Fusion~\cite{li2022bnv} additionally add post-optimization after fusion.
To handle large scene reconstructions, that may contain loops, NIM-REM~\cite{yuan2022algorithm} proposes a SE(3)-transformation algorithm on NIM and implements a mapping\&remapping model supporting working alongside Bundle Adjustment or SLAM with Loop-closing.
Thinking outside of the box, Sucar et al. propose iMAP~\cite{sucar2021imap} a novel SLAM pipeline with neural implicit based incremental reconstruction.
With a differentiable rendering model, iMAP is able to online optimize the reconstruction by minimizing the image distances.
Zhu et al. presents NICE-SLAM~\cite{zhu2022nice}, that replaces the MLPs with convolutional occupancy grid to further improve the efficiency and quality of the reconstruction.

In this work, our model also relies on the regular spaced voxels as in~\cref{fig:PLV} for local encoding-decoding. 
Since we propose to use one model for all properties, it is not possible to pre-train such a model.
Therefore, we rely on a kernel method to achieve this goal.
% the data is not observed during the training, our model also relies on the encoder-decoder model.
%\subsection{Context Reconstruction}

\subsection{Kernel Function Approximation}

Kernel methods are not scalable as the mapping from $n$ points to a kernel requires $O(n^2)$ time complexity. 
One possible solution is with kernel matrix approximation but is not in the scope of this paper.
The other solution, kernel function approximation, proposes to improve the scalibility of kernel method using explicit vector maps. For example, $k(\V x_1, \V x_2) \approx  \pmb v(\V x_1)^T \pmb v(\V x_2)$ with mapping function $\pmb v:\mathbb{X}\rightarrow \mathbb{R}^l $~\cite{deng2022neuralef}.
%
There are two approaches to address the approximation. One is Random Fourier Features (RFFs)~\cite{rahimi2007random,rahimi2008weighted,yu2016orthogonal,munkhoeva2018quadrature,francis2021major} and the second is Nystr\"om method~\cite{francis2021major,williams2000using,deng2022neuralef}. 

% how RFFs do
RFFs explicitly handle the shift-invariant kernels by mapping the data using  the Fourier transform technique~\cite{francis2021major}.
However, RFFs are majorly used to handle shift-invariant kernels and maintain a large $l$.
In addition, as RFFs are data independent, this leads to a significantly worse generalization performance than Nystr\"om methods~\cite{yang2012nystrom}.
% how Nystr\"om do
Differently, Nystr\"om methods can be applied to approximate any positive-definite kernel~\cite{deng2022neuralef}.
It resorts to finding the eigenfunctions to form the approximation:
\begin{align*}
	k(\V x_1, \V x_2) = \sum_{i\ge 1} \mu_i \varphi_i(\V x_1)\varphi_i(\V x_2),
\end{align*}
where $\varphi_i$ and $\mu_i\ge 0$ are eigenfunctions and eigenvalues of kernel $ k(\V x_1, \V x_2) $ with respect to the probability measure $q$.
With the top-$l$ eigenpairs, the Nystr\"om method approximate a kernel with $ k(\V x_1, \V x_2) = \sum^l_{i\ge 1} \mu_i \varphi_i(x)\varphi_i(y)$.

% how neuralef do
However, Nystr\"om methods are still costly on medium-sized training data and require $N$-times evaluation of each sample which is not efficient to direct regression of a continuous map. %not acceptable in our application.
%Recently, based on the high regression capability of neural netowrk, NeuralEF propose to use MLPs to model the eigenfunction to further improve the scalibility.
%
%This inspire us to utilize kernel methods into large scale 3D continuous mapping.
%
%However, NeuralEF methods requires eigendecomposition for eigenpairs which requires specifically train. 
%While

We instead encode the local geometry and properties.
%, NeuralEF is not adequate accurate. Thus 
This means a full space approximation is not necessary.
By restricting the approximation to the limited space $\V x\in [-0.5,0.5]^3$, and applying the function in each local region, we overcome the high-computational burden.% and donot training the eigenfunction where the burden of neural model is eased. 
%Instead we learns a mapping to a positive normalized vector without assigning special meaning. 

% More details of our model please find the~\cref{sec:UE}.

