\subsection{Application: Open Vocabulary Scene Understanding}
\label{sec:openvoc}

%\begin{figure}[t]%[htbp]
%	\centering
%	\includegraphics[width=1.\linewidth]{im/scene_understanding}
%	\caption{Scene understanding application pipeline. Here we assume the pose of the frame is pre-known. The upper part of the white line is the fusing of LIM for the feature field. The lower part infers specific semantic information by input text command.}
%	\label{fig:scene_understanding}
%\end{figure}

This application follows OpenScene~\cite{peng2022openscene}, a learning approach that predicts dense features for 3D scene points. 
Where the features are co-embedded with text and image pixels in CLIP feature space.
%
%Different from traditional scene understanding methods, OpenScene donot use labeled 3D dataset for supervision.
%It relies on Language-Driven Semantic Segmentation models [] to provide \textbf{pretrained} scene encoder $\phi_{im}$ and text CLIP encoder $\phi_{text}$. 
%Then zero-shot 3D scene understanding is approached.
%
Inspired by this, we designed the mapping for surface feature fields (~\cref{sec:latent_fields}). 
The difference comparing to surface property fields is shown in ~\cref{fig:latent_diff}.
$\phi$ is taken from pretrained OpenSeg~\cite{ghiasi2022scaling}.
The voxel latent $\V F_m$ is encoded from points and points $\V X_m$ features $\V y_m$.
During inferencing, given open vocabulary text input $\V u$ and position input $\V x_*$, we obtain the CLIP space features.
By computing similarity, the point property is determined.
Pipeline of this application is shown in ~\cref{fig:recons_and_scene_understanding}.

%Though OpenScene is zero-shot methods, it still requires %training a neural network model to distill the feature for 3D %from 2D image features,
%while
% our Universal Continuous Mapping does not need any training.
In this application, image encoding function $f_{im}$ and text encoding function $f_{text}$ are taken from pre-trained model. While $f_{enc}$ and $f_{dec}$ in our Uni-Fusion are actually deterministic functions.
With those functions, our model construct a continues field for the CLIP feature on surface.

An highly-interesting and similar work to Uni-Fusion scene understanding application is VLMaps~\cite{huang2023visual}.
This work produces a 2D map while our model produces a surface CLIP feature field in 3D space.
