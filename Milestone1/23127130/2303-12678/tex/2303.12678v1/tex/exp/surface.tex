\subsection{Reconstruction Results}

For evaluation, we first use the ScanNet validation set with 312 sequences to thoroughly test the geometric reconstruction on a large variant of scenes.
%
Then, we use TUM RGB-D to compare our modified tracking model with related works.
Because this part is not the main contribution, we give a rough overview of the tracking results.
%
To fairly evaluate the color reconstruction, we use the high quality rendered Replica dataset to compare with related works, including NeRF.

%\subsubsection{Object}
% on instance-gp
% Objective data usually has more fine detail
% 1. for detail precision
% A: no, object reconstruction is not as good as instance-ngp, so cancelled.
\begin{table*}[!]
	\centering
	\caption{Comparison to ScanNet~\cite{dai2017scannet}.
       Our method generalizes better to various scenes.
       $^*$ indicates the result from our runs of the official BNV-Fusion code.}
	\small
	%\setlength{\tabcolsep}{5mm}
	\setlength{\tabcolsep}{0.9em}
		%\resizebox{\textwidth}{!}{
		\begin{tabular}{l  c c c| c c c }
			\toprule
			Method & \begin{tabular}{@{}c@{}}Pre-Train\\ with extra dataset\end{tabular} & \begin{tabular}{@{}c@{}}Train \\ with sequences\end{tabular} & Real-time & Accuracy (\%) $\uparrow$ & Completeness (\%) $\uparrow$ & F1 score $\uparrow$ \\
			\midrule
			TSDF Fusion~\cite{zhou2018open3d} & None & None & $\checkmark$ &73.83 & 85.85 & 78.84 \\
			iMAP~\cite{sucar2021imap} & None & Online train& &68.96 & 82.12 & 74.96 \\
			DI-Fusion~\cite{huang2021di} &Object Pretrain & None & $\checkmark$&66.34 & 79.65 & 72.97 \\
			BNV-Fusion~\cite{li2022bnv} &Object Pretrain &  Post Optimization& &{74.90} & \textbf{88.12} & {80.56} \\
			BNV-Fusion$^{*}$~\cite{li2022bnv} &Object Pretrain & Post Optimization &&{73.42} & {81.75} & {77.18} \\
			\textbf{Uni-Fusion (Ours)} &None &None &$\checkmark$&\textbf{80.43} & {84.91} & \textbf{82.44} \\
			\bottomrule
		\end{tabular}
	  %}
	\label{tab:scannet}
	\vspace{-.6cm}
\end{table*}
\begin{figure*}[t]
	\subfloat[width=.33\textwidth][Accuracy]{
		\centering
		\includegraphics[width=.22\linewidth]{im/exp/recons/scannet/scannet_acc.png}
		\includegraphics[width=.1\linewidth]{im/exp/recons/scannet/scannet_acc_box.png}
	}
	\subfloat[width=.33\textwidth][Completeness]{
		\centering
		\includegraphics[width=.22\linewidth]{im/exp/recons/scannet/scannet_comp.png}
		\includegraphics[width=.1\linewidth]{im/exp/recons/scannet/scannet_comp_box.png}
	}
	\subfloat[width=.33\textwidth][F1 score]{
		\centering
		\includegraphics[width=.22\linewidth]{im/exp/recons/scannet/scannet_F1.png}
		\includegraphics[width=.1\linewidth]{im/exp/recons/scannet/scannet_F1_box.png}
	}
	\label{fig:recon:scannet:elementwise}
	\caption{Quantitative comparison on 312 scenes of the ScanNet validation set.
       We demonstrate the performance of SOTA BNV-Fusion and our Uni-Fusion.
       We sort our evaluation value and reordered all of the scores.
       The zigzag pink is the BNV-Fusion result;
       we also plot a deep-pink smoothed curve for better visualization.}
\end{figure*}

\subsubsection{Evaluation on ScanNet Dataset~\cite{dai2017scannet}}
\label{sec:exp:scannet}

We use the 312 diversified scenes from the ScanNet validation set to evaluate the performance of surface reconstruction. 
We follow the pure mapping SOTA BNV-Fusion to take every 10th posed frame as input. 
%
Without using any learning (in contrast iMAP, DI-Fusion, and BNV-Fusion do) or any post optimization (as BNV-Fusion does), our Uni-Fusion is capable to achieve precise continuous mapping performance. 

As shown in~\cref{tab:scannet}, our Uni-Fusion achieves \textbf{$+6$ higher accuracy} than the incremental surface reconstruction SOTA BNV-Fusion.
Our model does not exceed on completeness comparing to BNV-Fusion that support completion in post-optimization.
Though, Uni-Fusion's completion is still much higher than one other optimization based model iMAP.
%We consider it because our model does not support hole-completion as the optimization based models iMap and BNV-Fusion.
Overall, our Uni-Fusion model achieves higher F1-scores that quantifies the overall quality.

Please note that, SOTA BNV-Fusion is not real-time capable, since it requires post optimization of all fed frames.
Without the post-optimization, the real-time model Di-Fusion shows much worse results.
However, our \textbf{real-time} model \textbf{Uni-Fusion} is able to achieves \textbf{much better} reconstruction quality than these approaches even without post-optimization. 

\newcommand{\scannetImSize}{.16}
\begin{figure*}[t!]
	\centering
	\setlength{\tabcolsep}{0.1em}
	\renewcommand{\arraystretch}{.1}
	\begin{tabular}{|c | c |c |||c |c | c|}
		\hline
		{\Large{BNV-Fusion}} & {\Large{Uni-Fusion}} &{\Large{Ground Truth}} & {\Large{BNV-Fusion}} &{\Large{Uni-Fusion}} & {\Large{Ground Truth}} \\ \hline \hline
		
\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0568_00_bnv.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0568_00_mine.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0568_00_gt.png}
		&		\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0164_00_bnv.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0164_00_mine.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0164_00_gt.png}\\
		
\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0249_00_bnv.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0249_00_mine.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0249_00_gt.png}
		&		\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0435_00_bnv.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0435_00_mine.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0435_00_gt.png}\\
		
\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0046_00_bnv.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0046_00_mine.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0046_00_gt.png}
		&		\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0050_00_bnv.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0050_00_mine.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0050_00_gt.png}\\
		\hline
	\end{tabular}
	%\captionof{figure}
	\caption{Surface reconstruction on ScanNet dataset.}
	\label{fig:recons:scannet_demo}
	\vspace{-.5cm}
\end{figure*}

We additionally run BNV-Fusion's official implementation (emphasized with $^*$) on the 312 videos of ScanNet and do a post element-wise comparison in \cref{fig:recon:scannet:elementwise}. 
Our result is the {\color{Cyan}light blue} curve, BNV-Fusion's result is colored with {\color{Lavender}pink}.
Scene index is sorted corresponding to the score value of Uni-Fusion.
For better visualization, we smooth BNV-Fusion's curve and plot it with dark pink.
It is obvious that the score of Uni-Fusion is overall higher than BNV-Fusion's. 
Moreover, we use box-plot to conclude the statistics besides the curve plot. Uni-Fusion's scores are distributed in a higher region. For completeness which is less obvious better, Uni-Fusion's box is smaller while in a relative higher position. This means that Uni-Fusion has more stable completeness result while BNV-Fusion is more likely to get low completeness in some cases.

To summarize, our model is almost better on all 312 scenes on all accuracy, completeness and F1-score.
Which is also revealed in \cref{tab:scannet} with BNV-Fusion$^*$, that the BNV-Fusion's official implementation does not exceed Uni-Fusion on all metrics.

We plot reconstruction on selected scenes from ScanNet in~\cref{fig:recons:scannet_demo}. 
Both BNV-Fusion and our Uni-Fusion are able to produce high quality reconstruction.
We see that BNV-Fusion gives lots of small meshes on walls, which are shown as small particles in the reconstruction. 
We consider it is because BNV-Fusion use very small voxel size ($0.02\si{\meter}$) to get a high score.
This is also revealed by their \textbf{\SI{247}{MB}} mesh in average, while ours is only \textbf{\SI{54}{Mb}} in average.
Furthermore, our Uni-Fusion's mesh is more smooth and
%Both BNV-Fusion and Uni-Fusion demonstrate high quality result.
also provides high-precise color to the mesh which is not available for the Surface SOTA.

%In this test, we purely evaluate the surface reconstruction capacity with SOTAs. 
%While reconstruction is not merely surface.  
%Thus in the following, we find benchmarks for both surface and color.


\subsubsection{Tracking Evaluation on TUM RGB-D Dataset~\cite{sturm2012benchmark}}
% follow nice-slam

In the above test, we compare the performance of pure mapping.
While tracking is not the contribution focus in our paper, it is part of the reconstruction model. We follow the novel reconstruction model NICE-SLAM~\cite{zhu2022nice} to evaluate the camera tracking on the small-scale TUM RGB-D dataset.
Our Uni-Fusion uses a coarse-to-fine strategy for 3D reconstruction tracking.
From~\cref{tab:tum_rmse}, it demonstrates overall better ATE RMSE than other implicit representation models.

\begin{table}[]
		\caption{Tracking on TUM RGB-D~\cite{sturm2012benchmark}.
		ATE RMSE [$\si{\centi\meter}$] ($\downarrow$) is used as the evaluation metric.
	}
	\centering
	\footnotesize
	\setlength{\tabcolsep}{0.7em}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{l|ccc}
			\hline
			& \tt{fr1/desk} &  \tt{fr2/xyz} &  \tt{fr3/office} \\
			
			\hline
			{iMAP}~\cite{sucar2021imap}      & 4.9 & 2.0 & 5.8  \\
			{iMAP$^*$}~\cite{sucar2021imap} & 7.2 & 2.1  & 9.0 \\
			{DI-Fusion~\cite{huang2021di}} & 4.4 & 2.3 & 15.6 \\
			NICE-SLAM~\cite{zhu2022nice}           & 2.7 & 1.8 & 3.0 \\
			Ours& 1.8& 0.5& 2.1 \\
			\hline
			{BAD-SLAM}\cite{schops2019bad} & 1.7  & 1.1  & 1.7 \\
			{Kintinuous}\cite{whelan2012kintinuous} & 3.7  &  2.9  & 3.0 \\
			{ORB-SLAM2}\cite{mur2017orb} & \bf 1.6  & \bf 0.4  & \bf 1.0 \\
			\hline
	\end{tabular}}
	\vspace{2pt}

	\label{tab:tum_rmse}
\end{table}

On the other hand, there also exist high accuracy algorithms from SLAM. 
By additional using Bundle Adjustment and Loop-closing techniques, their tracking quality is much better than all of the implicit based models.

%But it is dangerous to directly apply SLAM result on reconstruction. Please find our demonstration in Fig [?]. Which explains the more widely used frame-to-model strategy in 3D reconstruction.
Even though, our coarse-to-fine strategy firstly ensure not easy to lose track. Secondly, it is more suitable for surface fitting.

Which further support our test in Replica dataset.



\begin{table*}[t!]
	\centering
	\caption{Geometric (L1) and Photometric (PSNR) evaluation on the Replica dataset~\cite{sucar2021imap}.}
	\footnotesize
	\setlength{\tabcolsep}{0.36em}
	\renewcommand{\arraystretch}{1.2}
	\begin{tabular}{clcccccccccccccccccc}
		\toprule
		& & \multicolumn{1}{c}{\makecell{\tt{office-0}}} & \multicolumn{1}{c}{\makecell{\tt{office-1}}} & \multicolumn{1}{c}{\makecell{\tt{office-2}}}& \multicolumn{1}{c}{\makecell{\tt{office-3}}} & \multicolumn{1}{c}{\makecell{\tt{office-4}}} & \multicolumn{1}{c}{\makecell{\tt{room-0}}} & \multicolumn{1}{c}{\makecell{\tt{room-1}}} &  \multicolumn{1}{c}{\makecell{\tt{room-2}}} & Avg. \\
		\midrule
		\multicolumn{5}{l}{\textit{Non-continuous mapping method}}\\
		\multirow{2}{*}{\makecell{\textbf{TSDF-Fusion}~\cite{curless1996volumetric}}}
		& {\bf Depth L1} [$\si{\centi\meter}$] $\downarrow$
	 & 14.11 & 10.50 & 30.89 & 28.92 & 22.83	& 23.51 & 20.94 & 23.34 & 21.88 \\
		& {\bf PSNR } [$\si{\dB}$] $\uparrow$
		& 11.16 & 15.92 & 4.86 & 5.68 & 5.46 & 3.43 & 4.51 & 5.57 & 7.07 \\
		
		\midrule
		\multirow{2}{*}{\makecell{\textbf{$\sigma$-Fusion}\cite{rosinol2023probabilistic} }}
		& {\bf Depth L1} [$\si{\centi\meter}$] $\downarrow$
		 & 13.80 & 10.21 & 22.27 & 28.70 & 22.21& 21.92 & 19.28 & 22.40 & 20.10 \\
		& {\bf PSNR } [$\si{\dB}$] $\uparrow$
		 & 11.16 & 15.92 & 4.86 & 5.69 & 5.46& 3.45  & 4.51 & 5.57 & 7.08 \\
		
		
		
		
		
		\midrule
		\midrule
		\multicolumn{5}{l}{\textit{Continuous mapping method}}\\
		\multirow{2}{*}{\makecell{\textbf{iMAP$^*$}~\cite{sucar2021imap}}}
		& {\bf Depth L1} [$\si{\centi\meter}$] $\downarrow$
		 & 6.43 & 7.41 & 14.23 & 8.68 & 6.80& 5.70 & 4.93 & 6.94 & 7.64\\
		& {\bf PSNR } [$\si{\dB}$] $\uparrow$
		& 7.39 & 11.89 & 8.12 & 5.62 & 5.98& 5.66 & 5.31 & 5.64  & 6.95\\
		\midrule
		\multirow{2}{*}{{\makecell{\textbf{Nice-SLAM}~\cite{zhu2022nice} }}}
		& {\bf Depth L1} [$\si{\centi\meter}$] $\downarrow$
		& { 1.51 } & { 0.93 } & { 8.41 } & { 10.48 } & {2.43} & { 2.53 } & { 3.45 } & { 2.93 }  & { 4.08 } \\
		& {\bf PSNR } [$\si{\dB}$] $\uparrow$
		 & { 22.44 } & { 25.22 } & { 22.79 } & { 22.94 } & { 24.72 } & \textbf{ 29.90 } & \textbf{ 29.12 } & { 19.80 }& { 24.61 } \\
		
		
		
		
		\midrule	
		\multirow{2}{*}{{\makecell{\textbf{Uni-Fusion} (Ours) }}}
		% using abs(diff)
		%	& {\bf Depth L1} [$\si{\centi\meter}$] $\downarrow$ &\textbf{1.98}&\textbf{1.18}&\textbf{1.64}&\textbf{1.23}&\textbf{0.84}&\textbf{1.61}&\textbf{3.01}&\textbf{1.60} &\textbf{1.64}
		% follow nerf-slam to remove outlier gt first
		& {\bf Depth L1} [$\si{\centi\meter}$] $\downarrow$
		& \textbf{0.79}&\textbf{0.56}&\textbf{1.59}&\textbf{2.71}&\textbf{1.66}&\textbf{1.94}&\textbf{0.69}&\textbf{1.80}& \textbf{1.47}
		\\
		& {\bf PSNR } [$\si{\dB}$] $\uparrow$ &\textbf{33.88}&\textbf{33.31}&\textbf{25.84}&\textbf{26.01}&\textbf{28.14}&24.02&26.20&\textbf{27.17} &\textbf{28.07}
		\\
		
		\midrule
		\midrule
		\multicolumn{5}{l}{\textit{Neural radiance field method}}\\
		\multirow{2}{*}{{\makecell{\textbf{NeRF-SLAM}~\cite{rosinol2022nerf} }}}
		& {\bf Depth L1} [$\si{\centi\meter}$] $\downarrow$
	 & {2.49}   & {1.98}  & {9.13}  & {10.58} & {3.59}	& {2.97}  & {2.63}  & {2.58}  & {4.49} \\
		& {\bf PSNR } [$\si{\dB}$] $\uparrow$
	 & \textbf{48.07}  & \textbf{53.44} & \textbf{39.30} & \textbf{38.63} & \textbf{39.21} 	& \textbf{34.90} & \textbf{36.95} & \textbf{40.75}& \textbf{41.40} \\
		
		\bottomrule
	\end{tabular}%
	
	\label{tab:replica_per_scene}
\end{table*}


\begin{table*}[t!]
	\centering
	\caption{Differences among different Surface \& Color reconstruction models.}
	\small
	\setlength{\tabcolsep}{.6em}
	%{
		%\resizebox{\textwidth}{!}{
			\begin{tabular}{l | c c c c c c }
				\toprule
				Method & 
				\begin{tabular}{@{}c@{}}Pre-Train\\ with extra dataset\end{tabular}
				& \begin{tabular}{@{}c@{}}Train\\ with sequences\end{tabular}
				& Real-time	
				& Direct Output &  \begin{tabular}{@{}c@{}}Light\\ direction\end{tabular} 
				&Render\\
				\hline 
				TSDF-Fusion & None & None & $\checkmark$& Discrete TSDF &  &Ray Rasterization\\\hline
				$\sigma$-Fusion & None & None &$\checkmark$&Discrete TSDF  && Ray Rasterization\\\hline
				iMAP & None & Online Train && MLPs  & &Volumetric Rendering\\\hline
				NICE-SLAM & \begin{tabular}{@{}c@{}}Pretrain\\ with indoor dataset\end{tabular} & Online Train&& Neural Implicit Grid&  & Volumetric Rendering\\\hline
				
				NeRF-SLAM & None & Train hundred epochs &-&NeRF & $\checkmark$ &Volumetric Rendering \\\hline
				
				\textbf{Uni-Fusion} & None & None&$\checkmark$& Latent Implicit Map && Ray Rasterization\\				
				\hline
			\end{tabular}
		%}
	%}
	\label{tab:replica_diff}
\end{table*}
\subsubsection{Evaluation on Replica RGB-D Dataset~\cite{sucar2021imap}}
In this evaluation, we compare with implicit reconstruction (TSDF-Fusion, $\sigma$-Fusion) and latent implicit reconstruction models (iMAP, NICE-SLAM) that support colors. 
We also add a large-scale NeRF model, NeRF-SLAM in to the table.  
NeRF is SOTA in view-synthesis task, which is unfair to direct compare with the rest. As the rest model does not even model light directions.
We add NeRF in this part to demonstrate that Uni-Fusion strongly reduce the gap.
Note that, NeRF-SLAM embeds external tracking model ~\cite{teed2021droid} to provide poses while using SOTA NeRF implementation Instance-ngp~\cite{muller2022instant} for NeRF construction.
%Therefore it is considered the SOTA to model the colors.

Uni-Fusion track and follow our previous setting in ScanNet test to take every 10 frames for mapping.
NICE-SLAM and NeRF-SLAM produce depth and color by rendering,
To obtain result from Uni-Fusion, we cast rays from virtual camera to our result surface mesh for depth image. 
Then Uni-Fusion infer the cast points in Uni-Fusion's color LIM for color result.

From~\cref{tab:replica_per_scene}, Uni-Fusion demonstrate
best Depth L1 on all scenes with an average of \textbf{$\pmb{1.47}$$\si{\centi\meter}$ depth L1}. Which is \textbf{$\pmb{177\%}$ boost} comparing to the second best.

Moreover, excluding NeRF, our Uni-Fusion also shows the best performance to model the colors with an average of $28.07$$\si{\dB}$ PSNR.

However, it is strange that NICE-SLAM lost details while in two cases, it shows better PSNR than Uni-Fusion. 
To highlight the true result,
we plot the rendered image in \cref{fig:replica_render}.
It is obvious that our Uni-Fusion models the details of painting, carpet and quilt well. 
While NICE-SLAM just roughly models the average color.

Moreover, from the  \cref{fig:replica_render}, our Uni-Fusion's rendering quality is as precise as NeRF. 
Please also find the painting, carpet and quilt, Uni-Fusion recovered the original appearance well.
Please find the {\color{green} green window} for the emphasized region.
Uni-Fusion reproduce the high-quality appearance which is very close to NeRF on qualitative evaluation.
%It can hardly find difference between the results from NeRF-SLAM, Uni-Fusion and Ground Truth.
%
But, Uni-Fusion still has a quantitative score gap to the color rending of NeRF ($41.4$$\si{\dB}$).
Though the Uni-Fusion's rendering result is highly close to NeRF and ground truth.
%
We consider the main reasons are that \textbf{1.} Uni-Fusion does not model the light directions to points, which is essential to NeRF.
\textbf{2.} NeRF optimizes on the rendering image quality by focussing mainly on color while less on depth.
It can be revealed by the higher color rendering score with much worse depth rendering than our Uni-Fusion.
\textbf{3.} our Uni-Fusion does not support hole filling.
This directly leads to black holes in our rendered images.

We summarize the differences in \cref{tab:replica_diff}.
Similar to TSDF-Fusion and $\sigma$-Fusion, our Uni-Fusion is a forward method which, does not need any training, i.e., pre- or online training. 
Uni-Fusion also produces similar to NICE-SLAM and NeRF-SLAM an implicit map with set of latent that outputs results at arbitrary resolution.
However, we differ on the extracting of the signed distance field.
%FIXME: I do not understand the next sentence.
Uni-Fusion's latent feature rule its own region independently.
Each query value is directly inferred with the corresponding ruling latent.
While NICE-SLAM and NeRF-SLAM use a much denser grid to interpolate query features. This requires volumetric rendering for inference.

Similar to TSDF-Fusion, $\sigma$-Fusion, our Uni-Fusion is also a real-time algorithm.
iMAP, NICE-SLAM and NeRF-SLAM run hardly in real-time.
NeRF-SLAM is claiming to be real-time, which is questionable as they still need hundreds of epochs training after feeding the data.

Nevertheless, optimization with backpropagation learns pixel-to-pixel well.
It is theoretically advanced for a regression-and-fusion strategy. 
Though Uni-Fusion demonstrates its high capability to model the color, NeRF-like post-optimization would still be a good direction for further improvements of Uni-Fusion.

\newcommand{\replicaImSize}{.24}
\begin{figure*}[t]
	\centering
	\setlength{\tabcolsep}{0.1em}
	\renewcommand{\arraystretch}{.1}
	\begin{tabular}{|c | c |c |c| }
		 \hline
		{\Large{NICE-SLAM}} &{\Large{NeRF-SLAM}}&\textbf{\Large{Uni-Fusion}}&\Large{Ground Truth}\\
		%		\hline
		%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nice-slam/of2_1286.png} &
		%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/of2_1286.jpg} &
		%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/of2_1286.jpg} &
		%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/gt/of2_1286.jpg} \\
		
		\hline
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nice-slam/rm0_769_window.png} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nerf-slam/rm0_769_window.jpg} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/rm0_769_window.jpg} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/gt/rm0_769_window.jpg} \\
		\hline
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nice-slam/of3_575_window.png} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nerf-slam/of3_575_window.jpg} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/of3_575_window.jpg} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/gt/of3_575_window.jpg} \\
		\hline
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nice-slam/rm1_425_window.png} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nerf-slam/rm1_425_window.jpg} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/rm1_425_window.jpg} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/gt/rm1_425_window.jpg} \\
		\hline
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nice-slam/rm2_1085_window.png} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nerf-slam/rm2_1085_window.jpg} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/rm2_1085_window.jpg} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/gt/rm2_1085_window.jpg} \\		
		
	\end{tabular}
	%\captionof{figure}
	\caption{Demonstration of color rendering on the Replica dataset. Fine appearances are highlighted in {\color{green}green window}. Small flaws are in a {\color{red}red} box.}
	\label{fig:replica_render}
	\vspace{-.5cm}
\end{figure*}

%(2) NeRF model learning radiance field that model the light on different direction on surface. While Uni-Fusion naturally treat different directional light the same color.

%\begin{table*}[t!]
%	\centering
%	\setlength{\tabcolsep}{0.1em}
%	\renewcommand{\arraystretch}{.1}
%	\begin{tabular}{c | c |c |c |c }
%		\hline 
%		\rotatebox{90}{\large{NICE-SLAM}} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nice-slam/of3_575.png} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nice-slam/rm0_769.png} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nice-slam/rm1_425.png} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nice-slam/rm2_1085.png} \\
%		\hline
%		\rotatebox{90}{\large{NeRF-SLAM}} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/of3_575.jpg} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/rm0_769.jpg} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/rm1_425.jpg} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/rm2_1085.jpg} \\	
%		\hline
%		\rotatebox{90}{\textbf{\Large{Uni-Fusion}}} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/of3_575.jpg} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/rm0_769.jpg} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/rm1_425.jpg} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/rm2_1085.jpg} \\
%		\hline
%		\rotatebox{90}{\large{Ground Truth}} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/gt/of3_575.jpg} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/gt/rm0_769.jpg} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/gt/rm1_425.jpg} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/gt/rm2_1085.jpg} \\
%		\hline		
%		
%	\end{tabular}
%	\captionof{figure}{Demonstration of color rendering on Replica dataset.}
%\end{table*}

\subsection{Ablation study}
\label{exp:surface:ablation}

\begin{figure}[]
	\centering
%		\subfloat[width=\textwidth][Sample based]{
%		\centering
%		\includegraphics[width=.7\linewidth]{im/exp/ablation/GPIS/seq3_sample_color.png}
%	}\\
%	\subfloat[width=\textwidth][Derivative based]{
%		\centering
%		\includegraphics[width=.7\linewidth]{im/exp/ablation/GPIS/seq3_derivative_color.png}
%	}
		\includegraphics[width=.7\linewidth]{im/exp/ablation/GPIS/seq3_sample_color_a.png}
		\includegraphics[width=.7\linewidth]{im/exp/ablation/GPIS/seq3_derivative_color_b.png}
	\caption{Ablation study on surface construction basis. (a) Sample based. (b) Derivative based.}
	\label{fig:ablation:GPIS}
\end{figure}


\begin{table}[]
	\caption{Ablation study on tracking.
	}
	\centering
	\footnotesize
	\setlength{\tabcolsep}{0.7em}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{l|ccc}
			\hline
			& \tt{fr1/desk} &  \tt{fr2/xyz} &  \tt{fr3/office} \\
			\hline
			External& 2.1& 0.5& 2.5 \\
			External+Internal&1.8& 0.5& 2.1 \\
			\hline
	\end{tabular}}
	\vspace{-2pt}
	%\vspace{-1cm}
	\label{tab:tum_rmse2}
\end{table}

\begin{figure}
	\centering
	\includegraphics[width=.7\linewidth]{im/exp/ablation/voxel_size/seq_voxel_size.png}
	%	\subfloat[width=.33\textwidth][0.1]{
		%		\centering
		%		\includegraphics[width=.33\linewidth]{im/exp/ablation/voxel_size/seq3_0_1_color.png}
		%	}
	%	\subfloat[width=.3\textwidth][0.05]{
		%		\centering
		%		\includegraphics[width=.33\linewidth]{im/exp/ablation/GPIS/seq3_sample_color.png}
		%	}
	%	\subfloat[width=.33\textwidth][0.02]{
		%	\centering
		%	\includegraphics[width=.33\linewidth]{im/exp/ablation/voxel_size/seq3_0_02_color.png}
		%	}
	\caption{Ablation study on voxel size.}
	\label{fig:ablation:voxel_size}
\end{figure}




 \newcommand{\styleImSize}{.2}
\begin{figure*}[b!]
	\vspace{-.5cm}
	\centering
	\setlength{\tabcolsep}{0.1em}
	\renewcommand{\arraystretch}{.1}
	\resizebox{\textwidth}{!}{\begin{tabular}{ccccc}
			%		\includegraphics[width=\styleImSize\line]{im/exp/style/style/0} &
			%		\includegraphics[width=\styleImSize\linewidth]{im/exp/style/style/1} &
			%		\includegraphics[width=\styleImSize\linewidth]{im/exp/style/style/2} &
			%		\includegraphics[width=\styleImSize\linewidth]{im/exp/style/style/3} &
			%		\includegraphics[width=\styleImSize\linewidth]{im/exp/style/style/4} &
			%		\includegraphics[width=\styleImSize\linewidth]{im/exp/style/style/5} &
			%		\includegraphics[width=\styleImSize\linewidth]{im/exp/style/style/6} \\
			\hline\hline
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_0.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_1.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_2.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_3.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_4.png} \\
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_5.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_6.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_7.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_8.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_9.png} \\
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_10.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_11.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_12.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_13.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_14.png} \\
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_15.png} &
			%\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_16.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_17.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_18.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_19.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_20.png} 
			\\	\hline
		\end{tabular}
	}
	%\captionof{figure}
	\caption{Style transfer on 3D canvas.}
	\label{fig:style}
\end{figure*}


\subsubsection{ Sample-based or Derivative-based}

We select the surface model with our own captured sequences. 
All settings are detailed in \cref{sec:exp:details}.
As shown in~\cref{fig:ablation:GPIS}, reconstruction of Yijun's office is demonstrated. 
Both models are able to construct, but the derivative-based model produces a lot of noise on the surface.
This is because for smoothness purpose, we build voxels that are overlapped to its neighbor, which causes redundant voxels near the surface.
For those redundant voxels, no center sample is provided and thus the derivative based surface construction builds bad SDFs on unknow region of the voxels.


Instead, sample-based surface construction does not have this problem as it adds more points in voxels and is able to construct highly-smooth surfaces.
From which, we find well constructed and colored white board, chair, school bag and even the oranges.

\subsubsection{Tracking}


Our Uni-Fusion use a coarse-to-fine strategy for tracking. 
An external tracking model is running in one thread aside from the mapping thread.
In the mapping thread, it takes pose result from the external tracking and applies the internal tracking for colored point cloud.

The result is demonstrated in~\cref{tab:tum_rmse2}. 
The coarse-to-fine is relatively better on trajectory estimation.

\subsubsection{Voxel size}

Testing the office scene, we vary the voxel size from low to high. 
From~\cref{fig:ablation:voxel_size}, when low voxel size $0.02$m is used, the surface is rough.
Then when voxel size goes larger, the smoothness is improved.
However, when we use $0.1$m voxel size, the surface color is blur. 
Considering Uni-Fusion produces a surface color field, the quality of surface directly affect the coloring.
Thus, continuing enlarging the voxel size also results in worse surface results.

Therefore, in the above experiments, $0.05$m voxel size is utilized for surface construction.
In addition, each voxel for encoding are actually with size $0.1$m, since we use overlapped voxel.

%\subsubsection{Anchor number and feature dimension}

