\section{Related Works} % page 2
\label{sec:related_works}

We first discuss the development of continuous mapping from 2D scenes to 3D scenes. 
Then we explore the importance of recent state-of-the-art (SOTA) reconstructions utilizing neural implicit models.
Next, we examine the development of kernel methods that are closely related to our approach.

\subsection{Continuous Mapping}

The practice of continuous mapping originated in 2D scenarios.
Gaussian Process Occupancy Maps (GPOM)~\cite{o2012gaussian} use Gaussian Process Regression (GPR) to predict a continuous representation, allowing the construction of maps at arbitrary resolutions. 
To improve the scalability, Kim et al.~\cite{kim2013continuous} employ a divide and conquer strategy where GP is applied in each cluster.
Then, the incremental approach to GPOM is employed by leveraging the Bayesian Committee Machine (BCM) technique~\cite{ghaffari2018gaussian,yuan2018fast}.

Meanwhile, the Hilbert Maps approach~\cite{senanayake2017bayesian,zhi2019continuous} has been introduced. This approach is a mapping technique that does not require an explicit formulation.
Hilbert Maps achieve continuous mapping by continuously optimizing parameters. 

Due to the growing popularity of 3D sensors, there has been a shift in focus towards 3D scenarios.
Gaussian Process Implicit Surface (GPIS), has emerged in the field~\cite{martens2016geometric,lee2019online,wu2021faithful,ivan2022online}.
These methods,
given zero-level surface points, either sample points along the normal direction and assign distance values, or directly use derivative models with normals as labels (we will discuss this in more detail in our mapping~\cref{sec:surface_mapping}).
%
However, these methods focus primarily on shapes rather than entire scenes.
This is due to the fact that GPR-based methods naturally incur high computational costs when 
dealing with large amounts of data.
This is especially true when moving from 2D to 3D testing scenarios.
Therefore, we leverage the concept of Neural Implicit Maps~(\cref{sec:related:NIM}) to address this challenge and focus on scene reconstruction.

\subsection{Neural implicit based Reconstruction}
\label{sec:related:NIM}

Neural implicit reconstruction was originally introduced for SDF and occupancy-based reconstruction.
The seminal work by Park et al.~\cite{park2019deepsdf}, known as DeepSDF, employs a deep model to encode geometry prior using multi-layer perceptrons (MLPs) and extracts discretized SDF through decoding queries.
It then uses an algorithm similar to Marching Cubes~\cite{lorensen1987marching} for mesh extraction. 
For the latter, Occupancy Networks~\cite{mescheder2019occupancy} learn to estimate the occupancy probability at positions using an implicit function.
The Multiresolution IsoSurface Extraction (MISE) technique is used to generate meshes.
\begin{figure}[b]
	\centering
	\includegraphics[width=.8\linewidth]{im/PLV}
	\caption{Uniform, sparse voxels. Each voxel is encoded into one feature vector $\V F_m$~\cite{huang2021di}.}
	\label{fig:PLV}
\end{figure}

To enhance the efficiency of the reconstruction, 
DeepLS~\cite{chabra2020deep} uses multiple local deep priors and reconstructs based on a set of local SDFs. 
Jiang et al.~\cite{jiang2020local} additionally proposes the use of a local implicit grid to further simplify the model complexity of the encoding.
Conversely, Convolutional Occupancy Networks~\cite{peng2020convolutional} explore the local latent by replacing the encoder-decoder with optimization on a grid of local features, thereby alleviating the burden of MLPs. 

More recently, Neural Implicit Maps (NIM) have achieved real-time 3D reconstruction of real scenes.
Huang et al. propose DI-Fusion~\cite{huang2021di}, a neural implicit mapping method that performs incremental scene reconstruction.
DI-Fusion achieves high memory efficiency and produces improved reconstructions, by fusing on maps of latent features.
Similarly, NeuralBlox~\cite{lionar2021neuralblox} fuses a grid of latent features with known external pose estimation.
BNV-Fusion~\cite{li2022bnv} further improves feature quality by incorporating post-fusion optimization.
To address large scene reconstructions that may involve loops, NIM-REM~\cite{yuan2022algorithm} introduces an SE(3)-transformation algorithm for NIM and develops a mapping\&remapping model that works in conjunction with bundle adjustment and loop closing.
Thinking outside the box, Sucar et al.~\cite{sucar2021imap} propose iMAP, a novel SLAM pipeline that incorporates neural implicit-based incremental reconstruction.
Using a differentiable rendering model, iMAP performs online optimization of the reconstruction by minimizing the image distances.
Zhu et al. present NICE-SLAM~\cite{zhu2022nice}, which replaces MLPs with a convolutional occupancy grid to further improve the efficiency and quality of the reconstruction.

In this work, our model also uses regularly spaced voxels, as shown in~\cref{fig:PLV}, for local encoding-decoding. 
Since we propose to use a single model for all properties, pre-training such a model is not feasible.
Therefore, we employ a kernel method to achieve this objective.

\subsection{Kernel Function Approximation}

Kernel methods suffer from a scalability issue due to the $\mathcal{O}(n^3)$ time complexity required during regression.
One possible solution is kernel matrix approximation, but this is beyond the scope of this paper.
Another solution, kernel function approximation, aims to enhance the scalability of kernel methods by employing explicit vector maps. For example, kernel function $k(\V x_1, \V x_2) \approx   v(\V x_1)^T  v(\V x_2)$ with mapping function %$\pmb 
$v:\mathbb{X}\rightarrow \mathbb{R}^l $~\cite{deng2022neuralef}.
%
Two approaches are commonly used for approximation: Random Fourier Features (RFFs)~\cite{rahimi2007random,rahimi2008weighted,yu2016orthogonal,munkhoeva2018quadrature,francis2021major} and Nystr\"om methods~\cite{francis2021major,williams2000using,deng2022neuralef}. 

% how RFFs do
RFFs explicitly handle the shift-invariant kernels by mapping the data using the Fourier transform technique~\cite{francis2021major}.
However, RFFs are primarily employed for shift-invariant kernels and require a large $l$ for their operation.
Furthermore, since RFFs are data-independent, they exhibit significantly worse generalization performance compared to Nystr\"om methods~\cite{yang2012nystrom}.
% how Nystr\"om do
In contrast, Nystr\"om methods can approximate any positive definite kernel~\cite{deng2022neuralef}.
It relies on finding the eigenfunctions to form the approximation:
\begin{align*}
	k(\V x_1, \V x_2) = \sum_{i\ge 1} \mu_i \varphi_i(\V x_1)\varphi_i(\V x_2),
\end{align*}
where $\varphi_i$ and $\mu_i\ge 0$ are eigenfunctions and eigenvalues of kernel function $k$ with respect to the probability measure $q$.
With the top-$l$ eigenpairs, the Nystr\"om method approximates the kernel function with $ k(\V x_1, \V x_2) = \sum^l_{i\ge 1} \mu_i \varphi_i(\V x_1)\varphi_i(\V x_2)$.

% how neuralef do
However, Nystr\"om methods are computationally expensive for medium-sized training data and require evaluating each sample $N$ times, which is inefficient for direct regression of a continuous map.

Instead, we encode the local geometry and properties, eliminating the need for a full-space approximation.
By restricting the approximation to the limited space $\V x\in [-0.5,0.5]^3$, and applying the function in each local region, we reduce the computational burden.
