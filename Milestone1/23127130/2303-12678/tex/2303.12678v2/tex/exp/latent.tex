\begin{table*}[b!]
	%\renewcommand{\arraystretch}{0.9}
	%\setlength{\tabcolsep}{3pt}
	\caption{GZSL semantic segmentation results. Scores are in \%.
	  $^\dagger$ indicate 3DGenZ's adaption of the method.
       Note that, Uni-Fusion-SU does not even train with the seen classes.}
	\centering
	\begin{tabular}{l|c|c |c ||ccc|ccc}
		\toprule
		\multicolumn{1}{c}{}& \multicolumn{2}{c|}{Training set} & Inference input &\multicolumn{3}{c|}{ScanNet } & \multicolumn{3}{c}{S3DIS}\\
		& Backbone & Classifier & &$Seen$& $Unseen$ & $All$&$Seen$& $Unseen$ & $All$
%		\multicolumn{3}{c|}{mIoU} & 
%		\multicolumn{3}{c|}{mIoU} \\ 
%		&&&& $Seen$& $Unseen$ & $All$&$Seen$& $Unseen$ & $All$
		%\cellcolor{white}{}  
		%\cellcolor{white}{}
		\\
		\midrule
		
		\multicolumn{5}{l}{\textit{Supervised methods with different levels of supervision}}\\
		
		Full supervision & $seen \cup unseen$ & $seen \cup unseen$ & Point Cloud &43.3&51.9 &45.1&74.0&50.0&66.6 \\
		
		ZSL backbone & $seen$ & $seen \cup unseen$  &Point Cloud&41.5&39.2 & 40.3&60.9& 21.5&  48.7 \\
		
		ZSL-trivial & $seen$ & $seen$ &Point Cloud&39.2&0.0&31.3&70.2 &0.0&48.6  \\
		\midrule
		\multicolumn{5}{l}{\textit{Generalized zero-shot-learning methods}}\\
		
		ZSLPC-Seg~\cite{cheraghian2019zero}$^\dagger$ & $seen$ & $unseen$  &Point Cloud&28.2&0.0& 22.6&65.6 &0.0& 45.3\\
		
		DeViSe-3DSeg~\cite{frome2013devise}$^\dagger$ & $seen$ & $unseen$   &Point Cloud &20.0&0.0&16.0&70.2&0.0& 48.6\\ 
		%ZSLPC-Seg~\cite{cheraghian2019zero}$^\dagger$ & $seen$ & $unseen$  &  4.0&13.9\\
		%DeViSe-3DSeg~\cite{frome2013devise}$^\dagger$ & $seen$ & $unseen$   &  3.0&10.9\\
		3DGenZ~\cite{michele2021generative} & $seen$ & $seen \cup \hat{unseen}$  &Point Cloud &32.8&7.7& {27.8}&53.1&7.3&   \textbf{39.0} \\
		\midrule
		\multicolumn{5}{l}{\textit{Zero-shot learning + map fusion}}\\
		Uni-Fusion-SU (Ours) &None&None&Sparse Frames&31.0&\textbf{41.9}&\textbf{32.9} &31.3&\textbf{24.0}&29.0\\
		\bottomrule
		\multicolumn{1}{l}{}\\[-7pt]
	\end{tabular}

	\label{tab:sem_seg_overview}
\end{table*}

\begin{figure*}[t!]
	\centering
	\setlength{\tabcolsep}{0.1em}
	\renewcommand{\arraystretch}{.1}
	\begin{tabular}{|c | c |c |||c |c | c|}
		\toprule
		{\Large{3DGenZ}} & {\Large{Uni-Fusion}} &{\Large{Ground Truth}} & {\Large{3DGenZ}} &{\Large{Uni-Fusion-SU}} & {\Large{Ground Truth}} \\ \midrule
		
		\includegraphics[width=\scannetImSize\linewidth]{im/exp//ss/gen3dz_0568.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp//ss/mine_0568.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp//ss/gt_0568.png}
		&		\includegraphics[width=\scannetImSize\linewidth]{im/exp//ss/gen3dz_0164.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp//ss/mine_0164.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp//ss/gt_0164.png}\\
		
		
		\includegraphics[width=\scannetImSize\linewidth]{im/exp//ss/gen3dz_0249.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp//ss/mine_0249.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp//ss/gt_0249.png}
		&		\includegraphics[width=\scannetImSize\linewidth]{im/exp//ss/gen3dz_0435.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp//ss/mine_0435.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp//ss/gt_0435.png}\\
		
		
		\includegraphics[width=\scannetImSize\linewidth]{im/exp//ss/gen3dz_0046.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp//ss/mine_0046.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp//ss/gt_0046.png}
		&		\includegraphics[width=\scannetImSize\linewidth]{im/exp//ss/gen3dz_0050.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp//ss/mine_0050.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp//ss/gt_0050.png}\\
		\bottomrule
		
	\end{tabular}
	\includegraphics[width=\linewidth]{im/ss_colorbar}
	%\captionof{figure}
	\caption{Demonstration of semantic segmentation on the ScanNet dataset.
       Selected scenes are consistent with~\cref{fig:recons:scannet_demo}}
	\label{fig:segmentation_demo}
	
\end{figure*}

\subsection{Scene Understanding Results}

Saliency detection effectively highlights the objects of interest.
This is also considered part of 3D semantic understanding.
However, as the semantics categories vary, fusing different categories of semantics into multiple LIMs can be inefficient.
%
Therefore, in this section, we utilize Uni-Fusion to fuse and construct a surface field for high-dimensional CLIP embeddings.
With a single LIM, we can generate different semantic results based on corresponding commands.
%
Since now our Uni-Fusion works with OpenSeg for scene understanding purposes, we call it Uni-Fusion-SU.

\subsubsection{Semantic Segmentation}
\label{sec:exp:semantic}

We first evaluate our model on generalized zero-shot point cloud semantic segmentation application.
Generalized Zero-Shot Learning (GZSL) differs from Zero-Shot Learning (ZSL) in that ZSL only predicts classes unseen during training, while GZSL predicts both unseen and seen classes~\cite{michele2021generative}.
Therefore, comparing our results with GZSL SOTAs provides a better understanding of the potential of Uni-Fusion-SU, as it does not train on both seen and unseen. 

This test uses ScanNet and S3DIS datasets for benchmarking. 
It is important to note that the \textbf{compared baselines are trained on the corresponding datasets}.
Our Uni-Fusion-SU uses OpenSeg to provide the 2D image level feature ebmedding.
Although \textbf{Uni-Fusion-SU} is also zero-shot, \textbf{it does not touch any ScanNet or S3DIS annotations}.

We demonstrate the mIoU scores in~\cref{tab:sem_seg_overview}.
In particular, our model achieves best results among the zero-shot learning methods on the ScanNet dataset and remains competitive with fully supervised methods.

Furthermore, we provide results specifically for the unseen classes in~\cref{sup:tab:sn_acc_miou}.
Although not as good as the fully supervised approach, Uni-Fusion-SU performs much better than 3DGenZ.
In addition, our Uni-Fusion-SU demonstrates high precision in classes such as sofa and Toilet, even when compared to the fully supervised model.

\begin{table}[htbp]
		\caption{Classwise GZSL semantic segmentation performance (\%) on the ScanNet unseen split.}
	\centering
	\newcommand*\rotext{\multicolumn{1}{R{45}{1em}}}
	\setlength{\tabcolsep}{1.7pt}
	\begin{tabular}{@{}l@{~}c|rrrr|r@{}}
		\toprule		
		& &
		{\textbf{Bookshelf}} & {\textbf{Desk}} & {\textbf{Sofa}} & {\textbf{Toilet}} & \stackbox{mean} \\
		
		\midrule
		FSL (Fully supervise) & IoU & 	56.9&	30.0&	57.4&	63.4 & 51.9
		\\ 
		3DGenZ (Zero-shot) & IoU & 	6.3&	3.3&	13.1&	8.1 & 7.7
		\\
		Uni-Fusion-SU (Ours) & IoU &38.3&16.8&51.7&60.9&41.9
	\\ \midrule 
	3DGenZ (Zero-shot)& Acc. & 	13.4&	5.9&	49.6&	26.3 &23.8
	\\
	Uni-Fusion-SU (Ours) & Acc. &61.9&29.6&67.4&91.6& 62.6
		\\
		\bottomrule
	\end{tabular}

	\label{sup:tab:sn_acc_miou}
\end{table}

However, in the S3DIS dataset, our model does not outperform 3DGenZ and other methods as shown in~\cref{tab:sem_seg_overview}.

Even in the result of unsceened data, as presented in \cref{sup:tab:s3dis_acc_miou}, we observe that Uni-Fusion-SU hardly finds some classed, e.g. Beam and Column, which are not commonly annotated objects. 
However, for common objects like sofa and window, our model performs much better.

\begin{table}[htbp]
		\caption{Classwise GZSL semantic segmentation performance (\%) on the S3DIS unseen split.}
	\centering
	\newcommand*\rotext{\multicolumn{1}{R{45}{1em}}}
	\setlength{\tabcolsep}{1.7pt}
	\begin{tabular}{@{}l@{~}c|rrrr|r@{}}
		\toprule		
		& &
		{\textbf{Beam}} & {\textbf{Column}} & {\textbf{Sofa}} & {\textbf{Window}} & \stackbox{mean} \\
		
		\midrule
		FSL (Fully supervise) & IoU & 	63.1&	10.2&	54.1&	72.4 & 50.0
		\\ 
		3DGenZ (Zero-shot) & IoU & 	13.9&	2.4&4.9&	8.1 &7.3
		\\
		Uni-Fusion-SU (Ours) & IoU &5.5&0.02&57.4&32.9&	24.0
		\\ \midrule 
		3DGenZ (Zero-shot) & Acc. & 	20.0&	9.1&	62.4&	23.7 &28.8
		\\
		Uni-Fusion-SU (Ours) & Acc. &41.5&0.02&78.3&42.1& 40.5
		\\	
		\bottomrule
	\end{tabular}

	\label{sup:tab:s3dis_acc_miou}
\end{table}

We present the results of the semantic segmentation in~\cref{fig:segmentation_demo}. 
It is evident that, 3DGenZ's result contains more noise, as seen in the spotted sofa, bed and other objects.
Conversely, Uni-Fusion-SU's results are generally smoother and more precise.

%
%\begin{figure*}[htbp]
%	\centering
%	\includegraphics[width=.3\linewidth]{example-image-golden}
%	\includegraphics[width=.3\linewidth]{example-image-golden}
%	\includegraphics[width=.3\linewidth]{example-image-golden}
%	\\
%	\includegraphics[width=.3\linewidth]{example-image-golden}
%	\includegraphics[width=.3\linewidth]{example-image-golden}
%	\includegraphics[width=.3\linewidth]{example-image-golden}
%	
%	\caption{Semantic segmentation result on ScanNet.}
%\end{figure*}
%
%\begin{figure*}[htbp]
%	\centering
%	\includegraphics[width=.3\linewidth]{example-image-golden}
%	\includegraphics[width=.3\linewidth]{example-image-golden}
%	\includegraphics[width=.3\linewidth]{example-image-golden}
%	\\
%	\includegraphics[width=.3\linewidth]{example-image-golden}
%	\includegraphics[width=.3\linewidth]{example-image-golden}
%	\includegraphics[width=.3\linewidth]{example-image-golden}
%	
%	\caption{Semantic segmentation result on S3DIS.}
%\end{figure*}

\subsubsection{Scene Understanding with Different Properties}

\begin{figure*}[t!]
	\centering
	\setlength{\tabcolsep}{0.1em}
	\renewcommand{\arraystretch}{.1}
	\resizebox{\textwidth}{!}{\begin{tabular}{|c | c | c | c | c | c|}
			\toprule 
			& \textbf{scene0568\_00} & \textbf{scene0249\_00} & \textbf{scene0435\_00} & \textbf{office3} & \textbf{room0}\\
			\midrule
			{} &
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0568_color.png}} & %\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0164_color.png}} &
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0249_color.png}} & \raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0435_color.png}}
			&
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/replica/office3_color.png}}
			&
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/replica/room0_color.png}}\\ %\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0050_color.png}} %\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/replica/office3_color.png}
			\\
			\textbf{Desk}  &
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0568_lt_desk.png}}&
			%\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0164_lt_desk.png}}&
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0249_lt_desk.png}}&
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0435_lt_desk.png}}
			&
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/replica/office3_lt_desk.png}}
			&
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/replica/room0_lt_desk.png}}\\
			%\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0050_lt_desk.png}}
			%\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/replica/office3_saliency.png}
			\\
			
			\textbf{Sofa} &
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0568_lt_sofa.png}} &
			%\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0164_lt_sofa.png}} &
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0249_lt_sofa.png}} &
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0435_lt_sofa.png}}&
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/replica/office3_lt_sofa.png}}
			&
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/replica/room0_lt_sofa.png}}\\
			%\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0050_lt_sofa.png}}
			%\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/replica/office3_style.png}
			\\
			\textbf{Work} &
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0568_lt_work.png}} &
			%\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0164_lt_work.png}} &
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0249_lt_work.png}} &
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0435_lt_work.png}}&
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/replica/office3_lt_work.png}}
			&
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/replica/room0_lt_work.png}}\\
			%\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0050_lt_work.png}}
			%\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/replica/office3_style.png}
			\\
			\textbf{Sittable} &
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0568_lt_sit.png}} &
			%\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0164_lt_sit.png}} &
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0249_lt_sit.png}} &
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0435_lt_sit.png}}&
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/replica/office3_lt_sit.png}}
			&
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/replica/room0_lt_sit.png}}\\
			%\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0050_lt_sit.png}}
			%\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/replica/office3_style.png}
			\\
			\textbf{Wood} &
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0568_lt_wood.png}} &
			%\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0164_lt_wood.png}} &
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0249_lt_wood.png}} &
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0435_lt_wood.png}}&
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/replica/office3_lt_wood.png}}
			&
			\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/replica/room0_lt_wood.png}}\\
			%\raisebox{-.5\height}{\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/scannet/0050_lt_wood.png}}
			%\includegraphics[width=\fabImSize\linewidth]{im/exp/fab/replica/office3_style.png}
			\\
			
			
			\bottomrule
		\end{tabular}
	}
	%\captionof{figure}
	\caption{Demonstration of the original mesh, highlighted semantic mesh given various queries.}
	\label{fig:fab_lt}
	\vspace{-.5cm}
\end{figure*}

The main contribution of this application is that, Uni-Fusion is the first model to construct a continuous mapping of high-dimensional embeddings onto the surface without the need for any training of the map representation.
%
In the previous experiment (\cref{sec:exp:semantic}), we evaluate the performance of generalized zero-shot semantic segmentation.
However, the potential of Uni-Fusion goes beyond semantic segmentation.
%
By constructing a LIM, we obtain a surface CLIP feature field.
This enables us to query various semantic categories such as 
%without the need of multiple LIMs or rerun for other properties, we query 
\textbf{Object, Room Type, Material, Affordance and Activity} without requiring multiple LIMs or re-running the model.

We present the results in \cref{fig:fab_lt}, 
where we query object (desk, sofa), activity (work), affordance (sittable), and material (wood).
Uni-Fusion-SU accurately identifies and highlights the object and material regions.
However, for less specific commands such as work or sittable, the model provides a wider range of results with less confidence (indicated by dull yellow).
Nevertheless, the suggested options are also roughly correct.






