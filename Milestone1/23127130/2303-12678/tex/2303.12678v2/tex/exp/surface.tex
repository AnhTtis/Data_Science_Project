\subsection{Reconstruction Results}

In our evaluation, we first use the ScanNet validation set with 312 sequences to thoroughly test the geometric reconstruction on a wide variety of scenes.
%
Then, we use TUM RGB-D to compare our modified tracking model with related works.
Since the tracking part is not the contribution of this paper, we give a brief overview of the tracking results.
%
To fairly evaluate the color reconstruction, we compare with related works, including NeRF, on the high-quality rendered Replica dataset.
\begin{table*}[t!]
	\centering
	\caption{Surface comparison on ScanNet~\cite{dai2017scannet}.
		Scores are fetched from~\cite{li2022bnv}.
		$^*$ indicates the result from our runs of the official BNV-Fusion code.}
	\small
	%\setlength{\tabcolsep}{5mm}
	\setlength{\tabcolsep}{0.9em}
	%\resizebox{\textwidth}{!}{
		\begin{tabular}{l  c c c| c c c }
			\toprule
			Method & \begin{tabular}{@{}c@{}}Pre-Train\\ with extra dataset\end{tabular} & \begin{tabular}{@{}c@{}}Train \\ with sequences\end{tabular} & Real-time & Accuracy (\%) $\uparrow$ & Completeness (\%) $\uparrow$ & F1 score $\uparrow$ \\
			\midrule
			TSDF Fusion~\cite{zhou2018open3d} & None & None & $\checkmark$ &73.83 & 85.85 & 78.84 \\
			iMAP~\cite{sucar2021imap} & None & Online train& &68.96 & 82.12 & 74.96 \\
			DI-Fusion~\cite{huang2021di} &Object Pretrain & None & $\checkmark$&66.34 & 79.65 & 72.97 \\
			BNV-Fusion~\cite{li2022bnv} &Object Pretrain &  Post Optimization& &{74.90} & \textbf{88.12} & {80.56} \\
			BNV-Fusion$^{*}$~\cite{li2022bnv} &Object Pretrain & Post Optimization &&{73.42} & {81.75} & {77.18} \\
			\textbf{Uni-Fusion (Ours)} &None &None &$\checkmark$&\textbf{80.43} & {84.91} & \textbf{82.44} \\
			\bottomrule
		\end{tabular}
		%}
	\label{tab:scannet}
	\vspace{-0.3cm}
\end{table*}

\begin{figure*}[t!]
	\subfloat[width=.33\textwidth][Accuracy]{
		\centering
		\includegraphics[width=.22\linewidth]{im/exp/recons/scannet/scannet_acc.png}
		\includegraphics[width=.1\linewidth]{im/exp/recons/scannet/scannet_acc_box.png}
	}
	\subfloat[width=.33\textwidth][Completeness]{
		\centering
		\includegraphics[width=.22\linewidth]{im/exp/recons/scannet/scannet_comp.png}
		\includegraphics[width=.1\linewidth]{im/exp/recons/scannet/scannet_comp_box.png}
	}
	\subfloat[width=.33\textwidth][F1 score]{
		\centering
		\includegraphics[width=.22\linewidth]{im/exp/recons/scannet/scannet_F1.png}
		\includegraphics[width=.1\linewidth]{im/exp/recons/scannet/scannet_F1_box.png}
	}
	\label{fig:recon:scannet:elementwise}
	\caption{Quantitative comparison on 312 scenes of the ScanNet validation set.
		We demonstrate the performance of SOTA BNV-Fusion and our Uni-Fusion.
		We sort Uni-Fusion's evaluation value and reordered all of the scores.
		The zigzag pink is the BNV-Fusion result;
		we also plot a deep-pink smoothed curve for better visualization.}
	\vspace{-0.3cm}
\end{figure*}



\subsubsection{Evaluation on ScanNet Dataset~\cite{dai2017scannet}}
\label{sec:exp:scannet}

We use the 312 different scenes from the ScanNet validation set to evaluate the performance of surface reconstruction. 
We follow the pure mapping SOTA BNV-Fusion to take every 10th posed frame as input. 
%
Without using any learning (as iMAP, DI-Fusion, and BNV-Fusion do) or any post-optimization (as BNV-Fusion does), our Uni-Fusion is capable to achieve precise continuous mapping performance.

The results presented in~\cref{tab:scannet} demonstrate that our Uni-Fusion outperforms the SOTA method BNV-Fusion in terms of \textbf{$+6$ higher accuracy}. 
However, our model does not outperform BNV-Fusion in terms of completeness, since BNV-Fusion incorporates completion in post-optimization.
Nevertheless, Uni-Fusion's completion is still much higher than one other optimization based model iMAP.
Overall, our Uni-Fusion model achieves higher F1-scores, which quantifies the overall quality of reconstruction.

It is important to note that the SOTA BNV-Fusion is not capable of real-time performance as it requires post-optimization of all fed frames.
On the other hand, the real-time model Di-Fusion exhibits much worse results without using post-optimization.
In contrast, our \textbf{real-time} model, \textbf{Uni-Fusion} achieves \textbf{much better} reconstruction quality than these approaches even without post-optimization. 

We additionally run BNV-Fusion's official implementation (emphasized with $^*$) on the 312 videos from ScanNet and conduct a scene-wise comparison in \cref{fig:recon:scannet:elementwise}. 
Our result is shown with the {\color{Cyan}light blue} curve, BNV-Fusion's result is shown with {\color{Lavender}pink}.
The scene index is sorted based on the score value of Uni-Fusion.
To enhance visual clarity, we apply a smoothing technique to BNV-Fusion's curve and presented it with dark pink color.
The comparison clearly shows that Uni-Fusion is overall performing better than BNV-Fusion. 
In addition, we use box-plots to analyze the statistics alongside the curve plot. Uni-Fusion's scores show a higher concentration on the plots. 
While the difference in completeness may be less pronounced,
Uni-Fusion's box plot is smaller and positioned relatively higher. This indicates that Uni-Fusion achieves a more stable completeness result, while BNV-Fusion is more likely to achieve low completeness in some cases.

In summary, our Uni-Fusion model achieves superior results across almost all 312 scenes in terms of accuracy, completeness and F1-score.
This finding is consistent with the observation presented in~\cref{tab:scannet} with BNV-Fusion$^*$, that Uni-Fusion outperforms the official implementation of BNV-Fusion in all metrics.

We show reconstruction on selected scenes from ScanNet in~\cref{fig:recons:scannet_demo}. 
Both BNV-Fusion and our Uni-Fusion are able to produce high quality reconstructions.
However, we observe that BNV-Fusion generates numerous small meshes on walls, resulting in the appearance of small particles in the reconstruction. 
We attribute this behavior to BNV-Fusion's use of very small voxel size ($0.02\si{\meter}$) to achieve a high score.
This is further supported by the fact that their mesh averages \textbf{\SI{247}{MB}}, while ours averages only \textbf{\SI{54}{Mb}}.
Furthermore, our Uni-Fusion's mesh is smoother and
also provides highly-accurate color to the mesh which is not available for this surface SOTA.

Besides, we evaluate the storage cost of the latent representation. BNV-Fusion's latents require an average storage of \textbf{\SI{228}{MB}} across the 312 scenes of ScanNet, while Uni-Fusion achieves significantly lower storage requirements with an average of only \textbf{\SI{9}{MB}}.
%In this test, we purely evaluate the surface reconstruction capacity with SOTAs. 
%While reconstruction is not merely surface.  
%Thus in the following, we find benchmarks for both surface and color.


\newcommand{\scannetImSize}{.16}
\begin{figure*}[t!]
	\centering
	\setlength{\tabcolsep}{0.1em}
	\renewcommand{\arraystretch}{.1}
	\begin{tabular}{|c | c |c |||c |c | c|}
		\hline
		{\Large{BNV-Fusion}} & {\Large{Uni-Fusion}} &{\Large{Ground Truth}} & {\Large{BNV-Fusion}} &{\Large{Uni-Fusion}} & {\Large{Ground Truth}} \\ \hline \hline
		
		\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0568_00_bnv.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0568_00_mine.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0568_00_gt.png}
		&		\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0164_00_bnv.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0164_00_mine.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0164_00_gt.png}\\
		
		\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0249_00_bnv.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0249_00_mine.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0249_00_gt.png}
		&		\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0435_00_bnv.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0435_00_mine.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0435_00_gt.png}\\
		
		\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0046_00_bnv.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0046_00_mine.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0046_00_gt.png}
		&		\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0050_00_bnv.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0050_00_mine.png}
		&\includegraphics[width=\scannetImSize\linewidth]{im/exp/recons/scannet_qualifi/scene0050_00_gt.png}\\
		\hline
	\end{tabular}
	%\captionof{figure}
	\caption{Surface reconstruction on ScanNet dataset.}
	\label{fig:recons:scannet_demo}
\end{figure*}

\subsubsection{Tracking Evaluation on TUM RGB-D Dataset~\cite{sturm2012benchmark}}
% follow nice-slam
In the above test, we evaluate the performance of pure mapping.
Although tracking is not the contribution focus in our paper, it is still part of the incremental reconstruction model. We follow the novel incremental reconstruction model NICE-SLAM~\cite{zhu2022nice} to evaluate the camera tracking on the small scale TUM RGB-D dataset.
Our Uni-Fusion uses a coarse-to-fine strategy for 3D reconstruction tracking.
From~\cref{tab:tum_rmse}, it demonstrates overall better ATE RMSE than other implicit representation models.
\begin{table}[htbp]
	\caption{Tracking on TUM RGB-D~\cite{sturm2012benchmark}.
		ATE RMSE [$\si{\centi\meter}$] ($\downarrow$) is used as the evaluation metric.
	}
	\centering
	\footnotesize
	\setlength{\tabcolsep}{0.7em}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{l|ccc}
			\hline
			& \tt{fr1/desk} &  \tt{fr2/xyz} &  \tt{fr3/office} \\
			
			\hline
			{iMAP}~\cite{sucar2021imap}      & 4.9 & 2.0 & 5.8  \\
			%{iMAP$^*$}~\cite{sucar2021imap} & 7.2 & 2.1  & 9.0 \\
			{DI-Fusion~\cite{huang2021di}} & 4.4 & 2.3 & 15.6 \\
			NICE-SLAM~\cite{zhu2022nice}           & 2.7 & 1.8 & 3.0 \\
			Ours& 1.8& 0.5& 2.1 \\
			\hline
			{BAD-SLAM}\cite{schops2019bad} & 1.7  & 1.1  & 1.7 \\
			{Kintinuous}\cite{whelan2012kintinuous} & 3.7  &  2.9  & 3.0 \\
			{ORB-SLAM2}\cite{mur2017orb} & \bf 1.6  & \bf 0.4  & \bf 1.0 \\
			\hline
	\end{tabular}}
	\vspace{2pt}
	
	\label{tab:tum_rmse}
\end{table}

On the other hand, there also exist high accuracy algorithms from SLAM. 
By additionally using bundle adjustment and loop-closing techniques, their tracking quality is much better than all of the implicit based models.
%But it is dangerous to directly apply SLAM result on reconstruction. Please find our demonstration in Fig [?]. Which explains the more widely used frame-to-model strategy in 3D reconstruction.
Our coarse-to-fine strategy obtains a good score because, first it ensures it does not easly lose track. Second, it is more suitable for surface fitting.

This further supports our test on the Replica dataset.

\begin{table*}[t!]
	\centering
	\caption{Geometric (L1) and Photometric (PSNR) evaluation on the Replica dataset~\cite{sucar2021imap}.}
	\footnotesize
	\setlength{\tabcolsep}{0.36em}
	\renewcommand{\arraystretch}{1.2}
	\begin{tabular}{clcccccccccccccccccc}
		\toprule
		& & \multicolumn{1}{c}{\makecell{\tt{office-0}}} & \multicolumn{1}{c}{\makecell{\tt{office-1}}} & \multicolumn{1}{c}{\makecell{\tt{office-2}}}& \multicolumn{1}{c}{\makecell{\tt{office-3}}} & \multicolumn{1}{c}{\makecell{\tt{office-4}}} & \multicolumn{1}{c}{\makecell{\tt{room-0}}} & \multicolumn{1}{c}{\makecell{\tt{room-1}}} &  \multicolumn{1}{c}{\makecell{\tt{room-2}}} & Avg. \\
		\midrule
		\multicolumn{5}{l}{\textit{Non-continuous mapping method}}\\
		\multirow{2}{*}{\makecell{\textbf{TSDF-Fusion}~\cite{curless1996volumetric}}}
		& {\bf Depth L1} [$\si{\centi\meter}$] $\downarrow$
	 & 14.11 & 10.50 & 30.89 & 28.92 & 22.83	& 23.51 & 20.94 & 23.34 & 21.88 \\
		& {\bf PSNR } [$\si{\dB}$] $\uparrow$
		& 11.16 & 15.92 & 4.86 & 5.68 & 5.46 & 3.43 & 4.51 & 5.57 & 7.07 \\
		
		\midrule
		\multirow{2}{*}{\makecell{\textbf{$\sigma$-Fusion}\cite{rosinol2023probabilistic} }}
		& {\bf Depth L1} [$\si{\centi\meter}$] $\downarrow$
		 & 13.80 & 10.21 & 22.27 & 28.70 & 22.21& 21.92 & 19.28 & 22.40 & 20.10 \\
		& {\bf PSNR } [$\si{\dB}$] $\uparrow$
		 & 11.16 & 15.92 & 4.86 & 5.69 & 5.46& 3.45  & 4.51 & 5.57 & 7.08 \\
		

		\midrule
		\midrule
		\multicolumn{5}{l}{\textit{Continuous mapping method}}\\
		\multirow{2}{*}{\makecell{\textbf{iMAP$^*$}~\cite{sucar2021imap}}}
		& {\bf Depth L1} [$\si{\centi\meter}$] $\downarrow$
		 & 6.43 & 7.41 & 14.23 & 8.68 & 6.80& 5.70 & 4.93 & 6.94 & 7.64\\
		& {\bf PSNR } [$\si{\dB}$] $\uparrow$
		& 7.39 & 11.89 & 8.12 & 5.62 & 5.98& 5.66 & 5.31 & 5.64  & 6.95\\
		\midrule
		\multirow{2}{*}{{\makecell{\textbf{Nice-SLAM}~\cite{zhu2022nice} }}}
		& {\bf Depth L1} [$\si{\centi\meter}$] $\downarrow$
		& { 1.51 } & { 0.93 } & { 8.41 } & { 10.48 } & {2.43} & { 2.53 } & { 3.45 } & { 2.93 }  & { 4.08 } \\
		& {\bf PSNR } [$\si{\dB}$] $\uparrow$
		 & { 22.44 } & { 25.22 } & { 22.79 } & { 22.94 } & { 24.72 } & \textbf{ 29.90 } & \textbf{ 29.12 } & { 19.80 }& { 24.61 } \\
		
		
		\midrule	
		\multirow{2}{*}{{\makecell{\textbf{Uni-Fusion} (Ours) }}}
		% using abs(diff)
		%	& {\bf Depth L1} [$\si{\centi\meter}$] $\downarrow$ &\textbf{1.98}&\textbf{1.18}&\textbf{1.64}&\textbf{1.23}&\textbf{0.84}&\textbf{1.61}&\textbf{3.01}&\textbf{1.60} &\textbf{1.64}
		% follow nerf-slam to remove outlier gt first
		& {\bf Depth L1} [$\si{\centi\meter}$] $\downarrow$
		& \textbf{0.79}&\textbf{0.56}&\textbf{1.59}&\textbf{2.71}&\textbf{1.66}&\textbf{1.94}&\textbf{0.69}&\textbf{1.80}& \textbf{1.47}
		\\
		& {\bf PSNR } [$\si{\dB}$] $\uparrow$ &\textbf{33.88}&\textbf{33.31}&\textbf{25.84}&\textbf{26.01}&\textbf{28.14}&24.02&26.20&\textbf{27.17} &\textbf{28.07}
		\\
		
		\midrule
		\midrule
		\multicolumn{5}{l}{\textit{Neural radiance field method}}\\
		\multirow{2}{*}{{\makecell{\textbf{NeRF-SLAM}~\cite{rosinol2022nerf} }}}
		& {\bf Depth L1} [$\si{\centi\meter}$] $\downarrow$
	 & {2.49}   & {1.98}  & {9.13}  & {10.58} & {3.59}	& {2.97}  & {2.63}  & {2.58}  & {4.49} \\
		& {\bf PSNR } [$\si{\dB}$] $\uparrow$
	 & \textbf{48.07}  & \textbf{53.44} & \textbf{39.30} & \textbf{38.63} & \textbf{39.21} 	& \textbf{34.90} & \textbf{36.95} & \textbf{40.75}& \textbf{41.40} \\
		
		\bottomrule
	\end{tabular}%
	
	\label{tab:replica_per_scene}
\end{table*}


\begin{table*}[t!]
	\centering
	\caption{Differences among different Surface \& Color reconstruction models.}
	\small
	\setlength{\tabcolsep}{.6em}
	%{
		%\resizebox{\textwidth}{!}{
			\begin{tabular}{l | c c c c c c }
				\toprule
				Method & 
				\begin{tabular}{@{}c@{}}Pre-Train\\ with extra dataset\end{tabular}
				& \begin{tabular}{@{}c@{}}Train\\ with sequences\end{tabular}
				& Real-time	
				& Direct Output &  \begin{tabular}{@{}c@{}}Light\\ direction\end{tabular} 
				&Render\\
				\hline 
				TSDF-Fusion & None & None & $\checkmark$& Discrete TSDF &  &Ray Rasterization\\\hline
				$\sigma$-Fusion & None & None &$\checkmark$&Discrete TSDF  && Ray Rasterization\\\hline
				iMAP & None & Online Train && MLPs  & &Volumetric Rendering\\\hline
				NICE-SLAM & \begin{tabular}{@{}c@{}}Pretrain\\ with indoor dataset\end{tabular} & Online Train&& Neural Implicit Grid&  & Volumetric Rendering\\\hline
				
				NeRF-SLAM & None & Train hundred epochs &-&NeRF & $\checkmark$ &Volumetric Rendering \\\hline
				
				\textbf{Uni-Fusion} & None & None&$\checkmark$& Latent Implicit Map && Ray Rasterization\\				
				\hline
			\end{tabular}
		%}
	%}
	\label{tab:replica_diff}
\end{table*}
\subsubsection{Evaluation on Replica RGB-D Dataset~\cite{sucar2021imap}}

\newcommand{\replicaImSize}{.24}
\begin{figure*}[b!]
	\centering
	\setlength{\tabcolsep}{0.1em}
	\renewcommand{\arraystretch}{.1}
	\begin{tabular}{|c | c |c |c| }
		\hline
		{\Large{NICE-SLAM}} &{\Large{NeRF-SLAM}}&\textbf{\Large{Uni-Fusion}}&\Large{Ground Truth}\\
		%		\hline
		%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nice-slam/of2_1286.png} &
		%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/of2_1286.jpg} &
		%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/of2_1286.jpg} &
		%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/gt/of2_1286.jpg} \\
		
		\hline
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nice-slam/rm0_769_window.png} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nerf-slam/rm0_769_window.jpg} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/rm0_769_window.jpg} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/gt/rm0_769_window.jpg} \\
		\hline
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nice-slam/of3_575_window.png} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nerf-slam/of3_575_window.jpg} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/of3_575_window.jpg} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/gt/of3_575_window.jpg} \\
		\hline
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nice-slam/rm1_425_window.png} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nerf-slam/rm1_425_window.jpg} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/rm1_425_window.jpg} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/gt/rm1_425_window.jpg} \\
		\hline
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nice-slam/rm2_1085_window.png} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nerf-slam/rm2_1085_window.jpg} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/rm2_1085_window.jpg} &
		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/gt/rm2_1085_window.jpg} \\		
		
	\end{tabular}
	%\captionof{figure}
	\caption{Demonstration of color rendering on the Replica dataset. Fine appearances are highlighted in {\color{green}green window}. Small defects are in a {\color{red}red} box.}
	\label{fig:replica_render}
	\vspace{-.5cm}
\end{figure*}
In this evaluation, we compare with implicit reconstruction (TSDF-Fusion, $\sigma$-Fusion) and latent implicit reconstruction models (iMAP, NICE-SLAM) that support color. 
Additionally,  we include a large-scale NeRF model, NeRF-SLAM, in the comparison.  
It is important to note that NeRF is SOTA in the view-synthesis task, which gives it an unfair advantage over other models because it learns light directions and does not really model a surface.
However, we include NeRF in this evaluation to demonstrate  that Uni-Fusion significantly reduces the gap.
Notably, NeRF-SLAM embeds external tracking model~\cite{teed2021droid,rosinol2023probabilistic} to provide poses while using SOTA NeRF implementation Instant-ngp~\cite{muller2022instant} for NeRF construction.

Uni-Fusion tracks and follows the same setting as in ScanNet test to take every 10 frames for mapping.
NICE-SLAM and NeRF-SLAM create depth and color using volumetric rendering.
In Uni-Fusion, we cast rays from the virtual camera onto our result surface mesh for the depth image. 
The cast points are then inferred using Uni-Fusion's color LIM to obtain color results.

According to~\cref{tab:replica_per_scene}, Uni-Fusion demonstrate
the best Depth L1 on all scenes with an average of \textbf{$\pmb{1.47}$$\si{\centi\meter}$ depth L1}. 
This is a \textbf{$\pmb{177\%}$ boost} compared to the second best model.

Moreover, excluding NeRF, our Uni-Fusion also shows the best performance in modeling the colors, achieving an average PSNR of $28.07$$\si{\dB}$.

However, it is strange that NICE-SLAM loses details while in two cases, it shows better PSNR than Uni-Fusion. 
To highlight the true result,
we provide rendered images in~\cref{fig:replica_render}.
It is evident that our Uni-Fusion accurately models the details of painting, carpet and quilt, while NICE-SLAM only roughly models the average color.

In addition, from~\cref{fig:replica_render}, Uni-Fusion's rendering quality is as precise as NeRF. 
The painting, carpet and quilt in Uni-Fusion's results are very similiar to the original appearance.
The {\color{green} green window} highlights the regions of interest.
Uni-Fusion reproduces the high-quality appearances that are very close to NeRF in terms of qualitative evaluation.
%
However, Uni-Fusion still has a quantitative score gap to NeRF's color rendering ($41.4$$\si{\dB}$), despite the highly comparable qualitative results to NeRF and ground truth.
%
We attribute this difference to three main factors: 
\textbf{1.} Uni-Fusion does not model the light directions to points, which is essential to NeRF.
\textbf{2.} NeRF optimizes image quality by focusing primarily on color rather than depth,
which is evident from its higher color rendering score but much worse depth rendering compared to Uni-Fusion.
\textbf{3.} Uni-Fusion does not support hole filling,
which results in black holes in the rendered images.

We summarize the differences between Uni-Fusion and other models in~\cref{tab:replica_diff}.
Similar to TSDF-Fusion and $\sigma$-Fusion, Uni-Fusion is a forward method that does not require any training of map representation, i.e., pre- or on-line training. 
It shares similarities with NICE-SLAM and NeRF-SLAM in producing an implicit map with a set of latents, that outputs results at arbitrary resolution.
However, Uni-Fusion differs in the extraction of the signed distance field,
as each query value is directly inferred using the corresponding ruling latent,
while NICE-SLAM and NeRF-SLAM use a much denser grid of features for interpolation during volumetric rendering based inference.

Like TSDF-Fusion and $\sigma$-Fusion, our Uni-Fusion is a real-time algorithm,
whereas iMAP, NICE-SLAM and NeRF-SLAM are not capable of running in real time.
NeRF-SLAM claims to be real-time, which is questionable as it still needs hundreds of epochs training after feeding the data.

However, optimization with backpropagation allows for pixel-to-pixel learning,
which is theoretically superior to the regression and fusion strategy. 
Although Uni-Fusion demonstrates its high ability to model colors, exploring NeRF-like post-optimization would be a promising direction for further improvements.

%(2) NeRF model learning radiance field that model the light on different direction on surface. While Uni-Fusion naturally treat different directional light the same color.

%\begin{table*}[t!]
%	\centering
%	\setlength{\tabcolsep}{0.1em}
%	\renewcommand{\arraystretch}{.1}
%	\begin{tabular}{c | c |c |c |c }
%		\hline 
%		\rotatebox{90}{\large{NICE-SLAM}} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nice-slam/of3_575.png} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nice-slam/rm0_769.png} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nice-slam/rm1_425.png} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/nice-slam/rm2_1085.png} \\
%		\hline
%		\rotatebox{90}{\large{NeRF-SLAM}} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/of3_575.jpg} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/rm0_769.jpg} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/rm1_425.jpg} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/rm2_1085.jpg} \\	
%		\hline
%		\rotatebox{90}{\textbf{\Large{Uni-Fusion}}} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/of3_575.jpg} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/rm0_769.jpg} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/rm1_425.jpg} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/mine/rm2_1085.jpg} \\
%		\hline
%		\rotatebox{90}{\large{Ground Truth}} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/gt/of3_575.jpg} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/gt/rm0_769.jpg} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/gt/rm1_425.jpg} &
%		\includegraphics[width=\replicaImSize\linewidth]{im/exp/recons/replica/gt/rm2_1085.jpg} \\
%		\hline		
%		
%	\end{tabular}
%	\captionof{figure}{Demonstration of color rendering on Replica dataset.}
%\end{table*}


\begin{figure}[]
	\centering
%		\subfloat[width=\textwidth][Sample based]{
%		\centering
%		\includegraphics[width=.7\linewidth]{im/exp/ablation/GPIS/seq3_sample_color.png}
%	}\\
%	\subfloat[width=\textwidth][Derivative based]{
%		\centering
%		\includegraphics[width=.7\linewidth]{im/exp/ablation/GPIS/seq3_derivative_color.png}
%	}
		\includegraphics[width=.49\linewidth]{im/exp/ablation/GPIS/seq3_sample_color_a.png}
		\includegraphics[width=.49\linewidth]{im/exp/ablation/GPIS/seq3_derivative_color_b.png}
	\caption{Ablation study on surface construction basis. (a) Sample based. (b) Derivative based.}
	\label{fig:ablation:GPIS}
\end{figure}


%\begin{table}[]
%	\caption{Ablation study on tracking.
%	}
%	\centering
%	\footnotesize
%	\setlength{\tabcolsep}{0.7em}
%	\resizebox{\linewidth}{!}{
%		\begin{tabular}{l|ccc}
%			\hline
%			& \tt{fr1/desk} &  \tt{fr2/xyz} &  \tt{fr3/office} \\
%			\hline
%			External& 2.1& 0.5& 2.5 \\
%			External+Internal&1.8& 0.5& 2.1 \\
%			\hline
%	\end{tabular}}
%	\vspace{-2pt}
%	%\vspace{-1cm}
%	\label{tab:tum_rmse2}
%\end{table}

\begin{figure}
	\centering
	\includegraphics[width=.8\linewidth]{im/exp/ablation/voxel_size/seq_voxel_size.png}
	%	\subfloat[width=.33\textwidth][0.1]{
		%		\centering
		%		\includegraphics[width=.33\linewidth]{im/exp/ablation/voxel_size/seq3_0_1_color.png}
		%	}
	%	\subfloat[width=.3\textwidth][0.05]{
		%		\centering
		%		\includegraphics[width=.33\linewidth]{im/exp/ablation/GPIS/seq3_sample_color.png}
		%	}
	%	\subfloat[width=.33\textwidth][0.02]{
		%	\centering
		%	\includegraphics[width=.33\linewidth]{im/exp/ablation/voxel_size/seq3_0_02_color.png}
		%	}
	\caption{Ablation study on voxel size.}
	\label{fig:ablation:voxel_size}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{im/exp/ablation/approx/num_eigenpair}
	\includegraphics[width=1\linewidth]{im/exp/ablation/approx/num_sample}
	\caption{Ablation study on number of eigenpairs and number of anchor samples in kernel approximation.}
	\label{fig:ablation:approx}
\end{figure}


\newcommand{\styleImSize}{.2}
\begin{figure*}[]
	\centering
	\setlength{\tabcolsep}{0.1em}
	\renewcommand{\arraystretch}{.1}
	\resizebox{\textwidth}{!}{\begin{tabular}{ccccc}
			%		\includegraphics[width=\styleImSize\line]{im/exp/style/style/0} &
			%		\includegraphics[width=\styleImSize\linewidth]{im/exp/style/style/1} &
			%		\includegraphics[width=\styleImSize\linewidth]{im/exp/style/style/2} &
			%		\includegraphics[width=\styleImSize\linewidth]{im/exp/style/style/3} &
			%		\includegraphics[width=\styleImSize\linewidth]{im/exp/style/style/4} &
			%		\includegraphics[width=\styleImSize\linewidth]{im/exp/style/style/5} &
			%		\includegraphics[width=\styleImSize\linewidth]{im/exp/style/style/6} \\
			\hline\hline
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_0.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_1.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_2.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_3.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_4.png} \\
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_5.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_6.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_7.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_8.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_9.png} \\
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_10.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_11.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_12.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_13.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_14.png} \\
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_15.png} &
			%\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_16.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_17.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_18.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_19.png} &
			\includegraphics[width=\styleImSize\linewidth]{im/exp/style/processed/office0_20.png} 
			\\	\hline
		\end{tabular}
	}
	%\captionof{figure}
	\caption{Style transfer on 3D canvas.}
	\label{fig:style}
\end{figure*}
\subsection{Ablation study}
\label{exp:surface:ablation}


\subsubsection{Sample-based or Derivative-based}

We select the surface model with our own recorded sequences. 
All settings are detailed in \cref{sec:exp:details}.
As shown in~\cref{fig:ablation:GPIS}, reconstructions of Yijun's office are demonstrated. 
While both models are capable of construction, the derivative-based model introduces a lot of noise to the surface.
This issue arises, because, for smoothness purpose, we follow Di-Fusion~\cite{huang2021di} to build voxels that overlap with their neighbors, leading to redundant voxels near the surface.
For these redundant voxels, no center sample is provided and thus the derivative-based surface construction builds poor SDFs on unknown region of the voxels.

In contrast, the sample-based surface construction does not encounter this problem because it adds more points within the voxels, enabling the construction of very smooth surfaces.
We observe well-constructed and accurately colored objects such as the whiteboard, the chair, the school bag and even the oranges.

%\subsubsection{Tracking}
%
%
%Our Uni-Fusion use a coarse-to-fine strategy for tracking. 
%An external tracking model is running in one thread aside from the mapping thread.
%In the mapping thread, it takes pose result from the external tracking and applies the internal tracking for colored point cloud.
%
%The result is demonstrated in~\cref{tab:tum_rmse2}. 
%The coarse-to-fine is relatively better on trajectory estimation.

\subsubsection{Voxel size}

While testing of the office scene, we vary the voxel size from low to high. 
From~\cref{fig:ablation:voxel_size}, when a low voxel size $0.02\si{\meter}$ is used, the surface appears rough.
As the voxel size increases, the smoothness improves.
However, when a voxel size of $0.1\si{\meter}$ is employed, the surface appears blurry. 
Considering Uni-Fusion produces a surface color field, the quality of surface directly impacts the coloring.
Thus, further increasing the voxel size will result in deteriorated surface quality.

Therefore, in the above given experiments, $0.05\si{\meter}$ voxel size is used for surface construction.
Additionally, it should be clarified that each voxel used for encoding actually has a size of $0.1\si{\meter}$ due to the employment of overlapped voxels.

\subsubsection{Number of eigenpairs and anchor points}

The ablation study of the number of eigenpairs and anchor points should be conducted at the module level, as Uni-Fusion treats the approximation module as a cohesive entity, expecting it to behave like a real kernel.

This feature dimension $l$ corresponds to the number of eigenpairs retained during kernel approximation. 
We employ a uniform sampling of $256$ anchor points for the approximation. 
To access the accuracy in recovering the original kernel, we randomly sample $2000$ test samples in $[-.5,.5]^3$ and compute the matrix $\V K$ using original Mat\'ern Kernel. Our approximation, denoted as $\hat{\V K}$, is then evaluated. 
We calculate the Mean Absolute Error (MAE) between the two kernels and
observe the curve presented in~\cref{fig:ablation:approx}.
From the figure, we find that the error decreases fast until $l$ reaches $20$, and beyond that point, the improvement becomes marginal. 
Considering that the result for $l=20$ is very close to that at $l=40$ while requiring only half the storage space, the optimal selection for $l$ is $20$.

Similarly, we perform a module-level ablation for number of anchor points.
With $l$ fixed at $20$, a minimum of $20$ samples is required.
\cref{fig:ablation:approx} demonstrates that the approximation shows minimal improvement beyond $256$ samples.
Additionally, since this number of anchor samples only affects the computation time and not the storage space,
%a large value such as $256$ can be chosen for improved  
it is advisable to select a the large value such as $256$, but not the largest, to improve efficiency without sacrificing accuracy.
