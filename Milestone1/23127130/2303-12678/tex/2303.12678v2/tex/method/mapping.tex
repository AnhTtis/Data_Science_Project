\section{Universal Continuous Mapping} % 2 pages
\label{sec:UCM}

The previous~\cref{sec:UE} suggests a universal encoding model for different types of data.
%
Based on this function, in this section, our Universal Continuous Mapping produces a map of latents to implicitly represent the scene.
We refer to this scene representation as \textbf{Latent Implicit Maps (LIM)},
which supports surfaces, surface properties, and high-dimensional surface features. 
%
%\begin{figure}[htbp]
%	\centering
%	\includegraphics[width=.8\linewidth]{im/inherit_graph}
%	\caption{Inheritance graph for the class of Latent Implicit Maps (LIM).}
%	\label{fig:inherit}
%\end{figure}
\begin{figure}[htbp]
	\centering
	\psfragfig[width=1\linewidth]{im/eps/graph}{
		\psfrag{A}{\color{white}{\textbf{BaseMap}}}
		\psfrag{B}{\color{white}{\textbf{SurfaceMap}}}
		\psfrag{C}{\color{white}{\textbf{PropertyMap}}}
		\psfrag{D}{\color{white}{\textbf{LatentMap}}}
	}
	\caption{Inheritance graph for the class of Latent Implicit Maps (LIM).}
\label{fig:inherit}
\end{figure}

%Instead of directly producing a explicit map, our universal continuous mapping produces a map of latents to implicitly represent the scene.
%We name it \textbf{Latent Implicit Maps (LIM)}.
%FIXED: With was used before and should therefore be defined before.

The inheritance graph is shown in \cref{fig:inherit}.
We introduce a BaseMap to handle the voxel structure (\cref{sec:map_rep}), dynamically allocate space and fuse maps (\cref{sec:fuse}).
The SurfaceMap (\cref{sec:surface_mapping}) and the PropertyMap (\cref{sec:context_fields}) are derived from the BaseMap and are designed to handle specific data and applications.
The LatentMap (\cref{sec:latent_fields}) is derived from PropertyMap.
The main difference is that the PropertyMap works primarily with low-dimensional properties, such as color ($c=3$), infrared ($c=1$) data, and so on. 
On the other hand, the LatentMap handles much higher dimensional features, such as CLIP embeddings~\cite{ghiasi2022scaling} ($c=768$), depending on the specific application requirements.

\subsection{Map Representation}
\label{sec:map_rep}

We follow Neural Implicit Maps (\cite{huang2021di,yuan2022algorithm,li2022bnv}) to use uniformly spaced voxels to sparsely represent the scene. The scene is denoted as $\V V=\{\V v_m = (\V c_m, \V F_m, w_m)\}$, where $m$ is the voxel index. Each voxel $\V v_m$
comprises the
voxel center $\V c_m\in \mathbb R^3$, voxel latent feature $\V F_m\in \mathbb R^{l \times c}$, and the count of observed points $w_m\in\mathbb N$.

Given a sequence of incremental frames as input, our model constructs local LIMs (\cref{sec:surface_mapping}, \cref{sec:context_fields}, \cref{sec:latent_fields}) and fuses (\cref{sec:fuse}) them into a global LIM.
Then, we derive the explicit map from the global LIM.

\subsection{Surface Mapping}
\label{sec:surface_mapping}

Because the input point cloud $\V X$ is located on zero-level surface, it is not adequate to recover a 3D field of scene, $f_{SDF}:\mathbb R^3 \rightarrow \mathbb R$.
%
Therefore, we use the concept of Gaussian Process Implicit Surfaces (GPIS)~\cite{martens2016geometric,lee2019online,wu2021faithful,ivan2022online} to incorporate derivatives into kernel or to sample additional non-zero level points.
%
Both derivative-based and sample-based GPISs approaches utilize normal information.
Hence, we first preprocess $\V X$ to obtain normals $\V S$. 

\subsubsection{Using Derivatives based GPIS}
\label{sec:GPIS:deri}

From~\cite{martens2016geometric}, the derivatives of a GP are also Gaussian. 
Therefore, the covariance between data and derivatives is computed by differentiating the covariance function~\cite{solak2002derivative}. 
Specifically:
\begin{equation}
	\begin{split}
cov(\frac{\partial f_{SDF}(\V x) }{\partial x_i}, f_{SDF}(\V x^{'})) &= \frac{\partial k(\V x, \V x^{'})}{\partial x_i}\\
&=\frac{\partial}{\partial x_i}[f_{posi}(\V x)]f_{posi}(\V x^{'})^T.
	\end{split}
\end{equation}
Additionally,
\begin{equation}
\begin{split}
cov(\frac{\partial f_{SDF}(\V x) }{\partial x_i}, \frac{\partial f_{SDF}(\V x^{'}) }{\partial x_j}) &= \frac{\partial^2 k(\V x, \V x^{'})}{\partial x_i\partial x^{'}_j}\\
&=\frac{\partial}{\partial x_i}[f_{posi}(\V x)]\frac{\partial}{\partial x^{'}_j}[f_{posi}(\V x^{'})]^T.
\end{split}
\end{equation}
Given points $\{\V x_n\}^N_{n=1}$ with normals $\{\V s_n\}^N_{n=1}$ and field values $\{\V y_n=0\}^N_{n=1}$,
the position encoding function for derivatives is $f_{posi, deri}(\V x, i) = \frac{\partial}{\partial x_i}[f_{posi}(\V x)]$.
Its corresponding field value is the normal value $s_i$ on the axis $i$.
Therefore, we define
\begin{multline}
f_{posi, gpis}(\V X)=\\ [f_{posi}(\V X), f_{posi, deri}(\V X, 1), f_{posi, deri}(\V X, 2), f_{posi, deri}(\V X, 3)]
\end{multline}
with regression values $\V Y_{gpis}=[\V 0, \V s_{\cdot,1}, \V s_{\cdot,2}, \V s_{\cdot,3}]^T$, where $\V 0 = zeros(1,N)$, $\V s_{\cdot,i}=[s_{1,i},\cdots,s_{N,i}]$.

Then the local geometric encoding function is defined as
\begin{multline}
f_{enc, gpis}(\V X, \V Y, \V S)=\\
f_{posi, gpis}(\V X)^T(f_{posi, gpis}(\V X)f_{posi, gpis}(\V X)^T + \sigma_n^2\V I)^{-1}\V Y_{gpis}.
\end{multline}
By introducing derivatives into the kernels, the matrix size is increased by a factor of 15, 
while the encoded feature dimension remains low at $l$:
$\V F_{(\V X, \V Y, \V S)}=f_{enc, gpis}(\V X, \V Y, \V S)\in \mathbb R^{l\times 1}$.

For inference with the points $\V x_*$, the predictions are consistent with \cref{eq:decode}, $\V y_*=f_{posi}(\V x_*) \V F_{(\V X, \V Y, \V S)}$.

\subsubsection{Using Sample based GPIS}
\label{sec:GPIS:sample}

Sample-based GPIS is commonly used in GPIS research.
This method avoids the computation of Jacobians and allows for smaller kernel sizes, resulting in significant reductions in computational costs in terms of time and memory.

In sample-based GPIS, given points $\{\V x_n\}^N_{n=1}$ with normals $\{\V s_n\}^N_{n=1}$ and field value $\{\V y_n=0\}^N_{n=1}$, the dataset is extended by sampling points along the normal direction.
The corresponding field values are the signed distances as the sampled points move along the normal.
Then, \cref{eq:encode} is applied to the extended points and distances ($\V X_{ext}$, $\V Y_{ext}$).
The inference process of this model is the same as derivative-based GPIS in~\cref{sec:GPIS:deri}.

\bigskip

For each frame, points are assigned to their corresponding voxel.
Then, we encode the local geometry within each voxel using methods described in~\cref{sec:surface_mapping}, either derivative-based GPIS or sample-based GPIS to obtain the voxel representation $\V v=(\V c, \V F, w)$ where $\V F\in\mathbb R ^ {l\times 1}$ represents the geometric latent vector.
Subsequently, the local LIMs are fused into a global LIM according to the fusion procedure outlined in~\cref{sec:fuse}.

To visualize the surface result, we
construct the signed distance field from the global LIM by performing inference on sample points.
The sample points are generated on a grid within each voxel, with a certain resolution.
By applying the Marching Cubes algorithm to the SDF, we obtain a surface mesh that represents the reconstructed surface.


\subsection{Surface Property Fields}
\label{sec:context_fields}

The previous surface mapping approach discussed in~\cref{sec:surface_mapping} can be viewed as a special case of surface property mapping.
However, it is important to note that these two mappings operate in different spaces. 
Specifically, in our implementation, we do not derive the SurfaceMap class from PropertyMap. 
Instead, we introduce a BaseMap that performs common operations and allows them to be specific to local map construction and visualization, e.g., meshing and coloring.

We introduce the more general mapping of surface properties.
Since all points lie on the zero level of the signed distance fields, the PropertyMap naturally operates within a subspace of $\mathbb R^3$, the surface $\mathcal{S}$.
A surface property in this context refers to any property associated with each point, such as color, infrared values, and so on.
These properties are represented by the $\V y$ value in the encoder diagram shown in~\cref{fig:encoder}, with a dimensionality of $c$.

As an example, we take the most commonly used surface property, color.
Given an observed colored point cloud $\{(\V x_n, \V q_n)\}^N_{n=1}$ as input, where $\V q_n$ denote the RGB color values.
The corresponding surface property values are $\{\V y_n=\V q_n\}^N_{n=1}$.
We aggregate these values into two $N\times 3$ matrices $\V X$ and $\V Q$.
Therefore, the encoded feature for this point cloud is obtained using the following equation: 
\begin{equation}
\V F_{color}=f_{posi}(\V X)^T(f_{posi}(\V X)f_{posi}(\V X)^T +\delta^2_n\V I)^{-1}\V Q.
\end{equation}
Here, $\V F_{color}\in \mathbb R^{l\times 3}$ represents the color feature.

Since we use $l=20$ for the approximation function in our experiments, the color map only needs to store $20\times3$ float values in each voxel to represent a continuous color field.
It is important to note that our model requires no training and can be applied directly to different types of data.
During inference, since the field is in the surface space $\mathcal{S}$, we sample points $\V x_*$ at arbitrary resolutions, either from a known mesh or a surface constructed from previous surface mapping (\cref{sec:surface_mapping}).
Following~\cref{eq:decode}, the inference point $\V x_*$ is position encoded and multiplied by the color feature $\V F_{color,m}$ in the corresponding voxel $m$ to obtain its value $\V q_{*}=f_{dec}(\V x_{*}, \V F_{color,m})$.

\subsection{Surface Feature Fields}
\label{sec:latent_fields}

Surface feature fields represent an extension of the previous surface property fields discussed in~\cref{sec:context_fields}.
%
In this extension, we broaden the scope of surface properties to include features. 
This demonstrates the versatility of our mapping model, as it is applied directly without the need of any training.

We start by considering an embedding function $f_{im}:\mathbb R^{N\times 3}\rightarrow \mathbb R^{N\times c}$ that processes the input data $\V X$. We treat the feature of each point as a surface property, where
$\{\V y_n=f_{im} (\V X)_{\V x_n}\in \mathbb R^{l\times c}\}^N_{n=1}$.

Following the encoding and fusion steps described in~\cref{sec:context_fields} and~\cref{sec:fuse}, we construct latent implicit maps for the surface feature fields.
As a result, we extract maps of features at arbitrary resolutions using the function $f_{dec}(\cdot, \V F_\cdot)$.

We illustrate an application in~\cref{sec:openvoc}, specifically in the context of open-vocabulary scene understanding. 
In this application, our model constructs a CLIP space feature field on the surface,
enabling it to respond to textual input.
The key difference compared to surface property fields is demonstrated in~\cref{fig:latent_diff}.
In~\cref{fig:latent_diff}(c), a CLIP text encoder $f_{text}$ is added to encode the text command $\pmb u$ into the CLIP feature $\pmb U$.
By leveraging the surface field for CLIP embeddings generated by the left branch,
our model identifies the desired region by computing the similarity between features.
\begin{figure}[t]
	\centering
	\psfragfig[width=1\linewidth]{im/eps/latent}{
		\psfrag{t}{$f_{text}$}
		\psfrag{xm}{$\V X_m$}
		\psfrag{ym}{$\V Y_m$}
		\psfrag{fe}{$f_{enc}$}
		\psfrag{fm}{$f_{im}$}
		\psfrag{Fm}{$\V F_m$}
		\psfrag{x}{$\V x_{*}$}
		\psfrag{y}{$\V y_{*}$}
		\psfrag{fd}{$f_{dec}$}
		\psfrag{u}{$\pmb u$}
		\psfrag{U}{$\pmb U$}
		\psfrag{s}{$score$}
	}
	%\includegraphics[width=1\linewidth]{im/latent}
	\caption{Encoding-decoding diagram in various applications. (a) fold applications obtain point properties ($\V Y_\text{m}$) directly from the sensor. (b) fold applications derive point properties using a function $f_{im}$ that captures style, saliency and etc. (c) fold applications utilize feature as $\V Y_\text{m}$ to construct a LIM for a (CLIP) feature field. Then, a text command is used to extract the semantic information.}
	\label{fig:latent_diff}
\end{figure}

\subsection{Map Fusion}
\label{sec:fuse}

We adopt the voxel-to-voxel fusion approach from Neural Implicit Maps~\cite{huang2021di} to update the LIM. The fusion operation is performed as follows:
\begin{equation}
\label{eq:fuse}
\V F_m \leftarrow \frac{\V F_m w_m+\V F^{'}_m w^{'}_m}{w_m+w^{'}_m}, w_m\leftarrow w_m+w^{'}_m,
\end{equation}
where $\V v_m=(\V c_m, \V F_m, w_m)$ represents the voxel $m$ from the global LIM, and $\V v^{'}_m=(\V c^{'}_m, \V F^{'}_m, w^{'}_m)$ represents the voxel $m$ from  the local LIM.
