\section{Decoupled Regression as an Universal Encoding}
\label{sec:UE}

In this section, we propose a universal encoding model for point clouds.
Building on this encoder, in~\cref{sec:UCM}, we introduce Universal Continuous Mapping.

\subsection{Decoupled Gaussian Process Regression as Encoder-Decoder}% one page
\label{sec:encoder}

Inspired by DI-Fusion~\cite{huang2021di}, which introduces latent feature maps for continuous surface prediction, our goal is to generate a latent vector for each local patch.
%
% introduce the gaussian process
First, a universal encoder design comes from the Gaussian Process Regression (GPR), which is a widely used technique for low-dimensional regression.
It has been used in various mapping models~\cite{yuan2018fast,martens2016geometric}.
%
Given a set of $N$ observation points $\{(\V x_n, \V y_n)\}_{n=1}^{N}$ where point positions are $\V x_n \in \mathbb R^d$ ($d=3$ in this paper) and point properties are $\V y_n \in \mathbb R^c$, GPR is used to regress a function $f$ that best explains the data.
The $N$ points are aggregated in the $N\times d$ matrix $\V X$, and the targets are collected in the $N\times c$ matrix $\V Y$.
%The regression model estimation is $f(\V x)=E[\V y | \V X=\V x]$.
%based on the observation set. %Which is $\V y = f(\V x) + \espilon$ where $\epsilon$ is Gaussian noise. 

The Gaussian process assumes that the data is sampled from a multivariate Gaussian distribution, i.e.,
\begin{equation}
\begin{bmatrix}
\V Y\\
\V Y_{*}
\end{bmatrix}
	 \sim \mathcal{N}(\V 0, 
\begin{bmatrix}
k(\V X, \V X)\ k(\V X, \V X_{*})\\
k(\V X_{*}, \V X)\ k(\V X_{*}, \V X_{*})
\end{bmatrix}	 
	 ).
\end{equation}
where $(\V X, \V Y)$ and $(\V X_*, \V Y_*)$ represent the observation and inference pairs. 
For simplicity, we denote the matrix $\V K=k(\V X, \V X)$, $\V K_{*}=k(\V X, \V X_*)$, $\V K_{**}=k(\V X_*, \V X_*)$.

%
With the derivative in~\cite{williams2006gaussian}, we obtain 
\begin{equation}
	\V Y_{*} | \V X, \V Y, \V X_{*} \sim \mathcal{N} (\V K_{*}^T\V K^{-1}\V Y, \V K_{**}-\V K_{*}^T\V K^{-1}\V K_{*}).
\end{equation}
%
When an additional Gaussian error is introduced as $\V y=f(\V x)+\epsilon$, the covariance of $\V X$ is rewritten as $\V K+\delta^2_n\V I$.
%
The regression result is typically considered to be the mean
\begin{equation}
	\label{eq:mean}
	\V Y_{*} = \V K_{*}^T(\V K+\delta^2_n\V I)^{-1}\V Y.
\end{equation}
This well illustrates the challenge of using Gaussian process regression directly in large-scale reconstructions due to its $\mathcal{O}(n^3)$ time complexity, which is not feasible.
% with approxmiation
Furthermore, the formula (\cref{eq:mean}) is impractical since it requires the large input point cloud data to be maintained for the $ \V K_{*}$ computation. 

To address the issue of high complexity and avoid the need to maintain the entire point cloud, we propose to decouple the GPR to obtain the low-dimensional latent vectors for local regions.
%
The decoupling process involves approximating the kernel function as
\begin{equation}
	k(\V X,\V X_{*} ) \approx f_{posi}(\V X) f_\text{posi}(\V X_{*})^T
	\label{eq:k_approx}
\end{equation}
where $f_\text{posi}: \mathbb{R}^3\rightarrow \mathbb{R}^l$.
We refer to the function $f_\text{posi}$ as the position encoding function, consistent with the Neural Implicit Maps model, Di-Fusion~\cite{huang2021di}.
%
Thus, \cref{eq:mean} is rewritten as
\begin{equation}
	\label{eq:approx_K_mean}
	\begin{split}
	\V y_{*} %&= f_\text{posi}(\V X_*)^Tf_{posi}(\V X)(f_\text{posi}(\V X)^Tf_\text{posi}(\V X) +\delta^2_n\V I)^{-1}\V y\\
	%&
	=f_\text{posi}(\V X_*)f_\text{enc}(\V X, \V Y)
	\end{split}
\end{equation}
%With $X\in \mathbb{R}^{N\times 3}$, $y\in\mathbb{R}^{N\times c}$, we
where the content encoder function is
\begin{equation}
	\label{eq:encode}
	f_\text{enc}(\V X, \V Y) = f_\text{posi}(\V X)^T(f_\text{posi}(\V X)f_\text{posi}(\V X)^T +\delta^2_n\V I)^{-1}\V Y \in \mathbb{R}^{l \times c}.
\end{equation}

The encoded feature is denoted by $ \V F_{(\V X, \V Y)} = f_{enc}(\V X, \V Y) \in \mathbb{R}^{l\times c}$.
which serves as the basis for the construction of latent maps in the subsequent universal continuous mapping model (\cref{sec:UCM}).
Specifically, for geometry encoding we set $l=20$ and $c=1$, resulting in a $20$-dimensional vector feature.
Similarly, for color encoding, we have $l=20$ and $c=3$.

The decoding value for the inferred point $\V x_*$ is expressed as
\begin{equation}
	\label{eq:decode}
	f_{dec}(\V x_{*},\V F_{(\V X, \V Y)}) = f_{posi}(\V x_{*}) \V F_{(\V X, \V Y)}.
\end{equation}
Thus, a signed distance field or surface property field is approached.

In the following we derive the approximation function.

\begin{figure}[]
	\centering
    \psfragfig[width=1\linewidth]{im/eps/encoder1}{
	\psfrag{y1}{$\V y_1$}
	\psfrag{x1}{$\V x_1$}
	\psfrag{yn}{$\V y_n$}
	\psfrag{xn}{$\V x_n$}
	\psfrag{z1}{$\V z_1$}
	\psfrag{zn}{$\V z_n$}
	\psfrag{fp}{$f_{posi}$}
	\psfrag{fd}{$f_{dec}$}
	\psfrag{fe}{\color{mybronze}{$f_{enc}$}}
	\psfrag{ec}{\color{mybronze}{{Encoder}}}
	\psfrag{zm}{$\V Z_m$}
	\psfrag{ym}{$\V Y_m$}
	\psfrag{Fm}{$\V F_m$}
	\psfrag{x}{$\V x_{*}$}
	\psfrag{y}{$\V y_{*}$}
	\psfrag{f}{\tiny $\V Z_m^T(\V Z_m\V Z_m^T+\sigma_n\V I)^{-1}\V Y_m$}
}
	
	%\includegraphics[width=1\linewidth]{im/encoder}
	\caption{Interpreting formula with a graph that is coherent to the Encoder-decoder structure in Neural Implicit Maps~\cite{huang2021di}. }
	\label{fig:encoder}
	\vspace{-.3cm}
\end{figure}



\subsection{Position Encoding with Approximated Kernel Function} % one page
\label{sec:posi_encode}

Considering that our mapping needs to encode the local geometry \& property, the encoding function only requires to touch points in a limited region ($[-.5,.5]^3$ in our case).

As we discussed in related work, Nytr\"om methods offer greater accuracy than RFFs, because they depend on the given points. This property is well-suited to our application.

% introduce the Nytrom methods
The Nystr\"om method for kernel approximation begins with the use of eigenfunctions according to Mercer's theorem:
\begin{equation}
	 k(\V x_1, \V x_2) = \sum_{i\ge 1} \mu_i \psi_i(\V x_1)^T\psi_i(\V x_2)
	\label{eq:eigen}
\end{equation}
where $\psi_i$ and $\mu_i\ge 0$ are eigenfunctions and eigenvalues of kernel function $k$ with respect to the probability measure $q$.% With the top-$k$ eigenpairs, Nystr\"om method approximate kernel with $ k(\V x, \V y) = \sum^k_{i\ge 1} \alpha_i \varphi_i(\V x)\varphi_i(\V y)$.

Given a set of anchor samples $\hat{\V X}=\{\hat{\V x}_1,\cdots,\hat{\V x}_N\}$, 
% how we use it
we perform eigen-decomposition on the matrix $k(\hat{\V X}, \hat{\V X})$ to obtain its eigenpairs $\{(\lambda_i,\V u_i)\}_{i\in\{1,\cdots,l\}}$ with rank $l$.

Subsequently, Nystr\"om method produces
\begin{equation}
\label{eq:nytrom_1}
\psi_i(\V x) = \sum_{n}k(\V x, \hat{\V x}_n)\V u_{i,n}, i= 1,\cdots,l.
\end{equation}

% can be obtained with eigendecomposition of matrix $K(\hat{\V X}, \hat{\V X})$. 

%To approach
%$k(\V x, \V y) = \sum^{k}_{i= 1} \hat{\mu_i} \hat{\varphi_i(x)}\hat{\varphi_i(y)}$ 

%The \cref{eq:nytrom_1} is then rewriten as 
%\begin{equation}
%	\label{eq:nytrom_2}
%	\hat{\varphi}_i(\V x) = \frac{1}{N\hat{\mu_i}}\sum_{n=1}{N}k(\V x, \V x_n)\hat{\varphi}(\V x_n), i\in{1,\cdots,k}.
%\end{equation}


To simplify, we express the eigenfunction as 
\begin{equation}
\psi_i(\V x) = k(\V x, \hat{\V X})\V u_i .
\label{eq:nytrom_2}
\end{equation}
Similarly, the eigenvalue is written as $\mu_i=\frac{1}{\lambda_i}$.

For clarity, we introduce the notation $ \pmb \mu=[{\mu_1},\cdots,{\mu_l}]$ and
${\Psi}=[\psi_1,\cdots,{\psi_l}]^T$
 based on \cref{eq:eigen} where
${\Psi}:\mathbb{R}^3\rightarrow \mathbb{R}^l$.

To maintain consistency with \cref{eq:k_approx}, we set 
\begin{equation}
	f_{posi}(\V x) = diag(\sqrt {\pmb\mu})\Psi(\V x) 
	\label{eq:f_posi}
\end{equation}
where $diag$ produces diagonal matrix.
$f_{posi}$ above refers to the position encoder in \cref{eq:encode}.

In this paper, we employ the Mat\'ern kernel function~\cite{genton2001classes}\footnote{https://en.wikipedia.org/wiki/Mat\'ern\_covariance\_function}:
\begin{equation}
\label{eq:matern}
k(\V x_1, \V x_2) = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)}(\sqrt{2\nu}\frac{dist(\V x_1, \V x_2)}{\rho})K_{\nu}(\sqrt{2\nu}\frac{dist(\V x_1, \V x_2)}{\rho})
\end{equation}
where $\Gamma$ presents the gamma function, $K_{\nu}$ is the modified Bessel function of the second kind, $dist$ denotes the Euclidean distance, and $\sigma$ and $\rho$ are hyperparameters of the kernel function.
We utilize the half integer $\nu=3+\frac{1}{2}$, which results in the specific function:
\begin{equation}
\label{eq:matern_specific}
k(d) = \sigma^2(1+\frac{\sqrt{7}d}{\rho}+\frac{2}{5}(\frac{\sqrt{7}d}{\rho})^2+\frac{1}{15}(\frac{\sqrt{7}d}{\rho})^3)exp(-\frac{\sqrt{7}d}{\rho})
\end{equation}
where $d=dist(\V x_1, \V x_2)$ for short.

To approximate the above kernel function, anchor points are required.
We sample $N_a=256$ points uniformly 
from $[-.5,.5]^3$ cube (as $\hat{\V X}$) to compute the kernel matrix $\V K_{a}=k(\hat{\V X},\hat{\V X})$.
Subsequently, we perform an eigendecomposition on this kernel matrix, resulting in $\V K_{a}=\V U \Lambda \V U^T$. % with $U$ a $256\times $.
Lastly, $\V U$ and $\Lambda$ are utilized in \cref{eq:f_posi} and~\cref{eq:k_approx} to form the kernel function approximation.

It is important to note that the dimension of the encoded feature, $l$, used in \cref{sec:encoder}, depends on $\V U$. 
It further determines the size of the map in the next section (~\cref{sec:UCM}).% creating a trade-off between the approximation and map size.
%Therefore, the $l$ cannot be too large.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.\linewidth]{im/eigenvalue}
	\caption{Sorted eigenvalues for $\V K_{a}$'s eigendecomposition.}
	\label{fig:eigenvalues}
\end{figure}

The eigenvalues of $\Lambda$ are plotted in \cref{fig:eigenvalues}, revealing that the matrix is primarily influenced by a small number of pairs with significant eigenvalues.
Most of the eigenvalues are less than 1. 
Therefore, we choose $l=20$ which is about $0.8$ in this plot to approximate the kernel while maintaining a compact feature dimension.

Note that we perform a single sampling and decomposition step. Subsequently, the encoding-decoding process in \cref{fig:encoder} only requires loading and reusing the parameters for $f_{posi}$, $f_{enc}$ and $f_{dec}$. In contrast, related works often involve pre-training on large datasets of objects~\cite{huang2021di,li2022bnv} or indoor scenes~\cite{zhu2022nice}. 
For our model, however, \textbf{no training is needed}.

%For the convenience of use in the next section, this section also encapsulate functions into API as in~\cref{alg:encode-decode}.
%\begin{algorithm}[H]
%	\caption{Encoding and decoding}\label{alg:encode-decode}
%	\begin{algorithmic}
%		\STATE 
%		\STATE {\textsc{Encode}}$(\V X,\ \V y)$
%		\STATE \hspace{0.5cm}$\V Z \gets f_{posi}(\V X) $ \hspace{1in} \# \cref{eq:f_posi}
%		\STATE \hspace{0.5cm}$ \V F \gets \V Z(\V Z^T\V Z+\sigma_n\V I)^{-1}\V y$ \hspace{1cm} \# \cref{eq:encode}
%		\STATE \hspace{0.5cm}\textbf{return}  $\V F$
%		\STATE 
%		\STATE {\textsc{Decode}}$(\V {X_*},\ \V F )$
%		\STATE \hspace{0.5cm}$\V y_*=f_{posi}(\V X_*)^T\V F$ \hspace{2cm}\# \cref{eq:decode}
%		\STATE \hspace{0.5cm}\textbf{return}  $\V y_*$
%	\end{algorithmic}
%\end{algorithm}
