\section{Experiments}
\label{sec:exp}

In this section, we demonstrate the wide range of applications and the high capabilities of Uni-Fusion. 
First, we evaluate Uni-Fusion in application 1) Incremental surface and color reconstruction, comparing its performance with SOTAs.
%
For applications 2) and 5), which are new topics, no specific benchmarks are available. 
Therefore, we showcase the performance on existing results.
%
Next, we implement application 3) and compare it with SOTA zero-shot semantic segmentation models.
%
Finally, for application 4), since infrared data is not commonly used, we collect our own dataset containing infrared values and show all applications on this data.

\subsection{Implementation Details}
\label{sec:exp:details}

In the experiments, we use our sample-based GPIS for local geometry encoding.
For each point, two additional points are sampled along normal direction, one positive and one negative, with distance $d_s=0.1$ in the local voxel's normalized space. 
Compared to derivative-based GPIS, our sample-based GPIS is more efficient in both space and time. 
For the encoder, we randomly sample $256$ anchor points from the range $[-0.5,0.5]^3$.
We utilize the first $20$ eigenpairs, resulting in a feature dimension of $20$.
The model selection process is discussed in the ablation study.

Different latent maps use different granularities.
For the surface LIM, we use a voxel size of $5\si{\centi\meter}$. 
For color which requires later comparison to NeRF, we use a voxel size of $2\si{\centi\meter}$.
For other property LIM and feature LIM, we use a voxel size of $10\si{\centi\meter}$.

For smooth reconstruction, the encoded voxel is designed overlapped following~\cite{huang2021di}.
The encoded voxel uses twice the voxel size, resulting in a half-space overlap with each neighboring voxel.
During meshing, SDFs are retrieved and interpolated from the overlapped voxels~\cite{huang2021di}.
While for the remaining properties, we sample only from its own voxel part.

The implementation runs on a PC with AMD Ryzen 9 5950X 16-core CPU and an Nvidia Geforce RTX 3090 GPU (24 GB).

\subsection{Datasets}

We evaluate incremental reconstruction on the ScanNet dataset~\cite{dai2017scannet}, TUM RGB-D dataset~\cite{sturm2012benchmark}, and Replica dataset~\cite{sucar2021imap}.
Using MSG-Net~\cite{zhang2018multi}'s material set, we transfer styles to the 3D canvas.
For open-vocabulary scene understanding, we evaluate on ScanNet segmentation data~\cite{qi2017pointnet++} and S3DIS dataset~\cite{armeni20163d}.

\subsubsection{ScanNet~\cite{dai2017scannet}}

ScanNet is a densely annotated RGB-D video dataset.
It is captured with the structure sensor~\cite{occipital} and contains 1513 scenes for training and validation.
For each scene, both images and a 3D mesh is provided, along with their 2D and 3D semantic annotations. 

ScanNet provides 312 scenes for validation, which contains a wide range of different room structures.
It has now been widely used in the thorough evaluation of the performance of reconstruction and semantic segmentation.

\subsubsection{TUM RGB-D~\cite{sturm2012benchmark}}

TUM RGB-D is a benchmark to mainly evaluate the tracking performance.
It is captured with Microsoft Kinect sensor together with ground-truth trajectory from the sensor.

\subsubsection{Replica~\cite{sucar2021imap}}

The Replica dataset refers to iMAP's pre-processed dataset~\cite{sucar2021imap}.
It is a synthetic rendered RGB-D dataset from given 3D models.
The advantage of including this dataset is that Replica does not have motion blur. 
This is better to evaluate the capability of the algorithms on reconstructing surface color.

\subsubsection{MSG-Net Style~\cite{zhang2018multi}}

MSG-Net provides material images for transfering the styles.
We select 21style fold for demonstration.
These images are given in \cref{fig:style} together with our result.

\subsubsection{ScanNet Point Cloud Segmentation Data~\cite{qi2017pointnet++}}

For point cloud semantic segmentation benchmarking, PointNet++~\cite{qi2017pointnet++} preprocesses the original ScanNet~\cite{dai2017scannet} and generates subsampled point clouds and corresponding annotations for each scene.

\subsubsection{S3DIS~\cite{armeni20163d} and 2D-3D-S~\cite{armeni2017joint}}

S3DIS is a semantic segmentation dataset for 3D point clouds.
Which is also a subset of the 2D-3D-S dataset.
The 2D-3D-S dataset is a multi-modality dataset containing 2D, 2.5D and 3D domains. 
This dataset is densely annotated with semantic labels.

Note that 2D-3D-S's 2D captures is not a RGB-D video as ScanNet.
2D-3D-S's images only have small overlap. 
Therefore, it is only suitable for semantic segmentation and not for incremental reconstruction.

\subsection{Baselines}

For online surface mapping evaluation, we select TSDF-Fusion~\cite{curless1996volumetric}, iMAP~\cite{sucar2021imap}, SOTA DI-Fusion~\cite{huang2021di} and BNV-Fusion~\cite{li2022bnv} as four baseline methods.

For the color field, we choose TSDF-Fusion~\cite{curless1996volumetric}, $\sigma$-Fusion~\cite{rosinol2023probabilistic}, iMAP~\cite{sucar2021imap}, NICE-SLAM~\cite{zhu2022nice} and even the recent hot Neural Radiance Fields model NeRF-SLAM~\cite{rosinol2022nerf} as five baselines.
While including NeRF in the comparison may not be entirely fair, we want to show how Uni-Fusion narrows the performance gap.

For the scene understanding application, we evaluate generalized zero-shot point cloud semantic segmentation with ZSLPC~\cite{cheraghian2019zero}, DeViSe~\cite{frome2013devise} and SOTA 3DGenZ~\cite{michele2021generative} for comparison.

\subsection{Metrics}

For incremental reconstruction, we evaluate the geometric reconstruction using \textbf{Accuracy}, \textbf{Completeness}, and \textbf{F1 score} according to SOTA BNV-Fusion. It firstly uniformly samples $100,000$ points from the reconstruction and ground truth meshes respectively.
Then \textbf{Accuracy} (\textbf{Completeness}) measures the percentage of reconstruction-to-groundtruth (groundtruth-to-reconstruction) distances that are lower than $2.5\si{\centi\meter}$ threshold. \textbf{F1 score} is the harmonic mean of accuracy and completeness.
For tracking performance, we use \textbf{ATE RMSE}.

To evaluate color reconstruction, we follow SOTA on this topic, NeRF to render both depth and RGB images to compare the image level \textbf{Depth L1} and \textbf{RGB PSNR}.

To compare scene understanding, we follow zero-shot point cloud semantic segmentation SOTA 3DGenZ to evaluate the \textbf{Intersection-of-Union (IoU)} and \textbf{Accuracy}.


\input{tex/exp/surface}
\input{tex/exp/second}
\input{tex/exp/latent}


\subsection{Time}

We run all of the applications in a single pass using our captured office sequences and evaluate the time cost of construction and fusion of each LIM. 
The average time cost across frames is shown in~\cref{tab:time}.

\begin{table}[htbp]
	\caption{Time required for each frame.
	}
	\centering
	\footnotesize
	\setlength{\tabcolsep}{0.7em}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{l|ccccccc}
			\toprule
			&Surface & Color & Infrared & Style & Saliency & Latent&Internal Track \\ \midrule
			Time ($\si{\second}$)&0.100 & 0.038 & 0.045 & 0.048 & 0.045 &0.011 &0.225 \\ \bottomrule
	\end{tabular}}
	
	\label{tab:time}
\end{table}

\input{tex/exp/mine}

Using depth and property images of size $720\times1280$ as input, it is evident from the table, that our model operates at a frequency of $\sim10\si{\hertz}$ for  surface (sample mode) LIM construction and integration. 
It alse achieves a frequency of over $20\si{\hertz}$ for color, infrared, style, and saliency.
These results demonstrate the suitability of Uni-Fusion for real-time applications.

However, our internal tracking process takes around $0.225\si{\second}$ per frame, which is relatively slower compared to the mapping module. 
Nevertheless, Uni-Fusion uses external tracking to prevent tracking loss, enabling our internal tracking and mapping to operate at a lower frequency.
As a result, the entire model can be effectively applied in real-time in various scenarios.

\section{Extensive experiment on our own dataset}

In previous experiments, we evaluate the capabilities of Uni-Fusion in different applications. 
To further demonstrate its effectiveness in robotic environmental understanding, we capture our own dataset to show all applications together.

We capture two scenes: The office and apartment of the first author using a Microsoft Kinect Azure. 
%
RGB-D and infrared video are captured. After calibration, RGB, depth, infrared inputs have resolution of $720\times1280$.
Uni-Fusion tracks and reconstructs all applications in one pass.
%
While office data has been involved in ablation study (\cref{exp:surface:ablation}), we showcase all applications using the apartment dataset, as depicted in~\cref{fig:appartment}.

For better visualization, the ceiling of reconstruction is removed.
The top row of images presents the colored mesh with room details, the infrared mesh revealing the lighting effect, and the saliency reconstruction highlighting objects crucial for navigation.
Additionally, we select the second style from~\cref{fig:style} for style transfer to the apartment canvas.
%
As a result, the wooden floor in the room is colored with dark green.
The whole apartment is in a warm style.

The remaining results are generated from the surface field of the CLIP embeddings. 
We issue commands to locate objects, e.g., where is the sofa, desk and coat.
In addition, it easily identifies affordances such as being sittable.
For material, it successfully detects the wooden floor in each room.
