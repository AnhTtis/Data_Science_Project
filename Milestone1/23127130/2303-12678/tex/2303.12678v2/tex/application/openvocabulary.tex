\subsection{Application: Open-vocabulary Scene Understanding}
\label{sec:openvoc}

This application follows OpenScene and CLIP-Field~\cite{peng2022openscene,shafiullah2022clip}, which learn to predict dense features for 3D scene points,
where the features are co-embedded with text and image pixels in CLIP feature space.
Inspired by this, we design the mapping for surface feature fields (\cref{sec:latent_fields}). 
The distinction between surface property fields is illustrated in~\cref{fig:latent_diff}.
We use the pre-trained OpenSeg model~\cite{ghiasi2022scaling} to obtain the function
$f_{im}$ that produce CLIP feature for each image point.
Then we encodes the voxel latent $\V F_m$ from the CLIP features $\V Q_m$ and their corresponding positions $\V X_m$ .
During inference, given an open-vocabulary text input $\pmb u$ and a position input $\V x_*$, we obtain the CLIP space features
and determine the semantic property of the point based on similarity computation. 
The pipeline of this application is illustrated in ~\cref{fig:recons_and_scene_understanding}.

In this application, the image encoding function $f_{im}$ and the text encoding function $f_{text}$ are obtained from pre-trained model, while $f_{enc}$ and $f_{dec}$ in our Uni-Fusion framework are deterministic functions.
With these functions, our model constructs a continuous field for the CLIP feature on surface.

A very interesting and relevant work for Uni-Fusion's scene understanding application is VLMaps~\cite{huang2023visual}.
While VLMaps produces a 2D map, our model produces a surface CLIP feature field in 3D space.
