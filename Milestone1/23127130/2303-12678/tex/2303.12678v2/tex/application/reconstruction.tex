\subsection{Application: Incremental Reconstruction}
\label{sec:incremental_reconstruction}

\begin{figure*}[t]
	\centering
	\includegraphics[width=1.\linewidth]{im/application_pipeline}
	\caption{Reconstruction and scene understanding applications' pipeline. On the left incremental reconstruction application, external tracking runs in parallel to reconstruction to provide coarse poses. While doing reconstruction, internal tracking refines the pose estimation fur a better surface fit. ``$\cdots$'' means that we can add more other properties from \cref{sec:recons:other} and \cref{sec:fabircated_prop} into this pipeline. On the right scene understanding application, we assume that the pose of the frame is pre-known. The upper part of the white line is the fusion of LIM for the feature field. The lower part infers specific semantic information along with the text command.}
	\label{fig:recons_and_scene_understanding}
\end{figure*}
\label{sec:app_recons}

In this section, we present an application of incremental 3D reconstruction using RGB-D sequences.
Since RGB-D sequences provide both point positions and color values, it allows us to construct two types of LIMs: one for surface (\cref{sec:surface_mapping}), and one for color (\cref{sec:context_fields}).

The pipeline is illustrated in \cref{fig:recons_and_scene_understanding}.
When an RGB-D frame $i$ is fed into the framework, it is firstly converted into a colored point cloud ($\V X\in\mathbb R^{N\times 3}, \V Q\in\mathbb R^{N\times 3}$).
The tracking module takes $(\V X, \V Q)$ to estimate the current pose $\V T$.
%
Next, the transformed point cloud $\V X \V T^T$ is used as input to the surface mapping (\cref{sec:surface_mapping}), while the colored point cloud $(\V X \V T^T, \V Q)$ is used as input to the surface color mapping (\cref{sec:context_fields}), resulting in the generation of local LIMs.
%
Using the fusion operation in~\cref{eq:fuse}, the local LIM is fused into the global LIM on a voxel by voxel basis.

For visualization purposes, we first sample a grid within each voxel and infer using the global surface LIM to obtain the SDF.
The Marching Cube algorithm is then applied to extract the mesh.

Once the surface is reconstructed, we can sample points from it at arbitrary resolution and perform inference using the global color LIM to reconstruct the surface color.

\subsubsection{Tracking}
\label{sec:tracking}

According to Zhu et al.~\cite{zhu2022nice}, the current implicit scene representation-based tracking models, such as iMAP, DI-Fusion and NICE-SLAM, still have a performance gap compared to state-of-the-art tracking approaches such as BAD-SLAM and ORB-SLAM2.
Therefore, instead of following the neural implicit models to track with frame-to-model or ray-tracing based optimization, we incorporate ORB-SLAM2 in a separate thread to provide a pose prior. 
Hence, we call this external tracking.

Note that the primary focus of ORB-SLAM2 is on localization not scene reconstruction.
This means that direct use of ORB-SLAM2 provides coarse surface reconstruction.
We further use colored point cloud registration (CPCR)~\cite{park2017colored} as a tracking refiner.

In our implementation, ORB-SLAM2 runs independently over all frames.
Every few frames, CPCR tracks an initially posed colored point cloud to compute the odometry within a local window.
Mapping is then done in the same thread.
Therefore, the latter is called internal tracking.

\subsubsection{Other types of datas}
\label{sec:recons:other}

Application 4) uses a point cloud with infrared information, which is a straightforward modification of the color-based approach.
LIM feature dimension is also correspondingly reduced.
%
This flexibility allows different types of point cloud properties to be integrated into the continuous mapping pipeline.
