\Section{Introduction}
\label{sec:intro}
\vspace{\segsep}

% FAS
Face recognition (FR)~\cite{deng2019arcface} has achieved remarkable success and has been widely employed in mobile access control and electronic payments. Despite the promise, FR systems still suffer from presentation attacks (PAs), including print attacks, digital replay, and 3D masks. As a result, face anti-spoofing (FAS) has been an important topic for almost two decades~\cite{yang2014learn,liu2019deep,wang2022patchnet,atoum2017face,liu2018learning,yu2020face, kim2019basn}. 

% \input{subtex/figure_prob}
\input{subtex/figure_2}

% From intra-domain to cross-domain
In early systems like building access and border control with limited variations (\eg, lighting and poses), simple methods~\cite{boulkenafet2015face,freitas2012lbp,li2016original} have exhibited promise. These algorithms are designed for the closed-world setting, where the camera and environment are assumed to be the same between train and test. This assumption, however, rarely holds for in-the-wild applications, \eg, mobile face unlock and sensor-invariant ID verification.
Face images in those FAS cases may be acquired from wider angles, complex scenes, and different devices, where it is hard for training data to cover all the variations.
These differences between training and test data are termed domain gaps  and the FAS solutions to tackle the domain gaps are termed cross-domain FAS.


% Cross-domain FAS
Learning domain-invariant representation is the main approach in generic domain generalization~\cite{wang2022generalizing}, and has soon been widely applied to cross-domain FAS~\cite{wang2019improving,shao2019multi,jia2020ssdg,liu2021dual,liu2021adaptive,wang2022ssan}.
Those methods consider domain-specific signals as a confounding factor for model generalization, and hence aim to remove domain discrepancy from the feature representation partially or entirely. 
Adversarial training is commonly applied so that upon convergence the domain discriminator cannot distinguish which domain the features come from.
In addition, some methods apply metric learning to further regularize the feature space, \eg,~triplet loss~\cite{wang2019improving}, dual-force triplet loss~\cite{shao2019multi}, and single-side triplet loss~\cite{jia2020ssdg}.

% Cross-domain FAS Challenges
% 
There are two crucial issues that limit the generalization ability of these methods~\cite{wang2019improving,shao2019multi,jia2020ssdg,liu2021dual,liu2021adaptive,wang2022ssan} with domain-invariant feature losses. First, 
%
these methods posit a strong assumption that the feature space is perfectly domain-invariant after removing the domain-specific signals from training data. 
%
However, this assumption is unrealistic due to the limited size and domain variants of the training data, on which the loss might easily overfit during training. As shown in Fig.~\ref{fig:cmp_da_umap}, the test distribution is more expanded compared to the training one, and the spatial relation between live and spoof has largely deviated from the learned classifier.
% 
Second, feature space becomes ambiguous when domains are mixed together. Note that the domain can carry information on certain image resolutions, blurriness and sensor patterns.
If features from different domains are collapsed together~\cite{papyan2020prevalence}, the live/spoof classifier will undesirably leverage spurious correlations to make the live/spoof predictions as shown in Fig.~\ref{fig:teaser} (a), \eg, comparing live from low-resolution domains to spoof from high-resolution ones. Such a classifier will unlikely generalize to a test domain when the correlation does not exist.

%
 
% Our solution
In this work, we rethink feature learning for cross-domain FAS. Instead of constructing a domain-invariant feature space, we aim to find a generalized classifier while explicitly maintaining domain-specific signals in the representation. Our strategy can be summarized by the following two properties:
\begin{itemize}
    \tabvspace\item \textbf{Separability:} We encourage features from different domains and live/spoof classes to be separated which facilitates maintaining the domain signal.
    %which facilitates the learning of a domain-invariant decision boundary as shown in Fig.~\ref{fig:teaser} (right bottom). 
    According to~\cite{ben2006analysis}, representations with well-disentangled domain variation and task-relevant features are more general and transferable to different domains. 
    \tabvspace\item \textbf{Alignment:} Inspired by~\cite{jourabloo2018face}, we regard spoofing as the process of transition. For similar PA types\footnote{This work focuses on print and replay attacks.}, the transition process would be similar, regardless of environments and sensor variations. With this assumption, we regularize the live-to-spoof transition to be aligned in the same direction for all domains. 
    % 
\end{itemize}
We refer to this new learning framework as \textit{FAS with separability and alignment} (dubbed \textbf{SA-FAS}), shown in Fig.~\ref{fig:teaser} (b).
To tackle the separability, we leverage Supervised Contrastive Learning (SupCon)~\cite{2020supcon} to learn representations that force samples from the same domain and the same live/spoof labels to form a compact cluster. 
To achieve the alignment, we devise a novel Projected Gradient optimization strategy based on Invariant Risk Minimization (PG-IRM) to regularize the %live/spoof decision factors 
live-to-spoof transition invariant to the domain variance. 
%
With normalization, the feature space is naturally divided into two symmetric half-spaces: one for live and one for spoof (see Fig.~\ref{fig:umap}).
%
Domain variations will manifest inside the half-spaces but %be parallel 
have minimal impact to the live/spoof classifier. 
%

We summarize our contributions as three-fold:
\begin{compactitem}
    \item 
    We offer a new perspective for cross-domain FAS.
    Instead of removing the domain signal, we propose to maintain it and design the feature space based on separability and alignment;
    
    \item We first systematically exploit the domain-variant representation learning by combining contrastive learning and effectively optimizing invariant risk minimization (IRM) through the projected gradient algorithm for cross-domain FAS;
    
    \item We achieve state-of-the-art performance on widely-used cross-domain FAS benchmark, and provide in-depth analysis and insights on how separability and alignment lead to the performance boost.
\end{compactitem}
