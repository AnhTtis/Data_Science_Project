\Section{Experiments}
\label{sec:exp}

\SubSection{Experimental setups}


\Paragraph{Datasets and protocols} 
We  evaluate on four widely used  datasets: \texttt{Oulu-NPU} (\textbf{O})~\cite{boulkenafet2017oulu}, \texttt{CASIA} (\textbf{C})~\cite{zhang2012casia}, \texttt{Idiap Replay attack} (\textbf{I})~\cite{chingovska2012replay}, and \texttt{MSU-MFSD} (\textbf{M})~\cite{wen2015msu}. 
Following  prior works, we treat each dataset as one domain and apply the leave-one-out test protocol to evaluate their cross-domain generalization. 
Specifically, we refer \textbf{OCI$\rightarrow$M} to be the protocol that trains on \texttt{Oulu-NPU}, \texttt{CASIA}, \texttt{Idiap Replay attack} and tests on  \texttt{MSU-MFSD}. \textbf{OMI$\rightarrow$C}, \textbf{OCM$\rightarrow$I} and \textbf{ICM$\rightarrow$O} are defined in a similar fashion. 

% 
\Paragraph{Implementation details} 
The input images are cropped using MTCNN~\cite{zhang2016mtcnn} and resized to 256$\times$256. 
%
For fair comparisons with SoTA methods \cite{jia2020ssdg,wang2022ssan,wang2022patchnet}, we use the same ResNet-18 backbone.
We train the network with SGD optimizer and an initial learning rate of 5e\text{-}3, which is decayed by 2 at epoch 40 and 80 and the total training epoch is 100 in most set-ups\footnote{Due to the smaller training data size of \textbf{ICM}, we let the  \textbf{ICM$\rightarrow$O} to train for 300 epochs and decay at epoch 120 and 240. }.
We set the weight decay as 5e\text{-}4 and the batch size as 96 for each training domain. 
For \methodname hyperparameters, we set $\alpha\!=\!0.995$, $\lambda\!=\! 0.1$ and $T_a \!=\! 20$. 
%

\Paragraph{Evaluation metrics} We evaluate the model performance using three standard metrics: Half Total Error Rate
(HTER), Area Under Curve (AUC), and True Positive Rate (TPR95) at a False Positive Rate
(FPR) 5\%. 
While HTER and AUC assess the theoretical performance, TPR at a certain FPR is adept at
reflecting how well the model performs in practice. 
% 

\SubSection{Cross-domain performance}

Tab.~\ref{tab:best} summarizes our comparison with an extensive collection of recent studies, including SoTA methods: \texttt{PatchNet}~\cite{wang2022patchnet}, \texttt{SSAN}~\cite{wang2022ssan} and \texttt{SSDG}~\cite{jia2020ssdg}.
%
\methodname outperforms the rivals by a significant margin on cross-domain FAS benchmarks.
In particular, we improve upon the best baseline \cite{wang2022ssan} by 2.30\% in HTER in the setting \textbf{OCM$\rightarrow$I}, which is more than \textbf{25\%} improvement.


\Paragraph{Comparison upon convergence} 
Note that the performance in Tab.~\ref{tab:best} follows the convention in \cite{jia2020ssdg}, which is reported on the training snapshot (\eg, epoch 16) with the lowest test error. 
While this setting may manifest the best performance from the model, the results can significantly fluctuate on the test set and hard to reflect the generalization performance when a test set is unavailable (shown in Appendix~\ref{sec:whyfairsetting}).
To provide a more fair setting, we propose to report the average performance from the \textbf{last 10 epochs} upon convergence.
In our case, the stopping criterion is either (1) the binary classification loss for live/spoof is smaller than 1e\text{-}3 for consecutive 10 epochs, or (2) the epoch number reaches max limit, whichever comes first.

%

In Tab.~\ref{tab:mean}, we compare with SoTA methods in this setting, and provide three key observations: 
(1) The numbers are way worse than the ones in Tab.~\ref{tab:best} across all methods, indicating the best model selected by conventional lowest test errors \cite{jia2020ssdg} has large randomness.
This also shows that cross-domain FAS is far less-solved than expected.
%
(2) The standard deviation in Tab.~\ref{tab:mean} denotes how stable each method performs.
Most methods can converge to a relatively stable status, while methods with an adversarial loss (\eg, \cite{wang2022ssan}) have a relatively larger standard deviation, indicating adversarial loss might trigger more unstable training.
(3) In our setting, \methodname still largely outperforms SoTA \cite{wang2022patchnet,wang2022ssan,jia2020ssdg}, which further validates the superiority of our method.
Our method is also the most stable compared to SoTA, with the smallest standard deviation.
We proceed by analyzing why traditional methods are less favorable in cross-domain FAS and why our methods perform better.

%

\Section{Ablation and Discussion}
\input{subtex/tab_3}

\SubSection{Effectiveness of loss components}  
Our overall objective function Eq.~\eqref{eq:irm_constrain_overall} consists of two parts: (a) Separability loss ($\mathcal{L}_{sep}$) for feature space; and (b) Alignment loss ($\mathcal{L}_{align}$) for regularizing the classifier. We ablate the contribution of each component in Tab.~\ref{tab:abl_main}. 

\Paragraph{Separability loss}
%
We consider the two most common strategies used in the contrastive learning community (\ie, \texttt{SimCLR}~\cite{chen2020simclr}, \texttt{SimSiam}~\cite{chen2020simclr}) and one in face recognition (\ie, \texttt{Triplet loss}~\cite{schroff2015facenet}). We also provide the comparison of SupCon with the clustering policy used in SSDG~\cite{jia2020ssdg}\footnote{SSDG assumes live samples in all domains form one cluster, and spoof samples in each domain respectively form the other three clusters.}. 
%
All losses are directly applied on the penultimate layer's feature $\phi(\*x)$ with the same hyper-parameters, and all final classifications are supervised by ERM.
We observe that the SupCon loss used in our framework outperforms other rivals.
This validates the effectiveness of the domain-wise separable feature space for cross-domain FAS.

%
\Paragraph{Alignment loss}
IRM objective is known to be hard to optimize. 
Other than the proposed PG-IRM, existing works  \texttt{IRM-v1}~\cite{arjovsky2019irm}, \texttt{IB-IRM}~\cite{ahuja2021ibirm}, and \texttt{VRex}~\cite{krueger2021rex} alternatively consider a Lagrangian form:
\begin{equation}
\min _{\phi, \beta^{*}} \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}}\left[\mathcal{R}^{e}(\phi, \beta^{*})+\lambda\left\|\nabla_{\beta^{*}} \mathcal{R}^{e}(\phi, \beta^{*})\right\|_{2}^{2}\right].
\label{eq:irm_langran}
\end{equation}
In Tab.~\ref{tab:abl_main}, we compare PG-IRM with the baseline ERM as well as other IRM alternatives, and our method shows a better overall performance. This shows further evidence that the Lagrangian penalty term can be ineffective, especially in the non-linear case \cite{rosenfeld2020riskirm, kamath2021doesirm}. In comparison, PG-IRM optimizes the IRM objective directly with Projected Gradient, which clearly distinguishes it from existing methods. 

%
Overall, the ablation studies suggest all components in our framework are indispensable to enhancing the generalization ability of cross-domain spoof detection.
\input{subtex/figure_corr}
\input{subtex/figure_umap}
\input{subtex/figure_umap_cmp_da}


\SubSection{Separability and alignment analysis}  

SA-FAS aims to produce a feature space with two critical properties: Separability and Alignment. 
In this section, we empirically investigate if these two properties can lead to a better generalization performance. Specifically, we provide two corresponding measures based on the learned classifiers and the extracted feature vector $\*z$ of samples from the \textbf{test} domain. 
We define the separability score as:
$$S_{sep} =  1 - \cos(\mathbb{E}_{spoof}[\*z], \mathbb{E}_{live}[\*z]), $$
where we measure the cosine angle between the center of live/spoof features. 
A separated feature space naturally leads to a small cosine value and thus a larger $S_{sep}$ score. 
For the alignment score, we define:
\vspace{-2ex}
% 
\vspace{\baselineskip}
\begin{equation}
    S_{align} = \mathbb{E}_{e \in \mathcal{E}}[ \cos(\beta_e, \tikzmarknode{oracle}{\highlight{blue}{$\mathbb{E}_{spoof}[\*z] - \mathbb{E}_{live}[\*z])$}})],
\end{equation}
where the trajectory from the center of spoof to live is treated as an \textit{oracle} vector, which we measure how close it is with the norm vector of $\beta$ of the learned hyperplane.
\begin{tikzpicture}[overlay,remember picture,>=stealth,nodes={align=left,inner ysep=1pt},<-]
    \path (oracle.north) ++ (0,0.5em) node[anchor=south west,color=blue!67] (scalep){\textit{oracle vector}};
    \draw [color=blue!87](oracle.north) |- ([xshift=-0.3ex,color=blue]scalep.south east);
\end{tikzpicture}


With the measure of two properties, we show their correlation to the generalization performance (\ie, AUC) in Fig.~\ref{fig:corr_all}.
We see that $S_{sep}$ and $S_{align}$ are positively related to their test performance. 
It validates that these two properties are beneficial for a domain-invariant classifier. 
Specifically, Fig.~\ref{fig:corr_all}(a) compares the setting with and without PG-IRM. Using PG-IRM leads to a higher alignment score and AUC, which verifies that PG-IRM can better align the live-vs-spoof hyperplanes for the unseen domain and improve the generalization ability. Similarly, Fig.~\ref{fig:corr_all}(b) compares the setting with and without SupCon. The results validate that SupCon can lead to better separability in the feature space which benefits the classification. 

\SubSection{UMAP visualization}  
% 
Fig.~\ref{fig:umap} first provides UMAP~\cite{mcinnes2018umap} visualization of \methodname feature space from the penultimate layer. We see that the hyperplane between live samples and spoof samples is consistent across different training domains and also transferable to unseen test domains. 
For instance, in the setting of \texttt{OMI$\rightarrow$C}, 
the test live samples in blue circles can be separated from the test spoof samples in blue crosses by the hyperplane.  Another interesting finding is that some \texttt{CASIA} samples in blue are closer to \texttt{OULU} with \textbf{high resolution} and some are closer to \texttt{MSU} or \texttt{REPLAY} with \textbf{low resolution}, which reflects the fact that \texttt{CASIA} is a mixed dataset with both low and high resolution images. 
These findings validate that the domain gap (resolution) manifests in a way that is invariant to the live-vs-spoof hyperplane. 

%
Beyond numerical and visual results, the superiority of domain-variant feature space can also be validated by theoretical support. Specifically, the estimated error bound for binary
classification in domain generalization~\cite{blanchard2021domain} becomes larger if $(M, n)$ is replaced with $(1, Mn)$, where $M$ is the domain number and $n$ is the training set size per domain. 
It indicates that separately training datasets from different domains is better than pooling them into one mixed dataset.

% 
\Paragraph{DANN~\cite{ganin2016dann} and SSDG~\cite{jia2020ssdg} visualization} 
We also compare the feature space of methods that aim to remove domain-specific signals from its feature representation.
%
DANN~\cite{ganin2016dann} leverages the adversarial loss to encourage the backbone to provide a domain-invariant feature. 
%
Fig.~\ref{fig:cmp_da_umap}(a) shows that the domain gap yet still broadly exists, especially for the test data from an unseen domain, which backfires on the generalizability of the classifier.
%
Similarly, SSDG~\cite{jia2020ssdg} learns a partial domain-invariant feature space where all live samples are clustered in one group while spoof samples are kept to be domain-dispersed. 
Although the degradation direction aligns better between train and test, compared to DANN, the domain gap still exists for live training samples as shown in Fig.~\ref{fig:cmp_da_umap}(b). 
%
These findings further validate the necessity of regularizing the live-vs-spoof hyperplanes to be consistent across different domains.

% 


