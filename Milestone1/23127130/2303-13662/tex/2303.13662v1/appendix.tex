\clearpage
\newpage
\appendix

\input{subtex/figure_method_sup}


% \section{Methdology Illustration with Real Data}
% \label{sec:method-sup}



\section{Detailed Proof}
\label{sec:proof}
In the main paper, we propose to use a project gradient algorithm to efficiently optimize the hard IRM objective. In this section, we provide a formal proof composed of two main steps: 
(1) We show in Section~\ref{sec:proof_s1} that the original IRM objective is equivalent to the PG-IRM objective shown in Theorem~\ref{th:ourirm_objective}. (2) In Section~\ref{sec:proof_s2}, we show that the PG-IRM objective can be efficiently optimized by the project gradient descent algorithm illustrated in Alg.~\ref{alg:proj_grad_appendix}. 


\subsection{PG-IRM objective is equivalent to IRM}
\label{sec:proof_s1}
As a recap of our learning setting, a learner is given access to a set of training data from $E$ environments $\mathcal{E} = \{e^{(1)}, e^{(2)}, .., e^{(E)}\}$ and the IRM objective is the following constrained optimization problem:

\begin{align}
    & \min _{\phi, \beta^{*}} \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} \mathcal{R}^{e}(\phi, \beta^{*}) \quad  \label{eq:irm_target_appendix} \\ 
    & \text { s.t. } \quad \beta^{*} \in \underset{\beta}{\arg\min  } \mathcal{R}^{e}(\phi, \beta) \quad \forall e \in \mathcal{E},
\label{eq:irm_constrain_appendix}
\end{align}
where the risk function for a given domain/distribution e is:

$$
\mathcal{R}^{e}(\phi, \beta) \doteq \mathbb{E}_{(\*x_i, y_i, e_i=e) \sim \mathcal{D}} \ell\left(f(\*x_i;\phi,\beta), y_i\right).
$$



\begin{theorem*}  (Recap of Theorem~\ref{th:ourirm_objective})
For all $\alpha \in (0,1) $, the IRM objective is equivalent to the following objective: 

\begin{align}
    & \min _{\phi, \beta_{e^{(1)}}, ..., \beta_{e^{(E)}}} \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} \mathcal{R}^{e}(\phi, \beta_e) \quad  \label{eq:irm_target_sepirm} \\ 
    &\text { s.t. }  \forall e \in \mathcal{E}, \exists \beta_e \in \Omega_e(\phi), \beta_e \in \Upsilon_{\alpha}(\beta_e),
\label{eq:irm_constrain_sepirm}
\end{align}
where the parametric constrained set for each environment is simplified as 
$$ \Omega_e(\phi) = \underset{\beta}{\arg \min } \mathcal{R}^{e}(\phi, \beta),$$
and we define 
\begin{align}
\begin{split}
    & \Upsilon_{\alpha}(\beta_e) = \{\upsilon | \underset{\forall e' \in \mathcal{E}  \backslash e, \beta_{e'} \in \Omega_{e'}(\phi)}{\min}\|\upsilon - \beta_{e'}\|_2 \\
    & \le \alpha \underset{\forall e' \in \mathcal{E}  \backslash e, \beta_{e'} \in \Omega_{e'}(\phi)}{\min}\|\beta_e - \beta_{e'}\|_2\}
\end{split}
\end{align}
\label{th:pgirm_sup}
\end{theorem*}

\begin{proof}


The constraint (\ref{eq:irm_constrain_appendix}) means that the $\beta^*$ is the optimal linear classifier at all environments, which is equivalent to saying that $\beta^*$ lies in the joint of the optimal solution set in each environment.
Equivalently, we can formularize the optimization target (\ref{eq:irm_target_appendix}) as a parametric constrained optimization problem with constrain:  


% \begin{align}
%   \quad \beta^{*} \in  \underset{e \in \mathcal{E}}{\cap} \Omega_e(\phi), 
%   \label{eq:irm_constrain_set}
% \end{align}

% FANCY EQUATION 
\vspace{\baselineskip}
\begin{equation}
    \label{eq:irm_constrain_set_appendix}
    \beta^{*} \in  \underset{e \in \mathcal{E}}{\cap} \tikzmarknode{omega}{\highlight{blue}{$\Omega_e(\phi)$}}, 
\end{equation}
\begin{tikzpicture}[overlay,remember picture,>=stealth,nodes={align=left,inner ysep=1pt},<-]
    % For "X"
    \path (omega.north) ++ (0,0.5em) node[anchor=south west,color=blue!67] (scalep){$\underset{\beta}{\arg \min } \mathcal{R}^{e}(\phi, \beta)$};
    \draw [color=blue!87](omega.north) |- ([xshift=-0.3ex,color=blue]scalep.south east);
\end{tikzpicture}

where the parametric constrained set for each environment is $\Omega_e(\phi) = \underset{\beta}{\arg \min } \mathcal{R}^{e}(\phi, \beta)$ (Note that $\Omega_e(\phi)$ can be a set with cardinality bigger than 1, since the optimal linear classifier may not be unique). The constraint (\ref{eq:irm_constrain_set_appendix}) implies that $\beta^*$ lies in the joint set of $\Omega_e(\phi)$, which also means that there is an element in each $\Omega_e(\phi)$ equal to $\beta^*$. We refer to such element to be $\beta_e \in \Omega_e(\phi)$, and we have the alternative form: 

\begin{align}
   \forall e \in \mathcal{E}, \exists \beta_e \in \Omega_e(\phi), \beta^{*} = \beta_e
   \label{eq:ele_in_env_appendix}
\end{align}

Equivalently, 

% \begin{align}
%   \forall e \in \mathcal{E}, \exists \beta_e \in \Omega_e(\phi), \beta_e \in \underset{e' \in \mathcal{E}}{\cap} \Omega_{e'}(\phi)
% \end{align}

% FANCY EQUATION 
\vspace{\baselineskip}
\begin{equation}
   \forall e \in \mathcal{E}, \exists \beta_e \in \Omega_e(\phi), \beta_e \in \tikzmarknode{betastar}{\highlight{blue}{$\underset{e' \in \mathcal{E}  \backslash e}{\cap} \Omega_{e'}(\phi)$}}
   \label{eq:no_beta_star_appendix}
\end{equation}
\begin{tikzpicture}[overlay,remember picture,>=stealth,nodes={align=left,inner ysep=1pt},<-]
    % For "X"
    \path (betastar.north) ++ (0,0.5em) node[anchor=south west,color=blue!67] (scalep){by (\ref{eq:irm_constrain_set_appendix}) and (\ref{eq:ele_in_env_appendix})};
    \draw [color=blue!87](betastar.north) |- ([xshift=-0.3ex,color=blue]scalep.south east);
\end{tikzpicture}

The interpretation of constraint (\ref{eq:no_beta_star_appendix}) is that --- for all environments, there is a hyperplane in the optimal set $\Omega_{e}(\phi)$ that also lies in the intersection of other environments' optimal set ($\underset{e' \in \mathcal{E}  \backslash e}{\cap} \Omega_{e'}(\phi)$). 
Now we rewrite the optimization target (\ref{eq:irm_target}) as:

\begin{align}
    & \min _{\phi, \beta_{e^{(1)}}, ..., \beta_{e^E}} \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} \mathcal{R}^{e}(\phi, \beta_e) \quad  \label{eq:irm_target_v1} \\ 
    &\text { s.t. } \forall e \in \mathcal{E}, \exists \beta_e \in \Omega_e(\phi), \beta_e \in   \underset{e' \in \mathcal{E} \backslash e}{\cap} \Omega_{e'}(\phi)
\label{eq:irm_constrain_v1}
\end{align}

In this way, we can get rid of finding a unique $\beta^*$, but instead optimizing multiple linear classifiers $\beta_{e^{(1)}}, ..., \beta_{e^E}$, which is easier to optimize in a relaxed form as we will show next. 

One key challenge for this optimization problem is that there is no guarantee that $\underset{e' \in \mathcal{E} \backslash e}{\cap} \Omega_{e'}(\phi)$ is non-empty for a feature extractor $\phi$ and $\beta_e$. We therefore relax the optimization target as: \vspace{-0.2cm}

% \begin{align}
%     & \min _{\phi, \alpha, \beta_{e^{(1)}}, ..., \beta_{e^E}} \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} \mathcal{R}^{e}(\phi, \beta_e) \quad  \\ 
%     &\text { s.t. } \forall e \in \mathcal{E}, \exists \beta_e \in \Omega_e(\phi), \|\beta_e - \underset{e' \in \mathcal{E} \backslash e}{\cap} \Omega_{e'}(\phi)\|_2 < \alpha, 
% \label{eq:irm_constrain_v2_appendix}
% \end{align}

% FANCY EQUATION 
\vspace{\baselineskip}
\begin{align}
    & \min _{\phi, \epsilon, \beta_{e^{(1)}}, ..., \beta_{e^E}} \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} \mathcal{R}^{e}(\phi, \beta_e) \quad  \\ 
    &\text { s.t. } \forall e \in \mathcal{E}, \exists \beta_e \in \Omega_e(\phi),
    \tikzmarknode{approx}{\highlight{blue}{$\underset{e' \in \mathcal{E} \backslash e}{\max} \|\beta_e - \Omega_{e'}(\phi)\|_2 \le \epsilon$}}, 
   \label{eq:irm_constrain_v2_appendix}
\end{align}
\begin{tikzpicture}[overlay,remember picture,>=stealth,nodes={align=left,inner ysep=1pt},<-]
    \path (approx.south) ++ (0,-0.5em) node[anchor=north east,color=blue!67] (scalep){relax $\beta_e \in \underset{e' \in \mathcal{E} \backslash e}{\cap} \Omega_{e'}(\phi)$};
    \draw [color=blue!87](approx.south) |- ([xshift=-0.3ex,color=blue]scalep.south west);
\end{tikzpicture}


where we define the $l_2$ distance between a vector $\beta$ and a set $\Omega$ as : $\|\beta - \Omega\|_2 = \underset{\upsilon \in \Omega}{\min} \|\beta - \upsilon\|_2 $. 


Practically, $\epsilon$ can be set to be any variable converging to 0 during the optimization stage. Without losing the generality, we change the constraint (\ref{eq:irm_constrain_v2_appendix}) to the following form: 

\begin{align}
\begin{split}
 & \forall e \in \mathcal{E}, \exists \beta_e \in \Omega_e(\phi), \\ 
 & \underset{e' \in \mathcal{E} \backslash e}{\max}  \ \underset{\beta_{e'} \in \Omega_{e'}(\phi)}{\min}\|\beta_e - \beta_{e'}\|_2 \le \\ & \quad \quad \quad \quad \quad \quad \alpha 
 \underset{e' \in \mathcal{E} \backslash e}{\max} \ \underset{\beta_{e'} \in \Omega_{e'}(\phi)}{\min}\|\beta_e - \beta_{e'}\|_2, 
\label{eq:irm_constrain_v4_appendix}
\end{split}
\end{align}

where $\alpha \in (0,1) $. Note that constraint (\ref{eq:irm_constrain_v4_appendix}) will be satisfied only when $\underset{e' \in \mathcal{E} \backslash e}{\max}  \ \underset{\beta_{e'} \in \Omega_{e'}(\phi)}{\min}\|\beta_e - \beta_{e'}\|_2 = 0$. Therefore,  constraint (\ref{eq:irm_constrain_v4_appendix}) is equivalent to constraint (\ref{eq:no_beta_star_appendix}), and thus equivalent to the original constraint (\ref{eq:irm_constrain_appendix}). 

If we let the set 

\begin{align}
\begin{split}
    & \Upsilon_{\alpha}(\beta_e) = \{\upsilon | \underset{e' \in \mathcal{E} \backslash e}{\max}  \ \underset{\beta_{e'} \in \Omega_{e'}(\phi)}{\min}\|\upsilon - \beta_{e'}\|_2 \\
    & \le \alpha \underset{e' \in \mathcal{E} \backslash e}{\max}  \ \underset{\beta_{e'} \in \Omega_{e'}(\phi)}{\min}\|\beta_e - \beta_{e'}\|_2\}
\end{split}
\end{align}

Then the constraint (\ref{eq:irm_constrain_v4_appendix}) can be simplified to 
\begin{align}
 & \forall e \in \mathcal{E}, \exists \beta_e \in \Omega_e(\phi), \beta_e \in \Upsilon_{\alpha}(\beta_e)
\label{eq:irm_constrain_v5_appendix}
\end{align}

\end{proof}


\subsection{Projected Gradient Optimization for PG-IRM objective}
\label{sec:proof_s2}
We proceed with introducing how the Projected Gradient Descent can effectively optimize the PG-IRM objective. We start by introducing the background of  the Projected Gradient Descent algorithm. 

% \paragraph{Preliminaries on the Projected Gradient Descent (PGD)}

Projected Gradient Descent is commonly applied in constrained optimization, which aims to find a point $\theta$ achieving the smallest value of some loss function $\mathcal{L}$ subject to the requirement that $\theta$ is contained in the feasible set $\Omega$. Formally, the objective can be written as: 
$$\min _{\theta \in \Omega} \mathcal{L}(\theta)$$

If we minimize the objective $\mathcal{L}(\theta)$ by gradient descent, we have $$(\text{GD})\quad \theta := \theta - \gamma \nabla \mathcal{L}(\theta),$$
where $\gamma$ is the step size. However, it is not guaranteed that the updated $\theta$ still falls into the set $\Omega$. The projected gradient descent (PGD) algorithm is designed to project the solution back in the feasible set. Formally, 
$$(\text{PGD})\quad\theta := P_{\Omega}(\theta - \gamma \nabla \mathcal{L}(\theta)),$$
where the $P_{\Omega}(\cdot)$ is defined as the Euclidean Projection:

$$P_{\Omega}(u)=\underset{v \in \Omega}{\arg \min} \|u-v\|_2 $$

In the PG-IRM objective, we have the constraint set $\Omega = \Upsilon_{\alpha}(\beta_e)$, we show in the next Lemma~\ref{lemma:linear_interp}
 that the Euclidean Projection from $\beta_e$ to $\Upsilon_{\alpha}(\beta_e)$ is equivalent to the linear interpolation between $\beta_e$ and the farthest hyperplane $\beta_{\bar{e}}$ for environment $\bar{e}$. 


\begin{lemma} Given that 

\begin{align*}
\begin{split}
    & \Upsilon_{\alpha}(\beta_e) = \{\upsilon | \underset{e' \in \mathcal{E} \backslash e}{\max}  \ \underset{\beta_{e'} \in \Omega_{e'}(\phi)}{\min}\|\upsilon - \beta_{e'}\|_2 \\
    & \le \alpha \underset{e' \in \mathcal{E} \backslash e}{\max}  \ \underset{\beta_{e'} \in \Omega_{e'}(\phi)}{\min}\|\beta_e - \beta_{e'}\|_2\}
\end{split}
\end{align*}
We have:
    $$P_{\Upsilon_{\alpha}(\beta_e)}(\beta_e) = \alpha
    \beta_e + (1 - \alpha)  \beta_{\bar{e}},$$
where $\beta_{\bar{e}}$ is selected with $\bar{e}  = \underset{e' \in \mathcal{E}  \backslash e }{\text{argmax}} \|\beta_e - \beta_{e'}\|_2$.
\label{lemma:linear_interp}
\end{lemma}

\begin{proof}
    We give the proof in an intuitive way shown in Figure~\ref{fig:proof}. Specifically, the feasible region $\Upsilon_{\alpha}(\beta_e)$ can be regarded as an intersection of several hyper-spheres centered with all domain-wise live-vs-spoof hyperplanes $\beta_{e'}$. The radius is given by the $\alpha$ multiplying the distance to the farthest hyperplane $\beta_{\bar{e}}$. Therefore the Euclidean projection of $\beta_e$ to the feasible set simultaneously lies on the surface of the hypersphere and the line segments between $\beta_e$ and $\beta_{\bar{e}}$. It can be easily verified that $$P_{\Upsilon_{\alpha}(\beta_e)}(\beta_e) = \alpha
    \beta_e + (1 - \alpha)  \beta_{\bar{e}},$$ satisfies the given criteria.

    
\end{proof}

\input{subtex/figure_proof.tex}


\input{subtex/alg_1}

% \begin{theorem*} 
% If the PG-IRM+ loss w.r.t network $f$ is Lipschitz continuously differentiable with constant L. Then the iterates generated by Alg.~\ref{alg:proj_grad_appendix} converge to a local minimizer at O(1/t).
% \end{theorem*}


\paragraph{Main results.} When we have the projected form on the constraint set, deriving the optimization strategy is thus straightforward. As shown in Alg.~\ref{alg:proj_grad_appendix}, we first calculate the gradient of  hyperplanes for all domains
$$\tilde{\beta}^{t+1}_e = \beta^{t}_e - \gamma \nabla_{\beta^{t}_e} \mathcal{L}_{\textit{PG-IRM}}.$$ We then select the farthest domain-wise hyperplanes $\beta_{\bar{e}}$ from other environments. The final projection results are thus given by $$\beta_e^{t+1}=\alpha^{\prime} \beta_e^{t+1}+\left(1-\alpha^{\prime}\right) \beta_{\bar{e}},$$ as we demonstrated in Lemma~\ref{lemma:linear_interp}.


\paragraph{Remark on the $T_a$.} In the first $T_a$ epochs, we let the feature encoder $\phi$ and domain-wise hyperplanes $\beta_e$ trained in a standard way. The goal is to ensure that the hyperplanes $\beta_e$ will reach or be close to the minimum of the domain-wise empirical risk, and we have:
$$\beta_e \in \Omega_e(\phi).$$ In Alg.~\ref{alg:proj_grad_appendix}, we use an additional parameter $\alpha'$ to manifest this procedure:
$$\alpha^{\prime}:=1-\mathbf{1}_{t>T_a}(1-\alpha)$$
Specifically, when $t<T_a$, $\alpha'=1$, which means the original gradient descent algorithm is applied. When $t<T_a$, $alpha'=\alpha$, the projected gradient descent takes charge.

% We then select the 
%     \State select $\beta^{t}_{\bar{e}}$ with $\bar{e}  = \underset{e' \in \mathcal{E}  \backslash e }{\text{argmax}} \|\tilde{\beta}^{t+1}_e - \beta^{t}_{e'}\|_2$
%     \State $\beta^{t+1}_e = \alpha
%     \tilde{\beta}^{t+1}_e + (1 - \alpha)  \beta_{\bar{e}}$



\section{Why do we need a fair setting?}
\label{sec:whyfairsetting}

By visualizing the line plot of the HTER performance over 100 training epochs in Fig.~\ref{fig:test_curve}, we realize the test performance on the unseen domain is highly testset-dependent and unstable especially in the early epochs. Therefore, the best number reported commonly adopted in existing literature~\cite{wang2022patchnet, jia2020ssdg, wang2022ssan} usually happens in an unpredictable earlier epoch. Such ``best'' snapshot is also hard to be selected by validation strategy because we have zero information regarding the test domain. As an alternative, we noticed that the test performance is more stable in the last 10 epochs upon convergence, which motivates us to propose using a fairer comparison strategy introduced in Section~\ref{sec:exp}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/test_curve.pdf}
    \caption{The line plot of the HTER performance tested on \texttt{MSU} dataset when trained on \texttt{CASIA}, \texttt{Replay} and \texttt{OULU} with SSDG-R~\cite{jia2020ssdg} and SA-FAS  over 100 training epochs. }
    \label{fig:test_curve}
\end{figure}



\section{Convergence of PG-IRM}
\label{sec:cosine_curve}

Recall that in PG-IRM, we optimize multiple linear classifiers simultaneously $\beta_{e^{(1)}}, \beta_{e^{(2)}}, \beta_{e^{(3)}}$ and gradually align them during training. In this section, we would like to verify if PG-IRM indeed regularizes domain classifiers to be close to each other and finally converges to the same one $\beta^*=\beta_{e^{(1)}}=\beta_{e^{(2)}}=\beta_{e^{(3)}}$. Empirically, we use the averaged cosine distance between domain classifiers to measure the distance between them: 

$$S_{\text{cos}}= \mathbb{E}_{e,e'\in \mathcal{E}, e \neq e'}[cos(\beta_e,\beta_{e'})]$$

As shown in Fig.~\ref{fig:consine_curve}, the averaged cosine value between domain classifiers diminishes gradually and finally converges to 1, which suggests that they converge to a $\beta^*$ that is aligned for all domains. 

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/cosine_curve.pdf}
    \caption{The line plot of the $S_{\text{cos}}$ when trained on \texttt{CASIA}, \texttt{Replay} and \texttt{OULU} with PG-IRM over 100 training epochs. }
    \label{fig:consine_curve}
\end{figure}


\section{Sensitivity Analysis}
\label{sec:sensitivity}

In this section, we perform the sensitivity analysis of hyper-parameter settings for SA-FAS in Fig.~\ref{fig:hyper}. The performance comparison in the bar plot for each hyper-parameter is reported by fixing other hyper-parameters. In the figure, we observe that the performance of SA-FAS is less sensitive to the learning rate and the alignment starting epoch compared with the maximum gap of $1.2\%$ in the given range. We also notice that choosing the right alignment parameter $\alpha$ is more important, since a proper $\alpha$ ensures the domain-wise decision boundaries are aligned not too fast and not too slow. In the extreme case, if $\alpha=0$, it degenerates to the ERM after epoch $T_a$ and if $\alpha=1$, the domain-wise boundaries will never get aligned with each other. In summary, our algorithm does not require heavy hyper-parameter tuning as long as it falls into a reasonable range. 


\begin{figure*}[htb]
    \centering
    \includegraphics[width=1\linewidth]{figs/sensitivity.pdf}
    \caption{Sensitivity analysis of hyper-parameters: learning rate $\gamma$, alignment parameter $\alpha$, alignment starting epoch $T_a$. The HTER is reported on the mean performance based on the last 10 epochs.  The middle bar in each plot corresponds to the hyperparameter value used in our main experiments.  }
    \label{fig:hyper}
\end{figure*}



\section{Limitation}
\label{sec:limit}
Our work has two limitations. Firstly, our framework assumes the dataset collected from each domain contains both live and spoof data. For example, SA-FAS can not handle the  training data  with  live samples only from domain A and spoof samples only from domain B. Secondly, SA-FAS may cause extra computation costs when the domain amount is very large since we set up one hyperplane for each domain.

\input{subtex/figure_corr_supp}

