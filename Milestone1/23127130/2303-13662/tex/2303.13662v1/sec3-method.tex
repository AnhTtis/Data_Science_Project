%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% SETUP  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Section{Method}
\label{sec:method}
\vspace{\segsep}

We formally introduce the new learning framework, \textit{FAS with separability and alignment} (dubbed \textbf{SA-FAS}). The goal is to produce a feature space with two critical properties: 
(1) \textit{Separability}: We encourage samples from different domains and from different classes to be well-separated; 
(2) \textit{Alignment}: Live-to-spoof transition\footnote{The transition can be considered as a path in the high-D manifold. }  is aligned in the same direction for all domains.  
These two properties work jointly: 
\textit{separability} ensures the awareness of domain variance in the feature space; \textit{alignment} encourages the domain variance to be invariant to its live-vs-spoof hyperplane. 
%

This section is structured as follows:
Sec.~\ref{sec:setup} describes the problem setup, followed by the algorithm design of separability (Sec.~\ref{sec:sep}) and alignment (Sec.~\ref{sec:align}).
Finally, Sec.~\ref{sec:train_step} summarizes the training and inference processes.

\input{subtex/figure_3}

%

\SubSection{Problem Setup}
\label{sec:setup}
We start by defining the setting of the cross-domain FAS problem. We denote by $\mathcal{X}=\mathbb{R}^d$  the input space and 
$\mathcal{Y}=\{0 \text{ (live)}, 1 \text{ (spoof)}\}$ 
the output space. A learner is given access to a set of training data from $E$ domains $\mathcal{E} = \{e^{(1)}, e^{(2)}, .., e^{(E)}\}$ and is evaluated on test domain $e^*$. Let $e_i$ as the domain label for the $i$-th sample, we denote $\mathcal{D}=\{(\*x_i,y_i, e_i)\}_{i=1}^N$ drawn from an unknown joint data distribution $\mathcal{P}$ defined on $\mathcal{X} \times \mathcal{Y} \times \mathcal{E}$. 
Cross-domain FAS is a special binary classification problem to distinguish live and spoof faces from an unseen domain. The goal is to define a decision function:
\begin{align}
    f: \mathbf{x} \rightarrow \{0 \text{ (live)}, 1 \text{ (spoof)}\}, \nonumber
\end{align}
which classifies whether a sample $\*x$ from a new domain $e^*$ is live or spoof. 

In our network architecture, function $f$ consists of two components: (1) a deep neural network encoder $\phi:\mathcal{X} \rightarrow \mathbb{R}^m$ that maps the input $\bx$ to a $l_2$-normalized feature embedding $\*z = \phi(\bx)$; 
%
(2) a classifier (via a weight vector) $\beta:\mathbb{R}^m \rightarrow \mathbb{R}$ that maps the $m$-dimensional embedding $\*z$ to a scalar value, where a binary cross-entropy loss can be applied after using a sigmoid function. Because the true distribution of live/spoof data is unknown, the optimization commonly relies on an Empirical Risk Minimization (ERM).

\noindent \textbf{Remark on the terminology}: $\beta$ can be considered as a norm vector of the hyperplane separating live and spoof samples. In the remaining part of the paper, when we use ``\textbf{live-vs-spoof hyperplane}'' or ``\textbf{hyperplane}'', it has the same meaning as $\beta$. Note, ``live-to-spoof transition'' is an abstract procedure in the image space, while ``live-vs-spoof hyperplane'' refers to a concrete classifier in the feature space. 

\Paragraph{Preliminary on Empirical Risk Minimization (ERM):}
ERM principle~\cite{vapnik1991principles} is a ubiquitous strategy that merges data from all training domains and learns a predictor that minimizes an averaged training error.
Specifically,
\begin{align}
    & \mathcal{L}_{\textit{ERM}} = \min _{\phi, \beta} \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} \mathcal{R}^{e}(\phi, \beta), 
\label{eq:erm}
\end{align}
where the empirical risk function $\mathcal{R}^{e}(\phi, \beta)$ for a given environment $e$ is defined by:
$$
\mathcal{R}^{e}(\phi, \beta) \triangleq \mathbb{E}_{(\*x_i, y_i, e_i=e) \sim \mathcal{D}} \ell\left(f(\*x_i;\phi, \beta), y_i\right).
$$
Common choices of the loss function $\ell(\cdot,\cdot)$ include cross-entropy loss~\cite{jia2020ssdg} and $L_1$ regression loss~\cite{liu2018learning, george2019deep}.

However, if samples from different domains are mixed together, 
ERM can utilize the easiest difference (image resolution, blurriness, camera setting) to differentiate live \vs.~spoof. 
Such a classifier will undesirably leverage spurious correlations to make  live/spoof predictions~\cite{arjovsky2019irm}. Therefore, the naive strategy can hurt the generalization of the unseen domain. % 
As shown in Fig.~\ref{fig:method}(a), ERM tends to fit all training data together and fails to learn a domain-invariant classifier with the mixed feature space. 
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% METHOD  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\SubSection{Separability}
\label{sec:sep}

%
We characterize the domain separability as supervised contrastive learning (dubbed SupCon)~\cite{2020supcon}, one of the latest developments for visual representation learning. Unlike other contrastive learning methods~\cite{chen2020simclr, chen2021exploring} that treat the augmented samples as a single class, SupCon aims to learn a representation space that gathers samples with the same labels while repelling samples from different ones. It naturally suits the need for the cross-domain FAS setting, since we treat samples with the same domain and with the same live/spoof label to form a cluster. 
%


%\textbf{Training objective} 
Given a training mini-batch $\{\*x_i, y_i, e_i\}_{i=1}^b$, we augment~\cite{2020supcon} the mini-batch as $\{\tilde{\*x}_i, \tilde{y}_i, \tilde{e}_i\}_{i=1}^{2b}$, using two random augmentations $\tilde{\bx}_{2i}$ and $\tilde{\*x}_{2i-1}$ of inputs $\bx_i$, with $\tilde{y}_{2i-1} \!=\! \tilde{y}_{2i} \!=\! y_i$, $\tilde{e}_{2i-1} \!=\! \tilde{e}_{2i} \!=\! e_i$. These images are fed into the network, yielding $L_2$-normalized embeddings $\{\*z_i\}_{i=1}^{2b}$. The per-batch SupCon loss (separability loss) is defined as:
\begin{equation}
\mathcal{L}_\textit{sep}=\sum_{i=1}^{2b} \frac{-1}{|S(i)|} \sum_{j \in S(i)} \log \frac{\exp \left(\*z_{i} \cdot \*z_{j} / \tau\right)}{\sum_{t=1, t\neq i}^{2b}\exp \left(\*z_{i} \cdot \*z_{t} / \tau\right)},
\label{eq:supcon}
\end{equation}
where $\tau$ is a temperature parameter, $i$ is the index of a sample typically called the \textit{anchor}, $S(i) \!=\! \{j \!\in\!\{1,\ldots,2b\} \!:\! j\neq i, \tilde{y}_j = \tilde{y}_i, \tilde{e}_j = \tilde{e}_i\}$ is the index set of \emph{positive samples} 
%consisting of the indices of the augmented sample and other 
that have the same live/spoof labels and belong to the same domain as the anchor $i$, and $|S(i)|$ is its cardinality. 
All the other samples in the mini-batch are referred to as \textit{negative samples}. 
%
Since positive samples are pulled together and negative samples are pushed apart, SupCon in Fig.~\ref{fig:method}(b) is capable of providing more distinguishable feature clusters for different domains and liveness classes, compared to a typical feature space learned by a vanilla ERM in Fig.~\ref{fig:method}(a). 
%
\SubSection{Alignment}
\label{sec:align}

% 
Fig.~\ref{fig:method}(b) also shows that separability alone is not sufficient for improving domain generalization. 
The separated feature clusters can be located in any place in the feature space, and hence the domain-wise optimal hyperplane remains \textbf{variant}.
In this case, the global classifier can still undesirably incorporate the spurious correlation as the deciding factor as we show in Fig.~\ref{fig:teaser}.
%  
To tackle this, we naturally investigate the following problem: 

\textit{How do we regularize a global live-vs-spoof hyperplane to align with domain-wise live-vs-spoof hyperplanes?}

We propose to formulate this problem as Invariant Risk Minimization (IRM)~\cite{arjovsky2019irm}, which aims to jointly optimize the feature space $\phi$ and the global live-vs-spoof hyperplane $\beta$, where $\beta$ is also optimal for each domain, shown in Fig.~\ref{fig:method}(c). 

%   

\input{subtex/figure_4}

% 
\Paragraph{Preliminary on Invariant Risk Minimization (IRM):}
Specifically, the IRM objective can be formulated as the following constrained optimization problem:

\begin{align}
    & \min _{\phi, \beta^{*}} \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} \mathcal{R}^{e}(\phi, \beta^{*}) \rightarrow \mathcal{L}_{\textit{IRM}}  \label{eq:irm_target} \\ 
    & \textit{s.t.} \quad \beta^{*} \in \underset{\beta}{\arg\min} \mathcal{R}^{e}(\phi, \beta), \forall e \in \mathcal{E}.
\label{eq:irm_constrain}
\end{align}

%

Compared to the ERM~\eqref{eq:erm}, IRM enforces an additional constraint \eqref{eq:irm_constrain} to learn the domain-invariant hyperplanes. 
Specifically, if we define the domain-wise optimal hyperplane as 
%
$\beta_e \in {\arg\min  }_{\beta} \mathcal{R}^{e}(\phi, \beta)$.
A sufficient condition for constraint \eqref{eq:irm_constrain} to hold is $\beta_{e^{(1)}}=...=\beta_{e^{(E)}} = \beta^*$, which requires consistency between the globally optimal hyperplane and the domain-wise optimal hyperplanes.
However, IRM is known to be hard to solve~\cite{kamath2021doesirm,rosenfeld2020riskirm} due to the bi-level optimization nature of objective~(\ref{eq:irm_target}) and constraint~(\ref{eq:irm_constrain}). 

%

%\paragraph{PG-IRM objective} 
\Paragraph{Projected Gradient Optimization for IRM (PG-IRM):} 
%
We leverage Projected Gradient (PG) algorithm~\cite{nocedal1999numerical} to solve the non-trivial optimization objective~\eqref{eq:irm_target}, termed as PG-IRM. 
In PG-IRM, we propose to optimize multiple hyperplanes and converge them into a globally one via projected gradient. In Appendix~\ref{sec:proof_s1}, we provide detailed proof of PG-IRM objective being \textbf{equivalent} to IRM. Formally, the objective is rewritten as:
\begin{theorem}
\label{th:ourirm_objective}
\textbf{(PG-IRM objective)} For all $\alpha \!\in\! (0,1) $, the IRM objective is equivalent to the following objective: 

\begin{align}
    & \min _{\phi, \beta_{e^{(1)}}, ..., \beta_{e^{(E)}}} \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} \mathcal{R}^{e}(\phi, \beta_e) \rightarrow \mathcal{L}_{\textit{align}} \\ 
    &\text { s.t. }  \forall e \in \mathcal{E}, \exists \beta_e \in \Omega_e(\phi), \beta_e \in \Upsilon_{\alpha}(\beta_e), \nonumber 
    % 
\end{align}
where the parametric constrained set for each environment is simplified as 
$ \Omega_e(\phi) = \underset{\beta}{\arg \min } \mathcal{R}^{e}(\phi, \beta),$
and we define the \textbf{$\alpha$-adjacency set}: 
\begin{align}
    \Upsilon_{\alpha}(\beta_e) = \{\upsilon |& \underset{e' \in \mathcal{E} \backslash e}{\max}  \ \underset{\beta_{e'} \in \Omega_{e'}(\phi)}{\min}\|\upsilon - \beta_{e'}\|_2 \\
    & \le \alpha \underset{e' \in \mathcal{E} \backslash e}{\max}  \ \underset{\beta_{e'} \in \Omega_{e'}(\phi)}{\min}\|\beta_e - \beta_{e'}\|_2\}
    \label{eq:gamma_set}
\end{align}
\figvspace
\end{theorem}
Fig.~\ref{fig:illustration} shows the intuition of  the optimization process. 
%
For a $3$-domain case, PG-IRM starts with a shared feature space $\phi$ and $3$ separate hyperplanes $\beta_{e^{(1)}}$, $\beta_{e^{(2)}}$, $\beta_{e^{(3)}}$ for each domain (Fig.~\ref{fig:illustration}(b)). 
After each projected gradient descent, the hyperplanes move closer with feature space jointly updated (Fig.~\ref{fig:illustration}(c)). 
Upon convergence, $\beta_{e^{(1)}}, \beta_{e^{(2)}}, \beta_{e^{(3)}}$ become nearly identical (Fig.~\ref{fig:illustration}(d)), satisfying the IRM constraint $\beta^* \!=\! \beta_{e^{(1)}} \!=\! \beta_{e^{(2)}} \!=\! \beta_{e^{(3)}}$ for {\it all} domains.
%The above explanation delivers the following 
We provide two main insights of our PG-IRM  algorithm (see more details in Appendix~\ref{sec:proof}):
\begin{compactenum}
    \item \textbf{Optimizing multiple  hyperplanes:} 
    Compared to the conventional IRM that optimizes a single hyperplane, it is easier to converge for PG-IRM that optimizes multiple hyperplanes (\ie, one for each domain).
    Shown in Fig.~\ref{fig:illustration}(a-b), for the same feature space from the intermediate optimization stage, the solution $\beta^*$ to conventional IRM may not exist and the optimization has to be terminated.
    In contrast, $\beta_{e^{(1)}}, ..., \beta_{e^{(E)}}$ \textbf{always} exists (Fig.~\ref{fig:illustration}(b)) which makes solving for multiple hyperplanes more viable.
    %
    \item \textbf{Pushing hyperplanes to be closer:}
    To align $\beta_{e^{(1)}}$, $\beta_{e^{(2)}}$ and $\beta_{e^{(3)}}$,
    %
    PG-IRM updates domain-wise hyperplanes by interpolating with other hyperplanes. It can be mathematically considered as projecting the parameters of a hyperplane into the $\alpha$-adjacency set $\Upsilon_{\alpha}(\beta_e)$ as we illustrated in Fig.~\ref{fig:proj}.

    \input{subtex/figure_projection.tex}

    \textbf{Remark (why PG is not applicable to IRM):}
    The PG algorithm can be infeasible for the conventional IRM, as the solution set for \eqref{eq:irm_constrain} can be \textbf{empty} and is thus non-projectable.
    Our PG-IRM objective in Eq.~\eqref{eq:gamma_set} contains a non-empty $\alpha$-adjacency set $\Upsilon_{\alpha}(\beta_e)$, and guarantees being projectable by simple linear interpolation. 

    %
\end{compactenum}


%\Paragraph{Intuition.}  For the reader's convenience, 


% \input{subtex/alg_1}

% 


\input{subtex/alg_main}

% 
\input{subtex/tab_1}
\input{subtex/tab_2}


\SubSection{Training and inference}
\label{sec:train_step}

\Paragraph{Overall losses}
Considering the contrastive loss Eqn.~\eqref{eq:supcon}, the overall objective (dubbed as SA-FAS) can be written as: 

\begin{align}
    & \min _{\phi, \beta_{e^{(1)}}, ..., \beta_{e^{(E)}}} \mathcal{L}_{\textit{\textit{align}}} + \lambda \mathcal{L}_{\textit{sep}} \quad \rightarrow \mathcal{L}_{\textit{all}}  
    \label{eq:irm_constrain_overall} \\
    &\text { s.t. }  \forall e \in \mathcal{E}, \exists \beta_e \in \Omega_e(\phi), \beta_e \in \Upsilon_{\alpha}(\beta_e), \nonumber
    %\figvspace
\end{align}
where $\lambda$ is the coefficient for the loss term. The overall training pipeline is provided in Alg.~\ref{alg:main}.
% 

\Paragraph{Inference}
At the inference stage, we use the mean hyperplane from $\beta_{e^{(1)}}, ..., \beta_{e^{(E)}}$ to get the final score. 
% 
Specifically, the output is given by $$f(\*x) = \mathbb{E}_{e\in\mathcal{E}}[\beta_{e}^T\phi(\*x)].$$
Note that upon convergence, the cosine distance between any two of $\beta_{e^{(1)}}, ..., \beta_{e^{(E)}}$ is very close to 1, \ie, $\beta_{e^{(1)}} \!\approx\! ... \!\approx\! \beta_{e^{(E)}}$. 
This observation is verified in Appendix~\ref{sec:cosine_curve}, with an ablation (converged angles \vs different $\alpha$) in Appendix~\ref{sec:sensitivity}. 



% 