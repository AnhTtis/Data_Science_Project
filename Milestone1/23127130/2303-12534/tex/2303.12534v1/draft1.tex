\documentclass[]{revtex4-2}

\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{siunitx}
\usepackage{float}
\usepackage{mathtools}
\usepackage{fullpage}
\usepackage[hidelinks]{hyperref}
\usepackage{bm}% bold math
\usepackage{subdepth}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{algorithm}
% \usepackage[mathlines]{lineno}% Enable numbering of text and display math
% \linenumbers\relax % Commence numbering lines

% \usepackage[inline]{showlabels}
% \renewcommand{\showlabelfont}{\scriptsize\ttfamily\color{gray}}

\setcounter{secnumdepth}{3}

\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\thesubsection.\arabic{subsubsection}}

% move corresponding email to after affiliations
%\makeatletter
%\def\@email#1#2{%
% \endgroup
% \patchcmd{\titleblock@produce}
%  {\frontmatter@RRAPformat}
%  {\frontmatter@RRAPformat{\produce@RRAP{*#1\href{mailto:#2}{#2}}}\frontmatter@RRAPformat}
%  {}{}
%}%
%\makeatother

%%%%%%%%%%%%%%%%%%%%
% Macros
\newcommand*{\T}[1]{\mathcal{T}^{#1}}
\newcommand*{\sT}[1]{\mathcal{S}^{#1}}
\newcommand*{\I}{\mathcal{I}}

\DeclareMathOperator{\E}{\mathbbm{E}}
\let\Pr\undefined
\DeclareMathOperator{\Pr}{\mathbbm{P}}
\DeclarePairedDelimiter{\set}{\{}{\}}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiterX{\inner}[2]{\langle}{\rangle}{{#1},{#2}}

\newcommand*{\ind}[1]{\mathbbm{1}_{#1}}
\newcommand*{\bmin}{\wedge}                 % binary min
\newcommand*{\bmax}{\vee}                   % binary max
\newcommand*{\tran}{\mathsf{T}}             % transpose
\newcommand*{\comp}{\mathsf{c}}  
\newcommand*{\stp}[1]{{T({#1})}}
%%%%%%%%%%%%%%%%%%%%




\begin{document}

\title{Inexact iterative numerical linear algebra for neural network-based spectral estimation and rare-event prediction}

%\date{\today}

\author{John Strahan}
\affiliation{Department of Chemistry and James Franck Institute, University of Chicago, Chicago, Illinois 60637, United States}
\author{Spencer C. Guo}
\affiliation{Department of Chemistry and James Franck Institute, University of Chicago, Chicago, Illinois 60637, United States}
\author{Chatipat Lorpaiboon}
\affiliation{Department of Chemistry and James Franck Institute, University of Chicago, Chicago, Illinois 60637, United States}
\author{Aaron R. Dinner}
\email{dinner@uchicago.edu}
\affiliation{Department of Chemistry and James Franck Institute, University of Chicago, Chicago, Illinois 60637, United States}
\author{Jonathan Weare}
\email{weare@nyu.edu}
\affiliation{Courant Institute of Mathematical Sciences, New York University, New York, New York 10012, United States}


\begin{abstract}
    Understanding dynamics in complex systems is challenging because there are many degrees of freedom, and those that are most important for describing events of interest are often not obvious.  The leading eigenfunctions of the transition operator
    % ,
    % which encodes the statistics of the dynamics,
    are useful for visualization, and they can provide an efficient basis for computing statistics such as the likelihood and average time of events (predictions).  Here we  develop inexact iterative linear algebra methods for computing these eigenfunctions (spectral estimation) and making predictions from a data set of short trajectories sampled at finite intervals.  We demonstrate the methods on a low-dimensional model that facilitates visualization and a high-dimensional model of a biomolecular system.  Implications for the prediction problem in reinforcement learning are discussed.
\end{abstract}

\maketitle

\section{Introduction}

Modern observational, experimental, and computational approaches often yield high-dimensional time series data (trajectories) for complex systems.  In principle, these trajectories contain rich information about dynamics and, in particular, the infrequent events that are often most consequential.
% Careful statistical analysis of this data is essential for understanding the dynamics of complex systems and how those dynamics depend on parameters of the system. 
In practice, however, high-dimensional trajectory data are often difficult to parse for useful insight. The need for more efficient statistical analysis tools for trajectory data is critical, especially when the goal is to understand rare events that may not be well represented in the data.
% and the challenges are much more severe when the goal is to understand rare and extreme events.
% It is often difficult to distinguish between the many degrees of freedom that are often irrelevant and those that are relevant. The order and timing of steps leading up to these events (if they happen at all) can vary considerably from one realization to another.  

We consider dynamics that can be treated as Markov processes.  A common starting point for statistical analyses of Markov processes is the transition operator, which describes the evolution of function expectations.  The eigenfunctions of the transition operator characterize the most slowly decorrelating features (modes) of the system \cite{noe_variational_2013, nuske_variational_2014, klus2018data, webber2021error,lorpaiboon_integrated_2020}. 
% which are often associated with function \cite{}. 
These can be used for dimensionality reduction to obtain a qualitative understanding of the dynamics 
% and the ensemble of states most often visited 
\cite{mcgibbon_identification_2017,busto2021structural}, or they can be used as the starting point for further computations \cite{perez-hernandez_identification_2013, schwantes_improvements_2013,strahan_long-time-scale_2021}.  Similarly, prediction functions, which provide information about the likelihood and timing of future events as a function of the current state, are defined through linear equations of the transition operator\cite{thiede_galerkin_2019,strahan_long-time-scale_2021}.

A straightforward numerical approach to obtaining these functions is to convert the transition operator to a matrix by projecting onto a finite basis for Galerkin approximation\cite{swope_describing_2004, noe_constructing_2009, bowman_introduction_2014, noe_variational_2013, nuske_variational_2014, thiede_galerkin_2019, strahan_long-time-scale_2021, finkel_learning_2021}.
The performance of such a linear approximation depends on the choice of basis \cite{thiede_galerkin_2019, strahan_long-time-scale_2021,finkel_learning_2021}, and previous work often resorts to a set of indicator functions on a partition of the state space (resulting in a Markov state model or MSM \cite{bowman_introduction_2014}) for lack of a better choice.
While Galerkin approximation has yielded many insights \cite{antoszewski_kinetics_2021,guo_dynamics_2022}, the limited expressivity of the basis expansion has stimulated interest in (nonlinear) alternatives.

%kernel methods \cite{schwantes_modeling_2015} 

In particular, artificial neural networks can be harnessed to learn eigenfunctions of the transition operator and prediction functions from data \cite{andrew_deep_2013, mardt_vampnets_2018, wehmeyer_time-lagged_2018, lusch_deep_2018, chen_nonlinear_2019, lorpaiboon_integrated_2020, glielmo_unsupervised_2021, strahan_forecasting_2022, li_semigroup_2022}.  
%Include NNs based on infinitesimal generator?
% \cite{khoo_solving_2018, wang_pastfuture_2019, li_computing_2019, li_semigroup_2022, strahan_forecasting_2022, hasyim_supervised_2022} 
However, existing approaches based on neural networks suffer from various drawbacks.
As discussed in Ref.~\onlinecite{lorpaiboon_integrated_2020}, their performance can often be very sensitive to hyperparameters, requiring extensive tuning and varying with random initialization.  Many use loss functions that are estimated against the stationary distribution \cite{khoo_solving_2018, li_computing_2019, roux_string_2021, li_semigroup_2022, roux_transition_2022, rotskoff_active_2022}, so that metastable states contribute most heavily, which negatively impacts performance \cite{rotskoff_active_2022,strahan_forecasting_2022}.  Assumptions about the dynamics (e.g., microscopic reversibility) limit applicability.
In Ref.~\onlinecite{strahan_forecasting_2022} we introduced an approach that overcomes the issues above, but it uses multiple trajectories from each initial condition; this limits the approach to analysis of simulations and moreover requires specially prepared data sets.
 % enabling application to observational, experimental, and computational data sets.

% In general, despite the fact that MD data can be generated at unprecedented pace, identifying and extracting these useful quantities remains difficult without expert knowledge.
% Data-driven approaches 

% The algorithms above have largely been pursued from the perspective of unsupervised learning in that they seek to determine a function that assigns samples values based on their relationships to other samples.  Yet it is another branch of machine learning that centers on prediction functions: reinforcement learning seeks to estimate them in the form of value functions to find an optimal policy for achieving a goal \cite{sutton_reinforcement_2018}.  %cai_neural_2019
% This connection suggests that methods from reinforcement learning (and associated studies of Markov decision processes) can be useful for estimating prediction functions for characterizing dynamics and, by the same token, that methods for dynamics can advance reinforcement learning, particularly when the space of possible states and actions is so large that approximation is necessary.  Here, we pursue both of these possibilities.

%However, the updates are biased due to the double-sampling problem.  Our method circumvents this by iteratively updating the current estimated function and repeatedly applying our stochastic operator, analogous to the Bellman evaluation operator. 

%Although the eigenfunctions and forecast functions can be written in terms of the transition operator, it is either unknown or intractable to work with directly for high-dimensional systems.
%Instead, we have access to a fixed set of samples drawn from the underlying transition operator (usually from MD simulations).
%Importantly our method does not require direct access to the operator itself, freeing us from restrictive assumptions about the underlying model of dynamics.
%Recent work has shown that one can parameterize the desired quantities through a neural network, and these parameters can then be optimized via gradient descent using samples of the underlying dynamics.\cite{wen_batch_2020}

% Maybe we can just point out in the intro (in addition to the sentence about inexact pm for the invariant distribution) that there is a relationship between semigradient for TD and inexact power method that we will explain and then exploit later. 




The need to compute prediction functions from observed trajectory data also arises in reinforcement learning (RL).
There the goal is to optimize the prediction function (an expected future reward) over the distribution of a Markov process (a policy). For a fixed Markov process, the prediction problem in RL is often solved by temporal difference (TD) learning, a family of methods that allow the use of arbitrary ensembles of trajectories without knowledge of the details of the underlying dynamics\cite{sutton_reinforcement_2018}.  TD methods have a close relationship with an inexact form of power iteration, which, as we describe, can perform poorly on rare event related problems.

% Similar questions have been addressed in reinforcement learning (RL).  There the goal is to optimize a prediction function (an expected future reward) over the distribution of a Markov process (a policy). For a fixed Markov process, the prediction problem in RL is often solved by temporal difference (TD) learning, a family of methods that allow the use of arbitrary ensembles of trajectories without knowledge of the details of the underlying dynamics\cite{sutton_reinforcement_2018}. As we describe, TD methods have a close relationship with an inexact form of power iteration.


%In applications of reinforcement learning (RL), a similar problem has been addressed in estimating prediction functions (namely, the expected future value for a fixed control policy) from observational data.  For this task RL relies on temporal-difference (TD) learning, a family of methods that allow the use of arbitrary ensembles of trajectories without knowledge of the details of the underlying dynamics\cite{sutton_reinforcement_2018}. 
%% The structure of TD methods allows their application to trajectory data of any fixed, finite length.
%% Reinforcement learning (RL) relies on just such a tool to estimate prediction functions (expected future value) for a fixed policy.  That tool, temporal difference (TD) methods, is flexible enough to estimate prediction functions using observational, experimental, and computational data.
%As we describe, TD methods have a close relationship with an inexact form of power iteration. %We use this relationship to both explain the poor performance of TD methods in the rare-event context, as well as to motivate new, more effective algorithms for prediction of rare-events and the spectral estimation problem.

Motivated by this relationship, as well as  by an inexact power iteration scheme previously proposed for approximation of the stationary probability distribution of a Markov process using trajectory data~\cite{wen_batch_2020}, here we propose a computational framework for spectral estimation and rare-event prediction based on inexact iterative numerical linear algebra. 
% But inexact iterative schemes for multiple eigenpairs and for prediction have not been explored, nor have the difficulties posed by rare events.
% (previously used to learn a neural network approximation of the stationary distribution of a Markov process \cite{wen_batch_2020}), 
% We draw on work that uses neural networks to learn the stationary distribution of a Markov process from samples of dynamics by an inexact power method \cite{wen_batch_2020}.  
Within our framework, we demonstrate that convergence of iterative methods can be accelerated significantly by (inexact) subspace iteration.  
While we assume the dynamics can be modeled by a Markov process, we do not require knowledge of their form or a specific underlying model.
The method shares a number of further advantages with the approach discussed in Ref.\ \onlinecite{strahan_forecasting_2022} without the need for multiple trajectories from each initial condition in the data set.  This opens the door to treating a wide range of observational, experimental, and computational data sets.  

The remainder of the paper is organized as follows. In Section~\ref{sec:setup}, we describe the quantities that we seek to compute in terms of linear operators.  In Sections~\ref{sec:inexactPower} and \ref{sec:ieSI}, we introduce an inexact subspace iteration algorithm that we use to solve for these quantities. Section~\ref{sec:softplus} illustrates how the loss function can be tailored to the known properties of the desired quantity.
Section~\ref{sec:testproblems} summarizes the two test systems that we use to illustrate our methods: a two-dimensional potential, for which we can compute accurate reference solutions, and a molecular example that is high-dimensional but still sufficiently tractable that statistics for comparison can be computed from long trajectories.  In Section~\ref{sec:eig}, we explain the details of the invariant subspace iteration and then demonstrate its application to our two examples.  Lastly, Section~\ref{sec:forecast} details how the subspace iteration can be modified to compute prediction functions and compares the effect of different loss functions, as well as the convergence properties of power iteration and subspace iteration. % Finally, we illustrate our methods on the molecular example.
%We demonstrate our method on two well-characterized systems from the molecular dynamics literature. 
We conclude with implications for reinforcement learning.

%Our method uses the insight that, while it is not possible to write down a loss whose minimizer is the function of interest and whose gradient can be estimated from data, it is possible to use optimization to apply the transition operator to a known, fixed, function.  We therefore use classical iterative linear algebra algorithms which involve repeated matrix-vector products to solve for our desired functions.  The simplest such scheme for the eigenproblem is the power iteration.
%This method relies on the classical power iteration for the dominant eigenproblem to obtain the stationary distribution of the transition operator, which distribution is the dominant left eigenfunction.
%In many cases, in particular rare event problems, the power iteration will converge slowly.  The natural way to converge a power iteration faster is to use a subspace iteration.\cite{}
%For forecast functions, the analog of the power iteration is the Richardson iteration.  We show that the Richardson iteration suffers from the same slow convergence as the power iteration, so we convert the linear system into a dominant eigenproblem which can be solved by a subspace iteration.

%The typical usage of temporal-difference (TD) methods aims to optimize the mean-squared Bellman error by parameterizing the value function as a neural network. Since the expected reward function satisfies a dominant eigenproblem involving the Bellman operator, the classic TD method is a power method.  The method here could be applied to the same class of problems, although our subspace iteration will converge faster.


\section{Spectral estimation and the prediction problem}\label{sec:setup}
We have two primary applications in mind in this article.  First, we would like to estimate the \emph{dominant eigenfunctions and eigenvalues} of the transition operator $\T{t}$ for a Markov process $X^t\in \mathbb{R}^d$, defined as
\begin{equation}\label{eq:to}
    \T{t} f(x) = \E_x\left[ f(X^t) \right],
\end{equation}
where $f$ is an arbitrary real-valued function and the subscript $x$ indicates the initial condition $X^0=x$. The transition operator encodes how expectations of functions evolve in time.   The right eigenfunctions of $\T{t}$ with the largest eigenvalues characterize the most slowly decorrelating features (modes) of the Markov process \cite{noe_variational_2013, nuske_variational_2014,webber2021error,lorpaiboon_integrated_2020}.


Our second application is to compute \emph{prediction functions} of the general form 
\begin{equation}\label{eq:forecast}
    u(x) = \E_x\left[ \Psi(X^T) + \sum_{t=0}^{T-1} \Gamma(X^t)\right],
\end{equation}
where $T$ is the first time $X^t\notin D$ from some domain $D$, and $\Psi$ and $\Gamma$ are functions that characterize the escape event (rewards in reinforcement learning).  Prototypical examples of prediction functions that appear in our numerical results are the mean first passage time (MFPT)
\begin{equation}\label{eq:mfpt}
m(x) = \mathbbm{E}_x\left[ T  \right]
\end{equation}
and the committor
\begin{equation}\label{eq:committor}
q(x) = \mathbbm{E}_x\left[ \mathbbm{1}_B(X^T)  \right]=\mathbbm{P}_x\left[ X^T \in B \right]
\end{equation}
where $A$ and $B$ are disjoint sets (``reactant'' and ``product'' states) and $D= (A\cup B)^\comp$.
% for the committor.
% or $D= B^\comp$ for the MFPT.
The MFPT is important for estimating rates of transitions between metastable states, while the committor can serve as a reaction coordinate\cite{du_transition_1998, ma_automatic_2005, roux_transition_2022, krivov_reaction_2013} and as a key ingredient in transition path theory statistics~\cite{strahan_forecasting_2022, e_transition-path_2010, vanden2006transition}.
For any lag time $\tau>0$, the prediction function $u(x)$ satisfies the linear equation
\begin{equation}\label{eq:FK}
    \left( \mathcal{I} - \sT{\tau}\right) u(x)  
    = \E_x \left[ \sum_{t=0}^{(\tau\bmin T)-1} \Gamma(X^t)\right]
\end{equation}
for $x\in D$, with boundary condition
\begin{equation}
u(x) = \Psi(x)
\end{equation}
for $x\notin D$.
In \eqref{eq:FK}, $\mathcal{I}$ is the identity operator and 
\begin{equation}
    \mathcal{S}^tf(x) = \E_x\left[ f(X^{t\wedge T}) \right]
\end{equation}
is the ``stopped'' transition operator \cite{strahan_long-time-scale_2021}. We use the notation $\tau\wedge T = \min\{\tau, T\}$.   

% Computing prediction functions can enable detailed study of kinetics and mechanisms and insight into transition dynamics of complex systems.\cite{strahan_forecasting_2022, finkel_learning_2021, antoszewski_kinetics_2021, finkel_revealing_2022, guo_dynamics_2022}
% In this work, we focus specifically on the mean first passage time (MFPT) and the committor.

Our specific goal is to solve both the eigenproblem and the prediction problem for $X^t$ in high dimensions and without direct access to a model governing its evolution.  Instead, we have access to trajectories of $X^t$ of a fixed, finite duration $\tau$.
% As we have described in the Introduction, the state-of-the-art approaches to solving these problems in the context of MD applications are based on basis expansions and have important limitations.
A natural and generally applicable approach to finding an approximate solution to the prediction problem is to attempt to minimize the residual of \eqref{eq:FK} over parameters $\theta$ of a neural network $u_\theta(x)$ representing $u(x)$.  
% For example, for the prediction problem, 
For example, we recently suggested an algorithm that minimizes the residual norm 
\begin{equation}\label{eq:Res}
\frac{1}{2}\norm[\Big]{\left( \mathcal{I} - \sT{\tau}\right) u_\theta - r }_\mu^2,
\end{equation}
where $r(x)$ is the right-hand side of \eqref{eq:FK} and $\mu$ is an arbitrary distribution of initial conditions $X^0$ (boundary conditions were enforced by an additional term) \cite{strahan_forecasting_2022}.  The gradient of the residual norm in \eqref{eq:Res} with respect to neural-network parameters $\theta$ can be written
% \begin{equation}\label{eq:gradres}
% 2\int \left(\left( \mathcal{I} - \mathcal{S}^\tau\right) u_\theta(x) - b \right)\nabla_\theta u(x) \mu(dx) - 2\int \left(\left( \mathcal{I} - \mathcal{S}^\tau\right) u_\theta(x) - b \right)\mathcal{S}^\tau \nabla_\theta u(x) \mu(dx).
% \end{equation}
\begin{equation}\label{eq:gradres}
 \inner[\big]{\left( \mathcal{I} - \mathcal{S}^\tau\right) u_\theta - r}{\nabla_\theta u_\theta }_\mu - 
\inner[\big]{\left( \mathcal{I} - \mathcal{S}^\tau\right) u_\theta - r}{\mathcal{S}^\tau \nabla_\theta u_\theta}_\mu
\end{equation}
where ${\inner{f}{g}}_\mu = \int f(x) g(x) \,\mu(dx).$
Given a data set of initial conditions ${\{X^0_j\}}_{j=1}^n$ drawn from $\mu$ and a single sample trajectory ${\{X^t_j\}}_{t=0}^\tau$ of $X^t$ for each $X^0_j$, the first term in the gradient \eqref{eq:gradres} can be approximated without bias as
\begin{equation}\label{eq:semigrad}
\inner[\big]{\left( \mathcal{I} - \mathcal{S}^\tau\right) u_\theta - r} { \nabla_\theta u_\theta }_\mu \approx \frac{1}{n} \sum_{j=1}^n \left(u_\theta(X^0_j) - u_\theta(X^{\tau\bmin T_j}_j) - \sum_{t=0}^{(\tau\bmin T_j)-1} \Gamma(X^t_j)\right)\nabla_\theta u_\theta(X_j^0)
\end{equation}
where $T_j$ is the first time  $X^t_j\notin D$.

In contrast, unbiased estimation of the second term in \eqref{eq:gradres} requires access to two independent trajectories of $X^t$ for each sample initial condition since it is quadratic in $\mathcal{S}^\tau$~\cite{strahan_forecasting_2022,sutton_reinforcement_2018}. In the RL community, TD methods were developed for the purpose of minimizing a loss of a very similar form to \eqref{eq:Res}, and they perform a ``semigradient'' descent by following only the first term in \eqref{eq:gradres} \cite{sutton_reinforcement_2018}.
As in the semigradient approximation, the algorithms proposed in this paper only require access to inner products of the form ${\inner{ f }{ \mathcal{A} g}}_\mu$ for an operator $\mathcal{A}$ related to $\T{\tau}$ or $\sT{\tau}$, and they avoid terms non-linear in $\mathcal{A}$.  As we explain, such inner products can be estimated using trajectory data.
However, rather than attempting to minimize the residual directly by an approximate gradient descent, we view the eigenproblem and prediction problems through the lens of iterative numerical linear algebra schemes. 

\section{Inexact power iteration}\label{sec:inexactPower}

To motivate the iterative numerical linear algebra point of view, observe that the first term in \eqref{eq:gradres} is the exact gradient with respect to $\theta'$ of the loss\begin{equation}\label{eq:powerloss}
% \mathcal{L}(\theta') = 
\frac{1}{2}\norm[\big]{ u_{\theta'} - \mathcal{S}^\tau u_{\theta} - r }_\mu^2,
\end{equation}
evaluated at $\theta'=\theta$.  
The result of many steps of gradient descent (later, ``inner iterations'') on this loss with $\theta$ held fixed can then be viewed as 
an inexact Richardson iteration for \eqref{eq:FK}, resulting in a sequence:
\begin{equation}\label{eq:ierich}
u_{\theta^{s+1}} \approx \mathcal{S}^\tau u_{\theta^s} + r,
\end{equation}
where $u_{\theta^s}$ is a sequence of parametrized neural-network approximations of the solution to \eqref{eq:FK}.
To unify our presentation, we recast \eqref{eq:ierich} as an equivalent inexact power iteration:
\begin{equation}\label{eq:iepow}
    \bar u_{\theta^{s+1}} \approx \mathcal{A}\bar u_{\theta^s}
\end{equation}
where
\begin{equation}
\label{eq:A1}
\bar u_\theta = \left(\begin{array}{c}
1\\
u_\theta
\end{array}\right)
\quad\text{and}
\quad
\mathcal{A} = \left[
\begin{array}{cc}
1 & 0\\
r & \mathcal{S}^\tau
\end{array}
\right].
\end{equation}
Note that $(1, u)^\top$ is the dominant eigenfunction of $\mathcal{A}$.

% As has been observed in Ref.~\onlinecite{wen_batch_2020},
% an inexact power iteration  can also be used to approximate the dominant left eigenfunction of $\mathcal{T}^\tau$, its invariant probability measure, 
% by replacing $\mathcal{A}$ in \eqref{eq:iepow} by the appropriate adjoint of $\mathcal{T}^\tau$.
% In that context the authors also note that 
% any norm can be used to enforce \eqref{eq:iepow}, as long as  an unbiased estimator of its gradient is available.

% Naturally, the inexact power iteration in \eqref{eq:iepow} can also be used to estimate the single dominant eigenfunction/value of $\mathcal{T}^\tau$ by replacing $\mathcal{A}$ in \eqref{eq:iepow} by $\mathcal{A}=\mathcal{T}^\tau$ and normalizing $\bar u_{\theta^s}$ between applications of $\mathcal{A}.$  
%introducing some control on the growth or decay of $\bar u_{\theta^s}$.

Ref.~\onlinecite{wen_batch_2020} introduced an inexact power iteration to compute the stationary probability measure of $\mathcal{T}^\tau$, i.e., its dominant left eigenfunction.  As those authors note, an inexact power update such as \eqref{eq:iepow} can  be enforced using a variety of loss functions.
In our setting, the $L^2_\mu$ norm in \eqref{eq:powerloss} can be replaced by any other measure of the difference between $u_{\theta'}$ and $\mathcal{S}^\tau u_{\theta} + r$, as long as  an unbiased estimator of its gradient with respect to $\theta'$ is available.
% A similar observation is made in Ref.~\onlinecite{wen_batch_2020} in the context of an inexact power iteration scheme to find the invariant distribution of a Markov process from trajectory data. 
% Flexibility in the loss used to approximate the sequence of $u_\theta$ in \eqref{eq:ierich}
This flexibility is discussed in more detail in Section \ref{sec:softplus}, and we exploit it in applications presented later in this article.

For now, we focus on another important implication of this viewpoint: the flexibility in the form of the iteration itself. 
% Perhaps the most immediate improvement upon power iteration is to compute the dominant $k$ eigenfunctions/values of $\mathcal{A}$ by subspace iteration. 
For rare-event problems, one can expect that the spectral gap of $\mathcal{A}$ in \eqref{eq:A1} to be very small.  As a consequence, exact power iteration will converge slowly.  When the spectral gap is small, we will see that the inexact power iteration described above also fails to reach an accurate solution. In the next section, we address these issues by introducing an inexact \textit{subspace iteration}, which estimates the dominant $k$ eigenfunctions/values of $\mathcal{A}$.  For the eigenproblem involving $\mathcal{T}^\tau$ itself, the appeal of an inexact subspace iteration relative to inexact power iteration is self-evident: rather than finding only the single dominant eigenfunctions/values of the transition operator, we can find the dominant $k$ eigenfunctions/values. 


\section{An inexact subspace iteration}\label{sec:ieSI}

% In this section we will describe a general inexact subspace iteration to find the $k$ dominant eigenfunction/values of an operator $\mathcal{A}$. The central assumption guiding the design of the scheme is that, for any test functions $f$ and $h$, we can use trajectory data to estimate any inner product of the form 
% \begin{equation}\label{eq:exactSI}
% \langle h, \mathcal{A} f\rangle_\mu = \int h(x) \mathcal{A}f(x)\, \mu(dx)
% \end{equation}
% with increasing accuracy as $n$ (the number of trajectories in the data set) increases. The specific estimators used depend on the form of $\mathcal{A}$ and are detailed in later sections.


Our inexact subspace iteration for the $k$ dominant eigenfunctions/values of $\mathcal{A}$ proceeds as follows.  Let $\set{\varphi^a_{\theta^s}}_{a=1}^k$ be a sequence of $k$ basis functions parametrized by $\theta^s$. We can represent this basis as the vector valued function \begin{equation}\label{eq:U}
    U_{\theta^s} = \left(\varphi^1_{\theta^s},\varphi^2_{\theta^s},\dots,\varphi^k_{\theta^s}\right).
\end{equation}  
Then, we can obtain a new set of $k$ basis functions by approximately applying $\mathcal{A}$ to each of the components of $U_{\theta^s}$:
\begin{equation}\label{eq:ieSI}
U_{\theta^{s+1}}K^{s+1} \approx \mathcal{A}\, U_{\theta^s}
\end{equation}
where $K^{s+1}$ is an invertible, upper-triangular $k\times k$ matrix that does not change the span of the components of $U_{\theta^{s+1}}$ but is included to facilitate training.
One way the approximate application of $\mathcal{A}$ can be accomplished is by minimizing 
% \begin{equation}\label{eq:general_Loss}
% \theta^{s+1}, K^{s+1} = \arg\min_{\theta, K}
% \sum_{a=1}^k {\left\lVert \sum_{b=1}^k \varphi^b_{\theta}K_{ba} - \mathcal{A}\, \varphi^a_{\theta^s} \right\rVert}^2_\mu
% \end{equation}
\begin{equation}\label{eq:general_Loss}
\frac{1}{2}\sum_{a=1}^k \norm[\Bigg]{ \sum_{b=1}^k \varphi^b_{\theta}K_{ba} - \mathcal{A}\, \varphi^a_{\theta^s} }^2_\mu
\end{equation}
over $\theta$ and $K$
with $\theta^s$ held fixed.
% As noted above, the gradient of such a loss can be approximated from data.
% In exact subsapce iteration the distance between span of these basis functions and the span of the $k$ dominant eigenfunctions decreases by a factor of $\lambda_{k+1}/\lambda_k$ at each iteration~\cite{}. 
The eigenvalues and eigenfunctions of $\mathcal{A}$ are then approximated by solving the finite-dimensional generalized eigenproblem
\begin{equation}
    C^\tau W = C^0 W \Lambda
\end{equation}
where
\begin{align}
    C^\tau_{ab} &= {\langle \varphi_{\theta^s}^a, \mathcal{A} \varphi_{\theta^s}^b\rangle}_\mu \label{eq:ct}\\
    C^0_{ab} & =  {\langle \varphi_{\theta^s}^a, \varphi_{\theta^s}^b\rangle}_\mu \label{eq:c0},
\end{align}
 each inner product is estimated using trajectory data, and 
$W$ and $\Lambda$ are $k\times k$ matrices. The matrix $\Lambda$ is diagonal, and the $a$-th eigenvalue $\lambda_a$ of $\mathcal{A}$ is  approximated by $\Lambda_{aa}$; the corresponding eigenfunction  $v_a$ is 
approximated by $\sum_{b=1}^k W_{ab}\, \varphi_{\theta^s}^b$.

Even when sampling is not required to estimate the matrices in \eqref{eq:ct} and \eqref{eq:c0}, the numerical rank of $C^\tau$ becomes very small as the eigenfunctions become increasingly aligned with the single dominant eigenfunction.  To overcome this issue, we apply an orthogonalization step between iterations (or every few iterations). 
Just as the matrices $C^0$ and $C^\tau$ can be estimated using trajectory data, the orthogonalization procedure can also be implemented approximately using data. 
% In our scheme we
% define the $n\times k$ matrix $Y$ with entries 
% \[
% Y_{j a} = \tilde u_{\theta^{s+1}}^a(X_0^{(j)}).
% \]
% We then compute the factorization $Y=QR$ where $Q$ is an $n\times k$ matrix with orthogonal columns and $R$ is an upper triangular $k\times k$ matrix.  Finally we update $U_{\theta^{s+1}}K \approx \mathcal{A} U_{\theta^s} R^{-1}$  restraining $K$ to be near the identity matrix to preserve the orthogonalization. 

Finally, in our experiments we find it advantageous to damp large parameter fluctuations during training by mixing the operator $\mathcal{A}$ with a multiple of the identity, i.e., we perform our inexact subspace iteration on the operator $(1-\alpha_s) \mathcal{I} + \alpha_s \mathcal{A}$ in place of $\mathcal{A}$ itself.  This new matrix has the same eigenfunctions as $\mathcal{A}$.  In our experiments, decreasing the parameter $\alpha_s$ as the number of iterations increases results in better generalization properties of the final solution and helps ensure convergence of the iteration.
For our numerical experiments we use 
\begin{equation}
    \alpha_s =
    \begin{cases}
    1 \qquad & s < \sigma\\
    1 / \sqrt{s +1 - \sigma} \qquad & s\geq \sigma
    \end{cases}
\end{equation}
where $\sigma$ is a user chosen hyperparameter that sets the number of iterations performed before damping begins.


The details, including estimators for all required inner products, in the case of the eigenproblem ($\mathcal{A}=\mathcal{T}^\tau$) are given in Section~\ref{sec:eig} and Algorithm~\ref{alg:eig}. For the prediction problem with $\mathcal{A}$ as in \eqref{eq:A1}, they are given in Section~\ref{sec:forecast} and Algorithm~\ref{alg:forecast}.


\section{Alternative loss functions}\label{sec:softplus}

As mentioned above, the inexact application of the operator $\mathcal{A}$ can be accomplished by minimizing loss functions other than \eqref{eq:general_Loss}.  The key requirement for a loss function in the present study is that $\mathcal{A}$ appears in its gradient only through terms of the form $\langle f, \mathcal{A} g\rangle_\mu$ for some functions $f$ and $g$, so that the gradient can be estimated using trajectory data.
As a result, we have flexibility in the choice of loss and, in turn, the representation of $u$.  In particular, we consider the representation $u_\theta = z(w_\theta)$, where $z$ is an increasing function, and $w_\theta$ is a function parameterized by a neural network. An advantage of doing so is that the function $z$ can restrict the output values of $u_\theta$ to some range.  For example, when computing a probability such as the committor, a natural choice is the sigmoid function $z(x) = (1+e^{-x})^{-1}$. 

Our goal is to train a sequence of parameter values so that $u_{\theta^s}$ approximately follows a subspace iteration, i.e., so that $z(w_{\theta^{s+1}}) \approx \mathcal{A} u_{\theta^s}$.
To this end, we minimize with respect to $\theta$ the loss function
\begin{equation}\label{eq:genloss}
\E_{X^0\sim\mu}\left[ V(w_\theta)
- w_\theta \mathcal{A}u_{\theta^s}\right],
\end{equation}
where $V$ is an antiderivative of $z$, and $\theta^s$ is fixed. The subscript $X^0\sim \mu$ in this expression indicates that $X^0$ is drawn from $\mu$.
Note that, as desired, $\mathcal{A}$ appears in the gradient of \eqref{eq:genloss} only in an inner product of the required form, and the minimizer, $\theta^{s+1}$, of this loss (which we assume is attained) satisfies $z(w_{\theta^{s+1}}) \approx \mathcal{A} u_{\theta^s}$.  This general form of loss function is adapted from variational expressions for the divergence of two probability measures~\cite{Nguyen2010div,wen_batch_2020}.

The $L^2_\mu$ loss in \eqref{eq:Res}, which we use in several of our numerical experiments, corresponds to the choice $z(x) = x$ and $V(x)= x^2/2$.  The choice of $z(x) = (1+e^{-x})^{-1}$ mentioned above corresponds to $V(x) = \log(1+e^x)$; we refer to the loss in \eqref{eq:genloss} with this choice of $V$ as the ``softplus'' loss~\cite{peters_obtaining_2006,jung_artificial_2019, jung_autonomous_2021}.

\section{Test problems}\label{sec:testproblems}

We illustrate our methods with two well-characterized systems that exemplify features of molecular transitions.  In this section, we provide key details of these systems.

\begin{figure}[H]
    \centering
    \includegraphics{figures/mb_pmf.pdf}
    \caption{M\"uller-Brown potential energy surface.  The orange and red ovals indicate the locations of states $A$ and $B$ respectively when computing predictions. Contour lines are drawn every 1 $\beta^{-1}$.}
    \label{fig:MB}
\end{figure}

\subsection{M\"uller-Brown potential}

The first system is defined by the M\"uller-Brown potential \cite{muller_location_1979} (Figure~\ref{fig:MB}): 
\begin{equation}
\label{eq:MB}
V_{\rm MB}(y, z)=\frac{1}{20}\sum_{i=1}^4C_i \exp[a_i(y-y_i)^2+b_i(y-y_i)(z-z_i)+c_i(z-z_i)^2].
\end{equation}
The two-dimensional nature of this model facilitates visualization. The presence of multiple minima and saddlepoints that are connected by a path that does not align with the coordinate axes makes the system challenging for both sampling and analysis methods.  
In Sections \ref{sec:mbeigenfunctions} and \ref{sec:mbcommittor}, we use $C_i=\{-200,-100,-170,15\}$, $a_i=\{-1,-1,-6.5,0.7\}$,
$b_i=\{0,0,11,0.6\}$, 
$c_i=\{-10,-10,-6.5,0.7\}$, 
$y_i=\{1,-0.27,-0.5,-1\}$, 
$z_i=\{0,0.5,1.5,1\}$.
In Section~\ref{sec:Mod_MB}, we tune the parameters to make transitions between minima rarer; there, the parameters are $C_i=\{-250,-150,-170,15\}$, $a_i=\{-1,-3,-6.5,0.7\}$,
$b_i=\{0,0,11,0.6\}$, 
$c_i=\{-10,-30,-6.5,0.7\}$, 
$y_i=\{1,-0.29,-0.5,-1\}$, 
$z_i=\{0,0.5,1.5,1\}$.
For prediction, we analyze transitions between the upper left minimum ($A$) and the lower right minimum ($B$) in Figure \ref{fig:MB}; these states are defined as
\begin{equation}
\label{eqn:MBstates}
\begin{aligned}
&A=\set{ y, z : 6.5(y+0.5)^2-11(y+0.5)(z-1.5)+6.5(z-1.5)^2<0.3}\\
&B=\set{y, z:(y-0.6)^2+0.5(z-0.02)^2<0.2}.
\end{aligned}
\end{equation}

To generate a data set, we randomly draw 50,000 (unless otherwise noted) initial conditions $X^0_j$ uniformly from the region 
\begin{equation}
\label{eq:initregion}
\Omega=\set{y, z :-1.5<y<1.0,\ -0.5<z<2.5,\ V_{\rm{MB}}(y,z)<12}
\end{equation}
and, from each of these initial conditions,  generate one trajectory according to the discretized overdamped Langevin dynamics
\begin{equation}
    X^{t}_j = X^{t-1}_j - {\delta_t}\,\nabla V_\textrm{MB}(X^{t-1}_j) + \sqrt{{\delta_t}\,2  \beta^{-1}}\, \xi^{t}_j %\quad \text{for}\quad 1<t\leq \tau
\end{equation}
where $1<t\leq \tau$, the $\xi^t_j$ are independent standard Gaussian random variables, and the timestep is ${\delta_t}=0.001$.
We use an inverse temperature of $\beta = 2$, and we vary $\tau$ as indicated below.
%Throughout we set $\beta = 2$, which corresponds to a barrier height of approximately 8 $kT$ for the default parameters.

To validate our results, we compare the neural-network results against grid-based references, computed as described in Refs.\ \onlinecite{thiede_galerkin_2019, lorpaiboon_augmented_2022}.

\subsection{\texorpdfstring{AIB\textsubscript{9}}{AIB9} helix-to-helix transition}
\label{sec:aib_numerics}

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{figures/aib.pdf}
    \caption{Helix-to-helix transition of AIB\textsubscript{9}.  (left) Ball-and-stick representation of the left- and right-handed helices, which we use as state $A$ and $B$, respectively, when computing predictions.  Carbon atoms are shown in yellow; nitrogen atoms are shown in blue; oxygen atoms are shown in red; hydrogen atoms are not shown.
    (right) Potential of mean force constructed from the histogram of value pairs of the first two dihedral angle principal components; data are from the 20 trajectories of 5 $\mu$s that we use to construct reference statistics (see text). The left-handed helix corresponds to the left-most basin, and the right-handed helix corresponds to the right-most basin. Contour lines are drawn every 2 $k_{\rm B}T$ where $k_{\rm B}$ is the Boltzmann constant and $T$ is the temperature.}
    \label{fig:aib9}
\end{figure}


The second system is a peptide of nine $\alpha$-aminoisobutyric acids (AIB\textsubscript{9}; Figure~\ref{fig:aib9}).  Because AIB is achiral around its $\alpha$-carbon atom, AIB\textsubscript{9} can form left- and right-handed helices with equal probabilities, and we study the transitions between these two states.  This transition was previously  studied using MSMs and long unbiased molecular dynamics simulations~\cite{buchenberg_hierarchical_2015, perez_meld-path_2018}. AIB\textsubscript{9} poses a stringent test due to the presence of many metastable intermediate states.

The states are defined in terms of the internal $\phi$ and $\psi$ dihedral angles.  We classify an amino acid as being in the ``left'' state if its dihedral angle values are within a circle of radius $25^\circ$ centered at $(41^\circ, 43^\circ)$, that is
\begin{equation*}
    (\phi - 41)^2 + (\psi - 43)^2 \le 25^2.
\end{equation*}
Amino acids are classified as being in the ``right'' state using the same radius, but centered instead at $(-41^\circ, -43^\circ)$.
States $A$ and $B$ are defined by the amino acids at sequence positions 3--7 being all left or all right, respectively.
We do not use the two residues on each end of AIB\textsubscript{9} in defining the states as these are typically more flexible \cite{perez_meld-path_2018}.
The states can be resolved by projecting the states onto dihedral angle principal components (dPCs; Figure~\ref{fig:aib9}, right) as described previously \cite{sittel_principal_2017}.

% For our analysis, we use the data set described in Ref.~\citenum{perez_meld-path_2018}.
% Briefly, there are 13,800 simulations of AIB\textsubscript{9} in implicit solvent, each of which is approximately \SI{15}{\nano\second}, for a total of 
% 221~$\mu$s.
% Configurations were saved every \SI{40}{\pico\second}.
% For the analysis we exclude trajectories shorter than 300 time steps, or 12 ns, which reduces the total amount of sampling by less than 1 $\mu$s but enables us to test longer lag times, $\tau$.
Following a procedure similar to that described in Ref.~\citenum{perez_meld-path_2018}, we generate a data set of short trajectories.
From each of the 691 starting configurations in Ref.~\citenum{perez_meld-path_2018}, we initialize 10 trajectories of duration 20~ns, with initial velocities drawn randomly from a Maxwell-Boltzmann distribution at a temperature of 300~K.
We use a timestep of 4~fs together with a hydrogen mass repartitioning scheme~\cite{hopkins_long-time-step_2015}, and configurations are saved every 40~ps.
We employ the AIB parameters from Forcefield\_NCAA \cite{khoury_forcefield_ncaa_2014} and the GBNeck2 implicit-solvent model~\cite{nguyen_improved_2013}.
Simulations are performed with the Langevin integrator in OpenMM 7.7.0 \cite{eastman_openmm_2017} using a friction coefficient of \SI{1}{\per\pico\second}.
%In summary, our short trajectory data set contains 6,910 trajectories, corresponding to a total sampling time of 138.2 $\mu$s. %  {\color{blue} [WE NEED TO EXPLAIN HOW WE GET THESE NUMBERS.]}
To generate a reference for comparison to our results, we randomly select 20 configurations from the data set above and, from each, run a single simulation of 5 $\mu$s with the same simulation parameters.




% We assume that we have initial conditions in $\{X^j_0\}_{j=1}^n$ sampled from some (unknown) probability distribution $\mu$ on $\mathbb{R}^d$. For each of these points we also have a sample of the underlying process $X_t$ at $\tau$ units of time in the future, $X^j_\tau$.  In most cases each point $X^j_0$ or $X^j_\tau$ will itself represent a time-lag embedding. These observations may come from model simulation or from experimental observation.  

% We will be estimating properties of either the transition operator 
% \[
% \mathcal{T}^t f(x) = \mathbf{E}_x\left[ f(X_t)\right]
% \]
% or of the stopped transition operator 
% \[
% \mathcal{T}^t_D f(x) = \mathbf{E}_x\left[ f(X_{t\wedge T})\right]
% \]
% where $T$ is the first escape time of $X_t$ from some domain $D$.

\section{Spectral estimation}\label{sec:eig}

In this section, we provide some further numerical details for the application of our method to spectral estimation and demonstrate the method on the test problems.  For our subspace iteration, we require estimators for inner products of the form $\langle f, \T{\tau} g\rangle_\mu$.
For example, the gradient of the loss function \eqref{eq:general_Loss} involves inner products of the form
\begin{equation}
\left\langle\nabla_\theta \varphi_\theta^a,  \mathcal{T}^\tau \varphi_{\theta}^b\right\rangle_\mu.
\end{equation}
For these, we use the unbiased data-driven estimator
\begin{equation}
\langle f, \T{\tau} g\rangle_\mu \approx \frac{1}{n} \sum_{j=1}^n  f(X_j^0) g(X_j^\tau).
\end{equation}

As discussed in Section~\ref{sec:ieSI}, applying the operator $\mathcal{T}^\tau$ repeatedly causes each basis function to converge to the dominant eigenfunction and leads to numerical instabilities.  To avoid this, we orthogonalize the outputs of the networks with a QR decomposition at the end of each subspace iteration by constructing the matrix 
$\Phi_{ia} = \varphi^a_{\theta}(X_i^s)$
and then computing the factorization $\Phi =QR$ where $Q$ is an $n\times k$ matrix with orthogonal columns and $R$ is an upper triangular $k\times k$ matrix.  Finally we set $\tilde{\varphi}_s^a= \sum_{b=1}^k\varphi_\theta^b\,(R^{-1} N)_{ba}$, where $N$ is a diagonal matrix with entries equal to the norms of the columns of $\Phi$ (before orthogonalization).
To ensure that the networks remain well-separated (i.e., the eigenvalues of $C^0$ remain away from zero), we penalize large off-diagonal entries of $K$ by adding to the loss
\begin{equation}
\label{eq:K_reg}
\gamma_1{\norm{K-\mathrm{diag}(K)}}^2_F,
\end{equation}
where $\gamma_1$ allows us to tune the strength of this term relative to others, and $\norm{\cdot}_F$ is the Frobenius norm.
% We present the overall algorithm for subspace iteration in Algorithm~\ref{alg:eig}. 
We further establish normalization of the network outputs using the strategy from Ref.~\onlinecite{wen_batch_2020};  that is, we add to the loss a term of the form
\begin{equation}
\gamma_2\sum_{a=1}^k(2\nu_a(\langle \varphi_\theta^a,\varphi_\theta^a \rangle_\mu-1)-\nu_a^2),
\end{equation}
where we have introduced the conjugate variables $\nu_a$ which we maximize with gradient ascent (or similar optimization).
%The maximum of this term with respect to $\nu_a$ is attained when the basis functions are normalized, i.e., $\inner{\varphi_\theta^a}{\varphi_\theta^a}_\mu = 1$.
In general, our numerical experiments suggest that it is best to keep $\gamma_1$ and $\gamma_2$ relatively small.  We find that stability of the algorithm over many subspace iterations is improved if the matrix $K$ is set at its optimal value before each inner loop.  To do this, we set
\begin{equation}
K_{1:i,i}=\arg \min_{c} \norm[\Bigg]{\sum_{a=1}^i\varphi_{\theta}^{a}c_a-\mathcal{T}^{\tau}\tilde{\varphi}_s^a}+\gamma_2\sum_{a=1}^{i-1}c_a^2.
\end{equation}
The above minimization can be solved with linear least squares.
Finally, we note that in practice any optimizer can be used for the inner iteration steps, though the algorithm below implements stochastic gradient descent.  In this work, we use Adam\cite{kingma2017adam} for all numerical tests.
We summarize our procedure for spectral estimation in Algorithm~\ref{alg:eig}.
% Although the gradient-based optimization as described uses stochastic gradient descent, it is possible (and more effective in many cases) to use other optimizers such as Adam.


% \begin{algorithm}[H]
% \caption{Subspace iteration for spectral estimation}
% \label{alg:eig}
% \begin{algorithmic}[1]
% \Require{
% Subspace dimension $k$,
% transition data $\{X^0_j,X^{\tau}_j\}_{j=1}^n$,
% batch size $B$, learning rate $\eta$, number of subspace iterations $T$, number of inner iterations $M,$ regularization parameters $\gamma_1$ and $\gamma_2$}
% % \State $u_{s} \gets u_\theta$
% % \State $\varphi_{s}^a \gets \tilde{\varphi}_\theta^a$ for $a = 2\dots k$
% \State Initialize $\{\varphi_\theta^a\}_{a=1}^k,$ and $\set{\tilde \varphi_1^a}_{a=1}^k$
% \For{$s = 1\dots T$}
%     \For{$m = 1\dots M$}
%         \State Sample a batch of data $\{X^0_j,X^{\tau}_j\}_{j=1}^B$
%         \State $\nabla_\theta \hat {\mathcal{L}}
%         \gets \frac{1}{B} \sum_{j=1}^B \sum_{a=1}^k\sum_{b=1}^k\nabla_\theta \varphi_\theta^b(X_j^0)K_{b a} \left(\sum_{c=1}^k\varphi_\theta^{c}(X_j^0)K_{c a} - 
%         \tilde{\varphi}_s^a(X_j^{\tau})\right)$ % \Comment{eq.~\eqref{eq:loss_grad_est}}
%         \State $\nabla_\theta \hat {\mathcal{L}} \gets \nabla_\theta\hat{\mathcal{L}} + 4\gamma_2\sum_{a=1}^k\nu_a\frac{1}{B} \sum_{j=1}^B(\varphi_{\theta}^a(X^0_j)\nabla_\theta\varphi_{\theta}^a(X^0_j))$
%          \State $\nabla_\nu \hat {\mathcal{L}} \gets 2\gamma_2\sum_{a=1}^k(\frac{1}{B}\sum_{j=1}^B(\varphi_{\theta}^a(X^0_j))^2-1-\nu_a)$
%         % \Comment{SGD may be replaced by another optimizer}
%         \State $\nabla_K \hat {\mathcal{L}}
%         \gets \frac{1}{B} \sum_{j=1}^B \sum_{a=1}^k  \varphi_\theta^a(X_j^0) \sum_{b=1}^k\left(
%         \varphi_\theta^b(X_j^0)K_{b a} - 
%         \tilde{\varphi}_s^a(X_j^{\tau})\right)+2\gamma_1(K-\mathrm{diag}(K)).$
%         \State $\theta \gets \theta-\eta \nabla_\theta \hat {\mathcal{L}}$
%         \State $K \gets K-\eta \,(\mathrm{triu}(\nabla_K \hat{\mathcal{L}}))$ \Comment{ensure $K$ stays upper-triangular}
%         \State $\nu \gets \nu + \eta \nabla_\nu \hat{\mathcal{L}}$
%         \Comment gradient ascent
%         % \State Repeat 4-5 for each $\varphi_{\theta}^a$.
%     \EndFor
%     \State Compute the matrix $\Phi_{ia} = \varphi^a_{\theta}(X_i^0)$ 
%     \Comment{$\Phi \in \mathbb{R}^{n\times k}$}
%     \State Compute diagonal matrix $D^2_{aa}=\sum_i\varphi^a_{\theta}(X_i^0)^2$
%     \State Compute QR-decomposition $\Phi = QR$ \Comment{$Q\in \mathbb{R}^{n\times 
%     k}$ and $R\in \mathbb{R}^{k\times k}$}
%     \State $\tilde{\varphi}^a_{s} \gets \sum_{b=1}^k \varphi^b_{\theta}\,(R^{-1}D)_{ba}$
%     \State set $K_{1:i,i}=\arg \min_{c}\norm{\sum_{a=1}^i\varphi_{\theta}^{a}c_a-\mathcal{T}^{\tau}\tilde{\varphi}_s^a}+\gamma_2\sum_{a=1}^{i-1}c_a^2 $
%     %\State $K_{pr} \gets \sum_{q=1}^kR^{-1}_{pq}\sum_{j=1}^n\tilde{\varphi}_s^q(X_j^0)\tilde{\varphi}_s^r(X_j^t)$
%     %\State $\varphi_{s}^a \gets \tilde{\varphi}_\theta^a$ for $a = 1\dots k$
% \EndFor
% \State Compute the matrix $C^t_{a b}=\frac{1}{n}\sum_{j=1}^n\tilde \varphi_\theta^a(X_j^0)\tilde \varphi_\theta^b(X_j^t)$ \Comment{$C^t \in \mathbb{R}^{k\times k}$, $t=0,\tau$}
% \State Solve the generalized eigenproblem $C^\tau W=C^0 W\Lambda$ for $W$ and $\Lambda$ % \Comment{generalized eigenproblem as in TICA/VAC}
% \State \Return eigenvalues $\Lambda$, eigenfunctions $v_a=\sum_{b=1}^kW_{ab}\tilde \varphi_\theta^b$ 
% \end{algorithmic}
% \end{algorithm}


\begin{algorithm}[H]
\caption{Inexact subspace iteration (with $L^2_\mu$ loss) for spectral estimation}
\label{alg:eig}
\begin{algorithmic}[1]
\Require{
Subspace dimension $k$,
transition data $\{X^0_j,X^{\tau}_j\}_{j=1}^n$,
batch size $B$, learning rate $\eta$, number of subspace iterations $S$, number of inner iterations $M,$ regularization parameters $\gamma_1$ and $\gamma_2$}
% \State $u_{s} \gets u_\theta$
% \State $\varphi_{s}^a \gets \tilde{\varphi}_\theta^a$ for $a = 2\dots k$
\State Initialize $\{\varphi_\theta^a\}_{a=1}^k$ and $\set{\tilde \varphi_1^a}_{a=1}^k$
\For{$s = 1\dots S$}
    \For{$m = 1\dots M$}
        \State Sample a batch of data $\{X^0_j,X^{\tau}_j\}_{j=1}^B$
        \State $\hat{\mathcal{L}}_1 \gets \frac{1}{B} \sum_{j=1}^B \sum_{a=1}^k\left[\frac{1}{2} (\sum_{b=1}^k\varphi_\theta^{b}(X_j^0)K_{b a})^2 -  \alpha_s \sum_{b=1}^k\varphi_\theta^b(X_j^0)K_{ba}\tilde{\varphi}_s^a(X_j^{\tau}) - (1 - \alpha_s) \sum_{b=1}^k\varphi_\theta^b(X_j^0)K_{ba}\tilde{\varphi}_s^a(X_j^0)\right]$
        \State $\hat{\mathcal{L}}_{K} \gets \gamma_1{\norm{K-\mathrm{diag}(K)}}^2_F$
        \State $\hat{\mathcal{L}}_{\rm{norm}} \gets \gamma_2\sum_{a=1}^k(2\nu_a( \frac{1}{B}\sum_{j=1}^B(\varphi_\theta^a(X_j^0)^2)-1)-\nu_a^2)$
        \State $\hat{\mathcal{L}} \gets \hat{\mathcal{L}}_1 + \hat{\mathcal{L}}_K + \hat{\mathcal{L}}_{\rm{norm}}$
        \State $\theta \gets \theta-\eta \nabla_{\theta}\hat{\mathcal{L}}$
        \State $K\gets K-\eta\, \mathrm{triu}(\nabla_K\hat{\mathcal{L}})$
        \State $\nu_a \gets \nu_a+\eta\nabla_{\nu_a}\hat{\mathcal{L}}$
    \EndFor
    \State Compute the matrix $\Phi_{ia} = \varphi^a_{\theta}(X_i^0)$ 
    \Comment{$\Phi \in \mathbb{R}^{n\times k}$}
    \State Compute diagonal matrix $N^2_{aa}=\sum_i\varphi^a_{\theta}(X_i^0)^2$
    \State Compute QR-decomposition $\Phi = QR$ \Comment{$Q\in \mathbb{R}^{n\times 
    k}$ and $R\in \mathbb{R}^{k\times k}$}
    \State $\tilde{\varphi}^a_{s} \gets \sum_{b=1}^k \varphi^b_{\theta}\,(R^{-1}N)_{ba}$
    \State set $K_{1:i,i}=\arg \min_{c}\norm{\sum_{a=1}^i\varphi_{\theta}^{a}c_a-\mathcal{T}^{\tau}\tilde{\varphi}_s^a}+\gamma_2\sum_{a=1}^{i-1}c_a^2 $
    %\State $K_{pr} \gets \sum_{q=1}^kR^{-1}_{pq}\sum_{j=1}^n\tilde{\varphi}_s^q(X_j^0)\tilde{\varphi}_s^r(X_j^t)$
    %\State $\varphi_{s}^a \gets \tilde{\varphi}_\theta^a$ for $a = 1\dots k$
\EndFor
\State Compute the matrices $C^t_{a b}=\frac{1}{n}\sum_{j=1}^n\tilde \varphi_s^a(X_j^0)\tilde \varphi_s^b(X_j^t)$ for $t=0,\tau$ \Comment{$C^t \in \mathbb{R}^{k\times k}$}
\State Solve the generalized eigenproblem $C^\tau W=C^0 W\Lambda$ for $W$ and $\Lambda$ % \Comment{generalized eigenproblem as in TICA/VAC}
\State \Return eigenvalues $\Lambda$, eigenfunctions $v_a=\sum_{b=1}^kW_{ab}\tilde \varphi_s^b$ 
\end{algorithmic}
\end{algorithm}





\subsection{M\"uller-Brown model}\label{sec:mbeigenfunctions}


\begin{table}[htb]
    \centering
    \begin{tabular}{c||cc|ccc|c}
    % \hline
        & \multicolumn{2}{c|}{\textbf{Spectral Estimation}} & \multicolumn{3}{c|}{\textbf{Committor}} & \textbf{MFPT} \\ \hline
         \textbf{Hyperparameter} & M\"uller-Brown & AIB\textsubscript{9} & M\"uller-Brown & Modified M\"uller-Brown & AIB\textsubscript{9} & AIB\textsubscript{9} \\ \hline
         subspace dimension $k$ & 3 & 5 & 1 & 2, 1\footnote{Four subspace iterations with $k=2$ followed by ten iterations with $k=1$} & 1 & 5 \\
         input dimensionality & 2 & 174 & 2 & 2 & 174 & 174\\
         hidden layers & 6 & 6 & 6 & 6 & 6 & 6\\
         hidden layer width & 64 & 128 & 64 & 64 & 150 & 150\\
         hidden layer nonlinearity & CeLU & CeLU & ReLU & ReLU & ReLU & ReLU \\
         output layer nonlinearity & none & tanh & sigmoid/none & none & none & none \\
         outer iterations $S$ & 10 & 100 & 100 & 4 + 10\footnotemark[1] & 100 & 300 \\ 
         inner iterations $M$ & 5000 & 2000 & 200 & 5000 & 2000 & 1000\\ 
         $\sigma$ & 2 & 50 & 50 & 0 & 50 & 0\\ 
         batch size $B$ & 2000 & 1024 & 5000 & 2000 & 1024 & 2000 \\ 
         learning rate $\eta$ & 0.001 & 0.0001 & 0.001 & 0.001 & 0.001 & 0.001\\ 
         $\gamma_1$ & 0.15 & 0.001 & - & - & - & 0.1\\ 
         $\gamma_2$ & 0.01 & 0.01 & - & - & - & 10\\ 
         % initialization of $\tilde{\varphi}_1^1$ & $1$ & $1$ & $\ind{B}$ & $\ind{B}$ & random & $5\ind{A}$ \\ 
         % initialization of $\tilde{\varphi}_1^a$ for $a>1$ & $x^TR$ & $x^TR$ & - & $x^TR$ & - & $x^TR$ \\ 
        loss for  $\varphi_\theta^1$ & $L^2_\mu$ & $L^2_\mu$ & $L^2_\mu$/softplus & softplus & softplus & $L^2_\mu$\\ 
        loss for $\varphi_\theta^a$ for $a > 1$ & $L^2_\mu$ & $L^2_\mu$ & - & $L^2_\mu$ & - & $L^2_\mu$\\
        \hline
    \end{tabular}
    \caption{Parameter choices used in this work}
    \label{tab:params}
\end{table}

As a first test of our method, we compute the $k=3$ dominant eigenpairs for the M\"uller-Brown model.  
%Hyperparameters for this test and all others are given in Table \ref{tab:params}.
Since we know that the dominant eigenfunction of the transition operator is the constant function $v_1(y,z) = 1$ with eigenvalue $\lambda_1 = 1$, we directly include this function in the basis as a non-trainable function, i.e. $\varphi_\theta^1(y,z)=1.$
%For the remaining two eigenfunctions of interest, we use a single feed-forward neural network with two inputs, six fully connected hidden layers, each with 64 CeLU activation functions, and two output layers with tanh activation functions.
%We train the network for $T=10$ subspace iteration steps.  This choice of $S$ can be viewed as a form of regularization by early stopping; in general, training for many subspace iteration steps leads to overfitting for the M\"uller-Brown model.  Each subspace iteration step has $M=5000$ inner iterations, where an inner iteration consists of a single step of the Adam optimizer with a batch size of $B=2000$ trajectories.  The batches consist of trajectories drawn independently with replacement from the previously-described data set. We use a learning rate of $\eta=1\times 10^{-3}$, we start damping after $\sigma=2$ iterations, and we set $\gamma_1=0.15$ and $\gamma_2=0.01$.
To initialize $\tilde{\varphi}_1^a$ for each $a>1$, we choose a standard Gaussian vector $(Y^a,Z^a)\in \mathbb{R}^2$, and set $\tilde{\varphi}_1^a(y,z)=y\, Y^a + z\,Z^a.$  This ensures that the initial basis vectors are well-separated and the first QR step is numerically stable. 
 Here and in all subsequent M\"uller-Brown tests, batches of trajectories are drawn from the entire dataset with replacement. Other hyperparameters are listed in Table~\ref{tab:params}.

\begin{figure}[H]
\begin{center}
    \includegraphics[scale=.5]{figures/VPM_Eigs.png}
\end{center}
    \caption{First two non-trivial eigenfunctions of the M\"uller-Brown model.  (top) Grid-based reference. (bottom) Neural network subspace after ten subspace iteration steps, computed with $\tau=300$ (i.e., $0.3\,\delta_t^{-1}$). %{\color{blue} [$\tau$ IS NOW AN INTEGER]}. 
    }
    \label{fig:mb_eigfn}
\end{figure}


Figure~\ref{fig:mb_eigfn} shows that we obtain good agreement between the estimate produced by the inexact subspace iteration in Algorithm \ref{alg:eig} and reference eigenfunctions.   Figure~\ref{fig:mb_subdist} shows how the corresponding eigenvalues vary with lag time; again there is good agreement with the reference. Furthermore, there is a significant gap between $\lambda_3$ and $\lambda_4$, indicating that a three-dimensional subspace captures the dynamics of interest for this system.



\begin{figure}[H]
\centering
    \includegraphics[width=\textwidth]
    {figures/VPM_Eig_Lag_Test.png}
    \caption{
    Spectral estimation as a function of lag time (in units of $\delta_t^{-1}$) for the M\"uller-Brown model.
    (top left) Second eigenvalue.  (top right) Third and fourth eigenvalues; only the reference fourth eigenvalue is shown to illustrate the spectral gap.  (bottom left) Relative error in the first spectral gap (i.e., $1-\lambda_2$).  (bottom right) Subspace distance between estimated and reference three-dimensional invariant subspaces.
    }
    \label{fig:mb_subdist}
\end{figure}


We compare the subspace that we obtain from our method with that from an MSM with 400 sets determined by $k$-means clustering. This is a very fine discretization for this system, and the MSM is sufficiently expressive to yield eigenfunctions in good agreement with the reference. The relative error of $1 -\lambda_2$ is comparable for the two methods (Figure \ref{fig:mb_subdist}, lower left).
To compare two subspaces,  $\mathcal{U}$ and $\mathcal{V}$, 
we define the subspace distance as
\begin{equation}
    d(\mathcal{U}, \mathcal{V}) = \norm[\big]{(\I-\mathcal{P}_{\mathcal{V}})\mathcal{P}_{\mathcal{V}}}_F,
\end{equation}
where $\mathcal{P}_{\mathcal{U}}$ and $\mathcal{P}_{\mathcal{V}}$ are projection operators onto $\mathcal{U}$ and $\mathcal{V}$, respectively, and $\norm{\cdot}_F$ is the Frobenius norm.
Figure \ref{fig:mb_subdist} (lower left) shows the subspace distances from the reference as functions of lag time. We see that the inexact subspace iteration better approximates the three-dimensional dominant eigenspace for moderate to long lag times, even though the eigenvalues are comparable.
%We note that for a two-dimensional potential, using 400 indicator functions for an MSM constitutes a very-fine discretization.
%Given the difficulty of clustering high-dimensional data, we expect that equivalent accuracy would be difficult to obtain with an MSM in complex systems.



\subsection{\texorpdfstring{AIB\textsubscript{9}}{AIB9}}\label{sec:eig_aib9}
For the molecular test system, we compute the dominant five-dimensional subspace.
We compare the inexact subspace iteration in Algorithm \ref{alg:eig} with MSMs constructed on dihedral angles (``dihedral MSM'') and on Cartesian coordinates (``Cartesian MSM'').
%We use a network with six fully-connected hidden layers, each with 128 CeLU activation functions, followed by an output layer with four tanh activation functions (the constant function is again included directly rather than learned).
%For training we use $T=80$ subspace iteration steps.  In general, we find that we can train for more subspace iteration steps in the case of AIB\textsubscript{9} than the M\"uller-Brown model, presumably because the higher dimensionality makes it harder for the network to overfit the data. Each subspace iteration step has $M=5000$ inner iterations.
%Batches of size $B=1024$ are drawn for each inner iteration step by randomly choosing (with replacement) pairs of frames separated by $\tau$.
%The learning rate was $\eta=1\times 10^{-4}$, we started damping after $\sigma = 50$ iterations, and $\gamma_1 = 1\times 10^{-3}$ and $\gamma_2 = 1\times 10^{-2}$.
%We initialize $\tilde{\varphi}_1^a$ for each $a$ as $\tilde{\varphi}_1^a(x)=\sum_ix_i R_i$ where $R^a\in \mathbb{R}^{174}$ is a vector with standard normal entries as before.
We expect the dihedral MSM to be accurate given that the dynamics of AIB\textsubscript{9} are well-described by the backbone dihedral angles \cite{buchenberg_hierarchical_2015, perez_meld-path_2018}, and we thus use it as a reference.
It is constructed by clustering the sine and cosine of each of the backbone dihedral angles ($\phi$ and $\psi$) for the nine residues (for a total of $2 \times2\times 9 = 36$ input features) using $k$-means with 1000 clusters.
The Cartesian MSM is constructed by clustering based on the Cartesian coordinates of all non-hydrogen atoms after aligning the backbone atoms of the trajectories, for a total of 174 input features, again using the $k$-means algorithm and $k=1000$.
Because of the difficulty of clustering high-dimensional data, we expect the Cartesian MSM basis to give poor results.
%For each MSM, we solve for the first five eigenfunctions/values of the transition matrix.
The neural network for the inexact subspace iteration receives the same 174 Cartesian coordinates as input features.  We choose to use Cartesian coordinates rather than dihedral angles as inputs because it requires the network to identify nontrivial representations for describing the dynamics.
Here and for all following tests on AIB\textsubscript{9}, batches consist of pairs of frames separated by $\tau$ (i.e., a rolling window) drawn randomly with replacement from the short trajectory data set. As in the M\"uller-Brown example, ${\varphi}_\theta^1=1$ and we use a random linear combination of coordinate functions to initialize $\tilde{\varphi}_1^a$ for $a>1$. 
Other hyperparameters are listed in Table~\ref{tab:params}.


\begin{figure}[H]
    \centering
    \includegraphics{figures/aib9_evals_compare_xyz_init.pdf}
    \caption{First five eigenvalues of the transition operator for AIB\textsubscript{9} as a function of lag time (in ps units).
    (left) Comparison between eigenvalues computed using the dihedral MSM with 1000 clusters (solid lines) and the inexact subspace iteration (dashed lines). The shading indicates standard deviations over five trained networks for the subspace iteration. 
    (right) Comparison between a dihedral MSM (solid lines) and Cartesian MSMs with 1000 clusters (dashed lines). The standard deviations for the Cartesian MSMs over five random seeds for $k$-means clustering are too narrow to be seen.
    % [\color{blue} $\tau$ IS NOW AN INTEGER, BUT MAYBE WE CAN KEEP THE PLOTS AS IS IF WE USE THE LANGUAGE ``$\tau$ corresponds to'' IN THE TEXT]
    }
    \label{fig:aib9_evals}
\end{figure}

Although a spectral gap appears after the second eigenvalue (Figure~\ref{fig:aib9_evals}), we choose $k=5$ to make the test more demanding.
Taking the dihedral MSM as a reference, the Cartesian MSM systematically underestimates the eigenvalues.
The subspace iteration is very accurate for the first four eigenvalues but the estimates for the fifth are low and vary considerably from run to run.
We expect that learning $\lambda_5$ is challenging because of the small gap between it and $\lambda_4$.
In Figure~\ref{fig:aib9_eigfn}, we plot the first two non-trivial eigenfunctions ($v_2$ and $v_3$), which align with the axes of the dPC projection.
The eigenfunction $v_2$ corresponds to the transition between the left- and right-handed helices; the eigenfunction $v_3$ is nearly orthogonal to $v_2$ and corresponds to transitions between intermediate states.
It is challenging to visualize the remaining two eigenfunctions by projecting onto the first two dPCs because $v_4$ and $v_5$ are orthogonal to $v_2$ and $v_3$.
The estimates for $v_2$ are in qualitative agreement for all lag times tested (Figure \ref{fig:aib9_eigfn} shows results for $\tau$ corresponding to 40 ps), but the subspace iteration results are less noisy for the shortest lag times.
Moreover, the estimate for $v_3$ from subspace iteration agrees more closely with that from the dihedral MSM than does the estimate for $v_3$ from the Cartesian MSM.
The subspace distance for $v_2$ and $v_3$ between the subspace iteration and the dihedral MSM is 0.928, compared with a value of 0.956 for the subspace distance between the two MSMs.
Together, our results indicate that the neural networks are able to learn the leading eigenfunctions and eigenvalues of the transition operator (dynamical modes) of this system despite being presented with coordinates that are not the natural ones for describing the dynamics.


\begin{figure}[H]
    \centering
    \includegraphics{figures/aib9_msm_nn_subspace_xyz.pdf}
    \caption{First two non-trivial eigenfunctions of AIB\textsubscript{9} projected onto dPCs. (left) MSM constructed on sine and cosine of dihedral angles with 1000 clusters and  lag time corresponding to 40 ps. (middle) Inexact subspace iteration using Cartesian coordinates in and the same lag time.
    (right) MSM constructed on Cartesian coordinates with 1000 clusters and the same lag time.}
    \label{fig:aib9_eigfn}
\end{figure}

% The subspace distance between the inexact subspace iteration and  IVAC results is $0.41\pm0.05$ (over five replicates of the neural network), indicating consistency between the two estimates.
% Although there is no exact reference for this calculation, our results show good agreement between VIDyA and IVAC, suggesting that they are converging to the same subspace.
% Indeed, the projection of the neural network eigenfunctions appears smoother and less noisy than that of the IVAC eigenfunctions.


\section{Prediction}\label{sec:forecast}


Inexact subspace iteration for $\mathcal{A}$ in \eqref{eq:A1} is equivalent to performing the inexact Richardson iteration in \eqref{eq:ierich} on the first basis function $\varphi_\theta^1$ and then performing an inexact subspace iteration for the operator $\mathcal{S}^\tau$ on the rest of the basis functions. The iteration requires unbiased estimators of the forms
\begin{equation}
\inner{ f }{ \sT{\tau} g}_\mu \approx \frac{1}{n} \sum_{j=1}^n  f(X_j^0) g(X_j^{\tau\bmin T_j})
\end{equation}
and
\begin{equation}
\inner[\Bigg]{ f }{ \E_x\left[ \sum_{t=0}^{(\tau\bmin T)-1} \Gamma(X^t)\right] }_\mu \approx \frac{1}{n} \sum_{j=1}^n  f(X_j^0) \sum_{t=0}^{(\tau\bmin T_j)-1} \Gamma(X_j^t),
\end{equation}
where $T_j$ is the first time $X_j^t$ enters $D^\comp$.

The Richardson iterate, $\varphi_\theta^1$, must satisfy the boundary condition $\varphi_\theta^1(x) = \Psi(x)$ for $x\notin D$. The other basis functions should satisfy $\varphi^a_\theta(x) = 0$ for $x\notin D$.  
In practice,  we enforce the boundary conditions by setting  $\varphi^1_\theta(x)=\Psi(x)$  and $\varphi^a_\theta(x)=0$ for $a>1$ when  $x\notin D$.
% For committor calculations using the softplus loss we enforce the boundary conditions by 
% substituting $(1+e^{50})^{-1}$ for output the network  $-50$ for $x \in A$ and $(1+e^{-50})^{-1}$ for the output of the network for $x\in B$.
% In practice, for calculations in which an $L^2_\mu$ loss is used, we enforce the boundary conditions by substituting  $\Psi(x)$ for the output of the network  for  $x\notin D$. For committor calculations using the softplus loss we enforce the boundary conditions by 
% substituting $(1+e^{50})^{-1}$ for output the network  $-50$ for $x \in A$ and $(1+e^{-50})^{-1}$ for the output of the network for $x\in B$.

% such that the softplus applied to the network nearly satisfies the committor boundary conditions.

When the boundary condition is zero, as for the MFPT, we find an approximate solution of the form 
\begin{equation}
 u_{\theta} =  \sum_{a=1}^k w_a \varphi^a_{\theta} 
 \end{equation}
 by 
solving the $k$-dimensional linear system
\begin{equation}\label{eq:linsys}
\left(C^0 - C^\tau\right) w = p
\end{equation}
where, for $a, b \geq 1$,
\begin{equation}\label{eq:C}
C^t_{ab} = \inner{\varphi^a_{\theta}}{ \sT{t} \varphi^b_{\theta}}_\mu 
\end{equation}
for $t = 0, \tau$, and
\begin{equation}\label{eq:v}
    p_a = \inner[\big]{ \varphi^a_{\theta}}{\E_x\left[ \rho(X) \right]  }_\mu.
\end{equation}
In \eqref{eq:v}, we introduce the notation
 \begin{equation}
\rho(X) =\sum_{t=0}^{(\tau\bmin T)-1} \Gamma(X^t) 
\end{equation}
for use in Algorithm \ref{alg:forecast}.


When the boundary condition is non-zero, as for the committor, 
we restrict \eqref{eq:linsys} to a  $(k-1)$-dimensional linear system by excluding the indices  $a=1$ and $b=1$ in \eqref{eq:C} and \eqref{eq:v} 
and setting
 \begin{equation}
\rho(X) = \varphi^1_{\theta}(X^{\tau\wedge T}) 
 - \varphi^1_{\theta}(X^0)
 +\sum_{t=0}^{(\tau\bmin T)-1} \Gamma(X^t).
\end{equation}
In this case the corresponding approximate solution is 
 \begin{equation}
u_{\theta} = \varphi^1_{\theta} + \sum_{a=2}^k w_a \varphi^a_{\theta}.
\end{equation}
This approximate solution corresponds to the one given by dynamical Galerkin approximation \cite{thiede_galerkin_2019, strahan_long-time-scale_2021} with the basis ${\{\varphi^a_{\theta}\}}_{a=2}^k$ and a ``guess'' function of $\varphi^1_{\theta}$.


 When the boundary conditions are zero, the orthogonalization procedure is applied to all basis functions as in Section \ref{sec:eig}.
When the boundary conditions are non-zero, the orthogonalization procedure is only applied to the basis functions ${\{\varphi^a_{\theta}\}}_{a=2}^k$, and $K_{a 1} = I_{a 1}$. 
We summarize our procedure for prediction in Algorithm \ref{alg:forecast}.



% \begin{algorithm}[H]
% \caption{Subspace iteration for prediction functions}
% \label{alg:forecast}
% \begin{algorithmic}[1]
% \Require{Subspace dimension $k$, stopped transition data ${\set{X^0_j,X^{\tau \bmin T}_j }}_{j=1}^n$, cost data ${\set{ \rho(X_j)}}_{j=1}^n$, batch size $B$, learning rate $\eta$, number of subspace iterations $T$, number of inner iterations $M,$ regularization parameters $\gamma_1$ and $\gamma_2$}
% \State Initialize $\{\varphi_\theta^a\}_{a=1}^k$ and $\set{\tilde \varphi_1^a}_{a=1}^k$
% \For{$s = 1\dots T$}
%     \For{$m = 1\dots M$}
%         \State Sample a batch of data $\{X^0_j,X^{\tau\bmin T}_j\}_{j=1}^B$, $\set{\rho(X_j)}_{j=1}^B$
%         \State $\nabla_\theta \hat {\mathcal{L}}
%         \gets \frac{1}{B} \sum_{j=1}^B \sum_{a=1}^k\sum_{b=1}^k\nabla_\theta \varphi_\theta^{b}(X_j^0)K_{b a} \left(\sum_{c=1}^k\varphi_\theta^{c}(X_j^0)K_{c a} - 
%         \tilde{\varphi}_s^a(X_j^{\tau})-\rho(X_j)\delta_{a,1}\right)$ 
%         \State $\nabla_\theta \hat {\mathcal{L}} \gets \nabla_\theta\hat{\mathcal{L}} + 2\gamma_2\nu\frac{1}{B} \sum_{j=1}^B\sum_{a=1}^k(\varphi_{\theta}^a(X^0_j))^2$
%         \State $\nabla_\nu \hat {\mathcal{L}} \gets 2\gamma_2\sum_{a=2}^k(\frac{1}{B}\sum_{j=1}^B(\varphi_{\theta}^a(X^0_j))^2-1-\nu)$
%         % \Comment{SGD may be replaced by another optimizer}
%         %\State $\nabla_K \hat {\mathcal{L}}_n
%         %\gets \parbox[t]{.7\linewidth}{$\frac{1}{B} \sum_{j=1}^B %\sum_{a=1}^k\varphi_\theta^a(X_j^0)\left(
%         %\sum_{m=1}^k\varphi_\theta^{m}(X_j^0)K_{m a} - 
%         %\tilde{\varphi}_s^a(X_j^{\tau})-b(X_j)\delta_{a,1}\right)+2\gamma_1(K-\rm{diag}(K)).$}$
%         \State $\nabla_K \hat {\mathcal{L}}
%         \gets \frac{1}{B} \sum_{j=1}^B \sum_{a=1}^k\varphi_\theta^a(X_j^0)\left(
%         \sum_{b=1}^k\varphi_\theta^{b}(X_j^0)K_{b a} - 
%         \tilde{\varphi}_s^a(X_j^{\tau})-\rho(X_j)\delta_{a,1}\right)$
%         \State $\nabla_K \hat {\mathcal{L}}\gets \nabla_K \hat {\mathcal{L}} + 2\gamma_1(K-\rm{diag}(K)).$
%         \State $\theta \gets \theta-\eta \nabla_\theta \hat {\mathcal{L}}$
%         \State $K \gets K-\eta \,(\rm{triu}(\nabla_K \hat {\mathcal{L}}))$
%         \State $\nu \gets \nu+\eta \nabla_\nu \hat {\mathcal{L}}$
%     \EndFor
%     \If{$\psi(x)=0$}
%         \State Compute the matrix $\Phi_{ia}^s = \varphi^a_\theta (X_i^0)$ \Comment{$\Phi \in \mathbb{R}^{n\times k}$}
%     \Else
%         \State Compute the matrix $\Phi_{ia}^s = \varphi^a_\theta(X_i^0)$ for $a\geq 2$ \Comment{$\Phi \in \mathbb{R}^{n\times (k-1)}$}
        
%     \EndIf
%     \State Compute QR-decomposition $\Phi^s = QR$ 
%     % \Comment{$Q\in \mathbb{R}^{n\times k}$ and $R\in \mathbb{R}^{(k)\times (k)}$}
%     \State Compute diagonal matrix $D^2_{aa}=\sum_i\varphi^a_{\theta}(X_i^0)^2$
%     \State $\tilde{\varphi}^a_{s} \gets \sum_{b=1}^k \varphi^b_{\theta}\, (R^{-1}D)_{ba}$ 
%     \Comment{if $\psi(x) = 0$ exclude $a = 1$}
%     \State set $K_{1:i,i}=\arg \min_{r}\norm{\sum_{a=1}^i\varphi_{\theta}^{a}r_a-\mathcal{S}^{\tau}\tilde{\varphi}_s^a}+\gamma_2\sum_{a=1}^{i-1}r_a^2 $
%     % \State $\varphi_s^a \gets \tilde{\varphi}_{s}^a$
% \EndFor
% \State Compute the matrix $C^t_{a b}=\frac{1}{n}\sum_{j=1}^n \varphi^a_\theta (X_j^0) \varphi^b_\theta (X_j^{t})$  \Comment{$C^t\in \mathbb{R}^{k\times k},\ t=0,\tau$}
% \State Compute the vector $v_a= \frac{1}{n}\sum_{j=1}^n \varphi^a_\theta (X_j^0)\rho(X_j)$
% \State Solve linear system $(C^0-C^\tau)w=v$ \Comment{if $\psi(x) = 0$ enforce $w_1 = 1$}
% \State \Return $u=\sum_{a=1}^k w_a \varphi_\theta^a$ 
% % \EndIf
% % \algstore{bkbreak}
% \end{algorithmic}
% \end{algorithm}



\begin{algorithm}[H]
\caption{Inexact subspace iteration (with $L^2_\mu$ loss) for prediction functions}
\label{alg:forecast}
\begin{algorithmic}[1]
\Require{Subspace dimension $k$, stopped transition data ${\set{X^0_j,X^{\tau \bmin T_j}_j }}_{j=1}^n$, cost data ${\set{ \rho(X_j)}}_{j=1}^n$, batch size $B$, learning rate $\eta$, number of subspace iterations $S$, number of inner iterations $M,$ regularization parameters $\gamma_1$ and $\gamma_2$}
\State Initialize $\{\varphi_\theta^a\}_{a=1}^k$ and $\set{\tilde \varphi_1^a}_{a=1}^k$
\For{$s = 1\dots S$}
    \For{$m = 1\dots M$}
        \State Sample a batch of data $\{X^0_j,X^{\tau\bmin T_j}_j\}_{j=1}^B$, $\set{\rho(X_j)}_{j=1}^B$
        \State $\hat{\mathcal{L}}_1 \gets \frac{1}{B} \sum_{j=1}^B \sum_{a=1}^k \left[ \frac{1}{2} (\sum_{b=1}^k\varphi_\theta^{b}(X_j^0)K_{b a} )^2 
        - \alpha_s\left(\sum_{b=1}^k \varphi_\theta^b K_{ba} (\tilde{\varphi}_s^a(X_j^{\tau \bmin T_j})-\rho(X_j)I_{a 1} ) \right) \right]$
        \State $\hat{\mathcal{L}}_2 \gets - \frac{1}{B} \sum_{j=1}^B \sum_{a=1}^k (1 - \alpha_s) \sum_{b=1}^k \varphi_\theta^b K_{ba} \tilde{\varphi}_s^a(X_j^0)$
        \State $\hat{\mathcal{L}}_K \gets \gamma_1{\norm{K-\mathrm{diag}(K)}}^2_F$
        \State $\hat{\mathcal{L}}_{\rm{norm}} \gets \gamma_2\sum_{a=1}^k(2\nu_a( \frac{1}{B}\sum_{j=1}^B(\varphi_\theta^a(X_j^0)^2)-1)-\nu_a^2)$
        \State $\hat{\mathcal{L}} \gets \hat{\mathcal{L}}_1 +\hat{\mathcal{L}}_2 +  \hat{\mathcal{L}}_K + \hat{\mathcal{L}}_{\rm{norm}}$
        \State $\theta \gets \theta-\eta \nabla_\theta \hat {\mathcal{L}}$
        \State $K \gets K-\eta \,(\mathrm{triu}(\nabla_K \hat {\mathcal{L}}))$
        \State $\nu_a \gets \nu_a+\eta \nabla_{\nu_a} \hat {\mathcal{L}}$
    \EndFor
    \If{$\Psi(x)=0$}
        \State Compute the matrix $\Phi_{ia}^s = \varphi^a_\theta (X_i^0)$ \Comment{$\Phi \in \mathbb{R}^{n\times k}$}
    \Else
        \State Compute the matrix $\Phi_{ia}^s = \varphi^a_\theta(X_i^0)$ for $a > 1$ \Comment{$\Phi \in \mathbb{R}^{n\times (k-1)}$}
        
    \EndIf
    \State Compute QR-decomposition $\Phi^s = QR$ 
    % \Comment{$Q\in \mathbb{R}^{n\times k}$ and $R\in \mathbb{R}^{(k)\times (k)}$}
    \State Compute diagonal matrix $N^2_{aa}=\sum_i\varphi^a_{\theta}(X_i^0)^2$
    \State $\tilde{\varphi}^a_{s} \gets \sum_{b=1}^k \varphi^b_{\theta}\, (R^{-1}N)_{ba}$ 
    \Comment{if $\Psi(x) = 0$ exclude $a = 1$}
    \State set $K_{1:i,i}=\arg \min_{r}\norm{\sum_{a=1}^i\varphi_{\theta}^{a}r_a-\mathcal{S}^{\tau}\tilde{\varphi}_s^a}+\gamma_2\sum_{a=1}^{i-1}r_a^2 $
    % \State $\varphi_s^a \gets \tilde{\varphi}_{s}^a$
\EndFor
\State Compute the matrix $C^t_{a b}=\frac{1}{n}\sum_{j=1}^n \varphi^a_\theta (X_j^0) \varphi^b_\theta (X_j^{t})$ for $t=0,\tau \bmin T_j$ \Comment{$C^t\in \mathbb{R}^{k\times k}$}
\State Compute the vector $p_a= \frac{1}{n}\sum_{j=1}^n \varphi^a_\theta (X_j^0)\rho(X_j)$
\State Solve linear system $(C^0-C^\tau)w=p$ \Comment{if $\Psi(x) = 0$ enforce $w_1 = 1$}
\State \Return $u=\sum_{a=1}^k w_a \varphi_\theta^a$ 
% \EndIf
% \algstore{bkbreak}
\end{algorithmic}
\end{algorithm}



\subsection{M\"uller-Brown committor}\label{sec:mbcommittor}

In this section, we demonstrate the use of our method for prediction by estimating the committor for the M\"uller-Brown model with a shallow intermediate basin at $(-0.25,0.5)$ (Figure~\ref{fig:MB}).   Here the sets $A$ and $B$ are defined as in Eq.~\eqref{eqn:MBstates} and $T$ is the time of first entrance to $D^\comp = A\cup B$. In this case, the prediction problem is tractable with a one-dimensional subspace iteration (i.e., $k=1$ in Algorithm \ref{alg:forecast}).  Figure \ref{fig:MB_q_secondeigenvalue} shows the largest eigenvalue of the { stopped transition operator $\mathcal{S}^\tau$ (the second largest of $\mathcal{A}$ in \eqref{eq:A1}) computed} from our grid-based reference scheme.
% Richardson iteration should converge geometrically in this eigenvalue, and so, for moderate lag times, we can expect our method to converge in a few dozen iterations.  All hyperparameters are listed in Table~\ref{tab:params}.
% In this section, we demonstrate the use of our method for prediction by estimating the committor for the M\"uller-Brown model with a shallow intermediate at $(-0.25,0.5)$ (Figure~\ref{fig:MB}).  In this case, the prediction problem is tractable with a one-dimensional subspace iteration (i.e., $k=1$ in Algorithm \ref{alg:forecast}).  Figure \ref{fig:MB_q_secondeigenvalue} shows the second eigenvalue of the operator $\mathcal{A}$ in \eqref{eq:A1} computed from our grid-based reference scheme.  
Richardson iteration should converge geometrically in this eigenvalue\cite{GoluVanl96}, and so, for moderate lag times, we can expect our method to converge in a few dozen iterations. To initialize the algorithm we choose $\tilde{\varphi}_1^1=\ind{B}$.  
% When using the $L^2_\mu$ loss, we set the output of the net (after the output nonlinearity) to be 0 for $x \in A$ and 1 for $x\in B$.  For the softplus loss, we set the output of the net to be -50 for $x \in A$ and 50 for $x\in B$, such that the softplus applied to the net very nearly satisfies the committor boundary conditions.
All other hyperparameters are listed in Table~\ref{tab:params}.

\begin{figure}[H]
\begin{center}
    \includegraphics[width=0.4\textwidth]{figures/VPM_Q_Evals.png}
\end{center}
    \caption{
    First eigenvalue of $\mathcal{S}^\tau$ (second of  $\mathcal{A}$ in \eqref{eq:A1}) for the M\"uller-Brown model as function of lag time (in units of $\delta_t^{-1}$). The gap between this eigenvalue and the dominant eigenvalue, which is one, determines the rate of convergence of the Richardson iteration. 
    }
    \label{fig:MB_q_secondeigenvalue}
\end{figure}

%We use a fully-connected feed-forward neural network with two inputs, three hidden layers of, each with 64 ReLU activation functions, followed by a linear output layer with a single node.
%We evaluate both the $L^2_\mu$ loss and the softplus loss.
%In the latter case, the architecture is identical, except for the fact that points in $A$ and $B$ are set to be large negative and positive numbers, respectively, to ensure that the boundary conditions are enforced after the network $w_\theta$ is composed with the function $z(x) = (1 + e^{-x})^{-1}$.
%All results shown are run for 100 subspace iterations, each with $M=200$ inner iterations.
%We start damping after $\sigma=50$ subspace iterations, and we use a batch size of $B=5000$ trajectories.
%The Adam optimizer was used with a learning rate of $\eta=1\times 10^{-3}$.  We initialize $\tilde{\varphi}_1^1=\ind{B}$ where $\ind{B}$ is the indicator function on the set $B$.

As noted in Section \ref{sec:testproblems}, we compare results from our method to a grid-based reference. We also estimate the committor using an MSM with 400 states determined by $k$-means clustering of points outside $A$ and $B$.
In addition to the root mean square error (RMSE) for the committor itself, we show the RMSE of  
\begin{equation}\label{eq:logit}
\mathrm{logit}_{\varepsilon}(q)=\log\left(\frac{\varepsilon+q}{1+\varepsilon-q}\right)
\end{equation}
for points outside $A$ and $B$.  This function amplifies the importance of values close to zero and one.
We include $\varepsilon$ because we want to assign only a finite penalty if the procedure estimates $q$ to be exactly zero or one; we use $\varepsilon=e^{-20}$.

Results as a function of lag time are shown in Figure \ref{fig:MB_q}.  We see that the Richardson iterate is more accurate than the MSM for all but the shortest lag times.  When using the $L^2_\mu$ loss, the results are comparable, whereas the softplus loss allows the Richardson iterate to improve the RMSE of the logit function in \eqref{eq:logit} with no decrease in performance with respect to the RMSE of the committor.  Results as a function of the size of the data set are shown in Figure \ref{fig:MB_q_Data} for a fixed lag time of $\tau = 0.1 \delta_t^{-1}$.  The Richardson iterate generally does as well or better than the MSM. Again, the differences are more apparent in the RMSE of the logit function in \eqref{eq:logit}.  By that measure, the Richardson iterate obtained with both loss functions is significantly more accurate than the MSM for small numbers of trajectories. The softplus loss maintains an advantage even for large numbers of trajectories.

\begin{figure}[H]
\begin{center}
    \includegraphics[width=0.8\textwidth]{figures/VPM_Q_Test_Lag.png}
\end{center}
    \caption{
    Committor prediction for the M\"uller-Brown system as a function of lag time (in units of $\delta_t^{-1}$).
    (left) Comparison of the inexact Richardson iteration using the $L^2_\mu$ loss  and an MSM with 400 states. (right) Same comparison using the softplus loss in place of the $L^2_\mu$ loss.
    }
    \label{fig:MB_q}
\end{figure}

\begin{figure}[H]
\begin{center}
    \includegraphics[width=0.8\textwidth]{figures/VPM_Q_Test_Data.png}
\end{center}
    \caption{
    Committor prediction for the M\"uller-Brown as a function of number of initial conditions. (left) Comparison of inexact Richardson iteration using the $L^2_\mu$ loss  and an MSM with 400 states.  Richardson iteration results shown are from iteration 100. (right) Same comparison using the softplus loss in place of the $L^2_\mu$ loss.
    }
    \label{fig:MB_q_Data}
\end{figure}


\subsection{Accelerating convergence by incorporating eigenfunctions}\label{sec:Mod_MB}
% \subsection{\label{sec:Mod_MB} Modified M\"uller-Brown}

%Because the procedure above must shift probability %from $B$ to $A$, 
%For any Richardson iteration, the number of iterations to converge with $k=1$ (i.e., a pure Richardson iteration) scales with the MFPT from the intermediate to $D^c = (A \cup B)$.

The largest eigenvalue $\lambda_1$ of the stopped transition operator $\mathcal{S}^\tau$, which determines the convergence rate of Richardson iteration, also plays an important role in theoretical results related to the quasi-stationary distribution in $D$\cite{collett_quasi-stationary_2012}.  The quasi-stationary distribution in $D$ is defined by $\nu(dx) = \lim_{t\rightarrow \infty}\mathbb{P}\left[ X^t\in dx \mid t< T\right]$ where $dx$ is any subset of $D$.  Up to a constant multiple, the density of $\nu$ is exactly the eigenfunction of $\mathcal{S}^\tau$ corresponding to $\lambda_1$.  Moreover, if $X^0$ is drawn from $\nu$ then 
$-\tau/\log \lambda_1$ is the mean first passage time out of $D$, i.e., $\E_{X^0\sim\nu}\left[ T\right] = -\tau/\log \lambda_1.$  But $-\tau/\log \lambda_1$ also gives the number of iterations required for the error in (exact) Richardson iteration to reduce by a fixed factor (asymptotically independent of $\tau$ and $\lambda_1$).  We therefore expect that when the typical time of first entrance to $D^\comp$ is large, a large number of iterations will be required by inexact Richarson iteration.  In particular, if $D$ includes any metastable states, those states will have high likelihood in $\nu$ and result in large values of $-\tau/\log \lambda_1$.

With this in mind, we can expect inexact Richardson iteration for the M\"uller-Brown to perform poorly  if we deepen the intermediate basin at $(-0.25,0.5)$ as in Figure~\ref{fig:SI_Intro} (top left). 
Again the sets $A$ and $B$ are defined as in Eq.~\eqref{eqn:MBstates} and $T$ is the time of first entrance to $D^\comp = A\cup B$.
In this case, $\lambda_1$ is extremely close to one at all lag times, so the Richardson iteration is slow to converge (Figure~\ref{fig:SI_Intro}, bottom left).  For this system, $-1/\log\lambda_1$ is on the order of $100$ for the lag times we consider.
Estimates of the committor by inexact Richardson iteration do not reach the correct values even after hundreds of iterations (Figure~\ref{fig:SI_Intro}, bottom right).
% [CAN WE SAY THAT THEY WILL NOT EVER?]


% The convergence rate for Richardson iteration is the second eigenvalue of the operator being iterated, which in this case is $\mathcal{A}$ in \eqref{eq:A1}.  For forecast functions, the second eigenvalue of $\mathcal{A}$ is the dominant eigenvalue of the stopped transition operator $\mathcal{S}^\tau.$  This eigenvalue is the exponential rate of survival for trajectories initialized from the quasi-stationary distribution.  That is, given the quasi-stationary distribution $\nu,$ and the stopped transition operator with Perron-Frobenius eigenvalue $\lambda_1,$ then $P_{X^0\sim\nu}[X^t\in D \mid 0<t<T]=e^{-\theta t}$ with $\theta=-(\log\lambda_1)/\tau$ (see Ref.~\onlinecite{collett_quasi-stationary_2012}, Chapter 3 for finite Markov jump processes).  The quantity $\theta^{-1}$ is the MFPT from the quasi-stationary distribution to the boundary set $A \cup B$.  If the system has a deep intermediate, most of the quasi-stationary probability density is in the intermediate.
% Therefore, $\theta^{-1}$ is dominated by the escape time from the intermediate to the boundary set $A \cup B$ and $(\tau\theta)^{-1}$ gives the order of number of Richardson iterations needed for convergence.

% With this in mind, we can understand why the one-dimensional subspace iteration for the M\"uller-Brown fails to converge if we deepen the intermediate at $(-0.25,0.5)$ as in Figure~\ref{fig:SI_Intro} (top left).
% In this case, the second eigenvalue is extremely close to one at all lag times, so the Richardson iteration is slow to converge (Figure~\ref{fig:SI_Intro}, bottom left).  For this system, $(\tau\theta)^{-1}$ is on the order of $100$ for the lag times we consider here.
% Because each step of the Richardson iteration produces an estimate that is close to the previous one, the estimated committors of the intermediate are slow to increase.  Indeed, they do not reach the correct values even over hundreds of iterations (Figure~\ref{fig:SI_Intro}, bottom right).
% % Figure \ref{fig:SI_Intro} illustrates the modified system and the failure of pure Richardson iteration.   


\begin{figure}[H]
    \centering
    \includegraphics[scale=.7]{figures/SI_Intro.png}
    \caption{Richardson iteration for the committor converges slowly for a M\"uller-Brown potential with a deepened intermediate.  (top left) Potential energy surface, with states $A$ and $B$ indicated. Contour lines are drawn every 1 $\beta^{-1}$.  (top right) Reference committor. (bottom left) Dominant eigenvalue as a function of lag time (in units of $\delta_t^{-1}$) from an MSM with 400 clusters, subspace iteration, and the grid-based reference. 
    % Note that the magnitude of the second eigenvalue indicates that hundreds of Richardson iterations will be required.  
    (bottom right) Example of the Richardson iteration after 400 iterations.  Note the overfitting artifacts and lack of convergence near the intermediate state.}
    \label{fig:SI_Intro}
\end{figure}

% We now illustrate how convergence can be acccelerated by incorporating eigenfunctions (i.e., $k > 1$ in Algorithm \ref{alg:forecast}).
% %In this example, the dominant eigenfunction is parameterized by a fully connected network with three hidden layers, each with 64 ReLU activation functions, and a linear output layer with no activation function.  The dominant eigenfunction is trained with the softplus loss.  For the model that we consider here, the third eigenvalue is around $1\times 10^{-4}$ for a lag time of $\tau=1$ (while the second eigenvalue is near one as discussed above), so we choose $k=2$ owing to this spectral gap.  We parameterize the second eigenfunction with a neural network of the same architecture, and we update it using the $L^2_\mu$ loss function.  We use a batch size of $B=2000$ and $M=5000$ inner iterations.  The learning rate was $\eta=1\times 10^{-3}$.  Because $k=2$, the matrix $K$ is the identity, so no regularization on $K$ or normalization is used.
%  %[MOVED FROM CAPTION] Although every eigenfunction satisfies homogeneous boundary conditions, the Richardson iterate (bottom left panel) and the first non-dominant eigenfunction (bottom center panel) have different boundary conditions. This prevents these two vectors from collapsing onto each other and also means that the two should not be linearly combined by $K$.
%  For the model that we consider here, the third eigenvalue is around $1\times 10^{-4}$ for a lag time of $\tau=1$ (while the second eigenvalue is near one as discussed above), so we choose $k=2$ owing to this spectral gap.

We now show that convergence can be acccelerated dramatically by approximating additional eigenfunctions of $\mathcal{S}^\tau$ (i.e., $k > 1$ in Algorithm \ref{alg:forecast}).
For the M\"uller-Brown model with a deepened intermediate basin, the second eigenvalue of $\mathcal{S}^\tau$ is roughly $1\times 10^{-4}$ for a lag time of $\tau=1000=1\,{\delta_t}^{-1}$  (while the first is near one as discussed above). We therefore choose $k=2$ with $\tilde{\varphi}^2_1$ initialized as a random linear combination of coordinate functions as in previous examples. 
%Although this function does not satisfy homogenous boundary conditions,  $\varphi^2_\theta$ does respect them and so each subsequent $\tilde{\varphi}^2_s$ should approximately satisfy them as well. 
We run the subspace iteration for four iterations, compute the committor as a linear combination of the resulting functions, and then refine this result with a further ten Richardson iterations (i.e., $k=1$ with the starting vector as the output of the $k=2$ subspace iteration).
To combine the functions, we use a linear solve which incorporates memory  (Algorithm \ref{alg:memory}) \cite{darve_computing_2009,cao_advantages_2020}.  This algorithm improves the data-efficiency substantially for poorly conditioned problems.  For our tests here, we use two memory kernels, corresponding to $\Delta=\lfloor\tau / 4\rfloor$. % {\color{blue} DOES THIS NEED TO BE UPDATED BECAUSE OF INTEGER $\tau$?]}

\begin{algorithm}[H]
\caption{Memory-corrected linear solve for predictions}
\label{alg:memory}
\begin{algorithmic}[1]
\Require{Stopped transition data $\set{X^0_j,X^{1 \bmin T_j}_j, \dots, X^{\tau \bmin T_j}_j}_{j=1}^n$, guess function $h$, cost data ${\set{ \rho(X_j)}}_{j=1}^n$, basis set $\{f^a\}_{a=1}^k$, lag between memory kernels $\Delta$.}
% \State $\Delta\gets \left \lfloor \frac{\tau}{M+2}\right\rfloor$
% \For{$i =0\dots \tau$}
%     \State Set $b'(X^{i \bmin T}_j)=\psi(X^{i \bmin T}_j)+(X^{i \bmin T}_j)$
% \EndFor
\For{$s =0\dots (\tau / \Delta)$}
    \State Initialize the matrix $C^s$ with zeros \Comment{$C^s\in \mathbb{R}^{(k+1)\times (k+1)}$}
    \State $C_{11}^s\gets 1$
    \For{$a = 2\dots k$}
    \State $C_{a1}^s\gets \frac{1}{n}\sum_{j=1}^n f^a(X_j^0)\rho(X_j^{s\Delta \bmin T_j}))$
        \For{$b = 2\dots k$}
            \State $C_{ab}^s\gets \frac{1}{n}\sum_{j=1}^n f^a(X_j^0)f^b(X_j^{s\Delta \bmin T_j}))$
        \EndFor
    \EndFor
\EndFor
\State $A\gets C^1-C^0$
\For{$s = 0\dots (\tau / \Delta) - 2$}
    \State $M^s\gets C^{s+2}-C^s-A(C^0)^{-1}C^{s+1}-\sum_{j=0}^{s}M^j(C^0)^{-1}C^{s-j}$
\EndFor
\State $A_{\text{mem}}\gets A+\sum_{s=0}^{(\tau / \Delta) - 2} M^s$
\State Solve $A_{\text{mem}}w=w$
\State \Return $u=h +\sum_{a=2}^k w_a f^a$ 
\end{algorithmic}
\end{algorithm}


The bottom row of Figure~\ref{fig:SI_Illustration} illustrates the idea of the subspace iteration.
%We initialize  $\varphi_\theta^1 = \psi=\ind{B}$ and use $\tau = 1$.   The second eigenfunction 
The second eigenfunction (Figure \ref{fig:SI_Illustration}, center) is peaked at the intermediate.  As a result, the two neural-network functions linearly combined by the Galerkin approach with memory can yield a good result for the committor (Figure \ref{fig:SI_Illustration}, bottom right).
Figure~\ref{fig:SI_Q_error} compares the RMSE for the committor and the RMSE for the logit in \eqref{eq:logit} for Algorithm \ref{alg:forecast} with $k=1$ (pure Richardson iteration) and $k=2$ (incorporating the first non-trivial eigenfunction), and an MSM with 400 states.  We see that the Richardson iteration suffers large errors at all lag times; as noted previously, this error is mainly in the vicinity of the intermediate.  The MSM cannot accurately compute the small probabilities, but does as well as the subspace iteration in terms of RMSE.
%\section{\label{sec:memory}Linear solve with memory}
%Here we present, without derivation, the memory-corrected linear solve for the forecast subspace iteration

\begin{figure}[H]
    \centering
    \includegraphics[scale=.7]{figures/SI_Illustration.png}
    \caption{Illustration of the subspace iteration for the M\"uller-Brown committor.  (top left) Modified M\"uller-Brown potential. (top center) Reference second eigenfunction. (top right) Reference committor. (bottom left) Neural-network Richardson iterate after four iterations.  (bottom center) First non-dominant eigenfunction obtained from the neural network after four iterations. (bottom right) Committor resulting from linear combination of the Richardson iterate and second eigenfunction. Results shown are for $\tau=1000$ (i.e. $1\, {\delta_t}^{-1}$). 
    }
    \label{fig:SI_Illustration}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[scale=.6]{figures/SI_Q_Results.png}
    \caption{Committor for the modified M\"uller-Brown potential as a function of lag time (in ${\delta_t}^{-1}$ units). (left) Comparison of RMSE for subspace iteration as described above, Richardson iteration (as in  Section~\ref{sec:mbcommittor} but instead with 500 subspace iterations), and an MSM with 400 states.
    (right) RMSE of the logit function in \eqref{eq:logit}.}
    \label{fig:SI_Q_error}
\end{figure}

\subsection{\texorpdfstring{AIB\textsubscript{9}}{AIB9} prediction results}\label{sec:aib9_results}

As an example of prediction in a high-dimensional system, we compute the committor for the transition between the left- and right-handed helices of AIB\textsubscript{9} using the inexact Richardson iteration scheme ($k=1$ in Algorithm~\ref{alg:forecast}) with the softplus loss function. Specifically, for this committor calculation $T$ is the time of first entrance to $D^\comp = A\cup B$ with $A$ and $B$ defined in Section~\ref{sec:aib_numerics}.
As before, we initialize $\tilde{\varphi}_1^1 = \ind{B}$.
% {\color{blue}[WHAT DOES ``random'' INITIALIZATION MEAN IN THE TABLE? SPECIFY HERE. WHY NOT $\mathbbm{1}_B$?]}


To validate our results, we use the 5 $\mu$s reference trajectories to compute an empirical committor  as a function of the neural network outputs, binned into intervals:
\begin{equation}
\label{eq:emp_q}
    \bar{q}(s) = \mathbbm{P}\left[X^T\in B \mid u_\theta(X^0) \in [s, s + \Delta s]\right]
\end{equation}
for $s\in [0, 1 - \Delta s]$.
%Equation~\eqref{eq:emp_q} corresponds to the mean committor over isocommittor surfaces of the neural network output.
Here, we use $\Delta s = 0.05$.  
The overall error in the committor estimate is defined as
\begin{equation}
\label{eq:emp_error}
q\ \textrm{error}=
    \left(\Delta s\sum_{n = 0}^{1 / \Delta s - 1} \big[\bar{q}(n\Delta s) - n\Delta s \big]^2 \right)^{1/2}.
\end{equation}
While this measure of error can only be used when the data set contains trajectories of long enough duration to reach $D^\comp$, it has the advantage that it does not depend on the choice of projection that we use to visualize the results.

Results for the full data set with $\tau$ corresponding to 400 ps are shown in Figure~\ref{fig:aib_q}.   The projection on the principal components is consistent with the symmetry of the system, and the predictions show good agreement with the empirical committors.  As $\tau$ decreases, the results become less accurate (Figure~\ref{fig:emp_error}, top left); at shorter lag times we would expect further increases in the error.
% Results with varying lag time are shown in Figure~\ref{fig:aib_q}.
We also examine the dependence of the results on the size of the data set (Figure~\ref{fig:emp_error}, top right).
We find that the performance steadily drops as the number of initial conditions is reduced and degrades rapidly (Figure~\ref{fig:emp_error}, bottom) for the data sets subsampled more than 20-fold, corresponding to about 7 $\mu$s of total sampling.
%Surprisingly, the error does not appear to decrease significantly upon further addition of data, though this may be due to the limitations of computing accurate committors from our reference trajectories. 
We interpret the apparent floor in error of about 0.02 as deriving from the limitations inherent in the empirical committor.


% As an example of prediction in a high-dimensional system, we compute the committor and MFPT for the transition between the left- and right-handed helices of AIB\textsubscript{9}.
% To validate our results, we use the 5 $\mu$s reference trajectories to compute an empirical committor and MFPT as a function of the neural network outputs, binned into intervals:
% \begin{equation}
% \label{eq:emp_q}
%     \bar{q}(s) = \mathbbm{P}\left[X^T\in B \mid u_\theta(X^0) \in [s, s + \Delta s]\right]
% \end{equation}
% for $s\in [0, 1 - \Delta s]$;
% %Equation~\eqref{eq:emp_q} corresponds to the mean committor over isocommittor surfaces of the neural network output.
% here, we use $\Delta s = 0.05$.  The MFPT is unbounded, so we compute
% \begin{equation}
% \label{eq:emp_mfpt}
%     \bar{m}(s) = \E\left[T \mid u_\theta(X^0) \in [s, s + \Delta s]\right]
% \end{equation}
% for $s\in [0, m_{\max} - \Delta s]$ where $\Delta s=2.5$ and $m_{\max}=47.5$ ns.
% The overall error in a prediction function $u$ (committor or MFPT) is defined as
% \begin{equation}
% \label{eq:emp_error}
% u\ \textrm{error}=
%     \left(\Delta s\sum_{n = 0}^{1 / \Delta s - 1} \big[\bar{u}(n\Delta s) - n\Delta s \big]^2 \right)^{1/2}.
% \end{equation}
% An advantage of this measure of error is that it does not depend on the choice of projection that we use to visualize the results.

%For the committor, we use a network with six hidden layers, each with 150 ReLU activation functions and a linear output layer with a single node, and we train with the softplus loss.
%Boundary conditions are enforced as described in Section~\ref{sec:mbcommittor}.
%The network receives the same 174 Cartesian coordinates as for spectral estimation. 
%The training was performed with a batch size of $B=1024$ and $T=100$ subspace iterations, each with $M=2000$ inner iterations.
%We applied damping with $\sigma = 50$, and the learning rate for Adam was $\eta=1\times 10^{-3}$.

% We compare against an linear method using an MSM built from 200 clusters.
% Consistent with the M\"uller-Brown, we find that the iterations converge rapidly (Figure~\ref{fig:aib_q_iter10})





% The projection on the principal components is consistent with the symmetry of the system, and the predictions show good agreement with the empirical committors.  As the lag time decreases, the results become less accurate (Figure~\ref{fig:emp_error}, top left); at shorter lag times we would expect further increases in the error.
% % Results with varying lag time are shown in Figure~\ref{fig:aib_q}.
% We also examine the dependence of the results on the size of the data set (Figure~\ref{fig:emp_error}, top right).
% We find that the performance steadily drops as the number of initial conditions is reduced and degrades rapidly (Figure~\ref{fig:emp_error}, bottom) for the data sets subsampled more than 20-fold, corresponding to about 7 $\mu$s of total sampling.
% %Surprisingly, the error does not appear to decrease significantly upon further addition of data, though this may be due to the limitations of computing accurate committors from our reference trajectories. 
% We interpret the apparent floor in error of about 0.02 as deriving from the limitations inherent in the empirical committor.

\begin{figure}[htb]
    \centering
    \includegraphics{figures/aib9_q_ref_nn_emp_xyz.pdf}
    \vspace*{-18pt}
    \caption{AIB\textsubscript{9} committor for the transition between left- and right-handed helices.  Projection onto the dPCs of (left) the empirical committors and (middle) representative neural-network committors using $\tau$  corresponding to 400~ps trained on the full data set.
    (right) Comparison between empirical committors and neural network committors trained with $\tau$ corresponding to 400~ps. Error bars indicate standard deviations over ten different initializations of the neural-network parameters.}
    \label{fig:aib_q}
\end{figure}

\begin{figure}[htb]
    \centering\includegraphics{figures/aib9_empq_all_xyz_2.pdf}
    \caption{AIB\textsubscript{9} committor for the transition between left- and right-handed helices, as functions of lag time (in ps) and number of initial conditions. (top left) Error in the committor as a function of lag time  (in ps). Shading indicates the standard deviation over ten different initializations of the neural-network parameters.
    (top right) Error in the committor as a function of the number of initial conditions with $\tau$ corresponding to 160 ps.
    Shading indicates the standard deviation over ten different random samples of the trajectories.
    (bottom) Comparison between empirical committors and neural-network committors trained on data sets with (left) 1/2 and (right) 1/20 of the short trajectories. Error bars indicate standard deviations over ten random samples of the trajectories.}
    \label{fig:emp_error}
\end{figure}

%\subsection{Mean first passage time for \texorpdfstring{AIB\textsubscript{9}}{AIB9}}

Finally, we compute the MFPT to reach the right-handed helix using the same data set.     For the MFPT calculation $T$ is the time of first entrance to $D^\comp=B$.  Note that the time of first entrance to $B$ includes long dwell times in $A$ and is expected to be much larger than the time of first entrance to $A\cup B$.
 
 We compare against an empirical estimate of the MFPT defined by
\begin{equation}
\label{eq:emp_mfpt}
    \bar{m}(s) = \E\left[T \mid u_\theta(X^0) \in [s, s + \Delta s]\right]
\end{equation}
for $s\in [0, m_{\max} - \Delta s]$ where $\Delta s=2.5$ and $m_{\max}=47.5$ ns.
Overall error is defined analogously to Eq.~\eqref{eq:emp_error}.


In Figure~\ref{fig:aib9_mfpt}, we show the MFPT obtained from Algorithm \ref{alg:forecast} with $k=1$ (pure Richardson iteration) and $k=5$.  Both calculations use the $L^2_\mu$ loss function. 
Inialially $\tilde{\varphi}^1_1$ is set equal to $5\mathbbm{1}_A$ and $\tilde{\varphi}^a_s$ for $a>1$ is set to a random linear combination of coordinate functions.
The horizontal line in  Figure~\ref{fig:aib9_mfpt} indicates a MFPT of about 46~ns estimated from the long reference trajectories.
We see that the algorithm with $k=5$ converges much faster (note the differing scales of the horizontal axes) and yields accurate results at all lag times other than the shortest shown. The need to choose $k>1$ for this MFPT calculation is again consistent with theoretical convergence behavior of exact subspace iteration.  Because the typical time of first entrance to $B$ from points in $A$ will be very large, we expect the dominant eigenvalue of $\mathcal{S}^\tau$ to be very near to one when $D=B^\comp$.  In contrast, the committor calculation benefits from the fact that the time of first entrance to $A\cup B$ is much shorter, implying a smaller dominant eigenvalue of $\mathcal{S}^\tau$ when $D=(A\cup B)^\comp.$

% In Figure~\ref{fig:aib9_mfpt}, we show the MFPT obtained from Algorithm \ref{alg:forecast} with $k=1$ (pure Richardson iteration) and $k=5$.  
% %For this test, we use the same Cartesian coordinates as input features, we parameterize the dominant eigenfunction with a fully connected feed-forward neural network with six layers, each with 150 ReLU activation functions, and a linear output layer.  We represent the remaining two functions by a network that has a similar architecture but two outputs.  We use a batch size of $B=2000$ trajectories, regularization parameters $\gamma_1=0.1$ and $\gamma_2=10,$ a learning rate of $\eta=1\times 10^{-3}$, and $M=1000$ inner iterations.  We initialize $\tilde{\varphi}_1^1=5\,\ind{A}$, and the lower eigenfunctions are initialized $\tilde{\varphi}_1^a(x)=xR_a$ where each $R_a$ is a standard normal vector, and $x$ is the input features of the net.
% The horizontal line in  Figure~\ref{fig:aib9_mfpt} indicates a MFPT of about 46~ns estimated from the long reference trajectories.
% We see that the algorithm with $k=5$ converges much faster (note the differing scales of the horizontal axes) and yields accurate results at all lag times other than the shortest shown.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth] {figures/VPM_MFPT_Test.png}
    \caption{
    MFPT between left- and right-handed helices for the AIB\textsubscript{9} molecular system. (left) Convergence of Richardson iteration. (right) Convergence of a five-dimensional subspace iteration. Shading indicates standard deviations over ten different initializations of the neural-network parameters. (bottom left) MFPT after 100 and 200 subspace iterations as a function of lag time (in ps units).  (bottom right) Overall error in MFPT.
    % {\color{blue} [WE USE A DIFFERENT AXIS LABEL FOR LAG TIME IN THIS PLOT]}
    }
    \label{fig:aib9_mfpt}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth] {figures/VPM_AIB9_MFPT_illus.png}
    \caption{AIB\textsubscript{9} MFPT to the right-handed helix. Projection onto the dPCs of (left) the empirical MFPTs (middle) representative neural network MFPTs using $\tau$ corresponding to 400~ps.
    (right) Comparison between empirical MFPTs and neural network MFPTs trained with $\tau$ corresponding to 400~ps. Error bars indicate standard deviations over ten different initializations of the neural-network parameters.}
    \label{fig:aib_mfpt}
\end{figure}



\section{Conclusions}

In this work we have presented an efficient method for spectral estimation and rare-event prediction from short-trajectory data.  The key idea is that we use the data as the basis for an inexact subspace iteration.  For the test systems that we considered, the method not only outperforms high-resolution MSMs, but it can be tuned through the choice of loss function to compute committor probabilities accurately near the reactants, transition states, and products.  Other than the Markov assumption, our method requires no knowledge of the underlying model and puts no restrictions on its dynamics. 

As discussed in prior neural-network based prediction work\cite{strahan_forecasting_2022, rotskoff_active_2022}, our method is sensitive to the quality and distribution of the initial sampling data. However, our work shares with Ref.~\citenum{strahan_forecasting_2022} the major advantage of using arbitrary inner products.  This allows for adaptive sampling of the state space \cite{lucente_coupling_2022, strahan_forecasting_2022} and---together with the features described above---the application to observational and experimental data, for which the stationary distribution is generally unavailable.

%, allowing us to potentially guide sampling toward important regions of phase space 
%Therefore, our method lends itself to various adaptive sampling schemes to reduce variance.   
%Furthermore, the initial conditions of the trajectories can be drawn from an arbitrary distribution. 

%We showed that our method can accurately calculate prediction statistics and dominant eigenspaces more efficiently and accurately than high resolution MSMs.  Importantly, we showed that our subspace iteration method can handle the difficult problem of computing prediction functions for rare-event problems, and can even compute small committor probabilities accurately.

In the present work, we focused on statistics of transition operators, but our method can readily be extended to solve problems involving their adjoint operators as well.
By this means, we can obtain the stationary distribution as well as the backward committor.  The combination of forward and backward predictions allows the full analysis of transition paths using transition path theory without needing to generate full transition paths\cite{e_transition-path_2010, vanden2006transition,lorpaiboon_augmented_2022} and has been used to understand rare transition events in molecular dynamics\cite{strahan_long-time-scale_2021, noe_constructing_2009, antoszewski_kinetics_2021, guo_dynamics_2022, meng_transition_2016} and geophysical flows\cite{Finkel2020paths, miron_transition_2021, lucente_committor_2022, finkel2023revealing, finkel_data-driven_2023}.
We leave these extensions to future work.

In cases in which trajectories that reach the reactant and product states are available, it would be interesting to compare our inexact iterative schemes against schemes for committor approximation
based on maximum likelihood estimation and related approaches\cite{Ma2005nn, peters_obtaining_2006, peters2007extensions, hu2008two, jung_artificial_2019, jung_autonomous_2021, miloshevich_probabilistic_2023}.  These schemes are closely related to  what is called ``Monte-Carlo'' approximation in reinforcement learning \cite{sutton_reinforcement_2018}, and also to the inexact Richardson iteration that we propose here with $\tau = \infty$.

Indeed, temporal difference (TD) learning, a workhorse for prediction in RL, is closely related to an inexact form of Richardson iteration, which explains its poor performance in problems involving rare events.  Variants like TD$(\lambda)$, have similar relationships to inexact iterative schemes and similar limitations.   As we showed, subspace iteration is a natural way of addressing slow convergence.  We thus anticipate that our results have implications for reinforcement learning, particularly in scenarios in which the value function depends on the occurrence of some rare event. 
Finally, we note that our framework can be extended to the wider range of iterative numerical linear algebra algorithms.  In particular,  Krylov subspace methods may offer further acceleration and enhanced stability.


% In this work we have described an efficient method to obtain eigenfunctions and predictions of Markov processes.  Our method makes no assumptions on the underlying dynamics other than Markovianity.  We showed that our method can accurately calculate prediction statistics and dominant eigenspaces more efficiently and accurately than high resolution MSMs.  Importantly, we showed that our subspace iteration method can handle the difficult problem of computing prediction functions for rare-event problems, and can even compute small committor probabilities accurately.  Our method takes in short trajectories drawn from arbitrary starting distributions, which allows us to apply it to experimental data as well as simulation data.

% As discussed, our algorithm reframes the traditional TD algorithm in reinforcement learning as an inexact Richardson iteration, which allows natural extensions to subspace iteration for faster convergence.
% As a consequence, we anticipate that our results have implications scenarios where TD is used to approximate the value of a given policy.
% We also note that related algorithms in reinforcement learning known as ``Monte-Carlo'' methods, including the TD$(\lambda)$ algorithm, can also be seen within the framework we describe here.
% Monte-Carlo methods obtain estimates of the value function from full episodes, or trajectories which reach a final, absorbing state.
% In our scheme, this can be thought of letting $\tau \rightarrow\infty$, or running trajectories until they enter the target domain $D$.
% A similar version of this algorithm has been proposed by the name of aimless shooting\cite{peters_obtaining_2006, jung_autonomous_2021} for obtaining committors from molecular dynamics simulations by minimizing a loss similar to the softplus loss.


% The iterative schemes described for transition operators can naturally be extended to solve problems involving the adjoint operator.
% This includes the stationary distribution as well as backward-in-time predictions such as the backward committor (which can be framed as a stationary distribution calculation on an appropriately augmented Markov process).  The combination of forward and backward predictions allows the full analysis of transition paths using transition path theory without needing to generate full transition paths\cite{strahan_long-time-scale_2021, vanden2006transition}.
% We leave these extensions to future work.

% As discussed in prior neural-network based forecasting work\cite{strahan_forecasting_2022, rotskoff_active_2021}, our method is sensitive to the quality and distribution of the initial sampling data. However, our work shares with Ref.~\citenum{strahan_forecasting_2022} the major advantage of using arbitrary inner products, allowing us to potentially guide sampling toward important regions of phase space.
% Therefore, our method lends itself to various adaptive sampling schemes to reduce variance.

% Finally, we note that our framework can be extended to the wide range of accelerated iterative methods, such as Krylov or Arnoldi subspace methods.
% These not only promise further acceleration or enhanced stability, but also connect our work with the rich literature on numerical linear algebra.


% [MENTION KRYLOV], [MENTION IMPORTANCE OF SAMPLING], [MENTION TD($\lambda$) AND ``MONTE CARLO''], [CONNECT TO OTHER INEXACT PI/SI (FRI)], [IS THERE AN ``INTEGRATED'' FORM OF OUR SI METHOD]

\section*{Author's contributions}

J.S., S.C.G., A.R.D., and J.W. conceptualized research.  J.S. developed the method.  C.L. adapted the linear solve used for prediction from Refs.\ \onlinecite{darve_computing_2009,cao_advantages_2020}.  J.S. and S.C.G. performed the numerical tests.   A.R.D. and J.W. supervised the research.  All authors wrote and edited the manuscript.


\begin{acknowledgements}
We are grateful to Arnaud Doucet for pointing our attention to TD methods and the inexact power iteration in Ref.\ \onlinecite{wen_batch_2020}, both of which were key motivations for this work. We also thank Joan Bruna for helpful conversations about reinforcement learning.
This work was supported by National Institutes of Health award R35 GM136381 and National Science Foundation award DMS-2054306.
S.C.G.\ acknowledges support by the National Science Foundation Graduate Research Fellowship under Grant No.\ 2140001. J.W.\  acknowledges support from the Advanced Scientific Computing Research Program within the DOE Office of Science through award DE-SC0020427.
This work was completed in part with resources provided by the University of Chicago Research Computing Center, and we are grateful for their assistance with the calculations.
 ``Beagle-3: A Shared GPU Cluster for Biomolecular Sciences'' is supported by the National Institute of Health (NIH) under the High-End Instrumentation (HEI) grant program award1S10OD028655-0.
\end{acknowledgements}

\section*{Data Availability Statement}

The data that supports the findings of this study are available within the article.


\clearpage

\appendix

% \renewcommand{\thetable}{\arabic{table}}
% \renewcommand{\theHfigure}{S\arabic{figure}}
\renewcommand{\thefigure}{S\arabic{figure}}
% \renewcommand{\theequation}{S\arabic{equation}}

\setcounter{figure}{0}


%\section{\label{sec:numeric}Numerical details}
% text


%\appendix*

\clearpage


% \section{Supplementary Figures}

% \begin{figure}[ht]
%     \centering
%     \includegraphics{figures/aib9_q_lags.pdf}
%     \caption{Committor as a function of lag time for the left-right helix transition in AIB\textsubscript{9} projected onto the first two PCA coordinates. Data points are subsampled by a factor of 10 to increase clarity.}
%     \label{fig:aib_q}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics{figures/aib9_q_lags_iter10.pdf}
%     \caption{Committor as a function of lag time for the left-right helix transition in AIB\textsubscript{9} projected onto the first two PCA coordinates after 10 power iterations.}
%     \label{fig:aib_q_iter10}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics{figures/aib9_q_msm_lag8.pdf}
%     \caption{Committor for left-right helix transition in AIB\textsubscript{9} using an MSM with 200 clusters. The lag time was 1.28 ns.}
%     \label{fig:my_label}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics{figures/aib9_empq_lag.pdf}
%     \caption{Correlation between the VIDyA committors and mean empirical committor obtained from long trajectories for AIB\textsubscript{9} as a function of lag time. Error bars indicate standard deviation over 10 networks.}
%     \label{fig:emp_loss_lag}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics{figures/aib9_empq_skip.pdf}
%     \caption{Correlation between the VIDyA committors and mean empirical committor obtained from long trajectories for AIB\textsubscript{9} as a function of number of trajectories. Error bars indicate standard deviation over 10 random subsets of trajectories.}
%     \label{fig:emp_loss_skip}
% \end{figure}

% \clearpage


% \clearpage

\section*{References}


\bibliographystyle{unsrt}
\bibliography{draft1.bbl}

% \clearpage





\end{document}
