\section{The \grood Method}\label{sec:method}

\begin{figure*}[t]
    \centering
    \begin{tabular}{ccc}
	    \includegraphics[width=0.3\linewidth]{figures/figure1_exp_id_0-crop.pdf} &
	    \includegraphics[width=0.3\linewidth]{figures/figure1_exp_id_1-crop.pdf} &
	    \includegraphics[width=0.3\linewidth]{figures/figure1_exp_id_2-crop.pdf} \\ 
	    % \includegraphics[width=0.22\linewidth]{figures/auroc_comparison-crop.pdf} \\
	    (a) & (b) & (c) % & (d)
    \end{tabular}
    \caption{Complementarity of the LP and NM classifiers. (a) When applied to
    a problem with semantic shift only, the LP classifier tends to separate the
    ID and OOD datasets better. (b) For OOD problems with mixed semantic and
    domain shifts, NM classifier performs typically better. (c) On some
    problems, both perform well. 
    %(d) For different OOD problems one or the other classifier is a better
    %choice as shown in using the AUROC score. 
    Notice that moving from a single LP or NM similarity to a two-dimensional space
    already allows better separation in {\it all} cases. Compare this with
    detailed results in Tabs~\ref{tab:osr_results}-\ref{tab:semantic_results}.
    }
    \label{fig:lp_ncm_complementarity}
\end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\columnwidth]{figures/miscalibration.pdf}

    \caption{Mis-calibration of the logit scores. The graph shows cumulative
    distributions for ID (full line) and OOD (dashed) classes given the LP
    logit scores trained on the ID data. Here CIFAR10 is ID and TinyImageNet is
    OOD. Selecting a single logit threshold, 10 in this case, results in
    different ID class rejection rates. We call this {\it logits
    mis-calibration}.
    }
    \label{fig:mls_miscalibration}
\end{figure}

\begin{figure}[t]
    \centering
    % \includegraphics{}
    \includegraphics[width=0.44\textwidth]{figures/LP_NM_diagram.pdf}
    \caption{\grood motivation diagram. Class
    9 from CIFAR10 taken as ID and the TinyImageNet dataset as OOD. Two
    classifiers, LP and NM, produce a 2D space to which each sample is mapped to
    (green or red dots for ID and OOD respectively). Top/right axes: the marginal
    empirical distributions. The ID data are modelled
    as a bi-variate Normal distribution (green iso-lines). A "general" OOD distribution is constructed as another Normal distribution (gray
    iso-lines). Three
    possible decision strategies for different expected ID false rejection
    rates in the N-P task are plotted in black dashed lines with corresponding rejection rates
    marked. Note: The proposed methods do not have access to the OOD
    data, they are shown only to strengthen reader's intuition.
    }
    \label{fig:LP_NM}
\end{figure}

In this section we describe the proposed Generic
Representation based OOD detection approach, \grood in short, which exploits a representation
pre-trained on auxiliary large-scale non-OOD-related data. The intuition behind
the method is  that a generic  representation is
a good starting point for the OOD detection. The method also produces
well-calibrated classification scores for a given ID task corresponding
directly to the same false negative rate for {\it every} ID class.

We expect the representation to be strong, allowing in- and out-class data
separation by a low-complexity classifier.
In particular, we investigate two such classifiers, Linear Probe (LP) and
Nearest Mean (NM), trained on ID data only. The LP classifier consists of
a single linear projection layer followed by a softmax normalization (\ie
multi-class logistic regression model). This type of classifier has been used
in representation learning to test the expressiveness of
a representation~\cite{Chen2020}. The NM classifier assigns data to
the class with the nearest class mean as measured by the $L_2$ distance;
learning this classifier consists of computation of mean vector representation
for each class. We chose the LP and NM classifiers because of (i) their
simplicity -- simple classifiers generalize well, do not overfit to ID problem
-- and (ii) complementarity - one is based on a discriminative score and the
other on a distance metric - as illustrated in
Fig~\ref{fig:lp_ncm_complementarity}. At the test time, when OOD data points
are detected, the LP and MN classifier responses are simply thresholded (like
in~\cite{Vaze2022}) and this threshold is varied to compute the ROC curves in
the experiments.

We show in Sec~\ref{sec:experiments} that already with this setup one is
already able to achieve state-of-the-art OOD results. Although each of these
classifiers performs already better then state-of-the-art methods on most
benchmarks, we show in Sec~\ref{sec:complementarity} that they are in fact
complementary, each working better on different type of problems.
Further, as shown in Sec~\ref{fig:mls_miscalibration}, their logit/distance
scores are not well calibrated, \ie when setting an in-out decision threshold,
the ID classes are rejected unevenly, some producing higher false negative
rates then the others.

To solve these issues, we propose a new method, called \grood, which combines
the outputs of the two classifiers. The distribution of the outputs is modelled
as a bi-variate Guassian which permits addressing OOD as a formally defined
two-class Neyman-Pearson task~\cite{Schlesinger2002, Neyman1928, Neyman1933}
through which calibration of the OOD detector is achieved.\footnote{We
experiment with 2D space of scores only, as the amount of data for model
estimation is limited (100 or less examples in some cases).}

We illustrate the approach on an example OOD problem shown in
Fig~\ref{fig:LP_NM}. CIFAR10 is considered ID (class 9 shown here) and the
TinyImageNet represents an OOD dataset (see Sec~\ref{sec:experiments} for
details on datasets). The figure shows in green the ID and in red the OOD
distribution of LP scores (top) and NM similarity (right), see Sec~\ref{sec:LPNM} for the definitions. The
data are plotted as green (ID) and red (OOD) dots. The ID distribution is
specified by the desired ID classification problem, the OOD distribution may
vary depending on particular ID/OOD benchmark. 
Notice, that shifting the problem from a one-dimensional score (either LP or
NM) to a two-dimensional space allows us to leverage the best of LP and NM (cf
Fig~\ref{fig:lp_ncm_complementarity}) and leads to a better ID/OOD separation
when considering jointly {\it all} tested OOD problems.

In the proposed \grood method we model the ID distribution as a Normal
distribution. Although an approximation, we observed empirically that it holds
reasonably well over a wide range of tasks\footnote{A breaking point would be
the case of ID data where one class consists of multiple clusters. In this
case, the NM classifier would need to be modified to consider several
"means".}. Of course, nothing prevents us from using a more complex model of
the distribution, \eg the non-parametric Parzen estimate, if needed, but the
Normal distribution assumption makes the next step in designing \grood
significantly easier.
Next we formulate the ID/OOD classification problem as a multi-class
Neyman-Pearson task~\cite{Schlesinger2002, Neyman1928, Neyman1933}. We start by
considering a single ID class. Let $\mathcal{I}$ be a class representing the ID
samples and $\mathcal{O}$ the class for OOD data. Assume the data are sampled
from a two-dimension domain $\mathcal{X} = \mathcal{X}_{LP} \times
\mathcal{X}_{NM}$, where $\mathcal{X}_{LP}$ is the domain of LP logit scores
and $\mathcal{X}_{NM}$ the domain of NM distances. The task is then to find
a strategy $q^*(x): \mathcal{X} \rightarrow \{\mathcal{I}, \mathcal{O}\}$ such
that \begin{equation} \begin{aligned} q^* = \argmin_q & \int_{x: q(x) \neq
\mathcal{O}} p(x|\mathcal{O})\,dx \\ \textrm{s.t.} \quad & \epsilon_\mathcal{I}
= \int_{x: q(x) \neq \mathcal{I}} p(x|\mathcal{I})\,dx \leq \epsilon
\end{aligned} \end{equation} This optimization problem minimises the false ID
acceptance rate for a particular ID class and bounds the maximal ID rejection
rate by~$\epsilon$. For $K$ classes we specify $K$ such problems and use the
same constant $\epsilon$ for all of them, so that the same fixed rejection rate
is required for all classes.

It is known~(\cite{Schlesinger2002}) that the optimal strategy for a given
$x\in\mathcal{X}$ is constructed using a the likelihood ratio $r(x)
= p(x|\mathcal{I}) / p(x|\mathcal{O})$ so that $q(x) = \mathcal{I}$ if $r(x)
> \mu$ and $q(x) = \mathcal{O}$ if $r(x) \leq \mu$. The optimal strategy $q^*$
is obtained by selecting the minimal threshold $\mu$ such that
$\epsilon_\mathcal{I} \leq \epsilon$. The problem is solved either analytically
for some simple distributions (such as Gaussian) or numerically otherwise.

To solve this problem we still have to specify the $p(x|\mathcal{O})$
distribution. If we assume this distribution to be uniform in $\mathcal{X}$, we
would decide based on the quantiles of the Normal distribution
$p(x|\mathcal{I})$. However, we constructed $\mathcal{X}$ not from general 1D
random variables, but from the classification scores of LP and NM. It is thus
reasonable to assume that the OOD data will lie in the region where the LP score and NM similarity are low.

To implement this assumption, we construct $p(x|\mathcal{O})$ as another Normal
distribution with a zero mean and a diagonal covariance matrix with large
variances. For the LP, the zero mean assumption is motivated by the
fact that in high dimensional spaces, a random vector is likely to be close to
orthogonal to the id-class directional vectors (the weights of the linear layer
before the softmax). For the MN similarity, the choice was made empirically as 
a limit case for very large L2 distance from a class mean center.
The variance, in both directions, is set so that the range of the data is
a multiple of the in-distribution range, i.e. it is very broad. The in-distribution 
range was robustly estimated as a $90\%$ quantile of all in-distribution data scores 
and the multiplicative factor was set empirically. Both Normal distributions for ID and OOD are plotted
in Fig~\ref{fig:LP_NM} in green and gray solid line contours respectively.
Fig~\ref{fig:LP_NM} shows also three optimal strategies (if the assumptions
about normality were true) and their corresponding ID
rejection rates as gray dashed decision boundaries. Clearly, the strategy
rejects the least confident ID samples first.

Solving this Neyman-Pearson problem for each ID class gives $K$ strategies
$q^*_k$, each calibrated for the same rejection rate. In practice, we would
specify acceptable rejection rate $\epsilon$ and obtain the optimal strategies
for normally distributed data. For the evaluation where we need to sweep over
the values of $\epsilon$, we sample a limited set of values, find their
likelihood ratio $\mu$ and interpolate in-between.

Finally, for the ID classification we use the $\argmax_{k} p_k(x|\mathcal{I})$ of the 
probabilities obtained as bi-variate Normal distributions for each class $k$.
