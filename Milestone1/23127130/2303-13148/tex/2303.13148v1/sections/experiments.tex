\section{Experiments}\label{sec:experiments}

In this section we evaluate the proposed \grood method and other
state-of-the-art methods on a wide and diverse set of benchmark problems
described in the OOD literature. We select the benchmarks to cover various
scenarios and to demonstrate the generality of the proposed approach.

\subsection{Benchmarks}

There are several commonly used benchmarks to evaluate OOD and OSR methods and
most papers typically evaluate on their subset. In our evaluation we attempt to
cover most of the commonly used variations. We categorize the experiments based
on the presence/absence of the domain shift (DS) and the semantic shift (SS).

\noindent {\bf No DS, only SS.} For MNIST~\cite{Deng2012},
SVHN~\cite{Netzer2011} and CIFAR10~\cite{Krizhevsky2009} datasets we perform
the 6-vs-4 split~\cite{Chen2021, yangopenood, Vaze2022}. Here six classes are selected as ID at random and the
remaining four as OOD. The experiment is repeated five times with different
splits and the average metrics are reported together with their standard
deviations. 

For a bit larger CIFAR+10 and CIFAR+50 experiments, four classes are sampled
from CIFAR10 and are considered ID and another 10 (or 50) non-overlapping
classes are randomly selected from CIFAR100~\cite{Krizhevsky2009} and used as
OOD~\cite{Chen2021, yangopenood, Vaze2022}. Again, five trials are averaged. For the biggest TIN-20 experiment, twenty
classes are selected randomly as ID and 180 as OOD from the TinyImageNet
dataset~\cite{Torralba2008}. For the above experiments we use the same splits
as in~\cite{Chen2021} for compatibility with previous results.

Finally, to test this type of settings to its limits, we evaluate on the
fine-grained class splits from the Semantic Shift Benchmark~\cite{Vaze2022}.
Here three splits are given: easy, medium and hard, with increasing semantic
shift overlap with ID classes. This overlap is determined from a set of
detailed class attributes. We use the splits for the CUB~\cite{Wah2011}
(birds), StanfordCars~\cite{Krause2013}, and FGVC-Aircraft
datasets~\cite{Maji2013}.

\noindent {\bf DS and SS mixed.} Another common experimental setting is to
consider CIFAR10 as ID and use other datasets as OOD~\cite{liu2020energy, Chen2021, sun2022out, Vaze2022}. In this case there is an
explicit SS and an implicit DS. We evaluate against MNIST~\cite{Deng2012},
SVHN~\cite{Netzer2011}, Textures~\cite{Cimpoi2014}, Places365~\cite{Zhou2017},
CIFAR100~\cite{Krizhevsky2009}, iNaturalist~\cite{Van2018},
TinyImageNet~\cite{Torralba2008} and LSUN datasets~\cite{Xiao2010}.

\noindent {\bf DS only}. Special kind of shift is when the classes stay the
same, but the image domain changes. For this experiment we adopt the benchmark
from~\cite{Chen2021} based on DomainNet dataset~\cite{Peng2019}. The challenge
is to distinguish between photos of objects from 173 classes (ID) and
clipart/quickdraw images from the same classes (OOD). The benchmark also
contains an OOD part with real images from different 173 classes (SS task).

\subsection{Evaluation Metrics}
\label{sec:metrics}

There seems to be no consensus, which metrics to report for OOD detection. The
most commonly used is the AUROC metric, which measures the ability to
distinguish OOD data from ID data. Often this is the only metric reported even
though it does not show, how well the method perform on the ID classification
task. For ID classification people report either the ID accuracy, FPR95 or OSCR
score. In the tables in Sec~\ref{sec:experiments} we report the most commonly
used metric for particular OOD problem.

Assuming a binary ID vs OOD classification problem, the {\bf AUROC} measures
the area under the true positive (TP) -- false positive (FP) rates curve, where
the ID data is considered be the positive class. We adopt the evaluation code
from~\cite{Vaze2022, Chen2021}.

The {\bf FPR at $95\%$ TPR (FPR95)} metric measures the false positive rate at
$95\%$ true positive rate on the same binary problem as the AUROC measure.

The Open-Set Classification Rate {\bf (OSCR)}~\cite{Dhamija2018oscr, Chen2021}
measures the trade-off between the ID classification accuracy and OOD detection
accuracy. It is computed as area under
$\mathrm{CCR}(\theta)$-$\mathrm{FPR}(\theta)$ curve where
$\mathrm{CCR}(\theta)$ is the correct classification rate defined as
\begin{equation}
    \mathrm{CCR}(\theta) =
    \frac{|\{x\in \mathcal{T}_k | \argmax_j p(j|x) = k \land p(k|x) \ge \theta\}|}{|\mathcal{T}_k|}\,,
    \end{equation}
    where $\mathcal{T}_k$ is the sub-set of the ID training data belonging to the class $k$, and $\mathrm{FPR}(\theta)$ is the false positive rate defined as
\begin{equation}
    \mathrm{FPR}(\theta) = \frac{|\{x \in \mathcal{U} | \max_k p(k|x) \ge \theta \}|}{|\mathcal{U}|}\,,
\end{equation}
where $\mathcal{U}$ is the set of OOD data available at the test time.

Finally, the {\bf ACC} measures the accuracy on the ID classification problem.

\subsection{Low-Complexity Classifiers}
\label{sec:LPNM}

As we argue for a good and general enough representation as the basis for the
OOD detection, we use only simple low-complexity classifiers (\ie letting the
representation play the essential part in the decision). In particular, we use
the Linear Probe (LP) and the Nearest Mean (NM) classifiers. The LP classifier is trained on the ID data only. We use the training code
from~\cite{radford2021learning}. As an OOD detection score we use the Maximum
logit~\cite{Vaze2022}. The NM classifier's means are also estimated on the ID data only. The NM 
similarity is computed from the NM $L_2$ distance $d_{NM}$ as $1 / (1 + d_{NM})$.

\subsection{Power of a Good Representation}

\input{tables/repr_comparison}

We start by investigating the effect of different pre-trained representations
on various OOD problems. We consider two rich representations: one trained with
full supervision on the ImageNet1k classification task and the CLIP
representation~\cite{radford2021learning} trained using a self-supervised
contrastive objective on a large dataset of image-text description pairs.

The ImageNet pre-trained representation proved to be a strong baseline for many
problems in computer vision. We use the ViT-L/16 model pre-trained on
ImageNet1k available in the PyTorch Torchvision library and use its penultimate
layer as a feature extractor. It produces 1048-dimensional feature vectors.

The CLIP representation has shown outstanding performance on various zero-shot
classification problems~\cite{radford2021learning} demonstrating its
versatility. From the point of view of OOD detection, what makes the
representation appealing is that it was trained on image-text pairs instead of
a fixed set of classes. This, together with the self-supervised training
possibly allows the model to extract very rich representation of the visual
world. This makes it a good candidate for separating ID classes from OOD date
irrespective of the type of semantic and distribution shift if these shifts are
covered by natural language and represented sufficiently by the training data.
The CLIP model (we are using only the image encoder) produces 768-dimensional
feature vectors.

We have also considered smaller ImageNet and CLIP models, but they perform consistently worse, please refer to  supplementary materials for smaller models results.

For both representations,  we train  LP and NM classifiers and test over
a range of tasks. We obtain consistent relative performance over different OOD
tasks hence we report only the average metrics. We refer to supplementary
materials for full results.  Table~\ref{tab:repr_comparison} reports the
average performance on the studied benchmarks.  Our results show that the CLIP
representation works better irrespective of the classifier and the type of OOD
shift. Hence, we rely on this representation in the following experiments.

\subsection{Complementarity of LP and NM}
\label{sec:complementarity}

Table~\ref{tab:repr_comparison} shows that the LP and NM classifiers are
complementary, each performing well on different types of dataset. It is
further illustrated in Figure~\ref{fig:lp_ncm_complementarity}. The plots
show LP and NM scores distributions for different ID and OOD datasets.

We observed that for OOD tasks where ID and OOD classes are from the same
domain (\eg 6-vs-4 experiment on MNIST) and are thus close to each other in the
considered representation, LP tends to work better by finding a suitable linear
projection where the ID classes can be well separated whereas the NM classifier
struggles distinguishing small distances in the high-dimensional representation
(Fig~\ref{fig:lp_ncm_complementarity}a). When the ID and OOD classes are from
rather distant domains (\eg CIFAR10 and Places365), the NM method works better
as the $L_2$ distance starts to be discriminative
(Fig~\ref{fig:lp_ncm_complementarity}b). And there are some problems (\eg
CIFAR10 vs SVHN) where both classifiers produce similarly good separation
between ID and OOD classes (Fig~\ref{fig:lp_ncm_complementarity}c).

These observations motivated the development of the combined \grood method
described in Sec~\ref{sec:method}.

\subsection{Mis-calibration of the Logit Scores}
\label{sec:miscalibration}

Another issue revealed in our experiments is mis-calibration of the maximum
logit (or probability) approaches~\cite{Vaze2022,hendrycks2016baseline,liang2017enhancing,hendrycks2019anomalyseg}. We demonstrate this in
Fig~\ref{fig:mls_miscalibration}. When a logit score threshold is
selected (10 in the figure), it produces different false negative
rates for each class. This is in contrast with \grood method, where the
threshold is imposed directly on the class false negative (FN) rate. This
allows to specify an allowed FN rate while minimizing the FP rate (\ie the
number of OOD data classified as ID). This quality is important in
safe-critical applications where certain classes are reported as OOD more often
or in social-related applications where having uneven FN rates on ID classes
may lead to unwanted biases.

\subsection{\grood vs State-of-the-Art}

\input{tables/osr_results}
\input{tables/ood_results}
\input{tables/clipart_results}
\input{tables/semantic_shift_results}

Finally, we compare \grood with state-of-the-art methods on an extensive range
of benchmarks. See the results in
Tab~\ref{tab:osr_results}-\ref{tab:semantic_results}. In all the tables we
compare against a selection of best performing methods collected from
literature and indicate the respective source publication. For comparison with
many other methods see the benchmark papers~\cite{yangopenood,
yang2021generalized, Vaze2022}.

Tab~\ref{tab:osr_results} and Tab~\ref{tab:ood_results} summarize the most
common benchmarks used in literature, the first one with the semantic shift
only and the other with mixed semantic and domain shifts. Once again, the
complementarity of LP and NM approaches is clearly visible. The \grood method
performs similarly to the better of the two on individual problems and its
performance is consistent over different types of OOD detection tasks;
on average it is better.
Note that {\it not outperforming both constituent methods on some particular problems is not a weakness of the method}.
In classifier combination, a guarantee of being as good as the better (or best) of a set of classifiers requires an oracle; being outperformed on some particular problem could mean that the simple classifier is overfitted to the problem or its assumptions match the problem.
Compared to state-of-the-art methods, \grood outperforms all of them by a large
margin on most of the OOD problems. Especially on the mixed semantic and domain
shift problems in Tab~\ref{tab:ood_results}, our approach basically solves all
the benchmarks.

The proposed method is the most effective on more complex problems like CIFAR
variants and TIN and struggles a bit on the 6-vs-4 SVHN problem in
Tab~\ref{tab:osr_results}. We attribute this mainly to the dataset ground truth
construction. The images do not contain the single digit stated in GT label but
also ``some distracting digits to the sides of the digit of
interest''\footnote{\url{http://ufldl.stanford.edu/housenumbers}}). For CLIP
which was trained on many images containing text (with correct label) all
digits in the image influence the representation. The performance drop does not
happen for MNIST dataset with a single digit per-image, which supports our
analysis. Since method that train the representation on ID data does not suffer
from this phenomena, these issues can be potentially alleviated by fine-tuning
the representation or using more complex classifiers.


\grood is also very efficient on the problems with domain shift only as shown
in Tab~\ref{tab:clipart_results}. Here our method again outperforms the current
state-of-the-art significantly, showing the ability to distinguish data even
along such distribution shifts like real-image vs clipart vs quick draw.
There is still a space for improvement on the Clipart-A split which is very
similar to the ID Real-A dataset (same classes, photos vs complex clipart).
This is though difficult even for ARPL+CS which is trained on the ID data.

Finally, to test the limits of the proposed method we evaluated \grood on the
Semantic Shift Benchmark problems with very fine-grained semantic shift in
Tab~\ref{tab:semantic_results}. Although the class separation is often very
subtle, our method performs comparably to state-of-the-art. 
The Easy/Hard splits in SCars and FGVC-Aircrafts datasets (in contrast
to CUB) are not based on strictly visual attributes, but on attributes like
year of production or aircraft variant. They do not seem to correspond to
differences captured by the CLIP representation. This is more pronounced in
case of airplanes where, e.g., the overall shape do not change between
production years (as oppose to cars). We see this as a border case and
a weakness of the benchmark\footnote{``...open-set bins of different difficulties in
Stanford Cars are the most troublesome to define. This is because the rough
hierarchy in the class names may not always correspond to the degree of visual
similarity between the classes''\cite{Vaze2022}.} and a possible future direction.


Overall, the experiments demonstrate how using a powerful representation leads
not only to a state-of-the-art ID classification as demonstrated
earlier~\cite{radford2021learning}, but provides classifier with very
strong cues for OOD detection as well.
