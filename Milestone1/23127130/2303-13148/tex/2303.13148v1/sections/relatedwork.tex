\section{Related Work}\label{sec:relatedwork}
Out-of-distribution (OOD) detection refers to the identification of test
samples  that are drawn from a different distribution than the underlying
training distribution of  a given classification model. Hendrycks et
al.~\cite{hendrycks2016baseline} was one of the first to explore this problem
with modern neural networks using maximum softmax probability (MSP) obtained
from a classification model as a detection score. While being an classical
baseline in OOD detection, MSP can output high ID probabilities for unknown OOD
samples~\cite{sun2021react}. Subsequent work has attempted to provide more
robust OOD detection by either operating on a fixed model, or performing
additional ID training or  even leveraging auxiliary  OOD data. We refer
to~\cite{yang2021generalized} for a complete survey on the different lines of
OOD detection research.

\textit{Post-hoc methods} usually define different OOD detection scores or
perform manipulation of the input samples to increase the separability between
the distributions of ID and OOD
scores~\cite{liang2017enhancing,hsu2020generalized}. As a more robust
alternative to the MSP scores, ~\cite{liu2020energy} proposed to use the energy
of the output logits as a scoring function showing strong improvements over MSP
and more separable scores. Recently, \cite{hendrycks2019scaling} showed that
using maximum logit as an OOD detection score is significantly more roust than
MSP, suggesting that the normalization of the probability by the closed set
classes is the source for the overconfident predictions.

Of the distance based detection scores, we mention~\cite{lee2018simple} which
estimates the Mahalanobis  distance to the closest class. Based on the
estimated $L_2$ distances in the learned embedding space, ~\cite{sun2022out}
propose instead to use the K-nearest neighbour (KNN) distance as detection score.
KNN~\cite{sun2022out} improves significantly over the Mahalanobis
distance~\cite{lee2018simple}. Manipulating the logits of a pre-trained ID
classifier has its limits, which led to the second group of
approaches. 


\textit{Training based methods} target a stronger OOD detection performance
through regularizing the training such that the resulting classifier or
representations behave differently for ID compared to OOD inputs. Tackling the
same overconfident issue as in post-hoc methods,~\cite{wei2022mitigating}
proposed to train the ID classification model while enforcing a constant logit
norm. Deep ensemble~\cite{lakshminarayanan2017simple} combines adversarial
training with neural networks ensemble in addition to using the loss function
as a scoring rule. The computational cost of such approach might be prohibitive
for big networks. 

Other work aims at regularizing the training with virtual representatives of
OOD input. CSI~\cite{tack2020csi} utilizes contrastive training and apply
strong augmentations to the input images as an alternative to OOD data.
Adversarial Reciprocal Points Learning (ARPL)~\cite{Chen2021} proposes the
concept of "reciprocal" points as a proxy for OOD samples which are obtained by
combined discriminative and metric learning. This method showed state-of-the-art OOD detection performance, however, it requires complex training scheme and
large hyper-parameter tuning.  The proposed method is significantly more
efficient and improve over ARPL with a large margin in multiple benchmarks.

``A closed set classifier is all you need''~\cite{Vaze2022} is a recent work
suggesting that an improved training scheme that leads to better performance on
ID data discrimination offers very competitive  OOD detection quality that
rivals that of OOD regularized training such as ARPL~\cite{Chen2021}. More
recently,~\cite{yangopenood} has evaluated a wide range of OOD detection
methods and their empirical results suggest that strong input augmentation
techniques, e.g. MixUp~\cite{thulasidasan2019mixup},
CutMix~\cite{yun2019cutmix} and PixMix~\cite{hendrycks2022pixmix} are the most
effective type of training methods for OOD detection. 

The observations of ~\cite{Vaze2022} and~\cite{yangopenood} suggest that
obtaining a more robust \textit{closed ID} classification model either by
sophisticated training techniques or strong input processing is a key to robust
OOD detection. In this work we show that   what matters the most  is
\textit{not} the classification model trained specifically for ID data but
rather a strong underlying feature extractor. We show that a generic
representation such as CLIP~\cite{radford2021learning} can serve as a basis for
an extremely powerful OOD detector on a wide variety of benchmarks.

Pretrained representations have been shown to significantly boost the classification performance  in the context of few shot learning~\cite{tian2020rethinking} and continual learning~\cite{ostapenko2022continual}, but are very rarely explored in the OOD detection literature. 
Hendrycks et al.~\cite{hendrycks2019usingself} investigate the role of  self-supervised pre-training on the \emph{ ID task data only}. In \cite{hendrycks2019usingpre} Hendrycks et al., ImageNet pre-trained models are deployed. However, the pre-training was only leveraged  as an initialization  where the model is later finetuned on the specific ID task  before the OOD detection is conducted.

We are the first, to the best of our knowledge, to show that generic, self-supervised trained representation can be directly leveraged for OOD detection on various ID tasks without any finetuning of the representation on those tasks. In other words, we suggest that one does not need to learn and capture the specificity of a given ID task (as opposed to e.g.~\cite{Vaze2022}) but rather only a good metric space where similar (even out-of-distribution) samples lie nearby is sufficient.

