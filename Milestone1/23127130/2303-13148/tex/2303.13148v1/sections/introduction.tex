\section{Introduction}

The problem of detection of out-of-distribution data points, OOD in short, is
important in many computer vision
applications~\cite{Bergmann2021,Kuo2019,chan2021_bench}. One can even argue
that no model obtained by machine learning on a training set $\mathcal{T}$
should be deployed  without the  OOD ability, since in practice it is almost
never the case that all the data the model will make predictions on will be
drawn from the same distribution that generated~$\mathcal{T}$~\cite{Zhao2021}.
For undetected out-of-distribution data, the prediction will in general be
arbitrary, with possibly grave real-world consequences, especially in
safety-critical applications. The importance and ubiquity of OOD is evidenced
by the fact that virtually the same problem has emerged in different contexts
under different names - open set recognition, anomaly or outlier detection, and
one-class classification. 

The reasons for test data not being from the training set distribution are
diverse; they often influence the terminology used. In open set
recognition (OSR)~\cite{Mahdavi2021,yang2021generalized}, the semantic shift is
considered, i.e. the introduction of new classes at test time. Failures of the
measurements system generate outlier data. In anomaly detection, the presence
of out-of-distribution data is assumed rare. A domain shift, e.g. when
a classifier trained on real-world images is applied to clip art, leads to
a severe data distribution change.

So far, prior art has mainly developed OOD detection models by supervised
training on in-distribution (ID)
data~\cite{yang2021generalized,Bogdoll2022,Bitterwolf2022}. We follow the recent success of 
self-supervised representation  model training and
we are the first to investigate and apply it to out-of-distribution detection.
This novel approach significantly improves OOD performance; experiments confirm
the superiority of the generic representation over problem-specific approaches
that train or fine-tune the feature extractor on a particular in-distribution
(ID) training set. 

Any good representation should enable solving a given, a priori unknown,
downstream task. A good {\it generic} representation should enable solving
multiple tasks without the need of fine-tuning on the task data. To verify the
goodness and generality of tested representations, we first propose to use two
simple classifiers: (i) linear probe (LP), and (ii) the nearest mean (NM)
classifier which represents in-distribution classes by their mean. This already
outperforms the state-of-the-art on a broad range of problems, often by a large
margin. In fact, for many benchmarks, perfect or near perfect performance
($>$99\%) was reached.

This approach does not require any information about the out-of-distribution data,
e.g. in the form of a few examples of the anomalies or outliers,  and is
thus applicable to all the standard setting of the OOD and OSR
problems~{\cite{yang2021generalized}}.

Since the LP and NM methods perform each well on different classes of the OOD
problems, we formulate a Neyman-Pearson task~{\cite{Schlesinger2002,
Neyman1928, Neyman1933}} on their combination. We call this approach \grood
(for Generic Representation based OOD detection). It models the in-distribution
as a 2D Gaussian in the space of LP and NM responses, and provides a robust
solution to the OOD problem. To summarize, the contributions of the paper are:

\begin{itemize} 

\item We show that using a generic pre-trained representation together with
    a simple classifier achieves state-of-the-art performance on a number of
        OOD benchmarks.

\item We formulate the OOD detection as a Neyman-Pearson task in the space of LP and NM
    scores. The operating point is selected by the allowed false negative rate
        for {\it all} in-classes. This results in a well calibrated
        classification score on the ID task.

\item The proposed method outperforms the state-of-the-art by a large mangin on
most of OOD problems and even saturates several commonly used benchmarks.

\end{itemize}
