
%\documentclass[correspondence]{IEEEtaes}
% \documentclass[landscape,onecolumn,draftclsnofoot,12pt]{IEEEtran}
\documentclass{IEEEtran}
%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%\journal{XXX}

\usepackage{balance}
\usepackage{amsmath,graphicx}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{cite}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\usepackage{cuted}
\usepackage{array,multirow}
\usepackage{stfloats}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{algorithm,algorithmic} %
\usepackage{multicol}


\newcommand{\revisNew}{\textcolor{black}}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\ist}{\hspace*{.4mm}}
\newcommand{\rmv}{\hspace*{-.4mm}}

\begin{document}

%
%\sptitle{Correspondence}
%\title{Heterogeneous Unlabeled and Labeled RFS Filter Fusion for Scalable Multisensor Multitarget Tracking}
%
%\author{Tiancheng Li}
%\member{Senior Member, IEEE}
%\affil{Northwestern Polytechnical University, Xi'an 710129, China}
%
%\author{Ruibo Yan}
%\affil{Northwestern Polytechnical University, Xi'an 710129, China}
%
%\author{Kai Da}
%\affil{National University of Defense Technology, Changsha 410073, China}
%
%\author{Hongqi Fan}
%\affil{National University of Defense Technology, Changsha 410073, China}
%
%
%\receiveddate{Manuscript received XXXXX 00, 0000; revised XXXXX 00, 0000; accepted XXXXX 00, 0000.\\
%Preprint is available at https://arxiv.org/abs/2209.10433. This work was partially supported %by Key Laboratory Foundation of National Defence Technology under Grant 61424010306, %by Space Science and Technology
% by National Natural Science Foundation of China (Grant No. 62071389), Natural Science Basic Research Program of Shaanxi Province (Program No. 2023JC-XJ-22), %JWKJW Foundation (2021-JCJQ-JJ-0897), %, 2020-JCJQ-ZD-150-12),
% and Key Laboratory Foundation of National Defence Technology (No. JKWATR-210504) }
%%% \accepteddate{XXXXX XX XXXX}
%%% \publisheddate{XXXXX XX XXXX}
%
%\corresp{{\itshape (Corresponding author: T. Li)}.}
%
%\authoraddress{T.\ Li and R. Yan are with the Key Laboratory of Information Fusion Technology (Ministry of Education), School of Automation, Northwestern Polytechnical University, Xi'an 710129, China, e-mail: t.c.li@nwpu.edu.cn, 2018300490@mail.nwpu.edu.cn. K. Da and H. Fan are with the Key Laboratory of Science and Technology on ATR, National University of Defense Technology, Changsha 410073, Hunan, China. e-mail: dktm131@163.com, fanhongqi@nudt.edu.cn}
%%
%%\editor{Mentions of supplemental materials and animal/human rights statements can be included here.}
%%\supplementary{Color versions of one or more of the figures in this article are available online at {http://ieeexplore.ieee.org}.}
%
%\markboth{CORRESPONDENCE}{}


\title{Arithmetic Average Density Fusion - Part III: Heterogeneous Unlabeled and Labeled RFS Filter Fusion %based on PHD-AA
%for Scalable Multisensor Multitarget Tracking
%\\Fusion %based on PHD-AA
%\hspace{-0.3mm}for \hspace{-0.3mm}Scalable \hspace{-0.3mm}Multisensor \hspace{-0.3mm}Multitarget \hspace{-0.3mm}Tracking
}

\author{Tiancheng~Li,~\IEEEmembership{Senior Member,~IEEE}, Ruibo Yan, Kai Da and Hongqi Fan%%
%\thanks{Manuscript preprint: arxiv.org/abs/2303.09401} %preprint is available at https://arxiv.org/abs/2303.09401. Correction has been made. Simulation study has been completed.}
\thanks{%Manuscript preprint: arxiv.org/abs/2303.09401. 
This work was partially supported %by Key Laboratory Foundation of National Defence Technology under Grant 61424010306, %by Space Science and Technology
 by National Natural Science Foundation of China (Grant No. 62071389), Natural Science Basic Research Program of Shaanxi Province (Program No. 2023JC-XJ-22), %JWKJW Foundation (2021-JCJQ-JJ-0897), %, 2020-JCJQ-ZD-150-12),
 and Key Laboratory Foundation of National Defence Technology (No. JKWATR-210504). % and the Fundamental Research Funds for the Central Universities
}
\thanks{T.\ Li and R. Yan are with the Key Laboratory of Information Fusion Technology (Ministry of Education), School of Automation, Northwestern Polytechnical University, Xi'an 710129, China, e-mail: t.c.li@nwpu.edu.cn, 2018300490@mail.nwpu.edu.cn%
}
\thanks{K. Da and H. Fan are with the Key Laboratory of Science and Technology on ATR, National University of Defense Technology, Changsha 410073, China.
e-mail: dktm131@163.com, fanhongqi@nudt.edu.cn}
}


\maketitle

\begin{abstract}
This paper proposes a heterogenous density fusion approach to scalable multisensor multitarget tracking %using a heterogenous network
where the inter-connected sensors run different types of random finite set (RFS) filters according to their respective capacity and need. {These diverse RFS filters} result in heterogenous multitarget densities that are to be fused with each other in a proper means for more robust and accurate detection and localization of the targets.
%Our recent work %(Part II of the series of work)
%has exposed a key
%common property of effective arithmetic average (AA) fusion approaches to both unlabeled and labeled RFS filters which are all built on averaging their relevant unlabeled/labeled probability hypothesis densities (PHDs). %, namely the first order moments of the locally estimated multitarget probability densities. %However, existing approaches address only the fusion of homogeneous filters. %An improved PHD estimate will surely improve the filter estimate.
%Thanks to this, this
%%This
%paper proposes the first ever heterogenous unlabeled and labeled RFS filter cooperation approach
Our approach is based on Gaussian mixture implementations where the local Gaussian components (L-GCs) are {revised for PHD consensus, i.e., the corresponding unlabeled probability hypothesis densities (PHDs) of each filter best fit their average} regardless of the specific type of the local densities. To this end, a computationally efficient, \revisNew{coordinate descent} approach is proposed which only revises the weights of the L-GCs, keeping the other parameters %of the L-GCs
unchanged. % for approximating unlabeled PHD-AA fusion.
%To demonstrate this,
In particular, the PHD filter, the unlabeled and labeled multi-Bernoulli (MB/LMB) filters are considered. % in particular. %, which partially enables parallelization of the communication and the filtering calculation.
Simulations have demonstrated the effectiveness of the proposed approach for both homogeneous and heterogenous fusion of the PHD-MB-LMB filters in different configurations. %In addition, we show negative results of the heterogenous fusion of the unlabeled/labeled MB filter with the PHD filter. % based on the proposed PHD-AA fusion approach. % which ensures consistency in the (labeled) PHD estimation.
\end{abstract}

% Note that keywords are not normally used for peer-review papers.
\begin{IEEEkeywords}
Random finite set, arithmetic average fusion, heterogenous fusion, PHD consistency, multitarget tracking
\end{IEEEkeywords}


%\end{frontmatter}

%% \linenumbers


\section{Introduction}

With the proliferation of internet of things \cite{Qiu18HeteroIoT}, heterogenous sensor networks %\cite{} %Alam17,
become {promising for multisensor multitarget (MSMT) tracking} where the sensors have unequal computing and memory capacities {and/or different measurement models} and correspondingly run their own suitable algorithms.
In fact, it is {often required to operate} different algorithms even in a homogeneous sensor network, to the prejudice of the network reliability and lifetime \cite{Yarvis05hetero, Liggins17}. In either case, the local filters are allowed to apply different statistical models (regarding the target birth, detection, survival, and death and the clutter) and {approximation} algorithms. Thanks to the diversity, the combination {of heterogenous filters} is naturally more robust and reliable as compared with the unitary one.
In this paper, we consider such a {heterogenous MSMT tracking problem} where (whether homogenous or heterogenous) netted sensors run different types of multitarget filters/trackers {derived by using different families of random finite sets (RFSs)} such as unlabeled and labeled RFS filters \cite{Mahler14book,Vo15mtt}. {Our goal is to find a technically solid means to cooperate these cooperative filters via inter-sensor information fusion for improving their respective, heterogenous estimation.} %This leads to a significant challenge for inter-sensor multitarget information fusion since
%As a result,
%the locally obtained multitarget probability densities are heterogenous across the sensors. Yet, it is of significance to fuse them in a proper means which does not demolish the local filters. %can not be done as in any exiting works.

One solution to the above heterogenous filter cooperation could be given by inter-sensor sharing the raw measurements so that the heterogenous filters just use the aggregated measurements of different sensors. This {is particularly useful for dealing with the lack of observability \cite{Dimitri21,Xie18} which, however, }%needs an inter-node communication protocol like flooding \cite{Li17flooding} which is often communication costly and
does not suit the large-scale peer-to-peer networks. %, namely unscalable with the number of sensors.
More importantly, it is computationally intractable %for the RFS filters
to make the optimal use of the measurements of all sensors due to the explosive possibility for track-to-measurement association \cite{Mahler14book,Delande11MsPHD,Nannuru16MsCPHD,Wei16centLMB,Saucan17MsMB,Vo19msGLMB,Si20msPMBM,Robertson22MsLMB,Trezza22Msbirth,Moratuwage22MSMoT}.
%{In particular, a key design requirement for wireless sensor network applications is the restrictive on-board inter-node communication energy. Long range data transmission should be avoided to conserve energy. Instead, transmission is limited to among the closely distributed, neighbor nodes for which the distributed network is preferred.}
Differently, we resort to the computationally efficient and scalable density average consensus approach \cite{Li22chapter} to fuse the probability hypothesis density (PHD) filter \cite{Vo05,Vo06}, the unlabeled multiple Bernoulli (MB) filter \cite{Vo09CBmember} and the labeled MB (LMB) filter \cite{Vo13Label,Reuter14LMB}. % which remains a significant challenge. %can hardly be satisfied by existing information fusion algorithms.
{It requires only local data communications and simple fusion computation and is proven resilient to complicated/unknown inter-sensor correlation \cite{Bailey12,Li20AAmb}.}
According to the best of our knowledge, all existing density fusion approaches \cite{Li22chapter,Li22RFS-AA-Derivation} are homogeneous {in the parameter family of the fusing densities}, i.e., fusion is carried out among the same type of filters. Note that the defined heterogeneous fusion is different from %that where the locally estimated distributions describe different states of interest which are subsets of the joint state
what given in
\cite{Petitti11HeterEstimates,Dagan20HeterogeneousChannel,Yi21Heterogeneous,Arulampalam21HeteBearing} where the filters to be fused are still the same type. %,Dagan21HeterogeneousChannel
%Heterogeneous tracks
%In the context of single sensor MODT, a statistically rigorous and promising approach is given by the random finite set (RFS) theory \cite{Mahler14book} based on which different statistical models %(regarding the target birth, detection, survival, and death and the clutter)
%and simplifying approximations lead to a variety of useful RFS filters \cite{Vo15mtt}. % based on the average/consensus fusion \cite{Li22chapter}.

%While there have been considerable solutions for homogenous RFS filter fusion based on either centralized optimal fusion or distributed suboptimal fusion \cite{Li22chapter,Li22RFS-AA-Derivation}, it is still open how to fuse these different types of RFS densities, such as the popular probability hypothesis density (PHD) filter, the multi-Bernoulli (MB) filter, not to say to fuse unlabeled and labeled RFS filters.  %This motivates our work in this paper.

Our proposal in this paper is based on
%An emerging approach to multisensor RFS filter is given by
the arithmetic-average (AA) density fusion \cite{Li2021SomeResults,Li22RFS-AA-Derivation}, {which has recently been tailored to accommodate different RFS densities all demonstrating significant advantages in dealing with misdetection and in high computing efficiency}. %promising performance for MSMT tracking %
%especially {with the use of} distributed sensor networks.}
Effective AA fusion implementations of either unlabeled or labeled RFS filters are all derived from the same (labeled) PHD-AA formulation which ensures consistency in the (labeled) PHD estimation \cite{Li22RFS-AA-Derivation} {and ultimately} more accurate and robust detection and localization of the present targets.
%An obvious benefit of this common PHD-AA fusion framework is that it
This does not only expose the fact that existing linear-fusion-based multi-sensor RFS filters merely seek consensus over the (labeled) PHDs rather than the multi-object probability densities (MPDs) but also paves a way for heterogenous RFS filter cooperation via averaging their respective unlabled/labeled PHDs as shown in Fig. \ref{fig:HFframework}. In view of this, this paper presents the first ever heterogenous unlabeled and labeled RFS filter cooperation approach based on %approximate unlabeled PHD-AA fusion via
the Gaussian mixture (GM) implementation. %It allows the local filter to adopt different model assumptions (regarding the target birth, persistency, detection, etc), renders sufficient flexibility for the local filter design.


%To this end, we propose a heterogenous RFS filter fusion approach where each sensor operates an arbitrary multitarget (labeled) RFS filter according to its own capacity and need which is fused with the others via linearly averaging their (unlabeled) PHDs.

%
%% \subsection{Finite Set Statistics}
%The performance of the homogeneous RFS filter fusion has been well demonstrated with regard to the PHD filter \cite{Li17merging,Li17PC,Li17PCsmc}, % ,Li19Diffusion,Yi20AAfov
%the cardinalized PHD filter \cite{Yu16,Gao20cphd,Da_Li20TSIPN}, the Bernoulli filter \cite{Li19Bernoulli}, the MB filter \cite{Li20AAmb}, the Poisson MB mixture filter \cite{Li23AApmbm}, the labeled MB filter \cite{Gostar20,Gao20GLMB}. %Open-access links to some source codes are available in papers including \cite{Li18FloodingClustering,Ramachandran21,Gao21phdAAlmb} as well as in the following URL: sites.google.com/site/tianchengli85/matlab-codes/aa-fusion. None of these earlier works, however, is on heterogenous RFS filter fusion. %We omit the details of these homogeneous RFS filter fusion approaches. %which %as to be reviewed in this paper. These previous works
%%have demonstrated the performance of the approach

In this paper, the sensors are assumed connected with each other either via a centralized fusion center or via a peer-to-peer distributed network. In the latter, the popular average consensus approach \cite{Olfati-Saber07,Sayed14book} or the distributed flooding approach \cite{Li17flooding} can be used for inter-node communication. The AA density fusion can be easily applied in both cases. {Nevertheless, this paper focuses on the centralized fusion for brevity}, omitting inter-node communication issues.
%However, its application for multisensor RFS density fusion, both centralized and distributed,
We further assume that the %sensor network is connected, i.e., each sensor can be reached from each other sensor by one or multiple communication hops. The
sensors are synchronous and accurately coordinated, whose fields of view cover the region of interest (ROI). %, in which there are a random, time-varying number of targets.
%We further omit the time notation and the dependence of the filter estimate on the observation process.
{The GM implementations of the local PHD \cite{Vo06}, MB \cite{Vo09CBmember} and LMB \cite{Reuter14LMB} filters are the standard as given in the references in which details are available}. % in order to avoid distracting from our key contribution in the fusion algorithm design.
These limitations can be relaxed to extend our proposed approach. %The cardinality of a RFS ${X}$ which indicates the number of elements is denoted with $|{{X}}|$.
%{For brevity,} the


\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=4.5cm]{Hsensor.eps}\\  % _extended
  \caption{A heterogenous RFS filter cooperation scenario based on unlabeled PHD-AA fusion} \label{fig:HFframework}
  \vspace{-2mm}
\end{figure}


The paper is organized as follows. A brief introduction to the Poisson, unlabeled and labeled MBs and their GM implementations are given in sections \ref{sec:background} and \ref{GM-implementation}, respectively. Their heterogenous fusion performed by merely revising the weights of the local Gaussian components (L-GCs) is given in section \ref{sec:GMfit}. %As such, the structure of each local filter does not need to change at all.
Simulation is given in section \ref{sec:simulation} before the paper is concluded in section \ref{sec:conclusion}.

%
%\begin{figure}
%\centering
%\centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=3.5cm]{RFSscen.eps}
%\vspace{-1mm}
%\caption{%Set inclusion relationship among Bernoulli RFS, MB RFS and MBM RFS:
%What need to be estimated in standard RFS trackers: cardinality (the number of targets), state and label (trajectory of each target).}
%%% \colBlue{In particular, target 8 exists only for a short period of time.} }
%\label{fig:RFSscen}
%\vspace{-3mm}
%\end{figure}


\section{Preliminaries: PHD, MB and LMB} \label{sec:background}
\subsection{Basic MSMT Scenario}
The following scenario assumptions are made in this paper. Target births are independent of target survivals, which might be modelled differently at local sensors based on either Poisson or MB RFSs. Each target evolves and generates measurements independently. %The surviving process of each target is Bernoulli. That is, at
At time $k-1$, the target with state $\mathbf{x}_{k-1} \in \mathcal{X}$ will either die with probability $1-p^{\text{s}}_k(\mathbf{x}_k)$ or persists at time $k$ with survival probability $p^{\text{s}}_k(\mathbf{x}_k)$ and attains a new state $\mathbf{x}_k$ according to a Markov transition probability density function (PDF) $f_{k|k-1} (\mathbf{x}_k|\mathbf{x}_{k-1})$. Hereafter, $\mathcal{X} \subseteq \mathbb{R}^d$ denotes the $d$-dimensional state space.


%The target measurement at each sensor is also Bernoulli. That is, given
Given a target with state $\mathbf{x}_k \in \mathcal{X}$% at time $k$
, sensor $i \in \mathcal{I}$ either detects it with probability $p^{\text{d}}_{i,k}(\mathbf{x}_k)$ %(independent of the state)
and generates a measurement $\mathbf{z}_{i,k}\in Z_{i,k}$  with likelihood $g_{i,k}(\mathbf{z}_{i,k}|\mathbf{x}_k)$ or fails to detect it with probability $1-p^{\text{d}}_{i,k}(\mathbf{x}_k)$, where $Z_{i,k}$ denotes the {set} of the measurements received at time $k$ by sensor $i$.
The clutter (namely the measurement of no target) follows a Poisson RFS, independent of target measurements. % and independent among sensors. %The Poisson clutter intensity $\kappa_{i,k}$ at each sensor $i$ is independent of the target measurements.
%The states of a random number of targets can be described by a RFS ${X}_t = \big\lbrace \mathbf{x}_{t,1}, \dots, \mathbf{x}_{t,n} \big\rbrace \in \mathbb{X}$, where $n =|{{X}_t}|$ denotes the random number of targets at time $t$, %namely the cardinality of the set,
%and $\mathbf{x}_{t,i}\in \mathcal{X}$ is the state vector of the $i$-th target at time $t$ in the $d$-dimensional state space $\mathcal{X} \subseteq \mathbb{R}^d$.

\subsection{RFS Modeling}
% Mahler14book
%\begin{equation}
%  {X} = \big\lbrace \mathbf{x}_{1}, \dots, \mathbf{x}_{n} \big\rbrace \in \mathbb{X}
%\end{equation}
The states of a random number of targets are described by a RFS ${X} = \big\lbrace \mathbf{x}_{1}, \dots, \mathbf{x}_{n} \big\rbrace \in \mathbb{X}$, where $n =|{X}|$ denotes the random number of targets and $\mathbb{X}$ denotes all of the finite subsets of $\mathcal{X}$.
%The random nature of the multitarget RFS ${X}$ is captured by its multitarget probability density (MPD), denoted by $f({X})$.
For any realization of $X$ with a given cardinality $|{{X}}| =n$, namely ${X}_n = \big\lbrace \mathbf{x}_{1}, \dots, \mathbf{x}_{n} \big\rbrace$, \cite[Eq.2.36]{Mahler14book},
the MPD is defined as
%\begin{equation}\label{eq:fisst_mttPDF}
$f({X}_n) = n! \rho(n) f(\mathbf{x}_{1}, \dots, \mathbf{x}_{n})$
%\end{equation}
where the localization densities $f(\mathbf{x}_{1}, \dots, \mathbf{x}_{n} )$ are symmetric in their arguments and the cardinality distribution is given by $\rho(n)\triangleq \mathrm{Pr}\{|{{X}}|=n\} = \int_{|{X}| = n} {f({X})\delta {X}}$.
%is given by %\cite[Ch. 4.2.6]{Mahler14book}
%%\begin{align}
%$  \rho(n) % & \label{eq:def-card-mpd} \\
% = {\frac{1}{{n!}}\int_{\mathcal{X}^n} {f\big(\{ {\mathbf{x}_1},\dots,{\mathbf{x}_n}\} \big)d{\mathbf{x}_1}\dots d{\mathbf{x}_n}}}$.
%%\end{align}
%% and $ \left | X  \right |  $ denotes the cardinality of set $X$.
The set integral in $\mathbb{X}$ is defined as \cite[Ch. 3.3]{Mahler14book}
%\begin{align}
$\int_{\mathbb{X}} {f({X})\delta {X}}
    % = \sum\limits_{n = 0}^\infty  {\frac{1}{{n!}}\int_{S^n} {f(\{ {x_1},\dots,{x_n}\} )d{x_1}\dotsd{x_n}} } \\{\text{ \qquad \qquad \quad = }}
   = %\rho(0) + %f(\emptyset ) %
\sum_{n = 0}^\infty  {\frac{1}{{n!}}\int_{\mathcal{X}^n} {f\big(\{ {\mathbf{x}_1},\dots,{\mathbf{x}_n}\} \big)d{\mathbf{x}_1}\dots d{\mathbf{x}_n}}}
$ %\label{eq:set-integral-expansion}%\\
%   &= \sum\limits_{n = 0}^\infty \rho(n)
%\end{align}
where $f(\emptyset )= \rho(0)$.
%which %, similar to the conventional integral,
%is linear with regard to ${f}$, i.e.,
%$\int_\mathbb{X} {\left( {\alpha {f_1}({X}) + \beta {f_2}({X})} \right)\delta {X}}  = \alpha \int_\mathbb{X} {{f_1}({X})\delta {X}}  + \beta \int_\mathbb{X} {{f_2}({X})\delta {X}}$ and


%From above, it is easy to see that the cardinality distribution $\rho(n)$ of the RFS ${X}$ is given as \cite[Ch. 4.2.6]{Mahler14book} $\rho(n) = \int_{|{X}| = n} {f({X})\delta {X}}$.
% \begin{align}
%   \rho(n) & = \int_{|{X}| = n} {f({X})\delta {X}}  \label{eq:def-card-mpd} \\
%   &  = \frac{1}{{n!}}\int_{\mathcal{X}^n} {f(\{ {\mathbf{x}_1},\dots,{\mathbf{x}_n}\} )d{\mathbf{x}_1} \dots d{\mathbf{x}_n}}  \label{eq:def-card-mpd-expansion}
% \end{align}
The PHD $D(\mathbf{x})$, also known as the first moment density \cite[pp. 168-169]{Goodman97},
of the multitarget density $f({X})$ is a density function on single target $\mathbf{x}\in X$, defined as \cite[Ch.4.2.8]{Mahler14book} %\cite[Ch.4.2.8]{Mahler14book}
\begin{align}
D(\mathbf{x})   %&= \int_\mathbb{X}  {f\big( {\left\{ \mathbf{x} \right\} \cup X} \big)\delta X}\nonumber \\
& \triangleq \int_\mathbb{X}   {\bigg(\sum_{\mathbf{y}\in X}{\delta_\mathbf{y}}(\mathbf{x}) \bigg)f(X)\delta X} \label{def-PHD}
\end{align}
where \revisNew{${\delta_\mathbf{y}}(\mathbf{x})$ denotes the Dirac delta function concentrated at $\mathbf{y}$.} %${\delta_\mathbf{y}}(\mathbf{x}) = 1$ if $\mathbf{y} = \mathbf{x}$ and ${\delta_\mathbf{y}}(\mathbf{x}) = 0$ otherwise. %,which indicates that the PHD is a first moment density in the point process sense \cite[pp. 168-169]{Goodman97}.

The PHD is the density of the expected cardinality w.r.t. hyper-volume, which
has clear physical significance as its integral in any region $\mathcal{S} \subseteq \mathcal{X} $ gives the expected number $\hat{N}^{\mathcal{S}}$ of targets in that region, i.e.,
%\begin{equation}\label{eq:def-phd-integral=hatN}
$  \hat{N}^{\mathcal{S}} = \int_{\mathcal{S}} D(\mathbf{x}) d \mathbf{x}$.
%\end{equation}
Arguably, it tells how well the present targets are detected in the local region. The PHD plays a key role in all RFS filters and undoubtedly, a good PHD estimate implies a good filter estimate.
To distinguish the targets, one needs to use the labeled RFS (LRFS) which is a RFS whose elements are assigned with distinct labels \cite{Vo13Label,Vo14GLMB}. %$\int f({\mathbf x})d{\mathbf x}=\sum_{\ell\in\mathbb{L}}\int f(x,\ell)dx$.
Denote by {$\mathbb{L}$ all of the finite subsets of the label space.} %$\mathcal{L}$
A realization of a LRFS with cardinality $n$, multitarget state $X_n$ and label set $L_n = \big\lbrace l_{1}, l_{2}, \dots, l_{n} \big\rbrace \in \mathbb{L}$ is denoted by $\widetilde{X}_n = \{ (\mathbf{x}_1, l_1),(\mathbf{x}_2, l_2),...,(\mathbf{x}_n, l_n) \} \in \mathbb{X} \times \mathbb{L}$. The LRFS is completely characterized by its multitarget density $\pi\big(\widetilde{X}\big)$. % which is a joint distribution of the state and label.
Consequently, %the concept of PHD shall be extended to the labeled domain.
the labeled and unlabeled PHD for a labeled RFS are respectively given as follows \cite{Vo13Label}
\begin{align}
 % \rho(n) &=  \sum\limits_{c \in \mathbb{C}} \sum\limits_{L \in \mathbb{L}} {{\delta_n}[|L|] {\omega ^{(c)}}(L)}  \label{eq:glamb-cardinality-distribution} \\
  {\widetilde D} (\mathbf{x},l) & \triangleq \int_{\mathbb{X} \times \mathbb{L}}  {\pi}\big((\mathbf{x},l)\cup {\widetilde X}\big)\delta {\widetilde X} \\ %\label{eq:def-LRFS-phd-label} \\
  %&= \sum\limits_{c \in \mathbb{C}}{{s^{(c)}}(\mathbf{x},l)}\sum\limits_{L \in \mathbb{L}} {{\mathrm{1}_L}(l) {\omega ^{(c)}}(L)}  \label{eq:glamb-Lphd} \\
  D(\mathbf{x}) & \triangleq \sum\limits_{l \in \mathbb{L}} {\widetilde D}(\mathbf{x},l) \label{eq:def-LRFS-phd-nolabel} %\\
  %&= \sum\limits_{c \in \mathbb{C}}\sum\limits_{L \in \mathbb{L}} {{\omega ^{(c)}}(L) {\sum_{l\in L} {s^{(c)}}(\mathbf{x},l)}}  \label{eq:glamb-phd}
\end{align}
%where ${\mathrm{1}_x}(y) = 1$ if $y \subseteq x$ and ${\mathrm{1}_x}(y) = 0$ otherwise and the projection function $\mathcal{L}(\mathbb{X} \times \mathbb{L}) \rightarrow \mathbb{L}$ is given by $\mathcal{L}\big((\mathbf{x},l)\big) = l$.

In this paper, what indicated by the PHD is the unlabeled PHD by default unless otherwise stated.



\subsection{PHD of the Poisson, MB and LMB} \label{sec:Classic-RFS-distributions}
We hereafter omit the time notation and the dependence of the filter estimate on the observation process.
\subsubsection{Poisson RFS}

The {PHD of the Poisson RFS $X$ with mean $\lambda$ and MPD $f^{\text {p}}(X) = {{\text{e}}^{ - {\lambda}}}\prod_{\mathbf{x} \in X} {{\lambda}{s}(\mathbf{x})}$} is given by
\begin{align}
	%\label{eq:Poisson-PD} \\
%= \,\, & e^{-\int_{\mathcal{X}} D^{\text {p}}(\mathbf{x})d \mathbf{x}} \prod_{\mathbf{x}\in X}D^{\text {p}}(\mathbf{x}) \\
	D^{\text {p}}(\mathbf{x}) = \lambda {s}(\mathbf{x}) \label{eq:Poisson-PHD}
\end{align}
where $s(\mathbf{x})$ denotes the single-target probability density (SPD).


\subsubsection{MB}
%The density and PHD of a Bernoulli RFS $X^{\text{b}}$ with target existence probability $r$ and SPD $s(\mathbf{x})$ are, respectively, given by
%\begin{align}
%f^{\text{b}}\left(X\right) & = \begin{cases}
%1-r & X  =\emptyset\\
%rs\left(\mathbf{x}\right) & X =\left\{ \mathbf{x}\right\} \\
%0 & \mathrm{otherwise}
%\end{cases} \label{eq:Bernoulli-PD} \\
%D^{\text{b}}\left(\mathbf{x}\right) & = rs\left(\mathbf{x}\right) \label{eq:Bernoulli-PHD}
%\end{align}

An MB RFS $ X_n$ is the union of $n$ independent Bernoulli RFSs  \cite{Vo09CBmember} which can represent maximum $n$ targets. %, which is an efficient approximation to the multitarget Bayes filter.
Denoting the $\ell$-th Bernoulli component (BC) by $\big(r^{(\ell)},s^{(\ell)}(\mathbf{x})\big)$, the {PHD of MB RFS $ X_n$ with the MPD $f^\text{mb} (X_n) = \sum_{\uplus_{\ell=1}^{n}X^{(\ell)}=X}\prod_{\ell=1}^{n}f^\text{b} \left(X^{(\ell)}\right)$ is} given by
\begin{align}
	%\label{eq:def_MB}\\
	D^{\text {mb}}(\mathbf{x}) = \sum_{\ell=1}^n{r^{(\ell)}}{s^{(\ell)}}(\mathbf{x})  \label{eq:PHD-MB}
\end{align}
where $\uplus$ denotes the disjoint union and the density of a Bernoulli RFS $X^{(\ell)}$ with target existence probability $r^{(\ell)}$ and SPD $s^{(\ell)}(\mathbf{x})$ is given by
{$f^{\text{b}}\left(X^{(\ell)}\right) = 1-r^{(\ell)}$ if $X^{(\ell)}  =\emptyset$, $f^{\text{b}}\left(X^{(\ell)}\right) = r^{(\ell)}s^{(\ell)}\left(\mathbf{x}\right) $ if $X^{(\ell)}  =\left\{ \mathbf{x}\right\}$ and $f^{\text{b}}\left(X^{(\ell)}\right) = 0 $ otherwise}.
%\begin{align}
%f^{\text{b}}\left(X^{(\ell)}\right) = \begin{cases}
%1-r^{(\ell)} & X^{(\ell)}  =\emptyset\\
%r^{(\ell)}s^{(\ell)}\left(\mathbf{x}\right) & X^{(\ell)} =\left\{ \mathbf{x}\right\} \\
%0 & \mathrm{otherwise}
%\end{cases} \label{eq:Bernoulli-PD} % \\
%\end{align} %, the summation is taken over all mutually disjoint (and possibly empty) Bernoulli sets $X_1^\text{b},...,X_n^\text{b}$ whose union is $X^\text{mb}$.
%%\newpage

%When multiple MBs are used jointly and associated with different measurement-to-track hypotheses or labels, they result in the PMBM \cite{Williams15taesPMBM}, the MBM \cite{Angel18PMBMdeivation} and the GLMB \cite{Vo13Label,Vo14GLMB} which are all conjugate prior for multitarget filtering. %They are the exact Bayes-optimal multiple target filter.

%\subsubsection{$\delta$-GLMB}
%The MPD and labeled PHD of a $\delta$-GLMB RFS $\widetilde{X}$ are respectively given by \cite{Vo13Label}
%\begin{align} %\label{eq:delta-GLMB}
%	\pi^{{\delta}}\big(\widetilde{X}\big) %= \Delta \big(\widetilde{X}\big)\sum\limits_{(L,\xi) \in \mathbb{L} \times \Xi} {{\omega^{(L,\xi )}}{\delta _L}[\mathcal{L}\big(\widetilde{X}\big)]{{\left[ {{s^{(\xi )}}} \right]}^X}}.
%& = \Delta \big(\widetilde{X}\big) \sum\limits_{L \in \mathbb{L}} {\delta _L}\big[\mathcal{L}\big(\widetilde{X}\big)\big]  \sum\limits_{\xi \in \Xi} {{\omega^{(L,\xi )}}\prod\limits_{(\mathbf{x},l) \in \widetilde{X}} {{s^{(\xi)}}(\mathbf{x},l)}} \nonumber \\
% % \rho(n) &= \sum\limits_{L \in \mathbb{L}} {{\delta_n}\big(|L|\big) \sum\limits_{\xi \in \Xi} {\omega^{(L,\xi )}}}  \label{eq:glamb-cardinality-distribution} \\
%  {\widetilde D}(\mathbf{x},l) %& \triangleq \int  {\pi}\big((\mathbf{x},l)\cup {\widetilde X}\big)\delta {\widetilde X} \label{eq:def-LRFS-phd-label} \\
%  &=  \sum\limits_{L \in \mathbb{L}} {{\mathrm{1}_L}(\ell) \sum\limits_{\xi \in \Xi} {\omega^{(L,\xi )}} {{s^{(\xi)}}(\mathbf{x},l)} } \label{eq:glamb-phd} % \\
%%  D(\mathbf{x}) & \triangleq \sum\limits_{l \in \mathbb{L}} D(\mathbf{x},l) \label{eq:def-LRFS-phd-nolabel} \\
%%  &= \sum\limits_{L \in \mathbb{L}} { \sum\limits_{\xi \in \Xi} {\omega^{(L,\xi )}}  {\sum_{l\in L} {s^{(\xi)}}(\mathbf{x},l)}}  \label{eq:glamb-phd}
%\end{align}
%where the distinct label indicator $\Delta \big(\widetilde{X}\big)= \delta_{|\widetilde{X}|}\big[\mathcal{L}\big(\widetilde{X}\big)\big]$, ${\delta_I}[L] = 1$ if $I=L$ and ${\delta_I}[L] = 0$ otherwise, the projection function $\mathcal{L}(\mathbb{X} \times \mathbb{L}) \rightarrow \mathbb{L}$ is given by $\mathcal{L}\big((\mathbf{x},l)\big) = l$, the pair $(L,\xi)$ represents the hypothesis that the track set $L$ has a history $\xi$ of measurement-track association maps \cite{Vo14GLMB} and ${\omega^{(L,\xi )}} $ is the weight of the hypothesis $(L,\xi)$, the respective labeled SPDs $ s^{(\xi)}(\mathbf{x}_1,l_1),\ldots, s^{(\xi)}(\mathbf{x}_n,l_n)$ satisfy
%$\sum\limits_{L \in \mathbb{L}} {\sum\limits_{\xi \in \Xi} {\omega^{(L,\xi )}} }  = 1, \int_{\mathcal{X}} {{s^{(\xi)}}(\mathbf{x},{l})d\mathbf{x}}  = 1$.

\subsubsection{LMB}
%Treating the index to each BC in the MB as the unique label distinguishing different targets will lead to the MPD resembling the labeled MB. That is, the
Similar to the MB filter, the LMB filter \cite{Reuter14LMB} associates each labeled BC $l \in L \in \mathbb{L}$ with SPD ${s}(\mathbf{x},l) $ and existence probability $r^{(l)}$. {The MPD is much complicated as than that for the MB filter, which can be given as $\pi^{\text {lmb}}(\widetilde{X}) = \Delta \big(\widetilde{X}\big) \omega\big(\mathcal{L}\big(\widetilde{X}\big)\big) \prod_{(\mathbf{x},l) \in \widetilde{X}} {{s}(\mathbf{x},l)}$} %% The MPD and labeled PHD are, respectively, given by
where the distinct label indicator $\Delta \big(\widetilde{X}\big)= \delta_{|\widetilde{X}|}\big(|\mathcal{L}\big(\widetilde{X}\big)|\big)$, ${\delta_I}(L) = 1$ if $I=L$ and ${\delta_I}(L) = 0$ otherwise, the projection function $\mathcal{L}(\mathbb{X} \times \mathbb{L}) \rightarrow \mathbb{L}$ is given by $\mathcal{L}\big((\mathbf{x},l)\big) = l$, $\int_{\mathcal{X}} {{s}(\mathbf{x},{l})d\mathbf{x}}  = 1$, and $\omega(L)$ denotes the hypothesis weight corresponding to the label set $L \in \mathbb{L}$, which is given by $\omega(L) = \prod_{i\in \mathbb{L}} (1-r^{(i)}) \prod_{l\in L} \frac{{\mathrm{1}_\mathbb{L}}(l)  r^{(l)}}{1-r^{(l)}}$,
%\begin{equation}\label{eq:w(L)}
%  \omega(L) = \prod_{i\in \mathbb{L}} (1-r^{(i)}) \prod_{l\in L} \frac{{\mathrm{1}_\mathbb{L}}(l)  r^{(l)}}{1-r^{(l)}}
%\end{equation}
where ${\mathrm{1}_\mathbb{L}}(l) = 1$ if $l \in \mathbb{L}$ and ${\mathrm{1}_\mathbb{L}}(l) = 0$ otherwise and $\sum_{L \in \mathbb{L}} {\omega(L)} = 1$.
The labeled and unlabeled PHD of the LMB are respectively given as follows
\begin{align}
{\widetilde D}^{\text {lmb}}(\mathbf{x},l) &{= \sum\limits_{L \in \mathbb{L}} {\mathrm{1}_L}(l)  {\omega{(L)} {{s}(\mathbf{x},l)}} }\nonumber \\
& = r^{(l)} {s}(\mathbf{x},l) \\ % \label{eq:LMB-phd} \sum_{l\in L}
{D}^{\text {lmb}}(\mathbf{x}) &= \sum\limits_{l \in L } {\widetilde D}^{\text {lmb}}(\mathbf{x},l) \nonumber \\
& = \sum\limits_{l \in L } r^{(l)} {s}(\mathbf{x},l) \label{eq:LMB-phd} % \sum_{l\in L}
\end{align}

%, and $r^{(l)}$ is the target existing probability corresponding to the label $l\in L$. %$w\big[\mathcal{L}\big(\widetilde{X}\big)\big] = (1-r)^{\mathbb{L}-\mathcal{L}\le
%
%\section{PHD-Consistency and PHD-AA Fusion} \label{PHD-Consistency}
\subsection{PHD-AA Consistency}
For a set of PHDs $D_i(\mathbf{x})$ produced by RFS filters $i \in \mathcal{I} =\{1,2,...,I\}$, the AA fusion is simply given as follows
\begin{equation}\label{eq:PHD-AA}
{D_{\text{AA}}}(\mathbf{x}) \triangleq \sum\limits_{i \in {\mathcal{I}}} {{w_i}{D_i}(\mathbf{x})}
\end{equation}
where the fusion weights $w_1,\dots,w_I >0, \sum_{i \in {\mathcal{I}}} {w_i}  =1$, and the weight space $\mathbb{W} \triangleq\{\mathbf{w} \in \mathbb{R}^{I}|\mathbf{w}^\mathrm{T}\mathbf{1}_I = 1, w_i > 0, \forall i \in \mathcal{I}\}$.
%where the fusion weights $\mathbf{w} =\{w_1,\dots,w_I\} \in \mathbb{W} \subset \mathbb{R}^{I}$, and the weight space $\mathbb{W} \triangleq\{\mathbf{w} \in \mathbb{R}^{I}|\mathbf{w}^\mathrm{T}\mathbf{1}_I = 1, w_i > 0, \forall i \in \mathcal{I}\}$.

%The MPD-AA fusion is given by % leads to
%\begin{equation}\label{eq:RFS-AA-Whole}
%{f_{\text{AA}}}({X}) \triangleq \sum\limits_{i \in {\mathcal{I}}} {{w_i}{f_i}({X})}
%\end{equation}
%which corresponds to

The AA is a Fr\'{e}chet mean in the sense of the integrated squared difference (ISD) \cite{Li20AAmb}, i.e.,
\begin{equation}
  D_\mathrm{AA}(\mathbf{x})  = \operatorname*{arg\,min}_{g\in\mathcal{F}_{\mathcal{X}}} \sum\limits_{i \in {\mathcal{I}}}{w_i\int_{\mathcal{X}} \big(D_i(\mathbf{x})-g(\mathbf{x})\big)^2 \delta \mathbf{x}}   \label{eq:FrechetAAISD}
\end{equation}
where $\mathcal{F}_\mathcal{X} \triangleq \{f: \mathcal{X} \rightarrow \mathbb{R} \}$ %$\mathcal{F}_{\mathcal{X}}$ denotes the set of scalar-valued functions in space $\mathcal{X}$: $\mathcal{F}_{\mathcal{X}} = \{f: \mathcal{X} \rightarrow \mathbb{R} \}$.

Relevantly, it is more known as follows, which we refer to as the best fit of the mixture (BFoM) \cite{Li23AApmbm}, %BFoM, c.f., \eqref{eq:AA-density-KLD},
\begin{equation}\label{eq:RFS-AA-Whole-KLD}
 {D_{\text{AA}}}(\mathbf{x}) = \operatorname*{arg\,min}_{g\in\mathcal{F}_\mathcal{X}} \sum\limits_{i \in {\mathcal{I}}} {w_iD_\text{KL}\big(D_i\|g\big)}
\end{equation}
where the Kullback-Leibler (KL) divergence, {extended to the PHD domain,} is defined as $D_\text{KL}\big(f\|g \big) \triangleq \int_\mathcal{X} {f(\mathbf{x})\log \frac{f(\mathbf{x})}{g(\mathbf{x})} \delta \mathbf{x}}$.
%$\mathcal{F}_\mathcal{X} \triangleq \{f: \mathbb{X} \rightarrow \mathbb{R} \}$ and
%the set

However, in the context of labeled MPD fusion, the labels which suffer more or less from estimate errors conditional on random false/missing data and noises are often different across the sensors. They need to be pairwise-matched between the labeled filters {to comply with the} track/label-wise fusion rule \cite{Li22RFS-AA-Derivation}. Nevertheless, a proper quantitative
definition of the divergence/distance between labels is still missing, which is actually the base for label matching. %In the literature, the inter-filter matched labels are treated as the same and the label estimate errors are omitted.
Different labeling matching choices will {in turn} lead to very different divergence values and correspondingly different fusion results \cite{Li23BMsurvey}.
%In fact, a properly defined distance/divergence for the LRFS densities needs to include both divergences between the state distribution and between (the distribution of) discrete labels.
To overcome this challenge, the fusion in this work is limited in the unlabeled domain, {more precisely unlabeled PHD-AA fusion,} which is free of label matching, although the LRFS/LMB filters are involved.

%\subsection{Motivation for PHD-AA Fusion} \label{sec:Motivation-phd-AA-fusion}
%The PHD is a key statistical character of the RFS distribution, and so %whose integral in any region $\mathcal{S}\subseteq \mathcal{X}$ gives the estimated number of target in that region. Therefore,
%an accurate PHD estimate is vital in any RFS filters. On this point, we present the following definition relative to the effectiveness of the multisensor fusion and important result on the PHD-AA fusion. % which provides a unique means to approximate the MPD by respective
%%The PHD itself has been used for filter recursion of the Poisson point process (PPP), namely the PHD filter
%\begin{definition}[PHD Consistency] The accuracy of the multisensor fused PHD increases with the number of fusing estimators if the fusing weights are properly designed. That is, the PHD consistent fusion should result in statistically more accurate detection of the targets and the fused PHD converges to the first moment of the true MPD as the number of the fusing estimators increases indefinitely. %goes to infinity. % in the area. %, given by the integral of the fused PHD in that region. %This requires a linear fusion of the PHDs.
%\end{definition}
%
%
%We note that our above definition abides with the standard definition of the consistency of an unbiased estimator, i.e., as the number of data points used increases indefinitely, the resulting sequence of estimates converges in probability to the ground truth. %Here, in our case, the PHD consistency equivalently means that as the number of fusing estimators increase indefinitely, the fused PHD converges to the first moment of the true MPD.
%This is different from the concept of consistency given in \cite{Julier06consistency,Uney19consistency}, which means something else. % and is unrelated with the increase of the number of fusing estimators. %a probability density that at no point in the state space overlooks the density of their components.
%%\cite{Julier06consistencyUney19consistency} proposed the concept of consistency in cardinality and demonstrated that the GA fusion of finite-set distributions does not necessarily lead to consistent fusion of cardinality distributions.
%

%
%%Assume that the fusing estimators are conditionally statistically independent with each other and their PHD estimates are unbiased in any region $\mathcal{S}\subseteq \mathcal{X}$. % i.e., $\mathbb{E}[\int_{\mathcal{S}} D_i(\mathbf{x})d \mathbf{x}]= N^{\mathcal{S}}$,
%The PHD-AA fusion \eqref{eq:PHD-AA} strictly ensures consistency as long as the fusing filters are properly designed; see the analysis given in \cite{Li22RFS-AA-Derivation}.
%%\end{lemma}
%%\begin{remark} \label{remark:cardinality-wise-AA}
%In addition, the mixing operation and averaging calculation embedded in the AA density fusion can reduce the effects of clutter and missed detections \cite{Li17PC,Li17PCsmc}. %, % since the mixing operation and averaging calculation are insensitive to false and missing data,
%%improving the robustness of the local filters.
%%We further highlight the following point%In particular, in the case of low detection profile, the AA fusion has a significant advantage as compared with the geometric average fusion approach.
%\begin{remark}
%While the AA fusion \eqref{eq:PHD-AA} can be applied to any homogeneous or heterogeneous distributions (even if they are defined on different domains), the standard ISD and KL divergence are only defined for probability measures on a common measurable space. Regardless of some recently proposed disvergences/distances for two distributions of different dimensions \cite{Cai22DivDimensions,Caprio22}, there still lacks a proper distance/divergence metric for the LRFS densities with different, discrete labels. As such, both \eqref{eq:FrechetAAISD} and \eqref{eq:RFS-AA-Whole-KLD} can not be directly applied for the labeled RFS distributions in general unless their labels are impractically the same. %Here, the matched labels are treated as the same. %. Yet, this has been often violated in the literature.
%However, they can be applied to the labeled PHDs defined on $\mathcal{X}\times\mathcal{L}$ as long as the label spaces are the same among the sensors.
%\end{remark}
%%This, however, cannot be explained by the BFoM/MIL formulation that says nothing in the estimation accuracy or robustness. Simply, the GA fusion that minimizes similar divergence \eqref{eq:GA-density-KLD} has, however, been proven vulnerable to local misdetection \cite{Yu16,Li17merging,Li17PC,Li17PCsmc,Gao20cphd,Gao20GLMB,Da_Li20TSIPN,GLi22aaPMB} and cannot even maintain the unbiasedness in point estimation if all fusing estimators are unbiased \cite{Li19Second,Uney19consistency}. %That is, the advantage of the AA fusion is greatly beyond what can be interpreted by the BFoM/MIL. %does not relate the real distribution or even the multisensor optimal distribution and


\section{GM Representations of MB/LMB-PHD} \label{GM-implementation} % and Inter-sensor Association

\subsection{GM-PHD} \label{sec:GM-PHD}
It is theoretically justified and also practically convenient to represent the RFS posterior and at the same time the corresponding PHD by a GM, which facilitates the GM-PHD-AA fusion. Actually, what is estimated and propagated over time in the PHD filter \cite{Mahler03} is the PHD, not the MPD. %Hereafter,
Straightforwardly, the GM approximation of the PHD filter $i \in \mathcal{I}$ at filtering time $k$ can be written as \cite{Vo06}:
\begin{equation}\label{eq:GM_PHD}
D_{i, k}(\mathbf{x}) \approx \sum_{j=1}^{J_{i,k}} \omega_{i,k}^{(j)} \mathcal{N}(\mathbf{x};\bm{\mu}_{i,k}^{(j)},\bm{\Sigma}_{i,k}^{(j)}) %\hspace{0.5mm},
\end{equation}
where $\mathcal{N}\big(\mathbf{x};\bm{\mu}\rmv,\bm{\Sigma}\big)$ denotes a Gaussian PDF with mean vector $\bm{\mu} $ and covariance matrix $\bm{\Sigma}$, $J_{i,k}$ is the number of GCs in total, and $\omega_{i,k}^{(j)}$ is the weight of $j$th GC at sensor $i$.

The whole PHD is thereby completely determined by the parameter set $\mathcal{G}_{i,k} \triangleq \big\{ \big( \omega_{i,k}^{(j)} , \bm{\mu}_{i,k}^{(j)} , \bm{\Sigma}_{i,k}^{(j)}  \big) \big\}_{j=1,\dots,J_{i,k}}$. The expected number of targets at sensor $i\in \mathcal{I}$ at time $k$ can be approximated by
{$\hat{N}_{i,k} = \sum_{j=1}^{J_k} \omega_{i,k}^{(j)}$}.




\subsection{GM Representation of the MB PHD} \label{sec:MB-AA}
Consider the GM implementation of the MB posterior %\eqref{eq:def_MB}
represented by a set $L_{i,k}$ of BCs at filtering time $k$ by sensor $i$. Each BC $\big(r_{i,k}^{(\ell)}, s_{i,k}^{(\ell)}(\cdot)\big), \ell \in L_{i,k}$ is represented by $J_{i,k}^{(\ell)}$ GCs weighted by $\omega_{i,k}^{(\ell,\iota)} \!\ge\rmv 0$, $\iota=1,\dots,J_{i,k}^{(\ell)}$, i.e.,
\begin{equation}
s_{i,k}^{(\ell)}(\mathbf{x}) = \sum _{\iota=1}^{J_{i,k}^{(\ell)}} \omega_{i,k}^{(\ell,\iota)} \ist \mathcal{N}\big(\mathbf{x};\bm{\mu}_{i,k}^{(\ell,\iota)}\rmv,\bm{\Sigma}_{i,k}^{(\ell,\iota)}\big) \label{eq:BC_GM} %\ist,
\end{equation}
where
\begin{equation}\label{eq:BCweightofGC}
  \sum _{\iota=1}^{J_{i,k}^{(\ell)}} \omega_{i,k}^{(\ell,\iota)} = 1
\end{equation}

Each BC is completely determined by the {GM} parameter set $\mathcal{G}^{(\ell)}_{i,k} \triangleq \big\{ \big( \omega_{i,k}^{(\ell,\iota)} , \bm{\mu}_{i,k}^{(\ell,\iota)} , \bm{\Sigma}_{i,k}^{(\ell,\iota)}  \big) \big\}_{\iota=1,\dots,J_{i,k}^{(\ell)}}$ {by which the PHD \eqref{eq:PHD-MB}} is approximated as a set of GMs as follows
\begin{equation}\label{eq:mbPHD_GM}
{D_{i,k}}(\mathbf{x}) \approx \sum_{\ell \in L_{i,k}} r_{i,k}^{(\ell)} \sum _{\iota=1}^{J_{i,k}^{(\ell)}} \omega_{i,k}^{(\ell,\iota)} \mathcal{N}\big(\mathbf{x};\bm{\mu}_{i,k}^{(\ell,\iota)}\rmv,\bm{\Sigma}_{i,k}^{(\ell,\iota)}\big)
\end{equation}
which may be expressed as a unified GM of the parameter set $\mathcal{G}_{i,k} \triangleq \big\{ \mathcal{G}^{(\ell)}_{i,k} \big\}_{\ell \in L_{i,k}}$ as follows, c.f., \eqref{eq:GM_PHD},
\begin{equation}\label{eq:mbPHD_UnifiedGM}
{D_{i,k}}(\mathbf{x}) \approx \sum_{j=1}^{J_{i,k}} \omega_{i,k}^{(j)} \mathcal{N}(\mathbf{x};\bm{\mu}_k^{(j)},\bm{\Sigma}_k^{(j)})
\end{equation}
where $J_{i,k} = \sum_{\ell \in L_{i,k}}  J_{i,k}^{(\ell)}$ and the recombined weight of each GC labeled as $j$ is given as
\begin{equation}\label{eq:mbPHD_WeightGC}
\omega_{i,k}^{(j)} = r_{i,k}^{(\ell)}  \omega_{i,k}^{(\ell,\iota)}
\end{equation}
which uses the following unique index mapping at each sensor $i\in \mathcal{I}$ at time $k$,
\begin{equation}\label{eq:indexMapping}
  j \leftrightarrow (\ell,\iota)
\end{equation}
where $j=1,...,J_{i,k}, \ell  \in L_{i,k}, \iota=1,...,J^{(\ell)}_{i,k}$.

The mean-state of the $\ell$-th BC %estimated by sensor $i$ at time $k$
as expressed in \eqref{eq:BC_GM} is calculated by
%\begin{equation}\label{eq:BC_mean_GM}
$\bar{\bm{\mu}}_{i,k}^{(\ell)} = \sum _{\iota=1}^{J_{i,k}^{(\ell)}} \omega_{i,k}^{(\ell,\iota)} \bm{\mu}_{i,k}^{(\ell,\iota)} $. % \\
% r_{i,k}^{(\ell)}  & \approx \sum _{\iota=1}^{J_{i,k}^{(\ell)}} \omega_{i,k}^{(\ell,\iota)} \label{eq:BC_r_GM} ,
%\end{equation}
%which indicate the state of BC $\ell$ estimated by sensor $i$ at time $k$.% and are fully determined by $\mathcal{G}^{(\ell)}_{i,k}$.
%\textcolor{magenta}{(Note that our notation does not indicate that the quantities $w_{k}^{(\ell,\iota)}$,  $\psi_{k}^{(\ell,\iota)}(\mathbf{x})$, $\mathbf{m}_{k}^{(\ell,\iota)}$, $\mathbf{P}_{k}^{(\ell,\iota)}$, and $\mathbf{x}_{k}^{(\ell,\iota)}$ are functionally dependent on
%$Z_{1:k}$.)}
By integrating the PHD, %as in \eqref{eq:Card_Calculation},
the number of targets is estimated at sensor $i$ by
\begin{align}\label{eq:MB_Card}
  \hat{N}_{i,k}  & = \sum_{\ell\in L_{i,k}} r_{i,k}^{(\ell)} \nonumber \\
  & =  \sum _{j=1}^{J_{i,k} } \omega_{i,k}^{(j)}
\end{align}

%Here, we consider the MB-AA fusion from the perspective of PHD-AA/consistency. % and will show for the first time under special cases, the MB-AA fusion can be closed.
%Consider the MB RFS which has $n_i$ BCs in which the $j$-th BC has existence probability $r_{i,j}$ and SPD $s_{i,j}\left(\mathbf{x}\right)$.  The corresponding PHD-AA for the MBs can be expressed as, c.f., \eqref{eq:def-PHD-AA},
%\begin{align}
%	D_{\text{AA}}^{\text {mb}}(\mathbf{x}) & =  \sum\limits_{i \in {\mathcal{I}}}{w_i D_i^{\text {mb}}(\mathbf{x}) } \nonumber \\
%& =  \sum\limits_{i \in {\mathcal{I}}}{w_i  \sum_{\iota=1}^{n_i}{r_{i,j}}{s_{i,j}}(\mathbf{x}) } \nonumber \\ %
%& =  \sum_{j=1}^{n_{f}}{r_{j,\text{PHD-AA}}}{s_{j,\text{PHD-AA}}}(\mathbf{x})  \label{eq:MB-PHA-AA}
%\end{align}
%which corresponds to the following fused MB MPD, c.f., \eqref{eq:def_MB},
%\begin{equation}\label{MB-AA-MPD}
%  	f_{\text{PHD-AA}}^\text{mb} (X_n)  = \sum_{\uplus_{j=1}^{n_{f}}X^{(j)}=X}\prod_{j=1}^{n_{f}}f_{\text{PHD-AA}}^\text{b} \left(X^{(j)}\right)
%\end{equation}
%where the fused BC $f_{\text{PHD-AA}}^\text{b} \left(X^{(j)}\right)$ having parameters $\big( r_{j,\text{PHD-AA}},{s_{j,\text{PHD-AA}}}(\mathbf{x}) \big)$ is given in \eqref{eq:iidc-phd-aa} in Corollary \ref{corollary:Bernoulli} for each clustered/associated group of BCs, and the final number $n_{f}$ of fused BCs depends on the BC merging/clustering procedure, usually
%\begin{equation}\label{eq:MB-n}
%  \max\{n_i\}_{i \in {\mathcal{I}}} \leq n_{f} \leq  \sum_{i \in {\mathcal{I}}}n_i
%\end{equation}
%where the left equation holds iff all BCs from the other sensors are associated with those of a local sensor and the right equation holds iff the BCs are completely not associated with each other.

%The readers are referred to \cite{Li20AAmb} for the detail for grouping the fusing MBs via clustering or one-to-one association \cite{Li20AAmb}. {In addition, data-driven clustering and Bernoulli merging can be found} in \cite{Fontana22merging}. The MB-AA fusion has been recently used for PMB-AA-fusion \cite{GLi22aaPMB}. In the case of labeled MB, the BC association problem becomes to the label matching problem. %, see sections \ref{sec:LMB} and \ref{sec:label-matching}.


\subsection{GM Representation of the LMB PHD} \label{sec:MB-PHD}
The LMB can be viewed as a special MB with assigned label for each BC. That is, with slight abuse of notation and by interpreting each component index $\ell$ in \eqref{eq:BC_GM}, \eqref{eq:mbPHD_GM} and \eqref{eq:MB_Card} as a track label $l$ and set $L_{i,k}$ as the label set of sensor $i$ at time $k$, the above GM formulation \eqref{eq:mbPHD_GM} for the MB-PHD can be the same derived for calculating the unlabeled PHD of the LMB. %This somehow conflicts with the standard definition of the labels which
Note that the labels are usually ordered pairs of integers {$l = (k, \kappa)$}, where $k$ is the time of birth, and $\kappa$ is a unique index to distinguish new targets born at the same time \cite{Vo13Label}. %We omit the detail.
The unlabeled PHD $D_{i,k}(\mathbf{x})$ of the local LMB at time $k$ at sensor $i \in \mathcal{I}$, expressed by the parameter set $\mathcal{G}_{i,k} \triangleq \big\{ \big( \omega_{i,k}^{(j)} , \bm{\mu}_{i,k}^{(j)} , \bm{\Sigma}_{i,k}^{(j)}  \big) \big\}_{j=1,\dots,J_{i,k} }$, can be given by, c.f., \eqref{eq:mbPHD_UnifiedGM},
\begin{equation}
{D_{i,k}}(\mathbf{x}) = \sum_{j=1}^{J_{i,k}} \omega_{i,k}^{(j)} \mathcal{N}(\mathbf{x};\bm{\mu}_k^{(j)},\bm{\Sigma}_k^{(j)})
\end{equation}
where {multiple L-GCs may have the same label}.


\section{GM-PHD Fit via Optimizing L-GC Weights } \label{sec:GMfit}
This paper addresses averaging the PHDs of the PHD, MB, and LMB filters via the GM implementations, where the GM parameter set for time $k$ at sensor $i \in \mathcal{I}$ is denoted by $\mathcal{G}_{i,k}$ and the corresponding local PHD by $D_{i,k}(\mathbf{x})$. {The goal of the PHD-AA fusion is to update the local GM parameters so that their corresponding PHDs reach consensus as calculated by \eqref{eq:PHD-AA} in each filtering iteration. As a result, the local filters have similar or the same PHD while their MPDs are variant. A simple 1-D example is given in Fig. \ref{fig:PHDvsMB}, where the MB filter has the same PHD as that of the PHD filter while their filter posteriors are heterogeneously parameterized.}
%\begin{equation}\label{eq:AA-PHD}
%  D_\text{AA,k}(\mathbf{x}) = \sum_{i\in \mathcal{I}} w_iD_{i,k}(\mathbf{x})
%\end{equation}
%where the fusion weights $\mathbf{w} =\{w_1,\dots,w_I\} \in \mathbb{W} \subset \mathbb{R}^{I}$


\begin{figure}
  \centering
  {
  % Requires \usepackage{graphicx}
  \includegraphics[width=8.5cm]{PHDvsMB.eps}\\  % _extended
  \caption{A MB filter reaches PHD consensus with a PHD filter %for which the label matching is nontrivial.
   } \label{fig:PHDvsMB}
  \vspace{-4mm}
  }
\end{figure}

%The track association or label matching forms a significant challenge for heterogeneous filter fusion. % since simply the different filters have different algorithm structures. %which prevent matching
%For instance, it is hard to associate any GCs in the GM-PHD filter with those of the GM-LMB filter.
\begin{remark}
The key challenge to heterogeneous RFS filter fusion origins from the fact that different RFS filters result in different types of MPDs. %, although they may all be implemented by using GM approximation.
Unlike the GM-PHD filter, the GCs in the GMs of the MB and of LMB filters intrinsically belong to different (unlabeled or labeled) BCs. Therefore, if a new L-GC is created or an existing L-GC is deleted, one has to identify the BC/label it belongs to.
\end{remark}

The situation is more complicated in the case of MB mixture fusion \cite{Li23AApmbm} % and GLMB fusion \cite{Gao20GLMB}
where the measurement-to-track association hypothesis is further involved.
To address this challenge, we hereafter propose an approximate GM-PHD fitting approach which does not create or disregard any L-GCs in the local filters but only optimize their weights $\omega_{i,k}^{(j)}, j=1,...,J_{i,k}$ for consensus over their PHDs. Clearly, the resulted PHD is no more than an approximate to the {exact} PHD-AA. % but it is expected to be a good approximate; see section \ref{sec:homogeneousSim}.
This fit does not only save computation but also enables the parallel-communication-filtering operation similarly as was done in \cite{Li17PCsmc,Li19ParallelCC} since the mean $\bm{\mu}_{i,k}^{(j)}$ and covariance $\bm{\Sigma}_{i,k}^{(j)}$ of each L-GC $j$, as well as the total number $J_{i,k}$ of L-GCs, are unchanged and therefore can be updated in parallel during the fusion. We omit the detail here. Obviously, better fit can be expected if $\bm{\mu}_{i,k}^{(j)}$ and $\bm{\Sigma}_{i,k}^{(j)}$ are also optimized jointly with the weights, regardless of the computational cost.
%We leave this to the future work.  %
%We leave this to the future work. %The local GCs only need to adjust their weights by using scaling ratios yielded via fusion; see section \ref{sec:fusionfeedback}. The fusion feedback can be performed in the next filtering iteration such that the filter does not need to start the next filtering iteration after the communication and fusion.


\subsection{Minimizing ISD of GMs via Reweighting} \label{sec:ise}
To evaluate the {goodness} of fit, the KL divergence can be considered which, however, does not admit analytical solution for the GMs. For the sake of computational efficiency, one may consider the Cauchy-Schwarz divergence \cite{Kampa11CSD} \footnote{{It is necessary to note that the Cauchy-Schwarz divergence was originally defined on PDFs and is a normalized distance. Therefore, its application to the PHD is not straightforward but proper extension is needed.}} %,Hoang15
and the ISD metric as given in \eqref{eq:FrechetAAISD}, both of which allow analytical calculation for GMs. We consider the latter only, which complies with the BFoM as expressed by \eqref{eq:FrechetAAISD}. % and can thereby significantly simplify the calculation. % and can thereby significantly simplify the calculation.
Formally speaking, the goal of our approach is to determine the new weight for each L-GC at the local sensor in the following BFoM sense
\begin{align}
  \{ \omega_{i,k}\}^{\text{BFoM}} & = \operatorname*{arg\,min}_{\{ \omega_{i,k}\} \geq 0} \text{ISD}\big(D_{i,k}\| D_{\text{AA},k}\big) \label{eq:opt_W_ISD} \\
  & = \operatorname*{arg\,min}_{\{ \omega_{i,k}\} \geq 0} \int \bigg( (1-w_i) \sum_{j=1}^{J_{i,k}} \omega_{i,k}^{(j)} \mathcal{N}(\mathbf{x};\bm{\mu}_{i,k}^{(j)},\bm{\Sigma}_{i,k}^{(j)}) \nonumber \\ %D_{i,k}(\mathbf{x})
  & \hspace{3mm} - \sum_{s\in \mathcal{I}\setminus{i}} w_s \sum_{j=1}^{J_{s,k}} \omega_{s,k}^{(j)} \mathcal{N}(\mathbf{x};\bm{\mu}_{s,k}^{(j)},\bm{\Sigma}_{s,k}^{(j)}) \bigg)^2 d \mathbf{x} \label{eq:WeightISDfit}  %j\in \mathcal{I}
\end{align}
where $\big\{ \omega_{i,k}^{(j)}\big\}_{j=1,...,J_{i,k}}$ was written as $\{ \omega_{i,k}\}$ in short and $A\setminus B$ is the set difference of $A$ and $B$. %$\mathcal{I}\setminus{i} := \{1,2,...,i-1,i+1,I\}$.

%  {\bm{\mu}_n} & = \bm{\mu}_{i,k}^{(j)}  \label{eq:mu} \\
%  {\mathbf{P}_n} & = \bm{\Sigma}_{i,k}^{(j)}  \label{eq:P} \\
%  \beta_m  & = w_{s} \omega_{s,k}^{(l)}  \label{eq:beta} \\
%  {\bm{\mu}_{s,k}^{(l)}} & = \bm{\mu}_{s,k}^{(l)}  \label{eq:m} \\
%  {\bm{\Sigma}_{s,k}^{(l)}} & = \bm{\Sigma}_{s,k}^{(l)}  \label{eq:S} \\

{In addition to the above PHD-GM fit, we further communicate and average the multitarget set cardinality estimates of local sensors for more accurate estimation of the target number}, namely cardinality consensus (CC) \cite{Li19CC}, although the cardinality (distribution) is not directly estimated in the PHD, MB and LMB filters. By using \eqref{eq:MB_Card}, the AA of the estimated number of targets can be calculated as
{\begin{align}\label{eq:AA-card}
\hat{N}_{\text{AA},k} & = \sum_{i \in \mathcal{I}}  \hat{N}_{i,k} %\nonumber \\
%& = \sum_{i \in \mathcal{I}}  \sum_{\ell\in L_{i,k}} r_{i,k}^{(\ell)}
\end{align}
where $\hat{N}_{i,k} = \sum_{j=1}^{J_k} \omega_{i,k}^{(j)}$ as addressed earlier for the PHD filter in Sec. \ref{sec:GM-PHD} and for the MB/LMB filters in Sec. \ref{sec:MB-PHD}.}

{By taking into account both the non-negative constraint and the above CC constraint of the weights, the GM-weight-fit problem can be formulated as the following constrained multivariate optimization (CMO) problem
  \begin{align}
    &\operatorname*{min} J\big(\{ \omega_{i,k}\}\big) = \text{ISD}\big(D_{i,k}\| D_{\text{AA},k}\big) \label{eq:CMO} \\
    &\text{s.t.} \hspace{3mm} \omega_{i,k}^{(j)} \geq 0, j=1,...,J_{i,k} \\
    & \hspace{6mm} \sum_{j=1}^{J_k} \omega_{i,k}^{(j)} = \sum_{j'=1}^{J'_k} \omega_{i',k}^{(j')}, \forall i' \neq i \label{eq:CC_constraint}
  \end{align}}

{The most challenging issue of the above CMO problem is from the CC constraint \eqref{eq:CC_constraint}. To simplify this problem, we address it in a separate step at the end of our algorithm; see section \ref{sec:CC}. By removing it temporally, the left inequality CMO problem leads to the following Lagrangian function
\begin{align}
  L\big( \{ \omega_{i,k}\}, \{\lambda_j\} \big)= J\big(\{ \omega_{i,k}\}\big) - \sum_{j=1}^{J_{i,k}} \lambda_j \omega_{i,k}^{(j)} %,\{d_j\}
\end{align}
where $\{\lambda_j\}_{j=1,...,J_{i,k}}$ are the Lagrange multipliers. }

{The Karush-Kuhn-Tucker conditions for this CMO problem are given as follows
\begin{align}\label{eq:KKT}
  \frac{\partial J\big(\{ \omega_{i,k}\}\big) }{\partial \omega_{i,k}^{(j)}} - \lambda_j & = 0, j=1,2,..., J_{i,k} \\
  \lambda_j \omega_{i,k}^{(j)} & = 0, j=1,2,..., J_{i,k}  \\
  \lambda_j & \geq 0, j=1,2,..., J_{i,k}
\end{align}}

%{We further note that there is an additional cardinality consensus constraint to be satisfied as to be addressed }


\subsection{A Coordinate Descent CMO Solver and Over-fit}

\begin{lemma}
{Relaxing the non-negative constraint for the weights $\omega_{i,k}^{(j)}\geq 0$,} \eqref{eq:WeightISDfit} can be solved by, $\forall i \in \mathcal{I}, j=1,...,J_{i,k}$,
\begin{align}
 & \omega_{i,k}^{(j),\text{BFoM}} = \nonumber \\
 & \frac{\Delta} {(1-w_i)} {\sum\limits_{s\in \mathcal{I}\setminus{i}} \sum\limits_{l=1}^{J_{s,k}}{w_{s} \omega_{s,k}^{(l)}}\mathcal{N}\big({\bm{\mu}_{i,k}^{(j)}};{\bm{\mu}_{s,k}^{(l)}},{\bm{\Sigma}_{i,k}^{(j)}}{\rm{ + }}{\bm{\Sigma}_{s,k}^{(l)}}\big)}\nonumber \\
 &- {\Delta} \sum\limits_{j'\in \mathcal{J}_i^{-j} } {\omega_{i,k}^{(j')}\mathcal{N}\big({\bm{\mu}_{i,k}^{(j)}};\bm{\mu}_{i,k}^{(j')},{\bm{\Sigma}_{i,k}^{(j)}} + {\bm{\Sigma}_{i,k}^{(j')}}\big)} \label{eq:minISD-wi}
\end{align}
where $\Delta = {\left| {2 \bm{\Sigma}_{i,k}^{(j)} } \right|}^{1/2} { (2\pi )}^{d/2} $ and $\mathcal{J}_i^{-j} = \{1,..,J_{i,k}\} \setminus j$.
\end{lemma}

\begin{proof}
See Appendix \ref{appendix:iseMIN}. The result is from \eqref{eq:minISD-a}.
\end{proof}

%As the typical case of the constrained multivariate optimization,
%{Obviously,} in \eqref{eq:minISD-wi} depends on the other weights $\omega_{i,k}^{(j')}, j'\in \mathcal{J}_i^{-j} $ in the local GM.
By using \eqref{eq:minISD-wi}, \revisNew{we propose a coordinate descent method
that sequentially updates} the weight of each L-GC $j=1,...,J_{i,k}$ for one or multiple iterations. %in which $\omega_{i,k}^{(j)}$ is calculated based on} the newest available weights of the other L-GCs {in the non-negative space}.
That is, in iteration $t\in \mathbb{N}^+$, $\omega_{i,k}^{(j')}, j'\in \mathcal{J}_i^{-j} $ in the right hand of \eqref{eq:minISD-wi} is defined as follows for calculating $\omega_{i,k}^{(j,t),\text{BFoM}}$,
\begin{equation}\label{eq:omega_iteration}
  \omega_{i,k}^{(j')} :=  \begin{cases}
\omega_{i,k}^{(j',t)} &  j'< j\\
\omega_{i,k}^{(j',t-1)}  & j'>j
\end{cases}
\end{equation}
where $\omega_{i,k}^{(j,t)}$ denotes the updated weight of the $j$th L-GC in iteration $t$ and $\omega_{i,k}^{(j,0)} :=\omega_{i,k}^{(j)}$.

The above \revisNew{coordinate descent} approach to the {CMO} problem {often suffers} from over-fit, {i.e., the non-negative constraint $\{ \omega_{i,k}\} \geq 0$ is violated or the resulting weight is over-large}.
This can be addressed as follows, $\forall i \in \mathcal{I}, j=1,...,J_{i,k}$,
\begin{align}
 \tilde{\omega}_{i,k}^{(j,t)} & = \text{max}\Big( \epsilon,  \omega_{i,k}^{(j,t),\text{BFoM}} \Big) \label{eq:bound-w}
\end{align}
where $\epsilon \geq 0$ is a small constant (e.g., 0.01) specified to avoid generating negative weight. % for which we use $0.1\omega_{i,k}^{(j)}$, i.e. set the weight 90\% lower if it is supposed to be negative according to \eqref{eq:minISD-wi}.

More importantly, we further take the following strategy to avoid over-fit
\begin{align}
 \omega_{i,k}^{(j,t)} & = \alpha_t \tilde{\omega}_{i,k}^{(j,t)} + (1-\alpha_t) \omega_{i,k}^{(j,t-1)} \label{eq:upd-w-alpha}
\end{align}
where $0<\alpha_t<1$ is a scaling factor set for iteration $t$% to balance the final updating weight between the original weight and the new weight given in \eqref{eq:minISD-wi}. It
, which can be interpreted as a learning rate.

{The learning rate $\alpha_t$ may be set simply fixed. % for which we found a good choice often lies in $[0.1, 0.4]$.
However, there is no guarantee for the sequential fit approach with fixed learning rate $\alpha$ to converge to the optimal solution except that $\alpha$ gradually reduces in a proper means with the increase of $t$. In the latter, we suggest the following strategy by employing a hyper-parameter $\beta_t$
\begin{equation}\label{eq:LearningRateRule}
  \alpha_{t+1} = \beta_t\alpha_t
\end{equation}
where $0 < \beta_t \leq 1$ can be interpreted as a fading rate and $\beta_t =1$ means a fixed learning rate.}

The above {GM-weight-fit iteration based on either fixed or adaptive learning rate} may %start from the L-GC with the largest weights and will
be terminated at a convergence level (i.e., local PHDs approximately reach consensus, e.g., $\text{ISD}\big(D_{i,k}\| D_{\text{AA},k}\big)\leq \varepsilon$ where $\varepsilon$ is a small threshold) or up to a maximum number of fit iterations, e.g., $t\leq t_{\text{max}}= 3$. %Notably, as shown in our simulations in section \ref{sec:simulation}, a too large $t_{\text{max}}$ will lead to over-fit especially for the LMB filter. %We leave this open issue to our future work.



%For clarity, we write
%\begin{align}
%  (1-w_i)D_{i,k} & = \sum_{n}^{J_{i,k}}\alpha_n \mathcal{N}(\mathbf{x}; {\bm{\mu}_n},{\mathbf{P}_n}) \nonumber \\
%   \sum_{s\neq i} w_sD_{s,k} & = \sum_{m}^{J_0}\beta_m\mathcal{N}(\mathbf{x}; {\mathbf{m}_m},{\mathbf{S}_m})
%\end{align}
%where
%\begin{align}
%  \alpha_n & = \label{eq:alpha} \\
%  {\bm{\mu}_n} & = \bm{\mu}_{i,k}^{(j)}  \label{eq:mu} \\
%  {\mathbf{P}_n} & = \bm{\Sigma}_{i,k}^{(j)}  \label{eq:P} \\
%  \beta_m  & = w_{s} \omega_{s,k}^{(l)}  \label{eq:beta} \\
%  {\mathbf{m}_m} & = \bm{\mu}_{s,k}^{(l)}  \label{eq:m} \\
%  {\mathbf{S}_m} & = \bm{\Sigma}_{s,k}^{(l)}  \label{eq:S} \\
%  J_0 & = \sum_{s\neq i}J_{s,k}  \label{eq:J}
%\end{align}
%and \eqref{eq:beta}-\eqref{eq:S} indicate an unique mapping
%\begin{equation}\label{eq:j2j'}
%  m \leftrightarrow (j, l)
%\end{equation}
%where $m=1,...,J_0, j\neq i, j \in \mathcal{I}, l=1,...,J_{i,k}$.
%
%Then, the ISD given in \eqref{eq:WeightISDfit} can be expressed as follows
%\begin{align}
%\text{ISD}\big(D_{i,k}\| D_{\text{AA},k}\big)
%% \triangleq & \int{\bigg( \sum\limits_{j=1}^n{\alpha_n \mathcal{N}(\mathbf{x}; {\bm{\mu}_n},{\mathbf{P}_n}) }-\sum\limits_{m=1}^m{\beta_m\mathcal{N}(\mathbf{x}; {\mathbf{m}_m},{\mathbf{S}_m})} \bigg)^2 d \mathbf{x}} \\
%%=& \sum\limits_{n,n'} {{\alpha_n}{\alpha _{n'}}} \int {{p_n}(x)} {p_{n'}}(x)dx \nonumber \\
%%               & + \sum\limits_{m,m'} {{\beta_m}{\beta _{m'}}} \int {{q_m}(x)} {q_{m'}}(x)dx \nonumber\\
%%               &- 2\sum\limits_{n,m} {{\alpha_n}{\beta_m}\int {{p_n}(x)} {q_m}(x)dx} \nonumber\\
%= & \sum\limits_{n,n'} {{\alpha_n}{\alpha _{n'}}\mathcal{N}({\bm{\mu}_n}; {\bm{\mu} _{n'}},{\mathbf{P}_n}+ {\mathbf{P} _{n'}})}
%\label{eq:alphaBetaISD} \\ &
%{\rm{ + }}\sum\limits_{m,m'} {{\beta_m}{\beta _{m'}}\mathcal{N}({\mathbf{m}_m}; {\mathbf{m}_{m'}},{\mathbf{S}_m} + {\mathbf{S}_{m'}})}  \nonumber\\
% & - 2\sum\limits_{n,m} {{\alpha_n}{\beta_m}\mathcal{N}({\bm{\mu}_n}; {\mathbf{m}_m},{\mathbf{P}_n}{\rm{ + }}{\mathbf{S}_m})}
%\end{align}
%%The ISD approach was first proposed for MR in the context of multiple hypothesis tracking in \cite{Williams06}, which inspired the normalized ISD \cite{Petrucci05} and further development \cite{Chen10}. %, and the squared distance \citep{Kurkoski08MR}.
%%One distinctive feature of the method is the availability of exact analytical expressions for GMs. However, the ISD typically has many local minima; hence gradient-based methods cannot guarantee convergence to the global minimum, unless the initialization point happens to be close to the global minimum \cite{Williams06}. Of sufficient similarity to the ISD, %Cauchy-Schwarz inequality \citep{Scott01MR} and
%%Wasserstein distance \cite{Assa18} and Pearson $\chi^2$-divergence \cite{Kitagawa20} are also used for GM reduction.
%
%Here, it is obvious that, $\forall i\in \mathcal{I}$
%\begin{align}\label{eq:derISD}
%  \frac{\partial  \text{ISD}\big(D_{i,k}\| D_{\text{AA},k}\big) }{\partial \alpha_n} & = \sum\limits_{n' \neq n} {{\alpha _{n'}}\mathcal{N}({\bm{\mu}_n};{\bm{\mu} _{n'}},{\mathbf{P}_n}+ {\mathbf{P} _{n'}})}
%\nonumber\\ &
%{\rm{ + }} {2{\alpha _{n}}\mathcal{N}({\bm{\mu}_n}; {\bm{\mu} _{n}},2{\mathbf{P}_n} )} \nonumber\\
% & - 2\sum\limits_{m} {{\beta_m}\mathcal{N}({\bm{\mu}_n};{\mathbf{m}_m},{\mathbf{P}_n}{\rm{ + }}{\mathbf{S}_m})} \\
% \frac{\partial^2 \text{ISD}\big(D_{i,k}\| D_{\text{AA},k}\big)  }{\partial \alpha_n^2}  & = {2 \mathcal{N}({\bm{\mu}_n};{\bm{\mu}_n}, 2{\mathbf{P}_n})} \\
% & >0
%\end{align}
%Setting $\frac{\partial  \text{ISD}\big(D_{i,k}\| D_{\text{AA},k}\big) }{\partial \alpha_n}$ zero and using \eqref{eq:alpha} will lead to
%\begin{align}\label{eq:derISDzero}
% & \omega_{i,k}^{(j)}  = \nonumber \\
% &\frac{2\sum\limits_{m} {{\beta_m}\mathcal{N}({\bm{\mu}_n};{\mathbf{m}_m},{\mathbf{P}_n}{\rm{ + }}{\mathbf{S}_m})}-\sum\limits_{n' \neq n} {{\alpha _{n'}}\mathcal{N}({\bm{\mu}_n};{\bm{\mu} _{n'}},{\mathbf{P}_n} \rm{+} {\mathbf{P} _{n'}})}}{2(1-w_i)\mathcal{N}({\bm{\mu}_n}; {\bm{\mu} _{n}},2{\mathbf{P}_n} )}
%\end{align}
%which yields the minimum ISD.
%%By comparing \eqref{eq:alphaBetaISD} with \eqref{eq:WeightISDfit}, it is obvious

\subsection{Fusion Feedback} \label{sec:fusionfeedback}
\subsubsection{PHD filter} Each L-GC $j=1,...,J_{i,k}$ of sensor $i\in \mathcal{I}$ that is used for representing the PHD will be simply re-weighted as $\omega_{i,k}^{(j,t)}$ as calculated in \eqref{eq:upd-w-alpha} after $t$ GM-weight-fit iterations at time $k$, which can be written as follows
\begin{align}
\omega_{i,k}^{(j),\text{upd}} = \omega_{i,k}^{(j,t)}
\end{align}
\subsubsection{MB/LMB filter}
Based on the unique mapping \eqref{eq:indexMapping}, we get the updated, un-normalized weight for $\iota$th L-GC of the $\ell$th BC/track in sensor $i$ at time $k$ from the fused weight $\omega_{i,k}^{(j,t)}$ as calculated in \eqref{eq:upd-w-alpha} after $t$ GM-weight-fit iterations as follows
\begin{align}
\tilde{\omega}_{i,k}^{(\ell,\iota),\text{upd}} = \omega_{i,k}^{(j,t)}
\end{align}
where $j=1,...,J_{i,k}, \ell \in L_{i,k}, \iota=1,...,J^{(\ell)}_{i,k}$.

According to \eqref{eq:BCweightofGC}, the weight of the L-GCs in each BC/track should then be normalized, i.e.,
\begin{equation}
\omega_{i,k}^{(\ell,\iota),\text{upd}} = \frac{\tilde{\omega}_{i,k}^{(\ell,\iota),\text{upd}}}{\sum _{\iota=1}^{J_{i,k}^{(\ell)}} \tilde{\omega}_{i,k}^{(\ell,\iota),\text{upd}}}
\end{equation}

%We further propose to adjust the existence probability $r_{i,k}^{(\ell),\text{upd}}$ of each BC according to the overall change of the weights of all of its associated L-GCs as follows
%%\eqref{eq:mbPHD_WeightGC}, the new weight $ \omega_{i,k}^{(\ell,\iota),\text{upd}}$ of each L-GC and new should be adjusted as follows
%%\begin{equation}\label{eq:mbPHD_WeightGC_new}
%%\omega_{i,k}^{(l),\text{upd}} = r_{i,k}^{(\ell),\text{upd}}  \omega_{i,k}^{(\ell,\iota),\text{upd}}
%%\end{equation}
%\begin{equation}
%{r}_{i,k}^{(\ell),\text{upd}} = r_{i,k}^{(\ell)} \sum _{l=1}^{J_{i,k}^{(\ell)}} \tilde{\omega}_{i,k}^{(\ell,\iota),\text{upd}}  \\
%\end{equation}
%
%
\subsubsection{CC} \label{sec:CC}
 Regardless that the cardinality is not really used for estimating the number of targets in the local MB/LMB filters \footnote{As usually done in the standard single sensor filter, the number of targets is estimated from the posterior cardinality distribution by taking its mode in the MB filter \cite[Sec. III.D]{Vo09CBmember} while it depends on the existing probability of each track in comparison with application specific thresholds \cite[Sec. IV.E]{Reuter14LMB}.}, it can be used to re-scale the L-GC weights $\omega_{i,k}^{(j)}, j=1,2,...,J_{i,k}$ in the PHD filter and re-scale the track existence probability $r_{i,k}^{(\ell)}, \ell\in L_{i,k}$ in the case of MB/LMB filter, respectively, as follows
\begin{align}
\omega_{i,k}^{(j),\text{upd}} &=  \frac{\omega_{i,k}^{(j,t)} \hat{N}_{\text{AA},k}}{\hat{N}_{i,k}} \label{eq:CC-PHD} \\
  r_{i,k}^{(\ell),\text{upd}} &=  \frac{r_{i,k}^{(\ell)} \hat{N}_{\text{AA},k}}{\hat{N}_{i,k}} \label{eq:CC-MB}
\end{align}


\subsection{Insufficiency for Track Fusion}\label{sec:insufficiencyofB2B}
The proposed unlabeled PHD-AA suits the most the PHD filter but not the MB/LMB filters due to two reasons %, which implicitly or explicitly represent each target using separate tracks/BCs. The unlabeled PHD fusion is insufficient for MB/LMB fusion in twofold as follows.
\begin{remark} \label{remark:insufficientPHD1}
First, the PHD is the first order moment of the MPD for which the PHD-consensus is insufficient for MB/LMB consensus. In other words, two MB/LMB densities can be variant even if their PHDs are the same. This is the limitation of existing RFS-AA fusion approaches {based on the consensus of the PHD not of the MPD} \cite{Li22RFS-AA-Derivation}.
\end{remark}
\begin{remark}\label{remark:insufficientPHD_T2T}
Second, the fusion of the MBs/LMBs needs to properly match/associate the BCs {that represent latently the same target} %in one way or another %(and the MBs in the case of MB mixture fusion \cite{Li23AApmbm}) % and GLMB fusion \cite{Gostar20,Gao20GLMB}
and then carry out the fusion in each associated group of BCs; % which will result in an exactly averaged BC of averaged existing probability \cite{Li19Bernoulli};
see BC-to-BC (B2B) association methods \cite{Li20AAmb,Yi21Heterogeneous} %, label matching methods \cite{Jiang16MB-gci,wang16MB-gci,Gao20GLMB} % SLi17LableInconsistence,%for the detail for grouping the fusing BCs via clustering or one-to-one association
%and the discussion in \cite[Sec.V.F]{Li22RFS-AA-Derivation} %,
in the homogeneous fusion case. While the B2B-based AA fusion will result in an exactly averaged BC \cite{DaKai_Li_DCAI19,Li19Bernoulli}, it is, however, inapplicable when fusing the MB/LMB filters with the PHD filters that do not have distinct BCs.
\end{remark}

To say the least, the label matching is nontrivial as the labels can be very different with each other among diverse LMB filters. This can be illustrated in Fig. \ref{fig:PHDlabel} where four GM-LMBs with completely the same GM parameters $\mathcal{G}_i = \mathcal{G}_j$ but different labels $L_i \neq L_j$, $\forall i \neq j$. Different labeling corresponds to different labeled PHDs and different target-state estimates, although their unlabeled PHDs are identical.
Overall, our {present heterogeneous GM-weight-fit approach} overcomes these challenges as it does neither seek track-to-track fusion nor fuse in the labeled domain. % and the fusion calculation is carried out in an approximate manner. % unless the fusion is carried out with regard to each label/BC. %However, we consider the heterogenous fusion between unlabeled and labeled filters, disregarding the heterogenous data association problem.


\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=8.5cm]{PHDlabel.eps}\\  % _extended
  \caption{Four LMBs of the same unlabeled PHD but different labeled PHDs and different potential target-state-estimates. %for which the label matching is nontrivial.
   } \label{fig:PHDlabel}
  \vspace{-4mm}
\end{figure}


\section{Simulations} \label{sec:simulation}
We consider a ROI given by $[-1\ist\text{km}, 1\ist\text{km}]\times [-1\ist\text{km}, 1\ist\text{km}]$ which is viewed by 4 sensors. % shows the sensor network and the target trajectories.
The target state is denoted as $\mathbf{x}_k=[x_k \; \dot{x}_k \; y_k \; \dot{y}_k]^\text{T}\rmv$ with planar position $[x_k \; y_k]^\text{T}$ and velocity $[\dot{x}_k \; \dot{y}_k]^\text{T}$.
There are totally 12 targets born at different points as shown in Fig. \ref{fig:scenario}.
The target birth is modeled by a MB process with parameters $\{r_{\text{B}} ,p_{\text{B}}^{(\ell)}(\cdot) \}_{\ell=1}^{4}$ at each time-instant, where $r_{\text{B}} =0.03$%
, and $p_{\text{B}}^{(\ell)}(\mathbf{x})=\mathcal{N}(\mathbf{x};\mathbf{\mu}_\text{B}^{(\ell)},\bm{\Sigma}_\text{B})$ with parameters $ \mathbf{\mu}_\text{B}^{(1)}=[0,0,0,0]^\text{T}$, $\mathbf{\mu}_\text{B}^{(2)}=[400\text{m},0,-600\text{m},0]^\text{T}$, $\mathbf{\mu}_\text{B}^{(3)}=[-800\text{m},0,-200\text{m},0]^\text{T}$, $\mathbf{\mu}_\text{B}^{(4)}=[-200\text{m},0,800\text{m},0]^\text{T}$, $
\bm{\Sigma}_\text{B}^{\text{ \ \ \ }}=\mathrm{diag}([10\text{m},10\text{m/s},10\text{m},10\text{m/s}]^\text{T})^{2}$. {We note that this new-born target MB modeling matches the MB/LMB filters but not the PHD filter. %Nevertheless, it turns out that this mismatch does not cause real challenges for the PHD filter.
In practice, however, it is never known what the ground truth is. The local sensors assume the target birth model based on their own prior knowledge or needs. Both Poisson and MB models are reasonable. }

Each target has a constant survival probability $0.95$ and follows a constant velocity motion (with the sampling interval $\Delta =1$s) using noiseless transition density $f_{k|k-1}(\mathbf{x}_{k}|\mathbf{x}_{k-1})=\mathcal{N}(\mathbf{x}_{k};F \mathbf{x}_{k},\mathbf{0})$ for generating the ground truth while the filters use $f_{k|k-1}(\mathbf{x}_{k}|\mathbf{x}_{k-1})=\mathcal{N}(\mathbf{x}_{k};F \mathbf{x}_{k},\mathbf{Q})$, where % $f_{k|k-1}(\mathbf{x}_{k}|\mathbf{x}_{k-1})=%
$$
F=\mathbf{I}_{2}\otimes\left[\begin{array}{cc}
1 & \Delta\\
0 & 1
\end{array}\right], \,\mathbf{Q}=25 \times\mathbf{I}_{2}\otimes\left[\begin{array}{cc}
\Delta^{2}/2 & \Delta/2\\
\Delta/2 & \Delta
\end{array}\right] %,
$$
where $\otimes$ is the Kronecker product.

The simulation is performed 100 runs with conditionally independent measurement series for 100 seconds each run. {In the following, we first} consider four sensors with the same time-invariant target detection probability $p_d = 0.9$ and linear measurement model of senor $s$ as follows
\begin{equation}\label{eq:observationModel}
\mathbf{z}_{s,k}= \left[ \begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 \\
\end{array} \right] \mathbf{x}_k+ \left[ \begin{array}{c}
v_{1,k} \\
v_{2,k} \\
\end{array} \right]  %\hspace{0.5mm},
\end{equation}
with $v_{1,k}$ and $v_{2,k}$ as mutually independent zero-mean Gaussian noise with the same standard deviation of $10$m.

{We then consider four sensors with the same $p_d = 0.9$ but nonlinear measurement model as follows
\begin{equation} \label{eq:RangeBearing}
\mathbf{z}_{s,k}= \begin{bmatrix}
\sqrt{( x_k \!-\! x^{(s)} )^2 + (y_k \!-\! y^{(s)})^2} \,\,\\
\vspace{0.5mm}
\tan^{-1}\!\Big( \frac{ x_k - x^{(s)} }{ y_k - y^{(s)} } \Big)
\end{bmatrix}
+ \begin{bmatrix} v_{s,k}^{(1)} \\[0.5mm] v_{s,k}^{(2)} \end{bmatrix}
\end{equation}
where $x^{(s)}$ and $y^{(s)}$ are the coordinates of sensor $s$ which are $[-500\text{m},-800\text{m}]^\text{T}$, $[-500\text{m},800\text{m}]^\text{T}$, $[600\text{m},800\text{m}]^\text{T}$ and $[600\text{m},-800\text{m}]^\text{T}$, respectively in our case, $v_{s,k}^{(1)}$ and $v_{s,k}^{(2)}$ are independent zero-mean Gaussian with standard deviation $\sigma_1 \!=\! 10\ist$m and $\sigma_2 \rmv=\rmv (\pi/90)\ist$rad, respectively. The field of view of each sensor is a disk of radius $2$km centered at the sensor position $[x^{(s)},y^{(s)}]^\text{T}$ which covers the entire ROI.}

{In both cases, the clutter measurements of different sensors are independent which uniformly distributed over each sensor's field of view for which the number of clutter points at each scan is Poisson with rate $\kappa_s=10$.
%That is, the clutter intensity for the linear sensor is $\kappa_{k}(\mathbf{z}_k) = r_\text{c}/(2\pi \rmv\cdot\rmv 5000)$. where $\theta_{s,k}$ is zero-mean Gaussian with standard deviation $\sigma_\theta$ rad. They are $x_1=0$ m, $y_1=0$ m, $\sigma_r \!=\! 10$ m and $\sigma_\theta = -\pi/180$ rad for sensor 1 and $x_2=500$ m, $y_2=0$ m, $\sigma_r \!=\! 20$ m and $\sigma_\theta = -\pi/90$ rad for sensor 2, respectively.
}


%
%The PMBM filter implementation used a maximum number of global hypotheses
%$N_\text{max}=10$. Pruning was required for the maintenance of both PPP and MBM. Weight threshold $10^{-4}$ was used to prune the low-weighted hypotheses or Poisson components. In the latter, BCs whose existence probability was lower than
%$10^{-4}$ were removed too. Ellipsoidal gating based on the Mahalanobis distance with gate 20 was used for measurement-track association.

The filter performance is evaluated by the optimal subpattern assignment (OSPA) error \cite{Schuhmacher08}, which is given as follows,
%\begin{equation} \nonumber %\label{eq:ospa}
$d^{(c,p)}_\text{ospa}(\hat{{X}}, {X}) = \Big( \big(|\hat{{X}}|\big)^{-1}\big( d_\text{Loc}(\hat{{X}}, {X})   +  d_\text{Card} (\hat{{X}}, {X}) \big) \Big)^{\frac{1}{p}}$ for $|\hat{{X}}| \geq |{X}|$,
%\end{equation}
where the localization error (OSPA Loc) and cardinality error (OSPA Card) are defined as
$ d_\text{Loc}(\hat{{X}}, {X})  = {\mathop {\min }\limits_{\pi  \in {{\rm \Pi} _{|\hat{{X}}|}}} \sum\limits_{i = 1}^{|{X}|} {{d^{(c)}}{{({{X}_i},{\mathbf{\hat{x}}_{\pi (i)}})}^p}} }$ and $ d_\text{Card} (\hat{{X}}, {X})  = { {{c^p}}  (|\hat{{X}}| - |{X}|)}$, respectively.
%\begin{align}
%d_\text{Loc}(\hat{{X}}, {X}) & = {\mathop {\min }\limits_{\pi  \in {{\rm \Pi} _{|\hat{{X}}|}}} \sum\limits_{i = 1}^{|{X}|} {{d^{(c)}}{{({{X}_i},{\mathbf{\hat{x}}_{\pi (i)}})}^p}} } , \label{Carderr} \\
%d_\text{Card} (\hat{{X}}, {X}) & = { {{c^p}}  (|\hat{{X}}| - |{X}|)}. \label{Locerr}
%\end{align}
Here, $\pi$ and $ {\rm \Pi}_n $ are a permutation and the set of all permutations on $\{1,\ldots,n \}$, and $ {d^{(c)}}({X},\mathbf{y}) = \min \left( {d({X},\mathbf{y}),c} \right) $ is a metric between ${X}$ and $\mathbf{y}$ cut-off at $c$. If $|\hat{{X}}| < |{X}|$, $d^{(c,p)}_\text{ospa}(\hat{{X}}, {X}) = d^{(c,p)}_\text{ospa}({X}, \hat{{X}})$, $d_\text{Loc}(\hat{{X}}, {X}) = d_\text{Loc}({X},\hat{{X}})$, $d_\text{Card} (\hat{{X}}, {X})  = d_\text{Card} ({X}, \hat{{X}}) $. In our simulations, we use $c=100$m and $p=2$.

%A key difference of the GOSPA as compared with the popular OSPA \cite{Schuhmacher08} is that it adds up the localization errors for each detected target rather than averaging them. Therefore, localization errors will typically be more significant in the GOSPA.

We consider both homogeneous and heterogeneous fusion cases. In the former, all four sensors run the same PHD, MB or LMB filters, respectively, while in the latter the sensors run different filters. Different levels of fusion have been considered in both cases. They may not cooperate with each other at all (namely noncooperative), only communicate and fuse with each other the estimated number of targets via \eqref{eq:CC-PHD} using $\omega_{i,k}^{(j,t)}=\omega_{i,k}^{(j)}$ or via \eqref{eq:CC-MB} (namely CC only), or perform the proposed GM-PHD-AA fusion in various numbers of GM-weight-fit iterations, $t=1,2,...,6$. %The communication and fusion can be directional or un-directional. In the former, some sensors send its information to, but not receive any information from, some others.
{We use the learning rate $\alpha_1 = 0.2$ or $\alpha_1 = 0.4$  in \eqref{eq:upd-w-alpha} and $\beta_t=0$ (namely fixed learning rate), $\beta_t=0.6$ (namely adaptive learning rate) in \eqref{eq:LearningRateRule}, respectively.}
Furthermore, we use uniform fusing weights, no matter what filters/sensors are involved.

To set up the local filters, the maximum number of L-GCs in the local GM is $200$ for the PHD filters, the maximum number of tracks/BCs is $50$ and the maximum number of L-GC for each track/BC is $20$ in the MB/LMB filters. We note that in practice, these parameters should be designed according to the computational capacity and sensing rate of the local sensors. %There is a significant difference to our proposed, approximate fit approach in the calculation. % and the proposed GM fit can only obtain an approximate PHD-AA.
The other setup of the filters we used, as well as the ground truth, are the same as the standard one as given in the codes released by Vo-Vo at https://ba-tuong.vo-au.com/codes.html. {Codes for our simulations are available soon in the following URL: sites.google.com/site/tianchengli85/matlab-codes/aa-fusion.}

\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=8cm]{Groundtruth.eps}\\
  \caption{ROI and target trajectories starting from $\circ$ and ending at $\triangle$} \label{fig:scenario}
  \vspace{-2mm}
\end{figure}

\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=8cm]{measurements.eps}\\
  \caption{True tracks, measurements and target position estimates in one run by a local GM-PHD filter}
  \vspace{-2mm}
\end{figure}

\subsection{Homogeneous PHD/MB/LMB fusion} \label{sec:homogeneousSim}
We first consider the homogeneous PHD filter fusion in order to show the accuracy of the proposed GM fit approach based on the linear measurement model. {We also test the standard GM-PHD-AA fusion \cite{Li17PC} which averages all of the local GMs. Differently, the size of the local GM is constant in our proposed GM-PHD-fit approach while it increases and so requires appreciate GM reduction such as merging and pruning in the exact GM-PHD-AA fusion.}
The average OSPA errors, cardinality errors and localization errors over 100 runs for each filtering iteration at four sensors are shown in Fig. \ref{fig:PHD-opsa-I=1}. The average OSPA errors of the four PHD filters over all 100 runs of 100 filtering steps, {by using our proposed GM-weight-fit approach for PHD consensus based on either fixed or adaptive learning rate $\alpha_t$ and by using the standard GM-PHD-AA fusion approach, respectively}, have been given in Fig. \ref{fig:HomoPHDcomp}. It shows that the OSPA error has been significantly reduced by the proposed GM-PHD fit approach {especially when $\alpha_1=0.4, \beta_t=0.6$} although its reduction is not so significant as the standard GM-PHD-AA fusion does. {However, the GM-weight-fit approach using fixed learning rate $\alpha_t=0.2$ will diverge after $t \geq 4$ but not in the case using adaptive learning rate, although the latter slows down the convergence somehow and results in comparably less OSPA error reduction.}
%The minimum average OSPA error in the former is about $22$m while it is about $9$m in the latter; this difference can be reduced if the other parameters of the L-GCs are optimized with the weight to better fit the PHD-AA in the proposed approach.

%It is also noted that the exact GM-PHD-AA fusion will lead to almost the same result for all four local filters while these filters still differ from each other after the GM fit fusion. This complies with the fact that the latter only approximates PHD-AA fusion through merely revising the weights of the GM while the former exchanges the whole GM. %while the latter seeks exact PHD-AA fusion regardless of the GM merging error.

{Similarly, we also consider the homogeneous MB filter fusion by using the proposed GM-weight-fit approach for PHD consensus based on either fixed or adaptive learning rate $\alpha_t$ and by using the standard GM-PHD-AA fusion approach \cite{Li17PC}, respectively, in comparison with the standard MB-AA filter fusion based on the B2B association and inter-sensor GM exchange that enable the parallel Bernoulli-AA fusion, where the B2B association is carried out via the optimal assignment based on the Hungarian algorithm or clustering as addressed in \cite{Li20AAmb}. The average OSPA errors are given in Fig. \ref{fig:HomoMBcomp}. The results have demonstrated the effectiveness of the proposed, approximate GM-PHD-AA fit approach for fusing these MB filters especially in case of suitable learning rate and fading rate, although the fusion gain is much lower than the standard MB-AA fusion. Furthermore, we consider the LMB filters for which the relevant average OSPA results are given in Fig. \ref{fig:HomoLMBcomp}. Compared with the PHD and even the MB filters, the fused LMB filters are improved by the GM-weight-fit approach for PHD consensus much less and is prone to over-fit. It will even deteriorate when $t>2$ in the case of using fixed $\alpha_t=0.2$ and even when $t>1$ in case of over-large adaptive learning rate such as $\alpha_t=0.4$.}

{We conjugate that better results can be expected in the case of heterogeneous MB and LMB fusion when {an appropriate B2B association procedure is employed so that the GM-weight-fit can be performed with regard to each matched/associated BC.} To this end, the labels can be removed temporarily so that the LMBs reduce to MBs, %\cite{wang16MB-gci,SLi17LableInconsistence},
which enables BC association/clustering strategies as given for the MB-AA fusion \cite{Li20AAmb}. % when a small number of GM-weight-fit iterations are applied, e.g., $t=1$ or $2$.
Overall, the OSPA reduction in the case of MB and LMB filters is lower than that for the PHD filters---as noted in remark \ref{remark:insufficientPHD1}--- and over-fit is easier to occur in the case of using fixed learning rate. Over-fit (non-convergence) has been significantly reduced or even averted by using proper, gradually-decreasing learning rate. These results confirm the analysis given in Section \ref{sec:insufficiencyofB2B}.}

%The average OSPA error of the proposed GM-AA fit approach for MB and LMB filters are given in Figs. and \ref{fig:LMB-Aver-opsa}, respectively.  The average OSPA errors are also given in Fig. \ref{fig:MB-Aver-opsa}, in which two B2B association approaches perform significantly different with each other in this particular scenario but both outperform the approximate GM fit approach.

%%As addressed in Section \ref{sec:insufficiencyofB2B}, %unlike the GM-PHD, the MB and of LMB filters are so designed that they handle each target/track separately by a BC and the L-GCs belong to different (unlabeled or labeled) BCs/tracks. The
%%the best AA fusion of the MBs/LMBs should be given in a proper B2B manner %match the BCs first in one way or another, namely B2B/label-matching based AA fusion %\cite{Li20AAmb}
%%otherwise the fusion based on mere L-GC re-weighting gains minor in improving the state-estimates of each target.
%We leave the open issues for determining the best number of GM-weight-fit iterations and for label matching to the future work to avoid distracting the reader from our key contribution.

%{However, unfortunately the proposed GM-weight-fit approach does not have an computing efficiency advantage as compared with the standard GM-PHD-AA fusion approach in the homogeneous case. The latter only performs GM merging, pruning and component re-weighting where the re-weighting is performed via simple re-scaling according to the fusion weights, which does not need iterations and is computationally much faster than solving the CMO problem in the proposed GM-weight-fit approach.} %We added discussion on this.

\begin{figure}
  \centering
  \includegraphics[width=8cm]{PHDfitI=1.eps}\\
  \caption{{The OSPA error, cardinality error and localization error of each local PHD filter over time based on only one GM-weight-fit iteration ($\alpha_1=0.2$) for PHD consensus}} \label{fig:PHD-opsa-I=1}
  \vspace{-2mm}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=8cm]{HomoPHDcomp.eps}\\
  \caption{{Average OSPA errors of four PHD filters based on GM-weight-fit for PHD consensus using either fixed or adaptive learning rate in comparison with the standard GM-PHD-AA fusion approach given in \cite{Li17PC}}} \label{fig:HomoPHDcomp}
  \vspace{-2mm}
\end{figure}

%\begin{figure}
%  \centering
%  \includegraphics[width=8cm]{PHDospaHomo.eps}\\
%  \caption{Average OSPA error of each local PHD filter where the GM-PHD-AA fusion is obtained via GM merging as proposed in \cite{Li17PC}} \label{fig:Complete-PHD-Aver-opsa}.
%  \vspace{-2mm}
%\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=8cm]{HomoMBcomp.eps}\\
  \caption{{Average OSPA errors of four MB filters based on GM-weight-fit for PHD consensus using either fixed or adaptive learning rate in comparison with the standard MB-AA filters \cite{Li20AAmb} based on two different B2B association strategies respectively}} \label{fig:HomoMBcomp}
  \vspace{-2mm}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=8cm]{HomoLMBcomp.eps}\\
  \caption{{Average OSPA errors of four LMB filter based on GM-weight-fit for PHD consensus using either fixed or adaptive learning rate}} \label{fig:HomoLMBcomp}
  \vspace{-2mm}
\end{figure}

\subsection{Heterogeneous PHD, MB and LMB fusion}
This section tests the performance of the proposed GM-AA fit approach in the heterogeneous case. The average results of the proposed GM-AA fit approach for two PHD and two MB filters cooperation, for two PHD and two LMB filters and for two-MB and two-LMB filters cooperation, {all filters using adaptive learning rate $\alpha_1=0.2, \beta_t=0.6$,} are given in Figs. \ref{fig:HeterPHDMB}, \ref{fig:HeterPHDLMB} and \ref{fig:HeterMBLMB}, respectively. {As shown, the heterogeneous fusion reduces the OSPA error of the PHD filters and even the MB filters for which more GM-weight-fit iterations lead to greater reduction of the OSPA error. This confirms the effectiveness of the proposed approach for heterogeneous RFS filter fusion. Similar to the case of homogeneous fusion in section \ref{sec:homogeneousSim}, the proposed GM-AA fit approach improves the PHD filter more than the MB/LMB filter {when they are fused with each other}. However, surprisingly, %this is not true with the LMB filter for which
the heterogenous fusion with the MB filter does not benefit the LMB filter more than the CC-only strategy, which means the proposed sequential GM-weight-fit does not improve the LMB filter except the CC part. This is mainly due to the lack of B2B association as addressed in Section \ref{sec:insufficiencyofB2B}. To address it, the fusion needs to be carried out in a track-wise fashion for which appropriate B2B association across sensors is required. We leave this to the future work. } % and the latter are easier to suffer from over-fit than the former.
%In particular, the cardinality averaging lead to worse results for the MB filter. That is, the MB filters improve the PHD filters via the proposed heterogeneous fusion but the reverse is not true. Therefore, a solution in such a case is simply not let the fusion result be feedback to the LMB/MB filter (so that their results will not be affected by the fusion at all), but only the PHD filter of which the performance gain from the fusion. That is, it suggests a directional communication and fusion protocol where the local MB/LMB filters only send their information to the PHD filters but not receive theirs. The results for this setup has been shown in Fig. \ref{fig:2PHD2MBnoMBfusion-Aver-opsa} which shows similar results. That being said,
%This is probably because the proposed GM-PHD-fit completely disregards the labels/assocation of each L-GC to the BCs. A better result can be expected if the fusion regarding the MB and LMB be carried out with regard to each BC in a proper way. We leave this to the future work.


\begin{figure}
  \centering
  \includegraphics[width=8cm]{HeterPHDMB.eps}\\
  \caption{{The OSPA errors of two PHD and two MB filters against different levels of GM-weight-fit using adaptive learning rate $\alpha_1=0.2, \beta_t=0.6$}} \label{fig:HeterPHDMB}
  \vspace{-2mm}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[width=8cm]{HeterPHDLMB.eps}\\
  \caption{{The OSPA errors of two PHD and two LMB filters against different levels of GM-weight-fit using adaptive learning rate $\alpha_1=0.2, \beta_t=0.6$}} \label{fig:HeterPHDLMB}
  \vspace{-2mm}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[width=8cm]{HeterMBLMB.eps}\\
  \caption{{The OSPA errors of two MB and two LMB filters against different levels of GM-weight-fit using adaptive learning rate $\alpha_1=0.2, \beta_t=0.6$}} \label{fig:HeterMBLMB}
  \vspace{-2mm}
\end{figure}

We further consider a more complicated configuration of two PHD filters, one MB filter and one LMB filter. % as linked in Fig. \ref{fig:HFframework}.
The average OSPA error of each filter based on the proposed GM-weight-fit approach {for PHD consensus} is given in Fig. \ref{fig:linearPHDMBLMB}, which confirms that the heterogeneous unlabeled PHD-AA fusion based on GM-weight-fit improve all filters, especially the PHD filters. %It also shows that the best performance of the LMB filter is achieved when the GM-weight-fit for PHD consensus is carried out for 2 iterations. That is, after a number of GM-weight-fit iterations (namely 2 here) for PHD consensus, it is better not let the fusion result be feedback to the LMB and even the MB filters, but only the PHD filter for which a larger but not too large number of PHD GM-weight-fit iterations is better.
However, {it remains open how to best set the learning rate, the fading rate as well as the number of GM-weight-fit iterations for each filter in different cooperation configures. According to our experience, a rule of thumb choice is $\alpha_1=0.2, \beta_t=0.6$.} %, t_\text{max}=5$.} %it remains open how to best determine the most suitable number of GM-weight-fit iterations for each filter in different cooperation configures. {According to our experience with use of learning rate 0.2, the rule of thumb choice for the number of GM-weight-fit iterations is 3 for the PHD filter and 2 for the MB/LMB filters}.
%Instead of specifying the number of GM-weight-fit iterations for fusion feedback for each filter, a technically more sound solution should be based on online evaluating the fusion convergence of the local filter. Fusion should be terminated once the PHD-fit becomes worse with the increase of the number of GM-weight-fit iterations. We leave this issue to the future work.

{Moreover, the average computing times costed by one iteration of the PHD, MB, LMB filtering operation separately and by their fusion operation with different numbers of GM-weight-fit iterations are also given in Fig.~\ref{fig:linearPHDMBLMB}. As shown, the proposed GM-weight-fit approach is computing more costly as compared with the filtering operations in general due to the intensive computation for calculating the correlation among all GCs as in \eqref{eq:minISD-wi}. However, the computation time required increase slowly with the increase of the number of GM-weight-fit iterations. This is because the correlation between GCs only needs to be calculated once (and be recorded for multiple uses in different iterations) because the means and variances of the GCs are unchanged. } %Further strategies are needed to speed up the calculation of the correlation between GCs.  } %immediately when it becomes not changing the local filter density significantly.

{Finally, we test the heterogeneous fusion of two PHD filters, one MB filter and one LMB filter based on the nonlinear measurement model \eqref{eq:RangeBearing} for which the local filters employ the unscented approximation for dealing with the nonlinearity. The average OSPA errors and average computing times for one filtering iteration of each filter and for their heterogeneous fusion are given in Fig. \ref{fig:nonlinearPHDMBLMB}, respectively. The results comply with those in the linear measurement case. Differently, the two PHD filters that are localized at different positions which are related to their respective range-bearing measurements perform quite differently with each other, although both of them are improved by the heterogeneous fusion more significantly as compared with the MB and LMB filters.}


\begin{figure}
  \centering
  \includegraphics[width=9cm]{HeterPHDMBLMB.eps}\\
  \caption{{Linear measurement case: the OSPA errors and average computing time for each filtering step of two PHD, one MB and one LMB filters, and their heterogeneous fusion time against different levels of GM-weight-fit using adaptive learning rate $\alpha_1=0.2, \beta_t=0.6$}} \label{fig:linearPHDMBLMB}
  \vspace{-2mm}
\end{figure}

%\begin{figure}
%  \centering
%  \includegraphics[width=8cm]{ComputingTime.eps}\\
%  \caption{{Average computing times of one filtering step of the respective PHD, MB and LMB filters and their heterogeneous fusion operation against different numbers of GM-weight-fit iterations. }} \label{fig:CompuTime}
%  \vspace{-2mm}
%\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=9cm]{NonlinearHeterPHDMBLMB.eps}\\
  \caption{{Nonlinear measurement case: the OSPA errors and average computing time for each filtering step of two PHD, one MB and one LMB filters, and their heterogeneous fusion time against different levels of GM-weight-fit using adaptive learning rate $\alpha_1=0.2, \beta_t=0.6$}} \label{fig:nonlinearPHDMBLMB}
  \vspace{-2mm}
\end{figure}




\section{Conclusion and Future Work} \label{sec:conclusion}
We propose a heterogenous unlabeled and labeled RFS filter fusion approach which averages the unlabeled PHDs {of the diverse RFS filters} based on the GM implementation.
A computationally efficient, approximate approach to GM weight fit is proposed which sequentially revises only the weights of {the Gaussian components of the local GM in multiple iterations for PHD consensus. Both fixed and adaptive parameters have been suggested for controlling the convergence of the proposed fitting approach}. Simulations have demonstrated the effectiveness of the proposed approach {using adaptive fitting parameters} for both homogeneous and heterogeneous PHD-MB-LMB filter fusion. Our approach improves the PHD filter significantly by cooperating with the MB/LMB filters or the other PHD filters. In comparison, it improves the MB and LMB filters less. The reasons are twofold. % as addressed in remarks \ref{remark:insufficientPHD1} and \ref{remark:insufficientPHD_T2T}. %
First, the PHD is only the first order moment of the MB/LMB density for which the PHD-consensus is insufficient for MB/LMB consensus. Second, the proposed heterogenous fusion is carried out in the unlabeled domain which applies no track/label matching for track-wise fusion. This leaves great space for further improvement.
%However, the proposed approach leads to over-fit easier with the increase of the number of GM-weight-fit iterations in the LMB filter than in the MB/PHD filters.
%It is open to optimally determine the best number of GM-weight-fit iterations in different scenarios or  design a better multivariate optimization solver for the general PHD-GM fit problem. %multi-object optimization problem \eqref{eq:opt_W_ISD}. % such as optimizing all parameters of the L-GCs not only the weights. %Better approximate approach to the PHD-GM-weight-fit remains open.

Improvement can be expected if more parameters of the L-GCs such as the means and covariances can be jointly optimized with the weights for better GM-fit and if the BCs can be properly associated (or the labels can be properly matched) in the case of MB/LMB filter for B2B-based PHD fusion. {To this end, advanced approximation/optimization methods such as variational Bayes and expectation maximization can be considered.} %This may then lead to a hybrid of unlabeled PHD fusion and B2B-based/labeled PHD fusion, as illustrated in Fig. \ref{fig:HHFframework}.
However, label/track matching/association remains an open-ended, challenging issue even for homogeneous MB/LMB filter fusion and needs further investigation. {Valuable future works also include addressing the partially overlapping field of view of the sensors, unknown target birth/dynamics/death models, unknown sensor profiles (such as the target detection probability and clutter rate) and so on based on the framework of the heterogeneous fusion.} %Once this can be properly addressed, the fusion between MB, LMB, and other labeled filters can be performed in the track-wise manner, which can be done in parallel with the %, which together with the proper definition of the divergence/distance between LRFS densities still need further investigation.


%\begin{figure}
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=7cm]{HHsensor.eps}\\  % _extended
%  \caption{Heterogenous unlabeled/labeled RFS filter cooperation based on a hybrid of unlabeled PHD fusion and B2B/labeled PHD fusion accordingly. } \label{fig:HHFframework}
%\end{figure}


\appendix
\subsection{Partial differential of the ISD of GMs} \label{appendix:iseMIN}
The ISD between two GMs $p(\mathbf{x}) = \sum_{n \in \mathbf{I}_1}\alpha_n \mathcal{N}(\mathbf{x}; {\bm{\mu}_n},{\mathbf{P}_n}),
  q(\mathbf{x}) = \sum_{m \in \mathbf{I}_2}\beta_m\mathcal{N}(\mathbf{x}; {\mathbf{m}_m},{\mathbf{S}_m})$ is given as follows {\cite{Williams06}}
\begin{align}
\text{ISD}\big(p\|q\big)
 \triangleq & \int{\big( p(\mathbf{x}) - q(\mathbf{x}) \big)^2 d \mathbf{x}} \\
%=& \sum\limits_{n,n'} {{\alpha_n}{\alpha _{n'}}} \int {{p_n}(x)} {p_{n'}}(x)dx \nonumber \\
%               & + \sum\limits_{m,m'} {{\beta_m}{\beta _{m'}}} \int {{q_m}(x)} {q_{m'}}(x)dx \nonumber\\
%               &- 2\sum\limits_{n,m} {{\alpha_n}{\beta_m}\int {{p_n}(x)} {q_m}(x)dx} \nonumber\\
& = \text{ISD}_{\alpha} + \text{ISD}_{\beta} -2\text{ISD}_{\alpha \beta}
%=  & \sum\limits_{n,n'\in \mathbf{I}_1} {{\alpha_n}{\alpha _{n'}}\mathcal{N}({\bm{\mu}_n}; {\bm{\mu} _{n'}},{\mathbf{P}_n}+ {\mathbf{P} _{n'}})}
% \nonumber \\
% & + \sum\limits_{m,m'\in \mathbf{I}_2} {{\beta_m}{\beta _{m'}}\mathcal{N}({\mathbf{m}_m}; {\mathbf{m}_{m'}},{\mathbf{S}_m} + {\mathbf{S}_{m'}})}  \nonumber \\
% & - 2\sum\limits_{n\in \mathbf{I}_1,m\in \mathbf{I}_2} {{\alpha_n}{\beta_m}\mathcal{N}({\bm{\mu}_n}; {\mathbf{m}_m},{\mathbf{P}_n}{\rm{ + }}{\mathbf{S}_m})}\label{eq:alphaBetaISD}
\end{align}
where
\begin{align}
\text{ISD}_\alpha & = \sum\limits_{n,n'\in \mathbf{I}_1} {{\alpha_n}{\alpha _{n'}}\mathcal{N}({\bm{\mu}_n}; {\bm{\mu} _{n'}},{\mathbf{P}_n}+ {\mathbf{P} _{n'}})}
 \label{eq:alphaISD}\\
\text{ISD}_\beta & = \sum\limits_{m,m'\in \mathbf{I}_2} {{\beta_m}{\beta _{m'}}\mathcal{N}({\mathbf{m}_m}; {\mathbf{m}_{m'}},{\mathbf{S}_m} + {\mathbf{S}_{m'}})}  \label{eq:BetaISD}\\
\text{ISD}_{\alpha \beta} & =  \sum\limits_{n\in \mathbf{I}_1,m\in \mathbf{I}_2} {{\alpha_n}{\beta_m}\mathcal{N}({\bm{\mu}_n}; {\mathbf{m}_m},{\mathbf{P}_n}{\rm{ + }}{\mathbf{S}_m})} \label{eq:alphaBetaISD}
\end{align}
%The ISD approach was first proposed for MR in the context of multiple hypothesis tracking in \cite{Williams06}, which inspired the normalized ISD \cite{Petrucci05} and further development \cite{Chen10}. %, and the squared distance \citep{Kurkoski08MR}.
%One distinctive feature of the method is the availability of exact analytical expressions for GMs. However, the ISD typically has many local minima; hence gradient-based methods cannot guarantee convergence to the global minimum, unless the initialization point happens to be close to the global minimum \cite{Williams06}. Of sufficient similarity to the ISD, %Cauchy-Schwarz inequality \citep{Scott01MR} and
%Wasserstein distance \cite{Assa18} and Pearson $\chi^2$-divergence \cite{Kitagawa20} are also used for GM reduction.

Here, it is straightforward to derive that
\begin{align}
  \frac{\partial  \text{ISD}\big(p\|q\big) }{\partial \alpha_n} = & 2\sum\limits_{n' \neq n, n' \in \mathbf{I}_1} {{\alpha _{n'}}\mathcal{N}({\bm{\mu}_n};{\bm{\mu} _{n'}},{\mathbf{P}_n}+ {\mathbf{P} _{n'}})}
\nonumber\\
 & + {2{\alpha _{n}}\mathcal{N}({\bm{\mu}_n}; {\bm{\mu} _{n}},2{\mathbf{P}_n} )} \nonumber\\
 & - 2\sum\limits_{m\in \mathbf{I}_2} {{\beta_m}\mathcal{N}({\bm{\mu}_n};{\mathbf{m}_m},{\mathbf{P}_n}{\rm{ + }}{\mathbf{S}_m})} \label{eq:derISD}
 \end{align}
 \begin{align}
 \frac{\partial^2 \text{ISD}\big(p\|q\big) }{\partial \alpha_n^2}
 & = {2 \mathcal{N}({\bm{\mu}_n};{\bm{\mu}_n}, 2{\mathbf{P}_n})} \nonumber \\
 & = \frac{2}{{{\left| {{2\bf{P}}_n } \right|}^{1/2}} { (2\pi )}^{d/2} } \label{eq:TwicederISD} \\
 & >0
\end{align}
Setting \eqref{eq:derISD} %$\frac{\partial  \text{ISD}\big(p\|q\big)}{\partial \alpha_n}$
zero will lead to
\begin{align}
\alpha_n & =
% &\frac{\sum\limits_{m \in \mathbf{I}_2} {{\beta_m}\mathcal{N}({\bm{\mu}_n};{\mathbf{m}_m},{\mathbf{P}_n}{\rm{ + }}{\mathbf{S}_m})}} { \mathcal{N}({\bm{\mu}_n}; {\bm{\mu} _{n}},2{\mathbf{P}_n} )}\nonumber \\ &-\frac{\sum\limits_{n' \neq n, n'\in \mathbf{I}_1} {{\alpha _{n'}}\mathcal{N}({\bm{\mu}_n};{\bm{\mu} _{n'}},{\mathbf{P}_n} \rm{+} {\mathbf{P} _{n'}})}}{2 \mathcal{N}({\bm{\mu}_n}; {\bm{\mu} _{n}},2{\mathbf{P}_n} )} \label{eq:minISD-a}
  {{\left| {{2\bf{P}}_n } \right|}^{1/2}} { (2\pi )}^{d/2}\sum\limits_{m \in \mathbf{I}_2} {{\beta_m}\mathcal{N}({\bm{\mu}_n};{\mathbf{m}_m},{\mathbf{P}_n}{\rm{ + }}{\mathbf{S}_m})} \nonumber \\
&- {{\left| {{2\bf{P}}_n } \right|}^{1/2}} { (2\pi )}^{d/2}\sum\limits_{n' \neq n} {{\alpha _{n'}}\mathcal{N}({\bm{\mu}_n};{\bm{\mu} _{n'}},{\mathbf{P}_n} \rm{+} {\mathbf{P} _{n'}})}  \label{eq:minISD-a}
\end{align}
which yields the minimum ISD given all the other L-GC weights $\{\alpha_{n'}\}_{n' \neq n, n' \in \mathbf{I}_1}, \{\beta\}_{m \in \mathbf{I}_2}$.
%By comparing \eqref{eq:alphaBetaISD} with \eqref{eq:WeightISDfit}, it is obvious
%
%
%\subsection{Minimizing the CSD of GMs} \label{sec:iseMIN}
%The CSD between two distributions $p(\mathbf{x})$ and $q(\mathbf{x})$ is defined by:
%\[\begin{aligned}
%{D_{CS}}(q,p) =  &- \log \left( {\frac{{\int q (x)p(x)dx}}{{\sqrt {\int q {{(x)}^2}dx\int p {{(x)}^2}dx} }}} \right) \\
%=  &- \log \left( {\int q (x)p(x)dx} \right) \nonumber \\
%& + \frac{1}{2}\log \left( {\int q {{(x)}^2}dx} \right) + \frac{1}{2}\log \left( {\int p {{(x)}^2}dx} \right)
%\end{aligned}\]
%which, in the case of two GMs $p(\mathbf{x}) = \sum_{n \in \mathbf{I}_1}\alpha_n \mathcal{N}(\mathbf{x}; {\bm{\mu}_n},{\mathbf{P}_n}),
%  q(\mathbf{x}) = \sum_{m \in \mathbf{I}_2}\beta_m\mathcal{N}(\mathbf{x}; {\mathbf{m}_m},{\mathbf{S}_m})$, can be calculated as follows
%%\begin{align}
%%& \text{CSD}(q\|p)
%%= - \log \left( {\sum\limits_{n\in \mathbf{I}_1} {\sum\limits_{m \in \mathbf{I}_2} {{\alpha_n}} } {\beta_m}{z_{nm}}} \right) \nonumber \\
%% & + \frac{1}{2}\log \left( {\sum\limits_{n\in \mathbf{I}_1} {\frac{{\alpha_n^2{{\left| {{\bf{P}}_n^{ - 1}} \right|}^{1/2}}}}{{{{(2\pi )}^{d/2}}}}}  + 2\sum\limits_{n\in \mathbf{I}_1} {\sum\limits_{n' < n} {{\alpha_n}} } {\alpha _{n'}}{z_{nn'}}} \right) \\
%%&+ \frac{1}{2}\log \left( {\sum\limits_{m \in \mathbf{I}_2} {\frac{{\beta_m^2{{\left| {{\bf{S}}_m^{ - 1}} \right|}^{1/2}}}}{{{{(2\pi )}^{d/2}}}}}  + 2\sum\limits_{m \in \mathbf{I}_2} {\sum\limits_{m' < m} {{\beta_m}} } {\beta _{m'}}{z_{mm'}}} \right)
%%\end{align}
%\begin{align}
%& \text{CSD}(q\|p)
%= \frac{1}{2}\log \left( \text{ISD}_\alpha\right)  + \frac{1}{2}\log \left( \text{ISD}_\beta\right)- \log \left(\text{ISD}_{\alpha \beta}\right)
%\end{align}
%where \eqref{eq:alphaISD}, \eqref{eq:BetaISD} and \eqref{eq:alphaBetaISD} were used.
%
%Following \label{eq:derISD} and \label{eq:TwicederISD}, we can get
%\begin{align}
%  \frac{\partial  \text{CSD}\big(p\|q\big) }{\partial \alpha_n} = & \frac{1}{2\text{ISD}_\alpha} \sum\limits_{n' \neq n, n' \in \mathbf{I}_1} {{\alpha _{n'}}\mathcal{N}({\bm{\mu}_n};{\bm{\mu} _{n'}},{\mathbf{P}_n}+ {\mathbf{P} _{n'}})}
%\nonumber\\
% & + \frac{1}{\text{ISD}_\alpha} {{\alpha _{n}}\mathcal{N}({\bm{\mu}_n}; {\bm{\mu} _{n}},2{\mathbf{P}_n} )} \nonumber\\
% & - \frac{1}{\text{ISD}_\alpha\beta} \sum\limits_{m\in \mathbf{I}_2} {{\beta_m}\mathcal{N}({\bm{\mu}_n};{\mathbf{m}_m},{\mathbf{P}_n}{\rm{ + }}{\mathbf{S}_m})} \label{eq:derISD} \\
% \frac{\partial^2 \text{CSD}\big(p\|q\big) }{\partial \alpha_n^2} & >0
%\end{align}
%Setting \eqref{eq:derISD} %$\frac{\partial  \text{ISD}\big(p\|q\big)}{\partial \alpha_n}$
%zero will lead to
%\begin{align}
%\alpha_n & = complicated
%\end{align}
%%which yields the minimum ISD given that all the other L-GC weights $\{\alpha_{n'}\}_{n' \neq n, n' \in \mathbf{I}_1}, \{\beta\}_{m \in \mathbf{I}_2}$ are fixed.
%%By comparing \eqref{eq:alphaBetaISD} with \eqref{eq:WeightISDfit}, it is obvious

\section*{Acknowledgement}
The authors would like to thank the anonymous reviewers for their constructive and detailed comments, which motivated the idea of designing the adaptive learning rate in the proposed GM-weight-fit approach for PHD consensus.


%\section*{References}
%\bibliographystyle{elsarticle-num}
\bibliographystyle{IEEEtran}
\bibliography{HeterogenousFusion}

%\begin{thebibliography}{Reference}
%% \bibitem{label}
%% Text of bibliographic item
%\bibitem{}
%\end{thebibliography}

\end{document}
%\endinput

%%
%% End of file `elsarticle-template-num.tex'.

