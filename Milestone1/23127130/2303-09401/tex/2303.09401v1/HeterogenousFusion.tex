%%
%% Copyright 2007-2019 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references

%% $Id: elsarticle-template-num.tex 164 2019-01-14 09:57:55Z rishi $
%%
%%
\documentclass{IEEEtran}
%\documentclass[landscape,onecolumn,draftclsnofoot,12pt]{IEEEtran}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%\journal{XXX}


\usepackage{balance}
\usepackage{amsmath,graphicx}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{cite}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\usepackage{cuted}
\usepackage{array,multirow}
\usepackage{stfloats}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{algorithm,algorithmic} %
\usepackage{multicol}


\newcommand{\revis}{\textcolor{blue}}
\newcommand{\TSPrevis}{\textcolor{blue}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\ist}{\hspace*{.4mm}}
\newcommand{\rmv}{\hspace*{-.4mm}}

\begin{document}


%\begin{frontmatter}
%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{%Arithmetic Average Density Fusion - Part III:
%Arithmetic average density fusion - part III:
Heterogeneous Unlabeled and Labeled RFS Filter
\\Fusion %based on PHD-AA
\hspace{-0.3mm}for \hspace{-0.3mm}Scalable \hspace{-0.3mm}Multisensor \hspace{-0.3mm}Multitarget \hspace{-0.3mm}Tracking
}%On Arithmetic Average Fusion of Densities: The Good, the Bad and the Ugly*\\
%\title{A Technical Note on (Labeled) RFS-AA Fusion: %All Solidly, Strictly and Simply
%Derivation from PHD Consistency} %Cardinality and
%%% %: Parallel Filtering and Communication without Dissemination Delay --Simultaneous
%\author[address1]{Tiancheng~Li}
%%\author[address2]{Hongqi~Fan}
%%\thanks{Manuscript received}
%\address[address1]{Key Laboratory of Information Fusion Technology (Ministry of Education), School of Automation, Northwestern Polytechnical University, Xi'an 710072, China. E-mail:  t.c.li@nwpu.edu.cn.
%}
%\vspace{-5mm}% <-this % stops a space

\author{Tiancheng~Li,~\IEEEmembership{Senior Member,~IEEE} %
%\thanks{Manuscript preprint: arxiv.org/abs/2209.10433} %preprint is available at https://arxiv.org/abs/2209.10433. Correction has been made. Simulation study has been completed.}
\thanks{This work was partially supported %by Key Laboratory Foundation of National Defence Technology under Grant 61424010306, %by Space Science and Technology
 by National Natural Science Foundation of China (62071389).
% , JWKJW Foundation %(2021-JCJQ-JJ-0897),
% and Key Laboratory Foundation of National Defence Technology (JKWATR-210504).
}
\thanks{T.\ Li is with the Key Laboratory of Information Fusion Technology (Ministry of Education), School of Automation, Northwestern Polytechnical University, Xi'an 710129, China, e-mail: t.c.li@nwpu.edu.cn
}
%\thanks{K. Da and H. Fan are with the Key Laboratory of Science and Technology on ATR,
%National University of Defense Technology, Changsha 410073, Hunan, China.
%e-mail: dktm131@163.com, fanhongqi@nudt.edu.cn}
}


\maketitle

\begin{abstract}
This paper proposes a heterogenous density fusion approach to scalable multisensor multitarget tracking %using a heterogenous network
where the local, inter-connected sensors run different types of random finite set (RFS) filters according to their respective capacity and need. They result in heterogenous multitarget densities that are to be fused with each other in a proper means for more robust and accurate detection and localization of the targets. Our recent work %(Part II of the series of work)
has exposed a key common property of effective arithmetic average (AA) fusion approaches to both unlabeled and labeled RFS filters which are all built on averaging their relevant unlabeled/labeled probability hypothesis densities (PHDs). %, namely the first order moments of the locally estimated multitarget probability densities. %However, existing approaches address only the fusion of homogeneous filters. %An improved PHD estimate will surely improve the filter estimate.
Thanks to this, this
%This
paper proposes the first ever heterogenous unlabeled and labeled RFS filter cooperation approach based on Gaussian mixture implementations where the local Gaussian components (L-GCs) are so optimized that the resulting unlabeled PHDs best fit their AA, regardless of the specific type of the local densities. To this end, a computationally efficient, approximate approach is proposed which only revises the weights of the L-GCs, keeping the other parameters of L-GCs unchanged. % for approximating unlabeled PHD-AA fusion.
%To demonstrate this,
In particular, the PHD filter, the unlabeled and labeled multi-Bernoulli (MB/LMB) filters are considered. % in particular. %, which partially enables parallelization of the communication and the filtering calculation.
Simulations have demonstrated the effectiveness of the proposed approach for both homogeneous and heterogenous fusion of the PHD-MB-LMB filters in different configurations. %In addition, we show negative results of the heterogenous fusion of the unlabeled/labeled MB filter with the PHD filter. % based on the proposed PHD-AA fusion approach. % which ensures consistency in the (labeled) PHD estimation.
\end{abstract}

% Note that keywords are not normally used for peer-review papers.
\begin{IEEEkeywords}
Random finite set, arithmetic average fusion, heterogenous fusion, PHD consistency, multitarget tracking
\end{IEEEkeywords}


%\end{frontmatter}

%% \linenumbers


\section{Introduction}



With the proliferation of internet of things \cite{Qiu18HeteroIoT}, heterogenous sensor networks %\cite{} %Alam17,
become dominant where the sensors have unequal computing and memory capacities, may report different types of measurements and correspondingly run their own suitable algorithms.
In fact, it is also practically necessary to run different algorithms even in a homogeneous sensor network, to the prejudice of the network reliability and lifetime \cite{Yarvis05hetero, Liggins17}. In either case, the local filters are allowed to apply different statistical models (regarding the target birth, detection, survival, and death and the clutter) and algorithms. Thanks to the diversity, their combination is naturally more robust and reliable as compared with the unitary one.
In this paper, we consider such a fundamental setup for scalable multisensor multitarget (MSMT) tracking where different (whether homogenous or heterogenous) sensors run different types of multitarget filters/trackers such as unlabeled and labeled random finite set (RFS) filters \cite{Mahler14book,Vo15mtt}. %This leads to a significant challenge for inter-sensor multitarget information fusion since
As a result,
the locally obtained multitarget densities are heterogenous across the sensors. Yet, it is of significance to fuse them in a proper means which does not demolish the local filters. %can not be done as in any exiting works.

One solution to the above heterogenous filter cooperation could be given by inter-sensor sharing the raw measurements so that the heterogenous filters just use the exchanged measurements of the inter-connected sensors individually. This needs an inter-node communication protocol like flooding \cite{Li17flooding} which is often communication costly and does not suit the large-scale peer-to-peer distributed networks, namely unscalable with the number of sensors. More importantly, it is computationally intractable for the RFS filters to make the optimal use of the measurements of multiple sensors due to the explosive possibility for track-measurement association \cite{Mahler14book,Delande11MsPHD,Nannuru16MsCPHD,Wei16centLMB,Saucan17MsMB,Vo19msGLMB,Si20msPMBM,Robertson22MsLMB,Trezza22Msbirth,Moratuwage22MSMoT}. In this paper, we resort to the computationally efficient, scalable, robust density fusion approach \cite{Li22chapter} to the coordination of the probability hypothesis density (PHD) filter \cite{Vo05,Vo06}, the unlabeled multiple Bernoulli (MB) filter \cite{Vo09CBmember} and the labeled MB (LMB) filter \cite{Vo13Label,Reuter14LMB}. % which remains a significant challenge. %can hardly be satisfied by existing information fusion algorithms.
According to the best of our knowledge, all existing density fusion approaches \cite{Li22chapter,Li22RFS-AA-Derivation} are homogeneous, i.e., fusion is carried out among the same type of filters. Note that the defined heterogeneous fusion is different from %that where the locally estimated distributions describe different states of interest which are subsets of the joint state
what given in
\cite{Petitti11HeterEstimates,Dagan20HeterogeneousChannel,Yi21Heterogeneous,Arulampalam21HeteBearing} where the filters to be fused are still the same type. %,Dagan21HeterogeneousChannel
%Heterogeneous tracks
%In the context of single sensor MODT, a statistically rigorous and promising approach is given by the random finite set (RFS) theory \cite{Mahler14book} based on which different statistical models %(regarding the target birth, detection, survival, and death and the clutter)
%and simplifying approximations lead to a variety of useful RFS filters \cite{Vo15mtt}. % based on the average/consensus fusion \cite{Li22chapter}.

%While there have been considerable solutions for homogenous RFS filter fusion based on either centralized optimal fusion or distributed suboptimal fusion \cite{Li22chapter,Li22RFS-AA-Derivation}, it is still open how to fuse these different types of RFS densities, such as the popular probability hypothesis density (PHD) filter, the multi-Bernoulli (MB) filter, not to say to fuse unlabeled and labeled RFS filters.  %This motivates our work in this paper.

Our proposal in this paper is based on
%An emerging approach to multisensor RFS filter is given by
the arithmetic-average (AA) density fusion \cite{Li2021SomeResults,Li22RFS-AA-Derivation}, which has recently demonstrated promising performance for MSMT tracking %significant advantages in dealing with misdetection, frequent target appearance and unknown inter-filter correlation and in high computing efficiency,
especially in the case of distributed sensor networks.
Effective AA fusion implementations of the unlabeled/labeled RFS filters are strictly derived from the same (labeled) PHD-AA formulation which theoretically ensures consistency in the (labeled) PHD estimation \cite{Li22RFS-AA-Derivation}, resulting in more accurate and robust detection and localization of the present targets.
%An obvious benefit of this common PHD-AA fusion framework is that it
This greatly paves a way for heterogenous RFS filter cooperation via averaging their respective unlabled/labeled PHDs as shown in Fig. \ref{fig:HFframework}. In view of this, this paper presents the first ever heterogenous unlabeled and labeled RFS filter cooperation approach based on approximate unlabeled PHD-AA fusion via a computationally efficient Gaussian mixture (GM) implementation. The proposed heterogenous density fusion allows the local filter to adopt different model assumptions (regarding the target birth, persistency, detection, etc), rendering the maximum flexibility for the local filter design.


%To this end, we propose a heterogenous RFS filter fusion approach where each sensor operates an arbitrary multitarget (labeled) RFS filter according to its own capacity and need which is fused with the others via linearly averaging their (unlabeled) PHDs.

%
%% \subsection{Finite Set Statistics}
%The performance of the homogeneous RFS filter fusion has been well demonstrated with regard to the PHD filter \cite{Li17merging,Li17PC,Li17PCsmc}, % ,Li19Diffusion,Yi20AAfov
%the cardinalized PHD filter \cite{Yu16,Gao20cphd,Da_Li20TSIPN}, the Bernoulli filter \cite{Li19Bernoulli}, the MB filter \cite{Li20AAmb}, the Poisson MB mixture filter \cite{Li23AApmbm}, the labeled MB filter \cite{Gostar20,Gao20GLMB}. %Open-access links to some source codes are available in papers including \cite{Li18FloodingClustering,Ramachandran21,Gao21phdAAlmb} as well as in the following URL: sites.google.com/site/tianchengli85/matlab-codes/aa-fusion. None of these earlier works, however, is on heterogenous RFS filter fusion. %We omit the details of these homogeneous RFS filter fusion approaches. %which %as to be reviewed in this paper. These previous works
%%have demonstrated the performance of the approach

In this paper, the sensors are assumed connected with each other either via a centralized fusion center or via a peer-to-peer distributed network. In the latter, the popular average consensus approach \cite{Olfati-Saber07,Sayed14book} or the distributed flooding approach \cite{Li17flooding} can be used for inter-node information communication. The AA density fusion can be easily applied in both cases. For brevity, this paper focuses on the centralized fusion, omitting the inter-node communication issues.
%However, its application for multisensor RFS density fusion, both centralized and distributed,
We further assume that the %sensor network is connected, i.e., each sensor can be reached from each other sensor by one or multiple communication hops. The
sensors are synchronous and perfectly coordinated, whose field of view cover the region of interest (ROI). %, in which there are a random, time-varying number of targets.
%We further omit the time notation and the dependence of the filter estimate on the observation process.
These limitations can be relaxed to extend our proposed heterogeneous density fusion approach. %The cardinality of an RFS ${X}$ which indicates the number of elements is denoted with $|{{X}}|$.
%{For brevity,} the
The GM implementations of the local PHD \cite{Vo06}, MB \cite{Vo09CBmember} and LMB \cite{Reuter14LMB} filters are the standard as given in these references and details are omitted. % in order to avoid distracting from our key contribution in the fusion algorithm design.

\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=4cm]{Hsensor.eps}\\  % _extended
  \caption{A heterogenous RFS filter cooperation scenario based on unlabeled PHD-AA fusion} \label{fig:HFframework}
  \vspace{-2mm}
\end{figure}


The paper is organized as follows. A brief introduction to the Poisson, unlabeled and labeled MBs and their GM implementations are given in sections \ref{sec:background} \ref{GM-implementation}, respectively. Their heterogenous fusion performed by merely revising the weights of the local Gaussian components (L-GCs) is given in section \ref{sec:GMfit}. %As such, the structure of each local filter does not need to change at all.
Simulation is given in section \ref{sec:simulation} before the paper is concluded in section \ref{sec:conclusion}.

%
%\begin{figure}
%\centering
%\centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=3.5cm]{RFSscen.eps}
%\vspace{-1mm}
%\caption{%Set inclusion relationship among Bernoulli RFS, MB RFS and MBM RFS:
%What need to be estimated in standard RFS trackers: cardinality (the number of targets), state and label (trajectory of each target).}
%%% \colBlue{In particular, target 8 exists only for a short period of time.} }
%\label{fig:RFSscen}
%\vspace{-3mm}
%\end{figure}


\section{Preliminaries: PHD, MB and LMB} \label{sec:background}
\subsection{Basic MSMT Scenario}
The following scenario assumptions are made in this paper. Target births are independent of target survivals, which might be modelled differently at local sensors based on either Poisson or MB RFSs. Each target evolves and generates measurements independently. %The surviving process of each target is Bernoulli. That is, at
At time $k-1$, the target with state $\mathbf{x}_{k-1} \in \mathcal{X}$ will either die with probability $1-p^{\text{s}}_k(\mathbf{x}_k)$ or persists at time $k$ with survival probability $p^{\text{s}}_k(\mathbf{x}_k)$ and attains a new state $\mathbf{x}_k$ according to a Markov transition probability density function (PDF) $f_{k|k-1} (\mathbf{x}_k|\mathbf{x}_{k-1})$. Hereafter, $\mathcal{X} \subseteq \mathbb{R}^d$ denotes the $d$-dimensional state space.


%The target measurement at each sensor is also Bernoulli. That is, given
Given a target with state $\mathbf{x}_k \in \mathcal{X}$% at time $k$
, sensor $i \in \mathcal{I}$ either detects it with probability $p^{\text{d}}_{i,k}(\mathbf{x}_k)$ %(independent of the state)
and generates a measurement $\mathbf{z}_{i,k}\in \mathbf{Z}_{i,k}$  with likelihood $g_{i,k}(\mathbf{z}_{i,k}|\mathbf{x}_k)$ or fails to detect it with probability $1-p^{\text{d}}_{i,k}(\mathbf{x}_k)$, where $\mathbf{Z}_{i,k}$ denotes the random finite set of the measurements received at time $k$ by sensor $i$.
The clutter (namely the measurement of no target) follows a Poisson RFS, independent of target measurements. % and independent among sensors. %The Poisson clutter intensity $\kappa_{i,k}$ at each sensor $i$ is independent of the target measurements.
%The states of a random number of targets can be described by an RFS ${X}_t = \big\lbrace \mathbf{x}_{t,1}, \dots, \mathbf{x}_{t,n} \big\rbrace \in \mathbb{X}$, where $n =|{{X}_t}|$ denotes the random number of targets at time $t$, %namely the cardinality of the set,
%and $\mathbf{x}_{t,i}\in \mathcal{X}$ is the state vector of the $i$-th target at time $t$ in the $d$-dimensional state space $\mathcal{X} \subseteq \mathbb{R}^d$.

\subsection{RFS Modeling}
% Mahler14book
%\begin{equation}
%  {X} = \big\lbrace \mathbf{x}_{1}, \dots, \mathbf{x}_{n} \big\rbrace \in \mathbb{X}
%\end{equation}
The states of a random number of targets are described by a RFS ${X} = \big\lbrace \mathbf{x}_{1}, \dots, \mathbf{x}_{n} \big\rbrace \subseteq \mathbb{X}$, where $n =|{X}|$ denotes the random number of targets and $\mathbb{X}$ denotes all of the finite subsets of $\mathcal{X}$.
%The random nature of the multitarget RFS ${X}$ is captured by its multitarget probability density (MPD), denoted by $f({X})$.
For any realization of $X$ with a given cardinality $|{{X}}| =n$, namely ${X}_n = \big\lbrace \mathbf{x}_{1}, \dots, \mathbf{x}_{n} \big\rbrace$, \cite[Eq.2.36]{Mahler14book},
the multitarget probability density (MPD) is defined as
%\begin{equation}\label{eq:fisst_mttPDF}
$f({X}_n) = n! \rho(n) f(\mathbf{x}_{1}, \dots, \mathbf{x}_{n})$
%\end{equation}
where the localization densities $f(\mathbf{x}_{1}, \dots, \mathbf{x}_{n} )$ are symmetric in their arguments and the cardinality distribution is given by $\rho(n)\triangleq \mathrm{Pr}\{|{{X}}|=n\} = \int_{|{X}| = n} {f({X})\delta {X}}$.
%is given by %\cite[Ch. 4.2.6]{Mahler14book}
%%\begin{align}
%$  \rho(n) % & \label{eq:def-card-mpd} \\
% = {\frac{1}{{n!}}\int_{\mathcal{X}^n} {f\big(\{ {\mathbf{x}_1},\dots,{\mathbf{x}_n}\} \big)d{\mathbf{x}_1}\dots d{\mathbf{x}_n}}}$.
%%\end{align}
%% and $ \left | X  \right |  $ denotes the cardinality of set $X$.
The set integral in $\mathbb{X}$ is defined as \cite[Ch. 3.3]{Mahler14book}
%\begin{align}
$\int_{\mathbb{X}} {f({X})\delta {X}}
    % = \sum\limits_{n = 0}^\infty  {\frac{1}{{n!}}\int_{S^n} {f(\{ {x_1},\dots,{x_n}\} )d{x_1}\dotsd{x_n}} } \\{\text{ \qquad \qquad \quad = }}
   = %\rho(0) + %f(\emptyset ) %
\sum\limits_{n = 0}^\infty  {\frac{1}{{n!}}\int_{\mathcal{X}^n} {f\big(\{ {\mathbf{x}_1},\dots,{\mathbf{x}_n}\} \big)d{\mathbf{x}_1}\dots d{\mathbf{x}_n}}}
$ %\label{eq:set-integral-expansion}%\\
%   &= \sum\limits_{n = 0}^\infty \rho(n)
%\end{align}
where $f(\emptyset )= \rho(0)$.
%which %, similar to the conventional integral,
%is linear with regard to ${f}$, i.e.,
%$\int_\mathbb{X} {\left( {\alpha {f_1}({X}) + \beta {f_2}({X})} \right)\delta {X}}  = \alpha \int_\mathbb{X} {{f_1}({X})\delta {X}}  + \beta \int_\mathbb{X} {{f_2}({X})\delta {X}}$ and


%From above, it is easy to see that the cardinality distribution $\rho(n)$ of the RFS ${X}$ is given as \cite[Ch. 4.2.6]{Mahler14book} $\rho(n) = \int_{|{X}| = n} {f({X})\delta {X}}$.
% \begin{align}
%   \rho(n) & = \int_{|{X}| = n} {f({X})\delta {X}}  \label{eq:def-card-mpd} \\
%   &  = \frac{1}{{n!}}\int_{\mathcal{X}^n} {f(\{ {\mathbf{x}_1},\dots,{\mathbf{x}_n}\} )d{\mathbf{x}_1} \dots d{\mathbf{x}_n}}  \label{eq:def-card-mpd-expansion}
% \end{align}
The PHD $D(\mathbf{x})$, also known as the first moment density \cite[pp. 168-169]{Goodman97},
of the multitarget density $f({X})$ is a density function on single target $\mathbf{x}\in X$, defined as \cite[Ch.4.2.8]{Mahler14book} %\cite[Ch.4.2.8]{Mahler14book}
\begin{align}
D(\mathbf{x})   %&= \int_\mathbb{X}  {f\big( {\left\{ \mathbf{x} \right\} \cup X} \big)\delta X}\nonumber \\
& \triangleq \int_\mathbb{X}   {\bigg(\sum_{\mathbf{y}\in X}{\delta_\mathbf{y}}(\mathbf{x}) \bigg)f(X)\delta X} \label{def-PHD}
\end{align}
where ${\delta_\mathbf{y}}(\mathbf{x}) = 1$ if $\mathbf{y} = \mathbf{x}$ and ${\delta_\mathbf{y}}(\mathbf{x}) = 0$ otherwise. %,which indicates that the PHD is a first moment density in the point process sense \cite[pp. 168-169]{Goodman97}.

The PHD has clear physical significance as its integral in any region $\mathcal{S} \subseteq \mathcal{X} $ gives the expected number $\hat{N}^{\mathcal{S}}$ of targets in that region, i.e.,
%\begin{equation}\label{eq:def-phd-integral=hatN}
$  \hat{N}^{\mathcal{S}} = \int_{\mathcal{S}} D(\mathbf{x}) d \mathbf{x}$.
%\end{equation}
Arguably, it tells how well the present targets are detected in any local region. The PHD plays a key role in all RFS filters and undoubtedly, a good PHD estimate implies a good filter estimate.
To distinguish the targets, one needs to use the labeled RFS (LRFS) which is an RFS whose elements are assigned with distinct labels \cite{Vo13Label,Vo14GLMB}. %$\int f({\mathbf x})d{\mathbf x}=\sum_{\ell\in\mathbb{L}}\int f(x,\ell)dx$.
Denote by $\mathcal{L}$ the label space and by $\mathbb{L}$ all of the finite subsets of $\mathcal{L}$.
A realization of an LRFS with cardinality $n$, multitarget state $X_n$ and label set $L_n = \big\lbrace l_{1}, l_{2}, \dots, l_{n} \big\rbrace \subseteq \mathbb{L}$ is denoted by $\widetilde{X}_n = \{ (\mathbf{x}_1, l_1),(\mathbf{x}_2, l_2),...,(\mathbf{x}_n, l_n) \} \subseteq \mathbb{X} \times \mathbb{L}$. The LRFS is completely characterized by its multitarget density $\pi\big(\widetilde{X}\big)$. % which is a joint distribution of the state and label.
Consequently, %the concept of PHD shall be extended to the labeled domain.
the labeled and unlabeled PHD for a labeled RFS are respectively given as follows \cite{Vo13Label}
\begin{align}
 % \rho(n) &=  \sum\limits_{c \in \mathbb{C}} \sum\limits_{L \subseteq \mathbb{L}} {{\delta_n}[|L|] {\omega ^{(c)}}(L)}  \label{eq:glamb-cardinality-distribution} \\
  {\widetilde D} (\mathbf{x},l) & \triangleq \int_{\mathbb{X} \times \mathbb{L}}  {\pi}\big((\mathbf{x},l)\cup {\widetilde X}\big)\delta {\widetilde X} \\ %\label{eq:def-LRFS-phd-label} \\
  %&= \sum\limits_{c \in \mathbb{C}}{{s^{(c)}}(\mathbf{x},l)}\sum\limits_{L \subseteq \mathbb{L}} {{\mathrm{1}_L}(l) {\omega ^{(c)}}(L)}  \label{eq:glamb-Lphd} \\
  D(\mathbf{x}) & \triangleq \sum\limits_{l \in \mathbb{L}} {\widetilde D}(\mathbf{x},l) \label{eq:def-LRFS-phd-nolabel} %\\
  %&= \sum\limits_{c \in \mathbb{C}}\sum\limits_{L \subseteq \mathbb{L}} {{\omega ^{(c)}}(L) {\sum_{l\in L} {s^{(c)}}(\mathbf{x},l)}}  \label{eq:glamb-phd}
\end{align}
%where ${\mathrm{1}_x}(y) = 1$ if $y \subseteq x$ and ${\mathrm{1}_x}(y) = 0$ otherwise and the projection function $\mathcal{L}(\mathbb{X} \times \mathbb{L}) \rightarrow \mathbb{L}$ is given by $\mathcal{L}\big((\mathbf{x},l)\big) = l$.

In this paper, what indicated by the PHD is the unlabeled PHD by default unless otherwise stated.



\subsection{PHD of the Poisson, MB and LMB} \label{sec:Classic-RFS-distributions}
We hereafter omit the time notation and the dependence of the filter estimate on the observation process.
\subsubsection{Poisson RFS}

The MPD and PHD of the Poisson RFS $X$ with mean $\lambda$ are, respectively, given by
\begin{align}
	f^{\text {p}}(X) = \,\,& {{\text{e}}^{ - {\lambda}}}\prod\limits_{\mathbf{x} \in X} {{\lambda}{s}(\mathbf{x})}  \label{eq:Poisson-PD} \\
%= \,\, & e^{-\int_{\mathcal{X}} D^{\text {p}}(\mathbf{x})d \mathbf{x}} \prod_{\mathbf{x}\in X}D^{\text {p}}(\mathbf{x}) \\
	D^{\text {p}}(\mathbf{x}) =\,\,& \lambda {s}(\mathbf{x}) \label{eq:Poisson-PHD}
\end{align}
where $s(\mathbf{x})$ denotes the single-target probability density (SPD).


\subsubsection{MB}
%The density and PHD of a Bernoulli RFS $X^{\text{b}}$ with target existence probability $r$ and SPD $s(\mathbf{x})$ are, respectively, given by
%\begin{align}
%f^{\text{b}}\left(X\right) & = \begin{cases}
%1-r & X  =\emptyset\\
%rs\left(\mathbf{x}\right) & X =\left\{ \mathbf{x}\right\} \\
%0 & \mathrm{otherwise}
%\end{cases} \label{eq:Bernoulli-PD} \\
%D^{\text{b}}\left(\mathbf{x}\right) & = rs\left(\mathbf{x}\right) \label{eq:Bernoulli-PHD}
%\end{align}

An MB RFS $ X_n$ is the union of $n$ independent Bernoulli RFSs  \cite{Vo09CBmember} which can represent maximum $n$ targets. %, which is an efficient approximation to the multitarget Bayes filter.
Denoting the $\ell$-th Bernoulli component (BC) by $\big(r^{(\ell)},s^{(\ell)}(\mathbf{x})\big)$, the MPD and PHD of MB RFS $ X_n$ are, respectively, given by
\begin{align}
	f^\text{mb} (X_n) & = \sum_{\uplus_{\ell=1}^{n}X^{(\ell)}=X}\prod_{\ell=1}^{n}f^\text{b} \left(X^{(\ell)}\right) \label{eq:def_MB}\\
	D^{\text {mb}}(\mathbf{x}) & = \sum_{\ell=1}^n{r^{(\ell)}}{s^{(\ell)}}(\mathbf{x})  \label{eq:PHD-MB}
\end{align}
where $\uplus$ denotes the disjoint union and the density of a Bernoulli RFS $X^{(\ell)}$ with target existence probability $r^{(\ell)}$ and SPD $s^{(\ell)}(\mathbf{x})$ is given by
\begin{align}
f^{\text{b}}\left(X^{(\ell)}\right) = \begin{cases}
1-r^{(\ell)} & X^{(\ell)}  =\emptyset\\
r^{(\ell)}s^{(\ell)}\left(\mathbf{x}\right) & X^{(\ell)} =\left\{ \mathbf{x}\right\} \\
0 & \mathrm{otherwise}
\end{cases} \label{eq:Bernoulli-PD} % \\
\end{align} %, the summation is taken over all mutually disjoint (and possibly empty) Bernoulli sets $X_1^\text{b},...,X_n^\text{b}$ whose union is $X^\text{mb}$.
%\newpage

%When multiple MBs are used jointly and associated with different measurement-to-track hypotheses or labels, they result in the PMBM \cite{Williams15taesPMBM}, the MBM \cite{Angel18PMBMdeivation} and the GLMB \cite{Vo13Label,Vo14GLMB} which are all conjugate prior for multitarget filtering. %They are the exact Bayes-optimal multiple target filter.

%\subsubsection{$\delta$-GLMB}
%The MPD and labeled PHD of a $\delta$-GLMB RFS $\widetilde{X}$ are respectively given by \cite{Vo13Label}
%\begin{align} %\label{eq:delta-GLMB}
%	\pi^{{\delta}}\big(\widetilde{X}\big) %= \Delta \big(\widetilde{X}\big)\sum\limits_{(L,\xi) \subseteq \mathbb{L} \times \Xi} {{\omega^{(L,\xi )}}{\delta _L}[\mathcal{L}\big(\widetilde{X}\big)]{{\left[ {{s^{(\xi )}}} \right]}^X}}.
%& = \Delta \big(\widetilde{X}\big) \sum\limits_{L \subseteq \mathbb{L}} {\delta _L}\big[\mathcal{L}\big(\widetilde{X}\big)\big]  \sum\limits_{\xi \in \Xi} {{\omega^{(L,\xi )}}\prod\limits_{(\mathbf{x},l) \in \widetilde{X}} {{s^{(\xi)}}(\mathbf{x},l)}} \nonumber \\
% % \rho(n) &= \sum\limits_{L \subseteq \mathbb{L}} {{\delta_n}\big(|L|\big) \sum\limits_{\xi \in \Xi} {\omega^{(L,\xi )}}}  \label{eq:glamb-cardinality-distribution} \\
%  {\widetilde D}(\mathbf{x},l) %& \triangleq \int  {\pi}\big((\mathbf{x},l)\cup {\widetilde X}\big)\delta {\widetilde X} \label{eq:def-LRFS-phd-label} \\
%  &=  \sum\limits_{L \subseteq \mathbb{L}} {{\mathrm{1}_L}(\ell) \sum\limits_{\xi \in \Xi} {\omega^{(L,\xi )}} {{s^{(\xi)}}(\mathbf{x},l)} } \label{eq:glamb-phd} % \\
%%  D(\mathbf{x}) & \triangleq \sum\limits_{l \in \mathbb{L}} D(\mathbf{x},l) \label{eq:def-LRFS-phd-nolabel} \\
%%  &= \sum\limits_{L \subseteq \mathbb{L}} { \sum\limits_{\xi \in \Xi} {\omega^{(L,\xi )}}  {\sum_{l\in L} {s^{(\xi)}}(\mathbf{x},l)}}  \label{eq:glamb-phd}
%\end{align}
%where the distinct label indicator $\Delta \big(\widetilde{X}\big)= \delta_{|\widetilde{X}|}\big[\mathcal{L}\big(\widetilde{X}\big)\big]$, ${\delta_I}[L] = 1$ if $I=L$ and ${\delta_I}[L] = 0$ otherwise, the projection function $\mathcal{L}(\mathbb{X} \times \mathbb{L}) \rightarrow \mathbb{L}$ is given by $\mathcal{L}\big((\mathbf{x},l)\big) = l$, the pair $(L,\xi)$ represents the hypothesis that the track set $L$ has a history $\xi$ of measurement-track association maps \cite{Vo14GLMB} and ${\omega^{(L,\xi )}} $ is the weight of the hypothesis $(L,\xi)$, the respective labeled SPDs $ s^{(\xi)}(\mathbf{x}_1,l_1),\ldots, s^{(\xi)}(\mathbf{x}_n,l_n)$ satisfy
%$\sum\limits_{L \subseteq \mathbb{L}} {\sum\limits_{\xi \in \Xi} {\omega^{(L,\xi )}} }  = 1, \int_{\mathcal{X}} {{s^{(\xi)}}(\mathbf{x},{l})d\mathbf{x}}  = 1$.

\subsubsection{LMB}
%Treating the index to each BC in the MB as the unique label distinguishing different targets will lead to the MPD resembling the labeled MB. That is, the
Similar to the MB filter, the LMB filter \cite{Reuter14LMB} associates each labeled BC $l \in L \subseteq \mathbb{L}$ with SPD ${s}(\mathbf{x},l) $ and existence probability $r^{(l)}$. The MPD and labeled PHD are, respectively, given by
\begin{align}
	\pi^{\text {lmb}}(\widetilde{X}) &= \Delta \big(\widetilde{X}\big) \omega\big(\mathcal{L}\big(\widetilde{X}\big)\big) \prod\limits_{(\mathbf{x},l) \in \widetilde{X}} {{s}(\mathbf{x},l)} \label{eq:LMB} \\
{\widetilde D}^{\text {lmb}}(\mathbf{x},l) %&= \sum\limits_{L \subseteq \mathbb{L}} {\mathrm{1}_L}(l)  {\omega{(L)} {{s}(\mathbf{x},l)}} \nonumber \\
& = r^{(l)} {s}(\mathbf{x},l) \label{eq:LMB-phd} % \sum_{l\in L}
\end{align}
where the distinct label indicator $\Delta \big(\widetilde{X}\big)= \delta_{|\widetilde{X}|}\big(\mathcal{L}\big(\widetilde{X}\big)\big)$, ${\delta_I}(L) = 1$ if $I=L$ and ${\delta_I}(L) = 0$ otherwise, the projection function $\mathcal{L}(\mathbb{X} \times \mathbb{L}) \rightarrow \mathbb{L}$ is given by $\mathcal{L}\big((\mathbf{x},l)\big) = l$, $\int_{\mathcal{X}} {{s}(\mathbf{x},{l})d\mathbf{x}}  = 1$, and $\omega(L)$ denotes the hypothesis weight corresponding to the label set $L \subseteq \mathbb{L}$, which is given by
\begin{equation}\label{eq:w(L)}
  \omega(L) = \prod_{i\in \mathbb{L}} (1-r^{(i)}) \prod_{l\in L} \frac{{\mathrm{1}_\mathbb{L}}(l)  r^{(l)}}{1-r^{(l)}}
\end{equation}
where ${\mathrm{1}_\mathbb{L}}(l) = 1$ if $l \subseteq \mathbb{L}$ and ${\mathrm{1}_\mathbb{L}}(l) = 0$ otherwise and $\sum\limits_{L \subseteq \mathbb{L}} {\omega[L]} = 1$

The unlabeled PHD of the LMB is defined as follows
\begin{align}
{D}^{\text {lmb}}(\mathbf{x}) &= \sum\limits_{l \in L } {\widetilde D}^{\text {lmb}}(\mathbf{x},l) \nonumber \\
& = \sum\limits_{l \in L } r^{(l)} {s}(\mathbf{x},l) \label{eq:LMB-phd} % \sum_{l\in L}
\end{align}

%, and $r^{(l)}$ is the target existing probability corresponding to the label $l\in L$. %$w\big[\mathcal{L}\big(\widetilde{X}\big)\big] = (1-r)^{\mathbb{L}-\mathcal{L}\le
%
%\section{PHD-Consistency and PHD-AA Fusion} \label{PHD-Consistency}
\subsection{PHD-AA Consistency}
For a set of PHDs $D_i(\mathbf{x})$ produced by RFS filter $i \in \mathcal{I} =\{1,2,...,I\}$, the AA fusion is simply given as follows
\begin{equation}\label{eq:PHD-AA}
{D_{\text{AA}}}(\mathbf{x}) \triangleq \sum\limits_{i \in {\mathcal{I}}} {{w_i}{D_i}(\mathbf{x})}
\end{equation}
where the fusion weights $\mathbf{w} =\{w_1,\dots,w_I\} \in \mathbb{W} \subset \mathbb{R}^{I}$, the weight space $\mathbb{W} \triangleq\{\mathbf{w} \in \mathbb{R}^{I}|\mathbf{w}^\mathrm{T}\mathbf{1}_I = 1, w_i > 0, \forall i \in \mathcal{I}\}$.
%The MPD-AA fusion is given by % leads to
%\begin{equation}\label{eq:RFS-AA-Whole}
%{f_{\text{AA}}}({X}) \triangleq \sum\limits_{i \in {\mathcal{I}}} {{w_i}{f_i}({X})}
%\end{equation}
%which corresponds to

The AA is a Fr\'{e}chet mean in the sense of the integral square error (ISE) \cite{Li20AAmb}, i.e.,
\begin{equation}
  D_\mathrm{AA}(\mathbf{x})  = \operatorname*{arg\,min}_{g\in\mathcal{F}_{\mathcal{X}}} \sum\limits_{i \in {\mathcal{I}}}{w_i\int_{\mathcal{X}} \big(D_i(\mathbf{x})-g(\mathbf{x})\big)^2 \delta \mathbf{x}}   \label{eq:FrechetAAISE}
\end{equation}
where $\mathcal{F}_\mathcal{X} \triangleq \{f: \mathcal{X} \rightarrow \mathbb{R} \}$ %$\mathcal{F}_{\mathcal{X}}$ denotes the set of scalar-valued functions in space $\mathcal{X}$: $\mathcal{F}_{\mathcal{X}} = \{f: \mathcal{X} \rightarrow \mathbb{R} \}$.

Relevantly, it is more known as follows, which we refer to as the best fit of the mixture (BFoM) %BFoM, c.f., \eqref{eq:AA-density-KLD},
\begin{equation}\label{eq:RFS-AA-Whole-KLD}
 {D_{\text{AA}}}(\mathbf{x}) = \operatorname*{arg\,min}_{g\in\mathcal{F}_\mathcal{X}} \sum\limits_{i \in {\mathcal{I}}} {w_iD_\text{KL}\big(D_i\|g\big)}
\end{equation}
where the Kullback-Leibler (KL) divergence is defined as $D_\text{KL}\big(f\|g \big) \triangleq \int_\mathcal{X} {f(\mathbf{x})\log \frac{f(\mathbf{x})}{g(\mathbf{x})} \delta \mathbf{x}}$.
%$\mathcal{F}_\mathcal{X} \triangleq \{f: \mathbb{X} \rightarrow \mathbb{R} \}$ and
%the set

%\subsection{Motivation for PHD-AA Fusion} \label{sec:Motivation-phd-AA-fusion}
%The PHD is a key statistical character of the RFS distribution, and so %whose integral in any region $\mathcal{S}\subseteq \mathcal{X}$ gives the estimated number of target in that region. Therefore,
%an accurate PHD estimate is vital in any RFS filters. On this point, we present the following definition relative to the effectiveness of the multisensor fusion and important result on the PHD-AA fusion. % which provides a unique means to approximate the MPD by respective
%%The PHD itself has been used for filter recursion of the Poisson point process (PPP), namely the PHD filter
%\begin{definition}[PHD Consistency] The accuracy of the multisensor fused PHD increases with the number of fusing estimators if the fusing weights are properly designed. That is, the PHD consistent fusion should result in statistically more accurate detection of the targets and the fused PHD converges to the first moment of the true MPD as the number of the fusing estimators increases indefinitely. %goes to infinity. % in the area. %, given by the integral of the fused PHD in that region. %This requires a linear fusion of the PHDs.
%\end{definition}
%
%
%We note that our above definition abides with the standard definition of the consistency of an unbiased estimator, i.e., as the number of data points used increases indefinitely, the resulting sequence of estimates converges in probability to the ground truth. %Here, in our case, the PHD consistency equivalently means that as the number of fusing estimators increase indefinitely, the fused PHD converges to the first moment of the true MPD.
%This is different from the concept of consistency given in \cite{Julier06consistency,Uney19consistency}, which means something else. % and is unrelated with the increase of the number of fusing estimators. %a probability density that at no point in the state space overlooks the density of their components.
%%\cite{Julier06consistencyUney19consistency} proposed the concept of consistency in cardinality and demonstrated that the GA fusion of finite-set distributions does not necessarily lead to consistent fusion of cardinality distributions.
%


%Assume that the fusing estimators are conditionally statistically independent with each other and their PHD estimates are unbiased in any region $\mathcal{S}\subseteq \mathcal{X}$. % i.e., $\mathbb{E}[\int_{\mathcal{S}} D_i(\mathbf{x})d \mathbf{x}]= N^{\mathcal{S}}$,
The PHD-AA fusion \eqref{eq:PHD-AA} strictly ensures consistency as long as the fusing filters are properly designed; see the analysis given in \cite{Li22RFS-AA-Derivation}.
%\end{lemma}
%\begin{remark} \label{remark:cardinality-wise-AA}
In addition, the mixing operation and averaging calculation embedded in the AA density fusion can reduce the effects of clutter and missed detections \cite{Li17PC,Li17PCsmc}, % since the mixing operation and averaging calculation are insensitive to false and missing data,
improving the robustness of the local filters. We further highlight the following point%In particular, in the case of low detection profile, the AA fusion has a significant advantage as compared with the geometric average fusion approach.
\begin{remark}
While the AA fusion \eqref{eq:PHD-AA} can be applied to any homogeneous or heterogeneous distributions (even they are defined on different domains), the standard ISE and KL divergence are only defined for probability measures on a common measurable space. Regardless of some recent definitions of disvergence/distances for two distributions of different dimensions or domains \cite{Cai22DivDimensions}, there still lacks a proper distance/divergence for the LRFS densities with different, discrete labels. As such, both \eqref{eq:FrechetAAISE} and \eqref{eq:RFS-AA-Whole-KLD} can not be directly applied for the labeled RFS density distributions in general. Yet, this has been often violated in the literature. However, they can be applied to the labeled PHD defined on $\mathcal{X}\times\mathcal{L}$ as long as the label space are the same among the sensors.
\end{remark}
%This, however, cannot be explained by the BFoM/MIL formulation that says nothing in the estimation accuracy or robustness. Simply, the GA fusion that minimizes similar divergence \eqref{eq:GA-density-KLD} has, however, been proven vulnerable to local misdetection \cite{Yu16,Li17merging,Li17PC,Li17PCsmc,Gao20cphd,Gao20GLMB,Da_Li20TSIPN,GLi22aaPMB} and cannot even maintain the unbiasedness in point estimation if all fusing estimators are unbiased \cite{Li19Second,Uney19consistency}. %That is, the advantage of the AA fusion is greatly beyond what can be interpreted by the BFoM/MIL. %does not relate the real distribution or even the multisensor optimal distribution and


\section{GM Representations of MB/LMB-PHD} \label{GM-implementation} % and Inter-sensor Association

\subsection{GM-PHD}
It is theoretically justified and also practically convenient to represent the RFS posterior and at the same time the corresponding PHD by a GM, which facilitates the GM-PHD-AA fusion. %Hereafter,
Straightforwardly, the GM approximation of the PHD filter $i \in \mathcal{I}$ at filtering time $k$ can be written as \cite{Vo06}:
\begin{equation}\label{eq:GM_PHD}
D_{i, k}(\mathbf{x}) \approx \sum_{j=1}^{J_{i,k}} \omega_{i,k}^{(j)} \mathcal{N}(\mathbf{x};\bm{\mu}_{i,k}^{(j)},\bm{\Sigma}_{i,k}^{(j)}) %\hspace{0.5mm},
\end{equation}
where $\mathcal{N}\big(\mathbf{x};\bm{\mu}\rmv,\bm{\Sigma}\big)$ denotes a Gaussian PDF with mean vector $\bm{\mu} $ and covariance matrix $\bm{\Sigma}$, $J_{i,k}$ is the number of GCs in total, and $\omega_{i,k}^{(j)}$ is the weight of $j$th GC at sensor $i$.

The whole PHD is thereby completely determined by the parameter set $\mathcal{G}_{i,k} \triangleq \big\{ \big( \omega_{i,k}^{(j)} , \bm{\mu}_{i,k}^{(j)} , \bm{\Sigma}_{i,k}^{(j)}  \big) \big\}_{j=1,\dots,J_{i,k}}$. The expected number of targets at sensor $i\in \mathcal{I}$ at time $k$ can be approximated by
$\hat{N}_{k} \approx \sum_{l=1}^{J_k} \omega_{i,k}^{(j)}$.




\subsection{GM Representation of the MB PHD} \label{sec:MB-AA}
Consider the GM implementation of the MB posterior \eqref{eq:def_MB} represented by a set $L_{i,k}$ of BCs at filtering time $k$ by sensor $i$. Each BC $\big(r_{i,k}^{(\ell)}, s_{i,k}^{(\ell)}(\cdot)\big), \ell \in L_{i,k}$ is represented by $J_{i,k}^{(\ell)}$ GCs weighted by $\omega_{i,k}^{(\ell,\iota)} \!\ge\rmv 0$, $\iota=1,\dots,J_{i,k}^{(\ell)}$, i.e.,
\begin{equation}
s_{i,k}^{(\ell)}(\mathbf{x}) = \sum _{\iota=1}^{J_{i,k}^{(\ell)}} \omega_{i,k}^{(\ell,\iota)} \ist \mathcal{N}\big(\mathbf{x};\bm{\mu}_{i,k}^{(\ell,\iota)}\rmv,\bm{\Sigma}_{i,k}^{(\ell,\iota)}\big) \label{eq:BC_GM} %\ist,
\end{equation}
where
\begin{equation}\label{eq:BCweightofGC}
  \sum _{\iota=1}^{J_{i,k}^{(\ell)}} \omega_{i,k}^{(\ell,\iota)} = 1
\end{equation}

Each BC is completely determined by the parameter set $\mathcal{G}^{(\ell)}_{i,k} \triangleq \big\{ \big( \omega_{i,k}^{(\ell,\iota)} , \bm{\mu}_{i,k}^{(\ell,\iota)} , \bm{\Sigma}_{i,k}^{(\ell,\iota)}  \big) \big\}_{\iota=1,\dots,J_{i,k}^{(\ell)}}$ from which the PHD is approximated as a set of sub-GMs as follows
\begin{equation}\label{eq:mbPHD_GM}
{D_{i,k}}(\mathbf{x}) \approx \sum_{\ell \in L_{i,k}} r_{i,k}^{(\ell)} \sum _{\iota=1}^{J_{i,k}^{(\ell)}} \omega_{i,k}^{(\ell,\iota)} \mathcal{N}\big(\mathbf{x};\bm{\mu}_{i,k}^{(\ell,\iota)}\rmv,\bm{\Sigma}_{i,k}^{(\ell,\iota)}\big)
\end{equation}
which may be expressed as a unified GM determined by the parameter set $\mathcal{G}_{i,k} \triangleq \big\{ \mathcal{G}^{(\ell)}_{i,k} \big\}_{\ell \in L_{i,k}}$ as follows, c.f., \eqref{eq:GM_PHD},
\begin{equation}\label{eq:mbPHD_UnifiedGM}
{D_{i,k}}(\mathbf{x}) \approx \sum_{j=1}^{J_{i,k}} \omega_{i,k}^{(j)} \mathcal{N}(\mathbf{x};\bm{\mu}_k^{(j)},\bm{\Sigma}_k^{(j)})
\end{equation}
where $J_{i,k} = \sum_{\ell \in L_{i,k}}  J_{i,k}^{(\ell)}$ and the recombined weight of each GC labeled as $j$ is given as
\begin{equation}\label{eq:mbPHD_WeightGC}
\omega_{i,k}^{(j)} = r_{i,k}^{(\ell)}  \omega_{i,k}^{(\ell,\iota)}
\end{equation}
which uses the following unique index mapping at each sensor $i\in \mathcal{I}$ at time $k$,
\begin{equation}\label{eq:indexMapping}
  j \leftrightarrow (\ell,\iota)
\end{equation}
where $j=1,...,J_{i,k}, \ell  \in L_{i,k}, \iota=1,...,J^{(\ell)}_{i,k}$.

The (unweighted) mean-state of the $\ell$-th BC %estimated by sensor $i$ at time $k$
as expressed in \eqref{eq:BC_GM} is calculated by
%\begin{equation}\label{eq:BC_mean_GM}
$\bar{\bm{\mu}}_{i,k}^{(\ell)} = \sum _{\iota=1}^{J_{i,k}^{(\ell)}} \omega_{i,k}^{(\ell,\iota)} \bm{\mu}_{i,k}^{(\ell,\iota)} $. % \\
% r_{i,k}^{(\ell)}  & \approx \sum _{\iota=1}^{J_{i,k}^{(\ell)}} \omega_{i,k}^{(\ell,\iota)} \label{eq:BC_r_GM} ,
%\end{equation}
%which indicate the state of BC $\ell$ estimated by sensor $i$ at time $k$.% and are fully determined by $\mathcal{G}^{(\ell)}_{i,k}$.
%\textcolor{magenta}{(Note that our notation does not indicate that the quantities $w_{k}^{(\ell,\iota)}$,  $\psi_{k}^{(\ell,\iota)}(\mathbf{x})$, $\mathbf{m}_{k}^{(\ell,\iota)}$, $\mathbf{P}_{k}^{(\ell,\iota)}$, and $\mathbf{x}_{k}^{(\ell,\iota)}$ are functionally dependent on
%$\mathbf{Z}_{1:k}$.)}
By integrating the PHD, %as in \eqref{eq:Card_Calculation},
the number of targets is estimated at sensor $i$ by
\begin{align}\label{eq:MB_Card}
  \hat{N}_{i,k}  & = \sum_{\ell\in L_{i,k}} r_{i,k}^{(\ell)} \nonumber \\
  & =  \sum _{j=1}^{J_{i,k} } \omega_{i,k}^{(j)}
\end{align}

%Here, we consider the MB-AA fusion from the perspective of PHD-AA/consistency. % and will show for the first time under special cases, the MB-AA fusion can be closed.
%Consider the MB RFS which has $n_i$ BCs in which the $j$-th BC has existence probability $r_{i,j}$ and SPD $s_{i,j}\left(\mathbf{x}\right)$.  The corresponding PHD-AA for the MBs can be expressed as, c.f., \eqref{eq:def-PHD-AA},
%\begin{align}
%	D_{\text{AA}}^{\text {mb}}(\mathbf{x}) & =  \sum\limits_{i \in {\mathcal{I}}}{w_i D_i^{\text {mb}}(\mathbf{x}) } \nonumber \\
%& =  \sum\limits_{i \in {\mathcal{I}}}{w_i  \sum_{\iota=1}^{n_i}{r_{i,j}}{s_{i,j}}(\mathbf{x}) } \nonumber \\ %
%& =  \sum_{j=1}^{n_{f}}{r_{j,\text{PHD-AA}}}{s_{j,\text{PHD-AA}}}(\mathbf{x})  \label{eq:MB-PHA-AA}
%\end{align}
%which corresponds to the following fused MB MPD, c.f., \eqref{eq:def_MB},
%\begin{equation}\label{MB-AA-MPD}
%  	f_{\text{PHD-AA}}^\text{mb} (X_n)  = \sum_{\uplus_{j=1}^{n_{f}}X^{(j)}=X}\prod_{j=1}^{n_{f}}f_{\text{PHD-AA}}^\text{b} \left(X^{(j)}\right)
%\end{equation}
%where the fused BC $f_{\text{PHD-AA}}^\text{b} \left(X^{(j)}\right)$ having parameters $\big( r_{j,\text{PHD-AA}},{s_{j,\text{PHD-AA}}}(\mathbf{x}) \big)$ is given in \eqref{eq:iidc-phd-aa} in Corollary \ref{corollary:Bernoulli} for each clustered/associated group of BCs, and the final number $n_{f}$ of fused BCs depends on the BC merging/clustering procedure, usually
%\begin{equation}\label{eq:MB-n}
%  \max\{n_i\}_{i \in {\mathcal{I}}} \leq n_{f} \leq  \sum_{i \in {\mathcal{I}}}n_i
%\end{equation}
%where the left equation holds iff all BCs from the other sensors are associated with those of a local sensor and the right equation holds iff the BCs are completely not associated with each other.

%The readers are referred to \cite{Li20AAmb} for the detail for grouping the fusing MBs via clustering or one-to-one association \cite{Li20AAmb}. {In addition, data-driven clustering and Bernoulli merging can be found} in \cite{Fontana22merging}. The MB-AA fusion has been recently used for PMB-AA-fusion \cite{GLi22aaPMB}. In the case of labeled MB, the BC association problem becomes to the label matching problem. %, see sections \ref{sec:LMB} and \ref{sec:label-matching}.


\subsection{GM Representation of the LMB PHD} \label{sec:MB-AA}
The LMB can be viewed as a special MB with assigned label for each BC. That is, with slight abuse of notation by interpreting each component index $\ell$ in \eqref{eq:BC_GM}, \eqref{eq:mbPHD_GM} and \eqref{eq:MB_Card} as a track label $l$ and the set $L_{i,k}$ the label set of sensor $i$ at time $k$, the above GM formulation \eqref{eq:mbPHD_GM} for the MB-PHD can be the same derived for calculating the unlabeled PHD of the LMB. This somehow conflicts with the standard definition of the labels which are usually ordered pairs of integers $l = (k; \kappa)$, where $k$ is the time of birth, and $\kappa$ is a unique index to distinguish new targets born at the same time \cite{Vo13Label}. We omit the detail.
The unlabeled PHD $D_{i,k}(\mathbf{x})$ of the local LMB at time $k$ at sensor $i \in \mathcal{I}$, expressed by the parameter set $\mathcal{G}_{i,k} \triangleq \big\{ \big( \omega_{i,k}^{(j)} , \bm{\mu}_{i,k}^{(j)} , \bm{\Sigma}_{i,k}^{(j)}  \big) \big\}_{j=1,\dots,J_{i,k} }$, can be calculated by  c.f., \eqref{eq:mbPHD_UnifiedGM},
\begin{equation}
{D_{i,k}}(\mathbf{x}) = \sum_{j=1}^{J_{i,k}} \omega_{i,k}^{(j)} \mathcal{N}(\mathbf{x};\bm{\mu}_k^{(j)},\bm{\Sigma}_k^{(j)})
\end{equation}
where the L-GCs correspond to the labels in a multiple-to-single manner.


\section{GM-PHD Fit via Optimizing L-GC Weights } \label{sec:GMfit}
This paper addresses averaging the PHDs of the MB and of the LMB filters via the GM implementations, where the GM parameter set for time $k$ at sensor $i \in \mathcal{I}$ is denoted by $\mathcal{G}_{i,k}$ and the corresponding local PHD $D_{i,k}(\mathbf{x})$. The goal of the PHD-AA fusion is to update the local GM parameters such as the corresponding PHD best fits the weighted AA of all PHDs yielded at different sensors, calculated by \eqref{eq:PHD-AA}.
%\begin{equation}\label{eq:AA-PHD}
%  D_\text{AA,k}(\mathbf{x}) = \sum_{i\in \mathcal{I}} w_iD_{i,k}(\mathbf{x})
%\end{equation}
%where the fusion weights $\mathbf{w} =\{w_1,\dots,w_I\} \in \mathbb{W} \subset \mathbb{R}^{I}$

%The track association or label matching forms a significant challenge for heterogeneous filter fusion. % since simply the different filters have different algorithm structures. %which prevent matching
%For instance, it is hard to associate any GCs in the GM-PHD filter with those of the GM-LMB filter.
\begin{remark}
The key challenge to heterogeneous RFS filter fusion origins from the fact that different RFS filters result in different types of MPDs. %, although they may all be implemented by using GM approximation.
Unlike the GM-PHD filter, the GCs in the GMs of the MB and of LMB filters intrinsically belong to different (unlabeled or labeled) BCs. Therefore, if a new L-GC is created or an existing L-GC is deleted, one has to specify the BC/label first. This is more complicated in the case of MB mixture fusion \cite{Li23AApmbm} % and GLMB fusion \cite{Gao20GLMB}
where measurement-to-track association hypothesis is further involved.
\end{remark}
To address the above challenge, we hereafter propose an approximate GM-PHD fitting approach which does not create or disregard any L-GCs in the local filters but only optimize their weights $\omega_{i,k}^{(j)}, j=1,...,J_{i,k}$ in order to best fit their AA in the sense of PHD. Clearly, the resulted PHD is no more than an approximate to the desired PHD-AA. % but it is expected to be a good approximate; see section \ref{sec:homogeneousSim}.
This fit does not only save computation but also enables the parallel-communication-filtering operation similarly as was done in \cite{Li17PCsmc,Li19ParallelCC} since the mean $\bm{\mu}_{i,k}^{(j)}$ and covariance $\bm{\Sigma}_{i,k}^{(j)}$ of each L-GC $j$, as well as the total number $J_{i,k}$ of L-GCs, remain unchanged during the fusion. Obviously, better fit can be expected if $\bm{\mu}_{i,k}^{(j)}$ and $\bm{\Sigma}_{i,k}^{(j)}$ are also optimized with the weight, regardless of the computational cost. We leave this to the future work.  %
%We leave this to the future work. %The local GCs only need to adjust their weights by using scaling ratios yielded via fusion; see section \ref{sec:fusionfeedback}. The fusion feedback can be performed in the next filtering iteration such that the filter does not need to start the next filtering iteration after the communication and fusion.


\subsection{Minimizing ISE of GMs via Reweighting} \label{sec:ise}
To evaluate the degree of fit, one may use the KL divergence which, however, does not admits analytical solution for the GMs. For the sake of computational efficiency, one may consider the Cauchy-Schwarz divergence \cite{Kampa11CSD} %,Hoang15
and the ISE metric as given in \eqref{eq:FrechetAAISE}, both of which allow analytical calculation for GMs. We consider the latter only, which complies with the BFoM as expressed by \eqref{eq:FrechetAAISE}. % and can thereby significantly simplify the calculation. % and can thereby significantly simplify the calculation.
Formally speaking, the goal of our approach is to determine the new weight for each L-GC at the local sensor in the following BFoM sense
\begin{align}
  \mathbf{w}^{\text{BFoM}}_{i,k} & = \operatorname*{arg\,min}_{\mathbf{w}_{i,k} \in \mathbb{W}} \text{ISE}\big(D_{i,k}\| D_{\text{AA},k}\big) \label{eq:opt_W_ISE} \\
  & = \operatorname*{arg\,min}_{\mathbf{w}_{i,k} \in \mathbb{W}} \int{\bigg( (1-w_i)D_{i,k} - \sum_{s\in \mathcal{I}\setminus{i}} w_sD_{s,k} \bigg)^2 d \mathbf{x}} \label{eq:WeightISEfit}  %j\in \mathcal{I}
\end{align}
where $\mathbf{w}_{i,k} = \{ \omega_{i,k}^{(j)}\}_{j=1,...,J_{i,k}}$ and where $A\setminus B$ is the set difference of $A$ and $B$. %$\mathcal{I}\setminus{i} := \{1,2,...,i-1,i+1,I\}$.

%  {\bm{\mu}_n} & = \bm{\mu}_{i,k}^{(j)}  \label{eq:mu} \\
%  {\mathbf{P}_n} & = \bm{\Sigma}_{i,k}^{(j)}  \label{eq:P} \\
%  \beta_m  & = w_{s} \omega_{s,k}^{(l)}  \label{eq:beta} \\
%  {\bm{\mu}_{s,k}^{(l)}} & = \bm{\mu}_{s,k}^{(l)}  \label{eq:m} \\
%  {\bm{\Sigma}_{s,k}^{(l)}} & = \bm{\Sigma}_{s,k}^{(l)}  \label{eq:S} \\

\begin{lemma}
\eqref{eq:WeightISEfit} can be solved by, $\forall i \in \mathcal{I}, j=1,...,J_{i,k}$,
\begin{align}
 & \omega_{i,k}^{(j),\text{BFoM}} = \nonumber \\
 & \frac{\Delta} {(1-w_i)} {\sum\limits_{s\in \mathcal{I}\setminus{i}} \sum\limits_{l=1}^{J_{s,k}}{w_{s} \omega_{s,k}^{(l)}}\mathcal{N}\big({\bm{\mu}_{i,k}^{(j)}};{\bm{\mu}_{s,k}^{(l)}},{\bm{\Sigma}_{i,k}^{(j)}}{\rm{ + }}{\bm{\Sigma}_{s,k}^{(l)}}\big)}\nonumber \\
 &- {\Delta} \sum\limits_{j'\in \mathcal{J}_i^{-j} } {\omega_{i,k}^{(j')}\mathcal{N}\big({\bm{\mu}_{i,k}^{(j)}};\bm{\mu}_{i,k}^{(j')},{\bm{\Sigma}_{i,k}^{(j)}} + {\bm{\Sigma}_{i,k}^{(j')}}\big)} \label{eq:minISE-wi}
\end{align}
where $\Delta = {\left| {2 \bm{\Sigma}_{i,k}^{(j)} } \right|}^{1/2} { (2\pi )}^{d/2} $ and $\mathcal{J}_i^{-j} = \{1,..,J_{i,k}\} \setminus j$.
\end{lemma}

\begin{proof}
See Appendix \ref{appendix:iseMIN}. The result is from \eqref{eq:minISE-a}.
\end{proof}

\subsection{A Sequential Solution and Over-fit}
As the typical case of the multivariate optimization, the univariate fit of the weight $\omega_{i,k}^{(j)}$ in \eqref{eq:minISE-wi} depends on the other weights $\omega_{i,k}^{(j')}, j'\in \mathcal{J}_i^{-j} $ in the local GM. Here, a simple solution is given by sequentially updating the weights of all L-GCs in which the weight optimization for each L-GC uses the newest available weights of all the other L-GCs. That is, in iteration $t\in \mathbb{N}^+$, $\omega_{i,k}^{(j')}, j'\in \mathcal{J}_i^{-j} $ in \eqref{eq:minISE-wi} is defined as follows for calculating $\omega_{i,k}^{(j,t),\text{BFoM}}$,
\begin{equation}\label{eq:omega_iteration}
  \omega_{i,k}^{(j')} :=  \begin{cases}
\omega_{i,k}^{(j',t)} &  j'< j\\
\omega_{i,k}^{(j',t-1)}  & j'>j
\end{cases}
\end{equation}
where $\omega_{i,k}^{(j,t)}$ denotes the yielded weight of the $j$th L-GC in fit iteration $t$ and $\omega_{i,k}^{(j,0)} :=\omega_{i,k}^{(j)}$.

The above sequential, iterative univariate-fit approach to the multivariate optimization problem, however, may suffer from over-fit in practice, resulting in negative %or over-large
weights. This may be addressed as follows, $\forall i \in \mathcal{I}, j=1,...,J_{i,k}$,
\begin{align}
 \tilde{\omega}_{i,k}^{(j,t)} & = \text{max}\Big( \epsilon,  \omega_{i,k}^{(j,t),\text{BFoM}} \Big) \label{eq:bound-w}
\end{align}
where $\epsilon \geq 0$ is a small constant (e.g., 0.01) specified to avoid generating negative weight. % for which we use $0.1\omega_{i,k}^{(j)}$, i.e. set the weight 90\% lower if it is supposed to be negative according to \eqref{eq:minISE-wi}.

More importantly, we further take the following strategy to avoid over-fit
\begin{align}
 \omega_{i,k}^{(j,t)} & = \alpha \tilde{\omega}_{i,k}^{(j,t)} + (1-\alpha) \omega_{i,k}^{(j,t-1)} \label{eq:upd-w-alpha}
\end{align}
where $0<\alpha<1$ is a scaling factor% to balance the final updating weight between the original weight and the new weight given in \eqref{eq:minISE-wi}. It
, which
can be interpreted as a learning rate for which we found $0.2$ is a good choice.

The above fit iteration may %start from the L-GC with the largest weights and will
be terminated at a convergence level (i.e., the local PHDs approximate their average well, e.g., $\text{ISE}\big(D_{i,k}\| D_{\text{AA},k}\big)\leq \varepsilon$ where $\varepsilon$ is a small threshold) or up to a maximum number of fit iterations, e.g., $t\leq t_{\text{max}}= 3$. Notably, as shown in our simulations in section \ref{sec:simulation}, a too large $t_{\text{max}}$ will lead to over-fit especially for the LMB filter. %We leave this open issue to our future work.


%For clarity, we write
%\begin{align}
%  (1-w_i)D_{i,k} & = \sum_{n}^{J_{i,k}}\alpha_n \mathcal{N}(\mathbf{x}; {\bm{\mu}_n},{\mathbf{P}_n}) \nonumber \\
%   \sum_{s\neq i} w_sD_{s,k} & = \sum_{m}^{J_0}\beta_m\mathcal{N}(\mathbf{x}; {\mathbf{m}_m},{\mathbf{S}_m})
%\end{align}
%where
%\begin{align}
%  \alpha_n & = \label{eq:alpha} \\
%  {\bm{\mu}_n} & = \bm{\mu}_{i,k}^{(j)}  \label{eq:mu} \\
%  {\mathbf{P}_n} & = \bm{\Sigma}_{i,k}^{(j)}  \label{eq:P} \\
%  \beta_m  & = w_{s} \omega_{s,k}^{(l)}  \label{eq:beta} \\
%  {\mathbf{m}_m} & = \bm{\mu}_{s,k}^{(l)}  \label{eq:m} \\
%  {\mathbf{S}_m} & = \bm{\Sigma}_{s,k}^{(l)}  \label{eq:S} \\
%  J_0 & = \sum_{s\neq i}J_{s,k}  \label{eq:J}
%\end{align}
%and \eqref{eq:beta}-\eqref{eq:S} indicate an unique mapping
%\begin{equation}\label{eq:j2j'}
%  m \leftrightarrow (j, l)
%\end{equation}
%where $m=1,...,J_0, j\neq i, j \in \mathcal{I}, l=1,...,J_{i,k}$.
%
%Then, the ISE given in \eqref{eq:WeightISEfit} can be expressed as follows
%\begin{align}
%\text{ISE}\big(D_{i,k}\| D_{\text{AA},k}\big)
%% \triangleq & \int{\bigg( \sum\limits_{j=1}^n{\alpha_n \mathcal{N}(\mathbf{x}; {\bm{\mu}_n},{\mathbf{P}_n}) }-\sum\limits_{m=1}^m{\beta_m\mathcal{N}(\mathbf{x}; {\mathbf{m}_m},{\mathbf{S}_m})} \bigg)^2 d \mathbf{x}} \\
%%=& \sum\limits_{n,n'} {{\alpha_n}{\alpha _{n'}}} \int {{p_n}(x)} {p_{n'}}(x)dx \nonumber \\
%%               & + \sum\limits_{m,m'} {{\beta_m}{\beta _{m'}}} \int {{q_m}(x)} {q_{m'}}(x)dx \nonumber\\
%%               &- 2\sum\limits_{n,m} {{\alpha_n}{\beta_m}\int {{p_n}(x)} {q_m}(x)dx} \nonumber\\
%= & \sum\limits_{n,n'} {{\alpha_n}{\alpha _{n'}}\mathcal{N}({\bm{\mu}_n}; {\bm{\mu} _{n'}},{\mathbf{P}_n}+ {\mathbf{P} _{n'}})}
%\label{eq:alphaBetaISE} \\ &
%{\rm{ + }}\sum\limits_{m,m'} {{\beta_m}{\beta _{m'}}\mathcal{N}({\mathbf{m}_m}; {\mathbf{m}_{m'}},{\mathbf{S}_m} + {\mathbf{S}_{m'}})}  \nonumber\\
% & - 2\sum\limits_{n,m} {{\alpha_n}{\beta_m}\mathcal{N}({\bm{\mu}_n}; {\mathbf{m}_m},{\mathbf{P}_n}{\rm{ + }}{\mathbf{S}_m})}
%\end{align}
%%The ISE approach was first proposed for MR in the context of multiple hypothesis tracking in \cite{Williams06}, which inspired the normalized ISE \cite{Petrucci05} and further development \cite{Chen10}. %, and the squared distance \citep{Kurkoski08MR}.
%%One distinctive feature of the method is the availability of exact analytical expressions for GMs. However, the ISE typically has many local minima; hence gradient-based methods cannot guarantee convergence to the global minimum, unless the initialization point happens to be close to the global minimum \cite{Williams06}. Of sufficient similarity to the ISE, %Cauchy-Schwarz inequality \citep{Scott01MR} and
%%Wasserstein distance \cite{Assa18} and Pearson $\chi^2$-divergence \cite{Kitagawa20} are also used for GM reduction.
%
%Here, it is obvious that, $\forall i\in \mathcal{I}$
%\begin{align}\label{eq:derISE}
%  \frac{\partial  \text{ISE}\big(D_{i,k}\| D_{\text{AA},k}\big) }{\partial \alpha_n} & = \sum\limits_{n' \neq n} {{\alpha _{n'}}\mathcal{N}({\bm{\mu}_n};{\bm{\mu} _{n'}},{\mathbf{P}_n}+ {\mathbf{P} _{n'}})}
%\nonumber\\ &
%{\rm{ + }} {2{\alpha _{n}}\mathcal{N}({\bm{\mu}_n}; {\bm{\mu} _{n}},2{\mathbf{P}_n} )} \nonumber\\
% & - 2\sum\limits_{m} {{\beta_m}\mathcal{N}({\bm{\mu}_n};{\mathbf{m}_m},{\mathbf{P}_n}{\rm{ + }}{\mathbf{S}_m})} \\
% \frac{\partial^2 \text{ISE}\big(D_{i,k}\| D_{\text{AA},k}\big)  }{\partial \alpha_n^2}  & = {2 \mathcal{N}({\bm{\mu}_n};{\bm{\mu}_n}, 2{\mathbf{P}_n})} \\
% & >0
%\end{align}
%Setting $\frac{\partial  \text{ISE}\big(D_{i,k}\| D_{\text{AA},k}\big) }{\partial \alpha_n}$ zero and using \eqref{eq:alpha} will lead to
%\begin{align}\label{eq:derISEzero}
% & \omega_{i,k}^{(j)}  = \nonumber \\
% &\frac{2\sum\limits_{m} {{\beta_m}\mathcal{N}({\bm{\mu}_n};{\mathbf{m}_m},{\mathbf{P}_n}{\rm{ + }}{\mathbf{S}_m})}-\sum\limits_{n' \neq n} {{\alpha _{n'}}\mathcal{N}({\bm{\mu}_n};{\bm{\mu} _{n'}},{\mathbf{P}_n} \rm{+} {\mathbf{P} _{n'}})}}{2(1-w_i)\mathcal{N}({\bm{\mu}_n}; {\bm{\mu} _{n}},2{\mathbf{P}_n} )}
%\end{align}
%which yields the minimum ISE.
%%By comparing \eqref{eq:alphaBetaISE} with \eqref{eq:WeightISEfit}, it is obvious

\subsection{Fusion Feedback} \label{sec:fusionfeedback}
\subsubsection{PHD filter} Each L-GC $j=1,...,J_{i,k}$ of sensor $i\in \mathcal{I}$ that is used for representing the PHD will be simply re-weighted as $\omega_{i,k}^{(j,t)}$ as calculated in \eqref{eq:upd-w-alpha} after $t$ GM-fit iterations at time $k$, which can be written as follows
\begin{align}
\omega_{i,k}^{(j),\text{upd}} = \omega_{i,k}^{(j,t)}
\end{align}
\subsubsection{MB/LMB filter}
Based on the ``1-1'' mapping \eqref{eq:indexMapping}, we get the new, un-normalized weight for $\iota$th L-GC of the $\ell$th BC/track in sensor $i$ at time $k$ from the fused weight $\omega_{i,k}^{(j,t)}$ as calculated in \eqref{eq:upd-w-alpha} after $t$ GM-fit iterations as follows
\begin{align}
\tilde{\omega}_{i,k}^{(\ell,\iota),\text{upd}} = \omega_{i,k}^{(j,t)}
\end{align}
where $j=1,...,J_{i,k}, \ell \in L_{i,k}, \iota=1,...,J^{(\ell)}_{i,k}$.

According to \eqref{eq:BCweightofGC}, the weight of the L-GCs in each BC/track should then be normalized, i.e.,
\begin{equation}
\omega_{i,k}^{(\ell,\iota),\text{upd}} = \frac{\tilde{\omega}_{i,k}^{(\ell,\iota),\text{upd}}}{\sum _{\iota=1}^{J_{i,k}^{(\ell)}} \tilde{\omega}_{i,k}^{(\ell,\iota),\text{upd}}}
\end{equation}

%We further propose to adjust the existence probability $r_{i,k}^{(\ell),\text{upd}}$ of each BC according to the overall change of the weights of all of its associated L-GCs as follows
%%\eqref{eq:mbPHD_WeightGC}, the new weight $ \omega_{i,k}^{(\ell,\iota),\text{upd}}$ of each L-GC and new should be adjusted as follows
%%\begin{equation}\label{eq:mbPHD_WeightGC_new}
%%\omega_{i,k}^{(l),\text{upd}} = r_{i,k}^{(\ell),\text{upd}}  \omega_{i,k}^{(\ell,\iota),\text{upd}}
%%\end{equation}
%\begin{equation}
%{r}_{i,k}^{(\ell),\text{upd}} = r_{i,k}^{(\ell)} \sum _{l=1}^{J_{i,k}^{(\ell)}} \tilde{\omega}_{i,k}^{(\ell,\iota),\text{upd}}  \\
%\end{equation}
%
%
%\subsection{Cardinality-consensus}
In addition to the PHD fit, one may further apply the multitarget set cardinality AA fusion that communicates and averages the cardinality estimate among local sensors, namely cardinality consensus (CC) \cite{Li19CC}, although the cardinality (distribution) is not separately estimated in the PHD, MB and LMB filters. By using \eqref{eq:MB_Card},
the AA of the estimated number of targets can be calculated as
\begin{align}\label{eq:AA-card}
\hat{N}_{\text{AA},k} & = \sum_{i \in \mathcal{I}}  \hat{N}_{i,k}\nonumber \\
& = \sum_{i \in \mathcal{I}}  \sum_{\ell\in L_{i,k}} r_{i,k}^{(\ell)}
\end{align}
Regardless that this is not really used for estimating the number of targets in the local MB/LMB filters, it can be used to re-scale the L-GC weights $\omega_{i,k}^{(j)}, j=1,2,...,J_{i,k}$ in the PHD filter and re-scale the track existence probability $r_{i,k}^{(\ell)}, \ell\in L_{i,k}$ in the case of MB/LMB filter, respectively, as follows
\begin{align}
\omega_{i,k}^{(j),\text{upd}} &=  \frac{\omega_{i,k}^{(j,t)} \hat{N}_{\text{AA},k}}{\hat{N}_{i,k}} \label{eq:CC-PHD} \\
  r_{i,k}^{(\ell),\text{upd}} &=  \frac{r_{i,k}^{(\ell)} \hat{N}_{\text{AA},k}}{\hat{N}_{i,k}} \label{eq:CC-MB}
\end{align}


\subsection{Insufficiency for Track Fusion}\label{sec:GM-PHD-fit-summary}
The proposed unlabeled PHD-AA suits the most the PHD filter but not the MB/LMB filters due to two reasons %, which implicitly or explicitly represent each target using separate tracks/BCs. The unlabeled PHD fusion is insufficient for MB/LMB fusion in twofold as follows.
\begin{remark} \label{remark:insufficientPHD1}
First, the PHD is the first order moment of the MB/LMB density for which the PHD-consensus is insufficient for MB/LMB consensus. In other words, two MB/LMB densities can be variant even if their PHDs are the same.
\end{remark}
\begin{remark}\label{remark:insufficientPHD_T2T}
Second, the proper fusion of the MBs/LMBs needs to match/associate the BCs %(and the MBs in the case of MB mixture fusion \cite{Li23AApmbm}) % and GLMB fusion \cite{Gostar20,Gao20GLMB}
in one way or another and then carry out the fusion in each associated group of BCs; % which will result in an exactly averaged BC of averaged existing probability \cite{Li19Bernoulli};
see proposed BC-to-BC (B2B) association methods \cite{Li20AAmb,Yi21Heterogeneous}, label matching methods \cite{wang16MB-gci,SLi17LableInconsistence,Gao20GLMB} %Jiang16MB-gci, %for the detail for grouping the fusing BCs via clustering or one-to-one association
and the discussion in \cite[Sec.V.F]{Li22RFS-AA-Derivation} %,
in the homogeneous fusion case. While B2B AA fusion will result in an exactly averaged BC \cite{Li19Bernoulli}, it is, however, inapplicable when fusing the MB/LMB filters with the PHD filters that do not have distinct BCs.
\end{remark}

To say the least, label matching is nontrivial as the labels can be very different with each other among different LMB filters. This can be illustrated in Fig. \ref{fig:PHDlabel} where four GM-LMBs with completely the same GM parameters $\mathcal{G}_i = \mathcal{G}_j$ but different labels $L_i \neq L_j$, $\forall i \neq j$. Different labeling corresponds to different labeled PHDs and different target-state estimates, although their unlabeled PHDs are equal.
Overall, our proposed unlabeled PHD fusion via heterogeneous GM-fit overcomes these challenges as it does neither seek track-to-track fusion nor fuse in the MPD posterior level. % and the fusion calculation is carried out in an approximate manner. % unless the fusion is carried out with regard to each label/BC. %However, we consider the heterogenous fusion between unlabeled and labeled filters, disregarding the heterogenous data association problem.


\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=8.5cm]{PHDlabel.eps}\\  % _extended
  \caption{Four different LMBs of the same unlabeled PHD but different labeled PHDs and different potential target-state-estimates. %for which the label matching is nontrivial.
   } \label{fig:PHDlabel}
  \vspace{-4mm}
\end{figure}


\section{Simulations} \label{sec:simulation}
We consider a ROI given by $[-1\ist\text{km}, 1\ist\text{km}]\times [-1\ist\text{km}, 1\ist\text{km}]$ which is viewed by 4 sensors. % shows the sensor network and the target trajectories.
The target state is denoted as $\mathbf{x}_k=[x_k \; \dot{x}_k \; y_k \; \dot{y}_k]^\text{T}\rmv$ with planar position $[x_k \; y_k]^\text{T}$ and velocity $[\dot{x}_k \; \dot{y}_k]^\text{T}$.
There are totally 12 targets born at different points as shown in Fig. \ref{fig:scenario}.
The target birth is modeled by a MB process with parameters $\{r_{\text{B}} ,p_{\text{B}}^{(\ell)}(\cdot) \}_{\ell=1}^{4}$ at each time-instant, where $r_{\text{B}} =0.03$%
, and $p_{\text{B}}^{(\ell)}(\mathbf{x})=\mathcal{N}(\mathbf{x};\mathbf{\mu}_\text{B}^{(\ell)},\bm{\Sigma}_\text{B})$ with parameters $ \mathbf{\mu}_\text{B}^{(1)}=[0,0,0,0]^\text{T}$, $\mathbf{\mu}_\text{B}^{(2)}=[400\text{m},0,-600\text{m},0]^\text{T}$, $\mathbf{\mu}_\text{B}^{(3)}=[-800\text{m},0,-200\text{m},0]^\text{T}$, $\mathbf{\mu}_\text{B}^{(4)}=[-200\text{m},0,800\text{m},0]^\text{T}$, $
\bm{\Sigma}_\text{B}^{\text{ \ \ \ }}=\mathrm{diag}([10\text{m},10\text{m/s},10\text{m},10\text{m/s}]^\text{T})^{2}$.

Each target has a constant survival probability $0.95$ and follows a constant velocity motion (with the sampling interval $\Delta =1$s) using noiseless transition density $f_{k|k-1}(\mathbf{x}_{k}|\mathbf{x}_{k-1})=\mathcal{N}(\mathbf{x}_{k};F \mathbf{x}_{k},\mathbf{0})$ for generating the ground truth while the filters use $f_{k|k-1}(\mathbf{x}_{k}|\mathbf{x}_{k-1})=\mathcal{N}(\mathbf{x}_{k};F \mathbf{x}_{k},\mathbf{Q})$, where % $f_{k|k-1}(\mathbf{x}_{k}|\mathbf{x}_{k-1})=%
$$
F=\mathbf{I}_{2}\otimes\left[\begin{array}{cc}
1 & \Delta\\
0 & 1
\end{array}\right], \,\mathbf{Q}=25 \times\mathbf{I}_{2}\otimes\left[\begin{array}{cc}
\Delta^{2}/2 & \Delta/2\\
\Delta/2 & \Delta
\end{array}\right] %,
$$
where $\otimes$ is the Kronecker product.

The simulation is performed 100 runs with conditionally independent measurement series for 100 seconds each run. The clutter is uniformly distributed over the ROI for which the number of clutter points at each time is Poisson with rate $\kappa_s=10$. For simplicity, we consider four sensors with the same time-invariant target detection probability $p_d = 0.9$ and linear measurement model as follows
\begin{equation}\label{eq:observationModel}
\mathbf{z}_{s,k}= \left[ \begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 \\
\end{array} \right] \mathbf{x}_k+ \left[ \begin{array}{c}
v_{k,1} \\
v_{k,2} \\
\end{array} \right]  %\hspace{0.5mm},
\end{equation}
with $v_{k,1}$ and $v_{k,2}$ as mutually independent zero-mean Gaussian noise with the same standard deviation of $10$m.


%
%The PMBM filter implementation used a maximum number of global hypotheses
%$N_\text{max}=10$. Pruning was required for the maintenance of both PPP and MBM. Weight threshold $10^{-4}$ was used to prune the low-weighted hypotheses or Poisson components. In the latter, BCs whose existence probability was lower than
%$10^{-4}$ were removed too. Ellipsoidal gating based on the Mahalanobis distance with gate 20 was used for measurement-track association.

The filter performance is evaluated by the optimal subpattern assignment (OSPA) error \cite{Schuhmacher08}, which is given as follows, for $|\hat{{X}}| \geq |{X}|$,
\begin{equation} \label{eq:ospa}
d^{(c,p)}_\text{ospa}(\hat{{X}}, {X}) = \left( \frac{1}{|\hat{{X}}|}\left( d_\text{Loc}(\hat{{X}}, {X})   +  d_\text{Card} (\hat{{X}}, {X}) \right) \right)^{\frac{1}{p}}
\end{equation}
where the localization error (OSPA Loc) and cardinality error (OSPA Card) are respectively defined as
\begin{align}
d_\text{Loc}(\hat{{X}}, {X}) & = {\mathop {\min }\limits_{\pi  \in {{\rm \Pi} _{|\hat{{X}}|}}} \sum\limits_{i = 1}^{|{X}|} {{d^{(c)}}{{({{X}_i},{\mathbf{\hat{x}}_{\pi (i)}})}^p}} } , \label{Carderr} \\
d_\text{Card} (\hat{{X}}, {X}) & = { {{c^p}}  (|\hat{{X}}| - |{X}|)}. \label{Locerr}
\end{align}
Here, $\pi$ and $ {\rm \Pi}_n $ are a permutation and the set of all permutations on $\{1,\ldots,n \}$, and $ {d^{(c)}}({X},\mathbf{y}) = \min \left( {d({X},\mathbf{y}),c} \right) $ is a metric between ${X}$ and $\mathbf{y}$ cut-off at $c$. If $|\hat{{X}}| < |{X}|$, $d^{(c,p)}_\text{ospa}(\hat{{X}}, {X}) = d^{(c,p)}_\text{ospa}({X}, \hat{{X}})$, $d_\text{Loc}(\hat{{X}}, {X}) = d_\text{Loc}({X},\hat{{X}})$, $d_\text{Card} (\hat{{X}}, {X})  = d_\text{Card} ({X}, \hat{{X}}) $. In our simulations, we use $c=100$m and $p=2$.

%A key difference of the GOSPA as compared with the popular OSPA \cite{Schuhmacher08} is that it adds up the localization errors for each detected target rather than averaging them. Therefore, localization errors will typically be more significant in the GOSPA.

In the proposed GM-fit process, we use the scaling factor $\alpha = 0.2$ in \eqref{eq:upd-w-alpha}. We consider both homogeneous and heterogeneous cases. In the former, all four sensors run the same PHD, MB or LMB filters respectively while in the latter, different sensors may run different filters. Different levels of fusion have been considered in both cases. They may not cooperate with each other at all (namely noncooperative), only communicate and fuse with each other the estimated number of targets via \eqref{eq:CC-PHD} using $\omega_{i,k}^{(j,t)}=\omega_{i,k}^{(j)}$ or via \eqref{eq:CC-MB} (namely CC only), or perform the proposed GM-PHD-AA fusion in various numbers of GM-fitting iterations, $t=1,2,...,6$. %The communication and fusion can be directional or un-directional. In the former, some sensors send its information to, but not receive any information from, some others.
We note that, even in the case of homogeneous PHD filters, the approximate GM fit-based PHD-AA fusion is different from the standard, exact GM-PHD-AA fusion \cite{Li17PC} which merges all of the local GMs. Consequently, the size of the local GM is constant in the former while it increases and so requires GM reduction in the latter. Comparison between them will be shown.
Furthermore, we use uniform fusing weights, no matter what filters/sensors are involved.

To set up the local filters, the maximum number of L-GCs in the local GM is $200$ for the PHD filters, the maximum number of tracks/BCs is $50$ and the maximum number of L-GC for each track/BC is $20$ in the MB/LMB filters. We note that in practice, these parameters should be designed according to the computational capacity and sensing rate of the local sensors.%There is a significant difference to our proposed, approximate fit approach in the calculation. % and the proposed GM fit can only obtain an approximate PHD-AA.
The other setup of the filters we used are the same as the standard one as given in the codes released by Vo-Vo at https://ba-tuong.vo-au.com/codes.html. Codes for our simulations are available soon in the following URL: sites.google.com/site/tianchengli85/matlab-codes/aa-fusion.

\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=8cm]{Groundtruth.eps}\\
  \caption{Target trajectories starting from O and ending at $\triangle$} \label{fig:scenario}
  \vspace{-2mm}
\end{figure}

\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=8cm]{measurements.eps}\\
  \caption{Measurements and the position estimates in one run by a local GM-PHD filter}
  \vspace{-2mm}
\end{figure}

\subsection{Homogeneous PHD/MB/LMB fusion} \label{sec:homogeneousSim}
We first test the performance of the proposed GM-AA fit approach in the case of the PHD filter fusion in order to show the accuracy of the proposed GM fit approach.
The average OSPA errors, cardinality errors \eqref{Carderr} and localization errors \eqref{Locerr} over 100 runs for each filtering iteration at three sensors are shown in Fig. \ref{fig:PHD-opsa-I=1}. The average OSPA errors of the local PHD filters over all 100 runs and all 100 filtering steps have been significantly reduced by the proposed GM-PHD fit approach as shown in Fig. \ref{fig:PHD-Aver-opsa} of which the best number of GM-fit iterations is $4$, although its OSPA error reduction is not so significant as the exact GM-PHD-AA fusion does in Fig. \ref{fig:Complete-PHD-Aver-opsa}. The minimum average OSPA error in the former is about $22$m while it is about $9$m in the latter; this difference can be reduced if the other parameters of the L-GCs are optimized with the weight to better fit the PHD-AA in the proposed approach. It is also noted that the exact GM-PHD-AA fusion will lead to almost the same result for all four local filters while these filters still differ from each other after the GM fit fusion. This complies with the fact that the latter only approximates PHD-AA fusion through merely revising the weights of the GM while the former exchanges the whole GM. %while the latter seeks exact PHD-AA fusion regardless of the GM merging error.

The average-over-time OSPA error of the proposed GM-AA fit approach for MB and LMB filters are given in Figs. \ref{fig:MB-Aver-opsa} and \ref{fig:LMB-Aver-opsa}, respectively. Furthermore, we consider the MB filters based on the B2B association and GM inter-sensor exchange that enable the parallel Bernoulli-AA fusion, where the B2B association is carried out via the Hungary assignment or clustering as addressed in \cite{Li20AAmb}. The average OSPA errors are also given in Fig. \ref{fig:MB-Aver-opsa}, in which two B2B association approaches perform significantly different with each other in this particular scenario but both outperform the approximate GM fit approach. The results have demonstrated the effectiveness of the proposed, approximate GM-PHD-AA fit for fusing these filters when a small number of GM-fit iterations are applied, e.g., $t=1$ or $2$. However, the OSPA reduction in the case of MB and LMB filters is much lower than that for the PHD filters---as noted in remark \ref{remark:insufficientPHD1}--- and over-fit is easier to occur. The LMB filter performance will even deteriorate when a large number of GM-fit iterations are applied, when $t\geq 3$ as shown in Fig. \ref{fig:LMB-Aver-opsa}. These results confirm the analysis given in Section \ref{sec:GM-PHD-fit-summary}.
%%As addressed in Section \ref{sec:GM-PHD-fit-summary}, %unlike the GM-PHD, the MB and of LMB filters are so designed that they handle each target/track separately by a BC and the L-GCs belong to different (unlabeled or labeled) BCs/tracks. The
%%the best AA fusion of the MBs/LMBs should be given in a proper B2B manner %match the BCs first in one way or another, namely B2B/label-matching based AA fusion %\cite{Li20AAmb}
%%otherwise the fusion based on mere L-GC re-weighting gains minor in improving the state-estimates of each target.
%We leave the open issues for determining the best number of GM-fit iterations and for label matching to the future work to avoid distracting the reader from our key contribution.


\begin{figure}
  \centering
  \includegraphics[width=8cm]{PHDfitI=1.eps}\\
  \caption{The OSPA errors, cardinality errors and localization errors of each local PHD filter over time in the case of only 1 GM-Fit iteration} \label{fig:PHD-opsa-I=1}
  \vspace{-2mm}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=8cm]{PHDospa.eps}\\
  \caption{Average OSPA error of each local PHD filter in different cases of homogeneous inter-filter PHD GM-fit} \label{fig:PHD-Aver-opsa}
  \vspace{-2mm}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=8cm]{PHDospaHomo.eps}\\
  \caption{Average OSPA error of each local PHD filter where the GM-PHD-AA fusion is obtained via GM merging as proposed in \cite{Li17PC}} \label{fig:Complete-PHD-Aver-opsa}.
  \vspace{-2mm}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=8cm]{MBospaComp.eps}\\
  \caption{Average OSPA error of each local MB filter in different cases of homogeneous inter-filter PHD GM-fit, compared with that of the MB-AA filters \cite{Li20AAmb} based on B2B association and exact B2B-AA fusion} \label{fig:MB-Aver-opsa}
  \vspace{-2mm}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=8cm]{LMBosap.eps}\\
  \caption{Average OSPA error of each local LMB filter in different cases of homogeneous inter-filter PHD GM-fit} \label{fig:LMB-Aver-opsa}
  \vspace{-2mm}
\end{figure}

\subsection{Heterogeneous PHD, MB and LMB fusion}
This section tests the performance of the proposed GM-AA fit approach in the heterogeneous case. The average results of the proposed GM-AA fit approach for two-MB and two-LMB filters cooperation, for two PHD and two MB filters cooperation and for two PHD and two LMB filters cooperation are given in Figs. \ref{fig:2MB2LMB-Aver-opsa}, \ref{fig:2PHD2MB-Aver-opsa} and \ref{fig:2PHD2LMB-Aver-opsa}, respectively. The results have demonstrated the effectiveness of the proposed approach for heterogeneous RFS filter fusion. Similar to the case of homogeneous fusion in section \ref{sec:homogeneousSim}, the proposed GM-AA fit approach can benefit the PHD filters more than for the MB/LMB filters and the latter is easier to suffer from over-fit than the former with the increase of the number of GM-fit iterations. We conjugate that better results can be expected in the case of heterogeneous MB and LMB fusion when a proper B2B association procedure is employed so that the GM-fit can be performed with regard to associated BCs. To this end, the labels can be removed temporarily so that the LMBs reduces to MBs \cite{wang16MB-gci,SLi17LableInconsistence}, which enables BC association/clustering strategies as given for the MB-AA fusion \cite{Li20AAmb}.
%In particular, the cardinality averaging lead to worse results for the MB filter. That is, the MB filters improve the PHD filters via the proposed heterogeneous fusion but the reverse is not true. Therefore, a solution in such a case is simply not let the fusion result be feedback to the LMB/MB filter (so that their results will not be affected by the fusion at all), but only the PHD filter of which the performance gain from the fusion. That is, it suggests a directional communication and fusion protocol where the local MB/LMB filters only send their information to the PHD filters but not receive theirs. The results for this setup has been shown in Fig. \ref{fig:2PHD2MBnoMBfusion-Aver-opsa} which shows similar results. That being said,
%This is probably because the proposed GM-PHD-fit completely disregards the labels/assocation of each L-GC to the BCs. A better result can be expected if the fusion regarding the MB and LMB be carried out with regard to each BC in a proper way. We leave this to the future work.

Finally, we consider the configuration of one PHD filter, one MB filter and one LMB filter cooperation as linked in Fig. \ref{fig:HFframework}. The average OSPA error of each filter based on the proposed  GM-AA fit approach is given in Fig. \ref{fig:PHDMBLMB-Aver-opsa}, which confirms that, the heterogeneous unlabeled PHD-AA fusion based on GM-fit can benefit all filters, especially the PHD filters. It also shows that the best performance of the LMB filter is achieved when the PHD GM-fit is carried out for 2 iterations. That is, after a number of PHD GM-fit iterations (namely 2 here), it is better not let the fusion result be feedback to the LMB and even the MB filters, but only the PHD filter for which a larger but not too large number of PHD GM-fit iterations is better. However, it remains open how to best determine the most suitable number of GM-fit iterations for each filter in different cooperation configures. Instead of specifying the number of GM-fit iterations for fusion feedback for each filter, a technically more sound solution should be based on online evaluating the fusion convergence of the local filter. Fusion feedback should be terminated once the PHD-fit becomes worse with the increase of the number of GM-fit iterations. We leave this challenging issue to the future work. %immediately when it becomes not changing the local filter density significantly.

\begin{figure}
  \centering
  \includegraphics[width=8cm]{2MB2LMBosap.eps}\\
  \caption{Average OSPA error of two MB and two LMB filters in different cases of heterogeneous inter-filter PHD GM-fit} \label{fig:2MB2LMB-Aver-opsa}
  \vspace{-2mm}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=8cm]{2PHD2MBospa.eps}\\
  \caption{Average OSPA error of two PHD and two MB filters in different cases of heterogeneous inter-filter PHD GM-fit} \label{fig:2PHD2MB-Aver-opsa}
  \vspace{-2mm}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[width=8cm]{2PHD2LMBospa.eps}\\
  \caption{Average OSPA error of two PHD and two LMB filters in different cases of heterogeneous inter-filter PHD GM-fit} \label{fig:2PHD2LMB-Aver-opsa}
  \vspace{-2mm}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[width=8cm]{PHDMBLMBospa.eps}\\
  \caption{Average OSPA error of one PHD, one MB and one LMB filters in different cases of heterogeneous inter-filter PHD GM-fit} \label{fig:PHDMBLMB-Aver-opsa}
  \vspace{-2mm}
\end{figure}




\section{Conclusion and Future Work} \label{sec:conclusion}
We propose the first ever heterogenous unlabeled and labeled RFS filter collaboration approach which averages their unlabeled PHDs based on the GM implementation, seeking unlabeled PHD-AA fusion.
A computationally efficient, approximate approach is proposed which sequentially revises the weights of the L-GCs, perhaps in multiple iterations. Simulations have demonstrated the effectiveness of the proposed approach for both homogeneous and heterogeneous PHD-MB-LMB filter fusion. The proposed heterogenous fusion approach via optimizing the weight of L-GCs can benefit the PHD filter significantly by cooperating with the MB/LMB filters or the other PHD filters. However, the fusion gain to the MB/LMB filter is much lower than to the PHD filter. The reasons are twofold. % as addressed in remarks \ref{remark:insufficientPHD1} and \ref{remark:insufficientPHD_T2T}. %
First, the unlabeled PHD fusion applies no track/BC matching. Second, the PHD is only the first order moment of the MB/LMB density for which the PHD-consensus is insufficient for MB/LMB consensus.
The proposed approach leads to over-fit easier with the increase of the number of GM-fit iterations in the LMB filter than in the MB/PHD filters. It is open to optimally determine the best number of GM-fit iterations in different scenarios, or better solve the multi-object optimization problem \eqref{eq:opt_W_ISE} such as optimizing all parameters of the L-GCs not only the weights. %Better approximate approach to the PHD-GM-fit remains open.

Improvement can be expected if the BCs can be properly associated in the case of MB/LMB filter cooperation so that they can carry out B2B-based PHD fusion via GM-fit. This may then lead to a hybrid of unlabeled PHD fusion and B2B-based/labeled PHD fusion, as illustrated in Fig. \ref{fig:HHFframework}. However, reliable label/track matching remains an open-ended, challenging issue even for homogeneous MB/LMB filter fusion, which together with the proper definition of the divergence/distance between LRFS densities still need further investigation.


\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=7cm]{HHsensor.eps}\\  % _extended
  \caption{Heterogenous unlabeled/labeled RFS filter cooperation based on a hybrid of unlabeled PHD fusion and B2B/labeled PHD fusion accordingly. } \label{fig:HHFframework}
\end{figure}


\appendix
\subsection{Minimizing the ISE of GMs} \label{appendix:iseMIN}
The ISE between two GMs $p(\mathbf{x}) = \sum_{n \in \mathbf{I}_1}\alpha_n \mathcal{N}(\mathbf{x}; {\bm{\mu}_n},{\mathbf{P}_n}),
  q(\mathbf{x}) = \sum_{m \in \mathbf{I}_2}\beta_m\mathcal{N}(\mathbf{x}; {\mathbf{m}_m},{\mathbf{S}_m})$ is given as follows
\begin{align}
\text{ISE}\big(p\|q\big)
 \triangleq & \int{\big( p(\mathbf{x}) - q(\mathbf{x}) \big)^2 d \mathbf{x}} \\
%=& \sum\limits_{n,n'} {{\alpha_n}{\alpha _{n'}}} \int {{p_n}(x)} {p_{n'}}(x)dx \nonumber \\
%               & + \sum\limits_{m,m'} {{\beta_m}{\beta _{m'}}} \int {{q_m}(x)} {q_{m'}}(x)dx \nonumber\\
%               &- 2\sum\limits_{n,m} {{\alpha_n}{\beta_m}\int {{p_n}(x)} {q_m}(x)dx} \nonumber\\
& = \text{ISE}_{\alpha} + \text{ISE}_{\beta} -2\text{ISE}_{\alpha \beta}
%=  & \sum\limits_{n,n'\in \mathbf{I}_1} {{\alpha_n}{\alpha _{n'}}\mathcal{N}({\bm{\mu}_n}; {\bm{\mu} _{n'}},{\mathbf{P}_n}+ {\mathbf{P} _{n'}})}
% \nonumber \\
% & + \sum\limits_{m,m'\in \mathbf{I}_2} {{\beta_m}{\beta _{m'}}\mathcal{N}({\mathbf{m}_m}; {\mathbf{m}_{m'}},{\mathbf{S}_m} + {\mathbf{S}_{m'}})}  \nonumber \\
% & - 2\sum\limits_{n\in \mathbf{I}_1,m\in \mathbf{I}_2} {{\alpha_n}{\beta_m}\mathcal{N}({\bm{\mu}_n}; {\mathbf{m}_m},{\mathbf{P}_n}{\rm{ + }}{\mathbf{S}_m})}\label{eq:alphaBetaISE}
\end{align}
where
\begin{align}
\text{ISE}_\alpha & = \sum\limits_{n,n'\in \mathbf{I}_1} {{\alpha_n}{\alpha _{n'}}\mathcal{N}({\bm{\mu}_n}; {\bm{\mu} _{n'}},{\mathbf{P}_n}+ {\mathbf{P} _{n'}})}
 \label{eq:alphaISE}\\
\text{ISE}_\beta & = \sum\limits_{m,m'\in \mathbf{I}_2} {{\beta_m}{\beta _{m'}}\mathcal{N}({\mathbf{m}_m}; {\mathbf{m}_{m'}},{\mathbf{S}_m} + {\mathbf{S}_{m'}})}  \label{eq:BetaISE}\\
\text{ISE}_{\alpha \beta} & =  \sum\limits_{n\in \mathbf{I}_1,m\in \mathbf{I}_2} {{\alpha_n}{\beta_m}\mathcal{N}({\bm{\mu}_n}; {\mathbf{m}_m},{\mathbf{P}_n}{\rm{ + }}{\mathbf{S}_m})} \label{eq:alphaBetaISE}
\end{align}
%The ISE approach was first proposed for MR in the context of multiple hypothesis tracking in \cite{Williams06}, which inspired the normalized ISE \cite{Petrucci05} and further development \cite{Chen10}. %, and the squared distance \citep{Kurkoski08MR}.
%One distinctive feature of the method is the availability of exact analytical expressions for GMs. However, the ISE typically has many local minima; hence gradient-based methods cannot guarantee convergence to the global minimum, unless the initialization point happens to be close to the global minimum \cite{Williams06}. Of sufficient similarity to the ISE, %Cauchy-Schwarz inequality \citep{Scott01MR} and
%Wasserstein distance \cite{Assa18} and Pearson $\chi^2$-divergence \cite{Kitagawa20} are also used for GM reduction.

Here, it is straightforward to derive that
\begin{align}
  \frac{\partial  \text{ISE}\big(p\|q\big) }{\partial \alpha_n} = & 2\sum\limits_{n' \neq n, n' \in \mathbf{I}_1} {{\alpha _{n'}}\mathcal{N}({\bm{\mu}_n};{\bm{\mu} _{n'}},{\mathbf{P}_n}+ {\mathbf{P} _{n'}})}
\nonumber\\
 & + {2{\alpha _{n}}\mathcal{N}({\bm{\mu}_n}; {\bm{\mu} _{n}},2{\mathbf{P}_n} )} \nonumber\\
 & - 2\sum\limits_{m\in \mathbf{I}_2} {{\beta_m}\mathcal{N}({\bm{\mu}_n};{\mathbf{m}_m},{\mathbf{P}_n}{\rm{ + }}{\mathbf{S}_m})} \label{eq:derISE} \\
 \frac{\partial^2 \text{ISE}\big(p\|q\big) }{\partial \alpha_n^2}
 & = {2 \mathcal{N}({\bm{\mu}_n};{\bm{\mu}_n}, 2{\mathbf{P}_n})} \nonumber \\
 & = \frac{2}{{{\left| {{2\bf{P}}_n } \right|}^{1/2}} { (2\pi )}^{d/2} } \label{eq:TwicederISE} \\
 & >0
\end{align}
Setting \eqref{eq:derISE} %$\frac{\partial  \text{ISE}\big(p\|q\big)}{\partial \alpha_n}$
zero will lead to
\begin{align}
\alpha_n & =
% &\frac{\sum\limits_{m \in \mathbf{I}_2} {{\beta_m}\mathcal{N}({\bm{\mu}_n};{\mathbf{m}_m},{\mathbf{P}_n}{\rm{ + }}{\mathbf{S}_m})}} { \mathcal{N}({\bm{\mu}_n}; {\bm{\mu} _{n}},2{\mathbf{P}_n} )}\nonumber \\ &-\frac{\sum\limits_{n' \neq n, n'\in \mathbf{I}_1} {{\alpha _{n'}}\mathcal{N}({\bm{\mu}_n};{\bm{\mu} _{n'}},{\mathbf{P}_n} \rm{+} {\mathbf{P} _{n'}})}}{2 \mathcal{N}({\bm{\mu}_n}; {\bm{\mu} _{n}},2{\mathbf{P}_n} )} \label{eq:minISE-a}
  {{\left| {{2\bf{P}}_n } \right|}^{1/2}} { (2\pi )}^{d/2}\sum\limits_{m \in \mathbf{I}_2} {{\beta_m}\mathcal{N}({\bm{\mu}_n};{\mathbf{m}_m},{\mathbf{P}_n}{\rm{ + }}{\mathbf{S}_m})} \nonumber \\
&- {{\left| {{2\bf{P}}_n } \right|}^{1/2}} { (2\pi )}^{d/2}\sum\limits_{n' \neq n} {{\alpha _{n'}}\mathcal{N}({\bm{\mu}_n};{\bm{\mu} _{n'}},{\mathbf{P}_n} \rm{+} {\mathbf{P} _{n'}})}  \label{eq:minISE-a}
\end{align}
which yields the minimum ISE given all the other L-GC weights $\{\alpha_{n'}\}_{n' \neq n, n' \in \mathbf{I}_1}, \{\beta\}_{m \in \mathbf{I}_2}$.
%By comparing \eqref{eq:alphaBetaISE} with \eqref{eq:WeightISEfit}, it is obvious
%
%
%\subsection{Minimizing the CSD of GMs} \label{sec:iseMIN}
%The CSD between two distributions $p(\mathbf{x})$ and $q(\mathbf{x})$ is defined by:
%\[\begin{aligned}
%{D_{CS}}(q,p) =  &- \log \left( {\frac{{\int q (x)p(x)dx}}{{\sqrt {\int q {{(x)}^2}dx\int p {{(x)}^2}dx} }}} \right) \\
%=  &- \log \left( {\int q (x)p(x)dx} \right) \nonumber \\
%& + \frac{1}{2}\log \left( {\int q {{(x)}^2}dx} \right) + \frac{1}{2}\log \left( {\int p {{(x)}^2}dx} \right)
%\end{aligned}\]
%which, in the case of two GMs $p(\mathbf{x}) = \sum_{n \in \mathbf{I}_1}\alpha_n \mathcal{N}(\mathbf{x}; {\bm{\mu}_n},{\mathbf{P}_n}),
%  q(\mathbf{x}) = \sum_{m \in \mathbf{I}_2}\beta_m\mathcal{N}(\mathbf{x}; {\mathbf{m}_m},{\mathbf{S}_m})$, can be calculated as follows
%%\begin{align}
%%& \text{CSD}(q\|p)
%%= - \log \left( {\sum\limits_{n\in \mathbf{I}_1} {\sum\limits_{m \in \mathbf{I}_2} {{\alpha_n}} } {\beta_m}{z_{nm}}} \right) \nonumber \\
%% & + \frac{1}{2}\log \left( {\sum\limits_{n\in \mathbf{I}_1} {\frac{{\alpha_n^2{{\left| {{\bf{P}}_n^{ - 1}} \right|}^{1/2}}}}{{{{(2\pi )}^{d/2}}}}}  + 2\sum\limits_{n\in \mathbf{I}_1} {\sum\limits_{n' < n} {{\alpha_n}} } {\alpha _{n'}}{z_{nn'}}} \right) \\
%%&+ \frac{1}{2}\log \left( {\sum\limits_{m \in \mathbf{I}_2} {\frac{{\beta_m^2{{\left| {{\bf{S}}_m^{ - 1}} \right|}^{1/2}}}}{{{{(2\pi )}^{d/2}}}}}  + 2\sum\limits_{m \in \mathbf{I}_2} {\sum\limits_{m' < m} {{\beta_m}} } {\beta _{m'}}{z_{mm'}}} \right)
%%\end{align}
%\begin{align}
%& \text{CSD}(q\|p)
%= \frac{1}{2}\log \left( \text{ISE}_\alpha\right)  + \frac{1}{2}\log \left( \text{ISE}_\beta\right)- \log \left(\text{ISE}_{\alpha \beta}\right)
%\end{align}
%where \eqref{eq:alphaISE}, \eqref{eq:BetaISE} and \eqref{eq:alphaBetaISE} were used.
%
%Following \label{eq:derISE} and \label{eq:TwicederISE}, we can get
%\begin{align}
%  \frac{\partial  \text{CSD}\big(p\|q\big) }{\partial \alpha_n} = & \frac{1}{2\text{ISE}_\alpha} \sum\limits_{n' \neq n, n' \in \mathbf{I}_1} {{\alpha _{n'}}\mathcal{N}({\bm{\mu}_n};{\bm{\mu} _{n'}},{\mathbf{P}_n}+ {\mathbf{P} _{n'}})}
%\nonumber\\
% & + \frac{1}{\text{ISE}_\alpha} {{\alpha _{n}}\mathcal{N}({\bm{\mu}_n}; {\bm{\mu} _{n}},2{\mathbf{P}_n} )} \nonumber\\
% & - \frac{1}{\text{ISE}_\alpha\beta} \sum\limits_{m\in \mathbf{I}_2} {{\beta_m}\mathcal{N}({\bm{\mu}_n};{\mathbf{m}_m},{\mathbf{P}_n}{\rm{ + }}{\mathbf{S}_m})} \label{eq:derISE} \\
% \frac{\partial^2 \text{CSD}\big(p\|q\big) }{\partial \alpha_n^2} & >0
%\end{align}
%Setting \eqref{eq:derISE} %$\frac{\partial  \text{ISE}\big(p\|q\big)}{\partial \alpha_n}$
%zero will lead to
%\begin{align}
%\alpha_n & = complicated
%\end{align}
%%which yields the minimum ISE given that all the other L-GC weights $\{\alpha_{n'}\}_{n' \neq n, n' \in \mathbf{I}_1}, \{\beta\}_{m \in \mathbf{I}_2}$ are fixed.
%%By comparing \eqref{eq:alphaBetaISE} with \eqref{eq:WeightISEfit}, it is obvious


%\section*{References}
%\bibliographystyle{elsarticle-num}
\bibliographystyle{IEEEtran}
\bibliography{heterogenous}

%\begin{thebibliography}{Reference}
%% \bibitem{label}
%% Text of bibliographic item
%\bibitem{}
%\end{thebibliography}

\end{document}
%\endinput

%%
%% End of file `elsarticle-template-num.tex'.

