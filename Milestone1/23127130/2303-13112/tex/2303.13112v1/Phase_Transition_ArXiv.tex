
\documentclass[acmsmall, authorversion, nonacm]{acmart}
%\documentclass[sigconf, authordraft]{acmart}

\usepackage{booktabs} % For formal tables
%\usepackage{amssymb}
% subfig.sty for subfigures

\usepackage{amsfonts,amsmath}
%\usepackage[ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\usepackage{graphicx,epsfig}
%\usepackage{subfig}
\usepackage{color}
\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
%\acmDOI{10.475/123_4}

% ISBN
%\acmISBN{123-4567-24-567/08/06}
%Journal
%\acmJournal{TALG}
\iffalse
\acmVolume{37}
\acmNumber{4}
\acmArticle{111}
\acmMonth{5}
\fi
%Conference
%\acmConference[ACM SIGMETRICS 2019]{ACM SIG conference}{June 2019}{Phoenix, Arizona, USA}
%\acmYear{2023}
%\copyrightyear{2023}
%/IFIP Performance
%\acmPrice{15.00}

%\acmSubmissionID{123-A12-B3}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%      Line Spacing (e.g., \ls{1} for single, \ls{2} for double, even \ls{1.5})
%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setlength {\parindent}{0.14in} \normalmarginpar
%************************** MARGINPAR*******************************
%Comment out to remove margin pars
\newcommand {\mymarginpar}[1]{\marginpar{#1}}
\renewcommand {\marginpar}[1]{}


\def\_{\rule{.3em}{.15ex}}      % Get underscore by typing \_.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%      Line Spacing (e.g., \ls{1} for single, \ls{2} for double, even \ls{1.5})
%%
\newcommand{\ls}[1]
   {\dimen0=\fontdimen6\the\font
    \lineskip=#1\dimen0
    \advance\lineskip.5\fontdimen5\the\font
    \advance\lineskip-\dimen0
    \lineskiplimit=.9\lineskip
    \baselineskip=\lineskip
    \advance\baselineskip\dimen0
    \normallineskip\lineskip
    \normallineskiplimit\lineskiplimit
    \normalbaselineskip\baselineskip
    \ignorespaces
   }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% to be used in math mode:
\newcommand {\infin}{\infty}
\newcommand {\one}[1]{\mbox{$1\{ #1 \}$}}
\newcommand {\ass}{{\bf :=}}
\newcommand {\caret}{\widehat{~~}}
\newcommand {\bearn}{\begin{eqnarray*}}
\newcommand {\eearn}{\end{eqnarray*}}
\newcommand {\barr}{\begin{array}}
\newcommand {\earr}{\end{array}}
\newcommand {\cee}[2] {\left( \begin{array}{c}
         #1 \\ #2 \end{array} \right)}
\newcommand {\Set}{{\mathcal S}}

\newcommand\cf{{\mathcal F}}
\newcommand\cg{{\mathcal G}}
\newcommand\app{\approx}
%*****************PROBABILITY*****************
\newcommand {\given}{\; | \;}
\newcommand \absolute[1]{\left | #1 \right |}

%******************** CONvergence of  rv's
\newcommand\almostsure{\stackrel{a.s.}{\longrightarrow} }
\newcommand\indist{\stackrel{D}{\longrightarrow} }
\newcommand\inprob{\stackrel{P}{\longrightarrow} }

\newcommand\expect[1]{E \left [ \: #1 \: \right ] }
\newcommand\var[1]{\sigma_{#1}}
\newcommand\Var[1]{Var \left ({#1} \right )}
\newcommand\vartwo[1]{\sigma_{#1}^2}
\newcommand\expecttwo[2]{E_{#2} \left [ \: #1 \: \right ] }
\newcommand\xbar[1]{{\overline #1}}
\newcommand\xbartwo[2]{\left ({\overline #1} \right )      ^{#2}}
\newcommand\expectg[2]{E \[ #1 \given #2 \] }
\newcommand\indicator[2]{I_{#1}(#2)}
\newcommand\indicate[1]{I_{#1}}
\newcommand\generate[1]{{ \mathcal G}_{#1}}
\newcommand\moment[1]{{ \mathcal M}_{#1}}
\newcommand\laplace[1]{{\mathcal L}_{#1}}

\newcommand\equalst{=_{st}}
\newcommand\greaterst{>_{st}}
\newcommand\lesserst{<_{st}}
\newcommand\suchthat{\,:\,}
\newcommand\impulse[1]{u_0(#1)}

\newcommand\hospital{L'Hospital's rule }
\newcommand\Lr{L'Hospital's rule }
\newcommand\Rt{Rouch\'{e}'s theorem }
\def\defeq{\stackrel{\scriptstyle\rm def}{=}}
\def\twoLineSub#1#2{{#1}\atop{#2}}


\iffalse
%******************  NEWTHEOREMS**********************
\newtheorem{definition}{Definition}
\newtheorem{property}[definition]{Property}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{example}[definition]{Example}
\newtheorem{remark}[definition]{Remark}
\fi

%********************Random V ariables****************************
\newcommand{\degenerate}[1]
{\mbox {\bf  $\tilde D_{#1}$}}

\newcommand{\binomial}[3]
{\mbox {\bf  $\tilde B_{#1,#2}^{#3}$}}

\newcommand{\geometric}[3]
{\mbox {\bf  $\tilde G_{#1,#2}^{#3}$}}

\newcommand{\poisson}[2]
{\mbox {\bf  $\tilde P_{#1}^{#2}$}}

\newcommand{\erlang}[3]
{\mbox {\bf  $\tilde E_{#1,#2}^{#3}$}}

\newcommand{\uniform}[2]
{\mbox {\bf  $\tilde U_{#1}^{#2}$}}

\newcommand{\normal}[3]
{\mbox {\bf  $\tilde N_{#1,#2}^{#3}$}}

%********************** FACTORIAL EXPRESSIONS*****************
\newcommand{\comb}[2]
{\left ( \begin{array}{c} #1 \\#2 \end{array} \right ) }

\newcommand{\inlinecomb}[2]
{\mbox { \scriptsize $ \left ( \begin{array}{c} #1 \\#2
\end{array} \right ) $ \normalsize}}

\newcommand{\combr}[2] {
\left \langle \begin{array}{c} #1 \\#2
\end{array} \right \rangle }

\newcommand{\inlinecombr}[2]
{\mbox{ \scriptsize $ \left \langle \begin{array}{c} #1 \\#2
\end{array} \right \rangle
$ \normalsize}}

\newcommand{\stirltwo}[2] {
\left \{ \begin{array}{c} #1 \\#2
\end{array} \right \} }

\newcommand{\inlinestirltwo}[2]
{\mbox{ \scriptsize $ \left \{ \begin{array}{c} #1 \\#2
\end{array} \right \}
$ \normalsize}}


\newcommand{\stirlone}[2] {
\left [ \begin{array}{c} #1 \\#2
\end{array} \right ] }

\newcommand{\inlinestirlone}[2]
{\mbox{ \scriptsize $ \left [ \begin{array}{c} #1 \\#2
\end{array} \right ]
$ \normalsize}}

%\newcommand{\fact}[2]{(#1)_{#2}}
\newcommand{\factr}[2]{\langle #1\rangle _{#2}}

%*********************************LISTS*****************************
\newcommand {\benum} {\begin{enumerate}}
\newcommand {\eenum} {\end{enumerate}}

\newcommand {\bdesc} {\begin{description}}
\newcommand {\edesc} {\end{description}}
\newcommand {\ix}[1] {\index{#1}}

%************************INTEGRALS*******************************
\newcommand {\intlimits}[3] {\left. #1 \right |_{#3}^{#2}}

%************************** FIGURES*******************************
\newcommand {\bfig}[2] {\begin{figure}
  \centering
  \includegraphics[width=#2]{#1}}
\newcommand {\brotatefig}[2] {\begin{figure}[htbp]
                        \centerline {
                         \epsfig{figure={#1},clip=,angle=-90,width={#2}}}}
\newcommand {\bfigfirst}[2] {\begin{figure}[h]
                        \centerline {
                        \setlength{\epsfxsize}{#2}
                        \epsffile{#1}}}
\newcommand {\efig}[2]{ \caption{#2}
                        \label{fig:#1}
                        \end{figure}
                        \mymarginpar{fig:#1}}
\newcommand {\erotatefig}[2]{ \caption{#2}
                        \label{fig:#1}
                        \end{figure}
                        \mymarginpar{fig:#1}}
\newcommand {\rfig}[1]{Figure \ref{fig:#1}}

%************************** TABLES********************************
\newcommand {\btab}[1]{
                       \begin{table}
                       \centering
                       \begin{tabular}{#1}}
\newcommand {\etab}[3] {
                       \end{tabular}
                       \caption[#3]{#2}
                       \label{tab:#1}
                       \end{table}
                       \mymarginpar{tab:#1}
                       \vspace{.1in}}
\newcommand {\rtab}[1]{Table \ref{tab:#1}}

\newcommand {\btabular}[1]{\begin{center}
                       \begin{tabular}{#1}}
\newcommand {\etabular}{\end{tabular}
                       \end{center}}

%************************** DEFINITIONS********************************
\newcommand {\bdefin}[1]{\begin{definition}
                      \mymarginpar{def:#1}
                      \label{def:#1} }
\newcommand {\edefin}       {\end{definition}}
\newcommand {\rdef}[1]{Definition \ref{def:#1}}

%************************** PROPERTY********************************
\newcommand {\bpro}[1]{\begin{property}
                      \mymarginpar{pro:#1}
                      \label{pro:#1} }
\newcommand {\epro}   {\end{property}}
\newcommand {\rpro}[1]{Property \ref{pro:#1}}

%************************** PROPOSITION********************************
\newcommand {\bprop}[1]{\begin{proposition}
                      \mymarginpar{prop:#1}
                      \label{prop:#1} }
\newcommand {\eprop}       {\end{proposition}}
\newcommand {\rprop}[1]{Proposition \ref{prop:#1}}

%************************** LEMMA********************************
\newcommand {\blem}[1]{\begin{lemma}
                      \mymarginpar{lem:#1}
                      \label{lem:#1} }
\newcommand {\elem}   {\end{lemma}}
\newcommand {\rlem}[1]{Lemma \ref{lem:#1}}

%************************** THEOREM******************************
\newcommand {\bthe}[1]{\begin{theorem}
                      \mymarginpar{the:#1}
                      \label{the:#1} }
\newcommand {\ethe}   {\end{theorem}}
\newcommand {\rthe}[1]{Theorem \ref{the:#1}}

%************************** PROOF******************************
\newcommand {\bproof}{\noindent {\bf Proof.} \ }
\newcommand {\eproof} {\hfill \squares \\ \vspace{.3cm}}
%************************** COROLLARY******************************
\newcommand {\bcor}[1]{\begin{corollary}
                      \mymarginpar{cor:#1}
                      \label{cor:#1} }
\newcommand {\ecor}   {\end{corollary}}
\newcommand {\rcor}[1]{Corollary \ref{cor:#1}}

%************************** AXIOMS******************************
\newcommand {\bax}[1]{\begin{axiom}
                      \mymarginpar{ax:#1}
                      \label{ax:#1} }
\newcommand {\eax}       {\vspace{-.1in} \end{axiom}}
\newcommand {\rax}[1]{Axiom \ref{ax:#1}}

%************************** EXAMPLES **********************************
\newcommand {\bex}[2]{\vspace{.1in}
                      \begin{example}
                      \mymarginpar{ex:#1}
                       {\bf #2}
                      \label{ex:#1} }
\newcommand {\eex}       {\end{example} \vspace{.3cm} }
\newcommand {\rex}[1]{Example \ref{ex:#1}}

%************************** REMARK******************************
\newcommand {\brem}[1]{\begin{remark}
                      \mymarginpar{rem:#1}
                      \label{rem:#1} \em }
\newcommand {\erem}   {\end{remark}}
\newcommand {\rrem}[1]{Remark \ref{rem:#1}}

%************************** EQUATIONS**********************************
\newcommand {\beq}[1]{\mymarginpar{eq:#1}
                      \begin{equation}
                      \label{eq:#1} }

\newcommand {\beqno}[1]{\mymarginpar{eq:#1}
                      \begin{eqnarray}
                      \nonumber}

\newcommand {\eeq}       {\end{equation}}
\newcommand {\eeqno}       { && \end{eqnarray}}
\newcommand {\req}[1]{(\ref{eq:#1})}
\newcommand {\rear}[1]{(\ref{eqar:#1})}

\newcommand {\bear}[1]{\mymarginpar{eq:#1}
                       \begin{eqnarray}
                       \label{eq:#1} }

\newcommand {\bearno}[1]{\mymarginpar{eq:#1}
                       \begin{eqnarray}
                       \nonumber}

\newcommand {\eear}{\end{eqnarray}}
\newcommand {\eearno}{\end{eqnarray}}
%*****************SELECTION IN MATH*****************************
\newcommand {\bsel}{\left \{ \begin{array}{cl}}
\newcommand {\esel}{\end{array} \right.}

%*****************MATRICES IN MATH*****************************
\newcommand {\bmat}[1]{\left [ \begin{array}{#1}}
\newcommand {\emat}{\end{array} \right ]}
%************************** SECTIONS**********************************
\newcommand {\bsec}[2]{\mymarginpar{sec:#2}
                       \section{#1}
                       \label{sec:#2} }

\newcommand {\rsec}[1]{Section \ref{sec:#1}}

%***************************CHAPTER************************************
\newcommand {\rcha}[1]{Chapter #1}

%************************** SUBSECTIONS**********************************
\newcommand {\bsubsec}[2]{\mymarginpar{sec:#2}
                       \subsection{#1}
                       \label{sec:#2} }

\newcommand {\bsubsubsec}[2]{\mymarginpar{sec:#2}
                       \subsubsection{#1}
                       \label{sec:#2} }


\newcommand {\rsubsec}[1]{Section \ref{sec:#1}}

\newcommand {\heading}[1]{\vspace{.4in}
                          \noindent
                          \addcontentsline{toc}{subsection}
                          {\hspace{.5in} {\em #1}}
                           {\bf #1}
                           \vspace{.15in}}

\newcommand {\headingtwo}[1]{\vspace{.4in}
                          \noindent
                          \addcontentsline{toc}{subsection}
                          {\hspace{1in} {\em #1}}
                           {\bf #1}
                           \vspace{.15in}}

%************************** PROBLEMS****************************
\def\R{I\kern-0.30em R}
\def\N{I\kern-0.30em N}
\def\P{I\kern-0.30em P}
%*********** COmmands for Continuous Probability*******************
\newcommand\squares{\vrule height6pt width7pt depth1pt}
\newcommand{\de}{\buildrel \rm def \over =}
\renewcommand{\de}{\equiv}
\newcommand {\matrixnorm}[1]{||#1||_1}

% Macros used in this paper only


% Macros used in this paper only
\newcommand{\define}{\stackrel{\text{\tiny def}}{=}}
\newcommand{\se}{s}
\newcommand{\syn}{T}
\def\ex{{\bf\sf E}}
\def\pr{{\bf\sf P}}
\newcommand{\alphax}{X}
\newcommand{\peri}{p}
\newcommand{\rr}[1]{{\color{red} #1}}
\newcommand{\rb}[1]{{\color{blue} #1}}
\newcommand{\rg}[1]{{\color{green} #1}}

\newcommand{\aligneq}{\hspace*{-0.1in}&=&\hspace*{-0.1in}}
\newcommand{\alignleq}{\hspace*{-0.1in}&\leq&\hspace*{-0.1in}}
\newcommand{\alignless}{\hspace*{-0.1in}&<&\hspace*{-0.1in}}
\newcommand{\aligngeq}{\hspace*{-0.1in}&\geq&\hspace*{-0.1in}}
\newcommand{\aligngreater}{\hspace*{-0.1in}&>&\hspace*{-0.1in}}
\newcommand{\alignspace}{\hspace*{-0.1in}& &\hspace*{-0.1in}}
\newcommand{\alignrightarrow}{\hspace*{-0.1in}&\rightarrow&\hspace*{-0.1in}}

%\bibliog raphystyle{IEEE}

\begin{document}
\title{A Simple Explanation for the Phase Transition in Large Language Models with List Decoding}
% $N-2$ Degree of Rendezvous for the Multichannel Rendezvous Problem with $N$ channels}

%Packing the Pencil of Lines in a Finite Projective Plane
%\title{On the Asymptotic Approximation Ratio of Channel Hopping Sequences with Maximum Rendezvous Diversity}

%\title{IDEAL-CH: An Asymptotically Optimal Asynchronous Blind Rendezvous
%Algorithm with Maximum Rendezvous Diversity}


\iffalse
\author{Author 1, Author 2
%        ,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
\thanks{Author 1, Author 2}% <-this % stops a space
% <-this % stops a space
}


\author{Cheng-Shang Chang, ~\IEEEmembership{Fellow,~IEEE}\\
Institute of Communications Engineering\\
National Tsing Hua University \\
Hsinchu 30013, Taiwan, R.O.C. \\
Email:  cschang@ee.nthu.edu.tw}
\fi

\author{Cheng-Shang Chang}
\orcid{0000-0002-5386-4756}
\affiliation{%
	\institution{Institute of Communications Engineering,
		National Tsing Hua University}
	\city{Hsinchu}
	\country{Taiwan, R.O.C.}}
\email{cschang@ee.nthu.edu.tw}

\iffalse
%%% Use this for IEEE Communications letters
%% commented out for double-blind review
\author{Cheng-Shang~Chang,~\IEEEmembership{Fellow,~IEEE}
\thanks{C.-S. Chang is with  the Institute of Communications Engineering,
National Tsing Hua University,
Hsinchu 300, Taiwan, R.O.C.
email:   cschang@ee.nthu.edu.tw.}% <-this % stops a space
\thanks{This work was supported in part by the National Science and Technology under Grant
MOST 111-2221-E-007-045-MY3.}% <-this % stops a space
}
\fi



\begin{abstract}
Various recent experimental results show that large language models (LLM) exhibit emergent abilities that are not present in small models. System performance
is greatly improved after passing a certain critical threshold of scale. In this letter, we provide a simple explanation for such a phase transition phenomenon. For this, we model an LLM as a sequence-to-sequence random function.
Instead of using instant generation at each step, we use a list decoder that keeps a list of candidate sequences at each step and defers the generation of the output sequence at the end. We show that there is a critical threshold such that the expected number of {\em erroneous} candidate sequences remains bounded when an LLM is below the threshold, and it grows exponentially when an LLM is above the threshold. Such a threshold is related to the basic reproduction number in a contagious disease.

\end{abstract}

\maketitle

\iffalse
% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Phase transition, large language model.
\end{IEEEkeywords}
\fi

%\bsec{Introduction}{introduction}

%\IEEEPARstart{W}{ireless} networks


%\bsec{Problem statement}{statement}
\bsec{Introduction}{introduction}

%\IEEEPARstart{T}{he} multichannel

\iffalse
Language models have revolutionized natural language processing (NLP) in recent years. It is now well-known
that increasing the scale of language models (e.g., training compute, model parameters, etc.) can lead to
better performance and sample efficiency on a range of downstream NLP tasks (Devlin et al., 2019; Brown
et al., 2020, inter alia). In many cases, the effect of scale on performance can often be methodologically
predicted via scaling lawsâ€”for example, scaling curves for cross-entropy loss have been shown to empirically
span more than seven orders of magnitude (Kaplan et al., 2020; Hoffmann et al., 2022). On the other hand,
performance for certain downstream tasks counterintuitively does not appear to continuously improve as a
function of scale, and such tasks cannot be predicted ahead of time (Ganguli et al., 2022).
\fi

In recent years, language models have transformed natural language processing (NLP). It's now widely acknowledged that scaling up language models, such as increasing the training compute and model parameters, can enhance their performance and sample efficiency across a wide range of downstream NLP tasks (see, e.g., \cite{devlin2018bert,brown2020language}).
In many cases, scaling laws can be used to predict the impact of scale on performance \cite{kaplan2020scaling,hoffmann2022training}.
However, some downstream tasks' performance does not continuously improve with scale, which is contrary to expectations, and these tasks cannot be predicted in advance \cite{ganguli2022predictability}.


In recent papers \cite{wei2022emergent,wei2022inverse,GPT4}, it was shown by various numerical examples that
large language models (LLM) exhibit emergent abilities that are not present in small models. System performance
is greatly improved after passing a certain critical threshold of scale.
Such a phenomenon is commonly known as a {\em phase transition} in network science (see, e.g., the book \cite{Newman2010}). As indicated in \cite{wei2022emergent}, there are currently limited persuasive justifications for the manner in which these abilities develop.

The main objective of this letter is to provide a simple explanation for this phase transition phenomenon in LLMs.
For this, we model an LLM as a sequence-to-sequence random function on a certain token space with $M$ possible tokens. We assume there is an oracle that can always generate the desired output sequence to complete a requested task by the prompt sequence. The accuracy of an LLM is determined by the probability that an LLM can generate the same output sequence by the oracle. Instead of using the instant selection from a set of eligible tokens at each step in most LLMs in the literature, we use a list decoder that keeps a list of candidate sequences at each step and defers the generation of the output sequence at the end. At each step, an LLM examines a candidate sequence and enlarges it by appending a token that the LLM classifies to be eligible. There might be multiple eligible tokens for a candidate sequence, and the number of candidate sequences might grow with respect to the number of steps. We show that if the eligible token classier at each step has a bounded false alarm probability $\epsilon$ and $M \epsilon <1$, then the expected number of erroneous sequences (that are different from the oracle sequence) is bounded by a constant at each step, and thus the accuracy of an LLM can be guaranteed. On the other hand, if $M \epsilon>1$, then the expected number of erroneous sequences grows exponentially with respect to the number of steps, and there is no guarantee for accuracy. As such, there is a critical point $M \epsilon=1$. Transformer-based LLMs with more parameters and more training can memorize more patterns
\cite{vaswani2017attention,ramsauer2020hopfield} and thus are
more likely to bring down the false alarm probability $\epsilon$ below the percolation threshold $1/M$.

\bsec{Mathematical analysis}{analysis}

\bsubsec{Mathematical formulation for LLMs}{formulation}

LLMs such as GPT-4 \cite{GPT4} and PaLM-E \cite{driess2023palm} take a sequence of tokens as their input (prompts) and generate another sequence of tokens as their output (answers). To model these,
denote by ${\mathcal I}$ (resp. ${\mathcal O}$) be the input (resp. output) token space.
Without loss of generality, we
assume that both
%the total number of output tokens is $M$, i.e., $|{\mathcal O\}}=M$, and
the maximum length of an input  sequence and that of an output sequence are bounded by $N$.
An LLM can be represented by a random function
$$\phi: {\mathcal I}^{N} \mapsto {\mathcal O}^{N}.$$
Denote by ${\bf x}=(x_1, x_2, \ldots, x_N)$ (resp. ${\bf y}=(y_1, y_2, \ldots, y_N)$)  be the input (resp. output) sequence. For $1 \le t \le N$, let ${\bf y}_{1:t}=(y_1, y_2, \ldots, y_t)$.
An LLM is said to be {\em autoregressive} if $y_{t+1}$ is a random function of ${\bf x}$ and ${\bf y}_{1:t}$.
To generate $y_{t+1}$, a common practice for a Transformer-based autoregressive LLM is to generate a set of candidate tokens (e.g., the top-p sampling \cite{brown2020language}) and use a selection method (e.g., the Boltzmann selection) to select one token from the candidate set. One major drawback of an autoregressive LLM is that an incorrect selection for $y_{t+1}$ might lead to a cascaded ``hallucination'' (that generates an unrelated output). A better approach to address such a problem is to keep the candidate set in each step and defer the selection toward the end. Such an approach is commonly known as {\em list decoding} for communication systems (see, e.g., \cite{elias1957list,tal2015list,amalladinne2020coded}).

\bsubsec{An oracle}{oracle}

To compare the performance of an LLM, we assume that there is an oracle that can always generate the desired {\em deterministic} output sequence to complete the task requested by a prompt. Denote by $\phi^o$ the oracle function and ${\bf y^o}=(y_1^o, y_2^o, \ldots, y_N^o)$ be the oracle output sequence (with respect to the input $\bf x$). Similarly, let ${\bf y}^o_{1:t}=(y_1^o, y_2^o, \ldots, y_t^o)$.
The accuracy of an LLM $\phi$ (for the input $\bf x$) is defined as
\beq{accuracy1111}
\pr (\phi ({\bf x})= \phi^o ({\bf x})).
\eeq
%when the input sequence ${\bf x}$ is sampled according to a unknown distribution $\mathcal P({\bf x})$ from the input token space ${\mathcal I}^{N}$.
Intuitively, an LLM with a large accuracy is more likely to complete a requested task than one with a low accuracy.
% We assume that the desired output sequence is {\em unique}

\bsubsec{Equivalent representation of an autoregressive LLM with list decoding as a sequence of binary classifiers}{classifier}

For an autoregressive LLM with list decoding, we need to keep track of candidate sets in each step.
For this, let $L_t$ be the set of candidate sequences after the $t$-th step.
The process of   generating candidates in the $t+1$-th step is equivalent to using
a binary classifier to classify all the tokens in $\mathcal O$ into two sets: {\em eligible} (with output 1) and {\em not eligible} (with output 0).
Denote such a binary classifier by the random function:
$${\mathcal C}_{t+1}(y|{\bf x},{\bf y}_{1:t})$$ for $y \in {\mathcal O}$ given a prompt {\bf x} and a sequence ${\bf y}_{1:t}$.

For a candidate sequence $ {\bf y}_{1:t} \in L_t$, if a token $y$ is eligible, i.e.,
$${\mathcal C}_{t+1}(y|{\bf x},{\bf y}_{1:t})=1,$$
then the sequence ${\bf y}_{1:t}+y$
 is  added to $L_{t+1}$, where $+$ denotes the concatenation operation.
Note that for a given ${\bf x}$ and a given ${\bf y}_{1:t}$, there might be multiple $y's$ that are classified as eligible tokens at the $t+1$-{th} step. On the other hand, it is also possible that none of them are eligible.
As such, the number of candidate sequences in $L_{t}$ might vary with respect to $t$.
In \rfig{example}, we illustrate the evolution of the candidate set with respect to the number of steps. The oracle sequence is marked in blue. The erroneous sequences are marked in red.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{candidateexample2.png}
	\caption{An illustration of the evolution of the candidate set with respect to the number of steps. The oracle sequence is marked in blue. The erroneous sequences are marked in red.}
	\label{fig:example}
\end{figure}

In the following, we define the notion of $\epsilon$-compatible LLMs that could lead to comparable performance to the oracle when $\epsilon$ is very small. The key insight of this, as shown in \rfig{example}, is that the length of an erroneous sequence cannot be too long, and it will vanish in a small number of steps.


\bdefin{compatible} An autoregressive LLM with list decoding is said to be $\epsilon$-compatible (to the oracle)
if it satisfies the following two properties:
\begin{description}
\item[(i)] 100\% recall (no missing error): the output sequence generated by the oracle is always in the candidate sets for all $t$. Specifically,
if ${\bf y}_{1:t}+y ={\bf y}^o_{1:t+1}$, then
\beq{comp1111}
{\mathcal C}_{t+1}(y|{\bf x},{\bf y}_{1:t})=1
\eeq
for all $t$ and ${\bf x}$.
\item[(ii)] bounded false alarm probability: the false alarm probability is bounded by $\epsilon$. Specifically,
if
$${\bf y}_{1:t}+y \ne {\bf y}^o_{1:t+1},$$ then
\beq{comp2222}
\pr \Big ({\mathcal C}_{t+1}(y|{\bf x},{\bf y}_{1:t})=1 \Big) \le \epsilon
\eeq
for all $t$.
% when ${\bf x}$ is sampled according to a certain distribution $\mathcal P({\bf x})$ in ${\mathcal I}^N$.
\end{description}
\edefin

\bsubsec{A sufficient condition for guaranteed accuracy}{sufficient}

A sequence ${\bf y}_{1:t}$ in $L_t$ is said to be {\em erroneous} if ${\bf y}_{1:t}\ne {\bf y}^o_{1:t}$.
For an $\epsilon$-compatible LLM, we know that ${\bf y}^o_{1:t}$ is in $L_t$, and thus the number of
erroneous sequences in $L_t$ is exactly $|L_t|-1$. In the following theorem, we show that the expected number of
erroneous candidate sequences is bounded by a constant for an $\epsilon$-compatible LLM if
$M \epsilon<1$, where $M$ is the total number of tokens in the output token space ${\mathcal O}$.

%On the other hand, the complexity of using a non-autoregressive approach is very high as it requir

%Let ${\bf i}=(i_1, \ldots, i_

\bthe{main}
Let   $R_t=|L_t|-1$ be the number of erroneous candidate sequences in $L_t$.
For an $\epsilon$-compatible LLM, if $M \epsilon <1$, then for all $t$
\beq{main0000}
 \ex [R_{t}] \le \frac{M \epsilon}{1-M \epsilon}.
 \eeq
  \ethe

\bproof
We first show that
\beq{main1111}
 \ex[R_{t+1} |L_t] \le (M \epsilon) (R_t +1).
 \eeq
 Since the LLM is $\epsilon$-compatible, we know that the sequence ${\bf y}^o_{1:t}$ must be in $L_t$.
 On average, there are (less than) $(M-1)\epsilon$ erroneous sequences of the form ${\bf y}^o_{1:t}+y$ in $L_{t+1}$.
 On the other hand, an erroneous sequence ${\bf y}_{1:t}$ in $L_t$ generates on average $M \epsilon$ erroneous sequences of the form ${\bf y}_{1:t}+y$ in $L_{t+1}$. Since there are $R_t$ erroneous sequences in $L_t$,
 the expected number of erroneous sequences in $L_{t+1}$ (given $L_t)$ is bounded above by $M \epsilon R_t+ (M-1)\epsilon$. This shows the inequality in \req{main1111}. Taking the expectation on both sides of \req{main1111} leads to
\beq{main2222}
 \ex[R_{t+1} ] \le (M \epsilon) (\ex[R_t] +1).
 \eeq
 Since $R_0=0$ and $M \epsilon <1$, it is easy to show by induction that the inequality in
 \req{main0000} holds.

\eproof

As a direct consequence of the Markov inequality, we have from \rthe{main} that
\beq{main3333}
\pr( R_t=0) =1- \pr (R_t \ge 1) \ge \frac{1-2M\epsilon}{1-M\epsilon}.
\eeq
Thus, with a nonzero probability $\frac{1-2M\epsilon}{1-M\epsilon}$, an $\epsilon$-compatible LLM can generate the same
output sequence as the oracle. The accuracy for an $\epsilon$-compatible LLM is at least $\frac{1-2M\epsilon}{1-M\epsilon}$.

\bsubsec{The phase transition}{phase}

To see the phase transition,
suppose that the inequality in \req{comp2222} is reversed and $M \epsilon>1$. Following the same induction argument in the proof of \rthe{main}, one can show that
\beq{main4444}
\ex[R_t] \ge (M \epsilon)^{t}.
\eeq
Thus,
the expected number of erroneous sequences in $L_t$ grows exponentially with respect to the number of steps. As such,
the accuracy of the LLM is low.

In view of \req{main0000} and \req{main4444}, the critical point for the phase transition is $M \epsilon =1$.
This role of $M \epsilon$ is analogous to the basic reproduction number in a contagious disease \cite{Newman2010,chen2020time}. If the basic reproduction number is smaller than 1, then the disease can be contained. On the other hand, if the basic reproduction number is larger than 1, then it is likely to have a large outbreak.

\bsec{Conclusion}{con}


In this letter, we provided a simple explanation for the phase transition in LLMs. For an $\epsilon$-compatible LLM, we showed that the expected number of erroneous sequences is bounded by a constant and the accuracy of the LLM can be guaranteed when $M \epsilon <1$. On the other hand, if  the inequality in \req{comp2222} is reversed and $M \epsilon>1$, then the expected number of erroneous sequences grows exponentially with respect to the number of steps.

One possible extension of the list decoder is to track the probability of each candidate sequence. For most Transformer-based LLMs, it is possible to compute such a probability. Then the output sequence is generated by the candidate sequence with the largest probability at the end.

%  for the multichannel rendezvous problem in the symmetric, asynchronous, and heterogeneous setting.
%  Our new result has a roughly 50\% reduction of the state-of-the-art MTTR bound in the literature.

\begin{acks}

This work was supported in part by the National Science and Technology under Grant
MOST 111-2221-E-007-045-MY3.
\end{acks}

%%% -*-BibTeX-*-
%%% Do NOT edit. File created by BibTeX with style
%%% ACM-Reference-Format-Journals [18-Jan-2012].

\begin{thebibliography}{00}

%%% ====================================================================
%%% NOTE TO THE USER: you can override these defaults by providing
%%% customized versions of any of these macros before the \bibliography
%%% command.  Each of them MUST provide its own final punctuation,
%%% except for \shownote{}, \showDOI{}, and \showURL{}.  The latter two
%%% do not use final punctuation, in order to avoid confusing it with
%%% the Web address.
%%%
%%% To suppress output of a particular field, define its macro to expand
%%% to an empty string, or better, \unskip, like this:
%%%
%%% \newcommand{\showDOI}[1]{\unskip}   % LaTeX syntax
%%%
%%% \def \showDOI #1{\unskip}           % plain TeX syntax
%%%
%%% ====================================================================

\ifx \showCODEN    \undefined \def \showCODEN     #1{\unskip}     \fi
\ifx \showDOI      \undefined \def \showDOI       #1{#1}\fi
\ifx \showISBNx    \undefined \def \showISBNx     #1{\unskip}     \fi
\ifx \showISBNxiii \undefined \def \showISBNxiii  #1{\unskip}     \fi
\ifx \showISSN     \undefined \def \showISSN      #1{\unskip}     \fi
\ifx \showLCCN     \undefined \def \showLCCN      #1{\unskip}     \fi
\ifx \shownote     \undefined \def \shownote      #1{#1}          \fi
\ifx \showarticletitle \undefined \def \showarticletitle #1{#1}   \fi
\ifx \showURL      \undefined \def \showURL       {\relax}        \fi
% The following commands are used for tagged output and should be
% invisible to TeX
\providecommand\bibfield[2]{#2}
\providecommand\bibinfo[2]{#2}
\providecommand\natexlab[1]{#1}
\providecommand\showeprint[2][]{arXiv:#2}

\bibitem[\protect\citeauthoryear{Amalladinne, Chamberland, and
  Narayanan}{Amalladinne et~al\mbox{.}}{2020}]%
        {amalladinne2020coded}
\bibfield{author}{\bibinfo{person}{Vamsi~K Amalladinne},
  \bibinfo{person}{Jean-Francois Chamberland}, {and} \bibinfo{person}{Krishna~R
  Narayanan}.} \bibinfo{year}{2020}\natexlab{}.
\newblock \showarticletitle{A coded compressed sensing scheme for unsourced
  multiple access}.
\newblock \bibinfo{journal}{{\em IEEE Transactions on Information Theory\/}}
  \bibinfo{volume}{66}, \bibinfo{number}{10} (\bibinfo{year}{2020}),
  \bibinfo{pages}{6509--6533}.
\newblock


\bibitem[\protect\citeauthoryear{Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al\mbox{.}}{Brown
  et~al\mbox{.}}{2020}]%
        {brown2020language}
\bibfield{author}{\bibinfo{person}{Tom Brown}, \bibinfo{person}{Benjamin Mann},
  \bibinfo{person}{Nick Ryder}, \bibinfo{person}{Melanie Subbiah},
  \bibinfo{person}{Jared~D Kaplan}, \bibinfo{person}{Prafulla Dhariwal},
  \bibinfo{person}{Arvind Neelakantan}, \bibinfo{person}{Pranav Shyam},
  \bibinfo{person}{Girish Sastry}, \bibinfo{person}{Amanda Askell},
  {et~al\mbox{.}}} \bibinfo{year}{2020}\natexlab{}.
\newblock \showarticletitle{Language models are few-shot learners}.
\newblock \bibinfo{journal}{{\em Advances in Neural Information Processing
  Systems\/}}  \bibinfo{volume}{33} (\bibinfo{year}{2020}),
  \bibinfo{pages}{1877--1901}.
\newblock


\bibitem[\protect\citeauthoryear{Chen, Lu, Chang, and Liu}{Chen
  et~al\mbox{.}}{2020}]%
        {chen2020time}
\bibfield{author}{\bibinfo{person}{Yi-Cheng Chen}, \bibinfo{person}{Ping-En
  Lu}, \bibinfo{person}{Cheng-Shang Chang}, {and} \bibinfo{person}{Tzu-Hsuan
  Liu}.} \bibinfo{year}{2020}\natexlab{}.
\newblock \showarticletitle{A time-dependent SIR model for COVID-19 with
  undetectable infected persons}.
\newblock \bibinfo{journal}{{\em IEEE Transactions on Network Science and
  Engineering\/}} \bibinfo{volume}{7}, \bibinfo{number}{4}
  (\bibinfo{year}{2020}), \bibinfo{pages}{3279--3294}.
\newblock


\bibitem[\protect\citeauthoryear{Devlin, Chang, Lee, and Toutanova}{Devlin
  et~al\mbox{.}}{2018}]%
        {devlin2018bert}
\bibfield{author}{\bibinfo{person}{Jacob Devlin}, \bibinfo{person}{Ming-Wei
  Chang}, \bibinfo{person}{Kenton Lee}, {and} \bibinfo{person}{Kristina
  Toutanova}.} \bibinfo{year}{2018}\natexlab{}.
\newblock \showarticletitle{{BERT}: Pre-training of deep bidirectional
  transformers for language understanding}.
\newblock \bibinfo{journal}{{\em arXiv preprint arXiv:1810.04805\/}}
  (\bibinfo{year}{2018}).
\newblock


\bibitem[\protect\citeauthoryear{Driess, Xia, Sajjadi, Lynch, Chowdhery,
  Ichter, Wahid, Tompson, Vuong, Yu, et~al\mbox{.}}{Driess
  et~al\mbox{.}}{2023}]%
        {driess2023palm}
\bibfield{author}{\bibinfo{person}{Danny Driess}, \bibinfo{person}{Fei Xia},
  \bibinfo{person}{Mehdi~SM Sajjadi}, \bibinfo{person}{Corey Lynch},
  \bibinfo{person}{Aakanksha Chowdhery}, \bibinfo{person}{Brian Ichter},
  \bibinfo{person}{Ayzaan Wahid}, \bibinfo{person}{Jonathan Tompson},
  \bibinfo{person}{Quan Vuong}, \bibinfo{person}{Tianhe Yu}, {et~al\mbox{.}}}
  \bibinfo{year}{2023}\natexlab{}.
\newblock \showarticletitle{{PaLM-E}: An Embodied Multimodal Language Model}.
\newblock \bibinfo{journal}{{\em arXiv preprint arXiv:2303.03378\/}}
  (\bibinfo{year}{2023}).
\newblock


\bibitem[\protect\citeauthoryear{Elias}{Elias}{1957}]%
        {elias1957list}
\bibfield{author}{\bibinfo{person}{Peter Elias}.}
  \bibinfo{year}{1957}\natexlab{}.
\newblock \showarticletitle{List decoding for noisy channels}.
\newblock  (\bibinfo{year}{1957}).
\newblock


\bibitem[\protect\citeauthoryear{Ganguli, Hernandez, Lovitt, Askell, Bai, Chen,
  Conerly, Dassarma, Drain, Elhage, et~al\mbox{.}}{Ganguli
  et~al\mbox{.}}{2022}]%
        {ganguli2022predictability}
\bibfield{author}{\bibinfo{person}{Deep Ganguli}, \bibinfo{person}{Danny
  Hernandez}, \bibinfo{person}{Liane Lovitt}, \bibinfo{person}{Amanda Askell},
  \bibinfo{person}{Yuntao Bai}, \bibinfo{person}{Anna Chen},
  \bibinfo{person}{Tom Conerly}, \bibinfo{person}{Nova Dassarma},
  \bibinfo{person}{Dawn Drain}, \bibinfo{person}{Nelson Elhage},
  {et~al\mbox{.}}} \bibinfo{year}{2022}\natexlab{}.
\newblock \showarticletitle{Predictability and surprise in large generative
  models}. In \bibinfo{booktitle}{{\em 2022 ACM Conference on Fairness,
  Accountability, and Transparency}}. \bibinfo{pages}{1747--1764}.
\newblock


\bibitem[\protect\citeauthoryear{Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al\mbox{.}}{Hoffmann
  et~al\mbox{.}}{2022}]%
        {hoffmann2022training}
\bibfield{author}{\bibinfo{person}{Jordan Hoffmann}, \bibinfo{person}{Sebastian
  Borgeaud}, \bibinfo{person}{Arthur Mensch}, \bibinfo{person}{Elena
  Buchatskaya}, \bibinfo{person}{Trevor Cai}, \bibinfo{person}{Eliza
  Rutherford}, \bibinfo{person}{Diego de~Las Casas}, \bibinfo{person}{Lisa~Anne
  Hendricks}, \bibinfo{person}{Johannes Welbl}, \bibinfo{person}{Aidan Clark},
  {et~al\mbox{.}}} \bibinfo{year}{2022}\natexlab{}.
\newblock \showarticletitle{Training compute-optimal large language models}.
\newblock \bibinfo{journal}{{\em arXiv preprint arXiv:2203.15556\/}}
  (\bibinfo{year}{2022}).
\newblock


\bibitem[\protect\citeauthoryear{Kaplan, McCandlish, Henighan, Brown, Chess,
  Child, Gray, Radford, Wu, and Amodei}{Kaplan et~al\mbox{.}}{2020}]%
        {kaplan2020scaling}
\bibfield{author}{\bibinfo{person}{Jared Kaplan}, \bibinfo{person}{Sam
  McCandlish}, \bibinfo{person}{Tom Henighan}, \bibinfo{person}{Tom~B Brown},
  \bibinfo{person}{Benjamin Chess}, \bibinfo{person}{Rewon Child},
  \bibinfo{person}{Scott Gray}, \bibinfo{person}{Alec Radford},
  \bibinfo{person}{Jeffrey Wu}, {and} \bibinfo{person}{Dario Amodei}.}
  \bibinfo{year}{2020}\natexlab{}.
\newblock \showarticletitle{Scaling laws for neural language models}.
\newblock \bibinfo{journal}{{\em arXiv preprint arXiv:2001.08361\/}}
  (\bibinfo{year}{2020}).
\newblock


\bibitem[\protect\citeauthoryear{Newman}{Newman}{2009}]%
        {Newman2010}
\bibfield{author}{\bibinfo{person}{Mark Newman}.}
  \bibinfo{year}{2009}\natexlab{}.
\newblock \bibinfo{booktitle}{{\em Networks: an introduction}}.
\newblock \bibinfo{publisher}{OUP Oxford}.
\newblock


\bibitem[\protect\citeauthoryear{OpenAI}{OpenAI}{2023}]%
        {GPT4}
\bibfield{author}{\bibinfo{person}{OpenAI}.} \bibinfo{year}{2023}\natexlab{}.
\newblock \showarticletitle{{GPT}-4 Technical Report}.
\newblock \bibinfo{journal}{{\em https://cdn.openai.com/papers/gpt-4.pdf\/}}
  (\bibinfo{year}{2023}).
\newblock


\bibitem[\protect\citeauthoryear{Ramsauer, Sch{\"a}fl, Lehner, Seidl, Widrich,
  Adler, Gruber, Holzleitner, Pavlovi{\'c}, Sandve, et~al\mbox{.}}{Ramsauer
  et~al\mbox{.}}{2020}]%
        {ramsauer2020hopfield}
\bibfield{author}{\bibinfo{person}{Hubert Ramsauer}, \bibinfo{person}{Bernhard
  Sch{\"a}fl}, \bibinfo{person}{Johannes Lehner}, \bibinfo{person}{Philipp
  Seidl}, \bibinfo{person}{Michael Widrich}, \bibinfo{person}{Thomas Adler},
  \bibinfo{person}{Lukas Gruber}, \bibinfo{person}{Markus Holzleitner},
  \bibinfo{person}{Milena Pavlovi{\'c}}, \bibinfo{person}{Geir~Kjetil Sandve},
  {et~al\mbox{.}}} \bibinfo{year}{2020}\natexlab{}.
\newblock \showarticletitle{Hopfield networks is all you need}.
\newblock \bibinfo{journal}{{\em arXiv preprint arXiv:2008.02217\/}}
  (\bibinfo{year}{2020}).
\newblock


\bibitem[\protect\citeauthoryear{Tal and Vardy}{Tal and Vardy}{2015}]%
        {tal2015list}
\bibfield{author}{\bibinfo{person}{Ido Tal} {and} \bibinfo{person}{Alexander
  Vardy}.} \bibinfo{year}{2015}\natexlab{}.
\newblock \showarticletitle{List decoding of polar codes}.
\newblock \bibinfo{journal}{{\em IEEE Transactions on Information Theory\/}}
  \bibinfo{volume}{61}, \bibinfo{number}{5} (\bibinfo{year}{2015}),
  \bibinfo{pages}{2213--2226}.
\newblock


\bibitem[\protect\citeauthoryear{Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}{Vaswani et~al\mbox{.}}{2017}]%
        {vaswani2017attention}
\bibfield{author}{\bibinfo{person}{Ashish Vaswani}, \bibinfo{person}{Noam
  Shazeer}, \bibinfo{person}{Niki Parmar}, \bibinfo{person}{Jakob Uszkoreit},
  \bibinfo{person}{Llion Jones}, \bibinfo{person}{Aidan~N Gomez},
  \bibinfo{person}{{\L}ukasz Kaiser}, {and} \bibinfo{person}{Illia
  Polosukhin}.} \bibinfo{year}{2017}\natexlab{}.
\newblock \showarticletitle{Attention is all you need}.
\newblock \bibinfo{journal}{{\em Advances in Neural Information Processing
  Systems\/}}  \bibinfo{volume}{30} (\bibinfo{year}{2017}).
\newblock


\bibitem[\protect\citeauthoryear{Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud,
  Yogatama, Bosma, Zhou, Metzler, et~al\mbox{.}}{Wei et~al\mbox{.}}{2022b}]%
        {wei2022emergent}
\bibfield{author}{\bibinfo{person}{Jason Wei}, \bibinfo{person}{Yi Tay},
  \bibinfo{person}{Rishi Bommasani}, \bibinfo{person}{Colin Raffel},
  \bibinfo{person}{Barret Zoph}, \bibinfo{person}{Sebastian Borgeaud},
  \bibinfo{person}{Dani Yogatama}, \bibinfo{person}{Maarten Bosma},
  \bibinfo{person}{Denny Zhou}, \bibinfo{person}{Donald Metzler},
  {et~al\mbox{.}}} \bibinfo{year}{2022}\natexlab{b}.
\newblock \showarticletitle{Emergent abilities of large language models}.
\newblock \bibinfo{journal}{{\em arXiv preprint arXiv:2206.07682\/}}
  (\bibinfo{year}{2022}).
\newblock


\bibitem[\protect\citeauthoryear{Wei, Tay, and Le}{Wei et~al\mbox{.}}{2022a}]%
        {wei2022inverse}
\bibfield{author}{\bibinfo{person}{Jason Wei}, \bibinfo{person}{Yi Tay}, {and}
  \bibinfo{person}{Quoc~V Le}.} \bibinfo{year}{2022}\natexlab{a}.
\newblock \showarticletitle{Inverse scaling can become U-shaped}.
\newblock \bibinfo{journal}{{\em arXiv preprint arXiv:2211.02011\/}}
  (\bibinfo{year}{2022}).
\newblock


\end{thebibliography}



\iffalse
%\bibliographystyle{IEEEtran}
%\bibliography{bibliography}
%\bibliography{IEEEabrv}
\begin{thebibliography}{99}




\bibitem{Book} Z. Gu, Y. Wang, Q.-S. Hua, and F. C. M. Lau, {\em Rendezvous in Distributed Systems:
Theory, Algorithms and Applications}. Springer, 2017.



\end{thebibliography}
\fi


\end{document}
