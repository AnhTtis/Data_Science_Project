{
    "arxiv_id": "2303.11305",
    "paper_title": "SVDiff: Compact Parameter Space for Diffusion Fine-Tuning",
    "authors": [
        "Ligong Han",
        "Yinxiao Li",
        "Han Zhang",
        "Peyman Milanfar",
        "Dimitris Metaxas",
        "Feng Yang"
    ],
    "submission_date": "2023-03-20",
    "revised_dates": [
        "2023-04-11"
    ],
    "latest_version": 3,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Diffusion models have achieved remarkable success in text-to-image generation, enabling the creation of high-quality images from text prompts or other modalities. However, existing methods for customizing these models are limited by handling multiple personalized subjects and the risk of overfitting. Moreover, their large number of parameters is inefficient for model storage. In this paper, we propose a novel approach to address these limitations in existing text-to-image diffusion models for personalization. Our method involves fine-tuning the singular values of the weight matrices, leading to a compact and efficient parameter space that reduces the risk of overfitting and language-drifting. We also propose a Cut-Mix-Unmix data-augmentation technique to enhance the quality of multi-subject image generation and a simple text-based image editing framework. Our proposed SVDiff method has a significantly smaller model size (1.7MB for StableDiffusion) compared to existing methods (vanilla DreamBooth 3.66GB, Custom Diffusion 73MB), making it more practical for real-world applications.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.11305v1",
        "http://arxiv.org/pdf/2303.11305v2",
        "http://arxiv.org/pdf/2303.11305v3"
    ],
    "publication_venue": "Revised appendix with the addition of cross-attention regularization for single-subject generation"
}