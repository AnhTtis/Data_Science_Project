% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{times}
\usepackage{epsfig}
\usepackage[table,dvipsnames]{xcolor}
\definecolor{myblue}{RGB}{47, 114, 173}

\usepackage{tocloft}
\usepackage{titletoc}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\renewcommand*\contentsname{\normalsize Content of Appendix}
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\newcommand{\svdiff}{{SVDiff}}

% \usepackage{subfig}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\definecolor{myred}{RGB}{220,50,47} % Solarized color
\definecolor{mygreen}{RGB}{133,153,0}
\definecolor{newgreen}{RGB}{117,251,97}
\definecolor{commentcolor}{RGB}{133,153,0}
\definecolor{urlcolor}{rgb}{0.93,0.01,0.55}
\newcommand{\greencmark}{\textcolor{mygreen}{\cmark}}
\newcommand{\redxmark}{\textcolor{myred}{\xmark}}
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}

\usepackage{mathtools}
\newcommand{\defeq}{\coloneqq}
\newcommand{\grad}{\nabla}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Ea}[1]{\E\left[#1\right]}
\newcommand{\Eb}[2]{\E_{#1}\!\left[#2\right]}
\newcommand{\Vara}[1]{\Var\left[#1\right]}
\newcommand{\Varb}[2]{\Var_{#1}\left[#2\right]}
\newcommand{\kl}[2]{D_{\mathrm{KL}}\!\left(#1 ~ \| ~ #2\right)}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\bone}{\mathbf{1}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bxh}{\hat{\mathbf{x}}}
\newcommand{\balpha}{{\boldsymbol{\alpha}}}
\newcommand{\btheta}{{\boldsymbol{\theta}}}
\newcommand{\bphi}{{\boldsymbol{\phi}}}
\newcommand{\bepsilon}{{\boldsymbol{\epsilon}}}
\newcommand{\bmu}{{\boldsymbol{\mu}}}
\newcommand{\bnu}{{\boldsymbol{\nu}}}
\newcommand{\bSigma}{{\boldsymbol{\Sigma}}}
\newcommand{\bsigma}{{\boldsymbol{\sigma}}}
\newcommand{\bdelta}{{\boldsymbol{\delta}}}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{\svdiff{}: Compact Parameter Space for Diffusion Fine-Tuning}

% \author{Ligong Han\affmark[1,2]\thanks{Work done during an internship at Google Research.}\quad\quad Yinxiao Li\affmark[2]\quad\quad Han Zhang\affmark[2] \\Peyman Milanfar\affmark[2]\quad\quad
% Dimitris Metaxas\affmark[1]\quad\quad Feng Yang\affmark[2]\\
% {\affmark[1]Rutgers University\quad\quad\quad\affmark[2]Google Research }
% }
\author{Ligong Han\affmark[1,2]\thanks{Work done during an internship at Google Research.}\quad Yinxiao Li\affmark[2]\quad Han Zhang\affmark[2]\quad Peyman Milanfar\affmark[2]\quad Dimitris Metaxas\affmark[1]\quad Feng Yang\affmark[2]\\
{\affmark[1]Rutgers University\quad\quad\quad\affmark[2]Google Research }
}

\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
   Diffusion models have achieved remarkable success in text-to-image generation, enabling the creation of high-quality images from text prompts or other modalities. However, existing methods for customizing these models are limited by handling multiple personalized subjects and the risk of overfitting. Moreover, their large number of parameters is inefficient for model storage. In this paper, we propose a novel approach to address these limitations in existing text-to-image diffusion models for personalization. Our method involves fine-tuning the singular values of the weight matrices, leading to a compact and efficient parameter space that reduces the risk of overfitting and language-drifting. We also propose a Cut-Mix-Unmix data-augmentation technique to enhance the quality of multi-subject image generation and a simple text-based image editing framework. Our proposed \svdiff{} method has a significantly smaller model size (1.7MB for StableDiffusion) compared to existing methods (vanilla DreamBooth 3.66GB, Custom Diffusion 73MB), making it more practical for real-world applications.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\begin{figure}[t]
  \centering
  % \includegraphics[width=1\linewidth]{figures/hero_v8.pdf}
  \includegraphics[width=1\linewidth]{figures/hero_v9-3.pdf}
\caption{Applications of \svdiff{}. \textbf{Style-Mixing}: mix features from personalized subjects and create novel renderings; \textbf{Multi-Subject}: generate multiple subjects in the same scene; \textbf{Single-Image Editing}: text-based editing from a single image.}
  \label{fig:hero}
\end{figure}

Recent years have witnessed the rapid advancement of diffusion-based text-to-image generative models~\cite{ho2020denoising,saharia2022photorealistic,ho2022cascaded,ramesh2022hierarchical,rombach2022high}, which have enabled the generation of high-quality images through simple text prompts. These models are capable of generating a wide range of objects, styles, and scenes with remarkable realism and diversity. These models, with their exceptional results, have inspired researchers to investigate various ways to harness their power for image editing~\cite{kawar2022imagic,mokady2022null,zhang2022sine}.

In the pursuit of model personalization and customization, some recent works such as Textual-Inversion~\cite{gal2022image}, DreamBooth~\cite{ruiz2022dreambooth}, and Custom Diffusion~\cite{kumari2022multi} have further unleashed the potential of large-scale text-to-image diffusion models. By fine-tuning the parameters of the pre-trained models, these methods allow the diffusion models to be adapted to specific tasks or individual user preferences.

Despite their promising results, there are still some limitations associated with fine-tuning large-scale text-to-image diffusion models. One limitation is the large parameter space, which can lead to overfitting or drifting from the original generalization ability~\cite{ruiz2022dreambooth}. Another challenge is the difficulty in learning multiple personalized concepts especially when they are of similar categories~\cite{kumari2022multi}.

To alleviate overfitting, we draw inspiration from the efficient parameter space in the GAN literature~\cite{robb2020few} and propose a compact yet efficient parameter space, \emph{spectral shift}, for diffusion model by only fine-tuning the singular values of the weight matrices of the model. This approach is inspired by prior work in GAN adaptation showing that constraining the space of trainable parameters can lead to improved performance on target domain~\cite{rebuffi2017learning,mo2020freeze,noguchi2019image,sunsingular}. Comparing with another popular low-rank constraint~\cite{hu2021lora}, the spectral shifts utilize the full representation power of the weight matrix while being more compact (\eg 1.7MB for StableDiffusion~\cite{rombach2022high,stable_github,von-platen-etal-2022-diffusers}, full weight checkpoint consumes 3.66GB of storage). The compact parameter space allows us to combat overfitting and language-drifting issues, especially when prior-preservation loss~\cite{ruiz2022dreambooth} is not applicable. We demonstrate this use case by presenting a simple DreamBooth-based single-image editing framework.

To further enhance the ability of the model to learn multiple personalized concepts, we propose a simple Cut-Mix-Unmix data-augmentation technique. This technique, together with our proposed spectral shift parameter space, enables us to learn multiple personalized concepts even for semantically similar categories (\eg a ``cat'' and a ``dog'').

In summary, our main contributions are:
\begin{itemize}
    \item We present a compact ($\approx$2,200$\times$ fewer parameters compared with vanilla DreamBooth~\cite{ruiz2022dreambooth}, measured on StableDiffusion~\cite{rombach2022high}) yet efficient parameter space for diffusion model fine-tuning based on singular-value decomposition of weight kernels.
    \item We present a text-based single-image editing framework and demonstrate its use case with our proposed spectral shift parameter space.
    \item We present a generic Cut-Mix-Unmix method for data-augmentation to enhance the ability of the model to learn multiple personalized concepts.
\end{itemize}

This work opens up new avenues for the efficient and effective fine-tuning large-scale text-to-image diffusion models for personalization and customization. Our proposed method provides a promising starting point for further research in this direction.

\section{Related Work}
\noindent \textbf{Text-to-image diffusion models}
Diffusion models~\cite{sohl2015deep,song2019generative,ho2020denoising,song2020score,nichol2021improved,song2021denoising,gu2022vector,song2023consistency,chang2023muse} have proven to be highly effective in learning data distributions and have shown impressive results in image synthesis, leading to various applications~\cite{brack2022stable,iluz2023word,huang2023composer,liu2023cones,poole2022dreamfusion,wu2022tune,shin2023edit,jiang2023object,huang2023reversion,jiang2023avatarcraft}. Recent advancements have also explored transformer-based architectures~\cite{peebles2022scalable,bao2023one,tu2022maxvit}. In particular, the field of text-guided image synthesis has seen significant growth with the introduction of diffusion models, achieving state-of-the-art results in large-scale text-to-image synthesis tasks~\cite{nichol2021glide,ramesh2022hierarchical,saharia2022photorealistic,rombach2022high,balaji2022ediffi}. Our main experiments were conducted using StableDiffusion~\cite{rombach2022high}, which is a popular variant of latent diffusion models (LDMs)~\cite{rombach2022high} that operates on a latent space of a pre-trained autoencoder to reduce the dimensionality of the data samples, allowing the diffusion model to utilize the well-compressed semantic features and visual patterns learned by the encoder.

% \noindent \textbf{Few-shot adaptation of generative models}
% To customize and personalize the text-to-image diffusion models while taking advantage of the large-scale pretraining, recent works focused on fine-tuning the text embedding~\cite{gal2022image}, the full weights~\cite{ruiz2022dreambooth}, or the cross-attention layers~\cite{kumari2022multi} of text-to-image diffusion models using a few personalized images. Recent works have also explored training-free approaches for fast adaption~\cite{gal2023designing,wei2023elite}. In the GAN literature, FSGAN~\cite{robb2020few} proposed fine-tuning only the singular values of the weight matrices of the pre-trained GAN model. NaviGAN~\cite{cherepkov2021navigating} further advanced this idea with an unsupervised method for discovering semantic directions in this compact parameter space. Our method, \svdiff{}, introduces the same concept to few-shot fine-tuning of diffusion models. Another concurrent work, LoRA~\cite{hu2021lora}, explores low-rank adaptation for text-to-image diffusion fine-tuning. While LoRA constrains the difference of the weight matrices before and after fine-tuning to be low-rank, our proposed \svdiff{} optimizes all singular values of the weight matrix, resulting in an even smaller model checkpoint. Similar idea has also been explored in few-shot segmentation literature~\cite{sunsingular}.
\noindent \textbf{Fine-tuning generative models for personalization}
Recent works have focused on customizing and personalizing text-to-image diffusion models by fine-tuning the text embedding~\cite{gal2022image}, full weights~\cite{ruiz2022dreambooth}, or cross-attention layers~\cite{kumari2022multi} using a few personalized images. Other works have also investigated training-free approaches for fast adaptation~\cite{gal2023designing,wei2023elite,chen2023subject,jia2023taming}. The idea of fine-tuning only the singular values of weight matrices was introduced by FSGAN~\cite{robb2020few} in the GAN literature and further advanced by NaviGAN~\cite{cherepkov2021navigating} with an unsupervised method for discovering semantic directions in this compact parameter space. Our method, \svdiff{}, introduces this concept to the fine-tuning of diffusion models and is designed for few-shot adaptation. A similar approach, LoRA~\cite{hu2021lora}, explores low-rank adaptation for text-to-image diffusion fine-tuning, while our proposed \svdiff{} optimizes all singular values of the weight matrix, leading to an even smaller model checkpoint. This idea has also been explored in the few-shot segmentation literature~\cite{sunsingular}.

\noindent \textbf{Diffusion-based image editing}
Diffusion models have also shown great potential for semantic editing~\cite{liu2022compositional,avrahami2022blended,avrahami2022blendedlatent,kawar2022imagic,zhang2022sine,mokady2022null,orgad2023editing,wallace2022edict,tumanyan2022plug,su2022dual,parmar2023zero,bansal2023universal,wallace2023end}. These methods typically focus on inversion~\cite{song2021denoising} and reconstruction by optimizing the null-text embedding or overfitting to the given image~\cite{zhang2022sine}. Our proposed method, \svdiff{}, presents a simple DreamBooth-based~\cite{ruiz2022dreambooth} single-image editing framework that demonstrates the potential of \svdiff{} in single image editing and mitigating overfitting. %Our method also relates to spectrum-based image editing~\cite{talebi2013global,talebi2014nonlocal}.

\section{Method}
\subsection{Preliminary}
\noindent \textbf{Diffusion models}
StableDiffusion~\cite{rombach2022high}, the model we experiment with, is a variant of latent diffusion models (LDMs)~\cite{rombach2022high}. LDMs transform the input images $\bx$ into a latent code $\bz$ through an encoder $\mathcal{E}$, where $\bz = \mathcal{E}(\bx)$, and perform the denoising process in the latent space $\mathcal{Z}$. Briefly, a latent diffusion model $\hat{\bepsilon}_\theta$ is trained with a denoising objective:
\begin{equation}
\Eb{\bz,\bc,\bepsilon,t}{ \|\hat{\bepsilon}_\theta(\bz_t | \bc, t) - \bepsilon \|^2_2},
\label{eq:ldm}
\end{equation}
where $(\bz, \bc)$ are data-conditioning pairs (image latents and text embeddings), $\bepsilon \sim \mathcal{N}(\bzero, \bI)$, $t \sim \text{Uniform}(1, T)$, and $\theta$ represents the model parameters. We omit $t$ in the following for brevity.

\noindent \textbf{Few-shot adaptation in compact parameter space of GANs}
The method of FSGAN~\cite{robb2020few} is based on the Singular Value Decomposition (SVD) technique and proposes an effective way to adapt GANs in few-shot settings. It takes advantage of the SVD to learn a compact update for domain adaptation in the parameter space of a GAN. Specifically, FSGAN reshapes the convolution kernels of a GAN, which are in the form of $W_{tensor}\in \mathbb{R}^{c_{out}\times c_{in}\times h \times w}$, into 2-D matrices $W$, which are in the form of $W=\texttt{reshape}(W_{tensor}) \in \mathbb{R}^{c_{out}\times (c_{in}\times h \times w)}$. FSGAN then performs SVD on these reshaped weight matrices of both the generator and discriminator of a pretrained GAN and adapts their singular values to a new domain using a standard GAN training objective.
\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{figures/pipeline_svd_v2.pdf}
  \caption{Performing singular value decomposition (SVD) on weight matrices. In an intermediate layer of the model, \textbf{(a)} the convolutional weights $W_{tensor}$ \textbf{(b)} serve as an associative memory~\cite{bau2020rewriting}. \textbf{(c)} SVD is performed on the reshaped 2-D matrix $W$.}
  \label{fig:pipeline_svd}
\end{figure}
\subsection{Compact Parameter Space for Diffusion Fine-tuning}\label{sec:method_svd}
\noindent \textbf{Spectral shifts}
The core idea of our method is to introduce the concept of spectral shifts from FSGAN~\cite{robb2020few} to the parameter space of diffusion models. To do so, we first perform Singular Value Decomposition (SVD) on the weight matrices of the pre-trained diffusion model. The weight matrix (obtained from the same reshaping as FSGAN~\cite{robb2020few} mentioned above) is denoted as $W$ and its SVD is $W=U\Sigma V^\top$, where $\Sigma=\text{diag}(\bsigma)$ and $\bsigma=[\sigma_1, \sigma_2, ...]$ are the singular values in descending order. Note that the SVD is a one-time computation and can be cached. This procedure is illustrated in \cref{fig:pipeline_svd}. Such reshaping of the convolution kernels is inspired by viewing them as linear associative memories~\cite{bau2020rewriting}. The patch-level convolution can be expressed as a matrix multiplication, $\mathbf{f}_{out}=W \mathbf{f}_{in}$, where $\mathbf{f}_{in}\in \mathbb{R}^{(c_{in}\times h \times w)\times 1}$ is flattened patch feature and $\mathbf{f}_{out}\in \mathbb{R}^{c_{out}}$ is the output pre-activation feature corresponding to the given patch. Intuitively, the optimization of spectral shifts leverages the fact that the singular vectors correspond to the close-form solutions of the eigenvalue problem~\cite{shen2021closed}: $\max_\bn{\|W \bn\|^2_2}$ s.t. $\|\bn\|=1$.

Instead of fine-tuning the full weight matrix, we only update the weight matrix by optimizing the \emph{spectral shift}~\cite{cherepkov2021navigating}, $\bdelta$, which is defined as the difference between the singular values of the updated weight matrix and the original weight matrix. The updated weight matrix can be re-assembled by
\begin{align}
    W_{\bdelta} = U \Sigma_{\bdelta} V^\top ~~ \text{with} ~~ \Sigma_{\bdelta}=\text{diag}(\text{ReLU}(\bsigma+\bdelta)).\label{eq:svd}
\end{align}

\noindent \textbf{Training loss}
The fine-tuning is performed using the same loss function that was used for training the diffusion model, with a weighted prior-preservation loss~\cite{ruiz2022dreambooth,bau2020rewriting}:
\begin{align}
    \mathcal{L}(\bdelta) &= \Eb{\bz^*,\bc^*,\bepsilon,t}{\|\hat{\bepsilon}_{\theta_\bdelta}(\bz_t^* | \bc^*) - \bepsilon\|^2_2}+\lambda\mathcal{L}_{pr}(\bdelta) ~\text{with}~ \nonumber\\
    \mathcal{L}_{pr}(\bdelta) &= \Eb{\bz^{pr},\bc^{pr},\bepsilon,t}{\|\hat{\bepsilon}_{\theta_\bdelta}(\bz_t^{pr} | \bc^{pr}) - \bepsilon\|^2_2}
\end{align}
\noindent where $(\bz^*, \bc^*)$ represents the target data-conditioning pairs that the model is being adapted to, and $(\bz^{pr}, \bc^{pr})$ represents the prior data-conditioning pairs generated by the pretrained model.
This loss function extends the one proposed by Model Rewriting~\cite{bau2020rewriting} for GANs to the context of diffusion models, with the prior-preservation loss serving as the smoothing term.
In the case of single image editing, where the prior-preservation loss cannot be utilized, we set $\lambda=0$.

\noindent \textbf{Combining spectral shifts}
Moreover, the individually trained spectral shifts can be combined into a new model to create novel renderings. This can enable applications including interpolation, style mixing (\cref{fig:style}), or multi-subject generation (\cref{fig:weight_sum}). 
Here we consider two common strategies, addition and interpolation. To add $\bdelta_1$ and $\bdelta_2$ into $\bdelta'$, we have
\begin{align}
\Sigma_{\bdelta'}=\text{diag}(\text{ReLU}(\bsigma+\bdelta_1+\bdelta_2)).\label{eq:svd_sum}
\end{align}
For interpolation between two models with $0\leq \alpha\leq 1$, we use:
\begin{align}
\Sigma_{\bdelta'}=\text{diag}(\text{ReLU}(\bsigma+\alpha\bdelta_1+(1-\alpha)\bdelta_2)).\label{eq:svd_interp}
\end{align}
This allows for smooth transitions between models and the ability to interpolate between different image styles.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{figures/pipeline_cutmix_v3.pdf}
  \caption{Cut-Mix-Unmix data-augmentation for \textbf{multi-subject generation}. The figure shows the process of Cut-Mix-Unmix data augmentation for training a model to handle multiple concepts. The method involves \textbf{(a)} manually constructing image-prompt pairs where the image is created using a CutMix-like data augmentation~\cite{yun2019cutmix} and the corresponding prompt is written as, for example, ``photo of a [$V_2$] sculpture and a [$V_1$] dog''. The prior preservation image-prompt pairs are created in a similar manner. The objective is to train the model to separate different concepts by presenting it with explicit mixed samples. \textbf{(b)} To perform unmix regularization, we use MSE on non-corresponding regions of the cross-attention maps to enforce separation between the two subjects. The goal is to encourage that the dog's special token should not attend to the panda and vice versa. \textbf{(c)} During inference, a different prompt, such as ``photo of a [$V_1$] dog sitting besides a [$V_2$] sculpture''.}
  \label{fig:pipeline_cutmix}
\end{figure*}
\subsection{Cut-Mix-Unmix for Multi-Subject Generation}
We discovered that when training the StableDiffusion~\cite{rombach2022high} model with multiple concepts simultaneously (randomly choosing one concept at each data sampling iteration), the model tends to mix their styles when rendering them in one image for difficult compositions or subjects of similar categories~\cite{kumari2022multi} (as shown in \cref{fig:cutmix}). To explicitly guide the model not to mix personalized styles, we propose a simple technique called Cut-Mix-Unmix. By constructing and presenting the model with ``correctly'' cut-and-mixed image samples (as shown in \cref{fig:pipeline_cutmix}), we instruct the model to \emph{unmix} styles. In this method, we manually create CutMix-like~\cite{yun2019cutmix} image samples and corresponding prompts (\eg ``photo of a [$V_1$] dog on the left and a [$V_2$] sculpture on the right'' or ``photo of a [$V_2$] sculpture and a [$V_1$] dog'' as illustrated in \cref{fig:pipeline_cutmix}). The prior loss samples are generated in a similar manner.
During training, Cut-Mix-Unmix data augmentation is applied with a pre-defined probability (usually set to 0.6). This probability is not set to 1, as doing so would make it challenging for the model to differentiate between subjects.
During inference, we use a different prompt from the one used during training, such as ``a [$V_1$] dog sitting beside a [$V_2$] sculpture''. However, if the model overfits to the Cut-Mix-Unmix samples, it may generate samples with stitching artifacts even with a different prompt. We found that using negative prompts can sometimes alleviate these artifacts, as detailed in appendix.

We further present an extension to our fine-tuning approach by incorporating an ``unmix'' regularization on the cross-attention maps. This is motivated by our observation that in fine-tuned models, the dog's special token (``sks'') attends largely to the panda, as depicted in \cref{fig:analysis_attn}. To enforce separation between the two subjects, we use MSE on the non-corresponding regions of the cross-attention maps. This loss encourages the dog's special token to focus solely on the dog and vice versa for the panda. The results of this extension show a significant reduction in stitching artifact.
\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{figures/pipeline_sine_v3.pdf}
  \caption{Pipeline for \textbf{single image editing} with a text-to-image diffusion model. \textbf{(a)} The model is fine-tuned with a single image-prompt pair, where the prompt describes the input image without a special token. \textbf{(b)} During inference, desired edits are made by modifying the prompt. For edits with no significant structural changes, the use of DDIM inversion~\cite{song2021denoising} has been shown to improve the editing quality.}
  \label{fig:pipeline_sine}
\end{figure}

% \subsection{Compact Parameter Space for Single-Image Editing}
\subsection{Single-Image Editing}
In this section, we present a framework for single image editing, called CoSINE (\underline{Co}mpact parameter space for \underline{SIN}gle image \underline{E}diting), by fine-tuning a diffusion model with an image-prompt pair.  The procedure is outlined in \cref{fig:pipeline_sine}. The desired edits can be obtained at inference time by modifying the prompt. For example, we fine-tune the model with the input image and text description \textit{``photo of a crown with a blue diamond and a golden eagle on it''}, and at inference time if we want to remove the eagle, we simply sample from the fine-tuned model with text \textit{``photo of a crown with a blue diamond on it''}. To mitigate overfitting during fine-tuning, CoSINE uses the spectral shift parameter space instead of full weights, reducing the risk of overfitting and language drifting. The trade-off between faithful reconstruction and editability, as discussed in~\cite{meng2021sdedit}, is acknowledged, and the purpose of CoSINE is to allow more flexible edits rather than exact reconstructions. 

% For edits that do not require large structural changes (like repose, ``standing'' $\rightarrow$ ``lying down'' or ``zoom in''), results can be improved with DDIM inversion~\cite{song2021denoising}. Before sampling, we run DDIM inversion with classifier-free guidance~\cite{ho2021classifier} with scale 1 conditioned on the target text prompt and encode the input image to a latent noise map $\bz_T$. Inference starts from the encoded noise map, but large structural changes may still require more noise injection through setting $\eta>0$ or perturbing $\bz_T$ through interpolation between $\bz_T$ and a random noise vector $\bepsilon$. As expected, large structural changes may still require more noise being injected in the denoising process. Here we consider two types of noise injection: 1) setting $\eta>0$ (as defined in DDIM~\cite{song2021denoising}, and 2) perturbing $\bz_T$. For the latter, we interpolate between $\bz_T$ and a random $\bepsilon\sim\mathcal{N}(0,\bI)$ with slerp~\cite{shoemake1985animating}, $\tilde{\bz}_T=\text{slerp}(\gamma,\bz_T,\bepsilon)$ ($0<\gamma<1$ and $\gamma=0$ recovers $\bz_T$).
% We additionally introduce low-rank~\cite{hu2021lora} regularization into CoSINE and give comparison and discussion in appendix. For more results and analysis, please see the experimental section.
For edits that do not require large structural changes (like repose, ``standing'' $\rightarrow$ ``lying down'' or ``zoom in''), results can be improved with DDIM inversion~\cite{song2021denoising}. Before sampling, we run DDIM inversion with classifier-free guidance~\cite{ho2021classifier} scale 1 conditioned on the target text prompt $\bc$ and encode the input image $\bz^*$ to a latent noise map,
\begin{equation}
    \bz_T=\text{DDIMInvert}(\bz^*,\bc;\theta'),\label{eq:ddim_inv}
\end{equation}
\noindent ($\theta'$ denotes the fine-tuned model parameters) from which the inference pipeline starts. As expected, large structural changes may still require more noise being injected in the denoising process. Here we consider two types of noise injection: i) setting $\eta>0$ (as defined in DDIM~\cite{song2021denoising}, and ii) perturbing $\bz_T$. For the latter, we interpolate between $\bz_T$ and a random noise $\bepsilon\sim\mathcal{N}(0,\bI)$ with spherical linear interpolation~\cite{shoemake1985animating,song2021denoising},
\begin{equation}
    \small
    \tilde{\bz}_T = \text{slerp}(\alpha,\bz_T,\bepsilon) = \frac{\sin((1-\alpha)\phi)}{\sin(\phi)}\bz_T + \frac{\sin(\alpha \phi)}{\sin(\phi)}\bepsilon,\label{eq:slerp}
\end{equation}

\noindent with $\phi=\arccos{(\cos(\bz_T, \bepsilon))}$.
% We additionally introduce low-rank~\cite{hu2021lora} regularization into CoSINE and give comparison and discussion in appendix.
For more results and analysis, please see the experimental section.

Other approaches, such as Imagic~\cite{kawar2022imagic}, have been proposed to address overfitting and language drifting in fine-tuning-based single-image editing. Imagic fine-tunes the diffusion model on the input image and target text description, and then interpolates between the optimized and target text embedding to avoid overfitting. However, Imagic requires fine-tuning on each target text prompt at test time. We leave possible ways to combine Imagic and CoSINE in future work.

\section{Experiment}
The experiments evaluate \svdiff{} on various tasks such as single-subject generation, multi-subject generation, single image editing, and ablations. The DDIM~\cite{song2021denoising} sampler with $\eta=0$ is used for all generated samples, unless specified otherwise.
\subsection{Single-Subject Generation}\label{sec:exp_single}
In this section, we present the results of our proposed \svdiff{} for customized single-subject generation proposed in DreamBooth~\cite{ruiz2022dreambooth}, which involves fine-tuning the pretrained text-to-image diffusion model on a single object or concept (using 3-5 images). The original DreamBooth was implemented on Imagen~\cite{saharia2022photorealistic} and we conduct our experiments based on its StableDiffusion~\cite{rombach2022high} implementation~\cite{drembooth_github,von-platen-etal-2022-diffusers}. We provide visual comparisons of 5 examples in \cref{fig:single}. All baselines were trained for 500 or 1000 steps with batch size 1 (except for Custom Diffusion~\cite{kumari2022multi}, which used a default batch size of 2), and the best model was selected for fair comparison. As \cref{fig:single} shows, \svdiff{} produces similar results to DreamBooth (which fine-tunes the full model weights) despite having a much smaller parameter space. 
% However, as \cref{fig:single}-h for the ``No-Face'' sculpture example shows, full model fine-tuning sometimes still generates samples that don't resemble the target object (lower right corner). 
Custom Diffusion, on the other hand, tends to underfit the training images as seen in rows 2, 3, and 5 of \cref{fig:single}.
We assess the text and image alignment in \cref{fig:score_single}. The results show that the performance of \svdiff{} is similar to that of DreamBooth, while Custom Diffusion tends to underfit as seen from its position in the upper left corner of the plot.

\begin{figure*}[t]
  \centering
  \includegraphics[width=1\linewidth]{figures/single_v3.pdf}
  \caption{Results for \textbf{single subject generation}. DreamBooth~\cite{ruiz2022dreambooth} and Custom Diffusion~\cite{kumari2022multi} are implemented in StableDiffusion with Diffusers library~\cite{von-platen-etal-2022-diffusers}. Each subfigure consists 3 samples: a large one on the left and 2 small one on the right. The text prompt under input images are used for training and the text prompt under sample images are used for inference. We observe that \svdiff{} performs similarly as DreamBooth (full-weight fine-tuning), and preserves subject identities better than Custom Diffusion for row 2, 3, 5.}
  \label{fig:single}
\end{figure*}

\subsection{Multi-Subject Generation}\label{sec:exp_multi}
In this section, we present the multi-subject generation results to illustrate the advantage of our proposed ``Cut-Mix-Unmix'' data augmentation technique. When enabled, we perform Cut-Mix-Unmix data-augmentation with probability of 0.6 in each data sampling iteration and two subjects are randomly selected without replacement.
A comparison between using ``Cut-Mix-Unmix'' (marked as ``w/ Cut-Mix-Unmix'') and not using it (marked as ``w/o Cut-Mix-Unmix'', performing augmentation with probability 0) are shown in \cref{fig:cutmix}. Each row of images are generated using the same text prompt displayed below the images.
Note that the Cut-Mix-Unmix data augmentation technique is generic and can be applied to fine-tuning full weights as well. 

To assess the visual quality of images generated using the ``Cut-Mix-Unmix'' method with either SVD or full weights, we conducted a user study using Amazon MTurk~\cite{amt} with 400 generated image pairs . 
The participants were presented with an image pair generated using the same random seed, and were asked to identify the better image by answering the question, ``Which image contains both objects from the two input images with a consistent background?''
Each image pair was evaluated by 10 different raters, and the aggregated results showed that SVD was favored over full weights 60.9\% of the time, with a standard deviation of 6.9\%.
More details will be provided in the appendix.

Additionally, we also conducted experiments that involve training on three concepts simultaneously. During training, we still construct Cut-Mix samples with probability 0.6 by randomly sample two subjects. Interestingly, we observe that for concepts that are already semantically well-separated, e.g. ``dog/building'' or ``sculpture/building'', the model can successfully generate desired results even without using Cut-Mix-Unmix. However, it fails to disentangle semantically more similar concepts, \eg ``dog/panda'' as shown in \cref{fig:cutmix}-g.

\begin{figure*}
  \centering
  \includegraphics[width=0.95\linewidth]{figures/cutmix_v4.pdf}
  \caption{Results for \textbf{multi-subject generation}. (a-d) show the results of fine-tuning on two subjects and (e-g) show the results of fine-tuning on three subjects. Both full weight (``Full'') fine-tuning and \svdiff{} (``SVD'') can benefit from the Cut-Mix-Unmix data-augmentation. Without Cut-Mix-Unmix, the model struggles to disentangle subjects of similar categories, as demonstrated in the last two columns of (a,b,c,d,g).}
  \label{fig:cutmix}
\end{figure*}

\subsection{Single Image Editing}
\begin{figure*}[t]
  \centering
  \includegraphics[width=1\linewidth]{figures/cosine_v9.pdf}
  \caption{Results for \textbf{single image editing}. \svdiff{} (``Ours'') enables successful image edits despite slight misalignment with the original image. \svdiff{} performs desired modifications when full model fine-tuning (``Full'') fails, such as removing an object (2nd edit in (a)), adjusting pose (2nd edit in (c)), or zooming in (3rd edit in (d)). The backgrounds in some cases may be affected, however, the subject of the image remains well-preserved. For ours, we use DDIM inversion~\cite{song2021denoising} for all edits in (a,c,e) and the first edit in (d).}
  \label{fig:sine}
\end{figure*}
In this section, we present results for the single image editing application. As depicted in \cref{fig:sine}, each row presents three edits with fine-tuning of both spectral shifts (marked as ``Ours'') and full weights (marked as ``Full''). The text prompts for the corresponding edited images are given below the images. The aim of this experiment is to demonstrate that regularizing the parameter space with spectral shifts effectively mitigates the language drift issue, as defined in~\cite{ruiz2022dreambooth} (the model overfits to a single image and loses its ability to generalize and perform desired edits).

As previously discussed, when DDIM inversion is not employed, fine-tuning with spectral shifts can lead to sometimes over-creative results. We show some examples of its behavior (in comparison with LoRA~\cite{hu2021lora}) in \cref{fig:lora_brief}. More examples of with and without DDIM inversion~\cite{song2021denoising} are given in the appendix (\cref{fig:sine_inv}). Our results show that DDIM inversion improves the editing quality and alignment with the input image for non-structural edits when using our spectral shift parameter space, but may worsen the results for full weight fine-tuning. For example, in \cref{fig:sine}, we use DDIM inversion for the edits in (a,c,e) and the first edit in (d). The second edit in (d) presents an interesting example where our method can actually make the statue hold an apple with its hand. Additionally, our fine-tuning approach still produces the desired edit of an empty room even with DDIM inversion, as seen in the third edit of \cref{fig:sine}-a.
Overall, we see that \svdiff{} can still perform desired edits when full model fine-tuning exhibits language drift, \ie it fails to remove the picture in the second edit of (a), change the pose of the dog in the second edit of (c), and zoom-in view in (d).

\subsection{Analysis and Ablation}\label{sec:analysis}
Due to space limitations, we present \emph{weight combination}, \emph{interpolation} and \emph{style mixing} analysis in this section and provide further analysis including \emph{rank}, \emph{scaling}, and \emph{correlation} in the appendix.
\begin{figure*}[t!]
  \centering
\includegraphics[width=1\linewidth]{figures/weight_sum_v2.pdf}
  \caption{Effects of combining spectral shifts ($\Sigma_{\bdelta'}=\text{diag}(\text{ReLU}(\bsigma+\bdelta_1+\bdelta_2))$) and weight deltas ($W'=W+\Delta W_1+\Delta W_2$) in one model. The combined model retains individual subject features but may mix styles for similar subjects. The results also suggests that the task arithmetic property~\cite{ilharco2022editing} of language models also holds in StableDiffusion.}
  \label{fig:weight_sum}
\end{figure*}
\begin{figure}[t]
  \centering
\includegraphics[width=1\linewidth]{figures/style_v2.pdf}
  \caption{Results for personalized style transfer and mixing. Changing coarse class word: (d) and (i); Appending ``in style of'': (e) and (j); Combined spectral shifts: (a,b,f,g).}
  \label{fig:style}
\end{figure}
\begin{figure}[t]
  \centering
\includegraphics[width=1\linewidth]{figures/score_and_corr_single_v5.pdf}
  \caption{(a) Correlation of individually learned spectral shifts for different subjects. The cosine similarities between the spectral shifts of two subjects are averaged across all layers and plotted. The diagonal shows average similarities between two runs with different learning rates. (b) Text- and image-alignment for single-subject generation. The generated image is denoted as $\tilde{\bx}$. The text-alignment is measured by the CLIP score~\cite{radford2021learning,chen2023revisiting} $\cos(\tilde{\bx},\bc)$, and the image-alignment is defined as $1-\mathcal{L}_\text{LPIPS}(\tilde{\bx},\bx^*)$~\cite{zhang2018unreasonable}.}
  \label{fig:score_single}
\end{figure}

\noindent \textbf{Weight combination}
We analyze the effects of weight combination by \cref{eq:svd_sum}. \cref{fig:weight_sum} shows a comparison between combining only spectral shifts (marked in ``SVD'') and combining the full weights (marked in ``Full''). The combined model in both cases retains unique features for individual subjects, but may blend their styles for similar concepts (as seen in (e)). For dissimilar concepts (such as the [$V_2$] sculpture and [$V_3$] building in (j)), the models can still produce separate representations of each subject. Interestingly, combining full weight deltas can sometimes result in better preservation of individual concepts, as seen in the clear building feature in (j). We posit that this is due to the fact that \svdiff{} limits update directions to the eigenvectors, which are identical for different subjects. As a result, summing individually trained spectral shifts tends to create more ``interference'' than summing full weight deltas.

\noindent \textbf{Style transfer and mixing}
We demonstrate the capability of style-mixing~\cite{voynov2023p+} using our proposed method. We show that by using a single fine-tuned model, the personalized style can be transferred to a different class by changing the class word during inference, or by adding a prompt such as \textit{``in style of''}. We also show that by summing two sets of spectral shifts (as discussed above), their styles can be mixed. The results show different outcomes of different style-mixing strategies, with changes to both the class and personalized style.

\noindent \textbf{Interpolation}
\cref{fig:interp} shows the results of weight interpolation for both spectral shifts and full weights. The models are marked as ``SVD'' and ``Full'', respectively. The first two rows of the figure demonstrate interpolating between two different classes, such as ``dog'' and ``sculpture'', using the same abstract class word ``thing'' for training. Each column shows the sample from $\alpha$-interpolated models. For spectral shifts (``SVD''), we use \cref{eq:svd_interp} and for full weights, we use $W'=W + \alpha \Delta W_1 + (1-\alpha)\Delta W_2=\alpha W_1 + (1-\alpha) W_2$. The images in each row are generated using the same random seed with the deterministic DDIM sampler~\cite{song2021denoising} ($\eta=0$). As seen from the results, both spectral shift and full weight interpolation are capable of generating intermediate concepts between the two original classes.
\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{figures/interp2.pdf}
  \caption{Effects of interpolating spectral shifts ($\Sigma_{\bdelta'}=\text{diag}(\text{ReLU}(\bsigma+\alpha\bdelta_1+(1-\alpha)\bdelta_2))$) or weight deltas ($W'=W + \alpha \Delta W_1 + (1-\alpha)\Delta W_2=\alpha W_1 + (1-\alpha) W_2$).}
  \label{fig:interp}
\end{figure}
\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{figures/lora_brief_v3.pdf}
  \caption{Comparison of \svdiff{} (``SVD''), LoRA~\cite{hu2021lora} (``LoRA'', rank-1), and full weight fine-tuning (``Full''). The results show that LoRA tends to underfit the input images, as shown in rows (b,c), it also fails to remove the picture in example (a).}
  \label{fig:lora_brief}
\end{figure}

\subsection{Comparison with LoRA}\label{sec:lora}
In our comparison of \svdiff{} and LoRA~\cite{hu2021lora} for single image editing, we find that while LoRA tends to underfit, \svdiff{} provides a balanced trade-off between faithfulness and realism. Additionally, \svdiff{} results in a significantly smaller delta checkpoint size, being 1/2 to 1/3 that of LoRA. However, in cases where the model requires extensive fine-tuning or learning of new concepts, LoRA's flexibility to adjust its capability by changing the rank may be beneficial. Further research is needed to explore the potential benefits of combining these approaches. A brief comparison is shown in \cref{fig:lora_brief} and a complete comparison can be found in the appendix.

It is noteworthy that, with rank one, the storage and update requirements for the $W$ matrix of shape $M\times N$ in \svdiff{} are $\min(M,N)$ floats, compared to $(M+N)$ floats for LoRA. This may be useful for amortizing or developing training-free approaches for DreamBooth~\cite{ruiz2022dreambooth}. Additionally, exploring functional forms~\cite{talebi2013global,talebi2014nonlocal} of spectral shifts is an interesting avenue for future research.

\section{Conclusion and Limitation}
In conclusion, we have proposed a compact parameter space, spectral shift, for diffusion model fine-tuning. The results of our experiments show that fine-tuning in this parameter space achieves similar or even better results compared to full weight fine-tuning in both single- and multi-subject generation. Our proposed Cut-Mix-Unmix data-augmentation technique also improves the quality of multi-subject generation, making it possible to handle cases where subjects are of similar categories. Additionally, spectral shift serves as a regularization method, enabling new use cases like single image editing.

% \noindent \textbf{Limitations} However, there are limitations to our method, such as the performance degradation of Cut-Mix-Unmix when more subjects are added and the possibility of a poorly-preserved background in single image editing. Nevertheless, we believe that our method offers a promising solution for diffusion model fine-tuning and we look forward to exploring its potential in future work, such as investigating the combination of spectral shifts with LoRA, as well as fast adaptation of personalized concepts or even training-free approaches.
\noindent \textbf{Limitations} Our method has certain limitations, including the decrease in performance of Cut-Mix-Unmix as more subjects are added and the possibility of an inadequately-preserved background in single image editing. Despite these limitations, we see great potential in our approach for fine-tuning diffusion models and look forward to exploring its capabilities further in future research, such as combining spectral shifts with LoRA or developing training-free approaches for fast personalizing concepts.

% \clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\input{supp_arxiv}

\end{document}
