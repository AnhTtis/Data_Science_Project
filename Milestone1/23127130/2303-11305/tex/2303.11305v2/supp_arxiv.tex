\appendix

\section*{Appendix}
% \addcontentsline{toc}{section}{Appendix}
{\hypersetup{linkcolor=urlcolor}
\startcontents[sections]
\printcontents[sections]{l}{1}{\setcounter{tocdepth}{2}}
}
% ============================================================== %
\section{Implementation Details}
\noindent \textbf{Implementation}
The original DreamBooth was implemented on Imagen~\cite{saharia2022photorealistic} and we conduct our experiments based on its StableDiffusion~\cite{rombach2022high} implementation~\cite{drembooth_github}. DreamBooth~\cite{ruiz2022dreambooth} and Custom Diffusion~\cite{kumari2022multi} are implemented in StableDiffusion with Diffusers library~\cite{von-platen-etal-2022-diffusers}. For LoRA~\cite{hu2021lora}, we use our own implementation for fair comparison, in which we also fine-tune the 1-D weight kernels, and use rank-1 for 2-D and 4-D weight kernels. This results in a slightly larger delta checkpoint (of size 5.62MB) than the official LoRA implementation~\cite{lora_stable}.
\begin{figure*}[t]
  \begin{center}
    \hfill
    \subfloat[Single Subject]{\includegraphics[width=0.48\linewidth]{figures/correlations_single_fig.pdf}}\hfill
    \subfloat[Single Image]{\includegraphics[width=0.48\linewidth]{figures/correlations_sine_fig.pdf}}\hfill
    \caption{Correlation of individually learned spectral shifts for different subjects/images. The cosine similarities between the spectral shifts of two subjects are averaged across all layers and plotted. The diagonal shows average similarities between two runs with different learning rates. High similarities are observed between conceptually similar subjects.}
    \label{fig:correlation}
  \end{center}
\end{figure*}
\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/cutmix_ane.pdf}
  \caption{Results for Cut-Mix-Unmix with Attend-and-Excite~\cite{chefer2023attend}. Even without Cut-Mix-Unmix, Attend-and-Excite successfully separate the dog and the cat by design, despite the color of the cat is slightly leaked to the dog. Cut-Mix-Unmix helps better disentangle respective visual features of the dog and the cat.}
  \label{fig:cm_ane}
\end{figure*}

\noindent \textbf{Learning rate} 
Our experiments show that the learning rate for these spectral shifts needs to be much larger (1,000 times, \eg $10^{-3}$) than the learning rate used for fine-tuning the full weights. For 1-D weights that are not decomposed, we use either the original learning rate of $10^{-6}$ to prevent overfitting or a larger learning rate to allow for a more rapid adaptation of the model, depending on the desired trade-off between stability and speed of adaptation.
\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{figures/neg_prompt_v2.pdf}
  \caption{Ablation of negative prompting. Using negative prompt helps to remove stitching artifact for both ``SVD'' and ``Full''.}
  \label{fig:neg_prompt}
\end{figure}
% ============================================================== %
\section{Single Image Editing}
\subsection{DDIM inversion}
\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{figures/cosine_inv_v6.pdf}
  \caption{Results for single image editing with DDIM inversion~\cite{song2021denoising}. If inversion is not used, DDIM sampler with $\eta=0$ is applied. If inversion is employed, we use DDIM sampler with $\eta=0.5$ and $\alpha=0$ ($\alpha$ is for slerp defined in \cref{eq:slerp}), except for edits in (d,f,h) where $\eta=0.9$ and $\alpha=0.9$. Results show that DDIM inversion improves editing quality and alignment with input images for non-structural edits when using our spectral shift parameter space. As shown, DDIM inversion can have adverse effects on results for ``Full'' and ``LoRA'', \eg (b) making the room empty, (i) changing the color to purple.}
  \label{fig:sine_inv}
\end{figure*}
We show comparisons of with and without DDIM inversion~\cite{song2021denoising} using ours (``SVD''), LoRA~\cite{hu2021lora} (``LoRA''), and DreamBooth (``Full'') on single-image editing in \cref{fig:sine_inv}. If inversion is not used, DDIM sampler with $\eta=0$ is applied. If inversion is employed, we use DDIM sampler with $\eta=0.5$ and $\alpha=0$, except for edits in \cref{fig:sine_inv}-(d,f,h) where $\eta=0.9$ and $\alpha=0.9$. Interestingly, for the chair example (row 2) we need to inject large amount of noise to get desired edits. For other edits (a,b,e,g,i,j) DDIM inversion improves editing quality and alignment with input images for ``SVD'', but makes results worse for ``Full'' in edits (b,g,i) and for ``LoRA'' in edits (b,i). We can conclude that DDIM inversion improves editing quality and alignment with input images for non-structural edits when using our spectral shift parameter space. We also observe that LoRA in general tends to underfit the input image, as shown in (c,d,e,i) (without inversion).

\subsection{Comparison with other methods}
Furthermore, we compare our method with the popular Instruct-Pix2Pix~\cite{brooks2022instructpix2pix} in \cref{fig:sine_ip2p} (marked as ``ip2p''). The comparison is \emph{not} entirely fair as Instruct-Pix2Pix does not require fine-tuning on individual images. Nevertheless, it is worth investigating fast personalized adaptation and avoiding per-image fine-tuning in future work.
\begin{figure*}[t]
  \centering
  \includegraphics[width=1\linewidth]{figures/cosine_ip2p_v2.pdf}
  \caption{Comparison of our method and Instruct-Pix2Pix~\cite{brooks2022instructpix2pix} (marked as ``ip2p'') on single-image editing. The instructions are displayed in bold and italicized purple text. Results show that Instruct-Pix2Pix tends to alter the overall color scheme and struggles with significant or structural edits, as seen in (a) emptying the room and (d) zoom-in view.}
  \label{fig:sine_ip2p}
\end{figure*}
\begin{figure*}[t]
  \centering
  \includegraphics[width=1\linewidth]{figures/analysis_attn.pdf}
  \caption{Analysis of cross-attention maps of the fine-tuned model without using unmix regularization. Visualization is obtained by Prompt-to-Prompt~\cite{hertz2022prompt}. As shown, the dog's special token (``sks'') attends largely to the panda.}
  \label{fig:analysis_attn}
\end{figure*}

% ============================================================== %
\section{Multi-Subject Generation}
\subsection{User study}
In \cref{tab:multi_user}, we present the results of human evaluation comparing our method (``SVD'') and the full weight fine-tuning method (``Full''). For each of the four subject combinations, 1000 ratings were collected. Participants were shown two generated images side-by-side and were asked to choose their preferred image or indicate that it was ``hard to decide'' (4.1\%, 1.2\%, 2.1\%, and 2.2\% respectively). %The results show that ``SVD'' is favored over ``Full'' with a preference of 60.9\%.
\begin{table}[h]
    \centering
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{cc}
    \toprule
    Subject Combinations & Human Preference (SVD \vs Full) \\ \hline
    Teddy~$+$~Tortoise & \textbf{53.2}\% : 46.8\% \\
    Dog~$+$~Cat        & \textbf{62.9}\% : 37.1\% \\
    Dog~$+$~No-Face    & \textbf{65.0}\% : 35.0\% \\
    Dog~$+$~Panda      & \textbf{62.0}\% : 38.0\% \\
    \bottomrule
    \end{tabular}}
    \caption{Human evaluation results comparing ``SVD'' and ``Full'' for different subject combinations, with 1000 human ratings for each combination.}
    \label{tab:multi_user}
\end{table}
\begin{figure*}[t]
  \centering
  \includegraphics[width=1\linewidth]{figures/rank2.pdf}
  \caption{Effect of limiting rank of spectral shifts. The figure displays examples of the subject's reconstruction and edition with varying ranks of the spectral shifts. Results indicate that a lower rank leads to limited ability to capture details in the edited samples, with better performance observed for a subject that is easier for the model to adapt to (\ie Teddybear).}
  \label{fig:rank}
\end{figure*}
\begin{figure*}[t]
  \centering
  \includegraphics[width=1\linewidth]{figures/weight_scale.pdf}
  \caption{Effects of scaling spectral shifts ($\Sigma_{\bdelta'}=\text{diag}(\text{ReLU}(\bsigma+s\bdelta))$) and weight deltas ($W'=W+s\Delta W$). Note that this scale is different from the classifier-free guidance scale. Scaling both spectral shift and weight delta changes the attribute strength, with too large a scale causing deviation from the text prompt and visual artifacts.}
  \label{fig:weight_scale}
\end{figure*}
\begin{figure*}[t]
  \centering
  \includegraphics[width=1\linewidth]{figures/single_lora_fast_v3.pdf}
  \caption{Results of fast adaptation for single subject generation. All models are fine-tuned for 100 steps without prior-preservation loss~\cite{ruiz2022dreambooth} (for main results we fine-tune 500-1000 steps with prior-preservation loss). The results indicate that all three methods perform similarly, except for the ``No-Face'' sculpture in (c) where LoRA shows underfitting and DreamBooth exhibits overfitting. In (e), SVDiff also shows overfitting, which could be a result of the large learning rate used.}
  \label{fig:single_lora_fast}
\end{figure*}

\subsection{Negative prompt}
To perform negative prompting, we repurpose the prior-preservation prompts as negative prompts $\bc^{neg}$. 
Recall that \textit{Classifier-free guidance}  (CFG)~\cite{ho2022classifier} extrapolates the conditional score by a scale factor $s>1$,
\begin{align}
    \hat{\bepsilon}_{\theta,s}(\bz_t|\{\bc,\emptyset\}) = s \cdot \hat{\bepsilon}_\theta(\bz_t | \bc) + (1-s) \cdot \hat{\bepsilon}_{\theta}(\bz_t | \emptyset). \label{eq:cfg}
\end{align}
Similar to~\cite{tumanyan2022plug}, we replace the null-conditioned score $\hat{\bepsilon}_{\theta}(\bz_t | \emptyset)$ in \cref{eq:cfg} by $\tilde{\bepsilon}_{\theta,\beta}$ defined as following,
\begin{align}
    \tilde{\bepsilon}_{\theta,\beta}(\bz_t|\{\bc^{neg},\emptyset\}) = \beta \cdot\hat{\bepsilon}_{\theta}(\bz_t | \emptyset) + (1-\beta) \cdot\hat{\bepsilon}_{\theta}(\bz_t | \bc^{neg}) \label{eq:cfg_neg}
\end{align}
\noindent where $0<\beta<1$. This can be easily extended to including multiple negative prompts.
\cref{fig:neg_prompt} shows a few examples of using negative prompts to remove the stitching artifacts introduced by Cut-Mix-Unmix. We hypothesize that this is because the model is trained to associate the prior prompt to the stitching style so negative prompting can help removing the stitching edges. However, we observe that negative prompting may not always help.

\subsection{Extensions}
We show a preliminary extension of our Cut-Mix-Unmix to Attend-and-Excite~\cite{chefer2023attend}. As shown in \cref{fig:cm_ane}, Cut-Mix-Unmix helps better disentangle respective visual features of the dog and the cat. It is also possible to extend and integrate our method to other attention-based methods~\cite{hertz2022prompt,tumanyan2022plug,feng2022training}.

% ============================================================== %
\section{Single-Subject Generation}
\subsection{Fast adaptation}
Here we show results of fast adaptation for single subject generation in \cref{fig:single_lora_fast}. This setting is slightly different from the experiments in the main text since we limit the fine-tuning steps as 100 without prior-preservation loss~\cite{ruiz2022dreambooth} (for main results we fine-tune 500-1000 steps with prior-preservation loss). Thus we tune the learning rate for each method to balance between faithfullness and realism~\cite{meng2021sdedit}. The learning rates we used are as follows:
\begin{itemize}
    \item \textbf{SVDiff}: 1-D weights $2\times 10^{-3}$, 2-D and 4-D weights $5\times 10^{-3}$
    \item \textbf{LoRA}~\cite{hu2021lora}: 1-D weights $2\times 10^{-3}$, 2-D and 4-D weights $1\times 10^{-4}$
    \item \textbf{DreamBooth}~\cite{ruiz2022dreambooth}: 1-D weights $1\times 10^{-3}$, 2-D and 4-D weights $5\times 10^{-6}$
\end{itemize}

In \cref{fig:single_lora_fast}, the performance comparison of our method, LoRA~\cite{hu2021lora} and DreamBooth~\cite{ruiz2022dreambooth} is shown under fast fine-tuning setting. The results indicate that all three methods perform similarly, except for the ``No-Face'' sculpture in (c) where LoRA shows underfitting and DreamBooth exhibits overfitting. In (e), SVDiff also shows overfitting, which could be a result of the large learning rate used.

% ============================================================== %
\section{Analysis on Spectral Shifts}
\subsection{Rank}
\cref{fig:rank} shows the results of limiting the rank of the spectral shifts of 2-D and 4-D weight kernels during training. Two examples are shown for each of the three subjects, one with the training prompt (to ``reconstruct'' the subject) and one with an edited prompt. Results show that the model can still reconstruct the subject with rank 1, but may struggle to capture details with an edited prompt when the rank of spectral shift is low. The visual differences between reconstructed and edited samples are smaller for the Teddybear than the building and panda sculpture, potentially because the pre-trained model already understands the concept of a Teddybear.

\subsection{Correlations}
We present the results of the correlation analysis of individually learned spectral shifts for each subject in \cref{fig:correlation}. Each entry in the figure represents the average cosine similarities between the spectral shifts of two subjects, computed across all layers. The diagonal entries show the average cosine similarities between two runs with the learning rate of 1-D weights set to $10^{-3}$ and $10^{-6}$, respectively. The results indicate that the similarity between conceptually similar subjects is relatively high, such as between the ``panda'' and ``No-Face'' sculptures or between the ``Teddybear'' and ``Tortoise'' plushies.

\subsection{Scaling}
\cref{fig:weight_scale} demonstrates the effect of scaling spectral shifts (labeled as ``SVD'', $\Sigma_{\bdelta'}=\text{diag}(\text{ReLU}(\bsigma+s\bdelta))$ with scale $s$) and weight deltas (marked as ``full'', $W'=W+s\Delta W$ with scale $s$). Samples are generated using the same random seed. Scaling both the spectral shift and full weight delta affects the presence of personalized attributes and features. The results show that scaling the weight delta also influences attribute strength. However, a scale value that is too large (\eg $s=2$) can cause deviation from the text prompt and result in dark samples.

\section{Image Attribution}
Avocado plushy: \url{https://unsplash.com/photos/8V4y-XXT3MQ}.

Pink chair: \url{https://unsplash.com/photos/1JJJIHh7-Mk}.

Brown and white puppy: \url{https://unsplash.com/photos/brFsZ7qszSY}, \url{https://unsplash.com/photos/eoqnr8ikwFE}, \url{https://unsplash.com/photos/LHeDYF6az38}, and \url{https://unsplash.com/photos/9M0tSjb-cpA}.

Crown: \url{https://unsplash.com/photos/8Dpi2Mb1-PM}.

Bedroom: \url{https://unsplash.com/photos/x53OUnxwynQ}.

Dog with flower: \url{https://unsplash.com/photos/Sg3XwuEpybU}.

Statue-of-Liberty: \url{https://unsplash.com/photos/s0di82cRiUQ}.

Beetle car: \url{https://unsplash.com/photos/YEPDV3T8Vi8}.

% Building: \url{https://finmath.rutgers.edu/admissions/how-to-apply}, and \url{https://www.istockphoto.com/photo/waksman-institute-of-microbiology-gm655087400-119154713}.
Building: \url{https://finmath.rutgers.edu/admissions/how-to-apply} and luvemakphoto/ Getty Images.

Teddybear, tortoise plushy, grey dog, and cat images are taken from Custom Diffusion~\cite{kumari2022multi}: \url{https://www.cs.cmu.edu/~custom-diffusion/assets/data.zip}.

Panda and ``No-Face'' sculpture images were captured and collected by the authors.
