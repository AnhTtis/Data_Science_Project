\section{Experiments}\label{sec:ex}

\subsection{Datasets and Evaluation}
We evaluate the effectiveness of our network on an in-home dataset and two public datasets, e.g., PMG2000, HAM10000~\cite{tschandl2018ham10000}, and APTOS2019~\cite{aptos2019-blindness-detection}.
%
%
\textbf{(a) PMG2000.}  We collect and annotate a benchmark dataset (denoted as PMG2000) for placental maturity grading (PMG) with four categories\footnote{Our data collection is approved by the Institutional Review Board (IRB).}. 
PMG2000 is composed of 2,098 ultrasound images, and we randomly split the whole data into a training set and a testing set with a ratio of 8:2.
%
\textbf{(b) HAM10000.}  
HAM10000~\cite{tschandl2018ham10000} is from the Skin Lesion Analysis Toward Melanoma Detection 2018 challenge, and it contains 10,015 skin lesion images with predefined 7 categories. 
%
\textbf{(c) APTOS2019.}  In APTOS2019~\cite{aptos2019-blindness-detection}, there are 3,662 labeled fundus images for grading diabetic retinopathy into five categories. 
%%
Following the same protocol in~\cite{gong2020distractor},  we split HAM10000 and APTOS2019 into a train set and a test set with the same ratio of 7:3. 
These three datasets are with different medical image modalities. PMG2000 is gray-scale and class-balanced ultrasound images; HAM10000 is colorful but class-imbalanced dermatoscopic images; and APTOS2019  is another class-imbalanced dataset with colorful Fundus images. 
%
%
Moreover, we introduce two widely-used metrics (i.e., Accuracy and F1-score) for quantitatively comparing our network and state-of-the-art methods. 

\subsection{Implementation Details}
All the experiments are implemented with the PyTorch on one NVIDIA RTX 3090 GPU. 
%
We center-crop the image and then resize the spatial resolution of the cropped image to $224$$\times$$224$. 
%%
Random flipping and rotation are implemented for data augmentation in the training process. 
%
Our network is trained in an end-to-end manner using the Adam optimizer with a batch size of 32.
The initial learning rate is set as 1×10$^{-3}$ for the denoising model U-Net, and 2×10$^{-4}$ for the DCG model (see Section~\ref{sec:dcg}) when training the whole network of our method in all experiments. 
%
Following~\cite{marrakchi2021fighting}, the number of training epochs is set as 1,000 for all three datasets. 
%
In inference, we empirically set the total diffusion time step $T$ as 100 for PMG2000, 250 for HAM10000, and 60 for APTOS2019, which is much smaller than most of the existing works~\cite{ho2020denoising,han2022card}.
%
%%
The average running time of our DiffMIC is about 0.056 seconds for classifying an image with a spatial resolution of $224$$\times$$224$.

\begin{table*}[!tbp]
\caption{Quantitative comparison to state-of-the-art methods on three classification datasets. The best results are marked in bold font.}

\centering
\subtable[PMG2000]{
\vspace{-3mm}
    \resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|ccccc|c} 
\toprule
\multicolumn{2}{c|}{Methods}                  & ResNet~\cite{he2016deep} & ViT~\cite{dosovitskiy2020image}   & Swin~\cite{liu2021swin}  & PVT~\cite{wang2021pyramid}   & GMIC~\cite{shen2021interpretable}   & \textbf{Our DiffMIC}  \\ 
\hline
\multirow{2}{*}{PMG2000} & Accuracy & 0.879  & 0.886 & 0.893 & 0.907 & 0.900  & \textbf{0.931}       \\
                                   & F1-score & 0.881  & 0.890 & 0.892 & 0.902 & 0.901 & \textbf{0.926}       \\
\bottomrule

\end{tabular}
\label{tab:main1}
    }}

\subtable[HAM10000 and APTOS2019]{

    \resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|cccccc|c} 
\toprule
\multicolumn{2}{c|}{Methods}          & LDAM~\cite{cao2019learning}  & OHEM~\cite{shrivastava2016training}  & MTL~\cite{liao2018deep}   & DANIL~\cite{gong2020distractor} & CL~\cite{marrakchi2021fighting}    & ProCo~\cite{yang2022proco} & \textbf{Our DiffMIC}  \\ 
\hline
\multirow{2}{*}{HAM10000}  & Accuracy & 0.857 & 0.818 & 0.811 & 0.825 & 0.865 & 0.887 & \textbf{0.906}          \\
                           & F1-score & 0.734 & 0.660 & 0.667 & 0.674 & 0.739 & 0.763 & \textbf{0.816}          \\ 
\hline
\multirow{2}{*}{APTOS2019} & Accuracy & 0.813 & 0.813 & 0.813 & 0.825 & 0.825 & 0.837 & \textbf{0.858}          \\
                           & F1-score & 0.620 & 0.631 & 0.632 & 0.660 & 0.652 & 0.674 & \textbf{0.716}          \\
\bottomrule
\end{tabular}
\label{tab:main2}
    }}

\label{tab:main}
\vskip -5pt
\end{table*}




%
\subsection{Comparison with State-of-the-art Methods}  
In Table~\ref{tab:main1}, we compare our DiffMIC against many state-of-the-art CNNs and transformer-based networks, including ResNet, Vision Transformer (ViT), Swin Transformer (Swin), Pyramid Transformer (PVT), and a medical image classification method (i.e., GMIC) on PMG2000. 
Apparently, PVT has the largest Accuracy of 0.907, and the largest F1-score of 0.902 among these methods.
More importantly, our method further outperforms PVT. 
It improves the Accuracy from 0.907 to 0.931, and the F1-score from 0.902 to 0.926.
%

 
Note that both HAM10000 and APTOS2019 have a class imbalance issue. 
Hence, we compare our DiffMIC against state-of-the-art long-tailed medical image classification methods, and report the comparison results in Table~\ref{tab:main2}.
%
For HAM10000, our method produces a promising improvement over the second-best method ProCo of 0.019 and 0.053 in terms of Accuracy and F1-score, respectively. 
For APTOS2019, our method obtains a considerable improvement over ProCo of 0.021 and 0.042 in Accuracy and F1-score respectively.
%




\begin{table*}[!tbp]
\centering
  \caption{Effectiveness of each module in our DiffMIC on the PMG2000 dataset. 
  }
  \vskip -5pt
    \resizebox{0.65\textwidth}{!}{%
\begin{tabular}{c|ccc|cc} 
\toprule
 & Diffusion & DCG & MMD-reg & Accuracy & F1-score \\
\hline
basic    &     \textbf{-}               &       \textbf{-}       &       \textbf{-}           & 0.879             & 0.881             \\
C1          & \checkmark         &        \textbf{-}      &      \textbf{-}            & 0.906             & 0.899             \\
C2          & \checkmark         & \checkmark   &    \textbf{-}              & 0.920             & 0.914             \\
\hline
\textbf{Our method}        & \checkmark         & \checkmark   & \checkmark       & \textbf{0.931}    & \textbf{0.926}    \\
\bottomrule
\end{tabular}
    }
\label{tab:ablation}
\vskip -5pt
\end{table*}

\begin{figure*}[!t]
    \centering
    \resizebox{0.85\textwidth}{!}{%
    \subfigure{\rotatebox{90}{\scriptsize{~~~PMG2000}}
        \includegraphics[width=\textwidth]{fig/vis_pmg.pdf}\label{fig:pmg}}\vspace{-10pt}}
    \resizebox{0.85\textwidth}{!}{%
    \subfigure{\rotatebox{90}{\scriptsize{~~~HAM10000}}
        \includegraphics[width=\textwidth]{fig/vis_ham.pdf}\label{fig: ham}}\vspace{-10pt}}
    \resizebox{0.85\textwidth}{!}{%
    \subfigure{\rotatebox{90}{\scriptsize{~~APTOS2019}}
        \includegraphics[width=\textwidth]{fig/vis_aptos.pdf}\label{fig: aptos}}}
    \vskip -5pt
    \caption{t-SNE obtained from the denoised feature embedding by the diffusion reverse process during inference on three datasets. $t$ is the current diffusion time step for inference. As the time step encoding progresses, the noise is gradually removed, thereby obtaining a clear distribution of classes; see the last column (please zoom in).}
    \label{fig: vis}
\end{figure*}


\subsection{Ablation Study}
We conduct ablation studies to evaluate the effectiveness of major modules of our network. To do so, we build three baseline networks from our method.  
%%
The first baseline (denoted as ``basic'') is to remove all diffusion operations and the MMD regularization loss from our network. It means that ``basic'' is equal to the classical ResNet18.
Then, we apply the vanilla diffusion process onto ``basic'' to construct another baseline network (denoted as ``C1''), and further add our dual-granularity conditional guidance into the diffusion process to build a baseline network, which is denoted as ``C2''. Hence, ``C2'' is equal to removing the MMD regularization loss from our network for image classification. 
%%
%%
Table~\ref{tab:ablation} reports the Accuracy and F1-score results of our method and three baseline networks on our PMG2000 dataset. Apparently, compared to ``basic'', ``C1'' has an Accuracy improvement of 0.027 and an F1-score improvement of 0.018, which indicates that the diffusion mechanism can learn more discriminate features for medical image classification, thereby improving the PMG performance. 
Moreover, the better Accuracy and F1-score results of ``C2'' over ``C1'' demonstrates that introducing our dual-granularity conditional guidance into the vanilla diffusion process can benefit the PMG performance.
%%
Furthermore, our method outperforms ``C2'' in terms of Accuracy and F1-score, which indicates that exploring the MMD regularization loss in the diffusion process can further help to enhance the PMG results.


\subsection{Visualization of our Diffusion Procedure} 
To illustrate the diffusion reverse process guided by our dual-granularity conditional encoding, we used the t-SNE tool to visualize the denoised feature embeddings at consecutive time steps. Figure~\ref{fig: vis} presents the results of this process on all three datasets. As the time step encoding progresses, the denoise diffusion model gradually removes noise from the feature representation, resulting in a clearer distribution of classes from the Gaussian distribution. The total number of time steps required for inference depends on the complexity of the dataset.
