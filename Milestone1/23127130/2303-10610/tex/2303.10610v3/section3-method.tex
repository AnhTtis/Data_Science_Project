\section{Method}
\label{sec:method}

\begin{figure*}[!t]
\centering
\includegraphics[width=1.0\textwidth]{fig/framework.pdf}
\vskip -5pt
\caption{\textbf{Overview of our DiffMIC framework. } (a) The training phase (forward process) and (b) The inference phase (reverse process) are constructed, respectively. 
(The noise of feature embedding is greater with the darker color.) (c) The DCG Model $\tau_\mathcal{D}$ guides the diffusion process by the dual priors from the raw image and ROIs.}
\label{fig:overview} 
\end{figure*}


Figure~\ref{fig:overview} shows the schematic illustration of our network for medical image classification. %% 
Given an input medical image $x$, we pass it to an image encoder to obtain the image feature embedding $\rho(x)$, and a dual-granularity conditional guidance (DCG) model to produce the global prior $\hat{y}_{g}$ and local prior $\hat{y}_{l}$.
%%
At the training stage, we apply the diffusion process on ground truth $y_0$ and different priors to generate three noisy variables $y^g_t$, $y^l_t$, and $y_t$ (the global prior for $y^g_t$, the local prior for $y^l_t$, and dual priors for $y_t$). 
Then, we combine the three noisy variables $y^g_t$, $y^l_t$, and $y_t$ and their respective priors and project them into a latent space, respectively. We further integrate three projected embeddings with the image feature embedding $\rho(x)$ in the denoising U-Net, respectively, and predict the noise distribution sampled for $y^g_t$, $y^l_t$, and $y_t$. 
%%
We devise condition-specific maximum-mean discrepancy (MMD) regularization loss on the predicted noise of $y^g_t$ and $y^l_t$, and employ the noise estimation loss by mean squared error (MSE) on the predicted noise of $y_t$ to collaboratively train our DiffMIC network.
%%%


\vspace{3pt}
\noindent 
\textbf{Diffusion Model.} \ 
%
Following DDPM~\cite{ho2020denoising}, our diffusion model also has two stages: a forward diffusion stage (training) and a reverse diffusion stage (inference). 
%
In the forward process, the ground truth response variable $y_0$ is added Gaussian noise through the diffusion process conditioned by time step $t$ sampled from a uniform distribution of $[1, T]$, and such noisy variables are denoted as $\{y_1,...,y_t,..,y_T\}$.
%%
As suggested by the standard implementation of DDPM, we adopt a UNet as the denoising network to parameterize the reverse diffusion process and learn the noise distribution in the forward process. 
In the reverse diffusion process, the trained UNet $\epsilon_\theta$ generates the final prediction $\hat{y}_0$ by transforming the noisy variable distribution $p_\theta(y_T)$ to the ground truth distribution $p_\theta(y_0)$: 
\begin{equation} 
p_\theta(y_{0:T-1}|y_T,\rho(x)) = \prod_{t=1}^{T}p_\theta(y_{t-1}|y_t,\rho(x)), \; \text{ and} \quad
p_\theta(y_T)=\mathcal{N}(\frac{\hat{y}_g+\hat{y}_l}{2},\mathbb{I}
),
\label{eq:mmd}
\end{equation} 
where $\theta$ is parameters of the denoising UNet, $\mathcal{N}(\cdot,\cdot)$ denotes the Gaussian distribution, and $\mathbb{I}$ is the identity matrix.
%%

\subsection{Dual-granularity Conditional Guidance (DCG) Strategy}
\label{sec:dcg}
\textbf{DCG Model.}
In most conditional DDPM, the conditional prior will be a unique given information. 
However, medical image classification is particularly challenging due to the ambiguity of objects. It is difficult to differentiate lesions and tissues from the background, especially in low-contrast image modalities, such as ultrasound images. 
Moreover, unexpected noise or blurry effects may exist in regions of interest (ROIs), thereby hindering the understanding of high-level semantics. 
Taking only a raw image $x$ as the condition in each diffusion step will be insufficient to robustly learn the fine-grained information, resulting in classification performance degradation. 

To alleviate this issue, we design a Dual-granularity Conditional Guidance (DCG) for encoding each diffusion step. 
Specifically, we introduce a DCG model $\tau_\mathcal{D}$ to compute the global and local conditional priors for the diffusion process.
Similar to the diagnostic process of a radiologist, we can obtain a holistic understanding from the global prior and also concentrate on areas corresponding to lesions from the local prior when removing the negative noise effects.
%
As shown in Figure~\ref{fig:overview} (c), for the global stream, the raw image data $x$ is fed into the global encoder $\tau_g$ and then a $1\times1$ convolutional layer to generate a saliency map of the whole image. The global prior $\hat{y}_{g}$ is then predicted from the whole saliency map by averaging the responses. 
For the local stream, we further crop the ROIs whose responses are significant in the saliency map of the whole image. 
Each ROI is fed into the local encoder $\tau_l$ to obtain a feature vector. 
We then leverage the gated attention mechanism\cite{ilse2018attention} to fuse all feature vectors from ROIs to obtain a weighted vector, which is then utilized for computing the local prior $\hat{y}_{l}$ by one linear layer.

\vspace{2mm}
\noindent
\textbf{Denoising Model.}
%%
The noisy variable $y_t$ is sampled in the diffusion process based on the global and local priors computed by the DCG model following:
\begin{equation}
    y_t = \sqrt{\bar{\alpha}_t}y_0+\sqrt{1-\bar{\alpha}_t}\epsilon+(1-\sqrt{\bar{\alpha}_t})(\hat{y}_{g}+\hat{y}_{l}),
\end{equation}
where $\epsilon \sim \mathcal{N}(0, I)$, $\bar{\alpha}_t=\prod_{t}\alpha_t, \alpha_t=1-\beta_t$ with a linear noise schedule $\{\beta_t\}_{t=1:T}\in (0,1)^T$. 
After that, we feed the concatenated vector of the noisy variable $y_t$ and dual priors into our denoising model UNet $\epsilon_\theta$ to estimate the noise distribution, which is formulated as:
%
\begin{equation}
    \epsilon_\theta(\rho(x), y_t, \hat{y}_{g}, \hat{y}_{l}, t) = D(E(f([y_t,\hat{y}_{g},\hat{y}_{l}]), \rho(x),t),t),
\end{equation}
where $f(\cdot)$ denotes the projection layer to the latent space. $[\cdot]$ is the concatenation operation. $E(\cdot)$ and $D(\cdot)$ are the encoder and decoder of UNet. 
Note that the image feature embedding $\rho(x)$ is further integrated with the projected noisy embedding in the UNet to make the model focus on high-level semantics and thus obtain more robust feature representations.
In the forward process, we seek to minimize the noise estimation loss $\mathcal{L}_\epsilon$:
\begin{equation}
    \mathcal{L}_\epsilon = ||\epsilon-\epsilon_\theta(\rho(x), y_t, \hat{y}_{g}, \hat{y}_{l}, t)||^2 .
\end{equation}
Our method improves the vanilla diffusion model by conditioning each step estimation function on priors that combine information derived from the raw image and ROIs. 

%\vspace{1mm}
\subsection{Condition-specific MMD Regularization}
%
%Maximum-Mean Discrepancy (MMD) is to quantify the similarity between two distributions by comparing all of their moments~\cite{gretton2006kernel,li2015generative}. It can be efficiently implemented using a kernel trick. 
Maximum-Mean Discrepancy (MMD) is to measure the similarity between two distributions by comparing all of their moments~\cite{gretton2006kernel,li2015generative}, which can be efficiently achieved by a kernel function. 
Inspired by InfoVAE~\cite{zhao2017infovae}, we introduce an additional pair of condition-specific MMD regularization loss to learn mutual information between the sampled noise distribution and the Gaussian distribution. 
To be specific, we sample the noisy variable $y_t^g$ from the diffusion process at time step $t$ conditioned only by the global prior and then compute an MMD-regularization loss as:
\begin{equation}
    \begin{aligned}
         \mathcal{L}^{g}_{MMD}(n||m) & =  \mathbb{K}(n,n^{'})-2\mathbb{K}(m,n)+\mathbb{K}(m,m^{'}), \\
   \text{with}   \;\;  &n=\epsilon, \;\;  m = \epsilon_\theta(\rho(x),\sqrt{\bar{\alpha}_t}y_0+\sqrt{1-\bar{\alpha}_t}\epsilon+(1-\sqrt{\bar{\alpha}_t})\hat{y}_{g},\hat{y}_{g},t),  
    \end{aligned}
    \label{eq:mmdg}
\end{equation} 
where $\mathbb{K}(\cdot, \cdot)$ is a positive definite kernel to reproduce distributions in the Hilbert space. The condition-specific MMD regularization is also applied on the local prior, as shown in Figure~\ref{fig:overview} (a).
%Such condition-specific regularization helps train the diffusion model in a complementary way to converge faster in a stable way for each guidance. 
While the general noise estimation loss $\mathcal{L}_\epsilon$ captures the complementary information from both priors, the condition-specific MMD regularization maintains the mutual information between each prior and target distribution.
This also helps the network better model the robust feature representation shared by dual priors and converge faster in a stable way.




\subsection{Training and Inference Scheme}

%
\vspace{2mm}
\noindent
\textbf{Total loss.} \
%%
By adding the noise estimation loss and the MMD-regularization loss, we compute the total loss $\mathcal{L}_{diff}$ of our denoising network as follows:
\begin{equation}
    \mathcal{L}_{diff} = \mathcal{L}_\epsilon + \lambda(\mathcal{L}^{g}_{MMD}+\mathcal{L}^{l}_{MMD}),
\end{equation}
where $\lambda$ is a balancing hyper-parameter, and it is empirically set as $\lambda$=0.5.

%\vspace{2mm}
%\noindent
%\textbf{Training Scheme.} \
%%
%
\vspace{2mm}
\noindent
\textbf{Training details.} \ The diffusion model in this study leverages a standard DDPM training process, where the diffusion time step $t$ is selected from a uniform distribution of $[1, T]$, and the noise is linearly scheduled with $\beta_1 = 1\times10^{-4}$ and $\beta_T = 0.02$. 
We adopt ResNet18 as the image encoder $\rho(\cdot)$. 
Following \cite{han2022card}, we concatenate $y_t$,$\hat{y}_{g}$,$\hat{y}_{l}$, and apply a linear layer with an output dimension of 6144 to obtain the fused vector in the latent space. 
%We perform a Hadamard product between such vector and a timestep embedding to obtain a response embedding conditioned on the timestep. We then perform Hadamard product between the image feature embedding and response embedding to integrate these variables, and send the resulting vector through two more fully-connected layers with the same dimension, each would first be followed by a Hadamard product with a timestep embedding, and lastly a fully-connected layer with an output dimension of classes as the noise prediction. Note that all fully-connected layers are also followed by a batch normalization layer and a Softplus non-linearity, except the output layer.
To condition the response embedding on the timestep, we perform a Hadamard product between the fused vector and a timestep embedding. We then integrate the image feature embedding and response embedding by performing another Hadamard product between them. The output vector is sent through two consecutive fully-connected layers, each followed by a Hadamard product with a timestep embedding. Finally, we use a fully-connected layer to predict the noise with an output dimension of classes. It is worth noting that all fully-connected layers are accompanied by a batch normalization layer and a Softplus non-linearity, with the exception of the output layer.
%
For the DCG model $\tau_D$, the backbone of its global and local stream is ResNet. We adopt the standard cross-entropy loss as the objective of the DCG model. We jointly train the denoising diffusion model and DCG model after pretraining the DCG model 10 epochs for warm-up, thereby resulting in an end-to-end DiffMIC for medical image classification.


\vspace{2mm}
\noindent
\textbf{Inference stage.} \
As displayed in Figure~\ref{fig:overview} (b), given an input image $x$, we first feed it into the DCG model to obtain dual priors $\hat{y}_g, \hat{y}_l$. 
Then, following the pipeline of DDPM, the final prediction $\hat{y}_0$ is iteratively denoised from the random prediction $y_T$ using the trained UNet conditioned by dual priors $\hat{y}_g, \hat{y}_l$ and the image feature embedding $\rho(x)$.
