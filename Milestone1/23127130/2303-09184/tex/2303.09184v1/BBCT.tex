\documentclass[8pt, a4paper, oneside]{article}

\usepackage{amsmath, amsthm, amssymb, graphicx, multicol, authblk, geometry, indentfirst, threeparttable, caption, subcaption, placeins}

\usepackage[table,xcdraw]{xcolor}

\title{Block-wise Bit-Compression of Transformer-based Models}

\author{Gaochen Dong, Wei Chen}

\affil{Tensorchip, Beijing, China \\ gaochendong\_buaa@outlook.com}

\date{}

\linespread{1}

\geometry{right=1.5cm,left=1.5cm,top = 2.0cm, bottom = 2.0cm}

\begin{document}

\maketitle

\begin{multicols}{2}

{\centering\section*{abstract}}

With the popularity of the recent Transformer-based models represented by BERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range of natural language processing tasks. However, the massive computations, huge memory footprint, and thus high latency of Transformer-based models is an inevitable challenge for the cloud with high real-time requirement. To tackle the issue, we propose BBCT, a method of block-wise bit-compression for transformer without retraining. Our method achieves more fine-grained compression of the whole transformer, including embedding, matrix multiplication, GELU, softmax, layer normalization, and all the intermediate results. As a case, we compress an efficient BERT with the method of BBCT. Our benchmark test results on General Language Understanding Evaluation (GLUE) show that BBCT can achieve less than 1\%  accuracy drop in most tasks.

\section{Introduction}

Deep learning has attracted wide attention and made considerable progress. Since the birth of the attention\cite{vaswaniAttentionAllYou2017} mechanism, Transformer-based models, like BERT\cite{devlinBERTPretrainingDeep2019}, GPT\cite{radfordImprovingLanguageUnderstanding}, GPT-2\cite{radfordLanguageModelsAre}, GPT-3\cite{brownLanguageModelsAre2020} and ChatGPT, have solved many problems in the field of NLP and even the field of CV better, such as sentiment classification\cite{socherRecursiveDeepModels}, machine translation\cite{wangLearningDeepTransformer2019}, document analysis\cite{vanakenHowDoesBERT2019}, question answering, text summarization\cite{zhangHIBERTDocumentLevel2019}, multi-round dialogue, etc. Due to superior performance of Transformer-based models, there are increasing demands to deploy them in the cloud, which requires both low latency and high accuracy. 

However, massive computations, huge memory footprint, and thus high latency of Transformer-based models bring great difficulties to deployment and application in the cloud. Built upon transformer, for example, BERT uses multi-head self-attention mechanism to encode the information of input sentence to obtain better representative ability. Encoder of self-attention mechanism contains a large number of matrix multiplication calculations, which need huge computational complexity and memory footprint. The explicit universal architecture of Transformer-based models is depicted in Figure 1. 

Compared with 32-bit floating point numbers, low precision numbers not only occupy less storage space, but also have faster calculation speed and lower bandwidth requirements. In this case, several methods of compression are proposed to accelerate inference of transformer with negligible loss of accuracy. And these work have achieved valuable results in various degrees. 

In this paper, we investigate the method of block-wise bit-compression of Transforer-based models through algorithm/hardware co-design, denoted as BBCT, and take BERT for example. BBCT quantize BERT fully, including embedding, matrix multiplication, GELU\cite{hendrycksGaussianErrorLinear2020}, softmax, layer normalization\cite{baLayerNormalization2016}, and all the intermediate results. Experiments demonstrate that the accuracy drop of BBCT is less than 1\% on most GLUE datasets without retraining. 

The main contributions of this paper can be summarized as follows: (i) We propose an efficient method of block-wise compression of Transformer-based models. (ii)For each layer of transformer, we provide a well-designed trick to compress it to low-percision data type, mainly int4, int8, and fp8. (iii) Without retraining, we evaluate BBCT on GLUE datasets, which achieves less then 1\% accuracy drop in most tasks. In theory, BBCT is suitable for deployment of Transformer-based models on any chip.

\begin{figure*}[htbp]
\centering
\includegraphics[scale = 0.6]{BERT-arch.png}
\caption{The Architecture of Transformer-based Models}
\end{figure*}

\section{Related Work}

Transformer-based models use multi-head self-attention mechanism to compute the representation of a sequence by relating different positions of it multiple times in parallel. In this way, they are able to achieve excellent results on major NLP tasks. However, the model size of Transformer-based models are usually up to 100M, or even higher, which results in massive computations, huge memory footprint, and thus high latency.

Compress and accelerate Transformer-based models has become a research hot spot, and some viable methods have been proposed, such as pruning\cite{huangAutomaticEfficientBERT2022}, quantization\cite{bhandareEfficient8BitQuantization2019}, distillation\cite{jiaoTinyBERTDistillingBERT2020}, etc. In the case of quantization of BERT, Q8BERT\cite{zafrirQ8BERTQuantized8Bit2019} and Q-BERT\cite{shenQBERTHessianBased2019} only quantize part of parameters of BERT, where other part of operations in the inference are still carried out with floating point arithmetic (aka fake quantization). I-BERT\cite{kimIBERTIntegeronlyBERT2021} applies mixed precision, int8 for matrix multiplication and int32 for nonlinear operations, to quantize the entire inference of BERT with interger-only arithmetic. FQ-BERT\cite{liuHardwareAccelerationFully2021} also fully quantizes the BERT mixedly, int4 for partial matrix multiplication and int8 for other operations, and can be deployed on FPGA. However, both I-BERT and FQ-BERT need retraining to fine tune the parameters of model.

\section{Method}

We propose to compress each layer of BERT, including embedding, matrix multiplication, GELU, softmax, layer normalization, and all the intermediate results. Before introducing specific compression strategies for each layer of BERT, we define several functions of compression. In this paper, we apply a symmetric shift quantization method\cite{miyashitaConvolutionalNeuralNetworks2016}. For k-bit quantization of tensor x, the function is:$$ shift = \lfloor log_{2}(\frac{2^{k-1}}{max(|x|)}) \rfloor $$ $$ x_q = clip(\left[ x << shift \right], MIN, MAX) $$ MIN and MAX are calculated statistically by measuring KL divergence. For k-bit dequantization of tensor x\_q, the function is:$$ x = x_q >> shift $$

In addition to quantizing model parameters to fixed point numbers, we can convert parameters of certain layer to fp8 data type to reduce computation, since fp8, as a newly proposed data type, has the advantages of low cost of floating-point number multiplication(using shift operation) and low precision. We can limit the fp32 data to the represent range of fp8 data type to get fp8 data. The represent ranges of the data types we usually use is shown in Table 1.

\end{multicols}

\begin{table*}[h!]
\centering
\caption{The Represent Ranges of Different Data Types}
\begin{tabular}{lll}
\hline
Data Type & Dynamic Range                                               & Min Positive Value           \\ \hline
fp32      & -3.4*10\textasciicircum{}38$\sim$3.4*10\textasciicircum{}38 & 1.4*10\textasciicircum{}-45  \\
int4      & -8$\sim$7                                                   & 1                            \\
int8      & -128$\sim$127                                               & 1                            \\
fp8(e4m3) & -240$\sim$240                                               & 1.95*10\textasciicircum{}-3  \\
fp8(e5m2) & -57344$\sim$57344                                           & 1.526*10\textasciicircum{}-5 \\ \hline
\end{tabular}
\end{table*}

\begin{figure*}[htbp]
\centering
\includegraphics[scale = 0.6]{BERT-compute.png}
\caption{The Computing Stream of BBCT}
\end{figure*}

\begin{multicols}{2}



Figure 2(a) is the general computing stream of BBCT method, where clip, compress and recompress are optional.

\subsection{Matrix Multiplication}
Matrix multiplication is the main source of computation of self-attention mechanism. Matrix multiplication can be converted into matrix block multiplication so that we can only calculate the multiplication and accumulation of one block at a time. In this paper, we compress weight to 4/8-bit and bias to 8-bit. We compute matrix block multiplication with 4/8-bit multiplication to obtain the product. Then we add the product and bias to obtain the final output of matrix multiplication, and recompress the final output to appropriate data type.

Taking $ y=x \cdot w^{T} + b $ for example, we can compress weight and bias of matrix multiplication in blocks. After blocking, x, w, b, and y turn into A, B, C and D respectively.
$$
\begin{bmatrix}
x_{11} & \cdots & x_{1n} \\
\vdots & \ddots & \vdots \\
x_{m1} & \cdots & x_{mn} \\
\end{bmatrix}
=
\begin{bmatrix}
A_{11} & \cdots & A_{1q} \\
\vdots & \ddots & \vdots \\
A_{m1} & \cdots & A_{pq} \\
\end{bmatrix}
$$
$$
\begin{bmatrix}
A_{11} & \cdots & A_{1q} \\
\vdots & \ddots & \vdots \\
A_{m1} & \cdots & A_{pq} \\
\end{bmatrix}
\stackrel{compress}{\longrightarrow}
\begin{bmatrix}
cA_{11} & \cdots & cA_{1q} \\
\vdots & \ddots & \vdots \\
cA_{m1} & \cdots & cA_{pq} \\
\end{bmatrix}
$$

Others are similar. Then, we can complete matrix multiplication with low precision and get the final result cD, where c means compression.
$$
{cA}\cdot{cB}^{T}+{cC}\stackrel{recompress}{\longrightarrow}cD
$$

Here are some noteworthy details if we compress matrix multiplication by quantization. Before accumulation, we have to normalize the shift of product of matrix blocks participating in accumulation. In detail, q shift values $shift_k$ of $ cA_{ik}\cdot cB_{jk} $ should be uniformed:
$$
shift_{max}=max(shift_k)
$$
The details of matrix block multiplication with low precision are as follows: 
$$
acc_{ij} = \sum_{k=1}^{q}\left[ cA_{ik}\cdot cB_{jk}^{T} << (shift_{max} - shift_k) \right] $$
Similarly, the shift values of $acc_{ij}$ and $cC_{j}$ should be uniform before adding them together. Figure 2(b) is how matrix multiplication works in BBCT.

\subsection{Nonlinear Operations} 

For nonlinear operations (like GELU, Softmax, and LayerNorm), there does not exist a straightforward method of quantization because quantization relies on the linear property of the operator. An effective solution is to use floating-point number, like fp8, or apply lookup table. For latter, we quantize a certain range of input, and treat quantized input as the key of lookup table. And we quantize output corresponding to the input, and treat quantized output as the value of lookup table. Therefore, we finally build a lookup table with 256 key-value pairs for any int8 input. Based on lookup tables, we can use some interpolation algorithms or others to approximate these nonlinear operations. For example, we can find two keys $x_0$, $x_1$ that are closest to the input x. After looking up the table, we get two values $y_0$, $y_1$. And the output after fitted is 
$$
\frac{x - x_1}{x_0 - x_1}y_0 + \frac{x - x_0}{x_1 - x_0}y_1
$$

\subsubsection{GELU}

The function of GELU is 
$$
GELU(x)=x\cdot \frac{1}{2} \left[ 1+erf(x/\sqrt{2}) \right]
$$

It has great linear property, similar to ReLU, and is suitable for linear fitting. So we can directly quantize the input and output of GELU, build the lookup table and use interpolation algorithm with negligible accuracy drop. 

\subsubsection{Softmax}

The function of softmax is
$$
Softmax(x_i)=\frac{exp(x_i)}{\sum_{j}exp(x_j)}
$$

Softmax operation contains exponential calculation, which is also a non-linear operation. Our method is to replacing the exponential function with a table lookup operation. Then we can obtain the output of softmax by integer arithmetic. A useful trick before building a lookup table for exponential fuction is that all inputs are subtracted from their maximum value before quantization. In this way, we limit the output to between 0 and 1 without changing the output of softmax function. It avoids the defect that the output distribution of exponential function is too wide to quantize. What's more, in other to handle numbers that are out of sampling bounds, we can use the rule for exponents $ e^{x+y} = e^{x}\times e^{y} $. Using x and y as keys to lookup table separately, we can get the value corresponding to key x+y. 

\subsubsection{LayerNorm}

The function of LayerNorm is
$$
LayerNorm(x)=\frac{x-E[x]}{\sqrt{Var[x]+\varepsilon}}\times \gamma + \beta 
$$

The main nonlinear part of layer normalization function is the function of the square root. We quantize the input and output of the sqrt function to build the lookup table. Then we can obtain the output of LayerNorm by integer arithmetic. Like exponents, we also have a method to handle numbers that are out of sampling bounds by using rule of $\sqrt{x + y} = \sqrt{x} * \sqrt{y}$.

\subsection{Intermediate Results}
Similar to matrix multiplication, intermediate results can also be divided into blocks and be quantized in blocks. A rule to be observed is that the intermediate results must be passed with 4/8-bit data type (int4, int8 or fp8) between two layers of model. To achieve this goal, we must recompress the final result of layer to 4/8-bit if necessary.

\section{EXPERIMENTS}

\subsection{Dataset}

We experiment with the datasets of GLUE\cite{wangGLUEMultiTaskBenchmark2019} benchmark. Depending on the purpose of the task and the difficulty level of the dataset, it consists of three types of tasks (single-sentence tasks, similarity and paraphrase tasks, and inference tasks). This paper selects one single-sentence task(SST-2\cite{seoBidirectionalAttentionFlow2018}), one similarity and paraphrase tasks (STS-B\cite{cerSemEval2017TaskSemantic2017}), and two inference tasks (MNLI\cite{williamsBroadCoverageChallengeCorpus2018}, and RTE\cite{bentivogliFifthPASCALRecognizing}).

\subsection{Setup}

The baseline models are unquantized BERT-base models provided by the Pytorch-Transformers\footnote{https://github.com/huggingface/transformers} after retraining. The proposed BBCT is compared with two quantized methods, Q8BERT and FQ-BERT. In detail, our models can be divided into four types according to the different data types of each layer, shown in Table 2. It is recommended that the encoder, which need heavy computation, is deployed in DSA and the embedding, which need negligible computation, is deployed in CPU.

\end{multicols}

\begin{center}
\begin{table*}[!htp]\centering
\caption{Data Types of BBCT, Q8BERT, FQ-BERT}
\tabcolsep=0.3cm
\tiny
\begin{tabular}{llllllll}
\hline                                                                                     
models         & embedding(w/b) & Linear(w/b/i\tnote{*})  & MatMul(w/b/i)  & Softmax(i) & LayerNorm(w/b/i) & FFN(w/b/i)     & GELU(i) \\ \hline
Q8-BERT        & int8/int8 & fp32/fp32/fp32 & fp32/fp32/fp32   & fp32      & fp32/fp32/fp32    & int8/int8/int8       & fp32 \\ 
BBCT\_int8/fp32    & int8/int8 & fp32/fp32/fp32 & fp32/fp32/fp32   & fp32      & fp32/fp32/fp32    & int8/int8/int8       & fp32 \\ \hline
FQ-BERT        & fp32/fp32 & int4/int32/int8 & int8/int32/int8 & int8      & int8/int32/int8   & int8/int32/int8      & fp32 \\ 
BBCT\_int4/8   & fp32/fp32 & int4/int8/int8 & int4/int8/int8   & int8      & int8/int8/int8    & int4/int8/int8       & int8 \\
BBCT\_int8 & fp32/fp32 & int8/int8/int8 & int8/int8/int8   & int8      & int8/int8/int8    & int8/int8/int8       & int8 \\ \hline
BBCT\_fp8    & fp32/fp32 & fp8/fp8/fp8    & fp8/fp8/fp8      & fp8       & fp8/fp8/fp8       & fp8/fp8/fp8          & fp8  \\ \hline
\end{tabular}
\begin{tablenotes}
\footnotesize
\item[*] we denote w as the data type of weight, b as the data type of bias and i as the data type of intermediate results.
\end{tablenotes}
\end{table*} 
\end{center}

\begin{multicols}{2}

Of the four models, BBCT\_fp8 has the least computational effort, followed by BBCT\_int4/8.

\subsection{Performance}

We use the accuracy of SST-2, MNLI, RTE, and the Spearman correlation of STS-B as comparison metrics. The benchmark results of GLUE tasks are shown in Table 3.  

\end{multicols}

\begin{center}
\begin{table*}[!htb]\centering
\caption{Benchmark Results of BBCT}
\begin{tabular}{lllllllll}
\hline
            & SST-2   & STS-B   & MNLI-m   & RTE     \\ \hline
Bert-base   & 91.74\% & 87.36\% & 83.61\%  & 62.45\% \\
BBCT\_int8/fp32 & 91.86\% & 87.39\% & -        & 64.26\% \\
BBCT\_int8 & 92.2\%  & -       & 82.14\% & - \\
BBCT\_int4/8 & 90.94\%       & -       & 80.08\% & -       \\
BBCT\_fp8 & 91.74\% & 87.36\% & 83.61\%        & 62.45\% \\ \hline
 \hline
\end{tabular}
\end{table*}
\end{center}

\begin{multicols}{2}

\begin{center}
\begin{table}[!htb]\centering
\caption{Accuracy Drop }
\begin{tabular}{lllllllll}
\hline
            & SST-2   & STS-B   & MNLI-m  & RTE     \\ \hline
Q8BERT      & -0.13\% & -0.65\% & -       & -1.32\% \\ 
BBCT\_int8/fp32 & +0.12\% & +0.03\%       & -       & +1.81\% \\ \hline
FQ-BERT     & -0.81\% & -       & -3.61\% & -       \\
BBCT\_int8 & +0.46\% & - & -1.47\% & - \\
BBCT\_int4/8 & -0.80\%       & -       & -3.53\%  & -       \\ \hline
BBCT\_fp8 & +0.00\%  & +0.00\%  & +0.00\%  & +0.00\%  \\ \hline
\end{tabular}
\end{table}
\end{center}

\end{multicols}

\begin{center}
\begin{table*}[!htb]\centering
\caption{Accuracy Drop after Quantizing Different Parts of BERT}
\begin{tabular}{llllllll}
\hline
MatMul & Softmax & LayerNorm & GELU & FQ-BERT & BBCT\_int4/8 \\ \hline
\checkmark & - & - & - & -1.04\% & -0.78\% \\
\checkmark & \checkmark & - & - & -0.46\% & -0.40\% \\
\checkmark & \checkmark & \checkmark & - & - & -0.73\% \\
\checkmark & \checkmark & \checkmark & \checkmark & -0.81\% & -0.80\% \\   \hline         
\end{tabular}
\end{table*}
\end{center}

\begin{multicols}{2}

In most tasks, BBCT shows lower accuracy drop than Q8BERT. This is mainly due to the block-wise compression strategy of BBCT. Block-wise means better compression at a finer range, taking correlations into account. The parameters of BBCT\_int4/8 have lower precision than FQ-BERT. For example, the weight and bias of MatMul $QK^{T}$ in BBCT\_int4/8 is 4-bit, 8-bit separately while the weight and bias of MatMul $QK^{T}$ in FQ-BERT is 8-bit, 32-bit. This means BBCT\_int4/8 needs lower computing than FQ-BERT. However, BBCT\_int4/8 has better performance performance than FQ-BERT. For SST-2 datasets, the accuracy of BBCT\_int4/8 drops 0.8\%, less than 0.81\% drop of FQ-BERT. On other datasets, BBCT also showes excellent performance when choosing different data type.

Further, using SST-2 dataset, we compare BBCT\_int4/8 and FQ-BERT after each compressing part of BERT to analyze the performance of BBCT, shown in Table 4. We can find that the non-linear operations also have excellent performance, besides block-wise.


\section{Conclusion}

In this work, we propose BBCT, a block-wise bit-compression of Transformer-based models without retraining, for cloud computing. BBCT can reduce computational effort, memory footprint and latency with negligible accuracy drop (usually less than 1\%). Specially, if we use fp8 or int4 data types, transformer has the best trade-off between performance and computation requirements. Compared to the previous quantization method, BBCT is also more hardware-friendly by using shift quantization method, and more user-friendly with no retraining. This brings great benefits to deployment of Transformer-based models, such as BERT, CPT-3, ChatGPT, etc.

\bibliographystyle{ieeetr}

\bibliography{distilling, layer, NLP, pruning, quantization}

\end{multicols}

\end{document}
