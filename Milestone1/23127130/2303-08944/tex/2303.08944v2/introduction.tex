\section{Introduction} 








Robustness to adversarial examples is considered a major contemporary challenge in machine learning. Adversarial examples are carefully crafted perturbations or manipulations of natural examples that cause machine learning predictors to miss-classify at test-time \citep{goodfellow2014explaining}. One particularly challenging aspect of this problem is the asymmetry between the learner and the adversary. Specifically, a learner needs to produce a predictor that is \textit{correct} on a randomly drawn natural example and \textit{robust} to potentially \textit{exponentially} many possible perturbations of it; while, the adversary needs to find just a single perturbation that fools the learner. In fact, because of this, adversarially robust learning has proven to require more sophisticated learning algorithms that go beyond standard Empirical Risk Minimization (ERM) in non-robust learning \citep{pmlr-v99-montasser19a}. 


In patch attacks on images, for instance, an adversary can select one of an exponential number of designs for a patch to be placed in the image in order to cause a classification error. To address this exponential asymmetry between the learner and the adversary, recently \citet{xiang2022patchcleanser} introduced a clever algorithmic scheme, known as Patch-Cleanser, that provably reduces the exponential number of ways that an adversary can attack to a polynomial number of ways through the idea of masking images. 

Specifically, Patch-Cleanser's \emph{double-masking} approach is based on zero-ing out  two different contiguous blocks of an input image, hopefully to remove the adversarial patch. For each one-masked image, if for all possible locations of the second mask, the prediction model outputs the same classification, it means that the first mask removed the adversarial patch, and the agreed-upon prediction is correct. Any disagreements in these predictions imply that the mask was not covered by the first patch. 
\kmsmargincomment{Alt vsn: Crucially the Patch Cleanser requires that for a given image the classifier has zero error for every double masked image. In order to,... }Crucially, the Patch-Cleanser algorithm requires a {\em two-mask correctness} guarantee from an underlying predictor $F$ that is defined as follows: for a given input image $x$ and label $y$, if for any pair of masks applied to $x$, predictor $F$ outputs the correct prediction $y$, then $F$ has a two-correctness guarantee on $(x,y)$ \citep[see Definition 2 in][]{xiang2022patchcleanser}. In order to train a predictor with the two-mask correctness guarantee, \citet{xiang2022patchcleanser} augment the training dataset with pairs of masks at random locations of training images, and perform \emph{empirical risk minimization} ($\ERM$) on the augmented dataset.



\paragraph{Our Contributions}
When no predictor is perfectly correct on {\em all} perturbations (e.g., all two-mask operations), which we refer to as the 
the \textit{non-realizable} or \emph{agnostic} setting, we exhibit an example where plain $\ERM$ on the augmented dataset fails (See \prettyref{exmpl:ERM-failure}%
). At  a high-level, the main issue is that plain $\ERM$ on the augmented data-set treats all mistakes equally and so this could lead to learning a predictor with very high robust loss, i.e. on {\em many} training examples.
Our first contribution is to investigate whether the reduction proposed by~\citet{xiang2022patchcleanser} can be extended to the \emph{non-realizable} setting.
We answer this question affirmatively in~\prettyref{sec:non-realizable-oracle}, by building upon a prior work by~\citet{DBLP:conf/colt/FeigeMS15}.%








Next, in \prettyref{sec:multi-robustness}, we consider a multi-group setting and investigate the question of \emph{agnostic multi-robust learning} using an $\ERM$ oracle. This question is inspired by the literature on \emph{multi-calibration} and \emph{multi-group learning}~\citep{multicalib, multi-accuracy-Kim-etal,multiagnostic,multiagstrat, bugbounty}.
Our objective is that given a hypothesis class $\calH$ and a (potentially) rich collection of subgroups $\calG$, learn a predictor $h$ such that for each group $g\in\calG$, $h$ has low robust loss on $g\in \calG$. However, we highlight that the prior work on multi-group learning does not extend to the setting of robust loss since they do not consider adversarial perturbations of natural examples. To our knowledge, our work is the first to consider the notion of multi-group learning for robust loss. That being said we emphasize that there is a trade-off here; our guarantees are for the \emph{more challenging objective of robust loss}, but they are weaker than the ones given for PAC learning in the prior work. A detailed comparison is given in~\prettyref{sec:related-work}. 

Our motivation for studying multi-robustness is two-fold. First, to prohibit the adversary from targeting a specific demographic group
for adverse treatment. Additionally, it can increase the overall performance of the model by forcing the model to be robust on vulnerable examples. For instance, imagine a self-driving car system with a vision system recording a drive and we consider adversarial examples attacking individual frames of the video.
Ideally, the system would have robust performance over every frame. 
However, average robust error of $1\%$ could be very problematic if those errors instead of occurring uniformly then those errors concentrated on a specific adjacent set of frames.
In this %
example, imagine that the protected groups are nearby frames so that we maintain smooth and reliable performance \emph{locally and globally}.

To achieve multi-robustness, using plain $\ERM$ can fail by \emph{concentrating} the overall robust loss on a \emph{few} groups, instead of \emph{spreading} the loss across \emph{many} groups. However, building on our algorithm in \prettyref{sec:non-realizable-oracle} we propose~\prettyref{alg:boosting} that runs an additional layer of boosting with respect to groups to achieve multi-robustness guarantees across groups. We propose two types of multi-robustness guarantees, the first one is a randomized approach that guarantees the expected robust loss on each group is low (\prettyref{thm:generalization-multi-groups}). Next, we add a de-randomization step to derive deterministic guarantees for the robust loss incurred on each group (\prettyref{thm:generalization-multi-groups-deterministic}).






\subsection{Related Work}
\label{sec:related-work}
\paragraph{Patch Attacks} Patch attacks \citep{brown2017adversarial,karmon2018lavan,yang2020patchattack} 
are an important threat model in the general field of test-time evasion attacks \citep{goodfellow2014explaining}. 
Patch attacks realize adversarial test time evasion attacks to computer vision systems in the wild
by printing and attaching a patch to an object.
To mitigate this threat, 
there has been an active line of research for providing certifiable robustness guarantees against them \citep[see e.g.,][]{minorityreport, patchguard, patchguard++, bagcert, clippedbag,chiang2020}.

\paragraph{Adversarial Learning using $\ERM$}Recent work by \citep{DBLP:conf/colt/FeigeMS15} gives a reduction algorithm for adversarial learning using an ERM oracle, but their guarantee is only for finite hypothesis classes. We observe in this work that we can apply their reduction algorithm to our problem, and along the way, we extend the guarantees of their algorithm. A more detailed comparison is provided in \prettyref{sec:feigecomparison}.






\paragraph{Multi-group Learning}
Interestingly, the notion of multi-robustness has connections with a thriving area of work in algorithmic fairness centered on the notion of multi-calibration~\cite{multicalib, multi-accuracy-Kim-etal,multiagnostic,multiagstrat, bugbounty, gopalan2022loss}. %
The promise of these multi-guarantees, given a rich set of groups, is to ensure uniformly acceptable performance
on many groups simultaneously. 

Specifically, \citet{multiagnostic} show how to learn a predictor such that the loss experienced by every group is not much larger than the best
possible loss for this group within a given hypothesis class. However, we highlight that the prior work on multi-group learning does not extend to the setting of robust loss since their goal is not to minimize the robust loss by taking into consideration different perturbations of natural examples. In contrast, our approach can achieve multi-robustness guarantees by utilizing two layers of boosting to ensure `emphasis' on both specific groups and the adversarial perturbations.

\citet{multiagstrat,bugbounty} study the problem of minimizing a general loss function over a collection of subgroups. Their approach can capture the robust loss, however, the main distinction between their algorithm and our approach is that unlike them, we do not use group membership during the test time. This is essential when groups correspond to protected features, and therefore in some scenarios, it would be undesirable to incorporate them in decision models. Additionally, if we interpret some of the groups in our setting as objects to be classified like a stop-sign group or fire-hydrant group, %
then an approach that needs to detect group membership is too strong an assumption since the correct classification of those objects is our original goal.

However, we highlight that there is a trade-off here; To our knowledge, our work is the first one to achieve guarantees for the \emph{more challenging objective of robust learning without having access to the group membership of examples} but at the cost of achieving a weaker upper bound on the robust loss incurred on each group compared to the previous work on multi-group PAC learning. A detailed comparison is given in~\prettyref{sec:comparison-prior-work-multi-group-learning}.


 







\section{Setup and Notation} 
Let $\calX$ denote the instance space and $\calY$ denote the label space. Our main objective is to be robust against adversarial patches $\calA:\calX\to 2^\calX$, where $\calA(x)$ represents the (potentially infinite) set of adversarially patched images that an adversary might attack with at test-time on input $x$. \citet{xiang2022patchcleanser} showed that even though the space of adversarial patches $\calA(x)$ can be exponential or infinite, one can consider a ``covering'' %
function $\calU:\calX\to 2^\calX$ of masking operations on images where $\abs{\calU(x)}$ shows the covering set on input image $x$ and is polynomial in the image size. Thus, for the remainder of the paper, we focus on the task of learning a predictor robust to a perturbation set $\calU:\calX \to 2^{\calX}$, where %
$\calU(x)$is the set of allowed masking operations that can be performed on $x$. We assume that $\calU(x)$ is finite where $\abs{\calU(x)} \leq k$. 

We observe $m$ iid samples $S\sim \calD^m$ from an unknown distribution $\calD$, and our goal is to learn a predictor $\hat{h}$ achieving small robust risk:
\begin{equation}
\label{eqn:robrisk}
\Ex_{(x,y)\sim \calD} \insquare{ \max_{z\in \calU(x)} \ind[\hat{h}(z)\neq y]}.
\end{equation}

Let $\calH\subseteq \calY^\calX$ be a hypothesis class, and denote by $\vc(\calH)$ its VC dimension. Let $\ERM_\calH$ be an $\ERM$ oracle for $\calH$ that returns a hypothesis $h\in \calH$ that minimizes empirical loss. For any set arbitrary set $W$, denote by $\Delta(W)$ the set of distributions over $W$. 

In~\prettyref{sec:non-realizable-oracle}, we focus on a single-group setting where the benchmark $\OPT_{\calH}$ is defined as follows:
\begin{equation}
\label{eqn:opt}
    \OPT_{\calH} \triangleq \min_{h\in \calH} \Ex_{(x,y)\sim \calD} \max_{z \in \calU(x)} \ind\insquare{h(z)\neq y}. 
\end{equation}


In~\prettyref{sec:multi-robustness}, we consider a multi-group setting, where the instance space $\mathcal{X}$ is partitioned into a set of $g$ groups $\mathcal{G}=\{G_1,\dots,G_g\}$. These groups solely depend on the features $x$ and not the labels. The goal is to learn a predictor that has low robust loss on all the groups simultaneously. In this setup, the benchmark $\OPTDMax$ is as follows:
\begin{align}
\label{optmax}
   \OPTDMax =  \min_{h\in\calH} \max_{j\in[g]} \Ex_{(x,y)\sim D}\insquare{\max_{z\in \calU(x)} \ind[h(z)\neq y] \big| x\in G_j}
    \small
\end{align}
