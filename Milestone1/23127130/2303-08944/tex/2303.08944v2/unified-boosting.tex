\section{Multi-robustness guarantees on a set of groups}
\label{sec:multi-robustness}
In this section, we propose a boosting algorithm that learns a predictor with a low robust loss on a collection of subgroups simultaneously. 
First, we consider the case of disjoint groups and present our training-time algorithm for this case in~\prettyref{sec:multi-robust}. 
~\prettyref{sec:generalization-guarantees} provides generalization guarantees. In~\prettyref{sec:overlapping-groups-reduction}, we show a reduction from overlapping groups to disjoint groups. In the following, first we formalize the notions of robust loss on a specific group and multi-robustness. 



When the training dataset $S$ is partitioned into $g$ groups $\calG=\{G_1,\dots,G_g\}$,
the empirical robust loss of a predictor $h$ on group $G_j$ is defined as follows:
\begin{align}
&\RLoss_j(h)=\frac{1}{|G_j|}\sum_{(x,y)\in G_j}\max_{z\in\mathcal{U}(x)}\ind[h(z)\neq y]\label{eqn:unweighted-robust-loss}
\end{align}


The learning benchmark that we compete with on a dataset $S$ for the robust loss on each group is $\OPT^{S}_{\max}$ that is defined as follows:
\begin{align}
&\OPT^{S}_{\max}=\min_{h\in \mathcal{H}}\max_{j\in[g]}\frac{1}{|G_j|}\sum_{(x,y)\in G_j}\max_{z\in\mathcal{U}(x)}\ind[h(z)\neq y]
\label{defn: optmax}
\end{align}



\begin{defn}[Multi-Robustness]
\label{defn:multirob}
A hypothesis $h$ is multi-robust on a dataset $S$ if it achieves the following guarantee:
\begin{align*}
&\max_{j\in[g]}\frac{1}{|G_j|}\sum_{(x,y)\in G_j}\max_{z\in\mathcal{U}(x)}\ind[h(z)\neq y]\leq \OPT^S_{\max}+\eps 
\end{align*}
\label{def:multi-robustness}
\end{defn}

\begin{defn}[$\beta$-Multi-Robustness]
A hypothesis $h$ is $\beta$-multi-robust on a dataset $S$ if it achieves the following guarantee:
\begin{align*}
&\max_{j\in[g]}\frac{1}{|G_j|}\sum_{(x,y)\in G_j}\max_{z\in\mathcal{U}(x)}\ind[h(z)\neq y]\leq \beta(\OPT^{S}_{\max}+\eps) \label{eq:beta-multi-robustness}
\end{align*}
\label{defn:beta-multi-robustness}
\end{defn}

\begin{defn}[Multi-Robustness on Average]
A set of hypotheses $\mathcal{H'}=\{h_1,\dots,h_T\}$ is multi-robust on a dataset $S$ on average if the the following property holds:
\[\frac{1}{T}\max_{j\in [g]}\sum_{t=1}^T\RLoss_j(h_t)\leq \OPT^{S}_{\max}+\eps\]
\label{def:avg-multi-robustness}
\end{defn}

\begin{rem}
~\prettyref{def:multi-robustness} is a stronger notion of multi-robustness compared to~\prettyref{def:avg-multi-robustness}.
\end{rem}

\paragraph{Summary of Results.}
\prettyref{sec:multi-robust} investigates the case of disjoint groups and proposes a two-layer boosting algorithm (\prettyref{alg:boosting}) that achieves multi-robustness on the training dataset $S$. First, we show that $\calH'=\{h_1,\dots,h_T\}$ returned by~\prettyref{alg:boosting} is multi-robust on average (\prettyref{thm:randomized-multi-robustness}).~\prettyref{thm:deterministic-multi-robustness} exhibits that the majority-vote classifier over $\calH'$, i.e. $\MAJ(h_1,\dots,h_T)$, obtains $\beta$-multi-robustness for $\beta=2$. We remark that although~\prettyref{thm:randomized-multi-robustness} achieves a tighter upper bound on the multi-robustness guarantee,~\prettyref{thm:deterministic-multi-robustness} gives a guarantee for the stronger notion of multi-robustness.~\prettyref{sec:generalization-guarantees} provides generalization guarantees for both notions of average multi-robustness and $\beta$-multi-robustness. In~\prettyref{sec:overlapping-groups-reduction}, we show a reduction from overlapping groups to disjoint groups.

\subsection{Comparison to Prior Work on Multi-group Learning}
\label{sec:comparison-prior-work-multi-group-learning}
\citet{multiagnostic} study agnostic multi-group \emph{PAC learning} and guarantee that for each group $G_j$ in a collection of groups $\calG$:
\[\Ex\insquare{\ell(h(x),y)|x\in G_j}\leq \min_{h_{G_j}\in\calH}\Ex\insquare{\ell(h_{G_j}(x),y)|x\in G_j}\]
That is, the hypothesis $h$ must compete against a hypothesis $h_{G_j}\in \calH$
 trained specifically to minimize the error over the group $G_j\in\calG$, for every group in the collection. \emph{However, their results do not extend to the case of robust loss.} In contrast, in our notion of multi-robustness loss that holds for the \emph{more challenging objective of robust learning}, our benchmark is weaker (~\prettyref{def:multi-robustness}). 
We leave it as an open question to study whether our upper bounds for the robust loss over a collection of groups can be strengthened.

\subsection{Boosting algorithm achieving multi-robustness guarantees:}
\label{sec:multi-robust} 


In this section, we present~\prettyref{alg:boosting} that obtains %
multi-robustness guarantees on a set of disjoint groups. The algorithm follows the idea proposed by~\citet{freund1996game} that obtains boosting by playing a repeated game. Initially a sample set $S=\{(x_1,y_1),\dots,(x_m,y_m)\}$ partitioned into a set of disjoint groups $\mathcal{G}=\{G_1,\dots,G_{g}\}$ is received as input. %
$P_j^t$ shows the normalized weight of group $G_j$ in step $t$. Initially, for each group $G_j$, $P^t_j=1/g$.
In each round $t$, the weight of each group gets split between its examples equally: $p_i = P^t_j/|G_j|$ where $(x_i,y_i)\in G_j$. Subsequently, an oracle call is made to{~\prettyref{alg:FMS}} with sample weights $p_1,\dots,p_m$. %
~\prettyref{lem:avg-robust-loss-upper-bound} shows at each iteration $t$,{~\prettyref{alg:FMS}} returns a hypothesis $h_t$ such that its average robust loss across the groups is at most $\OPT^{S}_{\max}+\eps$. In the next iteration $t+1$, for each group $G_j$, the weights of examples in $G_j$ get decreased by a multiplicative factor of 
$1-\delta m_j^{\text{rob}}(h_t)$ where $m_j^{\text{rob}}(h_t)=1-\RLoss_j(h_t)$ and $\delta=\sqrt{{\ln g}/{T}}$.~\prettyref{thm:randomized-multi-robustness} exhibits that after %
$T=\calO({\ln g}/{\eps^2})$ rounds,~\prettyref{alg:boosting} outputs
a set of hypotheses $\mathcal{H'}=\{h_1,\dots,h_T\}$ such that for each group $G_j$ the average multi-robustness guarantee is obtained, i.e., $\frac{1}{T}\sum_{t=1}^T \RLoss_j(h_t)\leq \OPT^S_{\max}+\eps$.~\prettyref{thm:deterministic-multi-robustness} provides that $\MAJ(h_1,\dots,h_t)$ achieves
$\beta$-multi-robustness guarantee for $\beta=2$.

\begin{algorithm}[H]
\caption{Boosting Algorithm Achieving Multi-Robustness}
\label{alg:boosting}
\begin{algorithmic}
    \INPUT training dataset $S=\{(x_1,y_1),\dots,(x_m,y_m)\}$ partitioned into a set of groups $\{G_1,\cdots,G_g\}$\;
    
    Initially, $\forall 1\leq j\leq g: P_j^t = 1/g$\;
    
    \FOR{$t=1,\dots,T$}
    \STATE $p_i = P^t_j/|G_j|$ where $(x_i,y_i)\in G_j$\;
    
    \STATE 
    Call{~\prettyref{alg:FMS}} on $S$ with weights $(p_1,\dots,p_m)$ for $T'=\frac{36\ln k}{\eps^2}$ rounds.\;
    
    \STATE 
    Update $P^t_j,  \text{ for all }j\in[g]$:\;
    
    \STATE 
    \[ P^{t+1}_j=\frac{P^t_j\cdot\left(1-\delta m_j^{\text{rob}}(h_t)\right)}{Z_t}\]
    where $m_j^{\text{rob}}(h_t)=1-\RLoss_j(h_t)$, $Z_t$ is a normalization factor, and     $\delta=\sqrt{\frac{\ln g}{T}}$.\;
    \ENDFOR
    \OUTPUT 
    $\mathcal{H'}=\{h_1,\cdots,h_T\}$
\end{algorithmic}
\end{algorithm}

\begin{rem}
We remark that the output of~\prettyref{alg:boosting} is a set of majority-vote classifiers over $\calH$:
\begin{align*}
&\calH'=\Big\{\MAJ(h_{1,1},\dots, h_{1,T'}),\dots,\MAJ(h_{T,1},\dots, h_{T,T'}):\forall i\in[T], \forall j\in[T'], h_{i,j}\in \calH\Big\}
\end{align*}
\end{rem}

Before proving the multi-robustness guarantees, we show that~\prettyref{lem:avg-robust-loss-upper-bound} holds. {In order to prove that~\prettyref{lem:avg-robust-loss-upper-bound} holds, first we show in ~\prettyref{lem:extension-FMS-weights} that an extension of~\prettyref{lem:FMS} holds when $p_1,\cdots,p_m$ are arbitrary weights such that $\sum_{i=1}^m p_i = 1$.} Next, we restate the guarantee of the Multiplicative Weights algorithm that is a generalization of \emph{Weighted Majority} algorithm~\citep{littlestone1994weighted} and is equivalent to \emph{Hedge} developed by~\citet{freund1997decision}.

\begin{lem} [Extension to general weights]
For any dataset $S =\{(x_1,y_1),\dots,(x_m,y_m)\}\in (\calX\times\calY)^m$ and any corresponding weights $p_1,\dots, p_m > 0$ such that $\sum_{i=1}^{m} p_i = 1$, running
{\prettyref{alg:FMS}}
for $T$ rounds produces a mixed-strategy $\hat{Q} = \frac{1}{T} \sum_{t=1}^{T} h_t \in \Delta(\calH)$ satisfying:
\begin{align*}
   &\max_{\substack{P_1\in \Delta(\calU(x_1)),\\ \dots,\\P_m\in \Delta(\calU(x_m))}} \sum_{i=1}^{m} p_i\cdot \Ex_{z_i\sim P_i} \frac{1}{T} \sum_{t=1}^{T} \ind\insquare{h_t(z_i)\neq y_i} \\
   &\leq
    \min_{Q\in \Delta(\calH)} \max_{\substack{P_{1}\in \Delta(\calU(x_1)),\\ \dots, \\P_{m} \in \Delta(\calU(x_m))}} \sum_{i=1}^{m} p_i\cdot \Ex_{z_i\sim P_i } \Ex_{h\sim Q} \ind\insquare{h(z_i)\neq y_i} 
    + 2\sqrt{\frac{\ln k}{T}}
\end{align*}
\label{lem:extension-FMS-weights}
\end{lem}
\begin{lem}
In each round $t$ of~\prettyref{alg:boosting}, by making an oracle-call to{~\prettyref{alg:FMS}} after $T'=\frac{4\ln k}{\eps^2}$ rounds, %
a hypothesis $h_t$ is outputted such that $\E_{j\sim P^t}[\ell^{rob}_j(h_t)]=\sum_{j\in[g]}P_j^{t}\ell_j^{rob}(h_t)\leq \OPT^S_{\max}+\eps$.
\label{lem:avg-robust-loss-upper-bound}
\end{lem}

\begin{thm}[Mutiplicative Weights Algorithm \citep{kale2007efficient}]
\label{thm:MW_alg}
For any sequence of costs of experts $\vec{m}_1,\cdots,\vec{m}_T$ revealed by nature where all the costs are in $[0,1]$, the sequence of mixed strategies $\vec{p}_1,\cdots,\vec{p}_T$ produced by the Multiplicative Weights algorithm satisfies:
\[\sum_{t=1}^T \vec{m}_t\cdot \vec{p}_t\leq (1+\delta)\min_{\vec{p}}\sum_{t=1}^T\vec{m}_t\cdot \vec{p}+\frac{\ln n}{\delta}\]
where $n$ is the number of experts.
\end{thm}

\begin{thm}
\label{thm:avg-empirical-boosting}
When $T=\calO(\frac{\ln g}{\eps^2})$,~\prettyref{alg:boosting} computes a set of hypotheses $\mathcal{H}'=\{h_1,\cdots,h_T\}$, such that for each group $G_j$, 
$\frac{1}{T}\sum_{t=1}^T\RLoss_j(h_t)\leq \OPT^S_{\max}+\eps$.
\label{thm:randomized-multi-robustness}
\end{thm}
\begin{proof}

In each iteration $t$, we define average loss and reward terms as follows:

\[L(P^t,h_t)=\E_{j\sim P_t}\Big[\RLoss_j(h_t)\Big]=\sum_{j\in[g]}P^t_j\RLoss_j(h_t),\] 
\[M(P^t,h_t)=\E_{j\sim P_t}\Big[m_j^{\text{rob}}(h_t)\Big]\]
Substituting $\RLoss_j(h_t)=1-m_j^{\text{rob}}(h_t)$ provides:
\begin{align*}
&M(P^t,h_t)=\sum_{j\in[g]}P^t_j(1-\RLoss_j(h_t))=1-\sum_{j\in[g]}P^t_j\RLoss_j(h_t)\\
&=1-L(P^t,h_t)
\end{align*}
Now by setting $T=\frac{9\ln g}{\eps^2}$ which implies that $\delta=\sqrt{\frac{\ln g}{T}}=\frac{\eps}{3}$, and by using the guarantee of~\prettyref{thm:MW_alg}, the following bound is obtained.
\begin{align*}
&\frac{1}{T}\sum_{t=1}^T M(P^t,h_t)\leq \frac{(1+\delta)}{T}\min_{j\in[g]}\sum_{t=1}^T M(j,h_t)+\frac{\ln g}{\delta T}\\
&\rightarrow \frac{1}{T}\sum_{t=1}^T M(P^t,h_t)\leq \frac{1}{T}\min_{j\in[g]}\sum_{t=1}^T M(j,h_t)+\delta+\frac{\ln g}{\delta T}\\
&\rightarrow \frac{1}{T}\sum_{t=1}^T M(P^t,h_t)\leq \frac{1}{T}\min_{j\in[g]}\sum_{t=1}^T M(j,h_t)+\frac{2\eps}{3}
\end{align*}
where $M(j,h_t)$ is the reward term when the whole probability mass is concentrated on group $G_j$. Therefore for each group $j\in[g]$:
\begin{align}
&\frac{1}{T}\sum_{t=1}^T M(j,h_t)\geq \frac{1}{T}\sum_{t=1}^T M(P^t,h_t)-\frac{2\eps}{3}
\label{eqn:randomized-boosting-eq1}
\end{align}

\prettyref{lem:avg-robust-loss-upper-bound} provides that in each iteration $t$, $L(P^t,h_t)\leq \OPTSMax+\eps/3$ given that\sareplace{~\prettyref{alg:weighted-FMS}}{~\prettyref{alg:FMS}} is 
executed for $T'=\frac{36\ln k}{\eps^2}$ rounds.
Thus, at each iteration $t$, $M(P^t,h_t)\geq 1-(\OPTSMax+\eps/3)$. Therefore, $\frac{1}{T}\sum_{t=1}^T M(P^t,h_t)\geq 1-(\OPTSMax+\eps/3)$; combining with~\prettyref{eqn:randomized-boosting-eq1} implies that:
\begin{align*}
&\frac{1}{T}\sum_{t=1}^T M(j,h_t)\geq \frac{1}{T}\sum_{t=1}^T M(P^t,h_t)-\frac{2\eps}{3}\\
&\geq 1-(\OPT^S_{\max}+\frac{\eps}{3})-\frac{2\eps}{3}=1-(\OPT^S_{\max}+\eps)
\end{align*}
Plugging in the definition of $L(P^t,h_t)$ implies that:
\[\frac{1}{T}\sum_{t=1}^T L(j,h_t)\leq \OPT^S_{\max}+\eps\]
Which concludes the proof.
\end{proof}

\begin{cor}
~\prettyref{thm:randomized-multi-robustness} implies that if for each example a predictor is picked uniformly at random from $\calH'$ to predict its label, then for each group $G_j\in \calG$, the expected robust loss is at most $\OPT^S_{\max}+\eps$.
\label{cor:interpret-avg-loss}
\end{cor}



\begin{thm}
When $T=\calO(\frac{\ln g}{\eps^2})$,~\prettyref{alg:boosting} computes a set of hypotheses $\calH'=\{h_1,\dots,h_T\}$ such that for each group $G_j$, $\RLoss_j(\MAJ(h_1,\cdots,h_T))\leq 2(\OPT^S_{\max}+\eps)$.
\label{thm:deterministic-multi-robustness}
\end{thm}

\begin{proof}
By~\prettyref{thm:randomized-multi-robustness}, after $T=\calO(\frac{\ln g}{\eps^2})$ rounds, for each group $G_j$, $\frac{1}{T}\sum_{t=1}^T \RLoss_j(h_t)\leq \OPT^S_{\max}+\eps$. Therefore, %
the total number of robustness mistakes on $G_j$ across all the classifiers %
$h_1,\cdots,h_T$ is at most $T(\OPT^S_{\max}+\eps)|G_j|$ which is equal to $T/2\cdot 2(\OPT^S_{\max}+\eps)|G_j|$.

Therefore, the fraction of examples in $G_j$ that at least $T/2$ of the classifiers in $h_1,\cdots h_T$ make a robustness mistake on is at most $2(\OPT^S_{\max}+\eps)$. %
Hence, the fraction of examples in $G_j$ that are not robustly classified by the majority-vote classifier is at most $2(\OPT^S_{\max}+\eps)$.%
\end{proof}

\subsection{Generalization Guarantees}
\label{sec:generalization-guarantees}
In this section, we derive generalization guarantees for multi-robustness. First,~\prettyref{lem:vc-robustloss-groups} shows how to bound the VC-Dimension of the intersection of robust loss and groups. We can then invoke this Lemma to get uniform convergence guarantees that will allow us to get concentration for the conditional robust loss across groups (see \prettyref{def:multi-robustness}).

\begin{lem} [VC Dimension of Intersection of Robust Loss and Groups]
\label{lem:vc-robustloss-groups}
For any class $\calH$, any perturbation set $\calU$, and any group class $\calG$, denote the intersection function class by
\begin{align*}
&\calF^\calU_{\calH,\calG} \triangleq \{ (x,y)\mapsto \max_{z\in\calU(x)} \ind\insquare{h(z)\neq y} \wedge \ind[x\in G_j]:\\ 
&h\in\calH, G_j\in\calG \}
\end{align*}
Then, it holds that $\vc(\calF^\calU_{\calH,\calG}) \leq \Tilde{O}\inparen{\vc(\calL^{\calU}_{\calH}) + \vc(\calG)}$.
\end{lem}



\begin{thm}[Generalization guarantees for average multi-robustness]
\label{thm:generalization-multi-groups}
With $T=\calO(\ln g/\varepsilon^2)$ and $m= \Tilde{O}\inparen{\frac{\vc(\calH)\ln^2(k)}{\eps^4}+\frac{\vc(\calG) + \ln(1/\delta)}{\varepsilon^2}}$,
~\prettyref{alg:boosting} computes a set of hypotheses $\calH'=\{h_1,\dots, h_T\}$, such that $\forall G_j\in \calG$, 
\small{\begin{align*}
&\frac{1}{T}\sum_{t=1}^T\Prob_{(x,y)\in \calD}\Big[\exists z\in \calU(x): h_t(z)\neq y \mid x\in G_j \Big]\\
&\leq\inparen{1 + \frac{\varepsilon}{\Prob_{\calD}(x\in G_j)}}\inparen{\OPTSMax + \varepsilon}+\frac{\varepsilon}{\Prob_{\calD}(x\in G_j)}
\end{align*}}
\end{thm}


\begin{thm}[Generalization guarantees for $\beta$-multi-robustness]
\label{thm:generalization-multi-groups-deterministic}
With $T=\calO(\ln g/\varepsilon^2)$, \\$m=\Tilde{O}\inparen{\frac{\vc(\calH)\ln(g)\ln^2(k)}{\varepsilon^6}+\frac{\vc(\calG) + \ln(1/\delta)}{\varepsilon^2}}$, and $\beta=2$,
~\prettyref{alg:boosting} computes a set of hypotheses $\calH'=\{h_1,\dots, h_T\}$, such that $\forall G_j\in \calG$, 
\small{\begin{align*}
&\Prob_{(x,y)\in \calD}\Big[\exists z\in \calU(x): \MAJ(h_1,\dots,h_T)(z)\neq y \mid x\in G_j \Big]\\
&\leq\inparen{1 + \frac{\varepsilon}{\Prob_{\calD}(x\in G_j)}}\inparen{\beta(\OPTSMax + \varepsilon)}+\frac{\varepsilon}{\Prob_{\calD}(x\in G_j)}
\end{align*}}
\end{thm}

\begin{rem}
In~\prettyref{sec:proof-generalization-guarantees-deterministic}, we show how to achieve generalization guarantees in terms of $\OPTDMax$ instead of $\OPTSMax$.
\end{rem}

\section{Conclusion}
We exhibited an example showing how %
using $\ERM$ on an augmented dataset to learn a robust classifier can fail when the examples are robustly un-realizable. Next, we provided a ``boosting-style'' algorithm that uses $\ERM$ and obtains strong robust learning guarantees in the non-realizable regime. 
This work provides theoretical evidence that our existing methods of learning accurate classifiers i.e. \ERM, can be modified effectively to learn robust classifiers even in the agnostic robust regime. Next, we introduced a new multi-robustness objective to obtain robustness guarantees simultaneously across a collection of subgroups. We showed this objective can be achieved by adding a second layer of boosting to the first algorithm. %

Adversarial examples exist for many types of classifiers but are especially salient with modern neural-based vision methods. 
However, due to the large capacity of these networks, it is not clear that they would benefit from boosting. Therefore, the fact that our algorithms rely on boosting should not be interpreted as a firm recommendation 
to use boosting with neural networks, but instead as a theoretical proof-of-concept that plain ERM can be used to learn robust models, given the right algorithmic scheme, especially if such a scheme can reduce the effective number of perturbations available to the adversary.



