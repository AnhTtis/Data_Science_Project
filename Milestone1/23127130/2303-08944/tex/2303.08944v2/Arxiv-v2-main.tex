\documentclass[11pt,letterpaper]{article}
\usepackage[margin=1in]{geometry}
\input{preamble}
\input{macros}
\usepackage{float}
\usepackage{comment}
\usepackage[suppress]{color-edits}
\addauthor{sa}{teal}
\addauthor{kms}{blue}
\addauthor{ab}{purple}
\addauthor{om}{red}
\begin{document}
\title{Agnostic Multi-Robust Learning Using ERM\footnote{Authors are ordered alphabetically.}}
\author{Saba Ahmadi$^\dagger$}
\author{Avrim Blum$^\dagger$}
\author{Omar Montasser$^\ddagger$} 
\author{Kevin Stangl$^\dagger$}


\affil{$^\dagger$Toyota Technological Institute at Chicago\\
$^\ddagger$University of California, Berkeley\\
{\small\texttt{\{saba,avrim,omar,kevin\}@ttic.edu}}}

\maketitle



\begin{abstract}
A fundamental problem in robust learning is asymmetry: a learner needs to correctly classify every one of exponentially-many perturbations that an adversary might make to a test-time natural example. In contrast, the attacker only needs to find one successful perturbation. ~\citet{xiang2022patchcleanser} proposed an algorithm that in the context of patch attacks for image classification, reduces the effective number of perturbations from an exponential to a polynomial number of perturbations and learns using an ERM oracle. However, to achieve its guarantee, their algorithm requires the natural examples to be robustly realizable. 
This prompts the natural question; can we extend their approach to the non-robustly-realizable case where there is no classifier with zero robust error?

Our first contribution is to answer this question affirmatively by reducing this problem to a setting in which an algorithm proposed by ~\citet{DBLP:conf/colt/FeigeMS15} can be applied, and in the process extend their guarantees. Next, we extend our results to a multi-group setting and introduce a novel agnostic multi-robust learning problem where the goal is to learn a predictor that achieves low robust loss on a (potentially) rich collection of subgroups.
\end{abstract}

\input{introduction}
\input{single-population}
\input{unified-boosting}
\subsection*{Acknowledgements}
This work was supported in part by the National Science Foundation under grants CCF-2212968 and ECCS-2216899, by the Simons Foundation under the Simons Collaboration on the Theory of Algorithmic Fairness, and by the Defense Advanced Research Projects Agency under cooperative agreement HR00112020003. The views expressed in this work do not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. Approved for public release; distribution is unlimited. This work was done when OM was a PhD student at the Toyota Technological Institute at Chicago.
\bibliography{ref}
\bibliographystyle{plainnat}
\clearpage
\appendix
\input{appendix}

\end{document}
