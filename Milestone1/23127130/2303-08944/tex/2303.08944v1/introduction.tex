\section{Introduction} 

Patch attacks \citep{brown2017adversarial,karmon2018lavan,yang2020patchattack} are an important threat model in the general field of test-time evasion attacks \citep{goodfellow2014explaining}. 
In a patch attack, the adversary replaces a contiguous block of pixels with an adversarially crafted pattern. Patch attacks can realize physical world attacks to computer vision systems by printing and attaching a patch to an object.
To secure the performance of computer vision systems against patch-attacks, there has been an active line of research for providing certifiable robustness guarantees against them \citep[see e.g.,][]{minorityreport, patchguard, patchguard++, bagcert, clippedbag,chiang2020}.

\citet{xiang2022patchcleanser} recently proposed a state-of-the-art algorithm called Patch-Cleanser that can provably defend against patch attacks. They use a \emph{double-masking} approach based on zero-ing out  two different contiguous blocks of an input image, hopefully to remove the adversarial patch. For each one-masked image, if for all possible locations of the second mask, the prediction model outputs the same classification, it means that the first mask removed the adversarial patch, and the agreed-upon prediction is correct. Any disagreements in these predictions imply that the mask was not covered by the first patch. 

Crucially, the Patch-Cleanser algorithm relies on a {\em two-mask correctness} assumption of the prediction model that is defined as follows: for a given input $(x,y)$, if for any pair of masks applied to $x$, a prediction model $F$ outputs the correct prediction $y$, then $F$ has two-correctness property on $(x,y)$ \citep[see Definition 2 in][]{xiang2022patchcleanser}. In order to train a model with the two-mask correctness property, they augment the training set with pairs of masks at random locations of training images, and perform \emph{empirical risk minimization} ($\ERM$) on the augmented dataset.

However, when no predictor is perfectly correct on {\em all} two-mask operations on all images, which we refer to as the \textit{non-realizable} setting, we exhibit an example where plain $\ERM$ on the augmented dataset fails (See \prettyref{exmpl:ERM-failure} and \prettyref{fig:example}). At a high-level, the main issue is that plain $\ERM$ on the augmented dataset treats all mistakes equally and so this could lead to learning a predictor with very high robust loss, i.e. on {\em many} training examples, there is a two-mask operation where the predictor makes a mistake, as opposed to learning a predictor with low robust loss, i.e. there are a \emph{few} training examples where the predictor makes mistakes on many of the two-mask operations. This motivates studying  the following general question:


\begin{center}
\textit{Can we use $\ERM$ to learn predictors robust against adversarial \\patches in the non-realizable setting?}
\end{center}

Next, we generalize this problem to a setting with multiple groups where the goal is to use an $\ERM$ oracle and learn a predictor that achieves low robust loss on all the groups simultaneously. Here, using plain $\ERM$ can fail by \emph{concentrating} the overall robust loss on a \emph{few} groups, instead of \emph{spreading} the loss across \emph{many} groups. Thus, we also investigate the following question:

\begin{center}
%\textit{Can we use $\ERM$ to learn predictors robust against adversarial patches in the non-realizable setting in a \\multiple groups setting that have a low robust loss on all the groups simultaneously?}
\textit{Can we extend our results to a multiple groups setting where the goal is to learn \\a predictor that has low robust loss on all the groups simultaneously?}
\end{center}


\paragraph{Our Contributions} As mentioned before, we first show that performing plain $\ERM$ on an augmented training set can fail. In~\prettyref{sec:non-realizable-oracle}, we present a ``boosting-style''  algorithm that uses $\ERM$ %in a different way 
and obtains strong robust learning guarantees. %This is presented in Section \ref{sec:non-realizable-oracle}. 
In Section \ref{sec:multi-robustness}, we consider a %more general 
generalized setting with multiple groups where the goal is to learn a predictor using an $\ERM$ oracle that simultaneously has a low robust loss on all groups. Building on our algorithm in \prettyref{sec:non-realizable-oracle}, we show that we can run an additional layer of boosting with respect to groups to achieve a multi-robustness guarantee across groups. To our knowledge, this paper is the first that provides a `multi' guarantee in an adversarial robustness problem. 

\subsection{Other Related Work}

Interestingly, the notion of multi-robustness has connections with a thriving area of work in algorithmic fairness centered on the notion of multi-calibration~\cite{multicalib, multi-accuracy-Kim-etal,multiagnostic,multiagstrat, bugbounty}.
Generally speaking, these notions require a fairness or accuracy constraint hold on a large collection of sub-groups. Our approach has high-level connections to these algorithms as all of these approaches are based on boosting, but our multi-robustness framework has two layers of boosting to ensure `emphasis' on specific groups and also manage the adversarial perturbations.

\citet{multiagstrat,bugbounty}, in particular, learn a predictor in the form of a decision list where internal nodes are associated with binary functions specifying whether an example belongs to a group or not. However, in contrast to their approach, our algorithm learns a predictor that does not use group membership of examples during the test time.
This is important since features including gender and ethnicity are protected and in some scenarios, it would be better not to incorporate them in the decision model. 
We also interpret some of the groups in our setting as objects to be classified like a stop-sign group or fire-hydrant group, (and we want low robust loss on both)
thus an approach that needs to detect group membership is too strong an assumption since the correct classification of those objects is our original goal. In contrast, \citet{multiagnostic} show how to learn a unified predictor that does not use the group-membership of examples during the test time for a group of loss functions; however, their notion of loss does not capture the robust loss that we consider in this work.

\section{Setup and Notation} Let $\calX$ denote the instance space and $\calY$ denote the label space. Our main objective is to be robust against adversarial patches $\calA:\calX\to 2^\calX$, where $\calA(x)$ represents the (potentially infinite) set of adversarially patched images that an adversary might attack with at test-time. \citet{xiang2022patchcleanser} showed that even though the space of adversarial patches $\calA$ can be exponential or infinite, one can consider a ``covering'' set $\calU:\calX\to 2^\calX$ of masking operations on images where $\abs{\calU(x)}$ is polynomial in the image size.

Thus for the remainder of the paper, we focus on the task of learning a predictor robust to a perturbation set $\calU:\calX \to 2^{\calX}$, where $\calU(x)\subseteq \calX$ is the set of allowed masking operations that can be performed on $x$. We assume that $\calU(x)$ is finite where $\abs{\calU(x)} \leq k$. Let $\calH\subseteq \calY^\calX$ be a hypothesis class, and denote by $\vc(\calH)$ its VC dimension. Let $\ERM_\calH$ be an $\ERM$ oracle for $\calH$. For any set arbitrary set $W$, denote by $\Delta(W)$ the set of distributions over $W$. 

In~\prettyref{sec:non-realizable-oracle}, we focus on a single-group setting where the benchmark $\OPT_{\calH}$ is defined as follows:
\begin{equation}
\label{eqn:opt}
    \OPT_{\calH} \triangleq \min_{h\in \calH} \Ex_{(x,y)\sim \calD} \max_{z \in \calU(x)} \ind\insquare{h(z)\neq y}. 
\end{equation}


In~\prettyref{sec:multi-robustness}, we consider a multi-group setting, where the instance space $\mathcal{X}$ is partitioned into a set of $g$ groups $\mathcal{G}=\{G_1,\dots,G_g\}$, and the goal is to learn a predictor that has low robust loss on all the groups simultaneously. In this setup, the benchmark $\OPTDMax$ is as follows:
\begin{align}
\label{optmax}
   \OPTDMax =  \min_{h\in\calH} \max_{j\in[g]} \Ex_{(x,y)\sim D}\insquare{\max_{z\in \calU(x)} \ind[h(z)\neq y] \big| x\in G_j}
    \small
\end{align}
