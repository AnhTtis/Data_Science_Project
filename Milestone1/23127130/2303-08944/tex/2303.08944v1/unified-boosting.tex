\section{Multi-robustness guarantees on a set of groups}
\label{sec:multi-robustness}
In this section, we consider the problem of learning a predictor that has low robust loss across multiple groups. This objective can be justified from a fairness perspective. For instance, when the groups correspond to sensitive social or demographic groups like gender and race, ensuring reliable performance on each group is crucial. In this setting, if we use \prettyref{alg:FMS} to minimize the overall robust loss, it may result in concentrating the overall robust loss on a few groups, instead of spreading the loss across many groups. We propose a boosting algorithm that learns a predictor with low robust loss on all the groups simultaneously. We present our algorithm for the case of disjoint groups in~\prettyref{sec:multi-robust}. In the case of overlapping groups, we show a reduction to the case of disjoint groups in~\prettyref{sec:overlapping-groups-reduction}.

Suppose that the training dataset $S$ is partitioned into $g$ groups $\calG=\{G_1,\dots,G_g\}$. Robust loss of a predictor $h$ on group $G_j$ is defined as follows:
\begin{align}
&\RLoss_j(h)=\frac{1}{|G_j|}\sum_{(x,y)\in G_j}\max_{z\in\mathcal{U}(x)}\ind[h(z)\neq y]\label{eqn:unweighted-robust-loss}
\end{align}


The learning benchmark that we compete with on a dataset $S$ for the robust loss on each group is $\OPT^{S}_{\max}$ that is defined as follows:
%\begin{defn}
%\label{defn: optmax}
    %\[ 
\begin{align}
&\OPT^{S}_{\max}=\min_{h\in \mathcal{H}}\max_{j\in[g]}\frac{1}{|G_j|}\sum_{(x,y)\in G_j}\max_{z\in\mathcal{U}(x)}\ind[h(z)\neq y]
\label{defn: optmax}
\end{align}
    %\]
%\end{defn}


In the following, we formalize the notion of multi-robustness over a set of groups $\mathcal{G}=\{G_1,\dots,G_g\}$:

\begin{defn}[Multi-Robustness]
\label{defn:multirob}
A hypothesis $h$ is multi-robust on a dataset $S$ if it achieves the following guarantee:
\begin{align*}
&\max_{j\in[g]}\frac{1}{|G_j|}\sum_{(x,y)\in G_j}\max_{z\in\mathcal{U}(x)}\ind[h(z)\neq y]\leq \OPT^S_{\max}+\eps 
%\label{eq:multi-robustness}
\end{align*}
\label{def:multi-robustness}
\end{defn}
A hypothesis $h$ satisfies Definition \ref{defn:multirob} if it is within $\varepsilon$ robust loss of the min-max optimal classifier where the adversary gets to have two maximization options, e.g. maximizing over the worst-off group and for each example $x$ in that group, picking a worst-case perturbation. 

\begin{defn}[$\beta$-Multi-Robustness]
A hypothesis $h$ is $\beta$-multi-robust on a dataset $S$ if it achieves the following guarantee:
\begin{align*}
&\max_{j\in[g]}\frac{1}{|G_j|}\sum_{(x,y)\in G_j}\max_{z\in\mathcal{U}(x)}\ind[h(z)\neq y]\leq \beta(\OPT^{S}_{\max}+\eps) \label{eq:beta-multi-robustness}
\end{align*}
\end{defn}

\begin{defn}[Multi-Robustness on Average]
A set of hypotheses $\mathcal{H'}=\{h_1,\dots,h_T\}$ is multi-robust on a dataset $S$ on average if the the following property holds:
\[\frac{1}{T}\max_{j\in [g]}\sum_{t=1}^T\RLoss_j(h_t)\leq \OPT^{S}_{\max}+\eps\]
\label{def:avg-multi-robustness}
\end{defn}

\begin{rem}
~\prettyref{def:multi-robustness} is a stronger notion of multi-robustness compared to~\prettyref{def:avg-multi-robustness}. 
\end{rem}

In~\prettyref{sec:multi-robust}, we propose~\prettyref{alg:boosting} which is a two-layer boosting algorithm that achieves multi-robustness on the dataset $S$. %~\prettyref{alg:boosting} returns a set of hypotheses $\calH'=\{h_1,\dots,h_T\}$. 
First, we show that $\calH'=\{h_1,\dots,h_T\}$ returned by~\prettyref{alg:boosting} is multi-robust on average (\prettyref{thm:randomized-multi-robustness}).~\prettyref{thm:deterministic-multi-robustness} exhibits that the majority-vote classifier over $\calH'$, i.e. $\MAJ(h_1,\dots,h_T)$, obtains $\beta$-multi-robustness for $\beta<12$. We remark that although~\prettyref{thm:randomized-multi-robustness} achieves a tighter upper bound on the multi-robustness guarantee,~\prettyref{thm:deterministic-multi-robustness} gives a guarantee for the stronger notion of multi-robustness. Our boosting algorithm makes oracle calls to an extension of~\prettyref{alg:FMS} to weighted samples that is proposed in~\prettyref{alg:weighted-FMS}. In~\prettyref{lem:extension-FMS-weights}, we show an extension of~\prettyref{lem:FMS} holds for~\prettyref{alg:weighted-FMS}.

\begin{algorithm}
\caption{Extension of \citet*{DBLP:conf/colt/FeigeMS15} to Weighted Samples}
\label{alg:weighted-FMS}
\begin{algorithmic}[1]
  \INPUT weight update parameter $\eta>0$, training dataset $S=\{(x_1,y_1),\cdots, (x_m,y_m)\}$ and corresponding weights $p_1,\cdots,p_m$.
  \STATE Set $w_1(z, (x,y)) = 1$ for each $(x,y)\in S, z\in \calU(x)$.
  \STATE Set $P^1(z,(x,y)) = \frac{w_1(z,(x,y))}{\sum_{z'\in\calU(x)} w_1(z',(x,y))}$.
\FOR{$t\in 1,\dots,T$}

\STATE Call $\ERM$ on the empirical \emph{weighted distribution}: 
{\small\[h_t = \argmin_{h\in\mathcal{H}} \sum_{(x,y)\in S} \sum_{z\in \calU(x)} p_{(x,y)} P^t(z,(x,y)) \ind\insquare{h_{t}(z)\neq y}\]}%
\FOR{each $(x,y)\in S$ and $z\in\calU(x)$}
\STATE {\small $w_{t+1}(z,(x,y)) = (1+\eta \ind\insquare{h_{t}(z)\neq y}) \cdot w_{t}(z, (x,y))$}%
\STATE $P^{t+1}(z,(x,y))=\frac{w_t(z,(x,y))}{\sum_{z'\in\calU(x)} w_t(z',(x,y))}$.
\ENDFOR
\ENDFOR
\OUTPUT The majority-vote predictor $\MAJ(h_1,\dots, h_T)$. 
\end{algorithmic}
\end{algorithm}

 


\begin{lem} [Extension to weighted samples]
For any dataset $S =\{(x_1,y_1),\dots,(x_m,y_m)\}\in (\calX\times\calY)^m$ and any corresponding weights $p_1,\dots, p_m > 0$ such that $\sum_{i=1}^{m} p_i = 1$, running
%FMS
Algorithm \ref{alg:weighted-FMS}
for $T$ rounds produces a mixed-strategy $\hat{Q} = \frac{1}{T} \sum_{t=1}^{T} h_t \in \Delta(\calH)$ satisfying:
{\small
\begin{align*}
   &\max_{\substack{P_1\in \Delta(\calU(x_1)),\\ \dots,\\P_m\in \Delta(\calU(x_m))}} \sum_{i=1}^{m} p_i\cdot \Ex_{z_i\sim P_i} \frac{1}{T} \sum_{t=1}^{T} \ind\insquare{h_t(z_i)\neq y_i} \leq
    \min_{Q\in \Delta(\calH)} \max_{\substack{P_{1}\in \Delta(\calU(x_1)),\\ \dots, \\P_{m} \in \Delta(\calU(x_m))}} \sum_{i=1}^{m} p_i\cdot \Ex_{z_i\sim P_i } \Ex_{h\sim Q} \ind\insquare{h(z_i)\neq y_i} 
    + 2\sqrt{\frac{\ln k}{T}}
\end{align*}
}%
\label{lem:extension-FMS-weights}
\end{lem}

\subsection{Boosting algorithm achieving multi-robustness guarantees:}
\label{sec:multi-robust} 
In this section, we present~\prettyref{alg:boosting} that obtains %both multi-robustness on average and $\beta$-multi-robustness for $\beta<12$
multi-robustness guarantees. The algorithm follows the idea proposed by~\citet{freund1996game} that obtains boosting by playing a repeated game. Initially a sample set $S=\{(x_1,y_1),\dots,(x_m,y_m)\}$ partitioned into a set of disjoint groups $\mathcal{G}=\{G_1,\dots,G_{g}\}$ is received as input. %First, the weight of each group $G_j$ is set to $1/g$. At each round of boosting, initially, group weights are normalized. 
$P_j^t$ shows the normalized weight of group $G_j$ in step $t$. Initially, for each group $G_j$, $P^t_j=1/g$.
In each round $t$, the weight of each group gets split between its examples equally: $p_i = P^t_j/|G_j|$ where $(x_i,y_i)\in G_j$. Subsequently, an oracle call is made to~\prettyref{alg:weighted-FMS} with sample weights $p_1,\dots,p_m$.~\prettyref{lem:avg-robust-loss-upper-bound} shows that at each iteration $t$,~\prettyref{alg:weighted-FMS} returns a hypothesis $h_t$ such that its average robust loss across the groups is at most $\OPT^{S}_{\max}+\eps$. In the next iteration $t+1$, for each group $G_j$, the weights of examples in $G_j$ get decreased by a multiplicative factor of 
$1-\delta m_j^{\text{rob}}(h_t)$ where $m_j^{\text{rob}}(h_t)=1-\RLoss_j(h_t)$ and $\delta=\sqrt{{\ln g}/{T}}$.~\prettyref{thm:randomized-multi-robustness} exhibits that after %the average multi-robustness guarantee: for 
$T=\calO({\ln g}/{\eps^2})$ rounds,~\prettyref{alg:boosting} outputs
a set of hypotheses $\mathcal{H'}=\{h_1,\dots,h_T\}$ such that for each group $G_j$ the average multi-robustness guarantee is obtained, i.e., $\frac{1}{T}\sum_{t=1}^T \RLoss_j(h_t)\leq \OPT^S_{\max}+\eps$.~\prettyref{thm:deterministic-multi-robustness} provides that $\MAJ(h_1,\dots,h_t)$ achieves
$\beta$-multi-robustness guarantee for $\beta<12$.



\begin{algorithm}[H]
\caption{Boosting Algorithm Achieving Multi-Robustness Guarantee}
\label{alg:boosting}
\begin{algorithmic}
    \INPUT training dataset $S=\{(x_1,y_1),\dots,(x_m,y_m)\}$ partitioned into a set of groups groups $\{G_1,\cdots,G_g\}$
    \STATE Initially, $\forall 1\leq j\leq g: P_j^t = 1/g$
    \FOR{$t=1,\dots,T$}
    \STATE $p_i = P^t_j/|G_j|$ where $(x_i,y_i)\in G_j$
    \STATE Call~\prettyref{alg:weighted-FMS} on $S$ with weights $(p_1,\dots,p_m)$ and get back a predictor $h_t$ such that:
    \STATE \[\E_{j\sim P^t}[\ell^{rob}_j(h_t)]=\sum_{j\in[g]}P_j^{t}\ell_j^{rob}(h_t)\leq \OPT^S_{\max}+\eps\]
    \STATE Update $P^t_j,  \text{ for all }j\in[g]$:
    %\STATE \[ P^{t+1}_j=\frac{P^t_j\cdot\Biggl(1-\delta\Bigl(1-\RLoss_j(h_t)\Bigr)\Biggr)}{Z_t}\]
    \STATE \[ P^{t+1}_j=\frac{P^t_j\cdot\left(1-\delta m_j^{\text{rob}}(h_t)\right)}{Z_t}\]
    where $m_j^{\text{rob}}(h_t)=1-\RLoss_j(h_t)$, $Z_t$ is a normalization factor, and     $\delta=\sqrt{\frac{\ln g}{T}}$.
    \ENDFOR
    \OUTPUT 
    %a predictor $h$ selected uniformly at random from 
    $\mathcal{H'}=\{h_1,\cdots,h_T\}$%, and the majority-vote predictor over $\calH'$: $\MAJ(h_1,\dots,h_T)$
\end{algorithmic}
\end{algorithm}

\begin{rem}
We remark that the output of~\prettyref{alg:boosting} is a set of majority-vote classifiers over $\calH$:
\[\calH'=\Big\{\MAJ(h_{1,1},\dots, h_{1,T'}),\dots,\MAJ(h_{T,1},\dots, h_{T,T'}): \forall i\in[T], \forall j\in[T'], h_{i,j}\in \calH\Big\}\]
\end{rem}

Before proving the multi-robustness guarantees, we show~\prettyref{lem:avg-robust-loss-upper-bound} holds. Next, we restate the guarantee of the Multiplicative Weights algorithm that is a generalization of \emph{Weighted Majority} algorithm~\citep{littlestone1994weighted} and is equivalent to \emph{Hedge} developed by~\citet{freund1997decision}.

\begin{lem}
In each round $t$ of~\prettyref{alg:boosting}, by making an oracle-call to~\prettyref{alg:weighted-FMS} after $T'=\frac{4\ln k}{\eps^2}$ rounds, %we can find 
a hypothesis $h_t$ is outputted such that $\E_{j\sim P^t}[\ell^{rob}_j(h_t)]=\sum_{j\in[g]}P_j^{t}\ell_j^{rob}(h_t)\leq \OPT^S_{\max}+\eps$.
\label{lem:avg-robust-loss-upper-bound}
\end{lem}

\begin{thm}[Mutiplicative Weights Algorithm \citep{kale2007efficient}]
\label{thm:MW_alg}
For any sequence of costs of experts $\vec{m}_1,\cdots,\vec{m}_T$ revealed by nature where all the costs are in $[0,1]$, the sequence of mixed strategies $\vec{p}_1,\cdots,\vec{p}_T$ produced by the Multiplicative Weights algorithm satisfies:
\[\sum_{t=1}^T \vec{m}_t\cdot \vec{p}_t\leq (1+\delta)\min_{\vec{p}}\sum_{t=1}^T\vec{m}_t\cdot \vec{p}+\frac{\ln n}{\delta}\]
where $n$ is the number of experts.
\end{thm}

\begin{thm}
\label{thm:avg-empirical-boosting}
When $T=\calO(\frac{\ln g}{\eps^2})$,~\prettyref{alg:boosting} computes a set of hypotheses $\mathcal{H}'=\{h_1,\cdots,h_T\}$, such that for each group $G_j$, 
$\frac{1}{T}\sum_{t=1}^T\RLoss_j(h_t)\leq \OPT^S_{\max}+\eps$.
\label{thm:randomized-multi-robustness}
\end{thm}
\begin{proof}

In each iteration $t$, we define average loss and reward terms as follows:

\[L(P^t,h_t)=\E_{j\sim P_t}\Big[\RLoss_j(h_t)\Big]=\sum_{j\in[g]}P^t_j\RLoss_j(h_t), \quad M(P^t,h_t)=\E_{j\sim P_t}\Big[m_j^{\text{rob}}(h_t)\Big]\]
Substituting $\RLoss_j(h_t)=1-m_j^{\text{rob}}(h_t)$ provides:
\[M(P^t,h_t)=\sum_{j\in[g]}P^t_j(1-\RLoss_j(h_t))=1-\sum_{j\in[g]}P^t_j\RLoss_j(h_t)=1-L(P^t,h_t)\]
Now by setting $T=\frac{9\ln g}{\eps^2}$ which implies that $\delta=\sqrt{\frac{\ln g}{T}}=\frac{\eps}{3}$, and by using the guarantee of~\prettyref{thm:MW_alg}, the following bound is obtained.
\begin{align*}
&\frac{1}{T}\sum_{t=1}^T M(P^t,h_t)\leq \frac{(1+\delta)}{T}\min_{j\in[g]}\sum_{t=1}^T M(j,h_t)+\frac{\ln g}{\delta T}\\
&\rightarrow \frac{1}{T}\sum_{t=1}^T M(P^t,h_t)\leq \frac{1}{T}\min_{j\in[g]}\sum_{t=1}^T M(j,h_t)+\delta+\frac{\ln g}{\delta T}\\
%&\rightarrow \frac{1}{T}\sum_{t=1}^T M(P^t,h_t)\leq \frac{1}{T}\min_{j\in[g]}\sum_{t=1}^T M(j,h_t)+2\sqrt{\frac{\ln g}{T}}
&\rightarrow \frac{1}{T}\sum_{t=1}^T M(P^t,h_t)\leq \frac{1}{T}\min_{j\in[g]}\sum_{t=1}^T M(j,h_t)+\frac{2\eps}{3}
\end{align*}
where $M(j,h_t)$ is the reward term when the whole probability mass is concentrated on group $G_j$. Therefore for each group $j\in[g]$:
\begin{align}
&\frac{1}{T}\sum_{t=1}^T M(j,h_t)\geq \frac{1}{T}\sum_{t=1}^T M(P^t,h_t)-\frac{2\eps}{3}
\label{eqn:randomized-boosting-eq1}
\end{align}

\prettyref{lem:avg-robust-loss-upper-bound} provides that in each iteration $t$, $L(P^t,h_t)\leq \OPTSMax+\eps/3$ given that~\prettyref{alg:weighted-FMS} is 
executed for $T'=\frac{36\ln k}{\eps^2}$ rounds.
Thus, at each iteration $t$, $M(P^t,h_t)\geq 1-(\OPTSMax+\eps/3)$. Therefore, $\frac{1}{T}\sum_{t=1}^T M(P^t,h_t)\geq 1-(\OPTSMax+\eps/3)$; combining with~\prettyref{eqn:randomized-boosting-eq1} implies that:
\begin{align*}
&\frac{1}{T}\sum_{t=1}^T M(j,h_t)\geq \frac{1}{T}\sum_{t=1}^T M(P^t,h_t)-\frac{2\eps}{3}
\geq 1-(\OPT^S_{\max}+\frac{\eps}{3})-\frac{2\eps}{3}=1-(\OPT^S_{\max}+\eps)
\end{align*}
Plugging in the definition of $L(P^t,h_t)$ implies that:
\[\frac{1}{T}\sum_{t=1}^T L(j,h_t)\leq \OPT^S_{\max}+\eps\]
Which concludes the proof.
\end{proof}

\begin{cor}
~\prettyref{thm:randomized-multi-robustness} implies that if for each example a predictor is picked uniformly at random from $\calH'$ to predict its label, then for each group $G_j\in \calG$, the expected robust loss is at most $\OPT^S_{\max}+\eps$.
\label{cor:interpret-avg-loss}
\end{cor}

\begin{thm}
When $T=\calO(\frac{\ln g}{\eps^2})$,~\prettyref{alg:boosting} computes a set of hypotheses $\calH'=\{h_1,\dots,h_T\}$ such that for each group $G_j$, $\RLoss_j(\MAJ(h_1,\cdots,h_T))\leq 12(\OPT^S_{\max}+\eps)$.
\label{thm:deterministic-multi-robustness}
\end{thm}

\begin{proof}
By~\prettyref{thm:randomized-multi-robustness}, after $T=\calO(\frac{\ln g}{\eps^2})$ rounds, for each group $G_j$, $\frac{1}{T}\sum_{t=1}^T \RLoss_j(h_t)\leq \OPT^S_{\max}+\eps$. Therefore, at most $1/c$ of the classifiers $\mathcal{H'}=\{h_1,\cdots,h_T\}$ have robust loss greater than $c(\OPT^S_{\max}+\eps)$ on $G_j$. Let's call the set of these classifiers $\mathcal{H}''$. The total number of robustness mistakes on $G_j$ across all the classifiers in $\mathcal{H}'\setminus\mathcal{H}''$ is at most $T(1-1/c)c(\OPT^S_{\max}+\eps)|G_j|$ which is equal to:
{\small
\[T(1-1/c)c(\OPT^S_{\max}+\eps)|G_j|=(T/2-T/c).\frac{2c(c-1)}{c-2}(\OPT^S_{\max}+\eps)|G_j|\]
}%
Therefore, the fraction of examples in $G_j$ that at least $T/2-T/c$ of classifiers in $\mathcal{H}'\setminus\mathcal{H}''$ make a robustness mistake on is at most $\frac{2c(c-1)}{c-2}(\OPT^S_{\max}+\eps)$. Let $T_j$ denote the set of these examples. Thus, for each example in $G_j\setminus T_j$, at least $T-T/c-(T/2-T/c-1)=T/2+1$ of the classifiers in $\calH'\setminus \calH''$ are making no robustness mistakes on them, i.e., classifying all their perturbations correctly.
Hence, the fraction of examples in $G_j$ that are not robustly classified by the majority-vote classifier is at most $\frac{2c(c-1)}{c-2}(\OPT^S_{\max}+\eps)$. To find the best value of $c$, we solve the following optimization problem:
\begin{align*}
&\min \frac{2c(c-1)}{c-2}\\
&\text{s.t. } c>2
\end{align*}
Which gives $c\approx 3.41421$, and $\frac{2c(c-1)}{c-2}\approx 11.6569$.
\end{proof}

\subsection{Reduction from overlapping groups to disjoint groups}
\label{sec:overlapping-groups-reduction}
When the groups are overlapping, we reduce it to the case of disjoint groups. The reduction is as follows: for an input instance $\calI(\calG=\{G_1,\dots,G_g\}, S)$ of overlapping groups, create a new instance $\calI'(\calG'=\{G'_1,\dots,G'_g\},S')$ as follows. Initially, for all $G'_j\in \calG'$, $G'_j$ is an empty set. For each example $(x_i,y_i)\in S$ that belongs to a set of groups $\calG_i=\{G_{i,1},\cdots, G_{i,|\calG_i|}\}\subseteq \calG$ in $\calI$, create identical copies of $(x_i,y_i)$ and assign each copy including the original example to exactly one of the groups in $\calG'_i=\{G'_{i,1},\cdots, G'_{i,|\calG'_i|}\}$. Now we have an instance $\calI'$ with disjoint groups. By executing~\prettyref{alg:boosting} on $\calI'$, it returns a predictor $h$ that achieves %$\beta$-multi-robustness guarantee of at most $\beta\cdot \OPT^{\calI'}_{\max}$ on it. 
a $\beta$-multi-robustness guarantee. First, we argue that if $h$ is used on $\calI$, it achieves a multi-robustness guarantee of $\beta\cdot \OPT^{\calI'}_{\max}$. This is the case since either $h$ makes a robustness mistake on all copies of an example or does not make any robustness mistakes on any of them. Next, we show that $\OPT^{\calI'}_{\max}\leq \OPT^{\calI}_{\max}$. Consider a predictor $h^*\in \calH$ that achieves multi-robustness of $\OPT^{\calI}_{\max}$ on $\calI$. If $h^*$ is used on $\calI'$, for each example $(x,y)\in S$ that $h^*$ has zero robust loss on, it does not make any mistakes on any of its copies in $\calI'$. Additionally, if $h^*$ makes a robustness mistake on $(x,y)$, then it makes a robustness mistake on all its copies in $\calI'$. Thus, $h^*$ achieves a multi-robustness guarantee of $\OPT^{\calI}_{\max}$ on $\calI'$. Therefore, $\OPT^{\calI'}_{\max}\leq \OPT^{\calI}_{\max}$, and a $\beta\cdot \OPT^{\calI'}_{\max}$ multi-robustness guarantee on $\calI$ implies $\beta\cdot \OPT^{\calI}_{\max}$ multi-robustness. A similar argument holds for the average multi-robustness guarantee.

\begin{rem}
When $|\calG|$ is large, this reduction becomes computationally inefficient, since in the worst case, the number of samples gets increased by a multiplicative factor of $|\calG|$. However, this reduction is equivalent to keeping only one copy of each sample $(x_i,y_i)\in S$ and when executing~\prettyref{alg:boosting}, in each iteration $t$, assigning it a weight of $p_i=\sum_{j\in[g]:(x_i,y_i)\in G_j}P^t_j/|G_j|$.
\end{rem}

\subsection{Generalization Guarantees}
\label{sec:generalization-guarantees}
In this section, we derive generalization guarantees for multi-robustness. First,~\prettyref{lem:vc-robustloss-groups} shows how to bound the VC-Dimension of the intersection of robust loss and groups. We can then invoke this Lemma to get uniform convergence guarantees that will allow us to get concentration for the conditional robust loss across groups (see \prettyref{def:multi-robustness}).

\begin{lem} [VC Dimension of Intersection of Robust Loss and Groups]
\label{lem:vc-robustloss-groups}
For any class $\calH$, any perturbation set $\calU$, and any group class $\calG$, denote the intersection function class by
\[\calF^\calU_{\calH,\calG} \triangleq \{ (x,y)\mapsto \max_{z\in\calU(x)} \ind\insquare{h(z)\neq y} \wedge g(x): h\in\calH, g\in\calG \}.\]
Then, it holds that $\vc(\calF^\calU_{\calH,\calG}) \leq \Tilde{O}\inparen{\vc(\calL^{\calU}_{\calH}) + \vc(\calG)}$.
\end{lem}



\begin{thm}[Generalization guarantees for average multi-robustness]
\label{thm:generalization-multi-groups}
With $T=\calO(\ln g/\varepsilon^2)$ and $m= \Tilde{O}\inparen{\frac{\vc(\calH)\ln^2(k)}{\eps^4}+\frac{\vc(\calG) + \ln(1/\delta)}{\varepsilon^2}}$,
~\prettyref{alg:boosting} computes a set of hypotheses $\calH'=\{h_1,\dots, h_T\}$, such that $\forall G_j\in \calG$, 
\begin{align*}
&\frac{1}{T}\sum_{t=1}^T\Prob_{(x,y)\in \calD}\Big[\exists z\in \calU(x): h_t(z)\neq y \mid x\in G_j \Big]\leq\\
&~~~~~~~~~~~~~~~~~\inparen{1 + \frac{\varepsilon}{\Prob_{\calD}(x\in G_j)}}\inparen{\OPTSMax + \varepsilon}+\frac{\varepsilon}{\Prob_{\calD}(x\in G_j)}
\end{align*}
\end{thm}


\begin{thm}[Generalization guarantees for $\beta$-multi-robustness]
\label{thm:generalization-multi-groups-deterministic}
With $T=\calO(\ln g/\varepsilon^2)$ and 

$m=\Tilde{O}\inparen{\frac{\vc(\calH)\ln(g)\ln^2(k)}{\varepsilon^6}+\frac{\vc(\calG) + \ln(1/\delta)}{\varepsilon^2}}$,
~\prettyref{alg:boosting} computes a set of hypotheses $\calH'=\{h_1,\dots, h_T\}$, such that $\forall G_j\in \calG$, 
\begin{align*}
&\Prob_{(x,y)\in \calD}\Big[\exists z\in \calU(x): \MAJ(h_1,\dots,h_T)(z)\neq y \mid x\in G_j \Big]\leq\\
&~~~~~~~~~~~~\inparen{1 + \frac{\varepsilon}{\Prob_{\calD}(x\in G_j)}}\inparen{\beta(\OPTSMax + \varepsilon)}+\frac{\varepsilon}{\Prob_{\calD}(x\in G_j)}
\end{align*}
\end{thm}

\begin{rem}
In~\prettyref{sec:proof-generalization-guarantees-deterministic}, we show how to achieve generalization guarantees in terms of $\OPTDMax$ instead of $\OPTSMax$.
\end{rem}

\section{Conclusion}
In this work, we introduced a  counter-example showing how inflated $\ERM$ in the non-realizable setting can fail in robust learning. Then, we provided a %two-layer boosting 
``boosting-style'' algorithm that uses $\ERM$ %differently 
and obtains strong robust learning guarantees in the challenging non-realizable regime. We have also introduced a new multi-robustness objective and provided algorithms that obtain robustness guarantees simultaneously across groups. Future work can continue to explore the multi-robustness objective, connections with multi-calibration (and related `multi' notions), and other ways to use $\ERM$ oracles to learn robust classifiers. 

