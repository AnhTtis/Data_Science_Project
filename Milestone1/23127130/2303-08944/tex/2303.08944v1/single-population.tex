\section{Minimizing Robust Loss Using an ERM Oracle \label{erm}}
\label{sec:non-realizable-oracle}

First, we show an example where the approach of \citet{xiang2022patchcleanser} of calling $\ERM_\calH$ on the inflated dataset, i.e., original training points plus all possible perturbations resulting from the allowed masking operations, fails by obtaining %the largest possible gap 
a multiplicative gap of $k-1$ %in true robust loss and apparent 0-1 loss on the inflated dataset.
in the robust loss between the optimal robust classifier and the classifer returned by $\ERM_\calH$.

\begin{exmpl}
\label{exmpl:ERM-failure}
We exhibit an example that achieves a multiplicative gap of $k-1$ in the robust loss gap between 
the inflated $\ERM$ solution and the optimal robust classifier, where $k$ is the size of the perturbation sets.
This gap exists because $\ERM$ can exhibit a solution that incorrectly classifies at least one perturbation per natural example, while there is a robust classifier that concentrates error on one natural example, thus getting low robust loss. %Our only/primary constraint for this problem is that the perturbation balls need to be finite. 
We can even show this type of example in one dimension with $x \in [-2,2]$ in Figure \ref{fig:example}.

\begin{figure}[ht]
\label{fig:example}
\centering
\includegraphics[width=0.9\textwidth]{figure.pdf}
\caption{Inflated ERM failure mode: Blacks points indicate adversarial perturbations. There are $2N$ examples, where half of them are true positives and the rest are true negatives. Each example has exactly $k=N$ perturbations. For $N-1$ of the true negatives, all their perturbations  are in $[-1,-1/2]$ and thus are benign. However, for one of the true negatives $x_{neg}^{*}$, all $N$ of its perturbations land exactly at $0$. 
For the positive case, this is flipped. There is one example $x_{pos}^{*}$ where all its perturbations are in $[1/2,1]$. For every other true positive example, however, one perturbation is at $0$ and the rest are in $[1/2,1]$.  $\ERM$ classifies the points at $0$ as negative points and exhibits $h_{ERM}$. This induces a high robust loss since %there is a perturbation targeting $N-1$ positives. 
$N-1$ of the true positives are not robustly classified.
Thus $h_{ERM}$ has robust loss $\frac{N-1}{2N} \sim 50 \%$ while $h_{ROB}^{*}$ correctly classifies all points (natural and adversarial) other than $x_{neg}^{*}$ and so has robust loss
$\frac{1}{2N}$, which obtains the worst case multiplicative gap for when $k=N$.
}
\end{figure}

\end{exmpl}

Next, we present our first contribution: an algorithm to learn a predictor that is simultaneously robust to a set of (polynomially many) masking operations, using an $\ERM_\calH$ oracle. The algorithm is based on prior work due to \citet{DBLP:conf/colt/FeigeMS15}, but the analysis and application are novel in this work. The main interesting feature of this algorithm is that it achieves stronger robustness guarantees in the non-realizable regime when $\OPT_\calH \gg 0$, where the approach of \citet{xiang2022patchcleanser} can fail as mentioned in~\prettyref{exmpl:ERM-failure}.

\begin{algorithm}%[H]
\begin{algorithmic}
\caption{\citet*{DBLP:conf/colt/FeigeMS15}}
\label{alg:FMS}
  \INPUT weight update parameter $\eta>0$, number of rounds $T$, and training dataset $S=\{(x_1,y_1),\dots, (x_m,y_m)\}$.
  %and corresponding weights $p_1,\dots,p_m$.}
  \STATE Set $w_1(z, (x,y)) = 1$, for each $(x,y)\in S, z\in \calU(x)$.\\
  \STATE Set $P^1(z,(x,y)) = \frac{w_1(z,(x,y))}{\sum_{z'\in\calU(x)} w_1(z',(x,y))}$, for each $(x,y)\in S, z\in \calU(x)$.\\
\FOR{each $t\in \{1,\cdots T\}$}
\STATE Call $\ERM$ on the empirical weighted distribution: 
\STATE {\small\[h_t = \argmin_{h\in\mathcal{H}} \sum_{(x,y)\in S} \sum_{z\in \calU(x)} \frac{1}{m} P^t(z,(x,y)) \ind\insquare{h_{t}(z)\neq y}\]}%.\\
\FOR{each $(x,y)\in S$ and $z\in\calU(x)$}
\STATE {\small $w_{t+1}(z,(x,y)) = (1+\eta \ind\insquare{h_{t}(z)\neq y}) \cdot w_{t}(z, (x,y))$}%
\STATE $P^{t+1}(z,(x,y))=\frac{w_t(z,(x,y))}{\sum_{z'\in\calU(x)} w_t(z',(x,y))}$.
\ENDFOR
\ENDFOR
%\KwOutput{

%\STATE {\bfseries Output:}
\OUTPUT The majority-vote predictor $\MAJ(h_1,\dots, h_T)$. \\
\end{algorithmic}
\end{algorithm}

\begin{thm}
\label{thm:generalization-FMS}
Set $T(\eps) = \frac{32 \ln k}{\eps^2}$ and $m(\eps, \delta) = O\inparen{\frac{\vc(\calH)(\ln k)^2}{\eps^4}\ln \inparen{\frac{\ln k}{\eps^2}}+\frac{\ln(1/\delta)}{\eps^2}}$. Then, for any distribution $\calD$ over $\calX\times \calY$, with probability at least $1-\delta$ over $S\sim \calD^{m(\eps,\delta)}$, running \prettyref{alg:FMS} for $T(\eps)$ rounds produces $h_1,\dots,h_{T(\eps)}$ satisfying:

\small\[\Ex_{(x,y)\sim \calD} \insquare{ \max_{z\in \calU(x)} \ind\insquare{\MAJ(h_1,\dots, h_{T(\eps)})(z)\neq y} } 
\leq 2\OPT_{\calH} + \eps\]
\end{thm}

\paragraph{Comparison with prior related work} As presented, \cite{DBLP:conf/colt/FeigeMS15} only considered \emph{finite} hypothesis classes $\calH$ and provided generalization guarantees depending on $\log\abs{\calH}$. On the other hand, we consider here infinite classes $\calH$ with bounded VC dimension and provide tighter robust generalization bounds. The robust learning guarantee \citep[][Theorem 2]{attias2022improved} assumes access to a \emph{robust} $\ERM$ oracle, which minimizes the robust loss on the training dataset. On the other hand, at the expense of higher sample complexity, we provide a robust learning guarantee using only an $\ERM$ oracle in the challenging \emph{non-realizable} setting. Prior work due to \citet{DBLP:conf/nips/MontasserHS20} considered using an $\ERM$ oracle for robust learning but only in the simpler realizable setting (when $\OPT_\calH=0$).


Before proceeding with the proof of \prettyref{thm:generalization-FMS}, we describe at a high-level the proof strategy. The main insight is to solve a finite zero-sum game. In particular, our goal is to find a mixed-strategy over the hypothesis class that is approximately close to the value of the game:
\[\OPT_{S,\calH} \triangleq \min_{h\in \calH} \frac{1}{m}\sum_{i=1}^{m} \max_{z_i\in \calU(x_i)} \ind\insquare{h(z_i)\neq y_i}.\]
 

We observe that \prettyref{alg:FMS} due to \citep{DBLP:conf/colt/FeigeMS15} solves a similar finite zero-sum game (see \prettyref{lem:FMS}), and then we relate it to the value of the game we are interested in (see \prettyref{lem:opt}). Combined together, this only establishes that we can minimize the robust loss on the empirical dataset using an $\ERM$ oracle. We then appeal to uniform convergence guarantees for the robust loss in \prettyref{lem:unif-robloss} to show that, with large enough training data, our output predictor achieves robust risk that is close to the value of the game. 

\begin{lem}
\label{lem:opt}
For any dataset $S %=\SET{(x_1,y_1),\dots,(x_m,y_m)}\in (\calX\times\calY)^m$,
=\{(x_1,y_1),\dots,(x_m,y_m)\}\in (\calX\times\calY)^m$,
\begin{align*}
    &\OPT_{S,\calH} = \min_{h\in \calH} \frac{1}{m}\sum_{i=1}^{m} \max_{z_i\in \calU(x_i)} \ind\insquare{h(z_i)\neq y_i} \geq
    \min_{Q\in \Delta(\calH)} \max_{\substack{P_{1}\in \Delta(\calU(x_1)),\\ \dots\\ P_{m} \in \Delta(\calU(x_m))}} \frac{1}{m} \sum_{i=1}^{m} \Ex_{z_i\sim P_i } \Ex_{h\sim Q} \ind\insquare{h(z_i)\neq y_i}
\end{align*}
\end{lem}




\begin{lem} [\citet*{DBLP:conf/colt/FeigeMS15}]
\label{lem:FMS}
For any data set $S %=\SET{(x_1,y_1),\dots,(x_m,y_m)}\in (\calX\times\calY)^m$,
=\{(x_1,y_1),\dots,(x_m,y_m)\}\in (\calX\times\calY)^m$, running \prettyref{alg:FMS} for $T$ rounds produces a mixed-strategy $\hat{Q} = \frac{1}{T} \sum_{t=1}^{T} h_t \in \Delta(\calH)$ satisfying:
{\small
\begin{align*}
    &\max_{\substack{P_1\in \Delta(\calU(x_1)),\\ \dots,\\ P_m\in \Delta(\calU(x_m))}} \frac{1}{m}\sum_{i=1}^{m} \Ex_{z_i\sim P_i} \frac{1}{T} \sum_{t=1}^{T} \ind\insquare{h_t(z_i)\neq y_i} \leq
    \min_{Q\in \Delta(\calH)} \max_{\substack{P_{1}\in \Delta(\calU(x_1)),\\ \dots,\\ P_{m} \in \Delta(\calU(x_m))}} \frac{1}{m} \sum_{i=1}^{m} \Ex_{z_i\sim P_i } \Ex_{h\sim Q} \ind\insquare{h(z_i)\neq y_i} + 2\sqrt{\frac{\ln k}{T}}
\end{align*}
}%
\end{lem}


\removed{
\begin{lem} [Extension to weighted samples]
For any data set $S %=\SET{(x_1,y_1),\dots,(x_m,y_m)}\in (\calX\times\calY)^m$
=\{(x_1,y_1),\dots,(x_m,y_m)\}\in (\calX\times\calY)^m$ and any corresponding weights $p_1,\dots, p_m > 0$ such that $\sum_{i=1}^{m} p_i = 1$, running \prettyref{alg:FMS} for $T$ rounds produces a mixed-strategy $\hat{Q} = \frac{1}{T} \sum_{t=1}^{T} h_t \in \Delta(\calH)$ satisfying:
\begin{align*}
    \max_{P_1\in \Delta(\calU(x_1)),\dots,P_m\in \Delta(\calU(x_m))} &\sum_{i=1}^{m} p_i\cdot \Ex_{z_i\sim P_i} \frac{1}{T} \sum_{t=1}^{T} \ind\insquare{h_t(z_i)\neq y_i} \leq\\
    &\min_{Q\in \Delta(\calH)} \max_{P_{1}\in \Delta(\calU(x_1)),\dots, P_{m} \in \Delta(\calU(x_m))} \sum_{i=1}^{m} p_i\cdot \Ex_{z_i\sim P_i } \Ex_{h\sim Q} \ind\insquare{h(z_i)\neq y_i} + 2\sqrt{\frac{\ln k}{T}}.
\end{align*}
\label{lem:extension-FMS-weights}
\end{lem}
}


\begin{lem} [VC Dimension for the Robust Loss \citep{attias2022improved}]
\label{lem:unif-robloss}
For any class $\calH$ and any $\calU$ such that $\sup_{x\in\calX}\abs{\calU(x)}\leq k$, denote the robust loss class of $\calH$ with respect to $\calU$ by
\[\calL^{\calU}_{\calH} = \{(x,y)\mapsto \max_{z\in\calU(x)} \ind\insquare{h(z)\neq y}: h\in\calH\}.\]
Then, it holds that $\vc(\calL^{\calU}_{\calH})\leq O(\vc(\calH) \log(k))$. 
\end{lem}

We are now ready to proceed with the proof of \prettyref{thm:generalization-FMS}.

\begin{proof}[Proof of~\prettyref{thm:generalization-FMS}]
Let $S\sim \calD^m$ be an iid sample from $\calD$, where the size of the sample $m$ will be determined later. By invoking \prettyref{lem:FMS} and \prettyref{lem:opt}, we observe that running \prettyref{alg:FMS} on $S$ for $T$ rounds, produces $h_1,\dots, h_{T}$ satisfying

{\small
\[\max_{\substack{P_1\in \Delta(\calU(x_1)),\\ \dots,\\ P_m\in \Delta(\calU(x_m))}} \frac{1}{m}\sum_{i=1}^{m} \Ex_{z_i\sim P_i} \frac{1}{T} \sum_{t=1}^{T} \ind\insquare{h_t(z_i)\neq y_i} \leq \OPT_{S,\calH} + \frac{\varepsilon}{4}
\]
}
Next, the average robust loss for the majority-vote predictor $\MAJ(h_1,\dots, h_T)$ can be bounded from above as follows:
\begin{align*}
    &\frac{1}{m} \sum_{i=1}^{m} \max_{z_i\in \calU(x_i)} \ind\insquare{\MAJ(h_1,\dots,h_T)(z_i)\neq y_i}\\
    &\leq \frac{1}{m} \sum_{i=1}^{m} \max_{z_i\in \calU(x_i)}  2 \Ex_{t\sim [T]}\ind\insquare{h_t(z_i)\neq y_i}\\
    &= 2 \frac{1}{m} \sum_{i=1}^{m} \max_{z_i\in \calU(x_i)} \frac{1}{T} \sum_{t=1}^{T} \ind\insquare{h_t(z_i)\neq y_i}\\
    &\leq 2 \max_{\substack{P_1\in \Delta(\calU(x_1)),\\ \dots,\\ P_m\in \Delta(\calU(x_m))}} \frac{1}{m}\sum_{i=1}^{m} \Ex_{z_i\sim P_i} \frac{1}{T} \sum_{t=1}^{T} \ind\insquare{h_t(z_i)\neq y_i}\\
    &\leq 2 \OPT_{S,\calH} + \frac\varepsilon2.
\end{align*}
Next, we invoke \prettyref{lem:unif-robloss} to obtain a uniform convergence guarantee on the robust loss. In particular, we apply \prettyref{lem:unif-robloss} on the \emph{convex-hull} of $\calH$: $\calH^{T} = \{\MAJ(h_1,\dots, h_T): h_1,\dots, h_T\in \calH\}$. By a classic result due to \citet{blumer:89}, it holds that $\vc(\calH^T)=O(\vc(\calH)T\ln T)$. Combining this with \prettyref{lem:unif-robloss} and plugging-in the value of $T= \frac{32 \ln k}{\varepsilon^2}$, we get that the VC dimension of the robust loss class of $\calH^T$ is bounded from above by
\[\vc(\calL_{\calH^T}^\calU) \leq O\inparen{\frac{\vc(\calH)(\ln k)^2}{\varepsilon^2}\ln\inparen{\frac{\ln k}{\varepsilon^2}}}.\]
Finally, using Vapnik's ``General Learning'' uniform convergence \citep{vapnik:82}, with probability at least $1-\delta$ over $S\sim \calD^m$ where $m =  O\inparen{\frac{\vc(\calH)(\ln k)^2}{\varepsilon^4}\ln \inparen{\frac{\ln k}{\varepsilon^2}}+\frac{\ln(1/\delta)}{\varepsilon^2}}$, it holds that
\begin{align*}
&\forall f\in \calH^T: \Ex_{(x,y)\sim \calD} \insquare{\max_{z\in\calU(x)}\ind\insquare{f(z)\neq y}} 
\leq \frac{1}{m}\sum_{i=1}^{m} \max_{z_i\in\calU(x_i)}\ind\insquare{f(z_i)\neq y_i} + \frac\varepsilon4
\end{align*}
This also applies to the particular output $\MAJ(h_1,\dots, h_T)$ of \prettyref{alg:FMS}, and thus
\begin{align*}
    &\Ex_{(x,y)\sim \calD} \insquare{ \max_{z\in \calU(x)} \ind\insquare{\MAJ(h_1,\dots, h_{T(\varepsilon)})(z)\neq y} } \\
    &\leq \frac{1}{m} \sum_{i=1}^{m} \max_{z_i\in \calU(x_i)} \ind\insquare{\MAJ(h_1,\dots,h_T)(z_i)\neq y_i} + \frac{\varepsilon}{4}\\
    &\leq 2\OPT_{S,\calH} + \frac\varepsilon2 + \frac\varepsilon4.
\end{align*}

Finally, by applying a standard Chernoff-Hoeffding concentration inequality, we get that $\OPT_{S,\calH} \leq \OPT_\calH + \frac\varepsilon8$. Combining this with the above inequality concludes the proof.
\end{proof}