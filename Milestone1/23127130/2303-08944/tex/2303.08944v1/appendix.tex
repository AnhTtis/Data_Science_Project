\appendix
\section{Supplementary Materials}
\subsection{Proof of Lemma~\ref{lem:opt}}
\begin{proof}
By definition of $\OPT_{S,\calH}$, it follows that 
\begin{align*}
    &\OPT_{S,\calH} = \min_{h\in \calH} \frac{1}{m}\sum_{i=1}^{m} \max_{z_i\in \calU(x_i)} \ind\insquare{h(z_i)\neq y_i}\\
    &\geq \min_{h\in \calH} \max_{z_1\in \calU(x_1),\dots,z_m\in\calU(x_m)}\frac{1}{m}\sum_{i=1}^{m} \ind\insquare{h(z_i)\neq y_i}\\
    &\geq \min_{Q\in \Delta(H)} \max_{z_1\in \calU(x_1),\dots,z_m\in\calU(x_m)}\frac{1}{m}\sum_{i=1}^{m} \Ex_{h\sim Q}\ind\insquare{h(z_i)\neq y_i}\\
    &\geq  \min_{Q\in \Delta(\calH)} \max_{\substack{P_{1}\in \Delta(\calU(x_1)),\\ \dots, \\P_{m} \in \Delta(\calU(x_m))}} \frac{1}{m} \sum_{i=1}^{m} \Ex_{z_i\sim P_i } \Ex_{h\sim Q} \ind\insquare{h(z_i)\neq y_i}.
\end{align*}
\end{proof}

\subsection{Proof of \prettyref{lem:FMS}}
\begin{proof}
By the minimax theorem and \citep*[][Equation 3 and 9 in proof of Theorem 1]{DBLP:conf/colt/FeigeMS15}, we have that 
\begin{align*}
    &\max_{\substack{P_1\in \Delta(\calU(x_1)),\\ \dots,\\ P_m\in \Delta(\calU(x_m))}} \sum_{i=1}^{m} \Ex_{z_i\sim P_i} \frac{1}{T} \sum_{t=1}^{T} \ind\insquare{h_t(z_i)\neq y_i} \leq\\
    &\min_{Q\in \Delta(\calH)} \max_{\substack{P_{1}\in \Delta(\calU(x_1)),\\ \dots, \\P_{m} \in \Delta(\calU(x_m))}} \Ex_{z_i\sim P_i } \Ex_{h\sim Q} \ind\insquare{h(z_i)\neq y_i} + 2\frac{\sqrt{\mathcal{L}^*m\ln k}}{T},
\end{align*}
where $\calL^*=\sum_{i=1}^{m} \max_{z\in\calU(x_i)} \sum_{t=1}^{T}\ind\insquare{h_t(z)\neq y}$. By observing that $\calL^*\leq m T$ and dividing both sides of the inequality above by $m$, we arrive at the inequality stated in the lemma.
\end{proof}


\subsection{Proof of Lemma~\ref{lem:extension-FMS-weights}}
\begin{proof}
We generalize the argument in \citet*{DBLP:conf/colt/FeigeMS15} to accommodate the weights on the samples $p_1,\dots,p_m$. %First, we present a generalization of the algorithm presented by %\citet*{DBLP:conf/colt/FeigeMS15} in the following:
Specifically, let
\[L^{ON}_T = \sum_{t=1}^{T}\sum_{i=1}^{m}\sum_{z \in \calU(x_i)} p_iP^{t}(z,(x_i,y_i)) \ind\insquare{h_{t}(z)\neq y_i}\]
be the loss of \prettyref{alg:FMS} after $T$ rounds, and let 
\[L^*=\max_{P} \sum_{t=1}^{T}\sum_{i=1}^{m}\sum_{z \in \calU(x_i)} p_iP(z,(x_i,y_i)) \ind\insquare{h_{t}(z)\neq y_i}\]
be the benchmark loss. We show that $L^*(1-\eta)-\frac{\ln k}{\eta}\leq L^{ON}_T$. %The remainder of the analysis follows similarly to \citet*[][Equation 3-10 in proof of Theorem 1]{DBLP:conf/colt/FeigeMS15}. 

To this end, define $W^t_i=\inparen{\sum_{z\in\calU(x_i)}w_t(z,(x_i,y_i))}^{p_i}$ and $W^t=\prod_{i=1}^{m}W^t_i$. Let
\begin{align*}
&F^t_{i} = p_i \cdot \frac{\sum_{z\in\calU(x)} w_t(z,(x,y))\ind\insquare{h_t(z)\neq y}}{\sum_{z\in\calU(x)} w_t(z,(x,y))} \\
&= p_i \sum_{z\in \calU(x_i)} P^{t}(z,(x_i,y_i))\ind\insquare{h_t(z)\neq y}
\end{align*}
be the loss of \prettyref{alg:FMS} on example $(x_i,y_i)$ at round $t$. Observe that by the Step 6 in \prettyref{alg:weighted-FMS}, it holds that $W^T_i\geq (1+\eta)^{p_i \max_{z\in \calU(x_i)}\sum_{t=1}^{T} \insquare{h_t(z)\neq y}}$, and therefore $W^T\geq (1+\eta)^{L^*}$.

Observe also
{\small
\begin{align*}
&W^{t+1}_{i} = \\
&\inparen{\sum_{z:\insquare{h_t(z)\neq y}=0} w_t(z,(x,y)) + \sum_{z:\insquare{h_t(z)\neq y}=1} (1+\eta)w_{t}(z,(x,y)) }^{p_i} \\
&= W^t_i\inparen{1+\eta \frac{F^t_{i}}{p_i}}^{p_i}
\end{align*}
}

This implies that
\begin{align*}
&W^T=\prod_{i=1}^{m} W^T_i = \prod_{i=1}^{m} \insquare{k \prod_{t=1}^{T} \inparen{1+\eta \frac{F^t_{i}}{p_i}}}^{p_i} = \\
&k^{\sum_{i=1}^{m}p_i} \prod_{i=1}^{m}\prod_{t=1}^{T} \inparen{1+\eta \frac{F^t_{i}}{p_i}}^{p_i}
\end{align*}

Combining the above we have,
\[(1+\eta)^{L^*} \leq k\prod_{i=1}^{m}\prod_{t=1}^{T} \inparen{1+\eta \frac{F^t_{i}}{p_i}}^{p_i}.\]
We then apply a logarithmic transformation on both sides
\[L^{*}\ln(1+\eta) \leq \ln k + \sum_{i=1}^{m}\sum_{t=1}^{T} p_i\ln\inparen{1+\eta \frac{F^t_{i}}{p_i}}.\]
Since $a-a^2\leq \ln(1 + a) \leq a$ for $a\geq 0$, we have
\[L^*(\eta-\eta^2)\leq \ln k + \sum_{i=1}^{m}\sum_{t=1}^{T} \eta F^t_i = \ln k+ \eta L^{ON}_{T}.\]
By dividing by $\eta$ and rearranging terms we get $L^*(1-\eta)-\frac{\ln k}{\eta}\leq L^{ON}_T$. 

By setting $\eta=\sqrt{\frac{\ln k}{L^*}}$ and observing that $L^*\leq T$, the remainder of the analysis follows similar to \citet*[][Equation 3-10 in proof of Theorem 1]{DBLP:conf/colt/FeigeMS15}.
\end{proof}

\subsection{Proof of Lemma~\ref{lem:unif-robloss}}
\begin{proof}
By finiteness of $\calU$, observe that 
for any dataset $S\in (\calX\times \calY)^m$, each robust loss vector in the set of robust loss behaviors:
$$ \Pi_{\calL^{\calU}_{\calH}}(S) = \{(f(x_1,y_1),\dots, f(x_m,y_m)): f \in \calL^{\calU}_{\calH}\}$$ maps to a 0-1 loss vector on the \emph{inflated set} 
$S_\calU=\{(z^1_1,y_1),\dots, (z^k_1,y_1), \dots, (z_m^1,y_m),\dots, (z_m^k,y_m)\}$, 
{\small
\[\Pi_{{\calH}}(S_\calU)= \{(h(z^1_1),\dots, h(z^k_1),\dots, h(z_m^1),\dots, h(z_m^k)): h\in \calH\}\]
}
Therefore, it follows that $\abs{\Pi_{\calL^{\calU}_{\calH}}(S)}\leq \abs{\Pi_{{\calH}}(S_\calU)}$. Then, by applying the Sauer-Shelah lemma, it follows that $\abs{\Pi_{{\calH}}(S_\calU)} \leq O((mk)^{\vc(\calH)})$. Then, by solving for $m$ such that $O((mk)^{\vc(\calH)}) \leq 2^m$, we get that $\vc(\calL^{\calU}_{\calH})\leq O(\vc(\calH) \log(k))$.
\end{proof}


\subsection{Proof of~\prettyref{lem:avg-robust-loss-upper-bound}}

\begin{proof}
\begin{align}
&\E_{j\in [g]}[\ell^{rob}_j(h_t)]= \sum_j P_j^t(1/|G_j|)\sum_{(x,y)\in G_j} \max_{z\in \calU(x)}\ind\insquare{h_t(z)\neq y}\label{eqn:avg-robust-loss-def}\\
&=\sum_{i=1}^m p_i \cdot \max_{z\in\calU(x)}\ind\insquare{h_t(z)\neq y} \label{eqn:distribution-samples}\\
&\leq  \max_{\substack{P'_1\in \Delta(\calU(x_1)),\\ \dots,\\P'_m\in \Delta(\calU(x_m))}} \sum_{i=1}^{m} p_i\cdot \Ex_{z_i\sim P'_i} \frac{1}{T} \sum_{\tau=1}^{T} \ind\insquare{h^{\text{FMS}}_{\tau}(z_i)\neq y_i}\label{ref:replace-FMS}\\
&\leq \min_{Q\in \Delta(\calH)} \max_{\substack{P'_{1}\in \Delta(\calU(x_1)),\\ \dots, \\P'_{m} \in \Delta(\calU(x_m))}} \sum_{i=1}^{m} p_i\cdot \Ex_{z_i\sim P'_i } \Ex_{h\sim Q} \ind\insquare{h(z_i)\neq y_i} + 2\sqrt{\frac{\ln k}{T}}\label{ref:lem-weigthed-FMS}\\
&\leq \min_{h\in \calH} \max_{\substack{P'_{1}\in \Delta(\calU(x_1)),\\ \dots, \\P'_{m} \in \Delta(\calU(x_m))}} \sum_{i=1}^{m} p_i\cdot \Ex_{z_i\sim P'_i } \ind\insquare{h(z_i)\neq y_i} + 2\sqrt{\frac{\ln k}{T}}\\
&= \min_{h\in\calH} \max_{\substack{z_1\in \calU(x_1),\\ \dots, \\z_{m} \in \calU(x_m)}} \sum_{i=1}^{m} p_i\cdot   \ind\insquare{h(z_i)\neq y_i} + 2\sqrt{\frac{\ln k}{T}}\label{ineq:pure-strategy-suffices}\\
&\leq \min_{h\in\calH} \max_{j\in[g]} (1/|G_j|)\sum_{(x,y)\in G_j}\max_{z\in\calU(x)}\ind\insquare{h(z)\neq y}+2\sqrt{\frac{\ln k}{T}}\label{ineq:relate-to-opt-max}\\
&= OPT_{\max}+2\sqrt{\frac{\ln k}{T}}
\end{align}


\prettyref{eqn:avg-robust-loss-def} holds by plugging in the definition of $\RLoss_j(h_t)$(\prettyref{eqn:unweighted-robust-loss}).~\prettyref{eqn:distribution-samples} holds for a distribution $p_1,\dots,p_m$ on the samples. In~\Cref{ref:replace-FMS}, $h_t$ is replaced with the hypothesis selected by~\prettyref{alg:weighted-FMS} in each round $t$.~\Cref{ref:lem-weigthed-FMS} holds by~\prettyref{lem:extension-FMS-weights}.~\Cref{ineq:pure-strategy-suffices} holds since it suffices for the max-player to pick a pure strategy.~\Cref{ineq:relate-to-opt-max} holds since the whole probability mass is put as a uniform distribution on the worst-off group. Note that when defining $p_1,\cdots,p_m$, all individuals that belong to the same group have equal weights. 
\end{proof}

\subsection{Proof of~\prettyref{cor:interpret-avg-loss}}
\begin{proof}
Expected robust loss on each group $G_j\in \calG$ is:
\begin{align}
&\frac{1}{|G_j|}\sum_{(x,y)\in G_j} \max_{z\in\calU(x)}\frac{1}{T}\sum_{t=1}^T \ind[h_t(z)\neq y]\\
=&\frac{1}{|G_j|}\sum_{(x,y)\in G_j} \max_{z\in\calU(x)}\Ex_{h_t\sim U(\calH')} \ind[h_t(z)\neq y]\\
\leq&\frac{1}{|G_j|}\sum_{(x,y)\in G_j} \Ex_{h_t\sim U(\calH')} \max_{z\in \calU(x)}\ind[h_t(z)\neq y]\label{eqn:Jensen-avg-robustness}\\
=&\frac{1}{T}\sum_{t=1}^T \frac{1}{|G_j|}\sum_{(x,y)\in G_j} \max_{z\in\calU(x)}\ind[h_t(z)\neq y]\\
=&\frac{1}{T}\sum_{t=1}^T \RLoss_j(h_t)\leq \OPTSMax+\eps \label{eqn:thm-avg-robust-loss}
\end{align}
Where~\prettyref{eqn:Jensen-avg-robustness} holds by Jensen's inequality and~\prettyref{eqn:thm-avg-robust-loss} holds by~\prettyref{thm:randomized-multi-robustness}.
\end{proof}

\subsection{Proof of~\prettyref{lem:vc-robustloss-groups}}
\begin{proof}
The proof is inspired by the proof of \citep[claim B.1 in ][]{DBLP:conf/icml/KearnsNRW18} which proved a similar result for the standard $0$-$1$ loss, and here we extend the result to the robust loss using essentially the same proof. 

Let $S\subseteq \calX \times \calY$ be a dataset of size $m$ that is shattered by $\calF^\calU_{\calH,\calG}$. Then, observe that, by definition of $\calF^\calU_{\calH,\calG}$, the number of possible behaviors $\abs{\Pi_{\calF^\calU_{\calH,\calG}}(S)}$ is at most $\abs{\Pi_{\calL^{\calU}_{\calH}}(S)}\cdot \abs{\Pi_\calG(S)}$. By Sauer-Shelah Lemma, $\abs{\Pi_{\calL^{\calU}_{\calH}}(S)} \leq O(m^{\vc(\calL^{\calU}_{\calH})})$ and $ \abs{\Pi_\calG(S)}\leq O(m^{\vc(\calG)})$. Thus, $\abs{\Pi_{\calF^\calU_{\calH,\calG}}(S)} = 2^m \leq O(m^{\vc(\calL^{\calU}_{\calH})+\vc(\calG)})$, and solving for $m$ yields that $m=\Tilde{O}(\vc(\calL^{\calU}_{\calH})+\vc(\calG))$. Hence, $\vc(\calF^\calU_{\calH,\calG}) \leq \Tilde{O}\inparen{\vc(\calL^{\calU}_{\calH}) + \vc(\calG)}$.
\end{proof}

\subsection{Proof of~\prettyref{thm:generalization-multi-groups}}
\label{sec:proof-generalization-guarantees}

\begin{proof}
The output of~\prettyref{alg:boosting} is $\calH'=\{h_1,\dots,h_T\}$ where each of the predictors $h_1,\dots,h_T$ is a majority-vote predictor over $\calH$. Due to \citet{blumer:89}, the VC-dimension of the output space is $\vc(\calH^{T'})=\Big(\vc(\calH)T'\ln T'\Big)$ where $T'$ is the number of rounds of~\prettyref{alg:weighted-FMS} in each oracle call.

Set $m= \Tilde{O}\inparen{\frac{\vc(\calH^{T'})\ln (k)+\vc(\calG) + \ln (1/\delta)}{\varepsilon^2}}$.  
By setting $T'=\calO(\frac{\ln k}{\eps^2})$ and by invoking \prettyref{lem:unif-robloss} and \prettyref{lem:vc-robustloss-groups} on the hypothesis class $\calH$ and group class $\calG$, we get the following uniform convergence guarantee. With probability at least $1-\delta$ over $S\sim \calD^m$,
\begin{align*}
    &\inparen{\forall h\in \calH^{T'}}\inparen{\forall G_j\in\calG}:\\
    &\Bigg\lvert\Ex_{(x,y)\sim \calD} \insquare{\ind[x\in G_j] \wedge \max_{z\in \calU(x)}\ind[h(z)\neq y] } - \frac{1}{m}\sum_{(x,y)\in S} \ind[x\in G_j] \wedge \max_{z\in \calU(x)}\ind[h(z)\neq y]\Bigg\rvert\leq \varepsilon.
\end{align*}

We can rewrite the above guarantee in a conditional form which will be useful for us shortly in the proof. Namely, $\forall h\in \calH^{T'}, \forall G_j\in\calG$:
\begin{align}
    &\Prob_{(x,y)\sim \calD}\insquare{\exists z\in \calU(x): h(z)\neq y | x\in G_j } \leq \frac{\Prob_{S}(x\in G_j)}{\Prob_{\calD}(x\in G_j)} \frac{1}{|G_j|} \sum_{(x,y)\in S\wedge x\in G_j}\max_{z\in \calU(x)}\ind[h(z)\neq y] + \frac{\varepsilon}{\Prob_{D}(x\in G_j)}\label{eqn:conditional-uniform-convergence}
\end{align}
where $|G_j|=\sum_{(x,y)\in S} \ind[x\in G_j]$. 

\prettyref{thm:avg-empirical-boosting} shows that running \prettyref{alg:boosting} produces hypotheses $h_1,\dots, h_T$ such that, $\forall G_j\in \calG$:
\begin{align}
   &\frac{1}{T}\sum_{t=1}^{T} \frac{1}{|G_j|}\sum_{(x,y)\in S\wedge x\in G_j} \max_{z\in \calU(x)}\ind[h_t(z)\neq y] \leq \OPTSMax + \varepsilon\label{eqn:thm-alg-boosting-avg}
\end{align}

\prettyref{eqn:conditional-uniform-convergence} implies that $\forall G_j\in\calG$,
\begin{align}
    %&\inparen{\forall G_j\in\calG}:\\
    &\frac{1}{T}\sum_{t=1}^T\Prob_{(x,y)\sim \calD}\insquare{\exists z\in \calU(x): h_t(z)\neq y | x\in G_j } \leq\\ 
    &~~~~~~~~~~~~~~~\frac{1}{T}\sum_{t=1}^T\frac{\Prob_{S}(x\in G_j)}{\Prob_{\calD}(x\in G_j)} \frac{1}{|G_j|} \sum_{(x,y)\in S\wedge x\in G_j}\max_{z\in \calU(x)}\ind[h_t(z)\neq y] + \frac{\varepsilon}{\Prob_{\calD}(x\in G_j)},\label{eqn:avg-uniform-convergence-conditional}
\end{align}

Combining~\prettyref{eqn:thm-alg-boosting-avg} and~\prettyref{eqn:avg-uniform-convergence-conditional} implies:

\begin{align}
\frac{1}{T}\sum_{t=1}^T\Prob_{(x,y)\in \calD}\insquare{\exists z\in \calU(x): h_t(z)\neq y | x\in G_j } \leq\frac{\Prob_{S}(x\in G_j)}{\Prob_{\calD}(x\in G_j)} \inparen{\OPTSMax + \varepsilon}+\frac{\varepsilon}{\Prob_{\calD}(x\in G_j)}
\label{eqn:avg-generalization-prob-sample}
\end{align}

Now, given additional samples $\tilde{m}= O\inparen{\frac{\vc(\calG)+\log(2/\delta)}{\varepsilon^2}}$, in addition to the above, we can guarantee that:
\begin{align}
\forall G_j\in \calG: \frac{\Prob_{S}(x\in G_j)}{\Prob_{\calD}(x\in G_j)} \leq \frac{\Prob_{\calD}(x\in G_j) + \varepsilon}{\Prob_{\calD}(x\in G_j)} = 1 + \frac{\varepsilon}{\Prob_{\calD}(x\in G_j)}.
\label{eqn:avg-generalization-prob-distribution}
\end{align}

Combining~\prettyref{eqn:avg-generalization-prob-sample} and~\prettyref{eqn:avg-generalization-prob-distribution} implies that:

\begin{align*}
&\frac{1}{T}\sum_{t=1}^T\Prob_{(x,y)\sim \calD}\insquare{\exists z\in \calU(x): h_t(z)\neq y | x\in G_j } \leq \inparen{1 + \frac{\varepsilon}{\Prob_{\calD}(x\in G_j)}}\inparen{\OPTSMax + \varepsilon}+\frac{\varepsilon}{\Prob_{\calD}(x\in G_j)}
\end{align*}

which completes the proof. We can also obtain a bound in terms of $\OPTDMax$ instead of $\OPTSMax$ using a similar approach used in~\prettyref{sec:proof-generalization-guarantees-deterministic}.

\end{proof}

\subsection{Proof of~\prettyref{thm:generalization-multi-groups-deterministic}}
\label{sec:proof-generalization-guarantees-deterministic}

\begin{proof}
The output of~\prettyref{alg:boosting} is $\calH'=\{h_1,\dots,h_T\}$. Taking majority-vote over the predictors in $\calH'$ is equivalent to taking the majority-vote of majority-vote predictors over $\calH$. Therefore, due to \citet{blumer:89}, the VC-dimension of the output space is $\vc(\calH^{T'})^T=\Big(\vc(\calH)T'\ln T'\Big)T\ln T$, where $T'$ is the number of rounds of~\prettyref{alg:weighted-FMS} in each oracle call and $T$ is the number of rounds of~\prettyref{alg:boosting}.

Let the sample size $m= \Tilde{O}\inparen{\frac{\vc(\calH^{T'})^T\log(k)+\vc(\calG) + \log(1/\delta)}{\varepsilon^2}}$. By setting %$T=c\ln(g)/\ln(\frac{c^c\alpha}{(\alpha+c-1)^c})$
$T=\calO(\ln g/\eps^2)$ and $T'=\calO(\frac{\ln k}{\eps^2})$ and by invoking \prettyref{lem:unif-robloss} and \prettyref{lem:vc-robustloss-groups} on the hypothesis class $\calH$ and group class $\calG$, we get the following uniform convergence guarantee. With probability at least $1-\delta$ over the sample set $S\sim \calD^m$, $\forall h\in (\calH^{T'})^T$ and $\forall G_j\in\calG$:
\begin{align}
    &\Bigg\lvert\Ex_{(x,y)\sim \calD} \insquare{\ind[x\in G_j] \wedge \max_{z\in \calU(x)}\ind[h(z)\neq y] } -\frac{1}{m}\sum_{(x,y)\in S} \ind[x\in G_j] \wedge \max_{z\in \calU(x)}\ind[h(z)\neq y]\Bigg\rvert\leq \varepsilon\label{eqn:uniform-convergence-deterministic-eq1}
\end{align}

We can rewrite the above guarantee in a conditional form which will be useful for us shortly in the proof. Namely, $\forall h\in (\calH^{T'})^T$ and $\forall G_j\in\calG$:
\begin{align}
    \Prob_{(x,y)\sim \calD}\insquare{\exists z\in \calU(x): h(z)\neq y | x\in G_j } \leq 
    \frac{\Prob_{S}(x\in G_j)}{\Prob_{\calD}(x\in G_j)} \frac{1}{|G_j|} \sum_{(x,y)\in S\wedge x\in G_j}\max_{z\in \calU(x)}\ind[h(z)\neq y] + \frac{\varepsilon}{\Prob_{\calD}(x\in G_j)}\label{eqn:uniform-convergence-deterministic-proof}
\end{align}
where $|G_j|=\sum_{(x,y)\in S} \ind[x\in G_j]$.
\prettyref{thm:deterministic-multi-robustness} provides that %\prettyref{alg:deterministic-boosting} produces hypotheses $h_1,\dots, h_T$ such that 
$h^{\text{maj}}=\MAJ(h_1,\dots,h_T)$ satisfies:
\begin{align}
   &\forall G_j\in \calG: \frac{1}{|G_j|}\sum_{(x,y)\in S\wedge x\in G_j} \max_{z\in \calU(x)}\ind[h^{\text{maj}}(z)\neq y] \leq \beta (\OPTSMax + \varepsilon)\label{eqn:guarantee-deterministic-eq1}
\end{align}
Combining~\prettyref{eqn:uniform-convergence-deterministic-proof} and~\prettyref{eqn:guarantee-deterministic-eq1} implies that $\forall G_j\in \calG$:
\begin{align}
&\Prob_{(x,y)\sim \calD}\insquare{\exists z\in \calU(x): h^{\text{maj}}(z)\neq y | x\in G_j }\leq \frac{\Prob_{S}(x\in G_j)}{\Prob_{\calD}(x\in G_j)} \inparen{\beta(\OPTSMax + \varepsilon)}+\frac{\varepsilon}{\Prob_{\calD}(x\in G_j)}
\label{eqn:deterministic-robustness-eq2}
\end{align}

Now, given additional samples $\tilde{m}= O\inparen{\frac{\vc(\calG)+\log(2/\delta)}{\varepsilon^2}}$, guarantees that:
\begin{align} 
\forall G_j\in \calG: \frac{\Prob_{S}(x\in G_j)}{\Prob_{\calD}(x\in G_j)} \leq \frac{\Prob_{\calD}(x\in G_j) + \varepsilon}{\Prob_{\calD}(x\in G_j)} = 1 + \frac{\varepsilon}{\Prob_{\calD}(x\in G_j)}
\label{eqn:deterministic-robustness-eq3}
\end{align}

Combining~\prettyref{eqn:deterministic-robustness-eq2} and~\prettyref{eqn:deterministic-robustness-eq3} gives a refined bound on the average conditional robust loss that holds uniformly across groups. Namely, $\forall G_j\in \calG$,
\begin{align*}
%\forall G_j\in \calG:\\
& \Prob_{(x,y)\sim \calD}\insquare{\exists z\in \calU(x): h^{\text{maj}}(z)\neq y | x\in G_j }\leq\inparen{1 + \frac{\varepsilon}{\Prob_{\calD}(x\in G_j)}}\inparen{\beta(\OPTSMax + \varepsilon)}+\frac{\varepsilon}{\Prob_{\calD}(x\in G_j)}
\end{align*}
We can also obtain a guarantee in terms of $\OPTDMax$ instead of $\OPTSMax$, as follows. Let $h^*\in \calH$ be a predictor which attains $\OPTDMax$ defined as 
{\small
\[\OPTDMax = \min_{h\in\calH} \max_{G_j\in\calG} \Ex_{(x,y)\sim \mathcal{D}}\insquare{\max_{z\in \calU(x)} \ind[h(z)\neq y] \bigg| x\in G_j}.\]
}
%By the first uniform convergence guarantee in the proof, 
Dividing both sides of \prettyref{eqn:uniform-convergence-deterministic-eq1} by $\Prob_{S}(x\in G_j)$ provides that $\forall G_j\in\calG, \forall h\in\calH$:
\[\abs{\frac{\Prob_{\calD}(x\in G_j)}{\Prob_{S}(x\in G_j)} \Prob_{(x,y)\in \calD}\insquare{\exists z\in \calU(x): h(z)\neq y | x\in G_j } - 
\Prob_{(x,y)\in S}\insquare{\exists z\in \calU(x): h(z)\neq y | x\in G_j }} \leq \frac{\varepsilon}{\Prob_{S}(x\in G_j)}\]

and thus it implies that %$\forall G_j\in\calG$:
\begin{align*}
&\Prob_{(x,y)\in S}\insquare{\exists z\in \calU(x): h(z)\neq y | x\in G_j }\leq \\
&~~~~~~~~~~~~~~~\inparen{1+\frac{\varepsilon}{\Prob_{S}(x\in G_j)}}\Prob_{(x,y)\sim \calD}\insquare{\exists z\in \calU(x): h(z)\neq y | x\in G_j } + \frac{\varepsilon}{\Prob_{S}(x\in G_j)}
\end{align*}

Supposing that $\forall G_j\in\calG$, $\Prob_{S}(x\in G_j)\geq \gamma$. By taking a max over groups $G_j \in \calG$, we get
\[\OPTSMax \leq (1+\frac{\varepsilon}{\gamma}) \OPTDMax +\frac{\varepsilon}{\gamma}.\]

\end{proof}




