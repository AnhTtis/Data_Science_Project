{
    "arxiv_id": "2303.08944",
    "paper_title": "Certifiable (Multi)Robustness Against Patch Attacks Using ERM",
    "authors": [
        "Saba Ahmadi",
        "Avrim Blum",
        "Omar Montasser",
        "Kevin Stangl"
    ],
    "submission_date": "2023-03-15",
    "revised_dates": [
        "2023-03-17"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG",
        "cs.CR",
        "cs.CV"
    ],
    "abstract": "Consider patch attacks, where at test-time an adversary manipulates a test image with a patch in order to induce a targeted misclassification. We consider a recent defense to patch attacks, Patch-Cleanser (Xiang et al. [2022]). The Patch-Cleanser algorithm requires a prediction model to have a ``two-mask correctness'' property, meaning that the prediction model should correctly classify any image when any two blank masks replace portions of the image. Xiang et al. learn a prediction model to be robust to two-mask operations by augmenting the training set with pairs of masks at random locations of training images and performing empirical risk minimization (ERM) on the augmented dataset.\n  However, in the non-realizable setting when no predictor is perfectly correct on all two-mask operations on all images, we exhibit an example where ERM fails. To overcome this challenge, we propose a different algorithm that provably learns a predictor robust to all two-mask operations using an ERM oracle, based on prior work by Feige et al. [2015]. We also extend this result to a multiple-group setting, where we can learn a predictor that achieves low robust loss on all groups simultaneously.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.08944v1"
    ],
    "publication_venue": null
}