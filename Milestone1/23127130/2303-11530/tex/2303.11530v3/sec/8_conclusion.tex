\section{Conclusion}
\label{sec:conclusion}
We present the first active segmentation framework for high-accuracy instance segmentation of \xeable parts in real-world RGB images. Our active learning framework, integrating human feedback, iteratively refines predictions in a \emph{coarse-to-fine} manner, and \agp{achieves close-to-error-free performance on the test set}. By leveraging correlations between the scene, objects, and parts, we demonstrate that our method can achieve state-of-the-art performance on challenging scenes with multiple cross-categories objects, and \agp{significantly reduce human efforts for dataset preparation.}

Additionally, we \agp{contribute a high-quality and diverse dataset} of articulated objects in real-world scene, complete with precise \xeable part annotations. We will expand it further to support the vision community for understanding scene from images. We also hope our work catalyzes future motion- or functionality-aware vision tasks.
% \agp{
% We advocate active learning as a general and effective means to obtain high-accuracy instance segmentations. It may be the most
% viable option to achieve close-to-error-free performance on arbitrary test sets. If properly designed, AL can significantly reduce
% human annotation effort for dataset preparation. In this work, we realized both goals for the specific task of instance segmentation
% of interactable parts from real scene images containing articulated objects.

% Our contribution also includes a high-quality and diverse dataset of annotated real photographs, which we will continue to scale up
% to serve the vision community. We would also like to endow the annotated parts with motion parameters. On the technical side, there is
% much room to improve on speeding up the correction of erroneous segmentations during AL. Additional priors beyond object poses
% may also be explored to facilitate dynamic part segmentation. At last, we would like to extend our AL framework to other motion-
% or functionality-aware vision and annotation tasks.
% }

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/qual-ours.pdf}
    % \vspace{-3pt}
    \caption{Qualitative results on \textit{test set} from our dataset. We visualize predictions results on different object categories using 3 competing methods and our final model. Our method outputs better segmentation masks over \xeable parts across multiple objects in the image with clear separation of parts and small parts segmentation (Row 1, 4, 5). Our results also show that the \emph{coarse-to-fine} segmentation framework can effectively reduce segmentation errors from unwanted objects (Row 2) and object side surfaces (Row 2, 3, 6, 8). More results in the supplementary materials. }
    \label{fig:viz_ours}
\end{figure}