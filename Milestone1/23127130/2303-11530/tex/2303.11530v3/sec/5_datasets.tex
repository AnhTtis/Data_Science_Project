\section{Datasets and Metrics}
\label{sec:datasets}
\input{tables/data_dist}

\agp{
\paragraph{Datasets.} 
We use three real image datasets in our experiments: (1) OPDReal~\cite{jiang2022opd}, (2) OPDMulti~\cite{sun2024opdm}, and (3) our dataset. 
Our dataset images are obtained from the real world by taking photographs of articulated objects in indoor scenes from furniture stores, offices, and homes, captured using iPhone 12, 12Pro and 14. Images are captured with varying camera poses and distances
from the objects, and an image can contain more than one object, with multiple \xeable parts per object. Differences to OPD and OPDMulti datasets are explained in Section \ref{sec:rw}.


% Different from OPDReal and OPDMulti selecting frames from video data, our dataset comprises images captured through direct photography of indoor scenes in real world. These photographs are rich in detail and resolution, featuring multiple articulated objects with multiple \xeable parts captured from various viewpoints and distances. 

We consider six object categories: Storage, Fridge, Dishwasher, Microwave, Washer, and Oven. A comparison of dataset statistics is presented in Table~\ref{tab:data_dist}. OPDReal comprises of $\sim$30K images, with each image depicting a single articulated object. OPDMulti contains $\sim$64K images. Among these, only 19K images are considered ``valid", containing at least one articulated object. \rqw{Our dataset has a total of 2,550 images, with each image showcasing objects from several categories. We organize our dataset according to the primary object depicted in each image. Our dataset stands out by offering the highest diversity of objects and parts among the compared datasets, including 333 different articulated objects and 1,161 distinct parts. }
% Our dataset has a total of 2,550 images and exhibits larger diversity in terms of constituent objects and their \xeable parts. In total, our dataset consists of 333 different articulated objects and 1,161 distinct parts. 

In terms of the \xeable part annotation, both OPDReal and OPDMulti generate annotations on a 3D mesh reconstructed from the RGB-D scans, and project these 3D annotations back to the 2D image space to get 2D part masks. This process is prone to reconstruction and projection errors. We, on the other hand, create annotations on the captured images directly using our \emph{coarse-to-fine} active learning framework. See the supplementary material for annotation quality comparisons. %We provide a detailed comparison of annotation quality across datasets in the supplementary material. 

Table~\ref{tab:data_dist} shows that the majority (91.67\%) of data samples in OPDReal belong to the Storage category, with the rest distributed among the remaining categories. In contrast, our dataset offers a more uniform data distribution across all six categories. \vspace{-10pt}
\paragraph{Metrics.}
To evaluate model performance and AL efficiency, we use the following:
\begin{itemize}
    \item \textbf{Mean Average Precision (mAP)}: We report mAP@IoU=0.5 for correctly predicting the part label and 2D mask segmentation with IoU $\geq$ 0.5. This metric, which is applied to 2D mask segmentation, is more precise for evaluating segmentation quality than BBox mAP used by OPD~\cite{jiang2022opd}, which only assesses boundary accuracy and overlooks finer details such as mask edges and internal holes.

    \rzz{The {\em ground-truth\/} (GT) segmentations over an image dataset to measure mAP are obtained by applying AL over the dataset with full validation by humans.}
    \item \textbf{AL iterations}: We report the number of iterations required during active learning. This metric represents the efficiency of the overall AL pipeline. 
    \item \textbf{Annotated images}: We report the numbers of images and corresponding parts required for manual annotation for each iteration during AL. This metric helps us evaluate the efficiency of the AL sampling process. 
    \item \textbf{Total lab time}: We report the total lab time required for labeling a dataset. For methods which employ AL, it includes time spent on compulsory sampling after each iteration and manual annotation in each iteration. For methods without AL, it calculates the time spent on manual annotation of all failed predictions. This metric provides an overview of human effort required for all methods. See Section 3.4 in the Supplementary Materials for details of human efforts in our AL process.
    \vspace{-10pt}
\end{itemize}
}
% To evaluate the AL setup, we measure manual annotation \emph{and} verification times. For methods without AL, we record the time for labeling only the \emph{test set}, which is the manual annotation time on network segmentation predictions.

