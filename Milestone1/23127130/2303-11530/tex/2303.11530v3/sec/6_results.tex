\section{Experiments}
\label{sec:results}
\rqw{
\agp{We start our experiments by rendering synthetic images from the PartNet-Mobility dataset~\cite{Xiang_2020_SAPIEN} with diverse articulation states, enabling us to obtain sufficient annotations for training 2D segmentation networks and support transfer learning applications.} The synthetic dataset contains $\sim$32K images, evenly distributed across categories, and randomly partitioned into training  (90\%) and test sets (10\%). 

We implement our network in PyTorch on two NVIDIA Titan RTX GPUs. \agp{All images are resized to 256$\times$256 for training.} For pre-training on PartNet-Mobility, we use the Adam optimizer with an initial learning rate (\emph{lr}) of 2.5$e$-4, reducing it by %a factor of
$\gamma=0.1$ at 1K and 1.5K epochs separately over a total of 2K epochs. When fine-tuning on real images, we use the same \emph{lr} and $\gamma$ at 3.5K and 4K epochs over a total of 4.5K epochs.

\subsection{Competing methods}
We compare our active \textit{coarse-to-fine} part segmentation model with three 2D segmentation methods and also analyze two variants of our proposed approach. 
\begin{itemize}
    \item \textbf{Grounded-SAM}~\cite{Grounded-SAM_Contributors_Grounded-Segment-Anything_2023}, which combines Grounding-DINO~\cite{liu2023grounding} and Segment Anything~\cite{kirillov2023segany}, \agp{is a \rzz{foundational vision-language} model that can be used for zero-shot 2D object detection and segmentation, and supports text prompts.} \chk{In our experiments, we set the text prompt as [door, drawer] for segmentation results. } %as a powerful large model used for strong zero-shot object detection and segmentation in images given text captions.
    \item \textbf{OPD-C}~\cite{jiang2022opd}, which is the first work for detecting openable parts in images based on MaskRCNN~\cite{he2017mask}. This is the base variant without camera pose for training. 
    \item \textbf{OPDFormer-C}~\cite{sun2024opdm}, a follow-up of OPD-C based on Mask2Former~\cite{cheng2022masked}, is the SOTA for \agp{openable} part detection of multiple articulated objects in images.
% \end{itemize}
% \vspace{-10pt}
% And two variants of our approach,
% \begin{itemize}
    \item \textbf{$\text{Ours}_{w/o AL}$} \agp{is a variant that does not use human feedback -- it infers part segmentation results based only on the transformer-based model.} %\emph{test set} using our transformer-based model.
    \item \textbf{$\text{Ours}_{f-AL}$} \agp{is variant of our approach that uses only the \emph{fine} stage of the AL framework. That is, verification and annotation of just the part masks is done.}
    % \item \textbf{$\text{Ours}_{GAL}$} \agp{is an AL variant of our approach \emph{without} using the \emph{coarse-to-fine} AL strategy -- verification and annotations of just the part masks is done (\emph{fine} stage only).}
    %is an AL variant of our approach without using the \emph{coarse-to-fine} AL strategy; it is trained using the \emph{fine} AL with verification and annotation of part mask only.
    \vspace{-10pt}
\end{itemize}

\input{tables/mAP_ours}

\subsection{Evaluation on Our Dataset}

\rzz{We perform two key evaluations on our dataset, one for segmentation accuracy and one for
annotation efficiency, while comparing to SOTA alternatives, with or without AL.}

\rzz{
%\paragraph{Dataset split.}
%
We work on 2,550 images with a split of 50/500/2000 into \textit{train/enhancement/test} sets. The train set has been fully annotated manually and
it is used by all methods, except for Grounded-SAM, for fine-tuning. The
\emph{enhancement set}, initially unlabeled, is employed by AL models to progressively improve the learning. The \emph{test set} of 2,000 images is unseen by all methods, including AL, when evaluating segmentation accuracy. When assessing annotation efforts, we apply the methods on both the 500-image set and the 2,000-image set to examine how the efficiency achieved by our AL model scales.
%This demonstrates the initial scarcity of annotated data. We first evaluate the performance of all competing methods on our . Note that  Methods employing AL utilize our \emph{enhancement set} for further learning. In addition, we evaluate the annotation efficiency of all methods, with and without AL, on the \emph{enhancement set} and the \emph{test set} respectively. 
}

\vspace{-5pt}

\paragraph{\rzz{Segmentation accuracy} on \emph{test set}.} Table~\ref{tab:map_ours} compares four non-AL and two AL methods. Among four non-AL methods (columns 1-4), Grounded-SAM is without fine-tuning and \agp{has the lowest performance}. This demonstrates that current \rzz{generic large foundational models} are still limited in \agp{understanding} object parts without adequate \agp{training on} well-labeled data. 
%
\rzz{Despite the small (50-image) \emph{train set}, models fine-tuned on it produce significant improvements. Specifically, $\text{Ours}_{w/o AL}$ model surpasses all competing methods with over 75\% segmentation mAP, while OPDFormer-C falls short of 70\%, and OPD-C scores below 50\%. This discrepancy stems from the architectural designs of OPD-C and OPDFormer-C, which were built on vanilla MaskRCNN and Mask2Former for general segmentation tasks but fail to capture the nuances of articulated objects, where movable parts are closely tied to object pose and interaction directions.} In contrast, our network effectively leverages the hierarchical structure of the scene, objects, and parts therein, resulting in \agp{a much better} performance. 

% Conversely, $\text{Ours}_{w/o AL}$ model outperforms competing methods, achieving $>$75\% segmentation mAP. The \agp{architectural} designs of OPD-C and OPDFormer, based respectively on vanilla MaskRCNN and Mask2Former, are tailored for universal segmentation tasks. Therefore, they inadequately address the unique characteristics of articulated objects, whose movable parts are closely tied to object pose and interact\agp{ion} directions. 
%Beyond accuracy metrics, we observe that all non-AL methods require \agp{significant} human effort for labeling the \emph{test set}. \agp{This is mainly due} to imperfect predictions from these models. 

As seen in the last two columns of Table~\ref{tab:map_ours}, \chk{by performing AL on the \emph{enhancement set}}, \rzz{the performance is significantly boosted over non-AL methods,} reaching over 90\% accuracy, with \chk{less than 1.7 hours spent on} manual \rzz{segmentation}. 
Figure~\ref{fig:viz_ours} shows qualitative results of different methods on our \emph{test set}.

The segmentation accuracy of our two AL alternatives is close since they share the identical network architecture. But they differ in AL training strategies, which impacts labeling efficiency. \rzz{On the 500-image \emph{enhancement set}, our \emph{coarse-to-fine} AL strategy leads to a slight improvement (only 4.5\%) on human annotation effort. We show next that on a larger set to perform AL, the improvement becomes more significant.}

\vspace{-5pt}

\input{tables/al_ours}

\paragraph{Annotation efficiency comparison.}
%
\rzz{Table~\ref{tab:map_ours} shows that with 1.6 hours of manual segmentation to process images with missed predictions, our AL model is able to fully validate the \xeable part segmentations and semantic labels for the 500-image \emph{enhancement set}. 
To obtain the same GT annotations on this set, with respect to an non-AL method such as Grounded-SAM, one must manually correct all images with erroneous or imperfect segmentations.
Specifically, Grounded-SAM could only yield less than 5\% perfectly annotated images, with the rest (479) needing manual processing.

In Table~\ref{tab:al_ours} (top), we report and compare manual efforts, in terms of number of images, parts, and annotation times, required across different methods to obtain GT for the 500-image set.
%
%annotating images in this set. As shown in row 1-4, without AL, at least over 200 images require manual labeling. Specifically, Grounded-SAM and OPD-C have less than 30 perfect predictions, requiring more than 7.5 hours to manually label the remaining images. 
$\text{Ours}_{w/o AL}$ shows the best efficiency among non-AL methods, but it still takes 3.58 hours to annotate 210 images with 762 parts. Rows 5-8 underscore the benefits of AL for  annotation efficiency. By employing AL, in rows 5 \& 6, OPD-C and OPDFormer-C demonstrate marked improvements over their non-AL versions. However, they still require 5.7 and 3.9 hours, respectively. Due to their tendency to generate noisy predictions on irrelevant parts of the object or background, most predictions are categorized as \emph{fair} as described in Section~\ref{subsec:almethod}, leading to more iterations in AL and additional time spent on sampling. In contrast, as shown in rows 7 \& 8, both variants of our AL model complete in 3 iterations, with our \emph{coarse-to-fine} AL methods requiring the least images for labeling and minimum time efforts.

In Table~\ref{tab:al_ours} (bottom), we report all the numbers to obtain GT annotations for a larger set of (2,000) images. What is most notable is that the efficiency gain in manual annotation time by our \emph{coarse-to-fine} AL strategy has improved from less than 5\%, for the smaller image set of 500, to more than 13\% (6.5 hours vs.~7.5 hours). This demonstrates that our \emph{coarse-to-fine} AL approach is particularly beneficial for large-scale annotation tasks, where the time saved on annotation significantly outweighs the extra time spent on sampling. Please check our supplement for detailed AL iterations.
}

%\chk{To further assess the efficiency of AL methods, Table~\ref{tab:al_ours} (bottom) compares time savings between non-AL methods (row 1-4) and our AL methods (row 5 \& 6) for annotating our \emph{test set}, which contains more data. We can observe that the advantages of AL methods are more notable. When evaluating results on our \emph{enhancement set}, our \emph{coarse-to-fine} AL method shows marginal time savings of 0.005 hour over $\text{Ours}_{f_AL}$, as it requires an additional iteration for sampling interaction directions. The benefit of our \emph{coarse-to-fine} AL strategy becomes more pronounced with larger datasets; the time saved is more substantial as the volume of data needing annotation increases. Specifically, for 2000 images, the time saving escalates to 1 hour, highlighting the scalability and efficiency advantages of the method in handling extensive datasets. This demonstrates that our \emph{coarse-to-fine} AL approach is particularly beneficial for large-scale annotation tasks, where the time saved on annotation significantly outweighs the extra time spent on sampling. Please check our supplement for detailed AL iterations.}

\input{tables/ablation}

\vspace{-5pt}

\paragraph{Ablation study.}Table~\ref{tab:ablation} \agp{highlights the need and} contributions of key components of our method on improving prediction accuracy and minimizing human efforts. Columns 2-5 respectively indicate the presence of: \textbf{Mask} object mask head; \textbf{Pose} object pose estimation head; \textbf{Interaction direction} interact\agp{ion} direction prediction head; \textbf{AL} active learning. Row 4 and 5 uses the \emph{fine} AL stage on part mask \agp{alone} due to the absence of pose and interact\agp{ion} direction prediction module. Row 6 and 7 use our \emph{coarse-to-fine} AL strategy. Results in Table~\ref{tab:ablation} clearly justify the \emph{coarse-to-fine} design in our method, \agp{which gives the best performance (see row 7).} %where our method (Row 6) is the most accurate and efficient.



\subsection{Evaluation on OPDReal and OPDMulti}
In addition, we assess the performance of \agp{different} models on the OPDReal and OPDMulti dataset, using \agp{their respective train and test splits.} %training and test data partitions as provided by these datasets. 


\begin{wraptable}{r}{6cm}
\vspace{-25pt}
\caption{Quantitative comparison against competing segmentation methods and our model variant on OPDReal, OPDMulti test set.}\label{tab:map_opd}
\resizebox{0.5\columnwidth}{!}{
\begin{tabular}{@{}ccccc@{}}
\toprule
& \multicolumn{3}{c}{segm mAP ($\uparrow$)}\\
\cmidrule{2-5}
& Grounded-SAM & OPD-C & OPDFormer-C & $\text{Ours}_{w/o AL}$ \\
\cmidrule{2-5}
OPDReal & 16.5 & 44.5 & 46.3 & \textbf{51.6}\\
OPDMulti & 8.0 & 25.6 & 27.6 & \textbf{31.5}\\
\bottomrule
\end{tabular}
}
\vspace{-20pt}
\end{wraptable} 

As shown in Table~\ref{tab:map_opd}, $\text{Ours}_{w/o AL}$ method \agp{outperforms the rest}. 
However, with more than 70\% training data, all methods still fail to achieve $>$55\% accuracy on OPDReal. This limitation primarily stems from \agp{data skewness towards the Storage category in OPDReal, which constitutes more than 90\% of total samples, and results in poor generalization across other object categories.} Detailed category-wise results are \agp{provided} in the supplementary material. 

Performance on OPDMulti is further compromised by an abundance of noisy data in its test set\agp{\cite{sun2024opdm}}. From the qualitative results in Figure~\ref{fig:viz_opdreal}, we observe that some openable parts are cluttered or missed in the GT annotation, while our method accurately segments these parts. This discrepancy also contributes to the low accuracy.
}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{figs/qual-opdr.pdf}  
    \caption{Qualitative results on OPDReal and OPDMulti test set. $\text{Ours}_{w/o AL}$ outperforms others on noisy GT and multiple objects. \chk{See supplementary materials for more results.}}
    \label{fig:viz_opdreal}
    \vspace{-10pt}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/application.pdf}
    \caption{Part-level reconstruction and manipulation of the bottle and dishwasher}
\label{fig:application}
\vspace{-10pt}
\end{figure}




% \begin{figure}
%     \centering
%     \includegraphics[width=0.4\textwidth]{figs/lamp-bottle.pdf}
%     \caption{Generalization on bottle and different lamps.\rqc{this figure could be removed if the teaser contains results on lamp/bottle}}
% \label{fig:viz_lamp}
% \end{figure}
% \subsection{Model ablations}
% \label{subsec: ablation}
% The performance of our method is driven, in parts, by two specially designed prediction branches that individually output camera pose and object mask. We perform evaluations by ablating these two modules in our network architecture.
% \vspace{-10 pt}
% \paragraph{Ours w/o pose prediction branch.}
% Keeping all other modules, we remove that MLP branch in Figure \ref{fig:method-overview} which predicts 6 DoF camera pose for a given input image. %This setting demonstrates the need and efficiency of having a pose prediction branch for obtaining accurate 2D part segmentation masks in a given image.
% \vspace{-10 pt}
% \paragraph{Ours w/o \rqw{object} mask prediction branch.}
% Here, we only remove that MLP branch in Figure \ref{fig:method-overview} which predicts a binary object mask for the interactable objects (not parts) in the input image. %This setting analyses the need and effect of a coarse object mask in obtaining fine-grained 2D part segmentation.

%
% We observe that our model significantly outperforms competing methods on both datasets. This proves the efficacy of our model over competing methods. It is interesting to note the big jump in performance for \emph{all} the models when testing using models fine-tuned on our dataset, compared to their respective performance when fine-tuned on the OPDReal dataset. This is mainly due to data skewness towards the Storage category in OPDReal (makes for 91.67\% of the total samples), leading to a lack of generalizability on images with other object categories. Since our dataset is relatively balanced across different categories (except for the Washer), we observe that all three models perform much better than OPDReal counterparts, with our model achieving the best results again. These results validate the richness of our dataset over OPDReal for dynamic part segmentation tasks.

% \subsection{Ablation studies w/o active learning}
% \label{subsec:ablation_no_al}
% %

% We perform ablations on our network architecture to better understand the need for pose estimation MLP and the object mask prediction MLP at the output of the transformer decoder (shown in the orange-colored box on the right side of Figure \ref{fig:method-overview}). The segmentation performance is tabulated in Table \ref{tab:ablation}. Essentially, when both the pose estimation module and the mask predictor module are removed (row 1), our network reduces to the architecture of Mask2Former \cite{cheng2022masked}. Also, the pose estimation module seems to play more of a role than the object mask predictor module in achieving better part segmentation, with a positive net difference of 1.5\% in mAP score, see row 3 and row 2 in Table \ref{tab:ablation}, respectively. 

% This is because estimating 6-DoF pose enables us to obtain regions of interactable parts for objects in the image, whereas a coarse object binary mask provides information about the object as a whole. When both modules are present (row 4), the network achieves the best performance since they provide a coarse-to-fine refinement over their individual contributions. This is nothing but our setting.



% \subsection{Quantitative results with active learning}
% \label{subsec:quant_res_al}
% The learning framework and data modifications per iteration in the active learning (AL) setup are described in Section \ref{subsec:al_methodology}. In Table \ref{tab:al_map}, we present the mAP scores for different models, and contrast them with the active learning setup on our model, on the same train-test set of 50-500 real-world images. Naturally, all models without AL framework overfit on the 50-image training set, with the performance of the \emph{best} model being $\sim$10\% lower than our AL framework.

% We also compare annotation times required for labeling all images in our enhancement set (see Section \ref{subsec:al_methodology}) purely based on segmentation results output from OPD and Mask2Former. That is, no active learning is considered here. Table \ref{tab:al_on_all_methods} presents these results and compares them against our AL framework. We clearly see that the number of images as well as interactable parts that need manual annotation to segment all interactable parts is greater for OPD, followed by Mask2Former. However, this number drops drastically when our AL model is considered. As such, our AL model consumes the least time to get 2D segmentation masks. 

% Finally, we also show the AL process for all four iterations in Table \ref{tab:al_iter}. The important thing to read in this table are the last three columns that represent the number of images/parts with ``perfect" segmentations, and the amount of time taken to fix them, respectively (see Section \ref{subsec:al_methodology} for interpretation of ``perfect" and ``bad"). We observe that as the iterations progress, the amount of annotation time decreases, which can be attributed to the model's generalization ability with more labeled training data.
% }

% %Clearly, \rqw{our method with AL achieves the highest accuracy with a few time of human efforts. Table \ref{tab:al_iter} shows the detailed statistics of each iteration in AL, where the fourth column represents the numbers of bad images and corresponding annotations done by human. We compute the time cost based on the number of test results and manual annotations. Predictions on the test set are sorted based on the average prediction confidence and we consider a prediction as \emph{bad} if there is no correct prediction with over 75\% confidence. We manually go through predictions in order and select \emph{perfect} predictions, which include all interactable parts in the image with fine-grained segmentation mask. We estimates the time for checking test sets as 3 seconds per image, and manual annotation on bad predictions as 15 seconds per annotation based on our experiments. Perfect predictions are automatically merged into the training set without any human time cost.}


% \agp{
% \subsection{Ablation studies with active learning}
% In Table \ref{tab:al_ablation_ourcomp}, we show timing comparisons for annotating enhancement set images (refer to Section \ref{subsec:al_methodology} for terminology) using different versions of our model. We again observe that the pose estimation module (Row ID 2) provides better segmentation masks on unseen images compared to the object mask prediction module (Row ID 1), resulting in less human intervention for rectifying the masks, as recorded by the annotation time in the last column. Our proposed model (Row ID 3) requires minimum time effort from humans, validating, yet again, our network design choices.
% }

% %\rqw{\subsection{Ablation studies with active learning}
% %We perform ablations on time cost of AL using different methods, which aims to show the time savings using AL for labeling and segmentation task. Table~\ref{tab:al_on_all_methods} verifies the contribution of pose-aware masked attention scheme of our method on minimizing human labeling efforts. We compare our full method against the baseline with different settings of modules. It is clearly that our full method (last row) is the most efficient. 

% %To further evaluate the efficiency of AL, we apply different methods to label our \emph{enhancement} set. Table~\ref{tab:al_ablation_ourcomp} presents the quantitative evaluation results on human annotation and time cost. The second column of the table shows the corresponding numbers of manual annotated images and annotations of different methods. Without AL, both OPDRCNN-C and Mask2Former can barely generate a few \emph{perfect} annotations and most images need to be labeled manually. Our method with AL is capable for accurately labeling all images in the \emph{enhancement} set within less than 2 hours of manual annotation on 16.7\% of images, which significantly reduces the time of human efforts for 76.8\%. 
% %}




