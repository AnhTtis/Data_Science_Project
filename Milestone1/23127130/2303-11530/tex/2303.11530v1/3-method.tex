\section{Method}
\label{sec:method}
%
\agp{
\subsection{Terminology}
\label{subsec: terminology}

For exposition clarity, we define some terms that are frequently used across much of this work.\newline
% \vspace{-10 pt}
\textbf{Interactable objects} -- 3D shapes that contain moveable parts, such as a cabinet drawer, either in the rest state or in the articulated state, are said to be interactable objects. \textbf{Interactable parts} -- Moveable parts in interactable objects are called interactable parts.
% \vspace{-10 pt}
\textbf{Dynamic parts} -- Dynamic parts and interactable parts are used interchangeably throughout.
% \vspace{-10 pt}
\textbf{Articulated objects} -- Interactable objects with articulations on their dynamic parts are termed as articulated objects.
% \vspace{-10 pt}
%\textbf{Articulation images} -- Images that contain interactable and/or articulated object(s) are called articulation images.

\subsection{Problem statement}
\label{subsec: prob_descr}
Let $D$ be a real-world image dataset of interactable objects. Given an RGB image $I\in D$ containing one or more interactable objects $\{o_{j}\}$ as input, our goal is to output a set of 2D segmentation masks, $\{m_{i}\}$, corresponding to all interactable parts for each object $o\in \{o_{j}\}$ present in $I$, where each mask $m_{i}$ is represented by a 2D polygon.% covering interactable parts in the input image.

%Typically, labeled datasets allow to train neural models for generalized object detection and segmentation. However, a basic challenge in our problem setting stems from the fact that, for images in $D$, there is no labeled data for training 2D segmentation networks for all interactable parts in any given image, and obtaining manual annotations for them is quite a laborious and time-consuming process.

% To address the above challenge, 
We propose an active learning setup (for segmentation on unlabeled datasets) using a transformer model for continual mask refinement on unseen real images using a human-in-the-loop framework.  Figure \ref{fig:method-overview} provides an overview of our approach. It consists of two parts: (a) a pose-aware masked-attention network for 2D segmentation of interactable parts of $\{o_{i}\}$ in $I$, and (b) learning to generalize such segmentations using an active learning framework.
}

\input{tables/data_dist}

\subsection{Pose-aware masked-attention network}
\label{subsec:segm_transf}
\agp{
Fig ~\ref{fig:method-overview} (right) shows a detailed structure of our segmentation network, whose working can be broken down into five major steps as explained below. 

\textbf{Detector backbone.} First, the input image $I$ is passed through a \emph{pre-trained} 2D object detection network, MaskRCNN\cite{he2017mask}, to obtain a 2D object bounding box $bbox^{o}$, and features maps, $f$ for subsequent processing. %We use Detectron2 \cite{wu2019detectron2} in our implementation. %to obtain the feature maps and 2D object bounding boxes for subsequent processing.

\textbf{DD Decoder.} We use the \emph{pretrained} decoder from \emph{Deformable DETR} transformer module proposed by Zhu et al.\cite{zhu2020deformable}. Inspired by \cite{jantos2022poet}, we replace the learned object query embedding with the normalization of \rqw{centre coordinates $(c_x, c_y)$, width and height $(w,h)$ from the }detected 2D bounding box, so that the decoder can generate new object query embeddings which contain both local and global information extracted from the image, and the 2D bounding box can be used for 6DoF pose estimation.
% This decoder gradually upsamples the input features, $f$, to generate per-pixel embeddings, denoted by $f_{ViT}$.

\textbf{Task-specific MLPs.} \rqw{Object queries from decoder} are passed into three separate \rqw{MLP heads}, trained from scratch, for (a) object class prediction, (b) 6DoF object pose estimation and (c) binary object mask prediction. The class prediction head uses the cross-entropy loss, and the mask prediction head uses a pixel-wise cross-entropy loss. 

The pose estimation head, predicting 6DoF object pose, outputs a rotation matrix $\mathbf{R}$, and translation matrix $\mathbf{t}$, against whom the loss function is formulated. Specifically, for $\mathbf{t}$, we use an L2 loss: $L_t = \|\mathbf{t} - \Tilde{\mathbf{t}}\|_2 $, and for $R$, we use a geodesic loss as defined in \cite{mahendran20173d}: $L_{rot} = \arccos \frac{1}{2} \left(Tr \left(\mathbf{R}\Tilde{\mathbf{R}}^T\right) - 1\right)$, where $\Tilde{\mathbf{t}}$, $\Tilde{\mathbf{R}}$ are predictions. The loss for pose estimation head is: $L = \lambda_t L_t+\lambda_{rot} L_{rot}$, where $\lambda_t$ and $\lambda_{rot}$ are the weighting parameters, set to 2 and 1 respectively.

Using $bbox^{o}$ and the estimated 6DoF object pose, we can obtain the corresponding 3D \emph{oriented} bounding box $OBB^{o}$\rqw{, which tightly fit the $bbox^{o}$}. From among the eight vertices in $OBB^{o}$, we select vertices of the face with positive $x$ coordinates as the representative 2D box for object front, and use it to crop the input image. This cropped image may contain pixels that do not actually belong to the object of interest. We filter out such pixels by multiplying it with the 2D binary object mask, resulting in a refined binary object mask, $m^{o}_{rfnd}$.

% \textbf{Pixel decoder} We borrow the \emph{pre-trained} pixel decoder from MaskFormer \cite{cheng2021per} which takes $f$ as input, and upsamples these feature maps to output images features, $f_{pd}$.
\rqw{\textbf{Pixel decoder} We borrow the the \emph{pretrained} pixel decoder from MaskFormer \cite{cheng2021per} which takes $f$ as input, and upsamples the features to generate embeddings $f_{pd}$.}

\textbf{Masked-attention decoder.} To finally output segmentation masks corresponding to the interactable parts in $I$, we make use of masked-attention decoders from Mask\emph{2}Former \cite{cheng2022masked}. The structure of each layer $L_{i}$ is shown right below $L_{1}$ in Figure \ref{fig:method-overview}. $L_{1}$ takes as input $f_{pd}$ and the refined mask, $m^{o}_{rfnd}$, and outputs a binary mask which is fed to the next layer. The binary mask at the output of $L_{3}$ is multiplied with $f_{pd}$ resulting in part segmentation in the RGB space. The loss here is the pixel-wise cross entropy loss. We call it as our \emph{pose-aware masked-attention decoder}.

All these modules are \emph{jointly} trained in an \emph{end-to-end} fashion for synthetic image datasets (see Section \ref{sec:datasets}). When finetuning on real images where part annotations are made available, weights for all the modules, except the MLPs, are updated as GT pose and object masks are not collected for these images, which are required to train these MLPs.
}

% \iffalse
% \rqw{
% %%% key points:
% \begin{itemize}
%     \item a backbone detector (we used MaskRCNN), but it can be trained on top of any object detector architecture to obtain the feature maps and bounding boxes of the features. [We need to obtain this first because the fact that - the prespective projection of a 3D bounding box should fit tightly within its 2D detection window. 
%     3D bounding box can be defined as center $T = [t_x, t_y, t_z]^T$, dimensions $D = [d_x, d_y, d_z]$, orientation $R(\theta, \phi, \alpha)$. Given the pose of the object in the camera coordinate frame $(R, T)$ <- this is the rotation and translation matrix. the projection of a 3D point $\mathbf{X} = [X,Y,Z,1]^T$ in the object's coordinate frame into the image $\mathbf{x} = [x,y,1]^T$ is: $\mathbf{x} = K [R T]\mathbf{X}$. ]
%     \item feature maps used for the above object detection step are passed to the encoder of a multi-head attention-based transformer (vanilla ViT). 
%     \item the decoder of vanilla ViT is changed (structure is the same, input changed)=> first, learned object query in vanilla ViT is replaced by bounding box information: centre coordinates $(c_x, c_y)$, normalized width and height $(w, h)$ pass the position-encoded to generate object query embeddings. 
%     \item outputted by transformer are passed through three heads: class prediction, 6dof pose prediction and object mask prediction. The class prediction head uses cross-entropy loss. The object mask head uses pixel-wise cross-entropy loss.The prediction of 6DoF pose includes the prediction of rotation matrix $R$ and translation matrix $t$. The L2 loss of translation matrix is defined as $L_t = \|t - \Tilde{t}\|_2 $. Similar to \cite{zhou2019continuity}, the loss of the rotation matrix is defined using a geodesic loss\cite{mahendran20173d}: $L_{rot} = \arccos \frac{1}{2} \left(Tr \left(R\Tilde{R}^T\right) - 1\right)$. The final loss function of this layer is $L = \lambda_t L_t+\lambda_{rot}+L_{rot}$, where $\lambda_t$ and $\lambda_{rot}$ are the weighting parameters for both loss respectively. (they are set to $\lambda_t = 2$ and $\lambda_{rot} = 1$ in experiments).
%     \item We can obtain the 3D tight oriented bounding box based on the 2D object bounding box with a object centre coordinates and 6DoF as the visualization of 6 DoF pose. We can represent the coordinates of this 3D bounding box vertices through $\mathbf{x}_1 = [d_x / 2, d_y / 2, d_z/2]^T, \mathbf{x}_2 = [-d_x / 2, d_y / 2, d_z/2]^T, \dots, \mathbf{x}_8 = [-d_x / 2, -d_y / 2, -d_z/2]^T$, where $d$ is the dimension parameters and it is the distance from object center to the vertices. Now we choose the 4 vertices with positive $d_x$ as the 2D bbox of the object front pose, cropping the region in the image, and combine with the object mask to filter out the those pixels outside the object. 
%     \item We denote the masked attention decoder from Mask2Former, which takes pixel-level embeddings from pixel decoder (same as the one in maskformer\cite{cheng2021per} before mask2former) and queries (the vanilla ViT one, not the bbox). Instead of using the mask prediction obtained from $\mathbf{X_0}$ (before feeding query features into the Transformer decoder), we use the pruned mask as our $\mathbf{M}_0$, and the attention mask for following decoder layers $\mathbf{M}_{l-1}$ is the binarized of the prediction from the previous decoder layer. 
% \end{itemize}
% }
% \fi
% \begin{figure}
% \centering
%   \includegraphics[width=0.48\textwidth]{figures/decoder-details.pdf}
%   \caption{decoder}
%   \label{fig:decoder-details}
% \end{figure}


\subsection{Active learning for 2D part segmentation}% of interactable parts}
\label{subsec:al_methodology}
\agp{
In the active learning setup, we first consider a mini dataset, $E\in D$ of $m$ images, and use it to improve our segmentation model with a human-in-the-loop framework. We call this mini dataset $E$ the enhancement set as it iteratively helps enhance the segmentation masks at the output of our model. Next, we consider a really small training set, $T_{s}\in D$ (s.t $T_{s}\cap E$ is a nullset), of $r$ images, and fine-tune our model $M_{s}$ on this very small training set. As expected, $M_{s}$ fine-tuned on $T_{s}$ does not generalize well to images in $E$. This is where the active learning framework kicks in. In our implementation, $m=500$, and $r=50$.

We input images from $E$ to the fine-tuned model $M_s$, which outputs segmentation masks for interactable parts in the input image. three scenarios exist: (1) If the output mask is deemed to be perfect (i.e., covers all interactable parts without any holes), as determined by humans, we move such an example from $E$ to $T_{s}$. So, $|E|$ and $|T_{s}|$ are now decreased and increased by 1, respectively; (2) If the output mask is imperfect (i.e., holes exist in predicted masks and/or not all interactable parts are segmented), we keep that sample as-is in $E$. And, (3) if the output is deemed to be bad (i.e., no interactable part is segmented), we obtain manual annotation for segmentation masks on all interactable parts from humans in the loop. 

Once annotated, the examples are now ``perfect" and moved from $E$ to $T_{s}$. This process continues iteratively until all the examples in $E$ are moved to $T_{s}$, resulting in $|E|$ being zero; see Table \ref{tab:al_iter}. This framework allows $M_{s}$ to continually see new and good labeled training data on previously unseen images, helping it to learn better. We will show more details of human verification and annotation process in our supplementary material.
}

\iffalse
% \rqw{We present an active transformer-based neural network with position-aware masked attention for the interactable parts segmentation task. Given a single RGB image as its input, the segmentation mask of every dynamic part detected in the image is predicted. After generating the whole object mask and its 6DoF position parameters by passing the image features through a classic Transformer decoder, they are combined and processed by a masked-attention Transformer decoder. At the end, the dynamic part mask are estimated. }

% \rqw{For each test image, we use an active learning scheme which involves human guidance into the part segmentation process. At the end, we obtain a segmentation accuracy 94.801\% on the test images. }
% %3. Each key component of your method should be an individual section
% %\rqw{\subsection{Problem statement?}}

% \rqw{\subsection{Dynamic/interactable part segmentation module}}

% \rqw{Comparing to universal image segmentation, part level segmentation of an object is more challenging since the pixel-level features in-between an object is similar and more difficult to learn the distribution of different parts. For articulated object, humans often interact with them from front side, where the dynamic part is located. We hypothesize that object front surface is enough to cover its dynamic part and 6DoF bounding box of pose can be used to crop the front region. For this we propose a Transformer-based model with pose-aware masked attention decoder for accurate articulated part segmentation. Inspired by mask2former\cite{cheng2022masked} using masked attention instead of cross-attention to update query features, our pose-aware masked attention modulates the attention matrix via the combination of both object mask and object front surface mask. 

% Figure~\ref{fig:method-overview} (right) shows a detailed network structure of the segmentation module. Similar to PoET network\cite{jantos2022poet}, first, an input image contains one or more articulated objects is passed through a backbone object detection network. Generated feature maps and predicted 2D object bounding boxes are used for following 6DoF pose parameters prediction, including 3D translation $\mathbf{T}$ and 3D rotation $\mathbf{R}$ after positional encoding to object query embeddings and decoding by Transformer decoder. The different part is that aside from object position, we also make the prediction on object class and object mask. Consider the input image is in wild with noisy background and unrelated objects, only objects belonging to the articulated categories will be selected for further part-level segmentation.  }
% 
\fi