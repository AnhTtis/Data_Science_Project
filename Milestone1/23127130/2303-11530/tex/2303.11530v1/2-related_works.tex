\section{Related Works}
\agp{
\paragraph{Articulated objects dataset.}The last few years has seen the development of articulation datasets on 3D shapes. Of the many, ICON \cite{hu2017learning} build a dataset (unreleased) of 368 moving joints corresponding to various parts of 3D shapes from the ShapeNet dataset \cite{chang2015shapenet}. The Shape2Motion dataset \cite{wang2019shape2motion} provides kinematic motions for 2,240 3D objects across 45 categories sourced from ShapeNet and 3D Warehouse \cite{trimble_warehouse}. The PartNet-Mobility dataset \cite{Xiang_2020_SAPIEN} consists of 2,374 3D objects across 47 categories from the PartNet dataset \cite{Mo_2019_CVPR}, providing motion annotations and part segmentation in 3D. 

All these datasets are obtained via manual annotations and are \emph{synthetic} in nature. Since sufficient training data is made available by these synthetic datasets, models trained on them can be used for fine-tuning on \emph{real-world} 3D articulated object datasets with limited annotations.

A recent work, called OPD \cite{jiang2022opd}, provides a 2D image dataset of real-world articulated objects, OPDReal, obtained from RGB-D scans of indoor environments. The images in OPDReal come with 2D segmentation labels on all \emph{openable} parts along with their motion parameters. However, due to the nature of annotation process, the 2D part segmentation masks obtained via 3D-to-2D projection do not completely cover all interactable parts in the image. Also, in OPDReal, objects are scanned from within a limited distance range. Practical scenarios and use cases are likely going to have large camera pose and distance variations. 

To overcome these limitations, we contribute a 2D image dataset of articulated objects present in the real world (furniture stores, offices, homes), captured using iPhone12 Pro and 14. We then use our active-learning framework (see  Figure \ref{fig:method-overview} and Section \ref{sec:method}) to learn a generalized 2D segmentation for interactable object parts.
%Talk about articulation datasets in 3D, including on humans/animals. Then, talk about their 2D counterpart, if there exists any. Here you will need to write about OPDSynth. Then, you talk about OPDReal. Say what issues exist with OPDReal and how our dataset is different from it.
\vspace{-10 pt}
\paragraph{Part segmentation in images.}
Early approaches \cite{wang2015joint, wang2015semantic, xia2017joint} to 2D semantic part segmentation developed probabilistic models on human and animal images. While not addressing the 2D semantic part segmentation problem as such, \cite{huang2020arch, mehta2017vnect, ballan2012motion, kanazawa2018end, mueller2018ganerated} tackled the problem of estimating 3D articulations from human images, which requires an understanding of articulated regions in the input image. 

Recently, with the availability of 3D part datasets \cite{Mo_2019_CVPR, Xiang_2020_SAPIEN}, there have been works that estimate 3D articulations from articulation images \cite{abbatematteo2019learning, li2020category, zhang2021strobenet}. However, they learn a latent space of 3D articulation parameters and do not provide 2D segmentation masks for interactable parts. Our work aims at segmenting \emph{interactable} parts of a 3D object from image input. To our knowledge, OPD \cite{jiang2022opd} is the only work that can segment such object parts given an input image, and is built on the Mask RCNN architecture \cite{he2017mask}. In our work, we employ the Mask2Former \cite{cheng2022masked} architecture with task-specific modifications as described in Section \ref{sec:method}.
\vspace{-10 pt}
\paragraph{Active learning for image segmentation.}
Active learning (AL) is a well-known technique for improving model performance with limited labeled data. Prior works \cite{seneractive, Sinha_2019_ICCV, casanovareinforced, Xie_2020_ACCV, Shin_2021_ICCV} have demonstrated different ways of using the most informative data to acquire labels with minimum cost for 2D segmentation task. There exist AL algorithms for 2D segmentation task \cite{ning2021multi, wu2022d} that are specifically designed to reduce the domain gap by aligning two data distributions. We can not borrow such methods to reduce the domain gap between synthetic and real scene images of interactable objects because of large feature differences (our synthetic images contain no background, unlike real scene images).
%Most active learning based algorithms for segmentation tasks denote uncertainty-based active domain adaptation to acquire labels nearby the decision boundary\cite{ning2021multi, wu2022d}, which targets the alignment of two domains. However, the synthetic data of articulated object usually does not contain any meaningful background, which is difficult to align features in the real domain.

More recently, \cite{tang2022active, xie2022towards} employed AL to refine initial 2D segmentation mask through key point or region selection, requiring little human guidance. Due to potentially multiple interactable parts, such point/region selection are ambiguous for an interactable object. As such, we design an AL framework that reduces manual effort by focusing on: (a) using improved part segmentation model (see Section \ref{subsec:segm_transf}), and (b) simpler rules for modifying the test set for iterative model refinement (see Section \ref{subsec:al_methodology}).  %In our task, the selection of point and region are difficult within a same object, and in most cases, the initial prediction mask has very low quality. Instead of reducing human efforts in annotation, our method focus on getting a better segmentation sampling in AL, with the selection of region-of-interests (ROIs) from pose aware masked attention decoder. }

% \cite{sinha2019variational} introduces a mechanism learning representative sampling in an adversarial manner between unlabeled and labeled data. \cite{shin2021all, cai2021revisiting} involve pixel-level guidance to assist annotations. 
}

