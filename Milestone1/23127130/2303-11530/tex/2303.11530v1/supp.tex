\renewcommand{\thesection}{\Alph{section}}% For Alpha numeric number

% \agp{
We present additional results, both qualitative and quantitative, here in the supplementary material. We begin by showcasing differences in examples of real scene images from our dataset and compare them with the kind of real images from OPDReal \cite{jiang2022opd} dataset (Section \ref{sec:dataset_viz}). We then present additional qualitative and quantitative results for 2D segmentation masks on interactable parts of objects in real scene images (Section \ref{sec:results_no_al}), without any active learning (AL) setup. We conclude the supplementary material by providing more results on different models (OPD \cite{jiang2022opd}, Mask2Former \cite{cheng2022masked}, and ours) using the human-in-the-loop framework (Section \ref{sec:results_al}). We also attach a video in the supplementary package that demonstrates human feedback in the active learning setup.
% }

\section{Dataset Visualization}
\label{sec:dataset_viz}
We show more visualizations of raw images as well as annotated segmentation masks for interactable object parts in both, OPDReal dataset and ours, in Fig~\ref{fig:dataset-1} and ~\ref{fig:dataset-2}, respectively. 

We show examples in our dataset from different categories. %while there is no category information for each image in the OPDReal dataset. 
We can observe from the visualizations that the segmentation masks in OPDReal are not correctly annotated, with clear misalignments at part boundaries; see Fig~\ref{fig:dataset-1}(a) row 1, 4 right and, 6; Fig~\ref{fig:dataset-2}(a) row 1 left, 4 right, 5 left).  In addition, due to reconstruction errors in OPDReal, some images are poorly labeled with noisy annotations, especially on objects with reflective surfaces such as the ones shwon in Fig~\ref{fig:dataset-1}(a) row 2 right, 10 left and Fig~\ref{fig:dataset-2}(a) row 2, 9 right. 
As mentioned in our paper, OPD model is only used on images that contain a single instance of interactable objects. So, although there are multiple articulated objects in an image, only one of them will be annotated as seen in Fig~\ref{fig:dataset-1}(a), row 7 left, 9 left; Fig~\ref{fig:dataset-2}(a), row 9 right, 10 right. 

Apart from annotation quality, the OPDReal dataset is not guaranteed to have high-resolution images. Image data of OPDReal are captured through frames from videos, which are used for 3D reconstruction. Therefore, some images have motion blur as seen in Fig~\ref{fig:dataset-1}(a), row 7 right, row 8 left and; Fig~\ref{fig:dataset-2}(a) row 3 right, row 4 right, row 8 right), and are not captured from clean viewing angles, which makes the target part look obscure, as seen in Fig~\ref{fig:dataset-1}(a) row 2 left, row 3 left, row 4 left, row 5, row 7 right, row 8, row 9 right, row 10 right.

In our dataset, images are directly labeled in 2D. Hence, we have a rich image dataset quality and contain clear annotations of interactable parts for more than one object present in an image. We present visualizations of individual categories of objects in our dataset along with their segmentation masks, see Fig~\ref{fig:dataset-1}(b-c), and Fig~\ref{fig:dataset-2}(b-d).


\section{More Visualizations w/o Active Learning}
\label{sec:results_no_al}

% \subsection{Qualitative Results}
% \label{subsec:qual_res_no_al}
We show additional qualitative results of different methods on our test set in Fig~\ref{fig:ours-1}, ~\ref{fig:ours-2}, and on OPDReal dataset in Fig~\ref{fig:opd-1}, ~\ref{fig:opd-2}, without the active learning setup. %We also employ different components in our method for ablation study. 

On both datasets, our model outperforms the other two methods. It is worth noting that the segmentation results on OPDReal dataset using our method are superior to the so-called ground truth, as could be seen in the refinements of segmentation masks in Fig~\ref{fig:opd-1}, Fig~\ref{fig:opd-2}. Our methods is also able to detect multiple interactable objects in the same image. 

\section{More Results with Active Learning}
\label{sec:results_al}

In the active learning (AL) setup where we use feedback from humans in the loop, one of the major tasks is to judge the quality of annotations output from the base model used in the setup. To do this, we categorize the outputs into three types, and based on this categorization, the images are then moved to respective parts of the pipeline as described below.
\paragraph{Perfect} If the model prediction precisely segments all interactable parts of objects in the image, the prediction will be considered as perfect, and it will be automatically moved to the training set from the enhancement set. Note that not all objects will be segmented in our task - if the object is too small or only partly shown in the image, it will be ignored as they are not meaningful in future downstream tasks.
\vspace{-10 pt}
\paragraph{Fair} If the model prediction outputs partial segmentation masks with some minor mistakes (such as holes in the mask, noisy boundary, or wrong prediction on the background/side face of the object), then such a prediction will be considered as fair, and it will remain in the enhancement set for the next iteration.
\vspace{-10 pt}
\paragraph{Bad} If the model fails to segment interactable object parts, or incorrectly segments a non-interactable part (such as combing two interactable parts together, wrong segmentation boundary, or missing too many interactable parts), then the prediction will be considered as bad. These bad predictions will be manually annotated and merged to the training set. Note that at every iteration, a bad prediction will be automatically selected from a completely missed prediction (no prediction with more than 75\% confidence). Humans only needs to review the remaining predictions.

Fig~\ref{fig:al-pfb} shows a comparison of perfect, fair, and bad predictions based on our model employed in the AL setup. 


\subsection{Quantitative and Qualitative Results}
\label{subsec:qual_and_quant_res_al}
In Tables \ref{tab:al_iter_m2f}, \ref{tab:al_iter_opd}, we present additional qualitative results in the active learning context when Mask2Former \cite{cheng2022masked} and OPD \cite{jiang2022opd} are employed as the base models. Similar to the main paper where we present a table enumerating different data stats over each iteration of the active learning process, the above two tables are similar in spirit. These results are over the enhancement set of 500 images. 

We observe that OPD and Mask2Former take 7 iterations in the AL setup, whereas our model (see the main paper, Table 6)
takes only 4 iterations. Among the two competing methods, Mask2Former consumes less time over OPD, indicating that the predictions from Mask2Former are superior to OPD.

Finally, in Table \ref{tab:al_2k}, we also show the iteration process for our model in the AL setup on a new enhancement set of 2K images, which were previously used as the true test set. In Figure \ref{fig:al-pfb}, we present visualizations of segmentation masks on the 2K enhancement set using our model.

\subsection{User Study}
\label{subsec:us_al}
We conduct an in-lab user study to (i) obtain timing in reviewing the model predictions after each iteration, and (ii) obtain timing for manually annotating the bad predictions. We selected 7 users with basic computer operating skills. We provided them with the same tools and instructions as used for the justification of prediction results and labeling of interactable parts in the AL process. They were tasked to distinguish perfect, fair and bad predictions from visualization results, and annotate interactable parts for all bad predictions (the definitions of what classifies a result as perfect/fair/bad were provided to the users beforehand, as also explained in the beginning of this section). 

We use labelme\cite{wada2018labelme} as the labeling tool. The average times taken by humans for different tasks in our AL model are as follows: time to identify whether the prediction is perfect, fair or bad $\tau_{ident} = 2.4s$, time to click the corresponding selection button in our interface $\tau_{click} = 0.6s$, and average per-annotation labeling time $\tau_{anno} = 14.8s$. We also attach a video in the supplementary material showing the interface of our AL setup for sample selection and annotation. 


\input{tables/AL-iter-M2F}
\input{tables/AL-iter-OPD}
\input{tables/AL-2k}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/supp-dataset-1.pdf}
    \caption{Visualizations of dataset annotations of OPDReal and ours. (No AL) (Part 1)}
    \label{fig:dataset-1}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/supp-dataset-2.pdf}
    \caption{Visualizations of dataset annotations of OPDReal and ours. (No AL) (Part 2)}
    \label{fig:dataset-2}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/supp-ours-1.pdf}
    \caption{Qualitative results of different methods on our test set (No AL) (cabinet and dishwasher).}
    \label{fig:ours-1}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/supp-ours-2.pdf}
    \caption{Qualitative results of different methods on our test set (No AL) (fridge, microwave \& oven and washing machine).}
    \label{fig:ours-2}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/supp-opd-1.pdf}
    \caption{Qualitative results of different methods on OPDReal test set (No AL) (Part 1).}
    \label{fig:opd-1}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/supp-opd-2.pdf}
    \caption{Qualitative results of different methods on OPDReal test set (No AL) (Part 2).}
    \label{fig:opd-2}
\end{figure*}


\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/AL_pfb.pdf}
    \caption{Comparison of perfect, fair, and bad predictions.}
    \label{fig:al-pfb}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/AL-qual.pdf}
    \caption{Qualitative results of our method based on AL pipeline on 2k test images. (with AL) }
    \label{fig:al-qual}
\end{figure}
