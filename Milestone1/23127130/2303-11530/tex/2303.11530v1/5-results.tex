\section{Results and Evaluation}
\label{sec:results}
\agp{
We evaluate our pose and mask-aware interactable part segmentation model on real scene images (both ours and OPDReal) by comparing its segmentation results against two competing methods. Through ablation studies, we provide insights into the need for pose-aware and mask-aware components present in our network. Finally, we compare the segmentation results on \emph{our} real scene images, with and without the active learning framework.%, and evaluate the difference in manual annotation efforts when these images are processed using competing baselines, including ours.
%
\subsection{Competing Methods}
\label{subsec: baselines}
%
\paragraph{OPD \cite{jiang2022opd}.} As one of the first (and only) works for detecting interactable object parts for in RGB images, we use OPD as one of our comparisons. %Built on the Mask RCNN architecture \cite{he2017mask}, it provides 2D segmentation masks for interactable parts in a given input image. \rqw{
In our experiments, we select OPDRCNN-C for comparison.% and we evaluate its detection performance only. 
% } 
\vspace{-5 pt}
\paragraph{Mask2Former \cite{cheng2022masked}.} As an advanced, transformer-based extension of Mask RCNN architecture for generalized object detection and segmentation in 2D images, we compare against the Mask2Former architecture by employing it to detect all interactable object parts in input images.

\subsection{Model ablations}
\label{subsec: ablation}
The performance of our method is driven, in parts, by two specially designed prediction branches that individually output camera pose and object mask. We perform evaluations by ablating these two modules in our network architecture.
\vspace{-10 pt}
\paragraph{Ours w/o pose prediction branch.}
Keeping all other modules, we remove that MLP branch in Figure \ref{fig:method-overview} which predicts 6 DoF camera pose for a given input image. %This setting demonstrates the need and efficiency of having a pose prediction branch for obtaining accurate 2D part segmentation masks in a given image.
\vspace{-10 pt}
\paragraph{Ours w/o \rqw{object} mask prediction branch.}
Here, we only remove that MLP branch in Figure \ref{fig:method-overview} which predicts a binary object mask for the interactable objects (not parts) in the input image. %This setting analyses the need and effect of a coarse object mask in obtaining fine-grained 2D part segmentation.

\subsection{Evaluation Metrics}
\label{subsec: eval_metrics}
%
\paragraph{Mean Average Precision (\textbf{mAP}).} %To measure the quality of predicted 2D part segmentation masks in the input image, we resort to standard metrics of precision scores, as implemented in \cite{lin2014microsoft}.
%Following \cite{lin2014microsoft}, we report the mean of averaged precision scores, mAP, over all predicted segmentation masks across all the test images. 
Following OPD \cite{jiang2022opd}, we report mAP@IoU=0.5 scores for both 2D bounding box detection and 2D segmentation tasks. For brevity, in the rest of the section, we use mAP to denote mAP@IoU=0.5.
\vspace{-10 pt}
\paragraph{Annotation time.} In the active learning setup, the key components that determine the need and efficiency of human-in-the-loop framework are the number of manual annotations, the annotation time (measured in hours), and segmentation accuracy (mAP@IoU=0.5).%, in that order. 

Figure \rqw{\ref{fig:viz_opdreal} and \ref{fig:viz_ours}} show visual results on different models for test images from \rqw{OPDReal and our dataset}, respectively.

\subsection{Quantitative results w/o active learning}
\label{subsec:quant_res_no_al}
%
\input{tables/mAP_table}
%
We start off by comparing the performance of our model with competing methods without any active learning framework. As explained in Section \ref{sec:datasets}, all these models are pre-trained on synthetic renderings, which are then fine-tuned using images from the two OPDReal and our dataset. Our primary interest in the 2D part segmentation performance. As such, we report the segmentation mAP(@IoU=0.5) on both, the OPDReal dataset and our dataset. For completeness of evaluation, we also report the mAP scores for 2D bounding box corresponding to interactable parts. These results are tabulated in Table \ref{tab:mAP_table}.

%
\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/qual-opdr.pdf}
    \caption{Qualitative results on OPDreal test set. %Columns from left to right show the prediction using OPDRCNN-C, mask2former, our method and ground truth (GT). 
``Miss" represents the absence of part segmentation with $\ge$ 75\% confidence. Both OPDRCNN-C and mask2former fail to detect some/all dynamic parts for categories with fewer samples (ex. Washer and Dishwasher). Our method outperforms the two -- on part edges (first and second row), for noisy GT (third row), and on multiple objects (last row).}
\vspace{-10 pt}
\label{fig:viz_opdreal}
\end{figure}
%
We observe that our model significantly outperforms competing methods on both datasets. This proves the efficacy of our model over competing methods. It is interesting to note the big jump in performance for \emph{all} the models when testing using models fine-tuned on our dataset, compared to their respective performance when fine-tuned on the OPDReal dataset. This is mainly due to data skewness towards the Storage category in OPDReal (makes for 91.67\% of the total samples), leading to a lack of generalizability on images with other object categories. Since our dataset is relatively balanced across different categories (except for the Washer), we observe that all three models perform much better than OPDReal counterparts, with our model achieving the best results again. These results validate the richness of our dataset over OPDReal for dynamic part segmentation tasks.

\subsection{Ablation studies w/o active learning}
\label{subsec:ablation_no_al}
%
\input{tables/ablation}
We perform ablations on our network architecture to better understand the need for pose estimation MLP and the object mask prediction MLP at the output of the transformer decoder (shown in the orange-colored box on the right side of Figure \ref{fig:method-overview}). The segmentation performance is tabulated in Table \ref{tab:ablation}. Essentially, when both the pose estimation module and the mask predictor module are removed (row 1), our network reduces to the architecture of Mask2Former \cite{cheng2022masked}. Also, the pose estimation module seems to play more of a role than the object mask predictor module in achieving better part segmentation, with a positive net difference of 1.5\% in mAP score, see row 3 and row 2 in Table \ref{tab:ablation}, respectively. 

This is because estimating 6-DoF pose enables us to obtain regions of interactable parts for objects in the image, whereas a coarse object binary mask provides information about the object as a whole. When both modules are present (row 4), the network achieves the best performance since they provide a coarse-to-fine refinement over their individual contributions. This is nothing but our setting.

\input{tables/mAP_with_AL}

\subsection{Quantitative results with active learning}
\label{subsec:quant_res_al}
The learning framework and data modifications per iteration in the active learning (AL) setup are described in Section \ref{subsec:al_methodology}. In Table \ref{tab:al_map}, we present the mAP scores for different models, and contrast them with the active learning setup on our model, on the same train-test set of 50-500 real-world images. Naturally, all models without AL framework overfit on the 50-image training set, with the performance of the \emph{best} model being $\sim$10\% lower than our AL framework.

We also compare annotation times required for labeling all images in our enhancement set (see Section \ref{subsec:al_methodology}) purely based on segmentation results output from OPD and Mask2Former. That is, no active learning is considered here. Table \ref{tab:al_on_all_methods} presents these results and compares them against our AL framework. We clearly see that the number of images as well as interactable parts that need manual annotation to segment all interactable parts is greater for OPD, followed by Mask2Former. However, this number drops drastically when our AL model is considered. As such, our AL model consumes the least time to get 2D segmentation masks. 

Finally, we also show the AL process for all four iterations in Table \ref{tab:al_iter}. The important thing to read in this table are the last three columns that represent the number of images/parts with ``perfect" segmentations, and the amount of time taken to fix them, respectively (see Section \ref{subsec:al_methodology} for interpretation of ``perfect" and ``bad"). We observe that as the iterations progress, the amount of annotation time decreases, which can be attributed to the model's generalization ability with more labeled training data.
}

%Clearly, \rqw{our method with AL achieves the highest accuracy with a few time of human efforts. Table \ref{tab:al_iter} shows the detailed statistics of each iteration in AL, where the fourth column represents the numbers of bad images and corresponding annotations done by human. We compute the time cost based on the number of test results and manual annotations. Predictions on the test set are sorted based on the average prediction confidence and we consider a prediction as \emph{bad} if there is no correct prediction with over 75\% confidence. We manually go through predictions in order and select \emph{perfect} predictions, which include all interactable parts in the image with fine-grained segmentation mask. We estimates the time for checking test sets as 3 seconds per image, and manual annotation on bad predictions as 15 seconds per annotation based on our experiments. Perfect predictions are automatically merged into the training set without any human time cost.}

\input{tables/timing_al_vs_all_methods}

\agp{
\subsection{Ablation studies with active learning}
In Table \ref{tab:al_ablation_ourcomp}, we show timing comparisons for annotating enhancement set images (refer to Section \ref{subsec:al_methodology} for terminology) using different versions of our model. We again observe that the pose estimation module (Row ID 2) provides better segmentation masks on unseen images compared to the object mask prediction module (Row ID 1), resulting in less human intervention for rectifying the masks, as recorded by the annotation time in the last column. Our proposed model (Row ID 3) requires minimum time effort from humans, validating, yet again, our network design choices.
}

%\rqw{\subsection{Ablation studies with active learning}
%We perform ablations on time cost of AL using different methods, which aims to show the time savings using AL for labeling and segmentation task. Table~\ref{tab:al_on_all_methods} verifies the contribution of pose-aware masked attention scheme of our method on minimizing human labeling efforts. We compare our full method against the baseline with different settings of modules. It is clearly that our full method (last row) is the most efficient. 

%To further evaluate the efficiency of AL, we apply different methods to label our \emph{enhancement} set. Table~\ref{tab:al_ablation_ourcomp} presents the quantitative evaluation results on human annotation and time cost. The second column of the table shows the corresponding numbers of manual annotated images and annotations of different methods. Without AL, both OPDRCNN-C and Mask2Former can barely generate a few \emph{perfect} annotations and most images need to be labeled manually. Our method with AL is capable for accurately labeling all images in the \emph{enhancement} set within less than 2 hours of manual annotation on 16.7\% of images, which significantly reduces the time of human efforts for 76.8\%. 
%}

\input{tables/al_iter}

\input{tables/al_ablation}



\begin{figure*}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/qual-ours.pdf}
    \vspace{-10 pt}
    \caption{Qualitative results for test-set images from our dataset. Left to right -- Prediction results using OPDRCNN-C \cite{jiang2022opd}, Mask2Former\cite{cheng2022masked}, our method using only the object mask prediction branch (i.e., without the pose estimation branch), our method using only the pose estimation branch (i.e., without the object mask prediction branch), our method (i.e., with both prediction branches), and the ground truth (GT). We show results on different object categories, and observe that our method outputs better segmentation masks over interactable parts, even when multiple objects exist in the input image. Our results also show that the existence of object mask predictor and pose estimator modules can effectively reduce segmentation errors from unwanted objects (second row) and object side surfaces(first, fifth and last rows). More results in the supplementary material.}
    \label{fig:viz_ours}
\end{figure*}
