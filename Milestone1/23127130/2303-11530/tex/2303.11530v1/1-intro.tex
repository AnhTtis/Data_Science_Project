%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intrp}

%\begin{comment}
%
%\agp{AGP: Points for Richard
%\begin{enumerate}
%    \item Mention that the goal is to obtain part masks for interactable objects in real images where part labeled data is not present/hard to obtain
%    \item Achieved using an active-learning framework
%    \item The starting point, however, is training on synthetic images of articulate-able objects. Why? Bcz there is data to train on them
%    \item But such trained models can not be expected to do well on real images due to the domain gap
%    \item So, our idea is to leverage trained models on synthetic data and employ it in an active-learning framework for generalization on real images
%    \item Important to stress that we are not trying to do domain adaptation as defined in the traditional sense
%    \item Then, mention that there exists a real-images dataset of articulated objects, OPDReal. OPD has provided a solution to obtaining segmentation masks+motion parameters on these real images. So, why do this work?
%    \item X reasons: OPDReal dataset is low-quality, in the sense that the so-called ``ground truth" 2D part segmentation masks are not accurate. They were obtained completely through manual annotations (I know for a fact that they were annotated by students in the 3DLG group, and it appears that there was no Quality Assurance done on these annotations). Moreover, these images are taken from a very close distance of the camera to the object. Practical scenarios and use cases will vary. Not sure if OPD will perform well in such situations 
%    \item As such, we collect our own set of real-world images, with varying camera poses and distance to the object. This is one of our contributions.
%    \item To RQ -- With the above claims, we have to show results, both Quant and Qual, using comparative methods on our real images, as well as OPDReal. 
%    \item All the above make a good narrative
%    \item Now, the details. 
%    \item We propose a pose-aware mask attention network for obtaining 2D segmentation masks on interactable parts of an object as observed in the input image
%    \item Then, comes the active-learning part. Mention again that we are not doing domain adaptation as defined in the traditional sense. 
%    \item Then, summarize experiments; the kind of methods we compare against, how do we perform; what percentage of complete manual effort is present in our AL framework, where effort is measured in terms of time; how much is the jump in accuracy when AL is employed; does the jump in accuracy correlate with a reduction of manual effort, and finally, is this correlation justified
%\end{enumerate}
%
%\iffalse
%My concerns: (1) Involving humans-in-the-loop is a little weak to sell, (2) We only focus on 2D segmentations and do not detect motion parameters, unlike OPD. This weakens our position a little.
%\fi
%
%}
%
%\fg{Dynamic part segmentation of 2D objects is important for many applications, such as Robotics.}\fgc{Write more sentences here.}
%
%\end{comment}

Most objects we interact with in our daily lives have dynamic movable parts, where the part movements reflect how the objects function.
Perceptually, acquiring a visual and actionable understanding of object functionality is a fundamental task.
%
In recent years, motion perception and functional understanding of articulated objects have received increasing attention in computer vision, robotics, and VR/AR applications. %As a very common component of the indoor scene in both synthetic and natural environments, articulated objects involve different human interactions and manipulations. 
Aside from per-pixel or per-point motion prediction, the {\em segmentation\/} of dynamic, {\em interactable} parts serves as the basis for many downstream tasks, including robot manipulation, action planning, and part-based 3D reconstruction.

\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{figures/method-overview.pdf}
  \caption{\agp{Overview of our coarse-to-fine active segmentation method for interactable parts in real scene images. Our active learning setup (shown on the left; see Section \ref{subsec:al_methodology}) makes use of a synthetically trained, transformer-based, coarse-to-fine 2D segmentation model (shown on the right; see Section \ref{subsec:segm_transf}) to obtain segmentation masks on unseen real images through iterative refinement via human-in-the-loop feedback.}}
  \label{fig:method-overview}
\end{figure*}

In this paper, we tackle the problem of {\em instance\/} segmentation of interactable parts from RGB images of {\em real indoor scenes\/}. 
Most prior works on such segmentations~\cite{yan2020rpm,li2020category,huang2021multibodysync} operate on point clouds, which are more
expensive to capture than images while having lower resolution, noise, and outliers.
% Also, point samples acquired in real-world setting are often tempered with noise, missing data, and outliers. 
To our knowledge, OPD~\cite{jiang2022opd}, for ``openable part detection", represents the state-of-the-art in interactable part
segmentation from images. However, their method was trained and evaluated only on {\em single\/} objects, not scenes, and there remains a 
large gap between synthetic and real test performances. \rz{The best reported accuracy on dynamic part segmentation from real images, by OPD, is only 45\%.}
 
%require the depth information. With the development of embodied AI, many 3D interaction-based datasets are proposed in synthetic domain\cite{Xiang_2020_SAPIEN, Mo_2019_CVPR} to support these tasks. However, in real-world applications, image-based learning methods are more effective in data collection, model training, and product designs. Although 2D images can be easily obtained from 3D object projections, the domain gap between synthetic and real domains is still significant.}

Typical approaches to close the synthetic-to-real gap rely on domain adaptation using annotated real images, but the manual annotation process is highly
tedious for instance segmentation. To this end, OPD\cite{jiang2022opd} opted to manually annotate {\em mesh\/} models of real articulated 3D objects and 
then render them from many views to obtain OPDReal, a dataset of about \rz{20K} annotated images. However, there is an inevitable gap between projected
images of {\em digitally reconstructed\/} 3D meshes and real photographs, with both reconstruction errors and re-projection errors further hindering image quality.
%The OPDReal dataset is also lacking in diversity, \rz{with 92\% of the models being storage furniture.}

% \fg{The most recent work in 2D dynamic part segmentation is OPD\cite{jiang2022opd}, which annotated part segmentation in 3D polygonal meshes to generate many views of images with ground truth. However, the quality of their segmentation is not guaranteed due to the projection error from 3D to 2D. It is challenging to produce a high quality segmentation results \emph{only} from deep-learning based methods.}

To address the above challenges, we present an {\em active learning\/} (AL)~\cite{AL_survey2014,AL_survey2020,AL_comp_survey2022} approach to obtain 
high-accuracy instance segmentation of interactable parts from real scene images.
%
AL is a semi-supervised learning paradigm, relying on human feedback to continually improve the performance of a neural segmentation model. 
As with most human-in-the-loop approaches, the key criterion for success in AL is to minimize human
effort. To this end, we employ a transformer-based~\cite{dosovitskiy2020image} segmentation network that utilizes a masked-attention
mechanism~\cite{cheng2022masked}. To enhance the network for interactable part segmentation, we introduce a {\em coarse-to-fine\/} model which first uses an
{\em object-aware\/} masked attention and then a {\em pose-aware\/} one, leveraging a correlation between dynamic parts and object poses and leading to improved handling of multiple articulated 
objects in an image.

Our coarse-to-fine segmentation method learns both 2D instance and 3D pose information using the transformer network, which supervises the active segmentation and effectively 
reduces human effort. Unlike prior works on active segmentation~\cite{xie2022towards,tang2022active} which mainly focused on the efficiency of human annotation, our network learns the region-of-interests (ROI) from the pose-aware masked-attention decoder for better segmentation sampling in AL iterations. 

%It has already been adopted to produce high-accuracy segmentation results, e.g.,~\cite{wu2022d,siddiqui2020viewal,xie2022towards,tang2022active, xie2022towards}.

%\rqw{Targeting the difficulties in improving model performance with limited data, researchers additionally used an active learning based pipeline to involve human efforts to achieve high accuracy segmentation results\cite{wu2022d, siddiqui2020viewal, xie2022towards}. Most active learning based algorithms for segmentation tasks denote uncertainty-based active domain adaptation to acquire labels nearby the decision boundary, which targets the alignment of two domains. However, the synthetic data of articulated object usually does not contain any meaningful background, which is difficult to align features in the real domain. }

% \fg{Since there could be a data gap between train set and test set, the deep-learning based 2D segmentation methods are hard to achieve a perfect segmentation result for large-scale test set, researchers additionally used an active learning based pipeline to involve human efforts and achieve high accuracy segmentation results\cite{wu2022d, siddiqui2020viewal, xie2022towards}. However, Most active learning(AL) based algorithms for segmentation task\cite{wu2022d, siddiqui2020viewal, xie2022towards} denote uncertainty-based active domain adaptation to acquire labels nearby decision boundary, which targets the alignment of two domains. However, this strategy will fail for the dynamic part segmentation task, since there is a large data gap between the synthetic train set in SAPIEN\cite{Xiang_2020_SAPIEN} and real test images domain.}

In summary, our main contributions include:

\begin{itemize}
\vspace{-3pt}
\item We introduce the first active learning framework for instance segmentation of dynamic/interactable parts from RGB images of real indoor scenes. Our method achieves close to fully accurate (\rz{96\% and higher}) segmentation results on real images, with \rz{77\% time saving} over manual effort, where the training data consisting of \rz{only 16.6\% annotated real photographs.}
\vspace{-3pt}
\item %Targeting feature learning for articulated objects in indoor scenes, w
We present a coarse-to-fine, object- and pose-aware masked-attention mechanism for active segmentation, %taking both 2D instance and 3D pose information as supervised signals, 
leading to reduced human effort in AL and improved %of the transformer network 
interactable part segmentation over state-of-the-art methods, including OPD~\cite{jiang2022opd} and Mask2Former~\cite{cheng2022masked}.
\vspace{-3pt}
\item Our AL method has allowed us to annotate a dataset of 2,550 real photographs of articulated objects in indoor scenes. We show the superior quality and diversity of our new dataset over OPDReal, and the resulting improvements in segmentation accuracy. % by all the three trained segmentation modules: OPD, Mask2Former, and ours.
\end{itemize}
