\section{Datasets and Pre-training}
\label{sec:datasets}
\agp{
\textbf{Datasets.} We use two kinds of real image datasets in our experiments: (1) images from the OPDReal dataset \cite{jiang2022opd}, and (2) images from our dataset. Our dataset images are obtained from the real world by taking photographs of interactable objects in indoor scenes from furniture stores, offices, and homes, captured using iPhone12 Pro and iPhone14. On average, each interactable object is photographed from five distinct viewpoints with varying camera poses and distances from the objects. Also, a captured image can contain more than one object with interactable parts. As such, our dataset is quite diverse compared to OPDReal, where objects are scanned from within a limited distance range.% of the interactable object. 

For both datasets, we consider six object categories -- Storage, Fridge, Dishwasher, Microwave, Washer, and Oven. The data distribution of 3D objects per category and their 2D images are shown in Table \ref{tab:data_dist}. In total, our dataset contains 2,550 images and OPDReal contains $\sim$23K images. Unlike OPDReal where part segmentation masks are manually annotated on a 3D mesh and then projected back to the image space, for our dataset, we obtain manual annotations (i.e., the ground truth) for 2D segmentation masks directly on the captured images. Note that such manual annotations are used only to evaluate our active learning framework. %, around nine times the size of our dataset.

From Table \ref{tab:data_dist}, we observe that the majority of data samples in OPDReal belong to the Storage category (91.67\%), with the rest distributed among the remaining categories. The difference in distribution between the largest and the second largest category is 87.74\%, and between the largest and smallest categories is 91.14\%. With such data skewness towards one category, models trained on OPDReal will likely overfit to the dominant category.%, and do not generalize well on a variety of real world images containing different categories; see Table \ref{tab:mAP_table}. 
Our dataset, on the other hand, contains smaller variations in data distributions across the six object categories, where the difference in data distribution between the largest and second largest categories is 5.88\%, and between the largest and smallest categories is 29.47\%. \\
% \vspace{-5 pt}
\newline
\textbf{Pre-training.} We begin our experiments by rendering synthetic models from the PartNet-Mobility dataset \cite{Xiang_2020_SAPIEN} in various articulation states since this enables us to obtain sufficient annotations to train 2D segmentation networks and thus enable transfer learning applications. Our synthetic dataset contains around 32K articulation images, with equal data samples for each object category. We use a 90\%-10\% train-test split to train on this synthetic dataset, and use this trained model for fine-tuning on real images. To this end, we use all data samples from both datasets, OPDReal and ours, with 80\%-20\% train-test split.\\
\newline
We implement our network in PyTorch on a Nvidia RTX 2080 Ti GPU. All images are resized to 256$\times$256 for training. During pre-training on the PartNet-Mobility dataset, we use the Adam optimizer with a learning rate (lr) of 2.5e-4, and train for 2K epochs. When fine-tuning on real images, we use the same lr and run for 4.5K epochs.

%As we show in Table \ref{tab:mAP_table}, models trained on our dataset achieve better performance compared to when trained using OPDReal dataset.
}