%%%%%%%%% ABSTRACT
\begin{abstract}
We introduce the first {\em active learning\/} (AL) framework for high-accuracy instance segmentation of {\em dynamic}, {\em interactable\/} parts from RGB images
of {\em real indoor scenes}. As with most human-in-the-loop approaches, the key criterion for success in AL is to minimize human effort while still attaining high 
performance. To this end, we employ a transformer-based segmentation network that utilizes a masked-attention mechanism. To enhance the network, tailoring to our task,
we introduce a {\em coarse-to-fine\/} model which first uses {\em object-aware\/} masked attention and then a {\em pose-aware\/} one, leveraging a correlation between
%dynamic 
interactable parts and object poses and leading to improved handling of multiple articulated objects in an image. Our coarse-to-fine active segmentation module learns both 
2D instance and 3D pose information using the transformer, which supervises the active segmentation and effectively reduces human effort.
%
Our method achieves close to fully accurate (\rz{96\% and higher}) segmentation results on real images, with \rz{77\% time saving} over manual effort, where the training 
data consists of \rz{only 16.6\%} annotated real photographs.
%
At last, we contribute a dataset of 2,550 real photographs with annotated interactable parts, demonstrating its superior quality and diversity over the current best alternative.
\end{abstract}