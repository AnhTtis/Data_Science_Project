\section{Related Works}
\label{sec:rw}
% \rqw{
% \paragraph{Parts segmentation of articulated objects.}

% \paragraph{Active learning for image segmentation.}


% \paragraph{}
% }


\agp{
\paragraph{Articulated objects dataset.}
The last few years have seen the development of articulation datasets on 3D shapes. Of the many, ICON \cite{hu2017learning} build a dataset of 368 moving joints corresponding to various parts of 3D shapes from the ShapeNet dataset~\cite{chang2015shapenet}. The Shape2Motion dataset \cite{wang2019shape2motion} provides kinematic motions for 2,240 3D objects across 45 categories sourced from ShapeNet and 3D Warehouse~\cite{trimble_warehouse}. The PartNet-Mobility dataset~\cite{Xiang_2020_SAPIEN} consists of 2,374 3D objects across 47 categories from the PartNet dataset~\cite{Mo_2019_CVPR}, providing motion annotations and part segmentation in 3D. 

All these datasets are obtained via manual annotations and are \emph{synthetic} in nature. Since sufficient training data is made available by these synthetic datasets, models trained on them can be used for fine-tuning on \emph{real-world} 3D articulated object datasets with limited annotations.

Recently, OPD~\cite{jiang2022opd} and its follow-up work OPDMulti~\cite{sun2024opdm}, provide two 2D image datasets of real-world articulated objects: OPDReal and OPDMulti. In OPDReal, images are obtained from frames of RGB-D scans of indoor scenes containing a single object. OPDMulti, on the other hand, captures multiple objects. Both datasets come with 2D segmentation labels on all \emph{openable} parts along with their motion parameters. However, due to the nature of annotation process, the 2D part segmentation masks obtained via 3D-to-2D projection do not fully cover all openable parts in the image. Also, in OPDReal, objects are scanned from within a limited distance range. Practical scenarios and use cases are likely going to have large camera pose and distance variations. OPDMulti, on the other hand, although incorporates such viewpoint variations, a large portion of this dataset contains frames without any articulated objects \cite{sun2024opdm}, which directly affects model training on OPDMulti.

To overcome these limitations, we contribute a 2D image dataset of \xeable objects present in the real world (furniture stores, offices, homes), captured using iPhone 12 Pro and 14. We then use our \emph{coarse-to-fine} AL framework (Figure~\ref{fig:al-pipeline} and Section \ref{sec:method}) to learn generalized 2D segmentations for \xeable object parts.

% Talk about articulation datasets in 3D, including on humans/animals. Then, talk about their 2D counterpart, if there exists any. Here you will need to write about OPDSynth. Then, you talk about OPDReal. Say what issues exist with OPDReal and how our dataset is different from it.
\vspace{-10 pt}
\paragraph{Part segmentation in images.}
Early approaches \cite{wang2015joint, wang2015semantic, xia2017joint} to 2D semantic part segmentation developed probabilistic models on human and animal images. While not addressing the 2D semantic part segmentation problem as such, \cite{huang2020arch, mehta2017vnect, ballan2012motion, kanazawa2018end, mueller2018ganerated} tackled the problem of estimating 3D articulations from human images, which requires an understanding of articulated regions in the input image. 

% Recently, with the availability of 3D part datasets \cite{Mo_2019_CVPR, Xiang_2020_SAPIEN}, there have been works that estimate 3D articulations from articulation images \cite{abbatematteo2019learning, li2020category, zhang2021strobenet, patil2023rosi}. However, most of them learn a latent space of 3D articulation parameters and do not provide 2D segmentation masks for openable parts.
% Our work aims at segmenting \emph{openable} parts of a 3D object from image input. To our knowledge, OPD \cite{jiang2022opd} is the only work that can segment such object parts given an input image, and is built on the Mask RCNN architecture \cite{he2017mask}. In our work, we employ the Mask2Former \cite{cheng2022masked} architecture with task-specific modifications as described in Section \ref{sec:method}.
Recently, the development of large visual models, such as SAM \cite{kirillov2023segany}, has addressed classical 2D vision tasks, such as object segmentation, surpassing all existing models. Such large pre-trained models can be directly employed for \emph{zero-shot} segmentation on new datasets. Follow-up works \cite{zou2023segment, Grounded-SAM_Contributors_Grounded-Segment-Anything_2023} to SAM aim at multi-modal learning by generalizing to natural language prompts. For the task of \xeable part segmentation in real scene images, we observe an unsatisfactory performance using such models. This is expected since they were never trained on any \xeable parts datasets, and therefore, lack an understanding of articulated objects. To our knowledge, OPDFormer~\cite{sun2024opdm} is the state-of-the-art that can segment \xeable parts in an input image, and is built on the Mask2Former architecture~\cite{cheng2022masked}.
In our work, we use a transformer architecture in a \emph{coarse-to-fine} manner to obtain \xeable part segmentation (Section \ref{sec:method}).


% with the development of LLMs, numerous robust generic models have begun to dominate classic 2D visioin tasks. SAM~\cite{kirillov2023segany} spearheads this trend by demonstrating exceptional capabilities in segmenting unfamiliar objects and images without additional training. There have been works that combine SAM with multi-modal prompts to generalize on open-set tasks~\cite{zou2023segment, Grounded-SAM_Contributors_Grounded-Segment-Anything_2023}. However, without sufficient training data and functionality understanding of articulated objects, these models fail to encode the part information and segment parts in articulated objects correctly. In our work, we aim to segment \xeable parts of multiple articulated objects in images. To our knowledge, OPDFormer~\cite{sun2024opdm} is the state-of-the-art that can segment such parts given an input image, and it built on the Mask2Former architecture~\cite{cheng2022masked}. In our work, we use the hierarchical nature of the problem, leveraging a \emph{coarse-to-fine} Transformer-based segmentation network (Section \ref{sec:method}).} 
% }

\vspace{-10 pt}
\paragraph{Active learning for image segmentation.}
Active learning (AL) is a well-known technique for improving model performance with limited labeled data. \agp{This, in turn, allows the expansion of labeled datasets for downstream tasks.} Prior works \cite{seneractive, Sinha_2019_ICCV, casanovareinforced, Xie_2020_ACCV, Shin_2021_ICCV} have presented different AL frameworks to acquire labels with minimum cost for 2D segmentation tasks. There exist AL algorithms for such tasks \cite{ning2021multi, wu2022d} that are specifically designed to reduce the domain gap by aligning two data distributions. We cannot borrow such methods to reduce the domain gap between synthetic and real scene images of \xeable objects because of the large feature differences (our synthetic images contain no background, unlike real scene images).
%Most active learning based algorithms for segmentation tasks denote uncertainty-based active domain adaptation to acquire labels nearby the decision boundary\cite{ning2021multi, wu2022d}, which targets the alignment of two domains. However, the synthetic data of articulated object usually does not contain any meaningful background, which is difficult to align features in the real domain.

More recently, \cite{tang2022active, xie2022towards} employed AL to refine initial 2D segmentation masks through key point or region selection, requiring little human guidance. Due to potentially multiple \xeable parts, such point/region selection is ambiguous for articulated objects. As such, we design an AL framework that reduces manual effort by focusing on: (a) using an improved part segmentation model (Section~\ref{subsec:network}), and (b) employing a \emph{coarse-to-fine} strategy (Section \ref{subsec:almethod}).  %In our task, the selection of point and region are difficult within a same object, and in most cases, the initial prediction mask has very low quality. Instead of reducing human efforts in annotation, our method focus on getting a better segmentation sampling in AL, with the selection of region-of-interests (ROIs) from pose aware masked attention decoder. }

% \cite{sinha2019variational} introduces a mechanism learning representative sampling in an adversarial manner between unlabeled and labeled data. \cite{shin2021all, cai2021revisiting} involve pixel-level guidance to assist annotations. 

}
