\section{Datasets and Metrics}
\label{sec:datasets}


\agp{
\paragraph{Datasets.} 
We use three kinds of real image datasets in our experiments: (1) OPDReal~\cite{jiang2022opd}, (2) OPDMulti~\cite{sun2024opdm}, and (3) our dataset. 
Our dataset images are obtained from the real world by taking photographs of articulated objects in indoor scenes from furniture stores, offices, and homes, captured using iPhone 11Pro, 12, 12Pro and 14. Images are captured with varying camera poses and distances
from the objects, and an image can contain more than one object, with multiple \xeable parts per object. Differences to OPD and OPDMulti datasets are explained in Section \ref{sec:rw}.


% Different from OPDReal and OPDMulti selecting frames from video data, our dataset comprises images captured through direct photography of indoor scenes in real world. These photographs are rich in detail and resolution, featuring multiple articulated objects with multiple \xeable parts captured from various viewpoints and distances. 

We consider six object categories -- Storage, Fridge, Dishwasher, Microwave, Washer, and Oven. A comparison of dataset statistics is presented in Table~\ref{tab:data_dist}. OPDReal comprises of $\sim$30K images, with each image depicting a single articulated object. OPDMulti contains $\sim$64K images. Among these, only 19K images are considered ``valid", containing at least one articulated object. \rqw{Our dataset has a total of 2,550 images, with each image showcasing objects from several categories. We organize our dataset according to the primary object depicted in each image. Our dataset stands out by offering the highest diversity of objects and parts among the compared datasets, including 333 different articulated objects and 1,161 distinct parts. }
% Our dataset has a total of 2,550 images and exhibits larger diversity in terms of constituent objects and their \xeable parts. In total, our dataset consists of 333 different articulated objects and 1,161 distinct parts. 

In terms of the \xeable part annotation, both OPDReal and OPDMulti generate annotations on a 3D mesh reconstructed from the RGB-D scans, and project these 3D annotations back to the 2D image space to get 2D part masks. This process is prone to reconstruction and projection errors. We, on the other hand, create annotations on the captured images directly using our \emph{coarse-to-fine} active learning framework. See the supplementary material for annotation quality comparisons. %We provide a detailed comparison of annotation quality across datasets in the supplementary material. 

From Table 1, we observe that the majority of data samples in OPDReal belong to the Storage category (91.67\%), with the rest ($<$10\%) distributed among the remaining categories. In contrast, our dataset offers a more uniform data distribution across all six categories. \vspace{-10pt}
\paragraph{Metrics.}
To evaluate model performance and active learning efficiency, we use the following metrics:
\begin{itemize}
    \item \textbf{Mean Average Precision (mAP)}: Following OPD \cite{jiang2022opd}, we report mAP@IoU=0.5 for correctly predicting the part label and 2D mask segmentation with IoU $\geq$ 0.5. 
    \item \textbf{Annotation time}: To evaluate the AL setup, we measure manual annotation \emph{and} verification times. For methods without AL, we record the time for labeling only the \emph{test set}, which is the manual annotation time on network segmentation predictions. 
\end{itemize}
}


