%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intrp}

%\begin{comment}
%
%\agp{AGP: Points for Richard
%\begin{enumerate}
%    \item Mention that the goal is to obtain part masks for interactable objects in real images where part labeled data is not present/hard to obtain
%    \item Achieved using an active-learning framework
%    \item The starting point, however, is training on synthetic images of articulate-able objects. Why? Bcz there is data to train on them
%    \item But such trained models can not be expected to do well on real images due to the domain gap
%    \item So, our idea is to leverage trained models on synthetic data and employ it in an active-learning framework for generalization on real images
%    \item Important to stress that we are not trying to do domain adaptation as defined in the traditional sense
%    \item Then, mention that there exists a real-images dataset of articulated objects, OPDReal. OPD has provided a solution to obtaining segmentation masks+motion parameters on these real images. So, why do this work?
%    \item X reasons: OPDReal dataset is low-quality, in the sense that the so-called ``ground truth" 2D part segmentation masks are not accurate. They were obtained completely through manual annotations (I know for a fact that they were annotated by students in the 3DLG group, and it appears that there was no Quality Assurance done on these annotations). Moreover, these images are taken from a very close distance of the camera to the object. Practical scenarios and use cases will vary. Not sure if OPD will perform well in such situations 
%    \item As such, we collect our own set of real-world images, with varying camera poses and distance to the object. This is one of our contributions.
%    \item To RQ -- With the above claims, we have to show results, both Quant and Qual, using comparative methods on our real images, as well as OPDReal. 
%    \item All the above make a good narrative
%    \item Now, the details. 
%    \item We propose a pose-aware mask attention network for obtaining 2D segmentation masks on interactable parts of an object as observed in the input image
%    \item Then, comes the active-learning part. Mention again that we are not doing domain adaptation as defined in the traditional sense. 
%    \item Then, summarize experiments; the kind of methods we compare against, how do we perform; what percentage of complete manual effort is present in our AL framework, where effort is measured in terms of time; how much is the jump in accuracy when AL is employed; does the jump in accuracy correlate with a reduction of manual effort, and finally, is this correlation justified
%\end{enumerate}
%
%\iffalse
%My concerns: (1) Involving humans-in-the-loop is a little weak to sell, (2) We only focus on 2D segmentations and do not detect motion parameters, unlike OPD. This weakens our position a little.
%\fi
%
%}
%
%\fg{Dynamic part segmentation of 2D objects is important for many applications, such as Robotics.}\fgc{Write more sentences here.}
%
%\end{comment}

Most objects we interact with in our daily lives have dynamic movable parts, where the part movements reflect how the objects function.
Perceptually, acquiring a visual and actionable understanding of object functionality is a fundamental task.
%
In recent years, motion perception and functional understanding of articulated objects have received increasing attention in computer vision, robotics, and VR/AR applications. %As a very common component of the indoor scene in both synthetic and natural environments, articulated objects involve different human interactions and manipulations. 
Aside from per-pixel or per-point motion prediction, the {\em segmentation\/} of \xeable parts serves as the basis for many downstream tasks, including robot manipulation, action planning, and part-based 3D reconstruction.

\begin{figure*}
\centering
  \includegraphics[width=0.99\textwidth]{figs/method-overview.pdf}
  \caption{\rqw{Overview of our pose-aware masked attention network for \xeable part segmentation of articulated objects in real scene images. Utilizing a two-stage framework, we first derive a \emph{coarse} segmentation by predicting the object mask, its 6 DoF pose, and the interaction direction, subsequently isolating the interaction surface of the objects. In the \emph{fine} segmentation stage, we combine the object mask and interaction surface to form a refined mask, enabling the extraction of fine-grained instance segmentation of \xeable parts.} }
  \label{fig:method}
  \vspace{-10 pt}
\end{figure*}

In this paper, we tackle the problem of {\em instance\/} segmentation of \xeable parts \rz{in one or more articulated objects} from RGB {\em images of real indoor scenes\/}, \rz{as shown in Figure~\ref{fig:teaser}.}
\rz{Note that we use the term articulated objects in a somewhat loose sense to refer to all objects whose parts can undergo motions; such motions can include
opening a cabinet door, pulling a drawer\footnote{Strictly speaking, articulated motions are realized by ``two or more sections connected by a {\em flexible\/} joint,'' which would not
include the drawer opening motion. However, as has been done in other works in vision and robotics, we use the term loosely to encompass more general part motions.}, and the movements of a
lamp arm.}
Most prior works on \rz{motion-related} segmentations~\cite{yan2020rpm,li2020category,huang2021multibodysync} operate on point clouds, which are more
expensive to capture than images while having lower resolution, noise, and outliers.
\rz{Latest advances on large language models (LLMs) have led to the development of powerful generic models such as SAM~\cite{kirillov2023segany}, which can
excel at generating quality object masks and exhibit robust zero-shot performance across diverse tasks owing to their extensive training data. However, these methods 
remain limited in comprehensive understanding of \xeable object parts.} % to achieve high part segmentation accuracy.}
% Also, point samples acquired in real-world setting are often tempered with noise, missing data, and outliers. 

To our knowledge, OPD~\cite{jiang2022opd}, for ``openable part detection", \rz{and its follow-up, OPDMulti~\cite{sun2024opdm}, for ``openable part detection for multiple objects'', represent} 
the state of the art in \xeable part segmentation from images. \rz{However, despite the fact that both methods were trained on real object/scene images, there still remains a 
large gap between synthetic and real test performances: roughly 75\% vs.~30\% in segmentation accuracy~\cite{sun2024opdm}. The main reason is that manual instance segmentation on real images to form ground-truth training data is too costly. Hence,
as a remedy, OPD and OPDMulti both opted to manually annotate 3D mesh or RGB-D {\em reconstructions\/} from real-world articulated object scans and project the obtained segmentation
masks onto the real images. Clearly, such an indirect annotation still leaves a gap between rendered images of digitally reconstructed 3D models and real photographs, with both reconstruction errors and 
re-projection errors due to view discrepancies hindering the annotation quality on images.}

%then render them from many views to obtain OPDReal, a dataset of about \rz{20K} annotated images. However, 
%However, these methods were trained and evaluated only on {\em single\/} objects, not scenes, and  The best reported accuracy on dynamic part segmentation from real images, by OPD, is only 45\%.}
 
%require the depth information. With the development of embodied AI, many 3D interaction-based datasets are proposed in synthetic domain\cite{Xiang_2020_SAPIEN, Mo_2019_CVPR} to support these tasks. However, in real-world applications, image-based learning methods are more effective in data collection, model training, and product designs. Although 2D images can be easily obtained from 3D object projections, the domain gap between synthetic and real domains is still significant.}

\if 0 % OLD paragraph
Typical approaches to close the synthetic-to-real gap rely on domain adaptation using annotated real images, but the manual annotation process is highly
tedious for instance segmentation. To this end, OPD\cite{jiang2022opd} opted to manually annotate {\em mesh\/} models of real articulated 3D objects and 
then render them from many views to obtain OPDReal, a dataset of about \rz{20K} annotated images. However, there is an inevitable gap between projected
images of {\em digitally reconstructed\/} 3D meshes and real photographs, with both reconstruction errors and re-projection errors further hindering image quality.
%The OPDReal dataset is also lacking in diversity, \rz{with 92\% of the models being storage furniture.}

% \fg{The most recent work in 2D dynamic part segmentation is OPD\cite{jiang2022opd}, which annotated part segmentation in 3D polygonal meshes to generate many views of images with ground truth. However, the quality of their segmentation is not guaranteed due to the projection error from 3D to 2D. It is challenging to produce a high quality segmentation results \emph{only} from deep-learning based methods.}
\fi

\rz{To close the aforementioned gap by addressing the annotation challenge}, we present an {\em active learning\/} (AL)~\cite{AL_survey2014,AL_survey2020,AL_comp_survey2022} 
approach to obtain high-accuracy instance segmentation of \xeable parts, \rz{with semantic labels, {\em directly on\/}} real scene images \rz{containing one or more articulated objects.}
%
AL is a semi-supervised learning paradigm, relying on human feedback to continually improve the performance of a neural segmentation model. 
As with most human-in-the-loop approaches, the key criterion for success in AL is to minimize human
effort. To this end, we employ a transformer-based~\cite{dosovitskiy2020image} segmentation network that utilizes a masked-attention
mechanism~\cite{cheng2022masked}. To enhance the network for \xeable part segmentation, we introduce a {\em coarse-to-fine\/} AL model which first uses an
{\em object-aware\/} masked attention and then a {\em pose-aware\/} one, leveraging \rz{the hierarchical nature of the problem} and a correlation between \xeable parts and \rz{object poses
and interaction directions.} % and leading to improved handling of multiple articulated  objects in an image.

\rz{As shown in Figure~\ref{fig:method}, in the coarse annotation stage, our AL model with object-aware attention predicts object masks, poses, and interaction directions, so as to help isolate 
interaction surfaces on the articulated objects. In the fine annotation stage, we combine the object masks and interaction surfaces to predict refined segmentation masks for
\xeable object parts, also with human-in-the-loop.} Unlike prior works on active segmentation~\cite{xie2022towards,tang2022active} which mainly focused on the efficiency of human 
annotations, our network learns the regions-of-interests (ROIs) from the pose-aware masked-attention decoder for better segmentation sampling in AL iterations \rz{in the second stage.}

\if 0
\rqc{In the previous submission, we only apply the general AL method, which verifies the final output only, in this revision we improve this part: compared to general active learning methods applied to the final output, our approach employs a two-step training strategy, encompassing \emph{coarse} and \emph{fine} feature AL. Our \emph{coarse-to-fine} AL strategy accelerates the human annotation process and leverages the inherent hierarchical structure of the model, resulting in improved efficiency and enhanced model performance. }
Our coarse-to-fine segmentation method learns both 2D instance and 3D pose information using the transformer network, which supervises the active segmentation and effectively 
reduces human effort. \rqc{Our network learns both object interact direction and ROI}Unlike prior works on active segmentation~\cite{xie2022towards,tang2022active} which mainly focused on the efficiency of human annotation, our network learns the region-of-interests (ROI) from the pose-aware masked-attention decoder for better segmentation sampling in AL iterations. \rqc{Generalization: finally, we show the generalization of our method to other moveable parts.}
\fi

%It has already been adopted to produce high-accuracy segmentation results, e.g.,~\cite{wu2022d,siddiqui2020viewal,xie2022towards,tang2022active, xie2022towards}.

%\rqw{Targeting the difficulties in improving model performance with limited data, researchers additionally used an active learning based pipeline to involve human efforts to achieve high accuracy segmentation results\cite{wu2022d, siddiqui2020viewal, xie2022towards}. Most active learning based algorithms for segmentation tasks denote uncertainty-based active domain adaptation to acquire labels nearby the decision boundary, which targets the alignment of two domains. However, the synthetic data of articulated object usually does not contain any meaningful background, which is difficult to align features in the real domain. }

% \fg{Since there could be a data gap between train set and test set, the deep-learning based 2D segmentation methods are hard to achieve a perfect segmentation result for large-scale test set, researchers additionally used an active learning based pipeline to involve human efforts and achieve high accuracy segmentation results\cite{wu2022d, siddiqui2020viewal, xie2022towards}. However, Most active learning(AL) based algorithms for segmentation task\cite{wu2022d, siddiqui2020viewal, xie2022towards} denote uncertainty-based active domain adaptation to acquire labels nearby decision boundary, which targets the alignment of two domains. However, this strategy will fail for the dynamic part segmentation task, since there is a large data gap between the synthetic train set in SAPIEN\cite{Xiang_2020_SAPIEN} and real test images domain.}

In summary, our main contributions include:

\begin{itemize}
% \vspace{-3pt}
\item We introduce the first AL framework for instance segmentation, \rz{with semantic labels,} of \xeable parts from RGB images of real indoor scenes. Our method achieves close to fully accurate (\rqw{96\% and higher}) segmentations, with \rqw{82\% time saving} over manual effort, where the training data contains \rqw{only 11.45\%} annotated real images.
% \vspace{-3pt}
\item %Targeting feature learning for articulated objects in indoor scenes, w
Our coarse-to-fine active segmentation model, with both object- and pose-aware masked-attention mechanisms, %taking both 2D instance and 3D pose information as supervised signals, 
lead to reduced human effort and improved %of the transformer network 
accuracy in \xeable part segmentation over state-of-the-art methods, including OPD~\cite{jiang2022opd} \rz{and OPDMulti~\cite{sun2024opdm}}. %Mask2Former~\cite{cheng2022masked}.
% \vspace{-3pt}
\item Our \rz{scalable} AL model allows us to accurately annotate a dataset of \rqw{2,550} real photos of articulated objects in indoor scenes. We show the superior quality and diversity of our new dataset over \rz{current alternatives~\cite{jiang2022opd,sun2024opdm}}, and the resulting improvements in segmentation accuracy. % by all the three trained segmentation modules: OPD, Mask2Former, and ours.
\end{itemize}
