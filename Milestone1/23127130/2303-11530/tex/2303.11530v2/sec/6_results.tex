\section{Experiments}
\label{sec:results}
\rqw{
\agp{We start our experiments by rendering synthetic images from PartNet-Mobility dataset~\cite{Xiang_2020_SAPIEN} with diverse articulation states, enabling us to obtain sufficient annotations for training 2D segmentation networks and support transfer learning applications.} The synthetic dataset contains $\sim$32K images, evenly distributed across categories, and randomly partitioned into training  (90\%) and test sets (10\%). 

We implement our network in PyTorch on two NVIDIA Titan RTX GPUs. \agp{All images are resized to 256$\times$256 for training.} During pre-training on the PartNet-Mobility dataset, we use the Adam optimizer with an initial learning rate (\emph{lr}) of 2.5$e$-4, reducing it by a factor of $\gamma=$0.1 at 1K and 1.5K epochs separately over a total of 2K epochs. When fine-tuning on real images, we use the same \emph{lr} and $\gamma$ at 3.5K and 4K epochs over a total of 4.5K epochs.


\subsection{Competing methods}
We compare our active \textit{coarse-to-fine} part segmentation model with three 2D segmentation methods and also analyze two variants of our proposed approach. 
\begin{itemize}
    \item \textbf{Grounded-SAM}~\cite{Grounded-SAM_Contributors_Grounded-Segment-Anything_2023}, which combines Grounding-DINO~\cite{liu2023grounding} and Segment Anything~\cite{kirillov2023segany}, \agp{is a large visual model that can be used for zero-shot 2D object detection and segmentation, and supports text prompts.} %as a powerful large model used for strong zero-shot object detection and segmentation in images given text captions.
    \item \textbf{OPD-C}~\cite{jiang2022opd}, which is the first work for detecting openable parts in images based on MaskRCNN~\cite{he2017mask}.
    \item \textbf{OPDFormer-C}~\cite{sun2024opdm}, a follow-up of OPD-C based on Mask2Former~\cite{cheng2022masked}, is the state-of-the-art for \agp{openable} part segmentation of multiple articulated objects in images.
% \end{itemize}
% \vspace{-10pt}
% And two variants of our approach,
% \begin{itemize}
    \item \textbf{$\text{Ours}_{w/o AL}$} \agp{is a variant that does not use human feedback -- it infers part segmentation results based only on the transformer-based model.} %\emph{test set} using our transformer-based model.
    \item \textbf{$\text{Ours}_{f-AL}$} \agp{is variant of our approach that uses only the \emph{fine} stage of the AL framework. That is, verification and annotation of just the part masks is done.}
    % \item \textbf{$\text{Ours}_{GAL}$} \agp{is an AL variant of our approach \emph{without} using the \emph{coarse-to-fine} AL strategy -- verification and annotations of just the part masks is done (\emph{fine} stage only).}
    %is an AL variant of our approach without using the \emph{coarse-to-fine} AL strategy; it is trained using the \emph{fine} AL with verification and annotation of part mask only.
\end{itemize}
\input{tables/mAP_ours}
\input{tables/ablation}
\subsection{Evaluation on Our Dataset}
We \agp{split} our dataset into 550/2000 as train/test sets and evaluate the performance of all competing methods on our \emph{test set}. Note that all methods, except Grounded-SAM, undergo fine-tuning on our \emph{train set}. Figure~\ref{fig:viz_ours} shows qualitative results of different methods on our \emph{test set}.
\vspace{-10 pt}
\paragraph{Quantitative comparison.}Table~\ref{tab:map_ours} compares four non-AL methods. Among these, Grounded-SAM \agp{has the lowest performance} due to its direct evaluation on the \emph{test set} without fine-tuning. Its poor performance also shows that current \agp{large AI models} are still limited in \agp{understanding} object parts without adequate \agp{training on} well-labeled data. Conversely, $\text{Ours}_{w/o AL}$ model\agp{, fine-tuned on our \emph{train set}, outperforms competing methods}, achieving $>$80\% segmentation mAP. The \agp{architectural} designs of OPD-C and OPDFormer, based respectively on vanilla MaskRCNN and Mask2Former, are tailored for universal segmentation tasks. Therefore, they inadequately address the unique characteristics of articulated objects, whose movable parts are closely tied to object pose and interact\agp{ion} directions. In contrast, our approach effectively leverages the hierarchical structure of the scene, object, and part present in the input image, resulting in \agp{a much better} performance. Beyond accuracy metrics, we observe that all non-AL methods require \agp{significant} human effort for labeling the \emph{test set}. \agp{This is mainly due} to imperfect predictions from these models. 

\agp{As seen in the last two rows of Table~\ref{tab:map_ours}, using the AL framework}, our models show improved performance, reaching over 95\% accuracy, with 81.7\% time saving \agp{in terms of} manual effort. The segmentation performance of the two AL methods is close since they share identical network architecture. \agp{But they differ in training strategies, impacting labeling efficiency. Specifically, our method is 1 hour faster than $\text{Ours}_{f-AL}$}, demonstrating the importance of our \emph{coarse-to-fine} AL strategy in minimizing \agp{human effort}. \vspace{-10pt}
\paragraph{Ablation study.}Table~\ref{tab:ablation} \agp{highlights the need and} contributions of key components of our method on improving prediction accuracy and minimizing human efforts. Columns 2-5 respectively indicate the presence of: \textbf{Obj.} object mask head; \textbf{Pos.} pose estimation head; \textbf{Dir.} interact\agp{ion} direction prediction head; \textbf{AL} active learning. Row 4 uses the \emph{fine} AL stage on part mask \agp{alone} due to the absence of pose and interact\agp{ion} direction prediction module. Row 5 and 6 use our \emph{coarse-to-fine} AL strategy. Results in Table~\ref{tab:ablation} clearly justify the \emph{coarse-to-fine} design in our method, \agp{which gives the best performance (see row 6).} %where our method (Row 6) is the most accurate and efficient.


\input{tables/mAP_opd}
\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figs/qual-opdr.pdf}
    \caption{Qualitative results on OPDReal and OPDMulti test set. $\text{Ours}_{w/o AL}$ outperforms others on noisy GT and multiple objects. }
\label{fig:viz_opdreal}
\vspace{-10pt}
\end{figure}
\subsection{Evaluation on OPDReal and OPDMulti}
In addition, we assess the performance of \agp{different} models on the OPDReal and OPDMulti dataset, using \agp{their respective train and test splits.} %training and test data partitions as provided by these datasets. 
As shown in Table~\ref{tab:map_opd}, $\text{Ours}_{w/o AL}$ method \agp{outperforms the rest}. However, with more than 70\% training data, all methods still fail to achieve $>$55\% accuracy on OPDReal. This limitation primarily stems from \agp{data skewness towards the Storage category in OPDReal, which constitutes more than 90\% of total samples, and results in poor generalization across other object categories.} Detailed category-wise results are \agp{provided} in the supplementary material. Performance on OPDMulti is further compromised by an abundance of noisy data in its test set\agp{\cite{sun2024opdm}}. From the qualitative results in Figure~\ref{fig:viz_opdreal}, we observe that some openable parts are cluttered or missed in the GT annotation, while our method accurately segments these parts. This discrepancy also contributes to the low accuracy.

}
\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figs/application.pdf}
    \caption{Reconstruct and manipulate bottle and dishwasher}
\label{fig:application}
\vspace{-10pt}
\end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.4\textwidth]{figs/lamp-bottle.pdf}
%     \caption{Generalization on bottle and different lamps.\rqc{this figure could be removed if the teaser contains results on lamp/bottle}}
% \label{fig:viz_lamp}
% \end{figure}
% \subsection{Model ablations}
% \label{subsec: ablation}
% The performance of our method is driven, in parts, by two specially designed prediction branches that individually output camera pose and object mask. We perform evaluations by ablating these two modules in our network architecture.
% \vspace{-10 pt}
% \paragraph{Ours w/o pose prediction branch.}
% Keeping all other modules, we remove that MLP branch in Figure \ref{fig:method-overview} which predicts 6 DoF camera pose for a given input image. %This setting demonstrates the need and efficiency of having a pose prediction branch for obtaining accurate 2D part segmentation masks in a given image.
% \vspace{-10 pt}
% \paragraph{Ours w/o \rqw{object} mask prediction branch.}
% Here, we only remove that MLP branch in Figure \ref{fig:method-overview} which predicts a binary object mask for the interactable objects (not parts) in the input image. %This setting analyses the need and effect of a coarse object mask in obtaining fine-grained 2D part segmentation.

%
% We observe that our model significantly outperforms competing methods on both datasets. This proves the efficacy of our model over competing methods. It is interesting to note the big jump in performance for \emph{all} the models when testing using models fine-tuned on our dataset, compared to their respective performance when fine-tuned on the OPDReal dataset. This is mainly due to data skewness towards the Storage category in OPDReal (makes for 91.67\% of the total samples), leading to a lack of generalizability on images with other object categories. Since our dataset is relatively balanced across different categories (except for the Washer), we observe that all three models perform much better than OPDReal counterparts, with our model achieving the best results again. These results validate the richness of our dataset over OPDReal for dynamic part segmentation tasks.

% \subsection{Ablation studies w/o active learning}
% \label{subsec:ablation_no_al}
% %

% We perform ablations on our network architecture to better understand the need for pose estimation MLP and the object mask prediction MLP at the output of the transformer decoder (shown in the orange-colored box on the right side of Figure \ref{fig:method-overview}). The segmentation performance is tabulated in Table \ref{tab:ablation}. Essentially, when both the pose estimation module and the mask predictor module are removed (row 1), our network reduces to the architecture of Mask2Former \cite{cheng2022masked}. Also, the pose estimation module seems to play more of a role than the object mask predictor module in achieving better part segmentation, with a positive net difference of 1.5\% in mAP score, see row 3 and row 2 in Table \ref{tab:ablation}, respectively. 

% This is because estimating 6-DoF pose enables us to obtain regions of interactable parts for objects in the image, whereas a coarse object binary mask provides information about the object as a whole. When both modules are present (row 4), the network achieves the best performance since they provide a coarse-to-fine refinement over their individual contributions. This is nothing but our setting.



% \subsection{Quantitative results with active learning}
% \label{subsec:quant_res_al}
% The learning framework and data modifications per iteration in the active learning (AL) setup are described in Section \ref{subsec:al_methodology}. In Table \ref{tab:al_map}, we present the mAP scores for different models, and contrast them with the active learning setup on our model, on the same train-test set of 50-500 real-world images. Naturally, all models without AL framework overfit on the 50-image training set, with the performance of the \emph{best} model being $\sim$10\% lower than our AL framework.

% We also compare annotation times required for labeling all images in our enhancement set (see Section \ref{subsec:al_methodology}) purely based on segmentation results output from OPD and Mask2Former. That is, no active learning is considered here. Table \ref{tab:al_on_all_methods} presents these results and compares them against our AL framework. We clearly see that the number of images as well as interactable parts that need manual annotation to segment all interactable parts is greater for OPD, followed by Mask2Former. However, this number drops drastically when our AL model is considered. As such, our AL model consumes the least time to get 2D segmentation masks. 

% Finally, we also show the AL process for all four iterations in Table \ref{tab:al_iter}. The important thing to read in this table are the last three columns that represent the number of images/parts with ``perfect" segmentations, and the amount of time taken to fix them, respectively (see Section \ref{subsec:al_methodology} for interpretation of ``perfect" and ``bad"). We observe that as the iterations progress, the amount of annotation time decreases, which can be attributed to the model's generalization ability with more labeled training data.
% }

% %Clearly, \rqw{our method with AL achieves the highest accuracy with a few time of human efforts. Table \ref{tab:al_iter} shows the detailed statistics of each iteration in AL, where the fourth column represents the numbers of bad images and corresponding annotations done by human. We compute the time cost based on the number of test results and manual annotations. Predictions on the test set are sorted based on the average prediction confidence and we consider a prediction as \emph{bad} if there is no correct prediction with over 75\% confidence. We manually go through predictions in order and select \emph{perfect} predictions, which include all interactable parts in the image with fine-grained segmentation mask. We estimates the time for checking test sets as 3 seconds per image, and manual annotation on bad predictions as 15 seconds per annotation based on our experiments. Perfect predictions are automatically merged into the training set without any human time cost.}


% \agp{
% \subsection{Ablation studies with active learning}
% In Table \ref{tab:al_ablation_ourcomp}, we show timing comparisons for annotating enhancement set images (refer to Section \ref{subsec:al_methodology} for terminology) using different versions of our model. We again observe that the pose estimation module (Row ID 2) provides better segmentation masks on unseen images compared to the object mask prediction module (Row ID 1), resulting in less human intervention for rectifying the masks, as recorded by the annotation time in the last column. Our proposed model (Row ID 3) requires minimum time effort from humans, validating, yet again, our network design choices.
% }

% %\rqw{\subsection{Ablation studies with active learning}
% %We perform ablations on time cost of AL using different methods, which aims to show the time savings using AL for labeling and segmentation task. Table~\ref{tab:al_on_all_methods} verifies the contribution of pose-aware masked attention scheme of our method on minimizing human labeling efforts. We compare our full method against the baseline with different settings of modules. It is clearly that our full method (last row) is the most efficient. 

% %To further evaluate the efficiency of AL, we apply different methods to label our \emph{enhancement} set. Table~\ref{tab:al_ablation_ourcomp} presents the quantitative evaluation results on human annotation and time cost. The second column of the table shows the corresponding numbers of manual annotated images and annotations of different methods. Without AL, both OPDRCNN-C and Mask2Former can barely generate a few \emph{perfect} annotations and most images need to be labeled manually. Our method with AL is capable for accurately labeling all images in the \emph{enhancement} set within less than 2 hours of manual annotation on 16.7\% of images, which significantly reduces the time of human efforts for 76.8\%. 
% %}




\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{figs/qual-ours.pdf}
    % \vspace{-3pt}
    \caption{Qualitative results on \textit{test set} from our dataset. We visualize predictions results on different object categories using 3 competing methods and our final model. Our method outputs better segmentation masks over \xeable parts across multiple objects in the image with clear separation of parts and small parts segmentation (Row 1, 4, 5). Our results also show that the \emph{coarse-to-fine} segmentation framework can effectively reduce segmentation errors from unwanted objects (Row 2) and object side surfaces (Row 2, 3, 6, 8). More results in the supplementary materials. }
    \label{fig:viz_ours}
\end{figure*}
