\section{Method}
\label{sec:method}

\agp{
To address the above problem, we propose an active learning setup that consists of a transformer-based learning framework coupled with a human-in-the-loop feedback process. To this end, we present an end-to-end pose-aware masked-attention network (Fig \ref{fig:method}) that works in a \emph{coarse-to-fine} manner for part segmentation and label prediction. By making use of \emph{coarse} and \emph{fine} features from the network, segmentation masks are further refined by humans in the AL setup (Fig \ref{fig:al-pipeline}), resulting in precise \xeable part masks.
}
\vspace{-10pt}
\subsection{Pose-aware masked-attention network}
\label{subsec:network}

\agp{
% An overview of our network architecture is shown in Figure \ref{fig:method}. It consists of two stages -- \emph{coarse} and \emph{fine}.
%
% The \emph{coarse} stage, as the name suggests, outputs a coarse segmentation mask (referred to as the ``refined mask" in Figure \ref{fig:method}) on the object's surface that contains a \xeable part. And this surface supports the part's motion, which defines the so-called interaction direction. The \emph{fine} stage leverages this ``refined mask" to output part masks, bounding boxes, and semantic labels, for all \xeable parts in the input image. We explain these two stages below.
}
%
\rqw{Fig~\ref{fig:method} provides a comprehensive depiction of our network architecture, encompassing two distinct stages. In the \emph{coarse} stage, the network processes a single RGB image and computes a refined mask based on outputs from multiple heads, which accurately pinpoints the region containing \xeable parts. This stage filters out noise predictions on background and extraneous portions of the object. Subsequently, \emph{fine} stage takes the refined mask and image features to generate part masks, bounding boxes, and semantic labels.
\vspace{-10pt}
\paragraph{\emph{Coarse} stage.} There are three steps in \emph{coarse} stage. First, the input image is passed through a backbone object detector network, producing multi-scale feature maps $f$ and 2D object bounding boxes $b^{o}$. A pixel decoder~\cite{zhu2020deformable} upsamples $f$ for subsequent processing in the \emph{fine} stage. Second, we use a modified version of the multi-head attention-based encoder and decoder~\cite{zhu2020deformable} to process $f$. Inspired by \cite{jantos2022poet}, we replace the learned object query embedding with normalized centre coordinates $(c_x, c_y)$, width and height $(w,h)$ from the detected 2D bounding box, enabling the decoder to generate new object query embeddings and estimate 6DoF pose from the 2D bounding box. Third, the decoded queries are passed into multiple MLP heads for (a) object class prediction, (b) 6DoF object pose estimation, (c) object interaction direction and (d) object mask prediction. 
\input{tables/data_dist}
}

%
% Fig~\ref{fig:method} provides a comprehensive depiction of our network architecture, encompassing two distinct stages -- \emph{coarse} and \emph{fine}. In the \emph{coarse} stage, the network processes a single RGB image and computes a refined mask based on outputs from multiple heads, which accurately pinpoints the region containing \xeable parts. This stage filters out noise predictions on background and extraneous portions of the object. Subsequently, \emph{fine} stage takes the refined mask and image features to generate part masks, bounding boxes, and semantic labels.\vspace{-10pt}
%
% \paragraph{\emph{Coarse} stage.} \agp{It consists of the below three ordered steps. 
% \newline
% (a) The input image is passed through a backbone object detector network, producing a multi-scale feature map $f$ and a 2D object bounding box $b^{o}$ for every $o_{i}$ (see Figure \ref{fig:method}). The feature map $f$ is upsampled using a PixelDecoder \cite{zhu2020deformable} to output $f_{pd}$, for use in the \emph{fine} stage. 
% \newline
% (b) Every feature map $f$ is processed using a multi-head attention-based encoder-decoder~\cite{zhu2020deformable}, with modifications to query embeddings. Specifically, inspired by \cite{jantos2022poet}, we replace the learned object query embedding with normalized centre coordinates $(c_x, c_y)$, width, height $(w,h)$ of $b^{o}$. 
% \newline
% (c) The object embeddings at the decoder output are passed into multiple MLP heads for (a) object class prediction, (b) 6DoF object pose estimation, (c) object interaction direction and (d) object mask prediction.
% \input{tables/data_dist}

%
\agp{
% \newline
% Object class probabilities are obtained using a fully connected layer followed by a sigmoid activation. 
\rqw{We obtain the object class with fully connected layers. }For 6DoF pose estimation, \rqw{we use} two identical MLP heads with different output dimensions -- one for estimating camera translation $\tilde{t}=\left(\tilde{t}_{x}, \tilde{t}_{y}, \tilde{t}_{z}\right)$, and the other for estimating the camera rotation matrix $\tilde{R} \in S O(3)$ as described in \cite{zhou2019continuity}. 

The MLP head for interaction direction prediction outputs a set of possible interaction directions $d \in \{\pm x, \pm y, \pm z\}$ corresponding to the 6DoF coordinates. %\chk{(``6-DoF coordinates"??)}
Using $b^{o}$ and the estimated 6DoF object pose, we can obtain the corresponding 3D \emph{oriented} bounding box $B^{o}$, which tightly fits the $b^{o}$. From among the eight vertices in $B^{o}$, we select vertices of the face along the interaction direction as the representative 2D box for the interaction surface, and use it to crop the input image. \rqw{This cropped image is further multiplied with the object 2D binary mask to filter out background pixels, obtaining the refined binary object mask $m_r$, which guides the subsequent \emph{fine} stage to focus exclusively on the relevant features of the articulated object.}
% This image crop is then multiplied with the 2D binary mask of the object (another MLP head is used to output the 2D binary mask of $o_{i}$) to filter out background pixels. The result is what we refer to as the ``refined mask", $m_{r}$. This completes the \emph{coarse} stage.
}
%
% The interact direction prediction head outputs a set of possible interact directions $d \in \{\pm x, \pm y, \pm z\}$ corresponding to the 6DoF coordinates. Using $b^{o}$ and the estimated 6DoF object pose, we can obtain the corresponding 3D \emph{oriented} bounding box $B^{o}$\rqw{, which tightly fit the $b^{o}$}. From among the eight vertices in $B^{o}$, we select vertices of the face along the interact directions as the representative 2D box for object interact surface, and use it to crop the input image. This cropped image is further multiplied with the 2D binary mask to filter out background pixels, obtaining the refined binary object mask $m_r$, which guides the subsequent \emph{fine} stage to focus exclusively on the relevant features of the articulated object.
\vspace{-10pt}
\paragraph{\emph{Fine} stage.} 
\agp{
There is just one component to this stage, which is the masked-attention decoder from Mask\emph{2}Former \cite{cheng2022masked} (see Figure \ref{fig:method}). It is made up of a cascade of three identical layers, $L_i$'s. $L_{1}$ takes as input image features $f_{pd}$ and the refined mask, $m_r$, and outputs a binary mask which is fed to the next layer. Eventually, the binary mask at the output of $L_{3}$ is multiplied with $f_{pd}$ resulting in \xeable part segmentation in the RGB space. We call this our \emph{pose-aware masked-attention decoder}.
}
\vspace{-10 pt}
\paragraph{Loss functions.}\agp{We formulate the training loss as below
\begin{equation}
    L = L_{class} + L_{dir} + L_{om} + L_{pos} + L_{fine}
\end{equation}
where $L_{class}$ is the binary-cross entropy for object class prediction, $L_{dir}$ is the cross-entropy loss for interaction direction prediction, $L_{om}$ is the binary mask loss for object mask prediction. We define the loss for pose estimation as $L_{pos} = \lambda_t L_t+\lambda_{rot} L_{rot}$, where $L_t$ is the L2-loss of the translation head and $L_{rot}$ is the geodesic loss~\cite{mahendran20173d} of the rotation head. We set $\lambda_t$ and $\lambda_{rot}$ to 2 and 1 respectively. We use a pixel-wise cross-entropy loss for the \emph{fine} stage.
\newline
When pre-training, we jointly train our two-stage network in an end-to-end fashion (see Section~\ref{sec:results}). During fine-tuning on real images with part annotations, we fix MLP weights since ground truth poses and object masks are not available.
}
%
% \paragraph{\emph{Fine} stage.}This stage predicts the instance segmentation mask, bounding box and semantic label for each \xeable part. We make use of masked-attention decoders from Mask\emph{2}Former \cite{cheng2022masked}, which contains three identical layers $L_i$. $L_{1}$ takes as input $f_{pd}$, upsampled from $f$ by a pixel decoder~\cite{zhu2020deformable} along with the refined mask, $m_r$, and outputs a binary mask which is fed to the next layer. The binary mask at the output of $L_{3}$ is multiplied with $f_{pd}$ resulting in part segmentation in the RGB space.  We call it as our \emph{pose-aware masked-attention decoder}. \vspace{-10pt}
% \paragraph{Loss functions.}We define the total loss by
% \begin{equation}
%     L = L_{class} + L_{dir} + L_{om} + L_{pos} + L_{fine}
% \end{equation}
% where $L_{class}$ for object class prediction and $L{dir}$ for interact direction prediction as cross-entropy loss, $L_{om}$ for object mask prediction as binary mask loss. We define the loss for pose estimation as $L_{p} = \lambda_t L_t+\lambda_{rot} L_{rot}$, where $L_t$ is the L2-loss of the translation head and $L_{rot}$ is the geodesic loss~\cite{mahendran20173d} of the rotation head. We set weighting parameters $\lambda_t$ and $\lambda_{rot}$ to 2 and 1 respectively. We use pixel-wise cross entropy loss to define $L_{fine}$ for the \emph{fine} stage.

% We train both stages jointly in an end-to-end fashion for pre-training on synthetic datasets (see Section~\ref{sec:results}). During fine-tuning on real images with part annotations, we fix the weights of MLPs since the ground truth poses and object masks are not available.
\subsection{Coarse-to-fine active learning strategy}
\label{subsec:almethod}
\agp{
Our active learning setup, consisting of human-in-the-loop feedback, unfolds in a coarse-to-fine manner (see Figure~\ref{fig:al-pipeline}). We independently run AL workflow on outputs of both \emph{coarse} and \emph{fine} stages from Section \ref{subsec:network}. 

% When employing the AL workflow on the outputs of the \emph{coarse} stage, only the \emph{test set} samples are considered. 
\rqw{In \emph{coarse} AL part, \emph{Coarse} stage generates predictions for the \emph{test set}. During this phase, users validate interaction direction predictions and rectify inaccuracies. With ground-truth interaction directions established, refined masks $m_{r}$ are computed and input into the \emph{fine} stage.}
% Here, users validate interaction direction predictions and rectify inaccuracies for every input in the test set, establishing the ground truth. With the now-available ground truth interaction directions, refined masks, $m_{r}$'s, are computed and input into the \emph{fine} stage.

% Our active learning workflow, consisting of human-in-the-loop feedback process on interact direction and part mask, unfolds in a coarse-to-fine manner, as depicted in Figure~\ref{fig:al-pipeline}. We independently learn both \emph{coarse} and \emph{fine} features across two parts. In \emph{coarse} AL part, \emph{Coarse} stage generates predictions for the \emph{test set}. During this phase, users validate interact direction predictions and rectify inaccuracies. With ground-truth interact directions established, refined masks are computed and input into the \emph{fine} stage.

% Employing the AL workflow on the \emph{fine} stage means subjecting the final part segmentation masks and label predictions to user evaluation for one of the following three categories: perfect/missed/fair. 
\rqw{In \emph{fine} AL part, part segmentation mask and label outcome from the \emph{Fine} stage are subject to user evaluation, categorized as perfect, missed, or fair. Specifically: i)} A perfect prediction implies coverage of all \xeable parts in the final segmentation masks, without any gaps, as well as accurate class labels for each segmented part; ii) A missed prediction effectively refers to a null segmentation mask, and/or erroneous class labels; iii) A fair prediction denotes an output segmentation mask that may exhibit imperfections such as gaps or rough edges, and/or may have inaccuracies in some part class labels. \rqw{We provide extensive examples of these scenarios in our supplements.}
%See the supplementary material for details on these. 
During the \rqw{AL} process, perfect predictions are directly incorporated into the next-iteration training set. For all wrong predictions, we employ the labelme~\cite{yi2016scalable} annotation interface to manually annotate the part mask polygons, and include such images in the next-iteration training set. Fair predictions, on the other hand, remain in the \emph{test set} for re-evaluation.
% In \emph{fine} AL part, part segmentation mask and label outcome from the \emph{Fine} stage are subject to user evaluation, categorized as perfect, missed, or fair. Specifically: i) A perfect prediction implies comprehensive coverage of all \xeable parts by the output segmentation mask without any gaps, coupled with accurate class labels for each part; ii) A wrong prediction is characterized by a total omission of target parts in the output segmentation mask, and/or erroneous class labels; iii) A fair prediction denotes an output segmentation mask that may exhibit imperfections such as gaps or rough edges, and/or may have inaccuracies in some part class labels. We provide extensive examples of these scenarios in our supplementary materials. Throughout our iterative active training process, perfect predictions are directly incorporated into the subsequent training set. For all wrong predictions, we employ the labelme~\cite{yi2016scalable} annotation interface to delineate ground-truth part mask polygons, subsequently including these images in the training set of future iterations. Fair predictions, on the other hand, remain in the \emph{test set} for re-evaluation.

The AL workflow on the \emph{fine} stage continues iteratively until all images within the \emph{test set} transition to the training set, becoming well-labeled and eventually leaving the test set vacant. \rqw{Benefiting from the verified ground-truth interaction direction established in the \emph{coarse} AL part, the \emph{Fine} stage hones in on features of the target surface, omitting noisy object parts. This streamlined focus notably expedites the annotation process. }Further insights into the human verification and annotation procedures will be provided in our supplementary materials.
}

%%
%%
\iffalse
Fig~\ref{fig:method} provides a comprehensive depiction of our network architecture, encompassing two distinct stages. In \emph{coarse} stage, the network processes a single RGB image and computes a refined mask based on outputs from multiple heads, which accurately pinpoints the region containing \xeable parts. This stage filters out noise predictions on background and extraneous portions of the object. Subsequently, \emph{fine} stage takes the refined mask and image features to generate part masks, bounding boxes, and semantic labels.\vspace{-10pt}
\paragraph{\emph{Coarse} stage.} There are three steps in \emph{coarse} stage. First, the input image is passed through a backbone object detector network, producing multi-scale feature maps $f$ and 2D object bounding boxes $b^{o}$. Second, we use a modified version of the multi-head attention-based encoder and decoder~\cite{zhu2020deformable} to process $f$. Inspired by \cite{jantos2022poet}, we replace the learned object query embedding with normalized centre coordinates $(c_x, c_y)$, width and height $(w,h)$ from the detected 2D bounding box, enabling the decoder to generate new object query embeddings and estimate 6DoF pose from the 2D bounding box. Third, the decoded queries are passed into multiple MLP heads for (a) object class prediction, (b) 6DoF object pose estimation, (c) object interact direction and (d) object mask prediction. 
\input{tables/data_dist}

Two identical heads with different output dimension are used for the 6DoF pose estimation, which consist of the translation $\tilde{t}=\left(\tilde{t}_{x}, \tilde{t}_{y}, \tilde{t}_{z}\right)$ with respect to the camera frame and the 6D rotation matrix $\tilde{R} \in S O(3)$ as described in \cite{zhou2019continuity}. 

The interact direction prediction head outputs a set of possible interact directions $d \in \{\pm x, \pm y, \pm z\}$ corresponding to the 6DoF coordinates. Using $b^{o}$ and the estimated 6DoF object pose, we can obtain the corresponding 3D \emph{oriented} bounding box $B^{o}$\rqw{, which tightly fit the $b^{o}$}. From among the eight vertices in $B^{o}$, we select vertices of the face along the interact directions as the representative 2D box for object interact surface, and use it to crop the input image. This cropped image is further multiplied with the 2D binary mask to filter out background pixels, obtaining the refined binary object mask $m_r$, which guides the subsequent \emph{fine} stage to focus exclusively on the relevant features of the articulated object.\vspace{-10pt}
\paragraph{\emph{Fine} stage.}This stage predicts the instance segmentation mask, bounding box and semantic label for each \xeable part. We make use of masked-attention decoders from Mask\emph{2}Former \cite{cheng2022masked}, which contains three identical layers $L_i$. $L_{1}$ takes as input $f_{pd}$, upsampled from $f$ by a pixel decoder~\cite{zhu2020deformable} along with the refined mask, $m_r$, and outputs a binary mask which is fed to the next layer. The binary mask at the output of $L_{3}$ is multiplied with $f_{pd}$ resulting in part segmentation in the RGB space.  We call it as our \emph{pose-aware masked-attention decoder}. \vspace{-10pt}
\paragraph{Loss functions.}We define the total loss by
\begin{equation}
    L = L_{class} + L_{dir} + L_{om} + L_{pos} + L_{fine}
\end{equation}
where $L_{class}$ for object class prediction and $L{dir}$ for interact direction prediction as cross-entropy loss, $L_{om}$ for object mask prediction as binary mask loss. We define the loss for pose estimation as $L_{p} = \lambda_t L_t+\lambda_{rot} L_{rot}$, where $L_t$ is the L2-loss of the translation head and $L_{rot}$ is the geodesic loss~\cite{mahendran20173d} of the rotation head. We set weighting parameters $\lambda_t$ and $\lambda_{rot}$ to 2 and 1 respectively. We use pixel-wise cross entropy loss to define $L_{fine}$ for the \emph{fine} stage.

We train both stages jointly in an end-to-end fashion for pre-training on synthetic datasets (see Section~\ref{sec:results}). During fine-tuning on real images with part annotations, we fix the weights of MLPs since the ground truth poses and object masks are not available.
\subsection{Coarse-to-fine active learning strategy}
\label{subsec:almethod}
Our active learning workflow, consisting of human-in-the-loop feedback process on interact direction and part mask, unfolds in a coarse-to-fine manner, as depicted in Figure~\ref{fig:al-pipeline}. We independently learn both \emph{coarse} and \emph{fine} features across two parts. In \emph{coarse} AL part, \emph{Coarse} stage generates predictions for the \emph{test set}. During this phase, users validate interact direction predictions and rectify inaccuracies. With ground-truth interact directions established, refined masks are computed and input into the \emph{fine} stage.

In \emph{fine} AL part, part segmentation mask and label outcome from the \emph{Fine} stage are subject to user evaluation, categorized as perfect, missed, or fair. Specifically: i) A perfect prediction implies comprehensive coverage of all \xeable parts by the output segmentation mask without any gaps, coupled with accurate class labels for each part; ii) A wrong prediction is characterized by a total omission of target parts in the output segmentation mask, and/or erroneous class labels; iii) A fair prediction denotes an output segmentation mask that may exhibit imperfections such as gaps or rough edges, and/or may have inaccuracies in some part class labels. We provide extensive examples of these scenarios in our supplementary materials. Throughout our iterative active training process, perfect predictions are directly incorporated into the subsequent training set. For all wrong predictions, we employ the labelme~\cite{yi2016scalable} annotation interface to delineate ground-truth part mask polygons, subsequently including these images in the training set of future iterations. Fair predictions, on the other hand, remain in the \emph{test set} for re-evaluation.

The \emph{fine} AL part persists iteratively until all images within the \emph{test set} transition to the training set, becoming well-labeled and leaving the test set vacant. Benefiting from the verified ground-truth interact direction established in the \emph{coarse} AL part, the \emph{Fine} stage hones in on features of the target surface, omitting noisy object parts. This streamlined focus notably expedites the annotation process. Further insights into the human verification and annotation procedures will be provided in our supplementary material.

\fi
