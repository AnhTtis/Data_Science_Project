\begin{figure*}[t]
    \centering 
    \includegraphics[width=\linewidth]{figures/train_overview.pdf} 
        \caption{We visualize the loss functions used to train our method. The dynamics function is trained via reconstruction loss in embedding space (left). The distance function is trained via contrastive learning, with positive anchors chosen by predicting the next state using the ground truth action, $F(i_t, a_t)$, and predicting negative pairs chosen using noisy actions $F(i_t, \hat{a}^j)$.}
    \label{fig:train_overview}
    
\end{figure*}

\section{Methods}

\paragraph{Preliminaries} 

This paper considers learning goal-conditioned manipulation behaviors from image observations. The robot agent is provided a goal observation ($I_g$) -- e.g. target object in robot gripper -- and an observation ($I_t$) for the current time-step $t$. The robot must process these observations and decide an action to take ($a_t$). Note that all observations ($I$) are wrist-mounted RGB camera images with no depth or proprioceptive data. Additionally, the actions ($a$) are specified as arbitrary $SE(3)$ transforms for the robot's end-effector (more in Supplement ~\ref{supp:hardware}). Our goal is to learn a robotic manipulation controller, using a set of training trajectories $\mathcal{D} = \{ \tau_1, \dots, \tau_N \}$, where $\tau_i = \{I_g, I_1, a_1, \dots, a_{T-1}, I_T\}$. The test and train settings are visualized in Fig.~\ref{fig:prob_setting}.

\paragraph{Our Approach} 

Our method leverages a pre-trained representation network, $R$, to encode observations, $i_t = R(I_t)$, and enable control via distance learning. Specifically, we use contrastive representation learning methods~\cite{oord2018representation,mnih2013learning} to learn a distance metric, $d(i_j, i_k)$, within the pre-trained embedding space. The key idea is to use this distance metric to select which of the possible future state is closest to the goal state. But how do we predict possible future states? We explicitly learn a dynamics function, $F(i_t, a_t)$ that predicts future state for a possible action $a_t$. During test time, we predict multiple future states using different possible action and select the one which is closes to goal state. The following sections describe the learned dynamics module (see Sec.~\ref{sec:methods_dyn}), distance learning method (see Sec.~\ref{sec:methods_dist}), and test-time controller for robot deployment (see Sec.~\ref{sec:methods_test}) in detail. Fig.~\ref{fig:train_overview} provides a visual depiction of our training algorithm. Pseudo-code and hyper-parameters are presented in Supplement~\ref{supp:algorithm}.


\subsection{Dynamics Prediction}
\label{sec:methods_dyn}

An ideal dynamics function would perfectly capture how a robot's actions effects its environment. In our setting, this translates to predicting the next observation embedding, given the current embedding and commanded action: $F(i_t, a_t) = i_{t+1}$. Thus, $F$ can be learned via regression by minimizing reconstruction loss in embedding space: $\mathcal{L}_F = || F(i_t, a_t) - i_{t+1} ||_2$ (see Fig.~\ref{fig:train_overview}, left). This module has two primary uses: (1) during training it acts as a physically grounded regularizer that relays action information into the embedding space, and (2) during test time it enables the robot to plan actions in embedding space. As you will see, both properties are leveraged extensively in the rest of our method.

\subsection{Learning Task-Centric Distances}
\label{sec:methods_dist}

Our distance module $d(i_j, i_k)$ seeks to learn functional distances that encode task-centric reasoning (e.g. must reach for object before pushing it) alongside important physical priors (e.g. object is to the left so move left). How can we learn such a distance space? Since we have access to expert trajectories $\tau$, we know that the next state sequentially visited by the expert ($i_{t+1}$) is closer to the goal state ($i_g$) than arbitrary states ($\hat{i}$) reachable from $i_t$ -- i.e. $d(i_{t+1}, i_g) << d(\hat{i}, i_g)$. 

Our insight is that a distance metric with these properties can be learned via contrastive learning. Specifically, we define $d(i_j, i_k) = - cos(i_j, i_k)$, where $cos$ is cosine similarity, and sample a observation-state-goal tuples $(i_t, a_t, i_g)$, alongside random noise action candidates $\hat{a}^{1}, \dots, \hat{a}^{n}$ sampled from $\mathcal{D}$. We apply NCE loss and get: $$\mathcal{L}_d = \frac{exp( - d(F(i_t, a_t), i_g))}{exp( - d(F(i_t, a_t), i_g)) + \Sigma_j exp( - d(F(i_t, \hat{a}^{j}), i_g))}$$
This loss creates a warped embedding space where $i_{t+1}$ (hallucinated by $F(i_t, a_t)$) is pushed closer towards the goal state $i_g$, than other arbitrary states reachable from $i_t$ (again hallucinated by $F$). This process is shown visually in Fig.~\ref{fig:train_overview} (right), and precisely satisfies our criteria for good distance functions. 

\subsection{Training Details}
\label{sec:methods_train}
Both modules are trained jointly resulting in a final loss: $\mathcal{L} = \lambda_d \mathcal{L}_d + \lambda_F \mathcal{L}_F$. Note that $F$ is implemented as small neural networks with 2 hidden layer, while $d$ does not require any extra learned networks since its implemented via contrastive loss in the metric space. The shared representation network, $R$, we use is ResNet-18 initialized by R3M weights. The ADAM~\cite{kingma2014adam} stochastic gradient descent optimizer and back-propagation are used to train the network end-to-end. Please refer to Supplement~\ref{supp:training}. for additional details. 

\subsection{Test Time Robot Deployment}
\label{sec:methods_test}
During test time our learned modules must be able to solve real manipulation tasks when deployed on robot hardware. Thus, we develop a simple inference policy that solves for robot actions using our learned distance and dynamics function. First, a cost function $C_g(i) = d(i, i_g)$ is parameterized given a goal observation $I_g$ and the learned distance function. $C_g$ encodes how far an arbitrary image observation is from the goal image. The optimal action, $a^*_t$, will take the robot closest to the goal, hence: $a^*_t = \textbf{argmin}_a C_g(F(i_t, a))$. Note that we use the dynamics function $F$ to predict the next state, since we do not know the real $i^*_{t+1}$ during test time. The cost minimization is performed using the shooting method: random candidate actions, $a_1, \dots, a_N$, are sampled from $\mathcal{D}$, passed through the dynamics function, and the action with minimum cost $C_g(F(i_t, a_i))$ is executed on the robot. This process is illustrated in Fig.~\ref{fig:teaser}. Policy execution ends once the distance between the current state and the goal falls below a threshold value: $C_g(i_t) < \lambda$. While a more complicated planner could've been used, this solution was adopted due to its speed, simplicity, lack of hyper-parameters, and good empirical performance.

The final detail to discuss is gripper control for prehensile tasks (e.g. pick and place). Since the gripper status is binary (open/close) and highly correlated with the current state (i.e. close when object in hand), it makes more sense to handle it implicitly rather than as part of the action command $a_t$. Thus, we trained a gripper action classifier $\mathcal{G}(i_t) \in [0,1]$ that predicts the probability of closing the gripper given the current image embedding. This is also implemented by adding a single layer to ResNet-18 initialized by R3M weights and can be trained using a small amount ($< 100$) of human labelled images from the train dataset $\mathcal{D}$. As a result, our system can seamlessly handle gripper control in prehensile tasks, without requiring gripper labels for every frame in $\mathcal{D}$!
