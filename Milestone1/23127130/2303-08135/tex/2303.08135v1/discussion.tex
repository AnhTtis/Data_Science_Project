\section{Discussion}
\label{sec:discussion}

This paper demonstrates that neural image representations can be more than just state representations: a simple metric defined within the embedding space can help infer robot actions. We leverage this insight to learn a distance function and dynamics function using minimal low-cost human data. These modules parameterize a robotic planner that is validated across 4 representative manipulation tasks. Despite its simplicity, our method is able to outperform standard imitation learning and offline-RL methods in the robot learning field. This is especially true in situations involving multi-modal action distributions, where our method entirely outclasses a standard BC baseline. Finally, the ablation study demonstrates that stronger representations directly result in stronger control performance, and that dynamics grounding is important for our scheme to work in practice. 

\paragraph{Future Work}

We hope that this paper inspires future work in the space of representation learning and robotics. Follow up works should improve visual representations specifically for robotics, by more precisely capturing fine-grained interactions between gripper/hand and objects. This could improve performance on tasks like knob turning, where the pre-trained R3M encoder often struggled to detect small shifts of the gripper relative to the knob. Finally, we hope that our contrastive learning method could be extended to learn entirely without action labels. This would allow us to train on large scale manipulation datasets (e.g. YouTube videos) and transfer the results directly to the control setting. 