\begin{figure}[t]
    \centering 
    \includegraphics[width=\linewidth]{figures/prob_setting.pdf} 
        \caption{In our problem setting we use a low-cost reacher grabber tool (left) to collect training demonstrations. These demonstrations are used to acquire a robot controller purely through distance/representation learning. The final system is deployed on a robot (right) to solve various tasks at test-time.}
    \label{fig:prob_setting}
\end{figure}

\begin{figure*}[t]
    \centering 
    \includegraphics[width=\linewidth]{figures/results.pdf} 
        \caption{Trajectories executed on the robot using our learned distance function. For each task, we show the $1^{st}$ person view (top) and $3^{rd}$ person view images (bottom). We show the learned visual embedding can encode functional distances between states for challenging tasks, like pushing, pick and place, door opening, and knob turning.}
    \label{fig:results}
\end{figure*}


\section{Experimental Setup}
\label{sec:exp_setup}

Our method is tested on four different manipulation tasks (see Fig.~\ref{fig:tasks}) that require a mix of \textit{high-level reasoning} (e.g. go to object before target), \textit{low level precision} (must carefully grab knob to turn it), and \textit{time-time generalization} (e.g. push novel object). 
We collect behavior data for each task from human demonstrators as described below. The trained policies are deployed on a real Franka Pandas robot (see Fig.~\ref{fig:prob_setting}, right). Additional details on the hardware setup and control stack are presented in Supplement~\ref{supp:hardware}.

\subsection{Tasks}
\label{sec:exp_setuptasks}
We now introduce the tasks used in our experiments (pictured in Fig.~\ref{fig:tasks}), and provide more details on their train and test conditions:

\paragraph{Pushing}

For this task (see Fig.~\ref{fig:task_push}) the robot must approach a novel object placed on the table and push it to the goal location (marked by target printout). \textbf{Training:} Our method was trained on a dataset of 100 demonstrations with diverse objects and randomized targets. \textbf{Testing:} During test time, the method was evaluated using 20 trails that involved unseen objects and new target printouts placed in randomly sampled positions. A trail is deemed successful if the robot pushes the object onto the target.  This is the only task that did not require gripper control. 

\paragraph{Pick and Place}

This task (see Fig.~\ref{fig:task_pick}) is analogous to the pushing task (described above), except the robot must now pick the object up from its initial location and place it in a target bowl. This task requires more precision than pushing, since grasping an object is harder than localizing it. \textbf{Training:} We collect a dataset of 400 demonstrations for training, using randomized train objects and target bowls. \textbf{Testing:} The method is evaluated using 20 trails that used novel objects and unseen target bowls (i.e. analogous to pushing). A trail is successful if the robot places an object into the target. 

\paragraph{Door Opening}

The opening task (see Fig.~\ref{fig:task_open}) requires the robot to approach a handle, grasp it, and then pull the door open. While conceptually simple, this task requires a great deal of precision, since the commanded actions must (almost) exactly match the direction of the door hinge. \textbf{Training:} We created a toy kitchen setting for this task, and collected 100 demonstrations of opening the door when starting from various initial positions. \textbf{Testing:} We evaluated on 20 real world test trials, where the robot's initial position and the door's initial state (e.g. position and hinge angle) were randomized. A trial was deemed successful if the door is opened fully.

\paragraph{Knob Turning}

In this final task (see Fig.~\ref{fig:task_knob}) the robot must approach a knob in the toy kitchen, and then turn it clock-wise. Similar to the opening task, knob turning requires very precise motor control, since the robot must grab the knob at exactly the right location and carefully rotate in order to turn. \textbf{Training:} We collect a dataset of 100 knob turning demonstrations. \textbf{Testing:} The method is evaluated on 20 trials, with randomized robot/knob initial positions. Success is determined by if the knob is turned far enough to cause an audible ``click."

\subsection{Robot-Free Data Collection}
\label{sec:problem_data}

A major strength of our setup is that training data ($\mathcal{D}$) can come from a different agent than the test time robot. We leverage this property to collect videos \textit{directly} from humans operating a low-cost grabber stick (see Fig.~\ref{fig:prob_setting}, left), and use structure from motion~\cite{schoenberger2016sfm} to recover actions. This allows us to collect demonstrations \textit{without} an expensive tele-op setup~\cite{zhang2018deep,kumar2015mujoco} or time-intensive kinesthetic demonstrations (i.e. manually moving the robot by hand). While based on prior work, our stick setup makes two important contributions: it is easier build and use than the highly instrumented setup in Song et. al.~\cite{song2020grasping}, and it provides higher quality images and action labels than the GoPro used in Young et. al.~\cite{young2021visual}. Please refer to Supplement~\ref{supp:data} for more information on our training and testing setups. We will release our dataset publicly.


\section{Experiments}
\label{sec:exp}

The following experiments seek to validate our distance learning framework on real control tasks (described above), and give some fundamental insights as to why they work. First, our baseline study (see Sec.~\ref{sec:exp_baseline}) compares our method against representative SOTA techniques in the field. In addition, in Sec.~\ref{sec:exp_multimodal} we analyze a specific scenario with highly multi-modal actions, where our model especially shines. Finally, the ablation study (see Sec.~\ref{sec:exp_ablation}) investigates which components of our system are actually needed for good performance.

\subsection{Baseline Study}
\label{sec:exp_baseline}

\begin{table}[!t]
        \centering
        \resizebox{\linewidth}{!}{%
            \begin{tabular}{l|cccc}
                \toprule
                % \rowcolor{lightgray}
                 Task & \textbf{Ours} & BC~\cite{ross2011reduction,young2021visual} & IBC~\cite{florence2021implicit} & IQL~\cite{kostrikov2021offline}\\
                 \midrule
                 \textit{Pushing} & $\textbf{85}\%$ & $70\%$ & $70\%$ & $65\%$ \\
                 \textit{Pick and Place} & $\textbf{60}\%$ & $30\%$ & $30\%$ & $45\%$ \\
                 \textit{Door Opening} & $\textbf{55}\%$ & $40\%$ & $45\%$ & $\textbf{50}\%$ \\
                 \textit{Knob Turning} & $\textbf{40}\%$ & $20\%$ & $20\%$ & $\textbf{35}\%$ \\
                \bottomrule
            \end{tabular}
        }
        
        \caption{ We compare success rates for our method versus the baselines on all four manipulation tasks. Our distance learning method outperforms a suite of representative robot learning baselines. } %For more information on the tasks/evaluation scheme please see Sec.~\ref{sec:exp_setup}.
        \label{tab:baseline}
\end{table}

\begin{table}[!t]
        \centering
        \small
        \resizebox{\linewidth}{!}{%
            \begin{tabular}{l|ccc|ccc}
                
                % \rowcolor{lightgray}
                 & \multicolumn{3}{c|}{\textit{Pushing}} & \multicolumn{3}{c}{\textit{Pick and Place}} \\
                 
                 $\#$ Demos & \textbf{Ours} & BC & IBC & \textbf{Ours} & BC & IBC\\
                 
                 \midrule
                 50 & $\textbf{70}\%$ & $50\%$ & $40\%$ & $0\%$ & $0\%$ & $0\%$ \\
                 100 & $\textbf{85}\%$ & $70\%$ & $70\%$ & $\textbf{10}\%$ & $\textbf{10}\%$ & $0\%$ \\
                 200 & $90\%$ & $80\%$ & $\textbf{100}\%$ & $\textbf{20}\%$ & $\textbf{20}\%$ & $10\%$\\
                 400 & $\textbf{100}\%$ & $\textbf{100}\%$ & $\textbf{100}\%$ & $\textbf{60}\%$ & $30\%$ & $30\%$\\
                 600 & $\textbf{100}\%$ & $\textbf{100}\%$ & $\textbf{100}\%$ & $\textbf{70}\%$ & $50\%$ & $40\%$ \\
                \bottomrule
            \end{tabular}
        }

        \caption{ We compare our distance learning method versus the behavior cloning baselines (BC~\cite{ross2011reduction,young2021visual} and IBC~\cite{florence2021implicit}) when trained on varying amounts of data. Distance learning is able to learn faster than the baselines and scale better with data.}
        \label{tab:scaling_results}

\end{table}

We first note that our paper uses only demonstration data and no online data. Therefore, we compare our method against three SOTA baselines -- ranging from standard behavior cloning, to energy based modeling, and Offline Reinforcement Learning (RL) --  that cover a wide range of prior LfD work.  The baseline methods are described briefly below. To make the comparisons fair, we parameterize all neural networks with the same R3M representation backbone used by our method, and tune hyper-parameters for best possible performance. Please check Supplement~\ref{supp:baselines} for in depth details.

\begin{itemize}
    \item \textbf{Behavior Cloning~\cite{ross2011reduction,young2021visual} (BC):} BC learns a policy (via regression) that directly predicts actions from image observations: $\min_\pi ||\pi(I_t, I_g) - a_t||_2$. This provides a strong comparison point for a whole class of LfD methods that focus on learning motor policies directly (i.e. learn policies that predict actions).
    
    \item \textbf{Implicit Behavior Cloning~\cite{florence2021implicit} (IBC):} IBC learns an energy based model that can predict actions during test time via optimization: $a_t = \textit{argmin}_a E(a, I_t, I_g)$. This method is conceptually very similar to behavior cloning, but has the potential to better handle multi-modal action distributions and discontinuous actions. 
    
    \item \textbf{Implicit Q-Learning~\cite{kostrikov2021offline} (IQL):} IQL is an offline-RL baseline that learns a Q function $Q(s,a) = Q((I_t, I_g), a_t)$, alongside a policy that maximizes it $\pi(I_t, I_g) = \textit{argmax}_a Q(s, a)$. Note that IQL's training process require us to annotate our offline trajectories $\mathcal{D}$ with a reward signal $r_t$ for each time-step. While this can be done automatically in our setting (Supplement~\ref{supp:baselines}), this is an additional assumption that could be very burdensome in the general case. 
\end{itemize}

Our method is compared against the baselines on all four of our tasks (see Sec.~\ref{sec:exp_setuptasks}). Results are reported in Table~\ref{tab:baseline}. Note how our simple distance learning approach achieves the highest success rates on all of four tasks. In addition, our method is much simpler than the strongest baseline (IQL), which requires an expert defined reward signal and uses a more complicated learning algorithm. The final test time performance is visualized in Fig.~\ref{fig:results} and on our website: \website.

\paragraph{Data Scaling} A common cited strength of BC (versus RL) is its ability to continuously improve with additional expert data. Can our method do the same? Table~\ref{tab:scaling_results} compares our distance learning method versus the BC/IBC baselines (on pushing/pick-place tasks), when trained on various amounts of data. Note how our method both learns faster than BC/IBC, and continuously scales with added demonstrations. This suggests that distance learning is a powerful alternative to BC on robotic manipulation tasks, no matter how much training data is available.



\subsection{Multi-Modality Experiment}
\label{sec:exp_multimodal}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/tasks/obstacle.pdf}
    \caption{ Our method can solve tasks with highly multi-modal action distributions that are difficult for the baselines. In this example, our distance learning controller successfully pushes the block around the obstacle, while Behavior Cloning learns to incorrectly push the block forward (i.e. predicts mean action).}
    \label{fig:multimodal_task}
\end{figure}

A common challenge in policy learning is effectively handling multi-modal action predictions. We propose a simple test scenario that allows us to test our method in this setting without any other confounding variables. Specifically, we create a ``obstacle pushing" task where the robot must push an object to the target without disturbing an obstacle placed in the middle (see Fig.~\ref{fig:multimodal_task}). Observe that there are two equally valid ways that the robot could push the block; to the left or to the right. We collect a training dataset of 100 demos, equally split between going left and right. This creates action multi-modality where the agent must arbitrarily choose between two scenarios in order to successfully perform the task, but will fail if it simply averages the actions and pushes the block forward (into the obstacle). We expect that standard BC will fall into this failure mode, since it is trained using standard L2 action regression. In contrast, our method should still work since the low distance states will move the robot either to the left or right state, thus allowing it to  make a discrete choice.

To test this hypothesis, we test our method, the BC baseline, and IQL (the strongest non-BC baseline) on 20 trials in this task setting. Success is judged similarly as to pushing (see Sec.~\ref{sec:exp_setuptasks}), with the added condition that the robot should not push the obstacles into the target. Our method achieves a $\textbf{95}\%$ success rate, versus $0\%$ for BC and $90\%$ for IQL in this setting! As expected, our method is able to greatly improve upon BC in this setting, because it explicitly avoids action prediction. While IQL should not suffer as badly from multi-modality, thanks to its learned value function, our method is still able to outperform it despite being simpler algorithmically and trained without reward signal.


\subsection{Ablation Study}
\label{sec:exp_ablation}


Our final experiment removes components of our method in order to determine which parts were most crucial for control performance. The \textit{representation ablation} replaces the R3M backbone in our neural network with a pre-trained ImageNet representation (using same ResNet-18 architecture for both). We expect the ImageNet weights to be be weaker, since they were trained on a smaller dataset that did not include ego-centric videos (ImageNet vs Ego4D used by R3M). Additionally, the \textit{dynamics ablation} removes the dynamics module during training, thus leaving the distance model/embeddings without any physical grounding. This is effectively the method from Hahn et. al.~\cite{hahn2021nrns}, which trained a distance embedding for navigation tasks without considering state dynamics at all. We expect this ablation to perform worse on manipulation tasks, because dynamics is more important in this setting v.s. visual navigation.

The two ablations are evaluated and compared against our method on all four test tasks (see Table~\ref{tab:ablation}). We find that our method strongly outperforms both of our baselines, which suggests that our intuition outline above is correct. In particular, we conclude that stronger representations can directly stronger control performance using our method. Additionally, this shows that the dynamics grounding is important to ensure that our learned distances transfer onto real robot hardware during test time, especially in manipulation tasks that involve object interactions/contact.

\begin{table}[!t]
        \centering
       
        \resizebox{\linewidth}{!}{%
            \begin{tabular}{l|ccc}
                \toprule
                % \rowcolor{lightgray}
                 & \textbf{Ours} & ImageNet Repr. & No Dynamics~\cite{hahn2021nrns}\\
                 \midrule
                  \textit{Pushing} & $\textbf{90}\%$ & $40\%$ & $60\%$ \\
                 \textit{Pick and Place} & $\textbf{60}\%$ & $40\%$ & $35\%$ \\
                 \textit{Door Opening} & $\textbf{55}\%$ & $45\%$ & $\textbf{50}\%$ \\
                 \textit{Knob Turning} & $\textbf{40}\%$ & $15\%$ & $\textbf{40}\%$ \\
                \bottomrule
            \end{tabular}
        }
        
        \caption{ This table compares success rates for our method versus the ablations on the all four tasks. As you can see, removing the dynamics module from our method during training and training with a weaker representation both result in worse performance.}
        \label{tab:ablation}
\end{table}