\section{Related Work}
\label{sec:rw}


\paragraph{Behavior Cloning}

This paper adopts the Learning from Demonstration (LfD) problem setting~\cite{argall2009survey, billard2008survey, schaal1999imitation}, where a robot must acquire manipulation behaviors given expert demonstration trajectories. A standard approach in this space is to learn a policy ($\pi$) via Behavior Cloning~\cite{ross2011reduction} (BC), which directly optimizes $\pi$ to match the expert's action distribution. While conceptually simple, realistic action distributions are difficult to model, since they are inherently multi-modal and even small errors compound over time. Thus, policy learning requires extensive intensive network engineering (e.g. transformer architectures~\cite{dasari2020transformers,chen2021decision}, multi-modal prediction heads~\cite{shafiullah2022behavior,lynch2020learning}, etc.) and/or human-in-the-loop data collection algorithms~\cite{ross2011reduction,jang2022bc} to work in practice. Instead of learning a policy, we learn a \textit{functional distance metric} that captures the how ``close" a state is to reaching a target goal. This lets us build a manipulation controller using a simple greedy planner during test time, without any explicit action prediction! 

\paragraph{Offline RL}

Broadly speaking, the field of Reinforcement Learning~\cite{sutton2018reinforcement} (RL) seeks to learn a value/advantage/Q function by assuming access to a reward signal, which ``evaluates" a state (e.g. +1 reward for reaching goal). RL algorithms usually require environment interaction to learn, though the field of Offline-RL~\cite{levine2020offline} seeks to extend these systems to learn from offline trajectories. While these approaches have created some impressive robotics demos ranging from locomotion~\cite{lee2020learning,kumar2021rma} to dexterous manipulation~\cite{andrychowicz2020learning,nvidia2022dextreme,dasari2023pgdm}, RL methods are data hungry~\cite{rajeswaran2017learning,andrychowicz2020learning}, difficult to implement, require extreme ``reward engineering" (e.g. Meta-World~\cite{yu2019meta} reward functions are 50 lines!), hampered by unstable learning dynamics~\cite{kumar2020implicit}, and poorly generalize in realistic robotic manipulation settings~\cite{dasari2022rb2}. In contrast, our method learns a proxy for value functions (i.e. distances) purely from offline data, using representation learning algorithms (from the vision field) that are much more stable. 



\paragraph{Learning Visual Rewards}
Finally, our distance learning approach is analogous to learning visual rewards. Prior work learned success classifiers~\cite{sermanet2016unsupervised,xie2018few,torabi2018generative} and/or video similarity metrics~\cite{sermanet2018time,aytar2018playing,schmeckpeper2021reinforcement,bahl2022human,chen2021learning} from expert video demonstrations. Once learned, these modules were used to parameterize reward functions that could be used to train policies~\cite{abbeel2004apprenticeship}, and/or as cost functions for planners~\cite{angelov2020composing}. However, these papers mostly consider simple settings (e.g. simulation) or require the test setting to exactly match train time. This is because the learned reward functions are often noisy and/or poorly generalize to new scenes. In contrast, our learned distance metric is stable and can easily adapt to new scenarios, thanks to the pre-trained network's latent structure. This allows us to solve diverse tasks during test time on a real robot, using a simple (and fast) shooting method planner.

