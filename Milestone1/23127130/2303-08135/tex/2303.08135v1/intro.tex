\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
    \centering 
    \includegraphics[width=\linewidth]{figures/teaser_planner.pdf} 
        \caption{This paper proposes to solve a range of manipulation tasks (e.g. pushing) by learning a functional distance metric within the embedding space of a pre-trained network. This distance function -- in combination with a learned dynamics model -- can be used to greedily plan for robot actions that reach a goal state. Our experiments reveal that the proposed method can outperform SOTA robot learning methods across four diverse manipulation tasks. }
    \label{fig:teaser}
\end{figure}

The lack of suitable, large-scale data-sets is a major bottleneck in robot learning. Due to the physical nature of data collection, robotics data-sets are: (a) hard to scale; (b) collected in sterile, non-realistic environments (e.g. robotics lab); (c) too homogeneous (e.g. toy objects with fixed backgrounds/lighting). In contrast, vision data-sets contain diverse tasks, objects, and settings (e.g. Ego4D~\cite{grauman2022ego4d}). Therefore, recent approaches have explored transferring priors from large scale vision data-sets to robotics settings. What is the right way to accomplish this?

Prior work uses vision data-sets to pre-train representations~\cite{parisi2022unsurprising,nair2022r3m,Radosavovic2022} that encode image observations as state vectors (i.e. $s = R(i)$). This visual representation is then simply used as an input for a controller learned from robot data -- e.g. a policy $\pi(a | s)$, trained with expert data via Behavior Cloning~\cite{ross2011reduction}, or a Value function $V(s)$, trained using exploratory roll-outs via Reinforcement Learning~\cite{sutton2018reinforcement}. Is this approach the most efficient way to use pre-trained representations? We argue that pre-trained networks can do more than just represent states, since their latent space already encodes semantic, task-level information -- e.g. by placing semantically similar states more closely together. Leveraging this structure to infer actions, could enable us to use significantly less robotic data during train time.

Our paper achieves this by fine-tuning a pre-trained representation into: (a) a one-step dynamics module, $F(s, a)$, that predicts how the robot's next state given the current state/action; and (b) a ``functional distance module", $d(s,g)$, that calculates how close the robot is to achieving its goal $g$ in the state $s$. The distance function is learned with limited human demonstration data, using a contrastive learning objective (see Fig.~\ref{fig:train_overview}). Both $d$ and $F$ are used in conjunction to greedily plan robot actions (see Fig.~\ref{fig:teaser} for intuition). Our experiments demonstrate that this approach works better than policy learning (via Behavior Cloning), because the pre-trained representation itself does the heavy lifting (thanks to its structure) and we entirely dodge the challenge of multi-modal, sequential action prediction. Furthermore, our learned distance function is both stable and easy to train, which allows it to easily scale and generalize to new scenarios.


To summarize, we show a simple approach for exploiting the information hidden in pre-trained visual representations. Our contributions include:
\begin{itemize}
    \item Developing a simple algorithm for acquiring a distance function and dynamics model by fine-tuning a pre-trained visual representation on minimal (human collected) data.
    \item Creating an effective manipulation controller that substantially outperforms State-of-the-Art (SOTA) prior methods from the robot learning community (e.g. Behavior Cloning~\cite{ross2011reduction}, Offline-RL~\cite{kostrikov2021offline}, etc.).
    \item Demonstrating that our approach can handle four realistic manipulation tasks, generalize to new objects and settings, and solve challenging scenarios w/ multi-modal action distributions. 
\end{itemize}