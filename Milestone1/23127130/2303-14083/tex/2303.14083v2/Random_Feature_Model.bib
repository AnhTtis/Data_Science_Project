@Article{lecun2015deep,
  added-at = {2022-01-19T10:28:11.000+0100},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  biburl = {https://www.bibsonomy.org/bibtex/2fcb4570f4481d2e36239cc683ce7261e/msteininger},
  interhash = {6e8511bc64ba3e808ebf330db96a4ea5},
  intrahash = {fcb4570f4481d2e36239cc683ce7261e},
  journal = {Nature},
  keywords = {imported},
  number = 7553,
  pages = 436,
  publisher = {Nature Publishing Group},
  timestamp = {2022-01-19T10:28:11.000+0100},
  title = {Deep learning},
  volume = 521,
  year = 2015
}

@book{Goodfellow2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@inproceedings{Kri+12,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}

@article{Sil+17,
  added-at = {2017-12-15T02:14:58.000+0100},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2ecdfbfcceb55ee5f14c1c375ad71f2cb/achakraborty},
  description = {Mastering the game of Go without human knowledge | Nature},
  interhash = {c45d318e105d0f2d62ccc28c2699d9d4},
  intrahash = {ecdfbfcceb55ee5f14c1c375ad71f2cb},
  journal = {Nature},
  keywords = {2017 deep-learning deepmind google paper reinforcement-learning},
  month = oct,
  pages = {354--},
  publisher = {Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  timestamp = {2017-12-15T02:14:58.000+0100},
  title = {Mastering the game of Go without human knowledge},
  url = {http://dx.doi.org/10.1038/nature24270},
  volume = 550,
  year = 2017
}
  
  
@article{Carleo+2019,
  title = {Machine learning and the physical sciences},
  author = {Carleo, Giuseppe and Cirac, Ignacio and Cranmer, Kyle and Daudet, Laurent and Schuld, Maria and Tishby, Naftali and Vogt-Maranto, Leslie and Zdeborov\'a, Lenka},
  journal = {Rev. Mod. Phys.},
  volume = {91},
  issue = {4},
  pages = {045002},
  numpages = {39},
  year = {2019},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/RevModPhys.91.045002},
  url = {https://link.aps.org/doi/10.1103/RevModPhys.91.045002}
}


@article{Zhang+2017,
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
title = {Understanding Deep Learning (Still) Requires Rethinking Generalization},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/3446776},
doi = {10.1145/3446776},
abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training.Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice.We interpret our experimental findings by comparison with traditional models.We supplement this republication with a new section at the end summarizing recent progresses in the field since the original version of this paper.},
journal = {Commun. ACM},
month = {feb},
pages = {107–115},
numpages = {9}
}

@article{novak2018sensitivity,
author = {Theunissen, Marthinus and Davel, Marelie and Barnard, Etienne},
year = {2020},
month = {12},
pages = {},
title = {Benign interpolation of noise in deep learning},
volume = {32},
journal = {South African Computer Journal},
doi = {10.18489/sacj.v32i2.833}
}

@misc{canziani2017,
title={An Analysis of Deep Neural Network Models for Practical Applications},
author={Alfredo Canziani and Adam Paszke and Eugenio Culurciello},
year={2017},
url={https://openreview.net/forum?id=Bygq-H9eg}
}

@inproceedings{novak2019bayesian,
title={Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes},
author={Roman Novak and Lechao Xiao and Yasaman Bahri and Jaehoon Lee and Greg Yang and Daniel A. Abolafia and Jeffrey Pennington and Jascha Sohl-dickstein},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1g30j0qF7},
}

@inproceedings{neyshabur2018the,
title={The role of over-parametrization in generalization of neural networks},
author={Behnam Neyshabur and Zhiyuan Li and Srinadh Bhojanapalli and Yann LeCun and Nathan Srebro},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=BygfghAcYX},
}

@ARTICLE{Bartlett98,
  author={Bartlett, P.L.},
  journal={IEEE Transactions on Information Theory}, 
  title={The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network}, 
  year={1998},
  volume={44},
  number={2},
  pages={525-536},
  doi={10.1109/18.661502}}


@article{Geman1992neural,
  added-at = {2007-09-05T16:48:50.000+0200},
  address = {Cambridge, MA, USA},
  author = {Geman, Stuart and Bienenstock, Elie and Doursat, René},
  biburl = {https://www.bibsonomy.org/bibtex/23a1b80127bf67d5a654428704999b958/sb3000},
  interhash = {0b7c767562b5cad476df040cbc4e44fd},
  intrahash = {3a1b80127bf67d5a654428704999b958},
  issn = {0899-7667},
  journal = {Neural Computation},
  keywords = {learning theory},
  number = 1,
  pages = {1--58},
  publisher = {MIT Press},
  timestamp = {2007-09-05T16:48:50.000+0200},
  title = {Neural Networks and the Bias/Variance Dilemma},
  url = {http://portal.acm.org/citation.cfm?id=148062},
  volume = 4,
  year = 1992
}

@article{Belkin+19,
author = {Mikhail Belkin  and Daniel Hsu  and Siyuan Ma  and Soumik Mandal },
title = {Reconciling modern machine-learning practice and the classical bias–variance trade-off},
journal = {Proceedings of the National Academy of Sciences},
volume = {116},
number = {32},
pages = {15849-15854},
year = {2019},
doi = {10.1073/pnas.1903070116},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1903070116},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1903070116},
}

@Inbook{Neal1996priors,
author="Neal, Radford M.",
title="Priors for Infinite Networks",
bookTitle="Bayesian Learning for Neural Networks",
year="1996",
publisher="Springer New York",
address="New York, NY",
pages="29--53",
abstract="In this chapter, I show that priors over network parameters can be defined in such a way that the corresponding priors over functions computed by the network reach reasonable limits as the number of hidden units goes to infinity. When using such priors,there is thus no need to limit the size of the network in order to avoid ``overfitting''. The infinite network limit also provides insight into the properties of different priors. A Gaussian prior for hidden-to-output weights results in a Gaussian process prior for functions,which may be smooth, Brownian, or fractional Brownian. Quite different effects can be obtained using priors based on non-Gaussian stable distributions. In networks with more than one hidden layer, a combination of Gaussian and non-Gaussian priors appears most interesting.",
isbn="978-1-4612-0745-0",
doi="10.1007/978-1-4612-0745-0_2",
url="https://doi.org/10.1007/978-1-4612-0745-0_2"
}

@inproceedings{lee2018deep,
title={Deep Neural Networks as Gaussian Processes},
author={Jaehoon Lee and Jascha Sohl-dickstein and Jeffrey Pennington and Roman Novak and Sam Schoenholz and Yasaman Bahri},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=B1EA-M-0Z},
}

@inproceedings{matthews2018gaussian,
title={Gaussian Process Behaviour in Wide Deep Neural Networks},
author={Alexander G. de G. Matthews and Jiri Hron and Mark Rowland and Richard E. Turner and Zoubin Ghahramani},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=H1-nGgWC-},
}

@inproceedings{Williams96,
 author = {Williams, Christopher},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 pages = {},
 publisher = {MIT Press},
 title = {Computing with Infinite Networks},
 url = {https://proceedings.neurips.cc/paper/1996/file/ae5e3ce40e0404a45ecacaaf05e5f735-Paper.pdf},
 volume = {9},
 year = {1996}
}

@book{hertz1991introduction,
author = {J. Hertz and A. Krough and R. Palmer},
year = {1991},
month = {12},
pages = {},
publisher ={West view Press},
title = {Introduction To The Theory Of Neural Computation},
volume = {44},
journal = {Physics Today - PHYS TODAY},
doi = {10.1063/1.2810360}
}

@article{Wat_92+,
  title = {The statistical mechanics of learning a rule},
  author = {Watkin, Timothy L. H. and Rau, Albrecht and Biehl, Michael},
  journal = {Rev. Mod. Phys.},
  volume = {65},
  issue = {2},
  pages = {499--556},
  numpages = {0},
  year = {1993},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/RevModPhys.65.499},
  url = {https://link.aps.org/doi/10.1103/RevModPhys.65.499}
}

@article{Saad98,
author = {Saad, David},
year = {2000},
month = {12},
pages = {},
title = {On-Line Learning in Neural Networks},
volume = {95},
journal = {Journal of the American Statistical Association},
doi = {10.2307/2669811}
}

@book{Engel2001, place={Cambridge}, title={Statistical Mechanics of Learning}, DOI={10.1017/CBO9781139164542}, publisher={Cambridge University Press}, author={Engel, A. and Van den Broeck, C.}, year={2001}}


@article{Bahri+20,
author = {Bahri, Yasaman and Kadmon, Jonathan and Pennington, Jeffrey and Schoenholz, Sam S. and Sohl-Dickstein, Jascha and Ganguli, Surya},
title = {Statistical Mechanics of Deep Learning},
journal = {Annual Review of Condensed Matter Physics},
volume = {11},
number = {1},
pages = {501-528},
year = {2020},
doi = {10.1146/annurev-conmatphys-031119-050745}
}
@article{Gardner88,
doi = {10.1088/0305-4470/21/1/030},
url = {https://dx.doi.org/10.1088/0305-4470/21/1/030},
year = {1988},
month = {jan},
publisher = {},
volume = {21},
number = {1},
pages = {257},
author = {E Gardner},
title = {The space of interactions in neural network models},
journal = {Journal of Physics A: Mathematical and General},
abstract = {The typical fraction of the space of interactions between each pair of N Ising spins which solve the problem of storing a given set of p random patterns as N-bit spin configurations is considered. The volume is calculated explicitly as a function of the storage ratio, alpha =p/N, of the value kappa (&gt;0) of the product of the spin and the magnetic field at each site and of the magnetisation, m. Here m may vary between 0 (no correlation) and 1 (completely correlated). The capacity increases with the correlation between patterns from alpha =2 for correlated patterns with kappa =0 and tends to infinity as m tends to 1. The calculations use a saddle-point method and the order parameters at the saddle point are assumed to be replica symmetric. This solution is shown to be locally stable. A local iterative learning algorithm for updating the interactions is given which will converge to a solution of given kappa provided such solutions exist.}
}

@article{gyorgyi1990neural,
  title={Neural networks and spin glasses},
  author={Gy{\"o}rgyi, G and Tishby, N},
  journal={World Scientific, Singapore},
  pages={3--36},
  year={1990}
}

@article{Seung+92,
  title = {Statistical mechanics of learning from examples},
  author = {Seung, H. S. and Sompolinsky, H. and Tishby, N.},
  journal = {Phys. Rev. A},
  volume = {45},
  issue = {8},
  pages = {6056--6091},
  numpages = {0},
  year = {1992},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.45.6056},
  url = {https://link.aps.org/doi/10.1103/PhysRevA.45.6056}
}

@article{Opper94,
  title = {Learning and generalization in a two-layer neural network: The role of the Vapnik-Chervonvenkis dimension},
  author = {Opper, Manfred},
  journal = {Phys. Rev. Lett.},
  volume = {72},
  issue = {13},
  pages = {2113--2116},
  numpages = {0},
  year = {1994},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.72.2113},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.72.2113}
}

@article{riegler1995line,
doi = {10.1088/0305-4470/28/20/002},
url = {https://dx.doi.org/10.1088/0305-4470/28/20/002},
year = {1995},
month = {oct},
publisher = {},
volume = {28},
number = {20},
pages = {L507},
author = {P Riegler and  M Biehl},
title = {On-line backpropagation in two-layered neural networks},
journal = {Journal of Physics A: Mathematical and General},
abstract = {We present an exact analysis of learning a rule by on-line gradient descent in a two-layered neural network with adjustable hidden-to-output weights (backpropagation of error). Results are compared with the training of networks having the same architecture but fixed weights in the second layer.}
}

@inproceedings{goldt2019dynamics,
 author = {Goldt, Sebastian and Advani, Madhu and Saxe, Andrew M and Krzakala, Florent and Zdeborov\'{a}, Lenka},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup},
 url = {https://proceedings.neurips.cc/paper/2019/file/cab070d53bd0d200746fb852a922064a-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{Yoshida_2020,
doi = {10.1088/1742-5468/abc62f},
url = {https://dx.doi.org/10.1088/1742-5468/abc62f},
year = {2020},
month = {dec},
publisher = {IOP Publishing and SISSA},
volume = {2020},
number = {12},
pages = {124013},
author = {Yuki Yoshida and Masato Okada},
title = {Data-dependence of plateau phenomenon in learning with neural network—statistical mechanical analysis*},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
abstract = {The plateau phenomenon, wherein the loss value stops decreasing during the process of learning, has been reported by various researchers. The phenomenon was actively inspected in the 1990s and found to be due to the fundamental hierarchical structure of neural network models. Then, the phenomenon has been thought of as inevitable. However, the phenomenon seldom occurs in the context of recent deep learning. There is a gap between theory and reality. In this paper, using statistical mechanical formulation, we clarified the relationship between the plateau phenomenon and the statistical property of the data learned. It is shown that the data whose covariance has small and dispersed eigenvalues tend to make the plateau phenomenon inconspicuous.}
}

@article{Goldt+20,
  title = {Modeling the Influence of Data Structure on Learning in Neural Networks: The Hidden Manifold Model},
  author = {Goldt, Sebastian and M\'ezard, Marc and Krzakala, Florent and Zdeborov\'a, Lenka},
  journal = {Phys. Rev. X},
  volume = {10},
  issue = {4},
  pages = {041044},
  numpages = {32},
  year = {2020},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevX.10.041044},
  url = {https://link.aps.org/doi/10.1103/PhysRevX.10.041044}
}
@article{Schwarze+93,
doi = {10.1209/0295-5075/21/7/012},
url = {https://dx.doi.org/10.1209/0295-5075/21/7/012},
year = {1993},
month = {mar},
publisher = {},
volume = {21},
number = {7},
pages = {785},
author = {H. Schwarze and  J. Hertz},
title = {Generalization in Fully Connected Committee Machines},
journal = {Europhysics Letters},
abstract = {We study supervised learning in a fully connected committee machine trained to implement a rule of the same structure. The generalization error as a function of the number of training examples per weight is calculated within the annealed approximation. For binary weights we find a discontinuous transition from poor to perfect generalization. Beyond this transition metastable states exist even for large training sets. The scaling of the order parameters with the number of hidden units depends on the size of the training set. For continuous weights we find a discontinuous transition from a committee-symmetric solution to one with specialized hidden units.}
}

@article{biehl1995learning,
doi = {10.1088/0305-4470/28/3/018},
url = {https://dx.doi.org/10.1088/0305-4470/28/3/018},
year = {1995},
month = {feb},
publisher = {},
volume = {28},
number = {3},
pages = {643},
author = {M Biehl and  H Schwarze},
title = {Learning by on-line gradient descent},
journal = {Journal of Physics A: Mathematical and General},
abstract = {We study on-line gradient-descent learning in multilayer networks analytically and numerically. The training is based on randomly drawn inputs and their corresponding outputs as defined by a target rule. In the thermodynamic limit we derive deterministic differential equations for the order parameters of the problem which allow an exact calculation of the evolution of the generalization error. First we consider a single-layer perceptron with sigmoidal activation function learning a target rule defined by a network of the same architecture. For this model the generalization error decays exponentially with the number of training examples if the learning rate is sufficiently small. However, if the learning rate is increased above a critical value, perfect learning is no longer possible. For architectures with hidden layers and fixed hidden-to-output weights, such as the parity and the committee machine, we find additional effects related to the existence of symmetries in these problems.}
}
@article{saad1995line,
  title = {On-line learning in soft committee machines},
  author = {Saad, David and Solla, Sara A.},
  journal = {Phys. Rev. E},
  volume = {52},
  issue = {4},
  pages = {4225--4243},
  numpages = {0},
  year = {1995},
  month = {Oct},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.52.4225},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.52.4225}
}

@article{saad1995exact,
  title = {Exact Solution for On-Line Learning in Multilayer Neural Networks},
  author = {Saad, David and Solla, Sara A.},
  journal = {Phys. Rev. Lett.},
  volume = {74},
  issue = {21},
  pages = {4337--4340},
  numpages = {0},
  year = {1995},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.74.4337},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.74.4337}
}

@article{Biehltransient,
doi = {10.1088/0305-4470/29/16/005},
url = {https://dx.doi.org/10.1088/0305-4470/29/16/005},
year = {1996},
month = {aug},
publisher = {},
volume = {29},
number = {16},
pages = {4769},
author = {Michael Biehl and  Peter Riegler and  Christian Wöhler},
title = {Transient dynamics of on-line learning in two-layered neural networks},
journal = {Journal of Physics A: Mathematical and General},
abstract = {The dynamics of on-line learning in neural networks with continuous units is dominated by plateaux in the time dependence of the generalization error. Using tools from statistical mechanics, we show for a soft committee machine the existence of several fixed points of the dynamics of learning that give rise to complicated behaviour, such as cascade-like runs through different plateaux with a decreasing value of the corresponding generalization error. We find learning-rate-dependent phenomena, such as splitting and disappearing of fixed points of the equations of motion. The dependence of plateau lengths on the initial conditions is described analytically and simulations confirm the results.}
}

@article{Biehl+98,
doi = {10.1209/epl/i1998-00466-6},
url = {https://dx.doi.org/10.1209/epl/i1998-00466-6},
year = {1998},
month = {oct},
publisher = {},
volume = {44},
number = {2},
pages = {261},
author = {M. Biehl and  E. Schlösser and  M. Ahr},
title = {Phase transitions in soft-committee machines},
journal = {Europhysics Letters},
abstract = {Equilibrium statistical physics is applied to the off-line training of  layered neural networks with differentiable activation functions. A first analysis of soft-committee machines with an arbitrary number  (K) of hidden units and continuous weights learning a perfectly matching rule is performed. Our results are exact in the limit of high training temperatures (β → 0).  For K = 2 we find a second-order phase transition from unspecialized to  specialized student configurations at a critical size P of the training set,  whereas for K ⩾ 3 the transition is first order.  The limit K → ∞ can be performed analytically, the transition occurs after presenting  on the order of NK/β examples. However, an unspecialized metastable state  persists up to P ∝ NK2/β.}
}


@article{straat2019line,
title = {Hidden unit specialization in layered neural networks: ReLU vs. sigmoidal activation},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {564},
pages = {125517},
year = {2021},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2020.125517},
url = {https://www.sciencedirect.com/science/article/pii/S0378437120308153},
author = {Elisa Oostwal and Michiel Straat and Michael Biehl},
keywords = {Neural networks, Machine learning, Statistical physics},
abstract = {By applying concepts from the statistical physics of learning, we study layered neural networks of rectified linear units (ReLU). The comparison with conventional, sigmoidal activation functions is in the center of interest. We compute typical learning curves for large shallow networks with K hidden units in matching student teacher scenarios. The systems undergo phase transitions, i.e. sudden changes of the generalization performance via the process of hidden unit specialization at critical sizes of the training set. Surprisingly, our results show that the training behavior of ReLU networks is qualitatively different from that of networks with sigmoidal activations. In networks with K≥3 sigmoidal hidden units, the transition is discontinuous: Specialized network configurations co-exist and compete with states of poor performance even for very large training sets. On the contrary, the use of ReLU activations results in continuous transitions for all K. For large enough training sets, two competing, differently specialized states display similar generalization abilities, which coincide exactly for large hidden layers in the limit K→∞. Our findings are also confirmed in Monte Carlo simulations of the training processes.}
}

@article{Richert2022,
  title = {Soft mode in the dynamics of over-realizable online learning for soft committee machines},
  author = {Richert, Frederieke and Worschech, Roman and Rosenow, Bernd},
  journal = {Phys. Rev. E},
  volume = {105},
  issue = {5},
  pages = {L052302},
  numpages = {6},
  year = {2022},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.105.L052302},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.105.L052302}
}

@article{Naveh+20,
  title = {Predicting the outputs of finite deep neural networks trained with noisy gradients},
  author = {Naveh, Gadi and Ben David, Oded and Sompolinsky, Haim and Ringel, Zohar},
  journal = {Phys. Rev. E},
  volume = {104},
  issue = {6},
  pages = {064301},
  numpages = {19},
  year = {2021},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.104.064301},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.104.064301}
}

@article{Li2021statistical,
  title = {Statistical Mechanics of Deep Linear Neural Networks: The Backpropagating Kernel Renormalization},
  author = {Li, Qianyi and Sompolinsky, Haim},
  journal = {Phys. Rev. X},
  volume = {11},
  issue = {3},
  pages = {031059},
  numpages = {32},
  year = {2021},
  month = {Sep},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevX.11.031059},
  url = {https://link.aps.org/doi/10.1103/PhysRevX.11.031059}
}

@article{Cohen+21,
  title = {Learning curves for overparametrized deep neural networks: A field theory perspective},
  author = {Cohen, Omry and Malka, Or and Ringel, Zohar},
  journal = {Phys. Rev. Res.},
  volume = {3},
  issue = {2},
  pages = {023034},
  numpages = {22},
  year = {2021},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevResearch.3.023034},
  url = {https://link.aps.org/doi/10.1103/PhysRevResearch.3.023034}
}

@inproceedings{Lee2019wide,
 author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent},
 url = {https://proceedings.neurips.cc/paper/2019/file/0d1a9651497a38d8b1c3871c84528bd4-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{Chizat2019on,
 author = {Chizat, L\'{e}na\"{\i}c and Oyallon, Edouard and Bach, Francis},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On Lazy Training in Differentiable Programming},
 url = {https://proceedings.neurips.cc/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{Bach2022gradient,
  TITLE = {{Gradient Descent on Infinitely Wide Neural Networks: Global Convergence and Generalization}},
  AUTHOR = {Bach, Francis and Chizat, Lenaïc},
  URL = {https://hal.science/hal-03379011},
  BOOKTITLE = {{International Congress of Mathematicians}},
  ADDRESS = {Saint-Petersbourg, Russia},
  YEAR = {2022},
  PDF = {https://hal.science/hal-03379011/file/ICM-Bach-Chizat-HAL.pdf},
  HAL_ID = {hal-03379011},
  HAL_VERSION = {v1},
}

@inproceedings{ChiBa18,
 author = {Chizat, L\'{e}na\"{\i}c and Bach, Francis},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport},
 url = {https://proceedings.neurips.cc/paper/2018/file/a1afc58c6ca9540d057299ec3016d726-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{Chen+20,
 author = {Chen, Zixiang and Cao, Yuan and Gu, Quanquan and Zhang, Tong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {13363--13373},
 publisher = {Curran Associates, Inc.},
 title = {A Generalized Neural Tangent Kernel Analysis for Two-layer Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2020/file/9afe487de556e59e6db6c862adfe25a4-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{Jacot+18,
 author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{Arora+19,
 author = {Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On Exact Computation with an Infinitely Wide Neural Net},
 url = {https://proceedings.neurips.cc/paper/2019/file/dbc4d84bfcfe2284ba11beffb853a8c4-Paper.pdf},
 volume = {32},
 year = {2019}
}


@InProceedings{Arora2,
  title = 	 {Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author =       {Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {322--332},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/arora19a/arora19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/arora19a.html},
  abstract = 	 {Recent works have cast some light on the mystery of why deep nets fit any data and generalize despite being very overparametrized. This paper analyzes training and generalization for a simple 2-layer ReLU net with random initialization, and provides the following improvements over recent works: (i) Using a tighter characterization of training speed than recent papers, an explanation for why training a neural net with random labels leads to slower training, as originally observed in [Zhang et al. ICLR’17]. (ii) Generalization bound independent of network size, using a data-dependent complexity measure. Our measure distinguishes clearly between random labels and true labels on MNIST and CIFAR, as shown by experiments. Moreover, recent papers require sample complexity to increase (slowly) with the size, while our sample complexity is completely independent of the network size. (iii) Learnability of a broad class of smooth functions by 2-layer ReLU nets trained via gradient descent. The key idea is to track dynamics of training and generalization via properties of a related kernel.}
}

@inproceedings{Du19,
title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
author={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1eK3i09YQ},
}

@article{Geiger20,
doi = {10.1088/1742-5468/abc4de},
url = {https://dx.doi.org/10.1088/1742-5468/abc4de},
year = {2020},
month = {nov},
publisher = {IOP Publishing and SISSA},
volume = {2020},
number = {11},
pages = {113301},
author = {Mario Geiger and Stefano Spigler and Arthur Jacot and Matthieu Wyart},
title = {Disentangling feature and lazy training in deep neural networks},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
abstract = {Two distinct limits for deep learning have been derived as the network width h → ∞, depending on how the weights of the last layer scale with h. In the neural tangent Kernel (NTK) limit, the dynamics becomes linear in the weights and is described by a frozen kernel Θ (the NTK). By contrast, in the mean-field limit, the dynamics can be expressed in terms of the distribution of the parameters associated with a neuron, that follows a partial differential equation. In this work we consider deep networks where the weights in the last layer scale as αh −1/2 at initialization. By varying α and h, we probe the crossover between the two limits. We observe two the previously identified regimes of ‘lazy training’ and ‘feature training’. In the lazy-training regime, the dynamics is almost linear and the NTK barely changes after initialization. The feature-training regime includes the mean-field formulation as a limiting case and is characterized by a kernel that evolves in time, and thus learns some features. We perform numerical experiments on MNIST, Fashion-MNIST, EMNIST and CIFAR10 and consider various architectures. We find that: (i) the two regimes are separated by an α* that scales as . (ii) Network architecture and data structure play an important role in determining which regime is better: in our tests, fully-connected networks perform generally better in the lazy-training regime, unlike convolutional networks. (iii) In both regimes, the fluctuations δF induced on the learned function by initial conditions decay as , leading to a performance that increases with h. The same improvement can also be obtained at an intermediate width by ensemble-averaging several networks that are trained independently. (iv) In the feature-training regime we identify a time scale , such that for t ≪ t 1 the dynamics is linear. At t ∼ t 1, the output has grown by a magnitude  and the changes of the tangent kernel | |ΔΘ| | become significant. Ultimately, it follows  for ReLU and Softplus activation functions, with a &lt; 2 and a → 2 as depth grows. We provide scaling arguments supporting these findings.}
}

@article{Geiger21,
title = {Landscape and training regimes in deep learning},
journal = {Physics Reports},
volume = {924},
pages = {1-18},
year = {2021},
note = {Landscape and training regimes in deep learning},
issn = {0370-1573},
doi = {https://doi.org/10.1016/j.physrep.2021.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0370157321001290},
author = {Mario Geiger and Leonardo Petrini and Matthieu Wyart},
keywords = {Deep learning, Jamming, Lazy training, Feature learning, Neural networks, Loss landscape, Curse of dimensionality, Neural tangent kernel},
abstract = {Deep learning algorithms are responsible for a technological revolution in a variety of tasks including image recognition or Go playing. Yet, why they work is not understood. Ultimately, they manage to classify data lying in high dimension – a feat generically impossible due to the geometry of high dimensional space and the associated curse of dimensionality. Understanding what kind of structure, symmetry or invariance makes data such as images learnable is a fundamental challenge. Other puzzles include that (i) learning corresponds to minimizing a loss in high dimension, which is in general not convex and could well get stuck bad minima. (ii) Deep learning predicting power increases with the number of fitting parameters, even in a regime where data are perfectly fitted. In this manuscript, we review recent results elucidating (i, ii) and the perspective they offer on the (still unexplained) curse of dimensionality paradox. We base our theoretical discussion on the (h,α) plane where h controls the number of parameters and α the scale of the output of the network at initialization, and provide new systematic measures of performance in that plane for two common image classification datasets. We argue that different learning regimes can be organized into a phase diagram. A line of critical points sharply delimits an under-parametrized phase from an over-parametrized one. In over-parametrized nets, learning can operate in two regimes separated by a smooth cross-over. At large initialization, it corresponds to a kernel method, whereas for small initializations features can be learnt, together with invariants in the data. We review the properties of these different phases, of the transition separating them and some open questions. Our treatment emphasizes analogies with physical systems, scaling arguments and the development of numerical observables to quantitatively test these results empirically. Practical implications are also discussed, including the benefit of averaging nets with distinct initial weights, or the choice of parameters (h,α) optimizing performance.}
}

@article{Rahimi2008,
  title={Uniform approximation of functions with random bases},
  author={Ali Rahimi and Benjamin Recht},
  journal={2008 46th Annual Allerton Conference on Communication, Control, and Computing},
  year={2008},
  pages={555-561}
}

@inproceedings{Rahimi2007,
 author = {Rahimi, Ali and Recht, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Random Features for Large-Scale Kernel Machines},
 url = {https://proceedings.neurips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
 volume = {20},
 year = {2007}
}

@article{advani2020high,
title = {High-dimensional dynamics of generalization error in neural networks},
journal = {Neural Networks},
volume = {132},
pages = {428-446},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.08.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020303117},
author = {Madhu S. Advani and Andrew M. Saxe and Haim Sompolinsky},
keywords = {Neural networks, Generalization error, Random matrix theory},
abstract = {We perform an analysis of the average generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant “high-dimensional” regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that standard application of theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.}
}


@InProceedings{D'Ascoli2020,
  title = 	 {Double Trouble in Double Descent: Bias and Variance(s) in the Lazy Regime},
  author =       {D'Ascoli, St{\'e}phane and Refinetti, Maria and Biroli, Giulio and Krzakala, Florent},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {2280--2290},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/d-ascoli20a/d-ascoli20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/d-ascoli20a.html},
  abstract = 	 {Deep neural networks can achieve remarkable generalization performances while interpolating the training data. Rather than the U-curve emblematic of the bias-variance trade-off, their test error often follows a "double descent"—a mark of the beneficial role of overparametrization. In this work, we develop a quantitative theory for this phenomenon in the so-called lazy learning regime of neural networks, by considering the problem of learning a high-dimensional function with random features regression. We obtain a precise asymptotic expression for the bias-variance decomposition of the test error, and show that the bias displays a phase transition at the interpolation threshold, beyond it which it remains constant. We disentangle the variances stemming from the sampling of the dataset, from the additive noise corrupting the labels, and from the initialization of the weights. We demonstrate that the latter two contributions are the crux of the double descent: they lead to the overfitting peak at the interpolation threshold and to the decay of the test error upon overparametrization. We quantify how they are suppressed by ensembling the outputs of $K$ independently initialized estimators. For $K\rightarrow \infty$, the test error is monotonously decreasing and remains constant beyond the interpolation threshold. We further compare the effects of overparametrizing, ensembling and regularizing. Finally, we present numerical experiments on classic deep learning setups to show that our results hold qualitatively in realistic lazy learning scenarios.}
}

@article{Mei2022the,
author = {Mei, Song and Montanari, Andrea},
title = {The Generalization Error of Random Features Regression: Precise Asymptotics and the Double Descent Curve},
journal = {Communications on Pure and Applied Mathematics},
volume = {75},
number = {4},
pages = {667-766},
doi = {https://doi.org/10.1002/cpa.22008},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.22008},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.22008},
abstract = {Abstract Deep learning methods operate in regimes that defy the traditional statistical mindset. Neural network architectures often contain more parameters than training samples, and are so rich that they can interpolate the observed labels, even if the latter are replaced by pure noise. Despite their huge complexity, the same architectures achieve small generalization error on real data. This phenomenon has been rationalized in terms of a so-called ‘double descent’ curve. As the model complexity increases, the test error follows the usual U-shaped curve at the beginning, first decreasing and then peaking around the interpolation threshold (when the model achieves vanishing training error). However, it descends again as model complexity exceeds this threshold. The global minimum of the test error is found above the interpolation threshold, often in the extreme overparametrization regime in which the number of parameters is much larger than the number of samples. Far from being a peculiar property of deep neural networks, elements of this behavior have been demonstrated in much simpler settings, including linear regression with random covariates. In this paper we consider the problem of learning an unknown function over the -dimensional sphere , from i.i.d. samples , . We perform ridge regression on random features of the form , . This can be equivalently described as a two-layer neural network with random first-layer weights. We compute the precise asymptotics of the test error, in the limit with and fixed. This provides the first analytically tractable model that captures all the features of the double descent phenomenon without assuming ad hoc misspecification structures. In particular, above a critical value of the signal-to-noise ratio, minimum test error is achieved by extremely overparametrized interpolators, i.e., networks that have a number of parameters much larger than the sample size, and vanishing training error. © 2021 Wiley Periodicals LLC.},
year = {2022}
}


@article{Hastie22,
author = {Trevor Hastie and Andrea Montanari and Saharon Rosset and Ryan J. Tibshirani},
title = {{Surprises in high-dimensional ridgeless least squares interpolation}},
volume = {50},
journal = {The Annals of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {949 -- 986},
keywords = {interpolation, overparametrization, Random matrix theory, regression, Ridge regression},
year = {2022},
doi = {10.1214/21-AOS2133},
URL = {https://doi.org/10.1214/21-AOS2133}
}


@InProceedings{Yang21,
  title = 	 {Exact Gap between Generalization Error and Uniform Convergence in Random Feature Models},
  author =       {Yang, Zitong and Bai, Yu and Mei, Song},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11704--11715},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/yang21a/yang21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/yang21a.html},
  abstract = 	 {Recent work showed that there could be a large gap between the classical uniform convergence bound and the actual test error of zero-training-error predictors (interpolators) such as deep neural networks. To better understand this gap, we study the uniform convergence in the nonlinear random feature model and perform a precise theoretical analysis on how uniform convergence depends on the sample size and the number of parameters. We derive and prove analytical expressions for three quantities in this model: 1) classical uniform convergence over norm balls, 2) uniform convergence over interpolators in the norm ball (recently proposed by&nbsp;\citet{zhou2021uniform}), and 3) the risk of minimum norm interpolator. We show that, in the setting where the classical uniform convergence bound is vacuous (diverges to $\infty$), uniform convergence over the interpolators still gives a non-trivial bound of the test error of interpolating solutions. We also showcase a different setting where classical uniform convergence bound is non-vacuous, but uniform convergence over interpolators can give an improved sample complexity guarantee. Our result provides a first exact comparison between the test errors and uniform convergence bounds for interpolators beyond simple linear models.}
}


@article{Mei2022Generalization,
title = {Generalization error of random feature and kernel methods: Hypercontractivity and kernel matrix concentration},
journal = {Applied and Computational Harmonic Analysis},
volume = {59},
pages = {3-84},
year = {2022},
note = {Special Issue on Harmonic Analysis and Machine Learning},
issn = {1063-5203},
author = {Song Mei and Theodor Misiakiewicz and Andrea Montanari},
}

@article{Gardner1989,
  TITLE = {{Three unfinished works on the optimal storage capacity of networks}},
  AUTHOR = {Gardner, E. and Derrida, Bernard},
  URL = {https://hal.science/hal-03285594},
  JOURNAL = {{Journal of Physics A : Mathematical and General}},
  VOLUME = {22},
  NUMBER = {12},
  PAGES = {1983-1994},
  YEAR = {1989},
  DOI = {10.1088/0305-4470/22/12/004},
  PDF = {https://hal.science/hal-03285594/file/Three%20unfinished%20works%20on%20the%20optimal%20storage%20capacity%20of%20networks.pdf},
  HAL_ID = {hal-03285594},
  HAL_VERSION = {v1},
}

@inbook{Yehudai2019on,
author = {Yehudai, Gilad and Shamir, Ohad},
title = {On the Power and Limitations of Random Features for Understanding Neural Networks},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recently, a spate of papers have provided positive theoretical results for training over-parameterized neural networks (where the network size is larger than what is needed to achieve low error). The key insight is that with sufficient over-parameterization, gradient-based methods will implicitly leave some components of the network relatively unchanged, so the optimization dynamics will behave as if those components are essentially fixed at their initial random values. In fact, fixing these explicitly leads to the well-known approach of learning with random features (e.g. [27, 29]). In other words, these techniques imply that we can successfully learn with neural networks, whenever we can successfully learn with random features. In this paper, we formalize the link between existing results and random features, and argue that despite the impressive positive results, random feature approaches are also inherently limited in what they can explain. In particular, we prove that random features cannot be used to learn even a single ReLU neuron (over standard Gaussian inputs in ℝd and poly(d) weights), unless the network size (or magnitude of its weights) is exponentially large in d. Since a single neuron is known to be learnable with gradient-based methods, we conclude that we are still far from a satisfying general explanation for the empirical success of neural networks. For completeness we also provide a simple self-contained proof, using a random features technique, that one-hidden-layer neural networks can learn low-degree polynomials.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {592},
numpages = {11}
}

@article{Ghorbani2021linearized,
author = {Behrooz Ghorbani and Song Mei and Theodor Misiakiewicz and Andrea Montanari},
title = {{Linearized two-layers neural networks in high dimension}},
volume = {49},
journal = {The Annals of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {1029 -- 1054},
keywords = {approximation bounds, kernel ridge regression, neural tangent kernel, random features, Two-layers neural networks},
year = {2021},
doi = {10.1214/20-AOS1990},
URL = {https://doi.org/10.1214/20-AOS1990}
}

@inproceedings{Ghorbani2019limitation,
 author = {Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Limitations of Lazy Training of Two-layers Neural Network},
 url = {https://proceedings.neurips.cc/paper/2019/file/c133fb1bb634af68c5088f3438848bfd-Paper.pdf},
 volume = {32},
 year = {2019}
}


@InProceedings{Samarin22,
  title = 	 {Feature learning and random features in standard finite-width convolutional neural networks: An empirical study},
  author =       {Samarin, Maxim and Roth, Volker and Belius, David},
  booktitle = 	 {Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1718--1727},
  year = 	 {2022},
  editor = 	 {Cussens, James and Zhang, Kun},
  volume = 	 {180},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {01--05 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v180/samarin22a/samarin22a.pdf},
  url = 	 {https://proceedings.mlr.press/v180/samarin22a.html},
  abstract = 	 {The Neural Tangent Kernel is an important milestone in the ongoing effort to build a theory for deep learning. Its prediction that sufficiently wide neural networks behave as kernel methods, or equivalently as random feature models arising from linearized networks, has been confirmed empirically for certain wide architectures. In this paper, we compare the performance of two common finite-width convolutional neural networks, LeNet and AlexNet, to their linearizations on common benchmark datasets like MNIST and modified versions of it, CIFAR-10 and an ImageNet subset. We demonstrate empirically that finite-width neural networks, generally, greatly outperform the finite-width linearization of these architectures. When increasing the problem difficulty of the classification task, we observe a larger gap which is in line with common intuition that finite-width neural networks perform feature learning which finite-width linearizations cannot. At the same time, finite-width linearizations improve dramatically with width, approaching the behavior of the wider standard networks which in turn perform slightly better than their standard width counterparts. Therefore, it appears that feature learning for non-wide standard networks is important but becomes less significant with increasing width. We furthermore identify cases where both standard and linearized networks match in performance, in agreement with NTK theory, and a case where a wide linearization outperforms its standard width counterpart.}
}

@article{reents+98,
  title = {Self-Averaging and On-Line Learning},
  author = {Reents, G. and Urbanczik, R.},
  journal = {Phys. Rev. Lett.},
  volume = {80},
  issue = {24},
  pages = {5445--5448},
  numpages = {0},
  year = {1998},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.80.5445},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.80.5445}
}









@article{Mei+18,
author = {Song Mei  and Andrea Montanari  and Phan-Minh Nguyen },
title = {A mean field view of the landscape of two-layer neural networks},
journal = {Proceedings of the National Academy of Sciences},
volume = {115},
number = {33},
pages = {E7665-E7671},
year = {2018},
doi = {10.1073/pnas.1806579115},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1806579115},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1806579115},
abstract = {Multilayer neural networks are among the most powerful models in machine learning, yet the fundamental reasons for this success defy mathematical understanding. Learning a neural network requires optimizing a nonconvex high-dimensional objective (risk function), a problem that is usually attacked using stochastic gradient descent (SGD). Does SGD converge to a global optimum of the risk or only to a local optimum? In the former case, does this happen because local minima are absent or because SGD somehow avoids them? In the latter, why do local minima reached by SGD have good generalization properties? In this paper, we consider a simple case, namely two-layer neural networks, and prove that—in a suitable scaling limit—SGD dynamics is captured by a certain nonlinear partial differential equation (PDE) that we call distributional dynamics (DD). We then consider several specific examples and show how DD can be used to prove convergence of SGD to networks with nearly ideal generalization error. This description allows for “averaging out” some of the complexities of the landscape of neural networks and can be used to prove a general convergence result for noisy SGD.}}

@InProceedings{Mei+19,
  title = 	 {Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
  author =       {Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle = 	 {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages = 	 {2388--2464},
  year = 	 {2019},
  editor = 	 {Beygelzimer, Alina and Hsu, Daniel},
  volume = 	 {99},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--28 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v99/mei19a/mei19a.pdf},
  url = 	 {https://proceedings.mlr.press/v99/mei19a.html},
  abstract = 	 {We consider learning two layer neural networks using stochastic gradient descent. The mean-field description of this learning dynamics approximates the evolution of the network weights by an evolution in the space of probability distributions in $\mathbb{R}^D$ (where $D$ is the number of parameters associated to each neuron). This evolution can be defined through a partial differential equation or, equivalently, as the gradient flow in the Wasserstein space of probability distributions. Earlier work shows that (under some regularity assumptions), the mean field description is accurate as soon as the number of hidden units is much larger than the dimension $D$. In this paper we establish stronger and more general approximation guarantees. First of all, we show that the number of hidden units only needs to be larger than a quantity dependent on the regularity properties of the data, and independent of the dimensions. Next, we generalize this analysis to the case of unbounded activation functions, which was not covered by earlier bounds.  We extend our results to noisy stochastic gradient descent. Finally, we show that kernel ridge regression can be recovered as a special limit of  the mean field analysis. }
}

@misc{Nguyen2020,
  doi = {10.48550/ARXIV.2001.11443},
  
  url = {https://arxiv.org/abs/2001.11443},
  
  author = {Nguyen, Phan-Minh and Pham, Huy Tuan},
  
  keywords = {Machine Learning (cs.LG), Statistical Mechanics (cond-mat.stat-mech), Probability (math.PR), Statistics Theory (math.ST), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Radhakrishnan22,
  doi = {10.48550/ARXIV.2212.13881},
  
  url = {https://arxiv.org/abs/2212.13881},
  
  author = {Radhakrishnan, Adityanarayanan and Beaglehole, Daniel and Pandit, Parthe and Belkin, Mikhail},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Feature learning in neural networks and kernel machines that recursively learn features},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Zero v1.0 Universal}
}

@article{Hu2019,
  title={On the diffusion approximation of nonconvex stochastic gradient descent},
  author={Hu, Wenqing and Li, Chris Junchi and Li, Lei and Liu, Jian-Guo},
  journal={Annals of Mathematical Sciences and Applications},
  volume={4},
  number={1},
  pages={3--32},
  year={2019},
  publisher={International Press of Boston}
}

@article{Mandt2017,
author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
title = {Stochastic Gradient Descent as Approximate Bayesian Inference},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Stochastic Gradient Descent with a constant learning rate (constant SGD) simulates a Markov chain with a stationary distribution. With this perspective, we derive several new results. (1) We show that constant SGD can be used as an approximate Bayesian posterior inference algorithm. Specifically, we show how to adjust the tuning parameters of constant SGD to best match the stationary distribution to a posterior, minimizing the Kullback-Leibler divergence between these two distributions. (2) We demonstrate that constant SGD gives rise to a new variational EM algorithm that optimizes hyperparameters in complex probabilistic models. (3) We also show how to tune SGD with momentum for approximate sampling. (4)We analyze stochastic-gradient MCMC algorithms. For Stochastic-Gradient Langevin Dynamics and Stochastic-Gradient Fisher Scoring, we quantify the approximation errors due to finite learning rates. Finally (5), we use the stochastic process perspective to give a short proof of why Polyak averaging is optimal. Based on this idea, we propose a scalable approximate MCMC algorithm, the Averaged Stochastic Gradient Sampler.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {4873–4907},
numpages = {35},
keywords = {stochastic differential equations, variational inference, approximate Bayesian inference, stochastic optimization, stochastic gradient MCMC}
}

@article{Latz2021,
author = {Latz, Jonas},
title = {Analysis of Stochastic Gradient Descent in Continuous Time},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {4},
issn = {0960-3174},
url = {https://doi.org/10.1007/s11222-021-10016-8},
doi = {10.1007/s11222-021-10016-8},
abstract = {Stochastic gradient descent is an optimisation method that combines classical gradient descent with random subsampling within the target functional. In this work, we introduce the stochastic gradient process as a continuous-time representation of stochastic gradient descent. The stochastic gradient process is a dynamical system that is coupled with a continuous-time Markov process living on a finite state space. The dynamical system—a gradient flow—represents the gradient descent part, the process on the finite state space represents the random subsampling. Processes of this type are, for instance, used to model clonal populations in fluctuating environments. After introducing it, we study theoretical properties of the stochastic gradient process: We show that it converges weakly to the gradient flow with respect to the full target function, as the learning rate approaches zero. We give conditions under which the stochastic gradient process with constant learning rate is exponentially ergodic in the Wasserstein sense. Then we study the case, where the learning rate goes to zero sufficiently slowly and the single target functions are strongly convex. In this case, the process converges weakly to the point mass concentrated in the global minimum of the full target function; indicating consistency of the method. We conclude after a discussion of discretisation strategies for the stochastic gradient process and numerical experiments.},
journal = {Statistics and Computing},
month = {jul},
numpages = {25},
keywords = {Stochastic optimisation, 37A25, 60J25, Wasserstein distance, 90C30, Ergodicity, Piecewise-deterministic Markov processes, 65C40, 68W20}
}

@book{VANKAMPEN2007,
  abstract = {Of late there is an increasing interest in fluctuating phenomena and stochastic methods describing them, and the present book is an attempt to consolidate such efforts. The book consists of 14 chapters and can be conveniently divided into four parts. Part I consists of 1. Stochastic variables, 2. Random events, 3. Stochastic processes and 4. Markov processes; Part II comprises 5. The master equation, 6. One-step processes, 7. Chemical reactions, and 8. The Fokker-Planck equations and Langevin equations; Part III contains 9. The expansion of the master equation, 10. The diffusion type, and 11. Unstable systems; Part IV deals with diverse topics: 12. Fluctuation in continuous systems, 13. Statistics of jump events, and 14. Stochastic differential equations.

The earliest work on stochastic processes with special reference to problems of physical sciences dates back to 1943, namely, Chandrasekhar's classic review article. There were practically no books written in this flavour until the decade commencing from 1960, by which time the use of stochastic processes had become very popular. Many books have appeared in print since then. The book under review is distinctively different from the existing ones in content and more particularly in the basic approach. Part I covers material of a classical type; even there the approach is very stimulating and a graduate student, particularly in physics or chemistry, will profit enormously by studying the material and working out the problems indicated here and there. Parts II and III cover a good amount of the work of the author himself and it is here that even specialists in the subject will find it worthwhile to read through. The topics treated in Part IV take the reader to the very specialized areas in the frontier of research. Discussions of some topics like statistics of jump events and stochastic differential equations are rather incomplete. The author would have done well to bring them to the same level as the rest of the book. For instance, the use of stochastic calculi in modeling is a significant omission. Likewise, the statistics of jump events could have been improved to include a full discussion on the response phenomena.

On the whole the reviewer found the book most enjoyable and the book can be recommended to applied probabilists, physicists and physical chemists. The graduate student, especially the one who aspires to do modeling in physical sciences, will find the book very inspiring. },
  added-at = {2010-06-08T20:50:38.000+0200},
  author = {Kampen, NG Van},
  biburl = {https://www.bibsonomy.org/bibtex/266a3bdcdac5b6076573777801c9540b4/peter.ralph},
  description = {{Stochastic processes in physics and chemistry}},
  interhash = {def957921fdecb4a7225e1fe892543f9},
  intrahash = {66a3bdcdac5b6076573777801c9540b4},
  keywords = {linear_noise_approximation reference stochastic_transcription},
  publisher = {North Holland},
  timestamp = {2010-06-08T20:50:38.000+0200},
  title = {Stochastic processes in physics and chemistry},
  year = 2007,
  pages = {96--133}
 
}

@article{Marenko1967,
doi = {10.1070/SM1967v001n04ABEH001994},
url = {https://dx.doi.org/10.1070/SM1967v001n04ABEH001994},
year = {1967},
month = {apr},
publisher = {},
volume = {1},
number = {4},
pages = {457},
author = {V A Marčenko and  L A Pastur},
title = {DISTRIBUTION OF EIGENVALUES FOR SOME SETS OF RANDOM MATRICES},
journal = {Mathematics of the USSR-Sbornik},
abstract = {}
}

@article{Rotskoff2022,
author = {Rotskoff, Grant and Vanden-Eijnden, Eric},
year = {2022},
month = {09},
pages = {1889-1935},
title = {Trainability and Accuracy of Artificial Neural Networks: An Interacting Particle System Approach},
volume = {75},
journal = {Communications on Pure and Applied Mathematics},
doi = {10.1002/cpa.22074}
}

@misc{robert1990,
  title={Ash. Information Theory},
  author={Robert, B},
  journal={Dover Publications},
  year={1990},
  pages = {115},
 
}






