\appendix
\onecolumn
\vspace{0.5in}
\begin{center}
	\rule{6.875in}{0.7pt}\\ % 4.0
	{\Large\bf Supplementary Material for ``Robust Generalization against Photon-Limited \\ Corruptions via Worst-Case Sharpness Minimization''}
	\rule{6.875in}{0.7pt}
\end{center}



In the supplementary material, we first provide the details in the proof for the theoretical result in the main paper in Section.~\ref{appendix_proof}. Then, we give details about our implementation details in Section~\ref{appendix_details}. Finally, we show more experimental results using different types of corruptions in Section~\ref{appendix_exp_results}.
 
\section{Convergence Analyses} \label{appendix_proof}
\newtheorem{assumption}{Assumption}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]


\subsection{Preliminaries}
We first give some notations before we start our proof for the convergence.
\begin{enumerate}
    \item We denote the expectation value for the loss function as $\mathbb{L}(\theta,\omega):=\mathbb{E}_{(x,y)\sim Q}\mathcal{L}(\theta,\omega;(x,y))$, and so as the SAM function that $\mathbb{R}(\theta,\omega)=\mathbb{E}_{(x,y)\sim Q}R(\theta,\omega;(x,y))$. So our objective can be turned into: $\min_{\theta}\{\max_{\omega}\mathbb{L}(\theta,\omega)\}+\mathbb{R}(\theta,\omega)$. And recalling our SharpDRO algorithm, we restate the meaning of the parameters: the model is parameterized by $\theta$ and $\omega$ means the weighted sampling.
    % \item We give some notations for brief: $\theta_{t+1/2}\triangleq\theta_t+\rho g_{\theta}(\theta_t,\omega_t)$, so the update for $\theta$ can be simplified as: $\theta_{t+1}=\theta_t-\eta_{\theta}g_{\theta}(\theta_{t+1/2},\omega_t)$;
    \item $\kappa$ is the condition number that $\kappa=\frac{l}{\mu}$, where $l$ is the Lipschitz-smoothness in Assumption \ref{as:smooth} and $\mu$ means the PL condition in Assumption \ref{as:pl}.
    \item We define $\mathbb{L}^*(\theta)=\max_{\omega}\mathbb{L}(\theta,\omega)$ and $\omega^*(\theta)=arg\max_{\omega}\mathbb{L}(\theta,\omega)$.
\end{enumerate}




\subsection{Update Rule}
Before our theoretical analyses, we need to make the update rule for each variable explicit. We have to pay attention to the fact that our algorithm is stochastic that we can not directly get the real value of the gradient $\nabla\mathbb{L}(\theta,\omega)$, rather we estimate it by batches of samples $g_{\theta}(\theta,\omega)=\frac{1}{M}\sum_{i=1}^M\frac{\partial \mathcal{L}}{\partial \theta}(\theta,\omega;(x_i,y_i))$ and $g_{\omega}(\theta,\omega)=\frac{1}{M}\sum_{i=1}^M\frac{\partial \mathcal{L}}{\partial \omega}(\theta,\omega;(x_i,y_i))$, who hold some properties we will introduce in Assumption \ref{as:bv}. So the optimization iteration is executed as follows in reality:
\begin{equation}
    \begin{aligned}
    &\theta_{t+1}=\theta_t-\eta_{\theta}g_{\theta}(\theta_t+\rho g_{\theta}(\theta_t,\omega_t),\omega_t);\\
    &\omega_{t+1}=\omega_t+\eta_{\omega}\nabla_{\omega}g_{\omega}(\theta_t,\omega_t).
    \end{aligned}
\end{equation}

We further give a notation for brief that $\theta_{t+1/2}\triangleq\theta_t+\rho g_{\theta}(\theta_t,\omega_t)$, so the update for $\theta$ can be simplified as: $\theta_{t+1}=\theta_t-\eta_{\theta}g_{\theta}(\theta_{t+1/2},\omega_t)$.


\subsection{Assumptions}
We also have to make some necessary assumptions on our problem setting for this convergence proof:
\begin{assumption}[Bounded variance]\label{as:bv}
The unbiased estimation about the gradient of the loss function also has bounded variance that:
\begin{equation*}
    \begin{aligned}
    \mathbb{E}_{(x,y)\sim Q}[\frac{\partial\mathcal{L}}{\partial\theta}(\theta,\omega;(x,y))]=\nabla_{\theta}\mathbb{L}(\theta,\omega),&\quad \mathbb{E}_{(x,y)\sim Q}\|\frac{\partial \mathcal{L}}{\partial \theta}(\theta,\omega;(x,y))-\nabla_{\theta}\mathbb{L}(\theta,\omega)\|^2\leq\sigma^2;\\
    \mathbb{E}_{(x,y)\sim Q}[\frac{\partial\mathcal{L}}{\partial\omega}(\theta,\omega;(x,y))]=\nabla_{\omega}\mathbb{L}(\theta,\omega),&\quad \mathbb{E}_{(x,y)\sim Q}\|\frac{\partial \mathcal{L}}{\partial \omega}(\theta,\omega;(x,y))-\nabla_{\omega}\mathbb{L}(\theta,\omega)\|^2\leq\sigma^2.
    \end{aligned}
\end{equation*}
\end{assumption}
\begin{remark}
Since $g_{\theta}$ and $g_{\omega}$ are the averaged samples that: $g_{\theta}=\frac{1}{M}\sum_{i=1}^M \frac{\partial \mathcal{L}}{\partial \theta }(\theta,\omega;(x_i,y_i))$ and $g_{\omega}=\frac{1}{M}\sum_{i=1}^M\frac{\partial\mathcal{L}}{\partial\omega}(\theta,\omega;(x_i,y_i))$ respectively, they also have the unbiased property and have bounded variance:
\begin{equation*}
    \begin{aligned}
    \mathbb{E}_{(x,y)\sim Q}[g_{\theta}(\theta,\omega;(x,y))]=\nabla_{\theta}\mathbb{L}(\theta,\omega),&\quad \mathbb{E}_{(x,y)\sim Q}\|g_{\theta}(\theta,\omega;(x,y))-\nabla_{\theta}\mathbb{L}(\theta,\omega)\|^2\leq\frac{\sigma^2}{M};\\
    \mathbb{E}_{(x,y)\sim Q}[g_{\omega}(\theta,\omega;(x,y))]=\nabla_{\omega}\mathbb{L}(\theta,\omega),&\quad \mathbb{E}_{(x,y)\sim Q}\|g_{\omega}(\theta,\omega;(x,y))-\nabla_{\omega}\mathbb{L}(\theta,\omega)\|^2\leq\frac{\sigma^2}{M}.
    \end{aligned}
\end{equation*}
\end{remark}


\begin{assumption}[Lipschitz smooth]\label{as:smooth}
$\mathcal{L}(\theta,\omega;(x,y))$ is differential and $l$-Lipschitz smooth for every given sample $(x,y)$:
\begin{equation*}
    \begin{aligned}
    \|\nabla_{\theta}\mathcal{L}(\theta_1,\omega;(x,y))-\nabla_{\theta}\mathcal{L}(\theta_2,\omega;(x,y))\|&\leq l\|\theta_1-\theta_2\|,\quad\forall \omega, (x,y);\\
    \|\nabla_{\omega}\mathcal{L}(\theta,\omega_1;(x,y))-\nabla_{\omega}\mathcal{L}(\theta,\omega_2;(x,y))\|&\leq l\|\omega_1-\omega_2\|,\quad\forall \theta, (x,y).
    \end{aligned}
\end{equation*}
\end{assumption}
\begin{remark}
So the expectation function $\mathbb{L}$ also have the Lipschitz smooth property that:
\begin{equation*}
    \begin{aligned}
    \|\nabla_{\theta}\mathbb{L}(\theta_1,\omega)-\nabla_{\theta}\mathbb{L}(\theta_2,\omega)\|&\leq\mathbb{E}\|\nabla_{\theta}\mathcal{L}(\theta_1,\omega;(x,y))-\nabla_{\theta}\mathcal{L}(\theta_2,\omega;(x,y))\|\leq l\|\theta_1-\theta_2\|,\quad\forall \omega;\\
    \|\nabla_{\omega}\mathbb{L}(\theta,\omega_1)-\nabla_{\omega}\mathbb{L}(\theta,\omega_2)\|&\leq \mathbb{E}\|\nabla_{\omega}\mathcal{L}(\theta,\omega_1;(x,y))-\nabla_{\omega}\mathcal{L}(\theta,\omega_2;(x,y))\|\leq l\|\omega_1-\omega_2\|,\quad\forall \theta.
    \end{aligned}
\end{equation*}
\end{remark}


\begin{assumption}[PL condition]\label{as:pl}
The loss function $\mathbb{L}(\theta,\cdot)$ satisfies PL condition on every given $\theta$, i.e., there exists $\mu>0$ such that $\|\nabla_{\omega}\mathbb{L}(\theta,\omega)\|^2\geq2\mu[\max_{\omega}\mathbb{L}(\theta,\omega)-\mathbb{L}(\theta,\omega)], \forall \theta,\omega$.
\end{assumption}





\subsection{Useful Lemmas}
In this part, we will prove some necessary lemmas for us to prove the convergence bound. And we will give the definition of the stationary point of our problem at the beginning.

\begin{definition}[Stationary measure]
$\theta$ is defined as the $\epsilon$-stationary point of our problem if $\mathbb{E}\|\nabla\mathbb{L}^*(\theta)\|\leq\epsilon$ for any $\epsilon\geq0$.
\end{definition}
\begin{remark}
For minmax problem, there are usually two ways to measure the stationary point. The other one is measured two-side that: when $\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta,\omega)\|\leq\epsilon$ and $\mathbb{E}\|\nabla_{\omega}\mathbb{L}(\theta,\omega)\|\leq\epsilon$, we claim $(\theta,\omega)$ is the $(\epsilon,\epsilon)$-stationary point. It has been proved in \cite{yang2022faster} that these two measures can be translated into each other when $\mathbb{L}^*$ is smooth which will be shown in Lemma \ref{le:L*smooth}. But what we compute is the model parameter $\theta$ using the algorithm SharpDRO. So we choose the measure by $\mathbb{E}\|\mathbb{L}^*(\theta)\|$ here.
\end{remark}


\begin{lemma}~\cite{nouiehed2019solving}\label{le:L*smooth}
Under Assumption \ref{as:smooth} and \ref{as:pl}, $\mathbb{L}^*(\theta)$ is $(l+\frac{l^2}{2\mu})$-Lipschitz smooth with the gradient:
$$\nabla_{\theta}\mathbb{L}^*(\theta,\omega)=\nabla_{\theta}\mathbb{L}(\theta,\omega^*(\theta)).$$
\end{lemma}

\begin{lemma}~\cite{nouiehed2019solving}\label{le:w^*smooth}
Under Assumption \ref{as:smooth} and \ref{as:pl}, $\omega^*(\cdot)$ is smooth about its variable:
$$\|\omega^*(\theta_1)-\omega^*(\theta_2)\|\leq\frac{l}{2\mu}\|\theta_1-\theta_2\|, \quad\forall \theta_1,\theta_2.$$
\end{lemma}

\begin{lemma}\label{eq:gradientbound}
We give an estimation that $\mathbb{E}\|g_{\theta}(\theta_{t+1/2},\omega_t)\|^2\leq(4\rho^2l^2+2\rho l+2)\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2+(5\rho^2l^2+2)\frac{\sigma^2}{M}$.
\end{lemma}
\begin{proof}
\begin{equation}
    \begin{aligned}
    \mathbb{E}\|g_{\theta}(\theta_{t+1/2},\omega_t)\|^2=-\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2+\mathbb{E}\|g_{\theta}(\theta_{t+1/2},\omega_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2+2\mathbb{E}\langle g_{\theta}(\theta_{t+1/2},\omega_t),\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\rangle.
    \end{aligned}
\end{equation}

For the cross-product term, we divide it as follows:
\begin{equation}
    \begin{aligned}
    &\quad\mathbb{E}\langle g_{\theta}(\theta_{t+1/2},\omega_t),\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\rangle\\
    &=\mathbb{E}\langle g_{\theta}(\theta_{t+1/2},\omega_t)-g_{\theta}(\theta_t+\rho\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t),\omega_t),\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\rangle+\mathbb{E}\langle g_{\theta}(\theta_t+\rho\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t),\omega_t),\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\rangle\\
    &=\mathbb{E}\langle \nabla_{\theta}\mathbb{L}(\theta_{t+1/2},\omega_t)-\nabla_{\theta}\mathbb{L}(\theta_t+\rho\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t),\omega_t),\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\rangle+\mathbb{E}\langle\nabla_{\theta}\mathbb{L}(\theta_t+\rho\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t),\omega_t),\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\rangle\\
    &\overset{(i)}{\leq}\frac{1}{2}\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_{t+1/2},\omega_t)-\nabla_{\theta}\mathbb{L}(\theta_t+\rho\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t),\omega_t)\|^2+\frac{1}{2}\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2+\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2\\
    &\quad+\mathbb{E}\langle \nabla_{\theta}\mathbb{L}(\theta_t+\rho\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t),\omega_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t),\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\rangle\\
    &\overset{(ii)}{\leq}\frac{\rho^2l^2}{2}\mathbb{E}\|g_{\theta}(\theta_t,\omega_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2+\frac{3}{2}\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2 +\rho l\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2\\
    &\overset{(iii)}{\leq}(\rho l+\frac{3}{2})\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2+\frac{\rho^2l^2\sigma^2}{2M},
    \end{aligned}
\end{equation}
where the inequality $(i)$ is due to the Cauchy-Schwarz inequality; the inequality $(ii)$ is because of the Lipschitz-smoothness of $\mathbb{L}$ that $\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_{t+1/2},\omega_t)-\nabla_{\theta}\mathbb{L}(\theta_t+\rho\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t),\omega_t)\|^2\leq l^2\mathbb{E}\|\theta_{t+1/2}-\theta_t-\rho\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2$ and the property of Lipschitz-smoothness that $\langle \nabla_{\theta}\mathbb{L}(\theta_t+\rho\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t),\omega_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t),\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\rangle=\frac{1}{\rho}\langle \nabla_{\theta}\mathbb{L}(\theta_t+\rho\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t),\omega_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t),\rho\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\rangle\leq\frac{l}{\rho}\|\rho\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2$; and the inequality $(iii)$ makes use of the Assumption \ref{as:bv}.

As for the second term, we have:
\begin{equation}
    \begin{aligned}
    &\quad\mathbb{E}\|g_{\theta}(\theta_{t+1/2},\omega_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2\\
    &\leq2\mathbb{E}\|g_{\theta}(\theta_{t+1/2},\omega_t)-\nabla_{\theta}\mathbb{L}(\theta_{t+1/2},\omega_t)\|^2+2\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_{t+1/2},\omega_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2\\
    &\leq\frac{2\sigma^2}{M}+2l^2\mathbb{E}\|\theta_{t+1/2}-\theta_t\|^2\\
    &=\frac{2\sigma^2}{M}+2\rho^2l^2\mathbb{E}\|g_{\theta}(\theta_t,\omega_t)\|^2\\
    &\leq2\frac{\sigma^2}{M}(2\rho^2l^2+1)+4\rho^2l^2\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2,
    \end{aligned}
\end{equation}
where the last inequality comes from the fact that: $\mathbb{E}\|g_{\theta}(\theta_t,\omega_t)\|^2\leq2\mathbb{E}\|g_{\theta}(\theta_t,\omega_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2+2\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2$

By combining the above inequalities, we can get:
\begin{equation}
    \mathbb{E}\|g_{\theta}(\theta_{t+1/2},\omega_t)\|^2\leq(4\rho^2l^2+2\rho l+2)\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2+(5\rho^2l^2+2)\frac{\sigma^2}{M}.
\end{equation}
\end{proof}


\begin{lemma}\label{le:L^*gradient}
For the descending relationship of the function $\mathbb{L}^*$, we have:
\begin{equation*}
    \begin{aligned}
    \mathbb{E}[\mathbb{L}^*(\theta_{t+1})]
    &\leq\mathbb{E}[\mathbb{L}^*(\theta_t)]-\frac{\eta_{\theta}}{2}(1-5\rho l-2L\eta_{\theta}(4\rho^2l^2+2\rho l+2))\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2\\
    &\quad+[\frac{\eta_{\theta}}{2}(1+\frac{1}{2}\rho l)+L\eta_{\theta}^2(4\rho^2 l^2+2\rho l+2)]\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2+(5\rho^2l^2+2)\frac{L\eta_{\theta}^2\sigma^2}{2M},
    \end{aligned}
\end{equation*}
where we use the brief notation that $L=l+\frac{l\kappa}{2}$.
\end{lemma}
\begin{proof}
Since $\mathbb{L}^*(\theta)$ is $(l+\frac{l\kappa}{2})$-Lipschitz smooth according to Lemma \ref{le:L*smooth}, we have:
\begin{equation}\label{eq:phi_smooth}
    \begin{aligned}
    \mathbb{L}^*(\theta_{t+1})&\leq \mathbb{L}^*(\theta_t)+\langle\nabla\mathbb{L}^*(\theta_t),\theta_{t+1}-\theta_t\rangle+\frac{1}{2}(l+\frac{l\kappa}{2})\|\theta_{t+1}-\theta_t\|^2\\
    &=\mathbb{L}^*(\theta_t)-\eta_{\theta}\langle\nabla\mathbb{L}^*(\theta_t),g_{\theta}(\theta_{t+1/2},\omega_t)\rangle+\frac{1}{2}(l+\frac{l\kappa}{2})\eta_{\theta}^2\|g_{\theta}(\theta_{t+1/2},\omega_t)\|^2.
    \end{aligned}
\end{equation}

Taking expectation conditioned on $(\theta_t,\omega_t)$ and we get:
\begin{equation}
    \mathbb{E}[\mathbb{L}^*(\theta_{t+1})|\theta_t,\omega_t]\leq\mathbb{L}^*(\theta_t)-\eta_{\theta}\langle\nabla\mathbb{L}^*(\theta_t),\nabla_{\theta}\mathbb{L}(\theta_{t+1/2},\omega_t)\rangle+\frac{1}{2}(l+\frac{l\kappa}{2})\eta_{\theta}^2\mathbb{E}[\|g_{\theta}(\theta_{t+1/2},\omega_t)\|^2|\theta_t,\omega_t].
\end{equation} 

We again take expectation on both side on above inequality so we have:
\begin{equation}\label{eq:stophi_smooth}
    \mathbb{E}[\mathbb{L}^*(\theta_{t+1})]\leq\mathbb{E}[\mathbb{L}^*(\theta_t)]-\eta_{\theta}\mathbb{E}\langle\nabla\mathbb{L}^*(\theta_t),\nabla_{\theta}\mathbb{L}(\theta_{t+1/2},\omega_t)\rangle+\frac{1}{2}(l+\frac{l\kappa}{2})\eta_{\theta}^2\mathbb{E}\|g_{\theta}(\theta_{t+1/2},\omega_t)\|^2.
\end{equation}

For the second term, we decompose it as follows:
\begin{equation}\label{eq:phi_second}
    \begin{aligned}
    &\quad\; \, \mathbb{E}\langle\nabla\mathbb{L}^*(\theta_t),\nabla_{\theta}\mathbb{L}(\theta_{t+1/2},\omega_t)\rangle\\
    &=\mathbb{E}\langle\nabla\mathbb{L}^*(\theta_t),\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)+\nabla_{\theta}\mathbb{L}(\theta_{t+1/2},\omega_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\rangle\\
    &\geq \mathbb{E}\langle\nabla\mathbb{L}^*(\theta_t),\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\rangle-\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|\|\nabla_{\theta}\mathbb{L}(\theta_{t+1/2},\omega_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|\\
    &\geq \mathbb{E}\langle\nabla\mathbb{L}^*(\theta_t),\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\rangle-\rho l\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|\|g_{\theta}(\theta_t,\omega_y)\|\\
    &\geq\mathbb{E}\langle\nabla\mathbb{L}^*(\theta_t),\nabla\mathbb{L}^*(\theta_t)+\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)-\nabla\mathbb{L}^*(\theta_t)\rangle-\rho l\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|(\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|+\|g_{\theta}(\theta_t,\omega_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|)\\
    &\geq\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2-\frac{1}{2}\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2-\frac{1}{2}\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)-\nabla\mathbb{L}^*(\theta_t)\|^2-\rho l\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|\\
    &\quad -\frac{1}{2}\rho l\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2-\frac{1}{2}\rho l\mathbb{E}\|g_{\theta}(\theta_t,\omega_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2\\
    &\geq\frac{1-\rho l}{2}\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2-\frac{1}{2}\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)-\nabla\mathbb{L}^*(\theta_t)\|^2-\rho l\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|-\frac{\rho l\sigma^2}{2M}.
    \end{aligned}
\end{equation}

We continue estimating the last term in above inequality \ref{eq:phi_second}
\begin{equation}\label{eq:phi_second_last}
    \begin{aligned}
    &\quad\;\,\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|\\
    &=\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)-\nabla\mathbb{L}^*(\theta_t)+\nabla\mathbb{L}^*(\theta_t)\|\\
    &\leq\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2+\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)-\nabla\mathbb{L}^*(\theta_t)\|\\
    &\overset{(i)}{\leq}\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2+\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2+\frac{1}{4}\mathbb{E}\|\nabla_\theta\mathbb{L}(\theta_t,\omega_t)-\nabla\mathbb{L}^*(\theta_t)\|^2,
    \end{aligned}
\end{equation}
where the last inequality $(i)$ is due to Young's inequality.

By combining inequality \ref{eq:stophi_smooth} with \ref{eq:phi_second_last}, we can get:
\begin{equation}\label{eq:phi_second_final}
    \begin{aligned}
    &\quad\; \, \mathbb{E}\langle\nabla\mathbb{L}^*(\theta_t),\nabla_{\theta}\mathbb{L}(\theta_{t+1/2},\omega_t)\rangle\\
    &\geq \frac{1-\rho l}{2}\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2-\frac{1}{2}\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)-\nabla\mathbb{L}^*(\theta_t)\|^2-2\rho l\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2-\frac{\rho l}{4}\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)-\nabla\mathbb{L}^*(\theta_t)\|^2-\frac{\rho l\sigma^2}{2M}\\
    &=\frac{1}{2}(1-5\rho l)\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2-\frac{1}{2}(1+\frac{1}{2}\rho l)\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2-\frac{\rho l\sigma^2}{2M}.
    \end{aligned}
\end{equation}

Finally, we combine inequality \ref{eq:stophi_smooth} with Lemma \ref{eq:gradientbound} and inequality \ref{eq:phi_second_final}:
\begin{equation}
    \begin{aligned}
    &\quad\;\,\mathbb{E}[\mathbb{L}^*(\theta_{t+1})]\\
    &\leq\mathbb{E}[\mathbb{L}^*(\theta_t)]-\frac{\eta_{\theta}}{2}(1-5\rho l)\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2+\frac{\eta_{\theta}}{2}(1+\frac{1}{2}\rho l)\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2\\
    &\quad+\frac{1}{2}(l+\frac{l\kappa}{2})\eta_{\theta}^2((4\rho^2l^2+2\rho l+2)\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2+(5\rho^2l^2+2)\frac{\sigma^2}{M})\\
    &\overset{(i)}{\leq}\mathbb{E}[\mathbb{L}^*(\theta_t)]-\frac{\eta_{\theta}}{2}(1-5\rho l-\eta_{\theta}(2l+l\kappa)(4\rho^2l^2+2\rho l+2))\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2\\
    &\quad+[\frac{\eta_{\theta}}{2}(1+\frac{1}{2}\rho l)+\eta_{\theta}^2(l+\frac{l\kappa}{2})(4\rho^2 l^2+2\rho l+2)]\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2+\frac{1}{2}(l+\frac{l\kappa}{2})(5\rho^2l^2+2)\frac{\eta_{\theta}^2\sigma^2}{M},
    \end{aligned}
\end{equation}
where the last inequality $(i)$ uses the Cauchy-Schwarz inequality that $\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2\leq2\|\nabla\mathbb{L}^*(\theta_t)\|^2+2\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)-\nabla\mathbb{L}^*(\theta_t)\|^2$.
\end{proof}


\subsection{Theorem}
\begin{theorem}\label{le:pl:phi}
Under Assumption \ref{as:bv},\ref{as:smooth},\ref{as:pl}, and the learning rate satisfy that $\eta_{\theta}\leq\min\{\frac{1}{128\kappa^2l},\sqrt{\frac{M(\mathbb{E}[\mathbb{L}^*(\theta_0)]-\min_{\theta}\mathbb{E}[\mathbb{L}^*(\theta)])}{132T\kappa^4l\sigma^2}}\}$, $\eta_{\omega}\leq64\kappa^2\eta_{\theta}$ and $\rho\leq\frac{\eta_{\theta}}{2l}$, we have the convergence bound for our problem:
\begin{equation}
    \frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2\leq320\sqrt{\frac{3\kappa^4l(\mathbb{E}[\mathbb{L}^*(\theta_0)]-\min_{\theta}\mathbb{E}[\mathbb{L}^*(\theta)])\sigma^2}{11MT}}=\mathcal{O}(\frac{\kappa^2}{\sqrt{MT}}).
\end{equation}

\end{theorem}


\begin{proof}
First recall the descending relationship of the function $\mathbb{L}^*$ in Lemma \ref{le:L^*gradient}:
\begin{equation}
    \begin{aligned}
    &\quad\;\,\mathbb{E}[\mathbb{L}^*(\theta_{t+1})]\\
    &\leq\mathbb{E}[\mathbb{L}^*(\theta_t)]-\frac{\eta_{\theta}}{2}(1-5\rho l-2L\eta_{\theta}(4\rho^2l^2+2\rho l+2))\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2\\
    &\quad+[\frac{\eta_{\theta}}{2}(1+\frac{1}{2}\rho l)+L\eta_{\theta}^2(4\rho^2 l^2+2\rho l+2)]\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2+(5\rho^2l^2+2)\frac{L\eta_{\theta}^2\sigma^2}{2M}.
    \end{aligned}
\end{equation}

Then, using the smoothness of the variables $\theta$ and $\omega$ respectively, we can get:
\begin{equation*}
    \begin{aligned}
    \mathbb{L}(\theta_{t+1},\omega_t)&\geq \mathbb{L}(\theta_t,\omega_t)+\langle\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t),\theta_{t+1}-\theta_t\rangle-\frac{l}{2}\|\theta_{t+1}-\theta_t\|^2;    \\
    \mathbb{L}(\theta_{t+1},\omega_{t+1})&\geq \mathbb{L}(\theta_{t+1},\omega_t)+\langle\nabla_{\omega}\mathbb{L}(\theta_{t+1},\omega_t),\omega_{t+1}-\omega_t\rangle-\frac{l}{2}\|\omega_{t+1}-\omega_t\|^2.
    \end{aligned}
\end{equation*}

Taking expectation we can get:
\begin{equation}
    \begin{aligned}
    \mathbb{E}[\mathbb{L}(\theta_{t+1},\omega_t)]&\geq\mathbb{E}[\mathbb{L}(\theta_t,\omega_t)]-\eta_{\theta}\mathbb{E}\langle\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t),\nabla_{\theta}\mathbb{L}(\theta_{t+1/2},\omega_t)\rangle-\frac{l\eta_{\theta}^2}{2}\mathbb{E}\|g_{\theta}(\theta_{t+1/2},\omega_t)\|^2\\
    &\geq\mathbb{E}[\mathbb{L}(\theta_t,\omega_t)]-\eta_{\theta}\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2-\frac{\eta_{\theta}}{2}\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2\\&\quad-\frac{\eta_{\theta}}{2}\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_{t+1/2},\omega_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2-\frac{l\eta_{\theta}^2}{2}\mathbb{E}\|g_{\theta}(\theta_{t+1/2},\omega_t)\|^2\\
    &\geq \mathbb{E}[\mathbb{L}(\theta_t,\omega_t)]-\frac{3\eta_{\theta}}{2}\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2-\frac{l^2\rho^2\eta_{\theta}}{2}\mathbb{E}\|g_{\theta}(\theta_t,\omega_t)\|^2-\frac{l\eta_{\theta}^2}{2}\mathbb{E}\|g_{\theta}(\theta_{t+1/2},\omega_t)\|^2\\
    &\geq\mathbb{E}[\mathbb{L}(\theta_t,\omega_t)]-(\frac{3\eta_{\theta}}{2}+\frac{l^2\rho^2\eta_{\theta}}{2}+l\eta_{\theta}^2(2\rho^2l^2+\rho l+1))\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2\\&\quad-(\frac{l^2\rho^2\eta_{\theta}}{2}+\frac{l\eta_{\theta}^2}{2}(5\rho^2l^2+2))\frac{\sigma^2}{M};\\
    \mathbb{E}[\mathbb{L}(\theta_{t+1},\omega_{t+1})]&\geq\mathbb{E}[\mathbb{L}(\theta_{t+1},\omega_t)]+\eta_{\omega}\mathbb{E}\langle\nabla_{\omega}\mathbb{L}(\theta_{t+1},\omega_t),\nabla_{\omega}\mathbb{L}(\theta_t,\omega_t)\rangle-\frac{l\eta_{\omega}^2}{2}\mathbb{E}\|g_{\omega}(\theta_t,\omega_t)\|^2\\
    &\geq\mathbb{E}[\mathbb{L}(\theta_{t+1},\omega_t)]+\frac{\eta_{\omega}}{2}\mathbb{E}\|\nabla_{\omega}\mathbb{L}(\theta_t,\omega_t)\|^2-\frac{\eta_{\omega}}{2}\mathbb{E}\|\nabla_{\omega}\mathbb{L}(\theta_{t+1},\omega_t)-\nabla_{\omega}\mathbb{L}(\theta_t,\omega_t)\|^2-\frac{l\eta_{\omega}^2}{2}\mathbb{E}\|g_{\omega}(\theta_t,\omega_t)\|^2\\
    &\geq\mathbb{E}[\mathbb{L}(\theta_{t+1},\omega_t)]+\frac{\eta_{\omega}}{2}\mathbb{E}\|\nabla_{\omega}\mathbb{L}(\theta_t,\omega_t)\|^2-\frac{l\eta_{\theta}^2\eta_{\omega}}{2}\mathbb{E}\|g_{\theta}(\theta_{t+1/2},\omega_t)\|^2-\frac{l\eta_{\omega}^2}{2}\mathbb{E}\|g_{\omega}(\theta_t,\omega_t)\|^2\\
    &\geq\mathbb{E}[\mathbb{L}(\theta_{t+1},\omega_t)]+(\frac{\eta_{\omega}}{2}-\frac{l\eta_{\omega}^2}{2})\mathbb{E}\|\nabla_{\omega}\mathbb{L}(\theta_t,\omega_t)\|^2-(l\eta_{\theta}^2\eta_{\omega}(2\rho^2l^2+\rho l+1))\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2\\
    &\quad-(\frac{l\eta_{\omega}^2}{2}+\frac{l\eta_{\theta}^2\eta_{\omega}}{2}(5\rho^2l^2+2))\frac{\sigma^2}{M}.
    \end{aligned}
\end{equation}

Then we construct a potential function in the same way as~\cite{yang2022faster}:
$$V_t=V(\theta_t,\omega_t)=\mathbb{L}^*(\theta_t)+\alpha[\mathbb{L}^*(\theta_t)-\mathbb{L}(\theta_t,\omega_t)],$$
where $\alpha>0$ is a preset parameter. Then we come to evaluate the descending relationship of the potential function $V_t$.

Combining the above inequalities we can get the descending relationship of the potential function:
\begin{align}
    &\quad \mathbb{E}[V_{t+1}]-\mathbb{E}[V_t]  \notag\\
    &=(1+\alpha)(\mathbb{E}[\mathbb{L}^*(\theta_{t+1})]-\mathbb{E}[\mathbb{L}^*(\theta_t)])-\alpha(\mathbb{E}[\mathbb{L}(\theta_{t+1},\omega_{t+1})]-\mathbb{E}[\mathbb{L}(\theta_t,\omega_t)])  \notag\\
    &\leq(1+\alpha)\{-\frac{\eta_{\theta}}{2}(1-5\rho l-2L\eta_{\theta}(4\rho^2l^2+2\rho l+2))\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2  \notag\\
    &\quad+[\frac{\eta_{\theta}}{2}(1+\frac{1}{2}\rho l)+L\eta_{\theta}^2(4\rho^2 l^2+2\rho l+2)]\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2+(5\rho^2l^2+2)\frac{L\eta_{\theta}^2\sigma^2}{2M}\}  \notag\\
    &\quad-\alpha\{-(\frac{3\eta_{\theta}}{2}+\frac{l^2\rho^2\eta_{\theta}}{2}+l\eta_{\theta}^2(2\rho^2l^2+\rho l+1))\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2-(\frac{l^2\rho^2\eta_{\theta}}{2}+\frac{l\eta_{\theta}^2}{2}(5\rho^2l^2+2))\frac{\sigma^2}{M}  \notag\\
    &\quad+(\frac{\eta_{\omega}}{2}-\frac{l\eta_{\omega}^2}{2})\mathbb{E}\|\nabla_{\omega}\mathbb{L}(\theta_t,\omega_t)\|^2-(l\eta_{\theta}^2\eta_{\omega}(2\rho^2l^2+\rho l+1))\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2-(\frac{l\eta_{\omega}^2}{2}+\frac{l\eta_{\theta}^2\eta_{\omega}}{2}(5\rho^2l^2+2))\frac{\sigma^2}{M}\}  \tag{\stepcounter{equation}\theequation}\\
    &=-\frac{\eta_{\theta}}{2}(1+\alpha)(1-5\rho l-2L\eta_{\theta}(4\rho^2l^2+2\rho l+2))\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2  \notag\\
    &\quad+(1+\alpha)(\frac{\eta_{\theta}}{2}(1+\frac{1}{2}\rho l)+L\eta_{\theta}^2(4\rho^2 l^2+2\rho l+2))\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2  \notag\\
    &\quad+\alpha[(\frac{3\eta_{\theta}}{2}+\frac{l^2\rho^2\eta_{\theta}}{2}+l\eta_{\theta}^2(2\rho^2l^2+\rho l+1))+l\eta_{\theta}^2\eta_{\omega}(2\rho^2l^2+\rho l+1)]\mathbb{E}\|\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2  \notag\\
    &\quad-\alpha(\frac{\eta_{\omega}}{2}-\frac{l\eta_{\omega}^2}{2})\mathbb{E}\|\nabla_{\omega}\mathbb{L}(\theta_t,\omega_t)\|^2  \notag\\
    &\quad+[(1+\alpha)(5\rho^2l^2+2)\frac{L\eta_{\theta}^2}{2}+\alpha(\frac{l^2\rho^2\eta_{\theta}}{2}+\frac{l\eta_{\theta}^2}{2}(5\rho^2l^2+2))+\alpha(\frac{l\eta_{\omega}^2}{2}+\frac{l\eta_{\theta}^2\eta_{\omega}}{2}(5\rho^2l^2+2))]\frac{\sigma^2}{M}  \notag\\
    &\leq-\{\frac{\eta_{\theta}}{2}(1+\alpha)(1-5\rho l-2L\eta_{\theta}(4\rho^2l^2+2\rho l+2))-2\alpha[(\frac{3\eta_{\theta}}{2}+\frac{l^2\rho^2\eta_{\theta}}{2}+l\eta_{\theta}^2(2\rho^2l^2+\rho l+1))+l\eta_{\theta}^2\eta_{\omega}(2\rho^2l^2+\rho l+1)]\}\notag\\&\quad\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2  \notag\\
    &\quad+\{(1+\alpha)(\frac{\eta_{\theta}}{2}(1+\frac{1}{2}\rho l)+L\eta_{\theta}^2(4\rho^2 l^2+2\rho l+2))+2\alpha[(\frac{3\eta_{\theta}}{2}+\frac{l^2\rho^2\eta_{\theta}}{2}+l\eta_{\theta}^2(2\rho^2l^2+\rho l+1))+l\eta_{\theta}^2\eta_{\omega}(2\rho^2l^2+\rho l+1)]\}  \notag\\
    &\quad\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)-\nabla_{\theta}\mathbb{L}(\theta_t,\omega_t)\|^2  \notag\\
    &\quad-\alpha(\frac{\eta_{\omega}}{2}-\frac{l\eta_{\omega}^2}{2})\mathbb{E}\|\nabla_{\omega}\mathbb{L}(\theta_t,\omega_t)\|^2  \notag\\
    &\quad+[(1+\alpha)(5\rho^2l^2+2)\frac{L\eta_{\theta}^2}{2}+\alpha(\frac{l^2\rho^2\eta_{\theta}}{2}+\frac{l\eta_{\theta}^2}{2}(5\rho^2l^2+2))+\alpha(\frac{l\eta_{\omega}^2}{2}+\frac{l\eta_{\theta}^2\eta_{\omega}}{2}(5\rho^2l^2+2))]\frac{\sigma^2}{M}.
\end{align}


Since we have the following property according to Lemma \ref{le:L*smooth} and the PL condition \ref{as:pl}:
$$\|\nabla\mathbb{L}^*(\theta_t)-\nabla_{\theta}f(\theta_t,\omega_t)\|\leq l\|\omega^*(\theta_t)-\omega_t\|\leq\kappa\|\nabla_{\omega}f(\theta_t,\omega_t)\|.$$

So we can further the above inequality as follows:
\begin{equation}
    \begin{aligned}
    &\quad\mathbb{E}[V_{t+1}]-\mathbb{E}[V_t]\\
    &\leq-\{\frac{\eta_{\theta}}{2}(1+\alpha)(1-5\rho l-2L\eta_{\theta}(4\rho^2l^2+2\rho l+2))-2\alpha[(\frac{3\eta_{\theta}}{2}+\frac{l^2\rho^2\eta_{\theta}}{2}+l\eta_{\theta}^2(2\rho^2l^2+\rho l+1))+l\eta_{\theta}^2\eta_{\omega}(2\rho^2l^2+\rho l+1)]\}\\&\quad\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2\\
    &\quad-\{\alpha(\frac{\eta_{\omega}}{2}-\frac{l\eta_{\omega}^2}{2})-\kappa^2[(1+\alpha)(\frac{\eta_{\theta}}{2}(1+\frac{1}{2}\rho l)+L\eta_{\theta}^2(4\rho^2 l^2+2\rho l+2))\\&\quad+2\alpha[(\frac{3\eta_{\theta}}{2}+\frac{l^2\rho^2\eta_{\theta}}{2}+l\eta_{\theta}^2(2\rho^2l^2+\rho l+1))+l\eta_{\theta}^2\eta_{\omega}(2\rho^2l^2+\rho l+1)]]\}\mathbb{E}\|\nabla_{\omega}\mathbb{L}(\theta_t,\omega_t)\|^2\\
    &\quad+[(1+\alpha)(5\rho^2l^2+2)\frac{L\eta_{\theta}^2}{2}+\alpha(\frac{l^2\rho^2\eta_{\theta}}{2}+\frac{l\eta_{\theta}^2}{2}(5\rho^2l^2+2))+\alpha(\frac{l\eta_{\omega}^2}{2}+\frac{l\eta_{\theta}^2\eta_{\omega}}{2}(5\rho^2l^2+2))]\frac{\sigma^2}{M}.
    \end{aligned}
\end{equation}

Then we require the parameters satisfy: $\alpha=\frac{1}{16}$, $\rho l \leq\frac{1}{16}$, $\eta_{\theta}(2\rho l+1)^2\kappa l\leq\frac{1}{64}$, $\kappa^2\eta_{\theta}l\leq\frac{1}{128}$, $\rho\leq\frac{\eta_{\theta}}{2l}$ and $\eta_{\omega}\leq64\kappa^2\eta_{\theta}$.

So the inequality can be further simplified as:
\begin{equation}
    \begin{aligned}
    &\quad\mathbb{E}[V_{t+1}]-\mathbb{E}[V_t]\\
    &\leq-\frac{11}{80}\eta_{\theta}\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2-\frac{41}{32}\eta_{\theta}\kappa^2\mathbb{E}\|\nabla_{\omega}f(\theta_t,\omega_t)\|^2+129\kappa^4l\eta_{\theta}^2\frac{\sigma^2}{M}.
    \end{aligned}
\end{equation}

Telescoping the above inequality we can get:
\begin{equation}\label{eq:bound_1}
    \begin{aligned}
    \frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2\leq\frac{80}{11\eta_{\theta}T}(\mathbb{E}[V_0]-\mathbb{E}[V_T])+960\kappa^4l\eta_{\theta}\frac{\sigma^2}{M}.
    \end{aligned}
\end{equation}

Further, we can evaluate the first term that:
\begin{equation*}
    \begin{aligned}
    \mathbb{E}[V_0]-\mathbb{E}[V_T]&\leq\mathbb{E}[V_0]-\min_{\theta,\omega}\mathbb{E}[V(\theta,\omega)]\\
    &\leq \mathbb{E}[\mathbb{L}^*(\theta_0)]-\min_{\theta}\mathbb{E}[\mathbb{L}^*(\theta)]+\frac{1}{16}(\mathbb{E}[\mathbb{L}^*(\theta_0)]-\mathbb{E}[\mathbb{L}(\theta_0,\omega_0)])\\
    &=\mathbb{E}[\mathbb{L}^*(\theta_0)]-\min_{\theta}\mathbb{E}[\mathbb{L}^*(\theta)]+\frac{1}{16}\Delta_0,
    \end{aligned}
\end{equation*}
where we denote the initial error as: $\Delta_0=\mathbb{E}[\mathbb{L}^*(\theta_0)]-\mathbb{E}[\mathbb{L}(\theta_0,\omega_0)].$

Therefore, the inequality \ref{eq:bound_1} can be further evaluated as:
\begin{equation}\label{eq:bound_2}
    \begin{aligned}
    \frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2\leq\frac{80}{11\eta_{\theta}T}(\mathbb{E}[\mathbb{L}^*(\theta_0)]-\min_{\theta}\mathbb{E}[\mathbb{L}^*(\theta)])+\frac{5}{11\eta_{\theta}T}\Delta_0+960\kappa^4l\eta_{\theta}\frac{\sigma^2}{M},
    \end{aligned}
\end{equation}
when we select $\eta_{\theta}=\sqrt{\frac{M(\mathbb{E}[\mathbb{L}^*(\theta_0)]-\min_{\theta}\mathbb{E}[\mathbb{L}^*(\theta)])}{132T\kappa^4l\sigma^2}}$, and samples can be minibatch, the convergence can be bounded by:
\begin{equation}
    \frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\|\nabla\mathbb{L}^*(\theta_t)\|^2\leq320\sqrt{\frac{3\kappa^4l(\mathbb{E}[\mathbb{L}^*(\theta_0)]-\min_{\theta}\mathbb{E}[\mathbb{L}^*(\theta)])\sigma^2}{11MT}}=\mathcal{O}(\frac{\kappa^2}{\sqrt{MT}}).
\end{equation}

\end{proof}



\begin{table*}
	\small
	\centering
	\caption{\small Quantitative comparisons on distribution-aware robust generalization setting. Averaged accuracy ($\%$) with standard deviations are computed over three independent trails.}
	\vspace{-3mm}
	\setlength{\tabcolsep}{1.8mm}
	\label{tab:appendix_distribution_aware}
	\begin{tabular}{lllcccccc}
		\toprule[1pt]
		\multirow{2}{*}{Dataset} & \multirow{2}{*}{Type} & \multirow{2}{*}{Method} & \multicolumn{6}{c}{Corruption Severity} \\
		&  &  & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{5} \\ \midrule[0.6pt]
		\multirow{10}{*}{CIFAR10} & \multirow{5}{*}{Snow} & ERM & \multicolumn{1}{c}{$90.8\pm0.01$} & \multicolumn{1}{c}{$90.1\pm0.02$} & \multicolumn{1}{c}{$88.1\pm0.02$} & \multicolumn{1}{c}{$88.1\pm0.02$} & \multicolumn{1}{c}{$85.7\pm0.02$} & \multicolumn{1}{c}{$82.6\pm0.01$} \\
		&  & IRM & \multicolumn{1}{c}{$91.1\pm0.02$} & \multicolumn{1}{c}{$90.7\pm0.01$} & \multicolumn{1}{c}{$89.7\pm0.02$} & \multicolumn{1}{c}{$88.0\pm0.03$} & \multicolumn{1}{c}{$84.6\pm0.02$} & \multicolumn{1}{c}{$83.2\pm0.03$} \\
		&  & REx & \multicolumn{1}{c}{$91.8\pm0.02$} & \multicolumn{1}{c}{$\bm{91.9}\pm\bm{0.01}$} & \multicolumn{1}{c}{$88.4\pm0.01$} & \multicolumn{1}{c}{$88.3\pm0.01$} & \multicolumn{1}{c}{$88.6\pm0.01$} & \multicolumn{1}{c}{$83.0\pm0.02$} \\
		&  & GroupDRO & \multicolumn{1}{c}{$91.5\pm0.02$} & \multicolumn{1}{c}{$91.0\pm0.01$} & \multicolumn{1}{c}{$88.7\pm0.02$} & \multicolumn{1}{c}{$88.6\pm0.02$} & \multicolumn{1}{c}{$85.2\pm0.03$} & \multicolumn{1}{c}{$83.5\pm0.02$} \\
		&  & SharpDRO & \multicolumn{1}{c}{$\bm{93.1}\pm\bm{0.01}$} & \multicolumn{1}{c}{$91.8\pm0.01$} & \multicolumn{1}{c}{$\bm{90.5}\pm\bm{0.02}$} & \multicolumn{1}{c}{$\bm{90.8}\pm\bm{0.02}$} & \multicolumn{1}{c}{$\bm{87.9}\pm\bm{0.01}$} & \multicolumn{1}{c}{$\bm{84.3}\pm\bm{0.02}$} \\ \cline{2-9} 
		
		& \multirow{5}{*}{Shot} & ERM & \multicolumn{1}{c}{$\bm{92.5}\pm\bm{0.02}$} & \multicolumn{1}{c}{$91.1\pm0.02$} & \multicolumn{1}{c}{$89.9\pm0.01$} & \multicolumn{1}{c}{$85.6\pm0.03$} & \multicolumn{1}{c}{$85.7\pm0.01$} & \multicolumn{1}{c}{$78.8\pm0.01$} \\
		&  & IRM & \multicolumn{1}{c}{$90.4\pm0.01$} & \multicolumn{1}{c}{$90.3\pm0.02$} & \multicolumn{1}{c}{$89.4\pm0.02$} & \multicolumn{1}{c}{$86.3\pm0.01$} & \multicolumn{1}{c}{$84.3\pm0.02$} & \multicolumn{1}{c}{$79.1\pm0.02$} \\
		&  & REx & \multicolumn{1}{c}{$91.1\pm0.02$} & \multicolumn{1}{c}{$90.6\pm0.02$} & \multicolumn{1}{c}{$90.2\pm0.03$} & \multicolumn{1}{c}{$86.8\pm0.02$} & \multicolumn{1}{c}{$84.7\pm0.02$} & \multicolumn{1}{c}{$80.5\pm0.01$} \\
		&  & GroupDRO & \multicolumn{1}{c}{$92.2\pm0.01$} & \multicolumn{1}{c}{$\bm{91.4}\pm\bm{0.01}$} & \multicolumn{1}{c}{$89.4\pm0.02$} & \multicolumn{1}{c}{$84.0\pm0.01$} & \multicolumn{1}{c}{$84.7\pm0.02$} & \multicolumn{1}{c}{$78.3\pm0.01$} \\
		&  & SharpDRO & \multicolumn{1}{c}{$92.4\pm0.02$} & \multicolumn{1}{c}{$91.1\pm0.02$} & \multicolumn{1}{c}{$\bm{90.3}\pm\bm{0.02}$} & \multicolumn{1}{c}{$\bm{87.5}\pm\bm{0.02}$} & \multicolumn{1}{c}{$\bm{86.4}\pm\bm{0.02}$} & \multicolumn{1}{c}{$\bm{83.3}\pm\bm{0.02}$} \\
		\midrule[0.6pt]
		
	
		\multirow{10}{*}{CIFAR100} & \multirow{5}{*}{Snow} & ERM & $67.7\pm0.01$ & $68.1\pm0.01$ & $64.7\pm0.01$ & $63.1\pm0.01$ & $60.5\pm0.02$ & $57.3\pm0.01$ \\
		&  & IRM & $69.3\pm0.01$ & $67.5\pm0.02$ & $64.9\pm0.02$ & $61.0\pm0.01$ & $58.2\pm0.01$ & $55.1\pm0.01$ \\
		&  & REx & $66.4\pm0.01$ & $65.9\pm0.01$ & $62.4\pm0.01$ & $61.2\pm0.02$ & $57.5\pm0.03$ & $56.0\pm0.02$ \\
		&  & GroupDRO & $68.0\pm0.02$ & $68.2\pm0.01$ & $65.1\pm0.01$ & $60.9\pm0.03$ & $59.8\pm0.01$ & $58.1\pm0.02$ \\
		&  & SharpDRO & $\bm{71.5}\pm\bm{0.01}$ & $\bm{70.8}\pm\bm{0.03}$ & $\bm{67.5}\pm\bm{0.02}$ & $\bm{65.5}\pm\bm{0.01}$ & $\bm{62.3}\pm\bm{0.01}$ & $\bm{59.2}\pm\bm{0.03}$ \\ \cline{2-9} 
		
		
		& \multirow{5}{*}{Shot} & ERM & $67.6\pm0.03$ & $65.1\pm0.01$ & $62.9\pm0.01$ & $56.0\pm0.01$ & $55.1\pm0.01$ & $47.3\pm0.01$ \\
		&  & IRM & $67.5\pm0.02$ & $65.7\pm0.01$ & $62.7\pm0.01$ & $59.5\pm0.01$ & $55.8\pm0.01$ & $48.3\pm0.01$ \\
		&  & REx & $65.7\pm0.01$ & $63.8\pm0.02$ & $61.9\pm0.01$ & $59.3\pm0.03$ & $53.8\pm0.01$ & $48.1\pm0.01$ \\
		&  & GroupDRO & $67.0\pm0.02$ & $65.8\pm0.01$ & $63.1\pm0.01$ & $58.9\pm0.01$ & $57.5\pm0.01$ & $49.3\pm0.01$ \\
		&  & SharpDRO & $\bm{69.2}\pm\bm{0.01}$ & $\bm{67.3}\pm\bm{0.02}$ & $\bm{65.4}\pm\bm{0.03}$ & $\bm{62.5}\pm\bm{0.01}$ & $\bm{57.7}\pm\bm{0.02}$ & $\bm{51.6}\pm\bm{0.01}$ \\ \midrule[0.6pt]
		
		
		\multirow{10}{*}{ImageNet30} & \multirow{5}{*}{Snow} & ERM & $86.7\pm0.03$ & $85.2\pm0.01$ & $83.4\pm0.01$ & $81.1\pm0.01$ & $75.3\pm0.01$ & $75.6\pm0.01$ \\
		&  & IRM & $85.6\pm0.01$ & $84.0\pm0.02$ & $82.1\pm0.03$ & $79.7\pm0.01$ & $75.0\pm0.01$ & $75.6\pm0.01$ \\
		&  & REx & $85.4\pm0.01$ & $84.6\pm0.02$ & $82.7\pm0.02$ & $80.5\pm0.03$ & $75.7\pm0.03$ & $75.9\pm0.03$ \\
		&  & GroupDRO & $86.7\pm0.01$ & $85.5\pm0.03$ & $83.4\pm0.01$ & $81.2\pm0.02$ & $76.3\pm0.01$ & $76.6\pm0.01$ \\
		&  & SharpDRO & $\bm{88.2}\pm\bm{0.02}$ & $\bm{88.2}\pm\bm{0.01}$ & $\bm{85.4}\pm\bm{0.02}$ & $\bm{81.9}\pm\bm{0.01}$ & $\bm{79.8}\pm\bm{0.03}$ & $\bm{79.5}\pm\bm{0.02}$ \\ \cline{2-9} 
		
		
		& \multirow{5}{*}{Shot} & ERM & $86.9\pm0.01$ & $84.8\pm0.01$ & $83.6\pm0.01$ & $79.7\pm0.01$ & $75.4\pm0.01$ & $64.6\pm0.01$ \\
		&  & IRM & $86.8\pm0.01$ & $85.1\pm0.03$ & $81.5\pm0.01$ & $73.5\pm0.02$ & $68.5\pm0.03$ & $62.5\pm0.03$ \\
		&  & REx & $83.8\pm0.01$ & $86.3\pm0.03$ & $82.5\pm0.02$ & $73.9\pm0.01$ & $70.6\pm0.03$ & $64.0\pm0.02$ \\
		&  & GroupDRO & $86.7\pm0.01$ & $85.6\pm0.03$ & $84.5\pm0.01$ & $80.7\pm0.01$ & $76.2\pm0.04$ & $65.4\pm0.01$ \\
		&  & SharpDRO & $\bm{88.1}\pm\bm{0.01}$ & $\bm{87.2}\pm\bm{0.02}$ & $\bm{84.7}\pm\bm{0.01}$ & $\bm{82.2}\pm\bm{0.01}$ & $\bm{78.2}\pm\bm{0.01}$ & $\bm{67.9}\pm\bm{0.02}$ \\  \bottomrule[1pt]
	\end{tabular}
	\vspace{-2mm}
\end{table*}


\begin{table*}[t]
	\small
	\centering
	\caption{\small Quantitative comparisons on distribution-agnostic robust generalization setting. Averaged accuracy ($\%$) with standard deviations are computed over three independent trails.}
	\vspace{-3mm}
	\setlength{\tabcolsep}{1.8mm}
	\label{tab:appendix_distribution_agnostic}
	\begin{tabular}{lllcccccc}
		\toprule[1pt]
		\multirow{2}{*}{Dataset} & \multirow{2}{*}{Type} & \multirow{2}{*}{Method} & \multicolumn{6}{c}{Corruption Severity} \\
		&  &  & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{5} \\ \midrule[0.6pt]
		\multirow{6}{*}{CIFAR10} & \multirow{3}{*}{Snow} & JTT & \multicolumn{1}{c}{$88.6\pm0.02$} & \multicolumn{1}{c}{$87.8\pm0.03$} & \multicolumn{1}{c}{$86.5\pm0.02$} & \multicolumn{1}{c}{$87.2\pm0.02$} & \multicolumn{1}{c}{$84.2\pm0.02$} & \multicolumn{1}{c}{$83.2\pm0.03$} \\
		&  & EIIL & \multicolumn{1}{c}{$88.3\pm0.02$} & \multicolumn{1}{c}{$87.8\pm0.01$} & \multicolumn{1}{c}{$85.6\pm0.02$} & \multicolumn{1}{c}{$87.3\pm0.03$} & \multicolumn{1}{c}{$85.2\pm0.04$} & \multicolumn{1}{c}{$82.3\pm0.01$} \\
		&  & SharpDRO & $\bm{91.6}\pm\bm{0.01}$ & $\bm{91.1}\pm\bm{0.02}$ & $\bm{90.8}\pm\bm{0.01}$ & $\bm{89.7}\pm\bm{0.02}$ & $\bm{86.2}\pm\bm{0.01}$ & $\bm{83.8}\pm\bm{0.02}$ \\ \cline{2-9} 
		
		
		
		& \multirow{3}{*}{Shot} & JTT & \multicolumn{1}{c}{$91.3\pm0.02$} & \multicolumn{1}{c}{$90.5\pm0.03$} & \multicolumn{1}{c}{$89.3\pm0.01$} & \multicolumn{1}{c}{$86.5\pm0.02$} & \multicolumn{1}{c}{$83.1\pm0.02$} & \multicolumn{1}{c}{$79.8\pm0.02$} \\
		&  & EIIL & \multicolumn{1}{c}{$90.3\pm0.03$} & \multicolumn{1}{c}{$90.1\pm0.02$} & \multicolumn{1}{c}{$88.3\pm0.01$} & \multicolumn{1}{c}{$86.2\pm0.02$} & \multicolumn{1}{c}{$82.3\pm0.03$} & \multicolumn{1}{c}{$78.5\pm0.02$} \\
		&  & SharpDRO & $\bm{91.6}\pm\bm{0.01}$ & $\bm{90.5}\pm\bm{0.02}$ & $\bm{89.8}\pm\bm{0.02}$ & $\bm{88.7}\pm\bm{0.01}$ & $\bm{86.0}\pm\bm{0.02}$ & $\bm{81.7}\pm\bm{0.01}$ \\  \midrule[0.6pt]
		
		
		
		\multirow{6}{*}{CIFAR100} & \multirow{3}{*}{Snow} & JTT & $67.5\pm0.01$ & $68.1\pm0.02$ & $65.3\pm0.02$ & $64.3\pm0.02$ & $60.2\pm0.02$ & $57.8\pm0.02$ \\
		&  & EIIL & $68.2\pm0.03$ & $69.1\pm0.03$ & $65.2\pm0.02$ & $64.0\pm0.02$ & $61.0\pm0.04$ & $57.5\pm0.04$ \\
		&  & SharpDRO & $\bm{70.6}\pm\bm{0.02}$ & $\bm{69.9}\pm\bm{0.03}$ & $\bm{66.7}\pm\bm{0.03}$ & $\bm{64.4}\pm\bm{0.02}$ & $\bm{61.9}\pm\bm{0.03}$ & $\bm{60.7}\pm\bm{0.03}$ \\ \cline{2-9} 
		
		
			
		& \multirow{3}{*}{Shot} & JTT & $66.3\pm0.02$ & $65.3\pm0.03$ & $63.4\pm0.02$ & $56.6\pm0.04$ & $55.5\pm0.04$ & $48.6\pm0.04$ \\
		&  & EIIL & $66.5\pm0.02$ & $65.3\pm0.03$ & $62.8\pm0.04$ & $57.5\pm0.02$ & $56.5\pm0.01$ & $49.5\pm0.01$ \\
		&  & SharpDRO & $\bm{68.9}\pm\bm{0.02}$ & $\bm{66.2}\pm\bm{0.03}$ & $\bm{64.9}\pm\bm{0.03}$ & $\bm{60.1}\pm\bm{0.02}$ & $\bm{58.4}\pm\bm{0.03}$ & $\bm{52.7}\pm\bm{0.02}$ \\  \midrule[0.6pt]
		
		
		\multirow{6}{*}{ImageNet30} & \multirow{3}{*}{Snow} & JTT & $86.0\pm0.04$ & $85.8\pm0.02$ & $82.3\pm0.03$ & $80.4\pm0.02$ & $74.6\pm0.02$ & $73.5\pm0.02$ \\
		&  & EIIL & $87.5\pm0.01$ & $85.4\pm0.02$ & $83.5\pm0.04$ & $\bm{81.6}\pm\bm{0.01}$ & $76.3\pm0.01$ & $75.8\pm0.02$ \\
		&  & SharpDRO & $\bm{87.5}\pm\bm{0.03}$ & $\bm{86.7}\pm\bm{0.02}$ & $\bm{85.4}\pm\bm{0.02}$ & $81.5\pm0.03$ & $\bm{78.9}\pm\bm{0.02}$ & $\bm{78.5}\pm\bm{0.03}$ \\ \cline{2-9} 
		
		
		& \multirow{3}{*}{Shot} & JTT & $86.5\pm0.02$ & $85.4\pm0.03$ & $82.6\pm0.04$ & $79.6\pm0.04$ & $77.2\pm0.04$ & $65.0\pm0.01$ \\
		&  & EIIL & $85.5\pm0.01$ & $86.3\pm0.04$ & $81.6\pm0.02$ & $80.2\pm0.03$ & $75.3\pm0.02$ & $64.4\pm0.03$ \\
		&  & SharpDRO & $\bm{87.3}\pm\bm{0.02}$ & $\bm{87.2}\pm\bm{0.03}$ & $\bm{84.6}\pm\bm{0.03}$ & $\bm{83.2}\pm\bm{0.06}$ & $\bm{79.6}\pm\bm{0.03}$ & $\bm{68.0}\pm\bm{0.03}$ \\  \bottomrule[1pt]
	\end{tabular}
	\vspace{-2mm}
\end{table*}


\section{More Details}
\label{appendix_details}
In this section, we first give a practical implementation of our SharpDRO. Then, we provide more experimental details.

\subsection{Practical Implementation}
\label{appendix_implementation}
Our SharpDRO requires two backward phases, so the time complexity is twice as much as plain training, for efficient sharpness computation, please refer to~\cite{du2022efficient, du2022sharpness, zhang2022ga, zhao2022penalizing, zhao2022ss}. In the first step, we record the label prediction $p$ of each data during inference and simultaneously compute the loss $\mathcal{L}$. Additionally, in the first backward pass, we store the computed gradient $\nabla\mathcal{L}(\theta)$. Further, by adding $\epsilon^*$, we use the perturbed model to compute the second label prediction $\hat{p}$, which is further leveraged to compute the sharpness regularization $\mathcal{R}$. Moreover, in the distribution-agnostic setting, the predictions $p$ and $\hat{p}$ from two forward steps are used to compute the OOD score $\omega_i$. Then, we add the recorded gradient $\nabla\mathcal{L}(\theta)$ back to the model parameter and conduct sharpness minimization over the selected worst-case data. In this way, our SharpDRO can be correctly performed. 

\subsection{Experimental Details}
In our experiments, we choose Wide ResNet-28-2~\cite{zagoruyko2016wide} as our backbone model, using stochastic gradient descent with learning rate $3e-2$ as the base optimizer. The momentum and weight decay factor of the optimizer is set to $0.9$ and $5e-4$, respectively. We run all experiments for 200 epochs with three independent trials and report the average test accuracy with standard deviation.

\section{Additional Experiments}
\label{appendix_exp_results}
In the main paper, we have provided the results using ``Gaussian Noise'' corruption and ``JPEG compression'' corruption, here we conduct additional experiments to show the effectiveness of SharpDRO under ``Snow'' and ``Shot Noise'' corruptions. The results on CIFAR10, CIFAR100, and ImageNet30 datasets in both distribution-aware and distribution-agnostic scenarios are shown in Tables~\ref{tab:appendix_distribution_aware} and~\ref{tab:appendix_distribution_agnostic}. We can see that SharpDRO still performances effectively and surpasses other methods with large margin. Especially, on ImageNet30 dataset in both two problem settings, SharpDRO outperforms second-best method about $3\%$, which indicates the capability of SharpDRO on generalization against different corruptions.

