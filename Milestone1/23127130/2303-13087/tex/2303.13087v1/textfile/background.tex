\section{Robust Generalization Methods}
\label{sec:background}
Due to the practical significance of robust generalization, various approaches have been proposed to deal with distribution shift. Here we briefly introduce three typical baseline methods, namely Invariant Risk Minimization, Risk Extrapolation, and GroupDRO.

\textbf{Invariant Risk Minimization (IRM)}~\cite{arjovsky2019invariant,chang2020invariant,creager2021environment} aims to extract the invariant feature across different distributions (also denoted as environments). Specifically, the learning model is separated into a feature extractor $G$ and a classifier $C$. IRM assumes an invariant model $C\circ G$ over various environments can be achieved if the classifier $C$ constantly stays optimal. Then, the learning objective is formulated as:
\vspace{-2mm}
\begin{equation}
	\small
	\begin{aligned}
		&\min_{C^*\circ G} \big\{\mathcal{L}_{\text{IRM}}:=\sum_{e\in \mathcal{E}}\mathcal{L}^e(C^*\circ G)\big\} \\
		&\text{s. t.}\  C^*\in\argmin_{G}\mathcal{L}^e(C\circ G), \text{for all}\  e \in \mathcal{E},
	\end{aligned}
	\label{eq:irm}
\vspace{-2mm}
\end{equation}
where $C^*$ stands for the optimal classifier, and $e$ denotes a specific environment from a given environmental set $\mathcal{E}$. By solving Eq.~\ref{eq:irm}, the feature extractor $G$ can successfully learn invariant information without being influenced by the distribution shift between different environments.

\textbf{Risk Extrapolation (REx)}~\cite{krueger2021out} targets at generalization to out-of-distribution (OOD) environments. Inspired by the discovery that penalizing the loss variance across distributions helps achieve improved performance on OOD generalization, REx proposes to optimizing via:
\vspace{-2mm}
\begin{equation}
	\small
	\min_{\theta\in\Theta}\big\{ \mathcal{L}_{\text{REx}}:=\sum_{e\in \mathcal{E}}\mathcal{L}^e(\theta) + \beta Var(\mathcal{L}^e,..., \mathcal{L}^m)\big\},
	\label{eq:rex}
\vspace{-2mm}
\end{equation}
where $\beta$ controls the penalization level. Intuitively, REx seeks to achieve risk fairness among all $m$ training environments, so as to increase the similarity of different learning tasks. As a result, the training model can capture the invariant information that helps generalize to unseen distributions.

\textbf{GroupDRO}~\cite{sagawa2019distributionally, hashimoto2018fairness, piratla2021focus} deal with the situation when the correlation between class label $y$ and unknown attribute $a$ differs in the training and test set. Such a difference is called spurious correlation which could seriously misguide the model prediction. As a solution, GroupDRO considers each combination of class and attribute as a group $g$. By conducting risk minimization through:
\vspace{-2mm}
\begin{equation}
	\small
	\min_{\theta \in \Theta} \big\{\mathcal{L}_{\text{GroupDRO}}:=\max_{g} \mathbb{E}_{(x, y)\sim P_g}\left[\mathcal{L}(\theta; (x, y))\right]\big\},
	\label{eq:group_dro}
\vspace{-2mm}
\end{equation}
the worst-case group from distribution $P_g$ which commonly holds spurious correlation is emphasized, thus breaking the spurious correlation.


\noindent
\textbf{Discussion:} IRM and REx both focus on learning invariant knowledge across various environments. However, when the training set contains extremely imbalanced noisy distributions, as shown in Figure~\ref{fig:poisson}, the invariant learning results would be greatly misled by the most dominating distribution. Thus, the extracted invariant feature would be questionable for generalization against distribution shift. Although emphasizing the risk minimization of worst-case data via GroupDRO can alleviate the imbalance problem, its generalization performance is still sub-optimal when facing novel test data. However, SharpDRO can not only focus on the uncommon corrupted data but also effectively improve the generalization performance on the test set by leveraging worst-case sharpness minimization.

Our investigated problem is closely related to OOD generalization which is a broad field that contains many popular research topics, such as \textbf{Domain Generalization}~\cite{carlucci2019domain, peng2019moment, qiao2020learning, shu2021open, mahajan2021domain, muandet2013domain, huang2023harnessing, zhang2022towards}, \textbf{Causal Invariant Learning}~\cite{arjovsky2019invariant, krueger2021out, li2018deep, yang2021causalvae, scholkopf2021toward, yue2021transporting}. Generally, existing works mainly studies two types of research problem: 1) mitigating domain shift between training and test dataset; and 2) breaking the spurious correlation between causal factors. However, as generalization against corruptions \textbf{does not introduce any domain shift or spurious correlation}, such a problem cannot be naively solved by domain generalization methods or causal representation learning techniques. Therefore, in this paper, we focus on complementing this rarely-explored field and propose SharpDRO to enforce robust generalization against corruption. In the next section, we elaborate on the methodology of SharpDRO.