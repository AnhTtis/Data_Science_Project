\vspace{-8mm}
\section{Introduction}
\vspace{-1mm}
\label{sec:introduction}
% research motivation
Learning against corruptions has been a vital challenge in the practical deployment of computer vision models, as learning models are much more fragile to subtle noises than human perception systems~\cite{goodfellow2014explaining, hendrycks2016baseline, liu2021probabilistic}. During the training, the encountered corruptions are essentially perceived as distribution shift, which would significantly hinder the prediction results~\cite{liang2017enhancing, long2015learning, tzeng2017adversarial, xia2019anchor, xia2020part,xia2023moderate}. Therefore, to mitigate the performance degradation, enhancing generalization to corrupted data distributions has drawn lots of attention~\cite{arjovsky2019invariant,sagawa2019distributionally}.


\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{poisson.pdf}
	\vspace{-2mm}
	\caption{Illustration of photon-limited corruptions.}
	\vspace{-6mm}
	\label{fig:poisson}
\end{figure}

In the real world, noise corruptions are commonly known as photon-limited imaging problems~\cite{ingle2021passive, li2021photon, luisier2010image, timmermann1999multiscale} which arises due to numerous small corruption photons arriving to a image sensor. Consequently, different numbers of captured photons would form different levels of corruptions severity, further producing multiple data distributions and imposing varied impacts on learning models~\cite{hendrycks2019benchmarking}. Specifically, the encountered photon-limited corruption $\mathscr{E}$ is a composition of multiple noise photon $u$, which is triggered by some discrete factors with a certain probability during a time interval. For example, a photon $u$ can be triggered by each platform changing, re-distribution, transmission, etc. More photons are captured, and severer corruption would be applied to the image. Therefore, the severity $s$ of the photon-limited corruptions $\mathscr{E}$ can be modeled by Poisson distribution, \textit{i.e.}, $s\sim P\left( s; \lambda \right) = \frac{{e^{-\lambda } \lambda ^s }}{{s!}}$, which is illustrated in Figure~\ref{fig:poisson}. As a result, the real-world training set is not completely composed of clean data, but contains corrupted data with a smaller proportion as the severity goes stronger.


\begin{figure*}[t]
	\vspace{-10mm}
	\centering
	\includegraphics[width=0.9\linewidth]{visualization.pdf}
	\vspace{-3mm}
	\caption{Illustration of our motivation. (a) Loss surface visualization of GroupDRO and the proposed SharpDRO. The columns from left to right stand for corrupted distributions with severity $s=0$ to $5$. (b) Illustration of why a sharp loss surface hinders generalization to test data.}
	\vspace{-5mm}
	\label{fig:visualization}
\end{figure*}


% problem of the existing method
Dealing with such a realistic problem by vanilla empirical risk minimization can achieve satisfactory averaged accuracy on the whole training set. However, due to the extremely limited number of severely-corrupted data, the learning model would produce large training errors on the corrupted distributions, further hindering the robust performance under challenging real-world situations. A popular approach to achieve low error on the scarce corrupted data is distributionally robust optimization (DRO)~\cite{namkoong2016stochastic,sagawa2019distributionally,zhai2021doro, piratla2021focus, wang2022meta, wang2022improving}, which commonly optimizes the model parameter $\theta$ by optimizing:
\vspace{-1mm}
\begin{equation}
	\min_{\theta \in \Theta} \sup_{Q \in \mathcal{Q}} \mathbb{E}_{(x, y)\sim Q}\left[\mathcal{L}(\theta; (x, y))\right],
	\label{eq:general_dro}
	\vspace{-1mm}
\end{equation}
where $\mathcal{Q}$ denotes the uncertainty set that is utilized to estimate the possible test distribution. Intuitively, DRO assumes that $\mathcal{Q}$ consists of multiple sub-distributions, among which exists a worst-case distribution $Q$. By concentrating on the risk minimization of the worst-case distribution, DRO hopes to train a robust model which can deal with the potential distribution shift during the test phase. However, existing DRO methods usually leverage over-parameterized models to focus on a small portion of worst-case training data. Therefore, the worst-case data contaminated with severe corruption is highly possible to get stuck into sharp minima. As shown in the upper of Figure~\ref{fig:visualization} (a), a stronger corruption would cause the existing method to learn a sharper loss surface. Consequently, optimization via DRO fails to produce a flat loss landscape over the corrupted distributions, which leads to a large generalization gap between training and test set~\cite{keskar2017large,chaudhari2017entropy}.

%which significantly hinders the robust generalization performance over corrupted test distributions 


% my solution
To remedy this defect, in this paper, we propose SharpDRO method to focus on learning a flat loss landscape of the worst-case data, which can largely mitigate the training-test generalization gap problem of DRO. Specifically, we adopt the largest loss difference formed by applying weight perturbation~\cite{foret2020sharpness,wu2020adversarial} to measure the sharpness of the loss function. Intuitively, a sharp loss landscape is sensitive to noise and cannot generalize well on the test set. On the contrary, a flat loss landscape produces consistent loss values and is robust against perturbations (Figure~\ref{fig:visualization} (b)). By minimizing the sharpness, we can effectively enhance the generalization performance~\cite{keskar2017large, chaudhari2017entropy}. However, directly applying sharpness minimization on multiple distributions would yield poor results~\cite{cha2021swad}, as the computed sharpness could be influenced by the largest data distribution, and thus cannot generalize well to small corrupted data. Therefore, we only focus on worst-case sharpness minimization. In this way, as the lower of Figure~\ref{fig:visualization} (a) shows, SharpDRO successfully produces a flat loss surface, thus achieving robust generalization on the severely corrupted distributions.

In addition, identification of the worst-case distribution requires expensive annotations, which are not always practically feasible~\cite{liu2021just}. In this paper, we apply SharpDRO to solve two problem settings: 1) \textit{Distribution-aware robust generalization} which assumes that distribution indexes are accessible, and 2) \textit{Distribution-agnostic robust generalization} where the distributions are no longer identifiable, making the worst-case data hard to find. Existing approaches such as Just Train Twice (JTT) require two-stage training which is rather inconvenient. To tackle this challenge, we propose a simple (Out-of-distribution) OOD detection~\cite{hendrycks2016baseline,huang2021universal, huang2022they,liang2017enhancing, liu2020energy, wang2022watermarking, wang2023out} process to detect the worst-case data, which can be further leveraged to enable worst-case sharpness minimization. Through constructing training sets according to the Poisson distributed noisy distribution using CIFAR10/100 and ImageNet30, we show that SharpDRO can achieve robust generalization results on both two problem settings, surpassing well-known baseline methods by a large margin.

To sum up, our main contributions are three-fold:
\begin{itemize}
	\item We proposed a sharpness-based DRO method that overcomes the poor worst-case generalization performance of distributionally robust optimization.
	
	\item We apply the proposed SharpDRO method to both distribution-aware and distribution-agnostic settings, which brings a practical capability to our method. Moreover, we propose an OOD detection approach to select worst-case data to enable robust generalization.
	
	\item Theoretically, we show that SharpDRO has a convergence rate of $\mathcal{O}(\frac{\kappa^2}{\sqrt{MT}})$. Empirically, we form a photon-limited corruption dataset that follows Poisson distribution, and conduct extensive experiments to show a strong generalization ability of SharpDRO as well as its superiority to compared baseline methods.
\end{itemize}


% paper organization
In the following, we first briefly introduce the background and discuss the problem setting in section.~\ref{sec:background}. Then, we specify our SharpDRO over two problem settings in Section~\ref{sec:method}. Moreover, we give a detailed optimization process and provide convergence analysis in Section~\ref{sec:optimization}. Further, we conduct extensive experiments to validate our SharpDRO in Section~\ref{sec:experiments}. At last, we conclude this paper in Section~\ref{sec:conclusion}.