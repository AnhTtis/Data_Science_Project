\section{Supervised discovery of semantic directions}\label{sec:Supervised methods}
%%% Motivation 
%%% The unsupervised method are great but require manual inspection.
While the methods we presented in Sec.~\ref{sec:Unsupervised methods} discover interpretable semantic directions in a fully unsupervised fashion, their effects must be interpreted manually. In this section, we demonstrate a simple supervised approach to obtain latent directions corresponding to well-defined labels.


\paragraph{Linear semantic directions from examples}
The vector arithmetic property of $h$-space suggests an intuitive method for discovering semantically meaningful directions, by providing positive and negative examples of a desired attribute. 
Let $\{(\mathbf{x}_i^-,\mathbf{x}_i^+ )\}_{i=1}^n$ be a collection of generated images, such that all $\mathbf{x}_i^+$ have a desired attribute that is absent in~$\mathbf{x}_i^-$, \eg a smile, old age, glasses, \etc. 
Let~$\mathbf{q}_i^-$ and $\mathbf{q}_i^+$ denote the latent representation corresponding to the images~$\mathbf{x}_i^-$ and~$\mathbf{x}_i^+$. Then, we can find a semantic direction~$\mathbf{v}$ as 
\begin{equation}\label{eq:linear_direction}
    \mathbf{v} = \frac{1}{n} \sum_{i=1}^n\left(\mathbf{q}_i^+-\mathbf{q}_i^-\right).
\end{equation}

% Effect of editing in the DDIM Noise space vs $h$-space
Note that this method can be applied using either $\mathbf{h}_{T:1}$ or $\mathbf{x}_T$ as the latent variable. 
However, defining semantic directions using $\mathbf{h}_{T:1}$ as the latent variable requires far fewer samples than using $\mathbf{x}_T$. Figure~\ref{fig:h_vs_ddim_numsamples} illustrates this for DDIM ($\eta_t = 0$) for a direction corresponding to smile where \eqref{eq:linear_direction} is calculated using a varying number of samples.



\paragraph{Classifier annotation}\label{sec:supervised}
We now propose to find linear semantic directions by using pretrained attribute classifiers to annotate samples generated by the model. Using the attribute classifier from \cite{lin2021anycost}, we annotate samples with probabilities corresponding to the $40$ classes from CelebA \cite{liu2015celeba}, and use Hopenet \cite{Ruiz2018hopenet} to predict pose (yaw, pitch, and roll).
We sort the annotated samples according to the attribute scores and select the samples with the highest and lowest scores from each class as the positive and negative examples respectively. 
We then calculate semantic directions corresponding to the different attributes using the method given in  \eqref{eq:linear_direction}.

%% Attribute manipulation 
As shown in Fig.~\ref{fig:single_attr}, we can successfully find semantic directions controlling a wide selection of meaningful attributes like yaw, smile, gender, glasses, and age. 
%% Sequential Manipulation
Furthermore, directions calculated by \eqref{eq:linear_direction} can be applied in combination with one another. 
For example, adding~$\Delta \mathbf{h}_{T:1}$ for two attributes, like pose and smile, results in an image where both attributes are changed. 
Fig.~\ref{fig:anycost_sequential} illustrates sequential editing, showcasing changes in expression followed by pose, age, and eyeglasses for two samples. 
In SM Sec.~\ref{SM:bu3dfe} we show that this method can be applied to find directions corresponding to facial expressions using DDIM inversion and a real facial expression dataset \cite{yin2006bu3dfe} as supervision. 


\begin{figure}[t]
    \centering
\begin{subfigure}[b]{0.48\linewidth}
\includegraphics[width=\textwidth]{figs/anycost_yaw2.png}
\caption{Yaw}
\end{subfigure}
\begin{subfigure}[b]{0.48\linewidth}
\includegraphics[width=\textwidth]{figs/anycost_smiling2.png}
\caption{Smile}
\end{subfigure}
           
\begin{subfigure}[b]{0.49\linewidth}
\includegraphics[width=\textwidth]{figs/anycost_gender2.png}
\caption{Gender}
\end{subfigure}
\begin{subfigure}[b]{0.49\linewidth}
\includegraphics[width=\textwidth]{figs/anycost_pitch2.png}
\caption{Pitch}
\end{subfigure}
            
\begin{subfigure}[b]{0.49\linewidth}
\includegraphics[width=\textwidth]{figs/anycost_glasses2.png}
\caption{Glasses}
\end{subfigure}
\begin{subfigure}[b]{0.49\linewidth}
\includegraphics[width=\textwidth]{figs/anycost_age2.png}
\caption{Age}
\end{subfigure}

\caption{\textbf{Single attribute manipulation.}
Using a domain-specific binary attribute classifier, we find linear directions in $h$-space corresponding to a variety of semantic edits.}
\label{fig:single_attr}
\end{figure}

% \begin{figure*}[tb]
%     \centering
%         \begin{subfigure}[b]{0.19\linewidth}
%             \includegraphics[width=\textwidth]{figs/anycost/yaw2.png}
%             \caption{Yaw}
%             \end{subfigure}
%             \begin{subfigure}[b]{0.19\linewidth}
%             \includegraphics[width=\textwidth]{figs/anycost/smiling2.png}
%             \caption{Smile}
%             \end{subfigure}
%             \begin{subfigure}[b]{0.19\linewidth}
%             \includegraphics[width=\textwidth]{figs/anycost/gender2.png}
%             \caption{Gender}
%             \end{subfigure}
%             %\\
%             % \begin{subfigure}[b]{0.15\linewidth}
%             % \includegraphics[width=\textwidth]{figs/anycost/pitch2.png}
%             % \caption{Pitch}
%             % \end{subfigure}
%             \begin{subfigure}[b]{0.19\linewidth}
%             \includegraphics[width=\textwidth]{figs/anycost/glasses2.png}
%             \caption{Glasses}
%             \end{subfigure}
%             \begin{subfigure}[b]{0.19\linewidth}
%             \includegraphics[width=\textwidth]{figs/anycost/age2.png}
%             \caption{Age}
%         \end{subfigure}
%         \caption{\textbf{Single attribute manipulation.}
%         Using a domain-specific binary attribute classifier, we find linear directions in $h$-space corresponding to a variety of semantic edits.}
%         \label{fig:single_attr}
% \end{figure*}

\begin{figure}[tb]
\centering
    \begin{subfigure}[b]{0.99\linewidth}
        \includegraphics[width=\linewidth]{figs/anycost_h-vs-ddim-numsamples.png}
        \caption{Editing in $h$-space vs.\@ using $\mathbf{x}_T$.}
        \label{fig:h_vs_ddim_numsamples}
    \end{subfigure}\\[2mm]
    \begin{subfigure}[b]{0.99\linewidth}
        \includegraphics[width=\linewidth]{figs/anycost_sequential.png}
        \caption{Sequential manipulation.}
        \label{fig:anycost_sequential}
    \end{subfigure}
    \caption{
    \textbf{Editing properties of $h$-space.}
    \subref{fig:h_vs_ddim_numsamples} 
    A qualitative comparison of the editing effect using $\mathbf{x}_T$ (top) and $\mathbf{h}_{T:1}$ (bottom). Latent variables using a smiling direction found by \eqref{eq:linear_direction}.
    While the direction in $h$-space converges with a few labeled examples, more than $200$ are required to achieve a similar result using $\mathbf{x}_T$ as the latent variable.
    \subref{fig:anycost_sequential} 
    Directions found with our method can be combined with one another. Here, we sequentially accumulate four effects, starting from a single effect in the 2nd column up to four effects in the 5th column.
    }
\end{figure}


\paragraph{Disentanglement of semantic directions}
Latent directions found by \eqref{eq:linear_direction} might be semantically entangled, in the sense that editing in the direction corresponding to some desired attribute might also induce a change in some other undesired attributes. For example, a direction for eyeglasses may also affect the age if it correlates with eyeglasses in the training data. 
% Conditional manipulation
To remedy this, we propose conditional manipulation in $h$-space in a way similar to what was suggested in the context of GANs by Shen \etal~\cite{Shen2020Interfacegan,Shen2020InterfaceganTPAMI}. Let $\mathbf{v}_1$ and $\mathbf{v}_2$ be two linear semantic directions, where the two corresponding semantic attributes are entangled. We can define a new direction $\mathbf{v}_{1\perp 2}$ which only affects the semantics associated with $\mathbf{v}_1$, without changing the semantics associated with $\mathbf{v}_2$. This is done simply by removing from $\mathbf{v}_1$ the projection of $\mathbf{v}_1$ onto $\mathbf{v}_2$, namely $\mathbf{v}_{1\perp 2} = \mathbf{v}_1 - \langle \mathbf{v}_1  , \mathbf{v}_2 \rangle /  \| \mathbf{v}_2\|^2   \mathbf{v}_2$.
%% Disentangeling multiple semantics
In case of conditioning on multiple semantics simultaneously, our aim is to remove the effects of a collection of $k$ directions $\{\mathbf{v}_i\}_{i=1}^k$ from a primal direction $\mathbf{v}_0$ in order to define a new direction $\mathbf{v}$ which only affects the target attribute. 
This can be done by constructing the matrix $\mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, \cdots, \mathbf{v}_k]$ and projecting $\mathbf{v}_0$ onto the orthogonal complement of the column space of $\mathbf{V}$ by
\begin{equation}\label{eq:condition_multiple_semantics}
\mathbf{v} =  \left[\mathbf{I} -  \mathbf{V}\left(\mathbf{V}^\mathrm{T}\mathbf{V}\right)^{-1} \mathbf{V}^\mathrm{T} \right] \mathbf{v}_0.
\end{equation}  
The resulting direction will be disentangled from each of the directions $\{\mathbf{v}_i\}$, meaning that moving a sample along this new direction will result in a large change in the attribute associated with $\mathbf{v}_0$ while minimally affecting the attributes associated with the other directions. 
Figure.~\ref{fig:conditional_anycost} visualizes the effect of interpolating in the directions of age and eyeglasses for two samples. As can be seen, these directions are entangled with gender and age, respectively. By using our method we can successfully remove the entanglement and define a direction which only affects age or the presence of glasses. 

%% Conditional manipulation
%In some cases, dataset biases can cause the directions to be entangled with one another. For example, we observe that the direction for ``glasses'' is entangled with ``age'', which may be explained by the fact that these attributes are correlated in the training data. 

\begin{figure*}[tb]
    \centering
    \includegraphics[width=\linewidth]{figs/anycost_disentanglement_combined.png}
    \caption{\textbf{Disentanglement of semantic directions.}
    Given a direction that is entangled with other attributes, we can create a disentangled direction by removing the projection onto undesired semantics. The top row shows the original direction, whereas the bottom row shows the disentangled direction.}
    \label{fig:conditional_anycost}
\end{figure*}

% \begin{table}
% \caption{\textbf{Evaluation of disentanglement strategy.}
% % 
% We quantitatively evaluate the effect of disentangling semantic directions using linear projection. 
% The rows correspond to the applied directions, while the columns correspond to the effect of the edits according to CLIP.
% The strongest effect in each row is highlighted.}
% \label{tab:disentanglement}
% \begin{subtable}[t]{0.48\textwidth}
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{l|ccccc}  
% \diagbox[width=5em]{Edit}{Effect} &\rotatebox{45}{Smile}&\rotatebox{45}{Glasses}&\rotatebox{45}{Age}&\rotatebox{45}{Gender}&\rotatebox{45}{Hat}\\
% \hline
%  & \multicolumn{5}{c}{Original directions} \\
% \hline
% Smile   &0.26 &0.28 &0.07 &\textbf{0.33} &0.06\\
% Glasses &0.47 &0.31 &\textbf{0.71} &0.65 &0.15\\
% Age     &0.06 &0.41 &\textbf{0.77} &0.65 &0.16\\
% Gender  &0.31 &0.01 &0.37 &\textbf{0.64} &0.22\\
% Hat     &0.42 &0.00 &0.38 &\textbf{0.64} &0.41\\
% \hline
% \end{tabular}
% }
% \end{subtable}
% \begin{subtable}[t]{0.48\textwidth}
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{l|ccccc}  
% \diagbox[width=5em]{Edit}{Effect} &\rotatebox{45}{Smile}&\rotatebox{45}{Glasses}&\rotatebox{45}{Age}&\rotatebox{45}{Gender}&\rotatebox{45}{Hat}\\
% \hline
%  & \multicolumn{5}{c}{Disentangled directions} \\
% \hline
% Smile   &\textbf{0.24} &0.18 &0.03  &0.07 &0.03\\
% Glasses &0.20 &\textbf{0.36} &0.12  &0.09 &0.19\\
% Age     &0.01 &0.39 &\textbf{0.61} &0.15 &0.03\\
% Gender  &0.19 &0.02 &0.06 & \textbf{0.39} & 0.06\\
% Hat     &0.13 &0.03 &0.02 &0.09 &\textbf{0.43}\\
% \hline
% \end{tabular}
% }
% \end{subtable}
% \end{table}

%\newif\ifnewtable
% \usepackage{ifthen}
% \newboolean{long}  

% % \def\rot{0}
% % \setlength{\tabcolsep}{3pt}
% \begin{table*}
% \centering
% \normalsize
% \caption{\textbf{Evaluation of disentanglement strategy.} 
% We quantitatively evaluate the effect of disentangling semantic directions using linear projection. 
% The rows correspond to the applied directions, while the columns correspond to the effect of the edits according to CLIP. 
% We draw and edit 100 random samples and repeat the experiment 10 times with different seeds and report the mean and standard deviations. The strongest effect in each row is highlighted.
% }
% \label{tab:disentanglement}
% \begin{tabular}{l|ccccc|ccccc}  
% \diagbox[width=5em]{Edit}{Effect} 
% & Smile & Glasses & Age & Gender & Hat
% & Smile & Glasses & Age & Gender & Hat\\
% % \midrule
% \hline 
% \\[-8pt]
% & \multicolumn{5}{c}{Original directions}  &  \multicolumn{5}{c}{Disentangled directions}  
% \\[2pt]
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%% OLD TABLE BEGIN
% % \hline
% % Smile       &0.26$\pm$0.02 & 0.29$\pm$0.02 &0.08$\pm$0.02 & \textbf{0.31$\pm$0.04} & 0.07$\pm$0.01
% %             & \textbf{0.24$\pm$0.02} & 0.20$\pm$0.02 &0.04$\pm$0.02 & 0.09$\pm$0.03 & 0.03$\pm$0.01\\
% % Glasses     &0.48$\pm$0.02 & 0.32$\pm$0.02 & \textbf{0.68$\pm$0.03} & 0.66$\pm$0.04 & 0.14$\pm$0.02
% %             &0.22$\pm$0.01 & \textbf{0.38$\pm$0.02} &0.13$\pm$0.02 & 0.07$\pm$0.03 & 0.36$\pm$0.02\\
% % Age         &0.07$\pm$0.01 & 0.40$\pm$0.03 & \textbf{0.74$\pm$0.03} & 0.66$\pm$0.04 & 0.18$\pm$0.01
% %             &0.02$\pm$0.02 & 0.38$\pm$0.03 &\textbf{0.59$\pm$0.04} & 0.16$\pm$0.03 & 0.04$\pm$0.02\\
% % Gender      &0.40$\pm$0.02 & 0.28$\pm$0.03 &0.58$\pm$0.03 & \textbf{0.66$\pm$0.04} & 0.09$\pm$0.02
% %             &0.20$\pm$0.02 & 0.01$\pm$0.01 &0.08$\pm$0.02 & \textbf{0.39$\pm$0.03} & 0.07$\pm$0.02\\
% % Hat         &0.42$\pm$0.02 & 0.39$\pm$0.02 &0.37$\pm$0.03 & \textbf{0.66$\pm$0.04} & 0.41$\pm$0.02
% %             &0.13$\pm$0.01 & 0.03$\pm$0.03 &0.02$\pm$0.03 & 0.02$\pm$0.09 & \textbf{0.44$\pm$0.02}\\
% % \hline
% % OLD TABLE END
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \hline
% Smile       & \textbf{0.265} & 0.192 & 0.181 & 0.177 & 0.164
%             & \textbf{0.270} & 0.193 & 0.104 & 0.227 & 0.170\\
% Glasses     & 0.342 & 0.359 & \textbf{0.400} & 0.270 & 0.230
%             & 0.177 & \textbf{0.317} & 0.125 & 0.108 & 0.195\\
% Age         & 0.193 & 0.357 & \textbf{0.660} & 0.169 & 0.158
%             & 0.137 & 0.361 & \textbf{0.554} & 0.135 & 0.134 \\
% Gender      & 0.397 & 0.310 & 0.594 & \textbf{0.701} & 0.211
%             & 0.232 & 0.236 & 0.402 & \textbf{0.700} & 0.226\\
% Hat         & 0.240 & 0.234 & 0.163 & 0.274 & \textbf{0.309}
%             & 0.139 & 0.204 & 0.101 & 0.169 & \textbf{0.337}\\
% \hline
% \end{tabular}
% \end{table*} 
% %%%%%%% \def\rot{0}
% \setlength{\tabcolsep}{3pt}
% \begin{table*}
% \centering
% \normalsize
% \caption{\textbf{Evaluation of disentanglement strategy.} 
% We quantitatively evaluate the effect of disentangling semantic directions using linear projection. 
% The rows correspond to the applied directions, while the columns correspond to the effect of the edits according to CLIP. 
% We draw and edit 100 random samples and repeat the experiment 10 times with different seeds and report the mean and standard deviations. The strongest effect in each row is highlighted.
% }
% \label{tab:disentanglement}
% \begin{tabular}{l|ccccc|ccccc}  
% \diagbox[width=5em]{Edit}{Effect} 
% & Smile & Glasses & Age & Gender & Hat
% & Smile & Glasses & Age & Gender & Hat\\
% % \midrule
% \hline 
% \\[-8pt]
% & \multicolumn{5}{c}{Original directions}  &  \multicolumn{5}{c}{Disentangled directions}  
% \\[2pt]
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%% OLD TABLE BEGIN
% % \hline
% % Smile       &0.26$\pm$0.02 & 0.29$\pm$0.02 &0.08$\pm$0.02 & \textbf{0.31$\pm$0.04} & 0.07$\pm$0.01
% %             & \textbf{0.24$\pm$0.02} & 0.20$\pm$0.02 &0.04$\pm$0.02 & 0.09$\pm$0.03 & 0.03$\pm$0.01\\
% % Glasses     &0.48$\pm$0.02 & 0.32$\pm$0.02 & \textbf{0.68$\pm$0.03} & 0.66$\pm$0.04 & 0.14$\pm$0.02
% %             &0.22$\pm$0.01 & \textbf{0.38$\pm$0.02} &0.13$\pm$0.02 & 0.07$\pm$0.03 & 0.36$\pm$0.02\\
% % Age         &0.07$\pm$0.01 & 0.40$\pm$0.03 & \textbf{0.74$\pm$0.03} & 0.66$\pm$0.04 & 0.18$\pm$0.01
% %             &0.02$\pm$0.02 & 0.38$\pm$0.03 &\textbf{0.59$\pm$0.04} & 0.16$\pm$0.03 & 0.04$\pm$0.02\\
% % Gender      &0.40$\pm$0.02 & 0.28$\pm$0.03 &0.58$\pm$0.03 & \textbf{0.66$\pm$0.04} & 0.09$\pm$0.02
% %             &0.20$\pm$0.02 & 0.01$\pm$0.01 &0.08$\pm$0.02 & \textbf{0.39$\pm$0.03} & 0.07$\pm$0.02\\
% % Hat         &0.42$\pm$0.02 & 0.39$\pm$0.02 &0.37$\pm$0.03 & \textbf{0.66$\pm$0.04} & 0.41$\pm$0.02
% %             &0.13$\pm$0.01 & 0.03$\pm$0.03 &0.02$\pm$0.03 & 0.02$\pm$0.09 & \textbf{0.44$\pm$0.02}\\
% % \hline
% % OLD TABLE END
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \hline
% Smile       & \textbf{0.265} & 0.192 & 0.181 & 0.177 & 0.164
%             & \textbf{0.270} & 0.193 & 0.104 & 0.227 & 0.170\\
% Glasses     & 0.342 & 0.359 & \textbf{0.400} & 0.270 & 0.230
%             & 0.177 & \textbf{0.317} & 0.125 & 0.108 & 0.195\\
% Age         & 0.193 & 0.357 & \textbf{0.660} & 0.169 & 0.158
%             & 0.137 & 0.361 & \textbf{0.554} & 0.135 & 0.134 \\
% Gender      & 0.397 & 0.310 & 0.594 & \textbf{0.701} & 0.211
%             & 0.232 & 0.236 & 0.402 & \textbf{0.700} & 0.226\\
% Hat         & 0.240 & 0.234 & 0.163 & 0.274 & \textbf{0.309}
%             & 0.139 & 0.204 & 0.101 & 0.169 & \textbf{0.337}\\
% \hline
% \end{tabular}
% \end{table*} 
%%%%%%

\def\rot{0}
\setlength{\tabcolsep}{3pt}
\begin{table*}
\caption{\textbf{Evaluation of disentanglement strategy.} 
We quantitatively evaluate the effect of disentangling semantic directions using linear projection. 
The rows correspond to the applied directions, while the columns correspond to the effect of the edits according to CLIP. 
We draw and edit 100 random samples and repeat the experiment 10 times with different seeds and report the mean and standard deviations. The strongest effect in each row is highlighted.
}
\label{tab:disentanglement}
% \begin{subtable}[t]{0.48\textwidth}
\resizebox{\linewidth}{!}{%
\begin{tabular}{l|ccccc|ccccc}  
\diagbox[width=5em]{Edit}{Effect} 
&\rotatebox{\rot}{Smile}&\rotatebox{\rot}{Glasses}&\rotatebox{\rot}{Age}&\rotatebox{\rot}{Gender}&\rotatebox{\rot}{Hat}
&\rotatebox{\rot}{Smile}&\rotatebox{\rot}{Glasses}&\rotatebox{\rot}{Age}&\rotatebox{\rot}{Gender}&\rotatebox{\rot}{Hat}\\
% \midrule
\hline 
\\[-8pt]
& \multicolumn{5}{c}{Original directions}  &  \multicolumn{5}{c}{Disentangled directions}  
\\[2pt]
\hline
Smile       &0.26$\pm$0.02 & 0.29$\pm$0.02 &0.08$\pm$0.02 & \textbf{0.31$\pm$0.04} & 0.07$\pm$0.01
            & \textbf{0.24$\pm$0.02} & 0.20$\pm$0.02 &0.04$\pm$0.02 & 0.09$\pm$0.03 & 0.03$\pm$0.01\\
Glasses     &0.48$\pm$0.02 & 0.32$\pm$0.02 & \textbf{0.68$\pm$0.03} & 0.66$\pm$0.04 & 0.14$\pm$0.02
            &0.22$\pm$0.01 & \textbf{0.38$\pm$0.02} &0.13$\pm$0.02 & 0.07$\pm$0.03 & 0.36$\pm$0.02\\
Age         &0.07$\pm$0.01 & 0.40$\pm$0.03 & \textbf{0.74$\pm$0.03} & 0.66$\pm$0.04 & 0.18$\pm$0.01
            &0.02$\pm$0.02 & 0.38$\pm$0.03 &\textbf{0.59$\pm$0.04} & 0.16$\pm$0.03 & 0.04$\pm$0.02\\
Gender      &0.40$\pm$0.02 & 0.28$\pm$0.03 &0.58$\pm$0.03 & \textbf{0.66$\pm$0.04} & 0.09$\pm$0.02
            &0.20$\pm$0.02 & 0.01$\pm$0.01 &0.08$\pm$0.02 & \textbf{0.39$\pm$0.03} & 0.07$\pm$0.02\\
Hat         &0.42$\pm$0.02 & 0.39$\pm$0.02 &0.37$\pm$0.03 & \textbf{0.66$\pm$0.04} & 0.41$\pm$0.02
            &0.13$\pm$0.01 & 0.03$\pm$0.03 &0.02$\pm$0.03 & 0.02$\pm$0.09 & \textbf{0.44$\pm$0.02}\\
\hline
\end{tabular}
}
\end{table*} 

%%%%% NEW NUMBERS FOR THE TABLE
%for directions 
% ["Smiling", "Eyeglasses", "Young", "Male", "Wearing_Hat"]
% Primal directions
% [[0.2654 0.1919 0.1809 0.1771 0.164 ]
% [0.3416 0.3591 0.3997 0.2698 0.2296]
% [0.1926 0.3574 0.66   0.1687 0.1583]
% [0.397  0.3096 0.5938 0.7007 0.2108]
% [0.2401 0.2341 0.1626 0.2744 0.3093]]
 
% Directions with conditioning
% [[0.2695  0.1929  0.10443 0.2269  0.1696 ]
% [0.1768  0.3167  0.1254  0.1079  0.1954 ]
% [0.1368  0.361   0.5537  0.1346  0.1343 ]
% [0.2324  0.2356  0.402   0.7     0.226  ]
% [0.1392  0.2039  0.10065 0.1693  0.3374 ]]

%%% Comments on the CLIP experiment in the Table
To validate the effectiveness of our disentanglement strategy, we performed an experiment where we edited attributes corresponding to smile, glasses, age, gender, and wearing a hat.
We edited samples using both the original and the disentangled directions while measuring the effect of each edit using CLIP~\cite{Radford2021CLIP} as a zero-shot classifier. 
We selected appropriate positive and negative prompts for each attribute. For smiling, glasses, and hat we used {\tt "A smiling person"}, {\tt "A person wearing glasses"} and {\tt "A person wearing a hat"} for the positive prompts respectively, and {\tt "A person"} as the negative prompt. 
For age and gender, we used {\tt "A man"} / {\tt "A woman"} and   {\tt "An old person" } /  {\tt "A young person"} respectively.
For each sample, we edited each of the five attributes and measured the change in attribute score according to CLIP.
Table~\ref{tab:disentanglement} shows the results. 
We can see that the original directions are highly entangled with other attributes while the disentangled directions induce the largest changes in the intended attributes. This demonstrates that semantic directions can be disentangled by a simple linear projection. 

