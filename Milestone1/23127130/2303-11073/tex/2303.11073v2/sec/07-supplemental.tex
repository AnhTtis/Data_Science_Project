% \subsection*{Supplemental Materials}

\begin{center}
{\Large \textbf{Supplemental Materials}}
\end{center}
\vspace{1cm}

\subsection{Illustration of \texorpdfstring{$h$}{h}-space.} \label{sm:hspace-diagram}

In this paper, we define $h$-space as the space of bottleneck activations $\mathbf{h}_t$ across each of the  $T$ timesteps in the synthesis process. See illustration in Fig.~\ref{fig:hspace}. Each downsampling block increases the number of channels while decreasing the spacial dimension of the feature maps. 
In our case, using the pretrained DDPM model trained on  CelebA released by Google\footnote{\url{https://huggingface.co/google/ddpm-ema-celebahq-256}}. The input pixel space has dimensions $(3,256,256)$ and the deepest feature map has dimensions $(512,8,8)$. Thus an element of $h$-space, $\mathbf{h}_{T:1}$, has dimensions $(T,512,8,8)$ and is defined as

% Mathematically, we define a latent code $\mathbf{h}_{T:1}$ in $h$-space as
\begin{align}
    \mathbf{h}_{T:1} = \mathbf{h}_{T} \otimes \mathbf{h}_{T-1}  \otimes \cdots  \otimes \mathbf{h}_{2} \otimes \mathbf{h}_{1}.
\end{align}

We apply directions in $h$ space by perturbing  $\mathbf{h}_{T:1}$ with some offset as $\mathbf{h}_{T:1} + \Delta\mathbf{h}_{T:1}$ during the generative process in \eqref{eq:ddim-reverse}. When $\eta_t \neq 0$ the clean image is completely specified by the triple $(\mathbf{x}_T, \mathbf{z}_{T:1}, \Delta\mathbf{h}_{T:1})$ and for $\eta_t = 0$ (DDIM) it is determined by the tuple $(\mathbf{x}_T, \Delta\mathbf{h}_{T:1})$. 

\begin{figure}[b] %[h!]
\centering
\includegraphics[width=0.6\linewidth]{figs/hspace-diagram-new.pdf}
\caption{\textbf{Illustration of $h$-space.}
In this paper, we define the semantic latent space of DDMs as the activation after the deepest bottleneck layer of the U-Net.
}
\label{fig:hspace}
\end{figure}

%\clearpage
\subsection{The effect of Asyrp}
\label{SM:asyrp}
In the main text, we stated that using Asyrp \cite{Kwon2022ddmhavesemantic} acts to amplify the effect edits in $h$-space. 
However, Asyrp is computationally costly since it requires two forward passes of the U-Net at each denoising step. 
Hence, Asyrp is not used for any of the results shown in the main paper.
% as similar edits can be achieved by simply increasing the scale.
In Figs.~\ref{SM:asyrp-plot1} and \ref{SM:asyrp-plot2} we qualitatively compare edits with and without using Asyrp. We observe that simply adjusting the scale of the applied direction results in very similar edits.

%\clearpage
\begin{figure}[ht]
\centering
\begin{subfigure}[b]{0.35\linewidth}
\includegraphics[width=\linewidth]{figs/sm/eyes1.png}
\caption{Eyes}
\end{subfigure}
\begin{subfigure}[b]{0.35\linewidth}
\includegraphics[width=\linewidth]{figs/sm/mouth1.png}
\caption{Mouth}
\end{subfigure}
\caption{
\textbf{The Effect of Asyrp.} Results are shown for directions found with Alg.~\ref{alg:cap}.}
\label{SM:asyrp-plot1}
\end{figure}

% and classifier annotation respectively.
\begin{figure}[ht]
\centering
% \begin{subfigure}[b]{0.45\linewidth}
% \includegraphics[width=\linewidth]{figs/sm/eyes1.png}
% \caption{Eyes}
% \end{subfigure}
% \begin{subfigure}[b]{0.45\linewidth}
% \includegraphics[width=\linewidth]{figs/sm/mouth1.png}
% \caption{Mouth}
% \end{subfigure}

\begin{subfigure}[b]{0.35\linewidth}
\includegraphics[width=\linewidth]{figs/sm/age1.png}
\caption{Age}
\end{subfigure}
\begin{subfigure}[b]{0.35\linewidth}
\includegraphics[width=\linewidth]{figs/sm/rot1.png}
\caption{Rotation}
\end{subfigure}

\begin{subfigure}[b]{0.35\linewidth}
\includegraphics[width=\linewidth]{figs/sm/gender1.png}
\caption{Gender}
\end{subfigure}
\begin{subfigure}[b]{0.35\linewidth}
\includegraphics[width=\linewidth]{figs/sm/glasses1.png}
\caption{Glasses}
\end{subfigure}

\caption{
\textbf{The effect of Asyrp.} Results are shown for directions found using the supervised method presented in Sec.~\ref{sec:Supervised methods}.
}
\label{SM:asyrp-plot2}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\subsection{A Note on image-specific directions}\label{SM:noteonnoiseprediction}
In the main paper, we state that the right singular vectors of the Jacobian of $\bm{\epsilon}_t^\theta$ with respect to $h$-space, denoted as $\mathbf{J}_t$, are the set of orthogonal vectors in $h$-space which perturb the noise prediction $\bm{\epsilon}_t^\theta$ the most.
%
An equivalent statement is that those right singular vectors perturb the predicted image $\mathbf{P}_t(\mathbf{x}_t ,\mathbf{h}_t)$ at timestep $t$ the most. 
%
Specifically, since
\begin{equation}
\mathbf{P}_t(\mathbf{x}_t ,\mathbf{h}_t) =
\frac{\mathbf{x}_t - \sqrt{1-\alpha_t}}{\sqrt{\alpha_t} }
\bm{\epsilon}^\theta_t(\mathbf{x}_t,\mathbf{h}_t)
\end{equation}
 we have that 
\begin{align}
\frac{\partial}{\partial \mathbf{h}_t} \mathbf{P}_t(\mathbf{x}_t ,\mathbf{h}_t) 
&= -\frac{\sqrt{1-\alpha_t}}{\sqrt{\alpha_t}} \frac{\partial}{\partial \mathbf{h}_t} \\
\bm{\epsilon}^\theta_t(\mathbf{x}_t,\mathbf{h}_t)  &= -\frac{\sqrt{1-\alpha_t}}{\sqrt{\alpha_t}} \mathbf{J}_t.
% \frac{\mathbf{x}_t - \sqrt{1-\alpha_t}  }{\sqrt{\alpha_t}}
\end{align}
Thus, the eigenvectors  of $(\partial \mathbf{P}_t/\partial \mathbf{h}_t)^\mathrm{T}(\partial \mathbf{P}_t/\partial \mathbf{h}_t)$ and  $\mathbf{J}_t^\mathrm{T}\mathbf{J}_t$ are the same with the same ordering. 


\subsection{Image-specific directions at different timesteps} \label{SM:jacobiantimesteps}

Our proposed image-specific unsupervised method in Alg.~\ref{alg:cap} finds different directions for each timestep. 
In Figures \ref{SM:poweriter-seed199805}, \ref{SM:poweriter-seed445314}, \ref{SM:poweriter-seed655092} and \ref{SM:poweriter-seed825356} we show the effect of the three dominant directions (the three top singular vectors of the Jacobian) at different timesteps along the reverse diffusion process. 

\begin{figure*}[ht]
\centering
\includegraphics[width=0.95\linewidth]{figs/sm/poweriter-supplemental-seed199805_annotated.jpg}
\caption{\textbf{Directions found by Alg.~\ref{alg:cap}.}}
\label{SM:poweriter-seed199805}
\end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[width=0.95\linewidth]{figs/sm/poweriter-supplemental-seed445314_annotated.jpg}
\caption{\textbf{Directions found by Alg.~\ref{alg:cap}.}}
\label{SM:poweriter-seed445314}
\end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[width=0.95\linewidth]{figs/sm/poweriter-supplemental-seed655092_annotated.jpg}
\caption{\textbf{Directions found by Alg.~\ref{alg:cap}.}}
\label{SM:poweriter-seed655092}
\end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[width=0.95\linewidth]{figs/sm/poweriter-supplemental-seed825356_annotated.jpg}
\caption{\textbf{Directions found by Alg.~\ref{alg:cap}.}}
\label{SM:poweriter-seed825356}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\clearpage
\subsection{Sequential algorithm for Jacobian subspace iteration}\label{SM:jacobian}%~%

As mentioned in the main text, Alg.~\ref{alg:cap} can be memory intensive when calculating a large number of singular vectors in parallel. 
In cases where limited memory is available, we provide an alternative sequential version of our method in Alg.~\ref{alg:jacsubspace_sequential}. 
Here we calculate the singular values and vectors in mini-batches of size $ b $.  The value of $ b $ should be set according to the parallel computation capacity. For example, in the special case of $ b = 1 $, the algorithm computes the vectors one by one and will use small memory. Note that lowering the mini-batch size $b$ comes at the expense of longer running time.

% This is efficiently done by utilizing forward mode automatic differentiation.
% Further \eqref{eq:JacobianVectorProd} can be calculated in parallel for multiple vectors using the batched Jacobian-vector product \eg in Pytorch. 

\begin{algorithm}[ht]
\caption{Sequential Jacobian subspace iteration}\label{alg:jacsubspace_sequential}
\begin{algorithmic}
\Require function to differentiate $ \mathbf{f} : \mathbb{R}^{d_{\text{in}}} \to \mathbb{R}^{d_{\text{out}}}$, point at which to differentiate
$\mathbf{h} \in  \mathbb{R}^{d_{\text{in}}}$, initial guess $\mathbf{\Theta} \in  \mathbb{R}^{d_{\text{in}} \times k} $ [optional],
mini-batch size $ b<k $ 
\Ensure $ (\mathbf{U}, \mathbf{\Sigma}, \mathbf{V}^\mathrm{T}) $ -- $k$ top singular values and vectors of the Jacobian $ {\partial \mathbf{f} }/{ \partial \mathbf{h}}$
\State  \textbf{Initialization: } $
\mathbf{y} \gets \mathbf{f}(\mathbf{h}), \
i_{\text{start}} \gets 1, \
i_{\text{end}} \gets b, \
\mathbf{V} \gets [ \ ] , \
\mathbf{\Sigma} \gets [ \ ] , \
\mathbf{U} \gets [ \ ]  $
\While{$i_{\text{start}} \leq k$}
\If{$\mathbf{\Theta}$ is empty}
    \State $\mathbf{\Phi} \gets $ i.i.d.\@ standard Gaussian samples in $ \mathbb{R}^{d_{\text{in}}\times (i_{\text{end}}-i_{\text{start}}+1) } $
\Else
    \State $\mathbf{\Phi} \gets $ columns $i_{\text{start}}$ to $i_{\text{end}}$ of $\mathbf{\Theta}$
\EndIf
\State $  \mathbf{Q},\mathbf{R} \gets \mathrm{QR}(\mathbf{\Phi}) $
\Comment{Reduced QR decomposition}
\State $\mathbf{\Phi} \gets \mathbf{Q}$
\Comment{Ensures $ \mathbf{\Phi}^\mathrm{T} \mathbf{\Phi} = \mathbf{I} $}
\While{stopping criterion}
\If{$\mathbf{V}$ is not empty}
\State $\mathbf{\Phi} \gets \left[\mathbf{I} -  \mathbf{V}\left(\mathbf{V}^\mathrm{T}\mathbf{V}\right)^{-1} \mathbf{V}^\mathrm{T} \right] \mathbf{\Phi} $
\State $  \mathbf{\Phi},\mathbf{R} \gets \mathrm{QR}(\mathbf{\Phi}) $
\Comment{Reduced QR decomposition}
\EndIf
\State $\mathbf{\Psi} \gets \partial \mathbf{f} ( \mathbf{h}+a \mathbf{\Phi} ) / \partial a $ at $ a = 0$
\Comment{Batch forward}
\State $\hat{\mathbf{\Phi}} \gets \partial (\mathbf{\Psi}^\mathrm{T}\mathbf{ y })/\partial \mathbf{h}$
\State $\mathbf{\Phi},\mathbf{S}, \mathbf{R} \gets \mathrm{SVD}(\hat{\mathbf{\Phi}})$
\Comment{Reduced SVD}
\EndWhile
\State $\mathbf{V} \gets [\mathbf{V} ; \mathbf{\Phi}]$
\State\vspace*{-\baselineskip}
    \begin{fleqn}[\dimexpr(\leftmargini-\labelsep)]
        \setlength\belowdisplayskip{3pt}
        \setlength\abovedisplayskip{3pt}
        \begin{equation*}
            \mathbf{\Sigma} \gets
            \begin{bmatrix}
                \mathbf{\Sigma} & \mathbf{0} \\
                \mathbf{0}   &   \mathbf{S}^{1/2} 
            \end{bmatrix}
        \end{equation*}
    \end{fleqn}%
\State $\mathbf{U} \gets [\mathbf{U} ; \mathbf{\Psi}]$
\State  $i_{\text{start}} \gets i_{\text{start}}+b $
\State  $i_{\text{end}} \gets \min\{ i_{\text{end}}+b,k\} $

\EndWhile
\State Orthonormalize $\mathbf{U}$
%\State $\mathbf{\Sigma} \gets (\mathbf{\Sigma}^2)^{1/2} $


% \State $\mathbf{y} \gets \mathbf{f}(\mathbf{h})$ % , \;  a \gets 1  $
% \If{$\mathbf{V}$ is empty}
%     \State $\mathbf{V} \gets $ i.i.d.\@ standard Gaussian samples
% \EndIf
% \State $  \mathbf{Q},\mathbf{R} \gets \mathrm{QR}(\mathbf{V}) $
% \Comment{Reduced QR decomposition}
% \State $\mathbf{V} \gets \mathbf{Q}$
% \Comment{Ensures $ \mathbf{V}^\mathrm{T} \mathbf{V} = \mathbf{I} $}
% \While{stopping criteria}
% % \State $\mathbf{B} \gets \mathbf{h}-\mathbf{U} $ 
% % \Comment{$\mathbf{h}$ broadcasted}
% \State $\mathbf{U} \gets \partial \mathbf{f} ( \mathbf{h}+a \mathbf{V} ) / \partial a $ at $ a = 0$
% \Comment{Batch forward}
% \State $\hat{\mathbf{V}} \gets \partial (\mathbf{U}^\mathrm{T}\mathbf{ y })/\partial \mathbf{h}$
% \State $\mathbf{V},\mathbf{\Sigma^2}, \mathbf{R} \gets \mathrm{SVD}(\hat{\mathbf{V}})$
% \Comment{Reduced SVD}
\end{algorithmic}
\end{algorithm}


\clearpage
\subsection{Facial expressions from real data.}\label{SM:bu3dfe}
We conducted an additional experiment where domain-specific semantic directions were extracted using real images as supervision. We wish to find directions corresponding to expressions like happiness, sadness, and surprise. Here we used the BU3DFE data set~\cite{yin2006bu3dfe}.
BU3DFE contains real images of $100$ subjects, each performing a neutral expression in addition to each of the prototypical facial expressions at various intensity levels.
Using DDIM inversion ($\eta_t = 0$) we recorded $\mathbf{h}_{T:1}$ during the inversion process and used \eqref{eq:linear_direction} to calculate directions. 
We used the most intense expressions for the positive examples and the neutral expressions for the negative examples. 
The effect of the directions found using our method is shown in Fig.~\ref{fig:bu3dfe}. The extracted directions are shown on generated samples.  The figure shows that latent directions in $h$-space can successfully be found by applying our supervised method presented in Sec.~\ref{sec:supervised} on a dataset of real images.
\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{figs/sm/bu3dfe.png}
\caption{
\textbf{Facial expressions from real data.}
We extract semantic directions corresponding 
to different facial expressions using a data set of real images. 
The directions are calculated via DDIM inversion and applied in the semantic $h$-space to synthetic images.
}
\label{fig:bu3dfe}
\end{figure} %bu3dfe


\subsection{Broader impact}\label{sec:impact}

In this paper, we have introduced several techniques for semantic editing of human faces using DDMs. While the creation of high-quality edited images that are difficult to distinguish from real images has significant positive applications, there is also the potential for malicious or misleading use, such as in the creation of deepfakes. 
Although some research has focused on detecting and mitigating the risk of AI-edited images, these have mostly focused on GANs \cite{Wang2022GANgeneratedFD} and, so far, there has been little research into detecting images that have been edited using DDMs. Given the differences in the generative process between DDMs and GANs, methods which are effective in detecting images edited by GANs might not be as effective for images edited by DDMs \cite{meng2022sdedit}.
Further research is needed to develop effective methods for forensic analysis of edits using DDMs. 
Such research could help address the risk of malicious use of image-editing technologies.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsection{Unsupervised methods on other domains}
\label{SM:pca-lsun}

In addition to the model\footnote{\url{https://huggingface.co/google/ddpm-ema-celebahq-256}} trained on CelebA, which is used throughout the main paper, we also conducted experiments with models trained on 
churches\footnote{\url{https://huggingface.co/google/ddpm-ema-church-256}} 
and bedrooms\footnote{\url{https://huggingface.co/google/ddpm-ema-bedroom-256}}.
%
Although the unsupervised directions found with both PCA and Alg.~\ref{alg:cap}
on these models lead to various changes to the images, these directions are less interpretable than those obtained for faces in the main paper. 
We showcase the first $5$ PCA directions on the models trained on churches and bedrooms in Figures~\ref{SM:pca-church} and \ref{SM:pca-bedrooms} and directions found using 
Alg.~\ref{alg:cap} in Figures~\ref{SM:poweriter-churches} and \ref{SM:poweriter-bedrooms}.

\begin{figure*}[ht]
\centering
\includegraphics[width=0.75\textwidth]{figs/sm/pca-church.png}
\caption{
\textbf{PCA directions.}
For a DDM trained on churches.}
\label{SM:pca-church}
\end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[width=0.75\textwidth]{figs/sm/pca-bedrooms.png}
\caption{
\textbf{PCA directions.}
For a DDM trained on bedrooms.}
\label{SM:pca-bedrooms}
\end{figure*}


\begin{figure*}[ht]
\centering
\includegraphics[width=0.9\textwidth]{figs/sm/poweriter-supplemental-seed102952-bedrooms_annotated.jpg}
\caption{
\textbf{Directions found with Alg.~\ref{alg:cap}}.
For a DDM trained on bedrooms.}
\label{SM:poweriter-bedrooms}
\end{figure*}


\begin{figure*}[ht]
\centering
\includegraphics[width=0.9\textwidth]{figs/sm/poweriter-supplemental-seed217821-church_annotated.jpg}
\caption{
\textbf{Directions found with Alg.~\ref{alg:cap}}.
For a DDM trained on churches.}
\label{SM:poweriter-churches}
\end{figure*}



% \clearpage
% \subsection{Transferability of semantic directions}

% \begin{figure}[t]
% \centering
% \includegraphics[width=\linewidth]{figs/poweriter/poweriterfig3_annotated.png}
% \caption{
% \textbf{Unsupervised image-specific edits.}
% Spectral analysis of the Jacobian of $\bm{\epsilon}_t^\theta$ yields directions corresponding to localized changes in the generated image, \eg eyes opening/closing and raising of the eyebrows.
% Although this method is image-specific, directions found for one sample can be transferred to others, where they result in semantically similar edits. }
% \label{fig:poweriter}
% \end{figure}

% \begin{figure}[tb]
% \centering
% \includegraphics[width=\textwidth]{figs/pca/pixel-pca_test_all50steps250samples-eta1.jpg}
% \caption{Pixel CelebA PCA 50 inference steps 250 samples}
% \end{figure}




