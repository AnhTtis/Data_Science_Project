\section{Unsupervised semantic directions}\label{sec:Unsupervised methods}

\subsection{Global semantic directions}
 Our first goal is to uncover interesting global semantic directions in an unsupervised fashion. We use the term global to refer to directions that have the same semantic effect for all images. To this end, we explore the use of principal component analysis (PCA) in $h$-space.
In the context of GANs \cite{Harkonen2020GANSpace}, it was shown that the principal components of a collection of randomly sampled latent codes results in semantically interpretable editing direction. Here we demonstrate that the same is true for DDMs if the PCA is performed in the semantic $h$-space.

% Specifically, we consider PCA where we generate $n$ random samples and save the bottleneck activation $\mathbf{h}^{(i)}_t$ for each sample $i$ at all timesteps. 
% Then, for each timestep $t$ we vectorize $\{\mathbf{h}^{(i)}_t\}_{i=1}^n$ into a design matrix $\mathbf{H}_t$, from which we calculate the the principal components PCA $\{\mathbf{v}_{j}\}$. 


Specifically, we consider PCA where we generate $n$ random samples and save the bottleneck activation $\mathbf{h}^{(i)}_t$ for each sample $i$ at all timesteps. 
Then, for each timestep $t$ we vectorize $\{\mathbf{h}^{(i)}_t\}_{i=1}^n$ and calculate the principal components. 
We define the editing direction $\mathbf{v}_{j}$ as a concatenation of the $j$'th principal component from all timesteps.

%% Option 1
% Then, we concatenate the $j$'th principal component from all timestep, to define the direction $\mathbf{v}_{j}$ for all $j$
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/pca/PCA-Plot-new.png}
\caption{\textbf{Semantic directions unveiled by PCA.} 
PCA in $h$-space provides a way for discovering disentangled and semantically meaningful directions. Here we show edits corresponding to pose, smile, gender and age. }
\label{fig:pca}
\end{figure}

To demonstrate our method, we use Diffusers~\cite{von-platen-etal-2022-diffusers} and a DDPM\footnote{\url{https://huggingface.co/google/ddpm-ema-celebahq-256}} trained on the CelebA \cite{liu2015celeba} data set. 
%
Unless stated otherwise, all results are shown using $\eta_t= 1$ during the synthesis process. 
%
We observe that many principal directions have clear semantic interpretations, like yaw, rotation and gender.
In Fig.~\ref{fig:pca} we demonstrate the effect of several of these directions, including 
gender $(\mathbf{v}_{1})$, 
pose $(\mathbf{v}_{2})$, 
age $(\mathbf{v}_{4})$, 
and smile  $(\mathbf{v}_{10})$. 

%
Next, we compare the effect of applying the two dominant principal components to applying random directions in Fig.~\ref{fig:random_direction}. 
% Next, we compare the effect of applying the two dominant principal components to the effect of random directions in Fig.~\ref{fig:random_direction}. 
%
For a fair comparison, we set the norm of~$\Delta \mathbf{h}_t$ for the random directions to match that of the principal components. 
While interpolating along principal directions leads to semantically interpretable edits, shifting along random direction only induces minor changes to the image at small scales and rapid degradation of the image at larger scales. 




\begin{figure}[t]
\centering
\begin{subfigure}[b]{\linewidth}
\includegraphics[width=\linewidth]{figs/pca/top2pca_annotated.png}
\caption{PCA directions}
\end{subfigure}\\
\begin{subfigure}[b]{\linewidth}
\includegraphics[width=\linewidth]{figs/pca/random_direction.png}
\caption{Random directions}
\end{subfigure}
\caption{
\textbf{PCA vs.\@~random directions.}
While directions found with PCA have a clear semantic meaning, like pose and gender, interpolating along random directions results in only minor changes to the image when using the same scale. 
Increasing the scale results in a degradation of the image.}
\label{fig:random_direction}
\end{figure} % random directions

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/poweriter/poweriterfig3_annotated.png}
\caption{
\textbf{Unsupervised image-specific edits.}
Spectral analysis of the Jacobian of $\bm{\epsilon}_t^\theta$ yields directions corresponding to localized changes in the generated image, \eg eyes opening/closing and raising of the eyebrows.
Although this method is image-specific,
%different directions at different timesteps depending on the input image, \ie the directions are . Yet, 
directions found for one sample can be transferred to others, where they result in semantically similar edits. }
\label{fig:poweriter}
\end{figure*}

\subsection{Discovering image-specific semantic edits}
% ## steps for power itteration method 
% # (1) sps matrix A  = J.T @ J  
% # (2) v_hat = Av
% # (3) v = v_hat / norm(vhat)
% # (4) Av = J.T @ J v 
% # (5) d/dh < y, h> = Jv = q
% # define h = z + aq (where a is some scaler and z is the diff btw h and a*q)
% # now d/da f(z + aq) = d/da f(h)  = J.T @ q

%Motivation. 
Global directions have the advantage that they are computed based on many samples and are thus applicable to any image. However, many desirable semantic edits are not applicable to all images. 
For example, a direction that opens or closes eyes is clearly irrelevant for a person wearing dark sunglasses. Therefore, we now focus on finding image-specific semantic directions.

% What we do - approach. 
% To this end, we seek a set of orthogonal directions in $h$-space that induce the largest changes at the output of the noise-prediction network during the generation of a particular image. 
To this end, we seek a set of orthogonal directions in $h$-space that induce the largest change in the predicted clean image $\mathbf{P}_t(\bm{\epsilon}^\theta_t( \mathbf{x}_t ))$ at every timestep. 
%
This is equivalent to finding the directions that change $\bm{\epsilon}^\theta_t( \mathbf{x}_t )$ the most (SM 
Sec.~\ref{SM:noteonnoiseprediction}).
%
For small perturbations, these directions are the top right-hand singular vectors of the Jacobian of $\bm{\epsilon}^\theta_t$ with respect to $\mathbf{h}_{t}$. 
%
% Note that due to the skip-connections in the U-Net, the output of the network depends on both $\mathbf{x}_{t}$ and $\mathbf{h}_{t}$. However, here we only consider the dependency on the latent variable $\mathbf{h}_{t}$.
Due to the skip-connections in the U-Net, the output of the network depends on both $\mathbf{x}_{t}$ and $\mathbf{h}_{t}$. Yet, here we only consider the dependency on the latent variable $\mathbf{h}_{t}$.


Let $\mathbf{J}_t$ denote the Jacobian of $\bm{\epsilon}^\theta_t$ w.r.t.\@~$\mathbf{h}_t$, \ie
\begin{equation}
\mathbf{J}_t \triangleq \frac{\partial \bm{\epsilon}^\theta_t(\mathbf{x}_t, \mathbf{h}_t)}{\partial\mathbf{h}_t}, 
\end{equation}
and denote its singular value decomposition (SVD) by
\begin{equation}
    \mathbf{J}_t = \mathbf{U}_t\bm{\Sigma}_t \mathbf{V}^\mathrm{T}_t.
\end{equation}
Then the right singular vectors ($\mathbf{V}_t$'s columns) are the set of orthogonal vectors in $h$-space which perturb the predicted image the most.
%
Note that for each timestep $t$, we have a different set of directions.  
In practice, we find that semantically interesting effects are obtained by applying directions found at timestep $t$ across all timesteps. Thus, computing~$k$ directions per timestep provides us $kT$ potential edits corresponding to the~$k$ directions found in each of the~$T$ timesteps. In SM Sec.~\ref{SM:jacobiantimesteps} we illustrate the qualitative difference between directions computed at different timesteps.  

% The computation trick
In practice, calculating $\mathbf{J}_t$ directly is very computationally expensive. Instead, we find the dominant singular vectors by power-iteration over the matrix $\mathbf{J}_t^\mathrm{T} \mathbf{J}_t$, whose eigenvectors are precisely the right singular vectors of $\mathbf{J}_t$. 
%
Each iteration requires multiplication by $\mathbf{J}_t^\mathrm{T} \mathbf{J}_t$, which can be computed without ever storing the Jacobian matrix in memory. Specifically, for any vector $\mathbf{v}$, the Jacobian-vector product $\mathbf{J}_t\mathbf{v}$ can be computed as the derivative
\begin{equation}\label{eq:poweriter_trick}
    \mathbf{J}_t\mathbf{v} = \left.\frac{\partial}{\partial a} \bm{\epsilon}^\theta_t(\mathbf{x}_t,\mathbf{h}_t  + a \mathbf{v})\right|_{a=0}.
\end{equation}
The product $\mathbf{J}_t^\mathrm{T}\mathbf{J}_t\mathbf{v}$ can be computed as the gradient
\begin{equation}\label{eq:JacobianVectorProd}
    \mathbf{J}_t^\mathrm{T}\mathbf{J}_t\mathbf{v} = \frac{\partial}{\partial\mathbf{h}_t}
    \left\langle \bm{\epsilon}^\theta_t(\mathbf{x}_t,\mathbf{h}_t) ,\mathbf{J}_t\mathbf{v} \right \rangle.
\end{equation}


\begin{algorithm}[t]
\caption{Jacobian subspace iteration}\label{alg:cap}
\begin{algorithmic}
\Require $ \mathbf{f} : \mathbb{R}^{d_{\text{in}}} \to \mathbb{R}^{d_{\text{out}}} $, $ \mathbf{h} \in  \mathbb{R}^{d_{\text{in}}} $ and $ \mathbf{V} \in  \mathbb{R}^{d_{\text{in}} \times k} $ 
\Ensure $ (\mathbf{U}, \mathbf{\Sigma}, \mathbf{V}^\mathrm{T}) $ -- $k$ top singular values and vectors of the Jacobian $ {\partial \mathbf{f} }/{ \partial \mathbf{h}}$
\State $\mathbf{y} \gets \mathbf{f}(\mathbf{h})$ % , \;  a \gets 1  $
\If{$\mathbf{V}$ is empty}
    \State $\mathbf{V} \gets $ i.i.d.\@ standard Gaussian samples
\EndIf
\State $  \mathbf{Q},\mathbf{R} \gets \mathrm{QR}(\mathbf{V}) $
\Comment{Reduced QR decomposition}
\State $\mathbf{V} \gets \mathbf{Q}$
\Comment{Ensures $ \mathbf{V}^\mathrm{T} \mathbf{V} = \mathbf{I} $}
\While{stopping criteria}
% \State $\mathbf{B} \gets \mathbf{h}-\mathbf{U} $ 
% \Comment{$\mathbf{h}$ broadcasted}
\State $\mathbf{U} \gets \partial \mathbf{f} ( \mathbf{h}+a \mathbf{V} ) / \partial a $ at $ a = 0$
\Comment{Batch forward}
\State $\hat{\mathbf{V}} \gets \partial (\mathbf{U}^\mathrm{T}\mathbf{ y })/\partial \mathbf{h}$
\State $\mathbf{V},\mathbf{\Sigma^2}, \mathbf{R} \gets \mathrm{SVD}(\hat{\mathbf{V}})$
\Comment{Reduced SVD}
\EndWhile
\State Orthonormalize $\mathbf{U}$
\end{algorithmic}
\end{algorithm}

The power method starts by randomly initializing a set of vectors $\{\mathbf{v}_i \}_{i=1}^k$  and iteratively computes~\eqref{eq:poweriter_trick} and~\eqref{eq:JacobianVectorProd} using automatic differentiation, while enforcing orthogonality among them. This is summarized in Alg.~\ref{alg:cap}. 
Importantly, it was shown that batched power iteration with orthogonalization step, such as presented here, is guaranteed to converge to the SVD of positive semi-definite matrices \cite[Ch.~5]{saad2011numerical}. %  

Our proposed method successfully identifies semantically meaningful directions that correspond to highly localized semantic changes in the image, such as closing or opening of the eyes and mouth, or raising of the eyebrows. We show a selection of such localized edits at the top of Fig.~\ref{fig:poweriter}.
%
While the semantic directions found by this method are image-specific and may vary depending on the sample analyzed, we find that they result in the same localized changes when applied across different images. This is illustrated at the lower part of Fig.~\ref{fig:poweriter} where each of the found editing directions are applied with the same magnitude $\gamma$ across a selection of samples. These results suggest that our approach is effective in identifying meaningful semantic directions that generalize across different images.

% Few words are in place regarding implementation.
Regarding implementation, in \eqref{eq:poweriter_trick} we compute a derivative of high dimensional output w.r.t.\@ a scalar. 
This is efficiently done by utilizing forward mode automatic differentiation.
Further, \eqref{eq:JacobianVectorProd} can be calculated in parallel for multiple vectors using the batched Jacobian-vector product, \eg in Pytorch. 
However, parallel calculation of a large number of vectors can be memory intensive. 
For such cases, we give a sequential variant of Alg.\ref{alg:cap} in SM, Sec.~\ref{SM:jacobian}.

%\paragraph{Implementation details.}
%Our implementation relies on Diffusers \cite{von-platen-etal-2022-diffusers} and we use a pretrained DDPM trained by Google\footnote{\url{https://huggingface.co/google/ddpm-ema-celebahq-256}}  on the CelebA \cite{liu2015celeba} data set. Note that the weights of model checkpoint are kept frozen and not modified in any way. 
%We implement $h$-space by simply adding $\Delta\mathbf{h}$ to the feature map at the smallest scale, that is directly after the activation of the middle {\tt UNetMidBlock2D} layer.
% \red{
% and LSUN \cite{fisher2015lsun} 
% church\footnote{\url{https://huggingface.co/google/ddpm-ema-church-256}} 
% and bedrooms\footnote{\url{https://huggingface.co/google/ddpm-ema-bedroom-256}} 
% data sets.}
% \blue{
% % which is further mean-centered before.
% Due to the high dimensionality  of $\mathbf{H}$ we calculate the PCA via the eigenvalue decomposition of the kernel matrix $\mathbf{H}_t \mathbf{H}^\mathrm{T}_t =  \mathbf{V}_t \bm{\Lambda} \mathbf{V}^\mathrm{T}_t \in \mathbb{R}^{N\times N}$ and proceed to calculate the principal components as $\mathbf{V}_t^\mathrm{T}
% \mathbf{H}_t/\sqrt{N-1} = \bm{\Lambda}^{1/2} \mathbf{U}_t^\mathrm{T} /\sqrt{N-1}$.
% }

% We show that image-specific directions emerge as the singular vectors of the Jacobian of the noise predictions.
% Our method successfully finds directions corresponding to highly localized semantic changes in the image. For example closing or opening of the eyes, raising of the eyebrows 

% Although the found directions  differs depending on the particular sample our method is run on. The found semantic directions are consistent across samples. We demonstrate this in \ref{fig:poweriter} where we apply highly localised edits, such as closing of the eyes or pulling the right part of the mouth on a selection of samples. 


% We do this by finding the eigenvectors of $\mathbf{J}^\mathrm{T}\mathbf{J}\mathbf{v}$ by the power iteration method.
% We can find the dominant singular vectors of any matrix \red{symmetric matrix} $\mathbf{A}\in \mathbb{R}^{n\times m}$ 
% The under iteration of $\tilde{\mathbf{v}}_{k+1} =\mathbf{A}\mathbf{v}_k$ and $\mathbf{v}_k = \tilde{\mathbf{v}}_k/||\tilde{\mathbf{v}}_k|| $  then 
% $\mathbf{v}_k$ converges to the dominant singular vector of $\mathbf{A}$.  

%Let $\mathbf{u}_{it}$ be the $i$th column of $\mathbf{V}_t$, \ie the $i$th right hand singular vector of $\mathbf{J}_t$. We note that the singular vectors are orthogonal at every timestep $t$ $\langle \mathbf{u}_{it} , \mathbf{u}_{i^\prime t^\prime} \rangle = \delta_{i i \prime}$ if $t =  t^\prime$ but not necessarily otherwise.
%%
%We propose to add the direction globally by repeating $\mathbf{u}_{it}$ corresponding to the singular vectors at a particular timestep $t = t^\prime$ across each perturbation of the noise prediction $\bm{\epsilon}^\theta_t(\mathbf{x}_t| \gamma \mathbf{n}_{it^{\prime}}) $ where  $\gamma$ is a scalar parameter controlling the strength of the edit and $\mathbf{n}_{it^{\prime}}$ is the $i$th right hand singular vector of $\mathbf{V}_t$ at timestep $t=t^\prime$.
% repeated across timesteps
% $\mathbf{n}_{it^{\prime}} = \otimes_T^1 \mathbf{u}_{it^{\prime}}$ and .