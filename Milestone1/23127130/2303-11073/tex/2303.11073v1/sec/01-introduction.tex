% \vspace{-10pt}
\section{Introduction}

% DPMs outperform GANs but their latent space is not well understood
Denoising Diffusion Models (DDMs) \cite{sohl2015ddpm-noneqthermo} have emerged as a strong alternative to Generative Adversarial Networks (GANs) \cite{Goodfellow2014GAN}. Today, they outperform GANs in unconditional image synthesis \cite{Dhariwal2021dpmsbeatgans}, a task in which GANs have been dominating in recent years. Besides synthesizing high-quality and diverse images, unconditional DDMs are also being used for conditional synthesis tasks by guiding them on various user inputs \cite{ho2021classifierfree}, such as a user-provided reference image~\cite{Gihyun22, meng2022sdedit} or a text-prompt by utilizing Contrastive Language-Image Pretraining (CLIP) \cite{Radford2021CLIP}. 
Conditional DDMs have also seen great success, particularly in the context of text-based synthesis. Specifically, recent large-scale text-conditional systems like DALL-E \cite{ramesh2021dalle,ramesh2022dalle2}, Stable Diffusion \cite{rombach2021latentdiffusion} and Imagen \cite{Saharia2022Imagen} have sparked a surge of research related to text-driven image editing using DDMs \cite{nichol2021glide, mokady2022null, gal2022textual, ruiz2022dreambooth, kawar2022imagic, kim2022DiffusionCLIP, Hertz22,Narek22,Guillaume22}. 
However, despite their popularity, it is still not well understood how to leverage the latent space of DDMs for semantic image editing in the unconditional setting, \ie in the absence of CLIP-guidance and without conditioning on a reference image.
% Editing in the latent space of GANs is well explored - in DDMs it is not.
While there has been extensive research on finding disentangled editing directions in the latent space of unconditional GANs \cite{alaluf2023third, Shen2020Interfacegan,Harkonen2020GANSpace, Haas2022tensorGAN2,shen2021closedform, spingarn2021gansteerability,  Abdal2020Image2StyleGANpp}, comparatively little work has been done on this topic for unconditional DDMs. 

% Recently, Kwon \etal~\cite{Kwon2022ddmhavesemantic} proposed a semantic latent space for DDMs, coined `$h$-space'.
% This space is comprised of the bottleneck activations in the DDM's denoiser across all timesteps of the diffusion process.
% They show that it facilitates semantic image editing in a way reminiscent of GANs.
% , \ie it obeys vector arithmetic properties.


%% Our paper  
Recently, Kwon \etal~\cite{Kwon2022ddmhavesemantic} proposed a semantic latent space for DDMs, coined `$h$-space'. In this paper, we leverage the editing capabilities of $h$-space and explore several supervised and unsupervised methods for finding interpretable editing directions in unconditional DDMs. 

%% Unsupervised
We start by proposing two unsupervised methods. In Sec.~\ref{sec:Unsupervised methods} we demonstrate that interpretable editing directions, like pose, gender, and age emerge as the principal components in the semantic latent space. Additionally, we propose a novel unsupervised method for discovering image-specific semantic directions resulting in highly localized edits like opening/closing of the mouth and eyes that can be applied to other samples. We illustrate a selection of these unsupervised editing directions in Fig.~\ref{fig:teaser figure} (right pane). To the extent of our knowledge, we are the first to show that semantically meaningful editing directions can be found in unconditional DDMs in a fully unsupervised fashion.

% Supervised directions. 
Next, in Sec.~\ref{sec:Supervised methods} we utilize the linear properties of the semantic latent space and propose a simple supervised method for finding interpretable editing directions, like age and gender or the appearance of glasses or a smile. We illustrate examples of these edits in Fig.~\ref{fig:teaser figure} (left pane). We demonstrate our approach both by using a facial expression data set of real images and by annotating samples generated by the model with a pretrained attribute classifier. We further propose a simple method for disentangling directions that affect multiple attributes. 

 % Why our approach is attractive, main punchlines.
Our approaches allow for intuitive and semantically disentangled image editing and can be applied to the latent space of DDMs without requiring any CLIP guidance, fine-tuning, optimization, user-provided reference images or any adaptations to the architecture of existing DDMs. 


% The main contributions of this paper can be summarized as follows.

% %%% Option 1

% We first show that principal components in $h$-space are semantically meaningful for unconditional DDMs trained on a single domain. Secondly we propose a novel unsupervised method for finding image-specific editing directions using the singular vectors of the Jacobian of the denoising network.
% Finally, we propose a supervosed method for finding semantic directions using either classifier annotation or real-world data and further exploit the vector arithmetic properties of $h$-space to suggest a simple method for disentangling otherwise entangled editing directions.

%%% Option 2

% \begin{itemize}[noitemsep]
% \item We show that the principal components in $h$-space are semantically meaningful for unconditional DDMs trained on a single domain. 
% \item We propose a novel unsupervised method for finding image-specific editing directions using the singular vectors of the Jacobian of the denoising network.
% \item We propose to find semantic directions using either classifier annotation or real-world data and further exploit the vector arithmetic properties of $h$-space to suggest a simple method for disentangling otherwise entangled editing directions.
% \end{itemize}




% Further our unsupervised approaches successfully finds disentangled directions without the need for optimization based on CLIP text-embedding  ~\cite{Kwon2022ddmhavesemantic} or any user-specified semantic mask as~\cite{meng2022sdedit}.
% Summary of contributions

% All of these works use some multimodal text and image model, eg. CLIP \cite{Radford2021CLIP} to guide the synthesis process according to a user-specified text prompt using classifier free guidance \cite{ho2021classifierfree}. 
% Semantic editing has been studied extensively over GANs, including finding disentangled directions via classifier supervision \cite{Shen2020Interfacegan} or using various factorization procedures \cite{Harkonen2020GANSpace, Haas2022tensorGAN2, shen2021closedform, spingarn2021gansteerability}. 

% \item By leveraging the recently proposed $h$ space, we show that linear semantic directions can be found using classifier annotation and that  $h$-space allows for conditional manipulation allowing to disentangle, otherwise entangled semantics. 
% \item We propose techniques to discover interpretable linear semantic directions in diffusion models by leveraging the recently proposed $h$ space and show that DPMs naturally learns a rich set of semantics.
% \paragraph{\blue{Comparison  to GANS\\}}
% Generative Adversarial Networks (GANs) \cite{Goodfellow2014GAN} have shown impressive performance in tasks such at image generation \cite{karras2018pggan, Karras2019StyleGAN,Karras2020StyleGAN2,Karras2020StyleGANada,Karras2021StyleGAN3}, and semantic image editing \cite{Harkonen2020GANSpace, Haas2022tensorGAN2} 
% however they require GAN inversion \cite{Tov2021e4e, alaluf2021hyperstyle} for applications involving semantic editing of real images. 
% \cite{Sauer2023styleganT}
% One limitation of GANs, is that they require highly stuctured domains.
% Some work have focused on training large scale generators on unstructured domains \cite{Brock2019BigGAN, Sauer2022StyleganXL}
% Recently Denoising Diffusion models have been shown to generalize well when trained on large highly diverse datasets
% % Further it is possible to train DDMs on large unstructured datasets. 