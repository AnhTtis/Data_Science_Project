\section{Supervised discovery of semantic directions}\label{sec:Supervised methods}
%%% Motivation 
%%% The unsupervised method are great but require manual inspection.
%%% in some situaltion we want a specific direction like "glasses" or "surprice" (and for some reason we do not like CLIP)
% We validate our method both using a pretrained attribute classifier or a real facial expression data set. 
While the methods we presented in Sec.~\ref{sec:Unsupervised methods} discover interpretable semantic directions in a fully unsupervised fashion, their effects must be interpreted manually. 
In this section, we demonstrate a simple supervised approach to obtain latent directions corresponding to well-defined labels.


\paragraph{Linear semantic directions from examples.}
The vector arithmetic property of $h$-space suggests an intuitive method for discovering semantically meaningful directions, by providing positive and negative examples of a desired attribute. 
Let $\{(\mathbf{x}_i^-,\mathbf{x}_i^+ )\}_{i=1}^n$ be a collection of generated images, such that all $\mathbf{x}_i^+$ have a desired attribute that is absent in~$\mathbf{x}_i^-$, \eg a smile, old age, glasses, \etc. 
Let~$\mathbf{q}_i^-$ and $\mathbf{q}_i^+$ denote the latent representation corresponding to the images~$\mathbf{x}_i^-$ and~$\mathbf{x}_i^+$. Then, we can find a semantic direction~$\mathbf{v}$ as 
\begin{equation}\label{eq:linear_direction}
    \mathbf{v} = \frac{1}{n} \sum_{i=1}^n\left(\mathbf{q}_i^+-\mathbf{q}_i^-\right).
\end{equation}

% Effect of editing in the DDIM Noise space vs $h$-space
Note that this method can be applied using either $\mathbf{h}_{T:1}$ or $\mathbf{x}_T$ as the latent variable. 
Here we show that defining semantic directions using $\mathbf{h}_{T:1}$ as the latent variable requires far fewer samples than using $\mathbf{x}_T$.
Figure~\ref{fig:h_vs_ddim_numsamples} illustrates this for DDIM ($\eta_t = 0$) for a direction corresponding to smile where \eqref{eq:linear_direction} is calculated using a varying number of samples.


% in any latent space with~$ \mathbf{q} $ taken from the corresponding domain. 
% Specifically, when considering the noise space of DDMs, $ \mathbf{q} $ is~$\{\mathbf{x}_T, \mathbf{z}_{T:1}\} $.
% with noise space, 
% showcasing that $h$-space is indeed a highly semantic linear space. 

% $\mathbf{x}_T$ as the latent variable, using $\mathbf{h}_{T:1}$ requires far fewer samples.
% Although semantic editing directions can be defined using $\mathbf{x}_T$ as the latent variable, using $\mathbf{h}_{T:1}$ requires far fewer samples.
% Here the latent representation $\mathbf{q}$ is either $\mathbf{x}_T$ for DDIM 
% or $\mathbf{h}_{T:1}$ in $h$-space. 
% In both cases, 
% This equation goes b
% Although semantic editing directions can be defined using $\mathbf{x}_T$ as the latent variable, using $\mathbf{h}_{T:1}$ requires far fewer samples. 
%  we compare applying a semantic direction using the two latent representations where \eqref{eq:linear_direction} is calculated based on a different number of samples. 


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/anycost/h-vs-ddim-numsamples.png}
\caption{\textbf{Editing in $h$-space vs.\@ using $\mathbf{x}_T$.} 
We qualitatively compare the effect of editing using $\mathbf{x}_T$ (top) and $\mathbf{h}_{T:1}$ (bottom) as the latent variables using a smiling direction found by \eqref{eq:linear_direction}.
While the direction in $h$-space converges with a few labeled examples, more than $200$ are required to achieve  a similar result using $\mathbf{x}_T$ as the latent variable.
}
\label{fig:h_vs_ddim_numsamples}
\end{figure} % h_vs_ddim_numsamples


\begin{figure}[t] 
\centering
\begin{subfigure}[b]{0.45\linewidth}
\includegraphics[width=\textwidth]{figs/anycost/yaw2.png}

\caption{Yaw}
\end{subfigure}
\begin{subfigure}[b]{0.45\linewidth}
\includegraphics[width=\textwidth]{figs/anycost/smiling2.png}
\caption{Smile}
\end{subfigure}
\begin{subfigure}[b]{0.45\linewidth}
\includegraphics[width=\textwidth]{figs/anycost/gender2.png}
\caption{Gender}
\end{subfigure}
\begin{subfigure}[b]{0.45\linewidth}

\includegraphics[width=\textwidth]{figs/anycost/pitch2.png}
\caption{Pitch}
\end{subfigure}
\begin{subfigure}[b]{0.45\linewidth}
\includegraphics[width=\textwidth]{figs/anycost/glasses2.png}
\caption{Glasses}
\end{subfigure}
\begin{subfigure}[b]{0.45\linewidth}
\includegraphics[width=\textwidth]{figs/anycost/age2.png}
\caption{Age}
\end{subfigure}
\caption{
\textbf{Single attribute manipulation.}
 Using a domain-specific binary attribute classifier, we find linear directions in $h$-space corresponding to
a variety of semantic edits.
% changes in a wide range of image semantics.
 }
\label{fig:single_attr}
\end{figure} % single attr

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/anycost/disentanglement_combined.png}
\caption{
\textbf{Disentanglement of semantic directions.}
Given a direction that is entangled with other attributes, we can create a disentangled direction by removing the projection onto undesired semantics. 
The top row shows the original direction, whereas the bottom row shows the disentangled direction.
}
\label{fig:conditional_anycost}
\end{figure*} % conditional_anycost

\paragraph{Classifier annotation.}
We illustrate how to find linear semantic directions by using pretrained attribute classifiers to annotate samples generated by the model. We use the attribute classifier from \cite{lin2021anycost} to annotate samples with probabilities corresponding to the $40$ classes from CelebA \cite{liu2015celeba}, and use Hopenet \cite{Ruiz2018hopenet} to predict pose (yaw, pitch and roll).
We annotate samples, sort them according to the attribute scores, select the top and bottom samples from each class and calculate a semantic direction by \eqref{eq:linear_direction}.

%% Attribute manipulation 
As shown in Fig.~\ref{fig:single_attr}, we can successfully find semantic directions controlling a wide selection of meaningful attributes like smile, gender and glasses, as well as yaw and pitch.
%% Sequntial Manipulation
Furthermore, directions calculated by \eqref{eq:linear_direction} can be applied in combination with one another. 
For example, adding~$\Delta \mathbf{h}_{T:1}$ for two attributes, like pose and smile, results in an image where both attributes are changed. 
Figure~\ref{fig:anycost_sequential} illustrates sequential editing, showcasing changes in expression followed by pose, age and eyeglasses for two samples.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/anycost/sequential.png}
\caption{
\textbf{Sequential manipulation.}
Directions found with our method can be applied in combination with one another. Here, we sequentially accumulate four effects, starting from a single effect in the second column up to four effects in the fifth column.}
\label{fig:anycost_sequential}
\end{figure} % anycost_sequential



\begin{table}[t]
\centering
\begin{tabular}{l|ccccc}  
\diagbox[width=5em]{Edit}{Effect} &\rotatebox{45}{Smile}&\rotatebox{45}{Glasses}&\rotatebox{45}{Age}&\rotatebox{45}{Gender}&\rotatebox{45}{Hat}\\
\hline
 & \multicolumn{5}{c}{Original directions} \\
\hline
Smile   &0.26 &0.28 &0.07 &\textbf{0.33} &0.06\\
Glasses &0.47 &0.31 &\textbf{0.71} &0.65 &0.15\\
Age     &0.06 &0.41 &\textbf{0.77} &0.65 &0.16\\
Gender  &0.31 &0.01 &0.37 &\textbf{0.64} &0.22\\
Hat     &0.42 &0.00 &0.38 &\textbf{0.64} &0.41\\
\hline
 & \multicolumn{5}{c}{Disentangled directions} \\
\hline
Smile   &\textbf{0.24} &0.18 &0.03  &0.07 &0.03\\
Glasses &0.20 &\textbf{0.36} &0.12  &0.09 &0.19\\
Age     &0.01 &0.39 &\textbf{0.61} &0.15 &0.03\\
Gender  &0.19 &0.02 &0.06 & \textbf{0.39} & 0.06\\
Hat     &0.13 &0.03 &0.02 &0.09 &\textbf{0.43}\\
\end{tabular}
\caption{\textbf{Evaluation of disentanglement strategy.}
% 
We quantitatively evaluate the effect of disentangling semantic directions using linear projection. 
The rows correspond to the applied directions, while the columns correspond to the effect of the edits according to CLIP.
The strongest effect in each row is highlighted.}
\label{tab:disentanglement}
\end{table} %disentanglement


\paragraph{Disentanglement of semantic directions.}
Latent directions found by \eqref{eq:linear_direction} might be semantically entangled, in the sense that editing in the direction corresponding to some desired attribute might also induce a change in some other undesired attributes. For example, a direction for eyeglasses may also affect the age if it correlates with eyeglasses in the training data. 
% Conditional manipulation
To remedy this, we propose conditional manipulation in $h$-space in a way similar to what was suggested in the context of GANs by Shen \etal~\cite{Shen2020Interfacegan,Shen2020InterfaceganTPAMI}. Let $\mathbf{v}_1$ and $\mathbf{v}_2$ be two linear semantic directions, where the two corresponding semantic attributes are entangled. We can define a new direction $\mathbf{v}_{1\perp 2}$ which only affects the semantics associated with $\mathbf{v}_1$, without changing the semantics associated with $\mathbf{v}_2$. This is done simply by removing from $\mathbf{v}_1$ the projection of $\mathbf{v}_1$ onto $\mathbf{v}_2$, namely $\mathbf{v}_{1\perp 2} = \mathbf{v}_1 - \langle \mathbf{v}_1  , \mathbf{v}_2 \rangle /  \| \mathbf{v}_2\|^2   \mathbf{v}_2$.
%% Entangeling multiple semantics
In case of conditioning on multiple semantics simultaneously, our aim is to remove the effects of a collection of $k$ directions $\{\mathbf{v}_i\}_{i=1}^k$ from a primal direction $\mathbf{v}_0$. This can be done by constructing the matrix $\mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, \cdots, \mathbf{v}_k]$ and projecting $\mathbf{v}_0$ onto the orthogonal complement of the column space of $\mathbf{V}$ by
\begin{equation}\label{eq:condition_multiple_semantics}
\mathbf{v} =  \left[\mathbf{I} -  \mathbf{V}\left(\mathbf{V}^\mathrm{T}\mathbf{V}\right)^{-1} \mathbf{V}^\mathrm{T} \right] \mathbf{v}_0.
\end{equation}  
The resulting direction will be disentangled from each of the directions $\{\mathbf{v}_i\}$, meaning that moving a sample along this new direction will result in a large change in the attribute associated with $\mathbf{v}_0$ while minimally affecting the attributes associated with the other directions. 
%% Conditional manipulation
%In some cases, dataset biases can cause the directions we find to be entangled with one another. For example, we observe that the direction for ``glasses'' is entangled with ``age'', which may be explained by the fact that these attributes are correlated in the training data. 
Figure~\ref{fig:conditional_anycost} visualize the effect of interpolating in the directions of age and eyeglasses for two samples. As can be seen, these directions are entangled with gender and age, respectively. By using our method we can successfully remove the entanglement. 


%%% Comments on the CLIP experiment in the Table
To validate the effectiveness of our disentanglement strategy, we performed an experiment where we edited attributes corresponding to smile, glasses, age, gender, and wearing a hat.
We edited samples using both the original and the disentangled directions, while measuring the effect of each edit using CLIP~\cite{Radford2021CLIP} as a zero-shot classifier. 
We selected appropriate positive and negative prompts for each attribute. For smiling, glasses, and hat we used {\tt "A smiling person"}, {\tt "A person wearing glasses"} and {\tt "A person wearing a hat"} for the positive prompts respectively, and {\tt "A person"} as the negative prompt. 
For age and gender, we used {\tt "A man"} / {\tt "A woman"} and   {\tt "An old person" } /  {\tt "A young person"} respectively.
We edited each attribute in $1000$ samples and measured the score change according to CLIP for all the attributes. 
Table~\ref{tab:disentanglement} shows the results. 
We can see that the original directions are highly entangled with other attributes while the disentangled directions induce the largest changes in the intended attributes. This demonstrates that semantic directions can be disentangled by a simple linear projection. 

\paragraph{Facial expressions from real data.}
We further show that domain-specific semantic directions can be extracted using real images as supervision. Here we use the BU3DFE data set~\cite{yin2006bu3dfe}.
BU3DFE contains real images of $100$ subjects, each performing a neutral expression in addition to each of the prototypical facial expressions at various intensity levels.
Using DDIM inversion ($\eta_t = 0$) we record $\mathbf{h}_{T:1}$ during the inversion process and use \eqref{eq:linear_direction} to calculate directions. 
We use the most intense expressions for the positive examples and the neutral expressions for the negative examples. 
In Fig.~\ref{fig:bu3dfe} we show the effect of directions found using our method on generated samples, demonstrating that we can successfully define latent directions in $h$-space by using a data set of real images as supervision.
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/bu3dfe/bu3dfe.png}
\caption{
\textbf{Facial expressions from real data.}
We extract semantic directions corresponding 
to different facial expressions using a data set of real images. 
The directions are calculated via DDIM inversion and applied in the semantic $h$-space to synthetic images.}
\label{fig:bu3dfe}
\end{figure} %bu3dfe


