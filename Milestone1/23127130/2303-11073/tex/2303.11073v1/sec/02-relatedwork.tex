\section{Related work}

\subsection{The latent space of diffusion models}
%% Semantic space is well understood in GANs but not in DDMs
GANs have a well-defined latent space suitable for semantic editing. Whether DDMs possess such a convenient latent space is still a topic of ongoing research. Here we review two approaches for defining a latent space in DDMs.

%% DDIM view of the latent space
In DDIM~\cite{song2020ddim}, the generative process is a deterministic mapping from a Gaussian noise vector~$\mathbf{x}_T\sim \mathcal{N}(\mathbf{0},\mathbf{I})$ to a sampled image~$\mathbf{x}_0$. 
Here, Song \etal.~\cite{song2020ddim} regard the noise~$\mathbf{x}_T$ as the latent representation.
DDIM has the property that fixing~$\mathbf{x}_T$ leads to images with similar high-level features irrespective of the length of the generative process.
Furthermore, interpolating between two latent codes $\mathbf{x}_{T}^{(1)}$ and~$\mathbf{x}_{T}^{(2)}$ leads to images that vary smoothly between the two corresponding endpoint images, $\mathbf{x}_{0}^{(1)}$ and~$\mathbf{x}_{0}^{(2)}$.

% The DPM-encoder perspective
% \blue{
% For the stochastic DDPM~\cite{ho2020denoising} sampling scheme, Wu \etal.~\cite{Wu2022DPMEncoder} define the latent code as the collection $\{\mathbf{x}_T, \mathbf{z}_T, \mathbf{z}_{T-1} ,\ldots,\mathbf{z}_{1}\} $ where $\mathbf{z}_t$ is the noise added at step $t$ of the generative process. 
% They further propose an encoding algorithm, which maps images into this latent space, and explore its use for unpaired image-to-image translation.}

%DDMs Already Have A Semantic Latent Space perspective,
Kwon \etal~\cite{Kwon2022ddmhavesemantic} propose $h$-space for DDMs, the set of bottleneck feature maps of the U-Net \cite{ronneberger2015unet} across all timesteps, $\{\mathbf{h}_T,\ldots,\mathbf{h}_1\}$ as the latent space.  
Each bottleneck feature map $\mathbf{h}_t$ has a lower spatial dimension but more channels than the output image. They show that semantics can be edited by adding offsets $\Delta \mathbf{h}_t$ to the feature maps during the generative process. 
To find editing directions, they use an optimization procedure involving CLIP, where the semantics to be edited are described by text prompts. 
%
The $h$-space has the following properties: 
(i) a direction $\Delta \mathbf{h}_t$ typically has the same semantic effect on different samples; 
(ii) the magnitude of $\Delta \mathbf{h}_t$ controls the strength of the edit; 
(iii) $h$-space is additive in the sense that applying a linear combination of different directions where each $\Delta \mathbf{h}_t$ corresponds to a distinct attribute, results in a generated image where all attributes have been changed.


\subsection{Semantic image editing in generative models}
Semantic editing has been widely explored in GANs \cite{Shen2020Interfacegan,Harkonen2020GANSpace, Haas2022tensorGAN2,shen2021closedform, spingarn2021gansteerability, Patashnik2021styleclip,Abdal2020Image2StyleGANpp,Tewari2020StyleRig, Wu2020StyleSpace}. 
Shen \etal~\cite{Shen2020Interfacegan} used a binary classifier 
to annotate generated samples and trained a SVM to separate classes like pose, age, and gender. The corresponding linear directions in latent space were then defined as the normal vectors of the separating hyper-planes. H{\"a}rk{\"o}nen \etal~\cite{Harkonen2020GANSpace} found interpretable control directions in pretrained GANs by applying principal components of latent codes to appropriate layers of the generator.
Another line of work \cite{Haas2022tensorGAN2, shen2021closedform, spingarn2021gansteerability, zhu2021lowrankgan} uses various factorization techniques to define meaningful directions.

Semantic image editing has also been shown in DDMs but many existing methods make adaptations to the architecture, employ text-based optimization or model fine-tuning. 
In DiffusionAE~\cite{Preechakul2022DiffusionAutoencoder}, a DDM was trained in conjunction with an image encoder. This enabled attribute manipulation on real images, including modifications of gender, age, and smile, but requires modifying the DDM architecture.
Another line of work includes DiffusionCLIP~\cite{kim2022DiffusionCLIP}, Imagic~\cite{kawar2022imagic}, 
and UniTune~\cite{Valevski22},  combined CLIP-based text guidance with model fine-tuning.
Unlike these methods, our approaches do not require CLIP-based text-guidance nor model fine-tuning and can be applied to existing DDMs without retraining. 


% Recently several hybrid networks have been proposed for editing applications. They combine the architecture of diffusion models with either a variational autoencoder (VAEs)~\cite{Preechakul2022DiffusionAutoencoder} or generative adversarial networks (GANs)~\cite{Wang2022DiffusionGAN}. 
% SDEdit~\cite{meng2022sdedit} proposes to edit real images by adding appropriate noise to an image or a user-specified semantic mask and then starting the synthesis at some timestep $t_0$ with $ 0 < t_0 < T$. This allows for Domain-translation, and guided synthesis conditioned on a user-provided sketch. 
% In contrast, our approach does not require any input from the user
%\section{Background}

% \blue{
% Our method builds upon this work by Kwon et al.~\cite{Kwon2022ddmhavesemantic}, and also uses $h$-space as a semantic latent space. In contrast to their work we find semantic directions in $h$-space without the need for optimization or by using CLIP for text guidance.
% }


% Wu et al.~\cite{Wu2022DPMEncoder} proposed to make the synthesis process of DDPMs~\cite{ho2020denoising} deterministic by including the variance noises $\mathbf{z}_t$ in the latent code $\mathbf{q} = \mathbf{x}_T \otimes \mathbf{z}_T \otimes \mathbf{z}_{T-1}  \otimes\cdots \otimes \mathbf{z}_{0} $ and further and proposes a DPM-encoder as a way to construct $\mathbf{q}$ given $\mathbf{x}_0$.


% In ~\cite{Kwon2022ddmhavesemantic} is was proposed to 
% The space defined by $\mathbf{h}_t$ has a lower spatial dimension but more channels than $\mathbf{x}_T$ and $\mathbf{x}_0$ and is denoted as $h$-space.
% In ~\cite{Kwon2022ddmhavesemantic} is was proposed to regard the feature map
% $\mathbf{h}_t$ of smallest bottleneck layer of the U-Net as the latent representation.
% The space defined by $\mathbf{h}_t$ has a lower spatial dimension but more channels than $\mathbf{x}_T$ and $\mathbf{x}_0$ and is denoted as $h$-space.
% In each timestep $\mathbf{h}_t$ is naturally derived from $\mathbf{x}_t$ during the forward pass of the U-Net. Perturbing the bottleneck activation by injecting some $\Delta \mathbf{h}_t$ shifts the generative process resulting in an image with edited semantics.
% We denote $\bm{\epsilon}^\theta_t( \mathbf{x}_t| \Delta \mathbf{h}_t)$ as the output of the U-Net if the bottleneck features are shifted from  $\mathbf{h}_t$ to   $\mathbf{h}_t + \Delta\mathbf{h}_t$ for each timestep. 
% In~\cite{Kwon2022ddmhavesemantic} semantically meaningful shifts were found via an optimization procedure using CLIP so discover meaningful semantic directions based on a text input. 
% In \cite{Kwon2022ddmhavesemantic} it was further explained that $\Delta\mathbf{h}_t$ should be added to the synthesis process via an 
% \textbf{asy}metric \textbf{r}everse \textbf{p}rocess (asyrp). 
% Using the asyrp $\Delta\mathbf{h}_t$ is only injected into $\mathbf{P}_t$ of eq \eqref{eq:ddim-reverse} and not in $\mathbf{D}_t$.
% Thus the asyrp process can be written as
% \begin{align}
%     \mathbf{x}_{t-1} = 
%     \sqrt{\alpha_{t-1}} \mathbf{P}_t(\bm{\epsilon}^\theta_t(\mathbf{x}_t|\Delta \mathbf{h}_t)) 
% + \mathbf{D}_t (\bm{\epsilon}^\theta_t(  \mathbf{x}_t))
% + \sigma_t \mathbf{z}_t
% \end{align}
