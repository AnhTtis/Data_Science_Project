
% \subsection{Unsupervised methods}
% As shown in the previous section, using either a labeled data set or a pretrained domain-specific classifier for supervision can be an effective strategy for defining interpretable latent directions in the semantic latent space of DDMs. However in many situations such a data set or classifier might be difficult to obtain or not available at all. In this section we report on our findings using unsupervised methods for discovering interpretable  directions in the semantic latent space. 



\appendix

\section{Comment and Notes}
\subsection{Reversible DDIM}
\begin{tcolorbox}

% With DDIM inversion I get an extra factor of $\sqrt{\alphabar_{t+1}}$ when comparing the  formulation in prompt-to-prompt page 4 \url{https://arxiv.org/pdf/2211.09794.pdf}
% and diffusion models beats Gans on image synthesis
% \url{https://arxiv.org/abs/2105.05233} in the appendix F page 22. 
\begin{align}
    \mathbf{x}_{t+1} 
&= \sqrt{\alphabar_{t+1}}\mathbf{P}_t(\bm{\epsilon}^\theta_t(\mathbf{x}_t)) + \sqrt{1-\alphabar_{t+1}}\bm{\epsilon}^\theta_t(\mathbf{x}_t) 
\end{align}
From diffusionVAE supplemental / diffusionCLIP supplemental
\url{https://openaccess.thecvf.com/content/CVPR2022/supplemental/Kim_DiffusionCLIP_Text-Guided_Diffusion_CVPR_2022_supplemental.pdf}
\begin{align}
    \mathbf{x}_{t+1} 
&= \sqrt{\alphabar_{t+1}}\mathbf{P}_t(\bm{\epsilon}^\theta_t(\mathbf{x}_t)) + \sqrt{1-\alphabar_{t+1}}\bm{\epsilon}^\theta_t(\mathbf{x}_t) 
\end{align}


\begin{align}
\mathbf{x}_{t+1} 
&= \sqrt{\alphabar_{t+1}}\mathbf{P}_t(\bm{\epsilon}^\theta_t(\mathbf{x}_t)) + \sqrt{1-\alphabar_{t+1}}\bm{\epsilon}^\theta_t(\mathbf{x}_t) \\
&= \sqrt{\alphabar_{t+1}}\frac{\mathbf{x}_t - \sqrt{1-\alphabar_t} \bm{\epsilon}^\theta_t(\mathbf{x}_t) }{\sqrt{\alphabar_t}} + \sqrt{1-\alphabar_{t+1}}\bm{\epsilon}^\theta_t(\mathbf{x}_t)\\
&= \frac{\sqrt{\alphabar_{t+1}}}{\sqrt{\alphabar_t}}\mathbf{x}_t +
\left(\sqrt{1-\alphabar_{t+1}}-\frac{\sqrt{\alphabar_{t+1}}\sqrt{1-\alphabar_t}}{\sqrt{\alphabar_t}}\right)
\bm{\epsilon}^\theta_t(\mathbf{x}_t)\\
&= \frac{\sqrt{\alphabar_{t+1}}}{\sqrt{\alphabar_t}}\mathbf{x}_t 
+\sqrt{\alphabar_{t+1}} 
\left(
\sqrt{\frac{1}{\alphabar_{t+1}}-1 }
- 
\sqrt{\frac{1}{\alphabar_{t}}-1 }
\right)
\bm{\epsilon}^\theta_t(\mathbf{x}_t)
\end{align}
\end{tcolorbox}

\section{Reversible DDPM}

\paragraph{Reversible DDPM}
As hinted to in \cite{ho2020denoising} and made explicit in \cite{Wu2022DPMEncoder} and \red{[Cite Inbar's paper]} DDPM can be made deterministic if one regards the variance noise added in each step $\mathbf{z}_t$ as part of the latent representation 
\begin{align}
\mathbf{q} = \mathbf{x}_T\otimes \mathbf{z}_T \otimes \mathbf{z}_{t-1}\otimes \cdots \otimes \mathbf{z}_1 
\end{align}


The algorithm for inverting a real image $\mathbf{x}_0$ is straight-forward. 
First we sample from $p(\mathbf{x}_{1:T}|\mathbf{x}_0)$ to obtain $\mathbf{x}_t$ for every $t>0$ directly from $\mathbf{x}_0$ using
\begin{align}
    \mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}}\mathbf{z}_t
\end{align}
then we can solve for the every $\mathbf{z}$ in \eqref{eq:ddim-reverse} by
\begin{align}
    \mathbf{z}_t  = (\mathbf{x}_{t-1} - \bm{\mu}^\theta_t(\mathbf{x}_t))/\sigma_t
    % = \frac{\mathbf{x}_{t-1} - \frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}}}
    % \bm{\epsilon}^\theta_t(\mathbf{x}_t) )}{\sigma_t}
\end{align}
\begin{tcolorbox}
    Note there are other  options for pre-sampleing the $\mathbf{x}_t$s" than
    \begin{align}
    \mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}}\mathbf{z}_t
\end{align}
which has a huge effect on editing
\end{tcolorbox}


\section{Related work}
\paragraph{\red{The rest of these sections are maybe not needed, or should be shortened and included elsewhere.}}

\paragraph{Classifier guidance}
In~\cite{Dhariwal2021dpmsbeatgans} classifier guidance was proposed to enable class conditional synthesis by training a classifier at all noise levels. It was showed that including a classifier improves image quality in addition to enabling class conditional synthesis, however at the expense of adding additional computational cost in the forward pass due to the need to compute gradient through the classifier in addition to the burden of training a noise dependent classifiers.

In~\cite{Ho2022cfg} classifier-free guidance was introduced, which allows for class conditional synthesis without the need for explicit training of a noise dependent classifier.  Classifier guidance  introduces a trade-off between adherence to the conditional signal as the cost of reduces sample diversity. 

%% They use optimization
%% They require adaptation of thw architecture. 
%% Score matching tone down
%% Classifier guiance tone down 

\paragraph{Connection to score matching}
Closely related to DPMs are Score-Based Models (SBMs) \cite{song2021scorebased} \cite{song2019generative} \cite{song2019sliced} which defines the generative process in a reversing the Stochastic differential equation (SDE)
\begin{align}
    d\mathbf{x}= f(\mathbf{x},t)dt + g(t)d\mathbf{w}
\end{align}
The reverse SDE \red{[cite anderson 1982]} as 
\begin{align}
    d \mathbf{x} = 
    [f(\mathbf{x},t)- g^2\nabla_\mathbf{x}\log p_t(\mathbf{x})]
    + g(t)d\mathbf{w}
\end{align}
And score based models learn the score-function $\mathbf{s}_\theta(\mathbf{x},t)$ which approximates $ \nabla_\mathbf{x}\log p_t(\mathbf{x})$ which intuitively can be interpreted as a vector field pointing towards the high density regions of of the data at a given noise level $t$

% \begin{align}
%     \mathbf{s}_\theta(\mathbf{x},t)\approx \nabla_\mathbf{x}\log p_t(\mathbf{x})
% \end{align}

\paragraph{Faster sampling}
One important limitation of DPMs compared to GANs is the computational cost of synthesising an image. Since DPMs denoises an image iteratively, there has been a recent surge of attention in minimizing the number of forward passes or "Neural Function evaluations" (NFEs) to generate an image. 

Recent work \cite{Karras2022edm} explores the design space of DPMs, and proposes Heun's second order method (a variant of Runge-Kutta) for solving the ODE as the optimal tradeoff beween image quality and NFEs needed. 

In iDDPM \cite{Nichol2021iddpm} Shows that learning the variances allows for faster sampling with an order of magnitude fewer NFEs.

In order to  accelerate the inference process \cite{liu2022pseudo} proposes to view the synthesis process of DDPMs as solving a differential equations on a manifold.



% via 
%\begin{align}
%\mathbf{x}_{t+1} =  \sqrt{\alpha_{t+1}}\mathbf{P}_t(\bm{\epsilon}^\theta_t(\mathbf{x}_t)) + \sqrt{1-\alpha_{t+1}}\bm{\epsilon}^\theta_t(\mathbf{x}_t).
%\label{eq:reverseDDIM}
%\end{align}
%Thus DDIM generalizes DDPM to a wider class of models with varying levels of added stochastic noise.
%% Make the argument about t boosting 
%Note that we allow $\eta_t$ to be time-dependent since adding stochastic noise has been shown to improve image quality~\cite{Karras2022edm}. Specifically,~\cite{Kwon2022ddmhavesemantic} suggests adding noise towards the end of the synthesis process and refers to this as quality boosting. 
%%
% \paragraph{Diffusion Models}
%Diffusion models~\cite{sohl2015ddpm-noneqthermo} are a new class of generative models that can synthesize high-quality images. 
% \blue{
% Two related approaches are Score-based models (SBMs) \cite{song2021scorebased,song2019generative,song2019sliced} and Denoising Diffusion Probabilistic Models (DDPMs) \cite{ho2020denoising, Nichol2021iddpm}. Both use a forward process to add noise to data and a generative reverse process where a U-Net neural network removes the noise. SBMs train a network to approximate the score function $\nabla_\mathbf{x}\log p_t(\mathbf{x})$ enabling reversing [cite anderson]the forward process SDE. 
% }
%In this paper, we use DDPMs which train a network to predict the added noise, rather than the score.
% Equivalently, each $\mathbf{x}_t$ in the forward process can also be obtained directly from $\mathbf{x}_0$ as $\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\mathbf{n}$, where $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{t^\prime=1}^t \alpha_{t^\prime}$ 
% %then $\mathbf{x}_t \sim q(\mathbf{x}_t|\mathbf{x}_0)$ can be sampled directly  with
% and.
% That is, each noisy image in the process is sampled conditioned on the previous one according to the probability density


% \begin{align}
%     q(\mathbf{x}_t\mid \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t;\sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t \mathbf{I}),
% \end{align}
% where  $\{\beta_t\}_{t=1}^T$ is some schedule satisfying $\beta_T\approx 1$, so that $\mathbf{x}_T$ is approximately a standard Gaussian sample. 

%Following \cite{ho2020denoising}, which use a variance schedule $\{\beta_t\}_{t=1}^T$ satisfying $\sigma_t^2 = \beta_t$ and the forward (noising) process is defined as
%The forward process gradually destroys the information in the image and if $T$ is large $\mathbf{x}_T$ is approximately a standard Gaussian sample. 

% Equivalently, each $\mathbf{x}_t$ in the forward process can also be obtained directly from $\mathbf{x}_0$ as $\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\mathbf{n}$, where $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{t^\prime=1}^t \alpha_{t^\prime}$ 
% %then $\mathbf{x}_t \sim q(\mathbf{x}_t|\mathbf{x}_0)$ can be sampled directly  with
% and $\mathbf{n} \sim \mathcal{N}(\mathbf{0},\mathbf{I})$.

% \begin{align}
%     \mathbf{D}_t(\bm{\epsilon}^\theta_t(  \mathbf{x}_t ))  = \sqrt{1-\alpha_{t-1} - \sigma_t^2}
%     \bm{\epsilon}^\theta_t(  \mathbf{x}_t )
% \end{align}
% Moreover, $\sigma_t = \eta_t  \sqrt{(1-\alpha_{t-1})/(1-\alpha_t)}\sqrt{1- \alpha_t/\alpha_{t-1}}$.
% with $\eta_t > 0$ the reverse process is stochastic and with $\eta_t = 1$ it is equivalent to a form of DDPM. \red{Or is it strictly mathematically equivalent?}. 
% Thus DDIM generalizes DDPM to a wider class of models with varying levels of added stochastic noise.
% The variance, $\sigma_t \mathbf{z}_t$ is random noise with $\mathbf{z}_t\sim\mathcal{N}(\mathbf{0},\mathbf{I})$ and
% \begin{align}
%     % \sigma_t = \eta_t  \sqrt{\frac{1-\alpha_{t-1}}{1-\alpha_t}}\sqrt{1- \frac{\alpha_t}{\alpha_{t-1}}}     
%     \sigma_t = \eta_t  \sqrt{(1-\alpha_{t-1})/(1-\alpha_t)}\sqrt{1- \alpha_t/\alpha_{t-1}}         
% \end{align}


% \begin{tcolorbox}
% \red{Maybe exclude this part.}
% The reverse process is defined in terms of the joint distribution with learned Gaussian transitions 
% \begin{align}
%     p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T) \prod_{t=1}^T q(\mathbf{x}_{t-1} |\mathbf{x}_t)
% \end{align}
% where sampling $\mathbf{x}_{t-1}\sim q(\mathbf{x}_{t-1} |\mathbf{x}_t)$ can be achieved as
% \begin{align}
%     \mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}} 
%     \left ( 
%     \mathbf{x}_t- \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\bm{\epsilon}^\theta_t(\mathbf{x}_t) 
%     \right ) + \sigma_t \mathbf{z}_t
% \end{align}    
% \end{tcolorbox}
% Denoising Diffusion Dmplicit Models (DDIMs) redefine the forward process of DDPMs as a non-markovian process, the reverse process is still markovian.
% \red{[2DO Elaborate on this point]}.

% we have
% \begin{align}
% q(\mathbf{x}_t|\mathbf{x}_0) = 
% \mathcal{N}(\mathbf{x}_t;\sqrt{\bar{\alpha}_t}\mathbf{x}_0, (1-\bar{\alpha}_t)\mathbf{I})
% \end{align} 
% or equivalently 
% \begin{align}
% q(\mathbf{x}_t|\mathbf{x}_0) = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\mathbf{n} \quad \mathbf{n} \sim \mathcal{N}(\mathbf{0},\mathbf{I})
% \end{align} 
