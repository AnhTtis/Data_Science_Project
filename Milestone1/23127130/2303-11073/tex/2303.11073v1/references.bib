
@article{ramesh2022dalle2,
  author     = {Aditya Ramesh and
                Prafulla Dhariwal and
                Alex Nichol and
                Casey Chu and
                Mark Chen},
  title      = {Hierarchical Text-Conditional Image Generation with {CLIP} Latents},
  journal    = {CoRR},
  volume     = {abs/2204.06125},
  year       = {2022},
  url        = {https://doi.org/10.48550/arXiv.2204.06125},
  doi        = {10.48550/arXiv.2204.06125},
  eprinttype = {arXiv},
  eprint     = {2204.06125},
  timestamp  = {Tue, 19 Apr 2022 17:11:58 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2204-06125.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{
spingarn2021gansteerability,
title={{GAN} {S}teerability without optimization },
author={Nurit Spingarn and Ron Banner and Tomer Michaeli},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=zDy_nQCXiIj}
}
@InProceedings{Ruiz2018hopenet,
author = {Ruiz, Nataniel and Chong, Eunji and Rehg, James M.},
title = {Fine-Grained Head Pose Estimation Without Keypoints},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
month = {June},
year = {2018}
}

@inproceedings{
      meng2022sdedit,
      title={{SDE}dit: Guided Image Synthesis and Editing with Stochastic Differential Equations},
      author={Chenlin Meng and Yutong He and Yang Song and Jiaming Song and Jiajun Wu and Jun-Yan Zhu and Stefano Ermon},
      booktitle={International Conference on Learning Representations},
      year={2022},
}



@inproceedings{liu2015celeba,
  title = {Deep Learning Face Attributes in the Wild},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
  month = {December},
  year = {2015} 
}

@inproceedings{lin2021anycost,
  author    = {Lin, Ji and Zhang, Richard and Ganz, Frieder and Han, Song and Zhu, Jun-Yan},
  title     = {Anycost GANs for Interactive Image Synthesis and Editing},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2021},
}

 
@misc{ramesh2021dalle,
  author = {Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},
  title  = {Zero-Shot Text-to-Image Generation},
  year   = {2021},
  eprint = {arXiv:2102.12092}
}


@inproceedings{Saharia2022Imagen,
title={Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
author={Chitwan Saharia and William Chan and Saurabh Saxena and Lala Li and Jay Whang and Emily Denton and Seyed Kamyar Seyed Ghasemipour and Raphael Gontijo-Lopes and Burcu Karagol Ayan and Tim Salimans and Jonathan Ho and David J. Fleet and Mohammad Norouzi},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
}



@misc{rombach2021latentdiffusion,
  title         = {High-Resolution Image Synthesis with Latent Diffusion Models},
  author        = {Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
  year          = {2021},
  eprint        = {2112.10752},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@article{kawar2022imagic,
      title={Imagic: Text-Based Real Image Editing with Diffusion Models}, 
      author={Kawar, Bahjat and Zada, Shiran and Lang, Oran and Tov, Omer and Chang, Huiwen and Dekel, Tali and Mosseri, Inbar and Irani, Michal},
      year={2022},
      journal={arXiv preprint arXiv:2210.09276}
}

@inproceedings{Radford2021CLIP,
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  note      = {arXiv: 2103.00020},
  booktitle = {Proc.\@ ICML},
  author    = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and et al.},
  year      = {2021},
  month     = {Feb}
}


@misc{nichol2021glide,
  doi = {10.48550/ARXIV.2112.10741},
  
  url = {https://arxiv.org/abs/2112.10741},
  
  author = {Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Graphics (cs.GR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{Sauer2023styleganT,
  author    = {Axel Sauer and Tero Karras and Samuli Laine and Andreas Geiger and Timo Aila},
  title     = {StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis},
  journal   = {arXiv.org},
  volume    = {abs/2301.09515},
  year      = {2023},
  url       = {https://arxiv.org/abs/2301.09515},
}



@article{ruiz2022dreambooth,
  title={Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation},
  author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  journal={arXiv preprint arXiv:2208.12242},
  year={2022}
}


@InProceedings{kim2022DiffusionCLIP,
    author    = {Kim, Gwanghyun and Kwon, Taesung and Ye, Jong Chul},
    title     = {DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {2426-2435}
}

@misc{gal2022textual,
      doi = {10.48550/ARXIV.2208.01618},
      url = {https://arxiv.org/abs/2208.01618},
      author = {Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H. and Chechik, Gal and Cohen-Or, Daniel},
      title = {An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion},
      publisher = {arXiv},
      year = {2022},
      primaryClass={cs.CV}
}

@inproceedings{
  song2021scorebased,
  title={Score-Based Generative Modeling through Stochastic Differential Equations},
  author={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
  booktitle={International Conference on Learning Representations},
  year={2021},
  url={https://openreview.net/forum?id=PxTIG12RRHS}
}

@article{mokady2022null,
  title={Null-text Inversion for Editing Real Images using Guided Diffusion Models},
  author={Mokady, Ron and Hertz, Amir and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},
  journal={arXiv preprint arXiv:2211.09794},
  year={2022}
}

@inproceedings{song2019generative,
  title={Generative Modeling by Estimating Gradients of the Data Distribution},
  author={Song, Yang and Ermon, Stefano},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11895--11907},
  year={2019}
}


@inproceedings{song2019sliced,
  author    = {Yang Song and
               Sahaj Garg and
               Jiaxin Shi and
               Stefano Ermon},
  title     = {Sliced Score Matching: {A} Scalable Approach to Density and Score
               Estimation},
  booktitle = {Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial
               Intelligence, {UAI} 2019, Tel Aviv, Israel, July 22-25, 2019},
  pages     = {204},
  year      = {2019},
  url       = {http://auai.org/uai2019/proceedings/papers/204.pdf},
}


@inproceedings{Wu2022DPMEncoder,
  title={Unifying Diffusion Models' Latent Space, with Applications to {CycleDiffusion} and Guidance},
  author={Chen Henry Wu and Fernando De la Torre},
  booktitle={ArXiv},
  year={2022},
}

@misc{Yu2022Parti,
  doi = {10.48550/ARXIV.2206.10789},
  
  url = {https://arxiv.org/abs/2206.10789},
  
  author = {Yu, Jiahui and Xu, Yuanzhong and Koh, Jing Yu and Luong, Thang and Baid, Gunjan and Wang, Zirui and Vasudevan, Vijay and Ku, Alexander and Yang, Yinfei and Ayan, Burcu Karagol and Hutchinson, Ben and Han, Wei and Parekh, Zarana and Li, Xin and Zhang, Han and Baldridge, Jason and Wu, Yonghui},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Scaling Autoregressive Models for Content-Rich Text-to-Image Generation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}





@inproceedings{alaluf2021hyperstyle,
  title         = {HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing},
  author        = {Yuval Alaluf and Omer Tov and Ron Mokady and Rinon Gal and Amit H. Bermano},
  year          = {2022},
  booktitle     = {Proc.\@ CVPR},
  eprint        = {2111.15666},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@inproceedings{Brock2019BigGAN,
  title        = {Large Scale GAN Training for High Fidelity Natural Image Synthesis},
  url          = {http://arxiv.org/abs/1809.11096},
  abstractnote = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities speciﬁc to such scale. We ﬁnd that applying orthogonal regularization to the generator renders it amenable to a simple “truncation trick,” allowing ﬁne control over the trade-off between sample ﬁdelity and variety by reducing the variance of the Generator’s input. Our modiﬁcations lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Fre´chet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.65.},
  booktitle    = {International Conference on Learning Representations},
  year         = {2019},
  author       = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  month        = {Feb}
}


@inproceedings{Dhariwal2021dpmsbeatgans,
 author = {Dhariwal, Prafulla and Nichol, Alexander},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {8780--8794},
 publisher = {Curran Associates, Inc.},
 title = {Diffusion Models Beat GANs on Image Synthesis},
 url = {https://proceedings.neurips.cc/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf},
 volume = {34},
 year = {2021}
}



 @inproceedings{Goodfellow2014GAN,
  title     = {Generative Adversarial Nets},
  volume    = {27},
  url       = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  author    = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  editor    = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and Weinberger, K. Q.},
  year      = {2014},
  pages     = {2672–2680}
}
 @misc{esser2020taming,
  title         = {Taming Transformers for High-Resolution Image Synthesis},
  author        = {Patrick Esser and Robin Rombach and Björn Ommer},
  year          = {2020},
  eprint        = {2012.09841},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

 @inproceedings{Haas2021tensorGAN,
  title     = {Tensor-based Subspace Factorization for {StyleGAN}},
  author    = {Haas, René and Graßhof, Stella and Brandt, Sami Sebastian},
  booktitle = {Proc. \@ FG2021},
  year      = {2021}
}

@article{Haas2022tensorGAN2,
  title   = {Tensor-based Emotion Editing in the StyleGAN Latent Space},
  url     = {http://arxiv.org/abs/2111.04554},
  note    = {Accepted for poster presentation at AI4CC @ CVPRW},
  journal = {arXiv:2205.06102 [cs]},
  author  = {Haas, René and Graßhof, Stella and Brandt, Sami Sebastian},
  year    = {2022},
  month   = {May}
}

 @inproceedings{Harkonen2020GANSpace,
  title     = {GANSpace: Discovering Interpretable GAN Controls},
  author    = {Erik Härkönen and Aaron Hertzmann and Jaakko Lehtinen and Sylvain Paris},
  booktitle = {Proc.\@  NeurIPS},
  year      = {2020}
}



@inproceedings{ho2020denoising,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6840--6851},
 publisher = {Curran Associates, Inc.},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
 volume = {33},
 year = {2020}
}



@inproceedings{liu2022pseudo,
    title={Pseudo Numerical Methods for Diffusion Models on Manifolds},
    author={Luping Liu and Yi Ren and Zhijie Lin and Zhou Zhao},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=PlKWVd2yBkY}
}


@inproceedings{ho2021classifierfree,
title={Classifier-Free Diffusion Guidance},
author={Jonathan Ho and Tim Salimans},
booktitle={NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications},
year={2021},
url={https://openreview.net/forum?id=qw8AKxfYbI}
}




 @inproceedings{karras2018pggan,
  title     = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  url       = {http://arxiv.org/abs/1710.10196},
  note      = {arXiv: 1710.10196},
  journal   = {arXiv:1710.10196 [cs, stat]},
  author    = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  booktitle = {Proc.\@  ICLR},
  year      = {2018},
  month     = {Feb},
  language  = {en}
}

 @inproceedings{Karras2019StyleGAN,
  author    = {Karras, Tero and Laine, Samuli and Aila, Timo},
  booktitle = {Proc.\@ CVPR},
  title     = {A Style-Based Generator Architecture for Generative Adversarial Networks},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {4396-4405},
  doi       = {10.1109/CVPR.2019.00453}
}

 @inproceedings{Karras2020StyleGAN2,
  title     = {Analyzing and Improving the Image Quality of {StyleGAN}},
  author    = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  booktitle = {Proc.\@ CVPR},
  year      = {2020}
}

@inproceedings{Karras2020StyleGANada,
  title     = {Training Generative Adversarial Networks with Limited Data},
  author    = {Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},
  booktitle = {Proc.\@ NeurIPS},
  year      = {2020}
}

 @inproceedings{Karras2021StyleGAN3,
  author    = {Tero Karras and Miika Aittala and Samuli Laine and Erik H\"ark\"onen and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  title     = {Alias-Free Generative Adversarial Networks},
  booktitle = {Proc.\@ NeurIPS},
  year      = {2021}
}
 

@inproceedings{Karras2022edm,
  author    = {Tero Karras and Miika Aittala and Timo Aila and Samuli Laine},
  title     = {Elucidating the Design Space of Diffusion-Based Generative Models},
  booktitle = {Proc. NeurIPS},
  year      = {2022}
}

@inproceedings{
Kwon2022ddmhavesemantic,
title={Diffusion Models Already Have A Semantic Latent Space},
author={Mingi Kwon and Jaeseok Jeong and Youngjung Uh},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=pd1P2eUBVfq}
}



@inproceedings{Nichol2021iddpm,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6840--6851},
 publisher = {Curran Associates, Inc.},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{Wu2020StyleSpace,
  title     = {StyleSpace Analysis: Disentangled Controls for {StyleGAN} Image Generation},
  booktitle = {Proc.\@ CVPR},
  author    = {Wu, Zongze and Lischinski, Dani and Shechtman, Eli},
  year      = {2020},
  month     = {Dec}
}

@inproceedings{Tewari2020StyleRig,
  title        = {{StyleRig}: Rigging {StyleGAN} for 3D Control over Portrait Images},
  author       = {Tewari, Ayush and Elgharib, Mohamed and Bharaj, Gaurav and Bernard, Florian  and Seidel, Hans-Peter and P{\'e}rez, Patrick and Z{\"o}llhofer, Michael and Theobalt, Christian},
  booktitle    = {Proc.\@ CVPR)},
  month        = {June},
  organization = {{IEEE}},
  year         = {2020}
}

@inproceedings{Abdal2020Image2StyleGANpp,
  author    = {Rameen Abdal, Yipeng Qin and Peter Wonka},
  booktitle = {Proc.\@ CVPR},
  title     = {{Image2StyleGAN++}: How to Edit the Embedded Images?},
  year      = {2020},
  month     = {Aug},
  volume    = {},
  number    = {},
  pages     = {8293-8302},
  doi       = {10.1109/CVPR42600.2020.00832}
}

@InProceedings{Patashnik2021styleclip,
    author    = {Patashnik, Or and Wu, Zongze and Shechtman, Eli and Cohen-Or, Daniel and Lischinski, Dani},
    title     = {StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {2085-2094}
}

 @inproceedings{Preechakul2022DiffusionAutoencoder,
  address   = {New Orleans, LA, USA},
  title     = {Diffusion Autoencoders: Toward a Meaningful and Decodable Representation},
  isbn      = {978-1-66546-946-3},
  url       = {https://ieeexplore.ieee.org/document/9878402/},
  doi       = {10.1109/CVPR52688.2022.01036},
  booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  author    = {Preechakul, Konpat and Chatthee, Nattanat and Wizadwongsa, Suttisak and Suwajanakorn, Supasorn},
  year      = {2022},
  month     = {Jun},
  pages     = {10609–10619},
  language  = {en}
}
@misc{rombach2021highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{Shen2020Interfacegan,
  title     = {Interpreting the Latent Space of GANs for Semantic Face Editing},
  author    = {Shen, Yujun and Gu, Jinjin and Tang, Xiaoou and Zhou, Bolei},
  booktitle = {CVPR},
  year      = {2020}
}

@article{Shen2020InterfaceganTPAMI,
  title   = {InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs},
  author  = {Shen, Yujun and Yang, Ceyuan and Tang, Xiaoou and Zhou, Bolei},
  journal = {TPAMI},
  year    = {2020}
} 
 @inproceedings{Sauer2022StyleganXL,
  address      = {Vancouver BC Canada},
  title        = {StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets},
  isbn         = {978-1-4503-9337-9},
  url          = {https://dl.acm.org/doi/10.1145/3528233.3530738},
  doi          = {10.1145/3528233.3530738},
  abstractnote = {Computer graphics has experienced a recent surge of data-centric approaches for photorealistic and controllable content creation. StyleGAN in particular sets new standards for generative modeling regarding image quality and controllability. However, StyleGAN’s performance severely degrades on large unstructured datasets such as ImageNet. StyleGAN was designed for controllability; hence, prior works suspect its restrictive design to be unsuitable for diverse datasets. In contrast, we find the main limiting factor to be the current training strategy. Following the recently introduced Projected GAN paradigm, we leverage powerful neural network priors and a progressive growing strategy to successfully train the latest StyleGAN3 generator on ImageNet. Our final model, StyleGANXL, sets a new state-of-the-art on large-scale image synthesis and is the first to generate images at a resolution of 10242 at such a dataset scale. We demonstrate that this model can invert and edit images beyond the narrow domain of portraits or specific object classes. Code, models, and supplementary videos can be found at https://sites.google.com/view/stylegan-xl/.},
  booktitle    = {Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings},
  publisher    = {ACM},
  author       = {Sauer, Axel and Schwarz, Katja and Geiger, Andreas},
  year         = {2022},
  month        = {Aug},
  pages        = {1–10},
  language     = {en}
}
@inproceedings{shen2021closedform,
  title     = {Closed-Form Factorization of Latent Semantics in GANs},
  author    = {Shen, Yujun and Zhou, Bolei},
  booktitle = {CVPR},
  year      = {2021}
}
 @inproceedings{sohl2015ddpm-noneqthermo,
  title     = {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author    = {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  pages     = {2256--2265},
  year      = {2015},
  editor    = {Bach, Francis and Blei, David},
  volume    = {37},
  series    = {Proceedings of Machine Learning Research},
  address   = {Lille, France},
  month     = {07--09 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  url       = {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  abstract  = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}
 @article{song2020ddim,
  title   = {Denoising Diffusion Implicit Models},
  author  = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  journal = {arXiv:2010.02502},
  year    = {2020},
  month   = {October},
  abbr    = {Preprint},
  url     = {https://arxiv.org/abs/2010.02502}
}

 @article{Tov2021e4e,
  title    = {Designing an encoder for {StyleGAN} image manipulation},
  volume   = {40},
  issn     = {0730-0301},
  doi      = {10.1145/3450626.3459838},
  abstract = {Recently, there has been a surge of diverse methods for performing image editing by employing pre-trained unconditional generators. Applying these methods on real images, however, remains a challenge, as it necessarily requires the inversion of the images into their latent space. To successfully invert a real image, one needs to find a latent code that reconstructs the input image accurately, and more importantly, allows for its meaningful manipulation. In this paper, we carefully study the latent space of StyleGAN, the state-of-the-art unconditional generator. We identify and analyze the existence of a distortion-editability tradeoff and a distortion-perception tradeoff within the StyleGAN latent space. We then suggest two principles for designing encoders in a manner that allows one to control the proximity of the inversions to regions that StyleGAN was originally trained on. We present an encoder based on our two principles that is specifically designed for facilitating editing on real images by balancing these tradeoffs. By evaluating its performance qualitatively and quantitatively on numerous challenging domains, including cars and horses, we show that our inversion method, followed by common editing techniques, achieves superior real-image editing quality, with only a small reconstruction accuracy drop.},
  number   = {4},
  urldate  = {2022-03-07},
  journal  = {ACM Transactions on Graphics},
  author   = {Tov, Omer and Alaluf, Yuval and Nitzan, Yotam and Patashnik, Or and Cohen-Or, Daniel},
  month    = jul,
  year     = {2021},
  keywords = {generative adversarial networks, image editing, latent space},
  pages    = {133:1--133:14}
}


@article{Wang2022DiffusionGAN,
  title        = {Diffusion-GAN: Training GANs with Diffusion},
  url          = {http://arxiv.org/abs/2206.02262},
  abstractnote = {Generative adversarial networks (GANs) are challenging to train stably, and a promising remedy of injecting instance noise into the discriminator input has not been very eﬀective in practice. In this paper, we propose Diﬀusion-GAN, a novel GAN framework that leverages a forward diﬀusion chain to generate Gaussian-mixture distributed instance noise. Diﬀusion-GAN consists of three components, including an adaptive diﬀusion process, a diﬀusion timestep-dependent discriminator, and a generator. Both the observed and generated data are diﬀused by the same adaptive diﬀusion process. At each diﬀusion timestep, there is a diﬀerent noise-to-data ratio and the timestep-dependent discriminator learns to distinguish the diﬀused real data from the diﬀused generated data. The generator learns from the discriminator’s feedback by backpropagating through the forward diﬀusion chain, whose length is adaptively adjusted to balance the noise and data levels. We theoretically show that the discriminator’s timestep-dependent strategy gives consistent and helpful guidance to the generator, enabling it to match the true data distribution. We demonstrate the advantages of Diﬀusion-GAN over strong GAN baselines on various datasets, showing that it can produce more realistic images with higher stability and data eﬃciency than state-of-the-art GANs.},
  note         = {arXiv:2206.02262 [cs, stat]},
  number       = {arXiv:2206.02262},
  publisher    = {arXiv},
  author       = {Wang, Zhendong and Zheng, Huangjie and He, Pengcheng and Chen, Weizhu and Zhou, Mingyuan},
  year         = {2022},
  month        = {Oct},
  language     = {en}
}

@inproceedings{alaluf2023third,
  title={Third time’s the charm? image and video editing with stylegan3},
  author={Alaluf, Yuval and Patashnik, Or and Wu, Zongze and Zamir, Asif and Shechtman, Eli and Lischinski, Dani and Cohen-Or, Daniel},
  booktitle={Computer Vision--ECCV 2022 Workshops: Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part II},
  pages={204--220},
  year={2023},
  organization={Springer}
}


@article{Hertz22,
  title = {Prompt-to-Prompt Image Editing with Cross Attention Control},
  author = {Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},
  journal = {arXiv preprint arXiv:2208.01626},
  year = {2022},
}

@inproceedings{zhu2021lowrankgan,
  title     = {Low-Rank Subspaces in {GAN}s},
  author    = {Zhu, Jiapeng and Feng, Ruili and Shen, Yujun and Zhao, Deli and Zha, Zhengjun and Zhou, Jingren and Chen, Qifeng},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2021}
}

@article{Narek22,
  title = {Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation},
  author = {Tumanyan, Narek and Geyer, Michal and Bagon, Shai and Dekel, Tali},
  journal = {arXiv preprint arXiv:2211.12572},
  year = {2022},
}

@misc{Guillaume22,  
  author = {Couairon, Guillaume and Verbeek, Jakob and Schwenk, Holger and Cord, Matthieu},
  title = {DiffEdit: Diffusion-based semantic image editing with mask guidance},
  publisher = {arXiv},
  year = {2022},}

  @article{Gihyun22,
  title = {Diffusion-based Image Translation using Disentangled Style and Content Representation},
  author = {Kwon, Gihyun and Ye, Jong Chul},
  journal = {arXiv preprint arXiv:2209.15264},
  year = {2022},
}



@misc{Valevski22,
  author = {Valevski, Dani and Kalman, Matan and Matias, Yossi and Leviathan, Yaniv},
  title = {UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image},
  publisher = {arXiv},
  year = {2022},
}


@misc{von-platen-etal-2022-diffusers,
  author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Thomas Wolf},
  title = {Diffusers: State-of-the-art diffusion models},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/diffusers}}
}



@inproceedings{radford2016dcgan,
  author    = {Alec Radford and
               Luke Metz and
               Soumith Chintala},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Unsupervised Representation Learning with Deep Convolutional Generative
               Adversarial Networks},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1511.06434},
  timestamp = {Thu, 25 Jul 2019 14:25:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RadfordMC15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{yin2006bu3dfe,
  author    = {Lijun Yin and Xiaozhou Wei and Yi Sun and Jun Wang and Rosato, M.J.},
  booktitle = {7th Intern. Conf. on Automatic Face and Gesture Recognition (FGR06)},
  title     = {A 3D facial expression database for facial behavior research},
  year      = {2006},
  volume    = {},
  number    = {},
  pages     = {211-216},
  doi       = {10.1109/FGR.2006.6}
}



@article{fisher2015lsun,
  added-at = {2018-08-13T00:00:00.000+0000},
  author = {Yu, Fisher and Zhang, Yinda and Song, Shuran and Seff, Ari and Xiao, Jianxiong},
  biburl = {https://puma.ub.uni-stuttgart.de/bibtex/2446d4ffb99a5d7d2ab6e5417a12e195f/dblp},
  ee = {http://arxiv.org/abs/1506.03365},
  interhash = {3e9306c4ce2ead125f3b2ab0e25adc85},
  intrahash = {446d4ffb99a5d7d2ab6e5417a12e195f},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2019-09-27T10:01:17.000+0000},
  title = {LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1506.html#YuZSSX15},
  volume = {abs/1506.03365},
  year = 2015
}



@inproceedings{ronneberger2015unet,
author="Ronneberger, Olaf
and Fischer, Philipp
and Brox, Thomas",
editor="Navab, Nassir
and Hornegger, Joachim
and Wells, William M.
and Frangi, Alejandro F.",
title="U-Net: Convolutional Networks for Biomedical Image Segmentation",
booktitle="Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="234--241",
abstract="There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.",
isbn="978-3-319-24574-4"
}


@article{Wang2022GANgeneratedFD,
  title={GAN-generated Faces Detection: A Survey and New Perspectives},
  author={Xin Wang and Hui Guo and Shu Hu and Ming-Ching Chang and Siwei Lyu},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.07145}
}

@book{saad2011numerical,
  title={Numerical methods for large eigenvalue problems: revised edition},
  author={Saad, Yousef},
  year={2011},
  publisher={SIAM}
}