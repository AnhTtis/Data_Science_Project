\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1.8\columnwidth]{Figures/experiments/burger_demonstration_941.pdf}
    \caption{A solution of the Burgers' equation (prediction MSE appears in the title). The left panel displays the ground truth (GT), next to it the error-minimizing PDExplain prediction. No-PDE suffers from the largest error, which can be explained by its total lack of mechanistic knowledge. PDE-RHS achieves a relatively low MSE, but the quality of its prediction decreases over time.}\label{fig:experiments.burgers_demo}
\end{figure*}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\columnwidth]{Figures/experiments/pred_error_by_train_set_size.pdf} \\
    (a) \\
    \includegraphics[width=0.8\columnwidth]{Figures/experiments/params_error_by_train_set_size.pdf} \\
    (b) \\
    \caption{Constant coefficients PDE: (a) Prediction error of signal vs. train set size and (b) estimation error of parameter values vs. train set size. The error is calculated on a test set of 1000 samples. } \label{fig:experiments.pred_err_train}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\columnwidth]{Figures/experiments/pred_error_by_context_size.pdf} \\
    (a) \\
    \includegraphics[width=0.8\columnwidth]{Figures/experiments/params_error_by_context_size.pdf} \\
    (b) \\
    \caption{Constant coefficients PDE: (a) Prediction error vs. $\rho$ and (b) estimation error of parameter values error vs. $\rho$. The error is calculated on a test set of 1000 samples.}\label{fig:experiments.pred_err_context}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\columnwidth]{Figures/experiments/pred_rollout_error_by_time.pdf}
    \caption{Prediction error as prediction horizon increases, for different approaches, PDE with constant coefficients. PDExplain, in red, is our approach. No PDE, in blue, learns an encoding for the training data and applies a purely data-driven prediction. DI, in green, corresponds to direct identification of the PDE coefficients from the datapoint, followed by solving the PDE (no training). The last approach, PDE-RHS, learns an encoding for the training data and predicts the right-hand-side of an equation which is then solved to yield an estimate of the signal. The dashed vertical line is the value of the context ratio used for this experiment (fixed for all approaches), $\rho=0.21$.}\label{fig:experiments.pred_rollout}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\columnwidth]{Figures/experiments/params_a_r2.pdf}
    \caption{Constant coefficients PDE: estimated value of the $\partial^2 u / \partial^2 x$ coefficient vs. ground truth, for entire test set ($R^2 = 0.93)$.}\label{fig:experiments.param_a_r2}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\columnwidth]{Figures/experiments/burger_pred_rollout_error_by_time.pdf}
    \caption{Prediction error as prediction horizon increases, for different approaches, Burgers' PDE. PDExplain, in red, is our approach. No PDE, in blue, learns an encoding for the training data and applies a purely data-driven prediction. The last approach, PDE-RHS, learns an encoding for the training data and predicts the right-hand-side of an equation which is then solved to yield an estimate of the signal. The dashed vertical line is the value of the context ratio used for this experiment (fixed for all approaches), $\rho=0.2$. The DI approach cannot be applied to equations with non-constant coefficients and is therefore omitted from the comparison.}\label{fig:experiments.burger_pred_rollout}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\columnwidth]{Figures/experiments/burger_params_b_prediction.pdf}
    \caption{Estimation of the coefficient function $b(x,t,u)$ of the Burgers' equation, presented in Eq.~\eqref{eq:pde_exp2}. PDExplain manages to accurately estimate the spatio-temporal dynamics of the coefficient, based on a context ratio of $\rho=0.2$.}\label{fig:experiments.burger_params_b}
\end{figure}

We devote this section to two types of analyses: (a) a comparison of our approach to other solutions, and (b) an analysis of our approach in different regimes.

The dataset we use includes $10,000$ samples of size $100\times 40[t\: \textrm{points} \times x\: \textrm{points}]$, where each sample is a signal generated from a PDE with different coefficients.
We stress the fact that the test set contains signals generated by PDEs with coefficient vectors that \emph{do not} appear in the training data, resulting in a zero-shot prediction problem. More information about dataset creation can be found in the appendix.

In our experiments we implement three algorithms, in addition to our proposed PDExplain, corresponding to the different approaches presented in Figure~\ref{fig:experiments.mech_dd}. For the sake of fair comparison, all of the data utilizing approaches include a trainable encoder, similar to the one introduced in Section~\ref{ssec:method.PDExplain_training}.
The algorithms are:
\begin{itemize}
    \item ``No PDE'': Direct prediction of the signal $\hat{u}(x,t)$, given the partial signal (patch) $u^c(x,t)$ and an encoding of it, learned from training data. No use of mechanistic knowledge.
    \item ``DI'': Direct identification of the PDE coefficients from the partial signal $u^c(x,t)$ using a maximum likelihood approach, followed by solving the resulting PDE to predict the signal. Assumes PDE family is known, parameters unknown. No use of training data.
    \item ``PDE-RHS'': Prediction of the underlying PDE's right-hand-side based on the partial signal (patch) $u^c(x,t)$ and an encoding of it, followed by solving the resulting PDE to predict the signal. This an approach that combines training data with minimal mechanistic knowledge, similar to that suggested by \citet{chen2018neural}.
\end{itemize}
We summarize the different approaches in Table~\ref{tbl:experiments.comp_tbl}, adding the feature of explainability, available in schemes that explicitly estimate PDE coefficients.

\begin{table}
\caption{Characteristics of the implemented algorithms.}
\begin{tabular}{ |l|c|c|c| } 
 \hline
 Approach & Mechanistic & Training & Explainability \\ 
  & knowledge & data & \\ \hline
 No PDE & - & + & - \\ \hline
 PDE RHS & - & + & - \\ \hline
 DI & + & - & +\\ \hline
 PDExplain & + & + & +\\ \hline
\end{tabular}\label{tbl:experiments.comp_tbl}
\end{table} 

\subsection{Second Order PDE with Constant Coefficients}\label{ssec:pde_exp1}
The first family of PDEs used for our experiments is: 
\begin{align}
\label{eq:pde_exp1}
        \fder{u}{t}=a \sder{u}{x} + b \fder{u}{x} + c,
\end{align}
where $p=(a,b,c)$ are constants. Figure~\ref{fig:experiments.pred_rollout} demonstrates the clear advantage of our approach, which becomes increasingly larger as the prediction horizon increases (note the logarithmic scale of the vertical axis, representing the MSE of prediction). Since PDExplain harnesses both mechanistic knowledge and training data, it is able to predict the signal $\hat{u}(x,t)$ several timesteps ahead, while keeping the error to a minimum. Next, we analyze the two parameters that characterize the PDExplain algorithm: train set size and context ratio. 

\subsubsection{Train set size}\label{ssec:experiments.train}
The train set size corresponds to $N$, the number of samples in dataset $U$ of Algorithm~\ref{alg:algorithm1}. Figure~\ref{fig:experiments.pred_err_train} presents the decrease in the prediction and parameters error as we increase the train set size. This attests to the generalization achieved by the PDExplain architecture: as the train set grows and includes more samples with different values of coefficients, the ability to accurately estimate a new sample's parameters and predict its rollout improves. In this set of experiments, $3,000$ samples are generally enough to achieve a minimal error rate.

\subsubsection{Context Ratio}\label{ssec:experiments.context}
Another hyper-parameter of our system is the context ratio. Figure~\ref{fig:experiments.pred_err_context} presents the results of an experiment in which we vary its value as defined in Section~\ref{ssec:method.PDExplain_training}. Simply put, as the context size increases, PDExplain encodes more information regarding the input signal's dynamics, thus the improvement in signal and parameter value prediction. The error decreases rather quickly, and a context ratio of $0.15 - 0.2$ suffices for reaching a very low error, as is evident from the plots.

The last result for this set of experiments appears in Figure~\ref{fig:experiments.param_a_r2}. Here, we plot the estimated value of parameter $a$ of Eq.~\eqref{eq:pde_exp1}, against its true value. The plot and the high value of $R^2$ demonstrate the low variance of our prediction, with a strong concentration of values along the $y=x$ line.

\subsection{Burgers' equation}
Another family of PDEs we experiment with is the quasi-linear Burgers' equation, whose general form is
\begin{align}
    \label{eq:pde_exp2}
        \fder{u}{t}=a\sder{u}{x} + b(u)\fder{u}{x},
\end{align}
where $b(x,t,u)=-u$, as presented in \cite{bateman1915some}. We note that this equation is quasi-linear since its drift coefficient $b(x,t,u)$ depends on the solution $u$ itself.

The dataset for our experiments consists of signals with different values of $a$ and the same $b(u)=-u$, both unknown to the algorithm a priori.
We begin with a demonstration of a signal $u(x,t)$ and its prediction $\hat{u}(x,t)$ in Figure~\ref{fig:experiments.burgers_demo}. As can be seen both visually and from the value of the MSE (in each panel's title), our approach yields a prediction that stays closest to the ground truth (GT), even as time advances and the prediction horizon increases.

Figure~\ref{fig:experiments.burger_pred_rollout} displays a comparison between the different approaches to our problem. As before, the vertical axis of the plot is logarithmic, and the advantage of PDExplain over other approaches increases with the prediction horizon. The direct identification (DI) approach could not be applied to this family of PDEs, since it does not support coefficients that are functions, only constants.

In Figure~\ref{fig:experiments.burger_params_b} we focus on the ability to accurately predict coefficient functions with spatio-temporal dynamics, specifically in this case - the coefficient $b(x,t,u)$ of Eq.~\eqref{eq:pde_exp2}. The different panels corresponds to different points in time, showing that the coefficient estimator tracks the temporal evolution successfully.

\begin{table}[ht!]
\label{exp.table.constant_pde}
\caption{Results summary for both the coefficient identification and signals prediction task, on two experiments: constant coefficients equation and Burgers' equation. In both experiments we used $\rho=0.2$, and report the MSE error.
DI baseline in the constant coefficient experiment completely failed in predicting one of the test signals, so that measurement was omitted from the calculation to have a fair comparison.}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
\toprule
% Experiment &
% \begin{tabular}[c]{@{}c@{}}Context \\ Ratio $\rho$ \end{tabular} & 
Method & 
\begin{tabular}[c]{@{}c@{}}Parameters \\ MSE \end{tabular} & 
\begin{tabular}[c]{@{}c@{}}Prediction \\ MSE \end{tabular}
\\
\midrule
\multicolumn{3}{c}{Constant PDE coefficients} \\
\midrule
% \multirow{4}{*}{0.5} & 
PDExplain & $0.0116 \pm 0.014$ & $0.0014 \pm 0.003$ \\
DI & $0.0097 \pm 0.022$ & $0.0067^* \pm 0.161$ \\
No-PDE & N/A & $0.0720 \pm 0.051$ \\
PDE-RHS & N/A & $0.0174 \pm 0.326$ \\
\midrule
\multicolumn{3}{c}{Burgers' PDE} \\
\midrule
PDExplain & $0.0147 \pm 0.0188$ & $0.0001 \pm 0.0004$ \\
DI & N/A & N/A \\
No-PDE & N/A & $0.1303 \pm 0.0839$ \\
PDE-RHS & N/A & $0.0004 \pm 0.0010$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
