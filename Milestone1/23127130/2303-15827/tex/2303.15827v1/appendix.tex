We provide further information regarding the experiments described in \cref{sec:experiments}. We ran all of the experiments on a single standard GPU, and all training algorithms took $<10$ minutes to train.
Full code implementation for creating the datasets and implementing PDExplain and its baselines is avalable on {\textcolor{blue}{\url{github.com/orilinial/PDExplain}}}.

To create the dataset, we generated signals using the \verb|PyPDE| package. Each signal was generated with different initial conditions sampled from a Gaussian process posterior, and a-priori known Dirichlet boundary conditions~$u(x=0)=u(x=L)=0$. 
As discussed in \cref{sec:related}, we made an important change compared to other known methods: the PDE parameters $(a,b,c)$ are uniformly sampled for each signal, instead of being constant, making the task much harder.
In the constant coefficients experiment we generated the parameters from $a \sim U[0,2]$, $b,c \sim U[-1, 1]$.
For the Burgers' equation dataset we used $a \sim U[1, 2]$.

\subsection{Implementation details}
All algorithms described in this paper except for DI (i.e., PDExplain, No-PDE and PDE-RHS) share the same context-extraction architecture.
The architecture consists of an encoder-decoder network, both implemented as MLPs with 6 layers and 256 neurons in each layer.
We found that concatenating the latent vector in the output of the encoder to the initial conditions of the signal $u(t=0)$ greatly improved results and convergence time, since it encourages the encoder to focus on the dynamics of the observed signal, rather than the initial conditions of it.

The second part of each algorithm uses the latent vector as an input to a Context-To-Dynamics network.

In PDExplain we implemented this network as an MLP with 5 hidden layers, each with 1024 neurons.
The output of the network is then the task-specific parameters with the correct shape. 
For the Burgers' equation for example, the output of the network is the parameter $a$, and a function $b(u)$ with the same shape as $u$.

In the No-PDE algorithm, the Context-To-Dynamics network consumes the current PDE state and the latent vector, and outputs the PDE state in the next time step.
The optimization function for this algorithm therefore tries to minimize the prediction error of $u_{t+1}$ in addition to the autoencoder loss.

The PDE-RHS algorithm is similar to the PDExplain algorithm as it tries to learn a derivative and not the predicted state. The Context-To-Dynamics network in this case consumes the latent vector and the current state, and outputs the right-hand-side of the PDE equation.

Both PDE-RHS and No-PDE baselines have the same architecture of the Context-To-Dynamics net as PDExplain, and only differ in the shape of the output.
Sharing the same architecture allow us to carefully compare these methods and answer the question of how mechanistic knowledge can be used.