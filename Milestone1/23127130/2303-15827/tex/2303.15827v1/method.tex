The data we handle is a set of spatio-temporal signals, generated by an underlying PDE with partial knowledge of its form and its boundary conditions. The coefficient functions determining the exact PDE are unknown and may be different for each collection of data. Our goal is to estimate these coefficient functions and provide reliable predictions of the future time steps of the observed phenomenon.
The proposed method comprises three subsequent parts: (1) Creating a compact representation of the given signal, (2) estimating the PDE coefficients, and (3) solving the PDE using the acquired knowledge. Although the proposed method may be generalized to other PDE types, for ease of exposition, we focus on parabolic PDEs  in this section.

\subsection{Problem Formulation} 

% \dd{Funcional, $u_c(x,t)$, etc.}
% \dd{We should first explain what is PDExplain.}
%\dd{Sencond paragraph: define PDE appproximation, ....}

We now define the problem at hand formally. Let $u(x,t)$ denote a signal with spatial support $x\in[0,L]$ and temporal support $t\in[0,T]$. We refer to this as the \emph{complete} signal. Next, we define $u^c(x,t)$ to be a partial input signal, a \emph{patch}, where its support is $x\in[0,L]$, $t\in[0,t_0]$, and where $0<t_0<T$. The superscript $c$ stands for context.

We assume the signal $u(x,t)$ is the solution of a $k$-order PDE of the general form
\begin{align} \label{eq:general_PDE}
    \fder{u}{t}= \sum_{l=1}^k p_l(x,t,u) \frac{\partial u^l}{\partial x^l}+ p_0(x,t,u),
\end{align}
where we denote the vector of coefficient functions by $p=(p_0, \ldots,p_k)$. We adopt the notation of \citet{wang2021physics} and refer to a family of PDEs characterized by a vector $p$ as an operator $F(p, u)$, where solving $F(p,u)=0$ yields solutions of the PDE.

The problem we solve is as follows: given a patch $u^c(x,t)$, that solves a PDE of a \emph{known} operator $F$ with an \emph{unknown} coefficient vector $p$, we would like to (a) estimate the coefficient vector $\hat{p}$ and (b) predict the complete signal $\hat{u}(x,t)$ for $0 \le t \le T$.

Our solution is a concatenation of two neural networks, which we call \emph{PDExplain}. Its input is a patch, and its output is a vector $\hat{p}$. We feed this vector into an off-the-shelf PDE solver together with the operator $F(p,u)$ to obtain the predicted signal $\hat{u}(x,t)$.

The partial derivatives are estimated using standard numerical schemes for each point in the patch. We choose discretization parameters $\Delta x$ for the spatial axis and $\Delta t$ for the temporal axis where we solve the PDE numerically on the grid points $\{(i\Delta x, j\Delta t)\}_{i=0, j=0}^{N_x, N_t}$ with $L=N_x \Delta x$ and $T=N_t \Delta t$. Let us denote the numerical solution with $\hat{u}_{i,j}$. We use the \emph{forward-time central-space} scheme, so a second order scheme from \eqref{eq:general_PDE} would be
\begin{equation}
\begin{split}\label{eq:triangle_scheme}
\frac{\hat{u}_{i,j+1}-\hat{u}_{i,j}}{\Delta t}
=&
p_2(i,j,u(i,j))\frac{\hat{u}_{i+1,j}-2\hat{u}_{i,j} + \hat{u}_{i-1,j}}{\Delta x^2}\\
&+p_1(i,j,u(i,j))\frac{\hat{u}_{i+1,j}-\hat{u}_{i-1,j}}{2\Delta x}\\
&+ p_0(i,j,u(i,j))
\end{split}
\end{equation}
For ease of exposition we omit some details and refer the reader to  \cite{strikwerda2004finite} for a complete explanation. 

\subsection{PDExplain Inference} 

\label{ssec:method.PDExplain_inference}
We begin by outlining our inference process, presented in Fig.~\ref{fig:method.inference}.
The input to this process is a patch $u^{c}(x,t)$, where $x\in[0,L]$, $t\in[0,t_0]$ and an operator $F$ (e.g., the one introduced in Eq.~\eqref{eq:general_PDE} for $k=2$). The patch is fed into the PDExplain component, which generates the estimated coefficients $\hat{p}$, in the example, $\hat{p}=(\hat{a},\hat{b},\hat{c})$. The PDE solver then uses this estimate to predict the complete signal, $\hat{u}(x,t)$, $x\in[0,L]$, $t\in[t_0,T]$. An important feature of our approach is the explicit prediction of the coefficient functions, which contributes to the explainability of the solution.

The patch $u^{c}(x,t)$ is a partial signal that serves as an initial condition for the prediction and also represents the dynamics of the signal for estimating the PDE coefficients. In the sequel we refer to it as ``context''. The ratio of the context is denoted by $\rho$, such that $t_0=\rho T$, and is a hyper-parameter of our algorithm. We discuss the effect of context size in Section~\ref{ssec:experiments.context}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\columnwidth]{Figures/inference.png}
    \caption{Inference process. Dashed line is initial condition, supplied to the PDE solver together with the estimated coefficients and operator $F$.}\label{fig:method.inference}
\end{figure}

\subsection{PDExplain Training} \label{ssec:method.PDExplain_training}
The training process is presented in Fig.~\ref{fig:method.training}. Its input is a dataset $U$ that consists of $N$ complete signals $\{u_i(x,t)\}_{i=1}^N$ which are solutions of $N$ PDEs that share an operator $F$ but have unique coefficient vectors $\{p_i\}_{i=1}^N$. The support of the signals is $x\in[0,L]$ and $t\in[0,T]$. The loss we minimize is a weighted sum of two components: (i) the functional loss as defined in Eq. \eqref{eq:coefficient_estimator_loss} and (ii) the autoencoder reconstruction loss (AE; \citealp{hinton2006reducing}), which is defined in Eq. \eqref{eq:autoencoder_loss}. 

The PDExplain scheme is composed of two components: (1) an encoder and (2) a coefficient estimator, both of which proved essential over the course of our work. The encoder's goal is to capture the dynamics driving the signal $u_i$, thus creating a compact representation for the coefficient estimator. The encoder is trained on patches $u^c_i$ randomly taken from signals $u_i$ belonging to the train set. Each patch is of size $t_0 \times L$. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\columnwidth]{Figures/training.png}
    \caption{Training process.}\label{fig:method.training}
\end{figure}


The encoder loss is the standard autoencoder reconstruction loss \cite{hinton2006reducing}, namely the objective is
\begin{align}
\label{eq:autoencoder_loss}
\mathop{\min}\limits_{\theta,\phi}\mathcal{L_{\textrm{AE}}} = 
\mathop{\min}\limits_{\theta,\phi}\sum_{i=1}^N \textrm{loss}(u_i^c - f_{\theta}(g_{\phi}(u_i^c))),
\end{align}
where $f_{\theta}$ is the decoder, $g_{\phi}$ is the encoder and $\textrm{loss}(\cdot,\cdot)$ is a standard loss function (e.g., $L^2$ loss).

The second component is the coefficient estimator, whose input is the encoded context. The estimated coefficients output by this component, together with the operator $F$, form the functional objective:
\begin{align}
\label{eq:coefficient_estimator_loss}
\mathop{\min}\limits_{\omega}\mathcal{L_{\textrm{coef}}} = 
    \mathop{\min}\limits_{\omega}\sum_{i=1}^N 
    \left\Vert F(\hat{p}_\omega,u^c_i)\right\Vert^2,
\end{align}
where $\omega$ represents the parameters of the coefficient estimator network, and $\hat{p}$ is the estimator of $p$, acquired by applying the network $h_\omega$ to the output of the encoder.

The two components are trained simultaneously, and the total loss is a weighted sum of the losses in Eq.~\eqref{eq:autoencoder_loss} and Eq.~\eqref{eq:coefficient_estimator_loss}
\begin{align}
    \mathcal{L} = \alpha \cdot \mathcal{L}_{\textrm{AE}} + (1-\alpha) \cdot \mathcal{L}_{\textrm{coef}},
\end{align}
where $\alpha\in(0,1)$ is a hyper-parameter.

Both networks are simple fully connected networks. Naturally, they can be further optimized to improve performance, but the results presented in Section~\ref{sec:experiments} attest to the robustness of our approach. A key feature of the proposed method is lack of supervision, in the sense that unlike many previous approaches, the coefficient values of the PDEs represented in the train set are \emph{unknown even at training time}. The algorithm succeeds in its task by combining an autoencoder architecture with the mechanistic knowledge contained in the PDE functional form.
We experimented with removing the decoder and training the networks using the functional loss alone, but results proved to be poor.

To summarize this section, we present the inference scheme in Algorithm~\ref{alg:algorithm1}, and the full training algorithm  in Algorithm~\ref{alg:algorithm2}. 
\begin{algorithm}[tb]
   \caption{PDExplain inference scheme}
   \label{alg:algorithm1}
\begin{algorithmic}
    \STATE {\bfseries Input:} patch $u^{c}(x,t) $,  operator $F$, trained networks: decoder $g_{\phi}$, coefficient estimator $h_{\omega}$
    \STATE $\hat{p} \gets h_{\omega}(g_{\phi}(u^c))$
    \STATE $\hat{u} \gets \textrm{PDE\_solve}(F, \hat{p}, u^c(x,t=t_0))$
    \STATE return $\hat{u}, \hat{p}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[tb]
   \caption{Algorithm for training PDExplain}
   \label{alg:algorithm2}
\begin{algorithmic}
   \STATE {\bfseries Input:} dataset $U$, operator $F$, context ratio $\rho$, loss weight $\alpha$, number of epochs $N_{e}$
   \STATE {\bfseries Init:} random weights in encoder $g_{\phi}$, decoder $f_\theta$, coefficient estimator $h_{\omega}$
   \FOR{epoch in $N_{e}$}
    \STATE $\mathcal{L} \gets 0$
    \STATE $U^c_N \gets$ {$N$ random patches, one from each $u_i\in U$}
    \FOR{$u_i^c$ in $U^c_N$}
        \STATE $\hat{p}_i\gets h_{\omega}(g_{\phi}(u_i^c))$
        \STATE $\mathcal{L}_{\textrm{AE}} \gets (u_i^c-f_\theta(g_{\phi}(u_i^c)))^2$
        \STATE $\mathcal{L}_{\textrm{coef}} \gets \left\Vert F(\hat{p}_i,u_i^c)\right\Vert^2$
        \STATE $\mathcal{L} \gets \mathcal{L} + \alpha \cdot \mathcal{L}_{\textrm{AE}} + (1-\alpha) \cdot \mathcal{L}_{\textrm{coef}}$
    \ENDFOR
    \STATE $\phi, \theta, \omega \gets \mathop{\arg \min \mathcal{L}}$
   \ENDFOR
\end{algorithmic}
\end{algorithm}

