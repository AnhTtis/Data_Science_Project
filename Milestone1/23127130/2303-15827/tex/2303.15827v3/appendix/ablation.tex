In this section, we provide additional ablation studies to demonstrate how different modifications to the algorithm affect results, both in terms of signal prediction and in coefficient estimation.
We start by analyzing the two parameters that characterize the CONFIDE algorithm: train set size and context ratio, using the second order PDE from Section~\ref{ssec:pde_exp1}, and we continue by demonstrating how removing the decoder, or simply removing the initial conditions from the decoder, affect the algorithm's performance.

\subsection{Train set size}\label{ssec:app.train}
The train set size corresponds to $N$, the number of samples in dataset $U$ of Algorithm~\ref{alg:algorithm1}. Figure~\ref{fig:experiments.pred_err_train} presents the decrease in the prediction (panel a) and parameter error (panel b) as we increase the train set size. This attests to the generalization achieved by the CONFIDE architecture: as the train set grows and includes more samples with different values of coefficients, the ability to accurately estimate a new sample's parameters and predict its rollout improves. In this set of experiments, $3,000$ samples are generally enough to achieve a minimal error rate.

\subsection{Context Ratio}\label{ssec:app.context}
Another hyper-parameter of our system is the context ratio. Figure~\ref{fig:experiments.pred_err_context} presents the results of an experiment in which we vary its value as defined in Section~\ref{ssec:method.PDExplain_training}. Simply put, as the context size increases, CONFIDE encodes more information regarding the input signal's dynamics, thus the improvement in signal and parameter value prediction. The error decreases rather quickly, and a context ratio of $0.15 - 0.2$ suffices for reaching a very low error, as is evident from the plots.

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/experiments/pred_error_by_train_set_size.pdf}
         \caption{}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/experiments/params_error_by_train_set_size.pdf}
         \caption{}
     \end{subfigure}
     \hfill
    \caption{Constant coefficients PDE: \textbf{(a)} Prediction error of signal vs. train set size and \textbf{(b)} estimation error of parameter values vs. train set size. The error is calculated on a test set of 1000 samples. }
    \label{fig:experiments.pred_err_train}
\end{figure}

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/experiments/pred_error_by_context_size.pdf}
         \caption{}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/experiments/params_error_by_context_size.pdf}
         \caption{}
     \end{subfigure}
     \hfill
    \caption{Constant coefficients PDE: \textbf{(a)} Prediction error vs. $\rho$ and \textbf{(b)} estimation error of parameter value vs. $\rho$. The error is calculated on a test set of 1000 samples.}\label{fig:experiments.pred_err_context}
\end{figure}

\subsection{Autoencoder}
In this section, we demonstrate the effect that adding a decoder network has on CONFIDE.
To this end, we evaluate three different scenarios:
\begin{itemize}
\item \textbf{CONFIDE.} Using a decoder followed by a reconstruction loss, and feeding the initial conditions in addition to the latent vector (demonstrated in the text as initial conditions aware autoencoder)
\item \textbf{AE-IC.} Similarly, using a decoder followed by a reconstruction loss, but the autoencoder is not initial-conditions aware.
\item \textbf{No-AE.} The network trains solely on the PDE loss, without the decoder part (i.e., by setting $\alpha=0$).
\end{itemize}
Results of the three approaches on the constant PDE dataset are shown in Fig.~\ref{fig:ae_ablation}.
When comparing a setup with no decoder part (i.e., No-AE) with a setup that has a decoder, but does not use the initial conditions as a decoder input (i.e., AE/IC), we observe that merely adding a decoder network might have a negative effect on the results, especially when analyzing the parameter estimation results.
One reason for this may be that the neural network needs to compress the observed signal in a way that should both solve the PDE and reconstruct the signal. This modification of latent space has a negative effect in this case.
When also adding the initial conditions as an input to the decoder (i.e., the standard CONFIDE), we observe significant MSE improvement in both signal prediction and parameter estimation ($\sim$35\% improvement in both).
This result suggests that adding the initial conditions aware autoencoder enables the networks to learn a good representation of the dynamics of the observed signal in its latent space.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.5\textwidth]{appendix/figures/ablation.pdf}
    \caption{Ablation study on the constant coefficients PDE dataset. The Y axis shows the percentage difference between the different approaches and the standard CONFIDE one (thus it scores 0\%). We demonstrate the effects on both signal prediction (blue), and parameter estimation (red).}
    \label{fig:ae_ablation}
\end{figure*}
