% We devote this section to two types of analyses: (a) a comparison of our approach to other solutions, and (b) an analysis of our approach in different scenarios.

% The dataset we use includes $10,000$ samples of size $100\times 40[t\: \textrm{points} \times x\: \textrm{points}]$, 


We devote this section to analyse and compare our approach to other solutions, on four different systems of PDEs: (1) constant coefficients, (2) Burgers' equations, (3) 2D-FitzHugh-Nagumo, and (4) 2D-Navier-Stokes equation.
For each PDE task, we created a dataset of signals generated from a PDE with different coefficients.
We could not use off-the-shelf datasets, such as those appearing in PDEBench \citep{takamoto2022pdebench}, since each of the datasets there is generated from a single constant function (i.e., all data samples have the same context). We used well-known equations, therefore our datasets can serve as a benchmark for the emerging field of contextual PDE modelling. 
We stress the fact that the test set contains signals generated by PDEs with coefficient vectors that \emph{do not} appear in the training data, hence demonstrating different dynamics than the ones the model observed during training. 
In that sense, the task at hand is a zero-shot prediction problem. 
More information about dataset creation can be found in the appendix.


% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\columnwidth]{Figures/experiments/burger_demonstration_single_plots_621.pdf}
%     \caption{A solution of the Burgers' equation. The black plot in each figure displays the ground truth. Rows correspond with the predicted solution by the respective algorithm (top row for CONFIDE displayed in red). Each column shows the solution at a different time point. The rightmost column shows the solution at $t=100$ zoomed to demonstrate the differences.}\label{fig:experiments.burgers_demo}
% \end{figure}



We benchmark the performance of CONFIDE against several state of the art approaches:
\begin{enumerate}
    \item Neural ODE, based on the algorithm suggested by \citet{chen2018neural}, Section 5.1 (namely, Latent ODE).
    \item Fourier Neural Operator (FNO), introduced by \citet{li2020fourier}.
    \item U-Net, as presented by \citet{gupta2022towards}.
    \item DINo, as presented by \citet{yin2022continuous}.
\end{enumerate}
Additional details regarding the implementation of baselines can be found in Section ~\ref{app:implementation}.

\begin{figure*}[t]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         % \includegraphics[width=\columnwidth]{Figures/experiments/burger_pred_rollout_error_by_time.pdf}
         \includegraphics[width=\columnwidth]{Figures/experiments/burger_demonstration_single_plots_621.pdf}
         \caption{}
        \label{fig:experiments.burger_demo}
     \end{subfigure}
     % \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/experiments/burger_params_b_prediction.pdf}
         \caption{}
         \label{fig:experiments.burger_params_b}
     \end{subfigure}
     \hfill
    \caption{Burgers' PDE:
    \textbf{(a)} A solution of the Burgersâ€™ equation. The black plot in each figure displays the ground truth. Rows correspond with the predicted solution by the respective algorithm (top row for CONFIDE displayed in red). Each column shows the solution at a different time point. The rightmost column shows the solution at t = 100 zoomed to demonstrate the differences.
    \textbf{(b)} Estimation of the coefficient function $b(x,t,u)$ of the Burgers' equation from \eqref{eq:pde_exp2}. CONFIDE manages to accurately estimate the spatio-temporal dynamics of the coefficient, based on a context ratio of $\rho=0.2$.}
    \Description{Results on Burgers' PDE. on the left an example, how it evolves over time, and how CONFIDE is closest to the ground truth. On the right, an example of the coefficient function inferece.}
\end{figure*}

\subsection{Second Order PDE with Constant Coefficients}\label{ssec:pde_exp1}
The first family of PDEs used for our experiments is: 
\begin{align}
\label{eq:pde_exp1}
        \fder{u}{t}=a \sder{u}{x} + b \fder{u}{x} + c,
\end{align}
where $p=(a,b,c)$ are constants but differ between signals. 
Figure~\ref{fig:experiments.pred_rollout} demonstrates the clear advantage of our approach, which increases with the prediction horizon (note the logarithmic scale of the vertical axis, representing the MSE of prediction). Since CONFIDE harnesses both mechanistic knowledge and training data, it is able to predict the signal $\hat{u}(x,t)$ several timesteps ahead, while keeping the error to a minimum. 




Another result for this set of experiments appears in Figure~\ref{fig:experiments.param_a_r2}. Here, we plot the estimated value of parameter $a$ of \eqref{eq:pde_exp1}, against its true value. The plot and the high value of $R^2$ demonstrate the low variance of our prediction, with a strong concentration of values along the $y=x$ line.

Section~\ref{sec:app.ablation} presents the results of an ablation study on the hyper-parameters of CONFIDE for this equation.

% \begin{figure*}[ht!]
%     \centering
%     \includegraphics[width=0.95\textwidth]{Figures/experiments/burger_demonstration_621.pdf}
%     \caption{A solution of the Burgers' equation (prediction MSE appears in the title). The left panel displays the ground truth (GT), next to it the error-minimizing CONFIDE prediction. Neural ODE, FNO and U-Net achieve errors that are 1-3 orders of magnitude larger, exhibiting considerable deterioration over time.}\label{fig:experiments.burgers_demo}
% \end{figure*}







% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=0.8\textwidth]{Figures/experiments/pred_error_by_train_set_size.pdf} \\
%     (a) \\
%     \includegraphics[width=0.8\textwidth]{Figures/experiments/params_error_by_train_set_size.pdf} \\
%     (b) \\
%     \caption{Constant coefficients PDE: (a) Prediction error of signal vs. train set size and (b) estimation error of parameter values vs. train set size. The error is calculated on a test set of 1000 samples. } \label{fig:experiments.pred_err_train}
% \end{figure}

% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=0.8\textwidth]{Figures/experiments/pred_error_by_context_size.pdf} \\
%     (a) \\
%     \includegraphics[width=0.8\textwidth]{Figures/experiments/params_error_by_context_size.pdf} \\
%     (b) \\
%     \caption{Constant coefficients PDE: (a) Prediction error vs. $\rho$ and (b) estimation error of parameter values error vs. $\rho$. The error is calculated on a test set of 1000 samples.}\label{fig:experiments.pred_err_context}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.6\textwidth]{Figures/experiments/pred_rollout_error_by_time.pdf}
%     \caption{Constant coefficients PDE: Prediction error as prediction horizon increases, for different approaches. PDExplain, in red, is our approach. No PDE, in blue, learns an encoding for the training data and applies a purely data-driven prediction. DI, in green, corresponds to direct identification of the PDE coefficients from the datapoint, followed by solving the PDE (no training). The last approach, PDE-RHS, learns an encoding for the training data and predicts the right-hand-side of an equation which is then solved to yield an estimate of the signal. The dashed vertical line is the value of the context ratio used for this experiment (fixed for all approaches), $\rho=0.21$.}\label{fig:experiments.pred_rollout}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.85\textwidth]{Figures/experiments/params_a_r2.pdf}
%     \caption{Constant coefficients PDE: estimated value of the $\partial^2 u / \partial^2 x$ coefficient vs. ground truth, for entire test set ($R^2 = 0.93)$.}\label{fig:experiments.param_a_r2}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.85\textwidth]{Figures/experiments/burger_pred_rollout_error_by_time.pdf}
%     \caption{Prediction error as prediction horizon increases, for different approaches, Burgers' PDE. PDExplain, in red, is our approach. No PDE, in blue, learns an encoding for the training data and applies a purely data-driven prediction. The last approach, PDE-RHS, learns an encoding for the training data and predicts the right-hand-side of an equation which is then solved to yield an estimate of the signal. The dashed vertical line is the value of the context ratio used for this experiment (fixed for all approaches), $\rho=0.2$. The DI approach cannot be applied to equations with non-constant coefficients and is therefore omitted from the comparison.}\label{fig:experiments.burger_pred_rollout}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.85\textwidth]{Figures/experiments/burger_params_b_prediction.pdf}
%     \caption{Estimation of the coefficient function $b(x,t,u)$ of the Burgers' equation, presented in Eq.~\eqref{eq:pde_exp2}. PDExplain manages to accurately estimate the spatio-temporal dynamics of the coefficient, based on a context ratio of $\rho=0.2$.}\label{fig:experiments.burger_params_b}
% \end{figure}


\subsection{Burgers' equation}
Another family of PDEs we experiment with is the quasi-linear Burgers' equation, whose general form is
\begin{align}
    \label{eq:pde_exp2}
        \fder{u}{t}=a\sder{u}{x} + b(u)\fder{u}{x},
\end{align}
where $b(x,t,u)=-u$, as presented in \cite{bateman1915some}. We note that this equation is quasi-linear since its drift coefficient $b(x,t,u)$ depends on the solution $u$ itself. The dataset for our experiments consists of 10000 signals with different values of $a$ and the same $b(u)=-u$, both unknown to the algorithm a priori.
We begin with a demonstration of a signal $u(x,t)$ and its prediction $\hat{u}(x,t)$ in Figure~\ref{fig:experiments.burger_demo}. As can be seen both visually and from the value of the MSE (in each panel's title), our approach yields a prediction that stays closest to the ground truth (GT), even as the prediction horizon (vertical axis) increases.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figures/experiments/fn2d_pred_demonstration_9.pdf}
    \caption{2D-FitzHugh-Nagumo PDE: Figures in the top row show the ground truth of $R_v$ for different time points, and the rows below show the estimation of it by the different approaches.
    CONFIDE estimates $R_v$ directly and near-perfectly recovers the unknown part of the PDE even as the prediction horizon increases. For the other algorithms we evaluated $R_v=u-v$ from the predictions of $u$ and $v$.}\label{fig:experiments.fn2d}
    \Description{Results on predicting the results of FitzHugh-Nagumo PDE, where CONFIDE is closest to the ground truth.}
\end{figure}

In Figure~\ref{fig:experiments.burger_params_b} we focus on the ability to accurately predict coefficient functions with spatio-temporal dynamics, in this case: the coefficient $b(x,t,u)$ of \eqref{eq:pde_exp2}. The panels correspond to different points in time, showing that the coefficient estimator tracks the temporal evolution successfully.

\subsection{FitzHugh-Nagumo equations}
The next family of PDEs we examine is the FitzHugh-Nagumo PDE \citep{klaasen1984stationary} consisting of two equations:
\begin{align}
    \label{eq:fn2d_eq}
        \fder{u}{t} = a \Delta u + R_u(u,k,v), \qquad
        \fder{v}{t} = b \Delta v + R_v(u,v),
\end{align}
where $a$ and $b$ represent the diffusion coefficients of $u$ and $v$, and $\Delta$ is the Laplace operator.
For the local reaction terms, we follow \citet{yin2021augmenting} and set $R_u(u,k,v)=u-u^3-k-v$, and $R_v(u,v)=u-v$. The PDE state is $(u,v)$, defined on the 2-D rectangular domain $(x,y)$ with periodic Neumann boundary conditions.

The dataset created for this task consists of 1000 signals, each with a different value of $k$. 
We compare the prediction generated by CONFIDE to those yielded by other approaches, and present a typical result in Fig.~\ref{fig:experiments.fn2d}. In Fig.~\ref{fig:experiments.FN2D_error} we present the prediction error as a function of the prediction horizon, once again comparing CONFIDE to the baselines.

\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{Figures/experiments/fn2d_pred_rollout_error_by_time.pdf}
\caption{2D-FitzHugh-Nagumo PDE: prediction error as horizon increases, for different approaches.}
\label{fig:experiments.FN2D_error}
\Description{Prediction results on FitzHugh-Nagumo PDEs.}
\end{figure}

\subsection{Navier-Stokes equation}
For the last family of PDEs, we follow \citet{yin2022continuous} and examine the Navier-Stokes equations \citep{stokes1851effect} which correspond to incompressible fluid dynamics and have the form of
\begin{align}
    \label{eq:ns}
        \fder{w}{t} = -u\nabla w + \nu \Delta u + f, \quad
        w=\nabla\times u, 
        \quad \nabla u = 0,
\end{align}
where $u$ is the velocity field, $w$ the vorticity, $\nu$ the unknown viscosity coefficient and $f$ is a constant forcing term.
The PDE state $w$ is defined over the 2-D rectangular domain $(x,y)$ with periodic boundary conditions.
The dataset created for this task consists of 1000 signals, with different values of the viscosity $\nu$.
We note that typically \citep{li2020fourier,yin2022continuous,gupta2022towards}, 
the viscosity coefficient is treated as having a single constant value among all signals in the dataset.
In this work we increase the task difficulty by creating a dataset comprised of signals that have different viscosity values.
For each signal in the dataset (both train and test) we sample a different viscosity value uniformly from $\nu\sim U[1,2]\cdot 10^{-3}$. 
We compare the prediction generated by CONFIDE to other baselines and present a typical result in Fig.~\ref{fig:experiments.ns}. In Fig.~\ref{fig:experiments.NS_error} we present the prediction error as a function of the prediction horizon. 

We summarize the results of experiments for signal prediction across all setups and approaches in Table ~\ref{tbl:exp.summary}. The table includes results for CONFIDE, all baselines, and also a variant of CONFIDE which we refer to as CONFIDE-0. This zero-knowledge variant is applicable when we know that the signal obeys some differential operator $F$, but have no details regarding the actual structure of $F$. Thus, CONFIDE-0 does not estimate the equation parameters, and only yields a prediction for the signal, utilizing our context-based architecture. We elaborate further in Appendix~\ref{app:implementation}.
We note that Neural-ODE and DINO, which are integration-based approaches, converged to a solution resembling the average of the observed signal without any dynamics evolution over time.
This issue has also been demonstrated and discussed in several other related works \citep{abrevaya2023goku,turan2021multiple,iakovlev2022latent}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{Figures/experiments/ns_demonstration_2.pdf}
    \caption{2D-Navier-Stokes: Figures in the top row show the ground truth, i.e., the PDE state $w$ for different time points, while the rows below show its estimation by the different approaches. All approaches observe the first 50 time points and predict the next 150.
    CONFIDE near-perfectly predicts the given signal even when the horizon increases. 
    }
    \label{fig:experiments.ns}
    \Description{Navier-Stokes example results at different time points. All approaches observe the first 50 time points and predict the next 150.
    CONFIDE near-perfectly predicts the given signal even when the horizon increases.}
\end{figure}

\begin{table}[ht!]
\caption{Coefficient estimation error for different experimental setups: constant coefficients, Burgers' equation, two-dimensional FitzHugh-Nagumo and two-dimensional Navier-Stokes. The variance is calculated over the entire test set, namely 1000 signals for the first two setups and 100 signals for the last two.}
\centering
        \begin{tabular}{cc}
            \toprule
            Setup & Coefficient estimation error \\
            \midrule
            Constant coeff. & $0.0095 \pm 0.0131$ \\
            Burgers' & $0.0454 \pm 0.0333$ \\
            FitzHugh-Nagumo-2D & $0.0075 \pm 0.0123$ \\
            Navier-Stokes-2D & $1.5\cdot 10^{-8} \pm 1.0\cdot 10^{-8}$ \\
            \bottomrule
        \end{tabular}
\label{tbl:experiments.all_error}
\end{table}
    


\begin{table*}[t]
\caption{Result summary for the signal prediction task, on all four PDE systems, together with two OOD experiments. The numbers represent signal prediction error at the end of the prediction horizon, averaged over the entire test set.}
% \vskip 0.15in
\label{tbl:exp.summary}
\begin{center}
% \begin{small}
\begin{tabular}{lcccc|cc}
\toprule
Method & \shortstack{Constant \\ coefficients} & Burgers' & \shortstack{FitzHugh-\\Nagumo} & \shortstack{Navier-\\Stokes} & \shortstack{Burgers' OOD \\ Initial conditions} & \shortstack{Burgers' OOD \\ Coefficients} \\
\midrule
% \multirow{5}{*}{\shortstack{FitzHugh-\\Nagumo}} 
CONFIDE & $0.0023 \pm 0.0036$ & $0.0008 \pm 0.0011$ & $0.0083 \pm 0.0177$ & $0.0033 \pm 0.0027$ & $0.0010 \pm 0.0012$ & $0.0074 \pm 0.0100$\\
CONFIDE-0 & $0.0079 \pm 0.0218$ & $0.0009 \pm 0.0016$ & $0.0845 \pm 0.0978$ & $0.0173 \pm 0.0355$ & $0.0020 \pm 0.0022$ & $0.0057 \pm 0.0091$\\
Neural-ODE & $0.0680 \pm 0.0905$ & $0.0272 \pm 0.0627$ & $0.2944 \pm 0.2293$ & $0.1334 \pm 0.1391$ & $0.0133 \pm 0.0208$& $0.0423 \pm 0.0649$\\
FNO & $0.0538 \pm 0.0680$ & $0.9351 \pm 0.3091$ & $2.5727 \pm 17.732$ & $4.4223 \pm 5.5352$ & $0.9367 \pm 0.2322$ & $0.9646 \pm 0.3304$\\
Unet & $0.0160 \pm 0.0199$ & $0.0016 \pm 0.0023$ & $0.1293 \pm 0.1748$ & $1.6712 \pm 0.6500$ & $0.0015 \pm 0.0024$ & $0.0096 \pm 0.0121$\\
DINO & $0.0850 \pm 0.0994$ & $0.0142 \pm 0.0206$ & $0.1651 \pm 0.1279$ & $0.1378 \pm 0.1462$ & $0.0142 \pm 0.0189$ & $0.0336 \pm 0.0334$\\
\bottomrule
\end{tabular}
% \end{small}
\end{center}
\end{table*}


\subsection{Out-Of-Distribution Data}
In this subsection we provide additional experiments conducted on out-of-distribution (OOD) data. These experiments were selected to demonstrate how CONFIDE can handle observations that are significantly different than the data in the train set.
We divide the OOD experiments into two parts: (1) the initial conditions observed are not smooth and have some discontinuity, and (2) the parameters used to generate the signals in the test set are sampled from a different distribution than the one used in the train set.

\paragraph{\textbf{Non-smooth initial conditions}}
In this first benchmark, we demonstrate how CONFIDE handles the case where the test data has a discontinuity point in the test set, while it was trained on continuous data only.
The importance of this test is mainly because CONFIDE evaluates the spatio-temporal derivatives of the signal numerically using a finite-differences approach.
This computation might result in very high derivatives in these non-smooth locations and interfere with the ability of the algorithm to provide reliable predictions.
For this task, we generated a new test set based on the Burgers' equation experiment, but the initial conditions are sampled to demonstrate discontinuity in $u(t=0,x=L/2)$.
We stress that the train-set is still the original one, since our goal is to  test whether CONFIDE is able to handle OOD data, which, in this case, comes in the form of OOD initial conditions.
As shown in Table~\ref{tbl:exp.summary}, CONFIDE's prediction error remains low, suggesting that it successfully predicts the given observations and scores close to the original score on the original burgers' test-set.


\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{Figures/experiments/ns_pred_rollout_error_by_time}
\caption{2D-Navier-Stokes PDE: prediction error as horizon increases, for different approaches.}
\label{fig:experiments.NS_error}
\Description{Navier stokes equations - prediction error.}
\end{figure}


\paragraph{\textbf{OOD coefficients}}
In the second benchmark, we demonstrate how CONFIDE handles the case where the observed signal in the test set is generated from a PDE with coefficients that come from a distribution different from the ones in the train set.
For this task, we generated a new test set based on the Burgers' equation experiment, where the coefficient $a$ is sampled from $u\sim U[2,4]$ instead of $u\sim U[1,2]$ as in the train set. 
This modification in the coefficient distribution, results in generated signals that might be significantly different than the ones observed in the train set.
As shown in Table~\ref{tbl:exp.summary}, CONFIDE continues to provide good results compared to other baselines.
We note that since CONFIDE learns to output coefficients only in the range of the coefficients in the train set, it projects the observed signal to the range of coefficients in the train set so that it best describes the observed signal.

\begin{table}[H]
\centering
        \begin{tabular}{lcc}
            \toprule
            Algorithm & \shortstack{Prediction MSE \\ OOD initial conditions} & \shortstack{Prediction MSE \\ OOD coefficients}\\
            \midrule
    CONFIDE & $0.0010 \pm 0.0012$ & $0.0074 \pm 0.0100$\\
    Neural-ODE & $0.0133 \pm 0.0208$ & $0.0423 \pm 0.0649$\\
    FNO & $0.9367 \pm 0.2322$ & $0.9646 \pm 0.3304$\\
    Unet & $0.0015 \pm 0.0024$ & $0.0096 \pm 0.0121$\\
                \bottomrule
        \end{tabular}
\caption{Results summary on the two OOD benchmarks for different approaches.}
\label{tbl:app_ood}
\end{table}

\subsection{CONFIDE vs Neural ODE (Finite-Differences vs Integration)}
A key part in the CONFIDE algorithm is its use of finite-differences to evaluate the spatio-temporal derivatives.
As we demonstrate, CONFIDE can successfuly use this approach to provide both coefficient estimation and reliable predictions.
However, another important advantage that a finite-differences approach might have over ``integration'' approaches (such as the adjoint method used in Neural-ODE \citep{chen2018neural} and DINo \citep{yin2022continuous}) is the amount of time required for training the model.

To demonstrate this effect we created a toy example based on an ODE of a frictionless single pendulum: $\ddot{\theta} = - g/l \cdot \sin(\theta)$,
where $g$ is the gravitational parameter, $\theta$ is the angle of the pendulum, and $l$ is the length of the pendulum.
The two algorithms compared are CONFIDE and an ODE-aware version of Neural-ODE, where it is also given the ODE form of the pendulum.
Both algorithms are presented with the same information and have the same goals: estimating the length of the pendulum on a given signal $l$, and predicting the future of the observed signal at $t>T$.

CONFIDE evaluates the time derivative of the observed signal ${d^2\theta}/{dt^2}$ via finite-differences, and forces the derivative to be similar to the rest of the ODE. 
Neural-ODE uses the initial value ($\theta(t=0)$) and $\hat{l}$ to generate the signal $\hat{\theta}(t=0,\ldots,T)$ by integrating via an ODE-solver, and then optimizes the generated signal to match the observed one.
We trained both algorithms using the exact same setting, and observed that not only were CONFIDE's results better, but it also trained significantly faster. Specifically, CONFIDE's training time was 3.6 seconds, and Neural-ODE's training time was 159.2 seconds, making CONFIDE \textbf{$\sim 44$ times faster}.

\subsection{Autoencoder ablation study}
\label{sec:app.ablation}
In this section, we demonstrate the effect that adding a decoder network has on CONFIDE.
To this end, we evaluate three different scenarios:
\begin{itemize}
\item \textbf{CONFIDE.} Using a decoder followed by a reconstruction loss, and feeding the initial conditions in addition to the latent vector (demonstrated in the text as initial conditions aware autoencoder)
\item \textbf{AE-IC.} Similarly, using a decoder followed by a reconstruction loss, but the autoencoder is not initial-conditions aware.
\item \textbf{No-AE.} The network trains solely on the PDE loss, without the decoder part (i.e., by setting $\alpha=0$).
\end{itemize}
Results of the three approaches on the constant PDE dataset are shown in Fig.~\ref{fig:ae_ablation}.
When comparing a setup with no decoder part (i.e., No-AE) with a setup that has a decoder, but does not use the initial conditions as a decoder input (i.e., AE/IC), we observe that merely adding a decoder network might have a negative effect on the results, especially when analyzing the parameter estimation results.
One reason for this may be that the neural network needs to compress the observed signal in a way that should both solve the PDE and reconstruct the signal. This modification of latent space has a negative effect in this case.
When also adding the initial conditions as an input to the decoder (i.e., the standard CONFIDE), we observe significant MSE improvement in both signal prediction and parameter estimation ($\sim$35\% improvement in both).
This result suggests that adding the initial conditions aware autoencoder enables the networks to learn a good representation of the dynamics of the observed signal in its latent space.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\columnwidth]{appendix/figures/ablation.pdf}
    \caption{Ablation study on the constant coefficients PDE dataset. The Y axis shows the percentage difference between the different approaches and the standard CONFIDE one (thus it scores 0\%). We demonstrate the effects on both signal prediction (blue), and parameter estimation (red).}
    \label{fig:ae_ablation}
    \Description{Ablation study that shows that the autoencoder with initial conditions is the best architecture.}
\end{figure}