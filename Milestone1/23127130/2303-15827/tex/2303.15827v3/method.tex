The data we handle is a set of spatio-temporal signals generated by an underlying PDE, only the form of which is known. The coefficient functions determining the exact PDE are unknown and may be different for each collection of data. 
Our goal is to estimate these coefficient functions and provide reliable predictions of the future time steps of the observed phenomenon.
The proposed method comprises three subsequent parts: (1) Creating a compact representation of the given signal, (2) estimating the PDE coefficients, and (3) solving the PDE using the acquired knowledge. For ease of exposition we focus on parabolic PDEs in this section, however the extension to other types of PDEs is straightforward.

\subsection{Problem Formulation} 
We now define the problem formally.
Let $U(x,t)$ denote a spatio-temporal function defined over some compact spatial domain $\Omega \subseteq \mathbb{R}^d$, where $d$ is the number of spatial variables, and a temporal domain $\mathbb{R}$. $U(x,t)$ maps between points in the spatial domain $x\in\Omega$ at some point in time $t$ to an $n$-dim vector, where $n$ is the number of observed variables.
In other words, $U(x,t):\Omega\times\mathbb{R}\rightarrow\mathbb{R}^n$. 
In addition, the initial value of the function $U(x,t=0)$ changes between observed signals, therefore sampled from some unknown probability function $U(x,t=0)\sim P_{u_0}$.
An observed signal $u(x,t)$ is therefore a projection of the function $U(x,t)$ on a finite discrete observation grid and on discrete times.
In our formulation, we make the problem harder by considering the case where each signal $u(x,t)$ could also originate from a PDE with different coefficients, thus differing between observed signals.
This means that unlike most related works, in this work we assume that every observed signal corresponds to a different instance of the PDE family.
We refer to the signals with different coefficients as $u^c(x,t)$, where $c$ stands for \emph{context}, which changes between observed signals.
For example, we might know that $u^c(x,t)$ follows the Navier-Stokes equations, but some coefficients might change between observations (like the viscosity of the fluid).

Generally, $u^c(x,t)$ could obey any PDE, but in this work we focus on parabolic PDEs, hence the signal $u^c(x,t)$ is the solution of a $k$-th order PDE of the general form
\begin{align} \label{eq:general_PDE}
    \fder{u}{t}= \sum_{l=1}^k p_l(x,t,u) \frac{\partial u^l}{\partial x^l}+ p_0(x,t,u),
\end{align}
with a vector of coefficient functions $p=(p_0, \ldots,p_k)$. We adopt the notation of \citet{wang2021physics} and refer to a family of PDEs characterized by a vector $p$ as an operator $F(p, u)$, where solving $F(p,u)=0$ yields solutions of the PDE.

The problem we solve is as follows: given an observed signal $u^c(x,t)$, at times $t=0,\ldots,t_0$ that solves a PDE of a \emph{known} operator $F$ with an \emph{unknown} coefficient vector $p$, we would like to (a) estimate the coefficient vector $\hat{p}$ and (b) predict the signal at future times $t=t_0,\ldots,T$, for some $T>t_0$.

Our solution is a concatenation of two neural networks, which we call \emph{CONFIDE}. Its input is an observed signal $u^c(x,t=0,\ldots,t_0)$, and its output is a vector $\hat{p}$. We feed this vector into an off-the-shelf PDE solver together with the operator $F(p,u)$ to obtain the predicted signal $\hat{u}(x,t=t_0,\ldots,T)$. An explanation of our numerical scheme appears in Section \ref{sec:app_num}.


% We now define the problem formally. Let $u(x,t)$ denote a signal with spatial support $x\in[0,L]$ and temporal support $t\in[0,T]$. We refer to this as the \emph{complete} signal. Next, we define $u^c(x,t)$ to be a partial input signal, a \emph{patch}, where its support is $x\in[0,L]$, $t\in[0,t_0]$, $0<t_0<T$. The superscript $c$ stands for context. We assume the signal $u(x,t)$ is the solution of a $k$-order PDE of the general form
% \begin{align} \label{eq:general_PDE}
%     \fder{u}{t}= \sum_{l=1}^k p_l(x,t,u) \frac{\partial u^l}{\partial x^l}+ p_0(x,t,u),
% \end{align}
% with a vector of coefficient functions $p=(p_0, \ldots,p_k)$. We adopt the notation of \citet{wang2021physics} and refer to a family of PDEs characterized by a vector $p$ as an operator $F(p, u)$, where solving $F(p,u)=0$ yields solutions of the PDE.

% The problem we solve is as follows: given a patch $u^c(x,t)$, that solves a PDE of a \emph{known} operator $F$ with an \emph{unknown} coefficient vector $p$, we would like to (a) estimate the coefficient vector $\hat{p}$ and (b) predict the complete signal $\hat{u}(x,t)$ for $0 \le t \le T$.

% Our solution is a concatenation of two neural networks, which we call \emph{CONFIDE}. Its input is a patch, and its output is a vector $\hat{p}$. We feed this vector into an off-the-shelf PDE solver together with the operator $F(p,u)$ to obtain the predicted signal $\hat{u}(x,t)$. An explanation of our numerical scheme appears in Section \ref{sec:app_num}.





% The partial derivatives are estimated using standard numerical schemes for each point in the patch. We choose discretization parameters $\Delta x$ for the spatial axis and $\Delta t$ for the temporal axis where we solve the PDE numerically on the grid points $\{(i\Delta x, j\Delta t)\}_{i=0, j=0}^{N_x, N_t}$ with $L=N_x \Delta x$ and $T=N_t \Delta t$. Let us denote the numerical solution with $\hat{u}_{i,j}$. We use the \emph{forward-time central-space} scheme, so a second order scheme from \eqref{eq:general_PDE} would be
% \begin{equation}
% \begin{split}\label{eq:triangle_scheme}
% \frac{\hat{u}_{i,j+1}-\hat{u}_{i,j}}{\Delta t}
% =&
% p_2(i,j,u(i,j))\frac{\hat{u}_{i+1,j}-2\hat{u}_{i,j} + \hat{u}_{i-1,j}}{\Delta x^2}\\
% &+p_1(i,j,u(i,j))\frac{\hat{u}_{i+1,j}-\hat{u}_{i-1,j}}{2\Delta x}\\
% &+ p_0(i,j,u(i,j))
% \end{split}
% \end{equation}
% For ease of exposition we omit some details and refer the reader to  \cite{strikwerda2004finite} for a complete explanation. 

\subsection{CONFIDE Inference} 

\label{ssec:method.PDExplain_inference}
We begin by outlining our inference process, presented in Fig.~\ref{fig:method.inference}.
The input to this process is an observed signal $u^{c}(x,t)$, defined on some compact spatial domain $\Omega$, for times $t\in[0,t_0]$ and an operator $F$ 
% (e.g., the one introduced in \eqref{eq:general_PDE} for $k=2$). 
The input is fed into the CONFIDE component, which generates the estimated coefficients $\hat{p}$.
For example, taking $k=2$ in the example in \eqref{eq:general_PDE} results in a coefficient vector $\hat{p}=(\hat{a},\hat{b},\hat{c})$. 
The PDE solver then uses this estimate to predict the complete signal, $\hat{u}(x,t)$, $x\in\Omega$, $t\in[t_0,T]$. 
An important feature of our approach is the explicit prediction of the coefficient functions, which contributes to the explainability of the solution.

The observation $u^{c}(x,t=t_0)$ serves as an initial condition for the prediction and also represents the dynamics of the signal for estimating the PDE coefficients. In the sequel we refer to it as ``context''. The ratio of between the observed times and the required prediction time is denoted by $\rho$, such that $t_0=\rho T$, and is a hyper-parameter of our algorithm.
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.9\columnwidth]{Figures/inference.png}
%     \caption{Inference process. Dashed line is initial condition, supplied to the PDE solver together with the estimated coefficients and operator $F$.}\label{fig:method.inference}
% \end{figure}

\begin{algorithm}[htb]
   \caption{CONFIDE inference scheme}
   \label{alg:algorithm1}
\begin{algorithmic}
    \STATE {\bfseries Input:} observation $u^{c}(x,t) $,  operator $F$, trained networks: encoder $g_{\phi}$, coefficient estimator $h_{\omega}$
    \STATE $\hat{p} \gets h_{\omega}(g_{\phi}(u^c))$
    \STATE $\hat{u} \gets \textrm{PDE\_solve}(F, \hat{p}, u^c(x,t=t_0))$
    \STATE return $\hat{u}, \hat{p}$
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[htb]
   \caption{Algorithm for training CONFIDE}
   \label{alg:algorithm2}
\begin{algorithmic}
   \STATE {\bfseries Input:} dataset $\mathcal{D}$, operator $F$, time $t_0$, loss weight $\alpha$, number of epochs $N_{e}$
   \STATE {\bfseries Init:} random weights in encoder $g_{\phi}$, decoder $f_\theta$, coefficient estimator $h_{\omega}$
   \FOR{epoch in $N_{e}$}
    \STATE $\mathcal{L} \gets 0$
    \STATE $\{u_i^c\}_{i=1}^N \gets$ {Random batch of $N$ observations from $\mathcal{D}$}
    \FOR{$u_i^c$ in batch}
        \STATE $\hat{p}_i\gets h_{\omega}(g_{\phi}(u_i^c))$
        \STATE $\mathcal{L}_{\textrm{AE}} \gets (u_i^c-f_\theta(g_{\phi}(u_i^c), u_i(t=0)))^2$
        \STATE $\tau \gets$ Random value from $[0,t_0]$      \STATE $\mathcal{L}_{\textrm{coef}} \gets \left\Vert F(\hat{p}_i,u_i^c(t=\tau))\right\Vert^2$
        \STATE $\mathcal{L} \gets \mathcal{L} + \alpha \cdot \mathcal{L}_{\textrm{AE}} + (1-\alpha) \cdot \mathcal{L}_{\textrm{coef}}$
    \ENDFOR
    \STATE $\phi, \theta, \omega \gets \mathop{\arg \min \mathcal{L}}$
   \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{CONFIDE Training} \label{ssec:method.PDExplain_training}
The training process is presented in Fig.~\ref{fig:method.training}. Its input is a dataset $\mathcal{D}$ that consists of $N$ signals $\{u^c_i(x,t)\}_{i=1}^N$, which are solutions of $N$ PDEs that share an operator $F$ but have unique coefficient vectors $\{p_i\}_{i=1}^N$. We stress that the vectors $p_i$ are unknown even at train time. 
The signals are defined on some domain $\Omega$, and for times $t\in[0,t_0]$.
% The support of the signals is $x\in[0,L]$, $t\in[0,T]$. 
The loss we minimize is a weighted sum of two components: (i) the autoencoder reconstruction loss 
% (AE; \citealp{hinton2006reducing})
, which is defined in \eqref{eq:autoencoder_loss}, and (ii) the functional loss as defined in  \eqref{eq:coefficient_estimator_loss}. 

\begin{figure*}[t]
     \centering
     \begin{subfigure}{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/experiments/pred_rollout_error_by_time.pdf}
         \caption{}
         \label{fig:experiments.pred_rollout}
     \end{subfigure}
     % \hfill
     \begin{subfigure}{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/experiments/params_a_r2.pdf}
         \caption{}
         \label{fig:experiments.param_a_r2}
     \end{subfigure}
     \hfill
    \caption{Constant coefficients (Section~\ref{ssec:pde_exp1}). \textbf{(a)} Prediction error vs. prediction horizon, for different algorithms. CONFIDE, in red, is our approach. \textbf{(b)} Estimated value of the $\partial^2 u / \partial x^2 $ coefficient vs. ground truth, for test set ($R^2 = 0.93$).}
    \Description{Results of the PDE with Constant coefficients. of the left, Prediction error vs. prediction horizon, for different algorithms where CONFIDE beats all other algorithms. on the right, example of the estimated value of one of the PDE coefficients  vs. ground truth, for test set}
\end{figure*}

CONFIDE comprises two parts: (1) an encoder and (2) a coefficient estimator. The encoder's goal is to capture the dynamics driving the signal $u_i$, thus creating a compact representation for the coefficient estimator. The encoder is trained on signals $u^c_i$ in the train set. Each signal is of size $t_0 \times$ amount of spatial points (e.g., for $\Omega = [0,L]$, the size is  $t_0 \cdot L$ points). 

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.9\columnwidth]{Figures/training.png}
%     \caption{Training process.}\label{fig:method.training}
% \end{figure}

The encoder loss is the standard AE reconstruction loss, namely the objective is
\begin{align}
\label{eq:autoencoder_loss}
\mathop{\min}\limits_{\theta,\phi}\mathcal{L_{\textrm{AE}}} = 
\mathop{\min}\limits_{\theta,\phi}\sum_{i=1}^N \textrm{loss}(u_i^c - f_{\theta}(g_{\phi}(u_i^c))),
\end{align}
where $f_{\theta}$ is the decoder, $g_{\phi}$ is the encoder and $\textrm{loss}(\cdot,\cdot)$ is a standard loss function (e.g., $L^2$ loss).



The second component is the coefficient estimator, whose input is the encoded context and the signal  The estimated coefficients output by this component, together with the operator $F$, and the signal at at some random time $t=\tau\in[0,t_0]$, form the functional objective:
\begin{align}
\label{eq:coefficient_estimator_loss}
\mathop{\min}\limits_{\omega}\mathcal{L_{\textrm{coef}}} = 
    \mathop{\min}\limits_{\omega}\sum_{i=1}^N 
    \left\Vert F(\hat{p}_\omega,u^c_i(x,\tau)\right\Vert^2,
\end{align}
where $\omega$ represents the parameters of the coefficient estimator network, and $\hat{p}$ is the estimator of $p$ at time $\tau$, acquired by applying the network $h_\omega$ to the output of the encoder.
This design enables CONFIDE to learn a parameter vector which can depend on time, space, and the observation $u$.



The two components are trained simultaneously, and the total loss is a weighted sum of the losses in \eqref{eq:autoencoder_loss} and \eqref{eq:coefficient_estimator_loss}: 
$\mathcal{L} = \alpha \cdot \mathcal{L}_{\textrm{AE}} + (1-\alpha) \cdot \mathcal{L}_{\textrm{coef}}$, where $\alpha\in(0,1)$ is a hyper-parameter.

\paragraph{\textbf{Initial-conditions aware autoencoder}}
To further aid our model in learning the underlying dynamics of the observed phenomenon, we include the observed initial conditions of the signal (i.e., $u_i(t=0)$) along with the latent context vector (i.e., $g_{\phi}(u^c_i)$) as input to the decoder network.
This modification enables the model to learn a context vector that better represents the dynamics of the phenomenon, rather than other information such as the actual values of the signal.



We experimented with removing the decoder and training the networks using the functional loss alone, and without including the initial conditions as an input to the decoder.
In both cases, results proved to be inferior, suggesting that the autoencoder loss helps the model to focus on the underlying dynamics of the observed signal.

To summarize this section, we present the inference scheme in Algorithm~\ref{alg:algorithm1}, and the full training algorithm  in Algorithm~\ref{alg:algorithm2}. 
