Creating a neural-network based model for approximating the solution of a PDE has been studied extensively over the years, and dates back more than two decades \citep{lagaris1998artificial}.
We divide deep learning based approaches by their ability to incorporate mechanistic knowledge in their models, and by the type of information that can be extracted from using them.
Another distinction between different approaches is their ability to handle datasets originating from different contexts.
From a PDE perspective, a different context could refer to having data signals generated with different coefficients functions ($p_l$ in \eqref{eq:general_PDE}).
In many real-world applications, obtaining observed datasets originating from a single context is impractical.
For example, in cardiac electrophysiology \citep{neic2017efficient}, patients differ in cardiac parameters like resistance and capacitance, thus representing different contexts. 
In fluid dynamics, the topography of the underwater terrain (bathymetry) differs from one sample to another \citep{hajduk2020bathymetry}.

The first line of work is purely data-driven methods.
These models come in handy when we observe a spatio-temporal phenomenon, but either don't have enough knowledge of the underlying PDE dynamics, or the known equations are too complicated to solve numerically (as explained thoroughly by \citet{wang2021physics}). 
Recent advances demonstrate successful prediction results that are fast to compute (compared to numerically solving a PDE), and also provide decent predictions even for PDEs with very high dimensions \citep{brandstetter2022message,li2020fourier,han2018solving,lu2019deeponet}.
However, the downside of this approach is not being able to infer the PDE coefficients, which may hold valuable information and explanations as to why the model formed its predictions.

The second type of data-driven methods are approaches that utilize PDE forms known beforehand to some extent.
Works that adopt this approach can usually utilize the given mechanistic knowledge and provide reliable predictions, ability to generalize to unseen data, and in some cases even reveal part of the underlying PDE coefficient functions.
However, their main limitation is that they assume the entire training dataset is generated by a single coefficient function and only differ in the initial conditions (or possibly boundary conditions).
PDE-NET \citep{long2018pde}, its followup PDE-NET2 \citep{long2019pde}, DISCOVER \citep{du2022discover}, PINO \citep{li2021physics} and sparse-optimization methods \citep{schaeffer2017learning,rudy2017data} (expanding the idea originally presented on ODEs \cite{brunton2016discovering,champion2019data}), are not given the PDE system, but instead aim to learn some representation of the underlying PDE as a linear combination of base functions and derivatives of the PDE state.
PINN \citep{raissi2019physics} and NeuralPDE \citep{zubov2021neuralpde} assume full knowledge of the underlying PDE including the its coefficients, and aim to replace the numerical PDE solver by a fast and reliable model. 
They also provide a scheme for finding the PDE parameters as scalars, but assume the entire dataset is generated by a single coefficient value, while we assume each sample is generated with different coefficient values which could be functions of time, space and state (as described in \eqref{eq:general_PDE}). In \citet{negiar2022learning}, the authors incorporate knowledge of the PDE structure as a hard constraint while learning to predict the solution to the PDE.
Similarly, Learning-informed PDEs \citep{dong2022optimization,aarset2022learning} suggest a method that assumes full knowledge of the PDE derivatives and their coefficient functions, and infers the free coefficient function (namely $p_0(x,t,u)$ in \eqref{eq:general_PDE}). 
In \citep{lim2022physics}, the authors apply a finite difference approach to PINNs.

The last line of work, and closer in spirit to ours, includes context-aware methods that assume some mechanistic knowledge, with each sample in the train set generated by different PDE coefficients (we also refer to this concept as having different context) and initial conditions.
CoDA \citep{kirchmeyer2022generalizing} provides the ability to form predictions of signals with unseen contexts, but does not directly identify the PDE parameters.
GOKU \citep{linial2021generative} and ALPS \citep{yanglearning} provide context-aware inference of signals with ODE dynamics, when the observed signals are not the ODE variables directly.
Another important paper introduces the APHYNITY algorithm \citep{yin2021augmenting}, which also presents an approach to inferring PDE parameters from data. This work handles the scenario of fixed coefficients, as opposed to our ability to handle coefficients that are functions. Also, the case of coefficients that differ between samples is addressed only briefly, with a fixed, rather high, context ratio.

% Sparse optimization methods \cite{schaeffer2017learning,rudy2017data} also use the same knowledge, but use sparse-regression optimization scheme expanding the idea originally presented on ODEs in \cite{brunton2016discovering, champion2019data}.
% Papers that are purely data-driven, and can't provide any parameter identification. In cases where the the given PDE is hard to solve numerically, these can be a good solution. 
% 1. Message-passing neural solver \cite{brandstetter2022message}.
% 2. FNO \cite{li2020fourier}
% 3. Deep-BSDE \cite{han2018solving} 
% 4. DeepONet \cite{lu2019deeponet}
% Subsections:\\
% 1. Data driven methods for spatio-temporal data.\\
% 2. Methods for PDE parameters inference\\
% 3. Physics-informed methods for ODEs / PDEs / other physical systems
% 3. Context-aware methods?
% \\\\
% List of papers and their main idea:
% \\\\
% List of papers that aim to infer the PDE parameters directly, 
% \\\\
% Papers that are basically data-driven, but do use some mechanistic knowledge. These works offer parameter identification of the PDE, but limited to datasets built of samples with the exact same dynamics, but defer in initial conditions.\\
% 1. PDE-NET \cite{long2018pde} and its followup PDE-NET2 \cite{long2019pde} - They know that the data comes from a PDE, but don't know its form. They learn the PDE functional form from data.\\
% 2. Sparse optimization methods (\cite{schaeffer2017learning} and \cite{rudy2017data}) - Same as above, but use a sparse-regression optimization scheme expanding the idea originally presented on ODEs in \cite{brunton2016discovering, champion2019data}.\\
% 3. DISCOVER \cite{du2022discover} - similar idea, but with a different paradigm.
% \\\\
% Additional works that are the similar but use the mechanistic form in their calculations. These can infer the parameter, but are also limited to the case there the data is created from a single scalar parameter, and cannot learn it as a function as we suggest.
% This is also the case in the benchmark paper PDEBench by \cite{takamoto2022pdebench}, where each dataset is generated by a single set of parameters. While this may be the cases in some scenarios, in many cases we can only observe data originating from a set of possible parametric functions and not a single one.
% 1. PINN \cite{raissi2019physics}, and NeuralPDE \cite{zubov2021neuralpde}.
% 2. Learning-informed PDEs \cite{dong2022optimization,aarset2022learning} suggest a method that assumes full knowledge of the PDE derivatives and their coefficient functions, and infers the free coefficient function (namely $p_0(x,t,u)$ in \cref{eq:general_PDE}). They assume the entire data is generated using the same exact parameters.
% Context-aware methods:
% 1. CoDA \cite{kirchmeyer2022generalizing} - Do not provide direct inference of the parameters, but has the ability to generalize to unseen contexts.
% 2. GOKU \cite{linial2021generative} and ALPS \cite{yanglearning} - Provide context-aware inference of signals with ODE dynamics, when the observed signals are not the ODE variables directly.
