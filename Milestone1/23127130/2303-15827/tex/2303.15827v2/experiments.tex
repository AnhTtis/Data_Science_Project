% We devote this section to two types of analyses: (a) a comparison of our approach to other solutions, and (b) an analysis of our approach in different scenarios.

% The dataset we use includes $10,000$ samples of size $100\times 40[t\: \textrm{points} \times x\: \textrm{points}]$, 

We devote this section to analyse and compare our approach to other solutions, on three different systems of PDEs: (1) constant coefficients, (2) Burgers' equations, and (3) 2D-FitzHugh-Nagumo.
For each PDE task, we created a dataset of signals generated from a PDE with different coefficients.
We could not use off-the-shelf datasets, such as those appearing in PDEBench \cite{takamoto2022pdebench}, since each of the datasets there is generated from a single constant function (i.e., all data samples have the same context). We used well-known equations, therefore our datasets can serve as a benchmark for the emerging field of contextual PDE modelling. We stress the fact that the test set contains signals generated by PDEs with coefficient vectors that \emph{do not} appear in the training data, resulting in a zero-shot prediction problem. More information about dataset creation can be found in the appendix.

We benchmark the performance of CONFIDE against several state of the art approaches:
\begin{enumerate}
    \item Neural ODE, based on the algorithm suggested in \citet{chen2018neural}, Section 5.1 (namely, Latent ODE).
    \item Fourier Neural Operator (FNO), introduced in \citet{li2020fourier}.
    \item U-Net, as presented in \citet{gupta2022towards}.
\end{enumerate}
Additional details regarding the implementation of baselines can be found in Section ~\ref{app:implementation}.

\subsection{Second Order PDE with Constant Coefficients}\label{ssec:pde_exp1}
The first family of PDEs used for our experiments is: 
\begin{align}
\label{eq:pde_exp1}
        \fder{u}{t}=a \sder{u}{x} + b \fder{u}{x} + c,
\end{align}
where $p=(a,b,c)$ are constants but differ between signals. Figure~\ref{fig:experiments.pred_rollout} demonstrates the clear advantage of our approach, which increases with the prediction horizon (note the logarithmic scale of the vertical axis, representing the MSE of prediction). Since CONFIDE harnesses both mechanistic knowledge and training data, it is able to predict the signal $\hat{u}(x,t)$ several timesteps ahead, while keeping the error to a minimum. 

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/experiments/pred_rollout_error_by_time.pdf}
         \caption{}
         \label{fig:experiments.pred_rollout}
     \end{subfigure}
     % \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/experiments/params_a_r2.pdf}
         \caption{}
         \label{fig:experiments.param_a_r2}
     \end{subfigure}
     \hfill
    \caption{Constant coefficients (Section~\ref{ssec:pde_exp1}). \textbf{(a)} Prediction error vs. prediction horizon, for different algorithms. CONFIDE, in red, is our approach. \textbf{(b)} Estimated value of the $\partial^2 u / \partial x^2 $ coefficient vs. ground truth, for test set ($R^2 = 0.93$).}
\end{figure}

Another result for this set of experiments appears in Figure~\ref{fig:experiments.param_a_r2}. Here, we plot the estimated value of parameter $a$ of \eqref{eq:pde_exp1}, against its true value. The plot and the high value of $R^2$ demonstrate the low variance of our prediction, with a strong concentration of values along the $y=x$ line.

Section~\ref{sec:app.ablation} presents the results of an ablation study on the hyper-parameters of CONFIDE for this equation.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.95\textwidth]{Figures/experiments/burger_demonstration_621.pdf}
    \caption{A solution of the Burgers' equation (prediction MSE appears in the title). The left panel displays the ground truth (GT), next to it the error-minimizing CONFIDE prediction. Neural ODE, FNO and U-Net achieve errors that are 1-3 orders of magnitude larger, exhibiting considerable deterioration over time.}\label{fig:experiments.burgers_demo}
\end{figure*}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/experiments/burger_pred_rollout_error_by_time.pdf}
         \caption{}
         \label{fig:experiments.burger_pred_rollout}
     \end{subfigure}
     % \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/experiments/burger_params_b_prediction.pdf}
         \caption{}
         \label{fig:experiments.burger_params_b}
     \end{subfigure}
     \hfill
    \caption{Burgers' PDE:
    \textbf{(a)} Prediction error as prediction horizon increases, for different approaches with context ratio $\rho=0.2$. 
    \textbf{(b)} Estimation of the coefficient function $b(x,t,u)$ of the Burgers' equation from \eqref{eq:pde_exp2}. CONFIDE manages to accurately estimate the spatio-temporal dynamics of the coefficient, based on a context ratio of $\rho=0.2$.}
\end{figure}




% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=0.8\textwidth]{Figures/experiments/pred_error_by_train_set_size.pdf} \\
%     (a) \\
%     \includegraphics[width=0.8\textwidth]{Figures/experiments/params_error_by_train_set_size.pdf} \\
%     (b) \\
%     \caption{Constant coefficients PDE: (a) Prediction error of signal vs. train set size and (b) estimation error of parameter values vs. train set size. The error is calculated on a test set of 1000 samples. } \label{fig:experiments.pred_err_train}
% \end{figure}

% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=0.8\textwidth]{Figures/experiments/pred_error_by_context_size.pdf} \\
%     (a) \\
%     \includegraphics[width=0.8\textwidth]{Figures/experiments/params_error_by_context_size.pdf} \\
%     (b) \\
%     \caption{Constant coefficients PDE: (a) Prediction error vs. $\rho$ and (b) estimation error of parameter values error vs. $\rho$. The error is calculated on a test set of 1000 samples.}\label{fig:experiments.pred_err_context}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.6\textwidth]{Figures/experiments/pred_rollout_error_by_time.pdf}
%     \caption{Constant coefficients PDE: Prediction error as prediction horizon increases, for different approaches. PDExplain, in red, is our approach. No PDE, in blue, learns an encoding for the training data and applies a purely data-driven prediction. DI, in green, corresponds to direct identification of the PDE coefficients from the datapoint, followed by solving the PDE (no training). The last approach, PDE-RHS, learns an encoding for the training data and predicts the right-hand-side of an equation which is then solved to yield an estimate of the signal. The dashed vertical line is the value of the context ratio used for this experiment (fixed for all approaches), $\rho=0.21$.}\label{fig:experiments.pred_rollout}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.85\textwidth]{Figures/experiments/params_a_r2.pdf}
%     \caption{Constant coefficients PDE: estimated value of the $\partial^2 u / \partial^2 x$ coefficient vs. ground truth, for entire test set ($R^2 = 0.93)$.}\label{fig:experiments.param_a_r2}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.85\textwidth]{Figures/experiments/burger_pred_rollout_error_by_time.pdf}
%     \caption{Prediction error as prediction horizon increases, for different approaches, Burgers' PDE. PDExplain, in red, is our approach. No PDE, in blue, learns an encoding for the training data and applies a purely data-driven prediction. The last approach, PDE-RHS, learns an encoding for the training data and predicts the right-hand-side of an equation which is then solved to yield an estimate of the signal. The dashed vertical line is the value of the context ratio used for this experiment (fixed for all approaches), $\rho=0.2$. The DI approach cannot be applied to equations with non-constant coefficients and is therefore omitted from the comparison.}\label{fig:experiments.burger_pred_rollout}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.85\textwidth]{Figures/experiments/burger_params_b_prediction.pdf}
%     \caption{Estimation of the coefficient function $b(x,t,u)$ of the Burgers' equation, presented in Eq.~\eqref{eq:pde_exp2}. PDExplain manages to accurately estimate the spatio-temporal dynamics of the coefficient, based on a context ratio of $\rho=0.2$.}\label{fig:experiments.burger_params_b}
% \end{figure}


\subsection{Burgers' equation}
Another family of PDEs we experiment with is the quasi-linear Burgers' equation, whose general form is
\begin{align}
    \label{eq:pde_exp2}
        \fder{u}{t}=a\sder{u}{x} + b(u)\fder{u}{x},
\end{align}
where $b(x,t,u)=-u$, as presented in \cite{bateman1915some}. We note that this equation is quasi-linear since its drift coefficient $b(x,t,u)$ depends on the solution $u$ itself. The dataset for our experiments consists of 10000 signals with different values of $a$ and the same $b(u)=-u$, both unknown to the algorithm a priori.
We begin with a demonstration of a signal $u(x,t)$ and its prediction $\hat{u}(x,t)$ in Figure~\ref{fig:experiments.burgers_demo}. As can be seen both visually and from the value of the MSE (in each panel's title), our approach yields a prediction that stays closest to the ground truth (GT), even as the prediction horizon (vertical axis) increases.

Figure~\ref{fig:experiments.burger_pred_rollout} displays a comparison between the different approaches to our problem. As before, the vertical axis of the plot is logarithmic, and the advantage of CONFIDE over other approaches increases with the prediction horizon. In Figure~\ref{fig:experiments.burger_params_b} we focus on the ability to accurately predict coefficient functions with spatio-temporal dynamics, in this case: the coefficient $b(x,t,u)$ of \eqref{eq:pde_exp2}. The panels correspond to different points in time, showing that the coefficient estimator tracks the temporal evolution successfully.

\subsection{FitzHugh-Nagumo equations}
The last family of PDEs we examine is the FitzHugh-Nagumo PDE \citep{klaasen1984stationary} consisting of two equations:
\begin{align}
    \label{eq:fn2d_eq}
        \fder{u}{t} = a \Delta u + R_u(u,k,v), \qquad
        \fder{v}{t} = b \Delta v + R_v(u,v),
\end{align}
where $a$ and $b$ represent the diffusion coefficients of $u$ and $v$, and $\Delta$ is the Laplace operator.
For the local reaction terms, we follow \citet{yin2021augmenting} and set $R_u(u,k,v)=u-u^3-k-v$, and $R_v(u,v)=u-v$. The PDE state is $(u,v)$, defined on the 2-D rectangular domain $(x,y)$ with periodic Neumann boundary conditions.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/experiments/fn2d_pred_demonstration_9.pdf}
    \caption{Figures in the top row show the ground truth of $R_v$ for different time points, and the rows below show the estimation of it by the different approaches.
    CONFIDE near-perfectly recovers the unknown part of the PDE even as the prediction horizon increases.}\label{fig:experiments.fn2d}
\end{figure*}

The dataset created for this task consists of 1000 signals, each with a different value of $k$. 
We compare the prediction generated by CONFIDE to those yielded by other approaches, and present a typical result in Fig.~\ref{fig:experiments.fn2d}. In Figure 7 we present the prediction error as a function of the prediction horizon, once again comparing CONFIDE to the baselines.

\begin{align*}
\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\centering
\includegraphics[width=3in]{Figures/experiments/fn2d_pred_rollout_error_by_time.pdf}
\caption{2D-FitzHugh-Nagumo PDE: prediction error as horizon increases, for different approaches.}
% \label{fig:experiments.FN2D_error}
\end{figure}
\end{minipage}\qquad
\begin{minipage}[HT]{0.4\linewidth}
\begin{table}[H]
\centering
        \begin{tabular}{cc}
            \toprule
            & Coefficient \\ Setup & estimation error \\
            \midrule
            Constant coeff. & $0.0095 \pm 0.0131$ \\
            Burgers' & $0.0454 \pm 0.0333$ \\
            FN2D & $0.0075 \pm 0.0123$ \\
            \bottomrule
        \end{tabular}
\caption{Coefficient estimation error for different experimental setups: constant coefficients, Burgers' equation and two-dimensional FitzHugh-Nagumo. The variance is calculated over the entire test set, namely 1000 signals for the first two setups and 100 signals for FN.}
% \label{tbl:experiments.all_error}
\end{table}
\end{minipage}
\end{align*}\\

We summarize the results of experiments for signal prediction across all setups and approaches in Table ~\ref{tbl:exp.summary}. The table includes results for CONFIDE, all baselines, and also a variant of CONFIDE which we refer to as CONFIDE-0. This zero-knowledge variant is applicable when we know that the signal obeys some differential operator $F$, but have no details regarding the actual structure of $F$. Thus, CONFIDE-0 does not estimate the equation parameters, and only yields a prediction for the signal, utilizing our context-based architecture. We elaborate further in Section ~\ref{app:implementation}.

\begin{table}[h!]
\caption{Result summary for the signal prediction task, on all three PDE systems. The numbers represent signal prediction error at the end of the prediction horizon, averaged over the entire test set.}
% \vskip 0.15in
\label{tbl:exp.summary}
\begin{center}
\begin{small}
\begin{tabular}{lccc}
\toprule
Method & \shortstack{Constant \\ coefficients} & Burgers' & \shortstack{FitzHugh-\\Nagumo}\\
\midrule
% \multirow{5}{*}{\shortstack{FitzHugh-\\Nagumo}} 
CONFIDE & $0.0023 \pm 0.0036$ & $0.0008 \pm 0.0011$ & $0.0083 \pm 0.0177$ \\
CONFIDE-0 & $0.0079 \pm 0.0218$ & $0.0009 \pm 0.0016$ & $0.0845 \pm 0.0978$ \\
Neural-ODE & $0.0680 \pm 0.0905$ & $0.0272 \pm 0.0627$ & $0.2944 \pm 0.2293$ \\
FNO & $0.0538 \pm 0.0680$ & $0.9351 \pm 0.3091$ & $2.5727 \pm 17.732$ \\
Unet & $0.0160 \pm 0.0199$ & $0.0016 \pm 0.0023$ & $0.1293 \pm 0.1748$ \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\end{table}

