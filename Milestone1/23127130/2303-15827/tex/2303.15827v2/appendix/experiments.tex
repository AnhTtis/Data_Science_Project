We provide further information regarding the experiments described in Section~\ref{sec:experiments}. We ran all of the experiments on a single GPU (NVIDIA GeForce RTX 2080), and all training algorithms took $<10$ minutes to train.
All algorithms used ~5-10M parameters (more parameters on the FitzHugh-Nagumo experiment).
Full code implementation for creating the datasets and implementing CONFIDE and its baselines 
% will be made available upon acceptance.
is avalable on {\textcolor{blue}{\url{github.com/orilinial/CONFIDE}}}.

% \begin{table}[h!]
% \caption{Parameter count for all baselines on all datasets}
% \label{tbl:exp.param_num}
% \begin{center}
% \begin{small}
% \begin{tabular}{lccc}
% \toprule
% Method & \shortstack{Constant \\ coefficients} & Burgers' & \shortstack{FitzHugh-\\Nagumo}\\
% \midrule
% CONFIDE & 0 & 5M & 0 \\
% CONFIDE-0 & 0 & 0 & 0 \\
% Neural-ODE & 0 & 0 & 0 \\
% FNO & 0 & 0 & 0 \\
% Unet & 0 & 0 & 0 \\
% \bottomrule
% \end{tabular}
% \end{small}
% \end{center}
% \end{table}

\subsection{Dataset details}
To create the dataset, we generated signals using the \verb|PyPDE| package, where each signal was generated with different initial conditions.
In addition, as discussed in Section~\ref{sec:related}, we made an important change that makes our setting much more realistic than the one used by other known methods: the PDE parametric functions (e.g., $(a,b,c)$) are sampled for each signal, instead of being fixed across the dataset, making the task much harder.
To evaluate different models on the different datasets, we divided the datasets into 80\% train set, 10\% validation set and 10\% test set. 

\textbf{Second Order PDE with Constant Coefficients.}
For this task, we generated 10,000 signals on the spatial grid $x\in[0,20]$ with $\Delta x=0.5$, resulting in a spatial dimension consisting of 40 points.
Each signal was generated with different initial conditions sampled from a Gaussian process posterior that obeys the Dirichlet boundary conditions~$u(x=0)=u(x=L)=0$.
The hyper-parameters we used for the GP were $l=3.0, \sigma=0.5$, which yielded a rich family of signals, as demonstrated in Fig.~\ref{app:heat_ic_demonstration}.
The parameter vector was sampled uniformly: $a \sim U[0,2]$, $b$ and $c \sim U[-1,1]$ for each signal, resulting in various dynamical systems in a single dataset.
To create the signal we solved the PDE numerically, using the explicit method for times $t\in[0, 5.0]$ and $\Delta t = 0.05$.
Signals that were numerically unstable were omitted and regenerated, so that the resulting dataset contains only signals that are physically feasible.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{appendix/figures/heat_ic_demonstration.pdf}
        \caption{}
        \label{app:heat_ic_demonstration}
    \end{subfigure}
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{appendix/figures/fn2d_ic_demonstration_t.pdf}
        \caption{}
        \label{app:fn2d_ic_demonstration}
    \end{subfigure}
    \caption{Demonstration of the rich family of initial conditions generated.
    \textbf{(a)} Four examples for different initial conditions generated for the Second order PDE with constant coefficients dataset. The Dirichlet boundary conditions are $u(x=0)=u(x=L)=0$, and the initial conditions are drawn from a GP posterior obeying the boundary conditions.
    \textbf{(b)} Two examples of initial conditions for the 2D FitzHugh-Nagumo datasets. The left column describes the first state variable $u$, and the right column is the state variable $v$. Top row is the first example, and the bottom row is the second. All initial conditions are drawn from a GP prior not constrained to boundary conditions.}
\end{figure}

\textbf{Burgers' PDE.}
To create the Burgers' PDE dataset we followed the exact same process as with the constant coefficients PDE, except for the parameter sampling method.
Parameter $a$ was still drawn uniformly: $a \sim U[1,2]$, but $b$ here behaves as a function of $u$: $b(u) = -u$, commonly referred to as the viscous Burgers' equation.

\textbf{FitzHugh-Nagumo equations.}
For the purpose of creating a more challenging dataset with two spatial dimensions we followed \citet{yin2021augmenting}, and used the 2-D FitzHugh-Nagumo PDE (described in Eq.~\ref{eq:fn2d_eq}).
To make this task even more challenging and realistic, we created a small dataset comprising only 1000 signals defined on a 2D rectangular domain, discretized to the grid $[-0.16, 0.16] \times [-0.16,0.16]$.
The initial conditions for each signal were generated similarly to the other experiments, by sampling a Gaussian process prior with $l=0.1$, which generated a rich family of initial conditions, as can be seen in Fig.~\ref{app:fn2d_ic_demonstration}.
To create the coefficient function we sample $k\sim U[0,1]$ per signal, and set $(a,b)=(1e-3, 5e-3)$.
To create the signal we solved the PDE numerically, using the explicit method for times $t\in[0, 1.0]$ and $\Delta t = 0.01$.

\subsection{Implementation details}\label{app:implementation}

\paragraph{CONFIDE.}
The CONFIDE algorithm consists of two main parts: an auto-encoder part that is used for extracting the context, and a coefficient-estimation network.

The autoencoder architecture consists of an encoder-decoder network, both implemented as MLPs with 6 layers and 256 neurons in each layer, and a ReLU activation.
For the FitzHugh-Nagumo dataset, we wrap the MLP autoencoder with convolution and deconvolution layers for the encoder and decoder respectively, in order to decrease the dimensions of the observed signal more effectively.
We note that the encoder-decoder architecture itself is not the focus of the paper.
We found that making the autoencoder initial-conditions-aware by concatenating the latent vector in the output of the encoder to the initial conditions of the signal $u(t=0)$, greatly improved results and convergence time.
The reason is that it encourages the encoder to focus on the dynamics of the observed signal, rather than the initial conditions of it. We demonstrate this effect in Section~\ref{sec:app.ablation}.

The second part, which is the coefficient estimator part, is implemented as an MLP with 5 hidden layers, each with 1024 neurons, and a ReLU activation.
The output of this coefficient-estimator network is set to be the parameters for the specific task that is being solved.
In the constant-parameters PDE, the output is a 3-dim vector $(\hat{a},\hat{b},\hat{c})$.
In the Burgers' PDE, the output is composed of a scalar $\hat{a}$, which is the coefficient of $\sder{u}{x}$ and the coefficient function $b(u)$, which is a vector approximating the coefficient of $\fder{u}{x}$ on the given grid of $x$.
In the FitzHugh-Nagumo PDE, the output is a scalar $k$ used for inferring $R_u(u,v,k)$, and the function $R_v$ on the 2D grid $(x,y)$.

The next step in the CONFIDE algorithm is to evaluate the loss which is comprised of two losses: an autoencoder reconstruction loss $\mathcal{L}_{AE}$, and a PDE functional loss $\mathcal{L}_{coef}$.
The autoencoder loss is a straightforward $L^2$ evaluation on the observed signal $u^c$ and the reconstructed signal.
The functional loss is evaluated by first numerically computing all the derivatives of the given equation on the observed signal.
Second, evaluating both sides of the differential equations using the derivatives and the model's coefficient outputs, and lastly, minimizing the difference between the sides.
For example, in the Burgers' equation, we first evaluate $\fder{u}{t}$, $\sder{u}{x}$, and $\fder{u}{x}$, we then compute the coefficients $\hat{a}$ and $\hat{b}(u)$, and finally minimize:
\begin{align*}
    \mathop{\min}\limits_{\omega}\left\Vert \fder{u}{t} - \hat{a} \cdot \sder{u}{x} - \hat{b}(u) \cdot \fder{u}{x} \right\Vert.
\end{align*}
Since this algorithm evaluates numerical derivatives of the observed signals, it could be used for equations with higher derivatives, such as the wave equation, for instance.

\paragraph{CONFIDE-0.}
Similarly to the standard CONFIDE algorithm, we consider a zero-knowledge version, where we only know that the signal obeys some differential operator $F$, but have no details regarding the actual structure of $F$. Thus, the input for the coefficient-estimator network is the current PDE state ($u$ in the 1D experiment and $(u,v)$ in the 2D experiment), and the latent vector extracted from the auto-encoder.
The model then outputs an approximation for time derivative of the PDE states,
i.e., the model's inputs are $(u_{t}, g_\phi(u^c))$ and the output is an approximation for $\fder{u}{t}$.
The optimization function for this algorithm therefore tries to minimize the difference between the numerically computed time derivative and the output of the model:
\begin{align*}
    \mathcal{L}_{\text{CONFIDE-0}} = \alpha\cdot\mathcal{L}_{AE}+(1-\alpha)\cdot \sum_{i=1}^N \left\Vert \fder{u}{t} - m_\theta(u_i^c, g_\phi (u_i^c)) \right\Vert ^2,
\end{align*}
where $\mathcal{L}_{AE}$ is defined in Eq.~\ref{eq:autoencoder_loss}, $\fder{u}{t}$ is evaluated numerically, $m_\theta$ is the network estimating the temporal derivative, $g_\phi$ is the encoder network, and $u_i^c$ is the observed patch.

\paragraph{Hyper-parameters}
For both versions of CONFIDE we used the standard Adam optimizer, with learning rate of $1e^{-3}$, and no weight decay.
For all the networks we used only linear and convolution layers, and only used the ReLU activation functions.
For the $\alpha$ parameter we used $\alpha=0.5$ for all experiments, and all algorithms, after testing only two different values: $0$ and $0.5$ and observing that using the autoencoder loss helps scoring better and faster results.


\paragraph{Neural-ODE}
We implement the Neural-ODE algorithm as suggested by \citet{chen2018neural}, section 5.1 (namely, Latent-ODE).
We first transform the observed signal through a recognition network which is a 6-layer MLP. 
We then pass the signal through an RNN network backwards in time.
The output of the RNN is then divided into a mean function, and an std function, which are used to sample a latent vector. 
The latent vector is used as initial conditions to an underlying ODE in latent space which is parameterized by a 3-layer MLP with 200 hidden units, and solved with a DOPRI-5 ODE-solver.
The output signal is then transformed through a 5-layer MLP with 1024 hidden units, and generates the result signal.
The loss function is built of two terms, a reconstruction term and a KL divergence term, which is multiplied by a $\lambda_{KL}$.
After testing several optimization schemes, including setting $\lambda_{KL}$ to the constant values $\{1, 0.1, 0.01, 0.001, 0\}$, and testing a KL-annealing scheme where $\lambda_{KL}$ changes over time, we chose $\lambda_{KL}=1e^{-2}$ as it produced the lowest reconstruction score on the validation set.
We used an Adam optimizer with $1e^{-3}$ learning rate and no weight decay.

Our implementation is based on the code in \url{https://github.com/rtqichen/torchdiffeq}.

\paragraph{FNO and Unet}
For the Fourier-Neural-Operator we used the standard Neural-Operator package in \url{https://github.com/neuraloperator/neuraloperator}.
For the Unet implementation we used the implementation in \url{https://github.com/microsoft/pdearena}.
% The FNO algorithm is typically used for learning a function between initial conditions and the solution at some future time $t$. Therefore, it does not have a straight-forward version for the setting where each signal comes from different parameters.
The input we used for both of these algorithms is the entire context $u_{c}$ from time $t=0$ to $t=T-2$, and the output is a prediction of the solution at the next time point $u(t=T-1)$.
The loss is therefore an MSE reconstruction loss on $u(t=T-1)$.
