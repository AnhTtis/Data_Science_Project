{
    "arxiv_id": "2303.11306",
    "paper_title": "Localizing Object-level Shape Variations with Text-to-Image Diffusion Models",
    "authors": [
        "Or Patashnik",
        "Daniel Garibi",
        "Idan Azuri",
        "Hadar Averbuch-Elor",
        "Daniel Cohen-Or"
    ],
    "submission_date": "2023-03-20",
    "revised_dates": [
        "2023-03-21"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.GR",
        "cs.LG"
    ],
    "abstract": "Text-to-image models give rise to workflows which often begin with an exploration step, where users sift through a large collection of generated images. The global nature of the text-to-image generation process prevents users from narrowing their exploration to a particular object in the image. In this paper, we present a technique to generate a collection of images that depicts variations in the shape of a specific object, enabling an object-level shape exploration process. Creating plausible variations is challenging as it requires control over the shape of the generated object while respecting its semantics. A particular challenge when generating object variations is accurately localizing the manipulation applied over the object's shape. We introduce a prompt-mixing technique that switches between prompts along the denoising process to attain a variety of shape choices. To localize the image-space operation, we present two techniques that use the self-attention layers in conjunction with the cross-attention layers. Moreover, we show that these localization techniques are general and effective beyond the scope of generating object variations. Extensive results and comparisons demonstrate the effectiveness of our method in generating object variations, and the competence of our localization techniques.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.11306v1"
    ],
    "publication_venue": "Project page at https://orpatashnik.github.io/local-prompt-mixing/"
}