\section{Experiments Settings}

\subsection{Implementation Details}
In our experiments, we used $T = 50, T_3 = 44 \pm 1, T_2 = 34 \pm 1, T_1 = 15\pm 1$. We found that the optimal $T_3, T_2$ which indicates the prompt-mixing range may slightly change between different prompts and seeds. 

For segmenting the images using the self-attention maps, we used $5$ clusters across all our experiments. Note, that since we automatically label each cluster, it is possible to increase the number of clusters without affecting the results. We used $\sigma = 0.3$ for the clusters labeling.

Our method does not require optimization, and can therefore not consume a lot of memory. 

\vspace{-4pt}

\subsection{Evaluation Setup for Alternative Methods}
As we describe in Section~\ref{sec:exp-obj-varietions}, we compare our method to two types of methods: (i) Non-deterministic methods that provide different results for different seeds (SDEdit\cite{meng2022sdedit} and Inpainting\cite{Rombach2021HighResolutionIS}), and (ii) Text-guided image editing methods (Prompt-to-Prompt and Instruct-Pix2Pix).

The na\"{\i}ve way to attain variations with non-deterministic methods is to insert the input image into the method with a different seed each time. We also compare to these methods using our auto-generated proxy words, where we use the original prompt, with the proxy word replacing the word corresponding to the object of interest. In Figure~\ref{fig:quantitative_comparison}, both approaches are presented, the na\"{\i}ve way (indicated by the method's name, \eg, SDEdit) and each method when using proxy words (\eg, SDEdit-proxy).


As mentioned in Section~\ref{sec:exp-obj-varietions}, we used text-guided image editing methods to create variations by refining the prompt, adding adjectives to the explored object, or replacing the explored object with our proxy words.
Prompt-to-Prompt\cite{hertz2022prompt} supports both refining a prompt and replacing a word in a prompt. To apply Plug-n-Play\cite{pnpDiffusion2022}, we used prompts with proxy words.
Instruct-Pix2Pix\cite{brooks2022instructpix2pix} receives as input a prompt that defines the required edit. To refine an object, we used the prompt ``make the \{object name\} more \{adjective\} and keep the \{other object\}'', where the object name is the object of interest, and we use a different adjective for different variations. We also added the suffix ``keep the {other object}'' to preserve other objects in the image. We used to prompt ``change the \{object name\} to a \{proxy word\} and keep the \{other object\}'' to replace the object with a proxy word.
Zero-shot-Image2Image\cite{Parmar2023ZeroshotIT} supports only replacing an object, and therefore we didn't use prompt refinement with this method. In this method, 1000 sentences containing both the original and target words are required to replace an object. We used ChatGPT to create 1000 sentences for each proxy word.

\vspace{-4pt}

\subsection{Quantitative Comparison Dataset}
We first created three templates of prompts: ``A \{mug\} with {$w$}'', ``A \{sofa\} with a {$w$} on it'' and ``A \{basket\} with {$w$}''. For each template, we created five prompts by replacing $w$ with 5 different words (\eg, ``A \{mug\} with tea'' and ``A \{mug\} with hot chocolate'').
For each prompt we generated 10 initial images, using different seeds, and got a dataset that contains 150 images. With each method, we created 20 variations of the object of interest for each initial image in the dataset.


