\section{Related Work}


\subsection{Text-Guided Image Generation}
Text-to-image synthesis is a longstanding problem in computer vision and computer graphics. Early works were GAN-based~\cite{Xu2017AttnGANFT, Reed2016LearningWA, reed2016learning, Zhang2016StackGANTT} and were trained on small-scale datasets, typically of a single class. Recently, with the rapid progress in diffusion models~\cite{SohlDickstein2015DeepUL, Ho2020DenoisingDP, Dhariwal2021DiffusionMB}, auto-regressive models~\cite{Yu2022ScalingAM, Ding2021CogViewMT, Gafni2022MakeASceneST, Chang2022MaskGITMG}, and the availability of gigantic text-image datasets~\cite{Schuhmann2022LAION5BAO}, 
large-scale text-to-image models~\cite{Nichol2021GLIDETP, Rombach2021HighResolutionIS, Ramesh2021ZeroShotTG, Saharia2022PhotorealisticTD, Sauer2023ARXIV, balaji2022eDiff-I, kang2023gigagan} have lead to a huge leap in performance. Our work uses the publicly available Stable Diffusion model based on Latent Diffusion Models~\cite{Rombach2021HighResolutionIS}.


Large-scale text-to-image models allow the user to generate a gallery of images for a given text prompt.
The control over the generated image, however, is limited, with attributes such as image composition, object shape, color, and texture changing depending on the arbitrary randomly sampled initial noise.
Thus, recent works have introduced additional spatial conditions to the model such as segmentation maps~\cite{Avrahami2022SpaTextSR, bar2023multidiffusion}, bounding boxes~\cite{Li2023GLIGENOG, Rombach2021HighResolutionIS}, keypoints~\cite{Li2023GLIGENOG} and other visual conditions \cite{voynov2022sketch, zhang2023adding, lhhuang2023composer}. Such conditions offer spatial control, but no object-level control. Alternatively, to gain object-level control, numerous text-guided image editing methods have been recently developed. 

\subsection{Text-Guided Image Editing}
Generative models are a powerful tool for image editing~\cite{styleflow, shen2020interpreting, Patashnik_2021_ICCV, gal2021stylegannada, bar2022text2live}. With the increased performance of text-to-image diffusion models, many methods~\cite{meng2022sdedit, hertz2022prompt, Chefer2023AttendandExciteAS} have utilized them for text-guided image editing. A simple approach adds noise to the input image and then denoises it with a guiding prompt~\cite{avrahami2022blended, avrahami2022blended_latent, meng2022sdedit, Couairon2022DiffEditDS}. To localize the edit, a user-defined mask is required~\cite{Rombach2021HighResolutionIS}. Another approach manipulates internal representations of the model (\eg, attention maps) during the generation process~\cite{hertz2022prompt, pnpDiffusion2022, Parmar2023ZeroshotIT} to preserve the image layout. Other methods operate in the text encoder latent space~\cite{gal2022textual} and possibly fine-tune the generator~\cite{ruiz2022dreambooth, kawar2022imagic, Gal2023DesigningAE}, or train a model on image pairs~\cite{brooks2022instructpix2pix}. 

Recently, it has been shown that one can change an object's semantics by switching the text prompt along the denoising process~\cite{liew2022magicmix}. This method shares similarities with our method since they also mix prompts. However, their method is limited to appearance modifications as a means to change the semantics and explicitly preserve the object's shape. In contrast, our work focuses on object geometric modifications.
It should be noted that unlike all the editing methods above, generating object shape variations is not an editing task per-se, as it aims at generating multiple object-level variations for a given image while preserving the semantics of the object. 
Furthermore, our introduced editing localization techniques are complementary to the editing methods mentioned above as we later demonstrate.