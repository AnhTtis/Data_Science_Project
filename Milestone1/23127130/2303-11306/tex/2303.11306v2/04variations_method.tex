\section{Prompt-Mixing} \label{sec:analysis}

To generate object variations, we propose a method that operates during inference time and does not require any optimization or model training. Given a text prompt $P$ and an object of interest, represented by a word $w$, we manipulate the denoising process to obtain object-level variations. 

The key enabler for generating shape variations for an object is our prompt-mixing technique. 
In prompt-mixing, different prompts are used in different time intervals of the denoising process. Specifically, we define three timestep intervals, $[T, T_3], [T_3, T_2], [T_2, 0]$, and use a different prompt in each interval to guide the denoising process. We denote by $P_{[t, t']}$ the prompt used in interval $[t, t']$. 
This technique is based upon insights related to the coarse-to-fine nature of the denoising process. These insights were also mentioned in \cite{balaji2022eDiff-I, Chefer2023AttendandExciteAS, voynov2022sketch, daras2022multiresolution, choi2021ilvr}, and we further analyze them in Section~\ref{sec:diff-stages}.

Note that common to image editing techniques, localizing the manipulated region is crucial for a successful result. In Section~\ref{sec:localization}, we introduce two generic techniques for achieving localized editing, and use them in our full pipeline for object variations illustrated in Figure~\ref{fig:overview}.


\subsection{Denoising Diffusion Process Stages} \label{sec:diff-stages}

We analyze the timestep intervals defined above, to show the type of attributes in the image controlled by each interval. This analysis demonstrates the coarse-to-fine nature of the denoising process. We show the results of the analysis in Figure~\ref{fig:analysis-stages}.
In each row, the three leftmost images were generated 
using the prompt $P(w)= \enspace$``Two $\left<w\right>$ on the street'' along the entire denoising process, where $w$ represents a different word in each image. All images were generated using the same initial noise. 
For the two rightmost images in each row we apply prompt-mixing. Specifically,
we alter $w$ in the input prompt $P(w)$ in each time interval. 
As mentioned earlier, the input prompt is fed into the cross-attention layers, and directly affects the Keys and Values.
We use the altered prompt $P(w')$ to compute the Values, while using the original prompt $P(w)$ to compute the Keys. This design choice is explained in the supplementary materials.

In the fourth column of each row, we use $P_{[T, T_3]}({\color{ao}{w_1}})$ and $P_{[T_3, 0]}({\color{magenta}{w_2}})$.
As can be seen, we obtain an image containing {\color{magenta}{$w_2$}} (pyramids, mugs), with the layout and background of the images containing the balls ({\color{ao}{$w_1$}}).
In the fifth column, we use $P_{[T, T_3]}({\color{ao}{w_1}})$, $P_{[T_3, T_2]}({\color{magenta}{w_2}})$, and $P_{[T_2, 0]}({\color{blue}{w_3}})$.
As observed, we now obtain an image containing {\color{magenta}{$w_2$}} (pyramids, mugs), with the layout and background of the images containing the balls ({\color{ao}{$w_1$}}), and the fine visual details (\eg, texture) of {\color{blue}{$w_3$}} (fluffies, metals). 
We conclude that the first interval, $[T, T_3]$, controls the image layout, the second $[T_3, T_2]$ controls the shapes of the objects, and the third  $[T_2, 0]$ controls fine visual details (\eg, texture). We provide additional examples in the supplementary materials.



\subsection{Object Variations}

\paragraph{Mix-and-Match}

Let $P(w)$ denote the prompt of the original image, where $w$ denotes the word corresponding to the object of interest. 
To generate shape variations of the object of interest, we perform prompt-mixing
where $P_{[T, T_3]}(w) = P_{[T_2, 0]}(w)$. This is a special case of prompt-mixing, which we term \emph{Mix-and-Match}, since we perform mixing in the second interval, and match the prompts between the first and third intervals. Formally, our shape variations are achieved by using $P_{[T, T_3]}(w)$, $P_{[T_3, T_2]}(w')$, and $P_{[T_2, 0]}(w)$, where $w'$ is a ``proxy'' word (explained below). 


Mix-and-Match allows keeping the original image layout, formed in the first interval, the shape of $w'$, set in the second interval, and the fine visual details of the original object represented by $w$ during the third interval.

\vspace{-12pt}
\paragraph{Proxy Words}

Here we describe our scheme for determining the proxy words. Intuitively, a proxy word represents a semantically close object to the object of interest, and whose shape is rather different.
Motivated by the use of CLIP~\cite{Radford2021LearningTV} for extracting text encodings in Stable Diffusion, we use CLIP's text space for finding the set of proxy words.

Given a word $w$, we seek to find the $k$ most similar tokens $\{w'_1, ..., w_k'\}$ to $w$. To this end, we consider all tokens $t$ of CLIP's tokenizer and embed each to the CLIP embedding space using prompts of the form $P_{\text{sim}}(t) =$ ``A photo of a $\left<t\right>$''. We then take the $k$ tokens with the smallest CLIP-space distances to the encoding of $P_\text{sim}(w)$, the prompt representing our object of interest. This gives us the $k$ tokens with the closest semantic meaning to $w$.
To take into account the input prompt context, we rank these $k$ tokens according to CLIP's distance between $P(t)$ and $P(w)$. Finally, we define the top $m$ tokens as proxy words. For each proxy word, we perform Mix-and-Match to obtain an image with a variation of the object of interest.

Note, that since we consider embeddings at the token level, some proxy words may not have semantic meaning alone. However, we observe that they still provide meaningful variations when used in our Mix-and-Match technique due to their close proximity in CLIP's embedding space.

\input{figures/analysis/stages.tex}



