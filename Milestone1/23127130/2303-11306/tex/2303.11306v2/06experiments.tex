\vspace{-5pt}


\section{Experiments}

\subsection{Object Shape Variations} \label{sec:exp-obj-varietions}
We perform experiments to assess the effectiveness of our method in generating object-level shape variations, and compare it to other existing methods. Specifically, we compare our method with methods that directly provide variations for an image by changing seed (inpainting~\cite{Rombach2021HighResolutionIS}, SDEdit~\cite{meng2022sdedit}), as well as recent text-guided image editing baselines (P2P~\cite{hertz2022prompt}, I-Pix2Pix~\cite{brooks2022instructpix2pix}, PnP~\cite{pnpDiffusion2022}, Zero-shot Image2Image Translation~\cite{Parmar2023ZeroshotIT}, Imagic~\cite{kawar2022imagic}). 

It should be noted that our method differs from image editing approaches in two key aspects. First, most editing methods change the object to an object of a different class, whereas our method keeps class and offers alternative options for the same object. Second, editing methods require the user to provide specific instructions on the exact object they would like to obtain, whereas our method allows for an open-ended exploration without relying on such instructions. Therefore, we adopted two different approaches when comparing our method with image editing methods: (i) we refined the input prompt, and (ii) we replaced the word representing the object of interest with proxy words. The second approach, which involves using proxy words, can also be viewed as an ablation study for Mix-and-Match.

We separately evaluate three important aspects of our method. First, we aim at achieving high diversity in the shape of the object of interest. Second, the object class should remain the same. We term this evaluation objective as object faithfulness. Third, regions of the image other than the object of interest should be preserved.

\vspace{-8pt}
\paragraph{Qualitative Experiments}

In Figure~\ref{fig:our-results}, we show a gallery of images. Additional results are provided in the supplementary materials.
Observe the shape diversity in our method's results and the preservation of the original image. 

In Figure~\ref{fig:comp_inpainting} we compare our method with inpainting~\cite{Rombach2021HighResolutionIS} and SDEdit~\cite{meng2022sdedit} by sampling different seeds to get variations. For inpainting, we use a mask obtained from our segmentation technique.
Similarly to Figure~\ref{fig:inpainting}, we observe that applying inpainting results mainly in texture changes, while SDEdit performs small shape changes but does not preserve the background and other objects (\eg, cat). 

\input{figures/comparison_inrepainting}

Comparison to text-guided image editing methods is presented in Figure~\ref{fig:comparision}.
For each method, we guided the editing with refined prompts (``eggchair'', ``stool'') and with our automatic proxy words (``cart'', ''bed'', ``stool''). Note that the refined prompts were manually chosen to be types of chairs. 
It should be noted that refining prompts for objects that do not have subtypes (\eg, basket) is more challenging. 
In P2P~\cite{hertz2022prompt}, we show two versions of results, which differ in the number of self-attention injection steps (10\%, 40\%).
Additional information about the configuration of each method and comparison to additional methods are provided in the supplementary materials.

Our diverse results in Figure~\ref{fig:comparision} remain faithful to the class of chairs while preserving the rest of the image.
Not surprisingly, editing methods struggle at keeping the chair when a different object is specified in the prompt. For example, when replacing the chair with a cart, wheels are added. In our method, thanks to Mix-and-Match, we take the shape of the wheels but the fine visual details of the chair.
 
As can be seen in Figure~\ref{fig:comparision}, injecting self-attention maps for 40\% of the denoising steps in P2P~\cite{hertz2022prompt} prevents change in the object of interest (here, a chair). Conversely, when injecting self-attention maps for 10\% of the denoising steps, P2P struggles to preserve the dog and the background.  
Instruct-Pix2Pix~\cite{brooks2022instructpix2pix} results are diverse but inferior to our method in image preservation (eggchair) and faithfulness (bed, stool).
Plug-and-Play~\cite{pnpDiffusion2022} struggles at performing shape changes as it injects the entire self-attention maps along the denoising process.
Compared to Zero-shot Image2Image Translation~\cite{Parmar2023ZeroshotIT}, which requires 1000 prompts with the proxy word, our method preserves the dog colors better, and allows for more diverse shapes (see the stool). 
We also compared our method to Imagic \cite{kawar2022imagic} using Imagen \cite{Saharia2022PhotorealisticTD}. While Imagic produces high-quality results, our method has advantages in preserving the background and being more faithful to the class of a chair. Additionally, our method is more time-efficient than Imagic's optimization-based approach.

\input{figures/comparison}

\paragraph{Quantitative Experiments}

Given a collection of object-level variations of an image, we measure each objective as follows. For shape diversity, we extract a mask of the object of interest by using CLIPSeg~\cite{Lddecke2021ImageSU}, and average the IoU of each pair of masks in the collection. We define the diversity as $1 - \text{IoU}$. We measure faithfulness by employing CLIP~\cite{Radford2021LearningTV} and computing cosine similarity between an averaged embedding of images containing the object of interest and each of the images in the collection. To quantify image preservation we utilize LPIPS~\cite{zhang2018perceptual}.


We create a dataset of 150 images with various prompts, and generate 20 variations of the object of interest with each method. More details about the construction of the dataset are provided in the supplementary materials.
We test other methods with proxy words (all methods), random seeds (inpainting, SDEdit), and prompt refinement (I-Pix2Pix, P2P).

In Figure~\ref{fig:quantitative_comparison} we present quantitative results. Our method achieves a good trade-off between diversity, faithfulness, and image preservation. 
Methods with higher diversity scores than ours do not preserve the original image and are not faithful to the object of interest, as was also demonstrated in the qualitative comparison. As shown in the graph, methods with better preservation or faithfulness scores than ours, hardly change the shape of the object.

\input{figures/quantitative_comparison}

\vspace{-8pt}
\paragraph{Ablation Studies}
We ablate our full pipeline of generating object-level shape variations of a given image, showing the necessity of each part. We present the results in Figure~\ref{fig:ablation}. 
In the first row, we show the results of Mix-and-Match without the localization techniques. As can be seen, Mix-and-Match alone fails to preserve the dog and the background.
Adding the attention-based shape localization technique, where we create a mask for the self-attention map based on the word ``dog'', allows for the preservation of the dog. 
Finally, adding the controllable background preservation technique keeps the background of the original image.

\input{figures/ablation}


\subsection{Edit Localization}
We integrate our localization techniques with existing text-to-image methods to show improved results when using these methods in conjunction with our techniques.

\vspace{-14pt}
\paragraph{Attention-Based Shape Localization}
Previous methods~\cite{hertz2022prompt, pnpDiffusion2022} have injected the entire self-attention map to preserve the shapes of the original image. To demonstrate the effectiveness of our attention-based shape localization technique, we integrate it with P2P~\cite{hertz2022prompt} and show the results in Figure~\ref{fig:local_ablation} where we aim to change the hat of the dog into a crown. The figure shows the original generated image on the left, followed by P2P where we injected the entire self-attention map during 40\% and 20\% of the denoising steps, and P2P with our localization technique.
As can be seen, using P2P involves a tradeoff between accurately changing the hat into a crown and preserving the original dog. By integrating our method, we were able to selectively inject only the rows and columns that corresponded to the dog, achieving the desired transformation of the hat into a crown while preserving the original shape of the dog.

\input{figures/local_ablation}

\vspace{-10pt}
\paragraph{Controllable Background Preservation}
We test our background preservation technique with PnP~\cite{pnpDiffusion2022}, P2P~\cite{hertz2022prompt}, and SDEdit~\cite{meng2022sdedit} and show the results in Figure~\ref{fig:editing_bg}. 
For P2P we use their local blending.
To integrate each method with our technique, we first invert each image~\cite{mokady2022null}. Then, we segment and label each segment of the image with our technique, and create a mask of the main object in the image. At step $t=35$ of the denoising process we perform blending between the edited image and the original one, taking the background from the original image, and the object from the edited image. 

As can be seen, our method achieves plausible segmentation maps even for inverted images. For PnP and SDEdit, our method allows editing the object while keeping the background as in the original image. In P2P we see that our technique localizes the edit better, removing the cat's tail and feet from the image.

\input{figures/editing_with_bg}
