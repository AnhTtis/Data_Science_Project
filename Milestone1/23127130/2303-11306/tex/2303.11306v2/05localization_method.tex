\section{Edit Localization} \label{sec:localization}

As mentioned above, localizing the edit is especially challenging when changing the object's shape. To this end, we present two 
techniques that assist in localizing the edit from two different aspects. 
These localization techniques are crucial for successfully generating object shape variations. As we shall show, other known editing methods can also benefit from integrating them, leading to better localized manipulations.

\input{figures/attn_localization}

\subsection{Attention-Based Shape Localization} \label{sec:attn-based-loc}

To preserve the shapes of objects in the image, 
we introduce a shape localization technique based on injecting information from the self-attention maps of the source image into the self-attention maps of the generated image.
In the object variations pipeline, we apply this technique to objects that we aim to preserve. %
Injecting the full self-attention maps, even for a few steps, accurately preserves the structure of the original image, but at the same time prevents noticeable shape changes in the object we aim to change.

Our technique, depicted in Figure~\ref{fig:attn_localization}, revolves around a selective injection of self-attention maps. 
Consider a specific self-attention layer $l$ in the denoising network, which receives features of dimension $N\times N$, and the attention map formed by this layer, $S_t^{(l)}$, whose dimensions are $N^2 \times N^2$. The value $S_t^{(l)}[i, j]$ in the map indicates the extent to which pixel $j$ affects pixel $i$. In other words, row $i$ of the map shows the degree to which each pixel impacts pixel $i$, while column $j$ displays the degree to which pixel $j$ impacts all other pixels in the image. To preserve the shape of an object, we inject the rows and columns of the self-attention map that correspond to the pixels containing the object of interest.
Specifically, for a given denoising timestep $t$, the self-attention layer $l$,  and the self-attention map $S_t^{(l)}$, we define a corresponding mask $M_t^{(l)}$ by:
\vspace{-2pt}
\begin{equation}
    M_t^{(l)}[i, j] = 
    \begin{cases}
           1 &  i\in O_t^{(l)} \enspace\text{or}\enspace j \in O_t^{(l)}\\
           0 &\text{otherwise}, \\ 
         \end{cases}
\vspace{-2pt}
\end{equation}
where $O_t^{(l)}$ is the set of pixels corresponding to the object we aim to preserve. We explain later how we find $O_t^{(l)}$. After defining the mask $M_t^{(l)}$, the self-attention map in the newly generated image is changed to be: %
\vspace{-2pt}
\begin{equation}
    S_t^{*(l)} \leftarrow M_t^{(l)} \cdot S_t^{(l)} + (1 - M_t^{(l)}) S_t^{*(l)},
\end{equation}
where $S_t^{(l)}$ and $S_t^{*(l)}$ are the self-attention maps of the original and the newly generated images, respectively.
Additional mask controls are presented in the supplementary.

To find the pixels in which an object is located (\ie defining the set of pixels $O_t^{(l)}$), we leverage the cross-attention maps. These maps model the relations between each pixel in the image and each of the prompt's tokens. For an object we aim to preserve, we consider the cross-attention map of the corresponding token in the prompt. We then define the set $O_t^{(l)}$ of the object's pixels to be pixels with high activation in the cross-attention map by setting a fixed threshold.


\subsection{Controllable Background Preservation}

As shown in previous works~\cite{pnpDiffusion2022, hertz2022prompt}, self-attention injection preserves mainly structures.  
Therefore, we introduce a \emph{controllable background preservation} technique which preserves the appearance of the background and possibly some user-defined objects, specified by their corresponding nous in the input prompt. We give the user control to set the user-defined objects to be preserved since different images may require different configurations. 
For example, in Figure~\ref{fig:overview}, a user may want to preserve the oranges if the basket's size fits, while in other cases where the size of the basket is changed, it is desirable to change the oranges to properly fill a basket with a modified size.

To preserve the appearance of the desired regions, at $t=T_1$ we blend the original and the generated images, taking the changed regions (\eg, the object of interest) from the generated image and the unchanged regions (\eg, background) from the original image.
Next, we present our novel segmentation approach and describe the blending.

\vspace{-14pt}
\paragraph{Self-segmentation}
We perform the segmentation on noised latent codes and, as such, off-the-shelf semantic segmentation methods cannot be applied. Hence, we introduce a segmentation method that segments the image based on self-attention maps, and labels each segment by considering cross-attention maps. The method is based on the premise that internal features of a generative model encode the information needed for segmentation~\cite{Collins20, zhang21}.

At $t=T_1$, we average the $32^2 \times 32^2$ self-attention maps from the entire denoising process. We obtain an attention map of size $32^2 \times 32^2$, reshape it to $32 \times 32 \times 1024$, and cluster the deep pixels with the K-Means algorithm, where each pixel is represented by the $1024$ channels of the aggregated self-attention maps. Each resulting cluster corresponds to a semantic segment of the generated image. Several segmentation results are illustrated in Figure~\ref{fig:segmentation}.

\input{figures/method/segmentation}
\input{figures/our_results}



Having extracted the semantic segments, we match each segment with a noun in the input prompt. For each segment, we consider the normalized aggregated cross-attention maps of the prompt's nouns, and match each segment to a noun as follows. 
For segment $i$ that corresponds to a binary mask $M_i$, and for a noun $n$ in the prompt that corresponds to a normalized aggregated cross-attention mask $A_n$, we calculate a score $s(i, n) = \sum(M_i \cdot A_n) / \sum(M_i) $. We label segment $i$ with $\argmax_n {s(i, n)}$ if $\max_n {s(i, n) > \sigma}$ and label it as background otherwise. The threshold value $\sigma$ is fixed across all our experiments.

\vspace{-12pt}
\paragraph{Blending the original and generated images}
We use the segmentation map and the corresponding labels to overwrite relevant regions in the newly generated image. We retain pixels from the original image only if they are labeled as background or as a user-defined object in both the original and new images. 
This approach helps to overcome shape modifications in the object of interest, as illustrated by the example of the basket in Figure~\ref{fig:overview}, where the handle region is taken from the newly generated image. After blending the latent images, we proceed with the denoising process.
