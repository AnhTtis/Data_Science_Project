\section{Introduction}

Text-to-image diffusion models have recently shown unprecedented image quality and diversity \cite{Ramesh2022HierarchicalTI, Saharia2022PhotorealisticTD, Rombach2021HighResolutionIS}, and have opened a new era in image synthesis. 
Still, the control over the generated image is limited, resulting in a tedious selection procedure, where users 
sample numerous initial seed noises, 
from which they can select a preferred one. 
Images generated from different initial noise with the same text prompt share semantics, but the shape, appearance, and location of the generated shapes may differ greatly.
The uncontrolled global changes realized by such a sampling process, do not allow users to interact with the generated image and narrow down their open-ended exploration process. In particular, the lack of object-level control of the user with the generated image hinders the user's ability to focus on refining specific objects during their exploration.

A possible approach for interacting with the generated image as a means to explore the shape and appearance of a specific object in the image is to use text-guided image inpainting~\cite{Rombach2021HighResolutionIS} or SDEdit~\cite{meng2022sdedit}. These methods mainly excel in changing the texture of an object and are therefore more suitable for textural exploration.
However, changing the shape of an object, particularly if other regions in the image should be preserved, is significantly more challenging, and these methods struggle to achieve that without affecting the entire image.
Figure~\ref{fig:inpainting} demonstrates the lack of shape variations with the above approaches.

\input{figures/compare_inpainting}


In this paper, we deal with object-level shape exploration, where the user attains a gallery with variations of a specific object in the image, without having to provide any additional input. Specifically, acknowledging that previous works have struggled to perform geometric manipulations,
we focus on object-level shape variations, which are automatically generated and presented to the user, see Figure~\ref{fig:teaser}.


We introduce a prompt-mixing technique, where a mix of different prompts is used along the denoising process. This approach is built on the premise that
the denoising process is an innate coarse-to-fine synthesis,
which roughly consists of three stages. In the first stage, the general configuration or layout is drafted. In the second stage, the shapes of the objects are formed. Finally, in the third stage, their fine visual details are generated.

Common in text-based image editing techniques, a challenge arises when localizing the modification of the object.
Hence, we develop two novel means to localize edits.
First, to preserve the shapes of other objects, we inject a localized self-attention map from the original image into the newly generated image.
This injection leads to a rough alignment between the two images. Next, to further preserve appearance (\eg, image background),
we automatically extract labeled segmentation maps of both the original and generated images. 
The segmentation maps allow applying the edits locally in selected segments only. Finally, during the last denoising steps, we seamlessly blend all segments together.


We demonstrate that without any costly optimization process, our method offers the user the ability to generate object-level shape variations, while remaining faithful to the original image, either generated or real. Moreover, we show that our localization techniques are beneficial to not only object shape variations, but also to generic local image editing methods. We demonstrate the improved results achieved by integrating our localization techniques into existing image editing methods.
Extensive experiments are conducted to show that our approach can create more diverse results with larger shape changes and better content preservation compared to alternative methods.




