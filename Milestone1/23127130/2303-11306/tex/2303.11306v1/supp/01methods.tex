\section{Additional Details }

\subsection{Prompt-Mixing}

\paragraph{Denoising Diffusion Process Stages}
Additional examples of the denoising stages are shown in Figure~\ref{fig:analysis-stages-supp}. The used prompts are ``A $\left<w\right>$ is flying in the sky'' (first row), and ``A $\left<w\right>$ on the beach'' (second row).


\input{figures/analysis/stages_supp}

\paragraph{Cross-Attention Injection} \label{sec:cross-attention-injection}

We validate the intuition about the role of the Keys and Values in the cross-attention layers, as mentioned in the main paper. This experiment explains the design choice of modifying only the Values with the altered prompt when applying prompt-mixing.
We analyze prompt-mixing where the altered prompt is used to compute a different number of components of the cross-attention layer. In Figure~\ref{fig:analysis-kv}, the first two columns correspond to the generation of an image with a single prompt $P(w)$ along the entire denoising process. 
In the other columns, we perform prompt-mixing where we use $P_{[T, T_3]}({\color{ao}{w_1}})$ and $P_{[T_3, 0]}({\color{magenta}{w_2}})$.
In other words, we take the layout from {\color{ao}{$w_1$}} (``puppies'', ``surfborad'', ``bird''), and the shape and fine visual details from {\color{magenta}{$w_2$}} (``flowers'', ``box'', ``kitten''). 

In the third column, we use $P({\color{magenta}{w_2}})$ to compute the Values and $P({\color{ao}{w_1}})$ to compute the Keys in $[T_3, 0]$,
the layouts of the obtained images are similar to these obtained by the images generated by $P({\color{ao}{w_1}})$ in the first column. For example, in the first row, a black flower is added to capture the black puppy.
In the fourth column, where we use $P({\color{magenta}{w_2}})$ to compute the Keys and $P({\color{ao}{w_1}})$ to compute the Values, the appearance of the objects is still of the objects generated by $P({\color{ao}{w_1}})$ in the first column. This strengthens our intuition that the Values of cross-attention layers control object shape and appearance while the Keys hardly affect it. 
In the fifth column, we use $P({\color{magenta}{w_2}})$ to compute both the Keys and the Values in $[T_3, 0]$. As can be seen, the images in the fifth column capture the layout of the images generated by $P({\color{ao}{w_1}})$ worse than the images in the third column. For example, in the first row there are two flowers and the image looks similar to the one generated solely by $P({\color{magenta}{w_2}})$ in the second column. Additionally, in the third row there are two kittens, one instead of the bird and the other at the same place as the kitten in the image generated solely by $P({\color{magenta}{w_2}})$.
Therefore, when applying prompt mixing, the modified prompts are used to compute only the Values of the cross-attention layers in the corresponding timestep interval.



\input{figures/analysis/kv.tex}


\paragraph{Proxy Words} \label{sec:proxy_words}
We utilize proxy words as a technique for navigating through the prompt embedding space in a meaningful manner. Another potential method is to add noise to the prompt embedding or the cross-attention Values, but we have observed that this approach performs significantly worse and results in lower diversity. Our experimental findings indicate that our proxy-word selection method occasionally produces unexpected words that generate convincing variations in shape. For instance, the word ``something'' frequently appears as a proxy word in our examples. Additionally, we have discovered that certain words with a significant CLIP distance from the object of interest can still generate reasonable variations. However, we opt to use our simple ranking mechanism that avoids more intricate selections for words (considering for instance word concreteness) as we find that it enhances the likelihood of producing plausible variations. 

In Figure~\ref{fig:proxy-words}, we show a few proxy-words examples. For the prompt ``Luxury yellow \emph{purse} on a table'' we take three high-ranked proxy-words and three words with a large CLIP distance. As can be seen, even words that are loosely related to the word ``purse'' (\eg, ``butterfly'') sometimes allow for generating plausible shape variations. However, closer words tend to produce more successful variations.



\subsection{Attention-Based Shape Localization}

\paragraph{Self-Attention Injection Mask Controls}
As we describe in Section~\ref{sec:attn-based-loc}, 
we selectively inject the self-attention map of the original image into the generated image, by constructing a mask that contains rows and columns corresponding to pixels of the object we aim to preserve. Specifically, we define the mask as follows:
\begin{equation} \label{eq:row_cols_masl}
    M_t^{(l)}[i, j] = 
    \begin{cases}
           1 &  i\in O_t^{(l)} \enspace\text{or}\enspace j \in O_t^{(l)}\\
           0 &\text{otherwise}, \\ 
         \end{cases}
\vspace{-2pt}
\end{equation}
where $O_t^{(l)}$ is the set of pixels corresponding to the object we aim to preserve.
Injecting only the rows (\ie, setting 1 in $M_t^{(l)}$ if $i \in O_t^{(l)}$) keeps the effect of each pixel in the image on the pixels in $O_t^{(l)}$, but allows the pixels in $O_t^{(l)}$ to change their effect on other pixels in the image. When using such a ``rows'' mask, pixels in the image which weren't part of the object ($O_t^{(l)}$) in the original image, can behave as part of the object in the new image. This can be seen in Figure~\ref{fig:local_ablation_supp}, where ears were added to the dog in pixels that contained the hat in the original image. Conversely, injecting only the columns (\ie, setting 1 in $M_t^{(l)}$ if $j \in O_t^{(l)}$) allows pixels in the image to affect differently on pixels of the object to preserve ($O_t^{(l)}$) than in the original image, but keeps the effect of the pixels in $O_t^{(l)}$ on the other pixels in the image. In our experiments, we found the mask that contains both the rows of the columns (Equation~\ref{eq:row_cols_masl}) to be the most robust.

In some cases, using both ``rows'' and ``columns'' masks to preserve an object limits the required geometric changes of the object of interest (\ie, the object we aim at obtaining variations of). 
We define $O_t^{'(l)}$ as the set of pixels corresponding to the object of interest. In such cases, the relations between each pixel in $O_t^{(l)}$ to each pixel of $O_t^{'(l)}$ remain as they were in the original image. In those cases we suggest the following mask: 
\begin{equation}
    M_t^{(l)}[i, j] = 
    \begin{cases}
           1 &  i\in O_t^{(l)} \setminus O_t^{'(l)} \enspace\text{or}\enspace j \in O_t^{(l)} \setminus O_t^{'(l)} \\
           0 &\text{otherwise}. \\ 
         \end{cases}
\end{equation}

Removing the pixels of the object of interest from the mask unleashes the object of interest from its connection to the object we are preserving.
The choice of whether to remove $O_t^{'(l)}$ from the mask depends on the user's preferences.

\input{figures/local_ablation_supp}
\input{figures/proxy_words}


