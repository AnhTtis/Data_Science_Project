\documentclass[journal]{IEEEtran}

\ifCLASSINFOpdf

\else

\fi
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage[export]{adjustbox}
\usepackage{stfloats}
\usepackage{lettrine}
\usepackage{tikz}
\usetikzlibrary{calc}
\newtheorem{remark}{Remark}
\newtheorem*{conjecture}{Conjecture}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}{Proposition}
\hyphenation{op-tical net-works semi-conduc-tor}

\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture] \node (#1){};}

\begin{document}
\title{Federated Learning in MIMO Satellite Broadcast System}

\author{Raphael Pinard,~\IEEEmembership{Student Member,~IEEE,}, Mitra Hassani,
        Wayne Lemieux,~\IEEEmembership{Fellow,~IEEE,}}

%


% The paper headers
\markboth{}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}

\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Federated learning (FL) is a type of distributed machine learning at the wireless edge that preserves the privacy of clients' data from adversaries and even the central server. Existing federated learning approaches either use (i) secure multiparty computation (SMC) which is vulnerable to inference or (ii) differential privacy which may decrease the test accuracy given a large number of parties with relatively small amounts of data each. To tackle the problem with the existing methods in the literature, In this paper, we introduce incorporate federated learning in the inner-working of MIMO systems.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
MIMO channel, Linear Antenna, Channel Capacity.
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle


\section{Introduction}
\lettrine{C}{hannel} capacity is defined as the maximum rate at which data can be transmitted at an arbitrary small error probability \cite{gallager1968information}.
The capacity of a single-input–single-output (SISO) additive white Gaussian noise (AWGN) channel was first addressed by Shannon \cite{shannon1948mathematical}. With the invention of powerful space-time coding scheme \cite{tarokh1998space}, \cite{foschini1996layered}, the application of multiple-inputs multiple-output (MIMO) systems has been unraveled to  both academia and industry as mean of achieving channel capacity far beyond that of traditional techniques. This novel approach offers a unique
solution for increasing demand for high performance next generation wireless communications.
\\
In this paper, MIMO satellite communication (SatCom) is considered where the receiver is a linear array antenna. We aim to find the channel capacity for low Earth orbiting satellites whose positions are unknown to the terrestrial receiver. In low orbiting satellites, line of sight (LOS) becomes more dominant and the path loss reduces. Although its efficiency in a rich scattering environment has already been
demonstrated \cite{corazza2007digital}, less is studied under this new scenario. 
\\
The typical way to analyze channel capacity of MIMO systems is to find eigenvalue distribution for the channel matrix multiplied by its conjugate \cite{liang2005ergodic}. In our setup, when the channel is modeled as pure LoS, the channel matrix $H$ would become a Vandermonde matrix \cite{horn2012matrix}. However, for this setup, eigenvalue distribution is unknown and only there have been some studies over asymptotic behavior of it \cite{tucci2011eigenvalue,hamidi2022over,hamidi2019systems}. In \cite{tucci2011eigenvalue,struhsaker2020methods}, the author found a lower and upper bound for the maximum eigenvalue and presented the channel capacity for a sufficiently large matrices. 
In this paper, the average channel capacity and outage probability of such channels are analyzed, assuming the receiver has the perfect channel state information (CSI). This has been done by approximating the channel capacity and the accuracy of our method is justified with simulations.  
\\
The paper is organized as follows. We present the MIMO system model in Section \ref{Model}. Some basics to find the channel capacity of a MIMO system is reviewed in section \ref{sec:MIMO}. in section \ref{E(C)} the average channel capacity of MIMO SatCom systems, by finding moments of channel matrix multiplied by its conjugate, is presented. In \ref{sec:outage}, we discuss the outage capacity of MIMO SatCom systems. In section \ref{sec:max}, the optimum satellite arrangement is found in terms of maximum channel capacity. 




\subsection{Definitions and Assumptions}
\begin{itemize}
\item{In this paper we use $x^{\star}$, $x^{\prime}$ and $x^{\dag}$ to show conjugate, transpose and conjugate transpose of a vector x, respectively. Also, $n_{R}$ and $n_{T}$ are the number of receivers and transmitters respectively. }
\item{A complex random vector $z$ of size $n\time 1$  is said to be Gaussian if the $2n\times 1$ real vector $\hat{x}$ consisting of its real and imaginary parts, i.e. \begin{equation*}
\hat{x} =
\begin{bmatrix} 
\Re(x) \\
\Im(x)
\end{bmatrix},
\end{equation*}
is Gaussian.}
\item{The expectation and covariance of $\hat{x}$ are defined as $\mathbf{E}[\hat{x}] \in \Re^{2n}$ and $\mathbf{E}[(\hat{x}-\mathbf{E(\hat{x})})(\hat{x}-\mathbf{E(\hat{x})})^\dag] \in \Re^{2n}$, respectively.}
\item{A complex Gaussian random vector x is circularly symmetric ~\cite{telatar1999capacity} if the covariance of the corresponding $\hat{x}$ has the structure
\begin{equation*}
\mathbf{E}[(\hat{x}-\mathbf{E(\hat{x})})(\hat{x}-\mathbf{E(\hat{x})})^\dag]=\frac{1}{2}
\begin{bmatrix} 
\Re(Q) & -Im(Q)\\
\Im(Q) & Re(Q)
\end{bmatrix},
\end{equation*}
for some Hermitian non-negative definite matrix $Q\in \mathbb{C}^{n\times n}$}
\end{itemize}

\section{MIMO System Model} \label{Model}
Let us consider a MIMO system in which the transmitters comprise $n_{T}$ satellites revolving around the Earth that their positions are unknown for the user located on the Earth. We further assume that all these satellites are in visibility range of the server and located in a spherical cap. This cap could be defined as $\phi \in (-\pi, \pi)$ and $\theta \in (0 ,\frac{\pi}{2})$, where $\phi$ and $\theta$ are azimuth and elevation angles in standard spherical coordinate. Also, the receiver is a linear antenna made of $n_{R}$ antenna elements.
\\
The transmitted signals in each symbol period are denoted by a $t\times 1$ complex vector x, where the $x_{i}$ refers to the transmitted signal from antenna $i$. The total power of the complex transmitted signal $x$ is constrained to $P$ regardless of the number of transmit antennas, meaning that: 
\begin{align}
\mathbf{E}[x^{\dag}x]=trace(\mathbf{E}[xx^{\dag}])=P 
\end{align} 
The transmitted signal bandwidth is narrow enough, so its
frequency response can be considered as flat. The received
signal $y$ is given by
\begin{align}
y=\mathbf{H}x+n
\end{align}
Where H is a $n_{R}\times n_{T}$ complex channel gain matrix. The $ij^{th}$ entry of the matrix $H$ represents the channel gain from the $j^{th}$ transmit to the $i^{th}$ receive antenna.
The noise at the receiver is denoted by the $r\times 1$ vector $n$. We make an assumption that components of $n$ are statistically independent complex zero mean Gaussian random variables with independent and equal variance of real and imaginary parts. The covariance matrix of n is given by
\begin{align}
\mathbf{E}[nn^{\dag}]=\sigma^2\mathbf{I}_r
\end{align}
Where $\sigma^2$ is the identical noise power at each of the receive antennas.
We assume that the total power per receive antenna is equal to the total transmitted power. In other worlds, signal attenuations and amplifications in the propagation process are ignored. We
also assume that H is perfectly known to the receiver, but not at the transmitter. The channel matrix can be estimate at
the receiver by transmitting a training sequence. The estimated
CSI can be communicated to the transmitter via a reliable
feedback channel.
\section{MIMO channel Capacity} \label{sec:MIMO}
For an evenly spaced antenna array along y-axis, array factor (AF) is (Appendix \ref{app:AF})
\begin{align} \label{eq:4}
AF(\theta, \phi)=\sum_{m=0}^{M-1} I_m e^{jkmdsin(\theta)sin(\phi)}
\end{align}
where $I_m$ is the gain injected to the $m^{th}$ antenna, $k=\frac{2\pi}{\lambda}$ and $\theta$ and $\phi$ are elevation and azimuth angles in standard spherical coordinate. In our setup we assume that all $I_m$'s$~=1$.
\\
On the other hand, each column of matrix H corresponds to the gain received from one satellite over the antenna array, and therefore in each column of H, there is only two random variables ($\theta$ and $\phi$) and the other entries are determined based on the AF formula. Mathematically speaking, H could be written as:
\\

$\mathbf{H}=\frac{1}{\sqrt{n_Tn_R}}\times
\\
\begin{pmatrix*}[c]
1 &...& 1 \\
e^{jkd\sin(\theta_1)\sin(\phi_1)} &. . .&  e^{jkd\sin(\theta_{n_T})\sin(\phi_{n_T})}\\
e^{jk2d\sin(\theta_1)\sin(\phi_1)} & . . .&  e^{jk2d\sin(\theta_{n_T})\sin(\phi_{n_T})}\\
.& ...& .\\
.& ...& .\\
.& ...& .\\
e^{jk (n_{R}-1)d\sin(\theta_1)\sin(\phi_1)} & . . .& e^{jk (n_{R}-1)d\sin(\theta_{n_T})\sin(\phi_{n_T})}\\
\end{pmatrix*}
$
\\
\\
where $\theta_i \sim U(0,\frac{\pi}{2})$ and $\phi_i \sim U(-\pi,\pi)$ to define a symmetric spherical cap above the terrestrial server. This configuration is depicted in Figure \ref{fig1}.
\\


For the case of study, where H is assumed to be known at the receiver and not the transmitter from~\cite{telatar1995capacity} we have: 
\begin{align}
C=\max\limits_{{p(x):tr(\mathbf{E}[xx^{\dag}])=P}} I (x;y,\mathbf{H})
\end{align}

The power received from each satellite is assumed to be equal to $P$. In~\cite{liang2005ergodic} the author simplified the above equation to obtain

\begin{align} \label{eq:6}
C=\mathbf{E}_H \Big\{  \log_2 \Big( det(I_{n_R} +\frac{P}{\sigma^2}HH^\dag \Big) \Big\}
\end{align}

\begin{align}  \label{eq:7}
=\mathbf{E}_H \Big\{  \log_2 \Big( det(I_{n_T} +\frac{P}{\sigma^2}H^\dag H \Big) \Big\}
\end{align}

Equation \eqref{eq:6} is used when $n_R<n_T$ and  \eqref{eq:7} is for when $n_T \leq n_R$. Also we define:  

\[ \mathbf{W}=
    \begin{dcases}  
	 H^\dag H & n_T\leq n_R 
	\\ HH^\dag & n_R<n_T
    \end{dcases}
\]
To find the expected capacity based on equation \eqref{eq:7}, it is common to write $HH^\dag$ in terms of its eigenvalues. This means that~\cite{liang2005ergodic}
\begin{align}  \label{eq:8}
C=\mathbf{E}_{\lambda} \Big\{\sum_{i=1}^{n_T}  \log_2 \big( 1+\frac{P}{\sigma^2} \lambda_i \big)\Big\}
\end{align}
Where $\lambda$'s are the eigenvalues of matrix $ \mathbf{W}$.
\\
On the other hand we know that $\lambda$ depends on $\theta$ and $\phi$ and therefor equation \eqref{eq:8} becomes:
\begin{align}  \label{eq:9}
C=\sum_{i=1}^{n_T} \mathbf{E} \Big\{  \log_2 { \big( 1+ \frac{P}{\sigma^2 } \lambda_i (\theta, \phi) \big)}  \Big\}
\end{align}
or equivalently 
\begin{align}  \label{eq:10}
C=\sum_{i=1}^{n_T} \int  \int \ \log_2 \big( 1+ \frac{P}{\sigma^2} \lambda_i \big) f(\theta,\phi) d\theta d\phi 
\end{align}
where $ f(\theta,\phi)$ is the joint probability distribution between random variables $\theta$ and $\phi$.

\section{Finding the average channel capacity} \label{E(C)}
It could be simply proven (see appendix \ref{th:lambda}) that the trace of a matrix is equal to the summation of its eigenvalues. Therefore, for random $N\times N $matrix $\mathbf{A}$ defined on probability space of $\zeta$ one can say
\begin{align}  \label{eq:11}
\mathbf{E} \Big\{Trace(\mathbf{A}) \Big\}=\mathbf{E}\sum_{i=1}^{N} \lambda_i=\sum_{i=1}^{N}\mathbf{E}\Big\{ \lambda_i \Big\}=\sum_{i=1}^{N} \int_{\zeta} \lambda_i f(\theta,\phi) d\theta d\phi
\end{align}
\begin{theorem} \label{th:lambda}
The trace of a matrix to its kth power is equal to the sum of its eigenvalues raised to the kth power.
\end{theorem}
\begin{proof}
See Appendix \ref{th:lambda}.
\end{proof}
Thus, similarly we have
\begin{align}  \label{eq:12}
\mathbf{E} \Big\{Trace(\mathbf{A^k}) \Big\}=\sum_{i=1}^{N} \int_{\zeta} \lambda_i^k f(\theta,\phi) d\theta d\phi
\end{align}

On the other hand, we can approximate the $\log$ term in equation \ref{eq:10} by using Taylor expansion \cite{hummel1949generalization} as follows
\begin{align}  \label{eq:13}
\log_2 \big( 1+ \frac{P}{\sigma^2} \lambda_i \big)=\frac{1}{\ln(2)} \sum_{k=1}^{\infty} \frac{(-1)^{k+1}  (\frac{P}{\sigma^2}\lambda_i)^{k}}{k}
\end{align}
Plugging this expansion is in equation \eqref{eq:10} yields to
\begin{align}  \label{eq:14}
C=\frac{1}{\ln(2)} \sum_{k=1}^{\infty}  \frac{(-1)^{k+1} P^k}{k~\sigma^{2k}} \sum_{i=1}^{n_T} \int \int \lambda_i^k f(\theta,\phi) d\theta d\phi 
\end{align}
Because $\mathbf{W}$ is a $N\times N$ matrix whose eigenvalues are $\lambda$'s, by using equation \eqref{eq:12} in the above equation we have

\begin{align}  \label{eq:15}
C=\frac{1}{\ln(2)} \sum_{k=1}^{\infty}  \frac{(-1)^{k+1} P^k}{k~\sigma^{2k}} \mathbf{E} \Big\{Trace(\mathbf{W^k}) \Big\}
\end{align}
Thus, one can find the average capacity for the channel defined in this setup by finding $ \mathbf{E} \Big\{Trace(\mathbf{W^k}) \Big\}$. 
\\
In finding average capacity by \eqref{eq:15}, infinite terms must be evaluated among which there is no relation. However, by simulation we show that only three terms of this sigma would give us the average capacity with a high accuracy. 
In the following parts, it is attempted to find the first three terms of the above summation, which indeed requires to evaluate $ \mathbf{E} \Big\{Trace(\mathbf{W^k}) \Big\}$ for $k=1,2,3$.
\\
Later in this section, we justify our approximation. 
\subsection{ Evaluating  the First Term, $ \mathbf{E} \Big\{Trace(\mathbf{W}) \Big\}$}
We denote the entry of matrix $\mathbf{W}$ located in $i^{th}$ row and $j^{th}$ column by $w_{ij}$. Also, we define 
\begin{align}  \label{eq:16}
\gamma_{ij}= \sin(\theta_j)\sin(\phi_j)-\sin(\theta_i)\sin(\phi_i)
\end{align}
By the definition of $\mathbf{W}$ we therefore have
\begin{align}  \label{eq:17}
w_{ij}=\frac{1}{n_Tn_R}\sum_{m=0}^{n_R-1} e^{jkmd\gamma_{ij}}
\end{align}
It is clear that $w_{ii}=\frac{n_R}{n_Rn_T}=\frac{1}{n_T}$ for all $i=1,2,...,n_T$ and so for the first moment of trace we have:
\begin{align}  \label{eq:18}
\mathbf{E} \Big\{Trace(\mathbf{W}) \Big\}=Trace(\mathbf{W})=n_T\times \frac{1}{n_T}=1
\end{align}

\subsection{ Evaluating  the Second Term, $ \mathbf{E} \Big\{Trace(\mathbf{W}^2) \Big\}$}
First, we write the trace of $\mathbf{W}^2$ in terms of its entries 
\begin{align}  \label{eq:19}
Trace(\mathbf{W}^2)=\sum_{i=1}^{N_T} \sum_{j=1}^{N_T} w_{ij}w_{ji}=\sum_{i=1}^{N_T} \sum_{j=1}^{N_T} w_{ij}w_{ij}^{\star} \nonumber \\
=n_R^2n_T+2\sum_{i=1}^{n_T} \sum_{i>j}^{n_T} w_{ij}w_{ij}^{\star}~~~~~~~~~~~~
\end{align}
Where the second last equality is due to the fact that $\mathbf{W}$ is Hermitian. To put away the cases in which $i=j$ -this would be useful in following calculation-, the last equation in \eqref{eq:19} was derived.

To better analyze the summation, we expand the $ w_{ij}w_{ij}^{\star}$ term as follows

\begin{equation*}
\begin{multlined}
w_{ij}w_{ij}^{\star}=\frac{1}{(n_Rn_T)^2}\Big(1+e^{-jkd\gamma_{ij}}+...+e^{-jk(n_R-1)d\gamma_{ij}}+
\\
...+e^{jkd\gamma_{ij}}+1+...+e^{jk(n_R-2)d\gamma_{ij}}+
\\
...+e^{jk(n_R-1)d\gamma_{ij}}+e^{jk(n_R-2)d\gamma_{ij}}+...+1\Big)
 \end{multlined}
\end{equation*}
It is seen that every term would be added to its conjugate, meaning that it could be simplified to $\cos$ terms. By putting away all $1$'s the following equation is derived
\begin{align}  \label{eq:20}
w_{ij}w_{ij}^{\star}=\frac{1}{(n_Rn_T)^2}\Big(n_R+2\sum_{s=1}^{n_R-1} (n_R-s)\cos(skd\gamma_{ij})\Big)
\end{align}
Now by plugging equation \eqref{eq:20} into \eqref{eq:19} we obtain
\begin{align}  \label{eq:21}
(n_Rn_T)^2 \times Trace(\mathbf{W}^2)= \nonumber ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\
n_R^2n_T+2\sum_{i=1}^{n_T} \sum_{i>j}^{n_T} \Big( n_R+2\sum_{s=1}^{n_R-1} (n_R-s)\cos(skd\gamma_{ij}) \Big)&&
\\*
=n_R^2n_T+n_Rn_T(n_T-1)+\nonumber ~~~~~~~~~~~~~\\
4\sum_{i=1}^{n_T} \sum_{i>j}^{n_T}\sum_{s=1}^{n_R-1} (n_R-s)\cos(skd\gamma_{ij})~~~~~~~~~
\end{align}
Considering that $\gamma_{ij}$ is a function of $(\theta_i, \phi_i,\theta_j,\phi_j)$, to find the expected value of $Trace(\mathbf{W}^2)$ we have:
\begin{align}  \label{eq:23}
(n_Rn_T)^2 \times \mathbf{E} \Big\{Trace(\mathbf{W}^2) \Big\}=n_R^2n_T+n_Rn_T(n_T-1)+\nonumber ~~~~~~~~~~~~~\\
4\sum_{i=1}^{n_T} \sum_{i>j}^{n_T}\sum_{s=1}^{n_R-1} (n_R-s) \int_\zeta \cos(skd\gamma_{ij})~~~~~~~~~~~~~
\end{align}
where $\zeta$ is the probability space.
\\
Henceforth, we get down to finding $\int_\zeta \cos(skd\gamma_{ij})$ by substituting back the original value for $\gamma_{ij}$. Also,  
\begin{align}  \label{eq:24}
\cos(skd( \sin(\theta_j)\sin(\phi_j)-\sin(\theta_i)\sin(\phi_i)))= \nonumber \\
\cos(skd(\sin(\theta_j)\sin(\phi_j)))\cos(skd(\sin(\theta_i)\sin(\phi_i)))- \nonumber \\
\sin(skd(\sin(\theta_j)\sin(\phi_j)))\sin(skd(\sin(\theta_i)\sin(\phi_i)))
\end{align}

Meanwhile, we recall that $\theta_i$ and $\theta_j$  $\sim U(0,\frac{\pi}{2})$, and $\phi_i$ and $\phi_j$  $\sim U(-\pi,\pi)$, all of which are independent random variables; thus


\begin{align}  \label{eq:25}
\int_\zeta \cos(skd\gamma_{ij})=~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \nonumber \\
(\frac{1}{\frac{\pi}{2}}\frac{1}{2\pi})^2\Big(\int_{0}^{\frac{\pi}{2}}\int_{-\pi}^{\pi} \cos(skd(\sin(\theta_j)\sin(\phi_j))) d(\phi_j)d(\theta_j) \times  ~~~~~~ \nonumber \\ 
\int_{0}^{\frac{\pi}{2}}\int_{-\pi}^{\pi}\cos(skd(\sin(\theta_i)\sin(\phi_i)))d(\phi_i)d(\theta_i))\Big) - ~~~~~~~~~~\nonumber \\ 
(\frac{1}{\frac{\pi}{2}}\frac{1}{2\pi})^2\Big(\int_{0}^{\frac{\pi}{2}}\int_{-\pi}^{\pi} \sin(skd(\sin(\theta_j)\sin(\phi_j))))d(\phi_j)d(\theta_j)\times  ~~~~~~\nonumber \\ 
\int_{0}^{\frac{\pi}{2}}\int_{-\pi}^{\pi}\sin(skd(\sin(\theta_i)\sin(\phi_i)))) d(\phi_i)d(\theta_i) \Big) = ~~~~~~~~~ \\ \label{eq:26}
(\frac{1}{\frac{\pi}{2}}\frac{1}{2\pi})^2\Big(\int_{0}^{\frac{\pi}{2}}\int_{-\pi}^{\pi} \cos(skd(\sin(\theta_j)\sin(\phi_j))))d(\phi_j)d(\theta_j)\times  ~~~~~~ \nonumber \\ 
\int_{0}^{\frac{\pi}{2}}\int_{-\pi}^{\pi}\cos(skd(\sin(\theta_i)\sin(\phi_i)))) d(\phi_i)d(\theta_i)\Big)= ~~~~~~~~~\\
(\frac{1}{\frac{\pi}{2}})^2\Big(\int_{0}^{\frac{\pi}{2}}J_0(skd\sin(\theta_j))d(\theta_j) \times \int_{0}^{\frac{\pi}{2}}J_0(skd\sin(\theta_i))d(\theta_i) \Big)~~~~ \label{eq:27}
\end{align}
Where equation \eqref{eq:26} is hold because $\sin$ is an odd function and the inner integral is symmetric from $-\pi$ to $\pi$. 
\\
In equation \eqref{eq:27} $J_0(.)$ is the Bessel function of the first kind. Also because $\theta_i$ and $\theta_j$ have the same distribution, we can further simplify it to
\begin{align}  \label{eq:28}
\int_\zeta \cos(skd\gamma_{ij})=(\frac{1}{\frac{\pi}{2}})^2 \Big(\int_{0}^{\frac{\pi}{2}}J_0(skd\sin(\theta_j))d(\theta_j)\Big)^2 \nonumber \\
=(\frac{1}{\frac{\pi}{2}})^2 \Big( \int_{0}^{1} \frac{J_0(skd\mu)}{\sqrt{1-\mu^2}}d\mu\Big)^2~~~~~~~~~~~~~
\end{align}
Where the last equation is found by putting $\sin(\theta_j)=\mu$.
From \cite{prudnikov1986integrals}, part 2.12.21, we can find the above integral as follows:
\begin{align}  \label{eq:29}
=(\frac{1}{\frac{\pi}{2}})^2 \Big(\int_{0}^{1} \frac{J_0(skd\mu)}{\sqrt{1-\mu^2}}d\mu \Big)^2 \nonumber \\ 
(\frac{1}{\frac{\pi}{2}})^2 \Big(\frac{\pi}{2} J_0  \big( \frac{skd}{2} \big)^2\Big)^2  \nonumber \\ 
=J_0  \big( \frac{skd}{2} \big)^4
\end{align}
Therefore, the equation \eqref{eq:23} becomes
\begin{align} \label{eq:30}
\mathbf{E} \Big\{Trace(\mathbf{W}^2) \Big\}=\frac{1}{(n_Rn_T)^2}\Big(n_R^2n_T+n_Rn_T(n_T-1)+\nonumber ~~~~~~~~~~~~~\\
2\sum_{s=1}^{n_R-1}(n_R-s)n_T(n_T-1) J_0  \big( \frac{skd}{2} \big)^4\Big)~~~~~~~~~~~~~
\end{align}

\subsection{ Evaluating  the Third Term, $\mathbf{E} \Big\{Trace(\mathbf{W}^3) \Big\} $}
The formulas written in this part, because of mathematical complexity, are briefly mentioned without proof. One can simply prove them all by taking the same steps as the previous part.
\\
The formula to find $ Trace(\mathbf{W}^3) $ is as follows
\begin{align} \label{eq:31}
Trace(\mathbf{W}^3)=\frac{n_R^3n_T}{(n_Rn_T)^3}+ 6\sum_{i=1}^{n_T} \sum_{j>i}^{n_T}w_{ij} \sum_{s=i}^{n_T}w_{si}w_{js}+ \nonumber \\
\sum_{i=1}^{n_T} \sum_{j\neq i}^{n_T}w_{ij} \sum_{s\neq i,j}^{n_T}w_{si}w_{js}~~~~~~~~~~~~~~
\end{align}
and thus, by finding the expected value of the above equation with the same approach as the expected value for $Trace(\mathbf{W}^2)$, one can prove that

\begin{flalign}
\mathbf{E} \Big\{Trace(\mathbf{W}^3) \Big\}= \frac{1}{(n_Rn_T)^3} \Bigg(n_R^3nT+3n_R^2n_T(n_T-1)~~~~~~ \nonumber \\
+n_Rn_T(n_T-1)(n_T-2)~~~~~~ ~~~~~~ \nonumber \\
+6\sum_{s=1}^{n_R-1}n_R(n_R-s)n_T(n_T-1) J_{0} \big( \frac{skd}{2} \big)^2~~~~~~ ~~~~~~ \nonumber \\+6\sum_{s=1}^{n_R-1}(n_T-2)\prod_{k=0}^2(k+s)J_{0} \big( \frac{(n_R-s)kd}{2} \big)^2~~~~~ ~~~\nonumber \\
+6\sum_{s=1}^{\frac{n_R-1}{2}}(n_T-2)\prod_{k=0}^2(n_R-2s+k)J_{0} \big( \frac{skd}{2} \big)^2 J_{0} \big( {skd} \big)~~~~~ ~~~\nonumber \\
+\Big(6\sum_{s=1}^{n_R-1}\sum_{t=1}^{n_R-2s} (n_R-2m)^{+}(n_T-2) \times~~~~~ ~~~ \nonumber \\
\prod_{k=0}^2 (2n_R-2t-2s+2+k) \times ~~~~~ ~~~\nonumber \\
J_{0} \big( \frac{skd}{2} \big) J_{0} \big( \frac{(s+t)kd}{2} \big)J_{0} \big( \frac{(2s+t)kd}{2} \big) \Big)\Bigg)~~~~ ~~~~~
\end{flalign}

In the following figure, the channel capacity for a setup with 64 receive antenna and 64 satellite is simulated. The average capacity is found by generating 100000 random matrices. The results show the accuracy of first-three-term approximation.
\begin{figure}[htbp]
\centering{\includegraphics[scale=0.5]{64}}
    \caption{Channel Capacity for $64\times 64$ MIMO channel.}
    \label{CDD}
\end{figure}

%\section{Channel outage capacity} \label{sec:outage}
%Since H is a random matrix, the channel capacity is a random variable. This means that regardless of the rate threshold, there is a non-zero probability that the channel cannot satisfy low error rates \cite{telatar1999capacity}. To describe such channels, the capacity complementary cumulative distribution function is used, known as ccdf \cite{foschini1998limits}, \cite{smith2002gaussian}. This number shows the probability of achieving a specified channel capacity, denoted by $P_{ccdf}$. However, the outage capacity probability is the probability of not achieving a threshold capacity and it is defined as follows
%\begin{align} \label{eq:33}
%P_{out}= prob \{C<R_{th}\}
%\end{align}
%where $prob \{ . \}$ is taken over all realization of $H$. It is worth mentioning to say that $P_{out}=1-P_{ccdf}$.
%To find the channel capacity the following conjecture is made and left without proof. However, with many simulations, the accuracy of it has been proved to the authors.
%\begin{theorem} \label{theorem2}
%MIMO channel capacity of the channel described in this paper can be approximated by
%Gaussian approximation, for the case when the receiver has the perfect CSI but the transmitter does not
%\end{theorem}
%This means that to find the outage probability we only need to find $\mathbf{E} \{C \}$ and $\mathbf{Var}  \{C \}$.
%\\
%To prove the theorem \ref{theorem2}, we refer back to \eqref{eq:14}. Based on the intuition we have obtained so far, the behavior of the channel capacity is mainly dependent on the first three moments of $Trace\{\mathbf{W}\}$. The first moment is a constant number, and therefore the second and the third moments play a substantial role in determining the distribution of channel capacity. 
%\\
%\begin{lemma}
%The distribution of $Trace\{\mathbf{W^2}\}$ converges to the Normal distribution when $n_T$ is sufficiently large.
%\end{lemma}
%\begin{proof}
%What determines the distribution of $Trace\{\mathbf{W^2}\}$ is the second term of equation \eqref{eq:22} which could be re-written as 
%\begin{align}
%\Omega=\sum_{i=1}^{n_T} \sum_{j=1}^{n_T}\sum_{s=1}^{n_R-1} (n_R-s)\cos(skd\gamma_{ij})
%\end{align}
%in which the coefficient behind the sum is eliminated (with respect to equation \eqref{eq:22}). 
%\\
%The summation over $i$ and $j$ in $\Omega$ could be scrambled and replaced by a single summation over a new variable $l$. Also, we define new functions $F_{l}$ , $1 \leq l \leq n_T$, as shown in. By this definition we have
%\begin{align}
%\Omega=\sum_{l=1}^{n_T} F_{l}
%\end{align}
%
%\begin{figure*}[!t]
%\normalsize
%\begin{equation}
%\label{F1}
%\scriptstyle{F_1=\underbrace{\sum_{s=1}^{n_R-1} (n_R-s)\cos(skd\gamma_{11})}_{F_{11}}+\underbrace{\sum_{s=1}^{n_R-1} (n_R-s)\cos(skd\gamma_{22})}_{F_{12}}+
%\underbrace{\sum_{s=1}^{n_R-1} (n_R-s)\cos(skd\gamma_{33})}_{F_{13}}+...+\underbrace{\sum_{s=1}^{n_R-1} (n_R-s)\cos(skd\gamma_{n_Tn_T})}_{F_{1n_T}}} 
%\end{equation}
%\begin{equation}
%\label{F2}
%\scriptstyle{F_2=\underbrace{\sum_{s=1}^{n_R-1} (n_R-s)\cos(skd\gamma_{12})}_{F_{21}}+\underbrace{\sum_{s=1}^{n_R-1} (n_R-s)\cos(skd\gamma_{23})}_{F_{22}}+
%\underbrace{\sum_{s=1}^{n_R-1} (n_R-s)\cos(skd\gamma_{34})}_{F_{23}}+...+\underbrace{\sum_{s=1}^{n_R-1} (n_R-s)\cos(skd\gamma_{n_T1})}_{F_{2n_T}}} 
%\end{equation}
%\begin{equation}
%.\nonumber
%\end{equation}
%
%\begin{equation}
%.\nonumber
%\end{equation}
%
%\begin{equation}
%.\nonumber
%\end{equation}
%
%\begin{equation}
%\scriptstyle{F_{n_T}=\underbrace{\sum_{s=1}^{n_R-1} (n_R-s)\cos(skd\gamma_{1n_T})}_{F_{n_T1}}+\underbrace{\sum_{s=1}^{n_R-1} (n_R-s)\cos(skd\gamma_{21})}_{F_{n_T2}}+
%\underbrace{\sum_{s=1}^{n_R-1} (n_R-s)\cos(skd\gamma_{32})}_{F_{n_T3}}+...+\underbrace{\sum_{s=1}^{n_R-1} (n_R-s)\cos(skd\gamma_{n_Tn_{T-1}})}_{F_{n_Tn_T}}} 
%\end{equation}
%
%\hrulefill
%\vspace*{4pt}
%\end{figure*}
%Now, we note that all $n_T$ terms in each of $F_{l}$'s, where $1 \leq l \leq n_T$, are mutually independent. For instance, we consider $F_{1}$. It is seen that all $F_{1l}$ terms are mutually independent and also have them same distribution; therefore the Central Limit Theorem (CLT) is hold for $F_1$ and so is for other $F_l$'s. Hence, all $F_{l}$'s are normally distributed.  
%\\
%However, $F_{l}$'s are mutually dependent. Nevertheless, the sum of correlated Gaussian distribution is also Gaussian. Henceforth, we aim to find the parameters for this distribution. 
%\\
%First, we aim to find the expected value of $\Omega$ denoted by $\mu_{\Omega}$. Due to symmetry, the expected value for all $F_{i,j}$'s, where $1 \leq i \neq j \leq n_T$, are the same. For $F_{12}$, for instance, we have
%\begin{align}  \label{E(F12)}
%\mathbf{E} (F_{12})=\sum_{s=1}^{n_R-1}(n_R-s) J_0  \big( \frac{skd}{2} \big)^4
%\end{align}
%\\
%Also, for the all $F_{i,j}$'s with $1 \leq i = j \leq n_T$, we have $\cos(skd\gamma_{ii})=1$, and therefore, 
%\begin{align}  \label{E(F11)}
%\mathbf{E} (F_{11})=\sum_{s=1}^{n_R-1}(n_R-s)= \frac{n_R(n_R-1)}{2}
%\end{align}
%There are $n_T^2-n_T$ and  $n_T$ terms in $\Omega$ with the same expected value as $F_{12}$ and $F_{11}$, respectively. Therefore, 
%
%\begin{align}  \label{mu_omega}
%\mu_{\Omega}=
%\end{align}
%
%
%
%The correlated terms between $F_{1}$ and $F_2$ are the pairs $(F_{11},F_{21})$, $(F_{12},F_{22})$,..., $(F_{1n_T},F_{2n_T})$ and the other $n_T\times \ (n_T-1)$ terms are mutually independent. Hence we have
%\begin{align}  \label{covF}
%\mathbf{E}(F_1F_{2})-\mathbf{E}(F_1)\mathbf{E}(F_{2})  ~~~~~~~~\nonumber \\
%=\sum_{r=1}^{n_T} \mathbf{E} (F_{1r}F_{2r})-\sum_{r=1}^{n_T} \mathbf{E} (F_{1r})\mathbf{E} (F_{2r})  ~~~~~~~~\nonumber \\
%=n_T \times \mathbf{E} (F_{1n_T}F_{2n_T})-n_T \times \mathbf{E} (F_{1n_T})\mathbf{E} (F_{2n_T})
%\end{align}
%Where the last equation is obtained because of the symmetry. To evaluate equation \eqref{covF}, first we find $\mathbf{E} (F_{1n_T}F_{2n_T})$ and then $\mathbf{E} (F_{1n_T})\mathbf{E} (F_{2n_T})$.
%
%\begin{itemize}
%\item{$\mathbf{E} (F_{1n_T}F_{2n_T})$}
%\\
%First, we evaluate the terms in which coefficient $s$ is common, i.e., $A=\mathbf{E}\big(\cos(skd\gamma_{1n_T}) \cos(skd\gamma_{2n_T})  \big)$, where $s=1,2,...,n_R-1$.
%\begin{align}  \label{cos}
%\cos(skd\gamma_{1n_T}) \cos(skd\gamma_{2n_T})  ~~~~~~~~\nonumber \\
%=\frac{1}{2} \Big( \underbrace{\cos \big( skd(\gamma_{1n_T}+\gamma_{2n_T}) \big)}_{A1}+ \underbrace{\cos \big( skd(\gamma_{1n_T}-\gamma_{2n_T}) \big)}_{A2}  \Big) \nonumber \\
%\end{align}
%Now, we find $\mathbf{E} (A1)$ and $\mathbf{E} (A2)$ separately. Based on the definition of $\gamma$, we have
%\begin{align}  \label{A1}
%\mathbf{E} (A1)=\mathbf{E} \bigg( \cos \Big( skd \big( \sin(\theta_1)\sin(\phi_1) +\sin(\theta_2)\sin(\phi_2) \nonumber \\ 
%- 2\sin(\theta_{n_T})\sin(\phi_{n_T}) \big) \Big) \bigg) ~~~~~~~~~~~~~\nonumber \\
%=\mathbf{E} \bigg( \cos \Big( skd \big( \sin(\theta_1)\sin(\phi_1) +\sin(\theta_2)\sin(\phi_2)\big) \Big) \bigg)\nonumber \\
%\times \mathbf{E}\bigg(\cos \Big( skd \big( 2\sin(\theta_{n_T})\sin(\phi_{n_T}) \big) \Big) \bigg) ~~~~~~~~~~~~~\\ \label{eq:sin}
%=\mathbf{E}\bigg(\cos \Big( skd\sin(\theta_{1})\sin(\phi_{n1}) \Big) \bigg) ~~~~~~~~~~~~~\nonumber \\
%\times \mathbf{E}\bigg(\cos \Big( skd\sin(\theta_{2})\sin(\phi_{n2}) \Big) \bigg) ~~~~~~~~~~~~~\nonumber \\
%\times \mathbf{E}\bigg(\cos \Big( 2skd\sin(\theta_{n_T})\sin(\phi_{n_T}) \Big) \bigg) ~~~~~~~~~~~~~\\ \label{eq:j}
%=J_0  \big( \frac{skd}{2} \big)^4 \times J_0  \big( skd \big)^2 ~~~~~~~~~~~~~
%\end{align}
%Where \eqref{eq:sin} is hold because the $\sin(.)$ term in $\cos(.)$ expansion is odd. Also \eqref{eq:j} could be derived as the same as equation \eqref{eq:29}.
%\\
%$\mathbf{E} (A2)$ could be found similarly and for the sake of brevity we only mention the final answer
%\begin{align}  \label{A2}
%\mathbf{E} (A2)=J_0  \big( \frac{skd}{2} \big)^4
%\end{align}
%Therefore 
%\begin{align}  \label{Ecos}
%A=\mathbf{E}\big( \cos(skd\gamma_{1n_T}) \cos(skd\gamma_{2n_T})  \big) \nonumber \\
%\frac{1}{2} J_0  \big( \frac{skd}{2} \big)^4 \big( 1+  J_0  \big( skd \big)^2   \big)
%\end{align}
%Second, we evaluate the terms with different coefficient $s$ and we call them $s_ 1$ and $s_2$. As equation \eqref{cos} we have: 
%\begin{align}  \label{cos2}
%\cos(s_1kd\gamma_{1n_T}) \cos(s_2kd\gamma_{2n_T})  ~~~~~~~~~~~~~~~~~~\nonumber \\
%=\frac{1}{2} \Big( \underbrace{\cos \big( s_1kd\gamma_{1n_T}+s_2kd\gamma_{2n_T} \big)}_{B1}+ \underbrace{\cos \big( s_1kd\gamma_{1n_T}-s_2kd\gamma_{2n_T} \big)}_{B2}  \Big) \nonumber \\
%\end{align}
%Similar to  $\mathbf{E} (A1)$, one can find $\mathbf{E} (B1)$ as follows
%\begin{align}  \label{B1}
%\mathbf{E} (B1)
%=J_0  \big( \frac{s_1kd}{2} \big)^2 J_0  \big( \frac{s_2kd}{2} \big)^2 J_0  \big( \frac{(s_1+s_2)kd}{2} \big)^2 
%\end{align}
%Also, 
%\begin{align}  \label{B2}
%\mathbf{E} (B2)
%=J_0  \big( \frac{s_1kd}{2} \big)^2 J_0  \big( \frac{s_2kd}{2} \big)^2 J_0  \big( \frac{(s_1-s_2)kd}{2} \big)^2 
%\end{align}
%Therefore, 
%
%\begin{align}  \label{B1+B2}
%B=\mathbf{E} \big( \cos(s_1kd\gamma_{1n_T}) \cos(s_2kd\gamma_{2n_T}) \big) ~~~~~~~~~~~~~~~~~~\nonumber \\
%=\frac{J_0  \big( \frac{s_1kd}{2} \big)^2 J_0  \big( \frac{s_2kd}{2} \big)^2}{2} \Big(J_0 \big( \frac{(s_1+s_2)kd}{2} \big)^2 + J_0 \big( \frac{(s_1-s_2)kd}{2} \big)^2 \Big)
%\end{align}
%
%Now, we return back to find $\mathbf{E} (F_{1n_T}F_{2n_T})$. In this term, there are some $A$ and $B$ type terms each of which appearing with different coefficients. We break down the terms in $ F_{1n_T}$ and $F_{2n_T}$ as shown in table \ref{table:1}. $F_{1n_T}$ and $F_{2n_T}$ and their corresponding terms are written in each column. In this table, the red and blue arrows show expected value of type A and B, respectively.
%
%\begin{table}[h!]
%    \begin{tabular}[t]{ll}
%    $\mathbf{F_{1n_T}}$  & ~~~~~~~~~~$ \mathbf{F_{2n_T}}$   \\ \hline \hline
% $(n_R-1)\cos(kd\gamma_{1n_T})$~\tikzmark{11} &~~~~~~~~~~$\tikzmark{12} (n_R-1)\cos(kd\gamma_{2n_T})$  \\ 
%$(n_R-2)\cos(2kd\gamma_{1n_T})$\tikzmark{21}&~~~~~~~~~~$\tikzmark{22} (n_R-2)\cos(2kd\gamma_{2n_T})$  \\ 
%%$(n_R-3)\cos(3kd\gamma_{1n_T})$\tikzmark{31}&~~~~~~~~~~$\tikzmark{32} (n_R-3)\cos(3kd\gamma_{2n_T})$  \\ 
%.&~~~~~~~~~~. \\
%.&~~~~~~~~~~.\\
%.&~~~~~~~~~~.\\
%$\cos((n_R-1)kd\gamma_{1n_T})$~\tikzmark{L1}&~~~~~~~~~~$\tikzmark{L2} \cos((n_R-1)kd\gamma_{2n_T})$  \\ 
%    \end{tabular}
%\caption{The terms in $ F_{1n_T}$ and $F_{2n_T}$. The red and blue arrows show expected value of type A and B, respectively}
%\label{table:1}
%\end{table}
%
%\tikz[overlay,remember picture]\draw[red,<->] ($(11)$)+(.1em,0.2em)--($(12)+(-.6em,0.2em)$);
%\tikz[overlay,remember picture]\draw[red,<->] ($(21)$)+(.1em,0.2em)--($(22)+(-.6em,0.2em)$);
%%\tikz[overlay,remember picture]\draw[red,<->] ($(31)$)+(.1em,0.2em)--($(32)+(-.6em,0.1em)$);
%\tikz[overlay,remember picture]\draw[red,<->] ($(L1)$)+(.1em,0.2em)--($(L2)+(-.6em,0.2em)$);
%
%\tikz[overlay,remember picture]\draw[blue,<->] ($(11)+(.1em,0.2em)$)--($(22)+(-.6em,0.2em)$);
%%\tikz[overlay,remember picture]\draw[blue,<->] ($(11)+(.1em,0.2em)$)--($(32)+(-.6em,0.2em)$);
%\tikz[overlay,remember picture]\draw[blue,<->] ($(11)+(.1em,0.2em)$)--($(L2)+(-.6em,0.2em)$);
%
%\tikz[overlay,remember picture]\draw[blue,<->] ($(21)+(.1em,0.2em)$)--($(12)+(-.6em,0.2em)$);
%%\tikz[overlay,remember picture]\draw[blue,<->] ($(21)+(.1em,0.2em)$)--($(32)+(-.6em,0.2em)$);
%\tikz[overlay,remember picture]\draw[blue,<->] ($(21)+(.1em,0.2em)$)--($(L2)+(-.6em,0.2em)$);
%
%\tikz[overlay,remember picture]\draw[blue,<->] ($(L1)+(.1em,0.2em)$)--($(12)+(-.6em,0.2em)$);
%\tikz[overlay,remember picture]\draw[blue,<->] ($(L1)+(.1em,0.2em)$)--($(22)+(-.6em,0.2em)$);
%
%Hence, 
%
%\begin{align}  \label{red}
%\mathbf{E} (F_{1n_T}F_{2n_T})=\mathbf{E} \{ Red ~Arrows\} + \mathbf{E} \{ Blue~ Arrows\} 
%\end{align}
%Summation over red arrows, which are expected value of type A, yields to
%
%\begin{align}  \label{red}
%\mathbf{E} \{ Red ~Arrows\}=\sum_{s=1}^{n_R-1} (n_R-s)^2 \nonumber \\
%\times \frac{1}{2} J_0  \big( \frac{skd}{2} \big)^4 \big( 1+  J_0  \big( skd \big)^2   \big)
%\end{align}
%Also for the blue arrows we have
%\begin{align}  \label{blue}
%\mathbf{E} \{ Blue ~Arrows\}=\sum_{s_2=1}^{n_R-1} \sum_{s_1\neq s_2}^{n_R-1} (n_R-s_2)(n_R-s_1) \nonumber \\
%\times \frac{J_0  \big( \frac{s_1kd}{2} \big)^2 J_0  \big( \frac{s_2kd}{2} \big)^2}{2}~~~~~~~~~~~ \nonumber \\
%\times \Big(J_0 \big( \frac{(s_1+s_2)kd}{2} \big)^2 + J_0 \big( \frac{|s_1-s_2|kd}{2} \big)^2 \Big)
%\end{align}
%
%
%\item{$\mathbf{E} (F_{1n_T})\mathbf{E} (F_{2n_T})$}
%\\
%Due to symmetry we have $\mathbf{E} (F_{1n_T})=\mathbf{E} (F_{2n_T})$ and
%\begin{align}  \label{E(F1)E(F2)}
%\mathbf{E} (F_{1n_T})=\sum_{s=1}^{n_R-1}(n_R-s) J_0  \big( \frac{skd}{2} \big)^4
%\end{align}
%\end{itemize}
%
%Now, we aim to find the denominator of equation \eqref{eq:CF}. First we note that, due to symmetry, $\sqrt {\mathbf{Var}(F_1)\mathbf{Var}(F_{n+1})}=\mathbf{Var}(F_{1})$.
%\\
%Also,
%\begin{align}  \label{Varcov}
%\mathbf{Var}(F_{1})=\sum_{i=1}^{n_T} \mathbf{Var}(F_{1i})+ \sum_{i=1}^{n_T} \sum_{i\neq j}^{n_T} \mathbf{Cov}(F_{1i}F_{1j}) \\
%=n_T\mathbf{Var}(F_{1i})+\frac{n_T(n_T-1)}{2}\mathbf{Cov}(F_{1n_T}F_{1n_{T}-1})
%\end{align}
%Where the latter is obtained due to symmetry. $\mathbf{Cov}(F_{1n_T}F_{1n_{T}-1})$ was implicitly obtained in the previous part. 
%For $\mathbf{Var}(F_{1n_T})$, we need to first find 
%\begin{align}  \label{Var}
% \mathbf{E}(F_{1n_T})=\sum_{s=1}^{n_R-1}(n_R-s) J_0  \big( \frac{skd}{2} \big)^4
%\end{align}
%Furthermore, 
%\begin{align} 
% \mathbf{Var}(F_{1n_T})=\sum_{s=1}^{n_R-1} (n_R-s)^2 
%\times \frac{1}{2} \big( 1+  J_0  \big( skd \big)^4   \big) \nonumber \\
%+\sum_{s_2=1}^{n_R-1} \sum_{s_1\neq s_2}^{n_R-1} (n_R-s_2)(n_R-s_1) \nonumber \\
%\times \frac{1}{2}\Big(J_0 \big( \frac{(s_1+s_2)kd}{2} \big)^4 + J_0 \big( \frac{|s_1-s_2|kd}{2} \big)^4 \Big) \nonumber \\
%-\Big( \sum_{s=1}^{n_R-1}(n_R-s) J_0  \big( \frac{skd}{2} \big)^4 \Big)^2
%\end{align}
%All in all, correlation coefficient could be simplified as stated in equation 
%
%\begin{figure*}[!t]
%\normalsize
%
%\begin{equation}
%\scriptstyle {C_F(n)=\frac{\mathbf{Cov}(F_{1n_T}F_{1n_{T}-1})}{(\frac{n_T-1}{2})\mathbf{Cov}(F_{1n_T}F_{1n_{T}-1})+\sum_{s_2=1}^{n_R-1} \sum_{s_1=1}^{n_R-1} (n_R-s_2)(n_R-s_1) \frac{1}{2}\Big(J_0 \big( \frac{(s_1+s_2)kd}{2} \big)^4 + J_0 \big( \frac{|s_1-s_2|kd}{2} \big)^4 \Big)-\sum_{s_2=1}^{n_R-1}\sum_{s_1=1}^{n_R-1} (n_R-s_2)(n_R-s_1) J_0  \big( \frac{s1kd}{2} \big)^4 J_0  \big( \frac{s2kd}{2} \big)^4}}
%\end{equation}
%\begin{equation}
%\label{CFmain}
%\scriptstyle {C_F(n)=\frac{\sum_{s_2=1}^{n_R-1} \sum_{s_1=1}^{n_R-1} (n_R-s_2)(n_R-s_1)
%\frac{J_0  \big( \frac{s_1kd}{2} \big)^2 J_0  \big( \frac{s_2kd}{2} \big)^2}{2}
% \Big(J_0 \big( \frac{(s_1+s_2)kd}{2} \big)^2 + J_0 \big( \frac{|s_1-s_2|kd}{2} \big)^2 \Big)-\Big( \sum_{s=1}^{n_R-1}(n_R-s) J_0  \big( \frac{skd}{2} \big)^4 \Big)^2}
%{\sum_{s_2=1}^{n_R-1} \sum_{s_1=1}^{n_R-1} (n_R-s_2)(n_R-s_1) \frac{1}{2}\Big(J_0 \big( \frac{(s_1+s_2)kd}{2} \big)^4 + J_0 \big( \frac{|s_1-s_2|kd}{2} \big)^4 \Big)}}
%\end{equation}
%\hrulefill
%\vspace*{4pt}
%\end{figure*}
%
%\end{proof}
%The remaining of this section is divided into 2 subsections; In the \ref{variance}, we find the variance of the channel and in \ref{simout}, $P_{out}$ is calculated and some simulation results are provided to demonstrate the accuracy of the conjecture we made. 
%
%\subsection{Variance of the channel capacity} \label{variance}
%Since $\mathbf{Var}  \{C \}=\mathbf{E} \{C^2 \}- \big( \mathbf{E}\{C \} \big)^2$ and we have already found $\mathbf{E} \{C \}$ in \ref{E(C)}, we aim to find $\mathbf{E} \{C^2 \}$ in this part. 
%\\
%We have
%\begin{align}  \label{eq:34}
%C^2=\Big( \sum_{i=1}^{n_T}  \log_2 \big( 1+\frac{P}{\sigma^2} \lambda_i \big) \Big)^2~~~~~~~~~~~~~~~~~~ \nonumber \\
%=\underbrace {\sum_{i=1}^{n_T}  \log_2^2 \big( 1+\frac{P}{\sigma^2} \lambda_i \big)}_{C1}+\underbrace {\sum_{i\neq j}^{n_T}\log_2 \big( 1+\frac{P}{\sigma^2} \lambda_i \big) \log_2 \big( 1+\frac{P}{\sigma^2} \lambda_j \big)}_{C2}
%\end{align}
%and each $\log_2(.)$ term can be approximated by its Taylor expansion. The results we obtained in section \ref{E(C)} suggest that we can only keep those terms with their power less than equal to 3 to approximate the $\log$ terms. 
%\\
%First we find $C1$
%\begin{align}  \label{eq:35}
%C1=\big( \frac{1}{\ln(2)}\big)^2 \sum_{i=1}^{n_T} \Big(\frac{P}{\sigma^2} \lambda_i  -\frac{(\frac{P}{\sigma^2} \lambda_i)^2}{2}+\frac{(\frac{P}{\sigma^2} \lambda_i)^3}{3}+... \Big)^2 \nonumber \\
%=\big( \frac{1}{\ln(2)}\big)^2\sum_{i=1}^{n_T} \Big(\big( \frac{P}{\sigma^2} \lambda_i \big)^2-\big( \frac{P}{\sigma^2} \lambda_i \big)^3+...\Big) \nonumber \\
%\approx \big( \frac{1}{\ln(2)}\big)^2 \Big( \big( \frac{P}{\sigma^2}\big)^2 Trace(\mathbf{W}^2)-\big( \frac{P}{\sigma^2}\big)^3 Trace(\mathbf{W}^3) \Big)
%\end{align}
%For $C2$ we have
%\begin{align}  \label{eq:36}
%C2=\big( \frac{1}{\ln(2)}\big)^2~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \nonumber \\
%\times \sum_{i\neq j}^{n_T} \Big(\frac{P}{\sigma^2} \lambda_i  -\frac{(\frac{P}{\sigma^2} \lambda_i)^2}{2}+...\Big) \Big(\frac{P}{\sigma^2} \lambda_j  -\frac{(\frac{P}{\sigma^2} \lambda_j)^2}{2}+...\Big)  ~~~ ~~~~~\nonumber \\
%\approx \big( \frac{1}{\ln(2)}\big)^2 \sum_{i\neq j}^{n_T} \Big(\big( \frac{P}{\sigma^2}\big)^2 \lambda_i  \lambda_j - \big( \frac{P}{\sigma^2}\big)^3 \frac{\lambda_i^2 \lambda_j}{2}- \big( \frac{P}{\sigma^2}\big)^3 \frac{\lambda_i \lambda_j^2}{2}\Big)~~~~~\nonumber \\
%=\big( \frac{1}{\ln(2)}\big)^2 \Big( \underbrace { \sum_{i\neq j}^{n_T} \big( \frac{P}{\sigma^2}\big)^2 \lambda_i  \lambda_j}_{C21} - \underbrace { \sum_{i\neq j}^{n_T}  \big( \frac{P}{\sigma^2}\big)^3 \lambda_i^2}_{C22} \lambda_j \Big)~~~~~~~~~~
%\end{align}
%To find $C21$ we know that
%\begin{align}  \label{eq:37}
%\big(Trace(\mathbf{W})\big)^2=\Big( \sum_{i=1}^{n_T} \big(\lambda_i \big) \Big)^2  \nonumber \\
%= \sum_{i=1}^{n_T} \big(\lambda_i^2 \big)+  \sum_{i\neq j}^{n_T} \lambda_i  \lambda_j  \nonumber \\ 
%=Trace(\mathbf{W^2})+ \sum_{i\neq j}^{n_T} \lambda_i  \lambda_j 
%\end{align}
%and therefore
%\begin{align}  \label{eq:38}
%C21=\big( \frac{P}{\sigma^2}\big)^2 \Big(\big(Trace(\mathbf{W})\big)^2- Trace(\mathbf{W^2}) \Big)
%\end{align}
%To find $C22$, considering that $\sum_{i=1}^{n_T} \big(\lambda_i \big)=Trace(\mathbf{W})=1$, we have
%\begin{align}  \label{eq:39}
%Trace(\mathbf{W^2})=\Big( \sum_{i=1}^{n_T} \lambda_i^2 \Big) \Big(  \sum_{i=1}^{n_T} \lambda_i \Big) \nonumber \\ 
%= \sum_{i=1}^{n_T} \lambda_i^3 +\sum_{i\neq j}^{n_T} \lambda_i^2  \lambda_j  \nonumber \\ 
%=Trace(\mathbf{W^3})+\sum_{i\neq j}^{n_T} \lambda_i^2  \lambda_j
%\end{align}
%and thus
%\begin{align}  \label{eq:40}
%C22=\big( \frac{P}{\sigma^2}\big)^3 \Big(\big(Trace(\mathbf{W})\big)^2- Trace(\mathbf{W^3}) \Big)
%\end{align}
%Now we can find $C^2$ as follows:
%\begin{align}  \label{eq:41}
%C^2 \approx \big( \frac{1}{\ln(2)}\big)^2 \Big( \big( Trace(\mathbf{W})\big)^2-Trace(\mathbf{W^2})\Big) \nonumber \\
%=\big( \frac{1}{\ln(2)}\big)^2 \Big(1-Trace(\mathbf{W^2})\Big)~~~~~~~~
%\end{align}
%Hence, 
%\begin{align}  \label{eq:42}
%\mathbf{E} \{C^2 \} \approx \big( \frac{1}{\ln(2)}\big)^2  \Big(1-\mathbf{E} \big\{Trace(\mathbf{W^2})\big\}\Big)
%\end{align}
%and $\mathbf{E} \big\{Trace(\mathbf{W^2})\big\}$ is found by equation \eqref{eq:30}
%\subsection{Calculating $P_{out}$ and demonstrating its accuracy} \label{simout}
%Now, considering equation \eqref{eq:33} the Gaussian approximated outage capacity $P_{out}$ can be found as follows
%\begin{align}  \label{eq:43}
%P_{out}=1-Q \Big( \frac{R_{th}-\mathbf{E}\{C\}}{\sqrt{\mathbf{Var}  \{C \}}} \Big) \nonumber \\
%=Q \Big( \frac{\mathbf{E}\{C\}-R_{th}}{\sqrt{\mathbf{Var}  \{C \}}} \Big)
%\end{align}
%where $Q$ is the tail distribution function of the standard normal distribution and is defined as
%\begin{align}  \label{eq:44}
%Q(x)=\frac{1}{\sqrt{2\pi}}\int_{x}^{\infty} e^{\frac{-t^2}{2}}dt 
%\end{align}
%Here, we show the simulation result for outage capacity and we compare it to the one found by \eqref{eq:43}.
%\begin{figure}[htbp] \label{fig:outage}
%\centering{\includegraphics[scale=0.4]{Outage}}
%    \caption{Outage capacity for different number of satellites and receive antenna elements, SNR=10dB. Solid lines and dashed lines represent the outage capacity found by equation \eqref{eq:43} and simulation, respectively.}
%    \label{fig:outage}
%\end{figure}
%Figure \ref{fig:outage} justifies the accuracy of our conjecture.
%
%\begin{figure}[h!] \label{fig:outage2}
%\includegraphics[width=.65\textwidth,center]{outage2}
%    \caption{Outage capacity of a 64 $\times$ 64 MIMO setup for different SNR's. Solid lines and dashed lines represent the outage capacity found by equation \eqref{eq:43} and simulation, respectively.}
%    \label{fig:outage2}
%\end{figure}

%
%\section{Satellite arrangement to achieve maximum channel capacity} \label{sec:max}
%In this section, we aim to find the optimum satellites' position which maximizes the channel capacity. For this, we introduce a new variable $\nu_i=\sin(\theta_i)\sin(\phi_i)$. Also, we assume that $d\geq \frac{\lambda}{2}$, and therefore $kd\geq \pi$
%\\
%From Arithmetic Mean-Geometric Mean inequality we obtain
%\begin{align}  \label{eq:45}
%C= \sum_{i=1}^{n_T}  \log_2 \big( 1+\frac{P}{\sigma^2} \lambda_i \big)= \log_2 \Big( \prod_{i=1}^{n_T}\big( 1+\frac{P}{\sigma^2} \lambda_i \big)  \Big) \nonumber \\
%\leq  \log_2 \Big(\sum_{i=1}^{n_T} \big( 1+\frac{P}{\sigma^2} \lambda_i \big) \Big) ~~~ ~~~
%\end{align}
%On the other hand, because $\sum_{i=1}^{n_T} \lambda_i= Trace(\mathbf{W})=1$, inequality  \eqref{eq:45} becomes 
%\begin{align}  \label{eq:46}
%C \leq  \log_2 \Big( n_T+\frac{P}{\sigma^2} \Big) ~~~ ~~~
%\end{align}
%and equality happens when all $\lambda_i$'s are equal. Also, we know that all diagonal elements of matrix $\mathbf{W}$ are equal, meaning that if the off-diagonal elements become zero, we reach the maximum capacity. 
%
%We divide the problem into three cases:
%\begin{itemize}
%\item{$n_T=n_R$}
%In this case $\mathbf{W}=H^\dag H$. If $H^\dag $, or equivalently $H$, is unitary, then $\mathbf{W}$ would be a diagonal with them same eigenvalues. If $(kd \nu_i)$'s are evenly distributed on the unit complex circle, H becomes unitary; because in \eqref{eq:17} off diagonal elements become zero. Hence:
%\begin{align}  \label{e:47}
%kd\nu_i=\frac{2\pi \times i}{n_T}+ \mu_0 ~~ i=1,2,...,n_T
%\end{align}
%where $ -\frac{2\pi}{n_T}-\pi \leq \mu_0 \leq -\pi$ and it is such that $-\pi \leq kd\nu_i \leq \pi$.
%\begin{remark}
%Because $kd\geq \pi$,  $ -1\leq \nu_i=\sin(\theta_i)\sin(\phi_i) \leq 1$, which guarantee that $\theta_i$ and $\phi_i$ have solutions. 
%\end{remark}
%\item{$n_T<n_R$}
%Again $\mathbf{W}=H^\dag H$. In this case if $H^\dag $, or equivalently $H$, is semi-unitary then $\mathbf{W}$ is diagonal. To satisfy this, $n_T$ vectors of length $n_R$ must be mutually orthogonal over Hermitian inner product space. 
%\\
%First we define the following set with $n_R$ elements
%\begin{align}  \label{eq:48}
%kd \tilde{\nu_i}=\frac{2\pi \times i}{n_R}+ \mu_0 ~~ i=1,2,...,n_R
%\end{align}
%where $ -\frac{2\pi}{n_R}-\pi \leq \mu_0 \leq -\pi$ and is constant. Each subset of the above set with cardinality of $n_T$ makes the matrix H' semi unitary.  
%\item{$n_T>n_R$}
%\\
%In this case,  $\mathbf{W}=H H^\dag $, and hence the same discussion as case 2 could be held.
%\end{itemize}
%In the following figure, one realization of the satellite arrangement to achieve maximum capacity when   $n_T=n_R=4$ is depicted.
%\\
%To satisfy the above conditions, for instance, we begin with $\sin(\theta_i)=0.9$ for all users which means $\theta_i=0.35\pi$ for $i=1,2,3$ and $4$. This yields $\sin(\phi_1)=\frac{-5}{6}$, $\sin(\phi_2)=\frac{-5}{18}$,  $\sin(\phi_3)=\frac{5}{18}$ and  $\sin(\phi_4)=\frac{5}{6}$, or $(\phi_1,\phi_2,\phi_3,\phi_4)=(-0.69\pi,-0.08 \pi, 0.69\pi, 0.08 \pi)$
%
%
%\begin{remark}
%We found values for $\nu_i$'s which maximize the channel capacity in the above cases. When linear antenna is along x-axis $\nu_i=\cos(\theta_i)\sin(\phi_i)$ and when it is along the z-axis it is $\nu_i=\cos(\theta_i)$. Thus, the values found for $\nu_i$'s could be used correspondingly to maximize the channel capacity based on the antenna alignment. 
%\end{remark}
%
%\begin{remark}
%$\nu_i$'s are not unique, because $\mu_0$ is an interval. Once the value for $\mu_0$ is chosen, it would be fixed for all $\nu_i$'s.
%\end{remark}
%
%\begin{remark}
%Even for a constant $\mu_0$, for the antenna align X or Y axis, satellite configuration is not unique. ;However, the configuration would be unique for an antenna along Z axis. 
%\end{remark}
%
%In Figure \ref{fig:max}, the maximum capacity is compared with average capacity for 2 cases.
%\begin{figure}[htbp]
%\centering{\includegraphics[scale=0.4]{Max_capacity}}
%    \caption{Maximum Vs. Average channel capacity.}
%    \label{fig:max}
%\end{figure}
%
%\section{arrays with other configurations} \label{sec:config}
%This section considers two other possibilities for the antenna configuration which are mentioned in the following parts
%\subsection{Linear array not along y axis}
%One can derive the results we obtained so far for any other alignment of linear array antenna. This could be done by finding by rotating the array to make it along y-axis and then finding the corresponding $\phi$ and $\theta$ values. 
%\\
%For an array placed on z axis, the array factor in \eqref{eq:4} becomes
%\begin{align} \label{eq:49}
%AF(\theta, \phi)=\sum_{m=0}^{M-1} I_m e^{jkmd\cos(\theta)}
%\end{align}
%Only $\theta$ appears in matrix $H$ which makes the calculation simpler, and all the results can be obtained similarly for this array. In appendix D we have found, for instance, $\mathbf{E} \{C^2 \}$ for this alignment.
%\subsection{Rectangular array}
%The elements are arranged uniformly along a rectangular grid in the xy plane to form an $m \times n$ array. This array could be regarded as either $m$ or $n$ linear array with $n$ and $m$ antenna elements, respectively. Henceforth, each sub-array can be analyzed independently, and the result would be added up. 
%
%\section{minimizing channel interference} \label{Sec:Kissing}
%In this section, we assume that we want to put some satellites around the Earth serving terrestrial users such that the amount of interference is minimized. To do so, firstly, we put satellites so that the minimum distance between them (considering all possible pairs) is maximized. This question intuitively reminds us of the “Tammes problem” which has been extensively studied before. \\
%Tammes \cite{tarnai1991covering} problem looks for an answer for the following question: “How must N congruent non-overlapping spherical caps be packed on the surface of a unit sphere so that the angular diameter of spherical caps will be as great as possible”\\
%One can easily correspond our problem to the answer of Tammes problem. We
%can say all satellites are located on a unique sphere when revolving around the
%Earth. It is worth mentioning that when a satellite is relatively close
%to Earth, the orbit on which the satellite traverses is roughly a circle, and therefore our assumption is valid.\\
%Tammes problem was solved for some specific number of points(for N=1,2,…,12,23,24 and some other values \cite{schutte1951kugel} \cite{van1952punkte} \cite{robinson1961arrangement}).\\
%
%Let X be a finite subset of $S^{n-1}$ in $\mathbb{R}^{n}$. We define $\psi$ as follows:
%\begin{align}
%\psi(x)=\min_{x,y \in X} {dist(x,y)},   x\neq y
%\end{align}
%Then X is a spherical $\psi(X)$-code. Also, define $d_{N}$ the largest angular separation $\psi(X)$ with $|X|=N$ that could be obtained in $S^2$, meaning that:
%\begin{align}
%d_{N}=\max_{X \subset S^2} {\psi (X)}, |X|=N.
%\end{align}
%
%The remaining of this section considers two cases, one when there is no interference and not all parts of Earth is served by satellites, one when interference exists and all Earth surface is served. Afterward, channel capacity is calculated for the setup with minimum interference and it is compared with that of obtained in \ref{sec:max}.
% 
%\subsection{No interference}
%In this part, we assume that each non-overlapping spherical cap found for each N is the terrestrial coverage area for the corresponding satellite. In this case, each server on Earth is served by a single satellite, and therefore there is no interference. Having this setup, there exist some place on Earth not being served by any satellites. For this matter, we define coverage percentage for each configuration as ratio of the area covered by satellites to surface area of Earth. 
%\\
%The following table could be accordingly attained.  
%\\
%As seen, the coverage percentage is non-linear as N grows, however, it reaches it maximum value for N=12 among the values considered above. 
%\begin{table}[t]
%  \centering
%  \begin{tabular}{|l|c|c|c|c|c|}
%  \hline
%  Number of satellites & $d_{N}$ & Coverage percentage\\
%  \hline
%4   &  109.4712206 &  0.8386\\
%5  &  90.0000000&  0.7322\\
%6  &  90.0000000 &  0.8787\\
%7  & 77.8695421 & 0.7775\\
%8  & 74.8584922 & 0.8234\\
%9  & 70.5287794 & 0.8258\\
%10  & 66.1468220 & 0.8101 \\
%11  & 63.4349488 & 0.8214\\
%12  & 63.4349488 & 0.8961 \\
%13  & 57.1367031 & 0.7914\\
%14  & 55.6705700 & 0.8099\\
%15 & 53.6578501 & 0.8073\\
%16 & 52.2443957 & 0.8171\\
%17 & 51.0903285 & 0.8309\\
% \hline
% \end{tabular}
% \caption{No interference setup. $d_{N}$ and coverage percentage is shown for different number of satellites around the Earth}
% \label{tab1}
% \end{table}
%\subsection{Interference and Overlapping Coverage Area} \label{sub:inter}
% to cover the whole surface of Earth with existing satellites, the coverage area for each satellite needs to be enlarged. For this purpose, each spherical cap is equally enlarged till all points on the surface of the Earth would be covered by at least on satellite.
%\\ 
%In this case, there would be some terrestrial servers receiving signals from a couple of satellites. From \cite{mooers1994tammes}, for $N>6$, if a server receives signal from more than 1 satellites, the number of satellites seen by the server is at most 5 and at least 3. 
%\\
%In \cite{tarnai1991covering}, the author tries to find conjectured solutions for this problem. Also, it defined density denoted by $D_{N}$ which has the same meaning as Coverage Percentage defined in this paper. 
%
%\begin{table}[t]
%  \centering
%  \begin{tabular}{|l|c|c|c|c|c|}
%  \hline
%  Number of satellites & $d_{N}$ & Coverage percentage\\
%  \hline
% 4   & 70.5287 & 1.3333\\
%5  &  63.4349 & 1.3819\\
%6  &  54.7356 & 1.2679\\
%7  & 51.0265 & 1.2986\\
%8  & 48.1395 & 1.3307\\
%9  & 45.8788 & 1.3672\\
%10  & 42.3078 & 1.3023\\
%11  & 41.4271 & 1.3761\\
%12  & 37.3773 & 1.2320\\
%13  & 37.0685 & 1.3135\\
%14  & 34.9379 & 1.2615\\
%15 & 34.0399 & 1.2851\\ 
%16 & 32.8988 & 1.2829\\
%17 & 32.0929 & 1.2989\\
% \hline
% \end{tabular}
% \caption{Serving all parts of the Earth with $N$ satellites.  Coverage percentage $>1$. $d_{N}$ and coverage percentage are shown for different number of satellites around the Earth.}
% \label{tab2}
% \end{table}
%
%\subsection{Channel capacity}
%To compare the channel capacity for the satellite arrangement in\ref{sub:inter} to that of found in \ref{sec:5}, we only considers the satellites located in one hemisphere (The one that the receiver is located). We used the method elaborated in \cite{tarnai1991covering} to find the satellites location in the desired hemisphere. 
%\begin{figure}[htbp]
%\centering{\includegraphics[scale=0.4]{min_interefence}}
%    \caption{Maximum channel capacity Vs. channel capacity for the setup with minimum interference.}
%    \label{CDD}
%\end{figure}
%
%\section{Rotman lens} \label{sec:Rotman}
%This lens is capable of beam-forming without the need for switches or phase shifters. Its typical geometry and design parameters are shown in Figure \ref{Rotman}.
%
%\begin{figure}[htbp]
%\centering{\includegraphics[scale=0.3]{Rotman}}
%    \caption{Microwave Rotman Lens geometry and design parameters \cite{lee2009beamforming}.}
%    \label{Rotman}
%\end{figure}
%
%Since the invention of Rotman lens, the lens equation has been extensively studied \cite{kim2001scaling} , \cite{katagi1984improved}. Specifically, for a KU-band Rotman lens, the normalized transfer matrix model obtained in \cite{rahimian2013design} as follows
%\\
%\\
%\begin{equation}
%\begin{aligned}  \label{eq:S}
%\mathbf{S}=\frac{e^{jkW_{0}}}{\sqrt{n_Tn_R}}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%\\
%\times
%\begin{pmatrix*}[c]
%e^{-jk\eta_{1}\sin(\theta_1)} &. . .&  e^{-jk\eta_{1}\sin(\theta_{n_T})}\\
%e^{-jk\eta_{2}\sin(\theta_1)} & . . .&  e^{-jk\eta_{2}\sin(\theta_{n_T})}\\
%.& ...& .\\
%.& ...& .\\
%.& ...& .\\
%e^{-jk\eta_{n_R}\sin(\theta_1)} & . . .& e^{-jk \eta_{n_R}\sin(\theta_{n_T})}\\
%\end{pmatrix*}~~~~~~~~~~
%\\
%\times
%\begin{pmatrix*}[c]
%e^{-jk(F_1+V_1)} &. . .& 0 & 0\\
%0 & . . .&  e^{-jk(F_m+V_m)}& 0\\
%.& ...& .\\
%.& ...& .\\
%.& ...& .\\
%0 & . . .& 0 & e^{-jk(F_{n_T}+V_{n_T})} \\
%\end{pmatrix*}
%\end{aligned}  
%\end{equation}
%\\
%\\
%where $\eta_{n}=[n-\frac{(N+1)}{2}]\times d$ and $e^{-jk(F_m+V_m)}$ is an extra phase shift added to the $m^{th}$ RF beam port; and $(n=1,2,...,n_T), (m=1,2,...,n_R)$. 
%The transfer matrix has two parts: the common phase-delay shared by the array ports and the progressed phase through array port in which the latter contributes to RF beam forming.
%phase alignment requires the phase-delay path (i.e., $F_{m}+V_{m}$) to be constant so that the Rotman lens retain the true-time-delay (TTD) characteristic of the device. This method is essentially required in the case of simultaneous multiple RF beam port excitation to obtain reconfigurable far-field microwave radiation properties for the satellite RF lens \cite{zhang2012reconfigurable}, Meaning that
%\begin{align}  
% e^{-jk(F_m+V_m)}=constant
%\end{align}
%and therefore, the second matrix in \eqref{eq:S} becomes identity matrix. 
%\\
%If the beam angles $\theta_m$'s received from $n_T$ satellites are uniformly distributed over $(0, \frac{\pi}{2})$ and there are $n_R$ antenna elements over the array plan, the average channel capacity  of this setup could be obtained similar to that of obtained in Appendix \ref{Z axis}, i.e. a linear array along Z axis.
%\section{conclusion} \label{sec:conclusion}
%This paper analyzes the channel capacity of a MIMO system in which the receiver is equipped with a linear array antenna and the transmitters are some satellites whose locations are not known at the receiver. To address the characteristic of this MIMO channel, the average channel capacity is found and also the outage probability is presented. In addition, the optimum position of satellites are found such that the channel capacity is maximized. In contrast, the optimum position of satellites to cause minimum interference at the receiver is presented and the channel capacity in this setup is compared with the maximum possible channel capacity. As an application of this paper, the average capacity for a Rotman lens performing for KU-band satellites is also analyzed. 
%
%
%
%
%
%
%
%
%\appendices
%\section{Array factor derivation}\label{app:AF}
%The more general form the array factor expression is found using the following expression:
%\begin{align} \label{eq:AF}
%AF(\theta, \phi)=\sum_{m=0}^{M-1} I_m e^{jk\hat{r}.\overrightarrow{r}_m}
%\end{align}
%where $\overrightarrow{r}_m$ is the position vector to the $m^{th}$ element and $\hat{r}$ is a unit vector pointing in the direction of interest, i.e.,
%\begin{align}\label{eq:rhat}
%\hat{r}=\sin\theta \cos\phi \hat{x}+\sin\theta \sin\phi \hat{y}+\cos\theta \hat{z}
%\end{align}
%and for a linear configuration evenly distributed along y axis with spacing d, $\overrightarrow{r}_m=md \hat{y}$. Therefore $\hat{r}.\overrightarrow{r}_m=\sin\theta \sin\phi$ and the equation \ref{eq:4} is derived.
%
%
%
%
%\section{Proof of theorem \label{th:lambda}}
%Characteristic polynomial of an $n\times n$matrix $A$ could be written in terms of traces of matrices $A, A^2, ..., A^n$ as follows \cite{pennisi1987coefficients}
%\begin{align}  \label{eq:52}
%P(\lambda)=b_0\lambda^n+b_1\lambda^{n-1}+b_2\lambda^{n-2}+ ...+b_{n-1}\lambda+b_n
%\end{align}
%where 
%\begin{align} 
%b_0=(-1)^n, ~~~~~~ b1=-(-1)^n \mathbf{T}_1^A ~~~~~~~~~~~~  \nonumber \\
%b_2=-\frac{1}{2} \big( b_1 \mathbf{T}_1^A + (-1)^n \mathbf{T}_2^A \big)~~~~~~~~~~~~ \nonumber \\
%. ~~~~~~~~~~~~~~~~~~~~~~~~ \nonumber \\
%. ~~~~~~~~~~~~~~~~~~~~~~~~ \nonumber \\
%. ~~~~~~~~~~~~~~~~~~~~~~~~ \nonumber \\
%b_n=-\frac{1}{n}  \big( b_{n-1} \mathbf{T}_1^A +b_{n-2} \mathbf{T}_2^A +...+b_{1} \mathbf{T}_{n-1}+ (-1)^n \mathbf{T}_n^A \big)
%\end{align}
%and $\mathbf{T}_1^A, \mathbf{T}_2^A, ..., \mathbf{T}_n^A$ denote the traces of matrices $A, A^2, ..., A^n$.
%Also, the summation of roots of equation \eqref{eq:52} is equal to $\frac{-b_1}{b_0}=\mathbf{T}_1^A$.
%\\
%On the other hand, eigenvalues of matrix $A^k$ are  eigenvalues of matrix $A$ each to the power of $k$.
%Therefore if $\lambda_1, \lambda_2,...,\lambda_n$ are eigenvalues of matrix $A$, then $\lambda_1^k, \lambda_2^k,...,\lambda_n^k$ would be those of matrix $A^k$, and thus
%\begin{align}  \label{eq:52}
%\sum_{i=1}^n \lambda_i^k=\mathbf{T}_1^{A^k}
%\end{align}
%And the theorem is proved.
%
%
%
%\section{Average channel capacity for a linear array along z axis}\label{Z axis}
%The array factor for a linear array along z axis is mentioned in \eqref{eq:49}, where $0 \leq \theta \leq \frac{\pi}{2}$. It stands to reason that again $Trace(\mathbf{W})=1$. Using the formula we obtained in \eqref{eq:23}, one can justify that
%\begin{align} 
%\mathbf{E} \Big\{Trace(\mathbf{W}^2) \Big\}=\frac{1}{(n_Rn_T)^2}\Big(n_R^2n_T+n_Rn_T(n_T-1)+\nonumber ~~~~~~~~~~~~~\\
%2\sum_{s=1}^{n_R-1}(n_R-s)n_T(n_T-1) J_0  \big( \frac{skd}{2} \big)^2\Big)~~~~~~~~~~~~~
%\end{align}
%Also to obtain the third moment from equation \eqref{eq:31}, we have
%\begin{flalign}
%\mathbf{E} \Big\{Trace(\mathbf{W}^2) \Big\}= \frac{1}{(n_Rn_T)^3} \Bigg(n_R^3nT+3n_R^2n_T(n_T-1)~~~~~~ \nonumber \\
%+n_Rn_T(n_T-1)(n_T-2)~~~~~~ ~~~~~~ \nonumber \\
%+6\sum_{s=1}^{n_R-1}n_R(n_R-s)n_T(n_T-1) J_{0} \big( \frac{skd}{2} \big)^4~~~~~~ ~~~~~~ \nonumber \\+6\sum_{s=1}^{n_R-1}(n_T-2)\prod_{k=0}^2(k+s)J_{0} \big( \frac{(n_R-s)kd}{2} \big)^2~~~~~ ~~~\nonumber \\
%+6\sum_{s=1}^{\frac{n_R-1}{2}}(n_T-2)\prod_{k=0}^2(n_R-2s+k)J_{0} \big( \frac{skd}{2} \big)^4 J_{0} \big( {skd} \big)~~~~~ ~~~\nonumber \\
%+\Big(6\sum_{s=1}^{n_R-1}\sum_{t=1}^{n_R-2s} (n_R-2m)^{+}(n_T-2) \times~~~~~ ~~~ \nonumber \\
%\prod_{k=0}^2 (2n_R-2t-2s+2+k) \times ~~~~~ ~~~\nonumber \\
%J_{0} \big( \frac{skd}{2} \big) J_{0} \big( \frac{(s+t)kd}{2} \big)J_{0} \big( \frac{(2s+t)kd}{2} \big) \Big)\Bigg)~~~~ ~~~~~
%\end{flalign}
\bibliographystyle{IEEEtran}
\bibliography{refs}
\end{document}


