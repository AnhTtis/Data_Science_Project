@article{aasExplainingIndividualPredictions2021,
  title = {Explaining Individual Predictions When Features Are Dependent: {{More}} Accurate Approximations to {{Shapley}} Values},
  shorttitle = {Explaining Individual Predictions When Features Are Dependent},
  author = {Aas, Kjersti and Jullum, Martin and L{\o}land, Anders},
  year = {2021},
  journal = {Artificial Intelligence},
  volume = {298},
  langid = {english}
}

@misc{baiONNXOpenNeural2019,
  title = {{{ONNX}}: {{Open}} Neural Network Exchange},
  author = {Bai, Junjie and Lu, Fang and Zhang, Ke},
  year = {2019}
}

@misc{bostockJavaScriptDataAnalysis2021,
  title = {{{JavaScript}} for {{Data Analysis}}},
  author = {Bostock, Mike},
  year = {2021},
  url = {https://towardsdatascience.com/javascript-for-data-analysis-2e8e7dbf63a7}
}

@article{chenExplainingSeriesModels2022,
  title = {Explaining a Series of Models by Propagating {{Shapley}} Values},
  author = {Chen, Hugh and Lundberg, Scott M. and Lee, Su-In},
  year = {2022},
  journal = {Nature Communications},
  volume = {13},
  abstract = {Abstract             Local feature attribution methods are increasingly used to explain complex machine learning models. However, current methods are limited because they are extremely expensive to compute or are not capable of explaining a distributed series of models where each model is owned by a separate institution. The latter is particularly important because it often arises in finance where explanations are mandated. Here, we present Generalized DeepSHAP (G-DeepSHAP), a tractable method to propagate local feature attributions through complex series of models based on a connection to the Shapley value. We evaluate G-DeepSHAP across biological, health, and financial datasets to show that it provides equally salient explanations an order of magnitude faster than existing model-agnostic attribution techniques and demonstrate its use in an important distributed series of models setting.},
  langid = {english}
}

@inproceedings{chenXGBoostScalableTree2016,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  langid = {english}
}

@inproceedings{chenXGBoostScalableTree2016a,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {{{KDD}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  langid = {english}
}

@inproceedings{covertUnderstandingGlobalFeature2020,
  title = {Understanding Global Feature Contributions with Additive Importance Measures},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Covert, Ian and Lundberg, Scott M and Lee, Su-In},
  year = {2020},
  volume = {33}
}

@article{dahlNodeJsOpensource2009,
  title = {Node.Js: An Open-Source, Cross-Platform {{JavaScript}} Runtime Environment},
  author = {Dahl, Ryan},
  year = {2009},
  url = {https://nodejs.org/en/}
}

@misc{fletcherEthicalPrinciplesWeb2022,
  title = {Ethical {{Principles}} for {{Web Machine Learning}}},
  author = {Fletcher, James and Kostiainen, Anssi},
  year = {2022},
  url = {https://www.w3.org/TR/webmachinelearning-ethics/}
}

@article{hanWhichExplanationShould2022,
  title = {Which {{Explanation Should I Choose}}? {{A Function Approximation Perspective}} to {{Characterizing Post}} Hoc {{Explanations}}},
  shorttitle = {Which {{Explanation Should I Choose}}?},
  author = {Han, Tessa and Srinivas, Suraj and Lakkaraju, Himabindu},
  year = {2022},
  abstract = {Despite the plethora of post hoc model explanation methods, the basic properties and behavior of these methods and the conditions under which each one is effective are not well understood. In this work, we bridge these gaps and address a fundamental question: Which explanation method should one use in a given situation? To this end, we adopt a function approximation perspective and formalize the local function approximation (LFA) framework. We show that popular explanation methods are instances of this framework, performing function approximations of the underlying model in different neighborhoods using different loss functions. We introduce a no free lunch theorem for explanation methods which demonstrates that no single method can perform optimally across all neighbourhoods and calls for choosing among methods. To choose among methods, we set forth a guiding principle based on the function approximation perspective, considering a method to be effective if it recovers the underlying model when the model is a member of the explanation function class. Then, we analyze the conditions under which popular explanation methods are effective and provide recommendations for choosing among explanation methods and creating new ones. Lastly, we empirically validate our theoretical results using various real world datasets, model classes, and prediction tasks. By providing a principled mathematical framework which unifies diverse explanation methods, our work characterizes the behaviour of these methods and their relation to one another, guides the choice of explanation methods, and paves the way for the creation of new ones.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  journal = {arXiv:2206.01254}
}

@article{hongHumanFactorsModel2020,
  title = {Human {{Factors}} in {{Model Interpretability}}: {{Industry Practices}}, {{Challenges}}, and {{Needs}}},
  shorttitle = {Human {{Factors}} in {{Model Interpretability}}},
  author = {Hong, Sungsoo Ray and Hullman, Jessica and Bertini, Enrico},
  year = {2020},
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {4},
  abstract = {As the use of machine learning (ML) models in product development and data-driven decision-making processes became pervasive in many domains, people's focus on building a well-performing model has increasingly shifted to understanding how their model works. While scholarly interest in model interpretability has grown rapidly in research communities like HCI, ML, and beyond, little is known about how practitioners perceive and aim to provide interpretability in the context of their existing workflows. This lack of understanding of interpretability as practiced may prevent interpretability research from addressing important needs, or lead to unrealistic solutions. To bridge this gap, we conducted 22 semi-structured interviews with industry practitioners to understand how they conceive of and design for interpretability while they plan, build, and use their models. Based on a qualitative analysis of our results, we differentiate interpretability roles, processes, goals and strategies as they exist within organizations making heavy use of ML models. The characterization of interpretability work that emerges from our analysis suggests that model interpretability frequently involves cooperation and mental model comparison between people in different roles, often aimed at building trust not only between people and models but also between people within the organization. We present implications for design that discuss gaps between the interpretability challenges that practitioners face in their practice and approaches proposed in the literature, highlighting possible research directions that can better address real-world needs.},
  langid = {english},
  keywords = {model-update}
}

@inproceedings{hortonLayerWiseDataFreeCNN2022,
  title = {Layer-{{Wise Data-Free CNN Compression}}},
  booktitle = {{{ICPR}}},
  author = {Horton, Maxwell and Jin, Yanzi and Farhadi, Ali and Rastegari, Mohammad},
  year = {2022}
}

@inproceedings{jamaliHeteroMFRecommendationHeterogeneous2013,
  title = {{{HeteroMF}}: Recommendation in Heterogeneous Information Networks Using Context Dependent Factor Models},
  shorttitle = {{{HeteroMF}}},
  booktitle = {Proceedings of the 22nd International Conference on {{World Wide Web}}},
  author = {Jamali, Mohsen and Lakshmanan, Laks},
  year = {2013},
  langid = {english}
}

@article{kahngGANLabUnderstanding2019,
  title = {{{GAN Lab}}: {{Understanding Complex Deep Generative Models}} Using {{Interactive Visual Experimentation}}},
  shorttitle = {{{GAN Lab}}},
  author = {Kahng, Minsuk and Thorat, Nikhil and Chau, Duen Horng and Viegas, Fernanda B. and Wattenberg, Martin},
  year = {2019},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {25},
  keywords = {AID-education-tool}
}

@article{kahngGANLabUnderstanding2019a,
  title = {{{GAN Lab}}: {{Understanding Complex Deep Generative Models}} Using {{Interactive Visual Experimentation}}},
  shorttitle = {{{GAN Lab}}},
  author = {Kahng, Minsuk and Thorat, Nikhil and Chau, Duen Horng and Viegas, Fernanda B. and Wattenberg, Martin},
  year = {2019},
  journal = {IEEE TVCG},
  volume = {25},
  keywords = {AID-education-tool}
}

@article{krishnaDisagreementProblemExplainable2022,
  title = {The {{Disagreement Problem}} in {{Explainable Machine Learning}}: {{A Practitioner}}'s {{Perspective}}},
  shorttitle = {The {{Disagreement Problem}} in {{Explainable Machine Learning}}},
  author = {Krishna, Satyapriya and Han, Tessa and Gu, Alex and Pombra, Javin and Jabbari, Shahin and Wu, Steven and Lakkaraju, Himabindu},
  year = {2022},
  abstract = {As various post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to develop a deeper understanding of if and when the explanations output by these methods disagree with each other, and how such disagreements are resolved in practice. However, there is little to no research that provides answers to these critical questions. In this work, we introduce and study the disagreement problem in explainable machine learning. More specifically, we formalize the notion of disagreement between explanations, analyze how often such disagreements occur in practice, and how do practitioners resolve these disagreements. To this end, we first conduct interviews with data scientists to understand what constitutes disagreement between explanations generated by different methods for the same model prediction, and introduce a novel quantitative framework to formalize this understanding. We then leverage this framework to carry out a rigorous empirical analysis with four real-world datasets, six state-of-the-art post hoc explanation methods, and eight different predictive models, to measure the extent of disagreement between the explanations generated by various popular explanation methods. In addition, we carry out an online user study with data scientists to understand how they resolve the aforementioned disagreements. Our results indicate that state-of-the-art explanation methods often disagree in terms of the explanations they output. Our findings also underscore the importance of developing principled evaluation metrics that enable practitioners to effectively compare explanations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  journal = {arXiv:2202.01602}
}

@misc{LendingClubOnline2018,
  title = {Lending {{Club}}: {{Online Personal Loans}} at {{Great Rates}}},
  year = {2018},
  url = {https://www.lendingclub.com/}
}

@inproceedings{lundbergUnifiedApproachInterpreting2017,
  title = {A Unified Approach to Interpreting Model Predictions},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  author = {Lundberg, Scott M. and Lee, Su-In},
  year = {2017},
  series = {{{NIPS}}'17},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  keywords = {_tablet_modified,AID-interpretability}
}

@inproceedings{lundbergUnifiedApproachInterpreting2017a,
  title = {A Unified Approach to Interpreting Model Predictions},
  booktitle = {{{NeurIPS}}},
  author = {Lundberg, Scott M. and Lee, Su-In},
  year = {2017},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  keywords = {_tablet_modified,AID-interpretability}
}

@article{netoExplainableMatrixVisualization2021,
  title = {Explainable {{Matrix}} - {{Visualization}} for {{Global}} and {{Local Interpretability}} of {{Random Forest Classification Ensembles}}},
  author = {Neto, Mario Popolin and Paulovich, Fernando V.},
  year = {2021},
  journal = {IEEE TVCG},
  volume = {27},
  abstract = {Over the past decades, classification models have proven to be essential machine learning tools given their potential and applicability in various domains. In these years, the north of the majority of the researchers had been to improve quantitative metrics, notwithstanding the lack of information about models' decisions such metrics convey. This paradigm has recently shifted, and strategies beyond tables and numbers to assist in interpreting models' decisions are increasing in importance. Part of this trend, visualization techniques have been extensively used to support classification models' interpretability, with a significant focus on rule-based models. Despite the advances, the existing approaches present limitations in terms of visual scalability, and the visualization of large and complex models, such as the ones produced by the Random Forest (RF) technique, remains a challenge. In this paper, we propose Explainable Matrix (ExMatrix), a novel visualization method for RF interpretability that can handle models with massive quantities of rules. It employs a simple yet powerful matrix-like visual metaphor, where rows are rules, columns are features, and cells are rules predicates, enabling the analysis of entire models and auditing classification results. ExMatrix applicability is confirmed via different examples, showing how it can be used in practice to promote RF models interpretability.},
  langid = {english}
}

@inproceedings{panditNetprobeFastScalable2007,
  title = {Netprobe: A Fast and Scalable System for Fraud Detection in Online Auction Networks},
  shorttitle = {Netprobe},
  booktitle = {Proceedings of the 16th International Conference on {{World Wide Web}}},
  author = {Pandit, Shashank and Chau, Duen Horng and Wang, Samuel and Faloutsos, Christos},
  year = {2007},
  langid = {english}
}

@inproceedings{qianGeneratingAccurateCaption2021,
  title = {Generating {{Accurate Caption Units}} for {{Figure Captioning}}},
  booktitle = {Proceedings of the {{Web Conference}} 2021},
  author = {Qian, Xin and Koh, Eunyee and Du, Fan and Kim, Sungchul and Chan, Joel and Rossi, Ryan A. and Malik, Sana and Lee, Tak Yeon},
  year = {2021},
  langid = {english}
}

@article{ribeiroAnchorsHighPrecision2018,
  title = {Anchors: {{High Precision Model-Agnostic Explanations}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2018},
  abstract = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, ``sufficient'' conditions for predictions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
  langid = {english}
}

@inproceedings{ribeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  langid = {english},
  keywords = {AID-interpretability}
}

@inproceedings{ribeiroWhyShouldTrust2016a,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  booktitle = {{{KDD}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  langid = {english},
  keywords = {AID-interpretability}
}

@article{rudinStopExplainingBlack2019,
  title = {Stop {{Explaining Black Box Machine Learning Models}} for {{High Stakes Decisions}} and {{Use Interpretable Models Instead}}},
  author = {Rudin, Cynthia},
  year = {2019},
  journal = {Nature Machine Intelligence},
  volume = {1},
  langid = {english},
  keywords = {AID-interpretability}
}

@misc{schubertOpenAIMicroscope2020,
  title = {{{OpenAI Microscope}}},
  author = {Schubert, Ludwig and Petrov, Michael and Carter, Shan and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
  year = {2020},
  url = {https://microscope.openai.com/}
}

@article{shapleyValueNpersonGames1953,
  title = {A Value for N-Person Games},
  author = {Shapley, Lloyd S},
  year = {1953}
}

@article{smilkovDirectManipulationVisualizationDeep2017,
  title = {Direct-{{Manipulation Visualization}} of {{Deep Networks}}},
  author = {Smilkov, Daniel and Carter, Shan and Sculley, D. and Vi{\'e}gas, Fernanda B. and Wattenberg, Martin},
  year = {2017},
  journal = {arXiv:1708.03788},
  abstract = {The recent successes of deep learning have led to a wave of interest from non-experts. Gaining an understanding of this technology, however, is difficult. While the theory is important, it is also helpful for novices to develop an intuitive feel for the effect of different hyperparameters and structural variations. We describe TensorFlow Playground1, an interactive, open sourced2 visualization that allows users to experiment via direct manipulation rather than coding, enabling them to quickly build an intuition about neural nets.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {AID-education-tool}
}

@article{smilkovTensorFlowJsMachine2019,
  title = {{{TensorFlow}}.Js: {{Machine Learning}} for the {{Web}} and {{Beyond}}},
  shorttitle = {{{TensorFlow}}.Js},
  author = {Smilkov, Daniel and Thorat, Nikhil and Assogba, Yannick and Yuan, Ann and Kreeger, Nick and Yu, Ping and Zhang, Kangyi and Cai, Shanqing and Nielsen, Eric and Soergel, David and Bileschi, Stan and Terry, Michael and Nicholson, Charles and Gupta, Sandeep N. and Sirajuddin, Sarah and Sculley, D. and Monga, Rajat and Corrado, Greg and Vi{\'e}gas, Fernanda B. and Wattenberg, Martin},
  year = {2019},
  journal = {arXiv},
  abstract = {TensorFlow.js is a library for building and executing machine learning algorithms in JavaScript. TensorFlow.js models run in a web browser and in the Node.js environment. The library is part of the TensorFlow ecosystem, providing a set of APIs that are compatible with those in Python, allowing models to be ported between the Python and JavaScript ecosystems. TensorFlow.js has empowered a new set of developers from the extensive JavaScript community to build and deploy machine learning models and enabled new classes of on-device computation. This paper describes the design, API, and implementation of TensorFlow.js, and highlights some of the impactful use cases.},
  archiveprefix = {arxiv}
}

@inproceedings{tenneyLanguageInterpretabilityTool2020,
  title = {The Language Interpretability Tool: {{Extensible}}, Interactive Visualizations and Analysis for {{NLP}} Models},
  booktitle = {{{EMNLP Demo}}},
  author = {Tenney, Ian and Wexler, James and Bastings, Jasmijn and Bolukbasi, Tolga and Coenen, Andy and Gehrmann, Sebastian and Jiang, Ellen and Pushkarna, Mahima and Radebaugh, Carey and Reif, Emily and Yuan, Ann},
  year = {2020},
  abstract = {We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models\textemdash including classification, seq2seq, and structured prediction\textemdash and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.}
}

@misc{unescoRecommendationEthicsArtificial2021,
  title = {Recommendation on the {{Ethics}} of {{Artificial Intelligence}}},
  author = {UNESCO},
  year = {2021},
  url = {https://unesdoc.unesco.org/ark:/48223/pf0000380455}
}

@article{wangCNNExplainerLearning2020,
  title = {{{CNN Explainer}}: {{Learning Convolutional Neural Networks}} with {{Interactive Visualization}}},
  shorttitle = {{{CNN Explainer}}},
  author = {Wang, Zijie J. and Turko, Robert and Shaikh, Omar and Park, Haekyu and Das, Nilaksh and Hohman, Fred and Kahng, Minsuk and Chau, Duen Horng},
  year = {2020},
  journal = {IEEE Transactions on Visualization and Computer Graphics (TVCG)},
  keywords = {AID-education-tool}
}

@article{wangCNNExplainerLearning2020a,
  title = {{{CNN Explainer}}: {{Learning Convolutional Neural Networks}} with {{Interactive Visualization}}},
  shorttitle = {{{CNN Explainer}}},
  author = {Wang, Zijie J. and Turko, Robert and Shaikh, Omar and Park, Haekyu and Das, Nilaksh and Hohman, Fred and Kahng, Minsuk and Chau, Duen Horng},
  year = {2020},
  journal = {TVCG},
  keywords = {AID-education-tool}
}

@inproceedings{wangInterpretabilityThenWhat2022a,
  title = {Interpretability, {{Then What}}? {{Editing Machine Learning Models}} to {{Reflect Human Knowledge}} and {{Values}}},
  booktitle = {{{KDD}}},
  author = {Wang, Zijie J. and Kale, Alex and Nori, Harsha and Stella, Peter and Nunnally, Mark E. and Chau, Duen Horng and Vorvoreanu, Mihaela and Wortman Vaughan, Jennifer and Caruana, Rich},
  year = {2022},
  abstract = {Machine learning (ML) interpretability techniques can reveal undesirable patterns in data that models exploit to make predictions-potentially causing harms once deployed. However, how to take action to address these patterns is not always clear. In a collaboration between ML and human-computer interaction researchers, physicians, and data scientists, we develop GAM Changer, the first interactive system to help domain experts and data scientists easily and responsibly edit Generalized Additive Models (GAMs) and fix problematic patterns. With novel interaction techniques, our tool puts interpretability into action-empowering users to analyze, validate, and align model behaviors with their knowledge and values. Physicians have started to use our tool to investigate and fix pneumonia and sepsis risk prediction models, and an evaluation with 7 data scientists working in diverse domains highlights that our tool is easy to use, meets their model editing needs, and fits into their current workflows. Built with modern web technologies, our tool runs locally in users' web browsers or computational notebooks, lowering the barrier to use. GAM Changer is available at the following public demo link: https://interpret.ml/gam-changer.},
  keywords = {accountability,human agency,interpretability,model editing}
}

@inproceedings{wangTimberTrekExploringCurating2022,
  title = {{{TimberTrek}}: {{Exploring}} and {{Curating Sparse Decision Trees}} with {{Interactive Visualization}}},
  shorttitle = {{{TimberTrek}}},
  booktitle = {2022 {{IEEE Visualization}} and {{Visual Analytics}} ({{VIS}})},
  author = {Wang, Zijie J. and Zhong, Chudi and Xin, Rui and Takagi, Takuya and Chen, Zhi and Chau, Duen Horng and Rudin, Cynthia and Seltzer, Margo},
  year = {2022}
}

@inproceedings{wangTimberTrekExploringCurating2022a,
  title = {{{TimberTrek}}: {{Exploring}} and {{Curating Trustworthy Decision Trees}} with {{Interactive Visualization}}},
  booktitle = {{{VIS}}},
  author = {Wang, Zijie J. and Zhong, Chudi and Xin, Rui and Takagi, Takuya and Chen, Zhi and Chau, Duen Horng and Rudin, Cynthia and Seltzer, Margo},
  year = {2022}
}

@article{wexlerWhatIfToolInteractive2019,
  title = {The {{What-If Tool}}: {{Interactive Probing}} of {{Machine Learning Models}}},
  shorttitle = {The {{What-If Tool}}},
  author = {Wexler, James and Pushkarna, Mahima and Bolukbasi, Tolga and Wattenberg, Martin and Viegas, Fernanda and Wilson, Jimbo},
  year = {2019},
  journal = {IEEE TVCG},
  volume = {26},
  keywords = {interpretability}
}

@unpublished{wortmanvaughanHumancenteredAgendaIntelligible2022,
  title = {A Human-Centered Agenda for Intelligible Machine Learning},
  author = {Wortman Vaughan, Jennifer and Wallach, Hanna},
  year = {2022},
  abstract = {To build machine learning systems that are reliable, trustworthy, and fair, we must be able to provide relevant stakeholders with an understanding of how these systems work. Yet what makes a system ``intelligible'' is difficult to pin down. Intelligibility is a fundamentally human-centered concept that lacks a one-size-fits-all solution. Although many intelligibility techniques have been proposed in the machine learning literature, there are many more open questions about how best to provide stakeholders with the information they need to achieve their desired goals. In this chapter, we begin with an overview of the intelligible machine learning landscape and give several examples of the diverse ways in which needs for intelligibility can arise. We provide an overview of the techniques for achieving intelligibility that have been proposed in the machine learning literature. We discuss the importance of taking a human-centered strategy when designing intelligibility techniques or when verifying that these techniques achieve their intended goals. We also argue that the notion of intelligibility should be expanded beyond machine learning models to other components of machine learning systems, such as datasets and performance metrics. Finally, we emphasize the necessity of tight integration between the machine learning and human\textendash computer interaction communities.}
}
