\section{Method}
% In this section, we first present the cryo-EM image formation model in Sec.~\ref{sec:formulation}.
% %
% Next, we give an overview of \ours in Sec.~\ref{sec:overview}.
% %
% We then introduce the design of each component in Sec.~\ref{sec:encoders}, Sec.~\ref{sec:inr}, and Sec.~\ref{sec:decoder}.

\subsection{Image Formation Model}
\label{sec:formulation}
In the cryo-EM image formation model, the 3D biological structure is represented as a function  $\sigma: \mathbb{R}^{3} \mapsto \mathbb{R}^{+}$, which expresses the Coulomb potential induced by the atoms.
%The target of cryo-EM reconstruction is to determine the mapping $\sigma: \mathbb{R}^{3} \mapsto \mathbb{R}_{\ge 0}$, from the observed 2D projections $\{\mathbf{I}_i\}_{1\le i \le n}$.
To recover the potential function, the probing electron beam interacts with the electrostatic potential and results in projections $\{\mathbf{I}_i\}_{1\le i \le n}$.
%
Specifically, each projection can be expressed as
\begin{equation}
    \mathbf{I}(x, y) = g \star \int_{\mathbb{R}} \sigma(\mathbf{R}^{\top} \mathbf{x}+\mathbf{t})\, \mathrm{d}z + \epsilon, \quad \mathbf{x} = (x, y, z)^{\top}
    \label{eq:formation}
\end{equation}
%
where $\mathbf{R} \in SO(3)$ is an orientation representing the 3D rotation of the molecule and $\mathbf{t} = (t_x, t_y,0)^\top$ is an in-plane translation corresponding to an offset between the center of projected particles and center of the image.
%
The projection is, by convention, assumed to be along the $z$-direction after rotation.
%
The image signal is convolved with $g$, a pre-estimated point spread function (PSF) for the microscope, before being corrupted with the noise $\epsilon$ and registered on a discrete grid of size $D\times D$, where $D$ is the size of the image along one dimension.
%
A more detailed formulation for cryo-EM reconstruction is given in Appendix.

\subsection{Overview of CryoFormer}
\label{sec:overview}

We propose a novel approach, cryoFormer,
that uses an implicit spatial feature volume as well as transformer-based network architecture for
continuous heterogeneous cryo-EM reconstruction. 
Fig.~\ref{fig:method} demonstrates the pipeline of cryoFormer.
%
To extract conformation and pose information of an image, an input projection image $\mathbf{I}$ is fed into image encoders that output an orientation feature $\boldsymbol{F}_{\text{O}}$ and a deformation feature $\boldsymbol{F}_{\text{D}}$.
%
$\boldsymbol{F}_{\text{O}}$ is then converted to a pose $\hat{\phi}=(\hat{\mathbf{R}},\hat{\mathbf{t}})$, though it can be skipped if an accurate estimated pose is pre-computed.
%
$\hat{\mathbf{R}}$ is an rotation matrix to rotate a grid with size $D^3$ in the spatial domain.
%

The rotated coordinates are then fed into our implicit neural spatial feature volume, $\mathcal{V}_\Theta$, parameterized by $\Theta$.
%
$\mathcal{V}_\Theta$ is a mapping from coordinates in $\mathbb{R}^3$ to spatial features in $\mathbb{R}^{N\times C}$ that contains spatial-related structural information as well as learned conformational information.
%
We sample each 3D point in a rotated 3D grid, and implicit neural spatial feature volume outputs a spatial feature $\boldsymbol{F}_{\text{S}}$. 
%
Then we feed $\boldsymbol{F}_{\text{S}}$ into the deformation transformer decoder, where it interacts with structure queries $\boldsymbol{Q}$ and the deformation image feature $\boldsymbol{F}_{\text{D}}$ to output the density prediction $\sigma$.
%
This transformer-based architecture design improves the reconstruction performance and enables the highlighting of 3D flexible regions of results.
%
\subsection{Implicit Spatial Feature Volume}
\label{sec:inr}
To reduce the storage consumption while modeling continuous conformations of 3D structures, neural representation encodes the scene as a continuous high-dimensional manifold and has shown its huge potential in cryo-EM heterogeneous reconstruction tasks~\cite{cryodrgn,zhongreconstructing,cryodrgn2,cryoai,cryofire}.
%
We, for the first time, directly reconstruct continuous conformations of 3D structures using an implicit feature volume, denoted $\mathcal{V}_\Theta(x, y, z; \Theta)$ in the 3D spatial domain to model local changes of conformations.
%
Inspired by Instant NGP~\cite{muller2022instant} for efficient 3D representation training and rendering, we use a similar multi-resolution hash grid encoding augmented by a single-layer MLP to decode a high dimensional spatial feature $\boldsymbol{F}_S \in \mathbb{R}^{N\times C}$, which preserve the high-frequency details with highly reduced computational cost compared with a NeRF-like global coordinate-based MLP in other cryo-EM work~\cite{cryodrgn, cryoai, cryofire} 
%
% For fast and efficient modeling of 3D structures in the spatial domain, we use hash encoding~\cite{muller2022instant} augmented by a small MLP to encode spatial features $\boldsymbol{F}_S \in \mathbb{R}^{N\times C}$.
% 
% Instead of storing the discrete signal values in a grid of pixels or voxels, neural representation encodes data as samples of a continuous manifold and has shown its huge potential in cryo-EM reconstruction tasks~\cite{cryodrgn,zhongreconstructing,cryodrgn2,cryoai,cryofire}.
\begin{equation}
    \mathcal{V}_\Theta(x, y, z; \Theta) = \boldsymbol{F}_S
\end{equation}
%
%We use a neural representation in the spatial domain to map a coordinate $\mathbf{x}=(x,y,z)$ to a high-dimensional spatial feature $\boldsymbol{F}_S \in \mathbb{R}^{N\times C}$.
%
Compared to other methods that reconstruct structures in Fourier domain~\cite{cryodrgn,cryodrgn2,cryoai,cryofire}, reconstruction in the spatial domain~\cite{punjani20213d} is more explainable, easier to model local changes of flexible regions and redeems the artifacts from the sparse sampling rate of high frequency in Fourier domain.
% zy: 频域特别不连续 不存在导数smooth的特点， 可解释性， 频域高频sampling unbalaced 

\subsection{Orientation and Deformation Image Encoders}
\label{sec:encoders}
Since the underlying particle's relative pose and conformational state for each given cryo-EM image may be unknown, we use two image encoders to extract orientation features and deformation features respectively.
%
For both encoders, we adopt MLPs containing 10 hidden layers of width 128 with ReLU activations, following the image encoder design of cryoDRGN~\cite{cryodrgn, zhongreconstructing}.

\noindent \textbf{Orientation Encoder. } For each image, the orientation encoder estimates a biological structure's relative pose, which includes rotation and translation, i.e., $\hat{\phi}=(\hat{\mathbf{R}},\hat{\mathbf{t}})$. 
%
The rotation is first represented in a 6-dimensional space, denoted as $\mathbb{S}^2\times \mathbb{S}^2$~\cite{zhou2019continuity} and then converted to the matrix format.
%
The orientation encoder is designed for \textit{ab}-initio prediction or refinement of pose and can be removed when an accurate pose is pre-computed for each image.

\noindent \textbf{Deformation Encoder. } The deformation encoder maps a projection into a deformation feature representation $\boldsymbol{F}_\text{D}\in\mathbb{R}^{N\times C}$.
%
 $\boldsymbol{F}_\text{D}$ will interact with spatial features in the query-based deformation transformer decoder to output the density estimation $\hat{\sigma}$.
Therefore, $\hat{\sigma}$ is conditioned on $\boldsymbol{F}_\text{D}$.



\subsection{Query-based  Deformation Transformer Decoder}
\label{sec:decoder}
%
To generate the final density volume based on the extracted deformation and spatial features, we introduce a novel query-based deformation transformer decoder where
both features are fused into queries, to map to a specific
part of the structure. 
%
Spatial features from the implicit feature volume are fed into the query-based deformation transformer decoder, where they interact with the image deformation feature to generate the final estimated density value $\hat{\sigma}$.
%
The transformer decoder uses a cross-attention mechanism to fuse information from different sources or modalities.
%
We use $\operatorname{Attention}$ to denote the scaled dot-product attention, which operates as 
\vspace{-2mm}
\begin{equation}
    \operatorname{Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \operatorname{softmax}\left(\frac{\boldsymbol{Q} \boldsymbol{K}^{T}}{\sqrt{C}}\right) \boldsymbol{V},
    \label{equ_attention}
\end{equation}
where $\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V} \in \real^{N \times C}$ are called the query, key, and value matrices; $N$ and $C$ indicate the token number and the hidden dimension. 
%
When $\boldsymbol{Q} = \boldsymbol{K} = \boldsymbol{V}$, we call such operation  ``self-attention''.

\noindent\textbf{Structure Query Prototypes.}
We denote randomly-initialized learnable structure queries as $\boldsymbol{Q}\in\mathbb{R}^{N\times C}$. 
%
Ideally, each of the $N$ structure queries corresponds to some component of the reconstructed structure.
%
Every component has its own conformational heterogeneity leading to different behavior (or density distribution) in the space. % 例如，手可以抬高也可以放低
%

\noindent\textbf{Deformation-aware Decoder Block.}
Given an image with its deformation feature $\boldsymbol{F}_{\text{D}}$, the structure queries first interact with $\boldsymbol{F}_{\text{C}}$ in the deformation-aware decoder blocks. Each query captures information about the behavior of its corresponding component in the input image.
%
Each deformation-aware block sequentially consists of an inter-query self-attention block, a deformation-aware cross-attention layer, and a feed-forward network (FFN), where the deformation-aware cross-attention layer is computed as $\mathrm{Attention}(\boldsymbol{Q},\boldsymbol{F}_D,\boldsymbol{Q})$.
%
We stack three decoder blocks for fusing deformation cues into structure queries.

\noindent\textbf{Spatial Density Estimation.}
To estimate the density value at a specific coordinate, the queries then interact with its spatial feature $\boldsymbol{F}_S$  by spatial cross attention: $\operatorname{Attention}(\boldsymbol{Q},\boldsymbol{F}_C,\boldsymbol{Q})$.
%
In this operation, each query captures its underlying component's behavior (determined by the image deformation feature) at the arbitrary 3D position, directly manifested as the density value.
%
Finally, an FFN maps the queries to the estimated density $\hat{\sigma}$.




\subsection{Training Scheme}
To train our comprehensive system, we first calculate the projected pixel value of the predicted image from the estimated density volume corresponding to the input image ( Eqn.~\ref{eq:proj}), which is the output of a query-based deformation transformer decoder.
\begin{equation}
    \hat{\mathbf{I}}(x, y) = \hat{g} \star \int_{\mathbb{R}} \hat{\sigma}(\hat{\mathbf{R}}^{\top} \mathbf{x}+\hat{\mathbf{t}})\, \mathrm{d}z + \epsilon, \quad \mathbf{x} = (x, y, z)^{\top}
    \label{eq:proj}
\end{equation}
where $\hat{g}$ is the point spread function (PSF) of the projected image, assumed to be known from contrast transfer function (CTF) correction~\cite{rohou2015ctffind4} in the image pre-processing stage.

The loss function is to measure the squared error between the observed images $\{\mathbf{I}_i\}_{1\le i \le n}$ and the predicted images $\{\hat{\mathbf{I}}_i\}_{1\le i \le n}$ by Eqn.~\ref{eq:proj},
\begin{equation}
        \mathcal{L} = \sum_{i=1}^{n}\left\|\mathbf{I}_i - \hat{\mathbf{I}}_i\right\|_2^2 \\
\end{equation}

Unlike the squared loss function derived from the central slice theorem in the Fourier domain, as a spatial domain reconstruction approach, our loss term involves the multi-view constraints from orthogonal spatial projections of a 3D volume.


% 写作: 
% method v0: lxh 3.1
%   preliminary: 从NeRF开始
%   时域高频比较sparse 神经网络在时域好处
% Introduction: 前一半恺改过一遍; 后一半v0 lxh 3.1
%   加为什么要在时域做：频域特别不连续 不存在导数smooth的特点， 可解释性， 频域高频sampling unbalaced
% Related: lxh列一下纲要 zy完成第一版
%   传统、静态、动态（NeRF），（vision transformer：lxh）
% Results: layout lxh 3.2

% 实验结果：
% 1. synthetic 给pose 基本ready，能够打赢cryodrgn等
% 2. real 给pose 目前还没work
% 3. synthetic ab-initio 基本ready 结果打赢cryoAI等 （大家都不好）
% 4. real ab-initio 目前还没work 但大家都不好
% 倾向于用1，2，3三种setting
% 数据： 【1. cryodrgn数据】【2. 10076】【3. 7w（synthetic）】【46（下载）】

% 数据贡献：7w


% fig1&2的caption ready for review
% method初版完成 （应该需要再过很多遍）
% 曾焱一帆：
%     related works和7w数据章节请按照我给的框架先写好draft
%     3.2要的结果：扇子我们 vs drgn vs gt