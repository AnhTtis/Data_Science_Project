\section{Method}
We propose CryoFormer, a novel approach that leverages a real domain implicit spatial feature volume coupled with a transformer-based network architecture for continuous heterogeneous cryo-EM reconstruction. 
%
In this section, we begin by laying out the cryo-EM image formation model in Sec.~\ref{sec:formation}.
%
We then introduce the procedural framework of CryoFormer (Fig.~\ref{fig:method}), encompassing orientation and deformation encoders (Sec.~\ref{sec:encoders}), an implicit spatial feature volume $\mathcal{V}_\Theta$ (Sec.~\ref{sec:inr}) and a query-based deformation transformer (Sec.~\ref{sec:transformer}), with the training scheme described in Sec.~\ref{sec:training}
%

In this section, we use $\operatorname{Attention}$ to denote the scaled dot-product attention, which operates as 
\vspace{-0.1in}
\begin{equation}
    \operatorname{Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \operatorname{softmax}\left(\frac{\boldsymbol{Q} \boldsymbol{K}^{T}}{\sqrt{C}}\right) \boldsymbol{V},
    \label{equ_attention}
\end{equation}
where $\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V} \in \mathbb{R}^{N \times C}$ are called the query, key, and value matrices; $N$ and $C$ indicate the token number and the hidden dimension. 
%

\subsection{Cryo-EM Image Formation Model}
%
\label{sec:formation}
In the cryo-EM image formation model, the 3D biological structure is represented as a function  $\sigma: \mathbb{R}^{3} \mapsto \mathbb{R}^{+}$, which expresses the Coulomb potential induced by the atoms.
%
To recover the potential function, the probing electron beam interacts with the electrostatic potential, resulting in projections $\{\mathbf{I}_i\}_{1\le i \le n}$. 
% 
Specifically, each projection can be expressed as
\begin{equation}
    \mathbf{I}(x, y) = g \star \int_{\mathbb{R}} \sigma(\mathbf{R}^{\top} \mathbf{x}+\mathbf{t})\, \mathrm{d}z + \epsilon, \quad \mathbf{x} = (x, y, z)^{\top}
    \label{eq:formation}
\end{equation}
%
where $\mathbf{R} \in SO(3)$ is an orientation representing the 3D rotation of the molecule and $\mathbf{t} = (t_x, t_y,0)^\top$ is an in-plane translation corresponding to an offset between the center of projected particles and center of the image.
%
The projection is, by convention, assumed to be along the $z$-direction after rotation.
%
The image signal is convolved with $g$, a pre-estimated point spread function (PSF) for the microscope, before being corrupted with the noise $\epsilon$ and registered on a discrete grid of size $D\times D$, where $D$ is the size of the image along one dimension.
%
We give a more detailed formulation for cryo-EM reconstruction in Sec.~\ref{sec:cryoem}.




\subsection{Image Encoding for Orientation and Deformation Estimation}
\label{sec:encoders}
Given a set of input projections and their initial pose estimations, we extract latent representations for their orientation and conformational states using image encoders.
%
Following \citep{cryodrgn, zhongreconstructing} we adopt MLPs for both encoders.
%

\noindent \textbf{Orientation Encoding. } 
Given an input image $\mathbf{I}$, our orientation encoder predicts its orientation representation $\boldsymbol{F}_\text{O}\in\mathbb{R}^8$.
%
For optimization purposes, we represent rotations within the 6-dimensional space $\mathbb{S}^2\times \mathbb{S}^2$~\citep{zhou2019continuity} and translations with the remaining 2 dimensions.
%
We map each image's orientation representation $\boldsymbol{F}_\text{O}$ into a pose estimation $\hat{\phi}=(\hat{\mathbf{R}}, \hat{\mathbf{t}})$.
%
In line with cryoDRGN~\citep{cryodrgn, zhongreconstructing}, for each image $\mathbf{I}$, we compute an initial pose estimation $\phi_0 = (\mathbf{R}_0, \mathbf{t}_0)$ (via off-the-shelf softwares~\citep{scheres2012relion, punjani2017cryosparc}). 
%
While these initial estimations are not perfectly precise, particularly for cases with substantial motion, we utilize them as a guidance for our orientation encoder by pre-training it using
\begin{equation}
    \mathcal{L}_{\text{pose}} = \sum_{i=1}^{n}(\frac{1}{9}\left\|\hat{\mathbf{R}}_{i} - \mathbf{R}_{0,i}\right\|_2+\frac{1}{2}\left\|\hat{\mathbf{t}}_{i} - \mathbf{t}_{0,i}\right\|_1). 
\end{equation}
%
During the main stage of training, the orientation encoder estimates each image's pose to transform the 3D structure representation to minimize image loss (Eqn.~\ref{eqn:loss}). 
%
The gradient of the image loss is back-propagated to refine the pose encoder.
%

\noindent \textbf{Deformation Encoding. } The deformation encoder maps a projection $\mathbf{I}$ into a latent embedding for its conformational state, denoted as $\boldsymbol{F}_\text{D}$.
%
$\boldsymbol{F}_\text{D}$ subsequently interacts with 3D spatial features within the query-based deformation transformer decoder to produce the density estimation $\hat{\sigma}$.
 %
In this way, our approach models the structural heterogeneity and produces the density estimation conditioned on the conformational state of each input image.






\subsection{Implicit Spatial Feature Volume in the Real Domain}
\label{sec:inr}
%
In contrast with central slice sampling for Fourier domain reconstruction, real domain reconstruction requires sampling along $z$-direction for estimating each pixel.
%
Consequently, leveraging a NeRF-like global coordinate-based MLP adopted by~\citet{cryodrgn} and ~\citet{cryoai, cryofire} for real-domain cryo-EM reconstruction becomes computationally prohibitive.
%
We instead adopt multi-resolution hash grid encoding~\citep{muller2022instant} which has been used for real-time NeRF rendering as our 3D representation.
%
We derive the high-dimensional spatial feature at each coordinate from it to better preserve the high-frequency details with highly reduced computational cost.
% 

We use a hash grid $\mathcal{V}_\Theta$ parameterized by $\Theta$ as our 3D representation. For any given input coordinate $\mathbf{x}=(x,y,z)^{\top}$, the high-dimensional spatial feature is represented as
\begin{equation}
    \boldsymbol{F}_S = \mathcal{V}_\Theta(\mathbf{x}; \Theta).
\end{equation}
This feature encapsulates the local structural information of the specified input location. 
%
It later interacts with deformation features $\boldsymbol{F}_\text{D}$ in the deformation transformer decoder to yield the local density estimation for the coordinate conditioned on $\boldsymbol{F}_\text{D}$.

%

\subsection{Query-based Deformation Transformer Architecture}
\label{sec:transformer}
To generate the final density estimation $\hat{\sigma}$ at an arbitrary coordinate $\mathbf{x}$,  we introduce a novel query-based deformation transformer decoder, where spatial features $\boldsymbol{F}_\text{S}$ from the implicit feature volume interact with conformational state representation $\boldsymbol{F}_\text{D}$.
%
We denote randomly initialized learnable structure queries as $\boldsymbol{Q}\in\mathbb{R}^{N\times C}$ where $N$ is the number of queries and $C$ is the number of dimensions of each query.  
%
Spatial features and conformational states have shapes that match the dimensions of structure queries, specifically, $\boldsymbol{F}_{\text{S}},\boldsymbol{F}_{\text{D}}\in\mathbb{R}^{N\times C}$.


\noindent\textbf{Deformation-aware Decoder Block.}
Given an image with its conformational state $\boldsymbol{F}_{\text{D}}$, the structure queries $\boldsymbol{Q}$ first interact with $\boldsymbol{F}_{\text{D}}$ in the deformation-aware decoder blocks. 
%
Each deformation-aware block sequentially consists of an inter-query self-attention block ($\mathrm{Attention}(\boldsymbol{Q},\boldsymbol{Q},\boldsymbol{Q})$), a deformation-aware cross-attention layer, and a feed-forward network (FFN), where the deformation-aware cross-attention layer is computed as $\mathrm{Attention}(\boldsymbol{Q},\boldsymbol{F}_{\text{D}},\boldsymbol{Q})$.
%
We stack three decoder blocks for fusing deformation cues into structure queries.

\noindent\textbf{Spatial Density Estimation.}
To estimate the density value at a specific coordinate, structure queries $\boldsymbol{Q}$ then interact with the spatial feature $\boldsymbol{F}_{\text{S}}$ at this coordinate by spatial cross attention, computed as $\operatorname{Attention}(\boldsymbol{Q},\boldsymbol{F}_{\text{S}},\boldsymbol{Q})$.
%
Finally, an FFN maps the queries to the estimated density $\hat{\sigma}$.




\subsection{Training Scheme}
\label{sec:training}
To train our system, we first calculate the projected pixel values as 
\begin{equation}
    \hat{\mathbf{I}}(x, y) = \hat{g} \star \int_{\mathbb{R}} \hat{\sigma}(\hat{\mathbf{R}}^{\top} \mathbf{x}+\hat{\mathbf{t}})\, \mathrm{d}z + \epsilon, \quad \mathbf{x} = (x, y, z)^{\top}
    \label{eq:proj}
\end{equation}
where $\hat{g}$ is the point spread function (PSF) of the projected image, assumed to be known from contrast transfer function (CTF) correction~\citep{rohou2015ctffind4} in the image pre-processing stage.
%
The loss function for training is to measure the squared error between the observed images $\{\mathbf{I}_i\}_{1\le i \le n}$ and the predicted images $\{\hat{\mathbf{I}}_i\}_{1\le i \le n}$:
\begin{equation}
\label{eqn:loss}
        \mathcal{L} = \sum_{i=1}^{n}\left\|\mathbf{I}_i - \hat{\mathbf{I}}_i\right\|_2^2.
\end{equation}


