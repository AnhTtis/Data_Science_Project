\begin{figure*}
\begin{center}
\includegraphics[width=0.98\textwidth]{fig/figs/overview_re.jpg}
\end{center}
\vspace{-7.5mm}
\caption{
(a) The proposed MDQE architecture consists of a backbone and encoder that extract multi-scale features $F$ from a video clip, a query initialization module that produces temporally-aligned frame-level queries $q_t$, a decoder that decodes discriminative clip-level queries $q$, and a Mask Net that generates mask features $D$.
The mask features $D$ and clip-level queries $q$ are combined via a linear combination to obtain the clip-level instance mask $\hat{M}$, which is supervised by our proposed inter-instance mask repulsion loss in \cref{sec:inter-inst}.
(b) The frame-level query initialization consists of two steps: grid-guided query selection and inter-frame query association, resulting in temporally-aligned frame-level queries. Please refer to \cref{sec:query-init} for more details. 
% Firstly, the features are fed into the semantic segmentation head to produce a class-aware activation map, which is then divided into patches evenly using a grid. The peak point with the highest response (represented as white dots) in each grid is selected as the initial frame-level queries. Secondly, to ensure temporal consistency of frame-level queries, the embeddings of frame-level queries are projected using the embedding head, and their embedding similarity is used to associate them across frames. This results in temporal-aligned frame-level queries.
}\label{fig:overview}
\vspace{-3mm}
\end{figure*}