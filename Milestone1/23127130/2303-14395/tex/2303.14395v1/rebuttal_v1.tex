\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{float}
\usepackage{diagbox}
\usepackage{changepage}
\usepackage{multirow}
\usepackage{enumitem}


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{464} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Authors' Responses to the Reviews of Paper \#464}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
We thank all reviewers for their valuable comments. We are sorry that we may not present our method clearly. We will improve the description and correct typos in revision to make the paper easier to understand. 
%-------------------------------------------------------------------------

\noindent{\bf \large To Reviewer QkPn:}

{\bf Q1:} \textbf{Threshold of $O_i$.} We set $\epsilon=0.001$ as default, which is only used to find all nearby non-target instances. In theory, the combination of all instances is also possible. However, for two instances whose locations are far apart, their mask prediction usually do not affect each other. Therefore, the GT inter-instance mask only takes all nearby non-target instances into account. 

{\bf Q2:} \textbf{$\mathcal{L}_\text{inter-mask}$.} We correct a typo in the loss formula.
The weight should be $w_{O_i}= N / n_i$, where $N=THW$ is the total number of pixels in the predicted mask and is used to offset the multiplier in the BCE term.
In the inter-mask loss, the numerator is the sum of the BCE loss of $n_i+n_{O_i}$ pixels, but the denominator uses $n_i$. So the loss will be higher if $n_{O_i}$ is larger. 
The GT inter-instance mask is a binary matrix and supervises the predicted mask on the target instance and its nearby non-target instances regions via the common BCE loss, so its gradients can be backpropagated during training. The attraction term of $\mathcal{L}_\text{inter-mask}$ just pushes the segmenter to predict the pixels of the target instance as 1. While $\mathcal{L}_\text{intra-mask}$ executes on the entire pixels of the image, which also pushes the segmenter to predict background pixels as 0.

{\bf Q3:} \textbf{TCA.} Temporal cross-attention is based on the traditional multi-scale deformable attention (MSDefAttn) [1]. The difference is that MSDefAttn extracts deformable points from single-frame multi-scale feature maps, while TCA extracts deformable points from multi-frame single-scale feature maps.
    
[1] Xizhou Zhu, et al. Deformable detr: Deformable transformers for end-to-end object detection. ICLR 2021.

{\bf Q4:} \textbf{$\mathcal{L}_\text{init}$.} The specific formula $\mathcal{L}_\text{init}$ is provided in the part A of supplementary materials due to limited space. We will put it into the main paper in revision.

{\bf Q5:} \textbf{Others.} Object queries and instance queries represent per-frame queries and per-clip queries, respectively. Following common settings, we adopt both BCE and Dice losses for $L_\text{intra-mask}$ and $L_\text{mem-mask}$. For {$d_k$}, we employ same normalization term as used in transformer attention. 

%-------------------------------------------------------------------------
\vspace{+1mm}
\noindent{\bf \large To Reviewer 6GSR:}

{\bf Q1: \textbf{$AP_{so}, AP_{mo}, AP_{ho}$}.} Due to limited space, we only provide the comparison with SOTA methods on OVIS here. We will add a comprehensive comparison of occlusion metrics in the revised experimental results.
\begin{table}[h]
\begin{center}
\setlength{\tabcolsep}{0.5mm}{
      \linespread{2}
      \begin{tabular}
      % [t]{cclcccccccccc}
      {p{0.1\textwidth}<{\centering}p{0.115\textwidth}p{0.05\textwidth}<{\centering}p{0.05\textwidth}<{\centering}p{0.05\textwidth}<{\centering}p{0.05\textwidth}<{\centering}}
         \Xhline{1pt}
         Type & Methods & AP & AP$_{so}$ &AP$_{mo}$ &AP$_{ho}$  \\
         \Xhline{1pt}
         \multirow{2}{*}{Per-frame}
         & IDOL          &  24.3 & 34.8 & 29.0 & 8.1 \\
         & MinVIS        &  26.3 & 45.8 & 31.6 & 9.9 \\
         \Xhline{1pt}
         \multirow{3}{*}{Per-clip}
         & SeqFormer     &  15.1 & 31.8 & 17.3 & 3.2 \\   
         & VITA          &  19.6 & 32.3 & 20.4 & 8.2\\
         & MDQE (ours)   &  29.6 & 45.9 & 36.7 & 13.0\\
        \Xhline{1pt}
      \end{tabular}
}
\end{center}
\vspace{-5.5mm}
\caption{ Occlusion metrics comparison of SOTA VIS methods on OVIS valid set.
}\label{tab:sota_yt22long}
\vspace{-4.5mm}
\end{table}

{\bf Q2:} \textbf{Table 2.} The long videos on YouTube-VIS 2022 valid set are mostly captured by using fast-moving cameras, which is rarely occurred in the videos of the training set. This means that the training set may drive the per-clip MDQE to learn the temporal prior: objects typically have small motion between adjacent frames. However, the temporal prior does not hold on the YouTube-VIS 2022 valid set, resulting in slightly lower performance compared to per-frame MDQE. Figure 5 of supplementary materials visualizes instance masks on a long video shot by fast-moving device in YouTube-VIS 2022 valid set.

{\bf Q3:} \textbf{Table 4(c).} How much is the performance down when $T_o=1$? A lot! How $AP_{so}, AP_{mo}, AP_{ho}$?


%-------------------------------------------------------------------------
\vspace{+1mm}
\noindent{\bf \large To Reviewer 9uo4:}  

{\bf Q1:} \textbf{Missing details.} 1) For $\mathcal{L}_\text{init}$, please refer to \textbf{ Q4} of Reviewer QkPn. 2) In Line 462, $q^m$ means the memory-based instance queries. $q$ indicates the instance queries learned from the current clip, and $q^T$ is the transpose of $q$. Please refer to Figure 1 for a better distinction between $q^m$ and $q$. 3) For TCA, please refer to \textbf{Q3} of Reviewer QkPn. 4) We will add a detailed description for Figure 3 in revision. 5) We employ SeqFormer as the clip-level VIS baseline.
Please refer to Lines 314-319 or Line 586. 

{\bf Q2:} \textbf{Object re-association.} For objects that disappear and reappear in long videos, our MDQE can re-associate objects if they have some deviations in appearance.
% However, if the object disappears beyond 30 frames and the video contains many similar-looking instances, all existing VIS methods fail to accurately re-associate such instances. 
We will provide some visualization examples in revision.

{\bf Q3:} \textbf{Table 2.} Please refer to \textbf{Q2} of Reviewer 6GSR.

{\bf Q4:} \textbf{Related work.}
We will include all aforementioned related work of frame-level VIS methods in revision.

%-------------------------------------------------------------------------
\vspace{+1mm}
\noindent{\bf \large To Reviewer S1sz:}

{\bf Q1:} \textbf{Many additional components.}
We agree that our core contributions lie in two perspectives: spatial-occlusion and temporal-occlusion. Besides, grid-guided query selection and inter-frame query association (Sec. 3.2) have the ability to handle intra-clip spatial-occlusion and temporal-occlusion. We will consider to focus on main contributions and analysis it in-depth in revision.

{\bf Q2:} \textbf{Eq. (3).} Please refer to \textbf{Q2} of Reviewer QkPn, we correct a typo in the weight $w_{O_i}$. The complementary GT inter-instance mask $M_{O_i} = \cup_{j\in O_i} M_j$ is directly determined by the GT masks of its nearby non-target instances, which is a binary matrix and does not need any pre-processing. In code implementation, we further employ $M_{O_i} = M_{O_i} \cap (1-M_i)$ to remove pixels belonging to the target instance. This avoids the effect that the annotations of instance segmentation usually are not mutually exclusive. Consider that the number of pixels belonging to the nearby non-target instance may be much larger than those belonging to the target instance, i.e. $n_{O_i} \gg n_i$, then the loss will be very large. To address the instability during training, we set the denominator of the weight as $\max(n_i, \frac{n_{O_i} + n_i}{\lambda_2})$ in the code implementation, where $\lambda_2$ is the balanced weight of the intra-instance mask loss in Eq. (6).

{\bf Q3:} \textbf{Long videos.} In Eq. (7), the 'dice' item is made on overlapping frames, while the `cos' item is computed on instance embeddings of the previous $T_\text{mem}$ clips. Long-time messenger (LTM) can greatly reduce the strong dependence of clip-level VIS methods on overlapping frames, enabling MDQE to achieve high performance when using near-online inference. In Table 4(c), when introducing LTM into MDQE, and the performance gap between $T_o=3$ and $T_o=1$ shrinks from 3.1\% to 1.2\% AP, and the performance of using near-online inference ($T_o=1$) improves from 26.9\% to 29.6\% AP. 

% {\bf Q4:} \textbf{Tracking metrics.} A toy experiment....

% {\bf Q5:} \textbf{Convergence comparison with SeqFormer.} ....

{\bf Q4:} \textbf{Table 4(d).} When $T=9$, performance degradation may mainly come from complex trajectories of objects in long clips. Therefore, during training, we may need more long videos to cover various object motions, which helps the model fully explore object complex trajectories. However, there is only 901 videos in the training set of OVIS.


%%%%%%%%% REFERENCES
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

\end{document}
