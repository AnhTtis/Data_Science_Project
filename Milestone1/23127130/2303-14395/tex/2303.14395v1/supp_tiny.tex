% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,onecolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{float}
\usepackage{diagbox}
\usepackage{changepage}
\usepackage{cuted} % strip
\usepackage{lipsum,multicol}
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{464} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\def\thesection{\Alph{section}}


\begin{document}

    

    \newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
	\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
	\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}
	%%%%%%%%% TITLE - PLEASE UPDATE
	\title{Supplementary Materials to \\ `MDQE: Mining Discriminative Query Embeddings to Segment \\ Occluded Instances on Challenging Videos'}
	
     \author{{Minghan Li \qquad Shuai Li \qquad Wangmeng Xiang  \qquad Lei Zhang\thanks{Corresponding author.} } \\
    	Department of Computing, Hong Kong Polytechnic University  \\
    	{\tt\small liminghan0330@gmail.com, \{csshuaili, cswxiang, cslzhang\}@comp.polyu.edu.hk}
    }
	
\maketitle
\appendix


In this supplementary file, we provide the following materials:
  \begin{itemize}
    % \item Gradient descent of inter-instance mask repulsion loss ($cf.$ Section 3.3 in the main paper);
    \item Visualization of initial query locations selected by our query initialization ($cf.$ Section 4.2 in the main paper);
    \item Quantitative results with ResNet50 and ResNet101 on YouTube-VIS 2019 ($cf.$ Section 4.3 in the main paper);
    \item Visualization of segmented instance segmentation on challenging videos ($cf.$ Section 4.3 in the main paper).
    % \item Ablation studies on YouTube-VIS 2021 and YouTube-VIS 2022 valid sets ($cf.$ Section 4.3 in the main paper).
  \end{itemize}

% \section{Gradient descent of inter-instance mask repulsion loss}
% We analysis the gradient descent process of our inter-instance mask repulsion loss for better understanding.
% \textbf{Inter-instance BCE loss.} In Eq. (4) of the main paper, we adopt a weighted BCE form to assign a larger weight for the pixels belonging to the target instance and its nearby instances. Its gradient descent, therefore, is similar to the typical BCE loss, no more explanation here.
% \textbf{Inter-instance Dice loss.} Based on the formulas of typical Dice loss and our inter-instance Dice loss proposed in Eq. (2) and Eq. (5) of the main paper, their gradients on the instance mask of the pixel at position $p$ are
% \begin{equation}\label{eq:grad_dice}
    % \frac{\partial L_\text{Dice}}{\partial \hat{M}_p}=\frac{2|\hat{M} \odot M| - 2M_p(|\hat{M}| + |M|)}{(|\hat{M}| + |M|)^2},
% \end{equation}
% \begin{equation}\label{eq:grad_dice_inter}
    % \frac{\partial L_\text{Dice-inter}}{\partial \hat{M}_p}=\frac{2|\hat{M} \odot M| + |(1-\hat{M})\odot M_o| - (2M_p-M_{op})(|\hat{M}| + |M| + |M_o|)}{(|\hat{M}| + |M| + |M_o|)^2},
% \end{equation}
% where the index of instance ID $i$ is omitted here for clarify. 



\section{Visualization of initial query locations}

\cref{fig:query_init_visual} visualizes the initial query locations selected by our query initialization method on challenging videos of OVIS valid set. We see that frame-level queries are well associated in both spatial and temporal directions, even in crowded scenes. 

\section{Quantitative results with ResNet50 and ResNet101 on YouTube-VIS 2019 valid set}
The quantitative results on YouTube-VIS 2019 valid set are reported in \cref{tab:sota_yt19}. We see that the newly developed VIS methods utilizing transformer-based prediction heads can significantly improve the performance to 49.8\% mask AP with ResNet50 backbone and 51.9\% mask AP with ResNet101 backbone.
% For instance, with ResNet 101 backbone, IDOL \cite{IDOL} and SeqFormer \cite{seqformer} with deformable-attention Decoder obtain 50.1\% and 49.0\% mask AP, and VITA \cite{heo2022vita} with stronger masked-attention Decoder updates the state-of-the-art to 51.9\% mask AP.
Our proposed MDQE employs 5-frame clips as input and a deformable-attention decoder, reaching 47.3\% and 47.9\% mask AP with ResNet50 and ResNet101 backbones, respectively. 
The YouTube-VIS 2019 valid set contains mostly short videos. Therefore, methods like SeqFormer \cite{seqformer} and VITA \cite{heo2022vita}, which take the video-in video-out offline inference, can incorporate the temporal information to distinguish the objects that have obvious differences in feature space. This is the reason why they achieve leading mask AP scores on YouTube-VIS 2019 valid set. However, for objects that have similar-looking appearance or heavy occlusions, methods like SeqFormer \cite{seqformer} and VITA \cite{heo2022vita} cannot extract discriminative features to distinguish them accurately, thereby resulting in unsatisfactory results on OVIS data set ( $cf.$ Section 4.3 in the main paper).


\section{Visualization of segmented instance masks}
Figs. \ref{fig:yt21_visual} - \ref{fig:ovis_visual} show the instance masks obtained by the recently proposed top-performing methods with ResNet50 backbone, including IDOL\cite{IDOL} (ECCV 2022), MinVIS\cite{huang2022minvis} (NIPS 2022), VITA\cite{heo2022vita} (NIPS 2022) and our MDQE.

\textbf{YouTube-VIS 2021 valid set.} \cref{fig:yt21_visual} shows the visual comparisons on YouTube-VIS 2021 valid set. We can see that all recent top-performing methods can well process the simple videos on YouTube-VIS 2021 valid set.
% but obtain inaccurate instance association across frames on videos shot by fast-moving cameras. 
% Involving more similar videos in the training set can alleviate this issue.

\textbf{OVIS valid set.} \cref{fig:ovis_visual} and \cref{fig:ovis_visual2} compare the visual results of instance segmentation on OVIS valid set. Compared with IDOL \cite{IDOL} and MinVIS \cite{huang2022minvis} with per-frame input, our MDQE with per-clip input can exploit richer spatio-temporal features of objects, thereby segmenting occluded instances better. On the other hand, MDQE with discriminate query embeddings can track instances with complex trajectories more accurately in challenging videos, such as cross-over objects and heavily occluded objects in crowded scenes.

\inpu{fig/query_init_visual.tex}
\input{tab/sota_yt19.tex}

\input{fig/yt21_visual.tex}
\inpu{fig/ovis_visual.tex}
\inpu{fig/ovis_visual2.tex}  
% \inpu{fig/yt22_visual.tex}

\clearpage


% \section{Ablation study on YouTube-VIS 2021 and YouTube-VIS 2022 Long videos}
% \textbf{Initialization.} 
% We employ object queries with our proposed initialized embeddings and the `O2I' decoder architecture proposed in Sec. 3.2 of the main paper, the performance can reach 44.5\% and 29.7\% mask AP on YouTube-VIS 2021 and 2022 valid sets, bringing about 11.4\% and 14.1\% mask AP improvement respectively.

% \textbf{Inter-instance mask repulsion loss.} As shown in the first and third rows of \cref{tab:abl_yt21}(c), since there are a small number of objects in videos of YouTube-VIS date sets, our inter-instance repulsion mask loss proposed for occluded objects in crowded scene only brings 0.8\% and 1.3\% performance improvement on YouTube-VIS 2021 and 2022 valid sets respectively.

% \textbf{Long-time messenger (LTM).} As shown in \cref{tab:abl_yt21}(c), on the challenging long videos of YouTube-VIS 2022 valid set, by introducing the cosine similarity of query embeddings into the score matrix of tracking can increase the performance from 24.2\% to 29.2\% mask AP. After replacing the query embeddings $q^m$ in the memory pool with the propagated query embeddings $q^{\cdot|m}$ output from LTM into the above cosine similarity, the mask AP can be further increased to 31.6\%. 

% \textbf{The length of clip.} As shown in \cref{tab:abl_yt21}(b), our MDQE with video clips of varying lengths as input can maintain the stable performance on YouTube-VIS 2021 valid set, achieving around 45\% mask AP. While as the number of frames increases, the performance on YouTube-VIS 2022 Long videos shows a downward trend. We argue that due to the lack of fast-moving objects in training data, MDQE with per-clip input cannot effectively decode discriminative embeddings for fast-moving objects, especially in the inter-frame query association of embedding initialization. 

% \inpu{tab/ablations_yt21.tex}





%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{MDQE_CVPR2023/latex/main_ios4}
}

\end{document}
