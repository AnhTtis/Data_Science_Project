\section{Introduction}
\label{sec:intro}

\input{fig/overview.tex}

Video instance segmentation (VIS) \cite{yang2019video} aims to obtain pixel-level segmentation masks for instances of different classes over the entire video.
% which needs to simultaneously detect, classify, segment and track multiple objects in a video. 
The current VIS methods can be roughly divided into two paradigms: per-frame input based methods \cite{yang2019video, bertasius2020classifying, Li_2021_CVPR, yang2021crossover, ke2021pcan, IDOL, huang2022minvis} and per-clip input based methods \cite{Athar_Mahadevan20stemseg, li2022cico, yang2022TempEffi, wang2020vistr, wu2022trackletquery, hwang2021video, seqformer}. 
The former paradigm first partitions the whole video into individual frames to segment objects frame by frame, and then associate the predicted instance masks across frames, while the latter takes per-clip spatio-temporal features as input to predict multi-frame instance masks with the help of embedding learning \cite{Athar_Mahadevan20stemseg}, graph neural networks \cite{wang2021end} and transformer networks \cite{wang2020vistr, hwang2021video, seqformer,heo2022vita}. 

The recently proposed per-clip VIS methods \cite{wang2020vistr,wu2022trackletquery,hwang2021video,seqformer} have set new records on the YouTube-VIS datasets \cite{yang2019video}, achieving significant performance improvement over the per-frame VIS methods \cite{cao2020sipmask,Li_2021_CVPR, yang2021crossover,QueryInst,bertasius2020classifying, zhu2022IAI, he2023inspro}.  
SeqFormer \cite{seqformer} and VITA \cite{heo2022vita} locate an instance in each frame and aggregate temporal information to learn powerful representations of video-level instances via a naive weighted manner and a video-level decoder, respectively.
However, on the challenging OVIS dataset \cite{qi2021occluded}, which includes occluded or similar-looking instances in crowded scenes, the per-clip VIS methods lag behind the per-frame ones. 
Actually, the recently developed per-frame method IDOL\cite{IDOL} records state-of-the-art performance on OVIS by introducing contrastive learning \cite{pang2021quasi, wang2021densectt, dave2022temporalctt} to learn inter-frame instance embeddings. We argue that the per-clip VIS methods should be able to exploit richer spatial-temporal features and achieve better performance than their per-frame counterparts. However, there are two main issues that limit the existing per-clip methods to achieve this goal.

First, existing query-based VIS methods adopt zero or random input as the positional embeddings and content features of object queries in decoder layers, which cannot encode spatio-temporal prior of objects, resulting in poor results on challenging videos. 
Second, during training, the existing mask prediction loss mainly forces each query to match the pixels of its target instance \cite{Wang2022InstUnique,ke2021occlusion-aware} and mismatch the pixels of other instances and the background. No further inter-instance clue has been exploited to teach the segmenter to distinguish mutually occluded instances. 

To address the above issues, we propose to mine discriminative query embeddings (MDQE) to better segment hard instances on challenging videos for per-clip VIS methods. First, we propose to improve the initialization of object queries to specify discriminative spatial-temporal priors. We divide the activation map of each frame into several patches via a grid and select the peak point in each patch as the initial positions of frame-level queries, and then associate them across frames by embedding similarity to ensure that frame-level queries in the same grid of the video clip can correspond to the same object.
Second, to teach the query-based segmenter to distinguish occluded instances, we replace the original mask prediction loss with an inter-instance mask repulsion loss, which forces each query to activate the pixels of its target instance and suppress the pixels of its surrounding non-target instances. 

The proposed VIS method with per-clip input, namely MDQE, is the first to achieve contrastive learning of instance embeddings via query initialization and the inter-instance mask repulsion supervision, which can effectively segment hard instances on challenging videos. 
Our experiments on both OVIS and YouTube-VIS datasets validate that MDQE with per-clip input achieve competitive performance with its per-frame competitors. 
% recording new state-of-the-art mask AP scores while keeping a fast speed. ???




