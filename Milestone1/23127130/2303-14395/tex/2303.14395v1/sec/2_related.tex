\section{Related work}
\label{sec:related}
Our work is related to the many per-frame VIS methods, the recently proposed per-clip VIS methods, as well as the methods for learning query embeddings.

\textbf{Per-frame input based VIS methods.} A popular VIS pipeline \cite{yang2019video, cao2020sipmask,Li_2021_CVPR,lin2021video,liu2021sg,yang2021crossover,QueryInst,ke2021pcan,huang2022minvis,IDOL} is to extend the representative image instance segmentation methods \cite{he2017mask,bolya2019yolact,tian2020conditional,carion2020end,cheng2021mask2former} by adapting a frame-to-frame instance tracker. For example, in \cite{yang2019video,QueryInst,huang2022minvis}, the clues such as category score, box/mask IoU and instance embedding similarity are integrated into the tracker. However, these trackers may struggle in distinguishing instances with similar appearance. Inspired by contrastive learning \cite{chen2020simplectt, pang2021quasi,khosla2020supervisedctt,wang2021densectt,dave2022temporalctt}, IDOL \cite{IDOL} learns discriminative instance embeddings for multiple object tracking frame by frame, achieving state-of-the-art results on OVIS \cite{qi2021occluded}.
Besides, clip-to-clip trackers \cite{bertasius2020classifying, lin2021video, qi2021occluded} propagates the predicted instance masks from a key frame to other frames using deformable convolution \cite{bertasius2020classifying,dai2017deformable}, non-local block \cite{lin2021video}, correlation \cite{Li_2021_CVPR,qi2021occluded}, graph neural network \cite{wang2021end}, \etc. By exploiting the temporal redundancy among overlapped frames, clip-to-clip trackers improve much the performance of per-frame methods.

\textbf{Per-clip input based VIS methods.} A clip-in clip-out VIS pipeline was firstly proposed in \cite{Athar_Mahadevan20stemseg} to model a video clip as a single 3D spatio-temporal pixel embedding. 
In recent years, transformer based per-clip methods \cite{wang2020vistr,wu2022trackletquery,hwang2021video,yang2022TempEffi,seqformer} have achieved impressive progress on the YouTube-VIS datasets \cite{yang2019video}. VisTR \cite{wang2020vistr} views the VIS task as a direct end-to-end parallel sequence prediction problem, but it needs a large memory to store spatio-temporal features. To solve the issue, IFC\cite{hwang2021video} transfers inter-frame information via efficient memory tokens, and SeqFormer \cite{seqformer} locates an instance in each frame and aggregates temporal information to predict video-level instances. To keep object temporal consistency, EfficientVIS \cite{wu2022trackletquery} transfers inter-clip query embeddings via temporal dynamic convolution.

However, per-clip VIS methods do not perform well on the challenging OVIS videos \cite{qi2021occluded} with occluded objects in crowded scenes. Actually, occlusion-aware models have been developed for related tasks \cite{wang2018repulsion,xie2021pscocc,ke2021occlusion-aware,miao2021occreid}. For instance, a bilayer convolutional network is developed in \cite{ke2021bcnet} to infer the occluder and occluded instances in image segmentation. A repulsion detection loss is designed in \cite{wang2018repulsion} to distance the bounding box of an object from the surrounding non-target objects for detecting individual pedestrian in a crowd. Inspired by these works, we propose an inter-instance mask repulsion loss to distinguish the pixels of each instance from its nearby non-target instances.

\textbf{Query initialization.} Existing query-based VIS methods adopt zero-initialized (\eg, DETR \cite{carion2020end}) or randomly-initialized (\eg, Deformable DETR\cite{zhu2020deformable}) inputs as initial queries. The initial queries cannot encode well the spatio-temporal priors of objects, making the query-based segmenter difficult to distinguish occluded instances with similar appearance. Actually, query initialization with contextual and positional information has been used in many computer vision tasks \cite{li2022maskdino, bai2022lidatransfusion,he2022queryprop, pei2022osformer} for higher performance or faster convergence. 
However, it has not been well explored in VIS. In this paper, we thus propose a query initialization method to obtain temporally-aligned frame-level queries.



