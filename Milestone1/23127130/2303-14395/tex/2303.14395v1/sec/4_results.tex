
\input{tab/ablations_ovis.tex}
\input{fig/inter_mask_visual}

\section{Experimental Results}
% \input{tab/sota_yt22_long.tex}

\subsection{Experiments setup}

\textbf{Datasets.} YouTube-VIS \cite{yang2019video} 2019/2021 respectively contain 2,283/2,985 training, 302/421 validation, and 343/453 test videos over 40 categories. All the videos are annotated for every 5 frames. The number of frames per video is between 19 and 36. 
% The recently released 2022 version extends YouTube-VIS 2021 with 71/50 additional long videos in validation/test sets, respectively. The length of long videos is between 36 and 84 frames.
OVIS \cite{qi2021occluded} includes 607 training, 140 validation and 154 test videos, scoping 25 object categories. Different from YouTube-VIS series, OVIS dataset includes longer videos, up to 292 frames, with more objects per frame and varying occlusion levels. The proportions of objects with no occlusion, slight occlusion, and severe occlusion are 18.2\%, 55.5\%, and 26.3\%, respectively. Note that 80.2\%  of the instances are severely occluded in at least one frame, and only 2\% of the instances are not occluded in any frame. 

\input{tab/sota_yt21_ovis.tex}

\textbf{Evaluation metrics.} The commonly used metrics, including average precision (AP) at different IoU thresholds, average recall (AR) and the mean value of AP (mAP), are adopted for VIS model evaluation. OVIS divides  all  instances  into  three groups called slightly occluded (AP$_\text{so}$), moderately occluded (AP$_\text{mo}$) and heavily occluded (AP$_\text{ho}$), where the occlusion scores of instances are in the range  of [0, 0.25], [0.25, 0.5] and [0.5, 0.75], respectively, whose proportions are 23\%, 44\% and  49\% accordingly. 
% why the sum of three groups is more than 100% ???

\textbf{Implementation details.} Our method is implemented on top of detectron2 \cite{wu2019detectron2}. Following \cite{seqformer}, we set six layers with multi-scale deformable attention module in encoder and decoder, and employ $200$ object queries with dynamic one-to-many label assignment as in \cite{IDOL}.
We first pre-train our model on COCO \cite{lin2014microsoft}, and then finetune it on VIS datasets \cite{yang2019video, qi2021occluded}. In pre-training, we sample an image as the key frame, and resize it in a relative range [0.6, 1] as the reference frame. All models with ResNet50 backbone \cite{he2016deep} are trained on eight GeForce RTX 3090 GPUs with 24G RAM, and models with Swin Large backbone \cite{Liu_2021_ICCV} are trained on four NVIDIA RTX A6000 with 48G RAM. The initial learning rate is 0.0001, and it decays by 0.1 at 20k and 24k iterations. During training and inference, unless specified, all frames are resized to a shorter edge size of $360$ on YouTube-VIS datasets or $480$ on OVIS dataset, and the clip length is taken as $T=4$ on ResNet50 backbone and $T=3$ on Swin Large backbone, respectively.
% The inference of sll experiments with ResNet50 and Swin backbones can be executed in a single GeForce RTX 3090 GPU.

\subsection{Ablation study}
\vspace{-1mm}
This section performs ablation studies on MDQE and discusses the impact of query embeddings on the segmentation performance. The OVIS valid set is used.

\textbf{Query initialization.} In \cref{tab:abl_init}, we explore the performance improvements brought by query initialization. The baseline Seqformer \cite{seqformer} with `I2O' decoder architecture achieves 15.4\% mask AP. By using grid-guided query selection to initialize frame-level queries, the performance is improved to 19.8\% mask AP. By employing both query initialization and the `O2I' decoder architecture, the mask AP and AP$_{50}$ increase to 24.2\% and 47.5\%, respectively. Additionally, the temporal cross-attention layer (TCA) further improves mask AP by about 1\%. The proposed query initialization significantly enhances instance detection and segmentation accuracy in challenging videos.

\textbf{Inter-frame query association.} In Table \ref{tab:abl_inter_frame_asso}, we investigate the impact of window size $w$ on inter-frame query association for query initialization.
Without inter-frame query association ($w\!=\!0$), the performance is only 28.5\% mask AP. When the window size is set to 3, 5 and 7, the mask AP increases by 1.2\%, 2.1\% and 2.0\%, receptively. So we set the default size as 5. The top row of \cref{fig:inter_mask_visual} visualizes the initial query positions selected by our proposed query initialization method. We see that it keeps temporal consistency and avoids confusion between occluded objects. Visualization of the initial query positions on more videos can be found in the \textbf{supplementary materials}.

\input{fig/abl_study_clip_length}

\textbf{Inter-instance mask repulsion loss.} In \cref{tab:abl_inter_mask}, we compare the performance of typical mask prediction loss and our inter-instance mask repulsion loss.
MDQE with the typical mask prediction loss achieves only 29.0\% mask AP, and performs poorly in all occluded objects.
When the weight $\alpha$ of the inter-instance BCE loss is set to $2$, the performance increases to 30.5\% mask AP, and introducing inter-instance Dice loss further increases mask AP to 31.2\%. 
If we set a higher threshold $\epsilon$ in \cref{eq:biou-inter} to only consider heavily occluded objects, the performance drops by 1.4\% AP$_{so}$ on slightly occluded objects.
Overall, our inter-instance mask repulsion loss brings significant improvements on all occlusion metrics,
3.9\% AP$_{so}$, 3.2\%  AP$_{mo}$ and 1.7\% AP$_{ho}$, respectively, resulting in better instance segmentation results on challenging videos. The visual comparison of instance masks on a video with heavily occluded objects is displayed in \cref{fig:inter_mask_visual}.
% shows that our inter-instance mask repulsion loss obtains better instance segmentation results on challenging videos, compared with typical mask prediction loss.
% compares instance masks predicted by MDQE with typical mask prediction loss and our inter-instance mask repulsion loss on a challenging video, obtaining better instance segmentation results. 
% One can see that the inter-instance mask repulsion loss helps the decoder to exploit discriminative embeddings for occluded instances, resulting in better instance segmentation results. 

\input{tab/sota_occ_metrics.tex}

\textbf{Tracking.} In \cref{tab:abl_track}, we compare the performance of clip-by-clip tracker using different weights in the score matrix in \cref{eq:tracking}. 
Using only the mIou term ($\beta_1\!=\!1$) or the embedding similarity term ($\beta_2\!=\!1$) respectively achieves 29.1\% and 28.3\% mask AP, while the mIou term performs better on moderately and heavily occluded objects. By enabling both the two terms, the mask AP increases to 30.6\% and all occlusion metrics improve significantly. If the number of frames $T_\text{mem}$ in memory pool is reduced from 10 to 5 frames, the performance will drop slightly.

\textbf{Effect of the clip length.} In \cref{fig:abl_clip_length}, we explore the VIS results by varying the clip length. MDQE with $T\!=\!1$ achieves 27\% mask AP, slightly higher than the state-of-the-art per-frame input  method MinVIS\cite{huang2022minvis} in \cref{tab:sota_yt21_ovis}. The mask AP fluctuates between 30.0\% and 31\% with the increase of the frame number of input clip, peaks at around 31.0\% mask AP with 7-frame per clip, and then falls to 29\% mask AP with 9-frame per clip. When $T\!=\!9$, the performance drops because of the complex trajectories of objects in long clips, which can be alleviated by increasing the training samples of long videos.


\subsection{Main Results}
With ResNet50 backbone, we compare  in \cref{tab:sota_yt21_ovis} the proposed MDQE with state-of-the-art methods on OVIS and YouTube-VIS 2021 datasets. For those methods marked by `$^*$', we employ video-in video-out offline inference on YouTube-VIS valid sets (less than 84 frames), and clip-in clip-out inference with overlapping frames between clips on OVIS (at most 292 frames). The occlusion metrics of SOTA methods on OVIS is provided in \cref{tab:sota_occ_matric} as well. 
% The comparison on YouTube-VIS 2022 Long videos is shown in \cref{tab:sota_yt22long}. 
In addition, we report the performance comparison between recently proposed VIS methods with Swin Large backbone in \cref{tab:sota_swinl}. Due to the limit of space, the experiments on ResNet101 backbone and YouTube-VIS 2019 valid set can be found in the \textbf{supplementary materials}.

\textbf{YouTube-VIS \cite{yang2019video} 2021 valid set.} From \cref{tab:sota_yt21_ovis}, we can see that early VIS methods \cite{yang2019video, cao2020sipmask,Li_2021_CVPR, yang2021crossover} using dense anchors only obtain around 30\% mask AP, while recently proposed VIS methods \cite{IDOL, huang2022minvis,heo2022vita} using sparse object queries can reach more than 44\% mask AP. Since the videos in YouTube-VIS 2021 are short and simple, the performance gap between per-frame input and per-clip input based methods is not significant. Based on the strong decoder layers proposed in Mask2Former, the frame-level method MinVIS \cite{huang2022minvis} and the clip-level method VITA \cite{heo2022vita} respectively reset new state-of-the-arts with 44.2\% and 45.7\% mask AP. Without using the masked-attention in Mask2Former, our proposed MDQE achieves 44.5\% mask AP, which is only slightly lower than VITA \cite{heo2022vita}. 

\input{tab/sota_swinl.tex}


\textbf{OVIS \cite{qi2021occluded} valid set.} OVIS is much more difficult than YouTube-VIS. Its videos have much longer duration with occluded and similar-looking objects. The early per-frame methods MaskTrack R-CNN \cite{yang2019video} and CrossVIS \cite{yang2021crossover} achieve only 10.8\% and 14.9\% mask AP, respectively. The recent per-frame methods with query-based transformer decoder layers, IDOL \cite{IDOL} and MinVIS \cite{huang2022minvis}, bring impressive improvements, achieving 24.3\% and 26.3\% mask AP, respectively.
However, the query-based transformer methods with per-clip input show unexpectedly low performance, such as 15.1\% by Seqformer \cite{seqformer}. By introducing object token association between frames, VITA\cite{heo2022vita} achieves 19.6\% mask AP.
In comparison, our MDQE can reach 29.2\% mask AP, bringing 9.6\% performance improvement. Besides, by using videos of 720p, our MDQE can further improve the mask AP from 29.2\% to 33.0\%, which is the best result using ResNet50 backbone by far. 

We compare the occlusion metrics of competing VIS methods on OVIS valid set in \cref{tab:sota_occ_matric}. One can see that MDQE achieves impressive improvements on AP$_{mo}$ and AP$_{ho}$ metrics, validating that MDQE can handle the moderately and heavily occluded objects very well.

% \textbf{YouTube-VIS 2022 valid set.} Compared with YouTube-VIS 2021 valid set, the 2022 valid set is extended with long videos for validation and testing. The long videos are mostly captured by using fast-moving cameras, which is rarely occurred in the videos of the training set.
% Here we compare MDQE with those recent and top-performing methods in \cref{tab:sota_yt22long}. The newly proposed per-frame methods InstFormer \cite{koner2022instanceformer}, MinVIS \cite{huang2022minvis} and IDOL \cite{IDOL} achieve 24.8\%, 21.8\% and 34.0\% mask AP, respectively, and SeqFormer\cite{seqformer} and VITA \cite{heo2022vita}, which take the whole video as input, achieve 23.1\% and 32.0 \% mask AP, respectively. 
% Since there are not many occluded instances, with the help of long-time messenger, our MDQE with the clip length $T=1$ can handle the challenging videos well, achieving a competitive mask AP of 33.2\%. 

\textbf{Swin Large backbone.} VIS models with the Swin Large backbone can have higher detection and segmentation abilities on challenging videos. Due to limited space, only the recently developed transformer-based methods are compared in \cref{tab:sota_swinl}. IDOL \cite{IDOL}, SeqFormer \cite{seqformer} and our MDQE adopt the deformable DETR transformer architecture, while VITA \cite{heo2022vita} and MinVIS \cite{huang2022minvis} employ the stronger masked-attention transformer architecture.
On both YouTube-VIS 2021 and OVIS valid sets, MDQE obtains competitive performance with VITA \cite{heo2022vita} and MinVIS \cite{huang2022minvis}.
With 720p video input, IDOL \cite{IDOL} with inter-frame object re-association obtains 42.6\% mask AP on OVIS. Due to our limited computational memory, we only take 2-frame video clips as inputs to train our MDQE; however, it can still reach 42.6\% mask AP. We believe MDQE can achieve higher performance if more frames are used in the clip.
% while on the more challenging OVIS, our MDQE achieves leading performance. 
% , which is the best result by far. 


\textbf{Parameters and Speed.} We follow Detectron2 \cite{wu2019detectron2} to calculate the parameters and FPS. As shown in \cref{tab:sota_yt21_ovis}, compared with the latest per-clip method VITA \cite{heo2022vita}, our MDQE has 51.4M parameters and runs at 37.8 FPS, saving about 9.4\% parameters and speeding up 12\% the run-time.

The visualization of example segmentation results on challenging videos by the competing VIS methods can be found in the \textbf{supplementary materials}. 


