\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{float}
\usepackage{diagbox}
\usepackage{changepage}
\usepackage{multirow}
\usepackage{enumitem}


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{464} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Authors' Responses to the Reviews of Paper \#464}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
We thank all reviewers for their valuable comments. We are sorry that we may not present our method clearly. We will improve the description and correct typos in revision to make the paper easier to understand. We would like to first clarify the common questions raised by all reviewers.
\vspace{-1.5mm}
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=5pt,parsep=0pt,partopsep=0pt]
    \item \textbf{{\color{red} CQ1}: Correct formula of $\mathcal{L}_\text{inter-mask}$}. The weight should be $w_{O_i}= N / n_i$, where $N=THW$ is the total number of pixels in the predicted mask $\hat{M}_i\in R^{T\times H\times W}$ and $n_i$ is the number of pixels belonging to the instance with ID $i$.
    \begin{align}
        & \mathcal{L}_{\textbf{inter-mask}} \nonumber \\
        & =\! \sum\nolimits_{i=1}\nolimits^{K}  w_{O_i} [ \text{BCE}(\hat{M}_i \! \odot\! M_{O_i}, 0) 
        + \text{BCE}(\hat{M}_i \!\odot\! M_i, M_i)] \nonumber \\
        &=\! \sum\nolimits_{i=1}\nolimits^{K}  -\frac{1}{n_i} \sum_{p=1}^{N} M_{{O_i},p}log(1-\hat{M}_{i,p}) + M_{i,p}log(\hat{M}_{i,p}) \nonumber
        \label{eq:inter-inst}
    \end{align}
    
    \item \textbf{{\color{red} CQ2}: $\mathcal{L}_\text{init}$}. The specific formula $\mathcal{L}_\text{init}$ is provided in the part A of supplementary materials due to limited space. We will put it into the main paper in revision.
    \item \textbf{{\color{red} CQ3}: TCA}. Temporal cross-attention is based on the traditional multi-scale deformable attention (MSDefAttn) [1]. The difference is that MSDefAttn extracts deformable points from single-frame multi-scale feature maps, while TCA extracts deformable points from multi-frame single-scale feature maps.
    
    [1] Xizhou Zhu, et al. Deformable detr: Deformable transformers for end-to-end object detection. ICLR 2021.

    \item \textbf{{\color{red} CQ4}: Per-frame MDQE on Table 2.} The long videos on YouTube-VIS 2022 valid set are mostly captured by using fast-moving cameras, which is rarely occurred in the videos of the training set. This means that the training set may drive the per-clip MDQE to learn the temporal prior: objects typically have small motion between adjacent frames. However, the temporal prior does not hold on the YouTube-VIS 2022 valid set, resulting in slightly lower performance compared to per-frame MDQE. Figure 5 of supplementary materials visualizes instance masks on a long video shot by fast-moving device in YouTube-VIS 2022 valid set.
    
\end{itemize}

%[1] Conditional Convolutions for Instance Segmentation. ECCV 2020.
%[2] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. ICCV 2021.

%-------------------------------------------------------------------------

\noindent{\bf \large To Reviewer QkPn:}

{\bf Q1:} \textbf{Sec 3.2.}
1) Object queries indicate per-frame queries, while instance queries represent per-clip queries. 2) For TCA, please refer to \textbf{{\color{red} CQ3}}.

{\bf Q2:} \textbf{Threshold for GT inter-instance mask.} we set $\epsilon=0.001$ as default, which is only used to find all nearby non-target instances. In theory, combinations of all instances are also possible. However, for two instances whose locations are far apart, their mask prediction usually do not affect each other.  The complementary GT inter-instance mask $M_{O_i} = \cup_{j\in O_i} M_j$ is a binary matrix, which is supervised by the common BCE loss, so its gradients can be backpropagated during training.

{\bf Q3:} \textbf{Inter-instance mask repulsion loss.} Please refer to \textbf{{\color{red} CQ1}}, we fixed a bug in the loss formula. The BCE loss already be higher if the number of pixels $n_{O_i}$ belonging to the GT inter-instance mask $M_{O_i}$ is larger. The attraction term $BCE(\hat{M}_i\odot M_i, M_i)$ just pushes the segmenter to predict the pixels of the target instance as 1. While $\mathcal{L}_\text{inter-mask}$ executes on the entire pixels of the image, which also pushes the segmenter to predict background pixels as 0.

{\bf Q4:} \textbf{Others.}  1) Following common settings, we adopt both BCE and Dice losses for $L_\text{intra-mask}$ and $L_\text{mem-mask}$. 2) For {$d_k$}, we employ same normalization term as used in transformer attention. 3) For $\mathcal{L}_\text{init}$, please refer to \textbf{{\color{red} CQ2}}.

%-------------------------------------------------------------------------
\vspace{+1mm}
\noindent{\bf \large To Reviewer 6GSR:}

{\bf Q1: \textbf{$AP_{so}, AP_{mo}, AP_{ho}$}.} Due to limited space, we only provide the comparison with SOTA methods on OVIS here. We will add a comprehensive comparison of occlusion metrics in the revised experimental results.
\begin{table}[h]
\begin{center}
\setlength{\tabcolsep}{0.5mm}{
      \linespread{2}
      \begin{tabular}
      % [t]{cclcccccccccc}
      {p{0.1\textwidth}<{\centering}p{0.115\textwidth}p{0.05\textwidth}<{\centering}p{0.05\textwidth}<{\centering}p{0.05\textwidth}<{\centering}p{0.05\textwidth}<{\centering}}
         \Xhline{1pt}
         Type & Methods & AP & AP$_{so}$ &AP$_{mo}$ &AP$_{ho}$  \\
         \Xhline{1pt}
         \multirow{2}{*}{Per-frame}
         & IDOL          &  24.3 & 34.8 & 29.0 & 8.1 \\
         & MinVIS        &  26.3 & 45.8 & 31.6 & 9.9 \\
         \Xhline{1pt}
         \multirow{3}{*}{Per-clip}
         & SeqFormer     &  15.1 & 31.8 & 17.3 & 3.2 \\   
         & VITA          &  19.6 & 32.3 & 20.4 & 8.2\\
         & MDQE (ours)   &  29.6 & 45.9 & 36.7 & 13.0\\
        \Xhline{1pt}
      \end{tabular}
}
\end{center}
\vspace{-5.5mm}
\caption{ Occlusion metrics comparison of SOTA VIS methods on OVIS valid set.
}\label{tab:sota_yt22long}
\vspace{-4.5mm}
\end{table}

{\bf Q2:} \textbf{Table 2.} Please refer to \textbf{{\color{red} CQ4}}.

{\bf Q3:} \textbf{Table 4(c).} How much is the performance down when $T_o=1$??? $AP_{so}, AP_{mo}, AP_{ho}$?


%-------------------------------------------------------------------------
\vspace{+1mm}
\noindent{\bf \large To Reviewer 9uo4:}  

{\bf Q1:} \textbf{Missing details.} 1) For $\mathcal{L}_\text{init}$, please refer to \textbf{{\color{red} CQ2}}. 2) In Line 462, $q^m$ means the memory-based instance queries. $q$ indicates the instance queries learned from the current clip, and $q^T$ is the transpose of $q$. Please refer to Figure 1 for a better distinction between $q^m$ and $q$. 3) For TCA, please refer to \textbf{{\color{red} CQ3}}. 4) We will add a detailed description for Figure 3 in revision. 5) We employ SeqFormer as the clip-level VIS baseline.
Please refer to Lines 314-319 or Line 586. 

{\bf Q2:} \textbf{Object re-association.} To be honestly, if the object disappears beyond 30 frames and the video contains many similar-looking instances, all existing VIS methods fail to accurately re-associate such instances. Otherwise, our proposed MDQE can re-associate objects, if they have some bias in appearance and simply disappear and reappear in long videos. We will provide some visualization examples in revision.

{\bf Q3:} \textbf{Table 2.} Please refer to \textbf{{\color{red} CQ4}}.

{\bf Q4:} \textbf{Related work.}
We will include all aforementioned related work of frame-level VIS methods in revision.

%-------------------------------------------------------------------------
\vspace{+1mm}
\noindent{\bf \large To Reviewer S1sz:}

{\bf Q1:} \textbf{Many additional components.}
We agree that our core contributions lie in two perspectives: spatial-occlusion and temporal-occlusion. Note that inter-frame query association (Sec. 3.2) has the ability to handle intra-clip temporal-occlusion, and long-time messenger (Sec. 3.4) aims inter-clip temporal-occlusion. We will consider to focus on main contributions and analysis it in-depth in revision.

{\bf Q2:} \textbf{Eq. (3).} We correct the formula of $\mathcal{L}_\text{inter-mask}$ in \textbf{{\color{red} CQ1}}. The complementary GT inter-instance mask $M_{O_i} = \cup_{j\in O_i} M_j$ is determined by the GT masks of its nearby non-target instances, which is a binary matrix and does not need any pre-processing. In code implementation, we further add $M_{O_i} = M_{O_i} \cap (1-M_i)$ to avoid the effect that the annotations of instance segmentation usually are not mutually exclusive. Consider that the number of pixels belonging to the nearby non-target instance may be much larger than those belonging to the target instance, i.e. $n_{O_i} \gg n_i$, then the first item of $\mathcal{L}_{\textbf{inter-mask}}$ may be very large, resulting in $\mathcal{L}_{\textbf{inter-mask}}$ will be very large. To address the instability during training, we limit that the denominator of the weight should be $\max(n_i, (n_+n_{O_i})/3)$ in the implementation. 

{\bf Q3:} \textbf{Long videos.} In Table 4(c), for MDQE without long-time messenger (LTM), replacing $T_o=3$ with $T_o=1$ results in a 3.1\% AP drop in performance. While introducing LTM to MDQE narrows the performance gap to 1.2\% AP. We claim that our proposed MDQE can greatly alleviate the challenging tracking problem in long videos. Furthermore, LTM enables MDQE to achieve high performance when using near-online inference (i.e. $T_o=1$).

{\bf Q4:} \textbf{Tracking metrics.} A toy experiment....

{\bf Q5:} \textbf{Convergence comparison with SeqFormer.} ....

{\bf Q6:} \textbf{Table 4(d).} When $T=9$, performance degradation may mainly come from complex trajectories of objects. We know that object trajectories are usually more complex in long clips than in short clips. Therefore, during training, we may need more long videos to cover various object motions, which helps the model fully explore object complex trajectories. However, there is only 901 videos in the training set of OVIS.


%%%%%%%%% REFERENCES
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

\end{document}
