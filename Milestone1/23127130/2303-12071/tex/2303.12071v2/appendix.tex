\section*{Appendix}

In Section~\ref{leaderboard}, we report additional comparisons with the leaderboard results on the two benchmarks. Section~\ref{attn} presents the visualization of learned attentions across multi-sourced input. Section~\ref{latency} describes the details regarding the latency evaluation of various methods. Section~\ref{viz} shows more qualitative results to illustrate the prediction of the proposed approach. Section~\ref{more_ablation} provides more ablation study. 


\section{ProphNet on Leaderboards}
\label{leaderboard}

In the main paper, we have compared ProphNet with various published methods in Tables \ref{tab:Argo1ranking} and \ref{tab:Argo2}. Here we present more comparisons with the leading results (at the time of submission) including both published and unpublished methods that are reported on the leaderboards of Arogoverse-1\footnote{https://eval.ai/web/challenges/challenge-page/454/leaderboard/1279} and Argoverse-2\footnote{https://eval.ai/web/challenges/challenge-page/1719/leaderboard/4098}.  

To compare with the leaderboard results on Arogoverse-1, we follow the common practice of model ensembling and train six models with different random seeds. We then simply use non-maximum suppression to merge all predicted trajectories to produce the final prediction output. As demonstrated in Table~\ref{tab:Argo1}, ProphNet ranks the 1st among all 278 submissions. 

As for the leaderboard comparison on Arogoverse-2, we train three different models for ensembling, similar as that in Argoverse-1. As shown in Table~\ref{tab:Argo2r}, ProphNet ranks 2nd among all 32 submissions.

We then collect and compare the leading methods from the two leaderboards to see their generalizability across different benchmarks. As listed in Table~\ref{tab:Argo12}, one can find that only ProphNet and QCNet (unpublished) yield stable and superior performance on both leaderboards.  

To sum up, ProphNet achieves the 1st rank on the leaderboard of Argoverse-1 and the 2nd rank on the leaderboard of Argoverse-2. 
We have discussed the comparative study with published methods in the main paper. However, as the details of unpublished methods are not disclosed, we are unable to analyze the comparison with them qualitatively.  
  

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{atten_weights.png}
    \vspace{-6mm}
    \caption{Illustration of the attention distribution of a target agent (red box) over the neighboring agents (ego-vehicle in green box) and road polylines learned by ProphNet.}
    \label{fig:attnviz}
\end{figure}


\section{Visualization of Learned Attentions}
\label{attn}
Next we visualize the learned attentions to multi-sourced input for in-depth understanding of the internal attention mechanism of ProphNet. Figure~\ref{fig:attnviz} illustrates the attention distribution of a target agent to other neighboring agents and road polylines. As shown in this figure, we observe that the target agent pays more attention to its frontal and lateral agents that would be potentially interacted with, while less attention to the agents behind. For road polylines, the target agent attends more on the frontal ones including the straight, left or right polylines that the future trajectories may lie on. This attention distribution learned in ProphNet is reasonably alike how humans attend to different traffic elements at an intersection.  



\section{Details on Latency Evaluation}
\label{latency}
In this section, we describe the details of evaluating inference latency shown in Figure~\ref{intro} and Table~\ref{tab:latency}. As pointed out in the main paper, in addition to prediction accuracy, inference latency is an equally important measurement of a motion forecasting model for real-world driving deployment. Table~\ref{source} lists the sources that are used to conduct the inference latency evaluation. We employ the open-sourced code and models, and evaluate them under the same setting (tested with a single NVIDIA V100 GPU and the number of agents set to 64). As a special case, since there is no open-sourced material of Wayformer~\cite{wayformer}, we implement the model following the specifications detailed in the appendix (Table 5) of related paper~\cite{wayformer}.  

\begin{table*}[t]
  \centering
  \begin{tabular}{@{}lc@{}lc@{}lc@{}lc@{}lc@{}lc@{}lc@{}lc@{}lc@{}lc@{}lc@{}}
    \toprule
    Method &minADE$_6$& &minFDE$_6$& &minADE$_1$& &minFDE$_1$& &MR& &brier-minFDE\\
    \midrule
    %GNet &0.7890& &1.1602& &1.5689& &3.4067& &0.1168& &1.7512\\
    %Proln &0.8046& &1.1554& &1.5737& &3.4467& &0.1177& &1.7483\\
    Wayformer~\cite{wayformer} &0.77& &1.16& &1.64& &3.66& &0.12& &1.74 \\
    VI LaneIter*  &0.77 & &1.11& &1.52& &3.28& &\textbf{0.11}& &1.73 \\
    FFINet*  &0.76& &1.12& &1.53& &3.36& &\textbf{0.11}& &1.73\\
    QCNet*  &\textbf{0.74}& &\textbf{1.07}& &1.54& &3.37& &\textbf{0.11}& &1.70 \\
    ProphNet &0.76& &1.13& &\textbf{1.49}& &\textbf{3.26}& &\textbf{0.11}& &\textbf{1.69} \\
    \bottomrule
  \end{tabular}
  \vspace{-1mm}
  \caption{Comparison of the top 5 results of both published and unpublished methods on the leaderboard of Argoverse-1 by the date November 18, 2022. Note brier-minFDE is the primary ranking metric. * denotes unpublished methods. }
  
  \label{tab:Argo1}
\end{table*}

\begin{table*}[h]
  \centering
  \begin{tabular}{@{}lc@{}lc@{}lc@{}lc@{}lc@{}lc@{}lc@{}lc@{}lc@{}lc@{}lc@{}}
    \toprule
    Method &minADE$_6$& &minFDE$_6$& &minADE$_1$& &minFDE$_1$& &MR& &brier-minFDE\\
    \midrule
    OPPred* &0.71& &1.36& &1.79& &4.61& &0.19& &1.92\\
    TENET~\cite{tenet}  &0.70& &1.38& &1.84& &4.69& &0.19& &1.90 \\
    GNet*  &0.69& &1.34& &1.72& &4.40& &0.18& &1.90\\
    ProphNet &0.66& &1.31& &1.78& &4.80& &0.17& &1.89 \\
    QCNet* &\textbf{0.62}& &\textbf{1.19}& &\textbf{1.56}& &\textbf{3.96}& &\textbf{0.14}& &\textbf{1.78} \\
    \bottomrule
  \end{tabular}
  \vspace{-1mm}
  \caption{Comparison of the top 5 results of both published and unpublished methods on the leaderboard of Argoverse-2 by the date November 18, 2022. Note brier-minFDE is the primary ranking metric. * denotes unpublished methods. }
  \label{tab:Argo2r}
\end{table*}

\begin{table*}[!h]
  \centering
  \begin{tabular}{@{}lc@{}lc@{}lc@{}lc@{}lc@{}lc@{}lc@{}}
    \toprule
    Method &Rank (Argo-1)& &Rank (Argo-2)& &brier-minFDE (Argo-1)& & brier-minFDE (Argo-2)\\
    \midrule
    Proln* &6& &7& &1.75& &1.93\\
    TENET~\cite{tenet}  &11& &4& &1.77& &1.90 \\
    GNet*  &7& &3& &1.75& &1.90\\
    VI LaneIter* &4& &10& &1.73& &2.00\\
    QCNet* &2& &1& &1.70& &1.78&  \\
    ProphNet &1& &2& &1.69& &1.89 \\
    \bottomrule
  \end{tabular}
  \vspace{-1mm}
  \caption{Comparison of the top ranking results of both published and unpublished methods across the leaderboards of Argoverse-1 and Argoverse-2 by the date November 18, 2022. * denotes unpublished methods.}
  \label{tab:Argo12}
\end{table*}

    
\section{More Qualitative Results}
\label{viz}
We further provide more visualization of the predicted trajectories in challenging scenarios. Figure~\ref{fig:allviz} demonstrates four representative scenes with the holistic prediction output by ProphNet. As shown in this figure, (a-b) depict the complex road topologies, where ProphNet is able to produce multiple accurate trajectories that align to the lane centerlines reasonably well; and (c-d) illustrate the crowded intersections, where ProphNet predicts rich and rational multimodal future trajectories.       


\section{More Ablation Study}
\label{more_ablation}
Table~\ref{more-abl} provides more ablation studies for deeper understanding of our approach. We first compare different ways for initializing proposal queries in (a) and (f), where the random initialization is observed to be inferior to the orthogonal initialization. Figure~\ref{rand_orth} further shows the qualitative comparison. We then compare different ways of fusing proposals and anchors in (b) and (f). It is found that the simple summation performs better than the attention that incurs a higher computation cost. We next compare the direct use of anchor points with the proposed anchor embeddings in (c) and (f), and find that the latter is superior, validating the rich goal-oriented contexts encoded in the anchor embeddings. Finally, we compare different ways of input sequence encoding in (d-f), where gMLP outperforms both MLP and 1D-CNN.    
As for the failure cases of our approach, we find some long-horizon trajectories predicted in challenging scenes (e.g., roundabout) are occasionally off the road. One solution to mitigating this issue is to enforce an off-road penalty in training. 

\begin{table}[h]
  %\small
  \centering
  %\resizebox{\columnwidth}{!} 
  %{
  \begin{tabular}{@{}lc@{}lc@{}lc@{}lc@{}}
    \toprule
    Model & minADE$_1$& &minFDE$_1$ \\ % need these $$ to suppress compile errors
    \midrule
    (a) Random Initialization  &1.30& &2.81\\
    (b) Fusion by Attention  &1.29& &2.81\\
    (c) Use of Anchor Points  &1.29& &2.80\\
    (d) MLP Encoding  &1.40& &3.02\\
    (e) 1D-CNN Encoding  &1.31& &2.83\\
    (f) ProphNet-S  &\textbf{1.28}& &\textbf{2.77}\\
    \bottomrule
  \end{tabular}
  %}
  \vspace{-1mm}
  \caption{A variety of additional ablation studies on Argoverse-1.}
  \label{more-abl}
\end{table}

\begin{figure}[h]
        \centering
        \includegraphics[width=\linewidth]{rand_orth.png}
        %\vspace{-15pt}
        \caption{Illustration of the predicted trajectories by ProphNet-S with randomly (left) and orthogonally (right) initialized queries.}
        \label{rand_orth}
\end{figure}





\begin{table*}[t]
  \centering
  \begin{tabular}{p{0.19\textwidth}p{0.8\textwidth}}
    \toprule
    Method  &Source\\
    \midrule
    VectorNet~\cite{vectornet} &\href{https://github.com/Henry1iu/TNT-Trajectory-Prediction}{https://github.com/Henry1iu/TNT-Trajectory-Prediction}\\
    LaneGCN~\cite{liang2020learning}  &\href{https://github.com/uber-research/LaneGCN}{https://github.com/uber-research/LaneGCN}\\
    mmTransformer~\cite{liu2021multimodal}  &\href{https://github.com/decisionforce/mmTransformer}{https://github.com/decisionforce/mmTransformer}\\
    DenseTNT~\cite{densetnt}  &\href{https://github.com/Tsinghua-MARS-Lab/DenseTNT}{https://github.com/Tsinghua-MARS-Lab/DenseTNT} \\
    MultiPath++~\cite{mp++} &\href{https://github.com/stepankonev/waymo-motion-prediction-challenge-2022-multipath-plus-plus.}{https://github.com/stepankonev/waymo-motion-prediction-challenge-2022-multipath-plus-plus} \\
    Wayformer~\cite{wayformer} &Our implementation following the model and training details in the appendix (Table 5) of~\cite{wayformer}\\
    \bottomrule
  \end{tabular}
  \caption{Sources of different methods used for inference latency evaluation.}
  \label{source}
\end{table*}



\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{all_viz.png}
    \caption{Illustration of the multimodal future trajectories predicted by ProphNet at various challenging scenarios with complex road topologies (a-b) and crowded intersections (c-d).}
    \label{fig:allviz}
\end{figure*}

